{
  "title": "Contrasting Linguistic Patterns in Human and LLM-Generated News Text",
  "url": "https://openalex.org/W4392818979",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5025502160",
      "name": "Alberto Muñoz-Ortiz",
      "affiliations": [
        "Universidade da Coruña"
      ]
    },
    {
      "id": "https://openalex.org/A5030874155",
      "name": "Carlos Gómez‐Rodríguez",
      "affiliations": [
        "Universidade da Coruña"
      ]
    },
    {
      "id": "https://openalex.org/A5039380414",
      "name": "David Vilares",
      "affiliations": [
        "Universidade da Coruña"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6600505529",
    "https://openalex.org/W2528664121",
    "https://openalex.org/W6602544630",
    "https://openalex.org/W6605486206",
    "https://openalex.org/W6600004214",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W3104764318",
    "https://openalex.org/W1984940486",
    "https://openalex.org/W2016429631",
    "https://openalex.org/W2114719613",
    "https://openalex.org/W4285210452",
    "https://openalex.org/W3207937903",
    "https://openalex.org/W6601795639",
    "https://openalex.org/W6600212061",
    "https://openalex.org/W6601897980",
    "https://openalex.org/W6608998752",
    "https://openalex.org/W4389519059",
    "https://openalex.org/W3155632693",
    "https://openalex.org/W6607643177",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W4287889356",
    "https://openalex.org/W4285310604",
    "https://openalex.org/W3103649165",
    "https://openalex.org/W6628082049",
    "https://openalex.org/W6602430550",
    "https://openalex.org/W6600266280",
    "https://openalex.org/W6601548976",
    "https://openalex.org/W4353007481",
    "https://openalex.org/W4385565868",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4387561528",
    "https://openalex.org/W4377864601",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4386576626",
    "https://openalex.org/W4366999665",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W4389518954",
    "https://openalex.org/W1515847863",
    "https://openalex.org/W4392120887",
    "https://openalex.org/W3139815689",
    "https://openalex.org/W4378945475",
    "https://openalex.org/W3034287667",
    "https://openalex.org/W4366588626",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4389157038",
    "https://openalex.org/W4379468930",
    "https://openalex.org/W4226178180",
    "https://openalex.org/W4383751019",
    "https://openalex.org/W4307647693",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4389821462",
    "https://openalex.org/W3105900690",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W4323709074",
    "https://openalex.org/W4381586770",
    "https://openalex.org/W4376652984",
    "https://openalex.org/W4362707064",
    "https://openalex.org/W4361866126",
    "https://openalex.org/W4387293246",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4287829148",
    "https://openalex.org/W4386655575",
    "https://openalex.org/W4324098937",
    "https://openalex.org/W2964264262",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4226471298",
    "https://openalex.org/W4226379991",
    "https://openalex.org/W2143995218",
    "https://openalex.org/W4307413986",
    "https://openalex.org/W4392223289",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W3172205429",
    "https://openalex.org/W3168771811",
    "https://openalex.org/W4287332927",
    "https://openalex.org/W4385714610",
    "https://openalex.org/W2785674851",
    "https://openalex.org/W4287120901"
  ],
  "abstract": "<title>Abstract</title> We conduct a quantitative analysis contrasting human-written English news text with comparable large language model (LLM) output from from six different LLMs that cover three different families and four sizes in total. Our analysis spans several measurable linguistic dimensions, including morphological, syntactic, psychometric, and sociolinguistic aspects. The results reveal various measurable differences between human and AI-generated texts. Human texts exhibit more scattered sentence length distributions, more variety of vocabulary, a distinct use of dependency and constituent types, shorter constituents, and more optimized dependency distances. Humans tend to exhibit stronger negative emotions (such as fear and disgust) and less joy compared to text generated by LLMs, with the toxicity of these models increasing as their size grows. LLM outputs use more numbers, symbols and auxiliaries (suggesting objective language) than human texts, as well as more pronouns. The sexist bias prevalent in human text is also expressed by LLMs, and even magnified in all of them but one. Differences between LLMs and humans are larger than between LLMs.",
  "full_text": "Contrasting Linguistic Patterns in Human and LLM-\nGenerated News Text\nAlberto Muñoz-Ortiz \nUniversity of A Coruña\nCarlos Gómez-Rodríguez \nUniversity of A Coruña\nDavid Vilares \nUniversity of A Coruña\nResearch Article\nKeywords: Large Language Models, Computational Linguistics, Machine-generated text, Linguistic\nBiases\nPosted Date: March 14th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-4077382/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nVersion of Record: A version of this preprint was published at Arti\u0000cial Intelligence Review on August\n23rd, 2024. See the published version at https://doi.org/10.1007/s10462-024-10903-2.\nContrasting Linguistic Patterns in Human and\nLLM-Generated News Text\nAlberto Mu˜ noz-Ortiz1*, Carlos G´ omez-Rodr´ ıguez1 and\nDavid Vilares 1\n1*Departamento de Ciencias de la Computaci´ on y Tecnolog´ ıas de la\nInformaci´ on, Universidade da Coru˜ na, CITIC, Campus de Elvi˜ na s/n,A\nCoru˜ na, 15071, A Coru˜ na, Spain.\n*Corresponding author(s). E-mail(s): alberto.munoz.ortiz.com;\nContributing authors: carlos.gomez@udc.es; david.vilares@udc.es;\nAbstract\nWe conduct a quantitative analysis contrasting human-writte n English news text\nwith comparable large language model (LLM) output from from six diﬀ erent\nLLMs that cover three diﬀerent families and four sizes in total. O ur analysis spans\nseveral measurable linguistic dimensions, including morphol ogical, syntactic, psy-\nchometric, and sociolinguistic aspects. The results reveal va rious measurable\ndiﬀerences between human and AI-generated texts. Human texts exhibit more\nscattered sentence length distributions, more variety of vocab ulary, a distinct\nuse of dependency and constituent types, shorter constituen ts, and more opti-\nmized dependency distances. Humans tend to exhibit stronger negative emotions\n(such as fear and disgust) and less joy compared to text generated by LLMs,\nwith the toxicity of these models increasing as their size grow s. LLM outputs\nuse more numbers, symbols and auxiliaries (suggesting objectiv e language) than\nhuman texts, as well as more pronouns. The sexist bias prevalent in human text\nis also expressed by LLMs, and even magniﬁed in all of them but o ne. Diﬀerences\nbetween LLMs and humans are larger than between LLMs.\nKeywords: Large Language Models, Computational Linguistics, Machin e-generated\ntext, Linguistic Biases\n1\nFig. 1: We gather contemporary articles from the New York Times API and use their\nheadlines plus the 3 ﬁrst words of the lead paragraph as prompts to LLMs to gen erate\nnews. We use four LLMs from the LLaMa family (7B, 13B, 30B and 65B sizes), Falcon\n7B and Mistral 7B. We then compare both types of texts, assessing diﬀere nces in\naspects like vocabulary, morphosyntactic structures, and semantic attributes\n1 Introduction\nLarge language models (LLMs) (\nRadford et al. , 2018; Scao et al. , 2022; Touvron et al. ,\n2023) and instruction-tuned variants ( OpenAI, 2023; Taori et al. , 2023) output ﬂuent,\nhuman-like text in many languages, English being the best represent ed. The extent to\nwhich these models truly understand semantics ( Landgrebe and Smith , 2021; Søgaard,\n2022), encode representations of the world ( Li et al. , 2022), generate fake statements\n(Kumar et al. , 2023), or propagate speciﬁc moral and ethical values ( Santurkar et al. ,\n2023) is currently under active debate. Regardless, a crucial factor cont ributing to\nthe persuasiveness of these models lies, in the very ﬁrst place, in their exceptional\nlinguistic ﬂuency.\nA question that arises regards whether their storytelling strategie s align with the\nlinguistic patterns observed in human-generated texts. Do these m odels tend to use\nmore ﬂowery or redundant vocabulary? Do they exhibit preferences for speciﬁc voices\nor syntactic structures in sentence generation? Are they prone to ce rtain psychome-\ntric dimensions? However, contrasting such linguistic patterns i s not trivial. Firstly,\nthe creators of these models often insuﬃciently document the trai ning data used.\nEven with available information, determining the extent of the trai ning set’s inﬂu-\nence on a sentence or whether it is similar to an input sample remains challenging.\nSecond, language is subject to cultural norms, social factors, and geographi c varia-\ntions, which shape linguistic preferences and conventions. Thus, to contrast linguistic\npatterns between humans and machines, it is advisable to rely on a cont rolled envi-\nronment. However, little eﬀort has been made to measure diﬀerences , if any, in syntax,\ngrammar, and other linguistic aspects between the two types of texts. Instead, atten-\ntion has primarily been on explicit biases like societal and demographi c biases ( Liang\net al. , 2021).\n2\nResearch contributions and objectives\nWe study six generative large language models: Mistral 7B (\nJiang et al. , 2023), Falcon\n7B ( Almazrouei et al. , 2023) and the four models (7B, 13B, 30B and 65B) from the\nLLaMa family ( Touvron et al. , 2023). We contrast several linguistic patterns against\nhuman text using English news text. To do so, we recover human-gene rated news\nand ask the models to generate a news paragraph based on the headline and ﬁrs t\nwords of the news. We query the New York Times Archive API to retrieve news\npublished after all the models used were released, to guarantee ste rilization from the\ntraining set. We analyze various linguistic patterns: diﬀerences i n the distribution\nof the vocabulary, sentence length, part-of-speech (PoS) tags, syntac tic structures,\npsychometric features such as the tone of the news articles and emotion s detectable\nin the text, and sociolinguistic aspects like gender bias. We depic t an overview in\nFigure 1. We also explore if these disparities change across models of diﬀeren t sizes\nand families.\n2 Related work\nNext, we survey relevant work to the subject of this paper: (i) analyz ing inherent\nlinguistic properties of machine-generated text, (ii) distinguis hing between machine-\nand human-generated texts, (iii) applications of machine-generated te xt to economize\nhuman labor and time, (iv) generating synthetic text to perform data au gmentation,\nand (v) examining the propagation of biases through data extracted from the I nternet.\n2.1 Analysis of linguistic properties of AI-generated text\nCognitive scientists (\nCai et al. , 2023) have exposed models such as ChatGPT to exper-\niments initially designed for humans. They veriﬁed that it was able t o replicate human\npatterns like associating unfamiliar words to meanings, denoising cor rupted sentences,\nor reusing recent syntactic structures, among other abilities. Ye t, they also showed\nthat ChatGPT tends to refrain from using shorter words to compress me aning, as well\nas from using context to resolve syntactic ambiguities. Similarly, Zhou et al. (2023)\nconducted a thorough comparison between AI-created and human-created mi sinfor-\nmation. They ﬁrst curated a dataset of human-created misinformation per taining to\nthe COVID-19 pandemic. Then, they used these representative doc uments as prompts\nfor GPT-3 to generate synthetic misinformation. By analyzing and contr asting the\noutputs from both sources, the study revealed notable diﬀerences. AI -made fake news\ntended to be more emotionally charged, using eye-catching language. It al so frequently\nraised doubts without proper evidence and jumped to unfounded concl usions. Very\nrecently, Xu et al. (2023) have shed light on the lexical conceptual representations\nof GPT-3.5 and GPT-4. Their study demonstrated that these AI language mode ls\nexhibited strong correlations with human conceptual representation s in speciﬁc dimen-\nsions, such as emotions and salience. However, they encountered chall enges when\ndealing with concepts linked to perceptual and motor aspects, such as visual, gusta-\ntory, hand/arm, or mouth/throat aspects, among others. With the goal of measurin g\ndiﬀerences across both types of texts, Pillutla et al. (2021) introduced MAUVE, a\n3\nnew metric designed to compare the learned distribution of a language gen eration\nmodel with the distributions observed in human-generated texts. Given the inherent\nchallenge in open-ended text generation, where there is no single cor rect output, they\naddress the issue of gauging proximity between distributions by le veraging the con-\ncept of a divergence curve. Following the release of this work as a pre print, other\nauthors have studied the text generated by language models from a lingui stic point\nof view. Mart´ ınez et al. (2023) developed a tool to evaluate the vocabulary knowl-\nedge of language models, testing it on ChatGPT. Other works have also eval uated\nthe lexical abundance of ChatGPT and how it varies with regards to diﬀer ent param-\neters ( Mart´ ınez et al., 2024). Linguistic analysis is proving to be a valuable tool in\nunderstanding LLM outputs. In the line of our work, Rosenfeld and Lazebnik (2024)\nconducted a linguistic analysis of the outputs from three popular LLMs, c oncluding\nthat this type of information can be used for LLM attribution on machine-gene rated\ntexts.\n2.2 Identiﬁcation of synthetically-generated text\nThis research line aims to diﬀerentiate texts generated by machine s from those\nauthored by humans (\nCrothers et al. , 2023), thus contributing to accountability and\ntransparency in various domains. This challenge has been addressed from diﬀerent\nangles including statistical, feature-based methods ( Nguyen-Son et al. , 2017; Fr¨ ohling\nand Zubiaga , 2021) and neural approaches ( Rodriguez et al. , 2022; Zhan et al. , 2023).\nYet, Crothers et al. (2022) recently concluded that except from neural methods, the\nother approaches have little capacity to identify modern machine-ge nerated texts.\nIppolito et al. (2020) observed two interesting behaviors related to this classiﬁcati on\ntask: (i) that fancier sampling methods for generation (e.g., nucleus or untruncated\nrandom sampling) are helpful to better at deveiving humans, but conve rsely make the\ndetection for machines more accessible and simple, and (ii) that show ing longer inputs\nhelp both machines and humans to better detect synthetically-gene rated strings.\nMunir et al. (2021) showed that it was possible to attribute a given synthetically-\ngenerated text to the speciﬁc LLM model that produced it, using a st andard machine\nlearning classiﬁcation architecture that used XLNet ( Yang et al. , 2019) as its back-\nbone. In a diﬀerent line, Dugan et al. (2020) studied whether humans could identify the\nfencepost where an initially human-generated text transitions to a m achine-generated\none. There are also methods that have been speciﬁcally designed to ge nerate or detect\nmachine-generated texts for highly sensible domains, warning about th e dangers of\nlanguage technologies. The SCIgen software ( Stribling et al. , 2005) was able to create\nsemantically non-sense but grammatically correct research papers, wh ose content was\naccepted at some conferences with poor peer-review processes. Mor e recently, Liao\net al. (2023) showed that medical texts generated by ChatGPT were easy to detect :\nalthough the syntax is correct, the texts were more vague and provided on ly only gen-\neral terminology or knowledge. However, this is a hard task and methods t o detect\nAI-generated text are not accurate and are susceptible to suﬀer attacks ( Sadasivan\net al. , 2023).\n4\n2.3 Natural language annotation and data generation using\nLLMs\nThe quality of current synthetically-generated text has encouraged r esearchers to\nexplore their potential for complementing labor-intensive tasks, s uch as annotation\nand evaluation. For instance, He et al. (2022) generated synthetic unlabeled text tai-\nlored for a speciﬁc NLP task. Then, they used an existing supervised classiﬁer to\nsilver-annotate those sentences, aiming to establish a fully synt hetic process for gen-\nerating, annotating, and learning instances relevant to the target proble m. Related,\nChiang and Lee (2023) investigated whether LLMs can serve as a viable replacement\nfor human evaluators in downstream tasks. Particularly, they conducte d experiments\nwhere LLMs are prompted with the same instructions and samples as provi ded to\nhumans, revealing a correlation between the ratings assigned by both ty pes of eval-\nuators. Moreover, there is also work to automatically detect challengin g samples in\ndatasets. For instance, Swayamdipta et al. (2020) already used the LLMs ﬁne-tuning\nphase to identify simple, hard and ambiguous samples. Chong et al. (2022) demon-\nstrated that language models are useful to detect label errors in dataset s by simply\nranking the loss of ﬁne-tuned data.\nLLMs can also contribute in generating high-quality texts to pretrain ot her models.\nPrevious work has used language models to generate synthetic data to in crease the\namount of available data using pretrained models ( Kumar et al. , 2020). Some examples\nof downstream tasks are text classiﬁcation ( Li et al. , 2023), intent classiﬁcation ( Sahu\net al. , 2022), toxic language detection ( Hartvigsen et al. , 2022), text mining ( Tang\net al. , 2023), or mathematical reasoning ( Liu et al. , 2023b), inter alia. Synthetic data\nis also used to pretrain and distill language models. Data quality has be en shown to\nbe a determinant factor for training LLMs. Additional synthetic data can c ontribute\nto scale the dataset size to compensate a small model size, getting mor e capable small\nmodels. LLMs have allowed to generate high-quality, synthetic text that is useful to\ntrain small language models (SLM). One of such cases is ( Eldan and Li , 2023). They\ngenerated high quality data with a constrained vocabulary and topics usi ng GPT-\n3.5 and 4 to train SLM that show coherence, creativity and reasoning in a p articular\ndomain. The Phi models family ( Gunasekar et al. , 2023; Li et al. , 2023a; Javaheripi\net al. , 2023) showed the usefulness of synthetic data in training high-perfor mance but\nSLMs. The authors used a mixture of high-quality textbook data and synt hetically-\ngenerated textbooks to train a highly-competent SLM. Moreover, it has been used\nto create instruction tuning datasets to adequate LLMs behavior to us er prompts\n(Peng et al. , 2023). Synthetic data can also help preventing LLMs from adapting their\nanswers to previous human opinions when they are not objectively corre ct ( Wei et al. ,\n2023). However, although useful, synthetically-generated data may harm p erformance\nwhen the tasks or instances at hand are subjective ( Li et al. , 2023).\nSynthetic datasets provide data whose content is more controllable, as LLMs tend\nto reproduce the structure of the datasets they have been trained on. Most LLMs are\ntrained totally or partially on scraped data from the web, and such unﬁlte red internet\ndata usually contain biases or discrimination as they reproduce the he gemonic view\n(Bender et al. , 2021). Some widely-used huge datasets such as The Pile ( Gao et al. ,\n2020) conﬁrm this. Authors extracted co-occurrences on the data that reﬂe ct racial,\n5\nreligious and gender stereotypes, which are also shown in some models . Some datasets\nare ﬁltered and reﬁned to improve the quality of the data. However, th ey still reproduce\nthe biases in it ( Penedo et al. , 2023). Moreover, Dodge et al. (2021) did an extensive\nevaluation of the data of the C4 dataset ( Raﬀel et al. , 2020), pointing out ﬁltering\ncertain information could increase the bias on minorities. Prejudice s on the data are\nreproduced on the LLMs trained on them, as some studies have pointed out (Weidinger\net al. , 2021). LLMs show the same biases that occur in the datasets, ranging from\nreligious ( Abid et al. , 2021) to gender discrimination ( Lucy and Bamman , 2021).\n3 Data preparation\nNext, we will delve into our data collection process for both human- and machine-\ngenerated content, before proceeding to the analysis and comparison.\n3.1 Data\nWe generate the evaluation dataset relying on news published after the release date\nof the models that we will use in this work. This strategy ensures th at they did not\nhave exposure to the news headlines and their content during pre- training. It is also\nin line with strategies proposed by other authors - such as\nLiu et al. (2023) - who take\nan equivalent angle to evaluate LLMs in the context of generative search engi nes. The\nreference human-generated texts will be the news (lead paragraph) th emselves.\nCrawling\nWe use New York Times news, which we access through its Archive API\n1. Particularly,\nwe gathered all articles available between October 1, 2023, and January 24, 2024,\nresulting in a dataset of 13,371 articles. The articles are retrieved in JSON format,\nand include metadata such as the URL, section name, type of material, keyw ords,\nor publication date. Figure\n2 shows some general information about the topics and\ntype of articles retrieved. We are mainly interested on two ﬁelds: the headline and the\nlead paragraph. The lead paragraph is a summary of the information presented i n the\narticle. We discarded the articles that had an empty lead paragraph. The c ollected\narticles primarily consist of news pieces, although around 26% also inclu de other types\nof texts, such as reviews, editorials or obituaries.\nRationale for focusing on English and the news domain\nThe choice to focus on English texts in our research is guided by a coup le of con-\nsiderations. Firstly, the LLMs we use (as detailed in Section\n3.2) are English-centric.\nLLaMa’s dataset comprises over 70% English content, and Falcon’s even higher at\nover 80%. With Mistral, the speciﬁcs of the training data were not dis closed, adding\nan extra layer of complexity. In this context, it is worth noting that a model trained\npredominantly on data from speciﬁc demographics or regions might develop a bias\ntowards those linguistic patterns, potentially overlooking others . The clarity around\n1https://developer.nytimes.com/docs/archive-product/1/overview\n6\n \nU.S.\n18%\nWorld\n14%\nArts\n8%\nOpinion\n7%\nBusiness Day\n7%\nNew York\n5%\nCrosswords & Games\n4%\nBooks\n3%\nStyle\n3%\nMovies\n3%\nBriefing\n3%\nPodcasts\n2%\nFood\n2%\nThe Learning Network\n2%\nReal Estate\n2%\nCorrections\n2%\nT echnology\n2%\nMagazine\n1%\nScience\n1%\nWell\n1%\nClimate\n1%\nTheater\n1%\nHealth\n1%\nTravel\n1%\nThe Upshot\n1%\nT Magazine\n1%\nTimes Insider\n1% Sports\n0.45%\nObituaries0.232%\nYour Money0.152%\nHeadway0.0436%\n(a) Section name\n \nNews\n74%\nOp-Ed\n7%Review\n4%\nInteractive Feature\n4%\nbriefing\n3%\nObituary (Obit)\n3%\nVideo\n2%\nLetter\n1%\nQuote\n1%\n1%\nCorrection\n1%\nEditorial\n0.472%\nNews Analysis0.24%\nSlideshow0.203%\n(b) Type of material\nFig. 2: Treemaps for the ‘section name’ and ‘type of material’ ﬁelds of the cra wled\narticles\nthe inﬂuence of diverse linguistic inputs on model performance is also limited, further\ncomplicating a fair analysis.\nSecondly, studying additional languages presented extra logistical ch allenges. Col-\nlecting non-English news texts was diﬃcult, especially with the reliance on paid or\nservices with very limited capabilties for access to quality sourc es. The abundance and\naccessibility of English news sources, like the New York Times, great ly facilitated our\ncollection of analyzable content under usable licenses.\n3.2 Generation\nLet H = [ h1, h 2, ..., h N ] be a set of human-generated texts, such that hi is a tuple of\nthe form ( ti, s i) where ti is a headline and si is a paragraph of text with a summary\nof the corresponding news. Similarly, we will deﬁne M = [ m1, m 2, ..., m N ] as the set\nof machine-generated news articles produced by a LLM such that mi is also a tuple of\nthe from ( t′\ni, s ′\ni) where t′\ni = ti and s′\ni = [ w′\n1, w ′\n2, ..., w ′\n|si|] is a piece of synthetic text. For\nthe generation of high-quality text, language models aim to maximize the p robability\nof the next word based on the previous content. To ensure that the mode ls keep on\ntrack with the domain and topic, we initialize the previous content wi th the headline\n(the one chosen by the journalist that released the news) and the ﬁrst three words of\nthe human-generated lead paragraph to help the model start and follow the topic.\n2\nFormally, we ﬁrst condition the model on ci = t′\ni ·si[0:2] and every next word ( i ≥ 3)\nwill be predicted from a conditional distribution P (w′\ni|ci ·s′\ni[3:t−1]).\nTo generate a piece of synthetic text s′, we condition the models with a prompt\nthat includes the headline and ﬁrst words, as described above, and we keep generating\nnews text until the model decides to stop.\n3 We enable the model to output text\n2In preliminary experiments, certain LLM outputs encounter ed diﬃculties in adhering to a minimal\ncoherent structure when a minimum number of the body’s words were absent from the prompt. Also note\nthat the LLMs we are using are not instruction-tuned, and thu s prompting engineering is not particularly\nsuitable, nor the goal of this work.\n3In preliminary experiments, we explored hyperparameter va lues that generated ﬂuent and coherent texts:\ntemperature of 0.7, 0.9 top p tokens, and a repetition penalt y of 1.1.\n7\nwithout any forced criteria, except for not exceeding 200 tokens. The l ength limit\nserves two main purposes: (i) to manage computational resources eﬃcien tly4, and (ii)\nto ensure that the generated content resembles the typical length of human-written\nlead paragraphs, making it comparable to human-produced content. We arri ved at\nthis limit after comparing the average and standard deviation of the numb er of tokens\nbetween humans and models in early experiments.\n3.3 Selected models\nWe rely on six pre-trained generative language models that are represen tative within\nthe NLP community. These models cover 4 diﬀerent sizes (7, 13, 30 and 65 b illion\nparameters) and 3 model families. We only include diﬀerent sizes f or LLaMa as results\nwithin the same family are similar, and larger models need considerabl y more compute.\nWe brieﬂy mention their main particularities below:\nLLaMa models (LL) (\nTouvron et al. , 2023)\nThe main representative for our experiments will be the four model s from the LLaMa\nfamily, i.e. the 7B, 13B, 30B, and 65B models. The LLaMa models are trained on a\ndiverse mix of data sources and domains, predominantly in English, as de tailed in Table\n1. LLaMa is based on the Transformer architecture and integrates several in novations\nfrom other large language models. In comparison to larger models like GPT-3 (Brown\net al. , 2020), PaLM ( Chowdhery et al. , 2023), and Chinchilla ( Hoﬀmann et al. , 2022),\nLLaMa exhibits superior performance in zero and few-shot scenarios. It is also a good\nchoice as a representative example because the various versions, each with a diﬀerent\nsize, will enable us to examine whether certain linguistic patter ns become closer or\nmore diﬀerent to humans in larger models.\nFalcon 7B (F7B) (\nAlmazrouei et al. , 2023)\nIntroduced alongside its larger variants with 40 and 180 billion parameters , it is trained\non 1.5 trillion tokens from a mix of curated and web datasets (see Table 1). Its archi-\ntecture relies on multigroup attention (an advanced form of multiquery attention),\nRotary Embeddings (similar to LLaMa), standard GeLU activation, parallel atte n-\ntion, MLP blocks, and omits biases in linear layers. We primarily chos e this model to\ncompare the results in the following sections with those of its count erpart, LLaMa 7B,\nand to explore whether there are signiﬁcant diﬀerences among models of similar size.\nMistral 7B (M7B) ( Jiang et al. , 2023)\nIt surpasses larger LLaMa models in various benchmarks despite its smal ler size. Its\ndistinctive architecture features Sliding Window Attention, Rolling Buﬀer Cache, and\nPreﬁll and Chunking. The training data for Mistral 7B is not publicly d isclosed, and to\nﬁght against data contamination issues, our analysis only includes article s published\nafter the model’s release. The choice of this model as an object of stud y follows the\n4We ran the models on 2xA100 GPUs for 3 days to generate all text s. To address memory costs, we use\n8-bit precision.\n8\nTable 1: Size and training data of the models used in our experiments\nFamily Size Tokens Data sources\nLLaMa\n7B 1T English CommonCrawl (67%), C4 (15%),\n13B 1T GitHub (4.5%), Wikipedia (4.5%),\n30B 1.5T Gutenberg and Books3 (4.5%), ArXiv (2.5%),\n65B 1.5T Stack Exchange (2%)\nFalcon\nReﬁnedWeb-English (76%), ReﬁnedWeb-Euro (8%),\n7B 1.5T Gutenberg (6%), Conversations (5%)\nGitHub (3%), Technical (2%)\nMistral 7B ? ?\nsame thinking we used for the Falcon model. We want to see how well Mi stral 7B does\nand how its new features stack up against models of the same size.\n4 Analysis of linguistic patterns\nIn this section, we compare human- and machine-generated texts. We ﬁr st inspect the\ntexts under a morphosyntactic optic, and then focus on semantic aspec ts.\n4.1 Morphosyntactic Analysis\nTo compute linguistic representations, we rely on Stanza (\nQi et al. , 2020) to perform\nsegmentation, tokenization, part-of-speech (PoS) tagging, and dependenc y and con-\nstituent parsing. For these tasks, and in particular for the case of Engli sh and news\ntext, the performance is high enough to be used for applications ( Manning, 2011;\nBerzak et al. , 2016), and it can be even superior to that obtained by human anno-\ntations. This also served as an additional reason to focus our analysis on new s text,\nensuring that the tools we rely on are accurate enough to obtain meaningful results.\n4.1.1 Sentence length\nFigure\n3 illustrates the length distribution for the LLMs in comparison to human -\ngenerated news articles. We excluded a few outliers from the plot by ignoring sentences\nwith lengths over 80 tokens. The six LLMs exhibit a similar distribu tion across dif-\nferent sentence lengths, presenting less variation when compare d to human-generated\nsentences, which display a wider range of lengths and greater divers ity. Speciﬁcally,\nthe models exhibit a higher frequency of sentence generation with in the 10 to 30 token\nrange compared to humans, whereas humans tend to produce longer senten ces with\ngreater frequency.\n4.1.2 Richness of vocabulary and lexical variation\nWe analyze the diversity of vocabulary used by the LLMs and compare them agai nst\nhuman texts. We consider the total number of tokens (words), the count of unique\ntokens, and the Type-Token Ratio (TTR). The TTR is a measure of lexic al variation\nand is calculated by dividing the number of types (i.e., unique tok ens) by the total\n9\nFig. 3: Sentence length distribution for the human-written texts and eac h tested\nlanguage model. M stands for Mistral, F for Falcon and LL for LLaMa\nnumber of tokens. Table\n2 presents the results. Human texts displayed a higher diver-\nsity of unique tokens compared to most LLMs — except for the 65B LLaMa model —\ndespite using fewer total tokens. This resulted in a higher TTR and a richer vocabulary.\nIn terms of model size, we see that all LLaMa versions have a similar num ber\nof unique tokens and TTR, but there is a moderate increasing trend as model size\nincreases. Comparing models of the same size (LLaMa 7B, Falcon 7B, and Mistr al\n7B), Falcon 7B is the model that uses the fewest unique tokens (in absol ute terms) by\na wide margin. However, it shows the same TTR as LLaMa 65B, as it also tends to\ngenerate shorter texts and thus the ratio is similar. Finally, Mistr al 7B has the lowest\nTTR and the second lowest number of unique tokens.\nTable 2: Statistics related to the vocabulary\nof the articles generated by humans and each\ntested language model\nModel Tokens Unique Type-token ratio\nHuman 676 591 39 058 0.058\nM7B 741 489 34 399 0.041\nF7B 606 020 29 262 0.048\nLL7B 843 087 37 553 0.045\nLL13B 809 551 37 091 0.046\nLL30B 790 059 38 390 0.049\nLL65B 824 739 39 881 0.048\n10\nTable 3: UPOS frequencies (%) in human- and LLM-generated texts\nUPOS H M7B F7B LL7B LL 13B LL30B LL65B\nNOUN 19.69 17.85 17.72 17.75 17.44 17.64 17.74\nPUNCT 11.88 10.92 12.14 10.77 10.91 11.43 11.22\nADP 11.36 10.58 10.30 10.75 10.63 10.70 10.69\nVERB 9.97 10.37 9.23 10.26 10.23 10.14 10.29\nPROPN 9.61 8.75 9.44 9.14 9.18 9.52 9.50\nDET 9.04 9.00 10.72 8.65 8.64 8.76 8.63\nADJ 7.58 6.69 6.74 6.86 6.76 6.73 6.77\nPRON 5.32 7.12 6.11 7.08 7.33 6.96 6.93\nAUX 3.81 5.77 6.02 5.65 5.74 5.50 5.41\nADV 3.26 3.41 2.61 3.58 3.68 3.41 3.49\nCCONJ 2.65 2.72 2.52 2.68 2.70 2.61 2.67\nPART 2.43 2.76 2.80 2.64 2.63 2.52 2.58\nNUM 1.77 1.95 1.98 2.02 1.98 2.05 2.02\nSCONJ 1.41 1.84 1.37 1.84 1.85 1.71 1.72\nINTJ 0.12 0.08 0.08 0.08 0.08 0.08 0.09\nSYM 0.09 0.17 0.19 0.19 0.19 0.18 0.18\nX 0.03 0.03 0.02 0.05 0.04 0.06 0.07\n4.1.3 Part-of-speech tag distributions\nTable\n3 presents the frequency of universal part-of-speech (UPOS) tags ( Petrov et al. ,\n2012) for both human and LLM-generated texts. Figure 4 shows relative diﬀerences\nobserved across humans and each model, for a better understanding of th e relative use\nof certain grammatical categories. Overall, the behavior of LLMs and their gen erated\ntext tends to be consistent among themselves, yet shows diﬀeren ces when compared\nto human behavior, i.e., they exhibit in some cases a greater or lesse r use of certain\ngrammatical categories. To name a few, humans exhibit a preference for u sing certain\nkinds of content words, such as nouns and adjectives. Humans also use word s tagged as\npunctuation symbols more often (except when compared to Falcon), whic h may be con-\nnected to sentence length, as human users tend to rely on longer sent ences, requiring\nmore punctuation. Alternatively, the language models exhibit a pronoun ced inclina-\ntion towards relying on categories such as symbols or numbers, possibly indicating an\nextra eﬀort by language models to furnish speciﬁc data in order to soun d convincing.\nMoreover, they write pronouns more frequently; we will analyze thi s point later from a\ngender perspective. Comparing LLM families, Mistral and LLaMa show a sim ilar use\nof POS tags, with Mistral being the model that resembles humans the m ost. Falcon,\nhowever, has some strong anomalies in POS tags such as DET or ADV. Regarding model\nsize, the larger the model, the greater the similarity with humans. Nevertheless, dif-\nferences between diﬀerently-sized models are much smaller th an between models and\nhumans.\n11\nFig. 4: Percentage diﬀerences, following Table 3, in the use of each UPOS category\nfor each tested language model in comparison to humans\n4.1.4 Dependencies\nDependency arc lengths\nTable\n4 shows information about the syntactic dependency arcs in human and\nmachine-generated texts. In this analysis, we bin sentences by le ngth intervals\nto alleviate the noise from comparing dependency lengths on sentence s of mixed\nlengths ( Ferrer-i-Cancho and Liu , 2014). Results indicate that dependency lengths and\ntheir distributions are nearly identical for all the LLMs except Falcon and the human\ntexts, which both tend to use longer dependencies than the texts b y the rest of the\nLLMs. This ﬁnding holds true for every sentence length bin for Falcon, and for all but\nthe ﬁrst (length 1-10) in the case of human texts, so we can be reasonably su re that\nit is orthogonal to the variation in sentence length distribution betwe en human and\nLLM texts described earlier. It is also worth noting that, in spite of t he similarities\nbetween humans and Falcon in terms dependency lengths, their synt ax is not that\nsimilar overall: there is a substantial diﬀerence in directionali ty of dependencies, with\nFalcon using more leftward dependencies than both humans and other LLMs. The fact\nthat Falcon-generated texts are not really human-like in terms of depen dency syntax\nis further highlighted in the next section, where we consider a me tric that normalizes\ndependency lengths.\nOptimality of dependencies\nWe compare the degree of optimality of syntactic dependencies betwee n human texts\nand LLMs. It has been observed in human language that dependencies tend to be\nmuch shorter than expected by chance, a phenomenon known as depende ncy length\nminimization (\nFerrer-i-Cancho, 2004; Futrell et al. , 2015). This can be quantiﬁed in a\nrobust way (with respect to sentence length, tree topology and other f actors) by the Ω\noptimality score introduced in Ferrer-i Cancho et al. (2022). This score measures where\nobserved dependency lengths sit with respect to random word orders and optimal word\norders, and is deﬁned as: Ω = Drla −D\nDrla −Dmin\n, where D is the sum of dependency lengths\nin the sentence, Drla is the expected sum of lengths, and Dmin is the optimal sum of\nlengths for the sentence’s tree structure. For optimally-arranged t rees D = Dmin and\n12\nTable 4: Statistics for dependency arcs in sentences of diﬀerent lengths\nfor the texts generated by human writers and each tested language\nmodel. The meaning of the columns is as follows: (%L, %R) percentage\nof left and right arcs, ( ¯l) average arc length, ( ¯lL, ¯lR) average left and\nright arc length, ( σl) standard deviation of arc length, ( σlL , σlR ) stan-\ndard deviation of left and right arc length, and number of sentences\nl Model %L %R ¯l ¯lL ¯lR σ l σ lL σ lR # Sent\n1-10\nHuman 49.40 50.60 2.37 2.89 1.84 1.67 1.90 1.17 4 719\nM7B 50.94 49.06 2.37 2.93 1.83 1.65 1.88 1.16 6 190\nF7B 52.08 47.92 2.39 2.99 1.84 1.62 1.84 1.15 4 596\nLL7B 50.68 49.32 2.37 2.95 1.81 1.65 1.88 1.14 6 114\nLL13B 50.42 49.58 2.37 2.94 1.81 1.65 1.88 1.14 6 711\nLL30B 49.97 50.03 2.37 2.92 1.81 1.65 1.89 1.14 6 808\nLL65B 50.23 49.77 2.36 2.91 1.81 1.64 1.87 1.15 6 652\n11-20\nHuman 58.36 41.64 3.19 4.62 2.17 3.12 3.87 1.86 6 179\nM7B 59.76 40.24 3.12 4.63 2.10 3.03 3.80 1.74 12 113\nF7B 61.41 38.59 3.20 4.79 2.19 3.06 3.85 1.83 9 265\nLL7B 59.74 40.26 3.11 4.63 2.09 3.03 3.81 1.72 12 361\nLL13B 59.69 40.31 3.12 4.63 2.11 3.03 3.81 1.75 12 762\nLL30B 59.62 40.38 3.12 4.63 2.11 3.03 3.80 1.76 13 039\nLL65B 59.43 40.57 3.13 4.63 2.10 3.04 3.81 1.75 12 767\n21-30\nHuman 60.40 39.60 3.64 5.52 2.41 4.42 5.71 2.68 6 153\nM7B 61.00 39.00 3.53 5.50 2.26 4.28 5.65 2.33 10 449\nF7B 62.51 37.49 3.62 5.70 2.38 4.32 5.72 2.46 8 222\nLL7B 60.87 39.13 3.51 5.47 2.25 4.26 5.64 2.30 11 014\nLL13B 60.86 39.14 3.53 5.49 2.27 4.27 5.64 2.34 11 017\nLL30B 60.71 39.29 3.53 5.48 2.27 4.26 5.61 2.34 10 810\nLL65B 60.47 39.53 3.53 5.47 2.26 4.28 5.63 2.35 10 884\n31-40\nHuman 60.84 39.16 3.90 6.07 2.50 5.49 7.32 3.19 4 770\nM7B 60.48 39.52 3.79 5.95 2.38 5.35 7.15 2.98 4 676\nF7B 61.98 38.02 3.89 6.11 2.52 5.35 7.16 3.12 4 064\nLL7B 60.79 39.21 3.78 5.98 2.35 5.34 7.19 2.90 5 790\nLL13B 60.51 39.49 3.79 5.96 2.38 5.33 7.14 2.93 5 280\nLL30B 60.35 39.65 3.81 5.95 2.40 5.33 7.09 2.99 4 949\nLL65B 60.35 39.65 3.79 5.95 2.37 5.31 7.10 2.93 5 430\n+41\nHuman 60.48 39.52 4.01 6.28 2.53 6.20 8.32 3.58 2 967\nM7B 60.09 39.91 3.95 6.23 2.44 6.24 8.45 3.39 1 415\nF7B 61.77 38.23 4.04 6.43 2.56 6.18 8.46 3.44 1 318\nLL7B 59.83 40.17 3.97 6.25 2.44 6.24 8.41 3.43 2 035\nLL13B 60.47 39.53 3.99 6.29 2.48 6.23 8.41 3.50 1 693\nLL30B 60.21 39.79 3.98 6.24 2.49 6.21 8.34 3.53 1 579\nLL65B 60.08 39.92 3.95 6.22 2.45 6.16 8.33 3.37 1 880\nΩ takes a value of 1, whereas for a random arrangement it has an expected value of 0.\nNegative values are possible (albeit uncommon) if dependency lengths ar e larger than\nexpected by chance.\nFigure 5 displays the distribution of Ω values across sentences for human and\nLLM-generated texts. The values were calculated using the LAL library ( Alemany-\nPuig et al. , 2021). Results indicate that the distribution of Ω values is almost ident ical\nbetween all of the LLMs, but human texts show noticeably larger values. T his means\nhuman texts are more optimized in terms of dependency lengths, i.e. they have shorter\ndependencies than expected by a larger margin than those generated by t he LLMs. At\na ﬁrst glance, this might seem contradictory with the results in the previous section,\nwhich showed that human texts had longer dependencies on average than non-Falcon\n13\nFig. 5: Ω value distribution for the human- and LLM-generated texts\nLLM texts. However, there is no real contradiction as the object of measur ement is\ndiﬀerent, and in fact this is precisely the point of using Ω to reﬁn e and complement\nthe previous analysis. While previously we measured dependency d istances in absolute\nterms, Ω measures them controlling for tree topology, i.e., given the s hape of a tree (e.g.\na linear tree which is arranged as a chain of dependents, or a star tree wh ere one node\nhas all the others as dependents), are the words arranged in an order that mi nimizes\ndependencies within possible with that shape?Thus, combining the results from both\nsections we can conclude that while humans produce longer dependenc ies, this is due\nto using syntactic structures with diﬀerent topology, but their w ord order is actually\nmore optimized to make dependencies as short as possible. In turn, we also note that\nwhile Falcon’s dependency lengths seemed diﬀerent from the other LLMs (and more\nhuman-like) in absolute terms, the diﬀerences vanish (with all LLMs including Falcon\nhaving almost identical distributions, and humans being the outlie r) when considering\nΩ.\nDependency types\nTable\n5 lists the frequencies for the main syntactic dependency types i n human and\nmachine-generated texts. We observe similar trends to the previou s sections, with\nLLM texts exhibiting similar uses of syntactic dependencies among t hemselves, with\nFalcon being the most distinct model, while all of them present diﬀ erences compared\nto human-written news. In terms of the LLaMa models - same model in di ﬀerent sizes\n- larger models are slightly closer to the way humans use dependency types. For the\nfull picture, Figure 6 depicts all relative diﬀerences in their use (humans versus each\nLLM), but we brieﬂy comment on a few relevant cases as representative e xamples. For\ninstance, nummod dependencies are more common in LLM-generated texts compared to\nhuman texts. This is coherent with the higher use of the NUM tag in the part-of-speech\ntag distribution analysis. Additionally, we observed higher ratios for ot her dependency\ntypes, such as aux (for which the use of auxiliary verbs was also signiﬁcantly higher\naccording to the UPOS analysis), copula and nominal subjects ( nsubj). Furthermore,\n14\nTable 5: Percentage of words generated by humans and each of the tested\nLLMs that are labeled with a speciﬁc dependency type (deprel). We on ly\ninclude relations with a frequency surpassing 1% within the human texts\ndeprel H M7B F7B LL7B LL13B LL30B LL65B\npunct 11.88 10.92 12.15 10.78 10.91 11.44 11.23\ncase 11.69 10.81 10.75 10.98 10.76 10.89 10.85\ndet 8.88 8.81 10.59 8.45 8.43 8.56 8.43\namod 6.98 5.57 5.73 5.79 5.60 5.71 5.75\nnsubj 6.09 7.20 6.89 7.00 7.21 7.11 7.02\nobl 5.50 5.24 4.67 5.39 5.31 5.36 5.31\nnmod 4.95 4.45 4.84 4.50 4.40 4.47 4.47\ncompound 4.87 4.04 4.46 4.20 4.13 4.27 4.33\nobj 4.28 4.41 3.91 4.22 4.23 4.19 4.27\nadvmod 3.46 3.63 2.91 3.83 3.98 3.65 3.76\nconj 3.07 2.80 2.71 2.83 2.79 2.73 2.83\nmark 2.65 3.35 2.94 3.27 3.28 3.07 3.12\ncc 2.63 2.73 2.54 2.72 2.73 2.63 2.69\nnmod:poss 2.34 2.21 2.01 2.21 2.19 2.19 2.17\nflat 2.04 1.67 1.72 1.79 1.80 1.92 1.91\naux 1.91 2.74 2.72 2.68 2.71 2.58 2.55\nadvcl 1.80 1.67 1.30 1.69 1.70 1.62 1.67\ncop 1.26 1.98 2.28 1.90 2.02 1.92 1.86\nacl:relcl 1.22 1.33 1.29 1.38 1.29 1.26 1.28\nappos 1.19 0.85 1.07 0.92 0.92 0.99 1.00\nnummod 1.14 1.16 1.16 1.22 1.21 1.23 1.21\nxcomp 1.10 1.40 1.27 1.37 1.36 1.30 1.34\nacl 1.06 0.93 0.84 0.92 0.87 0.88 0.93\nFig. 6: Percentage diﬀerences, following Table 5, in the use of dependency relations\nfor each tested language model in comparison to humans\nsyntactic structures from LLMs exhibit signiﬁcantly fewer subtre es involving adjective\nmodiﬁers ( amod dependency type) and appositional modiﬁers ( appos).\n15\nTable 6: Statistics for constituents that arise in sen-\ntences of diﬀerent lengths for the text generated by\nhuman writers and each tested LLM. The meaning of\nthe rows are: ( ¯l) average constituent length, ( σl) stan-\ndard deviation of constituent length, and number of\nsentences\nModel 1-10 11-20 21-30 31-40 +41\n¯l\nH 4.32 6.37 7.90 9.38 10.60\nM7B 4.39 6.55 8.27 9.77 11.01\nF7B 4.43 6.47 8.03 9.47 10.76\nLL7B 4.40 6.57 8.33 9.89 11.19\nLL13B 4.40 6.55 8.27 9.76 11.01\nLL30B 4.40 6.49 8.21 9.68 10.86\nLL65B 4.36 6.53 8.25 9.73 10.96\nσl\nH 2.35 4.64 6.97 9.19 11.24\nM7B 2.35 4.66 6.92 9.13 11.18\nF7B 2.33 4.63 6.80 8.94 11.01\nLL7B 2.35 4.69 6.99 9.24 11.35\nLL13B 2.33 4.68 6.95 9.14 11.19\nLL30B 2.36 4.66 6.94 9.14 11.17\nLL65B 2.34 4.68 6.96 9.17 11.23\n# Sent\nH 4 679 6 180 6 154 4 770 2 966\nM7B 6 108 12 113 10 448 4 678 1 414\nF7B 4 575 9 266 8 211 4 011 1 318\nLL7B 6 039 12 362 11 014 5 789 2 035\nLL13B 6 627 12 762 11 018 5 279 1 693\nLL30B 6 713 13 044 10 806 4 949 1 579\nLL65B 6 569 12 765 10 844 5 430 1 880\n4.1.5 Constituents\nConstituent lengths\nTable\n6 shows the comparison between the distribution of syntactic constit uent lengths\nacross both types of texts. While human-generated sentences, on aver age, surpass the\nlength of those generated by LLMs, the average length of a sentence constit uent for\nLLMs is observed to be greater than for humans. The standard deviation exhi bits simi-\nlar values across all models for each sentence length range. Similar to pr evious sections,\nFalcon 7B also displays the largest diﬀerences among language models. Wit hin the\nLLaMa models, we can observe a clear decreasing trend with size which is broken by\nthe 65B model, for which constituent lengths increase again across most of t he length\nbins.\n16\nTable 7: Percentage of spans generated by humans and LLMs labeled\nwith a speciﬁc constituent type. Only constituent types that con form\nmore than 1% of the human’s texts spans are shown\nType H M7B F7B LL7B LL13B LL30B LL65B\nNP 42.91 40.17 40.02 40.69 40.54 39.96 41.42\nVP 18.08 20.18 20.29 19.97 20.02 20.59 20.19\nPP 14.12 12.91 12.69 12.94 12.81 12.62 12.81\nS 11.79 13.09 13.31 13.12 13.12 13.40 13.27\nSBAR 3.64 4.34 4.29 4.09 4.15 4.34 3.84\nADVP 2.39 2.50 2.62 2.44 2.49 2.37 1.86\nADJP 1.97 1.78 1.82 1.76 1.79 1.75 1.80\nNML 1.73 1.43 1.43 1.47 1.52 1.40 1.66\nWHNP 1.41 1.47 1.49 1.53 1.44 1.46 1.47\nConstituent types\nTable\n7 and Figure 7 examine the disparities in constituent types between human- and\nLLM-generated texts. Our focus was on constituent types that occur mor e than 1%\nof the times. Comparing humans and LLMs, some outcomes are in the same line of\nearlier ﬁndings: human-generated content displays heightened use of noun, adjective,\nand prepositional phrases ( NP, ADJP, and PP, respectively). On the contrary, there is\nminimal divergence in the frequency of adverb phrases ( ADVP) except for Falcon 7B,\nwhich shows a great diﬀerence. human and LLM-generated texts, the latt er exhibits\na more pronounced propensity for verb phrases ( VP). Despite the similar frequency\nof the VERB UPOS tag in human and LLM-generated texts, the latter exhibit a more\npronounced propensity for verb phrases ( VP), consistent with the increased use of\nauxiliary verbs (whose UPOS tag is AUX, not VERB) that we saw in previous sections.\nFinally, we see that language models use a considerably larger amount of sub ordinate\nclauses ( SBAR). Regarding model families, results are similar to those of depende ncies\nand POS tags, but when looking at model size, previous trends are less obvious.\n4.2 Semantic Analysis\nAs in the previous section, we are relying on state-of-the-art NLP model s to accurately\nanalyze diﬀerent semantic dimensions: (i) emotions, (ii) text sim ilarities, and (iii)\ngender biases, in an automated way.\n4.2.1 Emotions\nTo study diﬀerences in the emotions conveyed by human- and LLM-generat ed outputs,\nwe relied on the\nHartmann (2022) emotion model. Table 8 provides the percentage of\narticles labeled with distinct emotional categories, including anger, disgust, fear,\njoy, sadness, surprise, and a special tag neutral to denote that no emotion is\npresent in the text. Figure 8 depicts the percentage of articles associated with each\nemotion for each large language model used, as compared to human-written tex ts.\nAs anticipated in journalistic texts, a substantial majority of the le ad paragraphs are\n17\nFig. 7: Percentage diﬀerences, following Table 7, in the use of constituent labels for\neach tested language model in comparison to humans\nclassiﬁed as neutral. This category accounts for over 50% of the texts across all models\nand human-generated samples, with the LLM-generated text demonstratin g a slightly\nhigher inclination towards neutrality.\nConcerning the remainder of the samples, human texts demonstrate a gr eater\ninclination towards negative and aggressive emotions like disgust and fear . However,\nhumans and LLMs generated roughly the same amount of angry texts. In contrast,\nLLMs tend to generate more texts imbued with positive emotions, such as surprise and\nespecially joy. The LLMs also produce many sad texts, a passive but ne gative emo-\ntion, yet less toxic than emotions such as anger or fear. Across LLaMa models, fear\nincreases as the number of parameters grows (from LLaMa 13B), making them mor e\nakin to human texts. Since LLaMa (version 1 models) were not ﬁne-tune d with rein-\nforcement learning with human feedback, we hypothesize the main sou rce contributing\nto this issue might be some pre-processing steps used for the LLaMa m odels, such as\nremoving toxic content from its data. Yet, LLaMa’s technical report (\nTouvron et al. ,\n2023) mentioned an increase in model toxicity as they scaled up in size d espite using\nthe same pre-processing in all cases, which is coherent with our ﬁn dings. When looking\nat families, Mistral comes closest to expressing emotions in a way si milar to humans,\nand Falcon expresses more joy and less anger and surprise than the rest of the models.\n4.2.2 Text similarity\nWe conducted an analysis of the cosine semantic similarity between le ad paragraphs\ngenerated by various LLMs and their human-authored counterparts. Our obje ctive\nwas to investigate the impact of model sizes on the semantic similarit y between these\ntexts. To achieve so, we used a a state-of-the-art sentence similar ity model called\nall-mpnet-base-v2\n5 (Reimers and Gurevych , 2019). Figure 9 illustrates the distribu-\ntion of the similarity scores obtained from our analysis. Results show t hat smaller-sized\n5https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n18\nTable 8: Percentage of articles generated by humans and LLMs that are\nlabeled with diﬀerent emotions\nModel Emotion\nanger disgust fear joy neutral sadness surprise\nH 8.04 9.35 10.77 8.30 52.16 8.51 2.87\nM7B 7.29 7.65 8.34 9.80 53.83 9.72 3.37\nF7B 6.11 8.32 8.77 8.53 56.55 8.99 2.73\nLL7B 7.13 7.19 8.68 8.97 55.57 9.43 3.01\nLL13B 7.72 7.41 8.69 9.00 53.95 9.72 3.51\nLL30B 7.39 7.45 8.61 9.54 54.23 9.59 3.19\nLL65B 7.45 8.26 9.25 9.10 53.65 8.80 3.49\nFig. 8: Relative diﬀerence of emotion labels of articles generated by diﬀere nt LLMs\nin comparison to human texts\nLLMs do not necessarily result in a decrease in sentence similarity compared to the\nhuman-authored texts. Diﬀerences across families are negligible.\n4.2.3 Gender bias\nAlthough related as well with our study with part-of-speech tag distri bution, we here\nseparately analyze the proportion between masculine and feminine pron ouns used in\nboth human- and LLM-generated text. Based on the morphological output by Stanz a,\nwe ﬁnd the words that are pronouns have the features Gender=Masc and Gender=Fem,\nrespectively. Results in Table\n9 indicate that the already biased human texts use male\n19\nFig. 9: Similarity scores between the sentences generated by the LLMs and h uman\ntext\nTable 9: Male-to-female ratio of pronouns used by the text\ngenerated by humans and each LLM\nModel Male-Female ratio Diﬀerence with humans\nH 1.71 -\nM7B 1.74 3.06 %\nF7B 1.64 -7.54 %\nLL7B 1.86 14.30%\nLL13B 1.89 17.13 %\nLL30B 1.87 15.73 %\nLL65B 1.88 17.04 %\npronouns 1.71 times more frequently than female pronouns. This is exace rbated by all\nmodels but Falcon 7B, which, although still heavily biased towards mal e pronouns,\nreduces the bias by 7.5%. LlaMa models, on the contrary, use around 15% more m ale\nthan female pronouns in comparison to humans. This quantity is roughly th e same\nfor every size. Mistral 7B lies in the middle, with a slight increas e of the male-female\nratio of 3% with regards to human text.\n5 Conclusion\nThis paper presented a comprehensive study on linguistic pattern s in texts produced\nby both humans and machines, comparing them under controlled condition s. To keep\nup with current trends, we used modern generative models. To en sure the novelty\nof texts and address memorization concerns, we fed the LLMs headlines fr om news\narticles published after the release date of the models. The study revealed that despite\ngenerating highly ﬂuent text, these models still exhibited noti ceable diﬀerences when\ncompared to human-generated texts. More precisely, at the lexical le vel, large language\nmodels relied on a more restricted vocabulary, except for LLaMa 65B. Addi tionally, at\nthe morphosyntactic level, discernible distinctions were obser ved between human and\n20\nmachine-generated texts, the latter having a preference for parts of speech displaying\n(a sense of) objectivity - such as symbols or numbers - while using s ubstantially\nfewer adjectives. We also observed variations in terms of syntactic s tructures, both\nfor dependency and constituent representations, speciﬁcally in t he use of dependency\nand constituent types, as well as the length of spans across both types of t exts. In\nthis respect our comparison shows, among other aspects, that all tested LLMs choose\nword orders that optimize dependency lengths to a lesser extent th an humans; while\nthey have a tendency to use more auxiliary verbs and verb phrases and less noun and\nprepositional phrases. In terms of semantics, while exhibiting a gr eat text similarity\nwith respect to the human texts, the models tested manifested l ess propensity than\nhumans for displaying aggressive negative emotions, such as fear or anger. M istral 7B\ngenerated texts whose emotion distributions are more similar to humans than those\nof LLaMa and Falcon models. However, we noted a rise in the volume of negative\nemotions with the models’ size. This aligns with prior ﬁndings that as sociate larger\nsizes with heightened toxicity (\nTouvron et al., 2023). Finally, we detected an inclination\ntowards the use of male pronouns, surpassing the frequency in compari son to their\nhuman counterparts. All models except Falcon 7B exacerbated this bias .\nFunding\nWe acknowledge the European Research Council (ERC), which has funde d this\nresearch under the Horizon Europe research and innovation programme (SALSA,\ngrant agreement No 101100615); SCANNER-UDC (PID2020-113230RB-C21) funded\nby MICIU/AEI/10.13039/501100011033; Xunta de Galicia (ED431C 2020/11); GAP\n(PID2022-139308OA-I00) funded by MICIU/AEI/10.13039/501100011033/ and by\nERDF, EU; Grant PRE2021-097001 funded by MICIU/AEI/10.13039/501100011033\nand by ESF+ (predoctoral training grant associated to project PID2020-113230RB -\nC21); and Centro de Investigaci´ on de Galicia “CITIC”, funded by the Xun ta de\nGalicia through the collaboration agreement between the Conseller´ ıa d e Cultura, Edu-\ncaci´ on, Formaci´ on Profesional e Universidades and the Galician univers ities for the\nreinforcement of the research centres of the Galician University Sy stem (CIGUS).\nAuthor contribution\nConceptualization: AMO, CGR, DV; Data curation: AMO; Investigation: AMO, C GR,\nDV; Visualization: AMO; Software: AMO; Methodology: AMO, CGR, DV; Project\nAdministration: CGR, DV; Software: AMO; Validation: AMO, CGR, DV; Experi -\nments: AMO; Formal analysis: AMO, CGR, DV; Writing - original draft: AMO, C GR,\nDV; Writing - Review & Editing: AMO, CGR, DV; Funding Adquisition; C GR, CV\nConﬂict of interest\nThe authors have no competing interest to declare that are relevant t o the content of\nthis paper.\n21\nReferences\nAlmazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M.,\nGoﬃnet, ´E., Hesslow, D., Launay, J., Malartic, Q., et al.: The falcon series of op en\nlanguage models. arXiv preprint arXiv:2311.16867 (2023)\nAlemany-Puig, L., Esteban, J., Ferrer-i-Cancho, R.: The Linear Arrangem ent Library.\nA new tool for research on syntactic dependency structures. In: Pr oceedings of the\nSecond Workshop on Quantitative Syntax (Quasy, SyntaxFest 2021), pp. 1–16. Asso-\nciation for Computational Linguistics, Soﬁa, Bulgaria (2021).\nhttps://aclanthology.\norg/2021.quasy-1.1\nAbid, A., Farooqi, M., Zou, J.: Persistent anti-muslim bias in large l anguage models.\nIn: Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, pp.\n298–306 (2021)\nBender, E.M., Gebru, T., McMillan-Major, A., Shmitchell, S.: O n the dangers of\nstochastic parrots: Can language models be too big? In: Proceedings of th e 2021\nACM Conference on Fairness, Accountability, and Transparency, pp. 610–623\n(2021)\nBerzak, Y., Huang, Y., Barbu, A., Korhonen, A., Katz, B.: Anchoring and agreemen t in\nsyntactic annotations. In: Proceedings of the 2016 Conference on Empiri cal Methods\nin Natural Language Processing, pp. 2215–2224. Association for Computational\nLinguistics, Austin, Texas (2016).\nhttps://doi.org/10.18653/v1/D16-1239 . https:\n//aclanthology.org/D16-1239\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Neelakan-\ntan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., K rueger,\nG., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter , C., Hesse, C.,\nChen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCan-\ndlish, S., Radford, A., Sutskever, I., Amodei, D.: Language models are f ew-shot\nlearners. In: Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M.F., Lin , H. (eds.)\nAdvances in Neural Information Processing Systems, vol. 33, pp. 1877–1901. Cur-\nran Associates, Inc., ??? (2020).\nhttps://proceedings.neurips.cc/paper ﬁles/paper/\n2020/ﬁle/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\nCai, Z.G., Haslett, D.A., Duan, X., Wang, S., Pickering, M.J.: Does chat gpt resemble\nhumans in language use? arXiv preprint arXiv:2303.08014 (2023)\nChong, D., Hong, J., Manning, C.: Detecting label errors by using pre- trained lan-\nguage models. In: Proceedings of the 2022 Conference on Empirical Method s in\nNatural Language Processing, pp. 9074–9091. Association for Computational Lin-\nguistics, Abu Dhabi, United Arab Emirates (2022).\nhttps://aclanthology.org/2022.\nemnlp-main.618\nCrothers, E., Japkowicz, N., Viktor, H.L.: Machine-generated text: A c omprehensive\n22\nsurvey of threat models and detection methods. IEEE Access (2023)\nCrothers, E., Japkowicz, N., Viktor, H., Branco, P.: Adversarial robustn ess of neural-\nstatistical features in detection of generative transformers. In: 2022 I nternational\nJoint Conference on Neural Networks (IJCNN), pp. 1–8 (2022). IEEE\nChiang, C.-H., Lee, H.-y.: Can large language models be an alternative to human\nevaluations? In: Proceedings of the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pp. 15607–15631. Association\nfor Computational Linguistics, Toronto, Canada (2023).\nhttps://aclanthology.org/\n2023.acl-long.870\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Roberts, A., Barham,\nP., Chung, H.W., Sutton, C., Gehrmann, S., et al.: Palm: Scaling language modeling\nwith pathways. Journal of Machine Learning Research 24(240), 1–113 (2023)\nDugan, L., Ippolito, D., Kirubarajan, A., Callison-Burch, C.: RoFT: A to ol for\nevaluating human detection of machine-generated text. In: Proceedi ngs of the\n2020 Conference on Empirical Methods in Natural Language Processing: System\nDemonstrations, pp. 189–196. Association for Computational Linguistics, Online\n(2020).\nhttps://doi.org/10.18653/v1/2020.emnlp-demos.25 . https://aclanthology.\norg/2020.emnlp-demos.25\nDodge, J., Sap, M., Marasovi´ c, A., Agnew, W., Ilharco, G., Groeneveld , D., Mitchell,\nM., Gardner, M.: Documenting large webtext corpora: A case study on th e colossal\nclean crawled corpus. arXiv preprint arXiv:2104.08758 (2021)\nEldan, R., Li, Y.: Tinystories: How small can language models be and still s peak\ncoherent english? arXiv preprint arXiv:2305.07759 (2023)\nFerrer-i-Cancho, R.: Euclidean distance between syntactically l inked words. Physical\nReview E 70, 056135 (2004) https://doi.org/10.1103/PhysRevE.70.056135\nFerrer-i-Cancho, R., G´ omez-Rodr´ ıguez, C., Esteban, J.L., Aleman y-Puig, L.: Opti-\nmality of syntactic dependency distances. Physical Review E 105(1), 014308\n(2022)\nFerrer-i-Cancho, R., Liu, H.: The risks of mixing dependency lengt hs from sequences\nof diﬀerent length. Glottotheory 5(2), 143–155 (2014) https://doi.org/10.1515/\nglot-2014-0014\nFutrell, R., Mahowald, K., Gibson, E.: Large-scale evidence of depen dency length\nminimization in 37 languages. Proceedings of the National Academy of Sciences\n112(33), 10336–10341 (2015)\nhttps://doi.org/10.1073/pnas.1502134112\nFr¨ ohling, L., Zubiaga, A.: Feature-based detection of automated language mode ls:\ntackling gpt-2, gpt-3 and grover. PeerJ Computer Science 7, 443 (2021)\n23\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He,\nH., Thite, A., Nabeshima, N., et al.: The pile: An 800gb dataset of diverse text for\nlanguage modeling. arXiv preprint arXiv:2101.00027 (2020)\nGunasekar, S., Zhang, Y., Aneja, J., Mendes, C.C.T., Del Giorno, A., Gopi , S., Java-\nheripi, M., Kauﬀmann, P., Rosa, G., Saarikivi, O., et al.: Textbooks ar e all you\nneed. arXiv preprint arXiv:2306.11644 (2023)\nHartmann, J.: Emotion English DistilRoBERTa-base. https://huggingface.co/\nj-hartmann/emotion-english-distilroberta-base/ (2022)\nHoﬀmann, J., Borgeaud, S., Mensch, A., Buchatskaya, E., Cai, T., Ruther ford, E.,\nCasas, D.d.L., Hendricks, L.A., Welbl, J., Clark, A., et al.: Training c ompute-optimal\nlarge language models. arXiv preprint arXiv:2203.15556 (2022)\nHartvigsen, T., Gabriel, S., Palangi, H., Sap, M., Ray, D., Kamar, E.: Tox iGen: A large-\nscale machine-generated dataset for adversarial and implicit hate speec h detection.\nIn: Proceedings of the 60th Annual Meeting of the Association for Computati onal\nLinguistics (Volume 1: Long Papers), pp. 3309–3326. Association for Computational\nLinguistics, Dublin, Ireland (2022).\nhttps://doi.org/10.18653/v1/2022.acl-long.234\n. https://aclanthology.org/2022.acl-long.234\nHe, X., Nassar, I., Kiros, J., Haﬀari, G., Norouzi, M.: Generate, annotate, and\nlearn: NLP with synthetic text. Transactions of the Association for Compu tational\nLinguistics 10, 826–842 (2022) https://doi.org/10.1162/tacl a 00492\nIppolito, D., Duckworth, D., Callison-Burch, C., Eck, D.: Automatic detection of gen-\nerated text is easiest when humans are fooled. In: Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics, pp. 1808–1822. Associ-\nation for Computational Linguistics, Online (2020).\nhttps://doi.org/10.18653/v1/\n2020.acl-main.164 . https://aclanthology.org/2020.acl-main.164\nJavaheripi, M., Bubeck, S., Abdin, M., Aneja, J., Bubeck, S., Men des, C.C.T., Chen,\nW., Del Giorno, A., Eldan, R., Gopi, S., et al.: Phi-2: The surprising power of small\nlanguage models. Microsoft Research Blog (2023)\nJiang, A.Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D.S., C asas, D.d.l.,\nBressand, F., Lengyel, G., Lample, G., Saulnier, L., et al.: Mistral 7b. arXiv preprint\narXiv:2310.06825 (2023)\nKumar, S., Balachandran, V., Njoo, L., Anastasopoulos, A., Tsvetkov, Y.: Language\ngeneration models can cause harm: So what can we do about it? an actionable\nsurvey. In: Proceedings of the 17th Conference of the European Chapter of the\nAssociation for Computational Linguistics, pp. 3299–3321. Association for Com-\nputational Linguistics, Dubrovnik, Croatia (2023).\nhttps://aclanthology.org/2023.\neacl-main.241\n24\nKumar, V., Choudhary, A., Cho, E.: Data augmentation using pre-trained tr ansformer\nmodels. In: Proceedings of the 2nd Workshop on Life-long Learning for Sp oken\nLanguage Systems, pp. 18–26. Association for Computational Linguistics, Suzhou,\nChina (2020).\nhttps://aclanthology.org/2020.lifelongnlp-1.3\nLucy, L., Bamman, D.: Gender and representation bias in GPT-3 generated stories. In:\nProceedings of the Third Workshop on Narrative Understanding, pp. 48–55. Assoc i-\nation for Computational Linguistics, Virtual (2021). https://doi.org/10.18653/v1/\n2021.nuse-1.5 . https://aclanthology.org/2021.nuse-1.5\nLi, Y., Bubeck, S., Eldan, R., Del Giorno, A., Gunasekar, S., Lee, Y.T.: T extbooks are\nall you need ii: phi-1.5 technical report. arXiv preprint arXiv:2309.05463 (2023)\nLiu, B., Bubeck, S., Eldan, R., Kulkarni, J., Li, Y., Nguyen, A., Ward, R ., Zhang,\nY.: Tinygsm: achieving¿ 80% on gsm8k with small language models. arXiv preprin t\narXiv:2312.09241 (2023)\nLi, K., Hopkins, A.K., Bau, D., Vi´ egas, F., Pﬁster, H., Wattenberg, M.: E mergent\nworld representations: Exploring a sequence model trained on a syn thetic task. arXiv\npreprint arXiv:2210.13382 (2022)\nLiao, W., Liu, Z., Dai, H., Xu, S., Wu, Z., Zhang, Y., Huang, X., Zhu, D., Cai, H., Li u,\nT., et al.: Diﬀerentiate chatgpt-generated and human-written medic al texts. arXiv\npreprint arXiv:2304.11567 (2023)\nLandgrebe, J., Smith, B.: Making ai meaningful again. Synthese 198, 2061–2081\n(2021)\nLiang, P.P., Wu, C., Morency, L.-P., Salakhutdinov, R.: Towards unde rstanding and\nmitigating social biases in language models. In: International Conferen ce on Machine\nLearning, pp. 6565–6576 (2021). PMLR\nLiu, N.F., Zhang, T., Liang, P.: Evaluating veriﬁability in generative se arch engines.\narXiv preprint arXiv:2304.09848 (2023)\nLi, Z., Zhu, H., Lu, Z., Yin, M.: Synthetic data generation with large langu age\nmodels for text classiﬁcation: Potential and limitations. In: Bouamor, H. , Pino,\nJ., Bali, K. (eds.) Proceedings of the 2023 Conference on Empirical Met hods in\nNatural Language Processing, pp. 10443–10461. Association for Computational\nLinguistics, Singapore (2023).\nhttps://doi.org/10.18653/v1/2023.emnlp-main.647 .\nhttps://aclanthology.org/2023.emnlp-main.647\nManning, C.D.: Part-of-speech tagging from 97% to 100%: is it time for some lin guis-\ntics? In: International Conference on Intelligent Text Processin g and Computational\nLinguistics, pp. 171–189 (2011). Springer\nMunir, S., Batool, B., Shaﬁq, Z., Srinivasan, P., Zaﬀar, F.: Through th e looking\n25\nglass: Learning to attribute synthetic text generated by language model s. In: Pro-\nceedings of the 16th Conference of the European Chapter of the Association for\nComputational Linguistics: Main Volume, pp. 1811–1822. Association for Computa-\ntional Linguistics, Online (2021).\nhttps://doi.org/10.18653/v1/2021.eacl-main.155 .\nhttps://aclanthology.org/2021.eacl-main.155\nMart´ ınez, G., Conde, J., Reviriego, P., Merino-G´ omez, E., Hern´ andez, J.A., Lombardi,\nF.: How many words does chatgpt know? the answer is chatwords. arXiv prep rint\narXiv:2309.16777 (2023)\nMart´ ınez, G., Hern´ andez, J.A., Conde, J., Reviriego, P., Merino, E.: Beware of words:\nEvaluating the lexical richness of conversational large language models. ar Xiv\npreprint arXiv:2402.15518 (2024)\nNguyen-Son, H.-Q., Tieu, N.-D.T., Nguyen, H.H., Yamagishi, J., Zen, I.E. : Identify-\ning computer-generated text using statistical analysis. In: 2017 Asia-P aciﬁc Signal\nand Information Processing Association Annual Summit and Conference (AP SIPA\nASC), pp. 1504–1511 (2017). IEEE\nOpenAI: GPT-4 Technical Report (2023)\nPetrov, S., Das, D., McDonald, R.: A universal part-of-speech tagset . In: Proceed-\nings of the Eighth International Conference on Language Resources and Evaluati on\n(LREC’12), pp. 2089–2096. European Language Resources Association (ELRA),\nIstanbul, Turkey (2012).\nhttp://www.lrec-conf.org/proceedings/lrec2012/pdf/274\nPaper.pdf\nPeng, B., Li, C., He, P., Galley, M., Gao, J.: Instruction tuning with gpt-4. arXiv\npreprint arXiv:2304.03277 (2023)\nPenedo, G., Malartic, Q., Hesslow, D., Cojocaru, R., Cappelli, A., Al obeidli, H.,\nPannier, B., Almazrouei, E., Launay, J.: The reﬁnedweb dataset for falc on llm:\noutperforming curated corpora with web data, and web data only. arXiv prep rint\narXiv:2306.01116 (2023)\nPillutla, K., Swayamdipta, S., Zellers, R., Thickstun, J., Wel leck, S., Choi, Y.,\nHarchaoui, Z.: Mauve: Measuring the gap between neural text and human tex t\nusing divergence frontiers. Advances in Neural Information Processi ng Systems 34,\n4816–4828 (2021)\nQi, P., Zhang, Y., Zhang, Y., Bolton, J., Manning, C.D.: Stanza: A python natu -\nral language processing toolkit for many human languages. In: Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics: Syste m\nDemonstrations, pp. 101–108. Association for Computational Linguistics, Online\n(2020).\nhttps://doi.org/10.18653/v1/2020.acl-demos.14 . https://aclanthology.org/\n2020.acl-demos.14\n26\nReimers, N., Gurevych, I.: Sentence-bert: Sentence embeddi ngs using siamese bert-\nnetworks. In: Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing. Association for Computational Linguistics, ??? (2019). https:\n//arxiv.org/abs/1908.10084\nRodriguez, J., Hay, T., Gros, D., Shamsi, Z., Srinivasan, R.: Cross- domain detection\nof GPT-2-generated technical text. In: Proceedings of the 2022 Conferen ce of the\nNorth American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies, pp. 1213–1233. Association for Computational Linguistics,\nSeattle, United States (2022).\nhttps://doi.org/10.18653/v1/2022.naacl-main.88 .\nhttps://aclanthology.org/2022.naacl-main.88\nRosenfeld, A., Lazebnik, T.: Whose llm is it anyway? linguistic compar ison and llm\nattribution for gpt-3.5, gpt-4 and bard. arXiv preprint arXiv:2402.14533 (2024)\nRadford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.: Impro ving language\nunderstanding by generative pre-training (2018)\nRaﬀel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y. , Li,\nW., Liu, P.J.: Exploring the limits of transfer learning with a uni ﬁed text-to-text\ntransformer. Journal of Machine Learning Research 21(140), 1–67 (2020)\nSanturkar, S., Durmus, E., Ladhak, F., Lee, C., Liang, P., Hashimoto, T.: Wh ose\nopinions do language models reﬂect? arXiv preprint arXiv:2303.17548 (2023)\nScao, T.L., Fan, A., Akiki, C., Pavlick, E., Ili´ c, S., Hesslow, D., C astagn´ e, R., Luccioni,\nA.S., Yvon, F., Gall´ e, M., et al.: Bloom: A 176b-parameter open-access mul tilingual\nlanguage model. arXiv preprint arXiv:2211.05100 (2022)\nStribling, J., Krohn, M., Aguayo, D.: Scigen-an automatic cs paper generat or (2005)\nSadasivan, V.S., Kumar, A., Balasubramanian, S., Wang, W., Feizi, S.: Can ai-\ngenerated text be reliably detected? arXiv preprint arXiv:2303.11156 (2023)\nSøgaard, A.: Understanding models understanding language. Synthese 200(6), 443\n(2022)\nSahu, G., Rodriguez, P., Laradji, I., Atighehchian, P., Vazquez, D., Bahdanau, D.:\nData augmentation for intent classiﬁcation with oﬀ-the-shelf large language models.\nIn: Proceedings of the 4th Workshop on NLP for Conversational AI, pp. 47–57.\nAssociation for Computational Linguistics, Dublin, Ireland (2022).\nhttps://doi.org/\n10.18653/v1/2022.nlp4convai-1.5 . https://aclanthology.org/2022.nlp4convai-1.5\nSwayamdipta, S., Schwartz, R., Lourie, N., Wang, Y., Hajishirzi, H., Smi th, N.A.,\nChoi, Y.: Dataset cartography: Mapping and diagnosing datasets with training\ndynamics. In: Proceedings of the 2020 Conference on Empirical Methods in Natural\n27\nLanguage Processing (EMNLP), pp. 9275–9293. Association for Computational Lin-\nguistics, Online (2020).\nhttps://doi.org/10.18653/v1/2020.emnlp-main.746 . https:\n//aclanthology.org/2020.emnlp-main.746\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Li ang,\nP., Hashimoto, T.B.: Alpaca: A strong, replicable instruction-followi ng model.\nStanford Center for Research on Foundation Models. https://crfm. stanf ord.\nedu/2023/03/13/alpaca. html 3(6), 7 (2023)\nTang, R., Han, X., Jiang, X., Hu, X.: Does synthetic data generation of llms hel p\nclinical text mining? arXiv preprint arXiv:2303.04360 (2023)\nTouvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bas hlykov,\nN., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferre r, C.C., Chen,\nM., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Ful ler, B., Gao, C.,\nGoswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas,\nM., Kerkez, V., Khabsa, M., Kloumann, I., Korenev, A., Koura, P.S., Lachau x, M.-\nA., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Miha ylov, T.,\nMishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R ., Saladi,\nK., Schelten, A., Silva, R., Smith, E.M., Subramanian, R., Tan, X.E. , Tang, B.,\nTaylor, R., Williams, A., Kuan, J.X., Xu, P., Yan, Z., Zarov, I., Zhang, Y. , Fan,\nA., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., S cialom, T.:\nLlama 2: Open Foundation and Fine-Tuned Chat Models (2023)\nWei, J., Huang, D., Lu, Y., Zhou, D., Le, Q.V.: Simple synthetic data red uces\nsycophancy in large language models. arXiv preprint arXiv:2308.03958 (2023)\nWeidinger, L., Mellor, J., Rauh, M., Griﬃn, C., Uesato, J., Huang, P.-S. , Cheng, M.,\nGlaese, M., Balle, B., Kasirzadeh, A., et al.: Ethical and social risks of harm from\nlanguage models. arXiv preprint arXiv:2112.04359 (2021)\nXu, Q., Peng, Y., Wu, M., Xiao, F., Chodorow, M., Li, P.: Does conceptual r epre-\nsentation require embodiment? insights from large language models. arXi v preprint\narXiv:2305.19103 (2023)\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V.: Xlnet: Gen-\neralized autoregressive pretraining for language understanding. Advance s in neural\ninformation processing systems 32 (2019)\nZhan, H., He, X., Xu, Q., Wu, Y., Stenetorp, P.: G3detector: General gpt-ge nerated\ntext detector. arXiv preprint arXiv:2305.12680 (2023)\nZhou, J., Zhang, Y., Luo, Q., Parker, A.G., De Choudhury, M.: Synthetic lies:\nUnderstanding ai-generated misinformation and evaluating algorithmic and human\nsolutions. In: Proceedings of the 2023 CHI Conference on Human Factors in\nComputing Systems, pp. 1–20 (2023)\n28",
  "topic": "Linguistics",
  "concepts": [
    {
      "name": "Linguistics",
      "score": 0.5531882643699646
    },
    {
      "name": "Natural language processing",
      "score": 0.42878854274749756
    },
    {
      "name": "Computer science",
      "score": 0.4069759249687195
    },
    {
      "name": "History",
      "score": 0.33963102102279663
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3343062698841095
    },
    {
      "name": "Philosophy",
      "score": 0.0860782265663147
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I11019714",
      "name": "Universidade da Coruña",
      "country": "ES"
    }
  ]
}