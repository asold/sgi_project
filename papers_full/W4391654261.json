{
  "title": "Vision-Language Model for Generating Textual Descriptions From Clinical Images: Model Development and Validation Study",
  "url": "https://openalex.org/W4391654261",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2139889941",
      "name": "Jia Ji",
      "affiliations": [
        "Shenzhen Institute of Information Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2017486844",
      "name": "Yongshuai Hou",
      "affiliations": [
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2098275461",
      "name": "Xinyu Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2489957300",
      "name": "Youcheng Pan",
      "affiliations": [
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A1966593198",
      "name": "Yang Xiang",
      "affiliations": [
        "Peng Cheng Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2152772232",
    "https://openalex.org/W2334763311",
    "https://openalex.org/W2611650229",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2903721568",
    "https://openalex.org/W2979861699",
    "https://openalex.org/W3104609094",
    "https://openalex.org/W3204650824",
    "https://openalex.org/W3157560208",
    "https://openalex.org/W4375869712",
    "https://openalex.org/W4385571155",
    "https://openalex.org/W4385570257",
    "https://openalex.org/W3103694015",
    "https://openalex.org/W3177048142",
    "https://openalex.org/W3174714208",
    "https://openalex.org/W3181252431",
    "https://openalex.org/W3173688449",
    "https://openalex.org/W4285531589",
    "https://openalex.org/W4281729070",
    "https://openalex.org/W3130502265",
    "https://openalex.org/W4285108627",
    "https://openalex.org/W4385570515",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2133512280",
    "https://openalex.org/W3101156210"
  ],
  "abstract": "Background The automatic generation of radiology reports, which seeks to create a free-text description from a clinical radiograph, is emerging as a pivotal intersection between clinical medicine and artificial intelligence. Leveraging natural language processing technologies can accelerate report creation, enhancing health care quality and standardization. However, most existing studies have not yet fully tapped into the combined potential of advanced language and vision models. Objective The purpose of this study was to explore the integration of pretrained vision-language models into radiology report generation. This would enable the vision-language model to automatically convert clinical images into high-quality textual reports. Methods In our research, we introduced a radiology report generation model named ClinicalBLIP, building upon the foundational InstructBLIP model and refining it using clinical image-to-text data sets. A multistage fine-tuning approach via low-rank adaptation was proposed to deepen the semantic comprehension of the visual encoder and the large language model for clinical imagery. Furthermore, prior knowledge was integrated through prompt learning to enhance the precision of the reports generated. Experiments were conducted on both the IU X-RAY and MIMIC-CXR data sets, with ClinicalBLIP compared to several leading methods. Results Experimental results revealed that ClinicalBLIP obtained superior scores of 0.570/0.365 and 0.534/0.313 on the IU X-RAY/MIMIC-CXR test sets for the Metric for Evaluation of Translation with Explicit Ordering (METEOR) and the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) evaluations, respectively. This performance notably surpasses that of existing state-of-the-art methods. Further evaluations confirmed the effectiveness of the multistage fine-tuning and the integration of prior information, leading to substantial improvements. Conclusions The proposed ClinicalBLIP model demonstrated robustness and effectiveness in enhancing clinical radiology report generation, suggesting significant promise for real-world clinical applications.",
  "full_text": "Original Paper\nVision-Language Model for Generating Textual Descriptions From\nClinical Images: Model Development and Validation Study\nJia Ji1, MSc; Yongshuai Hou2, PhD; Xinyu Chen3, BSc; Youcheng Pan2, PhD; Yang Xiang2, PhD\n1Shenzhen Institute of Information Technology, Shenzhen, China\n2Peng Cheng Laboratory, Shenzhen, China\n3Harbin Institute of Technology, Shenzhen, China\nCorresponding Author:\nYoucheng Pan, PhD\nPeng Cheng Laboratory\nNo. 2 Xingke 1st Street\nShenzhen, 518000\nChina\nPhone: 86 18566668732\nEmail: panyoucheng4@gmail.com\nAbstract\nBackground: The automatic generation of radiology reports, which seeks to create a free-text description from a clinical\nradiograph, is emerging as a pivotal intersection between clinical medicine and artificial intelligence. Leveraging natural language\nprocessing technologies can accelerate report creation, enhancing health care quality and standardization. However, most existing\nstudies have not yet fully tapped into the combined potential of advanced language and vision models.\nObjective: The purpose of this study was to explore the integration of pretrained vision-language models into radiology report\ngeneration. This would enable the vision-language model to automatically convert clinical images into high-quality textual\nreports.\nMethods: In our research, we introduced a radiology report generation model named ClinicalBLIP, building upon the foundational\nInstructBLIP model and refining it using clinical image-to-text data sets. A multistage fine-tuning approach via low-rank adaptation\nwas proposed to deepen the semantic comprehension of the visual encoder and the large language model for clinical imagery.\nFurthermore, prior knowledge was integrated through prompt learning to enhance the precision of the reports generated. Experiments\nwere conducted on both the IU X-RAY and MIMIC-CXR data sets, with ClinicalBLIP compared to several leading methods.\nResults: Experimental results revealed that ClinicalBLIP obtained superior scores of 0.570/0.365 and 0.534/0.313 on the IU\nX-RAY/MIMIC-CXR test sets for the Metric for Evaluation of Translation with Explicit Ordering (METEOR) and the\nRecall-Oriented Understudy for Gisting Evaluation (ROUGE) evaluations, respectively. This performance notably surpasses that\nof existing state-of-the-art methods. Further evaluations confirmed the effectiveness of the multistage fine-tuning and the integration\nof prior information, leading to substantial improvements.\nConclusions: The proposed ClinicalBLIP model demonstrated robustness and effectiveness in enhancing clinical radiology\nreport generation, suggesting significant promise for real-world clinical applications.\n(JMIR Form Res 2024;8:e32690) doi: 10.2196/32690\nKEYWORDS\nclinical image; radiology report generation; vision-language model; multistage fine-tuning; prior knowledge\nIntroduction\nRadiology reports offer essential textual descriptions of\nradiographs and play a pivotal role in the medical diagnosis and\ntreatment process. Their precise interpretation can directly\ninfluence patient outcomes, underscoring the gravity of each\nassessment. However, even for seasoned radiologists,\ninterpreting these images can be a meticulous task, often taking\nseveral minutes per image. In an era where timely medical\nintervention can be lifesaving, streamlining this process becomes\nimperative. Recognizing the immense potential to ease the\nworkload of the health care sector and improve patient care,\nthere has been a growing interest in the research for automatic\nradiology report generation.\nJMIR Form Res 2024 | vol. 8 | e32690 | p. 1https://formative.jmir.org/2024/1/e32690\n(page number not for citation purposes)\nJi et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nAs shown in Figure 1, several attempts have been made in the\nmedical field to create medical reports from images. In the early\nstage, most researchers used traditional deep learning methods,\nsuch as convolutional neural networks (CNNs) and recurrent\nneural networks (RNNs), to produce radiology reports. IU\nX-RAY proposed by Demner-Fushman et al [1] was a significant\nstep in this direction. In addition, Shin et al [2] innovatively\napplied a CNN-RNN model for structured report creation. Wang\net al [3] used a nonhierarchical CNN-long-short term memory\napproach, emphasizing both semantic and visual cues. Vinyals\net al [4] introduced visual attention mechanisms in the realm\nof image captioning with CNN-RNN structures. Subsequently,\nradiology report creation has evolved to adopt transformer-based\nmodels [5,6]. The Knowledge-Driven Encode, Retrieve,\nParaphrase method was proposed by Li et al [7] to ensure\naccurate medical report generation. To better recognize common\nradiographic findings, Yuan et al [8] suggested pretraining\nencoders with an array of chest x-ray images. Chen et al [9] put\nforward the idea of producing radiology reports using a\nmemory-centric transformer. Meanwhile, Pino et al [10]\nadvocated for a template-driven methodology for x-ray report\ngeneration. In their model, clinical templates are defined for\neach abnormality, signaling its presence or lack thereof.\nHowever, this method falls short in conveying specific patient\ndetails like anatomical positions or size dimensions. Addressing\nthis, Wang et al [11] introduced a template-oriented\nmultiattention report generation model, which is tailored\nespecially for standard reports.\nFigure 1. Example of a radiology report generation task.\nRecently, vision-language models (VLMs) [12-15] have become\nleading approaches, which use pretrained transformer models\nto handle both visual and textual data at the same time. These\nmodels are very good at understanding and creating content\nbased on images and texts. One key feature is cross-modal\nlearning [16,17], where VLMs learn to match specific image\npatterns with their related descriptions or findings. This\nunderstanding helps in making reports that are more relevant\nand accurate. VLMs have the potential to greatly improve\nradiology report generation by increasing accuracy, making\nprocesses faster, and ensuring consistency. However, it is\nimportant to address challenges related to data quality,\nintegration, and rules when using VLMs in clinical settings.\nThus, designing an effective fine-tuning method to boost VLM’s\nknowledge and understanding of medical images and texts is a\nvery interesting research direction.\nIn this study, we fine-tune a medical VLM named ClinicalBLIP\nthrough a multistage fine-tuning strategy for the radiology report\ngeneration task. First, a joint optimization method that combines\nself-attention fine-tuning via low-rank adaptation (LoRA) [18]\nwith layer normalization [19] is proposed to enhance the\nunderstanding of clinical images by a general visual encoder.\nThe training target is the text generation loss of the large\nlanguage model (LLM) without introducing extra clinical\nimage-text pairs for further pretraining. Second, the joint\nfine-tuning process for both the image-text transformation layer\nand the multilayer perceptron (MLP) layer of the language\nmodel is designed to allow the LLM to draw upon its internal\ncapability to generate the final report. In addition, we further\nincorporate the prior information to light the specialized clinical\nknowledge inherent in the LLM. Also, the clinical tag and brief\ndescription of the image as a text prompt are fed into the model\nfor training and prediction. Experiments were conducted on the\nIU X-RAY [1] and MIMIC-CXR [20] data sets. We compared\nthe proposed model with 11 competitor methods and analyzed\nthe performance in several aspects. It is demonstrated that the\nproposed ClinicalBLIP achieved state-of-the-art performance\nand can effectively combine the introduced textual prior\nknowledge with clinical images to generate better reports.\nMethods\nData Set\nWe evaluated our proposed method on the IU X-RAY [1] and\nMIMIC-CXR [20] data sets. Both data sets have been\nautomatically deidentified.\nThe IU X-ray data set comprises 7470 images and 3955 reports.\nThe images consist of chest x-rays originating from Indiana\nUniversity. Each report in the data set primarily encompasses\nmultiple attributes such as comparison, indications, findings,\nand impressions. Reports with empty findings were excluded,\nresulting in 3337 remaining reports. Subsequently, we divided\nthe remaining reports into training and testing sets in a 4:1 ratio,\nyielding 2668 reports for training and 669 reports for testing.\nThe MIMIC-CXR data set was created by the Massachusetts\nInstitute of Technology. The images are sourced from 65,379\npatients who presented to the Beth Israel Deaconess Medical\nCenter Emergency Department between 2011 and 2016. We\nused 152,173 medical reports for training and 1196 reports for\ntesting. In this data set, each data entry comprises a specific\nreport and 1 to 3 corresponding images.\nJMIR Form Res 2024 | vol. 8 | e32690 | p. 2https://formative.jmir.org/2024/1/e32690\n(page number not for citation purposes)\nJi et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nOverview of the Proposed Method\nOur work aims to transform clinical radiographs, accompanied\nby additional information, into textual descriptions that convey\nthe same semantic meaning as the images. To achieve this, we\nintroduce the ClinicalBLIP model, as depicted in Figure 2. This\nmodel comprises three core modules: (1) a visual encoder for\nconverting clinical images into semantic representations; (2)\nquery transformer (Q-Former), a crucial component for bridging\nthe image-text gap; and (3) a LLM for generating textual reports\nbased on queries learned from Q-Former and textual prompts.\nFigure 2. Overview of the proposed ClinicalBLIP model. LLM: large language model; Q-Former: query transformer.\nInitially, we briefly introduce the structure and pretraining of\nthe ClinicalBLIP, which draws inspiration from Li et al [21],\nespecially how Q-Former as an intermediate module effectively\nconnects visual and textual data. Subsequently, we delve into\nthe details of how to effectively fine-tune the task of radiology\nreport generation.\nQ-Former to Bridge the Modality Gap\nQ-Former is designed to link a fixed image encoder with a\nstandard LLM. Notably, it can extract a consistent set of features\nfrom the visual encoder, regardless of the input image resolution.\nAs shown in Figure 3, the model is composed of two primary\ntransformer submodules: (1) an image transformer for direct\ninteraction with the visual encoder and (2) a text transformer\nthat serves as both encoder and decoder. The efficacy of the\nQ-Former is greatly influenced by learnable query embeddings,\nwhich facilitate self-attention and cross-attention layer\ninteractions. These embeddings also enable communication\nwith text through similar attention mechanisms. During its 2\npretraining phases, that is, vision-language representation\nlearning and vision-to-language generative learning, Q-Former\nuses distinct attention masks for specific tasks, controlling the\ninteraction between queries and text.\nFigure 3. Model architecture of query transformer (Q-Former).\nVision-Language Representation Learning From\nVisual Encoder\nIn the representation learning phase, Q-Former, connected to a\nfrozen visual encoder, undergoes pretraining with image-text\npairs. The objective here is to train the model to enable queries\nto extract visual representations corresponding to the text.\nInspired by Li et al [22], 3 pretraining tasks are jointly\noptimized, using the same input format and model parameters.\nAs illustrated in Figure 3, these tasks include image-text\ncontrastive learning, image-grounded text generation, and\nimage-text matching. Image-text contrastive learning aligns\nJMIR Form Res 2024 | vol. 8 | e32690 | p. 3https://formative.jmir.org/2024/1/e32690\n(page number not for citation purposes)\nJi et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nimage and text representations by contrasting the similarity of\na positive image-text pair against that of negative pairs.\nImage-grounded text generation encourages the Q-Former to\ncompel the queries to extract visual features that contain the\nwhole information of the text. Image-text matching seeks to\ncapture fine-grained alignment between image and text\nrepresentations through a binary classification task. Each task\nuses a specific attention-masking strategy to control the\ninteraction between queries and text.\nVision-to-Language Generative Learning From LLM\nDuring the generative pretraining phase, Q-Former, connected\nto a frozen LLM, leverages its language generation capabilities.\nA fully connected layer is used to linearly project the output\nquery embeddings to match the dimension of the LLM’s text\nembedding. These embeddings then act as visual prompts,\nguiding the LLM based on the visual representation captured\nby Q-Former. Since Q-Former has been trained to extract visual\nrepresentations that are informative for language, it effectively\nserves as an information filter, providing only the most relevant\ninformation to the LLM and excluding unnecessary visual\ndetails. This setup reduces the load on the LLM to learn\nvision-language alignment, mitigating the risk of the catastrophic\nforgetting problem.\nGeneral Vision-Language Instruction Tuning\nFollowing the pretraining phases, as in Dai et al [23], Q-Former\nundergoes a vision-language instruction tuning process. Here,\nthe LLM integrates visual encodings from Q-Former with\nadditional instruction text tokens. The instruction interacts with\nthe query embeddings through the Q-Former’s self-attention\nlayers. This interaction aids in extracting relevant image\nfeatures, which are further provided to optimize the LLM for\nfollowing instructions. Both quantitative and qualitative analyses\nconfirm the effectiveness of the instruction tuning process in\nachieving vision-language zero-shot generalization.\nEffective Fine-Tuning of Radiology Report Generation\nTo enhance the performance of a general visual encoder and an\nLLM for medical image understanding and report generation,\nvarious aspects need careful consideration. As shown in Figure\n4, a multistage parameter fine-tuning approach is used to\nimprove model performance, namely visual encoder\nenhancement and vision-language joint training.\nFigure 4. Multistage fine-tuing on radiology report generation. LLM: large language model; Q-Former: query transformer.\nIn the first stage, the model’s weights are adjusted to focus more\non relevant features within medical images. This refinement\naids in understanding critical elements such as lesions, organs,\nand more. Concurrently, layer normalization is applied to\nmaintain a consistent response across varying image scales and\nbrightness levels. The primary objective here is the generation\nloss of the LLM, aiming to improve the quality of final reports\nby enhancing the visual encoder’s ability to use visual\ninformation more effectively during report generation, without\nthe need for additional medical text data for further pretraining.\nIn the second stage, the joint training process encompasses the\nfusion of visual and textual inputs, and crucially, the\nincorporation of the attention layer and MLP layer of the LLM.\nThe model simultaneously processes information from the visual\nencoder and textual sources. The attention layer enables dynamic\nfocus on specific regions of medical images, aligning with\nfeatures crucial for report generation. Meanwhile, the MLP\nlayer transforms the combined visual-textual data, boosting the\nmodel’s ability to generate contextually accurate and coherent\nmedical reports. The whole approach ensures full use of the\nmodel’s attention and transformation capabilities, yielding\nmedically precise and linguistically sound reports, thus\neffectively bridging the gap between visual and textual data.\nMoreover, general LLMs often struggle with the absence of\nspecialized medical domain knowledge in the medical report\ngeneration task. To mitigate the issue, we incorporate the prior\ninformation into the model during the second stage. Specifically,\nmedical tags related to the medical image and a brief image\ndescription are embedded as text prompts. In training, these\nprompts are linked to corresponding medical images, facilitating\nthe model’s comprehension of the image content. This\nassociation enables the model to better learn medical\nJMIR Form Res 2024 | vol. 8 | e32690 | p. 4https://formative.jmir.org/2024/1/e32690\n(page number not for citation purposes)\nJi et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\ndomain-specific terms and concepts. This embedding of text\nprompts guides the model with domain knowledge, addressing\nits limitations in the medical field. During prediction, these\nprompts provide additional contextual information, enabling\nthe model to better comprehend medical images, identify\nfeatures within them, and express the medical reports in a more\nprofessional manner.\nIn our research, we analyze 2 relevant data sets: IU X-RAY [1]\nand the MIMIC-CXR [20]. Each of them has unique prior\ninformation as input. The IU X-RAY data set enriches the model\nwith essential prior information, including “problem,” “image,”\nand “indication.” The input template for the IU X-RAY data\nset is formatted as follows:\n“Problems: {problem} \\n Image: {image} \\n Indication:\n{indication} \\n”,\nexemplified by “Problems: normal \\n Image: Chest, 2 views,\nfrontal and lateral \\n Indication: Pruritic \\n”.\nIn contrast, the MIMIC-CXR data set lacks direct access to\nsimilar prior information. To maintain consistency, we use the\nCheXBERT [24] model to extract medical observations from\nthe reports within the MIMIC-CXR data set. The input template\nfor this data set is formatted as follows:\n“Symptoms of existence: {} \\n Symptoms of non-existence: {}\n\\n”,\nillustrated by “Symptoms of existence: Cardiomegaly,\nAtelectasis \\n Symptoms of non-existence: Edema,\nConsolidation \\n”.\nExperimental Settings\nWe adopt the InstructBLIP [23] as the base model, in which\ncontrastive language-image pretraining [13] and Flan-T5-XL\n[25] are used as visual encoders and LLM structures,\nrespectively. In the training phase, we integrated LoRA [18]\ninto both the visual encoder and the language model. This\nintegration of LoRA was strategically implemented within the\nquery projection and value projection stages during self-attention\noperations, enhancing the model’s ability to capture and leverage\nrelational information. For the training process, we configured\nour settings as follows: a batch size of 3 was used, and gradient\naccumulation was carried out over 4 steps to facilitate stable\nand efficient training. The initial learning rate for the Q-Former\nparameters was set to 1×10–4, while the initial learning rate for\nthe LoRA-related parameters was established at 5×10–4. To\ndynamically adapt the learning rate during training, we used a\ncosine decay learning rate scheduler, optimizing the convergence\nand fine-tuning process. Furthermore, to enhance the training\nefficiency and minimize memory consumption, we used float16\nprecision, a half-precision training technique, which effectively\nbalances training speed and model performance. This\ncomprehensive approach allowed us to train our model\neffectively, incorporating LoRA’s enhancements for improved\nperformance and robustness. All the experiments are conducted\non a graphics processing unit (NVIDIA V100).\nTo evaluate the performance of the ClinicalBLIP model, we\ncompared our method with the following 11 state-of-the-art\nmethods. R2GEN [9] is a memory-driven radiology report\ngeneration model with a relational memory to record the\ninformation from the previous generation processes and a layer\nnormalization mechanism to incorporate the memory. CA [26]\nis a contrastive attention model to capture and depict\nabnormalities by comparing the input image with known normal\nimages. CMCL [27] is a novel competence-based multimodal\ncurriculum learning framework to alleviate data bias by\nefﬁciently using limited medical data for medical report\ngeneration. Posterior-and-Prior Knowledge\nExploring-and-distilling [28] is an effective approach to\nexploring and distilling posterior and prior knowledge for\nradiology report generation. R2GEN enhanced with cross-modal\nmemory networks [29] is a radiology report generation model\nwith cross-modal memory networks in which a memory matrix\nis used to record the alignment and interaction between images\nand texts, and another memory is used to perform querying and\nresponding to obtain the shared information across modalities.\nALIGNTRANSFORMER [30] is a radiology report generation\nmodel to alleviate the data bias problem and model the very\nlong sequence. Knowledge Matters [31] is a novel radiology\ngeneration framework assisted by general and speciﬁc\nknowledge. Meshed-Memory Transformer [32] is a simple but\neffective progressive text generation model to produce the\nradiology report by incorporating high-level concepts into the\ngeneration progress. Reinforcement Learning Over a\nCross-Modal Memory (CMM-RL) [33] is an enhanced radiology\nreport generation model with reinforced cross-modal alignment\nto alleviate the requirement of annotated supervision while\nfacilitating interactions across modalities. Cross-Modal\nContrastive Attention (CMCA) [34] is a novel model to capture\nboth visual and semantic information from similar cases.\nObservation-Guided Radiology Report Generation (ORGAN)\n[35] is a generation framework that can maintain the clinical\nconsistency between radiographs and generated free-text reports.\nWe adopted natural language generation metrics to evaluate the\nmethods. Specifically, we selected Bilingual Evaluation\nUnderstudy (BLEU) [36], Metric For Evaluation of Translation\nwith Explicit Ordering (METEOR) [37], and Recall-Oriented\nUnderstudy for Gisting Evaluation (ROUGE) [38]. BLEU-1,\nBLEU-2, BLEU-3, BLEU-4, METEOR, and ROUGE-L are\nreported.\nBLEU is primarily used to evaluate the quality of\nmachine-generated translations by comparing them to 1 or more\nreference translations. It computes a precision-based metric by\ncounting the number of n-grams (contiguous sequences of n\nitems, usually words) in the generated translation that matches\nany reference translation. In this work, BLEU is used to evaluate\nthe generated text report.\nMETEOR is based on the harmonic mean of unigram precision\nand recall, with recall weighted higher than precision. It\nincorporates features not found in other metrics, such as\nstemming and synonymy matching, along with standard exact\nword matching. The metric was designed to address some of\nthe issues found in the more popular BLEU metric and to\nproduce a good correlation with human judgment at the sentence\nor segment level.\nJMIR Form Res 2024 | vol. 8 | e32690 | p. 5https://formative.jmir.org/2024/1/e32690\n(page number not for citation purposes)\nJi et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nROUGE compares an automatically produced text against a\nreference or a set of reference text. It measures the overlap of\nn-grams and word sequences between the generated text and\nreference text. ROUGE captures both precision and recall,\nproviding a more balanced evaluation, and can be adapted for\ndifferent summary lengths.\nEthical Considerations\nThis study complied with all relevant ethical regulations. All\nthe publicly available data sets have been deidentified and\nanonymized. With institutional review board approval\n(OHSRP#5357) by the National Institutes of Health Office of\nHuman Research Protection Programs, the IU X-RAY data set\nwas made publicly available by Indiana University, and no\ninformed consent was necessary [1]. The MIMIC-CXR data set\nwas originally approved by the institutional review board of the\nBeth Israel Deaconess Medical Center and the requirement for\nindividual patient consent was waived [20].\nResults\nQuantitative Evaluation\nTables 1 and 2 provide the quantitative results of the IU X-RAY\nand MIMIC-CXR test sets, respectively. The detailed results\nshow that the ClinicalBLIP model exhibited robust performance\nwhen compared with other methods across the IU X-RAY and\nMIMIC-CXR data sets. For the IU X-RAY data set, as shown\nin Table 1, although ClinicalBLIP was slightly inferior to the\ncompetitor methods on some individual metrics, it significantly\nsurpassed the competitor methods on most metrics. With a\nBLEU-A score of 0.296, it boasted an improvement of roughly\n6.9% over its nearest competitor, CMCA, which had a BLEU-A\nscore of 0.277. This showcases ClinicalBLIP’s enhanced\ncapability in producing reports that are more aligned with the\nreference. Moreover, when assessing the METEOR metric,\nwhich provides insights into the robustness of generation,\nClinicalBLIP achieved a score of 0.570. This was approximately\n1.7 times higher than CMCA’s 0.209, reflecting ClinicalBLIP’s\nsuperior relevance to the generated report. The ROUGE-L metric\nfurther solidified this observation; ClinicalBLIP’s score of 0.534\nwas about 33.8% higher than ORGAN’s score of 0.399,\nsuggesting that ClinicalBLIP consistently maintained a high\nlevel of linguistic quality and relevance in its results.\nFor the MIMIC-CXR data set, as shown in Table 2, there were\nareas where ClinicalBLIP did not have the highest score, but\nits comprehensive performance remains commendable. The\nBLEU-A score for ClinicalBLIP stood at 0.162, which, while\nmarginally behind ORGAN’s score of 0.184, indicates a\ncompetitive translation quality. However, ClinicalBLIP made\na strong comeback in the METEOR metric, recording a score\nof 0.365, which is approximately 1.25 times higher than\nORGAN’s score of 0.162. This underlines ClinicalBLIP’s\nproficiency in generating semantically relevant reports.\nFurthermore, with a ROUGE-L score of 0.313, ClinicalBLIP\nmanaged to surpass ORGAN by roughly 6.8%, emphasizing its\nconsistent linguistic excellence.\nIn summary, while individual metrics might have seen close\ncompetition, the overall trend clearly indicates the\ncomprehensive strength of the ClinicalBLIP model. Its\nconsistently high scores across various data sets and metrics\ndemonstrate its versatility and reliability in the realm of clinical\nreport generation.\nJMIR Form Res 2024 | vol. 8 | e32690 | p. 6https://formative.jmir.org/2024/1/e32690\n(page number not for citation purposes)\nJi et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nTable 1. The BLEUa, METEORb, and ROUGE-Lc scores of the generated reports by various methods on the IU X-RAY data set.\nIU X-RAYMethods\nROUGE-LMETEORBLEU-AdBLEU-4BLEU-3BLEU-2BLEU-1\n0.371N/Ae0.2290.1650.2190.3040.470R2GEN\n0.3810.1930.2350.1690.2220.3140.492CAf\n0.3780.1860.2280.1620.2170.3050.473CMCLg\n0.376N/A0.2360.1680.2240.3150.483PPKEDh\n0.3750.1910.2340.1700.2220.3090.475M2TRi\n0.3900.1920.2410.1730.2320.3170.486R2GENCMNj\n0.379N/A0.2370.1730.2250.3130.484ALIGNTRANSFORMER\n0.381N/A0.2480.1780.2380.3270.496KNOWMATk\n0.3840.2010.2460.1810.2350.3210.494CMM-RL\n0.3920.2090.2770.2150.2680.3490.496CMCAl\n0.3990.2050.2650.1950.2550.3460.510ORGANm\n0.5340.5700.2960.2540.2900.3430.433ClinicalBLIP\naBLEU: Bilingual Evaluation Understudy.\nbMETEOR: Metric for Evaluation of Translation With Explicit Ordering.\ncROUGE-L: Recall-Oriented Understudy for Gisting Evaluation-L.\ndBLEU-A: average of the BLEU-2/3/4 scores.\neN/A: not available.\nfCA: contrastive attention.\ngCMCL: competence-based multimodal curriculum learning.\nhPPKED: Posterior-and-Prior Knowledge Exploring-and-distilling.\niM2TR: Meshed-Memory Transformer.\njR2GENCMN: R2GEN enhanced with cross-modal memory networks.\nkKNOWMAT: Knowledge Matters.\nlCMCA: Cross-Modal Contrastive Attention Model.\nmORGAN: Observation-Guided Radiology Report Generation Framework.\nJMIR Form Res 2024 | vol. 8 | e32690 | p. 7https://formative.jmir.org/2024/1/e32690\n(page number not for citation purposes)\nJi et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nTable 2. The BLEUa, METEORb, and ROUGE-Lc scores of the generated reports by various methods on the MIMIC-CXR data set.\nMIMIC-CXRMethods\nROUGE-LMETEORBLEU-AdBLEU-4BLEU-3BLEU-2BLEU-1\n0.2700.1420.1550.1030.1450.2180.353R2GEN\n0.2830.1510.1600.1090.1520.2190.350CAe\n0.2810.1330.1510.0970.1400.2170.344CMCLf\n0.2840.1490.1600.1060.1490.2240.360PPKEDg\n0.2780.1420.1570.1060.1480.2180.353M2TRh\n0.2720.1450.1640.1070.1540.2320.378R2GENCMNi\n0.283N/Aj0.1680.1120.1560.2350.378ALIGNTRANSFORMER\n0.284N/A0.1660.1150.1560.2280.363KNOWMATk\n0.2870.1510.1650.1090.1550.2320.381CMM-RL\n0.2870.1480.1670.1170.1560.2270.360CMCAl\n0.2930.1620.1840.1230.1720.2560.407ORGANm\n0.3130.3650.1620.1150.1530.2190.332ClinicalBLIP\naBLEU: Bilingual Evaluation Understudy.\nbMETEOR: Metric for Evaluation of Translation With Explicit Ordering.\ncROUGE-L: Recall-Oriented Understudy for Gisting Evaluation-L.\ndBLEU-A: average of the BLEU-2/3/4 scores.\neCA: contrastive attention.\nfCMCL: Competence-Based Multimodal Curriculum Learning.\ngPPKED: Posterior-and-Prior Knowledge Exploring-and-distilling.\nhM2TR: Meshed-Memory Transformer.\niR2GENCMN: R2GEN enhanced with cross-modal memory networks.\njN/A: not available.\nkKNOWMAT: Knowledge Matters.\nlCMCA: Cross-Modal Contrastive Attention Model.\nmORGAN: Observation-Guided Radiology Report Generation Framework.\nAblation Study\nWe also conducted an ablation study to analyze the impact of\nfine-tuning on different modules, such as the original\nInstructBLIP (without any fine-tuning on this task), LLM, visual\nencoder, and prior information, and show the results in Table\n2. Based on the ablation study results presented in Table 3,\nseveral observations can be made regarding the performance of\ndifferent methods on the IU X-RAY data set. The ClinicalBLIP\nmethod achieved a BLEU score of 0.296, a METEOR score of\n0.570, and a ROUGE-L score of 0.534, indicating its robust\nperformance across the metrics. When the effective tuning was\nremoved, namely InstructBLIP, there was a significant drop in\nall metrics, especially in the BLEU score, which dropped to a\nmere 0.011. This highlights the importance of effective tuning\nfor the model’s performance. Similarly, removing prior\ninformation also led to a decline in performance, with the\nMETEOR metric showing a noticeable drop, to 0.339. The\nremoval of LLM tuning and visual encoder tuning resulted in\nreduced scores, but this was not as drastic as in the former cases.\nThe BLEU score dropped to 0.149 and 0.245, respectively,\nwhile the METEOR score was 0.458 and 0.513 for the same\nconditions.\nIn summary, effective fine-tuning and prior information played\na vital role in achieving optimal performance, and LLM tuning\nand visual encoder tuning were also important components for\nenhancing the model’s results. All the components together\ncontributed to the best results.\nJMIR Form Res 2024 | vol. 8 | e32690 | p. 8https://formative.jmir.org/2024/1/e32690\n(page number not for citation purposes)\nJi et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nTable 3. Experimental results of ablation study on the IU X-RAY test set.\nROUGE-Lc scoreMETEORb scoreBLEU-Aa scoreMethods\n0.5340.5700.296ClinicalBLIP with all fine-tuning\n0.0570.0960.011ClinicalBLIP without effective tuning\n0.2830.3390.091ClinicalBLIP without prior information\n0.4120.4580.149ClinicalBLIP without LLMd tuning\n0.4740.5130.245ClinicalBLIP without visual encoder tuning\naBLEU-A: the average of the BLEU-2/3/4 scores.\nbMETEOR: Metric for Evaluation of Translation With Explicit Ordering.\ncROUGE-L: Recall-Oriented Understudy for Gisting Evaluation–L.\ndLLM: large language model.\nDiscussion\nPrincipal Results\nOur proposed model, ClinicalBLIP, achieved the best METEOR\nand ROUGE-L scores and competitive BLEU scores on the test\nsets of both IU X-RAY and MIMIC-CXR. The primary\noutcomes of this study are to (1) propose a multistage\nfine-tuning strategy that separately enhances the visual encoder\nand the LLM’s understanding of medical image and text,\nallowing the LLM to harness the knowledge acquired during\nthe pretraining process and (2) incorporate the medical tags of\nmedical images and brief introductions of these images in the\nform of prompts into the model’s training and prediction\nprocesses, the large model can effectively combine the\nintroduced text-based prior knowledge with medical images to\ngenerate a more accurate report. Experimental results\ndemonstrate that ClinicalBLIP has great potential to help\nmedical experts facilitate radiology report generation and\nimprove the efficiency of decision-making for clinical diagnosis\nand treatment.\nCase Study\nIn addition to quantitative evaluations, we conducted an\nextensive set of qualitative case studies to analyze the generated\nreport. Figure 5 shows 4 cases selected from the generated\nreports on the MIMIC-CXR test set.\nBy comparing the prediction and the gold standard, it can be\nfound that case 1 and case 2 are good cases. For case 1, although\nthe prediction and the gold standard are not exactly the same,\nthere are differences in the order of symptom descriptions and\nword choices; the deep semantic meanings expressed by the\ntwo are basically consistent. However, the gold standard\nprovides more details than the prediction, which also explains\nwhy the BLEU score is not ideal in certain situations. For case\n2, both the prediction and the gold standard reports are closely\naligned and convey the same overall findings. The patient’s\nchest x-ray does not reveal any significant abnormalities. This\nis a good case as it highlights the consistency and accuracy of\nradiological interpretation.\nBesides the first 2 good cases, there are also areas that need\nimprovement and enhancement. Cases 3 and 4 in Figure 5 show\n2 bad cases. For case 3, both the prediction and the gold standard\nstate that the heart is within the normal size, and the lungs appear\nclear with no signs of pleural effusion or pneumothorax.\nHowever, the prediction mentions mild anterior wedging of a\nmidthoracic vertebral body with slight degenerative changes\nalong the midthorax. In contrast, the the gold standard report\nmentions degenerative changes in the thoracic spine but does\nnot specify the location or type of degeneration. The\ndiscrepancies in the description of the bony structures between\nthe prediction and the the gold standard report could also be of\nconcern. Different types and locations of degenerative changes\ncan have different clinical implications. For case 4, while the\nprediction and the gold standard largely align on most\nobservations, there are subtle differences in phrasing. For\ninstance, the prediction mentions the cardiomediastinal\nsilhouette is normal in size, whereas the the gold standard\nemphasizes the normal contours of the heart and mediastinum.\nSuch subtle linguistic variations can potentially lead to\nmisunderstandings in diagnosis or interpretation, especially in\ncritical medical decisions. Therefore, even though the general\nassessments align, precision in wording remains essential.\nIn summary, it is crucial to ensure that automatic or artificial\nintelligence–based predictions in radiology are meticulously\nvalidated and cross-referenced with expert opinions to ensure\npatient safety and accurate diagnosis.\nJMIR Form Res 2024 | vol. 8 | e32690 | p. 9https://formative.jmir.org/2024/1/e32690\n(page number not for citation purposes)\nJi et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nFigure 5. Cases generated by ClinicalBLIP on the MIMIC-CXR test set.\nComparison With Prior Work\nIn the medical or clinical field, there has been a surging interest\nin developing artificial intelligence applications for image\ncaptioning, that is, radiology report generation. Most studies\nhave focused on improving the quality of the generated report\nby using cross-modal memory to facilitate the generation process\n[28], reinforcing learning to align the cross-modal information\n[32], and planning and iterative refinement for long text\ngeneration [25]. However, these methods have not explored the\ncapabilities of large VLMs for this task. In this study, we\nsuccessfully applied large VLMs to the radiology report\ngeneration task by designing effective multistage fine-tuning\nstrategies and incorporating prior knowledge mechanisms. We\nvalidated our approach on multiple task data sets and achieved\nstate-of-the-art performance.\nLimitations and Future Work\nAlthough ClinicalBLIP has made significant strides and shown\npromising outcomes, there are still some unresolved issues. As\nmentioned above, ClinicalBLIP has discrepancies in\nterminological expressions in some cases compared to the the\ngold standard and sometimes lacks or misinterprets\ncomprehensive details in certain descriptions. Therefore, in\nfuture work, we will continue to optimize ClinicalBLIP,\nconsidering the integration of reasoning techniques like chain\nof thoughts into the fine-tuning process. This aims to enhance\nthe model’s semantic consistency in professional expressions\nand provide more detailed descriptions while also verifying the\nmodel’s generalization capabilities on more data sets. Moreover,\nwe will seek collaboration from professional practitioners,\nincluding both directions for model improvement and methods\nfor model evaluation.\nConclusions\nIn this study, the ClinicalBLIP model was introduced, leveraging\nlarge VLMs for radiology report generation. Tested on the IU\nX-RAY/MIMIC-CXR data sets, ClinicalBLIP significantly\noutperformed several competitor methods in METEOR and\nROUGE scores, showcasing its potential to enhance automatic\nreport generation in clinical radiology and streamline patient\ncare processes.\nJMIR Form Res 2024 | vol. 8 | e32690 | p. 10https://formative.jmir.org/2024/1/e32690\n(page number not for citation purposes)\nJi et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\nAcknowledgments\nThis work was jointly supported by grants from the Research Project of Shenzhen Institute of Information Technology (grant\nSZIIT2022KJ050), and the project was funded by the China Postdoctoral Science Foundation (grant 2023M741843), and the\nNatural Science Foundation of China (grant 62106115).\nAuthors' Contributions\nJJ and XC proposed the methods, designed and carried out the experiments, and drafted the manuscript. YH supervised the\nresearch and participated in the study design. YP critically revised the manuscript and made substantial contributions to interpreting\nthe results. YX provided guidance and reviewed the manuscript. All authors provided feedback and approved the final version\nof the manuscript.\nConflicts of Interest\nNone declared.\nReferences\n1. Demner-Fushman D, Kohli MD, Rosenman MB, Shooshan SE, Rodriguez L, Antani S, et al. Preparing a collection of\nradiology examinations for distribution and retrieval. J Am Med Inform Assoc. 2016;23(2):304-310. [FREE Full text] [doi:\n10.1093/jamia/ocv080] [Medline: 26133894]\n2. Shin HC, Roberts K, Lu L, Demner-Fushman D, Yao J, Summers RM. Learning to read chest X-rays: recurrent neural\ncascade model for automated image annotation. Presented at: IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR); June 27-30, 2016, 2016;2497-2506; Las Vegas, NV. [doi: 10.1109/cvpr.2016.274]\n3. Wang X, Peng Y, Lu L, Lu Z, Bagheri M, Summers RM. ChestX-Ray8: hospital-scale chest X-ray database and benchmarks\non weakly-supervised classification and localization of common thorax diseases. Presented at: IEEE Conference on Computer\nVision and Pattern Recognition (CVPR); July 21-26, 2017, 2017;2097-2106; Honolulu, HI. [doi: 10.1109/cvpr.2017.369]\n4. Vinyals O, Toshev A, Bengio S, Erhan D. Show and tell: a neural image caption generator. Presented at: IEEE Conference\non Computer Vision and Pattern Recognition (CVPR); June 07-12, 2015, 2015;3156-3164; Boston, MA. [doi:\n10.1109/cvpr.2015.7298935]\n5. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. Presented at: The\nThirty-first Annual Conference on Neural Information Processing Systems (NIPS); December 04-09, 2017, 2017;5998-6008;\nLong Beach, CA.\n6. Devlin J, Chang MW, Lee K, Toutanova K. Bert: pre-training of deep bidirectional transformers for language understanding.\nPresented at: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies; June 02-07, 2019, 2019;4171-4186; Minneapolis, MN. URL: https:/\n/aclanthology.org/N19-1423/ [doi: 10.18653/v1/N19-1423]\n7. Li CY, Liang X, Hu Z, Xing EP. Knowledge-driven encode, retrieve, paraphrase for medical image report generation. In:\nProc AAAI Conf Artif Intell. Presented at: The Thirty-Third AAAI Conference on Artificial Intelligence; January 27-February\n01, 2019, 2019;6666-6673; Honolulu, HI. URL: https://ojs.aaai.org/index.php/AAAI/article/view/4637 [doi:\n10.1609/aaai.v33i01.33016666]\n8. Yuan J, Liao H, Luo R, Luo J. Automatic radiology report generation based on multi-view image fusion and medical concept\nenrichment. Presented at: Medical Image Computing and Computer Assisted Intervention – MICCAI 2019; October 13–17,\n2019, 2019;721-729; Shenzhen, China. [doi: 10.1007/978-3-030-32226-7_80]\n9. Chen Z, Song Y, Chang TH, Wan X. Generating radiology reports via memory-driven transformer. Presented at: Proceedings\nof the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP); November 16–20, 2020,\n2020;1439-1449; Online. URL: https://aclanthology.org/2020.emnlp-main.112/ [doi: 10.18653/v1/2020.emnlp-main.112]\n10. Pino P, Parra D, Besa C, Lagos C. Clinically correct report generation from chest X-rays using templates. In: Machine\nLearning in Medical Imaging. Presented at: 12th International Workshop on Machine Learning in Medical Imaging 2021;\nSeptember 27, 2021, 2021;654-663; Strasbourg, France. [doi: 10.1007/978-3-030-87589-3_67]\n11. Wang X, Zhang Y, Guo Z, Li J. TMRGM: A template-based multi-attention model for x-ray imaging report generation. J\nArtif Intell Med Sci. 2021;2(1-2):21-32. [FREE Full text] [doi: 10.2991/jaims.d.210428.002]\n12. Jia C, Yang Y, Xia Y, Chen YT, Parekh Z, Pham H, et al. Scaling up visual and vision-language representation learning\nwith noisy text supervision. Presented at: Proceedings of the 38th International Conference on Machine Learning; July\n18-24, 2021, 2021;4904-4916; Virtual.\n13. Radford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal S, et al. Learning transferable visual models from natural\nlanguage supervision. Presented at: Proceedings of the 38th International Conference on Machine Learning; July 18-24,\n2021, 2021;8748-8763; Virtual.\n14. Li Y, Hu B, Chen X, Ma L, Xu Y, Zhang M. LMEye: An interactive perception network for large language models. ArXiv.\nPreprint posted online on September 28, 2023. 2023 [FREE Full text] [doi: 10.48550/arXiv.2305.03701]\nJMIR Form Res 2024 | vol. 8 | e32690 | p. 11https://formative.jmir.org/2024/1/e32690\n(page number not for citation purposes)\nJi et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\n15. Li Y, Wang L, Hu B, Chen X, Zhong W, Lyu C, et al. A comprehensive evaluation of GPT-4V on knowledge-intensive\nvisual question answering. ArXiv. Preprint posted online on November 13, 2023. 2023 [FREE Full text]\n16. Li Y, Hu B, Xinyu C, Ding Y, Ma L, Zhang M. A multi-modal context reasoning approach for conditional inference on\njoint textual and visual clues. Presented at: Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics; July 09-14, 2023, 2023;10757-10770; Toronto, ON. [doi: 10.18653/v1/2023.acl-long.601]\n17. Li Y, Hu B, Ding Y, Ma L, Zhang M. A neural divide-and-conquer reasoning framework for image retrieval from linguistically\ncomplex text. Presented at: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics; July\n09-14, 2023, 2023;16464-16476; Toronto, ON. URL: https://aclanthology.org/2023.acl-long.909/ [doi:\n10.18653/v1/2023.acl-long.909]\n18. Hu EJ, Shen Y, Wallis P, Allen-Zhu Z, Li S, Wang S, et al. LoRA: Low-rank adaptation of large language models. Presented\nat: The Tenth International Conference on Learning Representations; April 25-29, 2022, 2022; Virtual. URL: https:/\n/openreview.net/forum?id=nZeVKeeFYf9\n19. Ba JL, Kiros JR, Hinton GE. Layer normalization. ArXiv. Preprint posted online on July 21, 2026. 2016 [FREE Full text]\n20. Johnson AEW, Pollard TJ, Greenbaum NR, Lungren MP, Deng C, Peng Y, et al. MIMIC-CXR-JPG, a large publicly\navailable database of labeled chest radiographs. ArXiv. Preprint posted online on November 14, 2019. 2019 [FREE Full\ntext]\n21. Li J, Li D, Savarese S, Hoi S. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large\nlanguage models. Presented at: Proceedings of the 40th International Conference on Machine Learning; July 23-29, 2023,\n2023;19730-19742; Honolulu, HI. URL: https://dl.acm.org/doi/10.5555/3618408.3619222[doi: 10.5555/3618408.3619222]\n22. Li J, Li D, Xiong C, Hoi S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding\nand generation. Presented at: Proceedings of the 39th International Conference on Machine Learning; July 17-23, 2022,\n2022;12888-12900; Baltimore, MA.\n23. Dai W, Li J, Li D, Tiong AMH, Zhao J, Wang W, et al. InstructBLIP: towards general-purpose vision-language models\nwith instruction tuning. ArXiv. Preprint posted online on June 15, 2023. 2023 [FREE Full text]\n24. Smit A, Jain S, Rajpurkar P, Pareek A, Ng A, Lungren M. Combining automatic labelers and expert annotations for accurate\nradiology report labeling using BERT. Presented at: Proceedings of the 2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP); November 16-20, 2020, 2020;1500-1519; Virtual. [doi: 10.18653/v1/2020.emnlp-main.117]\n25. Chung HW, Hou L, Longpre S, Zoph B, Tay Y, Fedus W, et al. Scaling instruction-finetuned language models. ArXiv.\nPreprint posted online on December 6, 2022. 2022 [FREE Full text]\n26. Liu F, Yin C, Wu X, Ge S, Zhang P, Sun X. Contrastive attention for automatic chest x-ray report generation. Presented\nat: Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021; August 1-6, 2021, 2021;269-280;\nVirtual. [doi: 10.18653/v1/2021.findings-acl.23]\n27. Liu F, Ge S, Wu X. Competence-based multimodal curriculum learning for medical report generation. Presented at:\nProceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing; August 1-6, 2021, 2021;3001-3012; Virtual. [doi:\n10.18653/v1/2021.acl-long.234]\n28. Liu F, Wu X, Ge S, Fan W, Zou Y. Exploring and distilling posterior and prior knowledge for radiology report generation.\nPresented at: 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR); June 20-25, 2021,\n2021;13753-13762; Nashville, TN. [doi: 10.1109/cvpr46437.2021.01354]\n29. Chen Z, Shen Y, Song Y, Wan X. Cross-modal memory networks for radiology report generation. Presented at: Proceedings\nof the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference\non Natural Language Processing; August 1-6, 2021, 2021;5904-5914; Virtual. [doi: 10.18653/v1/2021.acl-long.459]\n30. You D, Liu F, Ge S, Xie X, Zhang J, Wu X. Aligntransformer: hierarchical alignment of visual regions and disease tags\nfor medical report generation. Presented at: Medical Image Computing and Computer Assisted Intervention–MICCAI 2021;\nSeptember 27-October 1, 2021, 2021;72-82; Strasbourg, France. [doi: 10.1007/978-3-030-87199-4_7]\n31. Yang S, Wu X, Ge S, Zhou SK, Xiao L. Knowledge matters: chest radiology report generation with general and specific\nknowledge. Med Image Anal. 2022;80:102510. [FREE Full text] [doi: 10.1016/j.media.2022.102510] [Medline: 35716558]\n32. Nooralahzadeh F, Gonzalez NP, Frauenfelder T, Fujimoto K, Krauthammer M. Progressive transformer-based generation\nof radiology reports. Presented at: Findings of the Association for Computational Linguistics: EMNLP 2021; November\n16-20, 2021, 2021;2824-2832; Punta Cana, Dominican Republic. [doi: 10.18653/v1/2021.findings-emnlp.241]\n33. Qin H, Song Y. Reinforced cross-modal alignment for radiology report generation. Presented at: Findings of the Association\nfor Computational Linguistics: ACL 2022; May 22-27, 2022, 2022;448-458; Dublin, Ireland. [doi:\n10.18653/v1/2022.findings-acl.38]\n34. Song X, Zhang X, Ji J, Liu Y, Wei P. Cross-modal contrastive attention model for medical report generation. Presented at:\nProceedings of the 29th International Conference on Computational Linguistics; May 22-27, 2022, 2022;2388-2397;\nGyeongju, Republic of Korea.\n35. Hou W, Xu K, Cheng Y, Li W, Liu J. ORGAN: observation-guided radiology report generation via tree reasoning. Presented\nat: Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics; July 09-14, 2023,\n2023;8108-8122; Toronto, ON. URL: https://aclanthology.org/2023.acl-long.451/ [doi: 10.18653/v1/2023.acl-long.451]\nJMIR Form Res 2024 | vol. 8 | e32690 | p. 12https://formative.jmir.org/2024/1/e32690\n(page number not for citation purposes)\nJi et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX\n36. Papineni K, Roukos S, Ward T, Zhu WJ. Bleu: a method for automatic evaluation of machine translation. Presented at:\nProceedings of the 40th Annual Meeting of the Association for Computational Linguistics; July 6-12, 2002, 2002;311-318;\nPhiladelphia, PA. [doi: 10.3115/1073083.1073135]\n37. Banerjee S, Lavie A. METEOR: An automatic metric for MT evaluation with improved correlation with human judgments.\nPresented at: Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation\nand/or Summarization; June 29, 2005, 2005;65-72; Ann Arbor, MI. URL: https://aclanthology.org/W05-0909/ [doi:\n10.3115/1626355.1626389]\n38. Lin CY. ROUGE: a package for automatic evaluation of summaries. Presented at: Text Summarization Branches Out; July,\n2004, 2004;74-81; Barcelona, Spain. URL: https://aclanthology.org/W04-1013/\nAbbreviations\nBERT: bidirectional encoding representation of transformer\nBLEU: Bilingual Evaluation Understudy\nCMCA: Cross-Modal Contrastive Attention\nCMM-RL: Reinforcement Learning Over a Cross-Modal Memory\nCNN: convolutional neural network\nLLM: large language model\nLoRA: low-rank adaptation\nMETEOR: Metric for Evaluation of Translation With Explicit Ordering\nMLP: multilayer perceptron\nORGAN: Observation-Guided Radiology Report Generation\nQ-Former: query transformer\nRNN: recurrent neural network\nROUGE: Recall-Oriented Understudy for Gisting Evaluation\nVLM: vision-language model\nEdited by A Mavragani; submitted 17.10.23; peer-reviewed by N Hong, J Zhang, Y Liu; comments to author 10.11.23; revised version\nreceived 12.12.23; accepted 10.01.24; published 08.02.24\nPlease cite as:\nJi J, Hou Y, Chen X, Pan Y, Xiang Y\nVision-Language Model for Generating Textual Descriptions From Clinical Images: Model Development and Validation Study\nJMIR Form Res 2024;8:e32690\nURL: https://formative.jmir.org/2024/1/e32690\ndoi: 10.2196/32690\nPMID: 38329788\n©Jia Ji, Yongshuai Hou, Xinyu Chen, Youcheng Pan, Yang Xiang. Originally published in JMIR Formative Research\n(https://formative.jmir.org), 08.02.2024. This is an open-access article distributed under the terms of the Creative Commons\nAttribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work, first published in JMIR Formative Research, is properly cited. The complete\nbibliographic information, a link to the original publication on https://formative.jmir.org, as well as this copyright and license\ninformation must be included.\nJMIR Form Res 2024 | vol. 8 | e32690 | p. 13https://formative.jmir.org/2024/1/e32690\n(page number not for citation purposes)\nJi et alJMIR FORMATIVE RESEARCH\nXSL•FO\nRenderX",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7450867891311646
    },
    {
      "name": "Natural language processing",
      "score": 0.5995402932167053
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5440617203712463
    },
    {
      "name": "Standardization",
      "score": 0.509940505027771
    },
    {
      "name": "Deep learning",
      "score": 0.4909572899341583
    },
    {
      "name": "Metric (unit)",
      "score": 0.4596400558948517
    },
    {
      "name": "Closed captioning",
      "score": 0.45654913783073425
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.4335392415523529
    },
    {
      "name": "Information retrieval",
      "score": 0.3765830397605896
    },
    {
      "name": "Machine learning",
      "score": 0.327489972114563
    },
    {
      "name": "Image (mathematics)",
      "score": 0.18289437890052795
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I158809036",
      "name": "Shenzhen Institute of Information Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    }
  ]
}