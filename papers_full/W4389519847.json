{
  "title": "Retrieval-Augmented Few-shot Text Classification",
  "url": "https://openalex.org/W4389519847",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2149614316",
      "name": "Guoxin Yu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128793998",
      "name": "Lemao Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2161667016",
      "name": "Hai-Yun Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105557964",
      "name": "Shuming Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103824910",
      "name": "Xiang Ao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W1988790447",
    "https://openalex.org/W3213415226",
    "https://openalex.org/W3176489198",
    "https://openalex.org/W4281956169",
    "https://openalex.org/W2963829526",
    "https://openalex.org/W3176038554",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2028175314",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3206455169",
    "https://openalex.org/W3096580779",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W3104713013",
    "https://openalex.org/W2049633694",
    "https://openalex.org/W4281477951",
    "https://openalex.org/W3174784402",
    "https://openalex.org/W4285122897",
    "https://openalex.org/W4287122359",
    "https://openalex.org/W2601450892",
    "https://openalex.org/W3099655892",
    "https://openalex.org/W4385571693",
    "https://openalex.org/W3012625278",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W4384652670",
    "https://openalex.org/W3126880001",
    "https://openalex.org/W2949380545",
    "https://openalex.org/W3176913643",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W3196692796",
    "https://openalex.org/W3104499181",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4226069413",
    "https://openalex.org/W2141833813",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W4385570330",
    "https://openalex.org/W3100132436",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3202729335",
    "https://openalex.org/W2251648804",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W4301243929"
  ],
  "abstract": "Retrieval-augmented methods are successful in the standard scenario where the retrieval space is sufficient; whereas in the few-shot scenario with limited retrieval space, this paper shows it is non-trivial to put them into practice. First, it is impossible to retrieve semantically similar examples by using an off-the-shelf metric and it is crucial to learn a task-specific retrieval metric; Second, our preliminary experiments demonstrate that it is difficult to optimize a plausible metric by minimizing the standard cross-entropy loss. The in-depth analyses quantitatively show minimizing cross-entropy loss suffers from the weak supervision signals and the severe gradient vanishing issue during the optimization. To address these issues, we introduce two novel training objectives, namely EM-L and R-L, which provide more task-specific guidance to the retrieval metric by the EM algorithm and a ranking-based loss, respectively. Extensive experiments on 10 datasets prove the superiority of the proposed retrieval augmented methods on the performance.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 6721–6735\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nRetrieval-Augmented Few-shot Text Classification\nGuoxin Yu\n ∗, Lemao Liu\n †, Haiyun Jiang\n , Shuming Shi\n , Xiang Ao\n †\nKey Lab of Intelligent Information Processing of Chinese Academy of Sciences (CAS),\nInstitute of Computing Technology, CAS, Beijing 100190, China.\nPeng Cheng Laboratory.\nUniversity of Chinese Academy of Sciences, Beijing 100049, China.\nTencent AI Lab, China.\n Institute of Intelligent Computing Technology, Suzhou, CAS.\n{yuguoxin20g,aoxiang}@ict.ac.cn\n{redmondliu, haiyunjiang, shumingshi}@tencent.com\nAbstract\nRetrieval-augmented methods are successful in\nthe standard scenario where the retrieval space\nis sufficient; whereas in the few-shot scenario\nwith limited retrieval space, this paper shows\nit is non-trivial to put them into practice. First,\nit is impossible to retrieve semantically simi-\nlar examples by using an off-the-shelf metric\nand it is crucial to learn a task-specific retrieval\nmetric; Second, our preliminary experiments\ndemonstrate that it is difficult to optimize a\nplausible metric by minimizing the standard\ncross-entropy loss. The in-depth analyses quan-\ntitatively show minimizing cross-entropy loss\nsuffers from the weak supervision signals and\nthe severe gradient vanishing issue during the\noptimization. To address these issues, we intro-\nduce two novel training objectives, namely EM-\nL and R-L, which provide more task-specific\nguidance to the retrieval metric by the EM al-\ngorithm and a ranking-based loss, respectively.\nExtensive experiments on 10 datasets prove the\nsuperiority of the proposed retrieval augmented\nmethods on the performance.\n1 Introduction\nFew-shot text classification, which entails learn-\ning a new task based on limited training data,\nhas been advanced by pre-trained language mod-\nels (PLMs) (Brown et al., 2020; Liu et al., 2023)\nand prompt engineering (Gao et al., 2021; Chen\net al., 2022a). However, since training numerous\nparameters of PLMs on scarce data is prone to pro-\nduce over-fitting (Liu et al., 2021) and unstable\ngeneralization, only using the trained parameters\nfor inference usually leads to unsatisfactory perfor-\nmance on unseen test data.\n*Work done while this author was an intern at Tencent.\n†Corresponding authors.\nOn the other hand, retrieval-based methods have\nwitnessed success on various natural language pro-\ncessing tasks, thanks to their capability of incorpo-\nrating retrieved memoryalongside parameters for\nbetter generalization. These methods retrieve rele-\nvant examples as memories from a large-scale cor-\npus through either a static retrieval metric (Lewis\net al., 2020; Wang et al., 2022) or a joint learning-\nbased metric (Cai et al., 2021; Siriwardhana et al.,\n2023) and then the retrieved examples are used\nto make a prediction. In this way, their general-\nization ability is achieved by not only the model\nparameters but also the retrieved memory.\nDespite the theoretical potential of promoting\ngeneralization by using retrieved memory, previous\nretrieval-augmented methods empirically struggle\nto showcase compelling ability in few-shot learning\nscenarios, where the retrieval space (i.e., the few-\nshot training data) is limited. Specifically, static re-\ntrieval may lack neighbors with high metrics in the\ncase of limited retrieval space. Even though such\nneighbors exist, static retrieval cannot be reliable\nfor retrieving really helpful samples for target pre-\ndiction, because its metric is not task-specific. In\nparticular, for joint learning-based retrieval which\nminimizes the standard cross-entropy based loss,\nalthough the retrieval metric is updated towards\nthe downstream task, it suffers from the gradient\nvanishing problem during the optimization process\nas quantitatively measured in Fig. 2 (see §5.2 later).\nAs a result, in a few-shot scenario, the retrieval met-\nric might be not optimized well due to insufficient\ntraining data.\nTo overcome the aforementioned challenges,\nwe propose two novel training objectives, namely\nExpectation Maximization-based Loss (EM-L) and\nRanking-based Loss (R-L), for learning to retrieve\n6721\nexamples from a limited space more effectively.\nBoth objectives are committed to obviating the gra-\ndient vanishing problem and prioritizing more ben-\neficial examples for specific downstream tasks. In\nthe EM-L approach, the retrieved examples are\ntreated as latent variables, and an iterative process\nof Expectation-step and Maximization-step is em-\nployed until convergence (Dempster et al., 1977).\nThe posterior distribution of the latent variable is es-\ntimated to measure the importance of candidate ex-\namples in the E-step, while the M-step maximizes\nthe expectation log-likelihood. By approximating\nthe retrieval metric according to the posterior prob-\nability, more productive examples could be recalled\nfor downstream tasks with limited training data.\nFollowing a similar idea, R-L optimizes an addi-\ntional ranking loss function to provide more direct\nsupervision to the examples retriever, which draws\ninspiration from pair-wise ranking algorithm (Fre-\nund and Schapire, 1997; Burges et al., 2005; Rudin\nand Schapire, 2009). Such a tailored loss measures\nthe consistency between the retrieval metric and\nthe auxiliary function associated with each exam-\nple for classification purposes. Minimizing the loss\ncould effectively strengthen the supervision signals\nfor the example retriever.\nOur experimental evaluation on ten text classifi-\ncation datasets demonstrates the superiority of EM-\nL and R-L over existing retrieval methods within a\nlimited retrieval space. The comparative analyses\nfurther confirm that EM-L and R-L alleviate the\nweak supervisory signals and gradient vanishing\nissue suffered by joint learning-based retrieval. Our\ncontributions could be summarized as follows:\n• We discuss the weak supervision signals and gra-\ndient vanishing problem encountered by existing\nretrieval methods minimizing the standard cross-\nentropy loss, as quantitatively measured in §5.2.\n• We introduce two novel training objectives,\nnamely EM-L and R-L, which optimize the re-\ntriever more effectively, thus recalling more pro-\nductive examples from a limited space.\n• Extensive experiments and analyses demonstrate\nthat the proposed methods achieve better perfor-\nmance on few-shot text classification and alle-\nviate the supervision insufficiency and gradient\nvanishing issues.\n2 Revisiting Retrieval-augmented\nMethods in Few-shot Learning\n2.1 Retrieval-augmented Methods\nIn this paper, we revisit the retrieval-augmented\nmethods in few-shot text classification and formu-\nlate the task in a general framework. Our primary\nobjective is to retrieve examples from limited train-\ning data to improve the few-shot text classification.\nModel Formulation All retrieval methods could\ncomprise an example retriever and a text classi-\nfier. We provide the formal formulation inspired by\nSingh et al. (2021) and Izacard et al. (2022):\nPθ,ϕ(y|x) =\nm∑\nj=1\nPθ(y|x,zj)Pϕ(zj|x),\nPθ(y|x,zj) =softmax(fclf(x ⊕zj)),\nPϕ(zj|x) =fretr(x,zj),\n(1)\nwhere x and zj denote the representations of orig-\ninal input and a retrieved example from the train-\ning set, and ycorresponds to the class associated\nwith input x. fclf and fretr serve as the text clas-\nsifier and the example retriever, which selects ex-\namples according to a retrieval metric. θ and ϕ\ndenote the trainable parameters of the text classi-\nfier and examples retriever. mis a hyperparameter\nthat denotes the number of fetched examples. The\noperation ⊕signifies concatenation, and the term\nsoftmax refers to the normalized exponential func-\ntion. Specifically, z corresponds to a set of retrieval\nexamples, which can either be {⟨xs,ys⟩}pairs or\n{xs}. The latter form is adopted in this paper for\nsimple experiments.\nThe standard cross entropy is employed to opti-\nmize the classifier and example retriever as follows:\nL= −\nn∑\ni=1\nlog Pθ,ϕ(yi|xi), (2)\nwhere nis the total number of training instances\nand yi is the gold label of the i-th instance. During\ninference, for all retrieval methods, we select topm\nexamples according to Pϕ(zj|x) and get the final\nclassification results using the first line of Eq. (1).\nStatic Retrieval Given an input sentence x and\na retrieval corpus, static retrieval aims to search for\na set of relevant examples Z according to a fixed\nretrieval metric (Borgeaud et al., 2022; Wang et al.,\n2022; Li et al., 2022). Following the Eq. (1), its\n6722\nretrieval metric is defined as follows:\nPϕ(zj|x) =fretr(x,zj) =sim(x,zj). (3)\nHere, sim(x,zj) represents a fixed metric with-\nout any trainable parameters, such as TF-\nIDF (Sparck Jones, 1972), BM25 (Robertson et al.,\n2009), and semantic similarity encoded by PLMs.\nSuch fixed metrics cannot adapt to the downstream\ntask and prioritize the most helpful examples. Par-\nticularly, this limitation will be amplified in few-\nshot learning with scarce training data.\nJoint Learning based Retrieval Static retrieval\nassumes that higher similarity betweenzj and x im-\nplies a greater auxiliary effect of zj on x. However,\nthe assumption failed to hold in tasks where inputs\nwith high similarity have distinct labels, such as\nsentiment classification. To address this limitation,\njoint learning-based retrieval (Cai et al., 2021; Gao\net al., 2022; Siriwardhana et al., 2023) unifies the\nretriever and the downstream model to jointly train\nthem for specific tasks. Following Eq. (1),\nPϕ(zj|x) =fretr(x,zj) =\nexp(x ·z⊤\nj )\n∑m\nj=1 exp(x ·z⊤\nj ).\n(4)\nfretr(x,zj) is a trainable dot product attention. No-\ntably, the absence of ground truth for Pϕ(zj|x)\nmakes it challenging to determine which zj is the\nmost beneficial one, and it relies implicitly on dis-\ntant supervision from text classification.\nBoth static retrieval and joint learning-based re-\ntrieval are proposed to retrieve examples from a\nlarge-scale corpus. In this paper, we mainly focus\non few-shot text classification and retrieve the most\nhelpful examples from the limited training set.\n2.2 Challenges in Few-shot Learning\nWhile the above retrieval-augmented methods have\nshown advancements in various natural language\nprocessing tasks, their performance in few-shot\nlearning remains unconvincing. In other words, re-\ntrieving examples from a narrow space to improve\nfew-shot learning is still challenging due to lim-\nited training data. Previous studies (Li et al., 2022;\nSiriwardhana et al., 2023) have revealed that static\nretrieval may not fetch the most helpful examples in\ntasks where similar inputs correspond to different\nlabels, primarily due to their unreasonable assump-\ntion that higher similarity implies better suitability\nfor the downstream task. Moreover, we also find\nstatic retrieval even underperforms methods with-\nout retrieval in some few-shot tasks (see Table 1).\nSuch failure can also be attributed to data limitation\nin few-shot scenarios, where examples with high\nstatic similarities are scarce or non-existent.\nIn addition, joint learning-based retrieval meth-\nods (Ren et al., 2021; Cai et al., 2021; Siriward-\nhana et al., 2023) are good solutions to enhance the\nadaptability of the retrieval to downstream tasks.\nHowever, our study demonstrates that learnable\nmetrics struggle to be trained as anticipated and\nare inferior to static metrics in several few-shot\ntasks (see Table 1). The main underlying factors\nare the scarcity of data and the weak supervision\nsignals provided to the learnable retrieval metric. In\nmore detail, the retrieval metrics in joint learning-\nbased methods are adjusted solely based on distant\nsupervision from the downstream tasks, which is\nsignificantly further weakened by the limited data.\nThis fact is further supported by quantifying the\ngradient of retrieval parameters: the gradient norm\nof the parameters in retrieval metric is more than\n1e−6 for only about 40% updates in some datasets\nas shown in Figure 2 (see §5.2 later).\nIn this paper, our objective is to meet the chal-\nlenges of weak supervision signals for the retriever\nand insufficient data, aiming to retrieve the most\nhelpful examples to promote model generalization.\n3 Methodology\n3.1 Overview\nGiven the limitations posed by limited data and\nweak supervision signals, existing retrieval meth-\nods are inadequate for addressing these challenges.\nTo address these limitations, we propose two novel\ntraining objectives, which are achieved by two\nloss functions: Expectation Maximization-based\nLoss (EM-L) and Ranking-based Loss (R-L). Both\nmethods aim to enhance the retrieval quality by giv-\ning the retriever more supervisory signals and pri-\noritizing examples that are more beneficial for the\nspecific task with limited training data. In essence,\nwe seek to maximize the consistency between the\nmetric distribution P(zj|x) and the classification\ndistribution P(y|x,zj)[yi] as much as possible. In\nthis way, more suitable examples are retrieved and\nthe performance of text classification could be im-\nproved even in the few-shot scenario. Additionally,\nwe integrated EM-L, R-L, and two existing retrieval\nmethods with two popular text classification back-\nbones to compare their respective performance.\n6723\n3.2 Backbone\nFine-tune Pre-trained Language Models For\neach sentence, we use PLMs to tokenize the in-\nput sentence into {[CLS],x1,...,x l,[SEP]}with\n(l+ 2)tokens and extract the representation x of\n[CLS] as the sentence embedding. In the same\nway, the j-th retrieved example is represented as\nzj. These tensors are subsequently fed into the\nexample retriever and classifier, producing the final\nprobability estimated for label y.\nPrompt Learning Another backbone is to trans-\nform the text classification into a cloze question\nproblem (Schick and Schütze, 2021). Let Mbe\na masked language model with vocabulary V, and\nYdenote the label set of a specific downstream\ntask A. Prompt learning employs a function Pto\nconvert an input sentence into a phrase containing\na prompt with a [MASK] token. Then an injective\nfunction v: L→V is utilized to map each label to\na word from M’s vocabularyV. We first obtain the\nrepresentation of [MASK] and determine the most\nsuitable word from Vfor filling the [MASK]. For\ninstance, the application of prompt learning to sen-\ntiment classification can be outlined as follows:\nP(x) ={[CLS],x1,...,x l,it was [MASK],[SEP]}\nP(y|x) =g(P([MASK] = v(y)|x)),\nv(y) ∈{great,terrible},\n(5)\nwhere x is the representation of [MASK], g con-\nverts the probability of label words to classes, and\nl is sentence length. The representation zj of a\nretrieved example is yielded from a [MASK] token\nin the same way.\n3.3 Expectation Maximization-based\nLoss (EM-L)\nConsidering the absence of the ground truth for\nPϕ(zj|x) in Eq. (1), we regardz as a latent variable\nand propose an EM-based retrieval objective to\nestimate Pϕ(zj|x). This method alternates between\nan Expectation-step and a Maximization-step until\nconvergence. In the E-step, the current parameters\nare used to estimate the posterior distribution of the\nlatent variable given the observed data. Specifically,\nwe retrieve mexamples from the training set and\ncompute the conditional probabilities of the latent\nvariable using:\nPθ,ϕ(zj|x,y) = Pθ(y|x,zj)Pϕ(zj|x)∑m\nj=1 Pθ(y|x,zj)Pϕ(zj|x),\n(6)\nwhere Pθ(y|x,zj) and Pϕ(zj|x) are obtained from\nclassifier fclf and examples retriever fretr in Eq. (1)\nrespectively. mdenotes the number of retrieved\nexamples.\nIn the M-step, the parameters are updated by\nmaximizing the expected log-likelihood, which\nis taken with respect to the estimated posterior\nPθ,ϕ(zj|x,y) in the E-step:\nPθ,ϕ(y|x) =\nm∑\nj=1\nPθ,ϕ(zj|x,y) ·log Pθ(y|x,zj).\n(7)\nSince we sample mexamples from the training set\nby Pϕ(zj|x) and estimate Pθ,ϕ(zj|x,y) based on\nmexamples in the E-step, more supervision will be\nprovided to the retriever during the optimization in\nthe M-step. Please refer to Appendix A for proof of\nrationality of Eq.(6) and why EM-L can minimize\nthe likelihood-based loss defined in Eq. (2).\n3.4 Ranking-based Loss (R-L)\nFollowing the main idea claimed in § 3.1, Ranking-\nbased Loss (R-L) considers the process of retriev-\ning zj as a ranking task. Unlike EM-L, R-L em-\nploys a ranking loss to enhance the consistency\nbetween Pθ(y|x,zj)[yi] and Pϕ(zj|x) and provide\nmore direct signals to the retriever. The optimiza-\ntion objective of R-L aims to ensure that zj with\nhigher Pθ(y|x,zj)[yi] has higher Pϕ(zj|x) by min-\nimizing the following LR:\nLR =\nn∑\ni\nm∑\nj\nmax(Pθ(y|xi,zj)[yi]\n−Pϕ(zj|xi) +δ,0).\n(8)\nHere, Pθ(y|x,zj) and Pϕ(zj|x) are obtained from\nfclf and fretr in Eq. (1), mand ndenote the number\nof retrieved examples and training instances. δis a\nmargin parameter imposing the distance between\ntwo distributions to be larger than δ.\nThe ranking loss LR is added to the overall loss\nLin Eq. (2) with a weight λevery tstep:\nLsum = L+ λ·LR,\nλ=\n{\n1, step mod t= 0;\n0, otherwise;\n(9)\nwhere λ> 0 is a hyperparameter to trade off both\nloss terms, and step denotes the training steps.\n6724\nModel Single Sentence Sentence Pair ABSA Avg.\nSST-2 MR CR TREC QQP QNLI MNLI SNLI RES LAP\nPrompt Learning with RoBerta-Large\nVanilla84.84(6.80) 77.88(7.90) 88.36(2.89) 87.20(7.70) 67.09(6.70) 64.25(7.45) 60.69(4.08) 64.56(4.08) 72.05(4.08) 71.81(2.88) 73.87\nStatic 88.60(4.10) 83.67(6.80) 87.06(3.84) 90.95(1.36) 68.31(7.70) 66.27(4.98) 60.38(6.70) 68.17(5.62) 70.95(5.46) 73.01(3.03) 75.74\nJoint 90.71(1.20) 85.83(2.40) 86.76(6.50) 90.57(4.17) 67.26(4.40) 63.15(7.16) 61.95(4.65) 67.64(5.80) 71.07(2.97) 73.32(2.26) 75.83\nEM-L91.31(1.30) 87.58(1.40) 90.00(0.90) 92.13(1.41) 74.41(0.74) 67.66(3.77) 64.85(3.21) 69.52(3.69) 73.74(3.46) 76.02(1.90) 78.72\nR-L 91.58(1.30) 87.47(0.09) 89.93(1.70) 92.86(1.21) 73.79(2.28) 67.62(5.79) 66.04(3.18) 73.08(4.59) 76.79(2.60) 75.59(1.51) 79.46\nFine-tune RoBerta-Large\nVanilla81.59(4.50) 73.59(9.90) 81.63(4.08) 85.95(5.57) 61.42(8.19) 57.20(2.09) 59.90(5.72) 59.19(5.58) 69.21(4.14) 71.06(5.11) 70.07\nStatic 81.99(10.8) 72.69(5.05) 82.75(5.50) 87.02(3.25) 60.23(9.60) 57.11(3.90) 54.69(4.78) 62.65(5.10) 70.48(8.74) 71.37(3.03) 70.10\nJoint 83.49(3.20) 74.89(2.90) 80.63(5.42) 86.33(3.17) 63.50(8.08) 57.66(2.69) 60.99(4.98) 61.01(5.80) 70.23(3.57) 70.62(4.47) 70.94\nEM-L85.38(1.30) 75.80(2.20) 83.81(5.36) 89.36(2.64) 65.70(8.17) 60.93(1.56) 62.24(3.12) 65.25(3.20) 71.64(3.36) 72.69(3.18) 73.27\nR-L 84.69(2.29) 75.35(2.20) 83.17(3.22) 88.92(3.81) 70.53(2.68) 61.37(0.12) 62.18(1.72) 66.31(3.30) 73.28(3.13) 72.69(3.01) 73.85\nTable 1: Comparison results on 16-shot text classification. “Vanilla” denotes methods without retrieval, which only\nconsists of a sentence encoder and a classifier. “Static” and “Joint” are static retrieval and joint learning-based\nretrieval, which are introduced in §2. “EM-L” and “R-L” are methods implemented with our proposed new\nobjectives. All the reported results are average Accuracy and the standard deviation in the subscript.\n4 Experimental Results\n4.1 Experimental Settings\nDatasets We compared the proposed EM-L and\nR-L approaches with existing retrieval methods by\nconducting experiments on 10 widely used text\nclassification datasets, including single-sentence\nclassification, sentence pair classification, and\naspect-based sentiment classification. We created\nfew-shot datasets following Gao et al. (2021). For\nmore details, please refer to Appendix B.\nBaselines To prove the effectiveness of retrieving\nexamples from the training set, we develop a base-\nline method without retrieval for comparison. It\ncomprises an input encoder described in § 3.2 and a\nfeed-forward neural network for classification. For\ncomparing different retrieval methods, we evalu-\nated our EM-L and R-L against static retrieval and\njoint learning-based retrieval. We combine them\nwith two widely used backbones for text classifica-\ntion: pre-trained language models fine-tuning and\nprompt learning. Please refer to Appendix C for\nmore implementations, such as hyper-parameters\nand templates in prompt learning.\nEvaluation. We evaluate all the retrieval meth-\nods using two metrics: Accuracy and Kendall’s\nτ. Accuracy represents the proportion of correctly\nclassified instances out of the total number of in-\nstances. Kendall’s τ is employed to measure the\nconsistency and correlations between the retrieval\nmetric Pϕ(z|xi) and its auxiliary Pϕ(y|xi,z)[yi]\nfor classification. Kendall’s τ is defined as follows:\nτi =\n2\nm(m−1)\nm∑\nj<k\nsign(uj −uk) ·sign(vj −vk),\nu∼Pϕ(z|xi),v ∼Pϕ(y|xi,z)[yi],τi ∈[−1,1],\n(10)\nwhere sign(·) ∈{−1,0,1}is a sign function. A\nranking pair ⟨j,k⟩is concordant if their ranks have\nthe same order in Pϕ(z|xi) and Pϕ(y|xi,z)[yi].\nConsequently, a positive τi indicates a positive cor-\nrelation between two distributions, and vice versa.\nFor ninstances xi in the training set, we calculate\nthe proportion of xi with τi >0 as follows:\nτ′=\n∑n\ni step(τi)\nn ,\nstep(τi) =\n{0, τi ≤0\n1, τi >0 .\n(11)\nThe reported Kendall’s τ′in the following experi-\nment is actually τ′, which represents the proportion\nof instances with τi >0.\n4.2 Main Results\nThe experimental results for 16-shot setting on 10\ndatasets are reported in Table 1, where different\nretrieval-based methods are combined with two\nbackbones. Several insightful observations could\nbe drawn from the results.\nRetrieving examples from the training set is effec-\ntive in few-shot scenarios.Firstly, in most datasets,\nretrieval-augmented models outperform the vanilla\n6725\nKendall’sτ′ SST-2 CR QQP QNLI RES\nStatic 0.5344 0.5837 0.4307 0.5312 0.47857\nJoint 0.5413 0.6129 0.4776 0.5937 0.4732\nEM-L 0.6853 0.6451 0.6265 0.7500 0.6598\nR-L 0.7442 0.65620.6057 0.7185 0.6125\nTable 2: Kendall’s τ′ of Pϕ(zj|xi) and Pθ(y|xi,zj)[yi].\nmodel with two backbones, indicating that retriev-\ning examples from the training set could enhance\nthe generalization, even with a narrow search scope.\nSecondly, the joint learning-based retrieval, EM-\nL, and R-L perform better than the static retrieval,\nwhich is even less effective than the vanilla model.\nWe hold that this is because static retrieval fetches\nsome examples with high semantic similarities but\nis detrimental to the downstream tasks. In contrast,\nthe learnable retrieval methods, i.e. joint learning-\nbased retrieval, EM-L, and R-L, are more likely to\nalign with the goals of specific tasks.\nEM-L and R-L approaches train the retriever\nmore effectively than static retrieval and joint\nlearning-based retrieval. At first, our proposed\nEM-L and R-L achieve significantly higher accu-\nracy across different backbones, proving their effec-\ntiveness in fetching helpful examples and adapting\nto specific downstream tasks. Furthermore, on av-\nerage, R-L outperforms EM-L, potentially due to\nits utilization of a more direct ranking loss that\nprovides more significant signals and flexible guid-\nance to the example retriever. Finally, it is worth\nnoting that EM-L and R-L show smaller standard\ndeviations on most datasets than other methods,\nwe conjecture that the proposed training objectives\nenhance the stability of generalization by incorpo-\nrating retrieval memory alongside parameters.\nThe advantages of EM-L and R-L are more pro-\nnounced on challenging tasks,such as sentence\npair classification, and aspect-based sentiment anal-\nysis. In this regard, EM-L and R-L achieve im-\nprovements of more than 0.3 on most datasets for\nsentence pair classification and ABSA, whereas\nthe improvement on the single-sentence classifi-\ncation ranges from 0.1 to 0.2, which gain further\nhighlights the effectiveness of EM-L and R-L.\n4.3 Consistency Experiments\nThe Kendall’s τ′defined in Eq. (11) on selected\ndatasets are reported in Table 2, which measures\nthe consistency between retrieval metrics of fetched\nexamples and their auxiliaries to downstream tasks.\nCombing the results in Table 1, higher τ′of EM-L\nAccuracy SST-2 MR TREC QQP\nVanilla 80.22 60 .71 86 .05 64 .27\nStatic 76.58 67 .51 86 .94 60 .30\nJoint 85.41 71 .01 86 .57 61 .92\nEM-L 87.30 78.75 87.52 67.90\nR-L 89.79 77.38 88.78 66.77\nTable 3: Comparison results on8-shot text classification.\nStandard deviations are omitted to save space.\nMR TREC RES LAP\nAccuracy\nVanilla 90.80 96.80 86.53 80.87\nStatic 91.40 97.60 87.50 81.19\nJoint 90.90 97.80 87.58 82.13\nEM-L 91.70 98.00 88.04 82.76\nR-L 91.45 98.00 88.48 83.22\nKendall’sτ′\nStatic 0.4340 0.5280 0.5705 0.4310\nJoint 0.5075 0.6580 0.7187 0.7492\nEM-L 0.9195 0.7880 0.8700 0.8564\nR-L 0.9090 0.7160 0.8889 0.8903\nTable 4: Comparison results with full supervision of\nthe original datasets. Standard deviations are omitted to\nsave space.\nand R-L indicates that they could prioritize more\nhelpful examples according to their correspond-\ning metrics and improve the performance by train-\ning more effective retrievers. However, retriev-\ning examples according to static metrics and joint\nlearning-based metrics may result in the inclusion\nof harmful examples in the final performance.\n4.4 Auxiliary Experiment\nWe further conduct additional experiments in both\n8-shot and full supervision settings to investigate\nthe advantages of EM-L and R-L on different data\nscales. The results are presented in Table 3 and\nTable 4, respectively. It is obvious that EM-L and\nR-L consistently exhibit excellence in both settings.\nParticularly, we note a more significant improve-\nment of our methods in the 8-shot setting, which\nmanifests that the proposed training methods train\nthe retriever more effectively, especially when the\ntraining data is scarce.\nMoreover, another interesting phenomenon\nemerged: although EM-L and R-L achieve higher\nKendall’s τ′in the full supervision setting, their im-\nprovements in text classification are comparatively\n6726\nsmaller compared to that in few-shot scenarios. We\nbelieve this can be attributed to the fact that the\nclassifier in the full supervision setting is already\nwell-trained so the potential improvement from a\nbetter retrieval memory is relatively limited.\n5 Analysis\n5.1 Effects of the Number of Retrieved\nExamples\nTo examine the effects of the number m on var-\nious retriever training methods, we present line\ncharts in Fig. 1 that depict the relationship between\nAccuracy and m. First, all the charts demonstrate\nretrieving examples could enhance the performance\nof few-shot text classification, except for a slightly\nlower accuracy of static retrieval and joint learning-\nbased retrieval when mtakes specific values. This\ncould be attributed to the instability of their train-\ning process. Second, most methods achieve their\npeak performance at m = 5 or m = 10. As m\ncontinues to increase, the performance may start to\ndeteriorate. We guess the reason is that retrieving\ntoo many examples increases the training difficulty.\nThird, we observe EM-L and R-L maintain sus-\ntaining advantages and stability as mvaries, which\nverifies their stronger supervision signals. Another\nobservation is that the joint learning-based method\nfalls behind the static method on LAP. This find-\ning suggests that in certain tasks, a poorly trained\nlearnable metric even exhibits inferior performance\ncompared to a static metric.\n5.2 Gradient Updates\nIn order to assess the supervision signals exerted on\nthe retrievers by different methods, we quantify the\naverage gradients of all retrievers’ parameters. This\nmeasurement allows us to evaluate the guidance\nprovided by each method to the retriever during the\ntraining process. Fig. 2 illustrates the percentage of\ntraining steps where the average gradients of all re-\ntrievers’ parameters exceed the threshold of 1e−6.\nFor clarity, we exclude static retrieval from this fig-\nure since its retriever has no trainable parameters1.\nOur analysis revealed that on certain datasets, the\ngradient norm of the joint learning-based retriever\nexceeds the threshold of 1e−6 for only about 40%\nof the steps, whereas EM-L and R-L surpass this\nthreshold in over 60% of the steps. This obser-\nvation suggests that both static and joint learning-\n1This corresponds to a constant proportion of zero for steps\nwith a gradient norm exceeding 1e-6.\nbased retrieval provide weaker supervision signals\nto the retrievers and suffer from severe vanishing\nissues in few-shot text classification while EM-L\nand R-L alleviate such limitations.\n5.3 Case Study\nFinally, we present an illustrative example from\nthe LAP dataset along with the retrieved examples\nusing different methods in Fig. 3. In the input sen-\ntence, the aspect term “startup times” is negative.\nAlthough static retrieval fetches a semantic similar\nexample, it includes information that could poten-\ntially mislead the sentiment prediction, such as the\nterm \"spectacular\". The joint learning-based re-\ntrieval retrieves an example that seems unrelated to\nthe input sentence, possibly indicating that weak\nsupervision signals for the retriever are prone to\nworse retrieval results. In contrast, our EM-L and\nR-L methods are capable of retrieving examples\nthat may not possess high semantic similarity but\nare more beneficial for sentiment prediction.\n6 Related Work\n6.1 Retrieval-augmented Methods\nRetrieval-augmented methods enhance the ability\nof the Pre-trained Language Models in processing\nvarious natural language tasks by fetching relevant\nexamples from the training set or external knowl-\nedge base and prepending them with the original\ninput. These methods have improved the perfor-\nmance of a lot of tasks, such as neural machine\ntranslation (Zhang et al., 2018; Cai et al., 2021;\nLi et al., 2022; Wang et al., 2022), question an-\nswering (Li et al., 2020; Karpukhin et al., 2020;\nSingh et al., 2021; Wang et al., 2022; Siriward-\nhana et al., 2023; Li et al., 2023; Hofstätter et al.,\n2023), dialog generation (Fan et al., 2021; Thulke\net al., 2021; King and Flanigan, 2023), text classi-\nfication (Izacard et al., 2022; Lewis et al., 2020),\nkeyphrase generation (Gao et al., 2022), etc. Ac-\ncording to retrieval metrics, these methods could\nbe categorized as static retrieval methods and joint\nlearning-based methods, which use a fixed retrieval\nmetric and jointly learnable metric respectively.\nDifferent from the above methods, which fetch\nrelevant examples from the large-scale corpus, we\npropose two novel training objectives to retrieve\nexamples in a restricted retrieval space and analyze\ntheir advantages. Following Singh et al. (2021);\nIzacard et al. (2022), we formulate the retrieval-\naugmented methods into a retriever and a classifier\n6727\n(a) CR (b) QQP (c) LAP\nFigure 1: Effects of the number mof retrieved examples. The results are average Accuracy on the validation set.\nFigure 2: The proportion of steps in which the average\ngradient of retriever’s all parameters is more than1e−6.\nin Eq. (1) for a fair comparison.\n6.2 Prompt Engineering\nFueled by the birth of large-scale language mod-\nels (Brown et al., 2020), prompt-based learn-\ning (Liu et al., 2023) for the Pre-trained Lan-\nguage Models has been developed to convert dif-\nferent downstream tasks into cloze-style mask lan-\nguage model objectives, achieving impressive per-\nformance in text classification (Wang et al., 2021;\nGao et al., 2021; Hambardzumyan et al., 2021;\nLester et al., 2021; Schick et al., 2020; Schick\nand Schütze, 2021), sentiment classification (Seoh\net al., 2021; Yan et al., 2021; Chen and Qian,\n2020; Zhang et al., 2021), named entity recogni-\ntion (Cui et al., 2021), relation extraction (Chen\net al., 2022b,b), question answering (Lewis et al.,\n2019; Khashabi et al., 2020), commonsense reason-\ning (Shwartz et al., 2020), etc. Orthogonal to these\nstudies of prompt learning, our paper focuses on the\ncomparison of different retrieval methods, where\nprompt learning is just employed as a backbone.\n6.3 Few-shot Text Classification\nFew-shot Text Classification trains a classifier with\nlimited data for each class, which can also predict\nunseen classes. Existing studies for few-shot text\nclassification encompass various approaches such\nas prototypical networks (Jake et al., 2017), XLNet-\nbased methods (Zhilin et al., 2019), (Ro)BERT(a)-\nbased methods (Chen et al., 2020, 2022a), Pattern-\nexploiting training (Schick and Schütze, 2021),\nprompt tuning (Lester et al., 2021; Gao et al., 2021),\netc. And common sub-tasks in text classification\nconsist of intention classification, topic classifica-\ntion, sentiment classification, etc. We evaluate our\nmethods on different text classification tasks, with\na focus on adapting the idea of retrieval-augmented\nmethods to the few-shot scenarios through the de-\nsign of new training objectives.\n7 Conclusion\nThis paper studies the retrieval-augmented methods\nfor few-shot text classification and demonstrates\nthe challenges which hinder their success: it is im-\npossible to retrieve semantically similar examples\nby using an off-the-shelf metric and it is difficult\nto optimize a plausible metric by minimizing the\nstandard cross-entropy loss. Accordingly, it pro-\nposes two novel training objectives, EM-L and R-L,\nwhich provide stronger supervision signals to train\nthe retrieval metric effectively in few-shot scenar-\nios. It is worth mentioning that the idea of search-\ning within limited examples bears similarity to the\nconcept of demonstration selection in recent large\nlanguage models (LLMs). Exploring the applica-\ntion of our methods in LLMs holds promise for\nfuture research.\n6728\nInput: Startup timesare incredibly long : over two minutes. The sentiment polarity of startup timeswas <mask>.MethodsPredictionsRetrieved ExamplesThe internet speedis spectacular. The sentiment polarity of internet speedwas<mask>.StaticpositiveThat included the extra Sony Sonic Stage software , the speakers and the subwoofer I got -LRB-that WAS worth the money -RRB-, the bluetooth mouse for my supposedly bluetooth enabled computer , the extended life battery and the docking port.The sentiment polarity of docking portwas<mask>.Jointpositive\nIts not just slow on the internet, its slow in general. The sentiment polarity of internetwas <mask>.EM-LnegativeAnother thing is that after only a month the keyboardbroke and it costed $175 to send it in to fix it . The sentiment polarity ofkeyboardwas <mask>.R-Lnegative\nLabelsforRetrievedExamplespositive\nneutral\nnegative\nnegative\nFigure 3: Case Study. “Input” denotes an input sentence from LAP, “Predictions” represents the predicted sentiment\npolarities of different methods, and “Retrieved Examples” is the fetched examples with the highest metric in the\ntraining set. “Labels for Retrieved Example” denotes sentiment labels of the fetched examples.\nLimitations\nThere are three primary limitations of our methods.\nFirstly, EM-L and R-L require additional training\ntime compared to existing retrieval methods. It\nis due to the alternation between the E-step and\nM-step in EM-L and the optimization of an addi-\ntional loss of R-L. Specifically, the training time for\nEM-L per epoch is approximately 1.5 times that of\nstatic retrieval and 1.2 times that of joint learning-\nbased retrieval. Similarly, the training time for R-L\nper epoch is about 1.8 times that of static retrieval\nand 1.5 times that of joint learning-based retrieval.\nAlthough our proposed methods require more time,\nthey still fall within the acceptable range. Secondly,\nwe didn’t focus on designing more sophisticated\ntemplates for prompt engineering, as our main em-\nphasis was on exploring different retrieval methods.\nThirdly, we evaluate our methods in few-shot set-\ntings constructed from widely used datasets, rather\nthan real-world scenes. This could limit the gener-\nalizability of our findings to practical applications.\nAcknowledgements\nThe research work is supported by the National\nKey R&D Plan No. 2022YFC3303303, the Na-\ntional Natural Science Foundation of China un-\nder Grant (No.61976204). This study is also sup-\nported by grants from the Major Key Project of\nPCL (Grant Number: PCL2022D01) and the CAAI\nHuawei MindSpore Open Fund. Xiang Ao is also\nsupported by the Project of Youth Innovation Pro-\nmotion Association CAS, Beijing Nova Program\nZ201100006820062.\nReferences\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International conference on ma-\nchine learning, pages 2206–2240. PMLR.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large annotated\ncorpus for learning natural language inference. arXiv\npreprint arXiv:1508.05326.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nChris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,\nMatt Deeds, Nicole Hamilton, and Greg Hullender.\n2005. Learning to rank using gradient descent. In\nProceedings of the 22nd international conference on\nMachine learning, pages 89–96.\nDeng Cai, Yan Wang, Huayang Li, Wai Lam, and\nLemao Liu. 2021. Neural machine translation with\nmonolingual translation memory. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 7307–7318, Online.\nAssociation for Computational Linguistics.\n6729\nXiang Chen, Lei Li, Ningyu Zhang, Xiaozhuan Liang,\nShumin Deng, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. 2022a. Decoupling knowledge from\nmemorization: Retrieval-augmented prompt learning.\narXiv preprint arXiv:2205.14704.\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng,\nYunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. 2022b. Knowprompt: Knowledge-\naware prompt-tuning with synergistic optimization\nfor relation extraction. In Proceedings of the ACM\nWeb Conference 2022, pages 2778–2788.\nXilun Chen, Asish Ghoshal, Yashar Mehdad, Luke\nZettlemoyer, and Sonal Gupta. 2020. Low-resource\ndomain adaptation for compositional task-oriented\nsemantic parsing. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 5090–5100, Online. As-\nsociation for Computational Linguistics.\nZhuang Chen and Tieyun Qian. 2020. Enhancing aspect\nterm extraction with soft prototypes. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n2107–2117, Online. Association for Computational\nLinguistics.\nLeyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue Zhang.\n2021. Template-based named entity recognition us-\ning bart. arXiv preprint arXiv:2106.01760.\nArthur P Dempster, Nan M Laird, and Donald B Rubin.\n1977. Maximum likelihood from incomplete data\nvia the em algorithm. Journal of the royal statistical\nsociety: series B (methodological), 39(1):1–22.\nAngela Fan, Claire Gardent, Chloé Braud, and Antoine\nBordes. 2021. Augmenting transformers with knn-\nbased composite memory for dialog. Transactions of\nthe Association for Computational Linguistics, 9:82–\n99.\nYoav Freund and Robert E Schapire. 1997. A decision-\ntheoretic generalization of on-line learning and an\napplication to boosting. Journal of computer and\nsystem sciences, 55(1):119–139.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nYifan Gao, Qingyu Yin, Zheng Li, Rui Meng, Tong\nZhao, Bing Yin, Irwin King, and Michael Lyu. 2022.\nRetrieval-augmented multilingual keyphrase gener-\nation with retriever-generator iterative training. In\nFindings of the Association for Computational Lin-\nguistics: NAACL 2022, pages 1233–1246.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. W ARP: Word-level Adversarial\nReProgramming. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 4921–4933, Online. Association for\nComputational Linguistics.\nSebastian Hofstätter, Jiecao Chen, Karthik Raman, and\nHamed Zamani. 2023. Fid-light: Efficient and effec-\ntive retrieval-augmented text generation. In Proceed-\nings of the 46th International ACM SIGIR Confer-\nence on Research and Development in Information\nRetrieval, pages 1437–1447.\nMinqing Hu and Bing Liu. 2004. Mining and summa-\nrizing customer reviews. In Proceedings of the tenth\nACM SIGKDD international conference on Knowl-\nedge discovery and data mining, pages 168–177.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lucas\nHosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-\nYu, Armand Joulin, Sebastian Riedel, and Edouard\nGrave. 2022. Atlas: Few-shot learning with retrieval\naugmented language models.\nSnell Jake, Swersky Kevin, and Zemel Richard. 2017.\nPrototypical networks for few-shot learning. Ad-\nvances in Neural Information Processing Systems,\n30.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UNIFIEDQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2020, pages 1896–1907, Online. Association\nfor Computational Linguistics.\nBrendan King and Jeffrey Flanigan. 2023. Diverse\nretrieval-augmented in-context learning for dialogue\nstate tracking. In Findings of the Association for\nComputational Linguistics: ACL 2023, pages 5570–\n5585.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nPatrick Lewis, Ludovic Denoyer, and Sebastian Riedel.\n2019. Unsupervised question answering by cloze\ntranslation. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\n6730\npages 4896–4910, Florence, Italy. Association for\nComputational Linguistics.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nHuayang Li, Yixuan Su, Deng Cai, Yan Wang, and\nLemao Liu. 2022. A survey on retrieval-augmented\ntext generation. arXiv preprint arXiv:2202.01110.\nShiyang Li, Yifan Gao, Haoming Jiang, Qingyu Yin,\nZheng Li, Xifeng Yan, Chao Zhang, and Bing Yin.\n2023. Graph reasoning for question answering with\ntriplet retrieval. arXiv preprint arXiv:2305.18742.\nXiaoya Li, Yuxian Meng, Mingxin Zhou, Qinghong\nHan, Fei Wu, and Jiwei Li. 2020. Sac: Accelerating\nand structuring self-attention via sparse adaptive con-\nnection. Advances in Neural Information Processing\nSystems, 33:16997–17008.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nSuresh Manandhar. 2014. Semeval-2014 task 4: As-\npect based sentiment analysis. In Proceedings of the\n8th international workshop on semantic evaluation\n(SemEval 2014), pages 27–35.\nBo Pang and Lillian Lee. 2004. A sentimental education:\nSentiment analysis using subjectivity summarization\nbased on minimum cuts. arXiv preprint cs/0409058.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nRuiyang Ren, Yingqi Qu, Jing Liu, Wayne Xin Zhao,\nQiaoQiao She, Hua Wu, Haifeng Wang, and Ji-Rong\nWen. 2021. RocketQAv2: A joint training method\nfor dense passage retrieval and passage re-ranking.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2825–2835, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nStephen Robertson, Hugo Zaragoza, et al. 2009. The\nprobabilistic relevance framework: Bm25 and be-\nyond. Foundations and Trends® in Information Re-\ntrieval, 3(4):333–389.\nCynthia Rudin and Robert E Schapire. 2009. Margin-\nbased ranking and an equivalence between adaboost\nand rankboost.\nTimo Schick, Helmut Schmid, and Hinrich Schütze.\n2020. Automatically identifying words that can serve\nas labels for few-shot text classification. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 5569–5578, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nRonald Seoh, Ian Birle, Mrinal Tak, Haw-Shiuan Chang,\nBrian Pinette, and Alfred Hough. 2021. Open aspect\ntarget sentiment classification with natural language\nprompts. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6311–6322, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nVered Shwartz, Peter West, Ronan Le Bras, Chandra\nBhagavatula, and Yejin Choi. 2020. Unsupervised\ncommonsense question answering with self-talk. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4615–4629, Online. Association for Computa-\ntional Linguistics.\nDevendra Singh, Siva Reddy, Will Hamilton, Chris\nDyer, and Dani Yogatama. 2021. End-to-end train-\ning of multi-document reader and retriever for open-\ndomain question answering. Advances in Neural\nInformation Processing Systems, 34:25968–25981.\nShamane Siriwardhana, Rivindu Weerasekera, Elliott\nWen, Tharindu Kaluarachchi, Rajib Rana, and\nSuranga Nanayakkara. 2023. Improving the domain\nadaptation of retrieval augmented generation (rag)\nmodels for open domain question answering. Trans-\nactions of the Association for Computational Linguis-\ntics, 11:1–17.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\n6731\nKaren Sparck Jones. 1972. A statistical interpretation\nof term specificity and its application in retrieval.\nJournal of documentation, 28(1):11–21.\nDavid Thulke, Nico Daheim, Christian Dugast, and\nHermann Ney. 2021. Efficient retrieval augmented\ngeneration from unstructured knowledge for task-\noriented dialog. arXiv preprint arXiv:2102.04643.\nEllen M V oorhees and Dawn M Tice. 2000. Building a\nquestion answering test collection. In Proceedings\nof the 23rd annual international ACM SIGIR confer-\nence on Research and development in information\nretrieval, pages 200–207.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nChengyu Wang, Jianing Wang, Minghui Qiu, Jun\nHuang, and Ming Gao. 2021. TransPrompt: Towards\nan automatic transferable prompting framework for\nfew-shot text classification. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2792–2802, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nShuohang Wang, Yichong Xu, Yuwei Fang, Yang Liu,\nSiqi Sun, Ruochen Xu, Chenguang Zhu, and Michael\nZeng. 2022. Training data is more valuable than you\nthink: A simple and effective method by retrieving\nfrom training data. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 3170–3179,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nHang Yan, Junqi Dai, Tuo Ji, Xipeng Qiu, and Zheng\nZhang. 2021. A unified generative framework for\naspect-based sentiment analysis. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 2416–2429, Online.\nAssociation for Computational Linguistics.\nJingyi Zhang, Masao Utiyama, Eiichro Sumita, Gra-\nham Neubig, and Satoshi Nakamura. 2018. Guiding\nneural machine translation with retrieved translation\npieces. arXiv preprint arXiv:1804.02559.\nWenxuan Zhang, Yang Deng, Xin Li, Yifei Yuan, Li-\ndong Bing, and Wai Lam. 2021. Aspect sentiment\nquad prediction as paraphrase generation. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing, pages 9209–\n9219, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nYang Zhilin, Dai Zihang, Yang Yiming, Carbonell\nJaime, Salakhutdinov Ruslan, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. The 33rd Conference on Neu-\nral Information Processing Systems (NeurIPS 2019).\nA Proof of the EM-L Method\nProposition. Optimizing the following two like-\nlihood functions is equivalent in EM-L:\nmax\nθ,ϕ\nn∏\ni\nPθ,ϕ(y|xi) ⇐⇒\nmax\nθ,ϕ\nn∑\ni\nm∑\nj\nPθ,ϕ(zj|xi,y) logPθ(y|xi,zj),\nwhere Pθ,ϕ(zj|xi,y)\n:= Pθ(y|xi,zj)Pϕ(zj|xi)∑m\nj Pθ(y|xi,zj)Pϕ(zj|xi),\n(12)\nwhere xi is the representation of the i-th sentence.\nFor each xi, the retriever fetches mexamples from\nthe corpus to assist xi in text classification, where\neach example is represented as zj.\nProof. We first use variational inference to derive\nthe lower bound of the original likelihood:\nmax\nn∏\ni\nPθ,ϕ(y|xi)\n⇐⇒max log\nn∏\ni\nPθ,ϕ(y|xi)\n= max\nn∑\ni\nlog Pθ,ϕ(y|xi)\n= max\nn∑\ni\nlog\nm∑\nj\nPθ(y|xi,zj)Pϕ(zj|xi)\n= max\nn∑\ni\nlog\nm∑\nj\nPϕ,θ(y,zj|xi)\n(13)\nLet Q(zj) be a random distribution of zj:\nmax\nn∑\ni\nlog\nm∑\nj\nPϕ,θ(y,zj|xi)\n= max\nn∑\ni\nlog\nm∑\nj\nQ(zj)Pϕ,θ(y,zj|xi)\nQ(zj)\n≥max\nn∑\ni\nm∑\nj\nQ(zj) logPϕ,θ(y,zj|xi)\nQ(zj)\n(14)\n6732\nThe last step is according to Jansen inequality\nand equals if and only if Q(zj) is proportional to\nPθ,ϕ(y,zj|xi) and cis a constant. Such a propor-\ntional relationship can be expressed as:\nQ(zj)\nPθ,ϕ(y,zj|xi) = c,c is a constant\n⇐⇒cPθ,ϕ(y,zj|xi) =Q(zj), ∀i,j\n(15)\nSince ∑\nj Q(zj) = 1, we can sum z on both sides\nof the equation:\n⇐⇒c\nm∑\nj\nPθ,ϕ(y,zj|xi) = 1\n⇐⇒c= 1∑m\nj Pθ,ϕ(y,zj|xi)\n(16)\nNow we can derive a lower bound of∏n\ni Pθ,ϕ(y|xi)\nby substituting cinto Eq.(15) and then substituting\nQ(zj) to Eq.(14):\nQ(zj) = Pθ,ϕ(y,zj|xi)∑m\nj Pθ,ϕ(y,zj|xi)\n= Pθ(y|xi,zj)Pϕ(zj|xi)∑m\nj Pθ(y|xi,zj)Pϕ(zj|xi)\n= Pθ,ϕ(zj|xi,y)\n(17)\nmax\nn∏\ni\nPθ,ϕ(y|xi) ⇐⇒\nmax(\nn∑\ni\nm∑\nj\nQ(zj) logPθ,ϕ(y,zj|xi)\n−\nn∑\ni\nm∑\nj\nQ(zj) logQ(zj))\n(18)\nSince Pθ,ϕ(y,zj|xi) =Pθ(y|xi,zj)Pϕ(zj|xi), we\ncan further simplify Eq.(18) as follows:\nmax(\nn∑\ni\nm∑\nj\nQ(zj) logPθ,ϕ(y,zj|xi)\n−\nn∑\ni\nm∑\nj\nQ(zj) logQ(zj))\n= max(\nn∑\ni\nm∑\nj\nQ(zj) logPθ(y|xi,zj)Pϕ(zj|xi)\n−\nn∑\ni\nm∑\nj\nQ(zj) logQ(zj))\n= max(\nn∑\ni\nm∑\nj\nQ(zj) logPθ(y|xi,zj)\n+\nn∑\ni\nm∑\nj\nQ(zj) logPϕ(zj|xi)\n−\nn∑\ni\nm∑\nj\nQ(zj) logQ(zj))∗\n= max(\nn∑\ni\nm∑\nj\nQ(zj) logPθ(y|xi,zj))\n= max(\nn∑\ni\nm∑\nj\nPϕ(zj|xi,y) logPθ(y|xi,zj))\n(19)\nSpecifically, in the step denoted with\n*, ∑n\ni\n∑m\nj Q(zj) logPϕ(zj|xi) and∑n\ni\n∑m\nj Q(zj) logQ(zj)) can be canceled\nout, because Q(zj) = Pϕ(zj|xi,y) ≈Pϕ(zj|xi)\nin Eq. (17).\nFurther proof for convergence and equality of\nthe original two optimizations is ordinary to derive\nas the proof of the EM algorithm, which is omitted\nhere.\nB Dataset Detail\nB.1 Original Datasets\nAll the retrieval methods are evaluated on three\ntypes of datasets: single-sentence classification,\nsentence pair classification, and aspect-based senti-\nment analysis (ABSA). The single-sentence classi-\nfication consists of SST-2 (Socher et al., 2013),\nMR (Pang and Lee, 2004), CR (Hu and Liu,\n2004), and TREC (V oorhees and Tice, 2000).\nThe sentence pair classification includes QQP 2,\n2https://quoradata.quora.com\n6733\nDataset Input Output Train Test Type\nSST-2 sentencex 1: positive\n0: negative 6,920 872 sentiment classification\nMR sentencex 1: positive\n0: negative 8,662 2,000 sentiment classification\nCR sentencex 1: positive\n0: negative 1,775 2,000 sentiment classification\nTREC sentencex\n0: Personality\n1: Advisor\n2: Conclusion\n3: Human\n4 :Assignment\n5: Minute\n5,452 500 question classification\nQQP sentencex1, x2\n1: entailment\n0: not entailment363,846 40,431 paraphrase\nQNLI sentencex1, x2\n1: entailment\n0: not entailment104,743 5,463 Natural Language Inference\nMNLI sentencex1, x2\n2: entailment\n1: neutral\n0: contradiction\n392,702 9,815 Natural Language Inference\nSNLI sentencex1, x2\n2: entailment\n1: neutral\n0: contradiction\n549,367 9,842 Natural Language Inference\nRES sentencex, aspect a\n2: positive\n1: neutral\n0: negative\n3,044 800 aspect-based sentiment analysis\nLAP sentencex, aspect a\n2: positive\n1: neutral\n0: negative\n3,048 800 aspect-based sentiment analysis\nTable 5: Dataset details. The column labeled \"Train\" represents the number of instances in the original training set,\nwhile \"Test\" denotes the number of instances in the test set. The \"Type\" column describes the task type associated\nwith each dataset.\nQNLI (Rajpurkar et al., 2016), SNLI (Bowman\net al., 2015), and MNLI (Williams et al., 2017).\nThe aspect-based sentiment analysis datasets are\nRES (Manandhar, 2014) and LAP (Manandhar,\n2014). Particularly, for SST-2, MNLI, and QNLI\nfrom GLUE (Wang et al., 2018) and SNLI, we\nutilize their original validation sets for testing pur-\nposes.\nB.2 Few-shot Datasets\nFollowing the few-shot setting of Gao et al. (2021),\nwe randomly select 16 or 8 examples from the train-\ning set to create 16-shot or 8-shot experiments.\nSpecifically, we generate five distinct few-shot\ndatasets using different seeds and train models on\neach of them. It is noted that we use consistent five\nseeds on different datasets and retrieval methods\nto conduct a fair comparison. The best model is\nchosen based on the validation results, and the av-\nerage evaluation scores on the original test set are\nreported.\nC Experimental Settings\nC.1 Hyper-parameter Selection\nWe adopt grid search to choose the hyper-\nparameters of different methods. Specifically, the\nlearning rates are taken from{1e−5,2e−5,5e−5},\nthe batch sizes are from{4,8,16}, and the numbers\nof retrieved examples are taken from {5,10,15}.\nThe parameter t that determines the update fre-\nquency of loss LR is searched from {5,10,15}.\nThe loss coefficient λin ranking-based loss is set\nto {0.5,1,2}. For each dataset, we set the max\ntraining steps as 800 steps and use early stopping\nto avoid over-fitting. In each trial, we validate the\nmodel in each epoch and save the best checkpoint.\nWe adopt the AdamW optimizer and accumu-\nlate gradients for each batch. The code is imple-\n6734\nDataset Template Label\nSST-2\nInput sentence x, it was <mask>. 1: positive →good\n0: negative →terribleMR\nCR\nTREC Input sentence x, it was <mask>.\n0: Personality →Personality\n1: Advisor →Advisor\n2: Conclusion →Conclusion\n3: Human →Hum\n4 :Assignment →Assignment\n5: Minute →Minute\nQQP Input sentence x1, <mask>, x2. 1: entailment →Yes\n0: not entailment →No QNLI\nMNLI Input sentence x1, <mask>, x2. 2: positive →positive\n1: neutral →neutral\n0: negative →nagative\nSNLI\nRES Input sentence x, the a was <mask>. 2: positive →positive\n1: neutral →neutral\n0: negative →nagative\nLAP\nTable 6: Templates and label words for different datasets that we used for prompt-based fine-tuning.\nmented with PyTorch 1.9.0 and transformers 4.1.1\nand launched on an Ubuntu server with a single\nNVIDIA Tesla V100 (32G) or NVIDIA 4090. In\naddition, we will test our model with Mindspore,\nwhich is a new deep-learning framework3.\nC.2 Templates of Prompt-based Fine-tuning\nWe use RoBERTa-Large (Liu et al., 2019)4 with\n1024 dimensions to encode the input sentences\nwith the related template. The templates for var-\nious datasets are shown in Table 6. Since our\nmain aim is to investigate the difference among\nretrieval methods, we adopt the widely used and\neffective templates for these tasks in prompt-based\nfine-tuning refer to Gao et al. (2021). The specific\ntemplates are shown in Table 6.\n3https://www.mindspore.cn/\n4https://github.com/huggingface/transformers\n6735",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7382246255874634
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.6962038278579712
    },
    {
      "name": "Metric (unit)",
      "score": 0.6635316014289856
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.5623628497123718
    },
    {
      "name": "Information retrieval",
      "score": 0.4815997779369354
    },
    {
      "name": "Task (project management)",
      "score": 0.4571831226348877
    },
    {
      "name": "Image retrieval",
      "score": 0.4521614611148834
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4099689722061157
    },
    {
      "name": "Machine learning",
      "score": 0.34104156494140625
    },
    {
      "name": "Data mining",
      "score": 0.32458677887916565
    },
    {
      "name": "Image (mathematics)",
      "score": 0.10580021142959595
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210090176",
      "name": "Institute of Computing Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210136793",
      "name": "Peng Cheng Laboratory",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2250653659",
      "name": "Tencent (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    }
  ]
}