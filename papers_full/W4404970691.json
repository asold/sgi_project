{
  "title": "Zero-Shot Prompting and Few-Shot Fine-Tuning: Revisiting Document Image Classification Using Large Language Models",
  "url": "https://openalex.org/W4404970691",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5093428981",
      "name": "Anna Scius-Bertrand",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "University of Fribourg"
      ]
    },
    {
      "id": "https://openalex.org/A5075268402",
      "name": "Michael Jungo",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "University of Fribourg"
      ]
    },
    {
      "id": "https://openalex.org/A5016271960",
      "name": "Lars Vögtlin",
      "affiliations": [
        "University of Fribourg"
      ]
    },
    {
      "id": "https://openalex.org/A5114979385",
      "name": "Jean-Marc Spat",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland"
      ]
    },
    {
      "id": "https://openalex.org/A5019597653",
      "name": "Andreas Fischer",
      "affiliations": [
        "HES-SO University of Applied Sciences and Arts Western Switzerland",
        "University of Fribourg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3034556548",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2962772269",
    "https://openalex.org/W4312233877",
    "https://openalex.org/W1966382373",
    "https://openalex.org/W3000758063",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2964036463",
    "https://openalex.org/W3176851559",
    "https://openalex.org/W4402670135"
  ],
  "abstract": null,
  "full_text": "Zero-Shot Prompting and Few-Shot Fine-Tuning:\nRevisiting Document Image Classification Using\nLarge Language Models\nAnna Scius-Bertrand⋆1,2, Michael Jungo⋆1,2, Lars Vögtlin2, Jean-Marc Spat1,\nand Andreas Fischer1,2\n1 University of Applied Sciences and Arts Western Switzerland\n{anna.scius-bertrand,michael.jungo,jean-marc.spat,andreas.fischer}@hefr.ch\n2 University of Fribourg, Switzerland\n{anna.scius-bertrand,michael.jungo,lars.vogtlin,andreas.fischer}@unifr.ch\nAbstract. Classifying scanned documents is a challenging problem that\ninvolves image, layout, and text analysis for document understanding.\nNevertheless, for certain benchmark datasets, notably RVL-CDIP, the\nstate of the art is closing in to near-perfect performance when consid-\nering hundreds of thousands of training samples. With the advent of\nlarge language models (LLMs), which are excellent few-shot learners,\nthe question arises to what extent the document classification problem\ncan be addressed with only a few training samples, or even none at all.\nIn this paper, we investigate this question in the context of zero-shot\nprompting and few-shot model fine-tuning, with the aim of reducing the\nneed for human-annotated training samples as much as possible.\nKeywords: Document Image Classification · OCR · Deep Learning ·\nTransformers· Large Language Models· Few-Shot Learning· Prompting\n1 Introduction\nThe RVL-CDIP dataset, introduced by Harley et al. [6], is a subset of 400000 la-\nbeled document images derived from the IIT-CDIP collection, originating from\na litigation against the tobacco industry [12], which has significantly boosted\nthe exploration of deep learning methods for document image classification in\nthe last decade. Notable advancements include the application of convolutional\nneural networks [19], the integration of text, image, and layout embeddings as\ndemonstrated by LayoutLM [21], OCR-free document understanding through\nTransformers (Donut) [11], and cross-modal strategies that fuse image and tex-\ntual analysis techniques [2]. The cross-modal strategies stand out by achieving an\nimpressive classification accuracy of 97.05% across 16 distinct document types,\nincluding letters, forms, and emails.\nNevertheless, neural network models with a rising number of trainable pa-\nrameters require a large training set of labeled documents to perform satisfac-\ntorily. For example, the RVL-CDIP benchmark needs a training set of 320000\n⋆ These authors contributed equally to this work.\nProceedings of the27th International Conference on Pattern Recognition 2024 (ICPR 2024)\narXiv:2412.13859v1  [cs.CV]  18 Dec 2024\n2 A. Scius-Bertrand and M. Jungo et al.\nlabeled documents to distinguish between 16 document classes. If the document\ncategories change, or for a new dataset, the training set must be relabeled ac-\ncordingly, which leads to a costly and time-consuming human effort.\nThe recent advent of large language models (LLMs) has impressively shown\nthat large networks with billions, or even more than a trillion, parameters, only\nneed very few training samples, if any, to solve challenging tasks in natural\nlanguage processing, including closed-book question answering, translation, and\nreading comprehension [3]. LLMs typically rely on unsupervised pre-training of\ndecoder-onlytransformerarchitecturesonalargebodyoftextsfromtheinternet,\nsuch as the Common Crawl dataset [18], followed by reinforcement learning from\nhuman feedback, to achieve astounding generalization capabilities.\nWith respect to the task of document classification, the question arises to\nwhat extent LLMs may be capable of solving the task without the need of\nhundreds of thousands of learning samples, relying on their text understand-\ning capabilities of the document texts, which are extracted by means of optical\ncharacter recognition (OCR).\nIn the present paper, we explore this question in a comprehensive benchmark\nevaluation that takes into account several state-of-the art LLMs for text analysis\n(Mistral [8], GPT-3 [3], GPT-4 [1]), defines different training scenarios, and puts\nthe LLM results into a broader context by including also a selection of smaller\nlanguage models (RoBERTa [13]), text embedding models (Jina [5]), OCR-free\nimage-based models (Donut [11]), and multi-modal LLMs (GPT-4-Vision [1]) in\nthe comparison. The aim of the benchmark evaluation is to investigate the doc-\nument classification performance for an increasing number of learning samples,\nstarting with zero-shot prompting, where only a textual description of the task\nis provided to the model, and ending with few-shot model fine-tuning using 100\nsamples per class. Note that fine-tuning of LLMs is a challenging task on stan-\ndard hardware. We rely on Low-Rank Adaptation (LoRA) [7] to fine-tune one of\nthe smaller open source LLMs, Mistral-7B [9], for the purpose of our benchmark.\nNo new method is proposed in this paper. Instead, our contributions are:\n– A comprehensive benchmark3 for evaluating document classification in a\nfew-shot training scenario.\n– Comparing LLM prompting vs LLM fine-tuning for document classification.\n– Comparing generative vs embedding-based document classification.\n– Comparing text-based vs image-based document classification.\nWith this benchmark evaluation and comparisons of current interest, we aim to\ninspire and support further research on few-shot document classification.\nThe paper is structured as follows: An introduction of the dataset is given\nin Section 2, a description of the methods in Section 3, and in Section 4 the\nexperimental results are presented. Lastly, we draw some conclusions and discuss\nfuture work.\n3 We will make the benchmark data and models publicly available with the publication\nof the paper.\nRevisiting Document Image Classification Using LLMs 3\n2 Data\nThe RVL-CDIP dataset [6] contains 25000 scanned grayscale images per docu-\nment class for 16 classes: letter, form, email, handwritten, advertisement, scien-\ntific report, scientific publication, specification, file folder, news article, budget,\ninvoice, presentation, questionnaire, resume, and memo. Several examples from\ndifferent categories are shown in Figure 1.\nFig.1: Example document images from the RVL-CDIP dataset.\nMany document classification and understanding methods require the text\ncontained in the images, which can be obtained with any OCR engine. No matter\nhow good the OCR engines have become, there are still difficulties to obtain a\nperfect textual representation of the documents. Most notably, parts of some\nimages are illegible due to degradation or any other quality issues. This makes\npurely text-based methods more difficult, as they also need to deal with the noise\ncreated by the OCR.\nFurthermore, several categories contain images with very little textual in-\nformation, making it potentially more difficult to distinguish between them as\nfile folders or advertisements shown in Figure 1. There is also a mix between\nhandwriting and printed text in the documents, which could cause issues for\nthe OCR. In any case, the visual information that is attached to the different\ntypes of writing is lost in the process. Therefore, any OCR-based method relies\nmuch more heavily on the meaning of the content, rather than its structure,\nalthough the grammatical structure remains a significant part of the textual\nrepresentation.\n3 Methods\nIn the following, we describe the different methods used in the benchmark: Text-\nbased classification using generative LLMs, text embedding classification, image-\nbased document classification, and multi-modal LLMs.\n4 A. Scius-Bertrand and M. Jungo et al.\n3.1 Text-Based Classification\nOCR For OCR, we use Amazon’s Textract, which performed reasonably well\non a few example images from the RVL-CDIP dataset, especially with respect\nto low-resolution, skewed documents, and handwritten elements. The resulting\nmachine-readable text follows a natural reading order and the text lines are\ndelimited by line-break characters.\nLLMs We focus on some of the best-performing LLMs from the current state of\nthe art, including the GPT models from OpenAI [1,3,16,17] and the models from\nMistral AI [8]. The transformer-based models have been pretrained on texts from\nthe internet and fine-tuned to follow instructions, as well as using reinforcement\nlearning from human feedback.\nTable 1: LLM versions.\nModel Version\nMistral-7B Open source [8]\nMixtral-8x7B Open source [10]\nMistral-Medium mistral-medium-2312\nMistral-Large mistral-large-2402\nGPT-3.5 gpt-3.5-turbo-0125\nGPT-4 gpt-4-turbo-2024-04-09\nTable 1 lists the specific versions used. Mistral-7B has7×109 parameters [8],\nMixtral-8x7B has 47 × 109 [10], and GPT-3 has175 × 109 [3]. To the best of\nour knowledge, no official information is available at the moment for the larger\nmodels.\nLLM Prompts The LLMs are used in chat completion mode with a system\nprompt that specifies the classification task in natural language. The prompt\nwas first formulated by a human. Afterwards, we used GPT-4 in chat completion\nmode to refine the original prompt and used it again to further tweak the refined\nprompt. The three resulting prompts are shown in Figure 2.\nZero-shot prompting: Only the system prompt is used, relying on the semantics\nof the category names for classification.\nOne-shot prompting: We provide context to the LLM by including 16 additional\npairs of prompts (useri, assistanti) after the system prompt, one for each cat-\negory, whereuseri is an OCR text from the training set andassistanti is its\ncategory name.\nRevisiting Document Image Classification Using LLMs 5\nPrompt 1 (P1)\nWritten by a human\nYou are a specialist for\nclassifying documents based\non their OCR text. I will\nshow you the OCR text of a\ndocument. There are only 16\nclasses:\n[\"letter\", \"form\", [...],\n\"resume\", \"memo\"]\nChoose one of these 16\nclasses, only a single class,\nand only from the list. Do\nnot comment your result, I\nonly want to know the name of\nthe class.\nPrompt 2 (P2)\nP1 improved by GPT4\nYour task is to classify a\ndocument’s content based\nsolely on its OCR text.\nSelect one appropriate\ncategory for the document\nfrom the following 16\noptions:\n1. letter\n2. form\n[...]\n15. resume\n16. memo\nProvide only the chosen\ncategory name from the list\nas your response, without\nany additional commentary or\nexplanation.\nPrompt 3 (P3)\nP2 tweaked with GPT4\nClassify the content of a\ndocument using its OCR text.\nChoose exactly one category\nfrom the list below that best\nfits the document:\n- letter\n- form\n[...]\n- resume\n- memo\nRespond with the name of\nthe selected category only.\nDo not provide additional\ncomments or explanations.\nFig.2: Three system prompts considered for document classification. Note that\nthe list of 16 categories has been shortened in the figure, indicated by [...], to\nsave space.\nFine-Tuning LLMsLLMs have learned a deep understanding of text through\nan extensive pretraining. Even though their primary appeal is to use them gen-\neratively, which makes them extremely flexible, it would stand to reason that\ntheir capabilities can also serve as a base for a classification model. In particular,\nunderstanding documents in a low-resource context could benefit from their vast\nknowledge of all kinds of texts, including a large variety of documents. Hence,\nwe explore the task of document classification by adding a classifier on top of an\nLLM. In order to fine-tune the LLM, we further employ Low-Rank Adaptation\n(LoRA) [7], to adapt it to the specific documents at hand.\nAs a comparison, to evaluate the effectiveness of using an LLM as the base\nmodel, we also fine-tune the largest available pretrained RoBERTa [13], which\nonly has 355M parameters. Because of its comparatively small size, the model\nis fine-tuned fully instead of having to resort to LoRA adapters.\nFinally, to see whether adding a classifier head is really necessary, we also\nfine-tune the same model in a generative manner, where at the end of each\ndocument, a classification instruction is added. With the generative approach,\nthe model maintains the ability to be adapted to any kind of output, as opposed\nto the classifier, where the output is limited to the classes it has been trained for.\nThere are a few disadvantages that come with this flexibility, most notably that\nthe output is no longer guaranteed to be exactly the class that was asked for,\nand an increase in compute, as the model now needs to predict multiple tokens.\nEven when the class name consists of a single token, the generative model needs\nto predict at least an end-of-sequence token in addition to the class name, which\nrequires multiple forward passes.\n6 A. Scius-Bertrand and M. Jungo et al.\n3.2 Embedding-Based Methods\nIn addition to generative approaches for classifying OCR texts, we also consider a\nstandard classification setup based on feature vector representations of the OCR\ntexts in an embedding space with ak-nearest neighbor classification (KNN). We\nalso conducted preliminary experiments with multi-layer perceptions (MLP), the\nresults of which were close to the KNN but never outperformed it.\nWe focus on some of the best-performing text embedding models from the\ncurrent state of the art, specifically a selection of models proposed by Jina AI [5],\nMistral AI, and OpenAI [14]. The versions used are listed in Table 2.\nTable 2: Embedding model versions.\nModel Version Embedding size\nJina-v2 jina-embeddings-v2-base-en 768\nMistral-embed mistral-embed 1024\nOpenAI-small text-embedding-3-small 1536\nOpenAI-large text-embedding-3-large 3072\n3.3 Image-Based Methods\nDonut Donut is a Transformer-based model for Visual Document Understand-\ning (VDU) that operates entirely on images, without having to rely on the OCR.\nBesides a large number of synthetic documents, the pretraining also included\nthe full IIT-CDIP dataset, which makes it particularly well-suited for classifying\ndocuments from the RVL-CDIP subset.\nIn [11], they already fined-tuned Donut on various downstream tasks, includ-\ning the classification of RVL-CDIP, which showed excellent results. However,\nthis was conducted on the complete RVl-CDIP dataset with 320K images in the\ntraining set, whereas we are investigating whether it also performs well when\nthe training data is severely limited.\n3.4 Multi-Modal Methods\nGPT-4-Vision Recently, GPT-4-Vision was introduced as a multi-modal ex-\ntension of GPT-4, which accepts a combination of text and/or image inputs from\nthe user [1] (with the same model version as indicated in Table 1).\nImage-based zero-shot prompting.The first sentence of the system prompts listed\nin Figure 2 is changed to “Your task is to classify a document.” without men-\ntioning the OCR text. Only the document image is provided as input.\nRevisiting Document Image Classification Using LLMs 7\nBimodal zero-shot prompting.Thefirstsentenceofthesystempromptsischanged\nto “Your task is to classify a document based on its OCR text and scanned im-\nage, which are both provided by the user.” Afterwards, both the OCR text and\nthe document image are provided as input.\n4 Experimental Evaluation\nIn this section, we first describe the experimental setup of the benchmark, fol-\nlowed by results obtained for prompting, embedding, and fine-tuning, respec-\ntively. At the end, all results are summarized and put into context with other\nresults from the current state of the art.\n4.1 Setup\nTraining. For zero-shot and one-shot prompting, we consider zero samples and\n16 samples (one per class), respectively. For few-shot fine-tuning, we investigate\nan increasing number of 160 samples (ten per class), 800 samples (50 per class),\nand 1600 samples (100 per class), which are randomly chosen from the original\nRVL-CDIP training set. Note that the smaller training sets are included in the\nlarger ones.\nValidation. For optimizing hyperparameters, we consider a validation set of\nan additional 160 samples (ten per class), which are randomly chosen from the\noriginal RVL-CDIP validation set.\nTesting. We define a test scenarioRVL-CDIP-160x5, which consists of five\nrandom selections (without overlap) of 160 samples (ten per class) from the\noriginal test set, for which we obtain a high-quality OCR. We report the mean\naccuracy and standard deviation over the five test sets. As demonstrated in the\nexperiments (see Section 4.4), this selection results in a representative subset,\nwhich reduces the cost of testing significantly when using the APIs of Mistral\nAI and OpenAI for their LLMs, as they are not freely available.\nAdditionally, the originalRVL-CDIP-40Ktest set, which contains 40000\nsamples, is also included in the evaluation, whenever possible. Having this large\ntest set available, increases the confidence in the perceived evaluation and serves\nas a reference to existing results from the literature.\nOCR. For the training sets, validation set, and the RVL-CDIP-160x5 test sets,\nwe extract the text from the images by using Amazon’s Textract. As running the\nOCR on the full RVL-CDIP-40K test set goes beyond the scope of this research,\nwe fall back to the original OCR texts from the IIT-CDIP collection [12]. The\noriginal OCR was performed with a 90s-era OCR engine, which unquestionable\nproduced a lower quality output. Nevertheless, we include it to judge how rep-\nresentative our randomly selected subsets are with respect to an established test\nset of considerable size.\n8 A. Scius-Bertrand and M. Jungo et al.\nModels setup. For zero-shot and one-shot prompting, we rely on the APIs of\nMistral AI and OpenAI respectively, specifying a temparature of0 to encourage\nprecise, non-creative answers. The LLM versions used are indicated in Table 1.\nFor KNN-based classification of OCR embeddings, we use the APIs of Jina\nAI, Mistral AI, and OpenAI to extract the embedding vectors. The embedding\nmodels considered are indicated in Table 2. The parameterk ∈ {1, 3, 5, 7, 9} and\nthe metric (Euclidean or cosine) are optimized with respect to the classification\naccuracy on the validation set.\nFine-tuning Donut is performed by using the official implementation4 and\nwith the suggested configuration of the CORD dataset [15]. In particular, the\nimage size is fixed to 1280 × 960 pixels, which provides a good compromise\nbetween readability for the human eye and computational effort for the GPU.\nMistral-7B[9] is used as the base model for all experiments for the LLM\nfine-tuning. As proposed in QLoRA [4], the base model is quantized into 4-bit\nweights and trainable LoRA adapters are added to all linear layers. We chose\nrank r = 8with alpha = 16based on preliminary experiments on the validation\nset. On the other hand, for RoBERTa, all parameters are fine-tuned, since the\nmodel is small enough to be trained fully in a reasonable time on the available\nhardware.\nThe generative fine-tuning adds a classification directive at the end of each\ndocument, specifically “### Classification:” followed by the class name to\nbe predicted. Solely the tokens after the classification directive contribute to the\nloss and therefore the weight updates. This is not an instruction tuning, meaning\nthat the model does not receive the instructions that were used for the one-shot\nprompting.\nRoBERTa and Mistral-7B are both implemented and trained with Hugging-\nFace’s transformers [20] library andbitsandbytes5 to support QLoRA.\n4.2 Prompting Results\nIn a preliminary experiment on the validation set, we optimized the LLM system\nprompt (see Figure 2): Written by a human (P1), enhanced by GPT-4 (P2),\nand enhanced twice by GPT-4 (P3). For a few-shot learning task with GPT-3.5\nusing 32 training samples (two per class) and evaluating on 48 validation samples\n(three per class), the system prompt P2 achieved the best accuracy (60.4%),\noutperforming P1 (58.3%) and P3 (56.2%). That is, an enhancement of the\nhuman prompt by GPT-4 was beneficial and P2 was selected for all subsequent\nexperiments.\nTable 3 shows the prompting results on the test sets of RVL-CDIP-160x5.\nFor zero-shot prompting, the mean classification accuracy ranges from 25.0%\n(Mixtral-8x7B) to 69.9% (GPT-4-Vision), highlighting a large variability among\nthe models.\n4 https://github.com/clovaai/donut\n5 https://github.com/TimDettmers/bitsandbytes\nRevisiting Document Image Classification Using LLMs 9\nTable 3: Zero-shot and one-shot prompting. Document classification re-\nsults on the RVL-CDIP-160x5 test sets, indicating the number of training sam-\nples (#Train), as well as the mean and standard deviation of the classification\naccuracy (%) and invalid answers (%) across the five subsets. The best results\nper training scenario are marked inbold.\n#Train Input Model Accuracy Invalid\n0 OCR Mistral-7B 45.4 ± 2.8 17.0 ± 2.9\n0 OCR Mixtral-8x7B 25.0 ± 2.9 56.2 ± 1.7\n0 OCR Mistral-Medium 54.6 ± 4.1 6.4 ± 3.1\n0 OCR Mistral-Large 54.4 ± 4.1 14.6 ± 1.6\n0 OCR GPT-3.5 36.9 ± 1.1 32.1 ± 1.5\n0 OCR GPT-4 61.8 ± 2.0 2.1 ± 1.2\n0 Image GPT-4-Vision 69.9 ± 2.0 0.5 ± 0.5\n0 OCR+Image GPT-4-Vision 69.4 ± 1.7 0.8 ± 0.7\n16 OCR Mistral-7B 47.1 ± 3.7 22.9 ± 2.8\n16 OCR Mixtral-8x7B 48.2 ± 5.9 13.4 ± 3.3\n16 OCR GPT-3.5 58.8 ± 2.1 4.6 ± 1.8\nGPT-4-Vision significantly outperforms GPT-4, highlighting the importance\nofthedocumentimageforclassification.BimodalpromptingwithbothOCRtext\nand image did not further improve the results when compared with image-only\nprompting. Note, however, that GPT-4-Vision is capable of performing OCR, at\nleast implicitly, when the input consists only of the document image.\nOne of the main limitations of the smaller models (Mistral-7B, Mixtral-\n8x7B, GPT-3.5) are invalid answers. Instead of only responding with a category\nname, as requested in the prompt, the models tend to produce longer responses,\ne.g. “The text provided appears to be a notice for a membership investment\nin the Florida Retail Political Action Committee”. We do not post-process the\nresponses, thus any deviation from valid category names is considered invalid.\nThe one-shot prompting results show that the mean accuracy can be im-\nproved for the smaller models (Mistral-7B, Mixtral-8x7B, GPT-3.5) by provid-\ning one example per class, in particular Mixtral-8x7B is improved from 25.0%\nto 48.2% while reducing the number of invalid answers from 56.2% to 13.4%.\nWe did not test one-shot prompting for the larger models due to the large\nnumber of tokens that are added to the prompt (16 OCR texts or images, re-\nspectively), which significantly increases the costs of using the commercial APIs.\n10 A. Scius-Bertrand and M. Jungo et al.\n4.3 Embedding Results\nThe results for KNN-based classification of OCR text embeddings are shown\nin Table 4. The mean accuracy achieved by Mistral-embed, OpenAI-small, and\nOpenAI-large are fairly similar and outperform most of the LLM prompting\nresults when 800 or more training samples are considered, demonstrating that\nembeddings are a promising strategy for a few-shot learning scenario.\nTable 4: KNN classification of OCR text embeddings. Document clas-\nsification results on the RVL-CDIP-160x5 test sets, indicating the number of\ntraining samples (#Train), as well as the mean and standard deviation of the\nclassification accuracy (%) across the five subsets. The best results per training\nscenario are marked inbold.\n#Train Input Embedding Accuracy\n160 OCR Jina-v2 41.9 ± 1.6\n160 OCR Mistral-embed 56.4 ± 3.0\n160 OCR OpenAI-small 53.9 ± 2.9\n160 OCR OpenAI-large 54.4 ± 1.2\n800 OCR Jina-v2 52.9 ± 4.0\n800 OCR Mistral-embed 63.5 ± 2.0\n800 OCR OpenAI-small 62.4 ± 3.2\n800 OCR OpenAI-large 64.8 ± 3.1\n1600 OCR Jina-v2 57.1 ± 4.5\n1600 OCR Mistral-embed 66.8 ± 2.7\n1600 OCR OpenAI-small 65.8 ± 3.4\n1600 OCR OpenAI-large 67.8 ± 3.8\nThere is one exception: Zero-shot prompting using GPT-4-Vision (69.9%\nmean accuracy) outperforms all embedding models tested, even when using1 600\ntraining samples.\nA visualization of the OpenAI-large embeddings using t-SNE dimensionality\nreduction is depicted in Figure 3, illustrating the capability of the embedding\nmodel to form class-wise clusters based on the OCR text.\n4.4 Fine-Tuning Results\nTable 5 reports the results for fine-tuning RoBERTa, Mistral-7B, and Donut\nin the different few-shot training scenarios. All fine-tuned models outperform\nLLM prompting and embedding strategies from the previous sections when800\nRevisiting Document Image Classification Using LLMs 11\nFig.3: Embedding of the1 600training samples using the OpenAI-large model,\nvisualized with t-SNE.\ntraining samples or more are used. The results also indicate that almost no\ninvalid responses are generated by the fine-tuned models.\nOne model stands out with an excellent performance for few-shot learning:\nGenerative fine-tuning of Mistral-7B. Even when providing only160 training\nsamples (10 samples per class), the LLM achieves a promising mean accuracy of\n72.5% on the RVL-CDIP-160x5 test sets, creating a noticeable gap to the next\nbest model (RoBERTa,59.8%). Classification fine-tuning is less successful when\nconsidering only 160 training samples but catches up for 800 and more, achieving\nthe overall best result of83.4% for 1 600training samples.\nBesides the aforementioned results, Table 5 also contains the results for the\noriginal RVL-CDIP-40K test set. Regarding the OCR-based models, the results\nshow a systematic decrease in accuracy for RVL-CDIP-40K when compared to\nRVL-CDIP-160x5. This is most likely due to the rather low OCR quality of the\noriginal dataset. In contrast, we are considering state-of-the-art OCR results for\nRVL-CDIP-160x5.\nThis hypothesis is further strengthened when taking the OCR-free (Donut)\nresultsintoaccount,whichexhibitamuchsmallerdifferencebetweenthetwotest\nscenarios. Concretely, for image-based classification with Donut the RVL-CDIP-\n40K test results are within one or two standard deviations of the RVL-CDIP-\n160x5 test results for all three training scenarios (160, 800,1 600). Therefore, we\nconclude that the five-fold selection of 160 test samples is, indeed, representative\nfor evaluating the RVL-CDIP classification challenge.\nEven though the Mistral-7B-Class achieved the single highest result on the\nRVL-CDIP-160x5 test sets, the generative model (Mistral-7B-Gen) is much more\nconsistent across multiple scenarios. Given that the generative model retains its\n12 A. Scius-Bertrand and M. Jungo et al.\nTable 5:Few-shot model fine-tuning.Document classification results on the\nRVL-CDIP-160x5 and RVL-CDIP-40K test sets, indicating the number of train-\ning samples (#Train) and the classification accuracy (%) across the five subsets.\nThe best results per training scenario are marked inbold.\nRVL-CDIP-160x5 RVL-CDIP-40K\n#Train Input Model Accuracy Invalid Accuracy Invalid\n160 OCR RoBERTa 59.8 ± 3.8 0.0 ± 0.0 50.2 0.0\n160 OCR Mistral-7B-Class 51.1 ± 4.0 0.0 ± 0.0 23.4 0.0\n160 OCR Mistral-7B-Gen 72.5 ± 3.9 0.4 ± 0.3 66.7 0.2\n160 Image Donut 42.8 ± 3.0 0.8 ± 0.7 44.2 1.2\n800 OCR RoBERTa 74.9 ± 4.9 0.0 ± 0.0 66.8 0.0\n800 OCR Mistral-7B-Class 78.3 ± 2.4 0.0 ± 0.0 58.7 0.0\n800 OCR Mistral-7B-Gen 79.5 ± 3.3 0.5 ± 0.5 72.8 0.2\n800 Image Donut 70.1 ± 2.6 0.1 ± 0.2 71.4 0.2\n1600 OCR RoBERTa 78.0 ± 2.0 0.0 ± 0.0 69.3 0.0\n1600 OCR Mistral-7B-Class 83.4 ± 4.3 0.0 ± 0.0 66.6 0.0\n1600 OCR Mistral-7B-Gen 82.4 ± 2.1 0.5 ± 0.3 74.7 0.3\n1600 Image Donut 73.8 ± 1.9 0.0 ± 0.0 76.4 0.1\nflexibility, it makes it even more appealing than using a model with a classifier\nhead.\n4.5 Results Summary\nA summary of the classification results is given in Table 6 with the best models\nfor each training scenario (zero-shot prompting or few-shot learning) and input\nmodality (OCR text and/or image). In the case of OCR text, we include both\nthe embedding approach and end-to-end models. The results achieved on the\nRVL-CDIP-160x5 test sets are put into context with the results reported in\nthe literature on RVL-CDIP-40K that use the full 320000 documents for the\ntraining.\nFor zero-shot prompting, the largest LLMs from OpenAI, in particular the\nmulti-modal GPT-4-Vision model, demonstrate an impressive generalization ca-\npability with a mean accuracy of69.9% on the test sets, considering the fact\nthat in this scenario the document classes can be changed on the fly, without\nthe need to annotate learning samples.\nRegarding the fine-tuning, the smaller Mistral-7B model stands out in its\ncapability to rapidly adapt to the classification task with only very few training\nsamples when using the generative fine-tuning based on LoRA. Fine-tuning with\nten samples per class leads to a mean accuracy of72.5%.\nRevisiting Document Image Classification Using LLMs 13\nTable 6:Summary of document classification results. The best-performing\napproach is listed for each zero-shot to few-shot training scenario evaluated on\ntheRVL-CDIP-160x5testsets.Theresultsareputintocontextwithfullytrained\nmodels evaluated on the RVL-CDIP-40K test set.\n#Train Input Model Accuracy\n0 OCR GPT-4 61.8 ± 2.0\n0 Image GPT-4-Vision 69.9 ± 2.0\n0 OCR+Image GPT-4-Vision 69.4 ± 1.7\n160 OCR Mistral-embed+KNN 56.4 ± 3.0\n160 OCR Mistral-7B-Gen 72.5 ± 3.9\n160 Image Donut 42.8 ± 3.0\n800 OCR OpenAI-large+KNN 64.8 ± 3.1\n800 OCR Mistral-7B-Gen 79.5 ± 3.3\n800 Image Donut 70.1 ± 2.6\n1600 OCR OpenAI-large+KNN 67.8 ± 3.8\n1600 OCR Mistral-7B-Class 83.4 ± 4.3\n1600 Image Donut 73.8 ± 1.9\n320000 OCR BERT [2] 85.0\n320000 Image Donut [11] 95.3\n320000 OCR+Image BERT+NasNet [2] 97.1\nWhen fine-tuning with 100 samples per class, which is still considered to be\na small amount of training data for the document classification task, the fine-\ntuned Mistral-7B model with a classifier head achieves the overall best mean\naccuracy of83.4%. This is a notable achievement when compared to the85.0%\naccuracy reported in [2] for a fully trained BERT model using320 000training\nsamples. Admittedly, Mistral-7B is an order of magnitude larger than BERT in\nterms of parameters, nevertheless, it indicates that very promising results can\nbe achieved with much less training data.\n5 Conclusion\nThe RVL-CDIP dataset was originally proposed as a document classification\nchallenge using hundreds of thousands of training samples. Under these condi-\ntions, the state of the art has gradually achieved near-perfect performance. By\nrevisiting the question of document classification under the perspective of con-\nsidering only very few training samples (or none at all), this paper investigates\nthe capacity of current document models to rapidly generalize to new tasks.\n14 A. Scius-Bertrand and M. Jungo et al.\nWe contribute a comprehensive set of benchmark results that explore the\nquestion with prompts, embeddings, and model fine-tuning using methods from\nthe current state of the art for image and text analysis. We demonstrate the fea-\nsibility of zero-shot and few-shot document classification using LLMs, achieving\nresults that, although promising between69.9% and 83.4% mean accuracy, leave\nsignificant room for improvement.\nAn important line of future research is related to document foundation mod-\nels.Thestrongestfew-shotfine-tuningresultsreportedinthispaperwereachieved\nwith large text-based models (Mistral-7B). However, the state of the art clearly\ndemonstrates that combining image and text leads to significantly better results\nfor fully trained models. Therefore, integrating more visual information into doc-\nument foundation models is expected to significantly improve few-shot document\nclassification. There is an increasing number of multi-modal LLMs [22] that may\nbe investigated in this context.\nOther lines of research include improvements of the prompts, e.g. by pro-\nviding additional semantics to the LLM that go beyond only the name of the\ncategory. Finally, it would be beneficial to explore different learning strategies\nfor unlabeled data, including self-training and unsupervised contrastive learning,\nto further improve the results of few-shot fine-tuning.\nReferences\n1. Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L.,\nAlmeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al.Gpt-4\ntechnical report. arXiv preprint arXiv:2303.08774(2023).\n2. Bakkali, S., Ming, Z., Coustaty, M., and Rusiñol, M.Visual and textual\ndeep feature fusion for document image classification. InProc. Int. Conf. on Com-\nputer Vision and Pattern Recognition Workshops (CVPRW)(2020), pp. 562–563.\n3. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal,\nP., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al.Language\nmodels are few-shot learners.Advances in neural information processing systems\n(NeurIPS) 33 (2020), 1877–1901.\n4. Dettmers, T., Pagnoni, A., Holtzman, A., and Zettlemoyer, L.Qlora:\nEfficient finetuning of quantized llms.Advances in Neural Information Processing\nSystems 36 (2024).\n5. Günther, M., Ong, J., Mohr, I., Abdessalem, A., Abel, T., Akram, M. K.,\nGuzman, S., Mastrapas, G., Sturua, S., Wang, B., et al.Jina embeddings\n2: 8192-token general-purpose text embeddings for long documents.arXiv preprint\narXiv:2310.19923 (2023).\n6. Harley, A. W., Ufkes, A., and Derpanis, K. G.Evaluation of deep convo-\nlutional nets for document image classification and retrieval. InProc. Int. Conf.\non Document Analysis and Recognition (ICDAR)(2015), pp. 991–995.\n7. Hu, E. J., Shen, Y., Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang,\nL., and Chen, W.Lora: Low-rank adaptation of large language models.arXiv\npreprint arXiv:2106.09685 (2021).\n8. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S.,\nCasas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L.,\net al. Mistral 7b. arXiv preprint arXiv:2310.06825(2023).\nRevisiting Document Image Classification Using LLMs 15\n9. Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S.,\nCasas, D. d. l., Bressand, F., Lengyel, G., Lample, G., Saulnier, L.,\net al. Mistral 7b. arXiv preprint arXiv:2310.06825(2023).\n10. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bam-\nford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F.,\nLengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux,\nM.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L.,\nGervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E.Mixtral\nof experts. arXiv preprint arXiv:2401.04088(2024).\n11. Kim, G., Hong, T., Yim, M., Nam, J., Park, J., Yim, J., Hwang, W.,\nYun, S., Han, D., and Park, S.Ocr-free document understanding transformer.\nIn Proc. European Conference on Computer Vision (ECCV)(2022), Springer,\npp. 498–517.\n12. Lewis, D., Agam, G., Argamon, S., Frieder, O., Grossman, D., and\nHeard, J. Building a test collection for complex document information process-\ning. In Proceedings of the 29th annual international ACM SIGIR conference on\nResearch and development in information retrieval(2006), pp. 665–666.\n13. Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis,\nM., Zettlemoyer, L., and Stoyanov, V.Roberta: A robustly optimized bert\npretraining approach.arXiv preprint arXiv:1907.11692(2019).\n14. Neelakantan, A., Xu, T., Puri, R., Radford, A., Han, J. M., Tworek,\nJ., Yuan, Q., Tezak, N., Kim, J. W., Hallacy, C., et al.Text and code\nembeddings by contrastive pre-training.arXiv preprint arXiv:2201.10005(2022).\n15. Park, S., Shin, S., Lee, B., Lee, J., Surh, J., Seo, M., and Lee, H.Cord: A\nconsolidated receipt dataset for post-ocr parsing. InDocument Intelligence Work-\nshop at Neural Information Processing Systems(2019).\n16. Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., et al.Im-\nproving language understanding by generative pre-training.OpenAI (2018).\n17. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I.,\net al. Language models are unsupervised multitask learners.OpenAI blog 1, 8\n(2019), 9.\n18. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M.,\nZhou, Y., Li, W., and Liu, P. J.Exploring the limits of transfer learning with\na unified text-to-text transformer. Journal of machine learning research 21, 140\n(2020), 1–67.\n19. Tensmeyer, C., and Martinez, T.Analysis of convolutional neural networks\nfor document image classification. InProc. Int. Conf. on Document Analysis and\nRecognition (ICDAR)(2017), pp. 388–393.\n20. Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cis-\ntac, P., Rault, T., Louf, R., Funtowicz, M., et al.Huggingface’stransform-\ners: State-of-the-art natural language processing.arXiv preprint arXiv:1910.03771\n(2019).\n21. Xu, Y., Xu, Y., Lv, T., Cui, L., Wei, F., Wang, G., Lu, Y., Florencio,\nD., Zhang, C., Che, W., Zhang, M., and Zhou, L.LayoutLMv2: Multi-\nmodal pre-training for visually-rich document understanding. InProc. Int. Joint\nConference on Natural Language Processing (IJCNLP)(2021), pp. 2579–2591.\n22. Zhang, D., Yu, Y., Dong, J., Li, C., Su, D., Chu, C., and Yu, D.MM-\nLLMs: recent advances in multimodal large language models. arXiv preprint\narXiv:2401.13601 (2024).",
  "topic": "Shot (pellet)",
  "concepts": [
    {
      "name": "Shot (pellet)",
      "score": 0.8446713089942932
    },
    {
      "name": "Computer science",
      "score": 0.8088410496711731
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.5555800199508667
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5481002926826477
    },
    {
      "name": "Natural language processing",
      "score": 0.5270100831985474
    },
    {
      "name": "Image (mathematics)",
      "score": 0.45511937141418457
    },
    {
      "name": "One shot",
      "score": 0.44740888476371765
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.3432080149650574
    },
    {
      "name": "Speech recognition",
      "score": 0.3273885250091553
    },
    {
      "name": "Algorithm",
      "score": 0.322506844997406
    },
    {
      "name": "Linguistics",
      "score": 0.18624114990234375
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I173439891",
      "name": "HES-SO University of Applied Sciences and Arts Western Switzerland",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I154338468",
      "name": "University of Fribourg",
      "country": "CH"
    }
  ],
  "cited_by": 3
}