{
  "title": "A Transformer Model for Retrosynthesis",
  "url": "https://openalex.org/W2972608805",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2735527448",
      "name": "Pavel Karpov",
      "affiliations": [
        "Helmholtz Zentrum München",
        "Klinikum Ingolstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2114590262",
      "name": "guillaume godin",
      "affiliations": [
        "Firmenich (Switzerland)"
      ]
    },
    {
      "id": "https://openalex.org/A141730740",
      "name": "Igor V. Tetko",
      "affiliations": [
        "Helmholtz Zentrum München",
        "Klinikum Ingolstadt"
      ]
    },
    {
      "id": "https://openalex.org/A2735527448",
      "name": "Pavel Karpov",
      "affiliations": [
        "Institute of Groundwater Ecology",
        "Helmholtz Zentrum München"
      ]
    },
    {
      "id": "https://openalex.org/A2114590262",
      "name": "guillaume godin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A141730740",
      "name": "Igor V. Tetko",
      "affiliations": [
        "Institute of Groundwater Ecology",
        "Helmholtz Zentrum München"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2724493970",
    "https://openalex.org/W2436108096",
    "https://openalex.org/W2910734083",
    "https://openalex.org/W2790808809",
    "https://openalex.org/W2799620402",
    "https://openalex.org/W2783658781",
    "https://openalex.org/W2791657723",
    "https://openalex.org/W2160592148",
    "https://openalex.org/W2900592234",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W6610423178",
    "https://openalex.org/W6602415781",
    "https://openalex.org/W1965570048",
    "https://openalex.org/W2064535969",
    "https://openalex.org/W2621742623",
    "https://openalex.org/W2610148085",
    "https://openalex.org/W2796108585",
    "https://openalex.org/W2551217916",
    "https://openalex.org/W2769423117",
    "https://openalex.org/W3210732293",
    "https://openalex.org/W2747592475",
    "https://openalex.org/W3104636952",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2951696358",
    "https://openalex.org/W2953298093",
    "https://openalex.org/W1557379335",
    "https://openalex.org/W2618625858",
    "https://openalex.org/W29374554",
    "https://openalex.org/W3098269892",
    "https://openalex.org/W2792287754",
    "https://openalex.org/W2900281422",
    "https://openalex.org/W2904147875",
    "https://openalex.org/W2064000318",
    "https://openalex.org/W2780675302"
  ],
  "abstract": null,
  "full_text": "A Transformer Model for Retrosynthesis\nPavel Karpov1,3(B) , Guillaume Godin2 , and Igor V. Tetko1,3\n1 Helmholtz Zentrum M¨unchen – Research Center\nfor Environmental Health (GmbH), Institute of Structural Biology,\nIngolst¨adter Landstraße 1, 85764 Neuherberg, Germany\npavel.karpov@helmholtz-muenchen.de\n2 Research and Development Division, Firmenich International SA,\nRoute des Jeunes 1, 1227 Les Acacias, Switzerland\nguillaume.godin@firmenich.com\n3 BigChem GmbH, Ingolst¨adter Landstraße 1, b. 60w, 85764 Neuherberg, Germany\nitetko@bigchem.de\nAbstract. We describe a Transformer model for a retrosynthetic reac-\ntion prediction task. The model is trained on 45 033 experimental reac-\ntion examples extracted from USA patents. It can successfully predict\nthe reactants set for 42.7% of cases on the external test set. During the\ntraining procedure, we applied diﬀerent learning rate schedules and snap-\nshot learning. These techniques can prevent overﬁtting and thus can be\na reason to get rid of internal validation dataset that is advantageous\nfor deep models with millions of parameters. We thoroughly investigated\ndiﬀerent approaches to train Transformer models and found that snap-\nshot learning with averaging weights on learning rates minima works\nbest. While decoding the model output probabilities there is a strong\ninﬂuence of the temperature that improves at T = 1 .3 the accuracy of\nmodels up to 1–2%.\nKeywords: Retrosynthesis prediction\n·\nComputer aided synthesis planning · Character-based models ·\nTransformer\n1 Introduction\nNew chemical compounds drive technological advances in material, agricul-\ntural, environmental, and medical sciences, thus, embracing all ﬁelds of scien-\ntiﬁc activities which have been bringing social and economic beneﬁts through-\nout human history. Design of chemicals with predeﬁned properties is an\narena of QSAR/QSPR (Quantitative Structure Activity/Property Relation-\nships) approaches aimed at ﬁnding correlations between molecular structures\nand their desired outcomes and then applying these models to optimise activ-\nity/property of compounds.\nThe advent of deep learning [3,5] gave a new impulse for virtual modeling and\nalso opened a venue for a promising set of generative methods based on Recurrent\nc⃝ The Author(s) 2019\nI. V. Tetko et al. (Eds.): ICANN 2019, LNCS 11731, pp. 817–830, 2019.\nhttps://doi.org/10.1007/978-3-030-30493-5\n_78\n818 P. Karpov et al.\nNeural Networks [10], Variational Autoencoders [13], and Generative Adversar-\nial Networks trained with reinforcement learning [ 14,23]. These techniques are\nchanging the course of QSAR studies from the observation to the invention: from\na virtual screening of available compounds to direct synthesis of new candidates.\nGenerative models can produce big sets of promising molecules and impaired\nwith SMILES-based QSAR methods [18] provide a strong foundation for creat-\ning highly optimized focussed libraries, but estimation of synthetic availability of\nthese compounds is an open question though several approaches based on frag-\nmentation [11] and machine learning [ 7] approaches have been developed. To\nsynthesize a molecule, one should have a plan of a multi-step synthesis and also\na set of available reactants. Finding an optimal combination of reactants, reac-\ntions, and conditions to obtain the compound with good yield, suﬃcient quality,\nand quantity is not a trivial task even for experts in organic chemistry. Recent\nadvances in the computer-aided synthesis planning are reviewed in [2,6,9].\nThe retrosynthetic analysis worked out by Corey [8] tries to account for all\nfactors while deriving the synthetic route. It iteratively decomposes the molecule\non simpler blocks till all of them become available either by purchase or by syn-\nthesis described in the literature. At each step, Fig.1, all possible disconnections\n(rules) with known reactions simplify the target molecule bringing to the scene\nless complex compounds. Some of them may be already available, while the oth-\ners undergo the next step of retrosynthesis decomposition. Due to the recursive\nnature of the procedure, it can deal with thousands of putative compounds so\ncomputational retrosynthetic approaches can greatly help chemists in ﬁnding\nthe best routes. Managing of the database of such rules is complicated and more\ncritical the models based on it are not ready to accommodate new reactions and\nwill always be outdated. Unfortunately, almost more than 60 years of developing\nrule-based systems ended with no remarkable success in synthesis planning pro-\ngrams [28]. Another approach to tackle the problem is to use so-called template-\nfree methods inspired by the success of machine-translation. They don’t require\nthe database of templates and rules due to an inherent possibility to derive this\ninformation during training directly from a database of organic reactions with\nclearly designated roles of reactants, products, reagents, and conditions.\nThe analogy between machine translation and retrosynthesis is evident: each\ntarget molecule has its predecessors from which it can be synthesized as every\nmeaningful sentence one can translate from source language to target one. If all\nparts of a reaction are written in SMILES notation, then our source and target\nsentence are composed of valid SMILES tokens as words. The main goal of the\nwork is to build a model which could for a given target molecule for our exam-\nple\n1 COC(=O)c1cccc(−c2nc3cccnc3[nH]2)c1 in Fig.1 correctly predict the set of\nreactants. Namely, it should predict Nc1cccnc1N.COC(=O)c1cccc(C(=O)O)c1\nin this case.\nNeural sequence-to-sequence (seq2seq) approach has been recently applied for\na direct reaction prediction task [26,27] with outstanding statistical parameters\nof ﬁnal models – 90.4% of accuracy on test set. Seq2seq modeling has been also\n1 This reaction is in the test set and it was correctly predicted by our model.\nA Transformer Model for Retrosynthesis 819\nFig. 1. An example of a retrosynthetic reaction: on the left side of the arrow the target\nmolecule is depicted, and on the right side the one possible set of reactants that can lead\nto the target is shown in common chemistry-like scheme and using SMILES notation.\nHere two successive amidation reactions result in cyclisation and aromatization.\ntested on retrosynthesis task [21], but due to the complex nature of retrosynthesis\nitself and diﬃculty in estimating the correct predictions of reactants2, accuracy\non the test set was moderate 37.4% but still comparable to rule-based systems\n35.4%. We questioned about the possibility of improvement models for one-\nstep retrosynthesis utilizing modern neural network architectures and training\ntechniques. Applying the Transformer Model [29], together with cyclical learning\nrate schedule [24], resulted in a model with accuracy 42.7%, that is >5% higher\ncompare to the baseline model [21].\nOur main contributions are:\n– We show that Transformer can be eﬃciently used for a retrosynthesis predic-\ntion task.\n– We show that for this particular task there is no advantage to use a validation\ndataset for early-stopping or other parameters optimization. We trained all\nthe parameters directly from the training dataset.\n– Applying weights averaging and snapshot learning helped to train the most\nprecise model for one-step retrosynthesis prediction. We averaged weights on\n5 successive cycles of learning rate schedule.\n– Increasing the temperature while performing a beam-search procedure\nimproves the accuracy up to 2%.\n2 Approach\n2.1 Dataset\nIn this study we used the same dataset of reactions as in [ 21]. This dataset\nwas ﬁltered from the USPTO database [ 22] originally derived from the USA\npatents and contains 50 000 reactions classiﬁed into 10 reaction types [25]. The\nauthors [21] further preprocessed the database by splitting multiple products\n2 A target molecule usually can be synthesized with diﬀerent reactions starting from\ndiﬀerent sets of reactants. The predictions of the model may be correct from organic\nchemist point of view but diﬀer from the reactant set in ground truth. This may\nlead to underestimation of eﬀectiveness of models.\n820 P. Karpov et al.\nreactions into multiple single products reactions. The resulting dataset contains\n40 029, 5 004, and 5 004 reactions for training, validation, and testing respectively.\nInformation about the reaction type was discarded as we aimed at building a\ngeneral model using SMILES of products and reactants only.\n2.2 Model Input\nThe seq2seq models were developed to support machine translation where the\ninput is a sentence in one language, and the output is a sentence with approxi-\nmately the same meaning but in another language. String nature of data implies\nsome tokenization procedures similar to word2vec to be used for preprocessing\nthe input. Most of works in cheminformatics dealing with SMILES tokenize the\ninput with a regexp equal or similar to [26].\ntoken regex= \"( \\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\n\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9])\".\nThough such tokenization is more similar to way chemists think, it also has\nsome drawbacks that confuse network by putting forward low represented molec-\nular parts. For example, after applying this regexp to the database one can\nsee some not frequent moieties such as [C@@], [C@@H], [S@@], [C@], [C@H],\n[N@@+], [se], [C−], [Cl+3]. The thing in brackets according to SMILES spec-\niﬁcation can be quite a complex gathering not only the element’s name itself,\nbut also its isotopic value, stereochemistry conﬁguration, the formal charge, and\nthe number of hydrogens\n3. Strictly speaking, to do tokenization right one should\nalso parse the content of brackets just increasing the number of possible words\nin the vocabulary what eventually leads to the simplest tokenization only with\nletters. We tried diﬀerent schemes of tokenization in this work but did not see\nany improvements in using them over simple character-based method.\nOur ﬁnal vocabulary has length of 66 symbols4:\nchars = \" ^#%()+-.0123456789=@ABCDEFGHIKLMNOPRSTVXYZ[ \\\\]\nabcdefgilmnoprstuy$\"\nTo convert a token to a dense vector we used a trainable embedding5 of size\n64. It is well known that training neural networks in batches is more stable,\nfaster, and leads to more accurate models. To facilitate batch training we also\nused masks of input strings of shape (batch\nsize, max length) with elements equal\nto 1 for those positions where are valid SMILES symbols and 0 everywhere else.\n3 We do not want to exclude stereochemistry information from our model as well as\ncharges and explicit hydrogens that will lead to reducing of the dataset. Moreover,\nwork in generative models showed excellent abilities of models to close cycles, for\nexample, c1cc(COC)cccc1. If the model can capture such a long distance relation\nwhy should it be cracked on much simplier substrings enclosed by brackets?\n4 This vocabulary derived from the complete USPTO set and is a little bit wider than\nneeded for this study. But for future extending of the models it is better to ﬁx the\ninput shape to the biggest possible value.\n5 The encoder and the decoder share embeddings in this study.\nA Transformer Model for Retrosynthesis 821\n2.3 Transformer Model\nWe used a promising Transformer [ 29] model for this study which is a new\ngeneration of encoder-decoder neural networks family. The architecture is suited\nfor exploration of the internal representation of data by deriving questions (Q)\nthe data could be asked for, keys for its indexed knowledge ( K), and answers\nwritten as values (V ) corresponding to queries and keys. Technically these three\nentities are simply matrixes learned during the network training. Multiplying\nthem with the input (X) gives keys (k), questions (q), and values (v) relevant to\na current batch. Equipped with these calculated parameters of the input the self-\nattention layers transforms it pointing out to some encoding (decoding) parts\nbased on the attention vector.\nThe Transformer has wholly got rid of any recurrences or convolutional oper-\nations. To tackle distances between elements of a string a positional encoding\nmatrix was proposed with elements equal to the values of trigonometric functions\ndepending on the position in a string and also the position in the embedding\ndirection. Summed with learned embeddings positional encodings do their job\nlinking far located parts of the input together. The output of self-attention layers\nis then mixed with original data, layer-wise normalized, and passed position-wise\nthrough a couple of ordinary dense layers to go further either in next level of\nself-attention layers or to a decoder as an information-rich vector representing\nthe input. The decoder part of Transformer resembles the encoder but has an\nadditional self-attention layer which corresponds to encoder’s output.\nTransformer model shows the state-of-the-art results in machine translation\nand reaction prediction outcomes [27]. The latter work showed that training the\nTransformer on large and noisy datasets results in a model that can outperform\nnot only other machine models but also well qualiﬁed and experienced organic\nchemists.\n2.4 Model Inference\nThe model estimates the probability of the next symbol over the model’s vocabu-\nlary given all previous symbols in the string. Technically, the Transformer model\nﬁrst calculates logits, z\ni, and then transforms them to probabilities.\nzi = Transformer({x1,x2,x3, ..., xL},{y1,y2,y3, ..., yi−1}) (1)\nHere xi is the input of the models at i position; L – the length of the input\nstring; yi is the decoded output of the model up to position ( i − 1); and zi –\nlogits that are to be converted to probabilities:\nqi = exp(zi/T)∑ V\nj=0 exp(zj /T)\n(2)\nwhere V is the size of the vocabulary (66 in this work) and T stands for the\ntemperature6 usually assigned to 1.0 in standard softmax layers. With higherT\n6 Similar to formula of Boltzmann (Gibbs) distribution used in statistical mechanics.\n822 P. Karpov et al.\nthe landscape of the probability distribution becomes more smooth. During the\ntraining the model adapts its weights to better predict qi,s o yi = qi.\nDuring the inference however we have several possibilities how to convertqi\ninto yi, namely greedy and beam search. The ﬁrst one picks up a symbol with\nmaximum probability whereas the second one at each step holds top − K (K\n= beam’s size) suggestions of the model and summarises the overall likelihood\nfor each of K ﬁnal decodings. The beam search allows better inference and the\nprobability landscape exploration compared to the greedy search because at a\nparticular step of decoding it may choose a symbol with less than maximum\nprobability, but the total likelihood of the result can be higher due to more\nsigniﬁcant probabilities on the next steps.\n2.5 Training Heuristics\nTraining a Transformer model is a challenge, and several heuristics have been\nproposed [24], some of them were used in this study:\nUsing as Bigger Batch Size as Possible. Due to our hardware limitations we\ncould not set the batch size more then 64\n7;\nIncreasing the learning rate at the beginning of training up to warmup steps 8.\nThe authors of the original Transformer paper [29] used 4 000 steps for warming.\nThe Transformer model for reaction prediction task from [27] used 8 000 steps.\nWe analysed diﬀerent values for warmup and eventually found that 16 000 works\nwell with our model.\nApplying Cyclic Learning Rate Schedules. This tips can generally improve any\nmodel [17] through better loss landscape exploration with bigger learning rates\nafter the optimiser fell down to some local minima. For this study we used the\nfollowing scheme for learning rate calculation depending on the step:\nu(step)=\n{\nwarmup +( step mod cycle), if step ≥ cycle\nstep, otherwise\nwhere cycle stands for the number of steps while the learning rate is decreasing\nbefore raising to the maximum again.\nλ(step)= factor ∗ min(1.0,u(step)/warmup)\nmax (u(step),warmup) (3)\nwhere factor is just a constant. Big values of factor introduce numerical insta-\nbility during training, so after several trials we set factor =2 0.0. The curve for\nlearning rate in this study is shown in Fig.2, plot (4, f).\n7 Our ﬁrst implementation of the model required a lot of memory to deal with masks\nof reactants and products. Though later we improved the code we still remained this\nsize for consistency of the results.\n8 In our implementation 1 step is equivalent to 1 batch. The number of reactions for\ntraining is 40 029 + 5 004, so one epoch is equal to 704 batches.\nA Transformer Model for Retrosynthesis 823\nAveraging weights during last steps (usually 10–20) of training or at minima of\nlearning rates in case of snapshop learning [ 16]. Also with cyclic learning rate\nschedules it is possible to average weights of those models that have minimum\nin losses just before increasing of the rate. Such approach leads to more stable\nand plain region in loss landscapes [17].\n3 Results\n3.1 Learning Details and Evaluation of Models for Retrosynthesis\nPrediction\nFor this study, we implemented The Transformer model in Tensorﬂow [1] library\nto support its integration in our in-house programs set ( https://github.com/\nbigchem/retrosynthesis). All values reported are averages for three repetitive\nruns. Preliminary modeling showed that the architecture with 3 layers and 8\nattention heads works well for the datasets, though we tried combinations of 2,\n3, 4, 5 layers with 6, 8, 10, 12 heads. So all calculations were performed with\nthese values ﬁxed. The number of learnable parameters of the model is 1 882 176,\nembedding layer common for product and reactants has size 64.\nFollowing the standard machine learning protocol, we trained our ﬁrst models\n(T1) using three datasets for training, validation, and external testing (8:1:1) as\nwas done in [21]. Learning curves for T1 are depicted in Fig.2, (c) and (d) for train-\ning and validation loss, respectively, (a) shows the original learning rate schedule\ndeveloped by the authors of the Transformer but with 16 000 warmup steps. On\nreaching cross-entropy loss about 0.1 on the validation dataset, it stagnates with-\nout noticeable ﬂuctuations as training loss steadily decreases. After warming up\nphase the learning rate begins fading and eventually after 1 000 epochs its value\nreaches 2.8∗10\n−5 inevitable causing to stop training because of too small updates.\nDuring the decoding procedure, we explored the inﬂuence of the tempera-\nture parameter on the ﬁnal quality of prediction and found that inferring at\nhigher temperatures gives better result then at T = 1. This observation simi-\nlarly repeated for all our models. Figure3 shows the inﬂuence of this parameter\non the reactants prediction of the part of the training set. Clearly, at T = 1 .3\nthe model reaches the maximum of chemically-based accuracy. This fact one can\nexplain that at higher temperatures the landscape of output probabilities of the\nmodel is softer letting the beam-search procedure to ﬁnd more suitable ways\nduring decoding. Of course, the temperature inﬂuences only relative distances\nbetween peaks, so it does not aﬀect the greedy search method.\nIf we applied the early stopping technique, the training of a model is stopped\naround 200 epoch\n9. Eﬀectiveness of such a model marked T11 in Table1 resulted\nin TOP-1 37.9% on the test set. If we chose the last one model obtained at 1 000\nepoch, then the model T12 gave us better value – 39.8%. In this case, we did not\nsee any need of the validation dataset and keeping in mind that our model has\n9 Though we trained our models for 1 000 epochs we also saved their weights after\neach epoch and for imitating early stopping technique selected those weights that\ncorrespond to minimum in validation loss function.\n824 P. Karpov et al.\nalmost 2 millions of parameters we decided to combine training and validation\nsets and train our next models on both data, e.g., without validation. The model\nT2 was trained on all data and with the same learning rate schedule as T1. The\nresults obtained when applying T2 to the test set are better than for T1 model\nnamely 41.8% vs. 39.8%, respectively.\nThen we trained our model with cyclic learning rate schedule, Eq. 3, Fig. 2\n(b) for better exploration of loss landscape. During training, we also saved the\ncharacter-based accuracy of the model, Fig.2, (f). This snapshot training regime\n[16] produces a set of diﬀerent weights at each minimum of learning rate. Aver-\naging them is to some extent equivalent to a consensus of models but within one\nmodel [17]. We tried diﬀerent averaging regimes for T3 and found that averaging\nﬁve last cycles gives better results.\nOur ﬁnal T3 model outperforms [ 21] by 5.3% with beam search and more\ncritical it is also eﬀective with greedy search 40.6%. The latter one is much faster\nand consequently more suitable for virtual screening campaigns.\nIt worth to notice that TOP-5 accuracy reaches almost 70%. That means\nthe model can correctly predict reactants but sometimes scoring is wrong and\nFig. 2. Summary of learning curves for the Transformer model: (a) original learning\nrate schedule with warmup; (b) cyclic learning rate with warmup; (c) cross-entropy loss\nfor training and (d) validation; (e) cross-entropy loss and (f) character-based accuracy\nfor training a model wit cyclic learning schedule.\nA Transformer Model for Retrosynthesis 825\nTOP-1 is much less. We tried to improve TOP-1 scoring with internal conﬁdence\nestimation.\nFig. 3. Dependence of the beam search on temperature. For better exploration, higher\ntemperatures are more useful. In this study we explored T = 1.3. Bigger values signif-\nicantly worse for Top-3, and approximately the same for Top-1 and Top-5. This curve\nwas derived from the training dataset.\nTable 1. Accuracy (%) of the models on test set when all reactants were correctly\npredicted.\nModel Greedy Top-1 Top-3 Top-5 Description\nSeq2Seq 37.4 52.4 57.0 Literature result from [21] based on\nSeq2Seq architecture\nT1 34.4 37.9 57.3 62.7 Transformer Model trained with\nvalidation control set (early stopping,\n˜200 epochs)\nT11 37.3 39.8 59.1 63.9 The same as T1, but without early\nstopping (1000 epochs)\nT2 39.3 41.8 61.3 67.2 Transformer Model trained on both\ntraining and validation sets for 1000\nepochs\nT3 40.6 42.7 63.9 69.8 Transformer Model trained with cyclic\nlearning rate schedule for 1000 epochs.\nAveraging cycles 6, 7, 8, 9, and 10\n826 P. Karpov et al.\n3.2 Internal Scoring\nThe beam search calculates the sum of negative logarithms of probabilities of\nselecting a token at a particular step, and thus, this value can be a measure of\ninternal conﬁdence. To check this hypothesis, we selected T3-2 model and esti-\nmated its internal performance to distinguish between correct and invalid predic-\ntions. The parameters of the classiﬁer were: AUC = 0 .77, optimal threshold =\n0.00678. Then we validated the model with an additional condition: if the score\nis less than optimal threshold we selected the answer, otherwise we went to the\nnext candidate in the possible reactant sets returned by the beam search. The\nresults were even worse than without thresholds, 28.45 vs. 42.42. A possible\nexplanation is that the estimation does not deal with organic chemistry. The\nmodel tries to derive some character-based scoring relying only on tokens in a\nstring and increasing this value does not inﬂuence the quality of prognosis. The\nsame eﬀect we saw during training when the character accuracy is 98% whereas\nchemistry-based metric is much lower.\nFig. 4. Internal classiﬁcation performance.\nEstimation of optimal thresholds on training sets almost always a bad idea\ndue to the biasing of a model to its source data. The correct way is to use vali-\ndation dataset instead. We built the classiﬁer for the T1-2 with characteristics:\nAUC = 0.65, optimal threshold 0.00396, and applied it for testing the model.\nA Transformer Model for Retrosynthesis 827\nThe results were again worse, 14.1% vs. 40.85%. There are no signiﬁcant diﬀer-\nencies of accuracies when using unnormalized or normalized on the length of the\nreactants string scores. Figure 4 shows ROC curves for T1-2 and T3-2 models\nderived at T = 1.3. Evidently one cannot use this estimation to improve TOP-1\nscoring.\n4 Discussion\nMuch attention paid in the scientiﬁc literature for rule-based approaches [4,28].\nSince the authors of [20] have described the algorithm of automatic rule extrac-\ntion from mapped reaction database several implementations of the procedure\nappeared, and then widely accepted by researchers. However, it should be noticed\nthat, ﬁrst, there is no algorithm to make atom-mapping [ 2]i fi ti sa b s e n t( t h e\ntypical situation with laboratory notebooks (ELN) for example). Second, all\navailable information on synthesis usually contains only positive reactions, so\nall binary classiﬁcation accuracies are inevitable overestimated because of artiﬁ-\ncial negative sets exploited in studies. Finally, the absence of commonly accepted\ndataset for testing makes the results of diﬀerent groups practically disparate and\nbiased to those problems the authors tried to solve. The authors of [ 4] selected\n40 molecules from DrugBank database to test their multiscale models, whereas\n[21] used database specially prepared for classiﬁcation [25].\nOur model can correctly predict reactant set in TOP-5 with accuracy 69.8%.\nInternal conﬁdence estimation cannot guarantee a correct ordering of reactants\nsets, so diﬀerent scoring methods should be developed. One of the promising\nways is to use a forward reaction prediction model to estimate whether it is\npossible to assemble a target molecule from reactants proposed. The scoring\nmodel should have excellent characteristics and probably it is possible to apply\nthe same cycling learning rate and snapshot averaging to build it.\nFirst work on applying reinforcement learning for the whole retrosynthetic\npath [28] showed superior performance compared to the rule-based methods\ndeveloped before. More important if can deal with several steps of synthesis.\nBut the policy learned during the training again used extracted rules limiting\nthe method. Thus, the development of models for direct estimation of reactants\nis still of prime importance. During the encoding process, the Transformer ﬁnds\nan internal representation of a reaction which can be useful for multicomponent\nQSAR [19] for predicting rate constants [ 12] and yields of reactions. Embed-\nding such systems in policy networks within reinforcement learning paradigm\ncan bring forward an entirely data-driven approach to solve challenging organic\nsynthesis problems.\n5 Conclusions\nWe have described a Transformer model for retrosynthesis one-step prediction\ntask. Our ﬁnal model trained with cyclic learning rate schedule and its weights\n828 P. Karpov et al.\nwere averaged during last ﬁve loss minimum. The model outperforms the pre-\nvious published retrosynthetic character-based model by 5.3%. It also does not\nrequire the extraction of speciﬁc rules, atom mappings, and reaction types in\nreaction dataset. We believe it is possible to improve the model further applying\nknowledge distillation method [15] for example. The current model can be used\nas a building block for reinforcement learning aimed at solving complex organic\nproblems.\nAll source code and also models built are available online via github\nhttps://github.com/bigchem/retrosynthesis\nAcknowledgments. This study has been partially supported by ERA-CVD (https://\nera-cvd.eu) “Cardio-Oncology” project, BMBF 01KL1710 and by European Union’s\nHorizon 2020 research and innovation program under the Marie Sk/suppresslodowska-Curie grant\nagreement No. 676434, “Big Data in Chemistry” (“BIGCHEM”, http://bigchem.eu).\nThe authors thank NVIDIA Corporation for donating Quadro P6000 and Titan Xp\nand V graphics cards for this research.\nReferences\n1. Abadi, M., et al.: TensorFlow: large-scale machine learning on heterogeneous sys-\ntems (2015). https://www.tensorﬂow.org/\n2. Baskin, I.I., Madzhidov, T.I., Antipin, I.S., Varnek, A.A.: Artiﬁcial intelligence in\nsynthetic chemistry: achievements and prospects. Russ. Chem. Rev.86(11), 1127–\n1156 (2017). https://doi.org/10.1070/RCR4746\n3. Baskin, I.I., Winkler, D., Tetko, I.V.: A renaissance of neural networks in drug\ndiscovery. Expert Opin. Drug Discov. 11(8), 785–795 (2016). https://doi.org/10.\n1080/17460441.2016.1201262\n4. Baylon, J.L., Cilfone, N.A., Gulcher, J.R., Chittenden, T.W.: Enhancing retrosyn-\nthetic reaction prediction with deep learning using multiscale reaction classiﬁca-\ntion. J. Chem. Inf. Model. 59(2), 673–688 (2019). https://doi.org/10.1021/acs.\njcim.8b00801\n5. Chen, H., Engkvist, O., Wang, Y., Olivecrona, M., Blaschke, T.: The rise of deep\nlearning in drug discovery. Drug Discov. Today23(6), 1241–1250 (2018). https://\ndoi.org/10.1016/j.drudis.2018.01.039\n6. Coley, C.W., Green, W.H., Jensen, K.F.: Machine learning in computer-aided syn-\nthesis planning. Acc. Chem. Res. 51(5), 1281–1289 (2018). https://doi.org/10.\n1021/acs.accounts.8b00087\n7. Coley, C.W., Rogers, L., Green, W.H., Jensen, K.F.: SCScore: synthetic complex-\nity learned from a reaction corpus. J. Chem. Inf. Model. 58(2), 252–261 (2018).\nhttps://doi.org/10.1021/acs.jcim.7b00622\n8. Corey, E.J., Cheng, X.M.: The Logic of Chemical Synthesis. Wiley, Hoboken (1995)\n9. Engkvist, O., et al.: Computational prediction of chemical reactions: current status\nand outlook. Drug Discov. Today 23(6), 1203–1218 (2018). https://doi.org/10.\n1016/j.drudis.2018.02.014\n10. Ertl, P., Lewis, R., Martin, E., Polyakov, V.: In silico generation of novel, drug-like\nchemical matter using the LSTM neural network. arXiv (2017). arXiv:1712.07449\n11. Ertl, P., Schuﬀenhauer, A.: Estimation of synthetic accessibility score of drug-like\nmolecules based on molecular complexity and fragment contributions. J. Chemin-\nform. 1(1), 8 (2009). https://doi.org/10.1186/1758-2946-1-8\nA Transformer Model for Retrosynthesis 829\n12. Gimadiev, T., et al.: Bimolecular nucleophilic substitution reactions: predictive\nmodels for rate constants and molecular reaction pairs analysis. Mol. Inform. 37,\n1800104 (2018). https://doi.org/10.1002/minf.201800104\n13. G´omez-Bombarelli, R., et al.: Automatic chemical design using a data-driven con-\ntinuous representation of molecules. ACS Cent. Sci.4(2), 268–276 (2018). https://\ndoi.org/10.1021/acscentsci.7b00572\n14. Guimaraes, G.L., Sanchez-Lengeling, B., Outeiral, C., Farias, P.L.C., Aspuru-\nGuzik, A.: Objective-reinforced generative adversarial networks (ORGAN) for\nsequence generation models. arXiv (2017). arXiv:1705.10843\n15. Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network.\narXiv (2015). arXiv:1503.02531\n16. Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J.E., Weinberger, K.Q.: Snapshot\nensembles: train 1, get M for free. arXiv (2017). arXiv:1704.00109\n17. Izmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D., Wilson, A.G.: Aver-\naging weights leads to wider optima and better generalization. arXiv (2018).\narXiv:1803.05407\n18. Kimber, T.B., Engelke, S., Tetko, I.V., Bruno, E., Godin, G.: Synergy eﬀect\nbetween convolutional neural networks and the multiplicity of SMILES for improve-\nment of molecular prediction. arXiv (2018). arXiv:1812.04439\n19. Kravtsov, A.A., Karpov, P.V., Baskin, I.I., Palyulin, V.A., Zeﬁrov, N.S.: Prediction\nof rate constants of SN2 reactions by the multicomponent QSPR method. Dokl.\nChem. 440(2), 299–301 (2011). https://doi.org/10.1134/S0012500811100107\n20. Law, J., et al.: Route designer: a retrosynthetic analysis tool utilizing auto-\nmated retrosynthetic rule generation. J. Chem. Inf. Model.49(3), 593–602 (2009).\nhttps://doi.org/10.1021/ci800228y\n21. Liu, B., et al.: Retrosynthetic reaction prediction using neural sequence-to-\nsequence models. ACS Cent. Sci. 3(10), 1103–1113 (2017). https://doi.org/10.\n1021/acscentsci.7b00303\n22. Lowe, D.M.: Extraction of chemical structures and reactions from the litera-\nture. Ph.D. thesis, Pembroke College (2012). https://www.repository.cam.ac.uk/\nhandle/1810/244727\n23. Olivecrona, M., Blaschke, T., hongming Chen, O.E.: Molecular de-novo design\nthrough deep reinforcement learning. J Cheminform. 9(48), 1758–2946 (2017).\nhttps://doi.org/10.1186/s13321-017-0235-x\n24. Popel, M., Bojar, O.: Training tips for the transformer model. arXiv (2018).\nhttps://doi.org/10.2478/pralin-2018-0002\n25. Schneider, N., Stieﬂ, N., Landrum, G.A.: What’s what: the (nearly) deﬁnitive\nguide to reaction role assignment. J. Chem. Inf. Model.56(12), 2336–2346 (2016).\nhttps://doi.org/10.1021/acs.jcim.6b00564\n26. Schwaller, P., Gaudin, T., Lanyi, D., Bekas, C., Laino, T.: Found in translation:\npredicting outcomes of complex organic chemistry reactions using neural sequence-\nto-sequence models. arXiv (2018). arXiv:1711.04810\n27. Schwaller, P., Laino, T., Gaudin, T., Bolgar, P., Bekas, C., Lee, A.A.: Molecular\ntransformer for chemical reaction prediction and uncertainty estimation. arXiv\n(2018). arXiv:1811.02633\n28. Segler, M.H., Preuss, M., Waller, M.P.: Planning chemical synthesis with deep\nneural networks and symbolic AI. Nature 555, 604–610 (2018). https://doi.org/\n10.1038/nature25978\n29. Vaswani, A., et al.: Attention is all you need. arXiv (2017). arXiv:1706.03762\n830 P. Karpov et al.\nOpen Access This chapter is licensed under the terms of the Creative Commons\nAttribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/),\nwhich permits use, sharing, adaptation, distribution and reproduction in any medium\nor format, as long as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license and indicate if changes were\nmade.\nThe images or other third party material in this chapter are included in the chapter’s\nCreative Commons license, unless indicated otherwise in a credit line to the material. If\nmaterial is not included in the chapter’s Creative Commons license and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use, you will\nneed to obtain permission directly from the copyright holder.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7711265683174133
    },
    {
      "name": "Retrosynthetic analysis",
      "score": 0.7147402763366699
    },
    {
      "name": "Transformer",
      "score": 0.6634698510169983
    },
    {
      "name": "Electrical engineering",
      "score": 0.22360944747924805
    },
    {
      "name": "Engineering",
      "score": 0.09456104040145874
    },
    {
      "name": "Voltage",
      "score": 0.052096813917160034
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Total synthesis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3018134672",
      "name": "Helmholtz Zentrum München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I2801043613",
      "name": "Firmenich (Switzerland)",
      "country": "CH"
    }
  ]
}