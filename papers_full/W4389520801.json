{
    "title": "Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models",
    "url": "https://openalex.org/W4389520801",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2889095814",
            "name": "Laura Cabello",
            "affiliations": [
                "University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A3014545733",
            "name": "Emanuele Bugliarello",
            "affiliations": [
                "University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A2093000834",
            "name": "Stephanie Brandl",
            "affiliations": [
                "University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A2257973904",
            "name": "Desmond Elliott",
            "affiliations": [
                "University of Copenhagen"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3190434222",
        "https://openalex.org/W2990057723",
        "https://openalex.org/W2962787423",
        "https://openalex.org/W4318718936",
        "https://openalex.org/W4287891037",
        "https://openalex.org/W2109657353",
        "https://openalex.org/W2740983644",
        "https://openalex.org/W2145315825",
        "https://openalex.org/W2926555354",
        "https://openalex.org/W3177189402",
        "https://openalex.org/W4205731523",
        "https://openalex.org/W2990751682",
        "https://openalex.org/W4304183812",
        "https://openalex.org/W3166727371",
        "https://openalex.org/W2950888501",
        "https://openalex.org/W4379959055",
        "https://openalex.org/W4226462293",
        "https://openalex.org/W4221145068",
        "https://openalex.org/W2963518342",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3198409578",
        "https://openalex.org/W3153332739",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2963530300",
        "https://openalex.org/W3201264086",
        "https://openalex.org/W4226429201",
        "https://openalex.org/W2972413484",
        "https://openalex.org/W3177141404",
        "https://openalex.org/W3184735396",
        "https://openalex.org/W2970231061",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W4287889962",
        "https://openalex.org/W2899136066",
        "https://openalex.org/W3130478189",
        "https://openalex.org/W4385564993",
        "https://openalex.org/W4281291869",
        "https://openalex.org/W3034410824",
        "https://openalex.org/W4287208346",
        "https://openalex.org/W4380364765",
        "https://openalex.org/W3202415077",
        "https://openalex.org/W4285294416",
        "https://openalex.org/W3201522485",
        "https://openalex.org/W1933349210",
        "https://openalex.org/W4226225589",
        "https://openalex.org/W36434594",
        "https://openalex.org/W2963909453",
        "https://openalex.org/W4288007632",
        "https://openalex.org/W4226182655",
        "https://openalex.org/W2972697496",
        "https://openalex.org/W1773149199",
        "https://openalex.org/W3132103448",
        "https://openalex.org/W4280532631",
        "https://openalex.org/W4287867774"
    ],
    "abstract": "Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we define gender bias as our case study. We quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. We investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. Overall, we find that bias amplification in pretraining and after fine-tuning are independent. We then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without significantly compromising task performance.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8465–8483\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEvaluating Bias and Fairness in Gender-Neutral\nPretrained Vision-and-Language Models\nLaura Cabello Emanuele Bugliarello Stephanie Brandl Desmond Elliott\nDepartment of Computer Science, University of Copenhagen, Denmark\n{lcp,emanuele,brandl,de}@di.ku.dk\nAbstract\nPretrained machine learning models are known\nto perpetuate and even amplify existing biases\nin data, which can result in unfair outcomes that\nultimately impact user experience. Therefore, it\nis crucial to understand the mechanisms behind\nthose prejudicial biases to ensure that model\nperformance does not result in discriminatory\nbehaviour toward certain groups or populations.\nIn this work, we define gender bias as our case\nstudy. We quantify bias amplification in pre-\ntraining and after fine-tuning on three families\nof vision-and-language models. We investigate\nthe connection, if any, between the two learn-\ning stages, and evaluate how bias amplification\nreflects on model performance. Overall, we\nfind that bias amplification in pretraining and\nafter fine-tuning are independent. We then ex-\namine the effect of continued pretraining on\ngender-neutral data, finding that this reduces\ngroup disparities, i.e., promotes fairness, on\nVQAv2 and retrieval tasks without significantly\ncompromising task performance.\n1 Introduction\nAs shown by Mitchell (1980) and Montañez et al.\n(2019), inductive biases are essential for learning\nalgorithms to outperform random guessing. These\ntask-specific biases allow algorithms to generalize\nbeyond training data but, necessarily, they should\nnot be conflated with prejudicial or unwanted bi-\nases. Unwanted bias, such as bias against de-\nmographic groups, can be found in many appli-\ncations, from computer vision systems to natural\nlanguage processing (NLP). Vision-and-language\n(V&L) models lie at the intersection of these areas,\nwhere one of the key challenges is deploying robust\nmodels to perform high-level reasoning based on\nthe multimodal context instead of exploiting biases\nin data (Zhao et al., 2017).\nMultiple studies (Lee et al., 2021; Hirota et al.,\n2022b; Zhou et al., 2022) have shown that V&L\nmodels leverage co-occurrences between objects\nPretraining StageFine-Tuning Stage\nExtrinsic Bias\nIntrinsic Bias\n Task Performance\nFairness\nM D \nM D \nGENDER-NEUTRAL\nPRETRAINING\n21\nN\nFigure 1: A V&L model pretrained on data D (MD)\nis further pretrained on gender-neutral multimodal data\nDN, resulting in a gender neutral V&L model ( MN\nD).\nBoth models can then be used in a two-phase analysis:\n1) bias amplification is measured on the intrinsic bias\nof pretrained models, and 2) bias amplification, task\nperformance and fairness are evaluated on the extrinsic\nperformance of fine-tuned models.\nand their context to make predictions, and thus are\nsusceptible to unwanted biases. However, these\nauthors do not explore the broad landscape of V&L\nmodels and focus on biases in common visual\ndatasets (Wang et al., 2022; Hirota et al., 2022a),\nonly on pretrained models (Zhou et al., 2022) or\nonly focus on one application, e.g., image caption-\ning (Hendricks et al., 2018; Hirota et al., 2022b) or\nsemantic segmentation (Lee et al., 2021).\nIn this work, we investigate to what extent the\nunwanted bias in a V&L model is caused by thepre-\ntraining data. To answer this question, we focus on\none important aspect of bias encoded in V&L mod-\nels, namely bias amplification. Bias amplification\noccurs when a model exacerbates unwanted biases\nfrom the training data and, unlike other forms of\nbias, it is not solely attributed to the data, yet it can\nvary greatly during training (Hall et al., 2022).\nWe explore bias amplification in two encoder-\nonly V&L models: LXMERT (Tan and Bansal,\n2019) and ALBEF (Li et al., 2021), and the\nencoder-decoder model BLIP (Li et al., 2022).\nSpecifically, we quantitatively and qualitatively\n8465\nanalyse the relationship between the bias encoded\nin pretrained models, and after fine-tuning on down-\nstream tasks including visual question answering,\nvisual reasoning and image–text retrieval.\nWhile bias can be studied with respect to any\nprotected attribute, the majority of NLP research\nhas focused on (binary) gender (Sun et al., 2019;\nStanczak and Augenstein, 2021; Shrestha and Das,\n2022). We also use gender bias as our case study\nbut different to previous work, we advocate for\nthe inclusion of gender-neutral terms (Dev et al.,\n2021) and consider three gender categories based\non visual appearance: male, female and gender-\nneutral (e.g., PERSON ). The use of both visual and\ngrammatical gender information across V&L tasks\nis needed for identifying the target of, for exam-\nple, a question. But the demographics of the sub-\nject should not solely influence the outcome of the\nmodel. Otherwise, the model may reinforce harm-\nful stereotypes resulting in negative consequences\nfor certain group identities (van Miltenburg, 2016).\nMotivated by this argument, we investigate the\neffect of shifting the projection of gender-marking\nto a gender-neutral space by continued pretraining\non gender-neutral multimodal data–a form of do-\nmain adaptation (Gururangan et al., 2020)–and how\nit reflects on task performance after fine-tuning.\nFigure 1 depicts an overview of our full workflow.\nContributions We examine whether bias ampli-\nfication measured on pretrained V&L models (in-\ntrinsic bias) relates to bias amplification measured\non downstream tasks (extrinsic bias). We show\nthat a biased pretrained model might not translate\ninto biased performance on a downstream task to a\nsimilar degree. Likewise, we measure model fair-\nness through group disparity and show that it is\nnot unequivocally related to bias in a model. Fur-\nthermore, we empirically present a simple, viable\napproach to promote fairness in V&L models: per-\nforming an extra epoch of pretraining on unbiased\n(gender-neutral) data reduces fine-tuning variance\nand group disparity on VQAv2 and retrieval tasks\non the majority of models studied, without signifi-\ncantly compromising task performance.\nWe make our code publicly available to ensure\nreproducibility and foster future research.1\n1http://github.com/coastalcph/\ngender-neutral-vl\n2 Related Work\nBias in language In general, bias can be defined\nas “undue prejudice” (Crawford, 2017). Studies tar-\ngeting language models (Kurita et al., 2019; Zhao\net al., 2019) have shown that biases encoded in pre-\ntrained models (intrinsic bias) can be transferred\nto downstream applications ( extrinsic bias), but\nthe relationship between these biases is unclear.2\nThere are several studies (Goldfarb-Tarrant et al.,\n2021; Delobelle et al., 2021; Kaneko et al., 2022;\nCao et al., 2022; Orgad et al., 2022), showing that\nintrinsic bias in language models does not consis-\ntently correlate with bias measured extrinsically\non a downstream task or, similarly, with empirical\nfairness (Shen et al., 2022; Cabello et al., 2023).\nContrarily, Jin et al. (2021) observed that the effects\nof intrinsic bias mitigation are indeed transferable\nin fine-tuning language models. To the best of our\nknowledge, we are the first to investigate if the\nsame holds for V&L models.\nBias in vision & language Prior research ob-\nserved the presence of gender disparities in visual\ndatasets like COCO (Bhargava and Forsyth, 2019;\nZhao et al., 2021; Tang et al., 2021) and Flickr30k\n(van Miltenburg, 2016). Recent studies also re-\nvealed the presence of unwanted correlations in\nV&L models. Prejudicial biases found in V&L\nmodels are not only attributed to one domain, i.e.,\nvision or language, but they are compound (Wang\net al., 2019), and this should be studied together.\nSrinivasan and Bisk (2021); Hirota et al. (2022b)\nand Zhou et al. (2022) show that different model\narchitectures exhibit gender biases, often preferring\nto reinforce a stereotype over faithfully describing\nthe visual scene. Bianchi et al. (2023) show the\npresence of stereotypes in image generation mod-\nels and discuss the challenges of the compounding\nnature of language–vision biases. Another line of\nwork addresses visual contextual bias (Choi et al.,\n2012; Zhu et al., 2018; Singh et al., 2020) and study\na common failure of recognition models: an object\nfails to be recognized without its co-occurring con-\ntext. So far, little work has investigated bias ampli-\nfication in pretrained V&L models. Our study is\namong the first to cast some light on the gender bias\nencoded in pretrained V&L models and evaluate\n2As first suggested by Goldfarb-Tarrant et al. (2021), we\ncan broadly categorize bias into intrinsic and extrinsic. There-\nfore, intrinsic metrics are applied directly to word representa-\ntions and relate bias to the geometry of the embedding space,\nwhereas extrinsic metrics evaluate bias in downstream tasks.\n8466\nhow it translates to downstream performance.\nGender-neutral language Zhao et al. (2019) ex-\namine the effect of learning gender-neutral embed-\ndings during training of static word embeddings\nlike GloVe (Pennington et al., 2014). Sun et al.\n(2021) and Vanmassenhove et al. (2021) present\nrule-based and neural rewriting approaches to gen-\nerate gender-neutral alternatives in English texts.\nBrandl et al. (2022) find that upstream perplexity\nsubstantially increases and downstream task perfor-\nmance severely drops for some tasks when gender-\nneutral language is used in English, Danish and\nSwedish. Amend et al. (2021) show that the sub-\nstitution of gendered for gender-neutral terms on\nimage captioning models poses a viable approach\nfor reducing gender bias. In our work, we go one\nstep beyond and investigate the effect of contin-\nued pretraining V&L models on in-domain data\nwhere gendered terms have been replaced by their\ngender-neutral counterparts (e.g., sister →sibling).\n3 Problem Formulation\nWe characterize the gender bias encoded in V&L\nmodels in a two-phase analysis:\ni) Intrinsic bias: First, we investigate the bias\nencoded after the V&L pretraining phase.\nii) Extrinsic bias and task performance: Second,\nwe fine-tune the models on common down-\nstream tasks to further investigate how bias\naffects model performance.\nThese investigations will be performed using a\nset of original, pretrained models MD, and models\nthat have been further pretrained on gender-neutral\ndata MN\nD in order to mitigate any biases learned\nduring pretraining (§4.4). We hypothesize that this\nbias mitigation technique will decrease both intrin-\nsic and extrinsic biases encoded in the models.\nData Our analysis relies on data where the gen-\nder of the main actor of the image is known. This\nis, to some degree, annotated in the crowdsourced\ntext, e.g., image captions or questions.3 Following\nZhao et al. (2017) and Hendricks et al. (2018), im-\nages are labelled as ‘Male’ if the majority of its\n3Zhao et al. (2021) annotated samples from the COCO\ndataset (Lin et al., 2014) with the perceived attributes (gen-\nder and skin-tone) of the people in the images. However,\ntheir gender labels agree on 66.3% of the images compared\nto caption-derived annotations. To be consistent across all\ndatasets used in our project, we will not use their human-\ncollected annotations for analysing gender bias on COCO.\ncaptions include a word from a set of male-related\ntokens (e.g., BOY), and no caption includes a word\nfrom the set of female-related tokens (e.g., GIRL );\nand vice-versa for ‘Female’. Images are labelled\nas ‘Neutral’ if most of the subjects are listed as\ngender-neutral (e.g., PERSON ), or if there is no ma-\njority gender mention in the texts. Finally, images\nare discarded from the analysis when the text men-\ntions both male and female entities, or there are\nno people mentioned. This process can be applied\nto both pretraining data and downstream task data.\nSee Appendix A for the complete word list.\n4 Measuring Bias in V&L Models\n4.1 Intrinsic Bias\nWhen we measure the intrinsic bias of a model, we\nare interested in whether there are systematic dif-\nferences in how phrases referring to demographic\ngroups are encoded (Beukeboom et al., 2014). We\ncan measure the intrinsic bias using the model’s\nlanguage modelling task, where the tokens related\nto grammatical gender are masked.4\nLet MD be a V&L model pretrained on cor-\npora D. The masked words related to grammat-\nical gender are categorised on N = 3 disjoint de-\nmographic groups A = {Male, Female, Neutral}\nbased on reported visual appearance in the image.\nThe gender associated with an image is consid-\nered as the ground truth (see previous section for\nmore details). Let gi for i ∈[1, N] be the cate-\ngorical random variable corresponding to the pres-\nence of the group i. We investigate the gender–\ncontext distribution: the co-occurrence between\nattributes Ai = {a1, . . . , a|Ai|}, e.g., gender terms,\nfor a demographic group gi, and contextual words\nT = {t1, . . . , tT }, e.g., objects that appear in a\ngiven text. This results in a co-occurrence ma-\ntrix Cgi\na,t that captures how often pairs of attribute–\ncontext words occur in a defined context S, e.g., an\nimage caption in a corpus C. Formally, for every\ndemographic group gi, over the Ai attributes and\nT objects, and all possible contexts in corpus C\nCgi\na,t =\n∑\nS∈C\n|Ai|∑\nj=1\n|T|∑\nk=1\nS(aj, tk) with i ∈[1, N],\n(1)\nwhere S(aj, tk) = 1 if the attribute and object\nco-occur, zero otherwise. Based on Cgi\na,t, standard\n4We define gender correlations as our case study of rep-\nresentational bias, but note that our methodology can be ex-\ntended to analyse bias with regard to any protected attribute(s).\n8467\nstatistical metrics like precision, recall and F1 can\nbe computed. In addition, we will quantify the\nbias amplification in a given model MD to better\nunderstand the degree of bias exacerbated by the\nmodel. We use the metric presented by Wang and\nRussakovsky (2021), which is described in more\ndetail in the next section.\n4.2 Bias Amplification\nWe use the BiasAmp metric introduced by Wang\nand Russakovsky (2021), as it accounts for varying\nbase rates of group membership and naturally de-\ncouples the direction of bias amplification: While\nBiasAmpT→A measures the bias amplification due\nto the task influencing the protected attribute pre-\ndiction,5 BiasAmpA→T measures the bias ampli-\nfication due to the protected attribute influencing\nthe task prediction. We give a concise treatise of\nBiasAmpA→T here, and refer to Wang and Rus-\nsakovsky (2021) for further details.\nIn our setup, the set of attributes a ∈A is given\nby A = {Male, Female, Neutral}, and the set of\ntasks (or objects) t ∈ T are the most frequent\nnouns co-occurring with gendered terms in the\ntraining sets (see Appendix A for details). De-\nnote by P(Tt = 1) the probability that an example\nin the dataset belongs to class t. And, similarly,\nP( ˆTt = 1) the probability that an example in the\ndataset is labelled as class t by the model. Wang\nand Russakovsky (2021) introduce two terms to dis-\nambiguate the direction of bias amplification. The\nfirst term, ∆at, quantifies the difference between\nthe bias in the training data and the bias in model\npredictions.\nThe second term, yat, identifies the direction of\ncorrelation of Aa with Tt; that is, yat alters the sign\nof the ∆at to correct for the fact that the bias can\nhave two directions. Thereby,\nBiasAmpA→T = 1\n|A||T|\n∑\na∈A\nt∈T\nyat∆at −(1 −yat)∆at (2)\nBiasAmpA→T will be positive if the model pre-\ndictions amplify the prevalence of a class label\nt ∈T between groups a ∈A in the dataset. For\ninstance, bias is amplified if Aa = MALE im-\nages are more likely to appear in the presence of a\n5We do not consider gender prediction as a task per se, as\ngender –or any other sensitive attribute– prediction entangles\na complex categorization and a moral debate (Keyes, 2018;\nLarson, 2017). Instead, we use a MLM task as proxy and ask\nthe model to predict the subject of a sentence given its context.\nTt = SKATEBOARD in the model predictions, com-\npared to the prior distribution from the dataset. In\ncontrast, a negative value indicates that model pre-\ndictions diminish the bias present in the dataset. A\nvalue of 0 implies that the model does not amplify\nthe bias present in the dataset. Note that this does\nnot imply that the model predictions are unbiased.\n4.3 Extrinsic Bias & Fairness\nThe second phase of our analysis measures extrin-\nsic bias amplification: downstream performance\nand fairness (group disparity). A given model is\nfine-tuned on downstream tasks that require dif-\nferent reasoning skills based on the image context.\nWe evaluate model performance with respect to the\nthree demographic groups defined in A and com-\npare results in search of the more equitable system.\n4.4 Gender-neutral Domain Adaptation\nMotivated by the fact that models are known to\nacquire unwanted biases during pretraining (Hall\net al., 2022), we also investigate what happens if a\nmodel MD is further pretrained for one additional\nepoch on gender-neutral data, with the goal of creat-\ning a more gender-neutral model MN\nD. We hypothe-\nsize that this may be sufficient to reduce the biases\nencoded in the original model. Given a dataset\nD, a new dataset DN is created by substituting\ngender-related tokens in the text for gender-neutral\ntokens. The substitution is based on a hand-crafted\nlexicon,6 e.g., woman or man may be substituted to\nperson.7 The new model MN\nD is used for both the\nintrinsic and extrinsic bias evaluations.\n5 Experimental setup\n5.1 Models\nWe take the LXMERT architecture (Tan and Bansal,\n2019) as a popular representative of V&L mod-\nels, and build our controlled analysis on VOLTA\n(Bugliarello et al., 2021). VOLTA is an implementa-\ntion framework that provides a fair setup for com-\nparing V&L models pretrained under the same con-\nditions, which enables us to compare the influence\nof diverse training data on representational bias.\nIn this case, LXMERT 180K refers to the original\ncheckpoint and LXMERT 3M to the model trained\non CC3M (Bugliarello et al., 2021). We also study\n6See Appendix A\n7Note that when the pretraining data D is composed of\nmultiple corpora, we argue that domain adaptation to a non-\nbiased space should be performed only on clean data, and,\ntherefore, |DN |≤| D|.\n8468\nALBEF in two sizes and BLIP. Table 1 lists the\nmodels included in our analysis.\n5.2 Gender-neutral Data\nAs a natural extension to study representational\ngender bias, we want to evaluate to what extent\ngender-neutral data helps to mitigate gender bias.\nAmend et al. (2021) showed that gender-neutral\ntraining might be a viable approach for reducing\ngender bias in image captioning models. We study\nits effect in more generic pretrained V&L models.\nThe gender-neutral pretraining data is the result\nof substituting terms with grammatical gender for\ngender-neutral equivalents, e.g., “A woman walk-\ning her dog” translates into “A person walking their\ndog.” To this end, we create a list of gender enti-\nties8 by merging previous hand-curated lexicons\nused in a similar context to ours, provided by An-\ntoniak and Mimno (2021).9\nStarting from a pretrained checkpoint, we per-\nform an extra epoch of pretraining. The training\nis done based on a linear function that increases\nthe probability for a model to learn from gender-\nneutral captions. The starting rate is p=0.15 and, as\nthe training progresses, the probability of getting\na gender-neutral caption increases to p=1.0 at the\nlast step. Note that as the probability of getting a\ngender-neutral caption increases, the learning rate\ndecreases. This methodology supports our intuition\nthat starting with a gender-neutral corpus would be\ntoo drastic for the model to adapt to, and instead\ncause catastrophic forgetting.\nFinally, we continue pretraining the original\nmodel checkpoints for an extra epoch without the\ngender-neutral alternative (i.e., p=0.0). The evalu-\nation on this new checkpoint will help us to draw\nconclusions on longer training, as well as ensure\nthe correct implementation of our setup.\n5.3 Evaluation Tasks\nFor evaluation of downstream tasks, we report task\nperformance and analyse group disparities. Bias\namplification is reported on the validation splits.\nMLM We follow standard practice for assessing\ngender bias in V&L models (Zhao et al., 2017; Hen-\ndricks et al., 2018; Wang et al., 2019; Tang et al.,\n8See Appendix A for the complete list.\n9We deliberately omit tokens like ‘actor’ from the list if\nthe female (or male) equivalent is not always used (people do\nnot always use the word ‘actress’ when referring to a female\ncharacter). We also discard ‘male’ and ‘female’ as we suspect\nthat they are more often used on non-human entities.\nModel (MD) Gender-neutral model (MND)\nLXMERT180K LXMERTN180K\nLXMERT3M LXMERTN3M\nALBEF4M ALBEFN-COCO4M , ALBEFN-CC3M4M\nALBEF14M ALBEFN-COCO14M , ALBEFN-CC3M14M\nBLIP129M BLIPN129M\nTable 1: Summary of the models. The subscript in\nthe model name indicates the number of images in the\npretraining set. All gender-neutral models are pretrained\nwith in-domain data ( LXMERT N\n180K and BLIP N\n129M\non COCO; LXMERT N\n3M on CC3M). For models with\nmore than one gender-neutral version, the superscript\nindicates the dataset used for gender-neutral pretraining.\n2021; Srinivasan and Bisk, 2021; Agarwal et al.,\n2021; Cho et al., 2022) and expose representational\nbias in a masked language modelling (MLM) task.\nThe words masked are gendered terms given by the\nsame lexicon used in §5.2. Personal pronouns (if\nany) are also masked to avoid leaking gender infor-\nmation into the model representation. For example,\n“A woman walking her dog” would be masked as “A\n[MASK] walking [MASK] dog”. The image asso-\nciated with each sentence is also input to the model,\nin a setup that reflects the pretraining conditions.\nWe investigate the intrinsic bias of the models\nas detailed in §3, i.e., we look at the co-occurrence\nof context words ( e.g., car, ball) with particular\nword choices from the model (e.g., gender words\nlike woman, child). Previous work (Sedoc and Un-\ngar, 2019; Antoniak and Mimno, 2021; Delobelle\net al., 2021) showcases how the measure of bias\ncan be heavily influenced by the choice of target\nseed words. To avoid misleading results from low\nfrequency words, we define the set of target words\nto be the 100 most frequent common nouns that\nco-occur with the gender entities in the correspond-\ning training data. Table 2 provides a summary of\ngender distribution.\nTo evaluate intrinsic bias, we do not look at the\nexact word prediction but instead consider two op-\ntions to annotate the gender of the predicted word.\nFirst, we can extract and sum the probabilities of\nall male, female and gender-neutral tokens within\nour set to select the most probable gender entity.\nHowever, given that the distributions of tokens fol-\nlows Zipf’s Law, the probability mass computed\nfor each gender group is nearly equal, yielding in-\nconclusive results. Therefore, we use the gender\ncategory of the most probable token. Then, the bias\n8469\nCOCO CC3M VQAv2 GQA NLVR2 F30K\nImage Image Question Question Sentence Image\nMale 725 901 20000 8265 91 345\nFemale 363 945 9498 4860 99 207\nNeutral 1187 1095 18549 4442 377 336\nTotal 2275 2941 48047 17567 567 889\nTable 2: Gender distribution across validation splits\nin each dataset. Note that for COCO, this refers to\nthe minival split in (Tan and Bansal, 2019). COCO\nand F30K have five captions per image. Gender was\ninferred from image captions for COCO, CC3M and\nF30K. Gender was inferred from questions in VQAv2,\nGQA and from the sentence given in NLVR2.\npresent in model predictions is measured with the\nstatistical and bias amplification metrics presented\nin §4.2.\nVisual Question Answering VQA (Antol et al.,\n2015) requires the model to predict an answer given\nan image and a question. LXMERT formulates\nVQA as a multi-answer classification task, and AL-\nBEF and BLIP treat it as a language generation task.\nWe evaluate models on the VQAv2 (Goyal et al.,\n2017) and GQA datasets (Hudson and Manning,\n2019), and report performance as VQA-Score and\naccuracy, respectively.\nBias amplification is measured on the subset of\nquestion–answer pairs targeting people. Gender\nis inferred from the question, considering all the\ngender entities presented in Appendix A. We filter\nany answer category whose answer does not occur\nwith gender entities at least 50 times in the training\nset. Finally, numerical and yes/no question-answer\npairs are also removed leaving a total of 165 answer\ncategories in VQAv2 and 214 in GQA.\nNatural Language for Visual Reasoning\nNLVR2 (Suhr et al., 2019) requires the model to\npredict whether a text describes a pair of images.\nThe notion of bias amplification considered in this\nproject would require us to manually annotate the\ngender from all the images to be able to extract\ngender-context patterns from the training data. For\nthis reason, we only evaluate the group disparity\nin NLVR2 through differences in performance,\nreported as accuracy.\nImage–Text Retrieval This retrieval task con-\ntains two subtasks: text-to-image retrieval (IR),\nwhere we query the model with a caption to retrieve\nan image, and image-to-text retrieval (TR), where\nwe use an image to retrieve a suitable caption. We\nreport Recall@1 on the Flickr30K (Plummer et al.,\n2015) benchmark. Bias amplification is measured\non the subset of data targeting people. In IR, we\nquery the model with captions that include a word\nfrom the set of male-related or female-related to-\nkens and compare to the gender annotated in the\nimage retrieved. In TR, we query the model with\nimages annotated as ‘Male’ or ‘Female’ and com-\npare to the gendered terms in the caption retrieved.\nCaptions with gender-neutral terms are treated as\na separate case to assess how often the models\nretrieve images from each group, yet the image\nretrieved could be potentially valid for any gender\ncase. In both subtasks, we consider that the model\ndoes not amplify gender bias when the image or\ncaption retrieved has a gender-neutral subject.\n6 Results\n6.1 Intrinsic Bias\nWe evaluate intrinsic bias in encoder-only models.\nConsidering that bias varies as a function of the bias\nin a dataset, amongst other variables (Hall et al.,\n2022), we define our experiments with LXMERT\nvariants as our control setup: the same model archi-\ntecture is trained with the same hyperparameters on\ndisjoint corpora yielding two versions of the model,\nLXMERT 180K and LXMERT 3M .\nGender-neutral pretraining mitigates gen-\ndered outputs Figure 2 shows results for\nLXMERT 180K models; complete results are in Ap-\npendix C. A model is penalised when it predicts a\ntoken from the opposite gender, but we consider a\ngender-neutral term as a valid output.10 The mod-\nels pretrained with gender-neutral data, have near\nperfect F1 performance as they learnt to predict\ngender-neutral tokens when their standard counter-\nparts, LXMERT 180K and LXMERT 3M , had low\nconfidence on the most probable token.11 We pre-\nsume these are images where the visual appearance\nof the main subject is unclear. Interestingly, the\ntrade-off between precision and recall has opposite\ndirections for Female and Male groups vs Neutral\nin LXMERT 180K and LXMERT 3M : the models\ntend to output female- and male- tokens more often\nthan neutral-related, even when the subject in the\n10Predicting a gender-neutral term shows that the model\nunderstands the depicted visual concept at the generic level.\n11The models do not forget to predict gender-related to-\nkens. LXMERT N\n180K predicts ∼37% of the time a word\nfrom the set of neutral-related tokens (compared to ∼20%\nin LXMERT 180K ).\n8470\nFigure 2: Statistical analysis of gender bias in MLM\nwith gendered terms masked. Predicting a token from\nthe gender-neutral set is always considered correct (Pre-\ncision=1). Models report higher recall scores for Male\n(M) and Female (F) groups, showcasing the complete-\nness of positive predictions; it is the opposite for Neutral\n(N) tokens.\nimage was annotated as gender neutral (low recall).\nPretrained models reflect training data biases\nTable 3 shows the aggregated bias amplification\nmeasured in encoder-only model variants. Our\nbias mitigation strategy has the same consistent\nbehaviour across LXMERT models and evalua-\ntion data (COCO or CC3M): models tend to re-\nflect the same degree of bias present in the data\n(BiasAmpT→A closer to zero). ALBEF N-COCO\n14M\nand ALBEF N-CC3M\n14M models benefit from pretrain-\ning on gender-neutral data differently, as both de-\ncrease the overall bias amplification. Wang and\nRussakovsky (2021) caution against solely report-\ning the aggregated bias amplification value, as\nit could obscure attribute-task pairs that exhibit\nstrong bias amplification. We report it here as a\nrelative metric to compare the overall amplified\nbias between the models, and should not be consid-\nered in its own. See Appendix C for results broken\ndown by gender.\nWe also investigated the equivalent to\nLXMERT N\n3M , but pretrained on gender-neutral\ndata for a reduced number of steps to match\nthose in LXMERT N\n180K . We verified that more\npretraining steps on gender-neutral data equates to\na reduced bias amplification in absolute terms.\n6.2 Extrinsic Bias & Fairness\nTrade-offs in task performance Downstream\nperformance on the test sets is shown in Table 4.\nLXMERT 180K may require more pretraining steps\nto converge, as we verify that the performance im-\nprovement observed in LXMERT N\n180K is mainly\ndue to the extra pretraining steps regardless of\ngender-neutral data. Our strategy for mitigating\ngender bias on pretrained models generally leads\nto lower task performance on NLVR2 and image\nretrieval, revealing a trade-off between bias miti-\ngation and task performance. The same trade-off\nhas been observed in language models (He et al.,\n2022; Chen et al., 2023). However, gender-neutral\nmodels report similar or even superior performance\non question answering and text retrieval tasks com-\npared to their original versions.\nGender-neutral models consistently reduce\ngroup disparity Group performance is depicted\nin Figure 3 for a subset of models and tasks. Table 7\nin Appendix D shows the complete results. We ob-\nserve that group disparity is consistently reduced\non VQAv2 and retrieval tasks. An exception are\nLXMERT models, which show a minor, undesir-\nable increase in group disparity on VQAv2, GQA\nand text retrieval tasks. For instance, in question-\nanswering tasks with LXMERT, we observe a re-\nduction in the min-max gap of 4.5 (LXMERT N\n180K )\npoints in VQAv2, while the min-max gap increase\nin GQA is only of 0.4 points. Note that Tan and\nBansal (2019) pretrained LXMERT 180K on GQA\ntrain and validation data, which results in a very\nhigh performance ( ∼85.0 for all groups) on the\nGQA validation set. We speculate that the gains in\nperformance equality across groups could be due\nto a shift of the final word representations to a more\nequidistant vector space between gendered terms\nand their context. That is, the conditional probabil-\nity distribution of a gendered term given its context\nis smoother across different demographic groups.\nWe leave exploration of this for future work. In re-\ncent work, Feng et al. (2023) continued pretraining\nlanguage models on partisan corpora and observed\nthat these models do acquire (political) bias from\nsaid corpora. In our case, the continued pretraining\ncould make the MN\nD models more robust regarding\ngendered terms.\nGender-neutral training reduces fine-tuning\nvariance Dodge et al. (2020) and Bugliarello\net al. (2021) analysed the impact of random seeds\nin fine-tuning. We do this analysis on our con-\ntrol setup and observe that gender-neutral variants\nof LXMERT consistently report lower variance\nin performance on all tasks, except for NLVR2.\nWe, however, observe a strong variance in the\nfine-tuning process for NLVR2 due to the random\n8471\nLXMERT180K LXMERTN180K LXMERT3M LXMERTN3M ALBEF14M ALBEFN-COCO14M ALBEFN-CC3M14M\nCOCO -.0359 -.0008 -.0617 -.0014 -.0742 -.0517 -.0792\nCC3M -.0346 -.0062 -.0007 -.0002 -.0182 -.0367 -.0570\nTable 3: BiasAmp T→A averaged over attributes (gender entities) and tasks (top-100 nouns) for LXMERT and\nALBEF14M models. Light and dark backgrounds indicate bias amplification measured in-domain and out-of-domain\ndata respectively. Negative values indicate an overall decrease of the bias in model’s predictions.\nLXMERT 180K ALBEF 14M BLIP 129M\n70\n72\n74\n76\n78\n80\nmale\nfemale\nneutral\n(a) VQAv2\nLXMERT 180K ALBEF 14M BLIP 129M\n60\n65\n70\n75\n80\n85 male\nfemale\nneutral (b) GQA\nLXMERT 180K ALBEF 14M BLIP 129M\n50\n55\n60\n65\n70\n75\n80\n85\n90\nmale\nfemale\nneutral (c) F30K - IR\nLXMERT ALBEF BLIP\n60\n70\n80\n90\n100\nmale\nfemale\nneutral (d) F30K - TR\nFigure 3: Validation-set results of selected models (◦: LXMERT 180K , ALBEF 14M and BLIP 129M ) and their gender-\nneutral version (2: LXMERT N\n180K , ALBEF N-COCO\n14M and BLIP N\n129M , 3: ALBEF N-CC3M\n14M ). We report VQA-accuracy\nin VQAv2, accuracy in GQA, and Recall@1 in F30K by gender group: male (M), female (F), and neutral (N).\nVQAv2 GQA NLVR2 F30K\ntest-dev test-dev test-P test IR test TR\nLXMERT180K 70.3 59.4 74.5 53.0 61.1\nLXMERTN180K 71.6 59.3 74.5 53.9 66.2\nLXMERT3M 67.2 55.4 71.5 54.4 59.5\nLXMERTN3M 68.1 56.0 70.0 50.2 57.4\nALBEF4M 72.9 56.6 79.3 82.6 93.3\nALBEFN-COCO4M 72.9 56.3 77.1 82.5 94.0\nALBEFN-CC3M4M 72.9 56.6 78.4 82.4 94.2\nALBEF14M 74.4 58.4 82.4 85.9 95.1\nALBEFN-COCO14M 74.1 57.3 52.3 12 85.5 95.4\nALBEFN-CC3M14M 74.1 58.1 81.0 85.1 95.2\nBLIP129M 75.3 58.1 79.7 87.5 96.7\nBLIPN129M 75.2 58.3 79.3 86.9 96.2\nTable 4: Test results for a model MD and its gender-\nneutral version MN\nD. We report VQA-accuracy in\nVQAv2, accuracy in GQA and NLVR2, and Recall@1\nin F30K. Results for original models computed by us.\nweight initialisation of the classification layer. See\nAppendix E for specific results across 6 runs.\nIntrinsic & extrinsic bias are independent We\nestimate bias amplification in VQA tasks by evalu-\nating the fluctuations in models’ predictions when\nthey differ from the correct answer. Otherwise,\nthe models are said to not amplify the bias from\nthe data. We find that all model variants – MD\nand MN\nD – reduce the gender bias across tasks.\n12This result is inexplicably low, despite fifteen attempts\nat fine-tuning with different random seeds. We saw similar\ninstabilities when fine-tuning the released LXMERT models,\nbut we found seeds that gave above-chance accuracy.\nHowever, contrary to what we observed in pre-\ntrained models (Table 3), there is no evidence\nthat the gender-neutral pretraining influenced pos-\nitively (nor negatively) the extrinsic bias of the\nmodels: it depends on the model, downstream\ntask and gender group (see Appendix E for results\non BiasAmpA→T fine-tuning variance). Figure 4\ndisplays BiasAmp A→T broken-down by gender\ncategory measured on GQA for a subset of mod-\nels. Whereas the degree of bias amplification is\nfairly consistent between a model MD and MN\nD in\nVQAv2 (see Appendix D), there is higher variance\nin GQA: ALBEF N-COCO\n14M reduces the bias ampli-\nfication compared to ALBEF 14M , but we observe\nthe opposite effect on BLIP N-COCO .\nIn retrieval tasks, we look into models’ behavior\nwhen querying them with neutral instances. Re-\ngardless of the degree of intrinsic bias in the model,\nmodels exhibit the same trend: in IR, all models\nmostly retrieve images labeled as ‘Neutral’, but\ntwice as much ‘Male’ images as ‘Female’. We find\nsimilar results for TR, i.e., query images whose\nmain actor is defined as Neutral, but, in this sce-\nnario, only half of the captions retrieved relate to\npeople. See Appendix D for detailed results.\n7 Conclusion\nThis paper presented a comprehensive analysis of\ngender bias amplification and fairness of encoder-\nonly and encoder-decoder V&L models. The in-\n8472\nLXMERT 180K ALBEF 4M ALBEF 14M BLIP 129M\n−0.30\n−0.25\n−0.20\n−0.15\n−0.10\n−0.05\n0.00\n0.05\nBiasAmpA→T\nmale\nfemale\nneutral\nM\nMN\nFigure 4: Bias amplification measured on question-\nanswering (GQA) broken down by gender group. MN\nare gender-neutral pretrained on COCO.\ntrinsic bias analysis shows consistent results – in\nterms of bias mitigation – in models trained on\ngender-neutral data, even if these models reflect\nbiases present in data instead of diminishing them\n(as we observed with LXMERT). In line with previ-\nous findings in language models (Goldfarb-Tarrant\net al., 2021; Kaneko et al., 2022; Orgad et al., 2022),\nintrinsic bias in V&L models does not necessarily\ntransfer to extrinsic bias on downstream tasks. Sim-\nilarly, we find that the bias in a model and its empir-\nical fairness –group disparity on task performance–\nare in fact independent matters, which is in line\nwith the NLP literature (Shen et al., 2022; Cabello\net al., 2023). Intrinsic bias can potentially rein-\nforce harmful biases, but these may not impact the\ntreatment of groups (or individuals) on downstream\ntasks. We believe that bias and fairness should al-\nways be carefully evaluated as separate matters.\nOne of they key findings of our work is that the\nextra pretraining steps on gender-neutral data are\nbeneficial to reduce the group disparity in every\nmodel architecture tested on VQAv2, and in the ma-\njority of models for both retrieval tasks. Crucially,\nthere is no penalty to pay for this fair outcome: the\noverall task performance of gender-neutral models\nis similar or better than their original versions.\nLimitations\nThe framework to characterize gender bias in V&L\npresented in this study is general and extensible to\nanalyse other forms of bias in multimodal models.\nWe consider three base architectures to settle on\nthe implementation. However, our work would\nbenefit from analyzing a wider range of models.\nStudying the effects of gender-neutral pretraining\non V&L models with a frozen language model,\nsuch as ClipCap (Mokady et al., 2021) and BLIP-\n2 (Li et al., 2023), is left as future work.\nDue to computational limitations, we restricted\nmost of our analysis to single runs. We perform\na first analysis across multiple random seeds for\nLXMERT models in Appendix E. There, we no-\ntice that gender-neutral models seem to have lower\nvariance after fine-tuning. Yet, the cross-seed per-\nformance of a given model can fluctuate consider-\nably for some tasks (e.g., NLVR2), corroborating\nprevious findings from Bugliarello et al. (2021).\nLikewise, bias amplification, along with other fair-\nness metrics like group disparity, often fluctuates\nacross runs. We report bias amplification variance\nin fine-tuning of LXMERT models, but the absence\nof confidence intervals for all models and tasks –\ndue to the same reason stated above– should be\nconsidered. We hope to motivate future work to\naddress this issue.\nMoreover, despite the existence of multilingual\nmultimodal datasets (Elliott et al., 2016; Liu et al.,\n2021; Bugliarello et al., 2022, inter-alia), our ex-\nperimental setup is limited to English datasets and\nmodels. Studies of (gender) bias using only English\ndata are not complete and might yield inaccurate\nconclusions, albeit overcoming the structural per-\nvasiveness of gender specifications in grammatical\ngender languages such us German or Spanish is not\ntrivial (Gabriel et al., 2018). Likewise, our work\nconsiders a single dimension of social bias (gen-\nder). Further research on analyzing social biases on\nV&L models should account for intersectionality:\nhow different social dimensions, e.g., gender and\nrace, can intersect and compound in ways that can\npotentially impact model performance on most dis-\nfavoured groups, e.g., Black Women as discussed\nin Crenshaw (1989).\nEthics Statement\nThe models and datasets used in this study are pub-\nlicly available, and we strictly follow the ethical\nimplications of previous research related to the data\nsources. Our work is based on sensitive informa-\ntion such as gender, based on reported visual ap-\npearance in the image captions. We would like to\nemphasize that we are not categorizing biological\nsex or gender identity, but rather using the given\nimage captions as proxies to the outward gender\nappearance.\n8473\nAcknowledgments\nWe are grateful to Benjamin Rotendahl and Rita\nRamos for initial discussions about data and eval-\nuation. We also thank members of CoAStaL and\nLAMP groups for their valuable feedback. Laura\nCabello is funded by the Novo Nordisk Founda-\ntion (grant NNF 20SA0066568). ⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆⋆Emanuele\nBugliarello is supported by the funding from the\nEuropean Union’s Horizon 2020 research and inno-\nvation programme under the Marie Skłodowska-\nCurie grant agreement No 801199. Stephanie\nBrandl is funded by the European Union under\nthe Grant Agreement no. 10106555, FairER. Views\nand opinions expressed are those of the author(s)\nonly and do not necessarily reflect those of the\nEuropean Union or European Research Executive\nAgency (REA). Neither the European Union nor\nREA can be held responsible for them. This work\nwas supported by a research grant (VIL53122) from\nVILLUM FONDEN.\nReferences\nSandhini Agarwal, Gretchen Krueger, Jack Clark, Alec\nRadford, Jong Wook Kim, and Miles Brundage. 2021.\nEvaluating clip: Towards characterization of broader\ncapabilities and downstream implications.\nJack J Amend, Albatool Wazzan, and Richard Souvenir.\n2021. Evaluating gender-neutral training data for\nautomated image captioning. In 2021 IEEE Inter-\nnational Conference on Big Data (Big Data), pages\n1226–1235.\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-\ngaret Mitchell, Dhruv Batra, C. Lawrence Zitnick,\nand Devi Parikh. 2015. VQA: Visual question an-\nswering. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision (ICCV), pages\n2425 – 2433.\nMaria Antoniak and David Mimno. 2021. Bad seeds:\nEvaluating lexical methods for bias measurement.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n1889–1904, Online. Association for Computational\nLinguistics.\nCamiel J. Beukeboom, V ˜u, and Faculteit der So-\nciale Wetenschappen. 2014. Mechanisms of linguis-\ntic bias: How words reflect and maintain stereotypic\nexpectancies.\nShruti Bhargava and David Forsyth. 2019. Exposing\nand correcting the gender bias in image captioning\ndatasets and models.\nFederico Bianchi, Pratyusha Kalluri, Esin Durmus,\nFaisal Ladhak, Myra Cheng, Debora Nozza, Tat-\nsunori Hashimoto, Dan Jurafsky, James Zou, and\nAylin Caliskan. 2023. Easily accessible text-to-\nimage generation amplifies demographic stereotypes\nat large scale. In Proceedings of the 2023 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’23, page 1493–1504, New York,\nNY , USA. Association for Computing Machinery.\nStephanie Brandl, Ruixiang Cui, and Anders Søgaard.\n2022. How conservative are language models? adapt-\ning to the introduction of gender-neutral pronouns.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3624–3630, Seattle, United States. Association\nfor Computational Linguistics.\nEmanuele Bugliarello, Ryan Cotterell, Naoaki Okazaki,\nand Desmond Elliott. 2021. Multimodal pretraining\nunmasked: A meta-analysis and a unified framework\nof vision-and-language BERTs. Transactions of the\nAssociation for Computational Linguistics , 9:978–\n994.\nEmanuele Bugliarello, Fangyu Liu, Jonas Pfeiffer, Siva\nReddy, Desmond Elliott, Edoardo Maria Ponti, and\nIvan Vuli´c. 2022. IGLUE: A benchmark for transfer\nlearning across modalities, tasks, and languages. In\nProceedings of the 39th International Conference\non Machine Learning, volume 162 of Proceedings\nof Machine Learning Research , pages 2370–2392.\nPMLR.\nLaura Cabello, Anna Katrine Jørgensen, and Anders\nSøgaard. 2023. On the independence of association\nbias and empirical fairness in language models. In\nProceedings of the 2023 ACM Conference on Fair-\nness, Accountability, and Transparency, FAccT ’23,\npage 370–378, New York, NY , USA. Association for\nComputing Machinery.\nYang Cao, Yada Pruksachatkun, Kai-Wei Chang, Rahul\nGupta, Varun Kumar, Jwala Dhamala, and Aram Gal-\nstyan. 2022. On the intrinsic and extrinsic fairness\nevaluation metrics for contextualized language repre-\nsentations. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 561–570, Dublin,\nIreland. Association for Computational Linguistics.\nZhenpeng Chen, Jie M. Zhang, Federica Sarro, and\nMark Harman. 2023. A comprehensive empirical\nstudy of bias mitigation methods for machine learn-\ning classifiers.\nJaemin Cho, Abhay Zala, and Mohit Bansal. 2022. Dall-\neval: Probing the reasoning skills and social biases\nof text-to-image generative transformers.\nMyung Jin Choi, Antonio Torralba, and Alan S. Willsky.\n2012. Context models and out-of-context objects.\nPattern Recognition Letters, 33(7):853–862. Special\nIssue on Awards from ICPR 2010.\n8474\nKate Crawford. 2017. The trouble with bias. In Con-\nference on Neural Information Processing Systems,\ninvited speaker.\nKimberle Crenshaw. 1989. Demarginalizing the inter-\nsection of race and sex: A black feminist critique\nof antidiscrimination doctrine, feminist theory and\nantiracist politics. The University of Chicago Legal\nForum, 140:139–167.\nPieter Delobelle, Ewoenam Kwaku Tokpo, Toon\nCalders, and Bettina Berendt. 2021. Measuring\nfairness with biased rulers: A survey on quantify-\ning biases in pretrained language models. CoRR,\nabs/2112.07447.\nSunipa Dev, Masoud Monajatipoor, Anaelia Ovalle, Ar-\njun Subramonian, Jeff Phillips, and Kai-Wei Chang.\n2021. Harms of gender exclusivity and challenges in\nnon-binary representation in language technologies.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1968–1994, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith. 2020.\nFine-tuning pretrained language models: Weight ini-\ntializations, data orders, and early stopping.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on\nLearning Representations.\nDesmond Elliott, Stella Frank, Khalil Sima’an, and Lu-\ncia Specia. 2016. Multi30K: Multilingual English-\nGerman image descriptions. In Proceedings of the\n5th Workshop on Vision and Language , pages 70–\n74, Berlin, Germany. Association for Computational\nLinguistics.\nShangbin Feng, Chan Young Park, Yuhan Liu, and Yulia\nTsvetkov. 2023. From pretraining data to language\nmodels to downstream tasks: Tracking the trails of\npolitical biases leading to unfair NLP models. In\nProceedings of the 61st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 11737–11762, Toronto, Canada.\nAssociation for Computational Linguistics.\nUte Gabriel, Pascal Mark Gygax, and Elisabeth A. Kuhn.\n2018. Neutralising linguistic sexism: Promising but\ncumbersome? Group Processes & Intergroup Rela-\ntions, 21:844 – 858.\nSeraphina Goldfarb-Tarrant, Rebecca Marchant, Ri-\ncardo Muñoz Sánchez, Mugdha Pandya, and Adam\nLopez. 2021. Intrinsic bias metrics do not correlate\nwith application bias. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1926–1940, Online. Association\nfor Computational Linguistics.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the V in VQA\nmatter: Elevating the role of image understanding\nin Visual Question Answering. In Conference on\nComputer Vision and Pattern Recognition (CVPR).\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nMelissa Hall, Laurens van der Maaten, Laura Gustafson,\nMaxwell Jones, and Aaron Adcock. 2022. A system-\natic study of bias amplification.\nZexue He, Yu Wang, Julian McAuley, and Bod-\nhisattwa Prasad Majumder. 2022. Controlling bias\nexposure for fair interpretable predictions. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2022, pages 5854–5866, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nLisa Anne Hendricks, Kaylee Burns, Kate Saenko,\nTrevor Darrell, and Anna Rohrbach. 2018. Women\nalso snowboard: Overcoming bias in captioning mod-\nels. In Proceedings of the European Conference on\nComputer Vision (ECCV).\nYusuke Hirota, Yuta Nakashima, and Noa Garcia. 2022a.\nGender and racial bias in visual question answering\ndatasets. In 2022 ACM Conference on Fairness, Ac-\ncountability, and Transparency , FAccT ’22, page\n1280–1292, New York, NY , USA. Association for\nComputing Machinery.\nYusuke Hirota, Yuta Nakashima, and Noa Garcia. 2022b.\nQuantifying societal bias amplification in image cap-\ntioning. In Proc. IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR) . 10\npages.\nDrew A. Hudson and Christopher D. Manning. 2019.\nGqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Confer-\nence on Computer Vision and Pattern Recognition\n(CVPR), pages 6700–6709.\nXisen Jin, Francesco Barbieri, Brendan Kennedy, Aida\nMostafazadeh Davani, Leonardo Neves, and Xiang\nRen. 2021. On transferability of bias mitigation ef-\nfects in language model fine-tuning. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3770–3783,\nOnline. Association for Computational Linguistics.\n8475\nMasahiro Kaneko, Danushka Bollegala, and Naoaki\nOkazaki. 2022. Debiasing isn’t enough! – on the\neffectiveness of debiasing MLMs and their social\nbiases in downstream tasks. In Proceedings of the\n29th International Conference on Computational Lin-\nguistics, pages 1299–1310, Gyeongju, Republic of\nKorea. International Committee on Computational\nLinguistics.\nOs Keyes. 2018. The misgendering machines: Trans/hci\nimplications of automatic gender recognition. Proc.\nACM Hum.-Comput. Interact., 2(CSCW).\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W Black,\nand Yulia Tsvetkov. 2019. Measuring bias in con-\ntextualized word representations. In Proceedings of\nthe First Workshop on Gender Bias in Natural Lan-\nguage Processing, pages 166–172, Florence, Italy.\nAssociation for Computational Linguistics.\nBrian Larson. 2017. Gender as a variable in natural-\nlanguage processing: Ethical considerations. In Pro-\nceedings of the First ACL Workshop on Ethics in\nNatural Language Processing, pages 1–11, Valencia,\nSpain. Association for Computational Linguistics.\nSeungho Lee, Minhyun Lee, Jongwuk Lee, and Hyun-\njung Shim. 2021. Railroad is not a train: Saliency\nas pseudo-pixel supervision for weakly supervised\nsemantic segmentation. 2021 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR),\npages 5491–5501.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023. BLIP-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. In Proceedings of the 40th Interna-\ntional Conference on Machine Learning, Proceedings\nof Machine Learning Research. PMLR.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven C. H.\nHoi. 2022. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. In International Conference on Ma-\nchine Learning.\nJunnan Li, Ramprasaath R. Selvaraju, Akhilesh Deepak\nGotmare, Shafiq Joty, Caiming Xiong, and Steven\nHoi. 2021. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nIn Advances in Neural Information Processing Sys-\ntems.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C. Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In Computer Vision –\nECCV 2014, pages 740–755, Cham. Springer Inter-\nnational Publishing.\nFangyu Liu, Emanuele Bugliarello, Edoardo Maria\nPonti, Siva Reddy, Nigel Collier, and Desmond El-\nliott. 2021. Visually grounded reasoning across lan-\nguages and cultures. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 10467–10485, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nTom Michael Mitchell. 1980. The need for biases in\nlearning generalizations. In Rutgers CS tech report\nCBM-TR-117.\nRon Mokady, Amir Hertz, and Amit H Bermano. 2021.\nClipCap: CLIP prefix for image captioning. arXiv\npreprint arXiv:2111.09734.\nGeorge D. Montañez, Jonathan Hayase, Julius Lauw,\nDominique Macias, Akshay Trikha, and Julia\nVendemiatti. 2019. The futility of bias-free learn-\ning and search. In AI 2019: Advances in Artificial\nIntelligence, pages 277–288, Cham. Springer Inter-\nnational Publishing.\nHadas Orgad, Seraphina Goldfarb-Tarrant, and Yonatan\nBelinkov. 2022. How gender debiasing affects in-\nternal model representations, and why it matters. In\nProceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 2602–2628, Seattle, United States. Association\nfor Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing (EMNLP), pages 1532–1543, Doha, Qatar.\nAssociation for Computational Linguistics.\nBryan A. Plummer, Liwei Wang, Chris M. Cervantes,\nJuan C. Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. 2015. Flickr30k entities: Collecting\nregion-to-phrase correspondences for richer image-\nto-sentence models. In 2015 IEEE International\nConference on Computer Vision (ICCV), pages 2641–\n2649.\nJoão Sedoc and Lyle Ungar. 2019. The role of protected\nclass word lists in bias identification of contextual-\nized word representations. In Proceedings of the\nFirst Workshop on Gender Bias in Natural Language\nProcessing, pages 55–61, Florence, Italy. Association\nfor Computational Linguistics.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of ACL.\nAili Shen, Xudong Han, Trevor Cohn, Timothy Bald-\nwin, and Lea Frermann. 2022. Does representational\nfairness imply empirical fairness? In Findings of the\nAssociation for Computational Linguistics: AACL-\nIJCNLP 2022, pages 81–95, Online only. Association\nfor Computational Linguistics.\nSunny Shrestha and Sanchari Das. 2022. Exploring\ngender biases in ml and ai academic research through\nsystematic literature review. Frontiers in Artificial\nIntelligence, 5.\n8476\nKrishna Kumar Singh, Dhruv Kumar Mahajan, Kristen\nGrauman, Yong Jae Lee, Matt Feiszli, and Deepti\nGhadiyaram. 2020. Don’t judge an object by its con-\ntext: Learning to overcome contextual bias. 2020\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 11067–11075.\nTejas Srinivasan and Yonatan Bisk. 2021. Worst of both\nworlds: Biases compound in pre-trained vision-and-\nlanguage models.\nKarolina Stanczak and Isabelle Augenstein. 2021. A\nsurvey on gender bias in natural language processing.\nAlane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang,\nHuajun Bai, and Yoav Artzi. 2019. A corpus for\nreasoning about natural language grounded in pho-\ntographs. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6418–6428, Florence, Italy. Association for\nComputational Linguistics.\nTony Sun, Andrew Gaut, Shirlyn Tang, Yuxin Huang,\nMai ElSherief, Jieyu Zhao, Diba Mirza, Elizabeth\nBelding, Kai-Wei Chang, and William Yang Wang.\n2019. Mitigating gender bias in natural language\nprocessing: Literature review. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 1630–1640, Florence, Italy.\nAssociation for Computational Linguistics.\nTony Sun, Kellie Webster, Apu Shah, William Yang\nWang, and Melvin Johnson. 2021. They, them, theirs:\nRewriting with gender-neutral english.\nHao Tan and Mohit Bansal. 2019. LXMERT: Learning\ncross-modality encoder representations from trans-\nformers. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n5100–5111, Hong Kong, China. Association for Com-\nputational Linguistics.\nRuixiang Tang, Mengnan Du, Yuening Li, Zirui Liu,\nNa Zou, and Xia Hu. 2021. Mitigating gender bias in\ncaptioning systems. In Proceedings of the Web Con-\nference 2021, WWW ’21, page 633–645, New York,\nNY , USA. Association for Computing Machinery.\nEmiel van Miltenburg. 2016. Stereotyping and bias in\nthe flickr30k dataset. In Proceedings of Multimodal\nCorpora, pages 1–4.\nEva Vanmassenhove, Chris Emmery, and Dimitar Shteri-\nonov. 2021. NeuTral Rewriter: A rule-based and neu-\nral approach to automatic rewriting into gender neu-\ntral alternatives. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 8940–8948, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nAngelina Wang, Alexander Liu, Ryan Zhang, Anat\nKleiman, Leslie Kim, Dora Zhao, Iroha Shirai,\nArvind Narayanan, and Olga Russakovsky. 2022.\nRevise: A tool for measuring and mitigating\nbias in visual datasets. Int. J. Comput. Vision ,\n130(7):1790–1810.\nAngelina Wang and Olga Russakovsky. 2021. Direc-\ntional bias amplification. In ICML.\nTianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei\nChang, and Vicente Ordonez. 2019. Balanced\ndatasets are not enough: Estimating and mitigating\ngender bias in deep image representations. In ICCV.\nDora Zhao, Angelina Wang, and Olga Russakovsky.\n2021. Understanding and evaluating racial biases in\nimage captioning. In International Conference on\nComputer Vision (ICCV).\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Ryan Cotterell,\nVicente Ordonez, and Kai-Wei Chang. 2019. Gender\nbias in contextualized word embeddings. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long and Short Papers) , pages 629–634, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2017. Men also like\nshopping: Reducing gender bias amplification using\ncorpus-level constraints. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 2979–2989, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nKankan Zhou, Eason Lai, and Jing Jiang. 2022. VL-\nStereoSet: A study of stereotypical bias in pre-trained\nvision-language models. In Proceedings of the 2nd\nConference of the Asia-Pacific Chapter of the Asso-\nciation for Computational Linguistics and the 12th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 527–538,\nOnline only. Association for Computational Linguis-\ntics.\nXizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. 2018.\nDeformable convnets v2: More deformable, better\nresults. 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 9300–\n9308.\n8477\nA Seed words\nGender terms\n• Female: aunt, bride, businesswoman, daugh-\nter, daughters, fiancee, fiancée, gal, gals,\ngirl, girlfriend, girls, grandmother, her, her-\nself, lady, landlady, mama, mom, mother,\nqueen, she, sister, sisters, spokeswoman, wife,\nwoman, women, womens.\n• Male: boy, boyfriend, boys, brother, brothers,\nbusinessman, dad, dude, dudes, father, fiance,\nfiancé, gentleman, grandfather, groom, guy,\nhe, him, himself, his, husband, king, landlord,\nman, men, mens, papa, son, sons, spokesman,\nuncle.\n• Neutral: businessperson, child, childs, grand-\nparent, kid, kids, landlord, monarch, new-\nlywed, parent, partner, pbling, people, per-\nson, sibling, siblings, someone, spokesperson,\nspouse, their, them, themself, they.\nGender-neutral mappings Using the gender\nterms listed above, we generate mappings from\nmale and female to neutral terms: see Table 5 for\ndetails. These mappings are used to continue pre-\ntraining on gender-neutral (debiased) data as ex-\nplained in §5.2.\nObjects List of top-100 most frequent nouns co-\noccurring with gender terms in the training split in\nCOCO (Lin et al., 2014) and Conceptual Captions\n(CC3M) (Sharma et al., 2018).\n• COCO: tennis, group, street, baseball, table,\ndog, front, ball, player, field, snow, game,\nbeach, horse, skateboard, umbrella, water,\nphone, kite, hand, top, board, ski, couple,\nmotorcycle, food, elephant, People, picture,\npizza, surfboard, room, shirt, bench, wave,\nfrisbee, court, park, air, cake, bed, laptop,\ntrain, cell, racket, bat, bus, kitchen, plate,\nglass, ocean, side, grass, giraffe, building, city,\nskier, road, car, suit, trick, cat, tie, tree, bike,\nphoto, boat, hat, slope, baby, area, sign, chair,\nsidewalk, computer, hill, head, surfer, moun-\ntain, video, skateboarder, soccer, truck, ba-\nnana, couch, camera, skate, crowd, lot, snow-\nboard, background, wine, bear, day, back, lug-\ngage, cow, living, fence, ramp.\n• CC3M: player, team, actor, football, game,\nartist, hand, day, match, background, dress,\nMale Female Neutral\nboy girl child\nboyfriend girlfriend partner\nboys girls kids\nbrother sister sibling\nbrothers sisters siblings\nbusinessman businesswoman businessperson\ndad mom parent\ndude gal person\ndudes gals people\nfather mother parent\nfiance fiancee partner\nfiancé fiancée partner\ngentleman lady person\ngrandfather grandmother grandparent\ngroom bride newlywed\nguy gal person\nhe she they\nhim her them\nhimself herself themself\nhis her their\nhusband wife spouse\nking queen monarch\nlandlord landlady landlord\nman woman person\nmen women someone\nmens womens people\npapa mama parent\nson daughter kid\nsons daughters childs\nspokesman spokeswoman spokesperson\nuncle aunt pbling\nTable 5: Gender-neutral mappings used for continual\npre-training in gender-neutral data as described in §5.2.\nbeach, car, photo, dog, event, street, home,\nball, wedding, family, city, film, time, tree,\naward, goal, hair, front, night, water, baby,\nbusiness, illustration, politician, sport, show,\nway, portrait, face, book, premiere, fan, room,\nhead, friend, year, athlete, park, house, fash-\nion, soccer, character, flower, country, style,\nfield, side, party, festival, picture, stage, rock,\neye, couple, world, shirt, vector, camera, pop,\ntv, ceremony, hat, glass, snow, horse, school,\nroad, phone, arm, art, window, crowd, sea, ta-\nble, part, boat, suit, basketball, model, top,\nbirthday, star, student, view, tennis, smile,\nwall, celebrity, baseball.\n8478\nB Models\nIn this section, we provide an overview on the mod-\nels we use in our evaluation. We refer to their\noriginal work for more details.\nLXMERT (Tan and Bansal, 2019) is a cross-\nmodal architecture pretrained to learn vision-and-\nlanguage representations. It consists of three Trans-\nformer (Vaswani et al., 2017) encoders, where vi-\nsual and language inputs are encoded separately in\ntwo independent stacks of Transformer layers be-\nfore feeding them into the cross-modality encoder.\nThe cross-modality encoder uses bi-directional\ncross attention to exchange information and align\nthe entities across the two modalities. LXMERT is\ntrained with four objectives: masked language mod-\nelling (MLM), masked object prediction, image–\ntext matching (ITM) and image question answer-\ning.\nSimilar to LXMERT,ALBEF (Li et al., 2021) is\na dual-stream encoder (Bugliarello et al., 2021) that\nfirst learns separate visual and textual embeddings\nusing Transformer-based image and text encoders;\nand then fuses them in a cross-modal Transformer\nusing image–text contrastive loss (ITC), which en-\nables a more grounded vision and language repre-\nsentation learning. The model is pretrained with\ntwo other objectives: masked language modelling\n(MLM) and image–text matching (ITM) on the\nmultimodal encoder. Unlike LXMERT, ALBEF\ndoes not rely on image features extracted from an\noff-the-shelf object detector, but directly feeds the\nraw image into a Vision Transformer (Dosovitskiy\net al., 2021)\nBLIP (Li et al., 2022) is a versatile model based\non a multimodal mixture of encoder–decoder net-\nwork, that can be applied to a wide range of down-\nstream tasks. The authors introduce a novel boost-\nrapping method to generate synthetic captions and\nremove noisy pairs from large-scale web data. Un-\nlike LXMERT and ALBEF, BLIP is trained with an\nautoregressive language modelling objective that\nallows the generation of coherent captions given\nan image. The model is also pretrained using the\nunimodal image–text contrastive loss (ITC) and the\ncross-modal image–text matching (ITM) loss used\nby ALBEF.\nC Bias in Pretrained Models\nIntrinsic bias Figure 5 complements Figure 2\nfrom the main paper showing statistical results mea-\nsured on the intrinsic bias analysis in our control\nFigure 5: Statistical analysis of gender bias found\nthrough masked language modelling with gendered\nterms masked. Prediction of a token from the gender-\nneutral set is always considered correct (Precision=1).\nModels report higher recall scores for Male (M) and\nFemale (F) groups, showcasing the completeness of pos-\nitive predictions, whereas it is the opposite for Neutral-\nrelated (N) tokens.\nsetup.\nMLM experiment broken down by gender Ta-\nble 6 provides a more granular look at which gender\ngroups are actually amplifying/decreasing the bias\nin the pretrained models.\nD Bias & Fairness in Downstream Tasks\nExtrinsic Bias The following graphs comple-\nment results shown in § 6.2 for bias amplification\nmeasured on downstream tasks: Figure 6 shows\nresults on GQA; Figure 7 shows results on VQAv2;\nFigure 8 and Figure 9 show the bias revealed on\nimage–text retrieval tasks when querying the mod-\nels with a gender-neutral caption (or image), re-\nspectively.\nTask performance & Fairness We present gran-\nular results on task performance in validation in\nTable 7 and group disparity, defined as the min-\nmax difference between group performance (∆).\n8479\nCOCO\nMale Female Neutral\nLXMERT 180K -.0295 -.0048 -.0733\nLXMERT N\n180K -.0004 -.0008 -.0014\nLXMERT 3M -.0577 -.0230 -.1062\nLXMERT N\n3M -.0014 +.0001 -.0028\nLXMERT N-SC\n180K -.0082 -.0009 -.0109\nALBEF 4M -.1006 -.0517 -.1083\nALBEF N-COCO\n4M -.0748 -.1293 -.1529\nALBEF N-CC3M\n4M -.0754 -.0337 -.1073\nALBEF 14M -.0418 -.1146 -.0663\nALBEF N-COCO\n14M -.0559 -.0169 -.0824\nALBEF N-CC3M\n14M -.0556 -.0983 -.0837\nCC3M\nMale Female Neutral\nLXMERT 180K -.0281 -.0276 -.0482\nLXMERT N\n180K +.0008 -.0081 -.0113\nLXMERT 3M -.0043 -.0030 +.0055\nLXMERT N\n3M +.0002 +.0004 -.0011\nLXMERT N-SC\n180K -.0011 +.0003 -.0012\nALBEF 4M -.0473 -.0569 -.0422\nALBEF N-COCO\n4M -.0329 -.0514 -.0152\nALBEF N-CC3M\n4M -.0295 -.0497 -.0313\nALBEF 14M +.0159 -.0642 -.0062\nALBEF N-COCO\n14M -.0290 -.0250 -.0561\nALBEF N-CC3M\n14M -.0535 -.0641 -.0534\nTable 6: BiasAmpT→A (BA.) per gender group, aver-\naged over tasks (top-100 nouns) for LXMERT and AL-\nBEF models, evaluated on validation splits on COCO\n(top) and CC3M (bottom). Light and dark backgrounds\nindicate bias amplification measured within in-domain\nand out-of-domain data respectively. A model amplifies\nthe bias in the dataset if the value is positive. A negative\nvalue indicates an overall decrease of the bias in model’s\npredictions.\nLXMERT 3M ALBEF 4M ALBEF 14M\n−0.30\n−0.25\n−0.20\n−0.15\n−0.10\n−0.05\n0.00\n0.05\nBiasAmpA→T\nmale\nfemale\nneutral\nM\nMN\nFigure 6: Bias amplification measured on question-\nanswering (GQA) broken down by gender group. MN\nare gender-neutral pretrained on CC3M.\nLXMERT 180K ALBEF 4M ALBEF 14M BLIP 129M\n−0.07\n−0.06\n−0.05\n−0.04\n−0.03\n−0.02\n−0.01\n0.00\nBiasAmpA→T\nmale\nfemale\nneutral\nM\nMN\n(a) MN are gender-neutral models pretrained on COCO.\nLXMERT 3M ALBEF 4M ALBEF 14M\n−0.07\n−0.06\n−0.05\n−0.04\n−0.03\n−0.02\n−0.01\n0.00\nBiasAmpA→T\nmale\nfemale\nneutral\nM\nMN\n(b) MN are gender-neutral models pretrained on COCO.\nFigure 7: Bias amplification measured on question-\nanswering (VQAv2) broken down by gender group.\n8480\nVQAv2 GQA NLVR2 F30K\nAcc. ∆(↓) Acc. ∆(↓) Acc. ∆(↓) r@1 IR ∆(↓) r@1 TR ∆(↓)\nLXMERT180K\nM 77.5\n4.8\n83.8\n1.1\n81.9\n6.9\n55.8\n10.1\n61.6\n7.5F 76.3 84.9 75.0 58.5 62.8\nN 72.7 84.8 81.1 48.4 55.3\nLXMERTN\n180K\nM 79.6\n1.5\n83.9\n1.5\n79.3\n6.0\n56.1\n8.8\n66.4\n8.4F 78.1 85.4 74.2 58.1 67.6\nN 79.3 85.3 80.2 49.3 59.2\nLXMERT3M\nM 68.4\n6.0\n63.7\n1.9\n72.3\n14.8\n56.0\n8.2\n63.0\n8.7F 66.5 65.6 64.0 59.4 63.7\nN 62.4 64.5 78.8 51.2 55.0\nLXMERTN\n3M\nM 70.3\n2.4\n64.6\n2.3\n79.3\n11.6\n51.4\n5.0\n56.2\n9.0F 67.9 66.8 67.7 52.3 61.4\nN 70.1 64.5 78.8 47.3 52.4\nALBEF4M\nM 75.1\n1.4\n60.0\n2.5\n87.9\n11.1\n83.1\n10.1\n94.5\n7.0F 73.7 61.5 76.8 87.9 98.1\nN 74.3 62.5 79.6 77.8 91.1\nALBEFN-COCO\n4M\nM 75.0\n1.4\n60.5\n1.6\n85.7\n9.9\n83.7\n10.1\n94.2\n5.2F 73.6 60.7 75.8 87.2 96.6\nN 74.0 62.1 79.8 77.1 91.4\nALBEFN-CC3M\n4M\nM 75.0\n1.3\n61.0\n2.0\n84.6\n9.8\n82.2\n9.2\n94.8\n7.7F 73.7 60.7 74.8 87.1 97.6\nN 74.5 62.7 79.3 77.9 89.9\nALBEF14M\nM 76.0\n1.0\n59.7\n2.8\n86.8\n7.0\n87.5\n7.3\n95.9\n6.0F 75.0 59.6 79.8 89.1 100.0\nN 75.6 62.4 81.2 81.8 94.0\nALBEFN-COCO\n14M\nM 76.0\n1.1\n60.6\n2.4\n60.4\n7.9\n86.6\n5.9\n95.4\n5.0F 74.9 60.3 52.5 87.8 99.0\nN 75.5 62.7 57.6 81.9 94.0\nALBEFN-CC3M\n14M\nM 75.8\n0.8\n60.7\n2.3\n87.9\n10.1\n86.6\n7.8\n95.9\n4.6F 75.0 61.9 77.8 89.8 98.1\nN 75.5 63.0 81.7 82.0 93.5\nBLIP129M\nM 76.5\n1.4\n60.3\n2.0\n82.4\n6.0\n88.7\n6.2\n97.1\n3.8F 75.1 60.3 77.8 90.6 99.0\nN 75.6 62.3 83.8 84.4 95.2\nBLIPN\n129M\nM 76.4\n1.2\n60.1\n2.6\n84.6\n10.9\n88.1\n5.8\n98.3\n4.1F 75.2 59.7 73.7 89.6 99.0\nN 75.8 62.3 80.9 83.8 94.9\nTable 7: Validation results per group: male (M), female (F), and neutral (N). We report VQA-accuracy in VQAv2,\naccuracy in GQA and NLVR2, recall@1 in F30k and group disparity (∆) across tasks. Lower ∆ is better.\n8481\nLXMERT 180K ALBEF 4M ALBEF 14M BLIP 129M\n0\n10\n20\n30\n40\n50\n60\n70\n80%\nmale\nfemale\nneutral\nM\nMN\n(a) IR - MN are gender-neutral models pretrained on COCO.\nLXMERT 3M ALBEF 4M ALBEF 14M\n0\n10\n20\n30\n40\n50\n60\n70\n80%\nmale\nfemale\nneutral\nM\nMN\n(b) IR - MN are gender-neutral models pretrained on CC3M.\nFigure 8: Extrinsic bias measured on text-to-image\nretrieval (IR) on Flickr30K. Bias is measured as the\npercentage of images retrieved from each group when\nquerying the models with a gender-neutral caption.\nLXMERT 180K ALBEF 4M ALBEF 14M BLIP 129M\n0\n5\n10\n15\n20\n25\n30\n35%\nmale\nfemale\nneutral\nM\nMN\n(a) TR - MN are gender-neutral models pretrained on COCO.\nLXMERT 3M ALBEF 4M ALBEF 14M\n0\n5\n10\n15\n20\n25\n30\n35%\nmale\nfemale\nneutral\nM\nMN\n(b) TR - MN are gender-neutral models pretrained on CC3M.\nFigure 9: Extrinsic bias measured on image-to-text re-\ntrieval (TR) (c)-(d) on Flickr30K. Bias is measured as\nthe percentage of captions retrieved from each group\nwhen querying the models with a gender-neutral image.\n8482\nE Variance in fine-tuning\nTable 8 shows the mean and standard deviation\nin bias amplification when fine-tuning LXMERT\nmodels with different random seeds. The variance\nis due to random initialization. In line with what\nwe observed in §6.2, there is no clear trend when\ncomparing a model M with it’s gender-neutral pre-\ntraining counterpart, MN\nD.\nVQAv2 GQA\nmean±std mean ±std\nLXMERT180K\nmale -.0311 ±.0057 -.0192±.0071\nfemale -.0497±.0053 -.0477±.0088\nneutral +.0020±.0030 -.0252±.0089\nLXMERTN180K\nmale -.0301 ±.0031 -.0227±.0036\nfemale -.0538±.0034 -.0528±.0106\nneutral +.0007±.0014 -.0245±.0042\nLXMERT3M\nmale -.0169 ±.0054 -.0254±.0135\nfemale -.0667±.0041 -.0935±.0110\nneutral -.0157±.0056 -.0269±.0069\nLXMERTN3M\nmale -.0164 ±.0038 -.0188±.0060\nfemale -.0634±.0046 -.0971±.0131\nneutral -.0183±.0035 -.0194±.0109\nTable 8: BiasAmp A→T fine-tuning variance of\nLXMERT models across question answering tasks.\nEach model is fine-tuned 6 times on each task. We\nreport average VQA-accuracy in VQAv2 and average\naccuracy in GQA, together with its standard deviation.\nFigure 10 shows violin plots of the distribution\nof results when fine-tuning LXMERT models with\ndifferent random seeds. The variance is due to\nrandom initialization. Gender-neutral models re-\nveal lower standard deviation across tasks. This\nfinding reveals one of the benefits to perform ex-\ntra steps of pretraining on gender-neutral data: to\nreduce variance in downstream performance. This\nobservation aligns with the NLP literature showing\nthat biases in a model are independent from model\nperformance (Cabello et al., 2023).\nLXMERT\n180K\nLXMERT\n3M\nLXMERT\nN\n180K\nLXMERT\nN\n3M\n66\n68\n70\n72\n74\n(a) VQAv2\nLXMERT\n180K\nLXMERT\n3M\nLXMERT\nN\n180K\nLXMERT\nN\n3M\n52\n54\n56\n58\n60 (b) GQA\nLXMERT\n180K\nLXMERT\n3M\nLXMERT\nN\n180K\nLXMERT\nN\n3M\n40\n50\n60\n70\n80\n90\n(c) NLVR2\nLXMERT\n180K\nLXMERT\n3M\nLXMERT\nN\n180K\nLXMERT\nN\n3M\n40\n50\n60\n70\nIR\nTR (d) F30K\nFigure 10: Fine-tuning variance of LXMERT models\nacross tasks. On the left with white background, origi-\nnal models (MD). On the right with darker background,\nmodels after gender-neutral pretraining ( MN\nD). Each\nmodel is fine-tuned 6 times on each task. The dots\nrepresent the experimental observations. We report av-\nerage VQA-accuracy in VQAv2, accuracy in GQA and\nNLVR2, and recall@1 in F30k.\n8483"
}