{
    "title": "Accelerating Neural Transformer via an Average Attention Network",
    "url": "https://openalex.org/W2799001369",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2001262373",
            "name": "Zhang Biao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2351906288",
            "name": "Xiong, Deyi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2481319220",
            "name": "Su, Jinsong",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1924770834",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2949888546",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2552839021",
        "https://openalex.org/W2493519722",
        "https://openalex.org/W2949335953",
        "https://openalex.org/W2964125283",
        "https://openalex.org/W2610635252",
        "https://openalex.org/W2767206889",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2963598809",
        "https://openalex.org/W2964308564"
    ],
    "abstract": "With parallelizable attention networks, the neural Transformer is very fast to train. However, due to the auto-regressive architecture and self-attention in the decoder, the decoding procedure becomes slow. To alleviate this issue, we propose an average attention network as an alternative to the self-attention network in the decoder of the neural Transformer. The average attention network consists of two layers, with an average layer that models dependencies on previous positions and a gating layer that is stacked over the average layer to enhance the expressiveness of the proposed attention network. We apply this network on the decoder part of the neural Transformer to replace the original target-side self-attention model. With masking tricks and dynamic programming, our model enables the neural Transformer to decode sentences over four times faster than its original version with almost no loss in training time and translation performance. We conduct a series of experiments on WMT17 translation tasks, where on 6 different language pairs, we obtain robust and consistent speed-ups in decoding.",
    "full_text": "Accelerating Neural Transformer via an Average Attention Network\nBiao Zhang1,2, Deyi Xiong3 and Jinsong Su1,2∗\nXiamen University, Xiamen, China 3610051\nBeijing Advanced Innovation Center for Language Resources2\nSoochow University, Suzhou, China 2150063\nzb@stu.xmu.edu.cn, dyxiong@suda.edu.cn, jssu@xmu.edu.cn\nAbstract\nWith parallelizable attention networks, the\nneural Transformer is very fast to train.\nHowever, due to the auto-regressive archi-\ntecture and self-attention in the decoder,\nthe decoding procedure becomes slow. To\nalleviate this issue, we propose an average\nattention network as an alternative to the\nself-attention network in the decoder of\nthe neural Transformer. The average atten-\ntion network consists of two layers, with\nan average layer that models dependencies\non previous positions and a gating layer\nthat is stacked over the average layer to en-\nhance the expressiveness of the proposed\nattention network. We apply this network\non the decoder part of the neural Trans-\nformer to replace the original target-side\nself-attention model. With masking tricks\nand dynamic programming, our model en-\nables the neural Transformer to decode\nsentences over four times faster than its\noriginal version with almost no loss in\ntraining time and translation performance.\nWe conduct a series of experiments on\nWMT17 translation tasks, where on 6 dif-\nferent language pairs, we obtain robust and\nconsistent speed-ups in decoding.1\n1 Introduction\nThe past few years have witnessed the rapid de-\nvelopment of neural machine translation (NMT),\nwhich translates a source sentence into the tar-\nget language with an encoder-attention-decoder\nframework (Sutskever et al., 2014; Bahdanau\net al., 2015). Under this framework, various ad-\nvanced neural architectures have been explored\n∗Corresponding author.\n1Source code is available at\nhttps://github.com/bzhangXMU/transformer-aan.\n1RNN2CNN3Transformer\nFigure 1: Illustration of the decoding procedure\nunder different neural architectures. We show\nwhich previous target words are required to pre-\ndict the current target word yj in different NMT\narchitectures. kindicates the ﬁlter size of the con-\nvolution layer.\nas the backbone network for translation, ranging\nfrom recurrent neural networks (RNN) (Sutskever\net al., 2014; Luong et al., 2015), convolutional\nneural networks (CNN) (Gehring et al., 2017a,b)\nto full attention networks without recurrence and\nconvolution (Vaswani et al., 2017). Particularly,\nthe neural Transformer, relying solely on attention\nnetworks, has refreshed state-of-the-art perfor-\nmance on several language pairs (Vaswani et al.,\n2017).\nMost interestingly, the neural Transformer is ca-\npable of being fully parallelized at the training\nphase and modeling intra-/inter-dependencies of\nsource and target sentences within a short path.\nThe parallelization property enables training NMT\nvery quickly, while the dependency modeling\nproperty endows the Transformer with strong abil-\nity in inducing sentence semantics as well as trans-\nlation correspondences. However, the decoding of\nthe Transformer cannot enjoy the speed strength of\nparallelization due to the auto-regressive genera-\ntion schema in the decoder. And the self-attention\narXiv:1805.00631v3  [cs.CL]  7 May 2018\nInput LayerAverage LayerGating Layer\nFigure 2: Visualization of the proposed model.\nFor clarity, we show an example with only four\nwords.\nnetwork in the decoder even further slows it.\nWe explain this using Figure 1, where we pro-\nvide a comparison to RNN- and CNN-based NMT\nsystems. To capture dependencies from previ-\nously predicted target words, the self-attention in\nthe neural Transformer requires to calculate adap-\ntive attention weights on all these words (Figure\n1 (3)). By contrast, CNN only requires previous\nk target words (Figure 1 (2)), while RNN merely\n1 (Figure 1 (1)). Due to the auto-regressive gen-\neration schema, decoding inevitably follows a se-\nquential manner in the Transformer. Therefore the\ndecoding procedure cannot be parallelized. Fur-\nthermore, the more target words are generated, the\nmore time the self-attention in the decoder will\ntake to model dependencies. Therefore, preserv-\ning the training efﬁciency of the Transformer on\nthe one hand and accelerating its decoding on the\nother hand becomes a new and serious challenge.\nIn this paper, we propose an average attention\nnetwork (AAN) to handle this challenge. We show\nthe architecture of AAN in Figure 2, which con-\nsists of two layers: an average layer and gating\nlayer. The average layer summarizes history in-\nformation via a cumulative average operation over\nprevious positions. This is equivalent to a simple\nattention network where original adaptively com-\nputed attention weights are replaced with averaged\nweights. Upon this layer, we stack a feed forward\ngating layer to improve the model’s expressiveness\nin describing its inputs.\nWe use AAN to replace the self-attention part\nof the neural Transformer’s decoder. Considering\nthe characteristic of the cumulative average op-\neration, we develop a masking method to enable\nparallel computation just like the original self-\nattention network in the training. In this way, the\nwhole AAN model can be trained totally in par-\nallel so that the training efﬁciency is ensured. As\nfor the decoding, we can substantially accelerate\nit by feeding only the previous hidden state to the\nTransformer decoder just like RNN does. This is\nachieved with a dynamic programming method.\nIn spite of its simplicity, our model is capable of\nmodeling complex dependencies. This is because\nAAN regards each previous word as an equal con-\ntributor to current word representation. Therefore,\nno matter how long the input is, our model can\nalways build up connection signals with previous\ninputs, which we argue is very crucial for inducing\nlong-range dependencies for machine translation.\nWe examine our model on WMT17 translation\ntasks. On 6 different language pairs, our model\nachieves a speed-up of over 4 times with almost\nno loss in both translation quality and training\nspeed. In-depth analyses further demonstrate the\nconvergency and advantages of translating long\nsentences of the proposed AAN.\n2 Related Work\nGRU (Chung et al., 2014) or LSTM (Hochreiter\nand Schmidhuber, 1997) RNNs are widely used\nfor neural machine translation to deal with long-\nrange dependencies as well as the gradient van-\nishing issue. A major weakness of RNNs lies at\nits sequential architecture that completely disables\nparallel computation. To cope with this problem,\nGehring et al. (2017a) propose to use CNN-based\nencoder as an alternative to RNN, and Gehring\net al. (2017b) further develop a completely CNN-\nbased NMT system. However, shallow CNN can\nonly capture local dependencies. Hence, CNN-\nbased NMT normally develops deep archictures to\nmodel long-distance dependencies. Different from\nthese studies, Vaswani et al. (2017) propose the\nTransformer, a neural architecture that abandons\nrecurrence and convolution. It fully relies on at-\ntention networks to model translation. The prop-\nerties of parallelization and short dependency path\nsigniﬁcantly improve the training speed as well as\nmodel performance for the Transformer. Unfortu-\nnately, as we have mentioned in Section 1, it suf-\nfers from decoding inefﬁciency.\nThe attention mechanism is originally proposed\nto induce translation-relevant source information\nfor predicting next target word in NMT. It con-\ntributes a lot to make NMT outperform SMT. Re-\ncently, a variety of efforts are made to further im-\nprove its accuracy and capability. Luong et al.\n(2015) explore several attention formulations and\ndistinguish local attention from global attention.\nZhang et al. (2016) treat RNN as an alternative\nto the attention to improve model’s capability in\ndealing with long-range dependencies. Yang et al.\n(2017) introduce a recurrent cycle on the atten-\ntion layer to enhance the model’s memorization\nof previous translated source words. Zhang et al.\n(2017a) observe the weak discrimination ability of\nthe attention-generated context vectors and pro-\npose a GRU-gated attention network. Kim et al.\n(2017) further model intrinsic structures inside at-\ntention through graphical models. Shen et al.\n(2017) introduce a direction structure into a self-\nattention network to integrate both long-range de-\npendencies and temporal order information. Mi\net al. (2016) and Liu et al. (2016) employ stan-\ndard word alignment to supervise the automati-\ncally generated attention weights. Our work also\nfocus on the evolution of attention network, but\nunlike previous work, we seek to simplify the self-\nattention network so as to accelerate the decoding\nprocedure. The design of our model is partially in-\nspired by the highway network (Srivastava et al.,\n2015) and the residual network (He et al., 2015).\nIn the respect of speeding up the decoding of\nthe neural Transformer, Gu et al. (2018) change\nthe auto-regressive architecture to speed up trans-\nlation by directly generating target words with-\nout relying on any previous predictions. However,\ncompared with our work, their model achieves the\nimprovement in decoding speed at the cost of the\ndrop in translation quality. Our model, instead,\nnot only achieves a remarkable gain in terms of\ndecoding speed, but also preserves the translation\nperformance. Developing fast and efﬁcient atten-\ntion module for the Transformer, to the best of our\nknowledge, has never been investigated before.\n3 The Average Attention Network\nGiven an input layery = {y1,y2,..., ym}, AAN\nﬁrst employs a cumulative-average operation to\ngenerate context-sensitive representation for each\ninput embedding as follows (Figure 2 Average\nLayer):\ngj = FFN\n(\n1\nj\nj∑\nk=1\nyk\n)\n(1)\nwhere FFN (·) denotes the position-wise feed-\nforward network proposed by Vaswani et al.\n(2017), and both yk and gj have a dimension-\nality of d. Intuitively, AAN replaces the orig-\ninal dynamically computed attention weights by\nthe self-attention network in the decoder of the\nneural Transformer with simple and ﬁxed aver-\nage weights ( 1\nj ). In spite of its simplicity, the\ncumulative-average operation is very crucial for\nAAN because it builds up dependencies with pre-\nvious input embeddings so that the generated rep-\nresentations are not independent of each other.\nAnother beneﬁt from the cumulative-average op-\neration is that no matter how long the input is, the\nconnection strength with each previous input em-\nbedding is invariant, which ensures the capability\nof AAN in modeling long-range dependencies.\nWe treat gj as a contextual representation for\nthe j-th input, and apply a feed-forward gating\nlayer upon it as well as yj to enrich the non-linear\nexpressiveness of AAN:\nij,fj = σ(W [yj; gj])\n˜hj = ij ⊙yj + fj ⊙gj\n(2)\nwhere [·; ·] denotes concatenation operation, and\n⊙indicates element-wise multiplication. ij and fj\nare the input and forget gate respectively. Via this\ngating layer, AAN can control how much past in-\nformation can be preserved from previous context\ngj and how much new information can be captured\nfrom current input yj. This helps our model to de-\ntect correlations inside input embeddings.\nFollowing the architecture design in the neural\nTransformer (Vaswani et al., 2017), we employ a\nresidual connection between the input layer and\ngating layer, followed by layer normalization to\nstabilize the scale of both output and gradient:\nhj = LayerNorm\n(\nyj + ˜hj\n)\n(3)\nWe refer to the whole procedure formulated in Eq.\n(1∼3) as original AAN (·) in following sections.\n3.1 Parallelization in Training\nA computation bottleneck of the original AAN de-\nscribed above is that the cumulative-average oper-\nation in Eq. (1) can only be performed sequen-\ntially. That is, this operation can not be paral-\nlelized. Fortunately, as the average is not a com-\nplex computation, we can use a masking trick to\nenable full parallelization of this operation.\nWe show the masking trick in Figure 3, where\ninput embeddings are directly converted into\ntheir corresponding cumulative-averaged outputs\nModel Complexity Sequential Operations Maximum Path Length\nSelf-attention O\n(\nn2 ·d + n ·d2)\nO(1) O(1)\nOriginal AAN O\n(\nn ·d2)\nO(n) O(1)\nMasked AAN O\n(\nn2 ·d + n ·d2)\nO(1) O(1)\nTable 1: Maximum path lengths, model complexity and minimum number of sequential operations for\ndifferent models. nis the sentence length and dis the representation dimension.\nMask Matrix\nFigure 3: Visualization of parallel implementa-\ntion for the cumulative-average operation enabled\nby a mask matrix. {y1,y2,y3,y4}are the input\nembeddings.\nthrough a masking matrix. In this way, all the\ncomponents inside AAN (·) can enjoy full par-\nallelization, assuring its computational efﬁciency.\nWe refer to this AAN as masked AAN.\n3.2 Model Analysis\nIn this section, we provide a thorough analysis for\nAAN in comparison to the original self-attention\nmodel used by Vaswani et al. (2017). Unlike our\nAAN, the self-attention model leverages a scaled\ndot-product function rather than the average oper-\nation to compute attention weights:\nQ,K,V = f(Y)\nSelf-Attention (Q,K,V) =softmax\n(QKT\n√\nd\n)\nV\n(4)\nwhere Y ∈ Rn×d is the input matrix, f(·) is a\nmapping function and Q,K,V ∈Rn×d are the\ncorresponding queries, keys and values. Follow-\ning Vaswani et al. (2017), we compare both mod-\nels in terms of computational complexity, min-\nimum number of sequential operations required\nand maximum path length that a dependency sig-\nnal between any two positions has to traverse in\nthe network. Table 1 summarizes the comparison\nresults.\nOur AAN has a maximum path length of O(1),\nbecause it can directly capture dependencies be-\ntween any two input embeddings. For the original\nAAN, the nature of its sequential computation en-\nlarges its minimum number sequential operations\nto O(n). However, due to its lack of position-\nwise masked projection, it only consumes a com-\nputational complexity of O\n(\nn·d2)\n. By contrast,\nboth self-attention and masked AAN have a com-\nputational complexity of O\n(\nn2 ·d+ n·d2)\n, and\nrequire only O(1) sequential operation. Theoreti-\ncally, our masked AAN performs very similarly to\nthe self-attention according to Table 1. We there-\nfore use the masked version of AAN during train-\ning throughout all our experiments.\n3.3 Decoding Acceleration\nDiffering noticeably from the self-attention in the\nTransformer, our AAN can be accelerated in the\ndecoding phase via dynamic programming thanks\nto the simple average calculation. Particularly, we\ncan decompose Eq. (1) into the following two\nsteps:\n˜gj = ˜gj−1 + yj (5)\ngj = FFN\n(˜gj\nj\n)\n(6)\nwhere ˜g0 = 0. In doing so, our model can com-\npute the j-th input representation based on only\none previous state ˜gj−1, instead of relying on all\nprevious states as the self-attention does. In this\nway, our model can be substantially accelerated\nduring the decoding phase.\n4 Neural Transformer with AAN\nThe neural Transformer models translation\nthrough an encoder-decoder framework, with\neach layer involving an attention network fol-\nlowed by a feed forward network (Vaswani et al.,\n2017). We apply our masked AAN to replace\nthe self-attention network in its decoder part, and\nillustrate the overall architecture in Figure 4.\nGiven a source sentence x = {x1,x2,..., xn},\nthe Transformer leverages its encoder to induce\nsource-side semantics and dependencies so as to\nMulti-Head \nAttention\nFeed \nForward\nAdd & Norm\nInput \nEmbedding\nAdd & Norm\nPositional \nEncoding\nInputs\nMulti-Head \nAttention\nFeed \nForward\nAdd & Norm\nOutput \nEmbedding\nAdd & Norm\nPositional \nEncoding\nOutputs \n(shifted right)\nAverage \nAttention\nLinear\nSoftmax\nOutput \nProbabilities\nN x x N\nFigure 4: The new Transformer architecture with\nthe proposed average attention network.\nenable its decoder to recover the encoded informa-\ntion in a target language. The encoder is composed\nof a stack ofN = 6identical layers, each of which\nhas two sub-layers:\n˜hl = LayerNorm\n(\nhl−1 + MHAtt\n(\nhl−1,hl−1\n))\nhl = LayerNorm\n(\n˜hl + FFN\n(\n˜hl\n))\n(7)\nwhere the superscript l indicates layer depth, and\nMHAtt denotes the multi-head attention mecha-\nnism proposed by Vaswani et al. (2017).\nBased on the encoded source representation\nhN , the Transformer relies on its decoder to\ngenerate corresponding target translation y =\n{y1,y2,..., ym}. Similar to the encoder, the de-\ncoder also consists of a stack of N = 6 identical\nlayers. For each layer in our architecture, the ﬁrst\nsub-layer is our proposed average attention net-\nwork, aiming at capturing target-side dependen-\ncies with previous predicted words:\n˜sl = AAN\n(\nsl−1\n)\n(8)\nCarrying these dependencies, the decoder stacks\nanother two sub-layers to seek translation-relevant\nsource semantics for bridging the gap between the\nsource and target language:\nsl\nc = LayerNorm\n(\n˜sl + MHAtt\n(\n˜sl,hN\n))\nsl = LayerNorm\n(\nsl\nc + FFN\n(\nsl\nc\n)) (9)\nWe use subscript cto denote the source-informed\ntarget representation. Upon the top layer of this\ndecoder, translation is performed where a linear\ntransformation and softmax activation are applied\nto compute the probability of the next token based\non sN\nTo memorize position information, the Trans-\nformer augments its input layer h0 = x,s0 = y\nwith frequency-based positional encodings. The\nwhole model is a large, single neural network, and\ncan be trained on a large-scale bilingual corpus\nwith a maximum likelihood objective. We refer\nreaders to (Vaswani et al., 2017) for more details.\n5 Experiments\n5.1 WMT14 English-German Translation\nWe examine various aspects of our AAN on this\ntranslation task. The training data consist of\n4.5M sentence pairs, involving about 116M En-\nglish words and 110M German words. We used\nnewstest2013 as the development set for model se-\nlection, and newstest2014 as the test set. We eval-\nuated translation quality via case-sensitive BLEU\nmetric (Papineni et al., 2002).\n5.1.1 Model Settings\nWe applied byte pair encoding algorithm (Sen-\nnrich et al., 2016) to encode all sentences and\nlimited the vocabulary size to 32K. All out-of-\nvocabulary words were mapped to an unique to-\nken “unk”. We set the dimensionality dof all in-\nput and output layers to 512, and that of inner-\nFFN layer to 2048. We employed 8 parallel at-\ntention heads in both encoder and decoder lay-\ners. We batched sentence pairs together so that\nthey were approximately of the same length, and\neach batch had roughly 25000 source and target\ntokens. During training, we used label smoothing\nwith value ϵls = 0.1, attention dropout and resid-\nual dropout with a rate of p= 0.1. During decod-\ning, we employed beam search algorithm and set\nthe beam size to 4. Adam optimizer (Kingma and\nBa, 2015) with β1 = 0.9, β2 = 0.98 and ϵ= 10−9\nwas used to tune model parameters, and the learn-\ning rate was varied under a warm-up strategy with\nwarmup steps = 4000 (Vaswani et al., 2017).\nModel BLEU\nTransformer 26.37\nOur Model 26.31\nOur Model w/o FFN 26.05\nOur Model w/o Gate 25.91\nTable 2: Case-sensitive tokenized BLEU score\non WMT14 English-German translation. BLEU\nscores are calculated using multi-bleu.perl.\nThe maximum number of training steps was set to\n100K. Weights of target-side embedding and out-\nput weight matrix were tied for all models. We\nimplemented our model with masking tricks based\non the open-sourced thumt (Zhang et al., 2017b)2,\nand trained and evaluated all models on a single\nNVIDIA GeForce GTX 1080 GPU. For evalua-\ntion, we averaged last ﬁve models saved with an\ninterval of 1500 training steps.\n5.1.2 Translation Performance\nTable 2 reports the translation results. On the same\ndataset, the Transformer yields a BLEU score of\n26.37, while our model achieves 26.31. Both re-\nsults are almost the same with no signiﬁcant dif-\nference. Clearly, our model is capable of capturing\ncomplex translation correspondences so as to gen-\nerate high-quality translations as effective as the\nTransformer.\nWe also show an ablation study in terms of the\nFFN(·) network in Eq. (1) and the gating layer in\nEq. (2). Table 2 shows that without the FFN net-\nwork, the performance of our model drops 0.26\nBLEU points. This degeneration is enlarged to\n0.40 BLEU points when the gating layer is not\navailable. In order to reach comparable perfor-\nmance with the original Transformer, integrating\nboth components is desired.\n5.1.3 Analysis on Convergency\nDifferent neural architectures might require differ-\nent number of training steps to converge. In this\nsection, we testify whether our AAN would re-\nveal different characteristics with respect to con-\nvergency. We show the loss curve of both the\nTransformer and our model in Figure 5.\nSurprisingly, both model show highly similar\ntendency, and successfully converge in the end. To\ntrain a high-quality translation system, our model\nconsumes almost the same number of training\nsteps as the Transformer. This strongly suggests\n2https://github.com/thumt/THUMT\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n1 101 201 301 401 501 601 701 801 901\nLoss\nTraining Steps\nTransformer Our Model\nFigure 5: Convergence visualization. The\nhorizontal axis denotes training steps scaled by\n102, and the vertical axis indicates training loss.\nRoughly, our model converges similarly to the\nTransformer.\nTransformer Our Model △r\nTraining 0.2474 0.2464 1.00\nDecoding\nbeam=4 0.1804 0.0488 3.70\nbeam=8 0.3576 0.0881 4.06\nbeam=12 0.5503 0.1291 4.26\nbeam=16 0.7323 0.1700 4.31\nbeam=20 0.9172 0.2122 4.32\nTable 3: Time required for training and decod-\ning. Training denotes the number of global train-\ning steps processed per second; Decoding indi-\ncates the amount of time in seconds required for\ntranslating one sentence, which is averaged over\nthe whole newstest2014 dataset. △r shows the ra-\ntio between the Transformer and our model.\nthat replacing the self-attention network with our\nAAN does not have negative impact on the con-\nvergency of the entire model.\n5.1.4 Analysis on Speed\nIn Section 3, we demonstrate in theory that our\nAAN is as efﬁcient as the self-attention during\ntraining, but can be substantially accelerated dur-\ning decoding. In this section, we provide quantita-\ntive evidences to examine this point.\nWe show the training and decoding speed of\nboth the Transformer and our model in Table\n3. During training, our model performs approx-\nimately 0.2464 training steps per second, while\nthe Transformer processes around 0.2474. This\nindicates that our model shares similar computa-\ntional strengths with the Transformer during train-\ning, which resonates with the computational anal-\nysis in Section 3.\nWhen it comes to decoding procedure, the time\nof our model required to translate one sentence\n(0,8)\n[16\n,24)\n[32\n,40)\n[48\n,56)\n[64\n,72)\n20\n25\n30\n35\n40\nSentence Length\nBLEU Scores\nT ransformer\nOur Model\n1\n(0,8)\n[16\n,24)\n[32\n,40)\n[48\n,56)\n[64\n,72)\n20\n40\n60\nSentence Length\nA verage Length of T ranslation\nT ransformer\nOur Model\n1\nFigure 6: Translation statistics on WMT14\nEnglish-German test set (newstest14) with respect\nto the length of source sentences. The top ﬁg-\nure shows tokenized BLEU score, and the bottom\none shows the average length of translations, both\nvisa-vis sentence length\nis only a quarter of that of the Transformer, with\nbeam size ranging from 4 to 20. Another notice-\nable feature is that as the beam size increases, the\nratio of required decoding time between the Trans-\nformer and our model is consistently enlarged.\nThis demonstrates empirically that our model, en-\nhanced with the dynamic decoding acceleration al-\ngorithm (Section 3.3), can signiﬁcantly improve\nthe decoding speed of the Transformer.\n5.1.5 Effects on Sentence Length\nA serious common challenge for NMT is to\ntranslate long source sentences as handling long-\ndistance dependencies and under-translation is-\nsues becomes more difﬁcult for longer sentences.\nOur proposed AAN uses simple cumulative-\naverage operations to deal with long-range depen-\n(0,8)\n[16\n,24)\n[32\n,40)\n[48\n,56)\n[64\n,72)\n0\n0.2\n0.4\n0.6\n0.8\n1\nSentence Length\nDecoding Time per Sentence (seconds)\nT ransformer\nOur Model\n1\nFigure 7: Average time required for translating\none source sentence vs. the length of the source\nsentence. With the increase of sentence length,\nour model shows more clear and signiﬁcant advan-\ntage over the Transformer in terms of the decoding\nspeed.\ndencies. We want to examine the effectiveness of\nthese operations on long sentence translation. For\nthis, we provide the translation results along sen-\ntence length in Figure 6.\nWe ﬁnd that both the Transformer and our\nmodel generate very similar translations in terms\nof BLEU score and translation length, and obtain\nrather promising performance on long source sen-\ntences. More speciﬁcally, our model yields rel-\natively shorter translation length on the longest\nsource sentences but signiﬁcantly better transla-\ntion quality. This suggests that in spite of the sim-\nplicity of the cumulative-average operations, our\nAAN can indeed capture long-range dependences\ndesired for translating long source sentences.\nGenerally, the decoder takes more time for\ntranslating longer sentences. When it comes to\nthe Transformer, this time issue of translating long\nsentences becomes notably severe as all previous\npredicted words must be included for estimating\nboth self-attention weights and word prediction.\nWe show the average time required for translat-\ning a source sentence with respect to its sentence\nlength in Figure 7. Obviously, the decoding time\nof the Transformer grows dramatically with the in-\ncrease of sentence length, while that of our model\nrises rather slowly. We contribute this great decod-\ning advantage of our model over the Transformer\nto the average attention architecture which enables\nCase-sensitive BLEU Case-insensitive BLEU\nwinner Transformer Our Model △d winner Transformer Our Model △d\nEn→De 28.3 27.33 27.22 -0.11 28.9 27.92 27.80 -0.12\nDe→En 35.1 32.63 32.73 +0.10 36.5 34.06 34.13 +0.07\nEn→Fi 20.7 21.00 20.87 -0.13 21.1 21.54 21.47 -0.07\nFi→En 20.5 25.19 24.78 -0.41 21.4 26.22 25.74 -0.48\nEn→Lv 21.1 16.83 16.63 -0.20 21.6 17.42 17.23 -0.19\nLv→En 21.9 17.57 17.51 -0.06 22.9 18.48 18.30 -0.18\nEn→Ru 29.8 27.82 27.73 -0.09 29.8 27.83 27.74 -0.09\nRu→En 34.7 31.51 31.36 -0.15 35.6 32.59 32.36 -0.23\nEn→Tr 18.1 12.11 11.59 -0.52 18.4 12.56 12.03 -0.53\nTr→En 20.1 16.19 15.84 -0.35 20.9 16.93 16.57 -0.36\nEn→Cs 23.5 21.53 21.12 -0.41 24.1 22.07 21.66 -0.41\nCs→En 30.9 27.49 27.45 -0.04 31.9 28.41 28.33 -0.08\nTable 4: Detokenized BLEU scores for WMT17 translation tasks. Results are reported with multi-bleu-\ndetok.perl. “ winner” denotes the translation results generated by the WMT17 winning systems. △d\nindicates the difference between our model and the Transformer.\nour model to perform next-word prediction by cal-\nculating information just from the previous hid-\nden state, rather than considering all previous in-\nputs like the self-attention in the Transformer’s de-\ncoder.\n5.2 WMT17 Translation Tasks\nWe further demonstrate the effectiveness of our\nmodel on six WMT17 translation tasks in both di-\nrections (12 translation directions in total). These\ntasks contain the following language pairs:\n•En-De: The English-German language pair.\nThis training corpus consists of 5.85M sen-\ntence pairs, with 141M English words and\n135M German words. We used the concate-\nnation of newstest2014, newstest2015 and\nnewstest2016 as the development set, and the\nnewstest2017 as the test set.\n•En-Fi: The English-Finnish language pair.\nThis training corpus consists of 2.63M sen-\ntence pairs, with 63M English words and\n45M Finnish words. We used the concate-\nnation of newstest2015, newsdev2015, new-\nstest2016 and newstestB2016 as the develop-\nment set, and the newstest2017 as the test set.\n•En-Lv: The English-Latvian language pair.\nThis training corpus consists of 4.46M sen-\ntence pairs, with 63M English words and\n52M Latvian words. We used the news-\ndev2017 as the development set, and the new-\nstest2017 as the test set.\n•En-Ru: The English-Russian language pair.\nThis training corpus consists of 25M sen-\ntence pairs, with 601M English words and\n567M Russian words. We used the concate-\nnation of newstest2014, newstest2015 and\nnewstest2016 as the development set, and the\nnewstest2017 as the test set.\n•En-Tr: The English-Turkish language pair.\nThis training corpus consists of 0.21M sen-\ntence pairs, with 5.2M English words and\n4.6M Turkish words. We used the concate-\nnation of newsdev2016 and newstest2016 as\nthe development set, and newstest2017 as the\ntest set.\n•En-Cs: The English-Czech language pair.\nThis training corpus consists of 52M sen-\ntence pairs, with 674M English words and\n571M Czech words. We used the concatena-\ntion of newstest2014, newstest2015 and new-\nstest2016 as the development set, and the\nnewstest2017 as the test set.\nInterestingly, these translation tasks involves train-\ning corpora with different scales (ranging from\n0.21M to 52M sentence pairs). This help us thor-\noughly examine the ability of our model on differ-\nent sizes of training data. All these preprocessed\ndatasets are publicly available, and can be down-\nloaded from WMT17 ofﬁcial website.3\nWe used the same modeling settings as in the\nWMT14 English-German translation task except\nfor the number of training steps for En-Fi and En-\nTr, which we set to 60K and 10K respectively.\nIn addition, to compare with ofﬁcial results, we\nreported both case-sensitive and case-insensitive\ndetokenized BLEU scores.\n3http://data.statmt.org/wmt17/translation-\ntask/preprocessed/\nTransformer Our Model △r\nEn→De 0.1411 0.02871 4.91\nDe→En 0.1255 0.02422 5.18\nEn→Fi 0.1289 0.02423 5.32\nFi→En 0.1285 0.02336 5.50\nEn→Lv 0.1850 0.03167 5.84\nLv→En 0.1980 0.03123 6.34\nEn→Ru 0.1821 0.03140 5.80\nRu→En 0.1595 0.02778 5.74\nEn→Tr 0.2078 0.02968 7.00\nTr→En 0.1886 0.03027 6.23\nEn→Cs 0.1150 0.02425 4.74\nCs→En 0.1178 0.02659 4.43\nTable 5: Average seconds required for decoding\none source sentence on WMT17 translation tasks.\n5.2.1 Translation Results\nTable 4 shows the overall results on 12 transla-\ntion directions. We also provide the results from\nWMT17 winning systems4. Notice that unlike the\nTransformer and our model, these winner systems\ntypically use model ensemble, system combina-\ntion and large-scale monolingual corpus.\nAlthough different languages have different lin-\nguistic and syntactic structures, our model con-\nsistently yields rather competitive results against\nthe Transformer on all language pairs in both di-\nrections. Particularly, on the De →En translation\ntask, our model achieves a slight improvement\nof 0.10/0.07 case-sensitive/case-insensitive BLEU\npoints over the Transformer. The largest perfor-\nmance gap between our model and the Trans-\nformer occurs on the En →Tr translation task,\nwhere our model is lower than the Transformer by\n0.52/0.53 case-sensitive/case-insensitive BLEU\npoints. We conjecture that this difference may be\ndue to the small training corpus of the En-Tr task.\nIn all, these results suggest that our AAN is able\nto perform comparably to Transformer on differ-\nent language pairs with different scales of training\ndata.\nWe also show the decoding speed of both the\nTransformer and our model in Table 5. On all lan-\nguages in both directions, our model yields signif-\nicant and consistent improvements over the Trans-\nformer in terms of decoding speed. Our model\ndecodes more than 4 times faster than the Trans-\nformer. Surprisingly, our model just consumes\n0.02968 seconds to translate one source sentence\non the En →Tr language pair, only a seventh of\nthe decoding time of the Transformer. These re-\nsults show that the beneﬁt of decoding accelera-\n4http://matrix.statmt.org/matrix\ntion from the proposed average attention structure\nis language-invariant, and can be easily adapted to\nother translation tasks.\n6 Conclusion and Future Work\nIn this paper, we have described the average at-\ntention network that considerably alleviates the\ndecoding bottleneck of the neural Transformer.\nOur model employs a cumulative average oper-\nation to capture important contextual clues from\nprevious target words, and a feed forward gat-\ning layer to enrich the expressiveness of learned\nhidden representations. The model is further en-\nhanced with a masking trick and a dynamic pro-\ngramming method to accelerate the Transformer’s\ndecoder. Extensive experiments on one WMT14\nand six WMT17 language pairs demonstrate that\nthe proposed average attention network is able\nto speed up the Transformer’s decoder by over 4\ntimes.\nIn the future, we plan to apply our model on\nother sequence to sequence learning tasks. We will\nalso attempt to improve our model to enhance its\nmodeling ability so as to consistently outperform\nthe original neural Transformer.\n7 Acknowledgments\nThe authors were supported by Beijing Advanced\nInnovation Center for Language Resources, Na-\ntional Natural Science Foundation of China (Nos.\n61672440 and 61622209), the Fundamental Re-\nsearch Funds for the Central Universities (Grant\nNo. ZK1024), and Scientiﬁc Research Project of\nNational Language Committee of China (Grant\nNo. YB135-49). Biao Zhang greatly acknowl-\nedges the support of the Baidu Scholarship. We\nalso thank the reviewers for their insightful com-\nments.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2015. Neural machine translation by jointly\nlearning to align and translate. In Proc. of ICLR.\nJunyoung Chung, C ¸ aglar G¨ulc ¸ehre, KyungHyun Cho,\nand Yoshua Bengio. 2014. Empirical evaluation of\ngated recurrent neural networks on sequence model-\ning. CoRR.\nJonas Gehring, Michael Auli, David Grangier, and\nYann N. Dauphin. 2017a. A convolutional encoder\nmodel for neural machine translation. In Proc. of\nACL, pages 123–135.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017b. Convolutional\nsequence to sequence learning. Proc. of ICML.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor OK\nLi, and Richard Socher. 2018. Non-autoregressive\nneural machine translation. Proc. of ICLR.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Deep residual learning for image recog-\nnition. CoRR, abs/1512.03385.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nshort-term memory. Neural Comput., 9:1735–1780.\nYoon Kim, Carl Denton, Luong Hoang, and Alexan-\nder M. Rush. 2017. Structured attention networks.\nProc. of ICLR, abs/1702.00887.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. Proc. of ICLR.\nLemao Liu, Masao Utiyama, Andrew Finch, and Ei-\nichiro Sumita. 2016. Neural machine translation\nwith supervised attention. In Proc. of COLING\n2016, pages 3093–3102, Osaka, Japan. The COL-\nING 2016 Organizing Committee.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proc. of EMNLP,\npages 1412–1421.\nHaitao Mi, Zhiguo Wang, and Abe Ittycheriah. 2016.\nSupervised attentions for neural machine transla-\ntion. In Proc. of EMNLP, pages 2283–2288, Austin,\nTexas. Association for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proc. of ACL, pages\n311–318.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proc. of ACL, pages 1715–1725.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,\nShirui Pan, and Chengqi Zhang. 2017. Disan: Di-\nrectional self-attention network for rnn/cnn-free lan-\nguage understanding. CoRR, abs/1709.04696.\nRupesh Kumar Srivastava, Klaus Greff, and J ¨urgen\nSchmidhuber. 2015. Highway networks. CoRR,\nabs/1505.00387.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Z. Ghahramani, M. Welling, C. Cortes,\nN. D. Lawrence, and K. Q. Weinberger, editors, Ad-\nvances in Neural Information Processing Systems\n27, pages 3104–3112. Curran Associates, Inc.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran As-\nsociates, Inc.\nZichao Yang, Zhiting Hu, Yuntian Deng, Chris Dyer,\nand Alex Smola. 2017. Neural machine transla-\ntion with recurrent attention modeling. In Proc. of\nEACL, pages 383–387, Valencia, Spain. Association\nfor Computational Linguistics.\nBiao Zhang, Deyi Xiong, and Jinsong Su. 2016.\nRecurrent neural machine translation. CoRR,\nabs/1607.08725.\nBiao Zhang, Deyi Xiong, and Jinsong Su. 2017a. A\ngru-gated attention model for neural machine trans-\nlation. CoRR, abs/1704.08430.\nJiacheng Zhang, Yanzhuo Ding, Shiqi Shen, Yong\nCheng, Maosong Sun, Huan-Bo Luan, and Yang\nLiu. 2017b. THUMT: an open source toolkit for\nneural machine translation. CoRR, abs/1706.06415."
}