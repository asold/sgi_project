{
    "title": "Can You Tell Me How to Get Past Sesame Street? Sentence-Level Pretraining Beyond Language Modeling",
    "url": "https://openalex.org/W2951335443",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2327300082",
            "name": "Wang, Alex",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4288864405",
            "name": "Hula, Jan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227346703",
            "name": "Xia, Patrick",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287072624",
            "name": "Pappagari, Raghavendra",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227220491",
            "name": "McCoy, R. Thomas",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4286986043",
            "name": "Patel, Roma",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4302638862",
            "name": "Kim, Najoung",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4281591827",
            "name": "Tenney, Ian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2347499903",
            "name": "Huang Yinghui",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Yu, Katherin",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Jin, Shuning",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2752626035",
            "name": "Chen, Berlin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222221224",
            "name": "Van Durme, Benjamin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222857661",
            "name": "Grave, Edouard",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227228559",
            "name": "Pavlick, Ellie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221820538",
            "name": "Bowman, Samuel R.",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963842982",
        "https://openalex.org/W2951714314",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2970352191",
        "https://openalex.org/W2953369973",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2963090765",
        "https://openalex.org/W2257408573",
        "https://openalex.org/W2911300548",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W806995027",
        "https://openalex.org/W2794365787",
        "https://openalex.org/W2798512429",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2761988601",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2893141505",
        "https://openalex.org/W2158899491",
        "https://openalex.org/W2950133940",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W2159636675",
        "https://openalex.org/W2963804993",
        "https://openalex.org/W2823875838",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2250473257",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963149412",
        "https://openalex.org/W2798284509",
        "https://openalex.org/W2610858497",
        "https://openalex.org/W1486649854",
        "https://openalex.org/W2963918774",
        "https://openalex.org/W2548036585",
        "https://openalex.org/W2898700502",
        "https://openalex.org/W2793978524",
        "https://openalex.org/W2964331819",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2250861254",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W2888329843",
        "https://openalex.org/W2525127255",
        "https://openalex.org/W131533222",
        "https://openalex.org/W2963241825",
        "https://openalex.org/W1599016936",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2963854351",
        "https://openalex.org/W2907849599",
        "https://openalex.org/W2911681509",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2963756346",
        "https://openalex.org/W2952729433",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W2771275742",
        "https://openalex.org/W2785523195",
        "https://openalex.org/W2551396370",
        "https://openalex.org/W2963264012",
        "https://openalex.org/W2592170186",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2963748441"
    ],
    "abstract": "Natural language understanding has recently seen a surge of progress with the use of sentence encoders like ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2019) which are pretrained on variants of language modeling. We conduct the first large-scale systematic study of candidate pretraining tasks, comparing 19 different tasks both as alternatives and complements to language modeling. Our primary results support the use language modeling, especially when combined with pretraining on additional labeled-data tasks. However, our results are mixed across pretraining tasks and show some concerning trends: In ELMo's pretrain-then-freeze paradigm, random baselines are worryingly strong and results vary strikingly across target tasks. In addition, fine-tuning BERT on an intermediate task often negatively impacts downstream transfer. In a more positive trend, we see modest gains from multitask training, suggesting the development of more sophisticated multitask and transfer learning techniques as an avenue for further research.",
    "full_text": "Can You Tell Me How to Get Past Sesame Street?\nSentence-Level Pretraining Beyond Language Modeling\nAlex Wang,∗1 Jan Hula,2 Patrick Xia,2 Raghavendra Pappagari,2\nR. Thomas McCoy,2 Roma Patel,3 Najoung Kim,2 Ian Tenney,4 Yinghui Huang,6\nKatherin Yu,5 Shuning Jin,7 Berlin Chen,8 Benjamin Van Durme,2 Edouard Grave,5\nEllie Pavlick,3,4 and Samuel R. Bowman1\n1New York University,2Johns Hopkins University, 3Brown University,4Google AI Language,\n5Facebook, 6IBM, 7University of Minnesota Duluth, 8Swarthmore College\nAbstract\nNatural language understanding has recently\nseen a surge of progress with the use of\nsentence encoders like ELMo (Peters et al.,\n2018a) and BERT (Devlin et al., 2019) which\nare pretrained on variants of language mod-\neling. We conduct the ﬁrst large-scale sys-\ntematic study of candidate pretraining tasks,\ncomparing 19 different tasks both as alter-\nnatives and complements to language model-\ning. Our primary results support the use lan-\nguage modeling, especially when combined\nwith pretraining on additional labeled-data\ntasks. However, our results are mixed across\npretraining tasks and show some concern-\ning trends: In ELMo’s pretrain-then-freeze\nparadigm, random baselines are worryingly\nstrong and results vary strikingly across tar-\nget tasks. In addition, ﬁne-tuning BERT on\nan intermediate task often negatively impacts\ndownstream transfer. In a more positive trend,\nwe see modest gains from multitask training,\nsuggesting the development of more sophis-\nticated multitask and transfer learning tech-\nniques as an avenue for further research.\n1 Introduction\nState-of-the-art models in natural language pro-\ncessing (NLP) often incorporate encoder func-\ntions which generate a sequence of vectors in-\ntended to represent the in-context meaning of each\nword in an input text. These encoders have typ-\nically been trained directly on the target task at\nhand, which can be effective for data-rich tasks\nand yields human performance on some narrowly-\ndeﬁned benchmarks (Rajpurkar et al., 2018; Has-\nsan et al., 2018), but is tenable only for the few\ntasks with millions of training data examples. This\n∗This paper supercedes “Looking for ELMo’s Friends:\nSentence-Level Pretraining Beyond Language Modeling”, an\nearlier version of this work by the same authors. Correspon-\ndence to: alexwang@nyu.edu\nInput Text\nIntermediate Task Model\nIntermediate Task Output\nBERT\nInput Text\nIntermediate Task Model\nIntermediate Task Output\nELMo\nBiLSTM\nInput Text\nTarget Task Model\nTarget Task Output\nIntermediate Task-Trained BERT\nInput Text\nPretraining Task Model\nPretraining Task Output\nBiLSTM\nInput Text\nTarget Task Model\nTarget Task Output\nPretrained BiLSTM\nInput Text\nTarget Task Model\nTarget Task Output\nELMo\nIntermediate Task-Trained BiLSTM\n❄\n❄\n❄❄\nFigure 1: Learning settings that we consider. Model\ncomponents with frozen parameters are shown in gray\nand decorated with snowﬂakes. Top (pretraining): We\npretrain a BiLSTM on a task (left), and learn a target\ntask model on top of the representations it produces\n(right). Middle (intermediate ELMo training): We\ntrain a BiLSTM on top of ELMo for an intermediate\ntask (left). We then train a target task model on top of\nthe intermediate task BiLSTM and ELMo (right). Bot-\ntom (intermediate BERT training): We ﬁne-tune BERT\non an intermediate task (left), and then ﬁne-tune the re-\nsulting model again on a target task (right).\nlimitation has prompted interest in pretraining for\nthese encoders: The encoders are ﬁrst trained on\noutside data, and then plugged into a target task\nmodel.\nHoward and Ruder (2018), Peters et al. (2018a),\nRadford et al. (2018), and Devlin et al. (2019)\nestablish that encoders pretrained on variants of\nthe language modeling task can be reused to yield\nstrong performance on downstream NLP tasks.\nSubsequent work has homed in on language mod-\neling (LM) pretraining, ﬁnding that such mod-\narXiv:1812.10860v5  [cs.CL]  22 Jul 2019\nels can be productively ﬁne-tuned on intermedi-\nate tasks like natural language inference before\ntransferring to downstream tasks (Phang et al.,\n2018). However, we identify two open ques-\ntions: (1) How effective are tasks beyond lan-\nguage modeling in training reusable sentence en-\ncoders (2) Given the recent successes of LMs with\nintermediate-task training, which tasks can be ef-\nfectively combined with language modeling and\neach other.\nThe main contribution of this paper is a large-\nscale systematic study of these two questions. For\nthe ﬁrst question, we train reusable sentence en-\ncoders on 19 different pretraining tasks and task\ncombinations and several simple baselines, us-\ning a standardized model architecture and proce-\ndure for pretraining. For the second question,\nwe conduct additional pretraining on ELMo (Pe-\nters et al., 2018b) and BERT (Devlin et al., 2019)\nwith 17 different intermediate tasks and task com-\nbinations. We evaluate each of these encoders\non the nine target language-understanding tasks in\nthe GLUE benchmark (Wang et al., 2019), yield-\ning a total of 53 sentence encoders and 477 total\ntrained models. We measure correlation in perfor-\nmance across target tasks and plot learning curves\nto show the effect of data volume on both pretrain-\ning and target task training.\nWe ﬁnd that language modeling is the most\neffective pretraining task that we study. Multi-\ntask pretraining or intermediate task training of-\nfers modest further gains. However, we see several\nworrying trends:\n• The margins between substantially different\npretraining tasks can be extremely small in\nthis transfer learning regimen and many pre-\ntraining tasks struggle to outperform trivial\nbaselines.\n• Many of the tasks used for intermediate task\ntraining adversely impact the transfer ability\nof LM pretraining.\n• Different target tasks differ dramatically in\nwhat kinds of pretraining they beneﬁt most\nfrom, but na ¨ıve multitask pretraining seems\nineffective at combining the strengths of dis-\nparate pretraining tasks.\nThese observations suggest that while scaling up\nLM pretraining (as in Radford et al., 2019) is\nlikely the most straightforward path to further\ngains, our current methods for multitask and trans-\nfer learning may be substantially limiting our re-\nsults.\n2 Related Work\nWork on reusable sentence encoders can be traced\nback at least as far as the multitask model of Col-\nlobert et al. (2011). Several works focused on\nlearning reusable sentence-to-vector encodings,\nwhere the pretrained encoder produces a ﬁxed-size\nrepresentation for each input sentence (Dai and\nLe, 2015; Kiros et al., 2015; Hill et al., 2016; Con-\nneau et al., 2017). More recent reusable sentence\nencoders such as CoVe (McCann et al., 2017) and\nGPT (Radford et al., 2018) instead represent sen-\ntences as sequences of vectors. These methods\nwork well, but most use distinct pretraining objec-\ntives, and none offers a substantial investigation of\nthe choice of objective like we conduct here.\nWe build on two methods for pretraining sen-\ntence encoders on language modeling: ELMo and\nBERT. ELMo consists of a forward and back-\nward LSTM (Hochreiter and Schmidhuber, 1997),\nthe hidden states of which are used to produce\na contextual vector representation for each token\nin the inputted sequence. ELMo is adapted to\ntarget tasks by freezing the model weights and\nonly learning a set of task-speciﬁc scalar weights\nthat are used to compute a linear combination of\nthe LSTM layers. BERT consists of a pretrained\nTransformer (Vaswani et al., 2017), and is adapted\nto downstream tasks by ﬁne-tuning the entire\nmodel. Follow-up work has explored parameter-\nefﬁcient ﬁne-tuning (Stickland and Murray, 2019;\nHoulsby et al., 2019) and better target task adapta-\ntion via multitask ﬁne-tuning (Phang et al., 2018;\nLiu et al., 2019), but work in this area is nascent.\nThe successes of sentence encoder pretrain-\ning have sparked a line of work analyzing these\nmodels (Zhang and Bowman, 2018; Peters et al.,\n2018b; Tenney et al., 2019b; Peters et al., 2019;\nTenney et al., 2019a; Liu et al., 2019, i.a.). Our\nwork also attempts to better understand what is\nlearned by pretrained encoders, but we study this\nquestion entirely through the lens of pretraining\nand ﬁne-tuning tasks, rather than architectures or\nspeciﬁc linguistic capabilities. Some of our exper-\niments resemble those of Yogatama et al. (2019),\nwho also empirically investigate transfer perfor-\nmance with limited amounts of data and ﬁnd sim-\nilar evidence of catastrophic forgetting.\nMultitask representation learning in NLP is well\nstudied, and again can be traced back at least as\nfar as Collobert et al. (2011). Luong et al. (2016)\nshow promising results combining translation and\nparsing; Subramanian et al. (2018) beneﬁt from\nmultitask learning in sentence-to-vector encoding;\nand Bingel and Søgaard (2017) and Changpinyo\net al. (2018) offer studies of when multitask learn-\ning is helpful for lower-level NLP tasks.\n3 Transfer Paradigms\nWe consider two recent paradigms for transfer\nlearning: pretraining and intermediate training.\nSee Figure 1 for a graphical depiction.\nPretraining Our ﬁrst set of experiments is de-\nsigned to systematically investigate the effective-\nness of a broad range of tasks in pretraining sen-\ntence encoders. For each task, we ﬁrst train a\nrandomly initialized model to convergence on that\npretraining task, and then train a model for a tar-\nget task on top of the trained encoder. For these\nexperiments, we largely follow the procedure and\narchitecture used by ELMo rather than BERT, but\nwe expect similar trends with BERT-style models.\nIntermediate Training Given the robust suc-\ncess of LM pretraining, we explore methods of\nfurther improving on such sentence encoders. In\nparticular, we take inspiration from Phang et al.\n(2018), who show gains in ﬁrst ﬁne-tuning BERT\non an intermediate task, and then ﬁne-tuning again\non a target task. Our second set of experiments\ninvestigates which tasks can be used for interme-\ndiate training to augment LM pretraining. We de-\nsign experiments using both pretrained ELMo and\nBERT as the base encoder. When using ELMo, we\nfollow standard procedure and train a task-speciﬁc\nLSTM and output component (e.g. MLP for clas-\nsiﬁcation, decoder for sequence generation, etc.)\non top of the representations produced by ELMo.\nDuring this stage, the pretrained ELMo weights\nare frozen except for a set of layer mixing weights.\nWhen using BERT, we follow standard procedure\nand train a small task-speciﬁc output component\nusing the [CLS] output vector while also ﬁne-\ntuning the weights of the full BERT model.\nTarget Task Evaluation For our pretraining and\nintermediate ELMo experiments, to evaluate on a\ntarget task, we train a target task model on top\nof the representations produced by the encoder,\nTask |Train| Task Type\nGLUE Tasks\nCoLA 8.5K acceptability\nSST 67K sentiment\nMRPC 3.7K paraphrase detection\nQQP 364K paraphrase detection\nSTS 7K sentence similarity\nMNLI 393K NLI\nQNLI 105K QA (NLI)\nRTE 2.5K NLI\nWNLI 634 coreference resolution (NLI)\nOutside Tasks\nDisSent WT 311K discourse marker prediction\nLM WT 4M language modeling\nLM BWB 30M language modeling\nMT En-De 3.4M translation\nMT En-Ru 3.2M translation\nReddit 18M response prediction\nSkipThought 4M next sentence prediction\nTable 1: Tasks used for pretraining and intermediate\ntraining of sentence encoders. We also use the GLUE\ntasks as target tasks to evaluate the encoders. For the\nlanguage modeling (LM) tasks, we report the number\nof sentences in the corpora.\nwhich is again frozen throughout target task train-\ning except for a set of target-task-speciﬁc layer\nmixing weights. For our intermediate BERT ex-\nperiments, we follow the same procedure as in in-\ntermediate training: We train a target-task model\nusing the [CLS] representation and ﬁne-tune the\nencoder throughout target task training.\nWe use the nine target tasks in GLUE (Wang\net al., 2019) to evaluate each of the encoders we\ntrain. GLUE is an open-ended shared task compe-\ntition and evaluation toolkit for reusable sentence\nencoders, built around a set of nine sentence and\nsentence pairs tasks spanning a range of dataset\nsizes, paired with private test data and an online\nleaderboard. We evaluate each model on each of\nthe nine tasks, and report the resulting scores and\nthe GLUE score, a macro-average over tasks.\n4 Tasks\nOur experiments compare encoders pretrained or\nﬁne-tuned on a large number of tasks and task\ncombinations, where a task is a dataset–objective\nfunction pair. We select these tasks either to serve\nas baselines or because they have shown promise\nin prior work, especially in sentence-to-vector en-\ncoding. See Appendix A for details and tasks we\nexperimented with but which did not show strong\nenough performance to warrant a full evaluation.\nRandom Encoder A number of recent works\nhave noted that randomly initialized, untrained\nLSTMs can obtain surprisingly strong down-\nstream task performance (Zhang and Bowman,\n2018; Wieting and Kiela, 2019; Tenney et al.,\n2019b). Accordingly, our pretraining and inter-\nmediate ELMo experiments include a baseline of\na randomly initialized BiLSTM with no further\ntraining. This baseline is especially strong be-\ncause our ELMo-style models use a skip connec-\ntion from the input of the encoder to the output,\nallowing the task-speciﬁc component to see the\ninput representations, yielding a model similar to\nIyyer et al. (2015).\nGLUE Tasks We use the nine tasks included\nwith GLUE as pretraining and intermediate tasks:\nacceptability classiﬁcation with CoLA (Warstadt\net al., 2018); binary sentiment classiﬁcation with\nSST (Socher et al., 2013); semantic similarity with\nthe MSR Paraphrase Corpus ( MRPC; Dolan and\nBrockett, 2005), Quora Question Pairs 1 (QQP),\nand STS-Benchmark (STS; Cer et al., 2017); and\ntextual entailment with the Multi-Genre NLI Cor-\npus (MNLI Williams et al., 2018), RTE 1, 2, 3,\nand 5 (RTE; Dagan et al., 2006, et seq.), and data\nfrom SQuAD ( QNLI;2 Rajpurkar et al., 2016)\nand the Winograd Schema Challenge ( WNLI;\nLevesque et al., 2011) recast as entailment in the\nstyle of White et al. (2017). MNLI and QQP have\npreviously been shown to be effective for pretrain-\ning in other settings (Conneau et al., 2017; Sub-\nramanian et al., 2018; Phang et al., 2018). Other\ntasks are included to represent a broad sample of\nlabeling schemes commonly used in NLP.\nOutside Tasks We train language models on\ntwo datasets: WikiText-103 ( WT; Merity et al.,\n2017) and Billion Word Language Model Bench-\nmark (BWB; Chelba et al., 2013). Because rep-\nresentations from ELMo and BERT capture left\nand right context, they cannot be used in con-\njunction with unidirectional language modeling,\nso we exclude this task from intermediate train-\ning experiments. We train machine translation\n(MT) models on WMT14 English-German (Bo-\njar et al., 2014) and WMT17 English-Russian\n(Bojar et al., 2017). We train SkipThought-style\nsequence-to-sequence (seq2seq) models to read a\n1 data.quora.com/First-Quora-Dataset-\nRelease-Question-Pairs\n2QNLI has been re-released with updated splits since the\noriginal release. We use the original splits.\nsentence from WT and predict the following sen-\ntence (Kiros et al., 2015; Tang et al., 2017). We\ntrain DisSent models to read two clauses from WT\nthat are connected by a discourse marker such as\nand, but, or so and predict the the discourse marker\n(Jernite et al., 2017; Nie et al., 2019). Finally, we\ntrain seq2seq models to predict the response to a\ngiven comment from Reddit, using a previously\nexisting dataset obtained by a third party (available\non pushshift.io), comprised of 18M comment–\nresponse pairs from 2008-2011. This dataset was\nused by Yang et al. (2018) to train sentence en-\ncoders.\nMultitask Learning We consider three sets of\nthese tasks for multitask pretraining and interme-\ndiate training: all GLUE tasks, all non-GLUE\n(outside) tasks, and all tasks.\n5 Models and Experimental Details\nWe implement our models using the jiant\ntoolkit,3 which is in turn built on AllenNLP (Gard-\nner et al., 2017) and on a public PyTorch imple-\nmentation of BERT.4 Appendix A presents addi-\ntional details.\nEncoder Architecture For both the pretraining\nand intermediate ELMo experiments, we process\nwords using a pretrained character-level convolu-\ntional neural network (CNN) from ELMo. We use\nthis pretrained word encoder for pretraining exper-\niments to avoid potentially difﬁcult issues with un-\nknown word handling in transfer learning.\nFor the pretraining experiments, these input rep-\nresentations are fed to a two-layer 1024D bidirec-\ntional LSTM from which we take the sequence\nof hidden states from the top layer as the con-\ntextual representation. A task-speciﬁc model sees\nboth the top-layer hidden states of this model and,\nthrough a skip connection, the input token rep-\nresentations. For the intermediate ELMo experi-\nments, we compute contextual representations us-\ning the entire pretrained ELMo model, which are\npassed to a similar LSTM that is then trained on\nthe intermediate task. We also include a skip con-\nnection from the ELMo representations to the task\nspeciﬁc model. Our experiments with BERT use\nthe BASE case-sensitive version of the model.\n3https://github.com/nyu-mll/jiant/\ntree/bert-friends-exps\n4https://github.com/huggingface/\npytorch-pretrained-BERT\nPretr. Avg CoLA SST MRPC QQP STS MNLI QNLI RTE WNLI\nBaselines\nRandom 68.2 16.9 84.3 77.7/85.6 83.0/80.6 81.7/82.6 73.9 79.6 57.0 31.0*\nSingle-Task 69.1 21.3 89.0 77.2/84.7 84.7/81.9 81.4/82.2 74.8 78.8 56.0 11.3*\nGLUE Tasks as Pretraining Tasks\nCoLA 68.2 21.3 85.7 75.0/83.7 85.7/82.4 79.0/80.3 72.7 78.4 56.3 15.5*\nSST 68.6 16.4 89.0 76.0/84.2 84.4/81.6 80.6/81.4 73.9 78.5 58.8 19.7*\nMRPC 68.2 16.4 85.6 77.2/84.7 84.4/81.8 81.2/82.2 73.6 79.3 56.7 22.5*\nQQP 68.0 14.7 86.1 77.2/84.5 84.7/81.9 81.1/82.0 73.7 78.2 57.0 45.1*\nSTS 67.7 14.1 84.6 77.9/85.3 81.7/79.2 81.4/82.2 73.6 79.3 57.4 43.7*\nMNLI 69.1 16.7 88.2 78.9/85.2 84.5/81.5 81.8/82.6 74.8 79.6 58.8 36.6*\nQNLI 67.9 15.6 84.2 76.5/84.2 84.3/81.4 80.6/81.8 73.4 78.8 58.8 56.3\nRTE 68.1 18.1 83.9 77.5/85.4 83.9/81.2 81.2/82.2 74.1 79.1 56.0 39.4*\nWNLI 68.0 16.3 84.3 76.5/84.6 83.0/80.5 81.6/82.5 73.6 78.8 58.1 11.3*\nNon-GLUE Pretraining Tasks\nDisSent WT 68.6 18.3 86.6 79.9/86.0 85.3/82.0 79.5/80.5 73.4 79.1 56.7 42.3*\nLM WT 70.1 30.8 85.7 76.2/84.2 86.2/82.9 79.2/80.2 74.0 79.4 60.3 25.4*\nLM BWB 70.4 30.7 86.8 79.9/86.2 86.3/83.2 80.7/81.4 74.2 79.0 57.4 47.9*\nMT En-De 68.1 16.7 85.4 77.9/84.9 83.8/80.5 82.4/82.9 73.5 79.6 55.6 22.5*\nMT En-Ru 68.4 16.8 85.1 79.4/86.2 84.1/81.2 82.7/83.2 74.1 79.1 56.0 26.8*\nReddit 66.9 15.3 82.3 76.5/84.6 81.9/79.2 81.5/81.9 72.7 76.8 55.6 53.5*\nSkipThought 68.7 16.0 84.9 77.5/85.0 83.5/80.7 81.1/81.5 73.3 79.1 63.9 49.3*\nMultitask Pretraining\nMTL GLUE 68.9 15.4 89.9 78.9/86.3 82.6/79.9 82.9/83.5 74.9 78.9 57.8 38.0*\nMTL Non-GLUE 69.9 30.6 87.0 81.1/87.6 86.0/82.2 79.9/80.6 72.8 78.9 54.9 22.5*\nMTL All 70.4 33.2 88.2 78.9/85.9 85.5/81.8 79.7/80.0 73.9 78.7 57.4 33.8*\nTest Set Results\nLM BWB 66.5 29.1 86.9 75.0/82.1 82.7/63.3 74.0/73.1 73.4 68.0 51.3 65.1\nMTL All 68.5 36.3 88.9 77.7/84.8 82.7/63.6 77.8/76.7 75.3 66.2 53.2 65.1\nTable 2: Results for pretraining experiments on development sets except where noted. Bold denotes best result\noverall. Underlining denotes an average score surpassing the Random baseline. See Section 6 for discussion of\nWNLI results (*).\nTask-Speciﬁc Components We design task-\nspeciﬁc components to be as close to standard\nmodels for each task as possible. Though different\ncomponents may have varying parameter counts,\narchitectures, etc., we believe that results between\ntasks are still comparable and informative.\nFor BERT experiments we use the standard pre-\nprocessing and pass the representation of the spe-\ncial [CLS] representation to a logistic regression\nclassiﬁer. For seq2seq tasks (MT, SkipThought,\npushshift.io Reddit dataset) we replace the classi-\nﬁer with a single-layer LSTM word-level decoder\nand initialize the hidden state with the[CLS] rep-\nresentation.\nFor ELMo-style models, we use several model\ntypes:\n• Single-sentence classiﬁcation tasks : We\ntrain a linear projection over the output states\nof the encoder, max-pool those projected\nstates, and feed the result to an MLP.\n• Sentence-pair tasks: We perform the same\nsteps on both sentences and use the heuris-\ntic feature vector [h1; h2; h1 · h2; h1 − h2] in\nthe MLP, following Mou et al. (2016). When\ntraining target-task models on QQP, STS,\nMNLI, and QNLI, we use a cross-sentence\nattention mechanism similar to BiDAF (Seo\net al., 2017). We do not use this mechanism\nin other cases as early results indicated it hurt\ntransfer performance.\n• Seq2seq tasks (MT, SkipThought,\npushshift.io Reddit dataset): We use a\nsingle-layer LSTM decoder where the hid-\nden state is initialized with the pooled input\nrepresentation.\n• Language modeling: We follow ELMo by\nconcatenating forward and backward models\nand learning layer mixing weights.\nTo use GLUE tasks for pretraining or interme-\ndiate training in a way that is more comparable\nto outside tasks, after pretraining we discard the\nlearned GLUE classiﬁer, and initialize a new clas-\nsiﬁer from scratch for target-task training.\nTraining and Optimization For BERT experi-\nments, we train our models with the same opti-\nmizer and learning rate schedule as the original\nwork. For all other models, we train our mod-\nels with AMSGrad (Reddi et al., 2018). We do\nearly stopping using development set performance\nof the task we are training on. Typical experiments\n(pretraining or intermediate training of an encoder\nand training nine associated target-task models)\ntake 1–5 days to complete on an NVIDIA P100\nGPU.\nWhen training on multiple tasks, we randomly\nsample a task with probability proportional to its\ntraining data size raised to the power of 0.75. This\nsampling rate is meant to balance the risks of over-\nﬁtting small-data tasks and underﬁtting large ones,\nand performed best in early experiments. More\nextensive experiments with methods like this are\nshown in Appendix C. We perform early stopping\nbased on an average of the tasks’ validation met-\nrics.\nHyperparameters Appendix B lists the hyper-\nparameter values used. As our experiments re-\nquire more than 150 GPU-days on NVIDIA P100\nGPUs to run—not counting debugging or learning\ncurves—we do not have the resources for exten-\nsive tuning. Instead, we ﬁx most hyperparameters\nto commonly used values. The lack of tuning lim-\nits our ability to diagnose the causes of poor per-\nformance when it occurs, and we invite readers to\nfurther reﬁne our models using the public code.\n6 Results\nTables 2 and 3 respectively show results for our\npretraining and intermediate training experiments.\nThe Single-Task baselines train and evaluate a\nmodel on only the corresponding GLUE task. To\ncomply with GLUE’s limits on test set access,\nwe only evaluate the top few pretrained encoders.\nFor roughly comparable results in prior work, see\nWang et al. (2019) or www.gluebenchmark.\ncom; we omit them here in the interest of space.\nAs of writing, the best test result using a compa-\nrable frozen pretrained encoder is 70.0 from Wang\net al. (2019) for a model similar to our GLUE E,\nand the best overall published result is 85.2 from\nLiu et al. (2019) using a model similar to our\nGLUEB (below), but substantially larger.\nWhile it is not feasible to run each setting mul-\ntiple times, we estimate the variance of the GLUE\nscore by re-running three experiments ﬁve times\neach with different random seeds. We observe\nσ = 0.4 for the random encoder with no pretrain-\ning, σ = 0.2 for ELMo with intermediate MNLI\ntraining, and σ = 0.5 for BERT without inter-\nmediate training. This variation is substantial but\nmany of our results surpass a standard deviation of\nour baselines.\nThe WNLI dataset is both difﬁcult and adversar-\nial: The same hypotheses can be paired with dif-\nferent premises and opposite labels in the train and\ndevelopment sets, so models that overﬁt the train\nset (which happens quickly on the tiny training set)\noften show development set performance below\nchance, making early stopping and model selec-\ntion difﬁcult. Few of our models reached even the\nmost frequent class performance (56.3), and when\nevaluating models that do worse than this, we re-\nplace their predictions with the most frequent label\nto simulate the performance achieved by not mod-\neling the task at all.\n6.1 Pretraining\nFrom Table 2, among target tasks, we ﬁnd the\ngrammar-related CoLA task beneﬁts dramatically\nfrom LM pretraining: The results achieved with\nLM pretraining are signiﬁcantly better than the re-\nsults achieved without. In contrast, the meaning-\noriented STS sees good results with several kinds\nof pretraining, but does not beneﬁt substantially\nfrom LM pretraining.\nAmong pretraining tasks, language modeling\nperforms best, followed by MNLI. The remain-\ning pretraining tasks yield performance near that\nof the random baseline. Even our single-task base-\nline gets less than a one point gain over this simple\nbaseline. The multitask models are tied or out-\nperformed by models trained on one of their con-\nstituent tasks, suggesting that our approach to mul-\ntitask learning does not reliably produce models\nthat productively combine the knowledge taught\nby each task. However, of the two models that\nperform best on the development data, the multi-\ntask model generalizes better than the single-task\nmodel on test data for tasks like STS and MNLI\nwhere the test set contains out-of-domain data.\nIntermediate Task Avg CoLA SST MRPC QQP STS MNLI QNLI RTE WNLI\nELMo with Intermediate Task Training\nRandomE 70.5 38.5 87.7 79.9/86.5 86.7/83.4 80.8/82.1 75.6 79.6 61.7 33.8*\nSingle-TaskE 71.2 39.4 90.6 77.5/84.4 86.4/82.4 79.9/80.6 75.6 78.0 55.6 11.3*\nCoLAE 71.1 39.4 87.3 77.5/85.2 86.5/83.0 78.8/80.2 74.2 78.2 59.2 33.8*\nSSTE 71.2 38.8 90.6 80.4/86.8 87.0/83.5 79.4/81.0 74.3 77.8 53.8 43.7*\nMRPCE 71.3 40.0 88.4 77.5/84.4 86.4/82.7 79.5/80.6 74.9 78.4 58.1 54.9*\nQQPE 70.8 34.3 88.6 79.4/85.7 86.4/82.4 81.1/82.1 74.3 78.1 56.7 38.0*\nSTSE 71.6 39.9 88.4 79.9/86.4 86.7/83.3 79.9/80.6 74.3 78.6 58.5 26.8*\nMNLIE 72.1 38.9 89.0 80.9/86.9 86.1/82.7 81.3/82.5 75.6 79.7 58.8 16.9*\nQNLIE 71.2 37.2 88.3 81.1/86.9 85.5/81.7 78.9/80.1 74.7 78.0 58.8 22.5*\nRTEE 71.2 38.5 87.7 81.1/87.3 86.6/83.2 80.1/81.1 74.6 78.0 55.6 32.4*\nWNLIE 70.9 38.4 88.6 78.4/85.9 86.3/82.8 79.1/80.0 73.9 77.9 57.0 11.3*\nDisSent WTE 71.9 39.9 87.6 81.9/87.2 85.8/82.3 79.0/80.7 74.6 79.1 61.4 23.9*\nMT En-DeE 72.1 40.1 87.8 79.9/86.6 86.4/83.2 81.8/82.4 75.9 79.4 58.8 31.0*\nMT En-RuE 70.4 41.0 86.8 76.5/85.0 82.5/76.3 81.4/81.5 70.1 77.3 60.3 45.1*\nRedditE 71.0 38.5 87.7 77.2/85.0 85.4/82.1 80.9/81.7 74.2 79.3 56.7 21.1*\nSkipThoughtE 71.7 40.6 87.7 79.7/86.5 85.2/82.1 81.0/81.7 75.0 79.1 58.1 52.1*\nMTL GLUEE 72.1 33.8 90.5 81.1/87.4 86.6/83.0 82.1/83.3 76.2 79.2 61.4 42.3*\nMTL Non-GLUEE 72.4 39.4 88.8 80.6/86.8 87.1/84.1 83.2 /83.9 75.9 80.9 57.8 22.5*\nMTL AllE 72.2 37.9 89.6 79.2/86.4 86.0/82.8 81.6/82.5 76.1 80.2 60.3 31.0*\nBERT with Intermediate Task Training\nSingle-TaskB 78.8 56.6 90.9 88.5/91.8 89.9/86.4 86.1/86.0 83.5 87.9 69.7 56.3\nCoLAB 78.3 61.3 91.1 87.7/91.4 89.7/86.3 85.0/85.0 83.3 85.9 64.3 43.7*\nSSTB 78.4 57.4 92.2 86.3/90.0 89.6/86.1 85.3/85.1 83.2 87.4 67.5 43.7*\nMRPCB 78.3 60.3 90.8 87.0/91.1 89.7/86.3 86.6/86.4 83.8 83.9 66.4 56.3\nQQPB 79.1 56.8 91.3 88.5/91.7 90.5/87.3 88.1/87.8 83.4 87.2 69.7 56.3\nSTSB 79.4 61.1 92.3 88.0/91.5 89.3/85.5 86.2/86.0 82.9 87.0 71.5 50.7*\nMNLIB 79.6 56.0 91.3 88.0/91.3 90.0/86.7 87.8/87.7 82.9 87.0 76.9 56.3\nQNLIB 78.4 55.4 91.2 88.7/92.1 89.9/86.4 86.5/86.3 82.9 86.8 68.2 56.3\nRTEB 77.7 59.3 91.2 86.0/90.4 89.2/85.9 85.9/85.7 82.0 83.3 65.3 56.3\nWNLIB 76.2 53.2 92.1 85.5/90.0 89.1/85.5 85.6/85.4 82.4 82.5 58.5 56.3\nDisSent WTB 78.1 58.1 91.9 87.7/91.2 89.2/85.9 84.2/84.1 82.5 85.5 67.5 43.7*\nMT En-DeB 73.9 47.0 90.5 75.0/83.4 89.6/86.1 84.1/83.9 81.8 83.8 54.9 56.3\nMT En-RuB 74.3 52.4 89.9 71.8/81.3 89.4/85.6 82.8/82.8 81.5 83.1 58.5 43.7*\nRedditB 75.6 49.5 91.7 84.6/89.2 89.4/85.8 83.8/83.6 81.8 84.4 58.1 56.3\nSkipThoughtB 75.2 53.9 90.8 78.7/85.2 89.7/86.3 81.2/81.5 82.2 84.6 57.4 43.7*\nMTL GLUEB 79.6 56.8 91.3 88.0/91.4 90.3/86.9 89.2/89.0 83.0 86.8 74.7 43.7*\nMTL Non-GLUEB 76.7 54.8 91.1 83.6/88.7 89.2/85.6 83.2/83.2 82.4 84.4 64.3 43.7*\nMTL AllB 79.3 53.1 91.7 88.0/91.3 90.4/87.0 88.1/87.9 83.5 87.6 75.1 45.1*\nTest Set Results\nNon-GLUEE 69.7 34.5 89.5 78.2/84.8 83.6/64.3 77.5/76.0 75.4 74.8 55.6 65.1\nMNLIB 77.1 49.6 93.2 88.5/84.7 70.6/88.3 86.0/85.5 82.7 78.7 72.6 65.1\nGLUEB 77.3 49.0 93.5 89.0/85.3 70.6/88.6 85.8/84.9 82.9 81.0 71.7 34.9\nBERT Base 78.4 52.1 93.5 88.9/84.8 71.2/89.2 87.1/85.8 84.0 91.1 66.4 65.1\nTable 3: Results for intermediate training experiments on development sets except where noted.E and B respec-\ntively denote ELMo and BERT experiments. Bold denotes best scores by section. Underlining denotes average\nscores better than the single-task baseline. See Section 6 for discussion of WNLI results (*). BERT Base numbers\nare from Devlin et al. (2019).\nIntermediate Task Training Looking to Table\n3, using ELMo uniformly improves over training\nthe encoder from scratch. The ELMo-augmented\nrandom baseline is strong, lagging behind the\nsingle-task baseline by less than a point. Most in-\ntermediate tasks beat the random baseline, but sev-\neral fail to signiﬁcantly outperform the single-task\nbaseline. MNLI and English–German translation\nperform best with ELMo, with SkipThought and\nDisSent also beating the single-task baseline. In-\ntermediate multitask training on all the non-GLUE\ntasks produces our best-performing ELMo model.\nUsing BERT consistently outperforms ELMo\nand pretraining from scratch. We ﬁnd that in-\ntermediate training on each of MNLI, QQP, and\nSTS leads to improvements over no intermediate\ntraining, while intermediate training on the other\ntasks harms transfer performance. The improve-\nFigure 2: Learning curves (log scale) showing overall GLUE scores for encoders pretrained to convergence with\nvarying amounts of data, shown for pretraining (left) and intermediate ELMo (center) and BERT (right) training.\nTask Avg CoLA SST STS QQP MNLI QNLI\nCoLA 0.86 1.00\nSST 0.60 0.25 1.00\nMRPC 0.39 0.21 0.34\nSTS -0.36 -0.60 0.01 1.00\nQQP 0.61 0.61 0.27 -0.58 1.00\nMNLI 0.54 0.16 0.66 0.40 0.08 1.00\nQNLI 0.43 0.13 0.26 0.04 0.27 0.56 1.00\nRTE 0.34 0.08 0.16 -0.10 0.04 0.14 0.32\nWNLI -0.21 -0.21 -0.37 0.31 -0.37 -0.07 -0.26\nTable 4: Pearson correlations between performances on\na subset of all pairs of target tasks, measured over all\nruns reported in Table 2. The Avg column shows the\ncorrelation between performance on a target task and\nthe overall GLUE score. For QQP and STS, the corre-\nlations are computed respectively using F1 and Pearson\ncorrelation. Negative correlations are underlined.\nments gained via STS, a small-data task, versus\nthe negative impact of fairly large-data tasks (e.g.\nQNLI), suggests that the beneﬁt of intermediate\ntraining is not solely due to additional training,\nbut that the signal provided by the intermediate\ntask complements the original language model-\ning objective. Intermediate training on generation\ntasks such as MT and SkipThought signiﬁcantly\nimpairs BERT’s transfer ability. We speculate that\nthis degradation may be due to catastrophic for-\ngetting in ﬁne-tuning for a task substantially dif-\nferent from the tasks BERT was originally trained\non. This phenomenon might be mitigated in our\nELMo models via the frozen encoder and skip\nconnection. On the test set, we lag slightly behind\nthe BERT base results from Devlin et al. (2019),\nlikely due in part to our limited hyperparameter\ntuning.\n7 Analysis and Discussion\nTarget Task Correlations Table 4 presents an\nalternative view of the results of the pretraining\nexperiment (Table 2): The table shows correla-\ntions between pairs of target tasks over the space\nof pretrained encoders. The correlations reﬂect\nthe degree to which the performance on one tar-\nget task with some encoder predicts performance\non another target task with the same encoder. See\nAppendix D for the full table and similar tables for\nintermediate ELMo and BERT experiments.\nMany correlations are low, suggesting that dif-\nferent tasks beneﬁt from different forms of pre-\ntraining to a substantial degree, and bolstering the\nobservation that no single pretraining task yields\ngood performance on all target tasks. For reasons\nnoted earlier, the models that tended to perform\nbest overall also tended to overﬁt the WNLI train-\ning set most, leading to a negative correlation be-\ntween WNLI and overall GLUE score. STS also\nshows a negative correlation, likely due to the ob-\nservation that it does not beneﬁt from LM pretrain-\ning. In contrast, CoLA shows a strong correla-\ntion with the overall GLUE scores, but has weak\nor negative correlations with many tasks: The use\nof LM pretraining dramatically improves CoLA\nperformance, but most other forms of pretraining\nhave little effect.\nLearning Curves Figure 2 shows performance\non the overall GLUE metric for encoders pre-\ntrained to convergence on each task with vary-\ning amounts of data. Looking at pretraining tasks\nin isolation (left), most tasks improve slightly as\nthe amount of data increases, with the LM and\nMT tasks showing the most promising combina-\ntion of slope and maximum performance. Com-\nbining these tasks with ELMo (center) or BERT\n(right) yields less interpretable results: the rela-\ntionship between training data volume and perfor-\nmance becomes weaker, and some of the best re-\nsults reported in this paper are achieved by mod-\nels that combine ELMo with restricted-data ver-\nsions of intermediate tasks like MNLI and QQP.\nThis effect is ampliﬁed with BERT, with train-\ning data volume having unclear or negative rela-\ntionships with performance for many tasks. With\nlarge datasets for generation tasks, we see clear ev-\nidence of catastrophic forgetting with performance\nsharply decreasing in amount of training data.\nWe also measure the performance of target task\nperformance for three fully pretrained encoders\nunder varying amounts of target task data. We ﬁnd\nthat all tasks beneﬁt from increasing data quanti-\nties, with no obvious diminishing returns, and that\nmost tasks see a consistent improvement in per-\nformance with the use of pretraining, regardless of\nthe data volume. We present these learning curves\nin Appendix E.\nResults on the GLUE Diagnostic Set On\nGLUE’s analysis dataset, we ﬁnd that many of\nour pretraining tasks help on examples involv-\ning lexical-semantic knowledge and logical opera-\ntions, but less so on examples that highlight world\nknowledge. See Appendix F for details.\n8 Conclusions\nWe present a systematic comparison of tasks and\ntask combinations for the pretraining and interme-\ndiate ﬁne-tuning of sentence-level encoders like\nthose seen in ELMo and BERT. With nearly 60\npretraining tasks and task combinations and nine\ntarget tasks, this represents a far more comprehen-\nsive study than any seen on this problem to date.\nOur primary results are perhaps unsurprising:\nLM works well as a pretraining task, and no other\nsingle task is consistently better. Intermediate\ntraining of language models can yield modest fur-\nther gains. Multitask pretraining can produce re-\nsults better than any single task can. Target task\nperformance continues to improve with more LM\ndata, even at large scales, suggesting that further\nwork scaling up LM pretraining is warranted.\nWe also observe several worrying trends. Target\ntasks differ signiﬁcantly in the pretraining tasks\nthey beneﬁt from, with correlations between target\ntasks often low or negative. Multitask pretrain-\ning fails to reliably produce models better than\ntheir best individual components. When trained\non intermediate tasks like MT that are highly dif-\nferent than its original training task, BERT shows\nsigns of catastrophic forgetting. These trends sug-\ngest that improving on LM pretraining with cur-\nrent techniques will be challenging.\nWhile further work on language modeling\nseems straightforward and worthwhile, we believe\nthat the future of this line of work will require a\nbetter understanding of the settings in which target\ntask models can effectively utilize outside knowl-\nedge and data, and new methods for pretraining\nand transfer learning to do so.\nAcknowledgments\nParts of this work were conducted as part of the\nFifth Frederick Jelinek Memorial Summer Work-\nshop (JSALT) at Johns Hopkins University, and\nbeneﬁted from support by the JSALT sponsors and\na team-speciﬁc donation of computing resources\nfrom Google. We gratefully acknowledge the sup-\nport of NVIDIA Corporation with the donation of\na Titan V GPU used at NYU for this research.\nAW is supported by the National Science Founda-\ntion Graduate Research Fellowship Program under\nGrant No. DGE 1342536. PX and BVD were sup-\nported by DARPA AIDA. Any opinions, ﬁndings,\nand conclusions or recommendations expressed in\nthis material are those of the authors and do not\nnecessarily reﬂect the views of the National Sci-\nence Foundation.\nReferences\nJoachim Bingel and Anders Søgaard. 2017. Identify-\ning beneﬁcial task relations for multi-task learning\nin deep neural networks. In Proceedings of the 15th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: Volume 2, Short\nPapers, pages 164–169, Valencia, Spain. Associa-\ntion for Computational Linguistics.\nOndˇrej Bojar, Christian Buck, Rajen Chatterjee, Chris-\ntian Federmann, Yvette Graham, Barry Haddow,\nMatthias Huck, Antonio Jimeno Yepes, Philipp\nKoehn, and Julia Kreutzer. 2017. Proceedings\nof the second conference on machine translation.\nIn Proceedings of the Second Conference on Ma-\nchine Translation . Association for Computational\nLinguistics.\nOndrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve\nSaint-Amand, Radu Soricut, Lucia Specia, and Aleˇs\nTamchyna. 2014. Findings of the 2014 workshop\non statistical machine translation. In Proceedings of\nthe Ninth Workshop on Statistical Machine Trans-\nlation, pages 12–58. Association for Computational\nLinguistics.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings of\nthe 11th International Workshop on Semantic Eval-\nuation (SemEval-2017), pages 1–14. Association for\nComputational Linguistics.\nSoravit Changpinyo, Hexiang Hu, and Fei Sha. 2018.\nMulti-task learning for sequence tagging: An em-\npirical study. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics ,\npages 2965–2977, Santa Fe, New Mexico, USA. As-\nsociation for Computational Linguistics.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint 1312.3005.\nDanqi Chen and Christopher Manning. 2014. A fast\nand accurate dependency parser using neural net-\nworks. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 740–750. Association for Compu-\ntational Linguistics.\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011. Natural language processing (almost) from\nscratch. Journal of Machine Learning Research ,\n12(Aug):2493–2537.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc\nBarrault, and Antoine Bordes. 2017. Supervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2017, Copen-\nhagen, Denmark, September 9-11, 2017, pages 681–\n691.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2006. The PASCAL recognising textual entailment\nchallenge. In Machine learning challenges. evalu-\nating predictive uncertainty, visual object classiﬁca-\ntion, and recognising tectual entailment, pages 177–\n190. Springer.\nAndrew M. Dai and Quoc V . Le. 2015. Semi-\nsupervised sequence learning. In C. Cortes, N. D.\nLawrence, D. D. Lee, M. Sugiyama, and R. Garnett,\neditors, Advances in Neural Information Processing\nSystems 28 , pages 3079–3087. Curran Associates,\nInc.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long Papers).\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew\nPeters, Michael Schmitz, and Luke S. Zettlemoyer.\n2017. AllenNLP: A deep semantic natural language\nprocessing platform. arXiv preprint 1803.07640.\nHany Hassan, Anthony Aue, Chang Chen, Vishal\nChowdhary, Jonathan Clark, Christian Federmann,\nXuedong Huang, Marcin Junczys-Dowmunt, Will\nLewis, Mu Li, Shujie Liu, Tie-Yan Liu, Renqian\nLuo, Arul Menezes, Tao Qin, Frank Seide, Xu Tan,\nFei Tian, Lijun Wu, Shuangzhi Wu, Yingce Xia,\nDongdong Zhang, Zhirui Zhang, and Ming Zhou.\n2018. Achieving human parity on automatic Chi-\nnese to English news translation.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nfrom unlabelled data. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 1367–1377. Associ-\nation for Computational Linguistics.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nJulia Hockenmaier and Mark Steedman. 2007. CCG-\nbank: A Corpus of CCG Derivations and Depen-\ndency Structures Extracted from the Penn Treebank.\nComputational Linguistics.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the 36th International Conference\non Machine Learning.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339. Association for Com-\nputational Linguistics.\nMohit Iyyer, Varun Manjunatha, Jordan Boyd-Graber,\nand Hal Daum ´e III. 2015. Deep unordered com-\nposition rivals syntactic methods for text classiﬁca-\ntion. In Proceedings of the 53rd Annual Meeting of\nthe Association for Computational Linguistics and\nthe 7th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), vol-\nume 1, pages 1681–1691.\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. arXiv\npreprint 1705.00557.\nDouwe Kiela, Alexis Conneau, Allan Jabri, and Max-\nimilian Nickel. 2018. Learning visually grounded\nsentence representations. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pages 408–418. Association for Computa-\ntional Linguistics.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-Thought vectors. In\nAdvances in Neural Information Processing Sys-\ntems, pages 3294–3302.\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The Winograd schema challenge. In\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics. Association for Compu-\ntational Linguistics.\nMinh-Thang Luong, Quoc V . Le, Ilya Sutskever, Oriol\nVinyals, and Lukasz Kaiser. 2016. Multi-task se-\nquence to sequence learning. In Proceedings of the\nInternational Conference on Learning Representa-\ntions (ICLR).\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a Large Anno-\ntated Corpus of English: The Penn Treebank. Com-\nputational Linguistics.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In Advances in Neural In-\nformation Processing Systems, pages 6297–6308.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture\nmodels. In Proceedings of the International Con-\nference on Learning Representations (ICLR).\nLili Mou, Rui Men, Ge Li, Yan Xu, Lu Zhang, Rui Yan,\nand Zhi Jin. 2016. Natural language inference by\ntree-based convolution and heuristic matching. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 130–136, Berlin, Germany. As-\nsociation for Computational Linguistics.\nAllen Nie, Erin D Bennett, and Noah D Goodman.\n2019. DisSent: Sentence representation learning\nfrom explicit discourse relations. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics. Association for Compu-\ntational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227–\n2237. Association for Computational Linguistics.\nMatthew Peters, Sebastian Ruder, and Noah A. Smith.\n2019. To tune or not to tune? adapting pretrained\nrepresentations to diverse tasks. arXiv preprint\n1903.05987.\nMatthew E. Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018b. Dissecting contextual\nword embeddings: Architecture and representation.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing\n(EMNLP).\nJason Phang, Thibault Fvry, and Samuel R. Bow-\nman. 2018. Sentence encoders on STILTs: Supple-\nmentary training on intermediate labeled-data tasks.\narXiv preprint 1811.01088.\nAdam Poliak, Aparajita Haldar, Rachel Rudinger, J Ed-\nward Hu, Ellie Pavlick, Aaron Steven White, and\nBenjamin Van Durme. 2018. Towards a uniﬁed nat-\nural language inference framework to evaluate sen-\ntence representations. arXiv preprint 1804.08207.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. Unpublished\nmanuscript accessible via the OpenAI Blog.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Improving\nlanguage understanding by generative pre-training.\nUnpublished manuscript accessible via the OpenAI\nBlog.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for squad. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) , pages 784–\n789, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2383–2392. Asso-\nciation for Computational Linguistics.\nSashank J. Reddi, Satyen Kale, and Sanjiv Kumar.\n2018. On the convergence of Adam and beyond.\nIn Proceedings of the International Conference on\nLearning Representations (ICLR).\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017. Bidirectional attention\nﬂow for machine comprehension. In Proceedings of\nthe International Conference on Learning Represen-\ntations (ICLR).\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing ,\npages 1631–1642.\nAsa Cooper Stickland and Iain Murray. 2019. BERT\nand PALs: Projected attention layers for efﬁcient\nadaptation in multi-task learning. In Proceedings\nof the 36th International Conference on Machine\nLearning.\nSandeep Subramanian, Adam Trischler, Yoshua Ben-\ngio, and Christopher J. Pal. 2018. Learning gen-\neral purpose distributed sentence representations via\nlarge scale multi-task learning. In Proceedings of\nthe International Conference on Learning Represen-\ntations (ICLR).\nShuai Tang, Hailin Jin, Chen Fang, Zhaowen Wang,\nand Virginia de Sa. 2017. Rethinking Skip-thought:\nA neighborhood based approach. In Proceedings\nof the 2nd Workshop on Representation Learning\nfor NLP , pages 211–218. Association for Compu-\ntational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical nlp pipeline. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics . Association for\nComputational Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019b. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In Proceedings of the\nInternational Conference on Learning Representa-\ntions (ICLR).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000–6010.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the International Conference on Learn-\ning Representations (ICLR).\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2018. Neural network acceptability judg-\nments. arXiv preprint 1805.12471.\nAaron Steven White, Pushpendre Rastogi, Kevin Duh,\nand Benjamin Van Durme. 2017. Inference is ev-\nerything: Recasting semantic resources into a uni-\nﬁed evaluation framework. In Proceedings of the\nEighth International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers), vol-\nume 1, pages 996–1005.\nJohn Wieting and Douwe Kiela. 2019. No training\nrequired: Exploring random encoders for sentence\nclassiﬁcation. In Proceedings of the International\nConference on Learning Representations (ICLR).\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation. arXiv preprint\n1609.08144.\nYinfei Yang, Steve Yuan, Daniel Cer, Sheng-Yi Kong,\nNoah Constant, Petr Pilar, Heming Ge, Yun-hsuan\nSung, Brian Strope, and Ray Kurzweil. 2018.\nLearning semantic textual similarity from conver-\nsations. In Proceedings of The Third Workshop\non Representation Learning for NLP , pages 164–\n174, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nDani Yogatama, Cyprien de Masson d’Autume, Jerome\nConnor, Tomas Kocisky, Mike Chrzanowski, Ling-\npeng Kong, Angeliki Lazaridou, Wang Ling, Lei\nYu, Chris Dyer, and Phil Blunsom. 2019. Learning\nand evaluating general linguistic intelligence. arXiv\npreprint 1901.11373.\nKelly Zhang and Samuel R. Bowman. 2018. Language\nmodeling teaches you more syntax than translation\ndoes: Lessons learned through auxiliary task analy-\nsis. arXiv preprint 1809.10040.\nSheng Zhang, Rachel Rudinger, Kevin Duh, and Ben-\njamin Van Durme. 2017. Ordinal common-sense in-\nference. Transactions of the Association of Compu-\ntational Linguistics, 5(1):379–395.\nSmall Medium Large\nSteps btw. validations 100 100 1000\nAttention N N Y\nClassiﬁer dropout rate 0.4 0.2 0.2\nClassiﬁer hidden dim. 128 256 512\nMax pool projection dim. 128 256 512\nTable 5: Hyperparameter settings for target-task mod-\nels and target-task training for ELMo-style models.\nSmall-data tasks are RTE and WNLI; medium-data\ntasks are CoLA, SST, and MRPC; large-data tasks are\nSTS, QQP, MNLI, and QNLI. STS has a relatively\nsmall training set, but consistently patterns with the\nlarger tasks in its behavior.\nA Additional Pretraining Task Details\nDisSent To extract discourse model examples\nfrom the WikiText-103 corpus (Merity et al.,\n2017), we follow the procedure described in Nie\net al. (2019) by extracting clause-pairs that follow\nspeciﬁc dependency relationships within the cor-\npus (see Figure 4 in Nie et al., 2019). We use\nthe Stanford Parser (Chen and Manning, 2014)\ndistributed in Stanford CoreNLP version 3.9.1 to\nidentify the relevant dependency arcs.\nCross-Sentence Attention For MNLI, QQP,\nQNLI, and STS with ELMo-style models, we use\nan attention mechanism between all pairs of words\nrepresentations, followed by a 512D×2 BiLSTM\nwith max-pooling over time, following the mech-\nanism used in BiDAF (Seo et al., 2017).\nAlternative Tasks Any large-scale comparison\nlike the one attempted in this paper is inevitably\nincomplete. Among the thousands of publicly\navailable NLP datasets, we also performed ini-\ntial trial experiments on several datasets for which\nwe were not able to reach development-set per-\nformance above that of the random encoder base-\nline in the pretraining or as an intermediate task\nwith ELMo. These include image-caption match-\ning with MSCOCO (Lin et al., 2014), following\nKiela et al. (2018); the small-to-medium-data text-\nunderstanding tasks collected in NLI format by\nPoliak et al. (2018); ordinal common sense in-\nference (Zhang et al., 2017); POS tagging on the\nPenn Treebank (Marcus et al., 1993); supertag-\nging on CCGBank (Hockenmaier and Steedman,\n2007); and a variant objective on our Reddit data,\ninspired by Yang et al. (2018), where the model is\ntrained to select which of two candidate replies to\na given comment is correct.\nB Hyperparameters and Optimization\nDetails\nSee Section 5 for general comments on hyperpa-\nrameter tuning.\nValidation We evaluate on the validation set for\nthe current training task or tasks every 1,000 steps,\nexcept where noted otherwise for small-data target\ntasks. During multitask learning, we multiply this\ninterval by the number of tasks, evaluating every\n9,000 steps during GLUE multitask training, for\nexample.\nOptimizer For BERT, we use the same opti-\nmizer and learning rate schedule as Devlin et al.\n(2019), with an initial learning rate of 1e-5 and\ntraining for a maximum of three epochs at each\nstage (or earlier if we trigger a different early stop-\nping criterion). For all other experiments, we use\nAMSGrad (Reddi et al., 2018). During pretrain-\ning, we use a learning rate of 1e-4 for classiﬁcation\nand regression tasks, and 1e-3 for text generation\ntasks. During target-task training, we use a learn-\ning rate of 3e-4 for all tasks.\nLearning Rate Decay We multiply the learning\nrate by 0.5 whenever validation performance fails\nto improve for more than 4 validation checks. We\nstop training if the learning rate falls below 1e-6.\nEarly Stopping We maintain a saved check-\npoint reﬂecting the best validation result seen so\nfar. We stop training if we see no improvement af-\nter more than 20 validation checks. After training,\nwe use the last saved checkpoint.\nRegularization For BERT models, we follow\nthe original work. For non-BERT models, we\napply dropout with a drop rate of 0.2 after the\ncharacter CNN in pretraining experiments or af-\nter ELMo, after each LSTM layer, and after each\nMLP layer in the task-speciﬁc classiﬁer or regres-\nsor. For small-data target tasks, we increase MLP\ndropout to 0.4 during target-task training.\nPreprocessing For BERT, we follow Devlin\net al. (2019) and use the WordPiece (Wu et al.,\n2016) tokenizer. For all other experiments, we use\nthe Moses tokenizer for encoder inputs, and set a\nmaximum sequence length of 40 tokens. There is\nno input vocabulary, as we use ELMo’s character-\nbased input layer.\nFor English text generation tasks, we use the\nMoses tokenizer to tokenize our data, but use a\nword-level output vocabulary of 20,000 types for\ntasks that require text generation. For translation\ntasks, we use BPE tokenization with a vocabu-\nlary of 20,000 types. For all sequence-to-sequence\ntasks we train word embeddings on the decoder\nside.\nTarget-Task-Speciﬁc Parameters For non-\nBERT models, to ensure that baseline performance\nfor each target task is competitive, we ﬁnd it nec-\nessary to use slightly different models and training\nregimes for larger and smaller target tasks. We\nused partially-heuristic tuning to separate GLUE\ntasks into big-, medium- and small-data groups,\ngiving each group its own heuristically chosen\ntask-speciﬁc model speciﬁcations. Exact values\nare shown in Table 5.\nSequence-to-Sequence Models We found bilin-\near attention to be helpful for the SkipThought and\nReddit pretraining tasks but not for machine trans-\nlation, and report results for these conﬁgurations.\nFor ELMo-style models, we use the max-pooled\noutput of the encoder to initialize the hidden state\nof the decoder, and the size of this hidden state is\nequal to the size of the output of our shared en-\ncoder. For BERT, we use the representation corre-\nsponding to the [CLS] token to initialize the hid-\nden state of the decoder. We reduce the dimension\nof the output of the decoder by half via a learned\nlinear projection before the output softmax layer.\nC Multitask Learning Methods\nOur multitask learning experiments have three\nsomewhat distinctive properties: (i) We mix tasks\nwith very different amounts of training data—at\nthe extreme, under 1,000 examples for WNLI, and\nover 1,000,000,000 examples from LM BWB. (ii)\nOur goal is to optimize the quality of the shared\nencoder, not the performance of any one of the\ntasks in the multitask mix. (iii) We mix a relatively\nlarge number of tasks, up to eighteen at once in\nsome conditions. These conditions make it chal-\nlenging but important to avoid overﬁtting or un-\nderﬁtting any of our tasks.\nRelatively little work has been done on this\nproblem, so we conduct a small experiment here.\nAll our experiments use the basic paradigm of ran-\ndomly sampling a new task to train on at each\nstep, and we experiment with two hyperparame-\nters that can be used to control over- and underﬁt-\nting: The probability with which we sample each\nPretraining Tasks\nSampling GLUE S1 S2 S3\nUniform 69.1 53.7 82.1 31.7\nProportional 69.8 52.0 83.1 36.6\nLog Proportional 68.8 54.3 82.9 31.2\nPower 0.75 69.3 51.1 82.7 37.9\nPower 0.66 69.0 53.4 82.8 35.5\nPower 0.5 69.1 55.6 83.3 35.9\nTable 6: Comparison of sampling methods on four\nsubsets of GLUE using uniform loss scaling. The re-\nported scores are averages of the development set re-\nsults achieved for each task after early stopping. Re-\nsults in bold are the best within each set.\nLoss Scaling\nSampling Uniform Proportional Power 0.75\nUniform 69.1 69.7 69.8\nProportional 69.8 69.4 69.6\nLog Proportional 68.8 68.9 68.9\nPower 0.75 69.3 69.1 69.0\nTable 7: Combinations of sampling and loss scaling\nmethods on GLUE tasks. Results in bold are tied for\nbest overall GLUE score.\ntask and the weight with which we scale the loss\nfor each task. Our experiments follow the setup\nin Appendix B, and do not use the ELMo BiL-\nSTM. For validation metrics like perplexity that\ndecrease from high starting values during training,\nwe include the transformed metric 1 − metric\n250 in\nour average, where the constant 250 was tuned in\nearly experiments.\nTask Sampling We consider several approaches\nto determine the probability with which to sample\na task during training, generally making this prob-\nability a function of the amount of data available\nfor the task. For task i with training set size Ni,\nthe probability is pi = f(Ni)/∑\nj f(Nj), where\nf(Ni) = 1(Uniform), Ni (Proportional), log(Ni)\n(Log Proportional), or Na\ni (Power a) where ais a\nconstant.\nLoss Scaling At each update, we scale the loss\nof a task with weight wi = f(Ni)/maxjf(Nj),\nwhere f(Ni) = 1(Uniform), Nj (Proportional),\nor Na\nj (Power a).\nExperiments For task sampling, we run exper-\niments with multitask learning on the full set of\nnine GLUE tasks, as well as three subsets: sin-\ngle sentence tasks (S1: SST, CoLA), similarity\nand paraphrase tasks (S2: MRPC, STS, QQP), and\ninference tasks (S3: WNLI, QNLI, MNLI, RTE).\nThe results are shown in Table 6.\nWe also experiment with several combinations\nof task sampling and loss scaling methods, using\nonly the full set of GLUE tasks. The results are\nshown in Table 7.\nWhile no combination of methods consistently\noffers dramatically better performance than any\nother, we observe that it is generally better to ap-\nply only one of non-uniform sampling and non-\nuniform loss scaling at a time rather than apply\nboth simultaneously, as they provide roughly the\nsame effect. Following encouraging results from\nearlier pilot experiments, we use power 0.75 task\nsampling and uniform loss scaling in the multitask\nlearning experiments shown in Table 2.\nD Additional Target Task Correlations\nTables 8, 9, and 10 respectively show the full tar-\nget task correlations computed over pretraining,\nintermediate ELMo, and intermediate BERT ex-\nperiments.\nSee Section 7 for a discussion about correla-\ntions for the pretraining experiments. The general\ntrends in correlation vary signiﬁcantly between\nthe three experimental settings, which we take to\nroughly indicate the different types of knowledge\nencoded in ELMo and BERT. The exception is that\nWNLI is consistently negatively correlated with\nthe other target tasks and often the overall GLUE\nscore.\nFor intermediate ELMo experiments, correla-\ntions are generally low, with the exception of\nMNLI with other tasks. CoLA is negatively cor-\nrelated with most other tasks, while QQP and SST\nare positively correlated with most tasks.\nFor intermediate BERT experiments, correla-\ntions with the GLUE score are quite high, as we\nfound that intermediate training often negatively\nimpacted GLUE score. QQP is highly negatively\ncorrelated with most other tasks, while the smaller\ntasks like MRPC and RTE are most highly corre-\nlated with overall GLUE score.\nE Additional Learning Curves\nFigure 3 shows learning curves reﬂecting the\namount of target-task data required to train a\nmodel on each GLUE task, starting from three se-\nlected encoders. See Section 7 for discussion.\nF Diagnostic Set Results\nTables 11 and 12 show results on the four coarse-\ngrained categories of the GLUE diagnostic set for\nall our pretraining experiments. This set con-\nsists of about 1000 expert-constructed examples\nin NLI format meant to isolate a range of relevant\nphenomena. Results use the target task classiﬁer\ntrained on the MNLI training set.\nNo model achieves performance anywhere\nclose to human-level performance, suggesting that\neither none of our pretrained models extract fea-\ntures that are suitable for robust reasoning over\ntext, or that the MNLI training set and the MNLI\ntarget-task model are not able to exploit any such\nfeatures that exist. See Section 7 for further dis-\ncussion.\nWhile no model achieves near-human perfor-\nmance, the use of ELMo and appears to be helpful\non examples that highlight world knowledge and\nlexical-semantic knowledge, and less so on exam-\nples that highlight complex logical reasoning pat-\nterns or alternations in sentence structure. This\nrelative weakness on sentence structure is some-\nwhat surprising given the ﬁnding in Zhang and\nBowman (2018) that language model pretraining\nis helpful for tasks involving sentence structure.\nUsing BERT helps signiﬁcantly with under-\nstanding sentence structure, lexical knowledge,\nand logical reasoning, but does not seem to help\non world knowledge over using ELMo. Encourag-\ningly, we ﬁnd that intermediate training of BERT\non all of our pretraining tasks outperforms inter-\nmediate training on one or no tasks in two of the\nfour categories.\nFigure 3: Target-task training learning curves for each GLUE task with three encoders: the random encoder without\nELMo (left), random with ELMo (center), and MTL Non-GLUE pretraining (right).\nTask Avg CoLA SST MRPC STS QQP MNLI QNLI RTE WNLI\nCoLA 0.86 1.00\nSST 0.60 0.25 1.00\nMRPC 0.39 0.21 0.34 1.00\nSTS -0.36 -0.60 0.01 0.29 1.00\nQQP 0.61 0.61 0.27 -0.17 -0.58 1.00\nMNLI 0.54 0.16 0.66 0.56 0.40 0.08 1.00\nQNLI 0.43 0.13 0.26 0.32 0.04 0.27 0.56 1.00\nRTE 0.34 0.08 0.16 -0.09 -0.10 0.04 0.14 0.32 1.00\nWNLI -0.21 -0.21 -0.37 0.31 0.31 -0.37 -0.07 -0.26 0.12 1.00\nTable 8: Pearson correlations between performances on different target tasks, measured over all pretraining runs\nreported in Table 2.\nTask Avg CoLA SST MRPC STS QQP MNLI QNLI RTE WNLI\nCoLA 0.07 1.00\nSST 0.32 -0.48 1.00\nMRPC 0.42 -0.20 0.29 1.00\nSTS 0.41 -0.40 0.26 0.21 1.00\nQQP 0.02 0.08 0.26 0.18 0.15 1.00\nMNLI 0.60 -0.21 0.33 0.38 0.72 0.21 1.00\nQNLI 0.50 0.10 0.03 0.12 0.63 -0.01 0.72 1.00\nRTE 0.39 -0.13 -0.15 0.21 0.27 -0.04 0.60 0.59 1.00\nWNLI -0.14 0.02 0.23 -0.29 -0.02 0.15 0.02 -0.25 -0.22 1.00\nTable 9: Pearson correlations between performances on different target tasks, measured over all ELMo runs re-\nported in Table 3. Negative correlations are underlined.\nTask Avg CoLA SST MRPC STS QQP MNLI QNLI RTE WNLI\nCoLA 0.71 1.00\nSST 0.41 0.32 1.00\nMRPC 0.83 0.67 0.62 1.00\nSTS 0.82 0.34 0.21 0.60 1.00\nQQP -0.41 0.01 0.04 -0.05 -0.64 1.00\nMNLI 0.73 0.31 0.10 0.42 0.69 -0.68 1.00\nQNLI 0.73 0.38 0.29 0.56 0.43 -0.11 0.62 1.00\nRTE 0.88 0.47 0.22 0.56 0.87 -0.70 0.68 0.55 1.00\nWNLI 0.45 -0.10 -0.03 0.20 0.79 -0.89 0.65 0.11 0.69 1.00\nTable 10: Pearson correlations between performances on different target tasks, measured over all BERT runs\nreported in Table 3. Negative correlations are underlined.\nPretr. Knowledge Lexical Semantics Logic Predicate/Argument Str.\nBaselines\nRandom 17.6 19.6 12.5 26.9\nGLUE Tasks as Pretraining Tasks\nCoLA 15.3 24.2 14.9 31.7\nSST 16.1 24.8 16.5 28.7\nMRPC 16.0 25.2 12.6 26.4\nQQP 12.8 22.5 12.9 30.8\nSTS 16.5 20.2 13.0 27.1\nMNLI 16.4 20.4 17.7 29.9\nQNLI 13.6 21.3 12.2 28.0\nRTE 16.3 23.1 14.5 28.8\nWNLI 18.8 19.5 13.9 29.1\nNon-GLUE Pretraining Tasks\nDisSent WP 18.5 24.2 15.4 27.8\nLM WP 14.9 16.6 9.4 23.0\nLM BWB 15.8 19.4 9.1 23.9\nMT En-De 13.4 24.6 14.8 30.1\nMT En-Ru 13.4 24.6 14.8 30.1\nReddit 13.9 20.4 14.1 26.0\nSkipThought 15.1 22.0 13.7 27.9\nMultitask Pretraining\nMTL All 16.3 21.4 11.2 28.0\nMTL GLUE 12.5 21.4 15.0 30.1\nMTL Outside 14.5 19.7 13.1 26.2\nTable 11: GLUE diagnostic set results, reported as R3 correlation coefﬁcients ( ×100), which standardizes the\nscore of random guessing by an uninformed model at roughly 0. Human performance on the overall diagnostic set\nis roughly 80. Results in bold are the best overall.\nPretr. Knowledge Lexical Semantics Logic Predicate/Argument Str.\nELMo with Intermediate Task Training\nRandomE 19.2 22.9 9.8 25.5\nCoLAE 17.2 21.6 9.2 27.3\nSSTE 19.4 20.5 9.7 28.5\nMRPCE 11.8 20.5 12.1 27.4\nQQPE 17.5 16.0 9.9 30.5\nSTSE 18.0 18.4 9.1 25.5\nMNLIE 17.0 23.2 14.4 23.9\nQNLIE 17.4 24.1 10.7 30.2\nRTEE 18.0 20.2 8.7 28.0\nWNLIE 16.5 19.8 7.3 25.2\nDisSent WPE 16.3 23.0 11.6 26.5\nMT En-DeE 19.2 21.0 13.5 29.7\nMT En-RuE 20.0 20.1 11.9 21.4\nRedditE 14.7 22.3 15.0 29.0\nSkipThoughtE 20.5 18.5 10.4 26.8\nMTL GLUEE 20.6 22.1 14.7 25.3\nMTL Non-GLUEE 15.7 23.7 12.6 29.0\nMTL AllE 13.8 18.4 10.8 26.7\nBERT with Intermediate Task Training\nSingle-TaskB 20.3 36.3 21.7 40.4\nCoLAB 18.5 34.0 23.5 40.1\nSSTB 19.8 36.0 23.2 39.1\nMRPCB 20.6 33.3 20.9 37.8\nQQPB 17.4 35.7 23.8 40.5\nSTSB 21.3 34.7 24.0 40.7\nMNLIB 19.1 34.0 23.3 41.7\nQNLIB 20.3 38.4 24.4 41.5\nRTEB 15.4 32.6 20.2 38.5\nWNLIB 20.8 35.8 23.1 39.3\nDisSent WPB 17.9 34.0 23.7 39.1\nMT En-DeB 18.6 33.8 20.7 37.4\nMT En-RuB 14.2 30.2 20.3 36.5\nRedditB 16.5 29.9 22.7 37.1\nSkipThoughtB 15.8 35.0 20.9 38.3\nMTL GLUEB 17.0 35.2 24.3 39.6\nMTL Non-GLUEB 18.7 37.0 21.8 40.6\nMTL AllB 17.8 40.3 27.5 41.0\nTable 12: GLUE diagnostic set results, reported as R3 correlation coefﬁcients ( ×100), which standardizes the\nscore of random guessing by an uninformed model at roughly 0. Human performance on the overall diagnostic set\nis roughly 80. Results in bold are the best by section."
}