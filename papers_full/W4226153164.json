{
    "title": "Hybrid LSTM-Transformer Model for Emotion Recognition From Speech Audio Files",
    "url": "https://openalex.org/W4226153164",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4224692075",
            "name": "Felicia Andayani",
            "affiliations": [
                "Swinburne University of Technology Sarawak Campus"
            ]
        },
        {
            "id": "https://openalex.org/A9834695",
            "name": "Lau Bee Theng",
            "affiliations": [
                "Swinburne University of Technology Sarawak Campus"
            ]
        },
        {
            "id": "https://openalex.org/A4224692077",
            "name": "Mark Teekit Tsun",
            "affiliations": [
                "Swinburne University of Technology Sarawak Campus"
            ]
        },
        {
            "id": "https://openalex.org/A2021800307",
            "name": "Caslon Chua",
            "affiliations": [
                "Swinburne University of Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3154715947",
        "https://openalex.org/W3158430841",
        "https://openalex.org/W3133066644",
        "https://openalex.org/W3145643603",
        "https://openalex.org/W3198561248",
        "https://openalex.org/W6781729723",
        "https://openalex.org/W6795990123",
        "https://openalex.org/W2803193013",
        "https://openalex.org/W175750906",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2972691009",
        "https://openalex.org/W2972811324",
        "https://openalex.org/W2777468850",
        "https://openalex.org/W6774818996",
        "https://openalex.org/W2058819080",
        "https://openalex.org/W2998064461",
        "https://openalex.org/W2097207180",
        "https://openalex.org/W3028860956",
        "https://openalex.org/W2959546144",
        "https://openalex.org/W3015249983",
        "https://openalex.org/W2892071465",
        "https://openalex.org/W6771351431",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2766745135",
        "https://openalex.org/W3107569919",
        "https://openalex.org/W6941135759",
        "https://openalex.org/W6675354045",
        "https://openalex.org/W6760599669",
        "https://openalex.org/W2791925433",
        "https://openalex.org/W3094335887",
        "https://openalex.org/W2923871787",
        "https://openalex.org/W3165872120",
        "https://openalex.org/W4293054852",
        "https://openalex.org/W2101234009",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3010714856",
        "https://openalex.org/W3048645559",
        "https://openalex.org/W4288009632"
    ],
    "abstract": "Emotion is a vital component in daily human communication and it helps people understand each other. Emotion recognition plays a crucial role in developing human-computer interaction and computer-based speech emotion recognition. In a nutshell, Speech Emotion Recognition (SER) recognizes emotion signals transmitted through human speech or daily conversation where the emotions in a speech strongly depend on temporal information. Despite the fact that much existing research showed that a hybrid system performs better than traditional single classifiers used in SER, there are some limitations in each of them. As a result, this paper discussed a proposed hybrid Long Short-Term Memory (LSTM) Network and Transformer Encoder to learn the long-term dependencies in speech signals and classify emotions. Speech features are extracted with Mel Frequency Cepstral Coefficient (MFCC) and fed into the proposed hybrid LSTM-Transformer classifier. A range of performance evaluations was conducted on the proposed LSTM-Transformer model. The results indicate that it achieves a significant recognition improvement compared with existing models offered by other published works. The proposed hybrid model reached 75.62&#x0025;, 85.55&#x0025;, and 72.49&#x0025; recognition success with the RAVDESS, Emo-DB, and language-independent datasets.",
    "full_text": "Received February 13, 2022, accepted March 24, 2022, date of publication March 31, 2022, date of current version April 7, 2022.\nDigital Object Identifier 10.1 109/ACCESS.2022.3163856\nHybrid LSTM-Transformer Model for Emotion\nRecognition From Speech Audio Files\nFELICIA ANDAYANI\n 1, LAU BEE THENG1, (Senior Member, IEEE),\nMARK TEEKIT TSUN\n 1, (Member, IEEE), AND CASLON CHUA2\n1Faculty of Engineering, Computing, and Science, Swinburne University of Technology Sarawak Campus, Kuching, Sarawak 93350, Malaysia\n2Faculty of Science, Engineering and Technology, Swinburne University of Technology, Melbourne, VIC 3122, Australia\nCorresponding author: Felicia Andayani (fandayani@swinburne.edu.my)\nThis work was supported in part by the Swinburne University of Technology Sarawak Higher Degree by Research (HDR) Support Fund.\nABSTRACT Emotion is a vital component in daily human communication and it helps people under-\nstand each other. Emotion recognition plays a crucial role in developing human-computer interaction and\ncomputer-based speech emotion recognition. In a nutshell, Speech Emotion Recognition (SER) recognizes\nemotion signals transmitted through human speech or daily conversation where the emotions in a speech\nstrongly depend on temporal information. Despite the fact that much existing research showed that a hybrid\nsystem performs better than traditional single classiﬁers used in SER, there are some limitations in each\nof them. As a result, this paper discussed a proposed hybrid Long Short-Term Memory (LSTM) Network\nand Transformer Encoder to learn the long-term dependencies in speech signals and classify emotions.\nSpeech features are extracted with Mel Frequency Cepstral Coefﬁcient (MFCC) and fed into the proposed\nhybrid LSTM-Transformer classiﬁer. A range of performance evaluations was conducted on the proposed\nLSTM-Transformer model. The results indicate that it achieves a signiﬁcant recognition improvement\ncompared with existing models offered by other published works. The proposed hybrid model reached\n75.62%, 85.55%, and 72.49% recognition success with the RA VDESS, Emo-DB, and language-independent\ndatasets.\nINDEX TERMS Attention mechanism, long short-term memory network, speech emotion recognition,\ntransformer encoder.\nI. INTRODUCTION\nEmotion is an important aspect that exists in daily human\nactivities. Emotions help people understand each other and\nassist people in decision-making [24], [26]. They also assist\ncommunication in the context of safety and security [24]. For\nexample, when sharing with someone upset, we can be more\ncareful and gentler to avoid hurting that person.\nThere are different modalities for recognizing human emo-\ntions, such as speech, text, and facial expressions. Speech\nis obviously an important channel and a source for study-\ning human emotions [25]. SER is the task of recognizing\nemotions expressed through human speech. SER has played\nan important role in numerous applications, such as Human-\nComputer Interactions (HCI), Human-Robot Interfaces [1],\nintelligent call-centers [1], intelligent teaching systems [2],\nand many more. In addition, adding emotion recognition\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Manuel Rosa-Zurera.\nfeatures is believed to be a crucial factor in creating a device\nthat could act like a human [3]. Therefore, SER research is\nstill actively pursued and has gained increasing interest from\nmany researchers to develop a better performing recognition\nmodel.\nMost SER research focuses on Machine Learning (ML)\narchitectures to develop the SER model. This method\ninvolves extracting features from raw speech data. The\nextracted features are used as input to train the ML algo-\nrithm based on the samples of the input-output pairs. After\nthe training, the ML algorithm predicts the emotions from\nthe validation and testing data. Different types of features,\nsuch as prosodic, voice-quality, spectral, wavelet, spectro-\ngram image, and deep features, have been widely used in\ncurrent SER models. However, to date, no single feature set\nhas been identiﬁed as a one-stop solution for recognizing\nemotion in speech data. Researchers often perform the testing\nor combine a vast number of features to gain some insight,\nand various feature selection methods can be used to remove\n36018\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.\nFor more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nVOLUME 10, 2022\nF. Andayaniet al.: Hybrid LSTM-Transformer Model for Emotion Recognition From Speech Audio Files\nredundant features. The process of selecting the ML archi-\ntecture used to perform the classiﬁcation task is also crucial\nin SER, where the classiﬁcation paradigm of the SER model\nmust be able to process high-dimensional features at as low a\ncomputational cost as possible.\nII. RELATED WORK\nThe emergence of Deep Learning (DL) has increased the\nefﬁciency possibilities for researchers to develop better per-\nforming SER. The models range from Deep Neural Network\n(DNN), Convolutional Neural Network (CNN), to Recurrent\nNeural Network (RNN) based applications.\nIn 2019, Lee et al. [4] used DNN to classify eight emo-\ntions on the Emo-DB dataset. They extracted 20 MFCCs\nwith the delta and delta-delta values. They tested the model\non four different sets of the extracted features. The highest\nresults were obtained using 20 MFCCs with DNN, which\nachieved 69.4% recognition accuracy. The experiment also\nshowed that DNN could perform better than a Support Vector\nMachine (SVM) model.\nThe successful application of CNN in image processing\nhas motivated some researchers to develop end-to-end SER\nmodels. Such models often use CNN combined with LSTM\nto learn both spatial and temporal features. For instance,\nTzirakis, Zhang, and Schuller [5] proposed an end-to-end\nmodel which used CNN and LSTM architectures. CNN was\nused to extract the features from the raw speech signals, while\nLSTM was used to learn the contextual information in the\ndata and perform the ﬁnal prediction. Their proposed model\noutperformed other state-of-the-art methods in terms of con-\ncordance correlation coefﬁcient on the RECOLA dataset.\nThe use of the Attention Mechanism also shows possible\nimprovements in the SER models. The Attention Mechanism\nis used to learn the critical features, and it is often com-\nbined with the LSTM architecture. Xie et al.[6] utilized the\nAttention Mechanism as an alternative to the forgetting gate\nin the LSTM architecture. In addition, they also applied the\nAttention Weighting Mechanism to distinguish the emotional\nsaturation among the time segments. Besides, the Attention\nWeighting Mechanism could address the problem that differ-\nent features may vary in their ability to distinguish emotions.\nThe model used the frame-level speech features extracted\nusing the OpenSmile library as an input. Their experiments\non the CASIA, eNTERFACE, and GEMEP datasets showed\nrecognition accuracy improvemen.\nThe successful application of the Attention Mechanism\nmotivated Vaswasni et al. [7] to develop an architecture\ncalled the Transformer. The architecture was developed based\non the Attention Mechanism, which allows parallelization\nand a global relationship between the input and output\n[7].The Transformer uses Multi-Head Attention Mechanisms\ninstead of a single attention head. The model shows bet-\nter performance in some natural language processing (NLP)\ntasks. Hence, some researchers have started to apply the\nTransformer architecture to emotion recognition tasks. How-\never, researchers often combine different modalities to\nperform the job in the emotion recognition task. For exam-\nple, Heusser et al.[8] proposed a reinforcement learning\napproachfor SER usinga pre-trained Transformer language\nmodel, which combined the SER task with the Speech\nRecognition and Text Emotion Recognition tasks. They used\nCNN-LSTM to perform the SER task anda pre-trained Trans-\nformer for the other two tasks. The proposed model was eval-\nuated on the IEMOCAP database and achieved 73.5% and\n71% recognition rates. Lee, Han, and Ko [9] also proposed\na pre-trained Transformer model and CNN to perform the\nSER task. The model was trained and tested on the Emo-\nDB and IEMOCAP datasets. The recognition rates of the\n‘‘speaker-dependent’’ and ‘‘speaker-independent’’ samples in\nthe Emo-DB dataset were 94.23% Weighted Accuracy (WA)\nand 92.1% Unweighted Accuracy (UA) versus 88.43% WA\nand 86.04% UA, respectively. The IEMOCAP dataset’ recog-\nnition rates were 69.51% WA and 71.36% UA versus 66.47%\nWA and 67.12% UA, respectively. In [23], an improved model\nof Transformer was proposed and used inthe SER. They used\ndifferent methods of positional encoding and Taylor Lin-\near Attention (TLA) in Multi-Head Attention. Their model\nachieved 74.9% UA when tested on the Emo-DB dataset and\n80% UAwhen tested on the URDU dataset.\nEven though RNN has been widely used in SER research\ndue to its ability to process sequential data and handle\nvariable-length input, it suffers from a long-term depen-\ndency problem. However, the SER system requires a model\nthat can sufﬁciently learn the long-term dependencies in the\nspeech signal because emotions in speech signals strongly\ndepend on temporal information.LSTM was developed to\nsolve the RNN’ problem using its memory architecture. It can\nremember the information for an extended period of time.\nNevertheless, LSTM might not work well on longer-term\ndependencies [15].\nThe Transformer, a Multi-Head Attention Mechanism, was\nintroduced in 2017 [7], and it has been widely used in many\nNLP ﬁelds. In contrast to RNN and LSTM, it can be par-\nallelized. Moreover, it has shown outstanding performance\nin broad NLP tasks. However, the Transformer’s weakness\nis that it loses the sequential information of its position and\nneeds to re-compute the entire history in the context window\nat each time step [11].\nLooking at the advantages and disadvantages of the LSTM\nand Transformer architectures, we have gained insight into\ncombining them to enjoy the beneﬁts of both architectures\nwhile preventing their respective drawbacks. Moreover, both\narchitectures have been incorporated into different research\nﬁelds and achieved state-of-the-art results, such as language\nmodeling [15], [19], text generation [10], and modeling\nmulti-leg trips [11].\nHowever, the combination has not been used in SER. Thus,\nthe primary goal of this study is to combine recent advances in\nthe LSTM and Transformer architectures in the SER system\nand investigate the impact of the combination on improv-\ning the SER classiﬁcation performance. The state-of-the-art\nresults in [10], [11] have motivated us to explore the hybrid\nVOLUME 10, 2022 36019\nF. Andayaniet al.: Hybrid LSTM-Transformer Model for Emotion Recognition From Speech Audio Files\nFIGURE 1. Overview of the proposed LSTM-Transformer model.\nLSTM-Transformer model to solve the emotion classiﬁcation\nproblems in SER.\nWe propose a hybrid LSTM-Transformer architecture that\nimproves the SER classiﬁcation performance via learning\nthe long-term dependencies. In addition, we investigated the\nperformance of the LSTM-Transformer model on recogniz-\ning emotions in speech signals by learning the long-term\ndependencies, which have not been explored previously.The\nproposed LSTM-Transformer model replaces the positional\nencoding in the Transformer architecture with the LSTM\nrecurrent process to learn the hidden state of the input fea-\ntures. In addition, instead of using a single Attention layer\non the LSTM, we combined the LSTM with the Multi-Head\nAttention Mechanism in the Transformer encoder layer.\nThe maincontributions of this work are the design, devel-\nopment, and evaluation of the model, selection of parameters,\nand selection of the MFCC feature for th LSTM-Transformer\nmodel to classify emotions. This work utilizes the RA VDESS,\nEmo-DB, and language-independent datasets. Our language-\nindependent dataset is developed froma combination of\nRA VDESS and Emo-DB datasets.\nIII. PROPOSED SOLUTION\nThe overview of the proposed approach is depicted in Fig. 1,\nwhere the speech samples are pre-processed and converted\ninto spectrograms. It is then followed by the MFCC features\nbeing extracted from the spectrograms and the mean statistics\nfrom the extracted MFCC features are calculated to compute\nthe feature vector. Upon extracting the features, standardiza-\ntion is applied to have standardized distributed data. Towards\nthe end of the pipeline, the feature vector is used for training\nand testing the LSTM-Transformer model.\nA. PRE-PROCESSING AND FEATURE EXTRACTION\nDiverse datasets have different characteristics, with some\ncontaining noise while others are clean recordings. Some\ndatasets also contain silenceat the beginning or end of\nthe recordings, and the recorded speech data durationmay\nvary in some datasets. Therefore, speech data requires pre-\nprocessing to maintain consistent training and testing data.\nThe samples from the selected datasets were loaded and\nresampled to 22050 kHz so that the language-independent\ndataset could adhere to a consistent sampling rate. Then, the\nsilence parts at the beginning and the end were removed, and\nthe signal containing the speech information was obtained for\nfurtherprocessing.\nUpon completin the pre-processing, the speech samples\nwere converted into spectrograms as the input to the proposed\nmodel. This study extracted the Mel Frequency Cepstral\nCoefﬁcient features from the spectrograms [35], one of the\nmost widely used audio features in speech processing appli-\ncations. The MFCC is often used due to its ability to mimic\nthe human hearing system and provide information on the\nhuman vocal tract’ shape [35]. The feature extraction process\nis implemented using the Librosa library [12]. To obtain the\nspectrogram, the Short-Term Fourier Transform (STFT) was\nperformed on the speech signal with a window length of\n1024 and a hop length of 512. The MFCCs were obtained by\napplying Mel ﬁlters, taking the log-magnitude, and applying\na Discrete Cosine Transform (DCT) to the spectrograms.\nIn this research, the mean of 50 MFCCs was obtained\nand used for further training and testing the proposed model.\nAccording to [27], emotional characteristics were inherited\nfrom the whole speech ﬁle and were not affected by the\ndetails in the individual frames. Thus, the mean values of\nextracted MFCCs were calculated and mapped into the fea-\nture vector to avoid losing the temporal information when fed\nto static classiﬁers, such as Neural Networks. Besides, the\nmean values of MFCCs were calculated to have a ﬁxed-length\nfeature vector [28]. Such features have been widely used in\nSER systems and have achieved state-of-the-art results [21],\n[22], [29]–[32].\nLastly, standardization was applied before the recognition\nstep to have standardized distributed data. In this research,\nStandard Scaling was used and computed using the Scikit-\nlearn library to standardize the features by removing the mean\nand scaling to unit variance [20].\nB. LSTM-TRANSFORMER MODEL\nWe combined both the LSTM and the Transformer layers to\nlearn the long-term dependencies in speech signalsfor emo-\ntion recognition.\n36020 VOLUME 10, 2022\nF. Andayaniet al.: Hybrid LSTM-Transformer Model for Emotion Recognition From Speech Audio Files\nIn this study, the LSTM layer replaces the positional\nencoding in the Transformer architecture. In addition, the\nuse of positional encoding in the Transformer architecture\nis believed to be the source of higher computational costs\nbecause it must learn the entire history from the beginning at\neach time step [11]. With the recurrent process of the LSTM,\nthe hidden state of the input features is preserved.\nMoreover, LSTM has been developed to solve the short-\nterm memory proble; however, the LSTM alone is insufﬁcient\nto solve the longer-termdependency problem [15]. Therefore,\nthe Multi-Head Attention in the Transformer encoder layer\nhas been modeled and integrated to improve the model’s abil-\nity to learn the long-term dependencies. Multi-Head Atten-\ntion can jointly attend to information from the extracted\nfeatures at different sequence positions. The Transformer\nencoder layer also contains a feed-forward network layer with\nReLu activation and layer normalization.\nThe Transformer layer allows for parallelization, which\nLSTM cannot. This layer enables the model to perform faster\nin learning the long-term dependencies. The combination of\nboth architectures is expected to be better at understanding\nthe long-term dependencies. Thus, it is suitable for the SER\nmodel because the emotion in speech depends highly on\ntemporal information. Finally, the Linear and Softmax layers\nare applied to get the ﬁnal predicted emotion. Dropouts are\nalso applied to the LSTM, the Transformer encoder, and the\nﬁnal Linear layer to avoid overﬁtting.\nIV. EVALUATION\nThis section discusses the speciﬁcations of datasets used\nto evaluate the model’s performance and the experimental\nsettings.\nA. DATASETS\nThe performance of the proposed model was evaluated\non three dataset: RA VDESS, Emo-DB, and the language-\nindependent dataset. The datasets were chosen based on their\ncredibility and wide usage in most research in the SER ﬁeld.\nThese widely-used datasets provide efﬁciency in comparing\nthe performance of the proposed model with other studies\nutilizing the same datasets. Table 1 shows the overview of\ndatasets involved in this experiment.\nThe Ryerson Audio-Visual Database of Emotional Speech\nand Song (RA VDESS) [13] is a publicly available dataset\ncontaining recorded speech and song data. However, in this\nresearch, only the recorded speech data were used.The\nspeech data were recorded in American English by 24 actors\n(12 males and 12 females). The speech data were recordedat\nnormal and strong intensities, except for the neutral emotion.\nOne thousand four hundred forty samples were obtained from\nthe speech data, containing eight different emotions: happi-\nness, sadness, anger, fear, surprise, disgust, calm, neutral.\nAnother dataset used was the Emo-DB [14]. It contains\n535 speech samples in German, recorded by ten actors ﬁve\nmales and ﬁve females). Moreover, the dataset covers seven\ndifferent emotions: anger, fear, disgust, boredom, neutral,\nTABLE 1. Overview of the datasets used.\nhappiness, an sadness. It is also a publicly available dataset\nin the SER research ﬁeld.\nAs there exist many spoken languages around the world,\ndeveloping a versatile SER model that is not limited to a\nspeciﬁc language is important. Therefore, we analyze the\nlanguage-independent dataset, which contains speech from\nEnglish and German languages, to develop a SER model that\nis oblivious to the language of the spoken speech. As a result,\nwe could achieve a versatile SER model that will broaden the\napplication of the proposed SER model that is not limited\nto a speciﬁc language. The language-independent dataset\nwas developed by combining the RA VDESS and Emo-DB\ndatasets. However, only the emotion components contained\nin both datasets were used.\nFurthermore, investigating the model in language-\ndependent and language-independent datasets provides some\ninsights into how the emotion patterns are shared across\ndifferent languages.\nB. EXPERIMENTAL SETTINGS\nThe experiments were conducted on Google Colaborator\nequipped with an Intel(R) Xeon(R) CPU @ 2.20GHz, 25GB\nRAM, and NVIDIA Tesla T4 GPUs. We also used the Python\nprogramming framework to implement the proposed model.\nThe model used 64 dimensions of the LSTM hidden layer\nand four layers of the Transformer encoder layer. Each Trans-\nformer encoder layer consisted of four heads of self-attention\nfollowed by 512 dimensions of feed-forward layer and layer\nnormalization. The Linear and Softmax layers further pro-\ncessed the ﬁnal output to predict the emotion.\nThe batch size was set to 60 because it showed the best\nperformance, while the SGD optimizer was adapted with a\nlearning rate of 0.001 and 0.9 momentum during the training.\nThe model was trained for 750 epochs. Lastly, 10-fold cross-\nvalidation [21], [22], [31] and a hold-out procedure were\nused to assess the model’s performance. Both procedures are\ncommon techniques used in evaluatingSER models perfor-\nmance. In addition, 10% of the samples from each emotion\non each dataset were held-out for ﬁnal testing. The rest of the\nsamples were used to perform the 10-fold cross-validation.\nVOLUME 10, 2022 36021\nF. Andayaniet al.: Hybrid LSTM-Transformer Model for Emotion Recognition From Speech Audio Files\nTABLE 2. The results of 10-fold cross-validation on three datasets.\nThe 10-fold cross-validation was performed as emotion-\nindependent cross-validation. The weighted accuracy (WA)\nand unweighted accuracy (UA) were calculated to evalu-\nate the performance of the proposed model. A comparative\nstudy was also conducted to analyze the proposed model’s\nperformance.\nV. RESULTS AND DISCUSSIONS\nThe model was ﬁrst evaluated using 10-fold cross-validation\non each dataset in the experiments.Ten percent of each emo-\ntion on each dataset was held out for testing the model after\nthe 10-fold cross-validation process. The rest of the samples\nwere shufﬂed and randomly split into ten folds of approxi-\nmately equal size. Each k subset was used for validation, and\nthe remaining k-1 subsets were used for training the model.\nThe process was repeated ten times. The accuracy results\nof 10 folds were obtained, and the average accuracy of the\nten folds was used to determine the performance. Table 2\nshows the results obtained from the 10-fold cross-validation\ncalculated from the validation set.\nTable 2 shows that the highest recognition was obtained\nwhen the model was trained with the Emo-DB dataset,\nachieving an average of 77.19% recognition rate. The model\nalso acquired 69.61and 70.20% average recognition rates on\nthe RA VDESS and language-independent datasets. Although\nRA VDESS achieved the lowest average recognition rate, the\nmodel achieved up to 74.42% recognition rate and performed\nwell on the hold-out test set. Besides, a 70.20% recognition\nrate with the language-independent dataset showed that the\nmodel could predict emotions in different languages.\nSubsequently, the best model for each dataset was selected\nand used to plot the accuracy and loss curves. Fig. 2, Fig. 3,\nand Fig. 4 show the learning curves on RA VDESS, Emo-\nDB, and language-independent datasets, respectively. The\nlearning curves show that the validation loss and the valida-\ntion accuracy improve with the number of iterations. How-\never, they become rather constant after around 300 epochs.\nNevertheless, the model achieved better performance when\ntrained on 750 epochs instead of 300 epochs, which means\nTABLE 3. Performance of the proposed model on the hold-out test set on\nthree datasets.\nincreasing the number of iterations can improve the model’s\nperformance, despite the minor improvements in validation\nloss and accuracy. It is similar to the study done by [33].\nIn [33], the model performed better on 4000 epochs than on\n100 epochs.\nMoreover, the validation accuracycurve of the Emo-DB\ndataset, as shown in Fig. 3, achieved 91.67% accuracy, which\nis higher than the training accuracy curve. It may be caused\nby the imbalanced division of the dataset on that partic-\nular fold and the complexity of the validation set, which\nrequires further investigation.The validation loss curves on\nthe RA VDESS and language-independent datasets can be\nfurther investigate, as they seem to be relatively high in the\nlearning curves.\nThe selected best model for each dataset was tested on the\n10% hold-out test set. The WA and UA were calculated from\nthe test set, as shown in Table 3. According to Table 3, the\nbest accuracy was on the Emo-DB dataset, followed by the\nRA VDESS and the language-independent datasets. In addi-\ntion, the RA VDESS result (77.33% WA and 75.62% UA)\nwas slightly lower than the result obtained from the Emo-\nDB dataset. It is an acceptable rate due to the complexity\nof the RA VDESS dataset. Therefore, it requires further pre-\nprocessing steps than the Emo-DB dataset, where the samples\ncontain less noise. Consequently, when tested on the Emo-\nDB dataset, the model achieved a higher recognition rate\n(87.72% WA and 85.55% UA). Moreover, when tested on the\nlanguage-independent dataset, the model achieved a 71.43%\nWA and a 72.50% UA The UA performed in a language-\nindependent dataset indicates that the model can deal with\nimbalanced data among emotional classes in that dataset.\nFurthermore, the results obtained from the hold-out test\nset were higher than the average accuracy obtained from\nthe 10-fold cross-validation,indicating that the model could\ngeneralize well on the new data while not demonstrating any\noverﬁtting.\nThe confusion matrix for the best classiﬁcation result on\neach dataset was generated to show the actual predicted emo-\ntions. Each row in the confusion matrix shows the actual emo-\ntion, whereas each column in the confusion matrix shows the\npredicted emotion. The diagonal values indicate the number\nof correctly predicted emotions.\nThe confusion matrices on RA VDESS, Emo-DB, and\nlanguage-independent datasets are shown in Table 4, 5, and 6,\nrespectively. Table 4 (RA VDESS) shows that the best\n36022 VOLUME 10, 2022\nF. Andayaniet al.: Hybrid LSTM-Transformer Model for Emotion Recognition From Speech Audio Files\nFIGURE 2. RAVDESS dataset loss (left) and accuracy (right) curves from fold 10.\nFIGURE 3. EmoDB dataset loss (left) and accuracy (right) curve from fold 2.\nFIGURE 4. Language-independent dataset loss (left) and accuracy (right) curve from fold 7.\nrecognition was achieved for the calm emotion (95%). Our\nmodel acquired the highest recognition for the calm emotion\namong the models proposed in other studies to the best\nof our knowledge. The neutral emotion achieved the least\nVOLUME 10, 2022 36023\nF. Andayaniet al.: Hybrid LSTM-Transformer Model for Emotion Recognition From Speech Audio Files\nTABLE 4. Confusion matrix of the proposed model on RAVDESS with 75.62% of unweighted accuracy. The model achieved the best recognition for calm\nemotion.\nTABLE 5. Confusion matrix of the proposed model on Emo-DB with 85.55% of unweighted accuracy. The model achieved the best recognition for anger,\nfear, and sad emotions.\nTABLE 6. Confusion matrix of the proposed model on language-independent dataset with 72.50% of unweighted accuracy. The model achieved the best\nrecognition for neutral emotion.\nrecognition rate, which is affected by the small sample size of\nneutral emotion in the RA VDESS dataset. Table 5 (Emo-DB)\nshows that the model obtained the best recognition results for\nanger, fear, and sadness, which achieved a 100% recognition\n36024 VOLUME 10, 2022\nF. Andayaniet al.: Hybrid LSTM-Transformer Model for Emotion Recognition From Speech Audio Files\nTABLE 7. Performance of the proposed model against other publications\non the RAVDESS dataset.\nrate. It reached a lower recognition for disgust emotion\nmainly due to the small training sample size. On the other\nhand, the model performed well for all the emotions in the\nlanguage-independent dataset, except for happiness and fear,\nas shown in Table 6. In addition, there is an improvement\nin the neutral emotion recognition when evaluated under\na language-independent method, indicating that the neutral\nemotion shares the same emotion pattern between the English\nand German languages. However, the low recognition of\nhappiness and fear emotions shows that both emotions may\nhave different patterns between the English and German\nlanguages, which requires further investigation to differen-\ntiate them. Nevertheless, the model performed well on the\nlanguage-independent dataset. Thus, it will perform well\nwhen implemented in real-time applications where a versatile\nSER model is required.\nThe results obtained in the experiments are compared\nwith other LSTM and Transformer models proposed by\nexisting researchers. The comparative study was done as\nthey shared the same datasets. The comparisons should be\nviewed as indicative benchmarking instead of absolute one-\nto-one performance rankings [34] because the methods used\nbetween researchers are different, such as the experimental\nsettings, the features utilized, and the classiﬁcation models.\nTable 7 and Table 8 show the performance comparison of\nthe proposed model with other models on the RA VDESS and\nEmo-DB datasets, respectively.\nOur proposed hybrid model outperformed the models of\nParry et al. [16], Jalal et al. [17], and Zeng et al. [18]\nusing the RA VDESS dataset in our testing. Likewise, our\nproposed model also performed better than Parry et al.[16],\nKerkeni et al.[21], Kerkeni et al. [22], and Jing, Manting,\nand Li [23] when evaluated using the Emo-DB dataset.\nIn addition, our proposed model outperformed Jing, Manting,\nand Li’s [23] Transformer model with positional encoding,\nwhich demonstrated that incorporation of LSTM into the\nTransformer model improved the recognition performance by\nTABLE 8. Performance of the proposed model against other publications\non the Emo-DB dataset.\nmaintaining the hidden state of the input features with long-\nterm dependency.\nVI. CONCLUSION AND FUTURE WORK\nThis paper proposes a hybrid model for SER by combining\nthe LSTM and Transformer architectures. The strengths of\nboth architectures are adapted to improve the recognition\nperformance in the SER. Our hybrid model performed better\nat learning the long-term dependencies in speech signals by\npreserving the hidden state of input features using LSTM\nand the use of Multi-Head Attention on the Transformer\nencoder layer and the MFCC feature vectors. The proposed\nhybrid model is able to learn the temporal information from\nthe frequency distributions in the MFCCs of each emo-\ntion in both language-independent and language-dependent\ndatasets. The hybrid model’s effectiveness is shown through\nthe results obtained from the experiments in this research.\nThe proposed model was evaluated on the RA VDESS, Emo-\nDB, and language-independent datasets. The recognition\nrate improved from 15.27% to 21.65% for the RA VDESS\ndataset compared with other published models [16]–[18]\nand improved from 2.55% to 25.88% for the Emo-DB\ndataset compared with other published works [16], [21]–[23].\nIn addition, the proposed model achieved a WA of 71.43%\nand a UA of 72.50% on the language-independent dataset.\nIn conclusion, the proposed model shows a signif-\nicant improvement for the language-dependent datasets,\nRA VDESS and Emo-DB. Besides, the results from the\nlanguage-independent dataset indicate that the model can\nperform well in a mixed language situation. It also shows\nthat the happiness and fear emotions have different patterns\nVOLUME 10, 2022 36025\nF. Andayaniet al.: Hybrid LSTM-Transformer Model for Emotion Recognition From Speech Audio Files\nbetween languages, whereas the neutral emotion shows a\nsimilar pattern.\nIn the future, an improvement to the pre-processing method\nmay be carried out, especially on the language-independent\ndataset. It was discovered that resampling the speech data\non the language-independent dataset could affect the quality\nof the speech samples, which may have deteriorated the\nrecognition outcomes. Besides, our proposed hybrid model\ncan be improved by incorporating additional feature types in\nthe SER training and recognition, such as prosodic or deep\nfeatures. It can also be enhanced for real-time SER systems\nby incorporating the variable input sequence and raw audio\ninputs. Data augmentation can be applied to overcome the\nproblem of data shortage in training and testing datasets.\nDatasets from other languages can be added to improve the\nlanguage-independent emotion recognition ability. The pro-\nposed hybrid model on cross-corpus speech emotion recog-\nnition should also be investigated.\nREFERENCES\n[1] J. Sidorova, ‘‘Speech emotion recognition,’’ Master Thesis, Univ. Pompeu\nFabra, Barcelona, Tech. Rep., 2007, doi: 10.13140/RG.2.1.3498.0724\n[2] L. Kerkeni, Y . Serrestou, M. Mbarki, K. Raoof, and M. A. Mahjoub,\n‘‘A review on speech emotion recognition: Case of pedagogical\ninteraction in classroom,’’ in Proc. Int. Conf. Adv. Technol. Sig-\nnal Image Process. (ATSIP), Fez, Morocco, May 2017, pp. 1–7, doi:\n10.1109/ATSIP.2017.8075575.\n[3] M. Lech, M. Stolar, C. Best, and R. Bolia, ‘‘Real-time speech emotion\nrecognition using a pre-trained image classiﬁcation network: Effects of\nbandwidth reduction and companding,’’ Frontiers Comput. Sci., vol. 2,\np. 14, May 2020, doi: 10.3389/fcomp.2020.00014.\n[4] K. H. Lee, H. Kyun Choi, B. T. Jang, and D. H. Kim, ‘‘A study on speech\nemotion recognition using a deep neural network,’’ in Proc. Int. Conf. Inf.\nCommun. Technol. Converg. (ICTC), Jeju Island, South Korea, Oct. 2019,\npp. 1162–1165, doi: 10.1109/ICTC46691.2019.8939830.\n[5] P. Tzirakis, J. Zhang, and B. W. Schuller, ‘‘End-to-end speech emotion\nrecognition using deep neural networks,’’ in Proc. IEEE Int. Conf. Acoust.,\nSpeech, Signal Process. (ICASSP), Calgary, AB, Canada, Apr. 2018,\npp. 5089–5093, doi: 10.1109/ICASSP.2018.8462677.\n[6] Y . Xie, R. Liang, Z. Liang, C. Huang, C. Zou, and B. Schuller, ‘‘Speech\nemotion classiﬁcation using attention-based LSTM,’’ IEEE/ACM Trans.\nAudio, Speech, Language Process., vol. 27, no. 11, pp. 1675–1685,\nJul. 2019, doi: 10.1109/TASLP.2019.2925934.\n[7] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ 2017,\narXiv:1706.03762.\n[8] V . Heusser, N. Freymuth, S. Constantin, and A. Waibel, ‘‘Bimodal\nspeech emotion recognition using pre-trained language models,’’ 2019,\narXiv:1912.02610.\n[9] S. Lee, D. K. Han, and H. Ko, ‘‘Fusion-ConvBERT: Parallel convolution\nand BERT fusion for speech emotion recognition,’’ Sensors, vol. 20, no. 22,\np. 6688, Nov. 2020, doi: 10.3390/s20226688.\n[10] A. Tanikawa. (2020). Text Generation With LSTM+Transformer\nModel (Japanese). [Online]. Available: https://note.com/diatonic\n_codes/n/nab29c78bbf2e\n[11] Y . Sakatani, ‘‘Combining RNN with transformer for modeling multi-leg\ntrips,’’ in Proc. ACM WSDM WebTour, Jerusalem, Israel, 2021, pp. 50–52.\n[12] B. McFee et al., ‘‘Librosa/librosa,’’ Zenodo, 2020, doi: 10.5281/zenodo.\n591533.\n[13] S. R. Livingstone and F. A. Russo, ‘‘The Ryerson Audio-Visual Database\nof Emotional Speech and Song (RA VDESS): A dynamic, multimodal\nset of facial and vocal expressions in North American English,’’ PLoS\nONE, vol. 13, no. 5, May 2018, Art. no. e0196391, doi: 10.1371/\njournal.Pone.0196391.\n[14] F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, and B. Weiss,\n‘‘A database of German emotional speech,’’ in Proc. INTERSPEECH,\nLisbon, Portugal, 2005, pp. 1517–1520, doi: 10.21437/Interspeech.2005-\n446.\n[15] Z. Dai, Z. Yang, Y . Yang, J. Carbonell, Q. V . Le, and R. Salakhutdinov,\n‘‘Transformer-XL: Attentive language models beyond a ﬁxed-\nlength context,’’ in Proc. 57th Annu. Meeting Assoc. Comput.\nLinguistics, Florence, Italy, 2019, pp. 2978–2988. [Online]. Available:\nhttps://aclanthology.org/P19-1285.pdf\n[16] J. Parry, D. Palaz, G. Clarke, P. Lecomte, R. Mead, M. Berger, and\nG. Hofer, ‘‘Analysis of deep learning architectures for cross-corpus speech\nemotion recognition,’’ in Proc. INTERSPEECH, Graz, Austria, 2019,\npp. 1650–1656, doi: 10.21437/Interspeech.2019-2753.\n[17] A. Md Jalal, E. Loweimi, R. K. Moore, and T. Hain, ‘‘Learning tem-\nporal clusters using capsule routing for speech emotion recognition,’’\nin Proc. INTERSPEECH, Graz, Austria, 2019, pp. 1701–1705, doi:\n10.21437/Interspeech.2019-3068.\n[18] Y . Zeng, H. Mao, D. Peng, and Z. Yi, ‘‘Spectrogram based multi-task audio\nclassiﬁcation,’’ Multimedia Tools Appl., vol. 78, no. 3, pp. 3705–3722,\nFeb. 2019, doi: 10.1007/s11042-017-5539-3.\n[19] Z. Huang, P. Xu, D. Liang, A. Mishra, and B. Xiang, ‘‘TRANS-BLSTM:\nTransformer with bidirectional LSTM for language understanding,’’ 2020,\narXiv:2003.07000.\n[20] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel,\nM. Blondel, A. Müller, J. Nothman, G. Louppe, P. Prettenhofer, R. Weiss,\nV . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher,\nM. Perrot, and E. Duchesnay, ‘‘Scikit-learn: Machine learning in Python,’’\nJ. Mach. Learn. Res., vol. 12, pp. 2825–2830, Oct. 2012.\n[21] L. Kerkeni, Y . Serrestou, M. Mbarki, K. Raoof, and M. A. Mahjoub,\n‘‘Speech emotion recognition: Methods and cases study,’’ in Proc. 10th\nInt. Conf. Agents Artif. Intell., Madeira, Portugal, 2018, pp. 175–182, doi:\n10.5220/0006611601750182.\n[22] L. Kerkeni, Y . Serrestou, M. Mbarki, K. Raoof, M. A. Mahjoub, and\nC. Cleder, ‘‘Automatic speech emotion recognition using machine learn-\ning,’’ in Proc. Social Media Mach. Learn., London, U.K., 2019, pp. 1–4,\ndoi: 10.5772/intechopen.84856.\n[23] D. Jing, T. Manting, and Z. Li, ‘‘Transformer-like model with linear\nattention for speech emotion recognition,’’ J. Southeast Univ., vol. 37,\nno. 2, pp. 164–170, Jun. 2021, doi: 10.3969/j.issn.1003-7985.2021.02.005.\n[24] D. Craft, Dialectical Behavior Therapy: Control Your Emotions, Overcome\nMood Swings and Balance Your Life With DBT. Scotts Valley, CA, USA:\nCreateSpace Independent Publishing Platform, 2018.\n[25] L. Huang, J. Dong, D. Zhou, and Q. Zhang, ‘‘Speech emotion recognition\nbased on three-chanel feature fusion of CNN and BiLSTM,’’ in Proc.\nICIAI, Xiamen, China, 2020, pp. 52–58, doi: 10.1145/3390557.3394317.\n[26] S. Ottl, S. Amiriparian, M. Gerczuk, V . Karas, and B. Schuller, ‘‘Group-\nlevel speech emotion recognition utilizing deep spectrum features,’’\nin Proc. ICMI, Amsterdam, The Netherlands, 2020, pp. 821–826, doi:\n10.1145/3382507.3417964.\n[27] D. Rana and A. Jain, ‘‘Effect of windowing on the calculation of MFCC\nstatistical parameter for different gender in Hindi speech,’’ Int. J. Comput.\nAppl., vol. 98, no. 8, pp. 6–10, 2014.\n[28] D. Bitouk, R. Verma, and A. Nenkova, ‘‘Class-level spectral features for\nemotion recognition,’’ Speech Commun., vol. 52, nos. 7–8, pp. 613–625,\n2010, doi: 10.1016/j.specom.2010.02.010.\n[29] U. Tiwari, M. Soni, R. Chakraborty, A. Panda, and S. Kopparapu, ‘‘Multi-\nconditioning and data augmentation using generative noise model for\nspeech emotion recognition in noisy conditions,’’ in Proc. IEEE Int. Conf.\nAcoust., Speech Signal Process. (ICASSP), Barcelona, Spain, May 2020,\npp. 7194–7198, doi: 10.1109/ICASSP40776.2020.9053581.\n[30] J. Ancilin and A. Milton, ‘‘Improved speech emotion recognition with\nMel frequency magnitude coefﬁcient,’’ Appl. Acoust., vol. 179, Aug. 2021,\nArt. no. 108046, doi: 10.1016/j.apacoust.2021.108046.\n[31] R. Rumagit, G. Alexander, and I. Saputra, ‘‘Model comparison in\nspeech emotion recognition for Indonesian language,’’ Proc. Comput. Sci.,\nvol. 179, pp. 789–797, Dec. 2021.\n[32] O. U. Kumala and A. Zahra, ‘‘Indonesian speech emotion recognition using\ncross-corpus method with the combination of MFCC and teager energy\nfeatures,’’Int. J. Adv. Comput. Sci. Appl., vol. 12, no. 4, pp. 163–168, 2021.\n[33] S. Shahsavarani, ‘‘Speech emotion recognition using convolution neural\nnetworks,’’ M.S. thesis, Dept. Comp. Sci. Eng., Nebraska Univ., Lincoln,\nNebraska, 2018.\n[34] J. Rintala, ‘‘Speech emotion recognition from raw audio using deep\nlearning,’’ M.S. thesis, Dept. Comp. Sci. Eng., KTH Royal Inst. Tech.,\nStockholm, Sweden, 2020.\n[35] D. M. Waqar, T. S. Gunawan, M. A. Morshidi, and M. Kartiwi, ‘‘Design\nof a speech anger recognition system on Arduino nano 33 BLE sense,’’\nin Proc. IEEE 7th Int. Conf. Smart Instrum., Meas. Appl. (ICSIMA),\nMay 2021, pp. 64–69, doi: 10.1109/ICSIMA50015.2021.9526323.\n36026 VOLUME 10, 2022\nF. Andayaniet al.: Hybrid LSTM-Transformer Model for Emotion Recognition From Speech Audio Files\nFELICIA ANDAYANI received the bachelor’s\ndegree in information and communication technol-\nogy from the Swinburne University of Technology\nSarawak Campus, Malaysia, in 2019, and the\nMaster of Science (by Research) degree in artiﬁ-\ncial intelligence and speech emotion recognition\nresearch.\nLAU BEE THENG (Senior Member, IEEE) has\nactively contributed to her research areas with var-\nious edited books, peer-reviewed journals, con-\nference proceedings, higher degrees in research,\nand funded research projects. She is currently a\nSenior Member of the Association of Comput-\ning Machinery, a Professional Technologist, and\na Certiﬁed Software Tester. Her research interests\ninclude artiﬁcial intelligence in activity recogni-\ntion, natural scene text recognition, speech emo-\ntion detection, road accidents recognition, wafer surface defect detection,\nﬁnancial risks recognition, and aesthetic preference of design objects.\nMARK TEEKIT TSUN(Member, IEEE) received\nthe B.Sc. degree (Hons.) in computer science\nfrom Coventry University, in 2005, the master’s\ndegree in software engineering from OUM, and the\nB.Eng. (Hons.) and Ph.D. degrees from the Swin-\nburne University of Technology Sarawak Cam-\npus, in 2014 and 2018, respectively. He worked\nin the software development industry, until 2008.\nHe joined the Faculty of Engineering, Computing,\nand Science as a Lecturer, in 2019. His research\ninterests include computer game development, drone technology applica-\ntions, assistive robotics for injury prevention, human–robot interaction, assis-\ntive technologies, virtual, augmented, mixed reality, drone development, and\nthe Internet of Things (IoT).\nCASLON CHUA received the B.Sc., M.Sc.,\nand Ph.D. degrees in computer science from\nDe La Salle University, Manila, Philippines,\nin 1988, 1993, and 1999, respectively. He is cur-\nrently the Acting Department Chair of Comput-\ning Technologies with the School of Software\nand Electrical Engineering, Swinburne Univer-\nsity of Technology, Hawthorn, VIC, Australia.\nHis research interests include computing edu-\ncation, data visualization, database systems,\nhuman–computer interactions, and software engineering.\nVOLUME 10, 2022 36027"
}