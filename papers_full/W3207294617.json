{
  "title": "A deep learning based graph-transformer for whole slide image classification",
  "url": "https://openalex.org/W3207294617",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5101675151",
      "name": "Yi Zheng",
      "affiliations": [
        "Boston University"
      ]
    },
    {
      "id": "https://openalex.org/A5042123787",
      "name": "Rushin Gindra",
      "affiliations": [
        "Boston University"
      ]
    },
    {
      "id": "https://openalex.org/A5058798782",
      "name": "Margrit Betke",
      "affiliations": [
        "Boston University"
      ]
    },
    {
      "id": "https://openalex.org/A5059272397",
      "name": "Jennifer Beane",
      "affiliations": [
        "Boston University"
      ]
    },
    {
      "id": "https://openalex.org/A5003969045",
      "name": "Vijaya B. Kolachalama",
      "affiliations": [
        "Boston University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2040414046",
    "https://openalex.org/W2126096721",
    "https://openalex.org/W2216754497",
    "https://openalex.org/W2964756323",
    "https://openalex.org/W2971376088",
    "https://openalex.org/W2952846726",
    "https://openalex.org/W2760946358",
    "https://openalex.org/W2796409016",
    "https://openalex.org/W3108327761",
    "https://openalex.org/W3165497806",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W2988856610",
    "https://openalex.org/W3042024476",
    "https://openalex.org/W3034203199",
    "https://openalex.org/W2329674354",
    "https://openalex.org/W130099911",
    "https://openalex.org/W3005680577",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2995426144",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3034065136",
    "https://openalex.org/W3112516115",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2136655611",
    "https://openalex.org/W2243462087",
    "https://openalex.org/W3099404130"
  ],
  "abstract": "Abstract Deep learning is a powerful tool for assessing pathology data obtained from digitized biopsy slides. In the context of supervised learning, most methods typically divide a whole slide image (WSI) into patches, aggregate convolutional neural network outcomes on them and estimate overall disease grade. However, patch-based methods introduce label noise in training by assuming that each patch is independent with the same label as the WSI and neglect the important contextual information that is significant in disease grading. Here we present a Graph-Transformer (GT) based framework for processing pathology data, called GTP, that interprets morphological and spatial information at the WSI-level to predict disease grade. To demonstrate the applicability of our approach, we selected 3,024 hematoxylin and eosin WSIs of lung tumors and with normal histology from the Clinical Proteomic Tumor Analysis Consortium, the National Lung Screening Trial, and The Cancer Genome Atlas, and used GTP to distinguish adenocarcinoma (LUAD) and squamous cell carcinoma (LSCC) from those that have normal histology. Our model achieved consistently high performance on binary (tumor versus normal: mean overall accuracy = 0.975 ± 0.013) as well as three-label (normal versus LUAD versus LSCC: mean accuracy = 0.932 ± 0.019) classification on held-out test data, underscoring the power of GT-based deep learning for WSI-level classification. We also introduced a graphbased saliency mapping technique, called GraphCAM, that captures regional as well as contextual information and allows our model to highlight WSI regions that are highly associated with the class label. Taken together, our findings demonstrate GTP as a novel interpretable and effective deep learning framework for WSI-level classification.",
  "full_text": "ZHENG et al.: GRAPH TRANSFORMERS FOR DIGITAL PATHOLOGY 1\nA deep learning based graph-transformer for\nwhole slide image classiﬁcation\nYi Zheng1,2, Rushin Gindra2, Margrit Betke1, Jennifer E. Beane2, Vijaya B. Kolachalama1,2\n1 Department of Computer Science, College of Arts and Sciences, Boston University, Boston, MA, USA\n2 Department of Medicine, Boston University School of Medicine, Boston, MA, USA\nAbstract— Deep learning is a powerful tool for assessing\npathology data obtained from digitized biopsy slides. In the\ncontext of supervised learning, most methods typically di-\nvide a whole slide image (WSI) into patches, aggregate con-\nvolutional neural network outcomes on them and estimate\noverall disease grade . However, patch-based methods in-\ntroduce label noise in training by assuming that each patch\nis independent with the same label as the WSI and neglect\nthe important contextual information that is signiﬁcant in\ndisease grading. Here we present a Graph-Transformer (GT)\nbased framework for processing pathology data, called\nGTP, that interprets morphological and spatial information\nat the WSI-level to predict disease grade. To demonstrate\nthe applicability of our approach, we selected 3,024 hema-\ntoxylin and eosin WSIs of lung tumors and with normal\nhistology from the Clinical Proteomic Tumor Analysis Con-\nsortium, the National Lung Screening Trial, and The Can-\ncer Genome Atlas, and used GTP to distinguish adeno-\ncarcinoma (LUAD) and squamous cell carcinoma (LSCC)\nfrom those that have normal histology. Our model achieved\nconsistently high performance on binary (tumor versus\nnormal: mean overall accuracy = 0.975 ± 0.013) as well\nas three-label (normal versus LUAD versus LSCC: mean\naccuracy = 0.932 ± 0.019) classiﬁcation on held-out test\ndata, underscoring the power of GT-based deep learning\nfor WSI-level classiﬁcation. We also introduced a graph-\nbased saliency mapping technique, called GraphCAM, that\ncaptures regional as well as contextual information and\nallows our model to highlight WSI regions that are highly\nassociated with the class label. Taken together, our ﬁndings\ndemonstrate GTP as a novel interpretable and effective\ndeep learning framework for WSI-level classiﬁcation.\nIndex Terms— Digital pathology, Graph convolutional\nnetwork, Lung cancer, Transformer\nI. I NTRODUCTION\nC\nOMPUTATIONAL pathology [1]–[4], which entails the\nanalysis of digitized biopsies of a bodily tissue, is gaining\nincreased attention over the past few years. The sheer amount\nof information on a single whole slide image (WSI) typically\ncan exceed over a gigabyte, so traditional image analysis\nroutines may not be able to fully process all this data in an\nefﬁcient fashion. Modern machine learning methods such as\ndeep learning have allowed us to make great progress in terms\nof analyzing WSIs including disease classiﬁcation [5], tissue\nsegmentation [6], mutation prediction [7], spatial proﬁling of\nimmune inﬁltration [8], and so on. Most of these methods\nrely on systematic breakdown of WSIs into image patches,\nfollowed by development of deep neural networks at patch-\nlevel and integration of outcomes on these patches to create\noverall WSI-level estimates. While patch-based approaches\ncatalyzed research in the ﬁeld, the community has begun\nto appreciate the conditions in which they confer beneﬁt\nand in those where they cannot fully capture the underlying\npathology. For example, methods focused on identifying the\npresence or absence of a tumor on an WSI can be developed\non patches using computationally efﬁcient techniques such\nas multiple instance learning [9]. On the other hand, if the\ngoal is to identify the entire tumor region or capture the\nconnectivity of the tumor microenvironment characterizing the\nstage of disease, then it becomes important to assess both local\nand regional information on the WSI. There are several other\nscenarios where both the patch- and WSI-level features need to\nbe identiﬁed to assess the pathology [10], and computational\nmethods to perform such analysis are much needed.\nThe success of patch-based deep learning methods can be\nattributed to the availability of pre-trained deep neural net-\nworks on natural images from public databases (i.e., ImageNet\n[11]). Since there are millions of parameters in a typical deep\nneural network, de novo training of this network requires\naccess to a large set of pathology data, and such resources\nare not necessarily available at all locations. To address\nthis bottleneck, researchers have leveraged transfer learning\napproaches that are pre-trained on ImageNet to accomplish\nvarious tasks. Recently, transformer architectures were applied\ndirectly to sequences of image patches for various classi-\nﬁcation tasks. Speciﬁcally, Vision Transformers (ViT) were\nshown to achieve excellent results compared to state-of-the-\nart convolutional networks while requiring substantially fewer\ncomputational resources for training [12]. Position embeddings\nwere used in ViTs to retain spatial information and capture\nthe association of different patches within the input image.\nExcitingly, the self-attention mechanism in ViT requires the\ncalculation of pairwise similarity scores on all the patches,\nresulting in memory efﬁciency and a simple time complex-\nity that is quadratic in the number of patches. Leveraging\nsuch approaches to perform pathology image analysis is not\ntrivial because each WSI can contain thousands of patches.\nAdditionally, some approximations are often made on these\npatches such as using the WSI-level label on each patch during\ntraining, which are not ideal in all scenarios as there is a\nneed to process both the local information as well as the WSI\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n2\nin its entirety to better understand the pathological correlates\nof disease. Similar to the local and WSI-level examination,\nwe argue that an expert pathologist’s workﬂow also involves\nexamination of the entire biopsy slide using manual operations\nsuch as panning and zooming in and out of speciﬁc regions\nof interest to assess various aspects of disease at multiple\nscales. In the zoom-in assessment, pathologists perform in-\ndepth, microscopic evaluation of local pathology whereas, the\nzoom-out assessment involves obtaining a rational estimate\nof the contextual features on the entire WSI. Both these\nassessments are critical as the pathologist obtains a gestalt on\nvarious features to comprehensively assess the disease [10].\nRecent attempts to perform WSI-level analysis have shown\npromising results in terms of assessing the overall tissue\nmicroenvironment. In particular, graph-based approaches have\ngained a lot of traction due to their ability to represent the\nentire WSI and analyze patterns to predict various outcomes\nof interest. Zhou and colleagues developed a cell-graph con-\nvolutional neural network on WSIs to predict the grade of\ncolorectal cancer (CRC) [13]. In this work, the WSI was\nconverted to a graph, where each nucleus was represented by\na node and the cellular interactions were denoted as edges\nbetween these nodes to accurately predict CRC grade. Also,\nAdnan and colleagues developed a two-stage framework for\nWSI representation learning [14], where patches were sampled\nbased on color and a graph neural network was constructed\nto learn the inter-patch relationships to discriminate lung\nadenocarcinoma (LUAD) from lung squamous cell carcinoma\n(LSCC). In another recent work, Lu and team developed a\ngraph representation of the cellular architecture on the entire\nWSI to predict the status of human epidermal growth factor\nreceptor 2 and progesterone receptor [15]. Their architecture\nattempted to create a bottom-up approach (i.e., nuclei- to WSI-\nlevel) to construct the graph, and in so doing, achieved a\nrelatively efﬁcient framework for analyzing the entire WSI.\nWe contend that integration of computationally efﬁcient\napproaches such as ViTs along with graphs can lead to more\nefﬁcient approaches for the assessment of WSIs. To address\nthis aspect, we developed a graph-based vision transformer\ncalled GTP that leverages the graph-based representation of\npathology images and the computational efﬁciency of trans-\nformer architectures to perform WSI-level analysis. The GTP\nframework involves construction of a graph convolutional\nnetwork by embedding image patches in feature vectors using\ncontrastive learning, followed by the application of a trans-\nformer to predict a WSI-level label corresponding to a speciﬁc\ndisease type. We used WSIs from three publicly available data\nresources to develop a GTP model to distinguish normal WSIs\nfrom those with lung tumors. Additionally, we extended our\nframework to classify normal WSIs from those with LUAD\nor LSCC. We also introduce graph-based class activation\nmapping (GraphCAM), a novel approach to generate WSI-\nlevel saliency maps that are able to identify image regions\nthat are highly associated with the class label.\nII. M ATERIALS AND METHODS\nTABLE I: Study population. Whole slide images and cor-\nresponding clinical information from three distinct cohorts\nincluding the Clinical Proteomic Tumor Analysis Consortium\n(CPTAC), The Cancer Genome Atlas (TCGA) and the Na-\ntional Lung Screening Trial (NLST) were used.\n(a) CPTAC\nDescription Value\nNumber of patients 435\nNumber of whole slide images 2071\nNumber of whole slide images per class 1 719, 667, 685\nNumber of patches 2 1277, [100-8478]\nAge 3 1, 4, 23, 80, 134, 89, 5, 99\nGender 4 235, 101, 99\nRace 5 89, 5, 1, 1, 339\n(b) TCGA\nDescription Value\nNumber of patients 256\nNumber of whole slide images 288\nNumber of whole slide images per class 1 92, 97, 99\nNumber of patches 2 571.5, [100-7570]\nAge 3 10, 1, 12, 42, 93, 84, 16, 8\nGender 4 144, 112, 0\nRace 5 188, 28, 3, 0, 37\n(c) NLST\nDescription Value\nNumber of patients 345\nNumber of whole slide images 665\nNumber of whole slide images per class 1 75, 378, 212\nNumber of patches 2 2679.5, [110-7029]\nAge 3 0, 0, 0, 87, 201, 57, 0, 0\nGender 4 211,134,0\nRace 5 315, 14, 11, 1, 4\n1 Normal, LUAD, LSCC 2 Median, Range\n3 Binned: 20-29, 30-39, 40-49, 50-59, 60-69, 70-79, 80-89, Unknown\n4 Males, Females, Unknown\n5 White, Black or African-American, Asian, American Indian or Alaskan\nNative, Other/unknown\nA. Study population\nWe obtained access to WSI data of lung tumors (LUAD and\nLSCC) and normal tissue from the Clinical Proteomic Tumor\nAnalysis Consortium (CPTAC), the National Lung Screening\nTrial (NLST) and The Cancer Genome Atlas (TCGA) (Table\nI). CPTAC is a national effort to accelerate the understanding\nof the molecular basis of cancer through the application of\nlarge-scale proteome and genome analysis [16]. NLST was\na randomized controlled trial to determine whether screening\nfor lung cancer with low-dose helical computed tomography\nreduces mortality from lung cancer in high-risk individuals\nrelative to screening with chest radiography [17]. TCGA is a\nlandmark cancer genomics program, which molecularly char-\nacterized thousands of primary cancer and matched normal\nsamples spanning 33 cancer types [18]. For each of these\ncases, we also obtained relevant demographic and clinical\ninformation.\nB. Graph-Transformer\nOur proposed Graph-Transformer (GT) network fuses a\ngraph representation G of an WSI and a transformer that can\ngenerate WSI-level predictions in a computationally efﬁcient\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint \nZHENG et al.: GRAPH TRANSFORMERS FOR DIGITAL PATHOLOGY 3\nFig. 1: Schematic of GTP deep learning framework. Each whole slide image (WSI) was divided into patches followed\nby elimination of the patches that predominantly contained the background. Each image patch was then embedded in feature\nvectors by a contrastive learning-based patch embedding module. The feature vectors were then used to build the graph followed\nby a transformer that takes the graph as the input and predicts WSI-level class label.\nfashion (Figure 1). Let G = (V,E ) be an undirected graph\nwhere V is the set of nodes representing the image patches and\nE is the set of edges between the nodes in V that represent\nwhether two image patches are adjacent to each other. We\ndenote the adjacency matrix of Gas A= [Aij] where Aij = 1\nif there exists an edge (vi,vj) ∈Eand Aij = 0 otherwise. An\nimage patch must be connected to other patches and can be\nsurrounded by at most 8 adjacent patches, so the sum of each\nrow or column of Ais at least one and at most 8. A graph\ncan be associated with a node feature matrix F, F ∈I RN×D,\nwhere each row contains the D-dimensional feature vector\ncomputed for an image patch, i.e. node, and N = |V|. An\nexample of a WSI, its patches, associated graph, and node\nfeature matrix are illustrated in Figure S1.\nAs shown in Fig. 1, given a WSI, the classiﬁcation task\ncontains two steps, graph construction and graph interpretation\nby a transformer. The second step aims to learn a mapping\nfrom the WSI-associated graph and its node feature matrix to\nthe corresponding label of the WSI.\nUsing all the pixels within each image patch as features\ncan make model training computationally intractable. Instead,\nour framework applies a feature extractor to generate a vector\ncontaining features and uses it to deﬁne the information\ncontained in an image patch, which is a node in the graph. This\nstep reduces the node feature dimension from Wp×Hp×Cp to\nD, where Wp,Hp, and Cp are width, height, and channel of\nthe image patch, and D ×1 is the dimension of extracted\nfeature vector. The expectation is that the derived feature\nvector provides an efﬁcient representation of the node and\nalso serves as a robust means by which to deﬁne a uniform\nrepresentation of an image patch for graph-based classiﬁcation.\nAs described above, current methods that have been de-\nveloped at patch-level impose WSI-level labels on all the\npatches or use weakly supervised learning to extract feature\nvectors that are representative of the WSI. This strategy is\nnot suitable for all scenarios, especially when learning the\ncontextual information on the WSI is needed. We leveraged a\nstrategy based on self-supervised contrastive learning [19], to\nextract features from the WSIs. This framework enables robust\nrepresentations that can be learned without the need for manual\nlabels. Our approach involves using contrastive learning to\ntrain a CNN that produces embedding representations by max-\nimizing agreement between two differently augmented views\nof the same image patch via a contrastive loss in the latent\nspace (Figure S2). GTP tiles the WSIs from the training set\ninto patches and randomly samples a mini-batch of K patches.\nTwo different data augmentation operations are applied to each\npatch (p), resulting in two augmented patches (p i and pj).\nThe pair of two augmented patches from the same patch is\ndenoted as a positive pair. For a mini-batch ofKpatches, there\nare 2K augmented patches in total. Given a positive pair, the\nother 2K −1 augmented patches are considered as negative\nsamples. Subsequently, our GTP approach uses a CNN to\nextract representative embedding vectors (f i, fj) from each\naugmented patch (p i, pj). The embedding vectors are then\nmapped by a projection head to a latent space (z i, zj) where\ncontrastive leaning loss is applied. The contrastive learning\nloss function for a positive pair of augmented patches (i, j) is\ndeﬁned as:\nli,j = −log exp(sim(zi,zj)/τ)\n∑2K\nk=1 I 1[k̸=i] exp(sim(zi,zk)/τ)\n, (1)\nwhere I 1[k̸=i] ∈{0,1}is an indicator function evaluating to 1\nif and only if k ̸= i and τ denotes a temperature parameter.\nAlso, sim(u,v) = uT v/∥u∥∥v∥ denotes the dot product\nbetween L2 normalized u and v (i.e., cosine similarity). For\nmodel training, the patches were densely cropped without\noverlap and treated as individual images. The ﬁnal loss was\ncomputed across all positive pairs, including both (i, j) and\n(j, i) in a mini-batch. After convergence, we kept the feature\nextractor and used it for our GTP model to compute the\nfeature vectors of the patches from the WSIs. GTP uses\nthese computed feature vectors as node features in the graph\nconstruction phase. Speciﬁcally, we obtained the node-speciﬁc\nfeature matrix F = [f1; f2; ... ; fN ], F ∈I RN×D, where fi\nis the D-dimensional embedding vector obtained from Resnet\ntrained using contrastive learning and N is the number of\npatches from one WSI. Note that N is variable since different\nWSIs contain different numbers of patches. As a result, each\nnode in F corresponds to one patch of the WSI. We deﬁned\nan edge between a pair of nodes in F based on the spatial\nlocation of its corresponding patches on the WSI. If patch i\nis a neighbor of patch j on the WSI (Figure S1), then GTP\ncreates an edge between node i and node j as well as set\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint \n4\nAij = 1 and Aji = 1, otherwise Aij = 0 and Aji = 0. GTP\nuses feature node matrix F and adjacent matrix Ato construct\na graph to represent each WSI.\nThe Graph Transformer component of GTP consists of a\ngraph convolutional (GC) layer, a transformer layer, and a\npooling layer. We implemented the GC layer, introduced by\nKipf & Welling [20], to handle the graph-structured data. The\nGC layer operates message propagation and aggregation in the\ngraph, and is deﬁned as:\nHm+1 = ReLU( ˆAHmWm), m = 1,2,..,M (2a)\nˆA= ˜D−1\n2 ˜A˜D−1\n2 (2b)\nwhere ˆAis the symmetric normalized adjacency matrix of A\nand M is the number of GC layers. Here, ˜A= A+ I is the\nadjacency matrix with a self-loop added to each node, and ˜D\nis a diagonal matrix where ˜Dii = ∑\nj ˜Aij. Hm is the input of\nthe m-th GC layer and H1 is initialized with the node feature\nmatrix F. Additionally, Wm ∈I RCm×Cm+1 is the matrix of\nlearnable ﬁlters in the GC layer, where Cm is the dimension\nof the input and Cm+1 is the dimension of the output.\nThe GC layer of GTP enables learning of node embeddings\nthrough propagating and aggregating needed information.\nHowever, it is not trivial for a model to learn hierarchical\nfeatures that are crucial for graph representation and classiﬁ-\ncation. To address this limitation, we introduced a transformer\nlayer that selects the most signiﬁcant nodes in the graph and\naggregates information via the attention mechanism. Trans-\nformers use a Self-Attention (SA) mechanism to model the\ninteractions between all tokens in a sequence [21], by allowing\nthe tokens to interact with each other (“self”) and ﬁnd out\nwho they should pay more attention to (“attention”), and the\naddition of positional information of tokens further increases\nthe use of sequential order information. Excitingly, the Vison\nTransformer (ViT) enables the application of transformers to\n2D images [12]. Inspired by these studies, we here propose a\ntransformer layer to interpret our graph-structured data. While\nthe SA mechanism has been extensively used in the context\nof natural language processing, we extended the framework\nfor WSI data. Brieﬂy, the standard qkv self-attention [21] is a\nmechanism to ﬁnd the words of importance for a given query\nword in a sentence, and it receives as input a 1D sequence of\ntoken embeddings. For the graph, the feature nodes are treated\nas tokens in a sequence and the adjacency matrix is used to\ndenote the positional information. Given that x ∈RN×D is\nthe sequence of patches (feature nodes) in the graph, where N\nis the number of patches and Dis the embedding dimension of\neach patch, we compute q(query), k(key) and v(value) (Eq.3a).\nThe attention weights Aij are based on the pairwise similarity\nbetween two patches of the sequence and their respective query\nqi and key kj in Eq.3b. Multihead Self-Attention (MSA) is a\nmechanism that involves combining the knowledge explored\nby k number of SA operations, called ”heads”. It projects\nconcatenated outputs of SA in Eq.3c. Dh (Eq.3a) is typically\nset to D/k to facilitate computation and maintain the number\nof parameters constant when changing k.\n[q,k,v] = xUqkv, Uqkv ∈RD×3Dh (3a)\nA= softmax(qkT /\n√\nDh), A ∈RN×N (3b)\nSA(x) = Av, (3c)\nMSA(x) = [SA1(x); SA2(x); ... SAk(x)]Umsa,and\nUmsa ∈Rk·Dh×D. (3d)\nThe goal of the transformer layer is to learn the mapping:\nH →T, where H is the graph space, and T is the transformer\nspace. We deﬁne the mapping of H →T as:\nt0 = [xclass; h(1); h(2); ... ; h(N)], h (i) ∈H (4a)\nt\n′\nl = MSA(LN(tl−1)) + tl−1, l = 1 ...L (4b)\ntl = MLP(LN(t\n′\nl)) + t\n′\nl, l = 1 ...L (4c)\nwhere MSA is the Multiheaded Self-Attention (Eq.3), MLP is\na Multilayer Perceptron, and LN denotes Layer Norm. L is the\nnumber of MSA blocks [12]. The transformer layer consists of\nLMSA layers (Eq.4b) and LMLP blocks (Eq.4c). In order to\nlearn the mapping T →Y from transformer space T to label\nspace Y, we prepared a learnable embedding (t (0)\n0 = xclass)\nto the feature nodes (Eq.4a), whose state at the output of the\ntransformer layer (z0\nL) serves as mapping of T →Y:\ny= LN(z(0)\nL ). (5)\nIn a recent work [12], position embeddings were added\nto the patch embeddings to retain positional information.\nTypically, the position embedding explores absolute position\nencoding (e.g., sinusoidal encoding, learnable absolute encod-\ning) as well as conditional position encoding. However, the\nlearnable absolute encoding is commonly used in problems\nwith ﬁxed length sequences and does not meet the requirement\nfor variable length of input patches in WSI analysis, because\nthe number of patches tiled from the corresponding WSI often\nvaries due to the inherently variable size of the WSI. To handle\nthis problem, Islam and colleagues showed that the addition of\nzero padding can provide an absolute position information for\nconvolution [22]. In our work, the adjacency matrix in the WSI\ngraph which contains the spatial information is encoded with\nthe position information and added to the node features during\ngraph convolution. By taking advantage of graph convolutions\nto aggregate context information, the node features are able to\nobtain both local and contextual information, which enriches\nthe features that are encompassed in each node. In this fashion,\nwe were able to avoid the need of adding an additional encoder\nfor position embeddings, thus reducing the complexity of our\nmodel.\nThe softmax function is typically used as a row-by-row\nnormalization function in transformers for vision tasks [23],\n[24]. The standard self-attention mechanism requires the cal-\nculation of similarity scores between each pair of nodes,\nresulting in both memory and time complexity quadratic in\nthe number of nodes. Since the number of patches in WSIs is\nlarge (potentially several thousands), applying the transformer\nlayer directly to the convolved graphs is not trivial. We\ntherefore added a mincut pooling layer [25] between the graph\nconvolution and transformer layers and reduced the number\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint \nZHENG et al.: GRAPH TRANSFORMERS FOR DIGITAL PATHOLOGY 5\nof input nodes to the transformer layer. In so doing, our\nGTP graph-transformer was able to accommodate thousands\nof image patches as input, which underscores the novelty of\nour approach and its application to WSI data.\nC. Class activation mapping\nTo understand how GT processes WSI data and identi-\nﬁes regions that are highly associated with the class label,\nwe proposed a novel class activation mapping technique on\ngraphs. In what follows, we use the term GraphCAM to refer\nto this technique. Our technique was inspired by the recent\nwork by Chefer and colleagues [26], who used the deep\nTaylor decomposition principle to assign local relevance scores\nand propagated them through the layers by maintaining the\ntotal relevancy across layers. In a similar fashion, our method\ncomputes the class activation map from the output class to the\ninput graph space, and reconstructs the ﬁnal class activation\nmap for the WSI from its graph representation.\nFig. 2: Schematic of the GraphCAM. Gradients and rele-\nvance are propagated through the network and integrated with\nan attention map to produce the transformer relevancy maps.\nTransformer relevancy maps are then mapped to graph class\nactivation maps via reverse pooling.\nLet A(l) represent the attention map of the MSA block l in\nEq.3b. Following the propagation procedure of relevance and\ngradients by Chefer and colleagues [26], GraphCAM computes\nthe gradient ∇A(l) and layer relevance R(nl) with respect to\na target class for each attention map A(l), where nl is the\nlayer that corresponds to the softmax operation in Eq.3b of\nblock l. The transformer relevance map Ct is then deﬁned as\na weighted attention relevance:\nCt =\nL∏\nl=1\n¯A(l) (6a)\n¯A(l) = Eh(∇A(l) ⊙R(nl)) + I (6b)\nwhere ⊙is the Hadamard product, Eh is the mean across the\n“heads” dimension, and I is the identity matrix to avoid self\ninhibition for each node.\nThe pooled node features by the mincut pooling layer are\ncomputed as Xpool = ST X, where S ∈RNg×Nt is the dense\nlearned assignment, and Nt and Ng are the number of nodes\nbefore and after the pooling layer. To yield the graph relevance\nmap Cg from transformer relevance map Ct, our GraphCAM\nperforms mapping Ct to each node in the graph based on the\ndense learned assignments as Ct\nS\n− →Cg. Finally, GraphCAM\nreconstructs the ﬁnal class activation map on the input WSI\nusing the adjacency matrix of the graph and coordinates of\npatches from the WSI.\nD. Data and code availability\nAll the WSIs and corresponding clinical data can be\ndownloaded freely from CPTAC, TCGA and NLST websites.\nPython scripts and manuals are made available on GitHub\n(https://github.com/vkola-lab/GraphCAM).\nIII. E XPERIMENTS\nWe performed several experiments to train and test our GTP\nframework. The NLST data (1.8 million patches) was exclu-\nsively used for contrastive learning to generate patch-speciﬁc\nfeatures (and the feature extractor), which were then used\nto represent each node. The GTP framework was trained on\nthe CPTAC data (2, 071 WSIs) using 5-fold cross validation,\nand the TCGA data (288 WSIs) was used as an independent\ndataset for model testing using the same hyperparameters. We\nalso conducted ablation studies to understand the contributions\nof various components on the overall GTP framework. By\nblocking out the GTP components, we were left with frame-\nworks that were comparable to the state-of-the-art in the ﬁeld.\nFinally, we used GraphCAMs to identify salient regions on\nthe WSIs, and explored their validity in terms of highlighting\nthe histopathologic regions of interest.\nA. Experimental settings\nEach WSI was cropped to create a bag of 512 ×512 non-\noverlapping patches at 20×magniﬁcations, and background\npatches with non-tissue area >50% were discarded. We used\nResnet18 as the CNN backbone used for the feature extractor\n[27]. We adapted the Adam optimizer with an initial learning\nrate of 0.0001, a cosine annealing scheme for learning rate\nscheduling [28], and a mini-batch size of 512. We kept the\ntrained feature extractor and used it to build graphs for the\nGraph-Transformer. We used one graph convolutional layer,\nand set the transformer layer conﬁgurations as L=3, MLP\nsize=128, D=64 and k=8 (Eq.4, Eq.3). The GTP model was\ntrained in batches of 8 examples for 150 iterations. We adopted\nAdam [29] as the optimizer. The learning rate was set to 10−3\ninitially, and decayed to 10−4 and 10−5 at step 30 and 100,\nrespectively.\nB. Ablation studies\nWe compared the effect of contrastive learning on the GTP\nmodel performance by performing studies with and without it.\nLater, we removed the transformer component and trained the\ngraph and compared it with the full GTP framework. In both\nthese studies, we explored various options to build the model,\nincluding the use of pre-training to generate the features in\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint \n6\nlieu of contrastive learning, and also used a graph-based CNN\nto predict the class label as a replacement to the transformer.\nIn essence, these ablation studies allowed us to fully evaluate\nthe power of our interpretable GTP framework in predicting\nWSI-level class labels.\nC. Computational infrastructure\nWe implemented the proposed model using PyTorch\n(v1.9.0). The model was trained using a single NVIDIA\n1080Ti graphics card with 12 GB memory on a GPU worksta-\ntion. The training speed was about 2.4 iterations/s, and training\ntook less than a day to reach convergence. The inference speed\nwas about 30 ms per WSI when the test batch size was 2.\nD. Performance metrics\nFor the tumor versus normal classiﬁcation task, we gen-\nerated receiver operating characteristic (ROC) and precision-\nrecall (PR) curves based on model predictions on the CPTAC\nand TCGA datasets. The ROC curve was computed between\nthe true positive rate and false positive rate using different\nprobability thresholds while PR curve was computed between\nthe true positive rate and the positive predictive value using\ndifferent probability thresholds. For each ROC and PR curve,\nwe also computed the area under curve (AUC), precision,\nrecall, speciﬁcity, and accuracy. For the 3-label classiﬁcation\ntask (LUAD vs. LSCC vs. normal), we also computed the\nprecision, recall, speciﬁcity, and accuracy scores of each class\nalong with confusion matrices for each fold-level prediction.\nThe ROC and PR curves were computed for each label. Since\nwe used 5-fold cross validation, we took all the curves from\ndifferent folds and calculated the mean area under curves and\nthe variance of the curves. Finally, GraphCAMs were used to\ngenerate visualizations and gain a qualitative understanding on\nthe model performance.\nIV. R ESULTS\nThe GTP framework that leveraged contrastive learning\nfollowed by fusion of a graph with a transformer provided\naccurate predictions of WSI-level class labels across a range\nof classiﬁcation tasks (Table II). For the normal vs. tumor\nclassiﬁcation task, high model performance was consistently\nobserved on all the computed metrics including precision,\nrecall, sensitivity, and overall accuracy on both CPTAC test\nand TCGA datasets (all > 0.9), indicating a high degree\nof generalizability. Similar performance was observed on the\nnormal vs. LUAD vs. LSCC task on the CPTAC data but\ndropped slightly on the TCGA dataset. The drop in the model\nperformance was observed particularly on the precision scores\nfor the LUAD and on the recall scores for the LSCC class\nlabels. High model performance was also conﬁrmed via the\nreceiver-operating characteristic (ROC) and precision-recall\n(PR) curves generated on both the CPTAC and TCGA datasets\nfor all the classiﬁcation tasks (Figure 3). On each classiﬁcation\ntask, the mean area under the ROC and PR curves was high\n(all > 0.9) on the CPTAC test data. For the TCGA dataset,\nwhich served as an external testing cohort, the mean area under\nTABLE II: Performance metrics for each class in the 3-label\nand 2-label classiﬁcation tasks.\n(a) 2-label: Normal vs. Tumor (LUAD+LSCC)\nCPTAC Precision Recall/Sensitivity Speciﬁcity\nNormal 0.953 ± 0.022 0.978 ± 0.017 0.974 ± 0.012\nTumor 0.988 ± 0.009 0.974 ± 0.012 0.978 ± 0.017\nTCGA Precision Recall/Sensitivity Speciﬁcity\nNormal 0.902 ± 0.020 0.937 ± 0.023 0.952 ± 0.011\nTumor 0.970 ± 0.011 0.952 ± 0.011 0.937 ± 0.023\n(b) 3-label: Normal vs. LUAD vs. LSCC\nCPTAC Precision Recall/Sensitivity Speciﬁcity\nNormal 0.953 ± 0.022 0.978 ± 0.017 0.974 ± 0.012\nLUAD 0.925 ± 0.032 0.901 ± 0.021 0.965 ± 0.016\nLSCC 0.919 ± 0.022 0.915 ± 0.042 0.959 ± 0.011\nTCGA Precision Recall/Sensitivity Speciﬁcity\nNormal 0.902 ± 0.020 0.937 ± 0.023 0.952 ± 0.011\nLUAD 0.747 ± 0.022 0.841 ± 0.032 0.854 ± 0.021\nLSCC 0.885 ± 0.015 0.741 ± 0.031 0.949 ± 0.009\n(c) Overall accuracy\nCPTAC Accuracy\n2-label 0.975 ± 0.013\n3-label 0.932 ± 0.019\nTCGA Accuracy\n2-label 0.947 ± 0.011\n3-label 0.838 ± 0.009\nthe ROC and PR curves dropped slightly, especially for the\nLUAD and LSCC classiﬁcation tasks. The confusion matrices\nfor the 3-label classiﬁcation problem indicated similar results\n(Figure S3), where the model performance was excellent on\nthe CPTAC test dataset but was slightly lower on the TCGA\ndataset. In particular, the model leaned towards incorrectly\nclassifying a few LSCC cases as LUAD but correctly classiﬁed\nmost of the WSIs with no tumor. However, for the two-label\nclassiﬁcation task (i.e., tumor vs. no-tumor), the mean area\nunder the ROC and PR curves were very high on both the\nCPTAC and TCGA datasets (all > 0.95), indicating accurate\nmodel performance and a fair degree of model generalizability\n(Figure S4).\nThe GT-based class activation maps (GraphCAMs) identi-\nﬁed WSI regions that were highly associated with the output\nclass label of interest (Figure 4). Importantly, the same set\nof WSI regions were highlighted by our method across the\nvarious cross-validation folds (Figure S5), thus indicating\nconsistency of our technique in highlighting salient regions\nof interest. Also, the generated GraphCAMs are class-speciﬁc,\nthus underscoring the superiority of our technique compared to\nother state-of-the-art methods such as self-attention maps. In\nsome cases, we also noticed that the GraphCAMs generated\nfor each class identiﬁed different regions of importance on\nthe same WSI, raising the possibility that a single image may\ncontain disease related information relevant to multiple types\nof lung cancer. On the other hand, the self-attention map that\ncombines attention across all the layers of the model resulted\nin a single heatmap that may not indicate disease speciﬁcity\nbut rather only an association with the classiﬁcation task.\nAlso, since we can generate class-speciﬁc probability for each\nGraphCAM, our approach allows for better appreciation of\nthe model performance and its interpretability in predicting\nan output class label. We must however note that in certain\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint \nZHENG et al.: GRAPH TRANSFORMERS FOR DIGITAL PATHOLOGY 7\nTABLE III: Ablation studies. We used different feature extractors for graph construction also explored the effect of using the\ntransformer by replacing it with a graph classiﬁer. Here, Resnet ⋆ indicates the use of a pre-trained Resnet18 network without\nﬁne-tuning. Also, Resnet † indicates the use of a pre-trained Resnet18 with ﬁne-tuning. CAE represents convolutional auto\nencoder, CL represents contrastive learning used in our method and GT represents the Graph-Transformer. Overall values of\nmean accuracy ±standard deviation, computed across the ﬁve folds, are computed.\n(a) Performance metrics\n2-label 3-label\nCPTAC Label Precision Recall/Sensitivity Speciﬁcity Label Precision Recall/Sensitivity Speciﬁcity\nResnet⋆ + GT\nNormal 0.874 ± 0.051 0.912 ± 0.029 0.927 ± 0.035 Normal 0.874 ± 0.051 0.912 ± 0.029 0.927 ± 0.035\nTumor 0.953 ± 0.013 0.927 ± 0.035 0.912 ± 0.029 LUAD 0.809 ± 0.082 0.725 ± 0.105 0.910 ± 0.047\nLSCC 0.786 ± 0.057 0.800 ± 0.090 0.885 ± 0.055\nResnet† + GT\nNormal 0.901 ± 0.032 0.915 ± 0.031 0.946 ± 0.019 Normal 0.901 ± 0.032 0.915 ± 0.031 0.946 ± 0.019\nTumor 0.955 ± 0.015 0.946 ± 0.019 0.915 ± 0.031 LUAD 0.771 ± 0.030 0.756 ± 0.035 0.894 ± 0.013\nLSCC 0.790 ± 0.039 0.788 + 0 .022 0.894 ± 0.028\nCAE + GT\nNormal 0.893 ± 0.017 0.864 ± 0.042 0.944 ± 0.012 Normal 0.893 ± 0.017 0.864 ± 0.042 0.944 ± 0.012\nTumor 0.930 ± 0.020 0.944 ± 0.012 0.864 ± 0.042 LUAD 0.733 ± 0.041 0.674 ± 0.058 0.881 ± 0.031\nLSCC 0.753 ± 0.028 0.829 ± 0.036 0.863 ± 0.027\nCL + GraphAtt\nNormal 0.878 ± 0.024 0.902 ± 0.028 0.933 ± 0.015 Normal 0.878 ± 0.024 0.902 ± 0.028 0.933 ± 0.015\nTumor 0.948 ± 0.014 0.933 ± 0.015 0.902 ± 0.028 LUAD 0.791 ± 0.027 0.758 + 0 .063 0.905 + 0 .017\nLSCC 0.835 ± 0.046 0.841 ± 0.032 0.916 ± 0.029\nTCGA Label Precision Recall/Sensitivity Speciﬁcity Label Precision Recall/Sensitivity Speciﬁcity\nResnet⋆ + GT\nNormal 0.639 ± 0.203 0.146 ± 0.116 0.964 ± 0.037 Normal 0.639 ± 0.203 0.146 ± 0.116 0.964 ± 0.037\nTumor 0.707 ± 0.027 0.964 ± 0.037 0.146 ± 0.116 LUAD 0.375 ± 0.009 0.893 ± 0.052 0.244 ± 0.050\nLSCC 0.695 + 0.098 0.257 ± 0.040 0.941 ± 0.019\nResnet† + GT\nNormal 0.638 ± 0.036 0.765 ± 0.022 0.794 ± 0.034 Normal 0.638 ± 0.036 0.765 ± 0.022 0.794 ± 0.034\nTumor 0.878 ± 0.010 0.794 ± 0.034 0.765 ± 0.022 LUAD 0.515 ± 0.019 0.536 ± 0.039 0.742 ± 0.033\nLSCC 0.662 ± 0.024 0.509 ± 0.060 0.865 ± 0.014\nCAE + GT\nNormal 0.620 ± 0.019 0.894 ± 0.027 0.742 ± 0.025 Normal 0.620 ± 0.019 0.894 ± 0.027 0.742 ± 0.025\nTumor 0.937 ± 0.014 0.742 ± 0.025 0.894 ± 0.027 LUAD 0.501 ± 0.062 0.342 ± 0.067 0.821 ± 0.051\nLSCC 0.598 ± 0.014 0.529 ± 0.074 0.813 ± 0.034\nCL + GraphAtt\nNormal 0.826 ± 0.016 0.913 ± 0.017 0.909 ± 0.008 Normal 0.826 ± 0.016 0.913 ± 0.017 0.909 ± 0.008\nTumor 0.957 ± 0.008 0.909 ± 0.008 0.913 ± 0.017 LUAD 0.718 ± 0.040 0.753 ± 0.035 0.850 ± 0.023\nLSCC 0.858 ± 0.027 0.732 ± 0.026 0.937 ± 0.012\n(b) Accuracy\nResnet⋆ + GT Resnet † + GT CAE + GT CL + GraphAtt\nCPTAC 2-label 0.922 ± 0.017 0.935 ± 0.010 0.917 ± 0.009 0.923 ± 0.013\n3-label 0.815 ± 0.010 0.822 ± 0.019 0.791 ± 0.011 0.835 ± 0.022\nTCGA 2-label 0.703 ± 0.036 0.785 ± 0.022 0.790 ± 0.015 0.911 ± 0.011\n3-label 0.435 ± 0.022 0.600 ± 0.021 0.583 ± 0.018 0.797 ± 0.026\ncases when the model fails to predict the class label, the\nGraphCAMs may not result in interpretable ﬁndings (Figure\nS6). Nevertheless, we note that the saliency maps reported in\nFigure 4 closely match with expert-identiﬁed regions of tumor\npathology.\nAblation studies revealed that our GTP framework that uses\ncontrastive learning and combines a graph with a transformer\nserved as a superior model for WSI-level classiﬁcation (Ta-\nble III). For example, when contrastive learning was replaced\nwith a pre-trained architecture (Resnet18 with and without ﬁne\ntuning), the model performance for both the 2- and 3-label\nclassiﬁcation tasks dropped. The reduction in performance\nwas evident on both CPTAC and TCGA datasets. The model\nperformance also dropped for both 2- and 3-label classiﬁcation\nwhen we trained a novel convolutional auto-encoder [30]\nin lieu of contrastive learning. These results imply that the\nfeature maps generated via contrastive learning were sufﬁcient\nand maybe even better than other frameworks to encode a\nlarge variety of visual information for GT-based classiﬁcation\nwith a sufﬁcient degree of generalizability. We also replaced\nour proposed mincut pooling with an attention-based pooling\n(AttPool) layer that selects the most signiﬁcant nodes in the\ngraph and aggregates information via the attention mechanism.\nWe then used the same graph convolutional layer as GTP\nin the ablation study and denoted this method as GraphAtt.\nBy aggregating the neighborhood node information via self-\nattention, GTP outperformed GraphAtt for both the 2- and\n3-label classiﬁcation tasks (Table S1). These ﬁndings indicate\nthat our proposed GTP framework is capable of integrating\ninformation across the entire WSI that is represented as a graph\nto accurately predict the output label of interest.\nV. D ISCUSSION\nIn this work, we developed a novel deep learning approach\nthat integrates graphs with vision transformers to generate an\nefﬁcient classiﬁer to predict WSI-level presence of lung tu-\nmors. Our approach also differentiated WSIs with LUAD from\nthose with LSCC. Based on the standards of various model\nperformance metrics, our approach resulted in classiﬁcation\nperformance that exceeded other deep learning architectures\nthat incorporated various state-of-the-art conﬁgurations (see\nablation studies). Finally, our novel class activation mapping\ntechnique allowed us to identify salient WSI regions that were\nhighly associated with the output class label of interest. Thus,\nour ﬁndings represent novel contributions to the ﬁeld of in-\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint \n8\n(a) CPTAC ROC and PR curves\n(b)\nTCGA ROC and PR curves\nFig.\n3: Model performance on the (a) CPTAC and (b)\nTCGA datasets. Mean ROC and PR curves along with\nstandard deviations for the classiﬁcation tasks (normal vs.\ntumor; LUAD vs. others; LSCC vs. others) are shown.\nterpretable deep learning while also simultaneously advancing\nthe ﬁelds of computer vision and digital pathology.\nThe ﬁeld of computational pathology has made important\nstrides in the recent years due to advancements in vision-\nbased deep learning. Still, owing to the sheer size of pathology\nimages generated at high resolution, assessment of WSI-level\ninformation that can integrate spatial signatures along with\nlocal, region-speciﬁc information for prediction of tumor grade\nremains a challenge. The large body of work to date has fo-\ncused on patch-level deep neural networks that may accurately\npredict tumor grade but fail to capture spatial connectivity\ninformation. As a result, identiﬁcation of important image-\nlevel features via such techniques may lead to inconsistent re-\nsults. Our GT-based deep learning framework precisely tackled\nthis scenario by integrating WSI-level information via a graph\nstructure and thus represents an important advancement in the\nﬁeld.\nOne of the novel contributions in our work is the generation\nof graph-based class activation maps (GraphCAM), which\ncan highlight the WSI regions that are highly associated\nwith the output class label. Unlike other saliency mapping\ntechniques such as self-attention maps, GraphCAMs can gen-\nerate class-speciﬁc heatmaps. While self-attention maps can\nidentify image regions (or pixels) that are important for a\nspeciﬁc classiﬁcation task, GraphCAMs can identify image\nregions that trigger the model to associate the image with\na speciﬁc class label. This is a major advantage because an\nimage may contain information pertaining to multiple classes,\nand for these scenarios, identiﬁcation of class-speciﬁc feature\nmaps becomes important. This is especially true in real-world\nscenarios such as pathology images containing lung tumors.\nTypically, lung cancer subtype on WSIs is determined based\non the most predominant pattern, but different patterns may be\npresent on the same WSIs. In such cases, training well-known\nsupervised deep learning classiﬁers such as convolutional\nneural networks that use the overall WSI label for classiﬁcation\nat patch-level or even at the WSI-level may not necessarily\nperform well and even misidentify the regions of interest\nassociated with the class label. By generating class-speciﬁc\nCAMs learned at the WSI-level, our GTP approach provides\nan accurate way by which to identify regions of interest on\nWSIs that are highly associated with the corresponding class\nlabel.\nOur study has a few limitations. We leveraged contrastive\nlearning to generate patch-level feature vectors before con-\nstructing the graph, which turned out to be a computationally\nintensive task. However, our ablation studies revealed that\ncontrastive learning improved model performance when com-\npared to other techniques for feature extraction. Future studies\ncan explore other possible techniques for feature extraction\nthat lead to improved model performance. Our graph was\nconstructed by dividing the WSI into image patches, followed\nby creation of nodes using the embedding features from these\npatches leading to construction of the graph. Other alternative\nways can be explored to deﬁne the nodes and create graphs\nthat are more congruent and spatially connected. While we\nhave demonstrated the applicability of GTP to lung tumors,\nextension of this framework to other cancers is needed to fully\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint \nZHENG et al.: GRAPH TRANSFORMERS FOR DIGITAL PATHOLOGY 9\nWhole Slide Image GraphCAM Self-Attention Map\nLU\nAD LU\nAD (p=0.98) LSCC (p=0.01)\nLSCC LU\nAD (p=0.14) LSCC (p=0.85)\nFig.\n4: Class-speciﬁc GraphCAMs. For each WSI, we generated class-speciﬁc GraphCAMs and also compared them with\nself-attention maps. The ﬁrst column contains the original WSIs, the second and third columns contain LUAD-speciﬁc and\nLSCC-speciﬁc GraphCAMs, and the ﬁnal column contains the self-attention maps. The ﬁrst row represents an LUAD case\nwhere our model also predicted LUAD, and the second row represents an LSCC case where our model predicted LSCC. The\nbold font underneath certain GraphCAMs was used to indicate the model predicted class label for the respective cases. Also,\nthe model-generated probability values are noted beneath each GraphCAM. Since this is a 3-label classiﬁcation task (normal\nvs. LUAD vs. LSCC), the LUAD and LSCC probability values do not add up to 1. Several patches in the GraphCAM with\nthe correct prediction show high values (“warm colors”) while the self attention maps mostly miss the relevant patches (except\nin the last row).\nappreciate its role in terms of assessing WSI-level correlates of\ndisease. In fact, our method is not speciﬁc to cancers and could\nbe adapted to other computational pathology applications.\nIn conclusion, our GTP framework produced an accurate,\ncomputationally efﬁcient model by capturing the entire infor-\nmation available on an WSI to predict the output class label.\nAs a supervised learning framework, GTP can tackle large\nresolution WSIs and predict multiple class labels, leading to\ngeneration of interpretable ﬁndings that are class-speciﬁc. Our\nGTP framework could be scaled to WSI-level classiﬁcation\ntasks on other organ systems and also to predict response to\ntherapy, cancer recurrence and patient survival.\nREFERENCES\n[1] Thomas J. Fuchs and Joachim M. Buhmann, “Computational pathology:\nChallenges and promises for tissue analysis,” Computerized Medical\nImaging and Graphics, vol. 35, no. 7, pp. 515–530, 2011, Whole Slide\nImage Process.\n[2] David N. Louis, Georg K. Gerber, Jason M. Baron, Lyn Bry, Anand S.\nDighe, Gad Getz, John M. Higgins, Frank C. Kuo, William J. Lane,\nJames S. Michaelson, Long P. Le, Craig H. Mermel, John R. Gilbertson,\nand Jeffrey A. Golden, “Computational Pathology: An Emerging\nDeﬁnition,” Archives of Pathology Laboratory Medicine, vol. 138, no.\n9, pp. 1133–1138, 09 2014.\n[3] David N. Louis, Michael Feldman, Alexis B. Carter, Anand S. Dighe,\nJohn D. Pfeifer, Lynn Bry, Jonas S. Almeida, Joel Saltz, Jonathan Braun,\nJohn E. Tomaszewski, John R. Gilbertson, John H. Sinard, Georg K.\nGerber, Stephen J. Galli, Jeffrey A. Golden, and Michael J. Becich,\n“Computational Pathology: A Path Ahead,” Archives of Pathology\nLaboratory Medicine, vol. 140, no. 1, pp. 41–50, 06 2015.\n[4] Esther Abels, Liron Pantanowitz, Famke Aeffner, Mark D Zarella, Jeroen\nvan der Laak, Marilyn M Bui, Venkata NP Vemuri, Anil V Parwani,\nJeff Gibbs, Emmanuel Agosto-Arroyo, Andrew H Beck, and Cleopatra\nKozlowski, “Computational pathology deﬁnitions, best practices, and\nrecommendations for regulatory guidance: a white paper from the Digital\nPathology Association,” The Journal of Pathology, vol. 249, no. 3, pp.\n286–294, 2019.\n[5] Xi Wang, Hao Chen, Caixia Gan, Huangjing Lin, Qi Dou, Efstratios\nTsougenis, Qitao Huang, Muyan Cai, and Pheng-Ann Heng, “Weakly\nsupervised deep learning for whole slide lung cancer image analysis,”\nIEEE Transactions on Cybernetics, vol. 50, no. 9, pp. 3950–3962, 2020.\n[6] Shidan Wang, Donghan M. Yang, Ruichen Rong, Xiaowei Zhan, and\nGuanghua Xiao, “Pathology image analysis using segmentation deep\nlearning algorithms,” The American Journal of Pathology, vol. 189, no.\n9, pp. 1686–1698, 2019.\n[7] Nicolas Coudray, Paolo Santiago Ocampo, Theodore Sakellaropoulos,\nNavneet Narula, Matija Snuderl, David Feny ¨o, Andre L. Moreira,\nNarges Razavian, and Aristotelis Tsirigos, “Classiﬁcation and mutation\nprediction from non–small cell lung cancer histopathology images using\ndeep learning,” Nature Medicine, vol. 24, pp. 1559–1567, 2018.\n[8] J. Saltz, Rajarsi R. Gupta, L. Hou, T. Kurc ¸, P. Singh, Vu Nguyen,\nD. Samaras, K. Shroyer, Tianhao Zhao, R. Batiste, John S. Van Arnam,\nI. Shmulevich, A. Rao, A. Lazar, Ashish Sharma, and V . Thorsson,\n“Spatial organization and molecular correlation of tumor-inﬁltrating\nlymphocytes using deep learning on pathology images.,” Cell reports,\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint \n10\nvol. 23 1, pp. 181–193.e7, 2018.\n[9] Kausik Das, Sailesh Conjeti, Jyotirmoy Chatterjee, and Debdoot Sheet,\n“Detection of breast cancer from whole slide histopathological images\nusing deep multiple instance CNN,” IEEE Access, vol. 8, pp. 213502–\n213511, 2020.\n[10] Yi Zheng, Clarissa A. Cassol, Saemi Jung, Divya Veerapaneni, Vipul C.\nChitalia, Kevin Y .M. Ren, Shubha S. Bellur, Peter Boor, Laura M.\nBarisoni, Sushrut S. Waikar, Margrit Betke, and Vijaya B. Kolachalama,\n“Deep-learning–driven quantiﬁcation of interstitial ﬁbrosis in digitized\nkidney biopsies,” The American Journal of Pathology, vol. 191, no. 8,\npp. 1442–1453, 2021.\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-\nFei, “ImageNet: A large-scale hierarchical image database,” in 2009\nIEEE Conference on Computer Vision and Pattern Recognition, 2009,\npp. 248–255.\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,\nMatthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby, “An image is worth 16x16 words: Transformers for image\nrecognition at scale,” ICLR, 2021.\n[13] Yanning Zhou, Simon Graham, Navid Alemi Koohbanani, Muhammad\nShaban, Pheng-Ann Heng, and Nasir M. Rajpoot, “CGC-Net: Cell\ngraph convolutional network for grading of colorectal cancer histology\nimages,” in 2019 IEEE/CVF International Conference on Computer\nVision Workshops, ICCV Workshops 2019, Seoul, Korea (South), October\n27-28, 2019. IEEE, 2019, p. 388–398, IEEE.\n[14] Mohammed Adnan, S. Kalra, and H. Tizhoosh, “Representation learning\nof histopathology images using graph neural networks,” 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops\n(CVPRW), pp. 4254–4261, 2020.\n[15] Wenqi Lu, Simon Graham, Mohsin Bilal, Nasir Rajpoot, and Fayyaz\nMinhas, “Capturing cellular topology in multi-gigapixel pathology\nimages,” in 2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition Workshops (CVPRW), 2020, pp. 1049–1058.\n[16] Nathan J. Edwards, Mauricio Oberti, Ratna R. Thangudu, Shuang Cai,\nPeter B. McGarvey, Shine Jacob, Subha Madhavan, and Karen A.\nKetchum, “The CPTAC data portal: A resource for cancer proteomics\nresearch,” Journal of Proteome Research, vol. 14, no. 6, pp. 2707–2713,\n2015, PMID: 25873244.\n[17] The National Lung Screening Trial Research Team, “Reduced lung-\ncancer mortality with low-dose computed tomographic screening,” New\nEngland Journal of Medicine, vol. 365, no. 5, pp. 395–409, 2011, PMID:\n21714641.\n[18] National Cancer Institute, “The cancer genome atlas program,” .\n[19] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E.\nHinton, “A simple framework for contrastive learning of visual rep-\nresentations,” in Proceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Virtual Event. 2020,\nvol. 119 of Proceedings of Machine Learning Research, pp. 1597–1607,\nPMLR.\n[20] Thomas N. Kipf and Max Welling, “Semi-supervised classiﬁcation\nwith graph convolutional networks,” in 5th International Conference\non Learning Representations, ICLR 2017, Toulon, France, April 24-26,\n2017, Conference Track Proceedings. 2017, OpenReview.net.\n[21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin, “Attention\nis all you need,” in Advances in Neural Information Processing\nSystems, I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, Eds. 2017, vol. 30, Curran Associates,\nInc.\n[22] Md Amirul Islam*, Sen Jia*, and Neil D. B. Bruce, “How much position\ninformation do convolutional neural networks encode?,” in International\nConference on Learning Representations, 2020.\n[23] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and\nJifeng Dai, “Deformable DETR: deformable transformers for end-to-\nend object detection,” in 9th International Conference on Learning\nRepresentations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\n2021, OpenReview.net.\n[24] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan\nWang, Le Lu, Alan L. Yuille, and Yuyin Zhou, “Transunet: Transformers\nmake strong encoders for medical image segmentation,” CoRR, vol.\nabs/2102.04306, 2021.\n[25] Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi, “Spectral\nclustering with graph neural networks for graph pooling,” inProceedings\nof the 37th International Conference on Machine Learning, ICML 2020,\n13-18 July 2020, Virtual Event. 2020, vol. 119 of Proceedings of\nMachine Learning Research, pp. 874–883, PMLR.\n[26] Hila Chefer, Shir Gur, and Lior Wolf, “Transformer interpretability\nbeyond attention visualization,” CoRR, vol. abs/2012.09838, 2020.\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, “Deep\nresidual learning for image recognition,” in 2016 IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR), 2016, pp. 770–778.\n[28] Ilya Loshchilov and Frank Hutter, “SGDR: stochastic gradient descent\nwith warm restarts,” in 5th International Conference on Learning Rep-\nresentations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference\nTrack Proceedings. 2017, OpenReview.net.\n[29] Diederik Kingma and Jimmy Ba, “Adam: A method for stochastic\noptimization,” International Conference on Learning Representations,\n12 2014.\n[30] Jonathan Masci, Ueli Meier, Dan Cires ¸an, and J ¨urgen Schmidhuber,\n“Stacked convolutional auto-encoders for hierarchical feature extrac-\ntion,” in Artiﬁcial Neural Networks and Machine Learning – ICANN\n2011, Timo Honkela, Włodzisław Duch, Mark Girolami, and Samuel\nKaski, Eds., Berlin, Heidelberg, 2011, pp. 52–59, Springer Berlin\nHeidelberg.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint \nZHENG et al.: GRAPH TRANSFORMERS FOR DIGITAL PATHOLOGY 11\nSUPPLEMENT\nFig. S1: Graph construction. Whole slide images were divided into patches and each patch that contained more than 50% of\nthe area covered by tissue was considered for further processing. Each selected patch was represented as a node and a graph\nwas constructed on the entire WSI using these nodes with an 8-node adjacency matrix. Here, two sets of patches of a WSI\nand their corresponding subgraphs are shown. The subgraphs are connected within the graph representing the entire WSI.\nFig. S2: Contrastive learning to train the feature extractor. We applied two distinct augmentation functions, including\nrandom color distortions, random Gaussian blur, and random cropping followed by resizing back to the original size, on the\nsame sample in a mini-batch. If the mini-batch size is K, then we ended up with 2 ×K augmented observations in the mini-\nbatch. The ResNet received an augmented image leading to an embedding vector as the output. Subsequently, a projection head\nwas applied to the embedding vector which produced the inputs to contrastive learning. The projection head is a multilayer\nperceptron (MLP) with 2 dense layers. In this example, we considered K = 3 samples in a minibatch (A, B & C). For the\nsample A, the positive pairs are (A1, A2) and (A2, A1), and the negative pairs are (A1, B1), (A1, B2), (A1, C1), (A1, C2).\nAll pairs were used for computing contrastive learning loss to train the Resnet. Once the system was trained, we used the\nembedding vectors (straight from the ResNet) for constructing the graph.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint \n12\nFig. S3: Model performance on the CPTAC and TCGA datasets. Confusion matrices for the 5-fold cross validation on the\n(a) CPTAC and the (b) TCGA datasets are shown. A separate confusion matrix is shown for each fold prediction along with\ncorresponding accuracies. Note that for CPTAC dataset, the model performance was evaluated only on the held-out test data.\nModel performance on the TCGA dataset was evaluated using the CPTAC model that was constructed on each fold.\n(a) CPTAC ROC curves\n (b) CPTAC PR curves\n(c) TCGA ROC curves\n (d) TCGA PR curves\nFig. S4: Model performance on the CPTAC and TCGA dataset. Mean ROC and PR curves along with standard deviations\nfor the binary classiﬁcation task (normal vs. tumor) are shown.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint \nZHENG et al.: GRAPH TRANSFORMERS FOR DIGITAL PATHOLOGY 13\nWhole Slide Image GraphCAM from cross-validation models\nLUAD Model 1 (p=0.77) Model 2 (p=0.62) Model 3 (p=0.87) Model 4 (p=0.81) Model 5 (p=0.88)\nLSCC Model 1 (p=0.94) Model 2 (p=0.98) Model 3 (p=0.96) Model 4 (p=0.97) Model 5 (p=0.98)\nFig. S5: Graph class activation map (GraphCAM) performance. GraphCAMs generated on WSIs across the runs performed\nvia 5-fold cross validation are shown. The ﬁrst column shows the original WSI and the other columns show the generated\nGraphCAMs along with prediction probabilities on the cross-validated model runs. The ﬁrst row shows a sample WSI from\nthe LUAD class and the second row shows an WSI from the LSCC class. The colormap of the GraphCAM represents the\nprobability by which an WSI region is associated with the output label of interest. The probability values based on each model\nprediction are noted beneath each GraphCAM. The models created with the ﬁve folds produced GraphCAMs that mostly\nhighlighted the same set of patches at similar levels, thus underscoring the robustness of our method across the folds.\nWhole Slide Image GraphCAM\nLUAD LUAD (p=0.31) LSCC (p=0.50)\nLSCC LUAD (p=0.71) LSCC (p=0.27)\nFig. S6: Class-speciﬁc GraphCAMs for failure cases. The ﬁrst row shows a sample WSI from the LUAD class but the\nmodel prediction was LSCC, and the second row shows an WSI from the LSCC class but the model prediction was LUAD.\nThe ﬁrst column shows the original WSI, and the second and third columns show the generated GraphCAMs along with\nprediction probabilities. The bold font underneath certain GraphCAMs was used to indicate the model predicted class label for\nthe respective cases. Also, the model-generated probability values are noted beneath each GraphCAM. Since this is a 3-label\nclassiﬁcation task (normal vs. LUAD vs. LSCC), the LUAD and LSCC probability values do not add up to 1.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint \n14\nResnet⋆ + GT Resnet † + GT CAE + GT CL + GraphAtt\nCPTAC 3.208 ± 0.378‡ 2.995 ± 0.633§ 3.524 ± 0.561‡ 3.503 ± 0.607‡\nTCGA 10.949 ± 3.742‡ 5.383 ± 0.373‡ 5.622 ± 0.239‡ 1.256 ± 0.492\nTABLE S1: Two-tailed DeLong test to compare AUCs. We used the DeLong test to compare the AUC values of the\nmodels used in the ablation studies. For the ablation studies, we used different feature extractors for graph construction also\nexplored the effect of using the transformer by replacing it with a graph classiﬁer. Here, Resnet ⋆ indicates the use of a\npre-trained Resnet18 network without ﬁne-tuning. Also, Resnet † indicates the use of a pre-trained Resnet18 with ﬁne-tuning.\nCAE represents convolutional auto encoder, CL represents contrastive learning used in our method and GT represents the\nGraph-Transformer. Overall values of mean z-statistic ±standard deviation obtained from the DeLong test are reported. The\nsymbol ‡indicates p-value< 0.001, and § indicates p-value< 0.005. When the CL + GraphAtt model was compared with our\nmodel, the p-value was 0.209 on the TCGA dataset.\n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted October 18, 2021. ; https://doi.org/10.1101/2021.10.15.21265060doi: medRxiv preprint ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6640673279762268
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6488332152366638
    },
    {
      "name": "Deep learning",
      "score": 0.5734336972236633
    },
    {
      "name": "Digital pathology",
      "score": 0.515108048915863
    },
    {
      "name": "Convolutional neural network",
      "score": 0.43301141262054443
    },
    {
      "name": "Grading (engineering)",
      "score": 0.43090492486953735
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.42795228958129883
    },
    {
      "name": "Discriminative model",
      "score": 0.42032933235168457
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I111088046",
      "name": "Boston University",
      "country": "US"
    }
  ],
  "cited_by": 20
}