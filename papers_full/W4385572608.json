{
  "title": "Plug-and-Play Knowledge Injection for Pre-trained Language Models",
  "url": "https://openalex.org/W4385572608",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101584823",
      "name": "Zhengyan Zhang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5104236158",
      "name": "Zhiyuan Zeng",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5043098453",
      "name": "Yankai Lin",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A5100773085",
      "name": "Huadong Wang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5046085212",
      "name": "Deming Ye",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5014699953",
      "name": "Chaojun Xiao",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5101259627",
      "name": "Xu Han",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5100320723",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Peng Cheng Laboratory",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5100432640",
      "name": "Peng Li",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5046448314",
      "name": "Maosong Sun",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A5101853761",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3172335055",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3175518369",
    "https://openalex.org/W2970959783",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W3155220426",
    "https://openalex.org/W3173169192",
    "https://openalex.org/W4385571865",
    "https://openalex.org/W4221141423",
    "https://openalex.org/W3106255016",
    "https://openalex.org/W2184957013",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2997195635",
    "https://openalex.org/W2963777632",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W3182352988",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W3034828027",
    "https://openalex.org/W4294808066",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W4206292552",
    "https://openalex.org/W4224863259",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W3201233724",
    "https://openalex.org/W2789018230",
    "https://openalex.org/W3114916066",
    "https://openalex.org/W4287888691",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W4376654497",
    "https://openalex.org/W2971136144",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4225323055",
    "https://openalex.org/W3176390686",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W2950336186",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3035153870",
    "https://openalex.org/W4385569933",
    "https://openalex.org/W3105601320",
    "https://openalex.org/W4221141197"
  ],
  "abstract": "Zhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong Wang, Deming Ye, Chaojun Xiao, Xu Han, Zhiyuan Liu, Peng Li, Maosong Sun, Jie Zhou. Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2023.",
  "full_text": "Plug-and-Play Knowledge Injection for Pre-trained Language Models\nZhengyan Zhang1∗, Zhiyuan Zeng1∗, Yankai Lin2,3, Huadong Wang1, Deming Ye1\nChaojun Xiao1, Xu Han1†, Zhiyuan Liu1,4,5†, Peng Li6, Maosong Sun1,4, Jie Zhou7\n1NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing\n2Gaoling School of Artiﬁcial Intelligence, Renmin University of China, Beijing\n3 Beijing Key Laboratory of Big Data Management and Analysis Methods\n4International Innovation Center of Tsinghua University, Shanghai5 Quan Cheng Laboratory\n6 Institute for AI Industry Research (AIR), Tsinghua University, China\n7 Pattern Recognition Center, WeChat AI, Tencent Inc\n{zy-z19,zengzy20}@mails.tsinghua.edu.cn {liuzy,sms}@tsinghua.edu.cn\nAbstract\nInjecting external knowledge can improve the\nperformance of pre-trained language models\n(PLMs) on various downstream NLP tasks.\nHowever, massive retraining is required to\ndeploy new knowledge injection methods or\nknowledge bases for downstream tasks. In\nthis work, we are the ﬁrst to study how\nto improve the ﬂexibility and efﬁciency of\nknowledge injection by reusing existing down-\nstream models. To this end, we explore a\nnew paradigm plug-and-play knowledge in-\njection, where knowledge bases are injected\ninto frozen existing downstream models by a\nknowledge plugin. Correspondingly, we pro-\npose a plug-and-play injection method map-\ntuning, which trains a mapping of knowl-\nedge embeddings to enrich model inputs with\nmapped embeddings while keeping model pa-\nrameters frozen. Experimental results on\nthree knowledge-driven NLP tasks show that\nexisting injection methods are not suitable\nfor the new paradigm, while map-tuning ef-\nfectively improves the performance of down-\nstream models. Moreover, we show that a\nfrozen downstream model can be well adapted\nto different domains with different mapping\nnetworks of domain knowledge. Our code\nand models are available at https://github.\ncom/THUNLP/Knowledge-Plugin.\n1 Introduction\nRecent years have witnessed rapid development\nin enhancing pre-trained language models (PLMs)\nwith various external knowledge bases, i.e., knowl-\nedge injection for PLMs (Levine et al., 2020; Zhou\net al., 2020; Zhang et al., 2019; Peters et al., 2019;\n∗Equal contribution\n†Corresponding authors\nFigure 1: Illustration of plug-and-play knowledge in-\njection, where knowledge bases and models are decou-\npled.\nBosselut et al., 2019; Guan et al., 2020). Knowl-\nedge injection improves the performance of PLMs\non a wide range of tasks such as information extrac-\ntion (Liu et al., 2020a; Wang et al., 2021b), ques-\ntion answering (Xiong et al., 2020; Wang et al.,\n2021a), and text generation (Chen et al., 2020).\nExisting injection methods commonly inject\nknowledge by knowledge-aware pre-training or\nﬁne-tuning (Peters et al., 2019; Yamada et al., 2020;\nLiu et al., 2020a; Wang et al., 2021a). However,\nrarely studied is how to inject knowledge into a\ndownstream model that is already adapted to a spe-\nciﬁc task. If we want to apply a new knowledge\ninjection method to enhance models on a speciﬁc\ntask, we have to discard task-speciﬁc downstream\nmodels and retrain them. In addition, one down-\nstream model working with multiple knowledge\nbases requires retraining itself to inject each knowl-\nedge base. Retraining models is time-consuming\nand resource-intensive, leading to the need for a\nﬂexible and efﬁcient injection paradigm.\nToward ﬂexible and efﬁcient injection, we ex-\nplore a novel paradigm plug-and-play knowledge\ninjection, where knowledge bases are injected into\nfrozen existing downstream models by knowledge\nmodules. The knowledge module bridges the\nknowledge base and the model, and we call it\na plugin vividly. Under this paradigm, a down-\nstream model would have multiple plugins, each\ncorresponding to a combination of an injection\nmethod and a knowledge base, which ensures ﬂex-\nibility. Moreover, knowledge plugins should be\nsmall enough to ensure efﬁciency. Intuitively, as\nshown in Figure 1, we treat models and knowledge\nbases as computers and ﬂash disks, respectively.\nIn this work, we study two settings for the plug-\nand-play knowledge injection paradigm. The ﬁrst\nis general plug-and-play knowledge injection, aim-\ning to inject knowledge into all downstream models\n(trained from a particular PLM) by a general plugin\nwithout any task-speciﬁc training. In this setting,\nall downstream models share exactly one plugin\nfor one combination of an injection method and a\nknowledge base. The second is task-speciﬁc plug-\nand-play knowledge injection, where knowledge\nplugins are trained to better adapt to downstream\ntasks while keeping downstream models frozen.\nBy our pilot study, we ﬁnd that existing meth-\nods (Poerner et al., 2020; Ye et al., 2022; Wang\net al., 2021a; Lewis et al., 2020) that can be used\ndirectly can not be well applied to the plug-and-\nplay injection paradigm. To this end, we propose\nmap-tuning, a preliminary exploration of learn-\ning knowledge plugins. Speciﬁcally, we train a\nlightweight mapping network that augments model\ninputs with mapped knowledge representations,\ne.g., TransE (Bordes et al., 2013). To meet the\ngeneral and task-speciﬁc injection requirements,\nwe design general map-tuning and task-speciﬁc\nmap-tuning, respectively. General map-tuning\nadopts language modeling as its objective to learn\nknowledge plugins and seeks better generalizabil-\nity. Task-speciﬁc map-tuning adopts task targets\nfor plugin learning and seeks better task adaptation.\nWe use three typical knowledge-driven NLP\ntasks to evaluate our plug-and-play knowledge in-\njection, including relation classiﬁcation (Han et al.,\n2018), entity typing (Xin et al., 2018), and question\nanswering (Sciavolino et al., 2021). The experi-\nmental results show that: (1) after adapting PLMs\nto downstream tasks through full-parameter ﬁne-\ntuning or parameter-efﬁcient tuning, also known as\ndelta tuning (Liu et al., 2021; Ding et al., 2022),\ninjecting knowledge into these downstream mod-\nels by general map-tuning leads to performance\nimprovements in almost all cases; (2) using task-\nspeciﬁc map-tuning to inject domain knowledge\nfurther enables a frozen downstream model to work\nwell in different domains. We hope our contribu-\ntion can draw more attention to the plug-and-play\nknowledge injection paradigm and inspire more\nfuture research.\n2 Plug-and-Play Knowledge Injection\nParadigm Description. Given a downstream\nmodel Dtrained on a downstream task with a PLM\nPas the backbone, we intend to improve its per-\nformance on this task by incorporating an extra\nknowledge base Band freezing D’s parameters,\nfor which we need to train a knowledge plugin M.\nNote that neither pre-training nor ﬁne-tuning trains\nthe model Dto cooperate with Bor M.\nTwo Injection Settings. As shown in Fig-\nure 2(a), plug-and-play knowledge injection de-\ncouples knowledge injection from model training,\nwhich is different from existing paradigms. For\ngeneral plug-and-play knowledge injection, M\nis obtained based on only Pand B, and then it\nis directly plugged into all downstream models,\nD1, D2, . . ., without any additional training. For\ntask-speciﬁc plug-and-play knowledge injection,\nit is allowed to train M1, M2, . . .for D1, D2, . . .\nrespectively while keeping D1, D2, . . .frozen.\nChallenges. The general plug-and-play knowl-\nedge injection poses serious challenges to methods\ndesigned for it. Mis expected to improve the per-\nformance of D, yet Dhas never seen Mor been\nseen by Mduring training. The only prior condi-\ntion is that Pand Bare visible during training M.\nTherefore, the designed methods for general injec-\ntion need to endow Mwith enough generalizabil-\nity such that Mcan adapt to unknown D1, D2, . . .\nwell. Even though the knowledge base Bmay have\nrich knowledge, without a good adaptation of M,\nuseful information brought to D will be less than\ndisruptive noise.\nThe task-speciﬁc plug-and-play knowledge in-\njection relaxes the restrictions, where Mi is al-\nlowed to be trained with frozen Di. Compared to\ninjection during ﬁne-tuning, the training of Mi\nshould be fast and the parameter number of Mi\nshould be small compared to that of Di. Otherwise,\nthe methods would be meaningless. Hence, it re-\nquires simple and efﬁcient architecture designs and\ninformative training objectives for Mi.\nPotentiality of Using Existing Methods. Few\nexisting knowledge injection methods can be di-\n[CLS]\n<latexit sha1_base64=\"XKJizic/5i5YT3Zj/VcmXmFrzf0=\">AAACHnicbVDLSsNAFJ3UV31XXboJFsFVSUTRZbEbFy58VYUmlMn0th06MwkzN2oJ+Q239mvciVv9GMFJ7UKtBwYO59zLuXOiRHCDnvfhlGZm5+YXyotLyyura+uVjc0bE6eaQZPFItZ3ETUguIImchRwl2igMhJwGw0ahX97D9rwWF3jMIFQ0p7iXc4oWikIEB4xazXOrsK8Xal6NW8Md5r4E1IlE5y3K59BJ2apBIVMUGNavpdgmFGNnAnIl4LUQELZgPagZamiEkyYjW/O3V2rdNxurO1T6I7VnxsZlcYMZWQnJcW++esV4n9eK8XucZhxlaQIin0HdVPhYuwWBbgdroGhGFpCmeb2Vpf1qaYMbU2/UiJp/6DggcVSUtXJgss8C4rAKMou86Iv/2870+Rmv+Yf1A4vDqr1k0lzZbJNdsge8ckRqZNTck6ahJGEPJFnMnJGzovz6rx9j5acyc4W+QXn/QsC/aQs</latexit>\nlove\n<latexit sha1_base64=\"fqWQc6zGpkyDFBQqNdaB73Szm1Y=\">AAACHXicbVDLSgNBEJyNrxhfUY9eFoPgKexKRI+iF48ajArZILOTTjJkHstMrxqW/Qyv+jXexKv4MYKzMQdjbGgoqrqp7ooTwS0GwadXmptfWFwqL1dWVtfWN6qbW9dWp4ZBi2mhzW1MLQiuoIUcBdwmBqiMBdzEw7NCv7kHY7lWVzhKoCNpX/EeZxQd1Y4QHjET+h7yu2otqAfj8mdBOAE1MqmLu+pX1NUslaCQCWptOwwS7GTUIGcC8kqUWkgoG9I+tB1UVILtZOOTc3/PMV2/p41rhf6Y/b2RUWntSMZuUlIc2L9aQf6ntVPsHXcyrpIUQbEfo14qfNR+8b/f5QYYipEDlBnubvXZgBrK0KU05RJL94OCB6alpKqbRc08iwrDOM6aeZFX+DedWXB9UA8b9cPLRu3kdJJcmeyQXbJPQnJETsg5uSAtwogmT+SZvHgv3qv35r3/jJa8yc42mSrv4xsJ9qQ+</latexit>\npeanut\n<latexit sha1_base64=\"HdjqFVT8unCJTgRGjM0IA4G+xaA=\">AAACH3icbVDLSsNAFJ34tr6qLt0Ei+CqJKLoUnTjUsU+oAllMr1th85M4syNWkK+w61+jTtx68cITtoubOuBgcM593LunCgR3KDnfTsLi0vLK6tr66WNza3tnfLuXt3EqWZQY7GIdTOiBgRXUEOOApqJBiojAY1ocF34jSfQhsfqAYcJhJL2FO9yRtFKYYDwglkCVKWYt8sVr+qN4M4Tf0IqZILbdvkn6MQslaCQCWpMy/cSDDOqkTMBeSlIDSSUDWgPWpYqKsGE2ejo3D2ySsftxto+he5I/buRUWnMUEZ2UlLsm1mvEP/zWil2L8KMqyRFUGwc1E2Fi7FbNOB2uAaGYmgJZZrbW13Wp5oytD1NpUTS/kHBM4ulpKqTBfd5FhSBUZTd50Vf/mw786R+UvVPq2d3p5XLq0lza+SAHJJj4pNzckluyC2pEUYeySt5I+/Ou/PhfDpf49EFZ7KzT6bgfP8Cx/WlKQ==</latexit>\nbutter\n<latexit sha1_base64=\"YTurMLA4tl/mqRTdkFPnehqG8wU=\">AAACH3icbVBNSwMxFMz6WetX1aOXxSJ4Krui6FH04lGLrUJ3kSR91WCSXZO3aln2d3i1v8abeO2PEcy2Pah1IDDMvMe8DEulsBgEQ29mdm5+YbGyVF1eWV1br21stm2SGQ4tnsjE3DBqQQoNLRQo4SY1QBWTcM0ezkr/+gmMFYm+wn4KsaJ3WvQEp+ikOEJ4wZxliGCK21o9aAQj+NMknJA6meDitvYVdROeKdDIJbW2EwYpxjk1KLiEohplFlLKH+gddBzVVIGN89HRhb/rlK7fS4x7Gv2R+nMjp8ravmJuUlG8t3+9UvzP62TYO45zodMMQfNxUC+TPiZ+2YDfFQY4yr4jlBvhbvX5PTWUuw5+pzDl/qDhmSdKUd3No2aRR2UgY3mzKPsK/7YzTdr7jfCgcXh5UD85nTRXIdtkh+yRkByRE3JOLkiLcPJIXskbGXgD79378D7HozPeZGeL/II3/AbXVKUy</latexit>\n.\n<latexit sha1_base64=\"oR40QGMmh45lOMswonCXgLASu2o=\">AAACHHicbVBNS8NAEN3Ur1q/qh69BIvgqSRS0WPRi8da7Ac2pWy203bp7ibsbtQS8i+82l/jTbwK/hjBTZuDbR0YeLw3j5l5fsio0o7zbeXW1jc2t/LbhZ3dvf2D4uFRUwWRJNAgAQtk28cKGBXQ0FQzaIcSMPcZtPzxbaq3nkAqGogHPQmhy/FQ0AElWBvq0dPwYlxxOekVS07ZmZW9CtwMlFBWtV7xx+sHJOIgNGFYqY7rhLobY6kpYZAUvEhBiMkYD6FjoMAcVDeeXZzYZ4bp24NAmhbanrF/HTHmSk24byY51iO1rKXkf1on0oPrbkxFGGkQZL5oEDFbB3b6vt2nEohmEwMwkdTcapMRlphoE9LCFp+bHwQ8k4BzLPqxV09iL13o+3E9SfNyl9NZBc2LslspX95XStWbLLk8OkGn6By56ApV0R2qoQYiSKBX9Iam1tR6tz6sz/lozso8x2ihrK9fwCCjiQ==</latexit>\n[SEP]\n<latexit sha1_base64=\"z/vtTQ1yS8oS2ADAkIL9fIaFBFg=\">AAACHnicbVBNS8NAFNz4bf2qevQSLIKnkkhFj0URPNZqVWhC2WxfdenuJuy+qCXkb3jVX+NNvOqPEdy0PdjqwMIw8x7zdqJEcIOe9+XMzM7NLywuLZdWVtfWN8qbW9cmTjWDFotFrG8jakBwBS3kKOA20UBlJOAm6p8W/s0DaMNjdYWDBEJJ7xTvcUbRSkGA8IRZ+/KsEeadcsWrekO4f4k/JhUyRqNT/g66MUslKGSCGtP2vQTDjGrkTEBeClIDCWV9egdtSxWVYMJseHPu7lml6/ZibZ9Cd6j+3sioNGYgIzspKd6baa8Q//PaKfaOw4yrJEVQbBTUS4WLsVsU4Ha5BoZiYAllmttbXXZPNWVoa5pIiaT9g4JHFktJVTcLmnkWFIFRlDXzoi9/up2/5Pqg6teqhxe1Sv1k3NwS2SG7ZJ/45IjUyTlpkBZhJCHP5IW8Oq/Om/PufIxGZ5zxzjaZgPP5Aw08pDI=</latexit>\n/\n<latexit sha1_base64=\"muys/2ZQJ40hek8CS3JlhPgiw3k=\">AAACGnicbVBNS8NAEN3U7/pV9eglWARPNRFFj6IXjyq2Fpoim+20Xbq7CbsTtYT8Ca/6a7yJVy/+GMFNm4NtHRh4vDfDm3lhLLhBz/t2SnPzC4tLyyvl1bX1jc3K1nbDRIlmUGeRiHQzpAYEV1BHjgKasQYqQwH34eAy1+8fQRseqTscxtCWtKd4lzOKlmoGCM+YHmYPlapX80blzgK/AFVS1PVD5SfoRCyRoJAJakzL92Jsp1QjZwKycpAYiCkb0B60LFRUgmmno3szd98yHbcbadsK3RH7dyOl0pihDO2kpNg301pO/qe1EuyetVOu4gRBsbFRNxEuRm7+vNvhGhiKoQWUaW5vdVmfasrQRjThEkr7g4InFklJVScNbrM0yA3DML3N8rz86XRmQeOo5h/XTm6Oq+cXRXLLZJfskQPik1NyTq7INakTRgR5Ia/kzXlz3p0P53M8WnKKnR0yUc7XL/whopk=</latexit>\n[MASK]\n<latexit sha1_base64=\"Dd6HywciKItWZcpdkVWYL5tDQCw=\">AAACH3icbVDLSsNAFJ3U97vq0k2wCK5KIoouq24EEeqjKjRBJtPbOnRmEmdu1BLyHW71a9yJ236M4KR2YasHBg7n3Mu5c6JEcIOe13dKE5NT0zOzc/MLi0vLK+XVtWsTp5pBg8Ui1rcRNSC4ggZyFHCbaKAyEnATdY8L/+YRtOGxusJeAqGkHcXbnFG0UhggPGPWPDu8PA3zu3LFq3oDuH+JPyQVMkT9rvwVtGKWSlDIBDWm6XsJhhnVyJmAfD5IDSSUdWkHmpYqKsGE2eDo3N2ySsttx9o+he5A/b2RUWlMT0Z2UlK8N+NeIf7nNVNsH4QZV0mKoNhPUDsVLsZu0YDb4hoYip4llGlub3XZPdWUoe1pJCWS9g8KnlgsJVWtLLjIs6AIjKLsIi/68sfb+Uuud6r+bnXvfLdSOxo2N0s2yCbZJj7ZJzVyQuqkQRh5IC/klbw5b8678+F8/oyWnOHOOhmB0/8GqS2kgA==</latexit>\n/\n<latexit sha1_base64=\"muys/2ZQJ40hek8CS3JlhPgiw3k=\">AAACGnicbVBNS8NAEN3U7/pV9eglWARPNRFFj6IXjyq2Fpoim+20Xbq7CbsTtYT8Ca/6a7yJVy/+GMFNm4NtHRh4vDfDm3lhLLhBz/t2SnPzC4tLyyvl1bX1jc3K1nbDRIlmUGeRiHQzpAYEV1BHjgKasQYqQwH34eAy1+8fQRseqTscxtCWtKd4lzOKlmoGCM+YHmYPlapX80blzgK/AFVS1PVD5SfoRCyRoJAJakzL92Jsp1QjZwKycpAYiCkb0B60LFRUgmmno3szd98yHbcbadsK3RH7dyOl0pihDO2kpNg301pO/qe1EuyetVOu4gRBsbFRNxEuRm7+vNvhGhiKoQWUaW5vdVmfasrQRjThEkr7g4InFklJVScNbrM0yA3DML3N8rz86XRmQeOo5h/XTm6Oq+cXRXLLZJfskQPik1NyTq7INakTRgR5Ia/kzXlz3p0P53M8WnKKnR0yUc7XL/whopk=</latexit>\nMap\n<latexit sha1_base64=\"gl8lzF6VILwNkPD/JDrZ88PO1Y4=\">AAACHHicbVBNS8NAEN34bf2qevQSLIKnkoiiR9GLF6EWW4tNKZvttF3c3YTdiVpC/oVX+2u8iVfBHyO4aXuw1YGBx3szvJkXxoIb9LwvZ25+YXFpeWW1sLa+sblV3N6pmyjRDGosEpFuhNSA4ApqyFFAI9ZAZSjgLny4zPW7R9CGR+oWBzG0JO0p3uWMoqXuA4RnTK9pnLWLJa/sjcr9C/wJKJFJVdrF76ATsUSCQiaoMU3fi7GVUo2cCcgKQWIgpuyB9qBpoaISTCsdXZy5B5bpuN1I21bojtjfGymVxgxkaCclxb6Z1XLyP62ZYPeslXIVJwiKjY26iXAxcvP33Q7XwFAMLKBMc3ury/pUU4Y2pCmXUNofFDyxSEqqOmlQzdIgNwzDtJrlefmz6fwF9aOyf1w+uTkunV9Mklshe2SfHBKfnJJzckUqpEYYUeSFvJKhM3TenHfnYzw650x2dslUOZ8/3/6jnA==</latexit>\nMap\n<latexit sha1_base64=\"gl8lzF6VILwNkPD/JDrZ88PO1Y4=\">AAACHHicbVBNS8NAEN34bf2qevQSLIKnkoiiR9GLF6EWW4tNKZvttF3c3YTdiVpC/oVX+2u8iVfBHyO4aXuw1YGBx3szvJkXxoIb9LwvZ25+YXFpeWW1sLa+sblV3N6pmyjRDGosEpFuhNSA4ApqyFFAI9ZAZSjgLny4zPW7R9CGR+oWBzG0JO0p3uWMoqXuA4RnTK9pnLWLJa/sjcr9C/wJKJFJVdrF76ATsUSCQiaoMU3fi7GVUo2cCcgKQWIgpuyB9qBpoaISTCsdXZy5B5bpuN1I21bojtjfGymVxgxkaCclxb6Z1XLyP62ZYPeslXIVJwiKjY26iXAxcvP33Q7XwFAMLKBMc3ury/pUU4Y2pCmXUNofFDyxSEqqOmlQzdIgNwzDtJrlefmz6fwF9aOyf1w+uTkunV9Mklshe2SfHBKfnJJzckUqpEYYUeSFvJKhM3TenHfnYzw650x2dslUOZ8/3/6jnA==</latexit>\nAdditional Input\n<latexit sha1_base64=\"onu66ZApQHer6gmA3Do/knHnV+c=\">AAACK3icbVDLSgNBEJz1bXxFPXjwMhgET2FXFD36uOhNxRghG8LsbEcHZ2aXmV41LPs1XvVrPCle/QzB2SQHozY0FFXdFFVRKoVF33/zxsYnJqemZ2Yrc/MLi0vV5ZUrm2SGQ4MnMjHXEbMghYYGCpRwnRpgKpLQjO6OS715D8aKRF9iL4W2YjdadAVn6KhOdS1EeMT8MI5FSTBJT3WaYdGp1vy63x/6FwRDUCPDOetUv8I44ZkCjVwya1uBn2I7ZwYFl1BUwsxCyvgdu4GWg5opsO28H6Cgm46JaTcxbjXSPvvzI2fK2p6K3KVieGt/ayX5n9bKsLvfzkUZCTQfGHUzSTGhZRs0FgY4yp4DjBtXAaf8lhnG0XU24hIpl0HDA0+UYjrOw4siD0vDKMovirKv4Hc7f8HVdj3Yqe+e79QOjobNzZB1skG2SED2yAE5IWekQTgpyBN5Ji/ei/fqvXsfg9Mxb/izSkbG+/wGEqSpWg==</latexit>\nAdditional Input\n<latexit sha1_base64=\"onu66ZApQHer6gmA3Do/knHnV+c=\">AAACK3icbVDLSgNBEJz1bXxFPXjwMhgET2FXFD36uOhNxRghG8LsbEcHZ2aXmV41LPs1XvVrPCle/QzB2SQHozY0FFXdFFVRKoVF33/zxsYnJqemZ2Yrc/MLi0vV5ZUrm2SGQ4MnMjHXEbMghYYGCpRwnRpgKpLQjO6OS715D8aKRF9iL4W2YjdadAVn6KhOdS1EeMT8MI5FSTBJT3WaYdGp1vy63x/6FwRDUCPDOetUv8I44ZkCjVwya1uBn2I7ZwYFl1BUwsxCyvgdu4GWg5opsO28H6Cgm46JaTcxbjXSPvvzI2fK2p6K3KVieGt/ayX5n9bKsLvfzkUZCTQfGHUzSTGhZRs0FgY4yp4DjBtXAaf8lhnG0XU24hIpl0HDA0+UYjrOw4siD0vDKMovirKv4Hc7f8HVdj3Yqe+e79QOjobNzZB1skG2SED2yAE5IWekQTgpyBN5Ji/ei/fqvXsfg9Mxb/izSkbG+/wGEqSpWg==</latexit>\n[CLS]\n<latexit sha1_base64=\"XKJizic/5i5YT3Zj/VcmXmFrzf0=\">AAACHnicbVDLSsNAFJ3UV31XXboJFsFVSUTRZbEbFy58VYUmlMn0th06MwkzN2oJ+Q239mvciVv9GMFJ7UKtBwYO59zLuXOiRHCDnvfhlGZm5+YXyotLyyura+uVjc0bE6eaQZPFItZ3ETUguIImchRwl2igMhJwGw0ahX97D9rwWF3jMIFQ0p7iXc4oWikIEB4xazXOrsK8Xal6NW8Md5r4E1IlE5y3K59BJ2apBIVMUGNavpdgmFGNnAnIl4LUQELZgPagZamiEkyYjW/O3V2rdNxurO1T6I7VnxsZlcYMZWQnJcW++esV4n9eK8XucZhxlaQIin0HdVPhYuwWBbgdroGhGFpCmeb2Vpf1qaYMbU2/UiJp/6DggcVSUtXJgss8C4rAKMou86Iv/2870+Rmv+Yf1A4vDqr1k0lzZbJNdsge8ckRqZNTck6ahJGEPJFnMnJGzovz6rx9j5acyc4W+QXn/QsC/aQs</latexit>\nlove\n<latexit sha1_base64=\"fqWQc6zGpkyDFBQqNdaB73Szm1Y=\">AAACHXicbVDLSgNBEJyNrxhfUY9eFoPgKexKRI+iF48ajArZILOTTjJkHstMrxqW/Qyv+jXexKv4MYKzMQdjbGgoqrqp7ooTwS0GwadXmptfWFwqL1dWVtfWN6qbW9dWp4ZBi2mhzW1MLQiuoIUcBdwmBqiMBdzEw7NCv7kHY7lWVzhKoCNpX/EeZxQd1Y4QHjET+h7yu2otqAfj8mdBOAE1MqmLu+pX1NUslaCQCWptOwwS7GTUIGcC8kqUWkgoG9I+tB1UVILtZOOTc3/PMV2/p41rhf6Y/b2RUWntSMZuUlIc2L9aQf6ntVPsHXcyrpIUQbEfo14qfNR+8b/f5QYYipEDlBnubvXZgBrK0KU05RJL94OCB6alpKqbRc08iwrDOM6aeZFX+DedWXB9UA8b9cPLRu3kdJJcmeyQXbJPQnJETsg5uSAtwogmT+SZvHgv3qv35r3/jJa8yc42mSrv4xsJ9qQ+</latexit>\npeanut\n<latexit sha1_base64=\"HdjqFVT8unCJTgRGjM0IA4G+xaA=\">AAACH3icbVDLSsNAFJ34tr6qLt0Ei+CqJKLoUnTjUsU+oAllMr1th85M4syNWkK+w61+jTtx68cITtoubOuBgcM593LunCgR3KDnfTsLi0vLK6tr66WNza3tnfLuXt3EqWZQY7GIdTOiBgRXUEOOApqJBiojAY1ocF34jSfQhsfqAYcJhJL2FO9yRtFKYYDwglkCVKWYt8sVr+qN4M4Tf0IqZILbdvkn6MQslaCQCWpMy/cSDDOqkTMBeSlIDSSUDWgPWpYqKsGE2ejo3D2ySsftxto+he5I/buRUWnMUEZ2UlLsm1mvEP/zWil2L8KMqyRFUGwc1E2Fi7FbNOB2uAaGYmgJZZrbW13Wp5oytD1NpUTS/kHBM4ulpKqTBfd5FhSBUZTd50Vf/mw786R+UvVPq2d3p5XLq0lza+SAHJJj4pNzckluyC2pEUYeySt5I+/Ou/PhfDpf49EFZ7KzT6bgfP8Cx/WlKQ==</latexit>\nbutter\n<latexit sha1_base64=\"YTurMLA4tl/mqRTdkFPnehqG8wU=\">AAACH3icbVBNSwMxFMz6WetX1aOXxSJ4Krui6FH04lGLrUJ3kSR91WCSXZO3aln2d3i1v8abeO2PEcy2Pah1IDDMvMe8DEulsBgEQ29mdm5+YbGyVF1eWV1br21stm2SGQ4tnsjE3DBqQQoNLRQo4SY1QBWTcM0ezkr/+gmMFYm+wn4KsaJ3WvQEp+ikOEJ4wZxliGCK21o9aAQj+NMknJA6meDitvYVdROeKdDIJbW2EwYpxjk1KLiEohplFlLKH+gddBzVVIGN89HRhb/rlK7fS4x7Gv2R+nMjp8ravmJuUlG8t3+9UvzP62TYO45zodMMQfNxUC+TPiZ+2YDfFQY4yr4jlBvhbvX5PTWUuw5+pzDl/qDhmSdKUd3No2aRR2UgY3mzKPsK/7YzTdr7jfCgcXh5UD85nTRXIdtkh+yRkByRE3JOLkiLcPJIXskbGXgD79378D7HozPeZGeL/II3/AbXVKUy</latexit>\n.\n<latexit sha1_base64=\"oR40QGMmh45lOMswonCXgLASu2o=\">AAACHHicbVBNS8NAEN3Ur1q/qh69BIvgqSRS0WPRi8da7Ac2pWy203bp7ibsbtQS8i+82l/jTbwK/hjBTZuDbR0YeLw3j5l5fsio0o7zbeXW1jc2t/LbhZ3dvf2D4uFRUwWRJNAgAQtk28cKGBXQ0FQzaIcSMPcZtPzxbaq3nkAqGogHPQmhy/FQ0AElWBvq0dPwYlxxOekVS07ZmZW9CtwMlFBWtV7xx+sHJOIgNGFYqY7rhLobY6kpYZAUvEhBiMkYD6FjoMAcVDeeXZzYZ4bp24NAmhbanrF/HTHmSk24byY51iO1rKXkf1on0oPrbkxFGGkQZL5oEDFbB3b6vt2nEohmEwMwkdTcapMRlphoE9LCFp+bHwQ8k4BzLPqxV09iL13o+3E9SfNyl9NZBc2LslspX95XStWbLLk8OkGn6By56ApV0R2qoQYiSKBX9Iam1tR6tz6sz/lozso8x2ihrK9fwCCjiQ==</latexit>\n[SEP]\n<latexit sha1_base64=\"z/vtTQ1yS8oS2ADAkIL9fIaFBFg=\">AAACHnicbVBNS8NAFNz4bf2qevQSLIKnkkhFj0URPNZqVWhC2WxfdenuJuy+qCXkb3jVX+NNvOqPEdy0PdjqwMIw8x7zdqJEcIOe9+XMzM7NLywuLZdWVtfWN8qbW9cmTjWDFotFrG8jakBwBS3kKOA20UBlJOAm6p8W/s0DaMNjdYWDBEJJ7xTvcUbRSkGA8IRZ+/KsEeadcsWrekO4f4k/JhUyRqNT/g66MUslKGSCGtP2vQTDjGrkTEBeClIDCWV9egdtSxWVYMJseHPu7lml6/ZibZ9Cd6j+3sioNGYgIzspKd6baa8Q//PaKfaOw4yrJEVQbBTUS4WLsVsU4Ha5BoZiYAllmttbXXZPNWVoa5pIiaT9g4JHFktJVTcLmnkWFIFRlDXzoi9/up2/5Pqg6teqhxe1Sv1k3NwS2SG7ZJ/45IjUyTlpkBZhJCHP5IW8Oq/Om/PufIxGZ5zxzjaZgPP5Aw08pDI=</latexit>\n/\n<latexit sha1_base64=\"muys/2ZQJ40hek8CS3JlhPgiw3k=\">AAACGnicbVBNS8NAEN3U7/pV9eglWARPNRFFj6IXjyq2Fpoim+20Xbq7CbsTtYT8Ca/6a7yJVy/+GMFNm4NtHRh4vDfDm3lhLLhBz/t2SnPzC4tLyyvl1bX1jc3K1nbDRIlmUGeRiHQzpAYEV1BHjgKasQYqQwH34eAy1+8fQRseqTscxtCWtKd4lzOKlmoGCM+YHmYPlapX80blzgK/AFVS1PVD5SfoRCyRoJAJakzL92Jsp1QjZwKycpAYiCkb0B60LFRUgmmno3szd98yHbcbadsK3RH7dyOl0pihDO2kpNg301pO/qe1EuyetVOu4gRBsbFRNxEuRm7+vNvhGhiKoQWUaW5vdVmfasrQRjThEkr7g4InFklJVScNbrM0yA3DML3N8rz86XRmQeOo5h/XTm6Oq+cXRXLLZJfskQPik1NyTq7INakTRgR5Ia/kzXlz3p0P53M8WnKKnR0yUc7XL/whopk=</latexit>\n/\n<latexit sha1_base64=\"muys/2ZQJ40hek8CS3JlhPgiw3k=\">AAACGnicbVBNS8NAEN3U7/pV9eglWARPNRFFj6IXjyq2Fpoim+20Xbq7CbsTtYT8Ca/6a7yJVy/+GMFNm4NtHRh4vDfDm3lhLLhBz/t2SnPzC4tLyyvl1bX1jc3K1nbDRIlmUGeRiHQzpAYEV1BHjgKasQYqQwH34eAy1+8fQRseqTscxtCWtKd4lzOKlmoGCM+YHmYPlapX80blzgK/AFVS1PVD5SfoRCyRoJAJakzL92Jsp1QjZwKycpAYiCkb0B60LFRUgmmno3szd98yHbcbadsK3RH7dyOl0pihDO2kpNg301pO/qe1EuyetVOu4gRBsbFRNxEuRm7+vNvhGhiKoQWUaW5vdVmfasrQRjThEkr7g4InFklJVScNbrM0yA3DML3N8rz86XRmQeOo5h/XTm6Oq+cXRXLLZJfskQPik1NyTq7INakTRgR5Ia/kzXlz3p0P53M8WnKKnR0yUc7XL/whopk=</latexit>\nMap\n<latexit sha1_base64=\"gl8lzF6VILwNkPD/JDrZ88PO1Y4=\">AAACHHicbVBNS8NAEN34bf2qevQSLIKnkoiiR9GLF6EWW4tNKZvttF3c3YTdiVpC/oVX+2u8iVfBHyO4aXuw1YGBx3szvJkXxoIb9LwvZ25+YXFpeWW1sLa+sblV3N6pmyjRDGosEpFuhNSA4ApqyFFAI9ZAZSjgLny4zPW7R9CGR+oWBzG0JO0p3uWMoqXuA4RnTK9pnLWLJa/sjcr9C/wJKJFJVdrF76ATsUSCQiaoMU3fi7GVUo2cCcgKQWIgpuyB9qBpoaISTCsdXZy5B5bpuN1I21bojtjfGymVxgxkaCclxb6Z1XLyP62ZYPeslXIVJwiKjY26iXAxcvP33Q7XwFAMLKBMc3ury/pUU4Y2pCmXUNofFDyxSEqqOmlQzdIgNwzDtJrlefmz6fwF9aOyf1w+uTkunV9Mklshe2SfHBKfnJJzckUqpEYYUeSFvJKhM3TenHfnYzw650x2dslUOZ8/3/6jnA==</latexit>\nMap\n<latexit sha1_base64=\"gl8lzF6VILwNkPD/JDrZ88PO1Y4=\">AAACHHicbVBNS8NAEN34bf2qevQSLIKnkoiiR9GLF6EWW4tNKZvttF3c3YTdiVpC/oVX+2u8iVfBHyO4aXuw1YGBx3szvJkXxoIb9LwvZ25+YXFpeWW1sLa+sblV3N6pmyjRDGosEpFuhNSA4ApqyFFAI9ZAZSjgLny4zPW7R9CGR+oWBzG0JO0p3uWMoqXuA4RnTK9pnLWLJa/sjcr9C/wJKJFJVdrF76ATsUSCQiaoMU3fi7GVUo2cCcgKQWIgpuyB9qBpoaISTCsdXZy5B5bpuN1I21bojtjfGymVxgxkaCclxb6Z1XLyP62ZYPeslXIVJwiKjY26iXAxcvP33Q7XwFAMLKBMc3ury/pUU4Y2pCmXUNofFDyxSEqqOmlQzdIgNwzDtJrlefmz6fwF9aOyf1w+uTkunV9Mklshe2SfHBKfnJJzckUqpEYYUeSFvJKhM3TenHfnYzw650x2dslUOZ8/3/6jnA==</latexit>\nhamster\n<latexit sha1_base64=\"UqFhaguoowE9uZMWnSSV/GKz9b8=\">AAACJHicbVDLTgIxFO3gC/E16tLNRGLiiswYjC6JblwikUfCENIpF2hoO5O2g5LJ/Ilb+Rp3xoUb/8TEDrAQ8CRNTs69J+f2BBGjSrvul5Xb2Nza3snvFvb2Dw6P7OOThgpjSaBOQhbKVoAVMCqgrqlm0IokYB4waAaj+2zeHINUNBRPehJBh+OBoH1KsDZS17Z9DS/GlwwxVxpk2rWLbsmdwVkn3oIU0QLVrv3j90IScxCaMKxU23Mj3Umw1JQwSAt+rCDCZIQH0DZUYA6qk8wuT50Lo/ScfijNE9qZqX8diblKTXhgNjnWQ7U6y8T/Zu1Y9287CRVRrEGQeVA/Zo4OnawGp0clEM0mhmAiqbnVIUMsMTEdLKcE3PxBwDMJOceil/i1NPGzwCBIamnWl7fazjppXJW8cun6sVys3C2ay6MzdI4ukYduUAU9oCqqI4LG6BW9oak1td6tD+tzvpqzFp5TtATr+xfoDKa8</latexit>\nhamster\n<latexit sha1_base64=\"UqFhaguoowE9uZMWnSSV/GKz9b8=\">AAACJHicbVDLTgIxFO3gC/E16tLNRGLiiswYjC6JblwikUfCENIpF2hoO5O2g5LJ/Ilb+Rp3xoUb/8TEDrAQ8CRNTs69J+f2BBGjSrvul5Xb2Nza3snvFvb2Dw6P7OOThgpjSaBOQhbKVoAVMCqgrqlm0IokYB4waAaj+2zeHINUNBRPehJBh+OBoH1KsDZS17Z9DS/GlwwxVxpk2rWLbsmdwVkn3oIU0QLVrv3j90IScxCaMKxU23Mj3Umw1JQwSAt+rCDCZIQH0DZUYA6qk8wuT50Lo/ScfijNE9qZqX8diblKTXhgNjnWQ7U6y8T/Zu1Y9287CRVRrEGQeVA/Zo4OnawGp0clEM0mhmAiqbnVIUMsMTEdLKcE3PxBwDMJOceil/i1NPGzwCBIamnWl7fazjppXJW8cun6sVys3C2ay6MzdI4ukYduUAU9oCqqI4LG6BW9oak1td6tD+tzvpqzFp5TtATr+xfoDKa8</latexit>\nhamster\n<latexit sha1_base64=\"UqFhaguoowE9uZMWnSSV/GKz9b8=\">AAACJHicbVDLTgIxFO3gC/E16tLNRGLiiswYjC6JblwikUfCENIpF2hoO5O2g5LJ/Ilb+Rp3xoUb/8TEDrAQ8CRNTs69J+f2BBGjSrvul5Xb2Nza3snvFvb2Dw6P7OOThgpjSaBOQhbKVoAVMCqgrqlm0IokYB4waAaj+2zeHINUNBRPehJBh+OBoH1KsDZS17Z9DS/GlwwxVxpk2rWLbsmdwVkn3oIU0QLVrv3j90IScxCaMKxU23Mj3Umw1JQwSAt+rCDCZIQH0DZUYA6qk8wuT50Lo/ScfijNE9qZqX8diblKTXhgNjnWQ7U6y8T/Zu1Y9287CRVRrEGQeVA/Zo4OnawGp0clEM0mhmAiqbnVIUMsMTEdLKcE3PxBwDMJOceil/i1NPGzwCBIamnWl7fazjppXJW8cun6sVys3C2ay6MzdI4ukYduUAU9oCqqI4LG6BW9oak1td6tD+tzvpqzFp5TtATr+xfoDKa8</latexit>\nhamster\n<latexit sha1_base64=\"UqFhaguoowE9uZMWnSSV/GKz9b8=\">AAACJHicbVDLTgIxFO3gC/E16tLNRGLiiswYjC6JblwikUfCENIpF2hoO5O2g5LJ/Ilb+Rp3xoUb/8TEDrAQ8CRNTs69J+f2BBGjSrvul5Xb2Nza3snvFvb2Dw6P7OOThgpjSaBOQhbKVoAVMCqgrqlm0IokYB4waAaj+2zeHINUNBRPehJBh+OBoH1KsDZS17Z9DS/GlwwxVxpk2rWLbsmdwVkn3oIU0QLVrv3j90IScxCaMKxU23Mj3Umw1JQwSAt+rCDCZIQH0DZUYA6qk8wuT50Lo/ScfijNE9qZqX8diblKTXhgNjnWQ7U6y8T/Zu1Y9287CRVRrEGQeVA/Zo4OnawGp0clEM0mhmAiqbnVIUMsMTEdLKcE3PxBwDMJOceil/i1NPGzwCBIamnWl7fazjppXJW8cun6sVys3C2ay6MzdI4ukYduUAU9oCqqI4LG6BW9oak1td6tD+tzvpqzFp5TtATr+xfoDKa8</latexit>\nGeneral Map-tuning\n<latexit sha1_base64=\"LlfxlhCsfSHbHEvVS8P9BmuHzRY=\">AAACL3icbVDLahsxFNUkaZu6LyddZiNiCt3UzJSUdhmSRboppKF+gMeYK/mOIyJpBulOEzPMIl+TbfM1pZuQbT+iUI3jRV4HBIdz7uXoHlFo5SmO/0Qrq2tPnj5bf9568fLV6zftjc2+z0snsSdznbuhAI9aWeyRIo3DwiEYoXEgTvYbf/ATnVe5/UHzAscGZlZlSgIFadLeSgnPSGTVAVp0oPk3KD5QaZWd1ZN2J+7GC/CHJFmSDlvicNL+l05zWRq0JDV4P0rigsYVOFJSY91KS48FyBOY4ShQCwb9uFocUfN3QZnyLHfhWeIL9fZGBcb7uRFh0gAd+/teIz7mjUrKvowrZYuS0MqboKzUnHLeNMKnyqEkPQ8EpFPhr1wegwNJobc7KcKEGyyeytwYsNMqPaqrtAkUojqqm76S++08JP2P3WSn++n7Tmd3b9ncOtti2+w9S9hntsu+skPWY5Kdswv2i11Gl9Hv6Cq6vhldiZY7b9kdRH//AxhZqt8=</latexit>\nTask-speciﬁc Map-tuning\n<latexit sha1_base64=\"uJYNi/HxFYwUqicgsQyV5rxJvWw=\">AAACNXicbVBNTxsxFPTSL75aQjlycRshcSHaRUHtEcGlFyRABJCyUfTsvA1WbO/Kfts2Wu2ZX8MVfksPvVW99hcg1RtyKNCRLI1m3tP4jSi08hTHP6KFFy9fvX6zuLS8svr23Vpr/f25z0snsSdznbtLAR61stgjRRovC4dghMYLMTls/Iuv6LzK7RlNCxwYGFuVKQkUpGHrQ0r4nURWnYGf7PgCZWPyIyh2qLTKjuthqx134hn4c5LMSZvNcTxs3aejXJYGLUkN3veTuKBBBY6U1Fgvp6XHAuQExtgP1IJBP6hmp9R8KygjnuUuPEt8pv67UYHxfmpEmDRAV/6p14j/8/olZZ8HlbJFSWjlQ1BWak45b3rhI+VQkp4GAtKp8Fcur8CBpNDeoxRhwg0Wv8ncGLCjKj2tq7QJFKI6rZu+kqftPCfnu52k29k76bb3D+bNLbJN9pFts4R9YvvsCztmPSbZNbtht+wuuot+Rr+i3w+jC9F8Z4M9QvTnLxWtrWM=</latexit>\nAdditional Input\n<latexit sha1_base64=\"onu66ZApQHer6gmA3Do/knHnV+c=\">AAACK3icbVDLSgNBEJz1bXxFPXjwMhgET2FXFD36uOhNxRghG8LsbEcHZ2aXmV41LPs1XvVrPCle/QzB2SQHozY0FFXdFFVRKoVF33/zxsYnJqemZ2Yrc/MLi0vV5ZUrm2SGQ4MnMjHXEbMghYYGCpRwnRpgKpLQjO6OS715D8aKRF9iL4W2YjdadAVn6KhOdS1EeMT8MI5FSTBJT3WaYdGp1vy63x/6FwRDUCPDOetUv8I44ZkCjVwya1uBn2I7ZwYFl1BUwsxCyvgdu4GWg5opsO28H6Cgm46JaTcxbjXSPvvzI2fK2p6K3KVieGt/ayX5n9bKsLvfzkUZCTQfGHUzSTGhZRs0FgY4yp4DjBtXAaf8lhnG0XU24hIpl0HDA0+UYjrOw4siD0vDKMovirKv4Hc7f8HVdj3Yqe+e79QOjobNzZB1skG2SED2yAE5IWekQTgpyBN5Ji/ei/fqvXsfg9Mxb/izSkbG+/wGEqSpWg==</latexit>\nAdditional Input\n<latexit sha1_base64=\"onu66ZApQHer6gmA3Do/knHnV+c=\">AAACK3icbVDLSgNBEJz1bXxFPXjwMhgET2FXFD36uOhNxRghG8LsbEcHZ2aXmV41LPs1XvVrPCle/QzB2SQHozY0FFXdFFVRKoVF33/zxsYnJqemZ2Yrc/MLi0vV5ZUrm2SGQ4MnMjHXEbMghYYGCpRwnRpgKpLQjO6OS715D8aKRF9iL4W2YjdadAVn6KhOdS1EeMT8MI5FSTBJT3WaYdGp1vy63x/6FwRDUCPDOetUv8I44ZkCjVwya1uBn2I7ZwYFl1BUwsxCyvgdu4GWg5opsO28H6Cgm46JaTcxbjXSPvvzI2fK2p6K3KVieGt/ayX5n9bKsLvfzkUZCTQfGHUzSTGhZRs0FgY4yp4DjBtXAaf8lhnG0XU24hIpl0HDA0+UYjrOw4siD0vDKMovirKv4Hc7f8HVdj3Yqe+e79QOjobNzZB1skG2SED2yAE5IWekQTgpyBN5Ji/ei/fqvXsfg9Mxb/izSkbG+/wGEqSpWg==</latexit>\npeanut butter\n<latexit sha1_base64=\"oAegdQBdBKnynEHkST7p7V2/Osc=\">AAACK3icbVBNSwMxFMz6Wb+rHjx4WSyCp7Irih5FLx5rsbXQLSVJXzWYZJfkrVqW/TVe7a/xpHj1Zwhm2x6sOhAYZt5j8oYlUlgMgjdvZnZufmGxtLS8srq2vlHe3GraODUcGjyWsWkxakEKDQ0UKKGVGKCKSbhh9xeFf/MAxopYX+MggY6it1r0BafopG55J0J4cntZAlSnGHVZiggm75YrQTUYwf9LwgmpkAlq3fJX1It5qkAjl9Tadhgk2MmoQcEl5MtRaiGh/J7eQttRTRXYTjY6IPf3ndLz+7FxT6M/Un9uZFRZO1DMTSqKd/a3V4j/ee0U+6edTOgkRdB8HNRPpY+xX7Th94QBjnLgCOVGuL/6/I4ayl0H0ylMuRs0PPJYKap7WVTPs6gIZCyr50Vf4e92/pLmYTU8qh5fHVXOzifNlcgu2SMHJCQn5IxckhppEE5y8kxeyNAbeq/eu/cxHp3xJjvbZAre5zcN+Kns</latexit>\npeanut butter\n<latexit sha1_base64=\"oAegdQBdBKnynEHkST7p7V2/Osc=\">AAACK3icbVBNSwMxFMz6Wb+rHjx4WSyCp7Irih5FLx5rsbXQLSVJXzWYZJfkrVqW/TVe7a/xpHj1Zwhm2x6sOhAYZt5j8oYlUlgMgjdvZnZufmGxtLS8srq2vlHe3GraODUcGjyWsWkxakEKDQ0UKKGVGKCKSbhh9xeFf/MAxopYX+MggY6it1r0BafopG55J0J4cntZAlSnGHVZiggm75YrQTUYwf9LwgmpkAlq3fJX1It5qkAjl9Tadhgk2MmoQcEl5MtRaiGh/J7eQttRTRXYTjY6IPf3ndLz+7FxT6M/Un9uZFRZO1DMTSqKd/a3V4j/ee0U+6edTOgkRdB8HNRPpY+xX7Th94QBjnLgCOVGuL/6/I4ayl0H0ylMuRs0PPJYKap7WVTPs6gIZCyr50Vf4e92/pLmYTU8qh5fHVXOzifNlcgu2SMHJCQn5IxckhppEE5y8kxeyNAbeq/eu/cxHp3xJjvbZAre5zcN+Kns</latexit>\npeanut butter\n<latexit sha1_base64=\"oAegdQBdBKnynEHkST7p7V2/Osc=\">AAACK3icbVBNSwMxFMz6Wb+rHjx4WSyCp7Irih5FLx5rsbXQLSVJXzWYZJfkrVqW/TVe7a/xpHj1Zwhm2x6sOhAYZt5j8oYlUlgMgjdvZnZufmGxtLS8srq2vlHe3GraODUcGjyWsWkxakEKDQ0UKKGVGKCKSbhh9xeFf/MAxopYX+MggY6it1r0BafopG55J0J4cntZAlSnGHVZiggm75YrQTUYwf9LwgmpkAlq3fJX1It5qkAjl9Tadhgk2MmoQcEl5MtRaiGh/J7eQttRTRXYTjY6IPf3ndLz+7FxT6M/Un9uZFRZO1DMTSqKd/a3V4j/ee0U+6edTOgkRdB8HNRPpY+xX7Th94QBjnLgCOVGuL/6/I4ayl0H0ylMuRs0PPJYKap7WVTPs6gIZCyr50Vf4e92/pLmYTU8qh5fHVXOzifNlcgu2SMHJCQn5IxckhppEE5y8kxeyNAbeq/eu/cxHp3xJjvbZAre5zcN+Kns</latexit>\npeanut butter\n<latexit sha1_base64=\"oAegdQBdBKnynEHkST7p7V2/Osc=\">AAACK3icbVBNSwMxFMz6Wb+rHjx4WSyCp7Irih5FLx5rsbXQLSVJXzWYZJfkrVqW/TVe7a/xpHj1Zwhm2x6sOhAYZt5j8oYlUlgMgjdvZnZufmGxtLS8srq2vlHe3GraODUcGjyWsWkxakEKDQ0UKKGVGKCKSbhh9xeFf/MAxopYX+MggY6it1r0BafopG55J0J4cntZAlSnGHVZiggm75YrQTUYwf9LwgmpkAlq3fJX1It5qkAjl9Tadhgk2MmoQcEl5MtRaiGh/J7eQttRTRXYTjY6IPf3ndLz+7FxT6M/Un9uZFRZO1DMTSqKd/a3V4j/ee0U+6edTOgkRdB8HNRPpY+xX7Th94QBjnLgCOVGuL/6/I4ayl0H0ylMuRs0PPJYKap7WVTPs6gIZCyr50Vf4e92/pLmYTU8qh5fHVXOzifNlcgu2SMHJCQn5IxckhppEE5y8kxeyNAbeq/eu/cxHp3xJjvbZAre5zcN+Kns</latexit>\nMLM\n<latexit sha1_base64=\"ZrgITluuF6vmJwC4sBU06PhfIvY=\">AAACHHicbVBNS8NAEN34WetX1aOXYBE8lUQUPRa9eLBQi63FpshmO9XF3U3Ynagl5F94tb/Gm3gV/DGCm7YHqw4MPN6b4c28MBbcoOd9OjOzc/MLi4Wl4vLK6tp6aWOzZaJEM2iySES6HVIDgitoIkcB7VgDlaGAq/D+NNevHkAbHqlLHMTQlfRW8T5nFC11HSA8YVo7r2U3pbJX8Ubl/gX+BJTJpOo3pa+gF7FEgkImqDEd34uxm1KNnAnIikFiIKbsnt5Cx0JFJZhuOro4c3ct03P7kbat0B2xPzdSKo0ZyNBOSop35reWk/9pnQT7x92UqzhBUGxs1E+Ei5Gbv+/2uAaGYmABZZrbW112RzVlaEOacgml/UHBI4ukpKqXBo0sDXLDMEwbWZ6X/zudv6C1X/EPKocXB+XqySS5AtkmO2SP+OSIVMkZqZMmYUSRZ/JChs7QeXXenPfx6Iwz2dkiU+V8fAOBaaNk</latexit>\nLoss\n<latexit sha1_base64=\"gGEnD0lTvs3n+fgJ2GbS8suyRXk=\">AAACHXicbVDLSgNBEJyN7/iKevSyGARPYVciegx68eAhBmOEbJDZSUeHzGOZ6VXDsp/hVb/Gm3gVP0ZwNuagxoaGoqqb6q44EdxiEHx4pZnZufmFxaXy8srq2nplY/PS6tQwaDMttLmKqQXBFbSRo4CrxACVsYBOPDwp9M4dGMu1usBRAj1JbxQfcEbRUd0I4QGzM21tfl2pBrVgXP40CCegSibVvK58Rn3NUgkKmaDWdsMgwV5GDXImIC9HqYWEsiG9ga6DikqwvWx8cu7vOqbvD7RxrdAfsz83MiqtHcnYTUqKt/avVpD/ad0UB0e9jKskRVDs22iQCh+1X/zv97kBhmLkAGWGu1t9dksNZehS+uUSS/eDgnumpaSqn0WtPIsKwzjOWnmRV/g3nWlwuV8L67WD83q1cTxJbpFskx2yR0JySBrklDRJmzCiySN5Is/es/fivXpv36Mlb7KzRX6V9/4F5hSkKQ==</latexit>\nFrozen\n<latexit sha1_base64=\"p8E9BqFW1JMuhZOBcL1dRTZwzb4=\">AAACH3icbVDLSsNAFJ34tr6qLt0Ei+CqJFLRpSiISxXbCk2QyfRWB+cRZ27UGvIdbvVr3IlbP0Zw0nah1QMDh3Pu5dw5SSq4xSD49CYmp6ZnZufmKwuLS8sr1dW1ltWZYdBkWmhzmVALgitoIkcBl6kBKhMB7eT2qPTb92As1+oC+ynEkl4r3uOMopPiCOER82Ojn0AVV9VaUA8G8P+ScERqZITTq+pX1NUsk6CQCWptJwxSjHNqkDMBRSXKLKSU3dJr6DiqqAQb54OjC3/LKV2/p417Cv2B+nMjp9LavkzcpKR4Y8e9UvzP62TY249zrtIMQbFhUC8TPmq/bMDvcgMMRd8Rygx3t/rshhrK0PX0KyWR7g8KHpiWkqpuHp0XeVQGJkl+XpR9hePt/CWtnXrYqO+eNWoHh6Pm5sgG2STbJCR75ICckFPSJIzckWfyQl69V+/Ne/c+hqMT3mhnnfyC9/kNnVmlEA==</latexit>\nPLM\n<latexit sha1_base64=\"uxHmbiLcbFHbY1P8ny/uuNIxXpc=\">AAACHHicbVBNS8NAEN34WetX1aOXYBE8lUQqeix68aBQi61iU2SznerS3U3Ynagl5F94tb/Gm3gV/DGCm7YHqw4MPN6b4c28MBbcoOd9OjOzc/MLi4Wl4vLK6tp6aWOzZaJEM2iySET6OqQGBFfQRI4CrmMNVIYCrsL+Sa5fPYA2PFKXOIihI+md4j3OKFrqJkB4wrR+dp7dlspexRuV+xf4E1Amk6rflr6CbsQSCQqZoMa0fS/GTko1ciYgKwaJgZiyPr2DtoWKSjCddHRx5u5apuv2Im1boTtif26kVBozkKGdlBTvzW8tJ//T2gn2jjopV3GCoNjYqJcIFyM3f9/tcg0MxcACyjS3t7rsnmrK0IY05RJK+4OCRxZJSVU3DRpZGuSGYZg2sjwv/3c6f0Frv+JXKwcX1XLteJJcgWyTHbJHfHJIauSU1EmTMKLIM3khQ2fovDpvzvt4dMaZ7GyRqXI+vgGGf6Nn</latexit>\nTask\n<latexit sha1_base64=\"400J+XfkSfO33mkbANDTQbOf/Go=\">AAACHXicbVBNS8NAEN3U7++qRy/BIngqiVT0WPTiUcWq0BTZbKft0v0IuxO1hPwMr/prvIlX8ccIbmoOtjow8Hhvhjfz4kRwi0Hw6VVmZufmFxaXlldW19Y3qptb11anhkGLaaHNbUwtCK6ghRwF3CYGqIwF3MTD00K/uQdjuVZXOEqgI2lf8R5nFB3VjhAeMbuidpjfVWtBPRiX/xeEJaiRss7vql9RV7NUgkImqLXtMEiwk1GDnAnIl6PUQkLZkPah7aCiEmwnG5+c+3uO6fo9bVwr9Mfs742MSmtHMnaTkuLATmsF+Z/WTrF33Mm4SlIExX6MeqnwUfvF/36XG2AoRg5QZri71WcDaihDl9KESyzdDwoemJaSqm4WXeZZVBjGcXaZF3mF0+n8BdcH9bBRP7xo1JonZXKLZIfskn0SkiPSJGfknLQII5o8kWfy4r14r96b9/4zWvHKnW0yUd7HN85wpBs=</latexit>\nLoss\n<latexit sha1_base64=\"gGEnD0lTvs3n+fgJ2GbS8suyRXk=\">AAACHXicbVDLSgNBEJyN7/iKevSyGARPYVciegx68eAhBmOEbJDZSUeHzGOZ6VXDsp/hVb/Gm3gVP0ZwNuagxoaGoqqb6q44EdxiEHx4pZnZufmFxaXy8srq2nplY/PS6tQwaDMttLmKqQXBFbSRo4CrxACVsYBOPDwp9M4dGMu1usBRAj1JbxQfcEbRUd0I4QGzM21tfl2pBrVgXP40CCegSibVvK58Rn3NUgkKmaDWdsMgwV5GDXImIC9HqYWEsiG9ga6DikqwvWx8cu7vOqbvD7RxrdAfsz83MiqtHcnYTUqKt/avVpD/ad0UB0e9jKskRVDs22iQCh+1X/zv97kBhmLkAGWGu1t9dksNZehS+uUSS/eDgnumpaSqn0WtPIsKwzjOWnmRV/g3nWlwuV8L67WD83q1cTxJbpFskx2yR0JySBrklDRJmzCiySN5Is/es/fivXpv36Mlb7KzRX6V9/4F5hSkKQ==</latexit>\nFrozen\n<latexit sha1_base64=\"p8E9BqFW1JMuhZOBcL1dRTZwzb4=\">AAACH3icbVDLSsNAFJ34tr6qLt0Ei+CqJFLRpSiISxXbCk2QyfRWB+cRZ27UGvIdbvVr3IlbP0Zw0nah1QMDh3Pu5dw5SSq4xSD49CYmp6ZnZufmKwuLS8sr1dW1ltWZYdBkWmhzmVALgitoIkcBl6kBKhMB7eT2qPTb92As1+oC+ynEkl4r3uOMopPiCOER82Ojn0AVV9VaUA8G8P+ScERqZITTq+pX1NUsk6CQCWptJwxSjHNqkDMBRSXKLKSU3dJr6DiqqAQb54OjC3/LKV2/p417Cv2B+nMjp9LavkzcpKR4Y8e9UvzP62TY249zrtIMQbFhUC8TPmq/bMDvcgMMRd8Rygx3t/rshhrK0PX0KyWR7g8KHpiWkqpuHp0XeVQGJkl+XpR9hePt/CWtnXrYqO+eNWoHh6Pm5sgG2STbJCR75ICckFPSJIzckWfyQl69V+/Ne/c+hqMT3mhnnfyC9/kNnVmlEA==</latexit>\nDM\n<latexit sha1_base64=\"qk2n4EH0S3r+AsR+a1EjWr7xQq4=\">AAACG3icbVBNS8NAEN34bf2qevQSLIKnkkhFj0U9eBFqsVVpStlsp7p0dxN2J2oJ+RVe9dd4E68e/DGCm7YHax0YeLw3w5t5YSy4Qc/7cmZm5+YXFpeWCyura+sbxc2tpokSzaDBIhHpm5AaEFxBAzkKuIk1UBkKuA77p7l+/QDa8Ehd4SCGtqR3ivc4o2ip2wDhCdOzi6xTLHllb1juNPDHoETGVesUv4NuxBIJCpmgxrR8L8Z2SjVyJiArBImBmLI+vYOWhYpKMO10eHDm7lmm6/YibVuhO2R/b6RUGjOQoZ2UFO/NXy0n/9NaCfaO2ylXcYKg2MiolwgXIzf/3u1yDQzFwALKNLe3uuyeasrQZjThEkr7g4JHFklJVTcN6lka5IZhmNazPC//bzrToHlQ9ivlw8tKqXoyTm6J7JBdsk98ckSq5JzUSIMwIskzeSGvzqvz5rw7H6PRGWe8s00myvn8AcmNowU=</latexit>\nPLM\n<latexit sha1_base64=\"uxHmbiLcbFHbY1P8ny/uuNIxXpc=\">AAACHHicbVBNS8NAEN34WetX1aOXYBE8lUQqeix68aBQi61iU2SznerS3U3Ynagl5F94tb/Gm3gV/DGCm7YHqw4MPN6b4c28MBbcoOd9OjOzc/MLi4Wl4vLK6tp6aWOzZaJEM2iySET6OqQGBFfQRI4CrmMNVIYCrsL+Sa5fPYA2PFKXOIihI+md4j3OKFrqJkB4wrR+dp7dlspexRuV+xf4E1Amk6rflr6CbsQSCQqZoMa0fS/GTko1ciYgKwaJgZiyPr2DtoWKSjCddHRx5u5apuv2Im1boTtif26kVBozkKGdlBTvzW8tJ//T2gn2jjopV3GCoNjYqJcIFyM3f9/tcg0MxcACyjS3t7rsnmrK0IY05RJK+4OCRxZJSVU3DRpZGuSGYZg2sjwv/3c6f0Frv+JXKwcX1XLteJJcgWyTHbJHfHJIauSU1EmTMKLIM3khQ2fovDpvzvt4dMaZ7GyRqXI+vgGGf6Nn</latexit>\nKnowledge\n<latexit sha1_base64=\"TQxrrtf1GGMk+a+tOiQIJsCd1So=\">AAACJHicbVDLSsNAFJ34rO+oSzfBIrgqiSi6FN0IblSsCk0pk8ltHZxHmLmplpA/cWu/xp24cOOfCE5qF74ODBzOuZdz5ySZ4BbD8M2bmJyanpmtzc0vLC4tr/ira1dW54ZBk2mhzU1CLQiuoIkcBdxkBqhMBFwnd8eVf90HY7lWlzjIoC1pT/EuZxSd1PH9GOEBi1Ol7wWkPSg7fj1shCMEf0k0JnUyxlnH/4hTzXIJCpmg1raiMMN2QQ1yJqCcj3MLGWV3tActRxWVYNvF6PIy2HJKGnS1cU9hMFK/bxRUWjuQiZuUFG/tb68S//NaOXYP2gVXWY6g2FdQNxcB6qCqIUi5AYZi4AhlhrtbA3ZLDWXoyvqRkkj3BwX3TEtJVVrEF2URV4FJUlyUVV/R73b+kqudRrTb2DvfrR8ejZurkQ2ySbZJRPbJITkhZ6RJGOmTR/JEht7Qe/ZevNev0QlvvLNOfsB7/wSUp6aL</latexit>\nPre-training/Fine-tuning\n<latexit sha1_base64=\"AOWkeK+AA+CnXE8Spy/xTO+lUjM=\">AAACM3icbVBNSwMxEM36/W3VowcXi+DFuiuKHkVBPNZirdAtJZtOazDJLsmsWpY9+mu82h8j3sSrP0EwW3uwrQOBx3sz8zIvjAU36HlvzsTk1PTM7Nz8wuLS8spqYW39xkSJZlBlkYj0bUgNCK6gihwF3MYaqAwF1ML781yvPYA2PFLX2I2hIWlH8TZnFC3VLGwFCE+YljXsoaZccdXZv7C79jDJcdYsFL2S1y93HPgDUCSDKjcL30ErYokEhUxQY+q+F2MjpRo5E5AtBImBmLJ72oG6hYpKMI20f0jm7lim5bYjbZ9Ct8/+nUipNKYrQ9spKd6ZUS0n/9PqCbZPGilXcYKg2K9ROxEuRm6eitviGhiKrgWUaW7/6rI7qilDm92QSyjtDQoeWSQlVa00qGRpkBuGYVrJ8rz80XTGwc1ByT8sHV0dFk/PBsnNkU2yTXaJT47JKbkkZVIljDyTF/JKek7PeXc+nM/f1glnMLNBhsr5+gGS2ayk</latexit>\nDownstream\n<latexit sha1_base64=\"XkSfTJnEe/ceWT2SPqC/cOTtbo0=\">AAACJXicbVDLSgNBEJz1bXxFPXpZDIKnsCuKHoN68BjFRCEbwuyko4PzWGZ6jWHZT/FqvsabCJ78EsHZmIOJFgwUVd1UT8WJ4BaD4MObmZ2bX1hcWi6trK6tb5Q3t5pWp4ZBg2mhzW1MLQiuoIEcBdwmBqiMBdzED2eFf/MIxnKtrnGQQFvSO8V7nFF0Uqe8GSE8YXau+8pisZh3ypWgGozg/yXhmFTIGPVO+SvqapZKUMgEtbYVBgm2M2qQMwF5KUotJJQ90DtoOaqoBNvORqfn/p5Tun5PG/cU+iP190ZGpbUDGbtJSfHeTnuF+J/XSrF30s64SlIExX6CeqnwUftFD36XG2AoBo5QZri71Wf31FCGrq2JlFi6PyjoMy0lVd0susqzqAiM4+wqL/oKp9v5S5oH1fCwenR5WKmdjptbIjtkl+yTkByTGrkgddIgjPTJM3khQ2/ovXpv3vvP6Iw33tkmE/A+vwGfSqcZ</latexit>\nModel\n<latexit sha1_base64=\"+Cwi+aVNb39//t0rmMsdEPJXGjk=\">AAACHnicbVDLSsNAFJ34flt16SZYBFclkYoui27cCLVYKzRFJpPbdnAeYeZGLSG/4Va/xp241Y8RnNQufB0YOJxzL+fOiVPBLQbBuzc1PTM7N7+wuLS8srq2XtnYvLQ6MwzaTAttrmJqQXAFbeQo4Co1QGUsoBPfnJR+5xaM5Vpd4CiFnqQDxfucUXRSFCHcY36mExDFdaUa1IIx/L8knJAqmaB5XfmIEs0yCQqZoNZ2wyDFXk4NciagWIoyCyllN3QAXUcVlWB7+fjmwt91SuL3tXFPoT9Wv2/kVFo7krGblBSH9rdXiv953Qz7R72cqzRDUOwrqJ8JH7VfFuAn3ABDMXKEMsPdrT4bUkMZupp+pMTS/UHBHdNSUpXkUavIozIwjvNWUfYV/m7nL7ncr4X12sF5vdo4njS3QLbJDtkjITkkDXJKmqRNGEnJA3kkT96T9+y9eK9fo1PeZGeL/ID39gmWW6SD</latexit>\nPrevious Knowledge Injection\n<latexit sha1_base64=\"SsQJTJ3mP3xMsXw0hL4q8C8TXKI=\">AAACOXicbVBNTxsxFPQChRQopPTYi0WExCnarUBwjODSikuKCETKRpHtfRtc/LGyvQnRan9Bfw1X8ks49lZx5Vyp3iQHvp5kaTQzT+M3NBPcujB8CJaWVz6srtU+rm9sftrarn/eubQ6Nww6TAttupRYEFxBx3EnoJsZIJIKuKI3p5V+NQJjuVYXbpJBX5Kh4ilnxHlqUN+LHdw6mhZtAyOuc4vPlB4LSIaAf6hfwCpbOag3wmY4G/wWRAvQQItpD+r/4kSzXIJyTBBre1GYuX5BjONMQLke5xYywm7IEHoeKiLB9ovZOSXe80yCU238Uw7P2OcbBZHWTiT1TknctX2tVeR7Wi936XG/4CrLHSg2D0pzgZ3GVTc44cYfLCYeEGa4/ytm18QQ5nyDL1Ko9DcoGDMtJVFJEZ+XRVwFUlqcl1Vf0et23oLLb83ooHn486DROlk0V0Nf0S7aRxE6Qi30HbVRBzH0G92hezQNpsGf4G/wOLcuBYudL+jFBE//AQUcr2U=</latexit>\nDownstream\n<latexit sha1_base64=\"XkSfTJnEe/ceWT2SPqC/cOTtbo0=\">AAACJXicbVDLSgNBEJz1bXxFPXpZDIKnsCuKHoN68BjFRCEbwuyko4PzWGZ6jWHZT/FqvsabCJ78EsHZmIOJFgwUVd1UT8WJ4BaD4MObmZ2bX1hcWi6trK6tb5Q3t5pWp4ZBg2mhzW1MLQiuoIEcBdwmBqiMBdzED2eFf/MIxnKtrnGQQFvSO8V7nFF0Uqe8GSE8YXau+8pisZh3ypWgGozg/yXhmFTIGPVO+SvqapZKUMgEtbYVBgm2M2qQMwF5KUotJJQ90DtoOaqoBNvORqfn/p5Tun5PG/cU+iP190ZGpbUDGbtJSfHeTnuF+J/XSrF30s64SlIExX6CeqnwUftFD36XG2AoBo5QZri71Wf31FCGrq2JlFi6PyjoMy0lVd0susqzqAiM4+wqL/oKp9v5S5oH1fCwenR5WKmdjptbIjtkl+yTkByTGrkgddIgjPTJM3khQ2/ovXpv3vvP6Iw33tkmE/A+vwGfSqcZ</latexit>\nModel\n<latexit sha1_base64=\"+Cwi+aVNb39//t0rmMsdEPJXGjk=\">AAACHnicbVDLSsNAFJ34flt16SZYBFclkYoui27cCLVYKzRFJpPbdnAeYeZGLSG/4Va/xp241Y8RnNQufB0YOJxzL+fOiVPBLQbBuzc1PTM7N7+wuLS8srq2XtnYvLQ6MwzaTAttrmJqQXAFbeQo4Co1QGUsoBPfnJR+5xaM5Vpd4CiFnqQDxfucUXRSFCHcY36mExDFdaUa1IIx/L8knJAqmaB5XfmIEs0yCQqZoNZ2wyDFXk4NciagWIoyCyllN3QAXUcVlWB7+fjmwt91SuL3tXFPoT9Wv2/kVFo7krGblBSH9rdXiv953Qz7R72cqzRDUOwrqJ8JH7VfFuAn3ABDMXKEMsPdrT4bUkMZupp+pMTS/UHBHdNSUpXkUavIozIwjvNWUfYV/m7nL7ncr4X12sF5vdo4njS3QLbJDtkjITkkDXJKmqRNGEnJA3kkT96T9+y9eK9fo1PeZGeL/ID39gmWW6SD</latexit>\nPLM\n<latexit sha1_base64=\"uxHmbiLcbFHbY1P8ny/uuNIxXpc=\">AAACHHicbVBNS8NAEN34WetX1aOXYBE8lUQqeix68aBQi61iU2SznerS3U3Ynagl5F94tb/Gm3gV/DGCm7YHqw4MPN6b4c28MBbcoOd9OjOzc/MLi4Wl4vLK6tp6aWOzZaJEM2iySET6OqQGBFfQRI4CrmMNVIYCrsL+Sa5fPYA2PFKXOIihI+md4j3OKFrqJkB4wrR+dp7dlspexRuV+xf4E1Amk6rflr6CbsQSCQqZoMa0fS/GTko1ciYgKwaJgZiyPr2DtoWKSjCddHRx5u5apuv2Im1boTtif26kVBozkKGdlBTvzW8tJ//T2gn2jjopV3GCoNjYqJcIFyM3f9/tcg0MxcACyjS3t7rsnmrK0IY05RJK+4OCRxZJSVU3DRpZGuSGYZg2sjwv/3c6f0Frv+JXKwcX1XLteJJcgWyTHbJHfHJIauSU1EmTMKLIM3khQ2fovDpvzvt4dMaZ7GyRqXI+vgGGf6Nn</latexit>\nKnowledge\n<latexit sha1_base64=\"TQxrrtf1GGMk+a+tOiQIJsCd1So=\">AAACJHicbVDLSsNAFJ34rO+oSzfBIrgqiSi6FN0IblSsCk0pk8ltHZxHmLmplpA/cWu/xp24cOOfCE5qF74ODBzOuZdz5ySZ4BbD8M2bmJyanpmtzc0vLC4tr/ira1dW54ZBk2mhzU1CLQiuoIkcBdxkBqhMBFwnd8eVf90HY7lWlzjIoC1pT/EuZxSd1PH9GOEBi1Ol7wWkPSg7fj1shCMEf0k0JnUyxlnH/4hTzXIJCpmg1raiMMN2QQ1yJqCcj3MLGWV3tActRxWVYNvF6PIy2HJKGnS1cU9hMFK/bxRUWjuQiZuUFG/tb68S//NaOXYP2gVXWY6g2FdQNxcB6qCqIUi5AYZi4AhlhrtbA3ZLDWXoyvqRkkj3BwX3TEtJVVrEF2URV4FJUlyUVV/R73b+kqudRrTb2DvfrR8ejZurkQ2ySbZJRPbJITkhZ6RJGOmTR/JEht7Qe/ZevNev0QlvvLNOfsB7/wSUp6aL</latexit>\n(1) Fine-tuning\n<latexit sha1_base64=\"nyzgROdLylaYuaya1zj1yS8EFnA=\">AAACKnicbVDLSgMxFM34flsV3LgJFkEXlhlRdCkK4rIWW4VOKZn0tgaTzJDcUcs4P+NWv8aduPU3BDO1C6seCBzOfZybEyVSWPT9N29sfGJyanpmdm5+YXFpubSy2rBxajjUeSxjcx0xC1JoqKNACdeJAaYiCVfR7WlRv7oDY0WsL7GfQEuxnhZdwRk6qV1aDxEeMNsOduiZW7GLqRa6l7dLZb/iD0D/kmBIymSIarv0GXZinirQyCWzthn4CbYyZlBwCflcmFpIGL9lPWg6qpkC28oG9+d0yykd2o2NexrpQP05kTFlbV9FrlMxvLG/a4X4X62ZYveolQmdpAiafxt1U0kxpkUYtCMMcJR9Rxg3wt1K+Q0zjKOLbMQlUu4PGu55rBTTnSys5VlYGEZRVsuLvILf6fwljb1KsF85uNgvH58Mk5shG2STbJOAHJJjck6qpE44eSRP5Jm8eC/eq/fmvX+3jnnDmTUyAu/jC7h2qA0=</latexit>\n(2) Injection\n<latexit sha1_base64=\"p2a7N1jnb6y1GFf6Cnxuu4MuukI=\">AAACKHicbVDLSgNBEJz1bXzFx83LYBD0EnZDRI+iF71pMA/IhjA76SRjZmaXmVk1LvsvXvVrvEmu/ofgbJKDRhsaiqpuqruCiDNtXHfkzM0vLC4tr6zm1tY3Nrfy2zs1HcaKQpWGPFSNgGjgTELVMMOhESkgIuBQDwaXmV5/AKVZKO/MMIKWID3JuowSY6l2fs838GSSo9Ixvpb3QDM2becLbtEdF/4LvCkooGndtPNffieksQBpKCdaNz03Mq2EKMMohzTnxxoiQgekB00LJRGgW8n4+hQfWqaDu6GyLQ0esz83EiK0HorATgpi+npWy8j/tGZsumethMkoNiDpxKgbc2xCnEWBO0zZh/nQAkIVs7di2ieKUGMD++USCPuDhEcaCkFkJ/EraeJnhkGQVNIsL282nb+gVip65eLJbblwfjFNbgXtowN0hDx0is7RFbpBVUTRM3pBr+jNeXPenQ9nNBmdc6Y7u+hXOZ/fUuSnWQ==</latexit>\nPlug-and-play Knowledge Injection\n<latexit sha1_base64=\"j6OkZfZvhTuSocl1jD5dBiIQ2hQ=\">AAACPnicbVDLThsxFPXwKoRHQ1l2YzVCQkhEM1UQXUbthqqbgBpAykSR7bkTTPwY2R5oNJp/4GvYlt/gB7qr2LJBwpNkwetKlo7OOVfH99BMcOvC8C6Ym19YXPqwvFJbXVvf+Fjf/HRidW4YdJkW2pxRYkFwBV3HnYCzzACRVMApHf2o9NNLMJZr9duNM+hLMlQ85Yw4Tw3qu7GDP46mRUfkwz2ikr1MkDH+pfSVgGQI+Ke6AFZ5y0G9ETbDyeC3IJqBBppNZ1B/jBPNcgnKMUGs7UVh5voFMY4zAWUtzi1khI3IEHoeKiLB9ovJTSXe9kyCU238Uw5P2OcbBZHWjiX1TkncuX2tVeR7Wi936bd+wVWWO1BsGpTmAjuNq4Jwwo0/WIw9IMxw/1fMzokhzPkaX6RQ6W9QcMW0lL65Ij4ui7gKpLQ4Lqu+otftvAUnX5tRq7l/1Gq0v8+aW0af0Re0gyJ0gNroEHVQFzF0jW7QX3Qb3Ab/gv/B/dQ6F8x2ttCLCR6eAIVFsRU=</latexit>\nHamsters\n<latexit sha1_base64=\"27LKo39zMNGbwHFFuah0emPaqUg=\">AAACI3icbVBNS8NAFNzU789WPXoJFsFTSUTRY9FLj1VsFZpSNttXXbq7Cbsvagn5JV7tr/EmXjz4UwQ3bQ62OrAwzJvHvJ0wFtyg5306pYXFpeWV1bX1jc2t7XJlZ7dtokQzaLFIRPoupAYEV9BCjgLuYg1UhgJuw+FlPr99BG14pG5wFENX0nvFB5xRtFKvUg4QnjFtUGnQ2rJeperVvAncv8QvSJUUaPYq30E/YokEhUxQYzq+F2M3pRo5E5CtB4mBmLIhvYeOpYpKMN10cnjmHlql7w4ibZ9Cd6L+3kjtWWYkQ+uUFB/M/CwX/5t1Ehycd1Ou4gRBsWnQIBEuRm7egtvnGhiKkSWUaW5vddkD1ZTlJcykhNL+QcETi6Skqp8G11ka5IFhmF5neV/+fDt/Sfu45p/UTq9OqvWLorlVsk8OyBHxyRmpkwZpkhZhJCEv5JWMnbHz5rw7H1NrySl29sgMnK8f0vCmKA==</latexit>\n(a)\n<latexit sha1_base64=\"IiQcAAYHuIDgHl1FpQD455bTZiw=\">AAACHHicbVBNS8NAEN34WetX1aOXYBHqpSSi6FH04rEWW8WmlM122i7ubsLuRC0h/8Kr/TXexKvgjxHctD1odWDg8d4Mb+aFseAGPe/TmZtfWFxaLqwUV9fWNzZLW9tNEyWaQYNFItK3ITUguIIGchRwG2ugMhRwE95f5PrNA2jDI3WNwxjakvYV73FG0VJ3AcITphV6kHVKZa/qjcv9C/wpKJNp1Tqlr6AbsUSCQiaoMS3fi7GdUo2cCciKQWIgpuye9qFloaISTDsdX5y5+5bpur1I21bojtmfGymVxgxlaCclxYGZ1XLyP62VYO+0nXIVJwiKTYx6iXAxcvP33S7XwFAMLaBMc3urywZUU4Y2pF8uobQ/KHhkkZRUddOgnqVBbhiGaT3L8/Jn0/kLmodV/6h6fHVUPjufJlcgu2SPVIhPTsgZuSQ10iCMKPJMXsjIGTmvzpvzPhmdc6Y7O+RXOR/fKXSjMA==</latexit>\n(b)\n<latexit sha1_base64=\"roBzL38q0iLY5OiUBSulvSE7c0s=\">AAACHHicbVDLSgNBEJz1bXxFPXpZDIJewq4oehS9eFQxD8wGmZl0dHBmdpnpVcOyf+FVv8abeBX8GMHZJAeT2NBQVHVT3cUSKSwGwbc3NT0zOze/sFhaWl5ZXSuvb9RtnBoONR7L2DQZtSCFhhoKlNBMDFDFJDTYw1mhNx7BWBHra+wl0Fb0Touu4BQddRMhPGO2y/by23IlqAb98idBOAQVMqyL2/JP1Il5qkAjl9TaVhgk2M6oQcEl5KUotZBQ/kDvoOWgpgpsO+tfnPs7jun43di41uj32b8bGVXW9hRzk4rivR3XCvI/rZVi97idCZ2kCJoPjLqp9DH2i/f9jjDAUfYcoNwId6vP76mhHF1IIy5MuR80PPFYKao7WXSVZ1FhyFh2lRd5hePpTIL6fjU8qB5eHlROTofJLZAtsk12SUiOyAk5JxekRjjR5IW8kjfvzXv3PrzPweiUN9zZJCPlff0CKyWjMQ==</latexit>\nFigure 2: Left: Comparisons between previous paradigms and our proposed plug-and-play paradigm. Right: Two\nways for map-tuning. The input text is “Hamsters love peanut butter.”. “DM” refers to “downstream model”.\nrectly used for general plug-and-play knowledge\ninjection. We summarize the existing knowledge\ninjection methods1 that have the possibility to be\nused for general plug-and-play knowledge injec-\ntion as follows. (1) Embedding-based methods:\nE-BERT (Poerner et al., 2020) and PELT (Ye\net al., 2022) build an entity embedding lookup ta-\nble in the representation space of token embed-\ndings and combine entity embeddings with token\nembeddings to construct input embeddings. (2)\nRetrieval-based methods: RAG (Lewis et al., 2020)\nretrieves plain text from knowledge bases and aug-\nments the original input text with the plain text as\ninjected knowledge. (3) Adapter-based methods:\nK-Adapter (Wang et al., 2021a) computes knowl-\nedgeable representations based on the outputs of\nthe downstream models accompanied by knowl-\nedgeable adapters, which are trained with frozen\nPLMs and plugged into all downstream models.\nEven though these methods may bring knowledge\nwithout training PLMs, it is unclear whether they\nwork well in the plug-and-play knowledge injec-\ntion paradigm, i.e., whether the knowledge brought\nby them is utilizable for downstream models that\nhave never learned how to use these methods.\n3 Map-Tuning\nIn this section, we ﬁrst present the overall frame-\nwork of map-tuning, which is designed for plug-\nand-play knowledge injection. Then, we show how\nto use it for general injection and task-speciﬁc in-\njection, where the methods are called general map-\ntuning and task-speciﬁc map-tuning respectively.\n1These methods are originally designed for injecting\nknowledge during pre-training or ﬁne-tuning.\n3.1 Overall Framework\nWe target knowledge bases consisting of a set of\nentities and structured or unstructured knowledge\nabout these entities. To utilize such a knowledge\nbase B, we assume a knowledge representation\nmodel Kto assign each entity e an entity embed-\nding e ∈RdKE, where dKE is the dimension of\nentity embeddings. Map-tuning injects knowledge\nby mapping knowledge representations into the\nspace of token embeddings and using the mapped\nrepresentations as additional inputs, which is also\nadopted by Poerner et al. (2020); Ye et al. (2022).\nSpeciﬁcally, given an input text, we ﬁrst match\nthe entity mentions in the text with the entities in\nB. The input text is denoted by {w1, w2, . . . , wn},\nwhere wi is the i-th token and n is the number\nof tokens in the input text. We use a triple\n(e, l, r) to represent a mention span, where e\nis the matched entity, l and r are the left and\nright token indices of the mention span. The\ncorresponding mention span is {wl, wl+1, . . . , wr}.\nAssume there are m entities in the text,\n(e1, l1, r1), (e2, l2, r2), . . . ,(em, lm, rm), where\n1 ≤l1 ≤r1 < l2 ≤r2 < ··· < lm ≤rm ≤n.\nThe original sequence of input embeddings are\n{w1, w2, . . . ,wn}, where wi ∈RdPLM is the i-th\ntoken embedding and dPLM is the dimension of to-\nken embeddings. Then, we map each entity embed-\nding ei to M(ei) ∈RdPLM by a mapping network\nM. Finally, we replace {wli , wli+1, . . . ,wri }\nwith {M(ei), /,wli , . . . ,wri }for every (ei, li, ri)\nto construct a new input sequence. Note that / is\nthe token embedding of “/”.\n3.2 General Map-tuning\nGeneral map-tuning aims to train a mapping net-\nwork Mbased on Pand K. It requires Mto have\nenough generalizability to handle different down-\nstream tasks because Mwill be plugged into all\ndownstream models. Hence, we train Mwith a\ngeneral pre-training task while plugging it into P,\nsuch as language modeling, which has been shown\nto be an unsupervised multi-task learning (Radford\net al., 2019). We freeze the parameters of Pand\nonly train the mapping network Mto meet the\nrequirement of plug-and-play knowledge injection.\nWe adopt a variant of Masked Language Model\n(MLM) (Devlin et al., 2019), named Mention-\nMasked Language Modeling (MMLM), as the task\nfor training M. According to our observation in\nthe preliminary experiments, the prediction of most\ntokens requires only language ability instead of ex-\nternal knowledge, such as that of some stop words,\nwhile the prediction of entity mentions relies on\nexternal knowledge more often. Hence, as shown\nin Figure 2(b), we randomly mask only entity men-\ntions2 in the input text to ensure that the mapping\nnetwork is trained sufﬁciently and the mapped em-\nbeddings are well utilized in the PLM. In this way,\nthe ability of PLMs to predict masked entity men-\ntions is enhanced by the mapped embeddings of\nboth the masked entity and other entities in the\ncontext. We mask all tokens of a masked entity\nmention, and the MMLM loss is the same as the\noriginal MLM loss (Devlin et al., 2019).\nAfter general map-tuning, Mcan be used for the\ngeneral plug-and-play injection. Although the map-\nping network Mwas not trained with any down-\nstream model Dbefore, we can directly plug M\ninto each D.\n3.3 Task-speciﬁc Map-tuning\nTask-speciﬁc map-tuning aims to adapt a mapping\nnetwork Mfor a given downstream model D. We\nfreeze the parameters of Dand train the mapping\nnetwork Mon the downstream task, whose proce-\ndure is shown in Figure 2(b). The training objective\nis identical to the original objective of this task. If\nthe knowledge representations provide useful in-\nformation for this task, the mapping network will\nlearn to extract this information and to make it rec-\nognizable to the downstream model D. Note that\nthe mapping network can not only be trained from\nscratch, but can also be initialized with a mapping\nnetwork learned with general map-tuning, which\ncould provide a good starting point.\n2If the number of entity mentions is small, we can choose\nto cover all the masking combinations.\n4 Experiments\n4.1 Experimental Setups\nTraining Methods of Downstream Models. We\nadopt BERTbase (Devlin et al., 2019) as the back-\nbone PLM in the experiments and consider four\ntraining methods for its adaptation to downstream\ntasks. Besides vanilla full-model ﬁne-tuning,\nwe also consider three parameter-efﬁcient tuning\n(PET) methods, which have been becoming increas-\ningly important in the era of large-scale PLMs (Liu\net al., 2021). As resource-saving are both plug-and-\nplay knowledge injection and PET, it is meaning-\nful to apply this paradigm to downstream models\ntrained by PET methods in the resource-limited\nscenario. (1) Fine-tuning optimizes all the param-\neters of a PLM with the task objective following\nthe original BERT. (2) LoRA (Hu et al., 2021)\nfreezes the PLM parameters and injects trainable\nrank-decomposition matrices as additional parame-\nters. (3) Adapter (Houlsby et al., 2019) injects ad-\nditional adapter networks with the PLM parameters\nfrozen. (4) BitFit (Zaken et al., 2021) only opti-\nmizes the parameters of bias vectors and freezes the\nrest parameters. The hyper-parameters are reported\nin Appendix A.\nDownstream Tasks. We evaluate methods\nunder the plug-and-play knowledge injection\nparadigm on three kinds of knowledge-driven NLP\ntasks including relation classiﬁcation, entity typing,\nand question answering. For relation classiﬁca-\ntion, which requires models to classify the relation\nbetween two entities given a context, we experi-\nment on both few-shot and full-data settings. In\nthe few-shot setting, we aim to evaluate model\nperformance on long-tail relations whose training\ninstances are not sufﬁcient. Speciﬁcally, we use\nFewRel 1.0 (Han et al., 2018) and FewRel 2.0 (Gao\net al., 2019).3 In the full-data setting, we evaluate\nmodels on Wiki80 (Han et al., 2019), which con-\ntains 80 relation types from Wikidata, and follow\nthe data split of Zhang et al. (2019). For entity\ntyping, which requires models to classify the type\nof an entity given a context, we evaluate models\non Wiki-ET (Xin et al., 2018) containing 68 entity\ntypes from Freebase. For question answering, we\nevaluate models on EntityQuestions (Sciavolino\n3We randomly sample 5000 instances from test data of\nFewRel 1.0 and FewRel 2.0 respectively for fast evaluation.\nNote that the test data is not publicly released and we get the\ndata from the authors. We experiment with full test data of\nFewRel 1.0 on the ofﬁcial leaderboard in Section 4.4.\net al., 2021), an open-domain QA dataset consist-\ning of entity-centric questions. We use knowledge-\nenhanced models to directly answer questions with-\nout retrieving related documents. We report accu-\nracy on relation classiﬁcation and question answer-\ning, and F1 score on entity typing.\nKnowledge Bases. We use Wikidata5M (Wang\net al., 2021b) and UMLS4 as our external knowl-\nedge bases for the Wikipedia domain and PubMed5\ndomain, respectively. To avoid information leakage\nin the relation classiﬁcation task, we remove the\ntriples appearing in the datasets from these knowl-\nedge bases. We adopt TransE (Bordes et al., 2013)\nas our knowledge representation model and the\ndimension of knowledge embeddings is set to 128.\nEvaluated Existing Methods. We evaluate ex-\nisting methods that can be applied to general plug-\nand-play knowledge injection. (1) E-BERT (Po-\nerner et al., 2020) also obtains a mapping net-\nwork to transform knowledge embeddings. Dif-\nferent from map-tuning, E-BERT builds the con-\nnection between the vocabulary and entities by\nstring matching, and then make the mapped knowl-\nedge embeddings close to their corresponding to-\nken embeddings. In this work, E-BERT uses the\nsame TransE embeddings as map-tuning instead of\nwikipedia2vec for fair comparisons. (2) PELT (Ye\net al., 2022) aggregates the output representations\nof a speciﬁc entity in multiple contexts to build\nthe entity representation. Then, the entity rep-\nresentation can be appended to the model input\nwithout any mapping because the input space and\noutput space are the same for most PLMs. The\nentity-related context can be treated as an external\ntextual knowledge base. (3) Retrieval Augmen-\ntation (RA) is to augment input texts with addi-\ntional retrieved unstructured knowledge, such as\nRAG (Lewis et al., 2020) and REALM (Guu et al.,\n2020). In this work, we retrieve the entity descrip-\ntions from Wikidata5M and append them to the\ninput texts. (4) K-Adapter (Wang et al., 2021a)\nimplicitly stores knowledge in the parameters of\nadapter networks. We follow the original proce-\ndure of K-Adapter while keeping the parameters of\nPLMs and adapters frozen.6\nDetails of Map-tuning. The architecture of the\n4UMLS represents the Uniﬁed Medical Language System,\nwhich is the trademark of U.S. National Library of Medicine.\n5https://pubmed.ncbi.nlm.nih.gov/\n6This procedure still requires the training of the ﬁnal fully\nconnected layer, which does not strictly meet the setting of\ngeneral plug-and-play Injection.\nmapping network is simply an afﬁne transformation\nWe + b, where W ∈RdPLM×dKE and b ∈RdPLM .\nIn this work, the parameter amount of the map-\nping network is 768 ×128 + 768< 0.1M. For\nMention-Masked Language Modeling, we use the\nraw texts of Wiki20M (Gao et al., 2021), which is\nsampled from the Wikipedia corpus and provides\nthe annotations of entity linking. The total size is\naround 300MB, much smaller than common pre-\ntraining corpora. Since map-tuning only aims to\nadapt the mapping network for a PLM, it does not\nrequire much training data. We train the mapping\nnetwork for 5 epochs, which costs only 12 hours\non an NVIDIA Tesla V100. General map-tuning\nessentially builds an entity embedding lookup ta-\nble. To evaluate its quality, we evaluate it in the\ntraditional injection during ﬁne-tuning paradigm as\na preliminary experiment. To be more speciﬁc, we\nﬁne-tune the PLMs on downstream tasks, during\nwhich the mapping network is plugged into them.\nThe details are in Appendix E. We ﬁnd that map-\ntuning consistently outperforms E-BERT and PELT\nin the traditional paradigm, which also builds entity\nembedding lookup tables.\n4.2 General Plug-and-Play Injection\nIn this subsection, we evaluate knowledge injection\nmethods in the setting of general plug-and-play\nknowledge injection, where we directly plug knowl-\nedge modules into downstream models without any\ntraining. The results are reported in Table 1.\nFrom this table, we have four observations: (1)\nAll of the four existing methods can not consis-\ntently improve the performance of downstream\nmodels. In most cases, injecting these knowl-\nedge modules degrades the model performance,\noften to a large degree. It empirically proves that\nthe setting of general plug-and-play injection is\nchallenging and these four methods are not suit-\nable in this setting. The knowledge provided by\nthese methods can not be directly used, so they\nare basically disruptive noise to the downstream\nmodels. (2) Our proposed general map-tuning\nachieves consistent improvement on almost all\ndownstream models, suggesting that the mapping\nnetwork effectively transforms knowledge embed-\ndings into the space of token embeddings and the\nmapped embeddings can be directly used by down-\nstream models. We highlight the importance of\nMention-Masked Language Modeling, which pro-\nvides sufﬁcient training instances for general map-\nMethod Injection FewRel 1.0 Wiki80 Wiki-ET EntityQuestions5-1 5-5 10-1 10-5\nFine-tuning\n− 91.0 95.1 85.4 90.8 86.1 77.5 41.7\nE-BERT 91.0 (+0.0) 95.0 ( −0.1) 86.5 ( +1.1) 90.5 ( −0.3) 85.4 (−0.7) 77.0 (−0.5) 42.9 (+1.2)\nPELT 90.5 (−0.5) 94.8 ( −0.3) 85.3 ( −0.1) 89.8 ( −1.0) 85.0 (−1.1) 76.8 (−0.7) 46.8 (+5.1)\nRA 91.5 (+0.5) 95.5 ( +0.4) 85.8 ( +0.4) 91.7 (+0.9) 85.9 (−0.2) 76.7 (−0.8) 69.5 (+27.8)\nK-Adapter 88.6 (−2.4) 94.5 ( −0.6) 82.3 ( −3.1) 89.9 ( −0.9) 86.0 (−0.1) 77.8 (+0.3) 39.2 (−2.5)\nMap-tuning 92.6 (+1.6) 95.6 (+0.5) 88.1 (+2.7) 91.2 ( +0.4) 86.7 (+0.6) 76.6 (−0.9) 49.0 (+7.3)\nLoRA\n− 90.7 95.1 84.9 91.2 85.3 77.5 42.4\nE-BERT 90.7 (+0.0) 95.2 ( +0.1) 85.4 ( +0.5) 90.4 ( −0.8) 83.7 (−1.6) 77.6 (+0.1) 44.0 (+1.6)\nPELT 89.9 (−0.8) 94.8 ( −0.3) 84.6 ( −0.3) 89.8 ( −1.4) 83.1 (−2.2) 77.5 (+0.0) 47.7 (+5.3)\nRA 91.3 (+0.6) 95.8 ( +0.7) 85.0 ( +0.1) 92.5 (+1.3) 83.8 (−1.5) 76.8 (−0.7) 47.7 (+5.3)\nK-Adapter 90.0 (−0.7) 94.8 ( −0.3) 83.4 ( −1.5) 89.1 ( −2.1) 85.0 (−0.3) 77.3 (−0.2) 41.1 (−1.3)\nMap-tuning 92.3 (+1.6) 96.0 (+0.9) 87.4 (+2.5) 91.9 ( +0.7) 85.8 (+0.5) 78.3 (+0.8) 49.6 (+7.2)\nAdapter\n− 91.2 95.2 86.2 91.1 85.7 77.5 43.6\nE-BERT 91.3 (+0.1) 95.4 ( +0.2) 86.9 ( +0.7) 91.6 ( +0.5) 84.4 (−1.3) 78.4 (+0.9) 45.1 (+1.5)\nPELT 91.0 (−0.2) 95.4 ( +0.2) 86.3 ( +0.1) 91.3 ( +0.2) 84.3 (−1.4) 77.9 (+0.4) 48.4 (+4.8)\nRA 91.7 (+0.5) 95.5 ( +0.3) 85.8 ( −0.4) 92.3 (+1.2) 85.0 (−0.7) 76.8 (−0.7) 42.9 (−0.7)\nK-Adapter 89.9 (−1.3) 94.7 ( −0.5) 83.6 ( −2.6) 90.0 ( −1.1) 85.9 (+0.2) 77.7 (+0.2) 41.5 (−2.1)\nMap-tuning 92.6 (+1.4) 95.8 (+0.6) 88.2 (+2.0) 91.8 ( +0.7) 85.9 (+0.2) 79.2 (+1.7) 50.8 (+7.2)\nBitFit\n− 89.2 94.8 83.0 90.0 82.7 77.1 41.3\nE-BERT 88.7 (−0.5) 94.5 ( −0.3) 83.5 ( +0.5) 89.6 ( −0.4) 81.3 (−1.4) 77.2 (+0.1) 42.3 (+1.0)\nPELT 88.2 (−1.0) 94.3 ( −0.5) 80.9 ( −2.1) 88.3 ( −1.7) 80.3 (−2.4) 77.6 (+0.5) 46.7 (+5.4)\nRA 89.5 (+0.3) 95.2 ( +0.4) 82.7 ( −0.3) 91.1 (+1.1) 81.8 (−0.9) 74.0 (−3.1) 33.9 (−7.4)\nK-Adapter 86.4 (−2.8) 93.7 ( −1.1) 78.8 ( −4.2) 87.5 ( −2.5) 81.5 (−1.2) 77.2 (+0.1) 40.7 (−0.6)\nMap-tuning 90.4 (+1.2) 95.5 (+0.7) 85.2 (+2.2) 90.8 ( +0.8) 83.7 (+1.0) 78.0 (+0.9) 48.4 (+7.1)\nTable 1: Results of general plug-and-play injection. We adapt BERTbase to these datasets with four different training\nmethods. There are ﬁve different injection methods. E-BERT, PELT, and Map-tuning utilize entity representations.\nRA utilizes entity descriptions as additional text input. K-Adapter utilizes the knowledge implicitly stored in the\nadapter network. Note that downstream models and injection models are trained separately. N-K indicates the\nN-way K-shot conﬁguration. We boldface the best result for each training method.\ntuning, while the matched entity-token pairs for\nE-BERT are insufﬁcient for training the mapping\nnetwork. (3) Intuitively, general map-tuning may\nwork better with PET methods than with full-model\nﬁne-tuning because PET methods change much\nfewer parameters from the PLM and general map-\ntuning is trained based on the PLM. In fact, the per-\nformance improvement brought to models trained\nby full-model ﬁne-tuning is comparable to that of\nPET methods. It demonstrates that map-tuning\nis a promising method regardless of the training\nmethods of downstream models. (4) Remarkably\nhigh is the performance improvement brought by\nRA to ﬁne-tuned BERT on EntityQuestions. We\nobserve that the retrieved entity description con-\ntains the exact answer as a substring for 62.19%\nof instances in the test set, and we remove these\ninstances and report the result in Table 16. We ﬁnd\nthat RA still gets a slightly higher performance than\nmap-tuning does for ﬁne-tuned BERT, but brings a\nsigniﬁcant performance drop to other downstream\nmodels, while map-tuning brings consistent perfor-\nmance improvement to all downstream models. It\nsuggests that ﬁne-tuned BERT has the surprising\ngeneralization ability to extract a substring in the\nadditional context as the answer, and even to reveal\nthe answer hidden in the additional context without\nstring matches. On the contrary, other downstream\nMethod Wiki80 Wiki-ET EntityQuestions\nFine-tuning 86.1 77.5 41.7\n+ General Map-tuning 86.7 76.6 49.0\n+ Task-speciﬁc Map-tuning\nTrain from Scratch 87.2 78.8 57.7\nTrain from the General Map 87.8 78.9 58.9\nTable 2: Results of task-speciﬁc map-tuning. We train\nthe mapping network from scratch or initialize the map-\nping network with the general mapping network.\nmodels are not able to reveal the hidden answer.\nThus, it is worth investigating RA with pluggable\nknowledge modules to stably provide information\nfor different downstream models, rather than di-\nrectly appending unstructured text to model inputs.\n4.3 Task-speciﬁc Plug-and-Play Injection\nSince map-tuning achieves the best performance\nin the general plug-and-play injection setting, we\nfurther evaluate it in the setting of task-speciﬁc\nplug-and-play injection, where we train mapping\nnetworks based on downstream models with task\nobjectives. If we have already conducted general\nmap-tuning on a PLM, we can initialize the net-\nwork with the general mapping network. Other-\nwise, we have to train the network from scratch.\nWe ﬁrst evaluate task-speciﬁc map-tuning on\nWiki80 and Wiki-ET. The results are reported in\nTable 2. From the table, we have two observa-\ntions: (1) Task-speciﬁc map-tuning achieves better\nperformance on these two datasets than general\nmap-tuning does. It indicates that the mapping net-\nwork extracts more informative knowledge for the\nspeciﬁc task by task-speciﬁc training than the gen-\neral one does. (2) If the general mapping network\nis available, it is recommended to use it to initialize\nthe mapping network, which further improves the\nmodel performance.\nThen, we evaluate task-speciﬁc map-tuning in\ndomain adaptation, which is a more challenging\nsetting. In this setting, we aim to plug multiple\nknowledge bases into a single downstream model.\nSpeciﬁcally, a downstream model is trained on a\nsource domain, and then we plug the knowledge\nmodules of the target domain into it for domain\nadaptation. Here, we use the relation classiﬁcation\ndatasets on the Wikipedia domain (FewRel 1.0)\nand the PubMed domain (FewRel 2.0). FewRel\n1.0 is the source domain. FewRel 2.0 is the target\ndomain. The knowledge base for FewRel 2.0 is\nUMLS. Since the original FewRel 2.0 does not\nprovide training instances, we rearrange FewRel\n2.0 and have the following data split. As FewRel\n2.0 has 25 relations, we separate 15 relations for\ntraining and development and the rest 10 relations\nare used for testing.\nFrom Table 3, we have two observations: (1) For\nthe domain adaptation from Wikipedia to PubMed,\nmap-tuning signiﬁcantly improves the model per-\nformance (e.g., from 76.7 to 81.2 in 5-1) and\nachieves better performance than the model ﬁne-\ntuned on PubMed domain (e.g., from 78.6 to 81.2\nin 5-1). It suggests that it is promising to use map-\ntuning to introduce external knowledge for domain\nadaptation. (2) Multi-domain training degrades\nthe model performance on the Wikipedia domain\nand maintains its performance on the PubMed do-\nmain while map-tuning does not degrade the per-\nformance on each domain. It indicates that the\npluggable mapping networks are suitable for con-\ntinual domain adaptation.\n4.4 Computational Efﬁciency\nWe compare our proposed plug-and-play knowl-\nedge injection paradigm with previous knowledge\ninjection paradigms on the time cost. We evalu-\nate the training time on an NVIDIA Tesla V100\nand compare the model performance on the 10-\nway 1-shot setting of FewRel 1.0. ERNIE (Zhang\net al., 2019), KEPLER (Wang et al., 2021b), and\n101\n102\n103\nGPU Hours for Knowledge Injection\n82\n84\n86\n88Accuracy\nPELT\nMap-tuning\nERNIE\nKEPLER\nLUKE\nBERT\nFigure 3: Time cost of different knowledge injection\nmethods on an NVIDIA Tesla V100 GPU.\nLUKE (Yamada et al., 2020) inject knowledge\nduring pre-training. PELT (Ye et al., 2022) in-\njects knowledge during ﬁne-tuning. The results\nof ERNIE, KEPLER, LUKE, and PELT are taken\nfrom Ye et al. (2022). Map-tuning injects knowl-\nedge after ﬁne-tuning.\nThe results are shown in Figure 3. From this\nﬁgure, we observe that the training time of map-\ntuning is much shorter than those methods under\nthe paradigm of injecting during pre-training, and\nit is comparable to PELT. Besides, the performance\nof map-tuning is also competitive compared to pre-\nvious knowledge injection methods. Moreover,\nmap-tuning only optimizes additional 0.1% of pa-\nrameters and we report the number of parameters\noptimized for different knowledge injection meth-\nods in Appendix G. Plug-and-play knowledge in-\njection has great potential to be comparable to pre-\nvious paradigms w.r.t. task performance, while\nmaintaining its innate ﬂexibility and efﬁciency.\n4.5 Case Study\nWe present a qualitative analysis of map-tuning in\nTable 4. In the ﬁrst case, the original downstream\nmodel does not understand that “ﬂying ofﬁcer” is a\nmilitary rank and wrongly predicts the relation as\n“occupation”. With the general mapping network,\nwhich enriches the meaning of “ﬂying ofﬁcer”, the\nmodel correctly predicts the relation.\nThe general mapping network, however, may be\nmisleading in some cases. In the second case, it is\neasy for the original downstream model to recog-\nnize “Wasp” as a member of “Avengers” without\nany external knowledge since this fact could be\ninferred by the word “other”. Compared to the\nexternal knowledge provided by the task-speciﬁc\nmapping network, coarse-grained is that provided\nby the general mapping network, because there is\nno additional training before the inference. As a\nresult, the model wrongly recognizes “Avengers”\nas comic books instead of the ﬁctional superhero\nTraining Data Map-tuning Source Domain Target Domain\n5-1 5-5 10-1 10-5 5-1 5-5 10-1 10-5\nTarget Domain − 65.4 80.8 56.9 73.8 78.6 88.6 71.4 79.7\nMultiple Domains − 90.3 94.6 84.9 90.4 84.8 92.0 79.0 86.8\nSource Domain − 91.0 95.1 85.4 90.8 76.7 88.2 69.1 81.5\n✓ 92.9 95.6 88.2 91.1 81.2 89.8 72.6 83.3\nTable 3: Results of domain adaptation. The source domain is Wikipedia from FewRel 1.0. The target domain\nis PubMed from FewRel 2.0. We compare task-speciﬁc map-tuning with ﬁne-tuning on the target domain and\nmultiple domains consisting of both source and target domains.\nInput True label Injection Predicted label Logits\nErnest Russell Lyon was a\n::::ﬂying:::::ofﬁcer in 234 Squadron\nof the Royal Air Force\nduring the Second World War.\nmilitary_rank\n- occupation, military_rank,\nﬁeld_of_work 8.0, 4.7, 3.3\nGeneral military_rank, ﬁeld_of_work,\noccupation 6.3, 6.2, 3.9Map-tuning\nHe later enslaved Thor, then\ncaptured the Wasp and the other\n:::::::Avengers.\nmember_of\n- member_of, parts,\ncharacters 8.8, 5.0, 4.4\nGeneral characters, member_of,\nparts 6.9, 6.6, 4.7Map-tuning\nTask-specifc member_of, parts,\ncharacters 8.4, 5.7, 4.6Map-tuning\nTable 4: A case study for map-tuning on Wiki80. Underlinesand ::::wave::::lines highlight head entities and tail entities\nrespectively. We report the top 3 ranked predictions of different methods.\nteam, and thus changes the correct model predic-\ntion. Task-speciﬁc map-tuning, which is further\nadapted to the task, corrects the prediction.\n5 Related Work\nTo enhance PLMs with external knowledge, there\nare two mainstream paradigms: injection during\npre-training and injection during ﬁne-tuning (Yin\net al., 2022). For injection during pre-training, re-\nsearchers usually construct new knowledge-aware\nobjectives, such as entity prediction (Xu et al.,\n2021), entity discrimination (Xiong et al., 2020),\nentity and relation discrimination (Qin et al., 2021),\nand link prediction (Wang et al., 2021b). In this\nway, knowledge will be implicitly stored in the pa-\nrameters of PLMs. Injection knowledge during pre-\ntraining can simultaneously improve performance\non a range of downstream knowledge-driven tasks.\nHowever, the training cost of this paradigm is ex-\npensive. Taking the typical knowledge-enhanced\nPLMs LUKE (Yamada et al., 2020) and KE-\nPLER (Wang et al., 2021b) as an example, it takes\nmore than 3,000 GPU hours to train them.\nInjection knowledge during ﬁne-tuning is a rela-\ntively lightweight paradigm, where external knowl-\nedge is often used to augment model inputs for\nspeciﬁc tasks (Zhou et al., 2019; Lin et al., 2019;\nLiu et al., 2020b; Cheng et al., 2021; Kang et al.,\n2022). When injecting unstructured textual knowl-\nedge, some methods retrieve task-related informa-\ntion from external corpora to augment the origi-\nnal input text (Karpukhin et al., 2020; Liu et al.,\n2020a). When using structured knowledge, such as\nknowledge graphs, existing methods usually apply\nknowledge representation learning methods (Bor-\ndes et al., 2013; Lin et al., 2015) to encode struc-\ntured knowledge into embeddings, and then fuse\nthese knowledge embeddings with input token em-\nbeddings using knowledge injection methods (Sun\net al., 2020; Su et al., 2021; Yasunaga et al., 2021).\nIn general, existing knowledge injection meth-\nods mainly target PLMs and adopt paradigms\nwhere knowledge and models are highly coupled.\nToward ﬂexible and efﬁcient injection, we study a\nnew paradigm, plug-and-play knowledge injection,\nwhere we decouple models and knowledge sources,\nand then inject knowledge into downstream models\nwithout retraining the models. This work is also re-\nlated to parameter-efﬁcient tuning (Liu et al., 2021;\nDing et al., 2022) and plugins for large language\nmodels (Xiao et al., 2023; Dathathri et al., 2020;\nLauscher et al., 2021; Chronopoulou et al., 2022;\nYu et al., 2023; Xu et al., 2023; Alayrac et al., 2022)\nwhile we are the ﬁrst to study knowledge injection\nin a parameter-efﬁcient and plug-and-play way.\n6 Conclusion\nIn this work, we propose a new paradigm of in-\njection toward ﬂexible and efﬁcient knowledge in-\njection. In this paradigm, downstream models can\nbe enhanced with little computational cost, which\nbeneﬁts large amounts of models. We ﬁrst sys-\ntematically evaluate existing knowledge injection\nmethods and ﬁnd that they are not suitable for plug-\nand-play injection. Then, we propose map-tuning\nfor this paradigm, which effectively injects knowl-\nedge into downstream models to enhance them.\nThere are four promising directions for future\ninvestigation into plug-and-play knowledge injec-\ntion. (1) How can we reduce the performance gap\nbetween methods for this novel paradigm and those\nfor the previous injection paradigms, while main-\ntaining superior ﬂexibility and efﬁciency? (2) Be-\nsides factual knowledge, how can we effectively\nplug diverse knowledge bases, such as text corpora,\nvoice, images, and even other PLMs? (3) After\ninjecting the knowledge in a plug-and-play way,\nhow can the PLMs do various types of complex\nreasoning based on the injected knowledge (Onoe\net al., 2023)? (4) Can the plug-and-play knowledge\ninjection methods for these sources be uniﬁed, so\nwe can plug a combination of multiple sources? We\nhope this work can attract attention to and inspire\nresearch on these problems.\nLimitations\nIn this paper, we present a novel knowledge injec-\ntion paradigm plug-and-play knowledge injection\nfor PLMs. We show existing methods can not be\nwell applied to the new paradigm and proposemap-\ntuning as a preliminary exploration of methods.\nThe paradigm plug-and-play knowledge injec-\ntion has a limitation in terms of its assumption. It\nassumes that a PLM should be ﬁne-tuned for down-\nstream tasks. However, very large-scale PLMs can\nperform zero-shot learning or in-context learning\non downstream tasks without being ﬁne-tuned. Fu-\nture work may extend the deﬁnition of the proposed\nparadigm to make it meaningful in these scenes.\nThe method map-tuning has three limitations in\nterms of its applicability. Firstly, we did not evalu-\nate map-tuning for PLMs pre-trained by other lan-\nguage modeling objectives (e.g., casual language\nmodeling) besides MLM. As its spirit can be easily\ngeneralized to various language modeling objec-\ntives, we leave this evaluation as future work. Sec-\nondly, we did not evaluate whether the PLM can\ndo complex reasoning (e.g., multi-hop reasoning)\nbased on the knowledge injected by map-tuning.\nThirdly, map-tuning is designed to plug structural\nfact knowledge. It is also meaningful to plug other\ndiverse knowledge bases, including text corpora,\nvoice, images, and even other PLMs, which are not\ncovered by our work.\nAcknowledgments\nThis work is supported by the National Key\nR&D Program of China (No.2022ZD0116312), Na-\ntional Natural Science Foundation of China (No.\n62236004).\nAuthor Contributions Zhengyan Zhang,\nZhiyuan Zeng, Huadong Wang, and Deming Ye\nwrote the code and conducted the experiments.\nZhengyan Zhang constructed the basic experi-\nmental framework including codes and datasets.\nZhiyuan Zeng was in charge of plug-and-play and\nﬁne-tuning experiments. Huadong Wang and Dem-\ning Ye provided TransE and PELT embeddings\nrespectively. Zhengyan Zhang and Zhiyuan Zeng\ncontributed to the analysis experiments. Zhengyan\nZhang and Zhiyuan Zeng wrote the initial draft.\nYankai Lin, Huadong Wang, Chaojun Xiao, Xu\nHan, and Zhiyuan Liu signiﬁcantly edited and\nimproved the paper. Peng Li, Maosong Sun, and\nJie Zhou provided valuable advice to the research.\nReferences\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, Roman Ring, Eliza Rutherford, Serkan\nCabi, Tengda Han, Zhitao Gong, Sina Samangooei,\nMarianne Monteiro, Jacob L. Menick, Sebastian\nBorgeaud, Andy Brock, Aida Nematzadeh, Sahand\nSharifzadeh, Mikolaj Binkowski, Ricardo Barreira,\nOriol Vinyals, Andrew Zisserman, and Karén Si-\nmonyan. 2022. Flamingo: a visual language model\nfor few-shot learning. In Proceedings of NeurIPS.\nAntoine Bordes, Nicolas Usunier, Alberto García-\nDurán, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Proceedings of NeurIPS , pages\n2787–2795.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Asli Celikyilmaz, and Yejin Choi.\n2019. COMET: commonsense transformers for au-\ntomatic knowledge graph construction. In Proceed-\nings of ACL, pages 4762–4779.\nWenhu Chen, Yu Su, Xifeng Yan, and William Yang\nWang. 2020. KGPT: knowledge-grounded pre-\ntraining for data-to-text generation. In Proceedings\nof EMNLP, pages 8635–8648.\nHao Cheng, Yelong Shen, Xiaodong Liu, Pengcheng\nHe, Weizhu Chen, and Jianfeng Gao. 2021. Unit-\nedqa: A hybrid approach for open domain question\nanswering. In Proceedings of ACL , pages 3080–\n3090.\nAlexandra Chronopoulou, Matthew E. Peters, and\nJesse Dodge. 2022. Efﬁcient hierarchical domain\nadaptation for pretrained language models. In Pro-\nceedings of NAACL-HLT, pages 1336–1351.\nSumanth Dathathri, Andrea Madotto, Janice Lan, Jane\nHung, Eric Frank, Piero Molino, Jason Yosinski, and\nRosanne Liu. 2020. Plug and play language models:\nA simple approach to controlled text generation. In\nProceedings of ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL-HLT , pages\n4171–4186.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, Jing Yi, Weilin Zhao,\nXiaozhi Wang, Zhiyuan Liu, Hai-Tao Zheng, Jianfei\nChen, Yang Liu, Jie Tang, Juanzi Li, and Maosong\nSun. 2022. Delta tuning: A comprehensive study of\nparameter efﬁcient methods for pre-trained language\nmodels. arXiv preprint 2203.06904.\nTianyu Gao, Xu Han, Yuzhuo Bai, Keyue Qiu, Zhiyu\nXie, Yankai Lin, Zhiyuan Liu, Peng Li, Maosong\nSun, and Jie Zhou. 2021. Manual evaluation matters:\nReviewing test protocols of distantly supervised re-\nlation extraction. In Findings of ACL/IJCNLP 2021,\npages 1306–1318.\nTianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng Li,\nMaosong Sun, and Jie Zhou. 2019. Fewrel 2.0: To-\nwards more challenging few-shot relation classiﬁca-\ntion. In Proceedings of EMNLP, pages 6249–6254.\nJian Guan, Fei Huang, Minlie Huang, Zhihao Zhao,\nand Xiaoyan Zhu. 2020. A knowledge-enhanced\npretraining model for commonsense story genera-\ntion. TACL, 8:93–108.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. REALM: retrieval-\naugmented language model pre-training. arXiv\npreprint 2002.08909.\nXu Han, Tianyu Gao, Yuan Yao, Deming Ye, Zhiyuan\nLiu, and Maosong Sun. 2019. OpenNRE: An open\nand extensible toolkit for neural relation extraction.\nIn Proceedings of EMNLP-IJCNLP, pages 169–174.\nXu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan\nYao, Zhiyuan Liu, and Maosong Sun. 2018. Fewrel:\nA large-scale supervised few-shot relation classiﬁca-\ntion dataset with state-of-the-art evaluation. In Pro-\nceedings of EMNLP, pages 4803–4809.\nGeoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky,\nIlya Sutskever, and Ruslan R Salakhutdinov. 2012.\nImproving neural networks by preventing co-\nadaptation of feature detectors. arXiv preprint\narXiv:1207.0580.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of ICML, pages 2790–2799.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of ACL, pages 328–339.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large lan-\nguage models. arXiv preprint 2106.09685.\nMinki Kang, Jinheon Baek, and Sung Ju Hwang.\n2022. KALA: knowledge-augmented language\nmodel adaptation. In Proceedings of NAACL-HLT,\npages 5144–5167.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi\nChen, and Wen-tau Yih. 2020. Dense passage re-\ntrieval for open-domain question answering. In Pro-\nceedings of EMNLP, pages 6769–6781.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\nAnne Lauscher, Tobias Lüken, and Goran Glavas. 2021.\nSustainable modular debiasing of language models.\nIn Findings of ACL: EMNLP, pages 4782–4797.\nYoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan\nPadnos, Or Sharir, Shai Shalev-Shwartz, Amnon\nShashua, and Yoav Shoham. 2020. Sensebert: Driv-\ning some sense into BERT. In Proceedings of ACL,\npages 4656–4667.\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Pik-\ntus, Fabio Petroni, Vladimir Karpukhin, Naman\nGoyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih,\nTim Rocktäschel, Sebastian Riedel, and Douwe\nKiela. 2020. Retrieval-augmented generation for\nknowledge-intensive NLP tasks. In Proceedings of\nNeurIPS.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang\nRen. 2019. Kagnet: Knowledge-aware graph net-\nworks for commonsense reasoning. In Proceedings\nof EMNLP, pages 2829–2839.\nYankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and\nXuan Zhu. 2015. Learning entity and relation em-\nbeddings for knowledge graph completion. In Pro-\nceedings of AAAI, pages 2181–2187.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020a. K-BERT:\nenabling language representation with knowledge\ngraph. In Proceedings of AAAI, pages 2901–2908.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZhenghao Liu, Chenyan Xiong, Maosong Sun, and\nZhiyuan Liu. 2020b. Fine-grained fact veriﬁcation\nwith kernel graph attention network. In Proceedings\nof ACL, pages 7342–7351.\nYasumasa Onoe, Michael J. Q. Zhang, Shankar Pad-\nmanabhan, Greg Durrett, and Eunsol Choi. 2023.\nCan lms learn new entities from descriptions? chal-\nlenges in propagating injected knowledge. arXiv\npreprint arXiv:2305.01651.\nMatthew E. Peters, Mark Neumann, Robert L. Logan\nIV , Roy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge enhanced con-\ntextual word representations. In Proceedings of\nEMNLP-IJCNLP, pages 43–54.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2020. E-BERT: Efﬁcient-yet-effective entity embed-\ndings for BERT. In Findings of EMNLP, pages 803–\n818.\nYujia Qin, Yankai Lin, Ryuichi Takanobu, Zhiyuan Liu,\nPeng Li, Heng Ji, Minlie Huang, Maosong Sun, and\nJie Zhou. 2021. ERICA: Improving entity and rela-\ntion understanding for pre-trained language models\nvia contrastive learning. In Proceedings of ACL.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog.\nChristopher Sciavolino, Zexuan Zhong, Jinhyuk Lee,\nand Danqi Chen. 2021. Simple entity-centric ques-\ntions challenge dense retrievers. In Proceedings of\nEMNLP, pages 6138–6148.\nYusheng Su, Xu Han, Zhengyan Zhang, Yankai Lin,\nPeng Li, Zhiyuan Liu, Jie Zhou, and Maosong Sun.\n2021. CokeBERT: Contextual knowledge selection\nand embedding towards enhanced pre-trained lan-\nguage models. AI Open, 2:127–134.\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo,\nYaru Hu, Xuanjing Huang, and Zheng Zhang. 2020.\nCoLAKE: Contextualized language and knowledge\nembedding. In Proceedings of COLING , pages\n3660–3670.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei, Xu-\nanjing Huang, Jianshu Ji, Guihong Cao, Daxin Jiang,\nand Ming Zhou. 2021a. K-adapter: Infusing knowl-\nedge into pre-trained models with adapters. In Find-\nings of ACL/IJCNLP, pages 1405–1418.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang.\n2021b. KEPLER: A uniﬁed model for knowledge\nembedding and pre-trained language representation.\nTACL, 9:176–194.\nChaojun Xiao, Zhengyan Zhang, Xu Han, Chi-Min\nChan, Yankai Lin, Zhiyuan Liu, Xiangyang Li,\nZhonghua Li, Zhao Cao, and Maosong Sun. 2023.\nPlug-and-play document modules for pre-trained\nmodels. In Proceedings of ACL.\nJi Xin, Yankai Lin, Zhiyuan Liu, and Maosong Sun.\n2018. Improving neural ﬁne-grained entity typing\nwith knowledge attention. In Proceedings of AAAI,\npages 5997–6004.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In Proceedings of ICLR.\nCanwen Xu, Yichong Xu, Shuohang Wang, Yang Liu,\nChenguang Zhu, and Julian McAuley. 2023. Small\nmodels are valuable plug-ins for large language mod-\nels. arXiv preprint 2305.08848.\nSong Xu, Haoran Li, Peng Yuan, Yujia Wang,\nYouzheng Wu, Xiaodong He, Ying Liu, and Bowen\nZhou. 2021. K-PLUG: knowledge-injected pre-\ntrained language model for natural language under-\nstanding and generation in e-commerce. In Findings\nof EMNLP, pages 1–17.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: deep\ncontextualized entity representations with entity-\naware self-attention. In Proceedings of EMNLP ,\npages 6442–6454.\nMichihiro Yasunaga, Hongyu Ren, Antoine Bosse-\nlut, Percy Liang, and Jure Leskovec. 2021. QA-\nGNN: reasoning with language models and knowl-\nedge graphs for question answering. In Proceedings\nof NAACL-HLT, pages 535–546.\nDeming Ye, Yankai Lin, Peng Li, Maosong Sun, and\nZhiyuan Liu. 2022. A simple but effective pluggable\nentity lookup table for pre-trained language models.\nIn Proceedings of ACL.\nDa Yin, Li Dong, Hao Cheng, Xiaodong Liu, Kai-Wei\nChang, Furu Wei, and Jianfeng Gao. 2022. A sur-\nvey of knowledge-intensive nlp with pre-trained lan-\nguage models. arXiv preprint arXiv:2202.08772.\nZichun Yu, Chenyan Xiong, Shi Yu, and Zhiyuan Liu.\n2023. Augmentation-adapted retriever improves\ngeneralization of language models as a zero-shot\nplug-in. In Proceedings of ACL.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021. Bitﬁt: Simple parameter-efﬁcient\nﬁne-tuning for transformer-based masked language-\nmodels. arXiv preprint 2106.10199.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: en-\nhanced language representation with informative en-\ntities. In Proceedings of ACL, pages 1441–1451.\nJie Zhou, Xu Han, Cheng Yang, Zhiyuan Liu, Lifeng\nWang, Changcheng Li, and Maosong Sun. 2019.\nGEAR: graph-based evidence aggregating and rea-\nsoning for fact veriﬁcation. In Proceedings of ACL,\npages 892–901.\nJunru Zhou, Zhuosheng Zhang, Hai Zhao, and Shuail-\niang Zhang. 2020. LIMIT-BERT : Linguistics in-\nformed multi-task BERT. In Findings EMNLP ,\npages 4450–4461.\nMethod Hyper-parameters FewRel Wiki80 Wiki-ET EntityQuestions\n- Sequence Length 512 128 64 64\nFine-tuning\nLearning Rate 2E-5 5E-5 1E-5 1E-4\nBatch Size 4 32 64 64\nTraining Step/Epoch 3000 15 2 5\nLoRA\nLearning Rate 8E-4 2E-3 1E-3 5E-4\nBatch Size 4 64 64 64\nTraining Step/Epoch 3000 60 2 5\nRank 32 32 4 4\nAdapter\nLearning Rate 5E-4 2E-3 1E-3 5E-4\nBatch Size 4 64 64 64\nTraining Step/Epoch 3000 60 2 5\nHidden Size 32 32 32 32\nBitFit\nLearning Rate 8E-4 2E-3 1E-3 5E-4\nBatch Size 4 64 64 64\nTraining Step/Epoch 3000 60 2 5\nTable 5: Hyper-parameters for four training methods.\nWe report the training steps of FewRel and the training\nepochs of Wiki80 and Wiki-ET.\nMethod FewRel Wiki80 Wiki-ET EntityQuestions\nFine-tuning Dropout 0.25 0.25 0.25 0.35\nEpoch 3 5 3 3\nLoRA Dropout 0.35 0.25 0.35 0.25\nEpoch 5 5 4 5\nAdapter Dropout 0.35 0.35 0.15 0.25\nEpoch 5 5 5 5\nBitFit Dropout 0.25 0.35 0.35 0.35\nEpoch 5 4 4 5\nTable 6: Hyper-parameters for general map-tuning.\nA Hyper-parameters\nA.1 Fine-tuning downstream PLMs\nWe experiment with four training methods for the\nadaptation of PLMs on downstream tasks, which\nare Full-model ﬁne-tuning, LoRA, Adapter, and\nBitFit. The embedding layer is frozen during\ntraining. We train all the models using AdamW\nwith 10% warming-up steps. We list our hyper-\nparameters in Table 5.\nA.2 General Map-tuning\nFor general map-tuning, we search the dropout rate\nin {0.15, 0.25, 0.35, 0.45}. We train all the map-\nping networks using Adam (Kingma and Ba, 2015).\nThe learning rate is 3E-5 and the batch size is 64.\nWe train the mapping network on the Wikipedia\ncorpus for 5 epochs. The hyper-parameters of the\nbest mapping network in all cases are listed in Ta-\nble 6. When we evaluate RA on these datasets, we\nset the sequence length to 512.\nA.3 Task-specifc Map-tuning\nWe report hyper-parameters for task-speciﬁc map-\ntuning in Table 7. We train all mapping networks\nusing Adam with 10% warming-up steps\nRegarding the results reported in Table 2, during\ntask-speciﬁc map-tuning, we use dropout in the at-\ntention probabilities and all fully connected layers\nHyper-parameters FewRel Wiki80 Wiki-ET EntityQuestions\nLearning Rate 2E-5 4E-4 5E-5 2E-3\nBatch Size 4 64 128 64\nTraining Step/Epoch 3000 30 2 5\nTable 7: Hyper-parameters for task-speciﬁc map-\ntuning.\nFewRel Wiki80 Wiki-ET EntityQuestions\nTraining Epoch 5 2 2 4\nTable 8: Hyper-parameters for map-tuning on the\nWikipedia corpus, after which we ﬁne-tune BERT on\ndownstream tasks with the mapping network plugged.\nof the PLM. The dropout rate is 0.30, 0.20, and\n0.00 for Wiki80, Wiki-ET, and EntityQuestions,\nrespectively. Regarding the results reported in Ta-\nble 3, when using training data from the source\ndomain for task-speciﬁc map-tuning, the dropout\nrate is 0.35. In these cases, the training data for\ntask-speciﬁc map-tuning are identical to those for\nﬁne-tuning the downstream models. We search\nthe dropout rate in {0.00, 0.15, 0.20, 0.25, 0.30,\n0.35}. When using training data from the target\ndomain for task-speciﬁc map-tuning, we do not use\ndropout.\nThe hyper-parameters for experiments with\nRoBERTa are identical to those with BERT.\nA.4 Fine-tuning with the Mapping Network\nRegarding the results reported in Table 14, the\nhyper-parameters for ﬁne-tuning BERT are iden-\ntical to those in Table 5. We train all mapping\nnetworks using Adam without dropout, and the\nbatch size is 64. For map-tuning on the Wikipedia\ncorpus, the learning rate is 1E-5. We report other\nhyper-parameters for map-tuning on the Wikipedia\ncorpus in Table 8, and those for map-tuning on\ndownstream data in Table 9.\nA.5 Details of K-Adapter\nWe use the open-source implementation of K-\nAdapter7, and we only consider facAdapter (Fac-\ntual Adapter). The BERTbase layers where adapter\nlayers plug in are {5, 10}. The hyper-parameters\nfor pre-training facAdapter are identical to those\nreported in Wang et al. (2021a).\nIn order to plug K-Adapter into frozen down-\nstream models in the setting of general plug-and-\nplay injection, we tune the ﬁnal fully connected\nlayer on downstream data. We use Adam with 10%\n7https://github.com/microsoft/k-adapter\nFewRel Wiki80 Wiki-ET EntityQuestions\nLearning Rate 2E-4 2E-4 1E-5 2E-4\nTraining Epoch 3 12 2 2\nTable 9: Hyper-parameters for map-tuning on down-\nstream data, after which we ﬁne-tune BERT on down-\nstream tasks with the mapping network plugged.\nFewRel Wiki80 Wiki-ET EntityQuestions\nLearning Rate 2E-5 5E-5 5E-5 5E-3\nBacth Size 4 32 64 64\nTraining Step/Epoch 3000 15 2 20\nTable 10: Hyper-parameters for tuning the ﬁnal fully\nconnected layer, during which we plug frozen K-\nAdapter into frozen downstream models.\nwarming-up steps, and other hyper-parameters are\nlisted in Table 10.\nA.6 Details of Data Preprocessing\nFor FewRel and Wiki80, we mark the subject and\nobject spans by # and $ tokens respectively. For\nWikiET and EntityQuestions, we mark the entity\nspan by $ token.\nTo evaluate encoder PLMs on EntityQuestions,\nwe append the [MASK] token to the question, and\nonly keep the instances whose answers are in the\nPLM token vocabulary. We train the model to ﬁll in\nthe [MASK] token. It is a classiﬁcation task, where\nall tokens in the vocabulary are choices. Only when\nthe answer token is ranked as the top 1 result is the\nmodel considered to give a correct prediction. We\nfurther remove the instances whose entity is not\nin the database. Finally, we have 37800 training\ninstances, 4693 validation instances, and 4731 test\ninstances.\nFewRel, Wiki80, and WikiET provide the anno-\ntation of entity linking, and for EntityQuestions we\ndo entity liking by string matching.\nB Stability of Map-tuning\nWe evaluate the stability of map-tuning in general\nplug-and-play knowledge injection. Training the\nPLMs on downstream tasks with three different\nseeds (one of which is used in all main experi-\nments), for each task, we have three different down-\nstream models, into which we plug the mapping\nnetwork. The mean and standard deviation of per-\nformance improvement brought by map-tuning is\nshown in Table 11. From this table, we observe that\nmap-tuning is not sensitive to downstream models\noverall, showing its decent stability.\nMethod FewRel 1.0 Wiki80 Wiki-ET EntityQuestions5-1 5-5 10-1 10-5\nFine-tuning 1.300±0.300 0.800 ±0.436 2.033 ±0.577 0.533 ±0.231 0.600±0.200 −0.567±0.306 6.967±0.850\nLoRA 1.633±0.153 0.833 ±0.115 2.800 ±0.361 0.833 ±0.115 0.600±0.100 1.000±0.200 7.000±0.173\nAdapter 1.367±0.058 0.733 ±0.115 2.067 ±0.208 0.833 ±0.153 0.267±0.306 1.100±0.529 6.967±0.252\nBitFit 1.367±0.208 0.500 ±0.265 2.333 ±0.153 0.867 ±0.058 0.700±0.300 0.700±0.173 7.233±0.153\nTable 11: The mean and standard deviation of performance improvement brought by map-tuning. We train PLMs\non each downstream task with three different seeds.\nTraining Data Map 5-1 5-5 10-1 10-5\nTarget Domain − 81.9 91.0 74.2 84.0\nMultiple Domains − 80.9 92.2 75.4 87.8\nSource Domain − 72.5 89.2 65.2 83.3\n! 91.6 96.6 88.1 94.5\nTable 12: Results of domain adaptation using\nRoBERTa. We report the performance on the target do-\nmain.\nC How Map-tuning Works with Other\nPLMs?\nIn this section, we experiment map-tuning with\nRoBERTa (Liu et al., 2019), another representative\nPLM, on the domain transfer setting using task-\nspeciﬁc map-tuning. The setting is identical to that\nin Section 4.3. The results are shown in Table 12.\nFrom this table, we observe that task-speciﬁc map-\ntuning signiﬁcantly improves the performance of\nthe model trained on the source domain by intro-\nducing the knowledge of the target domain. More-\nover, the model plugged with map-tuning is even\nmuch better than the model trained on multiple do-\nmains. It indicates that map-tuning is a universal\nknowledge injection method for different PLMs.\nD Empirical Analysis of MMLM\nWe conduct an empirical analysis of what MMLM\ntrains the mapping network to learn. Concretely,\nwe split the general map-tuning corpus into a train-\ning set and a test set. During training on the training\nset, we plug M(e1) and M(e2) before two entity\nmentions e1 and e2 for each instance, and mask\nonly the mention span of e1. During inference on\nthe test set, we evaluate the MMLM loss in four\nsettings. (1) No-Perturbation plugs the M(e1)\nand M(e2), which is identical to the setting of\ntraining. (2) Self-Perturbation replaces M(e1)\nwith M(ei), where ei is a random entity. (3)\nOther-Perturbation replaces M(e2) with M(ei).\n(4) All-Perturbation replaces both M(e1) and\nM(e2) with random ones. We also evaluate these\nMap-TuningEvaluation SettingLoss on Test Set\n− No-Plug 7.246\n!\nNo-Perturbation 5.316\nSelf-Perturbation 6.347\nOther-Perturbation 5.501\nAll-Perturbation 6.613\n#\nNo-Perturbation 7.179\nSelf-Perturbation 7.237\nOther-Perturbation 7.268\nAll-Perturbation 7.355\nTable 13: The MMLM loss on the test set in different\nevaluation settings.\nsettings with a randomly-initialized mapping net-\nwork without map-tuning. For analysis, we report\nthe result in the setting No-Plug where there is no\nplugged embedding.\nThe result is shown in Table 13. From this ta-\nble, we have three observations. (1) With map-\ntuning, the loss in Self-Perturbation is signiﬁcantly\nlarger than that in No-Perturbation, even close to\nthat in All-Perturbation. It proves that MMLM\ntrains the mapping network to extract the entity\ninformation stored in the knowledge embedding\nso that PLMs can utilize the information. (2) The\nloss in Other-Perturbation is also larger than that\nin No-Perturbation, which indicates that the map-\nping network learns to extract the connections be-\ntween different entities and to feed such informa-\ntion into PLMs. (3) Interestingly, the loss in All-\nPerturbation with map-tuning is smaller than that\nin No-Plug, and the loss in settings without map-\ntuning is close to the latter. The trained mapping\nnetwork may be able to convert an arbitrary knowl-\nedge embedding to an embedding that can activate\nthe PLM’s own memory of some factual knowl-\nedge. In conclusion, the three mentioned abilities\nof mapping networks trained by MMLM enable\nPLMs to know new knowledge or better recall\ntheir own knowledge. Future work may improve\nMMLM to get stronger mapping networks.\nMethod Map-tuning Corpus FewRel 1.0 Wiki80 Wiki-ET EntityQuestions5-1 5-5 10-1 10-5\nFine-tuning − 91.0 95.1 85.4 90.8 86.1 77.5 41.7\n+ E-BERT − 92.3 95.6 87.6 91.4 87.8 79.0 61.3\n+ PELT − 91.2 95.8 86.1 91.6 88.2 79.6 62.9\n+ General Map Wikipedia Corpus 93.7 96.2 89.6 92.4 88.8 79.9 62.9\nDownstream Data 93.2 96.2 88.2 92.0 89.1 81.0 62.0\nTable 14: Results of knowledge injection during ﬁne-tuning. For general map-tuning, we can use the Wikipedia\ncorpus mentioned in the previous section or use the data of downstream tasks.\nE Is Map-tuning Competitive in the\nTraditional Paradigm?\nIt is natural to use the general mapping net-\nwork in the traditional injection during ﬁne-tuning\nparadigm, as the general network essentially builds\nan entity embedding lookup table. We freeze the\nparameters of the mapping network and ﬁne-tune\nthe PLM on downstream tasks, during which we\naugment model inputs with mapped knowledge\nrepresentations. Intuitively, the models learn to ef-\nfectively extract information from mapped knowl-\nedge representations during ﬁne-tuning. Inspired\nby ULMFiT (Howard and Ruder, 2018), we also\nexperiment on the setting where we use the task’s\ntraining data as the corpus for general map-tuning.\nOur results are shown in Table 14.\nFrom this table, we have two observations: (1)\nmap-tuning consistently outperforms E-BERT and\nPELT in the traditional paradigm. Considering that\nE-BERT and map-tuning use the same knowledge\nembedding, we suggest that map-tuning provides\nmore useful knowledge representations for BERT\nthan E-BERT. (2) General map-tuning on down-\nstream data achieves comparable performance to\nthat on the large-scale unsupervised corpus. It indi-\ncates that general map-tuning does not necessitate\na large amount of training data for a speciﬁc task.\nF How do We Ensure the Generality of\nMap-tuning?\nIn the setting of general plug-and-play injection, we\ntrain a general mapping network based on a PLM\nand directly plug it into various downstream mod-\nels during inference. There exists a gap between\nthe general map-tuning procedure and the inference\non downstream tasks, i.e., the PLM used for map-\ntuning is different from downstream models. To re-\nduce this gap, we use dropout (Hinton et al., 2012)\nin the attention probabilities and all fully connected\nlayers of the PLM during general map-tuning. In-\n/uni00000013/uni00000011/uni00000013/uni00000013/uni00000013/uni00000011/uni00000014/uni00000018/uni00000013/uni00000011/uni00000015/uni00000018/uni00000013/uni00000011/uni00000016/uni00000018/uni00000013/uni00000011/uni00000017/uni00000018\n/uni00000027/uni00000055/uni00000052/uni00000053/uni00000052/uni00000058/uni00000057/uni00000003/uni00000035/uni00000044/uni00000057/uni0000004c/uni00000052\n/uni0000001c/uni00000013\n/uni0000001c/uni00000014\n/uni0000001c/uni00000015/uni00000024/uni00000046/uni00000046/uni00000058/uni00000055/uni00000044/uni00000046/uni0000005c\n/uni00000029/uni0000004c/uni00000051/uni00000048/uni00000010/uni00000057/uni00000058/uni00000051/uni0000004c/uni00000051/uni0000004a\n/uni0000002f/uni00000052/uni00000035/uni00000024\n/uni00000024/uni00000047/uni00000044/uni00000053/uni00000057/uni00000048/uni00000055\n/uni00000025/uni0000004c/uni00000057/uni00000029/uni0000004c/uni00000057\nFigure 4: Effect of dropout rates on the performance of\ngeneral map-tuning.\nMap-tuning PELT ERNIE KEPLER LUKE\n0.1M 123M 114M 123M 274M\nTable 15: Number of parameters optimized in knowl-\nedge injection. These methods are based on backbone\nPLMs with around 100 million parameters.\ntuitively, dropout simulates different variants of the\nPLM and makes the mapping network have better\ngenerality for different downstream models trained\nfrom the PLM. We explore ﬁve different dropout\nrates. The results on the 5-way 1-shot of FewRel\n1.0 are chosen as the representative and shown in\nFigure 4.\nFrom this ﬁgure, we have two observations: (1)\nTraining without dropout leads to the worst perfor-\nmance, which indicates that the generality of the\nmapping network is not good enough and down-\nstream models can not utilize the knowledge. (2)\nLarge dropout rates are also not optimal. Empiri-\ncally, the dropout rate of 0.25 is a good choice.\nG Numbers of Optimized Parameters\nCompared to previous knowledge injection meth-\nods, map-tuning is a parameter-efﬁcient method.\nThe numbers of optimized parameters for differ-\nent knowledge injection methods are shown in\nTable 15. In order to introduce external knowl-\nFine-tuning LoRA Adapter BitFit\n- 35.2 36.7 38.1 35.6\nE-BERT 36.9 ( +1.7) 38.4 ( +1.7) 39.2 ( +1.1) 35.8 ( +0.2)\nPELT 38.8 ( +3.6) 40.6 ( +3.9) 41.6 ( +3.5) 38.5 ( +2.9)\nRA 42.7 (+7.5) 29.0 ( −7.7) 25.0 ( −13.1) 17.4 ( −18.2)\nK-Adapter 32.3 ( −2.9) 35.8 ( −0.9) 35.8 ( −2.3) 35.7 ( +0.1)\nMap-tuning 41.9 ( +6.7) 42.8 (+6.1) 44.4 (+6.3) 41.1 (+5.5)\nTable 16: Performance on ﬁltered EntityQuestions.\nedge, previous methods usually optimize all pa-\nrameters during pre-training and ﬁne-tuning while\nmap-tuning only optimizes additional 0.1% of pa-\nrameters and freezes the original model, which\nmakes it ﬂexible to use mapping networks for dif-\nferent inputs with the same models.\nH Performance on EntityQuestions\nWe report the performance on ﬁltered EntityQues-\ntions in Table 16.",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.7572648525238037
    },
    {
      "name": "Computer science",
      "score": 0.5298452377319336
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4604334235191345
    },
    {
      "name": "Natural language processing",
      "score": 0.45875123143196106
    },
    {
      "name": "Computational linguistics",
      "score": 0.41085854172706604
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3866250514984131
    },
    {
      "name": "Programming language",
      "score": 0.38094696402549744
    },
    {
      "name": "Physics",
      "score": 0.14639872312545776
    },
    {
      "name": "History",
      "score": 0.13759014010429382
    },
    {
      "name": "China",
      "score": 0.07404041290283203
    },
    {
      "name": "Archaeology",
      "score": 0.06783807277679443
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}