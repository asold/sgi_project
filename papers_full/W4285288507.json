{
  "title": "Upstream Mitigation Is Not All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models",
  "url": "https://openalex.org/W4285288507",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2529298376",
      "name": "Ryan Steed",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2116994441",
      "name": "Swetasudha Panda",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2097567527",
      "name": "Ari Kobren",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2106020609",
      "name": "Michael Wick",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3095351420",
    "https://openalex.org/W2799159261",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3035591180",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2949678053",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W4294029935",
    "https://openalex.org/W3207830467",
    "https://openalex.org/W2972413484",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W3166961312",
    "https://openalex.org/W3178522238",
    "https://openalex.org/W3119746452",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3210599147",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3013770059",
    "https://openalex.org/W3166727371",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2962990575",
    "https://openalex.org/W3095105395"
  ],
  "abstract": "A few large, homogenous, pre-trained models undergird many machine learning systems — and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier's discriminatory behavior after fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 3524 - 3542\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nUpstream Mitigation IsNotAll You Need: Testing the Bias Transfer\nHypothesis in Pre-Trained Language Models\nRyan Steed\nCarnegie Mellon University\nryansteed@cmu.edu\nSwetasudha Panda, Ari Kobren, Michael Wick\nOracle Labs\n{swetasudha.panda,ari.kobren,michael.wick}@oracle.com\nA few large, homogenous, pre-trained models\nundergird many machine learning systems — and\noften, these models contain harmful stereotypes\nlearned from the internet. We investigate thebias\ntransfer hypothesis: the theory that social biases\n(such as stereotypes) internalized by large language\nmodels during pre-training transfer into harmful\ntask-speciﬁc behavior after ﬁne-tuning. For two\nclassiﬁcation tasks, we ﬁnd that reducing intrin-\nsic bias with controlled interventionsbeforeﬁne-\ntuning does little to mitigate the classiﬁer’s dis-\ncriminatory behaviorafterﬁne-tuning. Regression\nanalysis suggests that downstream disparities are\nbetter explained by biases in the ﬁne-tuning dataset.\nStill, pre-training plays a role: simple alterations to\nco-occurrence rates in the ﬁne-tuning dataset are\nineffective when the model has been pre-trained.\nOur results encourage practitioners to focus more\non dataset quality and context-speciﬁc harms.\n1 Introduction\nLarge language models (LLMs) and other mas-\nsively pre-trained “foundation” models are power-\nful tools for task-speciﬁc machine learning (Bom-\nmasani et al., 2021). Models pre-trained by well-\nresourced organizations can easily adapt to a wide\nvariety of downstream tasks in a process calledﬁne-\ntuning. But massive pre-training datasets and in-\ncreasingly homogeneous model design come with\nwell-known, immediate social risks beyond the ﬁ-\nnancial and environmental costs (Strubell et al.,\n2019; Bender et al., 2021).\nTransformer-based LLMs like BERT and GPT-\n3 contain quantiﬁableintrinsicsocial biases en-\ncoded in their embedding spaces (Goldfarb-Tarrant\net al., 2021). These intrinsic biases are typically\nassociated with representational harms, including\nstereotyping and denigration (Barocas et al., 2017;\nBlodgett et al., 2020; Bender et al., 2021). Sepa-\nrately, many studies document theextrinsicharms\nof the downstream (ﬁne-tuned & task-speciﬁc) ap-\n%DVH\u00030RGHO\u000bH\u0011J\u0011\u00035R%(57D\f\u0003\n3UH\u00107UDLQHG\u00030RGHO\n)LQH\u00107XQHG\u00030RGHO\n3UH\u00107UDLQLQJ\n)LQH\u00107XQLQJ\n3UH\u00107UDLQLQJ&RUSRUD\u000bH\u0011J\u0011\u0003:LNLSHGLD\f\u0003\n7DVN\u00106SHFLILF'DWDVHW\u0003\u000bH\u0011J\u0011\u0003%,26\f\u0003\n6FUXEEHG\u0003RUUH\u0010EDODQFHG\u0003\n(YDOXDWLRQ7HPSODWHV\u0003 0HDVXUH\u0003,QWULQVLF\u0003%LDV\u000bH\u0011J\u0011\u0003SURQRXQ\u0003UDQNLQJ\f\u0003\n0HDVXUH\u0003([WULQVLF\u0003%LDV\u000bH\u0011J\u0011\u0003735\u0003JDS\f\u0003\n3HUWXUEHG\n5DQGRPL]HG\n'H\u0010ELDVHG\n8SVWUHDP\n'RZQVWUHDP\n%LDV\u00037UDQVIHU\nFigure 1: Full pre-training to ﬁne-tuning pipeline, with\nexperimental interventions (green hexagons).\nplications of ﬁne-tuned LLMs, including discrimi-\nnatory medical diagnoses (Zhang et al., 2020), over-\nreliance on binary gender for coreference resolu-\ntion (Cao and Daumé, 2021), the re-inforcement\nof traditional gender roles in part-of-speech tag-\nging (Garimella et al., 2019), toxic text generation\n(Gehman et al., 2020), and censorship of inclu-\nsive language and AA VE (Blodgett and O’Connor,\n2017; Blodgett et al., 2018; Park et al., 2018; Sap\net al., 2019).\nDespite these risks, no research has investigated\nthe extent to which downstream systems inherit\nsocial biases from pre-trained models.1 Many stud-\n1We use the term “bias” to refer to statistical associations\n3524\nies warn that increasing intrinsic bias upstream\nmay lead to an increased risk of downstream harms\n(Bolukbasi et al., 2016; Caliskan et al., 2017). This\nhypothesis, which we call theBias Transfer Hy-\npothesis, holds that stereotypes and other biased\nassociations in a pre-trained model are transferred\nto post-ﬁne-tuning downstream tasks, where they\ncan cause further, task-speciﬁc harms. A weaker\nversion of this hypothesis holds that downstream\nharms are at least mostly determined by the pre-\ntrained model (Bommasani et al., 2021).\nIn the pre-training paradigm, the extent to which\nthe bias transfer hypothesis holds will determine\nthe most effective strategies for responsible design.\nIn the cases we study, reducing upstream bias does\nlittle to change downstream behavior. Still, there is\nhope: instead, developers can carefully curate the\nﬁne-tuning dataset, checking for harms in context.\nWe test the bias transfer hypothesis on two classi-\nﬁcation tasks with previously demonstrated perfor-\nmance disparities: occupation classiﬁcation of on-\nline biographies (De-Arteaga et al., 2019) and tox-\nicity classiﬁcation of Wikipedia Talks comments\n(Dixon et al., 2018). We investigate whether re-\nducing or exacerbating intrinsic biases encoded by\nRoBERTa (Liu et al., 2019) decreases or increases\nthe severity of downstream, extrinsic harms (Fig-\nure 1). We ﬁnd that the bias transfer hypothesis\ndescribes only part of the interplay between pre-\ntraining biases and harms after ﬁne-tuning:\n• Systematically manipulating upstream bias has\nlittle impact on downstream disparity, especially\nfor the most-harmed groups.\n• With a regression analysis, we ﬁnd that most\nvariation in downstream bias can be explained\nby bias in the ﬁne-tuning dataset (proxied by co-\noccurrence rates).\n• Altering associations in the ﬁne-tuning dataset\ncan sometimes change downstream behavior, but\nonly when the model is not pre-trained.\nWithout absolving LLMs or their owners of repre-\nsentational harms intrinsic to pre-trained models,\nour results encourage practitioners and application\nstakeholders to focus more on dataset quality and\ncontext-speciﬁc harm identiﬁcation and reduction.\nthat result in representational or allocational harms to histori-\ncally marginalized social groups (Blodgett et al., 2020).\n2 Related Work\nLittle prior work directly tests the bias transfer\nhypothesis. The closest example of this phenom-\nena is the “blood diamond” effect (Birhane and\nPrabhu, 2021), in which stereotyping and deni-\ngration in the pre-training corpora pervade sub-\nsequently generated images and language even\nbeforethe ﬁne-tuning stage (Steed and Caliskan,\n2021). Still, it is unclear to what extent unde-\nsirable values encoded in pre-training datasets or\nbenchmarks—such as Wikipedia or ImageNet—\ninduce task-speciﬁc harmsafterﬁne-tuning (Baro-\ncas et al., 2019).\nSome work explores the consistency of intrin-\nsic and extrinsic bias metrics:Goldfarb-Tarrant\net al.(2021) ﬁnd that intrinsic and extrinsic met-\nrics are not reliably correlated for static embed-\ndings likeword2vec. We focus instead on state-\nof-the-art transformer-based LLMs—the subject\nof intense ethical debate (Bender et al., 2021;\nBommasani et al., 2021)—which construct con-\ntextual, rather than static, embeddings. Contextual\nembeddings—token encodings that are conditional\non other, nearby tokens—pose an ongoing chal-\nlenge for intrinsic bias measurement (May et al.,\n2019; Kurita et al., 2019; Guo and Caliskan, 2021)\nand bias mitigation (Liang et al., 2020). We ﬁnd\nthat intrinsic and extrinsic metricsarecorrelated for\nthe typical LLM—but that the correlation is mostly\nexplained by biases in the ﬁne-tuning dataset.\nOther research tests the possibility that upstream\nmitigation could universally prevent downstream\nharm. Jin et al.(2021) show that an intermedi-\nate, bias-mitigating ﬁne-tuning step can help re-\nduce bias in later tasks. Likewise,Solaiman and\nDennison(2021) propose ﬁne-tuning on carefully\ncurated “values-targeted” datasets to reduce toxic\nGPT-3 behavior. Our results tend to corroborate\nthese methods: we ﬁnd that the ﬁne-tuning process\ncan to some extent overwrite the biases present in\nthe original pre-trained model. A recentpost-hoc\nmitigation technique, on the other hand, debiases\ncontextual embeddings before ﬁne-tuning (Liang\net al., 2020). Our results imply that while this type\nof debiasing may help with representational harms\nupstream, it is less successful for reducing harms\ndownstream.\n3 Methods\nTo empirically evaluate the bias transfer hypothe-\nsis, we examine the relationship between upstream\n3525\nbias and downstream bias for two tasks. We track\nhow this relationship changes under various con-\ntrolled interventions on the model weights or the\nﬁne-tuning dataset.\n3.1 Model\nFor each task, we ﬁne-tune RoBERTa2 (Liu et al.,\n2019). We split the ﬁne-tuning dataset into train\n(80%), evaluation (10%), and test (20%) partitions.\nTo ﬁne-tune, we attach a sequence classiﬁcation\nhead and train for 3 epochs.3\n3.2 Occupation Classiﬁcation\nThe goal of occupation classiﬁcation is to predict\nsomeone’s occupation from their online biogra-\nphy. We ﬁne-tune with theBIOS dataset scraped\nfrom Common Crawl (De-Arteaga et al., 2019),\nwhich includes over 400,000 online biographies\nbelonging to 28 common occupations. Since self-\nidentiﬁed gender was not collected, we will refer in-\nstead to the pronouns used in each biography (each\nbiography uses either he/him or she/her pronouns).\nFollowingDe-Arteaga et al.(2019), we use the\n“scrubbed” version of the dataset—in which all the\nidentifying pronouns have been removed—to mea-\nsure just the effects of proxy words (e.g. “mother”)\nand avoid overﬁtting on pronouns directly.\nDownstream Bias.—Biographies with she/her\npronouns are less frequently classiﬁed as be-\nlonging to certain traditionally male-dominated\nprofessions—such as “surgeon”—which could re-\nsult in lower recruitment or callback rates for job\ncandidates if the classiﬁer is used by an employer.\nThe empirical true positive rate (TPR) estimates\nthe likelihood that the classiﬁer correctly identiﬁes\na person’s occupation from their biography.\nWe follow previous work (De-Arteaga et al.,\n2019) in measuring downstream bias via the empir-\nical true positive rate (TPR) gap between biogra-\nphies using each set of pronouns. First, deﬁne\nTPRy,g = P[ˆY = y | G = g,Y = y],\nwhereg is a set of pronouns andy is an occupa-\ntion. Y and ˆY represent the true and predicted\noccupation, respectively. Then the TPR bias (TPB)\nis\nTPBy = TPRy,she/her\nTPRy,he/him\n. (1)\n2roberta-basefrom HuggingFace (Wolf et al., 2020).3See AppendixD for more details. Epochs and other\nparameters were chosen to match prior work (Jin et al., 2021).\nFor example, the classiﬁer correctly predicts “sur-\ngeon” for he/him biographies much more often\nthan for she/her biographies, so the TPR ratio for\nthe “surgeon” occupation is low (see AppendixA).\nUpstream Bias.—We adaptKurita et al.(2019)’s\npronoun ranking test to the 28 occupations in the\nBIOS dataset. Kurita et al.(2019) measure the\nencoded association of he/him and she/her pro-\nnouns by the difference in log probability scores be-\ntween pronouns appearing in templates of the form\n{pronoun} is a(n) {occupation}.W e\naugment this approach with 5 career-related tem-\nplates proposed byBartl et al.(2020) (see Ap-\npendixA). Formally, given a template sequence\nxy,g ﬁlled in with occupationy and pronoung,\nwe computepy,g = P(xy,g). As a baseline, we\nalso mask the occupation and compute the prior\nprobability⇡y,g = P(x⇡y,g). The pronoun ranking\nbias (PRB) for this template is the difference in log\nprobabilities:\nPRBy = logpy,she/her\n⇡y,she/her\n\u0000log py,he/him\n⇡y,he/him\n. (2)\n3.3 Toxicity Classiﬁcation\nFor toxicity classiﬁcation, we use theWIKI\ndataset, which consists of just under 130,000 com-\nments from the online forum Wikipedia Talks\nPages (Dixon et al., 2018). The goal of the task is to\npredict whether each comment is toxic. Each com-\nment has been labeled astoxicor non-toxic\nby a human annotator, where a toxic comment is a\n“rude, disrespectful, or unreasonable comment that\nis likely to make you leave the discussion” (Dixon\net al., 2018). FollowingDixon et al.(2018), we\nfocus on 50 terms referring to people of certain gen-\nders, races, ethnicities, sexualities, and religions.\nDownstream (Extrinsic) Bias.—Mentions of cer-\ntain identity groups—such as “queer”—are more\nlikely to be ﬂagged for toxic content, which could\nresult in certain communities being systematically\ncensored or left unprotected if an online plat-\nform uses the classiﬁer. The classiﬁer’s empirical\nfalse positive rate (FPR) estimates its likelihood\nto falsely ﬂag a non-toxic comment as toxic. The\nFPR corresponds to the risk of censoring inclusive\nspeech or de-platforming individuals who often\nmention marginalized groups.\nFollowingDixon et al.(2018), we express the\nclassiﬁer’s bias against comments or commenters\nharmlessly mentioning an identity term as the FPR\n3526\nbias (FPB).\nFPBi = P[ ˆT =0 | I = i, T= 1]\nP[ ˆT =0 | T = 1] , (3)\nwherei is an identity term andT =1 if the com-\nment was deemed toxic by a human annotator.\nUpstream Bias.—FollowingHutchinson et al.\n(2020), we measure upstream bias via sentiment\nassociations. We construct a set of templates of the\nform {identity} {person} is [MASK],\nwhere identities are the identity terms fromDixon\net al.(2018) (e.g. “gay” or “Muslim”) and the per-\nson phrases include “a person,” “my sibling,” and\nother relations. We predict the top-20 likely tokens\nfor the “[MASK]” position (e.g., “awesome” or\n“dangerous”). Using a pre-trained RoBERTA senti-\nment classiﬁer trained on the TweetEval benchmark\n(Barbieri et al., 2020), we then measure the average\nnegative sentiment score of the predicted tokens.\nThe model’s bias is the magnitude of negative asso-\nciation with each identity term.\nRoBERTa sometimes suggests terms which refer\nback to the target identity group. To mitigate this\neffect, we drop any predicted tokens that match the\n50 identity terms (e.g. “Latino”) fromDixon et al.\n(2018), but we are likely missing other confound-\ning adjectives (e.g. “Spanish”). We suspect this\nconfounding is minimal: we achieve similar results\nwith an alternative ranking-based bias metric (see\nAppendixC.2).\n4 Experiments\nWe measure changes in upstream and downstream\nbias subject to the following interventions (Fig.1):\n• No pre-training.To control for the effects of\npre-training, we test randomly initialized ver-\nsions of both models that have not been pre-\ntrained. We average over 10 trials.\n• Random perturbations.We instantiate a pre-\ntrained model and then add random noisee to\nevery weight in the embedding matrix. We try\nboth uniform noiseu ⇠ Unif(\u0000c, c) and Gaus-\nsian noisez ⇠N (0,\u00002), varyingc and\u00002. The\nﬁnal noise-added matrix is clipped so that its\nrange does not exceed that of the original matrix.\n• Bias mitigation.We apply theSENTDEBIAS al-\ngorithm to de-bias embeddings at the word-level\n(Liang et al., 2020). SENTDEBIAS estimates a\nbias subspaceVwith principal component anal-\nysis, then computes debiased word embeddings\nˆh = h \u0000 \u0000 Pk\nj=1hh,vjivj by subtracting the\nprojection ofh ontoV. We add the multiplier\n\u0000 to add or remove bias to various degrees—\nstandard SENTDEBIAS uses\u0000 =1 .0.\n• Re-balancing and scrubbing.For BIOS, we\nre-balance the ﬁne-tuning dataset by under-\nsampling biographies with the prevalent pronoun\nin each occupation. ForWIKI, we randomly re-\nmove from the ﬁne-tuning dataset↵ percent of\ncomments mentioning each identity term.\n4.1 Upstream variations have little impact on\ndownstream bias.\nOur goal is to test the bias transfer hypothe-\nsis, which holds that upstream bias is transferred\nthrough ﬁne-tuning to downstream models. By\nthis view, we would expect changes to the pre-\ntrained model to also change the distribution of\ndownstream bias—but we ﬁnd that for both tasks,\ndownstream bias is largely invariant to upstream\ninterventions. Figure2 summarizes the similarity\nof biases before and after each randomized event.\nThough randomizing the model weights signiﬁ-\ncantly reduces the mean and variance of upstream\nbias, the distribution of downstream bias changes\nvery little.4 For example, RoBERTa exhibits the\nsame disparities in performance after ﬁne-tuning re-\ngardless of whether the base model was pre-trained.\nLikewise, although theSENTDEBIAS mitigation\nmethod reduces pronoun ranking (upstream) bias as\nintended, we detect roughly the same downstream\nbiases no matter the level of mitigation applied\n(Figure3). For example, in theBIOS task, surgeons\nwith he/him pronouns are still 1.3 times more likely\nto have their biographies correctly classiﬁed than\ntheir she/her counterparts.\nThere is one notable exception to this trend: for\nthe WIKI task, adding noise (uniform or Gaussian)\nto the pre-trained model’s embedding matrix or\nnot pre-training the model yields a modest reduc-\ntion in median bias (Figure2). As upstream bias\nshifts towards zero, downstream bias also moves\nmarginally towards zero. Still, the largest biases\n(e.g., against the term “gay”) do not decrease and\nmay even increase after randomization.\n4See AppendixB.2for a full set of correlation tests.\n3527\nNurseSurgeon\nNotpre−trained80.4%accuracy\nGaussiannoise78.6%accuracy\nUniformnoise82.1%accuracy\nPre−trained86.3%accuracy\n0 1 2\nUpstream Bias\n(Log prob.: she/her − he/him)\nNurseSurgeon\n−0.5 0.0 0.5 1.0\nDownstream Bias\n(Log TPR: she/her − he/him)\n(a) BIOS\ngay\njewish\nNotpre−trained67.4%accuracy\nGaussiannoise80.4%accuracy\nUniformnoise93.1%accuracy\nPre−trained94.3%accuracy\n0.0 0.2 0.4 0.6\nUpstream Bias\n(Avg. Neg. Sentiment)\ngayjewish\n−4 −2 0 2\nDownstream Bias\n(Log FPR)\n(b) WIKI\nFigure 2: Bias per occupation after randomized interventions, averaged over 10 trials. Despite drastic changes to\nthe distribution of upstream bias (left), downstream bias remains roughly stable (right). For example, upstream\nbias with pre-training (purple) is not correlated with upstream biaswithoutpre-training. But downstream biasis\npartially correlated with the control (Pearson’s correlation coefﬁcient⇢BIOS =0 .93 and⇢WIKI =0 .64, p< 0.01).\n4.2 Most downstream bias is explained by the\nﬁne-tuning step.\nThough the results in the preceding section suggest\nthat there is no clear or consistent correspondence\nbetweenchangesin upstream bias andchanges\nin downstream bias, there is still a noticeable\ncorrelation between baseline upstream and down-\nstream bias (Pearson’s⇢ =0 .43, p =0 .022 for\nBIOS, ⇢ =0 .59, p< 10\u00005 for WIKI—see Ap-\npendixA). There is an important third variable\nthat helps explain this correlation: cultural arti-\nfacts ingrained in both the pre-training and ﬁne-\ntuning datasets.5 RoBERTa learns these artifacts\nthrough co-occurrences and other associations be-\ntween words in both sets of corpora.\nTo test this explanation, we conduct a simple\nregression analysis across interventions (Figure1)\n5For example, cultural biases about which pronouns be-\nlong in which occupations are likely to pervade both the pre-\ntraining dataset (e.g., Wikipedia) and the ﬁne-tuning dataset\n(internet biographies).\nand evaluation templates. We estimate\nlogTPBm,y = \u00000+\u00001PRBm,y,s+\u00002⇡y +fs +cm.\n(4)\nfor model treatmentm, occupationy, and pronoun\nranking templates. TPBis the TPR bias (down-\nstream bias) from Eq.1; PRBis the pronoun rank-\ning bias (upstream bias) from Eq.2; fs and cm\nare dummy variables (for ordinary least squares)\nor ﬁxed effects to capture heterogeneous effects\nbetween templates and models (such as variations\nin overall embedding quality). We control for sta-\ntistical “dataset bias” with⇡, the prevalence of\n“she/her” biographies within each occupationy in\nthe ﬁne-tuning data.\nWe ﬁnd that the “dataset bias” in the ﬁne-tuning\nstage explains most of the correlation between up-\nstream and downstream bias. Under the strong bias\ntransfer hypothesis, we would expect the coefﬁ-\ncient on upstream bias\u00001 to be statistically signiﬁ-\ncant and greater in magnitude than the coefﬁcient\n\u00002 on our proxy for dataset bias. But for both tasks,\n3528\nDJ\nModel\nNurse\nSurgeon\nDJ\nModel\nNurse\nSurgeon\nDownstream Bias (TPR Ratio)\nUpstream Bias (Pronoun Ranking)\n−50 −25 −10 −1 0 1 10 25 50\n0\n1\n2\n−0.25\n0.00\n0.25\n0.50\nMitigation Multiplier γ\nBias (she/her − he/him)\nFigure 3: Log TPR bias per occupation after scaled\nSENTDEBIAS on the BIOS task. Mitigation signiﬁ-\ncantly reduces pronoun ranking (upstream) bias com-\npared to base RoBERTa (top); but even when upstream\nbias decreases, the TPR ratio (downstream bias) re-\nmains mostly constant (bottom). The distribution of\ndownstream bias without any mitigation is almost per-\nfectly correlated with the distribution at\u0000 = 50(Pear-\nson’s⇢ =0 .96, p< 0.01).\nthe opposite is true: ﬁne-tuning dataset bias has a\nlarger effect than upstream bias. Figure4 reports\nthe coefﬁcient estimates for these two variables.\n(See AppendixC.1 for all estimates, standard er-\nrors, assumptions and additional speciﬁcations.)\nIn theBIOS task, a large decrease in upstream\nbias corresponds to a small but statistically signiﬁ-\ncantincreasein downstream bias. On average, a re-\nduction of 0.3 to the log likelihood gap—equivalent\nto the reduction in bias towards nurses after up-\nstream mitigation—corresponds to a 0.5% increase\nin the TPR ratio. Almost all the downstream bias in\nthe BIOS task is explained by dataset bias instead:\na 10% increase in the prevalence of she/her pro-\nnouns within an occupation corresponds to a much\nlarger 6.5% increase in the TPR ratio.\nIn theWIKI task, upstream bias has a more no-\nticeable effect—but the effect of dataset bias is\nstill much larger. The regression takes the same\nform as Eq.4, where downstream bias is FPR bias\n(Eq.3), upstream bias is negative sentiment, and\n⇡i is the proportion of toxic mentions of identity\ni. We additionally control for the prevalence of\neach identity term and the average length of toxic\nmentions of each identity term—longer comments\nare less likely to result in erroneous screening (Ap-\npendixC.1).\nAs in the previous regression, dataset bias ex-\nplains more of the variation in downstream bias\nthan does upstream bias. On average, alargein-\ncrease in average negative sentiment against a given\nidentity term (e.g. 0.1, one standard deviation) cor-\nresponds to only a modest 3.7% increase in FPR. In\ncomparison, only a 10% increase in the prevalence\nof toxic mentions of an identity corresponds to an\neven larger 6.3% increase in FPR.\nWe also check thatintrinsicdownstream bias\nalso changes due to ﬁne-tuning. We measure in-\ntrinsic bias again after ﬁne-tuning and regress on\ndownstream intrinsic bias instead of downstream\nextrinsic bias (Eq.4). The results are consistent: af-\nter controlling for the overall increase in log likeli-\nhood, the effect of upstream intrinsic bias on down-\nstream intrinsic bias is explained almost entirely by\nﬁne-tuning dataset bias (AppendixC.1).\n4.3 Re-sampling and re-scrubbing has little\neffect on downstream behavior.\nGiven the strong relation between our proxies for\ndataset bias and downstream bias, we test whether\nmanipulating these proxies admits some control\nover downstream bias. For example, were the ﬁne-\ntuning dataset to include exactly as many she/her\nnurse biographies as he/him, would the model still\nexhibit biased performance on that occupation?\nOur ﬁndings suggest not. No matter the amount\nof re-sampling, downstream bias remains rela-\ntively stable for pre-trained RoBERTa. The dis-\ntributions of downstream bias with and without\nre-balancing are almost perfectly correlated (Pear-\nson’s⇢ =0 .94, p< 0.01—see AppendixB.1).\nThough co-occurrence statistics help to explain\ndownstream bias, they are still only proxies for\ndataset bias. Directly altering these statistics via\nre-sampling the dataset does not alter the sentence-\nlevel context in which the words are used.\nBased on this result, we also try completely re-\nmoving mentions of identity terms. Scrubbing men-\ntions of identity terms—in all comments or only in\ntoxic comments—appears to reduce bias only when\nthe model is not pre-trained and all mentions of the\nterm are scrubbed (Figure5). For a pre-trained\nmodel trained on scrubbed data, a 10% decrease in\nmentions of an identity term corresponds to a 7.2%\n3529\n● ● ●●● ●\n● ●●●● ●\nFine−tuning\ndatasetbias\nPrevalanceof\nshe/her\nUpstreambias\nLikelihoodgap\n0.00 0.25 0.50 0.75\nCoefficient\n●\n●\n●\n●\n●\n●\nAll pre−\ntrained\nN=6020\nPre−trained\nN=140\nNoise added\nN=1400\nBalanced\nN=1400\nNot pre−\ntrained\nN=2940\nBias−\nmitigated\nN=1820\n(a) BIOS\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\nFine−tuning\ndatasetbias\nPrevalanceof\ntoxic mentions\nUpstreambias\nAvg.negative\nsentiment\n0.0 0.2 0.4 0.6 0.8\nCoefficient\n●\n●\n●\n●\n●\nAll pre−\ntrained\nN=12516\nPre−trained\nN=315\nNoise added\nN=6615\nScrubbed\nN=5901\nNot pre−\ntrained\nN=3150\n(b) WIKI\nFigure 4: Effect of upstream bias vs. ﬁne-tuning dataset bias on downstream bias, controlling for model & tem-\nplate ﬁxed effects. Bars depict heteroskedasticity-consistent standard errors. Statistically insigniﬁcant (p< 0.01)\ncoefﬁcients are hollow. For both tasks, reduction in ﬁne-tuning dataset bias corresponds to a greater alteration to\ndownstream bias than an equivalent reduction (accounting for scale) in upstream bias.\ndecrease in FPR. We speculate that RoBERTa re-\nlies on its high quality feature embeddings to learn\nproxy biases about identity terms based on the way\nthey are used in the pre-training corpora. For ex-\nample, our model classiﬁes a sequence containing\nonly the term “gay” as toxic without any context.\nIf a term like “gay” is often used pejoratively on\nthe web, RoBERTa is likely to infer that sentences\nincluding “gay” are toxic even if the term never\nappears in the ﬁne-tuning dataset.\nBut when the upstream model isnot pre-trained,\nthe ﬁne-tuned model has no such prejudices. In this\ncase, removing all mentions of identity results in a\ndistribution of bias entirely uncorrelated with the\ncontrol (Pearson’s⇢ =0 .09, p> 0.1). Notably,\nthough, even a small number of mentions of an\nidentity term like “gay” in the ﬁne-tuning dataset\nare enough for a randomly initialized model to\nexhibit the same biases as the pre-trained model\n(Figure5).\n5 Limitations\nOur approach comes with several limitations.\nFirst, our results may not generalize to all tasks—\nespecially non-classiﬁcation tasks—or all kinds of\nbias (e.g., bias against AA VE or non-English speak-\ners). Also, while similar studies of bias have been\nsuccessfully applied to vision transformers (Steed\nand Caliskan, 2021; Srinivasan and Uchino, 2021),\nour results may vary for substrates other than En-\nglish language.\nSecond,Goldfarb-Tarrant et al.(2021) conclude\nthat the lack of correlation between intrinsic bias\nindicators and downstream bias is because some\nembedding bias metrics are unsuitable for mea-\nsuring model bias. To ensure our intrinsic and\nextrinsic metrics measure the same construct, we\nchose upstream indicators that correlate with real-\nworld occupation statistics (Caliskan et al., 2017;\nKurita et al., 2019). Pronoun ranking in particular\nmay be more reliable for transformer models than\n3530\ngayjewish\ngayjewish\ngay\njewish\ngayjewish\ngay jewish\ngay\njewish\nAll\nToxic\nNone\n−4 −2 0 2 4\nDownstream Bias (Log FPR ratio)\nMentions Scrubbed\nPre−trained Not pre−trained\nFigure 5: FPR gap (downstream bias) after scrubbing\ntoxic mentions of identity terms from the WIKI ﬁne-\ntuning dataset. A combination of scrubbing and not\npre-training (orange) results in a zero-mean, noticeably\nre-ordered bias distribution. Scrubbing but still pre-\ntraining (purple) results in a bias distribution that is still\ncorrelated with the original bias distribution (Pearson’s\n⇢ =0 .99,0.93for toxic and all respectively,p< 0.01).\nother metrics (Silva et al., 2021). Still, downstream,\nannotator prejudices and other label biases could\nskew our extrinsic bias metrics as well (Davani\net al., 2021).\nThird, there may be other explanations for the re-\nlationship between upstream and downstream bias:\nfor example, decreasing the magnitude of upstream\nbias often requires a reduction in model accuracy,\nthough we attempt to control for between-model\nvariation with ﬁxed effects and other controls. Al-\nternate regression speciﬁcations included in Ap-\npendixC.1show how our results change with the\ninclusion of controls.\n6 Conclusion\nOur results offer several points of guidance to or-\nganizations training and distributing LLMs and the\npractitioners applying them:\n• Attenuating downstream bias via upstream\ninterventions—including embedding-space bias\nmitigation—is mostly futile in the cases we study\nand may be fruitless in similar settings.\n• For a typical pre-trained model trained for the\ntasks we study, the ﬁne-tuning dataset plays a\nmuch larger role than upstream bias in determin-\ning downstream harms.\n• Still, simply modulating co-occurrence statistics\n(e.g., by scrubbing harmful mentions of certain\nidentities) is not sufﬁcient. Task framing, de-\nsign, and data quality are also very important for\npreventing harm.\n• If a model is pre-trained, it may be more resis-\ntant to scrubbing, re-balancing, and other simple\nmodulations of the ﬁne-tuning dataset.\nBut, our results also corroborate a nascent, some-\nwhat optimistic view of pre-training bias. LLMs’\nintrinsic biases are harmful even before down-\nstream applications, and correcting those biases\nis not guaranteed to prevent downstream harms. In-\ncreased emphasis on the role of ﬁne-tuning dataset\nbias offers an opportunity for practitioners to shift\nto more careful, quality-focused and context-aware\napproach to NLP applications (Zhu et al., 2018;\nScheuerman et al., 2021).\nEthical Considerations\nThis study navigates several difﬁcult ethical issues\nin NLP ethics research. First, unlike prior work,\nwe do not claim to measure gender biases—only\nbiases related to someone’s choice of personal pro-\nnouns. However, our dataset is limited to the En-\nglish “he/him” and “she/her,” so our results do\nnot capture biases against other pronouns. Our\nstudy is also very Western-centric: we study only\nEnglish models/datasets and test for biases con-\nsidered normatively pressing in Western research.\nSecond, our training data (including pre-training\ndatasets), was almost entirely scraped from inter-\nnet users without compensation or explicit consent.\nTo avoid exploiting these users further, we only\nused already-scraped data and replicated already-\nexisting classiﬁers, and we do not release these\n3531\ndata or classiﬁers publicly. Finally, the models we\ntrained exhibit toxic, offensive behavior. These\nmodels and datasets are intended only for studying\nbias and simulating harms and, as our results show,\nshould not be deployed or applied to any other data\nexcept for this purpose.\nAcknowledgements\nThanks to Maria De-Arteaga and Benedikt Boeck-\ning for assistance withBIOS data collection.\nThanks also to the reviewers for their helpful com-\nments and feedback.\n3532\nReferences\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\npinosa Anke, and Leonardo Neves. 2020.Tweet-\nEval: Uniﬁed Benchmark and Comparative Evalu-\nation for Tweet Classiﬁcation. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 1644–1650, Online. Association for\nComputational Linguistics.\nSolon Barocas, Kate Crawford, Aaron Shapiro, and\nHanna Wallach. 2017. The problem with bias: Al-\nlocative versus representational harms in machine\nlearning. In9th Annual Conference of the Special\nInterest Group for Computing, Information and So-\nciety.\nSolon Barocas, Moritz Hardt, and Arvind Narayanan.\n2019. Fairness and Machine Learning. fairml-\nbook.org.\nMarion Bartl, Malvina Nissim, and Albert Gatt. 2020.\nUnmasking Contextual Stereotypes: Measuring and\nMitigating BERT’s Gender Bias. In Proceedings\nof the Second Workshop on Gender Bias in Natu-\nral Language Processing, pages 1–16, Barcelona,\nSpain (Online). Association for Computational Lin-\nguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021.On the\nDangers of Stochastic Parrots: Can Language Mod-\nels Be Too Big?In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, pages 610–623, New York, NY ,\nUSA. Association for Computing Machinery.\nAbeba Birhane and Vinay Uday Prabhu. 2021.Large\nimage datasets: A pyrrhic win for computer vision?\nIn 2021 IEEE Winter Conference on Applications of\nComputer Vision (WACV), pages 1536–1546. ISSN:\n2642-9381.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna Wallach. 2020.Language (Technology) is\nPower: A Critical Survey of “Bias” in NLP. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 5454–\n5476, Online. Association for Computational Lin-\nguistics.\nSu Lin Blodgett and Brendan O’Connor. 2017.Racial\nDisparity in Natural Language Processing: A Case\nStudy of Social Media African-American English.\narXiv:1707.00061 [cs]. ArXiv: 1707.00061.\nSu Lin Blodgett, Johnny Wei, and Brendan O’Connor.\n2018. Twitter Universal Dependency Parsing for\nAfrican-American and Mainstream American En-\nglish. InProceedings of the 56th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1415–1425, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou,\nVenkatesh Saligrama, and Adam T Kalai. 2016.\nMan is to Computer Programmer as Woman is to\nHomemaker? Debiasing Word Embeddings. In D D\nLee, M Sugiyama, U V Luxburg, I Guyon, and\nR Garnett, editors,Advances in Neural Information\nProcessing Systems 29, pages 4349–4357. Curran\nAssociates, Inc.\nRishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ\nAltman, Simran Arora, Sydney von Arx, Michael S.\nBernstein, Jeannette Bohg, Antoine Bosselut, Emma\nBrunskill, Erik Brynjolfsson, Shyamal Buch, Dal-\nlas Card, Rodrigo Castellon, Niladri Chatterji, An-\nnie Chen, Kathleen Creel, Jared Quincy Davis, Dora\nDemszky, Chris Donahue, Moussa Doumbouya,\nEsin Durmus, Stefano Ermon, John Etchemendy,\nKawin Ethayarajh, Li Fei-Fei, Chelsea Finn, Trevor\nGale, Lauren Gillespie, Karan Goel, Noah Good-\nman, Shelby Grossman, Neel Guha, Tatsunori\nHashimoto, Peter Henderson, John Hewitt, Daniel E.\nHo, Jenny Hong, Kyle Hsu, Jing Huang, Thomas\nIcard, Saahil Jain, Dan Jurafsky, Pratyusha Kalluri,\nSiddharth Karamcheti, Geoff Keeling, Fereshte\nKhani, Omar Khattab, Pang Wei Kohd, Mark Krass,\nRanjay Krishna, Rohith Kuditipudi, Ananya Kumar,\nFaisal Ladhak, Mina Lee, Tony Lee, Jure Leskovec,\nIsabelle Levent, Xiang Lisa Li, Xuechen Li, Tengyu\nMa, Ali Malik, Christopher D. Manning, Suvir Mir-\nchandani, Eric Mitchell, Zanele Munyikwa, Suraj\nNair, Avanika Narayan, Deepak Narayanan, Ben\nNewman, Allen Nie, Juan Carlos Niebles, Hamed\nNilforoshan, Julian Nyarko, Giray Ogut, Laurel Orr,\nIsabel Papadimitriou, Joon Sung Park, Chris Piech,\nEva Portelance, Christopher Potts, Aditi Raghu-\nnathan, Rob Reich, Hongyu Ren, Frieda Rong,\nYusuf Roohani, Camilo Ruiz, Jack Ryan, Christo-\npher Ré, Dorsa Sadigh, Shiori Sagawa, Keshav\nSanthanam, Andy Shih, Krishnan Srinivasan, Alex\nTamkin, Rohan Taori, Armin W. Thomas, Florian\nTramèr, Rose E. Wang, William Wang, Bohan Wu,\nJiajun Wu, Yuhuai Wu, Sang Michael Xie, Michi-\nhiro Yasunaga, Jiaxuan You, Matei Zaharia, Michael\nZhang, Tianyi Zhang, Xikun Zhang, Yuhui Zhang,\nLucia Zheng, Kaitlyn Zhou, and Percy Liang. 2021.\nOn the Opportunities and Risks of Foundation Mod-\nels. arXiv:2108.07258 [cs]. ArXiv: 2108.07258.\nAylin Caliskan, Joanna J. Bryson, and Arvind\nNarayanan. 2017.Semantics derived automatically\nfrom language corpora contain human-like biases.\nScience, 356(6334):183–186.\nYang Trista Cao and Hal Daumé, III. 2021.Toward\nGender-Inclusive Coreference Resolution: An Anal-\nysis of Gender and Bias Throughout the Machine\nLearning Lifecycle*. Computational Linguistics,\n47(3):615–661.\nAida Mostafazadeh Davani, Mohammad Atari, Bren-\ndan Kennedy, and Morteza Dehghani. 2021.Hate\nSpeech Classiﬁers Learn Human-Like Social Stereo-\ntypes. arXiv:2110.14839 [cs]. ArXiv: 2110.14839.\n3533\nMaria De-Arteaga, Alexey Romanov, Hanna Wal-\nlach, Jennifer Chayes, Christian Borgs, Alexandra\nChouldechova, Sahin Geyik, Krishnaram Kentha-\npadi, and Adam Tauman Kalai. 2019.Bias in Bios:\nA Case Study of Semantic Representation Bias in a\nHigh-Stakes Setting. In Proceedings of the Confer-\nence on Fairness, Accountability, and Transparency,\nFAT* ’19, pages 120–128, New York, NY , USA. As-\nsociation for Computing Machinery.\nLucas Dixon, John Li, Jeffrey Sorensen, Nithum Thain,\nand Lucy Vasserman. 2018.Measuring and Mitigat-\ning Unintended Bias in Text Classiﬁcation. In Pro-\nceedings of the 2018 AAAI/ACM Conference on AI,\nEthics, and Society, pages 67–73, New Orleans LA\nUSA. ACM.\nAparna Garimella, Carmen Banea, Dirk Hovy, and\nRada Mihalcea. 2019. Women’s Syntactic Re-\nsilience and Men’s Grammatical Luck: Gender-Bias\nin Part-of-Speech Tagging and Dependency Parsing.\nIn Proceedings of the 57th Annual Meeting of the\nAssociation for Computational Linguistics, pages\n3493–3498, Florence, Italy. Association for Compu-\ntational Linguistics.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020.RealToxic-\nityPrompts: Evaluating Neural Toxic Degeneration\nin Language Models. InFindings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nSeraphina Goldfarb-Tarrant, Rebecca Marchant, Ri-\ncardo Muñoz Sánchez, Mugdha Pandya, and Adam\nLopez. 2021.Intrinsic Bias Metrics Do Not Corre-\nlate with Application Bias. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 1926–1940, Online. As-\nsociation for Computational Linguistics.\nWei Guo and Aylin Caliskan. 2021.Detecting Emer-\ngent Intersectional Biases: Contextualized Word\nEmbeddings Contain a Distribution of Human-like\nBiases. InProceedings of the 2021 AAAI/ACM Con-\nference on AI, Ethics, and Society, pages 122–133.\nAssociation for Computing Machinery, New York,\nNY , USA.\nBen Hutchinson, Vinodkumar Prabhakaran, Emily\nDenton, Kellie Webster, Yu Zhong, and Stephen De-\nnuyl. 2020.Social Biases in NLP Models as Barriers\nfor Persons with Disabilities. InProceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5491–5501, Online. As-\nsociation for Computational Linguistics.\nXisen Jin, Francesco Barbieri, Brendan Kennedy, Aida\nMostafazadeh Davani, Leonardo Neves, and Xiang\nRen. 2021. On Transferability of Bias Mitigation\nEffects in Language Model Fine-Tuning. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n3770–3783, Online. Association for Computational\nLinguistics.\nKeita Kurita, Nidhi Vyas, Ayush Pareek, Alan W\nBlack, and Yulia Tsvetkov. 2019.Measuring Bias\nin Contextualized Word Representations. In Pro-\nceedings of the First Workshop on Gender Bias in\nNatural Language Processing, pages 166–172, Flo-\nrence, Italy. Association for Computational Linguis-\ntics (ACL).\nPaul Pu Liang, Irene Mengze Li, Emily Zheng,\nYao Chong Lim, Ruslan Salakhutdinov, and Louis-\nPhilippe Morency. 2020.Towards Debiasing Sen-\ntence Representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 5502–5515, Online. Asso-\nciation for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs]. ArXiv:\n1907.11692.\nDavid P. MacKinnon, Jennifer L. Krull, and Chon-\ndra M. Lockwood. 2000.Equivalence of the Me-\ndiation, Confounding and Suppression Effect. Pre-\nvention Science, 1(4):173–181.\nChandler May, Alex Wang, Shikha Bordia, Samuel R.\nBowman, and Rachel Rudinger. 2019.On Measur-\ning Social Biases in Sentence Encoders. InProceed-\nings of the 2019 Conference of the North, pages 622–\n628, Stroudsburg, PA, USA. Association for Compu-\ntational Linguistics.\nJi Ho Park, Jamin Shin, and Pascale Fung. 2018.Re-\nducing Gender Bias in Abusive Language Detec-\ntion. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2799–2804, Brussels, Belgium. Association\nfor Computational Linguistics.\nMaarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi,\nand Noah A. Smith. 2019.The Risk of Racial Bias\nin Hate Speech Detection. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 1668–1678, Florence,\nItaly. Association for Computational Linguistics.\nMorgan Klaus Scheuerman, Alex Hanna, and Emily\nDenton. 2021. Do Datasets Have Politics? Dis-\nciplinary Values in Computer Vision Dataset De-\nvelopment. Proceedings of the ACM on Human-\nComputer Interaction, 5(CSCW2):317:1–317:37.\nAndrew Silva, Pradyumna Tambwekar, and Matthew\nGombolay. 2021.Towards a Comprehensive Under-\nstanding and Accurate Evaluation of Societal Biases\nin Pre-Trained Transformers. InProceedings of the\n2021 Conference of the North American Chapter of\n3534\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 2383–2389, On-\nline. Association for Computational Linguistics.\nIrene Solaiman and Christy Dennison. 2021.Process\nfor adapting language models to society (palms)\nwith values-targeted datasets. InPre-Proceedings of\nAdvances in Neural Information Processing Systems,\nvolume 34.\nRamya Srinivasan and Kanji Uchino. 2021.Biases in\nGenerative Art: A Causal Look from the Lens of Art\nHistory. In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\nFAccT ’21, pages 41–51, New York, NY , USA. As-\nsociation for Computing Machinery.\nRyan Steed and Aylin Caliskan. 2021.Image Repre-\nsentations Learned With Unsupervised Pre-Training\nContain Human-like Biases. In Proceedings of the\n2021 ACM Conference on Fairness, Accountability,\nand Transparency, FAccT ’21, pages 701–713, New\nYork, NY , USA. Association for Computing Machin-\nery.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and Policy Considerations for\nDeep Learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020.Trans-\nformers: State-of-the-Art Natural Language Process-\ning. InProceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nHaoran Zhang, Amy X. Lu, Mohamed Abdalla,\nMatthew McDermott, and Marzyeh Ghassemi. 2020.\nHurtful words: quantifying biases in clinical con-\ntextual word embeddings. In Proceedings of the\nACM Conference on Health, Inference, and Learn-\ning, CHIL ’20, pages 110–120, New York, NY , USA.\nAssociation for Computing Machinery.\nHaiyi Zhu, Bowen Yu, Aaron Halfaker, and Loren\nTerveen. 2018. Value-Sensitive Algorithm De-\nsign: Method, Case Study, and Lessons. Proceed-\nings of the ACM on Human-Computer Interaction,\n2(CSCW):194:1–194:23.\n3535\nA Descriptive Statistics\nBIOS.— Biographies include the 28-most frequent\noccupations according to the BLS Standard Occu-\npation Classiﬁcation system.6 See Figure6 for a\nfull list of occupations and the prevalence of each\nset of pronouns.\nFigure 6: Frequency of occupations in BIOS dataset.\nUpstream bias (measured with pronoun rank-\ning) is depicted in Figure7. Table 1 Gives the\nfull list of templates used for testing. Traditionally\nfemale occupations (e.g., “nurse”) are generally bi-\nased towards “she/her,” with some exceptions (e.g.,\n“software engineer”). Downstream bias is similarly\ndistributed—Figure9 depicts the relationship be-\ntween upstream and downstream bias, which is\ngenerally linear (Pearson’s⇢ =0 .43, p< 0.05).\nThere are noticeable outliers (e.g., “surgeon”) for\nwhich real-world harms could be especially acute.\nWIKI.— Identity terms occur relatively infre-\nquently in the dataset and some terms appear in\ntoxic comments more often than others. See Fig-\nure8 for a full list of identity terms tested and their\nprevalence in toxic and non-toxic comments.\nUpstream and downstream bias have an even\nstronger relationship forWIKI than forBIOS (⇢ =\n0.59, p< 0.01). The terms “homosexual” and\n“gay” have especially high biases both upstream and\ndownstream, likely because of their use as common\npejoratives on certain internet spaces represented\nin the pre-training and ﬁne-tuning datasets.\nB Additional Results\nB.1 Partial Interventions\nThis section gives the full results for each par-\ntial intervention (e.g., scrubbing only half of the\n6https://www.bls.gov/soc/\nFigure 7: Distribution of pronoun ranking (upstream)\nbias across occupations in the BIOS task.\nFigure 8: Frequency of comments mentioning each\nidentity term in the WIKI dataset.\ntoxic mentions, rather than all of them). Figure10\nshows downstream bias at each level of pronoun\nre-balancing. To measure the effects of different\nlevels of gender balance, we either randomly over-\nsample the minority group to close↵ percent of the\ngap between the majority and minority pronouns,\nrandomly under-sample the majority group to close\n\u0000 percent of the gap, or some combination of both.\nFigure 11 shows the full set of partial scrub-\nbing interventions tested - notably, there is a slight\nconvergence to zero in downstream bias when the\nmodel isnot pre-trained.\nB.2 Correlation Tests\nTo quantify the effect of our experimental inter-\nventions on upstream and downstream bias, we\ncompared the distribution of bias after each in-\n3536\nBias Template Targets Source\nOccupation-Pronoun [TARGET] is a(n) [OCCUPATION] he, she ( Kurita et al., 2019)\nOccupation-Pronoun [TARGET] works as a(n) [OCCUPA-\nTION]\nhe, she ( Bartl et al., 2020)\nOccupation-Pronoun [TARGET] wants to become a(n) [OC-\nCUPATION]\nhe, she ( Bartl et al., 2020)\nOccupation-Pronoun [TARGET] applied for the position of\n[OCCUPATION]\nhe, she ( Bartl et al., 2020)\nIdentity-Sentiment [IDENTITY] [TARGET] are [MASK] people ( Dixon et al., 2018)\nIdentity-Sentiment [IDENTITY] [TARGET] is [MASK] my sibling (Dixon et al., 2018)\nmy friend\nmy parent\nmy partner\nmy spouse\nTable 1: Templates used for bias measurement.\nFigure 9: Correlation between upstream and down-\nstream bias across occupations (BIOS) and identity\nterms (WIKI). ⇢ is the Pearson correlation coefﬁcient.\ntervention to the distribution of bias after an un-\nmodiﬁed pre-trained model. We tested for statis-\ntical correlation between these two distributions\nwith both Pearson’s correlation coefﬁcient⇢ and\nKendall’s correlation coefﬁcient⌧. For Pearson’s,\nwe assume that the two distributions are approx-\nimately normally distributed. This assumptions\nseems reasonable because our samples are not too\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n● ●\n●\n●\n● ●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\n●\nDJ\nModel\nNurse\nSurgeon−0.25\n0.00\n0.25\n0.50\n0 10 20 30 40 50 60 70 80 90 100\n% pronoun gap reduced w/ undersampling\nDownstream Bias(TPR ratio: she/her − he/him)\nFigure 10: TPR gap (downstream bias) after balanc-\ning pronouns within each occupation of the BIOS ﬁne-\ntuning dataset. As shown, balancing pronoun preva-\nlence has little effect on downstream bias.\nsmall (N = 28andN = 50for BIOS and WIKI,\nrespectively), but a Shapiro-Wilk test of normality\nshows that downstream bias for both tasks is likely\nnon-normal (W =0 .81 and W =0 .67 for TPR\nratio and FPR ratio respectively,p< 0.01). So,\nwe also compute Kendall’s correlation coefﬁcient\n⌧, which is a nonparametric test of ordinal asso-\nciation. The results are similar in magnitude and\nsigniﬁcance (Table2).\nC Full Regression Results\nTables 3 and 4 report the full set of coefﬁ-\ncient estimates used to generate Figure4 and\nthe effects described in the paper. We use HC3\n3537\nPre−trained Not pre−trained\nToxic Mentions OnlyAll Mentions\n0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100\n−2.5\n0.0\n2.5\n5.0\n−2.5\n0.0\n2.5\n5.0\n% mentions scrubbed\nDownstream Bias (Log FPR ratio)\nFigure 11: FPR gap (downstream bias) after scrubbing toxic mentions of identity terms from the WIKI ﬁne-tuning\ndataset.\nheteroskedasticity-consistent standard errors.7 The\nVariance Inﬂation Factor (VIF) for the covariates\nare all less than2.5 for an unmodiﬁed pre-trained\nmodel for both tasks (a sign that multicollinearity\nmay not be too severe).\nThe ﬁxed effects regression requires a few as-\nsumptions for unbiased, normally-converging esti-\nmates. First, we assume that the error is uncorre-\nlated with every covariate (i.e., there are no omitted\nvariables; we discuss this possibility in the limita-\ntions section). Second, we assume that the sam-\nples are independent and identically distributed (in-\ndependence is assured by our experimental setup,\nwhich varies one factor at a time). Third, we as-\nsume that large outliers are unlikely (evident from\nthe distribution plots presented).\nC.1 Additional Speciﬁcations\nWe tested several regression speciﬁcations on just\nthe unmodiﬁed, pre-trained model (Tables5 and6).\nFor BIOS, note that the direct and indirect (after\ncontrolling for dataset bias) effects of upstream\nbias on downstream bias have opposite signs. The\nchange is the effect of including dataset bias, a col-\ninear confounder, in the regression. Confounders\n7For a simple OLS speciﬁcation onWIKI, the Breusch-\nPagan test rejects the hypothesis that our errors are ho-\nmoskedastic withBP =2 7.039, p< 10\u00003. For BIOS,\nthe hypothesis is not rejected (BP =5 .033, p =0 .41).\ncan be interpreted as “explaining” the relationship\nbetween the independent (upstream) and dependent\n(downstream) variables (MacKinnon et al., 2000).\nTo test whether the effect observed is mediated\nby a change in the model’s weights, also include\nan estimate of the effect of upstream intrinsic bias\n(e.g., from pronoun ranking) ondownstreamintrin-\nsic bias (intrinsic bias, measured after ﬁne-tuning).\nWe control for the overall increase in log likelihood\nby including in the regression the difference in log\nlikelihood of the neutral pronouns “they/them” be-\nfore and after ﬁne-tuning. We ﬁnd that a similar\nrelationship holds between upstream bias and in-\ntrinsic bias downstream as holds between upstream\nbias and extrinsic bias downstream, suggesting that\nthe model’s internal representations change in con-\ncert with its downstream behavior.\nC.2 Identity Ranking - Robustness Check\nBecause of the limitations of the sentiment-based\napproach, we check the robustness of our results\nwith an identity ranking approach based on pro-\nnoun ranking. Included in theDixon et al.(2018)\nstudy of toxicity classiﬁcation bias is an extensive\nevaluation set composed of 89,000 templates such\nas “[IDENTITY] is [ATTRIBUTE],” where the\nattributes include both positive (for non-toxic ex-\namples) and extremely negative words (for toxic\n3538\nUpstream Bias Downstream Bias\nIntervention Pearson’s⇢ Kendall’s⌧ Pearson’s⇢ Kendall’s⌧\nBIOS Pronoun ranking TPR ratio\nNot pre-trained -0.08 0.04 0.93 ⇤⇤⇤ 0.74⇤⇤⇤\nUniform noise 0.90 ⇤⇤⇤ 0.62⇤⇤⇤ 0.67⇤⇤⇤ 0.60⇤⇤⇤\nGaussian noise 0.29 0.09 0.90 ⇤⇤⇤ 0.56⇤⇤⇤\nSENTDEBIAS (\u0000 = 50) 0.87 ⇤⇤⇤ 0.61⇤⇤⇤ 0.96⇤⇤⇤ 0.77⇤⇤⇤\nDataset re-balancing (\u0000 =1 .0) 0.94 ⇤⇤⇤ 0.85⇤⇤⇤\nWIKI Negative sentiment FPR ratio\nNot pre-trained -0.21 -0.14 0.64 ⇤⇤⇤ 0.39⇤⇤⇤\nUniform noise 0.56 ⇤⇤⇤ 0.37⇤⇤⇤ 0.91⇤⇤⇤ 0.56⇤⇤⇤\nGaussian noise 0.36 ⇤⇤⇤ 0.24⇤⇤ 0.78⇤⇤⇤ 0.45⇤⇤⇤\nScrubbing toxic mentions 0.99 ⇤⇤⇤ 0.85⇤⇤⇤\nScrubbing all mentions 0.93 ⇤⇤⇤ 0.77⇤⇤⇤\nScrubbing toxic mentions, not pre-trained 0.30⇤⇤ 0.21⇤⇤ -0.11 -0.07\nScrubbing all mentions, not pre-trained 0.30⇤⇤ 0.21⇤⇤ 0.09 0.10\nNote:⇤p<0.1;⇤⇤p<0.05;⇤⇤⇤p<0.01\nTable 2: Correlation between bias distributions before and after each intervention. Statistically insigniﬁcant cor-\nrelation coefﬁcients indicate bias has changed drastically (red). Notably, downstream bias is correlated with the\ncontrol to some extent for every intervention except for scrubbing and not pre-training. Pearson’s correlation coefﬁ-\ncient⇢ measure of correlation strength and direction; Kendall’s⌧ is a measure of ordinal correlation. Randomized\ninterventions (e.g., not pre-training, adding noise) tend to re-order the bias distribution more than others, indicated\nby a lower⌧.\nTable 3: Effect of upstream on downstream bias for pre-trained RoBERTa on the BIOS task. Panel linear models\ninclude model ﬁxed effects.\nDependent variable:Log TPR ratio (downstream bias)OLS panel linearPre-trained Mitigated Noise added Random Balanced All pre-trained(1) (2) (3) (4) (5) (6)Likelihood gap (upstream bias)\u00000.068⇤⇤⇤ \u00000.058⇤⇤⇤ \u00000.063⇤ \u00000.013 \u00000.016⇤⇤ \u00000.018⇤⇤⇤\n(0.018) (0.005) (0.038) (0.011) (0.007) (0.007)\nPrevalance of she/her 0.485⇤⇤⇤ 0.458⇤⇤⇤ 0.534⇤⇤⇤ 0.739⇤⇤⇤ 0.820⇤⇤⇤ 0.633⇤⇤⇤\n(0.043) (0.011) (0.016) (0.048) (0.036) (0.027)\nConstant \u00000.090⇤⇤⇤\n(0.029)\nTemplate Dummies? Yes Yes Yes Yes Yes YesObservations 140 1,820 1,400 2,940 1,400 6,020R2 0.500 0.489 0.438 0.075 0.297 0.085Adjusted R2 0.477 0.484 0.432 0.067 0.289 0.078Residual Std. Error 0.109 (df = 133)F Statistic 22.149 ⇤⇤⇤(df = 6; 133) 287.282⇤⇤⇤(df = 6; 1801) 179.585⇤⇤⇤(df = 6; 1384) 39.276⇤⇤⇤(df = 6; 2913) 97.257⇤⇤⇤(df = 6; 1384) 92.307⇤⇤⇤(df = 6; 5971)\nNote: ⇤p<0.1;⇤⇤p<0.05;⇤⇤⇤p<0.01\n3539\nTable 4: Effect of upstream on downstream bias for pre-trained RoBERTa on the WIKI task. Panel linear models\ninclude model ﬁxed effects.\nDependent variable:\nFPR\nOLS panel linearPre-trained Noise added Random Scrubbed All pre-trained\n(1) (2) (3) (4) (5)\nAvg. negative sentiment (upstream bias) 0.591⇤⇤⇤ 0.255⇤⇤⇤ 0.029 0.571 ⇤⇤⇤ 0.376⇤⇤⇤\n(0.107) (0.027) (0.084) (0.025) (0.017)\nPrevalance of toxic mentions 0.650⇤⇤⇤ 0.556⇤⇤⇤ 0.425⇤⇤⇤ 0.716⇤⇤⇤ 0.626⇤⇤⇤\n(0.068) (0.013) (0.015) (0.018) (0.011)\nPrevalance of identity term\u00005.024 1.575 ⇤⇤ 6.526⇤⇤⇤ \u00007.274⇤⇤⇤ \u00001.231⇤⇤\n(3.740) (0.708) (0.815) (1.086) (0.612)\nAvg. length of toxic mentions\u00000.373⇤⇤⇤\n(0.077)\nTemplate Dummies? Yes Yes Yes Yes Yes\nObservations 315 6,615 3,150 5,901 12,516R2 0.296 0.225 0.210 0.283 0.241Adjusted R2 0.276 0.221 0.206 0.279 0.238Residual Std. Error 0.221 (df = 305)F Statistic 14.283 ⇤⇤⇤(df = 9; 305) 211.998⇤⇤⇤(df = 9; 6585) 92.711⇤⇤⇤(df = 9; 3131) 257.043⇤⇤⇤(df = 9; 5873) 439.225⇤⇤⇤(df = 9; 12467)\nNote: ⇤p<0.1;⇤⇤p<0.05;⇤⇤⇤p<0.01\nTable 5: Effect of upstream on downstream bias for pre-trained RoBERTa on the BIOS task.\nDependent variable:TPR ratio (downstream bias) Likelihood gap after ﬁne-tuning (intermediate bias)OLS panel panel linear linear(1) (2) (3) (4) (5)Likelihood gap (upstream bias) 0.043⇤⇤ \u00000.068⇤⇤⇤ 0.043⇤⇤ \u00000.068⇤⇤⇤ 0.046(0.021) (0.018) (0.021) (0.018) (0.575)\nPrevalance of she/her 0.485 ⇤⇤⇤ 0.485⇤⇤⇤ 10.311⇤⇤⇤\n(0.043) (0.043) (1.424)\nDifference in they/them log likelihood before and after pre-training 0.206⇤⇤\n(0.086)\nConstant \u00000.013 \u00000.090⇤⇤⇤\n(0.039) (0.029)\nTemplate dummies? Yes Yes No No NoObservations 140 140 140 140 140R2 0.031 0.500 0.031 0.500 0.366Adjusted R2 \u00000.005 0.477 \u00000.005 0.477 0.332Residual Std. Error 0.151 (df = 134) 0.109 (df = 133)F Statistic 0.856 (df = 5; 134) 22.149 ⇤⇤⇤(df = 6; 133) 4.279⇤⇤(df = 1; 134) 66.447⇤⇤⇤(df = 2; 133) 25.386⇤⇤⇤(df = 3; 132)\nNote: ⇤p<0.1;⇤⇤p<0.05;⇤⇤⇤p<0.01\nTable 6: Effect of upstream on downstream bias for pre-trained RoBERTa on the WIKI task.\nDependent variable:FPR Avg. negative sentiment after ﬁne-tuning (intermediate bias)OLS panel panel linear linear(1) (2) (3) (4) (5) (6)Avg. negative sentiment (upstream bias) 0.569⇤⇤⇤ 0.568⇤⇤⇤ 0.586⇤⇤⇤ 0.569⇤⇤⇤ 0.591⇤⇤⇤ \u00000.004(0.110) (0.106) (0.108) (0.110) (0.107) (0.014)\nPrevalance of toxic mentions 0.657⇤⇤⇤ 0.654⇤⇤⇤ 0.650⇤⇤⇤ \u00000.020⇤⇤\n(0.068) (0.070) (0.068) (0.009)\nPrevalance of identity term \u00004.973 \u00005.024 0.656(3.749) (3.740) (0.481)\nAvg. length of toxic mentions 0.00001(0.00002)\nConstant \u00000.281⇤⇤⇤ \u00000.371⇤⇤⇤ \u00000.375⇤⇤⇤\n(0.079) (0.077) (0.078)\nTemplate Dummies? Yes Yes Yes No No NoObservations 350 315 315 350 315 315R2 0.073 0.292 0.297 0.073 0.296 0.024Adjusted R2 0.054 0.274 0.274 0.054 0.276 \u00000.004Residual Std. Error 0.240 (df = 342) 0.221 (df = 306) 0.221 (df = 304)F Statistic 3.829 ⇤⇤⇤(df = 7; 342) 15.801⇤⇤⇤(df = 8; 306) 12.826⇤⇤⇤(df = 10; 304) 26.802⇤⇤⇤(df = 1; 342) 42.848⇤⇤⇤(df = 3; 305) 2.551⇤(df = 3; 305)Note: ⇤p<0.1;⇤⇤p<0.05;⇤⇤⇤p<0.01\n3540\nexamples). Templates of other forms are not in-\ncluded to reduce computation time.8 For each of\nthese templates, we mask the identity term and\ncompute the log probability score as described in\n§ 3.2. The model’s bias is described by the differ-\nence between the average log probability scores for\nthe toxic templates and the non-toxic templates for\neach identity term.\nFor the regressions (Tables7 and 8), the tem-\nplates are not paired, so we average ﬁrst across\ntoxic and non-toxic templates, then calculate the\nratio between the two. The relative size and statisti-\ncal signiﬁcance of the coefﬁcients are the same as\nfor the negative sentiment approach, suggesting the\nnegative sentiment metric is robust for our purposes\ndespite its limitations.\nD Replication\nWe provide our full results (upstream and down-\nstream bias for every intervention, for each task)\nand the scripts used to analyse them. We are not\nallowed to release the source code used to train\nour models and measure bias, but we include addi-\ntional details on our implementation to help others\nunderstand and replicate our results.\n• Our code for the pronoun ranking tests is\nadapted fromZhang et al.(2020)’s implemen-\ntation available athttps://github.com/\nMLforHealth/HurtfulWords.\n• Our code forSENTDEBIAS is adapted from the\noriginal authors’ (Liang et al., 2020), available\nat https://github.com/pliang279/\nsent_debias.\n• Epochs and other parameters were chosen\nto match prior work on the same tasks (Jin\net al., 2021). We train with 5 epochs, batch\nsizes 16 and 64 for training and evaluation\nrespectively, and a learning rate of5de \u0000\n6. Otherwise, we use the default hyper-\nparameters forroberta-base(https://\nhuggingface.co/roberta-base).\n• Code for scraping theBIOS dataset is pro-\nvided by the original authors at https:\n//github.com/microsoft/biosbias.\nThe WIKI dataset is available at https:\n8A full list of these templates can be found in (Dixon et al.,\n2018) orhttps://github.com/conversationai/\nunintended-ml-bias-analysis.\n//github.com/conversationai/\nunintended-ml-bias-analysis.\n• Fine-tuning a single model for either task takes\nfrom 4-6 hours on single NVIDIA Tesla V100\n16GB GPU. Our results include approximately\n60 model permutations for a total of 240-360\nGPU hours.roberta-basehas 125M param-\neters, but we did not pre-train any models from\nscratch.\n3541\nTable 7: Effect of upstream on downstream bias for pre-trained RoBERTa on the WIKI task.\nDependent variable:\nFPR\n(1) (2) (3)\nAvg. log likelihood ratio (upstream bias) 0.399⇤⇤ 0.292 0.286\n(0.192) (0.176) (0.187)\nPrevalance of toxic mentions 0.624 ⇤⇤⇤ 0.628⇤⇤⇤\n(0.180) (0.186)\nPrevalance of identity term 0.00001\n(0.0001)\nAvg. length of toxic mentions 0.096 ⇤⇤⇤ 0.008 0.004\n(0.034) (0.040) (0.058)\nObservations 50 50 50\nR2 0.083 0.269 0.269\nAdjusted R2 0.064 0.238 0.222\nResidual Std. Error 0.241 (df = 48) 0.217 (df = 47) 0.220 (df = 46)\nF Statistic 4.345 ⇤⇤(df = 1; 48) 8.649⇤⇤⇤(df = 2; 47) 5.648⇤⇤⇤(df = 3; 46)\nNote: ⇤p<0.1;⇤⇤p<0.05;⇤⇤⇤p<0.01\nTable 8: Effect of upstream on downstream bias for pre-trained RoBERTa on the WIKI task.\nDependent variable:\nFPR ratio (downstream bias)\nOLS panel linearPre-trained Noise added Random Scrubbed All pre-trained\n(1) (2) (3) (4) (5)\nAvg. log likelihood ratio (upstream bias) 0.292 0.073⇤⇤ 0.165 0.301 ⇤⇤⇤ 0.184⇤⇤⇤\n(0.176) (0.032) (0.167) (0.039) (0.022)\nPrevalance of toxic mentions 0.624⇤⇤⇤ 0.558⇤⇤⇤ 0.421⇤⇤⇤ 0.688⇤⇤⇤ 0.536⇤⇤⇤\n(0.180) (0.034) (0.039) (0.046) (0.021)\nPrevalance of identity term 2.611 \u00001.074 2.682 ⇤⇤\n(1.777) (2.700) (1.177)\nConstant 0.008 (0.040)\nObservations 50 1,050 500 1,000 3,050R2 0.269 0.216 0.196 0.247 0.200Adjusted R2 0.238 0.198 0.178 0.230 0.183Residual Std. Error 0.217 (df = 47)F Statistic 8.649 ⇤⇤⇤(df = 2; 47) 94.264⇤⇤⇤(df = 3; 1026) 59.616⇤⇤⇤(df = 2; 488) 106.960⇤⇤⇤(df = 3; 977) 248.602⇤⇤⇤(df = 3; 2986)\nNote: ⇤p<0.1;⇤⇤p<0.05;⇤⇤⇤p<0.01\n3542",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7108957171440125
    },
    {
      "name": "Language model",
      "score": 0.5398200750350952
    },
    {
      "name": "Transfer of learning",
      "score": 0.539620578289032
    },
    {
      "name": "Classifier (UML)",
      "score": 0.509671688079834
    },
    {
      "name": "Fine-tuning",
      "score": 0.49118873476982117
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4810336232185364
    },
    {
      "name": "Machine learning",
      "score": 0.4768032729625702
    },
    {
      "name": "Context (archaeology)",
      "score": 0.46232226490974426
    },
    {
      "name": "Natural language processing",
      "score": 0.33066368103027344
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ]
}