{
  "title": "Efficient Transformer for Single Image Super-Resolution",
  "url": "https://openalex.org/W3194042166",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2107287568",
      "name": "Zhisheng Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102445713",
      "name": "Hong Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099928315",
      "name": "Jun-Cheng Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2137939445",
      "name": "Linlin Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W3112326203",
    "https://openalex.org/W1930824406",
    "https://openalex.org/W2121058967",
    "https://openalex.org/W3104028135",
    "https://openalex.org/W2987150909",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W2923898639",
    "https://openalex.org/W3129158555",
    "https://openalex.org/W3113067059",
    "https://openalex.org/W2214802144",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3106615203",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W2964101377",
    "https://openalex.org/W2903417465",
    "https://openalex.org/W2803657611",
    "https://openalex.org/W3049455300",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2986556279",
    "https://openalex.org/W2866634454",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2795024892",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2915130236",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963610452",
    "https://openalex.org/W2954930822",
    "https://openalex.org/W2739757502",
    "https://openalex.org/W2895598217",
    "https://openalex.org/W3099982213",
    "https://openalex.org/W2121927366",
    "https://openalex.org/W2963645458",
    "https://openalex.org/W3163767593",
    "https://openalex.org/W2963182372",
    "https://openalex.org/W2242218935",
    "https://openalex.org/W2747898905",
    "https://openalex.org/W2503339013",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3089400682",
    "https://openalex.org/W2963986095",
    "https://openalex.org/W2999418043",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2953106684",
    "https://openalex.org/W3105328221",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W1885185971",
    "https://openalex.org/W2964125708"
  ],
  "abstract": "Single image super-resolution task has witnessed great strides with the development of deep learning. However, most existing studies focus on building a more complex neural network with a massive number of layers, bringing heavy computational cost and memory storage. Recently, as Transformer yields brilliant results in NLP tasks, more and more researchers start to explore the application of Transformer in computer vision tasks. But with the heavy computational cost and high GPU memory occupation of the vision Transformer, the network can not be designed too deep. To address this problem, we propose a novel Efficient Super-Resolution Transformer (ESRT) for fast and accurate image super-resolution. ESRT is a hybrid Transformer where a CNN-based SR network is first designed in the front to extract deep features. Specifically, there are two backbones for formatting the ESRT: lightweight CNN backbone (LCB) and lightweight Transformer backbone (LTB). Among them, LCB is a lightweight SR network to extract deep SR features at a low computational cost by dynamically adjusting the size of the feature map. LTB is made up of an efficient Transformer (ET) with a small GPU memory occupation, which benefited from the novel efficient multi-head attention (EMHA). In EMHA, a feature split module (FSM) is proposed to split the long sequence into sub-segments and then these sub-segments are applied by attention operation. This module can significantly decrease the GPU memory occupation. Extensive experiments show that our ESRT achieves competitive results. Compared with the original Transformer which occupies 16057M GPU memory, the proposed ET only occupies 4191M GPU memory with better performance.",
  "full_text": "Transformer for Single Image Super-Resolution\nZhisheng Lu1â€ , Juncheng Li2â€ , Hong Liu1*, Chaoyan Huang3, Linlin Zhang1, Tieyong Zeng2\n1Peking University Shenzhen Graduate School 2The Chinese University of Hong Kong\n3Nanjing University of Posts and Telecommunications\n{zhisheng lu, hongliu, catherinezll}@pku.edu.cn\ncvjunchengli@gmail.com, Huangchy2020@163.com, zeng@math.cuhk.edu.hk\nAbstract\nSingle image super-resolution (SISR) has witnessed\ngreat strides with the development of deep learning. How-\never, most existing studies focus on building more complex\nnetworks with a massive number of layers. Recently, more\nand more researchers start to explore the application of\nTransformer in computer vision tasks. However, the heavy\ncomputational cost and high GPU memory occupation of\nthe vision Transformer cannot be ignored. In this paper,\nwe propose a novel Efï¬cient Super-Resolution Transformer\n(ESRT) for SISR. ESRT is a hybrid model, which consists\nof a Lightweight CNN Backbone (LCB) and a Lightweight\nTransformer Backbone (LTB). Among them, LCB can dy-\nnamically adjust the size of the feature map to extract deep\nfeatures with a low computational costs. LTB is composed\nof a series of Efï¬cient Transformers (ET), which occupies a\nsmall GPU memory occupation, thanks to the specially de-\nsigned Efï¬cient Multi-Head Attention (EMHA). Extensive\nexperiments show that ESRT achieves competitive results\nwith low computational cost. Compared with the original\nTransformer which occupies 16,057M GPU memory, ESRT\nonly occupies 4,191M GPU memory. All codes are avail-\nable at https://github.com/luissen/ESRT.\n1. Introduction\nSingle image super-resolution (SISR) aims at recover-\ning a super-resolution (SR) image from its degraded low-\nresolution (LR) counterpart, which is a useful technol-\nogy to overcome resolution limitations in many applica-\ntions. However, it still is an ill-posed problem since there\nexist inï¬nite HR images. To address this issue, numer-\nous deep neural networks have been proposed [10, 13,\n18, 21, 22, 26, 39, 40, 45]. Although these methods have\nachieved outstanding performance, they cannot be easily\nutilized in real applications due to high computation cost\n*Corresponding author â€ Co-ï¬rst authors\nFigure 1. Examples of similar patches in images. These similar\npatches can help restore details from each other.\nand memory storage. To solve this problem, many recurrent\nnetworks and lightweight networks have been proposed,\nsuch as DRCN [19], SRRFN [23], IMDN [16], IDN [17],\nCARN [2],ASSLN [46], MAFFSRN [31], and RFDN [27].\nAll these models concentrate on constructing a more efï¬-\ncient network structure, but the reduced network capacity\nwill lead to poor performance.\nAs Figure 1 shows, the inner areas of the boxes with the\nsame color are similar to each other. Therefore, these simi-\nlar image patches can be used as reference images for each\nother, so that the texture details of the certain patch can be\nrestored with reference patches. Inspired by this, we in-\ntroduce the Transformer into the SISR task since it has a\nstrong feature expression ability to model such a long-term\ndependency in the image. In other words, we aim to explore\nthe feasibility of using Transformer in the lightweight SISR\ntask. In recent years, some Vision-Transformer [11, 28]\nhave been proposed for computer vision tasks. However,\nthese methods often occupy heavy GPU memory, which\ngreatly limits their ï¬‚exibility and application scenarios.\nMoreover, these methods cannot be directly transferred to\nSISR since the image restoration task often take a larger\nresolution image as input, which will take up huge memory.\nTo solve the aforementioned problems, an Efï¬cient\nSuper-Resolution Transformer (ESRT) is proposed to en-\nhance the ability of SISR networks to capture the long-\ndistance context-dependence while signiï¬cantly decreasing\nthe GPU memory cost. It is worth noting that ESRT is a hy-\narXiv:2108.11084v3  [cs.CV]  22 Apr 2022\nbrid architecture, which uses a â€œCNN+Transformerâ€ pattern\nto handle the small SR dataset. Speciï¬cally, ESRT can be\ndivided into two parts: Lightweight CNN Backbone (LCB)\nand Lightweight Transformer Backbone (LTB). For LCB,\nwe consider more on reducing the shape of the feature map\nin the middle layers and maintaining a deep network depth\nto ensure large network capacity. Inspired by the high-pass\nï¬lter, we design a High-frequency Filtering Module (HFM)\nto capture the texture details of the image. With the aid of\nHFM, a High Preserving Block (HPB) is proposed to extract\nthe potential features efï¬ciently by size variation. For fea-\nture extraction, a powerful Adaptive Residual Feature Block\n(ARFB) is proposed as the basic feature extraction unit with\nthe ability to adaptively adjust the weight of the residual\npath and identity path. In LTB, an efï¬cient Transformer\n(ET) is proposed, which use the specially designed Efï¬cient\nMulti-Head Attention (EMHA) mechanism to decrease the\nGPU memory consumption. It is worth noting that EMHA\njust considers the relationship between image blocks in a lo-\ncal region since the pixel in SR image is commonly related\nto its neighbor pixels. Even though it is a local region, it is\nmuch wider than a regular convolution and can extract more\nuseful context information. Therefore, ESRT can learn the\nrelationship between similar local blocks efï¬ciently, mak-\ning the super-resolved region have more references. The\nmain contributions are as follows\nâ€¢ We propose a Lightweight CNN Backbone (LCB),\nwhich use High Preserving Blocks (HPBs) to dynami-\ncally adjust the size of the feature map to extract deep\nfeatures with a low computational cost.\nâ€¢ We propose a Lightweight Transformer Backbone\n(LTB) to capture long-term dependencies between\nsimilar patches in an image with the help of the spe-\ncially designed Efï¬cient Transformer (ET) and Efï¬-\ncient Multi-Head Attention (EMHA) mechanism.\nâ€¢ A novel model called Efï¬cient SR Transformer\n(ESRT) is proposed to effectively enhance the fea-\nture expression ability and the long-term dependence\nof similar patches in an image, so as to achieve better\nperformance with low computational cost.\n2. Related Works\n2.1. CNN-based SISR Models\nRecently, many CNN-base models have been proposed\nfor SISR. For example, SRCNN [10] ï¬rst introduces\nthe deep CNN into SISR and achieves promising re-\nsults. EDSR [26] optimizes the residual block by remov-\ning unnecessary operations and expanding the model size.\nRCAN [44] proposes a deep residual network with residual-\nin-residual architecture and channel attention mechanism.\nSAN [9] presents a second-order attention network to en-\nhance the feature expression and feature correlation learn-\ning. IDN [17] compresses the model size by using the group\nconvolution and combining short-term and long-term fea-\ntures. IMDN [16] improves the architecture of IDN and\nintroduces the information multi-distillation blocks to ex-\ntract the hierarchical features effectively. LatticeNet [29]\ndesigns the lattice block that simulates the realization of\nFast Fourier Transformation with the butterï¬‚y structure. Al-\nthough these models achieved competitive results, they are\npure CNN-based model. This means that they can only ex-\ntract local features and cannot learn the global information,\nwhich is not conducive to the restoration of texture details.\n2.2. Vision Transformer\nThe breakthroughs of Transformer in NLP has leaded\nto a great interest in the computer vision community. The\nkey idea of Transformer is â€œself-attentionâ€, which can cap-\nture long-term information between sequence elements. By\nadapting Transformer in vision tasks, it has been success-\nfully applied in image recognition [11,24,36], object detec-\ntion [7,47], and low-level image processing [8,41]. Among\nthem, ViT [11] is the ï¬rst work to replace the standard\nconvolution with Transformer. To produce the sequence\nelements, ViT ï¬‚attened the 2D image patches in a vec-\ntor and fed them into the Transformer. IPT [8] used a\nnovel Transformer-based network as the pre-trained model\nfor low-level image restoration tasks. SwinIR [25] intro-\nduced Swin Transformer [28] into SISR and show the great\npromise of Transformer in SISR. Although these methods\nachieved promising results, they require a lot of training\ndata and need heavy GPU memory to train the model, which\nis not suitable for practical applications. Hence, we aim to\nexplore a more efï¬cient vision-Transformer for SISR.\n3. Efï¬cient Super-Resolution Transformer\nAs shown in Figure 2, Efï¬cient Super-Resolution Trans-\nformer (ESRT) mainly consists of four parts: shallow\nfeature extraction, Lightweight CNN Backbone (LCB),\nLightweight Transformer Backbone (LTB), and image re-\nconstruction. Deï¬ne ILR and ISR as the input and output\nof ESRT, respectively. Firstly, we extract the shallow fea-\nture from ILR with a convolutional layer\nF0 = fs(ILR), (1)\nwhere fs denotes the shallow feature extraction layer. F0\nis the extracted shallow feature, which is then used as the\ninput of LCB with several High Preserving Blocks (HPBs)\nFn = Î¶n(Î¶nâˆ’1(...(Î¶1(F0)))), (2)\nwhere Î¶n denotes the mapping of n-th HPB and Fn repre-\nsents the output of n-th HPB. All outputs of HPB are con-\nElement-wise \nSum\nLCB LTB\nHPB\nHPBâ€¦\nET\nETâ€¦\nConv-3\nPixel-\nshuffle\nConv-3\nPixel-\nshuffle\nConv-3\nConv-3\nFigure 2. The architecture of the proposed Efï¬cient Super-Resolution Transformer. Among them, LCB, LTB, HPB, and ET stand for the\nLightweight CNN Backbone, the Lightweight Transformer Backbone, high preserving block, and efï¬cient Transformers, respectively.\ncatenated to be sent to LTB with several Efï¬cient Trans-\nformers (ETs) to fuse these intermediate features\nFd = Ï†n(Ï†nâˆ’1(...(Ï†1([F1,F2,...,F n])))), (3)\nwhere Fd is the output of LTB andÏ†stands for the operation\nof ET. Finally, Fd and F0 are simultaneously fed into the\nreconstruction module to get the SR image ISR\nISR = f(fp(f(Fd))) +f(fp(F0)), (4)\nwhere f and fp stand for the convolutional layer and Pixel-\nShufï¬‚e layer, respectively.\n3.1. Lightweight CNN Backbone (LCB)\nThe role of Lightweight CNN Backbone (LCB) is to ex-\ntract potential SR features in advance, so that the model has\nthe initial ability of super-resolution. According to Figure 2,\nwe can observe that LCB is mainly composed of a series of\nHigh Preserving Blocks (HPBs).\nHigh Preserving Block (HPB). Previous SR networks\nusually keep the spatial resolution of feature maps un-\nchanged during processing. In this work, in order to re-\nduce the computational cost, a novel High Preserving Block\n(HPB) is proposed to reduce the resolution of processing\nfeatures. However, the reduction of the size of feature maps\nalways leads to the loss of image details, which causes vi-\nsually unnatural SR images. To solve this problem, in HPB,\nwe creatively preserve the High-frequency Filtering Module\n(HFM) and Adaptive Residual Feature Block (ARFB).\nAs shown in Figure 3, an ARFB is ï¬rst use to extract\nFnâˆ’1 as the input features for HFM. Then, HFM is used to\ncalculate the high-frequency information (marked asPhigh)\nof the features. After the Phigh is obtained, we reduce the\nsize of the feature map to reduce computational cost and\nfeature redundancy. The downsampled feature maps are de-\nnoted as F\nâ€²\nnâˆ’1. For F\nâ€²\nnâˆ’1, several ARFBs are utilized to\nexplore the potential information for completing the SR im-\nage. It is worth noting that these ARFBs share weights to\nreduce parameters. Meanwhile, a single ARFB is used to\nARFB\nHFM\nARFB\nARFB\nğ‘ƒğ‘ƒhigh\nARFB\nConv-1\nARFB\nÃ— 2/2\nconcat\nCAğ¹ğ¹ğ‘›ğ‘›âˆ’1\nARFB\nARFB\nshare weights\nğ¹ğ¹ğ‘›ğ‘›âˆ’1\nâ€²\nğ¹ğ¹ğ‘›ğ‘›âˆ’1\nâ€²â€²\nğ¹ğ¹ğ‘›ğ‘›\nğ‘ƒğ‘ƒâ„ğ‘–ğ‘–ğ‘–ğ‘–â„\nâ€²\nARFB\nFigure 3. The architecture of the proposed High Preserving Block\n(HPB), which mainly consists of High-frequency Filtering Module\n(HFM) and Adaptive Residual Feature Blocks (ARFBs).\nprocess the Phigh to align the feature space with F\nâ€²\nnâˆ’1. Af-\nter feature extraction,F\nâ€²\nnâˆ’1 is upsampled to the original size\nby bilinear interpolation. After that, we fuse the F\nâ€²\nnâˆ’1 with\nP\nâ€²\nhigh for preserving the initial details and obtain the feature\nF\nâ€²â€²\nnâˆ’1. This operation can be expressed as\nF\nâ€²â€²\nnâˆ’1 = [fa(Phigh),â†‘fâŸ³5\na (â†“F\nâ€²\nnâˆ’1)], (5)\nwhere â†‘and â†“denote the upsampling and downsampling\noperations, respectively. fa denotes the operation of ARFB.\nTo achieve good trade-off between the model size and per-\nformance, we use ï¬ve ARFBs in this part according to ab-\nlation studies and deï¬ne it as fâŸ³5\na .\nFor F\nâ€²â€²\nnâˆ’1, as it is concatenated by two features, a 1 Ã—1\nconvolution layer is used to reduce the channel number.\nThen, a channel attention module [14] is employed to high-\nlight channels with high activated values. Finally, an ARFB\nis used to extract the ï¬nal features and the global residual\nconnection is proposed to add the original features Fnâˆ’1 to\nFn. The goal of this operation is to learn the residual infor-\nmation from the input and stabilize the training.\n3.2. High-frequency Filtering Module (HFM)\nSince the Fourier Transform is difï¬cult to embed in\nCNN, a differentiable HFM is proposed in this work. The\ntarget of HFM is to estimate the high-frequency information\nof the image from the LR space. As shown in Figure 4, as-\nsuming the size of the input feature mapTL is CÃ—HÃ—W,\nğ¶ğ¶ Ã— ğ»ğ» Ã— ğ‘Šğ‘Š ğ¶ğ¶ Ã— ğ»ğ» Ã— ğ‘Šğ‘Šğ¶ğ¶ Ã— ğ»ğ»\nğ‘˜ğ‘˜ Ã— ğ‘Šğ‘Š\nğ‘˜ğ‘˜\nInput\nOutput\nTL\nTA\nTU\nAvgPool Upsampling\nFigure 4. The schematic diagram of the proposed HFM module.\nTL\nTU High-frequency \ninformation\nLR\nFigure 5. Visual activation maps of TL, TU, and obtained high-\nfrequency information. Best viewed in color.\nan average pooling layer is ï¬rst applied to TL:\nTA = avgpool(TL,k), (6)\nwhere kdenotes the kernel size of the pooling layer and the\nsize of the intermediate feature mapTA is CÃ—H\nk Ã—W\nk . Each\nvalue in TA can be viewed as the average intensity of each\nspeciï¬ed small area of TL. After that, TA is upsampled to\nget a new tensor TU of size CÃ—HÃ—W. TU is regarded as\nan expression of the average smoothness information com-\npared with the originalTL. Finally, TU is element-wise sub-\ntracted from TL to obtain the high-frequency information.\nThe visual activation maps of TL, TU , and high-\nfrequency information are also shown in Figure 5. It can\nbe observed that the TU is more smooth than TL as it is\nthe average information of the TL. Meanwhile, the high-\nfrequency information retains the details and edges of the\nfeature map before downsampling. Therefore, it is essential\nto save these information.\n3.2.1 Adaptive Residual Feature Block (ARFB)\nAs explored in ResNet [12] and VDSR [18], when the depth\nof the model grows, the residual architecture can mitigate\nthe gradient vanishing problem and augment the represen-\ntation capacity of the model. Inspired by them, a Adaptive\nResidual Feature Block (ARFB) is proposed as the basic\nRU\nRU\nconcat\nConv-1\nğ‘¥ğ‘¥\nğ¶ğ¶ Ã— ğ»ğ»Ã— ğ‘Šğ‘Š\nğ¶ğ¶ Ã— ğ»ğ»Ã— ğ‘Šğ‘Š\n2ğ¶ğ¶ Ã— ğ»ğ»Ã— ğ‘Šğ‘Š\nConv-3\n2ğ¶ğ¶ Ã— ğ»ğ»Ã— ğ‘Šğ‘Š\nğ‘¦ğ‘¦\nReduction\nExpansion\nğ¶ğ¶\n2 Ã— ğ»ğ»Ã— ğ‘Šğ‘Š\nğ¶ğ¶ Ã— ğ»ğ»Ã— ğ‘Šğ‘Š\nğœ†ğœ†ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿ\nğœ†ğœ†ğ‘¥ğ‘¥\nResidual Unit\nğœ†ğœ†ğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğ‘Ÿğœ†ğœ†ğ‘¥ğ‘¥\nğ¶ğ¶ Ã— ğ»ğ»Ã— ğ‘Šğ‘Š\nFigure 6. The complete architecture of the proposed ARFB.\nfeature extraction block. As shown in Figure 6, ARFB con-\ntains two Residual Units (RUs) and two convolutional lay-\ners. To save memory and the number of parameters, RU is\nmade up of two modules: Reduction and Expansion. For\nReduction, the channels of the feature map are reduced by\nhalf and recovered in Expansion. Meanwhile, a residual\nscaling with adaptive weights (RSA) is designed to dynam-\nically adjust the importance of residual path and identity\npath. Compared with ï¬xed residual scaling, RSA can im-\nprove the ï¬‚ow of gradients and automatically adjust the con-\ntent of the residual feature maps for the input feature map.\nAssume that xru is the input of RU, the process of RU can\nbe formulated as:\nyru = Î»res Â·fex(fre(xru)) +Î»x Â·x, (7)\nwhere yru is the output of RU, fre and fex represent the\nReduction and Expansion operations, Î»res and Î»x are two\nadaptive weights for two paths, respectively. These two op-\nerations use 1Ã—1 convolutional layers to change the number\nof channels to achieve the functions of reduction and expan-\nsion. Meanwhile, the outputs of two RUs are concatenated\nfollowed by a 1 Ã—1 convolutional layer to fully utilize the\nhierarchical features. In the end, a 3Ã—3 convolutional layer\nis adopted to reduce the channels of the feature map and\nextract valid information from the fused features.\n3.3. Lightweight Transformer Backbone (LTB)\nIn SISR, similar image blocks within the image can be\nused as reference images to each other, so that the tex-\nture details of the current image block can be restored with\nreference to other image blocks, which is proper to use\nTransformer. However, previous variants of vision Trans-\nformer commonly need heavy GPU memory cost, which\nhinders the development of Transformer in the vision area.\nIn this paper, we propose a Lightweight Transformer Back-\nbone (LTB). LTB is composed of specially designed Efï¬-\ncient Transformers (ETs), which can capture the long-term\nEfficient \nTransformer\nğµğµÃ— ğ¶ğ¶ Ã— ğ»ğ» Ã— ğ‘Šğ‘Š\nFeature Unfold\nâ€¦\nğµğµ Ã— (ğ‘˜ğ‘˜âˆ—ğ‘˜ğ‘˜ âˆ—ğ¶ğ¶) Ã— ğ»ğ»ğ‘Šğ‘Š\nğ‘˜ğ‘˜ Ã— ğ‘˜ğ‘˜ kernel\nâ€¦\nğµğµ Ã— (ğ‘˜ğ‘˜âˆ—ğ‘˜ğ‘˜ âˆ—ğ¶ğ¶) Ã— ğ»ğ»ğ‘Šğ‘Š\nFeature Fold\nğ‘˜ğ‘˜ Ã— ğ‘˜ğ‘˜ kernel\nğµğµÃ— ğ¶ğ¶ Ã— ğ»ğ» Ã— ğ‘Šğ‘Š\nFigure 7. The complete pre- and post-processing for the Efï¬cient Transformer (ET). Speciï¬cally, we use the unfolding technique to split\nthe feature maps into patches and use the fold operation to reconstruct the feature map.\ndependence of similar local regions in the image at a low\ncomputational cost.\n3.3.1 Pre- and Post-processing for ET\nThe standard Transformer takes a 1-D sequence as input,\nlearning the long-distance dependency of the sequence.\nHowever, for the vision task, the input is always a 2-D im-\nage. The common way to turn a 2-D image into a 1-D se-\nquence is to sort the pixels in the image one by one. How-\never, this method will lose the unique local correlation of the\nimage, leading to sub-optimal performance. In ViT [11], the\n1-D sequence is generated by non-overlapping block parti-\ntioning, which means there is no pixel overlap between each\nblock. According to our experiments, these pre-processing\nmethods are not suitable for SISR. Therefore, a novel pro-\ncessing way is proposed to handle the feature maps.\nAs shown in Figure 7, we use the unfolding technique\nto split the feature maps into patches and each patch is\nconsidered as a â€œwordâ€. Speciï¬cally, the feature maps\nFori âˆˆ RCÃ—HÃ—W are unfolded (by k Ã—k kernel) into a\nsequence of patches, i.e., Fpi âˆˆRk2Ã—C, i = {1,...,N },\nwhere N = H Ã—W is the amount of patches. Here, the\nlearnable position embeddings are eliminated for each patch\nsince the â€œUnfoldâ€ operation automatically reï¬‚ects the po-\nsition information for each patch. After that, those patches\nFp are directly sent to the ET. The output of ET has the\nsame shape as the input and we use the â€œFoldâ€ operation to\nreconstruct feature maps.\n3.3.2 Efï¬cient Transformer (ET)\nFor simplicity and efï¬ciency, like ViT [11], ET only uses\nthe encoder structure of the standard Transformer. As\nshown in Figure 8, in the encoder of ET, there consists of\nan Efï¬cient Multi-Head Attention (EMHA) and an MLP.\nMeanwhile, layer-normalization [4] is employed before ev-\nery block, and the residual connection is also applied after\neach block. Assume the input embeddings are Ei, the out-\nput embeddings Eo can be obtained by\nEm1 = EMHA(Norm(Ei)) +Ei,\nEo = MLP(Norm(Em1)) +Em1, (8)\nwhere Eo is the output of the ET, EMHA(Â·) and MLP(Â·)\nrepresent the EMHA and MLP operations, respectively.\nInput \nEmbeddings\nNorm\nEMHA\nOutput \nEmbeddings\nEfficient Transformer\nReduction\nLinear\nQ K V\nFeature Split\nV1, V2,â€¦, V s\nScaled Dot-Product \nAttention\nConcat\nExpansion\nEfficient Multi-Head Attention\nQi Ki Vi\nMatMul\nScale\nSoftmax\nMatMul\nNorm\nMLP\nQ1, Q2,â€¦, Q s\nK1, K2,â€¦, K s\nOi\nO1, O2,â€¦, Os\nFigure 8. Architecture of Efï¬cient Transformer. EMHA is the Ef-\nï¬cient Multi-Head Attention. MatMul is the matrix multiplication.\nEfï¬cient Multi-Head Attention (EMHA) . As shown\nin Figure 8, there are several modiï¬cations in EMHA to\nmake EMHA more efï¬cient and occupy lower GPU mem-\nory cost compared with the original MHA [37]. Assume\nthe shape of the input embedding Ei is BÃ—CÃ—N. Firstly,\na Reduction layer is used to reduce the number of chan-\nnels by half ( B Ã—C1 Ã—N,C1 = C\n2 ). After that, a linear\nlayer is adopted to project the feature map into three ele-\nments: Q(query), K (keys), and V (values). As employed\nin Transformer, we linearly project the Q, K, V, and m\ntimes to perform the multi-head attention. m is the num-\nber of heads. Next, the shape of three elements is reshaped\nand permuted to BÃ—mÃ—N Ã—C1\nm . In original MHA, Q,\nK, V are directly used to calculate the self-attention with\nlarge-scale matrix multiplication, which cost huge memory.\nAssume Q and K calculate the self-attention matrix with\nshape BÃ—mÃ—NÃ—N. Then this matrix computes the self-\nattention with V, the dimension in 3-th and 4-th are NÃ—N.\nFor SISR, the images usually have high resolution, caus-\ning that N is very large and the calculation of self-attention\nmatrix consumes a lot of GPU memory cost and computa-\ntional cost. To address this issue, a Feature Split (FS) Mod-\nule is used to split Q, K, and V into sequal segments with\nsplitting factor ssince the predicted pixels in super-resolved\nimages often only depend on the local adjacent areas in LR.\nTherefore, the dimension in 3-th and 4-th of the last self-\nmatrix is N\ns Ã—N\ns , which can signiï¬cantly reduce the com-\nputational and GPU memory cost. Denote these segments\nas Q1,...,Q s, K1,...,K s, and V1,...,V s. Each triplet of\nthese segments is applied with a Scaled Dot-Product Atten-\ntion (SDPA) operation, respectively. The structure of SDPA\nis also shown in Figure 8, which just omits the Mask oper-\nMethod Scale Params Set5 Set14 BSD100 Urban100 Manga109\nPSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM PSNR / SSIM\nVDSR [18]\nÃ—3\n666K 33.66 / 0.9213 29.77 / 0.8314 28.82 / 0.7976 27.14 / 0.8279 32.01 / 0.9340\nMemNet [34] 678K 34.09 / 0.9248 30.00 / 0.8350 28.96 / 0.8001 27.56 / 0.8376 32.51 / 0.9369\nEDSR-baseline [26] 1,555K 34.37 / 0.9270 30.28 / 0.8417 29.09 / 0.8052 28.15 / 0.8527 33.45 / 0.9439\nSRMDNF [43] 1,528K 34.12 / 0.9254 30.04 / 0.8382 28.97 / 0.8025 27.57 / 0.8398 33.00 / 0.9403\nCARN [2] 1,592K 34.29 / 0.9255 30.29 / 0.8407 29.06 / 0.8034 28.06 / 0.8493 33.50 / 0.9440\nIMDN [16] 703K 34.36 / 0.9270 30.32 / 0.8417 29.09 / 0.8046 28.17 / 0.8519 33.61 / 0.9445\nRFDN-L [27] 633K 34.47 / 0.9280 30.35 / 0.8421 29.11 / 0.8053 28.32 / 0.8547 33.78 / 0.9458\nMAFFSRN [31] 807K 34.45 / 0.9277 30.40 / 0.8432 29.13 / 0.8061 28.26 / 0.8552 - / -\nLatticeNet [29] 765K 34.53 / 0.9281 30.39 / 0.8424 29.15 / 0.8059 28.33 / 0.8538 - / -\nESRT(ours) 770K 34.42 / 0.9268 30.43 / 0.8433 29.15 / 0.8063 28.46 / 0.8574 33.95 / 0.9455\nVDSR [18]\nÃ—4\n666K 31.35 / 0.8838 28.01 / 0.7674 27.29 / 0.7251 25.18 / 0.7524 28.83 / 0.8870\nMemNet [34] 678K 31.74 / 0.8893 28.26 / 0.7723 27.40 / 0.7281 25.50 / 0.7630 29.42 / 0.8942\nEDSR-baseline [26] 1,518K 32.09 / 0.8938 28.58 / 0.7813 27.57 / 0.7357 26.04 / 0.7849 30.35 / 0.9067\nSRMDNF [43] 1,552K 31.96 / 0.8925 28.35 / 0.7787 27.49 / 0.7337 25.68 / 0.7731 30.09 / 0.9024\nCARN [2] 1,592K 32.13 / 0.8937 28.60 / 0.7806 27.58 / 0.7349 26.07 / 0.7837 30.47 / 0.9084\nIMDN [16] 715K 32.21 / 0.8948 28.58 / 0.7811 27.56 / 0.7353 26.04 / 0.7838 30.45 / 0.9075\nRFDN-L [27] 643K 32.28 / 0.8957 28.61 / 0.7818 27.58 / 0.7363 26.20 / 0.7883 30.61 / 0.9096\nMAFFSRN [31] 830K 32.20 / 0.8953 26.62 / 0.7822 27.59 / 0.7370 26.16 / 0.7887 - / -\nLatticeNet [29] 777K 32.30 / 0.8962 28.68 / 0.7830 27.62 / 0.7367 26.25 / 0.7873 - / -\nESRT (ours) 751K 32.19 / 0.8947 28.69 / 0.7833 27.69 / 0.7379 26.39 / 0.7962 30.75 / 0.9100\nTable 1. Quantitative comparison with SISR models. The Best and the second-best results are highlighted and underlined, respectively.\nation. Afterward, all the outputs ( O1,O2,...,O s) of SDPA\nare concatenated together to generate the whole output fea-\nture O. Finally, an Expansion layer is used to recover the\nnumber of channels.\n4. Experiments\n4.1. Datasets and Metrics\nIn this work, we use DIV2K [35] as the training dataset.\nFor evaluation, we use ï¬ve benchmark datasets, including\nSet5 [5], Set14 [42], BSD100 [30], Urban100 [15], and\nManga109 [3]. Meanwhile, PSNR and SSIM are used to\nevaluate the performance of the reconstructed SR images.\n4.2. Implementation Details\nTraining Setting. During training, we randomly crop16\nLR image patches with the size of 48 Ã—48 as inputs in each\nepoch. Random horizontal ï¬‚ipping and90 degree rotation is\nused for data augment. The initial learning rate is set to 2 Ã—\n10âˆ’4 and decreased half for every 200 epochs. The model is\ntrained by Adam optimizer with a momentum equal to 0.9.\nMeanwhile, L1 loss is used as it can produce more sharp\nimages compared to L2 loss. Meanwhile, we implement\nESRT with the PyTorch framework and train ESRT roughly\ntakes two days with one GTX1080Ti GPU.\nImplements Details. In ESRT, we set 3 Ã—3 as the size\nof all convolutional layers except for the Reduction mod-\nule, whose kernel size is 1 Ã—1. Each convolutional layer\nhas 32 channels except for the fusion layer which is twice.\nFor the image reconstruction part, following most previous\nmethods, we use PixelShufï¬‚e [32] to upscale the last coarse\nfeatures to ï¬ne features. The k in HFP is set to 2, which\nmeans that the feature map is down-scaled by half. Mean-\nwhile, the number of HPB is set to 3, the initial value of\nlearnable weight in ARFB is set to 1, and the number of ET\nin LTB is set to 1 to save the GPU memory. In addition, the\nsplitting factor s in ET is set to 4, the k in pre- and post-\nprocess of ET is set to 3, and the head number min EMHA\nis set to 8, respectively.\n4.3. Comparisons with Advanced SISR Models\nIn TABLE 1, we compare ESRT with other advanced\nSISR models. Obviously, our ESRT achieves competitive\nresults under all scaling factors. It can be seen that although\nthe performance of EDSR-baseline is close to ESRT, it has\nalmost twice as many parameters as ESRT. Meanwhile, the\nnumber of MAFFSRN and LatticeNet is close to ESRT,\nbut ESRT achieves better results than them. Moreover, we\ncan observe that our ESRT performs much better than other\nmodels on Urban100. This is because there are many sim-\nilar patches in each image of this dataset. Therefore, the\nintroduced LTB in our ESRT can used to capture the long-\nterm dependencies among these similar image patches and\nlearn their relevance, thus achieve better results.\nIn Figure 9, we also provide the visual comparison be-\ntween ESRT and other lightweight SISR models on Ã—2,\nÃ—3, and Ã—4. Obviously, SR images reconstructed by our\nESRT contains more accurate texture details, especially in\nthe edges and lines. It is worth noting that in the Ã—4 scale,\nthe gap between ESRT and other SR models is more appar-\nent. This beneï¬ts from the effectiveness of the proposed Ef-\nï¬cient Transformer, which can learn more information from\nother clear areas. All these experiments validate the effec-\ntiveness of the proposed ESRT.\nFigure 9. Visual comparison with lightweight SISR models. Obviously, ESRT can reconstruct realistic SR images with sharper edges.\nMethod Layers RL Param. FLOPs (x4) Running time\nVDSR [18] 20 Yes 0.67M 612.6G 0.00597s\nLapSRN [20] 27 Yes 0.25M 149.4G 0.00330s\nDRRN [33] 52 No 0.30M 6796.9G 0.08387s\nCARN [2] 34 Yes 1.6M 90.9G 0.00278s\nIMDN [16] 34 Yes 0.7M 40.9G 0.00258s\nESRT 163 Yes 0.68M 67.7G 0.01085s\nTable 2. Network structure settings comparison between our ESRT\nand other lightweight SISR models (the input size is 1280 Ã—720).\n4.4. Comparison on Computational Cost\nIn TABLE 2, we provide a more detailed comparison\nof each model. It can be seen that ESRT can achieve 163\nlayers while still achieves the second-least FLOPs (67.7G)\namong these methods. This is beneï¬ted from the pro-\nposed HPB and ARFB, which can efï¬ciently extract use-\nful features and preserving the high-frequency informa-\ntion. Meanwhile, we can observe that the execution time is\nshort even though ESRT uses the Transformer architecture.\nThe increased time is completely acceptable compared to\nCARN and IMDN. In addition, we also visualize the trade-\noff analysis between the number of model parameters and\nperformance in Figure 10. Obviously, we can see that our\nERST achieves a good trade-off between the size and per-\nformance of the model.\n4.5. Network Investigations\n4.5.1 Study of High Preserving Block (HPB)\nHPB is an important component of ESRT, which not only\ncan reduce the model size but maintain the high perfor-\nmance of the model. To prove it, we explore the effective-\nness of each component of ESRT in TABLE 3. According to\nCase Index 1 2 3 4\nHFM âˆš âˆš âˆš\nCA âˆš âˆš âˆš\nARFB âˆš âˆš âˆš\nRB âˆš\nParameters 658K 751K 724K 972K\nPSNR 32.02dB 32.19dB 32.08dB 32.20dB\nTable 3. Study of each component in HPB on Set5 (Ã—4).\n200 400 600 800 1000 1200 1400 1600 1800 2000\nNumber of parameters (K)\n29.4\n29.8\n30.2\n30.6\n31.0\n31.4\n31.8\n32.2\n32.6PSNR (dB)\nLapSRN\nSRCNN\nMemNet\nDRCN\nDRRN\nVDSR\nSRMDNF\nCARN\nIMDN EDSR-baseline\nESRT\nFigure 10. Study the trade-off between the number of model pa-\nrameters and performance on Urban100 (Ã—2).\nCases 1, 2, and 3, we can observe that the introduced HFM\nand CA can effectively improve the model performance at\nthe cost of a few parameters. According to Cases 2 and 4,\nwe can see that if RB is used to represent ARFB, the PSNR\nresult just rises 0.01dB but the number of parameters go up\nto 972K. This means that ARFB can signiï¬cantly reduce\nthe model parameters while maintaining excellent perfor-\nmance. All these results fully illustrate the necessity and\neffectiveness of these modules and mechanisms in HPB.\nCase PSNR(dB) Parame.(K) GPU Memory\nw/o TR 31.96 554 1931M\nOriginal TR [38] 32.14 971 16057M\n1 ET 32.18 751 4191M\n2 ET 32.25 949 6499M\nTable 4. Study of Efï¬cient Transformer (ET) on Set5 ( Ã—4). The\nGPU memory here refers to the cost of the model during training,\nwhich patch size = 48*48 and batch size=16.\nScale Model Param Set5 Set14 Urban100\nÃ—3 RCAN [44] 16M 34.74dB 30.65dB 29.09dB\nRCAN/2+ET 8.7M 34.69dB 30.63dB 29.16dB\nÃ—4 RCAN [44] 16M 32.63dB 28.87dB 26.82dB\nRCAN/2+ET 8.7M 32.60dB 28.90dB 26.87dB\nTable 5. Comparison between RCAN and RCAN/2+ET.\n4.5.2 Study of Efï¬cient Transformer (ET)\nTo capture the long-term dependencies of similar local re-\ngions in the image, we introduced the Transformer and pro-\nposed a Efï¬cient Transformer (ET). In TABLE 4, we ana-\nlyze the model with and without Transformer. It can be see\nthat if ESRT removes the Transformer, the model perfor-\nmance descends obviously from 32.18dB to 31.96dB. This\nis because the introduced Transformer can make full advan-\ntage of the relationship between similar image patches in an\nimage. In addition, we compare our ET with the original\nTransformer [11] in the table. Our model (1ET) achieves\nbetter results with fewer parameters and GPU memory con-\nsumption (1/4). This experiment fully veriï¬ed the effective-\nness of the proposed ET. Meanwhile, we can also see that\nwhen the number of ET increases, the model performance\nwill be further improved. However, it is worth noting that\nthe model parameters and GPU memory will also increase\nwhen the number of ET increases. Therefore, to achieve\na good balance between the size and performance of the\nmodel, only one ET is used in the ï¬nal ESRT.\nIn order to verify the effectiveness and universality of\nthe proposed ET, we also introduce ET into RCAN [44]. It\nis worth noting that we use a small version of RCAN (the\nresidual group number is set to 5) and add the ET before\nthe reconstruction part. According to TABLE 5, we can see\nthat the performance of the model â€œRCAN/2+ETâ€ is close\nor even better than the original RCAN with fewer parame-\nters. This further proves the effectiveness and universality\nof ET, which can be easily transplanted to any existing SISR\nmodels to further improve the performance of model.\n4.6. Real Image Super-Resolution\nTo further verify the validity of the model, we also com-\npare our ESRT with some classic lightweight SR models\non the real image dataset (RealSR [6]). According to TA-\nBLE 6, we can observe that ESRT achieves better results\nthan IMDN. In addition, ESRT achieves better performance\nScale IMDN [16] LK-KPN [6] ESRT (Ours)\nPSNR SSIM PSNR SSIM PSNR SSIM\nÃ—3 30.29 0.857 30.60 0.863 30.38 0.857\nÃ—4 28.68 0.815 28.65 0.820 28.78 0.815\nTable 6. Comparison with advanced SISR methods on RealSR.\nMethod Parame. GPU Memory BSD100 Manga109\nSwinIR 886K 6966M 29.20/0.8082 33.98/0.9478\nESRT 770K 4191M 29.15/0.8063 33.95/0.9455\nTable 7. A detailed comparison of SwinIR and ESRT (Ã—4).\nthan LK-KPN on Ã—4, which was speciï¬cally designed for\nthe real SR task. This experiment further veriï¬es its effec-\ntiveness on real images.\n4.7. Comparison with SwinIR\nThe EMHA in our ESRT is similar to the Swin Trans-\nformer layer of SwinIR [25]. However, SwinIR uses a\nsliding window to solve the high computation problem of\nthe Transformer while ESRT uses a splitting factor to re-\nduce the GPU memory consumption. According to Table 7,\ncompared with SwinIR, ESRT achieves close performance\nwith fewer parameters and GPU memory. It is worth noting\nthat SwinIR uses an extra dataset (Flickr2K [1]) for train-\ning, which is the key to further improving the model perfor-\nmance. For a fair comparison with methods such as IMDN,\nwe did not use this external dataset in this work.\n5. Conclusion\nIn this work, we proposed a novel Efï¬cient Super-\nResolution Transformer (ESRT) for SISR. ESRT ï¬rst uti-\nlizes a Lightweight CNN Backbone (LCB) to extract deep\nfeatures and then uses a Lightweight Transformer Backbone\n(LTB) to model the long-term dependence between similar\nlocal regions in an image. In LCB, we proposed a High Pre-\nserving Block (HPB) to reduce the computational cost and\nretain high-frequency information with the help of the spe-\ncially designed High-frequency Filtering Module (HFM)\nand Adaptive Residual Feature Block (ARFB). In LTB, an\nEfï¬cient Transformer (ET) is designed to enhance the fea-\nture representation ability with lower GPU memory occupa-\ntion under the help of the proposed Efï¬cient Multi-head At-\ntention (EMHA). Extensive experiments demonstrate that\nESRT achieves the best trade-off between model perfor-\nmance and computation cost.\n6. Acknowledgment\nThis work was supported by National Key R&D Pro-\ngram of China (2021YFE0203700), CRF (8730063), Na-\ntional Natural Science Foundation of China (U1613209),\nand Science and Technology Plan Projects of Shenzhen\n(JCYJ20190808182209321, JCYJ20200109140410340).\nReferences\n[1] Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge\non single image super-resolution: Dataset and study. In\nCVPRW, 2017. 8\n[2] N. Ahn, B. Kang, and K. A. Sohn. Fast, accurate, and\nlightweight super-resolution with cascading residual net-\nwork. In ECCV, pages 252â€“268, 2018. 1, 6, 7\n[3] Kiyoharu Aizawa, Azuma Fujimoto, Atsushi Otsubo, Toru\nOgawa, Yusuke Matsui, Koki Tsubota, and Hikaru Ikuta.\nBuilding a manga dataset â€œmanga109â€ with annotations for\nmultimedia applications. IEEE Multimedia , 27(2):8â€“18,\n2020. 6\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 5\n[5] Marco Bevilacqua, Aline Roumy, Christine Guillemot, and\nMarie Line Alberi-Morel. Low-complexity single-image\nsuper-resolution based on nonnegative neighbor embedding.\n2012. 6\n[6] Jianrui Cai, Hui Zeng, Hongwei Yong, Zisheng Cao, and Lei\nZhang. Toward real-world single image super-resolution: A\nnew benchmark and a new model. In ICCV, pages 3086â€“\n3095, 2019. 8\n[7] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In ECCV, pages\n213â€“229, 2020. 2\n[8] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-\ning Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,\nand Wen Gao. Pre-trained image processing transformer. In\nCVPR, pages 12299â€“12310, 2021. 2\n[9] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and\nLei Zhang. Second-order attention network for single im-\nage super-resolution. In CVPR, pages 11065â€“11074, 2019.\n2\n[10] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\nTang. Image super-resolution using deep convolutional net-\nworks. IEEE TPAMI, 38(2):295â€“307, 2015. 1, 2\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 2, 5, 8\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\npages 770â€“778, 2016. 4\n[13] Zewei He, Siliang Tang, Jiangxin Yang, Yanlong Cao,\nMichael Ying Yang, and Yanpeng Cao. Cascaded deep net-\nworks with multiple receptive ï¬elds for infrared image super-\nresolution. IEEE TCSVT, 29(8):2310â€“2322, 2018. 1\n[14] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\nworks. In CVPR, pages 7132â€“7141, 2018. 3\n[15] Jia-Bin Huang, Abhishek Singh, and Narendra Ahuja. Single\nimage super-resolution from transformed self-exemplars. In\nCVPR, pages 5197â€“5206, 2015. 6\n[16] Zheng Hui, Xinbo Gao, Yunchu Yang, and Xiumei Wang.\nLightweight image super-resolution with information multi-\ndistillation network. In ACMMM, pages 2024â€“2032, 2019.\n1, 2, 6, 7, 8\n[17] Zheng Hui, Xiumei Wang, and Xinbo Gao. Fast and accu-\nrate single image super-resolution via information distilla-\ntion network. In CVPR, pages 723â€“731, 2018. 1, 2\n[18] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate\nimage super-resolution using very deep convolutional net-\nworks. In CVPR, pages 1646â€“1654, 2016. 1, 4, 6, 7\n[19] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-\nrecursive convolutional network for image super-resolution.\nIn CVPR, pages 1637â€“1645, 2016. 1\n[20] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-\nHsuan Yang. Fast and accurate image super-resolution\nwith deep laplacian pyramid networks. IEEE TPAMI ,\n41(11):2599â€“2613, 2018. 7\n[21] Juncheng Li, Faming Fang, Kangfu Mei, and Guixu Zhang.\nMulti-scale residual network for image super-resolution. In\nECCV, pages 517â€“532, 2018. 1\n[22] Juncheng Li, Zehua Pei, and Tieyong Zeng. From beginner\nto master: A survey for deep learning-based single-image\nsuper-resolution. arXiv preprint arXiv:2109.14335, 2021. 1\n[23] Juncheng Li, Yiting Yuan, Kangfu Mei, and Faming Fang.\nLightweight and accurate recursive fractal network for image\nsuper-resolution. In ICCVW, 2019. 1\n[24] Yawei Li, Kai Zhang, Jiezhang Cao, Radu Timofte, and Luc\nVan Gool. Localvit: Bringing locality to vision transformers.\narXiv preprint arXiv:2104.05707, 2021. 2\n[25] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc\nVan Gool, and Radu Timofte. Swinir: Image restoration us-\ning swin transformer. InProceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 1833â€“1844,\n2021. 2, 8\n[26] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and\nKyoung Mu Lee. Enhanced deep residual networks for single\nimage super-resolution. In CVPRW, pages 136â€“144, 2017. 1,\n2, 6\n[27] Jie Liu, Jie Tang, and Gangshan Wu. Residual feature dis-\ntillation network for lightweight image super-resolution. In\nECCV, pages 41â€“55, 2020. 1, 6\n[28] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nICCV, 2021. 1, 2\n[29] Xiaotong Luo, Yuan Xie, Yulun Zhang, Yanyun Qu, Cui-\nhua Li, and Yun Fu. Latticenet: Towards lightweight image\nsuper-resolution with lattice block. In ECCV, pages 272â€“\n289, 2020. 2, 6\n[30] David Martin, Charless Fowlkes, Doron Tal, and Jitendra\nMalik. A database of human segmented natural images and\nits application to evaluating segmentation algorithms and\nmeasuring ecological statistics. In ICCV, pages 416â€“423,\n2001. 6\n[31] Abdul Muqeet, Jiwon Hwang, Subin Yang, JungHeum Kang,\nYongwoo Kim, and Sung-Ho Bae. Multi-attention based ul-\ntra lightweight image super-resolution. InECCV, pages 103â€“\n118, 2020. 1, 6\n[32] Wenzhe Shi, Jose Caballero, Ferenc Husz Â´ar, Johannes Totz,\nAndrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan\nWang. Real-time single image and video super-resolution\nusing an efï¬cient sub-pixel convolutional neural network. In\nCVPR, pages 1874â€“1883, 2016. 6\n[33] Ying Tai, Jian Yang, and Xiaoming Liu. Image super-\nresolution via deep recursive residual network. In CVPR,\npages 3147â€“3155, 2017. 7\n[34] Ying Tai, Jian Yang, Xiaoming Liu, and Chunyan Xu. Mem-\nnet: A persistent memory network for image restoration. In\nICCV, pages 4539â€“4547, 2017. 6\n[35] Radu Timofte, Shuhang Gu, Jiqing Wu, and Luc Van Gool.\nNtire 2018 challenge on single image super-resolution:\nMethods and results. In CVPRW, pages 852â€“863, 2018. 6\n[36] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv Â´e JÂ´egou. Training\ndata-efï¬cient image transformers & distillation through at-\ntention. In ICML, pages 10347â€“10357, 2021. 2\n[37] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NeurIPS, pages\n5998â€“6008, 2017. 5\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998â€“6008, 2017. 8\n[39] Yifan Wang, Lijun Wang, Hongyu Wang, and Peihua Li.\nResolution-aware network for image super-resolution. IEEE\nTCSVT, 29(5):1259â€“1269, 2018. 1\n[40] Yifan Wang, Lijun Wang, Hongyu Wang, and Peihua Li.\nResolution-aware network for image super-resolution. IEEE\nTCSVT, 29:1259â€“1269, 2019. 1\n[41] Fuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bain-\ning Guo. Learning texture transformer network for image\nsuper-resolution. In CVPR, pages 5791â€“5800, 2020. 2\n[42] Jianchao Yang, John Wright, Thomas S Huang, and Yi Ma.\nImage super-resolution via sparse representation. IEEE TIP,\n19(11):2861â€“2873, 2010. 6\n[43] Kai Zhang, Wangmeng Zuo, and Lei Zhang. Learning a\nsingle convolutional super-resolution network for multiple\ndegradations. In CVPR, pages 3262â€“3271, 2018. 6\n[44] Yulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng\nZhong, and Yun Fu. Image super-resolution using very deep\nresidual channel attention networks. In ECCV, pages 286â€“\n301, 2018. 2, 8\n[45] Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and\nYun Fu. Residual dense network for image super-resolution.\nIn CVPR, pages 2472â€“2481, 2018. 1\n[46] Yulun Zhang, Huan Wang, Can Qin, and Yun Fu. Aligned\nstructured sparsity learning for efï¬cient image super-\nresolution. NeurIPS, 34, 2021. 1\n[47] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 2",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.763418436050415
    },
    {
      "name": "Transformer",
      "score": 0.7294185161590576
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4992330074310303
    },
    {
      "name": "Deep learning",
      "score": 0.4829237163066864
    },
    {
      "name": "Computer engineering",
      "score": 0.3241792321205139
    },
    {
      "name": "Electrical engineering",
      "score": 0.15514805912971497
    },
    {
      "name": "Voltage",
      "score": 0.14169079065322876
    },
    {
      "name": "Engineering",
      "score": 0.1130877137184143
    }
  ]
}