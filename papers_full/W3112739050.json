{
  "title": "DA-HGT: Domain Adaptive Heterogeneous Graph Transformer.",
  "url": "https://openalex.org/W3112739050",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5059505330",
      "name": "Tiancheng Huang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100665814",
      "name": "Ke Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100665181",
      "name": "Donglin Wang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3173025631",
    "https://openalex.org/W2965857891",
    "https://openalex.org/W3035576098",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W3035237680",
    "https://openalex.org/W2125666396",
    "https://openalex.org/W103340358",
    "https://openalex.org/W3130062726",
    "https://openalex.org/W2125865219",
    "https://openalex.org/W3033623365",
    "https://openalex.org/W3156968278",
    "https://openalex.org/W2970066309",
    "https://openalex.org/W2984834462",
    "https://openalex.org/W2907380995",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3175971420",
    "https://openalex.org/W3175364065",
    "https://openalex.org/W2773963565",
    "https://openalex.org/W2022322548",
    "https://openalex.org/W3116214843",
    "https://openalex.org/W3004507689",
    "https://openalex.org/W2354939339",
    "https://openalex.org/W2611328865",
    "https://openalex.org/W2997964288",
    "https://openalex.org/W2743104969",
    "https://openalex.org/W3012644407",
    "https://openalex.org/W2963826681",
    "https://openalex.org/W2963919031",
    "https://openalex.org/W2159291411",
    "https://openalex.org/W2904510605",
    "https://openalex.org/W1932742904",
    "https://openalex.org/W2954595534",
    "https://openalex.org/W2779610669",
    "https://openalex.org/W2946053549",
    "https://openalex.org/W2965115497",
    "https://openalex.org/W1731081199",
    "https://openalex.org/W3012871709",
    "https://openalex.org/W1548361610",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2904255843",
    "https://openalex.org/W2911286998",
    "https://openalex.org/W2952343887",
    "https://openalex.org/W3173429910",
    "https://openalex.org/W2104094955",
    "https://openalex.org/W2766453196",
    "https://openalex.org/W3041133507",
    "https://openalex.org/W3094215950"
  ],
  "abstract": "Domain adaptation using graph networks is to learn label-discriminative and network-invariant node embeddings by sharing graph parameters. Most existing works focus on domain adaptation of homogeneous networks, and just a few works begin to study heterogeneous cases that only consider the shared node types but ignore the private node types in individual networks. However, for a given source and target heterogeneous networks, they generally contain shared and private node types, where private types bring an extra challenge for graph domain adaptation. In this paper, we investigate Heterogeneous Information Networks (HINs) with partial shared node types and propose a novel domain adaptive heterogeneous graph transformer (DA-HGT) to handle the domain shift between them. DA-HGT can not only align the distributions of identical-type nodes and edges in two HINs but also make full use of different-type nodes and edges to improve the performance of knowledge transfer. Extensive experiments on several datasets demonstrate that DA-HGT can outperform state-of-the-art methods in various domain adaptation tasks across heterogeneous networks.",
  "full_text": "GDA-HIN: A Generalized Domain Adaptive Model across\nHeterogeneous Information Networks\nTiancheng Huangâˆ—\nZhejiang University\nWestlake University\nWestlake Institute for Advanced\nStudy, Hangzhou, China\nhuangtiancheng@westlake.edu.cn\nKe Xuâˆ—\nWestlake University\nWestlake Institute for Advanced\nStudy, Hangzhou, China\nxuke@westlake.edu.cn\nDonglin Wangâ€ \nWestlake University\nWestlake Institute for Advanced\nStudy, Hangzhou, China\nwangdonglin@westlake.edu.cn\nABSTRACT\nDomain adaptation using graph-structured networks learns label-\ndiscriminative and network-invariant node embeddings by sharing\ngraph parameters. Most existing works focus on domain adaptation\nof homogeneous networks. The few works that study heteroge-\nneous cases only consider shared node types but ignore private\nnode types in individual networks. However, for given source and\ntarget heterogeneous networks, they generally contain shared and\nprivate node types, where private types bring an extra challenge\nfor graph domain adaptation. In this paper, we investigate Het-\nerogeneous Information Networks (HINs) with both shared and\nprivate node types and propose a Generalized Domain Adaptive\nmodel across HINs (GDA-HIN) to handle the domain shift between\nthem. GDA-HIN can not only align the distribution of identical-type\nnodes and edges in two HINs but also make full use of different-\ntype nodes and edges to improve the performance of knowledge\ntransfer. Extensive experiments on several datasets demonstrate\nthat GDA-HIN can outperform state-of-the-art methods in various\ndomain adaptation tasks across heterogeneous networks.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Machine learning.\nKEYWORDS\nHeterogeneous Information Networks, Transfer Learning, Domain\nAdaptation, Distribution Alignment\nACM Reference Format:\nTiancheng Huang, Ke Xu, and Donglin Wang. 2022. GDA-HIN: A General-\nized Domain Adaptive Model across Heterogeneous Information Networks.\nIn Proceedings of the 31st ACM International Conference on Information and\nKnowledge Management (CIKM â€™22), October 17â€“21, 2022, Atlanta, GA, USA.\nACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3511808.3557602\nâˆ—Both authors contributed equally to this research.\nâ€ Corresponding author.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nCIKM â€™22, October 17â€“21, 2022, Atlanta, GA, USA.\nÂ© 2022 Association for Computing Machinery.\nACM ISBN 978-1-4503-9236-5/22/10. . . $15.00\nhttps://doi.org/10.1145/3511808.3557602\nğ‘‹ğ‘†\nğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘ ğ‘‹ğ‘‡\nğ‘ â„ğ‘ğ‘Ÿğ‘’ğ‘‘\nğ‘‹ğ‘†\nğ‘ğ‘Ÿğ‘–ğ‘£ğ‘ğ‘¡ğ‘’ ğ‘‹ğ‘‡\nğ‘ğ‘Ÿğ‘–ğ‘£ğ‘ğ‘¡ğ‘’\nA\nP\nV\n[T|F]\nP\nA\nA\nV\nV\nA\nP\nT\nT\n T\n F\n F\nF\n F\nP\nP\nV\nV\nT\n T\n V\nA\n A\nA\nA\nT\nV\nP\n P\nP\nA\n A\nA\nA\n A\nV\n V\nV\n V\nP\n P\nP\n P\nF\n F\nF\n F\nA\nA\nP\nP\n P\nV\n(a) Pairwise Node-type Distribution Discrepancy\n(b) Topological StructureDiscrepancy\nSource Target\n{ğ’©ğ‘†\n1, ğ’©ğ‘†\n2} {ğ’©ğ‘‡\n1, ğ’©ğ‘‡\n2}\nFigure 1: Challenges on DA across HINs: (a) feature and (b)\nstructure discrepancies between source and target HINs.\n1 INTRODUCTION\nDomain Adaptation (DA) aims to learn transferable representations\nfor problems those sample and label spaces remain unchanged, but\nthe probability distribution is different [2, 7, 9, 20]. Recently, DA\nacross graphs has been beginning to draw much attention [11, 12, 16,\n19]. It aims to transfer the knowledge learned from a source network\nto a target network by learning label-discriminative and network-\ninvariant node embeddings. However, existing methods mainly\nfocus on the problem of homogeneous networks. In this paper,\nwe study a generalized domain adaptation across Heterogeneous\nInformation Networks (HINs), where nodes are fully labeled in the\nsource network while completely unlabeled in the target network.\nWe intend to minimize the domain shift and transfer the model\ntrained on the source network to the target network.\nHowever, this objective faces several challenges: 1) First, each\nHIN is composed of multiple different types of nodes and edges.\nFor example, the source network DBLP [ 5] in Fig. 1(a) contains\nfour types of nodes: papers (P), authors (A), venues (V), and terms\n(T), as well as different types of edges between them. This means\nthat different types of nodes and edges are in different semantic\nspaces and have different data distributions, which results in three\ndiscrepancies between pairwise node-type distributions (See Fig. 1\n(a.top)). 2) Second, the node and edge types in two heterogeneous\narXiv:2012.05688v3  [cs.LG]  25 Sep 2022\nCIKM â€™22, October 17â€“21, 2022, Atlanta, GA, USA. Tiancheng Huang, Ke Xu, and Donglin Wang\nnetworks are not identical in general. For instance, DBLP and MAG\n[13] citation networks have some private-type nodes and edges\nin their networks (i.e., term nodes for DBLP, and field (F) nodes\nfor MAG). Because private node types exist across the networks\n(See Fig. 1 (a.bottom)), a common semantic space needs to be built\nbefore they are aligned to capitalize on the information about such\ntype nodes. For topological structures from two different domains,\nthere is a discrepancy to be aligned for the center node embedding\neven after all distributions of node types have been aligned (See\nFig. 1 (b)). However, previous work [18] only aligns the shared type\nnode features but ignores that each network has its characteristics.\nTo address these problems, we propose a Generalized Domain\nAdaptive model across HINs (GDA-HIN). Several independent dis-\ncriminators are used to align the embeddings in different semantic\nspaces. Specifically, our model firstly constructs auto-encoders and\ndomain discriminators to align the node features. Secondly, we\nalign the topological structure in graph embedding space using an\nautomatically learned meta-path feature extractor (e.g., HGT [4])\nand a domain discriminator. A low-rank matrix completion method\nis adopted to handle private node types. Our model combines the\nlow-rank matrix completion method with the heterogeneity of HIN.\nIn this way, GDA-HIN can get rid of the precondition of matrix\ncompletion method, that corresponding instances [ 17], or a few\nlabeled target domain data [8]. The contributions of this work:\n1) To the best of our knowledge, we are the first to investigate\nthe generalized domain adaptation across HINs that contain both\nshare and private node types. Thus, it is a more general situation\non source and target HINs for real-world scenarios.\n2) We systematically analyze the challenges of DA across HINs.\nBased on the above analyses, we propose a novel GDA-HIN by de-\nsigning pairwise node-type distribution alignment and topological\nstructure alignment to accomplish DA across HINs.\n3) We conduct extensive experiments on the three citation net-\nworks and six groups of cross-network tasks for node classification\nto evaluate the performance of domain adaptation. The experiments\nshow that GDA-HIN outperforms state-of-the-art baselines.\n2 METHODOLOGY\n2.1 Problem Definition\nGiven a fully-labeled source ğ»ğ¼ğ‘ğ‘† =\n\u0010\nVğ‘™\nğ‘†,Eğ‘†,Ağ‘†,Rğ‘†\n\u0011\nand a fully-\nunlabeled target ğ»ğ¼ğ‘ğ‘‡ =\n\u0010\nVğ‘¢\nğ‘‡,Eğ‘‡,Ağ‘‡,Rğ‘‡\n\u0011\n, where Vand Eare\nnode and edge set, Aand Rdenote the sets of node and edge types.\nAğ‘† and Ağ‘‡ not only share same node types but also have their\nown private node types, also leading to the difference between Rğ‘†\nand Rğ‘‡. Vğ‘™\nğ‘† represents a set of labeled nodes while Vğ‘¢\nğ‘‡ represents\na set of unlabeled nodes. The transferable classification aims to\npredict the label on ğ»ğ¼ğ‘ğ‘‡ with label information from ğ»ğ¼ğ‘ğ‘†.\n2.2 Pairwise Node-type Distribution Alignment\nTo align each semantic component independently between source\nand target domains, we construct pairwise node-type auto-encoders\nand domain discriminators before aggregation and updating:\n1) Shared Node-type Distribution Alignment: Assuming there\nare ğ¾1 shared node-type pairs between source and target domains,\nthe pairwise node-type auto-encoder ğ´ğ¸ğ‘˜1,ğ‘˜1 = 1,...,ğ¾ 1 is used\nto encode and decode the node features of ğ‘˜1-th-type nodes. A\nreconstruction loss is used to constrain ğ´ğ¸ğ‘˜1 â€™s projection retaining\nsemantic information:\nLrecon1 =\nâˆ‘ï¸\nğ‘€ğ‘†ğ¸\n\u0010\nXğ‘˜1,^Xğ‘˜1\n\u0011\n, (1)\nwhere Xğ‘˜1 is the node feature of the ğ‘˜1-th shared node type for\nboth domains, ^Xğ‘˜1 is the corresponding reconstructed feature, and\nğ‘€ğ‘†ğ¸ represents the mean square error. Then, we construct a pair-\nwise node-type domain discriminator following with a Gradient\nReversal Layer (GRL) [3] for the features of individual-type nodes\nseparately. By minimizing the domain adversarial similarity loss,\nthe encoder is trained to make similar its output processed from\nboth domains while the domain discriminator learns to identify the\ndomain of the encoderâ€™s output. The domain discriminator loss for\nall discriminators ğ·ğ‘˜1 and auto-encoders ğ´ğ¸ğ‘˜1, is formulated as:\nLğ‘›ğ‘‘ğ‘1 =\nâˆ‘ï¸\nEğ‘¥âˆˆXğ‘˜1\nğ‘†\nh\nlog\n\u0010\n1 âˆ’ğ·ğ‘˜1\n\u0010\nğ´ğ¸ğ‘˜1 (ğ‘¥)\n\u0011\u0011i\n+Eğ‘¥âˆˆXğ‘˜1\nğ‘‡\nh\nlogğ·ğ‘˜1\n\u0010\nğ´ğ¸ğ‘˜1 (ğ‘¥)\n\u0011i\n, (2)\nwhere ğ·ğ‘˜1 is the discriminator for the ğ‘˜1-th-type nodes.\n2) Private Node-type Distribution Alignment: Unlike shared-\nnode types, private-node types contain many unknown values.\nReconstruction constraints applied to private-node types recover\nnot only the observed values but also unknown parts. Meanwhile,\nthe private-node types like term and field are semantically rele-\nvant [10], which makes the unobserved typeâ€™s embedding contain\nplenty of linear dependent columns. Hence, the problem of recovery\nunobserved typeâ€™s embedding turns into a low-rank matrix com-\npletion problem, which can be formulated to minimize the nuclear\nnorm under the constraint of reconstruction loss [1]. For private\nnode-type pairs, we recover the missing value and get a matrix ^W:\nW =\n\u0014 Xğ‘† 0\n0 X ğ‘‡\n\u0015\nğ‘Ÿğ‘’ğ‘ğ‘œğ‘£ğ‘’ğ‘Ÿ\n=â‡’ ^W =\n\u0014 ^Xğ‘† ^Uğ‘†\n^Uğ‘‡ ^Xğ‘‡\n\u0015\n. (3)\nIn ^W above, ^Xğ‘† and ^Xğ‘‡ represent the recovered observed el-\nements, while ^Uğ‘† and ^Uğ‘‡ represent the recovered unobserved\nelements. To recover ^W, we minimize the loss Lrecon2:\nLrecon2 =\nâˆ‘ï¸\nğ‘€ğ‘†ğ¸\n\u0010\nXğ‘˜2,^Xğ‘˜2\n\u0011\n+ğ›¿R\n\u0010\n^W\n\u0011\n, (4)\nwhere Xğ‘˜2 is the node feature of the ğ‘˜2-th private node type, ^Xğ‘˜2\nis corresponding observed part of recovered embedding, ğ¾2 is the\nnumber of the private-type pairs. R(^W)is a regularization term,\nwhere R(âˆ—)denotes nuclear norm operation, and ğ›¿ > 0 is a trade-\noff parameter. Under the reconstruction constraint, the encoderâ€™s\noutput can retain enough semantic information for the private-type\npairs. The loss of discriminators for ğ‘˜1 shared- and ğ‘˜2 private-type\npairs is Lğ‘›ğ‘‘ğ‘ = Lğ‘›ğ‘‘ğ‘1 +Lğ‘›ğ‘‘ğ‘2, where Lğ‘›ğ‘‘ğ‘2 is similar as Eq. (2).\n2.3 Topological Structure Alignment\nIn this subsection, we further elaborate on how to align the topo-\nlogical structure of source and target networks.\nRepresentation of h-hop: We choose the advanced HIN model\nHGT [4] as feature extractor ğº, which can learn embeddings of\nh-hop structure ({N1\n(ğ‘£),..., Nâ„\n(ğ‘£)}), that is h-hop embedding, to cap-\nture network topology information. In GDA-HIN, nodes from the\nGDA-HIN: A Generalized Domain Adaptive Model across Heterogeneous Information Networks CIKM â€™22, October 17â€“21, 2022, Atlanta, GA, USA.\nSource HIN\nTarget HIN\nPairwise Node-type Distribution Alignment \nd\nTarget-\nDomain\nPseudo\nLabel\nSorted by entropy in ascend order\nSelecting  \ntop   \nprecent\nHIN Feature \nExtractor\nTopological Structure Alignment \nDiscriminator \nClassifier \nEncDec\nDiscriminators \nAuto-encoders \nClassifier\n(b) Private Node-types \nAlignment  \n(a) Shared Node-types Alignment \nPhase I training: (a)+(c)\nPhase II training: (a)+(b)+(c)\n(c) Feature Extractor + Domain Discriminator + Label Classifier \nLayer\n2 \nEnc\nEnc\nDec\nDec\nEnc\nDec\nBuild\nEnc\nDecMatrix\nW \nLayer\n1 \nPre-train\nModel\nPrediction\nFigure 2: The framework of GDA-HIN. See more details in Methodology (Â§2).\nsource and target networks are encoded via a feature extractor\nwith shared learnable parameters. However, there are stillh-hop\nstructure discrepancies between both domains. Then, we adopt the\ndomain alignment on the output of HGT, which aligns the data\ndistribution in the embedding space of h-hop structure.\nTopological Domain Discriminator: After extracting each nodesâ€™\nh-hop structure embedding by feature extractorğº, which is a 2-layer\nHGT, another domain adversarial discriminatorğ·ğ‘¡ğ‘ is implemented\nto minimize the topological structure discrepancy, and its loss Lğ‘‘ğ‘\ndefined as:\nLğ‘‘ğ‘ = Eğ‘¥âˆˆğ‘¯ğ‘º\n\u0002\nlog \u00001 âˆ’ğ·ğ‘¡ğ‘ (ğº(ğ‘¥))\u0001\u0003\n(5)\n+Eğ‘¥âˆˆğ‘¯ğ‘»\n\u0002\nlogğ·ğ‘¡ğ‘ (ğº(ğ‘¥))\n\u0003\n,\nwhere Hğ‘† and Hğ‘‡ represent the outputs of encoders in source and\ntarget domains, respectively.\nDomain-invariant Classifier: The classifier ğ¶ is used to to pre-\ndict the label, and its loss Lğ‘ğ‘™ğ‘  is defined as:\nLğ‘ğ‘™ğ‘  = âˆ’1\nğ‘ğ‘™\nğ‘ğ‘™\nÎ£\nğ‘–=0\nğ‘Œğ‘–ğ‘™ğ‘œğ‘”\n\u0010\n^ğ‘Œğ‘–\n\u0011\n+ğœğ‘¡ğ‘Ÿ \u0000HâŠ¤LgH\u0001 , (6)\nwhere ğœ is a balance parameter, ğ‘ğ‘™ represents the number of la-\nbeled source- and target-domain nodes, and ^ğ‘Œğ‘– denotes the ğ‘–-th\nnodeâ€™s prediction. The regularization term in Eq.(6) is defined as\nğ‘¡ğ‘Ÿ(HâŠ¤LgH), where H represents the hidden state of auto-encoders\nfor all private-type nodes and Lg denotes the graph Laplacian ma-\ntrix for all private-type nodes. In particular, the graph Laplacian\nmatrix is formulated as Lg =\n\u0012LS 0\n0 L T\n\u0013\n, where LS and LT are\nthe Laplacian matrices of source and target domains computed\naccording to the adjacency matrices. In the matrix completion mod-\nule, there are unobserved elements involved in the computation\nof private-type nodesâ€™ embeddings, which are not constrained by\nreconstruction loss. We utilize graph Laplacian matrix to smooth\ntheir embedding over the graph, relying on the assumption that\nconnected nodes in the graph are likely to share the same label [6].\n2.4 Optimization\nPhase I Training: To minimize the nuclear norm under the con-\nstraint of reconstruction loss Eq.(4), GDA-HIN trains a Phase I\nTraining model on the shared node types of two networks to yeild\npseudo labels, and the overall objective function is composed of the\nfollowing four components:\nLğ‘1 = Lğ‘ğ‘™ğ‘  +ğ›¼Lğ‘Ÿğ‘’ğ‘ğ‘œğ‘›1 +ğ›½Lğ‘›ğ‘‘ğ‘1 +ğ›¾Lğ‘‘ğ‘, (7)\nwhere ğ›¼, ğ›½ and ğ›¾ are hyper-parameters.\nWhen there are only shared node types, Eq.(6) degenerates to\ncross-entropy loss, Lğ‘ğ‘™ğ‘  = âˆ’1\nğ‘ğ‘† Î£ğ‘ğ‘†\nğ‘–=0 ğ‘Œğ‘–\nğ‘†ğ‘™ğ‘œğ‘”\n\u0010\n^ğ‘Œğ‘–\nğ‘†\n\u0011\n,where ğ‘ğ‘† denotes\nthe number of source-domain nodes.\nPhase II Training: For phase II training, we select some predic-\ntions by Phase I Training model as pseudo labels for nodes in target\ndomain. Under the previous phaseâ€™s guidance, GDA-HIN considers\nboth shared- and private- node types from two networks, and the\noverall optimization objective is:\nLğ‘2 = Lğ‘ğ‘™ğ‘  +ğ›¼(Lğ‘Ÿğ‘’ğ‘ğ‘œğ‘›1 +Lğ‘Ÿğ‘’ğ‘ğ‘œğ‘›2)+ğ›½Lğ‘›ğ‘‘ğ‘ +ğ›¾Lğ‘‘ğ‘. (8)\n3 EXPERIMENTS\nTable 1: Dataset statistics.\nDataset Shared Private Shared Private\nP A V T/K/F P-A P-V P-T/K/F\nDBLP 14,328 4,057 20 2,517 19,645 14,328 8,647\nAminer 7,212 4,696 16 7,323 13,796 7,212 22,568\nMAG 6,206 5,861 20 2,370 12,615 6,206 8,701\nCIKM â€™22, October 17â€“21, 2022, Atlanta, GA, USA. Tiancheng Huang, Ke Xu, and Donglin Wang\nTable 2: Node classification accuracy (%) comparisons on six\ncross tasks. (D: DBLP; A: Aminer; M: MAG)\nMethods D â†’A D â†’M A â†’D A â†’M M â†’D M â†’A\nGCN+GRL 51.30 35.56 42.17 33.39 42.15 51.21\nUDA-GCN 55.86 36.14 47.10 34.55 46.86 51.24\nHAN+GRL 56.73 34.98 45.77 33.66 45.52 51.34\nHGT (ğ‘¤/ğ‘œDA) 34.11 28.83 30.74 29.14 27.24 31.35\nGDA-HINğ‘¤/ğ‘œ ğ‘ƒ 53.13 36.72 41.41 36.33 47.83 51.36\nGDA-HINğ‘¤/ğ‘œ ğ‘‡ 54.10 32.42 44.73 35.00 48.02 46.29\nGDA-HINğ‘¤/ğ‘† 58.18 37.71 47.77 36.80 48.66 52.87\nGDA-HIN(ours) 58.84 39.45 58.59 36.91 50.39 54.30\n3.1 Experiment Settings\nDatasets: DBLP [5]. We extract a subset of DBLP which contains\n14,328 papers (P), 4,057 authors (A), 20 venues (V), 2,517 terms (T),\nand the edges between nodes. Aminer [14]. We extract a subset\nof Aminer by selecting papers published from the year 2004 to\n2008, which contains 7,212 papers, 4,696 authors, 16 venues, 7,323\nkeywords (K), and the edges between nodes. MAG [13]. Here we\nextract a subset of MAG with publish date between the year 2017\nand 2019, which contains 6,206 papers, 5,861 authors, 20 venues,\n2,370 fields (F), and the edges between nodes. We categorize authors\naccording to their research areas: Database, Data Mining , Artificial\nIntelligence, and Information Retrieval . Summary statistics of the\ndatasets are displayed in Table 1.\nBaselines. We compare with the following baselines: GCN+GRL.\nThe model adopts homogeneous graph-based methods GCN [6] as\nfeature extractors and take GRL [3] as domain adaptation frame-\nwork. UDA-GCN. The method [16] adopts a multi-channel GCN\nwith a weight sharing strategy and takes GRL as the domain adap-\ntation framework. It effectively maintains local consistency and\nglobal consistency of the graphs. HAN+GRL. This model adopts\nHIN-based method HAN [15] as feature extractor, and takes GRL\nas the domain adaptation framework.\n3.2 Performance Comparison\nOur model carries out six groups of cross-network tasks, and the\nnode classification results are reported in Table 2. We have the\nfollowing primary observations: (1) Compared with all baselines,\nthe proposed GDA-HIN generally achieves the best performance.\nThe results demonstrate the effectiveness of GDA-HIN. (2) All HIN-\nbased methods exhibit better performance than homogeneous meth-\nods (i.e., GDA-HIN vs. GCN+GRL). This result reveals that consid-\nering heterogeneity helps the DA. In essence, it validates that for\nthe domain adaptation problem of HINs, it is essential to handle\nthe heterogeneity-caused multiple semantic spaces. (3) The GDA-\nHIN outperforms other heterogeneous graph-based methods, which\ndemonstrates that our method can better address the discrepancies\nof both pairwise node-type distributions and topological structure.\n3.3 Ablation Study\nTo gain insights about GDA-HIN, we study its variants: GDA-\nHINğ‘¤/ğ‘œ ğ‘ƒ, GDA-HINğ‘¤/ğ‘œğ‘‡ and GDA-HINğ‘¤/ğ‘†. Specifically, GDA-\nHINğ‘¤/ğ‘œ ğ‘ƒ& GDA-HINğ‘¤/ğ‘œğ‘‡ are two variants of GDA-HIN with\nthe pairwise node-type distribution alignment and the topological\n(a) HGT ( ğ‘¤/ğ‘œDA)\n (b) GDA-HIN\nFigure 3: The visualization: the data points of source and tar-\nget domains are colored by red and blue color, respectively.\nstructure alignment removed, respectively. GDA-HINğ‘¤/ğ‘† is only\nuses shared node types for phase I training , and without using\nprivate node types, to verify the effectiveness of private node types.\nFrom Table 2, we observe that GDA-HIN is better than GDA-\nHINğ‘¤/ğ‘œ ğ‘ƒ, demonstrating the effectiveness of the pairwise node-\ntype distribution alignment for domain shift. Similarly, it is observed\nthat GDA-HIN outperforms GDA-HINğ‘¤/ğ‘œğ‘‡ , verifying the effective-\nness of the topological structure alignment. Moreover, GDA-HIN\noutperforms GDA-HINğ‘¤/ğ‘† on all cross-domain tasks, indicating\nthe effectiveness of considering private types in domain alignment.\nNote that GDA-HINğ‘¤/ğ‘† outperforms other baselines, it indicates\nour model only using share- node types gains competitive results.\nIn summary, GDA-HIN outperforms three variants, indicating that\npairwise node-type alignment, which includes shared and private\nalignments, and topological structure alignment are indispensable.\n3.4 Visualization\nDue to the space limit, we report the t-SNE visualization of modelsâ€™\nembeddings on Aminerâ†’DBLP as an illustration. The results are\nshown in Fig. 3, where the source and target domains are colored\naccording to their own domains. From Fig. 3 (a) âˆ¼(b), we can\nfind that the result of (a) HGT contains a tremendous distribution\ndiscrepancy due to ğ‘¤/ğ‘œ DA. Apparently, the visualization of our\n(b) GDA-HIN performs best, where data points of source and target\ndomains are evenly mixed and hardly separated, which means that\nthe domain shift has been successfully minimized, and the node\nembeddings learned by GDA-HIN are actually domain-invariant.\nThe domain-invariant representations are beneficial to transferring\nfrom source domain to target domain.\n4 CONCLUSION\nIn GDA-HIN, we achieve a better domain adaptation performance\nby jointly considering the pairwise node-type distribution align-\nment and topology structure alignment. We adopt the matrix com-\npletion method to effectively deal with private-type nodes by pro-\njecting each private-type pair into a new common feature space.\nThe proposed scheme has been proven effective by experiments.\nACKNOWLEDGEMENTS\nThis work was supported by NSFC General Program (Grant No.\n62176215), and National Science and Technology Innovation 2030 -\nMajor Project (Grant No. 2022ZD0208800).\nGDA-HIN: A Generalized Domain Adaptive Model across Heterogeneous Information Networks CIKM â€™22, October 17â€“21, 2022, Atlanta, GA, USA.\nREFERENCES\n[1] Emmanuel J CandÃ¨s and Benjamin Recht. 2009. Exact matrix completion via\nconvex optimization. Foundations of Computational mathematics 9, 6 (2009),\n717â€“772.\n[2] Lisheng Fu, Thien Huu Nguyen, Bonan Min, and Ralph Grishman. 2017. Domain\nadaptation for relation extraction with domain adversarial neural network. In\nProceedings of the Eighth International Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers) . 425â€“429.\n[3] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo\nLarochelle, FranÃ§ois Laviolette, Mario Marchand, and Victor Lempitsky. 2016.\nDomain-adversarial training of neural networks. The journal of machine learning\nresearch 17, 1 (2016), 2096â€“2030.\n[4] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun. 2020. Heterogeneous\ngraph transformer. In Proceedings of The Web Conference 2020 . 2704â€“2710.\n[5] Ming Ji, Yizhou Sun, Marina Danilevsky, Jiawei Han, and Jing Gao. 2010. Graph\nregularized transductive classification on heterogeneous information networks.\nIn Joint European Conference on Machine Learning and Knowledge Discovery in\nDatabases. Springer, 570â€“586.\n[6] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with graph\nconvolutional networks. The International Conference on Learning Representations\n(2017).\n[7] Wouter M Kouw and Marco Loog. 2018. An introduction to domain adaptation\nand transfer learning. arXiv preprint arXiv:1812.11806 (2018).\n[8] Haoliang Li, Sinno Jialin Pan, Renjie Wan, and Alex C Kot. 2019. Heterogeneous\ntransfer learning via deep matrix completion with adversarial kernel embedding.\nIn Proceedings of the AAAI Conference on Artificial Intelligence , Vol. 33. 8602â€“8609.\n[9] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. 2015. Learning\ntransferable features with deep adaptation networks. In International conference\non machine learning . PMLR, 97â€“105.\n[10] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient\nestimation of word representations in vector space.arXiv preprint arXiv:1301.3781\n(2013).\n[11] Xiao Shen, Quanyu Dai, Fu-lai Chung, Wei Lu, and Kup-Sze Choi. 2020. Adversar-\nial deep network embedding for cross-network node classification. InProceedings\nof the AAAI Conference on Artificial Intelligence , Vol. 34. 2991â€“2999.\n[12] Xiao Shen, Quanyu Dai, Sitong Mao, Fu-lai Chung, and Kup-Sze Choi. 2020.\nNetwork together: Node classification via cross-network deep network embed-\nding. IEEE Transactions on Neural Networks and Learning Systems 32, 5 (2020),\n1935â€“1948.\n[13] Arnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Darrin Eide, Bo-June Hsu, and\nKuansan Wang. 2015. An overview of microsoft academic service (mas) and\napplications. In Proceedings of the 24th international conference on world wide web .\n243â€“246.\n[14] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. 2008. Ar-\nnetminer: extraction and mining of academic social networks. In Proceedings of\nthe 14th ACM SIGKDD international conference on Knowledge discovery and data\nmining. 990â€“998.\n[15] Xiao Wang, Houye Ji, Chuan Shi, Bai Wang, Yanfang Ye, Peng Cui, and Philip S Yu.\n2019. Heterogeneous graph attention network. InThe World Wide Web Conference .\n2022â€“2032.\n[16] Man Wu, Shirui Pan, Chuan Zhou, Xiaojun Chang, and Xingquan Zhu. 2020.\nUnsupervised domain adaptive graph convolutional networks. In Proceedings of\nThe Web Conference 2020 . 1457â€“1467.\n[17] Min Xiao and Yuhong Guo. 2013. A novel two-step method for cross language\nrepresentation learning. Advances in Neural Information Processing Systems 26\n(2013), 1259â€“1267.\n[18] Shuwen Yang, Guojie Song, Yilun Jin, and Lun Du. 2020. Domain adaptive\nclassification on heterogeneous information networks. In Proceedings of the\nTwenty-Ninth International Joint Conference on Artificial Intelligence . 1410.\n[19] Yizhou Zhang, Guojie Song, Lun Du, Shuwen Yang, and Yilun Jin. 2019. Dane:\nDomain adaptive network embedding. In Proceedings of the Twenty-Eight Inter-\nnational Joint Conference on Artificial Intelligence .\n[20] Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu\nZhu, Hui Xiong, and Qing He. 2020. A comprehensive survey on transfer learning.\nProc. IEEE 109, 1 (2020), 43â€“76.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6808329820632935
    },
    {
      "name": "Domain adaptation",
      "score": 0.5380200147628784
    },
    {
      "name": "Homogeneous",
      "score": 0.522503674030304
    },
    {
      "name": "Theoretical computer science",
      "score": 0.5176530480384827
    },
    {
      "name": "Graph",
      "score": 0.49689653515815735
    },
    {
      "name": "Discriminative model",
      "score": 0.43274638056755066
    },
    {
      "name": "Distributed computing",
      "score": 0.39394888281822205
    },
    {
      "name": "Topology (electrical circuits)",
      "score": 0.38429099321365356
    },
    {
      "name": "Artificial intelligence",
      "score": 0.24925819039344788
    },
    {
      "name": "Mathematics",
      "score": 0.16354900598526
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Classifier (UML)",
      "score": 0.0
    }
  ]
}