{
  "title": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models",
  "url": "https://openalex.org/W4386080721",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3162118723",
      "name": "Besta, Maciej",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287427498",
      "name": "Blach, Nils",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2291485079",
      "name": "Kubicek Ales",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4320864983",
      "name": "Gerstenberger, Robert",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287866724",
      "name": "Podstawski, Michal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3163817227",
      "name": "Gianinazzi, Lukas",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2638612461",
      "name": "Gajda Joanna",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Lehmann, Tomasz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4305238007",
      "name": "Niewiadomski, Hubert",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Nyczyk, Piotr",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744306648",
      "name": "Hoefler, Torsten",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4221161695",
    "https://openalex.org/W4376653732",
    "https://openalex.org/W4385570246",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W3011667710",
    "https://openalex.org/W3217295214",
    "https://openalex.org/W4286910674",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4323651349",
    "https://openalex.org/W4289523162",
    "https://openalex.org/W4385569771",
    "https://openalex.org/W4287123996",
    "https://openalex.org/W3152893301",
    "https://openalex.org/W4321636623",
    "https://openalex.org/W2056524360",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4385436532",
    "https://openalex.org/W4362508231",
    "https://openalex.org/W4360836968",
    "https://openalex.org/W4296594155",
    "https://openalex.org/W3154565472",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4378713467",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2558748708",
    "https://openalex.org/W2046526098",
    "https://openalex.org/W2963212338",
    "https://openalex.org/W4385570088",
    "https://openalex.org/W3206117777",
    "https://openalex.org/W1977628053",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W3166619165",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4365205411",
    "https://openalex.org/W4362656036",
    "https://openalex.org/W4206236515",
    "https://openalex.org/W4293651439",
    "https://openalex.org/W2981117684",
    "https://openalex.org/W2556970231",
    "https://openalex.org/W4285215850",
    "https://openalex.org/W4302011807",
    "https://openalex.org/W4287282215",
    "https://openalex.org/W4210257598",
    "https://openalex.org/W4377865309",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W1985644065",
    "https://openalex.org/W4308244910",
    "https://openalex.org/W4389518664",
    "https://openalex.org/W4388098329",
    "https://openalex.org/W2161130929",
    "https://openalex.org/W4281249911",
    "https://openalex.org/W4287780403",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4377130677",
    "https://openalex.org/W4361866080",
    "https://openalex.org/W4367859573",
    "https://openalex.org/W4225080353",
    "https://openalex.org/W3193434541",
    "https://openalex.org/W2953064419",
    "https://openalex.org/W3042505432",
    "https://openalex.org/W2039444222",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2218076943",
    "https://openalex.org/W4353112996",
    "https://openalex.org/W1972595474",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W4319323461",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4323927478"
  ],
  "abstract": "We introduce Graph of Thoughts (GoT): a framework that advances prompting capabilities in large language models (LLMs) beyond those offered by paradigms such as Chain-of-Thought or Tree of Thoughts (ToT). The key idea and primary advantage of GoT is the ability to model the information generated by an LLM as an arbitrary graph, where units of information (\"LLM thoughts\") are vertices, and edges correspond to dependencies between these vertices. This approach enables combining arbitrary LLM thoughts into synergistic outcomes, distilling the essence of whole networks of thoughts, or enhancing thoughts using feedback loops. We illustrate that GoT offers advantages over state of the art on different tasks, for example increasing the quality of sorting by 62% over ToT, while simultaneously reducing costs by &gt;31%. We ensure that GoT is extensible with new thought transformations and thus can be used to spearhead new prompting schemes. This work brings the LLM reasoning closer to human thinking or brain mechanisms such as recurrence, both of which form complex networks.",
  "full_text": "Graph of Thoughts: Solving Elaborate Problems with Large Language Models\nMaciej Besta1*, Nils Blach1*, Ales Kubicek1, Robert Gerstenberger1,\nMichał Podstawski2, Lukas Gianinazzi1, Joanna Gajda3, Tomasz Lehmann3,\nHubert Niewiadomski3, Piotr Nyczyk3, Torsten Hoefler1\n1ETH Zurich, 2Warsaw University of Technology,3Cledar\nbestam@inf.ethz.ch, nils.blach@inf.ethz.ch, htor@inf.ethz.ch\nAbstract\nWe introduce Graph of Thoughts (GoT): a framework that\nadvances prompting capabilities in large language models\n(LLMs) beyond those offered by paradigms such as Chain-of-\nThought or Tree of Thoughts (ToT). The key idea and primary\nadvantage of GoT is the ability to model the information gen-\nerated by an LLM as anarbitrary graph, where units of infor-\nmation (“LLM thoughts”) are vertices, and edges correspond\nto dependencies between these vertices. This approach en-\nables combining arbitrary LLM thoughts into synergistic out-\ncomes, distilling the essence of whole networks of thoughts,\nor enhancing thoughts using feedback loops. We illustrate\nthat GoT offers advantages over state of the art on different\ntasks, for example increasing the quality of sorting by 62%\nover ToT, while simultaneously reducing costs by >31%.\nWe ensure that GoT is extensible with new thought transfor-\nmations and thus can be used to spearhead new prompting\nschemes. This work brings the LLM reasoning closer to hu-\nman thinking or brain mechanisms such as recurrence, both\nof which form complex networks.\nWebsite & code: https://github.com/spcl/graph-of-thoughts\n1 Introduction\nLarge language models (LLMs) are taking over the world\nof AI. Recent years saw a rapid development of models pri-\nmarily based on the decoder-only Transformer variant [65],\nsuch as GPT [13, 14, 53, 54], PaLM [19], or LLaMA [63].\nPrompt engineering is a resource-efficient approach for\nsolving different LLM tasks. In brief, one includes the task\ndescription within the input sent to an LLM. If this descrip-\ntion is appropriately formulated, the LLM solves the task\nusing its autoregressive token-based mechanism for gener-\nating text. Such prompts may contain example tasks with\nsolutions (few-shot prompting, also referred to as in-context\nlearning (ICL)), or even no example tasks at all (zero-shot\nprompting). In recent years it was shown that this mecha-\nnism can be used to solve a broad set of tasks that involve\nmathematical, commonsense, or symbolic reasoning.\nChain-of-Thought (CoT) [71] is an approach for prompt-\ning, in which one includes the intermediate steps of rea-\nsoning within the prompt (intermediate “thoughts”), besides\nthe task input/output. CoT was shown to significantly im-\nprove the capability of LLMs to solve problems without re-\nsorting to any model updates. One major improvement over\n*Equal contribution\nCoT, Self-Consistency with CoT (CoT-SC) [67], is a scheme\nwhere multiple CoTs are generated, and then the best one is\nselected as the outcome. More recently, CoT and CoT-SC\nwere extended with Tree of Thoughts (ToT) [43, 75, 77],\nwhich models the LLM reasoning process with a tree. This\nfacilitates using different paths of thoughts, and offers novel\ncapabilities such as backtracking from non-promising out-\ncomes. Unfortunately, the ToT approaches still fundamen-\ntally limit the reasoning abilities within a prompt by impos-\ning the rigid tree structure on the thought process.\nIn this work, we argue that fundamentally more power-\nful prompting can be achieved by enabling LLM thoughts to\nform an arbitrary graph structure. This is motivated by nu-\nmerous phenomena such as human reasoning, brain struc-\nture, or algorithmic execution. When working on a novel\nidea, a human would not only follow a chain of thoughts\n(as in CoT) or try different separate ones (as in ToT), but\nwould actually form a more complex network of thoughts.\nFor example, one could explore a certain chain of reason-\ning, backtrack and start a new one, then realize that a cer-\ntain idea from the previous chain could be combined with\nthe currently explored one, and merge them both into a new\nsolution, taking advantage of their strengths and eliminat-\ning their weaknesses. Similarly, brains form complex net-\nworks, with graph-like patterns such as recurrence [28]. Ex-\necuting algorithms also expose networked patterns, often\nrepresented by Directed Acyclic Graphs. The correspond-\ning graph-enabled transformations bring a promise of more\npowerful prompting when applied to LLM thoughts, but they\nare not naturally expressible with CoT or ToT.\nWe observe that these (and many other) thought trans-\nformations can be naturally enabled when modeling the\nreasoning process of an LLM as a graph . For this, we\npropose Graph of Thoughts (GoT), an approach that en-\nhances LLMs’ capabilities through networked reasoning\n(contribution #1 ). In GoT, an LLM thought is modeled\nas a vertex, while an edge is a dependency between such\nthoughts. Using GoT, one can aggregate arbitrary thoughts\nby constructing vertices that have more than one incom-\ning edge. Overall, the graph abstraction harnessed by GoT\nseamlessly generalizes CoT and ToT to more complex\nthought patterns, without resorting to any model updates.\nYet, putting GoT to practice requires solving several de-\nsign challenges. For example, what is the best graph struc-\nture for different tasks? How to best aggregate thoughts to\nmaximize accuracy and minimize cost? To answer these and\narXiv:2308.09687v4  [cs.CL]  6 Feb 2024\nmany other questions, we carefully design a modular archi-\ntecture for implementing GoT ( contribution #2 ), coming\nwith two design highlights. First, we enable a fine-grained\ncontrol over individual thoughts . This enables us to fully\ncontrol the ongoing conversation with the LLM, and apply\nadvanced thought transformations, such as combining most\npromising thoughts from the ongoing reasoning into a new\none. Second, we ensure that our architecture can be seam-\nlessly extended with novel thought transformations, patterns\nof reasoning (i.e., graphs of thoughts), and LLM models.\nThis enables rapid prototyping of novel prompting ideas us-\ning GoT, while experimenting with different models such as\nGPT-3.5, GPT-4, or Llama-2 [64].\nWe illustrate several use cases for GoT (sorting, keyword\ncounting for summaries, set operations, document merging)\nand we detail how to implement them using the graph-based\nparadigm (contribution #3). We evaluate GoT and show its\nadvantages over the state of the art (contribution #4). Over-\nall, we observe that GoT is particularly well-suited for tasks\nthat can be naturally decomposed into smaller subtasks that\nare solved individually and then merged for a final solution.\nHere, GoT outperforms other schemes, for example improv-\ning upon CoT and ToT by, respectively, ≈70% and ≈62%,\nin terms of the quality of sorting, while simultaneously re-\nducing costs by >31% over ToT.\nWe qualitatively compare GoT to other prompting\nschemes1 in Table 1. GoT is the only one to enable arbitrary\ngraph-based thought transformations within a prompt, such\nas aggregation, embracing all previously proposed schemes.\nScheme Sc? Mc? Tr? Ag?\nChain-of-Thought (CoT) [71] /reve /reve /reve\nSelf-Consistency with CoT [67] /reve /reve\nThought decomposition [75] /reve\nTree-of-Thought (ToT) [43] /reve\nTree of Thoughts (ToT) [77] /reve\nGraph of Thoughts (GoT) \nTable 1: Comparison of prompting schemes, with re-\nspect to the supported transformations of thoughts. “Sc?”:\nsingle chain of thoughts? “Mc?”: multiple chains of\nthoughts? “Tr?”: tree of thoughts? “Ag?”: arbitrary graph\nof thoughts? “”: full support, “”: partial support, “/reve”:\nno support.\nFinally, we propose a new metric for evaluating a prompt-\ning strategy, the volume of a thought (contribution #5 ).\nWith this metric, we aim to understand better the differences\nbetween prompting schemes. For a given thought v, the vol-\nume of v is the number of LLM thoughts, from which one\ncan reach v using directed edges . Intuitively, these are all\nthe LLM thoughts that have had the potential to contribute\n1Note that we do not include a recent scheme called Graph-of-\nThought [79] because it is not a prompting scheme. While its\nname suggests close connections to ToT and CoT, as a fine-tuning\nscheme, it resorts to model updates, and is thus outside the focus\nof this work. Similarly, the graph-of-thoughts repository [52] does\nnot enable general graph-based reasoning and harnesses instead\nToT with BFS.\nto v. We show that GoT, by incorporating thought transfor-\nmations such as aggregation, enables thoughts to have fun-\ndamentally larger volumes than other schemes.\n2 Background & Notation\nWe first outline background concepts and notation.\n2.1 Language Models & In-Context Learning\nThe conversation with the LLM consists of user messages\n(prompts) and LLM replies (thoughts). We follow the estab-\nlished notation [77] and we denote a pre-trained language\nmodel (LM) with parameters θ as pθ. Lowercase letters such\nas x, y, z, ...indicate LLM thoughts. We purposefully do not\nprescribe what is a single “thought”, and instead make it use-\ncase specific. Hence, a single thought can be a paragraph\n(e.g., in article summary), a document (e.g., in document\ngeneration), a block of code (e.g., in code debugging or op-\ntimization), and so on.\nWe next describe specific prompting approaches.\nInput-Output (IO) The Input-Output (IO) prompting is a\nstraightforward approach, in which we use an LLM to turn\nan input sequence x into the output y directly, without any\nintermediate thoughts.\nChain-of-Thought (CoT) Second, in Chain-of-Thought\n(CoT), one introduces intermediate thoughts a1, a2, ...be-\ntween x and y. This strategy was shown to significantly en-\nhance various LM tasks over the plain IO baseline, such as\nmathematical puzzles [71] or general mathematical reason-\ning [24].\nMultiple CoTs Third, one can generalize CoT into multi-\nple CoTs by generating several (independent) k CoTs, and\nreturning the one with the best output (according to some\nprescribed scoring metric). It was introduced by Wang et\nal. in the scheme called Self-Consistency with CoT (CoT-\nSC) [67]. This approach enhances CoT because it offers an\nopportunity to explore different reasoning paths. However,\nit does not offer “local exploration” within a path, such as\nbacktracking.\nTree of Thoughts (ToT) Finally, the Tree of Thoughts\n(ToT) scheme was introduced independently by Yao [77]\nand Long [43] (where it is referred to as Tree-of-Thought);\nit was used implicitly to a certain degree by other schemes\nsuch as thought decomposition [75]. It enhances CoT-SC by\nmodeling the process or reasoning as a tree of thoughts. A\nsingle tree node represents a partial solution. Based on a\ngiven node, thethought generator constructs a given number\nk of new nodes. Then, the state evaluator generates scores\nfor each such new node. Depending on the use case, the eval-\nuation could be conducted using an LLM itself, or it can har-\nness human scores. Finally, the schedule of extending the\ntree is dictated by the utilized search algorithm (for example\nBFS or DFS).\n3 The GoT Framework\nWe now detail the GoT framework. We present it in Figure 1,\nand compare it to other prompting strategies.\n2\nInput\nOutput\nInput\nOutputOutput\nThoughts:\nUnscored\nNegative\nscore Output\nInput\nOutput\n[This work]\nInput\nPositive\nscore\nDependencies\nbetween thoughts\nAbandon thought\nBacktrack\nBasic Input-\nOutput (IO)\nLegend\nMultiple CoTs (CoT-SC)Chain-of-\n-Thought\n(CoT)\nTree of Thoughts (ToT) Graph of Thoughts (GoT)\nKey novelty:\nIntermediate\nLLM thoughts\nwithin a chain\nBranching out\nfrom a chain\nSelecting\na chain with\nthe best score\nAbandon a chain\nKey novelty\n(beyond CoT):\nHarnessing multiple\nindependent chains\nof thoughts\nKey novelty\n(beyond CoT-SC):\nGenerating several\nnew thoughts based\non a given arbitrary\nthought, exploring\nit further, and possibly\nbacktracking from it\nKey novelty (beyond ToT):\nArbitrary graph-based thought\ntransformations (aggregating \nthoughts into a new one, \nlooping over a thought to \nrefine it)\nBacktracking\nRefining\nAggregating\nthoughts\nBacktracking\nfrom a chain\nIntermediate\nthoughts are\nalso scored\nAggregating\nchains\nInput\nFigure 1: Comparison of Graph of Thoughts (GoT) to other prompting strategies.\nFormally, GoT can be modeled as a tuple (G, T , E, R),\nwhere G is the “LLM reasoning process” (i.e., all the LLM\nthoughts within the context, with their relationships), T are\nthe potential thought transformations,E is an evaluator func-\ntion used to obtain scores of thoughts, and R is a ranking\nfunction used to select most relevant thoughts.\n3.1 Reasoning Process\nWe model the reasoning process as a directed graph G =\n(V, E); V is a set of vertices and E ⊆ V × V is a set of\nedges. G is directed and thus the edges are a subset of or-\ndered vertex pairs E ⊆ V × V . A vertex contains a solution\nto a problem at hand (be it an initial, intermediate, or a fi-\nnal one). The concrete form of such a thought depends on\nthe use case; it could be a paragraph (in writing tasks) or a\nsequence of numbers (in sorting). A directed edge (t1, t2)\nindicates that thought t2 has been constructed using t1 as\n“direct input”, i.e., by explicitly instructing the LLM to use\nt1 for generating t2.\nIn certain use cases, graph nodes belong to different\nclasses. For example, in writing tasks, some vertices model\nplans of writing a paragraph, while other vertices modelthe\nactual paragraphs of text . In such cases, GoT embraces a\nheterogeneous graph G = (V, E, c) to model the LLM rea-\nsoning, where c maps vertices V into their respective classes\nC (in the above case, it would beC = {plan, par}). Hence,\nany vertex v can model different aspects of reasoning.\nWe associate G with the LLM reasoning process. To ad-\nvance this process, one applies thought transformations to\nG. An example of such a transformation is to merge best-\nscoring (so far) thoughts into a new one. Another example\nis to loop over a thought, in order to enhance it. Note that\nthese transformations strictly extend the set of transforma-\ntions available in the CoT, CoT-SC, or ToT.\n3.2 Transformations of Thoughts\nGoT enables novel transformations of thoughts thanks to\nthe graph-based model for reasoning. We refer to them as\n...\nGraph theory view Example sorting task Example writing task\nAggregationGeneration\n...\n1 2 7 8 1 1 4 52 3 6 7\n1 1 1 2 2 3 4 5 6 7 7 8\n...\nArticle\n1\nArticle\n2\nArticle\n3\nKeyword\nsummary\nA vertex models\na thought. An edge\nmodels dependency\n... ...\nMerging sorted subarrays\ninto a sorted array of numbers\nSplitting an unsorted array into\nsubarrays, for subsequent sorting\nCombining articles into\na coherent summary\n...\nKeyword\nsummary 1\nKeyword\nsummary 2\n1 4 6 2 4 2 4 9 8 7 5 4\n1 4 6 2    4 2 4 9    8 7 5 4\nArticle\n1\nGenerating summaries from\nan article, to maximize quality\nFigure 2: Examples of aggregation and generation thought\ntransformations.\ngraph-enabled transformations. For example, in writing,\none could combine several input articles into one coherent\nsummary. In sorting, one could merge several sorted subar-\nrays of numbers into a final sorted array. We illustrate exam-\nples of aggregation and generation in Figure 2.\nFormally, each such transformation can be modeled as\nT (G, pθ) where G = ( V, E) is the graph reflecting the\ncurrent state of the reasoning, and pθ is the used LLM. T\nmodifies G usually by adding new vertices and their incom-\ning edges. We have G′ = T (G, pθ) = ( V ′, E′), where\nV ′ = ( V ∪ V +) \\ V − and E′ = ( E ∪ E+) \\ E−. V +\nand E+ are new vertices and edges inserted intoG to model\nthe new thoughts and their dependencies, respectively. To\nmaximize the expressiveness of GoT – we also enable the\nuser to explicitly remove thoughts, by specifying the corre-\nsponding vertices and edges to be removed (V − and E−, re-\nspectively). Here, it is the user’s responsibility to ensure that\nthe sets V +, E+, V−, and E− come with consistent trans-\nformations (i.e., for example, that the user does not attempt\nto remove a vertex that does not exist). This enables seam-\n3\nless incorporation of schemes where, in order to save space\nwithin the context, one can remove parts of reasoning that\ndo not promise improvements.\nThe specific form of T and how it impacts G depends on\na specific transformation. We first detail the primary graph-\nenabled thought transformations, and then proceed to de-\nscribe how GoT embraces the transformations from the ear-\nlier schemes. Unless stated otherwise, V − = E− = ∅.\nAggregation Transformations First, with GoT, one can\naggregate arbitrary thoughts into new ones, to combine\nand reinforce the advantages of these thoughts, while elim-\ninating their disadvantages. In the basic form, in which\nonly one new vertex is created, V + = {v+} and E+ =\n{(v1, v+), ...,(vk, v+)}, where v1, ..., vk are the merged k\nthoughts. More generally, this enables aggregating reason-\ning paths, i.e., longer chains of thoughts, beyond just indi-\nvidual thoughts. With the graph model, it is simply achieved\nby adding outgoing edges from the vertices v1, ..., vk, mod-\neling final thoughts in several chains, into a single thought\nv+ combining these chains.\nRefining Transformations Another thought transforma-\ntion is the refining of a current thought v by modifying its\ncontent: V + = {} and E+ = {(v, v)}. This loop in the\ngraph indicates an iterated thought with the same connec-\ntions as the original thought.\nGeneration Transformations Finally, one can generate\none or more new thoughts based on an existing single\nthought v. This class embraces analogous reasoning steps\nfrom earlier schemes, such as ToT or CoT-SC. Formally, we\nhave V + = {v+\n1 , ..., v+\nk } and E+ = {(v, v+\n1 ), ...,(v, v+\nk )}.\n3.3 Scoring & Ranking Thoughts\nThoughts are scored to understand whether the current solu-\ntion is good enough. A score is modeled as a general func-\ntion E(v, G, pθ), where v is a thought to be evaluated. We\nuse the state of the whole reasoning process ( G) in E for\nmaximum generality, because – for example – in some eval-\nuation scenarios, scores may be relative to other thoughts.\nGoT can also rank thoughts. We model this with a func-\ntion R(G, pθ, h) where h specifies the number of highest-\nranking thoughts in G to be returned by R. While the spe-\ncific form of R depends on the use case, we most often use a\nsimple yet effective strategy whereh thoughts with the high-\nest scores are returned, i.e., v1, ..., vh = R(G, pθ, h).\nSpecific forms ofE and R depend on the use case. We dis-\ncuss the details in Section 5. For example, the score (or rank)\nfor sorting corresponds to the count of elements correctly\nsorted (or incorrectly, when using the error as a score).\n4 System Architecture & Extensibility\nThe GoT architecture consists of a set of interacting mod-\nules, see Figure 3 (the blue part). These modules are the\nPrompter (prepares the messages for the LLM), the Parser\n(extracts information from LLM thoughts), the Scoring\nmodule (verifies and scores the LLM thoughts), and the\nController (coordinates the entire reasoning process, and de-\ncides on how to progress it). The Controller contains two fur-\nther important elements: the Graph of Operations (GoO) and\nthe Graph Reasoning State (GRS). GoO is a static structure\nthat specifies the graph decomposition of a given task , i.e.,\nit prescribes transformations to be applied to LLM thoughts,\ntogether with their order & dependencies. GRS is a dynamic\nstructure that maintains the state of the ongoing LLM rea-\nsoning process (the history of its thoughts and their states).\n4.1 Prompter\nThe Prompter prepares the prompts to be sent to the LLM.\nThis module is responsible for the specifics of encoding the\ngraph structure within the prompt. The GoT architecture en-\nables the user to implement use case specific graph encod-\nings by providing full access to the graph structure.\n4.2 Parser\nThe Parser extracts information from LLM thoughts. For\neach such thought, the Parser constructs the thought state,\nwhich contains this extracted information. The thought state\nis then used to update the GRS accordingly.\n4.3 Scoring & Validation\nHere, we verify whether a given LLM thought satisfies po-\ntential correctness conditions, and then we assign it a score.\nDepending on how the score is derived, the module may\nconsult the LLM. Moreover, depending on the use case, the\nscore may also be assigned by a human. Finally, use cases\nsuch as sorting use simple local scoring functions.\n4.4 Controller\nThe Controller implements a specific strategy for select-\ning thoughts from its GRS structure. It also selects what\ntransformations should be applied to which thoughts, and\nthen passes this information to the Prompter. It also decides\nwhether the whole process should be finalized, or whether\nthe next round of interaction with the LLM should be initi-\nated. In our current design, this is dictated by the execution\nplan specified in the GoO.\n4.5 GoO & GRS\nThe user constructs a GoO instance, which prescribes the\nexecution plan of thought operations. The GoO is a static\nstructure that is constructed once, before the execution starts.\nEach operation object knows its predecessor and successor\noperations. Then, during the execution, an instance of the\nGRS maintains the continually updated information about\nthe LLM reasoning process. This includes which operation\nhas been executed so far, the states of all the generated LLM\nthoughts, their validity and scores, and any other relevant\ninformation.\nThe above elements offer extensible APIs, enabling\nstraightforward implementations of different prompting\nschemes. The APIs are outlines in the green part of Fig-\nure 3, and detailed in the documentation. We also provide\nexamples of prompts used by these operations and a corre-\nsponding GRS in the red part of Figure 3.\n4\nGoal: Build a prompt\nto be sent to the LLM\nLegend Architecture overview\nExample prompts and the Graph Reasoning State for the sorting use case (some examples within each prompt are omitted due to space constraints)\nParser\nGoal: Extract\ninformation from\nLLM thought Goal: Assess the\nquality of the\nLLM's solution\nControllerGoal: Initiate, coordinate, manage,\nand progress the GoT execution\nExternal entity Prompt Thought\nThought state\nScore\nOperation\nThought state + its\nassociated operations\nThought state\n+ thought's score\nDependencyModule of the\nGoT framework Graph of\nOperations\nGoal: Specify\nLLM thought\ntransformations\nGraph Reasoning State\nGoal: Maintain\nthe ongoing LLM\nreasoning process\nUser\nGoal: Indicate the\ntop-scoring thoughts\nGraph of Operations enables seamless specification of not only\nGoT, but also existing schemes such as CoT, CoT-SC, ToT\nAPI for Prompter (extensible)\n➡ Generate(t,k) //generate a prompt for k new thoughts, using thought t\n➡ //LLM params: model used, temperature, max tokens, api key, org, ...\n➡ //LLM cost features: prompt token cost, response token cost, ...\n➡ //Instances of Prompter + Parser + Graph of Operations,\n➡ //Any additional input parameters (e.g., numbers to be sorted).\n//Each of the above routines is responsible for parsing an LLM thought\n//to a corresponding Prompter routine (e.g., ParseScore parses Score).\n➡ Score(t) //score thought t\n➡ Validate(t) //generate a prompt to validate the correctness of thought t\n➡ ValidateAndImprove(t) //generate a prompt to enhance thought t,\n➡ Aggregate(t1,...,tk) //generate a prompt to combine thoughts t1, ..., tk \nAPI for Controller\nAPI for Parser (extensible)\nParseGenerate, ParseImprove, ParseScore,\nParseAggregate, ParseValidate, ...\n➡ Generate, Aggregate, Score, ... //see Prompter API\n➡ KeepBest(N) //preserves N best scoring thoughts\n➡ Repeat(k) //Repeat a given operation k times, generating k thoughts.\n    //For example, this enables \"Aggregate\" to generate multiple outcomes\n    //of the combination operation. Each such thought is maintained \n   //within the Graph Reasoning State and scored individually.\nAvailable operations when building the GoO (extensible)\nSpecifying the Structure of the Graph of Operations (GoO)\nRankingScoring &\nvalidation\nPrompter\nLLM\nHuman\nor LLM\nGray block\nBlue block\nA prompt used by\nAggregate(t1,t2)+Repeat(k=3)+KeepBest(N=1)\n<Instruction> Merge the following 2 sorted lists of length {length1} each, \ninto one sorted list of length {length2} using a merge sort style approach.\nOnly output the final merged list without any additional text or thoughts!\n</Instruction>\n<Approach>\nTo merge the two lists in a merge-sort style approach, follow these steps:\n1. Compare the first element of both lists.\n2. Append the smaller element to the merged list and move to the next \nelement in the list from which the smaller element came.\n3. Repeat steps 1 and 2 until one of the lists is empty.\n4. Append the remaining elements of the non-empty list to the merged list.\n</Approach>\nMerge the following two lists into one sorted list:\n1: {input1}\n2: {input2}\nMerged list:\nThis prompt is used by an operation Aggregate where the aggregation factor is \nk = 2 (2 input thoughts, t1 and t2, are aggregated). This is repeated by GoT 3 times, \nto maximize quality. Finally, the best result is selected. Note that, in this example, \nthe prompt explicitly requests the merge operation only. All the remaining opera-\ntions are specified in the GoO and are handled by the underlying GoT framework.\nThe input\nthoughts t1, t2\n3\nInitial/system prompt (optional)\nHello. I want to sort the following input sequence of numbers: {input}\nI\n1 2 3\nI\n4\nImprove(t)+Repeat(k=4)\n<Instruction> The following two lists represent an unsorted list of numbers \nand a sorted variant of that list. The sorted variant is not correct. Fix the \nsorted variant so that it is correct. Make sure that the output list is sorted in\nascending order, has the same number of elements as the input list ({length}),\nand contains the same elements as the input list. </Instruction>\n<Approach>\nTo fix the incorrectly sorted list follow these steps:\n1. For each number from 0 to 9, compare the frequency of that number in the\nincorrectly sorted list to the frequency of that number in the input list.\n2. Iterate through the incorrectly sorted list and add or remove numbers as\nneeded to make the frequency of each number in the incorrectly sorted list\nmatch the frequency of that number in the input list.\n</Approach>\n<Examples>\nInput: [3, 7, 0, 2, 8, 1, 2, 2, 2, 4, 7, 8, 5, 5, 3, 9]\nIncorrectly Sorted: [0, 0, 0, 0, 0, 1, 2, 2, 3, 3, 4, 4, 4, 5, 5, 7, 7, 8, 8, 9, 9, 9, 9]\nReason: The incorrectly sorted list contains four extra 0s, two extra 4s and\nthree extra 9s and is missing two 2s.\nOutput: [0, 1, 2, 2, 2, 2, 3, 3, 4, 5, 5, 7, 7, 8, 8, 9] \n    \nInput: [6, 4, 5, 7, 5, 6, 9, 7, 6, 9, 4, 6, 9, 8, 1, 9, 2, 4, 9, 0, 7, 6, 5, 6, 6, 2, 8,\n3, 9, 5, 6, 1]\nIncorrectly Sorted: [0, 1, 1, 2, 2, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7,\n7, 7, 8, 8, 9, 9, 9, 9, 9]\nReason: The incorrectly sorted list contains two extra 4s and is missing two\n6s and one 9.\nOutput: [0, 1, 1, 2, 2, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 8, 8, 9,\n9, 9, 9, 9, 9]\n</Examples>\nInput: {input}\nIncorrectly Sorted: {incorrectly_sorted}\nA prompt used by\n...\n...\nThis prompt is used by an operation\nImprove(t), which enhances a given thought t\nusing information provided in another thought.\nDepending on how the Improve + Repeat \noperation is implemented by the user within\nGoT, it can either generate a number of new \nthoughts in GRS (the upper graph on the right), \nsimilar to Generate + Repeat, or may refine \nthe same thought in GRS (the lower graph on \nthe right), chaining k=4 refinement iterations together.\n4\nThe input\nthought t\nA prompt used by\n Generate(t,k=4)\n<Instruction> Split the following list of 64 numbers into 4 lists of 16\nnumbers each, the first list should contain the first 16 numbers, the\nsecond list the second 16 numbers, the third list the third 16 numbers\nand the fourth list the fourth 16 numbers. Only output the final 4 lists\nin the following format without any additional text or thoughts!\n{{\n    \"List 1\": [3, 4, 3, 5, 7, 8, 1, ...],\n    \"List 2\": [2, 9, 2, 4, 7, 1, 5, ...],\n    \"List 3\": [6, 9, 8, 1, 9, 2, 4, ...],\n    \"List 4\": [9, 0, 7, 6, 5, 6, 6, ...]\n}} </Instruction>\n<Example>\nInput: [3, 1, 9, 3, 7, 5, 5, 4, 8, 1, 5, 3, 3, 2, 3, 0, 9, 7, 2, 2, 4, 4, 8, 5, 0, \n8, 7, 3, 3, 8, 7, 0, 9, 5, 1, 6, 7, 6, 8, 9, 0, 3, 0, 6, 3, 4, 8, 0, 6, 9, 8, 4, 1, \n2, 9, 0, 4, 8, 8, 9, 9, 8, 5, 9]\nOutput:\n{{\n    \"List 1\": [3, 1, 9, 3, 7, 5, 5, 4, 8, 1, 5, 3, 3, 2, 3, 0],\n    \"List 2\": [9, 7, 2, 2, 4, 4, 8, 5, 0, 8, 7, 3, 3, 8, 7, 0],\n    \"List 3\": [9, 5, 1, 6, 7, 6, 8, 9, 0, 3, 0, 6, 3, 4, 8, 0],\n    \"List 4\": [6, 9, 8, 4, 1, 2, 9, 0, 4, 8, 8, 9, 9, 8, 5, 9]\n}}\n</Example>\nInput: {input}\nThis prompt is used by an operation Generate where\nthe branching factor is k = 4. Four new thoughts are\nconstructed based on the LLM reply to this prompt.\nThe input\nthought t\n1\nGenerate(t,k=1)+Repeat(k=4)A prompt used by\n<Instruction> Sort the following list of numbers in ascending order.\nOutput only the sorted list of numbers, no additional text. </Instruction>\n<Example>\nInput: [3, 7, 0, 2, 8, 1, 2, 2, 2, 4, 7, 8, 5, 5, 3, 9, 4, 3, 5, 6, 6, 4, 4, 5, \n2, 0, 9, 3, 3, 9, 2, 1]\nOutput: [0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, \n6, 6, 7, 7, 8, 8, 9, 9, 9]\n</Example>\nInput: {input}\nThe input\nthought t\nThis prompt is used by an operation Generate where the\nbranching factor is k=1, which means, only one thought is\ngenerated. However, as we chain it with the operation Repeat\nwith k=4, the underlying GoT framework ensures that Generate\nexecutes 4 times and results in 4 separate thoughts. Note that, from the graph\ntheory perspective, the GRS is identical to that in the operation Generate(t, k=4).\nThe difference between these two is that Generate(t, k=4) gives the user more \ncontrol over how these multiple thoughts are constructed, while Generate(t, \nk=1)+Repeat(k=4) is less flexible but more easy to use. Moreover, with Repeat \none has 4 context-isolated responses from the LLM for identical prompts, \nwhereas without Repeat there is only one context where all 4 thoughts are\ngenerated and must be explicitly handled in a single prompt/session.\n2\nFigure 3: The system architecture of GoT, and the APIs of respective modules. The user can straightforwardly extend the design\ntowards new prompting schemes, experiment with novel thought transformations, and plug in different LLMs. The blue part of\nthe figure contains the architecture overview, the green part lists the API, and the red part contains example prompts together\nwith a GRS and operations involved.\n5\n5 Example Use Cases\nWe now describe several use cases of GoT. We detail one\nuse case (sorting) and summarize the others.\n5.1 Sorting\nWe focus on the decomposition of the sorting use case and\nGraph of Operations, which are central for implementing\nand executing any workload within GoT.\nWe consider sorting numbers 0–9 with duplicates. The\nconsidered LLMs are unable to sort a sequence of such num-\nbers correctly beyond a certain length consistently because\nduplicate counts do not match.\nIn GoT, we employ merge-based sorting: First, one de-\ncomposes the input sequence of numbers into subarrays.\nThen, one sorts these subarrays individually, and then re-\nspectively merges them into a final solution. Figure 4 illus-\ntrates this use case together with its graph decomposition.\nHere, an LLM thought is a sequence of sorted numbers.\nTo score an outcome, denote an input sequence with\n[a1, a2, ..., an] and an output one with [b1, b2, ..., bm]. We\nuse the following score that determines “the scope” of er-\nrors:\nerror-scope = X + Y\nwhere p ∈ {1, ..., m}, q ∈ {1, ..., n}, and\nX =\nm−1X\ni=1\nsgn(max(bi − bi+1, 0)),\nY =\n9X\ni=0\n| |{bp : bp = i}| − |{aq : aq = i}| |\nHere, X indicates how many consecutive pairs of num-\nbers are incorrectly sorted. If two numbers i and i + 1\nare incorrectly sorted (i.e., bi > bi+1), then the expression\nwithin the summation returns 1, increasing the error score\nby one. For two numbers correctly sorted, this expression\namounts to 0. Then, Y determines how well a given output\nsequence preserves the frequency of output numbers. Specif-\nically, for each considered number x (x ∈ {0, ...,9}), we\nobtain the difference between the count of input elements\nbeing equal to x, vs. the count of output elements equal\nto x. For an output sequence perfectly preserving the fre-\nquency of x, this would amount to 0. Any single “devia-\ntion” in this count, increases the “error scope” by 1. We\nthen sum this over all considered values of x. When plot-\nting this score, to improve the clarity of plots, we addition-\nally apply clipping min(error-scope, n), as some baselines\n(IO, CoT) result in large numbers of outliers with high er-\nror scope. Finally, to use a “positive score” describing “the\nscope of correctly sorted” elements, one can use the value\nmax(n − error-scope, 0).\n5.2 Set Operations\nMoreover, we also consider set operations, focusing on set\nintersection. They have numerous applications (particularly\nset intersection) in problems ranging from genome or docu-\nment comparisons to pattern matching [9–11, 20, 27, 38, 50,\n.....\n.....\n..... .....\n1 4  ...  4 3\n16 numbers\n8 2  ...  1 3\n16 numbers\n1 1  ...  4 2 1 9  ...  5 4\n16 numbers\n16 numbers\nSort\nPartial solution\nGraph of Operations (GoO) for sorting 64 numbers\nPartial solution Partial solution Partial solution\nk = 3\nGenerate(k)\nScore\nSort\nGenerate(k)\nSort\nGenerate(k)\n1 2  ...  7 8\n16 numbers\n1 1  ...  5 7\n16 numbers\nPartial solution Partial solution\n1 2  ...  4 8\n16 numbers\nPartial solution\n1 2  ...  7 8\n16 numbers\n1 1  ...  5 7\n16 numbers\nPartial solution Partial solution\n1 2 ... 4 8\n16 numbers\nPartial solution\nScore: 78% Score: 86%\nKeepBest(N)\nKeep the best\nscored thoughts\nN = 1\nMerge into a 32\nelement subarray\nAggregate(k)\nk = 10\nk = 3 k = 3\nAssess how well each sequence is sorted\nHow do we score?\n64 numbers\n1 4 6 2 4  ...  9 8 7 5 4\nSplitting into four\n16-element chunks\nGenerate(k) k = 1\nInput\nSort\nGenerate(k)\nk = 3\nScore: 100%\n1 2  ...  4 8\n16 numbers\nPartial solution\nScore: 100%\n1 3  ...  4 6\n16 numbers\nPartial solution\nScore: 97%\n..... .....\n.....\n1 1  ...  8 9\n32 numbers\nPartial solution\nScore: 100%\n1 1  ...  6 8\n32 numbers\nPartial solution\nScore: 97%\nMerge into a 64\nelement array\nAggregate(k)\nk = 10\nS\nScore\nS\nScore\nS\nScore\nS\nScore\nK\nG\nScore\nG\nScore\nScore Score\nK K K\nA\nG\nK\nA\nA\nScore\nScore\nK K\nK K\nK\nG\nS\nA\nK\nG\nLegend\nGenerate\nDetails of the highlighted\npart of GoO are below \nDetails of the highlighted part of the GoO from above\nThe first Generate\nsplits the 64-element\ninput array into four\n16-element chunks.\nSorting is implemented within\nthe Generate operation. Here,\nk=3 means that, for each 16\nelement chunk, we generate\nthree different sortings. \nHere, N=1 means that we\nmaintain a single best\nsorting outcome out of\nthe three input ones.\nHere, k=10 means that we try 10 different\naggregations of the two input 16-element subarrays.\nTo obtain the score, for every\nnumber 0 - 9, we get the\ndifference between the input\nand the sorted list, and we sum\nall 10 values. Zero indicates\ncorrectly sorted. To show\n\"the higher the better\", we do\nmax(input_length - score, 0)\nNote that this is an example graph decomposition. The structure\nof connections between all operations can be arbitrarily modified.\nSort\nKeepBest\nAggregate\nFigure 4: An example graph decomposition of the sorting\nuse case in GoT. All used operations (Generate, Aggregate,\nScore, KeepBest) are described in Figure 3.\n6\n58]. Set intersection of two sets is implemented similarly as\nthe sorting. The second input set is split into subsets and the\nintersection of those subsets with the first input set is deter-\nmined with the help of the LLM. Afterwards the resulting\nintersection sets are aggregated for the final results. For the\nevaluation we use different set sizes of 32, 64 and 128 el-\nements and we vary the number of elements found in both\nsets to be between 25% and 75%.\nOur score indicates the total number of missing or in-\ncorrectly included elements in the final intersection. Specif-\nically, denote two input sets with A = [ a1, a2, ..., an]\nand B = [ b1, b2, ..., bn], and the output set with C =\n[c1, c2, ..., cm]. Then,\nerror-scope = X1 + X2 + Xd\nwhere X1 = |C \\(A ∩B)| are the number of elements in C\nthat are not supposed to be there,X2 = |(A∩B)\\C| are the\nnumber of elements missing from C, and Xd is the number\nof duplicates in C (because the LLM expresses the set as a\nlist in natural language). Finally, to use a “positive score”\ndescribing “the scope of correctly computed” elements, one\ncan use the value max(n − error-scope, 0).\n5.3 Keyword Counting\nKeyword counting finds the frequency of keywords in a\ngiven category (countries in our example implementation)\nwithin the input text. GoT splits the input text into multiple\npassages, counts the keywords in each passage and aggre-\ngates the subresults. The number of passages is configurable\nand can also be left to the LLM, making it possible to treat\neach sentence as a separate passage. Here, to score a thought,\nwe first – for each keyword – derive the absolute difference\nbetween the computed count and the correct one. We then\nsum all these differences to get the final score.\n5.4 Document Merging\nFinally, we also provide document merging. Here, the goal\nis to generate a new Non-Disclosure Agreement (NDA) doc-\nument based on several input ones that partially overlap\nin terms of their contents. The goal is to ensure minimal\namount of duplication, while maximizing information reten-\ntion. Document merging is broadly applicable in, e.g., legal\nprocedures, where multiple sources of information have to\nbe combined into a single document or article. To score a\nsolution, we query the LLM for two values (3 times for each\nvalue, and take the average). The first value corresponds to\nthe solution redundancy (10 indicates no redundancy, 0 im-\nplies at least half the information is redundant), the second\nvalue stands for information retention (10 indicates all infor-\nmation is retained, 0 says that none is retained). We compute\nthe harmonic mean of these values.\n6 The Latency-Volume Tradeoff\nWe now show that GoT improves upon previous prompting\nschemes in terms of the tradeoff between latency (number of\nhops in the graph of thoughts to reach a given final thought)\nand volume. We define volume – for a given thought t – as\nthe number of preceding LLM thoughts that could have im-\npacted t. Formally, the volume oft is the number of thoughts\nfrom which there exists a path to t in the graph of thoughts.\nWe assume that outputting a single thought costs O(1) time\nand fix the total cost to Θ(n) for each prompting scheme.\nThe structure of the schemes is as follows. CoT-SC con-\nsists of k independent chains originating from a single start-\ning thought. ToT is a complete k-ary tree. Finally, in GoT, a\ncomplete k-ary tree is joined at its leaves with a “mirrored”\nk-ary tree of the same size but with its edges reversed.\nThe analysis is detailed in Table 2. CoT offers a large vol-\nume of up to N, but at the cost of a high latency of N. CoT-\nSC reduces the latency by a factor of k (which corresponds\nto its branching factor), but it simultaneously decreases the\nvolume by k as well. ToT offers a latency of logk N but\nalso has low volume. GoT is the only scheme to come with\nboth a low latency of logk N and a high volume N. This\nis enabled by the fact that GoT harnesses aggregations of\nthoughts, making it possible to reach the final thought from\nany other intermediate thought in the graph decomposition.\nScheme Latency Volume\nChain-of-Thought (CoT) N N\nSelf-Consistency with CoT (CoT-SC) N/k N/k\nTree of Thoughts (ToT) logk N O (logk N)\nGraph of Thoughts (GoT) logk N N\nTable 2: Comparison of prompting schemes, with respect\nto their fundamental tradeoff between latency and volume.\nGoT offers the best tradeoff.\n7 Evaluation\nWe show the advantages of GoT over the state of the art. We\nfocus on comparing GoT to ToT, as it was shown to consis-\ntently outperform other schemes. Still, for a broad compari-\nson, we also experiment with IO, CoT, and CoT-SC. As our\nanalysis results in a large evaluation space, we present rep-\nresentative results and omit data that does not bring relevant\ninsights (e.g., CoT-SC).\n7.1 Evaluation Methodology\nWe use 100 input samples for each task and comparison\nbaseline. We set the temperature to 1.0 and use a 4k con-\ntext size unless stated otherwise. For each experiment, we\nfix the numbers of thoughts in respective schemes to achieve\nsimilar costs in each experiment.\nParameters We experiment extensively with the branch-\ning factor k and the number of levels L to ensure that we\ncompare GoT to cost-effective and advantageous configu-\nrations. We plot two variants of ToT: one with higher k\nand lower depth (ToT), the other with lower k but higher L\n(ToT2). We usually aim to achieve a sweet spot in the trade-\noff between sparser generation rounds (lower k) vs. more\nrounds (larger L). Usually more responses per round is more\nexpensive (e.g., 80 vs. 60 total responses for Figure 7 but $6\nvs. $3 costs). We also try different problem sizes P (e.g., in\nsorting, P states how many numbers are to be sorted).\n7\nIO CoT ToT ToT2GoT\n0\n2\n4\n6\n8\n10\n12\n14\n16#incorrectly sorted elements; the lower the better\n32 elements\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nIO CoT ToT ToT2GoT\n0\n4\n8\n12\n16\n20\n24\n28\n32\n36\n40\n44\n48\n52\n56\n60\n64\n64 elements\n0.0\n0.3\n0.6\n0.9\n1.2\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0\n3.3\n3.6\n3.9\n4.2\n4.5\n4.8\nIO CoT ToT ToT2GoT\n0\n8\n16\n24\n32\n40\n48\n56\n64\n72\n80\n88\n96\n104\n112\n120\n128\n128 elements\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\nTotal Cost ($); the lower the better\nL=2\nk=20\nL=3\nk=10\nGoT: Figure 4 & Appendix\nclipped\nL=4\nk=20\nL=7\nk=10\nGoT: Figure 4\n& Appendix clipped\nL=4\nk=20\nL=10\nk=10\nGoT:\nFigure 4\n&\nAppendix\nFigure 5: Number of errors and cost in sorting tasks with ChatGPT-3.5. L and k indicate the structure of ToT (see Sections 3.2\nand 6).\nUsed LLMs Due to budget restrictions, we focus on GPT-\n3.5. We also experimented with Llama-2, but it was usually\nworse than GPT-3.5 and also much slower to run, making it\ninfeasible to obtain enough samples.\n7.2 Analysis of GoT’s Advantages\nThe results of the analysis are in Figure 5 (sorting), 6 (set\nintersection), 7 (keyword counting), and 8 (document merg-\ning); see Section 5 for the description of specific use cases.\nOverall, GoT improves the quality of outcomes over all the\nconsidered baselines and it reduces inference costs com-\npared to ToT.\nGoT vs. ToT GoT improves upon ToT and ToT2 by a\nlarge margin over all the considered problem instances. ToT\nusually comes with somewhat higher quality than ToT2, but\nsimultaneously much higher costs. GoT’s costs are always\nlower than ToT, and comparable (in some cases lower, in\nothers higher) to ToT2. For example, it reduces median er-\nror by ≈62%, thereby achieving a higher quality of sorting,\nfor P = 128 in comparison to ToT while ensuring >31%\ncost reductions. These advantages are due to GoT’s ability to\ndecompose complex tasks into simpler subtasks, solve these\nsubtasks independently, and then incrementally merge these\noutcomes into the final result.\nGoT vs. IO and CoT GoT consistently delivers much\nhigher quality of outcomes than IO/CoT. For example, for\nsorting (P = 64), GoT’s median error is ≈65% and ≈83%\nlower than, respectively, CoT and IO. Yet, the costs of GoT\n– and ToT – are much higher than in IO and CoT. This is\nmostly due to our configuration of CoT, where we do not ar-\ntificially inflate the lengths of the chains of reasoning if this\ndoes not improve the outcomes. The higher costs of GoT and\nToT are driven by k new thoughts built for each Generate\noperation; these multiple thoughts are one of the reasons for\nGoT’s superiority in quality.\nIncreasing Complexity of Tackled Problems Most im-\nportantly, the advantages of GoT in the quality increase for\nall the baselines with the growing size of the problemP. For\nexample, in sorting, while for P = 32 GoT only negligibly\nimproves upon ToT2, its median error count becomes lower\nby ≈61% for P = 64 and ≈69% for P = 128. The quar-\ntiles also become respectively better. The results for other\nschemes also follow the intuition; for example, IO becomes\nconsistently worse with the increasing P, which is expected\nas a single thought is unlikely to solve a large problem in-\nstance. Overall, this analysis illustrates that GoT is indeed\nwell-suited for elaborate problem cases , as the execution\nschedules usually become more complex with the growing\nproblem sizes.\n7.3 Discussion on Task Decomposition\nWhen splitting a task into subtasks and then solving these\nsubtasks, the size of responses and the input (in tokens) are\nreduced proportionally to the degree of the task decomposi-\ntion. However, the “static” part of the prompt (i.e., few-shot\nexamples) may become a significant overhead (see GoT4 to\nGoT8 in Figure 7). Here, we observe that these few-shot ex-\namples can usually also be reduced in size (e.g., the passages\nused to demonstrate keyword counting can also be made\nsmaller and still be indicative of the actual input size), thus\nactively working towards decreasing the cost (e.g., see the\ndifference between GoT8 and GoTx in Figure 7).\nThe overall goal when conducting graph decomposition is\nto break down a task to the point, where the LLM can solve\nit correctly for the majority of time using a single prompt\n(or with a few additional improvement steps). This signifi-\ncantly lowers the number of improvement/refinement steps\nneeded during the later stages of the graph exploration. Fur-\nthermore, as indicated by our results, combining or concate-\nnating subresults is usually an easier task than solving large\ntask instances from scratch. Hence, the LLM is often suc-\ncessful when aggregating the final solution.\n8 Related Work\nWe summarize relations between GoT and related work.\n8\nIO CoT T oTT oT2GoT\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18#incorrect elements; the lower the better\n7 6 31 29 43\n32 elements\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nIO CoT ToT ToT2GoT\n0\n4\n8\n12\n16\n20\n24\n28\n32\n 0 0 0 0 4\n64 elements\n0.0\n0.6\n1.2\n1.8\n2.4\n3.0\n3.6\n4.2\n4.8\nIO CoT ToT ToT2GoT\n0\n8\n16\n24\n32\n40\n48\n56\n64\n72\n80\n88\n0 0 0 0 0\n128 elements\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\nTotal Cost ($); the lower the better\nL=2\nk=20\nSolved \ncorrectly:\nL=7\nk=10\nL=4\nk=25\nL=9\nk=10\nL=3\nk=10\nL=4\nk=20\nGoT: Appendix GoT: Appendix GoT: Appendix\nFigure 6: Number of errors and cost in set intersection tasks with ChatGPT-3.5. L and k indicate the structure of ToT (see\nSections 3.2 and 6).\nIO CoT ToT ToT2 GoT4GoT8GoTx\n0\n5\n10\n15\n20\n25\n30\n35Number of errors; the lower the better\n0 0 1 0 8 7 25\n0\n1\n2\n3\n4\n5\n6\n7\n8\nTotal Cost ($); the lower the better\nSamples solved\ncorrectly\nSplits the input text into 4 passages, counts\nkeywords in each one, aggregates the sub-\nresults always 2 at a time\nL=4\nk=20\nL=6\nk=10\nSplits the\ninput into\nsentences\n(each input\nhas 12-19\nsentences)\nAs GoT4, but splits the\ninput text into 8 passages\nFigure 7: Number of errors and cost in keyword counting\nwith ChatGPT-3.5.L and k indicate the structure of ToT (see\nSections 3.2 and 6).\n8.1 Prompting Paradigms & Approaches\nWe detail different prompting paradigms in Section 1 and\nTable 1. There are numerous other works related to prompt-\ning. We now briefly summarize selected most related ones;\nmore extensive descriptions can be found in dedicated sur-\nveys [34, 40, 69, 70]. Wang et al. proposed Plan-and-\nSolve, an approach to enhance CoT with an explicit plan-\nning stage [66]. Using complexity-based criteria to enhance\nprompting within a CoT was designed by Fu et al. [29, 67].\nThe self-taught reasoner (STaR) [80] generates several chain\nof thoughts, and selects the ones that are valid. Similarly, a\nscheme by Shum et al. [61] generates a pool of CoT candi-\ndates, and selects the best candidate based on whether the\ncandidates match the ground truth and on a policy gradient-\nbased method. Automatic prompt generation overcomes the\nIO CoT ToT GoT GoT2\n0\n2\n4\n6\n8Score (out of 10); the higher the better\n0\n3\n6\n9\n12\n15\nTotal Cost ($); the lower the better\nL=3\nk=10\nAggregation of fully\nmerged NDAs\nAggregation\nof partially\nmerged\nNDAs\nFigure 8: Score and cost in document merging with\nChatGPT-3.5.L and k indicate the structure of ToT (see Sec-\ntions 3.2 and 6). Number of samples: 50; context size: 16k\ntokens.\nissues of scaling in CoT [41, 42, 59]. Zhou et al. pro-\npose to harness selecting the best prompt out of a candidate\nset [84]. Skeleon-of-Thought [47] generates at first a num-\nber of skeleton answers (brief bullet points of 3 to 5 words)\nand expands on these points in parallel in a second step.\nFinally, in prompt chaining, one cascades different LLMs.\nThis enables prompting different LLMs via different con-\ntexts, enabling more powerful reasoning [21, 23, 48, 51, 72,\n73, 73]. GoT is orthogonal to this class of schemes, as it\nfocuses on a single context capabilities.\n8.2 Self-Reflection & Self-Evaluation\nSelf-reflection and self-evaluation were introduced re-\ncently [45, 49, 60, 75, 85]. They are used to enhance dif-\nferent tasks, for example for code generation [17] or com-\n9\nputer operation tasks [39]. In GoT, we partially rely on\nself-evaluation when taking decisions on how to expand the\ngraph of thoughts within a prompt.\n8.3 LLMs & Planning\nThere are many works recently on how to plan complex\ntasks with LLMs [36, 37, 68, 76, 78, 81]. GoT could be seen\nas a generic framework that could potentially be used to en-\nhance such schemes, by offering a paradigm for generating\ncomplex graph-based plans.\n8.4 Graphs and Graph Computing\nGraphs have become an immensely popular and important\npart of the general computing landscape [31, 32, 44, 46, 56].\nRecently, there has been a growing interest in domains\nsuch as graph databases [2–4, 7, 55], graph pattern match-\ning [8, 10, 11, 18, 25, 62], graph streaming [1, 22, 26],\nand graph machine learning as well as graph neural net-\nworks [5, 6, 12, 16, 30, 33, 33, 57, 74, 82, 83]. The graph\nabstraction has been fruitful for many modern research do-\nmains, such as social sciences (e.g., studying human inter-\nactions), bioinformatics (e.g., analyzing protein structures),\nchemistry (e.g., designing chemical compounds), medicine\n(e.g., drug discovery), cybersecurity (e.g., identifying in-\ntruder machines), healthcare (e.g., exposing groups of peo-\nple who submit fraudulent claims), web graph analysis (e.g.,\nproviding accurate search services), entertainment services\n(e.g., predicting movie popularity), linguistics (e.g., model-\ning relationships between words), transportation (e.g., find-\ning efficient routes), physics (e.g., understanding phase tran-\nsitions and critical phenomena), and many others [15, 20,\n35, 38, 44]. In this work, we harness the graph abstraction\nas a key mechanism that enhances prompting capabilities in\nLLMs.\n9 Conclusion\nPrompt engineering is one of the central new domains of\nthe large language model (LLM) research. It enables using\nLLMs efficiently, without any model updates. However, de-\nsigning effective prompts is a challenging task.\nIn this work, we propose Graph of Thoughts (GoT), a new\nparadigm that enables the LLM to solve different tasks effec-\ntively without any model updates. The key idea is to model\nthe LLM reasoning as an arbitrary graph, where thoughts\nare vertices and dependencies between thoughts are edges.\nThis enables novel transformations of thoughts, such as ag-\ngregation. Human’s task solving is often non-linear, and it\ninvolves combining intermediate solutions into final ones,\nor changing the flow of reasoning upon discovering new in-\nsights. GoT reflects this with its graph structure.\nGoT outperforms other prompting schemes, for example\nensuring 62% increase in the quality of sorting over ToT,\nwhile simultaneously reducing costs by>31%. We also pro-\npose a novel metric for a prompting scheme, the volume of\na thought, to indicate the scope of information that a given\nLLM output could carry with it, where GoT also excels. This\nprovides a step towards more principled prompt engineering.\nThe graph abstraction has been the foundation of several\nsuccessful designs in computing and AI over last decades,\nfor example AlphaFold for protein predictions. Our work\nharnesses it within the realm of prompt engineering.\nAcknowledgements\nWe thank Hussein Harake, Colin McMurtrie, Mark Klein, An-\ngelo Mangili, and the whole CSCS team granting access to the\nAult and Daint machines, and for their excellent technical sup-\nport. We thank Timo Schneider for help with infrastructure at\nSPCL. This project received funding from the European Re-\nsearch Council (Project PSAP, No. 101002047), and the European\nHigh-Performance Computing Joint Undertaking (JU) under grant\nagreement No. 955513 (MAELSTROM). This project was sup-\nported by the ETH Future Computing Laboratory (EFCL), financed\nby a donation from Huawei Technologies. This project received\nfunding from the European Union’s HE research and innovation\nprogramme under the grant agreement No. 101070141 (Project\nGLACIATION).\nReferences\n[1] Besta, M.; Fischer, M.; Kalavri, V .; Kapralov, M.; and\nHoefler, T. 2023. Practice of Streaming Processing\nof Dynamic Graphs: Concepts, Models, and Systems.\nIEEE Transactions on Parallel and Distributed Sys-\ntems, 34(6): 1860–1876.\n[2] Besta, M.; Gerstenberger, R.; Blach, N.; Fischer, M.;\nand Hoefler, T. 2023. GDI: A Graph Database Inter-\nface Standard. https://github.com/spcl/GDI-RMA. Ac-\ncessed: 2023-09-05.\n[3] Besta, M.; Gerstenberger, R.; Fischer, M.; Podstawski,\nM.; Blach, N.; Egeli, B.; Mitenkov, G.; Chlapek, W.;\nMichalewicz, M.; Niewiadomski, H.; M ¨uller, J.; and\nHoefler, T. 2023. The Graph Database Interface: Scal-\ning Online Transactional and Analytical Graph Work-\nloads to Hundreds of Thousands of Cores. In Proceed-\nings of the International Conference for High Perfor-\nmance Computing, Networking, Storage and Analysis,\nSC ’23. ACM.\n[4] Besta, M.; Gerstenberger, R.; Peter, E.; Fischer, M.;\nPodstawski, M.; Barthels, C.; Alonso, G.; and Hoefler,\nT. 2023. Demystifying Graph Databases: Analysis and\nTaxonomy of Data Organization, System Designs, and\nGraph Queries. ACM Comput. Surv., 56(2).\n[5] Besta, M.; Grob, R.; Miglioli, C.; Bernold, N.;\nKwa´sniewski, G.; Gjini, G.; Kanakagiri, R.; Ashkboos,\nS.; Gianinazzi, L.; Dryden, N.; and Hoefler, T. 2022.\nMotif Prediction with Graph Neural Networks. In\nProceedings of the 28th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, KDD ’22,\n35–45.\n[6] Besta, M.; and Hoefler, T. 2022. Parallel and Dis-\ntributed Graph Neural Networks: An In-Depth Concur-\nrency Analysis. arXiv:2205.09702.\n[7] Besta, M.; Iff, P.; Scheidl, F.; Osawa, K.; Dryden, N.;\nPodstawski, M.; Chen, T.; and Hoefler, T. 2022. Neural\nGraph Databases. In Proceedings of the First Learning\non Graphs Conference, volume 198 of Proceedings of\nMachine Learning Research, 31:1–31:38. PMLR.\n10\n[8] Besta, M.; Kanakagiri, R.; Kwa ´sniewski, G.;\nAusavarungnirun, R.; Ber ´anek, J.; Kanellopoulos,\nK.; Janda, K.; V onarburg-Shmaria, Z.; Gianinazzi,\nL.; Stefan, I.; Luna, J. G.; Golinowski, J.; Copik,\nM.; Kapp-Schwoerer, L.; Di Girolamo, S.; Blach,\nN.; Konieczny, M.; Mutlu, O.; and Hoefler, T. 2021.\nSISA: Set-Centric Instruction Set Architecture for\nGraph Mining on Processing-in-Memory Systems. In\nProceedings of the 54th Annual IEEE/ACM Interna-\ntional Symposium on Microarchitecture, MICRO ’21,\n282–297.\n[9] Besta, M.; Kanakagiri, R.; Mustafa, H.; Karasikov,\nM.; R ¨atsch, G.; Hoefler, T.; and Solomonik, E. 2020.\nCommunication-Efficient Jaccard Similarity for High-\nPerformance Distributed Genome Comparisons. In\nProceedings of the IEEE International Parallel and\nDistributed Processing Symposium, IPDPS ’20, 1122–\n1132.\n[10] Besta, M.; Miglioli, C.; Labini, P. S.; T ˇetek, J.; Iff, P.;\nKanakagiri, R.; Ashkboos, S.; Janda, K.; Podstawski,\nM.; Kwa´sniewski, G.; Gleinig, N.; Vella, F.; Mutlu, O.;\nand Hoefler, T. 2022. ProbGraph: High-Performance\nand High-Accuracy Graph Mining with Probabilistic\nSet Representations. In Proceedings of the Interna-\ntional Conference on High Performance Computing,\nNetworking, Storage and Analysis, SC ’22. IEEE.\n[11] Besta, M.; V onarburg-Shmaria, Z.; Schaffner, Y .;\nSchwarz, L.; Kwa ´sniewski, G.; Gianinazzi, L.; Be-\nranek, J.; Janda, K.; Holenstein, T.; Leisinger, S.;\nTatkowski, P.; Ozdemir, E.; Balla, A.; Copik, M.; Lin-\ndenberger, P.; Konieczny, M.; Mutlu, O.; and Hoe-\nfler, T. 2021. GraphMineSuite: Enabling High-\nPerformance and Programmable Graph Mining Algo-\nrithms with Set Algebra. Proc. VLDB Endow., 14(11):\n1922–1935.\n[12] Bronstein, M. M.; Bruna, J.; LeCun, Y .; Szlam, A.; and\nVandergheynst, P. 2017. Geometric Deep Learning:\nGoing beyond Euclidean data. IEEE Signal Process-\ning Magazine, 34(4): 18–42.\n[13] Brown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Ka-\nplan, J. D.; Dhariwal, P.; Neelakantan, A.; Shyam,\nP.; Sastry, G.; Askell, A.; Agarwal, S.; Herbert-V oss,\nA.; Krueger, G.; Henighan, T.; Child, R.; Ramesh, A.;\nZiegler, D.; Wu, J.; Winter, C.; Hesse, C.; Chen, M.;\nSigler, E.; Litwin, M.; Gray, S.; Chess, B.; Clark, J.;\nBerner, C.; McCandlish, S.; Radford, A.; Sutskever, I.;\nand Amodei, D. 2020. Language Models are Few-Shot\nLearners. In Advances in Neural Information Process-\ning Systems (NeurIPS ’20) , volume 33, 1877–1901.\nCurran Associates.\n[14] Bubeck, S.; Chandrasekaran, V .; Eldan, R.; Gehrke,\nJ.; Horvitz, E.; Kamar, E.; Lee, P.; Lee, Y . T.; Li,\nY .; Lundberg, S.; Nori, H.; Palangi, H.; Ribeiro,\nM. T.; and Zhang, Y . 2023. Sparks of Artificial\nGeneral Intelligence: Early experiments with GPT-4.\narXiv:2303.12712.\n[15] Chakrabarti, D.; and Faloutsos, C. 2006. Graph Min-\ning: Laws, Generators, and Algorithms. ACM Comput.\nSurv., 38(1).\n[16] Chami, I.; Abu-El-Haija, S.; Perozzi, B.; R ´e, C.;\nand Murphy, K. 2020. Machine Learning on\nGraphs: A Model and Comprehensive Taxonomy.\narXiv:2005.03675.\n[17] Chen, X.; Lin, M.; Sch ¨arli, N.; and Zhou, D. 2023.\nTeaching Large Language Models to Self-Debug.\narXiv:2304.05128.\n[18] Cheng, J.; Yu, J. X.; Ding, B.; Philip, S. Y .; and Wang,\nH. 2008. Fast Graph Pattern Matching. In Proceedings\nof the IEEE 24th International Conference on Data En-\ngineering, ICDE ’08, 913–922.\n[19] Chowdhery, A.; Narang, S.; Devlin, J.; Bosma,\nM.; Mishra, G.; Roberts, A.; Barham, P.; Chung,\nH. W.; Sutton, C.; Gehrmann, S.; Schuh, P.; Shi, K.;\nTsvyashchenko, S.; Maynez, J.; Rao, A.; Barnes, P.;\nTay, Y .; Shazeer, N.; Prabhakaran, V .; Reif, E.; Du,\nN.; Hutchinson, B.; Pope, R.; Bradbury, J.; Austin, J.;\nIsard, M.; Gur-Ari, G.; Yin, P.; Duke, T.; Levskaya,\nA.; Ghemawat, S.; Dev, S.; Michalewski, H.; Garcia,\nX.; Misra, V .; Robinson, K.; Fedus, L.; Zhou, D.; Ip-\npolito, D.; Luan, D.; Lim, H.; Zoph, B.; Spiridonov,\nA.; Sepassi, R.; Dohan, D.; Agrawal, S.; Omernick,\nM.; Dai, A. M.; Pillai, T. S.; Pellat, M.; Lewkowycz,\nA.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou,\nZ.; Wang, X.; Saeta, B.; Diaz, M.; Firat, O.; Catasta,\nM.; Wei, J.; Meier-Hellstern, K.; Eck, D.; Dean, J.;\nPetrov, S.; and Fiedel, N. 2022. PaLM: Scaling Lan-\nguage Modeling with Pathways. arXiv:2204.02311.\n[20] Cook, D. J.; and Holder, L. B., eds. 2006. Mining\nGraph Data. John Wiley & Sons.\n[21] Creswell, A.; Shanahan, M.; and Higgins, I. 2022.\nSelection-Inference: Exploiting Large Language\nModels for Interpretable Logical Reasoning.\narXiv:2205.09712.\n[22] Dhulipala, L.; Blelloch, G. E.; and Shun, J. 2019. Low-\nLatency Graph Streaming Using Compressed Purely-\nFunctional Trees. In Proceedings of the 40th ACM\nSIGPLAN Conference on Programming Language De-\nsign and Implementation, PLDI ’19, 918–934.\n[23] Dohan, D.; Xu, W.; Lewkowycz, A.; Austin, J.; Bieber,\nD.; Lopes, R. G.; Wu, Y .; Michalewski, H.; Saurous,\nR. A.; Sohl-Dickstein, J.; Murphy, K.; and Sutton, C.\n2022. Language Model Cascades. In Beyond Bayes:\nPaths Towards Universal Reasoning Systems , Work-\nshop at ICML ’22.\n[24] Drori, I.; Zhang, S.; Shuttleworth, R.; Tang, L.; Lu, A.;\nKe, E.; Liu, K.; Chen, L.; Tran, S.; Cheng, N.; Wang,\nR.; Singh, N.; Patti, T. L.; Lynch, J.; Shporer, A.;\nVerma, N.; Wu, E.; and Strang, G. 2022. A neural net-\nwork solves, explains, and generates university math\nproblems by program synthesis and few-shot learning\nat human level. Proceedings of the National Academy\nof Sciences, 119(32): e2123433119.\n[25] Fan, W.; Li, J.; Ma, S.; Tang, N.; Wu, Y .; and Wu,\nY . 2010. Graph Pattern Matching: From Intractable\n11\nto Polynomial Time. Proc. VLDB Endow. , 3(1–2):\n264–275.\n[26] Feng, G.; Meng, X.; and Ammar, K. 2015. DIS-\nTINGER: A distributed graph data structure for mas-\nsive dynamic graph processing. In Proccedings of the\nIEEE International Conference on Big Data, Big Data\n’15, 1814–1822.\n[27] Friggeri, A.; Chelius, G.; and Fleury, E. 2011. Trian-\ngles to Capture Social Cohesion. In Proceedings of\nthe IEEE Third International Conference on Privacy,\nSecurity, Risk and Trust and IEEE Third International\nConference on Social Computing, PASSAT/SocialCom\n’11, 258–265.\n[28] Friston, K. 2008. Hierarchical Models in the Brain.\nPLOS Computational Biology, 4(11): 1–24.\n[29] Fu, Y .; Peng, H.; Sabharwal, A.; Clark, P.; and Khot,\nT. 2022. Complexity-Based Prompting for Multi-Step\nReasoning. arXiv:2210.00720.\n[30] Gianinazzi, L.; Fries, M.; Dryden, N.; Ben-Nun, T.;\nBesta, M.; and Hoefler, T. 2021. Learning Combina-\ntorial Node Labeling Algorithms. arXiv:2106.03594.\n[31] Gregor, D.; and Lumsdaine, A. 2005. Lifting Sequen-\ntial Graph Algorithms for Distributed-Memory Parallel\nComputation. SIGPLAN Not., 40(10): 423–437.\n[32] Gregor, D.; and Lumsdaine, A. 2005. The Parallel\nBGL: A generic library for distributed graph compu-\ntations. Parallel Object-Oriented Scientific Computing\n(POOSC).\n[33] Hamilton, W. L.; Ying, R.; and Leskovec, J. 2017. Rep-\nresentation Learning on Graphs: Methods and Appli-\ncations. Bulletin of the Technical Committee on Data\nEngineering, 40(3): 52–74.\n[34] Hartmann, M.; and Sonntag, D. 2022. A survey on\nimproving NLP models with human explanations. In\nProceedings of the First Workshop on Learning with\nNatural Language Supervision, 40–47. Association for\nComputational Linguistics.\n[35] Horv ´ath, T.; G¨artner, T.; and Wrobel, S. 2004. Cyclic\nPattern Kernels for Predictive Graph Mining. In Pro-\nceedings of the Tenth ACM SIGKDD International\nConference on Knowledge Discovery and Data Min-\ning, KDD ’04, 158–167.\n[36] Huang, W.; Abbeel, P.; Pathak, D.; and Mordatch, I.\n2022. Language Models as Zero-Shot Planners: Ex-\ntracting Actionable Knowledge for Embodied Agents.\nIn Proceedings of the 39th International Conference\non Machine Learning , volume 162 of Proceedings of\nMachine Learning Research, 9118–9147. PMLR.\n[37] Huang, W.; Xia, F.; Xiao, T.; Chan, H.; Liang, J.; Flo-\nrence, P.; Zeng, A.; Tompson, J.; Mordatch, I.; Cheb-\notar, Y .; Sermanet, P.; Brown, N.; Jackson, T.; Luu,\nL.; Levine, S.; Hausman, K.; and Ichter, B. 2022. In-\nner Monologue: Embodied Reasoning through Plan-\nning with Language Models. arXiv:2207.05608.\n[38] Jiang, C.; Coenen, F.; and Zito, M. 2013. A survey of\nfrequent subgraph mining algorithms. The Knowledge\nEngineering Review, 28(1): 75–105.\n[39] Kim, G.; Baldi, P.; and McAleer, S. 2023. Language\nModels can Solve Computer Tasks. arXiv:2303.17491.\n[40] Lertvittayakumjorn, P.; and Toni, F. 2021.\nExplanation-Based Human Debugging of NLP\nModels: A Survey. Transactions of the Association for\nComputational Linguistics, 9: 1508–1528.\n[41] Lester, B.; Al-Rfou, R.; and Constant, N. 2021. The\nPower of Scale for Parameter-Efficient Prompt Tun-\ning. In Proceedings of the Conference on Empiri-\ncal Methods in Natural Language Processing, EMNLP\n’21, 3045–3059. Association for Computational Lin-\nguistics.\n[42] Li, X. L.; and Liang, P. 2021. Prefix-Tuning:\nOptimizing Continuous Prompts for Generation.\narXiv:2101.00190.\n[43] Long, J. 2023. Large Language Model Guided Tree-\nof-Thought. arXiv:2305.08291.\n[44] Lumsdaine, A.; Gregor, D.; Hendrickson, B.; and\nBerry, J. 2007. Challenges in Parallel Graph Process-\ning. Parallel Processing Letters, 17(1): 5–20.\n[45] Madaan, A.; Tandon, N.; Gupta, P.; Hallinan, S.; Gao,\nL.; Wiegreffe, S.; Alon, U.; Dziri, N.; Prabhumoye, S.;\nYang, Y .; Gupta, S.; Majumder, B. P.; Hermann, K.;\nWelleck, S.; Yazdanbakhsh, A.; and Clark, P. 2023.\nSelf-Refine: Iterative Refinement with Self-Feedback.\narXiv:2303.17651.\n[46] Malewicz, G.; Austern, M. H.; Bik, A. J.; Dehnert,\nJ. C.; Horn, I.; Leiser, N.; and Czajkowski, G. 2010.\nPregel: A System for Large-Scale Graph Processing. In\nProceedings of the International Conference on Man-\nagement of Data, SIGMOD ’10, 135–146. ACM.\n[47] Ning, X.; Lin, Z.; Zhou, Z.; Wang, Z.; Yang, H.; and\nWang, Y . 2023. Skeleton-of-Thought: Large Language\nModels Can Do Parallel Decoding. arXiv:2307.15337.\n[48] Nye, M.; Andreassen, A. J.; Gur-Ari, G.; Michalewski,\nH.; Austin, J.; Bieber, D.; Dohan, D.; Lewkowycz, A.;\nBosma, M.; Luan, D.; Sutton, C.; and Odena, A. 2021.\nShow Your Work: Scratchpads for Intermediate Com-\nputation with Language Models. arXiv:2112.00114.\n[49] Paul, D.; Ismayilzada, M.; Peyrard, M.; Borges, B.;\nBosselut, A.; West, R.; and Faltings, B. 2023. RE-\nFINER: Reasoning Feedback on Intermediate Repre-\nsentations. arXiv:2304.01904.\n[50] Prat-P ´erez, A.; Dominguez-Sal, D.; Brunat, J. M.; and\nLarriba-Pey, J.-L. 2012. Shaping Communities out\nof Triangles. In Proceedings of the 21st ACM Inter-\nnational Conference on Information and Knowledge\nManagement, CIKM ’12, 1677–1681.\n[51] Qiao, S.; Ou, Y .; Zhang, N.; Chen, X.; Yao, Y .; Deng,\nS.; Tan, C.; Huang, F.; and Chen, H. 2023. Reasoning\nwith Language Model Prompting: A Survey. In Pro-\nceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics , ACL ’23, 5368–5393.\nAssociation for Computational Linguistics.\n[52] qrdlgit. 2023. graph-of-thoughts Repository. https:\n//github.com/qrdlgit/graph-of-thoughts. Accessed:\n2023-10-11.\n12\n[53] Radford, A.; Narasimhan, K.; Salimans, T.; and\nSutskever, I. 2018. Improving Language Understand-\ning by Generative Pre-Training. https://openai.com/\nresearch/language-unsupervised. Accessed: 2023-09-\n06.\n[54] Radford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.;\nand Sutskever, I. 2019. Language Models are Unsuper-\nvised Multitask Learners. https://openai.com/research/\nbetter-language-models. Accessed: 2023-09-06.\n[55] Robinson, I.; Webber, J.; and Eifrem, E. 2015. Graph\nDatabases: New Opportunities for Connected Data .\nO’Reilly Media, 2nd edition.\n[56] Sakr, S.; Bonifati, A.; V oigt, H.; Iosup, A.; Ammar, K.;\nAngles, R.; Aref, W.; Arenas, M.; Besta, M.; Boncz,\nP. A.; Daudjee, K.; Valle, E. D.; Dumbrava, S.; Har-\ntig, O.; Haslhofer, B.; Hegeman, T.; Hidders, J.; Hose,\nK.; Iamnitchi, A.; Kalavri, V .; Kapp, H.; Martens, W.;\n¨Ozsu, M. T.; Peukert, E.; Plantikow, S.; Ragab, M.; Ri-\npeanu, M. R.; Salihoglu, S.; Schulz, C.; Selmer, P.; Se-\nqueda, J. F.; Shinavier, J.; Sz ´arnyas, G.; Tommasini,\nR.; Tumeo, A.; Uta, A.; Varbanescu, A. L.; Wu, H.-\nY .; Yakovets, N.; Yan, D.; and Yoneki, E. 2021. The\nFuture is Big Graphs: A Community View on Graph\nProcessing Systems. Commun. ACM, 64(9): 62–71.\n[57] Scarselli, F.; Gori, M.; Tsoi, A. C.; Hagenbuchner, M.;\nand Monfardini, G. 2008. The Graph Neural Network\nModel. IEEE Transactions on Neural Networks, 20(1):\n61–80.\n[58] Schaeffer, S. E. 2007. Graph clustering. Computer\nScience Review, 1(1): 27–64.\n[59] Shin, T.; Razeghi, Y .; Logan IV , R. L.; Wallace, E.;\nand Singh, S. 2020. AutoPrompt: Eliciting Knowledge\nfrom Language Models with Automatically Generated\nPrompts. arXiv:2010.15980.\n[60] Shinn, N.; Labash, B.; and Gopinath, A. 2023. Re-\nflexion: Language Agents with Verbal Reinforcement\nLearning. arXiv:2303.11366.\n[61] Shum, K.; Diao, S.; and Zhang, T. 2023. Automatic\nPrompt Augmentation and Selection with Chain-of-\nThought from Labeled Data. arXiv:2302.12822.\n[62] Teixeira, C. H. C.; Fonseca, A. J.; Serafini, M.;\nSiganos, G.; Zaki, M. J.; and Aboulnaga, A. 2015.\nArabesque: A System for Distributed Graph Mining.\nIn Proceedings of the 25th Symposium on Operating\nSystems Principles, SOSP ’15, 425–440. ACM.\n[63] Touvron, H.; Lavril, T.; Izacard, G.; Martinet, X.;\nLachaux, M.-A.; Lacroix, T.; Rozi `ere, B.; Goyal,\nN.; Hambro, E.; Azhar, F.; Rodriguez, A.; Joulin,\nA.; Grave, E.; and Lample, G. 2023. LLaMA:\nOpen and Efficient Foundation Language Models.\narXiv:2302.13971.\n[64] Touvron, H.; Martin, L.; Stone, K.; Albert, P.; Alma-\nhairi, A.; Babaei, Y .; Bashlykov, N.; Batra, S.; Bhar-\ngava, P.; Bhosale, S.; Bikel, D.; Blecher, L.; Ferrer,\nC. C.; Chen, M.; Cucurull, G.; Esiobu, D.; Fernandes,\nJ.; Fu, J.; Fu, W.; Fuller, B.; Gao, C.; Goswami, V .;\nGoyal, N.; Hartshorn, A.; Hosseini, S.; Hou, R.; Inan,\nH.; Kardas, M.; Kerkez, V .; Khabsa, M.; Kloumann,\nI.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.; Lavril,\nT.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .;\nPoulton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.;\nSchelten, A.; Silva, R.; Smith, E. M.; Subramanian,\nR.; Tan, X. E.; Tang, B.; Taylor, R.; Williams, A.;\nKuan, J. X.; Xu, P.; Yan, Z.; Zarov, I.; Zhang, Y .; Fan,\nA.; Kambadur, M.; Narang, S.; Rodriguez, A.; Sto-\njnic, R.; Edunov, S.; and Scialom, T. 2023. Llama\n2: Open Foundation and Fine-Tuned Chat Models.\narXiv:2307.09288.\n[65] Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.;\nJones, L.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I.\n2017. Attention is All you Need. In Advances in Neu-\nral Information Processing Systems (NIPS ’17) , vol-\nume 30. Curran Associates.\n[66] Wang, L.; Xu, W.; Lan, Y .; Hu, Z.; Lan, Y .; Lee, R.\nK.-W.; and Lim, E.-P. 2023. Plan-and-Solve Prompt-\ning: Improving Zero-Shot Chain-of-Thought Reason-\ning by Large Language Models. In Proceedings of the\n61st Annual Meeting of the Association for Computa-\ntional Linguistics, ACL ’23, 2609–2634. Association\nfor Computational Linguistics.\n[67] Wang, X.; Wei, J.; Schuurmans, D.; Le, Q. V .; Chi,\nE. H.; Narang, S.; Chowdhery, A.; and Zhou, D. 2023.\nSelf-Consistency Improves Chain of Thought Rea-\nsoning in Language Models. In Proceedings of the\nEleventh International Conference on Learning Rep-\nresentations, ICLR ’23.\n[68] Wang, Z.; Cai, S.; Chen, G.; Liu, A.; Ma, X.; and\nLiang, Y . 2023. Describe, Explain, Plan and Select:\nInteractive Planning with Large Language Models En-\nables Open-World Multi-Task Agents. In Advances in\nNeural Information Processing Systems (NeurIPS ’23),\nvolume 36. Curran Associates.\n[69] Wang, Z.; Zhang, G.; Yang, K.; Shi, N.; Zhou, W.;\nHao, S.; Xiong, G.; Li, Y .; Sim, M. Y .; Chen, X.;\nZhu, Q.; Yang, Z.; Nik, A.; Liu, Q.; Lin, C.; Wang,\nS.; Liu, R.; Chen, W.; Xu, K.; Liu, D.; Guo, Y .; and\nFu, J. 2023. Interactive Natural Language Processing.\narXiv:2305.13246.\n[70] Wang, Z. J.; Choi, D.; Xu, S.; and Yang, D. 2021.\nPutting Humans in the Natural Language Processing\nLoop: A Survey. In Proceedings of the First Work-\nshop on Bridging Human-Computer Interaction and\nNatural Language Processing, 47–52. Association for\nComputational Linguistics.\n[71] Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Chi,\nE.; Le, Q.; and Zhou, D. 2022. Chain-of-Thought\nPrompting Elicits Reasoning in Large Language Mod-\nels. arXiv:2201.11903.\n[72] Wu, T.; Jiang, E.; Donsbach, A.; Gray, J.; Molina, A.;\nTerry, M.; and Cai, C. J. 2022. PromptChainer: Chain-\ning Large Language Model Prompts through Visual\nProgramming. In Extended Abstracts of the Confer-\nence on Human Factors in Computing Systems , CHI\nEA ’22. ACM.\n13\n[73] Wu, T.; Terry, M.; and Cai, C. J. 2022. AI Chains:\nTransparent and Controllable Human-AI Interaction\nby Chaining Large Language Model Prompts. In Pro-\nceedings of the Conference on Human Factors in Com-\nputing Systems, CHI ’22. ACM.\n[74] Wu, Z.; Pan, S.; Chen, F.; Long, G.; Zhang, C.; and Yu,\nP. S. 2021. A Comprehensive Survey on Graph Neural\nNetworks. IEEE Transactions on Neural Networks and\nLearning Systems, 32(1): 4–24.\n[75] Xie, Y .; Kawaguchi, K.; Zhao, Y .; Zhao, X.; Kan, M.-\nY .; He, J.; and Xie, Q. 2023. Self-Evaluation Guided\nBeam Search for Reasoning. In Advances in Neural\nInformation Processing Systems (NeurIPS ’23) , vol-\nume 36. Curran Associates.\n[76] Yang, S.; Nachum, O.; Du, Y .; Wei, J.; Abbeel, P.; and\nSchuurmans, D. 2023. Foundation Models for Deci-\nsion Making: Problems, Methods, and Opportunities.\narXiv:2303.04129.\n[77] Yao, S.; Yu, D.; Zhao, J.; Shafran, I.; Griffiths, T. L.;\nCao, Y .; and Narasimhan, K. R. 2023. Tree of\nThoughts: Deliberate Problem Solving with Large\nLanguage Models. In Advances in Neural Information\nProcessing Systems (NeurIPS ’23), volume 36. Curran\nAssociates.\n[78] Yao, S.; Zhao, J.; Yu, D.; Du, N.; Shafran, I.;\nNarasimhan, K. R.; and Cao, Y . 2023. ReAct: Syner-\ngizing Reasoning and Acting in Language Models. In\nProceedings of the Eleventh International Conference\non Learning Representations, ICLR ’23.\n[79] Yao, Y .; Li, Z.; and Zhao, H. 2023. Beyond Chain-\nof-Thought, Effective Graph-of-Thought Reasoning in\nLarge Language Models. arXiv:2305.16582.\n[80] Zelikman, E.; Wu, Y .; Mu, J.; and Goodman, N. 2022.\nSTaR: Bootstrapping Reasoning With Reasoning. In\nAdvances in Neural Information Processing Systems\n(NeurIPS ’22), volume 35, 15476–15488. Curran As-\nsociates.\n[81] Zhang, S.; Chen, Z.; Shen, Y .; Ding, M.; Tenenbaum,\nJ. B.; and Gan, C. 2023. Planning with Large Lan-\nguage Models for Code Generation. In Proceedings\nof the Eleventh International Conference on Learning\nRepresentations, ICLR ’23.\n[82] Zhang, Z.; Cui, P.; and Zhu, W. 2022. Deep Learning\non Graphs: A Survey. IEEE Transactions on Knowl-\nedge and Data Engineering, 34(1): 249–270.\n[83] Zhou, J.; Cui, G.; Hu, S.; Zhang, Z.; Yang, C.; Liu,\nZ.; Wang, L.; Li, C.; and Sun, M. 2020. Graph neural\nnetworks: A review of methods and applications. AI\nOpen, 1: 57–81.\n[84] Zhou, Y .; Muresanu, A. I.; Han, Z.; Paster, K.;\nPitis, S.; Chan, H.; and Ba, J. 2022. Large Lan-\nguage Models Are Human-Level Prompt Engineers.\narXiv:2211.01910.\n[85] Zhu, X.; Wang, J.; Zhang, L.; Zhang, Y .; Huang, Y .;\nGan, R.; Zhang, J.; and Yang, Y . 2023. Solving Math\nWord Problems via Cooperative Reasoning induced\nLanguage Models. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Linguis-\ntics, ACL ’23, 4471–4485. Association for Computa-\ntional Linguistics.\n14\nA Positive Score Evaluation\nThe following figures plot the same data as Figures 5 and 6\nrespectively, however use the ”positive score” described in\nSections 5.1 and 5.2.\nIO CoT ToT ToT2GoT\n0\n4\n8\n12\n16\n20\n24\n28\n32\n36\n40\n44\n48\n52\n56\n60\n64\n64 elements\n0.0\n0.3\n0.6\n0.9\n1.2\n1.5\n1.8\n2.1\n2.4\n2.7\n3.0\n3.3\n3.6\n3.9\n4.2\n4.5\n4.8\nIO CoT ToT ToT2GoT\n0\n8\n16\n24\n32\n40\n48\n56\n64\n72\n80\n88\n96\n104\n112\n120\n128\n128 elements\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\nTotal Cost ($); the lower the better\nIO CoT ToT ToT2GoT\n16\n18\n20\n22\n24\n26\n28\n30\n32#correct elements; the higher the better\n32 elements\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6L=2\nk=20\nL=3\nk=10\nGoT: Figure 4 GoT: Figure 4 GoT: Figure 4\nL=4\nk=20\nL=7\nk=10 L=4\nk=20\nL=10\nk=10\nFigure 9: Accuracy and cost in sorting tasks with ChatGPT-\n3.5. L and k indicate the structure of ToT (see Sections 3.2\nand 6).\nIO CoT ToT ToT2GoT\n8\n10\n12\n14\n16\n18\n20\n22\n24\n26\n28\n30\n32#correct elements; the higher the better\n7 6 31 29 43\n32 elements\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nIO CoT ToT ToT2GoT\n16\n20\n24\n28\n32\n36\n40\n44\n48\n52\n56\n60\n64\n0 0 0 0 4\n64 elements\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nIO CoT ToT ToT2GoT\n0\n8\n16\n24\n32\n40\n48\n56\n64\n72\n80\n88\n96\n104\n112\n120\n128\n 0 0 0 0 0\n128 elements\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0\n7.5\n8.0\nTotal Cost ($); the lower the better\nL=2\nk=20\nL=3\nk=10\nSamples\nsolved\ncorrectly:\nL=4\nk=20\nL=7\nk=10\nL=4\nk=25\nL=9\nk=10\nFigure 10: Accuracy and cost in set intersection with\nChatGPT-3.5.L and k indicate the structure of ToT (see Sec-\ntions 3.2 and 6).\nB Example Prompts - Sorting\nWe present the prompts only for the sorting of 32-element\nlists, as those for 64-element and 128-element lists are iden-\ntical, except for the split prompt where the number of ele-\nments in the one-shot example matches the problem size.\nFor sorting, we employ three distinct types of operations\nthat interact with the LLM, each with its corresponding\nprompts. First, there is the Generate operation, utilizing the\nsort prompt to guide the LLM in sorting a provided list of\nvalues, and thesplit prompt to direct the LLM to split a spec-\nified list into a designated number of sublists. Next, the Im-\nprove operation employs the improve prompt to instruct the\nLLM to refine a sorted list if it detects mistakes. Finally, the\nAggregate operation leverages the merge prompt to guide\nthe LLM in merging two pre-sorted lists into a single sorted\nlist.\nFirst, we present the prompt stubs (Table 3), serving as\ntemplates to dynamically generate appropriate prompts at\nruntime. For clarity, we display their corresponding few-shot\nexamples separately in Table 4. Following this, we outline\nthe LLM interactions throughout the process of solving the\nsorting use case (Table 5 - Table 9).\n15\nTable 3: Prompt stubs for the sorting tasks; parameters in single curly brackets will be substituted at runtime.\nsort prompt: <Instruction> Sort the following list of numbers in ascending order. Output only the sorted list of numbers,\nno additional text. </Instruction>\n<Examples> See Table 4 </Examples>\nInput: {input list}\nsplit prompt (32 elements): <Instruction> Split the following list of 32 numbers into 2 lists of 16 numbers each, the first\nlist should contain the first 16 numbers and the second list the second 16 numbers.\nOnly output the final 2 lists in the following format without any additional text or thoughts!:\n{{\n\"List 1\": [3, 4, 3, 5, 7, 8, 1, ...],\n\"List 2\": [2, 9, 2, 4, 7, 1, 5, ...]\n}}\n</Instruction>\n<Examples> See Table 4 </Examples>\nInput: {input list}\nimprove prompt: <Instruction> The following two lists represent an unsorted list of numbers and a sorted variant of that\nlist. The sorted variant is not correct. Fix the sorted variant so that it is correct. Make sure that the output list is sorted in\nascending order, has the same number of elements as the input list ( {length}), and contains the same elements as the input\nlist.</Instruction>\n<Approach>\nTo fix the incorrectly sorted list follow these steps:\n1. For each number from 0 to 9, compare the frequency of that number in the incorrectly sorted list to the frequency of that\nnumber in the input list.\n2. Iterate through the incorrectly sorted list and add or remove numbers as needed to make the frequency of each number in\nthe incorrectly sorted list match the frequency of that number in the input list.\n</Approach>\n<Examples> See Table 4 </Examples>\nInput: {input list}\nIncorrectly Sorted: {sorted list}\nmerge prompt: <Instruction> Merge the following 2 sorted lists of length {length} each, into one sorted list of length\n{length combined} using a merge sort style approach. Only output the final merged list without any additional text or\nthoughts!: </Instruction>\n<Approach>\nTo merge the two lists in a merge-sort style approach, follow these steps:\n1. Compare the first element of both lists.\n2. Append the smaller element to the merged list and move to the next element in the list from which the smaller element\ncame.\n3. Repeat steps 1 and 2 until one of the lists is empty.\n4. Append the remaining elements of the non-empty list to the merged list.\n</Approach>\nMerge the following two lists into one sorted list:\n1. {input list1}\n2. {input list2}\nMerged list:\n16\nTable 4: Few-shot examples for each prompt used for the sorting tasks; some lists are truncated for brevity.\nsort prompt:\n<Examples>\nInput: [5, 1, 0, 1, 2, 0, 4, 8, 1, 9, 5, 1, 3, 3, 9, 7]\nOutput: [0, 0, 1, 1, 1, 1, 2, 3, 3, 4, 5, 5, 7, 8, 9, 9]\nInput: [3, 7, 0, 2, 8, 1, 2, 2, 2, 4, 7, 8, 5, 5, 3, 9, 4, 3, . . .(Omitted 14/32 numbers)]\nOutput: [0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, . . .(Omitted 14/32 numbers)]\nInput: [4, 4, 9, 7, 9, 7, 0, 0, 4, 9, 1, 7, 9, 5, 8, 7, 5, 6, . . .(Omitted 46/64 numbers)]\nOutput: [0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, . . .(Omitted 46/64 numbers)]\n</Examples>\nsplit prompt (32 elements):\n<Examples>\nInput: [9, 6, 7, 7, 2, 0, 2, 2, 3, 5, 0, 9, 2, 2, 4, 4, 5, 2, . . .(Omitted 14/32 numbers)]\nOutput:\n{{\n\"List 1\": [9, 6, 7, 7, 2, 0, 2, 2, 3, 5, 0, 9, 2, 2, 4, 4],\n\"List 2\": [5, 2, 5, 1, 2, 8, 3, 8, 3, 9, 6, 0, 4, 2, 2, 3]\n}}\n</Examples>\nimprove prompt:\n<Examples>\nInput: [3, 7, 0, 2, 8, 1, 2, 2, 2, 4, 7, 8, 5, 5, 3, 9]\nIncorrectly Sorted: [0, 0, 0, 0, 0, 1, 2, 2, 3, 3, 4, 4, 4, 5, 5, 7, 7, 8, 8, 9, 9, 9, 9]\nReason: The incorrectly sorted list contains four extra 0s, two extra 4s and three extra 9s and is\nmissing two 2s.\nOutput: [0, 1, 2, 2, 2, 2, 3, 3, 4, 5, 5, 7, 7, 8, 8, 9]\nInput: [6, 4, 5, 7, 5, 6, 9, 7, 6, 9, 4, 6, 9, 8, 1, 9, 2, 4, . . .(Omitted 14/32 numbers)]\nIncorrectly Sorted: [0, 1, 1, 2, 2, 3, 4, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, . . .(Omitted 14/32 numbers)]\nReason: The incorrectly sorted list contains two extra 4s and is missing two 6s and one 9.\nOutput: [0, 1, 1, 2, 2, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, . . .(Omitted 14/32 numbers)]\nInput: [4, 4, 9, 7, 9, 7, 0, 0, 4, 9, 1, 7, 9, 5, 8, 7, 5, 6, . . .(Omitted 46/64 numbers)]\nIncorrectly Sorted: [0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, . . .(Omitted 46/64 numbers)]\nReason: The incorrectly sorted list contains one extra 8 and is missing two 2s, one 3, three 4s, two\n5s, one 6, six 7s and one 9.\nOutput: [0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, . . .(Omitted 46/64 numbers)]\n</Examples>\nmerge prompt: -\n17\nTable 5: Sorting of a 32 element list: Execution plan (GoO)\nGoO:\n1. Split the input list into two sub-lists of equal size (split prompt)\n2. For each sub-list: Sort the sub-list (sort prompt) five times; score each sort attempt; keep the best\n3. Merge the sorted sub-lists into one fully sorted list (merge prompt) 10 times; score each merge attempt; keep the best\n4. Fix any potential mistakes in the sorted list (improve prompt) 10 times; score each improvement attempt; keep the best\nTable 6: Sorting of a 32 element list: Step 1 (Prompt/Response)\nStep 1 – Prompt:\n<Instruction> Split the following list of 32 numbers into 2 lists of 16 numbers each, the first list should contain the first 16\nnumbers and the second list the second 16 numbers.\nOnly output the final 2 lists in the following format without any additional text or thoughts!:\n{{\n\"List 1\": [3, 4, 3, 5, 7, 8, 1, ...],\n\"List 2\": [2, 9, 2, 4, 7, 1, 5, ...]\n}}\n</Instruction>\n<Examples> See Table 4 </Examples>\nInput: [8, 7, 1, 1, 1, 1, 3, 3, 0, 9, 4, 1, 0, 2, 5, 1, 0, 5, 6, 7, 1, 4, 5, 9, 4, 6, 2, 5, 8, 6, 2, 6]\nStep 1 – Response (Fully Correct):\n{{\n\"List 1\": [8, 7, 1, 1, 1, 1, 3, 3, 0, 9, 4, 1, 0, 2, 5, 1],\n\"List 1\": [0, 5, 6, 7, 1, 4, 5, 9, 4, 6, 2, 5, 8, 6, 2, 6]\n}}\n18\nTable 7: Sorting of a 32 element list: Step 2 (Prompts/Responses)\nStep 2a – Prompt:\n<Instruction> Sort the following list of numbers in ascending order. Output only the sorted list of numbers, no additional\ntext. </Instruction>\n<Examples> See Table 4 </Examples>\nInput: [8, 7, 1, 1, 1, 1, 3, 3, 0, 9, 4, 1, 0, 2, 5, 1]\nStep 2a – 5 Responses:\n1. [0, 0, 1, 1, 1, 1, 1, 1, 2, 3, 3, 4, 5, 7, 8, 9] (Fully Correct)\n2. [0, 0, 1, 1, 1, 1, 1, 2, 3, 3, 4, 5, 7, 8, 9] (1 Error - Missing one 1)\n3. [0, 0, 1, 1, 1, 1, 1, 2, 3, 3, 4, 5, 7, 8, 9] (1 Error - Missing one 1)\n4. [0, 0, 1, 1, 1, 1, 1, 2, 3, 3, 4, 5, 7, 8, 9] (1 Error - Missing one 1)\n5. [0, 0, 1, 1, 1, 1, 1, 2, 3, 3, 4, 5, 7, 8, 9] (1 Error - Missing one 1)\nStep 2b – Prompt:\n<Instruction> Sort the following list of numbers in ascending order. Output only the sorted list of numbers, no additional\ntext. </Instruction>\n<Examples> See Table 4 </Examples>\nInput: [0, 5, 6, 7, 1, 4, 5, 9, 4, 6, 2, 5, 8, 6, 2, 6]\nStep 2b – 5 Responses:\n1. [0, 1, 2, 2, 4, 4, 5, 5, 5, 6, 6, 6, 7, 8, 9] (1 Error - Missing one 6)\n2. [0, 1, 2, 2, 4, 4, 5, 5, 5, 6, 6, 6, 7, 8, 9] (1 Error - Missing one 6)\n3. [0, 1, 2, 2, 4, 4, 5, 5, 5, 6, 6, 6, 7, 8, 9] (1 Error - Missing one 6)\n4. [0, 1, 2, 2, 4, 4, 5, 5, 5, 6, 6, 6, 6, 7, 8, 9] (Fully Correct)\n5. [0, 1, 2, 2, 4, 4, 5, 5, 5, 6, 6, 6, 7, 8, 9] (1 Error - Missing one 6)\n19\nTable 8: Sorting of a 32 element list: Step 3 (Prompt/Responses)\nStep 3 – Prompt:\n<Instruction> Merge the following 2 sorted lists of length 16 each, into one sorted list of length 32 using a merge sort style\napproach. Only output the final merged list without any additional text or thoughts!: </Instruction>\n<Approach>\nTo merge the two lists in a merge-sort style approach, follow these steps:\n1. Compare the first element of both lists.\n2. Append the smaller element to the merged list and move to the next element in the list from which the smaller element\ncame.\n3. Repeat steps 1 and 2 until one of the lists is empty.\n4. Append the remaining elements of the non-empty list to the merged list.\n</Approach>\nMerge the following two lists into one sorted list:\n1. [0, 0, 1, 1, 1, 1, 1, 1, 2, 3, 3, 4, 5, 7, 8, 9]\n2. [0, 1, 2, 2, 4, 4, 5, 5, 5, 6, 6, 6, 6, 7, 8, 9]\nMerged list:\nStep 3 – 10 Responses:\n1. [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 6, 7, 7, 8, 8, 9, 9]\n(2 Errors - Missing one 1 and one 5)\n2. [0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 8, 8, 9, 9]\n(2 Errors - Missing two 1s)\n3. [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 8, 8, 9, 9]\n(2 Errors - Missing one 1 and got one extra 6)\n4. [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 8, 8, 9]\n(3 Errors - Missing one 1, one 9 and got one extra 6)\n5. [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 8, 8, 9, 9]\n(2 Errors - Missing one 1 and got one extra 6)\n6. [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 6, 7, 7, 8, 8, 9, 9]\n(2 Errors - Missing one 1 and one 5)\n7. [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7, 8, 8, 9, 9]\n(2 Errors - Missing one 1 and got one extra 6)\n8. [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 8, 8, 9, 9]\n(1 Error - Missing one 1)\n9. [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 8, 8, 9, 9]\n(1 Error - Missing one 1)\n10. [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 6, 7, 7, 8, 8, 9, 9]\n(2 Errors - Missing one 1 and one 5)\n20\nTable 9: Sorting of a 32 element list: Step 4 (Prompt/Responses)\nStep 4 – Prompt:\n<Instruction> The following two lists represent an unsorted list of numbers and a sorted variant of that list. The sorted\nvariant is not correct. Fix the sorted variant so that it is correct. Make sure that the output list is sorted in ascending order,\nhas the same number of elements as the input list (32), and contains the same elements as the input list.</Instruction>\n<Approach>\nTo fix the incorrectly sorted list follow these steps:\n1. For each number from 0 to 9, compare the frequency of that number in the incorrectly sorted list to the frequency of that\nnumber in the input list.\n2. Iterate through the incorrectly sorted list and add or remove numbers as needed to make the frequency of each number in\nthe incorrectly sorted list match the frequency of that number in the input list.\n</Approach>\n<Examples> See Table 4 </Examples>\nInput: [8, 7, 1, 1, 1, 1, 3, 3, 0, 9, 4, 1, 0, 2, 5, 1, 0, 5, 6, 7, 1, 4, 5, 9, 4, 6, 2, 5, 8, 6, 2, 6]\nIncorrectly Sorted: [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 8,\n8, 9, 9]\nStep 4 – 10 Responses:\n1. Reason: The incorrectly sorted list is missing two 0s, one 3, one 4, one 6 and one\n9. The incorrectly sorted list also contains one extra 1, one extra 2, and one extra 5.\nOutput: [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 5, 5, 5,\n6, 6, 6, 6, 7, 7, 8, 8, 9, 9] 4 Errors - Missing one 1, two 4s and one 5\nNote: The output list has the same number of elements as the input list, is sorted in ascending order, and con-\ntains the same elements as the input list.\n2. Output: [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 7, 7,\n8, 8, 9, 9] 1 Error - One extra 6\n3. Output: [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6,\n7, 7, 7, 8, 8, 8, 9, 9, 9] 6 Errors - One extra 2, 5, 6, 7, 8 and 9\n4. Reason: The incorrectly sorted list is missing two 9s and one 0.\nOutput: [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7,\n7, 8, 8, 9, 9, 9, 9] 3 Errors - Missing one 1 and has two extra 9s\n5. Reason: The incorrectly sorted list is missing one 0, one 3, one 4, four 6s, and one 9.\nOutput: [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n6, 6, 7, 7, 7, 8, 8, 8, 9, 9, 9, 9]10 Errors - Missing one 1, 2, has four extra 6s, one extra 7, 8 and two extra 9s\n6. Output: [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6,\n7, 7, 7, 7, 8, 8, 8, 9, 9, 9, 9] 8 Errors - Has one extra 2, 3, 6, two extra 7s, one extra 8 and two extra 9s\n7. Output: [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 8,\n8, 9, 9] 2 Errors - Missing one 1 and has one extra 4\n8. Reason: The incorrectly sorted list is missing three 0s, two 3s, two 4s, one 5, two 6s, and one 9.\nOutput: [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 6,\n6, 7, 7, 8, 8, 9] 5 Errors - Missing one 1, 9, has one extra 3 and two extra 6s\n9. Reason: The incorrectly sorted list is missing one 0, three 2s, two 3s, two 4s, three 5s, two 6s, one 7, and one 9.\nOutput: [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 5, 5, 5, 5, 5, 6,\n6, 6, 6, 6, 7, 8, 8, 9, 9] 8 Errors - Missing one 1, one 7, has two extra 2s, two extra 3s, one extra 5 and 6\n10. Reason: The incorrectly sorted list is missing three 1s, one 3, one 4, two 5s, one 6, one 7, one 8, and one 9.\nOutput: [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7,\n7, 8, 8, 9, 9] (Fully Correct)\nFinal Result (Correctly Sorted):\n[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4, 4, 5, 5, 5, 5, 6, 6, 6, 6, 7, 7, 8, 8, 9, 9]\n21\nC Example Prompts - Set Intersection\nWe present the prompts only for the intersection of two 32-\nelement sets, as those for 64-element and 128-element sets\nare identical, except for the split prompt where the size of\nthe split is adjusted proportionally.\nFor set intersection, we employ two distinct types of op-\nerations that interact with the LLM, each with its corre-\nsponding prompts. First, there is the Generate operation,\nutilizing the intersect prompt to guide the LLM in inter-\nsecting two input sets, and the split prompt to direct the\nLLM to split a specified set into a designated number of dis-\ntinct subsets. Second, theAggregate operation leverages the\nmerge prompt to guide the LLM in combining two sets into\none.\nFirst, we present the prompt stubs (Table 10), serving as\ntemplates to dynamically generate appropriate prompts at\nruntime. For clarity, we display their corresponding few-shot\nexamples separately in Table 11. Following this, we outline\nthe LLM interactions throughout a complete set intersection\nprocess (Table 12 - Table 15).\n22\nTable 10: Prompt stubs for the set intersection tasks; parameters in single curly brackets will be substituted at runtime.\nintersect prompt: <Instruction> Find the intersection of two sets of numbers. Output only the set of numbers that are\npresent in both sets, no additional text.</Instruction>\n<Examples> See Table 11 </Examples>\nInput Set 1: {set1}\nInput Set 2: {set2}\nsplit prompt (32 elements): <Instruction> Split the following list of 32 numbers into 2 lists of 16 numbers each, the first\nlist should contain the first 16 numbers and the second list the second 16 numbers.\nOnly output the 2 lists in the following format without any additional text or thoughts!\n{{\n\"List 1\": [13, 16, 30, 6, 21, 7, 31, ...],\n\"List 2\": [25, 24, 10, 4, 27, 0, 14, ...]\n}}\n</Instruction>\n<Examples> See Table 11 </Examples>\nInput: {input}\nmerge prompt: <Instruction> Merge the following 2 lists into one list by appending the second list to the first list.\nOnly output the final list without any additional text or thoughts! </Instruction>\nList 1: {input1}\nList 2: {input2}\n23\nTable 11: Few-shot examples for each prompt used for the set intersection tasks; some lists are truncated for brevity.\nintersect prompt:\n<Examples>\nInput Set 1: [13, 16, 30, 6, 21, 7, 31, 15, 11, 1, 24, 10, 9, 3, 20, 8]\nInput Set 2: [25, 24, 10, 4, 27, 0, 14, 12, 8, 2, 29, 20, 17, 19, 26, 23]\nOutput: [24, 10, 20, 8]\nInput Set 1: [26, 40, 42, 57, 15, 31, 5, 32, 11, 4, 24, 28, 51, 54, . . .(Omitted 18/32 numbers)]\nInput Set 2: [16, 60, 36, 48, 0, 15, 5, 19, 46, 24, 1, 6, 61, 10, . . .(Omitted 18/32 numbers)]\nOutput: [40, 15, 5, 24, 35, 59, 16, 63]\nInput Set 1: [115, 61, 35, 103, 90, 117, 86, 44, 63, 45, 40, 30, 74, 33, . . .(Omitted 50/64 numbers)]\nInput Set 2: [13, 35, 20, 96, 34, 18, 47, 127, 126, 9, 21, 16, 77, 22, . . .(Omitted 50/64 numbers)]\nOutput: [115, 35, 90, 117, 63, 40, 30, 33, 15, 89, 50, 12, 2, 84, 7, 57, 96]\n</Examples>\nsplit prompt (32 elements):\n<Examples>\nInput: [26, 40, 42, 57, 15, 31, 5, 32, 11, 4, 24, 28, 51, 54, . . .(Omitted 18/32 numbers)]\nOutput:\n{{\n\"List 1\": [26, 40, 42, 57, 15, 31, 5, 32, 11, 4, 24, 28, 51, 54, 12, 22],\n\"List 2\": [33, 35, 7, 13, 2, 59, 8, 23, 43, 16, 29, 55, 25, 63, 21, 18]\n}}\n</Examples>\nmerge prompt: -\nTable 12: Intersection of two 32-element sets: Execution plan (GoO)\nGoO:\n1. Split the second input set into two sub-sets of equal size (split prompt)\n2. For each sub-set: Intersect the sub-set with the first input set (intersect prompt) five times; score each sort attempt; keep\nthe best\n3. Merge the resulting intersections into one full intersection set (merge prompt) 10 times; score each merge attempt; keep\nthe best\n24\nTable 13: Intersection of two 32-element sets: Step 1 (Prompt/Response)\nStep 1 – Prompt:\n<Instruction> Split the following list of 32 numbers into 2 lists of 16 numbers each, the first list should contain the first 16\nnumbers and the second list the second 16 numbers.\nOnly output the 2 lists in the following format without any additional text or thoughts!\n{{\n\"List 1\": [13, 16, 30, 6, 21, 7, 31, ...],\n\"List 2\": [25, 24, 10, 4, 27, 0, 14, ...]\n}}\n</Instruction>\n<Examples> See Table 11 </Examples>\nInput: [31, 11, 4, 63, 38, 58, 59, 24, 61, 14, 32, 39, 27, 46, 48, 19, 52, 57, 50, 56, 3, 2, 53, 29,\n5, 37, 62, 41, 36, 12, 49, 16]\nStep 1 – Response (Fully Correct):\n{{\n\"List 1\": [31, 11, 4, 63, 38, 58, 59, 24, 61, 14, 32, 39, 27, 46, 48, 19],\n\"List 1\": [52, 57, 50, 56, 3, 2, 53, 29, 5, 37, 62, 41, 36, 12, 49, 16]\n}}\n25\nTable 14: Intersection of two 32-element sets: Step 2 (Prompts/Responses)\nStep 2a – Prompt:\n<Instruction> Find the intersection of two sets of numbers. Output only the set of numbers that are present in both sets, no\nadditional text. </Instruction>\n<Examples> See Table 11 </Examples>\nInput Set 1: [11, 60, 1, 49, 21, 33, 14, 56, 54, 15, 23, 40, 45, 22, 7, 28, 20, 46, 51, 6, 34, 37, 3,\n50, 17, 8, 25, 0, 35, 47, 18, 19]\nInput Set 2: [31, 11, 4, 63, 38, 58, 59, 24, 61, 14, 32, 39, 27, 46, 48, 19]\nStep 2a – 5 Responses:\n1. [11, 14, 46, 14, 19] (1 Error - Duplicated 14)\n2. Output: [11, 14, 46, 19] (Fully Correct)\n3. [11, 14, 46, 14, 19] (1 Error - Duplicated 14)\n4. [11, 14, 46, 19] (Fully Correct)\n5. [11, 14, 46, 14, 19] (1 Error - Duplicated 14)\nStep 2b – Prompt:\n<Instruction> Find the intersection of two sets of numbers. Output only the set of numbers that are present in both sets, no\nadditional text. </Instruction>\n<Examples> See Table 11 </Examples>\nInput Set 1: [11, 60, 1, 49, 21, 33, 14, 56, 54, 15, 23, 40, 45, 22, 7, 28, 20, 46, 51, 6, 34, 37, 3,\n50, 17, 8, 25, 0, 35, 47, 18, 19]\nInput Set 2: [52, 57, 50, 56, 3, 2, 53, 29, 5, 37, 62, 41, 36, 12, 49, 16]\nStep 2b – 5 Responses:\n1. [50, 56, 49] (2 Errors - Missing 3 and 37)\n2. [50, 56, 37, 49] (1 Error - Missing 3)\n3. [56, 49, 37, 3, 50] (Fully Correct)\n4. [50, 56, 37, 49] (1 Error - Missing 3)\n5. [50, 56, 37, 49] (1 Error - Missing 3)\n26\nTable 15: Intersection of two 32-element sets: Step 3 (Prompt/Responses)\nStep 3 – Prompt:\n<Instruction> Merge the following 2 lists into one list by appending the second list to the first list.\nOnly output the final list without any additional text or thoughts! </Instruction>\nList 1: [11, 14, 46, 19]\nList 2: [56, 49, 37, 3, 50]\nStep 3 – 10 Responses:\n1. [11, 14, 46, 19, 56, 49, 37, 3, 50] (Fully Correct)\n2. [11, 14, 46, 19, 56, 49, 37, 3, 50] (Fully Correct)\n3. [11, 14, 46, 19, 56, 49, 37, 3, 50] (Fully Correct)\n4. [11, 14, 46, 19, 56, 49, 37, 3, 50] (Fully Correct)\n5. [11, 14, 46, 19, 56, 49, 37, 3, 50] (Fully Correct)\n6. [11, 14, 46, 19, 56, 49, 37, 3, 50] (Fully Correct)\n7. [11, 14, 46, 19, 56, 49, 37, 3, 50] (Fully Correct)\n8. [11, 14, 46, 19, 56, 49, 37, 3, 50] (Fully Correct)\n9. [11, 14, 46, 19, 56, 49, 37, 3, 50] (Fully Correct)\n10. [11, 14, 46, 19, 56, 49, 37, 3, 50] (Fully Correct)\nFinal Result (Correct Intersection):\n[11, 14, 46, 19, 56, 49, 37, 3, 50]\n27\nD Example Prompts - Keyword Counting\nWe present the prompts only forGoT4 of the keyword count-\ning task, as those used for GoT8 and GoTx are identical, ex-\ncept for minor differences in the split prompt where the size\nof the split is adjusted.\nFor keyword counting, we employ three distinct types of\noperations that interact with the LLM, each with its corre-\nsponding prompts. First, there is the Generate operation,\nutilizing the count prompt to guide the LLM in counting the\nkeywords in a text, and the split prompt to direct the LLM\nto split a given text into a number of passages. Next, theAg-\ngregate operation leverages the merge prompt to guide the\nLLM in merging two dictionaries of counted keywords into\none. Finally, the ValidateAndImprove operation employs\nthe improve merge prompt to instruct the LLM to correct\nmistakes that were made in a previous Aggregate operation.\nWe present the prompt stubs (Table 16 - Table 17), serving\nas templates to dynamically generate appropriate prompts at\nruntime. For clarity, we display their corresponding few-shot\nexamples separately in Table 18 and Table 19. Following\nthis, we outline the LLM interactions throughout a complete\nkeyword counting process (Table 20 - Table 28).\n28\nTable 16: Prompt stubs for the keyword counting task; parameters in single curly brackets will be substituted at runtime.\ncount prompt: <Instruction> Count the frequency of how many times each country is explicitly named in the input text.\nYou can generate any intermedate lists and states, but the final output should only contain the frequency of each country that\nappears at least once in the following json format, prefixed with ”Output: ” (make sure to keep the same spelling for each\ncountry in the output as in the input text):\n{{\n\"country1\": frequency1,\n\"country2\": frequency2,\n. . .\n}}\n</Instruction>\n<Approach>\nTo count the frequency for each country follow these steps:\n1. Split the input passage into four paragraphs of similar length.\n2. Count the frequency of each country in each paragraph.\n3. Combine the frequencies of each country from each paragraph by adding them together.\n</Approach>\n<Examples> See Table 18 </Examples>\nInput: {input text}\nsplit prompt: <Instruction> Split the following input text into 4 paragraphs of approximately same length.\nOnly output the final 4 paragraphs in the following format without any additional text or thoughts:\n{{\n\"Paragraph 1\": \"Some paragraph text . . .\",\n\"Paragraph 2\": \"Some paragraph text . . .\",\n\"Paragraph 3\": \"Some paragraph text . . .\",\n\"Paragraph 4\": \"Some paragraph text . . .\"\n}}\n</Instruction>\n<Example> See Table 19 </Example>\nInput: {input text}\n29\nTable 17: Prompt stubs for the keyword counting task continued; parameters in single curly brackets will be substituted at\nruntime.\nmerge prompt: <Instruction> Combine the following 2 dictionaries, each containing the frequency of countries in a text,\ninto a single dictionary. Simply add the frequencies together for each country and if a country is not present in one of the\ndictionaries, add it to the final dictionary with the frequency from the other dictionary.\nOnly output the final merged dictionary without any additional text or thoughts! </Instruction>\n<Approach>\nTo combine the 2 dictionaries into single one, follow these steps:\n1. Create a new dictionary to store the combined frequencies.\n2. Iterate through the keys of the first dictionary and add the frequency of each country to the new dictionary.\n3. Iterate through the keys of the second dictionary and add the frequency of each country to the new dictionary and if it is\nalready present, add the frequency to the existing value.\n</Approach>\nCombine the following 2 dictionaries into a single dictionary:\n{dictionary 1}\n{dictionary 2}\nCombined Output:\nimprove merge prompt: <Instruction> The following 2 dictionaries were combined into the third dictionary below. How-\never, some mistakes occured and the third dictionary is incorrect. Please fix the third dictionary so that it contains the correct\nfrequencies for each country. The correct frequencies are the sum of the frequencies from the first 2 dictionaries. If a country\nis not present in one of the dictionaries, add it to the final dictionary with the frequency from the other dictionary.\n</Instruction>\n<Example> See Table 19 </Example>\nDictionary 1: {dictionary 1}\nDictionary 2: {dictionary 2}\nIncorrectly Combined Dictionary: {dictionary incorrect}\nOutput:\n30\nTable 18: Few-shot examples for count prompt used for the keyword counting task; some paragraphs and dictionaries are\ntruncated and formatting is slightly adjusted for brevity.\ncount prompt:\n<Examples>\nInput: Alexandra boarded the first flight of her grand journey, starting from Canada. With a globe-trotting ... (Omitted)\nParagraphs:\nAlexandra boarded the first flight of her grand journey, starting from Canada. With a globe-trotting itinerary ... (Omitted)\nHer first stop was Mexico, where she marveled at the Mayan ruins. From there, she explored the rainforests ... (Omitted)\nSublist frequencies:\n{{ \"Canada\": 1 }}\n{{ \"Mexico\": 1, \"Brazil\": 1, \"Argentina\": 1}}\nOutput: {{ \"Canada\": 1, \"Mexico\": 1, \"Brazil\": 1, \"Argentina\": 1}}\nInput: The adventure led him to the peaks of Peru where he trekked to see the mysteries of Machu Picchu ... (Omitted)\nParagraphs:\nThe adventure led him to the peaks of Peru where he trekked to see the mysteries of Machu Picchu. He then ... (Omitted)\nA quick detour to Uruguay and Paraguay allowed him to experience the vibrancy of the local cultures before ... (Omitted)\nSublists:\n{{ \"Peru\": 1, \"Chile\": 1}}\n{{ \"Uruguay\": 1, \"Paraguay\": 1, \"Canada\": 1, \"Peru\": 1, \"Brazil\": 1, \"Mexico\": 1}}\nOutput: {{ \"Peru\": 2, \"Chile\": 1, \"Uruguay\": 1, \"Paraguay\": 1, \"Canada\": 1, \"Brazil\": 1, \"Mexico\": 1}}\nInput: Journeying westward, she admired the art in Italy and sipped coffee in France. The music of ... (Omitted)\nParagraphs:\nJourneying westward, she admired the art in Italy and sipped coffee in France.\nThe music of Spain and the history of Greece deepened her love for Europe. The Nordic beauty of Norway, ... (Omitted)\nShe danced in Ireland, explored castles in Scotland, and marveled at the architecture in Germany and Russia.\nItaly, Norway, Sweden and Germany will always stay her favourite destinations to visit.\nSublists:\n{{ \"Italy\": 1, \"France\": 1}}\n{{ \"Spain\": 1, \"Greece\": 1, \"Norway\": 1, \"Sweden\": 1, \"Finland\": 1, \"Denmark\": 1}}\n{{ \"Ireland\": 1, \"Scotland\": 1, \"Germany\": 1, \"Russia\": 1}}\n{{ \"Italy\": 1, \"Norway\": 1, \"Sweden\": 1, \"Germany\": 1}}\nOutput: {{ \"Italy\": 2, \"France\": 1, \"Spain\": 1, \"Greece\": 1, \"Norway\": 2, \"Sweden\": 2,. . .(Omitted) }}\n</Examples>\n31\nTable 19: Few-shot examples for split, merge and improve merge prompts used for the keyword counting task; some para-\ngraphs and dictionaries are truncated and formatting is slightly adjusted for brevity.\nsplit prompt:\n<Examples>\nInput: Journeying westward, she admired the art in Italy and sipped coffee in France. The music of Spain and the history of\nGreece deepened her love for Europe. The Nordic beauty of Norway, Sweden, Finland, and Denmark took her breath away.\nShe danced in Ireland, explored castles in Scotland, and marveled at the architecture in Germany and Russia. Italy, Norway,\nSweden and Germany will always stay her favourite destinations to visit.\nOutput:\n{{\n\"Paragraph 1\": \"Journeying westward, she admired the art in Italy and sipped coffee in France. \",\n\"Paragraph 2\": \"The music of Spain and the history of Greece deepened her love for . . .(Omitted)”,\n\"Paragraph 3\": \"She danced in Ireland, explored castles in Scotland, and marveled . . .(Omitted)”,\n\"Paragraph 4\": \"Italy, Norway, Sweden and Germany will always stay her favourite . . .(Omitted)”\n}}\n</Examples>\nmerge prompt: -\nimprove merge prompt:\n<Example>\nDictionary 1: {{ \"Peru\": 2, \"Chile\": 1, \"Uruguay\": 1, \"Paraguay\": 1 }}\nDictionary 2: {{ \"Peru\": 1, \"Argentina\": 1, \"Canada\": 1, \"Chile\": 3, \"Germany\": 2 }}\nIncorrectly Combined Dictionary:\n{{ \"Peru\": 3, \"Chile\": 2, \"Uruguay\": 1, \"Paraguay\": 1, \"Argentina\": 1, \"Chile\": 3, \"Germany\": 2}}\nOutput:\n{{ \"Peru\": 3, \"Chile\": 4, \"Uruguay\": 1, \"Paraguay\": 1, \"Argentina\": 1, \"Canada\": 1, \"Germany\": 2}}\n</Example>\nTable 20: Keyword counting for an example 4-passage split (GoT4): Execution plan (GoO)\nGoO:\n1. Split the input text into four paragraphs of roughly equal size (split prompt)\n2. For each paragraph: Count the occurrences of individual countries (count prompt) 10 times; score each counting attempt;\nkeep the best\n3. Merge the country counts into one dictionary (merge prompt) 3 times; validate and improve invalid merge attempts\n(improve merge prompt) up to 3 attempts each; score; keep the best\n32\nTable 21: Keyword counting for an example 4-passage split (GoT4):Step 1 (Prompt/Response)\nStep 1 – Prompt:\n<Instruction> Split the following input text into 4 paragraphs of approximately same length.\nOnly output the final 4 paragraphs in the following format without any additional text or thoughts:\n{{\n\"Paragraph 1\": \"Some paragraph text . . .\",\n\"Paragraph 2\": \"Some paragraph text . . .\",\n\"Paragraph 3\": \"Some paragraph text . . .\",\n\"Paragraph 4\": \"Some paragraph text . . .\"\n}}\n</Instruction>\n<Example> See Table 19 </Example>\nInput: My friend, Alex from Peru, once recounted his journey to Argentina where he learned about the strong cultural ties\nbetween Argentina and Brazil due to their shared history. He spoke fondly of his time in Argentina and Brazil, marveling at\nthe similar music, dance, and culinary traditions that seamlessly bridged the borders of these countries. It reminded me\nof a documentary I’d watched about the ancient Silk Road that spanned across Iran and China. It touched upon the shared\nhistorical narratives of Iran and China, highlighting how goods, ideas, and culture flowed between these two countries for\ncenturies. Intriguingly, the documentary also brought up some parallels between this eastern route and the vibrant exchange\nbetween Italy and France during the Renaissance. The cultural and intellectual bonds between Italy and France were indeed\nprofound, just as the resilient trade relations that existed between the United States and Canada. The United States\nand Canada, apart from their geographical proximity, shared an economic bond that strongly influenced their policies\nand international standing. Similarly, the ties between Australia and New Zealand shed light on their gestalt identity in the\nPacific region. Despite their unique characteristics, Australia and New Zealand were often viewed as a single entity due to\ntheir remarkably similar backgrounds in terms of culture, language, and colonial history. Inspired by these chronicles of\ninterconnectedness, I decided to delve deeper into history and found a fascinating account of how Ukraine and Poland had\ninfluenced each other through years of coexistence. Despite their tumultuous past, Ukraine and Poland shared a cultural\ntapestry that was deeply woven into their histories. It was quite similar to the complex relationship between North Korea\nand South Korea, which, despite their political differences, shared common heritage and traditions. Thus, from Argentina to\nSouth Korea, the world was an intricate web of countries intertwined with each other through shared histories, cultures,and\nsometimes, shared destinies.\nStep 1 – Response (Fully Correct):\n{{\n\"Paragraph 1\": \"My friend, Alex from Peru ... (Omitted) ... bridged the borders of these countries.\",\n\"Paragraph 2\": \"It reminded me of a ... (Omitted) ... between the United States and Canada.\",\n\"Paragraph 3\": \"The United States and Canada, apart ... (Omitted) ... through years of coexistence.\",\n\"Paragraph 4\": \"Despite their tumultuous past ... (Omitted) ... and sometimes, shared destinies.\"\n}}\n33\nTable 22: Keyword counting for an example 4-passage split (GoT4):Step 2a (Prompt/Responses)\nStep 2a – Prompt:\n<Instruction> Count the frequency of how many times each country is explicitly named in the input text. You can generate\nany intermedate lists and states, but the final output should only contain the frequency of each country that appears at least\nonce in the following json format, prefixed with ”Output: ” (make sure to keep the same spelling for each country in the\noutput as in the input text):\n{{\n\"country1\": frequency1,\n\"country2\": frequency2,\n. . .\n}}\n</Instruction>\n<Approach>\nTo count the frequency for each country follow these steps:\n1. Split the input passage into four paragraphs of similar length.\n2. Count the frequency of each country in each paragraph.\n3. Combine the frequencies of each country from each paragraph by adding them together.\n</Approach>\n<Examples> See Table 18 </Examples>\nInput: My friend, Alex from Peru, once recounted his journey to Argentina where he learned about the strong cultural ties\nbetween Argentina and Brazil due to their shared history. He spoke fondly of his time in Argentina and Brazil, marveling at\nthe similar music, dance, and culinary traditions that seamlessly bridged the borders of these countries.\nStep 2a – 10 Responses:\n1. {{ \"Peru\": 1, \"Argentina\": 1, \"Brazil\": 1 }} (3 Errors - Missing two ”Argentina” and one ”Brazil”)\n2. {{ \"Peru\": 1, \"Argentina\": 2, \"Brazil\": 2 }} (1 Error - Missing one ”Argentina”)\n3. {{ \"Peru\": 1, \"Argentina\": 2, \"Brazil\": 2 }} (1 Error - Missing one ”Argentina”)\n4. {{ \"Peru\": 1, \"Argentina\": 1, \"Brazil\": 1 }} (3 Errors - Missing two ”Argentina” and one ”Brazil”)\n5. {{ \"Peru\": 1, \"Argentina\": 2, \"Brazil\": 2 }} (1 Error - Missing one ”Argentina”)\n6. {{ \"Peru\": 1, \"Argentina\": 2, \"Brazil\": 2 }} (1 Error - Missing one ”Argentina”)\n7. {{ \"Peru\": 1, \"Argentina\": 1, \"Brazil\": 1 }} (3 Errors - Missing two ”Argentina” and one ”Brazil”)\n8. {{ \"Peru\": 1, \"Argentina\": 1, \"Brazil\": 1 }} (3 Errors - Missing two ”Argentina” and one ”Brazil”)\n9. {{ \"Peru\": 1, \"Argentina\": 1, \"Brazil\": 1 }} (3 Errors - Missing two ”Argentina” and one ”Brazil”)\n10. {{ \"Peru\": 1, \"Argentina\": 1, \"Brazil\": 1 }} (3 Errors - Missing two ”Argentina” and one ”Brazil”)\n34\nTable 23: Keyword counting for an example 4-passage split (GoT4):Step 2b (Prompt/Responses)\nStep 2b – Prompt:\n<Instruction> Count the frequency of how many times each country is explicitly named in the input text. You can generate\nany intermedate lists and states, but the final output should only contain the frequency of each country that appears at least\nonce in the following json format, prefixed with ”Output: ” (make sure to keep the same spelling for each country in the\noutput as in the input text):\n{{\n\"country1\": frequency1,\n\"country2\": frequency2,\n. . .\n}}\n</Instruction>\n<Approach>\nTo count the frequency for each country follow these steps:\n1. Split the input passage into four paragraphs of similar length.\n2. Count the frequency of each country in each paragraph.\n3. Combine the frequencies of each country from each paragraph by adding them together.\n</Approach>\n<Examples> See Table 18 </Examples>\nInput: It reminded me of a documentary I’d watched about the ancient Silk Road that spanned across Iran and China. It\ntouched upon the shared historical narratives of Iran and China, highlighting how goods, ideas, and culture flowed between\nthese two countries for centuries. Intriguingly, the documentary also brought up some parallels between this eastern route\nand the vibrant exchange between Italy and France during the Renaissance. The cultural and intellectual bonds between Italy\nand France were indeed profound, just as the resilient trade relations that existed between the United States and Canada.\nStep 2b – 10 Responses:\n1. {{ \"Iran\": 1, \"China\": 1, \"Italy\": 1, \"France\": 1, \"United States\": 1, \"Canada\": 1 }}\n(4 Errors - Missing one ”Iran”, one ”China”, one ”Italy” and one ”France”)\n2. {{ \"Iran\": 1, \"China\": 1, \"Italy\": 1, \"France\": 1, \"United States\": 1, \"Canada\": 1 }}\n(4 Errors - Missing one ”Iran”, one ”China”, one ”Italy” and one ”France”)\n3. {{ \"Iran\": 3, \"China\": 2, \"Italy\": 1, \"France\": 1, \"United States\": 1, \"Canada\": 1 }}\n(3 Errors - Surplus of one ”Iran”, missing one ”Italy” and one ”France”)\n4. {{ \"Iran\": 1, \"China\": 1, \"Italy\": 1, \"France\": 1, \"United States\": 1, \"Canada\": 1 }}\n(4 Errors - Missing one ”Iran”, one ”China”, one ”Italy” and one ”France”)\n5. {{ \"Iran\": 2, \"China\": 2, \"Italy\": 1, \"France\": 1, \"United States\": 1, \"Canada\": 1 }}\n(2 Errors - Missing one ”Italy” and one ”France”)\n6. {{ \"Iran\": 1, \"China\": 1, \"Italy\": 1, \"France\": 1, \"United States\": 1, \"Canada\": 1 }}\n(4 Errors - Missing one ”Iran”, one ”China”, one ”Italy” and one ”France”)\n7. {{ \"Iran\": 1, \"China\": 1, \"Italy\": 1, \"France\": 1, \"United States\": 1, \"Canada\": 1 }}\n(4 Errors - Missing one ”Iran”, one ”China”, one ”Italy” and one ”France”)\n8. {{ \"Iran\": 1, \"China\": 1, \"Italy\": 1, \"France\": 1, \"United States\": 1, \"Canada\": 1 }}\n(4 Errors - Missing one ”Iran”, one ”China”, one ”Italy” and one ”France”)\n9. {{ \"Iran\": 1, \"China\": 1, \"Italy\": 1, \"France\": 1, \"United States\": 1, \"Canada\": 1 }}\n(4 Errors - Missing one ”Iran”, one ”China”, one ”Italy” and one ”France”)\n10. {{ \"Iran\": 2, \"China\": 2, \"Italy\": 1, \"France\": 2, \"United States\": 1, \"Canada\": 1 }}\n(1 Error - Missing one ”Italy”)\n35\nTable 24: Keyword counting for an example 4-passage split (GoT4):Step 2c (Prompt/Responses)\nStep 2c – Prompt:\n<Instruction> Count the frequency of how many times each country is explicitly named in the input text. You can generate\nany intermedate lists and states, but the final output should only contain the frequency of each country that appears at least\nonce in the following json format, prefixed with ”Output: ” (make sure to keep the same spelling for each country in the\noutput as in the input text):\n{{\n\"country1\": frequency1,\n\"country2\": frequency2,\n. . .\n}}\n</Instruction>\n<Approach>\nTo count the frequency for each country follow these steps:\n1. Split the input passage into four paragraphs of similar length.\n2. Count the frequency of each country in each paragraph.\n3. Combine the frequencies of each country from each paragraph by adding them together.\n</Approach>\n<Examples> See Table 18 </Examples>\nInput: The United States and Canada, apart from their geographical proximity, shared an economic bond that strongly\ninfluenced their policies and international standing. Similarly, the ties between Australia and New Zealand shed light on\ntheir gestalt identity in the Pacific region. Despite their unique characteristics, Australia and New Zealand were often viewed\nas a single entity due to their remarkably similar backgrounds in terms of culture, language, and colonial history. Inspired\nby these chronicles of interconnectedness, I decided to delve deeper into history and found a fascinating account of how\nUkraine and Poland had influenced each other through years of coexistence.\nStep 2c – 10 Responses:\n1. {{ \"United States\": 1, \"Canada\": 1, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 1, \"Poland\": 1}}\n(Fully Correct)\n2. {{ \"United States\": 1, \"Canada\": 1, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 1, \"Poland\": 1}}\n(Fully Correct)\n3. {{ \"United States\": 1, \"Canada\": 1, \"Australia\": 1, \"New Zealand\": 1, \"Australia and New Zealand\":\n1, \"Ukraine\": 1, \"Poland\": 1 }}\n(3 Errors - Missing one ”Australia, one ”New Zealand” and surplus of one ”Australia and New Zealand”)\n4. {{ \"United States\": 1, \"Canada\": 1, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 1, \"Poland\": 1}}\n(Fully Correct)\n5. {{ \"United States\": 1, \"Canada\": 1, \"Australia\": 1, \"New Zealand\": 1, \"Australia and New Zealand\":\n1, \"Ukraine\": 1, \"Poland\": 1 }}\n(3 Errors - Missing one ”Australia, one ”New Zealand” and surplus of one ”Australia and New Zealand”)\n6. {{ \"United States\": 1, \"Canada\": 1, \"Australia\": 1, \"New Zealand\": 1, \"Ukraine\": 1, \"Poland\": 1}}\n(2 Errors - Missing one ”Australia” and one ”New Zealand”)\n7. {{ \"United States\": 1, \"Canada\": 1, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 1, \"Poland\": 1}}\n(Fully Correct)\n8. {{ \"United States\": 1, \"Canada\": 1, \"Australia\": 1, \"New Zealand\": 1, \"Ukraine\": 1, \"Poland\": 1}}\n(2 Errors - Missing one ”Australia” and one ”New Zealand”)\n9. {{ \"United States\": 1, \"Canada\": 1, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 1, \"Poland\": 1}}\n(Fully Correct)\n10. {{ \"United States\": 1, \"Canada\": 1, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 1, \"Poland\": 1}}\n(Fully Correct)\n36\nTable 25: Keyword counting for an example 4-passage split (GoT4):Step 2d (Prompt/Responses)\nStep 2d – Prompt:\n<Instruction> Count the frequency of how many times each country is explicitly named in the input text. You can generate\nany intermedate lists and states, but the final output should only contain the frequency of each country that appears at least\nonce in the following json format, prefixed with ”Output: ” (make sure to keep the same spelling for each country in the\noutput as in the input text):\n{{\n\"country1\": frequency1,\n\"country2\": frequency2,\n. . .\n}}\n</Instruction>\n<Approach>\nTo count the frequency for each country follow these steps:\n1. Split the input passage into four paragraphs of similar length.\n2. Count the frequency of each country in each paragraph.\n3. Combine the frequencies of each country from each paragraph by adding them together.\n</Approach>\n<Examples> See Table 18 </Examples>\nInput: Despite their tumultuous past, Ukraine and Poland shared a cultural tapestry that was deeply woven into their histo-\nries. It was quite similar to the complex relationship between North Korea and South Korea, which, despite their political\ndifferences, shared common heritage and traditions. Thus, from Argentina to South Korea, the world was an intricate web of\ncountries intertwined with each other through shared histories, cultures, and sometimes, shared destinies.\nStep 2d – 10 Responses:\n1. {{ \"Ukraine\": 1, \"Poland\": 1, \"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }} (Fully Correct)\n2. {{ \"Ukraine\": 1, \"Poland\": 1, \"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }} (Fully Correct)\n3. {{ \"Ukraine\": 1, \"Poland\": 1, \"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }} (Fully Correct)\n4. {{ \"Ukraine\": 1, \"Poland\": 1, \"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }} (Fully Correct)\n5. {{ \"Ukraine\": 1, \"Poland\": 1, \"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }} (Fully Correct)\n6. {{ \"Ukraine\": 1, \"Poland\": 1, \"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }} (Fully Correct)\n7. {{ \"Ukraine\": 1, \"Poland\": 1, \"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }} (Fully Correct)\n8. {{ \"Ukraine\": 1, \"Poland\": 1, \"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }} (Fully Correct)\n9. {{ \"Ukraine\": 1, \"Poland\": 1, \"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }} (Fully Correct)\n10. {{ \"Ukraine\": 1, \"Poland\": 1, \"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }} (Fully Correct)\n37\nTable 26: Keyword counting for an example 4-passage split (GoT4):Step 3a (Prompt/Responses)\nStep 3a – Prompt:\n<Instruction> Combine the following 2 dictionaries, each containing the frequency of countries in a text, into a single\ndictionary. Simply add the frequencies together for each country and if a country is not present in one of the dictionaries,\nadd it to the final dictionary with the frequency from the other dictionary.\nOnly output the final merged dictionary without any additional text or thoughts! </Instruction>\n<Approach>\nTo combine the 2 dictionaries into single one, follow these steps:\n1. Create a new dictionary to store the combined frequencies.\n2. Iterate through the keys of the first dictionary and add the frequency of each country to the new dictionary.\n3. Iterate through the keys of the second dictionary and add the frequency of each country to the new dictionary and if it is\nalready present, add the frequency to the existing value.\n</Approach>\nCombine the following 2 dictionaries into a single dictionary:\n{{ \"Peru\": 1, \"Argentina\": 2, \"Brazil\": 2 }}\n{{ \"Iran\": 2, \"China\": 2, \"Italy\": 1, \"France\": 2, \"United States\": 1, \"Canada\": 1 }}\nCombined Output:\nStep 3a – 3 Responses:\n1. {{ \"Peru\": 1, \"Argentina\": 2, \"Brazil\": 2, \"Iran\": 2, \"China\": 2, \"Italy\": 1, \"France\": 2, \"United\nStates\": 1, \"Canada\": 1 }} (Valid - No Improvement) (2 Errors - from counting)\n2. {{ \"Peru\": 1, \"Argentina\": 2, \"Brazil\": 2, \"Iran\": 2, \"China\": 2, \"Italy\": 1, \"France\": 2, \"United\nStates\": 1, \"Canada\": 1 }} (Valid - No Improvement) (2 Errors - from counting)\n3. {{ \"Peru\": 1, \"Argentina\": 2, \"Brazil\": 2, \"Iran\": 2, \"China\": 2, \"Italy\": 1, \"France\": 2, \"United\nStates\": 1, \"Canada\": 1 }} (Valid - No Improvement) (2 Errors - from counting)\n38\nTable 27: Keyword counting for an example 4-passage split (GoT4):Step 3b (Prompt/Responses)\nStep 3b – Prompt:\n<Instruction> Combine the following 2 dictionaries, each containing the frequency of countries in a text, into a single\ndictionary. Simply add the frequencies together for each country and if a country is not present in one of the dictionaries,\nadd it to the final dictionary with the frequency from the other dictionary.\nOnly output the final merged dictionary without any additional text or thoughts! </Instruction>\n<Approach>\nTo combine the 2 dictionaries into single one, follow these steps:\n1. Create a new dictionary to store the combined frequencies.\n2. Iterate through the keys of the first dictionary and add the frequency of each country to the new dictionary.\n3. Iterate through the keys of the second dictionary and add the frequency of each country to the new dictionary and if it is\nalready present, add the frequency to the existing value.\n</Approach>\nCombine the following 2 dictionaries into a single dictionary:\n{{ \"United States\": 1, \"Canada\": 1, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 1, \"Poland\": 1 }}\n{{ \"Ukraine\": 1, \"Poland\": 1, \"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }}\nCombined Output:\nStep 3b – 3 Responses:\n1. {{ \"United States\": 1, \"Canada\": 1, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 2, \"Poland\": 2,\n\"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }} (Valid - No Improvement) (Fully Correct)\n2. {{ \"United States\": 1, \"Canada\": 1, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 2, \"Poland\": 2,\n\"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }} (Valid - No Improvement) (Fully Correct)\n3. {{ \"United States\": 1, \"Canada\": 1, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 2, \"Poland\": 2,\n\"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }} (Valid - No Improvement) (Fully Correct)\n39\nTable 28: Keyword counting for an example 4-passage split (GoT4):Step 3c (Prompt/Responses)\nStep 3c – Prompt:\n<Instruction> Combine the following 2 dictionaries, each containing the frequency of countries in a text, into a single\ndictionary. Simply add the frequencies together for each country and if a country is not present in one of the dictionaries,\nadd it to the final dictionary with the frequency from the other dictionary.\nOnly output the final merged dictionary without any additional text or thoughts! </Instruction>\n<Approach>\nTo combine the 2 dictionaries into single one, follow these steps:\n1. Create a new dictionary to store the combined frequencies.\n2. Iterate through the keys of the first dictionary and add the frequency of each country to the new dictionary.\n3. Iterate through the keys of the second dictionary and add the frequency of each country to the new dictionary and if it is\nalready present, add the frequency to the existing value.\n</Approach>\nCombine the following 2 dictionaries into a single dictionary:\n{{ \"Peru\": 1, \"Argentina\": 2, \"Brazil\": 2, \"Iran\": 2, \"China\": 2, \"Italy\": 1, \"France\": 2, \"United\nStates\": 1, \"Canada\": 1 }}\n{{ \"United States\": 1, \"Canada\": 1, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 2, \"Poland\": 2,\n\"North Korea\": 1, \"South Korea\": 2, \"Argentina\": 1 }}\nCombined Output:\nStep 3c – 3 Responses:\n1. {{ \"Peru\": 1, \"Argentina\": 3, \"Brazil\": 2, \"Iran\": 2, \"China\": 2, \"Italy\": 1, \"France\": 2, \"United\nStates\": 2, \"Canada\": 2, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 2, \"Poland\": 2, \"North\nKorea\": 1, \"South Korea\": 2 }} (Valid - No Improvement) (2 Errors - from counting)\n2. {{ \"Peru\": 1, \"Argentina\": 3, \"Brazil\": 2, \"Iran\": 2, \"China\": 2, \"Italy\": 1, \"France\": 2, \"United\nStates\": 2, \"Canada\": 2, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 2, \"Poland\": 2, \"North\nKorea\": 1, \"South Korea\": 2 }} (Valid - No Improvement) (2 Errors - from counting)\n3. {{ \"Peru\": 1, \"Argentina\": 3, \"Brazil\": 2, \"Iran\": 2, \"China\": 2, \"Italy\": 1, \"France\": 2, \"United\nStates\": 2, \"Canada\": 2, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 2, \"Poland\": 2, \"North\nKorea\": 1, \"South Korea\": 2 }} (Valid - No Improvement) (2 Errors - from counting)\nFinal Result (2 Errors):\n{{ \"Peru\": 1, \"Argentina\": 3, \"Brazil\": 2, \"Iran\": 2, \"China\": 2, \"Italy\": 1, \"France\": 2, \"United\nStates\": 2, \"Canada\": 2, \"Australia\": 2, \"New Zealand\": 2, \"Ukraine\": 2, \"Poland\": 2, \"North Korea\":\n1, \"South Korea\": 2 }}\n40\nE Example Prompts - Document Merging\nWe present the prompts only for GoT of the document merg-\ning task, as GoT2 only differs in the fact that it merges the\n4 NDAs in 2 steps rather than 1. For document merging, we\nemploy four distinct types of operations that interact with the\nLLM, each with its corresponding prompts. First, there is the\nGenerate operation, utilizing the merge prompt to instruct\nthe LLM to merge the 4 NDAs into 1. Second, the Score\noperations instructs the LLM to score a given merged NDA\nusing the score prompt. Next, the Aggregate operation em-\nploys the aggregate prompt to instruct the LLM to aggregate\nmultiple merge attempts into a single, better one. Finally, the\nImprove operation leverages the improve prompt to instruct\nthe LLM to improve a merged NDA.\nFirst, we present the prompt stubs (Table 29 - Table 30),\nserving as templates to dynamically generate appropriate\nprompts at runtime. Following this, we outline the LLM in-\nteractions throughout a complete merging process (Table 31\n- Table 49). However, instead of displaying each input/gen-\nerated NDA in every prompt/response, we present the 4 in-\nput NDAs in Table 31 - Table 33 and the final merged NDA\nin Table 49. Furthermore, as scoring is done using the LLM\nas well, we will present these interactions for the best per-\nforming merged NDAs (Tables 39 - 40 and Tables 47 - 48).\nLastly, most responses are limited to a few lines only, as\nthey don’t offer any further insights and would otherwise\nspan multiple pages. However, we refer the interested reader\nto the results in the corresponding code repository 2 for full\nlogs and further examples.\n2https://github.com/spcl/graph-of-thoughts\n41\nTable 29: Prompt stubs for the document merging task; parameters in single curly brackets will be substituted at runtime.\nmerge prompt: Merge the following 4 NDA documents <Doc1> - <Doc4> into a single NDA, maximizing retained\ninformation and minimizing redundancy. Output only the created NDA between the tags <Merged> and </Merged>,\nwithout any additional text.\nHere are NDAs <Doc1> - <Doc4>:\n<Doc1> {doc1} </Doc1>\n<Doc2> {doc2} </Doc2>\n<Doc3> {doc3} </Doc3>\n<Doc4> {doc4} </Doc4>\nscore prompt: The following NDA <S> merges NDAs <Doc1> - <Doc4>.\nPlease score the merged NDA <S> in terms of how much redundant information is contained, independent of the original\nNDAs, as well as how much information is retained from the original NDAs.\nA score of 10 for redundancy implies that absolutely no information is redundant, while a score of 0 implies that at least half\nof the information is redundant (so everything is at least mentioned twice).\nA score of 10 for retained information implies that all information from the original NDAs is retained, while a score of 0\nimplies that no information is retained.\nYou may provide reasoning for your scoring, but the final score for redundancy should be between the tags<Redundancy>\nand </Redundancy>, and the final score for retained information should be between the tags<Retained> and </Retained>,\nwithout any additional text within any of those tags.\nHere are NDAs <Doc1> - <Doc4>:\n<Doc1> {doc1} </Doc1>\n<Doc2> {doc2} </Doc2>\n<Doc3> {doc3} </Doc3>\n<Doc4> {doc4} </Doc4>\nHere is the merged NDA <S>:\n<S> {s} </S>\naggregate prompt: The following NDAs <S1> - <S{num ndas summaries}> each merge the initial NDAs <Doc1> -\n<Doc4>.\nCombine the merged NDAs <S1> - <S{num ndas summaries}> into a new one, maximizing their advantages and overall\ninformation retention, while minimizing redundancy.\nOutput only the new NDA between the tags <Merged> and </Merged>, without any additional text.\nHere are the original NDAs <Doc1> - <Doc4>:\n<Doc1> {doc1} </Doc1>\n<Doc2> {doc2} </Doc2>\n<Doc3> {doc3} </Doc3>\n<Doc4> {doc4} </Doc4>\nHere are the merged NDAs <S1> - <S{num ndas summaries}>:\n<S1> {s1} </S1>\n. . .\n<S{num ndas summaries}> {s{num ndas summaries}} </S{num ndas summaries}>\n42\nTable 30: Prompt stubs for the document merging task continued; parameters in single curly brackets will be substituted at\nruntime.\nimprove prompt: The following NDA <S> merges initial NDAs <Doc1> - <Doc4>.\nPlease improve the merged NDA <S> by adding more information and removing redundancy. Output only the improved\nNDA, placed between the tags <Merged> and </Merged>, without any additional text.\nHere are NDAs <Doc1> - <Doc4>:\n<Doc1> {doc1} </Doc1>\n<Doc2> {doc2} </Doc2>\n<Doc3> {doc3} </Doc3>\n<Doc4> {doc4} </Doc4>\nHere is the merged NDA <S>:\n<S> {s} </S>\n43\nTable 31: Input NDA 1 and 2\n<Doc1>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n3. ”Confidential Information” includes all potentially commercially valuable information, specifically software development\ntactics, processes, and in-house research results.\n4. Receiving party is obligated to protect the Confidential Information, use it solely for the disclosed purpose, and not\ndisclose it without consent.\n5. Breach penalties include injunctive relief, other remedies, and a $200,000 fee per breach.\n6. The Agreement applies to the Parties and their successors and assigns. It contains all related agreements and lack of\nenforcement doesn’t imply waiver.\n7. The Agreement is under the laws of [State].\n8. Signed by [Your Company Name] and [Recipient Name] at the above date.\n</Doc1>\n<Doc2>\nNON-DISCLOSURE AGREEMENT (NDA)\nEffective from [Effective Date], this NDA involves [Your Company Name] (”Disclosing Party”), and [Recipient Name]\n(”Receiving Party”).\n1. Purpose: The Disclosing Party will disclose confidential information related to [Topic of Research] to the Receiving Party\nfor [Purpose].\n2. Confidential Information: Defined as all non-public reports, data, designs, and other materials provided by the Disclosing\nParty to the Receiving Party.\n3. Receiving Party’s Obligations:\na. Use, reproduce, or distribute the confidential information only for the agreed purpose.\nb. Restrict access to the information to necessary parties, ensuring they abide by strict confidentiality.\nc. Return or destroy all confidential information upon request or at the end of the agreement.\n4. Exclusions: Information will not be classified as confidential if it is already known to the Receiving Party, publicly known,\nor independently developed by the Receiving Party.\n5. Non-Competition: The Receiving Party will not engage in any competing business against the Disclosing Party during\nthe agreement and one year after its termination.\n6. Term and Termination: The agreement is valid for [e.g., ”two years”], unless terminated earlier with [e.g., ”30 days”]\nwritten notice. The Receiving Party’s non-disclosure and non-competition obligations persist post-termination.\n7. General Provisions:\na. Governing Law: [Your State]’s laws apply.\nb. Amendments: Only valid if written and signed by both parties.\nc. Entire Agreement: This contract overrules previous related agreements.\nSigned as of the Effective Date by [Your Company Name] - Disclosing Party [Recipient Name] - Receiving Party.\n</Doc2>\n44\nTable 32: Input NDA 3\n<Doc3>\nCONFIDENTIALITY & NON-DISCLOSURE AGREEMENT\nEntities Involved:\nEffective [Date], between [AquaBlue Innovations], established in [State], and [PineTree Solutions], a registered entity.\nObjective:\nTo safeguard classified data during talks of a potential technological alliance.\nSpecification of Protected Information:\nParticularly:\na. System designs and architectural schematics.\nb. Proprietary computational algorithms.\nReceiver’s Obligations:\na. Maintain strict non-disclosure using best practices.\nb. Employ solely for the aforementioned aim.\nc. No unveiling without explicit authorization.\nViolation Ramifications:\nA charge of $280,000 for every infringement, plus possible legal proceedings.\nGeneral Terms:\nBinding for both parties and any successors. This encapsulates the entire accord.\nLegal Reference:\nGoverned as per [State]’s legal framework.\nAttestation:\nDuly signed on [Date].\n[AquaBlue Innovations] [PineTree Solutions]\n</Doc3>\n45\nTable 33: Input NDA 4\n<Doc4>\nSECRECY & DISCLOSURE AGREEMENT\nContracting Parties:\nDated [Date], drawn between [AquaBlue Innovations], a [State]-based corporation, and [PineTree Solutions], a licensed\norganization.\nAim:\nTo protect exclusive insights amidst dialogues for a technological partnership.\nCategorization of Sensitive Data:\nIncludes:\na. Internal software blueprints.\nb. Intellectual property awaiting patents.\nCommitments of Recipient:\na. Uphold confidentiality, ensuring data integrity.\nb. Utilize strictly for collaborative ventures.\nc. No exposure without prior consensus.\nRepercussions for Non-Compliance:\n$295,000 fine for each transgression, and the option for legal recourse.\nOverall Provisions:\nLegally enforceable for signatories and successors. Complete and sole agreement.\nJuridical Standpoint:\nUnder the auspices of [State] laws.\nRatification:\nConfirmed and endorsed on [Date].\n[AquaBlue Innovations] [PineTree Solutions]\n</Doc4>\nTable 34: Merging 4 example NDAs: Execution plan (GoO)\nGoO:\n1. Merge the 4 NDAs into a single one 5 times; Score each attempt and keep the best 3\n2. Aggregate the merge attempts into a single one 5 times; Score each aggregation attempt and keep the overall best attempt\n(including Step 1)\n3. Improve the merged NDA 10 times; Score each and keep the best\n46\nTable 35: Merging 4 example NDAs:Step 1 (Prompt)\nMerge the following 4 NDA documents <Doc1> - <Doc4> into a single NDA, maximizing retained information and\nminimizing redundancy. Output only the created NDA between the tags<Merged> and </Merged>, without any additional\ntext.\nHere are NDAs <Doc1> - <Doc4>:\n<Doc1>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted, see Table 31)\n</Doc1>\n<Doc2>\nNON-DISCLOSURE AGREEMENT (NDA)\nEffective from [Effective Date], this NDA involves [Your Company Name] (”Disclosing Party”), and [Recipient Name]\n(”Receiving Party”).\n1. Purpose: The Disclosing Party will disclose confidential information related to [Topic of Research] to the Receiving Party\nfor [Purpose].\n. . .(Omitted, see Table 31)\n</Doc2>\n<Doc3>\nCONFIDENTIALITY & NON-DISCLOSURE AGREEMENT\nEntities Involved:\nEffective [Date], between [AquaBlue Innovations], established in [State], and [PineTree Solutions], a registered entity.\nObjective:\nTo safeguard classified data during talks of a potential . . .(Omitted, see Table 32)\n</Doc3>\n<Doc4>\nSECRECY & DISCLOSURE AGREEMENT\nContracting Parties:\nDated [Date], drawn between [AquaBlue Innovations], a [State]-based corporation, and [PineTree Solutions], a licensed\norganization.\nAim:\nTo protect exclusive insights amidst. . .(Omitted, see Table 33)\n</Doc4>\n47\nTable 36: Merging 4 example NDAs; we only present the highest scoring response fully in Tables 37-38:Step 1 (Responses)\nNote that due to the shortening of responses, the appear to be equal, but small differences can be found when observing\nthe fully generated summaries. The only small differences are reflected in the very similar scores (see Table 39-40 for the\nscoring of the best response).\nResponse (1/5):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted)\nScore: 6.60\nResponse (2/5):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted, see Tables 37 - 38)\nScore: 6.87\nResponse (3/5):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted)\nScore: 6.60\nResponse (4/5):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted)\nScore: 5.78\nResponse (5/5):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted)\nScore: 6.50\n48\nTable 37: Merging 4 example NDAs:Step 1 (Highest Scoring Response Part 1)\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n3. ”Confidential Information” includes all potentially commercially valuable information, specifically software development\ntactics, processes, and in-house research results.\n4. Receiving party is obligated to protect the Confidential Information, use it solely for the disclosed purpose, and not\ndisclose it without consent.\n5. Breach penalties include injunctive relief, other remedies, and a $200,000 fee per breach.\n6. The Agreement applies to the Parties and their successors and assigns. It contains all related agreements and lack of\nenforcement doesn’t imply waiver.\n7. The Agreement is under the laws of [State].\n8. Signed by [Your Company Name] and [Recipient Name] at the above date.\nEffective from [Effective Date], this NDA involves [Your Company Name] (”Disclosing Party”), and [Recipient Name]\n(”Receiving Party”).\nPurpose: The Disclosing Party will disclose confidential information related to [Topic of Research] to the Receiving Party\nfor [Purpose].\nConfidential Information: Defined as all non-public reports, data, designs, and other materials provided by the Disclosing\nParty to the Receiving Party.\nReceiving Party’s Obligations:\na. Use, reproduce, or distribute the confidential information only for the agreed purpose.\nb. Restrict access to the information to necessary parties, ensuring they abide by strict confidentiality.\nc. Return or destroy all confidential information upon request or at the end of the agreement.\nExclusions: Information will not be classified as confidential if it is already known to the Receiving Party, publicly known,\nor independently developed by the Receiving Party.\nNon-Competition: The Receiving Party will not engage in any competing business against the Disclosing Party during the\nagreement and one year after its termination.\nTerm and Termination: The agreement is valid for [e.g., ”two years”], unless terminated earlier with [e.g., ”30 days”] written\nnotice. The Receiving Party’s non-disclosure and non-competition obligations persist post-termination.\nGeneral Provisions:\na. Governing Law: [Your State]’s laws apply.\nb. Amendments: Only valid if written and signed by both parties.\nc. Entire Agreement: This contract overrules previous related agreements.\nSigned as of the Effective Date by [Your Company Name] - Disclosing Party [Recipient Name] - Receiving Party.\nEntities Involved:\nEffective [Date], between [AquaBlue Innovations], established in [State], and [PineTree Solutions], a registered entity.\nObjective:\nTo safeguard classified data during talks of a potential technological alliance.\n49\nTable 38: Merging 4 example NDAs:Step 1 (Highest Scoring Response Part 2)\nSpecification of Protected Information:\nParticularly:\na. System designs and architectural schematics.\nb. Proprietary computational algorithms.\nReceiver’s Obligations:\na. Maintain strict non-disclosure using best practices.\nb. Employ solely for the aforementioned aim.\nc. No unveiling without explicit authorization.\nViolation Ramifications:\nA charge of $280,000 for every infringement, plus possible legal proceedings.\nGeneral Terms:\nBinding for both parties and any successors. This encapsulates the entire accord.\nLegal Reference:\nGoverned as per [State]’s legal framework.\nAttestation:\nDuly signed on [Date].\nContracting Parties:\nDated [Date], drawn between [AquaBlue Innovations], a [State]-based corporation, and [PineTree Solutions], a licensed\norganization.\nAim:\nTo protect exclusive insights amidst dialogues for a technological partnership.\nCategorization of Sensitive Data:\nIncludes:\na. Internal software blueprints.\nb. Intellectual property awaiting patents.\nCommitments of Recipient:\na. Uphold confidentiality, ensuring data integrity.\nb. Utilize strictly for collaborative ventures.\nc. No exposure without prior consensus.\nRepercussions for Non-Compliance:\n$295,000 fine for each transgression, and the option for legal recourse.\nOverall Provisions:\nLegally enforceable for signatories and successors. Complete and sole agreement.\nJuridical Standpoint:\nUnder the auspices of [State] laws.\nRatification:\nConfirmed and endorsed on [Date].\n50\nTable 39: Merging 4 example NDAs:Step 1 (Scoring Example – Prompt)\nPlease score the merged NDA <S> in terms of how much redundant information is contained, independent of the original\nNDAs, as well as how much information is retained from the original NDAs.\nA score of 10 for redundancy implies that absolutely no information is redundant, while a score of 0 implies that at least half\nof the information is redundant (so everything is at least mentioned twice).\nA score of 10 for retained information implies that all information from the original NDAs is retained, while a score of 0\nimplies that no information is retained.\nYou may provide reasoning for your scoring, but the final score for redundancy should be between the tags<Redundancy>\nand </Redundancy>, and the final score for retained information should be between the tags<Retained> and </Retained>,\nwithout any additional text within any of those tags.\nHere are NDAs <Doc1> - <Doc4>:\n<Doc1>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n. . .(Omitted, see Table 31)\n</Doc1>\n<Doc2>\nNON-DISCLOSURE AGREEMENT (NDA)\nEffective from [Effective Date], this NDA involves [Your Company Name] (”Disclosing Party”), and [Recipient Name]\n(”Receiving Party”).\n. . .(Omitted, see Table 31)\n</Doc2>\n<Doc3>\nCONFIDENTIALITY & NON-DISCLOSURE AGREEMENT\nEntities Involved:\nEffective [Date], between [AquaBlue Innovations], established in [State], and . . .(Omitted, see Table 32)\n</Doc3>\n<Doc4>\nSECRECY & DISCLOSURE AGREEMENT\nContracting Parties:\nDated [Date], drawn between [AquaBlue Innovations], a [State]-based corporation, and . . .(Omitted, see Table 33)\n</Doc4>\nHere is the summary NDA <S>:\n<S>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted, see Tables 37-38)\n</S>\n51\nTable 40: Merging 4 example NDAs:Step 1 (Scoring Example – Responses)\nResponse (1/3):\n<Redundancy>5</Redundancy>\n<Retained>10</Retained>\nReasoning:\nIn terms of redundancy, there is some overlap in the information contained in the NDAs, such as the definition of ”Confiden-\ntial Information” and the obligations of the receiving party. However, there are also some unique details in each NDA, such\nas the specific purpose of information sharing and the penalties for breach. Overall, there is a moderate level of redundancy\nin the merged NDA.\nIn terms of retained information, all the key elements from the original NDAs are included in the merged NDA. This\nincludes the agreement between the parties, the purpose of information sharing, the definition of confidential information,\nthe obligations of the receiving party, the exclusions for confidential information, non-competition clauses, the term and\ntermination of the agreement, and the general provisions. Therefore, the merged NDA retains all the important information\nfrom the original NDAs.\nResponse (2/3):\n<Redundancy>8</Redundancy>\n<Retained>10</Retained>\nReasoning:\nThe merged NDA<S> contains multiple redundant sections from the original NDAs. For example, the introductory sections,\nsuch as the agreement between the parties and the purpose of information sharing, are repeated multiple times. Additionally,\nsections related to the definition of confidential information, the recipient’s obligations, exclusions, non-competition, term\nand termination, and general provisions are also redundantly included.\nHowever, in terms of retained information, the merged NDA retains all the key elements from the original NDAs, including\nthe parties involved, the objective, specification of protected information, recipient’s obligations, violation ramifications,\ngeneral terms, legal reference, and attestation. Overall, while there is redundancy in the merged NDA, it successfully retains\nall the necessary information from the original NDAs.\nResponse (3/3):\n<Redundancy>3</Redundancy>\nThe merged NDA contains some redundancy. There are three sections that have overlapping information between the original\nNDAs. These sections include:\n1. The agreement between the parties (Section 1 in Doc1 and the beginning of Doc2)\n2. Information sharing and the purpose of the NDA (Section 2 in both Doc1 and Doc2)\n3. General provisions such as governing law and amendments (Section 7 in Doc1 and Doc2)\n<Retained>9</Retained>\nThe merged NDA retains a significant amount of information from the original NDAs. It includes all key clauses and provi-\nsions, such as the definition of confidential information, the obligations of the receiving party, exclusions, non-competition,\nand general provisions. Only a few minor details, such as specific penalties and fine amounts, are not retained in the merged\nNDA. Overall, the merged NDA effectively combines the essential elements from the original NDAs.\nNote: Remember to remove the reasoning from between the tags when providing the final scores.\nFinal Overall Score (Harmonic Mean of Averages):\n6.87\n52\nTable 41: Merging 4 example NDAs; Note that S1-S3 are the three best scoring merge attemps from the previous step: Step 2\n(Prompt)\nThe following NDAs <S1> - <S3> each merge the initial NDAs <Doc1> - <Doc4>.\nCombine the merged NDAs <S1> - <S3> into a new one, maximizing their advantages and overall information retention,\nwhile minimizing redundancy.\nOutput only the new NDA between the tags <Merged> and </Merged>, without any additional text.\nHere are the original NDAs <Doc1> - <Doc4>:\n<Doc1>\n. . .(Omitted, see Table 31)\n</Doc1>\n<Doc2>\n. . .(Omitted, see Table 31)\n</Doc2>\n<Doc3>\n. . .(Omitted, see Table 32)\n</Doc3>\n<Doc4>\n. . .(Omitted, see Table 33)\n</Doc4>\nHere are the merged NDAs <S1> - <S3>:\n<S1>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n. . .(Omitted)\n</S1>\n<S2>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n. . .(Omitted, see Tables 37-38)\n</S2>\n<S3>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n. . .(Omitted)\n</S3>\n53\nTable 42: Merging 4 example NDAs:Step 2 (Responses)\nFor this run, none of the responses improve upon the previous best and thus will all be discarded by the next step. For that\nreason and due to their long length, we do not present them fully and exclude their scoring.\nResponse (1/5):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted)\nScore: 6.50\nResponse (2/5):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\nEffective from [Effective Date], this NDA involves [Your Company Name] (”Disclosing Party”), and [Recipient Name]\n(”Receiving Party”).\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n. . .(Omitted)\nScore: 5.62\nResponse (3/5):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted)\nScore: 6.60\nResponse (4/5):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted)\nScore: 6.15\nResponse (5/5):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted)\nScore: 6.22\n54\nTable 43: Merging 4 example NDAs:Step 3 (Prompt)\nThe following NDA <S> merges initial NDAs <Doc1> - <Doc4>.\nPlease improve the merged NDA <S> by adding more information and removing redundancy. Output only the improved\nNDA, placed between the tags <Merged> and </Merged>, without any additional text.\nHere are NDAs <Doc1> - <Doc4>:\n<Doc1>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted, see Table 31)\n</Doc1>\n<Doc2>\nNON-DISCLOSURE AGREEMENT (NDA)\nEffective from [Effective Date], this NDA involves [Your Company Name] (”Disclosing Party”), and [Recipient Name]\n(”Receiving Party”).\n1. Purpose: The Disclosing Party will disclose confidential information related to [Topic of Research] to the Receiving Party\nfor [Purpose].\n. . .(Omitted, see Table 31)\n</Doc2>\n<Doc3>\nCONFIDENTIALITY & NON-DISCLOSURE AGREEMENT\nEntities Involved:\nEffective [Date], between [AquaBlue Innovations], established in [State], and [PineTree Solutions], a registered entity.\nObjective:\nTo safeguard classified data during talks of a potential . . .(Omitted, see Table 32)\n</Doc3>\n<Doc4>\nSECRECY & DISCLOSURE AGREEMENT\nContracting Parties:\nDated [Date], drawn between [AquaBlue Innovations], a [State]-based corporation, and [PineTree Solutions], a licensed\norganization.\nAim:\nTo protect exclusive insights amidst. . .(Omitted, see Table 33)\n</Doc4>\nHere is the summary NDA <S>:\n<S>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted, see Tables 37-38)\n</S>\n55\nTable 44: Merging 4 example NDAs; we only present the highest scoring response fully in Table 46: Step 3 (Responses Part\n1)\nResponse (1/10):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted)\nScore: 6.35\nResponse (2/10):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted)\nScore: 6.35\nResponse (3/10):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date], for the purpose of [specific project or\npurpose].\n2. ”Confidential Information” includes all potentially commercially valuable information, specifically software develop-\nment tactics, processes, in-house research results, system designs, architectural schematics, proprietary computational\nalgorithms, internal software blueprints, and intellectual property awaiting patents.\n. . .(Omitted)\nScore: 6.87\nResponse (4/10):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date] for the purpose of [specific project or\npurpose].\n2. ”Confidential Information” includes all potentially commercially valuable information, such as software development\ntactics, processes, in-house research results, system designs, architectural schematics, proprietary computational algo-\nrithms, internal software blueprints, and intellectual property awaiting patents.\n. . .(Omitted, see Table 46))\nScore: 7.78\nResponse (5/10):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted)\nScore: 6.43\n56\nTable 45: Merging 4 example NDAs; we only present the highest scoring response fully in Table 46: Step 3 (Responses Part\n2)\nResponse (6/10):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose].\n. . .(Omitted)\nScore: 6.35\nResponse (7/10):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date] for the purpose of [specific project or\npurpose].\n2. ”Confidential Information” includes all potentially commercially valuable information, specifically software develop-\nment tactics, processes, in-house research results, system designs, architectural schematics, proprietary computational\nalgorithms, internal software blueprints, and intellectual property awaiting patents.\n. . .(Omitted)\nScore: 6.70\nResponse (8/10):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date] for the purpose of [specific project or\npurpose].\n2. ”Confidential Information” includes potentially commercially valuable information, such as software development tac-\ntics, processes, in-house research results, non-public reports, data, designs, and other materials provided by the Disclosing\nParty.\n. . .(Omitted)\nScore: 5.33\nResponse (9/10):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n2. Information sharing for the purpose of [specific project or purpose], including [Topic of Research].\n. . .(Omitted)\nScore: 7.30\nResponse (10/10):\n<Merged>\nNON-DISCLOSURE AGREEMENT (NDA)\nAgreement between [Your Company Name] and [Recipient Name] on [Date].\nInformation sharing for the purpose of [specific project or purpose].\n. . .(Omitted)\nScore: 6.40\n57\nTable 46: Merging 4 example NDAs:Step 3 (Highest Scoring Response)\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date] for the purpose of [specific project or\npurpose].\n2. ”Confidential Information” includes all potentially commercially valuable information, such as software development\ntactics, processes, in-house research results, system designs, architectural schematics, proprietary computational algo-\nrithms, internal software blueprints, and intellectual property awaiting patents.\n3. Receiving party is obligated to protect the Confidential Information, use it solely for the disclosed purpose, and not\ndisclose it without consent.\n4. Breach penalties include injunctive relief, other remedies, and a fine per breach (e.g., $200,000 or $280,000 for each\ninfringement, depending on the agreement).\n5. The Agreement applies to the Parties and their successors and assigns. It contains all related agreements and lack of\nenforcement doesn’t imply waiver.\n6. The Agreement is under the laws of [State] and is valid for a specified period (e.g., two years) unless terminated earlier\nwith written notice (e.g., 30 days).\n7. The Receiving Party agrees not to engage in any competing business against the Disclosing Party during the agreement\nand a specified period after its termination (e.g., one year).\n8. The Agreement can only be amended if written and signed by both parties, and overrules previous related agreements.\n9. The Agreement is legally enforceable and binding for signatories and successors.\n10. The Agreement is governed by the laws of [State].\nSigned as of the Effective Date by [Your Company Name] - Disclosing Party [Recipient Name] - Receiving Party, along\nwith the other parties involved.\n58\nTable 47: Merging 4 example NDAs:Step 3 (Scoring Example – Prompt)\nPlease score the merged NDA <S> in terms of how much redundant information is contained, independent of the original\nNDAs, as well as how much information is retained from the original NDAs.\nA score of 10 for redundancy implies that absolutely no information is redundant, while a score of 0 implies that at least half\nof the information is redundant (so everything is at least mentioned twice).\nA score of 10 for retained information implies that all information from the original NDAs is retained, while a score of 0\nimplies that no information is retained.\nYou may provide reasoning for your scoring, but the final score for redundancy should be between the tags<Redundancy>\nand </Redundancy>, and the final score for retained information should be between the tags<Retained> and </Retained>,\nwithout any additional text within any of those tags.\nHere are NDAs <Doc1> - <Doc4>:\n<Doc1>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date].\n. . .(Omitted, see Table 31)\n</Doc1>\n<Doc2>\nNON-DISCLOSURE AGREEMENT (NDA)\nEffective from [Effective Date], this NDA involves [Your Company Name] (”Disclosing Party”), and [Recipient Name]\n(”Receiving Party”).\n. . .(Omitted, see Table 31)\n</Doc2>\n<Doc3>\nCONFIDENTIALITY & NON-DISCLOSURE AGREEMENT\nEntities Involved:\nEffective [Date], between [AquaBlue Innovations], established in [State], and . . .(Omitted, see Table 32)\n</Doc3>\n<Doc4>\nSECRECY & DISCLOSURE AGREEMENT\nContracting Parties:\nDated [Date], drawn between [AquaBlue Innovations], a [State]-based corporation, and . . .(Omitted, see Table 33)\n</Doc4>\nHere is the summary NDA <S>:\n<S>\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date] for the purpose of [specific project or\npurpose].\n2. ”Confidential Information” includes all potentially commercially valuable information, such as software development\ntactics, processes, in-house research results, system designs, architectural schematics, proprietary computational algo-\nrithms, internal software blueprints, and intellectual property awaiting patents.\n. . .(Omitted, see Table 46)\n</S>\n59\nTable 48: Merging 4 example NDAs:Step 3 (Scoring Example – Responses)\nResponse (1/3):\n<Redundancy>5</Redundancy>\n<Retained>8</Retained>\nReasoning:\nThe merged NDA <S> retains most of the information from the original NDAs, combining relevant clauses and terms\ninto a single comprehensive agreement. However, some redundancy is present as similar information is mentioned multiple\ntimes, such as the definition of ”Confidential Information” and the obligations of the receiving party. Additionally, some\nspecific details from the original NDAs are not included in the merged NDA, such as the exclusion criteria in Doc2 and the\nprovisions for non-competition in Doc3 and Doc4. Nonetheless, the merged NDA captures the essence of the original NDAs\nand includes all key information necessary for a non-disclosure agreement.\nResponse (2/3):\n<Redundancy>8</Redundancy>\n<Retained>10</Retained>\nReasoning:\nThe merged NDA <S> retains all the key information from the original NDAs, including the purpose of the agreement,\nthe definition of confidential information, the obligations of the receiving party, breach penalties, applicability to parties and\nsuccessors, governing law, and signature requirements.\nThere is some redundant information, such as the inclusion of system designs, architectural schematics, proprietary com-\nputational algorithms, internal software blueprints, and intellectual property awaiting patents from both Doc3 and Doc4.\nHowever, this redundancy is minor and does not significantly impact the overall usefulness of the NDA.\nOverall, the merged NDA <S> retains all important information while minimizing redundancy, making it highly effective\nin conveying the necessary terms and obligations of the agreement. Therefore, it receives a score of 8 for redundancy and a\nscore of 10 for retained information.\nResponse (3/3):\n<Redundancy>7</Redundancy>\n<Retained>10</Retained>\nReasoning:\nThe merged NDA <S> contains some redundant information, such as the definition of ”Confidential Information” and the\nobligations of the receiving party. However, it retains all the important information from the original NDAs, including the\npurpose of the agreement, the specific types of confidential information, the breach penalties, the applicability to parties and\nsuccessors, the governing law, and the provisions regarding non-competition, amendment, and termination. Additionally, it\nincorporates the additional information about the agreement’s validity period and the option for legal recourse in case of\nnon-compliance. Therefore, the merged NDA <S> scores a 7 for redundancy and a 10 for retained information.\nFinal Overall Score (Harmonic Mean of Averages):\n7.78\n60\nTable 49: Merging 4 example NDAs:Final Merged NDA\nNON-DISCLOSURE AGREEMENT (NDA)\n1. Agreement between [Your Company Name] and [Recipient Name] on [Date] for the purpose of [specific project or\npurpose].\n2. ”Confidential Information” includes all potentially commercially valuable information, such as software development\ntactics, processes, in-house research results, system designs, architectural schematics, proprietary computational algo-\nrithms, internal software blueprints, and intellectual property awaiting patents.\n3. Receiving party is obligated to protect the Confidential Information, use it solely for the disclosed purpose, and not\ndisclose it without consent.\n4. Breach penalties include injunctive relief, other remedies, and a fine per breach (e.g., $200,000 or $280,000 for each\ninfringement, depending on the agreement).\n5. The Agreement applies to the Parties and their successors and assigns. It contains all related agreements and lack of\nenforcement doesn’t imply waiver.\n6. The Agreement is under the laws of [State] and is valid for a specified period (e.g., two years) unless terminated earlier\nwith written notice (e.g., 30 days).\n7. The Receiving Party agrees not to engage in any competing business against the Disclosing Party during the agreement\nand a specified period after its termination (e.g., one year).\n8. The Agreement can only be amended if written and signed by both parties, and overrules previous related agreements.\n9. The Agreement is legally enforceable and binding for signatories and successors.\n10. The Agreement is governed by the laws of [State].\nSigned as of the Effective Date by [Your Company Name] - Disclosing Party [Recipient Name] - Receiving Party, along\nwith the other parties involved.\n61\nF Evaluation - GoT Configurations\nWe detail the concrete operations that GoT was configured\nwith to solve the set intersection and sorting use cases.\nListing 1: GoT configuration for the set intersection use case\nwith 32 elements\n1 Generate (k =1) # Split second set into two halves of 16 elements\n2 foreach subset :\n3 Generate (k =5) # Determine intersected subset of subset and\nfirst input set\n4 Score (k =1) # Score locally the intersected subsets\n5 KeepBestN (1) # Keep the best intersected subset\n6 Aggregate (10) # Merge both intersected subsets\n7 Score (k =1) # Score locally the intersected result sets\n8 KeepBestN (1) # Keep the best result\n9 GroundTruth () # Compare to precomputed result\nListing 2: GoT configuration for the set intersection use case\nwith 64 elements\n1 Generate (k =1) # Split second set into four parts of 16 elements\n2 foreach subset :\n3 Generate (k =5) # Determine intersected subset of subset and\nfirst input set\n4 Score (k =1) # Score locally the intersected subsets\n5 KeepBestN (1) # Keep the best intersected subset\n6 merge step 1:\n7 Aggregate (10) # Merge intersected subsets 1 and 2\n8 Score (k =1) # Score locally the intersected result sets\n9 KeepBestN (1) # Keep the best result\n10 merge step 2:\n11 Aggregate (10) # Merge intersected subsets 3 and 4\n12 Score (k =1) # Score locally the intersected result sets\n13 KeepBestN (1) # Keep the best result\n14 final merge :\n15 Aggregate (10) # Merge intermediate intersected subsets from\nmerge step 1 and 2\n16 Score (k =1) # Score locally the intersected result sets\n17 KeepBestN (1) # Keep the best result\n18 GroundTruth () # Compare to precomputed result\nListing 3: GoT configuration for the set intersection use case\nwith 128 elements\n1 Generate (k =1) # Split second set into eight parts of 16\nelements\n2 foreach subset :\n3 Generate (k =5) # Determine intersected subset of subset and\nfirst input set\n4 Score (k =1) # Score locally the intersected subsets\n5 KeepBestN (1) # Keep the best intersected subset\n6 merge step 1:\n7 Aggregate (5) # Merge intersected subsets 1 and 2\n8 Score (k =1) # Score locally the intersected result sets\n9 KeepBestN (1) # Keep the best result\n10 merge step 2:\n11 Aggregate (5) # Merge intersected subsets 3 and 4\n12 Score (k =1) # Score locally the intersected result sets\n13 KeepBestN (1) # Keep the best result\n14 merge step 3:\n15 Aggregate (5) # Merge intersected subsets 5 and 6\n16 Score (k =1) # Score locally the intersected result sets\n17 KeepBestN (1) # Keep the best result\n18 merge step 4:\n19 Aggregate (5) # Merge intersected subsets 7 and 8\n20 Score (k =1) # Score locally the intersected result sets\nListing 4: GoT configuration for the set intersection use case\nwith 128 elements (cont.)\n21 KeepBestN (1) # Keep the best result\n22 merge step 5:\n23 Aggregate (5) # Merge intermediate intersected subsets from\nmerge step 1 and 2\n24 Score (k =1) # Score locally the intersected result sets\n25 KeepBestN (1) # Keep the best result\n26 merge step 6:\n27 Aggregate (5) # Merge intermediate intersected subsets from\nmerge step 3 and 4\n28 Score (k =1) # Score locally the intersected result sets\n29 KeepBestN (1) # Keep the best result\n30 final merge :\n31 Aggregate (5) # Merge intermediate intersected subsets from\nmerge step 5 and 6\n32 Score (k =1) # Score locally the intersected result sets\n33 KeepBestN (1) # Keep the best result\n34 GroundTruth () # Compare to precomputed result\nListing 5: GoT configuration for the sorting use case with 32\nelements\n1 Generate (k =1) # Split list into two halves of 16 elements\n2 foreach list part :\n3 Generate (k =5) # Sort list part\n4 Score (k =1) : # Score partially sorted list\n5 KeepBestN (1) : # Keep the best partially sorted list\n6 Aggregate (10) # Merge both partially sorted lists\n7 Score (k =1) # Score locally the sorted result lists\n8 KeepBestN (1) # Keep the best result\n9 Generate (k =10) # Try to improve solution\n10 Score (k =1) # Score locally the sorted result lists\n11 KeepBestN (1) # Keep the best result\n12 GroundTruth () # Compare to precomputed result\nListing 6: GoT configuration for the sorting use case with 64\nelements\n1 Generate (k =1) # Split list into four parts of 16 elements\n2 foreach list part :\n3 Generate (k =5) # Sort list part\n4 Score (k =1) # Score partially sorted list\n5 KeepBestN (1) # Keep the best partially sorted list\n6 merge step 1:\n7 Aggregate (10) # Merge partially sorted lists 1 and 2\n8 Score (k =1) # Score locally the partially sorted result lists\n9 KeepBestN (1) # Keep the best result\n10 Generate (k =5) # Try to improve the partial solution\n11 Score (k =1) # Score locally the partially sorted result lists\n12 KeepBestN (1) # Keep the best result\n13 merge step 2:\n14 Aggregate (10) # Merge partially sorted lists 3 and 4\n15 Score (k =1) # Score locally the partially sorted result lists\n16 KeepBestN (1) # Keep the best result\n17 Generate (k =5) # Try to improve the partial solution\n18 Score (k =1) # Score locally the partially sorted result lists\n19 KeepBestN (1) # Keep the best result\n20 final merge :\n21 Aggegrate (10) # Merge partially sorted lists from merge step\n1 and 2\n22 Score (k =1) # Score locally the sorted result lists\n23 KeepBestN (1) # Keep the best result\n24 Generate (k =10) # Try to improve solution\n25 Score (k =1) # Score locally the sorted result lists\n26 KeepBestN (1) # Keep the best result\n27 GroundTruth () # Compare to precomputed result\n62\nListing 7: GoT configuration for the sorting use case with\n128 elements\n1 Generate (k =1) # Split list into eight parts of 16 elements\n2 foreach list part :\n3 Generate (k =5) # Sort list part\n4 Score (k =1) # Score partially sorted list\n5 KeepBestN (1) # Keep the best partially sorted list\n6 merge step 1:\n7 Aggregate (10) # Merge partially sorted lists 1 and 2\n8 Score (k =1) # Score locally the partially sorted result lists\n9 KeepBestN (1) # Keep the best result\n10 Generate (k =5) # Try to improve the partial solution\n11 Score (k =1) # Score locally the partially sorted result lists\n12 KeepBestN (1) # Keep the best result\n13 merge step 2:\n14 Aggregate (10) # Merge partially sorted lists 3 and 4\n15 Score (k =1) # Score locally the partially sorted result lists\n16 KeepBestN (1) # Keep the best result\n17 Generate (k =5) # Try to improve the partial solution\n18 Score (k =1) # Score locally the partially sorted result lists\n19 KeepBestN (1) # Keep the best result\n20 merge step 3:\n21 Aggregate (10) # Merge partially sorted lists 5 and 6\n22 Score (k =1) # Score locally the partially sorted result lists\n23 KeepBestN (1) # Keep the best result\n24 Generate (k =5) # Try to improve the partial solution\n25 Score (k =1) # Score locally the partially sorted result lists\n26 KeepBestN (1) # Keep the best result\n27 merge step 4:\n28 Aggregate (10) # Merge partially sorted lists 7 and 8\n29 Score (k =1) # Score locally the partially sorted result lists\n30 KeepBestN (1) # Keep the best result\n31 Generate (k =5) # Try to improve the partial solution\n32 Score (k =1) # Score locally the partially sorted result lists\n33 KeepBestN (1) # Keep the best result\n34 merge step 5:\n35 Aggregate (10) # Merge partially sorted lists from merge step\n1 and 2\n36 Score (k =1) # Score locally the partially sorted result lists\n37 KeepBestN (1) # Keep the best result\n38 Generate (k =5) # Try to improve the partial solution\n39 Score (k =1) # Score locally the partially sorted result lists\n40 KeepBestN (1) # Keep the best result\n41 merge step 6:\n42 Aggregate (10) # Merge partially sorted lists from merge step\n3 and 4\n43 Score (k =1) # Score locally the partially sorted result lists\n44 KeepBestN (1) # Keep the best result\n45 Generate (k =5) # Try to improve the partial solution\n46 Score (k =1) # Score locally the partially sorted result lists\n47 KeepBestN (1 # Keep the best result\n48 final merge :\n49 Aggregate (10) # Merge partially sorted lists from merge step\n5 and 6\n50 Score (k =1) # Score locally the partially sorted result lists\n51 KeepBestN (1) # Keep the best result\n52 Generate (k =10) # Try to improve solution\n53 Score (k =1) # Score locally the sorted result lists\n54 KeepBestN (1) # Keep the best result\n55 GroundTruth () # Compare to precomputed result\n63",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7323254942893982
    },
    {
      "name": "Graph",
      "score": 0.6008792519569397
    },
    {
      "name": "Key (lock)",
      "score": 0.5686953663825989
    },
    {
      "name": "Theoretical computer science",
      "score": 0.5135179162025452
    },
    {
      "name": "Sorting",
      "score": 0.47826969623565674
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41251885890960693
    },
    {
      "name": "Cognitive science",
      "score": 0.3980887830257416
    },
    {
      "name": "Programming language",
      "score": 0.17943182587623596
    },
    {
      "name": "Psychology",
      "score": 0.1723518669605255
    },
    {
      "name": "Computer security",
      "score": 0.09411868453025818
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I108403487",
      "name": "Warsaw University of Technology",
      "country": "PL"
    }
  ]
}