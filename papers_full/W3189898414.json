{
  "title": "Segmenting Transparent Objects in the Wild with Transformer",
  "url": "https://openalex.org/W3189898414",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2530179367",
      "name": "Enze Xie",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2126920591",
      "name": "Wenjia Wang",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2133578637",
      "name": "Wenhai Wang",
      "affiliations": [
        "Nanjing University"
      ]
    },
    {
      "id": "https://openalex.org/A2946527707",
      "name": "Peize Sun",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2095922787",
      "name": "Hang Xu",
      "affiliations": [
        "Huawei Technologies (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2022074666",
      "name": "Ding Liang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1912778456",
      "name": "Ping Luo",
      "affiliations": [
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3035618398",
    "https://openalex.org/W3171516518",
    "https://openalex.org/W4308909683",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3117450517",
    "https://openalex.org/W2886934227",
    "https://openalex.org/W3119686997",
    "https://openalex.org/W2172859194",
    "https://openalex.org/W2412782625",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W3109733326",
    "https://openalex.org/W3115390238",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2962826264",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2560023338",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1923697677",
    "https://openalex.org/W2964242696",
    "https://openalex.org/W2963563573"
  ],
  "abstract": "This work presents a new fine-grained transparent object segmentation dataset, termed Trans10K-v2, extending Trans10K-v1, the first large-scale transparent object segmentation dataset. Unlike Trans10K-v1 that only has two limited categories, our new dataset has several appealing benefits. (1) It has 11 fine-grained categories of transparent objects, commonly occurring in the human domestic environment, making it more practical for real-world application. (2) Trans10K-v2 brings more challenges for the current advanced segmentation methods than its former version. Furthermore, a novel Transformer-based segmentation pipeline termed Trans2Seg is proposed. Firstly, the Transformer encoder of Trans2Seg provides the global receptive field in contrast to CNN's local receptive field, which shows excellent advantages over pure CNN architectures. Secondly, by formulating semantic segmentation as a problem of dictionary look-up, we design a set of learnable prototypes as the query of Trans2Seg's Transformer decoder, where each prototype learns the statistics of one category in the whole dataset. We benchmark more than 20 recent semantic segmentation methods, demonstrating that Trans2Seg significantly outperforms all the CNN-based methods, showing the proposed algorithm's potential ability to solve transparent object segmentation.Code is available in https://github.com/xieenze/Trans2Seg.",
  "full_text": "Segmenting Transparent Object in the Wild with Transformer\nEnze Xie1\u0003 , Wenjia Wang2 , Wenhai Wang3 , Peize Sun1 ,\nHang Xu4 , Ding Liang2 , Ping Luo1\n1The University of Hong Kong\n2Sensetime Retsearch\n3Nanjing University\n4Huawei Noah‚Äôs Ark Lab\nAbstract\nThis work presents a new Ô¨Åne-grained transpar-\nent object segmentation dataset, termed Trans10K-\nv2, extending Trans10K-v1, the Ô¨Årst large-scale\ntransparent object segmentation dataset. Unlike\nTrans10K-v1 that only has two limited categories,\nour new dataset has several appealing beneÔ¨Åts. (1)\nIt has 11 Ô¨Åne-grained categories of transparent ob-\njects, commonly occurring in the human domes-\ntic environment, making it more practical for real-\nworld application. (2) Trans10K-v2 brings more\nchallenges for the current advanced segmentation\nmethods than its former version. Furthermore,\na novel Transformer-based segmentation pipeline\ntermed Trans2Seg is proposed. Firstly, the Trans-\nformer encoder of Trans2Seg provides the global\nreceptive Ô¨Åeld in contrast to CNN‚Äôs local recep-\ntive Ô¨Åeld, which shows excellent advantages over\npure CNN architectures. Secondly, by formulat-\ning semantic segmentation as a problem of dic-\ntionary look-up, we design a set of learnable pro-\ntotypes as the query of Trans2Seg‚Äôs Transformer\ndecoder, where each prototype learns the statis-\ntics of one category in the whole dataset. We\nbenchmark more than 20 recent semantic segmen-\ntation methods, demonstrating that Trans2Seg sig-\nniÔ¨Åcantly outperforms all the CNN-based methods,\nshowing the proposed algorithm‚Äôs potential abil-\nity to solve transparent object segmentation.Code is\navailable in github.com/xieenze/Trans2Seg.\n1 Introduction\nModern robots, mainly mobile robots and mechanical manip-\nulators, would beneÔ¨Åt a lot from the efÔ¨Åcient perception of\nthe transparent objects in residential environments since the\nenvironments vary drastically. The increasing utilization of\nglass wall and transparent door in the building interior and\nthe glass cups and bottles in residential rooms has resulted in\nthe wrong detection in various range sensors. In robotic re-\nsearch, most systems perceive the environment by multi-data\nsensor fusion via sonars or lidars. The sensors are relatively\n\u0003Contact Author\n(a) Selected images and corresponding high-quality masks.\n40\n45\n50\n55\n60\n65\n70\n75\nTrans2SegTransLab\nDeepLabv3+\nDANetPSPNetOCNet\nDenseASPP\nFCNDUNetBiSeNetRefineNetHardNetHRNet\nFastSCNNContextNet\nLEDNetDFANet\nmIoU of Trans10K-v2\n200\n0\nGFLOPs\n(b) Performance comparison on Trans10K-v2.\nFigure 1: (a) shows the high diversity of our dataset and high-\nquality annotations. (b) is Comparisons between Trans2Seg and\nother CNN-based semantic segmentation methods. All methods are\ntrained on Trans10K-v2 with same epochs. mIoU is chosen as the\nmetric. Deeper color bar indicates methods with larger FLOPS. Our\nTrans2Seg signiÔ¨Åcantly surpasses other methods with lower Ô¨Çops.\nconsistent in detecting opaque objects but are still affected by\nthe scan mismatching due to transparent objects. The unique\nfeature of reÔ¨Çection, refraction, and light projection from the\ntransparent objects may confuse the sensors. Thus a reliable\nvision-based method, which is much cheaper and more robust\nthan high-precision sensors, would be efÔ¨Åcient.\nAlthough some transparent objects dataset [Chen et al.,\n2018a; Mei et al., 2020 ] were proposed, there are some ob-\nvious problems. (1) Limited dataset scale. These datasets of-\nten have less than 1K images captured from the real-world\nand less than 10 unique objects. (2) Poor diversity. The\nscene of these datasets is monotonous. (3) Fewer classes. All\nthese datasets have only two classes, background and trans-\nparent objects. They lack Ô¨Åne-grained categories, which lim-\nited their practicality. Recently, [Xie et al., 2020 ] proposed\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1194\na large-scale and high-diversity dataset termed Trans10K,\nwhich divide transparent objects as ‚ÄòThings‚Äô and ‚ÄòStuff‚Äô. The\ndataset is high diversity, but it also lacks Ô¨Åne-grained trans-\nparent categories.\nIn this paper, we proposes a Ô¨Åne-grained transparent ob-\nject segmentation dataset termed Trans10K-v2 with more\nelaborately deÔ¨Åned categories. The images are inherit from\nTrans10K-v1 [Xie et al., 2020]. We annotate the 10428 im-\nages with 11 Ô¨Åne-grained categories: shelf, jar, freezer, win-\ndow, glass door, eyeglass, cup, glass wall, glass bowl, water\nbottle, storage box. In Trans10K-v1, transparent things are\ndeÔ¨Åned to be grabbed by the manipulators and stuff are for\nrobot navigation. Though two basic categories can partially\nhelp robots to interact with transparent objects, the provided\nÔ¨Åne-grained classes in Trans10K-v2 can provide more. We\nanalyze these objects‚Äô functions and how robots interact with\nthem in appendix.\nBased on this challenging dataset, we design Trans2Seg,\nintroducing Transformer into segmentation pipeline for its\nencoder-decoder architecture. First, the Transformer encoder\nprovides a global receptive Ô¨Åeld via self-attention. Larger re-\nceptive Ô¨Åeld is essential for segmenting transparent objects\nbecause transparent objects often share similar textures and\ncontext with its surroundings. Second, the decoder stacks\nsuccessive layers to interact query embedding with Trans-\nformer encoder output. To facilitate the robustness of trans-\nparent objects, we carefully design a set of learnable class\nprototype embeddings as the query for Transformer decoder,\nand the key is the feature map from the Transformer encoder.\nCompared with convolutional paradigm, where the class pro-\ntotypes is the Ô¨Åxed parameters of convolution kernel weight,\nour design provides a dynamic and context-aware implemen-\ntation. As shown in Figure. 1b, we train and evaluate 20\nexisting representative segmentation methods on Trans10K-\nv2, and found that simply applying previous methods to this\ntask is far from sufÔ¨Åcient. By successfully introducing Trans-\nformer into this task, our Trans2Seg signiÔ¨Åcantly surpasses\nthe best TransLab [Xie et al., 2020] by a large margin (72.1\nvs. 69.0 on mIoU).\nIn summary, our main contributions are three-folds:\n‚Ä¢ We propose the largest glass segmentation dataset\n(Trans10K-v2) with 11 Ô¨Åne-grained glass image cate-\ngories with diverse scenarios and high resolution. All\nthe images are elaborately annotated with Ô¨Åne-shaped\nmasks and function-oriented categories.\n‚Ä¢ We introduce a new Transformer-based network for\ntransparent object segmentation with Transformer\nencoder-decoder architecture. Our method provides a\nglobal receptive Ô¨Åeld and is more dynamic in mask pre-\ndiction, which shows excellent advantages.\n‚Ä¢ We evaluate more than 20 semantic segmentation meth-\nods on Trans10K-v2, and our Trans2Seg signiÔ¨Åcantly\noutperforms these methods. Moreover, we show this\ntask largely unsolved. Thus more research is needed.\n2 Related Work\nSemantic Segmentation. In deep learning era, convolu-\ntional neural network (CNN) puts forwards the develop-\nment of semantic segmentation in various datasets, such as\nADE20K, CityScapes and PASCAL VOC. One of the pi-\noneer works approaches, FCN [Long et al., 2015 ], trans-\nfers semantic segmentation into an end-to-end fully convolu-\ntional classiÔ¨Åcation network. For improving the performance,\nespecially around object boundaries, [Chen et al., 2017;\nLin et al., 2016 ] propose to use structured prediction mod-\nule, conditional random Ô¨Åelds (CRFs) [Chen et al., 2014 ],\nto reÔ¨Åne network output. Dramatic improvements in perfor-\nmance and inference speed have been driven by aggregating\nfeatures at multiples scales, for example, PSPNet[Zhao et al.,\n2017] and DeepLab [Chen et al., 2017; Chen et al., 2018b],\nand propagating structured information across intermediate\nCNN representations [Gadde et al., 2016; Liu et al., 2017;\nWang et al., 2018].\nTransparent Object Datasets. [Chen et al., 2018a ] pro-\nposed TOM-Net. It contains 876 real images and 178K\nsynthetic images which are generated by POV-Ray. How-\never, only 4 unique objects are used in synthesizing the\ntraining data. Recently, [Xie et al., 2020 ] introduce the\nÔ¨Årst large-scale real-world transparent object segmentation\ndataset, termed Trans10K. It has 10K+ images while with\nonly 2 categories. In this work, our Trans10K-v2 inherited\nthe data and annotates 11 Ô¨Åne-grained categories.\nTransformer in Vision Tasks. Transformer [Vaswani et\nal., 2017 ] has been successfully applied in both high-level\nvision and low-level vision [Han et al., 2020]. In ViT [Doso-\nvitskiy et al., 2020 ], Transformer is directly applied to\nsequences of image patches to complete image classiÔ¨Åca-\ntion. In object detection areas [Carion et al., 2020; Zhu et\nal., 2020 ], DETR reasons about the relations of the object\nqueries and the global image context via Transformer and\noutputs the Ô¨Ånal set of predictions in parallel without non-\nmaximum suppression(NMS) procedures and anchor gener-\nation. SETR [Zheng et al., 2020 ] views semantic segmen-\ntation from a sequence-to-sequence perspective with Trans-\nformer. IPT [Chen et al., 2020 ] applies Transformer model\nto low-level computer vision task, such as denoising, super-\nresolution and deraining. In video processing, Transformer\nhas received signiÔ¨Åcantly growing attention. VisTR [Wang\net al., 2020 ] accomplishes instance sequence segmentation\nby Transformer. Multiple-object tracking [Sun et al., 2020;\nMeinhardt et al., 2021] employs Transformers to decode ob-\nject queries and feature queries of the previous frame into\nbounding boxes of the current frame, and merged by Hungar-\nian Algorithm or NMS.\n3 Trans10K-v2 Dataset\nDataset Introduction. Our Trans10K-v2 dataset is based\non Trans10K dataset [Xie et al., 2020]. Following Trans10K,\nwe use 5000, 1000 and 4428 images in training, valida-\ntion and testing respectively. The distribution of the im-\nages is abundant in occlusion, spatial scales, perspective dis-\ntortion. We further annotate the images with more Ô¨Åne-\ngrained categories due to the functional usages of differ-\nent objects. Trans10K-v2 dataset contains 10,428 images,\nwith two main categories and 11 Ô¨Åne-grained categories: (1)\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1195\nbackground\nshelf\nJar/Kettle\nfreezer\nwindow\ndoor\neyeglass\ncup wall\nbowl\nbottle\nbox\nFigure 2: Images in Trans10K-v2 dataset are carefully annotated with high quality. The Ô¨Årst row shows sample images and the second\nshows the segmentation masks. The color scheme which encodes the object categories are listed on the right of the Ô¨Ågure. Zoom in for best\nview.\nTrans10Kv2\nshelf door wall box freezer window cup bottle jar bowl eyeglass\nimage num\n280 1572 3059 603 90 501 3315 1472 997 340 410\nCMCC 3.36 5.19 5.61 2.57 3.36 4.27 1.97 1.82 1.99 1.31 2.56\npixel ratio(%) 2.49 9.23 38.42 3.67 1.02 4.28 22.61 6.23 6.75 3.67 0.78\nTable 1: Statistic information of Trans10K-v2.‚ÄòCMCC‚Äô denotes Mean Connected Components of each category. It is caculated by dividing\nthe connected components number of a certain category by the image number. It represents the complexity of the transparent objects.‚Äòimage\nnum‚Äô denotes the image number. ‚Äòpixel ratio‚Äô is the pixel number of a certain category accounts in all the pixels of transparent objects in\nTrans10K-v2.\nTransparent Things contain cup, bottle, jar, bowl and eye-\nglass. (2) Transparent Stuff contain windows, shelf, box,\nfreezer, glass walls and glass doors. In respect to Ô¨Åne-\ngrained categories and high diversity, Trans10K-v2 is very\nchallenging, and have promising potential in both computer\nvision and robotic researches.\nAnnotation Principle. The transparent objects are manu-\nally labeled by expert annotators with professional labeling\ntool. The annotators were asked to provide more than 100\npoints when they trace the boundaries of each transparent\nobject, which ensures the high-quality outline of the mask\nshapes. The way of annotation is mostly the same with se-\nmantic segmentation datasets such as ADE20K. We set the\nbackground with 0, and the 11 categories from 1 to 11. We\nalso provide the scene environment of each image locates at.\nThe annotators are asked to strictly following principles when\nthey label the images: (I) Highly transparent pixels of objects\nno matter made of glass, plastics or crystals are annotated as\nmasks, other semi-transparent and non-transparent pixels are\nignored. (II) When occluded by opaque objects, the pixels\nwill be cropped from the masks. (III) The detailed principle\nof how we categorize the objects is listed in appendix.\nDataset Statistics. The statistic information of CMCC,\nimaga number, pixel proportion are listed in Table 1 in detail.\nFrom Table1, the sum of all the image numbers is larger than\n10428 since some image has multiple categories of objects.\nSee the caption for detail.\nEvaluation Metrics. Results are reported in three metrics\nthat are widely used in semantic segmentation to benchmark\nthe performance of Ô¨Åne-grained transparent object segmenta-\ntion. (1) Pixel Accuracy indicates the proportion of correctly\nclassiÔ¨Åed pixels. (2) Mean IoU indicates mean intersection\nover union. (3) Category IoU indicates the intersection over\nunion of each category.\n4 Method\n4.1 Overall Pipeline\nThe overall Trans2Seg architecture contains a CNN back-\nbone, an encoder-decoder Transformer, and a small convo-\nlutional head, as shown in Figure 3. For an input image of\n(H; W;3),\n‚Ä¢ The CNN backbone generates image feature map of\n( H\n16 ; W\n16 ; C).\n‚Ä¢ The encoder takes in the summation of Ô¨Çattened feature\nof ( H\n16\nW\n16 ; C) and positional embedding of ( H\n16\nW\n16 ; C),\nand outputs encoded feature of ( H\n16\nW\n16 ; C).\n‚Ä¢ The decoder interacts the learned class prototypes of\n(N; C) with encoded feature, and generates attention\nmap of (N; M;H\n16\nW\n16 ), where N is number of categories,\nM is number of heads in multi-head attention.\n‚Ä¢ The small convolutional head up-samples the attention\nmap to (N; M;H\n4 ; W\n4 ), fuses it with high-resolution fea-\nture map Res2 and outputs attention map of(N; H\n4 ; W\n4 ).\nThe Ô¨Ånal segmentation is obtained by pixel-wise argmax op-\neration on the output attention map.\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1196\n3 x H x W\nC x (ùêª\n16\nùëä\n16)\nposition\nembedding\nCNN\n‚Ä¶\n‚Ä¶\nN x C\nLearned Class Prototypes\n‚Ä¶\n‚Ä¶\nC x (ùêª\n16\nùëä\n16) N x M xùêª\n16\tx ùëä\n16\nattention mapùë≠ ùë≠ùíÜ\nùë¨ùíÑùíçùíî\ninput image predicted mask\nN: number ofcategories\nC: feature channels\nH: image height\nW: image width\nM: number headsin Transformer\nsmall\nconv\nhead\nTransformer\nEncoder\nTransformer\nDecoder\nFigure 3: The whole pipeline of our hybrid CNN-Transformer architecture.First, the input image is fed to CNN to extract featuresF. Sec-\nond, for Transformer encoder, the features and position embedding are Ô¨Çatten and fed to Transformer for self-attention, and output feature(Fe)\nfrom Transformer encoder. Third, for Transformer decoder, we speciÔ¨Åcally deÔ¨Åne a set of learnable class prototype embeddings( Ecls)\nas query, Fe as key, and calculate the attention map with Ecls and Fe. Each class prototype embedding corresponds to a category of Ô¨Ånal\nprediction. We also add a small conv head to fuse attention map and Res2 feature from CNN backbone. Details of Transformer decoder and\nsmall conv head refer to Figure 4. Finally, we can get the predict results by doing pixel-wise argmax on the attention map. For example, in\nthis Ô¨Ågure, the segmentation mask of two categories (Bottle and Eyeglass) corresponds to two class prototypes with same colors.\nDecoder Layer\nDecoder Layer\nDecoder Layer\nDecoder Layer\nSmall Conv Head\nAttn. Map\nRes2. Feat\nTransformer Decoder\nEncoded\nFeatures\nCategory\nPrototypes \nResult. . . . . .\nNxHxW\nMulti-head Attention\nnew query\nEncoded Features\nC x !\n\"# x $\n\"#\nkey & value\nN x M x !\n\"# x $\n\"#\nAttention Map\nold query\nNxC\nCNN backbone\n4 x up & Concat\nConv & BN & Relu\nConv 1x1\nPoint-wise Argmax\nFigure 4: Detail of Transformer Decoder and small conv head.\nInput: The learnable category prototypes as query, features from\nTransformer encoder as key and value. The inputs are fed to Trans-\nformer decoder, which consists of several decoder layers. The at-\ntention map from last decoder layer and the Res2 feature from CNN\nbackbone are combined and fed to a small conv head to get Ô¨Ånal\nprediction result.\n4.2 Encoder\nThe Transformer encoder takes a sequence as input, so the\nspatial dimensions of the feature map ( H\n16 ; W\n16 ; C) is Ô¨Çattened\ninto one dimension ( H\n16\nW\n16 ; C). To compensate missing spa-\ntial dimensions, positional embedding [Gehring et al., 2017]\nis supplemented to one dimension feature to provide infor-\nmation about the relative or absolute position of the feature\nin the sequence. The positional embedding has the same di-\nmension ( H\n16\nW\n16 ; C) with the Ô¨Çattened feature. The encoder\nis composed of stacked encoder layers, each of which con-\nsists of a multi-head self-attention module and a feed forward\nnetwork [Vaswani et al., 2017].\n4.3 Decoder\nThe Transformer decoder takes input a set of learnable class\nprototype embeddings as query, denoted byEcls, the encoded\nfeature as key and value, denoted byFe, and output the atten-\ntion map followed by Small Conv Head to obtain Ô¨Ånal seg-\nmentation result, as shown in Figure 4.\nThe class prototype embeddings are learned category pro-\ntotypes, updated iteratively by a series of decoder layers\nthrough multi-head attention mechanisms. We denoted it-\nerative update rule by J, then the class prototype in each\ndecoder layer is:\nEs\ncls =\nK\ni=0;::;s\u00001\nsoftmax(Ei\nclsFe)Fe (1)\nIn the Ô¨Ånal decoder layer, the attention map is extracted out\nto into small conv head:\nattention map = Es\nclsFe (2)\nThe pseudo code of small conv head is shown in shown in\nFigure 4. The attention map from Transformer decode is the\nshape of (N; M;H\n16\nW\n16 ), where N is number of categories, M\nis number of heads in multi-head attention. It is up-sampled\nto (N; M;H\n4 ; W\n4 ), then fused with high-resolution feature\nmap Res2 in the second dimension to(N; M+C; H\n4 ; W\n4 ), and\nÔ¨Ånally transformed into output attention map of (N; H\n4 ; W\n4 ).\nThe Ô¨Ånal segmentation is obtained by pixel-wise argmax op-\neration on the output attention map.\n4.4 Discussion\nThe most related work with Trans2Seg is SETR and\nDETR [Zheng et al., 2020; Carion et al., 2020]. In this sec-\ntion we discuss the relations and differences in details.\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1197\nSETR . Trans2Seg and SETR are both segmentation\npipelines. Their key difference is reÔ¨Çected in the design of\nthe decoder. In SETR, the decoder is simple several con-\nvolutional layers, which is similar with most previous meth-\nods. However, the decoder of Trans2Seg is also Transformer,\nwhich fully utilize the advantages of attention mechanism in\nsemantic segmentation.\nDETR . Trans2Seg and DETR share similar components\nin the pipeline, including CNN backbone, Transformer en-\ncoder and decoder. The biggest difference is the deÔ¨Ånition of\nquery. In DETR, the decoder‚Äôs queries represents N learn-\nable objects because DETR is designed for object detection.\nHowever, in Trans2Seg, the queries represents N learnable\nclass prototypes, where each query represents one category.\nWe could see that the minor change on query design could\ngeneralize Transformer architecture to apply to diverse vision\ntasks, such as object detection and semantic segmentation.\n5 Experiments\n5.1 Implementation Details.\nWe implement Trans2Seg with Pytorch. The ResNet-50 [He\net al., 2016] with dilation convolution at last stage is adoped\nas the CNN extractor. For loss optimization, we use Adam\noptimizer with epsilon 1e-8 and weight decay 1e-4. Batch\nsize is 8 per GPU. We set learning rate 1e-4 and decayed by\nthe poly strategy [Yu et al., 2018 ] for 50 epochs. We use 8\nV100 GPUs for all experiments. For all CNN based methods,\nwe random scale and crop the image to 480 \u0002 480 in train-\ning, and resize image to 513 \u0002 513 in inference, following\ncommon setting on PASCAL VOC [Everingham and Winn,\n2011]. For our Trans2Seg, we adopt Transformer architec-\nture and need to keep the shape of learned position embedding\nsame in training/inference, so we directly resize the image to\n512 \u0002 512. Code has been released for community to follow.\n5.2 Ablation Studies.\nWe use the FCN[Long et al., 2015] as our baseline. FCN is a\nfully convolutional network with very simple design, and it is\nalso a very classic semantic segmentation method. First, we\ndemonstrate that Transformer encoder can build long range\nattention between pixels, which has much larger receptive\nÔ¨Åeld than CNN Ô¨Ålters. Second, we remove the CNN decoder\nin FCN and replace by our Transformer decoder, we design a\nset of learnable class prototypes as queries and show that this\ndesign further helps improve the accuracy. Third, we verify\nour method with Transformer at different scales.\nSelf-Attention of Transformer Encoder. As shown in\nTable 2, the FCN baseline without Transformer encoder\nachieves 62.7% mIoU. When adding Transformer encoder,\nthe mIoU directly improves 6.1%, achieving 66.8% mIoU.\nIt demonstrates that the self-attention module in Transformer\nencoder provides global receptive Ô¨Åled, which is better than\nCNN‚Äôs local receptive Ô¨Åeld in transparent object segmenta-\ntion.\nCategory Prototypes of Transformer Decoder. In Ta-\nble 2, we verify the effectiveness of learnable category proto-\ntypes in Transformer decoder. In row 2, with traditional CNN\nid Trans. Enc. Trans. Dec. CNN Dec. mIoU\n0 \u0002 \u0002 X 62.7\n1 X \u0002 X 68.8\n2 X X \u0002 72.1\nTable 2: Effectiveness of Transformer encoder and decoder.\n‚ÄòTrans.‚Äô indicates Transformer. ‚ÄòEnc.‚Äô and ‚ÄòDec.‚Äô means encoder\nand decoder.\nScale hyper-param. GFlops MParams mIoU\nsmall e128-n1-m2 40.9 30.5 69.2\nmedium e256-n4-m3 49.0 56.2 72.1\nlarge e768-n12-m4 221.8 327.5 70.3\nTable 3: Performance of Transformer at different scales. ‚Äòefag-\nnfbg-mfcg‚Äô means the Transformer with number of ‚Äòa‚Äô embedding\ndims, ‚Äòb‚Äô layers and ‚Äòc‚Äô mlp ratio.\ndecoder, the mIoU is 68.8%. However, with our Transformer\ndecoder, the mIoU boosts up to 72.1% with 3.3% improve-\nment. The strong performance beneÔ¨Åts from the Ô¨Çexible rep-\nresentation that learnable category prototypes as queries to\nÔ¨Ånd corresponding pixels in feature map.\nScale of Transformer. The scale of Transformer is mainly\ninÔ¨Çuenced by three hyper-parameters: (1) Embedding dim of\nfeature. (2) Number of attention layers. (3) MLP ratio in\nfeed forward layer. We are interested in whether enlarging\nthe model size can continuously improve performance. So\nwe set three combinations, as shown in Figure 3. We can Ô¨Ånd\nthat with the increase of the size of Transformer, the mIoU\nÔ¨Årst increases then decreases. We argue that without massive\npre-trained data, e.g.the large-scale nlp data BERT[Devlin et\nal., 2019] used, the size of Transformer is not the larger the\nbetter for our task.\n5.3 Comparison to the state-of-the-art.\nWe select more than 20 semantic segmentation methods:\nTranslab, Deeplabv3+, DABNet, PSPNet, OCNet, DenseA-\nspp, FCN, UNet, BiseNet, ReÔ¨ÅneNet, HardNet, HRNet,\nFastSCNN, ContextNet, LedNet, DUNet, ICNet, FPENet,\nDFANet, DANet, ESPNetv2, to evaluate on our Trans10K-\nv2 dataset, the methods selection largely follows the bench-\nmark of TransLab.For fair comparsion, we train all the meth-\nods with 50 epochs.\nTable 4 reports the overall quantitative comparison results\non test set. Our Trans2Seg achieves state-of-the-art 72.15%\nmIoU and 94.14% pixel ACC, signiÔ¨Åcant outperforms other\npure CNN-based methods. For example, our method is 2.1%\nhigher than TransLab, which is the previous SOTA method.\nWe also Ô¨Ånd that our method tend to performs much bet-\nter on small objects, such as ‚Äòbottle‚Äô and ‚Äôeyeglass‚Äô (10.0%\nand 5.0% higher than previous SOTA). We consider that the\nTransformer‚Äôs long range attention beneÔ¨Åts the small trans-\nparent object segmentation.\nIn Figure 5, we visualize the mask prediction of Trans2Seg\nand other CNN-based methods. We can Ô¨Ånd that owing to\nTransformer‚Äôs large receptive Ô¨Åeld and attention mechanism,\nour method can distinguish background and different cate-\ngories transparent objects much better than other methods,\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1198\nMethod FLOPs ACC\" mIoU\" Category IoU\"\nbg shelf Jar freezer window door eyeglass cup wall bowl bottle box\nFPENet 0.76 70.31 10.14 74.97 0.01 0.00 0.02 2.11 2.83 0.00 16.84 24.81 0.00 0.04 0.00\nESPNetv2 0.83 73.03 12.27 78.98 0.00 0.00 0.00 0.00 6.17 0.00 30.65 37.03 0.00 0.00 0.00\nContextNet 0.87 86.75 46.69 89.86 23.22 34.88 32.34 44.24 42.25 50.36 65.23 60.00 43.88 53.81 20.17\nFastSCNN 1.01 88.05 51.93 90.64 32.76 41.12 47.28 47.47 44.64 48.99 67.88 63.80 55.08 58.86 24.65\nDFANet 1.02 85.15 42.54 88.49 26.65 27.84 28.94 46.27 39.47 33.06 58.87 59.45 43.22 44.87 13.37\nENet 2.09 71.67 8.50 79.74 0.00 0.00 0.00 0.00 0.00 0.00 0.00 22.25 0.00 0.00 0.00\nHRNetw18 4.20 89.58 54.25 92.47 27.66 45.08 40.53 45.66 45.00 68.05 73.24 64.86 52.85 62.52 33.02\nHardNet 4.42 90.19 56.19 92.87 34.62 47.50 42.40 49.78 49.19 62.33 72.93 68.32 58.14 65.33 30.90\nDABNet 5.18 77.43 15.27 81.19 0.00 0.09 0.00 4.10 10.49 0.00 36.18 42.83 0.00 8.30 0.00\nLEDNet 6.23 86.07 46.40 88.59 28.13 36.72 32.45 43.77 38.55 41.51 64.19 60.05 42.40 53.12 27.29\nICNet 10.64 78.23 23.39 83.29 2.96 4.91 9.33 19.24 15.35 24.11 44.54 41.49 7.58 27.47 3.80\nBiSeNet 19.91 89.13 58.40 90.12 39.54 53.71 50.90 46.95 44.68 64.32 72.86 63.57 61.38 67.88 44.85\nDenseASPP 36.20 90.86 63.01 91.39 42.41 60.93 64.75 48.97 51.40 65.72 75.64 67.93 67.03 70.26 49.64\nDeepLabv3+ 37.98 92.75 68.87 93.82 51.29 64.65 65.71 55.26 57.19 77.06 81.89 72.64 70.81 77.44 58.63\nFCN 42.23 91.65 62.75 93.62 38.84 56.05 58.76 46.91 50.74 82.56 78.71 68.78 57.87 73.66 46.54\nOCNet 43.31 92.03 66.31 93.12 41.47 63.54 60.05 54.10 51.01 79.57 81.95 69.40 68.44 78.41 54.65\nReÔ¨ÅneNet 44.56 87.99 58.18 90.63 30.62 53.17 55.95 42.72 46.59 70.85 76.01 62.91 57.05 70.34 41.32\nTranslab 61.31 92.67 69.00 93.90 54.36 64.48 65.14 54.58 57.72 79.85 81.61 72.82 69.63 77.50 56.43\nDUNet 123.69 90.67 59.01 93.07 34.20 50.95 54.96 43.19 45.05 79.80 76.07 65.29 54.33 68.57 42.64\nUNet 124.55 81.90 29.23 86.34 8.76 15.18 19.02 27.13 24.73 17.26 53.40 47.36 11.97 37.79 1.77\nDANet 198.00 92.70 68.81 93.69 47.69 66.05 70.18 53.01 56.15 77.73 82.89 72.24 72.18 77.87 56.06\nPSPNet 187.03 92.47 68.23 93.62 50.33 64.24 70.19 51.51 55.27 79.27 81.93 71.95 68.91 77.13 54.43\nTrans2Seg 49.03 94.14 72.15 95.35 53.43 67.82 64.20 59.64 60.56 88.52 86.67 75.99 73.98 82.43 57.17\nTable 4: Evaluated state-of-the-art semantic segmentation methods. Sorted by FLOPs. Our proposes Trans2Seg surpasses all the other\nmethods in pixel accuracy and mean IoU, as well as most of the category IoUs (8 in 11).\nespecially when the image contains multiple objects of differ-\nent categories. Moreover, our method can obtain high quality\ndetail information,e.g. boundary of object, and tiny transpar-\nent objects, while other CNN-based methods fail to do so.\nMore results are shown in supplementary material.\n6 Conclusion\nIn this paper, we present a challenging Ô¨Åne-grained transpar-\nent object segmentation dataset with 11 common categories,\ntermed Trans10K-v2. Moreover, we propose a Transformer-\nbased pipeline with encoder having global receptive Ô¨Åeld and\ndecoder with category query, termed Trans2Seg, to solve this\nchallenging task. Finally, we evaluate more than 20 main-\nstream semantic segmentation methods and shows that our\nTrans2Seg clearly surpass these CNN-based segmentation\nmethods.\nIn the future, we are interested in exploring Transformer\nencoder-decoder on general segmentation tasks, such as\nCityscapes and PASCAL VOC. We will also put more efforts\nto solve transparent object segmentation task.\nAcknowledgements\nThis work is partially supported by the General Research\nFund of Hong Kong No. 27208720, and the Research Do-\nnation from Huawei. We Thank Yaojun Liu for insightful\ndiscussion.\nTrans2SegDeep\nLabv3+ FCN ICNet PSPNetImage\nGroundTruth\nFigure 5: Visual comparison of Trans2Seg to other CNN-based se-\nmantic segmentation methods. Our Trans2Seg clearly outperforms\nothers thanks to the Transformer‚Äôs global receptive Ô¨Åeld and atten-\ntion mechanism, especially in dash region. Zoom in for best view.\nRefer to supplementary materials for more visualized results.\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1199\nReferences\n[Carion et al., 2020] Nicolas Carion, Francisco Massa,\nGabriel Synnaeve, Nicolas Usunier, Alexander Kirillov,\nand Sergey Zagoruyko. End-to-End object detection with\ntransformers. In ECCV, 2020.\n[Chen et al., 2014] Liang-Chieh Chen, George Papandreou,\nIasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Se-\nmantic image segmentation with deep convolutional nets\nand fully connected crfs. 2014.\n[Chen et al., 2017] Liang-Chieh Chen, George Papandreou,\nIasonas Kokkinos, Kevin Murphy, and Alan L Yuille.\nDeeplab: Semantic image segmentation with deep convo-\nlutional nets, atrous convolution, and fully connected crfs.\nTPAMI, 2017.\n[Chen et al., 2018a] Guanying Chen, Kai Han, and Kwan-\nYee K. Wong. Tom-net: Learning transparent object mat-\nting from a single image. In CVPR, 2018.\n[Chen et al., 2018b] Liang-Chieh Chen, Yukun Zhu, George\nPapandreou, Florian Schroff, and Hartwig Adam.\nEncoder-decoder with atrous separable convolution for se-\nmantic image segmentation. In ECCV, 2018.\n[Chen et al., 2020] Hanting Chen, Yunhe Wang, Tianyu\nGuo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma,\nChunjing Xu, Chao Xu, and Wen Gao. Pre-trained image\nprocessing transformer. arXiv, 2020.\n[Devlin et al., 2019] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. BERT: pre-training of\ndeep bidirectional transformers for language understand-\ning. In Jill Burstein, Christy Doran, and Thamar Solorio,\neditors, NAACL-HLT 2019, 2019.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Min-\nderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition\nat scale. arXiv, 2020.\n[Everingham and Winn, 2011] Mark Everingham and John\nWinn. The pascal visual object classes challenge 2012\n(voc2012) development kit. Pattern Analysis, Statistical\nModelling and Computational Learning, Tech. Rep, 2011.\n[Gadde et al., 2016] Raghudeep Gadde, Varun Jampani,\nMartin Kiefel, Daniel Kappler, and Peter V Gehler. Super-\npixel convolutional networks using bilateral inceptions. In\nECCV, 2016.\n[Gehring et al., 2017] Jonas Gehring, Michael Auli, David\nGrangier, Denis Yarats, and Yann N Dauphin. Convolu-\ntional sequence to sequence learning. arXiv, 2017.\n[Han et al., 2020] Kai Han, Yunhe Wang, Hanting Chen,\nXinghao Chen, Jianyuan Guo, Zhenhua Liu, Yehui Tang,\nAn Xiao, Chunjing Xu, Yixing Xu, et al. A survey on\nvisual transformer. arXiv, 2020.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing\nRen, and Jian Sun. Deep residual learning for image recog-\nnition. In CVPR, 2016.\n[Lin et al., 2016] Guosheng Lin, Chunhua Shen, Anton Van\nDen Hengel, and Ian Reid. EfÔ¨Åcient piecewise training\nof deep structured models for semantic segmentation. In\nCVPR, 2016.\n[Liu et al., 2017] Sifei Liu, Shalini De Mello, Jinwei Gu,\nGuangyu Zhong, Ming-Hsuan Yang, and Jan Kautz.\nLearning afÔ¨Ånity via spatial propagation networks. In\nNIPS, 2017.\n[Long et al., 2015] Jonathan Long, Evan Shelhamer, and\nTrevor Darrell. Fully convolutional networks for seman-\ntic segmentation. In CVPR, 2015.\n[Mei et al., 2020] Haiyang Mei, Xin Yang, Yang Wang,\nYuanyuan Liu, Shengfeng He, Qiang Zhang, Xiaopeng\nWei, and Rynson W.H. Lau. Don‚Äôt hit me! glass detec-\ntion in real-world scenes. In CVPR, June 2020.\n[Meinhardt et al., 2021] Tim Meinhardt, Alexander Kirillov,\nLaura Leal-Taixe, and Christoph Feichtenhofer. Track-\nformer: Multi-object tracking with transformers. arXiv,\n2021.\n[Sun et al., 2020] Peize Sun, Yi Jiang, Rufeng Zhang, Enze\nXie, Jinkun Cao, Xinting Hu, Tao Kong, Zehuan Yuan,\nChanghu Wang, and Ping Luo. Transtrack: Multiple-\nobject tracking with transformer. arXiv, 2020.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NeuraIPS, 2017.\n[Wang et al., 2018] Xiaolong Wang, Ross Girshick, Abhinav\nGupta, and Kaiming He. Non-local neural networks. In\nCVPR, 2018.\n[Wang et al., 2020] Yuqing Wang, Zhaoliang Xu, Xinlong\nWang, Chunhua Shen, Baoshan Cheng, Hao Shen, and\nHuaxia Xia. End-to-end video instance segmentation with\ntransformers. arXiv, 2020.\n[Xie et al., 2020] Enze Xie, Wenjia Wang, Wenhai Wang,\nMingyu Ding, Chunhua Shen, and Ping Luo. Segmenting\ntransparent objects in the wild. 2020.\n[Yu et al., 2018] Changqian Yu, Jingbo Wang, Chao Peng,\nChangxin Gao, Gang Yu, and Nong Sang. Bisenet: Bi-\nlateral segmentation network for real-time semantic seg-\nmentation. In ECCV, 2018.\n[Zhao et al., 2017] Hengshuang Zhao, Jianping Shi, Xiao-\njuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene\nparsing network. In CVPR, 2017.\n[Zheng et al., 2020] Sixiao Zheng, Jiachen Lu, Hengshuang\nZhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu,\nJianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethink-\ning semantic segmentation from a sequence-to-sequence\nperspective with transformers. arXiv, 2020.\n[Zhu et al., 2020] Xizhou Zhu, Weijie Su, Lewei Lu, Bin\nLi, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection.\narXiv, 2020.\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1200",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8511046171188354
    },
    {
      "name": "Segmentation",
      "score": 0.8077894449234009
    },
    {
      "name": "Encoder",
      "score": 0.703350305557251
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6628640294075012
    },
    {
      "name": "Market segmentation",
      "score": 0.5392414927482605
    },
    {
      "name": "Scale-space segmentation",
      "score": 0.5022857189178467
    },
    {
      "name": "Transformer",
      "score": 0.48984333872795105
    },
    {
      "name": "Image segmentation",
      "score": 0.4773295521736145
    },
    {
      "name": "Segmentation-based object categorization",
      "score": 0.46900057792663574
    },
    {
      "name": "Computer vision",
      "score": 0.4413146376609802
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.39620277285575867
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}