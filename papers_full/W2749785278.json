{
  "title": "N-gram and Neural Language Models for Discriminating Similar Languages",
  "url": "https://openalex.org/W2749785278",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5026368719",
      "name": "Andre Cianflone",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5045594421",
      "name": "Leila Kosseim",
      "affiliations": [
        "Concordia University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2078396547",
    "https://openalex.org/W1533946607",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W244375653",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2949952663",
    "https://openalex.org/W2561747913",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2521605970",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W3088633338",
    "https://openalex.org/W2553512497"
  ],
  "abstract": "This paper describes our submission (named clac) to the 2016 Discriminating Similar Languages (DSL) shared task. We participated in the closed Sub-task 1 (Set A) with two separate machine learning techniques. The first approach is a character based Convolution Neural Network with a bidirectional long short term memory (BiLSTM) layer (CLSTM), which achieved an accuracy of 78.45% with minimal tuning. The second approach is a character-based n-gram model. This last approach achieved an accuracy of 88.45% which is close to the accuracy of 89.38% achieved by the best submission, and allowed us to rank #7 overall.",
  "full_text": "N-gram and Neural Language Models\nfor Discriminating Similar Languages\nAndre Cianﬂone and Leila Kosseim\nDept. of Computer Science & Software Engineering\nConcordia University\n{a cianfl|kosseim}@encs.concordia.ca\nAbstract\nThis paper describes our submission (named clac) to the 2016 Discriminating Similar Lan-\nguages (DSL) shared task. We participated in the closed Sub-task 1 (Set A) with two separate\nmachine learning techniques. The ﬁrst approach is a character based Convolution Neural Net-\nwork with a bidirectional long short term memory (BiLSTM) layer (CLSTM), which achieved\nan accuracy of 78.45% with minimal tuning. The second approach is a character-based n-gram\nmodel. This last approach achieved an accuracy of 88.45% which is close to the accuracy of\n89.38% achieved by the best submission, and allowed us to rank #7 overall.\n1 Introduction\nDiscriminating between languages is often the ﬁrst task to many natural language applications (NLP),\nsuch as machine translation or information retrieval. Current approaches to address this problem achieve\nimpressive results in ideal conditions: a small number of unrelated or dissimilar languages, enough\ntraining data and long enough sentences. For example, Sim ˜oes et al. achieved an accuracy of 97%\non the discrimination of 25 languages in TED talks (Sim ˜oes et al., 2014). However, in the case of\ndiscriminating between similar languages or dialects, such as French Canadian and European French,\nor Spanish varieties, the task is more challenging (Goutte and Leger, 2015). This problem is addressed\nspeciﬁcally in the DSL shared task at VarDial 2016 (DSL 2016). In comparison to results from Sim ˜oes\net al. who achieved a 97% accuracy, the best performing system at DSL 2016 achieved only an 89.38%\naccuracy.\nThis paper describes our system and submission at the DSL 2016 shared task. The shared task is\nsplit into two main sub-tasks. Sub-task 1 aims at discriminating between similar languages and national\nlanguage varieties; whereas Sub-task 2 focuses on Arabic dialect identiﬁcation. We will only describe\nthe speciﬁcs of Sub-task 1, for which we submitted results. For Sub-task 1, participants could chose\nbetween the closed submission, where only the use of the DSL Corpus Collection, provided by the\norganisers (see Section 3), was allowed; or the open task which permitted the use of any external data for\ntraining. Participants could also submit runs for two different data sets: Set A, composed of newspaper\narticles, and Set B, composed of social media data. We only participated in the closed Sub-task 1 using\nSet A. Hence, our task was to discriminate between 12 similar languages and national language varieties\nusing only the newspaper articles provided in the DSL corpus as training set. For a full description of\nall sub-tasks, see the overview paper (Malmasi et al., 2016), which also discusses data and results for all\nparticipants.\nIt was our ﬁrst participation to the DSL task, and registered late to the shared task. Hence our system\nis the result of a 3 person-week effort. We started with very little existing code. We had experimented\npreviously with neural language models (NLM) and wanted to evaluate their applicability to this task.\nIn addition, we believed that a convolutional plus long-short term memory network (CLSTM) would be\nappropriate for the task given their success in several other NLP tasks (see Section 2 for details). In the\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details:\nhttp://creativecommons.org/licenses/by/4.0/\narXiv:1708.03421v1  [cs.CL]  11 Aug 2017\nend, we managed to submit 3 runs: run1 and run2 consist of standard character-based n-gram models;\nwhile run3 is the CLSTM. Our best performance was achieved by run1, with an accuracy of 88.45%\nranking it 7th among the 17 participants, and arriving very close to the top run which had an accuracy of\n89.38%. Alas, our run3, the CLSTM, attained an accuracy of 78.45% but beneﬁted from very minimal\ntuning.\n2 Related Work\nThrough the years, statistical language identiﬁcation has received much attention in Natural Language\nProcessing. The standard technique of character n-gram modeling has traditionally been very success-\nful for this application (Cavnar and Trenkle, 1994), but other statistical approaches such as Markov\nmodels over n-grams (Dunning, 1994), dot products of word frequency vectors (Dafmashek, 1995), and\nstring kernels in support vector machines (Kruengkrai et al., 2005) have also provided impressive results.\nHowever, as noted by (Baldwin and Lui, 2010), more difﬁcult situations where languages are similar, less\ntraining data is available or the text to identify is short can signiﬁcantly degrade performance. This is\nwhy, more recently, much effort has addressed more difﬁcult questions such as the language identiﬁ-\ncation of related languages in social media texts (e.g. (Zubiaga et al., 2014)) and the discrimination of\nsimilar languages (e.g. (Zampieri et al., 2015; Malmasi et al., 2016)).\nThe second Discriminating Similar Languages shared task (DSL 2015) aimed to discriminate between\n15 similar languages and varieties, with an added “other” category. At this shared task, the best accuracy\nwas 95.54% and was achieved by (Malmasi and Dras, 2015). The authors used two classes of features:\ncharacter n-grams (with n=1 to 6) and word n-grams (with n=1 to 2). Three systems were submitted for\nevaluation. The ﬁrst was a single Support Vector Machine (SVM) trained on the features above; while\nthe other two systems were ensemble classiﬁers, combining the results of various classiﬁers with a mean\nprobability combiner. A second team at DSL 2015 relied on a two-stage process, ﬁrst predicting the\nlanguage group and then the speciﬁc language variant (Goutte and Leger, 2015). This team achieved\nan accuracy of 95.24%. As (Goutte et al., 2016) note, many other techniques were also used for the\ntask, such as TF-IDF and SVM, token-based backoff, prediction by partial matching with accuracies\nachieving between 64.04% and 95.54%. An interesting experiment at DSL-2015 consisted in having two\nversions of the corpora, where one corpus was the original newspaper articles; while the other substituted\nnamed entities with placeholders. The aim was to evaluate how strong a clue named entities are in the\nidentiﬁcation of language varieties. By relying heavily on geographic names, for example, which are\nhighly correlated to speciﬁc nations, it was thought that accuracy would increase signiﬁcantly. However,\nsurprisingly, accuracy on the modiﬁed data set was only 1 to 2 percentage points lower than the original\ndata set for all systems (Goutte et al., 2016).\nGiven the recent success of Recurrent Neural Networks in many NLP tasks, such as machine trans-\nlation (Bahdanau et al., 2015) and image captioning (Karpathy and Fei-Fei, 2015), we believed that an\ninteresting approach for the DSL task would be to use solely characters as inputs, and add the ability to\nﬁnd long-distance relations within texts. Neural models are quite efﬁcient at abstracting word meaning\ninto a dense vector representation. Mikolov et al. for example, developed an efﬁcient method to repre-\nsent syntactic and semantic word relationships through a neural network (Mikolov et al., 2013) and the\nresulting vectors can be used in a variety of NLP tasks. For certain NLP tasks however, Convolutional\nNeural Networks (ConvNets), extensively studied in computer vision, have been shown to be effective\nfor text classiﬁcation. For example, (Zhang et al., 2015) experimented with ConvNets on commonly used\nlanguage data sets, such as topic classiﬁcation and polarity detection. A key conclusion of their study\nis that traditional methods, such as n-grams, work best for small data sets, whereas character ConvNets\nwork best for data sets with millions of instances. Since the DSL data set contained a few thousand\ninstances (see Section 3), we decided to give it a try. Further, it has been shown recently that augmenting\nConvNets with Recurrent Neural Networks (RNNs) is an effective way to model word sequences (Kim et\nal., 2016), (Choi et al., 2016). For this reason, we developed a neural model based on the latter method.\n3 Data Set\nBecause we participated in the closed task, we only used the DSL Corpus Collection (Set A) (Tan et\nal., 2014) provided by the organisers. The data set contained 12 languages organised into 5 groups: two\ngroups of similar languages and three of national language varieties.\nGroup 1: Similar languages: Bosnian, Croatian, and Serbian\nGroup 2: Similar languages: Malay and Indonesian\nGroup 3: National varieties of Portuguese: Brazil and Portugal\nGroup 4: National varieties of Spanish: Argentina, Mexico, and Spain\nGroup 5: National varieties of French: France and Canada\nTable 1 illustrates statistics of the shared task data set. As shown in the table, the data set is equally\ndivided into 12 similar languages and national language varieties with 18,000 training instances for each\nlanguage. On average, each instance is 35 tokens long and contain 219 characters.\nGroup Language Code Train. Dev. Test Av. # char. Av. # token\n1 1 Bosnian bs 18,000 2,000 1,000 198 31\n2 Croatian hr 18,000 2,000 1,000 240 37\n3 Serbian sr 18,000 2,000 1,000 213 34\n2 4 Malaysian my 18,000 2,000 1,000 182 26\n5 Indonesian id 18,000 2,000 1,000 240 34\n3 6 Spanish (Argentina) es-AR 18,000 2,000 1,000 254 41\n7 Spanish (Spain) es-ES 18,000 2,000 1,000 268 45\n8 Spanish (Mexico) es-MX 18,000 2,000 1,000 182 31\n4 9 Portuguese (Brazil) pt-BR 18,000 2,000 1,000 241 40\n10 Portuguese (Portugal) pt-PT 18,000 2,000 1,000 222 36\n5 11 French (Canada) fr-CA 18,000 2,000 1,000 175 28\n12 French (France) fr-FR 18,000 2,000 1,000 216 35\nTotal 216,000 24,000 12,000 219 35\nTable 1: Statistics of DSL 2016 Data set A. We list the number of instances across languages for the\nTraining, Development and Test sets. The last two columns represent the average number of characters\nand average number of tokens of the training set.\nSince the results of our CLSTM model (See Section 4.2) were lower than expected during the devel-\nopment phase, we attempted to increase the size of the training set. Using the data set from DSL-2015,\nwe could ﬁnd additional training data for most languages, with the exception of French. We therefore\nattempted to use publicly vailable corpora for French. For Canadian French, we used the Canadian\nHansard1; whereas for France French, we used the French monolingual news crawl data set (2013 ver-\nsion) from the ACL 2014 Ninth Workshop on Statistical Machine Translation 2. However, upon closer\ninvestigation, this last corpus clearly contained non-French news content, heavily referencing locations\nand other international entities. Additionally, the majority of the Canadian French Hansard is translated\nfrom English, possibly not being representative of actual Canadian French. We experimented with these\ntwo additional data sets, but the accuracy of our models was far from our closed task equivalent. Given\nour short development time, we decided to drop the open task, and train our models on only the given\nDSL 2016 Data Set A.\n4 Methodology\nAs indicated in Section 1, we experimented with two main approaches: a standard n-gram model to\nuse as baseline, and a convolution neural network (ConvNet) with bidirectional long-short term memory\nrecurrent neural network (BiLSTM), which we refer to as CLSTM.\n1http://www.isi.edu/natural-language/download/hansard/\n2http://www.statmt.org/wmt14/translation-task.html\n4.1 N-gram Model\nOur baseline is a standard text-book character-based n-gram model (Jurafsky and Martin, 2014). Because\nwe used a simple baseline, the same unmodiﬁed character set (including no case-folding) is used for both\nof our approaches, for easier later comparison. During training, the system calculates the frequency of\neach n-gram for each language. Then, at test time, the model computes a probability distribution over all\npossible languages and selects the most probable language as the output. Unseen n-grams were smoothed\nwith additive smoothing with α = 0.1. As discussed in Section 5, surprisingly, this standard approach\nwas much more accurate than our complex neural network. We experimented with different values for\nnwith the development set given (see Section 3). As table 2 shows, the accuracy peaks at sizes n = 7\nand n = 8; while larger n-grams degrade in performance and explode in memory use. The curse of\ndimensionality seriously limits this type of approach.\nN-gram size Accuracy\n1 0.5208\n2 0.6733\n3 0.7602\n4 0.7523\n5 0.8035\n6 0.8303\n7 0.8424\n8 0.8474\nTable 2: Accuracy across n-grams of sizes 1 to 8 with the development Set A.\n4.2 Convolution Neural Network with Long Short Term Memory (CLSTM)\nOur second approach is a Convolution Neural Network with a Bidirectional Long Short Term Memory\nlayer (CLSTM). The goal of this approach was to build a single neural model without any feature engi-\nneering, solely taking the raw characters as input. Using characters as inputs has the added advantage of\ndetecting language patterns even with little data available. For example, a character based neural model\ncan predict the word running as being more likely to be in English thancourir if it has seen the word run\nin English training texts. In a word based model that has not seen the word in this form, running would\nbe represented as a random vector. Given the heavy computational requirements of training neural mod-\nels and the limited time we had, we could not develop an ensemble neural model system, which could\ncombine the strength of diverse models.\nThe input to the model is the raw text where each character in an instance has been mapped to its one-\nhot representation. Each character is therefore encoded as a vector of dimension d, where dis a function\nof the maximum number of unique characters in the corpus. Luckily, the languages share heavily in\nalphabets and symbols, limiting dto 217. A ﬁxed number of characters lis chosen from each instance.\nSince our texts are relatively short, as observed by the character average column in Table 1, we set lto\n256. Shorter texts are zero padded, while longer instances are cut after the ﬁrst 256 characters. Our input\nmatrix Ais thus a d×lmatrix where elements Aij ∈{0,1}. The input feeds into three sequences of\nconvolutions and max-pooling. We used temporal max-pooling, the 1D version equivalent in computer\nvision. Our ConvNet parameters are heavily based on (Zhang et al., 2015)’s empirical research who\nobserved that the temporal max-pooling technique is key to deep convolutional networks with text. We\nfurther improved results on our development set by stacking the ConvNet with a Bidirectional LSTM\n(BiLSTM). The BiLSTM effectively takes the output of the ConvNet as its input. As shown in Table 3,\nthe two LSTM layers are merged by concatenation and followed by a fully connected layer with 1024\nunits. ReLU is used as activation function and loss is measured on cross-entropy and optimized with\nthe Adam algorithm (Kingma and Ba, 2015). The system is built as a single neural network with no\npre-training. We could not test much wider networks due to lack of computing capability. However, as\nexperienced by (Zhang et al., 2015), it seems that much wider networks than our own would result in\nlittle, if any, performance improvement. The model is built in Keras3 and TensorFlow4.\nLayer Type Features Kernel Max-pooling\n1 Convolutional 256 7 3\n2 Convolutional 256 7 3\n3 Convolutional 256 3 3\n4 LSTM (left) 128 - -\n5 LSTM (right) 128 - -\n6 Dense 1024 - -\nTable 3: Layers used in our neural network. The Features column represents the number of ﬁlters for\nthe convolutional layers and hidden units for LSTM and Dense layers. Layers 4 and 5 are merged by\nconcatenation to form the BiLSTM layer. Dropout was added between layer 6 and the output layer (not\nlisted in the table).\nWith the development set provided, the accuracy of the CLSTM approach reached 82% on average\nwhich was below but comparable to the n-gram model. Additionally, our tests on the development set\nshowed that adding the BiLSTM on top of our ConvNet does indeed increase performance. We were\nable to improve accuracy by 2 to 3% on average, with little additional computing time.\n5 Results and Discussion\nWe submitted 3 runs for the closed test Set A: run1 – the N-gram of size 7, run2 – the N-gram of\nsize 8, and run3 – the CLSTM model. Table 4 shows the overall results of all 3 runs on the ofﬁcial test\nset. As the table shows, the standard n-gram model signiﬁcantly outperformed the CLSTM model. It\nis interesting to note that the difference between the two n-grams is negligible. This was also observed\nduring training (see Section 4.1). Recall from Table 2 that the accuracy peaked at sizesn= 7and n= 8\non the development set reaching 84.74%. The 3.71% increase with the test set was a welcome surprise.\nOn the other hand, the CLSTM performed about 3.55% lower during the test than it did at training time,\ndecreasing from an average of 82% to 78.46%. Overall, as Table 5 shows, our run1 (labeled clac)\nranked #7 with respect to the best runs of all 17 participating teams.\nRun Description Accuracy F1 (micro) F1 (macro) F1 (weighted)\nRun 1 N-gram 7 0.8845 0.8845 0.8813 0.8813\nRun 2 N-gram 8 0.8829 0.8829 0.8812 0.8812\nRun 3 CLSTM 0.7845 0.7845 0.7814 0.7814\nTable 4: Results of our 3 submissions on test set A (closed training).\nTable 6 shows the confusion matrix for our best run, the N-gram of size 7. For comparative purposes,\nwe have added the confusion matrix in Table 7 for our third and lesser performing model, the CLSTM.\nAs shown in Tables 6 and 7, for all language groups the N-gram performed signiﬁcantly better than\nthe CLSTM. However, with both models, misclassiﬁcations outside of a language group are sparse and\nstatistically insigniﬁcant. This may indicate that a two-stage hierarchical process, as proposed by (Goutte\nand Leger, 2015), is not necessary for the models we propose.\nAs shown in Tables 6 and 7, the major difﬁculty for our models was the classiﬁcation of the Spanish\nvarieties in Group 3. It seems that the addition of Mexican Spanish is a signiﬁcant challenge to dis-\ncriminating national varieties of Spanish. At DSL 2015, (Goutte and Leger, 2015) were able to classify\nEuropean Spanish and Argentine Spanish with an 89.4% accuracy, lower than for other languages. Given\nthe low variability among the best performing systems (see Table 5), and the lower performance with re-\nspect to previous iterations of the DSL shared task, this was likely a challenge for all systems at DSL\n2016.\n3https://keras.io/\n4https://www.tensorﬂow.org/\nRank Team Run Accuracy F1 (weighted)\n1 tubasfs run1 0.8938 0.8937\n2 SUKI run1 0.8879 0.8877\n3 GWU LT3 run3 0.8870 0.8870\n4 nrc run1 0.8859 0.8859\n5 UPV UA run1 0.8833 0.8838\n6 PITEOG run3 0.8826 0.8829\n7 clac run1 0.8845 0.8813\n8 XAC run3 0.8790 0.8786\n9 ASIREM run1 0.8779 0.8778\n10 hltcoe run1 0.8772 0.8769\n11 UniBucNLP run2 0.8647 0.8643\n12 HDSL run1 0.8525 0.8516\n13 Citius Ixa Imaxin run2 0.8525 0.8501\n14 ResIdent run3 0.8487 0.8466\n15 eire run1 0.8376 0.8316\n16 mitsls run3 0.8306 0.8299\n17 Uppsala run2 0.8252 0.8239\nTable 5: Results for all systems, data set A, closed track. Our system “clac” ranked 7th.\nGroup\n1 2 3 4 5\nGroup Code bs hr sr my id es-ar es-es es-mx pt-br pt-pt fr-ca fr-fr F1\n1\nbs 674 182 142 1 1 0.75\nhr 76 911 11 1 1 0.86\nsr 54 15 928 1 1 1 0.89\n2 my 992 8 0.99\nid 13 985 1 1 0.99\n3\nes-ar 927 58 15 0.83\nes-es 92 875 29 2 2 0.81\nes-mx 219 218 563 0.70\n4 pt-br 956 44 0.95\npt-pt 54 946 0.95\n5 fr-ca 972 28 0.93\nfr-fr 2 1 3 109 885 0.92\nTable 6: Confusion matrix for the n-gram of size 7, test Set A. We also add the F1 score in the last\ncolumn.\n6 Conclusion\nAlthough, it still achieved an accuracy of 78.46% with very little tuning and training set, we are disap-\npointed in the performance of the CSLTM. Based on the empirical study of (Zhang et al., 2015), character\nbased ConvNets performed in line with traditional methods with data sets in the hundreds of thousands,\nand better with data sets in the millions. Since the shared task data set size was in between, it was not\nclear which approach would perform best. We believe that a deep neural network can outperform the\ntraditional n-gram model for this task, but only once the data set size is dramatically increased and given\nmore time to experiment on the network parameters and structure. Since only raw texts are necessary,\ni.e. containing no linguistic annotations, increasing the data set does not constitute a problem.\nAs future work, we would like to explore once again the open task. With the addition of Mexican\nSpanish, France French and Canadian French, discriminating similar languages continues to be a chal-\nlenge. In Table 5 we see how the top 7 teams are within a 1% spread, but all below 90% accuracy. We\nbelieve that with a very large data set, a neural model could automatically learn key linguistic patterns to\ndifferentiate similar languages and possibly perform better than the current iteration of our CLSTM.\nGroup\n1 2 3 4 5\nGroup Code bs hr sr my id es-ar es-es es-mx pt-br pt-pt fr-ca fr-fr F1\n1\nbs 697 172 129 1 1 0.67\nhr 249 726 23 1 1 0.75\nsr 130 43 826 1 0.83\n2 my 909 91 0.94\nid 23 975 1 1 0.94\n3\nes-ar 2 816 87 93 2 0.71\nes-es 1 173 633 190 1 1 1 0.62\nes-mx 304 309 385 1 1 0.46\n4 pt-br 1 1 847 150 1 0.83\npt-pt 1 4 183 811 1 0.83\n5 fr-ca 972 28 0.90\nfr-fr 1 1 1 1 178 818 0.88\nTable 7: Confusion matrix for the CLSTM, test Set A. We also add the F1 score in the last column.\nAcknowledgement\nThe authors would like to thank the anonymous reviewers for their feedback on the paper. This work was\nﬁnancially supported by a grant from the Natural Sciences and Engineering Research Council of Canada\n(NSERC).\nReferences\n[Bahdanau et al.2015] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation\nby jointly learning to align and translate. In Advances in Neural Information Processing Systems (NIPS 2015),\npages 649–657, Montreal, Canada, December.\n[Baldwin and Lui2010] Timothy Baldwin and Marco Lui. 2010. Language identiﬁcation: The long and the short\nof the matter. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter\nof the Association for Computational Linguistics, HLT 2010, pages 229–237, May.\n[Cavnar and Trenkle1994] William B. Cavnar and John M. Trenkle. 1994. N-gram-based text categorization. In\nProceedings of the 3rd Annual Symposium on Document Analysis and Information Retrieval (SDAIR 1994) ,\npages 161–175, Las Vegas, Nevada, April.\n[Choi et al.2016] Keunwoo Choi, George Fazekas, Mark Sandler, and Kyunghyun Cho. 2016. Convolutional\nRecurrent Neural Networks for Music Classiﬁcation. arXiv preprint arXiv:1609.04243 – Submitted to the 42nd\nIEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2017).\n[Dafmashek1995] Marc Dafmashek. 1995. Gauging similarity with n-grams: Language-independent categoriza-\ntion of text. Science, 267(5199):843–848.\n[Dunning1994] Ted Dunning. 1994. Statistical identiﬁcation of language. Technical report, MCCS 940-273,\nComputing Research Laboratory, New Mexico State University.\n[Goutte and Leger2015] Cyril Goutte and Serge Leger. 2015. Experiments in discriminating similar languages.\nIn Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and\nDialects (LT4VarDial), pages 78–84, Hissar, Bulgaria, September.\n[Goutte et al.2016] Cyril Goutte, Serge L ´eger, Shervin Malmasi, and Marcos Zampieri. 2016. Discriminating\nsimilar languages: Evaluations and explorations. In Proceedings of the Tenth International Conference on\nLanguage Resources and Evaluation (LREC 2016), Portoroz, Slovenia, May.\n[Jurafsky and Martin2014] Dan Jurafsky and James H. Martin. 2014. Speech and Language Processing. Pearson\ncustom library. Prentice Hall, Pearson Education International.\n[Karpathy and Fei-Fei2015] Andrej Karpathy and Li Fei-Fei. 2015. Deep visual-semantic alignments for generat-\ning image descriptions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\npages 3128–3137, Boston, MA, USA, June.\n[Kim et al.2016] Yoon Kim, Yacine Jernite, David Sontag, and Alexander M Rush. 2016. Character-aware neural\nlanguage models. In Proceedings of the Thirtieth AAAI Conference on Artiﬁcial Intelligence (AAAI 2016), pages\n2741–2749, Phoenix, Arizona, USA, February.\n[Kingma and Ba2015] Diederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization.\nIn Proceeding of the 2015 International Conference on Learning Representation (ICLR 2015) , San Diego,\nCalifornia.\n[Kruengkrai et al.2005] Canasai Kruengkrai, Prapass Srichaivattana, Virach Sornlertlamvanich, and Hitoshi Isa-\nhara. 2005. Language identiﬁcation based on string kernels. In Proceedings of the 5th International Symposium\non Communications and Information Technologies (ISCIT 2005), pages 896–899.\n[Malmasi and Dras2015] Shervin Malmasi and Mark Dras. 2015. Language identiﬁcation using classiﬁer ensem-\nbles. In Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties\nand Dialects (LT4VarDial), pages 35–43, Hissar, Bulgaria, September.\n[Malmasi et al.2016] Shervin Malmasi, Marcos Zampieri, Nikola Ljube ˇsi´c, Preslav Nakov, Ahmed Ali, and J ¨org\nTiedemann. 2016. Discriminating between Similar Languages and Arabic Dialect Identiﬁcation: A Report on\nthe Third DSL Shared Task. In Proceedings of the 3rd Workshop on Language Technology for Closely Related\nLanguages, Varieties and Dialects (VarDial), Osaka, Japan, December.\n[Mikolov et al.2013] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed\nrepresentations of words and phrases and their compositionality. In Advances in neural information processing\nsystems (NIPS 2013), pages 3111–3119, Lake Tahoe, USA, December.\n[Sim˜oes et al.2014] Alberto Sim ˜oes, Jos ´e Jo ˜ao Almeida, and Simon D Byers. 2014. Language identiﬁcation: a\nneural network approach. In 3rd Symposium on Languages, Applications and Technologies (SLATE 2014) ,\npages 251–265. Schloss Dagstuhl-Leibniz-Zentrum f¨ur Informatik GmbH.\n[Tan et al.2014] Liling Tan, Marcos Zampieri, Nikola Ljube ˇsic, and J ¨org Tiedemann. 2014. Merging Comparable\nData Sources for the Discrimination of Similar Languages: The DSL Corpus Collection. In Proceedings of the\n7th Workshop on Building and Using Comparable Corpora (BUCC), pages 11–15, Reykjavik, Iceland.\n[Zampieri et al.2015] Marcos Zampieri, Liling Tan, Nikola Ljube ˇsic, J ¨org Tiedemann, and Preslav Nakov. 2015.\nOverview of the DSL shared task 2015. In Proceedings of the Joint Workshop on Language Technology for\nClosely Related Languages, Varieties and Dialects (LT4VarDial), pages 1–9.\n[Zhang et al.2015] Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for\ntext classiﬁcation. In Proceedings of the 29th Annual Conference on Neural Information Processing Systems\n(NIPS 2015), pages 649–657, Montreal, Canada, December.\n[Zubiaga et al.2014] Arkaitz Zubiaga, I ˜naki San Vicente, Pablo Gamallo, Jos ´e Ram´on Pichel, I ˜naki Alegria, Nora\nAranberri, Aitzol Ezeiza, and V ´ıctor Fresno. 2014. Overview of TweetLID: Tweet Language Identiﬁcation\nat SEPLN 2014. In Twitter Language Identiﬁcation Workshop at SEPLN 2014 , pages 1–11, Girona, Spain,\nSeptember.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7412412166595459
    },
    {
      "name": "Task (project management)",
      "score": 0.7407611608505249
    },
    {
      "name": "Character (mathematics)",
      "score": 0.7147336602210999
    },
    {
      "name": "n-gram",
      "score": 0.6712186932563782
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.6467567682266235
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.61567223072052
    },
    {
      "name": "Artificial neural network",
      "score": 0.6015857458114624
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5857092142105103
    },
    {
      "name": "Digital subscriber line",
      "score": 0.5832918882369995
    },
    {
      "name": "Natural language processing",
      "score": 0.5390580892562866
    },
    {
      "name": "Convolutional neural network",
      "score": 0.4965396523475647
    },
    {
      "name": "Convolution (computer science)",
      "score": 0.4901326596736908
    },
    {
      "name": "Language model",
      "score": 0.2708665728569031
    },
    {
      "name": "Programming language",
      "score": 0.14767926931381226
    },
    {
      "name": "Mathematics",
      "score": 0.11337500810623169
    },
    {
      "name": "Engineering",
      "score": 0.06201699376106262
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "cited_by": 3
}