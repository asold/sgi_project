{
    "title": "Transformer-CNN: Swiss knife for QSAR modeling and interpretation",
    "url": "https://openalex.org/W3030978062",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2735527448",
            "name": "Pavel Karpov",
            "affiliations": [
                "Klinikum Ingolstadt",
                "Helmholtz Zentrum München"
            ]
        },
        {
            "id": "https://openalex.org/A2114590262",
            "name": "Guillaume Godin",
            "affiliations": [
                "Firmenich (Switzerland)"
            ]
        },
        {
            "id": "https://openalex.org/A141730740",
            "name": "Igor V Tetko",
            "affiliations": [
                "Helmholtz Zentrum München",
                "Klinikum Ingolstadt"
            ]
        },
        {
            "id": "https://openalex.org/A2735527448",
            "name": "Pavel Karpov",
            "affiliations": [
                "Institute of Groundwater Ecology",
                "Helmholtz Zentrum München"
            ]
        },
        {
            "id": "https://openalex.org/A2114590262",
            "name": "Guillaume Godin",
            "affiliations": [
                "Firmenich (Switzerland)"
            ]
        },
        {
            "id": "https://openalex.org/A141730740",
            "name": "Igor V Tetko",
            "affiliations": [
                "Helmholtz Zentrum München",
                "Institute of Groundwater Ecology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6600020652",
        "https://openalex.org/W1990399577",
        "https://openalex.org/W417995331",
        "https://openalex.org/W2081320283",
        "https://openalex.org/W2436108096",
        "https://openalex.org/W2735246657",
        "https://openalex.org/W2529996553",
        "https://openalex.org/W6628094813",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W2169678694",
        "https://openalex.org/W2008381136",
        "https://openalex.org/W2914757825",
        "https://openalex.org/W2972498877",
        "https://openalex.org/W2145056192",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2100495367",
        "https://openalex.org/W2087563523",
        "https://openalex.org/W2901476322",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2947423323",
        "https://openalex.org/W2972608805",
        "https://openalex.org/W2972441196",
        "https://openalex.org/W2973136764",
        "https://openalex.org/W2070955875",
        "https://openalex.org/W2096541451",
        "https://openalex.org/W2578240541",
        "https://openalex.org/W2765224015",
        "https://openalex.org/W2916877561",
        "https://openalex.org/W1972234779",
        "https://openalex.org/W2594183968",
        "https://openalex.org/W1977224352",
        "https://openalex.org/W2042278642",
        "https://openalex.org/W2094836090",
        "https://openalex.org/W1968756812",
        "https://openalex.org/W2165495560",
        "https://openalex.org/W1965216736",
        "https://openalex.org/W1966441763",
        "https://openalex.org/W2342596260",
        "https://openalex.org/W2136605707",
        "https://openalex.org/W2153635508",
        "https://openalex.org/W2911964244",
        "https://openalex.org/W2295598076",
        "https://openalex.org/W1504991194",
        "https://openalex.org/W2907473220",
        "https://openalex.org/W2964045325",
        "https://openalex.org/W2527197703",
        "https://openalex.org/W2982978720",
        "https://openalex.org/W2013894207",
        "https://openalex.org/W2606780347",
        "https://openalex.org/W2898764670",
        "https://openalex.org/W3127557076",
        "https://openalex.org/W2951784549",
        "https://openalex.org/W2953384591",
        "https://openalex.org/W2964113829",
        "https://openalex.org/W3103092523",
        "https://openalex.org/W2034562813",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2241011319",
        "https://openalex.org/W4300945981",
        "https://openalex.org/W3098269892",
        "https://openalex.org/W3102476541"
    ],
    "abstract": null,
    "full_text": "Karpov et al. J Cheminform           (2020) 12:17  \nhttps://doi.org/10.1186/s13321-020-00423-w\nRESEARCH ARTICLE\nTransformer-CNN: Swiss knife for QSAR \nmodeling and interpretation\nPavel Karpov1,2*, Guillaume Godin3 and Igor V. Tetko1,2\nAbstract \nWe present SMILES-embeddings derived from the internal encoder state of a Transformer [1] model trained to canonize \nSMILES as a Seq2Seq problem. Using a CharNN [2] architecture upon the embeddings results in higher quality inter-\npretable QSAR/QSPR models on diverse benchmark datasets including regression and classification tasks. The proposed \nTransformer-CNN method uses SMILES augmentation for training and inference, and thus the prognosis is based on an \ninternal consensus. That both the augmentation and transfer learning are based on embeddings allows the method \nto provide good results for small datasets. We discuss the reasons for such effectiveness and draft future directions for \nthe development of the method. The source code and the embeddings needed to train a QSAR model are available on \nhttps ://githu b.com/bigch em/trans forme r-cnn. The repository also has a standalone program for QSAR prognosis which \ncalculates individual atoms contributions, thus interpreting the model’s result. OCHEM [3] environment (https ://ochem \n.eu) hosts the on-line implementation of the method proposed.\nKeywords: Transformer model, Convolutional neural neural networks, Augmentation, QSAR, SMILES, Embeddings, \nCharacter-based models, Cheminformatics, Regression, Classification\n© The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and \nthe source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material \nin this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material \nis not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the \npermitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creat iveco \nmmons .org/licen ses/by/4.0/. The Creative Commons Public Domain Dedication waiver (http://creat iveco mmons .org/publi cdoma in/\nzero/1.0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nIntroduction\nQuantitative Structure–Activity (Property) Relationship \n(QSAR/QSPR) approaches find a nonlinear function, often \nmodelled as an artificial neural network (ANN), that esti\n-\nmates the activity/property based on a chemical structure. \nIn the past, most QSAR works heavily relied on descrip\n-\ntors [4] that represent in a numerical way some features \nof a complex graph structure of a compound. Amongst \nnumerous families of descriptors, the fragment descriptors \nthat count occurrences of a subgraph in a molecule graph, \nhold a distinctive status due to simplicity in the calcula\n-\ntion. Moreover, there is a theoretical proof that one can \nsuccessfully build any QSAR model with them [5]. Even a \nsmall database of compounds contains thousands of frag\n-\nmental descriptors and some feature selection algorithm \nhas traditionally been used to find a proper subset of \ndescriptors for better quality, and to speed up the whole \nmodeling process. Thus, feature selection in conjunction \nwith a suitable machine learning method was key to suc\n-\ncess [6]. The rise of deep learning [7 ] allows us to bypass \ntiresome expert and domain-wise feature construction by \ndelegating this task to a neural network that can extract \nthe most valuable traits of the raw input data required for \nmodeling the problem at hand [8, 9].\nIn this setting, the whole molecule as a SMILES-strings \n[10, 11] (Simplified Molecular Input Line Entry Sys\n-\ntem) or a graph [12, 13] serves as the input to the neu -\nral network. SMILES notation allows for the writing of \nany complex formula of an organic compound in a string \nfacilitating storage and retrieval information about mol\n-\necules in databases [14]. It contains all information about \nthe compound sufficient to derive the entire configuration \n(3D-structure) and has a direct connection to the nature \nof fragmental descriptors, Fig.  1, thus, making SMILES \none of the best representation for QSAR studies.\nOpen Access\nJournal of Cheminformatics\n*Correspondence:  pavel.karpov@helmholtz-muenchen.de\n1 Institute of Structural Biology, Helmholtz Zentrum München-Research \nCenter for Environmental Health (GmbH), Ingolstädter Landstraße 1, \n85764 Neuherberg, Germany\nFull list of author information is available at the end of the article\nPage 2 of 12Karpov et al. J Cheminform           (2020) 12:17 \nOne of the first works exploiting direct SMILES input \nas descriptors used fragmentation of strings into groups \nof overlapping substrings forming a SMILES-like set or a \nhologram of a molecule [16]. Within this approach, there \nwas no need to derive a 2D/3D configuration of the mol\n-\necule with subsequent calculation of descriptors keeping \nthe quality of the models at the same level as with classical \ndescriptors or even better.\nSMILES strings are sequences of characters; therefore, \nthey can be analyzed by machine-learning methods suit\n-\nable for text processing, namely with convolutional and \nrecurrent neural networks. After the demonstration of \ntext understanding from character-level inputs [17], this \ntechnique was adopted in chemoinformatics [11, 18–21]. \nRecently, we showed that the augmentation of SMILES \n(using canonical as well as non-canonical SMILES during \nmodel training and inference) increases the performance \nof convolutional models for regression and classification \ntasks [22].\nTechnically modern machine-learning models consist \nof two parts working together. The first part encodes the \ninput data and extracts the most robust features by apply\n-\ning convolutional filters with different receptive fields \n(RF) or recurrent layers, whereas the second part directly \nbuilds the regular model based on these features using \nstandard dense layers as building blocks (so called classi\n-\ncal “MLP”), Fig.  2. Though powerful convolutional layers \ncan effectively encode the input within its internal repre -\nsentation, usually one needs a considerable training data -\nset and computational resources to train the encoder part \nof a network.\nThe concept of embeddings mitigates the problem by \nusing the pre-trained weights designed for image [23] \nor text processing [24] tasks. It allows transfer learning \nfrom previous data and speeds up the training process \nfor building models with significantly smaller datasets \ninaccessible for training from scratch. Typically, QSAR \ndatasets contain only several hundreds of molecules, and \nSMILES-embeddings could improve models by develop\n-\ning better features.\nOne way of separately obtaining SMILES embeddings \nis to use classical autoencoder [25] approach where the \ninput is the same as the output. In the case of SMILES, \nhowever, it would be more desirable to explore a vari\n-\nety of SMILES belonging to the same molecule due to \nredundant SMILES grammar, Fig.  1. We hypothesized \nthat it is possible to train a neural network to conduct a \nSMILES canonicalization task in a Sequence-to-Sequence \n(Seq2Seq) manner like a machine translation problem, \nwhere on the left side are non-canonical SMILES, and on \nthe right side are their canonical equivalents. Recently, \nSeq2Seq was successfully applied to translation from \nInChi [26] codes to SMILES (Inchi2Sml) as well as from \nSMILES arbitrary to canonical SMILES (Sml2canSml), \nand to build QSAR models on extracted latent variables \n[27].\nThe state-of-the-art neural architecture for machine \ntranslation consists of stacked Long Short-Term Memory \n(LSTM) cells [28]. The training process for such networks \ninherently has all kinds of Recurrent Neural Networks \ndifficulties, e.g., vanishing gradients, and the impossibil\n-\nity of parallelization. Recently, a Transformer model [1 ] \nwas proposed where all recurrent units are replaced with \nconvolutional and element-wise feed-forward layers. The \nwhole architecture shows a significant speed-up dur\n-\ning training and inference with improved accuracy over \nFig. 1 Benzylpenicillin canonical SMILES at the top, 2D and 3D structures derived from SMILES with OpenBabel [15] in the middle, and three \nnon-canonical SMILES examples at the bottom. A substructure of the phenyl ring is written in bold font\nPage 3 of 12\nKarpov et al. J Cheminform           (2020) 12:17 \n \ntranslation benchmarks. The Transformer model was \napplied for prediction of reaction outcomes [29] and for \nretrosynthesis [30].\nModern machine learning architectures although dem\n-\nonstrating incredible performance still lack interpretabil -\nity. Explaining the reasons for a particular prediction of a \nmodel avoids “Clever Hans” predictors with spurious or \nnon-relevant correlations [31] and foster trust and veri\n-\nfiability. One of the promising methods to open a “black \nbox” uses the Layer-wise Relevance Propagation (LRP) \nalgorithm [32], which splits the overall predicted value \nto a sum of contributions of individual neurons. In this \nmethod, the sum of relevance of all neurons of a layer, \nincluding the bias neuron, is kept constant. Propagation of \nthe relevance from the last layer to the input layer allows \nthe evaluation of the contributions of particular input fea\n-\ntures in to select the most relevant features for the whole \ntraining set [33] or to explain the individual neural net\n-\nwork prediction [32]. We apply the LRP method for an \nexplanation of individual results, checking the model get \nresults for the right reason.\nOur contributions in the article are as follows:\nPresenting a concept of dynamic SMILES embed\n-\ndings that may be useful for a wide range of chemin -\nformatics tasks;\nScrutinizing CharNN models based on these embed\n-\ndings for regression and classification tasks and show \nthat the method outperforms the state-of-the-art \nmodels;\nInterpretation of the model based on LRP method;\nOur implementation as well as source codes and \nSMILES-embeddings are available on https  ://githu \nb.com/bigch em/trans forme r-cnn. We also provide \nready-to-use implementation on https ://ochem .eu \nwithin the OCHEM [3 ] environment and a stan\n-\ndalone program for calculating properties and \nexplaining the results.\nFig. 2 Scheme of modern QSAR models based on ANN. The encoder part (left) extracts main features of the input data by means of RNN (top) \nor convolutional layers (bottom). Then the feature vector as usual descriptors feeds to the dense layer part consisting of residual and highway \nconnections, normalization layers, and dropouts\nPage 4 of 12Karpov et al. J Cheminform           (2020) 12:17 \nMethods\nSMILES canonicalization model\nDataset\nTo train the ANN to perform SMILES canonicalization, \nwe used the ChEMBL database [34] with SMILES strings \nof length less than or equal 110 characters (> 93% of the \nentire database). The original dataset was augmented \n10 times up to 17,657,995 canonicalization pairs writ\n-\nten in reactions format separated by ‘ >> ’ . Each pair con-\ntained on the left side a non-canonical, and on the right \nside—a canonical SMILES for the same molecule. Such \nan arrangement of the training dataset allowed us to re-\nuse the previous Transformer code, which was originally \napplied for retrosynthetic tasks [30]. For completeness, we \nadded for every compound a line where both left and right \nsides were identical, i.e. canonical SMILES, Fig.  3. Thus \neach molecule was present in the training set 11 times. \nIf a molecule had tautomeric forms then all of them were \naccounted for as separate entries in the training data file.\nModel input\nSeq2Seq models use one-hot encoding vector for the \ninput. Its values are zero everywhere except the position \nof the current token which is set to one. Many works on \nSMILES use tokenization procedure [35, 36] that com\n-\nbines some characters, for example ‘B’ and ‘r’ to one token \n‘Br’ . Other rules for handling most common two-letters \nelements, charges, and stereochemistry also are used for \npreparing the input for the neural network. According \nto our experience, the use of more complicated schemes \ninstead of simple character-level tokenization did not \nincrease the accuracy of models [30]. Therefore a sim\n-\nple character-level tokenization was used in this study. \nThe vocabulary of our model consisted of all possible \ncharacters from ChEMBL dataset and has 66 symbols: \n \n^#%()+–./0123456789= @ABCDEFGHIKLMNOPRST -\nVXYZ[\\]abcdefgilmnoprstuy$\nThus, the model could handle the entire diversity of \ndrug-like compounds including stereochemistry, differ -\nent charges, and inorganic ions. Two special characters \nwere added to the vocabulary: ‘^’ to indicate the start of \nthe sequence, and ‘$’ to inform the model of the end of \ndata input.\nTransformer model\nThe canonicalization model used in this work was based \nupon a Transformer architecture consisting of two sepa\n-\nrate stacks of layers for the encoder and the decoder, \nrespectively. Each layer incorporated some portion \nof knowledge written in its internal memory (V) with \nindexed access by keys (K). When new data arrived (Q), \nthe layer calculated attention and modified the input \naccordingly (see the original work on Transformers [1 ]), \nthus, forming the output of the self-attention layer and \nweighting those parts that carry the essential information. \nBesides a self-attention mechanism, the layer also con\n-\ntained several position-wise dense layers, a normalization \nlayer, and residual connections [1, 37]. Our model utilized \na three layer architecture of Transformer with 10 blocks \nof self-attention, i.e. the same one as used in our previ\n-\nous study [30]. After the encoding process was finished, \nthe output of the top encoder layer contained a represen\n-\ntation of a molecule suitable for decoding into canonical \nSMILES. In this study we used this representation as a \nwell-prepared latent representation for QSAR modeling.\nTensorflow v1.12.02 [38] was used as machine-learn\n-\ning framework to develop all parts of the Transformer, \nwhereas RDKit v.2018.09.2 [39] was used for SMILES \ncanonicalization and augmentation.\nQSAR model\nWe call the output of the Transformer’s encoder part \na dynamic SMILES-embedding, Fig.  4. For a molecule \nwith N-characters, the encoder produces the matrix with \ndimensions (N, EMBEDDINGS). Though technically this \nFig. 3 Example of the data in the training file for canonicalization model of a small molecule CHEMBL351484. Every line contains a pair of \nnon-canonical (left) and canonical (right) separated by “ >> ” . One line has identical SMILES on both sides, stressed with the red box\nPage 5 of 12\nKarpov et al. J Cheminform           (2020) 12:17 \n \nmatrix is not an embedding because equivalent char -\nacters have different values depending on position and \nsurroundings, it can be considered so due to its role: to \nconvert an input one-hot raw vectors to real-value vec\n-\ntors in some latent space. Because these embeddings have \nvariable lengths, we used a series of 1D convolutional fil\n-\nters as implemented in DeepChem [40] TextCNN method \n(https ://githu b.com/deepc hem).\nEach convolution had a kernel size from the list (1, 2, 3, \n4, 5, 6, 7, 8, 9, 10, 15, 20) and produced the following num\n-\nber of filters (100, 200, 200, 200, 200, 100, 100, 100, 100, \n100, 160, 160), respectively. After a GlobalMaxPool opera\n-\ntion and the subsequent concatenation of the pooling \nresults, the data went through Dropout [41] (rate = 0.25), \nDense (N = 512), Highway [42] layers, and, finally, con\n-\nverted to the output layer which consisted of only one \nneuron for regression and two neurons for classification \ntasks. The weights of the Transformer’s part were frozen \nin all experiments. All models used the Adam optimizer \nwith Mean Squared Error or Binary Cross-Entropy loss \ndepending on the problem at hand. A fixed learning rate \nλ = 10\n–4 was used. Early-stopping was used to prevent \noverfitting, to select a best model, and to reduce train -\ning time. OCHEM calculations were performed using \ncanonical SMILES as well as ten-fold augmented SMILES \nduring both training and prognosis. This number of \nSMILES augmentations was found to be an optimal one in \nour previous study [43]. An average value of the individual \npredictions for different representation of the same mol\n-\necule was used as the final model prediction to calculate \nstatistical parameters.\nThe same five-fold cross-validation procedure was used \nto compare the models with the results of our previous \nstudy [43]. The coefficients of determination [44]\nwhere  SS\ntot is total variance of data and  SSres is resid -\nual unexplained variance of data was used to compare \nregression models and Area Under the Curve (AUC) was \nused for classification tasks.\nValidation datasets\nWe used the same datasets (9 for regression and 9 for clas-\nsification) that were exploited in our previous studies [11, \n22]. Short information about these sets as well as links to \noriginal works are provided in Table  1. The datasets are \n(1)r2 = 1 − SS res/SS tot\nFig. 4 The architecture of the Transformer-CNN network\nTable 1 Descriptions of datasets used in the work\nCode Description Size Code Description Size\nRegression tasks Classification tasks\nMP Melting point [45] 19,104 HIV Inhibition of HIV replication [46] 41,127\nBP Boiling point [47] 11,893 AMES Mutagenicity [48] 6542\nBCF Bioconcentration factor [47] 378 BACE Human β-secretase 1 (BACE-1) inhibitors [46] 1513\nFreeSolv Free solvation energy [46] 642 Clintox Clinical trial toxicity [46] 1478\nLogS Solubility [49] 1311 Tox21 In-vitro toxicity [46] 7831\nLipo Lipophilicity [50] 4200 BBBP Blood–brain barrier [46] 2,039\nBACE IC50 of human β-secretase 1 (BACE-1) \ninhibitors [46]\n1513 JAK3 Janus kinase 3 inhibitor [51] 886\nDHFR Dihydrofolate reductase inhibition [52] 739 BioDeg Biodegradability [53] 1737\nLEL Lowest effect level [54] 483 RP AR Endocrine disruptors [55] 930\nPage 6 of 12Karpov et al. J Cheminform           (2020) 12:17 \navailable on the OCHEM environment on https  ://ochem \n.eu.\nResults and discussion\nSMILES canonicalization model\nThe Transformer model was trained for 10 epochs with \nthe learning rate changing according to the formula:\nwhere factor = 20, warmup = 16,000 steps, and if λ < 10–4 \nthen λ = 10−4. The settings for the learning rate were simi-\nlar to those used in our retro-synthesis study. Each epoch \ncontained 275,907 steps (batches). No early-stopping or \nweight-averaging was applied. Learning curves are shown \nin Fig. 5.\nTo validate the model, we sampled 500,000 ChEMBL-\nlike SMILES (only 8,617 (1.7%) of them were canonical) \nfrom a generator [56] and checked how accurately the \nmodel can restore canonical SMILES for these molecules. \nWe intentionally selected the generated SMILES keeping \nin mind possible applications of the proposed method \nin artificial intelligence-driven pipelines of de-novo drug \ndevelopment. The model correctly canonicalized 83.6% of \nall samples, Table 2.\nQSAR modeling\nFor the QSAR modelling the saved embedding was used. \nThe training was done using a fixed learning rate λ = 0.001 \nfor n = 100 epochs. Early stopping with 10% randomly \nselected SMILES was used to identify the optimal model. \nTable 3, Fig.  6 compares results for regression datasets \n(2)\n/afii9838= factor∗min (1.0, step/warmup )/max (step,warmup )\nwhile Table  4, Fig.  7 compares classification tasks. The \nstandard mean errors of the values were calculated using \na bootstrap procedure as explained elsewhere [53].   \nWith an exception of a few datasets, the proposed \nmethod provided similar or better results than those cal\n-\nculated using descriptor-based approaches as well as the \nother SMILES-based approaches investigated in our pre\n-\nvious study [43]. The data augmentation was critically \nimportant for the Transformer-CNN method to achieve \nits high performance. We used augmentation n = 10, i.e., \n10 SMILES were randomly generated and used for model \ndevelopment and application, which was found optimal in \nthe aforementioned previous study.\nSimilar to Transformer-CNN the Sml2canSml used an \ninternal representation, which was developed by mapping \narbitrary SMILES to canonical SMILES. The difference \nwas that Sml2canSml generated a fixed set of 512 latent \nvariables (CDDD descriptors), while the Transformer-\nCNN representation had about the same length as the \ninitial SMILES. Sml2canSml CDDD could be used as \ndescriptors for any traditional machine learning methods \nwhile Transformer-CNN required convolutional neural \nnetworks to process the variable length output and to cor\n-\nrelate it with the analysed properties. Sml2canSml was \nadded as CDDD descriptors to OCHEM. These descrip\n-\ntors were analysed by the same methods as used in the \nprevious work, i.e., LibSVM [57], Random Forest [58], \nXGBoost [59] as well as by Associative Neural Networks \n(ASNN) [60] and Deep Neural Networks [61]. Exactly \nthe same protocol, fivefold cross-validation, was used for \nall calculations. The best performance using the CDDD \ndescriptors was obtained by ASNN and LibSVM meth\n-\nods, which contributed models with the highest accuracy \nfor seven and five datasets respectively (LibSVM method \nprovided the best performance in the original study). \nTransformer-CNN provided better or similar results \ncompared to the CDDD descriptors for all datasets with \nan exception of Lipo and FreeSolv. It should be also men\n-\ntioned that CDDD descriptors could only process mole -\ncules which satisfy the following conditions:\nlogP ∈ (−5,7) and\nmol_weight ∈ (12,600) and\nnum_heavy_atoms ∈ (3, 50) and\nmolecule is organic.\nFig. 5 Learning curves: 1) learning rate schedule (axes bottom and \nright), and 2) character-based accuracy (axes bottom and left) on the \ntraining dataset for the first four epochs\nTable 2 Validation of canonicalization model\nStrings All Correctly canonicalized\nAll 500,000 418,233 (83.6%)\nStereo (with @) 77,472 28,821 (37.2%)\nCis/trans (with / or \\) 54,727 40,483 (73.9%)\nPage 7 of 12\nKarpov et al. J Cheminform           (2020) 12:17 \n \nTable 3 Coefficient of determination,  r2, calculated for regression sets (higher values are better)\nWe omitted the standard mean errors, which are 0.01 or less, for the reported values\na Results from our previous study [22]. bBest performance calculated with CDDD descriptors obtained using autoencoder Sml2canSml from [27]\nDataset Descriptor based \n methods2\nSMILES based \n(augm = 10)\na\nTransformer-CNN, \nno augm\nTransformer-CNN, \naugm = 10\nCDDD  descriptors\nb\nMP 0.83 0.85 0.83 0.86 0.85\nBP 0.98 0.98 0.97 0.98 0.98\nBCF 0.85 0.85 0.71 ± 0.02 0.85 0.81\nFreeSolv 0.94 0.93 0.72 ± 0.02 0.91 0.93\nLogS 0.92 0.92 0.85 0.91 0.91\nLipo 0.7 0.72 0.6 0.73 0.74\nBACE 0.73 0.72 0.66 0.76 0.75\nDHFR 0.62 ± 0.03 0.63 ± 0.03 0.46 ± 0.03 0.67 ± 0.03 0.61 ± 0.03\nLEL 0.19 ± 0.04 0.25 ± 0.03 0.2 ± 0.03 0.27 ± 0.04 0.23 ± 0.04\nFig. 6 Coefficient of determination,  r2, calculated for regression sets (higher values are better)\nTable 4 AUC calculated for classification sets (higher values are better)\nWe omitted the standard mean errors, which are 0.01 or less, for the reported values\na Results from our previous study [22]. bBest performance calculated with CDDD descriptors obtained using Sml2canSml autoencoder from [27]\nDataset Descriptor based \n methods\na\nSMILES based \n(augm = 10)\n2\nTransformer-CNN, \nno augm\nTransformer-CNN, \naugm = 10\nCDDD  descriptors\nb\nHIV 0.82 0.78 0.81 0.83 0.74\nAMES 0.86 0.88 0.86 0.89 0.86\nBACE 0.88 0.89 0.89 0.91 0.9\nClintox 0.77 ± 0.03 0.76 ± 0.03 0.71 ± 0.02 0.77 ± 0.02 0.73 ± 0.02\nTox21 0.79 0.83 0.81 0.82 0.82\nBBBP 0.90 0.91 0.9 0.92 0.89\nJAK3 0.79 ± 0.02 0.8 ± 0.02 0.70 ± 0.02 0.78 ± 0.02 0.76 ± 0.02\nBioDeg 0.92 0.93 0.91 0.93 0.92\nRP AR 0.85 0.87 0.83 0.87 0.86\nPage 8 of 12Karpov et al. J Cheminform           (2020) 12:17 \nThese limitations appeared due to the preparation of \nthe training set to develop the Sml2canSml encoder. The \nlimitations resulted in the exclusion of a number of mole-\ncules, which failed one or several of the above conditions. \nContrary to the Sml2canSml encoder, we trained Trans\n-\nformer-CNN with very diverse molecules from ChEMBL \nand thus the developed models could be applied to any \nmolecule which can be processed by RDKiT. The exclu\n-\nsion of molecules for which CDDD descriptors failed to \nbe calculated did not significantly change the results of \nTransformer models: some models improved while oth\n-\ners decreased their accuracy for ~ 0.01 respective perfor -\nmance values. For example, for Lipo and FreeSolv sets \nthe accuracy of the Transformer-CNN model increased \nto  r\n2 = 0.92 and 0.75 respectively, while for BBB the AUC \ndecreased to 0.91.\nInterpretability of the model\nLayer-wise relevance propagation was used to interpret \nthe models. For gated connections (in HighWay block) we \nimplemented the signal-take-all redistribution rule [62] \nwhile all other Dense and Convolutional layers were well \nfitted in the LRP framework [32] without any adaptation. \nIn this work, we stopped the relevance propagation on the \noutput of the Transformer’s encoder which is position-\nwise. It should be noted that we froze the encoder part of \nthe network during QSAR model training. Summing up \nall the individual features for each position in the SMILES \nstring calculated its contribution to the final result. If the \nLRP indicated a reasonable explanation of the contribu\n-\ntions of fragments then one can trust that the model \nmade predictions based on detected fundamental struc\n-\nture–property relationships. For explanation we selected \nclassification (AMES mutagenicity) and regression (water \nsolubility) models.\nAMES mutagenicity\nThe AMES test is a widely used qualitative test to deter -\nmine the mutagenic potential of a molecule, from which \nextensive structural alerts collections were derived [63]. \nExamples of these alerts are aromatic nitros, N-oxides, \naldehydes, monohaloalkenes, quinones, etc. A QSAR \nmodel for AMES had to pay special attention to these and \nsimilar groups to be interpretable and reliable. The Trans\n-\nformer-CNN model built on 6542 endpoints (3516 muta-\ngenic and 3026 nonmutagenic) results in AUC = 0.89, \nTable 4.\nThe structure of 1-Bromo-4-nitrobenzene gave the posi-\ntive AMES test. The output of the LRP procedure for one \nof possible SMILES for this compound, namely 1c([N +]\n([O-]) = O)ccc(c1)Br, is shown in Table 5.\nAccording to the LRP , the relevance was constant dur\n-\ning the propagation:\nHere (L) stood for a set of neurons in the last layer, \n(L−1)—in the layer before the last layer, and so on. Each \nlayer in the Transformer-CNN network contained biases \n(B), and thus some relevance dissipated on them. There\n-\nfore the above equation was corrected to:\n(3)\ny = R = f(x) =\n∑\nl∈(L)R l =\n∑\nl∈(L−1 )R l\n=\n∑\nl∈(L −2) R l =\n∑\nl∈(1)R l.\n(4)\n∑\nl∈(L) Rl =\n∑\nl∈(L−1 )Rl + B.\nFig.7 AUC calculated for classification sets (higher values are better)\nPage 9 of 12\nKarpov et al. J Cheminform           (2020) 12:17 \n \nWe calculated how much of the relevance was taken \nby biases and reported these values in the output of the \nochem.py script. Table  5 clearly shows that 24.6% of the \noutput signal was taken by biases and 75.4% were success-\nfully propagated to position-wise layers, which we used to \ninterpret the model. If less than 50% of the signal came to \nthe input, it may indicate an applicability domain problem \nor technical issues with relevance propagation. In these \ncases the interpretation could be questioned.\nIterating through all non-hydrogen atoms, the interpre\n-\ntation algorithm picked up an atom and drew a SMILES \nfrom it. Thus, every molecule had a corresponding set \nof SMILES equal to the number of atoms. The LRP was \nused for every SMILES, and then the individual predic\n-\ntions were averaged for the final output. 1-Bromo-4-ni -\ntrobenzene was predicted as mutagenic with the score \n0.88. Impacts of the atoms on the property is depicted in \nFig. 8. The model predicted this compound as mutagenic \nbecause of the presence of nitro and halogen benzene \nmoieties. Both are known to be structural alerts for muta\n-\ngenicity [63]. Charged oxygen provided a bigger impact \nthan the double bonded one in the nitro group because \nits presence contributed to the mutagenicity for nitro and \nalso for N-oxide compounds.\nAqueous solubility\nSolubility is a crucial property in drug-development. To \nhave a fast, robust, and explainable tool for its prediction \nand interpretation is highly desirable by both academia \nand industry. The Transformer-CNN model built on 1311 \ncompounds had the following statistics: q\n2 = 0.92 and \nRMSEp = 0.57 [64]. For demonstration of its interpretabil-\nity we choose haloperidol—a well-known antipsychotic \ndrug with 14 mg/l water solubility [65].\nThe Transformer model calculated the same solubil\n-\nity 14 ± 2  mg/L for this compound. The individual atom \ncontributions are shown in Fig. 9. Hydroxyl, carbonyl, ali-\nphatic nitrogen, and halogens contributed mostly to the \nsolubility. These groups can form ionizable zones in the \nmolecule thus helping water to dissolve the substance. \nTable 5 Local relevance conservation for c1c([N +]([O−]) = O)ccc(Br)c1\na All 0 values were all less than  10− 5\nLayer Relevance, R (L + 1) Relevance, R (L) Delta, R (L + 1)-R (L) Bias, Delta / R \n(L + 1) *100%\nResult 0.98119 – – –\nHighWay Output 0.98119 0.9300 0.0512 5.21\nHighWay Input 0.9300 0.7227 0.2073 22.3\nDeMaxPool 0.7227 0.7371  − 0.0144  − 1.98\nConv1 0.0090 0.0117  − 0.00271  − 30.1\nConv2 0.1627 0.1627 0a 0\nConv3 − 0.0443  − 0.0443 0 0\nConv4 0.0191 0.0191 0 0\nConv5 − 0.0984  − 0.0984 0 0\nConv6 − 0.0136  − 0.0136 0 0\nConv7 0.0806 0.0806 0 0\nConv8 0.0957 0.0957 0 0\nConv9 0.1528 0.1528 0 0\nConv10 0.0845 0.0845 0 0\nConv15 0.1038 0.1038 0 0\nConv20 0.1851 0.1851 0 0\nTotal 0.98119 0.7398 0.2414 24.6\nFig. 8 Visualization of atom contributions, in the case of a mutagenic \ncompound. The red color stands for mutagenic alerts, color green \nagainst it\nPage 10 of 12Karpov et al. J Cheminform           (2020) 12:17 \nSeveral aromatic carbons had negative contributions, \nwhich was expected since aromatic compounds are poorly \nsoluble in water. Thus the overall explanation made sense, \nand the model had an excellent statistics not because of \nspurious correlations, but because it found the right \nfragmental features responsible for modelled property. \nThe standalone program contributed in this work has no \ndependencies on machine learning frameworks, it is easy \nto install, to use, and to interpret the modelling results. \nThis will make it an indispensable work-horse for drug-\ndevelopment projects world-wide.\nConclusions and outlook\nFor the first time we proposed a SMILES canonicaliza -\ntion method based on Transformer architecture that \nextracts information-rich real-value embeddings during \nthe encoding process and exposes them for further QSAR \nstudies. Also, for the first time we developed a framework \nfor the interpretation of models based on the Transformer \narchitecture using a layer-wise relevance propagation \n(LPR) approach.\nTextCNN approaches efficiently worked with embed\n-\ndings generated by Transformer, and the final quality of \nthe QSAR models was higher compared to the models \nobtained with the state-of-the-art methods on the major\n-\nity of diverse benchmark datasets. The Transformer-CNN \narchitecture required less than a hundred iterations to \nconverge for QSAR tasks to model various biological \nactivity or physico-chemical properties. It can be easily \nembedded into de-novo drug development pipelines. The \nmodel predictions interpreted in a fragment contribu\n-\ntion manner using the LPR could be useful to design new \nmolecules with desired biological activity and ADMETox \nproperties. The source code is available on https  ://githu \nb.com/bigch em/trans forme r-cnn as well as an on-line \nversion on https  ://ochem .eu. For solubility and AMES \nmutagenicity we also deposited standalone models in the \nGitHub repository, which not only predict the respective \nproperties but also provide interpretations of predictions.\nThe Transformer-CNN predicts the endpoint based \non an average of individual prognosis for a batch of aug\n-\nmented SMILES belonging to the same molecule. The \ndeviation within the batch can serve as a measure of a \nconfidence interval of the prognosis. Dissipation of rel\n-\nevance on biases as well as analysis of restored SMILES \ncan be used to derive the applicability domains of mod\n-\nels. These questions will be addressed in the upcoming \nstudies.\nAlso, as a comment, we do not think that the authors \nbenchmarking their methods are impassioned about their \nwork. Such benchmarking could be properly done by \nother users, and we do hope to see the proposed method \nused soon in future publications. But indeed, remark\n-\nably, in this work we saw an outstanding performance of \nthe proposed architecture, which provided systemati\n-\ncally better or at least similar results compared to the best \ndescriptor-based approaches as well as several analysed \ndeep neural network architectures. Even more remark\n-\nably, the Transformer CNN has practically no adjustable \nmeta parameters and thus does not require spending time \nto tune hyperparameters of neural architectures, use the \ngrid search to optimise Support Vector Machines, opti\n-\nmise multiple parameters of XGBoost, apply various \ndescriptors filtering and preprocessing, which could easily \ncontribute to the overfitting of models. This as well as the \npossibility to interpret models makes Transformer CNN a \nSwiss-knife for QSAR modeling and interpretation, which \nwill help to make the QSAR great again!\nAbbreviations\nADMETox: Absorption, distribution, metabolism, excretion and toxicity; ANN: \nArtificial neural network; CNN: Convolutional neural network; LSTM: Long \nShort-Term memory; OCHEM: On-line chemical database and modeling envi-\nronment; SMILES: Simplified Molecular-Input Line-Entry System; QSAR/QSPR: \nQuantitative Structure Activity/Property Relationship; RF: Receptive field; RNN: \nRecurrent Neural Network; CNN: Convolutional Neural Network; Transformer-\nCNN: Transformer Convolutional Neural Network.\nAcknowledgments\nThe authors thank NVIDIA Corporation for donating Quadro P6000, Titan Xp, \nand Titan V graphics cards for this research work.\nAuthors’ contributions\nPK implemented the method, IVT and GC performed the analysis and bench-\nmarking. All authors read and approved the final manuscript.\nFunding\nThis study was funded by the European Union’s Horizon 2020 research and \ninnovation program under the Marie Skłodowska-Curie grant agreement No. \n676434, “Big Data in Chemistry” and ERA-CVD \"CardioOncology\" project, BMBF \n01KL1710.\nFig. 9 Visualization of atom contributions to aqueous solubility of \nhaloperidol. The greep bars stand for more soluble features, whereas \nthe red ones show the opposite effect\nPage 11 of 12\nKarpov et al. J Cheminform           (2020) 12:17 \n \nAvailability of data and materials\nThe source code of the Transformer-CNN is available on https ://githu b.com/\nbigch em/trans forme r-cnn. Ready-to-use implementation, training datasets, \nand models are available on OCHEM https ://ochem .eu.\nCompeting interests\nThe authors declare that they have no actual or potential conflicts of interests.\nAuthor details\n1 Institute of Structural Biology, Helmholtz Zentrum München-Research Center \nfor Environmental Health (GmbH), Ingolstädter Landstraße 1, 85764 Neuher-\nberg, Germany. 2 BIGCHEM GmbH, Ingolstädter Landstraße 1, 85764 Neuher-\nberg, Germany. 3 Firmenich International SA, Digital Lab, Geneva, Lausanne, \nSwitzerland. \nReceived: 12 October 2019   Accepted: 9 March 2020\nReferences\n 1. Vaswani A, Shazeer N, Parmar N, et al (2017) Attention is all you need. \nPaper presented at the 31st Conference on Neural Information Processing \nSystems (NIPS 2017), Long Beach, CA, USA. arXiv :1706.03762 \n 2. Zhang X, Zhao J, LeCun Y (2015) Character-level convolutional networks \nfor text classification. arXiv e-prints. arXiv :1509.01626 \n 3. Sushko I, Novotarskyi S, Körner R et al (2011) Online chemical modeling \nenvironment (OCHEM): web platform for data storage, model develop-\nment and publishing of chemical information. J Comput Aided Mol Des \n25:533–554. https ://doi.org/10.1007/s1082 2-011-9440-2\n 4. Mauri A, Consonni V, Pavan M, Todeschini R (2006) Dragon software: an \neasy approach to molecular descriptor calculations. Match 56:237–248\n 5. Baskin I, Varnek A (2008) Fragment descriptors in SAR/QSAR/QSPR studies, \nmolecular similarity analysis and in virtual screening. Chemoinformatics \napproaches to virtual screening. Royal Society of Chemistry, Cambridge, \npp 1–43\n 6. Eklund M, Norinder U, Boyer S, Carlsson L (2014) Choosing feature selec-\ntion and learning algorithms in QSAR. J Chem Inf Model 54:837–843. https \n://doi.org/10.1021/ci400 573c\n 7. Baskin II, Winkler D, Tetko IV (2016) A renaissance of neural networks \nin drug discovery. Expert Opin Drug Discov 11:785–795. https ://doi.\norg/10.1080/17460 441.2016.12012 62\n 8. Duvenaud D, Maclaurin D, Aguilera-Iparraguirre J, et al (2015) Convo-\nlutional networks on graphs for learning molecular fingerprints. arXiv \ne-prints. arXiv :1509.09292 \n 9. Coley CW, Barzilay R, Green WH et al (2017) Convolutional embedding of \nattributed molecular graphs for physical property prediction. J Chem Inf \nModel 57:1757–1772. https ://doi.org/10.1021/acs.jcim.6b006 01\n 10. Gómez-Bombarelli R, Wei JN, Duvenaud D et al (2018) Automatic chemical \ndesign using a data-driven continuous representation of molecules. ACS \nCentral Sci 4:268–276. https ://doi.org/10.1021/acsce ntsci .7b005 72\n 11. Kimber TB, Engelke S, Tetko IV, et al (2018) Synergy effect between convo-\nlutional neural networks and the multiplicity of smiles for improvement of \nmolecular prediction. arXiv e-prints. arXiv :1812.04439 \n 12. Gilmer J, Schoenholz SS, Riley PF, et al (2017) Neural message passing for \nquantum chemistry. Proceedings of the 34 th International conference on \nmachine learning, Sydney, Australia, PMLR 70. arXiv :1704.01212 \n 13. Shang C, Liu Q, Chen K-S, et al (2018) Edge attention-based multi-rela-\ntional graph convolutional networks. arXiv e-prints. arXiv :1802.04944 \n 14. Weininger D (1988) SMILES, a chemical language and information system. \n1. Introduction to methodology and encoding rules. J Chem Inf Comput \nSci 28:31–36. https ://doi.org/10.1021/ci000 57a00 5\n 15. O’Boyle NM, Banck M, James CA et al (2011) Open babel: an open chemi-\ncal toolbox. J Cheminform 3:33. https ://doi.org/10.1186/1758-2946-3-33\n 16. Vidal D, Thormann M, Pons M (2005) LINGO, an efficient holographic text \nbased method to calculate biophysical properties and intermolecular \nsimilarities. J Chem Inf Model 45:386–393. https ://doi.org/10.1021/ci049 \n6797\n 17. Zhang X, LeCun Y (2015) Text understanding from scratch. arXiv e-prints.  \narXiv :1502.01710 \n 18. Goh GB, Hodas NO, Siegel C, Vishnu A (2017) SMILES2Vec: an interpretable \ngeneral-purpose deep neural network for predicting chemical properties. \narXiv e-prints. arXiv :1712.02034 \n 19. Jastrzębski S, Leśniak D, Czarnecki WM (2016) Learning to SMILE(S). arXiv \ne-prints. arXiv :1602.06289 \n 20. Goh GB, Siegel C, Vishnu A, Hodas NO (2017) Using rule-based labels for \nweak supervised learning: a chemnet for transferable chemical property \nprediction. arXiv e-prints. arXiv :1712.02734 \n 21. Zheng S, Yan X, Yang Y, Xu J (2019) Identifying structure-property relation-\nships through SMILES syntax analysis with self-attention mechanism. J \nChem Inf Model 59:914–923. https ://doi.org/10.1021/acs.jcim.8b008 03\n 22. Tetko IV, Karpov P , Bruno E, Kimber TB, Godin G. Augmentation Is What You \nNeed! In: Tetko IV, Karpov P , Kurkova V (ed) 28th International Conference \non Artificial Neural Networks Munich, Germany, 2019 Sep 17, Proceedings, \nPart V, Workshop and Special sessions, Springer, Cham, pp 831–835\n 23. Kiela D, Bottou L (2014) Learning image embeddings using convolutional \nneural networks for improved multi-modal semantics. In: Proceedings of \nthe 2014 Conference on empirical methods in natural language process-\ning (EMNLP). pp 36–45\n 24. Pennington J, Socher R, Manning CD (2014) Glove: global vectors for word \nrepresentation. EMNLP\n 25. Hinton GE, Salakhutdinov RR (2006) Reducing the dimensionality of data \nwith neural networks. Science 313:504–507. https ://doi.org/10.1126/scien \nce.11276 47\n 26. Heller S, McNaught A, Stein S et al (2013) InChI - the worldwide \nchemical structure identifier standard. J Cheminform 5:7. https ://doi.\norg/10.1186/1758-2946-5-7\n 27. Winter R, Montanari F, Noé F, Clevert D-A (2019) Learning continuous and \ndata-driven molecular descriptors by translating equivalent chemical \nrepresentations. Chem Sci 10:1692–1701. https ://doi.org/10.1039/c8sc0 \n4175j \n 28. Hochreiter S, Schmidhuber J (1997) Long short-term memory. Neural \nComput 9:1735–1780. https ://doi.org/10.1162/neco.1997.9.8.1735\n 29. Schwaller P et al (2019) Molecular transformer: A model for uncertainty-\ncalibrated chemical reaction prediction. ACS Cent Sci 5:1572–1583. https \n://doi.org/10.1021/acsce ntsci .9b005 76\n 30. Karpov P , Godin G, Tetko IV. A transformer model for retrosynthesis. In: \nTetko IV, Theis F, Karpov P , Kurkova V (ed) 28th International Conference \non artificial neural networks, Munich, Germany, September 17–19, 2019 \nProceedings, Part V, Workshop and Special sessions. Springer\n 31. Samek W, Müller K-R (2019) Towards explainable artificial intelligence. In: \nSamek W, Montavon G, Vedaldi A, et al. (eds) Explainable AI: interpreting, \nexplaining and visualizing deep learning. Springer International Publish-\ning, Cham, pp 5–22\n 32. Montavon G, Binder A, Lapuschkin S et al (2019) Layer-wise relevance \npropagation: an overview. In: Samek W, Montavon G, Vedaldi A, et al. (eds) \nExplainable AI: interpreting, explaining and visualizing deep learning. \nSpringer International Publishing, Cham, pp 193–209\n 33. Tetko IV, Villa AE, Livingstone DJ (1996) Neural network studies. 2. Variable \nselection. J Chem Inf Comput Sci 36:794–803. https ://doi.org/10.1021/\nci950 204c\n 34. Gaulton A, Bellis LJ, Bento AP et al (2012) ChEMBL: a large-scale bioactivity \ndatabase for drug discovery. Nucleic Acids Res 40:D1100–D1107. https ://\ndoi.org/10.1093/nar/gkr77 7\n 35. Segler MHS, Kogej T, Tyrchan C, Waller MP (2017) Generating focussed \nmolecule libraries for drug discovery with recurrent neural networks\n 36. Gupta A, Múller AT, Huisma BJH et al (2018) Generative recurrent networks \nfor de novo drug design. Mol Inform 37:1700111\n 37. Rush A (2018) The annotated transformer. In: Proceedings of workshop for \nNLP open source software (NLP-OSS). pp 52–60\n 38. Abadi M, Barham P , Chen J, et al (2016) TensorFlow: a system for large-\nscale machine learning\n 39. Landrum G RDKit: Open-source cheminformatics. https ://www.rdkit .org\n 40. Ramsundar B, Eastman P , Walters P , Pande V (2019) Deep learning for \nthe life sciences: applying deep learning to genomics, microscopy, drug \ndiscovery, and more. O’Reilly Media Inc, Sebastopol\n 41. Srivastava N, Hinton G, Krizhevsky A et al (2014) Dropout: a simple way to \nprevent neural networks from overfitting. J Mach Learn Res 15:1929–1958\n 42. Srivastava RK, Greff K, Schmidhuber J (2015) Highway Networks. Paper \npresented at the Deep Learning Workshop, International Conference on \nMachine Learning, Lille, France. arXiv :1505.00387 \nPage 12 of 12Karpov et al. J Cheminform           (2020) 12:17 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc h ?  Choose BMC and benefit fr om: \n 43. Tetko IV, Karpov P , Bruno E, et al (2019) Augmentation Is What You Need!: \n28th International Conference on artificial neural networks, Munich, Ger-\nmany, September 17–19, 2019, Proceedings. In: Tetko IV, Kůrková V, Karpov \nP , Theis F (eds) Artificial neural networks and machine learning–ICANN \n2019: workshop and special sessions. Springer International Publishing, \nCham, pp 831–835\n 44. Draper NR, Smith H (2014) Applied regression analysis. Wiley, New York\n 45. Tetko IV, Sushko Y, Novotarskyi S et al (2014) How accurately can we \npredict the melting points of drug-like compounds? J Chem Inf Model \n54:3320–3329. https ://doi.org/10.1021/ci500 5288\n 46. Wu Z, Ramsundar B, Feinberg EN et al (2018) MoleculeNet: a benchmark \nfor molecular machine learning. Chem Sci 9:513–530\n 47. Brandmaier S, Sahlin U, Tetko IV, Öberg T (2012) PLS-optimal: a stepwise \nd-optimal design based on latent variables. J Chem Inf Model 52:975–983\n 48. Sushko I, Novotarskyi S, Körner R et al (2010) Applicability domains for \nclassification problems: benchmarking of distance to models for ames \nmutagenicity set. J Chem Inf Model 50:2094–2111\n 49. Tetko IV, Tanchuk VY, Kasheva TN, Villa AEP (2001) Estimation of aqueous \nsolubility of chemical compounds using e-state indices. J Chem Inf Com-\nput Sci 41:1488–1493\n 50. Huuskonen JJ, Livingstone DJ, Tetko IV IV (2000) Neural network modeling \nfor estimation of partition coefficient based on atom-type electrotopo-\nlogical state indices. J Chem Inf Comput Sci 40:947–955\n 51. Suzuki K, Nakajima H, Saito Y et al (2000) Janus kinase 3 (Jak3) is essential \nfor common cytokine receptor γ chain (γc)-dependent signaling: com-\nparative analysis of γc, Jak3, and γc and Jak3 double-deficient mice. Int \nImmunol 12:123–132\n 52. Sutherland JJ, Weaver DF (2004) Three-dimensional quantitative structure-\nactivity and structure-selectivity relationships of dihydrofolate reductase \ninhibitors. J Comput Aided Mol Des 18:309–331\n 53. Vorberg S, Tetko IV (2014) Modeling the biodegradability of chemical com-\npounds using the online chemical modeling environment (OCHEM). Mol \nInform 33:73–85. https ://doi.org/10.1002/minf.20130 0030\n 54. Novotarskyi S, Abdelaziz A, Sushko Y et al (2016) ToxCast EPA in vitro \nto in vivo challenge: insight into the rank-I model. Chem Res Toxicol \n29:768–775. https ://doi.org/10.1021/acs.chemr estox .5b004 81\n 55. Rybacka A, Rudén C, Tetko IV, Andersson PL (2015) Identifying potential \nendocrine disruptors among industrial chemicals and their metabolites – \ndevelopment and evaluation of in silico tools. Chemosphere 139:372–378\n 56. Xia Z, Karpov P , Popowicz G, Tetko IV (2019) Focused library generator: \ncase of Mdmx inhibitors. J Comp Aided Mol Des 1:1\n 57. Chang C-C, Lin C-J (2011) LIBSVM: A library for support vector machines. \nACM Transact Int Syst Technol 2:27. https ://doi.org/10.1145/19611 \n89.19611 99\n 58. Breiman L (2001) Random forests. Mach Learn 45:5–32. https ://doi.\norg/10.1023/A:10109 33404 324\n 59. Chen T, Guestrin C (2016) XGBoost: A scalable tree boosting system. arXiv \n[cs.LG]\n 60. Tetko IV (2002) Associative neural network. Neural Process Lett 16:187–\n199. https ://doi.org/10.1023/A:10199 03710 291\n 61. Sosnin S, Karlov D, Tetko IV, Fedorov MV (2019) Comparative study of \nmultitask toxicity modeling on a broad chemical space. J Chem Inf Model \n59:1062–1072. https ://doi.org/10.1021/acs.jcim.8b006 85\n 62. Arras L, Montavon G, Müller K-R, Samek W (2017) Explaining recurrent \nneural network predictions in sentiment analysis. Proceedings of the 8th \nworkshop on computational approaches to subjectivity, sentiment and \nsocial media analysis\n 63. Plošnik A, Vračko M, Dolenc MS (2016) Mutagenic and carcinogenic \nstructural alerts and their mechanisms of action. Arh Hig Rada Toksikol \n67:169–182. https ://doi.org/10.1515/aiht-2016-67-2801\n 64. Xia Z, Karpov P , Popowicz G, Tetko IV (2019) Focused library generator: case \nof Mdmx inhibitors. J Comput Aided Mol Des. https ://doi.org/10.1007/\ns1082 2-019-00242 -8\n 65. Huuskonen J (2000) Estimation of aqueous solubility for a diverse set of \norganic compounds based on molecular topology. J Chem Inf Comput Sci \n40:773–777. https ://doi.org/10.1021/ci990 1338\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations."
}