{
    "title": "New Open-Source Tools: Using Bonsai for Behavioral Tracking and Closed-Loop Experiments",
    "url": "https://openalex.org/W3152449813",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2153708016",
            "name": "Gonçalo Lopes",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2042048321",
            "name": "Patrícia Monteiro",
            "affiliations": [
                "University of Minho"
            ]
        },
        {
            "id": "https://openalex.org/A2153708016",
            "name": "Gonçalo Lopes",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2042048321",
            "name": "Patrícia Monteiro",
            "affiliations": [
                "University of Minho"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2930741432",
        "https://openalex.org/W2305636401",
        "https://openalex.org/W2620639795",
        "https://openalex.org/W2949649718",
        "https://openalex.org/W308600200",
        "https://openalex.org/W2121649028",
        "https://openalex.org/W3008394955",
        "https://openalex.org/W2060778855",
        "https://openalex.org/W3111481866",
        "https://openalex.org/W1970585834",
        "https://openalex.org/W2586907386",
        "https://openalex.org/W2028095873",
        "https://openalex.org/W2951842640",
        "https://openalex.org/W2431669487",
        "https://openalex.org/W2101797330",
        "https://openalex.org/W2887114371",
        "https://openalex.org/W2949787383",
        "https://openalex.org/W2052515926",
        "https://openalex.org/W2091695392",
        "https://openalex.org/W2162020181",
        "https://openalex.org/W2020429579",
        "https://openalex.org/W2998735174",
        "https://openalex.org/W2097617075",
        "https://openalex.org/W3131145322",
        "https://openalex.org/W2964618163",
        "https://openalex.org/W3103780983",
        "https://openalex.org/W4319984928"
    ],
    "abstract": "The ability to dynamically control a behavioral task based on real-time animal behavior is an important feature for experimental neuroscientists. However, designing automated boxes for behavioral studies requires a coordinated combination of mechanical, electronic, and software design skills which can challenge even the best engineers, and for that reason used to be out of reach for the majority of experimental neurobiology and behavioral pharmacology researchers. Due to parallel advances in open-source hardware and software developed for neuroscience researchers, by neuroscience researchers, the landscape has now changed significantly. Here, we discuss powerful approaches to the study of behavior using examples and tutorials in the Bonsai visual programming language, towards designing simple neuroscience experiments that can help researchers immediately get started. This language makes it easy for researchers, even without programming experience, to combine the operation of several open-source devices in parallel and design their own integrated custom solutions, enabling unique and flexible approaches to the study of behavior, including video tracking of behavior and closed-loop electrophysiology.",
    "full_text": "TECHNOLOGY REPORT\npublished: 31 March 2021\ndoi: 10.3389/fnbeh.2021.647640\nEdited by:\nMichael V. Baratta,\nUniversity of Colorado Boulder,\nUnited States\nReviewed by:\nMeaghan Claire Creed,\nWashington University in St. Louis,\nUnited States\nNicholas Worley,\nBoston College, United States\n*Correspondence:\nGonçalo Lopes\ng.lopes@neurogears.org\nPatricia Monteiro\npatriciamonteiro@med.uminho.pt\nSpecialty section:\nThis article was submitted to\nEmotion Regulation and Processing\na section of the journal\nFrontiers in Behavioral Neuroscience\n†These authors have contributed\nequally to this work\nReceived: 30 December 2020\nAccepted: 08 March 2021\nPublished: 31 March 2021\nCitation:\nLopes G and Monteiro P (2021) New\nOpen-Source Tools: Using Bonsai for\nBehavioral Tracking and Closed-Loop\nExperiments.\nFront. Behav. Neurosci. 15:647640.\ndoi: 10.3389/fnbeh.2021.647640\nNew Open-Source Tools: Using\nBonsai for Behavioral Tracking and\nClosed-Loop Experiments\nGonçalo Lopes1*† and Patricia Monteiro2,3*†\n1NeuroGEARS Limited, London, United Kingdom,2Life and Health Sciences Research Institute (ICVS), School of Medicine,\nUniversity of Minho, Braga, Portugal,3ICVS/3B’s–PT Government Associate Laboratory, Braga/Guimaraes, Portugal\nThe ability to dynamically control a behavioral task based on real-time animal behavior is\nan important feature for experimental neuroscientists. However, designing automated\nboxes for behavioral studies requires a coordinated combination of mechanical,\nelectronic, and software design skills which can challenge even the best engineers, and\nfor that reason used to be out of reach for the majority of experimental neurobiology\nand behavioral pharmacology researchers. Due to parallel advances in open-source\nhardware and software developed for neuroscience researchers, by neuroscience\nresearchers, the landscape has now changed signiﬁcantly. Here, we discuss powerful\napproaches to the study of behavior using examples and tutorials in the Bonsai visual\nprogramming language, towards designing simple neuroscience experiments that can\nhelp researchers immediately get started. This language makes it easy for researchers,\neven without programming experience, to combine the operation of several open-source\ndevices in parallel and design their own integrated custom solutions, enabling unique\nand ﬂexible approaches to the study of behavior, including video tracking of behavior\nand closed-loop electrophysiology.\nKeywords: behavior, neuroscience, open source, visual programming, software\nINTRODUCTION\nQuantifying animal behavior is crucial in many fields of biological research such as behavioral\npharmacology, neuroscience, or ecology. By observing animal behavior in diverse settings,\nresearchers try to extract information about internal states, aiming to understand the causal\nstructure and dynamic properties of genetic and environmental factors (Gomez-Marin et al., 2014;\nKrakauer et al., 2017).\nIt is common to consider behavior as simply the set of all movements exhibited by an\nanimal over time, and that the goal of the researcher is having the ability to predict the\ndistribution of these movements under tightly controlled experimental conditions. Behavior,\nhowever, is precisely an act of resistance against a changing environment (Marken, 2009)\nand there is thus a ‘‘tug-of-war’’ between the experimenter who wants to control the\nenvironment of the animal and the animal who wants to have control over that same\nenvironment. For this reason, researchers often find themselves heavily constraining the\nopportunities for an animal to win over that control, to reduce the variability in animal\nmovement. Head-fixation or even anesthesia are used both to make it easier to measure\nFrontiers in Behavioral Neuroscience | www.frontiersin.org 1 March 2021 | Volume 15 | Article 647640\nLopes and Monteiro Using Bonsai for Behavioral Neuroscience\ninternal physiological state and to simplify the analysis of the\nbehavior. However, such extreme conditions can fundamentally\nchange the nature of the relationship between the animal and\nits environment, with corresponding changes in the dynamics\nof neurophysiological activity and a non-negligible impact in the\ninterpretation of experimental results.\nIn this article, we consider two alternatives to the classical\nhead-fixation or anesthetized paradigms: virtual fixation and\nvoluntary fixation. We discuss how these approaches can\nresolve fundamental issues in the design and analysis of\nbehavioral experiments and introduce an emerging set of\nmodifiable open-source tools aiming to make them increasingly\nmore accessible to behavioral researchers. We include practical\nexamples and tutorials using the Bonsai visual programming\nlanguage to help researchers immediately start applying these\nmethods to analyze and study brain circuits and behavior (see\nalso Supplementary Material for basic tutorials).\nThis piece is part of the research topic ‘‘New Insights into\nBehavioral Pharmacology,’’ organized as an extended forum for\nthe workshop ‘‘From networks to behavior and back,’’ a satellite\nevent of the European Behavioral Pharmacology Society (EBPS)\nBiennial Meeting (August 2019, Braga, Portugal).\nVirtual Fixation\nOne of the gold standards for tests of causality is reproducibility:\nany putative relationship between a causal variable and its\neffect should be reliably observed. To pick up on such\nstatistical regularities, researchers try to establish comparable\nconditions under which the relationship can be recorded many\ntimes. In behavioral studies, it is common to consider the\nmapping between ‘‘perception’’ and ‘‘action.’’ Perception is the\ninformation about the state of the environment that is accessible\nto the animal at any given time, and action is the set of\nmovements performed by the animal to change its relationship\nwith the environment (Figure 1).\nTo reduce the variability of this complex interaction,\nresearchers often go to great lengths to constrain the variability\nin perception, in the hopes of reducing irrelevant variability\nin possible actions. Ironically, to do this effectively requires\nthe action of the animal to be constrained in the first place,\noften by head-fixation, since even small changes to the position\nor orientation of the head relative to the environment will\ndramatically change the amount and type of stimulation reaching\nthe sensors.\nAn alternative to head-fixation is the freely moving paradigm.\nIn this situation, the animal is free to move in the environment,\nthus reinstating control over its perception. This situation is\noften considered by neurophysiology researchers to be a ‘‘harder’’\nsetting for behavioral neurophysiology, as precise control over\nthe input stimulus is lost.\nSurprisingly, however, the neurophysiology and mechanisms\nof specific brain systems, such as hippocampal navigation, only\nreally started to emerge by allowing animals to be observed\nunder freely moving conditions (O’Keefe and Dostrovsky, 1971;\nKandel, 2014). The discovery and analysis of place cell activity\nrely on the ability to monitor neuronal firing during freely\nmoving navigation. Rather than constraining the input/output\nFIGURE 1 | The canonical model for the interaction between a behavioral\nsubject and its environment. The agent receives information about the state\nof the environment through a variety of sensors and can control the state of\nthe environment through the use of actuators. Actions are chosen to minimize\nthe difference between the perceived state of the world and the intended\nstate of the world. The amount by which this difference is reduced is also\nsometimes referred to as the utility of an action.\nmapping between stimulus and action, the researcher records\nthe spontaneous behavior of the animal in detail together with\nthe physiological data and then correlates neural activity across\ncomparable conditions during the experiment, such as the animal\nlocation in space. We call this paradigm ‘‘virtual fixation.’’\nIn virtual fixation, the goal is to identify reproducible\nconditions by precise continuous measurement of animal\nbehavior over time. By identifying moments where the\nconditions of interest can be reliably compared, behavior\nbecomes amenable to statistical analysis despite occurring\nspontaneously. Perceptual states can thus be fixed for analysis\nwithout artificially fixing the subject or the stimulus.\nOf course, this approach is highly dependent on the accuracy\nof our behavioral measurements, and on the perceptual states\nwe are interested in. Tracking the center of mass position of a\nsingle animal in the open field is enough to reconstruct reliable\nplace cell activity. However, it is not enough if we are interested\nin measuring the distance between the nose and an object, the\npostural angles of the limbs during walking, or in knowing how\nmany photons are hitting the retina at a given time point.\nFortunately, emerging data analysis techniques are expanding\nthe scope of possible conditions which are amenable to\nvirtual fixation. The use of machine learning technology has\nentered full-force into behavioral labs worldwide through the\nintroduction of tools such as DeepLabCut (Mathis et al., 2018),\nwhich can lower the cost for tracking any user-labeled feature in\nvideo datasets. If the human eye can identify a feature of interest\nin a video, there is now a good chance we can automatically\nderive a tracker to reliably extract that feature for analysis.\nMarkerless limb and body part video tracking used to be an\napproach limited to highly technical laboratories which is now\nmuch more accessible due to the open-source nature of these\ntools. Furthermore, first-order features can often be combined\nto yield other measures of interest. For example, tracking head\nposition can be used to infer what portion of the visual field\nis accessible to the animal at each moment, thus allowing\nthe researcher to precisely determine which visual information\nFrontiers in Behavioral Neuroscience | www.frontiersin.org 2 March 2021 | Volume 15 | Article 647640\nLopes and Monteiro Using Bonsai for Behavioral Neuroscience\nFIGURE 2 | Bonsai workﬂow for recording video from a single camera.\nis accessible to the animal, despite freely moving conditions\n(Walter and Couzin, 2020).\nVirtual Fixation in Practice\nUntil recently, the development and application of machine\nlearning tools have required moderate programming experience\n(often in Python), or otherwise relying on standardized video\nanalysis toolkits with a limited set of functionalities. However,\nthe broad applicability of these techniques to diverse datasets has\ntriggered widespread interest even in communities of researchers\nwith no explicit technical training in computer science or\ncomputer vision.\nGiven the diversity of researchers’ interests and the difficulties\noutlined above in defining what is behavior from a measurement\nperspective, such tools must ideally combine flexibility with ease\nof access for non-experts. For this tutorial, we will rely on\nthe visual programming language Bonsai1 (Lopes et al., 2015)\nto illustrate some basic principles of freely moving behavior\nmeasurement, analysis, and virtual fixation.\nAlthough many approaches can be used to measure ongoing\nanimal activity, we will start with video analysis as it remains\nthe tool of choice for non-invasive, flexible, and unbiased\ninvestigation of behavior (in contrast to ‘‘lever presses’’ and\n‘‘nose pokes,’’ the video does not entail too many assumptions\non what behavior is before making a measurement). It also\ndoes not require complicated hardware setups, as cameras can\nnow be acquired very cheaply and can be placed virtually\nanywhere, provided that an adequate view of the animal can\nbe obtained. Illumination and occlusion certainly pose a fair\nshare of problems, but existing extensive collections of resources\non photography and videography can help researchers to\nunderstand and resolve the majority of these issues.\nOnce the setup is in place, and a compatible camera is\nconnected to the computer, video can be acquired in Bonsai with\na simple workflow (Figure 2).\nIt can often be helpful to record videos from multiple\nperspectives to gain more information about the contingencies\nsurrounding the animal in the freely moving condition. In this\ncase, care should be given to ensure that it will be possible\nto correlate information from multiple perspectives frame-by-\nframe. This is possible in single-camera setups by using mirrors\nto direct light from different angles to the same sensor. In multi-\ncamera setups, we need to correlate the image acquisition in time\n1All schematics included in the tutorial are fully functional workflows for\nthe Bonsai programming language which can be executed in the development\nenvironment which is freely available online at https://bonsai-rx.org.\nFIGURE 3 | Bonsai workﬂow for correlating video from two cameras\nsimultaneously.\nto ensure that information from one camera can be matched to\ninformation from the other cameras. Camera models supporting\ndigital triggers can be used to precisely synchronize the exposure\nof each frame to an external pacemaker. Otherwise, we can\ncorrelate multi-camera acquisition in software by taking the latest\nexposure from the extra cameras, every time a new frame is\ncollected (Figure 3).\nFollowing video recording, data can be processed offline (see\nSupplementary Material for basic tutorials), but all examples\nshown here can also be used verbatim online to make real-time\ndecisions on the conditions presented to the animal during\nclosed-loop experiments (see ‘‘Voluntary Fixation’’ section).\nVirtual fixation requires the reliable extraction of features\nfrom the video which we can use to establish comparable\nconditions for data analysis. By far the most commonly used\nfeature is the spatial position of the animal (usually the center\nof mass), referenced to a fixed set-up (often a box or arena).\nThe reason for such popularity can be justified by how much\ninformation can be derived from this simple metric relative to\nhow easily it can be retrieved from the video (Figure 4).\nManipulating the resulting time-series of 2D positions can be\nused to gain insight into the recorded behavior. Binning the data\nspatially can generate occupancy maps, thus indicating where the\nanimal spends most of its time; the numerical difference over\ntime will give an approximation for speed or quantity of motion,\nthus indicating when the animal is active or quiet; and defining\nspatial entry or exit conditions allows identifying moments where\nthe same path was taken (e.g., in a maze). If physiological data is\navailable and synchronized with the video, it becomes possible to\ncorrelate animal position in the arena with physiological signal\npatterns (e.g., a spike from a specific cell); or the converse, what\nis the pattern of brain activity when the animal decides to enter a\nspecific area within the arena.\nBy feeding back the result of tracking it is possible to\ndynamically crop a region of interest around the subject to obtain\nan ego-centric video where the animal is always in the center\n(Figure 5 ). From there we can analyze the video consistently\nfor proximal cues surrounding the animal at any moment, at\nany point in the arena. For example, a common application\nof this strategy is to extract a sequence of frames used for\ntraining feature detectors by machine learning libraries such as\nDeepLabCut (Mathis et al., 2018). This way we can refine our\nFrontiers in Behavioral Neuroscience | www.frontiersin.org 3 March 2021 | Volume 15 | Article 647640\nLopes and Monteiro Using Bonsai for Behavioral Neuroscience\nFIGURE 4 | Bonsai workﬂow for tracking the position (centroid) of a single animal in a well-lit arena, assuming a simple contrast threshold (black on white\nbackground).\nFIGURE 5 | Bonsai workﬂow for dynamic cropping of a region of interest around the center of the animal.\ninitial tracking to include specific body parts such as the nose,\nears, or paws (Figure 6). Furthermore, cropping the video will\nreduce the dimensionality of the data, therefore significantly\nreducing training time and speeding-up inference performance\nfor online analysis.\nExpanding the basic center of mass tracking to specific body\nparts, using DeepLabCut or other simple image processing\ntechniques, expands the possibilities for virtual fixation even\nfurther, allowing fixing specifically the video around the nose\nor head of the animal, and then calculating distances between\nthose body parts and other points of interest (or other animals;\nKane et al., 2020).\nUltimately, even failure modes in tracking can be interesting.\nIf the tracking is well-calibrated or trained for a specific behavior\npattern, then unexpected failures can work as outlier detectors\nand may well reveal moments of interest in the video (where the\nanimal displays a novel behavior pattern or otherwise deviates\nfrom the usual path). In these situations, having an exploratory\ndata analysis workflow with the ability to go from temporal\ninformation about a tracking failure back to the video can be\nextremely useful to assist with interpretation.\nVoluntary Fixation\nBehavior is often considered to be intrinsically variable, but\nthe whole history of motor control tells us that the story is\nslightly more complicated than that (Lopes and Kampff, 2015).\nBehavior and motor control in most animals are incredibly\nprecise, depending instead on the constraints of the body, the\ntask, and most importantly, what the animal is trying to achieve.\nInterestingly, what tends to happen during training is that\nanimals will shape their degrees of freedom around the exact\nconstraints required to succeed at the task, leaving everything\nelse variable. It is common for example in high temporal\nprecision lever pressing tasks to see animals develop extreme\nstereotypical patterns which unfold through extremely reliable\nsequences, and yet will be highly idiosyncratic across individuals\n(Kawai et al., 2015).\nWe can observe this proposition by contrasting variability\nin movements (high), to variability in controlling desired state\n(not high). Animals will eat, drink and sleep when necessary in\nextremely reliable patterns, although the means to achieve those\nends might be highly variable. If constraints are introduced that\nneed to be overcome to achieve their goals, animals will reliably\novercome them, even if the means to do so might surprise and\nfrustrate the researcher. Means are variable, ends are less so, and\nindeed a large part of behavior is resisting external perturbations,\nno matter the cost, to achieve goals reliably.\nIt is thus not surprising to find researchers relying on\nnaturally expressed drives and behaviors to challenge the animal\nto engage with the researcher’s experimental apparatus. Water\nrestriction or aversive stimuli remain common motivators of\naction in behavior studies such as the shuttling box (Figure 7).\nAnother, arguably more humane, approach is to exploit the\nflexibility of the animal to adopt different goals, and indeed\nto be trained for different tasks using operant conditioning,\nwhere the animal is trained using secondary reinforcers to\nrespond to specific stimuli in ways which are of interest to\nthe researcher.\nBoth operant conditioning and intrinsic drives have been\nwidely exploited in experimental psychology and experimental\nneuroscience as means of creating reproducible conditions. For\nour purposes, we group the collection of both approaches under\nthe term ‘‘voluntary fixation.’’ The main goal of voluntary\nfixation is getting animals to constrain their behavior without\nthe need for pharmacological or physical restriction. Indeed,\nin some extreme cases, this might be the only way to even\napproach in-vivo fixation studies themselves. For example,\nrats are notoriously difficult to restrain in awake head-fixed\npreparations, as they will leverage their incredible arm strength\nand often damage their own skull in an attempt to escape. But\nFrontiers in Behavioral Neuroscience | www.frontiersin.org 4 March 2021 | Volume 15 | Article 647640\nLopes and Monteiro Using Bonsai for Behavioral Neuroscience\nFIGURE 6 | Bonsai workﬂow for tracking a speciﬁc body part using the Bonsai-DLC package.\nFIGURE 7 | Schematic of the shuttling box apparatus. Animals will forage for liquid reward in this environment, either sucrose or water if using water restriction, and\nwill readily shuttle between the two ports (adapted from Lopes et al., 2016).\nrecent self-fixation studies have shown that rats will happily\nself-fix in awake imaging apparatuses in exchange for a reward,\nas long as it remains their own self-paced decision to do so\n(Scott et al., 2013).\nAs with virtual fixation, voluntary fixation places the burden\non the researcher to design experimental apparatuses which\nwill work with the animal, either accommodating their natural\ndrives or using automation to deploy operant conditioning\nprotocols where the animal can learn to constrain its behavior\nin exchange for a reward. In both cases, the technological\ninvestment can be too much to bear. Designing automated boxes\nfor behavioral studies requires a coordinated combination of\nmechanical, electronic, and software design skills which can\nchallenge even the best engineers, and for that reason used to be\nout of reach for the majority of experimental neurobiology and\nbehavioral pharmacology researchers.\nVoluntary Fixation in Practice\nFollowing the advent of accessible hobby electronics platforms\nsuch as the Arduino2, and the opening up of Ph.D. programs to\nmulti-disciplinary candidates, neuroscience has now grown\na healthy community of hackers and rapid-prototyping\naficionados. Those early researchers who were sympathetic\nto the open-source and open-science movements ended up\nadopting those engineering practices while developing their\nwork, resulting in open platforms and tools developed and\nshared broadly across the neuroscience community. These open\ndevices can now be quickly assembled for monitoring animal\nactions such as licking and lever pressing or controlling the\n2Arduino is an open-source electronic prototyping platform described in more\ndetail at https://www.arduino.cc/.\nenvironment using motors, lights, and sounds (Freeman, 2015;\nWhite et al., 2019).\nThe challenge now remains on how to combine the operation\nof all these devices in a way that is easy to understand and\ncustomize for individual experiments. Ideally, we further want\nto combine this control with other tools for rich monitoring\nof behavior such as the techniques for virtual fixation based\non the video discussed above. Most interfaces require bespoke\nprogramming skills to achieve the required control over the\nprecise, moment to moment, the sequence of events in our\nexperiment, and support closed-loop interactions, so we will\nagain use Bonsai to design some simple, yet functional,\nillustrations of automated environments which can support\nvoluntary fixation paradigms.\nWe will start by changing different aspects of the environment\nusing an Arduino microcontroller, which provides different\ndigital output ports which can be controlled directly in Bonsai\nwith a simple workflow (Figure 8).\nThe state of each port can be modified simply by changing the\nstate of Boolean inputs which can be eitherTrue or False. These\nwill correspondingly change the voltage at the terminals of the\nFIGURE 8 | Bonsai workﬂow for controlling a single Arduino digital output.\nFrontiers in Behavioral Neuroscience | www.frontiersin.org 5 March 2021 | Volume 15 | Article 647640\nLopes and Monteiro Using Bonsai for Behavioral Neuroscience\nFIGURE 9 | Bonsai workﬂow for triggering a digital output based on a region of interest.\nArduino between +5 V and 0 V, which can be used to turn lights\nor lasers on or off, trigger reward, or open valves.\nTo support voluntary fixation, we need such changes to\nbe triggered automatically so the animal can control the\nenvironment using its behavior. This means we need to be able\nto compute a change in logical level while the animal is moving\nfreely in the environment. For example, to emulate the above\nforaging patches we can specify regions of interest in the arena\nwhich will activate a port whenever an animal enters the patch\n(Figure 9).\nChanging the nature of the electronic devices wired to the\nArduino allows a broad range of automated responses to be\ndesigned, from triggering lights and sounds to electrical or\noptogenetic stimulation. The online calculation can also easily\nbe changed to operate on other features of animal behavior,\nsuch as the amount of motion (Figure 10 ), which will allow\nfor output stimuli to become contingent on animal freezing or\nfast movements.\nSuch dynamic control procedures operated using low latency\nfeedback allow the design of environments with interactive\nproperties which can be directly operated by the animal in\nreal-time. Crucially, the experimenter is now in control of the\ninteraction and can modify the response characteristics of the\nclosed-loop system to investigate animal behavior, for instance\nby delaying, suppressing, or amplifying the feedback response\nparametrically. Using Bonsai, virtual fixation techniques can\nthemselves be used in the design of such closed-loop systems,\nfor example by using the increasing computational capabilities\nof GPUs for real-time pose estimation (Kane et al., 2020).\nClosed-loop systems are also not restricted simply to behavior\nand can be successfully used even for purely physiological\ninvestigations in living nervous systems. In the example\nshown below, Bonsai was used for a patch-clamp closed-loop\nexperiment (Figure 11). Ex-vivo brain slices were prepared from\ntransgenic mice expressing channelrhodopsin (ChR2) in cortical\ninhibitory interneurons [parvalbumin-positive interneurons\n(PV)]. Neighboring cortical pyramidal neurons (without ChR2)\nwere patched to record spontaneous firing activity and Bonsai\nwas used to count the number of action potentials fired by\nthe pyramidal neuron in real-time. Upon every 10th action\npotential, Bonsai triggered a 488 nm fiber-coupled LED, leading\nto optogenetic stimulation of PV interneurons and inhibition\nof pyramidal neurons. This design can impose a new self-paced\nfiring pattern where a period of ten action potentials is followed\nby a period of silence (5 s optogenetic induced silencing) using\ncontinuous real-time closed-loop feedback.\nFinally, voluntary fixation paradigms can be extended not just\nto the study of the relationships between a single animal and its\nenvironment, but also to interactions between multiple animals.\nThis is often perceived as much harder given the difficulty\nin identifying individual animals without complex computer\nvision algorithms. However, the use of simple features relying on\ninvariant geometric properties can yield behaviorally meaningful\nand surprisingly robust metrics. For example, the following\nworkflow will compute the distance between two animals in a\nsingle arena (Figure 12).\nSince distance is a commutative quantity, we completely\navoid the need to uniquely identify each animal, and thus easily\nachieve fast, real-time performance. By coupling this quantity\nto a digital output port in the Arduino in the same way as the\nabove examples, we would now be able to trigger stimulation\ncontingent on the distance to a conspecific.\nCONCLUSION AND FUTURE\nPERSPECTIVES\nIn numerous tasks involving operant conditioning and intrinsic\ndrives have been widely used in experimental psychology and\nneuroscience to reproducibly study animal behavior. Back in\n1959, David Premack proposed that reinforcers should be seen\nnot as stimuli but rather as opportunities to engage in a behavior\n(Premack, 1959). In his famous drinking vs. running experiment,\nhe showed that thirsty rats prefer to drink rather than to run\nin the wheel, but when rats are not thirsty, they prefer to run\nrather than to drink. In other words, the activity should be\nregarded as the reinforcer, not the stimulus of water (Premack\nand Anglin, 1973). Of relevance to this discussion, Premack\nsuggested that animals should be allowed to engage freely in\nactivities. Accordingly, as behavioral neuroscientists, we should\nconsider factors that may determine when, and how vigorously,\nFIGURE 10 | Bonsai workﬂow for triggering a digital output based on movement.\nFrontiers in Behavioral Neuroscience | www.frontiersin.org 6 March 2021 | Volume 15 | Article 647640\nLopes and Monteiro Using Bonsai for Behavioral Neuroscience\nFIGURE 11 | Closed-loop patch-clamp experiment with Bonsai (contributed by Gonçalo Lopes, André Marques-Smith, Luis Jacinto, and Patricia Monteiro\n[unpublished]).\nFIGURE 12 | Bonsai workﬂow for measuring the distance between the two largest objects.\nresponses will be freely performed and how exploitation of new\ntools and new behavioral paradigms might grant us experimental\ncontrol over those responses.\nNew technologies and open-source tools for neuroscience\nare rapidly pushing the boundaries of what we can study,\nand how we study, the brains of awake behaving animals\n(Freeman, 2015; White et al., 2019). Opposite to proprietary tools\nthat present limited collaborative development and restricted\nexperimental designs, open-source tools allow customization\nand flexibility. Since its publication in 2015 (Lopes et al.,\n2015), Bonsai has been widely used by many labs worldwide\nnot only for tracking animal behavior in different species of\nrodents, cephalopods, fish, and insects (Dreosti et al., 2015;\nWalker et al., 2015; Douglass et al., 2017) but also to control\nand acquire data from multiple streams. Being open-source\nsoftware, Bonsai is free to use and not proprietary to any\nFrontiers in Behavioral Neuroscience | www.frontiersin.org 7 March 2021 | Volume 15 | Article 647640\nLopes and Monteiro Using Bonsai for Behavioral Neuroscience\none company, thus many users have adopted it to create their\nspecific packages for EEG (Lopes, 2018), Miniscopes (Aharoni\nand Hoogland, 2019; Guo et al., 2020), Fiber photometry\n(Carvalho and Lopes, 2019), Open Ephys (Neto et al., 2016)\nand real-time video analysis with DeepLabCut (Kane et al.,\n2020). It also encourages good practices for experimental\nreproducibility by including a built-in package manager and\nsupport for portable deployment across rigs. This means that\nexperimental workflow environments can be shared across labs\nwhile ensuring the behavior control software is reproducible.\nThis has been leveraged with great success on large international\ncollaborative projects such as the International Brain Laboratory\n(The International Brain Laboratory et al., 2020).\nLeveraging on this exciting open-source hardware/software\necosystem, it is now possible to study animals’ naturalistic\nbehaviors while maintaining control over many other variables,\nand potentially also integrating it with large-scale housing\nenvironments (Castelhano-Carlos et al., 2017). In other words,\ninstead of letting existing behavioral paradigms drive the research\nquestion, scientists can now design and implement custom\nbehavioral neuroscience experiments with unprecedented\ncontrol and intellectual freedom.\nThere are, however, challenges ahead. Because of their flexible\nnature and large degrees of customization freedom, open-source\ntools will always require some troubleshooting and experimental\nvalidation. Although user community help and development\nare more and more available, technical support for specific\nuser problems might not be readily available. To overcome\nthis challenge, we need to foster stronger communities and\nplatforms for dissemination, sharing, and training in these\nopen technologies. Only by understanding how such tools work\ncan researchers fully leverage their advantage and realize a\nhealthy open-source mindset for neuroscience. Towards this\ngoal, it is also fundamental that funding agencies start to\nsupport and incentivize these development and dissemination\nefforts, which currently still rely mostly on the passion and\ndetermination of lone researchers to share the results of their\nwork, often at great cost to their professional careers as\nresearchers. To protect the future of open-source tools, the\nacademic science community needs to recognize and value such\ncontributions themselves.\nDespite being in its infancy and despite all the above\nchallenges, open-source tools have already demonstrated the\nbenefits of shared neuroscience and currently play a significant\nrole in the field of behavioral neuroscience. Their future is\nbright and adopting a collaborative mindset for the behavioral\nneuroscience field will prove itself crucial to driving our\nunderstanding of the brain. Ultimately, though, studying the\nneural basis of behaviors still depends on the ability to design the\nkey experiment. It is up to researchers to ask the right questions.\nDATA AVAILABILITY STATEMENT\nThe original contributions presented in the study are included\nin the article/Supplementary Material, further inquiries can be\ndirected to the corresponding author/s.\nAUTHOR CONTRIBUTIONS\nGL and PM conceptualized and wrote the entire manuscript. All\nauthors contributed to the article and approved the submitted\nversion.\nFUNDING\nThis work has been funded by Society in Science, The Branco\nWeiss fellowship, administered by Eidgenössische Technische\nHochschule (ETH) Zürich; the European Molecular Biology\nOrganization (EMBO) Long-Term Fellowship (ALTF 89-2016)\nand Fundação para a Ciência e a Tecnologia (FCT; grant\nnumber PTDC/MED-NEU/28073/2017 and POCI-01-0145-\nFEDER-028073). This work has also been supported by National\nfunds through the Foundation for Science and Technology\n(FCT)—project UIDB/50026/2020 and UIDP/50026/2020; and\nby the projects NORTE-01-0145-FEDER-000013 and NORTE-\n01-0145-FEDER-000023, through Norte Portugal Regional\nOperational Programme (NORTE 2020), under the PORTUGAL\n2020 Partnership Agreement, by the European Regional\nDevelopment Fund (ERDF).\nACKNOWLEDGMENTS\nWe would like to acknowledge Luis Jacinto for critical reading of\nthe manuscript.\nSUPPLEMENTARY MATERIAL\nThe Supplementary Material for this article can be found\nonline at: https://www.frontiersin.org/articles/10.3389/fnbeh.\n2021.647640/full#supplementary-material.\nREFERENCES\nAharoni, D., and Hoogland, T. M. (2019). Circuit investigations with open-source\nminiaturized microscopes: past, present and future.Front. Cell. Neurosci.13.\ndoi: 10.3389/fncel.2019.00141\nCarvalho, F., and Lopes, G. (2019).Neurophotometrics. Available online at: https://\ngithub.com/neurophotometrics/neurophotometrics.\nCastelhano-Carlos, M. J., Baumans, V., and Sousa, N. (2017). PhenoWorld:\naddressing animal welfare in a new paradigm to house and assess rat behaviour.\nLab. Anim.51, 36–43. doi: 10.1177/0023677216638642\nDouglass, A. M., Kucukdereli, H., Ponserre, M., Markovic, M., Gründemann, J.,\nStrobel, C., et al. (2017). Central amygdala circuits modulate food consumption\nthrough a positive-valence mechanism. Nat. Neurosci. 20, 1384–1394.\ndoi: 10.1038/nn.4623\nDreosti, E., Lopes, G., Kampff, A. R., and Wilson, S. W. (2015). Development\nof social behavior in young zebrafish. Front. Neural Circuits 9:39.\ndoi: 10.3389/fncir.2015.00039\nFreeman, J. (2015). Open source tools for large-scale neuroscience.Curr. Opin.\nNeurobiol. 32, 156–163. doi: 10.1016/j.conb.2015.04.002\nGomez-Marin, A., Paton, J. J., Kampff, A. R., Costa, R. M., and Mainen, Z. F.\n(2014). Big behavioral data: psychology, ethology and the foundations of\nneuroscience. Nat. Neurosci.17, 1455–1462. doi: 10.1038/nn.3812\nGuo, W., Zhang, J., Newman, J., and Wilson, M. (2020).\nLatent learning drives sleep-dependent plasticity in distinct\nFrontiers in Behavioral Neuroscience | www.frontiersin.org 8 March 2021 | Volume 15 | Article 647640\nLopes and Monteiro Using Bonsai for Behavioral Neuroscience\nCA1 subpopulations. bioRxiv [Preprint]. doi: 10.1101/2020.02.\n27.967794\nKandel, E. (2014). A place and a grid in the sun.Cell 159, 1239–1242. doi: 10.1016/j.\ncell.2014.11.033\nKane, G. A., Lopes, G., Saunders, J. L., Mathis, A., and Mathis, M. W. (2020). Real-\ntime, low-latency closed-loop feedback using markerless posture tracking.eLife\n9:e61909. doi: 10.7554/eLife.61909\nKawai, R., Markman, T., Poddar, R., Ko, R., Fantana, A. L., Dhawale, A. K., et al.\n(2015). Motor cortex is required for learning but not for executing a motor skill.\nNeuron 86, 800–812. doi: 10.1016/j.neuron.2015.03.024\nKrakauer, J. W., Ghazanfar, A. A., Gomez-Marin, A., MacIver, M. A.,\nand Poeppel, D. (2017). Neuroscience needs behavior: correcting a\nreductionist bias. Neuron 93, 480–490. doi: 10.1016/j.neuron.2016.\n12.041\nLopes, G. (2018). Bonsai-rx/Biosemi. Available online at: https://github.com/\nbonsai-rx/biosemi.\nLopes, G., and Kampff, A. R. (2015). Cortical control: learning from the lamprey.\nCurr. Biol.25, R203–R205. doi: 10.1016/j.cub.2015.01.020\nLopes, G., Bonacchi, N., Frazão, J., Neto, J. P., Atallah, B. V., Soares, S.,\net al. (2015). Bonsai: an event-based framework for processing and\ncontrolling data streams. Front. Neuroinform. 9:7. doi: 10.3389/fninf.2015.\n00007\nLopes, G., Nogueira, J., Dimitriadis, G., Menendez, J. A., Paton, J., and Kampff, A.\n(2016). A robust role for motor cortex. bioRxiv [preprint]. doi: 10.1101/\n058917\nMarken, R. S. (2009). You say you had a revolution: methodological foundations\nof closed-loop psychology. Rev. Gen. Psy. 13, 137–145. doi: 10.1037/\na0015106\nMathis, A., Mamidanna, P., Cury, K. M., Abe, T., Murthy, V. N., Mathis, M. W.,\net al. (2018). DeepLabCut: markerless pose estimation of user-defined body\nparts with deep learning.Nat. Neurosci. 21, 1281–1289. doi: 10.1038/s41593-\n018-0209-y\nNeto, J. P., Lopes, G., Frazão, J., Nogueira, J., Lacerda, P., Baião, P., et al.\n(2016). Validating silicon polytrodes with paired juxtacellular recordings:\nmethod and dataset. J. Neurophysiol. 116, 892–903. doi: 10.1152/jn.001\n03.2016\nO’Keefe, J., and Dostrovsky, J. (1971). The hippocampus as a spatial map.\nPreliminary evidence from unit activity in the freely-moving rat.Brain Res.34,\n171–175. doi: 10.1016/0006-8993(71)90358-1\nPremack, D. (1959). Toward empirical behavior laws: I. Positive reinforcement.\nPsychol. Rev.66, 219–233. doi: 10.1037/h0040891\nPremack, D., and Anglin, B. (1973). On the possibilities of self-control in man and\nanimals. J. Abnormal Psychol.81, 137–151. doi: 10.1037/h0034492\nScott, B. B., Brody, C. D., and Tank, D. W. (2013). Cellular resolution functional\nimaging in behaving rats using voluntary head restraint.Neuron 80, 371–384.\ndoi: 10.1016/j.neuron.2013.08.002\nThe International Brain Laboratory, Aguillon-Rodriguez, V., Angelaki, D. E.,\nBayer, H. M., Bonacchi, N., Carandini, M., Cazettes, F., et al. (2020). A\nstandardized and reproducible method to measure decision-making in mice.\nbioRxiv [Preprint]. doi: 10.1101/2020.01.17.909838\nWalker, S. J., Corrales-Carvajal, V. M., and Ribeiro, C. (2015). Postmating circuitry\nmodulates salt taste processing to increase reproductive output inDrosophila.\nCurr. Biol.25, 2621–2630. doi: 10.1016/j.cub.2015.08.043\nWalter, T., and Couzin, I. D. (2020). TRex, a fast multi-animal tracking system\nwith markerless identification and 2D estimation of posture and visual fields.\neLife 10:e64000. doi: 10.7554/eLife.64000. [Online ahead of print].\nWhite, S. R., Amarante, L. M., Kravitz, A. V., and Laubach, M. (2019). The\nfuture is open: open-source tools for behavioral neuroscience research.eNeuro\n6:ENEURO.0223-19.2019. doi: 10.1523/ENEURO.0223-19.2019\nConﬂict of Interest: GL is director at NeuroGEARS Limited.\nThe remaining author declares that the research was conducted in the absence of\nany commercial or financial relationships that could be construed as a potential\nconflict of interest.\nCopyright © 2021 Lopes and Monteiro. This is an open-access article distributed\nunder the terms of the Creative Commons Attribution License (CC BY). The use,\ndistribution or reproduction in other forums is permitted, provided the original\nauthor(s) and the copyright owner(s) are credited and that the original publication\nin this journal is cited, in accordance with accepted academic practice. No use,\ndistribution or reproduction is permitted which does not comply with these terms.\nFrontiers in Behavioral Neuroscience | www.frontiersin.org 9 March 2021 | Volume 15 | Article 647640"
}