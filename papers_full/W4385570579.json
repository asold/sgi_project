{
    "title": "Entity Tracking in Language Models",
    "url": "https://openalex.org/W4385570579",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5071542832",
            "name": "Najoung Kim",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5018467280",
            "name": "Sebastian Schuster",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2951976932",
        "https://openalex.org/W4362731048",
        "https://openalex.org/W2126209950",
        "https://openalex.org/W1525482321",
        "https://openalex.org/W2810346659",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4288000348",
        "https://openalex.org/W2252269235",
        "https://openalex.org/W4385572854",
        "https://openalex.org/W1789782362",
        "https://openalex.org/W2963372003",
        "https://openalex.org/W2945614092",
        "https://openalex.org/W3105291316",
        "https://openalex.org/W1523813253",
        "https://openalex.org/W2964285770",
        "https://openalex.org/W3173798466",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W2071089353",
        "https://openalex.org/W3034685497",
        "https://openalex.org/W1585104069",
        "https://openalex.org/W2971226772",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4385567216",
        "https://openalex.org/W2165256085",
        "https://openalex.org/W4385574135",
        "https://openalex.org/W2155069789",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2944226280",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W1752492850",
        "https://openalex.org/W4283793451",
        "https://openalex.org/W1927197813",
        "https://openalex.org/W3203259592",
        "https://openalex.org/W2963925965",
        "https://openalex.org/W4307413986",
        "https://openalex.org/W2971253865",
        "https://openalex.org/W2964222246",
        "https://openalex.org/W4302570567",
        "https://openalex.org/W2964027319",
        "https://openalex.org/W3035267217",
        "https://openalex.org/W1525961042",
        "https://openalex.org/W4287887107",
        "https://openalex.org/W3034723486",
        "https://openalex.org/W4307536702",
        "https://openalex.org/W4288089799"
    ],
    "abstract": "Keeping track of how states of entities change as a text or dialog unfolds is a key prerequisite to discourse understanding. Yet, there have been few systematic investigations into the ability of large language models (LLMs) to track discourse entities. In this work, we present a task probing to what extent a language model can infer the final state of an entity given an English description of the initial state and a series of state-changing operations. We use this task to first investigate whether Flan-T5, GPT-3 and GPT-3.5 can track the state of entities, and find that only GPT-3.5 models, which have been pretrained on large amounts of code, exhibit this ability. We then investigate whether smaller models pretrained primarily on text can learn to track entities, through finetuning T5 on several training/evaluation splits. While performance degrades for more complex splits, we find that even when evaluated on a different set of entities from training or longer operation sequences, a finetuned model can perform non-trivial entity tracking. Taken together, these results suggest that language models can learn to track entities but pretraining on text corpora alone does not make this capacity surface.",
    "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 3835–3855\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nEntity Tracking in Language Models\nNajoung Kim∗\nDepartment of Linguistics\nBoston University\nnajoung@bu.edu\nSebastian Schuster∗\nDept. of Language Science and Technology\nSaarland University\nseschust@lst.uni-saarland.de\nAbstract\nKeeping track of how states of entities change\nas a text or dialog unfolds is a key prereq-\nuisite to discourse understanding. Yet, there\nhave been few systematic investigations into\nthe ability of large language models (LLMs)\nto track discourse entities. In this work, we\npresent a task probing to what extent a lan-\nguage model can infer the ﬁnal state of an\nentity given an English description of the ini-\ntial state and a series of state-changing op-\nerations. We use this task to ﬁrst investi-\ngate whether Flan-T5, GPT-3 and GPT-3.5 can\ntrack the state of entities, and ﬁnd that only\nGPT-3.5 models, which have been pretrained\non large amounts of code, exhibit this abil-\nity. We then investigate whether smaller mod-\nels pretrained primarily on text can learn to\ntrack entities, through ﬁnetuning T5 on sev-\neral training/evaluation splits. While perfor-\nmance degrades for more complex splits, we\nﬁnd that even when evaluated on a different set\nof entities from training or longer operation se-\nquences, a ﬁnetuned model can perform non-\ntrivial entity tracking. Taken together, these\nresults suggest that language models can learn\nto track entities but pretraining on text corpora\nalone does not make this capacity surface.\n1 Introduction\nA key prerequisite to long-context understanding\nand generating coherent text is the ability to accu-\nrately represent entities as the discourse unfolds\n(Karttunen, 1976; Groenendijk and Stokhof, 1991;\nHeim, 2002; Nieuwland and Van Berkum, 2006;\nKamp et al., 2011, i.a.). For example, consider the\nfollowing example in the context of a recipe:\n(1) Put the eggs, sugar, ﬂour, and baking pow-\nder in a bowl and mix to form a light batter.\nMake sure that the ﬁnal batter does not con-\ntain any lumps of ﬂour or sugar.\n∗Equal contribution. Author order was determined by a\nfair process of letting Cookie the cat pick a colored ball.\nQ: Box 1 contains the book. Box 2 contains the apple. Box 4 contains \nthe brain. Move the book into Box 2. Put the bell into Box 4. Move the \nbell and the brain into Box 5. Box 2 contains ____ \nA: the apple and the book\nFigure 1: A sketch of our entity tracking task.\nIn order to understand this instruction, several dis-\ntinct abilities are necessary:\nNew discourse entity recognition:recognizing\nwhen new discourse entities are introduced. E.g., a\nbowl introduces a new discourse entity but the ﬁnal\nbatter or any lumps of ... does not.\nCoreference resolution:associating referring ex-\npressions with discourse entities. E.g., a light bat-\nter and the ﬁnal batter refer to the same entity.\nDiscourse entity tracking: tracking the state\nchanges made to each discourse entity. E.g., the\neggs are put into the bowl and mixed with the other\ningredients.\nThere exist many datasets that aim to evaluate\nthese abilities (e.g., Walker et al., 2006; Pradhan\net al., 2012; Rahman and Ng, 2012; Weston et al.,\n2015; Chen et al., 2018; Bamman et al., 2020;\nUryupina et al., 2020) and many NLP models that\naim to solve these tasks (e.g., Haghighi and Klein,\n2010; Lee et al., 2011; Hill et al., 2016; Henaff\net al., 2017; Ji et al., 2017; Lee et al., 2017; Bosselut\net al., 2018; Gupta and Durrett, 2019a,b; Aina et al.,\n2019; Toshniwal et al., 2020; Wu et al., 2020). In\nthe context of large language models (LLMs), Ten-\nney et al. (2019), Clark et al. (2019), and Sorodoc\net al. (2020) found that representations of LSTMs\nand Transformer-based models such as BERT (De-\nvlin et al., 2019) do capture coreference relations.\nLoáiciga et al. (2022) and Schuster and Linzen\n(2022) found that pretrained models are able to\n3835\ndetect whether noun phrases introduce discourse\nentities, albeit not fully systematically.\nThe question of whether LLMs can track the\nstate of discourse entities , however, has mostly\nbeen indirectly evaluated. Toshniwal et al. (2022)\nshowed that GPT-2 (Radford et al., 2019) can learn\nto predict valid chess moves based on a compact,\nnonlinguistic description of previous moves. Sim-\nilarly, Li et al. (2023) showed that a GPT model\ntrained on Othello can predict valid next moves,\nand that these predictions are tied to the model’s\ninternal representations of the board states. Still,\nthese results do not tell us whether LLMs track\nstate changes expressed in natural language dis-\ncourses. The most relevant evaluation is Li et al.\n(2021), where they tested whether model represen-\ntations encode entity states described in naturalistic\ntext. Using a probing classiﬁer, they found that the\nstates can be decoded from T5 (Raffel et al., 2020)\nand BART (Lewis et al., 2020) with high accuracy.\nHowever, as we show in a reanalysis of their results\n(Section 2), they do not provide deﬁnitive evidence\nfor entity tracking. Hence, whether LLMs can track\nentities during the processing of natural language\ndiscourse remains an open question.\nContributions This work attempts to answer this\nquestion by developing a task targeted towards eval-\nuating a language model’s ability to track state\nchanges of discourse entities (illustrated in Fig-\nure 1). We use this novel task to evaluate GPT-3\n(Brown et al., 2020), GPT-3.5, and Flan-T5 (Chung\net al., 2022) without any ﬁnetuning. We ﬁnd that\nonly models in the GPT 3.5 series, which have been\ntrained on both text and code, are able to perform\nnon-trivial entity tracking. We then show that a\nsmaller language model (T5) can learn to perform\nnon-trivial entity tracking and also demonstrates\nsome capacity to generalize to state descriptions\nwith more operations or with low lexical overlap.\nOur results suggest that language models can learn\nto track entities but pretraining on text corpora\nalone does not make this capacity surface. More\nbroadly, our task can also serve as a useful tool for\ninvestigations into emergent world models in LMs\n(e.g., Li et al., 2023; Tsai et al., 2023).1\n2 Reanalysis of Li et al. (2021)\nWe start from examining Li et al. (2021), the most\nrelevant work to ours. They adapted two exist-\n1Code and data are available at https://github.com/\nsebschu/entity-tracking-lms.\ning datasets, Alchemy (Long et al., 2016) and\nTextWorld (Côté et al., 2019), to test a model’s\nability to track state changes of an entity. The in-\nput to the model is a text description of the initial\nworld state followed by state-changing instructions.\nBased on this description, the model is expected to\nidentify the correct ﬁnal state of each entity. For\nexample, for Alchemy, the model receives formu-\nlaic descriptions of 7 beakers containing different\namounts of colored liquids, followed by instruc-\ntions that manipulate their contents such as pouring\nthe liquid from one beaker into another, or draining\na beaker. Given an input like (2), the model is ex-\npected to recognize that the ﬁrst beaker has 4 units\nof brown liquid, the second beaker has 2 units of\nred liquid, and the third beaker is empty.\n(2) The ﬁrst beaker has 1 green, the second\nbeaker has 2 red, the third beaker has 3 red.\nPour the last red beaker into beaker 1. Mix.\nUsing such descriptions, Li et al. (2021) found that\na probing classiﬁer that takes as input the encoding\nof these descriptions from T5 or BART is able to\ncorrectly predict the state of 75–76% of the enti-\nties, suggesting some degree of success on entity\ntracking.\nHowever, this conclusion becomes questionable\nwhen the datasets and the results are scrutinized\nfurther. Speciﬁcally, we conducted a ﬁne-grained\nanalysis of the success cases of the Alchemy ex-\nperiment. In this experiment, the state of each\nbeaker was probed after each state-changing in-\nstruction. Because each instruction targets at most\ntwo beakers (e.g., pour X into Y ) and there are 7\nbeakers in total, there is a sparse representation of\ncases probing a beaker that actually underwent a\nchange in the dataset. Indeed, 62.7% of all beaker\nstates probed were identical to the initial state,\nmeaning that a simple baseline that always predicts\nthe initial state already achieves 62.7% accuracy\n(this is also noted by Li et al.). A second potential\nfor shortcuts was the high rate of empty ﬁnal states\n(32.4%).2 For these cases, the initial state can of-\nten be entirely disregarded, due to the presence of\nan emptying instruction such as Drain the fourth\nbeaker. This instruction alone is sufﬁcient to pre-\ndict the fourth beaker’s ﬁnal state independent of\nits initial state. Therefore, such examples are also\n2This percentage also includes cases where a beaker was\nalready empty in the initial world state description. The per-\ncentage of cases where a beaker was empty but contained\nsome liquid in the initial description is 24.9%.\n3836\nnot best suited to fully assess entity tracking. Given\nthe high prevalence of these two trivial scenarios\n(87.6% in total), only 12.4% of the datapoints can\nbe considered as truly assessing state changes un-\nfolding over a discourse context. If the accuracy\nis computed on the trivial and non-trivial cases\nseparately, the probing classiﬁer achieves 86.8%\naccuracy on trivial cases but only 3.1% accuracy on\nnon-trivial cases, showing that most of the reported\nsuccess derives from the trivial cases.\nIn summary, our reanalysis suggests that the re-\nsults of Li et al. (2021) do not provide conclusive\nevidence for non-trivial state tracking abilities in\nlanguage models.3 However, it remains unclear\nwhether this is due to issues with the setup or a true\nlack of entity tracking capacity. To this end, we\npropose a new behavioral evaluation.\n3 Task Design and Dataset\n3.1 Desiderata\nThe ability to track entities should be largely inde-\npendent of speciﬁc linguistic forms . For a model\nthat can properly track entities, it should not matter\nwhether one talks about beakers or recipes or which\nspeciﬁc syntactic constructions are used. This\nmakes it an interesting ability to evaluate in the con-\ntext of assessing whether and how meaning is rep-\nresented, since at least classic language models are\nonly trained on forms (Bender and Koller, 2020).\nAt the same time, this independence of form and\nentity states that should hold for true entity tracking\nposes a challenge for evaluation, since one needs to\nensure that the state of entities cannot be predicted\nfrom individual lexical items or phrases (such as\nthe word drain in the Alchemy dataset, as discussed\nin Section 2). Furthermore, language models pre-\ntrained on large corpora may have learned common\nstates of entities; for instance, that eggs often end\nup in a bowl. For these reasons, any task that evalu-\nates entity tracking abilities should conform to the\nfollowing four desiderata:\n1. The probed states of entities should not follow\nsimilar distributional patterns to those that are\nlikely to be present in the pretraining data (see\nalso Linzen, 2020).\n2. Individual words or phrases should not predict\nby themselves the state of an entity without\n3Li et al. (2021) also presented two other sets of experi-\nments. See Appendix A for details on how the other experi-\nments exhibit similar issues.\nconsidering the previous discourse in order.\n3. If any data is used for demonstration, ﬁne-\ntuning or training, the training and evaluation\ndata should have little lexical overlap.\n4. If any data is used for demonstration, ﬁnetun-\ning or training, the task should not be solvable\nby slot-ﬁlling based on observed datapoints.\nThese properties cannot be guaranteed with natural-\nistic datasets such as recipes (Kiddon et al., 2015),\nscience texts (Dalvi et al., 2019), or the Alchemy\nand TextWorld datasets, which have been previ-\nously used to evaluate entity tracking abilities. We\ntherefore programmatically generated datasets for\nwhich these properties hold.\n3.2 Dataset\nWe take inspiration from Winograd (1971) and Li\net al. (2021) in designing our data. Our datasets\nconsist of text descriptions of a particular state of\nthe world followed by a sequence of changes. The\nworlds contain boxes that can be ﬁlled with objects.\nThe objects can be placed inside the box, taken out\nof the box, or moved from one box to another. We\ndeﬁne a world Was W= (O, n, m, e) where O is\na set of objects, n is the number of boxes, m is the\nmaximum number of objects one box can contain,\nand e is the expected number of objects in each\nbox in the initial world states. For our datasets, we\nused n = 7, m = 3, e = 2, and used a set of nouns\ndenoting items that can plausibly ﬁt inside a box\n(e.g., book, rock, brain; |O|= 100), selected from\na list of words with frequency greater than 27 in the\nBritish National Corpus (BNC; Leech et al. 2001).\nA dataset consists of multiple distinct scenarios.\nA scenario consists of an initial state and a set of\noperations applied to this initial state. We ﬁxed\nthe number of operations (NumOps) in each sce-\nnario to 12. We randomly sampled 2200 scenarios,\nwhere the initial state and the 12 operations were\nboth randomly sampled. The sampling process is\ndesigned such that only valid operations given the\ncurrent world state can be sampled. The initial state\nand the operations were converted into naturalistic\ndescriptions using predeﬁned templates.\nRelation to DesiderataWe selected the task of\nmoving objects across boxes because this is a do-\nmain where lexical contents of the entities do not\noffer cues to predict the outcome of state changes\n(Desideratum 1). We did not include an opera-\ntion that empties a box that allows the previous\n3837\ndiscourse to be discarded (Desideratum 2). For\nDesideratum 2, we furthermore considered exper-\niments using operation descriptions with greater\ncontext dependence. Speciﬁcally, we tested scenar-\nios with an additional Move contents of Box N to\nBox M operation that does not explicitly mention\nthe object names, and scenarios where object de-\nscriptions in the operations can only be fully disam-\nbiguated by knowing the current state of a box. For\nDesideratum 3, we considered experiments where\nthe phrasing of the states and operations differ en-\ntirely between demonstration/ﬁnetuning and evalu-\nation (see Table 2). Finally, for all experiments, we\ncomputed a “signature” of every initial state that\nindicates the number of objects contained in each\nbox.4 Then, we ensured that there were no two\nexamples with identical initial descriptions mod-\nulo the object names where one appeared in the\ntraining split and the other one in the evaluation\nsplit. This prevents this task from being solvable\nvia slot-ﬁlling (Desideratum 4).5\n3.3 Task\nWe deﬁne the entity tracking task as follows. Given\na natural language description of the initial state\nof the world followed by 0–12 state-changing op-\nerations, the content of each box at the end of\nthe description must be correctly identiﬁed. To\nevaluate this, we created a test example for each\nbox after each operation. This corresponds to\nn ×(NumOps + 1)examples per scenario (91 exs.\nin our datasets). Each example is formulated in the\nstyle of a cloze test. That is, the input describes the\ninitial state followed by a sequence of operations,\nending in Box N contains __. The expected output\nis the correct set of objects in Box N based on the\npreﬁx description. See Appendix B for an example.\n4 Experiment 1: In-context\nDemonstration\nIn the ﬁrst set of experiments, we evaluated pre-\ntrained LMs using a small number of in-context\n4For example, the signature of an initial state in which the\nﬁrst box contains two objects and the rest contains 1 object\neach would be 2111111.\n5Additionally, compared to the Alchemy setup, our setup\nalso has a beneﬁt of requiring fewer additional reasoning\nabilities. The beaker domain in Alchemy requires the model\nto count and perform simple arithmetic (e.g., adding one unit\nof liquid followed by adding two units of liquid results in\nthree units of liquid). Moreover, some of the operations in\nAlchemy require knowledge about how colors are combined\n(e.g., mixing red and green liquids results in a brown liquid).\nThe boxes domain removes these requirements.\ndemonstrations of the entity tracking task. This\nprovides a way to probe the model without requir-\ning substantial supervision for the task, as well as\nguiding the model to output the ﬁnal state in a con-\nsistent format that can be automatically assessed.6\n4.1 Models\nWe used models that are known to support task\nadaptation via in-context demonstrations: GPT-\n3 175B ( davinci: Brown et al. 2020), GPT-3.5\n(text-davinci-0037), and Flan-T5 (base and XL:\nChung et al. 2022). The little information that\nOpenAI revealed about their models8 suggests that\ndavinci is an autoregressive language model pri-\nmarily trained on text corpora.text-davinci-003\nwas trained on the language modeling objective\non a mix of text and code, and additionally tuned\nwith human feedback using reinforcement learning.\nFlan-T5 is based on T5, a sequence-to-sequence\nmodel trained on a denoising objective, that has\nbeen further instruction-ﬁnetuned on a battery of\ntasks. This has been shown to promote better\nresponses to instructions, both with and without\ndemonstrations (Chung et al., 2022). We evaluated\nthe GPT models through the OpenAI API and the\nFlan-T5 using the HuggingFace library (Wolf et al.,\n2020). See Table 1 for a summary of the models,\nand Appendix C for implementation details.\nWe compared these models against a baseline\nthat randomly outputs 0 to m = 3objects from the\nset of objects that appeared in the same clauses as\nthe box in question. Note that this baseline is much\nstronger than a fully random baseline that selects\noutputs from all mentioned objects.\n4.2 Prompting and Demonstrations\nOur prompts consist of: (a) a general instruction\nfor the task, (b) two examples of the task to demon-\nstrate the expected format, (c) an initial state de-\nscription followed by a series of operations, and (d)\nan incomplete sentence Box N contains ___ that the\nmodel should complete (see Appendix E for full\nprompts). We used demonstrations that output the\nstate of all boxes at once. However, in early experi-\nments, Flan-T5 frequently only output the state of\nthe ﬁrst box even when the in-context demonstra-\ntions contained ﬁnal descriptions of all box states.\n6For GPT-3.5, we were able to instruct the model to out-\nput predictions in a consistent format without any in-context\ndemonstrations, thus making it a true zero-shot experiment.\nWe discuss the results of this experiment in Appendix D.\n7https://beta.openai.com/docs/models/gpt-3\n8https://beta.openai.com/docs/model-index-for-researchers\n3838\nModel Size Code? Additional Training\nGPT-3davinci 175B \u0017 -\nGPT-3davinci-instruct-beta175B \u0017 Human demonstrations(ﬁnetuning)\nGPT-3text-davinci-001175B \u0017 Human demonstrations+ highly rated model outputs(ﬁnetuning)GPT-3.5code-davinci-002? \u0013 -\nGPT-3.5text-davinci-002? \u0013 Human demonstrations+ highly rated model outputs(ﬁnetuning)GPT-3.5text-davinci-003? \u0013 RL on human feedback\nFlan-T5 base 250M \u0017 1.6K tasks + instructions(ﬁnetuning)\nFlan-T5 XL 3B \u0017 1.6K tasks + instructions(ﬁnetuning)\nTable 1: Summary of the models used for the in-context\ndemonstration experiments.\nTherefore, for Flan-T5, we adjusted the prompts to\noutput each box individually.9\nDemonstration/Test Mismatch for Form-mean-\ning Disentanglement (AltForms) As discussed\nin Sections 3.1–3.2, we additionally experimented\nwith a setup where the demonstration and test ex-\namples were mismatched in the form of the descrip-\ntions of the states and operations. Under this setup,\nthe models were evaluated with set of object names\nand phrasings of the state and operation descrip-\ntions that were different from the demonstration\nexamples (see Table 2). Except for the determiner\nthe and the preposition into, the two sets share no\nwords (although subwords may be shared depend-\ning on tokenization).\nScenarios with Greater Context Dependence\n(MoveContents, AmbiRef) As also discussed in\nSections 3.1–3.2, we experimented with two addi-\ntional scenarios with greater context dependence.\nThe ﬁrst scenario (MoveContents) introduces an\nadditional operation Move contents of Box N to\nBox M that moves all objects in Box N to Box M,\nthat does not provide an explicit enumeration of\nobjects being moved. This requires the model to\nrely on the preceding description to identify the set\nof objects being moved, further removing room for\nheuristics that allow the prediction of the ﬁnal state\nwithout composing the initial description and the\noperations in temporal order. The second scenario\n(AmbiRef) adds adjectival modiﬁcation to object\nnames (e.g., the big brain and the small brain ),\n9To verify that this difference in the task format does not\nunderestimate the accuracy of the GPT models, we also con-\nducted an experiment in which we prompted GPT to only\noutput the state of one box at a time. We found that, contrary\nto the Flan-T5 models, the accuracy of GPT was lower when\nwe prompted it to output individual boxes than prompting it\nto output all boxes, so we can rule out that this difference in\nthe task format is underestimating GPT’s performance.\nstate != initial state state = initial state\n0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7\n0.00\n0.25\n0.50\n0.75\n1.00\nNumber of operations affecting box state\nAccuracy\nModel\nFlan2T5 Base\nFlan2T5 XL\nGPT23 davinci\nGPT23.5 text2davinci2003\nRandom baseline\nFigure 2: Accuracy on state prediction after n opera-\ntions that affect a speciﬁc box. Left: predictions for\nboxes whose content differs from the initial state, Right:\npredictions for boxes whose content is the same as in\nthe initial state. Error bars show 95% CIs.\nwhere the adjective can be omitted in some of the\noperations, depending on the state of the box being\ndescribed. Speciﬁcally, the modiﬁer is dropped if\nthere is only one object of a speciﬁc type in a box\n(e.g., Move the brain from Box 1 to Box 2 if there\nis only one brain in Box 1). When predicting the\ncontents of a box, the model is instructed to output\nthe object type with the correct adjective to fully\ndisambiguate the referring expression for the ob-\nject. This again requires composition of the initial\ndescription and the operations in temporal order to\ncorrectly interpret the otherwise ambiguous object\nmentions. (See Appendix B for example scenarios\nand operations.)\n4.3 Evaluation\nWe estimated the entity tracking capacity of the\nmodels by computing the accuracy of predicting the\ncontents of each box after each operation. Given\nthat we rely on arbitrary-length cloze completion to\npredict the contents, we had to score unconstrained\ngenerations. While we only considered instances as\ncorrect where the output mentions all objects (and\nno additional objects) in a given box, our evaluation\nallowed for minor deviations from the exact form\nof the response. Namely, we allowed the objects\nto appear in any order, the object names may be\nseparated by commas or and, and both complete\nnoun phrases with a determiner (e.g.,the furby) and\nbare nouns (e.g., furby) are considered correct.\nThe task becomes intrinsically harder as more\noperations are applied to a speciﬁc box, since the\ninitial state description needs to be combined se-\nquentially with all subsequent operations. Further,\nsimilarly to our observation in Section 2, every\noperation changes the state of at most two boxes.\n3839\nOperation Base AltForms\nMove Move the car from Box 1 to Box 3. Pick up the furby in Container A and place it into Container C.\nRemove Remove the car from Box 1. Take the furby out of Container A.\nPut Put the car into Box 1. Place the furby inside Container A.\nTable 2: Different phrasings of the state-changing operations under the AltForms evaluation setup.\nThis implies that the number of datapoints corre-\nsponding to fewer operations is much greater than\nthe number of datapoints for more operations. For\nthese reasons, we report accuracy as a function of\nthe number of operations affecting a particular box\nrather than reporting aggregate accuracy, and show\nall results with 95% conﬁdence intervals.\n4.4 Results\nFigure 2 shows the prediction accuracy for differ-\nent number of operations that affected a box (e.g., 3\nindicates that three operations changed the content\nof the box after the initial state). The left panel\nshows the instances where the probed state differed\nfrom the initial state; the right panel shows the in-\nstances where the probed state was the same as the\ninitial state. As the left panel shows, only GPT-3.5\ntext-davinci-003 consistently outperformed the\n(strong) random baseline. While, not surprisingly,\nthe accuracy of this model also decreases as the\nnumber of operations increases, it still correctly\npredicted all contents of a box after 7 operations in\nmore than 25% of the cases. The Flan-T5 models,\non the other hand, seemed to ignore the operations\nand primarily predicted the initial state description,\nas indicated by the consistently high accuracy when\nthe ﬁnal state matches the initial state (right panel),\nas well as the consistently low accuracy when the\nﬁnal state deviates from the initial state (left panel).\nGPT-3 davinci also primarily repeated the initial\nstate, but as indicated by the steep decrease in the\nright panel, it was distracted by intervening opera-\ntions even when repeating the initial state.\nForm-meaning Disentanglement We addition-\nally evaluated text-davinci-003, the only model\nthat exhibited a non-trivial entity tracking capacity\nin the ﬁrst set of results, under the AltForms setup\nas described in Section 4.2 where the demonstra-\ntion examples have low lexical overlap with the test\nexamples. Figure 3 shows the prediction accuracy\nof text-davinci-003 on a representative subsam-\nple10 of our data. The blue line represents the per-\nformance when the descriptions in the demonstra-\n10For each number of operations n affecting a box, we sam-\npled 100 states with at least one example with n operations.\ntion and the test examples have low lexical overlap.\nAs the comparison to the original results (red line)\nshows, the disjoint demonstrations did lead to a\nsmall drop in performance when there were more\nthan two operations affecting a box. Nevertheless,\ntext-davinci-003 was able to predict the correct\nstate of entities in many cases, further adding sup-\nport for its non-trivial entity tracking capacity.\nComparison of demonstration formats (GPT23.5 text2davinci2003)\n0 1 2 3 4 5 6 7\n0.00\n0.25\n0.50\n0.75\n1.00\nNumber of operations affecting box state\nAccuracy\nidentical as eval. lexically disjoint from eval.\nFigure 3: Entity tracking accuracy of\ntext-davinci-003 with low lexical overlap be-\ntween demonstration and test examples (AltForms).\nGreater Context DependenceFinally, we eval-\nuated text-davinci-003 on two additional\ndatasets described in Section 4.2: the AmbiRef\ndataset where adjectival modiﬁcation can be omit-\nted depending on the current context, and the Move-\nContents dataset where a new operation Move con-\ntents of Box N to M that requires the contents of\nBox N to be contextually identiﬁed. Figure 4 shows\nthe performance of text-davinci-003 on these\ntwo datasets compared to our random baseline. As\nthese plots show, even in these more challenging\nAmbiguous object referring expressions Dataset including 'move contents' operation\n0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7\n0.00\n0.25\n0.50\n0.75\n1.00\nNumber of operations affecting box state\nAccuracy\nGPT23.5 text2davinci2003 Random baseline\nFigure 4: Entity tracking accuracy of\ntext-davinci-003 for the AmbiRef (left) and\nMoveContents (right) datasets.\n3840\ncases, we observe non-trivial entity tracking abili-\nties. However, as the number of operations grows,\nperformance rapidly approaches the random base-\nline, suggesting that entity tracking becomes in-\ncreasingly more brittle as more state changes need\nto be considered jointly.\n4.5 Discussion\nOur results show that among the models we eval-\nuated, only GPT-3.5 text-davinci-003 exhibit\nnon-trivial entity tracking behavior. While its per-\nformance does decrease as the number of opera-\ntions increases, the model still produced many accu-\nrate predictions even after six or seven sequences of\noperations. Furthermore, through the AltForms ex-\nperiment with low lexical overlap between demon-\nstration/test, we also ruled out the possibility that\nthe demonstrations are teaching the model this task\nor that the model is primarily relying on superﬁcial\nslot-ﬁlling heuristics. Therefore, we conclude that\ntext-davinci-003 does have some capacity to\ntrack discourse entities in linguistically expressed\ncontexts. To a lesser extent, we also observed this\ncapacity in the highly context-dependent AmbiRef\nand MoveContents experiments. This provides fur-\nther evidence against superﬁcial heuristics in the\nperformance we observe; at the same time, this\nhighlights scenarios in which even the most recent\nmodels exhibit difﬁculties.\nOn the other hand, entity tracking behavior did\nnot surface in GPT-3 davinci (likely of similar\nsize as GPT-3.5 text-davinci-003), a model pre-\ntrained primarily on text corpora on the next word\nprediction objective. This was also true for de-\nnoising models that have been ﬁnetuned on many\ntasks combined with instructions and demonstra-\ntions: the Flan-T5 models also showed near-zero\naccuracy on non-trivial examples.\nOur results overall show that there exists a lan-\nguage model that can perform entity tracking to\nsome degree, but this capacity does not necessarily\nsurface in all sufﬁciently large models trained on\nlarge corpora. Then, which factors are responsi-\nble for this difference? Given that davinci and\ntext-davinci-003 differ along at least two di-\nmensions (text-davinci-003 is based on a model\nthat was trained on code and it was trained with ad-\nditional human feedback (Ouyang et al., 2022); see\nTable 1), our initial results do not shed light on what\nexactly contributes to this difference. We therefore\nconducted a follow-up experiment where we com-\npared a range of GPT-3 and GPT-3.5 models to\nComparison of GPT23/3.5 models\n0 1 2 3 4 5 6 7\n0.00\n0.25\n0.50\n0.75\n1.00\nNumber of operations affecting box state\nAccuracy\nModel\nGPT23 davinci\nGPT23 text2davinci2001\nGPT23 davinci2instruct2beta\nGPT23.5 code2davinci2002\nGPT23.5 text2davinci2002\nGPT23.5 text2davinci2003\nCode\nyes\nno\nFigure 5: Accuracy on state prediction for different\nGPT-3 models. Solid lines denote models trained on\ncode and text, and dotted lines denote models mainly\ntrained on text.\nidentify factors that contribute to the stark differ-\nence between davinci and text-davinci-003.11\nTraining on Code Encourages Entity Tracking\nBehavior As Table 1 shows, two key dimensions\nof variation across models are additional training\non human feedback and pretraining on code. If\nadditional training on human feedback imbues lan-\nguage models with the ability to track entities, all\nmodels except for GPT-3 davinci and GPT-3.5\ncode-davinci-002 should be able to track enti-\nties. If, on the other hand, pretraining on code\nleads to better entity tracking, we expect all GPT-\n3.5 models to outperform GPT-3 on our task. As\nFigure 5 shows, GPT-3.5 models that have been\ntrained on code systematically outperformed GPT-\n3 models, including code-davinci-002 that was\nnot trained on human feedback. This suggests that\na substantial representation of code in the pretrain-\ning data is beneﬁcial for a language model’s entity\ntracking capacity to surface.\nA further question that our results so far do not\nanswer is to what extent model size matters and\nwhether models at the scale of Flan-T5 can also\nexhibit non-trivial entity tracking behavior. Since\nthere exist no smaller models that have been trained\nwith the same objective and training data as the\nGPT-3.5 models, we explore this question through\nﬁnetuning experiments with T5.\n5 Experiment 2: Finetuning\nWe investigated whether smaller models at the scale\nof T5 can learn to track entity states through a\nseries of experiments where we provide supervised\n11To limit inference costs, we used the same subsample of\ndata as in the AltForms experiment.\n3841\npretraining vs. random initialization base / Vocab / NumOps splits identical vs. disjoint training format\n0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7\n0.00\n0.25\n0.50\n0.75\n1.00\nNumber of operations affecting box state\nAccuracy\nExperiment Base random init. Vocab NumOps AltForms AltForms+NumOps random baseline\nFigure 6: Results for ﬁnetuned T5 models.\ntraining to the models.\n5.1 Train/test splits\nAs discussed in Section 3.1, one challenge of eval-\nuating entity tracking abilities is distinguishing this\ncapacity from simple heuristics such as templatic\nslot-ﬁlling. We therefore designed various types of\ntraining/evaluation mismatches that block several\npossible shortcuts as described below.\nBase SplitHere, we used the same format for train-\ning and evaluation examples. All initial states dif-\nfered across training and evaluation to block simple\nslot-ﬁlling heuristics, as discussed in Section 3.2.\nNumOps Split The NumOps split restricts the\nmaximum number of operations within a single\nexample in the training set to 2, but includes up\nto 12 operations in the evaluation set. This split\nwas intended to test whether a ﬁnetuned model is\nable to generalize to longer sequences of operations\nthan it has seen during ﬁnetuning.\nVocab/AltForms Splits The vocab split tests\nwhether objects that are not part of the set of ob-\njects used during training can also be adequately\ntracked. We compiled a list of comparatively in-\nfrequent object names (e.g., pomelo, furby, Flav-R-\nStraw; not in BNC) and sampled the training and\ntest sets using two completely disjoint sets of object\nnames. The training set used the infrequent object\nlist and the test set used the original object list. The\nAltForm split follows the design described in Sec-\ntion 4.2. These splits aim to tease apart whether the\nmodel learns to associate speciﬁc words/phrases\nwith the operations or whether ﬁnetuning leads to\nmore generalizable entity tracking behavior.\nAmbiRef/MoveContents Splits The AmbiRef\nand MoveContents splits follow the design de-\nscribed in Section 4.2. These splits aim to test\nwhether the model can learn to interpret operations\nthat are underspeciﬁed without considering the cur-\nrent state of the affected boxes. For these splits, the\ntraining and test examples share the same format.\n5.2 Models\nWe evaluated T5-base, the best-performing model\nin Li et al. (2021), by ﬁnetuning it on each of the\ndatasets described above. As an additional baseline,\nwe compared against T5 with randomly initialized\nparameters trained directly on our datasets.\n5.3 Results and Discussion\nPretrained T5 can Learn to Perform Entity\nTracking As shown in Figure 6 (left), ﬁnetuning\nT5 leads to near-perfect accuracy on the base split.\nThis suggests that the model is capable of learning\nthis task. Training a randomly initialized T5 did\nnot yield the same result: the accuracy of a model\ntrained from random weights is considerably lower,\ndue to the model almost exclusively predicting that\na box is empty. These two results suggest that pre-\ntraining is crucial for the model to be able to learn\nthis task. Furthermore, the model’s entity tracking\ncapacity is robust to novel object names at test time,\nwith only minor degradation on accuracy (Figure 6,\nmiddle). Training only on operation sequences\nwith a maximum length of 2 (NumOps split) leads\nto a larger degradation in performance for longer\noperation sequences, but even for longer operation\nsequences, the model is able to infer the correct\nﬁnal state in more than 45% of the cases. Finally,\nthe model performance does degrade substantially\nwhen the training examples have low lexical over-\nlap with test examples (Figure 6, right). Neverthe-\nless, model performance remains above the random\nbaseline when the model is trained on up to 12 op-\nerations (pink line). If we trained only on up to two\noperations (blue line), however, the performance\ndegradation was compounded and performance no\nlonger exceeded the random baseline. These results\nsuggest that ﬁnetuning on an entity tracking task\ndoes lead to entity tracking abilities that generalize\n3842\nAmbiguous object referring expressions Dataset including 'move contents' operation\n0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7\n0.00\n0.25\n0.50\n0.75\n1.00\nNumber of operations affecting box state\nAccuracy\nT5 Random baseline\nFigure 7: Accuracy of ﬁnetuned T5 for AmbiRef (left)\nand MoveContents (right) datasets.\nto many challenging scenarios. At the same time,\nhowever, performance rapidly degrades as the ﬁne-\ntuning and evaluation splits become increasingly\ndissimilar, and it remains an open question to what\nextent ﬁnetuning on a limited domain such as the\nboxes environment transfers to more general entity\ntracking abilities in more naturalistic discourse.\nInterpreting Context-Dependent Operations\nRemains Challenging If we include operations\nthat either contain ambiguous referring expressions\n(Figure 7, left) or a Move contents operation (Fig-\nure 7, right) in the ﬁnetuning and evaluation data,\nwe see a slight degradation in performance and the\nmodel no longer achieves near-perfect accuracy,\ndespite training and evaluating on examples of the\nsame format. In both cases, the model almost ex-\nclusively made mistakes with examples that require\nthe interpretation of context-dependent operations.\nThis suggests that the context-dependent operations\ndo compound the difﬁculty of entity tracking even\nwhen the models receive direct training signal.\n6 Conclusion\nWe set out to investigate whether pretrained LLMs\nexhibit entity tracking behavior. We developed a\ntask that allowed us to evaluate whether LLMs can\npredict the state of an entity based on an initial\nstate description and operations that act upon it. In\nthe ﬁrst set of experiments, we found that GPT-3\ndavinci, a vanilla pretrained language model, and\nFlan-T5, an instruction-ﬁnetuned language model,\nfail at this task and simply repeat the initial state\ndescription. The GPT-3.5 models (the core differ-\nence with the aforementioned models being code\nin pretraining corpora), on the other hand, exhib-\nited non-trivial entity tracking behavior. Even after\nmany operations affecting an entity state, they con-\nsistently performed above a strong random baseline.\nIn the second set of experiments, we showed that\nthis behavior can also to a large extent be learned\nby smaller models such as T5. When we ﬁnetune\nthe model on this task, it is also able to perform en-\ntity tracking on examples that differ along several\ndimensions from the training data. Taken together,\nour results provide evidence that (a) vanilla lan-\nguage models that have mainly been trained on text\ndata do not exhibit entity tracking abilities out of\nthe box, (b) pretraining on code and text data con-\nsiderably improves this ability,12 and (c) ﬁnetuning\non this task can make this behavior also surface in\nsmaller models that have primarily been trained on\ntext data, although it remains an open question how\ngeneral this ability is in smaller ﬁnetuned models.\nWhat are reasons behind the efﬁcacy of training\nboth on text and code? For producing correct code,\ntracking the states of variables is important. There-\nfore, to speculate, this kind of pretraining data may\nprovide a stronger signal for the model to track\nentities compared to pure text data. It could also\nbe that, as previously speculated (Potts, 2020; Mer-\nrill et al., 2021, i.a.), pretraining on code provides\nadditional grounding.\nThe present results also highlight the importance\nof the composition of the pretraining data. Our\nresults showed that models that are trained on the\nlanguage modeling objective on large corpora show\ndramatically different behavior on entity tracking\ndepending on whether the training data includes\nsubstantial amount of code or not. In this light, fu-\nture work could investigate the effect of pretraining\non code more widely, including its effect on the\nmodel representations and to what extent it facili-\ntates the emergence of world models in LMs.\nFinally, we also laid out several principles that\nshould be followed when evaluating state tracking\nabilities. Apart from these speciﬁc principles, we\nmake a more general point that in assessing abilities\nrelated to meaning, one needs to consider potential\nstrategies that the model could use to solve the\ntask and make sure that the test examples do not\nmimic the distributional patterns of the training or\nﬁnetuning data. Only then can we properly assess\nmeaning-related capacities of LMs.\nLimitations\nOne limitation of this work is that we are only con-\nsidering behavioral data which makes it difﬁcult to\n12This is also compatible with the observation by Sap et al.\n(2022) that GPT-3.5 performs considerably better than GPT-3\nin answering factual questions about object location changes.\nFurther, see Madaan et al. (2022) for other tasks for which\npretraining on code seems to be beneﬁcial.\n3843\nestablish a fully causal link between entity tracking\ncapacities and high performance on our task. En-\ntity tracking is a high-level linguistic behavior and\nmany other capacities are necessary for achieving\nhigh accuracy on our task. Therefore, we cannot\nrule out that differences in some other capacity,\nsuch as interpreting sentences compositionally (see\nBogin 2022 and Bogin et al. 2022 for evidence\nthat GPT-3 and GPT-3.5 models differ in their com-\npositional generalization behavior), are the main\ndriver for the differences in behavior we see across\nmodels.\nA possible criticism of our setup is that it re-\nquires short-term memory capacities that exceed\nthe memory capacities of most, if not all, humans.\nThat is, if we presented humans with the same in-\nput as the model, we would not expect them to be\nable to keep track of the contents of all 7 boxes due\nto memory limitations. Therefore we are poten-\ntially expecting models to do super-human entity\ntracking, a setup that has been criticized for model\nevaluations of other linguistic abilities (Lampinen,\n2022). We nevertheless believe that our task is justi-\nﬁed given the architecture of the evaluated models.\nTransformer-based models can look back to any to-\nken in the entire input sequence within their context\nwindow, so a proper comparison between humans\nand models would be to present humans with the\nfull description in written form and let them re-read\nthe description after being prompted to state the\ncontents of a box. While we did not formally eval-\nuate whether humans have this ability on a larger\npopulation, we personally did not have any trouble\ntracking the contents of boxes when we had access\nto the written description.\nRelatedly, we designed our task such that the\nentire description ﬁts within the context window of\npretrained language models. However, as we men-\ntioned in the introduction, entity tracking is an im-\nportant ability for understanding long contexts and\ngiven the limited context window, our results do\nnot apply to texts whose length exceeds a model’s\ncontext window, and likely different model archi-\ntectures will be necessary to perform proper entity\ntracking for longer texts.\nFurther, while we found that GPT-3.5 models as\nwell as ﬁnetuned T5 models can track entities in\nour task with higher accuracy than a strong random\nbaseline, our results also indicate that this behavior\nis not very stable once several operations act on an\nentity. Our results should therefore not be taken\nas justiﬁcation for using these models for critical\napplications where high accuracy is needed.\nLastly, we only evaluated English models in this\nwork. Given that we showed that even without high\nlexical overlap between the training and evaluation\nexamples, models can keep track of entities to some\nextent, it seems likely that our results also apply to\nother languages. However, whether this actually\nthe case remains an open question.\nAcknowledgements\nWe thank Jacob Andreas, Ellie Pavlick, Allyson Et-\ntinger, Tal Linzen, Will Merrill, and the members\nof the NYU Computation and Psycholinguistics lab\nfor discussions, and Belinda Li for sharing model\noutputs and details about their data preparation\nprocedures and experiments. We thank Cookie for\ncontributing to the authorship decision and for emo-\ntional support. This research was conducted in part\nthrough the NYU IT High Performance Computing\nresources, services, and staff expertise, and it was\nsupported by the NSF under Grant #2030859 to\nthe Computing Research Association for the CIFel-\nlows Project and the European Research Council\n(ERC) under the European Union’s Horizon 2020\nResearch and Innovation Program (Grant Agree-\nment #948878). Any opinions, ﬁndings, and con-\nclusions or recommendations expressed in this ma-\nterial are those of the authors and do not necessarily\nreﬂect the views of the National Science Founda-\ntion nor the Computing Research Association.\nReferences\nLaura Aina, Carina Silberer, Ionut-Teodor Sorodoc,\nMatthijs Westera, and Gemma Boleda. 2019. What\ndo entity-centric models learn? insights from en-\ntity linking in multi-party dialogue. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 3772–3783, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nDavid Bamman, Olivia Lewke, and Anya Mansoor.\n2020. An annotated dataset of coreference in En-\nglish literature. In Proceedings of the Twelfth Lan-\nguage Resources and Evaluation Conference, pages\n44–54, Marseille, France. European Language Re-\nsources Association.\nEmily M. Bender and Alexander Koller. 2020. Climb-\ning towards NLU: On meaning, form, and under-\nstanding in the age of data. In Proceedings of the\n3844\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 5185–5198, Online. As-\nsociation for Computational Linguistics.\nBen Bogin. 2022. How does GPT-3 perform on COVR-\n10 splits with in-context learning? Twitter thread.\nBen Bogin, Shivanshu Gupta, and Jonathan Berant.\n2022. Unobserved local structures make composi-\ntional generalization hard. In Proceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP 2022) , Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nAntoine Bosselut, Omer Levy, Ari Holtzman, Corin\nEnnis, Dieter Fox, and Yejin Choi. 2018. Simulat-\ning action dynamics with neural process networks.\nIn International Conference on Learning Represen-\ntations (ICLR 2018).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D. Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nHong Chen, Zhenhua Fan, Hao Lu, Alan Yuille, and\nShu Rong. 2018. PreCo: A large-scale dataset\nin preschool vocabulary for coreference resolution.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n172–181, Brussels, Belgium. Association for Com-\nputational Linguistics.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai,\nMirac Suzgun, Xinyun Chen, Aakanksha Chowdh-\nery, Alex Castro-Ros, Marie Pellat, Kevin Robin-\nson, Dasha Valter, Sharan Narang, Gaurav Mishra,\nAdams Yu, Vincent Zhao, Yanping Huang, Andrew\nDai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean,\nJacob Devlin, Adam Roberts, Denny Zhou, Quoc V .\nLe, and Jason Wei. 2022. Scaling instruction-\nﬁnetuned language models. arXiv:2210.11416.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nMarc-Alexandre Côté, Ákos Kádár, Xingdi Yuan, Ben\nKybartas, Tavian Barnes, Emery Fine, James Moore,\nMatthew Hausknecht, Layla El Asri, Mahmoud\nAdada, Wendy Tay, and Adam Trischler. 2019.\nTextworld: A learning environment for text-based\ngames. In Computer Games, pages 41–75, Cham.\nSpringer International Publishing.\nBhavana Dalvi, Niket Tandon, Antoine Bosselut, Wen-\ntau Yih, and Peter Clark. 2019. Everything hap-\npens for a reason: Discovering the purpose of ac-\ntions in procedural text. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4496–4505, Hong Kong,\nChina. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJeroen Groenendijk and Martin Stokhof. 1991. Dy-\nnamic predicate logic. Linguistics and Philosophy ,\n14:39–100.\nAditya Gupta and Greg Durrett. 2019a. Effective use\nof transformer networks for entity tracking. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 759–\n769, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAditya Gupta and Greg Durrett. 2019b. Tracking dis-\ncrete and continuous entity state for process under-\nstanding. In Proceedings of the Third Workshop\non Structured Prediction for NLP, pages 7–12, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nAria Haghighi and Dan Klein. 2010. Coreference res-\nolution in a modular, entity-centered model. In Hu-\nman Language Technologies: The 2010 Annual Con-\nference of the North American Chapter of the Associ-\nation for Computational Linguistics, pages 385–393,\nLos Angeles, California. Association for Computa-\ntional Linguistics.\nIrene Heim. 2002. File change semantics and the fa-\nmiliarity theory of deﬁniteness. In Formal seman-\ntics: The essential readings , chapter 9, pages 223–\n248. Blackwell Publishing Oxford.\nMikael Henaff, Jason Weston, Arthur Szlam, Antoine\nBordes, and Yann LeCun. 2017. Tracking the\nworld state with recurrent entity networks. In Inter-\nnational Conference on Learning Representations\n(ICLR 2017).\n3845\nFelix Hill, Antoine Bordes, Sumit Chopra, and Jason\nWeston. 2016. The Goldilocks principle: Reading\nchildren’s books with explicit memory representa-\ntions. In International Conference on Learning Rep-\nresentations (ICLR 2016).\nYangfeng Ji, Chenhao Tan, Sebastian Martschat, Yejin\nChoi, and Noah A. Smith. 2017. Dynamic entity\nrepresentations in neural language models. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1830–\n1839, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nHans Kamp, Josef Van Genabith, and Uwe Reyle. 2011.\nDiscourse Representation Theory , pages 125–394.\nSpringer Netherlands, Dordrecht.\nLauri Karttunen. 1976. Discourse referents. In J. D.\nMcCawley, editor, Syntax and Semantics Vol. 7 ,\npages 363–386. Academic Press.\nChloé Kiddon, Ganesa Thandavam Ponnuraj, Luke\nZettlemoyer, and Yejin Choi. 2015. Mise en place:\nUnsupervised interpretation of instructional recipes.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n982–992, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nAndrew Kyle Lampinen. 2022. Can language models\nhandle recursively nested grammatical structures?\na case study on comparing models and humans.\narXiv:2210.15303.\nHeeyoung Lee, Yves Peirsman, Angel Chang,\nNathanael Chambers, Mihai Surdeanu, and Dan\nJurafsky. 2011. Stanford’s multi-pass sieve corefer-\nence resolution system at the CoNLL-2011 shared\ntask. In Proceedings of the Fifteenth Conference\non Computational Natural Language Learning:\nShared Task, pages 28–34, Portland, Oregon, USA.\nAssociation for Computational Linguistics.\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettle-\nmoyer. 2017. End-to-end neural coreference reso-\nlution. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 188–197, Copenhagen, Denmark. Association\nfor Computational Linguistics.\nGeoffrey Leech, Paul. Rayson, and Andrew Wilson.\n2001. Word frequencies in written and spoken En-\nglish: based on the British National Corpus . Long-\nman.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nBelinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021.\nImplicit representations of meaning in neural lan-\nguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1813–1827, Online. Association for\nComputational Linguistics.\nKenneth Li, Aspen K Hopkins, David Bau, Fernanda\nViégas, Hanspeter Pﬁster, and Martin Wattenberg.\n2023. Emergent world representations: Exploring\na sequence model trained on a synthetic task. In\nInternational Conference on Learning Representa-\ntions (ICLR 2023).\nTal Linzen. 2020. How can we accelerate progress to-\nwards human-like linguistic generalization? In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 5210–\n5217, Online. Association for Computational Lin-\nguistics.\nSharid Loáiciga, Anne Beyer, and David Schlangen.\n2022. New or old? exploring how pre-trained\nlanguage models represent discourse entities. In\nProceedings of the 29th International Confer-\nence on Computational Linguistics, pages 875–886,\nGyeongju, Republic of Korea. International Com-\nmittee on Computational Linguistics.\nReginald Long, Panupong Pasupat, and Percy Liang.\n2016. Simpler context-dependent logical forms via\nmodel projections. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1456–\n1465, Berlin, Germany. Association for Computa-\ntional Linguistics.\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang,\nand Graham Neubig. 2022. Language models of\ncode are few-shot commonsense learners. In Pro-\nceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1384–\n1403, Abu Dhabi, United Arab Emirates. Associa-\ntion for Computational Linguistics.\nWilliam Merrill, Yoav Goldberg, Roy Schwartz, and\nNoah A. Smith. 2021. Provable limitations of ac-\nquiring meaning from ungrounded form: What will\nfuture language models understand? Transactions\nof the Association for Computational Linguistics ,\n9:1047–1060.\nMante S. Nieuwland and Jos J. A. Van Berkum. 2006.\nWhen peanuts fall in love: N400 evidence for the\npower of discourse. Journal of Cognitive Neuro-\nscience, 18(7):1098–1111.\nOpenAI. 2023. GPT-4 technical report.\narXiv:2303.08774.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n3846\n2022. Training language models to follow instruc-\ntions with human feedback. arXiv:2203.02155.\nChristopher Potts. 2020. Is it possible for lan-\nguage models to achieve language understanding?\nMedium blog post.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nOlga Uryupina, and Yuchen Zhang. 2012. CoNLL-\n2012 shared task: Modeling multilingual unre-\nstricted coreference in OntoNotes. In Joint Confer-\nence on EMNLP and CoNLL - Shared Task , pages\n1–40, Jeju Island, Korea. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21:1–67.\nAltaf Rahman and Vincent Ng. 2012. Resolving com-\nplex cases of deﬁnite pronouns: The Winograd\nschema challenge. In Proceedings of the 2012 Joint\nConference on Empirical Methods in Natural Lan-\nguage Processing and Computational Natural Lan-\nguage Learning, pages 777–789, Jeju Island, Korea.\nAssociation for Computational Linguistics.\nMaarten Sap, Ronan Le Bras, Daniel Fried, and Yejin\nChoi. 2022. Neural theory-of-mind? on the limits of\nsocial intelligence in large LMs. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3762–3780, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nSebastian Schuster and Tal Linzen. 2022. When\na sentence does not introduce a discourse entity,\ntransformer-based models still sometimes refer to it.\nIn Proceedings of the 2022 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 969–982, Seattle, United States. Association\nfor Computational Linguistics.\nIonut-Teodor Sorodoc, Kristina Gulordava, and\nGemma Boleda. 2020. Probing for referential\ninformation in language models. In Proceedings\nof the 58th Annual Meeting of the Association\nfor Computational Linguistics , pages 4177–4189,\nOnline. Association for Computational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nShubham Toshniwal, Sam Wiseman, Allyson Ettinger,\nKaren Livescu, and Kevin Gimpel. 2020. Learn-\ning to Ignore: Long Document Coreference with\nBounded Memory Neural Networks. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n8519–8526, Online. Association for Computational\nLinguistics.\nShubham Toshniwal, Sam Wiseman, Karen Livescu,\nand Kevin Gimpel. 2022. Chess as a testbed for\nlanguage model state tracking. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 36, pages 11385–11393.\nChen Feng Tsai, Xiaochen Zhou, Sierra S Liu, Jing Li,\nMo Yu, and Hongyuan Mei. 2023. Can large lan-\nguage models play text games well? current state-\nof-the-art and open questions. arXiv:2304.02868.\nOlga Uryupina, Ron Artstein, Antonella Bristot, Feder-\nica Cavicchio, Francesca Delogu, Kepa J Rodriguez,\nand Massimo Poesio. 2020. Annotating a broad\nrange of anaphoric phenomena, in a variety of gen-\nres: the arrau corpus. Natural Language Engineer-\ning, 26(1):95–128.\nChristopher Walker, Stephanie Strassel, Julie Medero,\nand Kazuaki Maeda. 2006. ACE 2005 multilin-\ngual training corpus. Linguistic Data Consortium,\nPhiladelphia: LDC2006T06.\nJason Weston, Antoine Bordes, Sumit Chopra, Alexan-\nder M Rush, Bart Van Merriënboer, Armand Joulin,\nand Tomas Mikolov. 2015. Towards AI-complete\nquestion answering: a set of prerequisite toy tasks.\narXiv:1502.05698.\nTerry Winograd. 1971. Procedures as a representation\nfor data in a computer program for understanding\nnatural language. Ph.D. thesis, Massachusetts Insti-\ntute of Technology.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nWei Wu, Fei Wang, Arianna Yuan, Fei Wu, and Ji-\nwei Li. 2020. CorefQA: Coreference resolution as\nquery-based span prediction. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 6953–6963, Online. As-\nsociation for Computational Linguistics.\n3847\nA Additional Analyses of Results from Li\net al. (2021)\nAs mentioned in Footnote 3, Li et al. (2021) con-\nducted two more experiments that prima facie pro-\nvided additional evidence for implicit meaning rep-\nresentations and state tracking abilities in language\nmodels. However, the data and setup of these two\nexperiments also likely overestimates models’ abil-\nities.\nIn the second set of probing classiﬁer experi-\nments, Li et al. (2021) used data generated using\nthe TextWorld engine (Côté et al., 2019). In this\nsetup, there is a textual description of several en-\ntities in a text-based game (e.g., a wooden door)\nas well as actions that a player took (e.g., opening\nthe wooden door). Their probing classiﬁer takes\nthe representations of either one entity and a prop-\nerty of that entity (e.g., that the wooden door is\nclosed) or the representations of two entities and\na relation between them (e.g., that the king-sized\nbed is inthe bedroom) and from these representa-\ntions, the classiﬁer has to predict whether a given\nproposition is true or false considering the initial\ndescription and the series of actions that a player\ntook. Only propositions that involve entities that\nhave been mentioned are probed. The issue with\nthis setup is that there are many propositions that\nare always true in both the training and evaluation\nsplits (e.g., in all the game simulations, the chest\ndrawer is in the bedroom, so the probing classiﬁer\nshould always return true for this input independent\nof the previous context). Furthermore, even for\nthe entity-property propositions and entity-relation-\nentity propositions which are not always true in the\ntraining and evaluation data, the data is very biased\nand a baseline that predicts the most common an-\nswer in the training data without taking the initial\ndescriptions or the user actions into account (a vio-\nlation of Desideratum 3, see Section 3.1), already\nachieves an accuracy of 88.5%, a number that puts\nthe reported probing classiﬁer accuracy of 96.9%\ninto a bit more context.\nFurther, Li et al. (2021), also presented an exper-\niment where they manipulated speciﬁc entity rep-\nresentations of a synthetic version of the Alchemy\ndataset. In this experiment, they ﬁrst encoded an\ninitial description D and an operation O which af-\nfected a beaker X using a language model that had\nbeen ﬁnetuned to predict the next operation, result-\ning in representation R1. Then, they encoded the\nsame initial representation D and an operation of\nthe form Drain n from beaker Y , where beaker Y\nwas always different from beaker X and n was the\namount of liquid that was in beaker Y according to\nthe initial description. This resulted in representa-\ntion R2. Then, they extracted the representation of\nthe initial description of beaker Y from representa-\ntion R2 and replaced the representation of beaker\nY in R1 with the corresponding representation in\nR2 to obtain Rmixed. They then used Rmixed as\nan input to T5 and showed that the predicted next\noperation based on Rmixed was considerably more\noften compatible with both operations (the oper-\nation encoded in R1 and the operation encoded\nin R2) compared to predicting the next operation\nfrom R1 or R2, which they took as evidence that\nthe token representations of the initial description\nencoded the entities state after performing the op-\neration. The issue with this experiment is that the\noperation from which R2 was computed was al-\nways of the form Drain n from Y th beaker, so the\nﬁnal state of beaker Y was always empty. There-\nfore, this experiment primarily shows that the token\ndrain affected the representation of the initial state\n(and subsequently the prediction of the next oper-\nation) but not more generally, that the actual state\nof the manipulated beaker Y is fully encoded in\nits initial state description. To answer that ques-\ntion, one would have to repeat this experiment with\nmore complex operations that do not give away\nthe ﬁnal state (a violation of Desideratum 2, see\nSection 3.1).\nIn summary, the additional experiments in Li\net al. (2021) also violate some of the desiderata we\nlaid out in Section 3.1, and therefore it is difﬁcult\nto draw conclusions about state tracking abilities\nfrom these experiments.\nB Example Input-Output Pairs\n(3) shows an example input-output pair from our\nbase dataset (NumOps on Box 6 = 2).\n(3) a. INPUT : Box 0 contains the painting,\nBox 1 contains the bell, Box 2 contains\nthe guitar, Box 3 contains the egg and\nthe mirror and the sheet, Box 4 con-\ntains the chemical, Box 5 contains the\ndisk and the wire, Box 6 contains the\nglass and the knife. Move the glass\nfrom Box 6 to Box 4. Put the gift into\nBox 5. Move the guitar from Box 2 to\nBox 6. Put the milk into Box 4. Re-\nmove the mirror and the sheet from\n3848\nBox 3. Box 6 __\nb. OUTPUT : contains the guitar and the\nknife.\n(4) shows an example input-output pair from the\ndataset with operations that contain referring ex-\npressions that are ambiguous without considering\nthe initial state and the previous operations (Nu-\nmOps on Box 6 = 2). Note that in order to correctly\ninterpret Move the guitar from Box 2 to Box 6, the\nmodel has to consider the information that the blue\nguitar (as opposed to the red guitar) was in Box\n2 prior to the operation. Hence, this operation is\nambiguous out of context.\n(4) a. INPUT : Box 0 contains the yellow\nbook and the green ﬂower and the red\nguitar, Box 1 contains the small bomb\nand the small book and the blue bone,\nBox 2 contains the blue guitar, Box 3\ncontains the blue bell, Box 4 contains\nthe green paper and the yellow note\nand the yellow television, Box 5 con-\ntains the yellow bell, Box 6 is empty.\nMove the guitar from Box 2 to Box 6.\nPut the blue wire and the big television\ninto Box 5. Move the ﬂower from Box\n0 to Box 6. Box 6 __\nb. OUTPUT : contains the blue guitar and\nthe green ﬂower.\n(5) shows an example input-output pair from the\ndataset with the Move contents of operation (Nu-\nmOps on Box 6 = 2). Note that in order to correctly\ninterpret Move the contents of Box 2 to Box 6, the\nmodel has to consider the information that the tea\n(as opposed to the red guitar) was in Box 2 prior to\nthe operation.\n(5) a. INPUT : Box 0 contains the fan and the\ngift and the letter, Box 1 contains the\nbeer and the mirror and the tie, Box\n2 contains the tea, Box 3 contains the\nboot, Box 4 contains the coat and the\nplate and the shirt, Box 5 contains the\nbottle, Box 6 is empty. Move the con-\ntents of Box 2 to Box 6. Put the dress\nand the painting into Box 5. Move the\nletter from Box 0 to Box 6. Box 6 __\nb. OUTPUT : contains the letter and the\ntea.\nC Implementation Details\nIn-context For GPT-3, we used the OpenAI API.\nWe used greedy decoding with the temperature pa-\nrameter set to 0, used a maximum target generation\nlength of 150, and used the newline character as\nan additional stop token. Inference time and model\nsize is not available for GPT-3, but we generated\nabout 16 million tokens in total. All GPT-3 ex-\nperiments that we report here in the paper were\nconducted in January 2023.\nFor Flan-T5, we used a beam size of 3 and a\nmaximum target generation length of 256. Other\nhyperparameters were kept as the default values of\nT5ConditionalGeneration in the HuggingFace\nlibrary. Inference for T5-XL took about 5 hours\non a single A100 GPU, and for T5 base, about 2\nhours.\nFinetuning We ﬁnetuned T5 for a single epoch us-\ning a batch size of 8 and a learning rate of1×10−4.\nIn initial explorations, increasing the number of\nﬁnetuning epochs did not yield substantial gains on\ndevelopment set performance, and was sometimes\neven harmful. Training and inference took around\n3 hours on a single RTX8000 GPU.\nD Additional Results\nD.1 GPT-3.5 Zero-shot Results\nAs mentioned in the main text, GPT-3.5 was able\nto to output the contents of boxes in the correct\nformat without any in-context demonstrations (see\nTable 5 for the prompt template).\nFigure 9 compares the performance of\ntext-davinci-003 in the 2-shot setting to the\nzero-shot setting. Model performance degraded\nslightly without demonstration examples, which\nsuggests that the examples are indeed helpful\nin guiding the model to correctly perform the\ntask. Nevertheless, we observed non-negligible\nperformance even in the zero-shot setting, which\ncorroborates the conclusion that GPT 3.5 exhibits\nnon-trivial entity tracking capacities.\nD.2 GPT-4 Results\nWe also probed the more recently released GPT-4\nmodel (OpenAI, 2023) through the OpenAI API.\nFigure 8 compares the performance of GPT-3.5\ntext-davinci-003 and GPT-4 on the base dataset\n(left) as well as the more challenging MoveCon-\ntents (middle) and AmbiRef (right) datasets dis-\ncussed in Section 4.2. Across all datasets, GPT-4\n3849\nBase Dataset including 'move contents' operation Ambiguous object referring expressions\n0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7 0 1 2 3 4 5 6 7\n0.00\n0.25\n0.50\n0.75\n1.00\nNumber of operations affecting box state\nAccuracy\nGPT23.5 text2davinci2003 GPT 24\nFigure 8: Entity tracking accuracy of text-davinci-003 and GPT-4 in 2-shot in-context learning experiments.\nGPT23.5 text2davinci2003: in2context vs. zero2shot\n0 1 2 3 4 5 6 7\n0.00\n0.25\n0.50\n0.75\n1.00\nNumber of operations affecting box state\nAccuracy\n22shot in2context zero 2shot\nFigure 9: Entity tracking accuracy of GPT 3.5 under\nin-context (2-shot) and zero-shot settings.\nperformed considerably better than GPT-3.5. But\nsimilarly to GPT-3.5, performance degraded as\nmore operations affected a box. Considering that\nno public information about the training data, train-\ning procedure or architectural details about GPT-4\nexists, we cannot draw any conclusions about the\nreason behind the improved performance of GPT-4\ncompared to previous models.\nE Prompts\nTable 3 shows the 2-shot prompts used for in-\ncontext experiments. Table 4 shows the 2-shot\nprompts used for the AltForms experiments, where\nthe demonstrations contain descriptions that have\nlower lexical overlap with the test examples. Ta-\nble 5 shows the zero-shot prompts discussed in Ap-\npendix D. All prompts are available at https://\ngithub.com/sebschu/entity-tracking-lms.\nF Dataset Statistics and License\nInformation\nSee Tables 6 and 7 for descriptive statistics of our\ndatasets. All datasets are released under the GNU\nGeneral Public License v3.0.\n3850\n2-shot prompt with all boxes queried at once (GPT-3 experiments)\nGiven the description after \"Description:\", write a true statement about all boxes and their\ncontents to the description after \"Statement:\".\nDescription: Box 0 contains the car, Box 1 contains the cross, Box 2 contains the bag and the\nmachine, Box 3 contains the paper and the string, Box 4 contains the bill, Box 5 contains the\napple and the cash and the glass, Box 6 contains the bottle and the map.\nStatement: Box 0 contains the car, Box 1 contains the cross, Box 2 contains the bag and the\nmachine, Box 3 contains the paper and the string, Box 4 contains the bill, Box 5 contains the\napple and the cash and the glass, Box 6 contains the bottle and the map.\nDescription: Box 0 contains the car, Box 1 contains the cross, Box 2 contains the bag and the\nmachine, Box 3 contains the paper and the string, Box 4 contains the bill, Box 5 contains the\napple and the cash and the glass, Box 6 contains the bottle and the map. Remove the car from\nBox 0. Remove the paper and the string from Box 3. Put the plane into Box 0. Move the map\nfrom Box 6 to Box 2. Remove the bill from Box 4. Put the coat into Box 3.\nStatement: Box 0 contains the plane, Box 1 contains the cross, Box 2 contains the bag and the\nmachine and the map, Box 3 contains the coat, Box 4 contains nothing, Box 5 contains the\napple and the cash and the glass, Box 6 contains the bottle.\nDescription: {description}\nStatement: Box 0 contains\n2-shot prompt with boxes queried individually (T5 experiments)\nGiven the description after \"Description:\", write a true statement about a box and the contents\nof this box according to the description after \"Statement:\".\nDescription: Box 0 contains the car, Box 1 contains the cross, Box 2 contains the bag and the\nmachine, Box 3 contains the paper and the string, Box 4 contains the bill, Box 5 contains the\napple and the cash and the glass, Box 6 contains the bottle and the map.\nStatement: Box 1 contains the cross.\nDescription: Box 0 contains the car, Box 1 contains the cross, Box 2 contains the bag and the\nmachine, Box 3 contains the paper and the string, Box 4 contains the bill, Box 5 contains the\napple and the cash and the glass, Box 6 contains the bottle and the map. Remove the car from\nBox 0. Remove the paper and the string from Box 3. Put the plane into Box 0. Move the map\nfrom Box 6 to Box 2. Remove the bill from Box 4. Put the coat into Box 3.\nStatement: Box 2 contains the bag and the machine and the map.\nDescription: {description}\nStatement: Box {boxnum} contains\nTable 3: Prompts with 2-shot in-context demonstrations.\n3851\n2-shot prompt with disjoint surface forms from test examples\nGiven the description after \"Description:\", write a true statement about all containers or boxes\nand their contents to the description after \"Statement:\".\nDescription: The biscotti is in Container A, the icicle is in Container B, the granite and the\nmachine are in Container C, the folio and the encyclopedia are in Container D, the bill is in\nContainer E, the spork and the jackknife and the frappuccino are in Container F, the clipper and\nthe ladybug are in Container G.\nStatement: Container A contains the biscotti, Container B contains the icicle, Container C\ncontains the granite and the machine, Container D contains the folio and the encyclopedia, Con-\ntainer E contains the bill, Container F contains the spork and the jackknife and the frappuccino,\nContainer G contains the clipper and the ladybug.\nDescription: The biscotti is in Container A, the icicle is in Container B, the granite and the\nmachine are in Container C, the folio and the encyclopedia are in Container D, the bill is in\nContainer E, the spork and the jackknife and the frappuccino are in Container F, the clipper and\nthe ladybug are in Container G. Take the biscotti out of Container A. Take the folio and the\nencyclopedia out of container D. Place the tetrapod inside Container A. Pick up the ladybug in\nContainer G and place it into Container C. Take the bill out of Container E. Place the gumball\ninside Container D.\nStatement: Container A contains the tetrapod, Container B contains the icicle, Container C\ncontains the granite and the machine and the ladybug, Container D contains the gumball, Con-\ntainer E contains nothing, Container F contains the spork and the jackknife and the frappuccino,\nContainer G contains the clipper.\nDescription: {description}\nStatement: Box 0 contains\nTable 4: Prompt with 2-shot in-context demonstrations that have disjoint surface forms.\nZero-shot prompt\nGiven the description after \"Description:\", write a true statement about all boxes and their\ncontents according to the description after \"Statement:\". Format the statement as Box 0 contains\nthe A, Box 1 contains the B, Box 2 contains the C, Box 3 contains the D, Box 4 contains the\nE, Box 5 contains the F, Box 6 contains the G. A, B, C, D, E, F, G are placeholders for the\ncontents of each box.\nDescription: {description}\nStatement: Box 0 contains\nTable 5: Prompt template for zero-shot experiments.\n3852\nScenarios Examples\nDataset Demonstration Test Demonstration Test\nComplete 1 990 14 90,090\nSubsample 1 491 14 5,012\nAmbiRef. 1 474 14 4,991\nMoveContents 1 451 14 4,956\nTable 6: Dataset statistics for Experiment 1.\nScenarios Examples\nDataset Train Dev Test Train Dev Test\nBase 990 220 990 90,090 20,020 90,090\nNumOps 990 220 990 20,790 20,020 90,090\nV ocab 990 220 990 90,090 20,020 90,090\nAltForms 990 220 990 90,090 20,020 90,090\nAltForms+NumOps 990 220 990 20,790 20,020 90,090\nAmbiRef 990 220 990 90,090 20,020 90,090\nMoveContents 990 220 990 90,090 20,020 90,090\nTable 7: Dataset statistics for Experiment 2.\n3853\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nLimitations on page 10.\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. We evaluate existing models, so we don’t see any potential risks other than misinter-\npreting our results. We hope we managed to limit the potential for misunderstanding by discussing in\ndetail how to interpret the results.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract, Section 1 (\"Contributions\" paragraph on p2)\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nWe developed a novel procedure for programmatically generating datasets and we will release that\ncode and the generated datasets upon publication (described in Section 3).\nWe also discuss an analysis of the probing datasets from Li et al. (2021) in Section 2 and Appendix A.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nWe cited the pretrained models that we used (Sections 4 and 5). We cite Li et al. and relevant source\ndatasets in Section 2 and Appendix A.\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nSee Appendix E.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nOur data mostly concerns moving objects from one box to another, so do not envision a risk scenario\nof use outside of research contexts.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. We generated the data from a set lexicon and templates and therefore, we can rule\nout that any identifying information is contained in the data.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSee Section 3.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSee Appendix E.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n3854\nC □\u0013 Did you run computational experiments?\nSee Sections 4 and 5.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSee Section 4 and Appendix C.\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSee Appendices B, C, and D.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSee Sections 4 and 5.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSee Section 4 and Appendix C.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n3855"
}