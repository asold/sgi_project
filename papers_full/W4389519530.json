{
  "title": "LLM-FP4: 4-Bit Floating-Point Quantized Transformers",
  "url": "https://openalex.org/W4389519530",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4319548791",
      "name": "Liu, Shih-Yang",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2079374216",
      "name": "Liu Ze-Chun",
      "affiliations": [
        "META Health"
      ]
    },
    {
      "id": "https://openalex.org/A2368696699",
      "name": "Huang, Xijie",
      "affiliations": [
        "University of Hong Kong",
        "Hong Kong University of Science and Technology"
      ]
    },
    {
      "id": null,
      "name": "Dong, Pingcheng",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A4202058384",
      "name": "Cheng, Kwang-Ting",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4313069943",
    "https://openalex.org/W4377864164",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3038470071",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3034940165",
    "https://openalex.org/W3202028501",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4292119927",
    "https://openalex.org/W4378770729",
    "https://openalex.org/W2998218113",
    "https://openalex.org/W4309591680",
    "https://openalex.org/W3154596443",
    "https://openalex.org/W4221159612",
    "https://openalex.org/W2981751377",
    "https://openalex.org/W3166859509",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W4297812065",
    "https://openalex.org/W4292760969",
    "https://openalex.org/W4304080501",
    "https://openalex.org/W4360834835",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287812978",
    "https://openalex.org/W3211787299",
    "https://openalex.org/W3203149535"
  ],
  "abstract": "We propose LLM-FP4 for quantizing both weights and activations in large language models (LLMs) down to 4-bit floating-point values, in a post-training manner. Existing post-training quantization (PTQ) solutions are primarily integer-based and struggle with bit widths below 8 bits. Compared to integer quantization, floating-point (FP) quantization is more flexible and can better handle long-tail or bell-shaped distributions, and it has emerged as a default choice in many hardware platforms. One characteristic of FP quantization is that its performance largely depends on the choice of exponent bits and clipping range. In this regard, we construct a strong FP-PTQ baseline by searching for the optimal quantization parameters. Furthermore, we observe a high inter-channel variance and low intra-channel variance pattern in activation distributions, which adds activation quantization difficulty. We recognize this pattern to be consistent across a spectrum of transformer models designed for diverse tasks such as LLMs, BERT, and Vision Transformer models. To tackle this, we propose per-channel activation quantization and show that these additional scaling factors can be reparameterized as exponential biases of weights, incurring a negligible cost. Our method, for the first time, can quantize both weights and activations in the LLaMA-13B to only 4-bit and achieves an average score of 63.1 on the common sense zero-shot reasoning tasks, which is only 5.8 lower than the full-precision model, significantly outperforming the previous state-of-the-art by 12.7 points. Code is available at: https://github.com/nbasyl/LLM-FP4.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 592–605\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLLM-FP4: 4-Bit Floating-Point Quantized Transformers\nShih-yang Liu∗1, Zechun Liu∗2, Xijie Huang1, Pingcheng Dong1, Kwang-Ting Cheng1\n1Hong Kong University of Science and Technology,2Meta Reality Labs\n{sliuau, xhuangbs, pingcheng.dong}@connect.ust.hk\nzechunliu@meta.com\ntimcheng@ust.hk\nAbstract\nWe propose LLM-FP4 for quantizing both\nweights and activations in large language mod-\nels (LLMs) down to 4-bit floating-point val-\nues, in a post-training manner. Existing post-\ntraining quantization (PTQ) solutions are pri-\nmarily integer-based and struggle with bit\nwidths below 8 bits. Compared to integer\nquantization, floating-point (FP) quantization\nis more flexible and can better handle long-tail\nor bell-shaped distributions, and it has emerged\nas a default choice in many hardware platforms.\nOne characteristic of FP quantization is that its\nperformance largely depends on the choice of\nexponent bits and clipping range. In this re-\ngard, we construct a strong FP-PTQ baseline\nby searching for the optimal quantization pa-\nrameters. Furthermore, we observe a high inter-\nchannel variance and low intra-channel vari-\nance pattern in activation distributions, which\nadds activation quantization difficulty. We rec-\nognize this pattern to be consistent across a\nspectrum of transformer models designed for\ndiverse tasks, such as LLMs, BERT, and Vision\nTransformer models. To tackle this, we propose\nper-channel activation quantization and show\nthat these additional scaling factors can be repa-\nrameterized as exponential biases of weights,\nincurring a negligible cost. Our method, for the\nfirst time, can quantize both weights and acti-\nvations in the LLaMA-13B to only 4-bit and\nachieves an average score of 63.1 on the com-\nmon sense zero-shot reasoning tasks, which is\nonly 5.8 lower than the full-precision model,\nsignificantly outperforming the previous state-\nof-the-art by 12.7 points. Code is available at:\nhttps://github.com/nbasyl/LLM-FP4.\n1 Introduction\nSince the introduction of transformer architec-\nture (Vaswani et al., 2017), transformers have\nsuperseded recursive neural networks, emerging\nas the dominant architecture in numerous natu-\nral language processing (NLP) tasks (Kenton and\n*These authors contributed equally to this work\nToutanova, 2019; Lewis et al., 2020). The trans-\nformative impact of the transformer has been fur-\nther propelled by the emergence of models like\nGPT (Brown et al., 2020; OpenAI, 2023), cata-\npulting the popularity of this architecture to new\nheights. Meanwhile, the versatility of transformers\nextends beyond NLP, encompassing diverse do-\nmains such as vision (Dosovitskiy et al.; Touvron\net al., 2021), audio (Akbari et al., 2021), etc. This\ntrend towards a unified architecture for different\nmodalities represents a groundbreaking develop-\nment within the realm of deep learning.\nHowever, the advancements in transformer per-\nformance are accompanied by a corresponding in-\ncrease in model size and computational costs (Ka-\nplan et al., 2020). This poses significant challenges\nwhen attempting to leverage the full potential of\ntransformer models in use cases where memory\nor computational resources are limited. Despite\nthe extensive research and widespread adoption\nof transformers, the field of transformer compres-\nsion remains relatively underexplored. To address\nthis gap, our study focuses on the compression\nof transformers, especially through floating-point\npost-training quantization techniques.\nPost-training quantization (PTQ) offers the ad-\nvantages of simple to use with minimal fine-tuning\nrequirements (Nagel et al., 2020; Cai et al., 2020).\nExisting PTQ solutions for transformers primar-\nily focus on integer (INT) quantization (Liu et al.,\n2021; Yuan et al., 2022), which can be effective\nin certain scenarios but often break down when bit\nwidths are below 8 bit. On the other hand, floating-\npoint (FP) quantization has gained significant trac-\ntion as a more flexible alternative, capable of better\naccommodating various activation and weight dis-\ntributions. In fact, FP8 has emerged as the default\nchoice in various hardware platforms, including the\nNVIDIA H100.\nDifferent from integer (INT) quantization, a par-\nticular challenge in floating-point (FP) quantiza-\n592\ntion is how to select appropriate exponent bits and\nscale parameters. Improper parameter choices can\nlead to subpar or divergent quantization results. To\ntackle this challenge, we introduce a robust recipe\nfor FP quantization, which leverage layer-wise re-\nconstruction to jointly search for optimal exponent\nbits and maximum values. Compared to previous\napproaches that utilize gradient updates for expo-\nnent bits (Kuzmin et al., 2022), our search-based\nmethod proves to be more stable and consistently\ndelivers desirable quantization results, which estab-\nlishes a strong baseline for FP-PTQ.\nFurthermore, our investigation uncovers an in-\ntriguing pattern of activation distributions in trans-\nformers, characterized by high inter-channel vari-\nance and low intra-channel variance. Similar pat-\nterns are also observed in previous works (Xiao\net al., 2022; Dettmers et al., 2022), while we argue\nthat this pattern is inherent to transformer architec-\ntures and not limited to specific tasks, as we have\nobserved consistent patterns not only in large lan-\nguage models but also in BERT model and even\nvision transformers. Motivated by these findings,\nwe introduce a novel pre-shifted exponent biasfor\nFP quantization of transformers. Concretely, we\nleverage the per-channel activation variance com-\nputed from calibration data and reparameterize\nthese scales as the exponential bias of the corre-\nsponding FP quantized weight vectors. This ap-\nproach effectively addresses the challenge posed\nby high inter-channel variance while incurring neg-\nligible computational cost.\nIn summary, we study floating-point post-\ntraining quantization (PTQ) for transformer archi-\ntectures, and the contribution of this paper includes:\n•We propose a search-based framework for de-\ntermining the optimal exponent bias and maximal\nquantization value. This method outperforms ex-\nisting techniques in terms of stability and perfor-\nmance, establishing a strong baseline for floating-\npoint post-training quantization.\n•We propose a novel technique, pre-shifted expo-\nnent bias, which effectively addresses the challenge\nof high inter-channel variance in the transformer\nwith negligible computational overhead.\n•Experimental results demonstrate that the pro-\nposed method yields the first usable FP4 weight\nand activation quantized LLaMA-13B model with\nmere 5.8-point degradation in zero-shot reasoning\ntasks against the full-precision model, reducing the\ngap by ∼70% compared to the previous SoTA.\n•We further extend our method to BERT and vi-\nsion transformers. It surpasses the previous best 4-\nbit quantized BERT by 7.8 points on GLUE dataset\nand achieves 31.4 points higher accuracy compared\nto the previous SoTA ViT quantization method for\n4-bit DeiT-S on ImageNet dataset.\n2 Related Works\n2.1 Post-Training Quantization\nModel quantization can be mainly categorized\ninto quantization-aware training (QAT) and post-\ntraining quantization (PTQ), depending on whether\nit involves additional training for weight fine-\ntuning or not. Most PTQ studies are primarily\nfocused on convolutional neural networks (CNNs)\n(Nagel et al., 2020; Li et al., 2021; Wu et al., 2020;\nCai et al., 2020; Nagel et al., 2019). However, with\nthe growing popularity of transformer-based mod-\nels, only a limited number of works (Bondarenko\net al., 2021; Yuan et al., 2022; Ding et al., 2022)\nhave been conducted to realize PTQ on transform-\ners. Moreover, the existing works primarily focus\non visual transformer models and exhibit inferior\nperformance when the bit width is below 8. There-\nfore, in this work, we delve into the challenges of\nthe low-bit PTQ for language transformers.\n2.2 Floating-Point Quantization\nFloating-point (FP) quantization has emerged as a\npromising alternative to integer quantization due\nto its ability to handle long-tail distributions, and\noffers increased flexibility (Kuzmin et al., 2022).\nAdditionally, modern GPUs such as H100 (Micike-\nvicius et al., 2022) now support FP quantization.\nNonetheless, minimal research has been conducted\non FP quantization. Only (Kuzmin et al., 2022) pro-\nposes a general FP8 quantization scheme primarily\nfor vision tasks, and (Zhang et al., 2023) adopts\na mixture of FP and INT formats quantization for\nLLMs. In this work, we propose FPQ baseline as\na general guideline for low-bit floating-point PTQ\nto compress language transformer models.\n3 Preliminaries\n3.1 Formulation of Floating-Point Variables\nA standard floating-point number is represented as:\nXFP = (−1)s2p−b(1 + d1\n2 + d2\n22 + ...+ dm\n2m) (1)\nwhere s∈{0,1}is the sign bit. di ∈{0,1}is ith\nmantissa bit, mdenoted number of mantissa bits.\n593\n0 2! 2\" 2# #$%&=2−2'$2(\"!'!)=14!=2$=2\n(=2!'$\n)ℎ+,-*++./<2!\n-*++./\n)ℎ+,log\"-*++./≥1(=2,-.\"/#$$01'$ ./=2'23log\"-*++./=log\"-*+++67\n(\nFigure 1: An illustration of floating-point (FP) quantization process using FP5 (E2M2) positive axis. The real-valued\nclipped X′′\nR in Eq. 5 is rescaled by the real-valued scaling factor ˜α. Then, the quantization step-size vis determined\nby the range [2p,2p + 1) in which X′′\nR\n˜α falls (Eq. 9). Here, p∈{0,1,..., 2e−1}is the exponent bit value. Lastly, X\ncan be quantized to low-bit floating point values simply by XFP = ˜α·v·\n⌊\nX′′\nR\n˜α·v\n⌉\n(Eq. 8).\npis an integer in [0,2e −1], and edenotes number\nof exponent bits. bis an integer exponent bias. A\nfloating point with j number exponent bits and k\nmantissa bits is denoted as FP format EjMk.\n3.2 Floating-Point Quantization Process\nIn integer quantization, the real-valued variableXR\nis quantized to an integer XINT with the following\nformula:\nXINT = α\n⌊\nClip\n(XR\nα ,Qmin,Qmax\n)⌉\n(2)\nwhere ⌊·⌉is the rounding function. XR is the\nreal-valued variable, αrepresents the full-precision\nscaling factor, and Qmin, Qmax are the min/max\nvalue of the quantization range. Similarly, a real-\nvalued variable XR can be converted to floating-\npoint XFP in two steps.\n(1) Scale and clip. In FP quantization, we also\nscale and clip the real-valued variable before quan-\ntization as:\nX′\nR = Clip(XR,Qmin,Qmax) (3)\nwhere the min/max value range of signed floating-\npoint quantization can be calculated from Eq.1:\nQmax = −Qmin = (2 −2−m)22e−b−1 (4)\nHere the integer exponent bias b is another ad-\njustable hyperparameter controlling Qmax and\nQmin, which has similar functionality as α. There-\nfore, for simplicity, we reformulate Eq. 3 as:\nX′′\nR = Clip\n(\nXR, ˜Qmin, ˜Qmax\n)\n, (5)\nwhere\n˜Qmax = αQmax = α·(2 −2−m)22e−b−1\n= α·2−b ·(2 −2−m)22e−0−1\n= 2−˜b ·(2 −2−m)22e−0−1\n(6)\nNote that we combine the tensor-wise real-valued\nscaling factor α with integer exponent bias b to\nform a new scaling factor ˜α = 2 −˜b = 2 −b ·α.\nHere ˜bdenotes a relaxed tensor-wise real-valued\nexponent, and we can derive ˜b from the desired\nclipping value ˜Qmax from Eq. 6 as:\n˜b= 2e −log2 ˜Qmax + log2(2 −2−m) −1 (7)\n(2) Compare and quantize. Different from inte-\nger quantization, which simply utilizes the round-\ning function to convert the real-valued variables\nto quantized ones, in floating-point quantization,\nthere is an additional step of comparing X′′\nR with\nquantization levels and then quantize:\nXFP = ˜α·v·\n⌊X′′\nR\n˜α·v\n⌉\n(8)\nwhere X′′\nR is clipped real-valued variable (Eq. 5),\n˜αis the tensor-wise floating-point scaling factor,\nand vis an integer power of 2.\nv=\n{\n2⌊log2|X′′\nR|+˜b⌋−m if ⌊log2|X′′\nR|+˜b⌋≥1\n21−m otherwise (9)\nHere we select the quantization level vaccording\nto the magnitude of X′′\nR\n˜α , which equals to X′′\nR ·2˜b.\nThen the floating-point quantized variables can be\nderived with Eq.8. The illustration of the quantiza-\ntion process is in Fig. 1, detailed explanation can\nalso be found in (Micikevicius et al., 2022).\n3.3 Floating-Point Matrix Multiplication\nWith the floating-point quantized variables, the ma-\ntrix multiplication is formulated as:\nOi,k\nout = Xi,:\nFPW:,k\nFP = ˜αX ˜αk\nW\n˜Xi,:\nFP ˜W:,k\nFP (10)\nHere in per-tensor activation quantization and per-\nchannel weight quantization, Xi,:\nFP denotes ith row\n594\nin the activation matrix and W:,k\nFP denotes kth col-\numn in the weight matrix, such that each element\nOi,k\nout in the output matrix is computed by the prod-\nuct of two real-valued scalars ˜αX and ˜αk\nW times\nthe corresponding quantized activation and weight\nvectors. We depict all the possible quantization\ngranularity options that support such efficient ma-\ntrix multiplication in Appendix D.\n4 Method\nIn this section, we begin by introducing our joint\nformat and max value search, which establishes\nour strong baseline and already achieves state-of-\nthe-art results at 8-bit and 6-bit quantization. Then\nwe present an efficient pre-shifted exponent bias\nto tackle the catastrophic high inter-channel activa-\ntion variance in transformer models and push the\nquantization limit to 4-bit.\n4.1 Joint Format and Max Value Search\nThe objective of post-training quantization is to\nminimize the perturbation ( δX = XFP −XR)\nintroduced by quantization to the pre-trained real-\nvalued network:\nmin E[L(XR + δX) −L(XR)] (11)\nIn this study, we adopt the setting presented in\n(Choukroun et al., 2019; Wu et al., 2020), which\nassumes a positive correlation between the change\nin the intermediate output of the quantized model\nand Eq. 11. Therefore, minimizing the distance\nbetween the intermediate output of the quantized\nlayer ( ˆO) and the output of the original layer (O)\nleads to minimize Eq. 11. Hence, the objective loss\nmetric is formulated as:\nmin ( ˆO −O)2 (12)\nwhich is used to search for the optimal FP quantiza-\ntion function in the following proposed framework.\nThe challenges in FP quantization arise from its\nsensitivity to the quantization format and clipping\nrange. Undesirable format selection will result in a\ncatastrophic error rate. In addition, we observe that\nthe optimal clipping range varies depending on the\nformat used. Previous work (Kuzmin et al., 2022)\non floating-point (FP) quantization-aware training\n(QAT) proposed to learn both the FP format and\nmaximum value with gradients. However, we find\nthis method suffers from over-fitting in PTQ, with\naccuracy being even worse than naïve MinMax\nmethod, details can be found in Appendix E. In-\nstead, we propose a search-based algorithm that\njointly determines the optimal format and its asso-\nciated clipping range to address this challenge.\nThe searching process is conducted layer by\nlayer with the metric of minimizing Eq. 12. The\noutput of matrix multiplication corresponding to\neach sub-module is denoted as O = XY, where\nY can be either a weight tensor W or another acti-\nvation tensor.\nThe search space of q-bit FP format includes all\nformats except for the format with an exponent bit\nequal to 0, as the quantization of the format with\nan exponent bit equal to 1 already degenerates to\nINT quantization. We search for the real-valued ex-\nponent bias ˜b, which equals to the logarithm of the\nscaling factor. We initialize˜bX and ˜bY from Eq. 7\nwith Qmax equals the maximum value of |XR|and\n|YR|, respectively. We then define the search space\nof ˜bX and ˜bY by linearly dividing [γ1\n˜binit\nX ,γ2\n˜binit\nX ]\nand [γ1\n˜binit\nY ,γ2\n˜binit\nY ] into kintervals, where γ1 and\nγ2 are empirically set to 0.01 and 1.2, and k= 100.\nThe search process is outlined in Alg.1. We\nsearch the quantization scheme in all the matrix\nmultiplication layers in parallel following (Yuan\net al., 2022; Bai et al., 2022). The algorithm can be\ndivided into two parts. (1) Do forward propagation\nto store the intermediate raw output of each layer\nl. (2) Iteratively update the optimal format and bi-\nases for each layer for three rounds by minimizing\nthe reconstruction metric (Eq. 12). We name this\nsearch-based framework as Floating Point Quanti-\nzation Baseline(FPQ baseline), and it can already\nachieve state-of-the-art results on both 8-bit and 6-\nbit settings.\n4.2 Pre-Shifted Exponent Bias\nIn transformer architectures, we observed an in-\ntriguing phenomenon of high inter-channel vari-\nance. As shown in Fig.2, the magnitudes of values\nwithin the same channel are close to each other\nbut exhibit significant differences across different\nchannels. This phenomenon is not only observed\nin language models (i.e., LLaMA and BERT) but\nalso significant in vision transformer models. Since\noutlier channels are often orders of magnitude big-\nger than the rest, they will dominate the quantiza-\ntion precision of the quantized tensor, resulting in\nless representation capacity for those channels with\nsmaller magnitudes (Xiao et al., 2022). This makes\ntensor-wise or token-wise scaling factor insufficient\nfor accurate activations quantization.\n595\nAlgorithm 1 FPQ baseline\n1: Input: Calibration dataset, Full-precision Model M,\nQuantization format search space RX (e.g., RX =\n{E3M0,E2M1,E1M2} for FP4), number of round\nn= 3,\n2: Output: FP qQuantized model\n3: for lin 1st to Lth layer in M do\n4: Forward & collect raw output Ol = XlYl of layer l;\n5: end for\n6: for lin 1st to Lth layer in M do\n7: Initialize the FP format search space w.r.t Xl and Yl\nas RX = {r1\nX,r2\nX,...,r t\nX} and RY = {r1\nY,r2\nY,....rt\nY}.\n8: Initialize bias ˜bi\nX,˜bi\nY with Eq.7 for each format can-\ndidate ri\nX ∈ RX and ri\nY ∈ RY.\n9: Generate search space of ˜bX in t formats to be\n[γ1\n˜binit\nX ,γ2\n˜binit\nX ] and ˜bY to be [γ1\n˜binit\nY ,γ2\n˜binit\nY ].\n10: for 0 to n do\n11: Search for ˜bi\nX w.r.t each ri\nX that minimizes Eq.12\n12: Search for ri\nX ∈ RX that minimizes Eq.12\n13: Search for ˜bi\nY w.r.t each ri\nY that minimizes Eq.12\n14: Search for ri\nY ∈ RY that minimizes Eq.12\n15: end for\n16: end for\nHowever, applying per-channel scaling factors\nfor activations poses challenges to efficient matrix\nmultiplication, because the scaling factor is not\na shared constant along the multiplication direc-\ntion and cannot be extracted as Eq. 10. To address\nthis challenge, we introduce pre-shifted exponent\nbias, which allows us to calculate per-channel scal-\ning factors from activations. These scaling factors\nare then re-parameterized as the exponent biases\nof the corresponding weights. This method effec-\ntively handles high inter-channel variance while\nmaintaining nearly identical efficiency to per-tensor\nquantization.\nRecalling in Eq. 7, we extracted the tensor-wise\ninteger exponent bias b and times it with real-\nvalued scaling factor αand becomes a new scaling\nfactor ˜α= 2−˜b = 2−b ·α. Then, the floating-point\nquantization formula in Eq. 13 becomes:\nXFP =2−˜b(−1)s2p−0(1+d1\n2 +d2\n22 +...+dm\n2m) (13)\nWe note that after the bias is absorbed in the scal-\ning factor, the original bias term ( bori) in the FP\nformula is always zero. In dealing with the inter-\nchannel variance, we devise an innovative usage\nof this integer exponent bias: we set it to be a per-\nchannel variant (bori ∈Zc).\nThen the calculation of the channel-wise integer\nbias vector (bori) is very straightforward. We first\ncalculate the initial per-channel real-valued scal-\ning factor (2−˜bj ) from the per-channel maximum\nFigure 2: Magnitude of the output activations of the\nfeed-forward network blocks in LLaMA-7B, BERT, and\nDeiT.\nvalues:\n˜bj=2e−log2(max(|X:,j\nR |) ) + log2(2−2−m)−1 (14)\nHere X:,j\nR denotes the jth channel in the activation\nmatrix. Then we separate ˜b to a tensor-wise real-\nvalued scaling factor plus a channel-wise integer\nscaling factor:\n˜b = ˜ρ+ bori\n= ˜ρ+ clip(⌊˜b −˜ρ⌉,0,2e−1)\n(15)\nwhere ˜ρ ∈R1, bori ∈Zc. Then the formula for\none of the entries in the jth channel of X can be\nrewrote as follows:\nXFP =2−˜bj (−1)s2p−0(1 + d1\n2 + ...+ dm\n2m)\n=2−˜ρ(−1)s2p−bori\nj (1 + d1\n2 + ...+ dm\n2m)\n(16)\nNote that the bias bori is constrained to integers\nwithin [0,2e −1], compatible with the standard\nfloating-point number calculation. Nevertheless,\nadding different biases for each channel during\ninference may still cause some extra hardware\noperations. Thus, we re-parameterized the per-\nchannel activation bias into a weight tensor and\npre-computed the weights using the calibration set.\nThis way, the exponent biases shifting only hap-\npens in the calibration stage. Then, an element in\njth channel of activation tensors X becomes:\nXFP =2−˜ρ(−1)s2p−0(1+ d1\n2 +...+ dm\n2m) (17)\n596\n!\"!,!!\"!,#\n!\"#,!!\"#,#\n#$! #$#\n!%!,!!%!,#\n!%#,!!%#,#\n#$!$#$#$\n!\"!,!!\"!,#\n!\"#,!!\"#,#\n&+#$!%&'\n&+#$#%&'\n!%!,!!%!,#\n!%#,!!%#,#\n#$!$#$#$\n#$!%&'\n#$#%&'\n!\"!,!!\"!,#\n!\"#,!!\"#,#\n&Decompose#$(Eq.15)\nPre-shiftedexponentbias#$!%&'(Eq.18)\nTensor-wisescalingfactor&(Eq.17)\n'(( !%′!,!\n!%′#,!\n!\"!,!!\"!,#'($!\nEfficientmatrixmultiplication(Eq.22)\n* *\n!%!,!!%!,#\n!%#,!!%#,#\n#$!%&'\n#$#%&'\n!%′!,!!%′!,#\n!%′#,!!%′#,#\nPre-computeweightstostore!\"!\"#$inlow-bitFPformat\n#$-%&'∈ℤ!#$-∈ℝ!\n'((=2)*'($!=2)+,!\" \n#$.$∈ℝ!&∈ℝ!\n(a)\n(b)\n(c)\nFigure 3: Overview of pre-shifted exponent bias method : (a) Search phase: The real-valued channel-wise\nscaling exponent bias for activations (˜bj) is partitioned into a real-valued tensor-wise exponent bias (ρ), and the\ninteger-based channel-wise exponent bias (˜bori\nj ). (b) Reparameterization and weight pre-computation: Once the\noptimal values are determined on the calibration set, ˜bori\nj are re-parameterized into the weight tensor. The weights\nare pre-computed to apply the bias, therefore this is a one-time cost. (c) Inference phase: The method leverages\nefficient matrix multiplication between low-bit floating-point matrices.\nand the corresponding weight element in jth row\nof the weight tensor W becomes:\nWFP =2−˜bW\n(−1)s2p−bori\nj (1+ d1\n2 +...+ dm\n2m) (18)\nAs result, efficient matrix multiplication in Eq.10\nis reformulated as:\nOi,k\nout=Xi,:\nFPW:,k\nFP = ˜αX ˜αk\nW\n˜Xi,:\nFP(β⊙˜W:,k\nFP) (19)\nwhere ⊙is the element-wise multiplication, β =\n2−bori\nand (β ⊙ ˜W:,k\nFP) can be pre-calculated\nand stored in low-bit FP format. We depict the\noverall pre-shifted exponent biasmethod in Fig.3.\nThis method applies to quantizing all the fully-\nconnected layers. During the search process, we\ninitialize ˜ρX as the minj(˜bj). Then, we fixed ˜bX\nto be the bias calculated from the Eq. 14 and search\nfor the optimal ˜ρX from [γ1 ˜ρinit\nX ,γ2 ˜ρinit\nX ].\nCombining pre-shifted exponent biasmethod\nwith the joint format and max-value search\nframework(FPQ baseline), we name our method\nas (FPQ), short for Floating Point Quantization.\n5 Experiments\nTo validate the effectiveness of the proposed\nmethod, we conduct experiments on LLaMA (Tou-\nvron et al., 2023) and BERT (Devlin et al., 2019)\nmodels in 5.2.1 and Sections 5.2.2. Further, in\nSection 5.2.3 we show that our method also gen-\neralizes well to vision transformer architectures.\nWe present ablation studies on the calibration size\nand search range in Section 5.3, and analyze the\nhardware costs of implementing FP operators in\nSection 5.4.\n5.1 Experiments Details\nWe adopt per-tensor quantization for activation and\nper-channel quantization for weight. We employ\nlayer reconstruction following the settings of (Yuan\net al., 2022; Nagel et al., 2020), and parallel quanti-\nzation based on the approach outlined in (Bai et al.,\n2022; Yuan et al., 2022). A more detailed discus-\nsion regarding our implementation decisions can\nbe found in Appendix F. For LLaMA models, we\nquantize all the weight and activation tensors in\nfully-connected layers for a fair comparison with\nprevious work (Xiao et al., 2022; Liu et al., 2023).\nFor BERT and ViT models, both fully-connected\nlayers and activation-activation multiplication ten-\nsors in the self-attention module are quantized.\nNote that for FPQ on BERT (Devlin et al., 2019)\nand ViTs models, the reconstruction metric Eq. 12\nis substituted with a Hessian approximation loss\nmetric. This substitution is further detailed in Ap-\npendix A.\n5.2 Main Results\n5.2.1 LLM Zero-Shot Reasoning\nWe evaluate the effectiveness ofFPQ for LLaMA-\n7B/ LLaMA-13B (Touvron et al., 2023) on com-\nmon sense zero-shot reasoning tasks. For the cali-\nbration data, we sample 32 random segments with\n2048 tokens length from the C4 (Raffel et al., 2020)\n597\nQuant Method #Bits (E/W/A)# CalibBoolQ PIQA HellaSwag WinoGrande ARC-e ARC-c Avg.\nLLaMA-7B Full-precision 16/16/16 - 75.1 78.7 56.9 69.9 75.3 41.9 66.3\nMinMax INT Quant 8/8/8 32 64.3 66.8 40.5 57.4 59.0 29.6 52.9MinMax FP Quant (E4M3) 8/8/8 32 74.9 78.6 56.8 69.5 75.5 41.6 66.1SmoothQuant (Xiao et al., 2022)16/8/8 512 74.0 77.5 55.0 69.6 74.4 37.4 64.6FPQ baseline 8/8/8 32 75.8 78.3 55.9 69.5 75.6 41.3 66.1FPQ 8/8/8 32 75.6 78.2 56.6 70.2 74.6 40.7 66.0\nMinMax INT Quant 4/4/16 32 64.1 76.1 51.6 66.3 72.4 40.0 61.7MinMax FP Quant (E2M1) 4/4/16 32 73.0 77.9 55.2 69.1 73.6 40.9 64.9GPTQ (Frantar et al., 2023) 4/4/16 128 73.3 77.9 54.9 67.9 72.7 37.4 64.0FPQ baseline 4/4/16 32 74.8 77.9 55.6 69.5 75.2 41.0 65.7FPQ 4/4/16 32 74.2 77.8 55.8 69.9 74.9 40.4 65.5\nMinMax INT Quant 4/4/8 32 50.4 56.5 27.9 46.5 36.1 21.2 39.7MinMax FP Quant (E2M1/E4M3)4/4/8 32 73.0 77.5 55.0 69.3 73.6 40.9 64.9FPQ baseline 4/4/8 32 75.0 77.6 55.9 69.9 74.3 39.4 65.3FPQ 4/4/8 32 75.0 77.7 55.5 69.8 74.5 39.9 65.4\nMinMax INT Quant 4/4/4 32 54.1 51.7 25.6 49.8 24.7 22.9 38.1MinMax FP Quant (E2M1) 4/4/4 32 47.3 53.1 25.7 50.7 25.1 22.4 37.4SmoothQuant (Xiao et al., 2022)16/4/4 512 54.1 62.8 41.5 52.6 50.6 32.9 49.1LLM-QAT (Liu et al., 2023) 16/4/4 (QAT) 63.5 64.3 55.6 52.9 50.3 30.2 52.8FPQ baseline 4/4/4 32 57.4 56.6 30.2 51.1 37.7 23.2 42.7FPQ 4/4/4 32 64.2 73.5 47.8 63.7 65.9 33.6 58.1\nLLaMA-13B Full-precision16/16/16 - 77.9 79.2 59.9 72.6 77.4 46.4 68.9\nMinMax INT Quant 8/8/8 32 60.6 69.6 46.0 61.5 63.3 32.8 55.6MinMax FP Quant (E4M3) 8/8/8 32 78.0 79.1 60.0 72.3 77.2 47.1 68.9SmoothQuant (Xiao et al., 2022)16/8/8 512 76.5 78.0 58.0 72.1 76.3 45.5 68.2FPQ baseline 8/8/8 32 78.0 79.1 59.9 72.3 77.2 47.1 68.9FPQ 8/8/8 32 78.1 78.5 59.1 72.4 76.4 46.1 68.4\nMinMax INT Quant 4/4/8 32 52.1 65.0 36.4 53.9 52.3 29.0 48.1MinMax FP Quant (E2M1/E4M3)4/4/8 32 78.0 78.9 58.0 71.6 76.0 44.8 67.9FPQ baseline 4/4/8 32 76.2 78.2 57.9 71.9 75.1 43.9 67.2FPQ 4/4/8 32 76.4 78.5 58.2 72.1 75.2 44.7 67.5\nMinMax INT Quant 4/4/4 32 54.5 52.7 25.5 51.1 25.3 22.1 38.5MinMax FP Quant (E2M1) 4/4/4 32 45.8 51.7 25.5 49.5 25.0 22.8 36.7SmoothQuant (Xiao et al., 2022)16/4/4 512 57.6 61.3 56.0 52.6 49.9 25.1 50.4FPQ baseline 4/4/4 32 54.3 57.7 35.7 52.2 41.1 25.7 44.5FPQ 4/4/4 32 71.9 74.8 53.3 66.7 71.7 39.9 63.1\nTable 1: Zero-shot performance on common sense reasoning tasks with LLaMA (Touvron et al., 2023) models. We\ndenote E/W/A as the bit-width of word embeddings, model weight and activations, respectively.\ndataset following the setting of GPTQ (Frantar\net al., 2023). The data preprocessing and score\ncalculation are based on EleutherAI evaluation har-\nness1. In Table 1, we compare FPQ to the floating-\npoint PTQ baselines, and state-of-the-art PTQ and\nQAT methods, including SmoothQuant (Xiao et al.,\n2022) and GPTQ (Frantar et al., 2023), and LLM-\nQAT (Liu et al., 2023).\nIn general, all methods, except for the naïve Min-\nMax INT Quantization, produce comparable out-\ncomes in the 8-bit setting on both LLaMA-7B and\nLLaMA-13B. Additionally, we observe that the\nnaïve MinMax FP Quantization achieves nearly\nlossless results and even surpasses the state-of-\nthe-art integer post-training quantization method,\nSmoothQuant (Xiao et al., 2022), which indicates\nthat floating-point quantization naturally has a\nstrong capability in handling the distributions in\ntransformers. However, both MinMax FP Quant\nand FPQ baseline fail when pushing the quan-\n1https://github.com/EleutherAI/lm-evaluation-harness\ntization precision to ultra-low 4/4/4 bit setting,\nwith 28.9% and 23.8% accuracy degradation on\nLLaMA-7B, respectively. In this extreme case,\nthe previous state-of-the-art PTQ and QAT meth-\nods, SmoothQuant (Xiao et al., 2022) and LLM-\nQAT (Liu et al., 2023) also suffer severe accu-\nracy downgrade. In comparison, FPQ demonstrates\na strong capability of handling extra-low bit set-\ntings and achieves only 8.2/5.8% accuracy drop on\nLLaMA-7B/13B with 4/4/4 bit-width, outperform-\ning SmoothQuant (Xiao et al., 2022) by a large\nmargin, yet with less bit-width and smaller calibra-\ntion size. Moreover, FPQ even achieves 5.3% accu-\nracy improvements compared to LLM-QAT (Liu\net al., 2023) in the 4/4/4 setting and 1.5% over\nGPTQ (Frantar et al., 2023) in the 4/4/16 configu-\nration on LLaMA-7B.\nFor practitioners, a crucial consideration is\ndetermining the appropriate quantization meth-\nods for various bit-widths. Therefore, based\non our findings, we offer two recommendations\nthat balance the trade-off between accuracy and\n598\nQuant Method #Bits (E/W/A)# CalibMNLI−m QQP QNLI SST-2 CoLA STS-B MRPC RTE Avg.\n(Full-precision) 32-32-32 - 84.9 91.4 92.1 93.2 59.7 90.1 86.3 72.2 83.7\nMinMax INT Quant 8/8/8 128 77.0 89.9 88.9 92.9 51.8 88.2 83.8 71.5 80.5MinMax FP Quant (E2M5)8/8/8 128 78.9 90.8 88.6 92.9 52.7 88.4 84.3 69.0 80.7MinMax FP Quant (E3M4)8/8/8 128 84.5 90.9 91.5 93.2 58.3 89.3 87.7 71.8 83.4MinMax FP Quant (E4M3)8/8/8 128 84.7 90.9 91.7 93.0 58.6 89.3 86.5 72.2 83.4MinMax FP Quant (E5M2)8/8/8 128 84.1 90.9 91.4 93.6 58.1 89.2 87.5 71.8 83.3FPQ baseline 8/8/8 128 84.6 90.9 91.7 93.1 58.6 89.3 88.0 72.2 83.5FPQ 8/8/8 128 84.6 91.0 91.6 93.3 58.8 89.3 88.0 72.2 83.6\nMinMax INT Quant 6/6/6 128 31.9 62.0 52.8 58.8 0.0 12.7 32.1 52.7 37.9MinMax FP Quant (E2M3)6/6/6 128 43.5 85.4 79.4 90.5 45.2 86.0 66.9 59.9 69.6MinMax FP Quant (E3M2)6/6/6 128 83.9 90.8 90.8 92.2 58.2 88.6 87.0 72.2 83.0MinMax FP Quant (E4M1)6/6/6 128 84.4 90.2 90.1 92.2 58.2 89.2 85.3 69.7 82.4FPQ baseline 6/6/6 128 84.6 90.9 91.2 93.2 58.8 88.7 87.5 70.8 83.2FPQ 6/6/6 128 84.5 90.8 91.6 93.1 57.3 89.3 88.7 71.8 83.2\nMinMax INT Quant 4/4/8 128 33.1 63.8 60.1 49.3 0.0 44.0 50.2 49.1 43.7MinMax FP Quant (E2M1)4/4/8 128 60.6 70.9 77.4 79.9 5.5 78.6 46.8 56.6 59.5MREM-S (Bai et al., 2022)4/4/8 4096 83.5 90.2 91.2 91.4 55.1 89.1 84.8 71.8 82.1MREM-P (Bai et al., 2022)4/4/8 4096 83.4 90.2 91.0 91.5 54.7 89.1 86.3 71.1 82.2FPQ baseline 4/4/8 128 84.4 90.6 91.4 92.9 58.6 83.7 88.2 73.3 82.9FPQ 4/4/8 128 84.5 90.6 91.1 92.7 58.8 89.3 88.7 73.3 83.6\nMinMax INT Quant 4/4/4 128 31.8 39.7 50.5 49.1 0.0 6.7 31.6 54.5 32.9MinMax FP Quant (E2M1)4/4/4 128 33.6 54.0 50.6 50.8 0.0 0.0 31.6 52.0 34.1BrecQ (Li et al., 2021) 8/4/4 4096 31.9 62.3 50.7 50.9 0.9 6.4 31.7 52.3 35.8QDrop (Wei et al., 2022) 8/4/4 4096 71.4 79.0 76.8 88.1 40.9 81.9 79.2 60.7 72.3FPQ baseline 4/4/4 128 38.9 68.3 55.3 83.6 10.6 0.0 43.8 55.2 44.5FPQ 4/4/4 128 82.3 89.2 86.6 91.5 52.6 85.5 83.8 69.0 80.1\nTable 2: Results on the GLUE development set with BERT (Bai et al., 2022) model. We denote E/W/A as the\nbit-width of word embeddings, model weight and activations, respectively.\nsearch/optimization efficiency. First of all, since\nthe difference between MinMax FP Quant and the\nrest of the methods is marginal for the 8/8/8 set-\nting, we recommend simply using the MinMax FP\nQuant method for the 8/8/8 setting as the MinMax\nmethod does not involve search process. However,\nfor more demanding scenarios, especially with ac-\ntivation quantization to 4 bits, we recommend em-\nploying FPQ for minimizing accuracy degradation\nwith negligible inference overhead.\n5.2.2 BERT Model\nWe evaluate the proposed quantization techniques\nfor BERT model on GLUE tasks (Wang et al.,\n2019). Full-precision BERT-base models fine-\ntuned on GLUE datasets are obtained from Hug-\ngingface public repository2. We randomly sample\n128 data from the training set as the calibration set.\nIn Table 2, FPQ demonstrates remarkable perfor-\nmance, achieving absolute average accuracy im-\nprovements of 44.3% compared to BrecQ (Li et al.,\n2021) and 7.9% over QDrop (Wei et al., 2022)\nwith 4/4/4 bit setting. Further, with 4-bit weight\nand 8-bit activation, MREM-S/MREM-P (Bai et al.,\n2022) present a 1.6/1.5% accuracy gap to the full-\nprecision model with 4096 calibration data, while\nFPQ achieves almost no accuracy loss with only\n2https://huggingface.co/textattack/bert-base-uncased-\n{TASK_NAME}\n128 calibration data points.\n5.2.3 Generalizability on Vision Transformer\nBased on our findings that vision transformers also\nexhibit a consistent activation distribution pattern\nas language transformers, characterized by high\ninter-channel variance and low intra-channel vari-\nance, as detailed in Fig. 2, we extended our pro-\nposed methods to ViT and compared FPQ with\nfloating-point PTQ baselines and state-of-the-art\nPTQ method for ViT on the ImageNet classifica-\ntion task. Table 3 shows that findings on ViT are\nconsistent with that on language models: previous\nstate-of-the-art integer-based methods struggled to\nmaintain reasonable accuracy when quantizing the\ntransformer to lower bits. In comparison, the pro-\nposed FPQ outperformed both PTQ4ViT and APQ-\nViT on 6 bits, and also achieved 40.9% and 31.5%\nabsolute accuracy improvement over PTQ4ViT and\nAPQ-ViT on DeiT-S in the 4-bit configuration.\n5.3 Ablation Study\nIn this section, we first compare the influence of dif-\nferent calibration sizes on FPQ. We vary the calibra-\ntion size in {32,64,128,256}and test on MNLI,\nQQP, and CoLA. Table 4 shows that the evalua-\ntion on MNLI and QQP is more robust to different\nsettings, and the variance is more significant on\nCoLA. We observe that FPQ performs well with a\n599\nW/A Quant Method Deit-S Deit-B ViT-S\nFull-prec - 79.9 81.8 81.4\n6/6 PTQ4ViT(Yuan et al., 2022)76.3 80.3 78.66/6 APQ-ViT(Ding et al., 2022)77.8 80.4 79.26/6 MinMax FP Quant (E3M2)79.3 81.7 80.76/6 FPQ baseline 79.43 81.7 80.96/6 FPQ 79.5 81.8 81.1\n4/4 PTQ4ViT(Yuan et al., 2022)34.1 64.4 42.64/4 APQ-ViT (Ding et al., 2022)43.6 67.5 48.04/4 MinMax FP Quant (E2M1)0.4 0.1 0.14/4 FPQ baseline 6.57 0.71 0.34/4 FPQ 75.0 79.4 73.2\nTable 3: Comparison on the ImageNet dataset with\nvision transformer structures.\nE/W/A#CalibMNLI-M QQP CoLA\n4/4/4 32 81.5 89.4 44.44/4/4 64 81.8 89.4 47.94/4/4 128 82.3 89.2 52.64/4/4 256 81.9 89.0 52.9\n6/6/6 32 84.8 90.8 55.06/6/6 64 84.7 90.9 58.26/6/6 128 84.5 90.8 57.36/6/6 256 84.6 90.8 57.6\nTable 4: Ablation studies of different calibration sizes.\ncalibration set size of 128 data points. However,\nwe also find that it remains robust and maintains\ncompetitive accuracy even with limited access to\ncalibration data, such as when using as few as 32\ndata points.\nWe investigate the robustness of FPQ to dif-\nferent search ranges (γ1,γ2). Table 5 presents\nthe results of FPQ using three sets of (γ1,γ2):\n(0.01,1.2),(0.1,1.2),(0.5,1.5), on MNLI, QQP,\nand CoLA. It is observed that no single search\nrange outperforms the others consistently across\nall tasks. For instance, the search range (0.01,1.2)\nperforms better than (0.5,1.5) on MNLI and QQP,\nbut slightly worse on CoLA in the 4-bit configu-\nration. Overall, FPQ exhibits robustness to various\nγ1 and γ2, as long as the search range is not overly\naggressive.\n5.4 Hardware Cost\nWe further examine the hardware utilization of low-\nbit INT, FP, and mixed-format FP multiplication\noperators, including adder, multiplier, and multiply-\naccumulate (MAC) units, in terms of hardware area.\nMixed-format FP refers to the multiplication of\nfloating-point numbers with different formats, e.g.,\nE2M1 multiplies with E1M2. We implemented\nthe MAC operator by Verilog HDL and utilized\nCadence Genus to obtain the synthesized area un-\nder TSMC 40nm technology and 0.5GHz clock\nfrequency.\nTable 6 illustrates the hardware cost of the INT\nand FP operators, with the multiplier being the pri-\nE/W/A γ1, γ2 MNLI-M QQP CoLA\n4/4/4 0.01, 1.2 82.3 89.2 52.64/4/4 0.1, 1.2 82.2 89.1 53.64/4/4 0.5, 1.5 82.3 88.4 52.8\n6/6/6 0.01, 1.2 84.5 90.8 57.36/6/6 0.1,1.2 84.7 90.8 57.56/6/6 0.5,1.5 84.7 90.8 57.8\nTable 5: Ablation studies of different search range.\nFormat Adder(µm2) Multiplier(µm2) MAC(µm2)\nINT4 93 182 410INT6 132 340 529\nE2M1 111 92 443E3M2 223 138 498E2M1 * E1M2 105 107 432\nTable 6: Area differences of INT, FP and mixed Format\nFP operators across different bit-widths.\nmary cost for INT and the adder for FP. Notably, the\ndisparity between FP4 and INT4 adders is small,\nwhile INT has twice the hardware cost for the mul-\ntiplier. Moreover, the mixed-format FP4 operator\nhas comparable hardware area as the standard FP4\noperator. These findings indicate that the proposed\nFPQ approach imposes negligible overhead in terms\nof hardware implementation when compared to the\nstandard FP operators and the hardware cost for FP\nis comparable with INT.\n6 Conclusion\nThis paper presents the first successful demonstra-\ntion of 4-bit floating-point post-training quantiza-\ntion for weights, activations, and embeddings in\nnatural language transformer architectures, includ-\ning both large language models and BERT model.\nWe also extend our method to vision transform-\ners and observe its robust generalization ability.\nOur approach involves a practical search-based\ntechnique which establishes a strong baseline and\nachieves state-of-the-art results for 6-bit and 8-bit\nquantization. Furthermore, we address the chal-\nlenge of high inter-channel variance in transform-\ners by proposing pre-shifted exponent bias, which\nproves highly effective in achieving accurate 4-bit\nquantization.\nAcknowledgement\nThis research is supported by National Natural\nScience Foundation of China/ HKSAR Research\nGrants Council Joint Research Scheme under Grant\nNHKUST 627/20, and Foshan HKUST Projects\nunder Grant FSUST 21 −HKUST 10E.\n600\nLimitations\nOur experiments were conducted on publicly avail-\nable datasets with finite sentence lengths, and the\ngeneralizability of our method to extremely long\nsequences or streaming data has not been verified\nand may require further investigation. In addition,\nit remains to be seen how our proposed method can\ngeneralize to other domains beyond language and\nvision, such as audio. It would also be interesting\nto see the applicability of our method to generative\ntasks and other applications.\nReferences\nHassan Akbari, Liangzhe Yuan, Rui Qian, Wei-Hong\nChuang, Shih-Fu Chang, Yin Cui, and Boqing Gong.\n2021. Vatt: Transformers for multimodal self-\nsupervised learning from raw video, audio and text.\nAdvances in Neural Information Processing Systems,\n34:24206–24221.\nHaoli Bai, Lu Hou, Lifeng Shang, Xin Jiang, Irwin\nKing, and Michael Lyu. 2022. Towards efficient post-\ntraining quantization of pre-trained language models.\nIn Advances in Neural Information Processing Sys-\ntems.\nYelysei Bondarenko, Markus Nagel, and Tijmen\nBlankevoort. 2021. Understanding and overcoming\nthe challenges of efficient transformer quantization.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nYaohui Cai, Zhewei Yao, Zhen Dong, Amir Gholami,\nMichael W Mahoney, and Kurt Keutzer. 2020. Zeroq:\nA novel zero shot quantization framework. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 13169–13178.\nYoni Choukroun, Eli Kravchik, Fan Yang, and Pavel\nKisilev. 2019. Low-bit quantization of neural net-\nworks for efficient inference.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. Llm.int8(): 8-bit matrix multipli-\ncation for transformers at scale. Advances in Neural\nInformation Processing Systems, 35:30318–30332.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nYifu Ding, Haotong Qin, Qinghua Yan, Zhenhua Chai,\nJunjie Liu, Xiaolin Wei, and Xianglong Liu. 2022.\nTowards accurate post-training quantization for vi-\nsion transformer. In Proceedings of the 30th ACM\nInternational Conference on Multimedia, MM ’22,\npage 5380–5388, New York, NY , USA. Association\nfor Computing Machinery.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on\nLearning Representations.\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and\nDan Alistarh. 2023. GPTQ: Accurate post-training\ncompression for generative pretrained transformers.\nIn International Conference on Learning Representa-\ntions.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina\nToutanova. 2019. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. In\nProceedings of NAACL-HLT, pages 4171–4186.\nAndrey Kuzmin, Mart Van Baalen, Yuwei Ren, Markus\nNagel, Jorn Peters, and Tijmen Blankevoort. 2022.\nFp8 quantization: The power of the exponent. Ad-\nvances in Neural Information Processing Systems,\n35:14651–14662.\nJemin Lee, Yongin Kwon, Jeman Park, Misun Yu, and\nHwanjun Song. 2023. Q-hyvit: Post-training quan-\ntization for hybrid vision transformer with bridge\nblock reconstruction.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020. Bart:\nDenoising sequence-to-sequence pre-training for nat-\nural language generation, translation, and comprehen-\nsion. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n7871–7880.\nYuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng\nHu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi\nGu. 2021. Brecq: Pushing the limit of post-training\nquantization by block reconstruction. arXiv preprint\narXiv:2102.05426.\nZechun Liu, Barlas Oguz, Changsheng Zhao, Ernie\nChang, Pierre Stock, Yashar Mehdad, Yangyang\nShi, Raghuraman Krishnamoorthi, and Vikas Chan-\ndra. 2023. Llm-qat: Data-free quantization aware\ntraining for large language models. arXiv preprint\narXiv:2305.17888.\nZhenhua Liu, Yunhe Wang, Kai Han, Wei Zhang, Siwei\nMa, and Wen Gao. 2021. Post-training quantization\nfor vision transformer. Advances in Neural Informa-\ntion Processing Systems, 34:28092–28103.\n601\nPaulius Micikevicius, Dusan Stosic, Neil Burgess, Mar-\nius Cornea, Pradeep Dubey, Richard Grisenthwaite,\nSangwon Ha, Alexander Heinecke, Patrick Judd,\nJohn Kamalu, Naveen Mellempudi, Stuart Oberman,\nMohammad Shoeybi, Michael Siu, and Hao Wu.\n2022. Fp8 formats for deep learning.\nMarkus Nagel, Rana Ali Amjad, Mart Van Baalen,\nChristos Louizos, and Tijmen Blankevoort. 2020. Up\nor down? adaptive rounding for post-training quan-\ntization. In International Conference on Machine\nLearning, pages 7197–7206. PMLR.\nMarkus Nagel, Mart van Baalen, Tijmen Blankevoort,\nand Max Welling. 2019. Data-free quantization\nthrough weight equalization and bias correction.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Hervé Jé-\ngou. 2021. Training data-efficient image transform-\ners & distillation through attention. In International\nconference on machine learning, pages 10347–10357.\nPMLR.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding.\nXiuying Wei, Ruihao Gong, Yuhang Li, Xianglong Liu,\nand Fengwei Yu. 2022. QDrop: Randomly dropping\nquantization for extremely low-bit post-training quan-\ntization. In International Conference on Learning\nRepresentations.\nDi Wu, Qi Tang, Yongle Zhao, Ming Zhang, Ying Fu,\nand Debing Zhang. 2020. Easyquant: Post-training\nquantization via scale optimization.\nGuangxuan Xiao, Ji Lin, Mickael Seznec, Julien De-\nmouth, and Song Han. 2022. Smoothquant: Accurate\nand efficient post-training quantization for large lan-\nguage models. arXiv preprint arXiv:2211.10438.\nZhihang Yuan, Chenhao Xue, Yiqi Chen, Qiang Wu,\nand Guangyu Sun. 2022. Ptq4vit: Post-training\nquantization for vision transformers with twin uni-\nform quantization. In Computer Vision–ECCV 2022:\n17th European Conference, Tel Aviv, Israel, October\n23–27, 2022, Proceedings, Part XII, pages 191–207.\nSpringer.\nYijia Zhang, Lingran Zhao, Shijie Cao, Wenqiang Wang,\nTing Cao, Fan Yang, Mao Yang, Shanghang Zhang,\nand Ningyi Xu. 2023. Integer or floating point? new\noutlooks for low-bit quantization on large language\nmodels.\n602\nA Hessian-Based Loss Metric\nThe objective of post-training quantization is to\nminimize the perturbation ( δX = XFP −XR)\nintroduced by quantization to the pre-trained real-\nvalued network:\nmin E[L(XR + δX) −L(XR)] (20)\nFollowing the Taylor series expansion, we have\nE[L(XR + δX) −L(XR)]\n≈δXT¯g(X) + 1\n2δXT ¯H(X)δX\n≈1\n2δXT ¯H(X)δX\n(21)\nHere, ¯g(X) is the gradients and ¯H(X) is the Hes-\nsian matrix. Since the pre-trained model is well-\nconverged, we can assume that ¯g(X) has near zero\nvalue in every element, and thus term δXT¯g(X)\ncan be neglected.\nThe Hessian matrix ¯H(X) is computed as:\n¯H(X) = JT\nO(X) ¯H(O)JO(X) (22)\nwhere JO(X) denotes the Jacobian matrix of the\nlayer output O w.r.t X, and ¯H(O) is the Hessian\nmatrix w.r.tO. We then substitute the above equa-\ntion back to equation 21 :\nδXT ¯H(X)δX\n= (JO(X)δX)T ¯H(O)(JO(X)δX)\n≈( ˆO −O)T ¯H(O)( ˆO −O)\n(23)\nHere ˆO is the intermediate output of the quantized\nlayer and O is the original layer output. Note that\nunder the assumption thatδX is relatively small (Li\net al., 2021), we can approximate ( ˆO −O) as\nJO(X)δX using first-order Taylor expansion.\nNevertheless, the calculation of ¯H(O) is still bur-\ndensome, therefore, we use the diagonal entries of\nthe Fisher Information Matrix of O to substitute\n¯H(O) following (Li et al., 2021; Yuan et al., 2022),\nand the new Hessian-based metric becomes:\nE[( ˆO −O)Tdiag(( ∂L\n∂O1\n)2,..., ( ∂L\n∂On\n)2( ˆO −O)]\n(24)\nHere, each entry ofO is assumed to be independent\nand ndenoted the total number of elements in O.\nIn this study, this hessian-based metric is used as\nthe reconstruction metric to search for the optimal\nFP quantization function for both the weight and ac-\ntivation when performing layer-wise reconstruction\nin BERT and Vision Transformer models.\nB Quantization Error of Different\nFloating-Point Formats\nFigure 4 compares the quantization error of differ-\nent formats in 8-bit quantization, including INT8,\nE2M5, E3M4, E4M3, and E5M2. We apply these\nformats to different BERT modules in the first, fifth,\nand last layers. The figures demonstrate that the op-\ntimal FP formats differs depending on the specific\nmodule that we are quantizing.\nC Inter-Channel Variance Visualization\nFigure 5 and 6 depict the output of different fully-\nconnected layers in BERT for the MNLI task, DeiT-\nS for the ImageNet-1K task, and LLaMA-7B for\nthe zero-shot reasoning task. The visualizations\nreveal a noticeable inter-channel variance presented\nin both language and vision transformers.\nD Efficient Matrix Multiplication\nFigure 7 displays a comprehensive list of all the\ngranularity options that allow for efficient ma-\ntrix multiplication. While per-token quantization\ntheoretically provides greater precision in terms\nof quantization granularity, the accuracy gains\nachieved through this method are minimal and do\nnot justify the additional computational overhead\nrequired. As a result, we have opted to use per-\ntensor quantization when quantizing activations.\nE Learning Format and Maximum Value\nWe compare the previous gradient-based\nmethod (Kuzmin et al., 2022) with the proposed\nsearch-based method for finding the optimal\nformat and maximum value. On DeiT-S, the\nlearnable method only achieves 74.38% accuracy\nfor an 8-bit quantized model on ImageNet, in\ncontrast, FPQ can attain an almost loss-less result of\n79.88%. We analyze the gradients for the number\nof exponent bits e derived in (Kuzmin et al.,\n2022) and observe that each time the exponent\nbits change, the gradients experience exponential\nvariations, leading to high instability. Based\non this observation, we assert that employing a\nsearch-based method to determine the optimal\nformats is crucial in post-training quantization\n(PTQ).\nF Reconstruction Choices\nThe previous works on integer post-training quanti-\nzation involves breaking down the target model into\n603\nsub-modules and reconstructing them separately\n(Nagel et al., 2020; Li et al., 2021; Bai et al., 2022;\nYuan et al., 2022). This addresses the problem of\nover-fitting, given that only a limited amount of\nunlabeled calibration data is available. In this study\nwe find the layer-wise reconstruction and parallel\nquantization works best for floating-point PTQ:\nLayer Reconstruction: Recent research (Li\net al., 2021; Bai et al., 2022) suggests increasing the\nreconstruction granularity from layer reconstruc-\ntion (Nagel et al., 2020) to block reconstruction (Li\net al., 2021) or even larger granularity (Lee et al.,\n2023). This is achieved by jointly optimizing all the\nlinear layers or matrix multiplication components\nwithin each module to prevent the propagation of\nreconstruction errors among the layers. Despite\nthis, we have observed that increasing the recon-\nstruction granularity does not improve the accuracy\nof FPQ baseline or sometimes even lead to worse\nresults. Therefore, we choose layer reconstruction.\nParallel Quantization: Sequential quantization\nis the most commonly used approach (Wu et al.,\n2020; Nagel et al., 2020; Li et al., 2021) where\nmodules are quantized consecutively based on their\nsequential order, and the input for the current cali-\nbrating module is generated using all the previously\nquantized modules. However, some recent works\n(Yuan et al., 2022; Bai et al., 2022) proposed a new\nparallel quantization framework. This framework\nuses the raw output of the full-precision modules\nas input and makes the calibration of each module\nindependent from one another. In this work, we\nuse parallel quantization, as it yields better results\nthan its sequential counterparts.\nFigure 4: Quantization error of different formats for BERT layers.\nBert DeiT-S\nBert DeiT-S\nBert DeiT-S\nBert\n DeiT-S\nBert\n DeiT-S\nBert DeiT-S\n(a)\n(c)\n(b)\n(d)\n(e) (f)\nBert DeiT-S\nFigure 5: Magnitude of the output activations of different modules in BERT (left column), and DeiT-S (right\ncolumn).\n604\nLLaMa-7B\n(a)\n(b)\n(c)\nFigure 6: Magnitude of the output activations of differ-\nent modules in LLaMA-7B.\n!\"!,!!\"!,#\n!\"#,!!\"#,#\n!%!,!!%!,#\n!%#,!!%#,#\n.$! .$#\n.(!\n.(#\n!\"!,!!\"!,#\n!\"#,!!\"#,#\n((\n!%!,!!%!,#\n!%#,!!%#,#\n($\nPer-tensorPer-tensor\nPer-tokenPer-channel\nFigure 7: Quantization granularity options that support\nefficient matrix multiplication. The dimensions that\nshare the same scaling factor are indicated with red\ndotted frames\n605",
  "topic": "Quantization (signal processing)",
  "concepts": [
    {
      "name": "Quantization (signal processing)",
      "score": 0.8023743033409119
    },
    {
      "name": "Dither",
      "score": 0.7154678702354431
    },
    {
      "name": "Computer science",
      "score": 0.6029306650161743
    },
    {
      "name": "Transformer",
      "score": 0.5877547860145569
    },
    {
      "name": "Floating point",
      "score": 0.5687704086303711
    },
    {
      "name": "Algorithm",
      "score": 0.533672571182251
    },
    {
      "name": "Exponent",
      "score": 0.479274719953537
    },
    {
      "name": "Scaling",
      "score": 0.4378758668899536
    },
    {
      "name": "Arithmetic",
      "score": 0.36662164330482483
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3477315306663513
    },
    {
      "name": "Speech recognition",
      "score": 0.32093146443367004
    },
    {
      "name": "Mathematics",
      "score": 0.27586793899536133
    },
    {
      "name": "Computer vision",
      "score": 0.10061812400817871
    },
    {
      "name": "Electrical engineering",
      "score": 0.09799760580062866
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Noise shaping",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    }
  ]
}