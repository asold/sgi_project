{
  "title": "Characterizing Mechanisms for Factual Recall in Language Models",
  "url": "https://openalex.org/W4389520348",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2439114871",
      "name": "Qinan Yu",
      "affiliations": [
        "John Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A2970844468",
      "name": "Jack Merullo",
      "affiliations": [
        "John Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A2013784948",
      "name": "Ellie Pavlick",
      "affiliations": [
        "John Brown University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3010694149",
    "https://openalex.org/W4378501768",
    "https://openalex.org/W4389520380",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4327526719",
    "https://openalex.org/W2970453125",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385571791",
    "https://openalex.org/W4316135772",
    "https://openalex.org/W4308023630",
    "https://openalex.org/W4309217888",
    "https://openalex.org/W4386566901",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W4205460703",
    "https://openalex.org/W4303648884",
    "https://openalex.org/W4221151371",
    "https://openalex.org/W4385571244",
    "https://openalex.org/W4323697341",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Language Models (LMs) often must integrate facts they memorized in pretraining with new information that appears in a given context. These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict. On a dataset that queries for knowledge of world capitals, we investigate both distributional and mechanistic determinants of LM behavior in such situations. Specifically, we measure the proportion of the time an LM will use a counterfactual prefix (e.g., “The capital of Poland is London”) to overwrite what it learned in pretraining (“Warsaw”). On Pythia and GPT2, the training frequency of both the query country (”Poland”) and the in-context city (”London”) highly affect the models’ likelihood of using the counterfactual. We then use head attribution to identify individual attention heads that either promote the memorized answer or the in-context answer in the logits. By scaling up or down the value vector of these heads, we can control the likelihood of using the in-context answer on new data. This method can increase the rate of generating the in-context answer to 88% of the time simply by scaling a single head at runtime. Our work contributes to a body of evidence showing that we can often localize model behaviors to specific components and provides a proof of concept for how future methods might control model behavior dynamically at runtime.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9924–9959\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nCharacterizing Mechanisms for Factual Recall in Language Models\nQinan Yu Jack Merullo Ellie Pavlick\nBrown University\nDepartment of Computer Science\n{qinan yu,jack merullo,ellie pavlick}@brown.edu\nAbstract\nLanguage Models (LMs) often must integrate\nfacts they memorized in pretraining with new\ninformation that appears in a given context.\nThese two sources can disagree, causing com-\npetition within the model, and it is unclear how\nan LM will resolve the conflict. On a dataset\nthat queries for knowledge of world capitals,\nwe investigate both distributional and mech-\nanistic determinants of LM behavior in such\nsituations. Specifically, we measure the pro-\nportion of the time an LM will use a counter-\nfactual prefix (e.g., “The capital of Poland is\nLondon”) to overwrite what it learned in pre-\ntraining (“Warsaw”). On Pythia and GPT2, the\ntraining frequency of both the query country\n(”Poland”) and the in-context city (”London”)\nhighly affect the models’ likelihood of using\nthe counterfactual. We then use head attribu-\ntion to identify individual attention heads that\neither promote the memorized answer or the\nin-context answer in the logits. By scaling up\nor down the value vector of these heads, we can\ncontrol the likelihood of using the in-context\nanswer on new data. This method can increase\nthe rate of generating the in-context answer\nto 88% of the time simply by scaling a single\nhead at runtime. Our work contributes to a\nbody of evidence showing that we can often lo-\ncalize model behaviors to specific components\nand provides a proof of concept for how future\nmethods might control model behavior dynam-\nically at runtime.\n1 Introduction\nLarge Transformer Language Models (Vaswani\net al., 2017) (LMs) store information from pre-\ntraining which they can recall at inference time to\ngenerate text. This is paired with the exceptional\nability of models to use provided context in order to\nproduce coherent text that incorporates new facts.\nHowever, facts that are memorized in pretraining\nand facts that are provided in-context can often\ncompete with each other; in some cases it might\n…h0 h1 h2\nThe capital of Poland is London. \nQ: What is the capital of Poland? \nA: Warsaw       \nDownweight \nMemory Head\nThe capital of Poland is London. \nQ: What is the capital of Poland? \nA: London        \nMemorized Answer\nIn-context Answer\nAttention Layer n\nhi * /uni03B1\n!\nFigure 1: We find that individual attention heads can\nplay specific roles in using context information vs. re-\ncalling facts. By up or downweighting these heads, we\ncan often control whether LMs use information from\ncontext which conflicts with its pretraining knowledge.\nFor example, downweighting the memory attention head\ncauses the model to prefer “London” above.\nbe desirable that the model ignores facts from pre-\ntraining (e.g., updating outdated information with\na prompt), while in others we want the model to\nprefer what it learned in pretraining (e.g., ignoring\nfalse information in prompt injections). Currently,\nlittle is understood about the factors and mecha-\nnisms that control whether an LM will generate\ntext respecting either the in context or memorized\ninformation.\nRecent work in mechanistic interpretability aims\nto deeply explain the internal processes in LMs,\nallowing us to interpret the contributions of indi-\nvidual model components to the final predicted text\n(Olah et al., 2020; Wang et al., 2022; Nanda, 2022).\nWe can use tools from these studies to shed light on\nthe model components that are responsible for push-\ning the model more towards either memorized or\ncontextual information. We study this relationship\nusing a task that requires predicting a capital city in\nthe face of conflicting information (see Figure 1).\nWe measure how often a model will answer that the\ncapital city of a country is the in-context counter-\nfactual (e.g., London) vs. the memorized (ground\n9924\ntruth) city (Warsaw) it learned in pretraining. Our\nstudy consists of two key sets of experiments.\nFirst, in Section 5, we investigate how distri-\nbutional features of the pretraining data influence\nbehavior. We find that the frequency of a fact in the\npretraining corpus strongly correlates with model\nbehavior. This analysis reveals several key findings:\n(1) The more frequently a country appears in pre-\ntraining, the more likely the LM is to generate the\nmemorized capital city; (2) The more frequently\nthe in-context city (i.e., the one with which we want\nto overwrite) appears, the less likely the model is\nto use it, regardless of the frequency of the coun-\ntry; (3) Larger models (up to the scale tested: 2.8b\nparameters) are less likely overall to use in-context\ninformation and prefer the memorized answer, even\nwhen the fact is less frequent.\nNext, in Section 6, we use head attribution (El-\nhage et al., 2021; Nostalgebraist, 2020; Nanda,\n2022) to show that we can localize promotion of\nthe memorized or in-context answer to individual\nattention heads. By either upweighting or down-\nweighting the head by a scalar value, we can con-\ntrol which answer the model prefers. In the most\nsuccessful case, downweighting the memory head\nallows us to increase the rate of the in-context city\nto 88% while reducing the amount of memorized\npredictions to 4% on the world capitals task. In\na qualitative analysis of the weights of this head\n(Section 6.4), we show that it specifically promotes\ngeographic information. We find that forcing the\nopposite behavior, i.e., promoting the memorized\nanswer, is more difficult, and that the mechanism\nthese heads use doesn’t necessarily generalize well\n(§6.5). Still, the method we discover is surgical,\nand only requires scaling a single head (0.00001%\nof Pythia-1.4b’s parameters), suggesting that com-\nponents within an LM may specialize for specific\npredictable functions, and providing a promising\navenue for understanding the internal workings of\nLMs and further techniques for model editing.\n2 Related Work\nThe impressive, but sometimes unpredictable suc-\ncesses of LMs on performing tasks described in\ncontext (Brown et al., 2020) has spurred intense\ninterest in the factors that allow models to solve\ntasks this way. Studies on pretraining datasets have\nfound that higher pretraining term frequency is pos-\nitively correlated with task performance on factual\nassociation tasks (Kandpal et al., 2022) and numer-\nical reasoning (Razeghi et al., 2022), and relates\nto work on memorization vs. generalization in\nLMs (Hupkes et al., 2022). Haviv et al. (2023)\nanalyze mechanisms used to recall memorized in-\nformation by studying idiom generation. Model\nsize is also shown to be a factor that affects ten-\ndency to use memorized vs. in context information\n(Wei et al., 2023). Previous work has also exam-\nined how deeply LMs interact with context during\nin-context learning (Min et al., 2022; Xie et al.,\n2022). Other work has focused on LMs’ abilities\nto consider counterfactual or hypothetical contexts\n(Li et al., 2023; Qin et al., 2019), with mixed results\nin overwriting pretraining memory.\nOur work is built heavily on previous work in\nmechanistic interpretability, which aims to reverse\nengineer model computations into human under-\nstandable components (Nanda et al., 2023; Elhage\net al., 2021; Wang et al., 2022). While knowl-\nedge from pretraining has been found to be stored\nin the feedforward (MLP) sublayers (Meng et al.,\n2023; Geva et al., 2021; Kobayashi et al., 2023; Dai\net al., 2022), more recent work has also clarified the\nrole of attention in this same process: Geva et al.\n(2023) find that attention heads extract facts from\nan earlier mentioned subject token (e.g., Poland)\nwhen required. This naturally sets up our study,\nwhich also considers attention heads as the source\nof the competing effect between copying the coun-\nterfactual from earlier in context vs. extracting the\nmemorized fact from an earlier subject token. A\ncore technique in these works is projecting activa-\ntions from model components into the vocabulary\nspace to make claims about their roles, which we\ngenerically refer to here as logit attribution(Nos-\ntalgebraist, 2020; Wang et al., 2022; Merullo et al.,\n2023; Belrose et al., 2023; Dar et al., 2022; Mil-\nlidge and Black, 2022). We leverage this technique\nto localize attention heads which tend to promote\neither context or memorized information (§6).\n3 Task Design\nWe study the mechanism that language models use\nwhen given counterfactual information in context.\nFor our analysis, we focus on a simple zero-shot\ntask that requires producing the capital city for a\ngiven country, which serves as a representative ex-\nample of the type of common facts that a language\nmodel could learn in pretraining. It consists of 248\ncountry capital pairs with 6 additional aliases and\ntheir respective capitals. To create counterfactuals\n9925\nFigure 2: Results from total 62,992 inputs of every country paired with every counterfactual capital 1. We break\ndown all the inputs into 10 percentile bins from the least to most frequent by four frequency criteria. Every percentile\ncontains around a total of 6300 examples. The first two graphs reflect frequency based on the country (e.g., Poland).\nThe upward trajectory of the red lines show the positive correlation between the proportion of memorized answer\npredictions and frequency. The last two graphs reflect the frequency of the in-context capital(e.g., London). The\ndrop of the blue lines across all the four graphs show the negative correlation between proportion of in-context\nanswer predictions and term frequency.\nin the dataset, we pair up every country with the rest\nof the 247 capitals using the following format:\nThe capital of {country} is\n{in-context city}.\nQ: What is the capital of {country}?\nA:\nFor example, we can fill in{country} with Poland\nand {in-context city} with London. The model has\nlearned that the capital is Warsaw from pretraining,\nbut with the in-context prompt, the task becomes\nambiguous between whether it should output the\nknown answer Warsaw or overwrite it with London.\nWe query the model to generate a full sentence to\ndetermine which of the above task interpretations\nit preferred. We define Poland as the country and\nLondon as the in-context answer. Correspondingly,\nwe define Warsaw, the capital of Poland, as the\nmemorized answer. If London is included in the\nsentence, then we consider the model to have pro-\nduced the in-context answer. If the model generates\nWarsaw, then it is considered as the memorized an-\nswer. In total, we have 62,992 such pairs of country\nand capital.\nThe World Capital dataset is able to provide a\nclean analysis giving a unique memorized and in-\ncontext answer. Language models perform well on\nthis task producing one of the two expected cities at\nleast 80% of the time (varying depending on model\nsize). The lack of noise in the responses makes the\ntask a good choice for cleanly diagnosing model\npreference for memorized vs. in-context answers.\n4 Models\nWe analyze the overwriting behavior primarily on\nthe Pythia (Biderman et al., 2023) models as well\nas GPT2 series (Radford et al., 2019). The Pythia\nmodels are trained on the Pile (Gao et al., 2020)\nwhich we have full access to, allowing us to relate\nmodel behavior to frequency effects in the data.\nWhile we don’t have access to the pretraining data\nof GPT2, we still report results, using the Pile fre-\nquencies as an approximation of what GPT2 might\nhave seen.\n5 Effect of Term Frequency on Model\nTendency to Use Memorized Facts\n5.1 Experimental Setup\nWe hypothesize that the model will be less likely\nto overwrite information about more frequently ap-\npearing country/capital names. The number of in-\ncontext predictions will increase when the term fre-\nquency descreases. The number of memorized pre-\ndictions will increase when the term frequency in-\ncreases. To test this hypothesis, we search through\nthe the pretraining corpus of the Pythia model (i.e.,\nthe Pile (Gao et al., 2020), which contains 210 mil-\nlion text documents) in order to compute the term\nfrequency of country and city names.\n9926\nFigure 3: The proportion of in-context and memorized answers decomposed by the frequency of country(Poland)\nacross all Pythia models of different sizes (the 2nd graph in figure 2). The upward trend of the red lines shows as the\nmodel size increases, the model predicts more memorized answers. Blue and red shading indicates that the amount\nof in-context or memorized answers is higher, respectively. We find that as models get bigger, they first memorize\nmore frequent capitals before the lower frequency ones.\nWe search both for the frequencies of the in-\ndividual country and city names, as well as their\nco-occurrences in the dataset. Co-occurrence is\nmeasured by whether a country and a city appear\ntogether in the same document. We split the oc-\ncurrences and co-occurrences into 10 percentile\nbins, with the 0th bin containing the least fre-\nquent 10%, and the 9th bin containing the top 10%\nmost frequent terms. Every bin includes around\n25 countries and capitals. We mix every country\nwith all the other 248 capitals to form prompts.\nWe have in total around 6200 instances per bin\n(given 63k prompts). To give some qualitative\nexamples, capitals like Beijing are in the top per-\ncentile bin as measured by occurrence, while capi-\ntals like Akrotiri and Dhekelia are in the bottom.\nFor the co-occurrences between country and capi-\ntal, ⟨China, Beijing⟩is in the top percentile and\n⟨Guinea-bissau, Bissau⟩is in the bottom per-\ncentile.\nWe run the counterfactual world capital data\nthrough both the Pythia models as well as the GPT2\nseries of models. We generate the a full sentence by\ndecoding the output. We count the number of times\nthe in-context and memorized answers appear in\nthe decoded sentences and plot these counts as a\nfunction of the percentile bins described above.\n5.2 Results\nAs the frequency for the country increases, there is\nmore knowledge stored about the country during\npertaining. Therefore, we intuitively expect to see\nthat models are more inclined to predict the memo-\nrized answers as the frequency goes up. Figure 2\nsupports this intuition. We can see a clear upward\ntrend in the pink line, reflecting the increasing pro-\nportion of the memorized answers as a function of\nthe increase in term frequency. When the country\nis more prevalent in the training data, the model has\na greater tendency to predict memorized answers.\nWe also observe a relationship between the fre-\nquency of the in-context capital and the model’s\npredictions. As the frequency for either the coun-\ntry or the in-context capital increases, the number\nof in-context answer predictions decreases. This\nis demonstrated by the drop of the blue lines in\nFigure 2. When the given in-context capital is\nmore prevalent in the training data, for example\nBeijing, the model tends to predict the memorized\nanswer. However, when the given in-context cap-\nital is less prevalent, such as Palikir, the model\nis more likely to predict the in-context answer. We\nran the same experiments across all the Pythia and\nGPT2 models of different sizes (see Appendix A)\nand see the same frequency effect, especially in\nlarger models.\nFigure 3 shows the increase in sensitivity to fre-\nquency with respect to model size. We find that as\nmodels increase in size, they become more likely\noverall to produce the memorized answers rather\nthan in-context answers, and that this occurs with\nthe most frequent countries. That is, as larger\nmodels become more likely to produce the memo-\nrized answer, the changes are not evenly distributed\nacross frequency bins. Rather, a strong memoriza-\ntion bias is observed first for more frequent terms,\nand then as models get larger, this extends to in-\ncreasingly lower frequency terms. This can be\nobserved in transition from blue shading (more in-\ncontext answers) to red shading (more memorized\nanswers). See Appendix B.1 for results showing\nthis effect with respect to the frequency of cities\n9927\nFigure 4: The head attribution method showing the logit difference calculation for layer 15, head 7 in Pythia-1.4b\non the example from Figure 1. Pythia-1.4b has 24 layers and 16 heads for each layer, totaling 384 heads to check.\nWe obtain the memory head and in-context head in the following way: We divide the output weight matrix from an\nattention layer (WH\nO ) into 16 components (one for each head) (Elhage et al., 2021) Then, we take the dot product\nbetween each head iof the and the ith component of the weight matrix. Afterward, we extract the corresponding\nvectors in the unembedding matrix for the memorized answer (e.g., Warsaw) and in-context answer (e.g., London).\nWe dot product the projected head vector with the two vectors respectively, giving us a scalar value representing the\nlogit for each of those words represented by the head. Subtracting these two scalars give us the logit difference of\ntwo answers from one specific head. Blue in the heatmap indicates that the head is promoting in-context answer and\nred indicates the head is promoting memorized answer.\nand co-occurrences, where we observe the same\ntrend.\n6 Identifying and Manipulating\nMechanisms for Recall\nSo far we have shown that (larger) models tend to\nhave a preference to use the answer they have mem-\norized. In this section we ask if there is a specific\nmechanism within the model that controls whether\nthe memorized or in-context answer is generated,\nand whether that can be isolated from more gen-\neral language generating abilities. Because the task\nboils down to whether the model copies informa-\ntion that was provided in context or not, we focus\non analyzing the roles of specific attention heads.\nPrior work has demonstrated the importance of at-\ntention heads for performing copying tasks (Wang\net al., 2022; Elhage et al., 2021) as well as recall\nfrom memory (Geva et al., 2023), which motivates\nour analysis of attention heads. We perform this\nanalysis on only the largest models Pythia-1.4b,\nPythia-2.8b, as well as GPT2-xl (see Appendix D).\n6.1 Head Attribution\nThe idea behind logit attribution techniques (Nos-\ntalgebraist, 2020; Wang et al., 2022; Nanda, 2022)\nis to interpret activations or weights in a language\nmodel in terms of the vocabulary space. These\nmethods work by using the unembedding matrix\n(i.e., language modeling head) in order to under-\nstand the role of a given component for a given task.\nThis is built on the premise that the final hidden\nstate of the model is the summation of the outputs\nof all of the components before it (Elhage et al.,\n2021). That is, every layer of output can be traced\nback and decomposed as the contribution of each\nsublayer up to that point. We use head attribution\nto test whether individual heads tend to promote\neither the in-context capital or the memorized capi-\ntal. Using this method, we are able to find a single\nhead in each model that primarily controls the use\nof memorized information1.\nIn Figure 4, we illustrate the method. The addi-\ntive update made by the attention layer is composed\nof the individual updates of each attention head\nafter it is passed through the WH\nO output matrix\nwithin the attention layer. We can project the ith\nhead into the space of the residual stream by multi-\nplying with the ith (dhead,dmodel) slice of this ma-\ntrix (see Appendix C) and then multiplying with the\nunembedding matrix to get the logit values for the\nmemorized and in-context city tokens. We subtract\n1This is not to say that this is the only job of this head in\ngeneral, or that these are the only heads that play this role.\n9928\nFigure 5: With the chosen memory head (15.7) and in-context head (19.14), we apply a multiplicative factor (α) to\nmeasure the effect on producing either memorized or in-context answers. This was performed on two 100 example\ntuning sets (§6.1). The first graph demonstrates the most successful case of intervention. By tuning the memory\nhead (15.7) value by α= −0.7, can flip 86% of the examples from originally predicting memorized answers to\npredicting in-context answers. The dotted line shows no intervention (α= 1). The gray dot shows the value of α\nthat produces the best results according to our criteria.\nthese two scalar values to get the logit difference\n(see Wang et al. (2022)).\nIntuitively, this logit difference captures the ef-\nfect the head has in promoting one word (relative\nto another) to be output as the final prediction. This\nprovides us a practical way to calculate the the role\nof each head, and find heads that consistently push\nthe model towards the memorized or in-context\nanswer.\nData: To identify specific heads, we randomly\nsample 10 examples from each percentile for which\neach model predicts the in-context answers and an-\nother 10 examples for which each model predicts\nthe memorized answers. Thus, in total, we obtain\n100 examples on which the original model predicts\nin-context cities and 100 examples on which it pre-\ndicts memorized cities. We run these 200 examples\nthrough the model in batches of 5 and use head\nattribution to extract the logit difference between\neach head in every layer. We observe that there is\na variation in the roles of every head throughout\nthe batches, but we identify a series of heads that\nconsistently push the model towards one answer or\nthe other.\n6.2 Effect of Tuning Individual Attention\nHeads\nUsing head attribution, we identify two different\ntypes of heads: memory heads and in-context\nheads. The memory heads promote the predic-\ntion towards the memorized answer and the in-\ncontext heads promote the predictions towards the\nin-context answer. These heads are shown on the\nrighthand side of Figure 4, which plots the relative\neffect of each head at each layer for promoting the\nin-context vs. memorized answers.\nSince these heads heavily contribute to the logit\nincrease for one of the two answers, we hypothe-\nsize that multiplying the value vectors by a scalar\nwill enable us to increase or decrease the effect\nof each head. Let this multiplicative value be α.\nWe hypothesize that tuning up the memory head\nwill increase the number of answers that contain\nthe ground truth answer, while tuning it down will\nincrease the number that contain the in-context an-\nswer. The opposite should hold for the in-context\nhead.\nWith this assumption, we apply the scaling in-\ntervention on the series of potential memory heads\nand in-context heads on the 200 sampled examples.\nFrom the series of potential heads, we pick the head\nthat has the strongest effect in the intended direc-\ntion. See Appendix F for results using an alterna-\ntive memory head. For example, for the in-context\nhead, this effect is measured by the proportion of\ntimes the head changes the original memorized an-\nswers into in-context answers at it’s optimalα. The\nanalogous process is used to find and tune the mem-\nory head. Therefore, we identify one memory head\nand one in-context head (see Figure 5, Appendix\nC.3), each with their optimal α, as determined via\ntuning on the development set.\nFigure 5 shows the effect of the α parameter\non the proportion of in-context vs. memorized an-\nswers for both the memory and in-context heads\non Pythia-1.4b. Tuning the memory down has a\n9929\nFigure 6: We apply the chosen memory head (15.7) and\nin-context head (19.14) and the chosen respective scale\nfrom Figure 5, we apply the scaling intervention on all\nof the 62,992 examples. Negatively tuning the memory\nhead produced the most successful result.\nstrong effect on the generated text, flipping more\nthan 80% of the predictions to the given in-context\nanswers, and preventing the model from ever pro-\nducing the memorized answer. The other interven-\ntions show positive but weaker results. In general,\nthe in-context head is less effective at flipping pre-\ndictions, and promoting the memorized answer is\nmore difficult than promoting the in-context an-\nswer.\n6.3 Results of Interventions on the World\nCapital Dataset\nFigure 6 shows the intervention results on the full\nworld capital dataset with selected memory and\nin-context heads and their respective α. The result\naligns with our expectations. Negatively tuning\nthe memory head drastically increases proportion\nof the in-context answers. Specifically, whereas\nthe model originally predicted in-context answers\n26% of the time and memorized answers 43% of\nthe time, after our intervention, the model predicts\nin-context answers 86.2% of the time and memo-\nrized answers only 4% of the time. Note that, on\nPythia 1.4b, scaling a single head is analagous to\nmodifying 0.00001% of model parameters. This\nsuggests that this head plays a specific role in using\nthe memorized answer in this task. Positively tun-\ning the memory head also increases the memorized\nanswer prediction to 50%. Positively tuning the\nin-context head pushes the model in the expected\ndirection but has a more muted effect: increasing\nthe amount of in-context answers by 12% but drop-\nping the amount of memorized answers by about\n20%. We observe that changing in-context predic-\ntions to memorized predictions is more difficult. In\nthe fourth and fifth column, when positively tun-\ning the memory head and negatively tuning the\nin-context head, we hope to increase the propor-\ntion of memorized answers. While there is some\nincrease, it is less profound, only increasing 6%.\nGiven the connection between facts learned in pre-\ntraining and the MLP layers (Geva et al., 2021;\nMeng et al., 2023; Merullo et al., 2023), it’s possi-\nble that tuning attention alone is not enough to see\nhigher performance in this setting.\nWe break down the intervention results from\nFigure 7 into term frequency percentile bins as in\nSection 5. We focus on the occurrence count of the\ncountry and the occurrence count of the in-context\ncapital ( London). We select two interventions–\nnegatively tuning memory head and positively tun-\ning in-context head in Pythia-1.4b–both of which\nshould increase the in-context answers and de-\ncrease the memorized answers. We find that the\nintervention on the memory head overcomes the\npreviously-described frequency effects. Specifi-\ncally, the dashed blue and pink lines are flat across\npercentiles. When positively tuning the in-context\nhead, we observe that the frequency effects remain,\nand thus the intervention is not fully successful.\nIn particular, even after intervention, the memo-\nrized answers are still positively correlated with\nterm frequency, and the in-context answer is nega-\ntively correlated with frequency. Most prominently,\ntuning the in-context head does not substantially\nincrease the number of in-context answers when\nthe in-context city is high frequency (no mitigation\nof the city frequency effect) as shown by the blue\nlines in the 4th graph.\n6.4 Head Analysis\nTuning the memory head down inhibits the models’\nabilities to promote the memorized capital city as\nwe have shown in the previous section. In this sec-\ntion, we explore why this is the case by analyzing\nthe memory head weights. We find the selected\nmemory head (15.7) promotes geography related\ntokens in the output space, suggesting that this head\nis responsible for this information as opposed to a\nmore abstract ‘truthfulness’ direction.\n9930\nFigure 7: The frequency effect referred to in Section 5 disappears when we tune down the memory head, showing\nthe success of this strategy. Positively tuning the in-context head shows decent success on lower frequency\ncountries/capitals, but actually causes performance to fall apart in the higher frequency bins. The solid lines show\nthe original predictions and the dotted lines show predictions after the intervention.\nSingular Vector Decomposition The product of\nthe Value and Output weight matrices in an atten-\ntion head form the OV matrix (Elhage et al., 2021),\nwhich controls how attending to some token affects\nthe residual stream. Following Millidge and Black\n(2022); Belrose et al. (2023), we can decompose the\nOV matrix into SVD(OV) =USVh where Vh\nis the unitary matrix of the right singular vectors\nrepresenting the subspaces the given head writes\ninto the residual stream (as opposed to the Value\nweight matrix in OV). See Millidge and Black\n(2022); Belrose et al. (2023) for more information.\nThe ith singular vector has the same size as the\nresidual stream; if we decode this vector into the\nvocab space of the model with the unembedding\nmatrix (the LM head) we can observe the semantic\nclusters that a given head most strongly promotes.\nSince the singular vectors are ordered, we know\nthat the first singular vectors are the most important\nfor the head. We qualitatively define the semantic\nclusters promoted by each head by looking at the\ntop ktokens decoded from each singular vector.\nWe compare the memory head 15.7 with the con-\ntext head 19.4. If the memory head specifically\npromotes geographical information, we should\nsee clear emphasis on this information that is not\npresent in the context head. In Table 1, we de-\ncode the top 10 tokens from the first five singular\nvectors in each head and find that many of the mem-\nory head tokens are geographically focused. The\ntrend becomes even more clear when comparing\nall singular vectors (see Appendix G). It should\nbe noted that the alternate memory head studied\nin Appendix F is not as interpretable in the vocab\nspace, despite giving similar intervention results.\nUnderstanding the contribution of such heads is an\ninteresting direction to future work.\n6.5 Generalizing to New Data\nWe further explore the domain specificity of the\nmemory (and context) head by applying the method\nto the COUNTERFACT dataset (Meng et al.,\n2023), which queries factual knowledge from mul-\ntiple domains. We apply the same scale interven-\ntion to the heads in the discovered mechanism. We\nfocus on the paraphrase task of the dataset in the\nzero-shot setting. For example,\nApple A5 is developed by Google.\nApple A5 is created by\nThe dataset replaces the memorized answer\n(Google) with a counterfactual ( Apple). We fil-\nter the dataset down to examples where the model\npredicts the ground truth answer when the coun-\nterfactual prompts are not injected. We apply the\nsame memory head, in-context head and their re-\nspective intervention scale on the counterfactual\ndataset. That is, we do no additional data-set spe-\ncific analysis or tuning. We find that, despite the\nmemory head’s high impact on the world capital\ndataset (increasing the proportion of in-context an-\nswers by 60%) it doesn’t generalize to the COUN-\nTERFACT dataset. In both cases of interventions,\nboth the proportion of memorized answers and in-\ncontext answers decrease. The model produces a\nhigher proportion of invalid answers compared to\nthe intervention on the world capital dataset. This\n9931\nMemory Head (15.7)\n’ LW’, ’ Wade’, ’ WI’, ’liche’, ’ienne’, ’ ell’, ’owe’, ’iale’, ’uelle’, ’ете’\n’ Italian’, ’Italian’, ’ Italy’, ’ Ital’, ’ Io’, ’ Giovanni’, ’ pasta’, ’Io’, ’ Giul’, ’ Naples’\n’W A’, ’WS’, ’ W A’, ’owa’, ’ws’, ’wa’, ’Ws’, ’ Wa’, ’pora’, ’ WI’\n’ WM’, ’WM’, ’wm’, ’mw’, ’w’, ’nw’, ’ w’, ’ MW’, ’ Minnesota’, ’WN’\n’ Guatemala’, ’ Guatem’, ’usta’, ’osta’, ’ Tampa’, ’ Brazil’, ’ativa’, ’ Bah’, ’ Tamil’, ’Brazil’\nIn Context Head (19.4)\n’.,’, ’.;’, ’.\\u200b’, ’.*,’, ’.:’, ’.-’, ’.),’, ’.?’, ’.);’, ’.).’\n’ilogy’, ’vex’, ’必’, ’xspace’, ’verages’, ’loat’, ’?’, ’ゃ’, ’ cres’, ’HPP’\n’tron’, ’.%’, ’. ’, ’———’, ’ Salem’, ’ Telesc’, ’bsy’, ”.’,”, ’olean’, ’inn’\n’ometown’, ’LLY’, ’suit’, ’00000000’, ’ Caption’, ’ lib’, ’ETHERTYPE’, ’velt’, ’ESULT’, ’oxic’\n’..’, ’ ..’, ’..\\’, ’hers’, ’ DSL’, ’GHz’, ’ V ALUES’, ’..”’, ’mic’, ’ Experiment’\nTable 1: When comparing the decoded top 5 right singular vectors in the memory head vs. the context head, we\nnotice a clear trend in which the memory head especially encodes geography related information.\ncould be a result of the need for a more extended la-\nbel field. The COUNTERFACT dataset includes\nbroad label fields beyond geographical information\nsuch as names, dates and etc. The specific head\nwe selected (15.7) is shown to encode memory in a\nspecific field, therefore, this could lead to the poor\nperformance in COUNTERFACT.\n7 Discussion & Future Work\nThis paper investigates factors that influence a\nmodel’s propensity to favor in-context vs. mem-\norized factual associations, when the two compete\nwith one another. Our results demonstrate that the\nfrequency of information in the pretraining corpus\ncan affect the model’s tendency to use new, con-\nflicting information provided in context. Building\non this, we provide a proof of concept that this\ntendency can be controlled by a mechanism in the\nattention heads which allows us to manipulate LMs’\ntendency to prefer new in-context information with-\nout modifying any model parameters directly. By\nbuilding off insights from mechanistic interpretabil-\nity, we can localize single attention heads that con-\ntribute to this mechanism. This provides evidence\nthat decomposing complex neural networks into un-\nderstandable components is possible, even in mod-\nels with billions of parameters. Still, we observe\nthat the selected heads promote domain specific\nknowledge rather than a more abstract concept of\ntruthfulness. This brittleness is characteristic of\nmechanistic analyses of larger models, and should\nbe a priority for future work. Nonetheless, given\nthe early stage of research on this level of analy-\nsis of large language models, findings of this type\neven in an isolated setting are exciting and can lay\nthe groundwork for subsequently discovering more\ngeneral mechanisms.\nThe exploratory methods described here suggest\navenues via which future work might develop more\nsophisticated techniques for controlling and audit-\ning deployed language models. Adapting LMs post-\nhoc for applications that require domain-specific\ninformation is a growing problem. For example,\nthere are simultaneously reasons we might want to\nsuppress the use of in-context information at run\ntime (e.g., to combat prompt-injection attacks) as\nwell as reasons we might want to encourage it (e.g.,\nto enable users to provide new, personalized, or\nhypothetical information to the model). The inter-\nvention we describe in this work is intriguing in\nthat it can be used without changing the model and\ncan be turned on and off dynamically within the\nforward pass. It thus offers a promising direction\nfor further work on model editing.\n8 Conclusion\nIn the problem setting of predicting world capitals,\nour results show that the ability of language models\n(LMs) to overwrite information that it memorized\nin pretraining depends both on the frequency of the\nsubject of the new fact (the country, e.g., Poland),\nas well as the frequency of the overwriting informa-\ntion (the counterfactual city, e.g., London). We can\nintervene on attention heads that we find tend to\npush the prediction one way or another. By simply\nrescaling the value vectors of important heads, we\ncan control which city the model predicts without\nupdating any model parameters. We hope these\nresults encourage future work in understanding the\ninternal structure of neural networks in general.\n9932\nLimitations\nOur work aims to show that individual components\nin LMs can play predictable roles in certain model\nbehaviors; in this case, whether or not to overwrite\nmemorized information about world capitals. Fur-\nther work is required to understand how to control\nthe use of context or memorized information in\ngenerated text for this to be successfully applied\nin the most general cases. The dataset we use is\ntemplated and applied to the limited domain of\ncountry-capital relationships, meaning that we can\nnot make general statements about the role of in-\ndividual attention heads in arbitrary context. It is\nlikely, given the flexibility of LMs, that many dif-\nferent components can play this role depending on\nthe nature of the task. This work contributes to\nthe growing body of evidence that individual com-\nponents (e.g., attention heads) can specialize for\ncertain roles across contexts. We can not yet show\nhow to control this behavior in arbitrary settings,\nbut we provide a promising avenue for how this\nmight be done in the future.\n9 Acknowledgments\nWe thank Catherine Chen, William Rudman, Char-\nlie Lovering, Apoorv Khandelwal, Michael Lepori,\nLouis Castricato, Samuel Musker, Aaron Traylor,\nTian Yun for discussion and comments on this work.\nWe thank Daniel Wong for providing hardware sup-\nport during writing.\nEthics Statement\nOur work provides early indicators of future tools\nthat could aid in making models safer and more\ntrustworthy. Insights like those described could po-\ntentially lead to methods for better predicting how\nlanguage models might behave in certain settings,\npreventing models from generating personal infor-\nmation learned in pretraining (preventing access\nto some memorized information), or the opposite,\npreventing prompt injections from affecting model\nbehavior (preventing access to certain context infor-\nmation). Although we do not observe the quality of\ngenerated text changing substantially in our limited\nsetting, future work is needed to better understand\nhow manipulating the ‘intensity’ of model compo-\nnents, especially those which affect the recall of\npretraining information, can alter model behavior\nor make it easier to extract memorized text contain-\ning personal information.\nReferences\nNora Belrose, Zach Furman, Logan Smith, Danny Ha-\nlawi, Igor Ostrovsky, Lev McKinney, Stella Bider-\nman, and Jacob Steinhardt. 2023. Eliciting latent\npredictions from transformers with the tuned lens.\nStella Biderman, Hailey Schoelkopf, Quentin Anthony,\nHerbie Bradley, Kyle O’Brien, Eric Hallahan, Mo-\nhammad Aflah Khan, Shivanshu Purohit, USVSN Sai\nPrashanth, Edward Raff, Aviya Skowron, Lintang\nSutawika, and Oskar van der Wal. 2023. Pythia:\nA suite for analyzing large language models across\ntraining and scaling.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons in\npretrained transformers.\nGuy Dar, Mor Geva, Ankit Gupta, and Jonathan Berant.\n2022. Analyzing transformers in embedding space.\nNelson Elhage, Neel Nanda, Catherine Olsson, Tom\nHenighan, Nicholas Joseph, Ben Mann, Amanda\nAskell, Yuntao Bai, Anna Chen, Tom Conerly,\nNova DasSarma, Dawn Drain, Deep Ganguli, Zac\nHatfield-Dodds, Danny Hernandez, Andy Jones,\nJackson Kernion, Liane Lovitt, Kamal Ndousse,\nDario Amodei, Tom Brown, Jack Clark, Jared Ka-\nplan, Sam McCandlish, and Chris Olah. 2021. A\nmathematical framework for transformer circuits.\nTransformer Circuits Thread. Https://transformer-\ncircuits.pub/2021/framework/index.html.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang,\nHorace He, Anish Thite, Noa Nabeshima, Shawn\nPresser, and Connor Leahy. 2020. The pile: An\n800gb dataset of diverse text for language modeling.\nMor Geva, Jasmijn Bastings, Katja Filippova, and Amir\nGloberson. 2023. Dissecting recall of factual associ-\nations in auto-regressive language models.\nMor Geva, Roei Schuster, Jonathan Berant, and Omer\nLevy. 2021. Transformer feed-forward layers are\nkey-value memories.\nAdi Haviv, Ido Cohen, Jacob Gidron, Roei Schuster,\nYoav Goldberg, and Mor Geva. 2023. Understand-\ning transformer memorization recall through idioms.\nIn Proceedings of the 17th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics, pages 248–264, Dubrovnik, Croatia. As-\nsociation for Computational Linguistics.\n9933\nDieuwke Hupkes, Mario Giulianelli, Verna Dankers,\nMikel Artetxe, Yanai Elazar, Tiago Pimentel, Chris-\ntos Christodoulopoulos, Karim Lasri, Naomi Saphra,\nArabella Sinclair, et al. 2022. State-of-the-art gen-\neralisation research in nlp: a taxonomy and review.\narXiv preprint arXiv:2210.03050.\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric\nWallace, and Colin Raffel. 2022. Large language\nmodels struggle to learn long-tail knowledge.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2023. Feed-forward blocks control\ncontextualization in masked language models.\nJiaxuan Li, Lang Yu, and Allyson Ettinger. 2023. Coun-\nterfactual reasoning: Testing language models’ un-\nderstanding of hypothetical scenarios. arXiv preprint\narXiv:2305.16572.\nKevin Meng, David Bau, Alex Andonian, and Yonatan\nBelinkov. 2023. Locating and editing factual associa-\ntions in gpt.\nJack Merullo, Carsten Eickhoff, and Ellie Pavlick. 2023.\nA mechanism for solving relational tasks in trans-\nformer language models.\nBeren Millidge and Sid Black. 2022. The singular value\ndecompositions of transformer weight matrices are\nhighly interpretable. LessWrong.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstrations:\nWhat makes in-context learning work?\nNeel Nanda. 2022. Transformerlens.\nNeel Nanda, Lawrence Chan, Tom Lieberum, Jess\nSmith, and Jacob Steinhardt. 2023. Progress mea-\nsures for grokking via mechanistic interpretability.\nNostalgebraist. 2020. Interpreting gpt: The logit lens.\nChris Olah, Nick Cammarata, Ludwig Schubert, Gabriel\nGoh, Michael Petrov, and Shan Carter. 2020.\nZoom in: An introduction to circuits. Distill.\nHttps://distill.pub/2020/circuits/zoom-in.\nLianhui Qin, Antoine Bosselut, Ari Holtzman, Chandra\nBhagavatula, Elizabeth Clark, and Yejin Choi. 2019.\nCounterfactual story reasoning and generation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 5043–\n5053, Hong Kong, China. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nYasaman Razeghi, Robert L. Logan IV au2, Matt Gard-\nner, and Sameer Singh. 2022. Impact of pretraining\nterm frequencies on few-shot reasoning.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need.\nKevin Wang, Alexandre Variengien, Arthur Conmy,\nBuck Shlegeris, and Jacob Steinhardt. 2022. Inter-\npretability in the wild: a circuit for indirect object\nidentification in gpt-2 small.\nJerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert\nWebson, Yifeng Lu, Xinyun Chen, Hanxiao Liu,\nDa Huang, Denny Zhou, and Tengyu Ma. 2023.\nLarger language models do in-context learning dif-\nferently.\nSang Michael Xie, Aditi Raghunathan, Percy Liang,\nand Tengyu Ma. 2022. An explanation of in-context\nlearning as implicit bayesian inference.\nA Effect of Term Frequency on\nAdditional Models\nIn the main paper, we mainly focus on the term\nfrequency effect on the in-context and memo-\nrized predictions on Pythia-1.4b. We applied the\nsame analysis on the the rest of the Pythia models\nand series of GPT2 models: Pythia-70m, Pythia-\n160m, Pythia-410m, Pythia-1b, Pythia-2.8b, GPT2,\nGPT2-medium, GPT2-large, GPT2-xl. We observe\nthe similar conclusions as Section 5. Across all the\nmodels, there is a cohesive trend of blue line go-\ning down as the frequency increases and pink link\ngoing up. The same conclusion align: as the fre-\nquency goes up, models are more likely to predict\nthe memorized answers. As the frequency decrease,\nmodels prefer the in-context answers. See figures\n8, 9, 11, 10, 12,13, 14, 15, 16\n9934\nFigure 8: Frequency Effect on GPT2\nFigure 9: Frequency Effect on GPT2-medium\nFigure 10: Frequency Effect on GPT2-large\n9935\nFigure 11: Frequency Effect on GPT2-xl\nFigure 12: Frequency Effect on Pythia-70m\nFigure 13: Frequency Effect on Pythia-160m\n9936\nFigure 14: Frequency Effect on Pythia-410m\nFigure 15: Frequency Effect on Pythia-1bm\nFigure 16: Frequency Effect on Pythia-2.8b\n9937\nB Effect of Model Size\nHere, we show the effect of model size in relation\nto all of the measures of term frequency including\nthose which we did not include in the main paper.\nThis includes frequency with respect to the mem-\norized country, in-context capital, co-occurrences\nof the memorized country and capital, and the co-\noccurrences of the in-context capital and its corre-\nsponding country.\nB.1 Pythia Series\nSee figures 17, 18, 19, 20\nB.2 GPT2 Series\nSee figures 21, 22, 23, 24.\n9938\nFigure 17: Pythia-Effect of model sizes on predictions based on the frequency of country\nFigure 18: Pythia-Effect of model sizes on predictions based on the frequency of in-context capital\nFigure 19: Pythia-Effect of model sizes on predictions based on the frequency of country and memorized capital\nFigure 20: Pythia-Effect of model sizes on predictions based on the frequency of in-context capital and corresponding\ncountry\n9939\nFigure 21: GPT-2 Effect of model sizes on predictions based on the frequency of country\nFigure 22: GPT-2 Effect of model sizes on predictions based on the frequency of in-context capital\nFigure 23: GPT-2 Effect of model sizes on predictions based on the frequency of country and memorized capital\n9940\nFigure 24: GPT-2 Effect of model sizes on predictions based on the frequency of in-context capital and corresponding\ncountry\n9941\nModel Memory Head In-Context Head\nPythia-1.4b 15.7 19.12\nPythia-2.8b 17.17 17.31\ngpt2-xl 35.19 25.20\nTable 2: Specific memory head and in-context head\nC Head Attribution\nAttention heads are vectors of size dhead =\ndmodel/nh where nh is the number of heads in the\nmodel. The standard way to compute the output\nof attention layers is by concatenating all of the\nheads to form one dmodel sized vector, and pass-\ning it through an output weight matrix WH\nO of size\n(dmodel,dmodel). This would initially seem to pre-\nvent us from directly projecting an individual head\nfrom dmodel space into the vocab space using the\nunembedding matrix, since to get to dmodel space,\neach head has to interact in the WH\nO multiplication.\nHowever, as observed by (Elhage et al., 2021), this\noperation is equivalent to splitting the WH\nO matrix\ninto nh (dhead,dmodel) sized chunks, projecting in-\ndividual heads and adding the projections up. If we\nconsider the output of an attention layer by stack-\ning the attention result vector rh1 ,rh2 ,... and mul-\ntiply by an output matrix WH\nO , we can split WH\nO\ninto each size blocks for each heads [Wh1\nO ,Wh2\nO ...].\nTherefore, we can get the contribution to the atten-\ntion output for every head via Wh1\nO ˙rh1 .\nC.1 Memory Head & In-Context Head\nC.2 Individual Heatmap\nSee Figure 25\nC.3 Tuning Scale\nIn the main text, we show the tuning scale for\nPythia 1.4B. Here we present tuning curves for\nother models on their corresponding memory and\nin-context heads. See figures 26,27\n9942\nFigure 25: We extract individual heatmaps of head attribution across 20 randomly selected examples. We locate the\nmaximum positive values (4) and the the minimum negative value (-10). The scale is set from -10 to 10 to ensure\nthe same domain of all the maps. We can observe that there are more prominent memory head (dark red) compared\nto in-context head (dark blue). We can also see that the distribution of the memory heads and the in-context heads\nvary across different examples.\n9943\nFigure 26: Pythia-2.8b Tuning Scale\nFigure 27: GPT2-xl Tuning Scale\n9944\nFigure 28\nD Interventions\nHere, we provide the intervention experiments that\nwe could not include in the main text.\nD.1 World Capital Dataset\nSee figures 28, 29, 30\nD.2 COUNTERFACT Dataset\nSee figures 31, 32, 33\nD.3 Frequency Effect from Intervention\nSee figures 34, 35, 36, 37, 38, 39, 40, 41,42, 43, 44,\n45\nFigure 29\nFigure 30\nFigure 31\n9945\nFigure 32\nFigure 33\n9946\nFigure 34\nFigure 35\nFigure 36\nFigure 37\n9947\nFigure 38\nFigure 39\nFigure 40\nFigure 41\n9948\nFigure 42\nFigure 43\nFigure 44\nFigure 45\n9949\nE Decoded Result Analysis\nHere we look into specific decoded result before\nand after analysis to see how does the intervention\nchange the decoding. We focus on Pythia-1.4b for\nthis analysis. In the following example,\nThe capital of Antigua and Barbuda is\nTirana.\nQ: What is the capital of Antigua and\nBarbuda?\nA:\nThe original decoded text before intervention\nis:\nThe capital of Antigua and Barbuda is\nTirana\nQ: What is the capital of Antigua and\nBarbuda?\nA: Antigua and Barbuda is the capital\nof Antigua and Barbuda\nAfter the intervention by negatively tuning the\nmemory head,\nThe capital of Antigua and Barbuda is\nTirana\nQ: What is the capital of Antigua and\nBarbuda?\nA: The capital of Antigua and Barbuda\nis Tirana.\nWe observe that after the negative tuning the\nmemory head, one of the biggest change in the de-\ncoded text is that increase of decode text begin with\nThe capital of <country> is. In Pythia-1.4b,\nbefore the negatively tuning the memory head, only\n7961 decoded test begins with The capital of\n<country> is. After the intervention, this number\nraise to 57440. More than 90% of the predicted\nanswers begins with The capital of <country>\nis. This change significantly prompt the increase\nthe in-context. 92% of the text beginning with The\ncapital of <country> is predicts in-context\nanswer. By copying the injected prompt, the model\nare able to predict the in-context answer. We hy-\npothesize that tuning down the memory head will\nhelp the model to pay more attention to the given\nprompts and implement the copying task. More\nexperiments are required to test this hypothesis.\nMoreover, we also observe that a common rea-\nson for model to predict neither the in-context an-\nswer nor the memorized answer that the model will\njust repeat itself by outputting the given country\nname. In the above example, the model output A:\nAntigua and Barbuda is the capital of Antigua and\nBarbuda before intervention. 77% of the decoded\nanswer that gets neither the in-context answer nor\nthe memorized answer simply just repeat the given\ncountry name in context. 83% of these answer are\nchanged to predict the memorized answers after\nnegatively tuning the memory head. Negatively\ntuning the memory head can be responsible for\nshifting the copying mechanism from the country\nname ahead to the in-context capital.\n9950\nF Head 11.11 Analysis\nIn another set of sample, we find head 11.11 that\nhave the similar effect as the chosen head 15.7.\nHowever, we didn’t include this head in the main\npaper since this head offers less interpretable ex-\nplanation. See figures 46, 47, 48, 49, 50\n9951\nFigure 46: Scale graph on intervention for head 11.11\nFigure 47: Overwrite result with respective scale on Head 11.11 on the world-capital dataset\nFigure 48: frequency effect on negatively tuning head 11.11\n9952\nFigure 49: frequency effect on positively tuning head 11.1\nFigure 50: Overwrite result with respective scale on Head 11.11 on the COUNTERFACT dataset\n9953\nG Singular Value Decomposition\nWe included the top 10 decode for all 64 right\nsingular vectors in head 15.7 and head 19.4 in the\nmain paper to show that the memory head weights\nspecifically encourage geography related terms in\nthe next token prediction. 15.7 shows interpretable\ndecode clusters around the location information.\nHowever, head 19.4 didn’t show any meaningful\nclusters. We include the decodings for each of the\nsingular vectors in each head in this section, see\nTables 3 4.\n9954\n’ LW’, ’ Wade’, ’ WI’, ’liche’, ’ienne’, ’ ell’, ’owe’, ’iale’, ’uelle’, ’\\u0435 \\u0442 \\u0435’\n’ Italian’, ’Italian’, ’ Italy’, ’ Ital’, ’ Io’, ’ Giovanni’, ’ pasta’, ’Io’, ’ Giul’, ’ Naples’\n’W A’, ’WS’, ’ W A’, ’owa’, ’ws’, ’wa’, ’Ws’, ’ Wa’, ’pora’, ’ WI’\n’ WM’, ’WM’, ’wm’, ’mw’, ’w’, ’nw’, ’ w’, ’ MW’, ’ Minnesota’, ’WN’\n’ Guatemala’, ’ Guatem’, ’usta’, ’osta’, ’ Tampa’, ’ Brazil’, ’ativa’, ’ Bah’, ’ Tamil’, ’Brazil’\n’Greek’, ’ Greek’, ’ Greece’, ’ Knox’, ’ Greeks’, ’kappa’, ’ Athens’, ’greek’, ’ Kim’, ’ Athen’\n’ Ji’, ’ MN’, ’ NJ’, ’ Swiss’, ’ Jiang’, ’ Peng’, ’Italian’, ’ Italian’, ’azz’, ’elli’\n’ Kansas’, ’ NH’, ’NH’, ’ Nebraska’, ’ Omaha’, ’ NZ’, ’maha’, ’nek’, ’ Nepal’, ’ Het’\n’ Brazilian’, ’ Og’, ’Brazil’, ’ Brazil’, ’ Nigerian’, ’ Ethiop’, ’ Brasil’, ’ Nigeria’, ’ Danish’, ’ Ethiopia’\n’KM’, ’ KL’, ’mM’, ’Kam’, ’KD’, ’ HK’, ’mw’, ’MW’, ’ LM’, ’KH’\n’BN’, ’ BN’, ’ HK’, ’ ku’, ’ Uk’, ’Uk’, ’ RN’, ’ Yuk’, ’ Nak’, ’HK’\n’ Kaz’, ’ mos’, ’ Raz’, ’ KL’, ’ Jed’, ’ Malaysian’, ’ Malay’, ’ Sultan’, ’ Kom’, ’ KK’\n’MK’, ’ Panthers’, ’ Argentine’, ’ Spart’, ’ Carolina’, ’AZ’, ’ MK’, ’South’, ’zc’, ’ Chile’\n’ Moz’, ’ OM’, ’ Miz’, ’ Sach’, ’ MG’, ’XM’, ’ O’, ’OWS’, ’OMN’, ’ Judaism’\n’ NK’, ’ Kore’, ’ Milwaukee’, ’ ND’, ’ NL’, ’KN’, ’ Korea’, ’ Norway’, ’NL’, ’ Koh’\n’gh’, ’ Henderson’, ’Gh’, ’nh’, ’vh’, ’ Dh’, ’GH’, ’dh’, ’ GH’, ’ Gh’\n’ Rio’, ’ Trin’, ’ Mississippi’, ’AO’, ’ Munich’, ’Mb’, ’vor’, ’BV’, ’\\u00F2’, ’Brazil’\n’ Filip’, ’ Mexican’, ’ Philippines’, ’lv’, ’ Manila’, ’ V A’, ’NV’, ’ NV’, ’ Philippine’, ’ ANC’\n’KP’, ’ KDE’, ’ Hung’, ’DK’, ’Hung’, ’KR’, ’Viet’, ’ Kai’, ’ KD’, ’asian’\n’HT’, ’ Hos’, ’EH’, ’ HT’, ’ Texans’, ’ Haw’, ’HA’, ’ Texas’, ’UH’, ’ HI’\n’w’, ’wi’, ’ Wu’, ’ w’, ’ Wa’, ’ Kai’, ’wu’, ’W A’, ’nw’, ’ W A’\n’ Malta’, ’ Miami’, ’Dutch’, ’ Midd’, ’ Dutch’, ’ Amsterdam’, ’ Caribbean’, ’ Netherlands’, ’mf’,\n’ Jamaica’\n’ li’, ’ l’, ’ LI’, ’ Li’, ’ lis’, ’ LA’, ’LL’, ’lb’, ’ LN’, ’LB’\n’ Kumar’, ’ Indianapolis’, ’ Krish’, ’ji’, ’ Mari’, ’ Birmingham’, ’ Kansas’, ’ MG’, ’IU’, ’ Indy’\n’ Abdullah’, ’ Alger’, ’FN’, ’ FN’, ’bn’, ’ Saudi’, ’ Ahmed’, ’ BN’, ’ Niger’, ’ Belgium’\n’ Ohio’, ’VT’, ’ OH’, ’Ohio’, ’ Wisconsin’, ’ V olkswagen’, ’ VT’, ’\\u0442 \\u0435’, ’vt’, ’ Hamilton’\n’ Indones’, ’ Indonesian’, ’ Indonesia’, ’ Ramos’, ’ Lopez’, ’ Flores’, ’Dutch’, ’ Java’, ’ Luxem’,\n’ Jorge’\n’wc’, ’RPC’, ’ Va’, ’ Richmond’, ’WS’, ’ Dou’, ’ CR’, ’ Fran\\u00E7ois’, ’qs’, ’ WS’\n’ Trent’, ’ TT’, ’ Gomez’, ’ Scotia’, ’TT’, ’ Dalton’, ’ Thom’, ’Tg’, ’ammar’, ’ Iceland’\n’ Montana’, ’ Alaska’, ’sdk’, ’ Plains’, ’ Norway’, ’ Sau’, ’ FX’, ’ Kashmir’, ’Pak’, ’ Mong’\n’ Sanchez’, ’Mi’, ’ Fernando’, ’ Miami’, ’SI’, ’Indian’, ’ Si’, ’ Sar’, ’ S\\u00E3o’, ’ IB’\n’ Japan’, ’ Japanese’, ’ Singapore’, ’Japanese’, ’Japan’, ’ Taiwan’, ’jp’, ’jn’, ’ Tokyo’, ’ Lee’\n’ Kurd’, ’ Iraqi’, ’ EF’, ’ Ion’, ’acci’, ’ Hond’, ’ Tex’, ’ Ecuador’, ’ Texas’, ’ Iraq’\n’ AM’, ’ Abe’, ’AMD’, ’ Am’, ’MV’, ’ AMD’, ’AM’, ’amber’, ’V A’, ’ AMP’\n’ BV’, ’BV’, ’ Eb’, ’ Kab’, ’kB’, ’BH’, ’bv’, ’ vess’, ’bbe’, ’VB’\n’oux’, ’ Iowa’, ’ Sz’, ’ Shelby’, ’ Memphis’, ’ocy’, ’ Saskatchewan’, ’ Ottawa’, ’ Sask’, ’Sz’\n’ Lah’, ’LF’, ’ Nass’, ’fel’, ’lf’, ’ Levy’, ’ Nottingham’, ’ LF’, ’ FN’, ’ TN’\n’ Hamburg’, ’ Texans’, ’ Houston’, ’ Tex’, ’Houston’, ’WL’, ’ TEXAS’, ’gia’, ’GAL’, ’Gi’\n’HF’, ’ Holl’, ’ Hockey’, ’hf’, ’ HF’, ’ Argent’, ’FH’, ’ Argentina’, ’ HG’, ’ HM’\n’ ETH’, ’ Seth’, ’ SG’, ’ iodine’, ’ Eph’, ’ Belfast’, ’ETH’, ’GS’, ’ Ish’, ’IE’\n’French’, ’ Bav’, ’ French’, ’ Gust’, ’ w’, ’ Bou’, ’ franc’, ’EG’, ’Ot’, ’ TG’\n’OE’, ’ PF’, ’ PE’, ’ fp’, ’opf’, ’ PO’, ’EP’, ’ PDE’, ’ EP’, ’ Porter’\n’ Rag’, ’ Maur’, ’ Dh’, ’ RCC’, ’ Karn’, ’Mah’, ’uid’, ’ Kal’, ’ Rodrig’, ’ Mah’\n’Lau’, ’ l’, ’EC’, ’ Laurent’, ’ fro’, ’Au’, ’ Tigers’, ’Indian’, ’chen’, ’ Lac’\n’Hay’, ’hay’, ’ Chang’, ’ Hay’, ’ Bulls’, ’hl’, ’Rh’, ’ Kosovo’, ’ epiderm’, ’ SCC’\n’ UC’, ’UC’, ’ U’, ’uca’, ’ ud’, ’ u’, ’ Sacramento’, ’ UP’, ’ uc’, ’ UDP’\n’QS’, ’HS’, ’ Ecuador’, ’qs’, ’Sb’, ’ HS’, ’WS’, ’ SES’, ’oS’, ’ns’\n9955\n’ Ig’, ’PQ’, ’ Slav’, ’ Iz’, ’CPP’, ’ Pav’, ’ Shapiro’, ’hens’, ’IQ’, ’RCC’\n’OA’, ’ Ole’, ’ Ale’, ’ TL’, ’APE’, ’ Tol’, ’ Salvador’, ’Ale’, ’ Sul’, ’ SAL’\n’BT’, ’ Damascus’, ’PID’, ’ISP’, ’BD’, ’ Wis’, ’IBLE’, ’BI’, ’ Brady’, ’ Illinois’\n’India’, ’ India’, ’ IA’, ’ EL’, ’ Modi’, ’ JE’, ’ Indians’, ’ AE’, ’ Io’, ’ Wick’\n’ Os’, ’OSS’, ’ei’, ’ees’, ’ Pirates’, ’EE’, ’e’, ’Os’, ’OS’, ’OG’\n’vb’, ’ Bor’, ’VB’, ’pnt’, ’ BA’, ’ Bil’, ’BER’, ’ Bir’, ’Bir’, ’ Brun’\n’yg’, ’yy’, ’YS’, ’YP’, ’yc’, ’isi’, ’wei’, ’ Feld’, ’Eh’, ’Y’\n’PY’, ’ Zam’, ’ Tay’, ’Ay’, ’ Ky’, ’ y’, ’ Ay’, ’Ky’, ’ Theo’, ’Py’\n’nj’, ’ Egg’, ’ Jets’, ’ eggs’, ’ Yuan’, ’ECD’, ’Io’, ’ egg’, ’ IJ’, ’ ERR’\n’FER’, ’ Fran’, ’rf’, ’Fran’, ’IF’, ’ iT’, ’ Fang’, ’fi’, ’RF’, ’RL’\n’AW’, ’ AW’, ’AQ’, ’QA’, ’aq’, ’alias’, ’ AA’, ’ AO’, ’ BA’, ’ Falk’\n’ticos’, ’xB’, ’TES’, ’ IB’, ’ross’, ’IRST’, ’chos’, ’abis’, ’oracle’, ’robl’\n’ Hogan’, ’INGTON’, ’ Hyde’, ’ GT’, ’anson’, ’ Duncan’, ’ISPR’, ’ Roland’, ’ GD’, ’ Dum’\n’ember’, ’ Indians’, ’aste’, ’ Gem’, ’Ind’, ’stem’, ’ruby’, ’ Ghana’, ’estead’, ’ Rails’\n’ORK’, ’agma’, ’pb’, ’iq’, ’mr’, ’illo’, ’MSO’, ’ork’, ’mq’, ’ Amend’\n’ Rom’, ’ BB’, ’ rom’, ’Rom’, ’BR’, ’ Rams’, ’ Ramos’, ’ BR’, ’ Rangers’, ’BB’\n’tics’, ’iast’, ’ n\\u00FA’, ’gue’, ’ MN’, ’inx’, ’ilor’, ’ concess’, ’yc’, ’CTOR’\nTable 3: The top 10 decoded tokens for each right singular vector from the memory head 15.7\n9956\n’.,’, ’.;’, ’.\\u200B’, ’.*,’, ’.:’, ’.-’, ’.\\),’, ’.?’, ’.);’, ’.).’\n’ilogy’, ’vex’, ’\\u5FC5’, ’xspace’, ’verages’, ’loat’, ’\\uFFFD’, ’\\u3083’, ’ cres’, ’HPP’\n’tron’, ’.%’, ’. ’, ’———’, ’ Salem’, ’ Telesc’, ’bsy’, ”.’,”, ’olean’, ’inn’\n’ometown’, ’LLY’, ’suit’, ’00000000’, ’ Caption’, ’ lib’, ’ETHERTYPE’, ’velt’, ’ESULT’, ’oxic’\n’..’, ’ ..’, ’..\n’, ’hers’, ’ DSL’, ’GHz’, ’ V ALUES’, ’..”’, ’mic’, ’ Experiment’\n’ Integr’, ’Ob’, ’ Lands’, ’ harass’, ’whe’, ’ess’, ’ Land’, ’obenz’, ’acks’, ’ lord’\n’hea’, ’omics’, ’olu’, ’xa’, ’imus’, ’Ui’, ’irement’, ’ilder’, ’uren’, ’coin’\n’lor’, ’ Shot’, ’icult’, ’...’, ’java’, ’CUIT’, ’iento’, ’ Secondary’, ’Secondary’, ’iday’\n’ITED’, ’odel’, ’oda’, ’bench’, ’bie’, ’ peninsula’, ’olan’, ’igr’, ’pres’, ’itable’\n’ano’, ’boro’, ’Tg’, ’TN’, ’prises’, ’bil’, ’gen’, ’\n!\n!\n!’, ’heimer’, ’ Gen’\n’ ups’, ’skins’, ’dead’, ’ranging’, ’slant’, ’ JD’, ’posts’, ’PUBL’, ’thood’, ’arshal’\n’quo’, ’ Sign’, ’ibi’, ’express’, ’sign’, ’corner’, ’itten’, ’furt’, ’alam’, ’\\u53F7’\n’ Karn’, ’ chains’, ’ delays’, ’ Sout’, ’ 549’, ’ delay’, ’hog’, ’JavaScript’, ’NAME’, ’ember’\n’Transport’, ’Tra’, ’bus’, ’ Luc’, ’L’, ’ L’, ’cr’, ’ Del’, ’ Gl’, ’ask’\n’ .’, ’(.’, ’ GB’, ’\\u0101r’, ’GB’, ’opan’, ’azol’, ’r\\u00E4’, ’SUM’, ’.&’\n’rier’, ’extensions’, ’jh’, ’AUD’, ’oda’, ’ scler’, ’PLC’, ’pie’, ’ROW’, ’root’\n’omin’, ’\\uFFFD \\uFFFD’, ’osin’, ’ys’, ’ Reuters’, ’yzed’, ’DLL’, ’ctive’, ’GV’, ’ content’\n’rient’, ’olev’, ’gen’, ’urd’, ’LAY’, ’IENT’, ’inus’, ’heed’, ’ al’, ’ZH’\n’rial’, ’ precision’, ’ret’, ’feet’, ’st’, ’ WM’, ’Execution’, ’MC’, ’\\u00E2t’, ’oc’\n’ Mars’, ’half’, ’ half’, ’in’, ’ Notes’, ’ privately’, ’URN’, ’ halves’, ’Execution’, ’ Spar’\n’esan’, ’udson’, ’rese’, ’nar’, ’CHANT’, ’ hooked’, ’onium’, ’gus’, ’Orientation’, ’esium’\n’azer’, ’cons’, ’orno’, ’ deput’, ’)\\u2013’, ’gtr’, ’uffix’, ’iele’, ’ennas’, ’din’\n’Kay’, ’FT’, ’itor’, ’oda’, ’izards’, ’xym’, ’raj’, ’ aster’, ’IRST’, ’gether’\n’¡¿’, ’¿::’, ’erton’, ’cats’, ’spec’, ’bo’, ’cA’, ’hk’, ’h\\u00E4’, ’ curv’\n’teenth’, ’deal’, ’aughters’, ’xsl’, ’ check’, ’abin’, ’datab’, ’ Furn’, ’ots’, ’ride’\n’Cur’, ’ sch’, ’A’, ’Gh’, ’inetics’, ’ possession’, ’ knob’, ’ thousands’, ’aler’, ’ favour’\n’\\u671F’, ’ DEAL’, ’otto’, ’uff’, ’ decks’, ’nolimits’, ’assign’, ’afen’, ’deal’, ’ reload’\n’shots’, ’CRA’, ’rim’, ’ARS’, ’ bonds’, ’ $@’, ’ Buch’, ’ incumbent’, ’ contacts’, ’ars’\n’-¿’, ’ango’, ’eti’, ’ Procedure’, ’k\\u00E4’, ’beh’, ’-¿ ’, ’\n,’, ’]-¿’, ’ Entry’\n’J’, ’uppose’, ’ G’, ’ Coin’, ’ risk’, ’[[’, ’mathb’, ’coin’, ’ Jac’, ’ trail’\n’ K’, ’ k’, ’kubernetes’, ’ocent’, ’ Mats’, ’rels’, ’aren’, ’nger’, ’intf’, ’izione’\n’POSE’, ’obox’, ’7554’, ’ivers’, ’ai’, ’erral’, ’Ax’, ’AUX’, ’ahi’, ’lett’\n’VERTIS’, ’pendicular’, ’OF’, ’new’, ’zel’, ’hores’, ’aser’, ’fills’, ’ establ’, ’del’\n’ Rate’, ’unks’, ’rors’, ’RATE’, ’ITC’, ’acional’, ’IBLE’, ’Rate’, ’ Jen’, ’TL’\n’api’, ’Vill’, ’pec’, ’inction’, ’iere’, ’raw’, ’ Wis’, ’ CTL’, ’ources’, ’ determin’\n’ vacated’, ’ vacate’, ’cos’, ’ competitor’, ’rtl’, ’ather’, ’floor’, ’Cos’, ’below’, ’@’\n’ende’, ’ilor’, ’velle’, ’ Licensed’, ’ ai’, ’ai’, ’ transition’, ’transition’, ’ surname’, ’ Wiley’\n’leans’, ’zing’, ’eman’, ’agine’, ’anca’, ’adow’, ’ahan’, ’ Vu’, ’elta’, ’anic’\n’ Eug’, ’\\uFFFD’, ’ Thames’, ’Resolver’, ’ulic’, ’chro’, ’\\u00F4’, ’gels’, ’oku’, ’EMENT’\n’idden’, ’ugs’, ’Pix’, ’ pla’, ’ lid’, ’untu’, ’upe’, ’unds’, ’Uri’, ’omi’\n’rite’, ’hem’, ’RR’, ’ONS’, ’hib’, ’ Hos’, ’\\u307E \\u305B’, ’nice’, ’ INS’, ’\\uFFFD’\n’@’, ’ERTYPE’, ’minimum’, ’\\u308D’, ’ugu’, ’ Coy’, ’\\u00F8re’, ’unde’, ’fff’, ’ Buffalo’\n’ monot’, ’t’, ’ pace’, ’ione’, ’ denied’, ’ening’, ’ST’, ’ Rot’, ’ahan’, ’cons’\n’front’, ’ries’, ’encial’, ’artifactId’, ’ Reader’, ’ Set’, ’osis’, ’container’, ’llo’, ’rg’\n9957\n’ Meet’, ’Category’, ’seek’, ’MIX’, ’ambers’, ’Meet’, ’category’, ’ Articles’, ’onitrile’, ’umns’\n’qrt’, ’album’, ’ipart’, ’ERE’, ’ MAC’, ’msgstr’, ’Winter’, ’ENS’, ’letal’, ’eville’\n’enda’, ’ Za’, ’ipro’, ’NOS’, ’ Va’, ’ ectopic’, ’ Accept’, ’decor’, ’”\n}](#’, ’\\uFFFD’\n’\\u043B \\u044E’, ’opyright’, ’nex’, ’cept’, ’\\u043E \\u0431 \\u044B’, ’itel’,\n’olecule’, ’YY’, ’azol’, ’omic’\n’yset’, ’isan’, ’APS’, ’ Fly’, ’REC’, ’apolis’, ’ Pont’, ’anos’, ’hov’, ’Face’\n’DEV’, ’owa’, ’...\n’, ’uy’, ’swe’, ’ fd’, ’micromachines’, ’dev’, ’ NG’, ’rud’\n’k\\u00E9’, ’gle’, ’ieri’, ’inki’, ’attach’, ’ requ’, ’state’, ’mak’, ’Text’, ’State’\n’XT’, ’roc’, ’ ============================================\n====================’,\n’letal’, ’rican’, ’xt’, ’ =&’, ’elled’, ’rapper’, ’ Sci’\n’aptop’, ’com’, ’\\uB2C8 \\uB2E4’, ’npmjs’, ’target’, ’commit’, ’ Ct’, ’ lust’, ’\\u017Ce’, ’compact’\n’uin’, ’printStackTrace’, ’ozo’, ’ Jay’, ’uously’, ’ford’, ’decode’, ’ondon’, ’iance’, ’nbsp’\n’ilon’, ’yard’, ’W ARD’, ’ovember’, ’ moonlight’, ’yc’, ’rud’, ’ seller’, ’IPT’, ’ immature’\n’ beyond’, ’ Rivers’, ’your’, ’[ ’, ’beyond’, ’ Tut’, ’prob’, ’#:’, ’ Claus’, ’ Gore’\n’\\uFFFD’, ’ transitions’, ’\\uFFFD’, ’orf’, ’ECT’, ’ Mu’, ’isu’, ’¡’, ’eng’, ’embedded’\n’ieg’, ’ally’, ’IE’, ’icus’, ’vphantom’, ’OC’, ’ais’, ’.”).’, ’ilus’, ’amin’\n’CRO’, ’ ly’, ’\\u00A0 \\u00A0 \\u00A0 \\u00A0 \\u00A0\n\\u00A0 \\u00A0 \\u00A0 \\u00A0 \\u00A0 \\u00A0\n\\u00A0 \\u00A0 \\u00A0 \\u00A0 \\u00A0 \\u00A0\n\\u00A0 \\u00A0 \\u00A0 \\u00A0 \\u00A0 \\u00A0\n\\u00A0 \\u00A0 \\u00A0 \\u00A0 \\u00A0 \\u00A0 \\u00A0 \\u00A0 \\u00A0’,\n’eston’, ’INV AL’, ’fr’, ’ bead’, ’aden’, ’ICK’, ’slant’\n’-¿’, ’ pitch’, ’arent’, ’pitch’, ’pher’, ’ pitches’, ”[’”, ’iop’, ’kowski’, ’Vers’\n’ucid’, ’tty’, ’uve’, ’\\u0163’, ’Testing’, ’getText’, ’ Carey’, ’mys’, ’RESULTS’, ’precision’\n’ cure’, ’ curative’, ’\\u00E4 \\u00E4n’, ’mathchoice’, ’ARR’, ’aires’, ’ super’, ’Super’, ’Bits’, ’ sex’\n’NAM’, ’PR’, ’ batt’, ’asti’, ’Names’, ’ DPP’, ’ pd’, ’NAP’, ’ illustrated’, ’NAME’\n’cho’, ’code’, ’gang’, ’chal’, ’TF’, ’activ’, ’rip’, ’\\uFFFD’, ’\\u00F1o’, ’och’\nTable 4: The top 10 decoded tokens for each right singular vector from the context head 19.4\n9958\n’\n9959",
  "topic": "Counterfactual thinking",
  "concepts": [
    {
      "name": "Counterfactual thinking",
      "score": 0.8488956689834595
    },
    {
      "name": "Computer science",
      "score": 0.7849171161651611
    },
    {
      "name": "Context (archaeology)",
      "score": 0.7545372247695923
    },
    {
      "name": "Prefix",
      "score": 0.6083455085754395
    },
    {
      "name": "Control (management)",
      "score": 0.537401556968689
    },
    {
      "name": "Competition (biology)",
      "score": 0.5083391070365906
    },
    {
      "name": "Language model",
      "score": 0.49571502208709717
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4791233539581299
    },
    {
      "name": "Recall",
      "score": 0.46143245697021484
    },
    {
      "name": "Context model",
      "score": 0.42183488607406616
    },
    {
      "name": "Machine learning",
      "score": 0.3428218364715576
    },
    {
      "name": "Cognitive psychology",
      "score": 0.25974276661872864
    },
    {
      "name": "Psychology",
      "score": 0.15062189102172852
    },
    {
      "name": "Linguistics",
      "score": 0.12616953253746033
    },
    {
      "name": "Social psychology",
      "score": 0.09246161580085754
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I175594653",
      "name": "John Brown University",
      "country": "US"
    }
  ]
}