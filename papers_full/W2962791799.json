{
  "title": "Personalized neural language models for real-world query auto completion",
  "url": "https://openalex.org/W2962791799",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A1959352324",
      "name": "Nicolas Fiorini",
      "affiliations": [
        "National Center for Biotechnology Information"
      ]
    },
    {
      "id": "https://openalex.org/A2150237561",
      "name": "Zhiyong Lu",
      "affiliations": [
        "National Center for Biotechnology Information"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2102575587",
    "https://openalex.org/W2522306626",
    "https://openalex.org/W2131744502",
    "https://openalex.org/W2462998714",
    "https://openalex.org/W2081828059",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2964199361",
    "https://openalex.org/W144183690",
    "https://openalex.org/W581956982",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2093245971",
    "https://openalex.org/W1982858363",
    "https://openalex.org/W2164986850",
    "https://openalex.org/W2964331683",
    "https://openalex.org/W2949555952",
    "https://openalex.org/W2027691696",
    "https://openalex.org/W2124637141",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2163303745",
    "https://openalex.org/W2115584760",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2162965868",
    "https://openalex.org/W2594645728",
    "https://openalex.org/W2740195281",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2564395123",
    "https://openalex.org/W2060575618"
  ],
  "abstract": "Nicolas Fiorini, Zhiyong Lu. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers). 2018.",
  "full_text": "Proceedings of NAACL-HLT 2018, pages 208–215\nNew Orleans, Louisiana, June 1 - 6, 2018.c⃝2017 Association for Computational Linguistics\nPersonalized neural language models for real-world query auto\ncompletion\nNicolas Fiorini\nNational Center for Biotechnology Information\nNational Library of Medicine, NIH\nBethesda, MD, USA\nnicolas.fiorini@nih.gov\nZhiyong Lu\nNational Center for Biotechnology Information\nNational Library of Medicine, NIH\nBethesda, MD, USA\nzhiyong.lu@nih.gov\nAbstract\nQuery auto completion (QAC) systems are a\nstandard part of search engines in industry,\nhelping users formulate their query. Such sys-\ntems update their suggestions after the user\ntypes each character, predicting the user’s in-\ntent using various signals — one of the most\ncommon being popularity. Recently, deep\nlearning approaches have been proposed for\nthe QAC task, to speciﬁcally address the main\nlimitation of previous popularity-based meth-\nods: the inability to predict unseen queries. In\nthis work we improve previous methods based\non neural language modeling, with the goal of\nbuilding an end-to-end system. We particu-\nlarly focus on using real-world data by inte-\ngrating user information for personalized sug-\ngestions when possible. We also make use of\ntime information and study how to increase di-\nversity in the suggestions while studying the\nimpact on scalability. Our empirical results\ndemonstrate a marked improvement on two\nseparate datasets over previous best methods\nin both accuracy and scalability, making a step\ntowards neural query auto-completion in pro-\nduction search engines.\n1 Introduction\nPredicting the next characters or words follow-\ning a preﬁx has had multiple uses from helping\nhandicapped people (Swifﬁn et al., 1987) to, more\nrecently, helping search engine users (Cai et al.,\n2016). In practice, most search engines today use\nquery auto completion (QAC) systems, consisting\nof suggesting queries as users type in the search\nbox (Fiorini et al., 2017). The task suffers from\nhigh dimensionality, because the number of possi-\nble solutions increases as the length of the target\nquery increases. Historically, the query prediction\ntask has been addressed by relying on query logs,\nparticularly the popularity of past queries (Bar-\nYossef and Kraus, 2011; Lu et al., 2009). The idea\nis to rely on the wisdom of the crowd, as popular\nqueries matching a typed preﬁx are more likely to\nbe the user’s intent.\nThis traditional approach is usually referred\nto as MostPopularCompletion (MPC)(Bar-Yossef\nand Kraus, 2011). However, the performance of\nMPC is skewed: it is very high for popular queries\nand very low for rare queries. At the extreme,\nMPC simply cannot predict a query it has never\nseen. This becomes a bigger problem in academic\nsearch (Lankinen et al., 2016), where systems are\ntypically less used, with a wider range of possible\nqueries. Recent advances in deep learning, partic-\nularly in semantic modeling (Mitra and Craswell,\n2015) and neural language modeling (Park and\nChiba, 2017) showed promising results for pre-\ndicting rare queries. In this work, we propose to\nimprove the state-of-the-art approaches in neural\nQAC by integrating personalization and time sen-\nsitivity information as well as addressing current\nMPC limitations by diversifying the suggestions,\nthus approaching a production-ready architecture.\n2 Related work\n2.1 Neural query auto completion\nWhile QAC has been well studied, the ﬁeld has\nrecently started to shift towards deep learning-\nbased models, which can be categorized into two\nmain classes: semantic models (using Convolu-\ntional Neural Nets, or CNNs) (Mitra and Craswell,\n2015) and language models (using Recurrent Neu-\nral Nets, or RNNs) (Park and Chiba, 2017). Both\napproaches are frequently used in natural language\nprocessing in general (Kim et al., 2016) and tend\nto capture different features. In this work, we fo-\ncus on RNNs as they provide a ﬂexible solution to\ngenerate text, even when it is not previously seen\nin the training data.\nYet, recent work in this ﬁeld (Park and Chiba,\n2017) suffers from some limitations. Most im-\nportantly, the probability estimates for full queries\n208\nare directly correlated to the length of the sug-\ngestions, consequently favoring shorter queries\nin some cases and hampering some predictions\n(Park and Chiba, 2017). By appending these re-\nsults to MPC’s and re-ranking the list with Lamb-\ndaMART (Burges, 2010) in another step as sug-\ngested in previous work (Mitra and Craswell,\n2015), they achieve state-of-the-art performance\nin neural query auto completion at the cost of a\nhigher complexity and more computation time.\n2.2 Context information\nStill, these preliminary approaches have yet to in-\ntegrate standards in QAC, e.g. query personal-\nization (Koutrika and Ioannidis, 2005; Margaris\net al., 2018) and time sensitivity (Cai et al., 2014).\nThis integration has to differ from traditional ap-\nproaches by taking full advantage of neural lan-\nguage modeling. For example, neural language\nmodels could be reﬁned to capture interests of\nsome users as well as their actual language or\nquery formulation. The same can apply to time-\nsensitivity, where the probability of queries might\nchange over time (e.g. for queries such as “tv\nguide”, or “weather”). Furthermore, the feasibil-\nity of these approaches in real-world settings has\nnot been demonstrated, even more so on special-\nized domains.\nBy addressing these issues, we make the follow-\ning contributions in this work compared to the pre-\nvious approaches:\n•We propose a more straightforward architec-\nture with improved scalability;\n•Our method integrates user information\nwhen available as well as time-sensitivity;\n•We propose to use a balanced beam search\nfor ensuring diversity;\n•We test ona second datasetand compare the\ngeneralizability of different methods in a spe-\ncialized domain;\n•Our method achieves stronger performance\nthan the state of the art on both datasets.\nFinally, our source code is made available in a\npublic repository1. This allows complete repro-\nducibility of our resultsand future comparisons.\n1https://github.com/ncbi-nlp/NQAC\n3 Methods\n3.1 Personalized neural Language Model\nThe justiﬁcation of using a neural language model\nfor the task of predicting queries is that it has been\nproven to perform well to generate text that has\nnever been seen in the training data (Sutskever\net al., 2011). Particularly, character-level models\nwork with a ﬁner granularity. That is, if a given\npreﬁx has not been seen in the training data (e.g.\na novel or incomplete word), the model can use\nthe information shared across similar preﬁxes to\nmake a prediction nonetheless.\nRecurrent Neural Network The difﬁculty\nof predicting queries given a preﬁx is that the\nnumber of candidates explodes as the query\nbecomes longer. RNNs allow to represent each\ncharacter (or word) of a sequence as a cell state,\ntherefore reducing the dimensionality of the task.\nHowever, they also introduce the vanishing gradi-\nent problem during backpropagation, preventing\nthem from learning long-term dependencies. Both\ngated recurrent units (GRU) (Cho et al., 2014) and\nlong-short term memory cells (LSTMs) solve this\nlimitation — albeit with a different approach —\nand are increasingly used. In preliminary experi-\nments, we tried various forms of RNNs: vanilla\nRNNs, GRUs and LSTMs. GRUs performed\nsimilarly to LSTM with a smaller computational\ncomplexity due to fewer parameters to learn as\nwas previously observed (Jozefowicz et al., 2015).\nWord embedded character-level Neural\nLanguage ModelThe main novelty in (Park and\nChiba, 2017) is to combine a character-level neu-\nral language model with a word-embedded space\ncharacter. The incentive is that character-level\nneural language models beneﬁt from a ﬁner gran-\nularity for predictions but they lack the semantic\nunderstanding words-level models provide, and\nvice versa. Therefore, they encode text sequences\nusing one-hot encoding of characters, character\nembedding and pre-trained word embedding\n(using word2vec (Mikolov et al., 2013)) of the\nprevious word when a space character is encoun-\ntered. Our preliminary results showed that the\ncharacter embedding does not bring much to the\nlearning, so we traded it with the context feature\nvectors below to save some computation time\nwhile enriching the model with additional, diverse\ninformation.\n209\nUser representationWe make the assumption\nthat the way a user types a query is a function of\ntheir actual language/vocabulary, but also a func-\ntion of their interests. Therefore, a language model\ncould capture these user characteristics to better\npredict the query, if we feed the learner with the\ninformation. Each query qi is a set of words such\nthat qi = {w1,...,w n}. Uis a column matrix and a\nuser u∈U is characterized by the union of words\nin their k past queries, i.e. Qu = ∪k\ni=1qi. The\nobjective is to reduce, for each user, the vocabu-\nlary used in their queries to a vector of a dimen-\nsionality d of choice, or Qu → Rd. We chose\nd = 30, in order to stay in the same computa-\ntion order of previous work using character em-\nbedding (Park and Chiba, 2017). To this end, we\nadapted the approach PV-DBOW detailed in (Le\nand Mikolov, 2014). That is, at each training iter-\nation, a random word wi is sampled from Qu. The\nmodel is trained by maximizing the probability of\npredicting the user ugiven the word wi, i.e.:\n1\n|U|\n∑\nu∈U\n∑\nwi∈Qu\nlogP (u|wi). (1)\nThe resulting vectors are stored for each user ID\nand are used as input for the neural net (NN) (see\nArchitecture section).\nTime representation As an example, in the\nbackground data (see Section 4.1), the query “tv\nguide” appears 1,682 times and it is vastly repre-\nsented in evening and nights. For this reason, we\npropose to integrate time features in the language\nmodel. While there has been more elaborated ap-\nproaches to model it in the past (Shokouhi and\nRadinsky, 2012), we instead propose a straightfor-\nward encoding and leave the rest of the work to the\nneural net. For each query, we look at the time it\nwas issued, consisting of hour x , minute y and\nsecond z, and we derive the following features:\nsin\n(2π(3600x+ 60y+ z)\n86400\n)\n,\ncos\n(2π(3600x+ 60y+ z)\n86400\n)\n.\n(2)\nThis encoding has the beneﬁt of belonging to\n[−1,1], which is a range comparable to the rest\nof the features. It is also capable to model cyclic\ndata, which is important particularly around\nboundaries (e.g. considering a query at 11:55PM\nand another at 00:05AM). We proceed the same\nway to encode weekdays and we end up with four\ntime features.\nOverall architecture An overview of the ar-\nchitecture is proposed in Figure 1. The input of\nour neural language model is a concatenation of\nthe vectors deﬁned above, for each character and\nfor each query in the training set. We use zero-\npadding after the “ \\n” character to keep the se-\nquence length consistent, and the NN learns to rec-\nognize it. We feed this input vector into 2 layers\nof 1024 GRUs2, each followed by a dropout layer\n(with a dropout rate of 50%) to prevent overﬁtting.\nEach GRU cell is activated with ReLu(x) = x+\nand gradients are clipped to a norm of0.5 to avoid\ngradient exploding problems. The output of the\nsecond dropout layer is fed to a temporal softmax\nlayer, which allows to make predictions at each\nstate. The softmax function returns the probabil-\nity P(ci|c1,...,c i−1) of the character ci given the\nprevious characters of the sequence, which is then\nused to calculate the loss function by comparing it\nto the next character in the target query. Instead\nof using the objective denoted in (Park and Chiba,\n2017), we minimize the lossLdeﬁned as the aver-\nage cross entropy of this probability with the ref-\nerence probability ˆP(ci) across all queries, that is\nL=\n− 1\n|Q|\n∑\nq∈Q\n|q|−1∑\ni=1\nˆP(ci+1) ×logP (ci+1|c1,...,c i).\n(3)\nQis the set of queries in the training dataset, |Q|\nis the total number of queries in the set and |q|\nis the number of characters in the query q. Con-\nvergence stabilizes around 5-10 epochs for the\nAOL dataset (depending on the model) and 15-20\nepochs for the biomedical specialized dataset (see\nSection 4.1).\n3.2 Balanced diverse beam search\nThe straightforward approach for decoding the\nmost likely output sequence — in this case, a suf-\nﬁx given a preﬁx — is to use a greedy approach.\nThat is, we feed the preﬁx into the trained NN and\npick the most likely output at every step, until the\nsequence is complete. This approach has a high\n2It was reported that using more cells may not help the\nprediction while hurting computation (Park and Chiba, 2017).\n210\nFigure 1: Architecture of our proposed model.\nchance to output a locally optimal sequence and\na common alternative is to use a beam search in-\nstead. We propose to improve the beam search\nby adding a greedy heuristic within it, in order to\naccount for the diversity in the results. A similar\nsuggestion has been made in (Vijayakumar et al.,\n2016), and our proposition differs by rebalancing\nthe probabilities after diversity was introduced. In\n(Vijayakumar et al., 2016), at every step the most\nlikely prediction is not weighted while all others\nare, by greedily comparing them. This approach\neffectively always prefers the most likely charac-\nter over all other alternatives at each step. The\nﬁrst result will thus be the same as the local op-\ntimum using a greedy approach, which becomes\nproblematic for QAC where order is critical. By\nrebalancing the probability of the most likely sug-\ngestion with the average diversity weight given to\nother suggestions, we make sure probabilities stay\nuniform yet suggestions are diverse. We use a nor-\nmalized Levenshtein distance to assess the diver-\nsity.\n4 Experiments\n4.1 Dataset\nThe AOL query logs (Pass et al., 2006) are com-\nmonly used to evaluate the quality of QAC sys-\ntems. We rely on a background dataset for the\nNN; training and validation datasets for lamb-\ndaMART integrations; and a test dataset for eval-\nuations. Some adaptations are done to the AOL\nbackground dataset as in (Park and Chiba, 2017),\nsuch as removing the queries appearing less than 3\ntimes or longer that 100 characters. For each query\nin the training, validation and test datasets, we use\nall possible preﬁxes starting after the ﬁrst word as\nin (Shokouhi, 2013). We use the sets from (Park\nand Chiba, 2017) available online, enriched with\nuser and time information provided in the original\nAOL dataset. In addition, we evaluate the systems\non a second real-world dataset from a production\nsearch engine in the biomedical domain, PubMed\n(Fiorini et al., 2017; Lu, 2011; Mohan et al., 2018),\nthat was created in the same manner. The biomed-\nical dataset consists of 8,490,317 queries. The\nsizes of training, validation and test sets are com-\nparable to those used for the AOL dataset.\n4.2 Evaluation\nSystems are evaluated using the traditional Mean\nReciprocal Rank (MRR) metric. This metric as-\nsesses the quality of suggestions by identifying the\nrank of the real query in the suggestions given one\nof its preﬁxes. We also tested PMRR as introduced\nin (Park and Chiba, 2017) and observed the same\ntrends in results as MRR, so we do not show them\ndue to space limitation. Given the set of preﬁxes\n211\nP in the test dataset, MRR is deﬁned as follows:\nMRR = 1\n|Q|\n∑\nr∈P\n1\nrp\n, (4)\nwhere rp represent the rank of the match. Paired\nt-tests measure the signiﬁcance of score variations\namong systems and are reported in the Results sec-\ntion. We also evaluate prediction time as this is an\nimportant parameter for building production sys-\ntems. The prediction time is averaged over 10 runs\non the test set, on the same hardware for all mod-\nels. We do not evaluate throughput but rather com-\npare the time required by all approaches to process\none preﬁx.\n4.3 Systems and setups\nWe implemented the method in (Park and Chiba,\n2017) and used their best-performing model as a\nbaseline. We also compare our results to the stan-\ndard MPC (Bar-Yossef and Kraus, 2011). For our\nmethod, we evaluate several incremental versions,\nstarting with NQAC which follows the architec-\nture detailed above but with the word embeddings\nand the one-hot encoding of characters only. We\nadd the subscript U when the language model is\nenriched with user vectors and T when it inte-\ngrates time features. We append+D to indicate the\nuse of the diverse beam search to predict queries\ninstead of a standard beam search. Finally, we\nalso study the impact of adding MPC and Lamb-\ndaMART (+MPC, +λMART).\n5 Results\nA summary of the results is presented in Table 1.\nInterestingly, our simple NQAC model performs\nsimilarly to the state-of-the-art on this dataset,\ncalled Neural Query Language Model (NQLM),\non all queries. It is signiﬁcantly less good for seen\nqueries (-5.6%) and signiﬁcantly better for unseen\nqueries (+4.2%). Although GRUs have less ex-\npressive power than LSTMs, their smaller num-\nber of parameters to train allowed them to bet-\nter converge than all LSTM models we tested, in-\ncluding that of (Park and Chiba, 2017). NQAC\nalso beneﬁts from a signiﬁcantly better scalabil-\nity (28% faster than NQLM) and thus seems more\nappropriate for production systems. When we\nenrich the language model with user informa-\ntion, it becomes better for seen queries (+1.9%)\nwhile being about as fast. Adding time sensitiv-\nity does not yield signiﬁcant improvements on this\ndataset overall, but improves signiﬁcantly the per-\nformance for seen queries (+1.7%). Relying on\nthe diverse beam search signiﬁcantly hurts the pro-\ncessing time (39% longer) while not providing sig-\nniﬁcantly better performance. Our integration of\nMPC differs from previous studies. We noticed\nthat for Web search, MPC performs extremely\nwell and is computationally cheap (0.24 seconds).\nOn the other hand, all neural QAC systems are\nbetter for unseen queries but struggle to stay un-\nder a second of processing time. Since identifying\nif a query has been seen or not is done in con-\nstant time, we route the query either to MPC or\nto NQACUT and we note the overall performance\nas NQACUT +MPC. This method provides a sig-\nniﬁcant improvement over NQLM (+6.7%) overall\nwhile being faster on average. Finally, appending\nNQACUT ’s results to MPC’s and reranking the list\nwith LambdaMART provides the best results on\nthis dataset, but at the expense of greater compu-\ntational cost (+60%).\nWhile NQACUT +MPC appears clearly as the\nbest compromise between performance and qual-\nity for the AOL dataset, the landscape changes\ndrastically on the biomedical dataset and the qual-\nity drops signiﬁcantly for all systems. This shows\nthe potential difﬁculties associated with real-world\nsystems, which particularly occur in specialized\ndomains. In this case, the drop in performance\nis mostly due to the fact that biomedical queries\nare longer and it becomes more difﬁcult for mod-\nels to predict the entire query accurately only with\nthe ﬁrst keywords. While the generated queries\nmake sense and are relevant candidates, the chance\nfor generative models to predict the exact target\nquery diminishes as the target query is longer be-\ncause of combinatorial explosion. This is even\nmore true when the target queries are diverse as in\nspecialized domains (Islamaj Dogan et al., 2009;\nN´ev´eol et al., 2011). For example, for the pre-\nﬁx “breast cancer”, there are 1169 diverse sufﬁxes\nin a single day of logs used for training. These\ninclude “local recurrence”, “nodular prognosis”,\n“hormone receptor”, “circulating cells”, “family\nhistory”, “chromosome 4p16” or “herceptin re-\nview”, to cite only a few. Hence, while the model\npredicts plausible queries, it is a lot more difﬁ-\ncult to predict the one the user intended. The tar-\nget query length also has an impact on prediction\ntime, as roughly twice the time is needed for Web\nsearches. MPC is the exception, however, it per-\n212\nTable 1: MRR results for all tested models on the AOL and biomedical datasets with their average prediction time\nin seconds.\nAOL dataset Biomedical dataset\nModel MRR Time MRR Time\nSeen Unseen All Seen Unseen All\nMPC (Bar-Yossef and Kraus, 2011) 0.461 0.000 0.184 0.24 0.165 0.000 0.046 0.29\nNQLM(L)+WE+MPC+λMART (Park and Chiba, 2017) 0.430 0.306 0.356 1.33 0.159 0.152 0.154 2.35\nOur models in this paper\nNQAC 0.406 0.319 0.354 0.94 0.155 0.139 0.143 1.73\nNQACU 0.417 0.325 0.361 0.98 0.191 0.161 0.169 1.77\nNQACUT 0.424 0.326 0.365 0.95 0.101 0.195 0.157 1.81\nNQACUT+D 0.427 0.326 0.366 1.32 0.186 0.185 0.185 2.04\nNQACUT+MPC 0.461 0.326 0.380 0.68 0.165 0.195 0.187 1.20\nNQACUT+MPC+λMART 0.459 0.330 0.382 1.09 0.154 0.179 0.172 2.01\nforms poorly even on seen queries (0.165). This\nobservation suggests that more elaborate models\nare speciﬁcally needed for specialized domains.\nOn this dataset, NQAC does not perform as well as\nNQLM and it seems this time that the higher num-\nber of parameters in NQLM is more appropriate\nfor the task. Still, user information helps signiﬁ-\ncantly for seen queries (+23%), probably because\nsome users frequently check the same queries to\nkeep up-to-date. Time sensitivity seems to help\nsigniﬁcantly unseen queries (+21%) while signiﬁ-\ncantly hurting the quality for seen queries (-47%).\nDiversity is signiﬁcantly helpful on this dataset\n(+19%) and provides a balance in performance for\nboth seen and unseen queries. NQAC UT +MPC\nyields the best overall MRR score for this dataset,\nand LambdaMART is unable to learn how to re-\nrank the suggestions, thus decreasing the score.\nFrom these results, we draw several conclusions.\nFirst, MPC performs very well on seen queries\nfor Web searches and it should be used on them.\nFor unseen queries, the NQAC UT model we pro-\npose achieves a sub-second state-of-the-art perfor-\nmance. Second, it is clear that the ﬁeld of appli-\ncation will affect many of the decisions when de-\nsigning a QAC system. On a specialized domain,\nthe task is more challenging: fast approaches like\nMPC perform too poorly while more elaborate\napproaches do not meet production requirements.\nNQACU performs best on seen queries, NQACUT\non unseen queries. Finally, NQACUT +D provides\nan equilibrium between the two at a greater com-\nputational cost. Its overall MRR is similar to that\nof NQACUT +MPC but it is less redundant (see Ta-\nble 2). Particularly, the system seems not to be\nlimited anymore by the higher probability associ-\nTable 2: Comparison of the 10 top query candidates\nfrom the baselines and our approach for the preﬁx\n“www”.\nMPC (Park and Chiba, 2017) NQAC+D\nwww google com www google com www google comwww yahoo com www yahoo com www myspace comwww myspace com www myspace com www mapquest comwww google www google www yahoo comwww ebay com www hotmail com www hotmail comwww hotmail com www my www bankofamerica comwww mapquest com www myspace com www chase comwww myspace www mapquest com www disneychannel comwww msn com www yahoo www myspacewww bankofamerica com www disney channel com www disney channel com\nated with shorter suggestions (e.g. “www google”,\na form of “www google com”), thus bringing more\ndiversity. This aspect can be more useful for\nspecialized domains where the range of possible\nqueries is broader. Finally, we found that a lot\nmore data was needed for the biomedical domain\nthan for general Web search. After about a million\nqueries, NQAC suggests meaningful and plausi-\nble queries for both datasets. However, for the\nbiomedical dataset, the loss needs more epochs to\nstabilize than for the AOL dataset, mainly due to\nthe combinatorial explosion mentioned above.\n6 Conclusions and future work\nTo the best of our knowledge, we proposed the ﬁrst\nneural language model that integrates user infor-\nmation and time sensitivity for query auto com-\npletion with a focus on scalability for real-world\nsystems. Personalization is provided through pre-\ntrained user vectors based on their past queries.\nBy incorporating this information and by adapt-\ning the architecture, we were able to achieve state-\nof-the-art performance in neural query auto com-\npletion without relying on re-ranking, making this\napproach signiﬁcantly more scalable in practice.\n213\nWe studied multiple variants, their beneﬁts and\ndrawbacks for various use cases. We also demon-\nstrate the utility of this method for specialized do-\nmains such as biomedicine, where the query diver-\nsity and vocabulary are broader and MPC fails to\nprovide the same performance as in Web search.\nWe also found that user information and diversity\nimprove the performance signiﬁcantly more than\nfor Web search engines. To allow readers to easily\nreproduce, evaluate and improve our models, we\nprovide all the code on a public repository.\nThe handling of time-sensitivity may beneﬁt from\na more elaborate integration, for example session-\nbased rather than absolute time. Also, we evalu-\nated our approaches on a general search setup for\nboth datasets, while searches in the biomedical do-\nmain commonly contain ﬁelds (i.e. authors, title,\nabstract, etc.) which adds to the difﬁculty. The\nchoice of a diversity metric is also important and\ncould be faster or more efﬁcient (e.g., using word\nembeddings to diversify the semantics of the sug-\ngestions). These limitations warrant further work\nand we leave them as perspectives.\nAcknowledgement\nThis research was supported by the Intramural Re-\nsearch Program of the NIH, National Library of\nMedicine.\nReferences\nZiv Bar-Yossef and Naama Kraus. 2011. Context-\nsensitive query auto-completion. In Proceedings\nof the 20th international conference on World wide\nweb, pages 107–116. ACM.\nChristopher JC Burges. 2010. From ranknet to lamb-\ndarank to lambdamart: An overview. Learning,\n11(23-581):81.\nFei Cai, Maarten De Rijke, et al. 2016. A survey\nof query auto completion in information retrieval.\nFoundations and TrendsR⃝in Information Retrieval,\n10(4):273–363.\nFei Cai, Shangsong Liang, and Maarten De Rijke.\n2014. Time-sensitive personalized query auto-\ncompletion. In Proceedings of the 23rd ACM in-\nternational conference on conference on informa-\ntion and knowledge management, pages 1599–1608.\nACM.\nKyunghyun Cho, Bart Van Merri¨enboer, Dzmitry Bah-\ndanau, and Yoshua Bengio. 2014. On the properties\nof neural machine translation: Encoder-decoder ap-\nproaches. arXiv preprint arXiv:1409.1259.\nNicolas Fiorini, David J Lipman, and Zhiyong Lu.\n2017. Cutting edge: Towards pubmed 2.0. eLife,\n6:e28801.\nRezarta Islamaj Dogan, G Craig Murray, Aur ´elie\nN´ev´eol, and Zhiyong Lu. 2009. Understanding\npubmed R⃝user search behavior through log analy-\nsis. Database, 2009.\nRafal Jozefowicz, Wojciech Zaremba, and Ilya\nSutskever. 2015. An empirical exploration of recur-\nrent network architectures. In International Confer-\nence on Machine Learning, pages 2342–2350.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In AAAI, pages 2741–2749.\nGeorgia Koutrika and Yannis Ioannidis. 2005. A uni-\nﬁed user proﬁle framework for query disambigua-\ntion and personalization. In Proceedings of work-\nshop on new technologies for personalized informa-\ntion access, pages 44–53.\nMatti Lankinen, Hannes Heikinheimo, Pyry Takala,\nTapani Raiko, and Juha Karhunen. 2016. A\ncharacter-word compositional neural language\nmodel for ﬁnnish. arXiv preprint arXiv:1612.03266.\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188–1196.\nZhiyong Lu. 2011. Pubmed and beyond: a survey\nof web tools for searching biomedical literature.\nDatabase, 2011.\nZhiyong Lu, W John Wilbur, Johanna R McEntyre,\nAlexey Iskhakov, and Lee Szilagyi. 2009. Finding\nquery suggestions for pubmed. In AMIA Annual\nSymposium Proceedings, volume 2009, page 396.\nAmerican Medical Informatics Association.\nDionisis Margaris, Costas Vassilakis, and Panagiotis\nGeorgiadis. 2018. Query personalization using so-\ncial network information and collaborative ﬁltering\ntechniques. Future Generation Computer Systems,\n78:440–450.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems, pages 3111–3119.\nBhaskar Mitra and Nick Craswell. 2015. Query auto-\ncompletion for rare preﬁxes. In Proceedings of the\n24th ACM international on conference on informa-\ntion and knowledge management, pages 1755–1758.\nACM.\nSunil Mohan, Nicolas Fiorini, Sun Kim, and Zhiyong\nLu. 2018. A fast deep learning model for textual rel-\nevance in biomedical information retrieval. CoRR,\nabs/1802.10078.\n214\nAur´elie N ´ev´eol, Rezarta Islamaj Do ˘gan, and Zhiy-\nong Lu. 2011. Semi-automatic semantic annota-\ntion of pubmed queries: a study on quality, efﬁ-\nciency, satisfaction. Journal of biomedical informat-\nics, 44(2):310–318.\nDae Hoon Park and Rikio Chiba. 2017. A neural lan-\nguage model for query auto-completion. In Pro-\nceedings of the 40th International ACM SIGIR Con-\nference on Research and Development in Informa-\ntion Retrieval, pages 1189–1192. ACM.\nGreg Pass, Abdur Chowdhury, and Cayley Torgeson.\n2006. A picture of search. In InfoScale, volume\n152, page 1.\nMilad Shokouhi. 2013. Learning to personalize query\nauto-completion. In Proceedings of the 36th inter-\nnational ACM SIGIR conference on Research and\ndevelopment in information retrieval, pages 103–\n112. ACM.\nMilad Shokouhi and Kira Radinsky. 2012. Time-\nsensitive query auto-completion. In Proceedings of\nthe 35th international ACM SIGIR conference on\nResearch and development in information retrieval,\npages 601–610. ACM.\nIlya Sutskever, James Martens, and Geoffrey E Hin-\nton. 2011. Generating text with recurrent neural\nnetworks. In Proceedings of the 28th International\nConference on Machine Learning (ICML-11), pages\n1017–1024.\nAndrew Swifﬁn, John Arnott, J Adrian Pickering, and\nAlan Newell. 1987. Adaptive and predictive tech-\nniques in a communication prosthesis. Augmenta-\ntive and Alternative Communication, 3(4):181–191.\nAshwin K Vijayakumar, Michael Cogswell, Ram-\nprasath R Selvaraju, Qing Sun, Stefan Lee, David\nCrandall, and Dhruv Batra. 2016. Diverse beam\nsearch: Decoding diverse solutions from neural se-\nquence models. arXiv preprint arXiv:1610.02424.\n215",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8296513557434082
    },
    {
      "name": "RDF query language",
      "score": 0.518782377243042
    },
    {
      "name": "Natural language processing",
      "score": 0.49647217988967896
    },
    {
      "name": "Query language",
      "score": 0.4867279827594757
    },
    {
      "name": "Language model",
      "score": 0.4785773754119873
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46367713809013367
    },
    {
      "name": "Query by Example",
      "score": 0.4174308180809021
    },
    {
      "name": "Information retrieval",
      "score": 0.3655584454536438
    },
    {
      "name": "Web search query",
      "score": 0.25633102655410767
    },
    {
      "name": "Web query classification",
      "score": 0.16249138116836548
    },
    {
      "name": "Search engine",
      "score": 0.07736483216285706
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210109390",
      "name": "National Center for Biotechnology Information",
      "country": "US"
    }
  ],
  "cited_by": 39
}