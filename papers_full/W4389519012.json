{
  "title": "KICGPT: Large Language Model with Knowledge in Context for Knowledge Graph Completion",
  "url": "https://openalex.org/W4389519012",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2170408830",
      "name": "Wei Yan-bin",
      "affiliations": [
        "Southern University of Science and Technology",
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2287249697",
      "name": "Huang Qiushi",
      "affiliations": [
        "Southern University of Science and Technology",
        "University of Surrey"
      ]
    },
    {
      "id": "https://openalex.org/A2102355683",
      "name": "Zhang Yu",
      "affiliations": [
        "Southern University of Science and Technology"
      ]
    },
    {
      "id": null,
      "name": "Kwok, Tin Yau",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4389524426",
    "https://openalex.org/W2997897037",
    "https://openalex.org/W2759136286",
    "https://openalex.org/W4313483544",
    "https://openalex.org/W1533230146",
    "https://openalex.org/W4283828387",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2889234142",
    "https://openalex.org/W2971167006",
    "https://openalex.org/W4320839455",
    "https://openalex.org/W2728059831",
    "https://openalex.org/W4377865170",
    "https://openalex.org/W4225632115",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W3099206682",
    "https://openalex.org/W3082429057",
    "https://openalex.org/W2995448904",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2432356473",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W2250635077",
    "https://openalex.org/W2949434543",
    "https://openalex.org/W2499696929",
    "https://openalex.org/W4366559971",
    "https://openalex.org/W3201503287",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W205829674",
    "https://openalex.org/W3155001903",
    "https://openalex.org/W3103296573",
    "https://openalex.org/W2014415866",
    "https://openalex.org/W4389520779",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W4226278401"
  ],
  "abstract": "Knowledge Graph Completion (KGC) is crucial for addressing knowledge graph incompleteness and supporting downstream applications. Many models have been proposed for KGC. They can be categorized into two main classes: triple-based and text-based approaches. Triple-based methods struggle with long-tail entities due to limited structural information and imbalanced entity distributions. Text-based methods alleviate this issue but require costly training for language models and specific finetuning for knowledge graphs, which limits their efficiency. To alleviate these limitations, in this paper, we propose KICGPT, a framework that integrates a large language model (LLM) and a triple-based KGC retriever. It alleviates the long-tail problem without incurring additional training overhead. KICGPT uses an in-context learning strategy called Knowledge Prompt, which encodes structural knowledge into demonstrations to guide the LLM. Empirical results on benchmark datasets demonstrate the effectiveness of KICGPT with smaller training overhead and no finetuning. © 2023 Association for Computational Linguistics.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8667–8683\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nKICGPT: Large Language Model with Knowledge in Context for\nKnowledge Graph Completion\nYanbin Wei1, 2, Qiushi Huang1,3, Yu Zhang1∗, James T. Kwok2\n1 Southern University of Science and Technology\n2 Hong Kong University of Science and Technology\n3 University of Surrey\nAbstract\nKnowledge Graph Completion (KGC) is cru-\ncial for addressing knowledge graph incom-\npleteness and supporting downstream applica-\ntions. Many models have been proposed for\nKGC. They can be categorized into two main\nclasses: triple-based and text-based approaches.\nTriple-based methods struggle with long-tail\nentities due to limited structural information\nand imbalanced entity distributions. Text-based\nmethods alleviate this issue but require costly\ntraining for language models and specific fine-\ntuning for knowledge graphs, which limits their\nefficiency. To alleviate these limitations, in\nthis paper, we propose KICGPT, a framework\nthat integrates a large language model (LLM)\nand a triple-based KGC retriever. It allevi-\nates the long-tail problem without incurring\nadditional training overhead. KICGPT uses an\nin-context learning strategy called Knowledge\nPrompt, which encodes structural knowledge\ninto demonstrations to guide the LLM. Empiri-\ncal results on benchmark datasets demonstrate\nthe effectiveness of KICGPT with smaller train-\ning overhead and no finetuning.\n1 Introduction\nKnowledge Graphs (KGs) are powerful representa-\ntions of real-world knowledge. The relationships\namong entities are captured by triples of the form\n<head entity, relation, tail entity>. KGs serve as\nthe foundation for various applications such as rec-\nommendation systems, question-answering, and\nknowledge discovery. Knowledge Graph Comple-\ntion (KGC) plays a crucial role for KGs by com-\npleting incomplete triples and hence addressing the\ninherent incompleteness of KGs. This paper fo-\ncuses on the link prediction task in KGC, which\nis to predict the missing entity in an incomplete\ntriple.\nBased on the source of information used, ex-\nisting KGC methods can be categorized into two\n∗Corresponding author\nmain classes: triple-based methods and text-based\nmethods (Wang et al., 2022a). Triple-based meth-\nods (e.g., TransE (Bordes et al., 2013), R-GCN\n(Schlichtkrull et al., 2018), and HittER (Chen et al.,\n2021)) utilize the structure of the knowledge graph\nas the only source of information for KGC. Given\nthe typical imbalance of entities in KGs, long-tail\nentities are prevalent and the KGC task has limited\nstructural information about them. Consequently,\ndue to information scarcity, the performance of\ntriple-based methods tends to degrade when pro-\ncessing long-tail entities (Wang et al., 2022a). Text-\nbased methods (such as KG-BERT (Yao et al.,\n2019)) mitigate this information scarcity problem\nby encoding textual description as an extra infor-\nmation source (Wang et al., 2022a). Besides the\nuse of pre-trained language models (PLMs), fine-\ntuning remains necessary for text-based methods to\nhandle diverse knowledge graphs. However, this is\nresource-intensive and requires task-specific fine-\ntuning for each downstream task.\nLarge language models (LLMs), such as Chat-\nGPT and GPT-4 (OpenAI, 2023), have extensive\ninternal knowledge repositories from their vast pre-\ntraining corpora, which can be used as an extra\nknowledge base to alleviate information scarcity\nfor long-tail entities. Tay et al. (2022), Ouyang\net al. (2022) demonstrated the potential for LLMs\nto handle a wide range of tasks by specially-\ndesigned prompts without requiring training or fine-\ntuning. Given the appealing properties of LLMs, a\nstraightforward approach is to directly apply LLMs\nto KGC tasks. However, empirical evidence (Zhu\net al., 2023) indicates that pure LLMs still can-\nnot achieve state-of-the-art performance in KGC\ntasks such as link prediction. Additionally, there\nare several challenges that hinder the application\nof LLM on KGC tasks. First, the LLM outputs can\nbe unconstrained and may fall outside the scope\nof entities in the KGs. Second, LLMs impose\nlength limits on the input tokens, and the limits\n8667\nare far from sufficient for describing a complete\nKGC task. Lastly, there is no effective in-context\nlearning prompt design for LLM on KGC tasks.\nTo alleviate the aforementioned limitations, we\npropose in this paper a Knowledge In Context\nwith GPT (KICGPT) framework. This integrates\nLLMs with a traditional structure-aware KG model\n(which is called a retriever). Specifically, for each\nquery q = (h, r,?) or q = (?, r, t) in the link pre-\ndiction task, where “ ?\" denotes the missing tail\nor head entity to be predicted, the retriever first\nprocesses the query q independently and gener-\nates an ordered candidate entity list Rretriever that\nranks all entities in the KG based on their retrieval\nscores. The LLM then performs re-ranking on the\ntop m entities returned by Rretriever , and replaces\nthese m entities with re-ranked ones returned by the\nLLM as the final result RKICGPT . To achieve the\nLLM re-ranking, we propose Knowledge Prompt,\na strategy of in-context learning (ICL) (Wei et al.,\n2022), to encode the KG knowledge into demon-\nstrations in prompts. Note that in KICGPT we\nonly need to train the retriever. Compared with\nexisting triple-based methods, KICGPT enables si-\nmultaneous utilization of knowledge sources (i.e.,\nKG and LLM’s knowledge base) and facilitates\ntheir alignment and enrichment to alleviate infor-\nmation scarcity for long-tail entities. Moreover, dif-\nferent from existing text-based models, KICGPT\nleverages a much larger semantic knowledge base\nwithout incurring additional training overhead. Un-\nlike directly applying the LLM to the KGC task,\nwhich may generate undesired outputs, the pro-\nposed KICGPT constrains its output by formalizing\nlink prediction as a re-ranking task for the given\nsequence Rretriever . Moreover, the standard link\nprediction task requires ranking for all the entities,\nwhich is not feasible for LLM due to the length\nlimit on input tokens. To overcome this limitation,\nKICGPT utilizes the retriever to obtain the top-m\nentities in Rretriever , and only allows the LLM to\nperform re-ranking on these entities.\nIn summary, our contributions are as follows.\n• We propose a novel cost-effective framework\nKICGPT for KGC tasks. To the best of our\nknowledge, this is the first work that combines\nLLMs with triple-based KGC methods, offer-\ning a unique solution to address the task.\n• We propose a novel in-context learning strat-\negy, Knowledge Prompt, specifically de-\nsigned for KGC.\n• Extensive experiments on benchmark datasets\ndemonstrate that KICGPT achieves state-of-\nthe-art performance with low training over-\nhead.\n2 Related Work\nTriple-based KGC Most existing KGC meth-\nods are triple-based methods, which complete the\nknowledge graph solely based on the triple infor-\nmation. Early shallow knowledge graph embed-\nding (KGE) methods represent entities and rela-\ntionships as low-dimensional embedding vectors\nin a continuous embedding space. Based on the\nscoring function, these methods can be further cat-\negorized (Wang et al., 2017) as translation-based\n(e.g., TransE (Bordes et al., 2013)) and semantic\nmatching models (e.g., RESCAL (Nickel et al.,\n2011) and DistMult (Yang et al., 2014)).\nHowever, they suffer from limited expressive\npower due to the use of shallow network structures.\nIn recent years, more powerful network structures\nare integrated to solve KGC tasks. Examples in-\nclude the Graph Neural Networks (Schlichtkrull\net al., 2018), Convolutional Neural Networks\n(Dettmers et al., 2018), and Transformer (Chen\net al., 2021). Most of these aggregate local struc-\nture context into node embeddings and achieve\nmuch improved performance. However, they are\nstill limited by the imbalanced distribution of\nknowledge graph structure with insufficient knowl-\nedge about the long-tail entities. Meta-learning\n(Xiong et al., 2018; Chen et al., 2019) and logical\nrules (Sadeghian et al., 2019) can mitigate the long-\ntail problem in KG. The main difference between\nthem and our work is that they handle long-tail\nentities by extracting and summarizing common\nstructural patterns or rules from the limited infor-\nmation in KG, while KICGPT combines a vast\nexternal knowledge base inside the LLM with the\nstructural information in KGs, which can help alle-\nviate information scarcity.\nText-based KGC In light of the success of Natu-\nral Language Processing (NLP), text-based knowl-\nedge graph completion is gaining more attention.\nAs another mode of knowledge different from the\nstructured KG, text can provide rich semantic in-\nformation. DKRL (Xie et al., 2016) first introduces\ntextual descriptions into entity embeddings pro-\nduced by a convolutional neural network. Subse-\nquent works (such as KG-BERT (Yao et al., 2019),\nKEPLER (Wang et al., 2021b), and Pretrain-KGE\n8668\n(Zhang et al., 2020b)) use a pre-trained language\nmodel (PLM) to encode the text descriptions. More\nrecently, LMKE (Wang et al., 2022a) proposed a\ncontrastive learning framework that uses the PLM\nto obtain entity and relation embeddings in the\nsame space as word tokens, and demonstrated its\neffectiveness on long-tail problem These methods\ngenerally rely on the language models to process\ntext descriptions and require finetuning for different\nknowledge graphs. The proposed KICGPT, which\nuses LLM directly, is more efficient because it is\ntraining-free and requires no finetuning.\nLLMs for KGs Some recent works also explore\nthe ability of LLM on KG tasks. StructGPT (Jiang\net al., 2023) proposed a general framework for im-\nproving zero-shot reasoning ability of LLMs on\nstructured data. It leverages the LLM to perform\nreasoning on the KG Question Answering (KGQA)\ntask with the help of auxiliary interfaces that fetch\nthe needed information from KG. Though both\nStructGPT and our work utilize KG and LLM,\nStructGPT aims to help the LLM to handle struc-\ntured data and explore KGQA tasks, while ours uti-\nlizes the knowledge base inside the LLM to handle\nKGC’s long-tail problem. Moreover, StructGPT\nperforms multi-step reasoning directly based on the\nKG structure, while the proposed KICGPT utilizes\nthe KG information in a different way. First, we\nuse the whole KG to generate preliminary results\nbefore using the LLM. Moreover, we incorporate\na portion of the KG triples into ICL demonstra-\ntions to help the LLM conduct reasoning. Zhu\net al. (2023) directly evaluated the performance of\na LLM on KG reasoning. Our work design an ICL\nstrategy to guide the LLM to perform reasoning.\nMoreover, their experiments on the link prediction\ntask only involve 25 sampled instances from the\nFB15k-237 dataset (which has 20,466 test triples).\nInstead, we re-run their setting on ChatGPT and\nreport the new results as a baseline in our experi-\nments.\nICL for LLMs A typical way of approach-\ning LLM is through in-context learning (Brown\net al., 2020), by providing explicit instructions and\ndemonstrations to guide the model’s behavior. This\napproach has been effective in various language un-\nderstanding and generation tasks (Rae et al., 2021;\nWei et al., 2022). In-context learning exposes the\nmodel to specific contextual information, allowing\nit to grasp and reproduce necessary patterns and\nstructures for precise generation (Ouyang et al.,\n2022). However, the success of in-context learning\ndepends heavily on the quality of the prompt, and\ncrafting suitable prompts can be delicate (Wang\net al., 2022b). While there has been work explor-\ning ICL on different tasks (Dong et al., 2022), to\nthe best of our knowledge, no such work has been\ndone for KGC. Unlike existing works, the proposed\nICL strategy, Knowledge Prompt, considers the\ncharacteristics of KGC tasks and demonstrates its\neffectiveness on KGC tasks.\n3 Methodology\nIn this section, we introduce the proposed KICGPT\nmodel. The complete algorithm is in appendix A.1.\n3.1 Problem Setting\nA knowledge graph can be represented as a set\nof triples G = {(h, r, t)}, where E and R denote\nthe set of entities and relations in G, respectively,\nh ∈ E is a head entity, t ∈ E is a tail entity,\nand r ∈R represents the relation between them.\nLink prediction is an important task in KGC. Given\nan incomplete triple (h, r,?) or (?, r, t) as query,\nlink prediction aims to predict the missing entity\n(denoted as ?). A link prediction model usually\nneeds to score all plausible entities as missing entity\nand then rank all entities in descending order. For\nsimplicity of presentation, we focus on queries with\nmissing tail entities (i.e., (h, r,?)). Queries with\nmissing head entities (i.e., (?, r, t)) can be handled\nanalogously.\n3.2 Overview\nFigure 1 illustrates the KICGPT framework. There\nare two components: a triple-based KGC re-\ntriever and a LLM. For each query triple (h, r,?),\nthe retriever first generates the score of (h, r, e)\nfor each entity e ∈ E. The ranking (in de-\nscending score) of all the entities is denoted\nRretriever = [ e1st, e2nd, e3rd, . . . , e|E|th]. The\nLLM then performs a re-ranking of the top- m\nentities based on its knowledge and demonstra-\ntions in the proposed Knowledge Prompt. The\nre-ranking RLLM = [e′\n1st, e′\n2nd, e′\n3rd, . . . , e′\nmth] is\na permutation of [e1st, e2nd, e3rd, . . . , emth]. By\nreplacing the m leading entities in Rretriever\nby RLLM , the KICGPT outputs RKICGPT =\n[e′\n1st, e′\n2nd, e′\n3rd, . . . , e′\nmth, em+1th, . . . , e|E|th].\n8669\nFigure 1: An illustration of the KICGPT framework.\n3.3 Knowledge Prompt\nIn this section, we introduce Knowledge Prompt,\nan in-context learning strategy specially designed\nfor KGC tasks. By encoding part of the KG into\ndemonstrations, Knowledge Prompt boosts the per-\nformance of LLM on link prediction.\n3.3.1 Demonstration Pool\nFor each query (h, r,?), we construct two pools of\ntriples, Da and Ds, from the KG as demonstrations.\nAnalogy Pool The analogy pool Da contains\ntriples that help the LLM to better understand the\nsemantics of the query by analogy. Let Gtrain\nand Gvalid be the sets of KG triples for training\nand validation, respectively. Da = {(e′, r, e′′) ∈\nGtrain ∪Gvalid|e′, e′′ ∈E}includes triples with\nthe same relation as the query (h, r,?).\nSupplement Pool The supplement pool Ds con-\ntains triples which provide supplementary infor-\nmation about the query’s head entity h. Specifi-\ncally, Ds includes all triples with h as the head\nor tail entity in the training and validation parts:\n{(h, r′, e′) ∈Gtrain ∪Gvalid|r′ ∈R, e′ ∈E}∪\n{(e′, r′, h) ∈Gtrain ∪Gvalid|r′∈R, e′∈E}.\n3.3.2 Demonstration Ordering\nAs the demonstration order affects performance\n(Ye et al., 2023), we propose different ordering\nstrategies for the analogy and supplement pools.\nFor the analogy pool Da, all its triples are simi-\nlar to the query because they share the same rela-\ntion. As diversity of demonstrations is important\nso that the LLM can learn from various analogy\ndemonstrations, we propose an ordering strategy\nthat promotes diversity. Specifically, first we set a\nzero counter for each entity. A triple from Da is\nrandomly selected as demonstration, and the coun-\nters associated with its entities are increased by 1.\nWe iteratively choose as demonstration the triple\nwhose entities’ associated counters have the small-\nest sum (tie is resolved randomly). The associated\ncounters are then increased by 1. This is repeated\nuntil all triples in Da are used, and the resultant\ndemonstration list obtained is denoted La.\nFor the supplement pool Ds, as it serves to pro-\nvide supplementary information on the query’s\nhead entity h, we prefer related demonstrations\nto the query. Specifically, we rank all triples in\nDs according to their BM25 scores (Robertson and\nWalker, 1994). This score is used to evaluate the\ncorrelation between texts in each demonstration\nand query. The resultant ranking list, denoted Ls,\ncontains all triples in Ds.\nQueries with a specific relation or head entity use\na shared analogy or supplement pool. To prevent\ndouble counting, demonstration pools are created\nand ordered in data pre-processing. During infer-\nence, the corresponding pool is used based on the\nquery.\n3.3.3 Prompt Engineering\nPrompt engineering is an important in ICL (Zhao\net al., 2021). In this section, we show the whole\ninteraction workflow with the LLM, and also con-\nsiderations of the prompt design.\nThe demonstrations and query take the form\nof triples. However, LLMs require natural lan-\nguage input. To remedy this gap, KICGPT uses a\nunified prompt template to convert the query and\n8670\ndemonstrations to plain text with the same format.\nWe then perform multi-round interactions with the\nLLM, so as to guide it to perform re-ranking, where\nthese texts and some instructions are organized as\nprompt inputs. Figure 2 illustrates the workflow\nof a multi-round interaction with the LLM. For\neach link prediction query (h, r,?), KICGPT cre-\nates an independent conversational prompt. The\nwhole multi-round interaction process includes\nfour stages: responsibility description, question\nand demonstration description, multiple demon-\nstrations, and final query.\nResponsibility description is illustrated in the\nfirst part of Figure 2. We tell the LLM that its role\nis an assistant to ranking candidate answers for a\nquestion based on plausibility. We then check its\nfeedback to ensure that it knows the task.\nThe question and demonstration description\nstage is shown in the second part of Figure 2. We\ninput the question (corresponding to the query’s\ntext) and tell it that two types of examples are to be\nprovided and should be treated differently, one for\nanalogy and one containing supplementary infor-\nmation.\nNext, illustrated in the third part of Figure 2, we\nprovide the LLM with a batch of demonstrations\n(La and Ls) from the analogy pool and supple-\nment pool. To include more KG information and\ndemonstrations, we repeat this step as many times\nas possible subject to the input token length limit.\nFinally, we restate the query text, and ask the\nLLM to re-rank the top-m candidate entities. The\nfeedback is parsed into an ordered list RLLM ,\nwhich then replaces the top-m entities in Rretriever\nas the final answer.\n3.4 Text Self-Alignment\nIn this section, we propose Text Self-alignment for\nKG text cleaning. It transforms the raw text in\nthe KG to more understandable descriptions by the\nLLM.\nRaw and obscure text descriptions generally\nexist in the KG. For example, a raw rela-\ntion description in the FB15k-237 dataset is\n“/tv/tv_program/country_of_origin”. Most existing\nmethods (Yao et al., 2019; Zhang et al., 2020b;\nWang et al., 2021b, 2022a) convert it to a cleaner\ndescription by removing the symbols (e.g., “tv tv\nprogram country of origin”). By default, KICGPT\nuses “of” to organize such hierarchical relation,\nleading to “country of origin of tv program of tv”.\nFigure 2: Illustration of a multi-round interaction with\nthe LLM. Stage 3 is repeated many times to provide\nmore demonstrations.\nHowever, this description may still be hard for the\nLLM to comprehend, and may lead to incorrect\nresponses.\nTo address the above issue, we use ICL and let\nthe LLM to summarize natural text descriptions\nfrom the given demonstrations that use raw descrip-\ntions. Specifically, for a relation r, we use the\nanalogy pool Da for r (Section 3.3.1) as demon-\nstrations and order them to get the ordered list La\n(Section 3.3.2). These demonstrations are fed as\na prompt to the LLM (Appendix A.2), which is\nasked to summarize the semantics of the relation\ninto a sentence (the self-aligned text description\nof r). For the above example with raw relation\ntext “/tv/tv_program/country_of_origin”, the LLM\ngenerates the much clearer and natural self-aligned\ntext description “[T] is the country where the TV\n8671\nDataset FB15k-237 WN18RR\nMetric MRR Hits@1 Hits@3 Hits@10 MRR Hits@1 Hits@3 Hits@10\nTriple-based methods\nRESCAL(Nickel et al., 2011)♣ 0.356 0.266 0.390 0.535 0.467 0.439 0.478 0.516\nTransE (Bordes et al., 2013)♠ 0.279 0.198 0.376 0.441 0.243 0.043 0.441 0.532\nDistMult (Yang et al., 2014)♠ 0.241 0.155 0.263 0.419 0.430 0.390 0.440 0.490\nComplEx (Trouillon et al., 2016)♠ 0.247 0.158 0.275 0.428 0.440 0.410 0.460 0.510\nRotatE (Sun et al., 2019) 0.338 0.241 0.375 0.533 0.476 0.428 0.492 0.571\nTuckER (Balaževi´c et al., 2019) 0.358 0.266 0.394 0.544 0.470 0.443 0.482 0.526\nHAKE (Zhang et al., 2020a) 0.346 0.250 0.381 0.542 0.497 0.452 0.516 0.582\nCompGCN (Vashishth et al., 2019)0.355 0.264 0.390 0.535 0.479 0.443 0.494 0.546\nHittER(Chen et al., 2021) 0.344 0.246 0.380 0.535 0.496 0.449 0.514 0.586\nText-based methods\nPretrain-KGE (Zhang et al., 2020b)0.332 - - 0.529 0.235 - - 0.557\nKG-BERT (Yao et al., 2019)♠ - - - 0.420 0.216 0.041 0.302 0.524\nStAR (Wang et al., 2021a)♠ 0.263 0.171 0.287 0.452 0.364 0.222 0.436 0.647\nMEM-KGC (w/o EP) (Choi et al., 2021)0.339 0.249 0.372 0.522 0.533 0.473 0.570 0.636\nMEM-KGC (w/ EP) (Choi et al., 2021)0.346 0.253 0.381 0.531 0.557 0.475 0.604 0.704\nLLM-based methods\nChatGPTzero−shot(Zhu et al., 2023)⋄ - 0.237 - - - 0.190 - -\nChatGPTone−shot(Zhu et al., 2023)⋄ - 0.267 - - - 0.212 - -\nKICGPT 0.412 0.327 0.448 0.554 0.549 0.474 0.585 0.641\nKICGPTtsa 0.410 0.321 0.430 0.581 0.564 0.478 0.612 0.677\nTable 1: Comparison between the proposed methods and baseline methods. The best result in terms of each metric is\nshown in bold and the second best one is underlined. ♠indicates that results are copied from (Wang et al., 2022a),\n♣implies that results are copied from (Chen et al., 2021), ⋄means that the results are running on the entire data\naccording to the settings in (Zhu et al., 2023), and other results are taken from their original papers. EP denotes the\nentity prediction task in the MEM-KGC model.\nprogram [H] originated from. ”, where [T] and [H]\nare placeholders for the tail and head entities, re-\nspectively.\nData pre-processing uses text self-alignment to\ncreate clear and aligned descriptions for each rela-\ntion, which can be directly used as relation text in\nKICGPT for link prediction. This variant is called\nKICGPTtsa. Since the texts are derived from the\nLLM, they conform to its presentation conventions,\nmaking them easier to understand and improving\nperformance.\n4 Experiment\nIn this section, we empirically evaluate KICGPT.\n4.1 Setup\nDatasets. We evaluate the proposed methods\non the FB15k-237 (Toutanova et al., 2015) and\nWN18RR (Dettmers et al., 2018), which are widely-\nused benchmark for link prediction. FB15k-237 is\na subset of the freebase (Bollacker et al., 2008)\nknowledge graph, which includes commonsense\nknowledge about topics such as movies, sports,\nawards, and traveling. WN18RR is a subset of\nWordNet (Miller, 1995), which contains knowledge\nabout English morphology. Both the FB15k-237\nand WN18RR datasets remove redundant inverse\nrelations in case of information leakage. Com-\npared with FB15k-237, the knowledge graph of\nWN18RR is much sparser. The statistics of the two\ndatasets are shown in Table 2.\nDataset # Ent # Rel # train # valid # test\nWN18RR 40,943 11 86,835 3,034 3,134\nFB15k-237 14,541 237 272,115 17,535 20,466\nTable 2: Statistics of the benchmark datasets.\nBaselines. We compare the proposed KICGPT\nwith a number of triple-based, text-based and LLM-\nbased baselines. The triple-based baselines include\nRESCAL (Nickel et al., 2011), TransE (Bordes\net al., 2013), DistMult (Yang et al., 2014), Com-\nplEx (Trouillon et al., 2016), RotatE (Sun et al.,\n2019), TuckER (Balaževi ´c et al., 2019), HAKE\n(Zhang et al., 2020a), CompGCN (Vashishth et al.,\n2019), and HittER (Chen et al., 2021). The text-\nbased baselines 1 include Pretrain-KGE (Zhang\net al., 2020b), KG-BERT (Yao et al., 2019), StAR\n(Wang et al., 2021a), and MEM-KGC (Choi et al.,\n1We did not compare our method with Wang et al. (2022a)\ndue to a data leakage issue on the entity degrees.\n8672\n2021). The LLM-based baselines are based on\nChatGPT. Zhu et al. (2023) reported zero-shot and\none-shot link prediction results on 25 instances\nsampled from FB15k-237. We run their codes on\nthe whole dataset and report the updated results as\nChatGPTzero−shot and ChatGPTone−shot.\nImplementation details. Though the proposed\nmethod can be used with various retrievers and\nLLMs, we prefer lightweight models for efficiency\nconsiderations. Specifically, our KICGPT imple-\nmentation uses RotatE (Sun et al., 2019) as the\nretriever and ChatGPT (gpt-3.5-turbo) as the LLM.\nHyper-parameters for RotatE are set as in (Sun\net al., 2019). The value of m is selected from\n{10, 20, 30, 40, 50}, and the batch size from {4,\n8, 16, 32} on a small random subset with 200 in-\nstances from the validation set. For the ChatGPT\nAPI, we set temperature, presence_penalty, and\nfrequency_penalty to 0, and top_p to 1 to avoid\nrandomness. The detailed prompts are shown in\nAppendix A.2.\nMetrics. Link prediction outputs a ranked list of\nall KG entities. We report Mean Reciprocal Rank\n(MRR), and Hits@1, 3, 10 of the ranked list under\nthe “filtered” setting. The “filtered” setting (Bordes\net al., 2013) is a common practice that filters out\nvalid entities other than ground truth target entities\nfrom the ranked list.\n4.2 Experimental Results\nResults are shown in Table 1. 2 As can be seen,\nthe proposed KICGPT and KICGPT tsa achieve\nstate-of-the-art performance on both FB15k-237\nand WN18RR in most metrics.\nSpecifically, on the Fb15k-237 dataset, both\nvariants of KICGPT surpass all the baselines in\nterms of all the evaluation metrics. The text self-\nalignment method helps improve the Hits@10 per-\nformance but degrades the MRR and Hits@{1,3}\nperformance. This may be because these demon-\nstrations and aligned text are mutually supportive,\nwhich enhances the confidence of LLM to under-\nstand correct semantics, but for relations that LLM\ndoes not understand well, the aligned texts may not\nconvey the true semantics.\nOn the Fb15k-237 dataset, both KICGPT and\nKICGPTtsa surpass all the baselines in terms of\nall evaluation metrics. Text self-alignment helps\n2We only report Hits@1 for the LLM-based baselines be-\ncause the LLM only outputs an answer but not a list.\nimprove the Hits@10 performance but degrades\nthe MRR and Hits@{1,3} performance. This may\nbe because the text self-alignment texts are derived\nfrom the demonstrations, which are also used in\nthe LLM inference process along with the text self-\nalignment texts. While the mutually supportive text\nself-alignment texts and demonstrations enhance\nthe LLM’s understanding of correct semantics, they\nalso make incorrect interpretations of relational\nsemantics more persistent.\nWN18RR FB15k-237\n_also_see /tv/tv_program/country_of_origin\n_hypernym /location/location/partially_contains\n_has_part /common/topic/webpage.\n/common/webpage/category\nTable 3: Example relations from the WN18RR and\nFB15k-237 datasets.\nOn the WN18RR dataset, KICGPTtsa achieves\nstate-of-the-art performance on all metrics except\nHits@10. Unlike the FB15k-237 dataset, text\nself-alignment improves all metrics. This may\nbe partly because the higher average number of\ntriples per relation in WN18RR (8454.8) com-\npared to FB15k-237 (1308.5), which facilitates\nthe generation of more precise text descriptions\ndue to the increased availability of information for\neach relation. Besides, as illustrated in Table 3,\nWN18RR relations have more concise and direct\nsemantics than those in FB15k-237, making text\nself-alignment for WN18RR easier than for FB15k-\n237. These differences cause the LLM to sum-\nmarize more concise and precise descriptions for\nrelations from WN18RR, which boosts the perfor-\nmance with higher-quality aligned text.\nThe proposed methods show better performance\nover the triple-based and text-based baselines. This\ndemonstrates the usefulness of the knowledge base\ninside the LLM. Besides, compared with the LLM-\nbased baselines (i.e., ChatGPT baselines3), the pro-\nposed methods significantly outperform on both\ndatasets, which shows the superiority of integrat-\ning KG with LLMs for KGC. The performance\n3The results we report for ChatGPT baselines are worse\nthan those reported in (Zhu et al., 2023). This is because they\nexperimented on only 25 triples on the FB15k-237 dataset\nwhile we test on the whole 20,466 testing triples. Besides,\nthey manually restated descriptions in KG as clean and nat-\nural expressions for the 25 triple, but doing this for all the\ntesting triples is labor-intensive and time-costly. In our exper-\niment, we use the text generated by text self-alignment as a\nsubstitution.\n8673\nimprovement mainly comes from the injection of\ninformation contained in the knowledge graph.\nThe demonstration ordering process takes 50.13\nand 30.29 minutes for the FB15k-237 and\nWN18RR datasets, respectively. In terms of train-\ning efficiency, KICGPT takes only 28.36 and 20.63\nminutes to train on FB15k-237 and WN18RR, re-\nspectively, making it more efficient compared to\nexisting text-based methods that can take hours to\ntrain. This is because KICGPT does not require\nfine-tuning of the LLM.\n4.3 Ablation Study\nIn this experiment, using the FB15k-237 dataset,\nwe perform ablation studies to demonstrate useful-\nness of each component in KICGPT. Table 4 shows\nthe results.\nIn the first ablation study, we shuffle the top-m\nentities in Rretriever before feeding into the LLM.\nAs can be seen, this causes a slight performance\ndegradation. This shows that the order offered by\nthe retriever is important and can reduce the dif-\nficulty of re-ranking. Recently, Sun et al. (2023)\nalso noted the significance of the initial order in\nsearch re-ranking.\nIn the second study, we randomize the demon-\nstration orders from the analogy and supplement\npools. Again, the performance degrades, demon-\nstrating effectiveness of the proposed demonstra-\ntion ordering in Section 3.3.2.\nIn the third study, we retrieve random triples\nfrom the KG as demonstrations. The resultant per-\nformance degradation shows usefulness of the con-\nstruction of demonstration pools in Section 3.3.1.\nIn the fourth study, we exclude all demonstra-\ntions, while still providing the top-m candidates to\nconstrain the ChatGPT output. As can be seen, a\nsignificant performance gap with the full model is\nobserved, demonstrating the necessity of the pro-\nposed ICL strategy. Note also that Hit@3 without\nICL is superior to Hit@3 with random demonstra-\ntions. This may be attributed to the vast number\nof triples in the KG, rendering the random demon-\nstrations to have insignificant semantic relevance\nto the query. The use of irrelevant triples as demon-\nstrations to LLM can introduce noise, potentially\nresulting in misleading outcomes.\nIn the last ablation study (Appendix A.2), we\nuse a prompt without the prompt engineering com-\nponent (Section 3.3.3). This variant still uses the\nproposed demonstration pools (Section 3.3.1) and\nordering (Section 3.3.2). The observed degraded\nperformance implies that the specially designed\nprompts can make use of the properties of the KGC\ntask to boost performance.\nMRRHits@1Hits@3Hits@10\nKICGPT 0.412 0.327 0.448 0.554\nshuffle candidates 0.401 0.312 0.433 0.521\nw/o demonstration ordering0.368 0.283 0.417 0.497\nrandom demonstrations0.349 0.271 0.387 0.481\nw/o ICL 0.342 0.241 0.403 0.481\nw/o prompt engineering0.401 0.307 0.432 0.548\nTable 4: Ablation results on FB15k-237 (averaged over\n4 random runs).\n4.4 Analysis on Long-Tail Entities\nTo show the effectiveness of KICGPT and\nKICGPTtsa to handle long-tail entities, we follow\n(Wang et al., 2022a) and group entities by their\nlogarithm degrees in the knowledge graph. Entities\nwith lower degrees (and hence more likely to be\nlong-tail entities) are assigned to groups with lower\nindexes. A triple (h, r, t) is considered relevant to\ngroup d if h or t belongs to d.\nFigure 3 displays the Hits@1 and Hits@10 per-\nformance averages of various models on the FB15k-\n237 dataset, categorized by the logarithm of entity\ndegrees. Text-based methods demonstrate slightly\nbetter performance than triple-based methods on\nlong-tail entries. However, this improvement is not\nsignificant since it only applies to a small portion\nof long-tail entities (specifically, groups 0, 1, 2).\nCompared with these baselines, the proposed mod-\nels achieve performance improvement on almost all\ngroups and perform significantly better on long-tail\nentities, which confirms the benefits of combining\nthe LLM with KG.\nFigure 3: Average performance of different models (in\nterms of Hits@1 and Hits@10) grouped by the loga-\nrithm of entity degrees on the FB15k-237 dataset.\n5 Conclusion\nIn this paper, we propose KICGPT, an effective\nframework to integrate LLM and traditional KGC\nmethods for link prediction, and a new ICL strategy\n8674\ncalled Knowledge Prompt. KICGPT utilizes the\nLLM as an extra knowledge base. Compared with\ntext-based methods, KICGPT utilizes the training-\nfree property of LLM to significantly reduce train-\ning overhead, and does not require finetuning for\ndifferent KGs. Experimental results show that\nKICGPT achieves state-of-the-art performance on\nthe link prediction benchmarks and is effective in\nthe handling of long-tail entities.\nAcknowledgements\nThis work is supported by NSFC general grant\n62076118 and Shenzhen fundamental research pro-\ngram JCYJ20210324105000003.\nLimitations\nThe performance improvement of the proposed ap-\nproach in KGC is largely due to the large and ex-\ntensive knowledge base within the LLM. However,\nfor some KGs (such as personal preference data for\nusers in an E-commerce platform), the LLM may\nnot contain enough relevant knowledge. Therefore,\nthe proposed method works mostly for common-\nsense KGs. Besides, because of the limited token\nlength in the LLM, we cannot inject all relevant\nfacts from the KG as prompts.\nEthics Statement\nThere is no ethical problem in our study.\nReferences\nIvana Balaževi ´c, Carl Allen, and Timothy M\nHospedales. 2019. Tucker: Tensor factorization\nfor knowledge graph completion. arXiv preprint\narXiv:1901.09590.\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim\nSturge, and Jamie Taylor. 2008. Freebase: a collabo-\nratively created graph database for structuring human\nknowledge. In Proceedings of the 2008 ACM SIG-\nMOD international conference on Management of\ndata, pages 1247–1250.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. Advances in neural information pro-\ncessing systems, 26.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nMingyang Chen, Wen Zhang, Wei Zhang, Qiang Chen,\nand Huajun Chen. 2019. Meta relational learning for\nfew-shot link prediction in knowledge graphs. arXiv\npreprint arXiv:1909.01515.\nSanxing Chen, Xiaodong Liu, Jianfeng Gao, Jian Jiao,\nRuofei Zhang, and Yangfeng Ji. 2021. Hitter: Hierar-\nchical transformers for knowledge graph embeddings.\nIn Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nAssociation for Computational Linguistics.\nBonggeun Choi, Daesik Jang, and Youngjoong Ko.\n2021. Mem-kgc: Masked entity model for knowl-\nedge graph completion with pre-trained language\nmodel. IEEE Access, 9:132025–132032.\nTim Dettmers, Pasquale Minervini, Pontus Stenetorp,\nand Sebastian Riedel. 2018. Convolutional 2d knowl-\nedge graph embeddings. In Proceedings of the AAAI\nconference on artificial intelligence, volume 32.\nQingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiy-\nong Wu, Baobao Chang, Xu Sun, Jingjing Xu, and\nZhifang Sui. 2022. A survey for in-context learning.\narXiv preprint arXiv:2301.00234.\nJinhao Jiang, Kun Zhou, Zican Dong, Keming Ye,\nWayne Xin Zhao, and Ji-Rong Wen. 2023. Struct-\ngpt: A general framework for large language model\nto reason over structured data. arXiv preprint\narXiv:2305.09645.\nGeorge A Miller. 1995. Wordnet: a lexical database for\nenglish. Communications of the ACM, 38(11):39–41.\nMaximilian Nickel, V olker Tresp, Hans-Peter Kriegel,\net al. 2011. A three-way model for collective learning\non multi-relational data. In Icml, volume 11, pages\n3104482–3104584.\nOpenAI. 2023. Gpt-4 technical report. CoRR.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\n8675\nStephen E Robertson and Steve Walker. 1994. Some\nsimple effective approximations to the 2-poisson\nmodel for probabilistic weighted retrieval. In SI-\nGIR’94: Proceedings of the Seventeenth Annual In-\nternational ACM-SIGIR Conference on Research and\nDevelopment in Information Retrieval, organised by\nDublin City University, pages 232–241. Springer.\nAli Sadeghian, Mohammadreza Armandpour, Patrick\nDing, and Daisy Zhe Wang. 2019. Drum: End-to-\nend differentiable rule mining on knowledge graphs.\nAdvances in Neural Information Processing Systems,\n32.\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem,\nRianne Van Den Berg, Ivan Titov, and Max Welling.\n2018. Modeling relational data with graph convolu-\ntional networks. In The Semantic Web: 15th Inter-\nnational Conference, ESWC 2018, Heraklion, Crete,\nGreece, June 3–7, 2018, Proceedings 15, pages 593–\n607. Springer.\nWeiwei Sun, Lingyong Yan, Xinyu Ma, Pengjie\nRen, Dawei Yin, and Zhaochun Ren. 2023. Is\nchatgpt good at search? investigating large lan-\nguage models as re-ranking agent. arXiv preprint\narXiv:2304.09542.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian\nTang. 2019. Rotate: Knowledge graph embedding by\nrelational rotation in complex space. ICLR.\nYi Tay, Jason Wei, Hyung Won Chung, Vinh Q\nTran, David R So, Siamak Shakeri, Xavier Gar-\ncia, Huaixiu Steven Zheng, Jinfeng Rao, Aakanksha\nChowdhery, et al. 2022. Transcending scaling\nlaws with 0.1% extra compute. arXiv preprint\narXiv:2210.11399.\nKristina Toutanova, Danqi Chen, Patrick Pantel, Hoi-\nfung Poon, Pallavi Choudhury, and Michael Gamon.\n2015. Representing text for joint embedding of text\nand knowledge bases. In Proceedings of the 2015\nconference on empirical methods in natural language\nprocessing, pages 1499–1509.\nThéo Trouillon, Johannes Welbl, Sebastian Riedel, Éric\nGaussier, and Guillaume Bouchard. 2016. Complex\nembeddings for simple link prediction. In Interna-\ntional conference on machine learning, pages 2071–\n2080. PMLR.\nShikhar Vashishth, Soumya Sanyal, Vikram Nitin, and\nPartha Talukdar. 2019. Composition-based multi-\nrelational graph convolutional networks. ICLR.\nBo Wang, Tao Shen, Guodong Long, Tianyi Zhou, Ying\nWang, and Yi Chang. 2021a. Structure-augmented\ntext representation learning for efficient knowledge\ngraph completion. In Proceedings of the Web Confer-\nence 2021, pages 1737–1748.\nQuan Wang, Zhendong Mao, Bin Wang, and Li Guo.\n2017. Knowledge graph embedding: A survey of\napproaches and applications. IEEE Transactions\non Knowledge and Data Engineering, 29(12):2724–\n2743.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021b.\nKepler: A unified model for knowledge embedding\nand pre-trained language representation. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:176–194.\nXintao Wang, Qianyu He, Jiaqing Liang, and Yanghua\nXiao. 2022a. Language models as knowledge embed-\ndings. IJCAI.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc\nLe, Ed Chi, and Denny Zhou. 2022b. Rationale-\naugmented ensembles in language models. arXiv\npreprint arXiv:2207.00747.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, Ed H.\nChi, Tatsunori Hashimoto, Oriol Vinyals, Percy\nLiang, Jeff Dean, and William Fedus. 2022. Emer-\ngent abilities of large language models. Transactions\non Machine Learning Research . Survey Certifica-\ntion.\nRuobing Xie, Zhiyuan Liu, Jia Jia, Huanbo Luan, and\nMaosong Sun. 2016. Representation learning of\nknowledge graphs with entity descriptions. In Pro-\nceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 30.\nWenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo,\nand William Yang Wang. 2018. One-shot rela-\ntional learning for knowledge graphs. arXiv preprint\narXiv:1808.09040.\nBishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao,\nand Li Deng. 2014. Embedding entities and relations\nfor learning and inference in knowledge bases. arXiv\npreprint arXiv:1412.6575.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019. Kg-\nbert: Bert for knowledge graph completion. arXiv\npreprint arXiv:1909.03193.\nJiacheng Ye, Zhiyong Wu, Jiangtao Feng, Tao Yu,\nand Lingpeng Kong. 2023. Compositional ex-\nemplars for in-context learning. arXiv preprint\narXiv:2302.05698.\nZhanqiu Zhang, Jianyu Cai, Yongdong Zhang, and Jie\nWang. 2020a. Learning hierarchy-aware knowledge\ngraph embeddings for link prediction. In Proceed-\nings of the AAAI conference on artificial intelligence,\nvolume 34, pages 3065–3072.\nZhiyuan Zhang, Xiaoqian Liu, Yi Zhang, Qi Su, Xu Sun,\nand Bin He. 2020b. Pretrain-kge: learning knowl-\nedge representation from pretrained language models.\nIn Findings of the Association for Computational Lin-\nguistics: EMNLP 2020, pages 259–266.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697–12706. PMLR.\n8676\nYuqi Zhu, Xiaohan Wang, Jing Chen, Shuofei Qiao,\nYixin Ou, Yunzhi Yao, Shumin Deng, Huajun Chen,\nand Ningyu Zhang. 2023. Llms for knowledge\ngraph construction and reasoning: Recent capa-\nbilities and future opportunities. arXiv preprint\narXiv:2305.13168.\n8677\nA Appendix\nA.1 KICGPT Algorithm\nWe provide the algorithm of KICGPT here.\nAlgorithm 1:Demonstration pools from KG\nInput: KG G = (h, r, t), E, and R denote the entity set and the relation set of G, Gtrain and\nGvalid are sets of triples in G for training and validation\nlink prediction query q = (h, r,?)\nOutput: analogy pool Da\nsupplement pool Ds\n1 Da = {(e′, r, e′′) ∈Gtrain ∪Gvalid|e′, e′′∈E}\n2 Ds = {(h, r′, e′) ∈Gtrain ∪Gvalid|r′∈R, e′∈E}∪{(e′, r′, h) ∈Gtrain ∪Gvalid|r′∈R, e′∈\nE}\nAlgorithm 2:Diversity-based ordering for analogy pool\nInput: Analogy demonstration pool: Da, entity set E of KG\nOutput: ordered list La\n1 foreach element e ∈E do\n2 initialize a zero counter ce = 0for e\n3 Randomly select a triple (h′, r′, t′) from Da, append it to La and increase the counter ch′ and ct′ by\n1\n4 repeat\n5 Find a triple a that has the minimum sum of ce′ and ce′′ where e′and e′′are the head and tail\nentity in triple a (If there are multiple triples with minimum sum of entity counters, randomly\nselect one of them as a)\n6 Append a to La and remove a from Da\n7 increase the counter of entities ce′, ce′′ in triple a by 1\n8 until Da is empty\n8678\nAlgorithm 3:BM25 score-based ordering for supplement pool\nInput: Supplement demonstration pool: Ds\nlink prediction query q\nOutput: ordered list Ls\n1 foreach triple s in Ds do\ncalculate the BM25 score between text descriptions of s and q\n2 Ls ←Ordering triples in Ds according to the BM25 score in descending order\nAlgorithm 4:Algorithm for KICGPT\n/* For presentation simplicity, we represent the algorithm on missing tail queries. missing\nhead queries are handled in a similar way. */\nInput: KG G = (h, r, t), E, and R denote the entity set and the relation set of G, Gtrain and\nGvalid are sets of triples in G for training and validation\nlink prediction query q = (h, r,?)\nOutput: Ranked list RKICGPT contains all e ∈E\n1 Ranked list Rretriever contains all e ∈E ←Handling q by triple-based KGC retriever\nRretriever = [e1st, e2nd, e3rd, . . . , e|E|th]\n2 Analogy Pool Da, Supplement Pool Ds ←Fetch Demonstration Pools from G based on\nq = (h, r,?)\n3 La ←Ordering triples in Da based on diversity\n4 Ls ←Ordering triples in Ds based on BM25 score\n5 Use a unified prompt template to convert the query q and triples in La and Ls to text\n6 With some instructions, demonstrations arranged by order in La and Ls are organized to ICL\nprompts\n7 Given the ICL prompts and query, LLM is asked to perform re-ranking for first m entities in\nRretriever\nRLLM ←LLM (first m entities in Rretriever , ICL prompts, text of q)\nRLLM = [e′\n1st, e′\n2nd, e′\n3rd, . . . , e′\nmth], is permutation of first m entities in Rretriever\n8 RKICGPT = [e′\n1st, e′\n2nd, e′\n3rd, . . . , e′\nmth, em+1th, . . . , e|E|th], where first m entities is RLLM and\nremaining come from corresponding location of Rretriever\n8679\nA.2 Prompt Formatting\nWe list the prompts used in this paper as follows.\nDatasets Prompt Template ChatGPT\nFB15k-237 You are a good assistant to reading, understanding and summarizing.\nEcuador is the partially_contains of location of location of\nPacific Ocean\nAppalachian Mountains is the partially_contains of location of\nlocation of Massachusetts\nMoldavia is the partially_contains of location of location of\nMoldova\nEcuador is the partially_contains of location of location of South\nAmerica\nAdirondack Mountains is the partially_contains of location of\nlocation of Warren County\nIn above examples, What do you think \"partially_contains of\nlocation of location of \" mean? Summarize and descript its\nmeaning using the format: \"If the example shows something A is\npartially_contains of location of location of of something B, it\nmeans A is [mask] of B.\" Fill the mask and the statement should\nbe as short as possible.\nIf the example shows some-\nthing A is partially_contains of\nlocation of location of of some-\nthing B, it means A is located\npartially within the boundaries\nof B.\nWN18RR You are a good assistant to reading, understanding and summarizing.\nred indian be member of domain usage of disparagement compass be\nmember of domain usage of archaism\npenicillin v potassium be member of domain usage of trade name\nnanna be member of domain usage of united kingdom of great britain\nand northern ireland\nnerves be member of domain usage of plural form\nzyloprim be member of domain usage of trademark\nIn above examples, What do you think \"be member of domain usage\nof\" mean? Summarize and descript its meaning using the format:\n\"If the example shows something A be member of domain usage of\nsomething B, it means A is [mask] of B.\" Fill the mask and the\nstatement should be as short as possible.\nIf the example shows some-\nthing A be member of domain\nusage of something B, it means\nA is a term or word that be-\nlongs to the category or do-\nmain of B’s usage.\nTable 5: Examples of prompts for text self-alignment.\n8680\nPrompt Formatting Prompt Template ChatGPT\nTrivial Prompt predict the tail entity [MASK] from the given (Mel\nBlanc,type_of_union of marriage of people of spouse_s of person\nof people of , [MASK]) by completing the sentence \"what is the\ntype_of_union of marriage of people of spouse_s of person of\npeople of Mel Blanc? The answer is \". The answer is Marriage, so\nthe [MASK] is Marriage. predict the tail entity [MASK] from the\ngiven (Screen Actors Guild Life Achievement Award,award_winner of\naward_honor of award of winners of award_category of award of ,\n[MASK]) by completing the sentence \"what is the award_winner of\naward_honor of award of winners of award_category of award of\nScreen Actors Guild Life Achievement Award? The answer is \". The\nanswer is Stan Laurel, so the [MASK] is Stan Laurel.\nThe list of candidate answers is [Marriage, Domestic partnership,\nCivil union, Official Website, Rang De Basanti, HBO, Male,\nTelevision, Judaism-GB, Crusades]. And the question is predict the\ntail entity [MASK] from the given (Stan Laurel,type_of_union of\nmarriage of people of spouse_s of person of people of , [MASK]) by\ncompleting the sentence \"what is the type_of_union of marriage of\npeople of spouse_s of person of people of Stan Laurel? The answer\nis \". Now, based on the previous examples and your own knowledge\nand thinking, sort the list to let the candidate answers which are\nmore possible to be the true answer to the question prior. Output\nthe sorted order of candidate answers using the format \"[most\npossible answer | second possible answer | ... | least possible\nanswer]\" and please start your response with \"The final order:\".\nDo not output anything except the final order. Note your output\nsorted order should contain all the candidates in the list but\nnot add new answers to it.\nThe final order: [Marriage |\nDomestic partnership | Civil\nunion | Official Website | HBO\n| Male | Television | Rang De\nBasanti | Judaism-GB | Cru-\nsades].\nTable 6: Trivial Prompts on the FB15k-237 dataset.\n8681\nSteps Prompt Template ChatGPT\nResponsibility Description You are a good assistant to perform link prediction and sorting.\nGiven a goal question and a list of candidate answers to this\nquestion. You need to order these candidate answers in the list\nto let candidate answers which are more possible to be the answer\nto the question prior. If you have known your responsibility,\nrespond \"Yes\". Otherwise, respond \"No\". Do not output anything\nexcept \"Yes\" and \"No\".\nYes.\nQuestion and Demonstration\nDescription\nThe goal question is: predict the tail entity [MASK] from the given\n(Stan Laurel,type_of_union of marriage of people of spouse_s of\nperson of people of, [MASK]) by completing the sentence \"what\nis the type_of_union of the marriage of people of spouse_s of\nthe person of people of Stan Laurel? The answer is \". To sort\nthe candidate answers, typically you would need to refer to some\nother examples that may be similar to or related to the question.\nPart of the given examples are similar to the goal question, you\nshould analogy them to understand the potential meaning of the goal\nquestion. Another part of the given facts contains supplementary\ninformation, keep capturing this extra information and mining\npotential relationships among them to help the sorting. Please\ncarefully read, realize, and think about these examples. Summarize\nthe way of thinking in these examples and memorize the information\nyou think maybe help your sorting task. During I give examples\nplease keep silent until I let you output.\nOkay, I understand. I will wait\nfor your examples and instruc-\ntions.\nMultiple Demonstrations Examples used to Analogy: \"predict the tail entity [MASK] from the\ngiven (Mel Blanc,type_of_union of marriage of people of spouse_s\nof person of people of , [MASK]) by completing the sentence \"what\nis the type_of_union of marriage of people of spouse_s of person\nof people of Mel Blanc? The answer is \". The answer is Marriage,\nso the [MASK] is Marriage.\"\nExamples give supplement information: \"predict the tail entity\n[MASK] from the given (Screen Actors Guild Life Achievement\nAward,award_winner of award_honor of award of winners of\naward_category of award of , [MASK]) by completing the sentence\n\"what is the award_winner of award_honor of award of winners of\naward_category of award of Screen Actors Guild Life Achievement\nAward? The answer is \". The answer is Stan Laurel, so the [MASK]\nis Stan Laurel. Keep thinking but not output.\nOkay, I will keep thinking and\nanalyzing the given examples\nto identify potential relation-\nships and patterns that can help\nwith the sorting task.\nFinal Query The list of candidate answers is [Marriage, Domestic partnership,\nCivil union, Official Website, Rang De Basanti, HBO, Male,\nTelevision, Judaism-GB, Crusades]. And the question is predict the\ntail entity [MASK] from the given (Stan Laurel,type_of_union of\nmarriage of people of spouse_s of person of people of , [MASK]) by\ncompleting the sentence \"what is the type_of_union of marriage of\npeople of spouse_s of person of people of Stan Laurel? The answer\nis \". Now, based on the previous examples and your own knowledge\nand thinking, sort the list to let the candidate answers which are\nmore possible to be the true answer to the question prior. Output\nthe sorted order of candidate answers using the format \"[most\npossible answer | second possible answer | ... | least possible\nanswer]\" and please start your response with \"The final order:\".\nDo not output anything except the final order. Note your output\nsorted order should contain all the candidates in the list but\nnot add new answers to it.\nThe final order: [Marriage |\nDomestic partnership | Civil\nunion | Official Website | HBO\n| Male | Television | Rang De\nBasanti | Judaism-GB | Cru-\nsades].\nTable 7: Examples of prompts for KICGPT on the FB15k-237 dataset.\n8682\nSteps Prompt Template ChatGPT\nResponsibility Description Assume you’re a linguist of English lexicons. You will be first\ngiven some examples. Then use these examples as references and\nyour own knowledge to score for some statements. If you have known\nyour responsibility, respond \"Yes\". Otherwise, respond \"No\". Do\nnot output anything except \"Yes\" and \"No\".\nYes.\nQuestion and Demonstration\nDescription\nThe goal statements are about member of domain usage of trade name.\ntrade name : a name given to a product or service. Part of the\ngiven examples are similar to the statements, you should analogy\nthem to understand the potential meaning of the statements to be\nscored. Another part of the given examples contains supplementary\ninformation, keep capturing this extra information and mining\npotential relationships among them to help the scoring. Please\ncarefully read, realize and think about these examples. Summarize\nthe way of thinking in these examples and memorize the information\nyou think maybe help. DO NOT give me any feedback.\nOkay.\nMultiple Demonstrations Examples used to Analogy: trade name : a name given to a product\nor service. vinblastine : periwinkle plant derivative used as\nan antineoplastic drug (trade name Velban) that disrupts cell\ndivision. vinblastine be member of domain usage of trade name .\nvernacular : a characteristic language of a particular group (as\namong thieves); \"they don’t speak our lingo\". chink : a narrow\nopening as e.g. between planks in a wall. chink be member of\ndomain usage of vernacular.\nExamples give supplement information: trade name : a name given to\na product or service. cortone acetate : a corticosteroid hormone\n(trade name Cortone Acetate) normally produced by the adrenal\ncortex; is converted to hydrocortisone. cortone acetate be member\nof domain usage of trade name . trade name : a name given to\na product or service. phenelzine : monoamine oxidase inhibitor\n(trade name Nardil) used to treat clinical depression. phenelzine\nbe member of domain usage of trade name. Keep thinking but DO NOT\ngive me any feedback.\nOkay.\nFinal Query trade name : a name given to a product or service. verapamil\n: a drug (trade names Calan and Isoptin) used as an oral or\nparenteral calcium blocker in cases of hypertension or congestive\nheart failure or angina or migraine. verapamil be member of domain\nusage of trade name. Directly give a score out of 100 for the\nstatement and DO NOT output any other thing\n...\ntrade name : a name given to a product or service. nitrostat :\ntrade names for nitroglycerin used as a coronary vasodilator in\nthe treatment of angina pectoris. nitrostat be member of domain\nusage of trade name .. Directly give a score out of 100 for the\nstatement and DO NOT output any other thing\n...\ntrade name : a name given to a product or service. hydantoin :\nany of a group of anticonvulsant drugs used in treating epilepsy.\nhydantoin be member of domain usage of trade name .. Directly give\na score out of 100 for the statement and DO NOT output any other\nthing.\n...\n90.\n...\n100\n...\n50\n...\nTable 8: Examples of prompts for KICGPT on the WN18RR dataset.\n8683",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8040589094161987
    },
    {
      "name": "Knowledge graph",
      "score": 0.715086817741394
    },
    {
      "name": "Context (archaeology)",
      "score": 0.47971123456954956
    },
    {
      "name": "Graph",
      "score": 0.465654194355011
    },
    {
      "name": "Language model",
      "score": 0.41877955198287964
    },
    {
      "name": "Natural language processing",
      "score": 0.382277250289917
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35391563177108765
    },
    {
      "name": "Theoretical computer science",
      "score": 0.26216739416122437
    },
    {
      "name": "Geology",
      "score": 0.05453282594680786
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3045169105",
      "name": "Southern University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I28290843",
      "name": "University of Surrey",
      "country": "GB"
    }
  ]
}