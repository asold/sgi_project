{
  "title": "Intermediate-Task Transfer Learning with Pretrained Models for Natural Language Understanding: When and Why Does It Work?",
  "url": "https://openalex.org/W3023419341",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4221755963",
      "name": "Pruksachatkun, Yada",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223480189",
      "name": "Phang, Jason",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2846069892",
      "name": "Liu, Haokun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282320340",
      "name": "Htut, Phu Mon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1703330292",
      "name": "Zhang Xiaoyi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227072491",
      "name": "Pang, Richard Yuanzhe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3208483215",
      "name": "Vania, Clara",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221390147",
      "name": "Kann, Katharina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221820538",
      "name": "Bowman, Samuel R.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2946659172",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W2251199578",
    "https://openalex.org/W2250263931",
    "https://openalex.org/W2963691697",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2971339032",
    "https://openalex.org/W2525127255",
    "https://openalex.org/W2941666437",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W3099624838",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2970780738",
    "https://openalex.org/W2898700502",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W1752492850",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3037191812",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2963783335",
    "https://openalex.org/W2963123047",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2951873305",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W2088911157",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2592170186",
    "https://openalex.org/W2971044268",
    "https://openalex.org/W2605221307",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W2159636675",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W2963121782",
    "https://openalex.org/W3082274269"
  ],
  "abstract": "While pretrained models such as BERT have shown large gains across natural language understanding tasks, their performance can be improved by further training the model on a data-rich intermediate task, before fine-tuning it on a target task. However, it is still poorly understood when and why intermediate-task training is beneficial for a given target task. To investigate this, we perform a large-scale study on the pretrained RoBERTa model with 110 intermediate-target task combinations. We further evaluate all trained models with 25 probing tasks meant to reveal the specific skills that drive transfer. We observe that intermediate tasks requiring high-level inference and reasoning abilities tend to work best. We also observe that target task performance is strongly correlated with higher-level abilities such as coreference resolution. However, we fail to observe more granular correlations between probing and target task performance, highlighting the need for further work on broad-coverage probing benchmarks. We also observe evidence that the forgetting of knowledge learned during pretraining may limit our analysis, highlighting the need for further work on transfer learning methods in these settings.",
  "full_text": "Intermediate-Task Transfer Learning with Pretrained Models for Natural\nLanguage Understanding: When and Why Does It Work?\nYada Pruksachatkun1∗ Jason Phang1∗ Haokun Liu1∗ Phu Mon Htut1∗\nXiaoyi Zhang1 Richard Yuanzhe Pang1 Clara Vania1 Katharina Kann2\nSamuel R. Bowman1\n1New York University\n2University of Colorado Boulder\n{yp913,bowman}@nyu.edu\nAbstract\nWhile pretrained models such as BERT have\nshown large gains across natural language un-\nderstanding tasks, their performance can be\nimproved by further training the model on a\ndata-rich intermediate task, before ﬁne-tuning\nit on a target task. However, it is still poorly\nunderstood when and why intermediate-task\ntraining is beneﬁcial for a given target task. To\ninvestigate this, we perform a large-scale study\non the pretrained RoBERTa model with 110\nintermediate–target task combinations. We\nfurther evaluate all trained models with 25\nprobing tasks meant to reveal the speciﬁc\nskills that drive transfer. We observe that\nintermediate tasks requiring high-level infer-\nence and reasoning abilities tend to work best.\nWe also observe that target task performance\nis strongly correlated with higher-level abil-\nities such as coreference resolution. How-\never, we fail to observe more granular corre-\nlations between probing and target task per-\nformance, highlighting the need for further\nwork on broad-coverage probing benchmarks.\nWe also observe evidence that the forgetting\nof knowledge learned during pretraining may\nlimit our analysis, highlighting the need for\nfurther work on transfer learning methods in\nthese settings.\n1 Introduction\nUnsupervised pretraining—e.g., BERT (Devlin\net al., 2019) or RoBERTa (Liu et al., 2019b)—has\nrecently pushed the state of the art on many nat-\nural language understanding tasks. One method\nof further improving pretrained models that has\nbeen shown to be broadly helpful is to ﬁrst ﬁne-\ntune a pretrained model on an intermediate task,\nbefore ﬁne-tuning again on the target task of inter-\nest (Phang et al., 2018; Wang et al., 2019a; Clark\net al., 2019a; Sap et al., 2019), also referred to as\n∗Equal contribution.\nFigure 1: Our experimental pipeline with intermediate-\ntask transfer learning and subsequent ﬁne-tuning on tar-\nget and probing tasks.\nSTILTs. However, this approach does not always\nimprove target task performance, and it is unclear\nunder what conditions it does.\nThis paper offers a large-scale empirical study\naimed at addressing this open question. We per-\nform a broad survey of intermediate and target task\npairs, following an experimental pipeline similar to\nPhang et al. (2018) and Wang et al. (2019a). This\ndiffers from previous work in that we use a larger\nand more diverse set of intermediate and target\ntasks, introduce additional analysis-oriented prob-\ning tasks, and use a better-performing base model\nRoBERTa (Liu et al., 2019b). We aim to answer\nthe following speciﬁc questions:\n•What kind of tasks tend to make good inter-\nmediate tasks across a wide variety of target\ntasks?\n•Which linguistic skills does a model learn\nfrom intermediate-task training?\n•Which skills learned from intermediate tasks\nhelp the model succeed on which target tasks?\nThe ﬁrst question is the most straightforward: it\ncan be answered by a sufﬁciently exhaustive search\nover possible intermediate–target task pairs. The\nsecond and third questions address the why rather\nthan the when, and differ in a crucial detail: A\narXiv:2005.00628v2  [cs.CL]  9 May 2020\nmodel might learn skills by training on an inter-\nmediate task, but those skills might not help it to\nsucceed on a target task.\nOur search for intermediate tasks focuses on nat-\nural language understanding tasks in English. In\nparticular, we run our experiments on 11 interme-\ndiate tasks and 10 target tasks, which results in a\ntotal of 110 intermediate–target task pairs. We use\n25 probing tasks—tasks that each target a narrowly\ndeﬁned model behavior or linguistic phenomenon—\nto shed light on which skills are learned from each\nintermediate task.\nOur ﬁndings include the following: (i) Natural\nlanguage inference tasks as well as QA tasks which\ninvolve commonsense reasoning are generally use-\nful as intermediate tasks. (ii) SocialIQA and QQP\nas intermediate tasks are not helpful as a means to\nteach the skills captured by our probing tasks, while\nﬁnetuning ﬁrst on MNLI and CosmosQA result in\nan increase in all skills. (iii) While a model’s abil-\nity to learn skills relating to input-noising correlate\nwith target task performance, low-level skills such\nas knowledge of a sentence’s raw content preser-\nvation skills and ability to detect various attributes\nof input sentences such as tense of main verb and\nsentence length are less correlated with target task\nperformance. This suggests that a model’s abil-\nity to do well on the masked language modelling\n(MLM) task is important for downstream perfor-\nmance. Furthermore, we conjecture that a portion\nof our analysis is affected by catastrophic forgetting\nof knowledge learned during pretraining.\n2 Methods\n2.1 Experimental Pipeline\nOur experimental pipeline (Figure 1) consists\nof two steps, starting with a pretrained model:\nintermediate-task training, and ﬁne-tuning on a\ntarget or probing task.\nIntermediate Task Training We ﬁne-tune\nRoBERTa on each intermediate task. The training\nprocedure follows the standard procedure of\nﬁne-tuning a pretrained model on a target task, as\ndescribed in Devlin et al. (2019). We opt for single\nintermediate-task training as opposed to multi-task\ntraining (cf. Liu et al., 2019a) to isolate the effect\nof skills learned from individual intermediate\ntasks.\nTarget and Probing Task Fine-Tuning After\nintermediate-task training, we ﬁne-tune our models\non each target and probing task individually. Target\ntasks are tasks of interest to the general commu-\nnity, spanning various facets of natural language,\ndomains, and sources. Probing tasks, while poten-\ntially similar in data source to target tasks such as\nwith CoLA, are designed to isolate the presence\nof particular linguistic capabilities or skills. For\ninstance, solving the target task BoolQ (Clark et al.,\n2019a) may require various skills including coref-\nerence and commonsense reasoning, while prob-\ning tasks like the SentEval probing suite (Conneau\net al., 2018) target speciﬁc syntactic and metadata-\nlevel phenomena such as subject-verb agreement\nand sentence length detection.\n2.2 Tasks\nTable 1 presents an overview of the intermediate\nand target tasks.\n2.2.1 Intermediate Tasks\nWe curate a diverse set of tasks that either represent\nan especially large annotation effort or that have\nbeen shown to yield positive transfer in prior work.\nThe resulting set of tasks cover question answer-\ning, commonsense reasoning, and natural language\ninference.\nQAMR The Question–Answer Meaning Repre-\nsentations dataset (Michael et al., 2018) is a crowd-\nsourced QA task consisting of question–answer\npairs that correspond to predicate–argument re-\nlationships. It is derived from Wikinews and\nWikipedia sentences. For example, if the sentence\nis “Ada Lovelace was a computer scientist.”, a po-\ntential question is “What is Ada’s last name?”, with\nthe answer being “Lovelace.”\nCommonsenseQA CommonsenseQA (Talmor\net al., 2019) is a multiple-choice QA task derived\nfrom ConceptNet (Speer et al., 2017) with the help\nof crowdworkers, that is designed to test a range of\ncommonsense knowledge.\nSciTail SciTail (Khot et al., 2018) is a textual en-\ntailment task built from multiple-choice science\nquestions from 4th grade and 8th grade exams,\nas well as crowdsourced questions (Welbl et al.,\n2017). The task is to determine whether a hypothe-\nsis, which is constructed from a science question\nand its corresponding answer, is entailed or not\n(neutral) by the premise.\nCosmos QA Cosmos QA is a task for a\ncommonsense-based reading comprehension task\nName |Train| | Dev| task metrics genre/source\nCommonsenseQA 9,741 1,221 question answering acc. ConceptNet\nSciTail 23,596 1,304 natural language inference acc. science exams\nCosmos QA 25,588 3,000 question answering acc. blogs\nSocialIQA 33,410 1,954 question answering acc. crowdsourcing\nCCG 38,015 5,484 tagging acc. Wall Street Journal\nHellaSwag 39,905 10,042 sentence completion acc. video captions & Wikihow\nQA-SRL 44,837 7,895 question answering F1/EM Wikipedia\nSST-2 67,349 872 sentiment classiﬁcation acc. movie reviews\nQAMR 73,561 27,535 question answering F1/EM WikipediaIntermediate TasksQQP 363,846 40,430 paraphrase detection acc./F1 Quora questions\nMNLI 392,702 20,000 natural language inference acc. ﬁction, letters, telephone speech\nCB 250 57 natural language inference acc./F1 Wall Street Journal, ﬁction, dialogue\nCOPA 400 100 question answering acc. blogs, photography encyclopedia\nWSC 554 104 coreference resolution acc. hand-crafted\nRTE 2,490 278 natural language inference acc. news, Wikipedia\nMultiRC 5,100 953 question answering F1 α/EM crowd-sourced\nWiC 5,428 638 word sense disambiguation acc. WordNet, VerbNet, Wiktionary\nBoolQ 9,427 3,270 question answering acc. Google queries, WikipediaTarget TasksCommonsenseQA 9,741 1,221 question answering acc. ConceptNet\nCosmos QA 25,588 3,000 question answering acc. blogs\nReCoRD 100,730 10,000 question answering F1/EM news (CNN, Daily Mail)\nTable 1: Overview of the intermediate tasks (top) and target tasks (bottom) in our experiments. EM is short for\nExact Match. The F1 metrics for MultiRC is calculated over all answer-options.\nformulated as multiple-choice questions (Huang\net al., 2019). The questions concern the causes\nor effects of events that require reasoning not only\nbased on the exact text spans in the context, but also\nwide-range abstractive commonsense reasoning. It\ndiffers from CommonsenseQA in that it focuses\non causal and deductive commensense reasoning\nand that it requires reading comprehension over an\nauxiliary passage, rather than simply answering a\nfreestanding question.\nSocialIQA SocialIQA (Sap et al., 2019) is a task\nfor multiple choice QA. It tests for reasoning sur-\nrounding emotional and social intelligence in ev-\neryday situations.\nCCG CCGbank (Hockenmaier and Steedman,\n2007) is a task that is a translation of the Penn\nTreebank into a corpus of Combinatory Categorial\nGrammar (CCG) derivations. We use the CCG su-\npertagging task, which is the task of assigning tags\nto individual word tokens that jointly determine the\nparse of the sentence.\nHellaSwag HellaSwag (Zellers et al., 2019) is a\ncommonsense reasoning task that tests a model’s\nability to choose the most plausible continuation of\na story. It is built using adversarial ﬁltering (Zellers\net al., 2018) with BERT to create challenging nega-\ntive examples.\nQA-SRL The question-answer driven semantic\nrole labeling dataset (QA-SRL; He et al., 2015)\nfor a QA task that is derived from a semantic role\nlabeling task. Each example, which consists of\na set of questions and answers, corresponds to a\npredicate-argument relationship in the sentence it\nis derived from. Unlike QAMR, which focuses on\nall words in the sentence, QA-SRL is speciﬁcally\nfocused on verbs.\nSST-2 The Stanford sentiment treebank (Socher\net al., 2013) is a sentiment classiﬁcation task based\non movie reviews. We use the binary sentence\nclassiﬁcation version of the task.\nQQP The Quora Question Pairs dataset1 is con-\nstructed based on questions posted on the commu-\nnity question-answering website Quora. The task\nis to determine if two questions are semantically\nequivalent.\nMNLI The Multi-Genre Natural Language In-\nference dataset (Williams et al., 2018) is a crowd-\nsourced collection of sentence pairs with textual\nentailment annotations across a variety of genres.\n2.2.2 Target Tasks\nWe use ten target tasks, eight of which are drawn\nfrom the SuperGLUE benchmark (Wang et al.,\n2019b). The tasks in the SuperGLUE benchmark\n1http://data.quora.com/First-Quora-DatasetRelease-\nQuestion-Pairs\ncover question answering, entailment, word sense\ndisambiguation, and coreference resolution and\nhave been shown to be easy for humans but dif-\nﬁcult for models like BERT. Although we offer a\nbrief description of the tasks below, we refer read-\ners to the SuperGLUE paper for a more detailed\ndescription of the tasks.\nCommitmentBank (CB; de Marneffe et al.,\n2019) is a three-class entailment task that con-\nsists of texts and an embedded clause that ap-\npears in each text, in which models must determine\nwhether that embedded clause is entailed by the\ntext. Choice of Plausible Alternatives (COPA;\nRoemmele et al., 2011) is a classiﬁcation task that\nconsists of premises and a question that asks for the\ncause or effect of each premise, in which models\nmust correctly pick between two possible choices.\nWinograd Schema Challenge (WSC; Levesque\net al., 2012) is a sentence-level commonsense rea-\nsoning task that consists of texts, a pronoun from\neach text, and a list of possible noun phrases from\neach text. The dataset has been designed such that\nworld knowledge is required to determine which\nof the possible noun phrases is the correct referent\nto the pronoun. We use the SuperGLUE binary\nclassiﬁcation cast of the task, where each example\nconsists of a text, a pronoun, and a noun phrase\nfrom the text, which models must classify as being\ncoreferent to the pronoun or not. Recognizing Tex-\ntual Entailment (RTE; Dagan et al., 2005, et seq)\nis a textual entailment task. Multi-Sentence Read-\ning Comprehension (MultiRC; Khashabi et al.,\n2018) is a multi-hop QA task that consists of para-\ngraphs, a question on each paragraph, and a list\nof possible answers, in which models must distin-\nguish which of the possible answers are true and\nwhich are false. Word-in-Context (WiC; Pilehvar\nand Camacho-Collados, 2019) is a binary classiﬁ-\ncation word sense disambiguation task. Examples\nconsist of two text snippets, with a polysemous\nword that appears in both. Models must determine\nwhether the same sense of the word is used in both\ncontexts. BoolQ (Clark et al., 2019a) is a QA task\nthat consists of passages and a yes/no question as-\nsociated with each passage. Reading Comprehen-\nsion with Commonsense Reasoning (ReCoRD;\nZhang et al., 2018) is a multiple-choice QA task\nthat consists of news articles. For each article, mod-\nels are given a question about each article with one\nentity masked out and a list of possible entities\nfrom the article, and the goal is to correctly identify\nthe masked entity out of the list.\nAdditionally, we use CommonsenseQA and\nCosmos QA as target tasks, due to their unique\ncombination of small dataset size and high level of\ndifﬁculty for high-performing models like BERT\nfrom our set of intermediate tasks.\n2.2.3 Probing Tasks\nWe use well-established datasets for our probing\ntasks, including the edge-probing suite from Ten-\nney et al. (2019b), function word oriented tasks\nfrom Kim et al. (2019), and sentence-level probing\ndatasets (SentEval; Conneau et al., 2018).\nAcceptability Judgment Tasks This set of bi-\nnary classiﬁcations tasks was designed to inves-\ntigate if a model can judge the grammatical ac-\nceptability of a sentence. We use the following\nﬁve datasets: AJ-CoLA is a task that tests for\na model’s understanding of general grammatical-\nity using the Corpus of Linguistic Acceptability\n(CoLA) (Warstadt et al., 2019b), which is drawn\nfrom 22 theoretical linguistics publications. The\nother tasks concern the behaviors of speciﬁc classes\nof function words, using the dataset by Kim et al.\n(2019): AJ-WH is a task that tests a model’s\nability to detect if a wh-word in a sentence has\nbeen swapped with another wh-word, which tests\na model’s ability to identify the antecedent associ-\nated with the wh-word. AJ-Def is a task that tests\na model’s ability to detect if the deﬁnite/indeﬁnite\narticles in a given sentence have been swapped.AJ-\nCoord is a task that tests a model’s ability to detect\nif a coordinating conjunction has been swapped,\nwhich tests a model’s ability to understand how\nideas in the various clauses relate to each other.\nAJ-EOS is a task that tests a model’s ability to\nidentify grammatical sentences without indicators\nsuch as punctuation marks and capitalization, and\nconsists of grammatical text that are removed of\npunctuation.\nEdge-Probing Tasks The edge probing (EP)\ntasks are a set of core NLP labeling tasks, collected\nby Tenney et al. (2019b) and cast into Boolean\nclassiﬁcation. These tasks focus on the syntactic\nand semantic relations between spans in a sentence.\nThe ﬁrst ﬁve tasks use the OntoNotes corpus (Hovy\net al., 2006): Part-of-Speech tagging (EP-POS)\nis a task that tests a model’s ability to predict the\nsyntactic category (noun, verb, adjective, etc.) for\neach word in the sentence. Named entity recog-\nnition (EP-NER) is task that tests a model’s abil-\nity to predict the category of an entity in a given\nspan. Semantic Role Labeling(EP-SRL) is a task\nthat tests a model’s ability to assign a label to a\ngiven span of words that indicates its semantic role\n(agent, goal, etc.) in the sentence. Coreference\n(EP-Coref) is a task that tests a model’s ability to\nclassify if two spans of tokens refer to the same\nentity/event.\nThe other datasets can be broken down into both\nsyntactic and semantic probing tasks. Constituent\nlabeling (EP-Const) is a task that tests a model’s\nability to classify a non-terminal label for a span\nof tokens (e.g., noun phrase, verb phrase, etc.). De-\npendency labeling (EP-UD) is a task that tests a\nmodel on the functional relationship of one token\nrelative to another. We use the English Web Tree-\nbank portion of Universal Dependencies 2.2 release\n(Silveira et al., 2014) for this task.Semantic Proto-\nRole labeling is a task that tests a model’s ability\nto predict the ﬁne-grained non-exclusive semantic\nattributes of a given span. Edge probing uses two\ndatasets for SPR: SPR1 (EP-SPR1) (Teichert et al.,\n2017), derived from the Penn Treebank, and SPR2\n(EP-SPR2) (Rudinger et al., 2018), derived from\nthe English Web Treebank. Relation classiﬁca-\ntion (EP-Rel) is a task that tests a model’s ability\nto predict the relation between two entities. We\nuse the SemEval 2010 Task 8 dataset (Hendrickx\net al., 2009) for this task. For example, the relation\nbetween “Yeri” and “Korea” in “Yeri is from Ko-\nrea” is ENTITY-ORIGIN . The Deﬁnite Pronoun\nResolution dataset (Rahman and Ng, 2012) ( EP-\nDPR) is a task that tests a model’s ability to handle\ncoreference, and differs from OntoNotes in that it\nfocuses on difﬁcult cases of deﬁnite pronouns.\nSentEval Tasks The SentEval probing tasks (SE)\n(Conneau et al., 2018) are cast in the form of\nsingle-sentence classiﬁcation. Sentence Length\n(SE-SentLen) is a task that tests a model’s ability\nto classify the length of a sentence. Word Con-\ntent (SE-WC) is a task that tests a model’s abil-\nity to identify which of a set of 1,000 potential\nwords appear in a given sentence. Tree Depth(SE-\nTreeDepth) is a task that tests a model’s ability to\nestimate the maximum depth of the constituency\nparse tree of the sentence. Top Constituents (SE-\nTopConst) is a task that tests a model’s ability to\nidentify the high-level syntactic structure of the\nsentence by choosing among 20 constituent se-\nquences (the 19 most common, plus an other cat-\negory). Bigram Shift (SE-BShift) is a task that\ntests a model’s ability to classify if two consec-\nutive tokens in the same sentence have been re-\nordered. Coordination Inversion (SE-CoordInv)\nis a task that tests a model’s ability to identify if\ntwo coordinating clausal conjoints are swapped (ex:\n“he knew it, and he deserved no answer.”). Past-\nPresent (SE-Tense) is a task that tests a model’s\nability to classify the tense of the main verb of the\nsentence. Subject Number (SE-SubjNum) and\nObject Number (SE-ObjNum) are tasks that test\na model’s ability to classify whether the subject or\ndirect object of the main clause is singular or plural.\nOdd-Man-Out (SE-SOMO) is a task that tests the\nmodel’s ability to predict whether a sentence has\nhad one of its content words randomly replaced\nwith another word of the same part of speech.\n3 Experiments\nTraining and Optimization We use the large-\nscale pretrained model RoBERTaLarge in all experi-\nments. For each intermediate, target, and probing\ntask, we perform a hyperparameter sweep, varying\nthe peak learning rate ∈{2 ×10−5, 1 ×10−5, 5 ×\n10−6, 3 ×10−6}and the dropout rate ∈{0.2, 0.1}.\nAfter choosing the best learning rate and dropout\nrate, we apply the best conﬁguration for each task\nfor all runs. For each task, we use the batch size\nthat maximizes GPU usage, and use a maximum\nsequence length of 256. Aside from these details,\nwe follow the RoBERTa paper for all other training\nhyperparameters. We use NVIDIA P40 GPUs for\nour experiments.\nA complete pipeline with one intermediate task\nworks as follows: First, we ﬁne-tune RoBERTa on\nthe intermediate task. We then ﬁne-tune copies of\nthe resulting model separately on each of the 10\ntarget tasks and 25 probing tasks and test on their\nrespective validation sets. We run the same pipeline\nthree times for the 11 intermediate tasks, plus a set\nof baseline runs without intermediate training. This\ngives us 35×12×3 = 1260 observations.\nWe train our models using the Adam optimizer\n(Kingma and Ba, 2015) with linear decay and early\nstopping. We run training for a maximum of 10\nepochs when more than 1,500 training examples\nare available, and 40 epochs otherwise to ensure\nmodels are sufﬁciently trained on small datasets.\nWe use the jiant (Wang et al., 2019c) NLP\ntoolkit, based on PyTorch (Paszke et al., 2019),\nHugging Face Transformers (Wolf et al., 2019),\nand AllenNLP (Gardner et al., 2017), for all of our\nQAMR CSenseQA SciTail CosmosQASocialIQA CCG HellaSwag QA-SRL SST-2 QQP MNLI\nCB\nCOPA\nWSC\nRTE\nMultiRC\nWiC\nBoolQ\nCSenseQA\nCosmosQA\nReCoRD\nAvg. Target\nEP-POS\nEP-NER\nEP-SRL\nEP-Coref\nEP-Const\nEP-SPR1\nEP-SPR2\nEP-DPR\nEP-Rel\nEP-UD\nSE-SentLen\nSE-WC\nSE-TreeDepth\nSE-TopConst\nSE-BShift\nSE-Tense\nSE-SubjNum\nSE-ObjNum\nSE-SOMO\nSE-CoordInv\nAJ-CoLA\nAJ-Wh\nAJ-Def\nAJ-Coord\nAJ-EOS\n-4.0 -0.4 -6.2 -0.4 -21.7 -12.2 -3.1 -7.2 -1.2 -31.0 -0.4\n-4.0 8.7 4.3 6.0 -3.7 -20.7 6.7 -3.7 -2.0 0.7 -0.7\n-0.3 0.0 1.3 2.9 -4.8 -3.2 3.6 4.8 2.6 -3.8 0.3\n0.6 3.4 3.4 5.1 -4.3 -18.2 4.8 1.1 2.6 -2.4 3.1\n2.4 7.9 2.6 10.1 -10.6 -8.1 6.8 2.6 1.1 -4.2 6.5\n-1.3 0.1 2.5 1.7 -2.0 -1.1 0.1 2.1 -6.4 1.4 0.9\n-0.1 0.9 0.1 1.1 -2.8 -10.6 0.7 0.0 0.9 -4.2 1.4\n-4.7 -1.6 -2.6 0.1 -7.8 -12.0 0.4 -5.1 -0.9 -7.6 -2.6\n-2.5 -0.1 -2.1 -0.4 -9.1 -6.9 -0.0 -3.0 -0.0 -8.4 -0.5\n-4.0 -0.0 -1.5 -0.1 -12.4 -6.1 0.2 -4.7 -0.5 -11.9 -1.6\n-1.8 1.9 0.2 2.6 -7.9 -9.9 2.0 -1.3 -0.4 -7.1 0.7\n0.0 0.0 -0.0 -0.1 -0.1 -0.0 0.0 -0.0 0.1 -97.4 0.0\n-0.1 0.0 -0.1 -0.1 -21.5 -0.2 0.0 -0.2 0.0 -64.9 -0.3\n12.2 0.1 30.7 12.4 -61.7 31.2 30.9 31.1 31.9 -61.9 31.3\n0.0 0.0 0.0 0.1 -0.6 -0.3 0.1 0.0 -0.1 -13.4 0.1\n-0.0 -0.1 -0.1 0.0 -0.0 -0.2 -0.1 0.0 -0.9 -0.2 -0.1\n-0.2 0.1 0.1 0.2 -1.7 -0.4 0.2 0.1 0.3 -21.9 0.2\n-0.2 -0.0 -0.1 0.1 -3.9 -0.4 -0.1 -0.3 -0.1 -8.2 -0.1\n7.5 7.9 7.3 8.6 -15.6 3.5 8.3 8.2 7.9 -14.7 6.6\n0.1 -25.0 0.4 0.1 -55.1 0.2 0.4 -28.8 0.8 -85.4 0.1\n-0.2 0.0 0.0 0.1 -62.0 -0.2 0.0 -0.1 0.1 -89.7 -0.0\n-0.0 -0.2 -0.1 -0.3 -0.4 0.5 -0.1 0.1 0.1 -0.9 -0.2\n-0.1 -0.0 -0.0 -0.0 -33.3 -0.0 0.0 -0.0 -0.0 -33.8 -0.0\n0.1 -0.1 -0.1 -0.1 -1.1 0.3 -0.5 -0.1 -0.1 -1.4 -0.6\n-0.2 -0.3 -0.3 -0.1 -0.4 -0.2 -0.2 -0.2 -0.2 -0.4 -0.3\n-0.1 0.2 0.1 0.0 -0.4 -0.2 0.2 0.0 0.1 -0.1 0.1\n-1.1 -0.4 -0.5 -0.0 -0.3 -1.3 0.0 -0.8 -0.2 -1.5 -1.2\n0.3 0.5 0.4 0.9 -0.1 0.8 0.8 0.2 0.5 -0.1 0.4\n-0.6 -0.1 -0.1 0.0 -0.5 0.2 -0.3 0.2 -0.4 0.2 -0.1\n-2.2 0.4 -1.1 0.1 -4.1 -3.6 0.2 -1.8 -1.0 -2.5 -1.2\n-0.7 -0.1 -0.4 -0.2 -1.3 -1.0 -0.0 -0.3 -0.2 -3.0 -0.1\n-2.6 -0.7 -1.9 -1.6 -10.3 -6.9 -0.7 -3.7 -0.6 -5.5 -1.1\n13.4 26.8 3.4 14.5 14.2 26.8 14.5 28.4 28.4 3.8 11.8\n23.1 46.0 11.1 0.0 18.0 46.4 32.4 22.5 14.0 11.1 23.7\n25.2 17.7 11.1 20.2 22.3 32.6 11.1 22.2 17.4 11.1 11.1\n11.9 13.2 13.9 13.2 -21.3 8.5 5.0 11.8 -4.5 -13.9 6.0\nTargetProbing\nBaseline\nPerformance\n99.1\n86.0\n67.3\n83.5\n47.4\n70.5\n86.6\n74.0\n81.9\n86.0\n78.2\n98.1\n97.0\n61.9\n97.1\n88.8\n87.2\n83.8\n81.4\n85.4\n95.8\n46.4\n99.8\n76.1\n93.5\n97.7\n91.1\n93.3\n95.7\n77.2\n88.3\n68.1\n69.9\n47.2\n47.2\n84.7\nFigure 2: Transfer learning results between intermediate and target/probing tasks. Baselines (rightmost column)\nare models ﬁne-tuned without intermediate-task training. Each cell shows the difference in performance (delta)\nbetween the baseline and model with intermediate-task training. We use the macro-average of each task’s metrics\nas the reported performance. Refer to Table 1 for target task metrics.\nexperiments.\n4 Results and Analysis\n4.1 Investigating Transfer Performance\nFigure 2 shows the differences in target and probing\ntask performances (deltas) between the baselines\nand models trained with intermediate-task training,\neach averaged across three restarts. A positive delta\nindicates successful transfer.\nTarget Task Performance We deﬁne good inter-\nmediate tasks as ones that lead to positive trans-\nfer in target task performance. We observe that\ntasks that require complex reasoning and inference\ntend to make good intermediate tasks. These in-\nclude MNLI and commonsense-oriented tasks such\nas CommonsenseQA, HellaSWAG, and Cosmos\nQA (with our poor performance with the similar\nSocialIQA serving as a suprising exception). So-\ncialIQA, CCG, and QQP as intermediate tasks lead\nto negative transfer on all target tasks and the ma-\njority of probing tasks.\nWe investigate the role of dataset size in the inter-\nmediate tasks with downstream task performance\nby additionally running a set of experiments on\nvarying amounts of data on ﬁve intermediate tasks,\nwhich is shown in the Appendix. We do not ﬁnd\ndifferences in intermediate-task dataset size to have\nany substantial consistent impact on downstream\ntarget task performance.\nIn addition, we ﬁnd that smaller target tasks such\nas RTE, BoolQ, MultiRC, WiC, WSC beneﬁt the\nmost from intermediate-task training.2 There are\nno instances of positive transfer to Commitment-\nBank, since our baseline model achieves 100% ac-\ncuracy.\nProbing Task Performance Looking at\nthe probing task performance, we ﬁnd that\nintermediate-task training affects performance\n2The deltas for experiments with the same intermediate\nand target tasks are not 0 as may be expected. This is because\nwe perform both intermediate and target training phases in\nthese cases, with reset optimizer states and stopping criteria in\nbetween intermediate and target training.\non low-level syntactic probing tasks uniformly\nacross intermediate tasks; we observe little to no\nimprovement for the SentEval probing tasks and\nhigher improvement for acceptability judgment\nprobing tasks, except for AJ-CoLA. This is also\nconsistent with Phang et al. (2018), who ﬁnd\nnegative transfer with CoLA in their experiments.\nVariation across Intermediate Tasks There is\nvariable performance across higher-level syntactic\nor semantic tasks such as the Edge-Probing and\nSentEval tasks. SocialIQA and QQP have nega-\ntive transfer for most of the Edge-Probing tasks,\nwhile CosmosQA and QA-SRL see drops in per-\nformance only for EP-Rel. While we do see that\nintermediate-task trained models improve perfor-\nmance on EP-SRL and EP-DPR across the board,\nthere is little to no gain in SentEval probing tasks\nfrom any intermediate tasks. Additionally, tasks\nthat increase performance in the most number of\nprobing tasks perform well as intermediate tasks.\nDegenerate Runs We ﬁnd that the model may\nnot exceed chance performance in some training\nruns. This mostly affects the baseline (no interme-\ndiate training) runs on the acceptability judgment\nprobing tasks, excluding AJ-CoLA, which all have\nvery small training sets. We include these degener-\nate runs in our analysis to reﬂect this phenomenon.\nConsistent with Phang et al. (2018), we ﬁnd that\nintermediate-task training reduces the likelihood\nof degenerate runs, leading to ostensibly positive\ntransfer results on those four acceptability judg-\nment tasks across most intermediate tasks. On\nthe other hand, extremely negative transfer from\nintermediate-task training can also result in a higher\nfrequency of degenerate runs in downstream tasks,\nas we observe in the cases of using QQP and So-\ncialIQA as intermediate tasks. We also observe\na number of degenerate runs on the EP-SRL task\nas well as the EP-Rel task. These degenerate runs\ndecrease positive transfer in probing tasks, such\nas with SocialIQA and QQP probing performance,\nand also decrease the average amount of positive\ntransfer we see in target task performance.\n4.2 Correlation Between Probing and Target\nTask Performance\nNext, we investigate the relationship between target\nand probing tasks in an attempt to understand why\ncertain intermediate-task models perform better on\ncertain target tasks.\nWe use probing task performance as an indica-\ntor of the acquisition of particular language skills.\nWe compute the Spearman correlation between\nprobing-task and target-task performances across\ntraining on different intermediate tasks and mul-\ntiple restarts, as shown in Figure 3. We test for\nstatistical signiﬁcance at p = 0.05 and apply Holm-\nBonferroni correction for multiple testing. We omit\ncorrelations that are not statistically signiﬁcant. We\nopt for Spearman and not Pearson correlation be-\ncause of the wide variety of metrics used for the\ndifferent tasks.3\nWe ﬁnd that acceptability judgment probing task\nperformance is generally uncorrelated with the tar-\nget task performance, except for AJ-CoLA. Simi-\nlarly, many of the SentEval tasks do not correlate\nwith the target tasks, except for Bigram Shift (SE-\nBShift), Odd-Man-Out (SE-SOMO) and Coordi-\nnation Inversion (SE-CoordInv). These three tasks\nare input noising tasks—tasks where a model has to\npredict if a given input sentence has been randomly\nmodiﬁed—which are, by far, the most similar tasks\nwe study to the masked language modeling task\nthat is used for training RoBERTa. This may ex-\nplain the strong correlation with the performance\nof the target tasks.\nWe also ﬁnd that some of these strong correla-\ntions, such as with SE-SOMO and SE-CoordInv,\nare almost entirely driven by variation in the de-\ngree of negative transfer, rather than any positive\ntransfer. Intuitively, ﬁne-tuning RoBERTa on an\nintermediate task can cause the model to forget\nsome of its ability to perform the MLM task. Thus,\na future direction for potential improvement for\nintermediate-task training may be integrating the\nMLM objective into intermediate-task training or\nbounding network parameter changes to reduce\ncatastrophic forgetting (Kirkpatrick et al., 2016;\nChen et al., 2019).\nInterestingly, while intermediate tasks such as\nSocialIQA, CCG and QQP, which show negative\ntransfer on target tasks, tend to have negative trans-\nfer on these three probing tasks, the intermedi-\nate tasks with positive transfer, such as Common-\nsenseQA tasks and MNLI, do not appear to ad-\nversely affect the performance on these probing\ntasks. This asymmetric impact may indicate that,\nbeyond the similarity of intermediate and target\ntasks, avoiding catastrophic forgetting of pretrain-\n3Full correlation tables across all target and probing tasks\nwith both Spearman and Pearson correlations can be found in\nthe Appendix.\nCB\nCOPA\nWSC\nRTE\nMultiRC\nWiC\nBoolQ\nCSenseQA\nCosmosQA\nReCoRD\nEP-POS\nEP-NER\nEP-SRL\nEP-Coref\nEP-Const\nEP-SPR1\nEP-SPR2\nEP-DPR\nEP-Rel\nEP-UD\nSE-SentLen\nSE-WC\nSE-TreeDepth\nSE-TopConst\nSE-BShift\nSE-Tense\nSE-SubjNum\nSE-ObjNum\nSE-SOMO\nSE-CoordInv\nAJ-CoLA\nAJ-Wh\nAJ-Def\nAJ-Coord\nAJ-EOS\nCB\nCOPA\nWSC\nRTE\nMultiRC\nWiC\nBoolQ\nCSenseQA\nCosmosQA\nReCoRD\n1 .73 .74 .72 .82 .69 .71 .70 .72 .66 .74 .63 .75 .64 .71\n1 .67 .66 .74\n1 .63\n.67 1 .86 .83 .85 .68 .67 .71 .66 .71 .74 .80 .71\n.73 .86 1 .79 .76 .67 .66 .78 .74 .71 .73 .79\n1\n.74 .83 .79 1 .79 .80 .76 .74 .70 .69 .76 .68 .75 .82 .78\n.72 .66 .85 .76 .79 1 .85 .83 .61 .74 .77 .68 .69 .80 .72 .88 .76 .76\n.82 .68 .67 .80 .85 1 .86 .63 .70 .76 .66 .74 .81 .84 .87 .80 .83\n.69 .67 .66 .76 .83 .86 1 .66 .71 .77 .69 .73 .84 .76 .83 .79 .71\nTarget Probing\nFigure 3: Correlations between probing and target task performances. Each cell contains the Spearman correlation\nbetween probing-task and target-task performances across training on different intermediate tasks and random\nrestarts. We test for statistical signiﬁcance at p = 0.05 with Holm-Bonferroni correction, and omit the correlations\nthat are not statistically signiﬁcant.\ning is critical to successful intermediate-task trans-\nfer.\nThe remaining SentEval probing tasks have sim-\nilar delta values (Figure 2), which may indicate\nthat there is insufﬁcient variation among trans-\nfer performance to derive signiﬁcant correlations.\nAmong the edge-probing tasks, the more semantic\ntasks such as coreference (EP-Coref and EP-DPR),\nsemantic proto-role labeling (EP-SPR1 and EP-\nSPR2), and dependency labeling (EP-Rel) show\nthe highest correlations with our target tasks. As\nour set of target tasks is also oriented towards se-\nmantics and reasoning, this is to be expected.\nOn the other hand, among the target tasks,\nwe ﬁnd that ReCoRD, CommonsenseQA and\nCosmos QA—all commonsense-oriented tasks—\nexhibit both high correlations with each other as\nwell as a similar set of correlations with the prob-\ning tasks. Similarly, BoolQ, MultiRC, and RTE\ncorrelate strongly with each other and have similar\npatterns of probing-task performance.\n5 Related Work\nWithin the paradigm of training large pre-\ntrained Transformer language representations via\nintermediate-stage training before ﬁne-tuning on\na target task, positive transfer has been shown in\nboth sequential task-to-task (Phang et al., 2018)\nand multi-task-to-task (Liu et al., 2019a; Raffel\net al., 2019) formats. Wang et al. (2019a) perform\nan extensive study on transfer with BERT, ﬁnd-\ning language modeling and NLI tasks to be among\nthe most beneﬁcial tasks for improving target-task\nperformance. Talmor and Berant (2019) perform a\nsimilar cross-task transfer study on reading compre-\nhension datasets, ﬁnding similar positive transfer in\nmost cases, with the biggest gains stemming from\na combination of multiple QA datasets. Our work\nconsists of a larger, more diverse, set of interme-\ndiate task–target task pairs. We also use probing\ntasks to shed light on the skills learned by the inter-\nmediate tasks.\nAmong the prior work on predicting transfer per-\nformance, Bingel and Søgaard (2017) is the most\nsimilar to ours. They do a regression analysis that\npredicts target-task performance on the basis of var-\nious features of the source and target tasks and task\npairs. They focus on a multi-task training setting\nwithout self-supervised pretraining, as opposed to\nour single-intermediate task, three-step procedure.\nSimilar work (Lin et al., 2019b) has been done\non cross-lingual transfer—the analogous challenge\nof transferring learned knowledge from a high-\nresource to a low-resource language.\nMany recent works have attempted to understand\nthe knowledge and linguistic skills BERT learns,\nfor instance by analyzing the language model\nsurprisal for subject–verb agreements (Goldberg,\n2018), identifying speciﬁc knowledge or phenom-\nena encapsulated in the representations learned by\nBERT using probing tasks (Tenney et al., 2019b,a;\nWarstadt et al., 2019a; Lin et al., 2019a; Hewitt and\nManning, 2019; Jawahar et al., 2019), analyzing\nthe attention heads of BERT (Clark et al., 2019b;\nCoenen et al., 2019; Lin et al., 2019a; Htut et al.,\n2019), and testing the linguistic generalizations of\nBERT across runs (McCoy et al., 2019). How-\never, relatively little work has been done to analyze\nﬁne-tuned BERT-style models (Wang et al., 2019a;\nWarstadt et al., 2019a).\n6 Conclusion and Future Work\nThis paper presents a large-scale study on when\nand why intermediate-task training works with\npretrained models. We perform experiments on\nRoBERTa with a total of 110 pairs of intermedi-\nate and target tasks, and perform an analysis using\n25 probing tasks, covering different semantic and\nsyntactic phenomena. Most directly, we observe\nthat tasks like Cosmos QA and HellaSwag, which\nrequire complex reasoning and inference, tend to\nwork best as intermediate tasks.\nLooking to our probing analysis, intermediate\ntasks that help RoBERTa improve across the board\nshow the most positive transfer in downstream\ntasks. However, it is difﬁcult to draw deﬁnite con-\nclusions about the speciﬁc skills that drive positive\ntransfer. Intermediate-task training may help im-\nprove the handling of syntax, but there is little to no\ncorrelation between target-task and probing-task\nperformance for these skills. Probes for higher-\nlevel semantic abilities tend to have a higher corre-\nlation with the target-task performance, but these\nresults are too diffuse to yield more speciﬁc con-\nclusions. Future work in this area would beneﬁt\ngreatly from improvements to both the breadth and\ndepth of available probing tasks.\nWe also observe a worryingly high correlation\nbetween target-task performance and the two prob-\ning tasks which most closely resemble RoBERTa’s\nmasked language modeling pretraining objective.\nThus, the results of our intermediate-task training\nanalysis may be driven in part by forgetting of\nknowledge acquired during pretraining. Our re-\nsults therefore suggest a need for further work on\nefﬁcient transfer learning mechanisms.\nAcknowledgments\nThis project has beneﬁted from support to SB by\nEric and Wendy Schmidt (made by recommenda-\ntion of the Schmidt Futures program), by Samsung\nResearch (under the project Improving Deep Learn-\ning using Latent Structure), by Intuit, Inc., and by\nNVIDIA Corporation (with the donation of a Titan\nV GPU).\nReferences\nJoachim Bingel and Anders Søgaard. 2017. Identify-\ning beneﬁcial task relations for multi-task learning\nin deep neural networks. In Proceedings of the 15th\nConference of the European Chapter of the Associa-\ntion for Computational Linguistics: Volume 2, Short\nPapers, pages 164–169, Valencia, Spain. Associa-\ntion for Computational Linguistics.\nXinyang Chen, Sinan Wang, Bo Fu, Mingsheng Long,\nand Jianmin Wang. 2019. Catastrophic forgetting\nmeets negative transfer: Batch spectral shrinkage for\nsafe transfer learning. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d’ Alch´e-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 1906–1916. Curran Asso-\nciates, Inc.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019a. BoolQ: Exploring the surprising\ndifﬁculty of natural yes/no questions. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2924–2936, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019b. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nAndy Coenen, Emily Reif, Ann Yuan, Been Kim,\nAdam Pearce, Fernanda B. Vi´egas, and Martin Wat-\ntenberg. 2019. Visualizing and measuring the geom-\netry of BERT. Unpublished manuscript available on\narXiv.\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¨ıc Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126–2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The pascal recognising textual entailment\nchallenge. In Machine Learning Challenges Work-\nshop, pages 177–190. Springer.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew\nPeters, Michael Schmitz, and Luke S. Zettlemoyer.\n2017. Allennlp: A deep semantic natural language\nprocessing platform. Unpublished manuscript avail-\nable on arXiv.\nYoav Goldberg. 2018. Assessing BERT’s syntac-\ntic abilities. Unpublished manuscript available on\narXiv.\nLuheng He, Mike Lewis, and Luke Zettlemoyer. 2015.\nQuestion-answer driven semantic role labeling: Us-\ning natural language to annotate natural language.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n643–653, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nIris Hendrickx, Su Nam Kim, Zornitsa Kozareva,\nPreslav Nakov, Diarmuid ´O S ´eaghdha, Sebastian\nPad´o, Marco Pennacchiotti, Lorenza Romano, and\nStan Szpakowicz. 2009. SemEval-2010 task 8:\nMulti-way classiﬁcation of semantic relations be-\ntween pairs of nominals. In Proceedings of the\nWorkshop on Semantic Evaluations: Recent Achieve-\nments and Future Directions (SEW-2009), pages 94–\n99, Boulder, Colorado. Association for Computa-\ntional Linguistics.\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJulia Hockenmaier and Mark Steedman. 2007. CCG-\nbank: A corpus of CCG derivations and dependency\nstructures extracted from the Penn treebank. Com-\nputational Linguistics, 33(3):355–396.\nEduard Hovy, Mitchell Marcus, Martha Palmer, Lance\nRamshaw, and Ralph Weischedel. 2006. OntoNotes:\nThe 90% solution. In Proceedings of the Human\nLanguage Technology Conference of the NAACL,\nCompanion Volume: Short Papers , pages 57–60,\nNew York City, USA. Association for Computa-\ntional Linguistics.\nPhu Mon Htut, Jason Phang, Shikha Bordia, and\nSamuel R. Bowman. 2019. Do attention heads in\nbert track syntactic dependencies? Unpublished\nmanuscript available on arXiv.\nLifu Huang, Ronan Le Bras, Chandra Bhagavatula, and\nYejin Choi. 2019. Cosmos QA: Machine reading\ncomprehension with contextual commonsense rea-\nsoning. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n2391–2401, Hong Kong, China. Association for\nComputational Linguistics.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking be-\nyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252–262, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018.\nScitail: A textual entailment dataset from science\nquestion answering. In Thirty-Second AAAI Confer-\nence on Artiﬁcial Intelligence.\nNajoung Kim, Roma Patel, Adam Poliak, Patrick Xia,\nAlex Wang, Tom McCoy, Ian Tenney, Alexis Ross,\nTal Linzen, Benjamin Van Durme, Samuel R. Bow-\nman, and Ellie Pavlick. 2019. Probing what dif-\nferent NLP tasks teach machines about function\nword comprehension. In Proceedings of the Eighth\nJoint Conference on Lexical and Computational Se-\nmantics (*SEM 2019), pages 235–249, Minneapolis,\nMinnesota. Association for Computational Linguis-\ntics.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nJames Kirkpatrick, Razvan Pascanu, Neil C. Rabi-\nnowitz, Joel Veness, Guillaume Desjardins, An-\ndrei A. Rusu, Kieran Milan, John Quan, Tiago Ra-\nmalho, Agnieszka Grabska-Barwinska, Demis Hass-\nabis, Claudia Clopath, Dharshan Kumaran, and Raia\nHadsell. 2016. Overcoming catastrophic forgetting\nin neural networks. In Proceedings of the national\nacademy of sciences (PNAS).\nHector J. Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In\nProceedings of the Thirteenth International Con-\nference on Principles of Knowledge Representa-\ntion and Reasoning, KR’12, pages 552–561. AAAI\nPress.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019a.\nOpen sesame: Getting inside BERT’s linguistic\nknowledge. In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP , pages 241–253, Florence,\nItaly. Association for Computational Linguistics.\nYu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li,\nYuyan Zhang, Mengzhou Xia, Shruti Rijhwani,\nJunxian He, Zhisong Zhang, Xuezhe Ma, Antonios\nAnastasopoulos, Patrick Littell, and Graham Neu-\nbig. 2019b. Choosing transfer languages for cross-\nlingual learning. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3125–3135, Florence, Italy. Associa-\ntion for Computational Linguistics.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and Jian-\nfeng Gao. 2019a. Multi-task deep neural networks\nfor natural language understanding. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 4487–4496, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. Unpublished manuscript available on arXiv.\nMarie-Catherine de Marneffe, Mandy Simons, and\nJudith Tonhauser. 2019. The CommitmentBank:\nInvestigating projection in naturally occurring dis-\ncourse. In proceedings of Sinn und Bedeutung , vol-\nume 23, pages 107–124.\nR. Thomas McCoy, Junghyun Min, and Tal Linzen.\n2019. BERTs of a feather do not generalize to-\ngether: Large variability in generalization across\nmodels with similar test set performance. Unpub-\nlished manuscript available on arXiv.\nJulian Michael, Gabriel Stanovsky, Luheng He, Ido Da-\ngan, and Luke Zettlemoyer. 2018. Crowdsourcing\nquestion-answer meaning representations. In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 2 (Short Papers), pages 560–568, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Py-\ntorch: An imperative style, high-performance deep\nlearning library. In H. Wallach, H. Larochelle,\nA. Beygelzimer, F. d’ Alch´e-Buc, E. Fox, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc.\nJason Phang, Thibault F ´evry, and Samuel R. Bowman.\n2018. Sentence Encoders on STILTs: Supplemen-\ntary Training on Intermediate Labeled-data Tasks.\nUnpublished manuscript available on arXiv.\nMohammad Taher Pilehvar and Jose Camacho-\nCollados. 2019. WiC: the word-in-context dataset\nfor evaluating context-sensitive meaning represen-\ntations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 1267–1273, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. Unpublished manuscript available on arXiv.\nAltaf Rahman and Vincent Ng. 2012. Resolving com-\nplex cases of deﬁnite pronouns: The Winograd\nschema challenge. In Proceedings of the 2012 Joint\nConference on Empirical Methods in Natural Lan-\nguage Processing and Computational Natural Lan-\nguage Learning, pages 777–789, Jeju Island, Korea.\nAssociation for Computational Linguistics.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alterna-\ntives: An evaluation of commonsense causal reason-\ning. In 2011 AAAI Spring Symposium Series.\nRachel Rudinger, Adam Teichert, Ryan Culkin, Sheng\nZhang, and Benjamin Van Durme. 2018. Neural-\nDavidsonian Semantic Proto-role Labeling. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 944–\n955, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social IQa: Com-\nmonsense reasoning about social interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 4453–\n4463, Hong Kong, China. Association for Computa-\ntional Linguistics.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nde Marneffe, Samuel Bowman, Miriam Connor,\nJohn Bauer, and Chris Manning. 2014. A Gold\nStandard Dependency Corpus for English. In Pro-\nceedings of the Ninth International Conference on\nLanguage Resources and Evaluation (LREC’14) ,\npages 2897–2904, Reykjavik, Iceland. European\nLanguage Resources Association (ELRA).\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive Deep Models for\nSemantic Compositionality Over a Sentiment Tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nRobert Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of gen-\neral knowledge. In Thirty-First AAAI Conference on\nArtiﬁcial Intelligence.\nAlon Talmor and Jonathan Berant. 2019. MultiQA: An\nempirical investigation of generalization and trans-\nfer in reading comprehension. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4911–4921, Florence,\nItaly. Association for Computational Linguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4149–4158, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAdam Teichert, Adam Poliak, Benjamin Van Durme,\nand Matthew R Gormley. 2017. Semantic proto-role\nlabeling. In Thirty-First AAAI Conference on Artiﬁ-\ncial Intelligence.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019b. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nAlex Wang, Jan Hula, Patrick Xia, Raghavendra Pap-\npagari, R. Thomas McCoy, Roma Patel, Najoung\nKim, Ian Tenney, Yinghui Huang, Katherin Yu,\nShuning Jin, Berlin Chen, Benjamin Van Durme,\nEdouard Grave, Ellie Pavlick, and Samuel R. Bow-\nman. 2019a. Can you tell me how to get past sesame\nstreet? sentence-level pretraining beyond language\nmodeling. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4465–4476, Florence, Italy. Association for\nComputational Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R. Bowman. 2019b. SuperGLUE:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In H. Wallach,\nH. Larochelle, A. Beygelzimer, F. d’ Alch ´e-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neu-\nral Information Processing Systems 32, pages 3261–\n3275. Curran Associates, Inc.\nAlex Wang, Ian F. Tenney, Yada Pruksachatkun,\nKatherin Yu, Jan Hula, Patrick Xia, Raghu Pappa-\ngari, Shuning Jin, R. Thomas McCoy, Roma Pa-\ntel, Yinghui Huang, Jason Phang, Edouard Grave,\nHaokun Liu, Najoung Kim, Phu Mon Htut, Thibault\nF´evry, Berlin Chen, Nikita Nangia, Anhad Mo-\nhananey, Katharina Kann, Shikha Bordia, Nicolas\nPatry, David Benton, Ellie Pavlick, and Samuel R.\nBowman. 2019c. jiant 1.2: A software toolkit\nfor research on general-purpose text understanding\nmodels.\nAlex Warstadt, Yu Cao, Ioana Grosu, Wei Peng, Ha-\ngen Blix, Yining Nie, Anna Alsop, Shikha Bordia,\nHaokun Liu, Alicia Parrish, Sheng-Fu Wang, Jason\nPhang, Anhad Mohananey, Phu Mon Htut, Paloma\nJeretic, and Samuel R. Bowman. 2019a. Investi-\ngating BERT’s knowledge of language: Five anal-\nysis methods with NPIs. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 2870–2880, Hong Kong,\nChina. Association for Computational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019b. Neural network acceptability judg-\nments. Transactions of the Association for Compu-\ntational Linguistics (TACL), 7:625–641.\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017.\nCrowdsourcing multiple choice science questions.\nIn Proceedings of the 3rd Workshop on Noisy User-\ngenerated Text, pages 94–106, Copenhagen, Den-\nmark. Association for Computational Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. Unpublished manuscript available on arXiv.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. SW AG: A large-scale adversar-\nial dataset for grounded commonsense inference. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, pages 93–\n104, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can\na machine really ﬁnish your sentence? In Pro-\nceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4791–\n4800, Florence, Italy. Association for Computational\nLinguistics.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nRecord: Bridging the gap between human and ma-\nchine commonsense reading comprehension. Un-\npublished manuscript available on arXiv.\nA Correlation Between Probing and\nTarget Task Performance\nFigure 4 shows the correlation matrix using Spear-\nman correlation and Figure 5 shows the matrix\nusing Pearson correlation.\nB Effect of Intermediate Task Size on\nTarget Task Performance\nFigure 6 shows the effect of dataset size on interme-\ndiate task training on downstream target task per-\nformance for ﬁve intermediate tasks, which were\npicked to maximize the variety of original interme-\ndiate task sizes and effectiveness in transfer learn-\ning abilities.\nCB\nCOPA\nWSC\nRTE\nMultiRC\nWiC\nBoolQ\nCSenseQA\nCosmosQA\nReCoRD\nEP-POS\nEP-NER\nEP-SRL\nEP-Coref\nEP-Const\nEP-SPR1\nEP-SPR2\nEP-DPR\nEP-Rel\nEP-UD\nSE-SentLen\nSE-WC\nSE-TreeDepth\nSE-TopConst\nSE-BShift\nSE-Tense\nSE-SubjNum\nSE-ObjNum\nSE-SOMO\nSE-CoordInv\nAJ-CoLA\nAJ-Wh\nAJ-Def\nAJ-Coord\nAJ-EOS\nCB\nCOPA\nWSC\nRTE\nMultiRC\nWiC\nBoolQ\nCSenseQA\nCosmosQA\nReCoRD\nEP-POS\nEP-NER\nEP-SRL\nEP-Coref\nEP-Const\nEP-SPR1\nEP-SPR2\nEP-DPR\nEP-Rel\nEP-UD\nSE-SentLen\nSE-WC\nSE-TreeDepth\nSE-TopConst\nSE-BShift\nSE-Tense\nSE-SubjNum\nSE-ObjNum\nSE-SOMO\nSE-CoordInv\nAJ-CoLA\nAJ-Wh\nAJ-Def\nAJ-Coord\nAJ-EOS\n1 .31 .37 .58 .73 -.04 .74 .72 .82 .69 .59 .50 .55 .71 .08 .70 .72 .66 .39 .74 .02 .49 .29 .12 .63 .24 .38 -.12 .75 .64 .71 .12 .03 -.04 .55\n.31 1 .25 .67 .57 .17 .52 .66 .55 .59 .02 .42 -.10 .46 .13 .31 .41 .40 .22 .53 -.26 .15 -.08 .05 .58 .36 .25 .07 .74 .48 .55 .03 -.13 -.34 .38\n.37 .25 1 .36 .37 .16 .40 .45 .39 .42 .40 .38 .26 .44 .01 .49 .51 .63 .34 .52 .03 .34 .19 .31 .54 .47 .42 .21 .45 .44 .43 .16 -.26 .03 .35\n.58 .67 .36 1 .86 .27 .83 .85 .68 .67 .31 .60 .34 .71 .25 .58 .66 .51 .32 .71 .09 .40 .01 .14 .59 .29 .25 -.04 .74 .80 .71 .15 -.14 -.22 .38\n.73 .57 .37 .86 1 .16 .79 .76 .67 .66 .27 .51 .40 .78 .40 .55 .74 .57 .28 .71 .07 .46 .10 .11 .56 .23 .26 -.02 .73 .79 .59 .01 -.07 -.15 .48\n-.04 .17 .16 .27 .16 1 .05 .10 -.05 .01 -.29 -.00 -.04 .02 .18 .04 .04 .14 -.09 .11 -.03 .05 -.10 .10 .15 -.21 -.09 .35 .15 .06 .03 -.07 -.31 -.11 .26\n.74 .52 .40 .83 .79 .05 1 .79 .80 .76 .47 .59 .46 .74 .17 .70 .69 .58 .29 .76 .04 .41 .05 .11 .68 .37 .28 -.13 .75 .82 .78 .15 -.05 -.16 .35\n.72 .66 .45 .85 .76 .10 .79 1 .85 .83 .40 .61 .36 .74 .06 .77 .68 .69 .41 .80 .06 .48 .07 .27 .72 .47 .39 -.15 .88 .76 .76 .20 -.18 -.19 .40\n.82 .55 .39 .68 .67 -.05 .80 .85 1 .86 .59 .63 .51 .70 -.03 .76 .66 .74 .45 .81 .01 .54 .14 .24 .84 .33 .36 -.15 .87 .80 .83 .27 .06 -.18 .52\n.69 .59 .42 .67 .66 .01 .76 .83 .86 1 .44 .66 .43 .71 .08 .77 .69 .73 .45 .84 .09 .46 .21 .31 .76 .39 .42 -.11 .83 .79 .71 .21 -.10 -.15 .50\n.59 .02 .40 .31 .27 -.29 .47 .40 .59 .44 1 .67 .56 .45 -.28 .55 .48 .43 .47 .50 .35 .48 .45 .20 .42 .28 .34 -.35 .40 .39 .57 .40 .24 .06 .22\n.50 .42 .38 .60 .51 -.00 .59 .61 .63 .66 .67 1 .58 .63 .05 .52 .49 .51 .51 .61 .34 .46 .47 .24 .47 .28 .37 -.20 .57 .57 .60 .35 .03 .02 .29\n.55 -.10 .26 .34 .40 -.04 .46 .36 .51 .43 .56 .58 1 .45 .13 .39 .49 .48 .36 .53 .41 .51 .39 .25 .36 -.13 .06 -.11 .28 .53 .36 .25 .13 .04 .19\n.71 .46 .44 .71 .78 .02 .74 .74 .70 .71 .45 .63 .45 1 .35 .70 .69 .66 .33 .59 .19 .57 .29 .27 .59 .37 .37 -.16 .70 .72 .54 .15 -.10 -.04 .57\n.08 .13 .01 .25 .40 .18 .17 .06 -.03 .08 -.28 .05 .13 .35 1 -.06 .10 .18 -.15 .03 -.18 .10 -.11 -.12 .01 -.11 -.09 -.01 -.01 .25 -.20 -.21 -.18 -.02 .17\n.70 .31 .49 .58 .55 .04 .70 .77 .76 .77 .55 .52 .39 .70 -.06 1 .61 .68 .44 .75 .17 .52 .27 .41 .61 .55 .54 -.16 .74 .70 .61 .43 -.16 -.04 .48\n.72 .41 .51 .66 .74 .04 .69 .68 .66 .69 .48 .49 .49 .69 .10 .61 1 .74 .45 .80 .21 .48 .27 .31 .62 .38 .30 .01 .69 .71 .56 .08 .01 .04 .41\n.66 .40 .63 .51 .57 .14 .58 .69 .74 .73 .43 .51 .48 .66 .18 .68 .74 1 .29 .70 -.01 .44 .20 .53 .76 .29 .30 .06 .74 .68 .53 .25 -.17 .04 .55\n.39 .22 .34 .32 .28 -.09 .29 .41 .45 .45 .47 .51 .36 .33 -.15 .44 .45 .29 1 .62 .36 .53 .48 .32 .35 .40 .65 -.26 .32 .41 .46 .32 .27 .14 .21\n.74 .53 .52 .71 .71 .11 .76 .80 .81 .84 .50 .61 .53 .59 .03 .75 .80 .70 .62 1 .11 .54 .27 .33 .74 .43 .45 -.07 .79 .75 .74 .23 -.07 -.15 .45\n.02 -.26 .03 .09 .07 -.03 .04 .06 .01 .09 .35 .34 .41 .19 -.18 .17 .21 -.01 .36 .11 1 .44 .58 .26 -.03 -.05 .04 -.09 -.03 .21 .06 .39 .37 .15 -.08\n.49 .15 .34 .40 .46 .05 .41 .48 .54 .46 .48 .46 .51 .57 .10 .52 .48 .44 .53 .54 .44 1 .38 .19 .49 .28 .41 -.22 .48 .51 .40 .42 .28 .02 .41\n.29 -.08 .19 .01 .10 -.10 .05 .07 .14 .21 .45 .47 .39 .29 -.11 .27 .27 .20 .48 .27 .58 .38 1 .39 .06 .02 .34 .08 .13 .07 .22 .43 .26 .24 .31\n.12 .05 .31 .14 .11 .10 .11 .27 .24 .31 .20 .24 .25 .27 -.12 .41 .31 .53 .32 .33 .26 .19 .39 1 .18 .12 .20 .05 .20 .21 .13 .34 -.17 .13 .22\n.63 .58 .54 .59 .56 .15 .68 .72 .84 .76 .42 .47 .36 .59 .01 .61 .62 .76 .35 .74 -.03 .49 .06 .18 1 .29 .26 -.01 .84 .74 .78 .28 .00 -.22 .59\n.24 .36 .47 .29 .23 -.21 .37 .47 .33 .39 .28 .28 -.13 .37 -.11 .55 .38 .29 .40 .43 -.05 .28 .02 .12 .29 1 .66 -.18 .43 .28 .25 .16 -.11 .12 .02\n.38 .25 .42 .25 .26 -.09 .28 .39 .36 .42 .34 .37 .06 .37 -.09 .54 .30 .30 .65 .45 .04 .41 .34 .20 .26 .66 1 -.04 .44 .33 .34 .39 .06 .18 .18\n-.12 .07 .21 -.04 -.02 .35 -.13 -.15 -.15 -.11 -.35 -.20 -.11 -.16 -.01 -.16 .01 .06 -.26 -.07 -.09 -.22 .08 .05 -.01 -.18 -.04 1 .05 .01 -.04 .01 -.12 -.01 .15\n.75 .74 .45 .74 .73 .15 .75 .88 .87 .83 .40 .57 .28 .70 -.01 .74 .69 .74 .32 .79 -.03 .48 .13 .20 .84 .43 .44 .05 1 .74 .77 .26 -.06 -.20 .57\n.64 .48 .44 .80 .79 .06 .82 .76 .80 .79 .39 .57 .53 .72 .25 .70 .71 .68 .41 .75 .21 .51 .07 .21 .74 .28 .33 .01 .74 1 .68 .32 .04 -.18 .43\n.71 .55 .43 .71 .59 .03 .78 .76 .83 .71 .57 .60 .36 .54 -.20 .61 .56 .53 .46 .74 .06 .40 .22 .13 .78 .25 .34 -.04 .77 .68 1 .34 .02 -.27 .42\n.12 .03 .16 .15 .01 -.07 .15 .20 .27 .21 .40 .35 .25 .15 -.21 .43 .08 .25 .32 .23 .39 .42 .43 .34 .28 .16 .39 .01 .26 .32 .34 1 .22 .23 .19\n.03 -.13 -.26 -.14 -.07 -.31 -.05 -.18 .06 -.10 .24 .03 .13 -.10 -.18 -.16 .01 -.17 .27 -.07 .37 .28 .26 -.17 .00 -.11 .06 -.12 -.06 .04 .02 .22 1 .28 -.03\n-.04 -.34 .03 -.22 -.15 -.11 -.16 -.19 -.18 -.15 .06 .02 .04 -.04 -.02 -.04 .04 .04 .14 -.15 .15 .02 .24 .13 -.22 .12 .18 -.01 -.20 -.18 -.27 .23 .28 1 .00\n.55 .38 .35 .38 .48 .26 .35 .40 .52 .50 .22 .29 .19 .57 .17 .48 .41 .55 .21 .45 -.08 .41 .31 .22 .59 .02 .18 .15 .57 .43 .42 .19 -.03 .00 1\nFigure 4: Correlations between probing and target task performances. Each cell contains the Spearman correlation\nbetween probing and target tasks performances across training on different intermediate tasks and random restarts.\nCB\nCOPA\nWSC\nRTE\nMultiRC\nWiC\nBoolQ\nCSenseQA\nCosmosQA\nReCoRD\nEP-POS\nEP-NER\nEP-SRL\nEP-Coref\nEP-Const\nEP-SPR1\nEP-SPR2\nEP-DPR\nEP-Rel\nEP-UD\nSE-SentLen\nSE-WC\nSE-TreeDepth\nSE-TopConst\nSE-BShift\nSE-Tense\nSE-SubjNum\nSE-ObjNum\nSE-SOMO\nSE-CoordInv\nAJ-CoLA\nAJ-Wh\nAJ-Def\nAJ-Coord\nAJ-EOS\nCB\nCOPA\nWSC\nRTE\nMultiRC\nWiC\nBoolQ\nCSenseQA\nCosmosQA\nReCoRD\nEP-POS\nEP-NER\nEP-SRL\nEP-Coref\nEP-Const\nEP-SPR1\nEP-SPR2\nEP-DPR\nEP-Rel\nEP-UD\nSE-SentLen\nSE-WC\nSE-TreeDepth\nSE-TopConst\nSE-BShift\nSE-Tense\nSE-SubjNum\nSE-ObjNum\nSE-SOMO\nSE-CoordInv\nAJ-CoLA\nAJ-Wh\nAJ-Def\nAJ-Coord\nAJ-EOS\n1 .31 .37 .58 .73 -.04 .74 .72 .82 .69 .59 .50 .55 .71 .08 .70 .72 .66 .39 .74 .02 .49 .29 .12 .63 .24 .38 -.12 .75 .64 .71 .12 .03 -.04 .55\n.31 1 .25 .67 .57 .17 .52 .66 .55 .59 .02 .42 -.10 .46 .13 .31 .41 .40 .22 .53 -.26 .15 -.08 .05 .58 .36 .25 .07 .74 .48 .55 .03 -.13 -.34 .38\n.37 .25 1 .36 .37 .16 .40 .45 .39 .42 .40 .38 .26 .44 .01 .49 .51 .63 .34 .52 .03 .34 .19 .31 .54 .47 .42 .21 .45 .44 .43 .16 -.26 .03 .35\n.58 .67 .36 1 .86 .27 .83 .85 .68 .67 .31 .60 .34 .71 .25 .58 .66 .51 .32 .71 .09 .40 .01 .14 .59 .29 .25 -.04 .74 .80 .71 .15 -.14 -.22 .38\n.73 .57 .37 .86 1 .16 .79 .76 .67 .66 .27 .51 .40 .78 .40 .55 .74 .57 .28 .71 .07 .46 .10 .11 .56 .23 .26 -.02 .73 .79 .59 .01 -.07 -.15 .48\n-.04 .17 .16 .27 .16 1 .05 .10 -.05 .01 -.29 -.00 -.04 .02 .18 .04 .04 .14 -.09 .11 -.03 .05 -.10 .10 .15 -.21 -.09 .35 .15 .06 .03 -.07 -.31 -.11 .26\n.74 .52 .40 .83 .79 .05 1 .79 .80 .76 .47 .59 .46 .74 .17 .70 .69 .58 .29 .76 .04 .41 .05 .11 .68 .37 .28 -.13 .75 .82 .78 .15 -.05 -.16 .35\n.72 .66 .45 .85 .76 .10 .79 1 .85 .83 .40 .61 .36 .74 .06 .77 .68 .69 .41 .80 .06 .48 .07 .27 .72 .47 .39 -.15 .88 .76 .76 .20 -.18 -.19 .40\n.82 .55 .39 .68 .67 -.05 .80 .85 1 .86 .59 .63 .51 .70 -.03 .76 .66 .74 .45 .81 .01 .54 .14 .24 .84 .33 .36 -.15 .87 .80 .83 .27 .06 -.18 .52\n.69 .59 .42 .67 .66 .01 .76 .83 .86 1 .44 .66 .43 .71 .08 .77 .69 .73 .45 .84 .09 .46 .21 .31 .76 .39 .42 -.11 .83 .79 .71 .21 -.10 -.15 .50\n.59 .02 .40 .31 .27 -.29 .47 .40 .59 .44 1 .67 .56 .45 -.28 .55 .48 .43 .47 .50 .35 .48 .45 .20 .42 .28 .34 -.35 .40 .39 .57 .40 .24 .06 .22\n.50 .42 .38 .60 .51 -.00 .59 .61 .63 .66 .67 1 .58 .63 .05 .52 .49 .51 .51 .61 .34 .46 .47 .24 .47 .28 .37 -.20 .57 .57 .60 .35 .03 .02 .29\n.55 -.10 .26 .34 .40 -.04 .46 .36 .51 .43 .56 .58 1 .45 .13 .39 .49 .48 .36 .53 .41 .51 .39 .25 .36 -.13 .06 -.11 .28 .53 .36 .25 .13 .04 .19\n.71 .46 .44 .71 .78 .02 .74 .74 .70 .71 .45 .63 .45 1 .35 .70 .69 .66 .33 .59 .19 .57 .29 .27 .59 .37 .37 -.16 .70 .72 .54 .15 -.10 -.04 .57\n.08 .13 .01 .25 .40 .18 .17 .06 -.03 .08 -.28 .05 .13 .35 1 -.06 .10 .18 -.15 .03 -.18 .10 -.11 -.12 .01 -.11 -.09 -.01 -.01 .25 -.20 -.21 -.18 -.02 .17\n.70 .31 .49 .58 .55 .04 .70 .77 .76 .77 .55 .52 .39 .70 -.06 1 .61 .68 .44 .75 .17 .52 .27 .41 .61 .55 .54 -.16 .74 .70 .61 .43 -.16 -.04 .48\n.72 .41 .51 .66 .74 .04 .69 .68 .66 .69 .48 .49 .49 .69 .10 .61 1 .74 .45 .80 .21 .48 .27 .31 .62 .38 .30 .01 .69 .71 .56 .08 .01 .04 .41\n.66 .40 .63 .51 .57 .14 .58 .69 .74 .73 .43 .51 .48 .66 .18 .68 .74 1 .29 .70 -.01 .44 .20 .53 .76 .29 .30 .06 .74 .68 .53 .25 -.17 .04 .55\n.39 .22 .34 .32 .28 -.09 .29 .41 .45 .45 .47 .51 .36 .33 -.15 .44 .45 .29 1 .62 .36 .53 .48 .32 .35 .40 .65 -.26 .32 .41 .46 .32 .27 .14 .21\n.74 .53 .52 .71 .71 .11 .76 .80 .81 .84 .50 .61 .53 .59 .03 .75 .80 .70 .62 1 .11 .54 .27 .33 .74 .43 .45 -.07 .79 .75 .74 .23 -.07 -.15 .45\n.02 -.26 .03 .09 .07 -.03 .04 .06 .01 .09 .35 .34 .41 .19 -.18 .17 .21 -.01 .36 .11 1 .44 .58 .26 -.03 -.05 .04 -.09 -.03 .21 .06 .39 .37 .15 -.08\n.49 .15 .34 .40 .46 .05 .41 .48 .54 .46 .48 .46 .51 .57 .10 .52 .48 .44 .53 .54 .44 1 .38 .19 .49 .28 .41 -.22 .48 .51 .40 .42 .28 .02 .41\n.29 -.08 .19 .01 .10 -.10 .05 .07 .14 .21 .45 .47 .39 .29 -.11 .27 .27 .20 .48 .27 .58 .38 1 .39 .06 .02 .34 .08 .13 .07 .22 .43 .26 .24 .31\n.12 .05 .31 .14 .11 .10 .11 .27 .24 .31 .20 .24 .25 .27 -.12 .41 .31 .53 .32 .33 .26 .19 .39 1 .18 .12 .20 .05 .20 .21 .13 .34 -.17 .13 .22\n.63 .58 .54 .59 .56 .15 .68 .72 .84 .76 .42 .47 .36 .59 .01 .61 .62 .76 .35 .74 -.03 .49 .06 .18 1 .29 .26 -.01 .84 .74 .78 .28 .00 -.22 .59\n.24 .36 .47 .29 .23 -.21 .37 .47 .33 .39 .28 .28 -.13 .37 -.11 .55 .38 .29 .40 .43 -.05 .28 .02 .12 .29 1 .66 -.18 .43 .28 .25 .16 -.11 .12 .02\n.38 .25 .42 .25 .26 -.09 .28 .39 .36 .42 .34 .37 .06 .37 -.09 .54 .30 .30 .65 .45 .04 .41 .34 .20 .26 .66 1 -.04 .44 .33 .34 .39 .06 .18 .18\n-.12 .07 .21 -.04 -.02 .35 -.13 -.15 -.15 -.11 -.35 -.20 -.11 -.16 -.01 -.16 .01 .06 -.26 -.07 -.09 -.22 .08 .05 -.01 -.18 -.04 1 .05 .01 -.04 .01 -.12 -.01 .15\n.75 .74 .45 .74 .73 .15 .75 .88 .87 .83 .40 .57 .28 .70 -.01 .74 .69 .74 .32 .79 -.03 .48 .13 .20 .84 .43 .44 .05 1 .74 .77 .26 -.06 -.20 .57\n.64 .48 .44 .80 .79 .06 .82 .76 .80 .79 .39 .57 .53 .72 .25 .70 .71 .68 .41 .75 .21 .51 .07 .21 .74 .28 .33 .01 .74 1 .68 .32 .04 -.18 .43\n.71 .55 .43 .71 .59 .03 .78 .76 .83 .71 .57 .60 .36 .54 -.20 .61 .56 .53 .46 .74 .06 .40 .22 .13 .78 .25 .34 -.04 .77 .68 1 .34 .02 -.27 .42\n.12 .03 .16 .15 .01 -.07 .15 .20 .27 .21 .40 .35 .25 .15 -.21 .43 .08 .25 .32 .23 .39 .42 .43 .34 .28 .16 .39 .01 .26 .32 .34 1 .22 .23 .19\n.03 -.13 -.26 -.14 -.07 -.31 -.05 -.18 .06 -.10 .24 .03 .13 -.10 -.18 -.16 .01 -.17 .27 -.07 .37 .28 .26 -.17 .00 -.11 .06 -.12 -.06 .04 .02 .22 1 .28 -.03\n-.04 -.34 .03 -.22 -.15 -.11 -.16 -.19 -.18 -.15 .06 .02 .04 -.04 -.02 -.04 .04 .04 .14 -.15 .15 .02 .24 .13 -.22 .12 .18 -.01 -.20 -.18 -.27 .23 .28 1 .00\n.55 .38 .35 .38 .48 .26 .35 .40 .52 .50 .22 .29 .19 .57 .17 .48 .41 .55 .21 .45 -.08 .41 .31 .22 .59 .02 .18 .15 .57 .43 .42 .19 -.03 .00 1\nFigure 5: Correlations between probing and target task performances. Each cell contains the Pearson correlation\nbetween probing and target tasks performances across training on different intermediate tasks and random restarts.\nFigure 6: Results of experiments on impact of intermediate task data size on downstream target task performance.\nFor each subﬁgure, we ﬁnetune RoBERTa over a variety of dataset size (sampled randomly from the dataset). We\nreport the macro-average of each target task’s performance metrics after ﬁnetuning on each dataset size split.",
  "topic": "Task (project management)",
  "concepts": [
    {
      "name": "Task (project management)",
      "score": 0.7810086011886597
    },
    {
      "name": "Computer science",
      "score": 0.7641549110412598
    },
    {
      "name": "Forgetting",
      "score": 0.7212991714477539
    },
    {
      "name": "Transfer of learning",
      "score": 0.6464798450469971
    },
    {
      "name": "Coreference",
      "score": 0.6098107695579529
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5621513724327087
    },
    {
      "name": "Natural language understanding",
      "score": 0.5120642781257629
    },
    {
      "name": "Inference",
      "score": 0.5061644911766052
    },
    {
      "name": "Machine learning",
      "score": 0.4485916495323181
    },
    {
      "name": "Task analysis",
      "score": 0.4350031316280365
    },
    {
      "name": "Transfer (computing)",
      "score": 0.4332708716392517
    },
    {
      "name": "Natural language processing",
      "score": 0.4042426347732544
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3423018455505371
    },
    {
      "name": "Natural language",
      "score": 0.3161601424217224
    },
    {
      "name": "Resolution (logic)",
      "score": 0.3110028803348541
    },
    {
      "name": "Psychology",
      "score": 0.14821335673332214
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Parallel computing",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 52
}