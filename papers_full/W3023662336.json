{
  "title": "Multi-scale Transformer Language Models",
  "url": "https://openalex.org/W3023662336",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2325938190",
      "name": "Subramanian, Sandeep",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222857666",
      "name": "Collobert, Ronan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4281674764",
      "name": "Ranzato, Marc'Aurelio",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222489650",
      "name": "Boureau, Y-Lan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2950670227",
    "https://openalex.org/W2103504761",
    "https://openalex.org/W2970900903",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2807747378",
    "https://openalex.org/W1994166112",
    "https://openalex.org/W2034676897",
    "https://openalex.org/W2036317923",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W2951740384",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2952592807",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2510842514",
    "https://openalex.org/W2963970792",
    "https://openalex.org/W2970528773",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963951265",
    "https://openalex.org/W2945886944",
    "https://openalex.org/W2952276042",
    "https://openalex.org/W2799184518",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2099257174",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2963684275",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W648143168",
    "https://openalex.org/W1810943226",
    "https://openalex.org/W2584032004",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W2970401203",
    "https://openalex.org/W179875071",
    "https://openalex.org/W1902237438"
  ],
  "abstract": "We investigate multi-scale transformer language models that learn representations of text at multiple scales, and present three different architectures that have an inductive bias to handle the hierarchical nature of language. Experiments on large-scale language modeling benchmarks empirically demonstrate favorable likelihood vs memory footprint trade-offs, e.g. we show that it is possible to train a hierarchical variant with 30 layers that has 23% smaller memory footprint and better perplexity, compared to a vanilla transformer with less than half the number of layers, on the Toronto BookCorpus. We analyze the advantages of learned representations at multiple scales in terms of memory footprint, compute time, and perplexity, which are particularly appealing given the quadratic scaling of transformers' run time and memory usage with respect to sequence length.",
  "full_text": "Multi-scale Transformer Language Models\nSandeep Subramanian 1 2 Ronan Collobert 1 Marc’Aurelio Ranzato1 Y-Lan Boureau1\nAbstract\nWe investigate multi-scale transformer language\nmodels that learn representations of text at mul-\ntiple scales, and present three different architec-\ntures that have an inductive bias to handle the\nhierarchical nature of language. Experiments on\nlarge-scale language modeling benchmarks empir-\nically demonstrate favorable likelihood vs mem-\nory footprint trade-offs, e.g. we show that it is pos-\nsible to train a hierarchical variant with 30 layers\nthat has 23% smaller memory footprint and bet-\nter perplexity, compared to a vanilla transformer\nwith less than half the number of layers, on the\nToronto BookCorpus. We analyze the advantages\nof learned representations at multiple scales in\nterms of memory footprint, compute time, and per-\nplexity, which are particularly appealing given the\nquadratic scaling of transformers’ run time and\nmemory usage with respect to sequence length.\n1. Introduction\nHuman language displays simultaneous organization at mul-\ntiple scales and granularities: topics are maintained over\nlong spans of text without controlling every single word,\nwhile grammatical correctness imposes strong local con-\nstraints without inﬂuencing word choice at a very long range.\nThe choice of a word is thus constrained by both local and\nlonger-range information. This multi-scale structure is rem-\niniscent of well-studied hierarchies in many domains of\nnatural perception. In vision, the resolution of receptive\nﬁelds decreases as one moves either away from the center\nof the retina, or higher in the hierarchy of visual cortex ar-\neas (from V1 to V2 to V4). This gives rise to phenomena\nsuch as crowding (Pelli & Tillman, 2008), where only some\nensemble statistic of a set of elements can be perceived, and\nthe existence of metamers, distinct visual stimuli that look\nthe same when seen at the periphery (Freeman & Simon-\ncelli, 2011; Wallis et al., 2019). Incorporating both local\n* Work done while SS was an intern at Facebook. 1Facebook\nAI Research 2Mila, Université de Montréal. Correspondence to:\nSandeep Subramanian <sandeep.subramanian.1@umontreal.ca>.\nPreprint. Work in progress.\nand global organizational constraints and compressing rep-\nresentations with statistical pooling have been suggested\nto better explain empirical perceptual vision phenomena\n(Wallis et al., 2019). Multi-resolution pyramids such as\nLaplacian pyramids have been shown to produce more con-\nvincing image generation (Denton et al., 2015) and efﬁcient\ncompression (Burt & Adelson, 1983). Combining represen-\ntations at multiple scales has also been successful for speech\nand audio generation (Mehri et al., 2016).\nLanguage modeling efforts however, typically rely on mod-\neling the multi-scale nature of language without strong ar-\nchitectural priors. Representations are learned only at the\nﬁnest scale, usually that of words or subwords, and rely\non the training objective of predicting the next word in the\nsequence to implicitly capture the need to maintain consis-\ntency and coherence across multiple scales. Some previ-\nous efforts have advocated for multi-scale sequence models\n(Koutnik et al., 2014; Chung et al., 2016; Mehri et al., 2016),\nbut these haven’t seen widespread adoption in language\nmodeling on current large-scale benchmarks. The potential\nof leveraging multiple scales of representation to reduce\nmemory footprint (see Section 3.5) is especially appeal-\ning for language modeling because transformers (Vaswani\net al., 2017), which are currently the most popular archi-\ntectures, suffer from quadratic memory usage scaling as\ncontext length increases.\nWe argue here that multi-scale transformer architectures\ncan lead to better and more efﬁcient generative models of\ntext. We design three such architectures and present em-\npirical evidence of their advantages, as well as analyses to\nbetter understand how transformer language models (LMs)\nuse available context, following work by Khandelwal et al.\n(2018); Sankar et al. (2019). Our work thus makes the fol-\nlowing contributions: (1) We present three different multi-\nscale transformer architectures for language modeling. (2)\nWe show that on some benchmarks, these models typically\nhave smaller memory footprint for the same performance.\nFor example, we show that it is possible to train a multi-\nscale variant with 30 layers that has 23% smaller memory\nfootprint and better perplexity, compared to a vanilla trans-\nformer with less than half the number of layers. (3) We show\nthat transformer LMs suffer only minor perplexity increases\nwhen only looking at the last 8 or 16 tokens instead of 512,\nand show that combining a short context of 8 tokens at the\narXiv:2005.00581v1  [cs.CL]  1 May 2020\nMulti-scale Transformer Language Models\nﬁnest granularity with longer contexts at a coarser scale has\nperplexity similar to that of a model that looks at the entire\nlonger context at the ﬁnest granularity.\n2. Related Work\nTransformer-based language models (Vaswani et al., 2017;\nRadford et al., 2018; Al-Rfou et al., 2019) have become the\nmodel of choice for most large-scale language modeling\nbenchmarks. Kaplan et al. (2020) report power-law scaling\nof transformer language models with model capacity and\ndata size. As models get bigger, the amount of computa-\ntional resources, especially memory, grows quickly. In the\nnext paragraph we review some recent efforts attempting to\naddress this issue.\nMemory-Efﬁcient Transformers. Sukhbaatar et al.\n(2019) present an adaptive attention mechanism that learns\nhow far back into the past each head in a transformer should\nlook, and if implemented efﬁciently with sparse matrix op-\nerations, can help save memory. Liu et al. (2018); Rae et al.\n(2019) present approaches that compress the transformer’s\nmemory with strided convolutions. Speciﬁcally, Liu et al.\n(2018) compress the keys and values in the multi-headed\nattention by a factor of 3 for long-text abstractive summariza-\ntion. Child et al. (2019) present sparse transformers along\nwith efﬁcient CUDA kernels for sparse attention demonstrat-\ning the ability to generate very long sequences. Rae et al.\n(2019) compress the recurrent memory for a transformer-\nXL (Dai et al., 2019), but ﬁnd that the best performing\nvariant is one that does not learn the compression function\nend-to-end. Liu & Lapata (2019) proposes a hierarchical\nextension of the architecture proposed in Liu et al. (2018)\nthat attends over very long sequences, with the aim to bet-\nter model paragraph- and document-level contexts. Kitaev\net al. (2019) describe the different factors that contribute to\nlarge memory footprints in vanilla transformers. They use\nreversible layers to remove the need to store activations at\nevery layer in the forward pass, LSH attention to decrease\nmemory requirements from O(N2) to O(Nlog N) where\nN is the sequence length, and they split activations in the\nfeedforward layers. Bai et al. (2019) present a way to train\ninﬁnite-depth weight-tied feedforward nets via root-ﬁnding,\nyielding a memory footprint that is constant with the depth\nof the network, but incurs computational overhead, simi-\nlar to reversible layers. Unfortunately, this computational\noverhead is typically at least 50% (Gomez et al., 2017).\nMulti-scale architectures for sequential data (Schmidhu-\nber, 1992; El Hihi & Bengio, 1996; Koutnik et al., 2014)\nhave attempted to exploit the multi-scale nature of data like\nlanguage, speech, music and audio. Such models seek to\nbuild representations of the input signal at multiple reso-\nlutions to better control the generative process by keeping\nrepresentations of the high-level structure in the signal (e.g.,\ntopic) invariant to ﬁne-resolution local changes (e.g., precise\nword choice or grammar), while simultaneously allowing\nﬂexibility at ﬁner resolutions to model these complex phe-\nnomena. They have seen success in modeling conversations\n(Sordoni et al., 2015), raw audio, speech and music (Mehri\net al., 2016), and language (Chung et al., 2016). Sordoni\net al. (2015) use the available hierarchical structure in con-\nversations such as utterance level boundaries to deﬁne a\nshallow hierarchy. Koutnik et al. (2014) and Mehri et al.\n(2016) use ﬁxed time scales, while Chung et al. (2016) try\nto learn the clocking function using the task-speciﬁc train-\ning objective in an end-to-end manner, with an RNN-based\narchitecture. Garg et al. (2019) present multi-resolution\ntransformers where resolution hierarchies are obtained from\nexplicit boundaries in the data such as sentences or para-\ngraphs. This however limits the model’s ability to capture\nhierarchies that aren’t explicit in the data, e.g. within a sin-\ngle sentence or across sentences within the same paragraph.\nTo the best of our knowledge, more generic multi-scale\narchitectures not relying on limited data-dependent bound-\naries haven’t been explored in conjunction with transformer\nlanguage models.\n3. Models\nWe ﬁrst brieﬂy review language modeling and transformers\n(Vaswani et al., 2017) which are the base for our archi-\ntectures. We then describe the three different multi-scale\narchitectures we explore for language modeling. See Fig-\nure 1 for a visual depiction of our proposed architectures\nand Listings 1, 2 and 3 in Appendix for a PyTorch-like\nimplementation of the forward pass through the models.\n3.1. Language Modeling & Transformer Preliminaries\nThe goal of language modeling is to estimate the joint prob-\nability of a sequence of words (or subwords, but we will\nrefer to “words” for simplicity). The models we consider\nin this work factorize the joint probability over words into\na product of conditional probabilities of each word given\neverything that precedes it. Conditional probabilities are\ntypically parameterized as recurrent neural networks like\nLSTMs (Graves, 2013; Mikolov et al., 2010) or feedfor-\nward models like gated convnets (Dauphin et al., 2017),\ntransformers (Vaswani et al., 2017; Radford et al., 2018), or\nestimated with non-parametric count-based statistics like in\nn-gram language models.\nTransformers have become ubiquitous for large-scale lan-\nguage modeling (Radford et al., 2018; Baevski & Auli, 2018;\nRadford et al., 2019; Dai et al., 2019; Kaplan et al., 2020).\nThey are self-attentive models that have stacks of residual\nblocks, each of which contains layers of multi-headed self-\nattention and feedforward modules. Multi-headed attention\nMulti-scale Transformer Language Models\nx 1 4 x 2 4 x 3 4 x 4 4 \nx 116 x 216 x 316x 1 - x 16\nCausal Transformer Layers (16x Hierarchy)\nh 116 h 216 h 316 \n......... \nCausal Transformer Layers (4x Hierarchy)\n............ \nCausal Transformer Layers (1x Hierarchy)\nConvolutional \nUpsampler \n(u) \nConvolutional \nUpsampler \nConvolutional \nUpsampler \nConvolutional \nUpsampler \nConvolutional \nUpsampler \nConvolutional \nUpsampler \n............ \n............ \n............ \n............. \nA vg \nPool \n(d) \nx 17 - x 32 A vg \nPool x 33 - x 48 A vg \nPool \nx 12 - x 15\nA vg \nPool \nx 24 - x 27\nA vg \nPool \nx 52 - x 55\nA vg \nPool \nx 20 x 47 x 50 x 69\nP(w 17|w <17) P(w 21|w <21) P(w 51 |w <51) \n......... ............. \n(a) Top-down Model\nAttention Aggregation Layer (v)\n............ \nx 11 x 21\nCausal Transformer Layers (4x Hierarchy)\nx 116\nh 14\nAverage Pool\nAverage Pool\nh 24 ............ \nx 11 x 21 x 31 x 41 ............ \nCausal Transformer Layers (16x Hierarchy)\nAverage Pool\n............ ............ ............ \nx 14 x 24\nh 116\nx 1agg x 2agg x 3agg x 4agg ............ \nCausal Transformer Layers T1  (1x Hierarchy)\nh 11 h 21 h 31 h 41 ............ \nP(w 5 |w < 5 ) P(w 3 |w < 3 ) \n............ \n............ \n(b) Bottom-up Model\n(c) Vanilla attention mask\n (d) Local attention mask\nß 1 ß 2 ß 4 (e) Retina attention mask\nFigure 1.Our proposed model architectures: (a) Top-down model that builds representations from coarse (yellow) to ﬁne (red) scales.\n(b) Bottom-up Model that does the opposite while aggregating representations from different scales before operating at the ﬁnest scale.\n(c,d,e) Retina Model that treats different parts of of the model’s context with different granularities - nearby information is ﬁne-grained,\nbut coarse in the distant past. We visualize this using the model’s attention mask. (c) is a standard autoregressive mask in vanilla\ntransformers, (d) a local attention mask (Parmar et al., 2018) that only looks at local information (e) our proposed retina variant with green\nand yellow indicating progressively coarser scales\nis a generalization of dot-product attention (Bahdanau et al.,\n2014; Luong et al., 2015) where a score is computed be-\ntween a query Qand key Kfor different learned projections\nof both. The scores are then normalized with a softmax and\nused as weights to compute a weighted average over values\nV at each position in the sequence. In language models,\nwe use self-attention where Q = K = V. The resulting\nrepresentations are then summed with the input to the resid-\nual block and then normalized using LayerNorm (Ba et al.,\n2016) followed by a feedforward layer, normalization, and\nresidual summing again. All operations in the model have\nthe advantage of being easily parallelizable on current hard-\nware, and the attention mechanism allows the model to learn\nlong-range dependencies.\nIn the rest of this section, we consider estimating the joint\nprobability of n words, factorized autoregressively. We\nuse dmodel and dff to refer to the dimensionality of the\nintermediate hidden states and the convolutional feedfor-\nward layer in the model, respectively. Words are ﬁrst rep-\nresented as embeddings x1 ...x n of dimension dmodel.Let\nk1 <...<k m,denote the mscales of a multi-scale model,\nwhere k1 = 1 is the ﬁnest scale that looks at every to-\nken, and scales ki incorporates information that has been\ndown-sampled by a factor ki through a down-sampling op-\nerator d (e.g., an average over windows of size ki or a\nstrided convolution). For example, a model with scales\n(k1,k2,k3) = (1 ,4,16) would use three scales with each\nscale being 4 times coarser than the preceding scale. Let\nMulti-scale Transformer Language Models\nxki denote the representation that is fed as input to vanilla\ntransformer blocks tki at scale ki (with possibly different\nnumbers of layers at each scale), and hki the representation\nat scale ki. We now detail how xki,hki are computed for\neach of the variants we propose.\n3.2. Top-down Model\nThe Top-down model we propose (Fig. 1a) is largely in-\nspired by SampleRNN (Mehri et al., 2016), a multi-scale\nrecurrent architecture for generating audio. We call this a\n“Top-down” model because it builds representations of the\nsequence progressively from coarser to ﬁner scales, by run-\nning multiple transformer layers at a particular coarse scale\nfollowed by convolutional upsampling to the subsequent\n(ﬁner) scale. Predictions are made at the ﬁnest scale.\nThe Top-down transformer uses downsampling operators d,\nthat take as input a sequence of vectors and a factor ki by\nwhich to downsample them, to compute a representation of\nthe input at scale ki of length of n/ki; in our experiments, d\nis average pooling with a kernel of size ki, or causal strided\nconvolutions.\nThe input to the coarsest scale in the model xkm, is simply\nthe pooled token embeddings over windows of size km. In-\nputs to the ﬁner-scale transformers thereafter are obtained\nby combining the pooled token embeddings at the corre-\nsponding scale with the upsampled representation from the\nimmediately coarser scale:\n¯xki = d(x1 ...x n,ki),m ≥i≥1,\nxkm = ¯ xkm,\nhki = tki(xki),m ≥i≥1,\nxki−1 = f(¯xki−1 ,u(hki,ki/ki−1)),m ≥i≥1,\nwhere f denotes a function that concatenates its inputs (of\nequal dimension), followed by a learned linear projection\nto half the dimension of the concatenated vector; uis an\nupsampling function such that u(hki,ki/ki−1) indicates\nthat representations hki are being upsampled by a factor\nki/ki−1. The model is trained to predict the next word\nin the sequence using representations h1. To make sure\nrepresentations aren’t informed by the future at any position,\nwe slice and shift the inputs to dappropriately (see Listing\n1 and Figure 1a).\nThe motivation behind such a model is to have early layers\nlearn high-level or coarse outlines of what the model should\nbe generating and have those representations be progres-\nsively upsampled to include ﬁner and ﬁner details. Vanilla\ntransformer language models by contrast, entangle the learn-\ning and representation of coarse and ﬁne details.\nA beneﬁt of this model over vanilla transformer LMs is that\nthey are typically much faster at inference. This is because\ntransformer layers at the coarser scales do not need to run at\nevery time step, but only once every ki steps for a particular\nscale. A Top-down model with 26 layers is about 30% faster\nthan a vanilla transformer with 14 layers (see Appendix\nTable 5).\n3.3. Bottom-up Model\nIn contrast to the Top-down model, the Bottom-up model\n(Fig. 1b) builds representations progressively from ﬁne to\ncoarse scales. Instead of upsampling operators, the architec-\nture uses an aggregation layer, denoted by v, to incorporate\ninformation from coarser scales at the word level. This is a\ntransformer layer where certain subsets of heads attend to\nrepresentations from different scales. Speciﬁcally, inputs\nto the multi-headed attention module within the layer are\nthe word embeddings themselves denoted by x1 ...x n, and\nthe keys and values are representations at different coarser\nscales denoted by hk2 ...h km; we denote this operation via\nv(hk2 ...h km,x1 ...x n). This aggregation layer is given as\ninput to the ﬁnest transformer blocks that make word-level\npredictions. The attention mask is constructed appropriately\nfor each subset of heads to prevent looking at the future. To\nsummarize:\nhk1 = h1 = x1 ...x n,\nxki = d(hki−1 ,ki/ki−1),2 ≤i≤m,\nhki = tki(xki),2 ≤i≤m,\nxagg = v(hk2 ...h km,x1 ...x n),\nhout = t1(xagg),\nwhere hout is the output from which word probabilities are\npredicted.\n3.4. Retina Model\nWe also experiment with an architecture that combines pro-\ngressively coarser representations for tokens further away\nin the past, in a way that is reminiscent of the progressively\nbigger receptive ﬁelds in the retina as one moves away from\nthe center. The underlying intuition is that a lot of the ﬁne-\ngrained information at the word level might be necessary in\na close range (e.g., for grammaticality), but not so much at\nlarger distances (see also the shufﬂing results in Section 5.3).\nUnlike the Top-down and Bottom-up models, this Retina\nmodel does not have separate transformer layers at every\nscale. The Retina model looks at each token with a different\ngranularity: tokens in the recent past are looked at with\nﬁner granularity and distant tokens at a coarser granularity.\nThis is a simple modiﬁcation to the multi-headed attention\nmodule of a transformer: namely, a different subset of heads\nlook at representations from different scales. The queries\nto the multi-headed attention always come from the repre-\nsentations at the ﬁnest 1x ( k1) scale, while the keys and\nvalues are selected appropriately based on each input query\nMulti-scale Transformer Language Models\nposition and the scale assigned to each head. This amounts\nto having local attention (Parmar et al., 2018) at the ﬁnest 1x\n(k1) scale (for the ﬁrst subset of heads), and memory com-\npressed attention (Schmidhuber, 1992; Liu et al., 2018; Rae\net al., 2019) with added sparsity for the remaining coarser\nscales. Note that context windows across the attention heads\nat different scales can have arbitrary boundaries for each\nscale (see Fig. 1e).\nFormally, consider the same setting as in the Top-down\nmodel (see Sec. 3.2), with scales k1,...,k m and token em-\nbeddings x1,...,x n, predicting token at position t.Down-\nsampled representations ¯xki are obtained at various scales\nthrough average pooling or strided convolutions, and then\nused as keys and values in a transformer. Each scale uses its\nown context window. These context windows are obtained\nby slicing the full-length context into non-overlapping win-\ndows of sizes roughly proportional to the ki scales, with the\ncoarser scales using larger, further away context windows.\nDenoting context window boundaries by βki and assuming\na ﬁxed scale ratio r= ki/ki−1,this means that βki roughly\ngrows geometrically as ri,and each scale takes up about\nthe same space for representation, since the context size in-\ncrease exactly compensates the higher downsampling ratio.\nOverall, the output representation hout is given by:\n¯xki = d(xt−βki ...x t−βki−1 −1,ki),m ≥i≥1,\nhout = t(¯xk1 ... ¯xkm).\nThis basic description can easily be extended to arbitrary\nnon-overlapping context window boundaries (see ablation\nin Section 5.3), while still maintaining the general design\nthat the context windows of coarser scales are bigger and\nfurther away from t.\n3.5. Space & Time Complexity for Transformer Layers\nTransformer language models typically require large\namounts of GPU memory to train. There are a few fac-\ntors that contribute to this.\n1. They require intermediate activations at every time step\nand layer to be stored in memory for fast backpropaga-\ntion. Memory footprint therefore scales linearly with\nthe number of layers and we’d often like to ﬁt as big a\nmodel as we can into memory (Kaplan et al., 2020).\n2. Inside a transformer layer, multi-headed attention with\nqueries, keys and values Q,K,V requires computing\na score for every pair of elements in the query and key\n(i.e.) the QKT matrix is of size N ×N (assuming a\nbatch size of 1) where N is the number of elements\nin the sequence. Both memory and time complexities\nare therefore quadratic in the length of the sequence\nO(N2), computation however is typically less of a\nproblem since it can be parallelized easily across N.\nWith short sequences, where the embedding dimension\nof the queries, keys, and values are greater than the\nsequence length, the (QKT)V matrix becomes expen-\nsive to store and compute. The overall complexity is\nO(N2 + NH), where H is the embedding dimension.\n3. Transformer layers also contain position-wise linear\nlayers that are about 4 times as wide as the model\nembedding dimension and therefore require 4 times as\nmuch memory to store activations. In typical language\nmodeling setups, where N ≈ H, these activations\ndominate memory footprint (see Appendix Table 8).\nOur multi-scale architecture addresses 1 and 2 by reducing\nthe number of positions over which transformer layers at\ncoarser scales need to operate on. Figure 2 shows memory\nfootprint in GB and time taken for a forward pass in mil-\nliseconds for a single transformer layer at different scales as\na function of the input sequence length. Each transformer\nlayer in this setting is identical to the ones used in our Book-\nCorpus and Wikitext-103 experiments. Layers at the 4x,\n16x and 64x scales are more efﬁcient in terms of time and\nmemory. Memory footprint was calculated analytically by\naccounting for the number of stored activations in the layer\nand time by implementing the layer in PyTorch.\nBy contrast, recurrent models have far fewer layers (typi-\ncally an order of magnitude smaller) and are trained with\nBPTT, which only requires storing hidden state across\nall time steps for efﬁcient backprop, making complexity\nO(NH).\n4. Datasets & Experimental Setup\n4.1. Datasets\nWe consider three word/sub-word level language modeling\nbenchmarks of different sizes. In order to make our results\ncomparable to previous published research, we use standard\ndatasets that have been used in prior language modeling\nefforts (Baevski & Auli, 2018; Radford et al., 2018; Lample\net al., 2019) - Wikitext-103 (Merity et al., 2016), BookCor-\npus (Zhu et al., 2015) & CC-news (Liu et al., 2019). We\nuse the standard train/validation/test splits for Wikitext-103\nand use random splits for the Bookcorpus and CC-news.\nDataset statistics are reported in Table 1. Following Baevski\n& Auli (2018), we BPE tokenize (Sennrich et al., 2015)\nWikitext-103 with 32k replacements, but report perplexities\nat the word level for comparison with previous work, by\nre-normalizing the average perplexity by the actual num-\nber of words in valid/test sets. We use 30k replacements\nfor the Bookcorpus and CC-news datasets, and report the\naverage sub-word level perplexities since all models being\nMulti-scale Transformer Language Models\nFigure 2. Memory footprint and run time vs sequence length and\nrun time for a single transformer LM layer at different scales\nDataset Size\n(GB)\nTrain\nTokens\nValid\nTokens\nTest\nTokens\nV ocab\nSize\nWikitext-103 544M 111M 231K 261K 33,346\nToronto BookCorpus 3.6G 832M 108M 106M 31,300\nCC-news 83G 16.6B 370M 370M 63,724\nTable 1.Dataset statistics\ncompared are trained with the same tokenization and BPE\npre-processing.\n4.2. Experimental Details\nAcross all datasets, our primary point of comparison is\nwith a vanilla transformer language model (Radford et al.,\n2018). For the Wikitext-103 and Bookcorpus datasets, we\nuse a model that has either 12, 14 or 16 layers, dmodel =\n768, 12 attention heads, dff = 3,072, dropout of 0.1 every-\nwhere including the attention scores and GeLU activations\n(Hendrycks & Gimpel, 2016) - a conﬁguration similar to\nthe smallest GPT-2 model (Radford et al., 2019). For the\nCC-News dataset, we increase dmodel to 1,024 and dff to\n4,096. We use a batch size of 256 randomly sampled chunks\nof 512 tokens. For our CC-News models, we use gradient\naccumulation to simulate a batch size of 256. At inference,\nwe consider the entire piece of text as one contiguous block\nand use a sliding window of size 512 tokens with a stride\nof 256 and make predictions only over the last 256 tokens\nin the window. This ensures that for every minibatch of\nexamples, the model has some context to work with.\nOur multi-scale architectures use the same conﬁguration\nabove for every transformer layer, with added upsampling\nModel Layers Test\nPPL\nTrain Mem\n(GB)\nVanilla 10 20.99 16.76\nVanilla 12 20.23 19.04\nVanilla 14 19.62 21.31\nTop-down 5,5,6,6 20.92 14.66\nTop-down 5,5,9,9 20.26 18.35\nTop-down 2,4,8,12 19.47 20.96\nTable 2.Model performance on CC-news.\nand downsampling modules. Our best-performing models\nuse average pooling for downsampling and we present an\nablation study with strided convolutions in Table 4. We used\na single transpose convolution layer with an appropriate\nkernel size and stride for upsampling. Each scale increases\nby a factor of 4 from the previous.\nAll models were trained with mixed precision arithmetic\nand optimized using Adam (Kingma & Ba, 2014) with the\nlearning rate increased linearly to 2.5 ×e−4 over 40,000\nwarmup steps and then annealed to 0 over a million steps\nusing a cosine schedule (Radford et al., 2018). We tuned\nthe dropout rates in our Wikitext-103 in the range of 0.1 to\n0.5 with increments of 0.05, since we found that our models\nwere able to overﬁt the data quite easily. For BookCorpus\nand CC-News, we only experimented with the number of\ntransformer layers at each hierarchy and the number of\nhierarchies (see Table 4).\n5. Results & Discussion\n5.1. Perplexity vs Memory Footprint trade-off\nAs Kaplan et al. (2020) note, transformer language model\nperplexities scale as a power-law with model capacity and\ndataset size. Given a big enough dataset, we would there-\nfore like to ﬁt as big a model as our hardware will allow.\nOur multi-scale architectures brings us closer to this goal -\ndemonstrating better perplexities for the same model mem-\nory footprint, as Figure 3 shows. As expected, perplexity\ndecreases with depth and memory usage, but the multi-scale\ntransformer achieves lower perplexity for the same mem-\nory usage. Gains are particularly stark when comparing to\nvanilla transformer LMs with fewer than 8 layers.\nWhile it is possible to manually estimate the amount of mem-\nory a model uses as shown in Appendix Table 8, here and in\nthe remainder of the section we use PyTorch functions1 to\nreport actual measurements.\nTables 2 and 3 report perplexities of different models with\ntheir associated memory footprint. In all cases, multi-scale\ntransformers achieve either the same perplexity as strong\n1torch.cuda.max_memory_allocated\nMulti-scale Transformer Language Models\nFigure 3. Test perplexity vs train Memory footprint for vanilla\ntransformer LMs and our Top-down (64x,16x,4x,1x) and Bottom-\nup (16x,4x,1x) multi-scale variants on the Bookcorpus test set.\nNumbers next to each point indicates the number of layers per\nscale.\nModel Layers Test\nPPL\nMem\n(GB)\nPrevious Work\nVanilla (Welleck et al., 2019) 16 25.6 -\nTransformer-XL (Dai et al., 2019) 16 24.3 -\nDEQ-Transformer (Bai et al., 2019) - 24.2 -\nDEQ-Transformer (Adaptive) (Bai et al., 2019) - 23.2 -\nAdaptive Inputs (Baevski & Auli, 2018) 16 18.7 -\nOur Models\nVanilla 16 25.9 26.85\nTop-down (4x, 1x) 6,12 25.6 25.15\nTop-down (4x, 1x) 5,10 26.1 22.08\nBottom-up (4x, 1x) 5,10 26.8 21.83\nRetina (16x, 4x, 1x) 16 26.6 29.92\nTable 3.Model performance on Wikitext-103.\nbaselines, while using less memory; or for the same memory,\nthey achieve lower perplexity. On Wikitext-103, multi-scale\ntransformers overﬁt the data much faster than vanilla trans-\nformers, since they have a lot more capacity for the same\nmemory footprint. In the future, we plan to investigate\nbetter regularization methods to further improve general-\nization. In Appendix Figures 5 and 6, we further analyze\nwhich tokens are responsible for the overall improved per-\nformance similarly to Baevski & Auli (2018), and we found\nthat the multi-scale transformer outperforms the baseline by\nmodeling rare tokens better.\nFurther, in Appendix Table 6, we report nearest neighbors in\nthe wikitext-103 dataset using representations produced by\nthe model at different scales, ﬁnding that retrieved chunks\nare often very similar in topic. In Appendix Table 7, we\nalso report example completions from a model trained only\nFigure 4. Increase in NLL for a trained model on the Wikitext-\n103 and BookCorpus datasets when word order is destroyed by\nshufﬂing its context, as a function of the distance from the shufﬂed\ncontext.\nat different coarse scales (see Appendix Section 6.2 for\ndetails).\n5.2. Analyzing Transformer LM behavior to context\nperturbations\nKhandelwal et al. (2018) and Sankar et al. (2019) analyzed\nhow LSTM language models and dialog systems use context,\nby perturbing it with different kinds of noise and observing\nchanges in the likelihood assigned by the model to every\nsubsequent (unperturbed) token in the sequence. Khandel-\nwal et al. (2019) show that perturbations which destroy word\norder, like shufﬂing, affects the likelihood that the model\nassigns to words near the shufﬂed context, but not far away.\nWe observe similar patterns for transformer language mod-\nels on the Wikitext-103 and Bookcorpus datasets, as shown\nin Figure 4. In particular, we consider a sequence of 512\ntokens, shufﬂe the ﬁrst 256 tokens and observe the likeli-\nhood that an already trained model (without perturbations)\nassigns to the subsequent 256 tokens. When the shufﬂed\ncontext is more than 50 tokens away, the change in like-\nlihood, compared to when the model is presented with a\ncompletely unshufﬂed context is already minimal, indicat-\ning that the model may not be using word order beyond this\ndistance. This observation might partly explain why our\nRetina model, which only looks at ﬁne-grained context in\na local way while using coarser representations for distant\ncontext, can perform similarly as a model that uses the entire\ncontext at a ﬁne-grained scale (see results in Table 4).\nMulti-scale Transformer Language Models\n5.3. Ablations\nIn Table 4 we extensively investigate how the main design\nchoices of multi-scale tranformers, such as the number of\nscales and number of layers at each scale, impact perplexity.\nFirst, we observe that adding a coarser scale to the top-down\nmulti-scale transformer helps when moving from (4,1) to\n(16,4,1), as these have the same memory footprint but the\nlatter model has lower perplexity. However, we do not\nobserve gains when adding an additional coarser scale with\nthe (64,16,4,1) transformer.\nSecond, we ﬁnd that the best layer allocation places more\nlayers at the ﬁner scales. Therefore, the designer of the\narchitecture has to strike a trade-off between minimizing\nperplexity and limiting memory consumption since the ﬁnest\nscale is responsible for the bulk of memory usage.\nThird, one may wonder whether improved perplexity re-\nported by multi-scale models is merely due to the depth\nincrease of the overall architecture. We thus trained skinnier\nbut deeper vanilla transformers with dmodel set to 256 and\n512 and dff set to 1024, 2048, and found that they perform\nworse. Therefore, the gains are not entirely due to depth\nincrease.\nFourth, in the previous section we have already discussed\nhow transformers are not so sensitive to the order of the\ntokens in their farther context, motivating the use of the\nRetina variant of the multi-scale architecture. Here we show\na) that the performance of the baseline model only mildly\ndeteriorates when reducing the size of the context below 64\ntokens (in fact, only 3 points are lost when using a context\nsize of 8 tokens), and b) that this property can be leveraged\nby the Retina version of our model.\nIn particular, the “attention context window” column shows\nthe context size in the multi-headed attention of the retina\nmodel. For example, \"0:512\" indicates that a token at posi-\ntion tand can look at tokens tto t−512 and \"0:8, 8:256,\n256:512\" indicates that the attention heads at the 1x scale\ncan look at tokens t to t−8, t−8 to t−256 at the 4x\nscale, and t−256 to t−512 at the 16x scale. Using this\nconﬁguration and for the same memory footprint, our Retina\nmodel is able to improve over a model that looks at all 512\nwords at the 1x scale.\nFinally, we experimented with three different downsam-\npling modules - average pooling, max-pooling and strided\nconvolutions. We found average pooling to be better than\nconvolutions and max-pooling to be quite unstable during\ntraining.\n5.4. Sample based evaluation\nA hallmark of the recent progress in language modeling\nhas been improvements not only in perplexity, but sample\nquality as well. While evaluating generative models of text\npurely based on their samples is extremely difﬁcult and\nan on ongoing effort (Cífka et al., 2018; Semeniuta et al.,\n2018), we would like to ensure that we aren’t losing out\non sample quality with respect to vanilla transformer LMs.\nTable 5 in the Appendix shows no signiﬁcant difference in\nterms of generation quality between multiscale models and\nbaseline transformers according to several common metrics;\nsee Table 9 in Appendix for some qualitative comparisons.\nModel Scales\nAttention\nContext\nWindow\nLayers Test\nPPL\nMem\n(GB)\nAblation for number of scales\nVanilla 1 0:512 12 16.95 20.98\nVanilla 1 0:512 14 16.54 23.78\nTop-down 4,1 0:512 8,12 15.87 25.45\nTop-down 16,4,1 0:512 4,8,12 15.76 25.23\nTop-down 64,16,4,1 0:512 2,4,8,12 16.00 22.62\nAblation for capacity at different scales\nTop-down 64,16,4,1 0:512 16,12,6,1 22.14 10.61\nTop-down 64,16,4,1 0:512 10,10,8,5 17.89 13.59\nTop-down 64,16,4,1 0:512 8,8,7,7 17.16 16.87\nTop-down 64,16,4,1 0:512 7,7,8,8 16.86 18.22\nTop-down 64,16,4,1 0:512 5,5,9,9 16.57 19.47\nTop-down 64,16,4,1 0:512 3,3,10,10 16.31 20.71\nTop-down 64,16,4,1 0:512 2,2,11,11 16.12 22.05\nTop-down 64,16,4,1 0:512 1,1,12,12 15.95 23.40\nAblation for downsampler\nTop-down\n(Avg-Pool) 64,16,4,1 0:512 7,7,8,8 16.86 18.22\nTop-down\n(Conv) 64,16,4,1 0:512 7,7,8,8 17.95 18.45\nAblation for deep & narrow networks\nVanilla\n(256/1024) 1 0:512 30 22.15 22.07\nVanilla\n(512/2048) 1 0:512 20 17.70 22.77\nAblation for attention context window\nVanilla 1 0:512 12 16.95 23.78\nVanilla 1 0:256 12 17.00 23.78\nVanilla 1 0:128 12 17.39 23.78\nVanilla 1 0:64 12 17.88 23.78\nVanilla 1 0:16 12 19.01 23.78\nVanilla 1 0:8 12 20.19 23.78\nRetina 16,4,1 0:8,8:256,\n256:512\n12 16.81 23.95\nRetina 16,4,1 0:16,16:256,\n256:512\n12 17.07 23.95\nRetina 16,4,1 0:128,64:256,\n128:512\n12 17.08 23.95\nTable 4.Model ablations for the number of scales, number of trans-\nformer layers per scale, type of downsampling function, skinny\nand deep networks and different local attention/retina attention\nmasks. All reported results are on our BookCorpus test set. The\nattention context window column indicates what attention heads at\neach scale look at - for example, \"0:512\" implies all scales see the\nentire history while \"0:8,8:256,256:512\" indicates that the ﬁnest\nscale sees only the previous 8 tokens, the subsequent scale from\n8-256 and so on.\n6. Conclusion\nWe propose three multi-scale transformer architectures for\nlanguage modeling and show that they achieve competitive\nor better perplexities compared to vanilla transformer lan-\nMulti-scale Transformer Language Models\nguage models for the same model memory footprint across\nthree different language modeling benchmarks.\nThese models leverage the robustness of transformers to\nword ordering in distant context windows. In these architec-\ntures, the representation produced by coarser scales which\noperate on much shorter sequences, is combined with the\nrepresentation of the ﬁnest scale, thereby reducing the over-\nall computational and memory cost.\nFuture work will explore how to combine our multi-scale\narchitectures with adaptive attention head spans and how dif-\nferent types of information (e.g., topic, grammatical correct-\nness) are differently affected by architecture scale choices.\nReferences\nAl-Rfou, R., Choe, D., Constant, N., Guo, M., and Jones,\nL. Character-level language modeling with deeper self-\nattention. In Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, volume 33, pp. 3159–3166, 2019.\nBa, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.\narXiv preprint arXiv:1607.06450, 2016.\nBaevski, A. and Auli, M. Adaptive input representa-\ntions for neural language modeling. arXiv preprint\narXiv:1809.10853, 2018.\nBahdanau, D., Cho, K., and Bengio, Y . Neural machine\ntranslation by jointly learning to align and translate.arXiv\npreprint arXiv:1409.0473, 2014.\nBai, S., Kolter, J. Z., and Koltun, V . Deep equilibrium\nmodels. In Advances in Neural Information Processing\nSystems, pp. 688–699, 2019.\nBurt, P. and Adelson, E. The laplacian pyramid as a compact\nimage code. IEEE Transactions on communications, 31\n(4):532–540, 1983.\nChild, R., Gray, S., Radford, A., and Sutskever, I. Gen-\nerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019.\nChung, J., Ahn, S., and Bengio, Y . Hierarchical mul-\ntiscale recurrent neural networks. arXiv preprint\narXiv:1609.01704, 2016.\nCífka, O., Severyn, A., Alfonseca, E., and Filippova, K. Eval\nall, trust a few, do wrong to none: Comparing sentence\ngeneration models. arXiv preprint arXiv:1804.07972 ,\n2018.\nDai, Z., Yang, Z., Yang, Y ., Carbonell, J., Le, Q. V ., and\nSalakhutdinov, R. Transformer-xl: Attentive language\nmodels beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860, 2019.\nDauphin, Y . N., Fan, A., Auli, M., and Grangier, D. Lan-\nguage modeling with gated convolutional networks. In\nProceedings of the 34th International Conference on Ma-\nchine Learning-Volume 70 , pp. 933–941. JMLR. org,\n2017.\nDenton, E. L., Chintala, S., Fergus, R., et al. Deep generative\nimage models using a laplacian pyramid of adversarial\nnetworks. In Advances in neural information processing\nsystems, pp. 1486–1494, 2015.\nEl Hihi, S. and Bengio, Y . Hierarchical recurrent neural net-\nworks for long-term dependencies. In Advances in neural\ninformation processing systems, pp. 493–499, 1996.\nFan, A., Lewis, M., and Dauphin, Y . Hierarchical neural\nstory generation. arXiv preprint arXiv:1805.04833, 2018.\nFreeman, J. and Simoncelli, E. P. Metamers of the ventral\nstream. Nature neuroscience, 14(9):1195, 2011.\nGarg, V . K., Dhillon, I. S., and Yu, H.-F. Multireso-\nlution transformer networks: Recurrence is not essen-\ntial for modeling hierarchical structure. arXiv preprint\narXiv:1908.10408, 2019.\nGomez, A. N., Ren, M., Urtasun, R., and Grosse, R. B. The\nreversible residual network: Backpropagation without\nstoring activations. In Advances in neural information\nprocessing systems, pp. 2214–2224, 2017.\nGraves, A. Generating sequences with recurrent neural\nnetworks. arXiv preprint arXiv:1308.0850, 2013.\nHeaﬁeld, K. Kenlm: Faster and smaller language model\nqueries. In Proceedings of the sixth workshop on statis-\ntical machine translation, pp. 187–197. Association for\nComputational Linguistics, 2011.\nHendrycks, D. and Gimpel, K. Gaussian error linear units\n(gelus). arXiv preprint arXiv:1606.08415, 2016.\nKaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,\nChess, B., Child, R., Gray, S., Radford, A., Wu, J., and\nAmodei, D. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\nKhandelwal, U., He, H., Qi, P., and Jurafsky, D. Sharp\nnearby, fuzzy far away: How neural language models use\ncontext. arXiv preprint arXiv:1805.04623, 2018.\nKhandelwal, U., Clark, K., Jurafsky, D., and Kaiser, L.\nSample efﬁcient text summarization using a single pre-\ntrained transformer. In arXiv:1905.08836v1 [cs.CL] 21\nMay 2019, 2019.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nMulti-scale Transformer Language Models\nKitaev, N., Kaiser, L., and Levskaya, A. Reformer: The\nefﬁcient transformer. In International Conference on\nLearning Representations, 2019.\nKoutnik, J., Greff, K., Gomez, F., and Schmidhuber, J. A\nclockwork rnn. arXiv preprint arXiv:1402.3511, 2014.\nLample, G., Sablayrolles, A., Ranzato, M., Denoyer, L., and\nJégou, H. Large memory layers with product keys. In\nAdvances in Neural Information Processing Systems, pp.\n8546–8557, 2019.\nLiu, P. J., Saleh, M., Pot, E., Goodrich, B., Sepa-\nssi, R., Kaiser, L., and Shazeer, N. Generating\nwikipedia by summarizing long sequences.arXiv preprint\narXiv:1801.10198, 2018.\nLiu, Y . and Lapata, M. Hierarchical transformers\nfor multi-document summarization. arXiv preprint\narXiv:1905.13164, 2019.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V .\nRoberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692, 2019.\nLuong, M.-T., Pham, H., and Manning, C. D. Effective\napproaches to attention-based neural machine translation.\narXiv preprint arXiv:1508.04025, 2015.\nMehri, S., Kumar, K., Gulrajani, I., Kumar, R., Jain, S.,\nSotelo, J., Courville, A., and Bengio, Y . Samplernn: An\nunconditional end-to-end neural audio generation model.\narXiv preprint arXiv:1612.07837, 2016.\nMerity, S., Xiong, C., Bradbury, J., and Socher, R.\nPointer sentinel mixture models. arXiv preprint\narXiv:1609.07843, 2016.\nMikolov, T., Karaﬁát, M., Burget, L., ˇCernock`y, J., and\nKhudanpur, S. Recurrent neural network based language\nmodel. In Eleventh annual conference of the international\nspeech communication association, 2010.\nParmar, N., Vaswani, A., Uszkoreit, J., Kaiser, Ł., Shazeer,\nN., Ku, A., and Tran, D. Image transformer. arXiv\npreprint arXiv:1802.05751, 2018.\nPelli, D. G. and Tillman, K. A. The uncrowded window of\nobject recognition. Nature neuroscience, 11(10):1129,\n2008.\nRadford, A., Narasimhan, K., Salimans, T.,\nand Sutskever, I. Improving language un-\nderstanding by generative pre-training. URL\nhttps://s3-us-west-2. amazonaws. com/openai-\nassets/researchcovers/languageunsupervised/language\nunderstanding paper. pdf, 2018.\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., and\nSutskever, I. Language models are unsupervised multitask\nlearners. 2019.\nRae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap,\nT. P. Compressive transformers for long-range sequence\nmodelling. arXiv preprint arXiv:1911.05507, 2019.\nSankar, C., Subramanian, S., Pal, C., Chandar, S., and Ben-\ngio, Y . Do neural dialog systems use the conversation\nhistory effectively? an empirical study. arXiv preprint\narXiv:1906.01603, 2019.\nSchmidhuber, J. Learning complex, extended sequences\nusing the principle of history compression. Neural Com-\nputation, 4(2):234–242, 1992.\nSemeniuta, S., Severyn, A., and Gelly, S. On accurate\nevaluation of gans for language generation.arXiv preprint\narXiv:1806.04936, 2018.\nSennrich, R., Haddow, B., and Birch, A. Neural machine\ntranslation of rare words with subword units. arXiv\npreprint arXiv:1508.07909, 2015.\nSordoni, A., Bengio, Y ., Vahabi, H., Lioma, C., Grue Simon-\nsen, J., and Nie, J.-Y . A hierarchical recurrent encoder-\ndecoder for generative context-aware query suggestion.\nIn Proceedings of the 24th ACM International on Con-\nference on Information and Knowledge Management, pp.\n553–562, 2015.\nSukhbaatar, S., Grave, E., Bojanowski, P., and Joulin, A.\nAdaptive attention span in transformers. arXiv preprint\narXiv:1905.07799, 2019.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,\nL., Gomez, A. N., Kaiser, L., and Polosukhin, I. Atten-\ntion is all you need. In Advances in neural information\nprocessing systems, 2017.\nWallis, T. S., Funke, C. M., Ecker, A. S., Gatys, L. A.,\nWichmann, F. A., and Bethge, M. Image content is more\nimportant than bouma’s law for scene metamers.ELife,\n8:e42512, 2019.\nWelleck, S., Kulikov, I., Roller, S., Dinan, E., Cho, K.,\nand Weston, J. Neural text generation with unlikelihood\ntraining. arXiv preprint arXiv:1908.04319, 2019.\nZhu, Y ., Kiros, R., Zemel, R., Salakhutdinov, R., Urta-\nsun, R., Torralba, A., and Fidler, S. Aligning books and\nmovies: Towards story-like visual explanations by watch-\ning movies and reading books. In Proceedings of the\nIEEE international conference on computer vision , pp.\n19–27, 2015.\nMulti-scale Transformer Language Models\nAppendix\n6.1. NLL vs Word Frequency\nIn tables 5 and 6, we break down the token-level perplexities\nbased on their frequency of occurrence in the training set,\nsimilar to (Baevski & Auli, 2018). We bin tokens into 5 or\n10 equally sized bins, based on cumulative frequency and\nreport the average NLL within each bin, with bins sorted\nfrom those that contain least to most frequent words. We\nﬁnd that the performance gap between models with smaller\nand larger context window sizes stems from modeling of\nlow-frequency tokens.\nFigure 5. Test NLL vs word frequency on the Wikitext-103 dataset.\nLeft to right: rarest to most frequent word bins.\n6.2. Coarse-only Transformer LMs\nWe explore the possibility of training a model that makes\npredictions directly at a particular scale. For example, if\nwe consider a scale of 4, we initially downsample the input\nby a factor of 4 by averaging the embeddings of every non-\noverlapping 4-gram chunk. We then run transformer layers\non this input and use the ﬁnal representations at each step to\npredict the next 4-gram chunk. We train the model to make\nthese predictions by minimizing the cross-entropy between\nthe model’s output distribution and a uniform distribution\nover the subsequent 4-gram.\nTable 7 presents qualitative results showing model predic-\ntions for the following. We experimented with adding this\ncriterion of being able to predict the subsequent bag of\nwords distribution in our top-down model at all scales, but\ndidn’t observe any improvements. This experiment mostly\nserved as a sanity check to ensure that the representations\nlearned at different scales can be useful. To ensure that\nFigure 6. Test NLL vs word frequency on the BookCorpus dataset.\nLeft to right: rarest to most frequent word bins.\nrepresentations at different scales learn meaningful repre-\nsentations, we qualitatively analyze them by looking at near-\nest neighbors computed using them in Table 6. We found\nthat nearest neighbors computed on Wikitext-103 typically\nreturned segments within highly related topics.\n6.3. Sample-Level Evaluation Metrics\nA hallmark of the recent progress in language modeling\nhas been improvements not only in perplexity, but sample\nquality as well. While evaluating generative models of text\npurely based on their samples is extremely difﬁcult and an\nongoing effort (Cífka et al., 2018; Semeniuta et al., 2018),\nwe would like to ensure that we aren’t losing out on sample\nquality with respect to vanilla transformer LMs.\nWe therefore borrow a few sample-level evaluation met-\nrics typically used to evaluate GAN-based text generation\nmethods to compare samples from our model with vanilla\ntransformer LMs. In all sample-based evaluation setups, we\nprovide models with 64 tokens of context and generate se-\nquences of length 256 with topk sampling (Fan et al., 2018)\nand a temperature of 0.7.\nN-gram and GPT-2 PPL - We compute the likelihood\nof model generated samples under a pre-trained kneser-ney\nsmoothed 4-gram language model (Heaﬁeld, 2011) and a\npre-trained GPT-2 345M parameter model (Radford et al.,\n2019).\nRef BLEU - Given an a particular input context, we gen-\nerate 3 distinct completions and use these as references to\nMulti-scale Transformer Language Models\ncompute BLEU wrt the ground truth completion.\nN-gram repeats - Following Welleck et al. (2019), we\nalso estimate the fraction of 1,2,3 and 4-grams that the\nmodel repeats.\nQuantitative results for different models are presented in\nTable 5, with qualitative samples in Table 9. We could not\nobserve any signiﬁcant difference in sample quality between\nthe different models. We also report the time it takes for\neach model to generate a sequence of 256 tokens given a\ncontext of 64 tokens.\nModel Layers N-gram\nPPL\nGPT-2\nPPL\nRef\nBLEU\nN-gram\nRepeat\nTime\n(s/seq)\nVanilla 14 22.81 7.30 7.45 0.75/0.45/0.29/0.20 1.35\nTop-down 2,4,8,12 21.17 7.69 6.80 0.73/0.41/0.26/0.16 0.93\nBottom-up 8,9,9 24.54 8.53 7.04 0.75/0.45/0.28/0.18 1.36\nRetina 12 20.91 8.64 7.37 0.73/0.42/0.26/0.17 1.72\nTable 5.Sample based evaluation of models on the BookCorpus.\nThe N-gram repeat column reports 1/2/3/4-gram repeat fractions\nwith a within of 256 tokens\nMulti-scale Transformer Language Models\nReference Text :\ngood on Megadeth ’s claim to being the world ’s state @-@ of @-@ the @-@ art speed metal band \" . Musicologist Glenn\nPillsbury stated the guitar work on the album was a mixture of Mustaine ’s \" controlled chaos \" and the \" technical brilliance \" of\nMarty Friedman . Studio efforts released in the mid- and late 1990s featured songs with compact structures and less complicated\nrifﬁng . Megadeth ’s lyrics often focus on death , war , politics , and religion . The lyricism centers on nihilistic themes , but\noccasionally deals with topics such as alienation and social problems . The earliest releases featured themes such as occultism ,\ngraphic violence\nNearest Neighbors at the 4x Scale\nvocalist Jock Cheese and keyboardist / vocalist Eugene de la Hot Croix Bun , and enjoyed a large underground / independent\nfollowing . Their third album , Machiavelli and the Four Seasons , reached the Australian national top 10 in 1995 . TISM were\nknown for their hybrid of dance music and rock ’n’roll , high @-@ energy live shows and humorous lyrics . TISM ’s songs\nfrequently satirised modern culture , celebrities and the entertainment industry , classic literature and art , current affairs , politics\nand sport . The titles of their songs were often wordplays created by juxtaposing pop culture references with more intellectual\nones ( for\nn ; 1950 ’s blues artists Guitar Slim , Johnny Watson , and B.B. King ; R & B and doo @-@ wop groups ( particularly local\n<unk> groups ) ; and modern jazz . His own heterogeneous ethnic background , and the diverse social and cultural mix in\nand around greater Los Angeles , were crucial in the formation of Zappa as a practitioner of underground music and of his\nlater distrustful and openly critical attitude towards \" mainstream \" social , political and musical movements . He frequently\nlampooned musical fads like psychedelia , rock opera and disco . Television also exerted a strong inﬂuence , as demonstrated by\nquotations from show\nmagazine , \" I am a young adult now , and I think this album shows my growth vocally . \" Aaliyah was mastered by Bernie\nGrundman at his studio in Los Angeles . = = Music and lyrics = = An R & B and neo soul album , Aaliyah featured midtempo\nfunk songs , hip hop @-@ textured uptempo tracks , and slow jams that draw on older soul inﬂuences . Along with contemporary\nurban sounds , its music incorporated Middle @-@ Eastern inﬂuences , muted alternative rock , and , particularly on Timbaland\n’s songs for the album , Latin timbres . \" Never No More \" mixed both older soul and modern hip hop sounds with\nNearest Neighbors at the 16x Scale\nwell received by both critics and fans , and was responsible for bringing Slayer to the attention of a mainstream metal audience .\nKerrang ! magazine described the record as \" the heaviest album of all \" . Alongside Anthrax ’s Among the Living , Megadeth ’s\nPeace Sells ... but Who ’s Buying ? and Metallica ’s Master of Puppets , Reign in Blood helped deﬁne the sound of the emerging\nUS thrash metal scene in the mid @-@ 1980s , and has remained inﬂuential subsequently . Reign in Blood ’s release was delayed\nbecause of concerns regarding its graphic artwork and lyrical subject matter . The opening track , \" Angel of Death \" , which\nrefers to Josef\nmusic industry in 1992 , through his vocal contributions on Dr. Dre ’s The Chronic . That album is considered to have \"\ntransformed the entire sound of West Coast rap \" by its development of what later became known as the \" G @-@ funk \" sound .\nThe Chronic expanded gangsta rap with profanity , anti @-@ authoritarian lyrics and multi @-@ layered samples taken from\n1970 ’s P @-@ Funk records . Snoop Dogg contributed vocals to Dre ’s solo single , \" Deep Cover \" , which led to a high degree\nof anticipation among hip hop for the release of his own solo album . Doggystyle and The Chronic are associated with\nthe most innovative popular musicians in America if not the world \" but also \" the most politically ambitious . Not even in the\nheyday of [ the ] Clash has any group come so close to the elusive and perhaps ridiculous ’ 60s rock ideal of raising political\nconsciousness with music . \" Their music on the album inspired leftist and Afrocentric ideals among rap listeners who were\npreviously exposed to more materialist themes in the music . Reeves said it introduced black consciousness to the \" hip @-@\nhop youth \" of the \" post @-@ black power generation \" , \" as leather African medallions made popular by rappers like P.E.\nreplaced thick gold chains as\nNeighbors at the 64x Scale\nan album that established the concept for Metallica ’s following two records . Colin Larkin , writing in the Encyclopedia of\nPopular Music , singled out \" For Whom the Bell Tolls \" as an example of Metallica ’s growing music potential . Popoff regards\nRide the Lightning as an album where \" extreme metal became art \" . Megaforce initially printed 75 @,@ 000 copies of the\nalbum for the US market , while Music for Nations took care of the European market . By the autumn of 1984 , Ride the\nLightning had moved 85 @,@ 000 copies in Europe , resulting in Metallica ’s ﬁrst cover story for British rock magazine Kerrang\n! in its December issue\nbest record of derisive punk rock since Exile on Main St. ( 1972 ) by the Rolling Stones . In The New Yorker , Ellen Willis wrote\nthat she learned to appreciate Too Much Too Soon more than New York Dolls after seeing the band perform songs from the\nformer album in concert , particularly \" Human Being \" and \" Puss ’ n ’ Boots \" , while Ron Ross from Phonograph Record\nmagazine said the group ’s \" easy going ironic sensibility \" was expressed \" far more amusingly and accessibly \" here than on their\ndebut album . Some reviewers were critical of Too Much Too Soon for what they felt was a poorly recorded and overproduced\n= Riot Act features a diverse sound , including folk @-@ based and experimental songs . Stephen Thomas Erlewine of AllMusic\nsaid \" Riot Act is the album that Pearl Jam has been wanting to make since Vitalogy - a muscular art rock record , one that still\nhits hard but that ’s ﬁlled with ragged edges and odd detours . \" Gossard said \" Riot Act really seems to showcase all of our thing\n. There ’s the simple rock songs we could have written in the earlier era , but it covers all the different times and dynamics we ’ve\nhad and still holds together . \" The musical experiments also lead several songs on the album to use alternate\nTable 6.Nearest neighbors computed using representations obtained at different scales\nMulti-scale Transformer Language Models\nContext\nPrisoner of Azkaban was the third ﬁlm in the series . Radcliffe ’s performance was panned by New York Times journalist A. O.\nScott , who wrote that Watson had to carry him with her performance . Next was Harry Potter and the Goblet of Fire in 2005 .\nThe ﬁlm was the second @-@ highest grossing Harry Potter\n2x Completions - | ﬁlm | , |\n4x Completions - | ﬁlm | the | , | and |\n8x Completions - | the | ﬁlm | of | , | . | in | and | series |\n16x Completions - | grossing | the | , | worldwide | and | $ | in | . | million | of | @-@ | highest | grossed | ﬁlm | Part | @.@ |\n64x Completions - | . | Harry | the | , | and | of | in | a | \" | ﬁlm | was | to | ’s | @-@ | The | that | Watson | Gob@@ | Potter | for |\nrint | Rowling | her | as | Prison@@ | performance | with | Columbus | an | on | by | she | it | Stone | had | Times | Phoenix | at | Best\n| Radcliffe | Half | In | from | book | series | ﬁrst | grossing | his | one | role | instal@@ | Philosop@@ | but | be | $ | which | ) | he | (\n| ab@@ | is | novel | character |\nContext\nset ﬁre to the ship to prevent her from falling into enemy hands . Patriots in small boats sailed out to the burning ship , ﬁred\nsome of its cannons at the British ships , took what stores and loot they could , and retreated shortly before the ship ’s powder\nmagazine exploded . = = Aftermath = = The British did not\n2x Completions - | the | have |\n4x Completions - | the | their | to | ﬁre |\n8x Completions - | the | , | to | and | any | of | not | until |\n16x Completions - | the | , | . | of | and | to | British | a | was | in | were | that | The | ’s | as | on |\n64x Completions - | . | the | , | and | of | to | British | ships | a | in | was | on | had | that | The | were | ’s | for | with | by | ship | at\n| from | wounded | as | ﬂeet | sailed | American | his | her | which | they | but | HMS | not | been | an | @-@ | two | after | men |\nreturned | York | battle | harbor | be | ﬁre | = | she | their | killed | Boston | French | it | damage | off | captured | one | guns | In | crew\n| into | he | schooner |\nTable 7.Examples of coarse LM completions. Given a\nScale Layers Emb Q,K,V\nProj QKT QKT\n√\ndk\nV FC LN, Drop\nResidual\nOutput\n+ Grad\nHierarchical\nRepresentations\nTotal\n(GB)\nMemory footprint breakdown for a single transformer LM layer at different scales\n1x 1 - 0.150 0.06000 0.100 0.500 0.500 - - 1.325\n4x 1 - 0.038 0.00400 0.025 0.120 0.120 - - 0.319\n16x 1 - 0.009 0.00030 0.006 0.031 0.031 - - 0.079\n64x 1 - 0.002 0.00005 0.002 0.008 0.008 - - 0.020\nMemory footprint breakdown for a 12 layer vanilla transformer\n1x 12 0.05 1.81 0.80531 1.208 6.040 6.644 4.10 - 21.26\nModel - - - - - - - - - 0.266\nTotal - - - - - - - - - 21.52 (20.98)\nMemory footprint breakdown for a 30 layer top-down transformer\n1x 8 0.05 1.06 0.41 0.7 3.87 3.52 3.59 0.308 14.27\n4x 8 - 0.26 0.025 0.176 0.87 0.96 - 0.175 2.529\n16x 7 - 0.05 0.001 0.037 0.18 0.29 - 0.042 0.545\n64x 7 - 0.01 0.00006 0.008 0.04 0.05 - 0.009 0.121\nModel - - - - - - - - - 0.521\nTotal - - - - - - - - - 17.98 (18.22)\nMemory footprint breakdown for a 23 layer Bottom-up transformer\n1x 8 0.05 1.21 0.536 0.805 4.02 4.42 4.10 0.686 16.19\n4x 8 - 0.30 0.033 0.201 1.00 1.10 - 0.013 2.76\n16x 7 - 0.06 0.002 0.044 0.22 0.24 - 0.003 0.60\nModel - - - - - - - - - 0.427\nTotal - - - - - - - - - 19.97 (20.41)\nTable 8.Memory footprint breakdown within different models. The number within brackets in the \"Total\" rows corresponds to what\nPyTorch reports versus our computed numbers.\nMulti-scale Transformer Language Models\nListing 1.top-down Model\ndef TopDownTransformerLM(\nx, hierarchies, downsamplers, upsamplers,\ntransformer_layers, linear_layers, decoder\n):\n\"\"\"\nForward pass of the top-down model.\nArgs:\nx: a 2D tensor of token indices (batch size x time steps)\nhierarchies: a sorted (descending) list of hierarchies (ex: [16, 4, 1])\ndownsamplers: a ModuleDict of downsampling functions for each hierarchy\nupsamplers: a ModuleDict of upsampling functions for each hierarchy\ntransformer_layers: a ModuleDict of autoregressive transformer decoders\nfor each hierarchy\nlinear_layers: a ModuleDict containing a linear layer for every hierarchy\neach linear layer is of size (2 * emb_dim, emb_dim)\ndecoder: a Linear layer from emb_dim to vocab size\nReturns:\nloss: a scalar that contains average cross-entropy\n\"\"\"\n# (batch size x time step x embedding dim)\nx = WordAndPositionEmbeddings(x)\n# Get factor by which to upsample at each layer.\nupsample_factors = [\ncur // next\nfor cur, next in zip(hierarchies, hierarchies[1:] + [1])\n]\ntop_hierarchy = hierarchies[0]\n# Run transformers from coarsest to finest hierarchy\nupsampled_representation = None\nfor i, hierarchy in enumerate(hierarchies):\n# Downsample the input by a factor equal to the current hierarchy\n# NOTE: Downsampling must be \"causal\"\n# (ex: if downsampling by 4x, cannot use a kernel of size > 4)\n# NOTE 2: Dowsampler at hierarchy 1 is just the identity function.\nout = downsamplers[hierarchy](\nx[:, (top_hierarchy - hierarchy):-hierarchy],\nfactor=hierarchy\n)\n# As input to the transformer, use a learned combination of\n# 1) the upsampled representation from the previous hierarchy\n# 2) the word-level representations downsampled to this scale\nif upsampled_representation is not None:\nout = gelu(linear_layers[hier](concatenate(\n[out, upsampled_representation], dim=2)\n))\n# Run all transformer layers at this hierarchy\nfor layer in transformer_layers[hierarchy]:\nout = layer(out)\n# Upsample representations to the next tier if not the final hierarchy\nif upsample_factors[i] > 1:\nupsampled_representation = upsamplers[hierarchy](\nout, factor=upsample_factors[i]\n)\n# Compute LM loss\nreturn CrossEntropy(\ndecoder(out), x[:, top_hierarchy:]\n)\nMulti-scale Transformer Language Models\nListing 2.Bottom-up Model\ndef BottomupTransformerLM(\nx, hierarchies, downsamplers, aggregation_layer,\ntransformer_layers, decoder\n):\n\"\"\"\nForward pass of the Bottom-up model.\nArgs:\nx: a 2D tensor of token indices (batch size x time steps)\nhierarchies: a sorted (descending) list of hierarchies (ex: [16, 4, 1])\ndownsamplers: a ModuleDict of downsampling functions for each hierarchy\ntransformer_layers: a single transformer layer that aggregates representations from different time scales\ntransformer_layers: a ModuleDict of autoregressive transformer decoders\nfor each hierarchy\ndecoder: a Linear layer from emb_dim to vocab size\nReturns:\nloss: a scalar that contains average cross-entropy\n\"\"\"\n# (batch size x time step x embedding dim)\nx = WordAndPositionEmbeddings(x[:, :-1])\n# Get factor by which to downsample at each layer.\ndownsample_factors = [\ncur // next\nfor cur, next in zip(hierarchies[:-1], hierarchies[1:])\n][::-1]\nrepresentations = []\nfor i, hierarchy in enumerate(hierarchies[1:][::-1]):\n# Downsample the input by ratio between the next and current time scale\n# NOTE: Downsampling must be \"causal\"\n# (ex: if downsampling by 4x, cannot use a kernel of size > 4)\n# NOTE 2: Dowsampler at hierarchy 1 is just the identity function.\nout = downsamplers[hierarchy](x, factor=downsample_factors[i])\n# Run all transformer layers at this hierarchy\nfor layer in transformer_layers[hierarchy]:\nout = layer(out)\nrepresentations.append(out)\n# Aggregate information from different scales with x as query and each representation as key \\& value\nout = aggregation_layer(x[:, :-1], representations)\n# Run transformer layers at the finest scale\nfor layer in transformer_layers[’1’]\nout = layer(out)\n# Compute LM loss\nreturn CrossEntropy(decoder(out), x[:, 1:])\nMulti-scale Transformer Language Models\nListing 3.Retina Model\ndef RetinaTransformerLM(\nx, hierarchies, downsamplers,\nattention_masks, transformer_layers, decoder\n):\n\"\"\"\nForward pass of the retina model.\nArgs:\nx: a 2D tensor of token indices (batch size x time steps)\nhierarchies: a sorted (descending) list of hierarchies (ex: [16, 4, 1])\ndownsamplers: a ModuleDict of downsampling functions for each hierarchy\nattention_masks: a ModuelDict of tensors for each hierarchy that has the attention mask\ntransformer_layers: a list of autoregressive transformer layers with appropriate attention masks\ndecoder: a Linear layer from emb_dim to vocab size\nReturns:\nloss: a scalar that contains average cross-entropy\n\"\"\"\n# (batch size x time step x embedding dim)\nx = WordAndPositionEmbeddings(x)\nfor i, layer in enumerate(transformer_layers):\n# Get representations at all time scales by downsampling\n# NOTE: Downsampling must be \"causal\"\n# (ex: if downsampling by 4x, cannot use a kernel of size > 4)\n# NOTE 2: Dowsampler at hierarchy 1 is just the identity function.\nrepresentations = [\ndownsamplers[hierarchy](x[:, :-1], factor=hierarchy)\nfor hierarchy in in hierarchies\n]\n# Attention masks specify what positions to look at from the representations at different time scales\nx = layer(representations, attention_mask=attention_masks)\n# Compute LM loss\nreturn CrossEntropy(decoder(x), x[:, 1])\nMulti-scale Transformer Language Models\nRetina Model\none has the bluegrass and which one has blue grass? \"Zane barely resisted the impulse to shake him again. It wouldn’t help, but it\nmight help him feel better.\" I’m not asking where Burns told you you were. \"Ty nodded, refusing to comment on that. Zane shook his\nhead and then\nreached into his pocket to pull out his phone. He turned it on and pressed the buttons for the Miami Police Department. He had a feeling that\nBurns had told him about the speciﬁc questions he’d asked, and he looked over at Ty. \"You ever been to Texas?\" he asked. Ty shook his head and\nlooked away, giving Zane the privacy he needed. He glanced at Ty. \"I ’ll be in touch,\" he promised, and then turned to walk away. \"What?\" Zane\nasked, surprised. \"You ’re pretty sure of yourself. \"Zane grinned and nodded.\" Not even in the Marines? \"Ty shook his head.\" Never. \"Zane\nstared at him a moment longer, then walked over to the bar and poured himself a beer. He leaned against the wall and watched Ty walk away. He\ncouldn’t help but smile. \"You don’t have to go.\" Ty stopped and turned, looking back at\ntop-down Model\nI told Dominic silently, I just wigged out. Dominic’s gaze met mine, his dark eyes glowing, his hair falling around his cheeks. You told\nhim to get off. He didn’t. The prick hasn’t even apologized. I don ’t think he’s going to, I replied calmly, I don\n’t know what to say . Dominic’s voice was stern. \"I will not tolerate that kind of behavior from you.\" I knew I had to look at him, because he was\nglaring at me . He wasn’t being a very nice guy, but I knew he was right. He deserved to be treated like a human being. I’d never seen him so\nangry. \"Dominic,\" I said softly, trying to soothe him. \"I ’m sorry. I didn’t mean to make you angry.\" I tried to make him look at me, but he\nlooked away. \"I just don’t understand why you do it. You have to know that you’re not normal.\" He sighed and looked back at me. \"I know. I ’m\nsorry. I just don’t want to be like that.\" I sighed, knowing he was right. I didn’t want to be like that either. \"Well, I ’m going to go check\nVanilla Model\nAnna’s lips twisted. \"Over the past few days I’ve discovered that there are very few things that are impossible.\" \"He’s dead, \"Morgana\nsaid, as much to convince herself as the annoying pest standing before her. \"I watched him die.\" \"You betrayed him. \"Morgana’s\neyes narrowed. \"The king is dead, and the rest of the clan has been murdered.\"Anna’s jaw hardened with a ﬁerce fury.\"What do you want?\"\n\"I want you out of my ofﬁce. Now. \"Anna stiffened , but before she could react, Levet was at her side, his wings ﬂapping furiously in panic.\n\"Sacrebleu, I beg of you. I cannot leave my sister. \"Anna glared at the tiny demon. \"I know.\" \"I can ’t take you with me, \"Anna snapped. Levet’s\nwings ﬂapped in panic. \"I’m not leaving without Morgana.\" \"You can’t be leaving her.\" \"No, I can’t.\" \"Then don’t leave me alone.\" \"I can ’t do\nit.\" \"You can.\" \"No.\" Levet’s words were swallowed by a sudden roar of anger\nBottom-up Model\ndoor wide and I gasped as I stepped inside. It was more like a studio apartment than a guest bedroom; it was about three times the\nsize of our room back in Seattle. The walls were painted a surprisingly warm shade of gray, with furniture in a deep, contrasting dark\ncherry. The bedspread was black with intricate\npatterns in shades of red, gold, and green. The ﬂoors were of polished wood and a thick layer of brown dust covered the walls. The ceiling was\nvaulted, giving the room a more natural light. There was a huge bay window and an easel in the corner. A large bed and a large, ﬂat-screen TV\nsurrounded it. The only other thing that was obvious was the bathroom. There were two stalls off to the side, the toilet and sink. There was a\nbathroom to the left of the bedroom, opposite a large, white, marble bathroom. There were two sinks, both full of toilet paper. I walked over to\nthe bed and looked out the window. The door was slightly ajar, and a couple of windows had a view of the lake.\nTable 9.Samples from different models.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9735684990882874
    },
    {
      "name": "Memory footprint",
      "score": 0.7808335423469543
    },
    {
      "name": "Transformer",
      "score": 0.7226115465164185
    },
    {
      "name": "Language model",
      "score": 0.7077125310897827
    },
    {
      "name": "Computer science",
      "score": 0.6589404344558716
    },
    {
      "name": "Scaling",
      "score": 0.6374987959861755
    },
    {
      "name": "Quadratic equation",
      "score": 0.4850858747959137
    },
    {
      "name": "Inductive bias",
      "score": 0.4311610162258148
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3506013751029968
    },
    {
      "name": "Natural language processing",
      "score": 0.3332732617855072
    },
    {
      "name": "Mathematics",
      "score": 0.15084192156791687
    },
    {
      "name": "Voltage",
      "score": 0.13352707028388977
    },
    {
      "name": "Programming language",
      "score": 0.10163941979408264
    },
    {
      "name": "Electrical engineering",
      "score": 0.08583059906959534
    },
    {
      "name": "Engineering",
      "score": 0.08288556337356567
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    },
    {
      "name": "Multi-task learning",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I70931966",
      "name": "Université de Montréal",
      "country": "CA"
    }
  ]
}