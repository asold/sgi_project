{
    "title": "Unifying Visual-Semantic Embeddings with Multimodal Neural Language Models",
    "url": "https://openalex.org/W1527575280",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4294557344",
            "name": "Kiros, Ryan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3177323215",
            "name": "Salakhutdinov, Ruslan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4293455370",
            "name": "Zemel, Richard S.",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2149172860",
        "https://openalex.org/W2184188583",
        "https://openalex.org/W92662927",
        "https://openalex.org/W2952122856",
        "https://openalex.org/W2296385829",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W1858383477",
        "https://openalex.org/W2110933980",
        "https://openalex.org/W2096446497",
        "https://openalex.org/W2066134726",
        "https://openalex.org/W1753482797",
        "https://openalex.org/W68733909",
        "https://openalex.org/W2953276893",
        "https://openalex.org/W2159243025",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W1897761818",
        "https://openalex.org/W2091812280",
        "https://openalex.org/W2112912048",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2171086879",
        "https://openalex.org/W2122585011",
        "https://openalex.org/W2123024445",
        "https://openalex.org/W2164587673",
        "https://openalex.org/W2141599568",
        "https://openalex.org/W8316075",
        "https://openalex.org/W2102605133",
        "https://openalex.org/W1687846465",
        "https://openalex.org/W2251682575",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2185175083",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W1810943226",
        "https://openalex.org/W2142074148",
        "https://openalex.org/W2964222437",
        "https://openalex.org/W2149557440",
        "https://openalex.org/W2102765684",
        "https://openalex.org/W2005708641",
        "https://openalex.org/W2120190345",
        "https://openalex.org/W2161000554",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W2115752676",
        "https://openalex.org/W2171361956",
        "https://openalex.org/W2109586012"
    ],
    "abstract": "Inspired by recent advances in multimodal learning and machine translation, we introduce an encoder-decoder pipeline that learns (a): a multimodal joint embedding space with images and text and (b): a novel language model for decoding distributed representations from our space. Our pipeline effectively unifies joint image-text embedding models with multimodal neural language models. We introduce the structure-content neural language model that disentangles the structure of a sentence to its content, conditioned on representations produced by the encoder. The encoder allows one to rank images and sentences while the decoder can generate novel descriptions from scratch. Using LSTM to encode sentences, we match the state-of-the-art performance on Flickr8K and Flickr30K without using object detections. We also set new best results when using the 19-layer Oxford convolutional network. Furthermore we show that with linear encoders, the learned embedding space captures multimodal regularities in terms of vector space arithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars. Sample captions generated for 800 images are made available for comparison.",
    "full_text": "Unifying Visual-Semantic Embeddings with\nMultimodal Neural Language Models\nRyan Kiros, Ruslan Salakhutdinov, Richard S. Zemel\nUniversity of Toronto\nCanadian Institute for Advanced Research\n{rkiros, rsalakhu, zemel}@cs.toronto.edu\nAbstract\nInspired by recent advances in multimodal learning and machine translation, we\nintroduce an encoder-decoder pipeline that learns (a): a multimodal joint embed-\nding space with images and text and (b): a novel language model for decoding\ndistributed representations from our space. Our pipeline effectively uniﬁes joint\nimage-text embedding models with multimodal neural language models. We in-\ntroduce the structure-content neural language model that disentangles the structure\nof a sentence to its content, conditioned on representations produced by the en-\ncoder. The encoder allows one to rank images and sentences while the decoder\ncan generate novel descriptions from scratch. Using LSTM to encode sentences,\nwe match the state-of-the-art performance on Flickr8K and Flickr30K without\nusing object detections. We also set new best results when using the 19-layer Ox-\nford convolutional network. Furthermore we show that with linear encoders, the\nlearned embedding space captures multimodal regularities in terms of vector space\narithmetic e.g. *image of a blue car* - \"blue\" + \"red\" is near images of red cars.\nSample captions generated for 800 images are made available for comparison.\n1 Introduction\nGenerating descriptions for images has long been regarded as a challenging perception task integrat-\ning vision, learning and language understanding. One not only needs to correctly recognize what\nappears in images but also incorporate knowledge of spatial relationships and interactions between\nobjects. Even with this information, one then needs to generate a description that is relevant and\ngrammatically correct. With the recent advances made in deep neural networks, tasks such as object\nrecognition and detection have made signiﬁcant breakthroughs in only a short time. The task of\ndescribing images is one that now appears tractable and ripe for advancement. Being able to append\nlarge image databases with accurate descriptions for each image would signiﬁcantly improve the\ncapabilities of content-based image retrieval systems. Moreover, systems that can describe images\nwell, could in principle, be ﬁne-tuned to answer questions about images also.\nThis paper describes a new approach to the problem of image caption generation, casted into the\nframework of encoder-decoder models. For the encoder, we learn a joint image-sentence embedding\nwhere sentences are encoded using long short-term memory (LSTM) recurrent neural networks [1].\nImage features from a deep convolutional network are projected into the embedding space of the\nLSTM hidden states. A pairwise ranking loss is minimized in order to learn to rank images and their\ndescriptions. For decoding, we introduce a new neural language model called the structure-content\nneural language model (SC-NLM). The SC-NLM differs from existing models in that it disentangles\nthe structure of a sentence to its content, conditioned on distributed representations produced by the\nencoder. We show that sampling from an SC-NLM allows us to generate realistic image captions,\nsigniﬁcantly improving over the generated captions produced by [2]. Furthermore, we argue that\nthis combination of approaches naturally ﬁts into the experimentation framework of [3], that is, a\ngood encoder can be used torank images and captions while a good decoder can be used togenerate\nnew captions from scratch. Our approach effectively uniﬁes image-text embedding models (encoder\n1\narXiv:1411.2539v1  [cs.LG]  10 Nov 2014\nFigure 1: Sample generated captions. The bottom row shows different error cases. Additional results\ncan be found at http://www.cs.toronto.edu/~rkiros/lstm_scnlm.html\nphase) [4, 5, 6] with multimodal neural language models (decoder phase) [2] [7]. Furthermore, our\nmethod builds on analogous approaches being used in machine translation [8, 9, 10, 11].\nWhile the application focus of our work is on image description generation and ranking, we also\nqualitatively analyse properties of multimodal vector spaces learned using images and sentences. We\nshow that using a linear sentence encoder, linguistic regularities [12] also carry over to multimodal\nvector spaces. For example, *image of a blue car* - \"blue\" + \"red\" results in a vector that is near\nimages of red cars. We qualitatively examine several types of analogies and structures with PCA\nprojections. Consequently, even with a global image-sentence training objective the encoder can still\nbe used to retrieve locally (e.g. individual words). This is analogous to pairwise ranking methods\nused in machine translation [13, 14].\n1.1 Multimodal representation learning\nA large body of work has been done on learning multimodal representations of images and text.\nPopular approaches include learning joint image-word embeddings [4, 5] as well as embedding\nimages and sentences into a common space [6, 15]. Our proposed pipeline makes direct use of\nthese ideas. Other approaches to multimodal learning include the use of deep Boltzmann machines\n[16], log-bilinear neural language models [2], autoencoders [17], recurrent neural networks [7] and\ntopic-models [18]. Several bi-directional approaches to ranking images and captions have also been\nproposed, based off of kernel CCA [3], normalized CCA [19] and dependency tree recursive net-\nworks [6]. From an architectural standpoint, our encoder-decoder model is most similar to [20], who\nproposed a two-step embedding and generation procedure for semantic parsing.\n1.2 Generating descriptions of images\nWe group together approaches to generation into three types of methods, each described here in\nmore detail:\nTemplate-based methods.Template-based methods involve ﬁlling in sentence templates, such as\ntriplets, based on the results of object detections and spatial relationships [21, 22, 23, 24, 25]. While\n2\nFigure 2: Encoder: A deep convolutional network (CNN) and long short-term memory recurrent\nnetwork (LSTM) for learning a joint image-sentence embedding. Decoder: A new neural language\nmodel that combines structure and content vectors for generating words one at a time in sequence.\nthese approaches can produce accurate descriptions, they are often more ‘robotic’ in nature and do\nnot generalize to the ﬂuidity and naturalness of captions written by humans.\nComposition-based methods. These approaches aim to harness existing image-caption databases\nby extracting components of related captions and composing them together to generate novel de-\nscriptions [26, 27]. The advantage of these approaches are that they allow for a much broader\nand more expressive class of captions that are more ﬂuent and human-like then template-based ap-\nproaches.\nNeural network methods.These approaches aim to generate descriptions by sampling from condi-\ntional neural language models. The initial work in this area, based off of multimodal neural language\nmodels [2], generated captions by conditioning on feature vectors from the output of a deep con-\nvolutional network. These ideas were recently extended to multimodal recurrent networks with\nsigniﬁcant improvements [7]. The methods described in this paper produce descriptions that at least\nqualitatively on par with current state-of-the-art composition-based methods [27].\nDescription generation systems have been plagued with issues of evaluation. While Bleu and Rouge\nhave been used in the past, [3] has argued that such automated evaluation methods are unreliable\nand do not match human judgements. These authors instead proposed that the problem of ranking\nimages and captions can be used as a proxy for generation. Since any generation system requires a\nscoring function to access how well a caption and image match, optimizing this task should naturally\ncarry over to an improvement in generation. Many recent methods have since used this approach\nfor evaluation. None the less, the question on how to transfer improvements on ranking to gen-\nerating new descriptions remained. We argue that encoder-decoder methods naturally ﬁt into this\nexperimentation framework. That is, the encoder gives us a way to rank images and captions and\ndevelop good scoring functions, while the decoder can use the representations learned to optimize\nthe scoring functions as a way of generating and scoring new descriptions.\n1.3 Encoder-decoder methods for machine translation\nOur proposed pipeline, while new to caption generation, has already experienced several successes in\nNeural Machine Translation (NMT). The goal of NMT is to develop an end-to-end translation system\nwith a large neural network, as opposed to using a neural network as an additional feature function\nto an existing phrase-based system. NMT methods are based on the encoder-decoder principle.\nThat is, an encoder is used to map an English sentence to a distributed vector. A decoder is then\nconditioned on this vector to generate a French translation from the source text. Current methods\ninclude using a convolutional encoder and RNN decoder [8], RNN encoder and RNN decoder [9, 10]\nand LSTM encoder with LSTM decoder [11]. While still a young research area, these methods have\nalready achieved performance on par with strong phrase-based systems and have improved on the\nstart-of-the-art when used for rescoring.\nWe argue that it is natural to think of image caption generation as a translation problem. That is,\nour goal is to translate an image into a description. This point of view has also been used by [28]\nand allows us to make use of existing ideas in the machine translation literature. Furthermore, there\nis a natural correspondence between the concept of scoring functions (how well does a caption and\nimage match) and alignments (which parts of a description correspond to which parts of an image)\nthat can naturally be exploited for generating descriptions.\n3\n2 An encoder-decoder model for ranking and generation\nIn this section we describe our image caption generation pipeline. We ﬁrst review LSTM RNNs\nwhich are used for encoding sentences, followed by how to learn multimodal distributed represen-\ntations. We then review log-bilinear neural language models [29], multiplicative neural language\nmodels [30] and then introduce our structure-content neural language model.\n2.1 Long short-term memory RNNs\nLong short-term memory [1] is a recurrent neural network that incorporates a built in memory cell\nto store information and exploit long range context. LSTM memory cells are surrounded by gat-\ning units for the purpose of reading, writing and reseting information. LSTMs have been used to\nachieve state-of-the-art performance in several tasks such as handwriting recognition [31], sequence\ngeneration [32] speech recognition [33] and machine translation [11] among others. Dropout [34]\nstrategies have also been proposed to prevent overﬁtting in deep LSTMs. [35]\nLet Xt denote a matrix of training instances at time t. In our case, Xt is used to denote a\nmatrix of word representations for the t-th word of each sentence in the training batch. Let\n(It,Ft,Ct,Ot,Mt) denote the input, forget, cell, output and hidden states of the LSTM at time\nstep t. The LSTM architecture in this work is implemented using the following equations:\nIt = σ(Xt ·Wxi + Mt−1 ·Whi + Ct−1 ·Wci + bi) (1)\nFt = σ(Xt ·Wxf + Mt−1 ·Whf + Ct−1 ·Wcf + bf ) (2)\nCt = Ft •Ct−1 + It •tanh(Xt ·Wxc + Mt−1 ·Whc + bc) (3)\nOt = σ(Xt ·Wxo + Mt−1 ·Who + Ct ·Wco + bo) (4)\nMt = Ot •tanh(Ct) (5)\nwhere (σ) denotes the sigmoid activation function, ( ·) indicates matrix multiplication and ( •) indi-\ncates component-wise multiplication. 1\n2.2 Multimodal distributed representations\nSuppose for training we are given image-description pairs each corresponding to an image and a\ndescription that correctly describes the image. Images are represented as the top layer (before the\nsoftmax) of a convolutional network trained on the ImageNet classiﬁcation task [36].\nLet D be the dimensionality of an image feature vector (e.g. 4096 for AlexNet [36]), K the di-\nmensionality of the embedding space and let V be the number of words in the vocabulary. Let\nWI ∈RK×D and WT ∈RK×V be the image embedding matrix and word embedding matri-\nces, respectively. Given an image description S = {w1,...,w N }with words w1,...,w N , 2 let\n{w1,..., wN },wi ∈RK,i = 1,...,n denote the corresponding word representations to words\nw1,...,w N (entries in the matrix WT ). The representation of a sentence v is the hidden state of\nthe LSTM at time stepN(i.e. the vector mt). We note that other approaches for computing sentence\nrepresentations for image-text embeddings have been proposed, including dependency tree RNNs\n[6] and bags of dependency parses [15]. Let q ∈RD denote an image feature vector (for the image\ncorresponding to description S) and let x = WI ·q ∈RK be the image embedding. We deﬁne a\nscoring function s(x,v) =x ·v, where x and v are ﬁrst scaled to have unit norm (making sequiv-\nalent to cosine similarity). Let θ denote all the parameters to be learned ( WI and all the LSTM\nweights) 3. We optimize the following pairwise ranking loss:\nmin\nθ\n∑\nx\n∑\nk\nmax{0,α −s(x,v) +s(x,vk)}+\n∑\nv\n∑\nk\nmax{0,α −s(v,x) +s(v,xk)} (6)\nwhere vk is a contrastive (non-descriptive) sentence for image embeddingx, and vice-versa withxk.\nFor all of our experiments, we initialize the word embeddings WT to be pre-computed K = 300\ndimensional vectors learned using a continuous bag-of-words model [37]. The contrastive terms are\nchosen randomly from the training set and resampled every epoch.\n1For additional details on LSTM: http://people.idsia.ch/~juergen/rnn.html.\n2As a slight abuse of notation, we refer to wi as both a word and an index into the word embedding matrix.\n3We keep the word embedding matrix WT ﬁxed.\n4\n2.3 Log-bilinear neural language models\nThe log-bilinear language model (LBL) [29] is a deterministic model that may be viewed as a feed-\nforward neural network with a single linear hidden layer. Each word win the vocabulary is repre-\nsented as a K-dimensional real-valued vector w ∈RK, as in the case of the encoder. Let R denote\na V ×Kmatrix of word representation vectors4 where V is the vocabulary size. Let (w1,...w n−1)\nbe a tuple of n−1 words where n−1 is the context size. The LBL model makes a linear prediction\nof the next word representation as\nˆ r=\nn−1∑\ni=1\nC(i)wi, (7)\nwhere C(i),i = 1,...,n −1 are K ×K context parameter matrices. Thus, ˆ ris the predicted\nrepresentation of wn. The conditional probability P(wn = i|w1:n−1) of wn given w1,...,w n−1 is\nP(wn = i|w1:n−1) = exp(ˆ rT ri + bi)∑V\nj=1 exp(ˆ rT rj + bj)\n, (8)\nwhere b ∈RV is a bias vector. Learning is done with stochastic gradient descent.\n2.4 Multiplicative neural language models\nSuppose now we are given a vector u ∈ RK from the multimodal vector space, which has an\nassociation with a word sequence S = {w1,...,w N }. For example, u may be the embedded\nrepresentation of an image whose description is given byS. A multiplicative neural language model\n[30] models the distributionP(wn = i|w1:n−1,u) of a new wordwn given context from the previous\nwords and the vectoru. A multiplicative model has the additional property that the word embedding\nmatrix is instead replaced with a tensor T ∈RV ×K×G where Gis the number of slices. Given u,\nwe can compute a word representation matrix as a function of u as T u = ∑G\ni=1 uiT (i) i.e. word\nrepresentations with respect to u are computed as a linear combination of slices weighted by each\ncomponent ui of u. Here, the number of slices Gis equal to K, the dimensionality of u.\nIt is often unnecessary to use a fully unfactored tensor. As in e.g. [38, 39], we re-represent T in\nterms of three matrices Wfk ∈RF×K, Wfd ∈RF×G and Wfv ∈RF×V , such that\nT u = (Wfv )⊤·diag(Wfd u) ·Wfk (9)\nwhere diag(·) denotes the matrix with its argument on the diagonal. These matrices are parametrized\nby a pre-chosen number of factors F. In [30], the conditioning vector u is referred to as an attribute\nand using a third-order model of words allows one to model conditional similarity: how meanings\nof words change as a function of the attributes they’re conditioned on.\nLet E = (Wfk )⊤Wfv denote a ‘folded’K ×V matrix of word embeddings. Given the context\nw1,...,w n−1, the predicted next word representation ˆ ris given by:\nˆ r=\nn−1∑\ni=1\nC(i)E(:,wi), (10)\nwhere E(:,wi) denotes the column ofE for the word representation ofwi and C(i),i = 1,...,n −1\nare K ×K context matrices. Given a predicted next word representation ˆ r, the factor outputs\nare f = (Wfkˆ r) •(Wfd u), where •is a component-wise product. The conditional probability\nP(wn = i|w1:n−1,u) of wn given w1,...,w n−1 and u can be written as\nP(wn = i|w1:n−1,u) = exp\n(\n(Wfv (:,i))⊤f + bi\n)\n∑V\nj=1 exp\n(\n(Wfv (:,j))⊤f + bj\n),\nwhere Wfv (:,i) denotes the column ofWfv corresponding to wordi. In contrast to the log-bilinear\nmodel, the matrix of word representations R from before is replaced with the factored tensorT that\nwe have derived. We compared the multiplicative model against an additive variant [2] and found on\nlarge datasets, such as the SBU Captioned Photo dataset [40], the multiplicative variant signiﬁcantly\noutperforms its additive counterpart. Thus, the SC-NLM is derived from the multiplicative variant.\n4Note that this is a different matrix then that used by the encoder. We use the same vocabulary throughout\nboth models.\n5\n(a) Multiplicative NLM\n (b) Structure-content NLM\n (c) SC-NLM prediction\nFigure 3: Left: multiplicative neural language model. Middle: Structure-content neural language\nmodel (SC-NLM). Right: The prediction problem of an SC-NLM.\n2.5 Structure-content neural language models\nWe now describe the structure-content neural language model. Suppose that, along with a de-\nscription S = {w1,...,w N }, we are also given a sequence of word-speciﬁc structure variables\nT = {t1,...,t N }. Throughout our experiments, each ti corresponds to the part-of-speech for word\nwi, although other possibilities can be used instead. Given an embeddingu (the content vector), our\ngoal is to model the distribution P(wn = i|w1:n−1,tn:n+k,u) from previous word context w1:n−1\nand forward structure context tn:n+k, where kis the forward context size. Figure 3 gives an illus-\ntration of the model and prediction problem. Intuitively, the structure variables help guide the model\nduring the generation phrase and can be thought of as a soft template to help avoid the model from\ngenerating grammatical nonsense. Note that this model shares a resemblance with the NNJM of [41]\nfor machine translation, where the previous word context are predicted words in the target language,\nand the forward context are words in the source language.\nOur model can be interpreted as a multiplicative neural language model but where the attribute\nvector is no longer u but instead an additive function of u and the structure variables T. Let\n{tn,..., tn+k},ti ∈RK,i = n,...,n + k be embedding vectors for the structure variables T.\nThese are obtained from a learned lookup table in the same way as words are. We introduce a se-\nquence of G×Gstructure context matrices T(i),i = n,...,n + kwhich play the same role as the\nword context matrices C(i). Let Tu denote a G×K context matrix for the multimodal vector u.\nThe attribute vector ˆ uof combined structure and content information is computed as\nˆ u=\n[(n+k∑\ni=n\nT(i)ti\n)\n+ T(u)u + b\n]\n+\n(11)\nwhere [·]+ = max{·,0}is a ReLU non-linearity and b is a bias vector. The vector ˆ unow plays the\nsame role as the vector u for the multiplicative model previously described and the remainder of the\nmodel remains unchanged. Our experiments use G= K = 300and factors F = 100.\nThe SC-NLM is trained on a large collection of image descriptions (e.g. Flickr30K). There are\nseveral choices available for representing the conditioning vectors u. One choice would be to use\nthe embedding of the corresponding image. An alternative choice, which is the approach we take, is\nto condition on the embedding vector for the descriptionScomputed with the LSTM. The advantage\nof this approach is that the SC-NLM can be trained purely on text alone. This allows us to make\nuse of large amounts of monolingual text (e.g. non image captions) to improve the quality of the\nlanguage model. Since the embedding vectors of Sshare a joint space with the image embeddings,\nwe can also condition the SC-NLM on image embeddings (e.g. at test time, when no description\nis available) after the model has been trained. This is a signiﬁcant advantage over a conditional\nlanguage model that explicitly requires image-caption pairs for training and highlights the strength\nof a multimodal encoding space.\nDue to space limitations, we leave the full details of our caption generation procedure to the supple-\nmentary material.\n6\nFlickr8K\nImage Annotation Image Search\nModel R@1 R@5 R@10 Med r R@1 R@5 R@10 Med r\nRandom Ranking 0.1 0.6 1.1 631 0.1 0.5 1.0 500\nSDT-RNN [6] 4.5 18.0 28.6 32 6.1 18.5 29.0 29\n† DeViSE [5] 4.8 16.5 27.3 28 5.9 20.1 29.6 29\n† SDT-RNN [6] 6.0 22.7 34.0 23 6.6 21.6 31.7 25\nDeFrag [15] 5.9 19.2 27.3 34 5.2 17.6 26.5 32\n† DeFrag [15] 12.6 32.9 44.0 14 9.7 29.6 42.5 15\nm-RNN [7] 14.5 37.2 48.5 11 11.5 31.0 42.4 15\nOur model 13.5 36.2 45.7 13 10.4 31.0 43.7 14\nOur model (OxfordNet) 18.0 40.9 55.0 8 12.5 37.0 51.5 10\nTable 1: Flickr8K experiments. R@K is Recall@K (high is good). Med r is the median rank (low is good).\nBest results overall are bold while best results without OxfordNet features are underlined . A † infront of the\nmethod indicates that object detections were used along with single frame features.\n3 Experiments\n3.1 Image-sentence ranking\nOur main quantitative results is to establish the effectiveness of using an LSTM sentence encoder\nfor ranking image and descriptions. We perform the same experimental procedure as done by [15]\non the Flickr8K [3] and Flickr30K [42] datasets. These datasets come with 8,000 and 30,000 images\nrespectively with each image annotated using 5 sentences by independent annotators. As with [15],\nwe did not do any explicit text preprocessing. We used two convolutional network architectures\nfor extracting 4096 dimensional image features: the Toronto ConvNet 5 as well as the 19-layer\nOxfordNet [43] which ﬁnished 2nd place in the ILSVRC 2014 classiﬁcation competition. Following\nthe protocol of [15], 1000 images are used for validation, 1000 for testing and the rest are used for\ntraining. Evaluation is performed using Recall@K, namely the mean number of images for which\nthe correct caption is ranked within the top-K retrieved results (and vice-versa for sentences). We\nalso report the median rank of the closest ground truth result from the ranked list. We compare our\nresults to each of the following methods:\nDeViSE. The deep visual semantic embedding model [5] was proposed as a way of performing zero-\nshot object recognition and was used as a baseline by [15]. In this model, sentences are represented\nas the mean of their word embeddings and the objective function optimized matches ours.\nSDT-RNN. The semantic dependency tree recursive neural network [6] is used to learn sentence\nrepresentations for embedding into a joint image-sentence space. The same objective is used.\nDeFrag. Deep fragment embeddings [15] were proposed as an alternative to embedding full-frame\nimage features and take advantage of object detections from the R-CNN [44] detector. Descriptions\nare represented as a bag of dependency parses. Their objective incorporates both a global and\nfragment objectives, for which their global objective matches ours.\nm-RNN. The multimodal recurrent neural network [7] is a recently proposed method that uses per-\nplexity as a bridge between modalities, as ﬁrst introduced by [2]. Unlike all other methods, the\nm-RNN does not use a ranking loss and instead optimizes the log-likelihood of predicting the next\nword in a sequence conditioned on an image.\nOur LSTMs use 1 layer with 300 units and weights initialized uniformly from [-0.08, 0.08]. The\nmargin αwas set to α = 0.2, which we found performed well on both datasets. Training is done\nusing stochastic gradient descent with an initial learning rate of 1 and was exponentially decreased.\nWe used minibatch sizes of 40 on Flickr8K and 100 on Flickr30K. No momentum was used. The\nsame hyperparameters are used for the OxfordNet experiments.\n3.1.1 Results\nTables 1 and 2 illustrate our results on Flickr8K and Flickr30K respectively. The performance of\nour model is comparable to that of the m-RNN. For some metrics we outperform or match existing\nresults while on others m-RNN outperforms our model. The m-RNN does not learn an explicit em-\nbedding between images and sentences and relies on perplexity as a means of retrieval. Methods that\n5https://github.com/TorontoDeepLearning/convnet\n7\nFlickr30K\nImage Annotation Image Search\nModel R@1 R@5 R@10 Med r R@1 R@5 R@10 Med r\nRandom Ranking 0.1 0.6 1.1 631 0.1 0.5 1.0 500\n† DeViSE [5] 4.5 18.1 29.2 26 6.7 21.9 32.7 25\n† SDT-RNN [6] 9.6 29.8 41.1 16 8.9 29.8 41.1 16\n† DeFrag [15] 14.2 37.7 51.3 10 10.2 30.8 44.2 14\n† DeFrag + Finetune CNN [15] 16.4 40.2 54.7 8 10.3 31.4 44.5 13\nm-RNN [7] 18.4 40.2 50.9 10 12.6 31.2 41.5 16\nOur model 14.8 39.2 50.9 10 11.8 34.0 46.3 13\nOur model (OxfordNet) 23.0 50.7 62.9 5 16.8 42.0 56.5 8\nTable 2: Flickr30K experiments. R@K is Recall@K (high is good). Med r is the median rank (low is good).\nBest results overall are bold while best results without OxfordNet features are underlined . A † infront of the\nmethod indicates that object detections were used along with single frame features.\nlearn explicit embedding spaces have a signiﬁcant speed advantage over perplexity-based retrieval\nmethods, since retrieval is easily done with a single matrix multiply of stored embedding vectors\nfrom the dataset with the query vector. Thus explicit embedding methods are much better suited for\nscaling to large datasets.\nPerhaps more interestingly is the fact that both our method and the m-RNN outperform existing\nmodels that integrate object detections. This is contradictory to [6], where recurrent networks are the\nworst performing models. This highlights the effectiveness of LSTM cells for encoding dependen-\ncies across descriptions and learning meaningful distributed sentence representations. Integrating\nobject detections into our framework should almost surely improve performance as well as allow for\ninterpretable retrievals, as in the case of DeFrag.\nUsing image features from the OxfordNet model results in a signiﬁcant performance boost across\nall metrics, giving new state-of-the-art numbers on these evaluation tasks.\n3.2 Multimodal linguistic regularities\nWord embeddings learned with skip-gram [37] or neural language models [45] were shown by [12]\nto exhibit linguistic regularities that allow these models to perform analogical reasoning. For in-\nstance, \"man\" is to \"woman\" as \"king\" is to ? can be answered by ﬁnding the closest vector to\n\"king\" - \"man\" + \"woman\". A natural question we ask is whether multimodal vector spaces exhibit\nthe same phenomenon. Would *image of a blue car* - \"blue\" + \"red\" be near images of red cars?\nSuppose that we train an embedding model with a linear encoder, namely v = ∑N\ni=1 wi for word\nvectors wi and sentence vector v (where both v and the image embedding are normalized to unit\nlength). Using our example above, letvblue, vred and vcar denote the word embeddings for blue, red\nand car respectively. Let Ibcar and Ircar denote embeddings of images with blue and red cars. After\ntraining a linear encoder, the model has the property that vblue + vcar ≈Ibcar and vred + vcar ≈\nIrcar. It follows that\nvcar ≈ Ibcar −vblue (12)\nvred + vcar ≈ Ibcar −vblue + vred (13)\nIrcar ≈ Ibcar −vblue + vred (14)\nThus given a query image q, a negative word wn and a positive word wp (all with unit norm), we\nseek an image x∗such that:\nx∗= argmax\nx\n(q −wn + wp)⊤x\n∥q −wn + wp∥ (15)\nThe supplementary material contains qualitative evidence that the above holds for several types\nof regularities and images. 6 In our examples, we consider retrieving the top-4 nearest images.\nOccasionally we observed that a poor result would be obtained within the top-4 among good results.\nWe found a simple strategy for removing these cases is to ﬁrst retrieve the top N nearest images,\nthen re-sort these based on their distance to the mean of the N images.\nIt is worth noting that these kinds of regularities are not well observed with an LSTM encoder, since\nsentences are no longer just a sum of their words. The linear encoder is roughly equivalent to the\n6For this model we ﬁnetune the word representations.\n8\nDeViSE baselines in tables 1 and 2, which perform signiﬁcantly worse for retrieval than an LSTM\nencoder. So while these regularities are interesting the learned multimodal vector space is not well\napt for ranking sentences and images.\n3.3 Image caption generation\nWe generated image descriptions for roughly 800 images from the SBU captioned photo dataset [40].\nThese are the same images used to display results by the current state-of-the-art composition based\napproach, TreeTalk [27].7 Our LSTM encoder and SC-NLM decoder were trained by concatenating\nthe Flickr30K dataset with the recently released Microsoft COCO dataset [46], which combined\ngive us over 100,000 images and over 500,000 descriptions for training. The SBU dataset contains 1\nmillion images each with a single description and was used by [27] for training their model. While\nthe SBU dataset is larger, the annotated descriptions are noisier and more personalized.\nThe generated results can be found at http://www.cs.toronto.edu/~rkiros/lstm_\nscnlm.html 8. For each image we show the original caption, the nearest neighbour sentence\nfrom the training set, the top-5 generated samples from our model and the best generated result from\nTreeTalk. The nearest neighbour sentence is displayed to demonstrate that our model has not simply\nlearned to copy the training data. Our generated descriptions are arguably the nicest ones to date.\n4 Discussion\nWhen generating a description, it is often the case that only a small region is relevant at any given\ntime. We are developing an attention-based model that jointly learns to align parts of captions to\nimages and use these alignments to determine where to attend next, thus dynamically modifying the\nvectors used for conditioning the decoder. We also plan on experimenting with LSTM decoders as\nwell as deep and bidirectional LSTM encoders.\nAcknowledgments\nWe would like to thank Nitish Srivastava for assistance with his ConvNet package as well as prepar-\ning the Oxford convolutional network. We also thank the anonymous reviewers from the NIPS 2014\ndeep learning workshop for their comments and suggestions.\nReferences\n[1] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation,\n1997.\n[2] Ryan Kiros, Richard S Zemel, and Ruslan Salakhutdinov. Multimodal neural language models.\nICML, 2014.\n[3] Micah Hodosh, Peter Young, and Julia Hockenmaier. Framing image description as a ranking\ntask: Data, models and evaluation metrics. JAIR, 2013.\n[4] Jason Weston, Samy Bengio, and Nicolas Usunier. Large scale image annotation: learning to\nrank with joint word-image embeddings. Machine learning, 2010.\n[5] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeffrey Dean, and Tomas Mikolov\nMarcAurelio Ranzato. Devise: A deep visual-semantic embedding model. NIPS, 2013.\n[6] Richard Socher, Q Le, C Manning, and A Ng. Grounded compositional semantics for ﬁnding\nand describing images with sentences. In TACL, 2014.\n[7] Junhua Mao, Wei Xu, Yi Yang, Jiang Wang, and Alan L Yuille. Explain images with multi-\nmodal recurrent neural networks. arXiv preprint arXiv:1410.1090, 2014.\n[8] Nal Kalchbrenner and Phil Blunsom. Recurrent continuous translation models. In EMNLP,\n2013.\n[9] Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre, Fethi Bougares, Holger Schwenk,\nand Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical\nmachine translation. EMNLP, 2014.\n7http://ilp-cky.appspot.com/generation\n8These results use features from the Toronto ConvNet.\n9\n[10] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by\njointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\n[11] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\nnetworks. NIPS, 2014.\n[12] Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig. Linguistic regularities in continuous space\nword representations. In NAACL-HLT, 2013.\n[13] Karl Moritz Hermann and Phil Blunsom. Multilingual distributed representations without word\nalignment. ICLR, 2014.\n[14] Karl Moritz Hermann and Phil Blunsom. Multilingual models for compositional distributional\nsemantics. In ACL, 2014.\n[15] Andrej Karpathy, Armand Joulin, and Li Fei-Fei. Deep fragment embeddings for bidirectional\nimage sentence mapping. NIPS, 2014.\n[16] Nitish Srivastava and Ruslan Salakhutdinov. Multimodal learning with deep boltzmann ma-\nchines. In NIPS, 2012.\n[17] Jiquan Ngiam, Aditya Khosla, Mingyu Kim, Juhan Nam, Honglak Lee, and Andrew Ng. Mul-\ntimodal deep learning. In ICML, 2011.\n[18] Yangqing Jia, Mathieu Salzmann, and Trevor Darrell. Learning cross-modality similarity for\nmultinomial data. In ICCV, 2011.\n[19] Yunchao Gong, Liwei Wang, Micah Hodosh, Julia Hockenmaier, and Svetlana Lazebnik. Im-\nproving image-sentence embeddings using large weakly annotated photo collections. InECCV.\n2014.\n[20] Phil Blunsom, Nando de Freitas, Edward Grefenstette, Karl Moritz Hermann, et al. A deep\narchitecture for semantic parsing. In ACL 2014 Workshop on Semantic Parsing, 2014.\n[21] Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg,\nand Tamara L Berg. Baby talk: Understanding and generating simple image descriptions. In\nCVPR, 2011.\n[22] Ali Farhadi, Mohsen Hejrati, Mohammad Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia\nHockenmaier, and David Forsyth. Every picture tells a story: Generating sentences from\nimages. In ECCV. 2010.\n[23] Siming Li, Girish Kulkarni, Tamara L Berg, Alexander C Berg, and Yejin Choi. Composing\nsimple image descriptions using web-scale n-grams. In CONLL, 2011.\n[24] Yezhou Yang, Ching Lik Teo, Hal Daumé III, and Yiannis Aloimonos. Corpus-guided sentence\ngeneration of natural images. In EMNLP, 2011.\n[25] Margaret Mitchell, Xufeng Han, Jesse Dodge, Alyssa Mensch, Amit Goyal, Alex Berg, Kota\nYamaguchi, Tamara Berg, Karl Stratos, and Hal Daumé III. Midge: Generating image descrip-\ntions from computer vision detections. In EACL, 2012.\n[26] Polina Kuznetsova, Vicente Ordonez, Alexander C Berg, Tamara L Berg, and Yejin Choi.\nCollective generation of natural image descriptions. ACL, 2012.\n[27] Polina Kuznetsova, Vicente Ordonez, Tamara L. Berg, and Yejin Choi. Treetalk : Composition\nand compression of trees for image descriptions. TACL, 2014.\n[28] Marcus Rohrbach, Wei Qiu, Ivan Titov, Stefan Thater, Manfred Pinkal, and Bernt Schiele.\nTranslating video content to natural language descriptions. In ICCV, 2013.\n[29] Andriy Mnih and Geoffrey Hinton. Three new graphical models for statistical language mod-\nelling. In ICML, pages 641–648, 2007.\n[30] Ryan Kiros, Richard S Zemel, and Ruslan Salakhutdinov. A multiplicative model for learning\ndistributed text-based attribute representations. NIPS, 2014.\n[31] Alex Graves, Marcus Liwicki, Santiago Fernández, Roman Bertolami, Horst Bunke, and Jür-\ngen Schmidhuber. A novel connectionist system for unconstrained handwriting recognition.\nTPAMI, 2009.\n[32] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint\narXiv:1308.0850, 2013.\n[33] Alex Graves, Navdeep Jaitly, and Abdel-rahman Mohamed. Hybrid speech recognition with\ndeep bidirectional lstm. In IEEE Workshop on ASRU, 2013.\n10\n[34] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-\nnov. Dropout: A simple way to prevent neural networks from overﬁtting. JMLR, 2014.\n[35] Wojciech Zaremba, Ilya Sutskever, and Oriol Vinyals. Recurrent neural network regularization.\narXiv preprint arXiv:1409.2329, 2014.\n[36] Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton. Imagenet classiﬁcation with deep convo-\nlutional neural networks. In NIPS, 2012.\n[37] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n[38] Roland Memisevic and Geoffrey Hinton. Unsupervised learning of image transformations. In\nCVPR, pages 1–8, 2007.\n[39] Alex Krizhevsky, Geoffrey E Hinton, et al. Factored 3-way restricted boltzmann machines for\nmodeling natural images. In AISTATS, pages 621–628, 2010.\n[40] Vicente Ordonez, Girish Kulkarni, and Tamara L Berg. Im2text: Describing images using 1\nmillion captioned photographs. In NIPS, 2011.\n[41] Jacob Devlin, Rabih Zbib, Zhongqiang Huang, Thomas Lamar, Richard Schwartz, and John\nMakhoul. Fast and robust neural network joint models for statistical machine translation.ACL,\n2014.\n[42] Peter Young Alice Lai Micah Hodosh and Julia Hockenmaier. From image descriptions to vi-\nsual denotations: New similarity metrics for semantic inference over event descriptions.TACL,\n2014.\n[43] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale\nimage recognition. arXiv preprint arXiv:1409.1556, 2014.\n[44] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. Rich feature hierarchies for\naccurate object detection and semantic segmentation. CVPR, 2014.\n[45] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Janvin. A neural probabilistic\nlanguage model. JMLR, 2003.\n[46] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan,\nPiotr Dollár, and C Lawrence Zitnick. Microsoft coco: Common objects in context. arXiv\npreprint arXiv:1405.0312, 2014.\n11\n5 Supplementary material: Additional experimentation and details\n5.1 Multimodal linguistic regularities\n(a) Simple cases\n (b) Colors\n(c) Image structure\n (d) Sanity check\nFigure 4: Multimodal vector space arithmetic. Query images were downloaded online and retrieved\nimages are from the SBU dataset.\n(a) Colors\n (b) Weather\nFigure 5: PCA projection of the 300-dimensional word and image representations for (a) cars and\ncolors and (b) weather and temperature.\nFigure 4 illustrates sample results using a model trained on the SBU dataset. All queries were\ndownloaded online and retrieved images are from the SBU images used for training. What is of\ninterest to note is that the resulting images depend highly on the image used for the query. For\nexample, searching for the word ‘night’ retrieves arbitrary images taken at night. On the other\nhand, an image with a building predominantly as its focus will return night images when ‘day’ is\n12\nsubtracted and ‘night’ is added. A similar phenomenon occurs with the example of cats, bowls and\nboxes. As additional visualizations, we computed PCA projections of cars and their corresponding\ncolors as well as images and the weather occurrences in Figure 5. These results give us strong\nevidence for the regularities apparent in multimodal vector spaces trained with linear encoders. Of\ncourse, sensible results are only likely to be obtained if (a) the content of the image is correctly\nrecognized, (b) the subtraction word is relevant to the image and (c) an image exists that is sensible\nfor the corresponding query.\n5.2 Image description generation\nThe SC-NLM was trained on the concatenation of training sentences from both Flickr30K and Mi-\ncrosoft COCO. Given an image, we ﬁrst map it into the multimodal space. From this embedding,\nwe deﬁne 2 sets of candidate conditioning vectors to the SC-NLM:\nImage embedding.The embedded image itself. Note that the SC-NLM was not trained with images\nbut can be conditioned on images since the embedding space is multimodal.\ntop-N nearest words and sentences.After ﬁrst computing the image embedding, we obtain the\ntop-N nearest neighbour words and training sentences using cosine similarity. These retrievals are\ntreated as a ‘bag of concepts’ for which we compute an embedding vector as the mean of each\nconcept. All of our results use N = 5.\nAlong with the candidate conditioning vectors, we also compute candidate POS sequences used by\nthe SC-NLM. For this, we obtain a set of all POS sequences from the training set whose lengths\nwere between 4 and 12, inclusive. Captions are generated by ﬁrst sampling a conditioning vector,\nnext sampling a POS sequence, then computing a MAP estimate from the SC-NLM. We generate\na large list of candidate descriptions (1000 for each image in our results) and rank these candidates\nusing a scoring function. Our scoring function consists of two feature functions:\nTranslation model. The candidate description is embedded into the multimodal space using the\nLSTM. We then compute a translation score as the cosine similarity between the image embedding\nand the embedding of the candidate description. This scores how relevant the content of the candi-\ndate is to the image. We also augment to this score a multiplicative penalty to non-stopwords that\nappear too frequently in the description. 9\nLanguage model.We trained a Kneser-Ney trigram model on a large corpus and compute the log-\nprobability of the candidate under the model. This scores how reasonable of an English sentence is\nthe candidate.\nThe total score of a caption is then the weighted sum of the translation and language models. Due to\nthe challenge of quantitatively evaluating generated descriptions, we tuned the weights by hand on\nqualitative results alone. All of the candidate descriptions are ranked by their scores, and the top-5\ncaptions are returned.\n9For instance, given an image of a car, we would want a candidate to be ranked low if each noun in the\ndescription was ‘car’.\n13"
}