{
    "title": "LLM-Cloud Complete: Leveraging Cloud Computing for Efficient Large Language Model-based Code Completion",
    "url": "https://openalex.org/W4401427835",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2108694387",
            "name": "Zhang Mingxuan",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A2030840999",
            "name": "Bo Yuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2564608724",
            "name": "Hanzhe Li",
            "affiliations": [
                "New York University"
            ]
        },
        {
            "id": "https://openalex.org/A2119895751",
            "name": "Kangming Xu",
            "affiliations": [
                "Santa Clara University"
            ]
        },
        {
            "id": "https://openalex.org/A2108694387",
            "name": "Zhang Mingxuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2030840999",
            "name": "Bo Yuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2564608724",
            "name": "Hanzhe Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2119895751",
            "name": "Kangming Xu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4399184127",
        "https://openalex.org/W4399159250",
        "https://openalex.org/W4400163718",
        "https://openalex.org/W4401049618",
        "https://openalex.org/W4390405273",
        "https://openalex.org/W4400142294",
        "https://openalex.org/W4400469776",
        "https://openalex.org/W4399631589",
        "https://openalex.org/W4400912802",
        "https://openalex.org/W4399606263",
        "https://openalex.org/W4399453640",
        "https://openalex.org/W4400533090",
        "https://openalex.org/W4399539756",
        "https://openalex.org/W4396903525",
        "https://openalex.org/W4400099718",
        "https://openalex.org/W4402000902",
        "https://openalex.org/W4399159284",
        "https://openalex.org/W4400663131",
        "https://openalex.org/W4400163740",
        "https://openalex.org/W4399539825",
        "https://openalex.org/W4399184154",
        "https://openalex.org/W4399401100",
        "https://openalex.org/W4400256058",
        "https://openalex.org/W4399184337",
        "https://openalex.org/W4401035293",
        "https://openalex.org/W4400585834",
        "https://openalex.org/W4396903465",
        "https://openalex.org/W4399539769",
        "https://openalex.org/W4403904595",
        "https://openalex.org/W4390092128",
        "https://openalex.org/W4400851328",
        "https://openalex.org/W4401035297",
        "https://openalex.org/W4402027889",
        "https://openalex.org/W4395450810",
        "https://openalex.org/W4399540523",
        "https://openalex.org/W4400022817",
        "https://openalex.org/W4399184028",
        "https://openalex.org/W4401276489"
    ],
    "abstract": "This paper introduces LLM-CloudComplete, a novel cloud-based system for efficient and scalable code completion leveraging large language models (LLMs). We address the challenges of deploying LLMs for real-time code completion by implementing a distributed inference architecture, adaptive resource allocation, and multi-level caching mechanisms. Our system utilizes a pipeline parallelism technique to distribute LLM layers across multiple GPU nodes, achieving near-linear scaling in throughput. We propose an adaptive resource allocation algorithm using reinforcement learning to optimize GPU utilization under varying workloads. A similarity-based retrieval mechanism is implemented within a three-tier caching system to reduce computational load and improve response times. Additionally, we introduce several latency reduction strategies, including predictive prefetching, incremental completion generation, and sparse attention optimization. Extensive evaluations on diverse programming languages demonstrate that LLM-CloudComplete outperforms existing state-of-the-art code completion systems, achieving a 7.4% improvement in Exact Match accuracy while reducing latency by 76.2% and increasing throughput by 320%. Our ablation studies reveal the significant contributions of each system component to overall performance. LLM-CloudComplete represents a substantial advancement in cloud-based AI-assisted software development, paving the way for more efficient and responsive coding tools. We discuss limitations and future research directions, including privacy-preserving techniques and adaptability to diverse programming paradigms.",
    "full_text": null
}