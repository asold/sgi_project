{
  "title": "CMT: Convolutional Neural Networks Meet Vision Transformers",
  "url": "https://openalex.org/W3180659574",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2369507686",
      "name": "Guo, Jianyuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103390482",
      "name": "Han, Kai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098960966",
      "name": "Wu Han",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2357421627",
      "name": "Tang, Yehui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2631132782",
      "name": "Chen Xing-hao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A723829169",
      "name": "Wang YunHe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101184268",
      "name": "Xu Chang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3165150763",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W2964350391",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2124386111",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2804935296",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W3128099838",
    "https://openalex.org/W2963495494",
    "https://openalex.org/W2138011018",
    "https://openalex.org/W3035661013",
    "https://openalex.org/W1977295328",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2279098554",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2963843116",
    "https://openalex.org/W3035414587",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2883780447",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2340897893",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2898732869",
    "https://openalex.org/W3159663321",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3001591165",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3097055324",
    "https://openalex.org/W2789541106",
    "https://openalex.org/W2533598788",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W1836465849"
  ],
  "abstract": "Vision transformers have been successfully applied to image recognition tasks due to their ability to capture long-range dependencies within an image. However, there are still gaps in both performance and computational cost between transformers and existing convolutional neural networks (CNNs). In this paper, we aim to address this issue and develop a network that can outperform not only the canonical transformers, but also the high-performance convolutional models. We propose a new transformer based hybrid network by taking advantage of transformers to capture long-range dependencies, and of CNNs to model local features. Furthermore, we scale it to obtain a family of models, called CMTs, obtaining much better accuracy and efficiency than previous convolution and transformer based models. In particular, our CMT-S achieves 83.5% top-1 accuracy on ImageNet, while being 14x and 2x smaller on FLOPs than the existing DeiT and EfficientNet, respectively. The proposed CMT-S also generalizes well on CIFAR10 (99.2%), CIFAR100 (91.7%), Flowers (98.7%), and other challenging vision datasets such as COCO (44.3% mAP), with considerably less computational cost.",
  "full_text": "CMT: Convolutional Neural Networks Meet Vision Transformers\nJianyuan Guo1,2, Kai Han2, Han Wu1, Yehui Tang2, Xinghao Chen2, Yunhe Wang2*, Chang Xu1‚àó\n1 School of Computer Science, Faculty of Engineering, University of Sydney. 2 Huawei Noah‚Äôs Ark Lab.\n{jianyuan.guo, kai.han, yunhe.wang}@huawei.com; c.xu@sydney.edu.au\nAbstract\nVision transformers have been successfully applied to\nimage recognition tasks due to their ability to capture long-\nrange dependencies within an image. However, there are still\ngaps in both performance and computational cost between\ntransformers and existing convolutional neural networks\n(CNNs). In this paper, we aim to address this issue and de-\nvelop a network that can outperform not only the canonical\ntransformers, but also the high-performance convolutional\nmodels. We propose a new transformer based hybrid network\nby taking advantage of transformers to capture long-range\ndependencies, and of CNNs to extract local information.\nFurthermore, we scale it to obtain a family of models, called\nCMTs, obtaining much better trade-off for accuracy and\nefÔ¨Åciency than previous CNN-based and transformer-based\nmodels. In particular, our CMT-S achieves 83.5% top-1\naccuracy on ImageNet, while being 14x and 2x smaller on\nFLOPs than the existing DeiT and EfÔ¨ÅcientNet, respectively.\nThe proposed CMT-S also generalizes well on CIFAR10\n(99.2%), CIFAR100 (91.7%), Flowers (98.7%), and other\nchallenging vision datasets such as COCO (44.3% mAP),\nwith considerably less computational cost.\n1. Introduction\nThe past decades have witnessed the extraordinary contri-\nbution of CNNs [16, 46, 47, 52, 53] in the Ô¨Åeld of computer\nvision due to its ability of extracting deep discriminative fea-\ntures. Meanwhile, self-attention based transformers [9, 58]\nhas become the de facto most popular models for natural\nlanguage processing (NLP) tasks, and shown excellent ca-\npability of capturing long-distance relationships. Recently,\nmany researchers attempt to apply the transformer-based\narchitectures to vision domains, and achieve promising re-\nsults in various tasks such as image classiÔ¨Åcation [10, 57],\nobject detection [2, 70], and semantic segmentation [69]. Vi-\nsion transformer (ViT) [10] is the Ô¨Årst work to replace the\n*Corresponding author. Mindspore [23] implementation can be found\nin: https://gitee.com/mindspore/models/tree/master/research/cv/CMT. Py-\ntorch [43] implementation: https://github.com/ggjy/CMT.pytorch.\nconventional CNN backbone with a pure transformer. Input\nimages (224√ó224√ó3) are Ô¨Årst split into 196 non-overlapping\npatches (with a Ô¨Åxed size of 16√ó16√ó3 per patch), which are\nanalogous to the word tokens in NLP. The patches are then\nfed into stacked standard transformer blocks to model global\nrelations and extract feature for classiÔ¨Åcation. The design\nparadigm of ViT has heavily inspired the following trans-\nformer based models for computer vision, such as IPT [3] for\nlow-level vision and SETR [69] for semantic segmentation.\nDespite that transformers have demonstrated excellent ca-\npabilities when migrated to vision tasks, their performances\nare still far inferior to similar-sized convolutional neural net-\nwork counterparts, e.g., EfÔ¨ÅcientNets [53]. We believe the\nreason of such weakness is threefold. Firstly, images are\nsplit into patches in ViT [10] and other transformer-based\nmodels such as IPT [3] and SETR [69]. Doing so can greatly\nsimplify the process of applying transformer to image-based\ntasks. And the sequence of patches can be directly fed into\na standard transformer where long-range dependencies be-\ntween patches can be well captured. However, it ignores\nthe fundamental difference between sequence-based NLP\ntasks and image-based vision tasks,e.g., the 2D structure and\nspatial local information within each patch. Secondly, trans-\nformer is difÔ¨Åcult to explicitly extract low-resolution and\nmulti-scale features due to the Ô¨Åxed patch size, which poses\na big challenge to dense prediction tasks such as detection\nand segmentation. Thirdly, the computational and memory\ncost of self-attention modules in transformers are quadratic\n(O(N2C)) to the resolution of inputs, compared toO(NC2)\nof convolution-based CNNs. High resolution images are\nprevalent and common, e.g., 1333√ó800 in COCO [35] and\n2048√ó1024 in Cityscapes [7]. Using transformers to pro-\ncess such images would inevitably cause the problem of\ninsufÔ¨Åcient GPU memory and low computation efÔ¨Åciency.\nIn this paper, we stand upon the intersection of CNNs and\ntransformers, and propose a novel CMT (CNNs meet trans-\nformers) architecture for visual recognition. The proposed\nCMT takes the advantages of CNNs to compensate for the\naforementioned limitations when utilizing pure transformers.\nAs shown in Figure 2(c), input images Ô¨Årst go through the\nconvolution stem for Ô¨Åne-grained feature extraction, and are\narXiv:2107.06263v3  [cs.CV]  14 Jun 2022\n10 20 30 40 50\nFLOPs (Billions)\n72\n74\n76\n78\n80\n82\n84ImageNet Top-1 Accuracy (%)\nCMT-S\nEfficientNet-B7\nSwin-B\nSwin-B 384\nPVT-M\nDeiT-B\nDeiT-B 384\nRes152\nCMT\nEfficientNet\nSwin\nPVT\nDeiT\nResNet\nModel Top1 Acc. # Params # FLOPsDeiT-Ti [57]72.2%5M 1.3BCPVT-Ti-GAP [6]74.9% 6M 1.3BCeiT-T [67]76.4% 5M 1.2BEfÔ¨ÅcientNet-B3 [53]81.6% 12M 1.8BCMT-XS 81.7%14M 1.5BResNet-50 [16]76.2% 26M 4.1BDeiT-S [57]79.8% 22M 4.6BRegNetY-4GF [44]80.0% 21M 4.0BT2T-ViT-14 [68]80.6% 21M 4.8BResNeXt-101 [64]80.9% 84M 32BPVT-M [60]81.2% 44M 6.7BSwin-T [36]81.3% 29M 4.5BCPVT-S-GAP [6]81.5% 23M 4.6BCeiT-S [67]82.0% 24M 4.5BCvT-13-NAS [63]82.2%18M 4.1BEfÔ¨ÅcientNet-B4 [53]82.9% 19M 4.2BCMT-S 83.5%25M 4.0B\n(a) ImageNet Accuracy vs. FLOPs\n200 250 300 350 400 450 500\nFLOPs (Billions)\n32\n34\n36\n38\n40\n42\n44\n46COCO BBox AP (%)\nCMT-XS\nCMT-S\nCMT-B\nPVT-T\nPVT-S\nPVT-M\nPVT-L\nRelationNet++\nX101-32x4d\nX-101-64x4d\nRes50\nRes101\nConT-M\nCMT\nPVT\nRelationNet++\nResNeXt\nResNet\nConT\nModel mAP # FLOPsConT-M [66] 37.9% 217BResNet-101 [16]38.5% 315BRelationNet++ [4]39.2% 266BResNeXt-101-64x4d [64]41.0% 473BSwin-T [36] 41.5% 245BPVT-M [60] 42.0% 283BTwins-SVT-S [5]42.3% 209BCMT-S 44.3%230B\n(b) COCO mAP vs. FLOPs\nFigure 1. Performance comparison between CMT and other models. (a) Top-1 accuracy on ImageNet [8]. (b) Object detection results\non COCO val2017 [35] of different backbones using RetinaNet framework, all numbers are for single-scale, ‚Äú1x‚Äù training schedule.\nthen fed into a stack of CMT blocks for representation learn-\ning. SpeciÔ¨Åcally, the introduced CMT block is an improved\nvariant of transformer block whose local information is en-\nhanced by depth-wise convolution. Compared to ViT [10],\nthe features generated from the Ô¨Årst stage of CMT can main-\ntain higher resolution,i.e., H/4√óW/4 against H/16√óW/16\nin ViT, which are essential for other dense prediction tasks.\nFurthermore, we adopt the stage-wise architecture design\nsimilar to CNNs [16,46,53] by using four convolutional layer\nwith stride 2, to gradually reduce the resolution (sequence\nlength) and increase the dimension Ô¨Çexibly. The stage-wise\ndesign helps to extract multi-scale features and alleviate the\ncomputation burden caused by high resolution. The local\nperception unit (LPU) and inverted residual feed-forward net-\nwork (IRFFN) in CMT block can help capture both local and\nglobal structure information within the intermediate features\nand promote the representation ability of the network. Fi-\nnally, the average pooling is used to replace the class token in\nViT for better classiÔ¨Åcation results. In addition, we propose\na simple scaling strategy to obtain a family of CMT variants.\nExtensive experiments on ImageNet and other downstream\ntasks demonstrate the superiority of our CMT in terms of\naccuracy and FLOPs. For example, our CMT-S achieves\n83.5% ImageNet top-1 with only 4.0B FLOPs, while being\n14x and 2x less than the best existing DeiT [57] and EfÔ¨Åcient-\nNet [53], respectively. In addition to image classiÔ¨Åcation,\nCMT can also be easily transferred to other vision tasks and\nserve as a versatile backbone. Using CMT-S as the backbone,\nRetinaNet [34] can achieve 44.3% mAP on COCO val2017,\noutperforming the PVT-based RetinaNet [60] by 3.9% with\nless computational cost.\n2. Related Work\nThe computer vision community prospered in past\ndecades riding the wave of deep learning, and the most\npopular deep neural networks are often built upon basic\nblocks, in which a series of convolutional layers are stacked\nsequentially to capture local information within intermedi-\nate features. However, the limited receptive Ô¨Åeld of small\nconvolutional kernels makes it difÔ¨Åcult to obtain global in-\nformation, withholding the networks of high performance on\nchallenging tasks such as classiÔ¨Åcation, object detection, and\nsemantic segmentation. Therefore, many researchers start\nto dig deeper into self-attention based transformers which\nhave the ability to capture long-range information. Here we\nbrieÔ¨Çy review the conventional CNNs and recently proposed\nvision transformers.\nConvolutional neural networks. The Ô¨Årst standard CNN\nwas proposed by LeCun et al. [32] for handwritten charac-\nter recognition, and the past decades have witnessed that\nmany powerful networks [16, 22, 31, 47, 51] achieved un-\nprecedented success on large scale image classiÔ¨Åcation\ntask [8]. AlexNet [31] and VGGNet [47] showed that\na deep neural network composed of convolutional layers\nand pooling layers can obtain adequate results in recogni-\ntion. GoogleNet [51] and InceptionNet [52] demonstrated\nthe effectiveness of multiple paths within a basic block.\nResNet [16] showed better generalization by adding short-\ncut connections every two layers to the base network. To\nalleviate the limited receptive Ô¨Åelds in prior research, some\nresearches [20,21,41,45,59,62] incorporated attention mech-\nanisms as an operator for adaptation between modalities.\nWang et al. [59] proposed to stack attention modules se-\nquentially between the intermediate stages of deep residual\nnetworks. SENet [21] and GENet [20] adaptively recali-\nbrated channel-wise feature responses by modeling interde-\npendencies between channels. NLNet [61] incorporated the\nself-attention mechanism into neural networks, providing\npairwise interactions across all spatial positions to augment\nthe long-range dependencies. In addition to above archi-\ntectural advances, there has also been works [13, 24, 39]\nfocusing on improving over-parameterized deep neural net-\nworks by trading accuracy for efÔ¨Åciency. For example, Mo-\nbileNets [19, 46] and EfÔ¨ÅcientNets [53] both leveraged neu-\nral architecture search (NAS) to design efÔ¨Åcient mobile-size\nnetwork and achieved new state-of-the-art results.\nVision transformers. Since transformers achieved remark-\nable success in natural language processing (NLP) [9, 58],\nmany attempts [6, 10, 12, 14, 33, 36, 55‚Äì57, 60, 63, 67] have\nbeen made to introduce transformer-like architectures to vi-\nsion tasks. The pioneering work ViT [10] directly applied\nthe transformer architecture inherited from NLP to classi-\nÔ¨Åcation with image patches as input. While ViT required\na large private dataset JFT-300M [49] to achieve promis-\ning result, DeiT [57] introduced a new training paradigm to\nextend ViT to a data-efÔ¨Åcient transformer directly trained\non ImageNet-1K. T2T-ViT [68] proposed to embed visual\ntokens by recursively aggregating neighboring tokens into\none token. TNT [68] proposed to model both patch-level and\npixel-level representation by the inner and outer transformer\nblock, respectively. PVT [60] introduced the pyramid struc-\nture into ViT, which can generate multi-scale feature maps\nfor various pixel-level dense prediction tasks. CPVT [6] and\nCvT [63] are the most related to our work which leverage a\nconvolutional projection into conventional transformer block,\nbut we carefully investigate how to maximize the advantage\nof utilizing both CNNs and transformers by studying the\ndifferent components including shortcut and normalization\nfunctions and successfully obtain a more superior result. Be-\nsides, transformers are also used to solve other vision tasks\nsuch as object detection [2, 70], semantic segmentation [69],\nimage retrieval [11], and low-level vision task [3].\nAlthough there are many works successfully applying\ntransformers for vision tasks, they have not shown satisfac-\ntory results compared to conventional CNNs, which are still\nthe primary architectures for vision applications. Transform-\ners are especially good at modeling long-range dependencies\nnecessary for downstream vision tasks. However, locality\nshould also be maintained for visual perception. In this paper,\nwe demonstrate the potential of combining the transformer\nbased network together with convolutional layer, the over-\nall architecture follows the elaborated prior convolutional\nneural networks such as ResNet [16] and EfÔ¨ÅcientNet [53].\n3. Approach\n3.1. Overall Architecture\nOur intention is to build a hybrid network taking the ad-\nvantages of both CNNs and transformers. An overview of\nResNet-50 [16], DeiT [57], and the proposed small version\n(CMT-S) of CMT architectures are presented in Figure 2. As\nshown in Figure 2(b), DeiT directly splits an input image\ninto non-overlapping patches, however, the in-patch struc-\nture information can only be poorly modeled with linear\nprojections. To overcome this limitation, we utilize the stem\narchitecture [17] which has a 3 √ó3 convolution with a stride\nof 2 and an output channel of 32 to reduce the size of input\nimages, followed by another two 3 √ó3 convolutions with\nstride 1 for better local information extraction. Following\nthe design in modern CNNs (e.g., ResNet [16]), our model\nhas four stages to generate feature maps of different scales\nwhich are important for dense prediction tasks. To produce\nthe hierarchical representation, a patch embedding layer con-\nsisting of a convolution and a layer normalization (LN) [1]\nis applied before each stage to reduce the size of interme-\ndiate feature (2x downsampling of resolution), and project\nit to a larger dimension (2x enlargement of dimension). In\neach stage, several CMT blocks are stacked sequentially for\nfeature transformation while retaining the same resolution\nof the input. For example, the ‚ÄúStage 3‚Äù of CMT-S contains\n16 CMT blocks as illustrated in Figure 2(c). The CMT block\nis able to capture both local and long-range dependencies,\nand we will describe it in Sec. 3.2 in details. The model ends\nwith a global average pooling layer, a projection layer, and a\n1000-way classiÔ¨Åcation layer with softmax.\nGiven an input image, we can obtain four hierarchical\nfeature maps with different resolutions, similar to typical\nCNNs such as ResNet [16] and EfÔ¨ÅcientNet [53]. With\nthe above feature maps whose strides are 4, 8, 16, and 32\nwith respect to the input, our CMT can obtain multi-scale\nrepresentations of input images and can be easily applied\nto downstream tasks such as object detection and semantic\nsegmentation.\n3.2. CMT Block\nThe proposed CMT block consists of a local percep-\ntion unit (LPU), a lightweight multi-head self-attention\n(LMHSA) module, and an inverted residual feed-forward\nnetwork (IRFFN), as illustrated in Figure 2(c). We will\ndescribe these three parts in the following.\nLocal Perception Unit. Rotation and shift are two com-\nmonly used data augmentation manners in vision tasks, and\nthese operations should not alter the Ô¨Ånal results of the model.\nIn other words, we expect translation-invariance [27] in those\ntasks. However, the absolute positional encoding used in pre-\nvious transformers, initially designed to leverage the order\nof tokens, damages such invariance because it adds unique\npositional encoding to each patch [6]. Besides, vision trans-\nformers ignore the local relation [38] and the structure in-\nformation [26] inside the patch. To alleviate the limitations,\nwe propose the local perception unit (LPU) to extract local\ninformation, which is deÔ¨Åned as:\nLPU(X) = DWConv(X) +X. (1)\nwhere X ‚àà RH√óW√ód, H √óW is the resolution of the\ninput of current stage, dindicates the dimension of features.\nPosition Embedding\nLinear Projection of Patches\nInput Image\n[class] token\nClassifier\n[class] token\nInput Image\nStem\nAvg Pool\nClassifier\nStage 1\nResidual Block √ó 3\nStage 2\nResidual Block √ó 4\nStage 4\nResidual Block √ó 3\nInput Image\nAvg Pool\n1 √ó 1 Conv\nClassifier\nStage 1\nCMT Block √ó 3\nStage 3\nResidual Block √ó 6\n1 √ó 1 Conv\nBN ReLU\n3 √ó 3 Conv\nBN ReLU\n1 √ó 1 Conv\nBN\nReLU\n3 √ó 3 Conv stride=2\nGELU BN\n3 √ó 3 Conv\nGELU BN\n3 √ó 3 Conv\nGELU BN\nCMT Stem\nStage 2\nCMT Block √ó 3\nStage 3\nCMT Block √ó 16\nStage 4\nCMT Block √ó 3\n2 √ó 2 Conv stride=2\n2 √ó 2 Conv stride=2\n2 √ó 2 Conv stride=2\n2 √ó 2 Conv stride=2\nLayerNorm\nLightweight MHSA\nInverted Residual FFN\nLayerNorm\nLocal Perception Unit\n3√ó3 DW Conv\nMHSA\nMLP\nTransformer Encoder\nTransformer Block √ó 12\nLayerNorm\nLayerNorm\nInverted Residual FFN\nLightweight MHSA\nùëòùëò √ó ùëòùëò DW Conv stride=ùëòùëò\nMHSA\nLinear Linear Linear\nHi √ó Wi √ó Ci\nHi\nùëòùëò √ó Wi\nùëòùëò √ó Ci\nHi\nùëòùëò √ó Wi\nùëòùëò √ó CiHi √ó Wi √ó Ci\nHi √ó Wi √ó Ci\n1√ó1 Conv\n3√ó3 DW Conv\n1√ó1 Conv\nGELU BN\nGELU BN\nBN\n(a) ResNet-50 (b) DeiT-S (ViT-S) (c) CMT-S\nCMT Block\nCMT Stem\nFigure 2. Example of the CMT-S architecture. (a) ResNet-50 [16]. (b) DeiT-S [57] (ViT-S [10]) architecture, where MHSA denotes the\nmulti-head self-attention module. (c) The proposed CMT-S, described in Sec. 3. More details and other variants are shown in Table 1.\nDWConv(¬∑) denotes the depth-wise convolution.\nLightweight Multi-head Self-attention. In original self-\nattention module, the input X ‚àà Rn√ód is linearly trans-\nformed into query Q ‚ààRn√ódk , key K ‚ààRn√ódk , and value\nV ‚ààRn√ódv , where n= H√óW is the number of patches.\nAnd we omit the reshape operation fromH√óW√ódto n√ód\nof tensors in Figure 2(c) for simplicity. The notation d, dk\nand dv are the dimensions of input, key (query) and value,\nrespectively. Then the self-attention module is applied as:\nAttn(Q,K,V) = Softmax(QKT\n‚àödk\n)V. (2)\nTo mitigate the computation overhead, we use a k √ók\ndepth-wise convolution with stride k to reduce the spa-\ntial size of K and V before the attention operation, i.e.,\nK‚Ä≤= DWConv(K) ‚ààR\nn\nk2 √ódk and V‚Ä≤= DWConv(V) ‚àà\nR\nn\nk2 √ódv as shown in Figure 2(c). In addition, we add a rela-\ntive position bias B to each self-attention module, and the\ncorresponding lightweight attention is deÔ¨Åned as:\nLightAttn(Q,K,V) = Softmax(QK‚Ä≤T\n‚àödk\n+ B)V‚Ä≤. (3)\nwhere B ‚ààRn√ón\nk2 is randomly initialized and learnable.\nThe learnt relative position bias can also be easily trans-\nferred to B‚Ä≤ ‚àà Rm1√óm2 with a different size m1 √óm2\nthrough bicubic interpolation, i.e., B‚Ä≤= Bicubic(B). Thus\nit is convenient to Ô¨Åne-tune the proposed CMT for other\ndownstream vision tasks. Finally, the lightweight multi-head\nself-attention (LMHSA) module is deÔ¨Åned by considering h\n‚Äúheads‚Äù,i.e., hLightweightAttention functions are applied to\nthe input. Each head outputs a sequence of sizen√ód\nh. These\nhsequences are then concatenated into a n√ódsequence.\nInverted Residual Feed-forward Network. The original\nFFN proposed in ViT [10] is composed of two linear layers\nseparated by a GELU activation [18]. The Ô¨Årst layer expands\nthe dimension by a factor of 4, and the second layer reduces\nthe dimension by the same ratio:\nFFN(X) = GELU(XW1 + b1)W2 + b2. (4)\nwhere W1 ‚ààRd√ó4d and W2 ‚ààR4d√ód indicate weights of\nthe two linear layers, respectively. The notation b1 and b2\nare the bias terms. Figure 2(c) provides a schematic vi-\nsualization of our design. The proposed inverted residual\nfeed-forward network (IRFFN) appears similar to inverted\nresidual block [46] consisting of an expansion layer followed\nby a depth-wise convolution and a projection layer. Specif-\nically, we change the location of shortcut connection for\nbetter performance:\nIRFFN(X) = Conv(F(Conv(X))), (5)\nF(X) = DWConv(X) +X. (6)\nwhere the activation layer is omitted. We also include the\nbatch normalization after the activation layer and the last\nlinear layer according to [46]. The depth-wise convolution\nis used to extract local information with negligible extra\ncomputational cost. The motivation for inserting shortcut\nis similar to that of classic residual networks, which can\npromote the propagation ability of gradient across layers.\nWe show that such shortcut helps the network achieve better\nresults in our experiments.\nWith the aforementioned three components, the CMT\nblock can be formulated as:\nYi = LPU(Xi‚àí1), (7)\nZi = LMHSA(LN(Yi)) +Yi, (8)\nXi = IRFFN(LN(Zi)) +Zi. (9)\nwhere Yi and Zi denote the output features of LPU and\nLMHSA module for the i-th block, respectively. LN denotes\nthe layer normalization [1]. We stack several CMT blocks in\neach stage for feature transformation and aggregation.\n3.3. Complexity Analysis\nWe analyze the computational cost between standard\nViT [10] and our CMT in this section. A standard trans-\nformer block consists of a MHSA module and a FFN. Given\nan input feature of size n√ód, the computational complexity\n(FLOPs) can be calculated as:\nO(MHSA) = 2nd(dk + dv) +n2(dk + dv), (10)\nO(FFN) = 2nd2r, (11)\nwhere ris the expansion ratio of FFN, dk and dv are dimen-\nsions of key and value, respectively. More speciÔ¨Åcally, ViT\nsets d= dk = dv and r= 4, the cost can be simpliÔ¨Åed as:\nO(Transformer block) =O(MHSA) +O(FFN)\n= 12nd2 + 2n2d\n(12)\nUnder above setting, the FLOPs of CMT block is as follows:\nO(LPU) = 9nd, (13)\nO(LMHSA) = 2nd2(1 + 1/k2) + 2n2d/k2, (14)\nO(IRFFN) = 8nd2 + 36nd, (15)\nO(CMT block) =O(LPU) +O(LMHSA) +O(IRFFN)\n= 10nd2(1 + 0.2/k2) + 2n2d/k2 + 45nd,\n(16)\nwhere k‚â•1 is the reduction ratio in LMHSA. Compared to\nstandard transformer block, the CMT block is more friendly\nto computational cost, and is easier to process the feature\nmap under higher resolution (larger n).\n3.4. Scaling Strategy\nInspired by [53], we propose a new compound scaling\nstrategy suitable for transformer-based networks, which uses\na compound coefÔ¨Åcient œÜto uniformly scale the number of\nlayers (depth), dimensions, and input resolution in a princi-\npled way:\ndepth :Œ±œÜ, dimension :Œ≤œÜ, resolution :Œ≥œÜ,\ns.t. Œ± ¬∑Œ≤1.5 ¬∑Œ≥2 ‚âà2.5, Œ± ‚â•1,Œ≤ ‚â•1,Œ≥ ‚â•1\n(17)\nwhere Œ±, Œ≤, and Œ≥are constants determined by grid search\nto decide how to assign resources to network depth, dimen-\nsion and input resolution, respectively. Intuitively,œÜis the\ncoefÔ¨Åcient that controls how many more ( œÜ ‚â•1) or less\n(œÜ‚â§‚àí1) resources are available for model scaling. Notably,\nthe FLOPs of the proposed CMT block is approximately\nproportional1 to Œ±, Œ≤1.5, and Œ≥2 according to E.q. 16. And\nwe constraint Œ±¬∑Œ≤1.5 ¬∑Œ≥2 ‚âà2.5 so that for a given newœÜ, the\ntotal FLOPS will approximately increase by 2.5œÜ. This will\nstrike a balance between the increase of computational cost\nand performance gain. In our experiments, we empirically\nset Œ±=1.2, Œ≤=1.3, and Œ≥=1.15.\nWe build our model CMT-S to have similar model\nsize and computation complexity with DeiT-S (ViT-S) and\nEfÔ¨ÅcientNet-B4. We also introduce CMT-Ti, CMT-XS and\nCMT-B according to the proposed scaling strategy. The input\nresolutions are 1602, 1922, 2242, and 2562 for all four mod-\nels, respectively. The detailed architecture hyper-parameters\nare shown in Table 1.\n4. Experiments\nIn this section, we investigate the effectiveness of CMT\narchitecture by conducting experiments on several tasks in-\ncluding image classiÔ¨Åcation, object detection, and instance\nsegmentation. We Ô¨Årst compare the proposed CMT with\nprevious state-of-the-art models on above tasks, and then\nablate the important elements of CMT.\n4.1. ImageNet ClassiÔ¨Åcation\nExperimental Settings. ImageNet [8] is a image classiÔ¨Åca-\ntion benchmark which contains 1.28M training images and\n50K validation images of 1000 classes. For fair comparisons\nwith recent works, we adopt the same training and augmen-\ntation strategy as that in DeiT [57], i.e., models are trained\nfor 300 epochs (800 for CMT-Ti that requires more epochs\nto converge) using the AdamW [37] optimizer. All models\nare trained on 8 NVIDIA Tesla V100 GPUs.\nResults of CMT. Table 2 shows the performances of the\nproposed CMTs that are scaled from the CMT-S according\nto E.q. 17. Our models achieve better accuracy with fewer\nparameters and FLOPs compared to other convolution-based\nand transformer-based counterparts. In particular, our CMT-\nS achieves 83.5% top-1 accuracy with 4.0B FLOPs, which is\n1The precious proportion is associated with n and d. For example,\nCMT-S has n=3136 ‚â´ d=64 in ‚Äústage 1‚Äù andn=49 ‚â™ d=512 in ‚Äústage\n4‚Äù. The above proportion can already generate good variants for CMT.\nTable 1. Architectures for ImageNet classiÔ¨Åcation. The output size corresponds to the input resolution of 224 √ó224. Convolutions and\nCMT blocks are shown in brackets with the number of stacked blocks (see also Figure 2(c)). Hi and ki are the number of heads and\nreduction rates in LMHSA of stage i, respectively. Ri denotes the expansion ratio in IRFFN of stage i.\nOutput Size Layer Name CMT-Ti CMT-XS CMT-S CMT-B\n112 √ó112 Stem 3 √ó3, 16, stride 2\n[3 √ó3, 16] √ó2\n3 √ó3, 16, stride 2\n[3 √ó3, 16] √ó2\n3 √ó3, 32, stride 2\n[3 √ó3, 32] √ó2\n3 √ó3, 38, stride 2\n[3 √ó3, 38] √ó2\n56 √ó56 Patch Embedding 2 √ó2, 46, stride 2 2 √ó2, 52, stride 2 2 √ó2, 64, stride 2 2 √ó2, 76, stride 2\nStage 1\nLPU\nLMHSA\nIRFFN\n[ 3 √ó3, 46\nH1=1, k1=8\nR1=3.6\n]\n√ó2\n[ 3 √ó3, 52\nH1=1, k1=8\nR1=3.8\n]\n√ó3\n[ 3 √ó3, 64\nH1=1, k1=8\nR1=4\n]\n√ó3\n[ 3 √ó3, 76\nH1=1, k1=8\nR1=4\n]\n√ó4\n28 √ó28 Patch Embedding 2 √ó2, 92, stride 2 2 √ó2, 104, stride 2 2 √ó2, 128, stride 2 2 √ó2, 152, stride 2\nStage 2\nLPU\nLMHSA\nIRFFN\n[ 3 √ó3, 92\nH2=2, k2=4\nR2=3.6\n]\n√ó2\n[ 3 √ó3, 104\nH2=2, k2=4\nR2=3.8\n]\n√ó3\n[ 3 √ó3, 128\nH2=2, k2=4\nR2=4\n]\n√ó3\n[ 3 √ó3, 152\nH2=2, k2=4\nR2=4\n]\n√ó4\n14 √ó14 Patch Embedding 2 √ó2, 184, stride 2 2 √ó2, 208, stride 2 2 √ó2, 256, stride 2 2 √ó2, 304, stride 2\nStage 3\nLPU\nLMHSA\nIRFFN\n[ 3 √ó3, 184\nH3=4, k3=2\nR3=3.6\n]\n√ó10\n[ 3 √ó3, 208\nH3=4, k3=2\nR3=3.8\n]\n√ó12\n[ 3 √ó3, 256\nH3=4, k3=2\nR3=4\n]\n√ó16\n[ 3 √ó3, 304\nH3=4, k3=2\nR3=4\n]\n√ó20\n7 √ó7 Patch Embedding 2 √ó2, 368, stride 2 2 √ó2, 416, stride 2 2 √ó2, 512, stride 2 2 √ó2, 608, stride 2\nStage 4\nLPU\nLMHSA\nIRFFN\n[ 3 √ó3, 368\nH4=8, k4=1\nR4=3.6\n]\n√ó2\n[ 3 √ó3, 416\nH4=8, k4=1\nR4=3.8\n]\n√ó3\n[ 3 √ó3, 512\nH4=8, k4=1\nR4=4\n]\n√ó3\n[ 3 √ó3, 608\nH4=8, k4=1\nR4=4\n]\n√ó4\n1 √ó1 Projection 1 √ó1, 1280\n1 √ó1 ClassiÔ¨Åer Fully Connected Layer, 1000\n# Params 9.49 M 15.24 M 25.14 M 45.72 M\n# FLOPs 0.64 B 1.54 B 4.04 B 9.33 B\n3.7% higher than the baseline model DeiT-S [57] and 2.0%\nhigher than CPVT [6], indicating the beneÔ¨Åt of CMT block\nfor capturing both local and global information. Note that\nall previous transformer-based models are still inferior to\nEfÔ¨ÅcientNet [53] which is obtained via a thorough archi-\ntecture search, however, our CMT-S is 0.6% higher than\nEfÔ¨ÅcientNet-B4 with less computational cost, which demon-\nstrates the efÔ¨Åcacy of the proposed hybrid structure and show\nstrong potential for further improvement. We also plot the\naccuracy-FLOPs curve in Figure 1(a) to have an intuitive\ncomparison between these models. We can see that CMTs\nconsistently outperform other models by a large margin.\n4.2. Ablation Study\nTable 3. Ablation study of stage-\nwise architecture on ImageNet.\nModel Params FLOPs Top-1\nDeiT-S 22M 4.6B 79.8%\nDeiT-S-4Stage 25M 3.7B 81.4%\nStage-wise architec-\nture. Transformer-\nbased ViT/DeiT can\nonly generate single-\nscale feature map, los-\ning a lot of multi-\nscale information crucial for dense prediction tasks. We\nchange the columnar DeiT-S to hierarchical DeiT-S-4Stage,\nwhich has 4 stages like CMT-S in Table 1, but maintains the\noriginal FFN. We also change MHSA to LMHSA to reduce\ncomputational cost. As shown in Table 3, DeiT-S-4Stage\noutperforms DeiT-S by 1.6% with less FLOPs, demonstrat-\ning that the widely-adopted stage-wise design in CNNs is a\nbetter choice for promoting transformer-based architecture.\nCMT block. Ablations on different modules in CMT are\nshown in Table 5. DeiT-S-4Stage has 4 patch embedding\nlayers (the Ô¨Årst is a 4√ó4 convolution with stride 4). ‚Äú+ Stem‚Äù\nindicates that we add the CMT stem into the network and\nreplace the Ô¨Årst patch embedding layer with a 2 √ó2 convo-\nlution with stride 2. The improvement shows the beneÔ¨Åt of\nthe convolution-based stem. Besides, the proposed LPU and\nIRFFN can further boost the network by 0.8% and 0.6%, re-\nspectively. It is worth noticing that the shortcut connections\nin LPU and IRFFN are also crucial for the Ô¨Ånal performance.\nTable 5. Ablations of CMT block.\nModel Params FLOPs Top-1\nDeiT-S-4Stage 25M 3.7B 81.4%\n+ Stem 25M 3.9B 81.9%\n+ LPU 25M 3.9B 82.7%\nw/o shortcut 25M 3.9B 82.0%\n+ IRFFN 25M 3.9B 83.3%\nw/o shortcut 25M 3.9B 82.5%\n+ Projection 25M 4.0B 83.5%\nNormalization func-\ntion. Transformer-\nbased models usually\nuse LN [1] inherited\nfrom NLP. However,\nconvolution-based\nmodels usually utilize\nbatch normalization\n(BN) [25] to stabilize\nthe training. CMT maintains the LN before LMHSA and\nTable 2. ImageNet Results of CMT. CNNs and transformers with similar accuracy are grouped together for comparison. The proposed\nCMTs consistently outperform other methods with less computational cost.\nModel Top-1 Acc. Top-5 Acc. Throughput # Params Resolution # FLOPs Ratio\nCPVT-Ti-GAP [6] 74.9% - - 6M 2242 1.3B 2.6 √ó\nDenseNet-169 [22] 76.2% 93.2% - 14M 2242 3.5B 7 √ó\nEfÔ¨ÅcientNet-B1 [53] 79.1% 94.4% - 7.8M 2402 0.7B 1.2 √ó\nCMT-Ti 79.1% 94.5% 1323.5 9.5M 1602 0.6B 1 √ó\nResNet-50 [16] 76.2% 92.9% - 25.6M 2242 4.1B 2.7 √ó\nCoaT-Lite Mini [65] 78.9% - - 11M 2242 2.0B 1.3 √ó\nDeiT-S [57] 79.8% - 940.4 22M 2242 4.6B 3.1 √ó\nEfÔ¨ÅcientNet-B3 [53] 81.6% 95.7% 732.1 12M 3002 1.8B 1.2 √ó\nCMT-XS 81.8% 95.8% 857.4 15.2M 1922 1.5B 1 √ó\nResNeXt-101-64x4d [64] 80.9% 95.6% - 84M 2242 32B 8 √ó\nT2T-ViT-19 [68] 81.2% - - 39.0M 2242 8.0B 2 √ó\nPVT-M [60] 81.2% - 528.1 44.2M 2242 6.7B 1.7 √ó\nSwin-T [36] 81.3% - 755.2 29M 2242 4.5B 1.1 √ó\nCPVT-S-GAP [6] 81.5% - - 23M 2242 4.6B 1.2 √ó\nRegNetY-8GF [44] 81.7% - 591.6 39.2M 2242 8.0B 2 √ó\nCeiT-S [67] 82.0% 95.9% - 24.2M 2242 4.5B 1.1 √ó\nEfÔ¨ÅcientNet-B4 [53] 82.9% 96.4% 349.4 19M 3802 4.2B 1 √ó\nTwins-SVT-B [5] 83.1% - - 56.0M 2242 8.3B 2.1 √ó\nCMT-S 83.5% 96.6% 562.5 25.1M 2242 4.0B 1 √ó\nViT-B/16‚Üë384 [10] 77.9% - 85.9 85.8M 3842 77.9B 8.4 √ó\nTNT-B [14] 82.8% 96.3% - 65.6M 2242 14.1B 1.5 √ó\nDeiT-B‚Üë384 [57] 83.1% - 85.9 85.8M 3842 55.6B 6.0 √ó\nCvT-21‚Üë384 [63] 83.3% - - 31.5M 3842 24.9B 2.7 √ó\nSwin-B [36] 83.3% - 278.1 88M 2242 15.4B 1.5 √ó\nTwins-SVT-L [5] 83.3% - 288.0 99.2M 2242 14.8B 1.7 √ó\nCeiT-S‚Üë384 [67] 83.3% 96.5% - 24.2M 3842 12.9B 1.4 √ó\nBoTNet-S1-128 [48] 83.5% 96.5% - 75.1M 2562 19.3B 2.1 √ó\nEfÔ¨ÅcientNetV2-S [54] 83.9% - - 22M 2242 8.8B 1 √ó\nEfÔ¨ÅcientNet-B6 [53] 84.0% 96.8% 96.9 43M 5282 19.2B 2.0 √ó\nCMT-B 84.5% 96.9% 285.4 45.7M 2562 9.3B 1 √ó\nEfÔ¨ÅcientNet-B7 [53] 84.3% 97.0% 55.1 66M 6002 37B 1.9 √ó\nCMT-L 84.8% 97.1% 150.4 74.7M 2882 19.5B 1 √ó\nTable 4. Ablation study of the scaling strategy.\nModel (based on CMT-S) FLOPs Top-1 FLOPs Top-1\nScale: Œ±=2.2 (depth only) 1.7B (œÜ=-1) 80.8% 8.6B(œÜ=1) 83.4%\nScale: Œ≤=1.6 (dimension only) 1.7B (œÜ=-1) 81.3% 9.3B(œÜ=1) 83.8%\nScale: Œ±=1.3, Œ≤=1.3, Œ≥=1.15 1.5B (œÜ=-1) 81.8% 9.3B(œÜ=1) 84.5%\nIRFFN, and inserts BN after the convolutional layer. If all\nLNs are replaced by BNs, the model cannot converge during\ntraining. If all BNs are replaced by LNs, the performance of\nCMT-S drops to 83.0%, indicating that proper application of\nnormalization functions can improve the Ô¨Ånal performance.\nScaling strategy. Table 4 shows the ImageNet results of\nCMT architecture under different scaling strategies. Unidi-\nmensional scaling strategies are signiÔ¨Åcantly inferior to the\nproposed compound scaling strategy, especially for depth-\nonly scaling strategy which leads to a even worse result\nof 83.4% against 83.8% of the original CMT-S, when the\nnetwork is scaled up.\n4.3. Transfer Learning\n4.3.1 Object Detection and Instance Segmentation\nExperimental Settings. The experiments are conducted on\nCOCO [35], which contains 118K training images and 5K\nvalidation images of 80 classes. We evaluate the proposed\nCMT-S using two typical framework: RetinaNet [34] and\nMask R-CNN [15] for object detection and instance seg-\nmentation, respectively. SpeciÔ¨Åcally, we replace the original\nbackbone with our CMT-S to build new detectors. All mod-\nels are trained under standard single-scale and ‚Äú1x‚Äù schedule\n(12 epochs) following PVT [60].\nResults of CMT. We report the performance comparison\nresults of object detection task and instance segmentation\ntask in Table 6 and Table 7 respectively. For object detection\nwith RetinaNet as basic framework, CMT-S outperforms\nTwins-PCPVT-S [5] with 1.3% mAP and Twins-SVT-S [5]\nTable 6. Object detection results on COCO val2017. All models use RetinaNet [34] as basic framework and are trained in ‚Äú1x‚Äù schedule.\nFLOPs are calculated on 1280√ó800 input. ‚Ä†means the results are from [5].\nBackbone # Params # FLOPs mAP AP50 AP75 APS APM APL\nConT-M [66] 27.0M 217B 37.9 58.1 40.2 23.0 40.6 50.4\nResNet-101 [16] 56.7M 315B 38.5 57.6 41.0 21.7 42.8 50.4\nRelationNet++ [4] 39.0M 266B 39.4 58.2 42.5 - - -\nResNeXt-101-32x4d [64] 56.4M 319B 39.9 59.6 42.7 22.3 44.2 52.5\nPVT-S [60] 34.2M 226B 40.4 61.3 43.0 25.0 42.9 55.7\nSwin-T‚Ä† [36] 38.5M 245B 41.5 62.1 44.2 25.1 44.9 55.5\nTwins-SVT-S [5] 34.3M 209B 42.3 63.4 45.2 26.0 45.5 56.5\nTwins-PCPVT-S [5] 34.4M 226B 43.0 64.1 46.0 27.5 46.3 57.3\nCMT-S (ours) 44.3M 231B 44.3 65.5 47.5 27.1 48.3 59.1\nTable 7. Instance segmentation results on COCO val2017. All models use Mask R-CNN [15] as basic framework and are trained in ‚Äú1x‚Äù\nschedule. FLOPs are calculated on 1280√ó800 input. ‚Ä†means the results are from [5].\nBackbone # Params # FLOPs APbox APbox\n50 APbox\n75 APmask APmask\n50 APmask\n75\nResNet-101 [16] 63.2M 336B 40.0 60.5 44.0 36.1 57.5 38.6\nPVT-S [60] 44.1M 245B 40.4 62.9 43.8 37.8 60.1 40.3\nResNeXt-101-32x4d [64] 62.8M 340B 41.9 62.5 45.9 37.5 59.4 40.2\nSwin-T‚Ä† [36] 47.8M 264B 42.2 64.6 46.2 39.1 61.6 42.0\nTwins-SVT-S [5] 44.0M 228B 42.7 65.6 46.7 39.6 62.5 42.6\nTwins-PCPVT-S [5] 44.3M 245B 42.9 65.8 47.1 40.0 62.7 42.9\nCMT-S (ours) 44.5M 249B 44.6 66.8 48.9 40.7 63.9 43.4\nTable 8. Transfer Learning Results. Models are Ô¨Åne-tuned with the ImageNet pretrained checkpoints. ‚Ä†means the results are from [28].\nModel # Params # FLOPs CIFAR10 CIFAR100 Cars Flowers Pets\nResNet-152‚Ä† [16] 58.1M 11.3B 97.9% 87.6% 92.0% 97.4% 94.5%\nInception-v4‚Ä† [50] 41.1M 16.1B 97.9% 87.5% 93.3% 98.5% 93.7%\nEfÔ¨ÅcientNet-B7‚Üë600 [53] 64.0M 37.2B 98.9% 91.7% 94.7% 98.8% 95.4%\nViT-B/16‚Üë384 [10] 85.8M 17.6B 98.1% 87.1% - 89.5% 93.8%\nDeiT-B [57] 85.8M 17.6B 99.1% 90.8% 92.1% 98.4% -\nCeiT-S‚Üë384 [67] 24.2M 12.9B 99.1% 90.8% 94.1% 98.6% 94.9%\nTNT-S‚Üë384 [14] 23.8M 17.3B 98.7% 90.1% - 98.8% 94.7%\nCMT-S (ours) 25.1M 4.04B 99.2% 91.7% 94.4% 98.7% 95.2%\nwith 2.0% mAP. For instance segmentation with Mask R-\nCNN as basic framework, CMT-S surpasses Twins-PCPVT-\nS [5] with 1.7% AP and Twins-SVT-S [5] with 1.9% AP.\nWe also report the inference speed on COCO val2017 with\n1280√ó800 input, CMT-S based RetinaNet and Mask R-CNN\nachieve 14.8 FPS and 11.2 FPS, respectively.\n4.3.2 Other Vision Tasks\nWe also evaluate the proposed CMT on Ô¨Åve commonly\nused transfer learning datasets, including CIFAR10 [30],\nCIFAR100 [30], Standford Cars [29], Flowers [40], and\nOxford-IIIT Pets [42] (see Appendix for more details). We\nÔ¨Åne-tune the ImageNet pretrained models on new datasets\nfollowing [14, 53]. Table 8 shows the corresponding results.\nCMT-S outperforms other transformer-based models in all\ndatasets with less FLOPs, and achieves comparable perfor-\nmance against EfÔ¨ÅcientNet-B7 [53] with 9x less FLOPs,\nwhich demonstrates the superiority of CMT architecture.\n5. Conclusion\nThis paper proposes a novel hybrid architecture named\nCMT for visual recognition and other downstream computer\nvision tasks such as object detection and instance segmenta-\ntion, and addresses the limitations of utilizing transformers\nin a brutal force manner in the Ô¨Åeld of computer vision. The\nproposed CMT architectures take advantages of both CNNs\nand transformers to capture local and global information,\npromoting the representation ability of the network. In ad-\ndition, a scaling strategy is proposed to generate a family\nof CMT variants for different resource constraints. Exten-\nsive experiments on ImageNet and other downstream vision\ntasks demonstrate the effectiveness and superiority of the\nproposed CMT architecture.\nAcknowledgment Chang Xu was supported by the Aus-\ntralian Research Council under Project DP210101859 and\nthe University of Sydney SOAR Prize.\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv preprint arXiv:1607.06450, 2016.\n3, 5, 6\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European Confer-\nence on Computer Vision, 2020. 1, 3\n[3] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping\nDeng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and\nWen Gao. Pre-trained image processing transformer. arXiv\npreprint arXiv:2012.00364, 2020. 1, 3\n[4] Cheng Chi, Fangyun Wei, and Han Hu. Relationnet++: Bridg-\ning visual representations for object detection via transformer\ndecoder. arXiv preprint arXiv:2010.15831, 2020. 2, 8\n[5] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing\nRen, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Re-\nvisiting spatial attention design in vision transformers. arXiv\npreprint arXiv:2104.13840, 2021. 2, 7, 8\n[6] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and\nHuaxia Xia. Conditional positional encodings for vision\ntransformers. arXiv preprint arXiv:2102.10882, 2021. 2, 3, 6,\n7\n[7] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,\nStefan Roth, and Bernt Schiele. The cityscapes dataset for\nsemantic urban scene understanding. In Proceedings of the\nIEEE conference on computer vision and pattern recognition,\n2016. 1\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li\nFei-Fei. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, 2009. 2, 5\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In arXiv preprint\narXiv:1810.04805, 2018. 1, 3\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 2, 3, 4, 5, 7, 8\n[11] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\nHerv¬¥e J¬¥egou. Training vision transformers for image retrieval.\narXiv preprint arXiv:2102.05644, 2021. 3\n[12] Jianyuan Guo, Yehui Tang, Kai Han, Xinghao Chen, Han\nWu, Chao Xu, Chang Xu, and Yunhe Wang. Hire-mlp:\nVision mlp via hierarchical rearrangement. arXiv preprint\narXiv:2108.13341, 2021. 3\n[13] Kai Han, Yunhe Wang, Qi Tian, Jianyuan Guo, Chunjing\nXu, and Chang Xu. Ghostnet: More features from cheap\noperations. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2020. 3\n[14] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer. arXiv preprint\narXiv:2103.00112, 2021. 3, 7, 8\n[15] Kaiming He, Georgia Gkioxari, Piotr Doll ¬¥ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, 2017. 7, 8\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, 2016. 1, 2, 3, 4, 7, 8\n[17] Tong He, Zhi Zhang, Hang Zhang, Zhongyue Zhang, Jun-\nyuan Xie, and Mu Li. Bag of tricks for image classiÔ¨Åca-\ntion with convolutional neural networks. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2019. 3\n[18] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (gelus). arXiv preprint arXiv:1606.08415, 2016. 4\n[19] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh\nChen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu,\nRuoming Pang, Vijay Vasudevan, et al. Searching for mo-\nbilenetv3. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2019. 3\n[20] Jie Hu, Li Shen, Samuel Albanie, Gang Sun, and Andrea\nVedaldi. Gather-excite: Exploiting feature context in convo-\nlutional neural networks. arXiv preprint arXiv:1810.12348,\n2018. 2\n[21] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation\nnetworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2018. 2\n[22] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-\nian Q Weinberger. Densely connected convolutional networks.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 2017. 2, 7\n[23] Mindspore. Huawei. https://www.mindspore.cn/.\n2020. 1\n[24] Forrest N Iandola, Song Han, Matthew W Moskewicz, Khalid\nAshraf, William J Dally, and Kurt Keutzer. Squeezenet:\nAlexnet-level accuracy with 50x fewer parameters and¬° 0.5\nmb model size. arXiv preprint arXiv:1602.07360, 2016. 3\n[25] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. In International conference on machine learning,\n2015. 6\n[26] Md Amirul Islam, Sen Jia, and Neil DB Bruce. How much po-\nsition information do convolutional neural networks encode?\narXiv preprint arXiv:2001.08248, 2020. 3\n[27] Osman Semih Kayhan and Jan C van Gemert. On translation\ninvariance in cnns: Convolutional layers can exploit absolute\nspatial location. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2020. 3\n[28] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do\nbetter imagenet models transfer better? In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2019. 8\n[29] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d\nobject representations for Ô¨Åne-grained categorization. In Pro-\nceedings of the IEEE international conference on computer\nvision workshops, 2013. 8\n[30] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple\nlayers of features from tiny images. 2009. 8\n[31] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Im-\nagenet classiÔ¨Åcation with deep convolutional neural networks.\nAdvances in neural information processing systems, 2012. 2\n[32] Yann LeCun, L ¬¥eon Bottou, Yoshua Bengio, and Patrick\nHaffner. Gradient-based learning applied to document recog-\nnition. Proceedings of the IEEE, 1998. 2\n[33] Wenshuo Li, Hanting Chen, Jianyuan Guo, Ziyang Zhang,\nand Yunhe Wang. Brain-inspired multilayer perceptron with\nspiking neurons. arXiv preprint arXiv:2203.14679, 2022. 3\n[34] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll¬¥ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE international conference on computer\nvision, 2017. 2, 7, 8\n[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll¬¥ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean conference on computer vision. Springer. 1, 2, 7\n[36] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. arXiv\npreprint arXiv:2103.14030, 2021. 2, 3, 7, 8\n[37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101, 2017. 5\n[38] David G Lowe. Object recognition from local scale-invariant\nfeatures. In Proceedings of the seventh IEEE international\nconference on computer vision, 1999. 3\n[39] Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun.\nShufÔ¨Çenet v2: Practical guidelines for efÔ¨Åcient cnn architec-\nture design. In Proceedings of the European conference on\ncomputer vision (ECCV), 2018. 3\n[40] Maria-Elena Nilsback and Andrew Zisserman. Automated\nÔ¨Çower classiÔ¨Åcation over a large number of classes. In 2008\nSixth Indian Conference on Computer Vision, Graphics &\nImage Processing, 2008. 8\n[41] Jongchan Park, Sanghyun Woo, Joon-Young Lee, and In So\nKweon. Bam: Bottleneck attention module. arXiv preprint\narXiv:1807.06514, 2018. 2\n[42] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, and\nCV Jawahar. Cats and dogs. In 2012 IEEE conference on\ncomputer vision and pattern recognition, 2012. 8\n[43] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer,\nJames Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An im-\nperative style, high-performance deep learning library. arXiv\npreprint arXiv:1912.01703, 2019. 1\n[44] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaim-\ning He, and Piotr Doll¬¥ar. Designing network design spaces.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020. 2, 7\n[45] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-\nalone self-attention in vision models. arXiv preprint\narXiv:1906.05909, 2019. 2\n[46] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zh-\nmoginov, and Liang-Chieh Chen. Mobilenetv2: Inverted\nresiduals and linear bottlenecks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2018.\n1, 2, 3, 4, 5\n[47] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 1, 2\n[48] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon\nShlens, Pieter Abbeel, and Ashish Vaswani. Bottle-\nneck transformers for visual recognition. arXiv preprint\narXiv:2101.11605, 2021. 7\n[49] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhi-\nnav Gupta. Revisiting unreasonable effectiveness of data in\ndeep learning era. In Proceedings of the IEEE international\nconference on computer vision, 2017. 3\n[50] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and\nAlexander Alemi. Inception-v4, inception-resnet and the\nimpact of residual connections on learning. In Proceedings\nof the AAAI Conference on ArtiÔ¨Åcial Intelligence, 2017. 8\n[51] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2015. 2\n[52] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon\nShlens, and Zbigniew Wojna. Rethinking the inception ar-\nchitecture for computer vision. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2016.\n1, 2\n[53] Mingxing Tan and Quoc Le. EfÔ¨Åcientnet: Rethinking model\nscaling for convolutional neural networks. In International\nConference on Machine Learning, 2019. 1, 2, 3, 5, 6, 7, 8\n[54] Mingxing Tan and Quoc Le. EfÔ¨Åcientnetv2: Smaller models\nand faster training. In International Conference on Machine\nLearning, 2021. 7\n[55] Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Yanxi Li,\nChao Xu, and Yunhe Wang. An image patch is a wave: Phase-\naware vision mlp. arXiv preprint arXiv:2111.12294, 2021.\n3\n[56] Yehui Tang, Kai Han, Yunhe Wang, Chang Xu, Jianyuan Guo,\nChao Xu, and Dacheng Tao. Patch slimming for efÔ¨Åcient\nvision transformers. arXiv preprint arXiv:2106.02852, 2021.\n3\n[57] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Herv¬¥e J¬¥egou. Training\ndata-efÔ¨Åcient image transformers & distillation through atten-\ntion. arXiv preprint arXiv:2012.12877, 2020. 1, 2, 3, 4, 5, 6,\n7, 8\n[58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In arXiv preprint\narXiv:1706.03762, 2017. 1, 3\n[59] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li,\nHonggang Zhang, Xiaogang Wang, and Xiaoou Tang. Resid-\nual attention network for image classiÔ¨Åcation. In Proceedings\nof the IEEE conference on computer vision and pattern recog-\nnition, 2017. 2\n[60] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense predic-\ntion without convolutions. arXiv preprint arXiv:2102.12122,\n2021. 2, 3, 7, 8\n[61] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming\nHe. Non-local neural networks. In Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2018.\n2\n[62] Sanghyun Woo, Jongchan Park, Joon-Young Lee, and In So\nKweon. Cbam: Convolutional block attention module. In\nProceedings of the European conference on computer vision\n(ECCV), 2018. 2\n[63] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang\nDai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions\nto vision transformers. arXiv preprint arXiv:2103.15808 ,\n2021. 2, 3, 7\n[64] Saining Xie, Ross Girshick, Piotr Doll¬¥ar, Zhuowen Tu, and\nKaiming He. Aggregated residual transformations for deep\nneural networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 2017. 2, 7, 8\n[65] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-\nscale conv-attentional image transformers. arXiv preprint\narXiv:2104.06399, 2021. 7\n[66] Haotian Yan, Zhe Li, Weijian Li, Changhu Wang, Ming\nWu, and Chuang Zhang. Contnet: Why not use convo-\nlution and transformer at the same time? arXiv preprint\narXiv:2104.13497, 2021. 2, 8\n[67] Kun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Fengwei\nYu, and Wei Wu. Incorporating convolution designs into\nvisual transformers. arXiv preprint arXiv:2103.11816, 2021.\n2, 3, 7, 8\n[68] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 2, 3, 7\n[69] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xi-\nang, Philip HS Torr, et al. Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers.\narXiv preprint arXiv:2012.15840, 2020. 1, 3\n[70] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020. 1, 3",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8577706813812256
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7718896865844727
    },
    {
      "name": "FLOPS",
      "score": 0.7671645283699036
    },
    {
      "name": "Computer science",
      "score": 0.7282161712646484
    },
    {
      "name": "Artificial intelligence",
      "score": 0.543434739112854
    },
    {
      "name": "Artificial neural network",
      "score": 0.45966872572898865
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3785710334777832
    },
    {
      "name": "Machine learning",
      "score": 0.3704344630241394
    },
    {
      "name": "Parallel computing",
      "score": 0.13749796152114868
    },
    {
      "name": "Voltage",
      "score": 0.11013078689575195
    },
    {
      "name": "Engineering",
      "score": 0.10772299766540527
    },
    {
      "name": "Electrical engineering",
      "score": 0.08032560348510742
    }
  ]
}