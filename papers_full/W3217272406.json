{
  "title": "CycleTransGAN-EVC: A CycleGAN-based Emotional Voice Conversion Model with Transformer",
  "url": "https://openalex.org/W3217272406",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5085633547",
      "name": "Changzeng Fu",
      "affiliations": [
        "The University of Osaka"
      ]
    },
    {
      "id": "https://openalex.org/A5101905297",
      "name": "Chaoran Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5000037110",
      "name": "Carlos Toshinori Ishi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101425832",
      "name": "Hiroshi Ishiguro",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2902070858",
    "https://openalex.org/W2946555236",
    "https://openalex.org/W3170261818",
    "https://openalex.org/W3015336668",
    "https://openalex.org/W3004402693",
    "https://openalex.org/W2012086895",
    "https://openalex.org/W3147311044",
    "https://openalex.org/W2185573909",
    "https://openalex.org/W2471520273",
    "https://openalex.org/W3136699727",
    "https://openalex.org/W3030361580",
    "https://openalex.org/W1974745215",
    "https://openalex.org/W2511640485",
    "https://openalex.org/W2899361462",
    "https://openalex.org/W2077801020",
    "https://openalex.org/W3019173729",
    "https://openalex.org/W2105160541",
    "https://openalex.org/W3095930733",
    "https://openalex.org/W3034420534",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1588037970",
    "https://openalex.org/W2404839462"
  ],
  "abstract": "In this study, we explore the transformer's ability to capture intra-relations among frames by augmenting the receptive field of models. Concretely, we propose a CycleGAN-based model with the transformer and investigate its ability in the emotional voice conversion task. In the training procedure, we adopt curriculum learning to gradually increase the frame length so that the model can see from the short segment till the entire speech. The proposed method was evaluated on the Japanese emotional speech dataset and compared to several baselines (ACVAE, CycleGAN) with objective and subjective evaluations. The results show that our proposed model is able to convert emotion with higher strength and quality.",
  "full_text": "CYCLETRANSGAN-EVC: A CYCLEGAN-BASED EMOTIONAL VOICE CONVERSION\nMODEL WITH TRANSFORMER\nChangzeng Fu1,3, Chaoran Liu2, Carlos Toshinori Ishi2,3, Hiroshi Ishiguro1,2\n1Graduate School of Engineering Science, Osaka University, Japan\n2Advanced Telecommunications Research Institute International, Japan\n3Interactive Robot Research Team, Guardian Robot Project, RIKEN, Japan\nEmail: changzeng.fu@irl.sys.es.osaka-u.ac.jp\nABSTRACT\nIn this study, we explore the transformer’s ability to capture\nintra-relations among frames by augmenting the receptive\nﬁeld of models. Concretely, we propose a CycleGAN-based\nmodel with the transformer and investigate its ability in the\nemotional voice conversion task. In the training procedure,\nwe adopt curriculum learning to gradually increase the frame\nlength so that the model can see from the short segment till\nthe entire speech. The proposed method was evaluated on the\nJapanese emotional speech dataset and compared to several\nbaselines (ACV AE, CycleGAN) with objective and subjective\nevaluations. The results show that our proposed model is able\nto convert emotion with higher strength and quality.\nIndex Terms— emotional speech conversion, cycle-\nconsistent adversarial networks\n1. INTRODUCTION\nEmotional voice conversion is a special type of voice con-\nversion (VC), which aims to transform an utterance’s emo-\ntional features into a target one while retaining the semantic\ninformation and speaker identity. Some earlier research in this\nﬁeld focused on mapping the prosody and spectrogram with\npartial least square regression [1], Gaussian Mixed Model\n(GMM) [2, 3], and the sparse representation method [4, 5].\nRecently, some researchers leverage deep learning methods\nto improve the performance of EVC, such as deep neural net-\nwork (DNN) [6, 7], sequence-to-sequence model (seq2seq)\nwith long-short-term memory network (LSTM) [8], convo-\nlutional neural network (CNN) [9], as well as their combi-\nnations with the attention mechanism [10]. However, these\nmodels require to be trained on parallel data, that is, both the\nsource and target should be from the same speaker and have\nidentical linguistic information but in different emotions.\nTo reduce the models’ reliance on parallel training data,\nsome novel frameworks are introduced into this ﬁeld. Gao\nThis work was supported by the Grant-in-Aid for Scientiﬁc Research on\nInnovative Areas JP20H05576 (model training), and by JST, Moonshot R&D\nunder Grant JPMJMS2011 (model evaluation).\net al. [11] proposed a nonparallel data-driven emotional\nspeech conversion method with an auto-encoder. Lately,\nDing et al. [12] adopted vector quantized variational autoen-\ncoders (VQ-V AE) with the group latent embedding (GLE)\nfor nonparallel data training. Moreover, to better learn the\nmapping function between non-parallel data distributions,\ncycle-consistent adversarial network (CycleGAN) [13, 14]\nand variational autoencoder-generative adversarial network\n(V AE-GAN) [15] were introduced to EVC task. Furthermore,\nMoritani et al. [16] employed starGAN to realize non-parallel\nspectral envelope transformation. These EVC models with\nCNN-based layers trained on non-parallel data all achieved a\nnot bad performance.\nDespite the progress made in non-parallel data training,\nthere remains some room to improve the quality of converted\nemotional voice. Because speech is a time series with rich\nacoustic features, there are some interactive temporal rela-\ntionships among frames need to be considered. Although\nCNNs are well-known for their ability to handle temporal\ndata. To process speech data, which is a sort of lengthy tem-\nporal sequence, CNN-based models must be stacked very\ndeep in order to widen the receptive ﬁeld. However, the\ntemporal intra-relations would be diluted layer by layer with\nthis manner and make the model suffer from some instability\nproblems such as mispronunciations and skipped phonemes.\nConsidering to augment the receptive ﬁeld of models\nand the ability to capture intra-relations among frames, the\ntransformer has been widely discussed in the ﬁeld of com-\nputer vision [17] and natural language processing [18], and\nits attention distance is also explored. However, few studies\nhave investigated the capabilities of transformers for the task\nof speech generation or conversion. Therefore, to solve the\naforementioned problem, in this study:\n• We proposed a CycleGAN-based model with the trans-\nformer and investigated its ability in the EVC task, we\nnamed our model CycleTransGAN.\n• Moreover, to enhance the model’s ability for convert-\ning emotional voice, we adopted curriculum learning to\narXiv:2111.15159v1  [cs.SD]  30 Nov 2021\nFig. 1. Neural networks for the proposed model. ×n means the indicated block repeats n times. The type of Conv in\ndiscriminator was variant for spectrogram and F0, Conv2D was used for spectrogram whereas Conv1D for F0. The data\nbetween [ ]are hyper-parameters we set in the experiment. K indicates the kernel size, C stands for the number of ﬁlters, Sis\nthe stride, H represents the number of nods for the hidden layer, AttH is an abbreviation for attention head, DP means the\ndrop out process.[ ]∗indicates hyper-parameters for 2Dprocessing, [ ]for 1Dprocessing.\ngradually increase the frame length during the training.\n• The proposed method was evaluated on the Japanese\nemotional speech dataset and compared to several base-\nlines (i.e. ACV AE [19], CycleGAN [13]) and the dif-\nferent conﬁgurations of our proposed model.\n2. PROPOSED METHOD\n2.1. Preprocessing\nIn this study, we extracted F0 and spectrogram from the\nspeech as our model’s inputs inspired by Zhou et al. [13],\nMing et al. [20], and Kaneko et al. [21]. The following\ncontents demonstrate how the extraction was conducted.\nFor extracting the F0 feature, the F0 contour was extracted\nﬁrstly, then, the continuous wavelet transform (CWT) was\nadopted to decompose it into multiple temporal scales (see\nEq.1). F0(x) indicates the input signal, and φ denotes the\nMexican hat mother wavelet.\nW(F0)(τ,t) =τ−1/2\n∫\nF0(x)φ(x−t\nτ )dx (1)\nIn this study, we set the CWT analysis at 10 scales with one\noctave apart, which can be represented as:\nWi(F0)(t) =Wi(F0)(2i+1τ0,t)(i+ 2.5)−5/2 (2)\nwhere i ∈[1,10] and τ0 = 5ms. Finally, the signal is ap-\nproximately reconstructed as:\nF0(t) =\n10∑\ni=1\nWi(F0)(t)(i+ 2.5)−5/2 (3)\nFor the spectrogram extraction, a three-step method so-\ncalled CheapTrick [22] was adopted. First, we calculated\nthe power spectrogram based on the windowed waveform as\nEq.4, the y(t) stands for the waveform, while w(t) indicates\nthe window function:\n∫ 3τ0\n0\n(y(t)w(t))2dt= 1.125\n∫ τ0\n0\ny2(t)dt (4)\nSecond, CheapTrick technique smooths the spectrogram with\na window of width 2w0/3, where w0 = 2π/τ0:\nP(w) = 3\n2w0\n∫ w0/3\n−w0/3\nP(w+ σ)dσ (5)\nAfter that, the liftering is carried out in the quefrency domain\nto eliminate the ﬂuctuation:\nP(w) =exp(F[ls(τ)lq(τ)ps(τ)])\nls(τ) =sin(πF0τ)\nπF0τ\nlq(τ) =q0 + 2q1cos(2πτ/τ0)\nps(τ) =F−1[log(P(w))]\n(6)\nTable 1. Information of dataset.\nNeutral Happy Anger Sad\nTrain (min) 61.23 74.15 59.29 61.23\nTest (min) 4.35 5.56 3.98 3.66\nwhere Fand F−1 stands for the Fourier transform and its\ninversion respectively. ls(τ) indicates the liftering function\nfor smoothing the signal, lq(τ) stands for the liftering func-\ntion for spectral recovery. ps(τ) represents the Cepstrum of\nP(w). q0 and q1 were set to 1.18 and -0.09 respectively.\n2.2. Model and Training Strategy\nGiven the extracted F0 and spectrogram, we constructed a\nCycleGAN-based model with the transformer to learn the\nconverting function on non-parallel data (see Fig.1).\nFor converting spectrogram, we ﬁrst employed the 1-\ndimension CNN to encode the features. Meanwhile, another\nCNN branch with sigmoid activation function was designed,\nand multiply it with the encoded features for selecting the\nsalient ones. Then, we inserted a normalization layer after\n1-dimension CNN and repeated this CNN-based block two\ntimes. After that, a residual convolutional followed with a\ntransformer layer was designed to capture the temporal re-\nlationships among timesteps. The position embedding was\nadded before feeding features to the transformer. This block\nwas also repeated two times. Subsequently, the CNN-based\nblock with a normalization layer was used to do the feature\nselection again. Finally, we did a post processing with a\n1-dimension CNN.\nFor converting F0, the structure of the neural network was\nsimilar to the one for the spectrogram. By considering the\nquantity of information carried by F0 is much less than that of\nthe spectrogram, we removed the transformer layer and only\nutilized each block once to reduce the number of trainable\nparameters.\nThe CNN-based block was also used in the discrimina-\ntor. Firstly, we encoded features and selected the salient ones.\nThen, inserted a normalization layer after the CNN layer and\nreused this block three times. Finally, a dense layer with a\nsigmoid activation was employed to output the real/fake la-\nbel. Note that the types of CNN utilized in the discriminator\nwere different for spectrogram and F0; for spectrogram, the\n2-dimensional CNN was used, while the 1-dimensional CNN\nwas employed for F0. Furthermore, the proposed discrimina-\ntor not only produced a label at the utterance level, but also\ngave multiple outputs (ﬁne-grained level) that presented the\nreal or false samples according to the frames to determine\nhow close each frame was to the real samples.\nThe CycleGAN framework is incorporated with three\ntypes of loss: (1) consistency loss, (2) identity loss, and (3)\nAlgorithm 1Training strategy\n1: lr= 2e−4; optimizer = Adam(lr, beta1 = 0.5);\n2: inputlength = 0.5s; α= 1; β = 1;\n3: epochs= 500\n4: while inputlength< = maxlength do\n5: for epoch in epochs do\n6: if epoch>(epochs ×65%) then\n7: α= 1, β = 0.5;\n8: lr+ =−5e−8;\n9: end if\n10: end for\n11: inputlength + = 0.5s;\n12: inputlength = min(inputlength,maxlength );\n13: α= 1, β = 1;\n14: end while\nadversarial loss. Thus, the loss functions for training the pro-\nposed model were deﬁned as follows: (1) Eq.7 demonstrates\nthe cycle-consistency loss, where xa and xb are samples from\nA emotion and B emotion, respectively, GA→B presents a\ngenerator to convert a sample from A to B, and GB→A for\nB to A. We calculated the L1 loss to compare the distance\nbetween the reconstructed sample and the original one, noted\nas ∥·∥ 1. This loss is supposed to make the emotional infor-\nmation of the input consistent with the target one.\nLcyc(GA→B,GB→A)\n= Exa [∥GB→A(GA→B(xa)) −xa∥1]\n+ Exb [∥GA→B(GB→A(xb)) −xb∥1]\n(7)\n(2) Eq.8 introduces the identity loss, which encourages the\ngenerator to convert the input while retaining the original lin-\nguistic information.\nLid(GA→B,GB→A) =Exa [∥GB→A(xa) −xa∥1]\n+ Exb [∥GA→B(xb) −xb∥1] (8)\n(3) The adversarial loss is demonstrated as following equa-\ntions, where DA and DB annotate discriminators for emo-\ntions A and B, respectively. The ﬁnal adversarial loss is de-\nﬁned as Ladv = LA\nadv + LB\nadv. This loss attempts to tell\nwhether a generator follows the target distribution.\nLA\nadv(GB→A,DA) =Exa [DA(xa)]\n+ Exb [log(1 −DA(GA→B(xb))]\nLB\nadv(GA→B,DB) =Exb [DB(xb)]\n+ Exa [log(1 −DB(GB→A(xa))]\n(9)\nFinally, the overall loss function is deﬁned as:\nL= Ladv + αLcyc + βLid (10)\nwhere αand βare constants that will be deﬁned during train-\ning. After converting spectrogram and F0, we employed\nWORLD vocoder [23] to synthesize the waveform.\nFig. 2. Comparisons of MOS with 95% conﬁdence interval to evaluate emotion similarity, voice quality, and naturalness. GT\nindicates the ground truth.\n3. EXPERIMENT\n3.1. Dataset\nA Japanese emotional speech dataset [24] that contains non-\nparallel happy, anger, sad, and neutral utterances was used in\nthis study. Each category of this dataset has 1070 utterances\nin total. We assigned 1000 utterances to the training set and\nthe left 70 utterances for the testing set, the duration of each\nemotion is presented in Table 1. In this dataset, each sample\nhas a similar corresponding one in other emotions. However,\nthese similar samples are not paralleled. Samples of different\nemotions are used to express similar meanings with different\nexpressions and modal particles.\n3.2. Settings and Baselines\nThe hyper-parameters were set as Fig.1 shows 1. Moreover,\nwe designed a training schedule with curriculum learning.\nAs the Algorithm 1 demonstrates, we gradually increased the\nlength of input with 0.5 second every 500 epochs to allow\nthe model to see from the short speech to the long one. This\nstrategy was supposed to introduce more detailed emotional\nfeatures to the model. Furthermore, the learning rate was de-\ncaying, and the weight constant βwas changed to 0.5 from 1\nafter 325 epochs, and reset back to 1 every 500 epochs.\nAs for baselines, we retrained the ACV AE [19] and Cy-\ncleGAN [13] on the Japanese emotional speech dataset and\ncompare the performance with the proposed model. Ad-\nditionally, to verify the effects of the ﬁne-grained level\ndiscriminator and curriculum learning, we modiﬁed our\nproposed to different conﬁgurations, i.e., CycleTransGAN\n(without curriculum learning and ﬁne-grained level discrimi-\nnator), CycleTransGAN-CL (with curriculum learning only),\nCycleTransGAN-All (with curriculum learning and ﬁne-\ngrained level discriminator).\n1The implementation code and samples can be found at: https://\ngithub.com/CZ26/CycleTransGAN-EVC\n3.3. Results\nWe adopted Mean Opinion Score (MOS) to subjectively eval-\nuate emotional level, voice quality and naturalness of the\nsamples. We invited 15 subjects to participate in all the ex-\nperiments, and each subject evaluated 20 original utterances\nfrom the dataset and 75 converted utterances. Fig. 2 presents\nthe evaluation results with 95% conﬁdence interval. The\nground truth is the score of original emotional samples. For\nthe scores of emotion evaluation, the CycleTransGAN-All\nmodel achieves the highest score for happy emotion, while\nthe CycleTransGAN-CL achieves the highest score for angry\nand sad emotions. As for the evaluation of voice quality\nand naturalness, the CycleTransGAN-all outperforms all the\nother models. These results of CycleTransGAN-CL and Cy-\ncleTransGAN imply that using curriculum learning improves\nemotional feature conversion by allowing the model to learn\nfrom the shorter segment up to the entire sample. This is\nbecause the emotion of speech tends to appear in partly rather\nthan in the whole speech, and the ﬁne-grained level discrimi-\nnator somewhat distracts the ability of the model to focus on\nthe salient segment. Therefore, it leads to CycleTransGAN-\nAll becomes inferior to CycleTransGAN-CL in terms of\nemotion similarity.\n4. CONCLUSION\nIn this study, we proposed a CycleGAN-based emotional\nvoice conversion model with a transformer, which is named\nCycleTransGAN. With the help of the transformer, the model\nis able to augment the receptive ﬁeld to a wider range, which\nallows the generated speech to be more consistent in terms\nof temporal features, solving the instability problems of mis-\npronunciations and skipped phonemes to some extent. The\nexperiment results show that the proposed models improve\nemotion similarity, voice quality, and naturalness. However,\nthe clarity and emotional strength of the speech generated by\nthe proposed models still need some further improvements.\nFuture works will mainly focus on these parts.\n5. REFERENCES\n[1] Elina Helander, Tuomas Virtanen, Jani Nurminen, and Moncef\nGabbouj, “V oice conversion using partial least squares regres-\nsion,” IEEE Transactions on Audio, Speech, and Language\nProcessing, vol. 18, no. 5, pp. 912–921, 2010.\n[2] Ryo Aihara, Ryoichi Takashima, Tetsuya Takiguchi, and Yasuo\nAriki, “Gmm-based emotional voice conversion using spec-\ntrum and prosody features,” American Journal of Signal Pro-\ncessing, vol. 2, no. 5, pp. 134–138, 2012.\n[3] Hiromichi Kawanami, Yohei Iwami, Tomoki Toda, Hiroshi\nSaruwatari, and Kiyohiro Shikano, “Gmm-based voice con-\nversion applied to emotional speech synthesis,” 2003.\n[4] Huaiping Ming, Dongyan Huang, Lei Xie, Shaofei Zhang,\nMinghui Dong, and Haizhou Li, “Exemplar-based sparse rep-\nresentation of timbre and prosody for voice conversion,” in\n2016 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP). IEEE, 2016, pp. 5175–5179.\n[5] Ryoichi Takashima, Tetsuya Takiguchi, and Yasuo Ariki,\n“Exemplar-based voice conversion using sparse representation\nin noisy environments,” IEICE Transactions on Fundamentals\nof Electronics, Communications and Computer Sciences, vol.\n96, no. 10, pp. 1946–1953, 2013.\n[6] Susmitha Vekkot, Deepa Gupta, Mohammed Zakariah, and\nYousef Ajami Alotaibi, “Emotional voice conversion using\na hybrid framework with speaker-adaptive dnn and particle-\nswarm-optimized neural network,” IEEE Access, vol. 8, pp.\n74627–74647, 2020.\n[7] Zhaojie Luo, Tetsuya Takiguchi, and Yasuo Ariki, “Emotional\nvoice conversion using deep neural networks with mcc and f0\nfeatures,” in 2016 IEEE/ACIS 15th International Conference\non Computer and Information Science (ICIS). IEEE, 2016, pp.\n1–5.\n[8] Carl Robinson, Nicolas Obin, and Axel Roebel, “Sequence-to-\nsequence modelling of f0 for speech emotion conversion,” in\nICASSP 2019-2019 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp.\n6830–6834.\n[9] Hirokazu Kameoka, Kou Tanaka, Damian Kwa ´sny, Takuhiro\nKaneko, and Nobukatsu Hojo, “Convs2s-vc: Fully convo-\nlutional sequence-to-sequence voice conversion,” IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, vol.\n28, pp. 1849–1863, 2020.\n[10] Heejin Choi and Minsoo Hahn, “Sequence-to-sequence emo-\ntional voice conversion with strength control,” IEEE Access,\nvol. 9, pp. 42674–42687, 2021.\n[11] Jian Gao, Deep Chakraborty, Hamidou Tembine, and Olaitan\nOlaleye, “Nonparallel emotional speech conversion,” arXiv\npreprint arXiv:1811.01174, 2018.\n[12] Shaojin Ding and Ricardo Gutierrez-Osuna, “Group latent em-\nbedding for vector quantized variational autoencoder in non-\nparallel voice conversion.,” inINTERSPEECH, 2019, pp. 724–\n728.\n[13] Kun Zhou, Berrak Sisman, and Haizhou Li, “Transforming\nspectrum and prosody for emotional voice conversion with\nnon-parallel training data,” arXiv preprint arXiv:2002.00198,\n2020.\n[14] Songxiang Liu, Yuewen Cao, and Helen Meng, “Emotional\nvoice conversion with cycle-consistent adversarial network,”\narXiv preprint arXiv:2004.03781, 2020.\n[15] Yuexin Cao, Zhengchen Liu, Minchuan Chen, Jun Ma, Shao-\njun Wang, and Jing Xiao, “Nonparallel emotional speech con-\nversion using vae-gan.,” in INTERSPEECH, 2020, pp. 3406–\n3410.\n[16] Asuka Moritani, Ryo Ozaki, Shoki Sakamoto, Hirokazu\nKameoka, and Tadahiro Taniguchi, “Stargan-based emo-\ntional voice conversion for japanese phrases,” arXiv preprint\narXiv:2104.01807, 2021.\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa De-\nhghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\net al., “An image is worth 16x16 words: Transformers for\nimage recognition at scale,” arXiv preprint arXiv:2010.11929,\n2020.\n[18] Chuhan Wu, Fangzhao Wu, and Yongfeng Huang, “Da-\ntransformer: Distance-aware transformer,” arXiv preprint\narXiv:2010.06925, 2020.\n[19] Hirokazu Kameoka, Takuhiro Kaneko, Kou Tanaka, and\nNobukatsu Hojo, “Acvae-vc: Non-parallel voice conversion\nwith auxiliary classiﬁer variational autoencoder,” IEEE/ACM\nTransactions on Audio, Speech, and Language Processing, vol.\n27, no. 9, pp. 1432–1443, 2019.\n[20] Huaiping Ming, Dongyan Huang, Minghui Dong, Haizhou Li,\nLei Xie, and Shaofei Zhang, “Fundamental frequency model-\ning using wavelets for emotional voice conversion,” in 2015\nInternational Conference on Affective Computing and Intelli-\ngent Interaction (ACII). IEEE, 2015, pp. 804–809.\n[21] Takuhiro Kaneko and Hirokazu Kameoka, “Cyclegan-vc:\nNon-parallel voice conversion using cycle-consistent adversar-\nial networks,” in 2018 26th European Signal Processing Con-\nference (EUSIPCO). IEEE, 2018, pp. 2100–2104.\n[22] Masanori Morise, “Cheaptrick, a spectral envelope estimator\nfor high-quality speech synthesis,” Speech Communication,\nvol. 67, pp. 1–7, 2015.\n[23] Masanori Morise, Fumiya Yokomori, and Kenji Ozawa,\n“World: a vocoder-based high-quality speech synthesis system\nfor real-time applications,” IEICE TRANSACTIONS on Infor-\nmation and Systems, vol. 99, no. 7, pp. 1877–1884, 2016.\n[24] Sara Asai, Koichiro Yoshino, Seitaro Shinagawa, Sakriani\nSakti, and Satoshi Nakamura, “Emotional speech corpus for\npersuasive dialogue system,” in Proceedings of The 12th Lan-\nguage Resources and Evaluation Conference, 2020, pp. 491–\n497.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8401981592178345
    },
    {
      "name": "Computer science",
      "score": 0.684846818447113
    },
    {
      "name": "Speech recognition",
      "score": 0.5940536856651306
    },
    {
      "name": "Frame (networking)",
      "score": 0.4531838595867157
    },
    {
      "name": "Natural language processing",
      "score": 0.37907037138938904
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3687668442726135
    },
    {
      "name": "Engineering",
      "score": 0.1519370973110199
    },
    {
      "name": "Voltage",
      "score": 0.08980312943458557
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "cited_by": 8
}