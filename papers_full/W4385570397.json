{
  "title": "HyperMixer: An MLP-based Low Cost Alternative to Transformers",
  "url": "https://openalex.org/W4385570397",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5102917205",
      "name": "Florian Mai",
      "affiliations": [
        "Idiap Research Institute",
        "University of Geneva",
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5059678146",
      "name": "Arnaud Pannatier",
      "affiliations": [
        "Idiap Research Institute",
        "University of Geneva",
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5067581142",
      "name": "Fabio Fehr",
      "affiliations": [
        "Idiap Research Institute",
        "University of Geneva",
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5101905561",
      "name": "Haolin Chen",
      "affiliations": [
        "Idiap Research Institute",
        "University of Geneva",
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5026460704",
      "name": "François Marelli",
      "affiliations": [
        "Idiap Research Institute",
        "University of Geneva",
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5076094010",
      "name": "François Fleuret",
      "affiliations": [
        "Idiap Research Institute",
        "University of Geneva",
        "École Polytechnique Fédérale de Lausanne"
      ]
    },
    {
      "id": "https://openalex.org/A5084321238",
      "name": "James Henderson",
      "affiliations": [
        "Idiap Research Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4394642966",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3173788106",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3203515362",
    "https://openalex.org/W3037798801",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W3168656614",
    "https://openalex.org/W4312678820",
    "https://openalex.org/W4313056180",
    "https://openalex.org/W4385822820",
    "https://openalex.org/W3162090017",
    "https://openalex.org/W4312820606",
    "https://openalex.org/W3123161422",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2097998348",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034389353",
    "https://openalex.org/W4243586225",
    "https://openalex.org/W4287073339",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W2970283086",
    "https://openalex.org/W4322096544",
    "https://openalex.org/W4327656064",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W4308854803",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4315588916",
    "https://openalex.org/W4385565006",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4287119852",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4225764944",
    "https://openalex.org/W3010768098",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3129603602",
    "https://openalex.org/W4226483979"
  ],
  "abstract": "Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 15632–15654\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nHyperMixer: An MLP-based Low Cost Alternative to Transformers\nFlorian Mai†♠ Arnaud Pannatier†♠ Fabio Fehr†♠ Haolin Chen†♠\nFrançois Marelli†♠ François Fleuret♣♠† James Henderson†\n†Idiap Research Institute, Martigny, Switzerland\n♠EPFL, Lausanne, Switzerland\n♣University of Geneva, Geneva, Switzerland\nAbstract\nTransformer-based architectures are the model\nof choice for natural language understanding,\nbut they come at a significant cost, as they have\nquadratic complexity in the input length, re-\nquire a lot of training data, and can be difficult\nto tune. In the pursuit of lower costs, we investi-\ngate simple MLP-based architectures. We find\nthat existing architectures such as MLPMixer,\nwhich achieves token mixing through a static\nMLP applied to each feature independently, are\ntoo detached from the inductive biases required\nfor natural language understanding. In this pa-\nper, we propose a simple variant, HyperMixer,\nwhich forms the token mixing MLP dynami-\ncally using hypernetworks. Empirically, we\ndemonstrate that our model performs better\nthan alternative MLP-based models, and on par\nwith Transformers. In contrast to Transformers,\nHyperMixer achieves these results at substan-\ntially lower costs in terms of processing time,\ntraining data, and hyperparameter tuning.\n1 Introduction\nAttention-based architectures, such as the Trans-\nformer (Vaswani et al., 2017), have accelerated the\nprogress in many natural language understanding\ntasks. Part of their success is a result of a paralleliz-\nable training scheme over the input length. This\nimproves training times and allows for larger vol-\numes of data which makes these models amenable\nto pretraining (Radford et al., 2018; Devlin et al.,\n2019). Therefore, many current state-of-the-art\nmodels are fine-tuned extensions of large pretrained\nTransformers (Bommasani et al., 2021).\nHowever, these models come at a significant\ncomputational cost. They require considerable\nresources for pretraining and fine-tuning, which\ninduces high energy consumption (Strubell et al.,\n2019) and limits access to research (Bommasani\net al., 2021). Subsequently, Schwartz et al. (2020)\nargue the need for \"Green AI\". They propose a cost\nevaluation of a result Ras following:\nCost(R) ∝E·D·H,\nwhere E is the computational cost measured in\nfloating point operations (FPO) of a single exam-\nple, D is the dataset size, and H is the number\nof hyperparameter configurations required during\ntuning.\nTo achieve a cost reduction, this paper proposes\na simpler alternative to Transformers. We take\ninspiration from the computer vision community,\nwhich has recently seen a surge of research on\nMulti-Layer Perceptrons (MLPs). Most promi-\nnently, MLPMixer (Tolstikhin et al., 2021), which\nis a simple architecture based on two MLPs: one\nfor token mixing and one for feature mixing. How-\never, the token mixing MLP learns a fixed-size set\nof position-specific mappings, arguably making\nMLPMixer’s architecture too detached from the\ninductive biases needed for natural language under-\nstanding, in contrast to Transformers (Henderson,\n2020).\nIn this paper, we propose a simple variant, Hy-\nperMixer (Figure 1), which creates a token mix-\ning MLP dynamically using hypernetworks (Ha\net al., 2016). This variant is more appropriate, as it\nlearns to generate a variable-size set of mappings\nin a position-invariant way, similar to the attention\nmechanism in Transformers (Vaswani et al., 2017).\nIn contrast to Transformer’s quadratic complex-\nity, HyperMixer’s complexity is linear in the input\nlength. This makes it a competitive alternative for\ntraining on longer inputs.\nEmpirically, we demonstrate that HyperMixer\nworks substantially better on natural language un-\nderstanding tasks than the original MLPMixer and\nrelated alternatives. In comparison to Transform-\ners, HyperMixer achieves competitive or improved\nresults at a substantially lower cost Cost(R) ∝\nE·D·H: improved inference speeds (E), espe-\ncially for long inputs; favorable performance in the\n15632\nToken \nMixing\nSelf\nAttention\nFigure 1: The figure outlines a general model layer consisting of a token mixing component and a feature mixing\ncomponent (MLP). For token mixing, MLPMixer uses an MLP with a fixed size, maximum input length N and\nposition-specific weights. In contrast, HyperMixer generates an appropriately sized MLP based on the variable size\nof the input in a position-invariant way, similar to the attention mechanism. When using attention as token mixing\nthe whole layer is equivalent to a Transformer encoder layer.\nlow-resource regime (D); and efficient tuning for\nhyperparameters (H). We attribute HyperMixer’s\nsuccess to its ability to approximate an attention-\nlike function. Further experiments on a synthetic\ntask demonstrate that HyperMixer indeed learns to\nattend to tokens in similar pattern to the attention\nmechanism.\nIn summary, our contributions can be enumer-\nated as follows:\n1. A novel all-MLP model, HyperMixer, with\ninductive biases similar to Transformers. (Sec-\ntion: 2)\n2. A performance analysis of HyperMixer\nagainst alternative token mixing methods\nbased on controlled experiments on the GLUE\nbenchmark. (Section: 4.3)\n3. A comprehensive comparison of the cost\nCost(R) of HyperMixer and Transformers.\n(Sections: 4.4, 4.5, 4.6)\n4. An ablation demonstrating that HyperMixer\nlearns attention patterns similar to Transform-\ners. (Section: 4.7)\n2 Method\n2.1 Inductive Biases in NLP Models\nIn machine learning, the inductive biases of a\nmodel reflect implicit modeling assumptions which\nare key to facilitate learning and improve gener-\nalization on specific tasks. In NLP, well-known\nmodels with strong inductive biases include: recur-\nrent neural networks (Elman, 1990), which assume\nthe input to be a sequence; and recursive neural net-\nworks (Socher et al., 2013), which assume a tree-\nstructure. While both these inductive biases are\nreasonable, empirically, Transformers have been\nmore successful in recent years. Furthermore, we\nreiterate the arguments of Henderson (2020) for\ninductive biases in language and apply them to\nour model design. Henderson (2020) attributes\nthe Transformer’s success to two concepts: vari-\nable binding and systematicity. Variable binding\n15633\nrefers to the model’s ability to represent multiple\nentities at once. This is arguably challenging in\nsingle-vector representations such as recurrent neu-\nral networks. However, Transformers represent\neach token with its own vector which accounts for\nvariable binding as each token can be interpreted\nas an entity. Systematicity refers to the models\nability to learn generalizable rules that reflect the\nstructural relationship between entities (Fodor and\nPylyshyn, 1988). Transformers achieve system-\naticity through the attention mechanism which is\na learnable set of functions that determines the\ninteraction between entities by matching query rep-\nresentations to key representations (as shown in\nFigure 1). The mechanism modulates, for every\nposition in the sequence, how to functionally pro-\ncess any other position. Moreover, these function\nparameters are learnable and shared across all enti-\nties.\n2.2 MLPMixer\nA general layer of MLPMixer is shown in Figure 1.\nSimilarly to Transformers, each token is repre-\nsented as a vector of features, which undergo (non-\nlinear) transformations in multiple layers. MLP-\nMixer employs two MLPs at each layer, one for\nfeature mixing and one for token mixing . The\nfeature mixing component is applied to each to-\nken vector independently, which models the in-\nteractions between features. The Token Mixing\nMLP (TM-MLP) is applied to each feature inde-\npendently (i.e. its vector of values across tokens),\nwhich models the interactions between spatial lo-\ncations or positions. This could be interpreted as\na global attention mechanism which is static and\nposition-modulated. Practically, this is achieved\nby transposing the dimension representing the fea-\ntures and the dimension representing the positions.\nEach vector xT\ni ∈RN , representing feature i≤d,\nof some input of fixed length N, is input into\nTM-MLP, which has the following form:\nTM-MLP(xT\ni ) =W1(σ(WT\n2 xT\ni )), (1)\nwhere W1,W2 ∈ RN×d′\n, and σ represents\nthe GELU non-linearity (Hendrycks and Gimpel,\n2016). Finally, to facilitate learning, layer normal-\nization (Ba et al., 2016) and skip connections (He\net al., 2016) are added around each MLP, respec-\ntively. How to best arrange these components is\nstill an open question (Wang et al., 2019; Bach-\nlechner et al., 2021). We experiment with different\nvariants in Appendix F.\nConsiderations for NLP The token mixing MLP\nassumes an input of fixed dimension, which is nec-\nessary as the parameters need to be shared across all\nexamples. However, unlike images, textual input\nis generally of a variable dimension. Therefore, to\napply MLPMixer to texts of variable length, a sim-\nplistic approach is to assume a maximum length\n(e.g. the maximum in the dataset). Thereafter,\nall inputs are padded to the maximum length and\nmasks are applied in the token mixing MLP. This\nmodel is able to do variable binding, since each\ntoken is represented by its own vector. However,\nthis model lacks systematicity because the rules\nlearned to model interactions between tokens (i.e.\nthe MLP’s weights) are not shared across positions.\n2.3 HyperMixer\nAlgorithm 1 HyperMixer pseudo-code\nclass HyperMixing(nn.Module):\ndef __init__(self, d, d’):\n# learnable parameters\nself.hypernetwork_in = MLP([d, d, d’])\nself.hypernetwork_out = MLP([d, d, d’])\n# layer normalization improves training stability\nself.layer_norm = LayerNorm(d)\ndef forward(self, queries, keys, values):\n# queries: [B, M, d]\n# keys / values: [B, N, d]\n# add token information (e.g. position embeddings)\nhyp_in = add_token_information(keys)\nhyp_out = add_token_information(queries)\nW1 = self.hypernetwork_in(keys) # [B, N, d’]\nW2 = self.hypernetwork_out(queries) # [B, M, d’]\n# TM-MLP(x) = W_2 ( GELU ( W_1^T x) )\n# maps [B, d, N] -> [B, d, M]\ntoken_mixing_mlp = compose_TM_MLP(W1, W2)\n# transpose so MLP is applied to sequence dimension\nvalues = values.transpose(1, 2) # [B, d, N]\noutput = token_mixing_mlp(values) # [B, d, M]\n# transpose back\noutput = output.transpose(1,2) # [B, M, d]\n# optionally apply LayerNorm\nreturn self.layer_norm(output)\nHyperMixer includes systematicity into the\nMLPMixer architecture by introducing a novel to-\nken mixing mechanism, HyperMixing1, which can\nbe regarded as a drop-in replacement for attention.\nFor ease of understanding, we provide pseudo-code\nin Algorithm 1. While the queries, keys, and val-\nues in HyperMixing need not be the same, we will\nassume they are identical in the following formu-\nlation. HyperMixing relies on the use of hyper-\nnetworks, which are used to generate the weights\n1HyperMixing is to HyperMixer what self-attention is to\nTransformer encoders.\n15634\nW1,W2 of TM-MLP (Equation 1) dynamically\nas a function of the input. Let xj ∈Rd, j ≤N,\nwhere N is the (variable) dimension of the input,\nrepresent token j(i.e., query, key, and value).W1\nand W2 are generated by parameterized functions\nh1,h2 : RN×d →RN×d′\n. Theoretically, h1 and h2\ncould be any function, including sophisticated net-\nworks that consider non-linear interactions between\ntokens, such as the attention mechanism. However,\nthis would defeat the purpose of our model, which\nis simplicity. Therefore, we choose to generate\nthe rows of the weight matrices from each token\nindependently via another MLP. Concretely, a hy-\npernetwork function can be defined as\nhi(x) =\n\n\nMLPWi(x1 + p1)\n...\nMLPWi(xN + pN )\n\n∈RN×d′\n,\nwhere MLPW1,MLPW2 : Rd →Rd′\nare them-\nselves multi-layer perceptrons with GELU non-\nlinearity. pj ∈ Rd is a vector that can encode\nadditional information such as the position via ab-\nsolute position embeddings (Vaswani et al., 2017).\nIntuitively, for each token xj, h1 decides\nwhich information to send to the hidden layer of\nTM-MLP, where the information from all tokens\nare mixed, and h2 decides for each token how to\nextract information from the hidden layer. Note\nthat, even though h1 and h2 only consider one to-\nken at once, non-linear interactions between to-\nkens are still modeled through the hidden layer of\nTM-MLP.\nFinally, layer normalization (Ba et al., 2016) can\nbe applied to the output of TM-MLP. We found\nthis helpful to facilitate training with a wide variety\nof Transformer layouts (Appendix F).\nTying h1 and h2 In order to reduce the number\nof parameters and operations in the model, and\nthereby the complexity, we found it useful to tie h1\nand h2 by setting W2 = W1.\nConsiderations for NLP In comparison to the\nMLPMixer defined in Section 2.2, the use of hyper-\nnetworks overcomes two challenges. Firstly, the\ninput no longer has to be of fixed dimensionality.\nThe hypernetwork generates a token mixing MLP\nof appropriate dimension as a function of the input.\nSecondly, the hypernetwork models the interaction\nbetween tokens with shared weights across all posi-\ntions in the input. Hence, systematicity is ensured.\n3 Related Work\nResearch on all-MLP models like MLPMixer (Tol-\nstikhin et al., 2021) is widespread in the computer\nvision community (Tu et al., 2022; Yu et al., 2022;\nWang et al., 2022, among many others). How-\never, they lack some desirable inductive biases for\nNLP, which we discuss in length in Appendix A.2.\nSpecifically, in contrast to HyperMixer, none of the\npreviously proposed methods simultaneously pro-\nvide i) position invariance, which is important for\ngeneralization, ii) adaptive size for variable-length\ninputs, iii) a global receptive field, which allows\ninteractions to not be limited to small token neigh-\nborhoods, iv) learnabilty allowing for universal\napplicablility to various tasks, and v) dynamicity,\nwhich means that token mixing is a function of\nthe input. Consequently, only a few works have\nused MLP-based models as their backbone in NLP\ntasks. gMLP (Liu et al., 2021) serves as one of our\nbaselines and pnlp-mixer (Fusco et al., 2022) em-\nploys standard MLPMixer on top of a novel token\nembedding method.\nApart from all-MLP models, there is an abun-\ndance of research on efficient alternatives to stan-\ndard attention layers (Katharopoulos et al., 2020;\nBello, 2021, et cetera). While they don’t qualify as\nall-MLP models, they have close connections to our\nwork (see Appendix E) and aim at lowering the cost\nof AI, albeit it on fewer dimensions than our work\n(Appendix A.1). We employ FNet (Lee-Thorp\net al., 2021) and Linear Transformers (Katharopou-\nlos et al., 2020) as representatives of these as a\nbaseline.\n4 Experiments\nOur experiments are designed to test the following\nthree hypotheses. H1 (Section 4.3): Since Hy-\nperMixer reflects more inductive biases that are\nadequate for NLP, our hypothesis is that Hyper-\nMixer performs better at NLP tasks than MLP-\nMixer and similar MLP-based alternatives, specif-\nically at those tasks that require to model the in-\nteractions between tokens. H2: Since HyperMixer\nhas similar inductive biases as transformers but\nis considerably simpler conceptually and in terms\nof computational complexity, it can be seen as a\nlow cost alternative to Transformers, reducing the\ncost in terms of single example processing time\n(Section 4.4), required dataset size (Section 4.5),\nand hyperparameter tuning (Section 4.6). H3 (Sec-\ntion 4.7): Due to its inductive biases mirroring\n15635\nthose of Transformers, HyperMixer also learns sim-\nilar patterns as the attention mechanism.\n4.1 Datasets\nWe evaluate on four sentence-pair classification\ntasks and one single-sentence classification task.\nThe sentence-pair tasks are QQP (Iyer et al., 2017),\nQNLI (Rajpurkar et al., 2016), MNLI (Williams\net al., 2018) and SNLI (Bowman et al., 2015).\nFor uniformity, datasets are formatted as in the\nGLUE benchmark (Wang et al., 2018). We choose\nthese tasks for two properties: firstly, they have\nlarge training datasets (Table 2, appendix) enabling\nreasonable performances without pretraining; sec-\nondly, solving these tasks requires good modeling\nof the interactions between tokens from different\nsentences, which is the main focus of this paper.\nAs a control, we experiment on the single-input\ndataset SST2 (Socher et al., 2013), which is a sen-\ntiment classification task. Many examples in this\ndataset can be solved by identifying key sentiment\nwords, rather than modeling the token interaction.\n4.2 Baselines\nThe following baselines can be categorized into\nMLP-based (to support H1) and not MLP-based\n(e.g., Transformers, to support H2). Note that our\nstudy is about the design of the token mixing mod-\nule. Therefore, we only compare to models that fit\ninto the general framework displayed in Figure 1,\nwhere there is a feature mixing module and a to-\nken mixing module for textual inputs. As a result,\nmodels such as RNNs are excluded. To enable\na controlled experiment, we use the same feature\nmixing module in all models; the models only dif-\nfer in their token mixing module.\nMLP-based The conceptually closest baseline is\nMLPMixer (Tolstikhin et al., 2021), which com-\nbines both token and feature mixing using fixed\ndimensional MLPs, as described in Section 2.2.\nConcurrently, (Liu et al., 2021) proposed gMLP, in\nwhich token mixing is achieved through weighted\nsummation of all other inputs, similar to the atten-\ntion mechanism. However, rather than computing\nweights as function of the inputs like in attention,\nin gMLP the weights are fixed learnable parame-\nters. Additionally, linear gating initialized close to\none is introduced to facilitate training. The origi-\nnal gMLP method does not employ feature mixing\nmodules, as their token mixing module is capable\nof modeling feature interactions as well in a single\ngMLP block. However, for comparability we inject\ngMLP blocks as token mixing modules in our gen-\neral architecture and keep feature mixing modules\nas well.\nNon MLP-based Transformers (Vaswani\net al., 2017) are used in the current state of\nthe art in virtually all NLP tasks. Their key\ncomponent is the softmax-based self-attention\nmodule, which we use for token mixing.\nLinear Transformer (Katharopoulos et al.,\n2020) replaces softmax attention with a feature-\nmap based dot-product attention . Finally,\nFNet (Yu et al., 2021) replaces the self-attention\npart of Transformers with a fixed, non-learnable set\nof Fourier transforms for token mixing.\n4.3 Performance\nInitially we compare the performance of Hyper-\nMixer in comparison to our baselines. Thereafter,\nwe further explore the model’s benefits with re-\nspects to its cost.\nFor comparability, we adjust the size of the to-\nken mixing components such that all models have\nthe same number of parameters (11M). FNet is an\nexception since it has no learnable parameters in\nits token mixing component. We tune the learning\nrate of each model via grid-search, and report the\nperformance of the best configuration. Further ex-\nperimental details on all experiments can be found\nin Appendix B.\nResults Validation and test set results are shown\nin Table 1. On the test and the validation set, Hyper-\nMixer performs the best among MLP-based mod-\nels on all datasets, although for SST the difference\non the validation set is smaller than one standard\ndeviation. MLPMixer generally achieves good per-\nformances, outperforming Transformers on two\ndatasets.\nComparing to non-MLP-based methods, Hyper-\nMixer also outperforms vanilla Transformers on all\ndatasets. The differences are generally small (≤2\npoints), except on QNLI, where the difference is\n3.9 points. We suspect that this discrepancy is due\nto the relatively small training set of QNLI. We\ninvestigate low-resource behavior of Transformers\nin comparison to HyperMixer in Section 4.5. FNet\nperforms substantially worse than the other meth-\nods, particularly on SNLI and QQP. Linear Trans-\nformers achieve excellent performance on MNLI\nand SNLI, but perform poorly on QNLI and QQP.\n15636\n0 500 1000 1500 2000 2500 3000 3500 4000\nInput Length\n6\n8\n10\n12\n14\n16Time per 1000 examples\nHyperMixer (Time)\nTransformer (Time)\nHyperMixer (FOPs)\nTransformer (FOPs)\n0\n250\n500\n750\n1000\n1250\n1500\nFOPS (Millions)\nFigure 2: WCT / FOPs of propagating a single example\nthrough the token mixing of HyperMixer vs. Trans-\nformer depending on the input length.\nIn Appendix C.2, we discuss ablations such as\nuntied HyperMixer.\n4.4 Time per Example\nIn order to assess the efficiency of our model, we\nmeasure the wallclock-time of processing a single\ninput (repeated 1,000 times) through the token mix-\ning stages of HyperMixer and Transformer, respec-\ntively. As Schwartz et al. (2020) point out, wall-\nclock time has the downside of being dependent\non the specific implementation, and they therefore\nrecommend reporting the number of floating point\noperations (FOPs) required by one forward pass. In\nFigure 2, we show wallclock time and theoretical\nFOPs as a function of the input lengthN. For short\ninput sequences, the number of FOPs is dominated\nby the size of the hidden layer and hence slightly\nlower for Transformers than for HyperMixer. How-\never, in practical terms we observe that HyperMixer\nis still faster than Transformers. At longer input\nsequences, the size of N starts to dominate the to-\ntal complexity of Transformers, so that it becomes\nexceedingly slower than HyperMixer.\n4.5 Low Resource Performance\nLike MLPMixer, HyperMixer is a conceptually\nsimple architecture, as it only applies multi-layer\nperceptrons at its core. Simpler architectures of-\nten make for better performance on smaller scale\ndatasets. We investigate this by varying the number\nof examples used for training on the three large\ndatasets MNLI, SNLI, and QQP. For these exper-\niments, we use the best performing learning rate\nfound in the grid search from Section 4.3. In Fig-\n5 10 25 50 100\nFraction of Training Set [%]\n0\n2\n4\n6\n8\n10\n12\n14Relative Validation Acc. [%]\nMNLI\nQQP\nSNLI\nFigure 3: Relative improvement of HyperMixer over\nTransformer depending on what percentage of the train-\ning set is used.\nure 3, we plot the relative performance change of\nHyperMixer compared to Transformers as a func-\ntion of subsample size. On all datasets, the relative\nimprovement of HyperMixer over Transformers is\nlarger when training with 10% of the dataset than\nwith the full dataset. While the effect is small on\nQQP, it is particularly large on SNLI and MNLI,\nwhere HyperMixer performs almost 12-14% better\nwith 10% of the data, while the relative improve-\nment with the full dataset is less than 2%.\n4.6 Ease of Hyperparameter Tuning\nMLP-based token mixing has the advantage that it\nis conceptually simpler than self-attention, and that\nit is well-known how to facilitate training via mech-\nanisms such as skip-connections and layer normal-\nization. Both these aspects suggest that it might be\neasier to find hyperparameter configurations that\nyield good performances. In these experiments, we\ncompare HyperMixer (with tied hypernetworks) to\nTransformers in this regard. As recommended in\nSchwartz et al. (2020), we perform a random search\nto tune hyperparameters and compute the expected\nvalidation performance (Dodge et al., 2019, 2021).\nSpecifically, we tune the learning rate, whose log-\narithm is drawn from U(−8,−1), and the dropout\nprobability drawn from U(0,0.5) for 20 trials.\nResults In Figure 4, we show the relative ex-\npected validation performance, i.e., the relative\nperformance change of HyperMixer compared to\nTransformer, for all five datasets. With the notable\nexception of QNLI, the relative improvement of\nHyperMixer is higher at smaller budgets than at\nlarger budgets on all datasets. The effect is par-\n15637\nModel MNLI SNLI QQP QNLI SST # Params\nBaselines Validation set results (average accuracy / standard deviation over 10 seeds)\nFNet 59.7 (0.27) 75.3 (0.46) 79.4 (0.28) 59.9 (0.46) 79.7 (0.71) 9.5 M\nLin. Transformer 66.9 (0.48) 82.7 (0.22) 81.7 (0.28) 61.3 (0.29) 80.5 (0.46) 11 M\nTransformer 65.4 (0.51) 80.9 (0.40) 82.8 (0.22) 67.3 (2.03) 79.0 (0.86) 11 M\nMLPMixer 63.9 (0.34) 79.6 (0.11) 83.7 (0.42) 68.1 (2.10) 80.1 (0.67) 11 M\ngMLP 60.8 (0.95) 80.5 (0.55) 82.8 (0.21) 60.5 (0.49) 78.7 (0.74) 11 M\nHyperMixer (ours) 66.2 (0.21) 81.9 (0.27) 85.6 (0.20) 78.0 (0.19) 80.7 (0.84) 11 M\nBaselines Test set results (best model)\nFNet 59.8 75.3 78.4 59.6 80.0 9.5 M\nLin. Transformer 66.9 83.0 82.3 61.7 80.8 11 M\nTransformer 65.8 80.7 82.4 73.2 79.4 11 M\nMLPMixer 62.9 80.1 83.5 70.5 81.2 11 M\ngMLP 61.2 80.9 82.5 60.2 79.5 11 M\nHyperMixer (ours) 66.1 81.7 84.1 77.1 81.4 11 M\nTable 1: Top: Mean validation set accuracy and standard deviation over 10 different seeds of the best hyperparameter\nconfiguration. Results are printed in bold font if they exceed the second best result by at least one standard deviation.\nUnderline marks the best MLP-based model. Bottom: Test set results on natural language understanding tasks when\nusing the best model on the validation set. We evaluate on a single seed due to the limited test set access of GLUE.\n2 4 6 8 10 12 14 16 18 20\nHyperparameter Trials\n0\n2\n4\n6\n8\n10\n12\n14Relative Expected Validation Acc. [%]\nDataset\nQNLI\nQQP\nMNLI\nSNLI\nSST2\nFigure 4: Relative expected validation performance of\nHyperMixer compared to Transformer after tuning the\nlearning rate and dropout via random search.\nticularly strong on SNLI, where HyperMixer is\n6.5% better at small tuning budgets, but less than\n2% better at high budgets. These results indicate\nthat HyperMixer is substantially easier to tune than\nTransformers.\n4.7 HyperMixer Learns Attention Patterns\nWe hypothesized that the token mixing layer of\nHyperMixer offers a mechanism similar to atten-\ntion. To show this, we consider a toy problem with\n1d sequences composed of shape pairs of different\nheights as described in Fleuret (2019). The target\nvalue is the average height in each pair of shapes.\nAn example input is shown in Figure 5a. To solve\nthe task well, for each position, the model must\nattend to other positions with the same shape.\nModels We compare the token mixing layer of\nHyperMixer to three other models: i) None does\nnot model token interactions. All predictions are\nthus only made based on local information. This\nmodel should thus fail. ii) MLPMixer does model\ntoken interactions. Still, since its token mixing\nweights are position-specific, each position has to\nlearn to recognize each shape, which we expect\nto be difficult, especially with little data. iii) Self-\nattention can be considered the upper bound, as it\nmodels the interaction between every two positions\nexplicitly.\nResults Figure 5b shows the mean squared er-\nror on the test examples depending on the num-\nber of training examples. As expected, None fails\non this task. While all other models are able to\nsolve the task with enough training data, MLP-\nMixer is considerably less data-efficient than the\nother two models, requiring 5-10 times more data\nto reach the same performance. This is expected,\nsince in contrast to HyperMixer and self-attention,\nMLPMixer’s token mixing module is not position-\ninvariant. HyperMixer and self-attention reach ap-\nproximately the same performance when training\non 100k examples. However, HyperMixer is more\ndata-efficient than self-attention, which we attribute\nto the simpler model architecture.\n15638\n0 20 40 60 80 100\n0\n5\n10\n15\n20\n25\nInput\nOutput\n(a) Example from the synthetic task\n100 1,000 10,000 100,000\nNumber of training samples\n100\n101\n102\n103\nT est loss\nAttention\nHyperMixer\nMLPMixer\nNone (b) Test loss depending on number of examples.\n0 20 40 60 80 100\n0\n20\n40\n60\n80\n100\nAttention Layer\n(c) Pseudo-attention map of Attention\n0 20 40 60 80 100\n0\n20\n40\n60\n80\n100\nHyperMixer Layer (d) Pseudo-attention map of HyperMixer\nFigure 5: Results and pseudo-attention maps on the synthetic task (Fleuret, 2019).\nWe can measure the interactions between two to-\nkens by computing the gradient of an output token\nwith respect to an input token (pseudo-attention).\nFigures 5d and 5c show the pseudo-attention maps\nof HyperMixer in comparison to attention. We\nobserve that the pseudo-attention weights of Hy-\nperMixer and attention are similar. This indicates\nthat HyperMixer indeed learns an attention-like\nfunction. In contrast, we find these patterns to be\nweaker in MLPMixer (Figure 6, appendix).\n5 Discussion\nIn the following, we first discuss the merits of our\nproposed model, which are the core contributions\nof our paper. We then discuss the scope of our\nanalysis.\n5.1 Impact\nBest all-MLP model HyperMixer was designed\nas an MLP-based architecture with similar induc-\ntive biases as Transformers, which are beneficial\nfor natural language understanding. Our hypothesis\n(H1) is that this leads to improvements over other\nMLP-based methods. Our experimental results sup-\nport this hypothesis, as we find HyperMixer to out-\nperform all MLP-based baselines on all datasets\n(Section 4.3).\nLow cost model The main motivation for an\nMLP-based architecture is the efficiency benefits\ninduced by its simplicity. Therefore, we hypothe-\nsized (H2) that HyperMixer would reduce the cost\nCost(R) ∝E ·D·H to obtain an AI result R.\nThis hypothesis is supported by our experiments.\n15639\nWhile HyperMixer yields results that are on par\nwith Transformer’s results, it reduces the cost of\nall three cost factors: i) The cost of processing a\nsingle example (E) is lower, particularly for long\ninputs due to its linear complexity compared to the\nquadratic complexity of self-attention (Section 4.4).\nii) The number of required training examples (D)\nis reduced, as HyperMixer’s relative performance\nimprovement is larger in the low-resource scenario\n(Section 4.5). iii) HyperMixer requires less hyper-\nparameter tuning than Transformers to reach good\nresults, which is demonstrated by HyperMixer’s\nhigher expected relative improvements at low tun-\ning budgets (Section 4.6).\nAttention-like model Finally, our experiments\non a synthetic task indicate that HyperMixer can\nlearn very similar attention patterns as the self-\nattention mechanism in Transformers (Section 4.7),\nsupporting hypothesis H3. While MLPMixer can\nalso learn similar patterns given enough training\ndata, we believe that it is the introduction of ad-\nequate biases that allows HyperMixer to learn\nthese patterns efficiently. These biases were cho-\nsen based on an analysis of Transformer’s success\nby Henderson (2020). HyperMixer’s own success\nhence supports that analysis.\nIn summary, in our study, HyperMixer is the best-\nperforming MLP-based architecture, and shows\ncomparable performance and behavior as self-\nattention at substantially lower cost. HyperMixer\ncan thus be considered a low cost alternative to\nTransformers.\n5.2 Scope\nSmall resource scenario It is important to note\nthat our study is limited to the small resource sce-\nnario: Our models are small, not pretrained on large\ngeneral-purpose corpora, and trained on datasets\nwith fewer than 1 million examples. It is unclear\nif our results will also hold on larger scale. For\nexample, while gMLP and FNet perform poorly\nin the low-resource scenario as demonstrated in\nour experiments, both models are able to narrow\nthe gap to Transformer-based models as the re-\nsources for pretraining increase (Liu et al., 2021;\nLee-Thorp et al., 2021). We hypothesize that with\nenough resources, these models are able to over-\ncome their shortcomings in terms of inductive bi-\nases. However, there is no reason to believe that\nHyperMixer, being equipped with useful inductive\nbiases, wouldn’t perform on par with Transformers\nin high-resource scenarios while retaining its lower\noverall cost. Quite the contrary, HyperMixer’s lin-\near complexity in sequence length perhaps makes\nit more appropriate for large-scale pretraining on\nlong contexts than vanilla Transformers.\nVersatility One of the most impressive qualities\nof Transformers is their versatility: Not only are\nthey now the standard architecture for all NLP\ntasks, but over the years they have also become\nubiquitous in a wide range of applications domains\noutside of NLP. Of course, the present study cannot\ndetermine whether HyperMixer is as versatile as\nTransformers. However, subsequent studies have\nshown that HyperMixer has uses in speech recog-\nnition (Mai et al., 2023) and neural combinatorial\noptimization (Drakulic et al., 2023). Still, some\nmodeling advancements are needed. For example,\nHyperMixing is not yet applicable for decoder mod-\nels that make use of causal masking. As decoder-\nonly language models have become widely studied,\nthis constitutes promising future work.\n6 Conclusion\nWhile large pretrained Transformer language mod-\nels have led to impressive progress, they re-\nquire so much resources that many research labs\nare excluded from participation, leading to calls\nfor Green AI. We have proposed an MLP-based\nmethod, HyperMixer, that, in contrast to previous\nMLP-based methods, is equipped with the same\ninductive biases that made Transformers so suc-\ncessful for natural language understanding. While\nit performs on par with Transformers, it incurs sub-\nstantially lower cost in terms of processing time,\ntraining data, and hyperparameter tuning. Hence,\nwe believe our study demonstrates the merits of\nMLP-based models for natural language under-\nstanding as an alternative to attention-based mod-\nels, and we hope that the community pursues this\ndirection further. Avenues for future work include\nlarge-scale pretraining, evaluation on a wider range\nof tasks and domains, and the model’s adaptation\nto text generation.\n15640\nLimitations\nMany limitations of our study are already discussed\nin Section 5.2, however, we repeat and add to them\nexplicitly here.\nSmall resource scenario Our study investigates\nMLP-based architectures for text classification\ntasks and finds competitive performance with\nvanilla Transformers while having lower cost in\nterms of the Green AI equation. However, the\nscope of our findings is naturally limited to the test-\ning scenario, which is low-resource: Our models\nare relatively small, not pretrained on large general-\npurpose corpora, and trained on datasets with fewer\nthan 1 million examples. We may not say with\ncertainty that our results will also hold on larger\nscale. For the sake of hypothesis-driven research\nwe consider it more valuable to run many controlled\nsmall-scale experiments rather than few large-scale\nexperiments. Nonetheless, scaling up should cer-\ntainly be part of future research directions, as this\nis essential for optimal task performance.\nLimitation to English pairwise sentence classifi-\ncation tasks Since token mixing is the indepen-\ndent variable in our study, we put our main focus\non English sentence-pair classification tasks with\ntextual input only, which we presume (and provide\nsome evidence for) to be most useful to assess dif-\nferences between token mixing models. Of course,\nvanilla Transformers are very flexible in the sense\nthat, over the course of many studies, they have\nbeen shown to be very effective for a wide range\nof tasks, languages and data modalities. Whether\nor not the proposed HyperMixer model possesses\nsimilar flexibility cannot be answered in this study.\nThe HyperMixer encoder arguably possesses simi-\nlar inductive biases as Transformers. We thus ex-\npect it to be straight-forward to apply to tasks that\nare also solved well by Transformer encoders (e.g.,\nspan classification). For tasks such as language\nmodeling, which involve a Transformer decoder,\nsignificant modeling advancements are required to\nobtain a HyperMixer equivalent. We consider this\na very promising direction for future work.\nLimitation to MLP-based baselines Similar to a\ntrend in the computer vision community, our study\ninvestigates the suitability of MLP-based architec-\ntures for NLP. Due to their conceptual simplicity,\nthese models promise to be easier to train, poten-\ntially leading to reduced Green AI costs. To this\nend we compare our proposed HyperMixer model\nto a range of other MLP-based models, and Trans-\nformers. Apart from FNet and Linear Transform-\ners, which are efficient Transformer alternatives,\nwe do not attempt an exhaustive comparison to\nnon-MLP-based efficient NLP models. Hence, the\nscope of our claims does not extend to all efficient\nTransformer models. However, these models are\nof course very relevant to this study, as they are\ntargeted towards one of the factors of Green AI\ncost (single forward pass complexity). Therefore,\nwe regard a comprehensive comparison as valuable\nfuture work.\nAcknowledgements\nFlorian Mai was supported by the Swiss National\nScience Foundation under the project LAOS, grant\nnumber 200021_178862. Arnaud Pannatier was\nsupported by the Swiss Innovation Agency Inno-\nsuisse under the project MALAT, grant number\n“32432.1 IP-ICT”. Fabio Fehr was supported by\nthe Swiss National Centre of Competence in Re-\nsearch (NCCR) under the project Evolving Lan-\nguage, grant number “51NF40_180888”. Haolin\nChen was supported by the Swiss National Science\nFoundation under the project NAST, grant num-\nber “185010”. François Marelli was supported by\nthe Swiss National Science Foundation under the\nproject COMPBIO, grant number “179217”.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nThomas Bachlechner, Bodhisattwa Prasad Majumder,\nHenry Mao, Gary Cottrell, and Julian McAuley. 2021.\nRezero is all you need: Fast convergence at large\ndepth. In Uncertainty in Artificial Intelligence, pages\n1352–1361. PMLR.\nIrwan Bello. 2021. Lambdanetworks: Modeling long-\nrange interactions without attention. In International\nConference on Learning Representations.\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer. arXiv\npreprint arXiv:2004.05150.\nJames Bergstra and Yoshua Bengio. 2012. Random\nsearch for hyper-parameter optimization. Journal of\nMachine Learning Research, 13(10):281–305.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\n15641\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nJiaao Chen, Derek Tam, Colin Raffel, Mohit Bansal,\nand Diyi Yang. 2021. An empirical survey of data\naugmentation for limited data learning in nlp. arXiv\npreprint arXiv:2106.07499.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy\nSchwartz, and Noah A. Smith. 2019. Show your\nwork: Improved reporting of experimental results. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 2185–\n2194, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJesse Dodge, Suchin Gururangan, Dallas Card, Roy\nSchwartz, and Noah A. Smith. 2021. Expected vali-\ndation performance and estimation of a random vari-\nable’s maximum. In Findings of the Association\nfor Computational Linguistics: EMNLP 2021, pages\n4066–4073, Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nDarko Drakulic, Sofia Michel, Florian Mai, Ar-\nnaud Sors, and Jean-Marc Andreoli. 2023. Bq-\nnco: Bisimulation quotienting for generalizable\nneural combinatorial optimization. arXiv preprint\narXiv:2301.03313.\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science, 14(2):179–211.\nFrançois Fleuret. 2019. Attention mechanisms. Deep\nLearning Course - Chapter 13.2.\nJerry A Fodor and Zenon W Pylyshyn. 1988. Connec-\ntionism and cognitive architecture: A critical analysis.\nCognition, 28(1-2):3–71.\nFrancesco Fusco, Damian Pascual, and Peter Staar. 2022.\npnlp-mixer: an efficient all-mlp architecture for lan-\nguage. arXiv preprint arXiv:2202.04350.\nJianyuan Guo, Yehui Tang, Kai Han, Xinghao Chen,\nHan Wu, Chao Xu, Chang Xu, and Yunhe Wang.\n2021. Hire-mlp: Vision mlp via hierarchical rear-\nrangement.\nDavid Ha, Andrew Dai, and Quoc V Le. 2016. Hyper-\nnetworks. arXiv preprint arXiv:1609.09106.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nMichael A Hedderich, Lukas Lange, Heike Adel, Jan-\nnik Strötgen, and Dietrich Klakow. 2020. A sur-\nvey on recent approaches for natural language pro-\ncessing in low-resource scenarios. arXiv preprint\narXiv:2010.12309.\nJames Henderson. 2020. The unstoppable rise of com-\nputational linguistics in deep learning. pages 6294–\n6306. Association for Computational Linguistics.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790–2799. PMLR.\nShankar Iyer, Nikhil Dandekar, and Kornel Csernai.\n2017. First quora dataset release: Question pairs.\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa\nDehghani, and James Henderson. 2021. Parameter-\nefficient multi-task fine-tuning for transformers via\nshared hypernetworks. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 565–576, Online. Association\nfor Computational Linguistics.\nAngelos Katharopoulos, Apoorv Vyas, Nikolaos Pap-\npas, and François Fleuret. 2020. Transformers are\nrnns: Fast autoregressive transformers with linear\nattention. In International Conference on Machine\nLearning, pages 5156–5165. PMLR.\n15642\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nJames Lee-Thorp, Joshua Ainslie, Ilya Eckstein, and\nSantiago Ontanon. 2021. Fnet: Mixing tokens with\nfourier transforms.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, et al. 2021. Datasets: A commu-\nnity library for natural language processing. arXiv\npreprint arXiv:2109.02846.\nDongze Lian, Zehao Yu, Xing Sun, and Shenghua Gao.\n2022. AS-MLP: An axial shifted MLP architecture\nfor vision. In International Conference on Learning\nRepresentations.\nHanxiao Liu, Zihang Dai, David So, and Quoc V Le.\n2021. Pay attention to mlps. In Advances in Neural\nInformation Processing Systems, volume 34, pages\n9204–9215. Curran Associates, Inc.\nFlorian Mai, Juan Zuluaga-Gomez, Titouan Parcollet,\nand Petr Motlicek. 2023. Hyperconformer: Multi-\nhead hypermixer for efficient speech recognition. In\nInterspeech 2023. ISCA.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learning\nlibrary. In H. Wallach, H. Larochelle, A. Beygelz-\nimer, F. d'Alché-Buc, E. Fox, and R. Garnett, editors,\nAdvances in Neural Information Processing Systems\n32, pages 8024–8035. Curran Associates, Inc.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nTimo Schick and Hinrich Schütze. 2020. It’s not just\nsize that matters: Small language models are also\nfew-shot learners. arXiv preprint arXiv:2009.07118.\nRoy Schwartz, Jesse Dodge, Noah A Smith, and Oren\nEtzioni. 2020. Green ai. Communications of the\nACM, 63(12):54–63.\nKaren Simonyan, Andrea Vedaldi, and Andrew Zis-\nserman. 2014. Deep inside convolutional networks:\nVisualising image classification models and saliency\nmaps. In In Workshop at International Conference\non Learning Representations. Citeseer.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 conference on empiri-\ncal methods in natural language processing, pages\n1631–1642.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for deep\nlearning in nlp. arXiv preprint arXiv:1906.02243.\nChuanxin Tang, Yucheng Zhao, Guangting Wang,\nChong Luo, Wenxuan Xie, and Wenjun Zeng. 2021.\nSparse mlp for image recognition: Is self-attention\nreally necessary?\nYuki Tatsunami and Masato Taki. 2021. Raftmlp: How\nmuch can be done without attention and with less\nspatial locality?\nYi Tay, Zhe Zhao, Dara Bahri, Don Metzler, and Da-\nCheng Juan. 2021. Hypergrid transformers: Towards\na single model for multiple tasks. In ICLR 2021.\nIlya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov,\nLucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jes-\nsica Yung, Andreas Steiner, Daniel Keysers, Jakob\nUszkoreit, et al. 2021. Mlp-mixer: An all-mlp archi-\ntecture for vision. Advances in Neural Information\nProcessing Systems, 34.\nZhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang,\nPeyman Milanfar, Alan Bovik, and Yinxiao Li. 2022.\nMaxim: Multi-axis mlp for image processing. arXiv\npreprint arXiv:2201.02973.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nBen Wang and Aran Komatsuzaki. 2021. Gpt-j-6b: A 6\nbillion parameter autoregressive language model.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu,\nChangliang Li, Derek F Wong, and Lidia S Chao.\n2019. Learning deep transformer models for ma-\nchine translation. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1810–1822.\n15643\nZiyu Wang, Wenhao Jiang, Yiming Zhu, Li Yuan, Yib-\ning Song, and Wei Liu. 2022. Dynamixer: A vision\nmlp architecture with dynamic mixing.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nTan Yu, Xu Li, Yunfeng Cai, Mingming Sun, and Ping\nLi. 2022. S2-mlp: Spatial-shift mlp architecture for\nvision. In Proceedings of the IEEE/CVF Winter Con-\nference on Applications of Computer Vision (WACV),\npages 297–306.\nWeihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen\nZhou, Xinchao Wang, Jiashi Feng, and Shuicheng\nYan. 2021. Metaformer is actually what you need for\nvision.\nAndrey Zhmoginov, Mark Sandler, and Max Vladymy-\nrov. 2022. Hypertransformer: Model generation for\nsupervised and semi-supervised few-shot learning.\narXiv preprint arXiv:2201.04182.\n15644\nAppendix\nA Extended Related Work\nA.1 Green AI\nSchwartz et al. (2020) challenges the current pur-\nsuit for higher accuracy at the cost of larger com-\nputation with the notion of \"Green AI\". More-\nover, Strubell et al. (2019) estimated the monetary\nand environmental cost of large model pretrain-\ning. Apart from being problematic environmen-\ntally, they argue that the monetary cost of pretrain-\ning is too high to be widely accessible for most\nresearchers. In a research community that focuses\non task performance, low resourced researchers\nwould be disadvantaged. Therefore, metrics that\ntake the cost of reaching a result are important\nto consider (Schwartz et al., 2020). The metric\nCost(R) ∝E·D·H, is proposed and discussed\nin Section 1. However, reporting a single metric\nCost(R) is often ambiguous. Therefore, in our\nexperiments, we consider the factors E, D, and H.\nTo measure the computational cost per example\nE, Schwartz et al. (2020) propose a count of the\nfloating point operations (FPOs) required. In our\nexperiments, we adopt this metric and further in-\nclude wall-clock time for a practical application.\nThe component Devaluates the quantity of training\ndata needed to reach a given accuracy or the perfor-\nmance of a model in a low-resource scenario (Hed-\nderich et al., 2020; Chen et al., 2021). Finally,\nthe component H measures the cost associated\nwith hyperparameter tuning. This is reported us-\ning expected validation performance introduced\nby Dodge et al. (2019, 2021), which computes\nthe validation performance one would yield in ex-\npectation after khyperparameter trials of random\nsearch (Bergstra and Bengio, 2012).\nCurrent literature does not focus on all facets\nof Green AI as formalized as Cost(R). Typi-\ncally, improving efficiency involves making exist-\ning models more accessible. For example, improv-\ning accessibility through model distillation (Sanh\net al., 2019) or adapter modules (Houlsby et al.,\n2019). Another avenue involves reducing the com-\nputational complexity, with examples: prompt-\ntuning (Schick and Schütze, 2020), self-attention\nin Transformers (Child et al., 2019; Beltagy et al.,\n2020; Katharopoulos et al., 2020, et cetera). The\nlatter approach is similar to our work. However,\nthey focus the processing time of a single example\nEand do not consider the other facets of Green AI.\nIn our paper, we focus on MLP-based approaches,\nwhich we argue will have improvements in all\nfacets of Green AI due to their simplicity.\nA.2 MLP-based Models\nThe vision domain has seen promising results with\npurely MLP-based models (Tolstikhin et al., 2021),\nhowever, they lack the desired inductive biases for\nNLP. Some desirable properties for modeling lan-\nguage include: i) position invariance, which is\nimportant for generalization, ii) adaptive size for\nvariable-length inputs, iii) a global receptive field,\nwhich allows interactions to not be limited to small\ntoken neighborhoods, iv) learnabilty allowing for\nuniversal applicablility to various tasks, and v) dy-\nnamicity which implies that output is conditioned\non the input. MLP-based models are typically not\nused for NLP as including the inductive biases of\nposition invariance, adaptive size and global recep-\ntive field are non-trivial for MLPs.\nSeveral methods try to overcome the lack of\nadaptivity to size by introducing shifting opera-\ntions and local windows. Yu et al. (2022) and Lian\net al. (2022) uses spatial shifting to pass the infor-\nmation of adjacent tokens through an MLP. (Tang\net al., 2021) uses a circular shifting operator. How-\never, the position invariance is violated because\npositional information is required in the decision\nof which tokens are included in the neighborhood.\nThe aggregation of local information itself is done\nvia a (relative) position-specific MLP. Global inter-\nactions are modeled only through the inclusion of\nenough layers or through a hierarchical layout (Yu\net al., 2022; Guo et al., 2021).\nFor vision tasks it can be useful to exploit the\nfact that 2D images consist of two axes. Tatsunami\nand Taki (2021) make use of this fact by integrating\na respective inductive bias. (Tu et al., 2022) achieve\nlinear complexity by applying a gMLP (Liu et al.,\n2021) to only a single axis.\nA global receptive field in MLP-based models is\nachieved through token mixing and a weighted sum-\nmation of the inputs, similar to self-attention. This\nallows for interaction between tokens. Liu et al.\n(2021) propose the model gMLP, where the mixing\nweights are determined by a fixed learnable inter-\naction matrix between positions. However, this\ncomes at the cost of violating position-invariance,\nsize adaptivity, and dynamicity. DynaMixer (Wang\net al., 2022) enables dynamicity by estimating the\n15645\nmixing weights from the concatenation of the in-\nputs via a linear layer. This is efficient due to a di-\nmensionality reduction step, but the concatenation\nstill implies position-dependence and fixed-sized\ninputs. (Lee-Thorp et al., 2021) proposes the model\nFNet to use static Fourier transformations to model\ntoken interactions. This model made significant\nimprovements in computation cost, although the\nfunctions lack learnability and are position depen-\ndent.\nA.3 Hypernetworks\nA hypernetwork uses a network to generate the\nweights for another, often larger, network (Ha et al.,\n2016). Tay et al. (2021) leveraged task-conditioned\nhypernetworks for the GLUE benchmark. They\nachieved paralleled performance to the state-of-\nthe-art at the time, whilst being more parameter\nefficient. Karimi Mahabadi et al. (2021) applied\nhypernetworks to Transformers to allow for param-\neter sharing in multitask learning. Their results\nshowed parameter efficiencies and improved out of\ndomain generation. Zhmoginov et al. (2022) com-\nbine hypernetworks and transformers in the vision\ndomain for few shot generalization. LambdaNets\nare strongly related to our work, as they generate\nlinear functions from context, in a similar capacity\nto a hypernetwork (Bello, 2021). Their model is\nsimilar to the standard attention mechanism where\nthe weights of three matrices Q,K,V are learned.\nIn contrast, HyperMixer uses the inputs to create\nnon-linear transformations by generating an MLP.\nFeatures are combined based on their locations - a\ncomparison can be found in Appendix E.\nCombining MLPMixer and hypernetworks al-\nlows for an efficient and simple MLP-based model\nto have all the necessary inductive biases for NLP.\nThe MLPMixer provides a simple token interaction\nbackbone. By deploying hypernetworks to build\nthe weights of the token mixing MLP, the miss-\ning inductive biases of position invariance and size\nadaptation are obtained.\nB Experimental Details\nB.1 General Information\nImplementation We implemented all models\nwithin the same general framework based on Py-\nTorch (Paszke et al., 2019). We provide the code in\nthe supplementary material. For tokenization, we\nuse the pretrained tokenizer from BERT-Base (De-\nvlin et al., 2019). Datasets are downloaded directly\nfrom HuggingFace Datasets (Lhoest et al., 2021).\nAs such, they are directly downloaded by our train-\ning script. We apply no further preprocessing.\nFor computing expected validation performance,\nwe use the public implementation by Dodge et al.\n(2019).\nWe run our experiments on single-GPU servers\navailable to us as part of a computation grid, rang-\ning between GeForce GTX Titan X and RTX 3090.\nApart from Transformers on SNLI and MNLI,\nwhich take about 4 hours on slower GPUs, all ex-\nperiments finished within 3 hours.\nHyperparameters We provide CSV files detail-\ning all parameters of every run alongside their re-\nsults in the supplementary material, ensuring re-\nproducibility of our study. Note that the computa-\ntion environment (e.g., type of GPU) might lead to\nsmall differences.\nB.2 Peak Performance\nTo ensure a fair comparison, we aim to compare\nmodels of approximately the same number of\nparameters (≈11 M parameters). All models have\n6 layers with token embedding size d = 256and\nhidden size d′= 512. For MLPMixer and gMLP\nwe set the size of the token mixing modules to\nN = 250 and N = 100, respectively. These\nlengths are chosen to match the number of parame-\nters of the other models (11 M). The hidden layer\nsize is set to 512 in all models. We use dropout at\nthe input to each layer with a probability of 0.1. For\nall models, including the ablations, we first tune\nthe learning rate of Adam (Kingma and Ba, 2014)\nusing a logarithmically spaced grid of 7 valuesα∈\n{0.001,0.0005,0.0002,0.0001,0.00005,0.00002,\n0.00001}on the validation set. For our baselines,\nwe then evaluate 10 different seeds and report\nthe mean accuracy and standard deviation on the\nvalidation set. On the test set, we only report the\nresults of the model yielding the best results on\nthe validation set, as the GLUE benchmark (Wang\net al., 2018) has a hidden test set with limited\naccess. Ablations are evaluated on the validation\nset with a single seed.\nB.3 Time per Example\nDue to the lack of reliable software to measure\nFOPs in PyTorch, we calculate these numbers man-\nually. Our process is described in Appendix D. For\nthe measurement of wallclock time, we measured\nthe time of 1,000 batches through a single layer of\n15646\nDataset # Train # Valid # Test\nMNLI 392,702 9,815 9,796\nSNLI 549,367 9,842 9,824\nQQP 363,846 40,430 390,965\nQNLI 104,743 5,463 5,463\nSST 67,349 872 1,821\nTable 2: Number of examples in each dataset.\neach token mixing module with d= 256,d′= 512\n(as used in our experiments).\nB.4 Toy Task (Section 4.7)\nThis section gives more detail about how we set\nup the synthetic example (Fleuret, 2019) for eval-\nuating whether the different models were able to\nlearn some attention-like transformation. We have\na dataset made of 1D sequences that contain two\nrectangular and two triangular shapes. Each of\nthese shapes has a different height taken at ran-\ndom in the input sequence. The output sequence\nhas the same shapes in the same positions, but the\nheights of triangular shapes should be the mean\nof the two triangular shapes in the input sequence.\nSimilarly, the height of the rectangular shapes in\nthe output sequence is the mean of the height of the\ntwo rectangular shapes in the input sequence.\nSo the model should be able to see across the\nsequence and compute the mean of the two differ-\nent shapes to succeed at the task. All the models\nconsidered for this task have a similar structure:\nthey consist of a particular layer (MLPMixer, Hy-\nperMixer, or Attention) surrounded by two pairs of\n1D-convolutional layers with kernels of size five\nand a symmetric zero-padding of size two so that\nthe output shape is constant. We made an ablation\nto ensure that this layer was mandatory by chang-\ning it with another similar 1D convolutional layer,\nwhich corresponds to None in the figure 5b.\nBefore visualizing the pseudo-attention maps,\nall models were trained on 25,000 training exam-\nples. We use input-gradients (Simonyan et al.,\n2014) to evaluate whether models could « attend\n» to the different shapes. This method computes\nthe gradient of the output sequence with respect\nto the input sequence, giving the corresponding\nsaliency map, which can then be recombined into\na pseudo-attention matrix where the i-th column\ncorresponds to the saliency maps of the i-th out-\nput token. A large value in the (i,j) entries of the\npseudo-attention matrix means that the output to-\nken istrongly depends on the input j, and we can\nthus compare it to an attention matrix 6a.\nFigure 6 represents the pseudo-attention matri-\nces for the different models. We can notice that it\nindeed approximates the true attention matrix 6a\nand that the model with no special layer cannot at-\ntend to the correct part of the sequence, as expected.\nFinally, we can see that the pseudo-attention of the\nMixer layer is not as peaked as the one correspond-\ning to the Attention or HyperMixer layer.\nC Further Results\nC.1 Validation Set Results\nIn Table 3, we show the best scores on the vali-\ndation set that we obtained from the grid search\n(using a fixed seed), alongside the learning rate that\nyielded that score.\nIn Section 4.3, we reported the test set results\nof all models when using the best-performing seed.\nIn Table 4, we show test set results when using the\nmedian seed.\nC.2 Ablations\nWe first describe the ablation models before we\ndiscuss their results.\nFeature Mixing Only The most simplistic MLP\narchitecture is one that doesn’t use token mixing,\ni.e., the token mixing module is set to the identity\nfunction. The outputs at the last layer are aggre-\ngated via average pooling before plugged into the\nlinear classifier. This allows a baseline where the\ntoken interactions are not modeled. Therefore, this\narchitecture serves as a control for how important\ntoken mixing is in any given task.\nToken Mixing Only A simplistic single layer\nMLP architecture ablation. This model consists of\na variable dimension MLP where the weights are\ngenerated using a hypernetwork which only allows\nfor location interaction. This model is included\nto argue that the best simple model requires both\nlocation and feature mixing to efficiently model\ntextual inputs.\nShared Weight-Vector A simple way to obtain\na variable size location-mixing MLP is by weight-\nsharing. Concretely, we use a single learnable\nweight vector w1 ∈Rd′\n, which we copyNtimes to\ncreate a weight matrix W1 ∈RN×d′\n. Analogously,\nwe create W2 from a separate vector w2. Note that\nthis baseline does not support dynamicity, as the\n15647\nModel MNLI SNLI QQP QNLI SST # Params\nBaselines (accuracy / learning rate)\nFNet 59.6 / 5e-4 75.1 / .001 79.7 / .001 59.2 / 5e-4 80.4 / .001 9.5 M\nLinear Transformer 66.2 / .001 82.2 / 0.001 81.7 / 5e-4 61.1 / 1e-4 80.7 / 2e-4 11M\nTransformer 66.0 / 2e-4 81.2 / 2e-4 82.9 / 2e-4 65.4 / 5e-4 78.9 / 5e-4 11 M\nMLPMixer 64.2 / .001 80.5 / .001 83.6 / .001 68.7 / 5e-5 82.3 / .001 11 M\ngMLP 61.5 / .001 80.9 / 2e-4 83.0 / 5e-4 61.1 / 5e-5 79.2 / 1e-4 11 M\nHyperMixer (tied) 66.5 / 1e-4 81.8 / 2e-4 85.4 / 1e-4 77.5 / 5e-5 81.3 / 5e-4 11 M\nAblations (accuracy / learning rate)\nFeature Mixing only 54.4 / .001 67.2 / 5e-4 75.9 / .001 61.0 / .001 81.8 / 5e-4 9 M\nToken Mixing only 59.5 / 2e-4 73.6 / 2e-4 81.7 / 2e-4 61.8 / 2e-4 80.1 / 5e-4 9 M\nShared Weight-Vector 53.7 / 5e-4 68.1 / .001 83.0 / .001 66.4 / 5e-5 80.5 / .001 9.5 M\nHyperMixer (untied) 66.0 / .001 82.3 / .001 84.6 / .001 72.2 / 5e-5 81.3 / .001 12 M\nTable 3: Best validation set results on natural language understanding tasks after tuning the learning rate on a grid.\nModel MNLI SNLI QQP QNLI SST # Params\nBaselines\nFNet 58.8 75.2 78.4 59.0 80.2 9.5 M\nLin. Transformer 67.0 81.9 82.3 61.0 82.5 11 M\nTransformer 64.9 81.1 82.1 67.1 77.7 11 M\nMLPMixer 62.6 79.7 83.2 69.1 80.8 11 M\ngMLP 62.9 79.9 82.3 60.0 78.5 11 M\nHyperMixer (tied) 64.9 81.0 83.9 76.8 80.9 11 M\nTable 4: Test set results on natural language understanding tasks, when using the median seed.\nweight vector is independent of the inputs. This\nbaseline thus shows the importance of dynamicity\nin our model.\nResults Results are shown in Table 5. Untying\nthe hypernetworks in HyperMixer leads to slightly\ndecreased performance on all datasets. We hypothe-\nsize that without pretraining, the model cannot ben-\nefits from more capacious token interaction model-\ning introduced by untying. Nonetheless, the untied\nmodel still performs or a little better than vanilla\nTransformers.\nWhile the introduction of MLPMixer and similar\nmodels follows a trend towards conceptually more\nsimplistic models, our ablations show, perhaps un-\nsurprisingly, that simplicity is not better when it\nleads to discarding information, as both the Feature-\nMixing only and Location-Mixing only models per-\nform substantially worse than the full HyperMixer\nmodel. Moreover, it is not enough to use the same\nlearnable weight vector for all positions (Shared\nWeight-Vector), indicating the importance of gen-\nerating the MLP based on the input.\nThe simplistic Feature-Mixing only model per-\nforms poorly on all datasets except SST, where it\nperforms as well as the other models. This indi-\ncates that many instances in SST can be solved\nby looking at individual tokens alone, rather than\nmodeling their interactions.\nC.3 Visualizing Attention Patterns\nFigure 6 shows the pseudo-attention of all mod-\nels (except ’None’) alongside the true attention\nweights of attention. First, it should be noted\nthat pseudo-attention weights offer a somewhat\nblurry version of true attention weights, where high\nweights occur at positions that correspond to the\nsame shape (cmp. 6a to 6b). Second, we observe\nthat the pseudo-attention weights of HyperMixer\nand attention (cmp. Figure 6d to 6b) are simi-\nlar. This indicates that HyperMixer indeed learns\nan attention-like function. Third, MLPMixer also\nshows a similar pattern, but the relevant positions\nhave weak connections (Figure 6c). This confirms\nour finding that MLPMixer requires substantially\nmore training data to learn strong connections.\n15648\nModel MNLI SNLI QQP QNLI SST # Params\nAblations Validation set results (average accuracy / standard deviation over 10 seeds)\nFeature Mixing only 54.5 (0.25) 67.0 (0.14) 75.9 (0.06) 60.8 (0.42) 79.7 (0.64) 9 M\nToken Mixing only 59.0 (0.79) 74.5 (5.53) 79.5 (4.63) 61.8 (1.29) 76.3 (4.94) 10 M\nShared Weight-Vector 57.1 (2.38) 74.3 (1.96) 82.9 (0.10) 65.9 (0.42) 79.8 (0.52) 9.5 M\nHyperMixer (untied) 65.8 (0.46) 81.7 (0.30) 84.8 (0.23) 73.3 (0.53) 80.3 (0.35) 12 M\nHyperMixer (tied) 66.2 (0.21) 81.9 (0.27) 85.6 (0.20) 78.0 (0.19) 80.7 (0.84) 11 M\nTable 5: Mean and standard deviation of HyperMixer ablations on the validation set.\n0 20 40 60 80 100\n0\n20\n40\n60\n80\n100\nAttention Layer\n(a) True attention map of Attention\n0 20 40 60 80 100\n0\n20\n40\n60\n80\n100\nAttention Layer (b) Pseudo-attention map of Attention\n0 20 40 60 80 100\n0\n20\n40\n60\n80\n100\nMixer Layer\n(c) Pseudo-attention map of MLPMixer\n0 20 40 60 80 100\n0\n20\n40\n60\n80\n100\nHyperMixer Layer (d) Pseudo-attention map of HyperMixer\nFigure 6: Results and (pseudo-)attention maps on the synthetic task (Fleuret, 2019).\n15649\nD Comparison of #FOP\nWe want to compute the number of floating-point\noperations needed in self-attention vs. HyperMix-\ning for a single example. Let N be the sequence\nlength, dbe the embedding size of each token, and\nd′the hidden dimension.\nFor simplicity, we will assume basic mathemati-\ncal operators like exp,tanh,√xand division to be\nequal to one floating operation. However, their ac-\ntual cost is higher but depends on implementation\nand hardware.\nD.1 Basic Building Blocks\nWe first compute the number of operations infre-\nquently occurring in basic building blocks of neural\nnetworks.\nMatrix Multiplication Multiplying matrix A∈\nRN×d A ∈Rd×M takes 2d(NM) operations, as\n2doperations are needed for a single dot-product\nand there are NM entries in the resulting matrix.\nLinear Layer Passing a single vector of size d\nthrough a linear layer without bias of size (d,d′) is\nthe multiplication of a single vector with a matrix,\ni.e., incurs 2dd′operations in total.\nGELU GELU is usually approximated as\nGELU(x) = 0.5x\n[\n1 + tanh\n(√\n2/π(x+ cx3)\n)]\nSo in total, GELU is computed for every of the d\nfeatures and every of the N vectors, meaning the\nGELU activation layer takes 9dN operations.\nMLP (input = output size) Given hidden size\nd′and input/output size d, we have two linear lay-\ners of size (d,d′) and (d′,d), respectively, plus a\nGELU layer on d′dimensions, incurring 4dd′+9d′.\nMLP (input /= output size) Given hidden size\nd′, input size dand output size d′′, we have two\nlinear layers of sizes (d,d′) and (d′,d′′), incurring\n2dd′+ 2d′d′′+ 9d′.\nSoftmax Softmax is applied over N values, each\nof which goes through an exp and a division by\nthe normalization value. The normalization value\nrequires N additions. So in total, the number of\noperations is 3N.\nD.2 HyperMixer\nHyperNetwork (tied case) In the tied case, we\nhave one MLP that generates an output for each\nvector, so the number of operations needed for an\nMLP of input and hidden size dand output sizes\nd′: N(2d2 + 2dd′+ 9d)\nMixing MLP The mixing MLP has input and\noutput size N and hidden size d′, which is applied\nto each of the dembedding dimensions (i.e., after\ntransposition), incurring d(4d′N + 9′) operations\nin total.\nTotal: The total number of operations in Hyper-\nMixer is d(4Nd′+ 9d′) +N(2d2 + 2d′d+ 9d)\nD.3 Self-attention\nMulti-head self-attention with hheads applies self-\nattention independently to each head consisting of\nvectors of size d/h, respectively.\nSelf-attention consists of\n• 3 linear layers to transform queries, keys, and\nvalues: 6h(d/h)2\n• hmatrix multiplications with sizes N(d/h),\ntotalling 2h(d/h)N2 operations\n• softmax: 3N\n• a weighted average for each of the inputs, con-\nsisting of (2dN2) operations.\nIn total: 6h(d/h)2+hN22(d/h)+3N+(2dN2)\nE Connection with Lambda Layers and\nLinear Transformer\nWe saw in Section 4.7 that HyperMixer was able\nto allow a form of attention without computing\nan attention matrix directly and thus scaling only\nlinearly with the input length. In that regard, this\nmethod is similar to other methods such as (Bello,\n2021) or (Katharopoulos et al., 2020). We will de-\nscribe here the difference between these approaches\nand our method. Let us write the standard atten-\ntion formula and the HyperMixer layer under the\nfollowing form:\nAttention(Q,K,V ) =softmax(QKT )V (2)\nHyperMixer(X) =W1σ(WT\n2 X) (3)\nwhere Q,K,V ,W1,W2 ∈RN×d′\n, X ∈RN×d\nand W1,W2 are the weights generated by the hy-\npernetwork.\n15650\nWe can notice that the two operations differ\nmainly in the non-linearity location and the uses\nof linear or non-linear projection of the input. In-\ndeed, attention applies a non-linearity to QKT\nand uses linear projection of the input (Q,K,V )\nto construct the attention map. On the contrary,\nHyperMixer uses two linear mapping of the input\n(W1,W2) and applies a non-linearity to WT\n2 X,\nwhich is similar in a way to KT V . The quadratic\ncost of the attention layer comes from the place of\nthe non-linearity as it requires the explicit compu-\ntation of QKT ∈RN×N which is quadratic with\nrespect to the input size. Most of the strategies\nused to overcome this quadratic cost generally find\na way of moving this non-linearity. This is the\ncase of (Katharopoulos et al., 2020) which applies\nnon-linearities ϕindependently to Q and K and\n(Bello, 2021) that applies softmax only to K. In\nthat regard, these two methods can be compared\nwith HyperMixer as they all scale linearly with the\ninput size due to the non-linearity location. Still,\nHyperMixer is conceptually different because it\nuses a non-linear transformation of the input and\nbecause it uses, in our opinion, a simpler and more\nunderstandable design entirely based on MLPs.\nF Ablations on Transformer Layout\nWhile all Transformer layouts have a feature mix-\ning and a token mixing component in each layer,\nthe arrangement and connection of these compo-\nnents through skip connections and normalization\nlayers remains an open question. The original\nTransformer paper (Vaswani et al., 2017) uses what\nis now known as the \"post-norm\" layout:\nx1 = LayerNorm(x + token_mixing(x))\nxout = LayerNorm(x1 + feature_mixing(x1))\nwhere x ∈ RN×d is the input to the layer, and\nxout ∈RN×d is the output of the layer.\n(Wang et al., 2019) proposes the \"pre-norm\" lay-\nout:\nx1 = x + token_mixing(LayerNorm(x))\nxout = x1 + feature_mixing(LayerNorm(x1))\n(Bachlechner et al., 2021) proposes the \"ReZero\"\nnormalization, which introduces a learnable scalar\nα∈R, initialized to zero:\nx1 = x + α1 ·token_mixing(x)\nxout = x1 + α2 ·feature_mixing(x1)\n(Wang and Komatsuzaki, 2021) observe that a\nspeed-up can be obtained by parallelizing the two\ncomponents:\nxout = x+ token_mixing(LayerNorm(x))\n+ feature_mixing(LayerNorm(x))\n.\nFinally, (Chowdhery et al., 2022) call the follow-\ning the \"standard serialized\" formulation:\nx1 = x + token_mixing(LayerNorm(x))\nxout = x + feature_mixing(LayerNorm(x1)).\nAs Figure 1 shows, this is the model we have fixed\nfor all previous experiments.\nIn the following, we combine each of the pre-\nsented layouts with self-attention and HyperMix-\ning, respectively. Since we noticed early that\nthe training with HyperMixing is not stable with\nsome of the layouts, we also experimented with\nadding two different kinds of normalization to\nHyperMixer: layer normalization applied after\nTM-MLP, as shown in Algorithm 1, and length\nnormalization. For the latter, we simply scale the\ngenerated weight matrices by 1\nM , where M is the\nnumber of keys. The intuition is that this keeps\nthe magnitude of activations in the hidden layer of\nTM-MLP approximately the same across different\ninput lengths.\nResults Table 6 shows the best validation\nset results after tuning the learning rate using\na logarithmically spaced grid of 7 values α ∈\n{0.001,0.0005,0.0002,0.0001,0.00005,0.00002,\n0.00001}.\nThe results show that self-attention is relatively\ninsensitive with respect to the type of layout, as all\nmodels except for ReZero attain an accuracy of 76-\n77% on average. In contrast, HyperMixer without\nnormalization performs substantially worse with\nprenorm, ReZero, and the parallel layout. Length\nnormalization mitigates this problem to some de-\ngree, but the addition of layer normalization yields\nthe overall best results, where all models achieve\nbetween 77 and 78% of accuracy on average. We,\ntherefore, recommend adding layer normalization\nby default when using HyperMixing in a new con-\ntext.\n15651\nLayout MNLI SNLI QQP QNLI SST Average\nMulti-head self-attention\nserialized 65.71 80.88 82.99 69.67 79.70 75.79\npost-norm 66.13 81.70 84.31 71.54 79.70 76.68\npre-norm 66.60 80.59 82.96 73.13 80.73 76.80\nReZero 56.83 70.85 77.72 63.44 78.10 69.39\nparallel 66.30 81.46 83.12 71.55 79.70 76.43\nHyperMixing (no normalization)\nserialized 66.18 81.63 85.59 78.4 81.65 78.69\npost-norm 62.59 79.49 82.37 76.75 80.39 76.32\npre-norm 56.62 78.49 82.88 64.18 81.08 72.65\nReZero 35.45 33.82 63.18 49.46 49.08 46.20\nparallel 60.37 79.71 83.62 65.24 80.16 73.82\nHyperMixing (length normalization)\nserialized 65.91 81.27 85.27 77.80 81.88 78.43\npost-norm 62.67 79.46 82.61 76.53 80.39 76.33\npre-norm 64.83 80.71 84.41 76.31 81.65 77.58\nReZero 35.45 33.82 63.18 70.31 54.13 51.38\nparallel 65.37 81.12 84.44 76.77 80.28 77.60\nHyperMixing (layer normalization)\nserialized 66.47 81.36 85.74 77.72 80.50 78.36\npost-norm 64.26 80.05 83.81 76.62 80.85 77.12\npre-norm 64.72 81.05 83.81 76.11 81.54 77.45\nReZero 65.64 80.74 84.45 74.41 81.08 77.26\nparallel 65.49 80.59 84.43 76.53 81.65 77.74\nTable 6: Best validation set results on natural language understanding tasks after tuning the learning rate on a grid.\n15652\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□ A1. Did you describe the limitations of your work?\nLeft blank.\n□ A2. Did you discuss any potential risks of your work?\nLeft blank.\n□ A3. Do the abstract and introduction summarize the paper’s main claims?\nLeft blank.\n□ A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □ Did you use or create scientiﬁc artifacts?\nLeft blank.\n□ B1. Did you cite the creators of artifacts you used?\nLeft blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nLeft blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nLeft blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nLeft blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nLeft blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □ Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n15653\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nLeft blank.\nD □ Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nLeft blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nLeft blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nLeft blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nLeft blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nLeft blank.\n15654",
  "topic": "Chen",
  "concepts": [
    {
      "name": "Chen",
      "score": 0.4893614649772644
    },
    {
      "name": "Computer science",
      "score": 0.47401294112205505
    },
    {
      "name": "Geology",
      "score": 0.0602685809135437
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}