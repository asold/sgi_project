{
  "title": "Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling",
  "url": "https://openalex.org/W2963972328",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2100063962",
      "name": "Liyuan Liu",
      "affiliations": [
        "University of Southern California",
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2223914299",
      "name": "Jingbo Shang",
      "affiliations": [
        "University of Southern California",
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2798684433",
      "name": "Xiaotao Gu",
      "affiliations": [
        "University of Illinois Urbana-Champaign",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2105091540",
      "name": "Jian Peng",
      "affiliations": [
        "University of Illinois Urbana-Champaign",
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2103606203",
      "name": "Jiawei Han",
      "affiliations": [
        "University of Illinois Urbana-Champaign",
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2952729433",
    "https://openalex.org/W2574163994",
    "https://openalex.org/W2160236242",
    "https://openalex.org/W2963034893",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963983719",
    "https://openalex.org/W4294555862",
    "https://openalex.org/W2524428287",
    "https://openalex.org/W1690739335",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2963563735",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W2964118293",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2611669587",
    "https://openalex.org/W2319920447",
    "https://openalex.org/W2756381707",
    "https://openalex.org/W2098921539",
    "https://openalex.org/W2963703075",
    "https://openalex.org/W2951299559",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2963706742",
    "https://openalex.org/W2581624817",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2519314406",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W2004763266",
    "https://openalex.org/W2963674932",
    "https://openalex.org/W2963625095",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W2963839582",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2962676330",
    "https://openalex.org/W1539309091",
    "https://openalex.org/W2963000224",
    "https://openalex.org/W4300427683",
    "https://openalex.org/W2606347107",
    "https://openalex.org/W3099701504",
    "https://openalex.org/W2764043458",
    "https://openalex.org/W2769137120",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W2739034105",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W1486649854",
    "https://openalex.org/W4294344649"
  ],
  "abstract": "Many efforts have been made to facilitate natural language processing tasks with pre-trained language models (LMs), and brought significant improvements to various applications. To fully leverage the nearly unlimited corpora and capture linguistic information of multifarious levels, large-size LMs are required; but for a specific task, only parts of these information are useful. Such large-sized LMs, even in the inference stage, may cause heavy computation workloads, making them too time-consuming for large-scale applications. Here we propose to compress bulky LMs while preserving useful information with regard to a specific task. As different layers of the model keep different information, we develop a layer selection method for model pruning using sparsity-inducing regularization. By introducing the dense connectivity, we can detach any layer without affecting others, and stretch shallow and wide LMs to be deep and narrow. In model training, LMs are learned with layer-wise dropouts for better robustness. Experiments on two benchmark datasets demonstrate the effectiveness of our method.",
  "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1215–1225\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n1215\nEfﬁcient Contextualized Representation:\nLanguage Model Pruning for Sequence Labeling\nLiyuan Liu † Xiang Ren ♯ Jingbo Shang † Xiaotao Gu † Jian Peng † Jiawei Han †\n†University of Illinois at Urbana-Champaign, Urbana, IL, USA\n♯ University of Southern California, Los Angeles, CA, USA\n†{ll2, shang7, xiaotao2, jianpeng, hanj}@illinois.edu ♯xiangren@usc.edu\nAbstract\nMany efforts have been made to facilitate natu-\nral language processing tasks with pre-trained\nlanguage models (LMs), and brought signiﬁ-\ncant improvements to various applications. To\nfully leverage the nearly unlimited corpora and\ncapture linguistic information of multifarious\nlevels, large-size LMs are required; but for a\nspeciﬁc task, only parts of these information\nare useful. Such large-sized LMs, even in the\ninference stage, may cause heavy computation\nworkloads, making them too time-consuming\nfor large-scale applications. Here we propose\nto compress bulky LMs while preserving use-\nful information with regard to a speciﬁc task.\nAs different layers of the model keep differ-\nent information, we develop a layer selec-\ntion method for model pruning using sparsity-\ninducing regularization. By introducing the\ndense connectivity, we can detach any layer\nwithout affecting others, and stretch shallow\nand wide LMs to be deep and narrow. In\nmodel training, LMs are learned with layer-\nwise dropouts for better robustness. Experi-\nments on two benchmark datasets demonstrate\nthe effectiveness of our method.\n1 Introduction\nBeneﬁted from the recent advances in neural net-\nworks (NNs) and the access to nearly unlim-\nited corpora, neural language models are able to\nachieve a good perplexity score and generate high-\nquality sentences. These LMs automatically cap-\nture abundant linguistic information and patterns\nfrom large text corpora, and can be applied to fa-\ncilitate a wide range of NLP applications (Rei,\n2017; Liu et al., 2018; Peters et al., 2018).\nRecently, efforts have been made on learning\ncontextualized representations with pre-trained\nlanguage models (LMs) (Peters et al., 2018).\nThese pre-trained layers brought signiﬁcant im-\nprovements to various NLP benchmarks, yielding\nthe Recurrent Unit: the Input of the Recurrent Unit:\ninput\noutput\ninput\noutput\noutput\ninput\nIntroducing the dense connectivity \nto detach any layers without \neliminating other layers\nConducting pruning by \nlayer selection.\nPruned RNN\n(resulting model)\nDensely-connected RNN\n(deep & narrow)\nVanilla Stacked-RNN\n(shallow & wide)\nFigure 1: Leverage the dense connectivity to compress\nmodels via layer selection, and replace wide and shal-\nlow RNNs with deep and narrow ones.\nup to 30% relative error reductions. However, due\nto high variability of language, gigantic NNs (e.g.,\nLSTMs with 8,192 hidden states) are preferred to\nconstruct informative LMs and extract multifar-\nious linguistic information (Peters et al., 2017).\nEven though these models can be integrated with-\nout retraining (using their forward pass only), they\nstill result in heavy computation workloads during\ninference stage, making them prohibitive for real-\nworld applications.\nIn this paper, we aim to compress LMs for\nthe end task in a plug-in-and-play manner. Typ-\nically, NN compression methods require the re-\ntraining of the whole model (Mellempudi et al.,\n2017). However, neural language models are usu-\nally composed of RNNs, and their backpropaga-\ntions require signiﬁcantly more RAM than their\ninference. It would become even more cumber-\nsome when the target task equips the coupled LMs\nto capture information in both directions. There-\nfore, these methods do not ﬁt our scenario very\n1216\nwell. Accordingly, we try to compress LMs while\navoiding costly retraining.\nIntuitively, layers of different depths would\ncapture linguistic information of different lev-\nels. Meanwhile, since LMs are trained in a task-\nagnostic manner, not all layers and their extracted\ninformation are relevant to the end task. Hence,\nwe propose to compress the model by layer se-\nlection, which retains useful layers for the target\ntask and prunes irrelevant ones. However, for the\nwidely-used stacked-LSTM, directly pruning any\nlayers will eliminate all subsequent ones. To over-\ncome this challenge, we introduce the dense con-\nnectivity. As shown in Fig. 1, it allows us to detach\nany layers while keeping all remaining ones, thus\ncreating the basis to avoid retraining. Moreover,\nsuch connectivity can stretch shallow and wide\nLMs to be deep and narrow (Huang et al., 2017),\nand enable a more ﬁne-grained layer selection.\nFurthermore, we try to retain the effective-\nness of the pruned model. Speciﬁcally, we mod-\nify the L1 regularization for encouraging the se-\nlection weights to be not only sparse but bi-\nnary, which protects the retained layer connections\nfrom shrinkage. Besides, we design a layer-wise\ndropout to make LMs more robust and better pre-\npared for the layer selection.\nWe refer to our model as LD-Net, since the\nlayer selection and the dense connectivity form the\nbasis of our pruning methods. For evaluation, we\napply LD-Net on two sequence labeling bench-\nmark datasets, and demonstrated the effectiveness\nof the proposed method. In the CoNLL03 Named\nEntity Recognition (NER) task, the F1 score in-\ncreases from 90.78 ±0.24% to 91.86 ±0.15% by\nintegrating the unpruned LMs. Meanwhile, after\npruning over 90% calculation workloads from the\nbest performing model 1 (92.03%), the resulting\nmodel still yields 91.84 ±0.14%. Our implemen-\ntations and pre-trained models would be released\nfor futher study2.\n2 LD-Net\nGiven a input sequence of T word-level tokens,\n{x1, x2, ···, xT}, we use xt to denote the embed-\nding of xt. For a L-layers NN, we mark the input\nand output of the lth layer at the tth time stamp as\nxl,t and hl,t.\n1Based on their performance on the development sets\n2 https://github.com/LiyuanLucasLiu/\nLD-Net.\n2.1 RNN and Dense Connectivity\nWe represent one RNN layer as a function:\nhl,t = Fl(xl,t,hl,t−1) (1)\nwhere Fl is the recurrent unit of lth layer, it could\nbe any RNNs variants, and the vanilla LSTMs is\nused in our experiments.\nAs deeper NNs usually have more representa-\ntion power, RNN layers are often stacked together\nto form the ﬁnal model by setting xl,t = hl−1,t.\nThese vanilla stacked-RNN models, however, suf-\nfer from problems like the vanishing gradient, and\nit’s hard to train very deep models.\nRecently, the dense connectivity and residual\nconnectivity have been proposed to handle these\nproblems (He et al., 2016; Huang et al., 2017).\nSpeciﬁcally, dense connectivity refers to adding\ndirect connections from any layer to all its subse-\nquent layers. As illustrated in Figure 1, the input\nof lth layer is composed of the original input and\nthe output of all preceding layers as follows.\nxl,t = [xt,h1,t,··· ,hl−1,t]\nSimilarly, the ﬁnal output of the L-layer RNN is\nht = [xt,h1,t,··· ,hL,t]. With dense connectiv-\nity, we can detach any single layer without elim-\ninating its subsequent layers (as in Fig. 1). Also,\nexisting practices in computer vision demonstrate\nthat such connectivities can lead to deep and nar-\nrow NNs and distribute parameters into different\nlayers. Moreover, different layers in LMs usually\ncapture linguistic information of different levels.\nHence, we can compress LMs for a speciﬁc task\nby pruning unrelated or unimportant layers.\n2.2 Language Modeling\nLanguage modeling aims to describe the sequence\ngeneration. Normally, the generation probability\nof the sequence {x1,··· ,xT}is deﬁned in a “for-\nward” manner:\np(x1,··· ,xT) =\nT∏\nt=1\np(xt|x1,··· ,xt−1) (2)\nWhere p(xt|x1,··· ,xt−1) is computed based on\nthe output of RNN, ht. Due to the dense con-\nnectivity, ht is composed of outputs from different\nlayers, which are designed to capture linguistic in-\nformation of different levels. Similar to the bot-\ntleneck layers employed in the DenseNet (Huang\net al., 2017), we add additional layers to unify such\n1217\ninformation. Accordingly, we add an projection\nlayer with the ReLU activation function:\nh∗\nt = ReLU(Wproj ·ht + bproj) (3)\nBased on h∗\nt, it’s intuitive to calculate\np(xt|x1,··· ,xt−1) by the softmax function,\ni.e., softmax(Wout ·h∗\nt + b).\nSince the training of language models needs\nnothing but the raw text, it has almost unlimited\ncorpora. However, conducting training on ex-\ntensive corpora results in a huge dictionary, and\nmakes calculating the vanilla softmax intractable.\nSeveral techniques have been proposed to handle\nthis problem, including adaptive softmax (Grave\net al., 2017), slim word embedding (Li et al.,\n2018), the sampled softmax and the noise con-\ntrastive estimation (J´ozefowicz et al., 2016). Since\nthe major focus of our paper does not lie in the\nlanguage modeling task, we choose the adaptive\nsoftmax because of its practical efﬁciency when\naccelerated with GPUs.\n2.3 Contextualized Representations\nAs pre-trained LMs can describe the text genera-\ntion accurately, they can be utilized to extract in-\nformation and construct features for other tasks.\nThese features, referred as contextualized repre-\nsentations, have been demonstrated to be essen-\ntially useful (Peters et al., 2018). To capture in-\nformation from both directions, we utilized not\nonly forward LMs, but also backward LMs. Back-\nward LMs are based on Eqn. 4 instead of Eqn. 2.\nSimilar to forward LMs, backward LMs approach\np(xt|xt+1,··· ,xT) with NNs. For reference, the\noutput of the RNN in backward LMs for xt is\nrecorded as hr\nt.\np(x1,··· ,xn) =\nT∏\nt=1\np(xt|xt+1,··· ,xT) (4)\nIdeally, the ﬁnal output of LMs (e.g.,h∗\nt) would\nbe the same as the representation of the target\nword (e.g., xt+1); therefore, it may not contain\nmuch context information. Meanwhile, the output\nof the densely connected RNN (e.g., ht) includes\noutputs from every layer, thus summarizing all ex-\ntracted features. Since the dimensions of ht could\nbe too large for the end task, we add a non-linear\ntransformation to calculate the contextualized rep-\nresentation (rt):\nrt = ReLU(Wcr ·[ht,hr\nt] +bcr) (5)\nOur proposed method bears the same intuition\nas the ELMo (Peters et al., 2018). ELMo is de-\nsigned for the vanilla stacked-RNN, and tries to\ncalculate a weighted average of different layers’\noutputs as the contextualized representation. Our\nmethod, beneﬁted from the dense connectivity and\nits narrow structure, can directly combine the out-\nputs of different layers by concatenation. It does\nnot assume the outputs of different layers to be in\nthe same vector space, thus having more potential\nfor transferring the constructed token representa-\ntions. More discussions are available in Sec. 4.\n2.4 Layer Selection\nTypical model compression methods require re-\ntraining or gradient calculation. For the coupled\nLMs, these methods require even more computa-\ntion resources compared to the training of LMs,\nthus not ﬁtting our scenario very well.\nBeneﬁted from the dense connectivity, we are\nable to train deep and narrow networks. Moreover,\nwe can detach one of its layer without eliminating\nall subsequent layers (as in Fig. 1). Since different\nlayers in NNs could capture different linguistic in-\nformation, only a few of them would be relevant\nor useful for a speciﬁc task. As a result, we try\nto compress these models by the task-guided layer\nselection. For i-th layer, we introduce a binary\nmask zi ∈ {0,1}and calculate hl,t with Eqn. 6\ninstead of Eqn. 1.\nhl,t = zi ·Fl(xl,t,hl,t−1) (6)\nWith this setting, we can conduct a layer selection\nby optimizing the regularized empirical risk:\nmin L+ λ0 ·R (7)\nwhere Lis the empirical risk for the sequence la-\nbeling task and Ris the sparse regularization.\nThe ideal choice for Rwould be the L0 regu-\nlarization of z, i.e., R0(z) = |z|0. However, it\nis not continuous and cannot be efﬁciently opti-\nmized. Hence, we relax zi from binary to a real\nvalue (i.e., 0 ≤zi ≤1) and replace R0 by:\nR1 = |z|1\nDespite the sparsity achieved by R1, it could\nhurt the performance by shifting all zi far away\nfrom 1. Such shrinkage introduces additional\nnoise in hl,t and xl,t, which may result in inef-\nfective pruned LMs. Since our goal is to conduct\n1218\nFigure 2: Penalty values of various Rfor z with three dimensions. λ1 has been set to 2 for R2 and R3.\npruning without retraining, we further modify the\nL1 regularization to achieve sparsity while allevi-\nating its shrinkage effect. As the target of Ris to\nmake z sparse, it can be “turned-off” after achiev-\ning a satisfying sparsity. Therefore, we extend R1\nto a margin-based regularization:\nR2 = δ(|z|0 >λ1)|z|1\nIn addition, we also want to make up the re-\nlaxation made on z, i.e., relaxing its values from\nbinary to [0,1]. Accordingly, we add the penalty\n|z(1 −z)|1 to encourage z to be binary (Murray\nand Ng, 2010) and modify R2 into R3:\nR3 = δ(|z|0 >λ1)|z|1 + |z(1 −z)|1\nTo compare R1, R2 and R3, we visualize their\npenalty values in Fig. 2. The visualization is\ngenerated for a 3-dimensional z while the tar-\ngeted sparsity, λ1, is set to 2. Comparing to\nR1, we can observe that R2 enlarges the optimal\npoint set from 0 to all z with a satisfying spar-\nsity, thus avoiding the over-shrinkage. To better\ndemonstrate the effect of R3, we further visualize\nits penalties after achieving a satisfying sparsity\n(w.l.o.g., assuming z3 = 0). One can observe that\nit penalizes non-binary z and favors binary values.\n2.5 Layer-wise Dropout\nSo far, we’ve customized the regularization term\nfor the layer-wise pruning, which protects the re-\ntained connections among layers from shrinking.\nAfter that, we try to further retain the effective-\nness of the compressed model. Speciﬁcally, we\nchoose to prepare the LMs for the pruned inputs,\nthus making them more robust to pruning.\nAccordingly, we conduct the training of LMs\nwith a layer-wise dropout. As in Figure 3, a\nrandom part of layers in the LMs are randomly\ndropped during each batch. The outputs of the\ndropped layers will not be passed to their subse-\nquent recurrent layers, but will be sent to the pro-\njection layer (Eqn. 3) for predicting the next word.\nthe Recurrent Unit:\n(b)\nthe Input of the \nRecurrent Unit:\ninput\noutput\n(a)\ninput\noutput\nLayer-wise\n \nDropout\nthe Dropped\nRecurrent Unit:\nFigure 3: Layer-wise dropout conducted on a 4-layer\ndensely connected RNN. (a) is the remained RNN. (b)\nis the original densely connected RNN.\nIn other words, this dropout is only applied to the\ninput of recurrent layers, which aims to imitate the\npruned input without totally removing any layers.\n3 Sequence Labeling\nIn this section, we will introduce our sequence la-\nbeling architecture, which is augmented with the\ncontextualized representations.\n3.1 Neural Architecture\nFollowing the recent studies (Liu et al., 2018;\nKuru et al., 2016), we construct the neural archi-\ntecture as in Fig. 4. Given the input sequence\n{x1,x2,··· ,xT}, for tth token (xt), we assume\nits word embedding is wt, its label is yt, and its\ncharacter-level input is {ci,1,ci,2,··· ,ci, }, where\nci, is the space character following xt.\nThe character-level representations have be-\ncome the required components for most of the\nstate-of-the-art. Following the recent study (Liu\net al., 2018), we employ LSTMs to take the\ncharacter-level input in a context-aware manner,\nand mark its output for xt as ct. Similar to the\ncontextualized representation, ct usually has more\ndimensions than wt. To integrate them together,\nwe set the output dimension of Eqn. 5 as the di-\nmension of wt, and project ct to a new space with\nthe same dimension number. We mark the pro-\njected character-level representation as c∗\nt.\n1219\n␣ V i n k e n ␣\n  embedding\n lstm\ndown-projection\nc 1 , c 2 , 0 c 2 , 1 c 2 , 2 c 2 , 3 c 2 , 4 c 2 , 5 c 2 ,\nVinken\nVinken\nPierre ,\nPierre ,\nPierre ,Vinken\n[ ]\nE-PER\ny 2\n…\n…\n…\n…\n…\n……\n… …\n…\n…\n…\n…\n…\n…\n…\nCRF for Sequence Labeling\nBackward LM\nForward LM\n ﬁxed lstm\n[ ] concatenate\nw 2\nFigure 4: The proposed sequence labeling architecture\nwith contextualized representations.\nAfter projections, these vectors are concate-\nnated as vt = [c∗\nt; rt; wt],∀i ∈ [1,T] and fur-\nther fed into the word-level LSTMs. We refer to\ntheir output as U = {u1,··· ,uT}. To ensure\nthe model to predict valid label sequences, we ap-\npend a ﬁrst-order conditional random ﬁeld (CRF)\nlayer to the model (Lample et al., 2016). Speciﬁ-\ncally, the model deﬁnes the generation probability\nof y = {y1,··· ,yT}as\np(y|U) =\n∏T\nt=1 φ(yt−1,yt,ut)∑\nˆy∈Y(U)\n∏T\nt=1 φ(ˆyt−1,ˆyt,ut)\n(8)\nwhere ˆy = {ˆy1,..., ˆyT}is a generic label se-\nquence, Y(U) is the set of all generic label se-\nquences for U and φ(yt−1,yt,ut) is the potential\nfunction. In our model, φ(yt−1,yt,ut) is deﬁned\nas exp(Wyt ut + byt−1,yt ), where Wyt and byt−1,yt\nare the weight and bias parameters.\n3.2 Model Training and Inference\nWe use the following negative log-likelihood as\nthe empirical risk.\nL= −\n∑\nU\nlog p(y|U) (9)\nFor testing or decoding, we want to ﬁnd the op-\ntimal sequence y∗that maximizes the likelihood.\ny∗= argmax\ny∈Y(U)\np(y|U) (10)\nAlthough the denominator of Eq. 8 is complicated,\nwe can calculate Eqs. 9 and 10 efﬁciently by the\nViterbi algorithm.\nFor optimization, we decompose it into two\nsteps, i.e., model training and model pruning.\nModel training. We set λ0 to 0 and optimize\nthe empirical risk without any regularization, i.e.,\nmin L. In this step, we conduct optimization with\nthe stochastic gradient descent with momentum.\nFollowing (Peters et al., 2018), dropout would be\nadded to both the coupled LMs and the sequence\nlabeling model.\nModel pruning. We conduct the pruning based\non the checkpoint which has the best performance\non the development set during the model train-\ning. We set λ0 to non-zero values and optimize\nmin L+ λ0R3 by the projected gradient descent\nwith momentum. Any layer iwith zi = 0would\nbe deleted in the ﬁnal model to complete the prun-\ning. To get a better stability, dropout is only added\nto the sequence labeling model.\n4 Experiments\nWe will ﬁrst discuss the capability of the LD-Net\nas language models, then explore the effectiveness\nof its contextualized representations.\n4.1 Language Modeling\nFor comparison, we conducted experiments on the\none billion word benchmark dataset (Chelba et al.,\n2013) with both LD-Net (with 1,600 dimensional\nprojection) and the vanilla stacked-LSTM. Both\nkinds of models use word embedding (random ini-\ntialized) of 300 dimension as input and use the\nadaptive softmax (with default setting) as an ap-\nproximation of the full softmax. Additionally,\nas preprocessing, we replace all tokens occurring\nequal or less than 3 times with as UNK, which\nshrinks the dictionary from 0.79M to 0.64M.\nThe optimization is performed by the Adam al-\ngorithm (Kingma and Ba, 2014), the gradient is\nclipped at 5.0 and the learning rate is set to start\nfrom 0.001. The layer-wise dropout ratio is set\nto 0.5, the RNNs are unrolled for 20 steps with-\nout resetting the LSTM states, and the batch size\nis set to 128. Their performances are summa-\nrized in Table 1, together with several LMs used in\nour sequence labeling baselines. For models with-\nout ofﬁcial reported parameter numbers, we esti-\nmate their values (marked with†) by assuming they\nadopted the vanilla LSTM. Note that, for models\n3, 5, 6, 7, 8, and 9, PPL refers to the averaged per-\nplexity of the forward and the backward LMs.\nWe can observe that, for those models tak-\ning word embedding as the input, embedding\ncomposes the vast majority of model parame-\nters. However, embedding can be embodied as a\n“sparse” layer which is computationally efﬁcient.\nInstead, the intense calculations are conducted in\n1220\nNetwork Ind. # Hid. # Layer # Param.# (·107) PPL\nRNN Others\n8192-1024 (J´ozefowicz et al., 2016) 1 8192 2 15.1 ♯ 163♯ 30.6\nCNN-8192-1024 (J´ozefowicz et al., 2016) 2 8192 2 15.1 ♯ 89♯ 30.0\nCNN-4096-512 (Peters et al., 2018) 3 4096 2 3.8 ♯ 40.6♯ 39.7\n2048-512 (Peters et al., 2017) 4 2048 1 0.9 ♯ 40.6♯ 47.50\n2048-Adaptive (Grave et al., 2017) 5 2048 2 5.2 † 26.5† 39.8\nvanilla LSTM 6 2048 2 5.3 † 25.6† 40.27\n7 1600 2 3.2 † 24.2† 48.85\nLD-Net without Layer-wise Dropout 8 300 10 2.3 † 24.2† 45.14\nLD-Net with Layer-wise Dropout 9 300 10 2.3 † 24.2† 50.06\nTable 1: Performance comparison of language models. Models marked with †adopted adaptive softmax and the\nvanilla LSTMs, which has less softmax parameters. Models marked with ♯ employed sampled softmax LSTMs w.\nprojection, which results in less RNN parameters w.r.t. the size of hidden states.\nRNN layers and softmax layer for language mod-\neling, or RNN layers for contextualized represen-\ntations. At the same time, comparing the model\n8192-1024 and CNN-8192-1024, their only differ-\nence is the input method. Instead of taking word\nembedding as the input, CNN-8192-1024 utilizes\nCNN to compose word representation from the\ncharacter-level input. Despite the greatly reduced\nparameter number, the perplexity of the resulting\nmodels remains almost unchanged. Since replac-\ning embedding layer with CNN would make the\ntraining slower, we only conduct experiments with\nmodels taking word embedding as the input.\nComparing LD-Net with other baselines, we\nthink it achieves satisfactory performance with re-\ngard to the size of hidden states. It demonstrates\nthe LD-Net’s capability of capturing the underly-\ning structure of natural language. Meanwhile, we\nﬁnd that the layer-wise dropout makes it harder\nto train LD-Net and its resulting model achieves\nless competitive results. However, as would be\ndiscussed in the next section, layer-wise dropout\nallows the resulting model to generate better con-\ntextualized representations and be more robust to\npruning, even with a higher perplexity.\n4.2 Sequence Labeling\nFollowing TagLM (Peters et al., 2017), we eval-\nuate our methods in two benchmark datasets,\nthe CoNLL03 NER task (Tjong Kim Sang and\nDe Meulder, 2003) and the CoNLL00 Chunking\ntask (Tjong Kim Sang and Buchholz, 2000).\nCoNLL03 NER has four entity types and includes\nthe standard training, development and test sets.\nCoNLL00 chunking deﬁnes eleven syntactic\nchunk types (e.g., NP and VP) in addition to\nOther. Since it only includes training and test\nsets, we sampled 1000 sentences from training set\nas a held-out development set (Peters et al., 2017).\nIn both cases, we use the BIOES labeling\nscheme (Ratinov and Roth, 2009) and use the\nmicro-averaged F1 as the evaluation metric. Based\non the analysis conducted in the development set,\nwe set λ0 = 0.05 for the NER task, and λ0 = 0.5\nfor the Chunking task. As discussed before, we\nconduct optimization with the stochastic gradient\ndescent with momentum. We set the batch size,\nthe momentum, and the learning rate to 10, 0.9,\nand ηt = η0\n1+ρt respectively. Here, η0 = 0.015 is\nthe initial learning rate and ρ = 0.05 is the de-\ncay ratio. Dropout is applied in our model, and its\nratio is set to0.5. For a better stability, we use gra-\ndient clipping of 5.0. Furthermore, we employ the\nearly stopping in the development set and report\naveraged score across ﬁve different runs.\nRegarding the network structure, we use the\n30-dimension character-level embedding. Both\ncharacter-level and word-level RNNs are set to\none-layer LSTMs with 150-dimension hidden\nstates in each direction. The GloVe 100-dimension\npre-trained word embedding3 is used as the initial-\nization of word embedding wt, and will be ﬁne-\ntuned during the training. The layer selection vari-\nables zi are initialized as 1, remained unchanged\n3 https://nlp.stanford.edu/projects/glove/\n1221\nNetwork Avg. #FLOPs F1 score\n(LMs Ind.#) ppl ( ·106) (avg ±std)\nNoLM (/) / 3 94.42 ±0.08\nR-ELMo (6) 40.27 108 96.19 ±0.07\nR-ELMo (7) 48.85 68 95.86 ±0.04\nLD-Net ∗(8) 45.14 51 96.01 ±0.07\nLD-Net ∗(9) 50.06 51 96.05 ±0.08\nLD-Net (8) origin 51 96.13\npruned 13 95.46 ±0.18\nLD-Net (9) origin 51 96.15\npruned 10 95.66 ±0.04\nTable 2: Performance comparisons in the CoNLL00\nChunking task. LD-Net maked with ∗are trained with-\nout pruning (layer selection).\nduring the model training and only be updated dur-\ning the model pruning. All other variables are ran-\ndomly initialized (Glorot and Bengio, 2010).\nCompared methods. The ﬁrst baseline, referred\nas NoLM, is our sequence labeling model with-\nout the contextualized representations, i.e., calcu-\nlating vt as [c∗\nt; wt] instead of [c∗\nt; rt; wt]. Be-\nsides, ELMo (Peters et al., 2018) is the major base-\nline. To make comparison more fair, we imple-\nmented the ELMo model and use it to calculate\nthe rt in Eqn. 5 instead of [ht,hr\nt]. Results of re-\nimplemented models are referred with R-ELMo\n(λ is set to the recommended value, 0.1) and the\nresults reported in its original paper are referred\nwith O-ELMo. Additionally, since TagLM (Peters\net al., 2017) with one-layer NNs can be viewed as\na special case of ELMo, we also include its results.\nSequence labeling results. Table 2 and 3 sum-\nmarizes the results of LD-Net and baselines. Be-\nsides the F1 score and averaged perplexity, we\nalso estimate FLOPs (i.e., the number of ﬂoating-\npoint multiplication-adds) for the efﬁciency evalu-\nation. Since our model takes both word-level and\ncharacter-level inputs, we estimated the FLOPs\nvalue for a word-level input with 4.39 character-\nlevel inputs, while 4.39 is the averaged length of\nwords in the CoNLL03 dataset.\nBefore the model pruning, LD-Net achieves a\n96.05±0.08 F1 score in the CoNLL00 Chunking\ntask, yielding nearly 30% error reductions over the\nNoLM baseline. Also, it scores 91.86 ±0.15 F1 in\nthe CoNLL03 NER task with over 10% error re-\nductions. Similar to the language modeling, we\nNetwork Avg. #FLOPs F1 score\n(LMs Ind.#) ppl ( ·106) (avg ±std)\nNoLM (/) / 3 90.78 ±0.24\nO-ELMo (3) 39.70 79 ♯ 92.22±0.10\nTagLM (4) 47.50 22 ♯ 91.62±0.23\nR-ELMo (6) 40.27 108 91.99 ±0.24\nR-ELMo (7) 48.85 68 91.54 ±0.10\nLD-Net ∗(8) 45.14 98 91.76 ±0.18\nLD-Net ∗(9) 50.06 98 91.86 ±0.15\nLD-Net (8) origin 51 91.95\npruned 5 91.55 ±0.06\nLD-Net (9) origin 51 92.03\npruned 5 91.84 ±0.14\nTable 3: Performance comparison in the CoNLL03\nNER task. Models marked with † employed LSTMs\nwith projection, which is more efﬁcient than the vanilla\nLSTMs. LD-Net maked with ∗ are trained without\npruning (layer selection).\nobserve that the most complicated models achieve\nthe best perplexity and provide the most improve-\nments in the target task. Still, considering the\nnumber of model parameters and the resulting per-\nplexity, our model demonstrates its effectiveness\nin generating contextualized representations. For\nexample, comparing to our methods, R-ELMo (7)\nleverages LMs with the similar perplexity and pa-\nrameter number, but cannot get the same improve-\nments with our method on both datasets.\nActually, contextualized representations have\nstrong connections with the skip-thought vec-\ntors (Kiros et al., 2015). Skip-thought models try\nto embed sentences and are trained by predict-\ning the previous and afterwards sentences. Sim-\nilarly, LMs encode a speciﬁc context as the hid-\nden states of RNNs, and use them to predict fu-\nture contexts. Speciﬁcally, we recognize the cell\nstates of LSTMs are more like to be the sentence\nembedding (Radford et al., 2017), since they are\nonly passed to the next time stamps. At the same\ntime, because the hidden states would be passed to\nother layers, we think they are more like to be the\ntoken representations capturing necessary signals\nfor predicting the next word or updating context\nrepresentations4. Hence, LD-Net should be more\n4We tried to combine the cell states with the hidden states\nto construct the contextualized representations by concatena-\ntion or weighted average, but failed to get better performance.\n1222\nNetwork (LMs Ind.#) FLOPs Batch size Peak RAM Time (s) Speed\n103words/s 103sents/s\nR-ELMo (6) 108 200 8Gb 32.88 22 0.4\nLD-Net (9, origin) 51 80 8Gb 25.68 26 0.5\nLD-Net (9, pruned) 5 80 4Gb 6.90 98 2.0\n500 8Gb 4.86 ( 5X) 166 ( 6X) 2.9 ( 5X)\nTable 4: Speed comparison in the CoNLL03 NER task. We can observe that LD-Net (9, pruned) achieved about 5\ntimes speed up on the wall-clock time over LD-Net (9, origin).\neffective then ELMo, as concatenating could pre-\nserve all extracted signals while weighted average\nmight cause information loss.\nAlthough the layer-wise dropout makes the\nmodel harder to train, their resulting LMs generate\nbetter contextualized representations, even with-\nout the same perplexity. Also, as discussed in (Pe-\nters et al., 2018, 2017), the performance of the\ncontextualized representation can be further im-\nproved by training larger models or using the CNN\nto represent words.\nFor the pruning, we started from the model with\nthe best performance on the development set (re-\nferred with “origin”), and refer the performances\nof pruned models with “pruned” in Table 2 and 3.\nEssentially, we can observe the pruned models get\nrid of the vast majority of calculation while still re-\ntaining a signiﬁcant improvement. We will discuss\nmore on the pruned models in Sec. 4.4.\n4.3 Speed Up Measurements\nWe use FLOPs for measuring the inference ef-\nﬁciency as it reﬂects the time complexity (Han\net al., 2015), and thus is independent of spe-\nciﬁc implementations. For models with the same\nstructure, improvements in FLOPs would result in\nmonotonically decreasing inference time. How-\never, it may not reﬂect the actual efﬁciency of\nmodels due to the model differences in paral-\nlelism. Accordingly, we also tested wall-clock\nspeeds of our implementations.\nOur implementations are based on the PyTorch\n0.3.15, and all experiments are conducted on the\nCoNLL03 dataset with the Nvidia GTX 1080\nGPU. Speciﬁcally, due to the limited size of\nCoNLL03 test set, we measure such speeds on the\ntraining set. As in Table 4, we can observe that,\nthe pruned model achieved about 5 times speed\nup. Although there is still a large margin between\nWe think it implies that ELMo works as token representations\ninstead of sentence representations\n5http://pytorch.org/\nF 1 F 1\n(a) ConNLL00 Chunking (b) ConNLL03 NER\nFigure 5: The performance of pruned models in two\ntasks w.r.t. their efﬁciency (FLOPs).\nthe actual speed-up and the FLOPs speed-up, we\nthink the resulting decode speed (166K words/s) is\nsufﬁcient for most real-world applications.\n4.4 Case Studies\nEffect of the pruning ratio. To explore the effect\nof the pruning ratio, we adjustλ1 and visualize the\nperformance of pruned models v.s. their FLOPs #\nin Fig 5. We can observe that LD-Net outperforms\nits variants and demonstrates its effectiveness.\nAs the pruning ratio becoming larger, we can\nobserve the performance of LD-Net ﬁrst increases\na little, then starts dropping. Besides, in the\nCoNLL03 NER task, LMs can be pruned to a rel-\natively small size without much loss of efﬁciency.\nAs in Table 3, we can observe that, after prun-\ning over 90% calculations, the error of the re-\nsulting model only increases about 2%, yielding\na competitive performance. As for the CoNLL00\nChunking task, the performance of LD-Net decays\nin a faster rate than that in the NER task. For ex-\nample, after pruning over 80% calculations, the\nerror of the resulting model increases about 13%.\nConsidering the fact that this corpus is only half\nthe size of the CoNLL03 NER dataset, we can ex-\npect the resulting models have more dependencies\non the LMs. Still, the pruned model achieves a\n25% error reduction over the NoLM baseline.\n1223\nTable 1\n0 1 2 3 4 5 6 7 8 9 >10\n4 3 3 1 1 0 0 1 1 2 4\n# of times one layer is selected\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n>10\n# of layers\n0 1 2 3 4\n\u0000 1\nIrrevalent \nImportant \nFigure 6: The performance of pruned models in two\ntasks w.r.t. their efﬁciency (FLOPs).\nLayer selection pattern. We further studied the\nlayer selection patterns. Speciﬁcally, we use the\nsame setting of LD-Net (9) in Table 3, conduct\nmodel pruning using for 50 times, and summa-\nrize the statics in Figure 6. We can observe that\nnetwork layers formulate two clear clusters, one\nis likely to be preserved during the selection, and\nthe other is likely to be pruned. This is consistent\nwith our intuition that some layers are more impor-\ntant than others and the layer selection algorithm\nwould pick up layers meaningfully.\nHowever, there is some randomness in the se-\nlection result. We conjugate that large networks\ntrained with dropout can be viewed as a ensem-\nble of small sub-networks (Hara et al., 2016), also\nthere would be several sub-networks having the\nsimilar function. Accordingly, we think the ran-\ndomness mainly comes from such redundancy.\nEffectiveness of model pruning. Zhu and Gupta\n(2017) observed pruned large models consistently\noutperform small models on various tasks (includ-\ning LM). These observations are consistent with\nour experiments. For example, LD-Net achieves\n91.84 after pruning on the CoNLL03 dataset. It\noutperforms TagLM (4) and R-ELMo (7), whose\nperformances are 91.62 and 91.54. Besides, we\ntrained small LMs of the same size as the pruned\nLMs (1-layer densely connected LSTMs). Its per-\nplexity is 69 and its performance on the CoNLL03\ndataset is 91.55 ±0.19.\n5 Related Work\nSequence labeling. Linguistic sequence labeling\nis one of the fundamental tasks in NLP, encom-\npassing various applications including POS tag-\nging, chunking, and NER. Many attempts have\nbeen made to conduct end-to-end learning and\nbuild reliable models without handcrafted fea-\ntures (Chiu and Nichols, 2016; Lample et al.,\n2016; Ma and Hovy, 2016).\nLanguage modeling. Language modeling is a\ncore task in NLP. Many attempts have been paid to\ndevelop better neural language models (Zilly et al.,\n2017; Inan et al., 2016; Godin et al., 2017; Melis\net al., 2017). Speciﬁcally, with extensive corpora,\nlanguage models can be well trained to generate\nhigh-quality sentences from scratch (J ´ozefowicz\net al., 2016; Grave et al., 2017; Li et al., 2018;\nShazeer et al., 2017). Meanwhile, initial attempts\nhave been made to improve the performance of\nother tasks with these methods. Some methods\ntreat the language modeling as an additional su-\npervision, and conduct co-training for knowledge\ntransfer (Dai and Le, 2015; Liu et al., 2018; Rei,\n2017). Others, including this paper, aim to con-\nstruct additional features (referred as contextual-\nized representations) with the pre-trained language\nmodels (Peters et al., 2017, 2018).\nNeural Network Acceleration. There are mainly\nthree kinds of NN acceleration methods, i.e.,\nprune network into smaller sizes (Han et al.,\n2015; Wen et al., 2016), converting ﬂoat operation\ninto customized low precision arithmetic (Hubara\net al., 2018; Courbariaux et al., 2016), and using\nshallower networks to mimic the output of deeper\nones (Hinton et al., 2015; Romero et al., 2014).\nHowever, most of them require costly retraining.\n6 Conclusion\nHere, we proposed LD-Net, a novel framework\nfor efﬁcient contextualized representation. As\ndemonstrated on two benchmarks, it can conduct\nthe layer-wise pruning for a speciﬁc task. More-\nover, it requires neither the gradient oracle of LMs\nnor the costly retraining. In the future, we plan to\napply LD-Net to other applications.\nAcknowledgments\nWe thank Junliang Guo and all reviewers for their\nconstructive comments. Research was sponsored\nby the Army Research Laboratory and was ac-\ncomplished under Cooperative Agreement Num-\nber W911NF-09-2-0053 (the ARL Network Sci-\nence CTA). The views and conclusions in this doc-\nument are those of the authors and should not be\ninterpreted as representing the ofﬁcial policies, ei-\nther expressed or implied, of the Army Research\nLaboratory or the U.S. Government. The U.S.\nGovernment is authorized to reproduce and dis-\ntribute reprints for Government purposes notwith-\nstanding any copyright notation here on.\n1224\nReferences\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. Tech-\nnical report, Google.\nJason P. C. Chiu and Eric Nichols. 2016. Named entity\nrecognition with bidirectional lstm-cnns. TACL.\nMatthieu Courbariaux, Itay Hubara, Daniel Soudry,\nRan El-Yaniv, and Yoshua Bengio. 2016. Bina-\nrized neural networks: Training deep neural net-\nworks with weights and activations constrained to+\n1 or-1. arXiv preprint arXiv:1602.02830.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079–3087.\nXavier Glorot and Yoshua Bengio. 2010. Understand-\ning the difﬁculty of training deep feedforward neu-\nral networks. In Proceedings of the Thirteenth In-\nternational Conference on Artiﬁcial Intelligence and\nStatistics.\nFr´ederic Godin, Joni Dambre, and Wesley De Neve.\n2017. Improving language modeling using densely\nconnected recurrent neural networks. arXiv preprint\narXiv:1707.06130.\nEdouard Grave, Armand Joulin, Moustapha Ciss ´e,\nDavid Grangier, and Herv ´e J ´egou. 2017. Efﬁ-\ncient softmax approximation for gpus. In Inter-\nnational Conference on Machine Learning , pages\n1302–1310.\nSong Han, Jeff Pool, John Tran, and William Dally.\n2015. Learning both weights and connections for\nefﬁcient neural network. In Advances in neural in-\nformation processing systems, pages 1135–1143.\nKazuyuki Hara, Daisuke Saitoh, and Hayaru Shouno.\n2016. Analysis of dropout learning regarded as en-\nsemble learning. In International Conference on Ar-\ntiﬁcial Neural Networks, pages 72–79. Springer.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Identity mappings in deep residual net-\nworks. In European Conference on Computer Vi-\nsion, pages 630–645. Springer.\nGeoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.\n2015. Distilling the knowledge in a neural network.\nCoRR, abs/1503.02531.\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and\nKilian Q Weinberger. 2017. Densely connected con-\nvolutional networks. In CVPR.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry,\nRan El-Yaniv, and Yoshua Bengio. 2018. Quantized\nneural networks: Training neural networks with low\nprecision weights and activations. Journal of Ma-\nchine Learning Research, 18(187):1–30.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2016. Tying word vectors and word classiﬁers:\nA loss framework for language modeling. CoRR,\nabs/1611.01462.\nRafal J´ozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the lim-\nits of language modeling. CoRR, abs/1602.02410.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA method for stochastic optimization. CoRR,\nabs/1412.6980.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in neural information processing systems,\npages 3294–3302.\nOnur Kuru, Ozan Arkan Can, and Deniz Yuret. 2016.\nCharner: Character-level named entity recognition.\nIn Proceedings of COLING 2016, the 26th Inter-\nnational Conference on Computational Linguistics:\nTechnical Papers, pages 911–921.\nGuillaume Lample, Miguel Ballesteros, Kazuya\nKawakami, Sandeep Subramanian, and Chris Dyer.\n2016. Neural architectures for named entity recog-\nnition. In NAACL-HLT.\nZhongliang Li, Raymond Kulhanek, Shaojun Wang,\nYunxin Zhao, and Shuang Wu. 2018. Slim embed-\nding layers for recurrent neural language models.\nCoRR, abs/1711.09873.\nLiyuan Liu, Jingbo Shang, Frank F. Xu, Xiang Ren,\nHuan Gui, Jian Peng, and Jiawei Han. 2018. Em-\npower sequence labeling with task-aware neural lan-\nguage model. CoRR, abs/1709.04109.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end se-\nquence labeling via bi-directional lstm-cnns-crf. In\nACL.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2017. On\nthe state of the art of evaluation in neural language\nmodels. CoRR, abs/1707.05589.\nNaveen Mellempudi, Abhisek Kundu, Dheevatsa\nMudigere, Dipankar Das, Bharat Kaul, and Pradeep\nDubey. 2017. Ternary neural networks with ﬁne-\ngrained quantization. CoRR, abs/1705.01462.\nWalter Murray and Kien-Ming Ng. 2010. An algo-\nrithm for nonlinear optimization problems with bi-\nnary variables. Computational Optimization and\nApplications, 47(2):257–288.\nMatthew E. Peters, Waleed Ammar, Chandra Bhaga-\nvatula, and Russell Power. 2017. Semi-supervised\nsequence tagging with bidirectional language mod-\nels. In ACL.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer,\nMatt Gardner, Christopher Clark, Kenton Lee, and\nLuke S. Zettlemoyer. 2018. Deep contextualized\nword representations. In NAACL-HLT.\n1225\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever.\n2017. Learning to generate reviews and discovering\nsentiment. arXiv preprint arXiv:1704.01444.\nLev Ratinov and Dan Roth. 2009. Design challenges\nand misconceptions in named entity recognition. In\nCoNLL.\nMarek Rei. 2017. Semi-supervised multitask learning\nfor sequence labeling. In ACL.\nAdriana Romero, Nicolas Ballas, Samira Ebrahimi Ka-\nhou, Antoine Chassang, Carlo Gatta, and Yoshua\nBengio. 2014. Fitnets: Hints for thin deep nets.\nCoRR, abs/1412.6550.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc V . Le, Geoffrey E. Hinton, and\nJeff Dean. 2017. Outrageously large neural net-\nworks: The sparsely-gated mixture-of-experts layer.\nCoRR, abs/1701.06538.\nErik F Tjong Kim Sang and Sabine Buchholz. 2000.\nIntroduction to the conll-2000 shared task: Chunk-\ning. In Learning language in logic and CoNLL.\nErik F Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nNatural language learning at NAACL-HLT.\nWei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen,\nand Hai Li. 2016. Learning structured sparsity in\ndeep neural networks. In Advances in Neural Infor-\nmation Processing Systems, pages 2074–2082.\nMichael Zhu and Suyog Gupta. 2017. To prune, or not\nto prune: exploring the efﬁcacy of pruning for model\ncompression. CoRR, abs/1710.01878.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan\nKoutnk, and Jrgen Schmidhuber. 2017. Recurrent\nHighway Networks. In ICML.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8456202745437622
    },
    {
      "name": "Language model",
      "score": 0.615436851978302
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5640827417373657
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.5525069236755371
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.5414867401123047
    },
    {
      "name": "Inference",
      "score": 0.528683602809906
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5240129232406616
    },
    {
      "name": "Task (project management)",
      "score": 0.5031968951225281
    },
    {
      "name": "Computation",
      "score": 0.48815399408340454
    },
    {
      "name": "Pruning",
      "score": 0.4543168246746063
    },
    {
      "name": "Machine learning",
      "score": 0.44108280539512634
    },
    {
      "name": "Algorithm",
      "score": 0.1256985068321228
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800817003",
      "name": "Southern California University for Professional Studies",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I16820183",
      "name": "Illinois College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210135837",
      "name": "National Center for Supercomputing Applications",
      "country": "US"
    }
  ]
}