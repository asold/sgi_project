{
  "title": "Multiclass malaria parasite recognition based on transformer models and a generative adversarial network",
  "url": "https://openalex.org/W4387478200",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5111053937",
      "name": "Dianhuan Tan",
      "affiliations": [
        "Central South University",
        "Xiangya Hospital Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A2143858664",
      "name": "Xianghui Liang",
      "affiliations": [
        "Central South University",
        "Xiangya Hospital Central South University"
      ]
    },
    {
      "id": "https://openalex.org/A5111053937",
      "name": "Dianhuan Tan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2143858664",
      "name": "Xianghui Liang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4210985634",
    "https://openalex.org/W4286377062",
    "https://openalex.org/W4378901118",
    "https://openalex.org/W4381805480",
    "https://openalex.org/W2776592066",
    "https://openalex.org/W2103226587",
    "https://openalex.org/W3021610036",
    "https://openalex.org/W3102469298",
    "https://openalex.org/W3023012156",
    "https://openalex.org/W2485954569",
    "https://openalex.org/W2597241309",
    "https://openalex.org/W2006161553",
    "https://openalex.org/W3155270026",
    "https://openalex.org/W4382057758",
    "https://openalex.org/W4281611264",
    "https://openalex.org/W3183577140",
    "https://openalex.org/W2979390834",
    "https://openalex.org/W4381196760",
    "https://openalex.org/W3027581176",
    "https://openalex.org/W3005287255",
    "https://openalex.org/W4206070627",
    "https://openalex.org/W3213071984",
    "https://openalex.org/W6735913928",
    "https://openalex.org/W4300978573",
    "https://openalex.org/W6779669310",
    "https://openalex.org/W3096831136",
    "https://openalex.org/W6637568146",
    "https://openalex.org/W6761061368",
    "https://openalex.org/W4212978170",
    "https://openalex.org/W1921523184",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4312453657",
    "https://openalex.org/W6809815429",
    "https://openalex.org/W2982083293",
    "https://openalex.org/W6762585180",
    "https://openalex.org/W4282557815",
    "https://openalex.org/W2184815069"
  ],
  "abstract": "Abstract Malaria is an extremely infectious disease and a main cause of death worldwide. Microscopic examination of thin slide serves as a common method for the diagnosis of malaria. Meanwhile, the transformer models have gained increasing popularity in many regions, such as computer vision and natural language processing. Transformers also offer lots of advantages in classification task, such as Fine-grained Feature Extraction, Attention Mechanism etc. In this article, we propose to assist the medical professionals by developing an effective framework based on transformer models and a generative adversarial network for multi-class plasmodium classification and malaria diagnosis. The Generative Adversarial Network is employed to generate extended training samples from multiclass cell images, with the aim of enhancing the robustness of the resulting model. We aim to optimize plasmodium classification to achieve an exact balance of high accuracy and low resource consumption. A comprehensive comparison of the transformer models to the state-of-the-art methods proves their efficiency in the classification of malaria parasite through thin blood smear microscopic images. Based on our findings, the Swin Transformer model and MobileVit outperform the baseline architectures in terms of precision, recall, F1-score, specificity, and FPR on test set (the data was divided into train: validation: test splits). It is evident that the Swin Transformer achieves superior detection performance (up to 99.8% accuracy), while MobileViT demonstrates lower memory usage and shorter inference times. High accuracy empowers healthcare professionals to conduct precise diagnoses, while low memory usage and short inference times enable the deployment of predictive models on edge devices with limited computational and memory resources.",
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports\nMulticlass malaria parasite \nrecognition based on transformer \nmodels and a generative \nadversarial network\nDianhuan Tan  & Xianghui Liang *\nMalaria is an extremely infectious disease and a main cause of death worldwide. Microscopic \nexamination of thin slide serves as a common method for the diagnosis of malaria. Meanwhile, the \ntransformer models have gained increasing popularity in many regions, such as computer vision and \nnatural language processing. Transformers also offer lots of advantages in classification task, such \nas Fine-grained Feature Extraction, Attention Mechanism etc. In this article, we propose to assist \nthe medical professionals by developing an effective framework based on transformer models and a \ngenerative adversarial network for multi-class plasmodium classification and malaria diagnosis. The \nGenerative Adversarial Network is employed to generate extended training samples from multiclass \ncell images, with the aim of enhancing the robustness of the resulting model. We aim to optimize \nplasmodium classification to achieve an exact balance of high accuracy and low resource consumption. \nA comprehensive comparison of the transformer models to the state-of-the-art methods proves their \nefficiency in the classification of malaria parasite through thin blood smear microscopic images. Based \non our findings, the Swin Transformer model and MobileVit outperform the baseline architectures in \nterms of precision, recall, F1-score, specificity, and FPR on test set (the data was divided into train: \nvalidation: test splits). It is evident that the Swin Transformer achieves superior detection performance \n(up to 99.8% accuracy), while MobileViT demonstrates lower memory usage and shorter inference \ntimes. High accuracy empowers healthcare professionals to conduct precise diagnoses, while low \nmemory usage and short inference times enable the deployment of predictive models on edge devices \nwith limited computational and memory resources.\nMalaria, a mosquito-borne disease transmitted by various mosquito species, poses a significant global threat \nto both humans and other creatures. Malaria is caused by plasmodium, which is one of most pronounced \ndeadly protozoan parasites. It harms the red blood cells, and eventually causes serious  disease1. According to \nthe world malaria report in  20222, more than 619,000 deaths were reported in 2021, most of them are chil-\ndren. Malaria prevention and control efforts are burdened with numerous challenges, apart from the existing \nsignificant COVID-19 interruption and other healthcare system hurdles. These additional challenges include \nprolonged humanitarian crises, insufficient funding from donors, and the potential impact of climate change on \nthe spread of the disease.  Reference3 revealed that Malaria infection remains one of the main killers of children \naged 6–59 months.  Reference4 showed that the number of confirmed malaria cases in children under 5 years \nincreases with increase in rainfall and the number of tested cases reduces with increase in temperature. India \ncontributes to 1.7% of the global malaria cases, with a significant proportion of fatalities occurring in children \nunder the age of five within the country. The mosquito responsible for malaria transmission is prevalent in regions \nspanning America, Africa, Asia, and Latin America, primarily affecting areas lacking the necessary resources \nfor prevention. Africa, in particular, bears a staggering 90% of the global malaria-related mortality burden, \nwith 68% of Ethiopia’s population and 97% of the Democratic Republic of the Congo’s population residing in \nhigh-risk malaria infection  zones5,6. Severe malaria is primarily attributed to P . falciparum, commonly known \nas falciparum malaria, with symptoms typically manifesting 9–30 days after infection. Research conducted by \nRef.7 reveals that the majority of malaria-related deaths are caused by P . falciparum (p.f), whereas P . vivax(p.v), \nP . ovale(p.o), and P . malariae(p.m) generally result in milder forms of the disease.\nOPEN\nDepartment of Clinical Laboratory, Xiangya Hospital, Central South University, No.87 Xiangya Road, Kaifu District, \nChangsha 410008, Hunan, China. *email: liangxh901@csu.edu.cn\n2\nVol:.(1234567890)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\nThe reliable diagnosis of malaria faces a significant challenge due to the presence of P . falciparum parasites \nwith Pfhrp2/3(specific gene found in the Plasmodium falciparum parasite) gene deletions, which undermines the \neffectiveness of malaria diagnostic tests. To address this issue, a diverse range of diagnostic approaches is required. \nWhile methods that employ polymerase chain reaction (PCR) for malaria parasite DNA detection or utilize PCA \nModel for RNA-Seq Malaria Vector Data  Classification8 have been developed, their widespread application in \nmalaria-endemic areas has been hindered by their high cost and complexity. Consequently, the standard method \nfor malaria diagnosis remains the microscopic analysis of stained blood  slides9. While slide examination is an \neconomical and straightforward technique, it presents significant time and difficulty challenges. This is due to the \nfact that only a small portion of the slides is visible through the microscope, and small-sized parasites are often \nsparsely distributed. As a result, a comprehensive examination typically consumes approximately 10–15 min. It \nis susceptible to human errors, and the scarcity of experts further exacerbates the challenge. Therefore, there is a \npressing need for automated diagnosis methods aimed at detecting malaria parasite-infected blood cells through \nthe analysis of microscopic blood cell images. For this reason, key research and development challenges involve \nthe limited sensitivity of tools for detecting non-P . falciparum species and the necessity for a broader range of \ndiagnostic  applications2. Given their increasing precision and resilience in microscopic analysis, deep learning \nmodels offer a promising solution. In this paper, we propose the design of an efficient deep learning framework \nfor the noninvasive classification of multiple Plasmodium species and malaria diagnosis.\nDeep learning has a large number of applications in the image processing field. Due to their high speed, accu-\nracy, flexibility and low  cost1, deep learning models has gained huge popularity and has been widely applied in \nCT-scans10,  MRI11 and various microscopicimage  analyses12–14, such as microscopic examination of protozoan \nincluding plasmodium classification for thin smears. Furthermore, it’s worth noting that Transformer models \nhave also been employed in various classification tasks  recently15.  Reference16 evaluated their proposed classifier \non 27,560 samples, achieving an accuracy of 98.37% on average using tenfold cross-validation.\nCurrent works in malaria classification primarily focus on determining whether blood cells are infected or \nnot. To achieve this objective,  Diker17 introduced an optimized Residual Convolutional Neural Network that \nutilizes the Bayesian method to classify malaria cell images as infected or non-infected. In the classification of \nmalaria cell images, the addition of Neighborhood Components Analysis (NCA) has been observed to enhance \nthe performance of classifiers. Ufuktepe et al. 18 presented a channel-wise feature pyramid network specifically \ndesigned for medical applications, which utilizes the green channel of input images to detect parasite cells and \nclassify them as infected or non-infected. They expanded their approach by introducing an additional class for P . \nfalciparum and training the network to categorize different types of infected parasites. Molina et al.19 developed \na convolutional neural network approach capable of distinguishing between all stages of malaria-infected red \nblood cells. They utilized transfer learning with the VGG-16 architecture for their experiments. Y ang F et al. 20 \nproposed the first system that combines deep learning models and image processing techniques for detecting \nmalaria parasites in thick smears, and they successfully implemented these models on smartphones.  Reference21 \ndesigned a segmentation technique to detect parasite cells of malaria in thin blood smear images using edge-\nbased segmentation. The proposed method achieved high accuracy, SE, SP , PVP , and PVN values. Results in this \nstudy indicated that the edge-based segmentation technique is a promising technique to facilitate the malaria \nclassification.  Reference22 introduced a novel approach known as Deep Cycle Transfer Learning (DCTL) for \nparasite detection. DCTL offers an effective solution to circumvent the laborious process of data annotation by \ntransferring macro-level shape knowledge from human experts, thereby substituting the need for micro-level \nshape knowledge of parasites.  Reference23 introduced an innovative deep learning approach that is inherently \ngeometry-aware. This method leveraged a geometric-feature spectrum to effectively detect the presence of host \nnucleus, Toxoplasma, Trypanosome, and Babesia using an architecture based on ExtremeNet. Notably, this \napproach achieved a commendable feat by ensuring accurate detection without any instances of misdiagnosis. \nIt stands apart from conventional unsupervised learning methods due to its unique design and capabilities. In \nRef.24, an automated and quantitative analysis of Babesia-infected erythrocytes was conducted utilizing DenseNet. \nThis approach effectively circumvented the quantitative analysis errors that often arise from manual microscopy. \nMoreover, the utilization of Integrated Gradient as an interpretability tool for the model shed light on the primary \ncontributors to false positives, which were identified as cell boundaries, precipitate, and rouleaux formations.\nAfter the Plasmodium parasite infects an animal through a mosquito bite, it undergoes two distinct life stages \nwithin its host. The first stage is an asymptomatic liver stage, followed by a blood stage where clinical symptoms of \nmalaria manifest. Additionally, cells infected with the malaria parasite progress through various life stages during \nthe development of the disease, including gametocyte, ring, and schizont stages. Recognizing the significance of \nthese lifecycle stages, other researchers have focused their efforts on understanding and classifying Plasmodium \nat different stages. Salam et al.25 devised a two-stage algorithm that utilizes the filter method for feature ranking \nand incremental feature selection technique for analysis. They trained various machine learning models, such \nas SVM, k-NN, and ANN, on a dataset to evaluate their performance. To achieve improved classification perfor-\nmance, they developed a hybrid classifier that combines three different classifiers, surpassing the performance \nof individual classifiers. Arshad et al. 26 proposed a deep learning-based two-stage approach for the multi-class \nclassification of P . vivax lifecycle stages. They employed the Unet architecture to segment cells from images \ncaptured by a microscopic camera and selected the ResNet network for single-stage multi-class classification.\nAfter analysis of the literature concerning the diagnosis of malaria disease, it was evident that there exists a \npressing need to address the comprehensive classification of multiclass malaria species, which is unexplored. It \nis noteworthy that the majority of the existing research has been primarily centered around binary classification \ntasks. In addition to the requirement for infected cells classification, inadequate computing resources in least \ndeveloped countries pose a special challenge. To solve this issue, we present the performance of several models \nincluding VGG-19, Swin Transformer and MobileViT. The aim of this research paper is to systematically select \nthe best model for the purpose of malaria case finding in regions where malaria has remained endemic. To \n3\nVol.:(0123456789)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\nhandle real-world situation, a multi-labelled dataset is targeted. Data was collected from Hunan province in the \nSouth of China.\nIn this paper, we chosen two convolutional networks and three transformer-based models including Swin \nTransformer, Vision Transformer and MobileViT. As a general-purpose and effective model for computer vision, \nSwin Transformer is a model commonly used for medical image classification. Also, we present the performance \nof MobileViT taking resource and energy consumption into consideration. Besides, we chosen WGAN-GP to \naugment our dataset.\nThe primary objective of this paper is to present a robust solution for malaria diagnosis through microscopic \nexamination, leveraging deep learning models. In the following sections, we will provide details regarding dataset \ncollection and our approach to training the neural networks.\nThe rest of the paper is structured as follows. Section “ Dataset” elucidates the process of dataset acquisition \ninvolved in the proposed work. Section “Methods” demonstrates the methodology employed in the research to \nattain its objectives. Section “Results” describes the results of experiments done on the dataset using the proposed \nmethodology. Section “Discussions” is a comprehensive discussion of the proposed work and potential avenues \nfor future research. Section “Conclusion” outlines the limitations of the research.\nDataset\nIn this paper, our dataset, which consists of in total 390 blood smear images as shown in Figs.  1 and 2, is taken \nfrom approximately one hundred patients. Those images were collected from thin blood  smears26 from malaria-\ninfected patients in Changsha city of Hunan province. Blood smears were meticulously prepared by spreading \na thin layer of blood onto glass slides and leaving them to air-dry. Subsequently, to ensure fixation, methanol \nwas applied. Giemsa solution was then applied to stain the blood smear, with an incubation period of 10 to \n15 min. Finally, to secure the integrity of the specimens, cover-slips were permanently affixed to the slides. \nIt is noteworthy that these thin-film blood slides, constituting the dataset, were crafted by skilled profession-\nals within laboratory settings. The microscopic images were acquired using an high-resolution microscope at \nFigure 1.  Microscopic p.f ring image directly from smears.\nFigure 2.  Microscopic p.f gametocyte image directly from smears.\n4\nVol:.(1234567890)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\n100 × objective magnification, with annotations and captures overseen by an expert haematologist. The dataset \nstatistics are shown in Table 1.\nThe cells were carefully categorized into different classes, encompassing platelets, red blood cells, white blood \ncells, as well as four species of the malaria plasmodium, namely P . falciparum(p.f), whereas P . vivax(p.v), P . \novale(p.o), and P . malariae(p.m). Within our collection of microscopic images, each image included more than \none hundred cells in total, with a predominant presence of healthy red blood cells. In our proposed methodology, \nwe deliberately select only 300 of them for the balance across all classification classes.\nMethods\nFor the multi-classification of malaria parasite lifecycle, the Swin Transformer network has been designed using \na dataset of microscopic thin blood smear images. The system configuration used for experiments is Intel core \ni3-10,100 processor with Nvidia 1650 GPU on Windows 11 operating system with 64GB memory. The models \nare trained in Python 3.6.13 enviroment using pytorch 1.13.1. The microsocpic image dataset used as input data \nfor model training and valiadation is augmented with various techniques to improve the model performance as \nwell as help train robust and accurate machine-learning models. The source code for WGAN-GP can be accessed \nthrough the following link: https:// github. com/ igul2 22/ impro ved_ wgan_ train ing27. Additionally, the code for \nMobileViT is available at: https:// github. com/ micro nDLA/ Mobil eViTv328.\nThe framework for multiclass lifecycle malaria analysis is presented in Fig.  3. It contains four steps where \nthree pipelines exist.\nFollowing the initial data acquisition from blood smears in Section “Dataset” , this section proceeds to deline-\nate subsequent steps. Section “ Data labelling and augmentation” introduces the techniques employed for data \nlabeling and augmentation, aimed at expanding the dataset’s size and achieving a more balanced representation \nto mimic real-world scenarios. In Section “GAN architecture” , we delve into the details of the GAN (Generative \nAdversarial Network) architecture adopted in this research. Section “ Swin Transformer” provides an overview \nof the Swin-Transformer architecture, while Sects. “ Window-based self-attention” and “Shifted window self-\nattention” elucidate its advantages and how they contribute to enhanced accuracy. Lastly, Sect. “ Light-weight \nvision transformer: MobileViT ” introduces MobileViT, another transformer model distinguished by its light-\nweight and high efficiency.\nData labelling and augmentation\nThe dataset prepared for model training were single cells images labeled from microscopic smear images captured \nby an microsope at 1000 × magnification. Figure 4 show a example we labeled cell images.\nAfter obtaining a sufficient number of images for classifier training, a notable issue emerged: our dataset \nexhibited an overrepresentation of healthy red blood cells compared to other classes, resulting in a pronounced \nimbalance and bias. This disparity, where the healthy class was disproportionately represented, prompted us to \nimplement diverse augmentation techniques as a corrective measure. In addressing this challenge, we introduced \npractical approaches, including random flipping, shearing, and rotation at various angles, as illustrated in Fig. 5. \nThese transformations effectively contributed to the creation of a more balanced dataset, mitigating the issue of \nclass imbalance and bias.\nThe major approaches for Data Augmentation include: (1) geometric transformation, which reduce the dif-\nference in position, scale and perspective. (2) color adjustment that eliminate the difference caused by sunlight, \nluminance and color. (3) kernel filters, which improve the ability of models to process blur images. (4) Random \nerasing, a techniques to ensure the architecture to properly handle incomplete image and pay more attention to \nbig picture, in stead of part of it. (5) Generative Adversarial Network(GAN)29 where a generator works to find a \nartifical distribution similar to real distribution.\nThe GANs have many applications in medical image processing. Zhang et al. 12 used a pixel2pixel(a type of \ndeep learning model architecture used for image-to-image translation tasks) GAN with V-Net(a network based \non convolutional neural networks, specifically designed to handle 3D data) as the generator to correct motion \nartifacts in the right coronary artery, confirming that GANs have the potential to provide a certain approach of \nremoving motion artifacts in image processing. Besides, GANs performs well in image generation by studying \na collection of training examples and learn the probability distribution that generated them. They offer more \npossibilities than ordinary CNNs(convolutional neural networks)29. The Wasserstein GAN(WGAN) introduces \nTable 1.  Cells distribution. Single cell images annotated from original blood smears microscopic images (for \nexample, Figs. 1 and 2).\nClasses Number of cells\np.f (P . falciparum) 187\np.m (P . vivax) 113\np.o (P . ovale) 93\np.v (P . malariae) 168\nPLT (platelet) 457\nWBC (white blood cell) 300\nRBC (red blood cell) 300\n5\nVol.:(0123456789)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\nthe Wasserstein distance and provide a reliable training process indicator that improves the quality of generated \n images30. The WGAN with a gradient penalty (WGAN-GP) is an improved version of the WGAN that allows \nfor easier optimisation and more stable convergence compared to the original WGAN.\nThe training strategy of GANs is to define a game between two competing network. The objective of generator \nG is to generate counterfeit cell image samples, while discriminator D is tasked with discerning whether a given \ncell image sample is authentic, sourced from the training set, or counterfeit. These two models operate concur-\nrently through an adversarial process. Formally, the game between the G and the D is the minimax objective:\nOr\n(1)min max\nGD V (G, D ) = Ex∼Pr\n[\nlogD(x)\n]\n+ Ex∼Pg\n[\nlog\n(\n1 − D\n(\n˜x\n))]\n.\nFigure 3.  (a) presents the flowchart for steps by which we conducted our experiment. (b) illustrates the malaria \nanalysis pipelines employed in (a), the augmentation pipeline was employed in step1, training pipeline in step2 \nand step3, test pipeline in step3 and step4.\n6\nVol:.(1234567890)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\nwhere Pr is the real data distribution(real cells images in this research) and Pg is the fake data distribution (fake \ncells images generated by the generator G in this reasearch).\nThe fake data distribution Pg is implictly defined by ˜x=G(z) where the input z to the generator G is sampled \nfrom some simple noise distribution (such as a Isotropic Gaussian distribution).\nThe Eq. (2) is a modification of (1), designed to enhance computational performance. The reason may intui-\ntively be that (2) have greater gradients early when D(x) near zero, making the model adjust its parameters \nsteeply, although log(1− D( ˜x)) and logD(˜x) result in the same fixed point, where Pg = Pr and D(x ) = 1/2 . \n(the G generate samples as real and the D is unable to discriminate whether a sample from the training set or a \ngenerated set)31.\nTo solve the problem of unstable training and mode collapse which is common in  GANs30, propose WGAN \nand suggest that the Jensen-Shannon divergence which GANs frequently minimize are potentially not continuous \nwith respect to the generator’s parameters, leading to difficulty in convergence. The WGAN uses the Earth-mover \ndistance, which means the minimum distance to transform the distribution p to distribution q. It works to mini-\nmize the cost required to transform one probability distribution p into another q. The basic equation is as follows:\nW [p, q] is the Earth Mover’s Distance between two probability distributions.\ninf\nγ∈�[p,q]\n indicates that we are searching for the minimum value among all possible ways to transport the mass \nfrom p to q. The goal is to find the optimal transport plan that minimizes the total cost or work.\nd(x, y) represents the amount of mass to be transported from x in distribution p to y in distribution q accord-\ning to the optimal transport plan.\nγ( x, y) represents the distance associated with moving one unit of mass from x in distribution p to y in \ndistribution q.\nA simple discrete case of this equation could be illustrated as shown in Fig. 632.\nHowever, in continuous probability space, x as the starting point and y as the destination, the total amount \nof dirt moved is d(x, y) and the travelling distance is γ( x, y) and thus the cost is γ( x, y)d(x, y).\nOne of the most popular Generative Adversarial Networks is improved Wasserstein generative adversarial \nnetwork with a gradient penalty (WGAN-GP) 27,33. We used WGAN-GP to synthesize cells images. Compared \nto GAN, the WGAN was proposed in 2017 to solve the issue of gradient disappearance, gradient instability \nand collapse mode. The WGAN-GP improved on the basis of GAN, delivering better performance in gradient \nstability and image generation. The most common problems for a Generative Adeversial Network are gradient \n(2)min max\nGD V (G, D ) = Ex∼Pr\n[\nlogD(x)\n]\n− Ex∼Pg\n[\nlogD\n(\n˜x\n)]\n,\n(3)W [p, q]= inf\nγ ∈�[p,q]\n∫∫\nγ( x, y)d(x, y)dxdy\nFigure 4.  Single cell from a original image (a), four rings (b) are selected and labeled.\nFigure 5.  Common techniques for data augmentation: add noise (a) change light (b), cut out (c), rotation (d), \ncropping (e), shifting (f), flipping (g).\n7\nVol.:(0123456789)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\ndisapperance and model collapse. Thus, WGAN-GP used a loss founction directly constraining the gradient \nnorm of the critic’s output with respect to its input. To circumvent tractability issues, it enforce a soft version of \nthe constraint with a penalty on the gradient norm for random samples. The least-squares loss in the training \ncontent discriminator and training generator was defined as follows:\nThe total loss function in WGAN-GP combines the generator loss, discriminator loss, and the gradient pen-\nalty. The first two terms were explained in (2).\nThe gradient penalty is a crucial addition in WGAN-GP . Mathematically the third term in (4).\nWhere: λ is a hyperparameter that controls the importance of the gradient penalty. ∇  denotes the gradient \noperator. ˆx is the input of the discriminator. D(ˆx) is the discriminator’s output when applied to a random linear \ncombination of real data and generated data.\nThe gradient penalty is added to enforce a Lipschitz constraint on the discriminator, which helps stabilize \nthe training process. The gradient penalty term encourages the gradients of the discriminator to be close to 1 in \norder to prevent the vanishing gradient problem. It’s computed as the norm of the gradients of the discrimina-\ntor’s output with respect to random samples taken along straight lines between real and generated data points.\nGAN architecture\nIn this paper, we employed a Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP), \nconsisting of a generator and a discriminator. The training process is visually depicted in Fig. 7, where (a) deline-\nates the training procedure of the WGAN-GP and the generation of cell images, while (b) provides insight into \nthe generator’s performance throughout the training process.\nThe discriminator was a CNN with 6 non-linear steps, in which first 4 layers were convolutional layers, and \nthe activation function is leaky rectified linear unit (ReLU). A discrimitor was set to enhance the generaliz-\ntion of the generator. The discrimitor consisted of 5 non-linear blocks which each containing a convolution \nlayer, leaky ReLU and average pool. The princinple of leaky ReLU is to upsample by applying a 2D transposed \nconvolution operator over an input image composed of several input planes. This module can be seen as the \ngradient of Conv2d with respect to its input. The discriminator takes microscopic blood cell images of size 128 \n* 128 * 3 as input. It is designed to analyse the input images and then decide which images are real images and \nwhich images are fake images. The discriminator network uses convolution layers convolutional layers with a \nconvolution kernel of 3, stride of 2, padding of 1, and the activation function of leaky ReLU is processed after \nevery convolutional layer except for full connected layer which work to deliver an output. Softmax activation \nfunction is performed to compute the likelihood probability. This function is a function that turns a vector of K \nreal values into a vector of K real values that sum to 1. The input value could be zero, positive or negative, but the \nsoftmax function transform them into values between 0 and 1. Mathematically, the sigmoid activation function \nis given by the following equation:\n(4)L = E\n˜x∼Pg\n[D (˜x)] − E\nx∼Pr\n[D (x)] + /afii9838E\nˆx∼Px\n[(∇ˆxD\n(\nˆx\n)\n2 − 1\n)2 ]\nFigure 6.  To change P to look like Q: (1) move 2 shovelfuls from P1 to P2 so that (P1 ,Q1) match up. (2) move \n2 shovelfuls from P2 to P3 so that (P2, Q2) match up. (3) move 1 shovelfuls from P3 to P4 so that (P3 ,Q3) and (P4 \n,Q4) match up. Labeling the cost to pay to make (Pi,Qi) match as δ i, we have δ i +1 = δ i + Pi − Qi. In this example δ \n0 = 0, δ 1 = δ 0 + 3 − 1 = 2, δ 2 = 2 + 2 − 2 = 2, δ 3 = 2 + 1 − 4 = -1, δ 4 = − 1 + 4—3 = 0, W = ∑|δi|= 5.\n8\nVol:.(1234567890)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\nz is the input to the sigmoid function. It can be any real number, positive, negative, or zero. 1\n1+e−z  calculates \nthe final output of the sigmoid function. It takes the reciprocal of the denominator, resulting in a value between \n0 and 1, which represents the output of the sigmoid activation function.\nThe generator was a U-Net including 5 blocks. The encoder and decoder structure allow the generator to \nextract comprehensive image features. The input of generator is a set of arbitrary numbers from the normal \ndistribution, and its output is a fake image of shape 128 * 128 in RGB channel. The generator network has convo-\nlutional transpose layers. Every convolutional transpose layer is followed by and the activation function of leaky \nrectified linear unit (ReLU)34. The convolutional transpose layer converts a latent vector with 100 dimensions in \nlatent space into a dense with size 128 * 128 * 3.\nAfter feeding more than one thousand cell images belong to different classes to models, the pre-processing \nsteps were performed to make the dataset more understandable for computer to train these networks. Data \npre-processing in Machine Learning is a set of crucial steps that help enhance the quality of data to promote the \nextraction of meaningful insights from the data. The pre-processing steps include (1) Encoding the categorical \ndata. In this paper, one-hot encoding was applied on each label to convert the categorical format into machine-\nunderstandable vectors. (2) Splitting the dataset. To split a dataset into a training set and a test set. (3) Feature \nscaling. It is a technique to standardize the independent features in the data in a fixed range.\nSwin transformer\nRecently,  transformers35 have greater domination of deep learning architecture than ever before in natural lan-\nguage processing (NLP) tasks. The triumph has motivate more research effort to adapt transformers for vision \ntasks. The Swin-transformer is one of the most exciting pieces of research following up from orginal ViT. Swin-\ntransformer36 is a stand-of-art network which has transformer-based deep learning architecture with excellent \nperformance in visions tasks. Compared to the Vision Transformer(ViT) 37 that precedes it, Swin-transformer \nis highly efficent and more accurate. Thanks to these properties, Swin-transformer serves as the backbone in a \nlot of vision-based models today, it is a hierarchical Transformer model whose representation is compute with \nshift windows, which brings greater efficiency and allows for cross-window connection. Also, the hierarchical \narchitecture brings flexibility to model at different scales and has linear computational complexity with respect to \nimage size. The proposed method leverages the capabilities of the Swin-Transformer in processing microscopic \nimages, effectively extracting image information for the detection of cell classes.\nFigure 8 show the architecture of Swin-transformer(Swin-T). In Swin Transformer, the two key components \nare the ‘patch merging’ and the ‘Swin-T Block’ .\nAs for the ‘patch merging’ , the Swin Transformer builds herarchical feature maps by merging image patch \nas shown in Fig.  9, where ‘hierarchical’ refers to that the feature maps are merged from layer to layer, which is \nan effective downsampling operation reducing the spatial dimension of the feature maps from layer to layer. As \nfor Swin-T block, it replace the standard multi-head self-attention(MSA) module in Vision Transformer with a \nWindow-MSA and a Shifted Window MSA(SW-MSA) module. Figure 10 illustrate the Swin Transformer block. \nMoreover, in Swin-T block, the performance of SW-MSA is improved by using the masking technique and the \nrelative positional encoding.\n(5)σ( z) = 1\n1 + e−z = ez\n1 + ez\nFigure 7.  (a) training process (b) performance of generator.\n9\nVol.:(0123456789)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\nWindow-based self-attention\nWindow based Self-Attention(W-MSA) is proposed to compute self-attention within local windows. The W-MSA \nimprove the efficiency of the model by arranging the windows to partition the images in a non-overlapping \n manner36.\nThe MSA in Vision Transformer computes the relationship between each patch against all other patches using \nglobal self-attention. As a result, it has a quadratic complexity with respect to the number of patches, leading to \ndifficulty in processing high resolution images. The W-MSA, however, simply compute the self-attention only \nwithin each window that is a collection of patches. Supposing each window contain M∗ M patches, the computa-\ntional complexity of a global MSA module in ViT and a window-based approach on an image of h ∗ w patches are:\nAs evident from Eqs. (6), it is apparent that the computational complexity of W-MSA exhibits a linear relation-\nship with hw (the image size), whereas the computational complexity of MSA displays a quadratic relationship. \nWhen working with same image dimensions, the cost of windows is notably smaller compared to the cost of \nblocks. Consequently, this leads to a substantial reduction in computational complexity.\nDue to the fixed window size throughout the whole network, the computational complexity of W-MSA is \nlinear with respect to the number of patches or the resolution of the image, which greatly increases efficiency \nand saves computational resources compared to the quadratic complexity of standard MSA.\nShifted window self-attention\nAlthough the W-MSA has its merits, one obvious shortcoming is that the modelling power of the network was \nlimited because the W-MSA restricts self-attention to each window and lacks connections across windows. To \n(6)�(MSA) = 4hwC2 + 2(hw)2 C,\n�(W − MSA) = 4hwC2 + 2M 2 hwC\nFigure 8.  The architecture of Swin Transformer.\nFigure 9.  Patch merging. Due to a following 1 × 1 convolution, the final number of channels should be \n2 × instead of 4 ×.\nFigure 10.  The Swin Transformer block with W-MSA and SW-MSA module.\n10\nVol:.(1234567890)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\naddress this issue, the Shifted Window Self-Attention (SW-MSA) module is used after the W-MSA module to \nperform a shifted window partitioning approach.\nShifted Window MSA shifts the windows toward the bottom right corner by a factor of M/2.This Shift opera-\ntion results in isolated patches not belong to any window, but Swin Transformer applied a ‘cyclic shift’ technique \nto move these patches into windows with incompete patches. Therefore, the windows may consist of patches not \nadjacent in the original feature map and a masking mechanism is employed to reduce self-attention computation \nto within each sub-window. This cyclic-shift introduces cross-connections between windows and improve the \nperformance of the network while maintain the efficiency same as that of regular window partitioning.\nLight-weight vision transformer: MobileViT\nViT models for classification should be light-weight and quick to be effective. With regard to the performance \non resource-constrained mobile devices, ViT models is much inferior to light-weight CNNs, even when the \nmodel size is reduced to fit the mobile devices with limited resource. DeiT(Data-Efficient Image Transformer)38, \nfor example, is 3% less accurate than  MobileNetv339 due to a parameter budget of about 5–6 million. Therefore, \ndesigning a light-weight ViT model is a critical need. In this study, MobileViT played a pivotal role as a light-\nweight and rapid transformer model, efficiently handling cell images and extracting essential image data for cell \nclass detection. Diverging from conventional transformer models with substantial parameter sizes, MobileViT \nboasts a reduced parameter count and demands fewer computing resources and memory, rendering it highly \nsuitable for deployment on edge devices.\nThe MobileViT was proposed by by Sachin Mehta and Mohammad Rastegari in Ref. 28. In contrast to ViT \nand its derivatives (with and without convolutions), it takes a different approach to learning global representa -\ntions. For a standard convolution, the operation contains three steps: Unfolding, local processing, and folding. \nThe MobileViT introduces a new layer to replace local processing in convolutions with global processing using \ntransformers. Combining the strength of CNNs with transformers, this endows the MobileViT block both CNN \nand ViT-like features, simplifying its training steps while allowing it to learn better representations with fewer \nparameters. The most important attribute of the MobileViT is that it shows light-weight ViTs can achieve opti-\nmised computational performance in the level of light-weight CNN among a large number of mobile vision \ntasks using basic training methods.\nEthics approval\nThe experimental protocol was established, according to the ethical guidelines of the Helsinki Declaration and \nwas approved by the Human Ethics Committee of Xiangya Hospital. Written informed consent was obtained \nfrom individual or guardian participants.\nResults\nBased on our experimental findings, it is evident that the Swin Transformer outperformed other models in terms \nof precision, recall, F1-score, specificity, and exhibited the lowest false positive rate (FPR). However, it is worth \nnoting that the Swin Transformer lagged behind both MobileViT and MobileNet in terms of inference time, \nframes per second (FPS), and memory usage. In a holistic evaluation, the Swin Transformer and MobileViT \nemerged as standout performers among the state-of-the-art classifiers. Remarkably, MobileViT demonstrated \nthe added advantage of being more resource-efficient, making it an ideal choice for edge devices with limited \ncomputing resources (On resource-constrained devices like smartphones, IoT devices, and edge devices, effi -\ncient inference is crucial. These devices often have limited computational power, so models must be optimized \nto run quickly).\nThe datasets we used for experiments consist of three sets: the original dataset that is taken from microscopic \nimages, the augumented dataset that processed through rotation,cropping and other technices as well as the \nmixed dataset that we mixed fake images processed by WGAN-GP network with augmented dataset. The final \ndataset we used for model training is the mixed dataset. After we obtain the augmented dataset, the generator \nnetwork was trained by looping over mini-batches of the augmented dataset to generate fake images and then \nthe fake images were fed to the discrimitor to optimize the performance of the generator. This training could \ntake lots of time to complete and may require many iterations to output good images. Finally, this strategy results \nin a generator that is able to generate convincingly realistic data and a discriminator that is able to learn strong \nfeature representations of the real images. During this process, model loss fountions were introduced to evaluate \nthe gradients of the discriminator and generator loss with respect to the learnable parameters of the discrimi-\nnator and generator networks at each epoch, respectively. The loss in training generator and discriminator are \ndetermined as  follows27:\nwhere: ∇ denotes the gradient operator. Given an image  X , a generated image ˜X , define ˆX = εX+(1-ε) ˜X for \nrandom ε ∈ U(0,1). Y, ˜Y and ˆY represent the output of the discriminator for the inputs X , ˜X and ˆX , respectively.\nTable 2 shows the hyper-parameter configuration for WGAN-GP model during training process.\nHyperparameter tuning is an important and iterative process that requires several rounds of experimenta-\ntion. Our objective is to strike a balance between exploration (trying different configurations) and exploitation \n(refining promising configurations). However, it’s worth noting that WGAN-GP tends to converge more reliably, \nmitigating the risk of mode collapse—a common issue in GANs. Furthermore, the Adam optimizer typically \ndemands less hyperparameter tuning when compared to SGD.\n(7)lossD = ˜Y − Y +\n(∇ˆX ˆY\n\n2\n− 1\n)2\n, lossG =− ˜Y,\n11\nVol.:(0123456789)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\nThe Adam optimizer is known for its ability to adaptively adjust the learning rates for different model param-\neters, making it well-suited for various types of neural network architectures and training scenarios. In the Adam \noptimizer, \"B1\" and \"B2\" are hyperparameters that control the exponential moving averages of past gradients \n(first moment) and the squared gradients (second moment), respectively. These moving averages are used to \nadaptively adjust the learning rates for individual model parameters during optimization. Cross-entropy is used \nto measure the dissimilarity between predicted and true probability distributions. Minimizing this loss during \ntraining helps the model make accurate class predictions. In terms of activation founction, LeakyReLU helps \nmitigate this issue by allowing a small gradient for negative inputs, which keeps the neuron’s learning alive.\nAfter 3000 training epochs, we obtained the WGAN-GP model that generated 8400 fake cell images in each \nblood cell category for model training. The fake images belonging to different blood cell classes are shown in \nFig. 11. Those images were then combined with augmented dataset and randomly spilt into three subsets (train-\ning set, valiadation set and testing set) by the ratio of 13:4:3.\nIn summary, after collecting 2061 images from the original microscopic dataset, we expanded it to create a \ndataset comprising 14,400 images through the use of augmentation techniques. Furthermore, we carried out \nadditional augmentation to yield an extensive dataset totaling 64,800 images using WGAN-GP . Table 3 presents \nthe Details of the dataset in three different periods. The final testing set consists of six classes and each cell class \ncontains 1620 images.\nThe batch size is configured to be either 64 or 128, depending on the availability of GPU memory. For our \nloss function, we have opted for CrossEntropyLoss. The training process spans a total of 100 epochs. Learning \ncurves are plots that show changes in learning performance over time in terms of experience as shown in Fig. 12. \nThose curves of different models represent their performance on the train and validation datasets can be used to \ndiagnose an underfit, overfit, or well-fit model. The Swin Transformer and the MobileViT exhibited remarkable \naccuracy curves and smooth loss curves compared to other architectures, firmly establishing their superiority \namong the state-of-the-art deep learning models.\nTable 2.  Setting hyper-parameters for WGAN-GP model.\nName Hyper-parameters\nInput data Augmented dataset\nImage shape 128*128*3\nBatch-size 64\nEpochs 3000\nActivation founction LeakyReLU\nLearning rate 0.0002\nOptimizer Adam\nClip value 0.01\nCpu thread 8\nB1 0.5\nB2 0.999\nFigure 11.  Fake images generate by WGAN-GP: (a) PF , (b) PM, (c) PO, (d) PV , (e) WBC.\n12\nVol:.(1234567890)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\nThe detailed architecture analysis of the Swin Transformer, Mobile-ViT and other deep learning architecture \nwith AdmaW optimizer and weight decay is outlined in Table  4. Obviously, Swin-Transformer outperforms \nother methods in Precision, recall, F1-score and specificity. Table 5 states the prediction accuracy of each model \narchitecture at each blood cell class. The maximum average accuracy was 99.885% with Swin Transformer. On \nthe other hand the MobileNet has the least inference time as shown in Table 6. However, to balance both infer-\nence speed and accuracy, MobileViT should be the best choice.\nTable 3.  Data distribution during different periods.\nBlood cell catogories Original dataset Augumented dataset Mixed dataset\nPlasmodium falciparum 187 2400 10,800\nPlatelet 457 2400 10,800\nPlasmodium malariae 113 2400 10,800\nPlasmodium ovale 93 2400 10,800\nPlasmodium vivax 168 2400 10,800\nWhite blood cells 300 2400 10,800\nRed blood cells 300 2400 10,800\nTotal 2061 14,400 64,800\nFigure 12.  The performance of four different network after the training process with mixed malaria cell image \ndataset.\nTable 4.  Precision, Recall, F1-score and Specificity in percent (%) for different trained deep learning models.\nPrecision Recall F1-score Specificity FPR\nSwin-T 99.885 99.316 99.599 99.981 0.019\nViT 86.526 51.697 64.723 97.470 2.530\nMobile-ViT 99.868 99.212 99.539 99.978 0.022\nVGG-19 79.489 39.242 52.544 95.877 4.123\nMobileNet 96.570 82.431 88.942 99.411 0.589\n13\nVol.:(0123456789)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\nIn summary, the Swin Transform deliver better accuracy compared to other neural networks in terms of \nmalaria-infected microscopic blood cell classification, and MobileViT achieve almost the same efficacy with \nlower memory usage and faster inference time. In our deep learning models, an inference means the forward \npropagation process, which given a blood cell image, gets a classification result. This classification result deter -\nmines the class of the cell image. Knowing the inference time in advance can help you design a model that will \nperform better, and be optimized for inference.\nTable 6 demonstrates that MobileNet and Mobile-ViT outperformed other models in terms of short inference \ntimes, high frames per second (FPS), and low memory utilization when applied to blood cell images captured \nfrom smeared blood samples. This shows their potential for deployment on edge devices or smartphones char-\nacterized by low memories and limited computational resources.\nDiscussions\nOur work has certain limitations. Firstly, our approach has not been assessed on extensive multiclass datasets, \nprimarily due to the limited number of patients available for this study. However, it is worth noting that previ-\nous experiments involving transformers have been conducted on large binary  datasets16,40. Secondly, we have \nrefrained from testing larger models with an increased number of parameters in this research, primarily due to \nthe requirement for more powerful GPU resources.\nAccording to our results, swin-transformer outperform the baseline architectures in terms of precision, recall, \nF1-score, specificity, and FPR on test set. There are several reasons: (1)Hierarchical Structure: Swin Transformer \nintroduces a hierarchical structure that breaks down the input image into a series of smaller non-overlapping \npatches. (2) Shifted Windows: This shift operation helps the model attend to neighboring regions, improving \nits ability to capture spatial relationships and reducing the risk of information loss at patch boundaries. (3) \nLocal–Global Attention: Swin Transformer incorporates both local and global attention mechanisms. The local \nattention mechanism enables the model to focus on nearby patches.\nIt’s worth noting that the performance of a model can vary depending on the specific task, dataset, and the \nquality of training and fine-tuning. While Swin Transformer has shown promising results in our experiment, it \nmay not always be the best choice for every computer vision task. For edge devices, MobileViT with less param-\neters is a more suitable approach to achieve the balance between high accuracy and low resource consumption.\nTable 7 shows the demonstrates the detailed comparisons between existing and the proposed methods.\nConclusion\nMalaria, a severe febrile condition brought about by Plasmodium parasites, results in tens of thousands of fatali-\nties annually. The traditional method of malaria diagnosis, involving the microscopic examination of stained \nblood slides, is favored for its affordability and accessibility. Nevertheless, this procedure poses significant chal-\nlenges, as it necessitates a proficient workforce of medical laboratory technicians—a resource that is both valuable \nand scarce on a global scale.\nIn this paper, we developed a deep learning-based automated mechanism to help and assist the doctors and \npatients in malaria parasite screening at its early stage. A new dataset of microscopic images of blood cells and \ndifferent plasmodium species was collected, labelled and then augmented with WGAN-GP and other techniques. \nThe mean accuracy of the MobileViT and Swin Transformer were 99.885% and 99.867% respectively.\nOur results unequivocally validate the effectiveness and efficiency of the chosen models for multiclass \nPlasmodium classification. Notably, both the Swin Transformer and MobileViT surpass traditional CNNs in \nperformance, with MobileViT being particularly well-suited for edge devices with constrained computational \nTable 5.  Accuracy in percent for different networks on unseen data.\nPF PLT PM PO PV RBC WBC Average\nSwin-T 99.815 99.815 99.938 99.877 99.877 99.877 100.000 99.885\nViT 88.025 90.802 89.198 74.938 66.543 98.827 97.346 84.722\nMobile-ViT 99.753 100.000 99.938 99.753 99.815 99.815 100.000 99.867\nVGG-19 74.568 48.148 99.444 76.235 78.457 79.568 100.000 79.488\nMobileNet 96.049 94.877 99.012 99.012 92.346 95.000 99.444 96.534\nTable 6.  Inference time, Frames per Second and Memory size of Model architecture.\nNetwork Inference time (ms) FPS Memory (MB)\nSwin-T 20.070 49.825 107.81\nViT 32.422 30.842 335.22\nMobile-ViT 16.463 60.742 3.85\nVGG-19 23.565 42.436 545.32\nMobileNet 10.322 96.878 8.96\n14\nVol:.(1234567890)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\ncapabilities. Moreover, the incorporation of the WGAN-GP and other augmentation techniques proves to be \nvaluable tools for expanding the image dataset.\nThe research’s clinical relevance lies in its potential to revolutionize malaria diagnosis by providing accurate, \ncost-effective, and accessible solutions. This could lead to earlier detection, improved patient outcomes, and \na more efficient allocation of healthcare resources, ultimately contributing to the global efforts to control and \neliminate malaria. Enhancing the sensitivity of the malaria diagnosis system will not only improve its accuracy \nbut also enable it to tackle more intricate diagnostic challenges. This can lead to earlier and more precise diag -\nnoses, ultimately benefiting both individual patients and public health efforts to control and eradicate malaria.\nIn our future work, we intend to cultivate a heightened sensitivity within our system, enabling it to adeptly \ntackle intricate tasks like parasite number counting, the analysis of ambiguously smeared image slices, and the \nmulti-stage lifecycle classification of Plasmodium. To achieve this objective, we will implement several strate-\ngies: (1) Lager Dataset: We will expand the diversity and size of our training dataset by enlisting the participa -\ntion of more volunteers and implementing a broader range of data augmentation techniques. This approach \nwill empower our system with a more extensive and nuanced understanding of malaria-related image data. (2) \nAdvanced Transfer Learning: Building upon our existing utilization of transfer learning techniques, we will \nremain vigilant for improved pre-trained models that become available. Leveraging these models, which have \nbeen honed on expansive microscopy image datasets, will significantly elevate the sensitivity of our system, \nenhancing its ability to discern intricate patterns and features. (3) Integration of Multiple Data Sources: We will \nexplore the integration of multiple data sources beyond image analysis. Incorporating supplementary information \nsuch as patient demographics, geographic data, and climate data will infuse our system with a holistic perspective. \nThis multi-source data fusion will bolster the system’s sensitivity, empowering it to offer more accurate predic-\ntions regarding malaria cases and their potential severity.\nWe have assembled a novel multiclass dataset encompassing PLT, WBC, RBC and four malaria species, \nnamely PO, PV , PF , and PM. This dataset serves as a pivotal resource for training deep learning-based research \ndedicated to addressing this challenging problem. To tackle the inherent imbalance within the dataset, we intro-\nduced a Generative Adversarial Network (GAN) as a strategic solution. Furthermore, our investigation revealed \nthe exceptional efficiency of two transformer models in the context of multiclass malaria parasite classification.\nData availability\nThe data are not publicly available due to privacy. The data presented in this study are available on request from \nthe corresponding author.\nReceived: 14 June 2023; Accepted: 5 October 2023\nReferences\n 1. Zhang, C. et al. Deep learning for microscopic examination of protozoan parasites. Comput. Struct. Biotechnol. J. 20, 1036–1043. \nhttps:// doi. org/ 10. 1016/j. csbj. 2022. 02. 005 (2022).\n 2. https:// www. who. int/ teams/ global- malar ia- progr amme/ repor ts/ world- malar ia- report- 2022, Accessed 1 February 2023.\n 3. Chilot, D. et al. Pooled prevalence and risk factors of malaria among children aged 6–59 months in 13 sub-Saharan African coun-\ntries: A multilevel analysis using recent malaria indicator surveys. Plos one.  18(5), e0285265 (2023).\n 4. Tawiah, K. et al. Confirmed malaria cases in children under five years: The influence of suspected cases, tested cases, and climatic \nconditions. Health Soc. Care Community 2023, 1–8 (2023).\n 5. Alemu, M. et al. Performance of laboratory professionals working on malaria microscopy in Tigray, North Ethiopia. J. Parasitol. \nRes. 2017, 9064917. https:// doi. org/ 10. 1155/ 2017/ 90649 17 (2017).\n 6. Mukadi, P . et al. External quality assessment of Giemsa-stained blood film microscopy for the diagnosis of malaria and sleeping \nsickness in the Democratic Republic of the Congo. Bull. World Health Organ. 91(6), 441–448. https:// doi. org/ 10. 2471/ BLT. 12. \n112706 (2013).\nTable 7.  Previous deep learning methodologies for malaria classification.\nReference Dataset Methods Results Advantage Limitations\n16\n27,558 images with equal \ninstances of parasitized and \nuninfected cells\nMobileViT accuracy of 98.37% Stability to preventing biased-\nness\noscillation of accuracy and loss \ncurves\n40\n27,558 images with equal \ninstances of parasitized and \nuninfected cells\ncompact convolutional trans-\nformer accuracy of 99.23% CCT with Grad-CAM visu-\nalization\nNo comparison with other \nmodels\n41\n27,558 images with equal \ninstances of parasitized and \nuninfected cells\nCycle GAN with dynamic \ncriterion FID score of 0.0043 high quality synthetic blood \ncell images\nOnly two cell classes were \ntested\n21\n27,558 images with equal \ninstances of parasitized and \nuninfected cells\nMPP algorithm for edge-based \nsegmentation F-measure values of 0.9 high accuracy Lack of the classification part\n26 345 images consisting of 111 \nblood cells on average A two-stage approach Average accuracy of 96.26% a userfriendly mobile-based \napplication is built\nNeed to employing the mobile \napp on a large scale to verify \nits efficacy\nOurs 2061 images with 6 classes of \ncells Transformers and WGAN-GP\nA balance between high \naccuracy and low resources \nconsumption\nCombination of augmentation \ntechniques and transformers\nLack of a multiclass large \ndataset\n15\nVol.:(0123456789)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\n 7. Caraballo, H. & King, K. Emergency department management of mosquito-borne illness: Malaria, dengue, and West Nile virus. \nEmerg. Med. Pract. 16(5), 1–23 (2014).\n 8. Arowolo, Micheal Olaolu, et al. \"PCA model for RNA-Seq malaria vector data classification using KNN and decision tree algo-\nrithm.\" 2020 international conference in mathematics, computer engineering and computer science (ICMCECS). IEEE, 2020.\n 9. Mukadi, P . et al. External quality assessment of giemsa-stained blood film microscopy for the diagnosis of malaria and sleeping \nsickness in the democratic republic of the Congo. Bull. World Health Organ. 91, 441–448 (2013).\n 10. Wang, Bo. et al. AI-assisted CT imaging analysis for COVID-19 screening: Building and deploying a medical AI system. Appl. Soft  \nComput. 98, 106897 (2021).\n 11. Konar, D. et al. A quantum-inspired self-supervised network model for automatic segmentation of brain MR images. Appl. Soft  \nComput. 93, 106348 (2020).\n 12. Abbas, N. et al. Machine aided malaria parasitemia detection in Giemsa-stained thin blood smears. Neural Comput. Appl. 29, \n803–818 (2018).\n 13. Devi, S. S., Laskar, R. H. & Sheikh, S. A. Hybrid classifier based life cycle stages analysis for malaria-infected erythrocyte using \nthin blood smear images. Neural Comput. Appl. 29, 217–235 (2018).\n 14. Khashman, A. Investigation of different neural models for blood cell type identification. Neural Comput. Appl.  21, 1177–1183 \n(2012).\n 15. Ahmad, F ., Ghani Khan, M. U. & Javed, K. Deep learning model for distinguishing novel coronavirus from other chest related \ninfections in X-ray images. Comput Biol Med. 134, 104401. https:// doi. org/ 10. 1016/j. compb iomed. 2021. 104401 (2021).\n 16. Marefat A, Hassannataj Joloudari J, Rastgarpour M. A Transformer-based Algorithm for Automatically Diagnosing Malaria Parasite \nin Thin Blood Smear Images Using MobileViT. 2023.\n 17. Diker, A. An efficient model of residual based convolutional neural network with Bayesian optimisation for the classification of \nmalarial cell images. Comput. Biol. Med. 148, 105635 (2022).\n 18. Ufuktepe, D. K. et al. Deep Learning-Based Cell Detection and Extraction in Thin Blood Smears for Malaria Diagnosis, 2021 IEEE \nApplied Imagery Pattern Recognition Workshop (AIPR) (IEEE, 2021).\n 19. Molina, A. et al. Automatic identification of malaria and other red blood cell inclusions using convolutional neural networks. \nComput. Biol. Med. 136, 104680. https:// doi. org/ 10. 1016/j. compb iomed. 2021. 104680 (2021).\n 20. Y ang, F . et al. Smartphone-supported malaria diagnosis based on deep learning. In International workshop on machine learning in \nmedical imaging (ed. Y ang, F .) 73–80 (Springer, 2019).\n 21. Shambhu, Shankar, Deepika Koundal, and Prasenjit Das. \"Edge-Based Segmentation for Accurate Detection of Malaria Parasites \nin Microscopic Blood Smear Images: A Novel Approach using FCM and MPP Algorithms.\" 2023 2nd International Conference \non Smart Technologies and Systems for Next Generation Computing (ICSTSN). IEEE, 2023.\n 22. Li, S., Y ang, Q., Jiang, H., Cortés-Vecino, J. A. & Zhang, Y . Parasitologist-level classification of apicomplexan parasites and host \ncell with deep cycle transfer learning (DCTL). Bioinformatics 36(16), 4498–4505. https:// doi. org/ 10. 1093/ bioin forma tics/ btaa5 13 \n(2020).\n 23. Jiang, H. et al. Geometry-aware cell detection with deep learning. mSystems 5(1), e00840-19. https:// doi. org/ 10. 1128/ mSyst ems. \n00840- 19 (2020).\n 24. Durant, T. J. S. et al. Applications of digital microscopy and densely connected convolutional neural networks for automated \nquantification of Babesia-infected erythrocytes. Clin. Chem. 68(1), 218–229. https:// doi. org/ 10. 1093/ clinc hem/ hvab2 37 (2021).\n 25. Devi, S. S., Laskar, R. H. & Sheikh, S. A. Hybrid classifier based life cycle stages analysis for malaria-infected erythrocyte using \nthin blood smear images. Neural Comput. Appl. 29(8), 217–235 (2018).\n 26. Arshad, Q. A. et al. A dataset and benchmark for malaria lifecycle classification in thin blood smear images. Neural Comput. Appl. \nhttps:// doi. org/ 10. 1007/ s00521- 021- 06602-6 (2021).\n 27. Gulrajani, Ishaan, et al. \"Improved training of wasserstein gans.\" Advances in neural information processing systems 30 (2017).\n 28. Wadekar, Shakti N., and Abhishek Chaurasia. Mobilevitv3: Mobile-friendly vision transformer with simple and effective fusion \nof local, global and input features. Preprint at https:// arXiv. org/ quant- ph/ 2209. 15159 (2022).\n 29. Ian Goodfellow, et al., Generative adversarial nets, Adv. Neural Inf. Process. Syst. (2014) 27\n 30. M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein gan. Preprint at https:// arXiv. org/ quant- ph/ 1701. 07875 (2017).\n 31. Goodfellow, I. et al. Generative adversarial networks. Commun. ACM 63(11), 139–144 (2020).\n 32. Weng, Lilian. \"From gan to wgan.\" Preprint at https:// arXiv. org/ quant- ph/ 1904. 08994 (2019).\n 33. Deng, F . et al. Image restoration of motion artifacts in cardiac arteries and vessels based on a generative adversarial network. Quant. \nImaging Med. Surg. 12(5), 2755–2766. https:// doi. org/ 10. 21037/ qims- 20- 1400 (2022).\n 34. Bing Xu, et al., Empirical evaluation of rectified activations in convolutional network. Preprint at https:// arXiv. org/ quant- ph/ 1505. \n00853 (2015).\n 35. Vaswani, Ashish, et al. \"Attention is all you need.\" Advances in neural information processing systems 30 (2017).\n 36. Liu, Ze, et al. \"Swin transformer: Hierarchical vision transformer using shifted windows.\" Proceedings of the IEEE/CVF interna -\ntional conference on computer vision. 2021.\n 37. Dosovitskiy, Alexey, et al. An image is worth 16x16 words: Transformers for image recognition at scale. Preprint at https:// arXiv. \norg/ quant- ph/ 2010. 11929 (2020).\n 38. Touvron, Hugo, Matthieu Cord, and Hervé Jégou. \"Deit iii: Revenge of the vit.\" Computer Vision–ECCV 2022: 17th European \nConference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXIV . Cham: Springer Nature Switzerland, 2022.\n 39. Howard, Andrew, et al. \"Searching for mobilenetv3.\" Proceedings of the IEEE/CVF international conference on computer vision. \n2019.\n 40. Islam, M. R. et al. Explainable transformer-based deep learning model for the detection of malaria parasites from blood cell images. \nSensors (Basel) 22(12), 4358. https:// doi. org/ 10. 3390/ s2212 4358 (2022).\n 41. Liang, Z. & Huang, J. X. CycleGAN with dynamic criterion for malaria blood cell image synthetization. AMIA Jt. Summits Transl. \nSci. Proc. 2022, 323–330 (2022).\nAuthor contributions\nAll authors contributed to the study conception and design. Material preparation, data collection and analysis \nwere performed by X.L. The first draft of the manuscript was written by D.T. and all authors commented on \nprevious versions of the manuscript. All authors read and approved the final manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to X.L.\nReprints and permissions information is available at www.nature.com/reprints.\n16\nVol:.(1234567890)Scientific Reports |        (2023) 13:17136  | https://doi.org/10.1038/s41598-023-44297-y\nwww.nature.com/scientificreports/\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2023, corrected publication 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7725953459739685
    },
    {
      "name": "Transformer",
      "score": 0.6324521899223328
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6198800802230835
    },
    {
      "name": "Inference",
      "score": 0.5992176532745361
    },
    {
      "name": "Machine learning",
      "score": 0.5608044862747192
    },
    {
      "name": "Test set",
      "score": 0.5267179608345032
    },
    {
      "name": "Malaria",
      "score": 0.496161550283432
    },
    {
      "name": "Medicine",
      "score": 0.09595149755477905
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Immunology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139660479",
      "name": "Central South University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210159865",
      "name": "Xiangya Hospital Central South University",
      "country": "CN"
    }
  ],
  "cited_by": 11
}