{
    "title": "COCO-LM: Correcting and Contrasting Text Sequences for Language Model Pretraining",
    "url": "https://openalex.org/W3131870090",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2103641016",
            "name": "Meng Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746807616",
            "name": "Xiong Chenyan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224066854",
            "name": "Bajaj, Payal",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222630530",
            "name": "Tiwary, Saurabh",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3019377260",
            "name": "Bennett, Paul",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2125747881",
            "name": "Han, Jiawei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2108846051",
            "name": "Song Xia",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2962753370",
        "https://openalex.org/W2907252220",
        "https://openalex.org/W2396767181",
        "https://openalex.org/W2970727289",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W3092806700",
        "https://openalex.org/W3213189520",
        "https://openalex.org/W3118668786",
        "https://openalex.org/W3105422445",
        "https://openalex.org/W2949433733",
        "https://openalex.org/W3119866685",
        "https://openalex.org/W3104033643",
        "https://openalex.org/W2130158090",
        "https://openalex.org/W3122515622",
        "https://openalex.org/W3119492251",
        "https://openalex.org/W3035124264",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3172806051",
        "https://openalex.org/W2995923603",
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2966892770",
        "https://openalex.org/W3038572442",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W3106061119",
        "https://openalex.org/W3122924117",
        "https://openalex.org/W2948771346",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W3113747735",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2970641574",
        "https://openalex.org/W3106109117",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2996035354",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3042711927",
        "https://openalex.org/W3034978746",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2805206884",
        "https://openalex.org/W3167602185",
        "https://openalex.org/W3157758108",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W3037854022",
        "https://openalex.org/W2525127255",
        "https://openalex.org/W3040558716",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W2988217457",
        "https://openalex.org/W3153427360",
        "https://openalex.org/W3105816068",
        "https://openalex.org/W131533222",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2842511635",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W3172642864",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3105238007",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W3115295967",
        "https://openalex.org/W2971274815",
        "https://openalex.org/W3118062200",
        "https://openalex.org/W2787560479",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3034830866",
        "https://openalex.org/W3122890974"
    ],
    "abstract": "We present a self-supervised learning framework, COCO-LM, that pretrains Language Models by COrrecting and COntrasting corrupted text sequences. Following ELECTRA-style pretraining, COCO-LM employs an auxiliary language model to corrupt text sequences, upon which it constructs two new tasks for pretraining the main model. The first token-level task, Corrective Language Modeling, is to detect and correct tokens replaced by the auxiliary model, in order to better capture token-level semantics. The second sequence-level task, Sequence Contrastive Learning, is to align text sequences originated from the same source input while ensuring uniformity in the representation space. Experiments on GLUE and SQuAD demonstrate that COCO-LM not only outperforms recent state-of-the-art pretrained models in accuracy, but also improves pretraining efficiency. It achieves the MNLI accuracy of ELECTRA with 50% of its pretraining GPU hours. With the same pretraining steps of standard base/large-sized models, COCO-LM outperforms the previous best models by 1+ GLUE average points.",
    "full_text": "COCO-LM: Correcting and Contrasting Text\nSequences for Language Model Pretraining\nYu Meng1∗, Chenyan Xiong2, Payal Bajaj2, Saurabh Tiwary2,\nPaul Bennett2, Jiawei Han1, Xia Song2\n1 University of Illinois at Urbana-Champaign 2 Microsoft\n1 {yumeng5,hanj}@illinois.edu\n2 {chenyan.xiong,payal.bajaj,satiwary,\npaul.n.bennett,xiaso}@microsoft.com\nAbstract\nWe present a self-supervised learning framework, COCO-LM, that pretrains Lan-\nguage Models by COrrecting and COntrasting corrupted text sequences. Following\nELECTRA-style pretraining, COCO-LM employs an auxiliary language model\nto corrupt text sequences, upon which it constructs two new tasks for pretraining\nthe main model. The ﬁrst token-level task, Corrective Language Modeling, is to\ndetect and correct tokens replaced by the auxiliary model, in order to better capture\ntoken-level semantics. The second sequence-level task, Sequence Contrastive\nLearning, is to align text sequences originated from the same source input while en-\nsuring uniformity in the representation space. Experiments on GLUE and SQuAD\ndemonstrate that COCO-LM not only outperforms recent state-of-the-art pretrained\nmodels in accuracy, but also improves pretraining efﬁciency. It achieves the MNLI\naccuracy of ELECTRA with 50% of its pretraining GPU hours. With the same\npretraining steps of standard base/large-sized models, COCO-LM outperforms the\nprevious best models by 1+ GLUE average points.\n1 Introduction\nPretrained language models (PLMs) have reshaped the way AI systems process natural language [11,\n36, 39, 40]. Before task-speciﬁc training, it is now a common practice to ﬁrst pretrain the deep\nneural networks, often Transformers [ 53], via a self-supervised token-level language modeling\ntask [29, 31, 40]. Whether it is autoregressive [39], permutational [62], or masked language modeling\n(MLM) [11], the Transformer networks are pretrained to recover some omitted tokens using the rest\nof input texts. Then the language semantics captured during pretraining are conveyed to downstream\ntasks via the pretrained Transformer parameters [5, 8, 44].\nRecent research [14, 16, 25, 43] observed several challenges in this self-supervised learning frame-\nwork. One challenge is its efﬁciency. After pretrained for a while with the standard token-level\nlanguage modeling, the networks have already captured the basic language patterns, making a large\nfraction of pretraining signals no longer informative. Linear improvement in the model effectiveness\noften requires exponentially more pretraining compute and parameters [25], which is unsustainable.\nAnother challenge is the anisotropy of text representations from pretrained models. The sequence\nrepresentations from many pretrained models are quite irregular [ 30, 43] and require dedicated\nﬁne-tuning approaches to be useful in sequence-level applications [32, 60].\nClark et al. [7] proposed a new pretraining strategy, ELECTRA, that uses an auxiliary language model\n(“generator”) to replace tokens in input texts and pretrains the main Transformer (“discriminator”) to\n∗Part of this work was done while Yu was interning at Microsoft.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2102.08473v2  [cs.CL]  27 Oct 2021\ndetect replaced tokens. This improves the pretraining efﬁciency and effectiveness, but pretraining\nvia binary classiﬁcation hinders the model’s usage on applications requiring language modeling\ncapability (e.g., prompt-based learning [15, 28, 46]). It could further distort the representation space\nas the Transformers are pretrained to output the same “non-replacement” label for all actual tokens.\nIn this paper, we present a new self-supervised learning approach, COCO-LM, that pretrains Lan-\nguage Models by COrrecting and COntrasting corrupted text sequences. Following ELECTRA-style\npretraining, COCO-LM employs an auxiliary model to corrupt the input texts, upon which it intro-\nduces two new pretraining tasks for the main Transformer, one at token level and one at sequence\nlevel. The token-level task, corrective language modeling (CLM), pretrains the main Transformer\nto detect and correct the tokens in the corrupted sequences. It uses a multi-task setup to combine\nthe beneﬁts of replaced token detection and language modeling. The sequence-level task, sequence\ncontrastive learning (SCL), pretrains the model to align text sequences originated from the same\nsource sequence and enforce uniformity of the representation space.\nIn our experiments on GLUE [54] and SQuAD [41] benchmarks, COCO-LM not only outperforms\nstate-of-the-art pretraining approaches in effectiveness, but also signiﬁcantly improves the pretraining\nefﬁciency. Under the same setting, COCO-LM matches the MNLI accuracy of RoBERTa and\nELECTRA with 60% and 50% of their GPU hours in pretraining, respectively. When pretrained with\nthe same number of steps, COCO-LM outperforms the previous best models by 1+ GLUE average\npoints under the standard base/large-sized model evaluations. With 367 million parameters, COCO-\nLMLarge++ reaches the MNLI accuracy of Megatron3.9B [49], one of the largest BERT-style model with\n3.9 billion parameters. Our analyses provide further insights on the advantage of CLM in learning\ntoken representations and its effectiveness in prompted-based ﬁne-tuning, as well as the beneﬁt of\nSCL in ensuring alignment and uniformity in the representation space for better generalization1.\n2 Related Work\nVarious token-level tasks have been used to pretrain language models. The most classic auto-regressive\nlanguage modeling is to predict a token given all the previous tokens, or all subsequent ones [36, 39].\nBERT uses masked language modeling (MLM) that recovers randomly masked tokens using the rest\ninput. XLNet proposes permutation language modeling that conducts MLM in an autoregressive\nmanner [62]. UniLM uses pseudo MLM which uniﬁes autoregressive and MLM tasks [1, 13].\nSequence-level tasks are also explored, which often pretrain the model to predict certain co-\noccurrences of sequence pairs. For example, next sentence prediction [11], sentence ordering [27]\nand previous sentence prediction [56] concatenate two sentences (either correlated or random), and\ntrain the Transformer to classify the pair.\nEmpirically, MLM is still among the most effective tasks to pretrain encoders [ 29, 31, 40].\nRoBERTa [31] found the sentence-level task in BERT not beneﬁtial and discarded it. BART [29] and\nT5 [40] both observed that MLM is often the most effective task. The empirical advantages of other\npretraining tasks are more task-speciﬁc, for example, entity related masks for knowledge intensive\napplications [20, 24], and sequence-level tasks for long form text modeling [42].\nInstead of randomly altering texts, ELECTRA [7] uses a smaller auxiliary Transformer pretrained\nby MLM to replace some tokens in the text sequences using its language modeling probability, and\npretrains the main Transformer to detect the replaced tokens. ELECTRA achieves state-of-the-art\naccuracy in many language tasks [7]. Later, Clark et el. [6] developed ELECTRIC, which pretrains\nencoders by contrasting original tokens against negatives sampled from a cloze model. ELECTRIC\nre-enables the language modeling capability but underperforms ELECTRA in downstream tasks.\nOur work is also related to contrastive learning which has shown great success in visual representation\nlearning [4, 22, 34]. Its effectiveness of in language is more observed in the ﬁne-tuning stage, for\nexample, in sentence representation [16], dense retrieval [60], and GLUE ﬁne-tuning [19].\n3 Method\nWe present the preliminaries of PLMs, their challenges, and the new COCO-LM framework.\n1Code and pretrained models can be found at https://github.com/microsoft/COCO-LM.\n2\n3.1 Preliminary on Language Model Pretraining\nIn this work we focus on pretraining BERT-style bidirectional Transformer encoders [11] that are\nwidely used in language representation tasks. We ﬁrst recap the masked language modeling (MLM)\ntask introduced by BERT [11] and then discuss the pretraining framework of ELECTRA [7].\nBERT Pretraining uses the masked language modeling task (MLM) [11], which is to take an input\nsequence Xorig = [xorig\n1 ,...,x orig\ni ,...,x orig\nn ], with 15% random tokens replaced by [MASK] symbols\n(e.g., the i-th token), and train the model to predict the original tokens at the masked positions:\n[\nxorig\n1 ,..., [MASK]i,...,x orig\nn\n] Transformer\n−−−−−−→H\nMLM Head\n−−−−−−→pMLM(x|hi),\nwhere the Transformer generates contextualized representations H = {hi}n\ni=1. The MLM Head\npredicts the masked token from the vocabulary V using the hidden representation hi and token em-\nbeddings x. The pretraining minimizes the MLM loss on the set of masked positionsM. Speciﬁcally,\npMLM(x|hi) = exp(x⊤hi)∑\nxt∈V exp(x⊤\nt hi); LMLM = E\n(\n−\n∑\ni∈M\nlog pMLM\n(\nxorig\ni\n⏐⏐hi\n))\n.\nELECTRA Pretraining uses two Transformers, a “generator” pretrained by MLM, and a “discrimi-\nnator” pretrained using the generator’s outputs. We refer them asauxiliary and main Transformers,\nas the former is discarded after pretraining and the latter may be trained by “generative” tasks too.\nThe auxiliary model outputs a corrupted sequence XMLM by sampling from its predicted probability:\nxMLM\ni ∼pMLM (x|hi) , if i∈M ; xMLM\ni = xorig\ni , else. (1)\nThe masked positions are replaced by sampled tokens considered plausible in context by the auxiliary\nTransformer, which are more deceiving than random replacements. ELECTRA uses a skinnier\nauxiliary network (e.g., hidden dimension is 1/3 of the main model) to control the signal difﬁculty.\nThe main Transformer takes XMLM and classiﬁes the replaced tokens:\nXMLM Main Transformer\n−−−−−−−−−→H\nRTD Head\n−−−−−→pRTD\n(\n1 (xMLM\ni = xorig\ni )\n⏐⏐hi\n)\n,\nwhere 1 (·) is the indicator function. The Replaced Token Detection (RTD) head uses a sigmoid linear\nlayer to output the binary probability, and the main Transformer is trained with binary cross entropy\nloss. The RTD task is trained on all tokens instead of masked ones and improves efﬁciency.\nThe two Transformers are pretrained jointly. The auxiliary model gradually generates more realistic\nreplacement tokens and the main model learns to better detect them. This forms a natural learning\ncurriculum and signiﬁcantly improves ELECTRA’s accuracy in downstream tasks [7].\n3.2 Challenges of ELECTRA-Style Pretraining\n0.00 0.25 0.50 0.75 1.00 \nCosine Similarity\n0\n10\n20Estimated Density\nrandom\nsimilar\n(a) RoBERTa.\n0.00 0.25 0.50 0.75 1.00 \nCosine Similarity\n0\n20\n40Estimated Density\nrandom\nsimilar (b) ELECTRA.\nFigure 1: Cosine similarity distributions of ran-\ndom/similar sequence pairs using [CLS] embed-\ndings from pretrained models. Histograms/curves\nare distribution bins/kernel density estimates.\nMissing Language Modeling Beneﬁts. The\nclassiﬁcation task in ELECTRA is simpler and\nmore stable [61], but raises two challenges. The\nﬁrst is the lack of language modeling capability\nwhich is a necessity in some tasks [6]. For exam-\nple, prompt-based learning requires a language\nmodel to generate labels [15, 33, 45, 46]. The\nsecond is that the binary classiﬁcation task may\nnot be sufﬁcient to capture certain word-level\nsemantics that are critical for token-level tasks.\nSqueezing Representation Space. Another\nchallenge is that the representations from\nTransformer-based language models often reside\nin a narrow cone, where two random sentences\nhave high similarity scores (lack of uniformity),\nand closely related sentences may have more different representations (lack of alignment) [14, 16, 30].\n3\nAuxiliary Transformer\nA C D\n<latexit sha1_base64=\"Lz9UtVjW6dgDM2Q9ac+6GStMUiw=\">AAAB+HicbVBNS8NAEN34WetHox69LBbBU0mKoseKF0GEivYD0lA22027dLMJuxOxhv4SLx4U8epP8ea/cdvmoK0PBh7vzTAzL0gE1+A439bS8srq2npho7i5tb1Tsnf3mjpOFWUNGotYtQOimeCSNYCDYO1EMRIFgrWC4eXEbz0wpXks72GUMD8ifclDTgkYqWuXOsAeASDzbi7urv1x1y47FWcKvEjcnJRRjnrX/ur0YppGTAIVRGvPdRLwM6KAU8HGxU6qWULokPSZZ6gkEdN+Nj18jI+M0sNhrExJwFP190RGIq1HUWA6IwIDPe9NxP88L4Xw3M+4TFJgks4WhanAEONJCrjHFaMgRoYQqri5FdMBUYSCyapoQnDnX14kzWrFPa04tyflWjWPo4AO0CE6Ri46QzV0heqogShK0TN6RW/Wk/VivVsfs9YlK5/ZR39gff4AxHOTGg==</latexit>\n[MASK]\n<latexit sha1_base64=\"Lz9UtVjW6dgDM2Q9ac+6GStMUiw=\">AAAB+HicbVBNS8NAEN34WetHox69LBbBU0mKoseKF0GEivYD0lA22027dLMJuxOxhv4SLx4U8epP8ea/cdvmoK0PBh7vzTAzL0gE1+A439bS8srq2npho7i5tb1Tsnf3mjpOFWUNGotYtQOimeCSNYCDYO1EMRIFgrWC4eXEbz0wpXks72GUMD8ifclDTgkYqWuXOsAeASDzbi7urv1x1y47FWcKvEjcnJRRjnrX/ur0YppGTAIVRGvPdRLwM6KAU8HGxU6qWULokPSZZ6gkEdN+Nj18jI+M0sNhrExJwFP190RGIq1HUWA6IwIDPe9NxP88L4Xw3M+4TFJgks4WhanAEONJCrjHFaMgRoYQqri5FdMBUYSCyapoQnDnX14kzWrFPa04tyflWjWPo4AO0CE6Ri46QzV0heqogShK0TN6RW/Wk/VivVsfs9YlK5/ZR39gff4AxHOTGg==</latexit>\n[MASK]\n<latexit sha1_base64=\"Lz9UtVjW6dgDM2Q9ac+6GStMUiw=\">AAAB+HicbVBNS8NAEN34WetHox69LBbBU0mKoseKF0GEivYD0lA22027dLMJuxOxhv4SLx4U8epP8ea/cdvmoK0PBh7vzTAzL0gE1+A439bS8srq2npho7i5tb1Tsnf3mjpOFWUNGotYtQOimeCSNYCDYO1EMRIFgrWC4eXEbz0wpXks72GUMD8ifclDTgkYqWuXOsAeASDzbi7urv1x1y47FWcKvEjcnJRRjnrX/ur0YppGTAIVRGvPdRLwM6KAU8HGxU6qWULokPSZZ6gkEdN+Nj18jI+M0sNhrExJwFP190RGIq1HUWA6IwIDPe9NxP88L4Xw3M+4TFJgks4WhanAEONJCrjHFaMgRoYQqri5FdMBUYSCyapoQnDnX14kzWrFPa04tyflWjWPo4AO0CE6Ri46QzV0heqogShK0TN6RW/Wk/VivVsfs9YlK5/ZR39gff4AxHOTGg==</latexit>\n[MASK]\n<latexit sha1_base64=\"Lz9UtVjW6dgDM2Q9ac+6GStMUiw=\">AAAB+HicbVBNS8NAEN34WetHox69LBbBU0mKoseKF0GEivYD0lA22027dLMJuxOxhv4SLx4U8epP8ea/cdvmoK0PBh7vzTAzL0gE1+A439bS8srq2npho7i5tb1Tsnf3mjpOFWUNGotYtQOimeCSNYCDYO1EMRIFgrWC4eXEbz0wpXks72GUMD8ifclDTgkYqWuXOsAeASDzbi7urv1x1y47FWcKvEjcnJRRjnrX/ur0YppGTAIVRGvPdRLwM6KAU8HGxU6qWULokPSZZ6gkEdN+Nj18jI+M0sNhrExJwFP190RGIq1HUWA6IwIDPe9NxP88L4Xw3M+4TFJgks4WhanAEONJCrjHFaMgRoYQqri5FdMBUYSCyapoQnDnX14kzWrFPa04tyflWjWPo4AO0CE6Ri46QzV0heqogShK0TN6RW/Wk/VivVsfs9YlK5/ZR39gff4AxHOTGg==</latexit>\n[MASK]\nB\nsampling\nCA D F\nA C DB F\nA\nMain Transformer\nB D\n<latexit sha1_base64=\"b6GtcADrbBlMoXpWmUI0uPADk3E=\">AAAB9XicbVBNS8NAEJ34WetX1aOXYBE8laQoeqzowWMF+wFpLJvtpl262YTdiVpC/4cXD4p49b9489+4bXPQ1gcDj/dmmJkXJIJrdJxva2l5ZXVtvbBR3Nza3tkt7e03dZwqyho0FrFqB0QzwSVrIEfB2oliJAoEawXDq4nfemBK81je4ShhfkT6koecEjTSfQfZEyJmXv3y2h93S2Wn4kxhLxI3J2XIUe+Wvjq9mKYRk0gF0dpznQT9jCjkVLBxsZNqlhA6JH3mGSpJxLSfTa8e28dG6dlhrExJtKfq74mMRFqPosB0RgQHet6biP95XorhhZ9xmaTIJJ0tClNhY2xPIrB7XDGKYmQIoYqbW206IIpQNEEVTQju/MuLpFmtuGcV5/a0XKvmcRTgEI7gBFw4hxrcQB0aQEHBM7zCm/VovVjv1sesdcnKZw7gD6zPH55tkog=</latexit>\n[PAD]\n<latexit sha1_base64=\"b6GtcADrbBlMoXpWmUI0uPADk3E=\">AAAB9XicbVBNS8NAEJ34WetX1aOXYBE8laQoeqzowWMF+wFpLJvtpl262YTdiVpC/4cXD4p49b9489+4bXPQ1gcDj/dmmJkXJIJrdJxva2l5ZXVtvbBR3Nza3tkt7e03dZwqyho0FrFqB0QzwSVrIEfB2oliJAoEawXDq4nfemBK81je4ShhfkT6koecEjTSfQfZEyJmXv3y2h93S2Wn4kxhLxI3J2XIUe+Wvjq9mKYRk0gF0dpznQT9jCjkVLBxsZNqlhA6JH3mGSpJxLSfTa8e28dG6dlhrExJtKfq74mMRFqPosB0RgQHet6biP95XorhhZ9xmaTIJJ0tClNhY2xPIrB7XDGKYmQIoYqbW206IIpQNEEVTQju/MuLpFmtuGcV5/a0XKvmcRTgEI7gBFw4hxrcQB0aQEHBM7zCm/VovVjv1sesdcnKZw7gD6zPH55tkog=</latexit>\n[PAD]C\n<latexit sha1_base64=\"b6GtcADrbBlMoXpWmUI0uPADk3E=\">AAAB9XicbVBNS8NAEJ34WetX1aOXYBE8laQoeqzowWMF+wFpLJvtpl262YTdiVpC/4cXD4p49b9489+4bXPQ1gcDj/dmmJkXJIJrdJxva2l5ZXVtvbBR3Nza3tkt7e03dZwqyho0FrFqB0QzwSVrIEfB2oliJAoEawXDq4nfemBK81je4ShhfkT6koecEjTSfQfZEyJmXv3y2h93S2Wn4kxhLxI3J2XIUe+Wvjq9mKYRk0gF0dpznQT9jCjkVLBxsZNqlhA6JH3mGSpJxLSfTa8e28dG6dlhrExJtKfq74mMRFqPosB0RgQHet6biP95XorhhZ9xmaTIJJ0tClNhY2xPIrB7XDGKYmQIoYqbW206IIpQNEEVTQju/MuLpFmtuGcV5/a0XKvmcRTgEI7gBFw4hxrcQB0aQEHBM7zCm/VovVjv1sesdcnKZw7gD6zPH55tkog=</latexit>\n[PAD]\n<latexit sha1_base64=\"b6GtcADrbBlMoXpWmUI0uPADk3E=\">AAAB9XicbVBNS8NAEJ34WetX1aOXYBE8laQoeqzowWMF+wFpLJvtpl262YTdiVpC/4cXD4p49b9489+4bXPQ1gcDj/dmmJkXJIJrdJxva2l5ZXVtvbBR3Nza3tkt7e03dZwqyho0FrFqB0QzwSVrIEfB2oliJAoEawXDq4nfemBK81je4ShhfkT6koecEjTSfQfZEyJmXv3y2h93S2Wn4kxhLxI3J2XIUe+Wvjq9mKYRk0gF0dpznQT9jCjkVLBxsZNqlhA6JH3mGSpJxLSfTa8e28dG6dlhrExJtKfq74mMRFqPosB0RgQHet6biP95XorhhZ9xmaTIJJ0tClNhY2xPIrB7XDGKYmQIoYqbW206IIpQNEEVTQju/MuLpFmtuGcV5/a0XKvmcRTgEI7gBFw4hxrcQB0aQEHBM7zCm/VovVjv1sesdcnKZw7gD6zPH55tkog=</latexit>\n[PAD]\n<latexit sha1_base64=\"Ne6IepByaRW0EwZNw1lgS4KjoQ4=\">AAAB9XicbVDLTgJBEJzFF+IL9ehlIjHxRHaJRo8kXDx4wCiPZFnJ7NDAhNlHZnpVsuE/vHjQGK/+izf/xgH2oGAlnVSqutPd5cdSaLTtbyu3srq2vpHfLGxt7+zuFfcPmjpKFIcGj2Sk2j7TIEUIDRQooR0rYIEvoeWPalO/9QBKiyi8w3EMXsAGoegLztBI9x2EJ0RM3dr1rTfpFkt22Z6BLhMnIyWSod4tfnV6EU8CCJFLprXr2DF6KVMouIRJoZNoiBkfsQG4hoYsAO2ls6sn9MQoPdqPlKkQ6Uz9PZGyQOtx4JvOgOFQL3pT8T/PTbB/6aUijBOEkM8X9RNJMaLTCGhPKOAox4YwroS5lfIhU4yjCapgQnAWX14mzUrZOS/bN2elaiWLI0+OyDE5JQ65IFVyReqkQThR5Jm8kjfr0Xqx3q2PeWvOymYOyR9Ynz+yLJKV</latexit>\n[CLS]\n<latexit sha1_base64=\"Ne6IepByaRW0EwZNw1lgS4KjoQ4=\">AAAB9XicbVDLTgJBEJzFF+IL9ehlIjHxRHaJRo8kXDx4wCiPZFnJ7NDAhNlHZnpVsuE/vHjQGK/+izf/xgH2oGAlnVSqutPd5cdSaLTtbyu3srq2vpHfLGxt7+zuFfcPmjpKFIcGj2Sk2j7TIEUIDRQooR0rYIEvoeWPalO/9QBKiyi8w3EMXsAGoegLztBI9x2EJ0RM3dr1rTfpFkt22Z6BLhMnIyWSod4tfnV6EU8CCJFLprXr2DF6KVMouIRJoZNoiBkfsQG4hoYsAO2ls6sn9MQoPdqPlKkQ6Uz9PZGyQOtx4JvOgOFQL3pT8T/PTbB/6aUijBOEkM8X9RNJMaLTCGhPKOAox4YwroS5lfIhU4yjCapgQnAWX14mzUrZOS/bN2elaiWLI0+OyDE5JQ65IFVyReqkQThR5Jm8kjfr0Xqx3q2PeWvOymYOyR9Ynz+yLJKV</latexit>\n[CLS]\n<latexit sha1_base64=\"Ne6IepByaRW0EwZNw1lgS4KjoQ4=\">AAAB9XicbVDLTgJBEJzFF+IL9ehlIjHxRHaJRo8kXDx4wCiPZFnJ7NDAhNlHZnpVsuE/vHjQGK/+izf/xgH2oGAlnVSqutPd5cdSaLTtbyu3srq2vpHfLGxt7+zuFfcPmjpKFIcGj2Sk2j7TIEUIDRQooR0rYIEvoeWPalO/9QBKiyi8w3EMXsAGoegLztBI9x2EJ0RM3dr1rTfpFkt22Z6BLhMnIyWSod4tfnV6EU8CCJFLprXr2DF6KVMouIRJoZNoiBkfsQG4hoYsAO2ls6sn9MQoPdqPlKkQ6Uz9PZGyQOtx4JvOgOFQL3pT8T/PTbB/6aUijBOEkM8X9RNJMaLTCGhPKOAox4YwroS5lfIhU4yjCapgQnAWX14mzUrZOS/bN2elaiWLI0+OyDE5JQ65IFVyReqkQThR5Jm8kjfr0Xqx3q2PeWvOymYOyR9Ynz+yLJKV</latexit>\n[CLS]\n<latexit sha1_base64=\"Ne6IepByaRW0EwZNw1lgS4KjoQ4=\">AAAB9XicbVDLTgJBEJzFF+IL9ehlIjHxRHaJRo8kXDx4wCiPZFnJ7NDAhNlHZnpVsuE/vHjQGK/+izf/xgH2oGAlnVSqutPd5cdSaLTtbyu3srq2vpHfLGxt7+zuFfcPmjpKFIcGj2Sk2j7TIEUIDRQooR0rYIEvoeWPalO/9QBKiyi8w3EMXsAGoegLztBI9x2EJ0RM3dr1rTfpFkt22Z6BLhMnIyWSod4tfnV6EU8CCJFLprXr2DF6KVMouIRJoZNoiBkfsQG4hoYsAO2ls6sn9MQoPdqPlKkQ6Uz9PZGyQOtx4JvOgOFQL3pT8T/PTbB/6aUijBOEkM8X9RNJMaLTCGhPKOAox4YwroS5lfIhU4yjCapgQnAWX14mzUrZOS/bN2elaiWLI0+OyDE5JQ65IFVyReqkQThR5Jm8kjfr0Xqx3q2PeWvOymYOyR9Ynz+yLJKV</latexit>\n[CLS]\nOriginal Sequence: ABCDE\nCorrective Language Modeling\nMasked Sequence: A_CD_\nCropped Sequence: BCD\nRandom Mask\nRandom Crop\nInput\nInput\nInput\nsampling\nSequence Contrastive \nLearning\nMain Transformer\nB C D E\nCOCO-LM Pretraining Tasks:\nCorrective Language Modeling (CLM)\nSequence Contrastive Learning (SCL)\nFigure 2: The overview of COCO-LM. The auxiliary Transformer is pretrained by MLM. Its corrupted\ntext sequence is used as the main Transformer’s pretraining input in Corrective Language Modeling\nand paired with the cropped original sequence for Sequence Contrastive Learning.\nFigure 1 illustrates such behaviors with random sentence pairs (from pretraining corpus) and semanti-\ncally similar pairs (those annotated with maximum similarity from STS-B [3]). With RoBERTa, the\ncosine similarities of most random sentence pairs are near 0.8, bigger than many semantically similar\npairs. The representation space from ELECTRA is even more squeezed. Nearly all sentence pairs,\nboth random and similar ones, have around0.9 cosine similarity. This may not be surprising as ELEC-\nTRA is pretrained to predict the same output (“non-replacement”) for all tokens in these sequences.\nThe irregular representation space raises the risk of degeneration [ 37, 55] and often necessitates\nsophisticated post-adjustment or ﬁne-tuning to improve the sequence representations [16, 30, 32, 60].\n3.3 COCO-LM Pretraining\nCOCO-LM also employs an auxiliary Transformer to construct the corrupted text sequence, as in\nEqn. (1), but it introduces two new pretraining tasks upon the corrupted sequences to address the\nchallenges previously described. In the rest of this section, we present these two tasks and then the\ndetailed conﬁgurations of COCO-LM. Its framework is illustrated in Figure 2.\nCorrective Language Modeling (CLM) trains the main Transformer to recover the original tokens,\ngiven the corrupted text sequence XMLM:\nXMLM Main Transformer\n−−−−−−−−−→H\nCLM Head\n− −−−−− →pCLM(x|hi).\nThe CLM Head uses the hidden representations H to output a language modeling probability, instead\nof a binary classiﬁcation score. The forward pass of the CLM Head is the same as All-Token MLM,\na variation of ELECTRA [7] that consists of a language modeling layer and a binary classiﬁcation\nlayer for the copy mechanism:\npLM(xi|hi) =1\n(\nxi = xMLM\ni\n)\npcopy(1|hi) +pcopy(0|hi) exp(x⊤\ni hi)∑\nxt∈V exp(x⊤\nt hi),\npcopy(yi|hi) = exp(yi ·w⊤\ncopyhi)/\n(\nexp(w⊤\ncopyhi) + 1\n)\n,\nwhere wcopy is a learnable weight and pcopy(yi|hi) is the copy mechanism (yi = 1when the input\ntoken is original and can be directly copied to the output; yi = 0when the input token needs to be\ncorrected to another token from the vocabulary).\nIn ELECTRA, All-Token MLM performs worse than RTD [7]. Language modeling on the corrupted\ntext sequence XMLM is hard as the replaced tokens from the auxiliary model are more deceiving than\n[MASK]. To improve the language model learning, different from All-Token MLM, CLM employs a\n4\nmulti-task setup that combines the RTD task to explicitly train the copy mechanism pcopy(·):\nLcopy = −E\n( n∑\ni=1\n1\n(\nxMLM\ni = xorig\ni\n)\nlog pcopy(1|hi) +1\n(\nxMLM\ni ̸= xorig\ni\n)\nlog pcopy(0|hi)\n)\n, (2)\nLLM = −E\n(∑\ni∈M\nlog pLM\n(\nxorig\ni\n⏐⏐hi\n))\n= −E\n(∑\ni∈M\nlog\n(\n1\n(\nxMLM\ni = xorig\ni\n)\npsg\ncopy(1|hi) +psg\ncopy(0|hi) exp(x⊤\ni hi)∑\nxt∈V exp(x⊤\nt hi)\n))\n,\nLCLM =λcopyLcopy + LLM.\nThe hyperparameter λcopy balances the weights of the two tasks. The binary cross entropy loss in\nEqn. (2) explicitly trains the copy probability. We also use stop gradient (sg) to decouple the gradient\nbackpropagation to pcopy(·) from the LM task. This way, the main Transformer ﬁrst learns the easier\nclassiﬁcation task and then uses it to help learn the harder LM task. The binary classiﬁcation task is\ntrained on all tokens while the language modeling task is trained only on masked positions.\nCLM combines the advantages of MLM and ELECTRA: The main Transformer is trained on all\ntokens with the help of the binary classiﬁcation task while also being able to predict words, thus\nenjoying the efﬁciency beneﬁts of ELECTRA and preserving the language modeling beneﬁts.\nSequence Contrastive Learning (SCL) forms a contrastive learning objective upon the sequence\nembeddings to learn more robust representations. Broadly, contrastive learning is to align a positive\npair of instances, often different views of the same information [ 4, 34], in contrast to unrelated\nnegative instances [22, 60]. The different views are often obtained by applying data augmentations\non the same input, for example, rotation, cropping, and blurring on visual representations [4, 34], so\nthat the neural networks can learn representations robust to these data alterations.\nIn COCO-LM, the corrupted sequence XMLM already provides a form of data augmentation. We\npair it with another augmentation, Xcrop, a randomly cropped contiguous span of Xorig (the length of\nXcrop is 90% of Xorig so that the major sequence meaning is preserved), to construct the positive pair\nand to contrast with random negatives.\nSpeciﬁcally, a training batch Bin SCL includes a random set of corrupted and cropped sequences:\nB = {(XMLM\n1 ,Xcrop\n1 ),..., (XMLM\nN ,Xcrop\nN )}, with XMLM\nk and Xcrop\nk originated from Xorig\nk . A positive\ncontrastive pair (X,X+) consists of either (XMLM\nk ,Xcrop\nk ) or (Xcrop\nk ,XMLM\nk ) (symmetrical contrast).\nThe negative instances are all the remaining sequences in the batch B− = B\\{(X,X+)}. The\ncontrastive loss is formulated as:\nLSCL = −E\n(\nlog exp(cos(s,s+)/τ)\nexp(cos(s,s+)/τ) +∑\nX−∈B− exp(cos(s,s−)/τ)\n)\n,\n= −E\n(\ncos(s,s+)/τ −log\n(\nexp(cos(s,s+)/τ) +\n∑\nX−∈B−\nexp\n(\ncos(s,s−)/τ\n)\n))\n, (3)\nwhere s,s+,s− are the representations of X,X+,X−, respectively, from the main Transformer\n(i.e., h[CLS]). The similarity metric is cosine similarity (cos) and the temperature τ is set to 1.\nAs shown in Wang et al. [ 55], the ﬁrst term in Eqn. (3) (cos(s,s+)) improves alignment of the\nspace. It encourages representations to be robust to the corruptions and the alterations on the original\ntext. The second term in Eqn. (3) promotes uniformity. It pushes unrelated sequences apart in the\nrepresentation space and ensures low cosine similarity between random data points. Several studies\nhave observed improved generalization ability from better alignment and uniformity [16, 37, 55].\nAligning XMLM with Xcrop requires the main Transformer to produce sequence representations robust\nto both token-level (i.e., MLM replacements) and sequence-level (i.e., cropping) alterations. The\nmodel is thus encouraged to reason more using partially altered sequences to recover the original\ninformation.\nOverall Training. COCO-LM uses the following loss function:\nLCOCO-LM = LAux.\nMLM + LMain\nCLM + LMain\nSCL . (4)\n5\nThe auxiliary Transformer is pretrained by masked language modeling (MLM) and generates cor-\nrupted sequences. The main Transformer is pretrained to correct the corruption (CLM) and to contrast\nthe corrupted sequences with the cropped sequences (SCL). The two Transformers are pretrained\njointly with the loss in Eqn. (4). The main Transformer is used in downstream applications.\nNetwork Conﬁgurations. Similar to ELECTRA, the auxiliary Transformer is smaller than the main\nmodel, but we use different conﬁgurations in the auxiliary model: (1) We reduce the number of\nlayers to 1/3 or 1/4 (under base or large model setup, respectively) but keep its hidden dimension\nthe same with the main model, instead of shrinking its hidden dimensions; (2) We disable dropout in\nit when sampling replacement tokens. We ﬁnd such conﬁgurations empirically more effective and\nuse them as the backbone of COCO-LM. The main Transformer follows the standard architecture of\nBERT/ELECTRA and can be easily adopted by downstream application pipelines with almost no\nchanges.\n4 Experimental Setup\nPretraining Settings. We employ three standard settings, base, base++, and large++. Base is the\nBERTBase training conﬁguration [11]: Pretraining on Wikipedia and BookCorpus [ 63] (16 GB of\ntexts) for 256 million samples on 512 token sequences (125K batches with 2048 batch size). We use\nthe same corpus and 32,768 uncased BPE vocabulary [47] as with TUPE [26].\nBase++ trains the base size model with larger corpora and/or more training steps. Following recent\nresearch [1, 31, 62], we add in OpenWebText [18], CC-News [31], and STORIES [52], to a total of\n160 GB texts, and train for 4 billion (with 2048 batch size) samples [31]. We follow the prepossessing\nof UniLMV2 [1] and use 64,000 cased BPE vocabulary.\nLarge++ uses the same training corpora as base++ and pretrains for 4 billion samples (2048 batch\nsize). Its Transformer conﬁguration is the same with BERTLarge [11].\nModel Architecture. Our base/base++ model uses the BERTBase architecture [11]: 12 layer Trans-\nformer, 768 hidden size, plus T5 relative position encoding [40]. Our large++ model is the same\nwith BERTLarge, 24 layer and 1024 hidden size, plus T5 relative position encoding [40]. Our auxiliary\nnetwork uses the same hidden size but a shallow 4-layer Transformer in base/base++ and a 6-layer\none in large++. When generating XMLM we disable dropout in the auxiliary model.\nDownstream Tasks. We use the tasks included in GLUE [ 54] and SQuAD 2.0 reading compres-\nsion [41]. Please refer to Appendix A for more details about GLUE tasks. Standard hyperparameter\nsearch in ﬁne-tuning is performed, and the search space can be found in Appendix B. The ﬁne-tuning\nprotocols use the open-source implementation of TUPE [26]. The reported results are the median of\nﬁve random seeds on GLUE and SQuAD.\nBaselines. We compare with various pretrained models in each setting. To reduce the variance in\ndata processing/environments, we also pretrain and ﬁne-tune RoBERTa and ELECTRA under exactly\nthe same setting with COCO-LM, marked with “(Ours)”. All numbers unless marked by “(Ours)” are\nfrom reported results in recent research (more details in Appendix C).\nImplementation Details. Our implementation builds upon the open-source implementation from\nMC-BERT [61] and fairseq [35]. More implementation details are mentioned in Appendix D.\n5 Evaluation Results\nThree groups of experiments are conducted to evaluate COCO-LM and its two new pretraining tasks.\n5.1 Overall Results and Ablations\nOverall Results are listed in Table 1. Under all three settings, COCO-LM outperforms all recent\nstate-of-the-art pretraining models on GLUE average and SQuAD. It improves the state-of-the-art\nGLUE score by about one point under all three settings. COCO-LM also enjoys better parameter\nefﬁciency. Using less than 10% of Megatron’s parameters, COCO-LMLarge++ matches the MNLI\naccuracy of Megatron3.9B, one of the largest pretrained BERT-style encoders.\n6\nModel Params GLUE Single Task SQuAD 2.0\nMNLI-(m/mm) QQP QNLI SST-2 CoLA RTE MRPC STS-B A VG EM F1\nBase Setting:BERT Base Size, Wikipedia + Book Corpus (16GB)\nBERT [11] 110M 84.5/- 91.3 91.7 93.2 58.9 68.6 87.3 89.5 83.1 73.7 76.3\nRoBERTa [31] 125M 84.7/- – – 92.7 – – – – – – 79.7\nXLNet [62] 110M 85.8/85.4 – – 92.7 – – – – – 78.5 81.3\nELECTRA [7] 110M 86.0/85.3 90.0 91.9 93.4 64.3 70.8 84.9 89.1 83.7 80.5 83.3\nMC-BERT [61] 110M 85.7/85.2 89.7 91.3 92.3 62.1 75.0 86.0 88.0 83.7 – –\nDeBERTa [23] 134M 86.3/86.2 – – – – – – – – 79.3 82.5\nTUPE [26] 110M 86.2/86.2 91.3 92.2 93.3 63.6 73.6 89.9 89.2 84.9 – –\nRoBERTa (Ours) 110M 85.8/85.5 91.3 92.0 93.7 60.1 68.2 87.3 88.5 83.3 77.7 80.5\nELECTRA (Ours) 110M 86.9/86.7 91.9 92.6 93.6 66.2 75.1 88.2 89.7 85.5 79.7 82.6\nCOCO-LM 110M 88.5/88.3 92.0 93.1 93.2 63.9 84.8 91.4 90.3 87.2 82.4 85.2\nBase++ Setting:BERT Base Size, Bigger Training Data, and/or More Training Steps\nXLNet [62] 110M 86.8/- 91.4 91.7 94.7 60.2 74.0 88.2 89.5 84.6 80.2 –\nRoBERTa [31] 125M 87.6/- 91.9 92.8 94.8 63.6 78.7 90.2 91.2 86.4 80.5 83.7\nUniLM V2 [1] 110M 88.5/- 91.7 93.5 95.1 65.2 81.3 91.8 91.0 87.1 83.3 86.1\nDeBERTa [23] 134M 88.8/88.5 – – – – – – – – 83.1 86.2\nCLEAR [59] 110M 86.7/- 90.0 92.9 94.5 64.3 78.3 89.2 89.8 85.7 – –\nCOCO-LM 134M 90.2/90.0 92.2 94.2 94.6 67.3 87.4 91.2 91.8 88.6 85.4 88.1\nLarge++ Setting:BERT Large Size, Bigger Training Data, and More Training Steps\nXLNet [62] 360M 90.8/90.8 92.3 94.9 97.0 69.0 85.9 90.8 92.5 89.2 87.9 90.6\nRoBERTa [31] 356M 90.2/90.2 92.2 94.7 96.4 68.0 86.6 90.9 92.4 88.9 86.5 89.4\nELECTRA [7] 335M 90.9/- 92.4 95.0 96.9 69.1 88.0 90.8 92.6 89.4 88.0 90.6\nDeBERTa [23] 384M 91.1/91.1 92.3 95.3 96.8 70.5 – – – – 88.0 90.7\nCOCO-LM 367M 91.4/91.6 92.8 95.7 96.9 73.9 91.0 92.2 92.7 90.8 88.2 91.0\nMegatron1.3B[49] 1.3B 90.9/91.0 92.6 – – – – – – – 87.1 90.2\nMegatron3.9B[49] 3.9B 91.4/91.4 92.7 – – – – – – – 88.5 91.2\nTable 1: Results on GLUE and SQuAD 2.0 development set. All results are single-task, single-model\nﬁne-tuning. Results not available in public reports are marked as “–”. DeBERTa reported RTE,\nMRPC and STS-B results by ﬁne-tuning from MNLI checkpoints which are not single-task results.\nWe use Spearman correlation for STS, Matthews correlation for CoLA, and accuracy for the rest on\nGLUE. A VG is the average of the eight tasks on GLUE. All baseline results unless marked by (Ours)\nare reported by previous research.\nModel Params MNLI-(m/mm) QQP QNLI SST-2 CoLA RTE MRPC STS-B A VG\nBase/Base++ Setting:BERT Base Size\nBERTBase 110M 84.6/83.4 89.2 90.5 93.5 52.1 66.4 84.8 85.8 80.8\nELECTRABase++ 110M 88.5/88.0 89.5 93.1 96.0 64.6 75.2 88.1 90.2 85.6\nCOCO-LMBase++ 134M 89.8/89.3 89.8 94.2 95.6 68.6 82.3 88.5 90.3 87.4\nLarge/Large++ Setting:BERT Large Size\nBERTLarge 335M 86.7/85.9 89.3 92.7 94.9 60.5 70.1 85.4 86.5 83.2\nELECTRALarge++ 335M 90.7/90.2 90.4 95.5 96.7 68.1 86.1 89.2 91.7 88.5\nCOCO-LMLarge++ 367M 91.6/91.1 90.5 95.8 96.7 70.5 89.2 88.4 91.8 89.3\nTable 2: GLUE test set results obtained from the GLUE leaderboard. We perform hyperparameter\nsearch for each task with ten random seeds and use the best development set model for test predictions.\nAll results are from vanilla single-task ﬁne-tuning (no ensemble, task-speciﬁc tricks, etc.).\nTable 2 shows GLUE test set results which further conﬁrm the advantages of COCO-LM over\nprevious methods.\nEfﬁciency. In downstream tasks, the efﬁciency of COCO-LM is the same with BERT. In pretraining,\nthe auxiliary model and SCL introduce extra cost. However, as shown in Figure 3, COCO-LM is\nmore efﬁcient in GPU hours. It outperforms RoBERTa & ELECTRA by 1+ points on MNLI with\nthe same GPU hours and reaches their accuracy with around 60% & 50% GPU hours, respectively.\nAblation Studies. Table 3 shows the ablations of COCO-LM under the base setting on GLUE DEV .\nPretraining Task. With only RTD, our backbone model with the shallow auxiliary Transformer is\nquite effective. CLM and SCL both provide additional improvements on MNLI and GLUE average.\nTheir advantages are better observed on different tasks, for example, CLM on MNLI-mm and SCL\non RTE and MRPC. Combining the two in COCO-LM provides better overall effectiveness. In later\nexperiments, we further analyze the beneﬁts of these two tasks.\n7\nGroup Method MNLI-(m/mm) QQP QNLI SST-2 CoLA RTE MRPC STS-B A VG\nCOCO-LMBase 88.5/88.3 92.0 93.1 93.2 63.9 84.8 91.4 90.3 87.2\nPretrainingRTD Only 88.4/88.2 92.1 93.5 92.7 67.3 80.5 89.0 90.9 86.8\nTask CLM Only 88.6/88.4 92.0 93.2 93.7 67.4 80.1 90.0 90.4 86.9\nSCL + RTD 88.6/88.2 92.1 93.5 93.8 64.3 82.7 90.2 90.6 86.9\nNetwork w/o. Rel-Pos 88.2/87.7 92.2 93.4 93.7 68.8 82.7 91.2 90.6 87.6\nSetting w. ELECTRA’s Auxiliary 88.0/87.7 91.9 92.7 93.5 64.3 81.2 89.5 89.7 86.3\nTraining w. Random Replacements 84.9/84.7 91.4 91.1 91.4 41.6 70.0 87.3 87.1 80.6\nSignal w. Converged Auxiliary 88.3/88.1 92.0 92.8 94.3 64.2 78.3 90.4 90.2 86.3\nCLM All-Token LM Only 87.2/87.0 91.8 92.6 93.7 60.6 74.0 88.5 89.7 84.7\nSetup CLM w/o. Copy 88.0/87.9 91.8 93.1 94.4 66.6 76.9 89.5 90.1 86.3\nCLM w/o. Stop-grad 88.5/88.2 92.0 92.9 94.3 66.5 80.9 90.0 90.6 86.9\nTable 3: Ablations on GLUE Dev. that eliminate (w/o.), keep (Only) or switch (w.) one component.\n0 10 20 30 40\nTraining Time (Hours)\n80.0\n82.5\n85.0\n87.5\nDev. Set Acc.\nCOCO-LM\nRoBERTa\nELECTRA\n(a) MNLI-m\n0 10 20 30 40\nTraining Time (Hours)\n80.0\n82.5\n85.0\n87.5\nDev. Set Acc.\nCOCO-LM\nRoBERTa\nELECTRA (b) MNLI-mm\nFigure 3: COCO-LM Base on MNLI Dev. ( y-axes) at differ-\nent pretraining hours on four DGX-2 nodes ( 64 V100 GPUs).\nThe ﬁnal training hours and accuracy of RoBERTa (Ours) and\nELECTRA (Ours) measured in the same settings are marked.\n70% 90% 100%\nXcrop length (w.r.t. Xorig)\n86.0\n86.5\n87.0\n87.5\nGLUE AVG\nFigure 4: The performance of\nCOCO-LMBase when pretrained\nwith different crop fractions. The\nx-axis is the fraction of Xorig be-\ning kept (no cropping is 100%).\nArchitecture. Removing relative position encoding (Rel-Pos) leads to better numbers on some tasks\nbut signiﬁcantly hurts MNLI. Using a shallow auxiliary network and keeping the same hidden\ndimension (768) is more effective than ELECTRA’s12-layer but 256-hidden dimension generator.\nPretraining Signal Construction. Using randomly replaced tokens to corrupt text sequence hurts\nsigniﬁcantly. Using a converged auxiliary network to pretrain the main model also hurts. It is better\nto pretrain the two Transformers together, as the auxiliary model gradually increases the difﬁculty of\nthe corrupted sequences and provides a natural learning curriculum for the main Transformer.\nCLM Setup. Disabling the multi-task learning and using All-Token MLM [7] reduces model accuracy.\nThe copy mechanism is effective. The beneﬁts of the stop gradient operation are more on stability\n(preventing training divergence).\n5.2 Analyses of Contrastive Learning with SCL\nThis group of experiments analyzes the behavior of SCL. All experiments use the base setting.\nAblation on Data Augmentation. Figure 4 shows the effects of the cropping operation when\nforming positive SCL pairs with the corrupted sequence. Using the original sequence results in worse\nGLUE accuracy. It is less informative as the model no longer needs to learn representations robust to\nsequence-level alteration. Cropping too much (e.g., only keeping 70% of the original sequence), may\nhurt as it can alter the semantics too much. Empirically a simple alteration works the best, similar to\nthe observations in recent research [4, 16, 22].\nAlignment and Uniformity. Figure 5 plots the distribution of cosine similarities between random\nsequence pairs and similar ones using representations pretrained by COCO-LM. The representation\nspace from COCO-LM is drastically different from those in Figure 1. With COCO-LM, similar pairs\nare more aligned and random pairs are distributed more uniformly. Many similar pairs have near 1\ncosine similarity and are clearly separated from random pairs which center around 0. The t-SNE [9]\nplot in Figure 6 further demonstrates the beneﬁts of SCL. The similar sentence pairs (marked by same\nshapes) are aligned closer when pretrained with SCL. Their average cosine similarity is 0.925 when\n8\n1.0−1.0   −0.5        0.0         0.5\nCosine Similarity\n0\n5\n10Estimated Density\nrandom\nsimilar\nFigure 5: Cosine similarity of se-\nquence pairs randomly sampled\nfrom pretraining corpus and most\nsimilar pairs from STS-B using\n[CLS] from COCO-LMBase.\n−50 0 50\n−30\n0\n30\n(a) Without SCL\n−50 0 50\n−30\n0\n30 (b) With SCL\nFigure 6: The t-SNE of sequence representations learned with or\nwithout SCL. The points are sampled from the most semantically\nsimilar sentences pairs from STS-B (with 5-score labels). The\n[CLS] embeddings are not ﬁne-tuned. Some randomly selected\nsimilar pairs are marked by same shapes.\n0.0 0.5 1.0 1.5 2.0\nTraining Steps (×1e4)\n0.0\n0.5\n1.0Cosine Sim.\nPositive\nNegative\n(a) Without SCL\n0.0 0.5 1.0 1.5 2.0\nTraining Steps (×1e4)\n0.0\n0.5\n1.0Cosine Sim.\nPositive\nNegative (b) With SCL\n1 5 10 20\nTraining Set Size (%)\n82\n84\n86Dev. Set Acc.\nw/o. SCL\nw. SCL (c) MNLI-m\n1 5 10 20\nTraining Set Size (%)\n82\n84\n86Dev. Set Acc.\nw/o. SCL\nw. SCL (d) MNLI-mm\nFigure 7: Analyses of SCL. Figs. (a) and (b) show the average cosine similarity between the [CLS]\nembeddings of positive and negative contrastive pairs during pretraining. Figs. (c) and (d) show the\nfew-shot accuracy on MNLI with different fractions of MNLI training set used (x-axes). The error\nbars mark the max/min and the solid lines are the average of ﬁve ﬁne-tuning runs.\npretrained with SCL, while is 0.863 without SCL. This better alignment and uniformity is achieved\nby COCO-LM with SCL via pretraining, without using task-speciﬁc data nor supervised labels.\nRegularizing the Representation Learning for Better Few-Shot Ability. One would expect any\npretrained Transformers to easily align a pair of corrupted sequence and cropped sequence as the\ntwo share about 80% tokens. However, as shown in Figure 7a, that is not the case: Without SCL,\nthe cosine similarity of the positive pairs is even lower than random negatives. SCL is necessary to\nregularize the representation space and to reduce the risk of degeneration (Figure 7b).\nSimilar to empirical observations and theoretical analyses in recent research [ 14, 16, 55], a more\nregularized representation space results in better generalization ability in scenarios with limited labels.\nFigure 7c and 7d show the results when COCO-LM are trained (via standard ﬁne-tuning) with only\na fraction of MNLI labels. The improvements brought by SCL are more signiﬁcant when fewer\nﬁne-tuning labels are available. With 1% MNLI labels, pretraining with SCL improves MNLI-m/mm\naccuracy by 0.8/0.5 compared to that without SCL. Using only 10%/20% labels, COCO-LM with\nSCL reaches similar MNLI accuracy with RoBERTa (Ours)/ELECTRA (Ours) ﬁne-tuned with all\nlabels, respectively.\n5.3 Analyses of Language Modeling with CLM\nThe last group of experiments studies the effectiveness and beneﬁts of CLM.\nAblations on Training Conﬁgurations. Figure 8 illustrates pretraining process with CLM and\nAll-Token MLM. The plots demonstrate the difﬁculty of language modeling upon corrupted text\nsequences. It is quite an unbalanced task. For the majority of the tokens (Original) the task is\nsimply to copy its input at the same position. For the replaced tokens ( 7 −8% total), however,\nthe model needs to detect the abnormality brought by the auxiliary model and recover the original\ntoken. Implicitly training the copy mechanism as part of the hard LM task is not effective: The copy\naccuracy of All-Token MLM is much lower, and thus the LM head may confuse original tokens with\nreplaced ones. As shown in Table 3 and ELECTRA [7], pretraining with All-Token MLM performs\nworse than using the RTD task, though the latter is equivalent to only training the copy mechanism.\nThe multi-task learning of CLM is necessary for the main Transformer to stably learn the language\nmodeling task upon the corrupted text sequence.\n9\n0 5 10      \nTraining Steps (×1e4)\n0.20\n0.25\n0.30Copy Acc. All-Token MLM\nCLM\n(a) Copy Acc. (Replaced)\n0 5 10 \nTraining Steps (×1e4)\n0.989\n0.992\n0.995Copy Acc. (b) Copy Acc. (Original)\n0 5 10         \nTraining Steps (×1e4)\n0.02\n0.04\n0.06CLM Acc. (c) CLM Acc. (Replaced)\n0 5 10           \nTraining Steps (×1e4)\n0.998\n0.999\n1.000CLM Acc. (d) CLM Acc. (Original)\nFigure 8: The copying accuracy and the language modeling accuracy (y-axes) of CLM and All-Token\nMLM at different pretraining steps (x-axes, in 10K scale). The accuracy is averaged on tokens that\nare replaced by the auxiliary Transformer (Replaced) or those from the original input text (Original).\nModel MNLI-m MNLI-mm\nRoBERTaBase++ 60.1 (1.5) 61.8 (1.2)\nCOCO-LMBase++ 66.5 (2.1) 68.0 (2.3)\nRoBERTaLarge++ 70.7 (1.3) 72.0 (1.2)\nCOCO-LMLarge++ 72.0 (1.5) 73.3 (1.1)\nTable 4: Few-shot prompt-based ﬁne-tuning\nusing RoBERTa and COCO-LM trained on\n16 samples per class. Mean (and standard\ndeviation) accuracy results over 5 different\nsplits on MNLI-m/mm are shown.\nPrompt-Based Fine-Tuning with CLM.Table 4 in-\ncludes the prompt-based ﬁne-tuning experiments on\nMNLI for RoBERTa and COCO-LM underbase++\nand large++ sizes, following the same few-shot man-\nual prompt ﬁne-tuning with demonstration setup in\nLM-BFF [15]. We use {3e−6,4e−6,5e−6}for the\nlearning rate search of COCO-LM base++/large++\nmodel, with everything else kept same as described\nin LM-BFF. With exactly the same pipeline, COCO-\nLM outperforms RoBERTa under bothbase++ and\nlarge++ sizes by signiﬁcant margins on MNLI-\nm/mm. Such observations are interesting as COCO-\nLM’s main Transformer does not even see any\n[MASK] tokens during pretraining but still performs well on predicting masked tokens for prompt-\nbased learning. Note that ELECTRA and COCO-LM variants without the CLM task are not appli-\ncable: Their main Transformers are not pretrained by language modeling tasks (thus no language\nmodeling capability is learned to generate prompt label words). This points out the importance, if not\nnecessity, of COCO-LM in the family of ELECTRA-style pretraining models. With the beneﬁts and\nrapid developments of prompt-based approaches, the lack of language modeling capability is going to\nlimit the potential of ELECTRA’s self-supervised learning framework in many real-world scenarios.\nCOCO-LM not only addresses this limitation but also provides better prompt-based learning results.\n6 Conclusions and Future Work\nIn this paper, we present COCO-LM, which pretrains language models using Corrective Language\nModeling and Sequence Contrastive Learning upon corrupted text sequences. With standard pre-\ntraining data and Transformer architectures, COCO-LM improves the accuracy on the GLUE and\nSQuAD benchmarks, while also being more efﬁcient in utilizing pretraining computing resources\nand network parameters.\nOne limitation of this work is that the contrastive pairs are constructed by simple cropping and MLM\nreplacements. Recent studies have shown the effectiveness of advanced data augmentation techniques\nin ﬁne-tuning language models [16, 38, 51]. A future research direction is to explore better ways to\nconstruct contrastive pairs in language model pretraining.\nDespite the empirical advantage of this auxiliary-main dual model framework, the auxiliary Trans-\nformer training is not inﬂuenced by the main Transformer nor learns to generate the optimal pretrain-\ning signals for the main model. To better understand and tailor the training of the auxiliary model to\nthe main model is another important future research direction.\nAcknowledgments\nWe sincerely thank Guolin Ke for discussions and advice on model implementation. We also thank\nanonymous reviewers for valuable and insightful feedback, especially the suggestion of adding\nprompt-based ﬁne-tuning experiments.\n10\nReferences\n[1] Hangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan Yang, Xiaodong Liu, Yu Wang, Songhao Piao,\nJianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. UniLMv2: Pseudo-masked language models for uniﬁed\nlanguage model pre-training. In ICML, 2020.\n[2] Luisa Bentivogli, Peter Clark, Ido Dagan, and Danilo Giampiccolo. The ﬁfth pascal recognizing textual\nentailment challenge. In TAC, 2009.\n[3] Daniel Cer, Mona Diab, Eneko Agirre, Iñigo Lopez-Gazpio, and Lucia Specia. Semeval-2017 task 1:\nSemantic textual similarity multilingual and crosslingual focused evaluation. In International Workshop on\nSemantic Evaluation (SemEval), 2017.\n[4] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In ICML, 2020.\n[5] Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. What does BERT look at?\nan analysis of BERT’s attention. In ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, 2019.\n[6] Kevin Clark, Minh-Thang Luong, Quoc Le, and Christopher D Manning. Pre-training transformers as\nenergy-based cloze models. In EMNLP, 2020.\n[7] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. ELECTRA: Pre-training text\nencoders as discriminators rather than generators. In ICLR, 2020.\n[8] Peter Clark, Oyvind Tafjord, and Kyle Richardson. Transformers as soft reasoners over language. InIJCAI,\n2020.\n[9] Andy Coenen, Emily Reif, Ann Yuan, Been Kim, Adam Pearce, Fernanda Viégas, and Martin Wattenberg.\nVisualizing and measuring the geometry of BERT. In NeurIPS, 2019.\n[10] Ido Dagan, Oren Glickman, and Bernardo Magnini. The pascal recognising textual entailment challenge.\nIn Machine Learning Challenges Workshop, 2005.\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL-HLT, 2019.\n[12] William B Dolan and Chris Brockett. Automatically constructing a corpus of sentential paraphrases. In\nInternational Workshop on Paraphrasing (IWP), 2005.\n[13] Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and\nHsiao-Wuen Hon. Uniﬁed language model pre-training for natural language understanding and generation.\nIn NeurIPS, 2019.\n[14] Jun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-Yan Liu. Representation degeneration problem in\ntraining natural language generation models. In ICLR, 2019.\n[15] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot learners.\nIn ACL, 2021.\n[16] Tianyu Gao, Xingcheng Yao, and Danqi Chen. SimCSE: Simple contrastive learning of sentence embed-\ndings. In EMNLP, 2021.\n[17] Danilo Giampiccolo, Bernardo Magnini, Ido Dagan, and Bill Dolan. The third pascal recognizing textual\nentailment challenge. In ACL-PASCAL workshop on textual entailment and paraphrasing, 2007.\n[18] Aaron Gokaslan and Vanya Cohen. Openwebtext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus, 2019.\n[19] Beliz Gunel, Jingfei Du, Alexis Conneau, and Ves Stoyanov. Supervised contrastive learning for pre-trained\nlanguage model ﬁne-tuning. In ICLR, 2021.\n[20] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: Retrieval-\naugmented language model pre-training. In ICML, 2020.\n[21] R Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo Giampiccolo, Bernardo Magnini, and Idan\nSzpektor. The second pascal recognising textual entailment challenge. In PASCAL Challenges Workshop\non Recognising Textual Entailment, 2006.\n11\n[22] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised\nvisual representation learning. In CVPR, 2020.\n[23] Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. DeBERTa: Decoding-enhanced bert with\ndisentangled attention. In ICLR, 2021.\n[24] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld, Luke Zettlemoyer, and Omer Levy. SpanBERT:\nImproving pre-training by representing and predicting spans. In TACL, 2019.\n[25] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint\narXiv:2001.08361, 2020.\n[26] Guolin Ke, Di He, and Tie-Yan Liu. Rethinking the positional encoding in language pre-training. In ICLR,\n2021.\n[27] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut.\nALBERT: A lite BERT for self-supervised learning of language representations. In ICLR, 2020.\n[28] Teven Le Scao and Alexander M Rush. How many data points is a prompt worth? In NAACL-HLT, 2021.\n[29] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. BART: Denoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension. In ACL, 2019.\n[30] Bohan Li, Hao Zhou, Junxian He, Mingxuan Wang, Yiming Yang, and Lei Li. On the sentence embeddings\nfrom pre-trained language models. In EMNLP, 2020.\n[31] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach.\narXiv preprint arXiv:1907.11692, 2019.\n[32] Yi Luan, Jacob Eisenstein, Kristina Toutanove, and Michael Collins. Sparse, dense, and attentional\nrepresentations for text retrieval. In TACL, 2021.\n[33] Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong, Heng Ji, Chao Zhang, and Jiawei Han. Text\nclassiﬁcation using label names only: A language model self-training approach. In EMNLP, 2020.\n[34] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\ncoding. arXiv preprint arXiv:1807.03748, 2018.\n[35] Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. FAIRSEQ: A fast, extensible toolkit for sequence modeling. InNAACL-HLT Demonstrations,\n2019.\n[36] Matthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. Deep contextualized word representations. In NAACL-HLT, 2018.\n[37] Senthil Purushwalkam and Abhinav Gupta. Demystifying contrastive self-supervised learning: Invariances,\naugmentations and dataset biases. In NeurIPS, 2020.\n[38] Yanru Qu, Dinghan Shen, Yelong Shen, Sandra Sajeev, Jiawei Han, and Weizhu Chen. CoDA: Contrast-\nenhanced and diversity-promoting data augmentation for natural language understanding. In ICLR, 2021.\n[39] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\n[40] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.\nJournal of Machine Learning Research, 2019.\n[41] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. SQuAD: 100,000+ questions for\nmachine comprehension of text. In EMNLP, 2016.\n[42] Anirudh Ravula, Chris Alberti, Joshua Ainslie, Li Yang, Philip Minh Pham, Qifan Wang, Santiago Ontanon,\nSumit Kumar Sanghai, Vaclav Cvicek, and Zach Fisher. ETC: Encoding long and structured inputs in\ntransformers. In EMNLP, 2020.\n[43] Nils Reimers and Iryna Gurevych. Sentence-BERT: Sentence embeddings using siamese BERT-networks.\nIn EMNLP, 2019.\n12\n[44] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters\nof a language model? In EMNLP, 2020.\n[45] Timo Schick and Hinrich Schütze. Exploiting cloze questions for few-shot text classiﬁcation and natural\nlanguage inference. In EACL, 2021.\n[46] Timo Schick and Hinrich Schütze. It’s not just size that matters: Small language models are also few-shot\nlearners. In NAACL-HLT, 2021.\n[47] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with\nsubword units. In ACL, 2015.\n[48] Iyer Shankar, Dandekar Nikhil, and Csernai Kornél. First Quora dataset release: Question pairs, 2017.\n[49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro.\nMegatron-LM: Training multi-billion parameter language models using gpu model parallelism. arXiv\npreprint arXiv:1909.08053, 2019.\n[50] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and\nChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank. In\nEMNLP, 2013.\n[51] Nandan Thakur, Nils Reimers, Johannes Daxenberger, and Iryna Gurevych. Augmented SBERT: Data\naugmentation method for improving bi-encoders for pairwise sentence scoring tasks. In NAACL-HLT,\n2021.\n[52] Trieu H Trinh and Quoc V Le. A simple method for commonsense reasoning. arXiv preprint\narXiv:1806.02847, 2018.\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\n[54] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. GLUE: A\nmulti-task benchmark and analysis platform for natural language understanding. In EMNLP Workshop\nBlackboxNLP, 2018.\n[55] Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment\nand uniformity on the hypersphere. In ICML, 2020.\n[56] Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, and Luo Si. StructBERT:\nIncorporating language structures into pre-training for deep language understanding. In ICLR, 2020.\n[57] Alex Warstadt, Amanpreet Singh, and Samuel R Bowman. Neural network acceptability judgments. In\nTACL, 2019.\n[58] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence\nunderstanding through inference. In NAACL-HLT, 2018.\n[59] Zhuofeng Wu, Sinong Wang, Jiatao Gu, Madian Khabsa, Fei Sun, and Hao Ma. CLEAR: Contrastive\nlearning for sentence representation. arXiv preprint arXiv:2012.15466, 2020.\n[60] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett, Junaid Ahmed, and Arnold\nOverwijk. Approximate nearest neighbor negative contrastive learning for dense text retrieval. In ICLR,\n2021.\n[61] Zhenhui Xu, Linyuan Gong, Guolin Ke, Di He, Shuxin Zheng, Liwei Wang, Jiang Bian, and Tie-Yan Liu.\nMC-BERT: Efﬁcient language pre-training via a meta controller. arXiv preprint arXiv:2006.05744, 2020.\n[62] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. XLNet:\nGeneralized autoregressive pretraining for language understanding. In NeurIPS, 2019.\n[63] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading\nbooks. In ICCV, 2015.\n13\nA GLUE Tasks\nWe provide more details of the tasks included in the GLUE benchmark. Their statistics are listed in\nTable 5.\nMNLI: Multi-genre Natural Language Inference [58] contains 393K train examples obtained via\ncrowdsourcing. The task is to predict whether a given premise sentence entails, contradicts or neutral\nwith respect to a given hypothesis sentence.\nQQP: Question Pairs [48] contains 364K train examples from the Quora question-answering website.\nThe task is to determine whether a pair of questions asked are semantically equivalent.\nQNLI: Question Natural Language Inference contains108K train examples derived from the Stanford\nQuestion Answering Dataset (SQuAD) [41]. The task is to predict whether a given sentence contains\nthe answer to a given question sentence.\nSST-2: Stanford Sentiment Treebank [50] contains 67K train examples extracted from movie reviews\nwith human-annotated sentiment scores. The tasks is to determine if the sentence has positive or\nnegative sentiment.\nCoLA: Corpus of Linguistic Acceptability [57] contains 8.5K train examples from books and journal\narticles on linguistic theory. The task is to determine whether a given sentence is linguistically\nacceptable or not.\nRTE: Recognizing Textual Entailment [2, 10, 21, 17] contains 2.5K train examples from textual\nentailment challenges. The task is to predict whether a given premise sentence entails a given\nhypothesis sentence or not.\nMRPC: Microsoft Research Paraphrase Corpus [12] contains 3.7K train examples from online news\nsources. The task is to predict whether two sentences are semantically equivalent or not.\nSTS-B: Semantic Textual Similarity [3] contains 5.8K train examples drawn from multiple sources\nwith human annotations on sentence pair semantic similarity. The task is to predict how semantically\nsimilar two sentences are on a 1 to 5 scoring scale.\nB Hyperparameter Settings\nTuning hyperparameter of pretraining is often too costly and we keep most hyperparameters as default.\nThe auxiliary MLM pretraining uses the standard 15% [MASK] ratio. The crop transformation in the\nSCL task uses 10% crop ratio, resulting in a sub-sequence that is 90% long of the original sequence.\nThe softmax temperature in the SCL task is 1. All pretraining tasks in COCO-LM have equal weights\nexcept λcopy = 50since the loss of the binary classiﬁcation task is much lower than those of the\nLM tasks, which are over 30,000-way classiﬁcation tasks. All token embeddings (used in the input\nembedding layer and the language modeling head) are shared between the auxiliary Transformer and\nthe main Transformer. The detailed hyperparameters used are listed in Table 6 for pretraining, and\nTables 7 and 8 for GLUE and SQuAD ﬁne-tuning, respectively.\nAll reported methods use exactly the same (or equivalent) set of hyperparameters for pretraining and\nﬁne-tuning for fair comparison. For COCO-LM and all the baselines implemented under our setting,\nall ﬁne-tuning hyperparameters are searched per task; the median results of ﬁve runs with the same\nset of ﬁve different random seeds are reported on GLUE and SQuAD.\nC The Origins of Reported Baseline Scores\nThe baseline results listed in Table 1 are obtained from their original papers except the following:\nBERT from Bao et al. [ 1], RoBERTa base/base++ GLUE from and SQuAD from Bao et al. [ 1],\nELECTRA base/base++ GLUE from Xu et al. [61], XLNet base++ from Bao et al. [1], RoBERTa\nbase++ SQuAD from Bao et al. [ 1]. When multiple papers report different scores for the same\nmethod, we use the highest of them in our comparisons.\n14\nSize Task Metric(s) Domain\nMNLI 393K Inference Accuracy Misc.\nQQP 364K Similarity Accuracy/F1 Social QA\nQNLI 108K QA/Inference Accuracy Wikipedia\nSST-2 67K Sentiment Accuracy Movie Reviews\nCoLA 8.5K Acceptability Matthews corr. Misc.\nRTE 2.5K Inference Accuracy Misc.\nMRPC 3.7K Paraphrase Accuracy/F1 News\nSTS-B 5.7K Similarity Pearson/Spearman. Misc.\nTable 5: The list of tasks in GLUE, their training data size, language tasks, evaluation metrics, and\ndomain of corpus.\nParameters base base++ large++\nMax Steps 125K 1.95M 1.95M\nPeak Learning Rate 5e-4 2e-4 1e-4\nBatch Size 2048 2048 2048\nWarm-Up Steps 10K 10K 10K\nSequence Length 512 512 512\nRelative Position Encoding Buckets 32 64 128\nRelative Position Encoding Max Distance 128 128 256\nAdam ϵ 1e-6 1e-6 1e-6\nAdam (β1, β2) (0.9, 0.98) (0.9, 0.98) (0.9, 0.98)\nClip Norm 2.0 2.0 2.0\nDropout 0.1 0.1 0.1\nWeight Decay 0.01 0.01 0.01\nTable 6: Hyperparameters used in pretraining.\nD More Implementation Details\nPretraining and Fine-tuning Costs. The pretraining cost of COCO-LM’s CLM task is similar to\nELECTRA, which is BERT plus the auxiliary network whose size is 1/3 of the main network. The\naddition of SCL task requires one more forward and backward pass on the cropped sequence Xcrop.\nWith 256 V100 (32 GB Memory), one pretraining run takes about 20 hours in base setting, about\ntwo-three weeks in base++ setting, and about three-four weeks in large++ setting. The ﬁne-tuning\ncosts are the same with BERT plus relative positive encodings as the same Transformer model is\nused.\nMLM Mode for Corrective Language Modeling. When creating the MLM replaced sequence\nXMLM, we ﬁnd it slightly improves the downstream task performance to disable dropout (i.e., set the\nauxiliary MLM in inference mode) for computing the auxiliary network’s output distribution where\nplausible replacing tokens are sampled. We hypothesize that this leads to more stable generation of\nchallenging replaced tokens to be corrected by the main Transformer and thus improves downstream\ntask results.\nProjection Heads. For the auxiliary model trained with MLM, we follow the standard MLM head\nsetup in BERT/RoBERTa that includes a linear layer to project the contextualized embeddings from\nthe encoder to same-dimensional vectors before feeding to the ﬁnal linear layer that outputs the MLM\nprobability. However, we do not include the projection layer for the main model trained with the\nCLM task (i.e., only having the ﬁnal linear layer). We ﬁnd this improves the training stability.\nMasking Special Tokens for Auxiliary Model Training.BERT only masks real tokens (other than\nartiﬁcial symbols like [SEP] and [CLS]) for MLM training, while RoBERTa also masks special\ntokens. We follow the RoBERTa setting which results in slightly improved performance for some\ntasks.\n15\nParameters GLUE Small Tasks Search Space GLUE Large Tasks Search Space\nMax Epochs {2, 3, 5, 10} {2, 3, 5}\nPeak Learning Rate base/base++: {2e-5, 3e-5, 4e-5, 5e-5} base/base++: {1e-5, 2e-5, 3e-5, 4e-5}\nlarge++: {7e-6, 1e-5, 2e-5, 3e-5} large++: {5e-6, 7e-6, 1e-5, 2e-5}\nBatch Size {16, 32} 32\nLearning Rate Decay Linear Linear\nWarm-Up Proportion {6%, 10%} 6%\nSequence Length 512 512\nAdam ϵ 1e-6 1e-6\nAdam (β1, β2) (0.9, 0.98) (0.9, 0.98)\nClip Norm - -\nDropout 0.1 0.1\nWeight Decay 0.01 0.01\nTable 7: Hyperparameter ranges searched for ﬁne-tuning on GLUE. GLUE small tasks include CoLA,\nRTE, MRPC and STS-B. GLUE large tasks include MNLI, QQP, QNLI and SST-2.\nParameters SQuAD Search Space\nMax Epochs {2, 3}\nPeak Learning Rate base/base++: {2e-5, 3e-5, 4e-5, 5e-5}\nlarge++: {7e-6, 1e-5, 2e-5, 3e-5}\nBatch Size {16, 32}\nLearning Rate Decay Linear\nWarm-Up Proportion {6%, 10%}\nSequence Length 512\nAdam ϵ 1e-6\nAdam (β1, β2) (0.9, 0.98)\nClip Norm -\nDropout 0.1\nWeight Decay 0.01\nTable 8: Hyperparameter ranges searched for ﬁne-tuning on SQuAD.\nE More Discussions on PLM Research\nCurrently, the biggest challenge with PLM research is perhaps its prohibitive computation cost. On\none hand, PLMs have inﬂuenced a wide range of tasks, and any further technical improvement\nmatters a lot for downstream applications. On the other hand, its expensive computing cost and long\nexperimental cycles pose great challenges for careful and thorough studies of the problem space,\nas any test of new designs comes with a considerable computing cost—pretraining a new language\nmodel can easily consume thousands of dollars, or even millions for extra large models.\nSuch challenges call for more systematic evaluation pipelines that can accurately and reliably judge\nwhether or not a new PLM is really better than previous ones. Currently, the evaluation of PLMs\nlargely relies on GLUE-style benchmark which contains a set of different tasks that are weighed\nequally for PLM evaluations—usually the average performance over these tasks is treated as a ﬁnal\nmeasure for the effectiveness of a PLM. However, we ﬁnd that the small tasks in GLUE have very\nhigh variances which may provide unreliable indications for a PLM’s performance. For example,\non CoLA and RTE, ﬁne-tuning with different random seeds from the same pretrained checkpoint\ncan easily result in a 5-point difference between the best and the worst seed. In contrast, large tasks\nlike MNLI give relatively stable and consistent results for the same model pretrained/ﬁne-tuned with\ndifferent random seeds, and thus serve as better indicators for PLMs’ effectiveness.\nIn this paper, we try to improve the robustness of our observations, for example, by reporting the\ndownstream performance with different training time for future comparisons under limited computing\nbudget, and also by making our code and models publicly available for the reproducibility of our study.\nWe hope our efforts will facilitate more future research to improve the community’s understanding\nand development of this important problem space.\n16"
}