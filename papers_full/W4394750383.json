{
  "title": "Large Language Models, Social Demography, and Hegemony: Comparing Authorship in Human and Synthetic Text",
  "url": "https://openalex.org/W4394750383",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4213536778",
      "name": "AJ Alvero",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2095685919",
      "name": "Jinsook Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4319487058",
      "name": "Alejandra Regla-Vargas",
      "affiliations": [
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A192004035",
      "name": "René F. Kizilcec",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A245171893",
      "name": "Thorsten Joachims",
      "affiliations": [
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2105269874",
      "name": "anthony lising antonio",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3133702157",
    "https://openalex.org/W7005400557",
    "https://openalex.org/W3198409578",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W4238846128",
    "https://openalex.org/W4386302153",
    "https://openalex.org/W4384302919",
    "https://openalex.org/W4385852384",
    "https://openalex.org/W4376643691",
    "https://openalex.org/W4387821331",
    "https://openalex.org/W4312119976",
    "https://openalex.org/W4311894881",
    "https://openalex.org/W4388588313",
    "https://openalex.org/W1995862709",
    "https://openalex.org/W4221055872",
    "https://openalex.org/W4280524865",
    "https://openalex.org/W4286449063",
    "https://openalex.org/W4386651352",
    "https://openalex.org/W2801298410",
    "https://openalex.org/W4319301677",
    "https://openalex.org/W4250094535",
    "https://openalex.org/W3173168772",
    "https://openalex.org/W4385404651",
    "https://openalex.org/W1979287750",
    "https://openalex.org/W2118778378",
    "https://openalex.org/W2048529682",
    "https://openalex.org/W4387617694",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4365601444",
    "https://openalex.org/W4385645249",
    "https://openalex.org/W4361866125",
    "https://openalex.org/W4323043839",
    "https://openalex.org/W4320306010",
    "https://openalex.org/W4387568480",
    "https://openalex.org/W4399528455",
    "https://openalex.org/W4392781833",
    "https://openalex.org/W4287992619",
    "https://openalex.org/W3165482393",
    "https://openalex.org/W4366547384",
    "https://openalex.org/W4312516176",
    "https://openalex.org/W4298876402",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W2767689960",
    "https://openalex.org/W4391215438",
    "https://openalex.org/W4324312860",
    "https://openalex.org/W4298109031",
    "https://openalex.org/W2915177913",
    "https://openalex.org/W4383175701",
    "https://openalex.org/W2918035151",
    "https://openalex.org/W4289538664",
    "https://openalex.org/W4249749121",
    "https://openalex.org/W4385573090",
    "https://openalex.org/W3205313517",
    "https://openalex.org/W4380136083",
    "https://openalex.org/W4388020981",
    "https://openalex.org/W4392428647",
    "https://openalex.org/W4387947233",
    "https://openalex.org/W4385569878",
    "https://openalex.org/W4389072555",
    "https://openalex.org/W4321351832",
    "https://openalex.org/W4321452996",
    "https://openalex.org/W3083073863",
    "https://openalex.org/W4386981810",
    "https://openalex.org/W3034987021"
  ],
  "abstract": "** Final version published open-access in the Journal of Big Data: https://link.springer.com/article/10.1186/s40537-024-00986-7?utm_source=rct_congratemailt&amp;amp;utm_medium=email&amp;amp;utm_campaign=oa_20240927&amp;amp;utm_content=10.1186/s40537-024-00986-7#article-info** Large language models have become popular over a short period of time because they can generate text that resembles human writing across various domains and tasks. The popularity and breadth of use also put this technology in the position to fundamentally reshape how written language is perceived and evaluated. It is also the case that spoken language has long played a role in maintaining power and hegemony in society, especially through ideas of social identity and ``correct'' forms of language. But as human communication becomes even more reliant on text and writing, it is important to understand how these processes might shift and who is more likely to see their writing styles reflected back at them through modern AI. We therefore ask the following question: \\textit{who} does generative AI write like? To answer this, we compare writing style features in over 150,000 college admissions essays submitted to a large public university system and an engineering program at an elite private university with a corpus of over 25,000 essays generated with GPT-3.5 and GPT-4 to the same writing prompts. We find that human-authored essays exhibit more variability across various individual writing style features (e.g., verb usage) than AI-generated essays. Overall, we find that the AI-generated essays are most similar to essays authored by students who are males with higher levels of social privilege. These findings demonstrate critical misalignments between human and AI authorship characteristics, which may affect the evaluation of writing and calls for research on control strategies to improve alignment.",
  "full_text": "Large Language Models, Social Demography, and\nHegemony: Comparing Authorship in Human and\nSynthetic Text\nAJ Alvero1*, Jinsook Lee 2, Alejandra Regla-Vargas 3,\nRen´ e F. Kizilcec4, Thorsten Joachims 5, anthony lising antonio 6*\n1*Center for Data Science for Enterprise and Society, Cornell University.\n2,4-5Department of Information Science, Cornell University.\n3Department of Sociology, University of Pennsylvania.\n5Department of Computer Science, Cornell University.\n6Graduate School of Education, Stanford University.\n*Corresponding author(s). E-mail(s): ajalvero@cornell.edu;\naantonio@stanford.edu;\nContributing authors: jl3369@cornell.edu; aregla@sas.upenn.edu;\nkizilcec@cornell.edu; thorsten.joachims@cornell.edu;\nAbstract\n** Final version published open-access in theJournal of Big Data: https://link.springer.com/\narticle/10.1186/s40537-024-00986-7?utm source=rct congratemailt&utm medium=email&\nutm campaign=oa 20240927&utm content=10.1186/s40537-024-00986-7#article-info **\nLarge language models have become popular over a short period of time because\nthey can generate text that resembles human writing across various domains and\ntasks. The popularity and breadth of use also put this technology in the position\nto fundamentally reshape how written language is perceived and evaluated. It is\nalso the case that spoken language has long played a role in maintaining power\nand hegemony in society, especially through ideas of social identity and “correct”\nforms of language. But as human communication becomes even more reliant on\n1\ntext and writing, it is important to understand how these processes might shift\nand who is more likely to see their writing styles reflected back at them through\nmodern AI. We therefore ask the following question: who does generative AI\nwrite like? To answer this, we compare writing style features in over 150,000\ncollege admissions essays submitted to a large public university system and an\nengineering program at an elite private university with a corpus of over 25,000\nessays generated with GPT-3.5 and GPT-4 to the same writing prompts. We find\nthat human-authored essays exhibit more variability across various individual\nwriting style features (e.g., verb usage) than AI-generated essays. Overall, we\nfind that the AI-generated essays are most similar to essays authored by students\nwho are males with higher levels of social privilege. These findings demonstrate\ncritical misalignments between human and AI authorship characteristics, which\nmay affect the evaluation of writing and calls for research on control strategies\nto improve alignment.\nKeywords: authorship characteristics, large language models, computational social\nscience, linguistic sociology, social demography of text, AI homogenization\n2\n1 Introduction\nIn the 2018 science fiction film Sorry to Bother You, a Black telemarketer in Oak-\nland, California faces a dilemma. When people answer his calls and hear his African\nAmerican Vernacular English (AAVE) inflected voice, they immediately hang up and\nignore his sales pitch. With rising bills, debt, and desperation, he follows the advice of\nan older Black colleague to use a “White voice” (a US English dialect typically asso-\nciated with upper-middle-class White Americans). When using this dubbed “White\nvoice” (exaggerated for comedic effect), people no longer hang up, and in fact, they\nbecome high-paying customers. The film uses this play on spoken language, social\nidentity, and power to highlight the concept of hegemony, defined by Gramsci as the\nsociocultural norms that uphold structures of power and domination (Gramsci, 1992).\nLinguistic hegemony, the focus of this particular scene in the film, operates in similar\nways through social enforcement of particular ways to speak, write, and communicate.\nThough the film is fictional, current technologies can manipulate voices to sound\nlike specific individuals or reduce accents by replacing them with more “normalized”\nspeech (Payne et al, 2024). How these dynamics compare with written language is less\nknown. Biases in natural language processing (NLP) techniques are well documented\n(Field et al, 2021; Koenecke et al, 2020), but applications of sociolinguistic perspectives\non how large language model (LLM) communication styles track with specific social\ndemographics could be instructive in determining whether or not the issues presented\nin the movie with spoken language could emerge with written language (Alvero, 2023).\nEducational systems in the US have a long demonstrated preference for the writing\nand speaking styles of those from higher socioeconomic backgrounds (Bernstein, 1964);\neven if the preference is not explicitly stated, studies have found that writing patterns\nstratified by social class are highly predictive of standardized test scores and final col-\nlege GPA (Pennebaker et al, 2014; Alvero et al, 2021). This raises questions about how\nmodern society’s shift toward increased usage of text-based communication may affect\nthese identity dynamics, something which remains underexplored. With widely and\nglobally popular generative AI technology like ChatGPT that can write human-like\ntext, examining what linguistic styles they adopt could reveal much about underlying\nbiases in AI systems and the contexts in which they emerge.\nThe popularity of LLMs and the platforms they power (like ChatGPT, arguably the\ncurrent most popular LLM application) is largely due to their ability to “write like real\npeople” so convincingly that some have argued that the traditional Turing test has\nbeen inverted (Sejnowski, 2023) (ie. LLMs test the intelligence of humans rather than\nhumans test the intelligence of LLMs). However, the self-evident potential of LLMs\nhas raised critical questions about their biases, tendency to emulate certain political\nand moral stances, and ability to fabricate references when used as a research assistant\n(Alkaissi and McFarlane, 2023; Rozado, 2023). These studies examine specific types of\nresponses to structured sets of questions, whereas linguistic hegemony (as outlined by\nGramsci and others) operates on more fundamental levels, such as word choice reflect-\ning a universally understood “common sense” that does not consider sociolinguistic\nvariation as a naturally occurring social phenomenon but rather something to “fix”\n(Gramsci, 1992). Deviating from these linguistic norms (or at least being perceived as\n3\nlinguistically deviant) can put people at odds with the social order and subject them\nto hegemonic forces and pressures purely through their linguistic styles, tendencies,\nand preferences. Given the role of language in upholding social hegemony, it is vital\nto examine the linguistic styles and identities that LLMs adopt in the language they\ngenerate.\nMost text on the internet was written by people (at least for now, eg. Bohacek and\nFarid (2023)), so LLMs implicitly learn correlations between demographics, contexts,\nand communication styles from training data. If LLMs tend to write more like those\ndominant in the training data (ie. particular social strata) or if they are explicitly\ninstructed by their designers to write in a particular way, this could perpetuate cul-\ntural and linguistic hegemony by homogenizing expression (Boelaert et al, 2024; Lee\net al, in press). Some studies have found evidence of these trends with specific types of\nresponses to survey questions (Tao et al, 2023; Atari et al, 2023; Kim and Lee, 2023;\nZhang et al, 2024) and in terms of the limited range of content they produce for a\ngiven prompt or task (Padmakumar and He, 2023; Hovy et al, 2020). Investigating\nsimilarities and differences between human and AI writing across social groups can\nreveal biases in the demography of LLMs’ training data, which has implications for\nwidespread use of AI tools like ChatGPT in numerous contexts. It may also reveal\nsociolinguistically grounded forms of cultural hegemony if certain groups are over-\nrepresented in influencing LLMs’ writing styles (Woolard, 1985). If LLMs adopt the\nwriting patterns of privileged groups, this could shrink sociolinguistic variation or\ndeflate it artificially in settings where AI is in active use, like educational contexts.\nNew social divisions may emerge between those who write like AI versus those who\ndo not, not unlike well documented sociolinguistic divisons that operate under similar\nparameters. However, these concerns are presently not grounded in scientific evidence\nof sociodemographic patterns in written language. Understanding these dynamics\ncould point to deep social dimensions in broader issues like AI alignment (i.e., ensur-\ning that AI systems behave in line with human values, norms, and intentions). The\nhomogenization of writing and communication through LLMs’ styles could have major\nimplications for culture and communication. Further, linguistic hegemony may evolve\nthrough the popularization of LLMs based on correlations of whose writing is most\nreflective of the text generated by LLMs (Warschauer et al, 2023). As LLM usage\nincreases, comparative analyses between human and AI writing will only become more\nchallenging as people use it more in their daily lives.\nWe start to answer these questions by examining a key social process: college admis-\nsions. College admissions serve as an insightful context since identity is salient in\nshaping how applicants present themselves, particularly in selective admissions where\nholistic review of transcripts, essays, and other applicant information is the norm\n(Bastedo et al, 2018; Stevens, 2009). The personal information and written statements\nprovided by applicants are highly correlated with their social identity and background\ncontext. For example, past studies find strong predictive relationships between essay\ncontent and variables like family income and standardized test scores (Alvero et al,\n2021). Essays also show potential for use in replicating past admissions decisions, given\n4\ntrends toward test-optional and test-blind policies (Lee et al, 2023), and to infer per-\nsonal qualities of the applicants that predict academic success (Lira et al, 2023). In\nthis analysis, we focus on stylistic features of writing as captured by a popular method\nto analyze text: Linguistic Inquiry and Word Count (LIWC). LIWC is a dictionary\nbuilt on the basis of psycholinguistic research on the relationship between written lan-\nguage and psychological characteristics of the author (Pennebaker et al, 2015). Past\nstudies have also found that LIWC features are strong predictors of many different\ndimensions of the authors, such as a college applicant’s eventual grades after enrolling,\nSAT score, and household income (Alvero et al, 2021; Pennebaker et al, 2014). The\nhigh interpretability of the features is also useful: the metrics are based on relative\nand absolute frequencies of specific words and punctuation.\nTo compare the writing style of human-authored and AI-generated texts, we analyze a\ndataset of application essays submitted to the University of California system and to an\nengineering school at a private, selective university in the US. We generate AI-written\nessays using two popular LLMs responding to the same prompts from these two appli-\ncation contexts. We find that AI and human written essays have distinct distributions\nin how frequently they use words associated with individual LIWC features, such as\nverbs and analytical thinking. Moreover, AI-generated essays had notably lower vari-\nation in individual LIWC features than the human-written essays, suggesting a more\nnarrow linguistic style. We then compared these distributions split by applicant char-\nacteristics related to their identity and neighborhood context. We found the linguistic\nstyle of AI-generated essays to be closer to some identity groups, such as applicants\nwith college-educated parents and those living in ZIP Codes with higher socioeconomic\nmeasures, than other students. Gender was prominent in two ways. First, among the\nstyle features where male and female applicants diverged the most, the synthetic text\nwas more aligned with male usage. However, the opposite (female style features more\nsimilar to the AI style features) was more likely to be true when focusing on public\nschool applicants or analyzing multiple features simultaneously.\nFinally, we compared the essays using all of the LIWC features and found that these\ndifferences for individual features show strong social patterning when more informa-\ntion is provided to the model. In these analyses, we find that the AI-genereated essays\nare most similar to students from areas with higher social capital compared to the\naverage applicant. In situations where people use LLMs, the likelihood that users see\nwriting styles similar to their own is somewhat dependent on their demographic infor-\nmation. For those with higher levels of social capital, they are likely to be presented\nwith text they could have plausibly written. Regardless of whether individuals are\naware of these social dynamics, the broad uptake of LLMs along with these patterns\nhave the potential to homogenize written language to make people sound less like\nthemselves or others in their communities (outside the upper middle class). Focusing\non college admissions as a context for studying LLMs in this way provides unique\nand important perspectives. There is no clear standard for what constitutes a ”good”\nor ”bad” personal statement, and students and families spend much time and energy\nseeking support through various means (e.g., online forums, private admissions coun-\nselors). Within the college admissions ecosystem, modern AI systems may be viewed\n5\nas oracles, providing seemingly authoritative guidance for applicants as they navi-\ngate opaque admissions processes. However, as colleges and universities continue their\nefforts to diversify their student bodies, LLMs might inadvertently shrink the pool of\ndiverse experiences students have described in the past and make it more difficult to\ndistinguish applicants or groups of applicants.\n2 Related Work\nWe organize our review of related work into two parts: first, we review the characteris-\ntics and biases of LLMs. Then, we discuss their diverse applications in both scholarly\nand general settings. This review establishes a foundation for our investigation, which\naims to understand the sociolinguistic profiles of LLMs.\n2.1 Profiles of LLMs\nRecent advances in transformer-based architectures have enabled the development of\nLLMs that can generate impressively human-like text and respond to complex ques-\ntions and prompts (Brown et al, 2020). Models like OpenAI’s GPT-3.5 and GPT-4\nare trained on massive textual corpora extracted from the internet, allowing them\nto learn the statistical patterns of language and generate coherent new text given a\nprompt. Despite the notable advancements of LLMs compared to other NLP tech-\nniques, they demonstrate biases that are reminiscent of past NLP approaches along\nwith new forms of bias. The primary difference between bias in, say, word embeddings\nand LLMs is that people are able to directly interact with LLMs through platforms\nlike ChatGPT. A better understanding of LLMs will therefore require consideration\nof the bias literature alongside studies of sociolinguistic profiles of LLMs.\nExtensive research has uncovered notable biases within LLMs (Gallegos et al, 2023;\nNavigli et al, 2023; Dev et al, 2021; Bender et al, 2021; Omiye et al, 2023; Kotek\net al, 2023; Tao et al, 2023; Lee et al, in press). These biases emerge because LLMs\nare trained on extensive datasets collected from the internet, which generally mirror\nprevalent societal biases related to race, gender, and various attributes (Omiye et al,\n2023; Kotek et al, 2023; Dev et al, 2021; Dhingra et al, 2023). For example, Omiye and\ncolleagues (Omiye et al, 2023) found that four LLMs (e.g., ChatGPT, GPT-4, Bard,\n& Claude) contained medical inaccuracies pertaining to different racial groups such as\nthe belief that racial differences exist in pain tolerance and skin thickness. LLMs have\nbeen observed to reinforce gender stereotypes by associating women with professions\nlike fashion models and secretaries, while assigning occupations like executives and\nresearch scientists to males (Kotek et al, 2023). Beyond the obvious risks related to\nmisinformation and the perpetuation of socially harmful biases, LLMs are also widely\nmarketed as being able to “adapt” to users given their input (Kirk et al, 2024a),\nmaking it possible for these same biases to be continually reinforced while also being\ndifficult to properly audit (M¨ okander et al, 2023). For the millions of lay users of LLMs,\nreceiving these kinds of messages repeatedly could reinforce linguistic hegemony by\npointing to a more narrow set of possible outcomes in their use under the assumption\nthat the models are learning to adapt to the user. Despite the clear advancements in\n6\nsophistication and performance, LLMs still largely retain these well-documented forms\nof bias.\nApart from sociodemographic biases, recent research has noted the distinct charac-\nteristics or “profiles” of LLMs. Specifically, it has been observed that these models\nexhibit a left-leaning bias in their responses (Martin, 2023; Motoki et al, 2023) and\nan affinity for Western cultural values (Tao et al, 2023). These models have also\nbeen found to consistently imitate personality traits such as openness, conscientious-\nness, and agreeableness in their generated outputs (Pellert et al, 2022). These and\nmany other studies have focused on English language responses, but LLMs are able\nto generate non-English text as well (Workshop et al, 2022) making it possible to test\nthese results from many different sociocultural paradigms. LLMs are clearly capable\nof generating many different types and forms of language, but little work has taken a\nsociolinguistic perspective where the social characteristics of writers – human vs. AI –\nare the focus of textual comparisons. It is also the case that large platforms like Chat-\nGPT are subject to internal tweaks and modifications that users may not be entirely\naware of, though so far, this has only been observed with respect to specific types of\nquestion answering (Chen et al, 2024) rather than more holistic shifts to the ways that\nLLMs tend to use language and which people use language most similarly. Whether\nor not people want an LLM that can communicate like someone with the same social\nidentities (e.g., African-American Vernacular English and Black Americans) is also an\nimportant question (Lucy et al, 2023), but the capacity for LLMs to mimic different\nwriting styles itself is underexplored. While some research is underway on alignment\nalong macro perspectives (Kirk et al, 2024b), linguistic hegemony typically operates\nby presenting all speakers with an assumed standard way of using language. To take\nan example from the film mentioned in the introduction, the English associated with\nmiddle-to-upper-middle-class White people in the US is often considered the national\nstandard as opposed to the AAVE used by the protagonist. If AI alignment does not\nconsider non-predominant linguistic styles and communities, it could further reinforce\nextant linguistic hegemony. Our study therefore complements ongoing research about\nthe limitations of using LLMs to simulate human respondents due to their inabilities\nin portraying particular identity groups (Wang et al, 2024). Generally, identifying soci-\nolinguistic trends between humans and out-of-the-box LLMs would generate valuable\ninsights to a broad range of computational social scientists, especially when considering\nthe popularity of these tools.\n2.2 Applications of LLMs\nScholarly community\nLLMs are being rapidly adopted for a wide range of uses among scholars, including\ndocument annotation (Egami et al, 2024; T¨ ornberg, 2023), information extraction\n(Chen et al, 2023), and tool development with applications to healthcare, education,\nand finance (Koyuturk et al, 2023; Matelsky et al, 2023; Wu et al, 2023; Rajashekar\net al, 2024). Social scientists have also been cautiously optimistic about potential uses\nfor LLMs in research (Karell et al, 2023; Davidson, 2023). The breadth of these use\n7\ncases make it imperative to best understand their stylistic tendencies for language\nwith respect to various social and scientific contexts.\nFor example, scholars are leveraging LLMs in text annotation tasks, including iden-\ntifying the political party of Twitter users (T¨ ornberg, 2023), automating document\nclassification (Egami et al, 2024), and extracting counterfactuals from premise-\nhypothesis sentence pairs (Chen et al, 2023). Concurrently, others are employing LLMs\nto create tools that are capable of generating personalized feedback from open-ended\nresponses for students (Matelsky et al, 2023) and even collaborate with writers to co-\nauthor screenplays and scripts (Mirowski et al, 2023). Aside from these academic uses,\neveryday people are more likely to engage in the text-generation dimension of LLMs.\nBut this is especially the case for ChatGPT given its popularity among the world’s\npopulation, due partly to its ease of use relative to other LLM tools and technolo-\ngies early on (Sakirin and Said, 2023). Focusing on patterns in the types of text they\ngenerate allows us to imagine how people across social strata experience the tool.\nGeneral population\nExisting research indicates that LLMs are being used among the general population for\nwriting-related tasks (Mirowski et al, 2023; Berkovsky et al, 2020; Shen and Wu, 2023).\nFor example, Wordcraft (Yuan et al, 2022) employs a LLM (Thoppilan et al, 2022) to\nassist writers with tasks such as proofreading, scene interpolation, and content gener-\nation. When writers were asked about their experience using Wordcraft (Yuan et al,\n2022), they indicated it had reduced their writing time and eased the writing experi-\nence. Similarly, Dramatron (Mirowski et al, 2023), an LLM-powered co-writing tool,\nhelps industry professionals develop their scripts by generating content and offering\nsuggestions for improving their work. These capacities also threaten creative labor, as\nseen with the writers’ strike in the film and television industry. Recent findings from\na Pew Research report highlight a notable trend in the use of ChatGPT among U.S.\nteens. According to the report, about 13% of all U.S. teens have used ChatGPT for\nassistance with their schoolwork, a number that is likely to increase over time.\nDespite considerable research into the technical and ethical dimensions of LLMs, there\nremains a significant gap in understanding their linguistic profiles. For example, when\nstudents are using LLMs to help them with their homework and writing assignments,\nthere is limited information about the ways that LLMs are more strongly correlated\nwith writing styles and tendencies associated with particular groups of people. Con-\nversely, students who do not speak English as a first language might find that their\nwriting is more often labeled as “AI-generated” (Liang et al, 2023), pointing to addi-\ntional ways that the current AI ecosystem could reinforce linguistic hegemony (in this\ncase, English language hegemony). It is therefore crucial to examine their linguistic\nbiases and tendencies as they may lead to the marginalization of non-standard dialects\nand expressions. Studies have mapped out such patterns in the context of academic\nwriting and review (Liang et al, 2024b), but here we consider a context that is more\ncommon and familiar to people living in the US: writing a personal statement for col-\nlege admissions. Understanding LLMs’ linguistic tendencies is necessary to ensure that\nthey do not perpetuate cultural hegemony (Gramsci, 1992; Woolard, 1985), poten-\ntially reinforcing biases against diverse linguistic practices. We address this directly\n8\nby exploring the question: Who do these models mimic in their writing? This inves-\ntigation is crucial, as the styles emulated by LLMs may influence the landscape of\ndigital communication and by extension the outcomes of textual tasks (i.e. annotation,\ninformation extraction).\nResearch Questions\nWe contribute to the literature on the social dimensions and implications of LLMs\nthrough a comparative analysis of human-authored and AI-generated text. Specifically,\nwe organize our work around the following research questions:\n1. How does the writing style of AI compare with the writing style of humans? Are\nthere specific groups of people whose writing style is most closely imitated by AI?\n2. What are the social characteristics of the humans whose essay is the most similar\nto a given AI generated essay?\n3. What is the predicted social context of AI as an author?\nAnswering question one will generate insights into the ways that AI writing represents\nor not the type of variation seen in human writing as well as providing one perspective\nto the overarching question of “who does AI write like”. The second question takes\nthis one step further by pairing human- and AI-written essays based on similarity\nto see which students are most likely to deploy writing styles similar to AI. Finally,\nwe leverage past results showing the strong relationships between student essay writ-\ning and geographically distributed forms of social capital and mobility (Alvero et al,\n2022a) to locate which communities are producing text most closely imitated by AI.\nRecent studies indicate that AI has more negative tendencies toward dialectal forms\nof language (Hofmann et al, 2024), suggesting that AI is likely to use higher and more\nformal registers used by people and communities with higher socioeconomic status.\nWe hope these findings could spur more specified hypothesization that consider other\nsocial contexts as well as experimental and causal frameworks (Gururangan et al,\n2022; Feder et al, 2022). For example, depending on the nature of the social alignment\nbetween human and AI writing style, a follow-up study could examine the effects of\nwriting more or less like an AI on human evaluation (in our context:, evaluation of a\ncollege application).\nOur analyses and questions are distinctive in another way. Many, if not most, of the\nstudies we cite here examine LLMs in more controlled settings rather than connecting\ntheir behaviors to real world contexts and situations, something we explicitly try to\naddress in our work. In the US, millions of people have gone through the ritual of\ncrafting a personal statement describing themselves, their interests, and their goals\nas part of their college applications. Many more people have written similar types\nof documents, such as cover letters for job applications. We connect LLM behavior\nto human behavior 1 to posit how extremely popular models might communicate to\n1The human generated text we use was created prior to the release of ChatGPT, making it impossible\nthat our data contained any synthethic or human-AI hybrid text.\n9\npeople “right” and “wrong” ways to describe themselves when comparing authorship\ndemographics with AI writing style tendencies.\n3 Data and Context\nWe analyze data from two higher education contexts. The first is the University of\nCalifornia (UC) system, one of the largest public research university systems in the\nworld. The application process is streamlined: students who want to apply to any of\nthe 9 campuses that provide undergraduate degree programs only need to complete\none actual application. That one application is then submitted to the campuses that\nthe students would like to attend, including highly selective, elite campuses (Berkeley,\nLos Angeles) along with less selective campuses (Riverside, Merced). Students are\ntherefore unable to tailor their applications to specific campuses. Applicants have to\nwrite essays written to four of eight possible prompts (70 possible combinations). The\nessay prompts are similar to those used by the Common App, an independent, national\ncollege application platform (see Table A1 for a full list of the prompts). Our UC\ndata come from every Latinx identifying in-state applicant who applied during the\n2016-2017 academic year (well before ChatGPT was released). California has a long\nhistory of linguistic prejudice and bias against Latinx people and communities (Barrett\net al, 2022; Olson, 1991), making these students particularly vulnerable to linguistic\nhegemony. As a population that is still under-represented in US higher education,\nespecially so for the Mexican-American students who comprise the largest Latinx\npopulation in California and the rest of the country, they also represent a socially\ndistinct group of students to the more elite students in our second education context.\nThe second context is a large private research university in the northeastern United\nStates. Our data include undergraduate admissions essays that were submitted to the\nschool of engineering during the 2022-2023 admissions cycle via the Common App\n(essays were submitted by November 1, 2022, right before the release of ChatGPT).\nApplicants wrote three essays: one in response to one of seven Common App prompts\nand two unique to the school of engineering. We analyze the Common App essays.\nThis was done to make the cross prompt analyses as comparable as possible given\nthe similarity between the Common App prompts and the public prompts. As is the\ncase in this and other highly selective universities in the United States, students from\nelite social backgrounds are over-represented in the admissions pool despite the low\nprobability of acceptance (below 10%). In this way, the essays submitted by these\nstudents help serve as a counter-exemplary pool of students to the under-represented\napplicants to the large public university. The private school applicants we analyze\nrepresent the entire pool of engineering applicants to this particular university.\nFinally, we pair these human-written documents with essays generated using GPT-\n3.5 and GPT-4. We set the temperature (randomness-in-response hyperparameter 2)\nto 1.20 ( min = 0; max = 2.0). We generated 25 essays for each temperature set-\nting of increments of 0.1 and found that 1.2 produced the best results across the\nessay prompts and formats. Focusing on GPT 3.5 and GPT-4 also gave us additional\n2https://platform.openai.com/docs/api-reference/authentication\n10\ncontrol by standardizing the temperature hyperparameter instead of estimating tem-\nperature settings across models. We found that texts generated with temperatures\nabove 1.20 were more likely to include irrelevant or random text, and unicode errors\n(e.g., “haci\\u8db33” was included in an essay generated with a temperature of 1.6).\nSetting the temperature below 1.20 tended to generate highly repetitive texts (e.g.,\nmultiple essays generated with temperatures of 0.6 or below would begin with the sen-\ntence “As I reflect on my high school experience, one particular event stands out as a\nturning point in my personal growth and self-awareness.”). Testing the effects of these\nhyperparameters across models could be addressed in future studies. Our goal was to\ngenerate texts which reflected the wide variety of stories, experiences, and narratives\nthat students included in their admissions essays (Gebre-Medhin et al, 2022) as a way\nto try and match human writing as closely as possible. We focus on OpenAI’s GPT\nmodels because of their usage in ChatGPT, currently the most popular LLM-powered\nchatbot platform. Future research could take a more purely technical perspective and\nanalyze other LLMs, but we are interested in generating insights into the processes\ntaking place when people create text and use the most popular tools to generate new\ntext.\nTo match the distribution of the real applicants’ essays, we tailor the prompt based\non the empirical distribution of students’ selected essay questions for each respective\ncontext. Specifically, we generate an essay by prompting GPT to respond to the same\nquestion that the applicant chose from the list of seven Common App essay options or\nfour of the eight UC essay options. We also include the names of the schools (University\nof California and the anonymized private university) in the prompts. Future studies\nmay consider including demographic information in the LLM prompt. Table 1 provides\ndescriptive statistics for the human-authored and AI-generated essays. Note that we\nreport the number of synthetic essays produced by the GPT models in terms of total\ndocuments generated: technically, the public prompts each contained four different\nessays prompts but we only analyze the merged documents.\nTable 1: Descriptive Statistics for the Human-written and AI-generated essay samples.\nSource Applicants Essays Format\nPublic University 35,789 143,156 4 essays (from 8 prompts), merged\nPrivate University 10,619 10,619 1 essay (from 7 prompts)\nGPT-3.5 10,000 (Public prompts)\n2,964 (Private prompt) (same as above)\nGPT-4 10,143 (Public prompts)\n2,945 (Private prompt) (same as above)\nFor each application essay, we have a variety of metadata reflecting the sociode-\nmographic attributes and contexts of the applicant. Following previous work, the\nsociodemographic attributes we focus on include first-generation status (whether or\n3\\u8db3 might be a reference to a unicode character.\n11\nnot the applicant has as least one parent who completed a college degree) and gender\n(recorded only as a binary: Male or Female) (Alvero et al, 2020; Giebel et al, 2022; Lee\net al, 2023). These data are important given longstanding barriers for women enter-\ning into engineering and the underrepresentation of lower-income students at selective\nuniversities. On a practical level, these were also two of the only pieces of information\navailable to us for both the public and private school applicants. Future work should\nconsider other authorship characteristics.\nWe complement these individual-level features with social context variables, specifi-\ncally data from the Opportunity Insights Lab4. The primary research goal of the Lab is\nto understand socioeconomic patterning and mobility in the US and often pays partic-\nular attention to geography (Chetty et al, 2022a). We connect geographic information\nfor each applicant with socioeconomic data for their ZIP Code. We focus on one par-\nticular variable, economic connectedness (EC), for two reasons: (1) of all the metrics\ngenerated and collected by the Lab, they claim EC has the strongest relationship to\nsocioeconomic mobility; and (2) previous research finds that application essay features\nare most strongly correlated with EC (Chetty et al, 2022b; Alvero et al, 2022a). EC for\na given ZIP Code is a measurement of the proportion of friendship ties and networks\nacross social classes, such as how many individuals from lower socioeconomic status\nbackgrounds have friends from higher socioeconomic status backgrounds 5. There is\nimplicit information in EC that makes it a particularly useful metric for socioeco-\nnomic status: the amount of friendships containing anyone from high socioeconomic\nstatus backgrounds is contingent on how many live in a given ZIP Code (the opposite\nis also true with respect to lower socioeconomic status). Understanding how AI writ-\ning style tracks with economic connectedness could point to more complex, socially\nembedded ways that linguistic hegemony could be reshaped through LLMs. Table 2\nprovides descriptive statistics of the sociodemographic characteristics of applicants in\nour samples.\nTable 2: Descriptive Statistics of Applicant Characteristics.\nSource Zip codes Gender First-Generation Status Economic Connectedness\nPublic University 806 F: 61.57%\nM: 38.43%\nFirst-Gen: 56.35%\nContinuing-Gen: 43.70%\nMin: 0.25\nMean: 0.84\nMedian: 1.07\nMax: 1.70\nPrivate University 3,147 F: 33.89%\nM: 66.11%\nFirst-Gen: 20.02%\nContinuing-Gen: 79.98%\nMin: 0.35\nMean: 1.06\nMedian: 1.08\nMax: 1.70\n4https://opportunityinsights.org/\n5The Opportunity Insights Lab provides the following non-technical explanation of economic connected-\nness: https://opportunityinsights.org/wp-content/uploads/2022/07/socialcapitalnontech.pdf\n12\n4 Analytical Approach and Measures\nAfter generating essays based on the same prompts as the human applicant essays, we\nuse a variety of statistical and visualization techniques to compare the text by source:\npublic school applicants, private school applicants, and AI-generated text for each\nrespective writing prompt. We also compare essay features across different authorship\nsociodemographic characteristics and map them onto the AI-generated text to under-\nstand which students write most similarly to AI and vice versa. Hegemony functions\nby placing traits, preferences, and tendencies associated with one group above others,\nincluding language. Our approach will unearth patterns in LLM-produced texts and\ncompare them to people from different sociodemographic backgrounds. If there is con-\nsistent alignment between LLM style and writing styles favored by those from higher\nsocial privileged backgrounds, the prospect of AI writing style contributing towards\nthe existing machinery of linguistic hegemony would be more likely. The popularity of\nLLMs could also transform extant forms of linguistic hegemony in writing if the oppo-\nsite were true. To account for these possibilities, we frame our analyses and results in\nterms of the predicted contextof the AI author when compared to the actual context\n(e.g., social identities and geographically distributed forms of socioeconomic informa-\ntion) of the human authors. Framing these results in this way highlights the subtle and\nnot so subtle ways that LLMs potentially homogenize language and culture toward a\nspecific sociodemographic group and context.\nThere are many potential analytical techniques to describe a piece of text; we use the\n2015 Linguistic Inquiry and Word Count (LIWC-15, shortened to LIWC hereafter) for\nmultiple reasons. LIWC is a dictionary approach that counts the frequencies of writing\nfeatures, such as punctuation and pronoun usage, and cross-references those counts\nwith an external dictionary based on psycholinguistics research (Pennebaker et al,\n2015). Generally, style is understood as the interplay between word selection, seman-\ntics, register, pragmatics, affect, and other linguistic dimensions that people use to\ncommunicate their ideas in ways that are reflective of their identity; in computational\nsocial science, style is usually calculated at the word level. LIWC models style in this\nway but with special attention to psychological dimensions, such as the ways that lin-\nguistic style can predict things like successful romantic matching (Ireland et al, 2011)\noften in a way to correlate linguistic styles with personality. LIWC is a popular tool for\ntext analysis across many domains, including psychology, social science, and computer\nscience6, often in ways that use LIWC as a means to generate numerical features for a\ngiven document to use in some kind of predictive framework, including other studies\nof writing style and social demography (Eichstaedt et al, 2015). LIWC has also been\nused in studies of college admissions essays, including studies showing that writing\nstyle in the essay is strongly correlated with household income, SAT score, and col-\nlege GPA at graduation (Alvero et al, 2021; Pennebaker et al, 2014). Using LIWC also\nallowed us to directly compare the public and private applicant essays, whereas other\nmethods would have violated data-use agreements. A limitation of LIWC in settings\n6For more information on LIWC, see https://liwc.app/static/documents/LIWC2015%20Manual%20-%\n20Operation.pdf\n13\nlike ours is that it generates many variables (we use 76 in our analyses), and false pos-\nitives are possible. To address this, we present multiple analyses and perspectives as a\nway to triangulate our results. Future studies with similar questions should consider\nother methods, but LIWC gives us the opportunity to be in direct conversation with\nother studies in the same domain of our data (college admissions) as well as many\nother contexts involving computational text analysis. Table A2 presents the 76 LIWC\nfeatures we used in our analyses. We exclude several features, such as word count and\ndash (usage of the non-alphanumeric character “-”), due to issues like incompatibility\nacross text formats. The word count feature (literally the number of words in a given\ndocument) was found to be positively correlated with various sociodemographic fea-\ntures in past studies, but we excluded it because the essay prompts included explicit\ninstructions about document length.\nIn this study, we take an agnostic approach in selecting which of the many LIWC\nfeatures to examine since there is limited literature on the relationship between human-\nwritten and AI-generated text. After calculating LIWC features for each document,\nwe compare them with respect to each set of documents (human or AI-generated,\npublic or private school prompts) and human authorship characteristics. Our first\nset of analyses focuses on the distributions for each LIWC feature across each set\nof documents, drawing from sociolinguistic research that also analyzes variation and\ndistributions of communicative practices. To compare the distributions, we use the\nKolmogorov-Smirnov (KS) statistical test. The KS test, along with other analyses that\ncompare distributions like Kullback-Liebler divergence, is widely used in data science\nand machine learning as a way to compare the likelihood that two continuous variables\nwere drawn from the same distribution (in the two sample test case) (Severiukhina\net al, 2020). Formally, we are comparing the empirical cumulative distribution func-\ntions of human written essays F and AI generated essays G for each LIWC feature X\nfor each sample size n for humans and m for AI. The KS test compares the distribution\nusing the following equation:\nD = sup\nx\n|Fn(x) − Gm(x)| (1)\nD is then used as a test statistic. We use 0.05 as a standard threshold for statisti-\ncal significance that the two samples came from different distributions (i.e., rejecting\nthe null hypothesis that they came from the same distribution). We use the KS\ntest to determine which LIWC features vary the most across samples based on their\ndistributions. From this basic analytical framework, we will also compare the style fea-\ntures across sociodemographic characteristics of the human authors. Adding the social\ndimensions will then show which groups of student writing features (based on gender,\nfirst-generation status, and EC scores) are most similar to the AI. Combined, these\nanalyses address our first research question. Linguistic hegemony functions through\novert and covert associations that people make between themselves and other peo-\nple based on idealized, standardized forms of communicating. The KS test comparing\nwriting style distributions and the proximity to the AI-generated essay distributions\nare used to capture this dynamic.\n14\nWe also compare essays as represented by the full set of their LIWC features using\ntwo methods. First, we use cosine similarities to find similar essays given the vector\nrepresentation of all essays (Amer and Abdalla, 2020). Here, cosine similarity is the\nmetric used to find each AI-generated essay’s most similar human-written essay (i.e,\ntheir “twin” essay). To find “twin” essays, we take each vector of LIWC features from\nhuman-written essays f and AI-generated essays g and calculate the cosine similarities\nfor each human-AI pair. We then report which the demographic characteristics of the\nhumans that had the highest cosine similarity for each AI generated essay. We compute\ncosine similarity as the dot product of the vectors divided by the product of the norm\nfor each of the vectors:\nfg\n∥f∥∥g∥ (2)\nWe report the demographic breakdowns of the human authors for the twin documents\nto address research question two.\nNext, we fit linear regression models where we regress information about the appli-\ncants, specifically the EC score for their self-reported ZIP Code on LIWC features.\nThis model will then be applied to the AI-generated essays to impute the same char-\nacteristics (Leiby and Ahner, 2023) to answer our third research question. Past studies\nusing LIWC features to predict sociodemographic characteristics found high predictive\npower (adjusted R2 of 0.44 when predicting SAT score) (Alvero et al, 2021). Formally,\nafter fitting the LIWC features to the human sociodemographic features, we use the\ncoefficients to predict the social context of the AI (using the same notation as (Leiby\nand Ahner, 2023)):\nˆy = βX (3)\nTo estimate the coefficients for predicted EC, we use 10-fold cross-validation with\nan 80/20 train and test split to prevent overfitting in training the model. The final\nmeasurement we report is the adjusted R2. Note that the AI-generated essays do not\nhave the prediction outcome available (i.e., we do not have an EC score for them);\nrather, we are using a common prediction framework to estimate an EC score given\nthe linguistic features of the essay (i.e., the “predicted context” for an AI-generated\nessay).\n5 Results\nOur results are organized as follows: First, we present direct comparisons and con-\ntrasts of LIWC features between human and AI-generated text. Second, we compare\nessays across sociodemographic variables, specifically first-generation status and gen-\nder. Third, we report findings of the “twins” analysis using cosine similarity. Finally,\nwe present the results from the linear regression analysis on the predicted context\nof the AI-generated essays. Overall, the results for GPT-3.5 and GPT-4 were simi-\nlar, though in various instances, GPT-4 exhibited stronger tendencies than those of\n15\nGPT-3.5. This includes the relative similarity of statistical curves and distributions,\nless variance, and less variation in the human twin essay analysis (likely a product of\nthe lower overall variation). GPT-4 is more expensive to use, making it less accessi-\nble, especially for high school students writing admissions essays. Therefore, we focus\nmost (but not all) of the discussion of the results on the GPT-3.5 analyses.\n5.1 Direct Comparisons\nBefore comparing the writing styles of humans and AI along social dimensions, we\ncompare the writing between essay prompts and applicant pools. The figures and\nfindings we describe in this and the subsequent section present statistically significant\ndifferences, according to the KS test, between the human-written and AI-generated\nessay style features. These visual differences will also yield insights into basic questions\non whether or not AI writes like humans, something that the rest of our analyses\nfocus on with the rejoinder of “which humans.” The distributions for the AI stylistic\nfeatures we present in this section will be used throughout the rest of the paper (as\nopposed to the disaggregated features for the human authors). Across many of the\nLIWC features, the AI distributions tended to be notably different from the human\ndistributions. These differences were also notable in visualizations of the distributions\nof these features. For example, Figure 1 shows not only that AI uses longer words (six\nor more letters long, called Sixltr in LIWC) but also that the variation is much lower\nthan in the human essays (though this is more pronounced with the essays generated\nwith the public school prompts). We also noted that the distribution of this and\nother features for the private school applicants was slightly closer to the AI-generated\nessays. While this is representative of many of the LIWC features, some were much\nless clear. For example, humans and AI tend to write about affiliations (with groups,\npeople, organizations, and friends) at similar rates despite the AI not actually having\nany affiliations (see Figure 1). Future sociological studies might consider comparing\nthe kinds of group memberships claimed by LLMs (as captured by the “affiliations”\nLIWC feature) with those of humans. In the case where the distributions were more\ndistinctive (such as the Sixltr distributions), the private school applicants were slightly\ncloser to the distributions for the AI-generated essays.\nIt was also the case that for many features, the distributions for AI-generated essays\nwere more narrow (i.e., lower variance). These patterns of slight levels of similarity\nbetween some but not all applicants and lower levels of variance for the AI essay\nfeatures also emerged in our social comparison analyses. Another way to interpret\nthese results is that LLMs have a limited range in the text that they generate, which\nbecomes obvious only when compared to human variation across a consistent set of\nstylistic dimensions. This would make LLMs ripe for use in hegemonic processes of\nstandardization and homogenization, even if unintended. The slight similarity with\nsome groups and not others, along with the low variance, may further contribute to\nthese processes. Next, we examine applicant characteristics beyond the type of school\nto which they applied (though it is also the case that the public school applicants in\nour sample are, in the aggregate, underrepresented in US higher education).\n16\n(a)\n (b)\nFig. 1: Distributions of (a) Sixltr (usage of words with six or more letters) and (b)\naffiliation (affiliations) for each set of essays.\n5.2 Social Comparisons\nWe compared essays between several sociodemographic characteristics of applicants,\nspecifically their gender, first-generation status, and the EC score for their ZIP Code.\nWe found two patterns: (1) the writing style features of AI-generated essays tend to\nhave different distributions (mean and variance) than human-written essays, and (2)\nwhen disaggregating the human-written essays based on the authors’ social character-\nistics, some groups’ writing style more closely resembles AI’s writing style than that\nof other groups. First, we present a table of the most demographically distinctive fea-\ntures and whether AI tends to write like any particular group. Then, we examine the\ndistributions of human and AI-generated text that were considered distinct. Both of\nthese analyses used KS tests to measure and compare distinctiveness. We also note\nwhich groups of students used each LIWC feature most similarly to the LLM from\namong the independent distributions. We also present the results for the comparisons\nof the public and private school applicants separately.\nFirst, we directly answer the question, “Who do LLMs write like?” while limit-\ning the risk of false-positive results given the large number of LIWC features. We\nidentified the LIWC features that were significantly different between the three demo-\ngraphic groups for both Public and Private school applicants using KS tests: gender,\nparental education level, and socioeconomic context (modeled using ZIP Code-level\nEC scores). For the identified LIWC features, we compared the subgroup distribution\nof human-written essays with those of AI-generated essays. Table 3 shows the num-\nber of distinctive features for each group as well as the proportion of features most\nsimilar to any particular group; for each set of features and demographic groups, the\nAI-generated essays’ features most resembled male applicants with college-educated\nparents from high-EC ZIP Codes. What these results highlight are the ways that, in\ncontexts where users are directly interacting with the most popular LLMs, there are\nstylistic features favored by people that are also favored by the models. The features\n17\nfavored by this particular group of students as well as the AI include those found to\nbe predictive of college GPA (article, Analytic; Pennebaker et al (2014)), SAT scores\n(Sixltr, prep; Alvero et al (2021)), and is more reflective of usage patterns among\nenrolled college students (focusfuture, space; Alvero et al (2022c)). The full list of\nfeatures is included in the supplementary materials (Table A3). These traits, along\nwith the demographic dimension of who in the data adopts these stylistic features\nmore often, highlight how LLMs could reinforce linguistic hegemony in situations like\ncollege admissions.\nSig. differentLIWC featuresin human-written essay\nGPT 4 (Public)GPT 4 (Private)GPT 3.5 (Pub-lic) GPT 3.5 (Pri-vate)\nGender 29 79.3% male 65.5% male 72.4% male 75.9% male\nFirst-gen. Status37 75.7% cont.-gen.78.4% cont.-gen.75.7% cont.-gen.81.08% cont.-gen.\nEC 25 80.0% high EC88.0% high EC80.0% high EC92.0% high EC\nTable 3: Among the most distinctive LIWC features based on gender, first-generation\nstatus, and EC (using a median split to create a low and high EC group), for both the\npublic and private applicants, we present the proportions of features that are the most\naligned between the AI-generated essays and each respective subgroup. For example,\namong the 29 LIWC features for which the distributions were significantly different in\nessays written by men versus women applicants, the feature distributions of the AI-\ngenerated essays were closer to those of male applicants for at least 65.5% of features\n(GPT-4; Private applicants) and up to 79.3% of features (GPT-4; Public applicants).\nWe next describe our other sociodemographic analyses. For gender, we found more\ndistinctiveness in the distributions of the individual LIWC features for the public\nschool applicants’ essays than for private school applicants’ essays relative to those\ngenerated by the LLM (48 features for private school applicants, 75 for public school\napplicants). For many of these individual features, including common verb usage (see\nFigure 2), the differences between the AI-generated and human-written essays were\nappreciably greater than the differences between essays written by men and women.\nThe LIWC features for women were slightly more similar to those of LLMs for the\npublic school prompt, but this pattern did not hold for private school applicants.\nFor example, while the difference between LLMs and humans in the use of Analytic\nlanguage (Figure 3) was smaller than Figure 2, it was also the case that male appli-\ncants were slightly more similar to the LLM. Beyond their statistically significant\ndifferences, we highlight these LIWC features because of their associations with college\nGPA in previous work (negative for verb, positive for Analytic), suggesting potentially\nbroader implications for these small differences (Pennebaker et al, 2014). Though these\ndifferences are not large, they could become magnified in different social contexts and\nsituations.\nThe differences in writing style between first-generation and continuing-generation\nstudents were similar to those between gender groups: many stylistic features were\ndistinct between the human-written and AI-generated essays, but many of the features\n18\n(a)\n (b)\nFig. 2: Distribution of verb (verb usage) for each set of essays split by gender for (a)\npublic school applicants and (b) private school applicants.\n(a)\n (b)\nFig. 3: Distributions of Analytic (analytical thinking) for each set of essays split by\ngender for public school applicants (a) and private school applicants (b).\nwere slightly more similar to one group than the other. In this case, many writing\nfeature distributions of AI-generated essays were closer to the continuing-generation\napplicants.\nFinally, we compare the style features for the AI and human essays but split the appli-\ncant essays into EC score quartiles. Like the other results, the distributions between the\nhumans and AI were independent but some students had similar stylistic approaches\nto the AI than others. For example, the Analytic features for the students from the\nZIP Codes with the highest EC scores (represented by the yellow curves in Figure 5)\nwere closest to the curve for the AI essays. It was also the case that the curves were\n19\n(a)\n (b)\nFig. 4: Distribution verb (usage of common verbs) for each set of essays split by first-\ngeneration status for (a) public school applicants and (b) private school applicants.\n(a)\n (b)\nFig. 5: Distribution of Analytic (analytical thinking) for each set of essays split by\nEC quartiles for (a) public school applicants and (b) private school applicants.\nroughly sequential in their patterning, with the highest EC scores closest to the AI\ncurve and the lowest EC scores farthest away.\nThe individual writing style features showed differences across social groups. These\ndifferences reflected traditional forms of hegemony and social privilege, such as appli-\ncants with college-educated parents and those from areas with high levels of EC. For\nthe comparisons of individual LIWC features, it was also the case that AI wrote more\nlike male applicants among the most gendered stylistic features. But this was not uni-\nversally universally the case, as some of the style features were more closely aligned\nwith women. Although women are not typically associated with processes of hege-\nmony, they tend to submit stronger overall applications than men (Giebel et al, 2022),\n20\nSource % Female % Male % First-Generation % Continuing-Generation Mean EC (SD)\nPublic applicants 61.59 38.41 56.30 43.70 0.84 (0.25)\nPublic twin essay authors (GPT-3.5) 71.66 28.34 53.52 46.48 0.87 (0.23)\nPublic twin essay authors (GPT-4) 44.70 55.30 56.08 43.92 0.89 (0.19)\nPrivate applicants 33.79 66.21 20.02 79.97 1.06 (0.25)\nPrivate twin essay authors (GPT-3.5) 31.55 68.45 4.32 95.68 1.13 (0.25)\nPrivate twin essay authors (GPT-4) 30.25 69.75 6.9 93.14 1.14 (0.24)\nTable 4 : Sociodemographic distribution of Human-written essays and their AI-\ngenerated “twin” essay.\npointing to ways that context could shape answers to the question of which students\nare writing most like LLMs. This, paired with the low variation in these same features\nfor the AI, show how standardization and homogenization is likely but also likely to\nbecome associated in the writing styles of certain groups of people in hyperfocused\nways. The next two subsections consider the entire set of variables simultaneously.\n5.3 Cosine Similarity Derived Twin\nWe generated cosine similarities for each essay pair in the public and private applicant\ndata. Because the AI essays had low variation for the LIWC features, many of the\nmost similar documents were other AI essays. See Table 4 for a breakdown of the\ncharacteristics of the authors of the twin essays and the respective applicant pools.\nRegardless of the essay source (public or private school applicants), the authors of the\ntwin essays were more likely to be women, continuing gen, and come from ZIP Codes\nwith higher EC than their respective applicant pools. It was also the case that the\ndifferences for gender were more pronounced for the public school applicants and the\ndifferences for first-gen status were far more pronounced for the private applicants.\nAmong the twin essays for the private school applicants, approximately 95% were\nwritten by students who had at least one college-educated parent compared with 82%\nin the actual applicant pool. Combined, these results suggest that AI writing styles\n(in a holistic sense) are more similar to students from more privileged backgrounds\n(continuing gen) but also from students who tend to perform better in other metrics\nin the same process (women). Conceptions of hegemony tend to focus more on the\nformer, but more nuance might be needed with respect to the latter.\nAuthors of the twin essays also tended to come from ZIP Codes with higher EC scores\nthan the applicant pools, but this trend was more than five times stronger for the public\nschool applicants (difference of 0.16) then for the private school applicants (difference\nof 0.03). The types of communities reflected in these average scores were also different\nbetween the public and private school applicants. The three ZIP Codes closest to the\naverage EC score for the public school applicants were located in San Marcos (92069,\nEC = 0.837); Anaheim (92801, 0.837), and Dixon (rural community southwest of\nSacramento, 95620, 0.835). The public applicant twin essays were on average most\nsimilar to essays from Morro Bay (93442, 1.00); Coronado (adjacent to downtown\nSan Diego, 92101, 0.99); and Auburn (a suburb of Sacramento, 95603, 1.02). While\nthere is some variation across some locales, the most stark difference between these\n21\ncommunities for the applicants are the respective percentages of Hispanic residents 7.\nThe ZIP Codes in the actual applicant pool range from 40% (San Marcos) to 54%\n(Anaheim). In contrast, the Hispanic population percentages of the ZIP Codes and\nEC scores for the twin essays range from 13% (Auburn) to 19% (Coronado). These\ncommunities with higher EC also had less ethnic homophily for the entirely Latinx\napplicants in our dataset.\nAlthough the private school applicants were not limited to California like the public\nschool applicant data, we took the same approach to get a sense of the types of com-\nmunities that are writing essays most similar to the AI-generated text and vice versa.\nPrivate school applicants tended to come from communities with higher EC than the\npublic school applicants, and in the California context, these were also communities\nwith higher socioeconomic status. The most similar ZIP Codes were located in San\nPedro (90732, 1.07); Murrieta (92591, 1.067); and Pasadena (91106, 1.07). All of these\ncommunities were middle to upper-middle-class suburbs in the Los Angeles metropoli-\ntan area. When mapped onto California, the authors of the human twin essays come\nfrom well-known upper-middle to upper-class communities, including San Jose (down-\ntown area, 95116, 1.10); Santa Monica (90404, 1.10); and Pacific Palisades (90272,\n1.10). While more analysis would be needed on the specific communities where the\nprivate school applicants hail from, these California-based trends show how AI writ-\ning styles also reflect demographic and socioeconomic variation in ways that mirror\nsegregation and social status.\n5.4 Predicted Social Context\nFinally, we present the results for the predicted context of the AI essays through\npredicted EC scores. The linear model fitted to essays and EC values of public school\napplicants had adequate predictive power with an adjusted R2 value of 0.57, which\nmatches past results using a similar approach (Alvero et al, 2022b). However, the\nmodel for private school applicant essays achieved lower predictive power with an R2\nvalue of 0.15. To emphasize more reliable results, we report findings from public school\napplicants in this section. The discrepancy in the models’ predictive power could be\nexplained by the following two factors. First, the private school applicants came from\ncommunities with higher levels of EC and lower variation in EC; the lower variation\nwould mean that the model would likely have a harder time distinguishing patterns.\nSecond, the difference in variation might also be explained by which students are drawn\nto apply to highly selective universities and the most selective programs in those same\nschools (engineering). Many of the public school applicants had access to fee waivers\nto submit their materials given their socioeconomic status (including those who also\napplied to the private school), but if they felt as if their chances of acceptance were\nextremely low then they might not feel compelled to even apply.\nUsing the model trained on the public school applicant LIWC features, we predicted\nthe EC scores for each of the AI essays 8. Similar to the cosine twin analysis, the\n7See https://www.census.gov/quickfacts/\n8We remind the reader that the AI generated essays did not have an actual EC, our model is predicting\nwhat the ECwould have been given the style features.\n22\nFig. 6: Distributions of EC scores at the ZIP Code level and predicted values for AI\nessays with individual-level EC.\naverage imputed EC was higher (1.33) than the average EC for the applicant pool at\nthe level of the individual applicant (0.84) and the average ZIP Code (0.91); Figure 6\nshows these distributions. Visually, these differences may not seem particularly large,\nbut in terms of EC these differences were substantial. For example, among the ZIP\nCodes of public school applicants in our dataset, this difference in EC is the equivalent\nof slightly below median EC (0.88) with higher than the 90th percentile (1.28).\nAll of the results we just described are focused specifically on the analyses of essays\ngenerated using GPT-3.5. But as Figure 6 shows, these trends become exaggerated\nwith GPT-4 as it has an even higher average predicted EC score. Ironically, the higher\ncost to use GPT-4 mediates access to the tool based on income and then uses a writing\nstyle associated with people from the ZIP Codes with the highest levels of social\nmobility.\nWith the direct and social comparison analyses, we noted how many stylistic features\nwere independent between humans and AI while also noting that some groups of\nstudents tended to write slightly more similarly to the AI. But the predicted context\nand cosine twin analyses, each of which incorporated all of the style features, show\nthat these subtle differences quickly accumulate to produce text that resembles writing\nstyles of students from certain backgrounds. The relatively low levels of variance for the\nindividual features were not as prominent here as they were for the direct and social\ncomparison findings, but future studies might consider which writing style features\nare most noticeable to human readers to see if only one or several style markers are\nprominent.\n23\n6 Discussion and Conclusion\nIn this study, we considered the intersection of LLMs, social demography, and hege-\nmony through analyses of college admissions essays submitted by applicants to a public\nuniversity system and an engineering program at a private university. Using a popular\ndictionary-based method, LIWC, we compare the writing styles of human and AI-\ngenerated text in response to the same writing prompts for each applicant pool. Our\nfindings were generated through direct comparisons of the LIWC features between the\ndocuments, social comparisons using demographic information of the applicants, and\ncompositional analyses using all of the LIWC features (cosine similarity twins and the\npredicted context). We find that for individual stylistic features, LLMs are generally\ndistinct from humans: they used various LIWC features either systematically more or\nless than humans, and used them more consistently (i.e., with less variation across\nessays). The reduced variation could potentially narrow the scope of what is considered\nan appropriate way to present oneself in writing if the treatment of AI as an “oracle”\nthat is always correct persists (Messeri and Crockett, 2024). It was also the case that,\nwhen considering authorship identities and characteristics and individual features, the\ndifferences between the human-written and AI-generated essays were greater than the\ndifferences between groups of students (e.g., between men and women). However, in\nboth sets of findings, it was also the case that some groups of students used features\nslightly more like the AI than other groups. In our analyses using all of the features,\nthese differences became more acute as the essays resembled those from areas of higher\nEC than the average student.\nHowever, the social comparison analyses also show that the writing styles of LLMs\ndo not represent any particular group (Dominguez-Olmedo et al, 2023). Though the\nfeature distributions for the AI-generated essays were indeed closer to one sociodemo-\ngraphic group than another, as seen in Figure 3 and Table 3 for example, they were also\nquite distinct from those human groups. This points to two reasonable interpretations\nof the similarity between human and AI-generated text: a distributional perspective\nand a sampling perspective (see Figure 7 for a conceptual diagram). First, we might\nconsider a distributional perspective (not to be confused with (Bellemare et al, 2017)).\nIf the distribution of the writing style features is so distinctive and unaligned with\nany human style of writing, future studies might examine the extent to which peo-\nple consider LLM-generated text unhuman and artificial. Second, we might consider\na sampling perspective where we might focus less on the curves of the distributions\nof writing features and more on the peaks and expected characteristics. For exam-\nple, E[Analytic|LLM author] would be on the higher ends of the distributions for a\nfeature that is used more often by men (Figure 3) or less (Figure 2). Taking this inter-\npretation further and considering the low variance we also noted, these results could\nbe interpreted as indicating that the writing style features of LLMs are reflective of\nthe most masculine (in the case of Figure 3) or whatever group is most similar to the\nAI. Our analyses lend themselves more readily to the sampling perspective given our\nconsiderations of how individual students could be interacting with and comparing\ntheir writing styles with AI-generated text, but future studies could better elucidate\nthe nature of this homogenization and its impacts on society.\n24\nSampling perspective: \ndistances between group \nmeans\nDistributional perspective: relative variance across groups\nFig. 7 : A conceptual diagram of the distributional and sampling perspectives for\ncomparing human (left-hand distributions) and AI (right-hand distribution) text. A\nsampling perspective might focus on closeness in means (the peaks of the distributions)\nwhereas a distributional perspective might compare variance in the distributions (the\nwidths of the distributions).\nBeyond our findings here, future work comparing AI and human writing might benefit\nfrom specifying the type of perspective that is guiding a given study. For example, cur-\nrent work on AI alignment tends to focus on the sampling perspective (e.g., “can the\nAI respond like an average person with somme set of demographic characteristics?”).\nAnalyzing alignment from a distributional perspective might instead focus on ques-\ntions that consider variation in human language and communication as a conceptual\nstarting point. The importance of the latter will grow in prominence as AI increasingly\nenters into key social decision making processes that would require them to interact\nwith a breadth of sociolinguistic variation. The early results are not promising, as a\nrecent paper demonstrated that AI has strong social biases when given text that con-\ntains dialectal features (Hofmann et al, 2024). Further, as people interact with LLMs\nin their daily lives across many different contexts there is a chance that broad under-\nstanding of “correct” ways to write and communicate will become more constrained.\nIn this way, LLMs might undermine some of the ideals of college admissions where\nstudents are given a unique opportunity to highlight their experiences, ideas, and iden-\ntity by shrinking the breadth of those details. Future studies might more explicitly\nconsider how students are using LLM technology to better get at this issue.\nHegemony is a theory that encapsulates the myriad ways that that power is exerted\nthrough the supposedly common sense ways we understand the world and who deviates\nfrom these norms (Gramsci, 1992). LLMs, as examples of extremely popular technology\nin terms of users and use cases, will undoubtedly play a complex role in modern digital\n25\nhegemony. College admissions essays are unique in that they provide some creative\nflexibility for the authors while still being a primary data point used to either admit\nor reject students. There is a clear tension between sharing an authentic portrayal of\none’s life and experiences with the norms and expectations to be able to demonstrate\na certain level of writing ability and style. This tension has created an entire cottage\nindustry that helps students balance these expectations (Huang, 2023). The polished,\n“fancy” writing style of LLMs might give students enough of an incentive to put aside\ntheir own writing style and stories if it gives them an advantage in the admissions\nprocess. If the same advantages that come with writing in the same kinds of ways\nas those from higher social status backgrounds represent the linguistic disposition of\nLLMs, the many new tools and technologies relying on them could reinforce patterns\nwhere the language of some is structurally and systematically favored over the language\nof many. LLMs and platforms like ChatGPT might not be able to inflict direct control,\nviolence, or power, but by reinforcing extant language standards would contribute\nto the ideologies people have about language as it relates to power. This is possible\nbecause of the many social mechanisms already in place which operate under similar\nlogics and/or have similar outputs.\nWe hope our analyses can spur future studies that consider how everyday people\nmight interact with LLMs in contexts like writing personal statements (an activity\nwith many analogues in modern society). There is a robust research ecosystem focus-\ning on improving LLMs, better understanding their capacities, and trying to prevent\npernicious forms of bias from leaking. But there is room that consider how power in\nits current forms might be enacted through everyday use of these same tools. Consider\nthe counterfactual of our findings: “LLMs write like those from more vulnerable social\npositions”, not like those with (relatively) more power. It is possible that this would\nbecome a “problem” to fix, but in a practical sense it would be difficult to imagine\nLLMs permeating across social institutions as they have without sounding like the\npeople who have historically moved through these same institutions with relative ease\n(Schneider, 2022). If LLMs did not write in the ways we described, they might not\nbe as popular a tool for things like academic writing and research assistance, writing\nevaluation more generally, and the construction of the many types of documents in\nareas outside of education like the law, healthcare, and business.\nCollege admissions is a high-stakes competition, but putting aside one’s language in\norder to achieve success is the exact kind of hegemonic process in the film “Sorry to\nBother You” that we mentioned at the outset of this paper. It is not as if dialectal or\nnon-academic forms of writing are not valuable either, such as the case of AAVE being\nso widely used on the internet despite its real-life stigmatization in formal contexts\n(Brock Jr, 2020). Unlike AAVE, the linguistic styles and markers of the upper middle\nclass have long been held as the standard by which others are evaluated and compared,\na trend unlikely to change given our findings. Along these lines, future studies might\nalso compare the specific types of stories and lexical semantics in LLMs to extend our\nanalyses on writing style features generated through LIWC. These might also include\nstudies of the multilingual capacities of LLMs. Another study found that, despite well-\ndocumented social stigmas, approximately 20% of UC applicants include some form\n26\nof Spanish in their admissions essays, whereas 0% of the synthetic text we examine\nin our study did the same (Alvero and Pattichis, 2024). Other studies might consider\nhow not just the text generated by LLMs is stratified, but also things like access to\nthe technology and perceptions of who is able to use it correctly.\nIf we assume that, like college admissions essays, there is a correspondence between\nwriting style and social demographics, this paper might also shed some light on the\ndemographics of the people who generate the text on the internet which form the\ntraining data used to create LLMs. Beyond training, our results also implicate pat-\nterns and preferences that go into the massaging and fine-tuning of LLMs prior to\ntheir deployment (Lee et al, in press). The last step in the pre-deployment process is\nreminiscent to how newscasters are trained to speak in a specific way during broad-\ncasts that is intended to convey credibility and authority (Gasser et al, 2019), though\nhere we observe trends with writing and social demographics. Studies of the text on\nthe internet has noted similar trends, such as 85% of Wikipedia’s editors (a major\nsource of training data for LLMs) being White men 9. Though we do not explicitly\nconsider race, we do see similar trends in terms of specific stylistic features in writing\n(such as Analytic language) being used more often by male applicants and the AI-\ngenerated text. But it is also the case that in certain situations, the writing style of\nLLMs is more similar to the women in our dataset. These sociodemographic trends in\nwriting point to future studies that examine how writing and language with AI could\nplay a role in reproducing essentializing ideologies about not just gender and class\nbut also about race, especially in educational contexts (Roth et al, 2023). In this way,\nwe see our work as helping to lay the foundation for a sociologically oriented comple-\nment to ongoing work focused on LLMs and psychology (Demszky et al, 2023). In our\ntext-saturated world, a demography of text could yield many crucial insights.\nOur analysis focused on “out-of-the-box” AI-generated essays without prompts that\nspecify demographic information about the applicants (e.g., write this essay from the\nperspective of a first-generation college applicant). Although this points to future\ndirections for this kind of work, it is possible that many or most people using LLMs\nin their daily lives do not include demographic information in their prompts, such\nas “write this from the perspective of a first-generation, under-represented minority\nstudent”. It is also unclear if LLMs can be prompted or manipulated to such a degree\nthat they are able to mimic the lower-level stylistic trends we identify here without\nexplicit instruction to do so (e.g., “use more commas in the output”). As a comparison,\none study of similar data found that approximately 20% of the entire University of\nCalifornia applicant pool uses Spanish words or phrases in their admissions essays\n(Alvero and Pattichis, 2024). Though multilingualism was not the primary focus of this\nanalysis, the out-of-the-box model did not include any Spanish words in the generated\ntext. It would be easy to include these types of instructions, but figuring out which\ntypes of instructions and specific linguistic features to include would not be obvious\n(aside from possibly prompting the model to use “big words”, though that is already\nthe case). It might also be the case that the types of stylistic features most notable to\nstudents are stratified in specific ways or shaped through other hegemonic processes.\n9See https://www.thejournal.ie/wikipedia-founder-gender-imbalance-3668767-Oct2017/\n27\nExtensions of our analyses could focus on different elements of the relationship between\nhumans and LLMs. For example, our out-of-the-box approach based on text generated\nfor the same responses might be contrasted with a study on the types of prompts\npeople use in their everyday lives. These could include comparisons of how people craft\nprompts for the same goal or task (such as writing a college admissions essay). The\ntext generated from the slightly different prompts could then be analyzed using similar\napproaches and methods we adopted here. To address the issue of discrepancies in\nwriting style, other studies could consider fine-tuning on custom training data. These\nstudies could evaluate the controllability of LLMs to generate text and writing styles\noutside the low variance distributions we describe in this paper. Similar results have\nbeen observed in the shrinking vocabulary of peer review (Liang et al, 2024a). Outside\nof these more computationally focused studies, social scientists might also begin to\nanalyze the trends where LLMs write both like those with traditional social privilege\n(such as having college-educated parents) as well as those who tend to do well in\nspecific domains, contexts, and processes (such as women in education and college\nadmissions). There are clear tensions in terms of hegemony: if writing like people who\nhave privilege or are generally more successful, should other students adopt LLMs\nto assist in their writing in earnest? How might this exacerbate or ameliorate social\ninequality as it pertains to language and writing? These questions could be used to\nguide future studies not just on authors but also evaluators of text in a given situation,\nsuch as college admissions officers who read essays and evaluate applicants. Given the\nway that text is so widely used to evaluate people, the stakes of these answers are\nquite high, and the trends we describe in our analyses point to plausible hypotheses\nin many different domains.\nDeclarations\nEthics approval and consent to participate: IRB approval for all data used.\nConsent for publication: Authors agreed on the final version. Consent was not\nnecessary for applicants.\nAvailability of data and materials: Raw essays are not available. Public school data\n(LIWC features for the essays) are available on Harvard dataverse; private school data\n(respective LIWC features) will be made available soon.\nCompeting interests: Authors declare no competing interests.\nFunding: N/A\nAuthors’ contributions: All authors contributed to the writing, analysis, and interpre-\ntation for the paper.\nAcknowledgments: We are grateful for the comments and feedback received at (1) the\nGenerative AI and Sociology Workshop at Yale University organized by Daniel Karell\nand Thomas Davidson; (2) the 2024 Monash-Warwick-Zurich Text-as-Data Workshop\norganized by Elliott Ash, Sascha O. Becker, and Philine Widmer; (3) the Pens & Pixels\nGenerative AI in Education Conference organized by Mark Warschauer and Tamara\nTate.\n28\nAuthors’ information: N/A\nReferences\nAlkaissi H, McFarlane SI (2023) Artificial hallucinations in chatgpt: implications in\nscientific writing. Cureus 15(2)\nAlvero A (2023) Sociolinguistic Perspectives on Machine Learning with Text Data.\nIn: The Oxford Handbook of the Sociology of Machine Learning. Oxford University\nPress, https://doi.org/10.1093/oxfordhb/9780197653609.013.15\nAlvero A, Pattichis R (2024) Multilingualism and mismatching: Spanish language\nusage in college admissions essays. Poetics 105:101903\nAlvero A, Arthurs N, Antonio AL, et al (2020) AI and holistic review: informing\nhuman reading in college admissions. pp 200–206\nAlvero A, Giebel S, Gebre-Medhin B, et al (2021) Essay content and style are strongly\nrelated to household income and SAT scores: Evidence from 60,000 undergraduate\napplications. Science advances 7(42):eabi9031. Publisher: American Association for\nthe Advancement of Science\nAlvero A, Luque˜ no L, Pearman F (2022a) Social Influences on Textual Production:\nIntersectionality, Geography, and College Admissions Essays Publisher: SocArXiv\nAlvero A, Luque˜ no L, Pearman F, et al (2022b) Authorship identity and spatiality:\nSocial influences on text production. https://doi.org/10.31235/osf.io/pt6b2, URL\nosf.io/preprints/socarxiv/pt6b2\nAlvero A, Pal J, Moussavian KM (2022c) Linguistic, cultural, and narrative capi-\ntal: computational and human readings of transfer admissions essays. Journal of\nComputational Social Science 5(2):1709–1734. Publisher: Springer\nAmer AA, Abdalla HI (2020) A set theory based similarity measure for text clustering\nand classification. Journal of Big Data 7(1):74\nAtari M, Xue M, Park P, et al (2023) Which humans? https://doi.org/10.31234/osf.\nio/5b26t, URL https://doi.org/10.31234/osf.io/5b26t\nBarrett R, Cramer J, McGowan KB (2022) English with an accent: Language, ideology,\nand discrimination in the United States. Taylor & Francis, London\nBastedo MN, Bowman NA, Glasener KM, et al (2018) What are we talking about\nwhen we talk about holistic review? selective college admissions and its effects on\nlow-ses students. The Journal of Higher Education 89(5):782–805\nBellemare MG, Dabney W, Munos R (2017) A distributional perspective on rein-\nforcement learning. In: International conference on machine learning, PMLR, pp\n29\n449–458\nBender EM, Gebru T, McMillan-Major A, et al (2021) On the Dangers of Stochas-\ntic Parrots: Can Language Models Be Too Big? . In: Proceedings of the 2021\nACM Conference on Fairness, Accountability, and Transparency. ACM, Virtual\nEvent Canada, pp 610–623, https://doi.org/10.1145/3442188.3445922, URL https:\n//dl.acm.org/doi/10.1145/3442188.3445922\nBerkovsky S, Hijikata Y, Rekimoto J, et al (2020) How Novelists Use Generative\nLanguage Models: An Exploratory User Study. In: 23rd International Conference\non Intelligent User Interfaces\nBernstein B (1964) Elaborated and restricted codes: Their social origins and some\nconsequences. American anthropologist 66(6):55–69\nBoelaert J, Coavoux S, Ollion E, et al (2024) Machine bias. generative large language\nmodels have a view of their own. https://doi.org/10.31235/osf.io/r2pnb, URL osf.\nio/preprints/socarxiv/r2pnb\nBohacek M, Farid H (2023) Nepotistically trained generative-ai models collapse. arXiv\npreprint arXiv:231112202\nBrock Jr A (2020) Distributed blackness. In: Distributed Blackness. New York\nUniversity Press, New York\nBrown TB, Mann B, Ryder N, et al (2020) Language Models are Few-Shot Learners\nChen L, Zaharia M, Zou J (2024) How Is ChatGPT’s Behavior Changing Over Time?\nHarvard Data Science Review Https://hdsr.mitpress.mit.edu/pub/y95zitmz\nChen Z, Gao Q, Bosselut A, et al (2023) DISCO: Distilling Counterfactuals with Large\nLanguage Models. In: Proceedings of the 61st Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Papers). Association for Compu-\ntational Linguistics, Toronto, Canada, pp 5514–5528, https://doi.org/10.18653/v1/\n2023.acl-long.302, URL https://aclanthology.org/2023.acl-long.302\nChetty R, Jackson MO, Kuchler T, et al (2022a) Social capital I: measurement and\nassociations with economic mobility. Nature 608(7921):108–121. Publisher: Nature\nPublishing Group UK London\nChetty R, Jackson MO, Kuchler T, et al (2022b) Social capital i: measurement and\nassociations with economic mobility. Nature 608(7921):108–121\nDavidson T (2023) Start generating: Harnessing generative artificial intelligence for\nsociological research. SocArXiv\nDemszky D, Yang D, Yeager DS, et al (2023) Using large language models in\npsychology. Nature Reviews Psychology pp 1–14\n30\nDev S, Monajatipoor M, Ovalle A, et al (2021) Harms of Gender Exclusivity and\nChallenges in Non-Binary Representation in Language Technologies. In: Moens\nMF, Huang X, Specia L, et al (eds) Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing. Association for Computa-\ntional Linguistics, Online and Punta Cana, Dominican Republic, pp 1968–1994,\nhttps://doi.org/10.18653/v1/2021.emnlp-main.150, URL https://aclanthology.org/\n2021.emnlp-main.150\nDhingra H, Jayashanker P, Moghe S, et al (2023) Queer People are People First:\nDeconstructing Sexual Identity Stereotypes in Large Language Models. URL http:\n//arxiv.org/abs/2307.00101, arXiv:2307.00101 [cs]\nDominguez-Olmedo R, Hardt M, Mendler-D¨ unner C (2023) Questioning the survey\nresponses of large language models. arXiv preprint arXiv:230607951\nEgami N, Hinck M, Stewart B, et al (2024) Using imperfect surrogates for downstream\ninference: Design-based supervised learning for social science applications of large\nlanguage models. Advances in Neural Information Processing Systems 36\nEichstaedt JC, Schwartz HA, Kern ML, et al (2015) Psychological language on Twitter\npredicts county-level heart disease mortality. Psychological science 26(2):159–169.\nPublisher: Sage Publications Sage CA: Los Angeles, CA\nFeder A, Keith KA, Manzoor E, et al (2022) Causal inference in natural language\nprocessing: Estimation, prediction, interpretation and beyond. Transactions of the\nAssociation for Computational Linguistics 10:1138–1158\nField A, Blodgett SL, Waseem Z, et al (2021) A survey of race, racism, and anti-\nracism in nlp. In: Proceedings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint Conference on Natural\nLanguage Processing (Volume 1: Long Papers)\nGallegos IO, Rossi RA, Barrow J, et al (2023) Bias and Fairness in Large Language\nModels: A Survey. URL http://arxiv.org/abs/2309.00770, arXiv:2309.00770 [cs]\nGasser E, Ahn B, Napoli DJ, et al (2019) Production, perception, and communicative\ngoals of american newscaster speech. Language in Society 48(2):233–259\nGebre-Medhin B, Giebel S, Alvero A, et al (2022) Application essays and the ritual\nproduction of merit in us selective admissions. Poetics 94:101706\nGiebel S, Alvero A, Gebre-Medhin B, et al (2022) Signaled or suppressed? how gen-\nder informs women’s undergraduate applications in biology and engineering. Socius\n8:23780231221127537\nGramsci A (1992) Prison Notebooks, vol 1. Columbia University Press, New York\n31\nGururangan S, Card D, Dreier S, et al (2022) Whose language counts as high quality?\nmeasuring language ideologies in text data selection. In: Proceedings of the 2022\nConference on Empirical Methods in Natural Language Processing, pp 2562–2580\nHofmann V, Kalluri PR, Jurafsky D, et al (2024) Ai generates covertly racist decisions\nabout people based on their dialect. Nature pp 1–8\nHovy D, Bianchi F, Fornaciari T (2020) “you sound just like your father” commercial\nmachine translation systems include stylistic biases. In: Proceedings of the 58th\nAnnual Meeting of the Association for Computational Linguistics, pp 1686–1690\nHuang TJ (2023) Translating authentic selves into authentic applications: Pri-\nvate college consulting and selective college admissions. Sociology of Education p\n00380407231202975\nIreland ME, Slatcher RB, Eastwick PW, et al (2011) Language style matching predicts\nrelationship initiation and stability. Psychological science 22(1):39–44\nKarell D, Sachs J, Barrett R (2023) Synthetic duality: A framework for analyzing\nnatural language generation’s representation of social reality. SocArXiv\nKim J, Lee B (2023) Ai-augmented surveys: Leveraging large language models for opin-\nion prediction in nationally representative surveys. arXiv preprint arXiv:230509620\nKirk HR, Vidgen B, R¨ ottger P, et al (2024a) The benefits, risks and bounds of per-\nsonalizing the alignment of large language models to individuals. Nature Machine\nIntelligence pp 1–10\nKirk HR, Whitefield A, R¨ ottger P, et al (2024b) The prism alignment project: What\nparticipatory, representative and individualised human feedback reveals about the\nsubjective and multicultural alignment of large language models. arXiv preprint\narXiv:240416019\nKoenecke A, Nam A, Lake E, et al (2020) Racial disparities in automated speech\nrecognition. Proceedings of the National Academy of Sciences 117(14):7684–7689\nKotek H, Dockum R, Sun D (2023) Gender bias and stereotypes in Large Language\nModels. In: Proceedings of The ACM Collective Intelligence Conference. ACM, Delft\nNetherlands, pp 12–24, https://doi.org/10.1145/3582269.3615599, URL https://dl.\nacm.org/doi/10.1145/3582269.3615599\nKoyuturk C, Yavari M, Theophilou E, et al (2023) Developing Effective Educational\nChatbots with ChatGPT prompts: Insights from Preliminary Tests in a Case Study\non Social Media Literacy (with appendix)\nLee J, Thymes B, Zhou J, et al (2023) Augmenting Holistic Review in University\nAdmission using Natural Language Processing for Essays and Recommendation\n32\nLetters. arXiv preprint arXiv:230617575\nLee J, Hicke Y, Yu R, et al (in press) The life cycle of large language models in educa-\ntion: A framework for understanding sources of bias. British Journal of Educational\nTechnology\nLeiby BD, Ahner DK (2023) Multicollinearity applied stepwise stochastic imputation:\na large dataset imputation through correlation-based regression. Journal of Big Data\n10(1):23\nLiang W, Yuksekgonul M, Mao Y, et al (2023) Gpt detectors are biased against non-\nnative english writers. Patterns 4(7)\nLiang W, Izzo Z, Zhang Y, et al (2024a) Monitoring ai-modified content at scale: A\ncase study on the impact of chatgpt on ai conference peer reviews. arXiv preprint\narXiv:240307183\nLiang W, Zhang Y, Wu Z, et al (2024b) Mapping the increasing use of llms in scientific\npapers. arXiv preprint arXiv:240401268\nLira B, Gardner M, Quirk A, et al (2023) Using artificial intelligence to assess personal\nqualities in college admissions. Science Advances 9(41):eadg9405\nLucy L, Blodgett SL, Shokouhi M, et al (2023) ”One-size-fits-all”? Observations and\nExpectations of NLG Systems Across Identity-Related Language Features. URL\nhttp://arxiv.org/abs/2310.15398, arXiv:2310.15398 [cs]\nMartin JL (2023) The Ethico-Political Universe of ChatGPT. Journal of Social\nComputing 4(1):1–11. https://doi.org/10.23919/JSC.2023.0003, URL https://\nieeexplore.ieee.org/document/10184066/\nMatelsky JK, Parodi F, Liu T, et al (2023) A large language model-assisted education\ntool to provide feedback on open-ended responses. URL http://arxiv.org/abs/2308.\n02439, arXiv:2308.02439 [cs]\nMesseri L, Crockett M (2024) Artificial intelligence and illusions of understanding in\nscientific research. Nature 627(8002):49–58\nMirowski P, Mathewson KW, Pittman J, et al (2023) Co-Writing Screenplays and\nTheatre Scripts with Language Models: Evaluation by Industry Professionals. In:\nProceedings of the 2023 CHI Conference on Human Factors in Computing Systems,\npp 1–34\nMotoki F, Pinho Neto V, Rodrigues V (2023) More human than human:\nmeasuring ChatGPT political bias. Public Choice https://doi.org/10.1007/\ns11127-023-01097-2, URL https://doi.org/10.1007/s11127-023-01097-2\n33\nM¨ okander J, Schuett J, Kirk HR, et al (2023) Auditing large language models: a three-\nlayered approach. AI and Ethics https://doi.org/10.1007/s43681-023-00289-2, URL\nhttps://link.springer.com/10.1007/s43681-023-00289-2\nNavigli R, Conia S, Ross B (2023) Biases in Large Language Models: Origins,\nInventory, and Discussion. Journal of Data and Information Quality 15(2):1–21.\nhttps://doi.org/10.1145/3597307, URL https://dl.acm.org/doi/10.1145/3597307\nOlson W (1991) The shame of Spanish: Cultural bias in English first legislation.\nChicano-Latino L Rev 11:1. Publisher: HeinOnline\nOmiye JA, Lester JC, Spichak S, et al (2023) Large language models propa-\ngate race-based medicine. npj Digital Medicine 6(1):1–4. https://doi.org/10.1038/\ns41746-023-00939-z, URL https://www.nature.com/articles/s41746-023-00939-z,\nnumber: 1 Publisher: Nature Publishing Group\nPadmakumar V, He H (2023) Does writing with language models reduce content\ndiversity? In: The Twelfth International Conference on Learning Representations\nPayne AL, Austin T, Clemons AM (2024) Beyond the front yard: The dehumanizing\nmessage of accent-altering technology. Applied Linguistics p amae002\nPellert M, Lechner CM, Wagner C, et al (2022) AI Psychometrics: Assessing the\npsychological profiles of large language models through psychometric invento-\nries. preprint, PsyArXiv, https://doi.org/10.31234/osf.io/jv5dt, URL https://osf.\nio/jv5dt\nPennebaker JW, Chung CK, Frazee J, et al (2014) When small words foretell academic\nsuccess: The case of college admissions essays. PloS one 9(12):e115844\nPennebaker JW, Boyd RL, Jordan K, et al (2015) The development and psychometric\nproperties of liwc2015. Tech. rep.\nRajashekar NC, Shin YE, Pu Y, et al (2024) Human-algorithmic interaction using\na large language model-augmented artificial intelligence clinical decision support\nsystem. In: Proceedings of the CHI Conference on Human Factors in Computing\nSystems, pp 1–20\nRoth WD, van Stee EG, Regla-Vargas A (2023) Conceptualizations of race: essential-\nism and constructivism. Annual Review of Sociology 49\nRozado D (2023) The political biases of chatgpt. Social Sciences 12(3):148\nSakirin T, Said RB (2023) User preferences for chatgpt-powered conversational inter-\nfaces versus traditional methods. Mesopotamian Journal of Computer Science\n2023:24–31\n34\nSchneider B (2022) Multilingualism and ai: The regimentation of language in the age\nof digital capitalism. Signs and Society 10(3):362–387\nSejnowski TJ (2023) Large language models and the reverse turing test. Neural\ncomputation 35(3):309–342. Publisher: MIT Press\nSeveriukhina O, Kesarev S, Bochenina K, et al (2020) Large-scale forecasting of\ninformation spreading. Journal of Big Data 7:1–17\nShen H, Wu T (2023) Parachute: Evaluating Interactive Human-LM Co-writing\nSystems. URL http://arxiv.org/abs/2303.06333, arXiv:2303.06333 [cs]\nStevens ML (2009) Creating a class. Harvard University Press, Cambridge\nTao Y, Viberg O, Baker RS, et al (2023) Auditing and mitigating cultural bias in llms.\narXiv preprint arXiv:231114096\nThoppilan R, De Freitas D, Hall J, et al (2022) LaMDA: Language Models for Dialog\nApplications. URL http://arxiv.org/abs/2201.08239, arXiv:2201.08239 [cs]\nT¨ ornberg P (2023) ChatGPT-4 Outperforms Experts and Crowd Workers in Anno-\ntating Political Twitter Messages with Zero-Shot Learning\nWang A, Morgenstern J, Dickerson JP (2024) Large language models cannot replace\nhuman participants because they cannot portray identity groups. arXiv preprint\narXiv:240201908\nWarschauer M, Tseng W, Yim S, et al (2023) The affordances and contradictions of\nAI-generated text for writers of english as a second or foreign language. Journal\nof Second Language Writing 62:101071. https://doi.org/10.1016/j.jslw.2023.101071,\nURL https://www.sciencedirect.com/science/article/pii/S1060374323001091\nWoolard KA (1985) language variation and cultural hegemony: toward an integra-\ntion of sociolinguistic and social theory. American Ethnologist 12(4):738–748. https:\n//doi.org/10.1525/ae.1985.12.4.02a00090, URL https://anthrosource.onlinelibrary.\nwiley.com/doi/10.1525/ae.1985.12.4.02a00090\nWorkshop B, Scao TL, Fan A, et al (2022) Bloom: A 176b-parameter open-access\nmultilingual language model. arXiv preprint arXiv:221105100\nWu S, Irsoy O, Lu S, et al (2023) BloombergGPT: A Large Language Model for\nFinance. URL http://arxiv.org/abs/2303.17564, arXiv:2303.17564 [cs, q-fin]\nYuan A, Coenen A, Reif E, et al (2022) Wordcraft: Story Writing With Large Lan-\nguage Models. In: 27th International Conference on Intelligent User Interfaces.\nACM, Helsinki Finland, pp 841–852, https://doi.org/10.1145/3490099.3511105,\nURL https://dl.acm.org/doi/10.1145/3490099.3511105\n35\nZhang S, Xu J, Alvero A (2024) Generative ai meets open-ended survey responses:\nParticipant use of ai and homogenization. URL osf.io/preprints/socarxiv/4esdp\n36\nAppendix for: Large Language Models, Social\nDemography, and Hegemony: Comparing\nAuthorship in Human and Synthetic Text\nAJ Alvero1*, Jinsook Lee 2, Alejandra Regla-Vargas 3,\nRen´ e Kizilcec4, Thorsten Joachims 5, anthony lising antonio 6*\n1*Center for Data Science for Enterprise and Society, Cornell University.\n2,4-5Department of Information Science, Cornell University.\n3Department of Sociology, University of Pennsylvania.\n5Department of Computer Science, Cornell University.\n6Graduate School of Education, Stanford University.\n*Corresponding author(s). E-mail(s): ajalvero@cornell.edu;\naantonio@stanford.edu;\nContributing authors: jl3369@corne01ll.edu; aregla@sas.upenn.edu;\nkizilcec@cornell.edu; thorsten.joachims@cornell.edu;\nAppendix\n1\nTable A1 Essay Prompts\nPublic Private\nDescribe an example of your leadership experience in\nwhich you have positively influenced others, helped\nresolve disputes, or contributed to group efforts over\ntime.\nSome students have a background, identity, interest, or\ntalent that is so meaningful they believe their applica-\ntion would be incomplete without it. If this sounds like\nyou, then please share your story.\nEvery person has a creative side, and it can be expressed\nin many ways: problem solving, original and innovative\nthinking, and artistically, to name a few. Describe how\nyou express your creative side.\nReflect on a time when you questioned or challenged\na belief or idea. What prompted your thinking? What\nwas the outcome?\nWhat would you say is your greatest talent or skill?\nHow have you developed and demonstrated that talent\nover time?\nReflect on something that someone has done for you\nthat has made you happy or thankful in a surprising\nway. How has this gratitude affected or motivated you?\nDescribe the most significant challenge you have faced\nand the steps you have taken to overcome this challenge.\nHow has this challenge affected your academic achieve-\nment?\nThe lessons we take from obstacles we encounter can be\nfundamental to later success. Recount a time when you\nfaced a challenge, setback, or failure. How did it affect\nyou, and what did you learn from the experience?\nThink about an academic subject that inspires you.\nDescribe how you have furthered this interest inside\nand/or outside of the classroom.\nDescribe a topic, idea, or concept you find so engaging\nthat it makes you lose all track of time. Why does it\ncaptivate you? What or who do you turn to when you\nwant to learn more?\nWhat have you done to make your school or your com-\nmunity a better place?\nDiscuss an accomplishment, event, or realization that\nsparked a period of personal growth and a new under-\nstanding of yourself or others.\nBeyond what has already been shared in your appli-\ncation, what do you believe makes you stand out as\na strong candidate for admissions to the University of\nCalifornia?\nShare an essay on any topic of your choice. It can be one\nyou’ve already written, one that responds to a different\nprompt, or one of your own design.\nDescribe how you have taken advantage of a significant\neducational opportunity or worked to overcome an edu-\ncational barrier you have faced.\n2\nLIWC VariableDefinition Public HumanPublic GPT 3.5Public GPT 4Private HumanPrivate GPT 3.5Private GPT 4\nverb Total number verbs16.27 (2.55) 8.20 (1.45) 6.93 (1.48) 12.94 (2.91) 9.02 (1.94) 7.30 (1.66)affect Affect words5.44 (1.12) 7.73 (1.08) 6.54 (1.23) 5.10 (1.56) 6.78 (1.81) 5.66 (1.62)posemo Positive emotion4.03 (1.01) 6.60 (0.88) 5.24 (1.01) 3.39 (1.22) 4.94 (1.17) 3.99 (1.00)focuspresentPresent focus8.54 (2.33) 4.48 (1.24) 2.99 (1.04) 5.38 (2.12) 3.88 (1.56) 2.76 (1.14)function Function words55.84 (3.28) 44.46 (2.02) 42.23 (2.15) 50.88 (6.23) 47.96 (3.14) 47.36 (2.29)pronoun Total pronouns17.37 (2.44) 12.91 (1.14) 11.51 (1.30) 15.31 (3.20) 13.40 (2.17) 11.20 (1.67)ppron Personal pronouns12.05 (1.83) 10.06 (0.96) 8.24 (1.08) 11.07 (2.66) 9.33 (1.77) 7.06 (1.38)you Second person0.25 (0.36) 0.01 (0.11) 0.01 (0.09) 0.26 (0.49) 0.04 (0.19) 0.02 (0.08)social Social words7.59 (2.10) 6.92 (1.78) 6.28 (1.72) 6.62 (3.02) 5.39 (2.27) 4.17 (1.90)prep Prepositions14.95 (1.39) 14.92 (1.12) 14.50 (1.20) 14.72 (2.18) 15.57 (1.27) 16.13 (1.23)ipron Impersonal pronouns5.32 (1.40) 2.85 (0.64) 3.27 (0.76) 4.24 (1.48) 4.07 (1.03) 4.14 (0.90)drives Drives and needs10.72 (1.99) 14.87 (2.30) 13.44 (2.41) 8.56 (2.61) 11.31 (2.99) 8.72 (2.73)achieve Achievement3.76 (1.23) 6.96 (1.36) 6.00 (1.35) 2.92 (1.55) 5.22 (2.19) 4.12 (1.88)reward Reward focus2.05 (0.72) 2.31 (0.66) 1.56 (0.58) 1.44 (0.75) 1.82 (0.79) 1.24 (0.64)i First person singular9.71 (1.79) 9.21 (1.16) 7.19 (1.18) 8.86 (2.56) 7.91 (2.03) 6.01 (1.41)auxverb Auxiliary verbs8.39 (1.69) 3.39 (1.06) 2.55 (0.89) 5.65 (1.83) 3.88 (1.14) 3.50 (1.00)article Articles 6.22 (1.18) 5.63 (0.97) 6.36 (1.05) 6.90 (1.72) 7.20 (1.23) 9.07 (1.38)bio Biological processes1.54 (0.83) 1.19 (0.65) 1.25 (0.72) 1.99 (1.35) 1.50 (0.84) 1.16 (0.58)sexual Sexuality 0.08 (0.12) 0.25 (0.17) 0.25 (0.19) 0.09 (0.17) 0.33 (0.30) 0.23 (0.21)adj Adjectives 5.05 (0.93) 5.60 (0.88) 4.88 (0.97) 4.98 (1.24) 4.59 (0.97) 4.37 (0.94)percept Perceptual processes1.88 (0.78) 1.56 (0.62) 1.56 (0.62) 2.60 (1.31) 1.87 (0.99) 2.07 (1.25)feel Feeling 0.55 (0.33) 0.17 (0.17) 0.23 (0.22) 0.70 (0.53) 0.40 (0.34) 0.32 (0.27)work Work 6.75 (2.10) 12.16 (2.08) 10.96 (2.31) 4.20 (2.41) 6.75 (2.48) 5.76 (2.13)affiliation Affiliation 2.75 (1.17) 3.66 (1.39) 3.75 (1.38) 2.37 (1.54) 2.80 (1.76) 2.21 (1.40)cogproc Cognitive processes12.82 (2.24) 9.94 (1.70) 10.48 (1.67) 11.25 (2.77) 11.72 (1.91) 11.13 (1.85)cause Cause 2.55 (0.79) 2.83 (0.83) 2.78 (0.86) 1.95 (0.91) 2.15 (0.92) 1.74 (0.77)power Power 3.63 (0.97) 4.13 (1.02) 3.71 (1.12) 2.73 (1.14) 3.24 (1.30) 2.39 (1.11)insight Insight 3.26 (0.88) 4.08 (1.04) 4.07 (0.96) 3.23 (1.22) 4.84 (1.20) 4.45 (1.14)relativ Relativity 13.67 (1.87) 11.39 (1.48) 12.75 (1.71) 14.79 (2.95) 13.07 (1.92) 12.62 (1.78)space Space 6.46 (1.09) 7.04 (1.26) 7.48 (1.30) 6.91 (1.67) 7.19 (1.51) 7.18 (1.35)negate Negations 1.27 (0.55) 0.31 (0.22) 0.80 (0.32) 0.80 (0.51) 0.66 (0.37) 1.14 (0.43)differ Differentiation2.89 (0.88) 1.63 (0.61) 2.14 (0.70) 2.36 (0.95) 2.29 (0.96) 2.76 (0.88)interrog Interrogatives1.65 (0.57) 0.81 (0.35) 0.79 (0.38) 1.40 (0.66) 0.80 (0.43) 0.87 (0.44)see Seeing 0.65 (0.43) 0.41 (0.33) 0.47 (0.33) 1.02 (0.77) 0.52 (0.41) 0.67 (0.46)conj Conjunctions6.90 (1.10) 7.04 (0.83) 6.12 (0.92) 6.12 (1.34) 6.75 (1.11) 5.84 (0.99)focuspast Past focus 5.70 (1.69) 2.87 (0.93) 2.83 (0.94) 5.67 (2.12) 3.99 (1.72) 3.49 (1.29)focusfutureFuture focus0.87 (0.41) 0.27 (0.23) 0.31 (0.26) 0.75 (0.47) 0.62 (0.40) 0.67 (0.38)time Time 5.40 (1.25) 2.81 (0.88) 3.25 (1.03) 5.87 (1.71) 3.54 (1.17) 3.42 (1.04)adverb Common adverbs4.16 (1.05) 2.20 (0.63) 2.64 (0.74) 3.98 (1.27) 2.86 (0.82) 2.92 (0.78)motion Motion 1.99 (0.61) 1.60 (0.52) 2.07 (0.62) 2.29 (0.96) 2.52 (0.83) 2.20 (0.72)relig Religion 0.13 (0.24) 0.04 (0.09) 0.13 (0.16) 0.15 (0.41) 0.15 (0.27) 0.23 (0.23)negemo Negative emotion1.31 (0.64) 0.89 (0.52) 1.04 (0.55) 1.62 (0.99) 1.51 (1.24) 1.41 (1.11)tentat Tentativeness1.89 (0.73) 0.67 (0.35) 0.68 (0.40) 1.76 (0.86) 1.17 (0.64) 1.33 (0.63)compare Comparatives2.82 (0.71) 2.28 (0.56) 1.85 (0.61) 2.85 (0.93) 2.32 (0.67) 2.25 (0.69)quant Quantifiers 2.20 (0.63) 1.26 (0.43) 1.15 (0.50) 1.98 (0.81) 1.20 (0.53) 1.39 (0.58)number Numbers 1.11 (0.50) 0.31 (0.25) 0.28 (0.28) 0.99 (0.58) 0.38 (0.28) 0.41 (0.29)certain Certainty 1.72 (0.62) 1.05 (0.42) 0.92 (0.42) 1.56 (0.72) 1.25 (0.52) 0.99 (0.45)leisure Leisure 1.47 (1.01) 1.38 (0.77) 1.31 (0.73) 1.58 (1.30) 1.10 (1.09) 1.27 (1.11)health Health/illness0.72 (0.54) 0.37 (0.37) 0.44 (0.41) 0.74 (0.74) 0.56 (0.55) 0.42 (0.36)risk Risk focus 0.51 (0.32) 0.44 (0.31) 0.47 (0.33) 0.57 (0.45) 0.78 (0.71) 0.70 (0.63)discrep Discrepancies1.56 (0.62) 0.49 (0.31) 0.55 (0.35) 1.38 (0.72) 1.03 (0.49) 0.82 (0.45)anx Anxiety 0.37 (0.28) 0.22 (0.25) 0.35 (0.32) 0.47 (0.47) 0.40 (0.52) 0.34 (0.44)anger Anger 0.19 (0.20) 0.12 (0.16) 0.18 (0.20) 0.27 (0.32) 0.18 (0.23) 0.23 (0.25)body Body 0.35 (0.32) 0.15 (0.17) 0.18 (0.19) 0.67 (0.67) 0.41 (0.39) 0.31 (0.27)hear Hearing 0.47 (0.48) 0.42 (0.44) 0.37 (0.42) 0.58 (0.75) 0.30 (0.72) 0.69 (1.10)money Money 0.36 (0.36) 0.32 (0.34) 0.30 (0.34) 0.38 (0.52) 0.26 (0.34) 0.18 (0.25)informal Informal speech0.18 (0.17) 0.09 (0.12) 0.09 (0.16) 0.18 (1.00) 0.06 (0.12) 0.07 (0.14)nonflu Nonfluencies0.12 (0.13) 0.08 (0.12) 0.03 (0.08) 0.07 (0.12) 0.04 (0.09) 0.02 (0.07)we First person plural0.68 (0.60) 0.48 (0.48) 0.76 (0.62) 0.65 (0.84) 0.80 (0.99) 0.64 (0.81)friend Friends 0.21 (0.20) 0.18 (0.18) 0.15 (0.19) 0.24 (0.29) 0.11 (0.16) 0.07 (0.13)shehe Third person singular0.52 (0.62) 0.01 (0.06) 0.01 (0.07) 0.70 (1.09) 0.15 (0.41) 0.11 (0.31)male Male referents0.48 (0.60) 0.07 (0.12) 0.04 (0.11) 0.70 (1.12) 0.15 (0.44) 0.12 (0.33)they Third person plural0.89 (0.58) 0.36 (0.31) 0.26 (0.26) 0.60 (0.58) 0.44 (0.45) 0.28 (0.28)home Home 0.44 (0.36) 0.19 (0.21) 0.25 (0.29) 0.52 (0.56) 0.20 (0.29) 0.22 (0.32)family Family 0.68 (0.61) 0.12 (0.18) 0.13 (0.21) 0.71 (0.88) 0.12 (0.27) 0.11 (0.23)female Female referents0.51 (0.64) 0.01 (0.08) 0.03 (0.11) 0.58 (1.07) 0.11 (0.42) 0.08 (0.27)sad Sadness 0.29 (0.25) 0.17 (0.24) 0.16 (0.21) 0.40 (0.39) 0.51 (0.73) 0.44 (0.56)netspeak Netspeak 0.02 (0.06) 0.01 (0.04) 0.05 (0.13) 0.04 (0.14) 0.01 (0.06) 0.03 (0.11)assent Assent 0.03 (0.06) 0.00 (0.02) 0.00 (0.03) 0.05 (0.98) 0.01 (0.04) 0.02 (0.06)ingest Ingesting 0.25 (0.31) 0.29 (0.29) 0.23 (0.36) 0.40 (0.77) 0.15 (0.31) 0.17 (0.28)death Death 0.05 (0.10) 0.04 (0.10) 0.02 (0.07) 0.07 (0.18) 0.02 (0.06) 0.03 (0.09)swear Swear words0.01 (0.03) 0.00 (0.00) 0.00 (0.01) 0.01 (0.07) 0.00 (0.02) 0.00 (0.03)Analytic Analytical thinking13.08 (6.55) 24.71 (3.23) 27.24 (3.33) 19.01 (7.57) 24.12 (6.52) 30.61 (4.48)SemiC Semicolons 0.10 (0.16) 0.01 (0.04) 0.18 (0.20) 0.15 (0.22) 0.08 (0.13) 0.29 (0.23)Period Periods 4.94 (0.94) 5.28 (0.42) 4.88 (0.46) 5.22 (1.18) 4.91 (0.58) 4.49 (0.58)Sixltr Words longer than 6letters 19.78 (3.48) 39.31 (2.57) 40.06 (2.81) 20.86 (4.75) 33.18 (4.12) 34.69 (3.26)\nTable A2 LIWC Features and definition mean and SD\n3\nGroups LIWC Features GPT4 (Public) GPT4 (Private) GPT3.5 (Public) GPT3.5 (Private)\nGender\nnegemo male male male male\npercept male male male male\nfamily male male male male\nfemale male male male male\nfocuspast male female male female\npronoun male male male male\nquant female female female female\nmale female female female female\ntime female female female female\nsocial male male male male\nfeel male male male male\nbody male male male male\ni male male male male\nverb male female male female\nachieve male male male male\ningest male male female male\nnegate male female male male\nppron male male male male\nbio male male male male\nhear male female male male\nleisure female female female female\nhome male male male male\nanx male male male male\nshehe male male male male\nPeriod male male female male\narticle male male female male\ncompare female female female female\nreward female female male male\nwork male male male male\nFirst Gen\nnumber First Gen First Gen First Gen First Gen\ndiscrep Cont. Gen Cont. Gen Cont. Gen Cont. Gen\narticle First Gen Cont. Gen First Gen Cont. Gen\ni Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nAnalytic Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nprep First Gen Cont. Gen First Gen Cont. Gen\nsocial Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nsexual Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nsad Cont. Gen First Gen Cont. Gen First Gen\npronoun Cont. Gen Cont. Gen Cont. Gen Cont. Gen\ninterrog Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nipron Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nfocusfuture Cont. Gen Cont. Gen Cont. Gen Cont. Gen\naffect Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nyou Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nppron Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nwork First Gen Cont. Gen First Gen Cont. Gen\nauxverb Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nrelative First Gen First Gen First Gen First Gen\nhealth First Gen Cont. Gen First Gen Cont. Gen\nadj First Gen First Gen Cont. Gen First Gen\nfemale Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nposemo Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nverb Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nfocuspresent Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nquant First Gen First Gen First Gen First Gen\nnegate Cont. Gen First Gen Cont. Gen Cont. Gen\nspace First Gen Cont. Gen Cont. Gen Cont. Gen\nfriend First Gen First Gen First Gen First Gen\nfunction First Gen Cont. Gen Cont. Gen Cont. Gen\nleisure First Gen First Gen First Gen First Gen\nthey Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nfamily Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nadverb First Gen Cont. Gen Cont. Gen Cont. Gen\nhome Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nSixltr Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nfocuspast Cont. Gen Cont. Gen Cont. Gen Cont. Gen\nEC\narticle High EC High EC Low EC High EC\nAnalytic High EC High EC High EC High EC\ni High EC High EC High EC High EC\nprep Low EC High EC Low EC High EC\npronoun High EC High EC High EC High EC\ninterrog High EC High EC High EC High EC\nipron High EC High EC High EC High EC\nfocusfuture High EC High EC High EC High EC\nyou High EC High EC High EC High EC\nppron High EC High EC High EC High EC\nauxverb High EC High EC High EC High EC\nhealth Low EC High EC Low EC High EC\nadj Low EC Low EC High EC Low EC\nposemo High EC High EC High EC High EC\nverb High EC High EC High EC High EC\nfocuspresent High EC High EC High EC High EC\nnegate High EC Low EC High EC High EC\nspace High EC High EC High EC High EC\nfunction High EC High EC High EC High EC\nleisure Low EC Low EC Low EC Low EC\nfamily High EC High EC High EC High EC\nachieve Low EC High EC Low EC High EC\nadverb High EC High EC High EC High EC\nSixltr High EC High EC High EC High EC\nfocuspast High EC High EC High EC High EC\nTable A3 LIWC features and demographic subgroups are closer to each feature across GPT\nwritten essays\n4",
  "topic": "Hegemony",
  "concepts": [
    {
      "name": "Hegemony",
      "score": 0.667409360408783
    },
    {
      "name": "Sociology",
      "score": 0.4332050681114197
    },
    {
      "name": "Demography",
      "score": 0.36504828929901123
    },
    {
      "name": "Geography",
      "score": 0.32349637150764465
    },
    {
      "name": "Political science",
      "score": 0.28212568163871765
    },
    {
      "name": "Politics",
      "score": 0.06734615564346313
    },
    {
      "name": "Law",
      "score": 0.06350395083427429
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I33213144",
      "name": "University of Florida",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I79576946",
      "name": "University of Pennsylvania",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ]
}