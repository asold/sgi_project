{
  "title": "Utilizing Bidirectional Encoder Representations from Transformers for Answer Selection",
  "url": "https://openalex.org/W3098382480",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5059261409",
      "name": "Tahmid Rahman Laskar",
      "affiliations": [
        "York University"
      ]
    },
    {
      "id": "https://openalex.org/A5067722075",
      "name": "Enamul Hoque",
      "affiliations": [
        "York University"
      ]
    },
    {
      "id": "https://openalex.org/A5000409439",
      "name": "Jimmy Xiangji Huang",
      "affiliations": [
        "York University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998448492",
    "https://openalex.org/W2788343755",
    "https://openalex.org/W2799027006",
    "https://openalex.org/W2739983396",
    "https://openalex.org/W2060816264",
    "https://openalex.org/W2141520705",
    "https://openalex.org/W2948917253",
    "https://openalex.org/W3028907449",
    "https://openalex.org/W3022773562",
    "https://openalex.org/W2166045895",
    "https://openalex.org/W1983286042",
    "https://openalex.org/W2116401198",
    "https://openalex.org/W6600424091",
    "https://openalex.org/W2998320111",
    "https://openalex.org/W2915240437",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2984469754",
    "https://openalex.org/W2788835992",
    "https://openalex.org/W2737434030",
    "https://openalex.org/W2775696384",
    "https://openalex.org/W2598684926",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W2076039929",
    "https://openalex.org/W2067505258",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2539338396",
    "https://openalex.org/W2120735855",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3096783161",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2915633464",
    "https://openalex.org/W2118091490",
    "https://openalex.org/W3029927342",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2251921768",
    "https://openalex.org/W2989712500",
    "https://openalex.org/W2998847961",
    "https://openalex.org/W2995025784",
    "https://openalex.org/W2049653588",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3103283534",
    "https://openalex.org/W3101747393"
  ],
  "abstract": null,
  "full_text": "Utilizing Bidirectional Encoder Representations\nfrom Transformers for Answer Selection\nMd Tahmid Rahman Laskar, Enamul Hoque and Jimmy Xiangji Huang\nAbstract Pre-training a transformer-based model for the language modeling task\nin a large dataset and then ﬁne-tuning it for downstream tasks has been found very\nuseful in recent years. One major advantage of such pre-trained language models is\nthat they can effectively absorb the context of each word in a sentence. However,\nfor tasks such as the answer selection task, the pre-trained language models have\nnot been extensively used yet. To investigate their effectiveness in such tasks, in this\npaper, we adopt the pre-trained Bidirectional Encoder Representations from Trans-\nformer (BERT) language model and ﬁne-tune it on two Question Answering (QA)\ndatasets and three Community Question Answering (CQA) datasets for the answer\nselection task. We ﬁnd that ﬁne-tuning the BERT model for the answer selection\ntask is very effective and observe a maximum improvement of 13.1% in the QA\ndatasets and 18.7% in the CQA datasets compared to the previous state-of-the-art.\n1 Introduction\nThe Answer Selection task is a fundamental problem in the areas of Information\nRetrieval and Natural Language Processing (NLP) [35]. Given a question along with\nMd Tahmid Rahman Laskar\nDepartment of Electrical Engineering and Computer Science, York University, Toronto, Canada\nInformation Retrieval & Knowledge Management Research Lab, York University, Toronto, Canada\ne-mail: tahmedge@cse.yorku.ca\nEnamul Hoque\nSchool of Information Technology, York University, Toronto, Canada\ne-mail: enamulh@yorku.ca\nJimmy Xiangji Huang\nSchool of Information Technology, York University, Toronto, Canada\nInformation Retrieval & Knowledge Management Research Lab, York University, Toronto, Canada\ne-mail: jhuang@yorku.ca\n1\narXiv:2011.07208v1  [cs.CL]  14 Nov 2020\n2 Md Tahmid Rahman Laskar, Enamul Hoque and Jimmy Xiangji Huang\nTable 1 An example of the Answer Selection task. A question along with a list of candidate\nanswers are given. The sentence in the bold font is the correct answer.\nQuestion:\n• Who is the winner of the US Open 2019?\nList of Candidate Answers:\n• Rafael Nadal has won the French Open 2019.\n• Rafael Nadal has won the US Open 2019.\n• Roger Federer has won the Australian Open 2018.\nPotential Ranking:\n• Rafael Nadal has won the US Open 2019.\n• Rafael Nadal has won the French Open 2019.\n• Roger Federer has won the Australian Open 2018.\na list of candidate answers, the objective in the answer selection task is to rank\nthe candidate answers based on their relevance with the given question [11] (see\nTable 1). In such tasks, the relevance between a question and a candidate answer is\nmeasured by various sentence similarity modeling techniques [35].\nIn recent years, various sentence similarity models based on the neural network\narchitecture have been utilized to measure the similarity between the question and\nthe candidate answer [2, 3, 26]. In such neural models, ﬁrst, the word embedding\n(GloVe [23] or Word2Vec [21]) representations of the question and the candidate\nanswer are used as input to the model. Then the vector representations of these\nsentences produced by the neural model are utilized for the similarity calculation [2,\n3]. However, such word embeddings can only provide a ﬁxed representation of a\nword and fail to capture its context. Very recently, pre-trained language models have\nreceived a lot of attention as they can provide contextual representations of each\nword in different sentences [5, 24]. Among the pre-trained language models, ﬁne-\ntuning the transformer-based [31] BERT model yields state-of-the-art performance\nacross different NLP tasks [5]. However, the ﬁne-tuned BERT model is not deeply\ninvestigated for the answer selection task yet [11].\nTo be noted that, there are some issues to address regarding ﬁne-tuning a pre-\ntrained model in a new dataset. For instance, the BERT model has been pre-trained\nin two scenarios: a) when casing information was present, and b) when casing infor-\nmation was absent. Since it is not guaranteed that all datasets will have conventional\ncasing information, it is important to build models that are robust in scenarios when\ncasing information is missing [19]. In addition, it has been observed that neural mod-\nels which are trained in datasets having conventional casing perform very poorly in\nthe test data for tasks such as named entity recognition [1] when the conventional\ncasing is absent [19]. Thus, to address the above issues, in this paper, we ﬁne-tune\nUtilizing BERT for Answer Selection 3\nboth the cased and uncased versions of the BERT model for the answer selection\ntask. More concretely, our contributions presented in this paper are the following:\n• First, we conduct extensive experiments in ﬁve datasets by ﬁne-tuning the BERT\nmodel for the answer selection task and observe that the ﬁne-tuned BERT model\noutperforms all prior work where pre-trained language models were not utilized.\n• Second, we show that the cased model of BERT for answer selection is as effec-\ntive as its uncased counterpart in scenarios when casing information is absent.\n• Finally, we conduct ablation study to further investigate the effectiveness of\nﬁne-tuning BERT for answer selection. As a secondary contribution, we have\nmade our source codes publicly available here: https://github.com/\ntahmedge/BERT-for-Answer-Selection\n2 Related Work\nEarlier, various feature engineering-based approaches have been utilized for the an-\nswer selection task [27, 35]. However, the feature engineering-based approaches re-\nquire lots of handcrafted rules and are often error-prone [3]. Also, the features which\nare used in one dataset are not robust in other datasets [3].\nIn recent years, several models based on deep neural network have been applied\nfor the answer selection task and they showed impressive performance without re-\nquiring any handcrafted features [2, 3, 4, 9, 25, 26, 29]. To be noted that, these deep\nneural network models for answer selection mostly utilized the Recurrent Neural\nNetwork (RNN) architecture. However, very recently, models based on the trans-\nformer architecture [31] have outperformed the previously proposed RNN-based\nmodels in several NLP tasks [5, 18]. Though these transformer-based models uti-\nlized the pre-trained BERT architecture [5], models based on BERT have not been\ndeeply investigated for the answer selection task yet. Moreover, it was found that\nneural models trained on case sensitive texts performed poorly in scenarios when the\nconventional casing was missing in the test data [19]. Therefore, to address these is-\nsues, we utilize both the cased and uncased versions of the pre-trained BERT model\nand investigate its generalized effectiveness by conducting extensive experiments in\nﬁve answer selection datasets.\n3 Utilizing BERT for Answer Selection\nIn this section, we ﬁrst discuss the transformer encoder [31] which was utilized in\nBERT [5]. Then we brieﬂy describe how the BERT model was pre-trained, followed\nby demonstrating our approach of ﬁne-tuning the pre-trained BERT model for the\nanswer selection task.\n4 Md Tahmid Rahman Laskar, Enamul Hoque and Jimmy Xiangji Huang\n3.1 Transformer Encoder\nThe transformer model has an encoder which reads the text input and a decoder\nwhich produces the predicted output of the input text [31]. The BERT model only\nutilizes the encoder of transformer [5]. The transformer encoder uses the self-\nattention mechanism to represent each token in a sentence based on other tokens.\nThis self-attention mechanism works by creating three vectors for each token, which\nare: a query vector Q, a key vector K, and a value vector V. These three vectors were\ncreated by multiplying the embedding vector xi with three weight matrices (W Q,\nWK, WV) respectively. If dk is the dimension of the key and query vectors, then the\noutput Z of self-attention for each word is calculated based on the following:\nZ = so f tmax\n(\nQ ×KT\n√dk\n)\nV (1)\nSince the transformer encoder uses multi-head attention mechanism to give at-\ntention on different positions, the self attention is computed eight times with eight\ndifferent weight matrices which provides eight Z matrices. Then the eight Z matri-\nces are concatenated into a single matrix which is later multiplied with an additional\nweight matrix in order to send the resulting matrix to a feed-forward layer [31].\n3.2 Pre-training the BERT Model\nThe BERT model adopts the encoder of the transformer architecture [31]. The en-\ncoder of BERT was pre-trained for masked language modeling and the next sentence\nprediction task on the BooksCorpus (800M words) [38] dataset along with the En-\nglish Wikipedia (2,500M words) [5]. For the masked language modeling task, 15%\ntokens in each input sequence are replaced with the special [MASK] token. The\nmodel then learns to predict the original value of the masked words based on the\ncontext provided by the non-masked words in the input sequence. In the next sen-\ntence prediction task, the model receives a pair of sentences as input and attempts\nto predict if the second sentence in the input pair is a subsequent sentence in the\noriginal document.\n3.3 Fine-tuning BERT for Answer Selection\nLet’s assume that we have two sentencesX = x1, x2, ...,xm and Y = y1, y2, ...,yn. To\ninput them into the BERT model, they are combined together into a single sequence\nwhere a special token [SEP] is added at the end of each sentence. Another special\ntoken [CLS] is added at the beginning of the sequence. The ﬁne-tuning process of\nthe BERT model for the answer selection task is shown in Figure 1.\nUtilizing BERT for Answer Selection 5\nFig. 1 BERT Fine Tuning: The question X and the candidate answer Y are combined together as\ninput to the pre-trained BERT model for ﬁne-tuning.\nIn the ﬁne-tuned BERT model, the representation of the ﬁrst token ([CLS]), which\nis regarded as the aggregate representation of the sequence, is considered as the\noutput of the classiﬁcation layer. For ﬁne-tuning, parameters are added to the pre-\ntrained BERT model for the additional classiﬁcation layer W. All the parameters of\nthe pre-trained BERT model along with the additional parameters for the classiﬁer\nW are ﬁne-tuned jointly to maximize the log-probability of the correct label. The\nprobability of each label P ∈RK (where K is the total number of classiﬁer labels) is\ncalculated as follows:\nP = so f tmax(CWT ) (2)\nIn the answer selection task, there are two classiﬁer labels (where 1 indicates that\nthe candidate answer is relevant to the question, and 0 indicates the opposite). In the\noriginal BERT model [5], sentence pair classiﬁcation task was done by determining\nthe correct label. But in this paper, we modify the ﬁnal layer by following the ap-\nproach of [11] and only consider the predicted score Ptr for the similarity label to\nrank the answers based on their similarities with the question.\nPtr = P(C = 1|X,Y ) (3)\n4 Experimental Setup\nIn this section, we present the datasets, the training parameters, and the evaluation\nmetrics used in our experiments. To note that all experiments were run using Nvidia\nV100 with 4 GPUs.\n4.1 Datasets\nIn our experiments, we used ﬁve datasets: two of them were Question Answering\n(QA) datasets whereas rest were Community Question Answering (CQA) datasets.\nThe overall statistics of the datasets are shown in Table 2. In the following, we give\na brief description of each dataset.\n6 Md Tahmid Rahman Laskar, Enamul Hoque and Jimmy Xiangji Huang\nTable 2 An overview of the datasets used in our experiments. Here, “#” denotes “number of”.\nDataset # Questions # Candidate Answers\nTrain Valid Test Train Valid Test\nTREC-QA 1229 82 100 53417 1148 1517\nWikiQA 873 126 243 8672 1130 2351\nYahooCQA 50112 6289 6283 253440 31680 31680\nSemEval-2016CQA 4879 244 327 36198 2440 3270\nSemEval-2017CQA 4879 244 293 36198 2440 2930\nTREC-QA: The TREC-QA dataset is created from the QA track (8-13) of Text\nREtrieval Conference [32].\nWikiQA: The WikiQA is an open domain QA dataset [34] in which the answers\nwere collected from the Wikipedia.\nYahooCQA:The YahooCQA1 dataset is a community-based question answering\ndataset. In this CQA dataset, each question is associated with at most one correct\nanswer and four negative answers [29].\nSemEval-2016CQA: This is also a CQA dataset which is created from the Qatar\nLiving Forums2. Each candidate answer is tagged with “Good”, “Bad” or “Poten-\ntially Useful”. We consider “Good” as positive and other tags as negative [14, 28].\nSemEval-2017CQA: The training and validation data in this CQA dataset is\nsame as SemEval-2016CQA. However, the test sets are different [22].\n4.2 Training Parameters and Evaluation Metrics\nWe used both the cased and uncased models3 of BERTLarge and ﬁne-tuned them for\nthe pairwise sentence classiﬁcation task [5]. The parameters of the BERTLarge model\nwere: number of layers L = 24, hidden size H = 1024, number of self-attention\nheads A = 16, feed-forward layer size dff = 4096. For implementation, we used the\nTransformer library of Huggingface4 [33]. For training, we used cross entropy loss\nfunction to calculate the loss and utilized Adam as the optimizer. We set the batch\nsize to 16 and ran 2 epochs with learning rate being set to 2 ×10−5. We selected\nthe model for evaluation which performed the best in the validation set. To evaluate\nour models, we used the Mean Average Precision (MAP) and the Mean Reciprocal\nRank (MRR) as the evaluation metrics.\n1 https://webscope.sandbox.yahoo.com/catalog.php?datatype=l&did=10\n2 https://www.qatarliving.com/forum\n3 https://huggingface.co/transformers/pretrained_models.html\n4 https://github.com/huggingface/transformers\nUtilizing BERT for Answer Selection 7\nTable 3 Performance comparisons with the recent progress. Here, ‘FT’ denotes ‘Fine Tuning’.\nQA datasets CQA datasets\nModel TREC-QA WikiQA YahooCQA SemEval'16 SemEval'17\nMAP MRR MAP MRR MAP MRR MAP MRR MAP MRR\nKamath et al. [9] 0.852 0.891 - - - - - - - -\nSha et al. [28] - - 0.746 0.758 - - 0.801 0.872 - -\nTay et al. [30] - - - - - 0.801 - - - -\nNakov et al. [22] - - - - - - - - 0.884 0.928\nBERTLarge (Cased) FT 0.934 0.966 0.842 0.856 0.946 0.946 0.841 0.894 0.908 0.934\nBERTLarge (Uncased) FT 0.917 0.947 0.843 0.857 0.951 0.951 0.866 0.927 0.921 0.963\n5 Results and Analyses\nTo evaluate the performance of ﬁne-tuning the BERT model in the answer selection\ndatasets, we compare its performance with various state-of-the-art models [9, 22,\n28, 30]. We also conduct ablation studies to further investigate the effectiveness of\nﬁne-tuning. To note that, we pre-processed all datasets into the lower-cased format\nand evaluated with both the cased and uncased versions of the BERT model.\n5.1 Performance Comparisons\nWe show the results of our models in Table 3. We ﬁnd that in comparison to the prior\nwork in the TREC-QA dataset, the ﬁne-tuned BERTLarge (Cased) model performs the\nbest and outperforms the previous state-of-the-art [9] with an improvement of 9.6%\nin terms of MAP and an improvement of 8.4% in terms of MRR. However, in the\nWikiQA dataset, the uncased version performs the best in terms of both MAP and\nMRR. More speciﬁcally, BERT Large (Uncased) model improves the performance by\n13% in terms of MAP and 13.1% in terms of MRR compared to the previous state-\nof-the-art [28] in the WikiQA dataset.\nIn the CQA datasets, we again observe that both models outperform the prior\nwork. In terms of MRR, we ﬁnd that the BERTLarge (Uncased) model outperforms [30],\n[28], and [22] with an improvement of 18.7%, 6.3%, and 3.8% in the YahooCQA,\nSemEval-2016CQA, and SemEval-2017CQA datasets respectively.\nWhile comparing between the cased model and the uncased model, we ﬁnd that\neven though the cased model outperforms the uncased model in the TREC-QA\ndataset, it fails to outperform the uncased model in rest other datasets. To be noted\nthat, the cased model still provides competitive performance in comparison to the\nuncased model in all ﬁve datasets. In order to better analyze the performance of\nthese two models, we conduct signiﬁcant test. Based on the paired t-test, we ﬁnd\nthat the performance difference between the two models is not statistically signiﬁ-\ncant (p ≤.05). This indicates that the cased version of the ﬁne-tuned BERT model\nis robust in scenarios when the datasets do not contain any casing information.\n8 Md Tahmid Rahman Laskar, Enamul Hoque and Jimmy Xiangji Huang\nTable 4 Performance comparisons based on the Ablation Test. Here, ‘FT’ denotes ‘Fine Tuning’.\nQA datasets CQA datasets\nModel TREC-QA WikiQA YahooCQA SemEval'16 SemEval'17\nMAP MRR MAP MRR MAP MRR MAP MRR MAP MRR\nBERTLarge (Uncased) FT 0.917 0.947 0.843 0.857 0.951 0.951 0.866 0.927 0.921 0.963\nwithout FT 0.405 0.476 0.566 0.571 0.436 0.436 0.604 0.670 0.698 0.757\n5.2 Ablation Studies\nWe perform ablation test to investigate the effectiveness of our approach of ﬁne-\ntuning the BERT model. For the ablation test, we excluded ﬁne-tuning and only used\nthe feature-based embeddings generated from the pre-trained BERT Large (Uncased)\nmodel. In our ablation study, we used all ﬁve datasets to compare the performance.\nFrom the ablation test (see Table 4), we ﬁnd that removing ﬁne tuning from BERT\ndecreases the performance by 55.8%, 32.9%, 54.2%, 30.3%, and 24.2% in terms of\nMAP in the TREC-QA, WikiQA, YahooCQA, SemEval-2016CQA, and SemEval-\n2017CQA datasets respectively. The deterioration here without ﬁne-tuning isstatis-\ntically signiﬁcant based on paired t-test (p ≤.05).\n6 Conclusions and Future Work\nIn this paper, we adopt the pre-trained BERT model and ﬁne-tune it for the an-\nswer selection task in ﬁve answer selection datasets. We observe that ﬁne-tuning the\nBERT model for answer selection is very effective and ﬁnd that it outperforms all\nthe RNN-based models used previously for such tasks. In addition, we evaluate the\neffectiveness of the cased version of the BERT model in scenarios when the casing\ninformation is not present in the target dataset and demonstrate that the cased model\nprovides almost similar performance compare to the uncased model. We further in-\nvestigate the effectiveness of ﬁne-tuning the BERT model by conducting ablation\nstudies and observe that ﬁne-tuning signiﬁcantly improves the performance for the\nanswer selection task.\nIn the future, we will investigate the performance of different models [14] based\non the transformer architecture on other tasks, such as information retrieval ap-\nplications [6, 7, 8, 20, 36], sentiment analysis [16, 37], learning from imbalanced\ndatasets [15], query-focused abstractive text summarization [12, 13], real-world ap-\nplications [17], and automatic chart question answering [10].\nAcknowledgements This research is supported by the Natural Sciences & Engineering Research\nCouncil (NSERC) of Canada and an ORF-RE (Ontario Research Fund-Research Excellence)\naward in BRAIN Alliance. We acknowledge Compute Canada for providing us with the computing\nresources and also thank Dr. Qin Chen for helping us with the experiments.\nUtilizing BERT for Answer Selection 9\nReferences\n1. M. S. Bari, S. Joty, and P. Jwalapuram. Zero-Resource Cross-Lingual Named Entity Recog-\nnition. arXiv preprint arXiv:1911.09812, 2019.\n2. Q. Chen, Q. Hu, J. X. Huang, and L. He. CA-RNN: Using context-aligned recurrent neural\nnetworks for modeling sentence similarity. In Thirty-Second AAAI Conference on Artiﬁcial\nIntelligence, 2018.\n3. Q. Chen, Q. Hu, J. X. Huang, and L. He. CAN: Enhancing sentence similarity modeling with\ncollaborative and adversarial network. In Proceedings of the 41st International ACM SIGIR\nConference on Research & Development in Information Retrieval, pages 815–824, 2018.\n4. Q. Chen, Q. Hu, J. X. Huang, L. He, and W. An. Enhancing recurrent neural networks with\npositional attention for question answering. In Proceedings of the 40th International ACM\nSIGIR Conference on Research and Development in Information Retrieval , pages 993–996,\n2017.\n5. J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 4171–4186, 2019.\n6. X. Huang and Q. Hu. A bayesian learning approach to promoting diversity in ranking for\nbiomedical information retrieval. In Proceedings of the 32nd Annual International ACM SI-\nGIR Conference on Research and Development in Information Retrieval, , pages 307–314,\n2009.\n7. X. Huang, F. Peng, D. Schuurmans, N. Cercone, and S. E. Robertson. Applying machine\nlearning to text segmentation for information retrieval. Information Retrieval, 6(3-4):333–\n362, 2003.\n8. X. Huang, M. Zhong, and L. Si. York University at TREC 2005: Genomics track. In Proceed-\nings of the Fourteenth Text REtrieval Conference, TREC, 2005.\n9. S. Kamath, B. Grau, and Y . Ma. Predicting and integrating expected answer types into a\nsimple recurrent neural network model for answer sentence selection. In 20th International\nConference on Computational Linguistics and Intelligent Text Processing, 2019.\n10. D. H. Kim, E. Hoque, and M. Agrawala. Answering questions about charts and generating\nvisual explanations. In Proceedings of the 2020 CHI Conference on Human Factors in Com-\nputing Systems, pages 1–13, 2020.\n11. M. T. R. Laskar, E. Hoque, and J. Huang. Utilizing bidirectional encoder representations\nfrom transformers for answer selection task. In Proceedings of the V AMMCS International\nConference: Extended Abstract, page 221, 2019.\n12. M. T. R. Laskar, E. Hoque, and J. Huang. Query focused abstractive summarization via in-\ncorporating query relevance and transfer learning with transformer models. In Proceedings of\nthe 33rd Canadian Conference on Artiﬁcial Intelligence, pages 342–348, 2020.\n13. M. T. R. Laskar, E. Hoque, and J. X. Huang. Wsl-ds: Weakly supervised learning with dis-\ntant supervision for query focused multi-document abstractive summarization. arXiv preprint\narXiv:2011.01421, 2020.\n14. M. T. R. Laskar, X. Huang, and E. Hoque. Contextualized embeddings based transformer\nencoder for sentence similarity modeling in answer selection task. In Proceedings of the 12th\nLanguage Resources and Evaluation Conference, pages 5505–5514, 2020.\n15. Y . Liu, A. An, and X. Huang. Boosting prediction accuracy on imbalanced datasets with SVM\nensembles. In Proceedings of the 10th Paciﬁc-Asia Conference on Knowledge Discovery and\nData Mining, PAKDD, pages 107–118, 2006.\n16. Y . Liu, X. Huang, A. An, and X. Yu. ARSA: a sentiment-aware model for predicting sales\nperformance using blogs. In Proceedings of the 30th Annual International ACM SIGIR Con-\nference on Research and Development in Information Retrieval, pages 607–614, 2007.\n17. Y . Liu, X. Huang, A. An, and X. Yu. Modeling and predicting the helpfulness of online\nreviews. In 2008 Eighth IEEE International Conference on Data Mining , pages 443–452.\nIEEE, 2008.\n10 Md Tahmid Rahman Laskar, Enamul Hoque and Jimmy Xiangji Huang\n18. Y . Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,\nand V . Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n19. S. Mayhew, N. Gupta, and D. Roth. Robust Named Entity Recognition with Truecasing Pre-\ntraining. arXiv preprint arXiv:1912.07095, 2019.\n20. J. Miao, J. X. Huang, and Z. Ye. Proximity-based rocchio’s model for pseudo relevance. In\nProceedings of the 35th International ACM SIGIR Conference on Research and Development\nin Information Retrieval, pages 535–544, 2012.\n21. T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efﬁcient estimation of word representations in\nvector space. arXiv preprint arXiv:1301.3781, 2013.\n22. P. Nakov, D. Hoogeveen, L. M `arquez, A. Moschitti, H. Mubarak, T. Baldwin, and K. Ver-\nspoor. Semeval-2017 task 3: Community question answering. In Proceedings of the 11th\nInternational Workshop on Semantic Evaluation (SemEval-2017), pages 27–48, 2017.\n23. J. Pennington, R. Socher, and C. Manning. Glove: Global vectors for word representation. In\nProceedings of the 2014 Conference on Empirical Methods in Natural Language Processing,\npages 1532–1543, 2014.\n24. M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer.\nDeep contextualized word representations. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies, pages 2227–2237, 2018.\n25. J. Rao, H. He, and J. Lin. Noise-contrastive estimation for answer selection with deep neural\nnetworks. In Proceedings of the 25th ACM International on Conference on Information and\nKnowledge Management, pages 1913–1916, 2016.\n26. J. Rao, L. Liu, Y . Tay, W. Yang, P. Shi, and J. Lin. Bridging the gap between relevance\nmatching and semantic matching for short text similarity modeling. InProceedings of the 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International\nJoint Conference on Natural Language Processing, pages 5373–5384, 2019.\n27. A. Severyn and A. Moschitti. Automatic feature engineering for answer selection and ex-\ntraction. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language\nProcessing, pages 458–467, 2013.\n28. L. Sha, X. Zhang, F. Qian, B. Chang, and Z. Sui. A multi-view fusion neural network for\nanswer selection. In Thirty-Second AAAI Conference on Artiﬁcial Intelligence, 2018.\n29. Y . Tay, M. C. Phan, L. A. Tuan, and S. C. Hui. Learning to rank question answer pairs with\nholographic dual lstm architecture. In Proceedings of the 40th International ACM SIGIR\nConference on Research and Development in Information Retrieval, pages 695–704, 2017.\n30. Y . Tay, L. A. Tuan, and S. C. Hui. Hyperbolic representation learning for fast and efﬁcient\nneural question answering. In Proceedings of the 11th ACM International Conference on Web\nSearch and Data Mining, pages 583–591, 2018.\n31. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polo-\nsukhin. Attention is all you need. In Advances in Neural Information Processing Systems ,\npages 5998–6008, 2017.\n32. M. Wang, N. A. Smith, and T. Mitamura. What is the jeopardy model? a quasi-synchronous\ngrammar for qa. InProceedings of the 2007 Joint Conference on Empirical Methods in Natural\nLanguage Processing and Computational Natural Language Learning, 2007.\n33. T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf,\nM. Funtowicz, et al. Transformers: State-of-the-art natural language processing. arXiv\npreprint arXiv:1910.03771, 2019.\n34. Y . Yang, W.-t. Yih, and C. Meek. WikiQA: A challenge dataset for open-domain question an-\nswering. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language\nProcessing, pages 2013–2018, 2015.\n35. W.-t. Yih, M.-W. Chang, C. Meek, and A. Pastusiak. Question answering using enhanced\nlexical semantic models. In Proceedings of the 51st Annual Meeting of the Association for\nComputational Linguistics, volume 1, pages 1744–1753, 2013.\nUtilizing BERT for Answer Selection 11\n36. X. Yin, J. X. Huang, Z. Li, and X. Zhou. A survival modeling approach to biomedical search\nresult diversiﬁcation using wikipedia. IEEE Transactions on Knowledge and Data Engineer-\ning, 25(6):1201–1212, 2013.\n37. X. Yu, Y . Liu, X. Huang, and A. An. Mining online reviews for predicting sales performance:\nA case study in the movie domain. IEEE Transactions on Knowledge and Data Engineering,\n24(4):720–734, 2012.\n38. Y . Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning\nbooks and movies: Towards story-like visual explanations by watching movies and reading\nbooks. In Proceedings of the 2015 IEEE International Conference on Computer Vision, pages\n19–27, 2015.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8335795402526855
    },
    {
      "name": "Transformer",
      "score": 0.8234930038452148
    },
    {
      "name": "Language model",
      "score": 0.8080822825431824
    },
    {
      "name": "Question answering",
      "score": 0.7165917754173279
    },
    {
      "name": "Encoder",
      "score": 0.6850104331970215
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.6445642709732056
    },
    {
      "name": "Sentence",
      "score": 0.6095627546310425
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5637800097465515
    },
    {
      "name": "Natural language processing",
      "score": 0.5120227932929993
    },
    {
      "name": "Task (project management)",
      "score": 0.49362432956695557
    },
    {
      "name": "Machine learning",
      "score": 0.38026168942451477
    },
    {
      "name": "Voltage",
      "score": 0.0602874755859375
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}