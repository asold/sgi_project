{
  "title": "The role of valence, dominance, and pitch in perceptions of artificial intelligence (AI) conversational agents’ voices",
  "url": "https://openalex.org/W4312223248",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5090135482",
      "name": "Victor Kenji Medeiros Shiramizu",
      "affiliations": [
        "University of Strathclyde"
      ]
    },
    {
      "id": "https://openalex.org/A5059273300",
      "name": "Anthony J. Lee",
      "affiliations": [
        "University of Stirling"
      ]
    },
    {
      "id": "https://openalex.org/A5050764913",
      "name": "Daria Altenburg",
      "affiliations": [
        "Ghent University"
      ]
    },
    {
      "id": "https://openalex.org/A5002455066",
      "name": "David R. Feinberg",
      "affiliations": [
        "McMaster University"
      ]
    },
    {
      "id": "https://openalex.org/A5089676302",
      "name": "Benedict C. Jones",
      "affiliations": [
        "University of Strathclyde"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2015588385",
    "https://openalex.org/W2166711410",
    "https://openalex.org/W2114244872",
    "https://openalex.org/W2887882077",
    "https://openalex.org/W1971687303",
    "https://openalex.org/W2915063634",
    "https://openalex.org/W1975950487",
    "https://openalex.org/W2751194593",
    "https://openalex.org/W2864361694",
    "https://openalex.org/W2789539594",
    "https://openalex.org/W2768003415",
    "https://openalex.org/W2914703899",
    "https://openalex.org/W2899303905",
    "https://openalex.org/W2963229532",
    "https://openalex.org/W4231117022",
    "https://openalex.org/W2904104216",
    "https://openalex.org/W2136296521",
    "https://openalex.org/W2116093097",
    "https://openalex.org/W2170316478",
    "https://openalex.org/W2144038853",
    "https://openalex.org/W2152131237",
    "https://openalex.org/W2154504771",
    "https://openalex.org/W2774486220",
    "https://openalex.org/W3106686534",
    "https://openalex.org/W4288685345",
    "https://openalex.org/W2141845152",
    "https://openalex.org/W1526025733"
  ],
  "abstract": null,
  "full_text": "1\nVol.:(0123456789)Scientific Reports |        (2022) 12:22479  | https://doi.org/10.1038/s41598-022-27124-8\nwww.nature.com/scientificreports\nThe role of valence, dominance, \nand pitch in perceptions of artificial \nintelligence (AI) conversational \nagents’ voices\nVictor Kenji M. Shiramizu 1, Anthony J. Lee 2, Daria Altenburg 3, David R. Feinberg 4 & \nBenedict C. Jones 1*\nThere is growing concern that artificial intelligence conversational agents (e.g., Siri, Alexa) reinforce \nvoice-based social stereotypes. Because little is known about social perceptions of conversational \nagents’ voices, we investigated (1) the dimensions that underpin perceptions of these synthetic \nvoices and (2) the role that acoustic parameters play in these perceptions. Study 1 (N = 504) found that \nperceptions of synthetic voices are underpinned by Valence and Dominance components similar to \nthose previously reported for natural human stimuli and that the Dominance component was strongly \nand negatively related to voice pitch. Study 2 (N = 160) found that experimentally manipulating pitch \nin synthetic voices directly influenced dominance-related, but not valence-related, perceptions. \nCollectively, these results suggest that greater consideration of the role that voice pitch plays in \ndominance-related perceptions when designing conversational agents may be an effective method \nfor controlling stereotypic perceptions of their voices and the downstream consequences of those \nperceptions.\nPerceptions of human faces and voices influence important social  outcomes1,2. For example, people prefer to date, \nmate with, hire, and vote for individuals perceived as being particularly  attractive1,2. These social perceptions (e.g., \nour impressions of other people’s attractiveness or trustworthiness) can also influence life-or-death outcomes. \nFor example, untrustworthy-looking defendants are more likely to receive death  sentences3.\nSeveral recent studies suggest that social perceptions are underpinned by two key dimensions. For example, \nPrincipal Component Analysis (PCA) of ratings of natural human faces on a variety of different traits on which \npeople spontaneously assess faces (trustworthiness, emotional stability, responsibility, sociability, caringness, \nattractiveness, intelligence, confidence, weirdness, unhappiness, meanness, aggressiveness, dominance) reveals \ntwo key underlying  dimensions4,5. The first of these dimensions, often labelled valence, is highly correlated \nwith ratings of pro-social traits, such as trustworthiness, and is thought to reflect perceptions of an individual’s \nwillingness to inflict harm on  others4,5. The second of these dimensions, often labelled dominance, is highly \ncorrelated with traits such as dominance and aggressiveness and is thought to reflect perceptions of an individual’s \ncapacity to inflict harm on  others4,5. Comparable dimensions have also been observed in work that used similar \ndata-reduction methods to reveal the dimensions that underpin social perceptions of natural human  voices6,7.\nConversational agents (e.g., Siri and Alexa) have become ubiquitous and communicate with users via voices \ngenerated by artificial intelligence algorithms (i.e., synthetic voices). A recent report by UNESCO (United Nations \nEducational, Scientific and Cultural Organization) proposed that these synthetic voices have the potential to \nreinforce voice-based gender stereotypes and argued that research is urgently needed to ameliorate this  issue8. \nCrucially, understanding how people perceive synthetic voices is necessary to both understand and address this \nissue. While there is a large literature examining the factors that influence perceptions of natural social stimuli \n(natural human faces and voices), it is unclear whether our understanding of the factors that shape perceptions \nof natural social stimuli is applicable to perceptions of the synthetic voices employed by conversational agents. \nIndeed, previous work has shown qualitative differences in how people process and perceive natural human faces \nand synthetic  faces9,10. Such results suggest that the factors that underpin perceptions of natural human stimuli \nOPEN\n1School of Psychological Sciences & Health, University of Strathclyde, Glasgow, Scotland. 2Division of Psychology, \nUniversity of Stirling, Stirling, Scotland. 3Department of Marketing, Innovation and Organisation, Ghent University, \nGhent, Belgium. 4Department of Psychology, Neuroscience & Behaviour, McMaster University, Hamilton, \nCanada. *email: benedict.jones@strath.ac.uk\n2\nVol:.(1234567890)Scientific Reports |        (2022) 12:22479  | https://doi.org/10.1038/s41598-022-27124-8\nwww.nature.com/scientificreports/\nmay not necessarily underpin perceptions of synthetic stimuli. Indeed, some previous research suggests that \nnatural human voices are perceived to be more expressive, understandable, and likeable than synthetic voices, \nsuggesting that people can distinguish between natural human voices and synthetic voices and may process them \n differently11. However, other work suggests that the expectation that synthetic voices possess robotic qualities \ncan also cause listeners to have difficulty when they are asked to classify voices as natural or  artificial12. In light \nof these points, Study 1 first investigated whether trait-ratings of the synthetic voices used by conversational \nagents are underpinned by valence and dominance dimensions similar to those observed for natural human \nstimuli in previous  work4,5.\nWork on social perceptions of natural human voices has suggested that acoustic properties of voices can be \nkey predictors and determinants of voice  perceptions13,14. The majority of this work has focused on fundamental \nfrequency (the acoustic correlate of perceived pitch) and formant frequencies (a correlate of vocal tract length and \nbody size,13). For example, studies have reported strong negative relationships between dominance perceptions \nand voice pitch and/or that voices in which pitch was lowered are perceived to be more dominant than voices \nin which pitch was  raised13,14. In light of findings such as these, Study 1 also investigated possible relationships \nbetween the dimensions that underpin social perceptions of synthetic voices and both pitch and formant \nfrequencies.\nStudy 2 built directly on our results of Study 1 by testing whether Study 1’s results for correlations between \nsocial perceptions and measured acoustic properties of synthetic voices also occur when we experimentally \nmanipulated acoustic parameters of voices. Whereas stimuli in Study 1 were individual words, stimuli in Study \n2 were full sentences.\nCollectively, these studies may be an important first step in identifying how appropriate the large literature \non perceptions of natural social stimuli is to understand how we perceive and interact with the type of synthetic \nvoices employed by artificial conversational agents. They may also identify mechanisms through which designers \nof conversational agents can better control stereotypic perceptions of conversational agents’ voices and the \ndownstream consequences of these perceptions.\nStudy 1\nStudy 1 aimed to (1) identify the perceptual dimensions that underpin social judgments of synthetic voices and \n(2) investigate how these perceptual dimensions are related to voice pitch and formant frequencies.\nMethods\nEthics. All procedures were approved by the School of Psychological Sciences and Health (University of \nStrathclyde) Ethics Commitee, all work was undertaken in accordance with the Declaration of Helsinki, and all \nparticipants provided informed consent.\nStimuli. Forty-six synthetic (i.e., computer-generated) voices (17 male, 29 female) were downloaded from \nonline voice synthesis platforms (28 voices) or were provided to us by companies working on synthetic voice \nproduction (18 voices). Because studies that measured pitch and formant frequencies from recordings of natural \nhuman voices have generally used /α/, /ε/, /i/, /o/, and /u/ vowel  sounds13, the voices used in the current study \nspoke the words ‘Father, See, Bet, Note, Boot’. All stimuli were amplitude normalized to 70 dB prior to both \nacoustic analyses and collection of trait ratings. Voices spoke in English and reflected a range of accents. Stimuli \nare publicly available at https:// osf. io/ 4zgrf/.\nTrait-rating procedure. Five hundred and six participants (213 men, 291 women, two participants did not \nreport their gender; mean age = 34.0 years, standard deviation = 11.0 years) were recruited through the Prolific \nparticipant recruitment platform. All participants reported having English as their first language.\nParticipants were randomly allocated to rate all 46 voice stimuli for one of 17 traits (trustworthy, emotionally \nstable, responsible, sociable, caring, attractive, intelligent, confident, weird, unhappy, mean, aggressive, dominant, \ncompetent, old, masculine, feminine) on a 1 (not at all) to 7 (very) scale. The order in which stimuli were \npresented for rating was fully randomized and participants could play each voice as many times as they wanted to \nbefore rating it. Participants had to click a play button to play each voice and could not rate the voice and proceed \nto the next trial until the voice recording had been played in full. Ratings were made by clicking on labelled \nbuttons, preventing participants from entering invalid responses. Mean stimulus duration was 3.42 s (SD = 0.42 s). \nTraits were chosen to reflect those used in previous studies of the dimensions underpinning perceptions of social \nstimuli (trustworthy, emotionally stable, responsible, sociable, caring, attractive, intelligent, confident, weird, \nunhappy, mean, aggressive, dominant, old,4,5,7). Masculinity and femininity were also included as separate traits, \nrather than being treated as opposite ends of a single continuum, because of recent work suggesting that they \ncan have independent effects on perceptions of social stimuli and that including these characteristics as separate \ntraits increased the predictive accuracy of models of social  perception15. Competence was included because of \nresearch suggesting it plays an important role in a range of social outcomes (see, e.g., 16 for a recent review of \nthis literature). Age, masculinity, and femininity were included because they had been included in previous \nresearch that used similar data-reduction methods to identify the perceptual dimensions underpinning social \n judgments7,17. Words spoken by each voice were presented in a single file for rating.\nTable 1 shows the number of raters who rated stimuli on each trait, inter-rater agreement for those ratings \n(indicated by Cronbach’s alpha), the mean rating for each trait, and standard deviation. Because inter-rater \nagreement in ratings was relatively high for all traits, we calculated the mean rating for each voice. These mean \nratings were calculated separately for each trait and were used in subsequent analyses. All data and analysis code \nare publicly available at https:// osf. io/ 4zgrf/.\n3\nVol.:(0123456789)Scientific Reports |        (2022) 12:22479  | https://doi.org/10.1038/s41598-022-27124-8\nwww.nature.com/scientificreports/\nAcoustic analyses. Vowel sounds were extracted from each voice recording and analyzed using PRAAT 18. The \nmethods used to measure pitch (f0) and formant frequencies were identical to those used to measure acoustic properties \nof natural speech in recent work on social judgments of human  voices13. f0, f1, f2, f3, and f4 were measured separately \nfrom each vowel sound. Next, all measures were converted to z-scores. Standardized f0, averaged across vowel sounds, \nwas then used as our measure of pitch in our statistical analyses. The remaining standardized measures (f1, f2, f3, and \nf4) were first averaged for each vowel sound and then averaged across vowel sounds. This latter score was used as our \nmeasure of formants in our statistical analyses.\nResults\nAll analyses were carried out using  R19 and the packages tidyverse 1.3.0 20, readxl 1.3.121, psych 2.0.1222, paran \n1.5.223, kableExtra 1.3.4 24, knitr 1.31 25, and jtools 2.1.3 26. All data and analysis code are publicly available at \nhttps:// osf. io/ 4zgrf/.\nFirst, mean trait ratings for each voice were subject to Principal Component Analysis (PCA) with oblimin \nrotation. This analysis revealed two Principal Components (PCs), explaining 45 and 29% of the variance in \nratings, respectively. Factor loadings of the individual traits on both PCs are shown in Table 2.\nConsistent with previous research using natural human voices and faces as  stimuli4,5,7, the first PC was highly \ncorrelated with pro-social traits, such as trustworthiness, competence, responsibility, emotional stability, and \nsociableness, but weakly correlated with dominance and aggressiveness, and the second PC was highly correlated \nwith dominance and aggressiveness, but weakly correlated with trustworthiness, competence, responsibility, \nemotional stability, and sociableness. Following previous research showing this pattern of results, we labelled \nthese PCs Valence and Dominance, respectively. There was a non-significant, very weak, positive correlation \nbetween scores on both these components (r(46) = 0.07, p = 0.655).\nNext, we used regression analyses to investigate possible relationships between PC scores and the acoustic \ncharacteristics pitch and formants. Separate models were run for Valence and Dominance PC scores. PC scores \nwere our outcome variable. Predictors were voice gender (effect coded so that − 0.5 = male and 0.5 = female), \npitch, formants, all two-way interactions, and the three-way interaction.\nFull results from these regression analyses are summarized in Table  3 (Valence PC scores) and Table  4 \n(Dominance PC scores). For Valence, the regression model was not significant (F (7,38) = 1.04, adjusted R \nsquared = 0.01, p = 0.42) and the model showed no significant effects for any of the predictors (Table  3). For \nDominance, the regression model was significant (F(7, 38) = 21.8, adjusted R squared = 0.76, p < 0.001) and a \nstrong significant negative effect of voice pitch was observed (Table 4). Figure 1 shows the negative relationship \nbetween pitch and Dominance PC scores. Pitch explained 74% of the variance in Dominance PC scores.\nRobustness checks in which ratings of male and female voices were analyzed separately showed the same \npatterns of results as the analyses described above. That is, separate PCAs of ratings of male and female voices \nrevealed Valence and Dominance PCs that were similar to those we obtained from PCA of all voices. Further \nanalyses also showed that, for both male and female voices, pitch was strongly and negatively related to scores \non the Dominance PC. Full results for these analyses are given in the supplemental analyses for Study 1 (https:// \nosf. io/ 4zgrf/).\nTable 1.  Cronbach’s alpha for each trait, number of raters who rated voices for each trait, mean rating for each \ntrait, and standard deviation (SD) in Study 1.\nTrait Cronbach’s alpha Number of raters Mean SD\nAggressiveness 0.915 29 2.62 1.46\nAttractiveness 0.890 30 3.55 1.73\nCaringness 0.889 30 3.54 1.70\nCompetence 0.926 30 4.37 1.60\nConfidence 0.913 29 4.19 1.70\nDominance 0.951 30 3.87 1.68\nEmotional stability 0.901 30 4.25 1.70\nFemininity 0.991 30 4.07 2.13\nIntelligence 0.913 29 4.07 1.65\nMasculinity 0.996 29 3.20 2.25\nMeanness 0.808 30 3.19 1.74\nOld 0.963 30 3.94 1.47\nResponsibility 0.919 30 4.01 1.71\nSociable 0.859 30 3.74 1.67\nTrustworthiness 0.869 30 3.87 1.70\nUnhappiness 0.767 30 3.49 1.64\nWeirdness 0.923 30 3.93 1.85\n4\nVol:.(1234567890)Scientific Reports |        (2022) 12:22479  | https://doi.org/10.1038/s41598-022-27124-8\nwww.nature.com/scientificreports/\nStudy 2\nStudy 1 demonstrated that trait-ratings of synthetic voices are underpinned by Valence and Dominance \ncomponents similar to those previously reported for perceptions of natural human stimuli. Study 1 also found a \nstrong negative relationship between pitch and scores on the Dominance PC. By contrast, there was no significant \nrelationship between pitch and scores on the Valence PC. Study 2 attempted to validate the results of Study 1 \nby investigating the effects of manipulating voice pitch on perceptions of dominance and aggressiveness (i.e., \nthe two individual traits most strongly correlated with Dominance PC scores in Study 1) and perceptions of \ntrustworthiness and competence (i.e., the two individual traits most strongly correlated with Valence PC scores \nTable 2.  Correlations between each trait and scores on the Valence and Dominance components in Study 1. \nCorrelations with absolute values larger than 0.5 are bolded.\nTrait Valence PC Dominance PC\nAggressiveness − 0.135 0.866\nAttractiveness 0.897 − 0.162\nCaringness 0.644 − 0.587\nCompetence 0.937 0.185\nConfidence 0.797 0.297\nDominance 0.388 0.867\nEmotional stability 0.888 0.094\nFemininity 0.094 − 0.834\nIntelligence 0.845 0.232\nMasculinity 0.005 0.850\nMeanness 0.026 0.792\nOld 0.139 0.721\nResponsibility 0.884 0.264\nSociable 0.845 − 0.350\nTrustworthiness 0.939 − 0.179\nUnhappiness − 0.462 0.311\nWeirdness − 0.828 0.061\nTable 3.  Results of regression analysis of Valence PC scores in Study 1.\nUnstandardized estimate Standard error t p\nIntercept 0.46 0.45 1.02 0.315\nPitch − 0.36 0.35 − 1.01 0.317\nFormants 1.04 0.73 1.43 0.162\nVoice sex − 0.82 0.91 − 0.91 0.370\nPitch × formants 0.45 0.53 0.86 0.397\nPitch × voice sex − 0.35 0.71 − 0.50 0.621\nFormants × voice sex − 1.54 1.45 − 1.06 0.297\nPitch × formants × voice sex − 0.30 1.06 − 0.29 0.776\nTable 4.  Results of regression analysis of Dominance PC scores in Study 1.\nUnstandardized estimate Standard error t p\nIntercept − 0.13 0.22 − 0.60 0.550\nPitch − 0.76 0.17 − 4.40  < 0.001\nFormants − 0.50 0.35 − 1.42 0.165\nVoice sex 0.13 0.44 0.29 0.770\nPitch × formants − 0.26 0.26 − 1.01 0.321\nPitch × voice sex 0.26 0.35 0.74 0.461\nFormants × voice sex 0.57 0.71 0.80 0.428\nPitch × formants × voice sex 0.23 0.52 0.45 0.658\n5\nVol.:(0123456789)Scientific Reports |        (2022) 12:22479  | https://doi.org/10.1038/s41598-022-27124-8\nwww.nature.com/scientificreports/\nin Study 1). Whereas voices in Study 1 spoke individual words, stimuli in Study 2 spoke a full sentence commonly \nused in studies of social judgments of natural voices that have used full sentences as stimuli (“When the sunlight \nstrikes raindrops in the air, they act as a prism and form a rainbow. ”).\nMethods\nStimuli. Stimuli were manufactured from recordings of the same 46 voices used in Study 1, this time \nspeaking the sentence “When the sunlight strikes raindrops in the air, they act as a prism and form a rainbow” . \nThis sentence was chosen because it has been used in previous studies of social judgments of natural  voices27,28.\nTwo versions of each recording were created; one version (raised-pitch version) in which the pitch of the \nrecording was raised by 0.5 equivalent rectangular bandwidths (ERBs) and another version (lowered-pitch \nversion) in which the pitch of the recording had been lowered by 0.5 ERBs. The ERB scale corrects for the \ndifference between perceived pitch and actual fundamental frequency. Mean pitch for the lowered-pitch versions \nwas 142.06 Hz (SD = 8.71 Hz) and mean pitch for the raised-pitch versions was 184.83 Hz (SD = 9.09 Hz). Pitch \nwas manipulated using the same methods used in previous studies of the effect of manipulating pitch on social \njudgments of natural  voices29–31. Only the raised- and lowered-pitch versions were used in the study. All stimuli \nwere amplitude normalized to 70 dB prior to collection of trait ratings. Stimuli are publicly available at https://  \nosf. io/ 4zgrf/. Voices spoke in English and reflected a range of accents.\nTrait-rating procedure. One hundred and sixty participants (59 men, 93 women, eight participants did not \nreport their gender; mean age = 31.5 years, standard deviation = 11.6 years) were recruited through the Prolific \nparticipant recruitment platform. All participants reported having English as their first language.\nParticipants were randomly allocated to rate all 92 voice stimuli (the 46 raised-pitch and 46 lowered-pitch \nversions) for either competence (36 raters), trustworthiness (40 raters), dominance (44 raters), or aggressiveness \n(40 raters) on a 1 (not at all) to 7 (very) scale. The order in which stimuli were presented for rating was fully \nrandomized and participants could play each voice as many times as they wanted to before rating it. Participants \nhad to click a play button to play each voice and could not rate the voice and proceed to the next trial until \nthe voice recording had been played in full. Ratings were made by clicking on labelled buttons, preventing \nparticipants from entering invalid responses. Mean stimulus duration was 5.40 s (SD = 0.42 s). Inter-rater \nagreement was high for all traits (all Cronbach’s alpha > 0.93).\nResults\nAll analyses were carried out using  R19 and the packages tidyverse 1.3.020, broom.mixed 0.2.632, lmerTest 3.1-333, \npsych 2.0.1222, and jtools 2.1.326. All data and analysis code are publicly available at https:// osf. io/ 4zgrf/.\nWe used linear mixed models to investigate possible effects of the pitch manipulation on competence, \ntrustworthiness, dominance, and aggressiveness ratings. Separate models were run for each trait. Ratings were \nthe outcome variables and predictors were voice gender (effect coded so that − 0.5 = male and 0.5 = female), pitch \nmanipulation (effect coded so that lowered pitch = − 0.5 and raised pitch = 0.5), and the interaction between voice \nFigure 1.  The significant negative relationship between pitch and Dominance PC scores in Study 1. Axes show \nstandardized scores.\n6\nVol:.(1234567890)Scientific Reports |        (2022) 12:22479  | https://doi.org/10.1038/s41598-022-27124-8\nwww.nature.com/scientificreports/\ngender and pitch manipulation. The models included random intercepts for participant and stimulus. Random \nslopes were specified maximally.\nResults of these analyses are summarized in Table 5 (competence ratings), Table 6 (trustworthiness ratings), \nTable 7 (dominance ratings), and Table  8 (aggressiveness ratings). While our analyses of competence and \ntrustworthiness ratings showed no significant effect of pitch manipulation, our analyses of dominance and \naggressiveness ratings showed that the lowered-pitch versions of voices were judged significantly more dominant \nand aggressive than the raised-pitch versions. Figure  2 shows the negative effects of the pitch manipulation on \ndominance and aggressiveness perceptions.\nDiscussion\nPCA of ratings of synthetic voices used by conversational agents on a range of traits (trustworthiness, emotional \nstability, responsibility, sociability, caringness, attractiveness, intelligence, confidence, weirdness, unhappiness, \nmeanness, aggressiveness, dominance, competence, age, masculinity, femininity) produced two components. \nThe first component, which explained the bulk of the variance in ratings, was highly correlated with ratings of \npro-social traits, such as trustworthiness, competence, responsibility, emotional stability, and sociable, and weakly \ncorrelated with dominance and aggressiveness ratings. The second component, which explained substantially \nless of the variance in ratings, was highly correlated with dominance and aggressiveness ratings and weakly \ncorrelated with trustworthiness, competence, responsibility, emotional stability, and sociable ratings. This pattern \nof results is extremely similar to those obtained when ratings of natural human faces and voices were subject to \nPCA in previous  studies4–7, suggesting that social perceptions of synthetic voices are underpinned by valence \nand dominance dimensions similar to those previously found to underpin social perceptions of natural human \nstimuli.\nTable 5.  Results of our analysis of competence ratings in Study 2.\nUnstandardised estimate Standard error df t p\nIntercept 4.04 0.16 75.4 25.40  < 0.001\nPitch 0.07 0.20 94.3 0.34 0.732\nVoice gender − 0.38 0.20 98.4 − 1.88 0.063\nPitch × voice gender 0.19 0.39 91.6 0.50 0.619\nTable 6.  Results of our analysis of trustworthiness ratings in Study 2.\nUnstandardised estimate Standard error df t p\nIntercept 3.85 0.12 113.0 32.10  < 0.001\nPitch 0.20 0.19 97.1 1.05 0.298\nVoice gender − 0.15 0.19 97.8 − 0.82 0.413\nPitch × voice gender 0.19 0.37 91.8 0.51 0.610\nTable 7.  Results of our analysis of dominance ratings in Study 2.\nUnstandardised estimate Standard error df t p\nIntercept 3.77 0.13 87.3 28.40   < 0.001\nPitch − 0.53 0.17 103.0 − 3.20 0.002\nVoice gender − 1.00 0.19 123.0 − 5.31   < 0.001\nPitch × voice gender 0.14 0.32 91.5 0.45 0.651\nTable 8.  Results of our analysis of aggressiveness ratings in Study 2.\nUnstandardised estimate Standard error df t p\nIntercept 2.63 0.17 47.4 15.70   < 0.001\nPitch − 0.22 0.10 94.4 − 2.12 0.037\nVoice gender − 0.57 0.13 111.0 − 4.55    < 0.001\nPitch × voice gender − 0.07 0.20 91.0 − 0.35 0.724\n7\nVol.:(0123456789)Scientific Reports |        (2022) 12:22479  | https://doi.org/10.1038/s41598-022-27124-8\nwww.nature.com/scientificreports/\nAnalyses of acoustic properties of synthetic voices and their relationship to the PCs in Study 1 revealed a \nstrong negative correlation between pitch and scores on the Dominance component. Consistent with results of \nthis correlational analysis, experimentally manipulating the pitch of synthetic voices in Study 2 had a strong \neffect on dominance and aggressiveness perceptions (the two traits most strongly correlated with the Dominance \ncomponent). Synthetic voices with lowered pitch were judged significantly more dominant and aggressive than \nthose with raised pitch (Fig. 2). These results are consistent with previously reported results for both measured \nand manipulated pitch and dominance perceptions of natural human  voices13,14.\nPrevious studies of formant frequencies and social perceptions of natural human voices have typically \nreported that voices with lower formant frequencies are perceived to be more  dominant34. Previous studies \nof perceptions of natural human voices have also often reported associations between attractiveness and both \nvoice pitch and formant  frequencies31,34. In our study, we did not observe significant associations between scores \non the Dominance PC and measured formants (Study 1) or between scores on the Valence PC (very highly \ncorrelated with attractiveness) and measured pitch or formants (Study 1). It is currently unclear whether these \npotential differences in results for synthetic voices in the current studies and natural human voices in previous \nstudies reflect differences in how these two classes of stimuli are perceived or methodological differences among \nstudies (e.g., larger number of voices tested in studies of natural human voices). While further work is needed \nto address this issue, our results clearly indicate that pitch is particularly strongly related to (and, as our results \nfrom Study 2 indicate, directly influences) dominance-related perceptions of synthetic voices. Indeed, in Study \n1, pitch explained ~ 74% of the variance in Dominance PC scores. In a recent study of natural human voices, \nSchild et al.35 found that trustworthiness ratings were not significantly correlated with pitch, but that pitch was \na very good predictor of dominance ratings. Our findings for conversational agents’ voices very closely align \nwith Schild et al. ’s results.\nThree key aspects of the research reported here suggest that our results are likely to have good generalisability. \nFirst, we show that dominance-related perceptions of voices are both highly correlated with voice pitch (Study \n1) and directly influenced by experimentally manipulating voice pitch (Study 2). This pattern of results suggests \nthat our results generalise well across two different types of study design. Second, in Study 2 we analysed \nresponses using linear mixed models that take into account variability in responses across both raters and \nstimuli. This analytical strategy is known to produce results that generalise better to new sets of stimuli than \nthose of analytical approaches in which responses are aggregated across  stimuli36. Third, the similarity in our \nFigure 2.  The effect of pitch manipulation on (A) dominance, (B) aggressiveness, (C) trustworthiness, and \n(D) competence ratings in Study 2. Significant effects of pitch were observed for dominance and aggressiveness \nratings, but not trustworthiness or competence ratings. The points and distributions represent the average \nrating for each voice. The box plots show the median, first and third quartile, and the minimum and maximum \ndominance rating for low (purple) and high (green) pitch.\n8\nVol:.(1234567890)Scientific Reports |        (2022) 12:22479  | https://doi.org/10.1038/s41598-022-27124-8\nwww.nature.com/scientificreports/\nresults for dominance-related perceptions of synthetic voices to those reported elsewhere for natural human \nvoices suggest the large literature on dominance-related perceptions of natural human voices is likely to be useful \nfor understanding perceptions of synthetic voices.\nTo summarize, our results for social perceptions of conversational agents’ synthetic voices highlight two \nclear similarities in the characteristics of judgments of synthetic voices and those previously reported for natural \nhuman stimuli. Like natural human voices, our results show that perceptions of synthetic voices appear to be \nunderpinned by Valence and Dominance dimensions and that dominance-related perceptions are both strongly \nrelated to voice pitch and directly affected by experimental manipulations of voice pitch. Collectively, these results \nsuggest that greater consideration of the role that voice pitch plays in dominance-related social perceptions when \ndesigning conversational agents will be effective in controlling stereotypic perceptions of their voices and the \ndownstream consequences of those perceptions.\nData availability\nData, analysis code, and stimuli for both studies are publicly available on the Open Science Framework (https:// \nosf. io/ 4zgrf/).\nReceived: 19 October 2022; Accepted: 26 December 2022\nReferences\n 1. Olivola, C. Y ., Funk, F . & Todorov, A. Social attributions from faces bias human choices. Trends Cogn. Sci. 18(11), 566–570 (2014).\n 2. Rhodes, G. The evolutionary psychology of facial beauty. Annu. Rev. Psychol. 57, 199–226 (2006).\n 3. Wilson, J. P . & Rule, N. O. Facial trustworthiness predicts extreme criminal-sentencing outcomes. Psychol. Sci. 26(8), 1325–1331 \n(2015).\n 4. Jones, B. C. et al. To which world regions does the valence–dominance model of social perception apply?. Nat. Hum. Behav. 5(1), \n159–169 (2021).\n 5. Oosterhof, N. N. & Todorov, A. The functional basis of face evaluation. Proc. Natl. Acad. Sci. 105(32), 11087–11092 (2008).\n 6. Baus, C., McAleer, P ., Marcoux, K., Belin, P . & Costa, A. Forming social impressions from voices in native and foreign languages. \nSci. Rep. 9(1), 1–14 (2019).\n 7. McAleer, P ., Todorov, A. & Belin, P . How do you say ‘Hello’? Personality impressions from brief novel voices. PLoS ONE  9(3), \ne90779 (2014).\n 8. West, M., Kraut, R. & Chew, H. E. I’ d blush if I could: Closing gender divides in digital skills through education. UNESCO Technical \nReport (2019).\n 9. Balas, B. & Pacella, J. Trustworthiness perception is disrupted in artificial faces. Comput. Hum. Behav. 77, 240–248 (2017).\n 10. Balas, B., Tupa, L. & Pacella, J. Measuring social variables in real and artificial faces. Comput. Hum. Behav. 88, 236–243 (2018).\n 11. Cabral, J. P ., Cowan, B. R., Zibrek, K. & McDonnell, R. The influence of synthetic voice on the evaluation of a virtual character. In \nINTERSPEECH 229–233 (2017).\n 12. Wester, M., Aylett, M. P . & Braude, D. A. Bot or not: Exploring the fine line between cyber and human identity. In Proceedings of \nthe 19th ACM International Conference on Multimodal Interaction 506–507 (2017).\n 13. Armstrong, M. M., Lee, A. J. & Feinberg, D. R. A house of cards: Bias in perception of body size mediates the relationship between \nvoice pitch and perceptions of dominance. Anim. Behav. 147, 43–51 (2019).\n 14. Aung, T. & Puts, D. Voice pitch: A window into the communication of social power. Curr. Opin. Psychol. 33, 154–161 (2020).\n 15. Hester, N., Jones, B. C. & Hehman, E. Perceived femininity and masculinity contribute independently to facial impressions. J. Exp. \nPsychol. Gen. (2020).\n 16. Oh, D., Buck, E. A. & Todorov, A. Revealing hidden gender biases in competence impressions of faces. Psychol. Sci. 30(1), 65–79 \n(2019).\n 17. Sutherland, C. A. et al. Social inferences from faces: Ambient images generate a three-dimensional model. Cognition  127(1), \n105–118 (2013).\n 18. Boersma, P . & Weenink, D. Praat: Doing phonetics by computer (2018).\n 19. R Core Team. R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. \nRetrieved from http:// www.R- proje ct. org/ (2021).\n 20. Wickham, H. & Bryan, J. tidyverse: Easily Install and Load the ’Tidyverse’ (Version 1.3.0) [Computer software]. Retrieved from \nhttps:// cran.r- proje ct. org/ web/ packa ges/ tidyv erse/ index. html (2021).\n 21. Wickham, H. & Bryan, J. readxl: Read excel files. R Package Version (2019).\n 22. Revelle, W . Psych: Procedures for personality and psychological research (Version 1.6.12) [Computer software]. Retrieved from \nhttps:// cran.r- proje ct. org/ packa gepsy ch (2016).\n 23. Dinno, A. paran: Horn’s Test of Principal Components/Factors [R package version 1.5.2] (2018).\n 24. Zhu, H. KableExtra: Construct complex table with ’kable’ and pipe syntax. R Package Version (2019).\n 25. Xie, Y . knitr: A comprehensive tool for reproducible research in R. In Implementing Reproducible Research (eds Stodden, V . et al.) \n3–29 (CRC Press, 2014).\n 26. Long, J. A. jtools: Analysis and Presentation of Social Scientific Data. (Version 2.1.3). Retrieved from https:// cran.r-  proje ct. org/ \nweb/ packa ges/ jtools (2021).\n 27. Puts, D. A., Gaulin, S. J. & Verdolini, K. Dominance and the evolution of sexual dimorphism in human voice pitch. Evol. Hum. \nBehav. 27(4), 283–296 (2006).\n 28. Hodges-Simeon, C. R., Gaulin, S. J. & Puts, D. A. Different vocal parameters predict perceptions of dominance and attractiveness. \nHum. Nat. 21(4), 406–427 (2010).\n 29. Apicella, C. L. & Feinberg, D. R. Voice pitch alters mate-choice-relevant perception in hunter–gatherers. Proc. R. Soc. B Biol. Sci. \n276(1659), 1077–1082 (2009).\n 30. Feinberg, D. R., Jones, B. C., Little, A. C., Burt, D. M. & Perrett, D. I. Manipulations of fundamental and formant frequencies \ninfluence the attractiveness of human male voices. Anim. Behav. 69(3), 561–568 (2005).\n 31. Jones, B. C., Feinberg, D. R., DeBruine, L. M., Little, A. C. & Vukovic, J. A domain-specific opposite-sex bias in human preferences \nfor manipulated voice pitch. Anim. Behav. 79(1), 57–62 (2010).\n 32. Bolker, B. & Robinson, D. broom.mixed: Tidying Methods for Mixed Models (Version 0.2.6) [Computer software]. Retrieved from \nhttps:// cran.r- proje ct. org/ web/ packa ges/ broom. mixed/ index. html (2021).\n 33. Kuznetsova, A., Brockhoff, P . B. & Christensen, R. H. B. lmerTest package: Tests in linear mixed effects models. J. Stat. Softw. 82(13), \n1–26. https:// doi. org/ 10. 18637/ jss. v082. i13 (2017).\n9\nVol.:(0123456789)Scientific Reports |        (2022) 12:22479  | https://doi.org/10.1038/s41598-022-27124-8\nwww.nature.com/scientificreports/\n 34. Schild, C. et al. Linking human male vocal parameters to perceptions, body morphology, strength and hormonal profiles in contexts \nof sexual selection. Sci. Rep. 10(1), 1–16 (2020).\n 35. Schild, C., Braunsdorf, E., Steffens, K., Pott, F . & Stern, J. Gender and context-specific effects of vocal dominance and trustworthiness \non leadership decisions. Adapt. Hum. Behav. Physiol. 1–19 (2022).\n 36. Barr, D. J., Levy, R., Scheepers, C. & Tily, H. J. Random effects structure for confirmatory hypothesis testing: Keep it maximal. J. \nMem. Lang. 68(3), 255–278 (2013).\nAcknowledgements\nThis research was supported by the EPSRC grant ‘Designing Conversational Assistants to Reduce Gender Bias’ \n(EP/T023783/1), awarded to Benedict Jones. Daria Altenburg was supported by Grant BOF .24Y .2019.0006.01 \nof Ghent University, awarded to Adriaan Spruyt. For the purpose of Open Access, the authors have applied a \nCreative Commons Attribution (CC BY) to any Author Accepted Manuscript (AAM) version arising from this \nsubmission.\nAuthor contributions\nAll authors designed the studies. V .S., B.J., and A.L. carried out analyses. B.J. and V .S. wrote the first draft of the \nmanuscript, revising in light of feedback from D.A., A.L., and D.F . All authors reviewed the mansurciot.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to B.C.J.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2022",
  "topic": "Perception",
  "concepts": [
    {
      "name": "Perception",
      "score": 0.7680120468139648
    },
    {
      "name": "Dominance (genetics)",
      "score": 0.7513521313667297
    },
    {
      "name": "Valence (chemistry)",
      "score": 0.6679072976112366
    },
    {
      "name": "Psychology",
      "score": 0.6194004416465759
    },
    {
      "name": "Social psychology",
      "score": 0.41507112979888916
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3523332476615906
    },
    {
      "name": "Biology",
      "score": 0.22179877758026123
    },
    {
      "name": "Chemistry",
      "score": 0.13061240315437317
    },
    {
      "name": "Neuroscience",
      "score": 0.0835985541343689
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    }
  ]
}