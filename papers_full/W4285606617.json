{
  "title": "MMT: Multi-way Multi-modal Transformer for Multimodal Learning",
  "url": "https://openalex.org/W4285606617",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2116439671",
      "name": "Jia-Jia Tang",
      "affiliations": [
        "Center for Excellence in Brain Science and Intelligence Technology",
        "Hangzhou Dianzi University"
      ]
    },
    {
      "id": "https://openalex.org/A2047264473",
      "name": "Kang Li",
      "affiliations": [
        "Hangzhou Dianzi University",
        "Center for Excellence in Brain Science and Intelligence Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2083504203",
      "name": "Ming Hou",
      "affiliations": [
        "RIKEN Center for Advanced Intelligence Project"
      ]
    },
    {
      "id": "https://openalex.org/A2680359418",
      "name": "Xuanyu Jin",
      "affiliations": [
        "Center for Excellence in Brain Science and Intelligence Technology",
        "Hangzhou Dianzi University"
      ]
    },
    {
      "id": "https://openalex.org/A2137280915",
      "name": "Kong Wanzeng",
      "affiliations": [
        "Hangzhou Dianzi University",
        "Center for Excellence in Brain Science and Intelligence Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2109451195",
      "name": "Yu Ding",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136456385",
      "name": "Qibin Zhao",
      "affiliations": [
        "RIKEN Center for Advanced Intelligence Project"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3022398031",
    "https://openalex.org/W2546919788",
    "https://openalex.org/W2904518532",
    "https://openalex.org/W2740550900",
    "https://openalex.org/W6756739252",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W2519656895",
    "https://openalex.org/W1517538752",
    "https://openalex.org/W6759363029",
    "https://openalex.org/W2985862548",
    "https://openalex.org/W2947476638",
    "https://openalex.org/W6864014924",
    "https://openalex.org/W2901272442",
    "https://openalex.org/W6863631769",
    "https://openalex.org/W3034646123",
    "https://openalex.org/W2556418146",
    "https://openalex.org/W4287780442",
    "https://openalex.org/W2733520819",
    "https://openalex.org/W2964216663",
    "https://openalex.org/W2963710346",
    "https://openalex.org/W2808359495",
    "https://openalex.org/W2962931510",
    "https://openalex.org/W3037572520",
    "https://openalex.org/W2753840835",
    "https://openalex.org/W3128412859",
    "https://openalex.org/W2964346351",
    "https://openalex.org/W2964010806",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2886193235",
    "https://openalex.org/W2787581402"
  ],
  "abstract": "The heart of multimodal learning research lies the challenge of effectively exploiting fusion representations among multiple modalities.However, existing two-way cross-modality unidirectional attention could only exploit the intermodal interactions from one source to one target modality. This indeed fails to unleash the complete expressive power of multimodal fusion with restricted number of modalities and fixed interactive direction.In this work, the multiway multimodal transformer (MMT) is proposed to simultaneously explore multiway multimodal intercorrelations for each modality via single block rather than multiple stacked cross-modality blocks. The core idea of MMT is the multiway multimodal attention, where the multiple modalities are leveraged to compute the multiway attention tensor. This naturally benefits us to exploit comprehensive many-to-many multimodal interactive paths. Specifically, the multiway tensor is comprised of multiple interconnected modality-aware core tensors that consist of the intramodal interactions. Additionally, the tensor contraction operation is utilized to investigate intermodal dependencies between distinct core tensors.Essentially, our tensor-based multiway structure allows for easily extending MMT to the case associated with an arbitrary number of modalities. Taking MMT as the basis, the hierarchical network is further established to recursively transmit the low-level multiway multimodal interactions to high-level ones. The experiments demonstrate that MMT can achieve state-of-the-art or comparable performance.",
  "full_text": "MMT: Multi-Way Multi-Modal Transformer for Multimodal Learning\nJiajia Tang1\u0003, Kang Li1\u0003, Ming Hou2 , Xuanyu Jin1 , Wanzeng Kong1y, Yu Ding3\nand Qibin Zhao2\n1Key Laboratory of Brain Machine Collaborative Intelligence of Zhejiang Province, School of Computer\nScience and Technology, Hangzhou Dianzi University, China\n2RIKEN Center for Advanced Intelligence Project (AIP), Japan\n3Virtual Human Group, Netease Fuxi AI Lab\n{hdutangjiajia, jxuanyu599}@163.com,{likang bro, kongwanzeng}@hdu.edu.cn,{ming.hou,\nqibin.zhao}@riken.jp, dingyu01@corp.netease.com\nAbstract\nThe heart of multimodal learning research lies the\nchallenge of effectively exploiting fusion repre-\nsentations among multiple modalities. However,\nexisting two-way cross-modality unidirectional at-\ntention could only exploit the intermodal interac-\ntions from one source to one target modality. This\nindeed fails to unleash the complete expressive\npower of multimodal fusion with restricted num-\nber of modalities and ﬁxed interactive direction.\nIn this work, the multiway multimodal transformer\n(MMT) is proposed to simultaneously explore mul-\ntiway multimodal intercorrelations for each modal-\nity via single block rather than multiple stacked\ncross-modality blocks. The core idea of MMT\nis the multiway multimodal attention, where the\nmultiple modalities are leveraged to compute the\nmultiway attention tensor. This naturally beneﬁts\nus to exploit comprehensive many-to-many mul-\ntimodal interactive paths. Speciﬁcally, the multi-\nway tensor is comprised of multiple interconnected\nmodality-aware core tensors that consist of the in-\ntramodal interactions. Additionally, the tensor con-\ntraction operation is utilized to investigate inter-\nmodal dependencies between distinct core tensors.\nEssentially, our tensor-based multiway structure al-\nlows for easily extending MMT to the case as-\nsociated with an arbitrary number of modalities.\nTaking MMT as the basis, the hierarchical net-\nwork is further established to recursively transmit\nthe low-level multiway multimodal interactions to\nhigh-level ones. The experiments demonstrate that\nMMT can achieve state-of-the-art or comparable\nperformance.\n1 Introduction\nMultimodal learning is a very actively growing ﬁeld of re-\nsearch, which has witnessed many signiﬁcant advances in\n∗Equal contribution\n†Corresponding author: Wanzeng Kong\nsentiment analysis and speaker personality recognition [Ain\net al., 2017 ]. Indeed, the multimodal signals such as text\nmodality, visual modality, and acoustic modality consist of\ncomplementarity and consistency. Consequently, the heart of\nmultimodal learning lies the challenge of effectively account-\ning for the both intra-modality and inter-modality intercorre-\nlations among multiple modalities.\nMore recently, with the advent of self-attention mecha-\nnism, transformer-based fusion frameworks [Soleymani et\nal., 2017] have been extensively employed for exploiting the\nlong-range dependencies of modalities. For instance, MISA\n[Hazarika et al., 2020] and MAG [Rahman et al., 2020] lever-\nage self-attention to retrieve the contextual embeddings of\ntextual modality. Note that, the self-attention focuses on\nhighlighting the intra-modality dependencies within the in-\ndividual modality, leading to a lack of explicit interaction\namong different modalities. On the basis of conventional self-\nattention, MulT [Tsai et al., 2019 ] proposes a novel cross-\nmodality two-way attention component, which beneﬁts the\nmodel to directly explore the explicit cross-modality inter-\ncorrelations within two-way space. However, the unidirec-\ntional cross-modality attention could only exploit the inter-\nmodality interactions from one source to one target modal-\nity. Therefore, the above architectures have shown the lim-\nitation in offering the much more representative capabil-\nity with the ﬁxed interactive direction and restricted num-\nber of involved modalities, e.g., only up to two modalities.\nThis indeed overlooks the complex and essential global in-\nteractions among more modalities, which results in the in-\nformation loss and the deterioration of prediction. Essen-\ntially, their model attempt to sequentially stack distinct two-\nway cross-modality attention blocks into the hierarchical one\nfor the multimodal learning task, ie., (text → visual) →\nacoustic. Intuitevely, this sequential one-to-one procedure\n{Modalityi}→{Modality j}would lead to the signiﬁcant\nincrease of cost in computation and memory.\nTo overcome the above underlying research issues, we\npropose the multi-way multi-modal Transformer (MMT),\na method that extends the standard Transformer frame-\nwork to analyze multiple modalities simultaneously.\nThe core idea of MMT is the tensor-based multiway\nmultimodal attention, which provides an M-dimension\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3458\n                      (                                           ) \nDot product\nMultiway Multimodal Attention block\nMultiway \nInteractive\n(a) MMT{ X1;X2;X3}→{ X1;X2;X3}\nDot product\n                 (                                    ) \n(Source) (Target)\nTwo-way crossmodal Attention block\nTwo-way \nInteractive\n (b) MulTX1→X 2\nFigure 1: Comparison of MMT with existing crossmodal attention model. In our model, multiway multimodal attention is proposed to\nexploit multiple modality-aware multimodal intercorrelations simultaneously via the single block. Note that, compared to the two-way\nattention model, MMT is able to provide a multiway attention space that consists of many-to-many interactive paths, which signiﬁcantly\nboosting the expressive capability and efﬁciency of the learning model.\nattention space that consists of many-to-many multi-\nmodal interactive paths {Modality1;···;ModalityM } →\n{Modality1;···;ModalityM }, where M indicates the\nmodality number. This naturally contributes to both intra-\nmodality and complex inter-modality interactions. Thanks\nto the low-rank tensor fashion, the proposed module has the\nsuperior capability to simultaneously explore the multiway\nmultimodal attention space for each modality within the\nsingle attention block rather than multiple stacked cross-\nmodality blocks. Indeed, the presented fashion allows for all\nthe potential and comprehensive multiway interactions, as\nwell as a much lighter model. It is important to note that,\ncompared to the existing two-way attention module, the\ntensor-based multiway structure naturally provides us the\ngreat ﬂexibility to extend MMT to the case associated with\nan arbitrary number of modalities. Intuitively, MMT is able\nto scale linearly in the number of modalities. Subsequently,\ntaking MMT as the basis, the hierarchical architecture is\nestablished to transmit the low-level multiway multimodal\nintercorrelations to the high-level sophisticated presentation\nwith the recursive fashion.\n= ≈\nFigure 2: For the tensor-ring based Q, the corresponding entries are\nexpressed as qtt;tv;ta = tr(G(t)\nQ (:;tt;:)G(v)\nQ (:;tv;:)G(a)\nQ (:;ta;:)):\n2 Related Work\n2.1 Non-attention based Model\nRMFN [Liang et al., 2018] introduced the RNN to process the\ndata with the multistage fusion method. BC-LSTM [Poria et\nal., 2017] presented a bi-directional LSTM to exploit the tem-\nporal dependencies. MV-LSTM [Rajagopalan et al., 2016 ]\nand Self-MM [Yuet al., 2021] utilized the multi-view (multi-\ntask) strategy to compute sentiment information. Analo-\ngously, ICCN [Sun et al., 2020 ], DF [Nojavanasghari et al.,\n2016] and MISA attempt to capture the correlations across\nmultiple modalities via similarity bock. Moreover, tensor-\nbased works such as TFN [Zadeh et al., 2017] and LMF [Liu\net al., 2018 ] proposed a multi-dimension fusion fashion to\nexplicitly highlight multimodal intercorrelations among mul-\ntiple messages.Nevertheless, the above frameworks fail to ef-\nfectively draw the long-range temporal dependencies along\nthe sequence.\n2.2 Attention-based Model\nCompared to the above models, the attention-based network\nsuch as MFM [Tsai et al., 2018] is able to model long-range\ndependencies of the modality. RA VEN [Wang et al., 2019 ]\nproposed a multimodal attention gating block. Similarly,\nMAG proposed the multimodal attention gating component.\nMCTN [Pham et al., 2019] utilized the cyclic translation net-\nwork to investigate multimodal representations. MFN[Zadeh\net al., 2018a ] leveraged the Delta-memory attention to ana-\nlyze the private modality portion. On the contrast, MARN\n[Zadeh et al., 2018b] employed multimodal attention to high-\nlight cross-modality dynamics. MEMI [Wu et al., 2020 ] in-\ntroduced the cross-modality attention to measure the two-\nway interactions. Additionally, MulT presented the two-way\ncross-modality attention to exploit the inter-modality unidi-\nrectional interactions. However, the above manners fail to ex-\nploit all the potential and comprehensive multimodal interac-\ntions, due to the ﬁxed interactive direction and restricted num-\nber of involved modalities (only up to two modalities). Es-\nsentially, the existing models attempt to stack multiple cross-\nmodality blocks into the hierarchical one for realizing the\nmultimodal learning task, thus suffering from the signiﬁcant\nincrease of computational complexity and memory.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3459\nLinear Tensor Generation \nFunction\nAverage Pooling\nLinear Function\nFigure 3: Multiway Multimodal Attention Block: Given the modality presentations Xi, the proposed attention block attempts to exploit the\nmultiway attention space QKthat consists of many-to-many interactive paths fX1;\u0001\u0001\u0001 ;XIg!f X1;\u0001\u0001\u0001 ;XIg. This indeed brings forth\nthe much more comprehensive and sophisticated multimodal intercorrelations, as well as the boost of expressive capability.\n3 Methodology\n3.1 Preliminaries\nIn this part, we attempt to introduce the notation of the tensor\nnetwork utilized in our model, and present in detail the con-\nventional self-attention and cross-modality attention mecha-\nnism.\nTensor Network\nMulti-dimensional array can be represented by tensor.\nSpeciﬁcally, an Nth-order tensor S ∈ RI1×···×IN has\nN dimensions, and the speciﬁc entry of S is denoted by\nsi1;i2;···;iN = S(i1;i2;···;iN ), where iN ∈{1;2;···;IN }.\nAdditionally, the tensor network - Tensor Ring can be further\nemployed to decompose the higher-order tensor Sinto the\nlow-rank case. That is to say, a set of interconnected lower-\nrank tensors (core tensors)JG(1);···;G(N)K could be utilized\nto represent S. Notation JK is a simpliﬁed version of tensor\nring decomposition. Note that, the hadamard product denoted\nas ~ and the mode-1 Khatri-Rao product denoted as ⊙1 are\nthe essential operators in tensor analysis. Given two tensors\nA∈ RI1×I2×I3 and B∈ RI1×I2×I3 , the hadamard product\nyieds a 3-order tensor C= A~ B∈ RI1×I2×I3 with entries\nC(i1;i2;i3) = A(i1;i2;i3)B(i1;i2;i3). Given two matrixes\nA∈RN×P1 and B ∈RN×P2 , the mode-1 Khatri-Rao prod-\nuct yieds a matrix C = A⊙1 B =∈RN×P1P2 .\nSelf-attention and Cross-modality Attention\nAt the heart of Transformer is the self-attention mechanism,\nwhich attends to the long-range dependencies within the uni-\nmodality sequence. More speciﬁcally, Transformer is com-\nprised of query, key and value matrix denoted as Q, K, V\nrespectively. Considering the modalities set {Audio, Video,\nText}, the utterance-level presentation are represented as\nXa ∈ RTa×da, Xv ∈ RTv×dv , and Xt ∈ RTt×dt, re-\nspectively. In practice, Ti is utilized to denote sequence\nlength, and di refers to the feature dimension. The corre-\nsponding audio-based self-attention procedure can be written\nas:Attention(Qa;Ka;Va) = softmax( QaKT\na\n√dk\n)Va, which\nonly focus on the intra-modality correlations from the uni-\nmodality feature space.\nOn the basis of self-attention mechanism, MulT proposed\nthe cross-modality attention mechanism, which performs the\nunidirectional cross-modal adaption within the two-way fu-\nsion space that consists of two corresponding modalities. For\ninstance, the adaption text(t) → audio(a) can be written\nas Attention(Qa;Kt;Vt) = softmax( QaKT\nt\n√dk\n)Vt, which at-\ntends to the inter-modality correlations. Note that, the two-\nway cross-modality attention requires 4 transformer blocks\n{MulTt→a, MulTa→t, MulTa→a, MulTt→t}to obtain\nthe cross-modality dependencies that consist of both intra-\nmodality and inter-modality interactions. More importantly,\nthe extension of MulT to M modalities would require M2\ntransformer blocks, which results in the signiﬁcant increase\nof computations and storage.\n3.2 Multi-way Multi-modal Transformer\nIn this paper, tensor-based multi-way multi-modal atten-\ntion is proposed to produce the rich representations for\neach modality by simultaneously paying attention to all\nthe potential multiway interactions. Overall, our proposed\nMMT could accepts multiple inputs {Xa;Xv;Xt}, and pro-\nduces the modality-aware multiway multimodal fusion out-\nputs {Ya−aware;Yv−aware;Yt−aware}.\nGiven modality presentation {Xt, Xv, Xa}, we ﬁrst adopt\na tensor-ring based generation function for retrieving the mul-\ntiway multimodal query tensor Q = JG(t)\nQ ;G(v)\nQ ;G(a)\nQ K ∈\nRTa×Tv×Tt and key tensor K = JG(t)\nK ;G(v)\nK ;G(a)\nK K ∈\nRTa×Tv×Tt. Note that, the tensor ring format naturally pro-\nvides us the beneﬁt of efﬁciently measuring the attention\nscore on the low-rank core tensors (G (i)\nQ ∈ RTi×Rw×Rs ,\nG(i)\nK ∈RTi×Rw×Rs) rather than original large tensors (Q ,\nK), contributing to the great decrease of model storage and\ncomplexity, where the index i ∈{a;v;t }. Note that, Rw and\nRs refer to the tensor-ring rank, the index w and s∈{1;2;3},\nand w ̸=s. The above function can be formulated as:\nG(i)\nQ = reshape((XiW(1)\nQi ) \f1 (XiW(2)\nQi )) (1)\nwhere the linear transformation matrixes {W(1)\nQi ∈Rdi×Rw ,\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3460\nW(2)\nQi ∈Rdi×Rs}are utilized to account for the low-rank pro-\njection of input. Additionally, the mode-1 Khatri-Rao product\n⊙1 is introduced to incorporate the low-rank projections into\nthe modality-aware core tensor G(i)\nQ . Similarly, the key tensor\nKis received based on the same generation procedure.\nBased on the above query tensor Qand key tensor Kwith\nthe tensor ring format, the multiway multimodal attention\nprocedure can be presented as follows:\nMMT (Q;K) =MMT (JG(t)\nQ ;G(v)\nQ ;G(a)\nQ K;JG(t)\nK ;G(v)\nK ;G(a)\nK K):\n(2)\nThen, the Hadamard product~ is introduced to analyze the\nsimilarity between G(i)\nQ and G(i)\nK , which allows for exploit-\ning modality-based attention spaces G(i)\nQK ∈RTi×Rw×Rs in\nparallel. Indeed, above process is analogous to self-attention\nprocess (QKT ) in the standard Transformer framework. This\nnaturally results in the multiway multimodal attention tensor\nQK= JG(t)\nQK;G(v)\nQK;G(a)\nQKK ∈RTa×Tv×Tt. The attention core\ntensor G(i)\nQK are presented as follows:\nG(i)\nQK = G(i)\nQ ~ G(i)\nK (3)\nSubsequently, the corresponding attention pooling ma-\ntrixes M(i)\nQK ∈RRw×Rs are captured by averaging the G(i)\nQK\nalong the temporal dimension (T i), where each element of\nM(i)\nQK is comprised of the intra-modality temporal dependen-\ncies. Notably, the M(i)\nQK are then transmitted to the next pro-\ncess for integrating the distinct intra-modality attribution into\nthe multi-way multi-modality attention message.\nM(i)\nQK = average(G(i)\nQK(:;j;p ));1 \u0014j \u0014Rw;1 \u0014p\u0014Rs (4)\nGiven the modality-based attention pooling matrixes\nM(i)\nQK, we now attempt to explore the modality-aware mul-\ntiway multimodal attention attent, attenv and attena. The\ndetailed process is formulated as:\natteni = linear(G(i)\nQK \u00021\n3 M(j1)\nQK \u00021\n3 \u0001\u0001\u0001\u0002 1\n3 M\n(jM−1)\nQK ) 2RTi×di\n(5)\nwhere atteni refer to the modality-aware multiway multi-\nmodal attention matrix, ’M’ is the modality number, i ∈\n{a;v;t}, jm ∈ {a;v;t }, and jm ̸=i. Moreover, oper-\nation ×m\nn refers to the mode-( m\nn ) product (tensor contrac-\ntion operation). More speciﬁcally, the mode -( 1\n3) product\nof tensor G(t)\nQK and matrix M(a)\nQK, with the common mode\nR3, yields a 3-order cross-modality attention tensor is of\nTt ×R2 ×R1. Actually, the above process allows us\nto efﬁciently compute the cross-modality attention space,\ne.g., (G(t)\nQK ×1\n3 M(a)\nQK), which effectively highlights the inter-\ncorrelations between text and audio modality. Compared to\nthe conventional cross-modality two-way attention that only\nfocuses on the inter-modality interactive path, our method si-\nmultaneously attends to the both intra-modality and complex\ninter-modality interactive paths. Then, the mode -( 1\n3) prod-\nuct of the above 3-order tensor (G(t)\nQK ×1\n3 M(a)\nQK) and matrix\nM(v)\nQK, with the common mode R1, yields a 3-order tensor is\nof Tt ×R2 ×R2. Intuitively, above procedure allows the\ncross-modality tensor further absorb the contribution from\nvideo modality, contributing to the comprehensive many-\nto-many multimodal interactive paths {t;a;v}→{t;a;v }.\nNote that, {t;a;v}→{t;a;v }is comprised of the compre-\nhensive many-to-many multimodal interactive paths, which\ncould provide intra-modality and complex inter-modality in-\nteractions simultaneously. More speciﬁcally, Xi → Xi refer\nto the intra-modality interactions, and Xi → Xj refer to the\ninter-modality interactions (i ̸=j), where i;j ∈ {t;a;v }.\nThen, the linear projection function is further applied to re-\ntrieve the text-aware multiway multimodality attention matrix\nattent.\nIn an addition, the modality-aware multiway multimodal-\nity presentation is achieved with the help of corresponding\nmultiway attention matrix and a parameter a. Note that, ais\nintroduced to blend the contribution of modality-aware mul-\ntiway multimodality message and the original modality infor-\nmation, leading to the much more comprehensive multimodal\nrepresentation.\nYi−aware = atteniXi + aXi 2RTi×di (6)\nNote that, the tensor-ring function allows us to exploit\ninter-modality attention from low-rank small core tensors\nrather than original Mth-order large tensor, where M is the\nnumber of modality. This indeed signiﬁcantly reduce the\ncomplexity and storage. If tensor-ring function is done af-\nter getting attention, the number of parameters of Mth-order\ntensor grows exponentially with ‘M’, i.e., parameters = 2 ×\nT1 ×T2×;···;×TM .\nVideo\n Audio\nIt is very \ninteresting!\nText\nMMA \nDot Product\nLayer 1\nFig 3: Hierarchical Framework\nMMT\nMMA \nDot Product \nLayer N\nMMT\nClassfication \nFigure 4: The hierarchical framework associated with N stacked\nMMTs. The proposed network allows us to transmit the low-level\nmultiway multimodal intercorrelations to the high-level compact\npresentation with the recursive manner.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3461\n3.3 Hierarchical Architecture\nOn the basis of MMT, a hierarchical architecture was pro-\nposed for transmitting the low-level multiway multimodal\nintercorrelations to the much more comprehensive and ex-\npressive high-level intercorrelations with the recursive fash-\nion. As illustrated in Figure 4, the proposed framework\nis comprised of N stacked MMT blocks. Speciﬁcally, the\nﬁrst MMT accepts the inputs {Xa;Xv;Xt}and computes\nthe modality-aware multiway multimodal presentation out-\nputs set {Ya−aware;Yv−aware;Yt−aware}. Subsequently, the\nnext MMT takes the outputs of the previous MMT as the in-\nputs, and latently performs the similar multiway multimodal\nattention procedure. As a result, the much more comprehen-\nsive and expressive multimodal intercorrelations can be iden-\ntiﬁed and transmitted to the high-level layer via hierarchical\nstructure. The overall recursive procedure is formulated as\nfollows:\n[Y(1)\na−aware ;Y (1)\nv−aware ;Y (1)\nt−aware ] =MMT (1)(Xa;Xv;Xt)\n[Y(i+1)\na−aware ;Y (i+1)\nv−aware ;Y (i+1)\nt−aware ]\n= MMT (i+1)(Y(i)\na−aware ;Y (i)\nv−aware ;Y (i)\nt−aware );i 2[1;N] (7)\nNote that, due to the tensor-based multiway atten-\ntion strategy, the (i + 1) − th layer of the proposed\nhierarchical architecture is able to accept the inputs\n{Y(i)\na−aware;Y (i)\nv−aware;Y (i)\nt−aware}from the previous i −th\nlayer recursively. That is to say, the presented multiway\nscheme indeed provides great expressive capability and ﬂexi-\nbility of the learning model, giving each layer the strong abil-\nity to efﬁciently perform similar multiway multimodal atten-\ntion based on the previous multiple outputs.\n4 Experiments Setups\n4.1 Datasets\nThe public sentiment benchmark CMU-MOSI [Zadeh et al.,\n2016] is comprised of the aligned and preprocessed audio,\nvideo and text modality. CMU-MOSI consists of 2199 opin-\nion video clips. Each clip is annotated with the correspond-\ning sentiment intensity in the range of [-3, +3], spanning from\nthe strongly negative to the strongly positive sentiments. The\n2199 clips are spilt into 1284 train samples, 229 validation\nsamples, and 686 test samples. The POM dataset [Park et\nal., 2014] contains 903 movie opinion videos, which attends\nto the speaker trait recognition. Each video is annotated for\nfollowing speaker traits with the strength score [1, 5] or [1,\n7] : conﬁdent (con), passionate (pas), dominant (dom), vivid\n(viv), expertise (exp), entertaining (ent), and etc. The divi-\nsion of the train, validation and test sets is 600, 100 and 203,\nrespectively.\n4.2 Features and Evaluation Metrics\nThe modality features are leveraged in the same way as\nMEMI and MAG. The evaluation metrics are demonstrated\nas follows: 1) Mean Absolute Error (MAE) (lower is better);\n2) Pearson’s correlation (corr); 3) Binary Accuracy (Acc-2);\n4) F1-Score (F1); 5)7-class Accuracy (Acc-7). Note that, two\ndistinct manners are introduced to represent Acc-2 and F1:\n1) negative/non-negative classiﬁcation attached with the la-\nbel [-3, 0) and [0, 3][Zadeh et al., 2018b] 2) negative/positive\nclassiﬁcation attached with the label [-3, 0) and (0, 3][Tsai et\nal., 2019]. The marker -/- is employed to distinguish the dis-\ntinct strategies, where the left-side value refer to 1) and the\nright-side value stands for 2).\n4.3 Comparisons\nWe introduce the non-attention and attention based models as\nthe baselines. Non-attention based: RNN-based multistage\nfusion network (RMFN), Bi-directional LSTM (BC-LSTM),\nMulti-view LSTM (MV-LSTM), Interaction Canonical Cor-\nrelation Network (ICCN), Modality-Invariant and -Speciﬁc\nRepresentations for Multimodal Sentiment Analysis (MISA),\nTensor Fusion Network (TFN), Low-rank Multimodal Fusion\n(LMF), Deep multimodal fusion (DF). Attention-based: Re-\ncurrent Attended Variation Embedding Network (RA VEN),\nMultimodal Adaptation Gate (MAG), Multimodal Cyclic\nTranslation Network (MCTN), Memory Fusion Network\n(MFN), Multi-attention Recurrent Network (MARN), Mul-\ntimodal Factorization Model (MFM), Multimodal Explicit\nMany2many Interactions(MEMI), Multimodal Transformer\n(MulT), Self-Supervised Multi-task Multimodal model (Self-\nMM).\n4.4 Training Details and Model Complexity\nThe grid-search is performed over the hyper-parameters to\nﬁnd the model with the best validation task loss. The range\nof key hyper-parameters are summarized as follows: layer [2,\n7], tensor rank [2, 8], residual parameter \u000b [0.1, 0.7]. The\ncomplexity of baselines built upon Bert are summarized as\nfollows: MulT (O(M2 ×d2\ni ×Ti)), TFN (O(dy\nQM\ni=1 di)),\nLMF (O(dy ×Rw ×QM\ni=1 di)), MISA(O(M ×d2\ni ×Ti)),\nMAG(O(QM\ni=1 Ti ×di)), ICCN(O(M×Ti ×d2\ni )). Note that,\nthe complexity of our ’MMT’ isO(M×di ×Ti(Rw + Rs)),\nwhere di>Rw(Rs). Note that our approach use smaller\nmodel rather than larger model, while achieving better per-\nformance.\n5 Experiments Results and Analysis\n5.1 Performance Comparison with State-of-the-art\nModels\nFirstly, we analyze the performance between our MMT and\nthe state-of-the-art baselines. The bottom rows in Table 1 and\nTable 2 illustrate the results of our model. Note that ⊗from\n[Tsai et al., 2019]; a from [Sun et al., 2020]. As shown in Ta-\nble 1 (CMU-MOSI), we can ﬁnd that MMT exceeds the pre-\nvious best MAG on all metrics. Particularly, our MMT out-\nperforms the previous best MISA on the ’Acc-7’ by a margin\nof 5.9%. It is interesting to observe that, our MMT obtains the\nmuch better ’MAE’ (lower is better) than the two-way cross-\nmodality attention model MulT (Bert), with a large margin of\n20.4%. As shown in Table 2 (POM), MMT exceeds the MulT\non the ’Corr’ of Con by a margin of 14.3%. This indeed im-\nplies the superior expressive capability and efﬁciency of the\nproposed multiway multimodality attention mechanism and\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3462\nModels CMU-MOSI\nMAE(↓) Corr(↑) Acc-2(↑) F1(↑) Acc-7(↑)\nBC-LSTM 1.079 0.581\n73.9/- 73.9/- 28.7\nMV-LSTM 1.019 0.601\n73.9/- 74.0/- 33.2\nRMFN ⊗ 0.922 0.681\n78.4/- 78.0/- 38.3\nRAVEN ⊗ 0.915 0.691\n78.0/- 76.6/- 33.2\nMFN 0.965 0.632\n77.4/- 77.3/- 34.1\nMARN 0.968 0.625\n77.1/- 77.0/- 34.7\nTFN 0.970 0.633\n73.9/- 73.4/- 32.1\nLMF 0.912 0.668\n76.4/- 75.7/- 32.8\nMCTN ⊗ 0.909 0.676\n79.3/- 79.1/- 35.6\nMFM ⊗ 0.951 0.662\n78.1/- 78.1/- 36.2\nBert 0.739 0.782\n83.5/85.2 83.4/85.2 -\nMulT 0.871 0.698\n-/83.0 -/82.8 40.0\nTFN (Bert)\na\n0.901 0.698\n-/80.8 -/80.7 34.9\nLMF(Bert)\na\n0.917 0.695\n-/82.5 -/82.4 33.2\nMulT (Bert) 0.861 0.711\n81.5/84.1 80.6/83.9 -\nICCN (Bert) 0.860 0.710\n-/83.0 -/83.0 39.0\nMISA (Bert) 0.783 0.761\n81.8/83.4 81.7/83.6 42.3\nMAG (Bert) 0.712 0.796\n84.2/86.1 84.1/86.0 -\nSelf-MM (Bert) 0.713 0.798\n84.0/85.98 84.42/85.95 -\nMMT (ours)\n(Bert) 0.657 0.83\n85.8/87.0 85.8/87.0 48.2\nTable 1: Performances of baselines and MMT based on BERT in\nCMU-MOSI benchmark.\nMOSI \nACC-7\n42\n43\n44\n45\n46\nLayer\n2 3 4 5 6 7\nMOSI \nMAE\n0.69\n0.705\n0.72\n0.735\n0.75\nLayer\n2 3 4 5 6 7\nPOM (Dom)\nACC-7\n32\n33.5\n35\n36.5\n38\nLayer\n2 3 4 5 6 7\nPOM (Dom)\nMAE\n0.83\n0.853\n0.875\n0.898\n0.92\nLayer\n2 3 4 5 6 7\nFigure 5: Effect of the layer on CMU-MOSI and POM.\nthe hierarchical structure. Compared to the two-way attention\nmodel, our presented MMT is capable of exploring the mul-\ntiway attention space that consists of the many-to-many mul-\ntimodal interactive path, which naturally beneﬁts the learn-\ning model to measure both intermodal and intramodal inter-\nactions simultaneously. Essentially, thanks to the low-rank\ntensor-based strategy, MMT has the ability to effectively per-\nform the multimodal learning task only with a relatively small\nnumber of storage and computational complexity.\n5.2 Effect of the Layer of the Framework\nIn this part, we attempt to investigate the impact of various\nlayers of the hierarchical framework on the task performance.\nThe layer varies from 2 to 7. As shown in Figure 5, we\ncan observe that MMT reaches the peak value at the layer\n3 on the ’Acc-7’ on CMU-MOSI. Moreover, the MMT max-\nimizes the performance at the layer 4 on the ’MAE’ for the\ncase of CMU-MOSI. As for the Dom of POM benchmark,\nMMT reaches the highest point at the layer 3 on the ’Acc-7’,\nTask Ent Con\nPas Dom Viv Exp\nClasses 7 7\n7 7 7 5\nMetric MAE ↓\nLSTM 0.996 1.073\n1.148 0.904 1.045 1.067\nBC-LSTM 0.988 1.089\n1.141 0.915 1.024 1.096\nTFN 1.062 1.491\n1.335 1.077 1.184 1.215\nDF 0.972 1.097\n1.130 0.899 1.023 1.053\nMARN 1.011 1.057\n1.184 0.916 1.053 1.105\nMulT 0.961 0.989\n1.087 0.869 0.975 0.998\nMEMI 0.952 0.979\n1.108 0.856 0.959 0.957\nMMT (ours) 0.911 0.941\n0.987 0.845 0.944 0.934\nMetric Corr ↑\nLSTM 0.176 0.233\n0.179 0.201 0.172 0.153\nBC-LSTM 0.083 0.200\n0.219 0.318 0.241 0.177\nTFN 0.265 0.159\n0.158 0.067 0.232 0.149\nDF 0.254 0.164\n0.353 0.314 0.296 0.153\nMARN 0.020 0.219\n0.102 0.130 0.065 -0.008\nMulT 0.267 0.294\n0.332 0.282 0.286 0.319\nMEMI 0.275 0.432\n0.327 0.395 0.392 0.365\nMMT (ours) 0.386 0.437\n0.43 0.368 0.363 0.418\nMetric Acc (%) ↑\nLSTM 30.5 25.1\n25.1 31.5 29.6 27.6\nBC-LSTM 30.0 22.2\n21.7 32.0 30.0 27.1\nTFN 31.0 17.2\n23.2 33.0 29.1 24.1\nDF 27.6 25.1\n21.7 31.5 28.6 27.1\nMARN 32.5 28.6\n24.6 34.5 31.0 27.6\nMulT 31.5 25.1\n31 34 35 27.6\nMEMI 33.5 29.6\n28.1 34.5 40.9 38.4\nMMT (ours) 34.5 33.5\n34 39.9 39.4 35.5\nTable 2: Performances of baselines and MMT in POM benchmark.\nCMU-MOSI\nAcc-7\n39\n40.75\n42.5\n44.25\n46\nR1 R2\nR3\n2 3 4 5 6 7 8\nRank \nFigure 6: Effect of the tensor ranks on CMU-MOSI.\nand the relatively higher performance peak at the layer 2 on\nthe ’MAE’. Intuitively, the relative deeper framework bears\nthe potential to exploit the much more comprehensive and\nsophisticated sentimental attribution among multiple modal-\nity presentations. Subsequently, the deeper cases naturally\nachieve better performance than the shallow ones. Addition-\nally, the too complex MMT consists of overmuch redundancy\nmessage, which may result in the deterioration of the task per-\nformance. In conclusion, the experiment results demonstrate\nthat the hierarchical structure indeed is able to provide greater\nexpressive power and efﬁciency.\n5.3 Effect of Tensor Ranks of Tensor-ring Network\nIn this test, we are interested to examine how distinct tensor\nranks affect the predictive performance of our tensor-based\nnetwork. For simplicity, we attempt to analyze the perfor-\nmance of each ri separately, while the rest of tensor ranks\nrj are ﬁxed, where j ̸=i. For instance, as shown in ﬁgure\n6, r1 varies from 2 to 8, while r2 and r3 are ﬁxed. In Fig-\nure 6, we can observe that the case of r1 and r2 achieve the\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3463\nCMU-MOSI\nPerformance\n83\n83.75\n84.5\n85.25\n86\nModality\nAcc-2 F1\n(a, t) (v, t) (a, v, t)\nPOM (Dom)\nPerformance\n0\n10\n20\n30\n40\nModality\nAcc-7\nCorr\n(a, t) (v, t) (a, v, t)\nFigure 7: Effect of the number of involved modalities on CMU-\nMOSI and POM.\nMOSI (a-t)\nPerformance\n82\n82.75\n83.5\n84.25\n85\nParameter \n0.1 0.2 0.3 0.4 0.5 0.6 0.7\nAcc-2\nF1\nα\nMOSI (v-t)\nPerformance\n82.8\n82.95\n83.1\n83.25\n83.4\nParameter \n0.1 0.2 0.3 0.4 0.5 0.6 0.7\nAcc-2\nF1\nα\nFigure 8: Effect of the residual parameter \u000bon CMU-MOSI.\nbest performance at rank 2. Similarly, the case of r3 reaches\nthe peak point at the relatively low-rank value rank 4. This\nimplies that the tensor-based fashion indeed gives the learn-\ning model sufﬁcient expressive capability to efﬁciently ex-\nploit the task-related representation. Notably, the fairly good\nresults of low-rank cases demonstrate that tensor-based net-\nwork is able to signiﬁcantly decrease the computational com-\nplexity and storage without the greater deterioration of pre-\ndiction. The above observations signify the effectiveness and\nnecessity of leveraging the tensor network to deal with the\nmultiway multimodal attention mechanism, when analyzing\nthe case associated with an arbitrary number of modalities.\n5.4 Effect of the Number of Involved Modalities\nDue to the ﬂexibility of our proposed multimodal learning ar-\nchitecture, we can provide a multiway multimodal attention\nspace based on multiple involved modalities. Consequently,\nin this part, we attend to analyze the distinct framework de-\nsign associated with the speciﬁc number of modalities. As\nshown in Figure 7, the multimodal case (a, v, t) performs\nsigniﬁcantly better than the bi-modality cases {(a;t);(v;t)}.\nAnd, the POM depicts similar results. The above perfor-\nmance may indicate that our MMT gives the model strong\nability to highlight much more compact consistency and com-\nprehensive complementarity within the multiway multimodal\nattention space, especially when the model includes much\nmore modality information. Essentially, compared to results\nof the bi-way cross-modality attention model such as MulT\nwhich performed on the multimodal setting (a, v, t) (shown\nin Table 1), the test bi-modality setting (t, v) of our MMT\nobtains the comparable ’Acc-2’ (84.1) and better ’F1’ (84.1)\nprecisions, indicating the great expressive power brought by\nthe proposed multiway attention mechanism. Indeed, the bi-\nway cross-modality attention mechanism only concerns the\nintermodal interactions with the limits of structure, while\nMMT is able to provide both intermodal and intramodal in-\ntercorrelations.\n5.5 Effect of the Residual Parameter\u000b\nIn our work, the residual parameter \u000bis introduced to blend\nthe contribution of modality-aware multiway multimodality\nmessage and original modality message. Thus, we attempt to\nexamine how distinct \u000baffects the learning efﬁciency of the\npresented model. The \u000b ranges from 0.1 to 0.7. Note that,\nall the modalities {audio, video, text}share the same \u000b. For\nsimplicity, the related testing are performed on the settings\n{(a, t), (v, t)}. In Figure 8, we can observe that the MMT can\nreach the good task performance with respect to the tested\n\u000b. In particular, the proposed learning model maximizes the\nprediction results at the relatively low \u000b0.5 or 0.6 for all the\ntesting settings. Intuitively, the case associated with the lower\n\u000b mainly attends to the generated multimodal intercorrela-\ntions among the multiple modalities, and largely overlooks\nthe intrinsic sentimental attribution of the modality. Thus,\nthis may result in the lack of sufﬁcient comprehensive multi-\nmodal sentiment properties, as well as the deterioration of the\ntask performance. Additionally, the cases associated with the\ntoo large \u000bare likely to mainly concentrate on the sentimen-\ntal information of the original modality presentation, which\nmay introduce too much redundancy to the ﬁnal multimodal\nfusion message that may lead to the overﬁtting issue.\n6 Conclusion\nIn this paper, we proposed the tensor-based multiway mul-\ntimodal transformer for exploring the modality-aware multi-\nway multimodal intercorrelations for each modality in paral-\nlel. Indeed, compared to the unidirectional cross-modality at-\ntention, MMT is able to accommodate all potential multiway\nsophisticated interactions from all modalities simultaneously,\nusing the novel multiway multimodal attention. Essentially,\nthe tensor-based network naturally provides us the great ﬂex-\nibility to extend MMT to the case associated with an arbitrary\nnumber of modalities. In addition, MMT enjoys great scala-\nbility with respect to the number of modalities. In practice,\nMMT can also serve as the strong baseline to efﬁciently an-\nalyze the relation-extraction tasks, leading to the much more\nlatent and comprehensive relations within the multiway rep-\nresentative space.\nAcknowledgments\nThis work was supported by National Key R&D Pro-\ngram of China for Intergovernmental International Sci-\nence and Technology Innovation Cooperation Project (No.\n2017YFE0116800), National Natural Science Foundation of\nChina (Grant No.U20B2074, U1909202), JSPS KAKENHI\n(Grant No.20H04249, 20H04208), and supported by Key\nLaboratory of Brain Machine Collaborative Intelligence of\nZhejiang Province (2020E10010).\nReferences\n[Ain et al., 2017] Qurat Tul Ain, Mubashir Ali, Amna Riaz,\nAmna Noureen, Muhammad Kamran, Babar Hayat, and\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3464\nA Rehman. Sentiment analysis using deep learning tech-\nniques: a review. Int J Adv Comput Sci Appl, 8(6):424,\n2017.\n[Hazarika et al., 2020] Devamanyu Hazarika, Roger Zim-\nmermann, and Soujanya Poria. Misa: Modality-invariant\nand-speciﬁc representations for multimodal sentiment\nanalysis. In Proceedings of the 28th ACM International\nConference on Multimedia, pages 1122–1131, 2020.\n[Liang et al., 2018] Paul Pu Liang, Ziyin Liu, Amir Zadeh,\nand Louis-Philippe Morency. Multimodal language anal-\nysis with recurrent multistage fusion. arXiv preprint\narXiv:1808.03920, 2018.\n[Liu et al., 2018] Zhun Liu, Ying Shen, Varun Bharadhwaj\nLakshminarasimhan, Paul Pu Liang, Amir Zadeh, and\nLouis-Philippe Morency. Efﬁcient low-rank multimodal\nfusion with modality-speciﬁc factors. arXiv preprint\narXiv:1806.00064, 2018.\n[Nojavanasghari et al., 2016] Behnaz Nojavanasghari,\nDeepak Gopinath, Jayanth Koushik, Tadas Baltru ˇsaitis,\nand Louis-Philippe Morency. Deep multimodal fusion\nfor persuasiveness prediction. In Proceedings of the\n18th ACM International Conference on Multimodal\nInteraction, pages 284–288, 2016.\n[Park et al., 2014] Sunghyun Park, Han Suk Shim, Moitreya\nChatterjee, Kenji Sagae, and Louis-Philippe Morency.\nComputational analysis of persuasiveness in social mul-\ntimedia: A novel dataset and multimodal prediction ap-\nproach. In Proceedings of the 16th International Confer-\nence on Multimodal Interaction, pages 50–57, 2014.\n[Pham et al., 2019] Hai Pham, Paul Pu Liang, Thomas\nManzini, Louis-Philippe Morency, and Barnab ´as P ´oczos.\nFound in translation: Learning robust joint representations\nby cyclic translations between modalities. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelligence, vol-\nume 33, pages 6892–6899, 2019.\n[Poria et al., 2017] Soujanya Poria, Erik Cambria, Deva-\nmanyu Hazarika, Navonil Majumder, Amir Zadeh, and\nLouis-Philippe Morency. Context-dependent sentiment\nanalysis in user-generated videos. In Proceedings of\nthe 55th annual meeting of the association for computa-\ntional linguistics (volume 1: Long papers), pages 873–\n883, 2017.\n[Rahman et al., 2020] Wasifur Rahman, Md Kamrul Hasan,\nSangwu Lee, Amir Zadeh, Chengfeng Mao, Louis-\nPhilippe Morency, and Ehsan Hoque. Integrating mul-\ntimodal information in large pretrained transformers. In\nProceedings of the conference. Association for Computa-\ntional Linguistics. Meeting, volume 2020, page 2359. NIH\nPublic Access, 2020.\n[Rajagopalan et al., 2016] Shyam Sundar Rajagopalan,\nLouis-Philippe Morency, Tadas Baltrusaitis, and Roland\nGoecke. Extending long short-term memory for multi-\nview structured learning. In European Conference on\nComputer Vision, pages 338–353. Springer, 2016.\n[Soleymani et al., 2017] Mohammad Soleymani, David Gar-\ncia, Brendan Jou, Bj ¨orn Schuller, Shih-Fu Chang, and\nMaja Pantic. A survey of multimodal sentiment analysis.\nImage and Vision Computing, 65:3–14, 2017.\n[Sun et al., 2020] Zhongkai Sun, Prathusha Sarma, William\nSethares, and Yingyu Liang. Learning relationships be-\ntween text, audio, and video via deep canonical correla-\ntion for multimodal language analysis. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence, volume 34,\npages 8992–8999, 2020.\n[Tsai et al., 2018] Yao-Hung Hubert Tsai, Paul Pu Liang,\nAmir Zadeh, Louis-Philippe Morency, and Ruslan\nSalakhutdinov. Learning factorized multimodal represen-\ntations. arXiv preprint arXiv:1806.06176, 2018.\n[Tsai et al., 2019] Yao-Hung Hubert Tsai, Shaojie Bai,\nPaul Pu Liang, J Zico Kolter, Louis-Philippe Morency, and\nRuslan Salakhutdinov. Multimodal transformer for un-\naligned multimodal language sequences. In Proceedings\nof the conference. Association for Computational Linguis-\ntics. Meeting, volume 2019, page 6558. NIH Public Ac-\ncess, 2019.\n[Wang et al., 2019] Yansen Wang, Ying Shen, Zhun Liu,\nPaul Pu Liang, Amir Zadeh, and Louis-Philippe Morency.\nWords can shift: Dynamically adjusting word represen-\ntations using nonverbal behaviors. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, volume 33,\npages 7216–7223, 2019.\n[Wu et al., 2020] Liangqing Wu, Dong Zhang, Qiyuan Liu,\nShoushan Li, and Guodong Zhou. Speaker personality\nrecognition with multimodal explicit many2many interac-\ntions. In 2020 IEEE International Conference on Multi-\nmedia and Expo (ICME), pages 1–6, 2020.\n[Yu et al., 2021] Wenmeng Yu, Hua Xu, Ziqi Yuan, and Jiele\nWu. Learning modality-speciﬁc representations with self-\nsupervised multi-task learning for multimodal sentiment\nanalysis. arXiv preprint arXiv:2102.04830, 2021.\n[Zadeh et al., 2016] Amir Zadeh, Rowan Zellers, Eli Pincus,\nand Louis-Philippe Morency. Multimodal sentiment in-\ntensity analysis in videos: Facial gestures and verbal mes-\nsages. IEEE Intelligent Systems, 31(6):82–88, 2016.\n[Zadeh et al., 2017] Amir Zadeh, Minghai Chen, Soujanya\nPoria, Erik Cambria, and Louis-Philippe Morency. Tensor\nfusion network for multimodal sentiment analysis. arXiv\npreprint arXiv:1707.07250, 2017.\n[Zadeh et al., 2018a] Amir Zadeh, Paul Pu Liang, Navonil\nMazumder, Soujanya Poria, Erik Cambria, and Louis-\nPhilippe Morency. Memory fusion network for multi-view\nsequential learning. In Proceedings of the AAAI Confer-\nence on Artiﬁcial Intelligence, volume 32, 2018.\n[Zadeh et al., 2018b] Amir Zadeh, Paul Pu Liang, Soujanya\nPoria, Prateek Vij, Erik Cambria, and Louis-Philippe\nMorency. Multi-attention recurrent network for human\ncommunication comprehension. In Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, volume 32,\n2018.\nProceedings of the Thirty-First International Joint Conference on Artiﬁcial Intelligence (IJCAI-22)\n3465",
  "topic": "Exploit",
  "concepts": [
    {
      "name": "Exploit",
      "score": 0.7940612435340881
    },
    {
      "name": "Modalities",
      "score": 0.7589190006256104
    },
    {
      "name": "Computer science",
      "score": 0.7275661826133728
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.7272006273269653
    },
    {
      "name": "Tensor (intrinsic definition)",
      "score": 0.5470205545425415
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45590347051620483
    },
    {
      "name": "Multimodality",
      "score": 0.4412567913532257
    },
    {
      "name": "Transformer",
      "score": 0.43821603059768677
    },
    {
      "name": "Theoretical computer science",
      "score": 0.33698683977127075
    },
    {
      "name": "Mathematics",
      "score": 0.12625661492347717
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I50760025",
      "name": "Hangzhou Dianzi University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210126580",
      "name": "RIKEN Center for Advanced Intelligence Project",
      "country": "JP"
    }
  ]
}