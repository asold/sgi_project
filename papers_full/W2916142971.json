{
  "title": "Pretrained language model transfer on neural named entity recognition in Indonesian conversational texts",
  "url": "https://openalex.org/W2916142971",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4302260502",
      "name": "Leonandya, Rezka",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4292347682",
      "name": "Ikhwantri, Fariz",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1997064390",
    "https://openalex.org/W3146885639",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2962902328",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W2557800753",
    "https://openalex.org/W70399244",
    "https://openalex.org/W2574177360",
    "https://openalex.org/W3038058348",
    "https://openalex.org/W2759245808",
    "https://openalex.org/W2963651521",
    "https://openalex.org/W2890887452",
    "https://openalex.org/W2543306797",
    "https://openalex.org/W1584533718",
    "https://openalex.org/W1978563169",
    "https://openalex.org/W2120354757",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3216164635",
    "https://openalex.org/W2004384146",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2766179782",
    "https://openalex.org/W1547763504",
    "https://openalex.org/W2964052092",
    "https://openalex.org/W2574741565",
    "https://openalex.org/W2963706742",
    "https://openalex.org/W2128313564",
    "https://openalex.org/W2093647425",
    "https://openalex.org/W2963430224",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2963751529",
    "https://openalex.org/W2549835527"
  ],
  "abstract": "Named entity recognition (NER) is an important task in NLP, which is all the more challenging in conversational domain with their noisy facets. Moreover, conversational texts are often available in limited amount, making supervised tasks infeasible. To learn from small data, strong inductive biases are required. Previous work relied on hand-crafted features to encode these biases until transfer learning emerges. Here, we explore a transfer learning method, namely language model pretraining, on NER task in Indonesian conversational texts. We utilize large unlabeled data (generic domain) to be transferred to conversational texts, enabling supervised training on limited in-domain data. We report two transfer learning variants, namely supervised model fine-tuning and unsupervised pretrained LM fine-tuning. Our experiments show that both variants outperform baseline neural models when trained on small data (100 sentences), yielding an absolute improvement of 32 points of test F1 score. Furthermore, we find that the pretrained LM encodes part-of-speech information which is a strong predictor for NER.",
  "full_text": "Pretrained language model transfer on neural named\nentity recognition in Indonesian conversational texts\nRezka Leonandya, Fariz Ikhwantri\nKata Research Team, kata.ai, Jakarta, Indonesia\n{rezka,fariz}@kata.ai\nAbstract. Named entity recognition (NER) is an important task in NLP, which is\nall the more challenging in conversational domain with their noisy facets. More-\nover, conversational texts are often available in limited amount, making super-\nvised tasks infeasible. To learn from small data, strong inductive biases are re-\nquired. Previous work relied on hand-crafted features to encode these biases until\ntransfer learning emerges. Here, we explore a transfer learning method, namely\nlanguage model pretraining, on NER task in Indonesian conversational texts. We\nutilize large unlabeled data (generic domain) to be transferred to conversational\ntexts, enabling supervised training on limited in-domain data. We report two\ntransfer learning variants, namely supervised model ﬁne-tuning and unsupervised\npretrained LM ﬁne-tuning. Our experiments show that both variants outperform\nbaseline neural models when trained on small data (100 sentences), yielding an\nabsolute improvement of 32 points of test F1 score. Furthermore, we ﬁnd that the\npretrained LM encodes part-of-speech information which is a strong predictor for\nNER.\nKeywords: language model pretraining ·ﬁne-tuning ·transfer learning ·named\nentity recognition ·low resource language\n1 Introduction\nNamed entity recognition (NER), the task of assigning a class to a word or phrase\nof proper names in text, is an essential ability for conversational agents to have. For\nexample, in food delivery application, an agent needs to acquire information about the\ncustomer’s food detail and address. NER is all the more challenging on conversational\ntexts because of their noisy characteristics, such as typos, informal word variations,\nand inconsistent naming in named entities. Furthermore, conversational texts are often\navailable in diverse domains and limited amount, making supervised training arduous\ndue to data limitation. To learn from limited data, strong inductive biases are necessary.\nIn this work, we explore transfer learning techniques as a way to help neural models\nlearn and generalize from limited data on NER task in Indonesian conversational texts.\nTransfer learning, or sometimes known as domain adaptation, is an important ap-\nproach in NLP application, especially if one does not have enough data in the target\ndomain. In such scenarios, the goal is to transfer knowledge from source domain with\nlarge data to target domain so as to improve the model performance on the target do-\nmain and prevent overﬁtting. Early research in transfer learning, especially with entity\narXiv:1902.07938v1  [cs.CL]  21 Feb 2019\nrecognition in mind, were tackled by feature augmentation [8], bootstrapping [33], and\neven rule-based approach [6].\nRecently, neural networks emerge as one of the most potent tools in almost all NLP\napplications. Although neural models have achieved impressive advancement, they still\nrequire an abundant amount of data to reach good performance. With limited data, neu-\nral models generalization ability is severely curtailed, especially across different do-\nmains where the train and test data distributions are different [15]. Therefore, transfer\nlearning becomes more critical for neural models to enable them to learn in data-scarce\nsettings.\nTransfer learning in NLP is typically done with two techniques, namely parameter\ninitialization (INIT) and multi-task learning (MULT). INIT approach ﬁrst trains the\nnetwork on source domain and directly uses the trained parameters to initialize the\nnetwork on target domain [16], whereas MULT simultaneously trains the network with\nsamples from both domains [1]. Recently, INIT approaches were made highlight by\nthe incorporation of pretrained language models [22, 24, 26] to neural models, reaching\nstate-of-the-art performance across various NLP tasks.\nIndonesian NER itself has attracted many years of research, from as early as using\na rule-based approach [5] to more recent machine learning techniques, such as condi-\ntional random ﬁeld (CRF) [17, 19, 31], and support vector machine (SVM) [3, 29]. The\nlatest research was done by [14] where they investigated neural models performance\nwith word-level and character-level features in Indonesian conversational texts.\nIn this paper, we apply and evaluate a recent technique of transfer learning, namely\nlanguage model pretraining. We use the pretrained LM to extract additional word em-\nbedding input representations for the neural sequence labeler in Indonesian conversa-\ntional texts. The work in this paper is organized as follows: We ﬁrst train a language\nmodel on generic domain unlabeled Indonesian texts U (e.g., Wikipedia). We then use\na smaller domain-speciﬁc source corpus S (e.g., task-oriented conversational texts) to\neither: (a) ﬁne-tune the pretrained LM or (b) train a neural sequence labeler using the\npretrained LM’s representation as additional input. If we proceed with (a), then the next\nstep is to train a neural sequence labeler on the target domainT (e.g., small-talk conver-\nsational texts) using ﬁne-tuned LMs representation as additional input. If we proceed\nwith (b), then the next step is to ﬁne-tune the neural sequence labeler on the target do-\nmain corpus T. We evaluate and compare our approach with other models, namely a\nneural sequence labeler without LM pretraining and a multi-task approach trained on\nvarying amount of training data from T.\n2 Methodology\nTo allow models to learn from small conversational data, we introduce two variants of\nthree-step training procedure. Both variants use additional input of word embedding\nrepresentations derived from a bidirectional LSTM language model (biLM). The word\nembedding representations used in this paper is ELMo [22].\n2.1 Deep Bidirectional Language Models\nELMo derived from pretrained biLM [22] works exceptionally well in practice, ob-\ntaining state-of-the-art performance in six challenging natural language understanding\ntasks. The biLM is trained by jointly maximizing the log likelihood of the forward and\nbackward language models:\nN∑\nk=1\n(log p(tk|t1,...,t k−1; − →Θ)) + (logp(tk|tk+1,...,t N; ← −Θ))\nwhere N is the sequence length, tk is the token at timestep k, and − →Θ and ← −Θ are the\nforward and backward LSTM parameters, respectively.\nTo compute ELMo, ﬁrst a set of representations is collected from the biLM. For\neach token tk, we collect 2L+ 1representations from the biLM:\nRk = {xk,− →hk,j,← −hk,j|j = 1,..,L }\nwhere xk is the context-independent token representations, − →hk,j and ← −hk,j are the for-\nward and backward context-dependent representations, and j = 1,...,L is the index\nof the language model’s layers. The token representation xk is computed by a CNN\nover character embeddings [11, 12] and then passed to highway layers [28] and a linear\nprojection. The ﬁnal ELMo representations for the downstream model are derived by\ncollapsing all layers inRk into a single vector. In this experiment, we select the topmost\nlayer in Rk.\n2.2 Transfer Learning\nAs mentioned brieﬂy in Section 1, there are two variants of transfer learning used in\nthis paper. Details of both approaches are shown in Figure 1.\nSupervised ﬁne-tuning With ELMo representations, this approach ﬁrst trains a model\nusing labeled data from source domain S; next, it initializes the target model with the\nlearned parameters; ﬁnally, it ﬁne-tunes the target model using labeled data from target\ndomain T. All weights of the model except the pretrained LM are updated.\nUnsupervised ﬁne-tuning Unsupervised ﬁne-tuning is inspired by ULMFiT [26]. Rather\nthan training on the source domain and ﬁne-tuning on the target domain, this approach\nﬁne-tunes the pretrained LM using unlabeled data from source domain S; then, it trains\nthe target model using labeled data from target domain T. All weights of the model\nexcept the pretrained LM are updated.\n2.3 Dataset\nIn this paper, we use Indonesian language. There are three datasets: unlabeledU, source\nS, and target T. We evaluate our approach in the settings that U is of generic domain\n(newswire or formal) and the S and T are of speciﬁc domain (conversational).\nFig. 1: Double dash line represents weight transfer.Top: Supervised ﬁne-tuning by ﬁne-\ntuning the neural sequence labeler on the target domain. The CRF layer is replaced.\nBottom: Unsupervised ﬁne-tuning by ﬁne-tuning the pretrained LM on the source do-\nmain.\nFor the unlabeled data, we use Kompas-Tempo (newswire) [30] and Indonesian\nWikipedia. Unless stated otherwise, we refer to the former and the latter as KT and\nIdwiki, respectively.\nFor source and target domain data, we use our own manually labeled dataset, the\nsame dataset introduced by [14] , namely SMALL-TALK as the target domain and\nTASK-ORIENTED as the source domain. The former is a 16K conversational mes-\nsages from users having small talk with a chatbot, whereas the latter contains 12K\ntask-oriented messages such as movie tickets booking, and food delivery. For the rest\nof the paper, unless stated otherwise, we refer to the former as ST and the latter as TO.\nFor the unsupervised ﬁne-tuning approach, since we do not need labeled data for\nthe source domain S, we can easily add more unlabeled dataset to our source domainS.\nConveniently, we have a large unlabeled Indonesian conversational texts at our disposal,\nwhich is a superset of the TO labeled dataset. We refer to this bigger unlabeled conver-\nsational dataset as TOL. Using this dataset, we can perform unsupervised ﬁne-tuning\nwith a larger data size compared to that of supervised ﬁne-tuning.\nDue to proprietary and privacy reasons, unfortunately, we cannot publish our In-\ndonesian conversational texts. We present our dataset statistics in Table 1 and the label\ndetails in Table 2 and 3.\nTable 1: Number of sentences ( N),\nnumber of distinct words ( DW), num-\nber of tokens (T), and average sentence\nlength ( AVG) in TO , ST, and TOL\ndataset.\nN DW T AVG\nTO train 10142 16930 129841 12.7\nTO dev 1250 4291 16021 12.8\nTO test 1205 3868 14610 12.12\nST train 11577 9034 434408 3.74\nST dev 3289 3749 12583 3.82\nST test 1641 2265 6300 3.83\nTOL 916838 144028 6652122 7.25\nTable 2: Number of entities contained\nin the ST dataset. There are 6 labels in\ntotal.\nEntity Train Test Dev\nDATETIME 49 20 21\nEMAIL 20 7 8\nGENDER 241 64 85\nLOCATION 2672 813 867\nPERSON 2455 749 754\nPHONE 44 18 21\nTable 3: Number of entities contained in the TO dataset. There are 13 labels in total.\nEntityAREA CURRENCY DATETIME DURATION EMAIL LENGTH LOCATION NUMBER PERSON PHONE TEMPERATURE VOLUME WEIGHT\nTrain78 1213 2982 339 226 169 6322 3966 4031 466 70 58 134Dev 6 139 350 38 23 20 791 471 480 53 8 8 11Test 6 140 347 26 21 14 709 404 477 49 4 4 11\n3 Experiments and results\n3.1 Experiment setup\nWe use Rei et al.’s [25] implementation1 for the multitask model and AllenNLP2 for the\nrest of our models. For every combination of training dataset and model, we tune the\ndropout rate [27] by grid search on [0.25, 0.35, 0.5, 0.65, 0.75] for both the ELMo and\nthe neural sequence labeler using the same random seed for all conﬁgurations. We do\nnot tune other hyperparameters due to computational resource constraints.\nFor all models, we use identical data pre-processing: words are lowercased, BIO\nscheme is used, and start end tokens are excluded from the vocabulary. For the multi-\ntask model, we use identical settings to that of [25]. For the rest of the models, we use\ndifferent settings: word and character embedding sizes are set to 50 and 16, respectively.\nCharacter embeddings are formed by a CNN with 128 ﬁlters followed by highway lay-\ners and a ReLU activation layer. Both word and character embeddings are initialized\nrandomly. ELMo embedding size is set to 1024. The LSTMs are set to have 200 hidden\nunits and 2 layers. We apply L2 regularizer of 0.1 to all layers and early stopping is used\nwith a patience of 25. We use Adam [13] optimization with learning rate of 0.001 and\nwe clip the gradient at 5.0. We run our experiments with batch size of 32 and epochs\nof 150. We evaluated all our experiments with CoNLL evaluation: micro-averaged F1\nscore based on exact span matching.\n3.2 Impact of pretrained LM embeddings\nTo assess the impact of the pretrained LM embeddings, ﬁrst we compare the perfor-\nmance of the baseline model to the baseline models with pretrained LM embeddings on\nNER task evaluated on TO dataset. We use CNN-BiLSTM-CRF by [20] as our baseline\nmodel and ELMo as our pretrained LM embeddings. In addition to ELMo, we also use\nFlair [2] as it reaches state-of-the-art performance on English NER task3. Flair also dif-\nfers from ELMo because it is trained with character-level softmax. We use the default\nhyperparameters provided by [2] to train Flair LM4.\nFrom Table 4 we can see that adding LM embeddings improves the overall per-\nformance. ELMo yields small absolute improvement of dev F1 score when trained on\nboth Idwiki and KT than the baseline. Flair, although obtains roughly the same dev\nF1 score when trained on Idwiki, does not perform well when trained on KT. This\nresult might be attributed to the fact that KT contains far fewer sentences compared to\nIdWiki (around 9 times fewer). We link this result to an observation made by [34]\nwhich stated that character language models are unstable when the training data is not\nbig enough. Therefore, we do not proceed with Flair for the next experiment with our\ntwo approaches.\n1 https://github.com/marekrei/sequence-labeler\n2 https://github.com/allenai/allennlp\n3 as of 03/01/2019\n4 https://github.com/zalandoresearch/flair\nTable 4: Accuracy of the baseline model and the baselines with the pretrained embed-\ndings (ELMo and Flair trained on Idwiki and KT data) trained and evaluated on TO\ndata.\nModels F1 Dev F1 Test\nCNN-BiLSTM-CRF 85.94 85.85\n+ELMo Idwiki 87.67 87.72\n+ELMo KT 86.88 87.11\n+Flair Idwiki 87.12 88.41\n+Flair KT 71.16 71.73\n3.3 Main experiment\nWe experiment with three model groups. The ﬁrst group is the baseline models, con-\nsisting of a single-task learning model using CNN-BiLSTM-CRF [20] and a multitask\napproach [25], which uses an LSTM-BiLSTM-CRF with an additional LM loss. The\nsecond and third groups involve our ﬁrst and second approaches, which are the unsu-\npervised language model ﬁne-tuning and the supervised neural sequence labeler ﬁne-\ntuning, respectively.\nWe train the models in each group on different percentages of the target domain\ntraining data T. We do this to assess the impact of our supervised and unsupervised ﬁne-\ntuning approach when presented with varying amount of training data. Table 5 shows\nthe number of entities of the ST data with different percentages of training sentences\nused. Table 6 shows the result of our experiments. All numbers shown in the table come\nfrom models with the best hyperparameter on the target domain validation set.\nTable 5: Number of entities contained in the ST training dataset. The training data\npercentage is followed by the total number of training sentences.\nEntity 1%-100 5%-502 10%-1004 25%-2511 50%-5022 75%-7533\nDATETIME 2 4 8 18 27 39\nEMAIL 2 5 3 7 11 17\nGENDER 4 18 26 65 123 174\nLOCATION 30 137 265 640 1345 2019\nPERSON 20 122 251 606 1216 1829\nPHONE 1 2 5 8 22 31\nModel name conventionsHere we explain the patterns for naming our models in Table\n6 and the rest of our paper: LM indicates an unsupervised step, whereas Sup is for the\nsupervised step. A dash (-) shows a ﬁne-tuning step (supervised or unsupervised), an\nunderscore ( ) represents the move from LM training to BiLSTM training, and a square\nTable 6: Experiment results on the ST test data. Models are trained on ST training data\nwith varying number of training instances. Bold and underline indicate the highest and\nthe second highest test F1 score, respectively.\nGroup Model name Unlabeled data for Source domain Target domain test F1 score\npretrained LM data 1% 5% 10% 25% 50% 75% 100%\nBaselines CNN-BiLSTM-CRF - - 0.59 43.84 50.86 62.19 72.64 72.79 75.81\nRei (2017) - - 41.44 59.43 68.03 74.30 80.47 83.23 85.12\nSupervised ﬁne-tuningLM[Idwiki]Sup[TO-ST] Idwiki TO 73.17 77.2577.21 80.3182.2783.5484.62\nLM[KT]Sup[TO-ST] KT TO 71.8475.16 76.42 79.11 82.8883.18 84.49\nUnsupervised ﬁne-tuningLM[Idwiki-TOL]Sup[ST] Idwiki TOL 67.10 74.05 77.4979.7683.8283.1785.78\nLM[KT-TOL]Sup[ST] KT TOL 64.48 75.5577.77 80.9983.09 82.74 85.76\nbracket ([]) represents which dataset used in the unsupervised (LM) or the supervised\n(Sup) step. For example, one can interpret a model named LM[Idwiki] Sup[TO-ST]\nas a model which: (1) uses pretrained LM trained on Idwiki dataset, (2) trains the\nBiLSTM on the source domain dataset ( TO), and (3) ﬁne-tunes the BiLSTM on the\ntarget domain dataset (ST).\nResults discussion\nBaseline and multitask Unsurprisingly, CNN-BiLSTM-CRF fails when the training\ndata is as small as 100 sentences, reaching only 0.59% test F1 score. As the training\ndata gradually increases, CNN-BiLSTM-CRF performs fairly well, reaching 75.81%\ntest F1 score on the whole data. The multi-task model by [25] reaches 45.10% test F1\nscore when trained on 100 sentences training data. The absolute improvement from the\nbaseline is huge considering that it only beneﬁts from the LM loss and no additional\nsignals from external labeled/unlabeled data are incorporated. It is also competitive\ncompared to other models when trained on the whole data.\nSupervised ﬁne-tuning There is a signiﬁcant increase of test F1 score using supervised\nﬁne-tuning compared to the baseline on every level of training data size. The gain from\nsupervised ﬁne-tuning is largest when the data is small. Both LM[Idwiki] Sup[TO-ST]\nand LM[KT] Sup[TO-ST] achieve the highest and second highest test F1 score when\ntrained on 1% training data. However, when trained on 100% training data, supervised\nﬁne-tuning does not perform better than the multitask model. The gain of supervised\nﬁne-tuning seems to be diminishing as the training data grows larger. This hints that\nusing supervised ﬁne-tuning might not be necessary if labeled data is already present in\nadequate amount thus one can opt for simpler models such as the multitask model.\nUnsupervised ﬁne-tuning Overall, using unsupervised ﬁne-tuning yields competitive\nresult with the supervised ﬁne-tuning. A notable difference is when the model trained\nwith 1% and 100% training data. LM[Idwiki-TOL]Sup[ST] and LM[KT-TOL] Sup[ST]\nobtains the highest and the second highest F1 test score on 100% training data, respec-\ntively. On 1% training data, they fall behind the supervised ﬁne-tuning by about 5 points\nof test F1 score. The huge unlabeled source domain data for the LM ﬁne-tuning does\nnot seem to be helping much on small training data. This hints that labeled data in\nsmall-moderate size is still more effective compared to unlabeled data which comes\nin massive size. Also, there does not seem to be a signiﬁcant difference between the\nunsupervised ﬁne-tuning and the multitask approach on 75% and 100% training data.\nMultitask approach seems to perform competitively when the training data size is vast\nenough. Again, this tells us that we might not need to perform unsupervised ﬁne-tuning\nif labeled data is already present in adequate amount.\nAblation An ablation study is conducted for both the supervised and unsupervised\nﬁne-tuning. We perform three ablations resulting in four models: two for the supervised\nﬁne-tuning group and one for the unsupervised ﬁne-tuning. We carry out the ablations\non the development set. Table 7 shows the result of the ablated models compared with\nthe models of our two approaches.\nTable 7: Experiment results on the ST dev data. The original models and the ablation\nmodels are trained on ST training data with varying number of training instances. Bold\nand underline indicates the highest and the second highest test F1 score, respectively.\nGroup Model name Unlabeled data for Source domain Target domain dev F1 score\npretrained LM data 1% 5% 10% 25% 50% 75% 100%\nSupervised ﬁne-tuningLM[Idwiki]Sup[TO-ST] Idwiki TO 71.1275.02 77.28 80.36 81.85 81.82 82.96\nLM[KT]Sup[TO-ST] KT TO 70.4273.79 75.12 79.95 81.39 82.04 82.33\nUnsupervised ﬁne-tuningLM[Idwiki-TOL]Sup[ST] Idwiki TOL 65.43 74.64 77.71 81.14 82.68 83.7184.37\nLM[KT-TOL]Sup[ST] KT TOL 63.9075.7577.8482.3883.7583.2685.11\nAblation\nSup[TO-ST] - TO 66.08 69.94 71.91 76.68 81.00 80.78 82.04\nLM[Idwiki]Sup[ST] Idwiki - 55.99 69.15 71.94 74.92 79.60 80.31 80.71\nLM[KT]Sup[ST] KT - 53.79 66.70 71.39 74.61 78.68 78.91 81.02\nLM[TOL]Sup[ST] TOL - 61.59 75.05 78.6082.0083.91 85.0484.18\nThe ﬁrst ablation is removing the LM pretraining step but keeping the supervised\nﬁne-tuning using the source domain (Sup[TO-ST]). Notice that there is a sizable drop of\ntest F1 score on every training data size compared to the supervised ﬁne-tuning models,\nespecially on 1% training data. The second ablation is omitting the intermediate su-\npervised ﬁne-tuning step while keeping the pretrained LM intact (LM[Idwiki] Sup[ST]\nand LM[KT] Sup[ST]). Without the supervised ﬁne-tuning, the models’ performance\nis markedly curtailed. Another interesting pattern is that removing the LM pretraining\nstep does less harm than removing the supervised ﬁne-tuning step, especially on small\ntraining data. On 1% training data, Sup[TO-ST] outperforms LM[Idwiki] Sup[ST] and\nLM[KT] Sup[ST] by signiﬁcant margins. It seems that the supervised ﬁne-tuning part\nis where the model beneﬁts the most. Nonetheless, even without the supervised ﬁne-\ntuning, the pretrained LM alone is already a huge reinforcement for the models.\nThe last ablation is conducted by excluding the unsupervised ﬁne-tuning step, which\nis the model named LM[TOL] Sup[ST]. We do the LM pretraining directly with the\nsource domain data (TOL) without using the generic domain data. LM[TOL] Sup[ST]\nobtains highest F1 test score on 10%, 50%, and 75% training data and second high-\nest F1 test score on 5% and 25% training data. This is quite surprising considering\nthat LM[TOL] Sup[ST] is trained with the same fashion as LM[Idwiki] Sup[ST] and\nLM[KT] Sup[ST]: no ﬁne-tuning and only BiLSTM with LM pretraining. The only\ndifference is in the dataset used for the LM pretraining. We hypothesize that the gain\nstems from the fact that TOL is of conversational domain, whereas Idwiki and KT is\nof generic domain. Overall, LM[TOL] Sup[ST] also performs competitively compared\nto both the supervised and unsupervised ﬁne-tuning models on every level of training\ndata size. This highlights that one might not need to perform ﬁne-tuning from a generic\ndomain if a large in-domain unlabeled data is already at hand.\n4 Analysis\nFrom the observations of the previous section, we conclude that there are three main\nhighlights regarding our ﬁne-tuning approaches: (1) supervised transfer performs best\non 1% training data, (2) unsupervised transfer obtains the best F1 score on 100% train-\ning data, and (3) pretraining the LM directly on the source domain (TOL) without ﬁne-\ntuning works really well, attaining comparable result with both ﬁne-tuning approaches.\nHere, we seek to establish more understanding of why (1), (2), and (3) happen.\nRecent research have discovered that pretrained LM induce useful knowledge such\nas syntactic information for downstream tasks [4, 9, 18]. Motivated by these ﬁndings,\nwe formulate a hypothesis that the supervised and unsupervised ﬁne-tuning models also\nlearn advantageous knowledge for predicting named entities, namely part-of-speech\n(POS) information. It is also known that POS has been used as features for NER and\nthey help improve the overall performance [7]. To test this hypothesis, we train a diag-\nnostic classiﬁer [10, 32] using the models’ hidden state representations as features on\nan Indonesian part-of-speech conversational dataset. In this experiment, the diagnos-\ntic classiﬁer is a simple classiﬁer (single layer neural network with a softmax output)\nto predict the POS tags. During training, the weights of the models are all frozen ex-\ncept the diagnostic classiﬁer’s weights. To train the diagnostic classiﬁer, we use our\nown manually labeled POS dataset because to the best of our knowledge, there is no\nlabeled Indonesian part-of-speech conversational dataset that is publicly available. We\nalso cannot publish our POS dataset due to proprietary and privacy reasons. We provide\nthe dataset statistics in Table 8 and the label details in Table 9.\nPOS tag schema Here we explain the schema used in our POS tag dataset. The dataset\nis annotated by an Indonesian linguist. All 21 tags shown in Table 9 are the same tags\nas the English Penn Treebank POS tags [21] except for: (1) CDI, NEG, PRL, SC, VBI,\nand VBT, which were taken from the Indonesian POS Tagset 1 [23] and (2) NUM,\nPNP, and X, which were created based on the Indonesian grammar references. NUM\nstands for numbers (e.g., tujuh-seven). PNP is number pronouns, which is used to refer\nto person or object identiﬁed with numbers (e.g., keduanya-the two of them). X is for\nunrecognized words (e.g., wkwk-wkwk).\nWe train the diagnostic classiﬁer using the hidden representations of: (1) the pre-\ntrained LM (for both the supervised and unsupervised ﬁne-tuning approaches), (2) the\nneural sequence labeler (BiLSTM) trained on the source domain (for supervised ﬁne-\ntuning), and (3) the ﬁne-tuned language model on the source domain (for unsupervised\nTable 8: Number of sentences ( N), number of distinct words ( DW), number of tokens\n(T), and average sentence length (AVG) in POS dataset.\nN DW T AVG\nPOS train 4108 7077 34843 8.48\nPOS dev 1008 2109 9115 9.04\nPOS test 1283 3209 11111 8.66\nTable 9: Number of labels contained in our POS dataset. There are 23 labels in total.\nEntityCC CDI DT FW IN JJ MD NEG NN NNP NUM PNP PRL PRP RB RP SC SYM UH VBI VBT WP X\nTrain412 73 1120 1679 1493 750 1126 447 7236 5213 1178 2 27 1018 883 193 1126 4787 1589 892 2817 722 60\nDev 110 17 295 561 439 182 271 110 1857 1341 280 1 7 247 241 42 306 1207 382 235 807 169 8\nTest166 25 350 505 502 238 365 142 2306 1652 370 2 12 337 307 61 339 1481 511 286 924 216 14\nﬁne-tuning). We report the accuracy on the development set. The test is carried out to\ncheck which step encodes the part-of-speech information better for both the supervised\nand unsupervised ﬁne-tuning models. Note that here there is no target domain involved\nsince we only want to know the quality of the POS information learned from the ﬁne-\ntuning step.\nTable 10: Diagnostic classiﬁers trained on part-of-speech (POS) task using the different\nmodels’ hidden representations as input. We present the accuracy on the development\nset.\nHidden representations input Model name Pretrained LM data Source domain Dev acc\n- Most frequent tag - - 84.51\nPretrained LM\nLM[Idwiki] Idwiki - 86.14\nLM[KT] KT - 86.68\nLM[TOL] TOL - 92.05\nFine-tuned LM LM[Idwiki-TOL] Idwiki TOL 92.52\nLM[KT-TOL] KT TOL 93.08\nNeural sequence labeler (BiLSTM)\nLM[Idwiki]Sup[TO] Idwiki TO 63.64\nLM[KT]Sup[TO] KT TO 63.83\nSup[TO] - TO 58.62\nTable 10 shows the result of the diagnostic classiﬁer trained on our POS dataset. In\naddition to the diagnostic classiﬁer, we provide a simple baseline model that outputs\nthe most frequent tag in the training set for a given word. For OOV word, the simple\nbaseline model outputs the most frequent label in the training data. Given inputs from\nthe pretrained LM, the diagnostic classiﬁer performs considerably well on the develop-\nment set. LM[TOL], LM[Idwiki], and LM[KT] obtains higher accuracies than the sim-\nple baseline model. LM[TOL] reaches higher accuracy than LM[Idwiki] and LM[KT].\nThis result aligns with our previous result that TOL is of conversational domain hence\nit is more useful for downstream tasks in conversational domain. The diagnostic clas-\nsiﬁer reaches slightly better accuracy when trained using inputs from the ﬁne-tuned\nLM, yielding an absolute improvement of 1 point of dev accuracy. Since the differ-\nences between the accuracies of the diagnostic classiﬁer are minuscule, this may ex-\nplain why LM pretraining without ﬁne-tuning (LM[TOL] Sup[ST]) is competitive with\nits ﬁne-tuning counterpart (LM[Idwiki-TOL] Sup[ST] and LM[Idwiki-KT] Sup[ST]).\nUnsupervised ﬁne-tuning might not be necessary if one already has access to a huge\nunlabeled in-domain data. With this result, we can also conclude that the LM pretrain-\ning (ELMo) induces useful syntactic knowledge, which in this case is part-of-speech\ninformation.\nSurprisingly, the diagnostic classiﬁer performances badly deteriorate on the devel-\nopment set given inputs from the BiLSTM. All models from this group obtain accura-\ncies below the simple baseline model. This contradicts our previous result where su-\npervised ﬁne-tuning obtains adequate result on small training data. It seems that the\nBiLSTM does not encode part-of-speech information as good as the pretrained LM,\neven though it receives additional input from the pretrained LM (LM[Idwiki] Sup[TO]\nand LM[KT] Sup[TO]). We think that the supervised ﬁne-tuning models may learn\nsomething other than the part-of-speech information which helps them perform well\non the small training data. A plausible explanation would be that BiLSTM is learning\nNER-speciﬁc information during the supervised training on source domain, replacing\nthe part-of-speech information from the pretrained LM.\n5 Conclusion\nIn this paper, we investigate the impact of language model pretraining on named-entity\nrecognition task in Indonesian conversational texts. We use two variants of three step\ntraining procedure: supervised ﬁne-tuning (ﬁne-tuning the BiLSTM) and unsupervised\nﬁne-tuning (ﬁne-tuning the pretrained LM). Using both approaches, the neural models\nobtain signiﬁcant increase from the CNN-BiLSTM-CRF and the multitask baseline on\nsmall training data, yielding an absolute improvement of 32 points of test F1 score.\nHowever, one might not need to ﬁne-tune if: (1) a large unlabeled in-domain data is\nalready at hand then one can train language model directly without any ﬁne-tuning, and\n(2) an adequate amount of labeled in-domain data (in our case it’s>5000 sentences) is\npresent then one can opt for simpler models such as the multitask model. Furthermore,\nwe also ﬁnd that the pretrained LM encodes part-of-speech information, which is a\nstrong predictor for named entity recognition. The neural sequence labeler, on the other\nhand, seems to encode another information other than part-of-speech to help it perform\nwell on NER task on small training data.\nReferences\n1. Aguilar, G., Maharjan, S., L ´opez-Monroy, A.P., Solorio, T.: A multitask approach for named\nentity recognition in social media data (2017)\n2. Akbik, A., Blythe, D., V ollgraf, R.: Contextual string embeddings for sequence labeling. In:\nCOLING 2018, 27th International Conference on Computational Linguistics. pp. 1638–1649\n(2018)\n3. Aryoyudanta, B., Adji, T.B., Hidayah, I.: Semi-supervised learning approach for indonesian\nnamed entity recognition (ner) using co-training algorithm. 2016 International Seminar on\nIntelligent Technology and Its Applications (ISITIA) pp. 7–12 (2016)\n4. Blevins, T., Levy, O., Zettlemoyer, L.S.: Deep rnns encode soft hierarchical syntax. In: ACL\n(2018)\n5. Budi, I., Bressan, S., Wahyudi, G., Hasibuan, Z.A., Nazief, B.A.A.: Named entity recogni-\ntion for the indonesian language: Combining contextual, morphological and part-of-speech\nfeatures into a knowledge engineering approach. In: Hoffmann, A., Motoda, H., Scheffer, T.\n(eds.) Discovery Science. pp. 57–69. Springer Berlin Heidelberg, Berlin, Heidelberg (2005)\n6. Chiticariu, L., Krishnamurthy, R., Li, Y ., Reiss, F., Vaithyanathan, S.: Domain adaptation of\nrule-based annotators for named-entity recognition tasks. In: EMNLP (2010)\n7. Curran, J.R., Clark, S.: Language independent ner using a maximum entropy tagger. In:\nCoNLL (2003)\n8. Daum ´e, H.: Frustratingly easy domain adaptation. CoRR abs/0907.1815 (2007)\n9. Gulordava, K., Bojanowski, P., Grave, E., Linzen, T., Baroni, M.: Colorless green recurrent\nnetworks dream hierarchically. In: NAACL-HLT (2018)\n10. Hupkes, D., Veldhoen, S., Zuidema, W.H.: Visualisation and ’diagnostic classiﬁers’ reveal\nhow recurrent and recursive neural networks process hierarchical structure. In: IJCAI (2018)\n11. J ´ozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., Wu, Y .: Exploring the limits of lan-\nguage modeling. CoRR abs/1602.02410 (2016)\n12. Kim, Y ., Jernite, Y ., Sontag, D.A., Rush, A.M.: Character-aware neural language models. In:\nAAAI (2016)\n13. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. CoRR abs/1412.6980\n(2014)\n14. Kurniawan, K., Louvan, S.: Empirical evaluation of character-based model on neural named-\nentity recognition in indonesian conversational texts. CoRR abs/1805.12291 (2018)\n15. Lake, B.M., Baroni, M.: Still not systematic after all these years: On the compositional skills\nof sequence-to-sequence recurrent networks. CoRR abs/1711.00350 (2017)\n16. Lee, J.Y ., Dernoncourt, F., Szolovits, P.: Transfer learning for named-entity recognition with\nneural networks. CoRR abs/1705.06273 (2018)\n17. Leonandya, R., Distiawan, B., Praptono, N.H.: A semi-supervised algorithm for indonesian\nnamed entity recognition. 2015 3rd International Symposium on Computational and Busi-\nness Intelligence (ISCBI) pp. 45–50 (2015)\n18. Linzen, T., Dupoux, E., Goldberg, Y .: Assessing the ability of lstms to learn syntax-sensitive\ndependencies. TACL4, 521–535 (2016)\n19. Luthﬁ, A., Trisedya, B.D., Manurung, R.: Building an indonesian named entity recognizer\nusing wikipedia and dbpedia. 2014 International Conference on Asian Language Processing\n(IALP) pp. 19–22 (2014)\n20. Ma, X., Hovy, E.H.: End-to-end sequence labeling via bi-directional lstm-cnns-crf. CoRR\nabs/1603.01354 (2016)\n21. Marcus, M., Kim, G., Marcinkiewicz, M.A., MacIntyre, R., Bies, A., Ferguson, M.,\nKatz, K., Schasberger, B.: The penn treebank: Annotating predicate argument struc-\nture. In: Proceedings of the Workshop on Human Language Technology. pp. 114–119.\nHLT ’94, Association for Computational Linguistics, Stroudsburg, PA, USA (1994).\nhttps://doi.org/10.3115/1075812.1075835, https://doi.org/10.3115/1075812.\n1075835\n22. Peters, M.E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K., Zettlemoyer, L.S.:\nDeep contextualized word representations. In: NAACL-HLT (2018)\n23. Pisceldo, F.: Probabilistic part of speech tagging for bahasa indonesia (2009)\n24. Radford, A.: Improving language understanding by generative pre-training (2018)\n25. Rei, M.: Semi-supervised multitask learning for sequence labeling. In: ACL (2017)\n26. Ruder, S., Howard, J.: Universal language model ﬁne-tuning for text classiﬁcation. In: ACL\n(2018)\n27. Srivastava, N., Hinton, G.E., Krizhevsky, A., Sutskever, I., Salakhutdinov, R.: Dropout: a\nsimple way to prevent neural networks from overﬁtting. Journal of Machine Learning Re-\nsearch 15, 1929–1958 (2014)\n28. Srivastava, R.K., Greff, K., Schmidhuber, J.: Highway networks. CoRR abs/1505.00387\n(2015)\n29. Suwarningsih, W., Supriana, I., Purwarianti, A.: Imner indonesian medical named entity\nrecognition. 2014 2nd International Conference on Technology, Informatics, Management,\nEngineering and Environment pp. 184–188 (2014)\n30. Tala, F.Z.: A study of stemming effects on information retrieval in bahasa indonesia (2003)\n31. Tauﬁk, N., Wicaksono, A.F., Adriani, M.: Named entity recognition on indonesian microblog\nmessages. 2016 International Conference on Asian Language Processing (IALP) pp. 358–\n361 (2016)\n32. Veldhoen, S., Hupkes, D., Zuidema, W.H.: Diagnostic classiﬁers revealing how neural net-\nworks process hierarchical structure. In: CoCo@NIPS (2016)\n33. Wu, D., Lee, W.S., Ye, N., Chieu, H.L.: Domain adaptive bootstrapping for named entity\nrecognition. In: EMNLP (2009)\n34. Yu, X., Mayhew, S.D., Sammons, M., Roth, D.: On the strength of character language models\nfor multilingual named entity recognition. In: EMNLP (2018)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8073315024375916
    },
    {
      "name": "Artificial intelligence",
      "score": 0.667847216129303
    },
    {
      "name": "Task (project management)",
      "score": 0.6622629761695862
    },
    {
      "name": "Transfer of learning",
      "score": 0.6563416719436646
    },
    {
      "name": "Named-entity recognition",
      "score": 0.645301103591919
    },
    {
      "name": "Natural language processing",
      "score": 0.6281403303146362
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5257419347763062
    },
    {
      "name": "Language model",
      "score": 0.5219097137451172
    },
    {
      "name": "ENCODE",
      "score": 0.5196974873542786
    },
    {
      "name": "F1 score",
      "score": 0.4861619174480438
    },
    {
      "name": "Labeled data",
      "score": 0.43160852789878845
    },
    {
      "name": "Baseline (sea)",
      "score": 0.4196837246417999
    },
    {
      "name": "Speech recognition",
      "score": 0.3974452018737793
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 1
}