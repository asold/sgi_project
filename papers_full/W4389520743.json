{
  "title": "DSI++: Updating Transformer Memory with New Documents",
  "url": "https://openalex.org/W4389520743",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2136642368",
      "name": "Sanket Mehta",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2505043072",
      "name": "Jai Gupta",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2738935859",
      "name": "Yi Tay",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2112331270",
      "name": "Mostafa Dehghani",
      "affiliations": [
        "DeepMind (United Kingdom)",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2233625656",
      "name": "Vinh Tran",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2159906317",
      "name": "Jinfeng Rao",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2027155665",
      "name": "Marc Najork",
      "affiliations": [
        "Google (United States)",
        "DeepMind (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A80667657",
      "name": "Emma Strubell",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2151486164",
      "name": "Donald Metzler",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2554863749",
    "https://openalex.org/W3107969673",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W2560647685",
    "https://openalex.org/W2971970954",
    "https://openalex.org/W4281657280",
    "https://openalex.org/W2951013084",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W2047057213",
    "https://openalex.org/W4225620948",
    "https://openalex.org/W4285397142",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4377372274",
    "https://openalex.org/W4221166196",
    "https://openalex.org/W3098170909",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3206998803",
    "https://openalex.org/W4223974161",
    "https://openalex.org/W4297663362",
    "https://openalex.org/W3207523779",
    "https://openalex.org/W2133013156",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W4286897388",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W3194782062",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W2964189064",
    "https://openalex.org/W4288336773",
    "https://openalex.org/W3030364939",
    "https://openalex.org/W3035525746",
    "https://openalex.org/W4221151914",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4287646898",
    "https://openalex.org/W4224442590",
    "https://openalex.org/W2788388592",
    "https://openalex.org/W2963559848",
    "https://openalex.org/W2947461406",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W4283315779",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W2920252606",
    "https://openalex.org/W3044438666"
  ],
  "abstract": "Sanket Mehta, Jai Gupta, Yi Tay, Mostafa Dehghani, Vinh Tran, Jinfeng Rao, Marc Najork, Emma Strubell, Donald Metzler. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 8198–8213\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDSI++: Updating Transformer Memory with New Documents\nSanket Vaibhav Mehta1∗ Jai Gupta2 Yi Tay3 Mostafa Dehghani4 Vinh Q. Tran2\nJinfeng Rao3 Marc Najork4 Emma Strubell1 Donald Metzler2\n1Carnegie Mellon University 2Google Research 3Google 4Google DeepMind\nAbstract\nDifferentiable Search Indices (DSIs) encode a\ncorpus of documents in model parameters and\nuse the same model to answer user queries di-\nrectly. Despite the strong performance of DSI\nmodels, deploying them in situations where the\ncorpus changes over time is computationally ex-\npensive because reindexing the corpus requires\nre-training the model. In this work, we intro-\nduce DSI++, a continual learning challenge\nfor DSI to incrementally index new documents\nwhile being able to answer queries related to\nboth previously and newly indexed documents.\nAcross different model scales and document\nidentifier representations, we show that contin-\nual indexing of new documents leads to con-\nsiderable forgetting of previously indexed doc-\numents. We also hypothesize and verify that\nthe model experiences forgetting events dur-\ning training, leading to unstable learning. To\nmitigate these issues, we investigate two ap-\nproaches. The first focuses on modifying the\ntraining dynamics. Flatter minima implicitly al-\nleviate forgetting, so we optimize for flatter loss\nbasins and show that the model stably memo-\nrizes more documents (+12%). Next, we intro-\nduce a generative memory to sample pseudo-\nqueries for documents and supplement them\nduring continual indexing to prevent forgetting\nfor the retrieval task. Extensive experiments\non novel continual indexing benchmarks based\non Natural Questions (NQ) and MS MARCO\ndemonstrate that our proposed solution miti-\ngates forgetting significantly. Concretely, it im-\nproves the average Hits@10 by +21.1% over\ncompetitive baselines for NQ and requires 6\ntimes fewer model updates compared to re-\ntraining the DSI model for incrementally in-\ndexing five corpora in a sequence.\n1 Introduction\nDifferentiable Search Indices (DSIs; Tay et al.\n(2022)) represent a new modeling paradigm for\n∗Work performed during an internship at Google Re-\nsearch. Correspondence: sanketvmehta@google.com\ninformation retrieval tasks using sequence-to-\nsequence learning. Specifically, DSIs leverage\nTransformer memory (Vaswani et al., 2017) to en-\ncode all of the information in a corpus of docu-\nments and then use that memory to answer user\nqueries directly, thereby simplifying the retrieval\nprocess. DSIs achieve this functionality by jointly\noptimizing for indexing (or memorization) and re-\ntrieval tasks. The indexing task requires learning\na mapping from document content to its identifier,\ntypically represented by integers or short strings\n(document identifiers, abbreviated docids). Then,\nthe retrieval task necessitates mapping user queries\nto relevant docids. Besides its simplicity and end-\nto-end differentiable nature, DSI significantly out-\nperforms state-of-the-art “retrieve-and-rank\" meth-\nods based on dual-encoders (Ni et al., 2022).\nDespite the remarkable performance of DSI mod-\nels, there remain open questions about their appli-\ncability in the practical setting of dynamic corpora.\nConsider the realistic scenario wherein new doc-\numents are continually added to the indexed cor-\npus. Updating the index in dual-encoder-based\nmethods requires computing embeddings for new\ndocuments, followed by re-indexing all document\nembeddings (Karpukhin et al., 2020). In contrast,\nindex construction using a DSI involves training a\nTransformer model. Therefore, the model must be\nre-trained from scratch every time the underlying\ncorpus is updated, thus incurring prohibitively high\ncomputational costs compared to dual-encoders. In\nthis work, we aim to address this issue by devising\nmethods for effective incremental indexing using\nTransformer memory without re-training the DSI\nmodel from scratch.\nLifelong (or continual) learning (Thrun, 1995;\nParisi et al., 2019) is a biologically-inspired ma-\nchine learning paradigm that deals with contin-\nuous learning of new tasks by preserving past\nknowledge and using it to learn new concepts effi-\nciently. Based on this paradigm, we proposeDSI++\n8198\nD0 D1 D2 D3 D4 D5\nBatch of documents for continual indexing\n0\n20\n40\n60\n80\n100Indexing accuracy\nEval batch\nD0\nD1\nD2\nFigure 1: Indexing accuracy of D0,D1,and D2 docu-\nment corpora visualized as we continuously index new\ndocuments (averaged over 3 runs). We observe that\ncontinual indexing of new documents leads to severe\nforgetting of the previously memorized documents.\n(DSI + new documents), a continual learning chal-\nlenge for DSI to incrementally index new docu-\nments while maintaining the ability to answer user\nqueries related to both previously and newly in-\ndexed documents. To enable DSI++, we introduce\nnovel benchmarks constructed from existing Natu-\nral Questions (Kwiatkowski et al., 2019) and MS\nMARCO (Nguyen et al., 2016) datasets, simulating\nthe continual addition of documents to the system.\nTo our knowledge, there is no prior work studying\nincremental learning for DSI.\nA naive solution for DSI++ is to continuously\nfine-tune the model with an indexing objective\nover new documents. However, Figure 1 shows\nthat continual indexing of new documents leads\nto catastrophic forgetting of the previously memo-\nrized documents (more details in §2.1), a common\nphenomenon in neural networks wherein learning\nof the new concepts interferes with the previously\nacquired knowledge (McCloskey and Cohen, 1989).\nFurthermore, when we investigate the learning dy-\nnamics of the DSI model during memorization (Fig-\nure 3, we observe a significant number of docu-\nments (approx. 88%) experience forgetting events\nafter they have been memorized. Concretely, a\nforgetting event (Toneva et al., 2019) is when a\nprediction for an individual document goes from\ncorrect docid to incorrect one throughout learning.\nTherefore, implicit forgetting during memorization\nand explicit forgetting from continual indexing of\nnew documents are two key challenges to overcome\nfor successfully implementing a DSI++ system.\nTo reduce forgetting during memorization, we\npropose explicitly optimizing for flatter loss basins\nusing Sharpness-Aware Minimization (SAM; Foret\net al. (2021)). Recent works have shown that ge-\nometrical properties of the minima play a vital\nrole in forgetting, especially models in flatter loss\nbasins tend to undergo less forgetting while life-\nlong learning from task sequences (Mehta et al.,\n2023). Next, we introduce a generative memory to\nsample pseudo-queries for already indexed docu-\nments and use them to alleviate forgetting of the re-\ntrieval task during incremental indexing of the new\ndocuments. Also, the generative memory enables\ncontinual semi-supervised learning of the retrieval\ntask by generating pseudo-queries for an incoming\nbatch of new documents. Our main contributions\ncan be summarized as follows:\n• We introduce DSI++, a continual learning\nchallenge for the recently proposed Differen-\ntiable Search Indices (DSI) paradigm. To en-\nable DSI++ evaluations, we create two bench-\nmarks based on existing Natural Questions\nand MS MARCO datasets. To understand the\nseverity of the forgetting phenomenon across\nmultiple scenarios, we analyze a suite of pre-\ntrained models (T5-Base, T5-Large, T5-XL)\nand different document identifier representa-\ntions (unstructured atomic, naively structured,\nand semantically structured).\n• We hypothesize and verify that the DSI\nmodel experiences forgetting events through-\nout memorization. To alleviate these, we pro-\npose modifying training dynamics to promote\nflatter minima using SAM and show that the\nmodel stably memorizes +12% documents.\n• We propose a generative memory-based expe-\nrience rehearsal approach to alleviate explicit\nforgetting during continual indexing and im-\nprove the average Hits@1 by +25.0% and\nHits@10 by +21.1% over competitive base-\nlines for MS MARCO and NQ, respectively.\n2 DSI++: Continual learning challenge\nfor DSI\n2.1 Problem setup\nWe focus on a setup where we receive an initial\ncorpus of documents, D0 = {d1,··· ,dn}, and\nuser queries corresponding to a subset of them,\nR0 = {< qj,j >,∀j ∈YD}, where D ⊂D0.\nDSI paradigm involves two tasks: (i) memorization\ntask where the goal is to learn an indexerfθ : X→\n8199\nY, a text-to-text model parameterized by θ∈RP,\nthat takes document tokens (x∈X) as input and\nmaps it to a document identifier (docid) j ∈Y ,\nand (ii) retrieval task where the goal is to use the\nsame indexer fθ to directly map a user query qto\na relevant docid j ∈Y. Two different prompts are\nused to differentiate between these tasks. Tay et al.\n(2022) discusses several variants for representing\ndocids – unstructured atomic and structured string\ndocids, where each document is assigned a unique\ntoken and tokenized string, respectively. Under\nthe unified text-to-text format, both of the above\ntasks are cast as generation tasks, i.e., decoding one\nunique token (unstructured atomic) or decoding a\ntokenized string sequentially, one token at a time\n(naively/ semantically structured).\nIn the dynamic corpus scenario, we simulate the\narrival of new documents by updating the initial\ncorpus D0 with a sequence of batchesD1 →···→\nDt. In DSI++, we have access to the new batch\nof documents Di, but we do not have any queries\nrelated to these documents.\nGoal: Learn a DSI++ system that incrementally\nindexes D1,D2,··· in fθ while being able to an-\nswer queries related to previously as well as addi-\ntionally indexed documents.\n2.2 Benchmarks for DSI++\nTo enable research on DSI++, we introduce two\nbenchmarks constructed from the Natural Ques-\ntions (NQ; Kwiatkowski et al. (2019)) and MS\nMARCO (Nguyen et al., 2016) datasets. The NQ\ndataset consists of Wikipedia articles and corre-\nsponding natural language questions. Similar to\n(Tay et al., 2022), we consider Wikipedia articles\nfor memorization and the retrieval task as identi-\nfying the Wikipedia article that answers the given\nquestion. We use the original NQ train split to con-\nstruct train(80%)/ validation(20%) splits and use\nNQ validation as a test split. We randomly sample\n50Kunique articles to constitute the initial D0 cor-\npus. Next, we construct five corpora (D1,··· ,D5),\neach containing 10Kunique articles, to add them\nto the DSI model sequentially. Corresponding to\narticles in each of these corpora, we filter queries\nfrom original NQ train/ validation splits to con-\nstruct Rtrain\ni ,Rval\ni ,Rtest\ni (∀i∈{0,··· ,5}) splits.\nWe use R0 to train the DSI model for the retrieval\ntask and use Rtest\ni to evaluate previously and newly\nindexed articles. The full MS MARCO dataset has\napprox. 500K passage-query training pairs and\n6,980 validation pairs. Like the benchmark cre-\nated from the MS MARCO dataset (Pradeep et al.,\n2023), we randomly sample 50Kunique passages\nto constitute the initial D0 corpus and five more\ncorpora, each with 10Kpassages. See Table 2 (in\nthe Appendix) for exact dataset statistics for NQ\nand MS MARCO.\n2.3 Evaluation Metrics\nFor DSI evaluation, we report indexing accuracy\nfor memorization task and Hits@k (k ∈{1,10})\nmetric for retrieval task. Indexing accuracy and\nHits@k are the proportion of correctly memorized\ndocuments and correct documents ranked in the\ntop k predictions, respectively. We formally define\nmetrics to summarize the model performance as\nwe incrementally index new documents. Let Pn,o\ndenote the performance (e.g., indexing accuracy)\non corpus Do after training on corpus Dn. Follow-\ning prior work (Mehta et al., 2023), we compute\nthe average performance(An), forgetting (Fn)\nand learning performance (LAn) metrics after\nindexing the corpus Dn.\nThe term Fn (aka backward transfer) refers to\nthe effect of indexing the corpus Dn on the per-\nformance of all previously indexed documents Do,\nwhere 0 ≤o<n . LAn (or forward transfer) mea-\nsures the model’s ability to learn when presented\nwith a new corpus Dn and is defined as the average\nperformance over the new corpora D1,··· ,Dn.\nWhen the Dth\nn corpus is incrementally indexed, An,\nFn, and LAn are defined as follows:\nAn = 1\nn+ 1\nn∑\no=0\nPn,o; LAn = 1\nn\nn∑\no=1\nPo,o;\nFn = 1\nn\nn−1∑\no=0\nmax\no′∈{0,···,n−1}\n(Po′,o −Pn,o); (1)\n2.4 Case study: Forgetting and Forward\nTransfer\nAfter introducing the DSI++ problem setup, bench-\nmark, and evaluation metrics, we study the behav-\nior of the DSI model as new documents are contin-\nuously added to the system. Concretely, we are in-\nterested in investigating the following for continual\ntraining of the DSI model with indexing objective\non new documents – (Q1) How severe is the for-\ngetting for the initially indexed documents? (Q2)\nHow does continual updating of the DSI model\nover a sequence of corpora affect the forgetting?\n8200\nD0 D1 D2 D3 D4 D5\n50\n100Indexing accuracy\nAverage performance (An)\nD1 D2 D3 D4 D5\n50\n100\nForgetting (Fn)\nD1 D2 D3 D4 D5\n50\n100\nLearning performance (LAn)\nT5-Base\nT5-Large\nT5-XL\nT5-Base(N)\nT5-Base(S)\nD0 D1 D2 D3 D4 D5\nTraining corpus\n20\n40Hits@1\nD1 D2 D3 D4 D5\nTraining corpus\n20\n40\nD1 D2 D3 D4 D5\nTraining corpus\n20\n40\nFigure 2: Systematic study about forgetting and forward transfer when incrementally indexing new corpus of\ndocuments across different model sizes (T5-Base, T5-Large, T5-XL) and docid representations. We use atomic\ndocids by default and denote (N)/(S) for naively/ semantically structured docids. ↑indicates higher is better, ↓\nindicates lower is better. All results are averaged over 3 runs. We observe that the average An and learning LAn\nperformance improves by increasing the model scale. However, forgetting Fn is severe across all model scales.\nNext, we observe that naively structured docids, T5-Base(N), underperform unstructured atomic docids, T5-Base,\nacross all metrics - indexing accuracy, Hits@1, (see Figure 6 in Appendix for Hits@10 results). Imbuing the docid\nspace with a semantic (S) structure alleviates the forgetting compared to an arbitrary/ naive (N) structure.\n(Q3) How does the updated DSI model perform on\nnewly indexed documents, especially the retrieval\ntask? (Q4) How do different docid representation\nstrategies affect forgetting? (Q5) How does the DSI\nmodel scale affect forgetting? Figure 2 visualizes\nresults on the validation split of DSI++ and helps\nus convincingly answer these questions.\nForgetting. From Figure 2, we see that the T5-\nBase model with atomic docid representation (blue\nline plots) undergoes significant forgetting. This\ntrend holds across all DSI evaluation metrics - in-\ndexing accuracy, Hits@1, and Hits@10 (see 6 in\nAppendix). For the originally indexed D0 corpus,\nindexing accuracy and Hits@1 drop by approx. 25\nand 20 points, respectively. Further, as we con-\ntinue indexing the sequence of corpora, we see that\nforgetting becomes even more severe. For exam-\nple, after continually indexing the D5 corpus, F5\n(forgetting) for indexing accuracy increases to 75.\nThese results provide evidence to answer (Q1) &\n(Q2) that the DSI model undergoes severe forget-\nting under continual indexing of new documents.\nForward transfer. To answer (Q3), we visualize\nthe learning performance (LAn) for all DSI met-\nrics for sequential indexing. From Figure 2, we\nsee LAn increases in indexing accuracy, suggest-\ning that the DSI model is plastic enough to index\nnew documents. However, from Figure 2, we see\na declining trend for Hits@1. Due to the continu-\nous indexing updates, the underlying DSI model\ndrifts and becomes less effective for the retrieval\ntask. These findings hint at an approach that re-\nplays indexing and retrieval tasks during continual\nlearning (hence our proposed method in §4).\nDocid representations. For studying (Q4), we\nconsider unstructured atomic, naively(N) struc-\ntured, and semantically(S) structured docid repre-\nsentations. From Figure 2, we see that T5-Base(N)\nunderperforms T5-Base by a significant margin.\nFor example, the average performance A0 for the\nHits@1 metric is approx. 30 and 39 for naive and\natomic docids, respectively. Further, as the naively\nstructured approach treats unstructured docids as\ntokenizable strings as opposed to dedicated unique\ntokens in the case of atomic docids, they are rela-\ntively more prone to interference from new docids\n(see Fn subplot for indexing accuracy). Imbuing\nsemantic structure to the naive docid space helps\nto reduce forgetting however still underperforms\nunstructured docids.\nModel scale. As atomic docids are superior to\nnaive docids, we only consider atomic docids for\nanswering (Q5). From Figure 2, we observe that\nlarger models outperform their smaller counterparts\nin terms of the average performance An and the\nlearning performance LAn (T5-XL >T5-Large >\nT5-Base). However, empirically we report that for-\ngetting Fn is severe across all model scales, with-\nout any clear best performer, and therefore, we\nfocus on T5-Base for the rest of our experiments.\n8201\n3 Implicit Forgetting: SAM\nMemorization (or indexing) is a primary task in the\nDSI paradigm where the goal is to learn a neural\ncorpus indexer that takes document content as input\nand maps it to a document identifier (docid). Under\nthe unstructured atomic docid representation strat-\negy, each docid is assigned a unique token/class\nlabel. Now given a large number of documents in\nthe corpus (even more than a million), memoriza-\ntion constitutes an instance of challenging extreme\nclassification setting (Bengio et al., 2019). Further-\nmore, for every class, we have only one labeled\nexample (i.e., document and its identifier), making\nthis task setup rare. Motivated by this largely unex-\nplored setup, we investigate the learning dynamics\nfor the memorization task throughout training.\nForgetting events. In Figure 5, we visualize the\nindexing accuracy for the T5-Base model, opti-\nmized with Adafactor (Shazeer and Stern, 2018).\nWe note that the model performance fluctuates\nthroughout training, suggesting unstable memoriza-\ntion. We hypothesize that the model continuously\nundergoes the forgetting phenomenon wherein sub-\nsequent mini-batch updates interfere with the previ-\nously memorized documents. To differentiate this\nphenomenon from forgetting due to adding new\ndocuments, we refer to the earlier one as implicit\nforgetting and the latter as explicit forgetting. To\nquantify instability during memorization, we com-\npute forgetting event (Toneva et al., 2019) statis-\ntics. Forgetting event is defined when an individ-\nual document goes from being classified correctly\n(mapped to correct docid) to incorrectly throughout\nmemorization. In Figure 3, we plot the cumulative\nhistogram of forgetting events where almost 88%\nof the documents undergo forgetting at least once,\nvalidating our hypothesis about implicit forgetting.\nFlatness and forgetting. Mirzadeh et al. (2020)\nshows that during sequential learning of tasks, flat-\nter minima leads to less forgetting. Further, Mehta\net al. (2023) shows that pre-trained initialization\nimplicitly alleviates forgetting as they prefer flatter\nminima and explicitly optimizing for the flatness\nusing Sharpness-Aware Minimization (SAM; Foret\net al. (2021)) further lessens forgetting. Based on\nthese observations, we hypothesize that modifying\nthe training dynamics of the memorization tasks\nusing SAM should alleviate implicit forgetting.\n20 40 60 80 100\nPercentage of examples\n0\n5\n10\n15Number of forgetting events\nOptimizer\nAdafactor\nSAM\nFigure 3: Investigating the effectiveness of SAM for\nalleviating implicit forgetting in the T5-Base model by\nvisualizing cumulative histogram of forgetting events.\nA forgetting event (Toneva et al., 2019) is defined when\nan individual document goes from being classified cor-\nrectly to incorrectly over the course of memorization.\nSAM increases the percentage of examples experiencing\nzero forgetting events by absolute 12% over Adafactor.\nSharpness-Aware Minimization. For the loss\nfunction f, SAM seeks to find the parameters w\nthat lie in the neighborhood with uniformly low\nloss regions by optimizing the following minimax\nobjective: minwmax||ϵ||2≤ρf(w+ ϵ), where the\nmaximization region is defined to be a ℓp ball\nwith radius ρ for p = 2. Foret et al. (2021) es-\ntimates the gradient of the inner maximization by\nemploying first-order approximation as follows:\n∇wmax||ϵ||2≤ρf(w + ϵ) ≈ ∇wf(w)\n⏐⏐\nw+ˆϵ(w),\nwhere ˆϵ(w) = ρ∇wf(w)/||∇wf(w)||2. For a\ngiven mini-batch B, SAM approximately computes\na point w′= w+ ˆϵ(w) where loss is maximum and\nthen updates the current model weights w using\nthe gradient at w′. We defer readers to (Foret et al.,\n2021) for complete details about this derivation.\nSAM alleviates implicit forgetting.We investi-\ngate the applicability of SAM for alleviating the im-\nplicit forgetting phenomenon. We use a pre-trained\nT5-Base model to memorize D0 corpus containing\n50K unique documents. We compare the perfor-\nmance of the SAM with the Adafactor optimizer. In\nFigure 5, we see that SAM outperforms Adafactor\nin terms of the overall indexing accuracy. We also\nnote that SAM undergoes less severe fluctuations\nduring training, thus, hinting at less forgetting. To\nbolster this claim, in Figure 3, we see that SAM\nhas a significantly higher percentage of documents\ncorresponding to a lower cumulative number of\nforgetting events, i.e., SAM stably (with zero for-\n8202\ngetting events) memorizes +12% more documents\nthan Adafactor. We also note that SAM (35.9±2.2)\noutperforms Adafactor (32.5±6.4) when evaluated\non the retrieval task (Hits@1) corresponding to D0.\nTherefore, we set SAM to be our default optimizer\nfor the rest of the experiments.\nDiscussion. Mehta et al. (2023) show that ex-\nplicitly optimizing for flatness using SAM leads\nto less forgetting, especially in task-incremental\nlearning settings where data undergoes a clear dis-\ntributional shift. We extend this work to the new\nDSI paradigm and convincingly demonstrate that\nSAM helps with the stable memorization of docu-\nments. Our results generalize the earlier findings\neven to the settings where data does not undergo a\nclear distributional shift (i.e., memorization task).\nAlthough SAM helps stably memorize documents,\nthere is still room for improvement, and our work\ninvites more future work in this direction.\n4 Explicit Forgetting: Generative\nMemory\nThe DSI paradigm consists of two tasks – memo-\nrization and retrieval. The previous section show-\ncases that SAM alleviates implicit forgetting by sta-\nbly memorizing documents. In this section, we fo-\ncus on the forgetting phenomenon that arises from\nthe continual indexing of new documents, specifi-\ncally in the context of the retrieval task. Through\nour systematic study (in §2.4), we show that irre-\nspective of the model scale and docid representa-\ntions, DSI models undergo severe forgetting. More-\nover, we observe that the learning performance\nLAn keeps declining for the retrieval task (see Fig-\nures 2 and 6 for Hits@1 and Hits@10, respectively).\nThis observation suggests that as we continuously\nupdate the DSI model with the indexing objective,\nthe model forgets the retrieval task. In DSI, both\nmemorization and retrieval tasks return docid for\ninput. By setup, we can assume access to previ-\nous documents and continue indexing old and new\ndocuments to reduce forgetting of the retrieval task.\nHowever, in Figure 4, we see that the model still\nundergoes forgetting (more in §5.2).\nEpisodic memory. According to the Comple-\nmentary Learning Systems (McClelland et al.,\n1995) theory, humans use episodic memory to store\nand revisit past experiences for retaining learned\nknowledge. Based on this motivation, memory-\nbased approaches (Sodhani et al., 2022), like Ex-\nperience Replay (ER; Chaudhry et al. (2019)) for\ncontinual learning use a subset of previous task data\nto regularize the future task learning while mini-\nmizing forgetting. Based upon this, one approach\nfor DSI++ is to retain ground-truth queries for the\nretrieval task in episodic memory and use them to\nco-train with incremental indexing tasks. However,\nin DSI++, we cannot access ground-truth queries\nfor an incoming batch of new documents. Even if\none retains queries for the initial D0 corpus, we\nshow in Table 1 that such a method suffers from\nforward transfer to newly indexed documents.\nGenerative memory. Recent years have seen sig-\nnificant progress in the capabilities of the genera-\ntive language models (Raffel et al., 2020; Brown\net al., 2020). Motivated by the success of these\nmodels and the in-applicability of the episodic\nmemory for DSI++, we pose a question –instead of\nretaining the ground-truth queries, can we learn a\nparametric model to generate such queries given a\ndocument? Concretely, we propose to train a query\ngenerator model to sample queries for previously\nseen documents and supplement them during incre-\nmental indexing. Since we use the generator model\nto sample queries for sparse experience replay, our\nproposed method – generative memory. Moreover,\ngenerative memory is also used to generate pseudo-\nqueries for the incoming batch of new documents,\nthus, enabling continual semi-supervised learn-\ning of the retrieval task.\n5 Experimentation\nIn this section, the models are initialized with the\npre-trained T5-Base model, while the additional\nparameters for atomic docid tokens are randomly\ninitialized. See §A.1 for implementation details.\n5.1 Methods\nWe compare our proposed generative memory-\nbased approach with the following methods:\nContinual indexing, cl(Dn). The DSI model is\nsequentially fine-tuned with the indexing objective\non the incoming corpus of documents Dn.\nContinual indexing with all seen documents,\ncl(Un). The DSI model is continuously fine-tuned\nwith the indexing objective on the updated corpora\nUn (⋃n\ni=0 Di) with the same replay frequency for\nthe old ( ⋃n−1\ni=0 Di) and new ( Dn) corpora in the\ntasks mixture.\n8203\nAdded Method Eval corpus = D0 Eval corpus = D1\ncorpus (Catastrophic forgetting) (Forward transfer)\nIndex acc. Hits@1 Hits@10 Index acc. Hits@1 Hits@10\nD0 - 81.81.2 35.92.2 66.90.9 - - -\nD1\ncl(D1) 52.43.5 19.23.9 43.65.7 96.50.0 31.76.4 55.64.9\ncl(U1 = D0 ∪ D1) 78.20.5 28.98.9 59.07.9 91.80.4 34.02.4 60.21.9\ncl(U1)+epsmem(D0) 77.80.5 22.91.5 51.40.5 93.10.0 13.12.1 39.63.1\ncl(U1)+genmem(D0) 77.80.3 26.06.9 54.98.3 93.00.5 8.64.8 31.611.8\ncl(U1)+epsmem(D1) 53.23.1 7.72.1 26.02.0 96.50.0 48.32.3 70.71.9\ncl(U1)+genmem(D1) 50.10.8 7.01.2 23.12.2 96.50.0 57.71.5 76.70.9\ncl(U1)+genmem(U1) 78.20.3 18.42.8 47.53.9 92.10.3 48.56.1 73.82.9\ncl(U1, docid parameters only) 78.90.1 32.75.1 64.84.2 94.60.1 10.83.8 35.07.3\ntrain from scratch 78.70.6 35.91.4 66.40.0 79.20.3 32.91.8 63.91.2\nTable 1: Comparing performance on incremental indexing of D1 corpus across different methods - cl(D1): continue\nfine-tuning with indexing task on D1, cl(U1): continue fine-tuning on the updated corpus U1, cl(U1)+epsmem(D):\ncontinual indexing of U1 along with ER of queries for D, cl(U1)+genmem(D): continual indexing of U1 along with\nER of pseudo-queries for D. We observe that continual indexing on the updated corpus cl(U1) reduces forgetting\ncompared to just indexing new corpus cl(D1) in the Natural Questions (NQ) dataset (|D0|= 50K, |D1|= 10K).\nNext, ER with either D0 or D1 hurts forward transfer or forgetting. Our proposed approach of augmenting pseudo-\nqueries for all documents along with continual indexing, cl(U1)+genmem(U1), alleviates forgetting of D0 corpus\nand improves forward transfer to D1 corpus.\nContinual experience replay using generative\nmemory, genmem(Dn). In this method, the pro-\nposed generative memory model is used to sample\npseudo-queries corresponding to the corpus Dn.\nNext, these pseudo-queries are used for (sparse)\nexperience replay of the retrieval task samples.\nContinual experience replay using episodic\nmemory, epsmem(Dn). In this method, ground-\ntruth queries corresponding to the Dth\nn corpus are\nused for experience replay of the retrieval task.\ncl(Un, docid parameters only).In this method,\nwe only update the parameters corresponding to\natomic docid tokens using the updated Un corpus.\nThis method in spirit is a dual-encoder-baseline.\nTrain from scratch, (no cl).The DSI model is\ntrained from scratch every time a new corpus is\nadded. This method corresponds to a non-continual\nlearning setup and is computationally expensive.\n5.2 Results\nIn this section, we revisit some of the questions\n(Q1)-(Q3) raised in our case study (see §2.4) to\ninvestigate the effectiveness of our proposed gen-\nerative memory-based approach. To answer these\nquestions, in Table 1, we report the performance\nof the DSI model on D0 (to study the forgetting\nphenomenon) and D1 corpora (to answer forward\ntransfer question) after continual indexing on D1\nfor both NQ and MS MARCO datasets. In Figures\n4 and 7 (NQ) and Figure 8 (MS MARCO), we re-\nport overall performance across DSI metrics as we\ncontinuously update the model with the sequence\nof five corpora (D1 →···→ D5).\nDoes generative memory alleviate forgetting of\nold documents? In Table 1, for the NQ dataset,\nwe report Hits@1 to be 35.9 for the model after\ntraining on D0. We see that continually indexing\nboth D0 and D1 corpora (cl(U1) - 28.9), signifi-\ncantly reduce forgetting the retrieval task (Hits@1)\nover just indexing the new corpora D1 (cl(D1) -\n19.2). Next, we look at the performance of the ER\napproaches when augmented with the continual in-\ndexing of all documents. We see that both episodic\nmemory (cl( U1)+epsmem(D0) - 22.9), and gen-\nerative memory (cl(U1)+genmem(D0) - 26.0) re-\nduce forgetting compared to cl( D1) when we re-\nplay (pseudo-)queries corresponding to D0 cor-\npus. Moreover, generative memory outperforms\nepisodic memory without retaining original queries.\nAlthough from Table 1, we see generative memory,\ncl(U1)+genmem(U1), underperforms cl(U1), from\nFigures 4 and 7, we see that generative memory,\ncl(U5)+genmem(U5), outperforms cl(U5) both in\nterms of average performance An and forgetting\nFn over five sequential updates. These results con-\nvincingly show that the ER with generative memory\nsignificantly alleviates forgetting the retrieval task\ncompared to considered baselines.\n8204\nD0 D1 D2 D3 D4 D5\n0\n50\n100Indexing accuracy\nAverage performance (An)\nD1 D2 D3 D4 D5\n0\n50\n100\nForgetting (Fn)\nD1 D2 D3 D4 D5\n0\n50\n100\nLearning performance (LAn)\ncl(Dn)\ncl(Un)\ncl(Un)+genmem(Un) with r=32\ncl(Un)+genmem(Un) with r=2\ntrain from scratch (no cl)\nD0 D1 D2 D3 D4 D5\nTraining corpus\n0\n25\n50\n75Hits@10\nD1 D2 D3 D4 D5\nTraining corpus\n0\n25\n50\n75\nD1 D2 D3 D4 D5\nTraining corpus\n0\n25\n50\n75\nFigure 4: Investigating the effectiveness of generative memory in mitigating forgetting when continuously indexing\nnew corpus Dn (T5-Base model and atomic docids representation) for the NQ dataset. ↑indicates higher is better, ↓\nindicates lower is better. We observe that continual indexing of old and new documents cl(Un) helps to alleviate\nforgetting of older documents when evaluated on retrieval tasks. However, average Hits@10 (An) still undergo\n23 points drop after sequential updates (D0 →D1 ···→ D5). Generative memory enables sparse replaying of\npseudo-queries for old documents and continual semi-supervised learning with new documents. We observe that\naugmenting generative memory during continual indexing not only reduces the forgetting (Fn) but also improves\naverage Hits@10 (An) by +21.1% over considered baselines (see Figure 7 for Hits@1 results. Figure 8 for MS\nMARCO results in the Appendix).\nDoes generative memory enable forward trans-\nfer to new documents? One of the goals of\nDSI++ is to enable answering queries related to\nnewly indexed documents. Towards this goal, in\nTable 1, for the NQ dataset, we look at the retrieval\ntask performance (Hits@1) for D1 after incremen-\ntally indexing D1. To compare different methods,\nwe consider a baseline in the form of ER with\nground-truth queries for D1 (cl(U1)+epsmem(D1)\n- 48.3). We see that without any fine-tuning on the\nretrieval task for D1, incremental learning with in-\ndexing objective shows impressive forward transfer\n(or zero-shot gains, cl(D1) - 31.7 and cl(U1) - 34.0).\nMoreover, ER with generative memory outper-\nforms supervised baseline (cl( U1)+genmem(D1)\n- 57.7). However, we notice that replaying queries\ncorresponding to either D0 or D1 hurt forward\ntransfer to D1 (cl(U1)+genmem(D0) - 8.6) or am-\nplify forgetting of D0 (cl(U1)+genmem(D1) - 7.0).\nThese results suggest that the memory module\nshould include (pseudo-)queries corresponding to\nold and new documents. From Figure 4, we see that\ncontinual indexing method cl(Un) has a downward\ntrend for LAn (Hits@10), therefore, eventually for-\ngetting the retrieval task. On the other hand, ER\nwith generative memory is relatively constant, pro-\nviding evidence against forgetting. In summary, ER\nwith generative memory enhances retrieval task\nperformance by reducing forgetting of indexed doc-\numents and enabling forward transfer to newly\nindexed documents.\nDoes generative memory generalize to differ-\nent datasets? In Table 3, for the MS MARCO\ndataset, we report Hits@1 to be 78.2 after training\non D0 passages. We see that continually index-\ning both D0 and D1 corpora (cl(U1) - 76.5 and\ncl(U1)+genmem(U1) - 73.7), significantly reduce\nforgetting the retrieval task (Hits@1) over just in-\ndexing the new corpora D1 (cl(D1) - 68.0). Next,\nwe look at the retrieval task performance (Hits@1)\nfor D1 after incrementally indexing D1. We see\nthat without any fine-tuning on the retrieval task\nfor D1, incremental learning with indexing ob-\njective shows impressive forward transfer (cl(D1)\n- 36.1 and cl( U1) - 35.3). Moreover, ER with\ngenerative memory, cl( U1)+genmem(U1) - 80.6,\nperforms far superior to just incremental index-\ning objective. Similar to the results with the NQ\ndataset, we show that ER with generative memory,\ncl(Un)+genmem(Un), improves the overall perfor-\nmance for the retrieval task, reducing forgetting of\npreviously indexed documents and enables forward\ntransfer to new documents compared to continual\nindexing of all documents, cl(Un). We show that\n8205\nour results hold across two datasets, thus, show-\ncasing the generalizability of our approach.\nInvestigating the effectiveness of the generative\nmemory with the scale of a corpus.We con-\nduct experiments with a full MS MARCO dataset\n(≈ 8.9M passages). We construct two corpora\n– D0 = 8M and D1 = 841,823 passages. We\ntrain the DSI model using D0 passages and incre-\nmental add D1 passages. In Table 3, we report\nresults for MS MARCO. We see that continual fine-\ntuning with the indexing task on D1, cl(D1), com-\npletely forgets the retrieval task for D0 passages\n(Hits@1 goes to 0.1 from 16.3). However, the\ngenerative memory-based approach significantly\nreduces forgetting (Hits@1 of 7.3). Moreover, gen-\nerative memory enables continual semi-supervised\nlearning by augmenting pseudo-queries for D1 pas-\nsages, thereby improving forward transfer (Hits@1\nof 31.6 vs. 18.2 for cl(D1)). Our proposed solution\nreduces forgetting in large corpus settings.\nInvestigating sparsity of experience replay (ER)\non forgetting. ER with generative memory co-\ntrains the indexing and pseudo-labeled retrieval\ntasks. Tay et al. (2022) introduces a mixing ratio\nr to define the ratio of indexing to retrieval sam-\nples. The mixing ratio is inversely related to the\nsparsity of ER, i.e., higher r(more indexing sam-\nples) corresponds to sparse updates from pseudo-\nlabeled retrieval samples. Following (Tay et al.,\n2022), we consider r = {2,32}for our analysis.\nFrom Figure 4, we see that r= 32(sparse replay)\nslightly outperforms r= 2in terms of average per-\nformance, forgetting, and learning accuracy. These\nresults suggest that even sparse regularization up-\ndates from ER positively influence backward and\nforward transfer in DSI++.\nAnalyzing index construction time for DSI++.\nDSI involves training a Transformer model for in-\ndex construction. DSI++ allows incremental up-\ndating of the indexer. In Figures 4, 7, and 8, we\ndemonstrate that our incremental indexer updating\nmethod surpasses the “train from scratch” baseline\nin terms of An. Note that the “train from scratch”\nbaseline can serve as a performance upper bound\nfor continual learning when there is no detrimental\ninterference among tasks, and all tasks are evenly\nbalanced. However, in the case of DSI++, there\nexists an initial base corpus that is larger than sub-\nsequent corpora, leading to an imbalance among\ntasks. Consequently, “train from scratch” should\nbe regarded as a competitive baseline rather than\nan inherent upper bound. This is also the reason\nbehind reporting the learning accuracy (LAn) for\nevery metric, which can be seen as an upper bound\nsince it maintains a running average of the best per-\nformance across all corpora. Furthermore, one of\nthe key objectives of continual learning is to lever-\nage prior knowledge to enhance the learning of new\ntasks. Indeed, from Tables 1 and 3, we observe that\nour proposed method excels in forward transfer\ncompared to the “train from scratch” approach.\nFor the NQ dataset, indexing the initial D0 cor-\npus of 50K documents requires 350K training steps.\nIf we sequentially index additional D1 to D5 cor-\npora (10K each) by re-training the DSI model each\ntime, it would require around 1.75M steps. In con-\ntrast, our approach only requires slightly above\n300K additional updates to incrementally index all\ncorpora, which is approximately six times fewer\nupdates. Our approach achieves superior overall\nperformance compared to re-training from scratch,\nwhile also being more computationally efficient.\n6 Conclusion\nDSI++ introduces a new approach to address a cru-\ncial requirement of DSI models for practical use\nin production setups, where continuous addition of\nnew documents to the corpus is necessary. Through\nexperiments, we demonstrate the effectiveness of\nour proposed solutions: sharpness-aware minimiza-\ntion and generative memory, which significantly\nreduce catastrophic forgetting. This work estab-\nlishes a foundation for further research, benefiting\nboth DSI models and the broader community of\ncontinual (semi-supervised) learning.\nLimitations\nIn this study, we explore the phenomenon of forget-\nting in relation to the addition of new and distinct\ndocuments into the indexer. It is important to note\nthat when a new document refutes or modifies a\npreviously indexed document, the model’s behavior\nbecomes unpredictable, requiring further analysis.\nAdditionally, we examine the effectiveness of our\nproposed method on a larger dataset, such as the\nfull MS MARCO dataset. However, it is worth\nnoting that with this larger dataset, the method ex-\nhibits significant forgetting. As a result, additional\nresearch is necessary to enhance the model’s per-\nformance, particularly when dealing with datasets\nof larger scales.\n8206\nEthics Statement\nTraining large models is expensive and can have\na detrimental impact on the environment (Strubell\net al., 2019). Continual learning on top of existing\nmodels is preferable to re-training from scratch in\nthis regard since it requires many fewer training\nsteps. With DSI++, we aim to reduce the need to\nre-train DSI models from scratch whenever a new\nset of documents is added to the corpus thereby\nmaking it cheaper and better for the environment.\nConcretely, in §5.2, we analyze the index construc-\ntion time for DSI++ and show that our approach\nis computationally efficient in comparison to re-\ntraining the model from scratch. At the same time,\nwe acknowledge that reduced cost can increase\noverall consumption (Jevons’ paradox).\nAcknowledgements\nWe thank the anonymous reviewers for their valu-\nable feedback and suggestions, which helped im-\nprove the paper. We also thank Ronak Pradeep\nand Kai Hui for help with the MS MARCO setup,\nTal Schuster and Raghuram Mandyam Annasamy\nfor reviewing the paper, and William W. Cohen,\nAditya Gupta, Dara Bahri, and Fuzhao Xue for\nsharing insights and intuitions during initial discus-\nsions. We would like to thank COMEDY (COhorts\nof Maarten Sap, Emma Strubell, Daniel Fried, and\nYonatan Bisk) lab members for reviewing the paper\nand providing valuable comments; Jeremiah Mil-\nbauer, Clara Na, Jared Fernandez, Nupoor Gandhi,\nZhisong Zhang, and Vijay Viswanathan also gave\nconstructive feedback on drafts and tables.\nReferences\nBadr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona\nDiab, and Marjan Ghazvininejad. 2022. A review on\nlanguage models as knowledge bases. arXiv preprint\narXiv:2204.06031.\nDara Bahri, Hossein Mobahi, and Yi Tay. 2022.\nSharpness-aware minimization improves language\nmodel generalization. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 7360–\n7371.\nSamy Bengio, Krzysztof Dembczynski, Thorsten\nJoachims, Marius Kloft, and Manik Varma. 2019.\nExtreme classification (dagstuhl seminar 18291).\nIn Dagstuhl Reports, volume 8. Schloss Dagstuhl-\nLeibniz-Zentrum für Informatik.\nLuiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and\nRodrigo Nogueira. 2022. Inpars: Data augmentation\nfor information retrieval using large language models.\narXiv preprint arXiv:2202.05144.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International Conference on Ma-\nchine Learning, pages 2206–2240. PMLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems, 33:1877–1901.\nArslan Chaudhry, Marcus Rohrbach, Mohamed Elho-\nseiny, Thalaiyasingam Ajanthan, Puneet K Dokania,\nPhilip HS Torr, and Marc’Aurelio Ranzato. 2019. On\ntiny episodic memories in continual learning. arXiv\npreprint arXiv:1902.10486.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021. Edit-\ning factual knowledge in language models. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6491–\n6506.\nMatthias De Lange, Rahaf Aljundi, Marc Masana, Sarah\nParisot, Xu Jia, Aleš Leonardis, Gregory Slabaugh,\nand Tinne Tuytelaars. 2021. A continual learning sur-\nvey: Defying forgetting in classification tasks. IEEE\ntransactions on Pattern Analysis and Machine Intel-\nligence, 44(7):3366–3385.\nCyprien de Masson D’Autume, Sebastian Ruder, Ling-\npeng Kong, and Dani Yogatama. 2019. Episodic\nmemory in lifelong language learning. Advances in\nNeural Information Processing Systems, 32.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), page\n4171–4186.\nBhuwan Dhingra, Jeremy R Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W Cohen. 2022. Time-aware language mod-\nels as temporal knowledge bases. Transactions of the\nAssociation for Computational Linguistics, 10:257–\n273.\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and\nBehnam Neyshabur. 2021. Sharpness-aware mini-\nmization for efficiently improving generalization. In\nInternational Conference on Learning Representa-\ntions.\n8207\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. REALM: Retrieval\naugmented language model pre-training. In Inter-\nnational Conference on Machine Learning , pages\n3929–3938. PMLR.\nGautier Izacard and Édouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know. Transactions of the Association for\nComputational Linguistics, 8:423–438.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781.\nJames Kirkpatrick, Razvan Pascanu, Neil Rabinowitz,\nJoel Veness, Guillaume Desjardins, Andrei A Rusu,\nKieran Milan, John Quan, Tiago Ramalho, Ag-\nnieszka Grabska-Barwinska, et al. 2017. Over-\ncoming catastrophic forgetting in neural networks.\nProceedings of the National Academy of Sciences ,\n114(13):3521–3526.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of the\nAssociation for Computational Linguistics , 7:453–\n466.\nJames L McClelland, Bruce L McNaughton, and Ran-\ndall C O’Reilly. 1995. Why there are complementary\nlearning systems in the hippocampus and neocortex:\ninsights from the successes and failures of connec-\ntionist models of learning and memory. Psychologi-\ncal Review, 102(3):419.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: The\nsequential learning problem. In Psychology of Learn-\ning and Motivation, volume 24, pages 109–165. El-\nsevier.\nSanket Vaibhav Mehta, Darshan Patil, Sarath Chandar,\nand Emma Strubell. 2023. An empirical investiga-\ntion of the role of pre-training in lifelong learning.\nJournal of Machine Learning Research, 24(214):1–\n50.\nSanket Vaibhav Mehta, Jinfeng Rao, Yi Tay, Mihir Kale,\nAnkur Parikh, and Emma Strubell. 2022. Improving\ncompositional generalization with self-training for\ndata-to-text generation. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 4205–\n4219.\nKevin Meng, David Bau, Alex J Andonian, and Yonatan\nBelinkov. 2022. Locating and editing factual associ-\nations in GPT. In Advances in Neural Information\nProcessing Systems, volume 35.\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pas-\ncanu, and Hassan Ghasemzadeh. 2020. Understand-\ning the role of training regimes in continual learning.\nAdvances in Neural Information Processing Systems,\n33:7308–7320.\nEric Mitchell, Charles Lin, Antoine Bosselut, Chelsea\nFinn, and Christopher D Manning. 2022. Fast model\nediting at scale. In International Conference on\nLearning Representations.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao,\nSaurabh Tiwary, Rangan Majumder, and Li Deng.\n2016. MS MARCO: A human generated machine\nreading comprehension dataset. In Proceedings of\nthe Workshop on Cognitive Computation: Integrating\nneural and symbolic approaches 2016.\nJianmo Ni, Gustavo Hernandez Abrego, Noah Constant,\nJi Ma, Keith Hall, Daniel Cer, and Yinfei Yang. 2022.\nSentence-t5: Scalable sentence encoders from pre-\ntrained text-to-text models. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\npages 1864–1874.\nGerman I Parisi, Ronald Kemker, Jose L Part, Christo-\npher Kanan, and Stefan Wermter. 2019. Continual\nlifelong learning with neural networks: A review.\nNeural Networks, 113:54–71.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473.\nRonak Pradeep, Kai Hui, Jai Gupta, Adam D Lelkes,\nHonglei Zhuang, Jimmy Lin, Donald Metzler, and\nVinh Q Tran. 2023. How does generative retrieval\nscale to millions of passages? arXiv preprint\narXiv:2305.11841.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nSylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg\nSperl, and Christoph H Lampert. 2017. iCaRL: In-\ncremental classifier and representation learning. In\nProceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, pages 2001–2010.\n8208\nAdam Roberts, Hyung Won Chung, Anselm Levskaya,\nGaurav Mishra, James Bradbury, Daniel Andor, Sha-\nran Narang, Brian Lester, Colin Gaffney, Afroz\nMohiuddin, Curtis Hawthorne, Aitor Lewkowycz,\nAlex Salcianu, Marc van Zee, Jacob Austin, Se-\nbastian Goodman, Livio Baldini Soares, Haitang\nHu, Sasha Tsvyashchenko, Aakanksha Chowdh-\nery, Jasmijn Bastings, Jannis Bulian, Xavier Gar-\ncia, Jianmo Ni, Andrew Chen, Kathleen Kenealy,\nJonathan H. Clark, Stephan Lee, Dan Garrette, James\nLee-Thorp, Colin Raffel, Noam Shazeer, Marvin\nRitter, Maarten Bosma, Alexandre Passos, Jeremy\nMaitin-Shepard, Noah Fiedel, Mark Omernick, Bren-\nnan Saeta, Ryan Sepassi, Alexander Spiridonov,\nJoshua Newlan, and Andrea Gesmundo. 2022. Scal-\ning up models and data with t5x and seqio. arXiv\npreprint arXiv:2203.17189.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning,\npages 4596–4604. PMLR.\nHanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon\nKim. 2017. Continual learning with deep generative\nreplay. Advances in Neural Information Processing\nSystems, 30.\nShagun Sodhani, Mojtaba Faramarzi, Sanket Vaib-\nhav Mehta, Pranshu Malviya, Mohamed Abdel-\nsalam, Janarthanan Janarthanan, and Sarath Chan-\ndar. 2022. An introduction to lifelong supervised\nlearning. arXiv preprint arXiv:2207.04354.\nPablo Sprechmann, Siddhant Jayakumar, Jack Rae,\nAlexander Pritzel, Adria Puigdomenech Badia, Be-\nnigno Uria, Oriol Vinyals, Demis Hassabis, Razvan\nPascanu, and Charles Blundell. 2018. Memory-based\nparameter adaptation. In International Conference\non Learning Representations.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650.\nFan-Keng Sun, Cheng-Hao Ho, and Hung-Yi Lee. 2020.\nLAMAL: LAnguage modeling is all you need for life-\nlong language learning. In International Conference\non Learning Representations.\nYi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni,\nDara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe\nZhao, Jai Gupta, Tal Schuster, William W. Cohen,\nand Donald Metzler. 2022. Transformer memory as\na differentiable search index. In Advances in Neural\nInformation Processing Systems, volume 35.\nSebastian Thrun. 1995. Is learning the n-th thing any\neasier than learning the first? Advances in Neural\nInformation Processing Systems, 8.\nMariya Toneva, Alessandro Sordoni, Remi Tachet des\nCombes, Adam Trischler, Yoshua Bengio, and Geof-\nfrey J Gordon. 2019. An empirical study of example\nforgetting during deep neural network learning. In\nInternational Conference on Learning Representa-\ntions.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Process-\ning Systems, 30.\nZirui Wang, Sanket Vaibhav Mehta, Barnabás Póczos,\nand Jaime G Carbonell. 2020. Efficient meta lifelong-\nlearning with limited memory. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 535–548.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh\nBhojanapalli, Daliang Li, Felix Yu, and Sanjiv Kumar.\n2020. Modifying memories in transformer models.\narXiv preprint arXiv:2012.00363.\nShengyao Zhuang, Houxing Ren, Linjun Shou, Jian Pei,\nMing Gong, Guido Zuccon, and Daxin Jiang. 2022.\nBridging the gap between indexing and retrieval for\ndifferentiable search index with query generation.\narXiv preprint arXiv:2206.10128.\nA Appendix\nA.1 Implementation Details\nWe utilize the pre-trained T5-Base (Raffel et al.,\n2020) to initialize all models and randomly ini-\ntialize the additional parameters for atomic docid\ntokens. Bahri et al. (2022) demonstrates the suc-\ncessful applicability of SAM for language model\ngeneralization, especially in pre-trained T5 models.\nWe mainly follow (Bahri et al., 2022) to set our\nhyper-parameters: ρ= 0.15, batch size=32 for the\ninner maximization step in SAM.\nWhile indexing D0 corpus, we train all the mod-\nels for a maximum of 1M steps with a warmup of\n100K steps. During continual indexing of other cor-\npora, we train for a maximum of 100K steps with\na warmup of 100 steps. For the rest of the hyper-\nparameters, we follow (Tay et al., 2022) – set a\nlearning rate to 0.001, batch size to 128, and input\nsequence length to 32. We evaluate models after ev-\nery 5K steps and retain the checkpoint yielding the\nbest performance. For the initial training with D0\ncorpus, we co-train on indexing and retrieval tasks;\ntherefore, we use the average of all DSI metrics (in-\ndexing accuracy, Hits@1, and Hits@10) for model\n8209\n1 2 3 4 5\nTraining step ( x100,000)\n60\n65\n70\n75\n80Indexing accuracy\nOptimizer\nAdafactor\nSAM\nFigure 5: Investigating the effectiveness of SAM for\nalleviating implicit forgetting in the T5-Base model by\nvisualizing indexing accuracy during memorization. We\nobserve serious fluctuations in the indexing accuracy in\nthe case of the Adafactor optimizer, thereby suggesting\nunstable memorization. SAM leads to relatively stable\nmemorization of documents.\nselection. For the continual learning experiments,\nwe have access to only indexing accuracy for all\ninvolved corpora, so we use it for model selection.\nTo train a parametric model for generative mem-\nory, we utilize the retrieval datasetR0, which cor-\nresponds to the D0 corpus. We set the maximum\nsequence length for document contents to1024, the\ntarget length for generated queries to 32, batch size\nto 128, train for a maximum of100K steps, and use\nBLUE for model selection. We use beam decoding\nto generate pseudo-queries. We tune the learning\nrate amongst {0.001,0.0005}and linear warmup\namongst {1K,10K}. For all our experiments, we\nuse the T5X (Roberts et al., 2022) framework along\nwith 4-8 TPUv4 chips to train the models.\nA.2 Related Work\nWe review relevant prior work along two dimen-\nsions: Application setups related to DSI++ and\ncontinual learning methods to alleviate forgetting\nand enable forward transfer.\nLanguage models (LMs) as knowledge bases\n(KBs). Petroni et al. (2019) shows that pre-\ntrained BERT (Devlin et al., 2019) models cap-\nture relational knowledge comparable to that of\nthe KBs constructed using off-the-shelf techniques.\nConcretely, these models can be used to extract\nfactual knowledge about relations between entities\nby providing a prompt to predict missing words in\na cloze-style template (e.g., “New Delhi is the cap-\nital of ”). Similarly, Roberts et al. (2020) demon-\nstrates that pre-trained T5 (Raffel et al., 2020) mod-\nels can be employed to answer open-domain ques-\ntions without access to any external knowledge\nor context. However, unlike structured KBs, it is\nnon-trivial to update knowledge stored implicitly\nin the weights of these models. Therefore, Zhu\net al. (2020) introduces an experimentation setup\nwhere the task is to update facts stored within the\npre-trained models and proposes a constrained opti-\nmization method, similar to Elastic Weight Consol-\nidation (Kirkpatrick et al., 2017), to alleviate catas-\ntrophic forgetting. With similar motivation, (Dhin-\ngra et al., 2022) introduces a diagnostic dataset\nto probe LMs for facts that change over time. It\nalso suggests jointly modeling text with its times-\ntamp for improved memorization of seen facts. Re-\ncent works have been investigating efficient ways\nto localize and edit facts stored with the LMs\n(AlKhamissi et al., 2022) using finetuning (Zhu\net al., 2020; Dhingra et al., 2022), hyper-networks\n(De Cao et al., 2021; Mitchell et al., 2022), and\ndirect editing (Meng et al., 2022). Although a\ncrucial line of work around updating facts in the\npre-trained LMs, using prompting as our probing\nmechanism only provides a lower bound estimate\nof the knowledge contained in these models (Jiang\net al., 2020). On the other hand, we explicitly fo-\ncus on the memorization task in DSI++. This task\nhelps us to answer questions related to catastrophic\nforgetting more convincingly rather than bounded\nby the mechanism of how we probe these models.\nOptimization-based approaches for continual\nlearning encode the necessary inductive biases re-\nquired to enable continual learning by modifying\nthe training dynamics. Flatter minima are shown to\nalleviate forgetting (Mirzadeh et al., 2020). Further,\nMehta et al. (2023) showed that explicitly optimiz-\ning for flatter loss basins using Sharpness-Aware\nMinimization (SAM; Foret et al. (2021)) reduces\nforgetting. Building on these works, we show that\nflatter minima induced by SAM reduce implicit\nforgetting during memorization, thereby leading to\nmore stable memorization (see §3).\nMemory-based (aka data-based regularization)\napproaches for continual learning constrain the\nparameter updates based on the previous task ex-\namples sampled from memory. Sparse experience\nreplay using episodic memory (Chaudhry et al.,\n2019) is a prominent approach, and in §4, we dis-\ncuss its limitations of it for DSI++. Next, Shin et al.\n8210\nDataset #D Natural Questions (NQ) MS MARCO\n#Train #Validation #Test #Train #Validation #Test\nR0 50K 53.8K 13.5K 3.9K 2M 25.0K 3.6K\nR1 10K 10.7K 2.7K 809 400K 5.1K 762\nR2 10K 10.6K 2.7K 787 400K 5.1K 770\nR3 10K 10.7K 2.7K 727 400K 4.9K 734\nR4 10K 10.9K 2.7K 772 400K 4.9K 730\nR5 10K 10.7K 2.7K 847 400K 4.9K 660\nTable 2: DSI++ dataset statistics for NQ and MS MARCO: memorization and retrieval tasks.\nAdded Method Eval corpus = D0 Eval corpus = D1\ncorpus (Catastrophic forgetting) (Forward transfer)\nIndex acc. Hits@1 Hits@10 Index acc. Hits@1 Hits@10\nMS MARCO – |D0| = 50K, |D1| = 10K\nD0 - 99.40.2 78.20.2 95.00.1 - - -\nD1\ncl(D1) 46.718.6 68.02.0 87.31.3 99.80.0 36.19.5 65.86.9\ncl(U1) 99.40.0 76.50.7 94.20.3 99.80.0 35.34.1 64.43.3\ncl(U1)+genmem(U1) 99.30.1 73.70.2 93.90.3 99.80.0 80.61.0 95.50.1\ntrain from scratch 99.50.0 75.00.2 93.90.1 99.60.0 73.41.3 93.40.9\nMS MARCO (full) – |D0| = 8M, |D1| = 842K\nD0 - 99.4 16 .3 46 .8 - - -\nD1\ncl(D1) 0.0 0 .1 0 .6 97.9 18 .2 40 .5\ncl(U1)+genmem(U1) 20.4 7 .3 31 .3 86.6 31 .6 65 .8\nTable 3: Comparing performance on incremental indexing of D1 corpus across different methods - cl(D1): continue\nfine-tuning with indexing task on D1, cl(U1): continue fine-tuning on the updated corpus U1, cl(U1)+genmem(D):\ncontinual indexing of U1 along with ER of pseudo-queries for D. We observe that continual indexing on the\nupdated corpus cl( U1) reduces forgetting compared to just indexing new corpus cl( D1) in the MS MARCO\ndataset. Our proposed approach of augmenting pseudo-queries for all documents along with continual indexing,\ncl(U1)+genmem(U1), alleviates forgetting of D0 corpus and improves forward transfer to D1 corpus. We also show\nthat our proposed solution reduces forgetting of D0(= 8M) passages while incremental indexing in a large corpus\nsetting, MS MARCO (full) containing 8.9M passages.\nD0 D1 D2 D3 D4 D5\nTraining corpus\n20\n40\n60Hits@10\nAverage performance (An)\nD1 D2 D3 D4 D5\nTraining corpus\n20\n40\n60\nForgetting (Fn)\nD1 D2 D3 D4 D5\nTraining corpus\n20\n40\n60\nLearning performance (LAn)\nT5-Base\nT5-Large\nT5-XL\nT5-Base(N)\nT5-Base(S)\nFigure 6: Systematic study about forgetting and forward transfer when incrementally indexing new corpus of\ndocuments across different model sizes (T5-Base, T5-Large, T5-XL) and docid representations. We use atomic\ndocids by default and denote (N)/(S) for naively/semantically structured string docids. ↑indicates higher is better,\n↓indicates lower is better. We observe that by increasing the model scale, the average An and learning LAn\nperformance improves. However, forgetting Fn is severe across all model scales. Moreover, we observe that naive\nstring docids (N) underperform atomic docids across the Hits@10 metric. Similar to Figure 2, imbuing the docid\nspace with a semantic (S) structure alleviates the forgetting compared to an arbitrary/ naive (N) structure.\n(2017); Sun et al. (2020) learns a parametric model\nto reconstruct the examples for seen tasks. How-\never, in DSI++, we do not see queries for the new\ndocuments. Therefore, we use a parametric mem-\nory to generate pseudo-queries for already indexed\n(older) documents and an incoming batch of new\ndocuments, thus, enabling us to leverage unlabeled\ndata (in the form of new documents) for continual\nsemi-supervised learning. On the other hand, Sun\net al. (2020) assumes that the incoming data are\n8211\nD0 D1 D2 D3 D4 D5\nTraining corpus\n0\n20\n40\n60Hits@1\nAverage performance (An)\nD1 D2 D3 D4 D5\nTraining corpus\n0\n20\n40\n60\nForgetting (Fn)\nD1 D2 D3 D4 D5\nTraining corpus\n0\n20\n40\n60\nLearning performance (LAn)\ncl(Dn)\ncl(Un)\ncl(Un)+genmem(Un) with r=32\ncl(Un)+genmem(Un) with r=2\ntrain from scratch (no cl)\nFigure 7: Investigating the effectiveness of generative memory in mitigating forgetting when continuously indexing\nnew corpus Dn (T5-Base model and atomic docids representation) for the NQ dataset. ↑indicates higher is better, ↓\nindicates lower is better. We observe that continual indexing of old and new documents cl(Un) helps to alleviate\nforgetting of older documents when evaluated on retrieval tasks. However, average Hits@1 (An) still undergo 19\npoints drop after sequential updates (D0 →D1 ···→ D5). We observe that augmenting generative memory during\ncontinual indexing not only reduces the forgetting (Fn) but also improves average Hits@1 (An) by +17.3% over\ncontinual indexing.\nD0 D1 D2 D3 D4 D5\n0\n50\n100Indexing accuracy\nAverage performance (An)\nD1 D2 D3 D4 D5\n0\n50\n100\nForgetting (Fn)\nD1 D2 D3 D4 D5\n0\n50\n100\nLearning performance (LAn)\ncl(Dn)\ncl(Un)\ncl(Un)+genmem(Un) with r=1\ntrain from scratch (no cl)\nD0 D1 D2 D3 D4 D5\n0\n25\n50\n75Hits@1\nD1 D2 D3 D4 D5\n0\n25\n50\n75\nD1 D2 D3 D4 D5\n0\n25\n50\n75\nD0 D1 D2 D3 D4 D5\nTraining corpus\n0\n50\n100Hits@10\nD1 D2 D3 D4 D5\nTraining corpus\n0\n50\n100\nD1 D2 D3 D4 D5\nTraining corpus\n0\n50\n100\nFigure 8: Investigating the effectiveness of generative memory in mitigating forgetting when continuously indexing\nnew corpus Dn (T5-Base model and atomic docids representation) for the MS MARCO dataset. ↑indicates higher\nis better, ↓indicates lower is better. We observe that continual indexing of old and new documents cl(Un) helps\nto alleviate forgetting of older documents when evaluated on retrieval tasks. However, average Hits@10 ( An)\nstill undergo 25.0 points drop after sequential updates (D0 →D1 ···→ D5). Generative memory enables sparse\nreplaying of pseudo-queries for old documents and continual semi-supervised learning with new documents. We\nobserve that augmenting generative memory during continual indexing not only reduces the forgetting (Fn) but also\nimproves average Hits@10 (An) by +23.0% over considered baselines.\nfully labeled, which is not applicable in DSI++ (we\ndo not get to see queries for the new documents).\nFurthermore, Sun et al. (2020) shows that using\na parametric model underperforms episodic mem-\nory. In our work, we do not generate example pairs\n(x,y) but rather generate pseudo-queries (y), sim-\nilar to contemporary works (Zhuang et al., 2022;\nBonifacio et al., 2022). We show that our approach\n8212\noutperforms episodic memory. Lastly, in the con-\ntext of pseudo-query generation, neural models\nare prone to hallucinate additional content not sup-\nported by the input documents. Future works can\nstudy methods to filter out noisy pseudo-queries\n(Mehta et al., 2022) during incremental indexing.\nTest time adaptation approaches for contin-\nual learning use episodic memory at the inference\ntime to alter the model weights before making pre-\ndictions (Rebuffi et al., 2017; Sprechmann et al.,\n2018; de Masson D’Autume et al., 2019; Wang\net al., 2020). Updating the DSI indexer for ev-\nery user query is computationally expensive, so\nwe focus on continual learning methods during\ntraining. Apart from continual learning-focused\napproaches, retrieval augmented generation (Guu\net al., 2020; Izacard and Grave, 2021; Borgeaud\net al., 2022) family of approaches retrieve auxil-\niary passages/documents to enhance pre-trained\nlanguage models. These approaches alter test-time\npredictions of the generative models by augmenting\ntheir input with relevant passages retrieved from ex-\nternal retrievable memory. Moreover, one explicitly\ndisables the updates to the employed pre-trained\n(and retrieval) model using the external retrievable\nmemory. Such approaches do not faithfully assess\nthe fundamental challenge of learning continually,\nspecifically catastrophic forgetting. On the other\nhand, our work focuses on the recently introduced\nDSI paradigm (Tay et al., 2022), where information\nin the document corpus is encoded into the model\nparameters. Therefore, any updates to the under-\nlying corpus necessitate updating the model pa-\nrameters hence, undergoing severe forgetting. Our\nwork tackles a more challenging setup for study-\ning the forgetting phenomenon in detail. However,\nretrieval-augmented generation-based methods do\nnot analyze the forgetting phenomenon, only look-\ning at overall performance metrics. We agree that\ncontinual learning is broader than catastrophic for-\ngetting. However, in this work, we decided to study\nthe forgetting phenomenon in detail on one of the\nmost challenging setups, if not the most difficult.\nParameter isolation-based approaches for con-\ntinual learning assign different dedicated subsets\nof the model parameters to each task to prevent\nforgetting (De Lange et al., 2021). While learning\na new task, these methods either freeze a subset\nof the parameters corresponding to older tasks or\ndynamically add new parameters per new task. At\nthe prediction time, these methods typically require\ntask identity to activate the corresponding subset\nof parameters for inference. In the DSI paradigm,\nwe are given user queries at the inference time,\nand the goal is to predict relevant document iden-\ntifiers. Now during incremental indexing, if we\nconsider every new document corpus as a new task,\nthen a typical parameter isolation-based approach\nwould require corpus identity for every user query\nat the test time, defeating the whole purpose of the\nDSI paradigm. Due to this, the parameter isolation-\nbased approaches in their current form are rendered\nless useful for DSI++. Nevertheless, we believe\nthat by masking the weights for the already indexed\ncorpus, one is explicitly disabling the updates to\nthe underlying DSI model; therefore, parameter\nisolation-based methods would be robust to for-\ngetting, and future works should explore them for\nDSI++. We believe, however, that adapting these\nmethods for DSI++ is out of scope for this paper,\nand we would not be able to do both this topic\nand our current work justice in the limited space\navailable.\n8213",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7234861850738525
    },
    {
      "name": "Computer science",
      "score": 0.5834413766860962
    },
    {
      "name": "Natural language processing",
      "score": 0.5012567043304443
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3201027512550354
    },
    {
      "name": "Electrical engineering",
      "score": 0.22528228163719177
    },
    {
      "name": "Engineering",
      "score": 0.2074410319328308
    },
    {
      "name": "Voltage",
      "score": 0.047879576683044434
    }
  ]
}