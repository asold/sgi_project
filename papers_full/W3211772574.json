{
    "title": "On Pursuit of Designing Multi-modal Transformer for Video Grounding",
    "url": "https://openalex.org/W3211772574",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A1974863236",
            "name": "Cao Meng",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2107639259",
            "name": "Chen Long",
            "affiliations": [
                "Columbia University"
            ]
        },
        {
            "id": "https://openalex.org/A4221543151",
            "name": "Shou, Mike Zheng",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2099292875",
            "name": "Zhang Can",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2115955093",
            "name": "Zou Yuexian",
            "affiliations": [
                "Peng Cheng Laboratory",
                "Peking University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2998495542",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2950541952",
        "https://openalex.org/W2970898753",
        "https://openalex.org/W3174364033",
        "https://openalex.org/W3145269263",
        "https://openalex.org/W3034743747",
        "https://openalex.org/W2964232540",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3025323587",
        "https://openalex.org/W3190216403",
        "https://openalex.org/W2963662190",
        "https://openalex.org/W2948958195",
        "https://openalex.org/W1522734439",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3035640828",
        "https://openalex.org/W2963521717",
        "https://openalex.org/W3035022492",
        "https://openalex.org/W3115390238",
        "https://openalex.org/W2903901502",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3126013083",
        "https://openalex.org/W2894280539",
        "https://openalex.org/W2890502146",
        "https://openalex.org/W2798354744",
        "https://openalex.org/W4287330514",
        "https://openalex.org/W2964214371",
        "https://openalex.org/W2964089981",
        "https://openalex.org/W2963916161",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W3199858703",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2962766617",
        "https://openalex.org/W3092739351",
        "https://openalex.org/W3128723389",
        "https://openalex.org/W2964216549",
        "https://openalex.org/W3216763528",
        "https://openalex.org/W3132890542",
        "https://openalex.org/W2111078031",
        "https://openalex.org/W3212735573",
        "https://openalex.org/W3174421047",
        "https://openalex.org/W2963393391",
        "https://openalex.org/W2963095467",
        "https://openalex.org/W3214586131",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2897628926",
        "https://openalex.org/W3175082063",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3207520933",
        "https://openalex.org/W3122239467",
        "https://openalex.org/W607748843",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W3138878737",
        "https://openalex.org/W2997762001",
        "https://openalex.org/W3035339529",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2970401629",
        "https://openalex.org/W2962869524",
        "https://openalex.org/W2963017553",
        "https://openalex.org/W2997429269",
        "https://openalex.org/W3119686997"
    ],
    "abstract": "Video grounding aims to localize the temporal segment corresponding to a sentence query from an untrimmed video. Almost all existing video grounding methods fall into two frameworks: 1) Top-down model: It predefines a set of segment candidates and then conducts segment classification and regression. 2) Bottom-up model: It directly predicts frame-wise probabilities of the referential segment boundaries. However, all these methods are not end-to-end, i.e., they always rely on some time-consuming post-processing steps to refine predictions. To this end, we reformulate video grounding as a set prediction task and propose a novel end-to-end multi-modal Transformer model, dubbed as GTR. Specifically, GTR has two encoders for video and language encoding, and a cross-modal decoder for grounding prediction. To facilitate the end-to-end training, we use a Cubic Embedding layer to transform the raw videos into a set of visual tokens. To better fuse these two modalities in the decoder, we design a new Multi-head Cross-Modal Attention. The whole GTR is optimized via a Many-to-One matching loss. Furthermore, we conduct comprehensive studies to investigate different model design choices. Extensive results on three benchmarks have validated the superiority of GTR. All three typical GTR variants achieve record-breaking performance on all datasets and metrics, with several times faster inference speed. Our project is available at GTR. ¬© 2021 Association for Computational Linguistics",
    "full_text": "On Pursuit of Designing Multi-modal Transformer for Video Grounding\nMeng Cao1, Long Chen2‚àó, Mike Zheng Shou3, Can Zhang1, and Yuexian Zou1,4‚Ä†\n1School of Electronic and Computer Engineering, Peking University\n2Columbia University 3National University of Singapore 4Peng Cheng Laboratory\nmengcao@pku.edu.cn, zjuchenlong@gmail.com\nmike.zheng.shou@gmail.com, {zhangcan, zouyx}@pku.edu.cn\nAbstract\nVideo grounding aims to localize the tempo-\nral segment corresponding to a sentence query\nfrom an untrimmed video. Almost all existing\nvideo grounding methods fall into two frame-\nworks: 1) Top-down model: It predeÔ¨Ånes a set\nof segment candidates and then conducts seg-\nment classiÔ¨Åcation and regression. 2) Bottom-\nup model: It directly predicts frame-wise prob-\nabilities of the referential segment boundaries.\nHowever, all these methods are notend-to-end,\ni.e., they always rely on some time-consuming\npost-processing steps to reÔ¨Åne predictions. To\nthis end, we reformulate video grounding as a\nset prediction task and propose a novel end-to-\nend multi-modal Transformer model, dubbed\nas GTR. SpeciÔ¨Åcally, GTR has two encoders\nfor video and language encoding, and a cross-\nmodal decoder for grounding prediction. To fa-\ncilitate the end-to-end training, we use aCubic\nEmbedding layer to transform the raw videos\ninto a set of visual tokens. To better fuse these\ntwo modalities in the decoder, we design a new\nMulti-head Cross-Modal Attention. The whole\nGTR is optimized via a Many-to-One match-\ning loss. Furthermore, we conduct comprehen-\nsive studies to investigate different model de-\nsign choices. Extensive results on three bench-\nmarks have validated the superiority of GTR.\nAll three typical GTR variants achieve record-\nbreaking performance on all datasets and met-\nrics, with several times faster inference speed.\nOur project is available at GTR.\n1 Introduction\nVideo grounding is a fundamental while challeng-\ning task for video understanding and has recently\nattracted unprecedented research attention (Chen\net al., 2018, 2019b,a; Zhang et al., 2019a; Liu et al.,\n2018a; Yuan et al., 2021). Formally, it aims to iden-\ntify the two temporal boundaries of the moment of\ninterest based on an input untrimmed video and a\n‚àóWork started when Long Chen at Tencent AI Lab.\n‚Ä†Corresponding author.\n5√ófaster\n4.9%better\nFigure 1:Performance comparisons on TACoS in terms of\nR@1, IoU@0.5 and Query Per Second (the number of queries\nthat are retrieved each second during inference). Marker sizes\nare proportional to the model size. Our GTR-H is 4.9% better\nthan 2D-TAN (Zhang et al., 2020b) with 5 times faster speed.\nnatural language query. Compared to the conven-\ntional video action localization task (Shou et al.,\n2016; Zhao et al., 2017; Zhang et al., 2021), video\ngrounding is more general and taxonomy-free, i.e.,\nit is not limited by the predeÔ¨Åned action categories.\nThe overwhelming majority of the state-of-the-\nart video grounding methods fall into two frame-\nworks: 1) Top-down models (Anne Hendricks et al.,\n2017; Gao et al., 2017; Ge et al., 2019; Chen et al.,\n2018; Zhang et al., 2019b; Liu et al., 2018b; Yuan\net al., 2019a): They always use a propose-and-rank\npipeline, where they Ô¨Årst generate a set of moment\nproposals and then select the best matching one. To\navoid the proposal bottlenecks and achieve high re-\ncall, a vast number of proposals are needed. Corre-\nspondingly, some time-consuming post-processing\nsteps (e.g., non-maximum suppression, NMS) are\nintroduced to eliminate redundancy, which makes\nthe matching process inefÔ¨Åcient (cf. Figure 1). 2)\nBottom-up models (Mun et al., 2020; Rodriguez\net al., 2020; Zeng et al., 2020; Chen et al., 2020;\nLu et al., 2019): They directly regress the two tem-\nporal boundaries of the referential segment from\neach frame or predict boundary probabilities frame-\nwisely. Similarly, they need post-processing steps\nto group or aggregate all frame-wise predictions.\nAlthough these two types of methods have real-\nized impressive progress in video grounding, it is\nworth noting that they still suffer several notorious\nlimitations: 1) For top-down methods, the heuristic\nproposal generation process introduces a series of\nhyper-parameters. Meanwhile, the whole inference\nstage is computation-intensive for densely placed\ncandidates. 2) For bottom-up methods, the frame-\nwise prediction manner overlooks fruitful temporal\ncontext relationships, which strictly limits their per-\nformance. 3) All these methods are not end-to-end,\nwhich need complex post-processing steps to reÔ¨Åne\npredictions, and easily fall into the local optimum.\nIn this paper, we reformulate the video ground-\ning as a set prediction problem and propose a novel\nend-to-end multi-modal Transformer model GTR\n(video Grounding with TRansformer). GTR has\ntwo different encoders for video and language fea-\nture encoding, and a cross-modal decoder for Ô¨Å-\nnal grounding result prediction. SpeciÔ¨Åcally, we\nuse a Cubic Embedding layer to transform the raw\nvideo data into a set of visual tokens, and regard all\nword embeddings of the language query as textual\ntokens. Both these visual and textual tokens are\nthen fed into two individual Transformer encoders\nfor respective single-modal context modeling. Af-\nterwards, these contextualized visual and textual\ntokens serve as an input to the cross-modal decoder.\nOther input for the decoder is a set of learnable seg-\nment queries, and each query try to regress a video\nmoment by interacting with these contextualized\ntokens. To better fuse these two modalities in the\ndecoder, we design a new Multi-head Cross-Modal\nAttention module (MCMA). The whole GTR model\nis trained end-to-end by optimizing a Many-to-One\nMatching Loss which produces an optimal bipar-\ntite matching between predictions and ground-truth.\nThanks to this simple pipeline and the effective rela-\ntionship modeling capabilities in Transformer, our\nGTR is both effective and computationally efÔ¨Åcient\nwith extremely fast inference speed (cf. Figure 1).\nSince our community has few empirical expe-\nriences on determining the best design choice for\nmulti-modal Transformer-family models, we con-\nduct extensive exploratory studies on GTR to in-\nvestigate the inÔ¨Çuence of different model designs\nand training strategies, including: (a) Visual to-\nkens acquisition. We use a cubic embedding layer\nto transform raw videos to visual tokens, and dis-\ncuss the design of the cubic embedding layer from\nthree dimensions. (b) Multi-modal fusion mecha-\nnism. We propose six types of multi-modal fusion\nmechanisms, and compare their performance and\ncomputation cost thoroughly. (c) Decoder design\nprinciples. We explore some key design principles\nof a stronger multi-modal Transformer decoder,\nsuch as the tradeoff between depth and width, or\nthe attention head number in different layers. (d)\nTraining recipes. We discuss the inÔ¨Çuence of sev-\neral training tricks. We hope our exploration results\nand summarized take-away guidelines can help to\nopen the door for designing more effective and\nefÔ¨Åcient Transformer models in multi-modal tasks.\nIn summary, we make three contributions in this\npaper:\n1. We propose the Ô¨Årst end-to-end model GTR\nfor video grounding, which is inherently efÔ¨Å-\ncient with extremely fast inference speed.\n2. By the careful design of each component, all\nvariants of GTR achieve new state-of-the-art\nperformance on three datasets and all metrics.\n3. Most importantly, our comprehensive explo-\nrations and empirical results can help to guide\nthe design of more multi-modal Transformer-\nfamily models in other multi-modal tasks.\n2 Related Work\nVideo Grounding.The overwhelming majority of\nstate-of-the-art video grounding methods are top-\ndown models (Anne Hendricks et al., 2017; Gao\net al., 2017; Ge et al., 2019; Liu et al., 2018a, 2020;\nZhang et al., 2019a, 2020b; Chen et al., 2018; Yuan\net al., 2019a; Wang et al., 2020; Xu et al., 2019;\nXiao et al., 2021b,a; Liu et al., 2021). Although\nthese top-down models have dominated the perfor-\nmance, they suffer from two inherent limitations:\n1) The densely placed proposal candidates lead to\nheavy computation cost. 2) Their performance are\nsensitive to the heuristic rules (e.g., the number and\nthe size of anchors). Another type of methods is\nbottom-up models (Yuan et al., 2019b; Lu et al.,\n2019; Zeng et al., 2020; Chen et al., 2020, 2018;\nZhang et al., 2020a). Some works (He et al., 2019;\nWang et al., 2019) resort to Reinforcement Learn-\ning to guide the boundary prediction adjustment.\nHowever, all existing methods (both top-down and\nbottom-up) are not end-to-end and require complex\npost-processing steps. In this paper, we propose a\nend-to-end model GTR, which directly generates\npredictions with ultrafast inference speed.\nVision Transformer.Transformer (Vaswani et al.,\n2017) is a de facto standard language modeling\nPosition Embedding\nProjection &\nInput Embedding\nùêàùêØ‚àà‚Ñù\"√ó$\nùêï‚àà‚Ñù%√ó&√ó'√ó(\nQuery: She opens the drawer and takes out a cutting board and juicer.WordEmbeddingùêàùê¨‚àà‚Ñù\"√ó$!\nLanguage Encoder\n‚Ä¶\n‚Ä¶\n Cross-Modal Decoder\nSegment QueriesùëÑ‚àà‚Ñù%√ó$\nFFNFFNFFNFFNùêá\nùêí\n√óùëµùíÖ\n√óùëµùíî\n√óùëµùíÖ\nMulti-Head Self Attention\nAdd & Norm\nFeed Forward\nAdd & Norm\nAdd & Norm\nMCMA\nùëÑ ùêíùêáCross-Modal Decoder\nEncoderDecoderPrediction Heads\nscore; [start, end]\nùëò)\nùëò*\nùëò+\n Flatten\nVideo Cubic Embedding\nVideo Encoder\n‚Ä¶\n‚Ä¶√óùëµùíó\nFFN Many-to-One Matching LossSentence Embedding\nFigure 2:An overview of GTR. (1) Input Embedding transforms the video and language data to feature space. (2) Encoder is\napplied to encode global context. (3) Decoder contains a novel Multi-head Cross-Modal Attention module (MCMA) to fuse two\nmodalities. (4) Prediction Heads generate grounding results optimized by a Many-to-One Matching Loss.\narchitecture in the NLP community. Recently, a\npioneering object detection model DETR (Carion\net al., 2020) starts to formulate the object detec-\ntion task as a set prediction problem and use an\nend-to-end Transformer structure to achieve state-\nof-the-art performance. Due to its end-to-end na-\nture, DETR regains the CV community attention\nabout the Transformer, and a mass of vision Trans-\nformer models have been proposed for different\nvision understanding tasks, such as image classiÔ¨Å-\ncation (Wang et al., 2021), object detection (Carion\net al., 2020; Zhu et al., 2021), tracking (Meinhardt\net al., 2021; Xu et al., 2021; Chen et al., 2021;\nSun et al., 2020), person re-id (He et al., 2021),\nimage generation (Jiang et al., 2021), super resolu-\ntion (Yang et al., 2020), and video relation detec-\ntion (Gao et al., 2021). Unlike previous methods\nonly focusing on the vision modality, our GTR is\na multi-modal Transformer model, which not only\nneeds to consider the multi-modal fusion, but also\nhas few empirical experience for model designs.\n3 Video Grounding with TRansformer\nAs shown in Figure 2, GTR yields temporal seg-\nment predictions semantically corresponding to the\ngiven query by four consecutive steps: (1) Input\nEmbedding. Given a raw video and a query, this\nstep aims to encode them into the feature space\n(i.e., visual and textual tokens). 2) Encoder. Visual\nand textual token embeddings are enhanced with\na standard Transformer encoder by modeling the\nintra-modality correlations. (3) Cross-Modal De-\ncoder. Contextualized visual and textual token em-\nbeddings are fused by a Multi-head Cross-Modal\nAttention module (MCMA), and a set of learnable\nsegment queries are fed into the decoder to inter-\nact with these two modal features. (4) Prediction\nHeads. A simple feed-forward network (FFN) is\napplied to predict Ô¨Ånal temporal segments.\n3.1 Input Embedding and Encoder\nVideo Cubic Embedding.Aiming to build a pure\nTransformer model without the reliance on CNNs,\nViT (Dosovitskiy et al., 2021) decomposes input\nimages into a set of non-overlapping patches. To\nprocess video data, a straightforward solution is to\napply this partition for each frame. However, this\nsimple extension overlooks the frame-wise tempo-\nral correlations. Thus, we propose a Cubic Embed-\nding layer which directly extracts 3D visual tokens\nfrom the height, width, and temporal dimensions\nrespectively (cf. Figure 2).\nFormally, given a raw video, we Ô¨Årstly use fram-\nerate 1/Œ≥œÑ to sample the video, and obtain video\nclip V ‚ààRT√óH√óW√ó3. Then, we use a sampling\nkernel Œ∫with shape (kh,kw,kt) to transform the\nvideo into visual tokens, and the sampling kernel Œ∫\nis propagated with stride size s(sh,sw,st), where\nsh, sw, and st denote the stride size in the height,\nwidth, and temporal dimension respectively. Each\nsampled 3D visual patch is fed into a projection\nlayer, and the output of the cubic embedding layer\nis a set of visual token embeddings Iv ‚ààRF√ód,\nwhere F is the number of visual tokens, and dis\nthe dimension of the projected layer. In our experi-\nments, we set dto the same hidden dimension of\nthe Transformer. Apparently, F = Oh √óOw √óOt\nwhere Oh =\n‚åä\nH‚àíkh\nsh\n+ 1\n‚åã\n, Ow =\n‚åä\nW‚àíkw\nsw + 1\n‚åã\n,\nand Ot =\n‚åä\nT‚àíkt\nst + 1\n‚åã\n. Compared to the non-\noverlapping tokenization manners, our cubic em-\nbedding layer allows overlapping sampling, which\nimplicitly fuses adjacent spatial-temporal context\n(More experiments and discussions about the cu-\nbic embedding layer are shown in Sec. 4.2). In\nJoint Fusion\nMatMul\n!\"\nSoftMax\nMatMul\n[#!,%!] [#\",%\"] DividedFusion\nMatMulSoftMaxMatMul!\"\nSoftMaxMatMul!\"\nMatMulWeighted Add.\n#!#\" %!%\" HybridFusion\nConcatenation\nMatMulSoftMaxMatMulSoftMax\nMatMul\n!\" !\"#\" %\"[#!,%!] StepwiseFusionMatMulSoftMax\n!\"\nMatMulMatMulSoftMaxMatMul\n%\"#\" #! %!\nFigure 3:Different instantiations of the Multi-head Cross-Modal Attention module (MCMA).\nparticular, when setting sh = kh, sw = kw, and\nst = kt = 1, our cubic embedding degrades into\nthe prevalent patch embedding in ViT.\nSentence Embedding.For the language input, we\nÔ¨Årst encode each word with pretrained GloVe em-\nbeddings (Pennington et al., 2014), and then em-\nploy a Bi-directional GRU to integrate the sentence-\nlevel embedding feature Is ‚ààRS√óds, where Srep-\nresents the length of the sentence query, and ds is\nthe dimension of textual token embeddings.\nEncoder. We use two plain Transformer encoders\nto model the visual and textual intra-modality con-\ntext, respectively. SpeciÔ¨Åcally, for the video token\nembeddings Iv, we apply an encoder with Nv lay-\ners to obtain their corresponding contextualized\nvisual tokens H ‚ààRF√ód. Similarly, for the tex-\ntual token embeddings Is, we use an encoder with\nNs layers to obtain contextualized textual tokens\nS. Another feed-forward network with two fully-\nconnected layers is applied to adjust S to be the\nsame channel with H, i.e., S ‚ààRS√ód.\n3.2 Cross-modal Decoder\nAs shown in Figure 2, the inputs for cross-modal\ndecoder consist of the visual features H ‚ààRF√ód,\nlanguage features S ‚ààRS√ód, and a set of learn-\nable segment queries Q ‚ààRN√ód. Each segment\nquery qi ‚ààQ tries to learn a possible moment by\ninteracting with H and S, and the whole decoder\nwill decode N moment predictions in parallel. For\naccurate segment localization, video grounding re-\nquires modeling Ô¨Åne-grained cross-modal relations.\nTo this end, we design a cross-modal decoder with\na novel Multi-head Cross-Modal Attention mod-\nule (MCMA). As shown in Figure 3, we propose\nseveral speciÔ¨Åc instantiations of MCMA1:\nJoint Fusion.Given visual and language features\nH and S, we Ô¨Årstly generate a set of modal-speciÔ¨Åc\nkey ( Hk, Sk) and value ( Hv, Sv) pairs by lin-\n1More details are left in the supplementary materials.\near transformations: Hk = HWk\nh, Sk = SWk\ns,\nHv = HWv\nh, Sv = SWv\ns, where Wk\nh, Wk\ns,\nWv\nh and Wv\ns ‚ààRd√ód are all learnable parameters.\nThen joint fusion concatenates the two modalities\nbefore conducting the attention computing:\nYjoint = MHA( ÀÜQ,Hk ‚äóSk,Hv ‚äóSv),\nwhere ÀÜQ is the enhanced segment query embed-\ndings after the self-attention and ‚äódenotes the\nchannel concatenation. MHA stands for the stan-\ndard Multi-head Attention (Vaswani et al., 2017).\nDivided Fusion. We provide a modality-speciÔ¨Åc\nattention computation manner, i.e., Divided Fusion\ndecomposes the multi-modal fusion into two par-\nallel branches and the Ô¨Ånal results are summed up\nwith learnable weights.\nYdivided =MHA( ÀÜQ,Hk,Hv)‚äïMHA( ÀÜQ,Sk,Sv),\nwhere ÀÜQ, Hk, Hv, Sk and Sv are deÔ¨Åned the same\nas in the joint fusion. ‚äïdenotes the additive sum\nwith learnable weights.\nHybrid Fusion. Hybrid Fusion offers a compro-\nmise between Joint Fusion and Divided Fusion.\nSpeciÔ¨Åcally, the query-key multiplication is con-\nducted separately while the query-value multipli-\ncation is still in an concatenation format. Suppose\nthere are nh self-attention heads. The query, key\nand value embeddings are uniformly split into nh\nsegments ÀÜQi ‚ààRN√ódh, Hk\ni, Hv\ni ‚ààRF√ódh, Sk\ni,\nSv\ni ‚ààRS√ódh, {i = 1,2,...,n h}along channel\ndimension, where dh is the dimension of each head\nand equal to d/nh. For each head, we apply hybrid\nfusion in the form:\nheadi =\n(\nœÉ (\nÀÜQiHk‚ä§\ni‚àödv\n) ‚äóœÉ (\nÀÜQiSk‚ä§\ni‚àödv\n)\n)\n(Hv\ni ‚äóSv\ni ) ,\nwhere œÉ is the softmax function. The outputs of\nall heads are then again concatenated along the\nchannel dimension and a linear projection is Ô¨Ånally\napplied to produce the Ô¨Ånal output as follows:\nYhybrid = (head0 ‚äóhead1 ‚äó... headnh‚àí1) WO,\nwhere WO ‚ààRd√ód is linear projection parameters.\nStepwise Fusion.This fusion manner implements\nthe cross-modality reasoning in a cascaded way,\ni.e., attention computation is performed between\nÀÜQ and video features and then propagated to the\nsentence modality:\nYstep = MHA(MHA( ÀÜQ,Hk,Hv),Sk,Sv).\nWe further discuss more multi-modal fusion mech-\nanisms in Sec. 4.2 and supplementary materials.\n3.3 Many-to-One Matching Loss\nTraining: Based on the Cross-Modality Decoder\noutput, a feed forward network is applied to gen-\nerate a Ô¨Åxed length predictions ÀÜY = {ÀÜyi}N\ni=1,\nwhere ÀÜyi = (ÀÜbi; ÀÜci) contains temporal segment\npredictions ÀÜbi ‚àà[0,1]2 and the conÔ¨Ådence score\nÀÜci ‚àà[0,1]. The ground truth is denoted asY, which\ncontains the segment coordinate b ‚àà[0,1]2.\nGTR applies set prediction loss (Carion et al.,\n2020; Stewart et al., 2016) between the Ô¨Åxed-size\noutput sets and ground-truth. Notably, considering\neach language query only corresponds to one tem-\nporal segment, we adapt the many-to-many match-\ning in (Carion et al., 2020) to the many-to-one ver-\nsion. SpeciÔ¨Åcally, the loss computation is con-\nducted in two consecutive steps. Firstly, we need\nto determine the optimum prediction slot via the\nmatching cost based on the bounding box similarity\nand conÔ¨Ådence scores as follows:\ni‚àó= arg min\ni‚àà[0,N‚àí1]\nCmatch\n(\nÀÜY,Y\n)\n= arg min\ni‚àà[0,N‚àí1]\n[\n‚àíÀÜci + Cbox(b,ÀÜbi)\n]\n.\n(1)\nIn our many-to-one matching loss, the optimal\nmatch requires only one iteration of N generated\nresults, rather than checking all possible permu-\ntations as in (Carion et al., 2020), which greatly\nsimpliÔ¨Åes the matching process. For Cbox(¬∑,¬∑), we\ndeÔ¨Åne Cbox = Œª‚Ñì1 ‚à•b‚àíÀÜbi‚à•1 + ŒªiouCiou(b,ÀÜbi) with\nweighting parameters Œª‚Ñì1 , Œªiou ‚ààR. Here Ciou(¬∑,¬∑)\nis a scale-invariant generalized intersection over\nunion in (RezatoÔ¨Åghi et al., 2019). Then the sec-\nond step is to compute the loss function between\nthe matched pair:\nLset(y,ÀÜy) =‚àílog ÀÜci‚àó+ Cbox(b,ÀÜbi‚àó), (2)\nwhere i‚àóis the optimal match computed in Eq. (1).\nInference: During inference, the predicted seg-\nment set is generated in one forward pass. Then\nthe result with the highest conÔ¨Ådence score is se-\nlected as the Ô¨Ånal prediction. The whole inference\nprocess requires no predeÔ¨Åned threshold values or\nspeciÔ¨Åc post-processing processes.\n4 Experiments\nWe Ô¨Årst introduce experimental settings in Sec. 4.1.\nThen, we present detailed exploratory studies on\nthe design of GTR in Sec. 4.2. The comparisons\nwith SOTA methods are discussed in Sec. 4.3, and\nwe show visualization results in Sec. 4.4. More\nresults are left in supplementary materials.\n4.1 Settings\nDatasets. We evaluated our GTR on three challeng-\ning video grounding benchmarks: 1) ActivityNet\nCaptions (ANet)(Krishna et al., 2017): The aver-\nage video length is around 2 minutes, and the av-\nerage length of ground-truth video moments is 40\nseconds. By convention, 37,417 video-query pairs\nfor training, 17,505 pairs for validation, and 17,031\npairs for testing. 2) Charades-STA (Gao et al.,\n2017): The average length of each video is around\n30 seconds. Following the ofÔ¨Åcial splits, 12,408\nvideo-query pairs for training, and 3,720 pairs for\ntesting. 3) TACoS (Regneri et al., 2013): It is a\nchallenging dataset focusing on cooking scenarios.\nFollowing previous works (Gao et al., 2017), we\nused 10,146 video-query pairs for training, 4,589\npairs for validation, and 4,083 pairs for testing.\nEvaluation Metrics. Following prior works, we\nadopt ‚ÄúR@n, IoU@m‚Äù (denoted asRm\nn ) as the met-\nrics. SpeciÔ¨Åcally, Rm\nn is deÔ¨Åned as the percentage\nof at least one of top-n retrieved moments having\nIoU with the ground-truth moment larger than m.\nModal Variants.Following the practice of Visual\nTransformers or BERTs (Dosovitskiy et al., 2021;\nDevlin et al., 2019), we also evaluate three typical\nmodel sizes: GTR-Base (GTR-B, Nv = 4, Ns =\n4, Nd = 6, d= 320), GTR-Large (GTR-L, Nv =\n6, Ns = 6, Nd = 8, d = 320), and GTR-Huge\n(GTR-H, Nv = 8, Ns = 8, Nd = 8, d= 512).1\nImplementation Details.For input video, the sam-\npling rate 1/Œ≥œÑ was set to 1/8, all the frames were\nresized to 112 √ó112, the kernel shape and stride\nsize were set to (8, 8, 3) and (8, 8, 2), respectively.\nWe used AdamW (Loshchilov and Hutter, 2017)\nwith momentum of 0.9 as the optimizer. The initial\nlearning rate and weight decay were set to 10‚àí4.\nAll weights of the encoders and decoders were ini-\ntialized with Xavier init, and the cubic embedding\nModels R0.5\n1 R0.7\n1 GFLOPs\nGTR-B/8 49.67 28.45 20.55\nGTR-B/12 44.32 24.62 8.71\nGTR-B/16 43.14 23.93 5.44\nGTR-B/24 42.01 23.82 1.96\n(a) Spatial ConÔ¨Åguration\nkt R0.5\n1 R0.7\n1 GFLOPs\n3 49.67 28.45 20.55\n5 47.34 26.12 17.83\n7 47.87 26.91 15.03\n9 48.04 27.02 13.91\n(b) Temporal ConÔ¨Åguration\nModels Stride R0.5\n1 R0.7\n1 GFLOPs\nTemporal (8, 8, 1) 49.73 28.51 41.05\n(8, 8, 2) 49.67 28.45 20.55\nSpatial (4, 4, 3) 44.54 23.12 54.51\n(6, 6, 3) 44.23 22.81 24.26\nNone (8, 8, 3) 43.73 22.45 14.69\n(c) Overlapping Exploration\nTable 1:Input ablations for the GTR-B model2 on ANet dataset (%).\nModels R0.5\n1 R0.7\n1 GFLOPs\nFramewise 46.10 26.27 46.36\nCubic 49.67 28.45 20.55\nTable 2:Cubic vs. Framewise Embedding on ANet(%).\nModels R0.5\n1 R0.7\n1 Param(M) GFLOPs\nEarly Fusion 39.35 19.64 12.86 11.31\nConditional Fusion 35.25 14.57 12.42 10.60\nJoint Fusion 42.51 21.53 16.62 13.91\nDivided Fusion 46.91 26.21 25.16 21.02\nHybrid Fusion 46.82 25.45 20.94 17.25\nStepwise Fusion 49.67 28.45 25.98 20.55\nTable 3:Multi-modal fusion comparisons on ANet.(%).\nlayer was initialized from the ImageNet-pretrained\nViT (Dosovitskiy et al., 2021). We used random\nÔ¨Çip, random crop, and color jitter for video data\naugmentation. Experiments were conducted on 16\nV100 GPUs with batch size 64.\n4.2 Empirical Studies and Observations\nIn this subsection, we conducted extensive studies\non different design choices of GTR, and tried to\nanswer four general questions: Q1: How to trans-\nform a raw video into visual tokens? Q2: How to\nfuse the video and text features? Q3: Are there any\ndesign principles to make a stronger Transformer\ndecoder? Q4: Are there any good training recipes?\n4.2.1 Visual Tokens Acquisition (Q1)\nSettings. In cubic embedding layer, there are two\nsets of hyper-parameters (kernel shape (kw,kh,kt)\nand stride size (sw,sh,st)). We started our studies\nfrom a basic GTR-B model2, and discussed design\nchoices from three aspects: 1) Spatial conÔ¨Ågura-\ntion. We compared four GTR-B variants with dif-\nferent kernel spatial size kw,kh = {8,12,16,24},\nand denoted these models as GTR-B/*. Results are\nreported in Table 1 (a). 2) Temporal conÔ¨Åguration.\nWe compared four GTR-B variants with different\nkernel temporal depths kt = {3,5,7,9}. Results\nare reported in Table 1 (b). 3) Overlapping explo-\nration. We conducted experiments with several dif-\nferent stride sizes (sw,sh,st) in Table 1 (c), which\n2The baseline GTR-B is with the stepwise fusion strategy,\nand with sw = kw = sh = kh = 8, kt = 3, st = 2.\nFigure 4: Top:R0.5\n1 v.s. Frame Crop Size & Sample Rate;\nBottom: FLOPs v.s. Frame Crop Size & Sample Rate.\ncorresponds to three basic types (temporal overlap-\nping, spatial overlapping, and non-overlapping).\nObservations. 1) Models with smaller patch size\n(e.g., GTR-B/8) achieve better performance yet at\nthe cost of the dramatic increase of FLOPs. 2)\nModels with larger kernel temporal depth (kt) will\nnot always achieve better results. It is worth not-\ning that our cubic embedding will degrade into the\nprevalent framewise partition in vision Transform-\ners by setting kt = st = 1. We further compared\ncubic embedding with this special case in Table 2,\nand the results show the superiority of our cubic\nembedding layer. 3) Compared to non-overlapping\nand spatial overlapping, temporal overlapping can\nhelp to improve model performance signiÔ¨Åcantly.\nBesides, the performance is not sensitive to the\noverlapping degree in all overlapping cases.\nGuides. The temporal overlapping sampling strat-\negy can greatly boost the performance of the Cubic\nEmbedding layer at an affordable overhead.\n4.2.2 Multi-modal Fusion Mechanisms (Q2)\nSettings. As mentioned in Sec. 3.2, we design four\ntypes of multi-modal fusion mechanism in the de-\ncoder (i.e., joint/divided/hybrid/stepwise fusion),\nand we group all these fusion mechanisms as late\nfusion. Meanwhile, we also propose two additional\nfusion mechanisms1: 1) Early fusion: The multi-\nmodal features are fused before being fed into the\ndecoder. 2) Conditional fusion: The language fea-\ntures act as conditional signals of segment queries\nof the decoder. Results are reported in Table 3.\nObservations. 1) All four late fusion models out-\nModels ActivityNet Captions Charades-STA TACoS ParamR0.5\n1 R0.7\n1 R0.5\n5 R0.7\n5 Feat. R0.5\n1 R0.7\n1 R0.5\n5 R0.7\n5 R0.3\n1 R0.5\n1 R0.3\n5 R0.5\n5 QPS\n2D-TAN 44.51 26.54 77.13 61.96 VGG 39.81 23.25 79.33 52.15 37.29 25.32 57.81 45.04 2.36 93.37\nDRN 45.45 24.36 77.97 50.30 I3D 53.09 31.75 89.06 60.05 ‚Äî 23.17 ‚Äî 33.36 1.82 108.34\nSCDM 36.75 19.86 64.99 41.53 I3D 54.44 33.43 74.43 58.08 26.11 21.17 40.16 32.18 1.28 93.82\nQSPN 33.26 13.43 62.39 40.78 C3D 35.60 15.80 79.40 45.40 20.15 15.23 36.72 25.30 1.14 95.03\nCBP 35.76 17.80 65.89 46.20 C3D 36.80 18.87 70.94 50.19 27.31 24.79 43.64 37.40 1.92 98.55\n2D-TAN* 45.21 27.35 77.60 62.32 VGG 40.32 23.63 80.22 52.57 37.89 25.99 58.23 45.21 2.36 93.37\nDRN* 46.34 24.92 78.12 50.93 I3D 53.82 32.34 89.74 60.23 ‚Äî 23.84 ‚Äî 33.91 1.82 108.34\nGTR-B (Ours) 49.67 28.45 79.83 64.34 ‚Äî 62.45 39.23 91.40 61.76 39.34 28.34 60.85 46.67 13.4 25.98\nGTR-L (Ours) 50.43 28.91 80.22 64.95 ‚Äî 62.51 39.56 91.62 61.97 39.93 29.21 61.22 47.10 12.7 40.56\nGTR-H (Ours) 50.57 29.11 80.43 65.14 ‚Äî 62.58 39.68 91.62 62.03 40.39 30.22 61.94 47.73 11.8 61.35\nTable 4:Performance comparisons on three benchmarks(%). All the reported results on ANet and TACoS datasets are based on\nC3D (Tran et al., 2015) extracted feature. ‚Äú*‚Äù denotes Ô¨Ånetuning on corresponding backbones. For pair comparisons, Parma (M)\nincludes the parameter of feature extractor (C3D). GTR-B is more efÔ¨Åcient while GTR-H achieves the highest recall.\nperform the early fusion and conditional fusion\nones. 2) Among late fusion models, the stepwise\nfusion model achieves the best performance by us-\ning the largest number of parameters. 3) The per-\nformance is not sensitive to the crop size, but is pos-\nitively correlated with sample rate and converges\ngradually(cf. Figure 4). 4) We Ô¨Ånd that the FLOPs\ndifference does not increase signiÔ¨Åcantly regarding\nthe crop size and sample rate (cf. Figure 4).\nGuides. Stepwise fusion is the optimal fusion mech-\nanism, even for long and high-resolution videos.\n4.2.3 Decoder Design Principles (Q3)\nSettings. To explore the key design principles of\na stronger multi-modal Transformer decoder, we\nconsidered two aspects: 1) Deeper vs. Wider, i.e.,\nwhether the decoder should go deeper or wider?\nBased on the basic GTR-B model 2, we designed\ntwo GTR-B variants with nearly equivalent param-\neters: GTR-B-Wide (d= 352) and GTR-B-Deep\n(Nd = 8)1. The results are reported in Table 5 (a).\n2) Columnar vs. Shrink vs. Expand. , i.e., how to\ndesign the attention head number in different lay-\ners. We tried three different choices: i) the same\nnumber of heads in all layers (columnar); ii) grad-\nually decrease the number of heads (shrink); iii)\ngradually increase the number of heads (expand).\nResults are reported in Table 5 (b).\nObservations. 1) Under the constraint of similar\nparameters, the deep model (GTR-B-Deep) out-\nperforms the wide counterpart (GTR-B-Wide). 2)\nThe model with the expanding strategy (GTR-B-\nExpand) achieves the best performance while the\nshrinking one (GTR-B-Shrink) is the worst.\nGuides. Going deeper is more effective than going\nwider and progressively expanding the attention\nheads leads to better performance.\nModels Param(M) ActivityNet TACoS\nR0.5\n1 R0.7\n1 R0.3\n1 R0.5\n1\nGTR-B-Wide 34.05 49.94 28.62 39.52 28.70\nGTR-B-Deep 33.89 50.15 28.74 39.60 29.04\n(a) Wide vs. Deep.\nModels Heads R0.3\n1 R0.5\n1\nGTR-B-Columnar [4, 4, 4, 4, 4, 4] 49.42 28.34\nGTR-B-Shrink [8, 8, 4, 4, 2, 1] 49.01 27.95\nGTR-B-Expand [1, 2, 4, 4, 8, 8] 49.67 28.45\n(b) Columnar vs. Shrink vs. Expand on ANet.\nTable 5: (a)Performance (%) comparisons between the wide\nvariant and deep variant. (b) Performance (%) comparisons\nbetween different attention head number choices.\n4.2.4 Training Recipes (Q4)\nSettings. Due to the lack of inductive biases, vi-\nsual Transformers always over-rely on large-scale\ndatasets for training (Dosovitskiy et al., 2021; Tou-\nvron et al., 2020). To make multi-modal Transform-\ners work on relatively small multi-modal datasets,\nwe discussed several common training tricks: 1)\nPretrained weights. Following the idea of pretrain-\nthen-Ô¨Ånetune paradigm in CNN-based models3, we\ninitialize our video cubic embedding layer from the\npretrained ViT. SpeciÔ¨Åcally, we initialized our 3D\nlinear projection Ô¨Ålters by replicating the 2D Ô¨Ålters\nalong the temporal dimension. 2) Data augmen-\ntations. To study the impact of different data aug-\nmentation strategies, we apply three typical choices\nsequentially, i.e., random Ô¨Çip, random crop, and\ncolor jitter. Results are reported in Table 6.\nObservations. 1) Using the pretrained cubic em-\nbedding weights can help to improve model perfor-\nmance signiÔ¨Åcantly. 2) Color jitter brings the most\nperformance gains among all visual data augmenta-\ntion strategies. We conjecture that this may be due\nto the fact that color jitter can change the visual\nappearance without changing the structured infor-\n3Modern CNN models always rely on pretrained weights\nfrom large-scale datasets as initialization (e.g., ImageNet), and\nthen Ô¨Ånetune the weights on speciÔ¨Åc downstream tasks.\nModels R0.51 R0.71\nBaseline 41.50 20.13\n+ Pretrained Weight47.12+5.62 26.03+5.90\n+ Random Ô¨Çip 47.59+0.47 26.69+0.66\n+ Random crop 47.61+0.02 26.81+0.12\n+ Color Jitter 49.67+2.06 28.45+1.64\nTable 6:Results of different training tricks\nof the baseline GTR-B model2 on the ANet\ndataset.\n10-th\n20-th\n30-th 40-th\n50-th Figure 5:A visualization example of the self-attention in the video encoder.\nGround TruthPrediction34.51s 65.98s66.32s33.79s\nQuery: The lady puts contact lenses from her eye balls.\n#1#2 #3\nFigure 6: Top:A visualization example of the cross-attention\nin the decoder. Bottom: Visualization of the sentence atten-\ntion weights.\nmation, which is critical for multi-modal tasks.\nGuides. Using pretrained embedding weights and\ncolor jitter video augmentation are two important\ntraining tricks for multi-modal Transformers.\n4.3 Comparisons with State-of-the-Arts\nSettings. We compared three variants of GTR (i.e.,\nGTR-B, GTR-L, and GTR-H) to state-of-the-art\nvideo grounding models: 2D-TAN (Zhang et al.,\n2020b), DRN (Zeng et al., 2020), SCDM (Yuan\net al., 2019a), QSPN (Xu et al., 2019),CBP (Wang\net al., 2020). These video grounding methods were\nbased on pre-extracted video features (e.g., C3D or\nI3D) while our GTRs were trained in an end-to-end\nmanner. For more fair comparisons, we selected\ntwo best performers (2D-TAN and DRN), and re-\ntrained them by Ô¨Ånetuning the feature extraction\nnetwork. Results are reported in Table 41.\nResults. From Table 4, we have the following ob-\nservations: 1) All three GTR variants outperform\nexisting state-of-the-art methods on all benchmarks\nand evaluation metrics. Particularly, the GTR-H\nachieves signiÔ¨Åcant performance gains on more\nstrict metrics (e.g., 6.25% and 4.23% absolute per-\nformance differences on Charades-STA with metric\nR0.7\n1 and TACoS with metricR0.5\n1 , respectively.) 2)\nThe inference speed of all GTR variants are much\nfaster than existing methods ( e.g., 13.4 QPS in\nGTR-B vs. 2.36 QPS in 2D-TAN). Meanwhile, our\nGTR has fewer parameters than existing methods\n(e.g., 25.98M in GTR-B vs. 93.37M in 2D-TAN).\n4.4 Visualizations\nSelf-Attention in Video Encoder.We showed an\nexample from TACoS dataset in Figure 5. For a\ngiven video snippet ( e.g., the 30- th frame4), we\nselected two reference patches4, and visualized the\nattention weight heatmaps of the last self-attention\nlayer on other four frames (i.e., the 10-th frame to\n50-th frame). From Figure 5, we can observe that\nthe self-attention in the video encoder can effec-\ntively focus more on the semantically correspond-\ning areas ( e.g., the person and chopping board)\neven across long temporal ranges, which is beneÔ¨Å-\ncial for encoding global context.\nCross-attention in Decoder. An example from\nANet dataset was presented in Figure 6. We took\nthe stepwise fusion strategy for MCMA in the de-\ncoder. For the output, we selected the segment\nquery slot which outputs the highest conÔ¨Ådence\nscores and visualized its attention weights on two\nmodal features. For the background video frames4\n(#1), the decoder mainly focuses on Ô¨Ågure outline.\nFor ground-truth video frames4 (#2, #3), it captures\nthe most informative parts (e.g., moving hands and\ntouching eye action), which plays an important role\nin location reasoning. As for the language atten-\ntion weights, it focuses on essential words, e.g., the\nsalient objects (lady, eye) and actions (put).\n5 Conclusions and Future Works\nIn this paper, we propose the Ô¨Årst end-to-end multi-\nmodal Transformer-family model GTR for video\ngrounding. By carefully designing several novel\ncomponents (e.g., cubic embedding layer, multi-\nhead cross-modal attention module, and many-to-\none matching loss), our GTR achieves new state-\nof-the-art performance on three challenging bench-\nmarks. As a pioneering multi-modal Transformer\n4We slightly abuse \"frame\" and \"patch\" here, and we refer\nto them as a video snippet and a video cube, respectively.\nmodel, we also conducted comprehensive explo-\nrations to summarize several empirical guidelines\nfor model designs, which can help to open doors for\nthe future research on multi-modal Transformers.\nMoving forward, we aim to extend GTR to more\ngeneral settings, such as weakly-supervised video\ngrounding or spatial-temporal video grounding.\n6 Acknowledgement\nThis paper was partially supported by National\nEngineering Laboratory for Video Technology-\nShenzhen Division, and Shenzhen Municipal De-\nvelopment and Reform Commission (Disciplinary\nDevelopment Program for Data Science and In-\ntelligent Computing). It is also supported by\nNational Natural Science Foundation of China\n(NSFC 6217021843). Special acknowledgements\nare given to AOTO-PKUSZ Joint Research Center\nof ArtiÔ¨Åcial Intelligence on Scene Cognition tech-\nnology Innovation for its support. Mike Shou does\nnot receive any funding for this work.\nReferences\nLisa Anne Hendricks, Oliver Wang, Eli Shechtman,\nJosef Sivic, Trevor Darrell, and Bryan Russell. 2017.\nLocalizing moments in video with natural language.\nIn ICCV, pages 5803‚Äì5812.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve,\nNicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. 2020. End-to-end object detection with\ntransformers. In ECCV, pages 213‚Äì229.\nJingyuan Chen, Xinpeng Chen, Lin Ma, Zequn Jie, and\nTat-Seng Chua. 2018. Temporally grounding natural\nsentence in video. In EMNLP, pages 162‚Äì171.\nJingyuan Chen, Lin Ma, Xinpeng Chen, Zequn Jie, and\nJiebo Luo. 2019a. Localizing natural language in\nvideos. In AAAI, pages 8175‚Äì8182.\nLong Chen, Chujie Lu, Siliang Tang, Jun Xiao, Dong\nZhang, Chilie Tan, and Xiaolin Li. 2020. Rethink-\ning the bottom-up framework for query-based video\nlocalization. In AAAI, pages 10551‚Äì10558.\nXin Chen, Bin Yan, Jiawen Zhu, Dong Wang, Xiaoyun\nYang, and Huchuan Lu. 2021. Transformer tracking.\nIn CVPR.\nZhenfang Chen, Lin Ma, Wenhan Luo, and Kwan-\nYee K Wong. 2019b. Weakly-supervised spatio-\ntemporally grounding natural sentence in video. In\nACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. 2021.\nAn image is worth 16x16 words: Transformers for\nimage recognition at scale. In ICLR.\nJiyang Gao, Chen Sun, Zhenheng Yang, and Ram Neva-\ntia. 2017. Tall: Temporal activity localization via\nlanguage query. In ICCV, pages 5267‚Äì5275.\nKaifeng Gao, Long Chen, Yifeng Huang, and Jun Xiao.\n2021. Video relation detection via tracklet based vi-\nsual transformer. In ACM MM.\nRunzhou Ge, Jiyang Gao, Kan Chen, and Ram Nevatia.\n2019. Mac: Mining activity concepts for language-\nbased temporal localization. In WACV, pages 245‚Äì\n253.\nDongliang He, Xiang Zhao, Jizhou Huang, Fu Li, Xiao\nLiu, and Shilei Wen. 2019. Read, watch, and move:\nReinforcement learning for temporally grounding\nnatural language descriptions in videos. In AAAI,\npages 8393‚Äì8400.\nShuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang. 2021. Transreid: Transformer-based\nobject re-identiÔ¨Åcation. arXiv.\nYifan Jiang, Shiyu Chang, and Zhangyang Wang. 2021.\nTransgan: Two transformers can make one strong\ngan. arXiv.\nRanjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei,\nand Juan Carlos Niebles. 2017. Dense-captioning\nevents in videos. In ICCV, pages 706‚Äì715.\nDaizong Liu, Xiaoye Qu, Jianfeng Dong, Pan Zhou,\nYu Cheng, Wei Wei, Zichuan Xu, and Yulai Xie.\n2021. Context-aware biafÔ¨Åne localizing network for\ntemporal sentence grounding. arXiv.\nDaizong Liu, Xiaoye Qu, Xiao-Yang Liu, Jianfeng\nDong, Pan Zhou, and Zichuan Xu. 2020. Jointly\ncross-and self-modal graph attention network for\nquery-based moment localization. In ACM MM ,\npages 4070‚Äì4078.\nMeng Liu, Xiang Wang, Liqiang Nie, Xiangnan He,\nBaoquan Chen, and Tat-Seng Chua. 2018a. Atten-\ntive moment retrieval in videos. In SIGIR, pages 15‚Äì\n24.\nMeng Liu, Xiang Wang, Liqiang Nie, Qi Tian, Bao-\nquan Chen, and Tat-Seng Chua. 2018b. Cross-\nmodal moment localization in videos. In ACM MM,\npages 843‚Äì851.\nIlya Loshchilov and Frank Hutter. 2017. Decoupled\nweight decay regularization. arXiv.\nChujie Lu, Long Chen, Chilie Tan, Xiaolin Li, and Jun\nXiao. 2019. Debug: A dense bottom-up grounding\napproach for natural language video localization. In\nEMNLP, pages 5147‚Äì5156.\nTim Meinhardt, Alexander Kirillov, Laura Leal-Taixe,\nand Christoph Feichtenhofer. 2021. Trackformer:\nMulti-object tracking with transformers. arXiv.\nJonghwan Mun, Minsu Cho, and Bohyung Han. 2020.\nLocal-global video-text interactions for temporal\ngrounding. In CVPR, pages 10810‚Äì10819.\nJeffrey Pennington, Richard Socher, and Christopher D\nManning. 2014. Glove: Global vectors for word rep-\nresentation. In EMNLP, pages 1532‚Äì1543.\nMichaela Regneri, Marcus Rohrbach, Dominikus Wet-\nzel, Stefan Thater, Bernt Schiele, and Manfred\nPinkal. 2013. Grounding action descriptions in\nvideos. TACL, pages 25‚Äì36.\nHamid RezatoÔ¨Åghi, Nathan Tsoi, JunYoung Gwak,\nAmir Sadeghian, Ian Reid, and Silvio Savarese.\n2019. Generalized intersection over union: A met-\nric and a loss for bounding box regression. InCVPR,\npages 658‚Äì666.\nCristian Rodriguez, Edison Marrese-Taylor, Fate-\nmeh Sadat Saleh, Hongdong Li, and Stephen Gould.\n2020. Proposal-free temporal moment localization\nof a natural-language query in video using guided\nattention. In WACV, pages 2464‚Äì2473.\nZheng Shou, Dongang Wang, and Shih-Fu Chang.\n2016. Temporal action localization in untrimmed\nvideos via multi-stage cnns. In CVPR, pages 1049‚Äì\n1058.\nRussell Stewart, Mykhaylo Andriluka, and Andrew Y\nNg. 2016. End-to-end people detection in crowded\nscenes. In CVPR, pages 2325‚Äì2333.\nPeize Sun, Yi Jiang, Rufeng Zhang, Enze Xie, Jinkun\nCao, Xinting Hu, Tao Kong, Zehuan Yuan, Changhu\nWang, and Ping Luo. 2020. Transtrack: Multiple-\nobject tracking with transformer. arXiv.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Herv√© J√©-\ngou. 2020. Training data-efÔ¨Åcient image transform-\ners & distillation through attention. arXiv.\nDu Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Tor-\nresani, and Manohar Paluri. 2015. Learning spa-\ntiotemporal features with 3d convolutional networks.\nIn ICCV, pages 4489‚Äì4497.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NeurIPS.\nJingwen Wang, Lin Ma, and Wenhao Jiang. 2020. Tem-\nporally grounding language queries in videos by con-\ntextual boundary-aware prediction. In AAAI, pages\n12168‚Äì12175.\nWeining Wang, Yan Huang, and Liang Wang. 2019.\nLanguage-driven temporal activity localization: A\nsemantic matching reinforcement learning model.\nIn CVPR, pages 334‚Äì343.\nWenxiao Wang, Lu Yao, Long Chen, Deng Cai, Xi-\naofei He, and Wei Liu. 2021. Crossformer: A versa-\ntile vision transformer based on cross-scale attention.\narXiv.\nShaoning Xiao, Long Chen, Jian Shao, Zhuang Yuet-\ning, and Jun Xiao. 2021a. Natural language video\nlocalization with learnable moment proposals. In\nEMNLP.\nShaoning Xiao, Long Chen, Songyang Zhang, Wei Ji,\nJian Shao, Lu Ye, and Jun Xiao. 2021b. Bound-\nary proposal network for two-stage natural language\nvideo localization. In AAAI.\nHuijuan Xu, Kun He, Bryan A Plummer, Leonid Si-\ngal, Stan Sclaroff, and Kate Saenko. 2019. Multi-\nlevel language and vision integration for text-to-clip\nretrieval. In AAAI, pages 9062‚Äì9069.\nYihong Xu, Yutong Ban, Guillaume Delorme, Chuang\nGan, Daniela Rus, and Xavier Alameda-Pineda.\n2021. Transcenter: Transformers with dense queries\nfor multiple-object tracking. arXiv.\nFuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and\nBaining Guo. 2020. Learning texture transformer\nnetwork for image super-resolution. InCVPR, pages\n5791‚Äì5800.\nYitian Yuan, Xiaohan Lan, Xin Wang, Long Chen, Zhi\nWang, and Wenwu Zhu. 2021. A closer look at tem-\nporal sentence grounding in videos: Datasets and\nmetrics. arXiv.\nYitian Yuan, Lin Ma, Jingwen Wang, Wei Liu, and\nWenwu Zhu. 2019a. Semantic conditioned dy-\nnamic modulation for temporal sentence grounding\nin videos. In NeurIPS.\nYitian Yuan, Tao Mei, and Wenwu Zhu. 2019b. To Ô¨Ånd\nwhere you talk: Temporal sentence localization in\nvideo with attention based location regression. In\nAAAI, pages 9159‚Äì9166.\nRunhao Zeng, Haoming Xu, Wenbing Huang, Peihao\nChen, Mingkui Tan, and Chuang Gan. 2020. Dense\nregression network for video grounding. In CVPR,\npages 10287‚Äì10296.\nCan Zhang, Meng Cao, Dongming Yang, Jie Chen,\nand Yuexian Zou. 2021. Cola: Weakly-supervised\ntemporal action localization with snippet contrastive\nlearning. In CVPR, pages 16010‚Äì16019.\nDa Zhang, Xiyang Dai, Xin Wang, Yuan-Fang Wang,\nand Larry S Davis. 2019a. Man: Moment alignment\nnetwork for natural language moment retrieval via\niterative graph adjustment. In CVPR, pages 1247‚Äì\n1257.\nHao Zhang, Aixin Sun, Wei Jing, and Joey Tianyi Zhou.\n2020a. Span-based localizing network for natural\nlanguage video localization. In ACL.\nSongyang Zhang, Houwen Peng, Jianlong Fu, and\nJiebo Luo. 2020b. Learning 2d temporal adjacent\nnetworks for moment localization with natural lan-\nguage. In AAAI, pages 12870‚Äì12877.\nZhu Zhang, Zhijie Lin, Zhou Zhao, and Zhenxin Xiao.\n2019b. Cross-modal interaction networks for query-\nbased moment retrieval in videos. In SIGIR, pages\n655‚Äì664.\nYue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu,\nXiaoou Tang, and Dahua Lin. 2017. Temporal ac-\ntion detection with structured segment networks. In\nICCV, pages 2914‚Äì2923.\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. 2021. Deformable detr: De-\nformable transformers for end-to-end object detec-\ntion. In ICLR.\n7 Appendix\n7.1 GTR Variant Settings\nWe list typical GTR variant parameter settings in\nTable 7.\nModels Nv Ns Nd d Params(M)\nGTR-B 4 4 6 320 25.98\nGTR-L 6 6 8 320 40.56\nGTR-H 8 8 8 512 61.35\nGTR-B-Wide 4 4 6 352 34.05\nGTR-B-Deep 4 4 8 320 33.89\nGTR-L-Wide 6 6 8 352 47.18\nGTR-L-Deep 6 6 10 320 47.23\nTable 7:Typical GTR variant parameter settings.\n7.2 Empirical Studies and Observations\n7.2.1 Visual Token Acquisition\nThrough ablation studies, we have found that tem-\nporal overlapping sampling plays a critical role in\nCubic Embedding. Here we present more experi-\nmental results to conÔ¨Årm our Ô¨Åndings. SpeciÔ¨Åcally,\nwe choose three kernel sizes (8, 8, 3), (16, 16, 3),\nand (8, 8, 6). In each case, we set Ô¨Åve sets of stride\nshape corresponding to temporal overlapping, spa-\ntial overlapping, and non-overlapping cases, respec-\ntively. Experimental results in Table. 8 show that\ntemporal overlapping brings about signiÔ¨Åcant per-\nformance improvement under various kernel shape\nsettings.\nWe also compare our Cubic Embedding layer\nwith the framewise partition in Vision Transformer.\nPart of the results have been listed in the main pa-\nper and the full results are shown in Table. 9. It\ndemonstrates the superiority of our cubic embed-\nding layer.\n7.2.2 Decoder Design Principles\nColumnar vs. Shrink vs. Expand. We provide more\nexperiments to determine how to design the atten-\ntion head number in different layers. We set up\nthree different distributions of attention heads: i)\nthe same number of heads in all layers (columnar);\nii) gradually decrease the number of heads (shrink);\niii) gradually increase the number of heads (ex-\npand). The results are listed in Table. 10. It is con-\nsistent with our conclusion that progressively ex-\npanding the attention head leads to better per-\nformance.\nDeep vs. Wide . We have developed two variants\n(GTR-B-Wide, GTR-B-Deep) and reported the re-\nsults on ActivityNet Caption and TACoS datasets,\nwhich demonstrates that going deeper is more ef-\nfective than going wider. Here we provide two\nModels Stride R0.51 R0.71 GFLOPs\nkernel (8, 8, 3)\nTemporal (8, 8, 1) 49.73 28.51 41.05\n(8, 8, 2) 49.67 28.45 20.55\nSpatial (4, 4, 3) 44.54 23.12 54.51\n(6, 6, 3) 44.23 22.81 24.26\nNone (8, 8, 3) 43.73 22.45 14.69\nkernel (16, 16, 3)\nTemporal (16, 16, 1) 43.23 24.13 10.83\n(16, 16, 2) 43.14 23.93 5.44\nSpatial (8, 8, 3) 41.26 21.58 13.33\n(12, 12, 3) 41.03 21.35 6.42\nNone (16, 16, 3) 40.83 21.26 3.90\nkernel (8, 8, 6)\nTemporal (8, 8, 2) 46.71 26.84 17.93\n(8, 8, 4) 46.10 26.71 8.99\nSpatial (4, 4, 6) 42.96 21.93 22.21\n(6, 6, 6) 42.95 21.90 9.90\nNone (8, 8, 6) 42.84 21.61 6.01\nTable 8:Input ablations for GTR-B on ActivityNet Caption\ndataset(%).\nadditional variants (GTR-L-Wide, GTR-L-Deep)\nand report results on all three datasets in Table 12.\nSimilarly, the deep model also outperforms the\nwide model, which further conÔ¨Årms our conclusion.\n7.2.3 Multi-modal Fusion Mechanisms\nIn general, multi-modal fusion methods can be di-\nvided into three categories (cf. Figure. 7): 1) Early\nfusion: The multi-modal features are fused before\nbeing fed into the decoder. 2) Conditional fusion:\nThe language feature acts as conditional signals of\nsegment queries of the decoder. We concatenate it\nwith the learnable segment queries features. 3)Late\nfusion: Multi-modal fusion is conducted with the\ndecoder via the proposed Multi-head Cross-modal\nAttention (MCMA) Module.\nFor the speciÔ¨Åc instantiations of MCMA, except\nfor the four fusion strategies mentioned in the main\npaper, we additionally present two additional vari-\nants in Figure. 8. 1) Hybrid Fusion (value split)\nconcatenates the key features and computes the\nquery and value multiplication separately. Specif-\nically, ÀÜQ ‚ààRN√ód, Hk ‚ààRF√ód, Sk ‚ààRS√ód gen-\nerate the attention matrix with shape (F + S) √ód,\nwhich is divided into two parts and each is with\nshape F√ódand S√ód, respectively. These two di-\nvided attention weights are used to computed with\nHv and Sv separately to generate the Ô¨Ånal results.\n2) Stepwise Fusion (language-vision) fuses the lan-\nguage feature Ô¨Årstly and then the video features.\nModels ActivityNet Captions Charades-STA TACoS\nR0.5\n1 R0.7\n1 R0.5\n5 R0.7\n5 R0.5\n1 R0.7\n1 R0.5\n5 R0.7\n5 R0.3\n1 R0.5\n1 R0.3\n5 R0.5\n5\nFramewise 46.10 26.27 76.51 62.04 60.05 36.94 89.40 60.25 35.98 26.34 57.09 45.72\nCubic 49.67 28.45 79.83 64.34 62.45 39.23 91.40 61.76 39.34 28.34 60.85 46.67\nTable 9:Framewise: ViT embedding manner; Cubic: ours.\nModels Heads ActivityNet Captions Charades-STA TACoS\nR0.51 R0.71 R0.55 R0.75 R0.51 R0.71 R0.55 R0.75 R0.31 R0.51 R0.35 R0.55\nGTR-B-Columnar[4, 4, 4, 4, 4, 4]49.42 28.34 79.75 64.2062.34 39.02 91.25 61.4139.01 27.93 60.42 46.39\n[8, 8, 8, 8, 8, 8]49.43 28.34 79.76 64.2362.36 39.03 91.26 61.4139.03 27.93 60.42 46.39\nGTR-B-Shrink[8, 8, 4, 4, 2, 1]49.01 27.95 79.26 64.0062.15 38.81 90.93 61.4538.95 27.73 59.93 45.92\n[16, 16, 8, 8, 4, 1]49.03 27.96 79.26 64.0262.16 38.81 90.93 61.4538.95 27.74 59.94 45.92\nGTR-B-Expand[1, 2, 4, 4, 8, 8]49.67 28.45 79.83 64.3462.45 39.23 91.40 61.7639.34 28.34 60.85 46.67\n[1, 4, 8, 8, 16, 16]49.64 28.42 79.81 64.3262.45 39.22 91.41 61.7739.33 28.33 60.85 46.67\nTable 10:Columnar vs. Shrink vs. Expand. Progressively expanding the attention heads leads to better performance.\nThe experimental results are presented in Ta-\nble. 13 and we have the following Ô¨Åndings. 1) Step-\nwise fusion has the best performance on all three\ndatasets by using the largest number of parameters.\n2) The performance of hybrid fusion (value split) is\nalmost the same as hybrid fusion. 3) Also, stepwise\nfusion(L-V) shares the similar performance with\nstepwise fusion, which demonstrates that the order\nof fusion of visual or language information is not\nsensitive.\nTo investigate the inÔ¨Çuence of frame crop size\nand sample rate, we conduct more experiments on\nstepwise fusion models. The results in Table. 11\nshow that 1) the performance is not sensitive o the\nframe crop size; 2) the performance is positively\ncorrelated with the sample rate but gradually reach\nconvergence.\n7.3 Performance of GTR\nThe performance of GTR under more IoUs is avail-\nable in Table. 14.\nSample Rate Crop size R0.5\n1 R0.7\n1 R0.5\n5 R0.7\n5\n1/8\n112 49.67 28.45 79.83 64.34\n140 49.72 28.52 79.95 64.40\n168 49.73 28.52 79.97 64.40\n224 49.73 28.52 79.97 64.40\n1/8\n112\n49.67 28.45 79.83 64.34\n1/6 51.54 29.43 80.11 64.76\n1/4 52.34 29.57 80.14 64.79\n1/2 52.35 29.57 80.14 64.79\nTable 11:Performance (%) on ANet on different frame crop size and sample rate.\nModels Param(M) ActivityNet Captions Charades-STA TACoS\nR0.51 R0.71 R0.55 R0.75 R0.51 R0.71 R0.55 R0.75 R0.31 R0.51 R0.35 R0.55\nGTR-B-Wide 34.05 49.94 28.62 79.86 64.4362.49 39.41 91.42 61.8539.52 28.70 60.91 46.84\nGTR-B-Deep 33.89 50.15 28.74 80.14 64.6062.50 39.50 91.58 61.9339.60 29.04 61.04 47.03\nGTR-L-Wide 47.18 50.48 28.96 80.28 65.0062.51 39.58 91.62 61.9439.98 29.35 61.45 47.30\nGTR-L-Deep 47.23 50.52 29.05 80.37 65.0362.54 39.66 91.62 62.0040.12 29.51 61.75 47.52\nTable 12:Performance (%) comparisons between the wide variants and deep variants.\nModels ActivityNet Captions Charades-STA TACoS Param(M)GFLOPsR0.51 R0.71 R0.55 R0.75 R0.51 R0.71 R0.55 R0.75 R0.31 R0.51 R0.35 R0.55\nEarly Fusion39.35 19.64 70.39 56.3454.13 32.72 84.69 54.5328.25 19.56 50.46 36.0612.86 11.31\nConditional Fusion35.25 14.57 66.35 51.3450.62 25.65 79.34 49.0324.62 14.37 47.34 34.7412.42 10.60\nJoint Fusion42.51 21.53 71.24 58.0457.66 33.68 87.52 57.3433.69 23.72 54.27 40.1416.62 13.91\nDivided Fusion46.91 26.21 76.85 63.1360.93 38.22 89.84 59.9537.82 26.31 58.13 44.8025.16 21.02\nHybrid Fusion46.82 25.45 76.20 62.4160.21 37.25 89.63 59.5137.01 25.77 57.63 44.3220.94 17.25\nHybrid Fusion(value split)46.69 25.32 75.83 61.9359.94 36.93 89.52 59.3236.85 25.49 57.42 44.0520.82 16.75\nStepwise Fusion49.67 28.45 79.83 64.3462.45 39.23 91.40 61.7639.34 28.34 60.85 46.6725.98 20.55\nStepwise Fusion(L-V)49.62 28.37 79.76 64.2562.37 38.93 91.17 61.5339.19 28.29 60.62 46.5525.98 20.55\nTable 13:Multi-modal fusion comparisons.\nSelf-Attention\nFeedforward\n√ó\"!\nWord Embedding\nSelf-Attention\nFeedforward\n√ó\"\"\nVideo Embedding\nSegment Query Q\nFeedforward\nSelf-Attention√ó\"#\nCross-Attention\n[S, E]\nvideoquery\nEarly fuse\nSelf-Attention\nFeedforward\n√ó\"\"\nVideo Embedding\nSelf-Attention\nFeedforward\n√ó\"!\nWord Embedding\nMCMA\nFeedforward\nSelf-Attention\nSegment Query Q√ó\"#\n[S, E]\nvideo query\nLate fuse\nvideoquery\nConditional fuse\nSelf-Attention\nFeedforward\n√ó\"!\nWord Embedding\nSelf-Attention\nFeedforward\n√ó\"\"\nVideo Embedding\nSegment Query Q\nFeedforward\nSelf-Attention√ó\"#\nCross-Attention\n[S, E]\nFigure 7:Three methodologies of multi-modal fusion.\nJoint Fusion\nMatMul\n!\"\nSoftMax\nMatMul\n[#!,%!] [#\",%\"] DividedFusion\nMatMulSoftMaxMatMul!\"\nSoftMaxMatMul!\"\nMatMulWeighted Add.\n#!#\" %!%\" HybridFusion\nConcatenation\nMatMulSoftMaxMatMulSoftMax\nMatMul\n!\" !\"#\" %\"[#!,%!] StepwiseFusionMatMulSoftMax\n!\"\nMatMulMatMulSoftMaxMatMul\n%\"#\" #! %!\nHybirdFusion (value split)\nMatMulSoftMax & Split\nMatMul!\"\nMatMulWeighted Add.\n#! %![#\",%\"] StepwiseFusion (language-vision)\nMatMulSoftMax\n!\"\nMatMulMatMulSoftMaxMatMul\n#\"%\" %! #!\nFigure 8:Modal variants of Multi-head Cross-Modal Attention module (MCMA). The Ô¨Årst fourth variants are already presented\nin the main paper and the rightmost two are two variants for Hybrid Fusion and Stepwise Fusion respectively.\nModels ActivityNet Captions Charades-STA TACoS\nR0.3\n1 R0.5\n1 R0.7\n1 R0.3\n1 R0.5\n5 R0.7\n5 R0.5\n1 R0.7\n1 R0.5\n5 R0.7\n5 R0.1\n1 R0.3\n1 R0.5\n1 R0.1\n1 R0.3\n5 R0.5\n5\nGTR-B (Ours) 66.15 49.67 28.45 87.94 79.83 64.34 62.45 39.23 91.40 61.76 48.85 39.34 28.34 72.59 60.85 46.67\nGTR-L (Ours) 66.72 50.43 28.91 88.21 80.22 64.95 62.51 39.56 91.62 61.97 49.25 39.93 29.21 73.14 61.22 47.10\nGTR-H (Ours) 66.80 50.57 29.11 88.34 80.43 65.14 62.58 39.68 91.62 62.03 49.41 40.39 30.22 73.24 61.94 47.73\nTable 14:Performance comparisons on three benchmarks(%)."
}