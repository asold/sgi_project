{
  "title": "Aligning Language Models to User Opinions",
  "url": "https://openalex.org/W4389518978",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2127272427",
      "name": "EunJeong Hwang",
      "affiliations": [
        "University of British Columbia",
        "Vector Institute"
      ]
    },
    {
      "id": "https://openalex.org/A3091915718",
      "name": "Bodhisattwa Majumder",
      "affiliations": [
        "Allen Institute"
      ]
    },
    {
      "id": "https://openalex.org/A1990453627",
      "name": "Niket Tandon",
      "affiliations": [
        "Allen Institute"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4389523771",
    "https://openalex.org/W3207835719",
    "https://openalex.org/W4385567201",
    "https://openalex.org/W2970990259",
    "https://openalex.org/W2963908320",
    "https://openalex.org/W4361807105",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4361866126",
    "https://openalex.org/W2605350416",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4402671806",
    "https://openalex.org/W3146348615",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W3133045291",
    "https://openalex.org/W565543850",
    "https://openalex.org/W4200630887",
    "https://openalex.org/W2731858580",
    "https://openalex.org/W2053154970",
    "https://openalex.org/W2134661443",
    "https://openalex.org/W4361193179",
    "https://openalex.org/W3046423960",
    "https://openalex.org/W4378770449",
    "https://openalex.org/W1988466242"
  ],
  "abstract": "An important aspect of developing LLMs that interact with humans is to align models' behavior to their users. It is possible to prompt an LLM into behaving as a certain persona, especially a user group or ideological persona the model captured during its pertaining stage. But, how to best align an LLM with a specific user and not a demographic or ideological group remains an open question. Mining public opinion surveys (by PEW research), we find that the opinions of a user and their demographics and ideologies are not mutual predictors. We use this insight to align LLMs by modeling relevant past user opinions in addition to user demographics and ideology, achieving up to 7 points accuracy gains in predicting public opinions from survey questions across a broad set of topics. Our work opens up the research avenues to bring user opinions as an important ingredient in aligning language models.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5906–5919\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAligning Language Models to User Opinions\nEunJeong Hwang1, 2, Bodhisattwa Prasad Majumder*3, and Niket Tandon*3\n1University of British Columbia\n2Vector Institute for AI\n3Allen Institute of AI\nejhwang@cs.ubc.ca, bodhisattwam@allenai.org, nikett@allenai.org\n*contributes equally\nAbstract\nAn important aspect of developing LLMs that\ninteract with humans is to align models’ behav-\nior to their users. It is possible to prompt an\nLLM into behaving as a certain persona, espe-\ncially a user group or ideological persona the\nmodel captured during its pertaining stage. But,\nhow to best align an LLM with a specific user\nand not a demographic or ideological group\nremains an open question. Mining public opin-\nion surveys (by PEW research), we find that\nthe opinions of a user and their demographics\nand ideologies are not mutual predictors. We\nuse this insight to align LLMs by modeling\nrelevant past user opinions in addition to user\ndemographics and ideology, achieving up to 7\npoints accuracy gains in predicting public opin-\nions from survey questions across a broad set\nof topics1. Our work opens up the research\navenues to bring user opinions as an important\ningredient in aligning language models.\n1 Introduction\nPersonality is a defining feature of human beings,\nshaped by a complex interplay of demographic\ncharacteristics, moral principles, and social expe-\nriences (Weil, 1957; McLellan, 1989). In turn,\na person’s personality has a significant influence\non their ability to make decisions (Lauriola and\nLevin, 2001; Busic-Sontic et al., 2017). Owing\nto the wide-scale adaptation of the large language\nmodels (LLMs) for assisting individuals in their\ndecision-making process (Jiang et al., 2021; Gao\net al., 2023a), it becomes increasingly critical to en-\nsure that these models are aligned with the unique\npersonalities of their users.\nWith lower barriers to entry, several recent works\nfocused on prompting LLMs with persona or role-\nbased prompts such as Pretend you are a\nDemocrat (Deshpande et al., 2023; Santurkar et al.,\n1Project page:\nhttps://github.com/eujhwang/personalized-llms\nFigure 1: Opinions can vary even when two users have\nthe same demographic traits.\n2023). However, the extent to which these ap-\nproaches align language models with users remains\nunclear due to the subjective nature of defining\nuser personas. Users have nuanced opinions that\ncan change over time and vary depending on con-\ntext. While alignment with normalized user groups\nlike religion or political inclination may be easier,\nLLMs continue to struggle to align with individual\nusers or the long tail of user groups. Additionally,\nLLMs tend to form opinions based on their pre-\ntraining data and feedback collected from crowd\nworkers and model designers. As a result, they\nexhibit low steerability, even with user groups that\nhave major representation (Santurkar et al., 2023).\nAligning LLMs to individual and long-tail opin-\nions has received less attention, while mostly fo-\ncusing on aligning to user groups. In our analysis\nover PEW surveys, we found that people can share\nall of their demographic traits but still exhibit a\nlarge variance in their opinions, rendering the cur-\nrent group-based LLM alignment insufficient. This\n5906\npaper investigates the relationship between demo-\ngraphic traits and individual opinions in LLM align-\nment. Specifically, we seek to answer the following\nresearch question:\nWhat do we need to align an LLM to\na user: demographic traits, fine-grained\nopinions, or both?\nThe majority of the past work in NLP literature\nfocused on aligning LLMs with normalized user\ngroups (Santurkar et al., 2023; Majumder et al.,\n2019; Salemi et al., 2023). In social science stud-\nies, however, it has been shown that all users are\nunique even if they belong to the same broader user\ngroup, and normalizing user groups is not a true\nrepresentative of a user’s opinion (Chu et al., 2023;\nKim and Lee, 2023). We apply insights from these\nsocial science studies to an empirical setting where\nwe try to model individuals’ opinions based on their\nvarious persona information such as demographic\ntraits, ideological inclinations, and past opinions.\nIn this paper, we give a thorough analysis of\npublic survey responses in the OpinionQA dataset\n(Santurkar et al., 2023) with respect to their de-\nmographics, ideology, and implicit opinions and\npresent comprehensive experimental results using\nthe GPT-3 model with various combinations of in-\nputs (i.e., demographic, ideology, user’s past opin-\nions). Through our dataset analysis, we found that\nusers’ opinions and demographics do not neces-\nsarily correlate with each other. Our experimental\nresults show incorporating both user opinions, de-\nmographics, and ideology, results in significant\ngains of up to 7 points in QA accuracy for certain\ntopics, and utilizing the most relevant past opin-\nions helps the model to pinpoint the more accurate\nanswers for the users.\n2 What makes a persona?\nWe present a study on various components that\nmakes a personality (in short, persona) of a user.\nWe use the OpinionQA dataset, which contains 15\ntopics, and each topic contains an average of 100\nquestions and 5K users (Santurkar et al., 2023).\n2.1 Demographics\nThe dataset records eight demographic information\nof a user: region, sex, age, education, race, citizen,\nmarital status, and income. These are the markers\nof social experience that a user is most likely to\ngo through. For example, the social experience\ncan be determined by the region a user belongs, or\ntheir age determines whom they socialize with on\na regular basis. However, this runs with the risk\nof stereotyping (i.e., an old individual is less likely\nto mix with younger people or they are conserva-\ntive in thinking). We later show that demographic\ninformation is not enough to model an individual.\n2.2 Ideology\nIdeology is formed by an individual understanding\nof politics and economics. In our dataset, we have\neach subject’s political affiliation and inclinations\ntoward well-known political ideologies (e.g., con-\nservative, liberal). We use this information as an\nindividual’s ideology.\n2.3 Opinions\nOpinionQA uses a well-established method of cap-\nturing human opinions from public opinion surveys.\nIn these surveys, subjects are asked to answer sub-\njective questions that reflect their unique opinions\nand what makes them different from other individ-\nuals. Figure 1 shows an example of opinions that a\nuser provided during a survey.\n2.4 Deriving insights from public surveys\nWe derive insights from the OpinionQA dataset,\nwhere we analyze the degree of agreement in user’s\nopinions where they same demographics and how\nthis agreement varies across topics. This statistical\nanalysis generates useful insights that we later use\nfor our modeling approach. We also look for sim-\nilar (dis)agreements in opinions when users have\nthe same ideologies.\nOpinions differ despite same demographics\nWe first take all pairs of users sharing the same\ndemographics and compare their opinions. To\ncalculate the agreement score between users, we\nutilize Cohen’s kappa coefficient (Cohen, 1960),\nwhich ranges from −1 to 1. Even though two users\nshare the same demographics, agreement scores\non the implicit opinions are gathered around 0.5\n(Figure 2). This shows that solely relying on demo-\ngraphic information is not enough to personalize\nthe model, and users’ implicit opinions can play a\ncritical role in personalization.\nOpinions differ across topics In Figure 2, we\nalso show the topic-wise agreement scores. On\ncertain topics, including Family & Relationships\nand Guns, users exhibit relatively higher agreement\n5907\nscores. On the other hand, for some topics, includ-\ning Race and America in 2050, users have lower\nagreement scores, indicating that certain topics may\nhave larger variability in terms of user opinions. We\nlater analyze if this variability appears in a model’s\npredictive performance when it is used to predict\nuser opinions across different topics.\nFigure 2: Topic-wise agreement score; x-axis: agree-\nment score, y-axis: topic. This graph shows that users\nwith similar demographics/ ideology can have different\nopinions (cohen kappa scores of around 0.4 show not\nsome but not substantial correlation in opinions)\nOpinions differ despite same ideology To ana-\nlyze the correlation between user opinions and their\nideology, we extract user pairs that two users who\nanswered at least more than 10 common questions\nand compare their opinions and political ideologies.\nTable 1 shows the percentage of user pairs shar-\ning similar opinions, where 70% of opinions are\nmatched between two users, and the percentages of\nthe same ideologies and different ideologies within\nthose user pairs. We observe that even though the\nusers have similar opinions, around 80% of the\nuser pairs have different ideologies. In contrast, we\nobserve the percentage of sharing similar opinions\namong the users having similar ideologies is rela-\ntively higher than the percentage of sharing similar\nideologies among the users having similar opin-\nions in Appendix A. This implies that while having\nsimilar opinions does not necessarily imply shared\nideologies among users, the presence of similar\nideologies may suggest that users are more likely\nto have similar opinions. We particularly notice\nthis phenomenon on the Guns and Family topics,\nas highlighted in Table 1. While the percentage\nof user pairs with shared opinions is higher com-\npared to other topics, the percentage of user pairs\nwith differing ideologies within these pairs is no-\ntably higher than the percentage of user pairs with\nsimilar ideologies.\nBased on the insights derived above, we incorpo-\nrate them in our modeling approaches and analyze\nif these translate to the predictive performance of\na model when used to predict user opinions as col-\nlected from the surveys.\n3 Aligning LLMs with persona\nIn this section, we detail our task, possible mod-\neling approaches, and evaluation protocols in Sec-\ntion 3.1 and discuss how to select the most relevant\npast opinions of a user in Section 3.3.\n3.1 Setup\nTask We use LLMs to model a user; how-\never, to concretely measure the performance,\nwe use a simple question-answering (QA) setup.\nFor our QA task, we use existing questions\nfrom the surveys and try to predict the choice\nfrom multiple-choice originally given to the\nsubjects. We use a prompting-based zero-\nshot approach to perform the multiple-choice\nQA. We experiment with gpt-4 (GPT-4),\ntext-davinci-003 (GPT-3), gpt-3.5-turbo\n(GPT-3.5), Vicuna-13b, LLaMA-7b as the\nLLMs. Vicuna-13b (Chiang et al., 2023) is a large\nlanguage model which was built upon LLaMA-\n13b (Touvron et al., 2023) and LLaMA-7b/13b is a\ntransformer-based language model trained on tril-\nlions of tokens exclusively sourced from publicly\navailable data. Vicuna-13b performs on par with\nChatGPT (Chiang et al., 2023).\nModeling Approaches We sample 100 users per\ntopic. 20% of implicit questions belonging to the\nspecific user are used as the user’s implicit persona,\nand the rest are used to test the model’s personal-\nization ability. We have the following variants of\nour model where the model is gradually exposed to\ndifferent levels of user information: demographic,\nideological information, and user past opinions.\nHere is a rough sketch of what a prompt would\ncontain for each modeling variation:\n•no persona: this is a case where default LLM\nopinion is evaluated w.r.to the individual’s opinion\n(Santurkar et al., 2023).\n5908\nGuns Auto Gender Sex.\nharass.\nBiomed-\nfood\nLeadership 2050\nUS\nTrust-\nScience\nSimilar op. user pair 45 13 30 12 11 37 23 21\nSimilar op. & ideol. 19 18 21 30 19 24 20 20\nSimilar op. & diff. ideol. 81 82 79 70 81 76 80 80\nRace Misinfo. Privacy Family Econ.\nInequal.\nGlobal\nAttitudes\nPolitics\nSimilar op. user pair 12 29 21 43 25 24 16\nSimilar op. & ideol. 30 20 17 19 25 33 40\nSimilar op. & diff. ideol. 70 80 83 81 75 67 60\nTable 1: Percentage of user pairs sharing similar opinions (similar op. user pair) and the percentages of similar\nideologies (similar op. & ideol.) and different ideologies (similar op. & diff. ideol.) within user pairs sharing\nsimilar opinions. (Auto: Automation, Sex. harass.: Sexual harassment, Biomed-food: Biomedical food, Misinfo.:\nMisinformation, Econ. Inequal.: Economic Inequality, Politics: Political Views)\n•ideology: here, we observe if ideological inclina-\ntions from the user help the model to align better\nto them (Santurkar et al., 2023).\n•ideology + demographics: here, we observe if\nboth demographic information and ideological in-\nclinations from the user help the model to align\nbetter with them (Santurkar et al., 2023).\n•ideology + opinions: we combine ideological in-\nclinations and opinions and measure if these help\nthe model to align better with an individual.\n•demographic + ideology + opinions: when we\ncombine all possible personal information, i.e.,\ndemographic, ideology, and opinions, and mea-\nsure if these help the model to align better with an\nindividual. See Figure 5 for the complete prompt.\n3.2 Evaluation Metric\nFor accuracy evaluation, we utilize two types of\naccuracy measures, overall accuracy and collapsed\naccuracy. For overall accuracy, we simply calcu-\nlate the accuracy of the predicted answer choice\nwith respect to the gold answer choice from the\ndataset. We also present collapsed accuracy be-\ncause most answer choices in the opinion QA\ndataset have around 3 to 4 classes. In cases where\nthere are more than 4 classes, it is possible to fur-\nther group the classes into super classes without\nlosing substantial finer information. For exam-\nple, the following answer choices: [Very likely,\nSomewhat likely, Not too likely, Not\nat all likely] , can be grouped into [Likely,\nUnlikely]. We consolidate such answer choices\ninto two classes, referred to as collapsed accuracy,\nand present the results accordingly.\nWe follow Santurkar et al. (2023) for theopinion\nalignment score evaluation. For the alignment\nscores, we calculate the difference between user\nand model opinion distributions. The difference\nis calculated using 1-Wassaerstein distance(WD),\nwhich measures the minimum cost for transforming\none distribution to the other:\nA(Dm,Dh; Q) = 1\nQ\n∑\nq∈Q\n1−WD(Dm(q),Dh(q))\nN −1\nwhere N is the number of answer choices exclud-\ning refusal, and the normalization factor N −1\nis the maximum WDbetween two answer choice\ndistributions. The final score can be interpreted\nas how well the model distribution aligns with the\nhuman distribution.\n3.3 LLM mimicking a person with ideologies,\ndemographic, and opinions\nOur main goal is to use different components of\na user’s persona (demographics, ideology, opin-\nions) to align an LLM with an individual. Specif-\nically, by having two experiments, one with past\nopinions+ideology and the other with past opin-\nions+ideology+demographics, we aim to analyze\nthe role of demographics when predicting user re-\nsponses. In addition, we hypothesize that giving\nusers’ past opinions may offer useful insights into\ntheir perspective (followed from Table 1), and LLM\ncan benefit from that information when predict-\ning the future answer for the specific user. When\nadding the user’s past opinions, we compare the\nmodel with all opinions (maximum 16) to the\nmodel with top-kopinions (kis a hyperparameter\n∈3,5,8). The top-kopinions are obtained by com-\nparing the embedding similarity between the user’s\nprevious opinions and the question at hand, where\nwe employ text-embedding-ada-002 to obtain\nthe embeddings. We hypothesize that all opinions\n5909\nExact match Collapsed match\nModel L-7b V-13b GPT-3.5 GPT-3 GPT-4 L-7b V-13b GPT-3.5 GPT-3 GPT-4\nNo Persona 0.33 0.36 0.37 0.43 0.53 0.60 0.62 0.63 0.62 0.68\nDemo.+Ideo. 0.35 0.39 0.47 0.47 0.54 0.62 0.62 0.66 0.65 0.70\nDemo.+Ideo.+Opinionall 0.37 0.41 0.50 0.51 0.58 0.61 0.62 0.69 0.69 0.73\nOpiniontop3 0.35 0.42 0.50 0.51 0.55 0.61 0.62 0.67 0.67 0.71\nOpiniontop8 0.36 0.42 0.50 0.52 0.56 0.63 0.63 0.68 0.68 0.72\nIdeo.+Opiniontop8 0.36 0.43 0.51 0.53 0.57 0.62 0.64 0.69 0.69 0.73\nDemo.+Opiniontop8 0.37 0.43 0.50 0.53 0.57 0.61 0.63 0.69 0.69 0.73\nDemo.+Ideo.+Opiniontop3 0.35 0.42 0.51 0.53 0.58 0.61 0.63 0.70 0.69 0.73\nDemo.+Ideo.+Opiniontop8 0.37 0.43 0.51 0.54 0.58 0.61 0.63 0.70 0.70 0.74\nTable 2: Overall QA accuracy. For statistical significance, all models scored ±0.01, which are computed using\nWilson score intervals for α= 99%. GPT-4 was tested over 50% of the testset due to cost, but we found the\nperformance with GPT-3 with the subset remains similar to the original testset by having < 0.004 standard deviation.\n(L-7b: LLaMA-7b, V-13b: Vicuna-13b)\nmay incorporate some unrelated viewpoints to an-\nswer the question, and hence offering more perti-\nnent opinions would enhance the model’s ability to\naccurately anticipate its future response for the user.\nWe show a complete prompt that uses all available\npast information of individuals to predict their fu-\nture opinions and all prompts for other modeling\napproaches in Appendix C.\n4 Results and Analysis\nHere, we first analyze our model variants (Sec-\ntion 4.1) to validate hypotheses that we gather from\nanalyzing the dataset (in Section 2.4). We also pro-\nvide our model’s performance when we use similar\nmodeling setup to predict group-level opinions in\nSections 4.2 and 4.3.\n4.1 LLM for an individual\nHere we discuss results of using an LLM to model\nan individual in the light of the evaluation metrics\ndescribed in Section 4.1.\nExact match vs. Collapsed match The accuracy\nwith the exact match and with the collapsed match\nin Table 2 shows a similar trend for the performance\nof our model variants. This suggests that leveraging\nimplicit opinions enables the model to align with\nthe correct range of answer choices, even though\nit does not precisely predict the exact same answer\nas the user’s choice.\nOverall Accuracy Table 2 presents overall QA\naccuracy with exact match and collapsed match\nfor answer choices. In most cases, utilizing\ndemographic+ideology+top-8 opinions performs\nthe best. Moreover, adding demographic and ide-\nology information outperforms the model without\nany persona, indicating that some questions might\nbe highly correlated with the user’s demograph-\nics, and LLM is able to make a guess with the\ndemographic information. Incorporating the user’s\nprevious opinions, up to 16 in total, along with\ndemographic information, substantially enhances\nthe performance in both overall and collapsed ac-\ncuracy across all models. This implies that users’\npast opinions are indeed important to make correct\npredictions.\nInterestingly, utilizing the top- k most relevant\nprevious opinions does not yield a significant in-\ncrease in collapsed accuracy. However, it does\nimprove the exact match accuracy by up to 3 points\nwith GPT-3 when using both demographics and\nideology along with the user’s previous opinions.\nThis implies that having top-kmost relevant past\nopinions can help the model pinpoint more accurate\nanswers, and providing the user’s past opinions is\nalready pushing the model to be in the correct range\nof the answer choices. We noticed that utilizing\nthe top-3 opinions yields similar performance to us-\ning the top-8 opinions, indicating that a few of the\nmost relevant opinions carry the most performance\nimprovement of the model.\nMoreover, simply using the top 3 most relevant\nopinions performs on par with the model with user\ndemographic, ideology, and user’s past 16 random\nopinions. This confirms again that utilizing the\nmost relevant opinions as feedback is essential to\nget personalized answers from LLM. Lastly, pro-\nviding additional demographic information with\nideology slightly improves the model performance,\nimplying that the demographic information may\ncontribute valuable insights to the model to a cer-\ntain degree.\n5910\nModel GPT-3 GPT-3+CoT\nOpiniontop8 0.52 0.51\nIdeo.+Opiniontop8 0.53 0.52\nDemo.+Opiniontop8 0.53 0.52\nDemo.+Ideo.+Opiniontop8 0.54 0.53\nTable 3: QA accuracy comparison between GPT-3 and\nGPT-3 with Chain of Thought style prompt.\nComparison between LLMs GPT-4 produces a\nsimilar performance when using all opinions and\nwhen using top-krelevant opinions. This implies\nthat GPT-4 itself might be able to identify the most\nrelevant user’s previous opinions.\nWe find that the LLaMA-7b model does not un-\nderstand what to produce when opinions are added\nto the prompt resulting in 18% cases where it does\nnot yield any answer. Likewise, GPT-3.5 tends to\nproduce no answer for questions that are about per-\nsonal opinions (e.g. GPT-3.5 with top-3 opinions\nproduces 59% no-answers). When we prompted\nthe model to generate an answer without an ex-\nplanation by changing the suffix of the prompt:\n“Answer choice:” →“Answer choice without\nexplanation:”, and substantially fewer (less than\n1%) cases have no-answers.\nThe trend of the performance where we add top-\nkopinions remains the same for both GPT-3 and\nGPT-3.5. There is no clear winner between dif-\nferent versions of GPT as also found in the other\nliterature (Fu et al., 2023).\nChain of Thought (CoT) Prompting CoT (Wei\net al., 2023) has shown that encouraging LLMs\nto explain their reasoning step-by-step improves\nthe model performance. To see whether CoT also\nbenefits in personalizing answers, we test the GPT-\n3 model with CoT-style prompting by changing\nthe suffix of our prompt from “Answer choice:”\nto “Let’s think step by step and choose\none answer choice:” and present the results in\nTable 3. We discover that using the CoT prompt\nconsistently decreases the performance by 1 point.\nThis is because the model attempts to provide the\nreason for the potentially irrelevant information\npresented in the prompt, suggesting its inability\nof selecting the most relevant information about\nthe answer. E.g., in Figure 4, the model generates\nreasoning exclusively based on demographic and\nideology information but ignores user opinions.\nOpinion Alignment Scores Among high-\nperforming models, since only GPT-3 provides\nprediction probabilities, we compare opinion\nModel Opinion Alignment Score\nNo Persona 0.670\nDemo.+Ideo. 0.763\nDemo.+Ideo.+Opinionall 0.780\nOpiniontop3 0.777\nOpiniontop8 0.779\nIdeo.+Opiniontop8 0.793\nDemo.+Opiniontop8 0.789\nDemo.+Ideo.+Opiniontop3 0.796\nDemo.+Ideo.+Opiniontop8 0.795\nTable 4: Opinion alignment scores (§3.2) with GPT-3\n(text-davinci-003).\nalignment scores (defined in §3.2) with various\nsetups using GPT-3 model in Table 4. Overall\nscores exhibit a similar trend as seen in Table 2,\nin which utilizing top- k most relevant opinions\noutperforms the model that uses all opinions. This\nis consistent with our finding that having top- k\nmost relevant past opinions helps the model find\nmore accurate answers for the user.\nTopic-wise Accuracy Figure 3 demonstrates the\nmodel’s accuracy across different topics with vari-\nous setups, measured by the exact match for answer\nchoices. The model with demographic and implicit\nopinions particularly achieves higher scores on the\nBiomedical-food and Guns topics, implying that\nthese two topics may lead the users to have simi-\nlar opinions of each other. In contrast, the model\nexhibits slightly decreased performance when in-\ncorporating implicit opinions on topic Automation.\nThis suggests that the LLM can make accurate pre-\ndictions up to some extent based on user demo-\ngraphic and ideology information. However, in-\ncorporating implicit opinions, which may include\nviewpoints not aligned with users’ demographic or\nideologies, can potentially confuse the model in its\nprediction process.\nCommon Errors In Table 5, we manually an-\nalyzed 30 randomly sampled opinions where\nGPT-3 produces correct answers with demo-\ngraphic+ideology information but incorrect an-\nswers when opinions are added. The model mostly\nconfuses when there is a high overlap between\nuser opinions and some answer (possibly wrong)\nchoices. For a question: “How well do the follow-\ning phrases describe you?” and a correct answer:\n“Describes me well,” the model often predicts cor-\nrectly purely based on demographic information\n(e.g., Black). However, when the user’s past opin-\nions were added and since it includes the phrase\n“supporter of rights for LGBT people do not de-\n5911\nFigure 3: Overall topic-wise accuracy based on the exact match for answer choices. (demo.: demographic, ideo.:\nideology, op.: opinions). GPT-3 model is used for the experiment. See Appendix B for the scores.\nOpinion contains Percentage (%)\nWord-overlap with answer choice 36\nIrrelevant information 30\nRelevant information 30\nTable 5: Types of common errors with their percentages\nwhen prompting with opinions confuses the model.\nscribe me well,” it predicts a wrong answer choice:\n“does not describe me well” just because the direct\nphrase overlap. While models often ignore addi-\ntional relevant information and commit errors, it is\nalso sensitive to irrelevant opinions (even in top-k)\nwhen additionally added with demographic infor-\nmation. For e.g., while predicting user opinion on\nthe U.S. remaining as a world superpower in 2050,\nthe user’s past opinion about violence against Jews\n(retrieved in top-8) confuses the model leading to\nan incorrect prediction. This also explains why\njust adding top-3 opinions often archives the high-\nest accuracy instead of adding all opinions. See\nAppendix D for examples of all types of errors.\n4.2 LLM with majority answer choices\nAdditionally, we also wanted to understand if simi-\nlar performances can be achieved if we model an\nindividual as a member of a (sub-)population, mir-\nroring (Santurkar et al., 2023). While it is true that\nopinions are necessary to predict individual-level\npreferences, here we investigate if demographic\ninformation is sufficient for predicting population-\nlevel opinions or not. For this, we first merge our\nQA data points using a particular ideological group\nvalue (e.g., democrat) and obtain the answer choice\nthat is chosen by most of the group members (i.e.,\na majority vote) and treat that answer as the gold\nanswer for the question while calculating the accu-\nracy (Kim and Lee, 2023).\nWe prompt our model to predict an answer given\na question assuming the role of a group representa-\nQuestion: How much, if at all, do you worry about\nthe following happening to you? Being the victim of a\nterrorist attack\nChoices: Worry a lot, Worry a little, Do not worry at all\nOpinions: I never used a gun to threaten or intimidate\nanyone in family.\nThere were no guns in my household when I was\ngrowing up.\nDemographics and Ideology:\nPolitical party: Republican, Political ideology: Con-\nservative, Region: South, Marital status: Married,\nEducation: College graduate/some postgrad\nCoT reasoning:\nStep 1: The person is conservative and Republican, so\nthey may be more likely to worry about a terrorist attack.\nStep 2: The person is married and has a college degree,\nso they may be more aware of the potential risks of a\nterrorist attack.\nStep 3: The person is in the South, so they may be more\nlikely to worry about a terrorist attack due to the region’s\nhistory of terrorism.\nConclusion: The person is likely to select answer choice\nA: Worry a lot.\nUser-answer (expected): Do not worry at all\nDemo.+ideo.+opinions with CoT: Worry a lot✗\nDemo.+ideo.+opinions w/o CoT: Do not worry at all ✓\nFigure 4: An example of incorrect CoT reasoning con-\nfusing the model. As we see in CoT reasoning, the\nmodel reasons about demographic information, not opin-\nions provided.\ntive, i.e., a person having a majority vote answers\nbelonging to a specific group (See Appendix C\nfor the prompt). We see that the LLM is good\nat predicting the answer given by the majority of\nthe group member belonging to a certain ideology,\nsuggesting that LLMs are good at modeling a rep-\nresentative individual of a sub-population (e.g., all\ndemocrats). The overall performance without ideol-\nogy information is 0.597 (with exact answer choice\n5912\nExact match Collapsed match\nMajority answer 0.597 0.674\nIndependent 0.546 0.674\nDemocrat 0.578 0.665\nRepublican 0.523 0.639\nTable 6: Performance with LLM with ideology informa-\ntion. GPT-3 model is used for the experiment.\nmatch) and 0.674 (with collapsed answer choice\nmatch), as presented in Table 6. This also indicated\nthat the default opinions from the LLMs are some-\nwhat aligned with the majority opinions seen at a\npopulation level.\n4.3 LLM as a person with an ideology\nNext, we add the ideological information to see if\nthis additional information can help the LLM per-\nform better to model a user belonging to a group\nthat believes in a specific ideology (e.g., conserva-\ntive) (See Appendix C for the prompt). We find\nthat the LLM is moderately good at modeling a\nuser with group-level information to predict the\ngroup-level majority opinion. This indicates that\nthe additional ideological information is not par-\nticularly helpful. The average performance with\nideology information is 0.549 (with exact answer\nchoice match) and 0.659 (with collapsed answer\nchoice match), as shown in Table 6. We see a\nsimilar trend in results for modeling an individual\nwith their demographics and/or ideology and/or\npast opinions since an individual’s opinion does\nnot align with the group’s majority opinion that the\nperson belongs to.\n5 Related work\nPersonalization Past works that focused on mod-\neling individual users were from the pre-LLMs era\nand mainly hail from the recommender systems lit-\nerature (Gao et al., 2023b; He et al., 2017; Li et al.,\n2021; Majumder et al., 2019). However, these sys-\ntems were trained on domain-specific annotated\ndatasets or using latent information about the users\n(e.g. their previously written reviews). The recent\nLLMs have seen less content from the long tail of\nuser groups during their pre-training phase, and\nthere has been a lack of large-scale datasets of in-\ndividual opinions until recently (Santurkar et al.,\n2023). Thus it remains an open problem whether\nLLMs can be aligned effectively with individual\nuser persona and how different user information\n(e.g., demographic traits vs. past opinions) influ-\nences how well an LLM can model individual’s\nopinions. For a comprehensive comparison among\nall previous work, see Table 9.\nRole of demographics and ideology There have\nbeen several studies investigating the correlation\nbetween ideological attitudes and psychological\ntraits (Zmigrod et al., 2021; Crockett and Wallen-\ndorf, 2004; Chan and Palmeira, 2021). Crockett\nand Wallendorf (2004) found that normative politi-\ncal ideology is central to understanding shopping\nas a manifestation of social and political connec-\ntions. Chan and Palmeira (2021) found that the\ncognitive decision-making strategies of individuals\nreflect their ideological attitudes. Differently, we\nshow that ideology is not the only important factor\nin predicting the user’s opinion using an LLM.\nLLMs with retrieval-based approach Exten-\nsive prior work has used retrievals from a text cor-\npus to aid QA (Madaan et al., 2022; Pan et al.,\n2019), or retrievals of prior QA pairs for nearest-\nneighbor QA (Khandelwal et al., 2020). Madaan\net al. (2022) uses a memory of user opinions to\nretrieve past relevant data points for the prompt.\nKhandelwal et al. (2020) showed the effectiveness\nof the nearest neighbor search for language mod-\neling by extending a pre-trained language model\n(LM). Differently from work on LLMs and group-\nlevel personalization, we show that LLMs can be\ntuned for individual users with their opinions.\n6 Conclusion and Outlook\nThis paper offers a new insight that aligning LLMs\nto users is best done by modeling user demograph-\nics, ideologies, and the most relevant past opinions.\nLarge-scale experiments on PEW surveys present\nin the OpinionQA dataset show an approximately\n7% absolute QA accuracy over strong demography-\nbased baselines. We proactively offer suggestions\nto avoid personalized LLMs from becoming echo\nchambers (see Ethics Statement). An exciting fu-\nture direction is to continuously store user opinions\nand grow the memory of opinions.\nAn aligned LLM offers the benefit to offer per-\nsonalized perspectives that align with a user’s val-\nues and cultural beliefs. However, there exist cir-\ncumstances when LLMs can become an amplifier\nfor unethical and biased views. Our work lays\nthe foundation for a robust LLM alignment ap-\nproach. By using memory-based personalization\n5913\nand recording interactions saved in a growing mem-\nory, the model can inform future instances of the\nmost relevantpast opinions.\nLimitations\nNon-subjective questions. For non-subjective\nquestions, such as “How many years have you lived\nin ...”, it might not be necessary to use past opinions\n(see error analysis).\nLack of user information. In this work, we fo-\ncus on all past opinions on the same topic due to\na lack of availability of user information in the\ndataset. More insights can be derived by investi-\ngating users’ opinions on multiple different related\ntopics. We leave it as our future work.\nLack of temporal information. Another missing\naspect is time. User opinions change over time,\nbut the dataset available to us does not contain\ntimestamps. It would be interesting to see how\nconflicting user opinions can be modeled, perhaps\nby biasing toward the most recent opinion.\nEthics Statement\nData The dataset used in our work, OpinionQA\n(Santurkar et al., 2023) is publicly available. The\ndataset includes subjective opinions from humans\nand may contain offensive content to some people.\nModels The large language models we used for\nthe experiments are trained on a large-scale web\ncorpus and some of them utilize human feedback.\nThis may also bring some bias when predicting user\nanswers. With an aligned LLM, users can select\ninformation that adheres to their system of beliefs\nand to amplify potentially biased and unethical\nviews. Such an echo chamber (Del Vicario et al.,\n2016) can eventually cause harm by reinforcing\nundesirable or polarized a user’s views.\nA viable mitigation is to show user demography\nor ideology group answers in addition to the per-\nsonalized answer (e.g., showing how an average\nDemocrat with similar demographics would think\non this topic and why). Further, past opinions can\nbe used to ground an explanation (e.g., the current\npersonalized answer is influenced by a user’s spe-\ncific past opinion), thus offering an opportunity for\nthe user to introspect their past opinions.\nAcknowledgements\nWe thank the members of the Aristo team at AI2\nand Kurt Gray for their insightful feedback on this\nwork. EH was funded, in part, by the Vector In-\nstitute for AI, Canada CIFAR AI Chairs program,\nan NSERC discovery grant, and a research gift\nfrom AI2. BPM was funded, in part, by an Adobe\nResearch Fellowship.\nReferences\nAnte Busic-Sontic, Natalia V Czap, and Franz Fuerst.\n2017. The role of personality traits in green decision-\nmaking. Journal of Economic Psychology, 62:313–\n328.\nEugene Y . Chan and Mauricio Palmeira. 2021. Politi-\ncal ideology moderates consumer response to brand\ncrisis apologies for data breaches. Computers in\nHuman Behavior, 121:106801.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90% chatgpt\nquality.\nEric Chu, Jacob Andreas, Stephen Ansolabehere, and\nDeb Roy. 2023. Language models trained on media\ndiets can predict public opinion.\nJacob Cohen. 1960. A coefficient of agreement for\nnominal scales. Educational and Psychological\nMeasurement, 20:37 – 46.\nDavid Crockett and Melanie Wallendorf. 2004. The role\nof normative political ideology in consumer behavior.\nJournal of Consumer Research, 31:511–528.\nMichela Del Vicario, Gianna Vivaldo, Alessandro Bessi,\nFabiana Zollo, Antonio Scala, Guido Caldarelli, and\nWalter Quattrociocchi. 2016. Echo chambers: Emo-\ntional contagion and group polarization on facebook.\nScientific reports, 6(1):37825.\nAmeet Deshpande, Vishvak Murahari, Tanmay Rajpuro-\nhit, Ashwin Kalyan, and Karthik Narasimhan. 2023.\nToxicity in chatgpt: Analyzing persona-assigned lan-\nguage models. arXiv preprint arXiv:2304.05335.\nYao Fu, Litu Ou, Mingyu Chen, Yuhao Wan, Hao-Chun\nPeng, and Tushar Khot. 2023. Chain-of-thought hub:\nA continuous effort to measure large language mod-\nels’ reasoning performance. ArXiv, abs/2305.17306.\nYunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong,\nHaofen Wang, and Jiawei Zhang. 2023a. Chat-\nrec: Towards interactive and explainable llms-\naugmented recommender system. arXiv preprint\narXiv:2303.14524.\n5914\nYunfan Gao, Tao Sheng, Youlin Xiang, Yun Xiong,\nHaofen Wang, and Jiawei Zhang. 2023b. Chat-rec:\nTowards interactive and explainable llms-augmented\nrecommender system.\nXiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie,\nXia Hu, and Tat-Seng Chua. 2017. Neural collabora-\ntive filtering.\nLiwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ro-\nnan Le Bras, Maxwell Forbes, Jon Borchardt, Jenny\nLiang, Oren Etzioni, Maarten Sap, and Yejin Choi.\n2021. Delphi: Towards machine ethics and norms.\nArXiv, abs/2110.07574.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nJunsol Kim and Byungkyu Lee. 2023. Ai-augmented\nsurveys: Leveraging large language models for opin-\nion prediction in nationally representative surveys.\nMarco Lauriola and Irwin P Levin. 2001. Personality\ntraits and risky decision-making in a controlled ex-\nperimental task: An exploratory study. Personality\nand individual differences, 31(2):215–226.\nShuyang Li, Bodhisattwa Prasad Majumder, and Julian\nMcAuley. 2021. Self-supervised bot play for conver-\nsational recommendation with justifications.\nAman Madaan, Niket Tandon, Peter Clark, and Yiming\nYang. 2022. Memory-assisted prompt editing to im-\nprove GPT-3 after deployment. InProceedings of the\n2022 Conference on Empirical Methods in Natural\nLanguage Processing, pages 2833–2861, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nBodhisattwa Prasad Majumder, Shuyang Li, Jianmo\nNi, and Julian McAuley. 2019. Generating\npersonalized recipes from historical user prefer-\nences. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP),\npages 5976–5982, Hong Kong, China. Association\nfor Computational Linguistics.\nDavid McLellan. 1989. Simone Weil: utopian\npessimist. Springer.\nXiaoman Pan, Kai Sun, Dian Yu, Jianshu Chen, Heng\nJi, Claire Cardie, and Dong Yu. 2019. Improv-\ning question answering with external knowledge.\nIn Proceedings of the 2nd Workshop on Machine\nReading for Question Answering, pages 27–37,\nHong Kong, China. Association for Computational\nLinguistics.\nAlireza Salemi, Sheshera Mysore, Michael Bendersky,\nand Hamed Zamani. 2023. Lamp: When large lan-\nguage models meet personalization.\nShibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo\nLee, Percy Liang, and Tatsunori Hashimoto. 2023.\nWhose opinions do language models reflect? ArXiv,\nabs/2303.17548.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nSimone Weil. 1957. Écrits de Londres et dernières\nlettres. Gallimard.\nLeor Zmigrod, Ian Eisenberg, Patrick Bissett, Trevor\nRobbins, and Russell Poldrack. 2021. The cog-\nnitive and perceptual correlates of ideological at-\ntitudes: A data-driven approach. Philosophical\nTransactions of the Royal Society B: Biological\nSciences, 376:20200424.\n5915\nA Similar ideologies and different\nopinions\nWe show the percentage of user pairs having similar\nideologies and the percentages of user pairs having\nsimilar opinions and different opinions within the\nuser pairs sharing similar ideologies in Table 7.\nB Topicwise QA Accuracy\nWe show an overall topic-wise accuracy based on\nthe exact match for answer choice in Table 8.\nC Prompt\nWe provide a comprehensive display of all prompts\nused in the models incorporating user demograph-\nics, ideology, and opinions, which were employed\nfor individual-user level tests in Figure 5, 6 and 7.\nAdditionally, we present the prompts utilized for\nexperiments conducted at the group-level tests in\nFigure 8 and 9.\nD Error Examples\nWe give additional error examples in Figure 10, 11,\nand 12, where opinions either confuse the model\nwith relevant information or provide no useful in-\nformation.\nFigure 10 shows the second common error,\nwhich is when Relevant information in opinions\nconfuses the model. For example, when the ques-\ntion is “Is what you know about dietitians because\nyou have heard or read about this in the news” and\nthe user’s past opinions contain “I did not learn\nabout dietitians in my job.”. In this case, while the\nuser’s answer to the question is “Yes, have heard or\nread about this in the news”, the model produces\n“No, have not heard or read about this in the news”\nbecause of the opinions provided.\nThe third most common error is with Irrelevant\ninformation as shown in Figure11. Some opinions\ndo not contain useful information at all to reason\nfor the correct answer. Lastly, there are some ques-\ntions that opinions are not important to predict the\nanswer (e.g. “About how many years have you\nlived in your local community?”) as presented. In\nthis case, the model may make an arbitrary predic-\ntion.\nE Additional related work comparison\nWe show additional related works that can be com-\npared to our paper in Table 9.\nA person can be described as follows:\nAge: 30-49\nIncome: 75, 000−100,000\nPolitical ideology: Conservative\nPolitical party: Republican\nReligion: Roman Catholic\n...\nThe person has the following opinions on Guns.\nOpinions:\n1. The most important reason why I own a\ngun is for sport shooting, including target\nshooting and trap and skeet.\n2. The ease with which people can illegally\nobtain guns contributes to gun violence in\nthe country today.\n...\nBased on the above list of opinions and the\ndemographic information, which answer choice\nwill this person select for the question:\nQuestion: Thinking about gun owners who do\nnot have children in their home how important\ndo you think it is for them to: Take gun\nsafety courses\nAnswer choices:\nA. Essential\nB. Important but not essential\nC. Not important\nD. Should not be done\nAnswer:\nFigure 5: Prompt using demographics, ideology, and\nGPT embeddings based top-kpast opinions to predict\nthe answer to a question.\nA person has the following opinions on Guns.\nOpinions:\n1. < opinion1 >\n2. < opinion2 >\n...\nBased on the above list of opinions, which\nanswer choice will this person select for the\nquestion:\nQuestion: < question >\nAnswer choices:\nA.< choice1 >\nB.< choice2 >\nC.< choice3 >\n...\nAnswer:\nFigure 6: prompt for implicit-only model\n5916\nTopic similar ideol. user pair (%) similar ideol. & op. (%) similar ideol.-diff. op. (%)\nGuns 16.05 52.67 47.33\nAutomation 15.99 15.26 84.74\nViews on gender 16.26 39.58 60.42\nSexual harassment 16.95 22.14 77.86\nBiomedical, food 15.69 13.44 86.56\nGender, Leadership 17.52 50.25 49.75\nAmerica in 2050 15.34 30.21 69.79\nTrust in Science 15.53 26.27 73.73\nRace 16.32 21.05 78.95\nMisinformation 16.22 34.83 65.17\nPrivacy, Surveillance 16.13 22.14 77.86\nFamily, Relationships 17.01 48.88 51.12\nEconomic inequality 16.24 39.25 60.75\nGlobal attitudes 16.75 47.78 52.22\nPolitical views 16.65 37.59 62.41\nTable 7: Percentage of user pairs sharing similar ideologies (similar ideol. user pair) and the percentages of similar\nopinions (similar ideol. & op.) and different opinions (similar ideol.-diff. op.) within user pairs sharing similar\nideologies.\nAccuracy with exact match\nNo Persona Demo. + Ideo. Demo. + Ideo.+ Opinion top8\nGuns 0.40 ± 0.08 0.51 ± 0.13 0.63 ± 0.14\nAutomation 0.44 ± 0.10 0.49 ± 0.12 0.48 ± 0.09\nViews on gender 0.43 ± 0.09 0.44 ± 0.10 0.57 ± 0.09\nSexual harassment 0.40 ± 0.10 0.44 ± 0.09 0.47 ± 0.09\nBiomedical, food 0.51 ± 0.12 0.55 ± 0.12 0.60 ± 0.10\nGender, Leadership 0.50 ± 0.13 0.45 ± 0.11 0.59 ± 0.13\nAmerica in 2050 0.43 ± 0.12 0.41 ± 0.12 0.46 ± 0.11\nTrust in science 0.52 ± 0.12 0.50 ± 0.10 0.59 ± 0.09\nRace 0.38 ± 0.12 0.42 ± 0.12 0.51 ± 0.10\nMisinformation 0.48 ± 0.12 0.48 ± 0.12 0.54 ± 0.12\nPrivacy, Surveillance 0.36 ± 0.10 0.42 ± 0.12 0.51 ± 0.11\nFamily, Relationships 0.46 ± 0.10 0.49 ± 0.12 0.57 ± 0.12\nEconomic inequality 0.38 ± 0.10 0.47 ± 0.09 0.55 ± 0.08\nGlobal attitudes 0.38 ± 0.12 0.44 ± 0.15 0.48 ± 0.10\nPolitical views 0.41 ± 0.13 0.51 ± 0.11 0.52 ± 0.10\nTable 8: Overall topic-wise accuracy based on exact match and collapsed match for answer choices. (demo.:\ndemographic, ideo.: ideology, op.: opinions). GPT-3 model is used for the experiment.\nUser-profile\nexplicitly observed\nModeling\nindividuals\nSupervision-free\nPersonalized generation: OpinionQA (Santurkar\net al., 2023), RecipeGen (Majumder et al., 2019),\nLAMP (Salemi et al., 2023)\n✗ ✗ or ✓\n(mostly group)\n✗ or ✓\nRecommender Systems : ChatRec (Gao et al.,\n2023b), Collaborative Filtering (He et al., 2017),\nBotPlay (Li et al., 2021)\n✗ or ✓\n(mostly latent)\n✓ ✗ or ✓\n(mostly supervised)\nOurs ✓ ✓ (+ group) ✓\nTable 9: Placement of our work w.r.t. related work\n5917\nA person can be described as follows:\nAge: < age >\nEducation: < education >\n...\nBased on the demographic information, which\nanswer choice will this person select for the\nquestion:\nQuestion: < question >\nAnswer choices:\nA.< choice1 >\nB.< choice2 >\nC.< choice3 >\n...\nAnswer:\nFigure 7: prompt for demographic-only model\nThinking of yourself as a\n[republican/independent/democrat], please\nselect the right choice.\n< question >\nChoice: [choice1, choice2, choice3]\nAnswer:\nFigure 8: prompt for group-level ideology test\nThinking of yourself as a person, please\nselect the right choice.\n< question >\nChoice: [choice1, choice2, choice3]\nAnswer:\nFigure 9: prompt for LLM majority answer test\nQuestion:\nIs what you know about dietitians because you\nhave heard or read about this in the news\nChoices:\nYes, have heard or read about this in the\nnews\nNo, have not heard or read about this in the\nnews\nRefused\nOpinions:\nI did not learn about dietitians in my job.\n...\nDemographics and Ideology:\nPolitical party: Republican,\nRace: White,\nReligion: Protestant,\n...\nUser-answer (expected): Yes, have heard or\nread about this in the news\nDemo.-ideology-opinions: No, have not heard\nor read about this in the news ✗\nDemo.-ideology: Yes, have heard or read about\nthis in the news ✓\nFigure 10: An example of a relevant opinion confusing\nthe model.\n5918\nQuestion:\nHow likely do you think it is that the\nfollowing will happen in the next 30 years?\nChina will overtake the U.S. as the world’s\nmain superpower\nChoices:\nYes, have heard or read about this in the\nnews\nNo, have not heard or read about this in the\nnews\nRefused\nOpinions: (irrelevant)\nPopulation growth in the U.S. will be a major\nproblem in 2050.\nIt is probable that there will be increasing\nviolence against Jews in the U.S. in the next\n30 years.\n...\nDemographics and Ideology:\nCitizenship: Yes, Political party: Moderate,\nRace: White,\nRegion: Midwest,\n...\nUser-answer (expected): Will probably not\nhappen\nDemo.-ideology-opinions: Will probably happen\n✗\nDemo.-ideology: Will probably not happen ✓\nFigure 11: An example of opinions containing no useful\ninformation to predict the answer.\nQuestion:\nHow well, if at all, do the following words\nor phrases describe you? Supporter of the\nBlack Lives Matter movement\nChoices:\nDescribes me well\nDoes not describe me well\nRefused\nOpinions:\nThe words or phrases \"supporter of rights for\nLGBT people\" do not describe me well.\n...\nDemographics and Ideology:\nPolitical party: Democrat,\nRace: Black,\nReligion: Protestant,\n...\nUser-answer (expected): Describes me well\nDemo.-ideology-opinions: Does not describe\nme well ✗\nDemo.-ideology: Describes me well ✓\nFigure 12: An example of a not relevant opinion confus-\ning the model\n5919",
  "topic": "Persona",
  "concepts": [
    {
      "name": "Persona",
      "score": 0.872545599937439
    },
    {
      "name": "Ideology",
      "score": 0.8329808712005615
    },
    {
      "name": "Demographics",
      "score": 0.8108273148536682
    },
    {
      "name": "Public opinion",
      "score": 0.5582305788993835
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5572696328163147
    },
    {
      "name": "Computer science",
      "score": 0.5552727580070496
    },
    {
      "name": "User group",
      "score": 0.511744499206543
    },
    {
      "name": "Work (physics)",
      "score": 0.45892196893692017
    },
    {
      "name": "Data science",
      "score": 0.4437256455421448
    },
    {
      "name": "Internet privacy",
      "score": 0.3754844069480896
    },
    {
      "name": "Human–computer interaction",
      "score": 0.30942362546920776
    },
    {
      "name": "World Wide Web",
      "score": 0.2975311279296875
    },
    {
      "name": "Political science",
      "score": 0.2523198425769806
    },
    {
      "name": "Sociology",
      "score": 0.18348857760429382
    },
    {
      "name": "Engineering",
      "score": 0.13516244292259216
    },
    {
      "name": "Politics",
      "score": 0.08010753989219666
    },
    {
      "name": "Demography",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141945490",
      "name": "University of British Columbia",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210127509",
      "name": "Vector Institute",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I4210140341",
      "name": "Allen Institute",
      "country": "US"
    }
  ]
}