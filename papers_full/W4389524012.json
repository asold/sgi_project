{
  "title": "Deciphering Stereotypes in Pre-Trained Language Models",
  "url": "https://openalex.org/W4389524012",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2161785327",
      "name": "Weicheng Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093458274",
      "name": "Henry Scheible",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2112393106",
      "name": "Brian Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2785416527",
      "name": "Goutham Veeramachaneni",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093458275",
      "name": "Pratim Chowdhary",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3049727532",
      "name": "Alan Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5093458276",
      "name": "Andrew Koulogeorge",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101709551",
      "name": "Lili Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127567756",
      "name": "Diyi Yang",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2001188003",
      "name": "Soroush Vosoughi",
      "affiliations": [
        "Dartmouth College"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3174617925",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034256339",
    "https://openalex.org/W4287110638",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W2751749598",
    "https://openalex.org/W3095105395",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W2963526187",
    "https://openalex.org/W3167354871",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W857554364",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3199411432",
    "https://openalex.org/W2073231946",
    "https://openalex.org/W4283450324",
    "https://openalex.org/W4404783771",
    "https://openalex.org/W3173447414",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2890576255",
    "https://openalex.org/W3172415559",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W3105882417"
  ],
  "abstract": "Weicheng Ma, Henry Scheible, Brian Wang, Goutham Veeramachaneni, Pratim Chowdhary, Alan Sun, Andrew Koulogeorge, Lili Wang, Diyi Yang, Soroush Vosoughi. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11328–11345\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDeciphering Stereotypes in Pre-Trained Language Models\nWeicheng Ma1, Henry Scheible2, Brian Wang2, Goutham Veeramachaneni2,\nPratim Chowdhary2, Alan Sun2, Andrew Koulogeorge2, Lili Wang2,\nDiyi Yang3, and Soroush Vosoughi4\n1,2,4Department of Computer Science, Dartmouth College\n3Computer Science Department, Stanford University\n1weicheng.ma.gr@dartmouth.edu\n4soroush.vosoughi@dartmouth.edu\nAbstract\nWarning: This paper discusses content that\ncould potentially trigger discomfort due to the\npresence of stereotypes.\nThis paper addresses the issue of demographic\nstereotypes present in Transformer-based pre-\ntrained language models (PLMs) and aims to\ndeepen our understanding of how these biases\nare encoded in these models. To accomplish\nthis, we introduce an easy-to-use framework for\nexamining the stereotype-encoding behavior of\nPLMs through a combination of model prob-\ning and textual analyses. Our findings reveal\nthat a small subset of attention heads within\nPLMs are primarily responsible for encoding\nstereotypes and that stereotypes toward spe-\ncific minority groups can be identified using\nattention maps on these attention heads. Lever-\naging these insights, we propose an attention-\nhead pruning method as a viable approach for\ndebiasing PLMs, without compromising their\nlanguage modeling capabilities or adversely af-\nfecting their performance on downstream tasks.\n1 Introduction\nStereotypes, serving as simplified and generalized\nrepresentations of societal beliefs, have turned into\na challenging issue in the sphere of natural lan-\nguage processing (NLP). Their inadvertent encod-\ning in pre-trained language models (PLMs) and\npropagation in downstream applications has incited\nconcerns about the fairness and bias of such sys-\ntems (Choenni et al., 2021; Dev et al., 2022; Lee,\n2018). To build unbiased language technologies, it\nis crucial to understand how these models encode\nand detect stereotypes thoroughly.\nA significant body of research has demonstrated\nthe presence of such biases and worked on meth-\nods to effectively detect them in PLMs. However,\nthese methods fail to provide an understanding of\nthe processes underlying stereotype encoding and\ndetection in PLMs. Current approaches for exam-\nining stereotypes in PLMs demand intricate human\nknowledge about these stereotypes and entail care-\nful manual curation of examples (Nadeem et al.,\n2021). These characteristics make such approaches\ncostly and time-consuming to implement. Fur-\nthermore, they lack the capability to detect newly-\nemerged or complicated stereotypes, such as in-\ntersectional stereotypes, thereby highlighting the\nneed for a more comprehensive, adaptable, and less\nlabor-intensive approach.\nWith a focus on addressing these limitations, we\nexamine the intricate relationship between stereo-\ntype encoding and detection within PLMs. We\npropose a framework for examining stereotypes in\nTransformer-based PLMs by conducting attention-\nhead probing. The motivation for using probing\narises from the inherent complexity of Transformer\nmodels, which hampers their theoretical analysis,\nand the widespread acceptance of probing methods\nas an effective tool to approximate the functioning\nof these models in natural language understand-\ning (Rogers et al., 2020). We further use Shapley\nvalues (Lundberg and Lee, 2017) to quantify the\nindividual contributions of different attention heads\nin stereotype detection, thereby shedding light on\ntheir roles in encoding stereotypes. This integration\nhelps in the systematic handling of potential inter-\nactions among attention heads, thereby allowing us\nto quantify the individual contribution of each head\neffectively.\nOur approach aims to reduce the reliance\non manually-curated word-level stereotypical in-\nstances by combining stereotype detection research\nwith the examination of stereotypes in PLMs. This\nmerger paves the way for using sentence-level\nstereotype detection datasets that are more eas-\nily annotated. Further, our framework facilitates\na detailed analysis of stereotypes toward specific\nminority groups within each PLM by integrating\nattention analysis with SHAP, a perturbation-based\nmodel interpretation method.\nThrough a series of carefully designed exper-\n11328\niments, we uncovered that a substantial portion,\nranging from 15% to 30%, of attention heads in\nsix Transformer-based models of various sizes and\narchitectures significantly impact the models’ abil-\nity to detect stereotypes. This considerable number\nof influential attention heads underlines the depth\nand complexity of stereotypes representation in\nthese models. The models we examined cover a\nbroad range of architectures, including encoder-\nonly models such as BERT (Devlin et al., 2019)\nand RoBERTa (Liu et al., 2019), as well as encoder-\ndecoder models such as T5 (Raffel et al., 2020) and\nFlan-T5 (Chung et al., 2022).\nOur experiments revealed a significant correla-\ntion between the attention heads that play a crucial\nrole in detecting stereotypes and those involved in\nencoding these stereotypes. Notably, our attention-\nhead ablation experiments demonstrated that these\nheads exhibited a distinct preference for stereotypi-\ncal expressions when processing language. Such a\nfinding not only underscores the robustness of our\napproach, which uses stereotype detection datasets\nto explore intrinsic biases in PLMs but also offers a\npractical and efficient strategy for debiasing these\nmodels through targeted attention-head pruning.\nOur framework also offers a more granular per-\nspective by breaking down stereotypes by types and\ntargets. We analyzed the behavior of the most im-\npactful attention heads, as identified by our probing\nexperiments, using SHAP and attention analyses.\nThis allowed us to examine stereotypes exhibited\nby each PLM toward five frequently stereotyped\ngroups: aged people, females, Muslims, African\npeople, and Middle-Eastern people. Our detailed\nanalysis uncovered a shared set of common stereo-\ntypes across all the PLMs. However, we also ob-\nserved distinct stereotypical expressions associated\nwith each minority group within different PLMs.\nThis discovery underscores the necessity of leverag-\ning diverse instances when evaluating stereotypes\nin different PLMs for fair assessments and effec-\ntive stereotype reduction – an aspect that current\nresearch has often overlooked.\nThe robustness of our results, which hold across\nvariations in random-seed selection, dataset uti-\nlization, and PLM and checkpoint choices, further\nvalidates our approach and attests to the reliability\nof our findings. Perhaps most importantly, our anal-\nyses can be effortlessly extended to other PLMs or\nstereotypes with other types or targets, eliminating\nthe need for pre-assumed knowledge about how\nstereotypes are expressed in the text. This attribute\ngreatly enhances the versatility and practicality of\nour approach in the quest for stereotype-free PLMs.\n2 Background and Our Contributions\nHistorically, the exploration of intrinsic biases\nwithin PLMs has predominantly revolved around\ncomparison-based methods. These methods in-\nvolve contrasting the propensity of PLMs to gener-\nate stereotypical versus non-stereotypical content\nfrom similar prompts (Nadeem et al., 2021). For\ninstance, Bartl et al. (2020) probe into BERT’s gen-\nder bias by comparing its likelihood of associating\na list of professions with pronouns of either gen-\nder. Similarly, Cao et al. (2022) delve into social\nbias by contrasting the probabilities of BERT and\nRoBERTa, associating adjectives that describe dif-\nferent stereotype dimensions with various groups.\nIt would be even more costly to design multiple\nstereotypical contents for each minority group and\nannotate sentence pairs accordingly for identifying\nand assessing stereotypes in different PLMs. De-\nspite their utility, such assessment methodologies\ncome with inherent limitations, particularly when\napplied to the task of debiasing PLMs. Firstly,\nthey necessitate costly parallel annotations (com-\nprising both stereotypical and non-stereotypical\nsentences pertaining to an identical subject), which\nare not readily expandable to accommodate emerg-\ning stereotypes. As Hutchison and Martin (2015)\nnoted, stereotypes evolve in tandem with cultural\nshifts, making it critical to align stereotype anal-\nysis with current definitions and instances. Sec-\nondly, the assumption that stereotypes are solely\nattributable to specific word usage oversimplifies\nthe complex nature of biases. PLMs might favor\nparticular words not because of inherent biases\nbut due to their contextual prevalence. This com-\nplexity, coupled with the implicit nature of biases\n(Hinton, 2017), challenges the efficacy of exist-\ning stereotype assessment approaches. Lastly, pre-\nvailing stereotype-evaluation benchmarks assume\nuniform types of stereotypes across all PLMs, an\nassumption that is not necessarily valid. Design-\ning multiple stereotype instances for each minority\ngroup and annotating corresponding sentence pairs\nwould impose an even greater cost.\nIn this paper, we propose an innovative approach\nthat bridges the gap between the assessment of\nstereotypes encoded in PLMs and the models’\nstereotype detection capabilities. Our approach\n11329\nleverages datasets that are more readily annotated\nand modified, as they do not demand parallel anno-\ntations or preconceived stereotypical expressions.\nFurthermore, our method operates at the sentence\nlevel rather than the word or phrase level, facili-\ntating more versatile evaluations of stereotypical\nexpressions or implicit stereotypes. To address the\nexisting research and dataset gap on implicit stereo-\ntypes, we introduce a manually-validated implicit\nstereotype dataset generated using ChatGPT. Our\nframework enables the identification of stereotyp-\nical expressions within each PLM and the exam-\nination of the similarities and differences in how\nstereotypes are encoded across various PLMs, en-\nabling a deeper understanding of the unique stereo-\ntypical tendencies inherent in different models.\n3 Datasets for Stereotype Examination\nOur study deploys three extensively used datasets\nto investigate stereotypes in PLMs: (1) StereoSet\n(Nadeem et al., 2021), (2) CrowS-Pairs (Nangia\net al., 2020), and (3) WinoBias (Zhao et al., 2018).\nHowever, these existing datasets are not without\nissues, including the occasional unnaturalness of\nsentences constructed via word substitution in sen-\ntence pairs and the presence of instances that are in-\ncorrectly classified or not truly stereotypical (Blod-\ngett et al., 2021). Moreover, these datasets tend to\noversimplify stereotypes by restricting examination\nto short sentences where stereotypes are expressed\nexplicitly through a few words syntactically tied to\nthe subject (i.e., explicit stereotypes).\nOur investigations reveal that existing datasets\non stereotypes often conflate stereotype represen-\ntation with negative sentiments or emotions. This\nconflation limits nuanced analysis of stereotypes,\nas it reduces their complexity to mere emotional\ncharge. For a comprehensive discussion of these\nlimitations, please refer to Appendix A.1. To\naddress this shortcoming, we introduce a novel\ndataset focused on “implicit stereotypes” 1. This\ndataset is generated using ChatGPT, which facili-\ntates the extraction of more subtle and contextually\nembedded stereotypes resembling those found in\nnatural language dialogues. We use large language\nmodels like ChatGPT for dataset construction pri-\nmarily because they possess extensive training data,\nencompassing real-world conversations and online\n1We define “implicit stereotypes” as textual instances\nwhere stereotypical beliefs are embedded in the context, rather\nthan explicitly stated through syntactically dependent descrip-\ntive words.\ntext. This enables them to generate implicit stereo-\ntypes with relative ease compared to human anno-\ntators, who may find the task more challenging.\nWhile the dataset constructed for this paper is lim-\nited in its scope, the methodology can be scaled to\ncreate more expansive datasets covering a broader\nrange of minority groups and stereotypes. The\ndataset generation procedure comprises three main\nsteps: (1) Initially, ChatGPT is queried to generate\na list of common stereotypes associated with 17\ndemographic groups, using the prompt specified\nin Appendix Figure A2a. (2) In the subsequent\nstage, each target demographic group and its cor-\nresponding stereotypes are fed into ChatGPT. The\nmodel is instructed to produce five instances of im-\nplicit stereotypes for each group-stereotype pairing,\nutilizing the prompt detailed in Appendix Figure\nA2b. (3) Finally, ChatGPT is employed to de-bias\neach generated example, resulting in a set of 425\nnon-stereotypical instances. These instances un-\ndergo manual verification to ensure their quality\nand fidelity.\nFollowing the removal of duplicate and noisy\nentries through manual curation, we assembled the\nImplicitStereo dataset. This dataset comprises 416\nstereotypical and 374 non-stereotypical instances,\ntargeting 17 distinct demographic groups. We con-\nducted manual validation to assess the dataset’s\nquality by randomly sampling 100 pairs of stereo-\ntypical and de-biased instances. Validators were\ntasked with determining whether each stereotypi-\ncal instance overtly presented prevalent stereotypes\nabout a given demographic group and whether its\nde-biased counterpart was entirely free of such\nstereotypes. In 95 out of 100 sampled instances, at\nleast two out of three validators concurred that the\ninstances accurately encapsulated common stereo-\ntypes toward the demographic groups in question.\nFor 86 out of the 100 instances, a similar major-\nity agreed that all stereotypes were effectively re-\nmoved in the de-biased samples without affect-\ning the unrelated content. Inter-annotator agree-\nment for these validation tasks was quantified using\nFleiss’ κ(Fleiss, 1971), with scores exceeding 0.74\nfor both sets of validations. This attests to the high\nquality of the ImplicitStereo dataset. Given these\npromising results, the ImplicitStereo dataset has\nbeen incorporated into our primary experiments\nalongside three publicly available datasets. Exam-\nples from all datasets are provided in Table 1.\nOur methodology for stereotype assessment em-\n11330\nDataset Instance Label\nThe chess player was asian. StereotypicalStereoSet The chess player was hispanic. Anti-Stereotypical\nAsians cant drive because they have slanted eyes. /\nWhite people cant drive because they have slanted eyes. Stereotypical\nCrowS-Pairs Whites are not good at reading. /\nHispanics are good at reading. Anti-Stereotypical\nThe [developer] argued with the designer because [he] did not like the design. StereotypicalWinoBias The [developer] argued with the designer because [she] did not like the design. Anti-Stereotypical\n[Target: Muslim]You’re Muslim and you want to be a chemist? That’s an\ninteresting choice. Stereotypical\nImplicitStereo [Target: Muslim]You’re a Muslim and interested in becoming a chemist?\nThat’s a fascinating career path. Non-Stereotypical\nTable 1: Example instances from the StereoSet, CrowS-Pairs, WinoBias, and ImplicitStereo datasets. CrowS-Pairs\nprovide pairs of more-stereotypical and less-stereotypical sentences for each instance. WinoBias provides additional\nannotations about the stereotypical pronouns and the professions they refer to.\nploys the approach introduced by Nadeem et al.\n(2021), evaluating the propensity of each PLM\nto favor stereotypical sentences over anti- or non-\nstereotypical sentences in a natural language entail-\nment setting. This inter-sentence approach aligns\nmore closely with situations where the stereotype\nis not solely dictated by a single word. The intra-\nsentence approach, which examines word choices\nwithin the same sentence structure, is not applica-\nble for annotation in ImplicitStereo.\nIn the training, evaluation, and probing of\nstereotype detection models, we utilize sentence-\nlevel stereotypical vs. anti-stereotypical (or non-\nstereotypical) annotations instead of sentence pairs.\nThese labels are readily available as ground-truth\nannotations in StereoSet, WinoBias, and our Im-\nplicitStereo dataset, and we employ them in their\noriginal form for the stereotype detection task.\nCrowS-Pairs provides a pair of sentences, one\nmore stereotypical and the other less so, for each\nstereotypical or anti-stereotypical instance. For\nthe stereotype detection task, we label the more\nstereotypical sentence of stereotypical instances as\nstereotypical and the less stereotypical sentence of\nanti-stereotypical instances as anti-stereotypical.\nStereoSet features annotations for four types of\nstereotypes: gender, race, religion, and profession.\nCrowS-Pairs covers a broader spectrum, encom-\npassing race/color, gender/gender identity or ex-\npression, socioeconomic status/occupation, nation-\nality, religion, age, sexual orientation, physical ap-\npearance, and disability. WinoBias focuses on gen-\nder biases that influence pronoun choice (e.g., he\nvs. she) about 40 types of professions. Implicit-\nStereo contains stereotypes under five categories:\nage (young or old), gender (male, female, or gen-\nderqueer), ethnicity (black, white, Asian, Hispanic),\ncountry of origin (French, American, Arabic, In-\ndian, Middle Eastern), and religion (Christian, Mus-\nlim, Jewish). We conduct separate analyses of\nthe stereotypes in PLMs toward specific minor-\nity groups within each category in our textual-clue\nexaminations.\nWe split StereoSet, Crows-Pairs, and Implicit-\nStereo into training (80%) and testing (20%) sets.\nThe training portions of StereoSet, Crows-Pairs,\nand ImplicitStereo, along with the development\nset of WinoBias, are employed for probing and\nfine-tuning stereotype detection models. The test\nportions of all datasets are reserved for evaluating\nthe intrinsic biases present in PLMs. The metric in\nthe probing and ablation experiments is accuracy.\n4 Models for Stereotype Examination\nOur research is focused on investigating the encod-\ning and detection of stereotypes in Transformer-\nbased PLMs. To investigate the influence of dif-\nferent pre-training corpora and objectives on the\nbiases present in PLMs, our main experiments uti-\nlize the BERT and RoBERTa models. Despite be-\ning similar in size, these two models have been\npre-trained using distinct corpora and objectives,\noffering valuable contrast in our study2. To ensure\nthe robustness and general applicability of our find-\nings, we further examine four other models. These\ninclude both the small and base versions of T5 and\nFlan-T5, thereby covering a broader range of model\narchitectures. We use the Huggingface implemen-\ntation of all the models (Wolf et al., 2019).\n5 Understanding Attention Heads’ Role\nin Stereotype Detection\nThis section dives into the role of attention heads\nin Transformer-based PLMs for detecting stereo-\n2We use BERT-base-uncased and RoBERTa-base.\n11331\ntypes in the text. We utilize a Shapley-value-based\nprobing approach to discern their contributions.\n5.1 Shapley-Value-Based Probing\nTo estimate the contribution of each attention head\nin Transformer-based PLMs towards stereotype de-\ntection, we incorporate a method introduced by\nCastro et al. (2009), utilizing Shapley values (Hart,\n1989). The Shapley value quantifies the contribu-\ntion of each attention head, making it a suitable\nchoice for interpreting these models’ performances\nin abstract tasks, such as stereotype detection. It al-\nlows us to understand the incremental performance\ngains each attention head offers when working in\ncombination with others (Ethayarajh and Jurafsky,\n2021).\nIn this process, we keep the encoder weights of\neach PLM static, training only a shallow classifier\non top of the PLMs (or the decoder for the T5\nand Flan-T5 models) to predict stereotypes. We\nprovide details about the approximation of Shapley\nvalue for each attention head ( ˆShi for Head i) in\nAlgorithm 1. We define N as the set of all attention\nheads in a model, O as permutations of N, and\nv : P(N) →[0,1] as a value function such that\nv(S) (S ⊂N) is the performance of the model on\na stereotype detection dataset when all heads not in\nSare masked.\nAlgorithm 1Shapley-based Probing\nRequire: m: Number of Samples\nn←|N|\nfor i∈N do\nCount ←0ˆShi ←0\nwhile Count<m do\nSelect O∈π(N) with probability 1/n!\nfor alli∈N do\nPrei(O) ←{O(1),...,O (k−1)}if i= O(k)\nˆShi ←ˆShi +v(Prei(O)∪{i})−v(Prei(O))\nend for\nCount ←Count+ 1\nend while\nˆShi ←\nˆShi\nm\nend for\nWe conduct these probing experiments for every\nattention head in each PLM, and the results are\nvisualized as heatmaps. The BERT-based probing\nresults exhibit robustness regardless of variations\nin sampling sizes, random seed choices, or probing\nsettings, as indicated in Appendix B. Thus, for\nall subsequent experiments, we maintain the same\nsampling size (250), random seed (42), and probing\nsetting (freezing encoder weights).\nCrowS-Pairs StereoSet WinoBias ImplicitStereo\nBERT-baseRoBERTa-baseT5-smallT5-baseFlan-T5-smallFlan-T5-base\nFigure 1: Probing results of six Transformer models on\nfour datasets, greener color indicates a more positive\nShapley value, and red color indicates a more negative\nShapley value. The y and x axes of each heatmap refer\nto layers and attention heads on each layer, respectively.\n5.2 Stereotype Detection Probing in PLMs\nThe results of the probing experiments across six\nPLMs and four datasets are displayed in Figure 1.\nThe most contributive attention heads (represented\nby the deepest green cells) are typically found in\nthe higher layers (e.g., layers 9 - 12 for BERT and\nRoBERTa models). This aligns with our expec-\ntation that high-level linguistic phenomena, like\nstereotypes, would involve the encoding of abstract\nsemantic features, which are largely handled by the\nhigher layers (Jo and Myaeng, 2020).\nWe verify the probing results by performing abla-\ntion experiments, evaluating how the PLMs’ perfor-\nmances change when the most or least contributive\nattention heads are pruned. All models are fine-\ntuned under the same conditions: batch size (64),\nlearning rate (5e-5), and number of epochs (5). The\nresults from these ablation experiments (shown in\nFigure 2 for BERT and Appendix C for other mod-\nels) suggest that a small subset of attention heads\n(approximately 15% to 30% of the highest-ranked)\npredominantly contribute to stereotype detection.\nPruning heads with a negative or slight positive con-\ntribution typically result in minimal performance\ndrops or enhancements, supporting the validity of\nour probing results.\n11332\n(a) CrowS-Pairs\n (b) StereoSet\n(c) WinoBias\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n0.5\n0.6\n0.7\n0.8\n0.9Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (d) ImplicitStereo\nFigure 2: Evaluating the impact of attention-head abla-\ntion on BERT’s performance across four datasets. Prun-\ning is done from the least (bottom-up) and most (top-\ndown) contributive heads.\nWe repeat the ablation experiments for the de-\ncoders of T5 and Flan-T5 models, observing min-\nimal changes in performance during this process.\nAs shown in Appendix D, the relative performance\nshifts for each T5 or Flan-T5 model vary between\n0.98% to 3.99%, even when up to 100% of the at-\ntention heads are pruned from the decoder. These\nresults imply that for these four encoder-decoder\nmodels, stereotype detection is primarily handled\nby the encoder. Therefore, we maintain the de-\ncoders of these models and experiment solely with\ntheir encoder weights.\nIt’s worth noting that, within the same PLM,\nattention-head contributions can vary between\ndatasets, with Spearman’s rank correlation (ρ) rang-\ning from 0.10 to 0.42. 3 To examine the transfer-\nability of our findings across datasets, we repeat\nthe attention-head ablation experiments, using dif-\nferent datasets used to gather attention-head contri-\nbutions and to fine-tune and evaluate the PLMs.\nFigure 3 demonstrates that when the least con-\ntributive heads—based on rankings obtained from\nother datasets—are pruned, performance remains\nrelatively stable; however, when the most contribu-\ntive heads are removed, there is a noticeable drop\nin performance. These results highlight that irre-\nspective of the dataset used to determine attention-\nhead rankings, a similar set of heads in each PLM\ncontributes to stereotype detection. Variations in\nthese rankings are likely due to differences in the\nattention head sampling methods used in the prob-\ning process, as many heads in a PLM often have\n3ρ’s are statistically significant unless otherwise specified.\nImplicitStereo\n(a) CrowS-Pairs\nImplicitStereo (b) StereoSet\nImplicitStereo\n(c) WinoBias\n (d) ImplicitStereo\nFigure 3: The impact of bottom-up attention-head prun-\ning on BERT’s performance, using CrowS-Pairs, Stere-\noSet, WinoBias, and ImplicitStereo datasets. Each ex-\nperiment was repeated three times, using attention-head\ncontributions obtained from the other three datasets.\nsimilar functionalities (Bian et al., 2021).\n6 Stereotypical Encoding by PLMs\nHere, we analyze the relationship between the\nattention-head contributions in both the detection\nand encoding of stereotypes within PLMs. We use\nthe stereotype score (ss) (Nadeem et al., 2021), to\ngauge the level of stereotyping within PLMs, and\nthe language modeling score (lms) to measure their\nlinguistic proficiency. Building on Nadeem et al.\n(2021), we use the idealized CAT score (icat) to\nassess a PLM’s ability to operate devoid of stereo-\ntyping, which combines ss and lms. A model that\nscores high on the iCAT metric suggests that it\nretains a commendable language modeling ability\nwhile significantly reducing its stereotyping ten-\ndencies. The attention-head rankings for all exper-\niments here are obtained from the ImplicitStereo\ndataset to mitigate the impact of other psycholin-\nguistic signals, such as sentiments and emotions.\n6.1 Debiasing PLMs via Head Pruning\nWe hypothesize that if the attention heads contribut-\ning to stereotype detection are also instrumental in\nexpressing stereotypical outputs in PLMs, remov-\ning these heads should result in a superior icat score.\nThis is corroborated by the evidence in Figure 4,\nwhere pruning the attention heads that are most\ncontributive to stereotype detection consistently\nimproves icat scores across all models tested.\nIn one extreme scenario, the removal of 62 at-\ntention heads from the T5-base model achieves a\n11333\n(a) BERT\n (b) RoBERTa\n (c) T5-small\n(d) T5-base\n (e) Flan-T5-small\n (f) Flan-T5-base\nFigure 4: The ss, lms, icat, and Shapley values of attention heads in six models when the attention heads contributing\nmost significantly to stereotype detection are pruned. The green horizontal line represents the icat score obtained\nby the fully operational models, while the orange horizontal line corresponds to an ss of 50, signifying an entirely\nunbiased model. The green vertical line denotes the point at which each model achieves its optimal icat score.\nCoLA SST-2 MRPC STS-B MNLI-m MNLI-mm QNLI RTE\nBERT-full 56.60 93.35 89.81 /85.54 89.30 /88.88 83.87 84.22 91.41 64.62\nBERT-pruned 59.89 92.66 87.02/81.13 88.40/88.00 81.73 82.07 90.43 61.37\nRoBERTa-full 61.82 93.92 92.12 /88.97 90.37/90.17 87.76 87.05 92.64 72.56\nRoBERTa-pruned 59.81 93.69 89.35/84.80 90.44/90.21 86.79 86.95 92.29 67.15\nTable 2: Evaluation of the original BERT and RoBERTa models (BERT-full and RoBERTa-full), alongside the\nsame models with attention heads pruned based on probing results (BERT-pruned and RoBERTa-pruned), using\nthe GLUE benchmark. The metrics reported include Matthew’s correlation coefficients for CoLA, accuracy for\nSST-2, MNLI-matched (MNLI-m), MNLI-mismatched (MNLI-mm), QNLI, and RTE, both accuracy and F1-score\nfor MRPC, and Pearson’s and Spearman’s correlation coefficients for STS-B. The best-performing scores for each\nmodel are highlighted in bold.\n3.09 icat score increase while reducing the ss to\na mere 0.11 away from 50, the ideal ss for a non-\nstereotypical model, with the lms also improving.\nIn the case of the Flan-T5-base model, the optimal\nicat score without detriment to lms is attained by\npruning 11 attention heads. However, even better\nicat scores are achieved later when 131 heads are\npruned, as a significant drop in ss compensates for\nthe loss in lms.\nSubsequently, we assess the GLUE benchmark\n(Wang et al., 2018) performance of the head-pruned\nmodels to ascertain that the removal of these heads\ndoes not significantly impair the PLMs’ perfor-\nmance on downstream tasks. Due to known issues\nwith dataset splits, we exclude the QQP and WNLI\ndatasets 4. Only the BERT and RoBERTa encoder\nmodels are evaluated here, as the other four gen-\nerative models would require intensive training to\n4https://gluebenchmark.com/faq\nallow their language modeling heads to predict nu-\nmerical labels, which might negate the effects of\nattention-head pruning on model performance.\nAs shown in Table 2, the pruned models ex-\nhibit similar, if not better, performance than their\nfull counterparts across most tasks. The most\npronounced performance drops are observed on\nthe MRPC and RTE datasets. It is plausible that\nthe smaller datasets for these tasks, which require\nhigher-level semantic understanding, are insuffi-\ncient for training the models to high performance.\nConsequently, additional active pre-trained atten-\ntion heads that encode semantic information are\nnecessary to capture relevant information from the\ntext. In comparison, the other tasks similar to\nMRPC (STS-B) and RTE (QNLI and MNLI) in\nthe GLUE benchmark feature larger sizes or sim-\npler task objectives, reducing the models’ reliance\non more pre-trained weights.\nThe results of our attention-head ablation experi-\n11334\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n50\n55\n60\n65\n70\n75\n80\n85Stereotype and Language Modeling Scores (ss/lms/icat)\nlms\nss\nicat\nShapley Value\n0.005\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\nAttention-Head Contribution Score (Shapley Value)\n(a) BERT\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n40\n45\n50\n55\n60\n65\n70\n75Stereotype and Language Modeling Scores (ss/lms/icat)\nlms\nss\nicat\nShapley Value\n0.0050\n0.0025\n0.0000\n0.0025\n0.0050\n0.0075\n0.0100\n0.0125\n0.0150\nAttention-Head Contribution Score (Shapley Value) (b) RoBERTa\nFigure 5: Comparison of ss, lms, icat, and Shapley val-\nues of attention heads in BERT and RoBERTa models\nwhen the most contributive attention heads for stereo-\ntype detection in the alternate model are pruned. The\ngreen horizontal line indicates the icat score achieved\nby the unmodified models, and the orange line denotes\nan ss of 50, symbolizing a completely unbiased model.\nments suggest that the attention heads that most\nsignificantly contribute to stereotype detection are\nalso instrumental in encoding stereotypes within\nPLMs. This discovery facilitates integrating stereo-\ntype detection research with stereotype assessment\nand PLM debiasing, reducing the need to annotate\npairwise stereotype assessment or manually curate\nword-level stereotype datasets. Our approach of\npruning the attention heads most contributive to\nstereotype detection offers an efficient method to\nreduce bias in PLMs without requiring re-training.\nThis can be complemented with other debiasing\nmethods to further minimize stereotypes in PLMs\nwhile preserving their linguistic capabilities.\nFurthermore, in Appendix B.4, we demonstrate\nthat the attention-head rankings procured from a\nmodel can be employed to prune and debias differ-\nent checkpoints of that same model. This robust-\nness serves as further proof of the generalizability\nof our approach and hints towards a direction of\ntransferable, adaptable debiasing that can stream-\nline the process of bias reduction in multiple ver-\nsions of a model.\nHowever, it is important to acknowledge that the\nimpact of head pruning on icat may not be consis-\ntently beneficial, particularly when the heads to be\npruned are contributive to stereotype detection (i.e.,\nthey possess positive Shapley values in the probing\nresults). Our observations suggest that some atten-\ntion heads encoding lexical features (for instance,\nthe presence of overtly stereotypical words) may\nachieve low positive Shapley values as they aid\nPLMs in identifying explicit stereotypes. Upon re-\nmoval of these heads, a drop in icat may occur due\nto the negative effect on the language modeling\ncapacity of the PLMs. Yet, this should not un-\ndermine the usefulness of the attention-head prun-\ning method for debiasing PLMs. It’s important\nto note that the gap between the heads that nega-\ntively impact icat when pruned and those encoding\nstereotypes (as depicted in Figure 4) is quite evi-\ndent and can be managed through lms evaluation\ntrials. These trials can help set an empirical limit on\nhow much pruning can be done without excessively\nhurting language modeling capabilities, ensuring\na careful balance between bias mitigation and lan-\nguage understanding performance.\n6.2 Cross-Model Transferability\nWe examine whether the attention-head contribu-\ntions obtained from one PLM can be utilized to\ndebias other PLMs of the same size and architec-\nture. To this end, we perform the ss, lms, and\nicat evaluation with attention-head ablations on\nRoBERTa using the attention-head contributions\nacquired from BERT and vice versa. As shown\nin Figure 5, the effects of pruning attention heads\nmost contributive to detecting stereotypes in differ-\nent models do not consistently improve the icat as\neffectively as using the attention-head ranking of\nthe same model. This is expected, given that differ-\nent PLMs are pre-trained with differing objectives\nand corpora, leading to disparate functionalities for\nattention heads in identical positions. Nonetheless,\nhigher icat scores than the full models are achieved\nwhen 29 and 37 attention heads are pruned from the\nBERT and RoBERTa models. Our results suggest\nthe potential transferability of attention-head con-\ntributions across different models in both encod-\ning and detecting stereotypes. This could indicate\nthat similar linguistic features trigger stereotyp-\ning across different PLMs, as significant attention\nheads for encoding linguistic features typically re-\nside in the same or adjacent layers for PLMs of the\nsame size (Rogers et al., 2020).\n7 Analyzing Textual Clues\nHere, we undertake an analysis of the textual clues\nthat significantly influence the inferences of inher-\nent stereotypes within each PLM. This examina-\ntion arises from our previous experimental findings\nthat the same attention heads are crucial to both\nencoding and detecting stereotypes. We focus on\nanalyzing the most attended single words and word\npairs in the top-5 attention heads as revealed by\nour probing experiments, averaging the attention\nacross these heads. We use SHAP to reduce poten-\ntial noise (such as high attention scores on function\nwords or special tokens).\n11335\nGroups BERT RoBERTa\nAged\nPeople\naccent foolish pest disgusted\npest fat gross bankrupt\ndisgusted shooting struggled worthless\nworthless idiot accent fat\ngross bankrupt knitting idiot\nFemales\nlonely rude rude busy\ndistracted gossip lonely gossip\npoor immature poor distracted\nsexy mothers mothers sexy\nsilly worried knit burst\nMuslims\nterrorists everyone terrorists violent\nterrorist violent terrorist kill\nkill threats threats islam\nislam hated hated family\nreligious family religion arabia\nAfrican\nPeople\nviolent hate religious terrorist\npirates slaves crude dirty\ndirty smell hate annoying\nmuslims forced forced lazy\ncrude criminal ignorant slaves\nMiddle-\nEastern\nPeople\nrude hate blame rough\noppressive racist crime rude\nrough violent corrupt dirty\nserious bombs oppressive racist\nterrorists crime terrorism destruction\nTable 3: Top 10 words with the highest SHAP-adjusted\nattention scores (ranked in order) for detecting stereo-\ntypes towards five minority groups, ranked by the top-5\ncontributive attention heads in BERT and RoBERTa.\nWe use the attention-head contributions obtained\nfrom ImplicitStereo to avoid biasing the textual\nanalyses since ImplicitSteroe does not pre-assume\nany word to be always stereotypical toward a group.\nWe combine all four datasets, selecting instances\nrelated to five minority groups (specifically, the\nelderly, females, Muslims, African, and Middle-\nEastern people) for our textual-clue analysis.\nFor illustrative purposes, we present the single-\nword textual analysis results on BERT and\nRoBERTa in Figure 3. We reserve the complete\nset of results for Appendix E. Our analyses re-\nveal that the top-10 word-level textual clues align\nwith common stereotypes towards the five minority\ngroups. For instance, words like “fat”, “bald”, and\n“bankrupt” stand out for age stereotypes; “gossip”\nand “silly” for gender stereotypes; “violent”, “kill”,\nand “terrorists” for religious (Muslims) and racial\n(Middle-Eastern people) stereotypes; and “dirty”,\n“crime”, and “crude” for racial (African people)\nstereotypes. This is consistent with prior studies\non the prevalence of stereotypes about minority\ngroups in PLMs. For example, Abid et al. (2021)\ndiscovered that GPT-3 (Brown et al., 2020) gen-\nerates stereotypical associations of Muslims with\nphrases like “shooting at will” and “bombing.” Our\nword-pair analyses also present similar rankings\nbut with word pairs usually expressing stronger\nand more stereotype-laden meanings, such as “all\nterrorists” for Muslims, implying all Muslims are\nterrorists. The anti-stereotypical words and word\npairs we find are mostly antonyms of the associated\nstereotypical words, such as “nice” and “caring,”\nwhich these models use to identify anti-stereotypes\ntowards Middle-Eastern people.\nWe also note some variability across the mod-\nels in our study, despite many models highlight-\ning common stereotypical words and word pairs.\nOne potential reason for these differences could\nbe the size of the models, as larger models might\ncapture more instances of co-occurrence between\nstereotypical words and minority group mentions.\nFor instance, T5-base and Flan-T5-base rank “al-\ncohol” and “urine” highly for the elderly, whereas\ntheir smaller counterparts do not. Variations can\nalso stem from differences in pre-training corpora\nand objectives between PLMs with the same archi-\ntecture and structure. For example, BERT high-\nlights “violent” and “pirates” for African people\nand “bombs” for Middle-Eastern people, while\nRoBERTa does not. Similarly, RoBERTa ranks\n“bankrupt” and “gross” much higher than BERT for\nstereotypes towards the elderly.\nThese findings indicate that different PLMs\nmight harbor diverse stereotypes towards minority\ngroups. Therefore, using the same dataset to assess\nstereotype levels across all PLMs might underes-\ntimate certain stereotypes. The combined appli-\ncation of our probing technique and textual-clue\nanalysis framework could aid in identifying the\nmost pronounced stereotypes within each PLM.\n8 Conclusion and Future Work\nIn this paper, we sought to deepen the understand-\ning of the connection between the encoding and\ndetection of stereotypes within PLMs. We per-\nformed extensive probing and ablation studies and,\ninformed by the results, developed a framework to\nexplore the intrinsic stereotypes within each PLM.\nThis framework leverages both textual and atten-\ntion analyses and solely relies on stereotype detec-\ntion annotations. Our study unveils that stereotypes\nare not uniformly distributed across different PLMs.\nThis highlights the need for model-specific stereo-\ntype assessment datasets and tailored debiasing\ntechniques. Our framework introduces an efficient\nmeans of debiasing PLMs without restructuring or\nretraining and without the need for expensive pair-\nwise stereotype/anti-stereotype annotations. Future\nresearch can potentially merge our methods with\nother PLM debiasing techniques.\n11336\n9 Limitations\nThis study has certain limitations. Firstly, our ex-\nperiments were primarily conducted in English due\nto the scarcity of stereotype assessment datasets in\nother languages. While our framework is capable\nof handling complex scenarios such as intersec-\ntional stereotypes, we were unable to explore these\ndue to a lack of adequately annotated datasets.\nWe also constructed an implicit stereotype\ndataset using ChatGPT alongside three publicly\navailable datasets. We pursued this approach be-\ncause existing stereotype examination datasets of-\nten oversimplify the task and have known quality\nconcerns, as indicated by Blodgett et al. (2021).\nOur dataset addresses several issues of prior ones,\nsuch as unnatural phrasing, overly explicit stereo-\ntype expression, and excessive intertwining of\nstereotypes with negative emotions and sentiments.\nNonetheless, we recognize potential noise in the\nChatGPT-generated data, despite our careful man-\nual curation and high-quality human validation re-\nsults via Amazon Mechanical Turk. Unvalidated\ndata points may still contain biases or incorrect in-\nformation from ChatGPT, which could influence\nour results. Further, our dataset doesn’t completely\nrectify the problems with existing datasets. How-\never, we view ImplicitStereo as an additional data\nsource, providing a more equitable examination of\nstereotypes in PLMs, and any potential biases from\nImplicitStereo should not significantly impact our\nanalyses.\n10 Ethics Statement\nThis research tackles harmful stereotypes present\nin widely used pre-trained language models. Our\naim is to identify and reduce these biases. We\nacknowledge that our analysis could be distressing\nor offensive to some, and have therefore included a\nwarning at the outset of this paper. To ensure ethical\npractices, we relied on publicly available stereotype\ndatasets, thereby avoiding exposing annotators to\npotentially harmful language.\nWe also enlisted human validators from Ama-\nzon Mechanical Turk to validate the ImplicitStereo\ndataset we constructed and to understand the im-\nportance of constructing a higher-quality and more\nchallenging implicit stereotype dataset. The dataset\nwas examined beforehand. The validators received\nan hourly rate of $15.00, greatly surpassing the\nminimum state and federal (in the U.S.) hourly\nwage. This research is driven by the quest to de-\nvelop more equitable and unbiased AI models, and\nwe appreciate the contribution of all participants\ninvolved.\nReferences\nAbubakar Abid, Maheen Farooqi, and James Zou. 2021.\nPersistent anti-muslim bias in large language models.\nIn Proceedings of the 2021 AAAI/ACM Conference\non AI, Ethics, and Society, pages 298–306.\nMarion Bartl, Malvina Nissim, and Albert Gatt. 2020.\nUnmasking contextual stereotypes: Measuring and\nmitigating BERT’s gender bias. In Proceedings of\nthe Second Workshop on Gender Bias in Natural\nLanguage Processing, pages 1–16, Barcelona, Spain\n(Online). Association for Computational Linguistics.\nYuchen Bian, Jiaji Huang, Xingyu Cai, Jiahong Yuan,\nand Kenneth Church. 2021. On attention redundancy:\nA comprehensive study. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 930–945, Online. As-\nsociation for Computational Linguistics.\nSu Lin Blodgett, Gilsinia Lopez, Alexandra Olteanu,\nRobert Sim, and Hanna Wallach. 2021. Stereotyping\nNorwegian salmon: An inventory of pitfalls in fair-\nness benchmark datasets. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 1004–1015, Online. Association\nfor Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nYang Cao, Anna Sotnikova, Hal Daumé III, Rachel\nRudinger, and Linda Zou. 2022. Theory-grounded\nmeasurement of U.S. social stereotypes in English\nlanguage models. In Proceedings of the 2022 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies, pages 1276–1295, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nJavier Castro, Daniel Gómez, and Juan Tejada. 2009.\nPolynomial calculation of the shapley value based\non sampling. Computers & Operations Research,\n36(5):1726–1730.\nRochelle Choenni, Ekaterina Shutova, and Robert van\nRooij. 2021. Stepmothers are mean and academics\nare pretentious: What do pretrained language models\nlearn about you? In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, pages 1477–1491, Online and Punta Cana,\n11337\nDominican Republic. Association for Computational\nLinguistics.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nSunipa Dev, Emily Sheng, Jieyu Zhao, Aubrie Amstutz,\nJiao Sun, Yu Hou, Mattie Sanseverino, Jiin Kim, Ak-\nihiro Nishi, Nanyun Peng, and Kai-Wei Chang. 2022.\nOn measures of biases and harms in NLP. In Find-\nings of the Association for Computational Linguis-\ntics: AACL-IJCNLP 2022, pages 246–267, Online\nonly. Association for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nKawin Ethayarajh and Dan Jurafsky. 2021. Attention\nflows are shapley value explanations. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 2: Short Papers), pages 49–54, Online. As-\nsociation for Computational Linguistics.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nSergiu Hart. 1989. Shapley value. In Game theory,\npages 210–216. Springer.\nPerry Hinton. 2017. Implicit stereotypes and the predic-\ntive brain: cognition and culture in “biased” person\nperception. Palgrave Communications, 3(1):1–9.\nJacqui Hutchison and Douglas Martin. 2015. The Evo-\nlution of Stereotypes, pages 291–301. Springer Inter-\nnational Publishing, Cham.\nJae-young Jo and Sung-Hyon Myaeng. 2020. Roles and\nutilization of attention heads in transformer-based\nneural language models. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 3404–3417, Online. Association\nfor Computational Linguistics.\nNicol Turner Lee. 2018. Detecting racial bias in algo-\nrithms and machine learning. Journal of Information,\nCommunication and Ethics in Society.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nScott M Lundberg and Su-In Lee. 2017. A unified ap-\nproach to interpreting model predictions. In I. Guyon,\nU. V . Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems 30, pages\n4765–4774. Curran Associates, Inc.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356–5371, Online. Association for\nComputational Linguistics.\nNikita Nangia, Clara Vania, Rasika Bhalerao, and\nSamuel R. Bowman. 2020. CrowS-pairs: A chal-\nlenge dataset for measuring social biases in masked\nlanguage models. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1953–1967, Online. As-\nsociation for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\nThibault Sellam, Steve Yadlowsky, Ian Tenney, Jason\nWei, Naomi Saphra, Alexander D’Amour, Tal Linzen,\nJasmijn Bastings, Iulia Raluca Turc, Jacob Eisen-\nstein, et al. The multiberts: Bert reproductions for\nrobustness analysis. In International Conference on\nLearning Representations.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nJieyu Zhao, Tianlu Wang, Mark Yatskar, Vicente Or-\ndonez, and Kai-Wei Chang. 2018. Gender bias in\ncoreference resolution: Evaluation and debiasing\nmethods. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 15–20, New\n11338\nOrleans, Louisiana. Association for Computational\nLinguistics.\n11339\nA The ImplicitStereo Dataset\nA.1 Shortcomings of Existing Stereotype\nDatasets\nFigure A1: Example of using ChatGPT to flip the emo-\ntional valence of a stereotypical sentence from Stere-\noSet.\nThere are known problems in existing datasets\nfor stereotype examinations, e.g., the instances are\nall short and simple sentences and the stereotypes\nare carried out by single words or short phrases.\nWe conduct additional experiments to see if this\nsimplified setting reduces the stereotype analysis\nproblem to simpler ones, e.g., sentiment analysis\nand emotion recognition. Specifically, we sample\n50 stereotypical instances from the CrowS-Pairs,\nStereoSet, and WinoBias datasets, rewrite them\ninto sentences with the same stereotypes but differ-\nent sentiment polarity or emotional valence, and\ntest if the predictions of stereotype detection mod-\nels fine-tuned on these datasets change frequently\non the rewritten instances. The task settings of the\nthree datasets could be heavily intertwined with\nthe two lower-level tasks if the predictions change\nmuch. We use the ChatGPT model to rewrite the\nsentences. Figure A1 shows an example prompt we\nuse to query ChatGPT and the response it generates.\nAfter rewriting, we conduct manual validations on\nAmazon Mechanical Turk (MTurk) to validate the\nquality of the rewritings. We regard each instance\nto be high in quality if at least 2 out of 3 validators\nagree that the instance is with the same stereotype\nas the original sentence and (1) the same emotional\narousal and the opposite emotional valence (for\nemotional-valence flipping) or (2) the opposite sen-\ntiment (for sentiment flipping). The final validation\nresults show satisfaction rates of 86%, 84%, and\n94% for emotional-valence flipping and 90%, 88%,\nand 96% for sentiment flipping on examples from\nthe CrowS-Pairs, StereoSet, and WinoBias dataset,\n(a)\n(b)\nFigure A2: Example of using ChatGPT to (a) retrieve\nstereotypes targeting each group of people and (b) gen-\nerate example implicit stereotypical utterances of each\nspecific stereotype.\nrespectively. The inter-annotator agreement rates\nare always above 0.76 in Fleiss’ κ(Fleiss, 1971)\nin all the cases, suggesting the high quality of our\n11340\nrewritings generated by ChatGPT.\nWe then apply stereotype detection models fine-\ntuned on the three datasets on pairs of the original\nsentence from each dataset and its rewritings to see\nhow often the models’ predictions are flipped when\nthe emotional valence or sentiment in each sen-\ntence changes. Our experiments cover all six mod-\nels that we examine in the main paper, i.e., BERT,\nRoBERTa, T5-small, T5-base, Flan-T5-small, and\nFlan-T5-base. We find that these models’ predic-\ntions are flipped in 56% to 88% cases when the\nemotional valence changes and in 66% to 92%\ncases when the sentiment changes. These results\nshow that for the three publicly-available datasets,\nstereotypes in the sentences are so heavily inter-\ntwined with the emotional valence or sentiment\npolarity that models fine-tuned on these datasets\nlearn to identify stereotypes based mostly on the\ntwo lower-level linguistic features, which oversim-\nplifies the stereotype detection and examination\ntasks.\nA.2 Example Prompts for Constructing\nImplicitStereo\nWe use ChatGPT to retrieve stereotypes targeting\n17 demographic groups and generate dialogues\nwhere the retrieved stereotypes are implicitly ex-\npressed. Figure A2a shows one example prompt\nwe used to query ChatGPT and get common stereo-\ntypes toward 17 demographic groups, and Figure\nA2b shows the prompt used for generating the dia-\nlogues.\nB Robustness of Probing Results\nThis section introduces robustness tests of the\nShapley-based probing that we use to determine the\ncontributions of attention heads to the stereotype\ndetection models. For succinctness, we present\nexperiments conducted using BERT and, in most\ncases, only the StereoSet dataset, while we have\nrepeated the experiments for all the models and\ndatasets and the findings echo.\nB.1 Random Seed Robustness\nAs Figure B1 shows, the probing results of BERT\non StereoSet are very robust to the choices of ran-\ndom seeds.\nB.2 Sampling Size Robustness\nWe further conduct repeated probing experiments\nusing the BERT model with different sampling\n1 2 3 4 5 6 7 8 9 10 11 12\nHead\n121110987654321\nLayer\n(a) seed=42\n1 2 3 4 5 6 7 8 9 10 11 12\nHead\n121110987654321\nLayer (b) seed=2022\n1 2 3 4 5 6 7 8 9 10 11 12\nHead\n121110987654321\nLayer (c) seed=2023\nFigure B1: The results of the probing experiments on\nthe StereoSet dataset using the BERT model, with three\ndifferent random seeds (42, 2022, and 2023).\n1 2 3 4 5 6 7 8 9 10 11 12\nHead\n121110987654321\nLayer\n(a) m= 250\n1 2 3 4 5 6 7 8 9 10 11 12\nHead\n121110987654321\nLayer (b) m= 500\n1 2 3 4 5 6 7 8 9 10 11 12\nHead\n121110987654321\nLayer\n(c) m= 750\n1 2 3 4 5 6 7 8 9 10 11 12\nHead\n121110987654321\nLayer (d) m= 1,000\nFigure B2: The results of the probing experiments on\nthe StereoSet dataset using the BERT model, with four\ndifferent sampling sizesm∈250,500,750,1,000. The\nheatmap shows the Shapley values of each attention\nhead. Green cells indicate attention heads with positive\nShapley values, while red cells indicate attention heads\nwith negative Shapley values. The deeper the color, the\nhigher the absolute Shapley value.\nsizes and random seeds. Figure B2 shows the con-\nsistency of the results when varying the number\nof random permutations used during the probing\nprocess. The results are highly consistent with four\ndifferent sampling sizes ranging between 250 and\n1,000, with Spearman’s ρfor each pair of probing\nresults between 0.96 and 0.98. As shown in Fig-\nure B1, the results also remain consistent when\nusing different random seeds , with a fixed sam-\npling size of m = 250, particularly for the top-\ncontributing attention heads. The Spearman’s ρ\nbetween the attention-head rankings with different\nrandom seeds are between 0.96 and 0.97 for all\nthree datasets, indicating the high robustness of our\nprobing results to random-seed selection. There-\n11341\nStereoSet CrowS-Pairs WinoBias\nFigure B3: Probing results of BERT with the encoder\nweights jointly trained with the classification layer in\nthe probing process.\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75Stereotype Detection Performance (Accuracy)\nSettings\nbottom-up\ntop-down\n(a) StereoSet\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80Stereotype Detection Performance (Accuracy)\nSettings\nbottom-up\ntop-down (b) CrowS-Pairs\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80Stereotype Detection Performance (Accuracy)\nSettings\nbottom-up\ntop-down\n(c) WinoBias\nFigure B4: Attention-head ablation evaluations of the\nBERT model on three datasets with the encoder weights\nalso fine-tuned during the probing process.\nfore, we use a random seed of 42 and sample size\nm= 250for all the probing experiments.\nB.3 Probing Setting Robustness\nWe also compare two probing settings: training\nonly the classification layer while freezing the en-\ncoder weights of PLMs, and jointly training the\nclassification layer with the encoder weights. As\nshown in Figure B3, the probing results of BERT\nwith its encoder weights trained during probing\ndiffer substantially from those when the encoder\nweights of BERT are frozen in the probing process\n(As shown in Figure 1). The Spearman’s ρ be-\ntween each pair of attention-head rankings ranges\nbetween 0.35 and 0.69. To validate the correct-\nness of our previous probing results, we conducted\nattention-head ablation experiments using the prob-\ning results with encoder weights trained during\nprobing. As shown in Figure B4, the performance\nchanges are consistent with those in Figure 2. This\nsuggests that the attention-head contributions ob-\ntained by training or not training the encoder\nweights are both valid. The variations in attention-\n(a) Seed 0 Step 2000\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n50\n52\n54\n56\n58\n60Stereotype & Language Modeling Scores\nlms\nss\nicat (b) Seed 2 Step 2000\n(c) Seed 3 Step 2000\nFigure B5: Stereotype examination with attention-head\npruning using three BERT models from MultiBERTs\nthat are pre-trained with different random seeds. The\nexperiment is conducted on StereoSet.\nhead rankings may be due to the redundancy of\nattention-heads with similar functionalities. There-\nfore, we use the probing results obtained without\ntraining the encoder weights in all the analyses in\nthis paper.\nB.4 Checkpoint Robustness\nOur attention-head pruned models yield improved\nicat scores (i.e., being less stereotypical while as\nstrong in language modeling ability) in our main ex-\nperiments. Here we provide another set of attention-\nhead pruning experiments on StereoSet using three\nBERT checkpoints pre-trained with different ran-\ndom seeds since, according to Sellam et al., differ-\nent checkpoints of the same model might behave\ndifferently despite the shared pre-training objec-\ntive and data. The attention-head rankings used for\nattention-head pruning come from the Huggingface\nBERT checkpoint, which is also used in our main\nexperiments. As Figure B5 shows, the changes\nof icat score, ss, and lms when attention heads are\npruned from these 3 checkpoints are very consistent\nwith those for the BERT model from Huggingface.\nThese results suggest the high robustness of our\nexperiments to the choice of checkpoints for each\nmodel. As such, we use only one checkpoint of\neach model in the main paper to save space.\n11342\n(a) CrowS-Pairs\n (b) StereoSet\n (c) WinoBias\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n0.5\n0.6\n0.7\n0.8\n0.9Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (d) ImplicitStereo\n0 10 20 30 40 50\nNumber of Pruned Attention Heads\n0.48\n0.50\n0.52\n0.54\n0.56\n0.58\n0.60\n0.62\n0.64Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down\n(e) CrowS-Pairs\n0 10 20 30 40 50\nNumber of Pruned Attention Heads\n0.70\n0.75\n0.80\n0.85\n0.90Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (f) StereoSet\n0 10 20 30 40 50\nNumber of Pruned Attention Heads\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (g) WinoBias\n0 10 20 30 40 50\nNumber of Pruned Attention Heads\n0.5\n0.6\n0.7\n0.8\n0.9Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (h) ImplicitStereo\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n0.50\n0.55\n0.60\n0.65Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down\n(i) CrowS-Pairs\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n0.70\n0.75\n0.80\n0.85\n0.90Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (j) StereoSet\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (k) WinoBias\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n0.6\n0.7\n0.8\n0.9Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (l) ImplicitStereo\n0 10 20 30 40 50\nNumber of Pruned Attention Heads\n0.475\n0.500\n0.525\n0.550\n0.575\n0.600\n0.625\n0.650\n0.675Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down\n(m) CrowS-Pairs\n0 10 20 30 40 50\nNumber of Pruned Attention Heads\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (n) StereoSet\n0 10 20 30 40 50\nNumber of Pruned Attention Heads\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (o) WinoBias\n0 10 20 30 40 50\nNumber of Pruned Attention Heads\n0.5\n0.6\n0.7\n0.8\n0.9Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (p) ImplicitStereo\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down\n(q) CrowS-Pairs\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (r) StereoSet\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n0.5\n0.6\n0.7\n0.8Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (s) WinoBias\n0 20 40 60 80 100 120 140\nNumber of Pruned Attention Heads\n0.5\n0.6\n0.7\n0.8\n0.9Stereotype Detection Performance (Accuracy)\nSettings\nBottom Up\nTop Down (t) ImplicitStereo\nFigure C1: Attention-head ablation results on four stereotype detection datasets using the RoBERTa ((a) - (d)),\nT5-small ((e) - (h)), T5-base ((i) - (l)), Flan-T5-small ((m) - (p)), and Flan-T5-base ((q) - (t)) models. Bottom up\nand top down refer to two settings where the attention heads are pruned from the least or most contributive attention\nheads, respectively.\nC Additional Attention-Head Ablation\nExperiments for Stereotype Detection\nWe show the attention-head ablation results of the\nRoBERTa, T5-small, T5-base, Flan-T5-small, and\nFlan-T5-base in Figure C1. Clearly, all the perfor-\nmance changes are clean when the most important\nattention heads for stereotype detection (according\nto our probing results) are pruned. Except for the\nsmall numbers of very contributive attention heads,\npruning other attention heads does not strongly\nnegatively affect the performance of the stereotype\ndetection models. These results support the high\nquality of our probing results.\nD Ablating Decoder Attention Heads of\nEncoder-Decoder Models\nDifferent from the encoder-only models such as\nBERT, there are three strategies of attention-head\npruning for encoder-decoder models like T5, i.e.,\npruning attention heads from the encoder, the de-\ncoder, or both. As Figure D1 shows, ablating at-\ntention heads in the decoder of 4 T5 and Flan-T5\nmodels that receive the highest Shapley values does\n11343\n0 10 20 30 40 50\nNumber of Pruned Attention Heads\n0.742\n0.744\n0.746\n0.748\n0.750\n0.752\n0.754Stereotype Detection Performance (Accuracy)\nSettings\nbottom-up\ntop-down\n(a) StereoSet\n0 10 20 30 40 50\nNumber of Pruned Attention Heads\n0.811\n0.812\n0.813\n0.814\n0.815\n0.816\n0.817Stereotype Detection Performance (Accuracy)\nSettings\nbottom-up\ntop-down (b) CrowS-Pairs\n0 10 20 30 40 50\nNumber of Pruned Attention Heads\n0.750\n0.755\n0.760\n0.765\n0.770\n0.775Stereotype Detection Performance (Accuracy)\nSettings\nbottom-up\ntop-down\n(c) WinoBias\n (d) ImplicitStereo\nFigure D1: Performance of the T5-small model achieved\non four datasets when the decoder weights are pruned.\nnot affect the models’ performance drastically. The\nhighest performance drop is merely 2.10% in ac-\ncuracy for all the models. We speculate the result\nto be the lower contributions of decoder weights\nto the stereotype detection task. Accordingly, we\nconduct attention-head pruning experiments only\non the encoders of models in the main paper.\nE Additional Textual Analysis Results\nWe show the word-level textual analysis results for\nthe T5 and Flan-T5 models in Table E1. The bi-\ngram results are strongly correlated with the word-\nlevel results, usually connecting tone modifiers or\nminority-group indicators to these stereotypical\nwords. The results share a lot in common, while\nthere are also different stereotypical expressions for\nthe same target group across models. This might\nresult from the different pre-training corpora and\nobjectives of these models. We analyze these com-\nmonalities and differences in Section 7. As such,\nstereotype examination and mitigation might have\nto be adapted to each model to achieve the best\nresults.\nF Computational Infrastructure\nWe use a single RTX-6000 card for all the probing\nexperiments and GLUE evaluations.\n11344\nMinority T5-small T5-base Flan-T5-small Flan-T5-base\nAged\nPeople\ndisrespect slow bald sweater waste gross alcohol exhausted\ncareful asleep health dirty mad bald gross loud\nlosing classical alcohol men dirty weak bald terrible\ngun weak weight smoke promotion surgery urine tired\nexperienced bankrupt urine disrespect bank selfish selfish retirement\nFemales\nselfish poor hate clothes clothes effortlessly selfish poor\nchore mess terrible ruined busy cruel annoying mess\nappearance predator gossip romantic tired lazy trouble predator\nskin gossip evil annoying poor distracted weak weak\nannoying wedding appearance predator happy elf mess wedding\nMuslims\nviolent beautiful kill islam threats kill threats violence\nscary women evil terrorist scary hate scary allah\ndestruction shame destruction threats terrorist shame prayed guilt\nterrorist threats dangerous prayed allah terrorism terrorist violent\nevil guilt hate religious violence religious terrorism evil\nAfrican\nPeople\ncleanliness athlete dumb fighting ignorant slave fat blame\ndumb steal slave crude fear thieves noisy dangerous\nbrave hijack selling horror hostile killed nervous crime\nstruggling notorious steal terrorism dumb colored extreme hostile\nterrorism thieves fear terrorist crude crime emotional steal\nMiddle-\nEastern\nPeople\nevil silent exploded corrupt threaten rough extreme crime\nracist tough terribly wealthy attacked conservative emotional dead\nintelligent economy racist disrespect hijack horrific blame pray\ncorrupt threaten trouble terrorism trouble noisy smoke crazy\nmalicious brutal rough weapons tough crazy dangerous hijack\nTable E1: 10 words with the highest SHAP-adjusted attention scores (ranked in order) on the top-5 contributive\nattention heads in T5-small, T5-base, Flan-T5-small, and Flan-T5-base models for detecting stereotypes toward 5\nminority groups.\n11345",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5963945388793945
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4761212468147278
    },
    {
      "name": "Natural language processing",
      "score": 0.47270748019218445
    },
    {
      "name": "Cognitive science",
      "score": 0.4551960229873657
    },
    {
      "name": "Natural language",
      "score": 0.44209450483322144
    },
    {
      "name": "Psychology",
      "score": 0.23081696033477783
    }
  ]
}