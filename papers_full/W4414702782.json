{
  "title": "Large language models forecast patient health trajectories enabling digital twins",
  "url": "https://openalex.org/W4414702782",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2631366578",
      "name": "Nikita Makarov",
      "affiliations": [
        "Roche (Switzerland)",
        "Helmholtz Zentrum München",
        "Ludwig-Maximilians-Universität München"
      ]
    },
    {
      "id": "https://openalex.org/A4212329102",
      "name": "Maria Bordukova",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "Roche (Switzerland)",
        "Helmholtz Zentrum München"
      ]
    },
    {
      "id": "https://openalex.org/A2966560117",
      "name": "Papichaya Quengdaeng",
      "affiliations": [
        "Helmholtz Zentrum München",
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A3129095539",
      "name": "Dániel Garger",
      "affiliations": [
        "Ludwig-Maximilians-Universität München",
        "Helmholtz Zentrum München"
      ]
    },
    {
      "id": "https://openalex.org/A4209720500",
      "name": "Raul Rodriguez-Esteban",
      "affiliations": [
        "Roche (Switzerland)"
      ]
    },
    {
      "id": "https://openalex.org/A2042728754",
      "name": "Fabian Schmich",
      "affiliations": [
        "Roche (Switzerland)"
      ]
    },
    {
      "id": "https://openalex.org/A2092378952",
      "name": "Michael P. Menden",
      "affiliations": [
        "University of Melbourne",
        "Helmholtz Zentrum München"
      ]
    },
    {
      "id": "https://openalex.org/A2631366578",
      "name": "Nikita Makarov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4212329102",
      "name": "Maria Bordukova",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2966560117",
      "name": "Papichaya Quengdaeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3129095539",
      "name": "Dániel Garger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4209720500",
      "name": "Raul Rodriguez-Esteban",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2042728754",
      "name": "Fabian Schmich",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2092378952",
      "name": "Michael P. Menden",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2078417144",
    "https://openalex.org/W3172586930",
    "https://openalex.org/W3184569878",
    "https://openalex.org/W4387966880",
    "https://openalex.org/W4293109418",
    "https://openalex.org/W4410393029",
    "https://openalex.org/W4367840585",
    "https://openalex.org/W3047286661",
    "https://openalex.org/W2979455765",
    "https://openalex.org/W4317780575",
    "https://openalex.org/W4226276053",
    "https://openalex.org/W4391642957",
    "https://openalex.org/W4413642769",
    "https://openalex.org/W4385381606",
    "https://openalex.org/W4393177791",
    "https://openalex.org/W4389664922",
    "https://openalex.org/W4402670000",
    "https://openalex.org/W1934266686",
    "https://openalex.org/W2415251397",
    "https://openalex.org/W4387355843",
    "https://openalex.org/W4321854470",
    "https://openalex.org/W4388284782",
    "https://openalex.org/W4313439128",
    "https://openalex.org/W2109140840",
    "https://openalex.org/W2122264016",
    "https://openalex.org/W2797937217",
    "https://openalex.org/W2992010020",
    "https://openalex.org/W3171884590",
    "https://openalex.org/W4391940656",
    "https://openalex.org/W4414939355",
    "https://openalex.org/W3204905970",
    "https://openalex.org/W2163830215",
    "https://openalex.org/W2990115702",
    "https://openalex.org/W1984069221",
    "https://openalex.org/W4319825984",
    "https://openalex.org/W2039879950",
    "https://openalex.org/W2947401288",
    "https://openalex.org/W3184796196",
    "https://openalex.org/W4402670074",
    "https://openalex.org/W4404134492",
    "https://openalex.org/W3011119029",
    "https://openalex.org/W4414625440",
    "https://openalex.org/W4401857333"
  ],
  "abstract": "Abstract Generative artificial intelligence is revolutionizing digital twin development, enabling virtual patient representations that predict health trajectories, with large language models (LLMs) showcasing untapped clinical forecasting potential. We developed the Digital Twin—Generative Pretrained Transformer (DT-GPT), extending LLM-based forecasting solutions to clinical trajectory prediction. DT-GPT leverages electronic health records without requiring data imputation or normalization and overcomes real-world data challenges such as missingness, noise, and limited sample sizes. Benchmarking on non-small cell lung cancer, intensive care unit, and Alzheimer’s disease datasets, DT-GPT outperformed state-of-the-art machine learning models, reducing the scaled mean absolute error by 3.4%, 1.3% and 1.8%, respectively. It maintained distributions and cross-correlations of clinical variables, and demonstrated explainability through a human-interpretable interface. Additionally, DT-GPT’s ability to perform zero-shot forecasting highlights potential advantages of LLMs as clinical forecasting platforms, proposing a path towards digital twin applications in clinical trials, treatment selection, and adverse event mitigation.",
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-02004-3\nLarge language models forecast patient\nhealth trajectories enabling digital twins\nCheck for updates\nNikita Makarov1,2,3,7, Maria Bordukova1,2,3,7, Papichaya Quengdaeng2,4,D a n i e lG a r g e r2,3,\nRaul Rodriguez-Esteban5,F a b i a nS c h m i c h1 & Michael P. Menden2,6\nGenerative artiﬁcial intelligence is revolutionizing digital twin development, enabling virtual patient\nrepresentations that predict health trajectories, with large language models (LLMs) showcasing\nuntapped clinical forecasting potential. We developed the Digital Twin— Generative Pretrained\nTransformer (DT-GPT), extending LLM-based forecasting solutions to clinical trajectory prediction.\nDT-GPT leverages electronic health records without requiring data imputation or normalization and\novercomes real-world data challenges such as missingness, noise, and limited sample sizes.\nBenchmarking on non-small cell lung cancer, intensive care unit, and Alzheimer’s disease datasets,\nDT-GPT outperformed state-of-the-art machine learning models, reducing the scaled mean absolute\nerror by 3.4%, 1.3% and 1.8%, respectively. It maintained distributions and cross-correlations of\nclinical variables, and demonstrated explainability through a human-interpretable interface.\nAdditionally, DT-GPT’s ability to perform zero-shot forecasting highlights potential advantages of\nLLMs as clinical forecasting platforms, proposing a path towards digital twin applications in clinical\ntrials, treatment selection, and adverse event mitigation.\nClinical forecasting involves predicting patient-speciﬁc health outcomes\nand clinical events over time, which is essential for patient monitoring,\ntreatment selection, and drug development\n1.A ne m e r g i n ga p p r o a c ht o\nsupport such forecasting is the use of digital twins2,3. These are virtual\nrepresentations of patients that generate detailed, multivariable predictions\nof future health states by leveraging longitudinal medical history3,4.W h e n\ninitialized with individual patient characteristics, digital twins can simulate\nreal-time personalized responses to medical interventions or treatments\n2,4,5.\nDigital twins offer a comprehensive framework for patient modeling by\nintegrating diverse data streams, which can include history of medical\nexaminations, diagnoses and treatments, deep molecular proﬁling, lifestyle\nand environmental factors, as well as general biomedical knowledge6–8.T h e y\nprovide a holistic reﬂe c t i o no fa ni n d i v i d u a l’s status within the broader\ncontext of the patient population, accounting for the interplay of disease\ndynamics and medical interventions\n4. By bridging the gap between\npopulation-level evidence and individual-level insights, the application of\ndigital twins is poised to revolutionize healthcare in areas such as precision\nand personalized medicine, predictive analytics, virtual testing, continuous\nmonitoring, and enhanced decision support3,4.\nGenerative artiﬁcial intelligence (AI) holds promise for creating digital\ntwins due to its potential to produce synthetic yet realistic data, but this area\nof application is still in its infancy\n4. Generative AI methods for predicting\npatient trajectories include recurrent neural networks, transformers and\nstable diffusion\n9–13. These often fall short in terms of handling missing data,\ninterpretability and performance. These challenges can be partially\naddressed by causal machine learning\n14, but these algorithms face limita-\ntions related to small datasets or being conﬁned to simulations15.\nRecent breakthroughs in generative AI have been achieved with\nfoundation models, which are pre-trained AI models adaptable to various\nspeciﬁc tasks involving different types of data. Most foundation models for\npatient forecasting focus on single-point predictions rather than compre-\nhensive longitudinal patient trajectories, which are needed for clinical\ndecision-making\n16. Recently, clinically focused, LLM-inspired methods\nhave been proposed17, however, with their evaluation focus still being on\nsingle-point predictions rather than longitudinal trajectories, and without\nusing the knowledge of pretrained LLMs. Less explored for this purpose\nremain text-focused Large Language Models (LLMs), which have demon-\nstrated forecasting capabilities\n18,19, including some approaches showing the\n1Roche Innovation Center Munich (RICM), Penzberg, Germany.2Computational Health Center, Helmholtz Munich, Munich, Germany.3Department of Biology,\nLudwig Maximilian University of Munich, Munich, Germany.4TUM School of Computation, Information and Technology, Technical University of Munich,\nMunich, Germany.5Roche Innovation Center Basel (RICB), Basel, Switzerland.6Department of Biochemistry and Pharmacology, Bio21 Molecular Science and\nBiotechnology Institute, The University of Melbourne, Melbourne, VIC, Australia.7These authors contributed equally: Nikita Makarov, Maria Bordukova.\ne-mail: fabian.schmich@roche.com; michael.menden@unimelb.edu.au\nnpj Digital Medicine|           (2025) 8:588 1\n1234567890():,;\n1234567890():,;\nability of zero-shot forecasting, i.e., forecasting without any prior speciﬁc\ntraining in the task, thus highlighting their remarkable generalizability20–22.\nLLM-based forecasting has made great progress in general forecasting.\nHowever, some common methods, such as LSTPrompt20,L L M T i m e21,\nTime-LLM22,a n dG P T 4 T S23, make assumptions which may not necessarily\nhold in clinical trajectory forecasting. One example is channel indepen-\ndence, whereby, for multivariate time series, channel-independent models\nprocess each time series separately, without modeling interactions and inter-\ntime series dependencies. This approach may not be optimal in the clinical\nsetting, in which we often observe correlated time series, putatively driven by\ncausal biological links, highlighting the need to process all aspects of a\npatient simultaneously.\nWe propose the creation of digital twins based on LLMs that leverage\ndata from electronic health records (EHRs) from real world data (RWD)\nand observational studies. EHRs are a key source of training data for\nmachine learning models in healthcare, as they record patient characteristics\nsuch as demographics, diagnoses, and lab results over time\n24. However, they\npose speciﬁc challenges such as data heterogeneity, rare events, sparsity, and\nquality issues16. There have been developments in machine learning to\novercome these challenges, especiallyfor data sparsity, usually by adapting\nthe model’s architecture, resulting in increased model complexity and the\nintroduction of furthera s s u m p t i o n so nt h ed a t a10,13.\nWe hypothesize that LLMs will empower the next generation of digital\ntwins in healthcare. Here, we introduce the Digital Twin - Generative\nPretrained Transformer (DT-GPT) model (Fig.1), which\nenables: (i) forecasting of clinical variable trajectories, (ii) zero-shot\npredictions of clinical variables not previously trained on, and (iii) pre-\nliminary interpretability utilizing chatbot functionalities. DT-GPT is an\nextension of previous LLM-based forecasting solutions, based onﬁne-\ntuning LLMs on clinical data using a straightforward data encoding scheme.\nThe method is designed to solve clinically speciﬁci s s u e s ,b em o d e l - a g n o s t i c\nand to be applied to any text-focused LLM without any further architectural\nchanges.\nResults\nWe analyzed the performance of DT-GPT by forecasting various clinical\nvalues on diverse datasets, includingon a short-term scale (next 24 h) for\nIntensive Care Unit (ICU) patients, a medium-term scale (up to 13 weeks)\nfor non-small cell lung cancer (NSCLC) patients, as well as a long-term\nAlzheimer’s Disease dataset (next 24 months). The ICU dataset is based on\nMedical Information Mart for Intensive Care IV (MIMIC-IV)25 with 35,131\npatients, whilst the NSCLC dataset isbased on the the nationwide Flatiron\nHealth EHR-derived de-identiﬁed database, containing 16,496 NSCLC\npatients (“Methods”; Supplementary Tables 1–4; Supplementary Note 1).\nThe Alzheimer’s disease dataset is derived from the Alzheimer’sD i s e a s e\nNeuroimaging Initiative (ADNI) dataset, containing 1,140 patients (Sup-\nplementary Tables 1 and 5; Supplementary Note 1). The datasets comple-\nment the analysis to understand how the model works on short-, medium-\nand long-term scales, as well as on different amounts of patients available for\ntraining. All details on task setup, data preprocessing, model training, and\nevaluation are provided in the“Methods” section.\nDT-GPT achieved state-of-the-art forecasting performance\nDT-GPT achieved the lowest overallscaled mean absolute error (MAE)\nacross benchmark tasks in comparison with state-of-the-art models (Table\n1), with the z-score scaling allowing comparison and aggregation across\nvariables (“Methods”). In the NSCLC dataset, we predicted six laboratory\nvalues weekly for up to 13 weeks post-therapy initiation, leveraging all pre-\ntreatment data to model patient trajectories under treatment. For the ICU\ntask, we forecasted the next 24 h by predicting respiratory rate, magnesium\nand oxygen saturation based on the previous 24 h history, enabling real-time\nmonitoring and timely intervention. In the Alzheimer’s dataset, we fore-\ncasted Mini Mental State Examination (MMSE)\n26, Clinical Dementia Rating\nsum of boxes (CDR-SB)27 and Alzheimer’sD i s e a s eA s s e s s m e n tS c a l e\n(ADAS11)28 cognitive scores, over the next 24 months at 6 month intervals\nusing baselines measurements. All comparisons were performed on unseen\npatients.\nWe compared DT-GPT to 14 multi-step, multivariate baselines,\nranging from a naïve model that copies over the last observed value to\nstate-of-the-art forecasting models. These included linear regression\nmodel, time series LightGBM model, Temporal Fusion Transformer\n(TFT), Temporal Convolutional Network (TCN), Recurrent Neural\nNetwork (RNN), Long Short-Term Memory (LSTM), Transformer, and\nTime-series Dense Encoder (TiDE) model\n12,29,30. The naïve model\nensured that models with better performance capture nonstationary\ntime series, whilst advanced models were chosen for their ability to\nhandle future variables and achieving state-of-the-art performance in\nboth medical and standard time series forecasting\n31,32. To understand the\ncontribution of ﬁne-tuning, we also run the general, state-of-the-art\nLLM Qwen3-32B and the biomedical LLM BioMistral-7B33,34. Note that\nDT-GPT is aﬁne-tuned 7-billion-parameter model based on BioMistral,\nwhilst Qwen3 is a signiﬁcantly larger model at 32 billion parameters.\nAdditionally, we benchmarked advanced time-series LLM-based\nmethods, i.e. Time-LLM and LLMTime\n21,22, as well as a patch based\nmodel PatchTST35, all of which are channel-independent models, which\nprocess each input time series separately.\nFig. 1 | The LLM-based DT-GPT framework enables forecasting patient trajec-\ntories, identifying key variables, and zero-shot predictions.Here exempliﬁed,\na sparse patient timeline, whichb DT-GPT utilizes for generating longitudinal\nclinical variable forecasts, e.g.,c neutrophil andd hemoglobin blood levels. DT-GPT\ncan e chat and respond to inquiries about important variables, as well as (f) perform\nzero-shot forecasting on clinical variables previously not used during training.\nhttps://doi.org/10.1038/s41746-025-02004-3 Article\nnpj Digital Medicine|           (2025) 8:588 2\nTable 1 | Benchmark of clinical variable forecasting across three datasets\nScaled mean\nabsolute error\nModel Non-small cell lung cancer (NSCLC) Intensive care unit Alzheimer ’s disease\nHemoglobin Leukocytes Lymphocytes/\nLeukocytes\nLymphocytes Neutrophils Lactate\nDehydrogenase\nMagnesium Resp.\nRate\nOxygen\nSaturation\nMMSE CDR-SB ADAS11\nChannel-\nIndependent Input\nCopy Forward 0.698 0.969 0.731 0.569 0.974 0.433 0.681 0.769 0.746 0.654 0.539 0.519\nPatchTST 0.684 0.968 0.719 0.560 0.959 0.447 0.671 0.635 0.646 0.654 0.540 0.506\nTime-LLM 0.665 0.894 0.684 0.544 0.878 0.443 0.664 0.655 0.665 0.654 0.540 0.506\nLLMTime 0.736 0.923 0.725 0.601 0.900 0.437 0.759 0.686 0.688 0.654 0.539 0.503\nChannel-\nDependent Input\nBioMistral-7B 0.984 1.097 0.756 0.997 1.953 0.600 0.790 0.770 0.945 2.064 0.883 0.728\nQwen3-32B 0.670 0.942 0.736 0.573 0.937 0.453 0.709 0.720 0.791 0.686 0.546 0.555\nTCN 0.660 0.857 0.752 0.606 0.832 0.731 0.612 0.713 0.726 Not\nApplicable\nNot\nApplicable\nNot\nApplicable\nLinear Regression 0.486 0.782 0.668 0.506 0.778 0.475 0.606 0.680 0.681 0.551 0.449 0.457\nRNN 0.529 0.806 0.671 0.511 0.801 0.433 0.597 0.647 0.674 0.545 0.463 0.465\nTransformer 0.496 0.749 0.683 0.503 0.741 0.514 0.537 0.644 0.651 0.553 0.485 0.481\nLSTM 0.526 0.781 0.665 0.495 0.764 0.441 0.567 0.643 0.642 0.545 0.468 0.475\nTemporal Fusion\nTransformer\n0.469 0.719 0.651 0.463 0.717 0.480 0.537 0.635 0.644 0.520 0.451 0.466\nTiDE 0.464 0.737 0.655 0.465 0.740 0.453 0.534 0.635 0.652 0.578 0.498 0.506\nLightGBM 0.453 0.727 0.644 0.456 0.734 0.425 0.520 0.634 0.644 0.540 0.455 0.462\nDT-GPT (ours) 0.439 0.687 0.643 0.434 0.701 0.418 0.505 0.636 0.635 0.535 0.417 0.458\nDT-GPT outperformed the baselines in the majority of cases of the non-small cell lung cancer (NSCLC), intensive-care unit (ICU), and Alzheimer’s disease dataset. All errors refer to mean absolute error (MAE; lower is better) scaled by standard deviation.\nhttps://doi.org/10.1038/s41746-025-02004-3 Article\nnpj Digital Medicine|           (2025) 8:588 3\nOn the NSCLC dataset, DT-GPT achieved an average scaled MAE of\n0.55 ± 0.04, whilst LightGBM, the second best model, achieved an average\nscaled MAE of 0.57 ± 0.05, showing a relative improvement of 3.4% (Table\n1), On the ICU dataset, DT-GPT achieved an average scaled MAE of\n0.59 ± 0.03, whilst the second best model, LightGBM, performed at\n0.60 ± 0.03, equivalent toa 1.3% improvement (Table1). On the Alzhei-\nmer’s disease dataset, DT-GPT achieved an average scaled MAE of\n0.47 ± 0.03, with Temporal Fusion Transformer being the second best\nmodel with 0.48 ± 0.02, representing a relative improvement of 1.8%. We\nnote that the scaled MAE is normalized by standard deviation, with DT-\nGPT consistently achieving absolute MAE (Supplementary Tables 6–17)\nthat is lower than the standard deviation, indicating that forecasting errors\nare smaller than the natural variability present in the data. DT-GPT is shown\nto be the best performer out of 14 models across all datasets, and achieving\nstatistical signiﬁcance over the second-best performing model on the\nNSCLC (p value < 9.6162 × 10\n−17) and ICU (p-value < 0.00043) datasets\n(Supplementary Note 2; Supplementary Tables 18–19; Supplementary\nFigs. 1–2).\nChannel-independent models,such as LLMTime, Time-LLM and\nPatchTST, perform worse with respect to scaled MAE on variables that are\nmore sparse and correlate less with other time series. Inversely, we see that\nthe channel-independent models perform relatively better on respiratory\nrate and oxygen saturation, which have generally more dense measurements\nand are less correlated to time series such as treatment, in comparison, for\nexample, to neutrophils in NSCLC.\nThe LLMs withoutﬁne-tuning performed signiﬁcantly worse than DT-\nGPT, often incorrectly hallucinating results. DT-GPT outperformed Bio-\nMistral by 47.9%, 29.1% and 61.1%, and outperformed the larger Qwen3-\n32B model by 22.9%, 19.9% and 21.1% on NSCLC, ICU and Alzheimer’s\ndisease datasets, respectively.\nTo comprehensively evaluate DT-GPT, we assessed a range of metrics,\nincluding derived classiﬁcation metrics (“Methods”). DT-GPT consistently\nperformed well across various metrics, cap t u r i n gt r a j e c t o r yt r e n d se f f e c t i v e l y\n(Supplementary Tables 6–17). For forecasting, we compared scaled MAE,\nabsolute MAE, mean absolute scaled error (MASE), symmetric mean abso-\nlute percentage error (SMAPE), and Spearman Correlation (“Methods”). For\nclassiﬁcation, we evaluated the area underthe receiver operating character-\nistic curve (AUC) of clinically low/high predictions and overall trends\n(Supplementary Tables 6–11). These metrics collectively offer insights into\ndifferent aspects of model performance (Supplementary Note 2).\nDT-GPT shows strong potential in capturing clinically relevant lab\ntrends but has limitations in predicting speciﬁc critical events. For example,\nDT-GPT struggled with forecasting critically low hemoglobin levels\n( < 7.5 g/dL; ROC AUC = 0.506), likely due to their low prevalence (1.2%;\nMethods; Supplementary Tables 6–11; Supplementary Note 2). Similarly,\nprediction of high leukocyte counts ( > 11.0 × 10⁹/L) was modest (ROC\nAUC = 0.578) and fell below the copy-forward baseline (ROC\nAUC = 0.616).\nNotably, DT-GPT demonstrated robust predictive performance across\nseveral routine yet clinically informative laboratory parameters. This\nincludes detection of mild anemia (hemoglobin below reference; ROC\nAUC = 0.793) and elevated LDH (lactate dehydrogenase; >222 U/L; ROC\nAUC = 0.793), a marker of NSCLC progression\n36. It also captured three-\nweek trends in hemoglobin (increasing/decreasing; ROC\nAUCs = 0.704/0.638) and rising leukocytes, lymphocytes, and neu-\ntrophils suggestive of inﬂammation (ROC AUCs = 0.65–0.68)37.\nDT-GPT forecasts preserved inter-variable relationships. The corre-\nlations between the variables forecasted by DT-GPT aligned with the cor-\nrelations between the variables in the test datasets with an R\n2 of 0.98 and\n0.99, whilst those of LightGBM achieved an R2 of 0.97 and 0.99 (Supple-\nmentary Fig. 3) on the NSCLC and ICU datasets, respectively. Additionally,\nDT-GPT outperformed LightGBM in the majority of timepoints in both\ndatasets, demonstrating that the improvement was consistent across time\n(Fig. 2a, b). For Alzheimer’s disease, both DT-GPT and the second best\nmodel TFT achieved an R2 of 0.99.\nDT-GPT can be further improved by utilising alternative trajectory\naggregation methods. To inspect both low and high MAE predictions from\nD T - G P T ,w ev i s u a l i z e dt w os a m p l ei n dividual-patient forecasts for the\nvariable neutrophils (Fig.2c, d) picked from the low and high end of per-\nformance distribution (Fig.2e).\nIt is important to note that theﬁnal prediction was derived by aver-\naging 30 generated trajectories and that, even in poor performing cases,\nindividual non-averaged forecasted trajectories sometimes succeeded in\ncapturing aspects of the true trajectory.\nTo assess the impact of trajectory aggregation, we calculated the error\ngiven an optimal aggregation. To thisend, we selected the individual tra-\njectories with the lowest scaled MAE and recalculated the hypothetical\nscaled MAE on the NSCLC dataset, achieving a 26% improvement in error\nto 0.40 ± 0.02, without any further model training, noting that this is a\ntheoretical lower bound. Finally, we observed that in the distribution of\nscaled MAE for neutrophils across all patients, most of the errors were right-\nskewed, indicating that high errorscame from a small number of patients\nwith likely uncommon trajectories (Fig.2e).\nDT-GPT preserves the overall distribution of target variables—a\nproperty that, while not sufﬁcient, is arguably necessary for clinically\nmeaningful forecasting. To assess this, we computed the\nKolmogorov–Smirnov (KS) statistic across all target variables in the\nNSCLC cohort, comparing predicted and true distributions (Fig.3a).\nDT-GPT exhibited the lowest median KS score among all models,\nindicating the best distributional alignment. Notably, several recent\nbaselines, including TiDE, TCN, and TFT, struggled with the distribution\nmodeling. We also visualized the distributions of the ground truth (Fig.3b)\nand DT-GPT predictions (Fig.3c), alongside LLMTime which had the\nsecond lowest mean score on the Kolomogorov-Smirnov statistic (Fig.3d),\nand LightGBM which was the best performing baseline with respect to\nscaled MAE (Fig.3e).\nDT-GPT is robust to common RWD challenges\nDT-GPT isﬂexible and robust to common practical data challenges, exhi-\nbiting desired properties in a variety of ablation studies, here exempliﬁed on\nthe average performance on all six clinical variables of the NSCLC dataset.\nFirst, DT-GPT performance was competitive with baselines after training\nwith data corresponding to 5000 patients and it further improved with the\nnumber of patients in the training dataset (Fig.4a; Table1), and consistent in\nfurther subsampling ablation studies (Supplementary Table 20). Addi-\ntionally, DT-GPT could handle increased input missingness, with perfor-\nmance degradation only showing after more than 20% of the input was\nrandomly masked, on top of the 94.4% initial missingness of the NSCLC\ndataset (Fig.4b). Thirdly, DT-GPT was stable to misspellings in the input,\nonly signiﬁcantly degrading in performance after 25 misspellings per patient\nsample (Fig.4c). We note that misspellings cannot be handled by most\nestablished machine learning methods and either require completely\ndropping or manual curation of the data.\nDT-GPT enables prediction insights and zero-shot forecasting\nDT-GPT retains its conversational capability post-ﬁne-tuning for the\nforecasting task, facilitating user interaction and enabling the inquiries into\nthe reasoning behind predictions. For each patient sample, 10 predicted\ntrajectories were generated, accompanied by a set of explanatory variables\nelucidating these predictions (Fig.5a). We extracted explanatory variables\nfrom 25,575 out of 27,730 chatbot responses. The most inﬂuential variables\nwere therapy, ECOG status and leukocyte count (Fig.5b; Supplementary\nTable 21; Supplementary Figs. 4–9; Supplementary Note 3).\nTherapy emerged as a key determinant of hemoglobin dynamics,\naligning with existing literature38,39. Patients receiving immunotherapy and\ntargeted therapy generally exhibited higher hemoglobin levels over time\ncompared to those undergoing chemotherapy or combination therapies\n(i.e., chemotherapy and immunotherapy), where hemoglobin levels tended\nto decline due to the chemotherapy-induced bone marrow suppression\n(Fig. 5c; Supplementary Fig. 4)\n40.\nhttps://doi.org/10.1038/s41746-025-02004-3 Article\nnpj Digital Medicine|           (2025) 8:588 4\nECOG status also played a signiﬁcant role in shaping hemoglobin\ntrajectories. The last recorded ECOG value in a patient’s medical history was\npredictive of future hemoglobin levels (Fig.5d), with lower ECOG values—\nindicative of fewer performance restrictions—correlating with higher\nhemoglobin levels over time, consistent with prior research41,42. Addition-\na l l y ,a g eh a sb e e nw i d e ly recognized as an important prognostic factor43.\nNotably, theseﬁndings are also reﬂected in the original data, reinforcing the\nvalidity of DT-GPT’s predictions (Supplementary Fig. 9).\nDT-GPT enables zero-shot forecasting of non-target clinical variables,\nexpanding its applicability beyondﬁne-tuned predictions. It can forecast 69\nnon-target clinical variables that arerecorded in patient medical histories\nbut were not explicitly included during modelﬁne tuning. In our experi-\nments, we forecasted each non-target variable separately (Fig.5e) and\nextracted 81,004 trajectories from 81,918 forecasting results.\nTo benchmark DT-GPT’s zero-shot performance, we compared it\nagainst a traditional machine learningapproach. We extensively trained 69\nLightGBM models, each using data fromover 13,000 patients for individual\ntarget variables, and compared their performance to a single DT-GPT\nmodel that received no such additional training (i.e., zero-shot setting) and\ntherefore was at a disadvantage. LightGBM was therefore anticipated to\nperform better than the zero-shot DT-GPT model.\nSurprisingly, zero-shot DT-GPT outperformed LightGBM on 13 out of\n69 non-target variables (Fig.5f. The variables with improved performance\ncan be described as closely related to the target variables (Fig.5g). For\ninstance, segmented neutrophils, band form neutrophilsand neutrophils by\nautomated counthave different LOINC codes from the trained variable\n(30451-9, 26507-4, 751-8, respectively), but these measurements were\nfunctionally related to the target variableneutrophils(LOINC 26499-4). A\ntable containing scaled MAE values for DT-GPT and the LightGBM\nbaseline is provided in Supplementary Table 22.\nWe identiﬁed that DT-GPT performs better in zero-shot predictions\nfor variables highly correlated with theﬁne-tuned targets. Speciﬁcally, 11 of\n13 non-target variables for which DT-GPT demonstrates equal or superior\nperformance compared to LightGBM, exhibit a strong Spearman correla-\ntion coefﬁcient ( |ρ | > 0.7) with at least oneﬁne-tuned target variable\n(Supplementary Fig. 10). For the remaining well-performing zero-shot\ntargets without strong correlations, feature importance analysis and relevant\nliterature suggest that DT-GPT may capture clinically meaningful rela-\ntionships, such as the ferritin-to-hemoglobin ratio and components of the\nAlbumin-Bilirubin (ALBI) score in NSCLC patients (Supplementary Fig.\n11; Supplementary Table 23)\n44–46.\nDiscussion\nOur mainﬁnding is that a simple yet effective method allows training LLMs\non EHRs and study data to generate detailed patient trajectories that pre-\nserve inter-variable correlations. This method achieves state-of-the-art\nperformance in clinical forecasting,while closely reproducing the dis-\ntribution of original data and outperforming baselines in predicting clini-\ncally meaningful events in the trajectory. This highlights the potential of\nusing LLMs as a digital twin platform that can mimic individual patients,\nwith applications such as treatment selection and clinical trial support.\nBuilding on past LLM research in general forecasting, DT-GPT out-\nperforms existing baselines\n20,21 in NSCLC, ICU and Alzheimer’ sd i s e a s e\ndatasets. Theseﬁndings align with recent LLM forecasting developments,\ndemonstrating that clinically-speci ﬁc adjustments enable accurate\npredictions18,19. Further analysis of several existing LLM forecasting\nFig. 2 | DT-GPT achieves state-of-the-art performance for clinical trajectory\nforecasting. aThe long-term non-small cell lung cancer (NSCLC) andb the short-\nterm intensive-care unit (ICU) dataset, with the x-axis showing relative time points\nand the y-axis the corresponding scaled mean absolute error (MAE), comparing\nwith the second best forecasting model LightGBM. The scaling is done by the\nstandard deviation, allowing comparison across variables with different value ranges\nand calculating aﬁnal performance score by averaging across the variables. Here\nexempliﬁed, DT-GPT forecasts of neutrophil counts in patients with (c) low and\nd high error, for all weeks where the ground truth exists.e Histogram of MAE\ndistribution for all predicted neutrophil counts.\nhttps://doi.org/10.1038/s41746-025-02004-3 Article\nnpj Digital Medicine|           (2025) 8:588 5\napproaches reveals that channel dependent modeling is a crucial aspect for\npatient trajectories, with DT-GPT showing that even a simple approach here\ncan be highly effective. Notably,ﬁne-tuning remains necessary for optimal\nperformance, as demonstrated by the lower accuracy of non-ﬁne-tuned\nLLMs, even when benchmarked against signiﬁcantly larger models. Addi-\ntionally, DT-GPT’s generative nature allows for multiple trajectory simu-\nlations per patient, offering insightsinto possible patient scenarios, cohort\nsimulations, and uncertainty estimates. Finally, while all models were\noptimized for the forecasting task only, DT-GPT consistently outperformed\nbaselines in classiﬁcation tasks in detecting clinically relevant events by\nachieving best or second-best performance.\nT h ep o s i t i v ep e r f o r m a n c eo fL L M sfor patient forecasting may stem\nfrom parallels between natural language and biomedical data, such as non-\nrandom missingness. For example, a doctor might skip measuring blood\npressure if a patient appears healthy, indicating information by omission.\nNatural language implicitly handles such ambiguity; unspoken words can\nstill convey meaning or none at all. Recent advancements suggest that LLMs\ncan capture these complex relationships\n47.\nDT-GPT addresses EHR challenges including noise, sparsity, and lack\nof data normalization16. Unlike most established machine learning models\nthat require data normalization and imputation, DT-GPT operates without\nthese requirements. Here, we demonstrated its robustness to sparsity,\nmisspellings, and noisy medical data often encountered in real-world\ndatasets. Moreover, EHR data oftencontain mixed data encodings; for\ninstance, drug information may vary in encoding, such as the dosage used or\nnoted only as“administered”, both of which DT-GPT handles without\nadditional preprocessing. Overall, DT-GPT simpliﬁes and streamlines data\npreparation, thus enabling faster deployment across diverse datasets.\nDT-GPT can be inquired about the rationale of predictions, which\nincreases the interpretability of the model. This capability helps bridge\nFig. 3 | DT-GPT resembles the distribution of the original data. aThe distribution\nof DT-GPT forecasted values is the closest to the ground truth distribution according\nto the absolute Kolmogorov-Smirnov distance. This can also be observed from the\ndistribution histograms associated with (b) the ground truth,c DT-GPT,\nd LLMTime, ande LightGBM. While LightGBM has the lowest scaled MAE after\nDT-GPT, LLM-based methods such as LLMTime more accurately resemble ground-\ntruth data distribution. Lymph./Leuk. lymphocytes/leukocytes).\nFig. 4 | DT-GPT is robust to common RWD issues\nin the long-term NSCLC dataset. aMean absolute\nerror (MAE) according to the number of patients in\nthe training set. Assessing impact on MAE based on\n(b) added missingness, on top of the baseline 94.4%\nmissingness of the NSCLC dataset, andc injected\nmisspellings in the input.\nhttps://doi.org/10.1038/s41746-025-02004-3 Article\nnpj Digital Medicine|           (2025) 8:588 6\nthe gap between medical expert and model, enabling the exploration of\nprediction rationales and alternative patient scenarios efﬁciently. We\nbelieve that this advancement c ould enhance human-computer\ninteraction with AI predictions and may positively affect clinical\np r a c t i c e si nt h en e a rf u t u r e .\nDT-GPT enables zero-shot predictions, demonstrating its ability to\nforecast variables not explicitly included in itsﬁne-tuning phase by learning\ntheir dynamics and adapting to novel tasks. Remarkably, zero-shot DT-GPT\noutperforms a supervised, fully-trained machine learning model on a subset\nof clinical variables, highlighting the pioneering potential of LLM-based\napproaches in RWD forecasting.\nApplying the preliminary interpretability approach also on the zero\nshot variables, we hypothesize that themodel is potentially able to capture\nlatent clinical knowledge, such as the importance of the ferritin-to-\nhemoglobin ratio and parts of the Albumin-Bilirubin (ALBI) score, both\nwhich are emerging prognostic biomarkers in NSCLC\n45,46. It is important to\nnote that the underlying BioMistral 7B model was trained on a vast amount\nof biomedical databases and publications. Therefore, these are preliminary\nFig. 5 | DT-GPT preserves its conversational ability after theﬁne-tuning,\nallowing inquiring into prediction rationale and zero-shot forecasting.\na Example of a chatbot interaction providing explanations for predictions.b Five\nmost important variables for predicting all variables derived from forecasting test\npatient samples with 10 predicted trajectories each.c The most important variable,\ntherapy, inﬂuences predicted hemoglobin trajectories, with (d) the corresponding\nground truth. Here, the lines show average trajectories and the error bars correspond\nto the standard error.e The second most important variable, ECOG, inﬂuences\npredicted hemoglobin trajectories, andf showing the corresponding ground truth.\nLines represent average trajectories and the error bars correspond to the standard\nerror. g Example of a chatbot interaction for forecasting a variable not previously\ntrained on.h We train 69 separate LightGBM models on other variables, whilst the\nsingle DT-GPT model receives no further training, resulting in DT-GPT out-\nperforming LightGBM models on 13 out of 69 non-target variables.i DT-GPT is\nsuperior for variables more biologically related to the target variables used during\nﬁne tuning, with the respective LOINC codes depicted in parentheses. ECOG\nEastern Cooperative Oncology Group performance status scale, LDH lactate\ndehydrogenase, ALT alanine aminotransferase).\nhttps://doi.org/10.1038/s41746-025-02004-3 Article\nnpj Digital Medicine|           (2025) 8:588 7\nhypotheses that require extensive investigation and validation from clinical\nexperts.\nDT-GPT shows promise for clinical trajectory forecasting, with strong\nperformance on standard metrics (e.g., MAE) and robust modeling of\ntemporal dependencies. It effectively detects moderate abnormalities such as\nanemia, tracks inﬂammation-related trends, and predicts progression\nmarkers such as elevated LDH. However, performance declines for speciﬁc\nacute events—e.g., severe hemoglobin drops or high leukocyte counts—\nhighlighting the challenge of forecasting low-prevalence, high-variance\noutcomes. Future improvements will require methods that enhance sensi-\ntivity to high-risk events, such as tailored loss functions, anomaly detection,\nand integration of unstructured clinical data.\nA challenge of LLM-based models is the restricted number of\nsimultaneously forecasted variab les. The current constraint on the\nnumber of forecasted variables is due to the limited sequence length of\nboth input and output of the LLMs used inﬁne-tuning. Advances in\nextending the context length will enable modeling of additional patient\nvariables, such as by using larger, more advanced models such as\nQwen3-32B as the base model. Furthermore, we anticipate that tran-\nsitioning from zero-shot to few- shot learning, where the model\nreceives further training on a small subset of data, would enable a wider\nspan of forecasted variables and extend DT-GPT’s applicability to\nbroader clinical challenges.\nFuture work can also take inspiration from developments in LLM-\nbased forecasting. Speciﬁcally, ideas such as patching and prompt-as-a-\npreﬁxf r o mT i m e - L L M\n22, as well as normalization and generation of con-\ntinuous likelihoods from LLMTime21, can be adapted for clinical use, further\nimproving forecasting performance. Additionally, even though DT-GPT\nwas able to capture the clinically relevant events better than other models,\nperformance can still be improved to increase clinical relevance, therefore\nwe consider the optimization of the classiﬁcation performance to be an\nimportant direction of future work. Related to this, future research should\nalso focus on developing disease-speciﬁc forecasting metrics that correlate\nwell with clinical utility.\nAnother established shortcoming of LLM-based models is their ten-\ndency to hallucinate, as well as recreating the biases from the underlying\ndata. In our case, the hallucination could be reﬂected in explainability results\nnot necessarily providing true answers. This is a critical aspect for the\nmedical domain, and we believe that a human-in-the-loop setup will be\nrequired, together with advanced training of clinicians on the use of LLM\noutputs. Regarding model biases, it is well established that models recreate\nthe biases from the underlying data, which is especially pronounced in\nminority populations\n48. To overcome the bias issues, methodological work,\ntraining of users, as well as the gathering of large scale, diverse clinical\ndatasets, is needed.\nFinally, we observe that high error predictions often occur due to the\nhigh variance between the multiple generated trajectories of each patient\nsample, with the mean aggregation into theﬁnal prediction not capturing\nkey dynamics. It is thus an open challenge to develop improved aggregation\nmethods, for example by using a second LLM as an arbiter or by having a\nhuman expert select the most realistic trajectory.\nIn conclusion, DT-GPT highlights the utility of using LLMs as a\ndigital twin forecasting platform, enabling state-of-the-art and stable\npredictions, exploratory interpretability via a natural-language inter-\nface, and forecasting of patient variables not used in ﬁne-tuning.\nWhilst further advancements are needed for wide-scale deployment,\nDT-GPT exhibits digital twin behaviors, potentially reproducing many\naspects of the patients it represents, and surpassing traditional AI\nmethods optimized for individual variables. We believe that through\nfurther method development and extensive validation, patient-level\ndigital twins will impact clinical trials by supporting biomarker\nexploration, trial design, and interim analysis. Additionally, future\ndigital twins will assist doctors i n treatment selection and patient\nmonitoring. Overall, we envision LLM-powered digital twins becom-\ning integral to healthcare systems.\nMethods\nDT-GPT is a method that employs pre-trained LLMsﬁne-tuned on clinical\ndata (Fig.6a). Notably, this method is agnostic regarding the underlying\nLLM and can be applied without architectural changes to any general-\npurpose or specialized text-focused LLM. We trained and evaluated DT-\nGPT for forecasting patients’ laboratory values across three independent\ndatasets, i.e., non-small cell lung cancer (NSCLC), intensive care unit (ICU),\nand Alzheimer’s disease patients.\nNSCLC dataset\nFor the US-based NSCLC dataset, we used the nationwide Flatiron Health\nEHR-derived de-identiﬁed database. The data are de-identiﬁed and subject\nto obligations to prevent re-identiﬁcation and protect patient con-\nﬁdentiality. The Flatiron Health database is a longitudinal database, com-\nprising de-identiﬁed patient-level structured and unstructured data, curated\nvia technology-enabled abstraction49,50. During the study period, the de-\nidentiﬁed data originated from approximately 280 cancer clinics ( ~ 800\nsites of care).\nThe study included 16,496 patients diagnosed with NSCLC from 01\nJanuary 1991 to 06 July 2023. The majority of patients in the database\noriginate from community oncology settings; relative community/academic\nproportions may vary depending on thestudy cohort. Patients with a birth\nyear of 1938 or earlier may have an adjusted birth year in Flatiron Health\ndatasets due to patient de-identiﬁcation requirements. To harmonize the\ndata, we aggregated all values in a week based on the last observed value.\nWe focused on the 50 most commondiagnoses and 80 most common\nlaboratory measurements, complemented by the Eastern Cooperative\nOncology Group (ECOG) score, metastases, vitals, drug administrations,\nresponse, and mortality variables totaling 773,607 patient-days across 320\nvariables.\nFor every NSCLC patient, we dividedtheir trajectory into input and\noutput segments based on the start date of each line of therapy to create each\npatient sample. All variables up to the start date were considered input data.\nThe objective was to predict the weekly values up to 13 weeks after the start\ndate of the following variables and their respective LOINC codes: hemo-\nglobin (718-7), leukocytes (26464-8), lymphocytes/leukocytes (26478-8),\nlymphocytes (26474-7), neutrophils (26499-4) and lactate dehydrogenase\n(2532-0). These variables were selected due to their frequent measurement\nand relevance in reﬂecting key characteristics of NSCLC treatment response\n(Supplementary Tables 1, 2).\nICU dataset\nTo demonstrate the generalizability of DT-GPT, we analyzed ICU trajec-\ntories from the publicly-accessible Medical Information Mart for Intensive\nCare IV (MIMIC-IV) dataset\n25. We employed an established processing\npipeline, resulting in 300 input variables across 1,686,288 time points from\n35,131 patients\n51.\nHere, the objective was to predict a patient’s future hourly lab variables\ngiven theirﬁrst 24 h in the ICU. Speciﬁcally, the patient history was con-\nsidered as theﬁrst 24 h for all variables, and the task was to forecast the\nfuture 24 hourly values for the following variables: O2 saturation pulse\noximetry, respiratory rate and magnesium. These variables were selected\ndue to having the highest temporal variability, thus making the forecasting\ntask more challenging, and the fact that at least 50% of patients had at least\none measurement for each, highlighting their widespread clinical usage\n(Supplementary Tables 1, 3, 4). These criteria not only increased the fore-\ncasting challenge, but also ensured wide representation across the patient\npopulation.\nAlzheimer’s disease dataset\nTo further demonstrate the generalizability of DT-GPT, we ran DT-GPT\nand the baseline models on the Alzheimer’s disease dataset, based on the\nAlzheimer’s Disease Neuroimaging Initiative (ADNI) database (adni.loni.\nusc.edu). The ADNI was launched in 2003 as a public-private partnership,\nled by Principal Investigator Michael W. Weiner, MD. The primary goal of\nhttps://doi.org/10.1038/s41746-025-02004-3 Article\nnpj Digital Medicine|           (2025) 8:588 8\nADNI has been to test whether serial magnetic resonance imaging (MRI),\npositron emission tomography (PET), other biological markers, and clinical\nand neuropsychological assessment can be combined to measure the pro-\ngression of mild cognitive impairment (MCI) and early Alzheimer’s\ndisease (AD).\nWe preprocessed the dataset, including 1140 patients. The task was to\npredict the 24 month trajectory of three cognitive variables, given the\nbaseline measurements of the patients. Speciﬁcally, the variables were Mini\nMental State Examination (MMSE), Clinical Dementia Rating sum of boxes\n(CDR-SB) and Alzheimer’s Disease Assessment Scale (ADAS11), which are\nkey indicators of cognitive decline commonly measured in Alzheimer’s\ndisease patients (Supplementary Tables 1, 5).\nData splitting andﬁltering\nThe NSCLC and ICU datasets were spl i ta tt h ep a t i e n tl e v e li n t o8 0 %\ntraining, 10% validation, and 10% test set. The splitting was performed\nrandomly for the ICU dataset, whilst stratiﬁed by group stage, smoking\nstatus, number of observations per visit and number of visits with drug\nadministrations to ensure a balanced evaluation. The Alzheimer’sd i s e a s e\ndataset was randomly split into 80% training and 20% test, selected due to\nthe small sample size, with all hyperparameters determined via a further\nsplitting on the training set. Thus, each set comprised disjoint sets of patients\nto avoid data leakage. The test sets were solely used forﬁnal evaluation and\nto assess the model’s generalizability (Fig.6b).\nWe applied a two-step outlierﬁltering procedure on all datasets: all\ntarget values below or above three standard deviations wereﬁltered outﬁrst,\nthen we calculated new standard deviation values on theﬁltered dataset and\nclipped target values below and above those values. This approach ensured\nthat the noise present in the data was removed, while some of the outliers\nwere replaced with reasonable low or high values to maintain the biological\nsignal. The data for all of the baselines excluding DT-GPT were then also\nstandardized using z-scores.\nEncoding\nWe encoded patient trajectories by using templates that converted medical\nhistories based on EHRs into a text format compatible with LLMs, as pro-\nposed by Xue et al.19 and Liu et al.19,20 (Fig.6c, d; Supplementary Note 4). The\ninput template is structured into fourcomponents: (1) patient history, (2)\ndemographic data, (3) forecast dates and (4) prompt. The patient history\ncontains a chronological description of patient visits, requiring no data\nimputation for missing variables. The output trajectories were also encoded\nusing templates, containing only the relevant output variables for the\nforecasted time points. We utilized a manually developed template for input\nencoding and JSON-format encoding for the output (Supplementary\nFig. 12).\nLLMs andﬁne-tuning\nWe utilized the biomedical LLM BioMistral 7B DARE, since it is pro-\nvided with an open source license and based on a recognized LLM33.\nFurthermore, BioMistral is instruction tuned and through its biomedical\nspecialization incorporates compressed representations of vast amounts\nof biomedical knowledge. We furtherﬁne tuned this LLM using the\nFig. 6 | The DT-GPT framework transforms EHRs into text and subsequently\nﬁne-tunes an LLM on this data. aOverview of the pipeline: datasets are split and\nencoded into input/output text based on landmark timepoints, then used toﬁne-\ntune an LLM, here BioMistral. The model output is evaluated for trajectory fore-\ncasting whilst zero-shot predictions and variable importances are explored via a chat\ninterface. b Sample size, visit frequency, and sparsity of the Alzheimer’s disease\n(AD), non-small cell lung cancer (NSCLC), intensive care unit (ICU) datasets.\nc Input andd output encoded examples, emphasizing the chronological encoding of\nobservations.\nhttps://doi.org/10.1038/s41746-025-02004-3 Article\nnpj Digital Medicine|           (2025) 8:588 9\nstandard cross entropy loss, masked so that the gradient was only\ncomputed on the output text. We performed 30 predictions for each\npatient sample during evaluation, then took the mean for each time\npoint as theﬁnal prediction\n21,52. All hyperparameters of DT-GPT used\nﬁne-tuning (Supplementary Note 5) and are compared to baseline\nmodels (Supplementary Note 6).\nHandling of missing and noise data\nWe investigated the ability of DT-GPT as a LLM-based model to handle\nmissing data and misspelling in the input prompts. For the missing data\nstudy, we randomly masked between 0 and 80% of data, in addition to the\nalready missing data in a dataset. Evaluation of the effect of missingness was\nperformed on a randomly sampled 200 patients from the test set, which can\npotentially lead to higher variance in the results, but allowed for a more\nextensive exploration.\nFor the noise study, we introduce a misspelling algorithm. This algo-\nr i t h mr a n d o m l yp e r f o r m se i t h e rp e r t urbation, insertion, deletion, or\nreplacement, using all ASCII letters & digits, applied to the entire input text.\nThis includes dates, variable names, values, baseline information, and\nprompts. One operation is considered one misspelling.\nFor the evaluation of the effects of RWD missingness and noise we\nrandomly sampled 200 patients of the test set, which can potentially lead to\nhigher variance in the results, but allowed for a more extensive exploration.\nChatbot and zero-shot learning\nWe employed the DT-GPT model to run a chatbot based on patient his-\ntories for prediction explanation and zero-shot forecasting. For this,ﬁrst we\nused DT-GPT to generate forecasting results from patient history and,\nconsecutively, added a task-speciﬁc prompt surrounded by the respective\ninstruction-indication tokens to the DT-GPT chat history for receiving a\nresponse. For prediction explanation, the prompt asked for the most\nimportant variables inﬂuencing the predicted trajectory. For zero-shot\nforecasting, the prompt speciﬁed the output format and days to predict new\nclinical variables that were not subject to optimizationduring training.\nExample prompts and chatbot interactions for both tasks are provided in\nSupplementary Note 7 and Fig.5a, e.\nForecasting evaluation\nForecasting metrics, i.e. Eqs. (1)–(5), are designed to quantify the disparity\nbetween predicted and observed numeric values, providing an objective\nmeasure of the model’s predictive accuracy (Supplementary Note 8). Letvt\nðiÞ\nbe an observed (non-missing) value of clinical variablev for a subjecti,\ni ¼ 1; /C1/C1/C1 ; n,w h e r en is the total number of subjects, and time stept,\nt ¼ 1; /C1/C1/C1 ; Ti,w h e r eTi is the total number of time steps for the subjecti.L e t\nbaseline valuevðiÞ\n0 be the baseline value at time stept0, t ¼ 0. We denote\npredicted values as^vðiÞ\nt . The forecasting metrics used are mean absolute error\n(MAE), scaled MAE, mean absolute scaled error (MASE), symmetric mean\nabsolute percentage error (SMAPE) and Spearman correlation coefﬁcient\ndeﬁned as follows:\nMAE ¼\n1\nn\nXn\ni¼1\n1\nTi\nXTi\nt¼1\njvðiÞ\nt /C0 ^vðiÞ\nt j ð1Þ\nscaled MAE¼ MAE\nσ ð2Þ\nwhereσ is the standard deviation of the clinical variable after outlierﬁltering;\nMASE ¼ MAE\n1\nn\nPn\ni¼1\n1\nTi\nPTi\nt¼1jvðiÞ\nt /C0 vðiÞ\n0 j ð3Þ\nSMAPE ¼ 200\nn\nXn\ni¼1\n1\nTi\nXTi\nt¼1\njvt\nðiÞ /C0 ^vt\nðiÞj\njvðiÞ\nt jþj ^vðiÞ\nt j\n1fjvðiÞ\nt jþj^vðiÞ\nt j6¼0g ð4Þ\nwhere 1 is the indicator function to avoid division by 0;\nSpearmanρ ¼\nPn\ni¼1\n1\nTi\nPTi\nt¼1 ðR½vðiÞ\nt /C138/C0 R½v/C138Þð R½bvðiÞ\nt /C138/C0 R½bv/C138Þ\nﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPn\ni¼1\n1\nTi\nPTi\nt¼1ðR½vðiÞ\nt /C138/C0 R½v/C138Þ 2 Pn\ni¼1\n1\nTi\nPTi\nt¼1 ðR½bvðiÞ\nt /C138/C0 R½bv/C138Þ 2\nq\nð5Þ\nwhere R½:/C138 is a rank function, ordering values from lowest to the highest,\nwhereby, for the data points with the same value, their average rank is\nassigned, and\nR½v/C138¼ 1\nn\nPn\ni¼1\n1\nTi\nPTi\nt¼1 R½vðiÞ\nt /C138 and\nR½bv/C138¼ 1\nn\nPn\ni¼1\n1\nTi\nPTi\nt¼1 R½bvðiÞ\nt /C138 are the mean ranks of actual and predicted\nvalues, respectively.\nWe chose scaled MAE, i.e., Eq. (2), as our primary metric as it allows\ncomparison across all variables, and hence can be used to benchmark dif-\nferent models on all datasets.\nClassiﬁcation evaluation. Classiﬁcation metrics assess the model’s\nclinical utility to capture events, such as abrupt changes in clinical vari-\nables indicative of acute conditions (e.g., sudden drops or increases) or\nprolonged trends in variable changes that are characteristic of a chronic\ncondition (e.g., gradual increases or decreases over extended periods).\nBelow, we provide detailed deﬁnitions of the metrics employed in our\nevaluation. An interpretation of introduced metrics is provided in Sup-\nplementary Note 8.\nFirst, we assess the model’s ability to detect values outside the normal\nrange of clinical variables. Let½v\nmin ; vmax/C138 be the reference interval for the\nclinical variablev. We label the observed variable valuevðiÞ\nt as “low” if\nvðiÞ\nt < vmin,a s“high” if vðiÞ\nt > vmax and as“normal” if vmin < vðiÞ\nt < vmax.W e\ndeﬁne vðiÞ\nt as “not low” if it is“normal” or “high”,a s “not high” if it is\n“normal” or “low”,a n da s“not normal” if it is“low” or “high”.A n a l o g o u s l y ,\nwe label each predicted variable value^vðiÞ\nt . With this, we are in the classiﬁ-\ncation task settings.\nFor the binary classiﬁcation tasks“low” versus“not low”, “high” versus\n“not high”,a n d“normal” versus “not normal”, we calculate area under the\nreceiver operating characteristic curve (AUC ROC) and denote it as\nAUC\nlow, AUChigh and AUCnormal, respectively. For the multiclass classiﬁ-\ncation task“low” versus“normal” versus“high”,w ec a l c u l a t ew e i g h t e dA U C\nROC, denoted by AUC weighted (Eq. (6)), that is given by\nAUCweighted ¼\nðAUClow × #lowÞþð AUCnormal ×# n o r m a lÞþð AUChigh ×# h i g hÞ\n#low þ #normal þ #high\nð6Þ\nwhere #low, #normal and #high, correspond to the number of observed\nvariables valuesvt\nðiÞ labeled as“low”, “normal” and “high” respectively.\nWeighted aggregation accounts for the class imbalance, whereby most of the\nvariable values fall within the reference range and are labeled as“normal”.\nWe evaluated the model’s trend forecasting performance by analyzing\nits predicted value trajectories over a speciﬁed time intervals.W i t h i nt h e s e\nforecasts, a predicted value v\nðiÞ\nt was classiﬁed as ‘decreasing trend’ if\nvðiÞ\ntþ1 < vðiÞ\nt or as an‘increasing trend’ if vðiÞ\ntþ1 > vðiÞ\nt . For a trend to be classiﬁed\nat timet, the direction of change betweenconsecutive predicted values had\nto be consistent throughout the entirepreceding lookback window. Speci-\nﬁcally, vðiÞ\nt was classiﬁed as‘decreasing trend’ only ifvðiÞ\nkþ1 < vðiÞ\nk for all time\nsteps k within the interval½timeðtÞ/C0 s; timeðtÞ/C138 ,a n d‘increasing trend’ only\nif vðiÞ\nkþ1 > vðiÞ\nk for allk in that same interval. Here,timeðtÞ represents the time\nsince the last input measurement. Ground truth trends were derived simi-\nlarly from observed data. We then assessed the model’s classiﬁcation of these\ntrends in its forecasts using two binary classiﬁcation tasks: ‘decreasing’\nversus‘not decreasing’,a n d‘increasing’ versus‘not increasing’.P e r f o r m a n c e\nwas quantiﬁed by calculating the area under the receiver operating char-\nacteristic curve (AUC) based on the forecasted values, yieldingAUCtrend#\nand AUCtrend\". Forecasted values were excluded from this analysis if\ntimeðtÞ < s to ensure a complete lookback window was available. We provide\nan example and illustration in Supplementary Fig. 13.\nhttps://doi.org/10.1038/s41746-025-02004-3 Article\nnpj Digital Medicine|           (2025) 8:588 10\nWe performed the classiﬁcation evaluation only on the NSCLC data.\nFor this, we used parameters for the reference ranges½vmin; vmax/C138 as found in\nthe literature. For hemoglobin [g/dL], we set [14, 18] and [12, 16] for male\nand female patients\n53, respectively. We set [4.5, 11.0] for leukocytes [109/L]54,\n[20, 40] for leukocytes/lymphocytes [%]54, [1.0, 4.0] for lymphocytes\n[109/L]55, [1.8, 7.5] for neutrophils [109/L]55 and [122, 222] for lactate\ndehydrogenase [U/L]36.\nWe further address the model ability to detect a signiﬁcant drop in\nhemoglobin associated with a bleeding by calculating AUClow with\nvmin = 7.5. As for the trend detection, we consider time intervals of 3 weeks\nand sets ¼ 21 days for all NSCLC variables. This time period is clinically\nrelevant to capture the increasing ordecreasing dynamics of a clinical\nvariable.\nData availability\nT h eF l a t i r o nH e a l t hd a t at h a ts u p p o r tt h eﬁndings of this study were ori-\nginated by and are the property of Flatiron Health, Inc., which has restric-\ntions prohibiting the authors from making the data set publicly available.\nRequests for data sharing by license or by permission for the speciﬁc purpose\nof replicating results in this manuscript can be submitted to Pub-\nlicationsDataAccess@ﬂatiron.com. The Medical Information Mart for\nIntensive Care IV (MIMIC-IV) is available online upon request under\nhttps://physionet.org/content/mimiciv. The Alzheimer’sD i s e a s eN e u r o i -\nmaging Initiative (ADNI) dataset is available online upon request under\nhttps://adni.loni.usc.edu/data-samples/adni-data/.\nCode availability\nT h ec o d ei sa v a i l a b l ea thttps://github.com/MendenLab/DT-GPT,i n c l u d -\ning all package and Python versions, as well as license information. Speciﬁc\nparameters used to generate and analyze the datasets presented in this\nmanuscript are detailed in the repository’sR E A D M Eﬁle and relevant\nconﬁgurationﬁles.\nR e c e i v e d :5M a y2 0 2 5 ;A c c e pted: 13 September 2025;\nReferences\n1. Schachter, A. D. & Ramoni, M. F. Clinical forecasting in drug\ndevelopment. Nat. Rev. Drug Discov.6, 107–108 (2007.\n2. Allen, A. et al. A digital twins machine learning model for forecasting\ndisease progression in stroke patients.Appl. Sci.11, 5576 (2021).\n3. Boulos, M. N. K. & Zhang, P. Digital twins: From personalised\nmedicine to precision public health.J. Pers. Med.11, 745 (2021).\n4. Bordukova, M., Makarov, N., Rodriguez-Esteban, R., Schmich, F. &\nMenden, M. P. Generative artiﬁcial intelligence empowers digital twins\nin drug discovery and clinical trials.Expert Opin. Drug Discov.19,\n33–42 (2024).\n5. Coorey, G. et al. The health digital twin to tackle cardiovascular\ndisease— a review of an emerging interdisciplinaryﬁeld. npj Digit.\nMed. 5, 126 (2022).\n6. Venkatesh, K. P., Raza, M. M. & Kvedar, J. C. Health digital twins as\ntools for precision medicine: Considerations for computation,\nimplementation, and regulation. npj Digit.Med 5, 150 (2022).\n7. Bordukova, M. et al. Generative AI and digital twins: shaping a\nparadigm shift from precision to truly personalized medicine.Expert\nOpin. Drug Discov.20, 821–826 (2025).\n8. Moingeon, P., Chenel, M., Rousseau, C., Voisin, E. & Guedj, M. Virtual\npatients, digital twins and causal disease models: Paving the ground\nfor in silico clinical trials.Drug Discov. Today28, 103605 (2023).\n9. Nguyen, M. et al. Predicting Alzheimer’s disease progression using\ndeep recurrent neural networks.NeuroImage 222, 117203 (2020).\n10. Jung, W., Mulyadi, A. W. & Suk, H. I. Uniﬁed Modeling of Imputation,\nForecasting, and Prediction for AD Progression. inLecture Notes in\nComputer Science168–176 (2019).\n11. Wu, F. et al. Forecasting Treatment Outcomes Over Time Using\nAlternating Deep Sequential Models.IEEE Transactions on\nBiomedical EngineeringPP,1 –10 (2023).\n12. Phetrittikun, R. et al. Temporal Fusion Transformer for forecasting vital\nsign trajectories in intensive care patients. in2021 13th Biomed Eng\nInt Conf (BMEiCON)1–5 (2021).\n13. Chang, P. et al. A transformer-based diffusion probabilistic model for\nheart rate and blood pressure forecasting in Intensive Care Unit.\nComput. Methods Prog. Biomed.246, 108060 (2024).\n14. Melnychuk, V., Frauen, D. & Feuerriegel, S. Causal Transformer for\nEstimating Counterfactual Outcomes. inInternational Conference on\nMachine Learning15293–15293 (2022).\n15. Kaddour, J., Lynch, A., Liu, Q., Kusner, M. J. & Silva, R. Causal\nmachine learning: A survey and open problems.Foundations and\nTrendsr in Optimization9,1 –247 (2025).\n16. Wornow, M. et al. The shaky foundations of large language models\nand foundation models for electronic health records.npj Digital Med.\n6, 135 (2023).\n17. Renc, P. et al. Zero shot health trajectory prediction using transformer.\nnpj Digit.Med 7, 256 (2024).\n18. Liang, Y. et al. Foundation Models for Time Series Analysis: A Tutorial\nand Survey. inProceedings of the 30th ACM SIGKDD conference on\nknowledge discovery and data mining(2024).\n19. Xue, H. & Salim, F. D. PromptCast: A New Prompt-based Learning\nParadigm for Time Series Forecasting.IEEE Transactions on\nKnowledge and Data Engineering(2023).\n20. Liu, H., Zhao, Z., Wang, J., Kamarthi, H. & Prakash, B. B. LSTPrompt:\nLarge Language Models as Zero-Shot Time Series Forecasters by\nLong-Short-Term Prompting. inAssociation for Computational\nLinguistics Findings 2024(2024).\n21. Gruver, N., Finzi, M., Qiu, S. & Wilson, A. G. Large Language Models\nAre Zero-Shot Time Series Forecasters. inAdvances in Neural\nInformation Processing Systems(2023).\n22. Jin, M. et al. Time-LLM: Time Series Forecasting by Reprogramming\nLarge Language Models. inInternational Conference on Learning\nRepresentations (2024). https://doi.org/10.48550/arxiv.2310.01728.\n23. Zhou, T., Niu, P., Wang, X., Sun, L. & Jin, R. One Fits All:Power General\nTime Series Analysis by Pretrained LM. arXiv (2023)https://doi.org/\n10.48550/arxiv.2302.11939.\n24. Loureiro, H. et al. Correlation between early trends of a prognostic\nbiomarker and overall survival in non–small-cell lung cancer clinical\ntrials. JCO Clin. Cancer Inform.7, e2300062 (2023).\n25. Johnson, A. E. et al. MIMIC-IV, a freely accessible electronic health\nrecord dataset.Sci. Data10, 1 (2023).\n26. Tombaugh, T. N. & McIntyre, N. J. The mini-mental state examination:\nA comprehensive review.J. Am. Geriatr. Soc.40, 922–935 (1992).\n27. O ’Bryant, S. E. et al. Validation of the new interpretive guidelines for\nthe clinical dementia rating scale sum of boxes score in the national\nAlzheimer’s coordinating center database.Arch. Neurol.67, 746–749\n(2010).\n28. Kueper, J. K., Speechley, M. & Montero-Odasso, M. The Alzheimeras\ndisease assessment scale-cognitive subscale (ADAS-Cog):\nmodiﬁcations and responsiveness in pre-dementia populations. a\nnarrative review.Journal of Alzheimeras Disease63, 423–444 (2018).\n29. Lim, B., Arık, S., Loeff, N. & Pﬁster, T. Temporal Fusion Transformers\nfor interpretable multi-horizon time series forecasting.Int. J. Forecast.\n37, 1748–1764 (2021).\n30. Das, A. et al. Long-term Forecasting with TiDE: Time-series Dense\nEncoder. Transactions on Machine Learning Research(2023).\n31. Nespoli, L. & Medici, V. Multivariate Boosted Trees and Applications\nto Forecasting and Control.J. Mach. Learn. Res.23,1 –47 (2022).\n32. Ke, G., Meng, Q., Finley, T., Wang, T., Chen, W., Ma, W., Ye, Q. & Liu, T.\nY. Lightgbm: A highly efﬁcient gradient boosting decision tree. In\nAdvances in neural information processing systems.30 (2017).\nhttps://doi.org/10.1038/s41746-025-02004-3 Article\nnpj Digital Medicine|           (2025) 8:588 11\n33. Labrak, Y. et al. BioMistral: A collection of open-source pretrained\nlarge language models for medical domains. arXiv (2024).\n34. Yang, A. et al. Qwen3 technical report.arXiv https://doi.org/10.48550/\narxiv.2505.09388 (2025).\n35. Nie, Y., Nguyen, N. H., Sinthong, P. & Kalagnanam, J. A Time Series is\nWorth 64 Words: Long-term Forecasting with Transformers. in\nInternational Conference on Learning Representations (2023).\n36. Farhana, A. & Lappin, S. L.Biochemistry, Lactate Dehydrogenase. (2023).\n37. Margraf, A., Lowell, C. A. & Zarbock, A. Neutrophils in acute\ninﬂammation: current concepts and translational implications.Blood\n139, 2130–2144 (2022).\n3 8 . G r o o p m a n ,J .E .&I t r i ,L .M .C h e m o t h e r a p y - i n d u c e da n e m i ai n\nadults: incidence and treatment.J. Natl. Cancer Inst.91,1 6 1 6–1634\n(1999).\n39. Abdel-Razeq, H. & Hashem, H. Recent update in the pathogenesis\nand treatment of chemotherapy and cancer induced anemia.Crit.\nRev. Oncol. Hematol.145, 102837 (2020).\n40. Wang, Y., Probin, V. & Zhou, D. Cancer therapy-induced residual bone\nmarrow injury: Mechanisms of induction and implication for therapy.\nCurr. Cancer Ther. Rev.2, 271–279 (2006).\n41. Cella, D. The functional assessment of cancer therapy-anemia (FACT-\nAn) Scale: A new tool for the assessment of outcomes in cancer\nanemia and fatigue.Semin. Hematol.34,1 3–19 (1997).\n42. Pathak, N. et al. Improving the performance status in advanced\nnon-small cell lung cancer patients with chemotherapy (ImPACt\ntrial): A phase 2 study.J. Cancer Res. Clin. Oncol.149,6 3 9 9–6409\n(2023).\n43. Tas, F., Ciftci, R., Kilic, L. & Karabulut, S. Age is a prognostic factor\naffecting survival in lung cancer patients.Oncol. Lett.6, 1507–1513\n(2013).\n44. Lee, S., Jeon, H. & Shim, B. Prognostic value of ferritin-to-hemoglobin\nratio in patients with advanced non-small-cell lung cancer.J. Cancer\n10, 1717–1725 (2019).\n45. Matsukane, R. et al. Prognostic signiﬁcance of pre-treatment ALBI\ngrade in advanced non-small cell lung cancer receiving immune\ncheckpoint therapy.Sci. Rep.11, 15057 (2021).\n46. Tomita, M., Shimizu, T., Hara, M., Ayabe, T. & Onitsuka, T. Impact of\npreoperative hemoglobin level on survival of non-small cell lung\ncancer patients.Anticancer Res28, 1947–1950 (2008).\n47. Sravanthi, S. L. et al. PUB: A Pragmatics Understanding Benchmark\nfor Assessing LLMs’Pragmatics Capabilities. inFindings of the\nAssociation for Computational Linguistics: ACL 2024(2024).\n48. Cross, J. L., Choma, M. A. & Onofrey, J. A. Bias in medical AI:\nImplications for clinical decision-making.PLOS Digit. Heal.3,\ne0000651 (2024).\n49. Ma, X., Long, L., Moon, S., Adamson, B. & Baxi, S. Comparison of\nPopulation Characteristics in Real-World Clinical Oncology\nDatabases in the US: Flatiron Health, SEER, and NPCR.medRxiv\n2020, (2023).\n50. Birnbaum, B. et al. Model-assisted cohort selection with bias analysis\nfor generating large-scale cohorts from the EHR for oncology\nresearch. arXiv preprint arXiv:2007.XXXX(2020).\n51. Gupta, M. et al. An Extensive Data Processing Pipeline for MIMIC-IV.\nin Proceedings of Machine Learning Research311–325 (2022).\n52. Wang, X. et al. Self-Consistency Improves Chain of Thought\nReasoning in Language Models. inThe Eleventh International\nConference on Learning Representations(2022).\n53. Billett, H. H., Walker, H. K., 1, W. D. H. & Hurst, J. W. Hemoglobin and\nHematocrit. inClinical Methods: The History, Physical, and Laboratory\nExaminations. 3rd Edition(1990).\n54. Riley, L. K. & Rupert, J. Evaluation of patients with leukocytosis.Am.\nFam. physician92, 1004–1011 (2015).\n55. Haematology reference ranges.https://www.gloshospitals.nhs.uk/\nour-services/services-we-offer/pathology/haematology/\nhaematology-reference-ranges/ (2024).\nAcknowledgements\nWe would like to thank Anton Kraxner for providing crucial insights into\nNSCLC, as well as Ginte Kutkaite, Hugo Loureiro, Franziska Braun, Rudolf\nKinder, Venus So, Guy Amster and Will Shapiro for their valuable input and\ndiscussions. This study was funded by F. Hoffmann-La Roche and the\nEuropean Union's Horizon 2020 Research and Innovation Programme\n(Grant agreement No. 950293–COMBAT-RES). The funder played no role in\nstudy design, data collection, analysis and interpretation of data, or the\nwriting of this manuscript. Data collection and sharing for the Alzheimer’s\nDisease Neuroimaging Initiative (ADNI) is funded by the National Institute on\nAging (National Institutes of Health Grant U19AG024904). The grantee\norganization is the Northern California Institute for Research and Education.\nIn the past, ADNI has also received funding from the National Institute of\nBiomedical Imaging and Bioengineering, the Canadian Institutes of Health\nResearch, and private sector contributions through the Foundation for the\nNational In-Institutes of Health (FNIH) including generous contributions from\nthe following: AbbVie, Alzheimer’s Association; Alzheimer’s Drug Discovery\nFoundation; Araclon Biotech; BioClinica, Inc.; Biogen; BristolMyers Squibb\nCompany; CereSpir, Inc.; Cogstate; Eisai Inc.; Elan Pharmaceuticals, Inc.; Eli\nLilly and Company; EuroImmun; F. Hoffmann-La Roche Ltd and its afﬁliated\ncompany Genentech, Inc.; Fujirebio; GE Healthcare; IXICO Ltd.; Janssen\nAlzheimer Immunotherapy Research & Development, LLC.; Johnson &\nJohnson Pharmaceutical Research & Development LLC.; Lumosity; Lund-\nbeck; Merck & Co., Inc.; Meso Scale Diagnostics, LLC.; NeuroRx Research;\nNeurotrack Technologies; Novartis Pharmaceuticals Corporation; Pﬁzer\nInc.; Piramal Imaging; Servier; Takeda Pharmaceutical Company; and\nTransition Therapeutics.\nAuthor contributions\nN.M., M.B. and P.Q. performed data processing. N.M. and M.B. performed\nmodel implementation. N.M., M.B. and D.G. performed model evaluation.\nR.R.-E., F.S. and M.P.M. supervised, designed and directed the project. N.M\nand M.B. drafted the manuscript. N.M., M.B., D.G., R.R.-E., F.S. and M.P.M.\nsubstantially revised this manuscript. All authors have read and approved\nthe manuscript.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nCompeting interests\nN.M., M.B., R.R.E. and F.S. are all employees of F. Hoffmann-La Roche.\nM.P.M. collaborates and isﬁnancially supported by GSK, F. Hoffmann-La\nRoche, and AstraZeneca. M.P.M. is supported by the European Union’s\nHorizon 2020 Research and Innovation Programme (Grant agreement No.\n950293— COMBAT-RES). N.M., M.B., R.R.E., F.S. and M.P.M. are authors\nof an in-force patent entitled“Forecasting of subject-related attributes using\ngenerative machine-learning model” (patent publication number 2025/\n021719, patent application number EP2024070632) owned by F. Hoffmann-\nLa Roche and Helmholtz Zentrum Munich. The patent covers application of\nlarge language models such as DT-GPT for forecasting of clinical trajectories\nof patients during a clinical trial. The authors have no other relevant afﬁlia-\ntions orﬁnancial involvement with any organization or entity with aﬁnancial\ninterest in orﬁnancial conﬂict with the subject matter or materials discussed\nin the manuscript apart from those disclosed.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-02004-3\n.\nCorrespondenceand requests for materials should be addressed to\nFabian Schmich or Michael P. Menden.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nhttps://doi.org/10.1038/s41746-025-02004-3 Article\nnpj Digital Medicine|           (2025) 8:588 12\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long\nas you give appropriate credit to the original author(s) and the source,\nprovide a link to the Creative Commons licence, and indicate if changes\nwere made. The images or other third party material in this article are\nincluded in the article’s Creative Commons licence, unless indicated\notherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons licence and your intended use is not permitted\nby statutory regulation or exceeds the permitted use, you will need to\nobtain permission directly from the copyright holder. To view a copy of this\nlicence, visithttp://creativecommons.org/licenses/by/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-02004-3 Article\nnpj Digital Medicine|           (2025) 8:588 13",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I3018134672",
      "name": "Helmholtz Zentrum München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I8204097",
      "name": "Ludwig-Maximilians-Universität München",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I4210094361",
      "name": "Roche Pharma AG (Germany)",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I118019719",
      "name": "Roche (Switzerland)",
      "country": "CH"
    }
  ],
  "cited_by": 5
}