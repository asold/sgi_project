{
    "title": "BatteryBERT: A Pretrained Language Model for Battery Database Enhancement",
    "url": "https://openalex.org/W4229443452",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5026142685",
            "name": "Shu Huang",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": "https://openalex.org/A5068607578",
            "name": "Jacqueline M. Cole",
            "affiliations": [
                "Rutherford Appleton Laboratory",
                "University of Cambridge"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3115677442",
        "https://openalex.org/W3127365350",
        "https://openalex.org/W2953641512",
        "https://openalex.org/W3112850967",
        "https://openalex.org/W3175823016",
        "https://openalex.org/W3202771869",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W2964864162",
        "https://openalex.org/W4214535912",
        "https://openalex.org/W4206078246",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W3047398431",
        "https://openalex.org/W2523785361",
        "https://openalex.org/W3200122731",
        "https://openalex.org/W2121879602",
        "https://openalex.org/W2086838561",
        "https://openalex.org/W2963323070",
        "https://openalex.org/W2950784811",
        "https://openalex.org/W1833243315",
        "https://openalex.org/W2800667994",
        "https://openalex.org/W3201869313",
        "https://openalex.org/W4248414713",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3088409176",
        "https://openalex.org/W4224442790"
    ],
    "abstract": "A great number of scientific papers are published every year in the field of battery research, which forms a huge textual data source. However, it is difficult to explore and retrieve useful information efficiently from these large unstructured sets of text. The Bidirectional Encoder Representations from Transformers (BERT) model, trained on a large data set in an unsupervised way, provides a route to process the scientific text automatically with minimal human effort. To this end, we realized six battery-related BERT models, namely, BatteryBERT, BatteryOnlyBERT, and BatterySciBERT, each of which consists of both cased and uncased models. They have been trained specifically on a corpus of battery research papers. The pretrained BatteryBERT models were then fine-tuned on downstream tasks, including battery paper classification and extractive question-answering for battery device component classification that distinguishes anode, cathode, and electrolyte materials. Our BatteryBERT models were found to outperform the original BERT models on the specific battery tasks. The fine-tuned BatteryBERT was then used to perform battery database enhancement. We also provide a website application for its interactive use and visualization.",
    "full_text": null
}