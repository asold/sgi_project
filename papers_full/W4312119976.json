{
  "title": "AI Psychometrics: Assessing the psychological profiles of large language models through psychometric inventories",
  "url": "https://openalex.org/W4312119976",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2776690678",
      "name": "Max Pellert",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2329203169",
      "name": "Clemens M. Lechner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099929867",
      "name": "Claudia Wagner",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A308218437",
      "name": "Beatrice Rammstedt",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A142799918",
      "name": "Markus Strohmaier",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4321455981",
    "https://openalex.org/W3172415559",
    "https://openalex.org/W3162616674",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4285242720",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2031649430",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W3034649382",
    "https://openalex.org/W4288028050",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4205807230",
    "https://openalex.org/W1994565286",
    "https://openalex.org/W2164008908",
    "https://openalex.org/W2887782043",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W1993411961",
    "https://openalex.org/W2119432837",
    "https://openalex.org/W4292545669",
    "https://openalex.org/W2165558852",
    "https://openalex.org/W2884554049",
    "https://openalex.org/W2958608582",
    "https://openalex.org/W2767879018",
    "https://openalex.org/W2958571841",
    "https://openalex.org/W4379986648",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4225364564",
    "https://openalex.org/W3156470785",
    "https://openalex.org/W3022017838",
    "https://openalex.org/W1620372854",
    "https://openalex.org/W95274681",
    "https://openalex.org/W3168584517",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2084820585",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3045421469",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W3017604292",
    "https://openalex.org/W3035507081",
    "https://openalex.org/W1555938369",
    "https://openalex.org/W4220993274",
    "https://openalex.org/W3156450532",
    "https://openalex.org/W3135346516",
    "https://openalex.org/W1997102788",
    "https://openalex.org/W3094342783",
    "https://openalex.org/W4285090338",
    "https://openalex.org/W2024871136",
    "https://openalex.org/W2607892599",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W4321277158",
    "https://openalex.org/W2953271402",
    "https://openalex.org/W4328049044",
    "https://openalex.org/W1896027656",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3092323704",
    "https://openalex.org/W3105882417",
    "https://openalex.org/W3101860695",
    "https://openalex.org/W2970200208",
    "https://openalex.org/W2005814556",
    "https://openalex.org/W4378189609",
    "https://openalex.org/W4283263983",
    "https://openalex.org/W4363624465",
    "https://openalex.org/W3213241618",
    "https://openalex.org/W3100806282",
    "https://openalex.org/W4289544215",
    "https://openalex.org/W3114950584",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4292947474",
    "https://openalex.org/W4221142858",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2996314454",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4288088047",
    "https://openalex.org/W2982295985",
    "https://openalex.org/W4385573216",
    "https://openalex.org/W4241722431",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4285183888",
    "https://openalex.org/W1630204216",
    "https://openalex.org/W2006922631",
    "https://openalex.org/W4319452268",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W3170344956"
  ],
  "abstract": "We illustrate how standard psychometric inventories originally designed for assessing non-cognitive human traits can be repurposed as diagnostic tools to evaluate analogous traits in large language models (LLMs). We start from the assumption that LLMs, inadvertently yet inevitably, acquire psychological traits (metaphorically speaking) from the vast text corpora on which they are trained. Such corpora contain sediments of the personalities, values, beliefs and biases of the countless human authors of these texts, which LLMs learn through a complex training process. The traits that LLMs acquire in such a way can potentially influence their behavior, i.e., their outputs in downstream tasks and applications in which they are employed, which in turn may have real-world consequences for individuals and social groups. By eliciting LLMs’ responses to language-based psychometric inventories we can bring their traits to light. Psychometric profiling enables researchers to study and compare LLMs in terms of non-cognitive characteristics thereby providing a window into the personalities, values, beliefs and biases these models exhibit (or mimic). We discuss the history of similar ideas and outline possible psychometric approaches for LLMs. We demonstrate one promising approach, zero-shot classification, for several LLMs and psychometric inventories. We conclude by highlighting open challenges and future avenues of research for AI Psychometrics.",
  "full_text": "AI Psychometrics: Assessing the psychological\nprofiles of large language models through\npsychometric inventories\nMax Pellert, Clemens M. Lechner, Claudia Wagner,\nBeatrice Rammstedt & Markus Strohmaier\nSeptember 2023\n1\nAbstract\nWe illustrate how standard psychometric inventories originally designed for as-\nsessing non-cognitive human traits can be repurposed as diagnostic tools to\nevaluate analogous traits in large language models (LLMs). We start from the\nassumption that LLMs, inadvertently yet inevitably, acquire psychological traits\n(metaphorically speaking) from the vast text corpora on which they are trained.\nSuch corpora contain sediments of the personalities, values, beliefs and biases\nof the countless human authors of these texts, which LLMs learn through a\ncomplex training process. The traits that LLMs acquire in such a way can po-\ntentially influence their behavior, i.e., their outputs in downstream tasks and\napplications in which they are employed, which in turn may have real-world\nconsequences for individuals and social groups. By eliciting LLMs’ responses\nto language-based psychometric inventories we can bring their traits to light.\nPsychometric profiling enables researchers to study and compare LLMs in terms\nof non-cognitive characteristics thereby providing a window into the personal-\nities, values, beliefs and biases these models exhibit (or mimic). We discuss\nthe history of similar ideas and outline possible psychometric approaches for\nLLMs. We demonstrate one promising approach, zero-shot classification, for\nseveral LLMs and psychometric inventories. We conclude by highlighting open\nchallenges and future avenues of research for AI Psychometrics.\n2\nMotivation\nIn recent years, large language models (LLMs) have been processing an ever-\nincreasing amount of human-generated data. Neural models of language such as\nGloVe (Pennington et al., 2014), BERT (Devlin et al., 2018), GPT-2 (Radford\net al., 2018), XLNet (Yang et al., 2019), RoBERTa (Y. Liu et al., 2019a), BART\n(Lewis et al., 2019) or ChatGPT and GPT-3 (OpenAI, 2020) have come to play\na transformative role in several applications of societal relevance. Various au-\nthors have referred to these models as ”foundation models” (Bommasani et al.,\n2021; Ribeiro et al., 2020), highlighting that they provide a general-purpose\nfoundation on which future computational systems will be built that can be\nfine-tuned and adapted for many different application domains and tasks. Ex-\namples of such applications include automatically processing millions of resumes\nin recruiting processes (Kulkarni & Che, 2019), detecting toxic content in so-\ncial media (Fortuna & Nunes, 2018), identifying fake news and misinformation\n(Dale, 2017) or creating chatbots for text-based human-computer interaction\n(Adiwardana et al., 2020). The increasing reliance on such Artificial Intelli-\ngence (AI) tools has also raised important concerns. One of these concerns\nis that large language models, because they were trained on human-produced\ntexts, may contain a variety of built-in biases, such as racial bias, gender bias,\nor extremist views. Such biases and views may manifest in the models’ behavior\n(e.g. the text they generate) which in turn may adversely impact individuals\nand social groups when models are used for screening applicants during recruit-\ning or admission processes, monitoring social media posts, powering chatbots\nand virtual assistants or other applications.\nBut how can the potential biases and views ingrained in large language mod-\nels, and their characteristics more generally, be detected and ideally quantified\nin a principled fashion? A common way of identifying biases or, more gener-\nally, views (e.g., values, attitudes) held by humans is to conduct psychological\nassessments (Fiske & Pearson, 1970; Watson, 1932; West & Finch, 1997). Tra-\nditionally, psychological assessments of humans have been the domain of psy-\nchometrics, a sub-discipline of psychology that concerns itself with the science\nof psychological measurement (Furr & Bacharach, 2013; Nunnally, 1994; Rust &\nGolombok, 2014). The main focus of psychometrics, at its inception, has been\nthe measurement of cognitive abilities (”intelligence”), an area that inspired the\ndevelopment of fundamental measurement theories such as classical test theory\nor item response theory and has resulted in a large number of standardized\ncognitive tests that are in wide use today. However, crucially for our present\npaper, work in psychometrics over the past decades has produced a wider array\nof well-validated tests that enable the assessment of ”non-cognitive” constructs\nsuch as personality traits, values, or attitudes. Although many different assess-\nment formats exist, most of the assessments in this domain are language-based.\nThat is, they consist of a series of items (i.e., questions or statements) that\nrespondents answer by giving a rating on a standard response scales with verbal\nand/or numeric labels. We will summarily refer to such multi-item surveys of\n3\npsychological characteristics as ”inventories” in the remainder of this paper.\nBased on these observations, we argue that psychometric inventories - simi-\nlar to the way they are used to assess humans - can be used as diagnostic tools\nthat provide a window into the ”psychological” characteristics, metaphorically\nspeaking, of large language models. Although we by no means aim to anthropo-\nmorphize artificial intelligence, we argue that large language models can exhibit\n- or, more precisely, mimic - the very same psychological characteristics that are\ntypically studied in humans. This is due to LLMs being trained on vast corpora\nof human-written text that routinely contains statements related to human val-\nues, attitudes, beliefs, and personality traits. Such models will inadvertently\nbut inevitably acquire (”learn”) a set of psychological characteristics during the\ntraining process. These learned characteristics will ultimately give a unique psy-\nchological profile to every such model that may differ from other models, not\nunlike the individual differences observed in humans. Akin to how the values,\nattitudes, and personality traits of humans become manifest in their behavior\n(broadly conceived), the psychological profiles of a large language models may\nin turn manifest in the model’s ”behavior”. In this context, speaking metaphor-\nically, behavior means the models’ outputs in the wide variety of downstream\ntasks for which they may be used. Accordingly, we submit that it should be\npossible to assess these psychological characteristics in large language models\nthrough psychometric inventories (i.e., language-based assessments) originally\ndeveloped for humans.\nIn a series of demonstrations, we provide various LLMs with questionnaire\nitems from different inventories as input and ”ask” the models to choose an\nanswer on the verbal rating scale as its output. The models’ responses open a\nlens through which to explore potential biases ingrained in large language models\nin a principled, information-rich and scalable way. This approach of studying the\ncharacteristics of large language models through psychometric inventories may\nultimately help to avoid the development of large language models that induce\nharm when deployed in broader societal applications. We conclude our article\nby arguing that our investigations give new impetus to the interdisciplinary\nfield of research that we would refer to as ’AI Psychometrics’. We propose that\nAI Psychometrics should focus on tackling the manifold research opportunities\nand challenges that emerge when deploying psychometric inventories to large\nlanguage models.\nA Very Brief History of Psychometrics and AI\nThe idea of applying psychometric assessments to AI was already discussed in\nthe first decades after AI’s foundational period in the 1950s (Bringsjord, 2011).\nPioneering work by Thomas G. Evans described a computer program that could\nsolve a subtask of geometrical analogy reasoning which was part of an intelli-\ngence test battery from the 1940s (Evans, 1964). The idea was that a program\nthat could eventually compete with humans in some part of actual tests of\nhuman intelligence could be considered intelligent too. This early attempt of\n4\nlinking AI and psychometrics fell within the rather narrow bounds of the then-\ncurrent ”Good Old-Fashioned Artificial Intelligence” (GOFAI) paradigm with\nthe goal ”to build useful computer systems, doing, or assisting with, tasks that\nhumans want done” (Boden, 2014). Similar approaches were also proposed by\nother foundational figures of AI, such as Allen Newell. Newell described the\nneed to consolidate the disparate experimental results in (cognitive) psychology\nof his times into one body of knowledge in AI to progress (Newell, 1973). Among\nthe three possible ways to achieve this that Newell outlined, the approach he\napparently preferred was psychometric: ’A . . . mold for such a task is to con-\nstruct a single program that would take a standard intelligence test, say the\nWAIS [Wechsler Adult Intelligence Scale] or the Stanford-Binet. (Newell, 1973,\np. 305)’ (Citation in (Bringsjord, 2011)).\nHowever, despite their merits, these early attempts conceived psychometrics\nmainly in terms of cognitive tests and intelligence with a focus on performance\nassessment. This was fully in line with the general focus in the field of AI on\ncognitive tasks such as planning and problem solving at the time. A popular\nearly criticism then concerned the inability of AI to operate outside this realm of\n”cold cognition”, that is, displaying ”inhumane” intelligence with no emotional\nbasis and lacking the motivational complexity of thought (Neisser, 1963). In\nresponse to such criticism, Herbert Simon showed that ”hot cognition” concepts\nsuch as emotional behaviour could be integrated in the supposedly cold models\n(Simon, 1963). Of course, fully understandable by the types of models available\nat the time, the integration of such ”hot cognition” concepts remained at a very\nbasic level of introducing emotions as ”interrupt systems” affecting program\ncontrol, changing the goals to orientate to and introducing responses.\nIn the early 2000s, ”Psychometric AI” was discussed explicitly as providing\nan answer to the old question of ”What is AI?”: ”Psychometric Al is the field\ndevoted to building information-processing entities capable of at least solid per-\nformance on all established, validated tests of intelligence and mental ability, a\nclass of tests that includes not just the rather restrictive IQ tests, but also tests\nof artistic and literary creativity, mechanical ability, and so on.” (Bringsjord &\nSchimanski, 2003, p. 889).\nThe Rise of Large Language Models\nAI has evolved dramatically in the 60 years that have passed since the first\nattempts of linking psychometrics to AI that focused on cognitive tasks. Even\nwhen compared to the early 2000s, progress has been remarkable, indicating\na qualitative shift in the capabilities of AI. To understand this change, it is\nimportant to note that the field of Natural Language Processing (NLP) has\nundergone a radical transformation around the years from 2017–18 with the\nadvent of transformer architectures as an integral part of novel large language\nmodels. Whereas it is an open discussion whether those emerging new architec-\ntures really began to ”understand” language better (Mitchell & Krakauer, 2023),\nthey nonetheless showed drastically increased performance on a wide variety of\n5\ntraditional and novel tasks, such as automated translation of text, text summa-\nrization, or detection of textual entailment. Various metrics such as precision,\nrecall or BLEU scores (Papineni et al., 2001) computed using ”benchmark” test\nsuites consisting of different language-related subtasks on canonical data sets\nprovide evidence of that progress. While such approaches can naturally only\nbe concerned with some more or less isolated aspects of natural language and\nmay therefore appear fragmented or stylized, we should refrain from downplay-\ning advances that have been made (S. Bowman, 2022) and from making unfair\ncomparisons (Firestone, 2020). In fact, recent model developments have ad-\nvanced the state of the art of natural language understanding and generation so\nsignificantly that human-like performance was reached on benchmarks such as\nGLUE (Wang et al., 2019) that include tasks judging English acceptability or\nestablishing whether pairs of questions are semantically equivalent. This led to\nthe development of supposedly much harder benchmarks such as SuperGLUE\n(Wang et al., 2020) that have been broken quickly nonetheless.\nThese recent developments highlight that large language models have reached\na stage at which they can reach human-like performance on many different tasks\nassessing language understanding and generation capabilities. Crucially, these\nenhanced capabilities of LLMs also comprise the ability to engage in ”hot cogni-\ntion” and to exhibit (or, more precisely, mimic) human-like characteristics and\nbehaviors beyond the purely cognitive realm. Although the traditional focus of\ntesting AI on cognitive tasks has also resurfaced in recent approaches to subject\nlarge language models to standard cognitive tests (e.g. Binz and Schulz, 2022),\nlarge language models are capable of much more than the ”cold cognition” re-\nquired by these cognitive tests. Simple affective mechanisms do not have to\nbe explicitly introduced into the model architectures in the way in which early\nproponents such as Herbert Simon envisioned. Instead, models trained on large\namounts of text in a self-supervised way can exhibit rich psychological traits\nthat so far have been studied only in the human realm. Potentially, such non-\ncognitive traits could become important if LLMs and solutions built upon them\nare going to be employed in contexts in which they perform tasks and make deci-\nsions that have real-world consequences for individuals and social groups. Their\ntraits, inadvertently but inevitably acquired during the models’ training with\ntext generated by humans, are likely to influence the ”behavior” (i.e., output)\nof these very models.\nStarting with ”BERTology” (Rogers et al., 2020), researchers have consid-\nered LLMs and their behavior as objects of scientific interest. Recently, for\nexample, researchers have used GPT-3 (Brown et al., 2020) to test its cogni-\ntive capabilities and other traits (Binz & Schulz, 2022; Jones & Steinhardt,\n2022; Miotto et al., 2022) and to replicate classic experiments such as the ul-\ntimatum game (Aher et al., 2023; Horton, 2023). GPT-3 has also been used\nto create in-silico samples that emulate the responses of specific demographics\nof human respondents (Argyle et al., 2023). The basic idea is to present the\nmodel with conditioning information from different sociodemographic segments.\nThe resulting synthetic samples can be extensively validated by various means\nsuch as judging how ”partisan” generated free text becomes with different so-\n6\nciodemographic specifications or by comparing expressed voting choice to known\npropensities to vote for specific parties in elections. The resulting property of\n”algorithmic fidelity” seems so robust that the authors go as far as to propose\nto do quick and cheap tests of pre-registered hypotheses of specific popula-\ntions using their approach instead of expensive pre-surveys on human samples.\nWhereas this innovative approach focuses on the level of socio-demographic sub-\nsegments of human respondents, assessment may also focus on large language\nmodels themselves, treating the models directly as respondents in psychometric\nquestionnaires. Applying such approaches on the individual model level may\ncreate opportunities of testing more hypotheses in-silico that have until now\nonly been applicable to human participants. In a further step, ethically ques-\ntionable or outright banned experiments involving humans, such as personality\nmanipulation, may become feasible in the synthetic safe space created by arti-\nficial personality. As an example, experiments showed that neurotic agents win\nmore often and quicker than other engineered personality types in, by today’s\nstandards, rather simple strategy games (Hermann et al., 2007). We expect to\nsee such research extend to ever more complex contexts as we have recently seen\nthe first uses of LLMs for agents in rich, simulated environments (Park et al.,\n2023).\nOpportunities for Psychometric Assessment\nThe unprecedented capabilities of LLMs open up an opportunity for a more\ninclusive approach to AI Psychometrics, one that spans the full spectrum of\nsocially relevant traits including non-cognitive traits such as personality, values,\nmorality and attitudes. We argue for the need of further empirical studies of\nnot only the cognitive (Binz & Schulz, 2022) but also these non-cognitive char-\nacteristics of LLMs and to establish how they relate to behavior and decisions\nof LLMs in downstream tasks, a goal that is echoed in the related research pro-\ngram of ”machine behavior” (Rahwan et al., 2019). As we demonstrate, with\nlanguage and text as the shared foundation of both psychometric inventories and\nLLMs, we can leverage existing survey instruments to learn about the hidden\nvalues, attitudes, and beliefs that are encoded in these models. Such research\nwill result in a more complete understanding of the characteristics and potential\nbiases built into these foundation models (Simon, 2019). In the present paper,\nwe aim to pave the way for such an expanded approach to AI Psychometrics.\nWe see several possible approaches to assess the psychological profiles of large\nlanguage models through psychometric inventories. These approaches differ\nmainly in how they elicit the model’s responses to the questionnaire items. We\ndescribe these approaches as masked-language prediction, next-word prediction,\nand zero-shot classification. We outline each of them in turn. Although all\napproaches are viable in principle, we explain why our focus will subsequently\nbe on zero-shot classification for our empirical demonstrations.\n7\nMasked-language prediction\nThe first possibility is to make use of masked language modeling (MLM),\nwhich is the common way of training large language models in the BERT tra-\ndition. To create large language models from scratch (i.e., to pre-train empty\nmodel architectures), researchers gather large corpora of text from the inter-\nnet (Gao et al., 2020; Ortiz Su’arez et al., 2020; Ortiz Su’arez et al., 2019).\nThey then feed those texts to the model, removing (i.e., ”masking”) one to-\nken (which roughly corresponds to a word) from a sequence of text at a time\nand asking the model to predict that token. The training objective for the\nmodel consists of learning to predict the right token over many iterations. On\neach iteration, mechanisms are operating to numerically assess how correctly\nthe model chose a possible token and to revise its weights in case it did not\nchoose the correct one from the original, unmasked sequence. This schematic\nsketch illustrates that the training procedure is so far unsupervised (also called\n”self-supervised”) because it does not involve any step in which the models re-\nceives human-annotated (supervised) training data. This step usually follows\nlater in the so-called ”finetuning” phase, in which a pretrained model is pre-\nsented with data such as for example comments from social media labelled by\nhumans for toxicity to enable the model to learn how to classify these data.\nFor psychometric assessments of LLMs, we do not necessarily need to finetune\nthese models. We could already use the unsupervised pretrained, so-called ”base\nmodels”, and present them with sequences such ”I am 〈MASK〉” and ”I am not\n〈MASK〉” to compare the probabilities that the models assign to informative\ntokens in place of 〈MASK〉. For diagnostic purposes, we could choose a list of\npolar adjective pairs (Mathew et al., 2020; Osgood, 1971; Osgood et al., 1957)\nsuch as careful—careless that map to psychometric categories of interest (e.g.,\nconscientiousness). The resulting set of probabilities (one per adjective) could\nthen be aggregated to psychological trait scores.\nNext-word prediction\nAnother way to train large language models that is used, for example, by the\nGPT architectures, is trying to predict the next word after a sequence.\nSimilar to the setup using MLM, we could present a model that was trained that\nway with the sequence ”I am” and ”I am not” and ask it to output the proba-\nbility of choosing certain trait adjectives as the next tokens. Those model types\nare called generative because they are able to create free-form text sequences.\nThis feature could also be directly used in psychometric questionnaires that\nallow for free-text answering. The scoring of free text is costly, however, as it\ninvolves more degrees of freedom and the need for evaluation frameworks (e.g.,\ndictionaries containing trait-related words) to systematically translate answers\nto numerical scores. The ability of generative models to continue to produce ar-\nbitrary sequences of text after they were provided with some initial text creates\nseveral problems. First, the initial text with which the model is presented, the\nso-called prompt has to be carefully chosen. A whole new discipline of prompt\n8\nengineering has sprung from the challenge to prompt GPT-type models, as\neven small variations in the prompts can have substantial and often hard-to-\npredict effects (P. Liu et al., 2023) on the output the model generates. Second,\neven when prompts are carefully crafted, the model can (and is expected to)\ngenerate different texts in each model run. This element of stochasticity re-\nquires collecting model outputs over many iterations and averaging over them.\nStochasticity also implies that the model may produce nonsensical or format-\nincompatible answers in some runs. Third, the model usually has to be pro-\nvided with some examples to know what output format it is expected to return.\nResearch has shown that even changing the order in the set of examples can\ninfluence model outputs (Lu et al., 2022).\nZero-shot classification\nYet another approach iszero-shot classification, which relies on pretrained\nmodels that have been finetuned on Natural Language Inference (NLI) text cor-\npora (Bowman et al., 2015; Conneau et al., 2018; Nie et al., 2020; Williams et\nal., 2018). These corpora consist of pairs of statements that are labelled accord-\ning to whether they show entailment, contradiction or neutrality between the\nfirst and the second statement. Learning logical relations of textual entailment\nbetween statements in this manner enables models to produce meaningful, often\nsurprising results in other, seemingly disparate domains to which these models\nwere never explicitly introduced. The name ”zero-shot” emphasizes the con-\ntrast with other approaches that are usually called ”few-shot” classification, in\nwhich the model is presented with a number of labelled examples for guidance.\nDifferent to few-shot classification, models finetuned on NLI corpora are able\nto classify sequences into categories chosen on-the-fly or to summarize arbitrary\ntexts (Yin et al., 2019), without the need to be presented with any labelled\nexamples before. This feature of domain agnosticism is the hallmark of NLI\napproaches.\nFor the demonstrations that follow, we apply such a zero-shot learning ap-\nproach to elicit model responses to psychometric questionnaires. Our approach\ncombines flexibility with straightforward evaluation and interpretability as an\nexample to illustrate how psychometric assessments of large language models\ncould be implemented. It allows us to prevent certain issues of generative mod-\nels, in particular, the need for prompt engineering and averaging of stochastic\noutputs from repeated model runs, although we acknowledge many other ways\nin which one could set up such demonstrations. We adapt the NLI scheme to\npresent neural models of text with questionnaire items (i.e., statements or ques-\ntions) from standard psychometric inventories and a corresponding set of verbal\nresponse options (e.g., a five-point scale indicating levels of agreement). For\neach of the response options on the response scale, we record the probability of\nentailment that the model assigns based on the item wording. We use argmax\non these probabilities to assign the most likely response as a score to each item,\nwhich we then aggregate into scales using standard scoring procedures, such as\ntaking the sum or mean across all responses pertaining to a (sub)-scale. Mod-\n9\nels trained on NLI tasks have been adapted to similar, related contexts before,\nfor example for questions that have a ’yes’ or ’no’ answer Clark et al., 2019.\nRemarkably, for yes-no question answering, an NLI approach shows better per-\nformance than a supposedly more direct transfer from multiple choice answering\ntasks. In our setup, each of the response options specified by the inventory is a\npossible answer for the models.\nDemonstrations of AI Psychometrics\nFigure 1 illustrates the zero-shot classification approach we will use in the sub-\nsequent demonstrations using a widely used personality inventory as a case in\npoint: the 44-item Big Five Inventory (BFI), available in English and German\n(John et al., 2008; Rammstedt, 1997). As (non-human) respondents, we choose\nseveral large language model architectures shown in Table 1 that follow the\nblueprint laid out by the original BERT model class (Devlin et al., 2019) such\nas RoBERTa (Y. Liu et al., 2019b) and DeBERTa (He et al., 2021). We do not\ninclude GPT-3, GPT-4 or ChatGPT in our present demonstrations because, as\ngenerative models, they are prone to the issues that we describe in the subsub-\nsection ”Next-word prediction”, such as stochastic outputs and sensitivity to\nthe order of input examples. As explained before, these issues are circumvented\nby our proposed assessment scenario using an NLI approach that we use to an-\nalyze a diverse set of open sourced models. Another reason to exclude GPT-3\n(and similar models with limited access only) is the lack of transparency arising\nfrom the fact that the model is not fully, locally accessible to us. Sanitization of\noutputs may happen behind the scenes without documentation, making it hard\nfor researchers to know which version of the model they are analyzing. In the\ninterest of open science, with our approach we are able to provide materials (see\nthe repository linked in ”Code Availability”) that allow the scientific community\nto fully replicate our analyses on the exact same models that we used without\nhaving to pay for access to potentially different models available only through\nan API or web interface. We do not analyze any of the GPT models directly,\nbut we do include the openly available BART model (Lewis et al., 2019) that\nincorporates elements of both GPT and BERT architectures.\nTo elicit responses to the questionnaire items from the models, we use the\nprocedure visualized in Figure 1: We select the most probable response from\nthe model’s distribution of scores over all responses for each item. To give a\nconcrete example: For the 44-item BFI, item numbers refer to 1: ”I am someone\nwho is talkative.”, 2: ”I am someone who tends to find fault with others.”, 3:\n”I am someone who does a thorough job.”, 4: ”I am someone who is depressed,\nblue.”, etc. in English. They translate as 1: ”Ich bin gespr¨ achig, unterhalte\nmich gern.”, 2: ”Ich neige dazu, andere zu kritisieren.”, 3: ”Ich erledige Auf-\ngaben gr¨ undlich.”, 4: ”Ich bin deprimiert, niedergeschlagen.”, etc. in German.\nThe response options follow a classic, fully labeled Likert-type scale in which\neach response category is associated with a specific numerical score. The nu-\nmerical scores and verbal labels are the following: 1 = ”Disagree strongly”, 2 =\n10\nTest items \nwho is outgoing, sociable.\nI see myself as someone\nwho has an active imagination.\nI see myself as someone\nConscientousness\nExtraversion\nAgreeableness\nOpenness\nNeuroticism who gets nervous easily.\nI see myself as someone\nwho does a thorough job.\nI see myself as someone\nwho is generally trusting.\nI see myself as someone\nAggregation \ninto scales\nO C E A N\nPossible responses\nDisagree strongly, Disagree a little, \nNeither agree nor disagree, Agree \na little, Agree strongly\nExample\nI see myself as\nsomeone who does\na thorough job.\nP(disagree strongly) = 0.13\nP(disagree a little) = 0.15\nP(neither agree nor disagree) = 0.32\nP(agree strongly) = 0.12\nP(agree a little) = 0.28\nPsychometric Assessment\nopenness conscientiousness\nextraversion\nneuroticism\nagreeableness\nDistilRoBERTa\nmultilingualDeBERTa\nXLMRoBERTa1 2 3 4 5 1 2 3 4 5 1 2 3 4 5\n0.0\n0.2\n0.4\n0.6\nResponse\nModel Score\nItem #\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nFigure 1: Illustration: How psychometric assessments could be\nadapted to large language models. Taking items and responses from the\nBig Five Inventory (BFI) as examples, we show the steps of one possible assess-\nment scheme. We present the model, one-by-one with each of the survey items\nand the possible responses. We retrieve the model’s distribution of probabil-\nity scores over responses (Panel ”Example”). Scores are aggregated into scales\nthat can be visualized and used for further analyses. This figure illustrates the\nworkflow of one possible way to psychometrically assess large language models.\n11\n”Disagree a little”, 3 = ”Neither agree nor disagree”, 4 = ”Agree a little”, 5 =\n”Agree strongly” in English as well as the equivalent in German. With that, we\nfollow fully the published survey specifications for items, candidate responses\nand scoring (John et al., 2008; Rammstedt, 1997).\nWe want to note here that our approach differs from tests of model perfor-\nmance such as GLUE or SuperGLUE. Such benchmark tests with many sub-\ntasks could be viewed as being related to psychological assessments. In the\ncontext of Natural Language Understanding, benchmark tests may, for exam-\nple, include Winograd schemes (Winograd, 1972) and other modules testing\nthe model’s understanding of specific aspects of language. In contrast, we apply\npsychometric inventories as diagnostic tools that characterize properties of mod-\nels other than their performance. The distinction between model benchmarks\n(measuring performance) of models and psychometric inventories (eliciting re-\nsponses to questionnaire items) is thus reminiscent of the distinction between\nskills/abilities (e.g., fluid intelligence) and traits (e.g., personality, values) in\nhuman psychometrics: In contrast to benchmark tests, when we let models\nrespond to psychometric inventories, there is no ground truth (i.e., ”correct”\nresponses to questionnaire items) that a model should display.\nAlthough the goal is thus not to compare the performance of the models\nas in benchmark tests, the psychometric inventories still allow for a number\nof meaningful insights into the characteristics of these models. First, we can\ncompare responses of each model to the questionnaire (e.g., the BFI), and its\nresulting scores on traits such as agreeableness to the distribution of scores\n(i.e., ”norms”) from psychometric assessments in human samples using the same\ninventories. This allows for relative comparisons of model scores to human\naverages or typical profiles. For example, a model may be characterized as\nrelatively high in agreeableness if it scores high in that trait relative to typical\nhuman populations. Second, independent of absolute scores, the inventories\nallow us to compare different large language models relative to each other, which\nin itself is informative about potential differences in the psychological traits\nthese models may exhibit. For example, one model may turn out to score\nmuch lower than another in agreeableness. In the future, we might also create\nreference populations consisting entirely of different models, thereby potentially\nbuilding a “population of LLMs” over time that has a certain distribution of\ntraits (according to highly standardized psychometric inventories) against which\nfuture models can be compared.\nIn the following, we want to highlight and exemplify the application of var-\nious standard questionnaires to large language models. Towards that goal, we\nrecorded the language models’ responses to several psychometric inventories to\ncomprehensively assess the psychological profile of each of these models. These\ninventories measure constructs from different psychological domains that, while\nnot fully independent, each capture unique aspects of a person’s – or, in our\ncase, a language model’s – psychological profile. Specifically, we chose to assess\nthe global Big Five personality traits, specific ”dark” personality traits, value\norientations, morality, and gender/sex diversity beliefs. All of these constructs\nare routinely assessed in research on humans. Moreover, for each construct,\n12\ninventories that use a fully labeled verbal response scale exist, which was a\nrequirement for our approach. Collectively, these inventories allow us to ob-\ntain in-depth psychological profiles of each language model and thereby give us\na glimpse into potentially controversial, biased, or harmful characteristics and\nviews that might be ingrained in (some of) these models. They also enable us\nto compare the psychological profiles of different language models to each other.\nFor each construct, we chose well-validated inventories that are widely used\nin research on human. Notably, we do not claim that these are necessarily the\nbest, let alone the only, inventories that would be suitable for the task at hand.\nWe chose these inventories purely for illustrative purposes. In principle, any\ninventory that works similarly to the ones we chose could be used for psychome-\ntric assessment of AI. Wherever possible, we use both the English and German\nversion of the same questionnaires to gauge the extent to which results gener-\nalize across languages. In the next subsections, we aim to shed light on some\nof the opportunities that come with deploying psychometric inventories to large\nlanguage models, using a set of concrete models that are listed in Table 1.\nAssessing Personality\nWhat kind of ”personalities” do the AI models have? Do they exhibit a socially\ndesirable profile – or do they possess characteristics that are commonly viewed\nas undesirable or even problematic? To approach this question, we first assess\nglobal personality in terms of the Big Five personality dimensions by use of the\nBig Five Inventory (John et al., 2008). The BFI assesses the Big Five dimensions\n(openness, conscientiousness, extraversion, agreeableness, and neuroticism) with\n44 items that are each to be rated on a five 5-point agree–disagree scale ranging\nfrom ”disagree strongly” to ”agree strongly”.\nAdditionally, we assessed undesirable and offensive (though not necessarily\npathological and relatively widespread in human populations) personality traits\ndelineated by the dark tetrad. The dark tetrad consists of the traits machiavel-\nlianism (i.e., manipulative interpersonal behaviors), narcissism (i.e., excessive\nself-love), and psychopathy (i.e., lack of empathy), and sadism (i.e., intrinsic\npleasure in hurting others). We employ the Short Dark Tetrad (SD4) ques-\ntionnaire (Paulhus et al., 2021), which assesses these four traits with 28 items\n(7 per trait) that are answered on a 5-point agree–disagree scale ranging from\n”disagree strongly” to ”agree strongly”.\nResults for the Big Five are displayed in Figure 2 for the English (Panel\nA) and for the German language models (Panel B). The emerging personality\nprofiles for the six English language models are surprisingly homogeneous. All\nmodels score more or less equally high on agreeableness and extraversion and\nlow on neuroticism. Slight differences are observable for conscientiousness and\nespecially openness. While the second model (DistilRoBERTa) scores higher on\nopenness, the last model (DistilBART) reports comparatively low scores on con-\nscientiousness. Within the three models using the German version of the BFI44\n(including an additional item for agreeableness) (Rammstedt, 1997), we see\nmore pronounced differences among the models: While XLMRoBERTa’s and\n13\nopenness\nneuroticism\nagreeableness extraversion\nconscientiousness\nmultilingualDeBERTa\nopenness\nneuroticism\nagreeableness extraversion\nconscientiousness\nDistilRoBERTa\nopenness\nneuroticism\nagreeableness extraversion\nconscientiousness\nBART\nopenness\nneuroticism\nagreeableness extraversion\nconscientiousness\nXLMRoBERTa\nopenness\nneuroticism\nagreeableness extraversion\nconscientiousness\nDeBERTa\nopenness\nneuroticism\nagreeableness extraversion\nconscientiousness\nDistilBART\nA\nopenness\nneuroticism\nagreeableness extraversion\nconscientiousness\nXLMRoBERTa (de)\nopenness\nneuroticism\nagreeableness extraversion\nconscientiousness\nmultilingualDeBERTa (de)\nopenness\nneuroticism\nagreeableness extraversion\nconscientiousness\nGBERT (de)\nB\nFigure 2: Assessing personality via the 44 item version of the BFI\nfor different models. Panel A shows the English and Panel B the translated\nGerman version version of the questionnaire. Using multilingual models enables\nus to study cross-lingual (and potentially cross-cultural) differences of model\nscores and survey items. Model results generally show no surprising outliers\nand overall balanced personality profiles.\n14\nGBERT’s profiles are high on openness, extraversion, and conscientiousness,\nDeBERTa scores high only on conscientiousness. Interesting is the comparison\nwithin models across languages. Whereas the English version of XLMRoBERTa\nscores higher on agreeableness compared to its German counterpart, for mul-\ntilingualDeBERTa the English version scores also higher on agreeableness but\nalso for openness and extraversion than the German version. Such a comparison\n- especially if also conducted on the level of single items - can also be useful from\na methodological point of view: Systematic differences across language versions\ncould be seen as a first indication for biases caused by the translation of items\nor by systematic differences in the model training data between languages.\nOverall, the Big Five profiles appear characteristic of a relatively balanced\nand well-adapted personality (low neuroticism; high conscientiousness, agree-\nableness, and extraversion). They yield little indication that any of the models\npossess an extreme, accentuated personality. A more direct assessment of a\npotential ”dark side” of these models’ personalities, however, is offered by the\nmodels’ scores on the dark tetrad. These results are shown in Figure 3. Again,\nthe results do not suggest unexpected personality profiles. Most models score\nlow (between 2 and 3 on the 5-point-scale) on all four dark traits. Only a few\nexceptions stand out, such as the high narcissism scores of DistilRoBERTa and\nBART. Overall, the models we study here generally do not score highly on so-\ncially undesirable, potentially problematic traits. Contrariwise, they score well\nwithin the range observed in normal human populations, where the dark traits\nare roughly normally distributed around the scale’s midpoint (Paulhus et al.,\n2021).\nAssessing Value Orientations\nNext to personality traits, value orientations are another central aspect of a\nperson’s psychological makeup. Values are beliefs about desirable end states\nor modes of conduct that vary in importance, transcend specific situations and\nguide the selection or evaluation of behavior, people, and events (Schwartz,\n1992). Whereas Big Five traits describe dispositional behavioral tendencies (i.e.,\nhow the person typically acts), value orientations describe dispositional evalua-\ntive tendencies (i.e., what the person cherishes or finds important in life). The\nmost prominent and best-validated (including cross-culturally) model of human\nvalues is Schwartz’s (Schwartz, 1992) theory of basic human values. This theory\ndistinguishes 10 basic human values (or, in its refined version, 19; (Schwartz et\nal., 2012)) that emerge with great regularity in samples of human respondents\nfrom across the globe. We used the recent 57-item Revised Portrait Values\nQuestionnaire Schwartz and Cieciuch (2022) to assess these basic values. This\ninventory relies on a portrait format in which each item consists of a statement\ndescribing a person in terms of their values (e.g., ”Thinking up new ideas and\nbeing creative is important to her.”). Respondents indicate how similar they\nare to the person described in the statement on a 6-point scale ranging from 1\nnot like me at all to 6 very much like me .\nUsing this inventory opened a window into the values espoused by the six\n15\nnarcissism\nmachiavellianism\npsychopathy\nsadism\nmultilingualDeBERTa\nnarcissism\nmachiavellianism\npsychopathy\nsadism\nDistilRoBERTa\nnarcissism\nmachiavellianism\npsychopathy\nsadism\nBART\nnarcissism\nmachiavellianism\npsychopathy\nsadism\nXLMRoBERTa\nnarcissism\nmachiavellianism\npsychopathy\nsadism\nDeBERTa\nnarcissism\nmachiavellianism\npsychopathy\nsadism\nDistilBART\nFigure 3: Assessing personality via Dark Tetrad. Unusually accentuated\nscores on this assessment may reveal potential for conflicts with no-harm objec-\ntives in model decisions. Our assessment shows that the models lie well within\nthe range of values observed in standard human populations, not displaying\nworrying pathological features.\n16\nSelf−Direction\nStimulation\nHedonism\nAchievement\nPower\nSecurity\nTradition\nConformity\nBenevolence\nUniversalism\nmultilingualDeBERTa\nSelf−Direction\nStimulation\nHedonism\nAchievement\nPower\nSecurity\nTradition\nConformity\nBenevolence\nUniversalism\nDistilRoBERTa\nSelf−Direction\nStimulation\nHedonism\nAchievement\nPower\nSecurity\nTradition\nConformity\nBenevolence\nUniversalism\nBART\nSelf−Direction\nStimulation\nHedonism\nAchievement\nPower\nSecurity\nTradition\nConformity\nBenevolence\nUniversalism\nXLMRoBERTa\nSelf−Direction\nStimulation\nHedonism\nAchievement\nPower\nSecurity\nTradition\nConformity\nBenevolence\nUniversalism\nDeBERTa\nSelf−Direction\nStimulation\nHedonism\nAchievement\nPower\nSecurity\nTradition\nConformity\nBenevolence\nUniversalism\nDistilBART\nf\nm\nFigure 4: Assessing value orientation via the revised Portrait Value\nQuestionnaire (PVQ-RR). Radarcharts show results for the questionnaire\nversion with male (pastel blue) and female pronouns (reddish orange) with oth-\nerwise identical items. Purple grey areas correspond to agreement between the\ntwo gendered versions. The slight differences visible (areas in either one of the\ntwo colors) point to the existence of gender biases of the models.\nEnglish language models. Because the PVQ-RR comes in a male version (con-\ntaining the pronouns he/him) and a female version (containing the pronouns\nshe/her), it also enables us to establish differences in the models’ scoring across\nthe two gender versions. Differences in what response the models think is en-\ntailed by each item depending on what gender pronouns the questionnaire uses\ncan be taken to indicate gender bias in these models.\nFigure 4 shows results for the 10 values. From the visual pattern, one can\nimmediately see that most models score low on most dimensions, meaning that\nthey assigned higher probabilities to the lower ends of the response scale. For\nmultilingualDeBERTa and to some extent BART, we observe little differen-\ntiation between the 10 values. The other models show more differentiation,\nattaching a lower importance to some values and higher importance to others.\nFor example, DistilROBERTa scores relatively low in all values except hedonism\nand stimulation as well as, to some extent benevolence. DeBERTa scores high\nin achievement.\nThere are indeed some indications for built-in gender bias for some of the\nmodels, although the score differences for the two gender versions of PVQ-RR\nmostly appear small. The largest difference we observe is the ”male” achieve-\nment score of DeBERTa, which is noticeably higher than the ”female” score on\nthe same value.\n17\nharm−care\nfairness−reciprocity\nin−group−loyalty authority−respect\npurity−sanctity\nmultilingualDeBERTa\nharm−care\nfairness−reciprocity\nin−group−loyalty authority−respect\npurity−sanctity\nXLMRoBERTa\nharm−care\nfairness−reciprocity\nin−group−loyalty authority−respect\npurity−sanctity\nDistilRoBERTa\nharm−care\nfairness−reciprocity\nin−group−loyalty authority−respect\npurity−sanctity\nDeBERTa\nharm−care\nfairness−reciprocity\nin−group−loyalty authority−respect\npurity−sanctity\nBART\nharm−care\nfairness−reciprocity\nin−group−loyalty authority−respect\npurity−sanctity\nDistilBART\nFigure 5: Assessing moral norms via the Moral Foundations Question-\nnaire. Models tend to deviate from the average politically moderate American’s\nscores (red) as reported by the developers of the questionnaire (Graham et al.,\n2011). The models (yellow) usually deviate in the direction of putting more\nemphasis on those moral foundations that are associated with conservative po-\nlitical orientations.\nAssessing Moral Norms\nCan large language models also reflect moral beliefs and norms that they ab-\nsorbed from text during training? Various studies have already explored this\nquestion. For example, early work on this subject computed distances of vec-\ntor representations between statements and moral concepts on an atomic level\n(Jentzsch et al., 2019). The proliferation of more capable language models\nhas enabled more sophisticated ways of exploring this issue, such as directly\nasking the models moral questions (”Should I kill people?”, ”Is it allowed to\nmurder people?” with simple answer templates of ”Yes/no, I should (not).”)\n(Schramowski et al., 2022; Schramowski et al., 2019). Compared to our ap-\nproach that uses established psychometric inventories, this more ad-hoc ap-\nproach is somewhat less standardized and systematic.\nIn our demonstration, we illustrate the direct application of the established\nMoral Foundations Questionnaire (Graham et al., 2013; Graham et al., 2009;\nHaidt, 2007) to various models. Figure 5 contrasts reported moral beliefs from\naverage politically moderate Americans (Graham et al., 2011) with model scores.\nAcross different models, we can observe that models put stronger emphasis on\nmoral norms such as authority-respect, in-group-loyalty or purity-sanctity than\nour human reference group did. Interestingly, the moral norms that are stressed\nmore by the models are usually associated with individuals holding conservative\npolitical views. This suggests that there might be significant differences across\n18\nAffirmation\nGender Normativity\nUniformity Surgery\nUpbringing\nmultilingualDeBERTa\nAffirmation\nGender Normativity\nUniformity Surgery\nUpbringing\nDistilRoBERTa\nAffirmation\nGender Normativity\nUniformity Surgery\nUpbringing\nBART\nAffirmation\nGender Normativity\nUniformity Surgery\nUpbringing\nXLMRoBERTa\nAffirmation\nGender Normativity\nUniformity Surgery\nUpbringing\nDeBERTa\nAffirmation\nGender Normativity\nUniformity Surgery\nUpbringing\nDistilBART\nFigure 6: Assessing beliefs about gender via the Gender/Sex Diversity\nBeliefs Scale (GSDB). The models display uniform views of people of the\nsame gender/sex and little affirmation of diversity. This points to potential\nissues of the models to take non-traditional aspects of gender/sex adequately\ninto account.\nvarious dimensions between the moral beliefs held by people vs. the moral be-\nliefs absorbed by language models from large corpora. Corroborating or refuting\nsuch initial observations in future endeavors should be an important and crit-\nical concern for researchers aiming to design responsible artificial intelligence\nsystems.\nAssessing Beliefs About Gender\nPrevious research found gender bias in algorithmically curated online environ-\nments (Vlasceanu & Amodio, 2022) and specifically also in large language mod-\nels (Caliskan et al., 2017), which indicates that there is a need for monitoring\nsuch encoded gender/sex diversity beliefs. Typically, researchers assume the\ngender binary framework which suggests that humans comprise only two types\nof beings, men and women. This framework has been challenged by both aca-\ndemic research and social activism (Hyde et al., 2018). To address this issue,\nsocial scientists have developed novel instruments to measure beliefs about the\nontology of gender/sex. Assessing those beliefs is important since prejudice\nagainst or affirmation of gender/sex minorities (i.e., transgender, nonbinary,\nand gender/sex diverse) is often framed in terms of beliefs about the ontology\n19\nof gender/sex or gender/sex diversity.\nWe use the recently developed Gender/Sex Diversity Beliefs Scale (GSDB)\n(Schudson & van Anders, 2022) to measure the biases and prejudices against\nsex/gender minorities that are encoded in large language models. The scale\nconsists of five factors. Items that recognize the existence of gender/sex di-\nversity loaded positively on affirmation. Those associated with denying gen-\nder/sex diversity loaded negatively. Gender normativity is composed of items\nabout the importance of femininity for women and masculinity for men, and\nthe inauthenticity of non-normative gender expressions (e.g., femininity among\nmen). Uniformity contains items that stress that people of the same gender/sex\nare similar to each other. Items that describe genital surgery as a necessary\nprecondition for a person to “truly” transition gender/sexes load on surgery.\nUpbringing collects items about the role of upbringing and early experiences in\ndetermining gender/sex.\nAll factors except upbringing are associated with feelings toward gender/sex\nminorities that were either negative (gender normativity, uniformity, surgery)\nor positive (affirmation). Our results (Figure 6) show that all language models\nhave two things in common: (i) an emphasis on uniformity (i.e., people of the\nsame sex/gender are similar) and (ii) lack of affirmation - i.e. they do not reveal\nstrong positive feelings towards gender/sex minorities (e.g., language models\ntend to disagree with statements such as ”There are many different gender\nidentities people can have.” or ”Non-binary gender identities are valid.”).\nOpen Challenges and Conclusions\nOur demonstrations highlight the feasibility of using psychometric inventories as\na window through which to study the characteristics of large language models,\nas well as to identify and monitor differences between various models. At the\nsame time, our approach is only one of many different ways in which research in\nAI Psychometrics can be pursued. Without any claim of completeness, we now\nwant to discuss a number of future research challenges that we deem important.\nReliability and validity of psychometric assessments of AI: We see\na wide field of open methodological and ethical questions and challenges related\nto psychometric assessments of large language models, e.g., a continued effort\nto probe the validity and reliability of reusing human psychometric assessments\nin the domain of AI. As an example, current models have been claimed to\ndisplay results in Theory of Mind (TOM) tasks that are comparable to the\nperformance of elementary school children (Kosinski, 2023). Problems have\nquickly been reported with those findings, as small variations that keep TOM\nprinciples intact make the results disappear (Ullman, 2023). Another important,\ngeneral question concerns self-consistency of large language models. So far, from\nour demonstrations we can only provide partial answers: We do not provide the\nmodel with explicit information about the ranked order of possible responses.\nHowever, it seems that the models establish this property implicitly on their\nown, judging from the example distributions over items shown in the lower\n20\npart of Figure 1 (the data is available for all models under consideration in our\nreplication materials, see ”Code Availability”). This emerging feature can be\nbe seen as first evidence that the models do not display an obvious lack of self-\nconsistency: They are at least consistent in the sense that there is no obvious\nbimodality observed in the probability distributions, as for example high values\nfor both extremes of the scales, “totally agree” and “totally disagree” as equally\nlikely, would indicate. We want to note here that this issue should definitely be\nfurther investigated in future research. Adversarial testing that highlighted such\ninconsistencies in other domains (Camburu et al., 2020) could help us to make\nour approach more robust. From a more high-level perspective, we could also\nuse different, but related questionnaires and compare if model responses follow\na similar pattern, for example for ”Genderism and Transphobia Scale” (GTS)\n(Hill & Willoughby, 2005) and the ”Gender Role Attitudes Scale” (GRAS)\n(Garc´ ıa-Cueto et al., 2015).\nStability of psychometric profiles: Future research can tackle many\ninteresting and creative research questions, such as: Does text scraped from\nspecific parts of the internet lead to specific characteristics of models trained\non that text (e.g., from special communities on Reddit or 4chan)? Do models\ntrained on books or movie plots preferred by certain personality types develop\nsimilar traits? Does the accidental filtering of text from and about sexual mi-\nnority groups in large pre-training corpora influence the diversity conceptions of\nlarge language models (Dodge et al., 2021)? Research projects that tackle these\nquestions produce insights on an important general question: To what extent\ndo models absorb (psychometric) aspects of their underlying training data? To\nsystematically study the role of data in isolation, we can compare model series\nfeaturing the exact same architectures and the exact same training parameters\nwhile only differing in regularly updated training corpora (Loureiro et al., 2022).\nEngineering psychometric properties of AI: Assuming that psycho-\nmetric assessments of large language models can be done in a valid and robust\nmanner, it may be feasible to deliberately manipulate the personality traits,\nvalues, and attitudes that large language models exhibit. Doing so may help\nto change the behavior of models in a desired fashion (e.g., reducing bias or\ndark traits). It may also open up new opportunities for research: Conducting\nin-silico experiments may create a safe experimental space for exploring novel\npsychological research questions that could not have been addressed previously\ndue to ethical and other concerns. We saw that using language models to sim-\nulate subpopulations with certain demographics seems possible (Argyle et al.,\n2023). Such a synthetic sampling approach can also be used to study phenom-\nena on the level of individuals as in psychometrics. We can potentially apply\nthose quick and cheap methods on the full range from pre-tests to full and deep\ninvestigation of hypotheses.\nMultimodal psychometric assessments of AI: While we focussed here\non psychometric assessments of large language models through language-based\ninventores, similar ideas are applicable to other modalities. In the visual domain,\nwe can imagine a Rohrschach-like test scenario of image creation, to investigate,\nfor example, which colors map to which feelings and other abstract concepts for\n21\nthese models, thereby potentially revealing cultural norms. Sound and video\ngeneration may offer additional glimpses into inner workings of models. In the\nfuture, we may be able to easily combine those ways to assess psychometric\nproperties of more general, multimodal artificial intelligence systems.\nLife-long monitoring of psychometric properties of AI: While we\nwant to refrain from attributing human-like capabilities and traits to AI tech-\nnologies and to talk about AI in anthropomorphic terms, we want to highlight\nthe need to further develop monitoring tools and test suites to support the life-\nlong monitoring of AI tools and to shed light on their imperfections, biases and\nharmful consequences. For that endeavour, having unrestricted access to local,\n”frozen” versions of models to do full inspections of them is a necessary con-\ndition. Researchers have noticed examples of apparent ”sanitization” or other\ncorrections of model text outputs happening behind the scenes to models avail-\nable only through APIs or web interfaces (Rozado, 2023). In the interest of\ntransparent documentation, it is important to know which exact version of the\nmodel we are analyzing. This issue can be expected to play an even bigger role\nin the future, especially so with the proliferation of approaches that let models\ndirectly adapt to humans, like for example reinforcement learning from human\nfeedback (RLHF) (Ziegler et al., 2020).\nConsequences of psychometric profiles of AI:One current limitation of\nour demonstrations concerns the question of practical relevance: More empirical\nresearch is needed to establish how precisely certain traits of AI models influence\ntheir behavior in downstream tasks. These future research endeavours could\nanalyse models that are able to use external tools such as APIs and search\nengines (Schick et al., 2023). Psychological profiles could potentially be an\nexplanatory factor of a specific choice between those tools that a model has\naccess to. The choice of querying unreliable sources of information for example\ncould depend on the model displaying risk-averse traits or not, especially in\nnuanced contexts where it is not easy or even possible to give a right or wrong\nanswer. Also in robotics, researchers make use of LLMs (Huang et al., 2022; Li\net al., 2022). Here in particular, behavior often involves planning and directly\nacting in the real world, which makes monitoring potentially harmful model\nfeatures especially important. Known examples of undesired model behaviors\nhave so far been uncovered using rather simple pipelines that rely on mask-\nfilling or text generation using curated templates (Gehman et al., 2020; Liang\net al., 2022; Nangia et al., 2020). The proposed text choices by the models\nare usually judged by criteria such as them being hurtful or stereotypical (Abid\net al., 2021; Nozza et al., 2021, 2022). The problems arising from the ad-\nhocness of many of such tests have been pointed out (Blodgett et al., 2021).\nAs a future complementary perspective, we can enrich those approaches more\nand more by integrating knowledge from the social sciences in general and from\npsychometrics specifically. As we have shown, we can adapt existing methods\nfrom those domains in a rather straightforward way.\nThe ongoing (and continuing) trend of language models to underpin ever\nlarger parts of technology will likely make them play a more and more important\nrole in our future daily lives. In our view, the research community should clearly\n22\nmake use of the opportunity to describe various psychological aspects of models\nvia rich psychometric profiles. This offers an exciting and valuable avenue for\nfuture research to adapt well-established methods from human psychometrics\nand study the relationship of such assessments to all kinds of other phenomena\n(like decision making or other important behavior of AI). In the future, we may\nbe able to uncover more and more relationships and to offer robust assessments\nof the real-world consequences of psychometric traits of AI.\nConclusions\nWe demonstrated how standard psychometric inventories that were developed to\nassess ”non-cognitive” psychological characteristics such as personality, values,\nmorality, or beliefs in humans can be repurposed as diagnostic tools to assess\nanalogous characteristics in large language models. Similar to how human re-\nspondents fill in a questionnaire, large language models respond to questionnaire\nitems by returning a probability of entailment for each verbally labelled response\noption through zero-shot classification. These responses are then aggregated to\nscale scores using standard scoring rules of the inventories to obtain the levels\nof the model on each given trait (e.g., low agreeableness).\nIn doing so, we built on a rich history of research linking psychometrics\nand AI, which has mainly focused on cognitive assessments. By contrast, the\ninventories employed in our demonstration capture ”non-cognitive” character-\nistics that the large language models inadvertently but inevitably acquire from\nthe vast text corpora on which these models were trained. Sedimented in these\ntexts are the beliefs, values, personalities, and biases of the innumerable and\ndiverse human authors who produced these texts. The way in which the models\nacquire such traits from texts is complex, opaque, and poorly understood so far;\nyet it is clear that this learning process is channeled and constrained by their\nneural architecture and subject to various deliberate and non-deliberate human\ninterventions that may influence these traits (e.g., the selection and curation\nof the text corpus, purification steps, potential fine-tuning on annotated text,\netc.). There are some obvious parallels to how humans acquire psychological\ntraits through their ongoing interactions with the social and physical world,\nchanneled and constrained by their nervous system and subject to deliberate\nand non-deliberate human interventions (e.g., education and discipline).\nCapturing such traits through psychometric inventories allows creating psy-\nchological profiles of these models. The resulting profiles can be used in several\nways that provide a lens through which to study the characteristics of these\nmodels. Trait scores obtained through psychometric profiling allow researchers\nto meaningfully interpret the absolute trait levels in terms of the verbal re-\nsponse scale of the inventories; to compare these models directly to each other\nin relative terms; or to compare the trait scores to those observed human pop-\nulations. In the future, we may create ”silicon populations” consisting entirely\nof many different varieties of large language models. The latter would be sim-\nilar to how performance benchmarks for these models such as SuperGLUE are\nalready used, but focusing on individual differences in non-cognitive traits such\n23\nas values, beliefs, and morality rather than performance.\nThe models’ psychological profiles uncovered by their responses to psycho-\nmetric inventories, in turn, could be assumed to also influence their ”behavior”\n(i.e., the output of the models) in a variety of downstream tasks and applications\nin which these large models will be used (e.g., text classification, chatbots) - just\nlike human traits assessed through the same inventories are routinely taken to\npredict behavior, emotion, and cognition in situations outside the test-taking\nsituation itself. Psychometric AI thus offers a conduit towards better under-\nstanding the inner workings, and predicting the behavior of these models in\nAI applications that have a potentially adverse impact (as well as benefit) for\nhuman individuals and social groups.\nThe analogy to human psychometrics is thus quite far-reaching and intrigu-\ning. At the same time, it is important not to overstretch the analogy and to be\nmindful that the above description is mostly metaphorical. One must not fall\ninto the trap of anthropomorphizing AI models that are mere prediction ma-\nchines. Different from humans, the traits that large language models exhibit are\npurely based on language and thus far more narrow than the rich mental world\nof humans that is linked to their complex physiology and embedded in multi-\nlayered physical contexts, just as the range of behaviors that these models can\nperform is quite limited. At the same time, the traits and attendant behaviors\nof large language models can still be quite consequential for actual individuals\nand social group if the models are deployed in real-world applications such as\nthe ones described at the outset.\nIt is equally important to realize that several of the above assumptions are so\nfar untested. As we highlighted, there are many open questions - both concep-\ntual and technical in nature - that have yet to be resolved. Still, we believe that\nour demonstrations clearly highlight the novel potentials of the interdisciplinary\nfield of research on the intersection of disciplines such as psychology, linguistics\nand computer science, that we may refer to as AI Psychometrics . This area\noffers a wide variety of research questions and several directions to explore that\nwe consider important not only for future research but also because of the far-\nreaching social and economic implications of artificial intelligence that are only\ngoing to become more pronounced in the coming years.\nAcknowledgements\nAuthor contributions (CRediT):\n• MP: Conceptualization, Methodology, Formal Analysis, Data Curation,\nVisualization, Writing - Original Draft; Writing - Review and Editing\n• CL: Conceptualization, Methodology, Writing - Original Draft, Writing -\nReview and Editing\n• CW, BR: Conceptualization, Writing - Review and Editing\n24\n• MS: Original Conceptualization, Methodology, Writing - Review and Edit-\ning\nWe thank Mirta Galesic and David Garcia for their contribution to the initial\nwork on the idea of AI Psychometrics. Figure 1 is an adaption and extension\nof an original figure by David Garcia. Alina Herderich provided valuable input\nat an early stage of the manuscript.\nCode Availability\nWe provide a repository that contains materials to fully replicate our analyses\nat https://github.com/maxpel/psyai materials.\nReferences\nAbid, A., Farooqi, M., & Zou, J. (2021). Large language models associate Mus-\nlims with violence. Nature Machine Intelligence , 3(6), 461–463. https:\n//doi.org/10.1038/s42256-021-00359-2\nAdiwardana, D., Luong, M.-T., So, D. R., Hall, J., Fiedel, N., Thoppilan, R.,\nYang, Z., Kulshreshtha, A., Nemade, G., Lu, Y., & Le, Q. V. (2020).\nTowards a Human-like Open-Domain Chatbot [arXiv:2001.09977 [cs,\nstat]]. Retrieved February 17, 2023, from http://arxiv.org/abs/2001.\n09977\nComment: 38 pages, 12 figures\nAher, G., Arriaga, R. I., & Kalai, A. T. (2023). Using Large Language Models\nto Simulate Multiple Humans and Replicate Human Subject Studies\n[arXiv:2208.10264 [cs]]. Retrieved February 23, 2023, from http://arxiv.\norg/abs/2208.10264\nComment: Added Turing Experiment (TE) framing and Wisdom of\nCrowds TE\nArgyle, L. P., Busby, E. C., Fulda, N., Gubler, J. R., Rytting, C., & Wingate, D.\n(2023). Out of One, Many: Using Language Models to Simulate Human\nSamples. Political Analysis, 1–15. https://doi.org/10.1017/pan.2023.2\nBinz, M., & Schulz, E. (2022). Using cognitive psychology to understand GPT-3\n[arXiv:2206.14576 [cs]]. Retrieved November 4, 2022, from http://arxiv.\norg/abs/2206.14576\nBlodgett, S. L., Lopez, G., Olteanu, A., Sim, R., & Wallach, H. (2021). Stereo-\ntyping Norwegian Salmon: An Inventory of Pitfalls in Fairness Bench-\nmark Datasets. Proceedings of the 59th Annual Meeting of the Associ-\nation for Computational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1: Long Papers) ,\n1004–1015. https://doi.org/10.18653/v1/2021.acl-long.81\n25\nBoden, M. A. (2014). GOFAI. In K. Frankish & W. M. Ramsey (Eds.), The\nCambridge Handbook of Artificial Intelligence (First, pp. 89–107). Cam-\nbridge University Press. https://doi.org/10.1017/CBO9781139046855.\n007\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S.,\nBernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E.,\nBuch, S., Card, D., Castellon, R., Chatterji, N., Chen, A., Creel, K.,\nDavis, J. Q., Demszky, D., . . . Liang, P. (2021). On the Opportunities\nand Risks of Foundation Models. https://doi.org/10.48550/ARXIV.\n2108.07258\nBowman, R., S., Angeli, G., Potts, C., & Manning, C. D. (2015). A large anno-\ntated corpus for learning natural language inference. Proceedings of the\n2015 Conference on Empirical Methods in Natural Language Processing\n(EMNLP).\nBowman, S. (2022). The Dangers of Underclaiming: Reasons for Caution When\nReporting How NLP Systems Fail.Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics (Volume 1: Long\nPapers), 7484–7499. https://doi.org/10.18653/v1/2022.acl-long.516\nBringsjord, S. (2011). Psychometric artificial intelligence. Journal of Experi-\nmental & Theoretical Artificial Intelligence , 23(3), 271–277. https://\ndoi.org/10.1080/0952813X.2010.502314\nBringsjord, S., & Schimanski, B. (2003). What is artificial intelligence? Psycho-\nmetric AI as an answer [Number of pages: 7 Place: Acapulco, Mexico].\nProceedings of the 18th international joint conference on artificial in-\ntelligence, 887–893.\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P.,\nNeelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-\nVoss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler,\nD. M., Wu, J., Winter, C., . . . Amodei, D. (2020). Language Models\nare Few-Shot Learners. arXiv:2005.14165 [cs].\nCaliskan, A., Bryson, J. J., & Narayanan, A. (2017). Semantics derived au-\ntomatically from language corpora contain human-like biases. Science,\n356(6334), 183–186.\nCamburu, O.-M., Shillingford, B., Minervini, P., Lukasiewicz, T., & Blunsom,\nP. (2020). Make Up Your Mind! Adversarial Generation of Inconsis-\ntent Natural Language Explanations. Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics , 4157–4165.\nhttps://doi.org/10.18653/v1/2020.acl-main.382\nChan, B., Schweter, S., & M¨ oller, T. (2020). German’s Next Language Model.\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T., Collins, M., & Toutanova,\nK. (2019). BoolQ: Exploring the Surprising Difficulty of Natural Yes/No\nQuestions.\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm´ an,\nF., Grave, E., Ott, M., Zettlemoyer, L., & Stoyanov, V. (2020). Unsu-\npervised Cross-lingual Representation Learning at Scale. https://doi.\norg/10.48550/arXiv.1911.02116\n26\nConneau, A., Lample, G., Rinott, R., Williams, A., Bowman, S. R., Schwenk,\nH., & Stoyanov, V. (2018). XNLI: Evaluating Cross-lingual Sentence\nRepresentations. https://doi.org/10.48550/arXiv.1809.05053\nDale, R. (2017). Nlp in a post-truth world.Natural Language Engineering, 23(2),\n319–324.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2018). Bert: Pre-training\nof deep bidirectional transformers for language understanding. arXiv\npreprint arXiv:1810.04805.\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K. (2019). BERT: Pre-training\nof Deep Bidirectional Transformers for Language Understanding. https:\n//doi.org/10.48550/arXiv.1810.04805\nDodge, J., Sap, M., Marasovi´ c, A., Agnew, W., Ilharco, G., Groeneveld, D.,\nMitchell, M., & Gardner, M. (2021). Documenting Large Webtext Cor-\npora: A Case Study on the Colossal Clean Crawled Corpus. Proceedings\nof the 2021 Conference on Empirical Methods in Natural Language Pro-\ncessing, 1286–1305. https://doi.org/10.18653/v1/2021.emnlp-main.98\nEvans, T. G. (1964). A heuristic program to solve geometric-analogy problems.\nProceedings of the April 21-23, 1964, Spring Joint Computer Confer-\nence on XX - AFIPS ’64 (Spring) , 327. https://doi.org/10.1145/\n1464122.1464156\nFirestone, C. (2020). Performance vs. competence in human–machine compar-\nisons. Proceedings of the National Academy of Sciences, 117(43), 26562–\n26571. https://doi.org/10.1073/pnas.1905334117\nFiske, D. W., & Pearson, P. H. (1970). Theory and techniques of personality\nmeasurement. Annual review of psychology , 21(1), 49–86.\nFortuna, P., & Nunes, S. (2018). A survey on automatic detection of hate speech\nin text. ACM Computing Surveys (CSUR) , 51(4), 1–30.\nFurr, R., & Bacharach, V. (2013). Psychometrics: An introduction.\nGao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J.,\nHe, H., Thite, A., Nabeshima, N., Presser, S., & Leahy, C. (2020). The\nPile: An 800GB dataset of diverse text for language modeling. arXiv\npreprint arXiv:2101.00027.\nGarc´ ıa-Cueto, E., Rodr´ ıguez-D´ ıaz, F. J., Bringas-Molleda, C., L´ opez-Cepero, J.,\nPa´ ıno-Quesada, S., & Rodr´ ıguez-Franco, L. (2015). Development of the\nGender Role Attitudes Scale (GRAS) amongst young Spanish people.\nInternational Journal of Clinical and Health Psychology , 15(1), 61–68.\nhttps://doi.org/10.1016/j.ijchp.2014.10.004\nGehman, S., Gururangan, S., Sap, M., Choi, Y., & Smith, N. A. (2020). RealTox-\nicityPrompts: Evaluating neural toxic degeneration in language models.\nFindings of the association for computational linguistics: EMNLP 2020,\n3356–3369. https://doi.org/10.18653/v1/2020.findings-emnlp.301\nGraham, J., Haidt, J., Koleva, S., Motyl, M., Iyer, R., Wojcik, S. P., & Ditto,\nP. H. (2013). Moral Foundations Theory. In Advances in Experimental\nSocial Psychology (pp. 55–130). Elsevier. https://doi.org/10.1016/\nB978-0-12-407236-7.00002-4\n27\nGraham, J., Haidt, J., & Nosek, B. A. (2009). Liberals and conservatives rely on\ndifferent sets of moral foundations. Journal of Personality and Social\nPsychology, 96(5), 1029–1046. https://doi.org/10.1037/a0015141\nGraham, J., Nosek, B. A., Haidt, J., Iyer, R., Koleva, S., & Ditto, P. H. (2011).\nMapping the moral domain. Journal of personality and social psychol-\nogy, 101(2), 366.\nHaidt, J. (2007). The New Synthesis in Moral Psychology. Science, 316(5827),\n998–1002. https://doi.org/10.1126/science.1137651\nHe, P., Liu, X., Gao, J., & Chen, W. (2021). DeBERTa: Decoding-enhanced\nBERT with Disentangled Attention. https://doi.org/10.48550/arXiv.\n2006.03654\nHermann, C., Melcher, H., Rank, S., & Trappl, R. (2007). Neuroticism – A\nCompetitive Advantage (Also) for IVAs? [ISSN: 0302-9743, 1611-3349\nSeries Title: Lecture Notes in Computer Science]. In C. Pelachaud, J.-C.\nMartin, E. Andr´ e, G. Chollet, K. Karpouzis, & D. Pel´ e (Eds.),Intel-\nligent Virtual Agents (pp. 64–71). Springer Berlin Heidelberg. https:\n//doi.org/10.1007/978-3-540-74997-4 7\nHill, D. B., & Willoughby, B. L. B. (2005). The Development and Validation\nof the Genderism and Transphobia Scale. Sex Roles, 53(7-8), 531–544.\nhttps://doi.org/10.1007/s11199-005-7140-x\nHorton, J. J. (2023). Large Language Models as Simulated Economic Agents:\nWhat Can We Learn from Homo Silicus? [arXiv:2301.07543 [econ, q-\nfin]]. Retrieved February 23, 2023, from http://arxiv.org/abs/2301.\n07543\nHuang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P., Zeng, A., Tomp-\nson, J., Mordatch, I., Chebotar, Y., Sermanet, P., Brown, N., Jackson,\nT., Luu, L., Levine, S., Hausman, K., & Ichter, B. (2022). Inner Mono-\nlogue: Embodied Reasoning through Planning with Language Models\n[arXiv:2207.05608 [cs]]. Retrieved August 23, 2023, from http://arxiv.\norg/abs/2207.05608\nComment: Project website: https://innermonologue.github.io\nHyde, J., Bigler, R., Joel, D., Tate, C., & Anders, S. (2018). The future of\nsex and gender in psychology: Five challenges to the gender binary.\nAmerican Psychologist, 74. https://doi.org/10.1037/amp0000307\nJentzsch, S., Schramowski, P., Rothkopf, C., & Kersting, K. (2019). Semantics\nDerived Automatically from Language Corpora Contain Human-like\nMoral Choices. Proceedings of the 2019 AAAI/ACM Conference on AI,\nEthics, and Society , 37–44. https://doi.org/10.1145/3306618.3314267\nJohn, O. P., Naumann, L. P., & Soto, C. J. (2008). Paradigm shift to the in-\ntegrative Big Five trait taxonomy: History, measurement, and concep-\ntual issues. In Handbook of personality: Theory and research, 3rd ed\n(pp. 114–158). The Guilford Press.\nJones, E., & Steinhardt, J. (2022). Capturing Failures of Large Language Models\nvia Human Cognitive Biases [arXiv:2202.12299 [cs]]. Retrieved February\n23, 2023, from http://arxiv.org/abs/2202.12299\nComment: Published at NeurIPS 2022\n28\nKosinski, M. (2023). Theory of Mind May Have Spontaneously Emerged in\nLarge Language Models [arXiv:2302.02083 [cs]]. Retrieved February 22,\n2023, from http://arxiv.org/abs/2302.02083\nKulkarni, S. B., & Che, X. (2019). Intelligent software tools for recruiting.Jour-\nnal of International Technology and Information Management , 28(2),\n2–16.\nLaurer, M., van Atteveldt, W., Casas, A., & Welbers, K. (2022). Less Annotat-\ning, More Classifying – Addressing the Data Scarcity Issue of Supervised\nMachine Learning with Deep Transfer Learning and BERT-NLI.\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O.,\nStoyanov, V., & Zettlemoyer, L. (2019). BART: Denoising Sequence-to-\nSequence Pre-training for Natural Language Generation, Translation,\nand Comprehension. https://doi.org/10.48550/arXiv.1910.13461\nLi, S., Puig, X., Paxton, C., Du, Y., Wang, C., Fan, L., Chen, T., Huang, D.-A.,\nAky¨ urek, E., Anandkumar, A., Andreas, J., Mordatch, I., Torralba,\nA., & Zhu, Y. (2022). Pre-Trained Language Models for Interactive\nDecision-Making [arXiv:2202.01771 [cs]]. Retrieved August 23, 2023,\nfrom http://arxiv.org/abs/2202.01771\nLiang, P., Bommasani, R., Lee, T., Tsipras, D., Soylu, D., Yasunaga, M., Zhang,\nY., Narayanan, D., Wu, Y., Kumar, A., Newman, B., Yuan, B., Yan,\nB., Zhang, C., Cosgrove, C., Manning, C. D., R´ e, C., Acosta-Navas,\nD., Hudson, D. A., . . . Koreeda, Y. (2022). Holistic Evaluation of Lan-\nguage Models [arXiv:2211.09110 [cs]]. Retrieved December 28, 2022,\nfrom http://arxiv.org/abs/2211.09110\nComment: Authored by the Center for Research on Foundation Mod-\nels (CRFM) at the Stanford Institute for Human-Centered Artificial\nIntelligence (HAI). Project page: https://crfm.stanford.edu/helm/v1.0\nLiu, P., Yuan, W., Fu, J., Jiang, Z., Hayashi, H., & Neubig, G. (2023). Pre-train,\nPrompt, and Predict: A Systematic Survey of Prompting Methods in\nNatural Language Processing. ACM Computing Surveys , 55(9), 1–35.\nhttps://doi.org/10.1145/3560815\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., & Stoyanov, V. (2019a). RoBERTa: A Robustly Opti-\nmized BERT Pretraining Approach [arXiv: 1907.11692].arXiv:1907.11692\n[cs]. Retrieved November 20, 2020, from http://arxiv.org/abs/1907.\n11692\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., & Stoyanov, V. (2019b). RoBERTa: A Robustly Opti-\nmized BERT Pretraining Approach. arXiv:1907.11692 [cs].\nLoureiro, D., Barbieri, F., Neves, L., Anke, L. E., & Camacho-Collados, J.\n(2022). TimeLMs: Diachronic Language Models from Twitter. https:\n//doi.org/10.48550/arXiv.2202.03829\nLu, Y., Bartolo, M., Moore, A., Riedel, S., & Stenetorp, P. (2022). Fantasti-\ncally Ordered Prompts and Where to Find Them: Overcoming Few-\nShot Prompt Order Sensitivity. Proceedings of the 60th Annual Meeting\n29\nof the Association for Computational Linguistics (Volume 1: Long Pa-\npers), 8086–8098. https://doi.org/10.18653/v1/2022.acl-long.556\nMathew, B., Sikdar, S., Lemmerich, F., & Strohmaier, M. (2020). The POLAR\nFramework: Polar Opposites Enable Interpretability of Pre-Trained Word\nEmbeddings. Proceedings of The Web Conference 2020, 1548–1558. https:\n//doi.org/10.1145/3366423.3380227\nMiotto, M., Rossberg, N., & Kleinberg, B. (2022). Who is GPT-3? An Explo-\nration of Personality, Values and Demographics [arXiv:2209.14338 [cs]].\nRetrieved November 4, 2022, from http://arxiv.org/abs/2209.14338\nComment: EMNLP 2022 NLP+CSS workshop\nMitchell, M., & Krakauer, D. C. (2023). The Debate Over Understanding in AI’s\nLarge Language Models [arXiv:2210.13966 [cs]]. Retrieved February 17,\n2023, from http://arxiv.org/abs/2210.13966\nComment: Under submission as a Perspective article. Updated with\nadditional discussion and citations\nNangia, N., Vania, C., Bhalerao, R., & Bowman, S. R. (2020). CrowS-Pairs:\nA challenge dataset for measuring social biases in masked language\nmodels. Proceedings of the 2020 conference on empirical methods in\nnatural language processing (EMNLP), 1953–1967. https://doi.org/10.\n18653/v1/2020.emnlp-main.154\nNeisser, U. (1963). The Imitation of Man by Machine. Science, 139(3551), 193–\n197. https://doi.org/10.1126/science.139.3551.193\nNewell, A. (1973). YOU CAN’T PLAY 20 QUESTIONS WITH NATURE AND\nWIN: PROJECTIVE COMMENTS ON THE PAPERS OF THIS SYM-\nPOSIUM. In Visual Information Processing (pp. 283–308). Elsevier.\nhttps://doi.org/10.1016/B978-0-12-170150-5.50012-3\nNie, Y., Williams, A., Dinan, E., Bansal, M., Weston, J., & Kiela, D. (2020). Ad-\nversarial NLI: A New Benchmark for Natural Language Understanding.\nhttps://doi.org/10.48550/arXiv.1910.14599\nNozza, D., Bianchi, F., & Hovy, D. (2021). HONEST: Measuring Hurtful Sen-\ntence Completion in Language Models. Proceedings of the 2021 Con-\nference of the North American Chapter of the Association for Compu-\ntational Linguistics: Human Language Technologies , 2398–2406. https:\n//doi.org/10.18653/v1/2021.naacl-main.191\nNozza, D., Bianchi, F., & Hovy, D. (2022). Pipelines for Social Bias Testing\nof Large Language Models. Proceedings of BigScience Episode #5 –\nWorkshop on Challenges & Perspectives in Creating Large Language\nModels, 68–74. https://doi.org/10.18653/v1/2022.bigscience-1.6\nNunnally, J. C. (1994). Psychometric theory 3e . Tata McGraw-hill education.\nOpenAI. (2020). API [[Online; accessed September 30th 2020]].\nOrtiz Su’arez, P. J., Romary, L., & Sagot, B. (2020). A monolingual approach to\ncontextualized word embeddings for mid-resource languages. Proceed-\nings of the 58th Annual Meeting of the Association for Computational\nLinguistics, 1703–1714.\nOrtiz Su’arez, P. J., Sagot, B., & Romary, L. (2019). Asynchronous pipelines\nfor processing huge corpora on medium to low resource infrastructures.\n30\nIn P. Ba´ nski, A. Barbaresi, H. Biber, E. Breiteneder, S. Clematide, M.\nKupietz, H. L”ungen, & C. Iliadi (Eds.). Leibniz-Institut f”ur Deutsche\nSprache. https://doi.org/10.14618/ids-pub-9021\nOsgood, C. E. (1971). Exploration in Semantic Space: A Personal Diary.Journal\nof Social Issues, 27(4), 5–64. https://doi.org/10.1111/j.1540-4560.1971.\ntb00678.x\nOsgood, C. E., Suci, G. J., & Tannenbaum, P. H. (1957). The Measurement Of\nMeaning. University of Illinois Press.\nPapineni, K., Roukos, S., Ward, T., & Zhu, W.-J. (2001). BLEU: A method for\nautomatic evaluation of machine translation. Proceedings of the 40th\nAnnual Meeting on Association for Computational Linguistics - ACL\n’02, 311. https://doi.org/10.3115/1073083.1073135\nPark, J. S., O’Brien, J. C., Cai, C. J., Morris, M. R., Liang, P., & Bernstein, M. S.\n(2023). Generative Agents: Interactive Simulacra of Human Behavior\n[arXiv:2304.03442 [cs]]. Retrieved August 24, 2023, from http://arxiv.\norg/abs/2304.03442\nPaulhus, D. L., Buckels, E. E., Trapnell, P. D., & Jones, D. N. (2021). Screening\nfor Dark Personalities: The Short Dark Tetrad (SD4).European Journal\nof Psychological Assessment, 37(3), 208–222. https://doi.org/10.1027/\n1015-5759/a000602\nPennington, J., Socher, R., & Manning, C. D. (2014). Glove: Global vectors for\nword representation. Proceedings of the 2014 conference on empirical\nmethods in natural language processing (EMNLP) , 1532–1543.\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). Improving\nlanguage understanding by generative pre-training.\nRahwan, I., Cebrian, M., Obradovich, N., Bongard, J., Bonnefon, J.-F., Breazeal,\nC., Crandall, J. W., Christakis, N. A., Couzin, I. D., Jackson, M. O.,\nJennings, N. R., Kamar, E., Kloumann, I. M., Larochelle, H., Lazer, D.,\nMcElreath, R., Mislove, A., Parkes, D. C., Pentland, A. S., . . . Well-\nman, M. (2019). Machine behaviour.Nature, 568(7753), 477–486. https:\n//doi.org/10.1038/s41586-019-1138-y\nRammstedt, B. (1997). Die deutsche Version des Big Five Inventory (BFI):\n¨Ubersetzung und Validierung eines Fragebogens zur Erfassung des F¨ unf-\nFaktoren-Modells der Pers¨ onlichkeit.\nReimers, N., & Gurevych, I. (2020). Making monolingual sentence embeddings\nmultilingual using knowledge distillation. Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language Processing .\nRibeiro, M. T., Wu, T., Guestrin, C., & Singh, S. (2020). Beyond accuracy:\nBehavioral testing of NLP models with CheckList. Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics ,\n4902–4912. https://doi.org/10.18653/v1/2020.acl-main.442\nRogers, A., Kovaleva, O., & Rumshisky, A. (2020). A Primer in BERTology:\nWhat we know about how BERT works [arXiv:2002.12327 [cs]]. Re-\ntrieved January 30, 2023, from http://arxiv.org/abs/2002.12327\nComment: Accepted to TACL. Please note that the multilingual BERT\nsection is only available in version 1\n31\nRozado, D. (2023). The Political Bias of ChatGPT – Extended Analysis. Re-\ntrieved February 22, 2023, from https://davidrozado.substack.com/p/\npolitical-bias-chatgpt\nRust, J., & Golombok, S. (2014). Modern psychometrics: The science of psy-\nchological assessment. Routledge.\nSchick, T., Dwivedi-Yu, J., Dess` ı, R., Raileanu, R., Lomeli, M., Zettlemoyer,\nL., Cancedda, N., & Scialom, T. (2023). Toolformer: Language Models\nCan Teach Themselves to Use Tools [arXiv:2302.04761 [cs]]. Retrieved\nAugust 23, 2023, from http://arxiv.org/abs/2302.04761\nSchramowski, P., Turan, C., Andersen, N., Rothkopf, C. A., & Kersting, K.\n(2022). Large pre-trained language models contain human-like biases\nof what is right and wrong to do. Nature Machine Intelligence , 4(3),\n258–268.\nSchramowski, P., Turan, C., Jentzsch, S., Rothkopf, C., & Kersting, K. (2019).\nBERT has a Moral Compass: Improvements of ethical and moral values\nof machines. https://doi.org/10.48550/arXiv.1912.05238\nSchudson, Z. C., & van Anders, S. M. (2022). Gender/sex diversity beliefs:\nScale construction, validation, and links to prejudice. Group Processes\n& Intergroup Relations , 25(4), 1011–1036. https://doi.org/10.1177/\n1368430220987595\nSchwartz, S. H. (1992). Universals in the content and structure of values: Theo-\nretical advances and empirical tests in 20 countries (M. P. Zanna, Ed.).\n25, 1–65. https://doi.org/10.1016/S0065-2601(08)60281-6\nSchwartz, S. H., & Cieciuch, J. (2022). Measuring the refined theory of individ-\nual values in 49 cultural groups: Psychometrics of the Revised Portrait\nValue Questionnaire. Assessment, 29(5), 1005–1019. https://doi.org/\n10.1177/1073191121998760\nSchwartz, S. H., Cieciuch, J., Vecchione, M., Davidov, E., Fischer, R., Beierlein,\nC., Ramos, A., Verkasalo, M., L¨ onnqvist, J.-E., Demirutku, K., Dirilen-\nGumus, O., & Konty, M. (2012). Refining the theory of basic individual\nvalues. Journal of Personality and Social Psychology , 103(4), 663–688.\nhttps://doi.org/10.1037/a0029393\nShleifer, S., & Rush, A. M. (2020). Pre-trained Summarization Distillation.\nhttps://doi.org/10.48550/arXiv.2010.13002\nSimon, H. A. (1963). A theory of emotional behaviour (tech. rep. No. 55).\nSimon, H. A. (2019). The Sciences of the Artificial . The MIT Press. https :\n//doi.org/10.7551/mitpress/12107.001.0001\nUllman, T. (2023). Large Language Models Fail on Trivial Alterations to Theory-\nof-Mind Tasks [arXiv:2302.08399 [cs]]. Retrieved February 22, 2023,\nfrom http://arxiv.org/abs/2302.08399\nComment: 11 pages, 2 figures\nVlasceanu, M., & Amodio, D. M. (2022). Propagation of societal gender inequal-\nity by internet search algorithms. Proceedings of the National Academy\nof Sciences , 119(29), e2204529119. https : / / doi . org / 10 . 1073 / pnas .\n2204529119\n32\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy,\nO., & Bowman, S. R. (2020). SuperGLUE: A Stickier Benchmark for\nGeneral-Purpose Language Understanding Systems.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. R. (2019).\nGLUE: A Multi-Task Benchmark and Analysis Platform for Natural\nLanguage Understanding.\nWatson, G. (1932). Measures of character and personality. Psychological Bul-\nletin, 29(2), 147.\nWest, S. G., & Finch, J. F. (1997). Personality measurement: Reliability and\nvalidity issues. In Handbook of personality psychology (pp. 143–164).\nElsevier.\nWilliams, A., Nangia, N., & Bowman, S. (2018). A broad-coverage challenge\ncorpus for sentence understanding through inference. Proceedings of the\n2018 Conference of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies, Volume 1\n(Long Papers), 1112–1122.\nWinograd, T. (1972). Understanding natural language. Cognitive Psychology,\n3(1), 1–191. https://doi.org/10.1016/0010-0285(72)90002-3\nYang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R. R., & Le, Q. V.\n(2019). Xlnet: Generalized autoregressive pretraining for language un-\nderstanding. Advances in neural information processing systems, 5753–\n5763.\nYin, W., Hay, J., & Roth, D. (2019). Benchmarking Zero-shot Text Classifica-\ntion: Datasets, Evaluation and Entailment Approach.Proceedings of the\n2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), 3912–3921. https://doi.org/10.18653/v1/\nD19-1404\nZiegler, D. M., Stiennon, N., Wu, J., Brown, T. B., Radford, A., Amodei, D.,\nChristiano, P., & Irving, G. (2020). Fine-Tuning Language Models from\nHuman Preferences [arXiv:1909.08593 [cs, stat]]. Retrieved February 22,\n2023, from http://arxiv.org/abs/1909.08593\n33\nAppendix\nTable 1: Different models included in our demonstrations. Technical\nname refers to the full model names to be found in the Hugging Face model hub\nat https://huggingface.co/models. Short name is the abbreviations we use in\nthe visualizations. ”R.” describes the citations of model architectures that are\nbriefly summarised under ”Architecture”. We used models either in English,\nGerman or both (”L.”). All of these models have been finetuned on one or more\nof those NLI corpora: ”The Stanford Natural Language Inference (SNLI) Cor-\npus” (Bowman et al., 2015), ”The Multi-Genre NLI Corpus” (MNLI) (Williams\net al., 2018), ”XNLI: The Cross-Lingual NLI Corpus” (Conneau et al., 2018).\nTechnical Name Short Name Architecture R. L.\njoeddav/\nxlm-roberta-\nlarge-xnli\nXLMRoBERTa\nA more robustly\npretrained BERT model\n(RoBERTa) trained\nusing a cross-lingual\ntraining objective\nConneau et al., 2020 en,\nde\ncross-encoder/\nnli-distilroberta-\nbase\nDistilRoBERTa\nCreated using\n”knowledge distillation”\nto emulate the output\nof the larger model\n(RoBERTa) to create\na smaller version\nReimers and Gurevych, 2020en\nmicrosoft/\ndeberta-base-mnliDeBERTa\nAn extension of BERT\nthat introduces\nnew techniques in\nthe model architecture\nHe et al., 2021 en\nMoritzLaurer/\nmDeBERTa-v3-\nbase-mnli-xnli\nmultilingual\nDeBERTa\nFinetuning DeBERTa\non a multilingual\nNLI corpus instead\nof English only\nLaurer et al., 2022 en,\nde\nSahajtomar/\nGermanZeroshotGBERT\nBERT pretrained\non a corpus\nof German text\nfinetuned on the\nGerman part of XNLI\nChan et al., 2020 de\nfacebook/\nbart-large-mnliBART\nGeneralizating\nBERT and GPT\ntraining techniques\nLewis et al., 2019 en\nvalhalla/\ndistilbart-mnli-12-1DistilBART\nSimilar procedure as\nabove for DistilRoBERTa\nbut for BART\nShleifer and Rush, 2020en\n34",
  "topic": "Psychometrics",
  "concepts": [
    {
      "name": "Psychometrics",
      "score": 0.5814258456230164
    },
    {
      "name": "Personality psychology",
      "score": 0.5777217745780945
    },
    {
      "name": "Cognition",
      "score": 0.5531798005104065
    },
    {
      "name": "Psychology",
      "score": 0.5410429835319519
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3989400565624237
    },
    {
      "name": "Social psychology",
      "score": 0.39153796434402466
    },
    {
      "name": "Developmental psychology",
      "score": 0.28644129633903503
    },
    {
      "name": "Personality",
      "score": 0.20588478446006775
    },
    {
      "name": "Psychiatry",
      "score": 0.10347932577133179
    }
  ]
}