{
    "title": "Double-head transformer neural network for molecular property prediction",
    "url": "https://openalex.org/W4321614295",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4322107630",
            "name": "Yuanbing Song",
            "affiliations": [
                "University of Shanghai for Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2139894305",
            "name": "Jinghua Chen",
            "affiliations": [
                "University of Shanghai for Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2107185361",
            "name": "Wen-Ju Wang",
            "affiliations": [
                "University of Shanghai for Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2100962162",
            "name": "Gang Chen",
            "affiliations": [
                "University of Shanghai for Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3177906998",
            "name": "Zhichong Ma",
            "affiliations": [
                "University of Shanghai for Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A4322107630",
            "name": "Yuanbing Song",
            "affiliations": [
                "University of Shanghai for Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2139894305",
            "name": "Jinghua Chen",
            "affiliations": [
                "University of Shanghai for Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2107185361",
            "name": "Wen-Ju Wang",
            "affiliations": [
                "University of Shanghai for Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2100962162",
            "name": "Gang Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3177906998",
            "name": "Zhichong Ma",
            "affiliations": [
                "University of Shanghai for Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2594183968",
        "https://openalex.org/W3198212015",
        "https://openalex.org/W3013625695",
        "https://openalex.org/W3198599144",
        "https://openalex.org/W2970852745",
        "https://openalex.org/W2503169225",
        "https://openalex.org/W2075613930",
        "https://openalex.org/W1553697943",
        "https://openalex.org/W2085074871",
        "https://openalex.org/W2153564408",
        "https://openalex.org/W3038823641",
        "https://openalex.org/W3113447514",
        "https://openalex.org/W2895420596",
        "https://openalex.org/W3054891022",
        "https://openalex.org/W2753962198",
        "https://openalex.org/W2122825543",
        "https://openalex.org/W2290847742",
        "https://openalex.org/W3024033076",
        "https://openalex.org/W2966357564",
        "https://openalex.org/W2769775068",
        "https://openalex.org/W2065354989",
        "https://openalex.org/W1481666845",
        "https://openalex.org/W2025580327",
        "https://openalex.org/W3129577962",
        "https://openalex.org/W4226072475",
        "https://openalex.org/W3163493952",
        "https://openalex.org/W4289173507",
        "https://openalex.org/W4223955854",
        "https://openalex.org/W2972987306",
        "https://openalex.org/W2028501442",
        "https://openalex.org/W2046589863",
        "https://openalex.org/W2527189750",
        "https://openalex.org/W3000478925",
        "https://openalex.org/W3007488165",
        "https://openalex.org/W2969457089",
        "https://openalex.org/W3119774682",
        "https://openalex.org/W2901003004",
        "https://openalex.org/W4288275971",
        "https://openalex.org/W3012117454",
        "https://openalex.org/W3005552578",
        "https://openalex.org/W2907079035",
        "https://openalex.org/W3034231628",
        "https://openalex.org/W3034902920",
        "https://openalex.org/W3018495986",
        "https://openalex.org/W3129168016",
        "https://openalex.org/W2096541451",
        "https://openalex.org/W2092285329",
        "https://openalex.org/W1993046136",
        "https://openalex.org/W2114162221",
        "https://openalex.org/W2096560421",
        "https://openalex.org/W2461470610",
        "https://openalex.org/W2276859037",
        "https://openalex.org/W2145578524",
        "https://openalex.org/W2565684601",
        "https://openalex.org/W1976526581",
        "https://openalex.org/W1975147762",
        "https://openalex.org/W4394665502",
        "https://openalex.org/W3100157108",
        "https://openalex.org/W3166272013",
        "https://openalex.org/W4310709716",
        "https://openalex.org/W4210313485"
    ],
    "abstract": "Abstract Existing molecular property prediction methods based on deep learning ignore the generalization ability of the nonlinear representation of molecular features and the reasonable assignment of weights of molecular features, making it difficult to further improve the accuracy of molecular property prediction. To solve the above problems, an end-to-end double-head transformer neural network (DHTNN) is proposed in this paper for high-precision molecular property prediction. For the data distribution characteristics of the molecular dataset, DHTNN specially designs a new activation function, beaf, which can greatly improve the generalization ability of the nonlinear representation of molecular features. A residual network is introduced in the molecular encoding part to solve the gradient explosion problem and ensure that the model can converge quickly. The transformer based on double-head attention is used to extract molecular intrinsic detail features, and the weights are reasonably assigned for predicting molecular properties with high accuracy. Our model, which was tested on the MoleculeNet [1] benchmark dataset, showed significant performance improvements over other state-of-the-art methods.",
    "full_text": "Song et al. Journal of Cheminformatics           (2023) 15:27  \nhttps://doi.org/10.1186/s13321-023-00700-4\nRESEARCH\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nOpen Access\nJournal of Cheminformatics\nDouble-head transformer neural network \nfor molecular property prediction\nYuanbing Song, Jinghua Chen, Wenju Wang*, Gang Chen and Zhichong Ma \nAbstract \nExisting molecular property prediction methods based on deep learning ignore the generalization ability of the \nnonlinear representation of molecular features and the reasonable assignment of weights of molecular features, \nmaking it difficult to further improve the accuracy of molecular property prediction. To solve the above problems, an \nend-to-end double-head transformer neural network (DHTNN) is proposed in this paper for high-precision molecular \nproperty prediction. For the data distribution characteristics of the molecular dataset, DHTNN specially designs a \nnew activation function, beaf, which can greatly improve the generalization ability of the nonlinear representation of \nmolecular features. A residual network is introduced in the molecular encoding part to solve the gradient explosion \nproblem and ensure that the model can converge quickly. The transformer based on double-head attention is used to \nextract molecular intrinsic detail features, and the weights are reasonably assigned for predicting molecular properties \nwith high accuracy. Our model, which was tested on the MoleculeNet [1] benchmark dataset, showed significant \nperformance improvements over other state-of-the-art methods.\nKeywords Molecular property prediction, Transformer, Deep learning, Residual network\nIntroduction\nMolecular property prediction refers to the effective \nidentification of molecular properties such as \nlipophilicity, binding affinity, biological activity, and \ntoxicity [2]. For fields such as drug design [3], materials \nscience [4], and genetic engineering [5], accurate and \nreliable prediction of molecular properties can accelerate \nthe development process and reduce the development \ncost. Therefore, molecular property prediction has \nsignificant research meaning and application value, and is \na popular research at present.\nThe quantitative structure-activity/property \nrelationship (QSAR/QSPR) has always been a hot \ntopic in materials chemistry [6]. This method uses \nmathematical and statistical methods to study the \nquantitative relationship between the chemical structure \nof a compound and its physicochemical properties in \norder to build predictive models [7, 8]. The chemical \ndescriptors used in the QSAR/QSPR model must be able \nto quantitatively represent the structural parameters \nof the molecule [9]. Therefore, the prediction accuracy \nof the model is strongly influenced by the chemical \ndescriptors. A large amount of research is needed to \ncalculate the structural parameters of molecules based \non physicochemical experiments [10], and there may be \nlarge errors.\nWith the rise of artificial intelligence, combining \nartificial intelligence with the field of molecular property \nprediction has become a major research trend for \nimproving the accuracy of molecular property prediction \n[11–14]. Current research on the prediction of molecular \nproperty by artificial intelligence is mainly divided into \ntwo categories: machine learning methods and deep \nlearning methods.\n*Correspondence:\nWenju Wang\nwangwenju@usst.edu.cn\nCollege of Communication and Art Design, University of Shanghai \nfor Science and Technology, Shanghai, China\nPage 2 of 16Song et al. Journal of Cheminformatics           (2023) 15:27 \nMachine learning methods\nCommonly used prediction models are ridge regression, \nrandom forest(RF), elastic network, support vector \nmachine(SVM), gradient boosting and extreme gradient \nboosting (XGBoost). Ridge regression [15] is a regressor \nthat has a kernel with a regularization term, and the \nmodel uses the sum of the weighted kernel functions of \nthe molecules to be predicted and all the molecules in the \ntraining set for prediction. RF [16] incorporates random \nattribute selection in the training process and integrates \nthe results of multiple decision tree models as predictions \nusing the bagging integration method. The model is easy \nto implement, and the computational cost is small. When \nthe chemical descriptor is Morgan fingerprints [17, 18], \nrunning Random Forest on Morgan fingerprints [17, \n18] can predict molecular property, such as the model \nRF on Morgan [19]. The elastic network [20] is a linear \nmodel that differs from ridge regression by penalizing the \nmixed regularization term (L1) and the regularization \nterm (L2), with an additional hyperparameter controlling \nL1 and L2. SVM [21–23] is a class of generalized linear \nclassifiers that perform binary classification of molecular \ndata by supervised learning. The decision boundary is \nthe maximum margin hyperplane for learning samples. \nIt can transform the molecular property prediction \nproblem into solving convex quadratic programming \nproblem. The use of kernel function avoids the dimension \ndisaster, but the selection of kernel function has a great \nimpact on the performance of SVM. Gradient boosting \n[24, 25] trains the new-joined weak classifier based on \nthe negative gradient information of the loss function \nfrom the current molecular property prediction model. \nIn each iteration, a weak classifier will be obtained. \nThese weak classifiers are accumulated to get the final \nmodel. However, this form has the disadvantages of \nbad parallelization, slow computational speed, and high \ncomputational complexity. Given the shortcomings of \ngradient boosting, XGBoost [26, 27] was proposed by \nimproving the loss function and regularization. XGBoost \n[28, 29] is an integrated tree model containing multiple \nclassification and regression trees (CART); it adds \ntogether the corresponding prediction values of each tree \nto obtain the final prediction value. XGBoost sorts the \ndata before training and saves it as a block structure to \nachieve parallel computation. CART and linear classifiers \ncan also be supported as base classifiers to speed up \ntraining. The method uses the idea of RF to support \nrow down-sampling and column down-sampling. The \nfirst- and second-order derivatives are also used in the \ncustom loss function calculations, and regular terms are \nadded. These methods can reduce the error of the model \nto prevent the overfitting phenomenon and reduce the \ncomputational complexity, which can facilitate faster and \nmore accurate gradient descent. In addition, XGBoost \ncan multiply the weights of leaf nodes by the learning rate \nafter one iteration to weaken the influence of each tree \nand expand the learning range.\nOverall, machine learning methods require domain \nexperts to extract features manually, but their \nhandcrafted molecular descriptors are easily limited by \nthe subjective experience and knowledge of the experts.\nDeep learning methods\nUnlike machine learning methods, deep learning \nenables features to be extracted automatically, so deep \nlearning methods are particularly suitable for molecular \nproperty prediction. The feed-forward neural network \n(FFN) is one of the simplest artificial neural network \n[30]. The neurons in the former layer are only \nconnected with those in the latter layer. FFN reads \nchemical descriptors to extract molecular features so as \nto perform prediction of molecular properties, such as \nthe models FFN on Morgan [19], FFN on Morgan \nCounts [19], and FFN on RDKit [19]. Later, a large \nnumber of neural networks emerged, for example, the \ndirected acyclic graph model [31], deep tensor neural \nnetwork [32] and message passing neural network \n(MPNN) [33], which can be used to predict molecular \nproperties. Wu et  al. [1 ] integrated these neural \nnetworks in the open-source library DeepChem [34]. \nExperiments were conducted on different datasets in \nMoleculeNet [1 ], and the best model was named \nMolNet [19]. The MPNN was proposed by Gilmer et al. \n[33] and is a graph-supervised general model \nframework for molecular property prediction. Its \nshortcomings are that it is difficult to use when the \nmolecular size is large, and the number of input \nmessages in the established fully connected graph \ndepends on the number of nodes. Withnall et  al. [35] \nintroduced the attention block and the edge memory \nblock into the MPNN framework and proposed the \nattention message passing neural network (AMPNN) \nmodel and the edge memory message neural network \n(EMNN) model. AMPNN and EMNN only need to use \nthe underlying chemical map data, without additional \nchemical descriptors. The introduction of the attention \nmechanism in AMPNN makes the model interpretable. \nWhile the performance of EMNN is better than that of \nAMPNN, the computing cost is also higher. Maziarka \net al. [36] applied the transformer encoder to molecules \nand proposed the molecule attention transformer \n(MAT) model. The attention mechanism in transformer \nis strengthened through the distance between atoms \nand the molecular graph structure. However, the lack of \nfeatures obtained by the model limits the improvement \nof the model performance. Furthermore, Wang et  al. \nPage 3 of 16\nSong et al. Journal of Cheminformatics           (2023) 15:27 \n \n[37] used graphs to represent molecular data, using \nvectors to represent atoms and representing each \nmolecule as a matrix according to the connections \nbetween atoms. In addition, to preserve the spatial \nconnectivity information on molecules, a convolutional \nspatial graph embedding layer (C-SGEL) is introduced \non the graph convolutional neural network, and \nmultiple C-SGELs are stacked to form a convolutional \nspatial graph embedding network. The network can \nlearn feature representations in molecular graphs while \nintroducing molecular fingerprints to improve the \ngeneralization ability of the network. Chen et  al. [38] \nproposed the algebraic graph-assisted bidirectional \ntransformer (AGBT) model to focus on three-\ndimensional (3D) information for molecules. Algebraic \ngraphs generate low-dimensional molecular \nrepresentations. Furthermore, the deep bidirectional \ntransformer (DBT) learns the basic principles of \nmolecular composition from datasets. The molecular \nproperty prediction task is completed through fine-\ntuning. There is a large error in fusing these two \nmolecular representations, which are from algebraic \ngraphs and DBT. Moreover, Cho et al. [39] proposed a \n3D graph convolution network to extract 3D molecular \nstructures from molecular graphs and combined it with \na graph convolution network, which can accurately \npredict the global and local property of molecules. The \nmethod has high generalization ability and is \nparticularly suitable for protein ligand binding affinity \nprediction. Sun et  al. [40] proposed InfoGraph, an \nunsupervised graph representation learning model, to \nmaximize the mutual information between the \nrepresentation of the whole graph and the \nrepresentation of substructures at different scales. \nSubsequently, it was extended to semi-supervised \nlearning tasks for graph-level representations, and the \nsemi-supervised learning model InfoGraph* was \nfurther proposed. InfoGraph* maximizes the mutual \ninformation between unsupervised graph \nrepresentations learned by InfoGraph and those \nlearned by existing supervised methods. InfoGraph is \nused to train unlabeled data, and supervised learning \ncan also be used to train labeled data. InfoGraph \nmodels and InfoGraph* models perform well in graph \nclassification and molecular property prediction, and \nhave enriched the research in the field of semi-\nsupervised learning graph structure data. Meng et  al. \n[41] proposed the extended graph convolution neural \nnetwork for the construction of new molecular graphs \nby fusing ideas such as the graph attention network and \ngated graph neural network. A new molecular graph is \nconstructed from the vertices of the atom groups, and \nan attention mechanism is added to focus on the atom \ngroups that affect the prediction of molecular \nproperties, making the model interpretable using gated \njump connections. However, the model does not have \nthe best performance on all tasks. Hu et  al. [42] \nproposed a pre-trained neural network strategy and a \nself-supervised approach based on pre-training a graph \nneural network with expressive power at the level of \nindividual nodes and the whole graph using easily \naccessible node-level information. This method learns \nboth local and global representations and generates \ngraph-level representations. This strategy, used \ntogether with the graph isomorphism network(GIN), \ncan avoid negative migration between downstream \ntasks and improve the generalization of downstream \ntasks, but its robustness still needs to be further \nimproved. Liao et  al. [43] proposed LanczosNet, a \nmultiscale graph convolution model, for efficient \nprocessing of graph structured data. The model is based \non the tri-diagonal decomposition of the Lanczos \nalgorithm, which is used to construct a low-rank \napproximation of the graph Laplacian operator. This \nmethod can efficiently calculate matrix powers and \ncollect the multiscale information, and also builds a \nlearnable spectral filter to expand the model capacity. \nChen et al. [44] proposed a local relational pool model \non the substructure counting to complete the molecular \nproperty prediction by considering the existence of \nsubstructures at the local level. This method is superior \nto most models and allows efficient counting of \nsubgraphs and induced subgraphs on random synthetic \ngraphs. In contrast to the GNN variant, it can learn \nsubstructure information from the data and does not \ndepend on manual production. Inspired by multi-view \nlearning, Ma et  al. [45] proposed a multi-view graph \nneural network (MV-GNN) considering the \ninformation of atoms and bonds. The method includes \na shared self-attention readout component to make the \nmodel interpretable. In order to enable information \ncommunication between two views, the method \nproposes a cross-dependent information transfer \nscheme that produces a variant of MV-GNN, \nMV-GNNcross, which has better expressiveness. Both \nmodels have strong robustness. Bécigneul et  al. [46] \nproposed a model for computing graph embeddings \nusing argument prototypes in order to address the \nproblem of the loss of structural or semantic \ninformation owing to averaging or summing the \nembedded nodes into an aggregated graph \nrepresentation. The method combines a parametric \ngraph model and optimal transport to learn graph \nrepresentation, which improves the representational \npower of the model. The model also produces a \nsmoother graph embedding space compared to the \nPage 4 of 16Song et al. Journal of Cheminformatics           (2023) 15:27 \ncommon GNN method. Tang et  al. [47] proposed a \ngraph neural network framework, which is based on a \nself-attention message passing neural network, to \nidentify the relationship between lipophilicity and \nwater solubility with structure, and thus study the \nrelationship between the molecular properties and \nstructure. The use of self-attention mechanisms \nimproves the interpretability of the model and enables \nvisualization based on the contribution of each atom to \nthe property. Yang et  al. [19] proposed the directed \nMPNN(DMPNN), which uses a mixed representation \nof key-centered convolution encoding molecules and \ndescriptors to make the encoding flexible and strongly \nprioritized, improving the generalization ability. The \nmodel obtains excellent performance on both public \nand private datasets, but the molecular property \nprediction performance is poor when the model \ncontains 3D information, the dataset is small, or the \nclasses are particularly unbalanced.\nIn conclusion, we found that the current molecular \nproperty prediction based on deep learning techniques \nhas the problem of low prediction accuracy. The main \nreason for this problem is poor generalization ability \ndue to the use of traditional activation functions, such as \nReLU, PReLU, and Tanh, in the nonlinear representation \nof molecular features. There may be problems with \ngradient disappearance or explosion in the network. The \nglobal information cannot be taken into account when \nmolecular detail features are extracted. In this regard, \nthis paper makes the following contributions. \n1. A new neural network framework, DHTNN, is \nproposed; it uses an activation function (Beaf ), \nresidual network, and transformer based on Double-\nhead attention to process and extract molecular \nfeatures for high-precision molecular property \nprediction.\n2. A new activation function, Beaf, is defined, which \ncan nonlinearize the molecular characteristics. \nCompared with other activation functions, the \nperformance of our model DHTNN using the \nactivation function beaf is improved.\n3. The molecular residual network is introduced to \nsolve the gradient problem of the neural network and \nensure that the model can converge quickly.\n4. The Transformer based on Double-head attention \nextracts the intrinsic detailed features of molecules \nand acquires global information in parallel, effectively \nimproving the performance of the model in \npredicting molecular properties.\n5. Our method was experimentally tested on six \ndatasets from the MoleculeNet [1] benchmark \ndataset, and achieved better performance compared \nto current machine learning and deep learning \nmethods.\nSpecific method\nThe neural network framework is divided into three \nparts, as shown in Fig.  1, which are the high-precision \nnonlinear generalization representation of molecular \nFig. 1 Overall DHTNN architectural diagram. A High-precision nonlinear generalization representation of molecular features. B Molecular residual \nnetwork encoding. C Molecular feature extraction of Transformer based on Double-head attention\nPage 5 of 16\nSong et al. Journal of Cheminformatics           (2023) 15:27 \n \nfeatures, the molecular residual network encoding, and \nthe molecular feature extraction of Transformer based \non the Double-head block. The high-precision nonlin -\near generalization representation of molecular features is \nused to improve the accuracy and generalization of the \nalgorithm model using a new activation function, Beaf, \nafter the molecular chemical formula is transformed \ninto a molecular map. The molecular residual network \nencoding part contains the directed MPNN, the batch \nnormalization layer, the molecular feed forward neural \nnetwork(Mole FFN), and the residual network. Its func -\ntion is to adjust the data distribution and pass the data \nforward after encoding the molecular map of the previ -\nous section into a matrix. A residual network is added \nto keep the neural network gradient from disappearing \nor exploding. The Molecular feature extraction of the \nTransformer based on the Double-head block quickly \nand accurately extracts intrinsic detailed features in mol -\necules and obtains molecular global information in paral-\nlel to further improve the model prediction performance.\nHigh‑precision nonlinear generalization representation \nof molecular features\nIn this paper, a DHTNN is proposed for molecular \nproperty prediction. The molecular residual network \nencoding structure is proposed in this neural network \nframework structure, in which a graph convolution \nneural network is used for the message passing process. \nHence, for any molecular dataset, the input molecular \nchemical formula needs to be first converted into \nthe form of a molecular map. In order to facilitate \ndata reading and memory saving by computers, the \nmolecular chemical formula is usually represented \nby a matrix [19, 47, 48], which contains atom features \nand bond features. The input and output of a neural \nnetwork need to be nonlinear so that the neural \nnetwork can fit complex functions as the number of \nlayers deepens. By introducing activation functions, \nneural networks can be equipped with nonlinear \ncharacteristics. The commonly used activation function \nhas some shortcomings, such as easy saturation, \ninability to map the negative value part, or inaccurate \nmapping of the negative value part, which ultimately \nmakes it difficult to improve the accuracy of molecular \nproperty prediction. For example, Tanh approaches \nsaturation at x =  3 (as shown in Fig.  2a). The gradient \ndisappears after saturation. From the ReLU function \nimage (as shown in Fig.  2b), the derivative is one when \nx > 0 , and there is no gradient decay. However, the \nvalue of the function is constant zero when x < 0 and \nthe function cannot complete the accurate mapping, \nwhich directly affects the accuracy of nonlinearized \nmolecular features. ELU improves on ReLU for the part \nof the function that is less than zero. From its function \nFig. 2 Images of Tanh (a), ReLU (b), ELU (c), GeLU (d) and Beaf (e)\nPage 6 of 16Song et al. Journal of Cheminformatics           (2023) 15:27 \nimage (as shown in Fig.  2c), it also has the mapping \ncapability in the negative part. However, the curves are \nflatter and there is little differentiation between values \nafter mapping. The GeLU function image (as shown \nin Fig.  2d) is smooth, but the function value quickly \ntends to zero in the negative half-axis. Therefore, the \nnonlinearized mapping about GeLU is very limited for \nthe part less than zero.\nTo address the shortcomings of the existing activation \nfunctions, such as Tanh is easy to saturate, the negative \npart of ReLU cannot be mapped, and the negative part of \nELU and GeLU are not mapped accurately. In this paper, \nwe propose the activation function Beaf, which is more \nsuitable for molecular feature nonlinearization mapping \nand has better generalization. The specific equation is as \nfollows:\nwhere x denotes the input, and f (x ) denotes the output. \nFrom Equation (1), Beaf consists of a primary function \n(1)\nf(x) = x ·tanh (s(x)) − c, where s (x) = SoftPlus(x) = In(1 + ex )\nx, Tanh, SoftPlus and a constant c, which enables a non -\nlinearized mapping. The function introduces a constant \nc, c ∈ (0, 0.004 ] . It can adjust the function up and down \ntranslation, so as to control the speed of the function \nvalue tends to zero, so that the function is more flexible. \nCombined with our proposed model DHTNN, we take \na value of 0.002 for the constant c here. This is because \nexperiments were performed on Lipophilicity, PDBbind, \nPCBA, BACE, Tox21, and SIDER datasets, and better \naccuracy of molecular property prediction achieves on all \nthese different datasets when c = 0.002. Thus, it is fur -\nther demonstrated that Beaf can better nonlinearize the \nmolecular features when c is taken as 0.002. The Beaf \nfunction image is shown in Fig.  2e, and in contrast to \nTanh (as shown in Fig.  2a), Beaf does not saturate and \nis derivable everywhere; The negative part can also be \nmapped compared to ReLU (as shown in Fig.  2b); Com-\npared with ELU (as shown in Fig.  2c), the nonlinear map-\nping in the negative part is more obvious, the distinction \nbetween values after mapping is greater, and the mapping \nis more accurate; Compared with GeLU (as in Fig.  2d), it \nFig. 3 Diagram of the molecular residual network encoding framework. The framework contains a directed MPNN, a batch normalization layer, a \nmolecular feed forward neural network, and a residual network\nPage 7 of 16\nSong et al. Journal of Cheminformatics           (2023) 15:27 \n \ndoes not converge to zero prematurely and is able to map \nmore negative values.\nMolecular residual network encoding\nAfter the high-precision nonlinear generalization rep -\nresentation of the molecular features in \"High-precision \nnonlinear generalization representation of molecular fea -\ntures\" Section is used to obtain the molecular map matrix, \nthe molecular map matrix is subsequently encoded with a \nmolecular residual network (shown in Fig. 3). The specific \nsteps are as follows:\nDirected MPNN [19]\nThis acts on the molecular map for encoding. The \ndirected MPNN can be divided into two phases: the \ndirected message passing phase and the readout phase.\nThe directed MPNN needs to initialize the hidden state \nof the bond ( h0\nvw ) before the message passing phase, as \nshown in Equation (2).\nwhere xv is the node feature, evw is the edge feature, W i is \nthe learnable matrix, cat(xv, evw ) splices the atom feature \nand the bond feature, and τ is the activation function \nReLU.\nThis is followed by a directed message passing phase, \nwhich contains the message function M t and the bond \nupdate function U t . M t sends bond-related messages to \nobtain m t+1\nvw  , as shown in Equation (3). Then, U t updates \nthe hidden state of each bond in the graph to obtain ht+1\nvw  , \nas shown in Equation (4).\nwhere N (v)\\w is the neighbourhood edge of the bond \nvw in the graph, and each step of the directed message \npassing phase is done, for a total of T steps.\nThe atom hidden state ( hv ) of the molecule is obtained \nby summing up the bond hidden states, as shown in \nEquations (5) and (6).\nWe sum hv to obtain Hv , and use the readout function R \nto yield the characteristic y of the molecule, as shown in \nEquations (7) and (8).\n(2)h0\nvw = τ (W icat(xv,evw))\n(3)m t+ 1\nvw =\n∑\nk∈{N(v)\\w}\nM t\n(\nxv,xk,ht\nvk\n)\n(4)\nht+1\nvw =U t\n(\nht\nvw,m t+1\nvw\n)\n= τ\n(\nh0\nvw + W m m t+1\nvw\n)\n,t∈{ 1,···,T}\n(5)m v =\n∑\nw∈N(v)\nhT\nvw\n(6)hv =τ (W acat (xv,m v))\nAdjusting data distribution\nWhen training a neural network, the parameters of \nthe previous layer affect the input of the later layer, \nthus making the training complicated. This requires \nnormalizing the encoded data, adjusting the distribution \nof the data, reducing the internal covariance bias, \nand improving the training speed. Therefore, batch \nnormalization is required to optimize the mean position \nand variance size to make the new data distribution more \nclosely match the real data distribution.\nNormalization is done mainly by processing the mean \n( E[y] ) and variance ( Var[y] ) of a batch of data consisting \nof one layer. In order to calculate the numerical stability, \nthe constant ǫ is added; the learnable parameters γ and β \nare introduced for optimization as a way to improve the \nnonlinear expression, as shown in Equation (9).\nAggregating to generate global features\nThe molecular feed forward neural network receives \nthe data ( YD ) after the batch normalization process for \naggregation. The molecular feed forward neural network \nconsists of five layers of network structure: the fully \nconnected layer, activation function, dropout layer, fully \nconnected layer, and dropout layer. The molecular feed \nforward neural network can aggregate local features \ninto global features, ( YF ), which reduces the influence of \nthe feature location on test results, prevents overfitting, \nand improves the model generalization ability. The \nimplementation process can be characterized as in \nEquation (10):\nPreventing gradients disappearance\nAs the number of neural network layers deepens, there \nis a gradual decrease in the accuracy of the training and \ntest sets owing to gradient disappearance and gradient \nexplosion, so the neural network cannot converge. The \nresidual network connection is used after the batch \nnormalization process and the molecular feed forward \nneural network, and the y obtained from the directed \nMPNN and the YF obtained from the molecular feed \n(7)Hv =\n∑\nv∈G\nhv\n(8)y=R ({H v |v ∈ G })\n(9)YD = y− E[y]√\nVar[y]+ǫ ∗ γ + β\n(10)YF = MoleFFN (YD )\nPage 8 of 16Song et al. Journal of Cheminformatics           (2023) 15:27 \nforward neural network are connected with residuals \nto obtain YR . The residual network learns the difference \nbetween the input and output, and these two layers do \nan all-equal mapping to ensure that the gradient problem \ndoes not affect the results of the neural network, as \nshown in Equation (11).\nMolecular feature extraction of Transformer based \non Double‑head attention\nThe molecular map matrix ( YR ) obtained from the molec-\nular residual network encoding is input to the molecular \nfeature extraction of Transformer based on the Double-\nhead attention block for obtaining molecular features \n(shown in Fig.  4), which contains double-head attention, \nMultilayer Perceptron (MLP), layer normalization, Drop-\npath, and residual connectivity. Its processing is divided \ninto three main steps:\nMolecular intrinsic detail feature extraction\nThe molecular graph matrix is input to the first part \nof the molecular feature extraction of Transformer \nbased on the Double-head attention block, as shown in \nFig. 4a. This part consists of layer normalization, double-\nhead attention, Droppath, and residual connection for \nextracting the intrinsic detail features in the molecular \ngraph and assigning the weights reasonably. \n(11)YR = y⊕ YF\n(1) Layer normalization: each data point ( YR ) obtained \nby Equation (11) is normalized to adjust the molec -\nular characteristic distribution. The normalization \nis processed by calculating the mean, E[YR ]l , and \nthe variance, Var[YR ]l , of each data point. In order \nto calculate the stability of the values and prevent \nthe denominator from being zero, the constant ǫ is \nadded. The learnable parameters γ and β are intro-\nduced as a way to improve the nonlinear expres -\nsion. The process is shown in Equation (12): \n(2) Double-head attention: The weights are rationally \nassigned, increasing the weight of important \ninformation and decreasing the weight of \nunimportant information. This process allows the \nmodel to learn relevant information from both \nspaces. W q , W k , and W v are three trainable shared \nmatrices. The YL obtained by layer normalization is \nmultiplied with W q , W k , and W v to obtain q, k, and \nv, respectively. The calculation processes are given \nin Equations (13, 14, 15). \n(12)YL = YR − E[YR ]l\n√\nVar [YR ]l + ǫ\n∗γ + β\n(13)q =YLW q\n(14)k =YLW k\nFig. 4 Molecular feature extraction of Transformer based on Double-head attention. a Molecular intrinsic detail feature extraction. b Layer structure \nfor integrating intrinsic detail features. c Adjusting the data distribution before output\nPage 9 of 16\nSong et al. Journal of Cheminformatics           (2023) 15:27 \n \n As the molecular graph matrix only has the \ninformation of the length and width, this paper \nproposes Double-head attention to extract \nthe information of the length and width of the \nmolecular graph matrix; that is head = 2 , so q, k \nand v are divided into two parts. q is split into q1 \nand q2 . k is split into k1 and k2 . v is split into v1 and \nv2 . Then, q1 , k1 and v1 belong to head1 . q2 , k2 and v2 \nbelong to head2 . head1 and head2 are calculated as \nshown in Equations (16, 17), where dk1 and dk2 are \nthe dimensions of k1 and k2 , respectively. \n The output ( YDH ) of Double-head attention \n(DoubleHead) is obtained by concatenating head1 \nand head2 together, and the calculation formula is \ngiven in Equation (18). Here, W o is the parameter \nmatrix for better fusion of the concatenated data \nand ensures that the vector lengths of the input and \noutput of DoubleHead remain unchanged. \n3) Droppath: This contains two types of droppings. One \nis local dropping, and the other is global dropping. \nLocal dropping means dropping layers randomly with \na certain probability, but it is guaranteed that one \nbranch must be through. Global dropping randomly \nselects a branch and discards the rest of the layers. \nThe two types of droppings are alternated during \nthe network training [49]. A Droppath operation is \nperformed on YDH , which is obtained in the above \ndouble-head attention to obtain Ypa , as shown in \nEquation (19). \n4) Residual connection: Residual connection is done \nfor the data ( Ypa ) obtained after Droppath, with \nYR obtained from the molecular residual network \nencoding, as shown in Equation (20). \n(15)v=YLW v\n(16)\nhead1 =Attention\n(\nq1,k1,v1\n)\n= softmax\n(\nq1k1T\n√\ndk1\n)\nv1\n(17)\nhead2 =Attention\n(\nq2,k2,v2\n)\n= softmax\n(\nq2k2T\n√\ndk2\n)\nv2\n(18)\nYDH = DoubleHead (q,k,v)\n= Concat (head1, head2)W o\n(19)Ypa = Droppath (YDH )\n(20)YRpa = YR ⊕ Ypa\nLayer structure for integrating intrinsic detail features\nThe extracted intrinsic detail features are integrated and \nused to output the final molecular property prediction \nresults. The composition structure is similar to that \nin part a. The only difference is that the double-head \nattention in part a is replaced by the MLP , as shown in \nFig. 4b. The calculation equations are given in Equations \n(21, 22, 23 and 24) as follows:\nAdjusting the data distribution before output\nAfter the Transformer based on the Double-head \nattention block, the distribution of data causes large \nchanges, so before outputting the results, layer \nnormalization is performed again, as shown in Fig.  4(c), \nto adjust the data distribution before output. The \ncalculation formula is shown in Equation (25).\nThe results of the final molecular property prediction are \nobtained from the linear layer, as shown in Equation (26).\nExperiment and discussion\nSources of experiment molecular datasets and evaluation \nmetrics\nDataset Source\nIn deep learning, datasets play a pivotal role in training \nthe model and verifying the generalization of the \nproposed algorithm. The dataset used in this paper \nis from the MoleculeNet [1] benchmark dataset. Six \ndatasets (i.e., Lipophilicity, PDBbind, PCBA, BACE, \nTox21, and SIDER) were selected for the task type, \nincluding regression and classification, covering three \ndomains (i.e., physiology, physical chemistry, and \nbiophysics). The datasets were divided into a training \nset, validation set, and test set in the ratio of 8:1:1 with \n(21)YL2 = YRpa − E\n[\nYRpa\n]l\n√\nVar\n[\nYRpa\n]l + ǫ\n∗γ + β\n(22)Yml =MLP (YL2)\n(23)YPml = Droppath (Yml)\n(24)YRPm =YRpa ⊕ YPml\n(25)YLN = YRPm − E[YRPm ]l\n√\nVar [YRPm ]l+ ǫ\n∗γ + β\n(26)Y = Linear(YLN )\nPage 10 of 16Song et al. Journal of Cheminformatics           (2023) 15:27 \nrandom and scaffold splitting. The training set was used \nto train the model, the validation set was used to adjust \nhyperparameters and optimize the model, and the test \nset was used to evaluate the model performance. At the \nminimum, the dataset comprises 168 molecules, while \nthe maximum was 437,928 molecules to ensure that the \nalgorithm was applicable to datasets of various sizes. \n(1) Lipophilicity [50] Lipophilicity is derived \nfrom the ChEMBL database, containing 4,200 \ncompounds. The value of lipophilicity was obtained \nexperimentally and calculated by the octanol/\nwater partition coefficient. Lipophilicity affects the \nmembrane permeability and aqueous solubility; \ntherefore, the prediction of lipophilicity is crucial in \ndrug discovery.\n(2) PDBbind [51–53] PDBbind is a protein-ligand \ncomplex binding affinity dataset that establishes \na PDB-wide connection between structural and \nenergetic information of protein-ligand complexes.\n(3) PCBA [54] PubChem BioAssay (PCBA) is a dataset \nof biological activity; it is generated through high-\nthroughput screening, with 128 bioassays that \nmeasure 400,000 compounds.\n(4) BACE [55] BACE is a dataset of inhibitors of \nhuman β-secretase 1 (BACE-1) containing quan -\ntitative (IC50) and qualitative (binary label) results \ncombined with data for 1,513 compounds.\n(5) Tox21 [56] Toxicology in the 21st Century created \nthe toxicity data collection system, known as the \nTox21 dataset, which is a toxicity dataset containing \n8,014 compounds.\n(6) SIDER [57, 58] The Side Effect Resource (SIDER) is \na database of listed drugs and adverse drug reactions \n(ADRs), containing data on 1,427 compounds. It \nis divided into 27 classes of compounds, with drug \nside effects according to the organ class.\nAlgorithm evaluation metrics\nWe tested our neural network framework on six data -\nsets, including two regression datasets (Lipophilic -\nity, PDBbind) and four classification datasets (PCBA, \nBACE, Tox21, SIDER). The algorithm evaluation met -\nric for the regression dataset was the root mean square \nerror (RMSE), which is the arithmetic square root of the \nexpected value of the squared difference between the \nparameter estimate and the true value of the parameter. \nA smaller RMSE indicates a smaller error and better pre -\ndiction performance. The algorithm evaluation metrics \nfor classification datasets were the area under the recall \ncurve (PRC-AUC) and the area under the receiver oper -\nating characteristic curve (ROC-AUC) [59]. Larger AUC \nvalues indicate more stable models and better prediction \nperformance.\nExperiment results and analysis\nValidation of activation function selection\nIn order to verify the algorithmic effectiveness of our \nproposed activation function Beaf on our model, we \nperformed validation experiments on the activation \nfunction selection. On the six datasets (i.e., Lipophilicity, \nPDBbind, PCBA, BACE, Tox21 and SIDER), we applied \nthe activation functions Beaf, ELU and GeLU to our \nalgorithmic model and compared their performances, \nshown in Tables 1 and 2, respectively.\nThe Lipophilicity and PDBbind datasets, shown in \nTable 1, are regression datasets. RMSE was used to evalu-\nate our algorithm performance based on these two data -\nsets. A lower RMSE value indicates better performance. \nAs can be seen from Table  1, the RMSE value for our \nalgorithmic model based on the Beaf on the Lipophilic -\nity dataset is 0.577 ± 0.049 , which is 0.146 lower than the \n0.723 ± 0.037 obtained by the ELU. It is also 0.058 lower \ncompared to using the GeLU (GeLU: 0.635 ± 0.040 ). On \nthe PDBbind dataset, the RMSE value for our algorithmic \nmodel based on the Beaf is 1.771 ± 0.300 , which is 0.283 \nlower compared to using the ELU (ELU: 2.054 ± 0.265 ). \nIt is also 0.248 lower than the 2.019 ± 0.278 obtained by \nthe GeLU. Therefore, there are significant advantages to \nuse Beaf on the Lipophilicity and PDBbind datasets.\nTable 1 Comparisons of performance for the activation \nfunctions Beaf, ELU, and GeLU on Lipophilicity and PDBbind \ndatasets (lower values are better)\nGeLU ELU Beaf\nLipophilicity 0.635 ± 0.040 0.723 ± 0.037 0.577 ± 0.049\nPDBbind 2.019 ± 0.278 2.054 ± 0.265 1.771 ± 0.300\nTable 2 Comparisons of performance for the activation \nfunctions Beaf, ELU, and GeLU on PCBA, BACE,Tox21 and SIDER \ndatasets (higher values are better)\nGeLU ELU Beaf\nPCBA 0.806 ± 0.002 0.663 ± 0.006 0.821 ± 0.005\nBACE 0.928 ± 0.019 0.909 ± 0.022 0.923 ± 0.035\nTox21 0.843 ± 0.025 0.840 ± 0.049 0.847 ± 0.015\nSIDER 0.652 ± 0.027 0.628 ± 0.012 0.679 ± 0.015\nPage 11 of 16\nSong et al. Journal of Cheminformatics           (2023) 15:27 \n \nIn Table  2, the PCBA, BACE, Tox21 and SIDER \ndatasets are classification datasets. AUC was used to \nevaluate our algorithm performance based on these \nfour datasets. A higher AUC value indicates better \nperformance. As can be seen from Table  2, the AUC \nvalue for our algorithm model based on the Beaf is \n0.821 ± 0.005 on the PCBA dataset. This represents \nan improvement in the AUC value of 0.158 over the \nmodel with ELU (ELU: 0.663 ± 0.006 ) and of 0.015 over \nthe model with GeLU (GeLU: 0.806 ± 0.002 ). On the \nBACE dataset, the AUC value for our algorithmic model \nbased on the Beaf is 0.923 ± 0.035 . This represents \nan improvement in the AUC value of 0.014 over the \n0.909 ± 0.022 obtained by the ELU. This is slightly \nlower, by 0.005, than the model with the GeLU (GeLU: \n0.928 ± 0.019 ). On the Tox21 dataset, the AUC value for \nour algorithmic model is 0.847 ± 0.015 based on the Beaf. \nThis represents an increase in the AUC value of 0.007 \nover the 0.840 ± 0.049 gained by the ELU. It represents \nan increase in the AUC value of 0.004 compared to using \nthe GeLU (GeLU: 0.843 ± 0.025 ). On the SIDER dataset, \nthe AUC value for our algorithmic model based on the \nBeaf is 0.679 ± 0.015 . This represents an improvement \nin the AUC value of 0.051 over the 0.628 ± 0.012 \nobtained by the ELU. It represents an increase the \nAUC value of 0.027 compared to the model with GeLU \n(GeLU: 0.652 ± 0.027 ). Therefore, there are significant \nadvantages of using Beaf on PCBA, BACE, Tox21, and \nSIDER datasets.\nIn conclusion, for ELU, all experimental results based \non the Beaf are better than those based on the ELU on \nthe datasets Lipophilicity and PDBbind. For GeLU, on \nthe four datasets (i.e., PCBA, BACE, Tox21, and SIDER), \nonly on the BACE dataset, the experimental results based \non the GeLU are slightly better than those based on the \nBeaf. The experimental results of the algorithmic model \nbased on the Beaf are better than those of the algorithmic \nmodel based on the GeLU on three of the four datasets. \nTherefore, we chose Beaf as the activation function for \nthe double-head transformer neural network (DHTNN) \nfor molecular property prediction.\nComparison of model performance\nOur experiments were run on a Windows 10 operating \nsystem with a 1.70 GHz Intel Xeon Bronze 3104 CPU, \n64 GB of RAM, and an NVIDIA RTX2080 GPU, using \npython 3.8 as the development language and PyTorch \n1.5.1 as the neural network framework for deep learning \ntraining.\nThe results of our algorithm were compared with the \nfollowing state-of-the-art methods: MolNet [1], RF on \nMorgan [19], FFN on Morgan [19], FFN on Morgan \ncounts [19], FFN on RDKit [19], and DMPNN [19]. The \nchemical descriptors used by RF on Morgan and FFN \non Morgan are Morgan fingerprints [17, 18]. FFN on \nMorgan counts uses count-based Morgan fingerprints. \nFFN on RDKit uses the chemical descriptors generated \nby RDKit [60]. The chemical descriptors of MolNet, \nDMPNN, and our model (DHTNN) are SMILES [61, 62].\nThe methods used for performance comparison \nincluded machine learning methods and deep learn -\ning methods, and RF on Morgan is currently the most \nadvanced method for machine learning. MolNet, FFN \non Morgan, FFN on Morgan Counts, FFN on RDKit \nand DMPNN are current advanced methods for deep \nlearning.\nFor the regression dataset, we calculated the RMSE to \nevaluate the performance of the algorithm. The lower \nthe RMSE, the better the model performance. As shown \nin Figs.  5a, b and 6 a, b, our model’s RMSE is lower \ncompared to the other models, whether by random \nsplitting or by scaffolding splitting. On the Lipophilicity \ndataset, our model’s performance (Ours: 0.577 ± 0.049 ) \nis 0.5% lower compared to DMPNN (DMPNN: \n0.582 ± 0.024 ) by random splitting (Table  3). Our model \nperformance (Ours: 0.590 ± 0.038 ) is by 5.8% lower \ncompared to DMPNN (DMPNN: 0.648 ± 0.057 ) by \nscaffold splitting (Table  4). This is because we use our \nproposed activation function Beaf in the high-precision \nnonlinear generalization representation of molecular \nfeatures. DMPNN uses the activation function ReLU, \nand the negative part of ReLU is mapped to zero, while \nBeaf is still able to map the negative part, especially \nthe values between −4 and 0. The negative values in \nthe Lipophilicity are concentrated between −2 and \n0, and after the nonlinear transformation by the Beaf \nactivation function, the neurons in the negative part do \nnot die. Therefore, our model outperforms DMPNN on \nthe regression dataset.\nFor the classification dataset, we calculated the PRC-\nAUC and ROC-AUC. The higher the AUC, the better \nthe model performance. As shown in Figs.  5c, d, e, f \nTable 3 Comparisons of performance with state-of-the-\nart methods on regression datasets, splitting the datasets by \nrandom splitting in a ratio of 8:1:1 (lower values are better)\nMethods Lipophilicity PDBbind\nMolNet [1] 0.655 ± 0.036 1.920 ± 0.070\nRF on Morgan [19] 0.823 ± 0.035 2.083 ± 0.324\nFFN on Morgan [19] 0.928 ± 0.044 2.778 ± 0.599\nFFN on Morgan counts [19] 0.874 ± 0.043 2.901 ± 0.812\nFFN on RDKit [19] 0.735 ± 0.039 2.020 ± 0.376\nDMPNN [19] 0.582 ± 0.024 1.945± 0.298\nOurs 0.577 ± 0.049 1.771 ± 0.300\nPage 12 of 16Song et al. Journal of Cheminformatics           (2023) 15:27 \nand 6c, d, e, f all of our models outperform the other \nmodels by random splitting. Our model also outper -\nforms the other models on three of the four datasets \nby scaffold splitting. Only on the Tox21 dataset, the \nexperimental results are slightly worse than those of \nother models. Compared with the random splitting \napproach, the scaffold splitting approach provides a \nmore realistic estimation of the model performance. \nOn the PCBA dataset, our model (Ours: 0.821 ± 0.005 ) \nimproves 61.4% compared to FFN on RDkit (FFN on \nRDkit: 0.207 ± 0.005 ) by random splitting (Table  5). \nAlso, our model (Ours: 0.715 ± 0.004 ) improves by \n55.4% compared to FFN on RDkit (FFN on RDkit: \n0.161 ± 0.005 ) by scaffold splitting (Table  6). The per -\nformance improvement is most significant on the \nPCBA dataset among all classified datasets. The molec -\nular feature extraction of Transformer based on the \nDouble-head block added to our model is used to learn \nindividual molecular features and atom-to-atom inter -\nrelationships. The greater the number of data samples, \nthe richer the intrinsic features learned and the better \nthe molecular property prediction. The PCBA contains \n430,000 data samples and is the largest dataset in the \nfour classification datasets used in our experiments. \nTherefore, the performance improvement of our algo -\nrithm is the greatest.\nWhether on regression or classification datasets, \nour model did not exhibit gradient disappearance or \nexplosion. The molecular residual network encoding in \nthe model played an important role in ensuring that the \nmodel converged.\nConclusion\nIn this paper, a new algorithmic framework, DHTNN, \nwas proposed for molecular property prediction. Beaf, \na new activation function, is included in the molecu -\nlar nonlinear representation part, and the negative part \nis also able to be mapped, making the mapping more \nTable 4 Comparisons of performance with state-of-the-art methods on classification datasets, splitting the datasets by random \nsplitting in a ratio of 8:1:1 (higher values are better)\nMethods PCBA BACE Tox21 SIDER\nMolNet [1] 0.136 ± 0.004 / 0.829 ± 0.006 0.648 ± 0.009\nRF on Morgan [19] / 0.825 ± 0.039 0.619 ± 0.015 0.572 ± 0.007\nFFN on Morgan [19] 0.263 ± 0.008 0.873 ± 0.040 0.788 ± 0.017 0.652 ± 0.010\nFFN on Morgan Counts [19] 0.268 ± 0.006 0.882 ± 0.030 0.790 ± 0.020 0.638 ± 0.020\nFFN on RDKit [19] 0.207 ± 0.005 0.858 ± 0.034 0.832 ± 0.016 0.654 ± 0.019\nDMPNN [19] 0.769 ± 0.010 0.892 ± 0.031 0.839 ± 0.022 0.657 ± 0.016\nOurs 0.821 ± 0.005 0.923 ± 0.035 0.847 ± 0.015 0.679 ± 0.015\nTable 5 Comparisons of performance with state-of-the-art \nmethods on regression datasets, splitting the datasets by scaffold \nsplitting in a ratio of 8:1:1 (lower values are better)\nMethods Lipophilicity PDBbind\nMolNet [1] 0.655 ± 0.036 1.920 ± 0.070\nRF on Morgan [19] 0.908 ± 0.052 2.011 ± 0.240\nFFN on Morgan [19] 1.045 ± 0.042 2.737± 0.518\nFFN on Morgan Counts [19] 1.003 ± 0.068 3.015 ± 0.636\nFFN on RDKit [19] 0.792 ± 0.032 1.842 ± 0.252\nDMPNN [19] 0.648 ± 0.057 1.858 ± 0.300\nOurs 0.590 ± 0.038 1.599 ± 0.199\nTable 6 Comparisons of performance with state-of-the-art methods on classification datasets, splitting the datasets by scaffold \nsplitting in a ratio of 8:1:1 (higher values are better)\nMethods PCBA BACE Tox21 SIDER\nMolNet [1] 0.136 ± 0.004 / 0.829 ± 0.006 0.648 ± 0.009\nRF on Morgan [19] / 0.804 ± 0.035 0.582 ± 0.031 0.540 ± 0.013\nFFN on Morgan [19] 0.189 ± 0.005 0.843 ± 0.052 0.722 ± 0.041 0.608 ± 0.035\nFFN on Morgan Counts [19] 0.195 ± 0.003 0.849 ± 0.047 0.725 ± 0.052 0.595 ± 0.033\nFFN on RDKit [19] 0.161 ± 0.005 0.833 ± 0.046 0.788 ± 0.046 0.618 ± 0.031\nDMPNN [19] 0.707 ± 0.002 0.759 ± 0.0291 0.779 ± 0.037 0.602 ± 0.024\nOurs 0.715 ± 0.004 0.774 ± 0.014 0.772 ± 0.023 0.661 ± 0.046\nPage 13 of 16\nSong et al. Journal of Cheminformatics           (2023) 15:27 \n \naccurate and improving the model nonlinear representa -\ntion accuracy and its generalization ability. In the molec -\nular encoding part, the addition of the residual network \nprevents the gradient from disappearing or exploding \nand ensures that the model can converge. In the extrac -\ntion of molecular features, the involvement of the Trans -\nformer based on Double-head attention can focus on the \nfeatures of the region of interest for the prediction results \nFig. 5 Performance of the model on Lipophilicity (a), PDBbind (b), PCBA (c), BACE (d), Tox21 (e) and SIDER (f) datasets. RMSE was calculated on \nLipophilicity (a), PDBbind (b), the lower the RMSE, the better the model performance. PCBA (c), BACE (d), Tox21 (e), and SIDER (f) on which AUC was \ncalculated; the higher the AUC, the better the model performance. Datasets were split by random\nPage 14 of 16Song et al. Journal of Cheminformatics           (2023) 15:27 \nand assign the weights reasonably. Running our model on \nsix datasets, our method outperformed current state-of-\nthe-art methods in all metrics. The experimental results \ndemonstrate the effectiveness of our proposed algorith -\nmic framework.\nAcknowledgements\nWe would like to thank Wu et al. [1] for providing the benchmark \ndatasets(Lipophilicity, PDBbind, PCBA, BACE, Tox21 and SIDER), which help us \nto train models and compare the performance of different models.\nFig. 6 Performance of the model on Lipophilicity (a), PDBbind (b), PCBA (c), BACE (d), Tox21 (e) and SIDER (f) datasets. RMSE was calculated on \nLipophilicity (a), PDBbind (b), the lower the RMSE, the better the model performance. PCBA (c), BACE (d), Tox21 (e), and SIDER (f) on which AUC was \ncalculated; the higher the AUC, the better the model performance. Datasets were split by scaffold\nPage 15 of 16\nSong et al. Journal of Cheminformatics           (2023) 15:27 \n \nAuthor contributions\nAll the authors made significant contributions to this work. Wenju Wang, \nJinghua Chen and Yuanbing Song conceived the algorithm; Yuanbing Song \nand Gang Chen performed the experiments; Yuanbing Song, Gang Chen and \nZhichong Ma analyzed the results; Yuanbing Song and Wenju Wang arranged, \nwrote and polished the manuscript. All authors have read and approved the \nfinal manuscript.\nFunding\nThe financial support for this work was provided by the Natural Science \nFoundation of Shanghai under Grant 19ZR1435900.\nAvailability of data and materials\nThe dataset used in the experiments is provided by MoleculeNet and ChEMBL \nat http:// ww82. molec ulenet. ai/ and http:// www. bioinf. jku. at/ resea rch/ lsc/ \nindex. html. The codes and models are available at https:// github. com/ songy \nuanbi ng6/ dhtnn.\nDeclarations\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 5 May 2022   Accepted: 16 February 2023\nReferences\n 1. Wu Z, Ramsundar B, Feinberg EN, Gomes J, Geniesse C, Pappu AS, \nLeswing K, Pande V (2018) Moleculenet: a benchmark for molecular \nmachine learning. Chem Sci 9(2):513–530\n 2. Li J, Jiang X (2021) Mol-bert: an effective molecular representation with \nbert for molecular property prediction. Wirel Commun Mob Comput \n2021:1–7. https:// doi. org/ 10. 1155/ 2021/ 71818 15\n 3. Toussi CA, Haddadnia J, Matta CF (2021) Drug design by machine-trained \nelastic networks: predicting ser/thr-protein kinase inhibitors’ activities. \nMol Divers 25(2):899–909\n 4. Cheng J, Zhang C, Dong L (2021) A geometric-information-enhanced \ncrystal graph network for predicting properties of materials. Commun \nMater 2(1):1–11\n 5. Woo G, Fernandez M, Hsing M, Lack NA, Cavga AD, Cherkasov A (2020) \nDeepcop: deep learning-based approach to predict gene regulating \neffects of small molecules. Bioinformatics 36(3):813–818\n 6. Roy K, Kar S, Das RN (2015) A primer on QSAR/QSPR modeling: funda-\nmental concepts. Springer, New York\n 7. Katritzky AR, Lobanov VS, Karelson M (1995) Qspr: the correlation and \nquantitative prediction of chemical and physical properties from struc-\nture. Chem Soc Rev 24(4):279–287\n 8. Yee LC, Wei YC (2012) Current modeling methods used in QSAR/QSPR. In: \nStatistical modelling of molecular descriptors in QSAR/QSPR,  vol 2, pp \n1–31\n 9. Tareq Hassan Khan M (2010) Predictions of the admet properties of \ncandidate drug molecules utilizing different qsar/qspr modelling \napproaches. Curr Drug Metab 11(4):285–295\n 10. Cao D-S, Liang Y-Z, Xu Q-S, Li H-D, Chen X (2010) A new strategy of outlier \ndetection for qsar/qspr. J Comput Chem 31(3):592–602\n 11. Shen J, Nicolaou CA (2019) Molecular property prediction: recent trends \nin the era of artificial intelligence. Drug Discov Today Technol 32:29–36\n 12. Walters WP , Barzilay R (2020) Applications of deep learning in mol-\necule generation and molecular property prediction. Acc Chem Res \n54(2):263–270\n 13. Hessler G, Baringhaus K-H (2018) Artificial intelligence in drug design. \nMolecules 23(10):2520\n 14. Gasteiger J (2020) Chemistry in times of artificial intelligence. ChemPhy-\nsChem 21(20):2233–2242\n 15. Faber FA, Hutchison L, Huang B, Gilmer J, Schoenholz SS, Dahl GE, \nVinyals O, Kearnes S, Riley PF, Von Lilienfeld OA (2017) Prediction errors of \nmolecular machine learning models lower than hybrid dft error. J Chem \nTheory Comput 13(11):5255–5264\n 16. Zou H, Hastie T (2005) Regularization and variable selection via the elastic \nnet. J Royal Stat Soc Ser B (Stat Methodol) 67(2):301–320\n 17. Kearnes S, McCloskey K, Berndl M, Pande V, Riley P (2016) Molecular graph \nconvolutions: moving beyond fingerprints. J Comput Aided Mol Des \n30(8):595–608\n 18. Pattanaik L, Coley CW (2020) Molecular representation: going long on \nfingerprints. Chem 6(6):1204–1207\n 19. Yang K, Swanson K, Jin W, Coley C, Eiden P , Gao H, Guzman-Perez A, Hop-\nper T, Kelley B, Mathea M et al (2019) Analyzing learned molecular repre-\nsentations for property prediction. J Chem Inf Model 59(8):3370–3388\n 20. McDonagh JL, Silva AF, Vincent MA, Popelier PL (2017) Machine learning \nof dynamic electron correlation energies from topological atoms. J Chem \nTheory Comput 14(1):216–224\n 21. Zhao C, Zhang H, Zhang X, Liu M, Hu Z, Fan B (2006) Application of sup-\nport vector machine (svm) for prediction toxic activity of different data \nsets. Toxicology 217(2–3):105–119\n 22. Chen N (2004) Support vector machine in chemistry. World Scientific, \nSingapore\n 23. Heikamp K, Bajorath J (2014) Support vector machines for drug discovery. \nExpert Opin Drug Discov 9(1):93–104\n 24. Zheng B, Gu GX (2021) Prediction of graphene oxide functionalization \nusing gradient boosting: implications for material chemical composition \nidentification. ACS Appl Nano Mater 4(3):3167–3174\n 25. Krmar J, Džigal M, Stojković J, Protić A, Otašević B (2022) Gradient \nboosted tree model: a fast track tool for predicting the atmospheric \npressure chemical ionization-mass spectrometry signal of antipsychotics \nbased on molecular features and experimental settings. Chemom Intell \nLab Syst 224:104554\n 26. Deng D, Chen X, Zhang R, Lei Z, Wang X, Zhou F (2021) Xgraphboost: \nextracting graph neural network-based features for a better prediction of \nmolecular properties. J Chem Inform Model 61(6):2697–2705\n 27. Wu J, Kong L, Yi M, Chen Q, Cheng Z, Zuo H, Yang Y (2022) Prediction and \nscreening model for products based on fusion regression and xgboost \nclassification. Comput Intell Neurosci. https:// doi. org/ 10. 1155/ 2022/ \n49876 39\n 28. Tian H, Ketkar R, Tao P (2022) Accurate admet prediction with xgboost. \narXiv Preprint. https:// doi. org/ 10. 48550/ arXiv. 2204. 07532\n 29. Paul A, Furmanchuk A, Liao W-K, Choudhary A, Agrawal A (2019) Property \nprediction of organic donor molecules for photovoltaic applications \nusing extremely randomized trees. Mol Inform 38(11–12):1900038\n 30. Svozil D, Kvasnicka V, Pospichal J (1997) Introduction to multi-layer feed-\nforward neural networks. Chemom Intell Lab Syst 39(1):43–62\n 31. Lusci A, Pollastri G, Baldi P (2013) Deep architectures and deep learning \nin chemoinformatics: the prediction of aqueous solubility for drug-like \nmolecules. J Chem Inform Model 53(7):1563–1575\n 32. Schütt KT, Arbabzadah F, Chmiela S, Müller KR, Tkatchenko A (2017) \nQuantum-chemical insights from deep tensor neural networks. Nat Com-\nmun 8(1):1–8\n 33. Gilmer J, Schoenholz SS, Riley PF, Vinyals O, Dahl GE (2017) Neural mes-\nsage passing for quantum chemistry. In: international conference on \nmachine learning. PMLR, 1263–1272\n 34. Ramsundar B (2018) Molecular machine learning with deepchem. PhD \nthesis, Stanford University\n 35. Withnall M, Lindelöf E, Engkvist O, Chen H (2020) Building attention and \nedge message passing neural networks for bioactivity and physical-\nchemical property prediction. J Cheminform 12(1):1–18\n 36. Maziarka Ł, Danel T, Mucha S, Rataj K, Tabor J, Jastrzębski S (2020) Mol-\necule attention transformer. arXiv Preprint. https:// doi. org/ 10. 48550/ arXiv. \n2002. 08264\n 37. Wang X, Li Z, Jiang M, Wang S, Zhang S, Wei Z (2019) Molecule property \nprediction based on spatial graph embedding. J Chem Inform Model \n59(9):3817–3828\n 38. Chen D, Gao K, Nguyen DD, Chen X, Jiang Y, Wei G-W, Pan F (2021) Alge-\nbraic graph-assisted bidirectional transformers for molecular property \nprediction. Nat Commun 12(1):1–9\n 39. Cho H, Choi IS (2019) Enhanced deep-learning prediction of molecu-\nlar properties via augmentation of bond topology. ChemMedChem \n14(17):1604–1609\nPage 16 of 16Song et al. Journal of Cheminformatics           (2023) 15:27 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 40. Sun F-Y, Hoffmann J, Verma V, Tang J (2019) Infograph: Unsupervised and \nsemi-supervised graph-level representation learning via mutual informa-\ntion maximization. Arxiv Preprint. https:// doi. org/ 10. 48550/ arXiv. 1908. \n01000\n 41. Meng M, Wei Z, Li Z, Jiang M, Bian Y (2019) Property prediction of \nmolecules in graph convolutional neural network expansion. In: 2019 \nIEEE 10th International Conference on Software Engineering and Service \nScience (ICSESS). IEEE, 263–266\n 42. Hu W, Liu B, Gomes J, Zitnik M, Liang P , Pande V, Leskovec J (2019) Strate-\ngies for pre-training graph neural networks. arXiv Preprint. https:// doi. \norg/ 10. 48550/ arXiv. 1905. 12265\n 43. Liao R, Zhao Z, Urtasun R, Zemel RS (2019) Lanczosnet: multi-scale deep \ngraph convolutional networks. arXiv Preprint. https:// doi. org/ 10. 48550/ \narXiv. 1901. 01484\n 44. Chen Z, Chen L, Villar S, Bruna J (2020) Can graph neural networks count \nsubstructures? Adv Neural Inform Process Syst 33:10383–10395\n 45. Ma H, Bian Y, Rong Y, Huang W, Xu T, Xie W, Ye G, Huang J (2020) Multi-\nview graph neural networks for molecular property prediction. arXiv \nPreprint. https:// doi. org/ 10. 48550/ arXiv. 2005. 13607\n 46. Chen B, Bécigneul G, Ganea O-E, Barzilay R, Jaakkola T (2020) Optimal \ntransport graph neural networks. Arxiv Preprint. https:// doi. org/ 10. 48550/ \narXiv. 2006. 04804\n 47. Tang B, Kramer ST, Fang M, Qiu Y, Wu Z, Xu D (2020) A self-attention based \nmessage passing neural network for predicting molecular lipophilicity \nand aqueous solubility. J Cheminform 12(1):1–9\n 48. Li Y, Li P , Yang X, Hsieh C-Y, Zhang S, Wang X, Lu R, Liu H, Yao X (2021) \nIntroducing block design in graph neural networks for molecular proper-\nties prediction. Chem Eng J 414:128817\n 49. Larsson G, Maire M, Shakhnarovich G (2016) Fractalnet: ultra-deep neural \nnetworks without residuals. arXiv Preprint. https:// doi. org/ 10. 48550/ arXiv. \n2006. 04804\n 50. Gaulton A, Bellis LJ, Bento AP , Chambers J, Davies M, Hersey A, Light \nY, McGlinchey S, Michalovich D, Al-Lazikani B et al (2012) Chembl: a \nlarge-scale bioactivity database for drug discovery. Nucleic Acids Res \n40(D1):1100–1107\n 51. Wang R, Fang X, Lu Y, Wang S (2004) The pdbbind database: collection of \nbinding affinities for protein- ligand complexes with known three-dimen-\nsional structures. J Med Chem 47(12):2977–2980\n 52. Wang R, Fang X, Lu Y, Yang C-Y, Wang S (2005) The pdbbind database: \nmethodologies and updates. J Med Chem 48(12):4111–4119\n 53. Liu Z, Li Y, Han L, Li J, Liu J, Zhao Z, Nie W, Liu Y, Wang R (2015) Pdb-wide \ncollection of binding data: current status of the pdbbind database. Bioin-\nformatics 31(3):405–412\n 54. Wang Y, Xiao J, Suzek TO, Zhang J, Wang J, Zhou Z, Han L, Karapetyan K, \nDracheva S, Shoemaker BA et al (2012) Pubchem’s bioassay database. \nNucleic Acids Res 40(D1):400–412\n 55. Subramanian G, Ramsundar B, Pande V, Denny RA (2016) Computa-\ntional modeling of β-secretase 1 (bace-1) inhibitors using ligand based \napproaches. J Chem Inform Model 56(10):1936–1949\n 56. Huang R, Xia M, Nguyen D-T, Zhao T, Sakamuru S, Zhao J, Shahane SA, \nRossoshek A, Simeonov A (2016) Tox21challenge to build predictive \nmodels of nuclear receptor and stress response pathways as mediated by \nexposure to environmental chemicals and drugs. Front Environ Sci 3:85\n 57. Kuhn M, Letunic I, Jensen LJ, Bork P (2016) The sider database of drugs \nand side effects. Nucleic Acids Res 44(D1):1075–1079\n 58. Altae-Tran H, Ramsundar B, Pappu AS, Pande V (2017) Low data drug \ndiscovery with one-shot learning. ACS Cent Sci 3(4):283–293\n 59. Davis J, Goadrich M (2006) The relationship between precision-recall \nand roc curves. In: proceedings of the 23rd international conference on \nmachine learning, 233–240\n 60. Landrum G, et al (2013) Rdkit: A software suite for cheminformatics, \ncomputational chemistry, and predictive modeling. Academic Press, \nCambridge, Massachusetts, USA\n 61. Weininger D (1988) Smiles, a chemical language and information system. \n1. Introduction to methodology and encoding rules. J Chem Inform \nComput Sci 28(1):31–36\n 62. Jastrzębski S, Leśniak D, Czarnecki WM (2016) Learning to SMILE(S). arXiv \nPreprint. https:// doi. org/ 10. 48550/ arXiv. 1602. 06289\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations."
}