{
  "title": "TransLO: A Window-Based Masked Point Transformer Framework for Large-Scale LiDAR Odometry",
  "url": "https://openalex.org/W4382459110",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2226840974",
      "name": "Jiuming Liu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2092552325",
      "name": "Guangming Wang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A3211688898",
      "name": "Chaokang Jiang",
      "affiliations": [
        "China University of Mining and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098541740",
      "name": "Zhe Liu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2111292751",
      "name": "Hesheng Wang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2226840974",
      "name": "Jiuming Liu",
      "affiliations": [
        "Ministry of Education of the People's Republic of China",
        "Ministry of Education"
      ]
    },
    {
      "id": "https://openalex.org/A2092552325",
      "name": "Guangming Wang",
      "affiliations": [
        "Ministry of Education of the People's Republic of China",
        "Ministry of Education"
      ]
    },
    {
      "id": "https://openalex.org/A3211688898",
      "name": "Chaokang Jiang",
      "affiliations": [
        "China University of Mining and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2098541740",
      "name": "Zhe Liu",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2111292751",
      "name": "Hesheng Wang",
      "affiliations": [
        "Ministry of Education of the People's Republic of China",
        "Ministry of Education"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2805521962",
    "https://openalex.org/W6662860747",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6679613971",
    "https://openalex.org/W6783568791",
    "https://openalex.org/W3174738881",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3034822742",
    "https://openalex.org/W6677174981",
    "https://openalex.org/W6793697131",
    "https://openalex.org/W2060475276",
    "https://openalex.org/W3118483371",
    "https://openalex.org/W6769983483",
    "https://openalex.org/W3024736934",
    "https://openalex.org/W4281551249",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2103420503",
    "https://openalex.org/W3101248344",
    "https://openalex.org/W6787910462",
    "https://openalex.org/W2950642167",
    "https://openalex.org/W2624503621",
    "https://openalex.org/W6809898271",
    "https://openalex.org/W3216314363",
    "https://openalex.org/W6817117944",
    "https://openalex.org/W2909908358",
    "https://openalex.org/W2414309118",
    "https://openalex.org/W3206776160",
    "https://openalex.org/W3201334943",
    "https://openalex.org/W3213294445",
    "https://openalex.org/W4285066052",
    "https://openalex.org/W3206941470",
    "https://openalex.org/W6802942239",
    "https://openalex.org/W3109537159",
    "https://openalex.org/W3007774126",
    "https://openalex.org/W2795802305",
    "https://openalex.org/W3170544306",
    "https://openalex.org/W4205412097",
    "https://openalex.org/W3212953023",
    "https://openalex.org/W6697304529",
    "https://openalex.org/W6694954142",
    "https://openalex.org/W4224116763",
    "https://openalex.org/W6788305448",
    "https://openalex.org/W6793609184",
    "https://openalex.org/W6736677999",
    "https://openalex.org/W6811284115",
    "https://openalex.org/W3045466720",
    "https://openalex.org/W3166470370",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W2130763585",
    "https://openalex.org/W2277848489",
    "https://openalex.org/W4297846601",
    "https://openalex.org/W4312638656",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3034906411",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W3108746553",
    "https://openalex.org/W2049981393",
    "https://openalex.org/W2296228853",
    "https://openalex.org/W3198819597",
    "https://openalex.org/W3167610791",
    "https://openalex.org/W3097717239",
    "https://openalex.org/W2115579991",
    "https://openalex.org/W3166975282",
    "https://openalex.org/W4312785900",
    "https://openalex.org/W3091498919",
    "https://openalex.org/W2983104849",
    "https://openalex.org/W4313007769",
    "https://openalex.org/W2934848969",
    "https://openalex.org/W4312030048",
    "https://openalex.org/W2609883120",
    "https://openalex.org/W4292402311",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W3118927214",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W3194973398",
    "https://openalex.org/W3035056458",
    "https://openalex.org/W4226322709",
    "https://openalex.org/W4313160444",
    "https://openalex.org/W4312878643"
  ],
  "abstract": "Recently, transformer architecture has gained great success in the computer vision community, such as image classification, object detection, etc. Nonetheless, its application for 3D vision remains to be explored, given that point cloud is inherently sparse, irregular, and unordered. Furthermore, existing point transformer frameworks usually feed raw point cloud of N×3 dimension into transformers, which limits the point processing scale because of their quadratic computational costs to the input size N. In this paper, we rethink the structure of point transformer. Instead of directly applying transformer to points, our network (TransLO) can process tens of thousands of points simultaneously by projecting points onto a 2D surface and then feeding them into a local transformer with linear complexity. Specifically, it is mainly composed of two components: Window-based Masked transformer with Self Attention (WMSA) to capture long-range dependencies; Masked Cross-Frame Attention (MCFA) to associate two frames and predict pose estimation. To deal with the sparsity issue of point cloud, we propose a binary mask to remove invalid and dynamic points. To our knowledge, this is the first transformer-based LiDAR odometry network. The experiment results on the KITTI odometry dataset show that our average rotation and translation RMSE achieves 0.500°/100m and 0.993% respectively. The performance of our network surpasses all recent learning-based methods and even outperforms LOAM on most evaluation sequences.Codes will be released on https://github.com/IRMVLab/TransLO.",
  "full_text": "TransLO: A Window-Based Masked Point Transformer Framework for\nLarge-Scale LiDAR Odometry\nJiuming Liu1*, Guangming Wang1*, Chaokang Jiang2, Zhe Liu3, Hesheng Wang1†\n1Department of Automation, Key Laboratory of System Control and Information Processing of Ministry of Education, Key\nLaboratory of Marine Intelligent Equipment and System of Ministry of Education, Shanghai Jiao Tong University\n2Engineering Research Center of Intelligent Control for Underground Space, Ministry of Education, School of Information\nand Control Engineering, Advanced Robotics Research Center, China University of Mining and Technology\n3MoE Key Lab of Artificial Intelligence, AI Institute, Shanghai Jiao Tong University, China\n{liujiuming,wangguangming,liuzhesjtu,wanghesheng}@sjtu.edu.cn\nts20060079a31@cumt.edu.cn\nAbstract\nRecently, transformer architecture has gained great success\nin the computer vision community, such as image classifi-\ncation, object detection, etc. Nonetheless, its application for\n3D vision remains to be explored, given that point cloud is\ninherently sparse, irregular, and unordered. Furthermore, ex-\nisting point transformer frameworks usually feed raw point\ncloud of N × 3 dimension into transformers, which limits\nthe point processing scale because of their quadratic compu-\ntational costs to the input size N. In this paper, we rethink\nthe structure of point transformer. Instead of directly apply-\ning transformer to points, our network (TransLO) can pro-\ncess tens of thousands of points simultaneously by projecting\npoints onto a 2D surface and then feeding them into a local\ntransformer with linear complexity. Specifically, it is mainly\ncomposed of two components: Window-based Masked trans-\nformer with Self Attention (WMSA) to capture long-range\ndependencies; Masked Cross-Frame Attention (MCFA) to\nassociate two frames and predict pose estimation. To deal\nwith the sparsity issue of point cloud, we propose a bi-\nnary mask to remove invalid and dynamic points. To our\nknowledge, this is the first transformer-based LiDAR odom-\netry network. The experiment results on the KITTI odom-\netry dataset show that our average rotational and transla-\ntional RMSE achieves 0.500 ◦/100m and 0.993 % respec-\ntively. The performance of our network surpasses all re-\ncent learning-based methods and even outperforms LOAM\non most evaluation sequences. Codes will be released on\nhttps://github.com/IRMVLab/TransLO.\nIntroduction\nLiDAR odometry plays a fundamental role in perceiving the\nenvironment for various applications, including Simultane-\nous Localization and Mapping (SLAM) system (Zhu et al.\n2022), robot navigation (Wang et al. 2018), autonomous\ndriving (Zheng and Zhu 2021), etc. As depicted in Fig. 1,\nthe target of this task is to estimate pose transformation be-\ntween two consecutive point cloud frames (Li et al. 2019).\n*These authors contributed equally.\n†Corresponding Author.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: The pipeline of our LiDAR odometry. We design a\nhierarchical transformer-based odometry network. Pose be-\ntween two consecutive point cloud frames is estimated by\nthe cross-attention mechanism and iterative refinement.\nTraditional methods treat LiDAR odometry as a scan reg-\nistration problem where the Iterative Closest Point algo-\nrithm (ICP) (Besl and McKay 1992) is generally employed\nto refine the transformation. Some feature-based methods\n(Zheng et al. 2020; Zhou et al. 2022) are also proposed\nusing point, line, or planar features. Since soft correspon-\ndences of point clouds can be learned by convolutional neu-\nral networks (CNNs) in an end-to-end manner, learning-\nbased methods have attracted increasing attention in recent\nyears. However, CNNs focus on the interrelation between lo-\ncal areas and enlarge the receptive field as the network deep-\nens. This will create a problem: CNN can’t capture relation-\nships between patches that have similar features but locate\nover great distances. To overcome this limitation, we further\nexplore transformer-based LiDAR odometry for long-range\ndependencies, based on our observation that global descrip-\ntors are helpful for correlating above similar patches.\nWe observe that some intrinsic qualities of transformer\nfit fairly well with the task: 1) Self-attention is naturally\ninvariant to permutation of input. Point cloud is irregular\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n1683\nand unordered, which means it also calls for a permutation-\ninvariant set operator. 2) As mentioned above, CNNs only\nfocus on local features while transformer architecture can\ncapture global features. This characteristic is important since\nsome dynamic objects and invalid points are not suitable for\nego-motion estimation. These outliers can be filtered out by\nthe global learning of feature maps. 3) Local transformer\nmethods reduce computational costs to linearity complexity,\nwhich enables the network to process large-scale 3D data.\nThe aforementioned three aspects support our research\nintuition to develop a transformer backbone for LiDAR\nodometry. Motivated by the motion information estimation\nmethod from soft correspondences in optical flow (Wang,\nRen, and Wang 2022) and scene flow tasks (Wang et al.\n2022a,b, 2021a), we directly estimate pose from conditioned\nfeatures after the cross attention. Overall, our contributions\nare as follows:\n• For the large-scale odometry task, we propose a novel\nhierarchical transformer-based network. We rethink the\nstructure of point transformer. Instead of original point\nclouds or their feature embeddings as input, we first\nproject point clouds on a cylindrical surface to acquire a\npseudo image. This procedure not only solves the irreg-\nularity problem of point clouds but also facilitates sub-\nsequent research since existing transformer methods for\nimages can be extended to the 3D vision community.\n• Window attention is applied (WMSA) in each feature ex-\ntraction layer consuming linear computational costs. Fur-\nthermore, a cross-frame transformer module (MCFA) is\ndesigned to associate two consecutive frames. We also\ndesign a projection-aware binary mask in both self and\ncross attention modules above to represent whether each\npixel-wise position is invalid in sparse feature maps.\n• Extensive experiments carried out on the KITTI odom-\netry dataset (Geiger et al. 2013) indicate our method\noutperforms all existing traditional and learning-based\nodometry methods. Our performance is even superior to\nLOAM with mapping on most sequences. To the best of\nour knowledge, this is the first time that transformer ar-\nchitecture is employed in the LiDAR odometry task.\nRelated Work\nTraditional LiDAR Odometry\nIterative Closest Point (ICP) (Besl and McKay 1992) is the\nmost common and popular method to align two point clouds,\nwhich calculates translation and rotation matrix by itera-\ntive updates until convergence. After that, two ICP vari-\nants (point-to-line ICP (Censi 2008) and point-to-plane ICP\n(Low 2004)) are proposed. As a renowned searching tree-\nbased method, LOAM (Zhang and Singh 2014) pioneered\na series of research. Edge and planar features are extracted\nrespectively to estimate motion transformation between two\npoint cloud frames. However, it doesn’t discern ground and\nnon-ground points which undermines accuracy and compu-\ntational cost. Some subsequent variants (Shan and Englot\n2018; Kuettel and Ferrari 2012) employ ground segmenta-\ntion or ground-constrained for improving their performance.\nDeep LiDAR Odometry\nAlthough deep learning has gained great progress in visual\nodometry (Li et al. 2020; Yang et al. 2020), deep LiDAR\nodometry is still underdeveloped. Nicolai et al. (Nicolai et al.\n2016) first investigate this field by projecting point clouds to\na 2D plane and applying convolution layers to them. How-\never, their experiment results are undesirable. DeepLO (Cho,\nKim, and Kim 2019) proposes the first unsupervised Li-\nDAR odometry framework. LO-Net (Li et al. 2019) presents\na deep convolution network for real-time odometry which\ncan implicitly exploit the sequential dependencies in the\ndata. Recently, many works are devoted to learning-based\nLiDAR odometry estimation in both supervised (Li et al.\n2021; Wang et al. 2022c) and unsupervised (Cho, Kim, and\nKim 2020; Wang et al. 2020) approaches. Among these net-\nworks, PWCLO-Net (Wang et al. 2021b) designs a hierar-\nchical embedding mask optimization pipeline to address 3D\nLiDAR odometry and achieves state-of-the-art performance.\nTransformer in Computer Vision\nTransformer (Vaswani et al. 2017) is proposed to tackle the\nmachine translation problem and now serves as a main-\nstream backbone in NLP. Inspired by its scaling successes\nin NLP, more researchers attempt to extend it to various\ncomputer vision tasks, such as semantic segmentation (Xie\net al. 2021; Liu et al. 2022; Zhang et al. 2022), object detec-\ntion (Carion et al. 2020; Yin et al. 2021), image classifica-\ntion (Dosovitskiy et al. 2021) etc. Nevertheless, transformer\nfor point cloud processing remains rarely studied, which are\nmostly designed for indoor or object-level tasks. Moreover,\nexisting works (Pan et al. 2021; Zhao et al. 2021; Guo et al.\n2021; Fischer et al. 2021) always feed raw points (for ex-\nample, 1024 points) into transformer. It blocks large-scale\napplications due to their quadratic complexity.\nTo achieve the linear complexity (Ren et al. 2022a; Dong\net al. 2022; Ren et al. 2022b), Swin Transformer (Liu et al.\n2021) partitions an image into a series of non-overlapped\nwindows and computes attention within each window. Its\nperformance surpasses previous pipelines by a large margin.\nInspired by Swin Transformer, we leverage window-based\ncross attention modules to associate two frames. Also, our\nwork is the first to leverage spatial shift on the point cloud\nassociation task. Intuitively, shift operation can capture more\nglobal features, which is significant for learning dynamics\nglobally and improving accuracy effectively.\nProposed Method\nOverall Architecture\nAn overview of our proposed methods is demonstrated in\nFig. 2. Given two consecutive frames of point clouds P C1\nand P C2, odometry aims to regress the ego-motion between\ntwo frames. To be specific, we first project point clouds onto\na cylindrical surface to get pseudo images. Then, several\nstride-based sampling layers coupled with Window-based\nMasked Self Attention (WMSA) are employed to encode\nfeature embeddings. Additionally, we enlarge the receptive\nfield through CNNs instead of patch merging methods in the\nvanilla Swin Transformer. To regress pose, a Masked Cross\n1684\nFigure 2: TransLO. Feature extraction of our TransLO is composed of both CNNs and transformer with Window-based Masked\nSelf Attention (WMSA). FFN is the abbreviation of a feed-forward network. Masked Cross Frame Attention (MCFA) block\nis employed to associate two frames and output initial embeddings in layer 2. Initial embeddings and features of Layer 3 are\ninput to MLP and Fully Connected (FC) layers for pose generation. This pose will warp features from upper layers through\nupsampling. MCFA and MLP layers are then employed again to refine the pose iteratively.\nFrame Attention (MCFA) module is designed. Even though\npoint clouds have an ordered representation after projec-\ntion, their sparsity issue is not intuitively solved. Thus, a\nprojection-aware mask indicating which coordinate has in-\nvalid points is added to both WMSA and WCFA. Finally,\nthe pose warping operation in (Wang et al. 2021b) is intro-\nduced for iterative pose refinement.\nCylindrical Projection and Feature Extraction\nCylindrical Projection: To obtain the corresponding 2D\nstructured grid-like coordinates, every raw 3D point cloud\nis projected as the following formulas:\nu = arctan2(y/x)/∆θ, (1)\nv = arcsin(z/\np\nx2 + y2 + z2)/∆ϕ, (2)\nin which x, y, zrepresent raw 3D coordinates of point clouds\nand ∆θ, ∆ϕ are horizontal and vertical resolutions of Li-\nDAR sensor. The output u, vare 2D corresponding coordi-\nnates. Previous works usually fill 2D coordinate positions\nwith depth or intensity values and then apply 2D CNNs. To\nmake the best use of raw geometric information, we directly\nfill raw 3D coordinates into their corresponding projected\n2D positions instead. This procedure outputs a tensor with\nthe size of H× W×3, where the last dimension means the\noriginal XY Zcoordinates of point clouds.\nEfficient Stride-based Sampling and Grouping: Clas-\nsic 3D points learning methods (Qi et al. 2017a,b) apply Far-\nthest Point Sampling (FPS) and KNN to sample and group\nfeatures, which is time-consuming. We propose a stride-\nbased sampling strategy as introduced in Fig. 4. A series of\nfixed-stride indexes are first generated as 2D kernel centers,\nand M neighbor points for each kernel are grouped and gath-\nered within a certain radius. This design is more effective\nthan random sampling because we can compute each kernel\noperation in parallel on CUDA. The formula is as follows:\nfG\ni = MAX\nm=1,2,···,M\n(MLP ((xm\ni − xi) ⊕ fm\ni ⊕ fi)), (3)\nwhere xi is the i-th sampled point (kernel center) and xm\ni\nrepresents its m−th neighbor point. fi and fm\ni are their fea-\ntures. fG\ni means the output feature. Operator ⊕ means con-\ncatenation of vectors. The feature extraction process above\nis adopted in every sampling layer coupled with the WMSA\nblock, which will be illustrated in the next section.\nWindow-based Masked Self Attention (WMSA)\nValidity Argument of Shifted Window: Inspired by Swin\nTransformer (Liu et al. 2021), our attention blocks retain\nwindow partition and shift operations, but also possess dif-\nferent characteristics. First of all, our LiDAR covers360◦ in\nthe horizontal direction but has a limited field vertically. This\nleads to strip-like feature maps where the width is rather\nlarger than the height. Thus, we need to reason whether win-\ndow partition and shift still work in advance.\nThe visualization result is displayed in Fig. 3, we set the\nwindow size as 4 and shift size as 2. The scale of the in-\nput feature map is 4×16. Upper figure in Fig. 3 shows how\ncyclic shift works toward the top-left direction. Then, shifted\nfeature map is divided into four windows. There are sev-\neral sub-windows in each partitioned window that are origi-\nnally far apart before shift, eg. B and C in window 3. In this\ncase, the masking process is used for preventing cross-sub-\nwindow attention computation in each window. As in Fig. 3,\n1685\nFigure 3: Cyclic shift and window partition. Upper figure\nshows how cyclic shift works and blue dotted lines indicate\nunshifted feature maps. Then, shifted feature map is divided\ninto four 4×4 windows. The masking process is utilized to\nprevent calculating attention among different sub-windows.\nwe visualize how masking process works, where tensors of\neach window are flattened and attention weights will be only\ncalculated within the same sub-window (colored yellow).\nBinary Mask and Window-based Point Transformer:\nOur proposed point transformer is built by replacing the\nstandard self-attention module with a window-based masked\nself-attention (WMSA) block as described in Fig. 4. Due\nto the inherent sparsity of point clouds, our projected fea-\nture maps are filled with many invalid pixels. Therefore, a\nprojection-aware binary mask is designed as:\nmask =\n\u001a0, X = 0, Y= 0, Z= 0\n1, otherwise . (4)\nNote that this binary mask is different from the aforemen-\ntioned masking mechanism for cutting the cross-subwindow\nconnection. This mask is introduced due to the sparsity char-\nacteristic of point clouds and pixel-corresponding to pro-\njected feature maps. Though it is simple, further studies car-\nried out in the experiment section prove its significance. Fol-\nlowing vanilla transformer architecture, one complete trans-\nformer stage in Fig. 2 can be described as:\nˆFl = W MSA(LN(Fl−1)) + Fl−1; (5)\nFl = MLP (LN( ˆFl)) + ˆFl; (6)\nˆFl+1 = sW MSA(LN(Fl)) + Fl; (7)\nFl+1 = MLP (LN( ˆFl+1)) + ˆFl+1; (8)\nwhere WMSA (sWMSA) represents (shifted) Window-\nbased Masked Self Attention. Fl−1 and Fl+1 are input\nand output features for stage l. MLP denotes the feed-\nforward network (FFN) with GELU non-linearity. Also,\nFigure 4: Window-based masked self attention (WMSA).\nOur attention block modifies the classic multi-head self-\nattention with shifted windows and a projection-aware bi-\nnary mask. ws means window size. After partitioning, ten-\nsors within each window (ws × ws) will be flattened to N.\nLayerNorm (LN) and residual connections are applied re-\nspectively before and after each module. For feature Fl =\b\nfk|fk ∈ Rd\tN\nk=1, WMSA (sWMSA) can be calculated as:\nW MSA(Fl) = (Head1 ⊕ ··· ⊕HeadH)WO, (9)\nHeadh = Attn(Fl × WQ\nh , Fl × WK\nh , Fl × WV\nh )\n= Attn(Qh, Kh, Vh) (10)\n= softmax( QhKh\n√dhead\n+ attn mask + Bias)V h,\nwhere Head1 ··· HeadH represent the output of H heads\nin Multi-head self-attention. WQ\nh ∈ Rd×dhead , WK\nh ∈\nRd×dhead , WV\nh ∈ Rd×dhead , WO ∈ RHdhead×d are learned\nprojected functions. Bias is the relative position encoding\noperation. And attn mask is the attention mask generated\nby the binary mask as illustrated in Fig. 4:\nattn mask =\n\u001a1 × e−10, where mask = 0\n0, otherwise . (11)\nMasked Cross Frame Attention (MCFA)\nThe masked cross-frame transformer is built for estimating\nego-motion in both the initial pose generation and pose re-\nfinement module. Similar to WMSA, shifted window and\nbinary masks are also used in our Masked Cross Frame At-\ntention (MCFA) as shown in Algorithm 1.\n1686\nTake the first frame for example, Fs\n1 and Ms\n1 are first\nfed into a WMSA module, which enables points to inter-\nact with other points in the same frame. Qs\n1, Ks\n1 , and V s\n1\nin WMSA are all from the same input frame through three\nindependent learnable projection matrices just like the for-\nmula (10). Then, output features of WMSA FI\n1 will at-\ntend to FI\n2 for ego-motion estimation. Specifically, FI\n1 is\nlinearly projected as query (QI\n1), FI\n2 is linearly projected\nas key (KI\n2 ) and value (V I\n2 ), and we calculate attention\nweights by entering them into the same (s)WMSA block.\nThis step enables the network to learn relative position trans-\nformation between two frames. Finally, we reverse window\npartition and shift and output transformed correlated fea-\ntures FLs\n1 , FLs\n2 for generating the initial motion embedding\nE1 = {ei|ei ∈ RC}N\ni=1 as illustrated in Fig. 2. Then, a\nweighting parameter is calculated as:\nW = softmax(MLP (E1 ⊕ F3\n1 )), (12)\nwhere W = {wi|wi ∈ RC}N\ni=1. The initial pose is then\ngenerated by:\nq3 =\nF C(\nnP\ni=1\nei ⊙ wi)\n|F C(\nnP\ni=1\nei ⊙ wi)|\n, (13)\nt3 = F C(\nnX\ni=1\nei ⊙ wi), (14)\nwhere F Cdenotes the fully connected layer. The quaternion\nq3 ∈ R4 and translation vector t3 ∈ R3 are refined from\ncoarse to fine as in (Wang et al. 2021b).\nLoss Function\nOur network outputs the poses from four layers and adopts\na multi-scale supervised loss. Ll = Ll\nt + λLl\nr denotes the\ntraining loss of the layer l in our network. Translation error\nLl\nt and rotation error Ll\nr can be calculated as:\nLl\nt = ∥tl\ngt − tl∥2, (15)\nLl\nr = ∥ql\ngt − ql\n∥ql∥∥2, (16)\nwhere ql, tl and ql\ngt, tl\ngt are the predicted and ground truth\nposes respectively.\nExperiments\nDataset and Implement Details\nWe evaluate our network performance on the KITTI odome-\ntry dataset which is widely used in pose estimation and point\ncloud registration tasks. We set the number of input points\nas 150000 and the initial feature map after cylindrical pro-\njection as 64 (H)×1792 (W). Note that this feature map size\nshould be divisible by window size 4. All training and eval-\nuation experiments are conducted on a single NVIDIA Ti-\ntan RTX GPU with PyTorch 1.10.1. The Adam optimizer is\nadopted with β1 = 0.9, β2 = 0.999. The initial learning rate\nis 0.001 and exponentially decays every 200000 steps until\n0.00001. The batch size is set as 8.\nAlgorithm 1: MCFA\nInput: Uncorrelated point cloud features Fs\n1 , Fs\n2 and their\ncorresponding masks Ms\n1 , Ms\n2 of two consecutive frames in\nstage s.\nParameter: Number of transformer blocks in stage s: Ls.\nOutput: Correlated point cloud features F1\nLs , F2\nLs .\n1: Let i = 0. (Parameter i is used for indicating whether\nthe cyclic shift is employed in WMSA.)\n2: while i <= Ls do\n3: Points attend to other points in the same frame.\nFI\n1 = W MSA(LN(Qs\n1, Ks\n1 , Vs\n1 ), Ms\n1 ) + Fs\n1 .\nFI\n2 = W MSA(LN(Qs\n2, Ks\n2 , Vs\n2 ), Ms\n2 ) + Fs\n2 .\n4: if i%2 == 0 then\n5: Points attend to the other frame points (without\nshift).\nFi\n1 = W MSA(LN(QI\n1, KI\n2 , VI\n2 ), Ms\n1 ) + FI\n1 .\nFi\n2 = W MSA(LN(QI\n2, KI\n1 , VI\n1 ), Ms\n2 ) + FI\n2 .\n6: else\n7: Points attend to the other frame points (with shift).\nFi\n1 = sW MSA(LN(QI\n1, KI\n2 , VI\n2 ), Ms\n1 ) + FI\n1 .\nFi\n2 = sW MSA(LN(QI\n2, KI\n1 , VI\n1 ), Ms\n2 ) + FI\n2 .\n8: end if\n9: i + = 1.\n10: end while\n11: return F1\nLs , F2\nLs\nComparison with The State-of-the-Art\nWe compare our network with the state-of-the-art on KITTI\ndataset, including both classic methods and learning-based\nones. Since existing training and testing sequence settings\nare inconsistent in different methods, we test and evaluate\nour framework accordingly for a fair comparison.\n00-06 as training sequences and 07-10 as testing se-\nquences. For classic odometry, we compare our perfor-\nmance with CLS (Velas, Spanel, and Herout 2016), GICP\n(Segal, Haehnel, and Thrun 2009), LOAM (Zhang and Singh\n2017; Shan and Englot 2018), SUMA (Behley and Stachniss\n2018), and (Vizzo et al. 2021). As demonstrated in Table 1,\nquantitative results illustrate the rotation RMSE ( ◦/100m)\nand translation error (%) of our network are extremely\nsmaller than theirs. Compared with full LOAM which dom-\ninates the KITTI odometry benchmark for years, our ap-\nproach outperforms it on most sequences. For learning-\nbased odometry SelfV oxeLO (Xu et al. 2020), LO-Net (Li\net al. 2019), SfMLearner (Zhou et al. 2017), and RSLO (Xu\net al. 2022), we observe that our odometry accuracy sur-\npasses all of theirs on most sequences. Even though our\nmethods don’t design an extra mask network, our perfor-\nmance is still superior to LO-Net (Li et al. 2019). Compared\nwith Xu et al. (Xu et al. 2022), our model performance out-\nperforms theirs without the need for a motion voting mecha-\nnism and two-stage estimation. For a fair comparison, we\nonly assess the odometry part of each network excluding\nmapping optimization.\n00-08 as training sequences and 09/10 as testing se-\nquences. We further compare our networks with three\nlearning-based methods, namely ConvLSTM (Zou et al.\n1687\n07† 08† 09† 10† Mean on\n07-10Method trel rr\nel trel rr\nel trel rr\nel trel rr\nel trel rr\nel\nICP-po2po 5.17 3.35 10.04 4.93 6.93 2.89 8.91 4.74 7.359 3.407\nICP-po2pl 1.55 1.42 4.42 2.14 3.95 1.71 6.13 2.60 4.735 1.932\nGICP 0.64 0.45 1.58 0.75 1.97 0.77 1.31 0.62 1.921 0.733\nCLS 1.04 0.73 2.14 1.05 1.95 0.92 3.46 1.28 2.148 0.995\nLOAM\nw/o mapping 10.87 6.76 12.72 5.77 8.10 4.30 12.67 8.79 10.820 5.426\nLOAM\nwith mapping 0.69 0.50 1.18 0.44 1.20 0.48 1.51 0.57 1.278 0.504\nLeGO-LOAM 1.12 0.81 1.99 0.94 1.97 0.98 2.21 0.92 2.49 1.00\nSuMa 1.75 1.17 2.53 0.96 1.92 0.78 1.81 0.97 2.93 0.92\nClassic\nVizzo\net al. 0.72 0.55 1.44 0.61 1.51 0.66 1.38 0.84 1.55 0.74\nSfMLearner 21.3 6.65 21.9 2.91 18.8 3.21 14.3 3.30 19.1 4.02\nLO-Net 1.70 0.89 2.12 0.77 1.37 0.58 1.80 0.93 1.330 0.688\nSelfV ox\neLO 2.51 1.15 2.65 1.00 2.86 1.17 3.22 1.26 2.81 1.15\nRSLO 2.37 1.15 2.14 0.92 2.61 1.05 2.33 0.94 2.36 1.02\nDL-basedOurs 0.55 0.43 1.29 0.50 0.95 0.46 1.18 0.61 0.993 0.500\nTable 1: Comparison with the state-of-the-art.trel, rrel indicate the average translation RMSE (%) and rotation RMSE (◦/100m)\nrespectively on all subsequences in the length of 100, 200, ...,800m. ‘†’ means the testing sequences. ‘NG’ means results are\nnot given. The best result for each sequence is bold, and the second best is underlined.\n09 10 Mean\nMethod trel rr\nel trel rr\nel trel rr\nel\nConvLSTM 3.49 1.00 5.81 1.80 4.650 1.400\nNubert\net al. 1.54 0.68 1.78 0.69 1.660 0.685\nSelf-VLO 2.58 1.13 2.67 1.28 2.620 1.210\nOurs 1.01 0.47 1.41 0.71 1.210 0.590\nTable 2: The LiDAR odometry results on sequences 09 and\n10 of KITTI odometry dataset.\n07 08 Mean\nMethod trel rr\nel trel rr\nel trel rr\nel\nLodoNet 1.86 1.64 2.04 0.97 1.950 1.305\nOurs 0.53 0.22 1.40 0.62 0.965 0.420\nTable 3: The LiDAR odometry results on sequences 07 and\n08 of KITTI odometry dataset.\n2020), (Nubert, Khattak, and Hutter 2021), and SelfVLO\n(Li et al. 2021). As illustrated in Table 2, both our trans-\nlation and rotation errors are smaller than theirs even with\nmapping. SelfVLO is self-supervised visual-LiDAR odom-\netry with flip consistency. Although our method utilizes no\nvisual information, we still achieve better performance.\n00-06/09/10 as training sequences and 07/08 as test-\ning sequences. LodoNet (Zheng et al. 2020) is a network\nwith 2d keypoints matching which also transfers the LiDAR\nframes to 2D space. As shown in Table 3, our accuracy is\nsuperior to LodoNet on both 07 and 08 sequences. Our trans-\nlation error is almost 1 (%) smaller than theirs. Also, we get\n0.885 (◦/100m) improvement with respect to rotation error.\nFigure 5: Trajectory of TransLO and ground truth. We visu-\nalize four 3D trajectory samples on KITTI dataset.\nAblation Study\nWe conduct ablation studies to assess the effectiveness of\nthree main components: projection-aware masks, Window-\nbased Masked Self Attention (WMSA), and Masked Cross\nFrame Attention (MCFA). We first compare our model with\nthe baseline (Wang et al. 2021b) in Table 4 (a), our average\nrotation errors are slightly larger (0.01 ◦/100m). However,\ntranslation errors are reduced by 10% of theirs. Moreover,\nour full network runtime has only a linear increase, which\nindicates the high efficiency of our TransLO.\nImportance of WMSA: Compared with CNNs, trans-\nformer focuses more on global features and this character-\nistic is significant to large-scale localization and navigation.\n1688\nFigure 6: Estimated trajectory of LOAM and ours on KITTI 07 sequence with ground truth. Our trajectory is even more accurate\nthan LOAM with mapping.\n07 08 09 10 Mean\nMethod trel rr\nel trel rr\nel trel rr\nel trel rr\nel trel rr\nel\nRuntime(/ms)\nBaseline (W\nang et al. 2021b) (a) 0.60 0.44 1.26 0.55 0.79 0.35 1.69 0.62 1.085 0.490 66.4\nOurs w/o\nmask and MCFA (b) 1.12 0.91 1.78 0.81 1.63 0.74 1.83 0.93 1.590 0.848 57.8\nOurs w/o\nMCFA (c) 0.80 0.54 1.25 0.58 1.11 0.54 1.47 0.78 1.158 0.610 58.0\nOurs w/o\nWMSA (d) 1.12 0.91 1.78 0.81 1.63 0.74 1.83 0.93 1.590 0.848 71.1\nOurs (e) 0.55 0.43 1.29 0.50 0.95 0.46 1.18 0.61 0.993 0.500 84.8\nTable 4: Ablation study for our proposed model. Notations: The best performance of each sequence is bold.\nOverall, the importance of WMSA can be seen when com-\nparing results in Table 4 (d) and (e).\nEffect of the binary Mask: As mentioned before, a bi-\nnary mask is generated simultaneously with the projected\nfeature map. It can filter outliers that are harmful to odom-\netry accuracy. Comparing Table 4 (b) and (c), the network\naccuracy descends when removing this mask.\nSignificance of MCFA: This module is used to establish\nsoft correspondences of two frames. By replacing it with the\ncost-volume-only methods in our baseline, both translation\nand rotation errors are larger as in Table 4 (c).\nVisualization\nWe visualize the trajectory of our network and the benefits\nof transformer-based methods in this section.\nTrajectory Visualization. We further visualize 3D tra-\njectory based on our estimated pose in Fig. 5. The figure\nshows our odometry can track the trajectory of the ground\ntruth fairly well. Also, we conduct experiments to compare\ntrajectory and estimation errors between LOAM and ours\nin Fig. 6. Although we do not have the mapping procedure,\nour odometry accuracy surpasses LOAM with mapping op-\ntimization as demonstrated in Fig. 6.\nWhy Transformer?Attention weights are also visualized\nas illustrated in Fig. 7. Compared with CNNs, our trans-\nformer can capture global learning of surroundings. For ex-\nample, trees are allocated similar attention weights even\nwith great distances. This characteristic is significant for\nlarge-scale 3D scene understanding as global features can be\ngathered without the distance constraint in CNN methods.\nFigure 7: Visualization of attention weights. Trees both near\nand far are allocated similar attention weights.\nConclusion\nIn this paper, we propose a novel end-to-end window-based\nmasked point transformer network for large-scale LiDAR\nodometry. We combine CNNs and transformers to extract\nmore global feature embeddings, which facilitate the out-\nliers’ rejection. We evaluate our framework on the KITTI\nodometry dataset. Experiment results show that our method\nachieves state-of-the-art performance.\nAcknowledgements\nThis work was supported in part by the Natural Sci-\nence Foundation of China under Grant 62073222, Grant\nU21A20480, and Grant U1913204; in part by the Sci-\nence and Technology Commission of Shanghai Municipal-\nity under Grant 21511101900; and in part by the Open\nResearch Projects of Zhejiang Laboratory under Grant\n2022NB0AB01. The authors gratefully appreciate the con-\ntribution of Xinrui Wu from Shanghai Jiao Tong University.\n1689\nReferences\nBehley, J.; and Stachniss, C. 2018. Efficient Surfel-Based\nSLAM using 3D Laser Range Data in Urban Environments.\nIn Robotics: Science and Systems, volume 2018, 59.\nBesl, P. J.; and McKay, N. D. 1992. Method for registration\nof 3-D shapes. In Sensor fusion IV: control paradigms and\ndata structures, volume 1611, 586–606. Spie.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European conference on computer vi-\nsion, 213–229. Springer.\nCensi, A. 2008. An ICP variant using a point-to-line metric.\nIn 2008 IEEE International Conference on Robotics and Au-\ntomation, 19–25. Ieee.\nCho, Y .; Kim, G.; and Kim, A. 2019. DeepLO: Geometry-\nAware Deep LiDAR Odometry. arXiv:1902.10562.\nCho, Y .; Kim, G.; and Kim, A. 2020. Unsupervised\ngeometry-aware deep lidar odometry. In 2020 IEEE Inter-\nnational Conference on Robotics and Automation (ICRA),\n2145–2152. IEEE.\nDong, X.; Bao, J.; Chen, D.; Zhang, W.; Yu, N.; Yuan, L.;\nChen, D.; and Guo, B. 2022. Cswin transformer: A general\nvision transformer backbone with cross-shaped windows. In\nProceedings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, 12124–12134.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representations.\nFischer, K.; Simon, M.; Olsner, F.; Milz, S.; Gross, H.-M.;\nand Mader, P. 2021. Stickypillars: Robust and efficient fea-\nture matching on point clouds using graph neural networks.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 313–323.\nGeiger, A.; Lenz, P.; Stiller, C.; and Urtasun, R. 2013. Vision\nmeets robotics: The kitti dataset. The International Journal\nof Robotics Research, 32(11): 1231–1237.\nGuo, M.-H.; Cai, J.-X.; Liu, Z.-N.; Mu, T.-J.; Martin, R. R.;\nand Hu, S.-M. 2021. Pct: Point cloud transformer. Compu-\ntational Visual Media, 7(2): 187–199.\nKuettel, D.; and Ferrari, V . 2012. Figure-ground segmenta-\ntion by transferring window masks. In 2012 IEEE Confer-\nence on Computer Vision and Pattern Recognition, 558–565.\nIEEE.\nLi, B.; Hu, M.; Wang, S.; Wang, L.; and Gong, X. 2021.\nSelf-supervised visual-LiDAR odometry with flip consis-\ntency. In Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision, 3844–3852.\nLi, Q.; Chen, S.; Wang, C.; Li, X.; Wen, C.; Cheng, M.; and\nLi, J. 2019. Lo-net: Deep real-time lidar odometry. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 8473–8482.\nLi, S.; Wang, X.; Cao, Y .; Xue, F.; Yan, Z.; and Zha, H. 2020.\nSelf-supervised deep visual odometry with online adapta-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 6339–6348.\nLiu, D.; Shetty, S.; Hinz, T.; Fisher, M.; Zhang, R.; Park, T.;\nand Kalogerakis, E. 2022. ASSET: autoregressive semantic\nscene editing with transformers at high resolutions. ACM\nTransactions on Graphics (TOG), 41(4): 1–12.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 10012–10022.\nLow, K.-L. 2004. Linear least-squares optimization for\npoint-to-plane icp surface registration. Chapel Hill, Univer-\nsity of North Carolina, 4(10): 1–3.\nNicolai, A.; Skeele, R.; Eriksen, C.; and Hollinger, G. A.\n2016. Deep learning for laser based odometry estimation.\nIn RSS workshop Limits and Potentials of Deep Learning in\nRobotics, volume 184, 1.\nNubert, J.; Khattak, S.; and Hutter, M. 2021. Self-supervised\nlearning of lidar odometry for robotic applications. In 2021\nIEEE International Conference on Robotics and Automation\n(ICRA), 9601–9607. IEEE.\nPan, X.; Xia, Z.; Song, S.; Li, L. E.; and Huang, G. 2021.\n3d object detection with pointformer. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 7463–7472.\nQi, C. R.; Su, H.; Mo, K.; and Guibas, L. J. 2017a. Pointnet:\nDeep learning on point sets for 3d classification and segmen-\ntation. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 652–660.\nQi, C. R.; Yi, L.; Su, H.; and Guibas, L. J. 2017b. Point-\nNet++: Deep Hierarchical Feature Learning on Point Sets in\na Metric Space. In Guyon, I.; Luxburg, U. V .; Bengio, S.;\nWallach, H.; Fergus, R.; Vishwanathan, S.; and Garnett, R.,\neds., Advances in Neural Information Processing Systems,\nvolume 30. Curran Associates, Inc.\nRen, P.; Li, C.; Wang, G.; Xiao, Y .; Du, Q.; Liang, X.; and\nChang, X. 2022a. Beyond Fixation: Dynamic Window Vi-\nsual Transformer. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 11987–\n11997.\nRen, S.; Zhou, D.; He, S.; Feng, J.; and Wang, X. 2022b.\nShunted Self-Attention via Multi-Scale Token Aggregation.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 10853–10862.\nSegal, A.; Haehnel, D.; and Thrun, S. 2009. Generalized-icp.\nIn Robotics: science and systems, volume 2, 435. Seattle,\nW A.\nShan, T.; and Englot, B. 2018. Lego-loam: Lightweight and\nground-optimized lidar odometry and mapping on variable\nterrain. In 2018 IEEE/RSJ International Conference on In-\ntelligent Robots and Systems (IROS), 4758–4765. IEEE.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.\n1690\nAttention is All you Need. In Guyon, I.; Luxburg, U. V .;\nBengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and\nGarnett, R., eds., Advances in Neural Information Process-\ning Systems, volume 30. Curran Associates, Inc.\nVelas, M.; Spanel, M.; and Herout, A. 2016. Collar line\nsegments for fast odometry estimation from velodyne point\nclouds. In 2016 IEEE International Conference on Robotics\nand Automation (ICRA), 4486–4495. IEEE.\nVizzo, I.; Chen, X.; Chebrolu, N.; Behley, J.; and Stachniss,\nC. 2021. Poisson surface reconstruction for LiDAR odom-\netry and mapping. In 2021 IEEE International Conference\non Robotics and Automation (ICRA), 5624–5630. IEEE.\nWang, G.; Hu, Y .; Wu, X.; and Wang, H. 2022a. Residual\n3-D Scene Flow Learning With Context-Aware Feature Ex-\ntraction. IEEE Transactions on Instrumentation and Mea-\nsurement, 71: 1–9.\nWang, G.; Jiang, C.; Shen, Z.; Miao, Y .; and Wang, H.\n2022b. SFGAN: Unsupervised Generative Adversarial\nLearning of 3D Scene Flow from the 3D Scene Self. Ad-\nvanced Intelligent Systems, 4(4): 2100197.\nWang, G.; Ren, S.; and Wang, H. 2022. Unsupervised learn-\ning of optical flow with non-occlusion from geometry.IEEE\nTransactions on Intelligent Transportation Systems.\nWang, G.; Tian, X.; Ding, R.; and Wang, H. 2021a. Unsuper-\nvised Learning of 3D Scene Flow from Monocular Camera.\nIn 2021 IEEE International Conference on Robotics and Au-\ntomation (ICRA), 4325–4331. IEEE.\nWang, G.; Wu, X.; Jiang, S.; Liu, Z.; and Wang, H. 2022c.\nEfficient 3d deep lidar odometry.IEEE Transactions on Pat-\ntern Analysis and Machine Intelligence.\nWang, G.; Wu, X.; Liu, Z.; and Wang, H. 2021b. Pwclo-\nnet: Deep lidar odometry in 3d point clouds using hierar-\nchical embedding mask optimization. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 15910–15919.\nWang, G.; Zhang, C.; Wang, H.; Wang, J.; Wang, Y .; and\nWang, X. 2020. Unsupervised learning of depth, optical flow\nand pose with occlusion from 3d geometry. IEEE Transac-\ntions on Intelligent Transportation Systems, 23(1): 308–320.\nWang, H.; Zheng, D.; Wang, J.; Chen, W.; and Yuan, J. 2018.\nEgo-motion estimation of a quadrotor based on nonlinear\nobserver. IEEE/ASME Transactions on Mechatronics, 23(3):\n1138–1147.\nXie, E.; Wang, W.; Yu, Z.; Anandkumar, A.; Alvarez, J. M.;\nand Luo, P. 2021. SegFormer: Simple and efficient design\nfor semantic segmentation with transformers. Advances in\nNeural Information Processing Systems, 34: 12077–12090.\nXu, Y .; Huang, Z.; Lin, K.-Y .; Zhu, X.; Shi, J.; Bao, H.;\nZhang, G.; and Li, H. 2020. SelfV oxeLO: Self-supervised\nLiDAR Odometry with V oxel-based Deep Neural Networks.\narXiv:2010.09343.\nXu, Y .; Lin, J.; Shi, J.; Zhang, G.; Wang, X.; and Li, H.\n2022. Robust self-supervised lidar odometry via represen-\ntative structure discovery and 3d inherent error modeling.\nIEEE Robotics and Automation Letters, 7(2): 1651–1658.\nYang, N.; Stumberg, L. v.; Wang, R.; and Cremers, D.\n2020. D3vo: Deep depth, deep pose and deep uncer-\ntainty for monocular visual odometry. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 1281–1292.\nYin, J.; Shen, J.; Gao, X.; Crandall, D.; and Yang, R. 2021.\nGraph neural network and spatiotemporal transformer atten-\ntion for 3D video object detection from point clouds. IEEE\nTransactions on Pattern Analysis and Machine Intelligence.\nZhang, J.; and Singh, S. 2014. LOAM: Lidar odometry and\nmapping in real-time. In Robotics: Science and Systems,\nvolume 2, 1–9. Berkeley, CA.\nZhang, J.; and Singh, S. 2017. Low-drift and real-time lidar\nodometry and mapping. Autonomous Robots, 41(2): 401–\n416.\nZhang, W.; Huang, Z.; Luo, G.; Chen, T.; Wang, X.; Liu,\nW.; Yu, G.; and Shen, C. 2022. TopFormer: Token Pyramid\nTransformer for Mobile Semantic Segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 12083–12093.\nZhao, H.; Jiang, L.; Jia, J.; Torr, P. H.; and Koltun, V . 2021.\nPoint transformer. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, 16259–16268.\nZheng, C.; Lyu, Y .; Li, M.; and Zhang, Z. 2020. Lodonet:\nA deep neural network with 2d keypoint matching for 3d\nlidar odometry estimation. In Proceedings of the 28th ACM\nInternational Conference on Multimedia, 2391–2399.\nZheng, X.; and Zhu, J. 2021. Efficient LiDAR odometry for\nautonomous driving. IEEE Robotics and Automation Let-\nters, 6(4): 8458–8465.\nZhou, L.; Huang, G.; Mao, Y .; Yu, J.; Wang, S.; and Kaess,\nM. 2022. PLC-LiSLAM: LiDAR SLAM with Planes, Lines\nand Cylinders. IEEE Robotics and Automation Letters.\nZhou, T.; Brown, M.; Snavely, N.; and Lowe, D. G. 2017.\nUnsupervised learning of depth and ego-motion from video.\nIn Proceedings of the IEEE conference on computer vision\nand pattern recognition, 1851–1858.\nZhu, Z.; Peng, S.; Larsson, V .; Xu, W.; Bao, H.; Cui, Z.;\nOswald, M. R.; and Pollefeys, M. 2022. Nice-slam: Neu-\nral implicit scalable encoding for slam. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 12786–12796.\nZou, Y .; Ji, P.; Tran, Q.-H.; Huang, J.-B.; and Chandraker,\nM. 2020. Learning monocular visual odometry via self-\nsupervised long-term modeling. In European Conference\non Computer Vision, 710–727. Springer.\n1691",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6630920171737671
    },
    {
      "name": "Point cloud",
      "score": 0.6464859247207642
    },
    {
      "name": "Transformer",
      "score": 0.6360650658607483
    },
    {
      "name": "Odometry",
      "score": 0.58094722032547
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5697619318962097
    },
    {
      "name": "Computer vision",
      "score": 0.48086482286453247
    },
    {
      "name": "Engineering",
      "score": 0.15784865617752075
    },
    {
      "name": "Mobile robot",
      "score": 0.10122132301330566
    },
    {
      "name": "Voltage",
      "score": 0.08809375762939453
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Robot",
      "score": 0.0
    }
  ]
}