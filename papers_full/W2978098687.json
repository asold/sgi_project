{
    "title": "The merits of Universal Language Model Fine-tuning for Small Datasets -- a case with Dutch book reviews",
    "url": "https://openalex.org/W2978098687",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5084972026",
            "name": "Benjamin van der Burgh",
            "affiliations": [
                "Leiden University"
            ]
        },
        {
            "id": "https://openalex.org/A5027124439",
            "name": "Suzan Verberne",
            "affiliations": [
                "Leiden University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2922565841",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2795247881",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2113459411",
        "https://openalex.org/W2945542139",
        "https://openalex.org/W2784121710",
        "https://openalex.org/W2787560479"
    ],
    "abstract": "We evaluated the effectiveness of using language models, that were pre-trained in one domain, as the basis for a classification model in another domain: Dutch book reviews. Pre-trained language models have opened up new possibilities for classification tasks with limited labelled data, because representation can be learned in an unsupervised fashion. In our experiments we have studied the effects of training set size (100-1600 items) on the prediction accuracy of a ULMFiT classifier, based on a language models that we pre-trained on the Dutch Wikipedia. We also compared ULMFiT to Support Vector Machines, which is traditionally considered suitable for small collections. We found that ULMFiT outperforms SVM for all training set sizes and that satisfactory results (~90%) can be achieved using training sets that can be manually annotated within a few hours. We deliver both our new benchmark collection of Dutch book reviews for sentiment classification as well as the pre-trained Dutch language model to the community.",
    "full_text": "The merits of Universal Language Model Fine-tuning for Small Datasets –\na case with Dutch book reviews\nBenjamin van der Burgh\nLeiden Institute of Advanced Computer Science\nLeiden University\nb.van.der.burgh@liacs.leidenuniv.nl\nSuzan Verberne\nLeiden Institute of Advanced Computer Science\nLeiden University\ns.verberne@liacs.leidenuniv.nl\nAbstract\nWe evaluated the effectiveness of using lan-\nguage models, that were pre-trained in one do-\nmain, as the basis for a classiﬁcation model\nin another domain: Dutch book reviews. Pre-\ntrained language models have opened up new\npossibilities for classiﬁcation tasks with lim-\nited labelled data, because representation can\nbe learned in an unsupervised fashion. In\nour experiments we have studied the effects of\ntraining set size (100–1600 items) on the pre-\ndiction accuracy of a ULMFiT classiﬁer, based\non a language models that we pre-trained on\nthe Dutch Wikipedia. We also compared\nULMFiT to Support Vector Machines, which\nis traditionally considered suitable for small\ncollections. We found that ULMFiT outper-\nforms SVM for all training set sizes and that\nsatisfactory results (~90%) can be achieved us-\ning training sets that can be manually anno-\ntated within a few hours. We deliver both our\nnew benchmark collection of Dutch book re-\nviews for sentiment classiﬁcation as well as\nthe pre-trained Dutch language model to the\ncommunity.\n1 Introduction\nTypically, results for supervised learning increase\nwith larger training set sizes. However, many real-\nworld text classiﬁcation tasks rely on relatively\nsmall data, especially for applications in speciﬁc\ndomains. Often, a large, unlabelled text collec-\ntion is available to use, but labelled examples re-\nquire human annotation. This is expensive and\ntime-consuming. Since deep and complex neural\narchitectures often require a large amount of la-\nbeled data, it has been difﬁcult to signiﬁcantly beat\nthe traditional models – such as Support Vector\nMachines – with neural models (Adhikari et al.,\n2019).\nIn 2018, a breakthrough was reached with the\nuse of pre-trained neural language models and\ntransfer learning (Howard and Ruder, 2018; Peters\net al., 2018; Devlin et al., 2018; Liu et al., 2019).\nTransfer learning no longer requires models to be\ntrained from scratch but allows researchers and de-\nvelopers to reuse features from models that were\ntrained on different, much larger text collections\n(e.g. Wikipedia). For this pre-training, no ex-\nplicit labels are needed; instead, the models are\ntrained to perform straightforward language mod-\nelling tasks, i.e. predicting words in the text.\nEven though the models are trained on these\nseemingly trivial predictive tasks, transfer learn-\ning with these models is highly effective: the pre-\ntrained language models can be ﬁne-tuned to per-\nform classiﬁcation tasks with a relatively small\namount of labelled task-speciﬁc data. Thus, pre-\ntrained language models can alleviate the small la-\nbelled data size for domain-speciﬁc data sets.\nIn their 2018 paper, Howard and Ruder show\nthe success of transfer learning with Universal\nLanguage Model Fine-tuning (ULMFiT) for six\ntext classiﬁcation tasks. They also demonstrate\nthat the model has a relatively small loss in accu-\nracy when reducing the number of training exam-\nples to as few as 100 (Howard and Ruder, 2018).\nIn this paper we further address the use of\nULMFiT for small training set sizes. We consider\nthe case of data from a new domain, where we\nhave a large amount of unlabelled data, but lim-\nited labelled data. Given the vast number of net-\nwork parameters and the limited number of train-\ning instances (100 to 1600), we expect to quickly\noverﬁt on the training data if all parameters are\noptimized using the small labelled data, often re-\nferred to as catastrophic forgetting. Alternatively,\nwe ‘freeze’ the parameters of the language model,\nwhich means that we ﬁx all network parameters\nexcept for the parameters of the ﬁnal layer. In do-\ning so, we limit the ability of the model to adapt\nto the target domain, but in return avoid the prob-\narXiv:1910.00896v1  [cs.IR]  2 Oct 2019\nlem of catastrophic forgetting, since the language\nmodel parameters are untouched.\nIn this paper, we evaluate ULMFiT with a pre-\ntrained language model and ﬁxed hyperparameters\nfor the representation layers. We only tune the\ndrop-out multiplier and learning rate for the linear\nlayers.\nFor our experiments on Dutch texts, we cre-\nated a new data collection consisting of Dutch-\nlanguage book reviews. We ﬁne-tune a general\npre-trained Wikipedia model on the reviews col-\nlection. We then take various-sized labelled por-\ntions of the book review data to (a) investigate\nthe effect of training set size, and (b) compare the\naccuracy of ULMFiT to the accuracy of Support\nVector Machines (SVM).\nThe contributions of this paper compared to pre-\nvious work are: (1) We deliver a new benchmark\ndataset for sentiment classiﬁcation in Dutch; (2)\nWe deliver pre-trained ULMFiT models for Dutch\nlanguage; (3) We show the merit of pre-trained\nlanguage models for small labeled datasets, com-\npared to traditional classiﬁcation models.\n2 Data\nData set We released the 110k Dutch Book Re-\nviews Dataset (110kDBRD). 1 This dataset con-\ntains book reviews along with associated binary\nsentiment polarity labels. It is inspired by the\nLarge Movie Review Dataset (Maas et al., 2011)\nand intended as a benchmark for sentiment classi-\nﬁcation in Dutch. We scraped 110 thousand book\nreviews from the website Hebban.2 These reviews\neach consist of a text and a score from 1 to 5,\nwhich we converted to categorical labels (1 and\n2: negative; 3: neutral; 4 and 5: positive).\nData split For our experiments, we split the\n110k documents as follows: 20k documents are\nused in the classiﬁer training and evaluation (posi-\ntive and negative classes balanced). Of those 20k,\nwe reserve 5k documents as a held-out test set that\ncannot be consulted during training nor language\nmodel pre-training. The remaining 15k documents\nare used for training the classiﬁer.\nData sampling for training For the experi-\nments on dataset size we use the following training\n1https://benjaminvdb.github.io/\n110kDBRD/, also including the scripts used to scrape\nthe data from the review website.\n2https://www.hebban.nl\nset sizes m = {100, 200, 400, 800, 1600}. Each\nexperiment is trained 10 times to investigate model\nstability. These 10 subsamples are chosen ran-\ndomly out of the complete 15k training set (not\nbalanced). Note that the same test set is used for\nall experiments to make the results directly com-\nparable.\n3 Language model training\n3.1 General-domain language model\npre-training\nIn order to learn text representations we use\nthe AWD-LSTM language modelling architecture\noriginally used by ULMFiT and implemented in\nthe Fast.ai Python library. This library also con-\nstructs a supervised dataset by randomly masking\nout words in a text. We use a unidirectional lan-\nguage model, i.e., the target word is predicted us-\ning words that precede it. Similarly to Howard\nand Ruder (2018), we have chosen Wikipedia for\nlanguage modelling, because it provides a large,\nfreely available corpus of high quality. Moreover,\nwe found that the pre-processing scripts for the\nEnglish version of Wikipedia could be re-used for\nthe Dutch.\nWe used a recent dump of Wikipedia and con-\nverted it to raw text, which was then split on white\nspaces into tokens. After that, we replaced all\nnumbers with the same placeholder token, such\nthat the speciﬁc value is ignored, but the fact that\na number occurred can be used in the model. The\n60k most frequent tokens were included in the vo-\ncabulary V and the remaining out-of-vocabulary\nwords were replaced with a special ‘unknown’ to-\nken. An embedding layer of size 400 was used to\nlearn a dense token representation, followed by 3\nLSTM layers with 1150 hidden units each to form\nthe encoder.\nThis is followed by a classiﬁcation module that\nmaps each representation to a score 0 ≤st ≤1,\nfor each token t ∈V where ∑st = 1 and can\nas such be interpreted as a probability distribution\nover the vocabulary. We used the reference im-\nplementation of ULMFiT in the Fast.ai Python li-\nbrary. The entire Wikipedia dataset was split into\n92M tokens for training and 185k for both testing\nand validating the language model. A slanted tri-\nangular learning rate (Howard and Ruder, 2018)\nwith a learning rate of 5 ∗10−3 was used for 20\nepochs.\n3.2 Target task language model ﬁne-tuning\nAfter training the language model on Wikipedia,\nwe continued training on data from our target do-\nmain, i.e., the 110k Dutch Book Review Dataset.\nThe preprocessing was done similarly to the pre-\nprocessing on Wikipedia, but the vocabulary of the\nprevious step was reused. We used all data except\nfor a 5k holdout set (105k reviews) to ﬁne-tune\nnetwork parameters using the same slanted trian-\ngular learning rates. However, this time we ﬁrst\ntrained the parameters of the classiﬁcation module\nto convert the pre-trained features into predictions\nfor the new target dataset. After that all network\nparameters were trained for 10 epochs.\n3.3 Target task classiﬁer ﬁne-tuning\nThe goal is to predict the sentiment polarity (pos-\nitive or negative) given a review text. Therefore,\nthe training dataset is constructed such that the\ndependent variable represents a sentiment polar-\nity instead of a token from the vocabulary. The\nencoder of the language model is kept, such that\na dense representation can be constructed given\nan input text, and the classiﬁcation module is re-\nplaced to adjust for the new target classes.\n4 Experiments\n4.1 Preprocessing\nWe applied the default text processing imple-\nmented in the Fast.ai Python library by splitting\non whitespace and padding texts within a batch to\nthe same length. The amount of required padding\ncharacters was reduced by grouping texts of sim-\nilar length together, while adding some random-\nness during training to avoid showing the network\nwith the same batches in each epoch.\n4.2 Hyperparameters\nWe optimized hyperparameters for each training\nset size and for each fold using HpBandster. 3 A\none-cycle policy, as outlined in (Smith, 2018), was\nused, which requires a lower and upper bound for\nthe momentum, describing its adaptive curve dur-\ning a single epoch. This resulted in ﬁve optimized\nhyperparameters: learning rate, momentum lower\nand upper, dropout and batch size. In the objective\nfunction, we optimized for binary cross-entropy\nloss.\n3https://github.com/automl/HpBandSter\n4.3 Baselines\nWe compared our classiﬁcation models to Lin-\near Support Vector Machines (SVM) because it\nis a commonly used and well performing classi-\nﬁer for small text collections. We used the im-\nplementation of LinearSVC in scikit-learn. 4 Lin-\nearSVC has one hyperparameter, C, which we op-\ntimized using HpBandster on the range of val-\nues from 10−4 to 104 with squared hinge loss as\noptimization function (default for LinearSVC in\nscikit-learn). For feature extraction we used the\nCountVectorizer and TF-IDF transformer in scikit-\nlearn. TF-IDF weights were trained on the same\n105k documents on which the ULMFiT model\nwas ﬁne-tuned.\nFor comparison we also trained two models,\none SVM and one ULMFiT model, with manu-\nally tuned hyperparameters on all available book\nreviews in the training set (15k). These models\nachieved 93.84% (ULMFiT) and 89.16% (SVM).\n5 Results\n5.1 Effect of training set size\nThe results of the experiments described in Sec-\ntion 4 can be found in Figure 1 (left). A few ob-\nservations can be made from this plot. Firstly, for\nthe ULMFiT model, the accuracy on the test set\nimproves with each increase in the training dataset\nsize, as can be expected.\nSecondly, both models behave rather unstable\nfor smaller training datasets, as can be seen by the\nlarge deviation from the mean and the outliers: dif-\nferent random subsamples give deviant results for\nthe smaller training set sizes. Since the pre-trained\nmodel is based on data from a different domain, it\ncan be expected that more than 100 instances are\nneeded to accommodate for the new domain.\n5.2 Comparison to SVM\nFigure 1 compares the prediction accuracies for\nULMFiT and SVM. We had expected the SVM\nmodel to perform better for smaller training set\nsizes, but it is outperformed by ULMFiT for each\nsize. Also, the ULMFiT models show smaller\ndeviations between random subsamples than the\nSVM models.\nWe also found that the prediction accuracy of\nthe SVM model using all 15,000 training items\n4https://scikit-learn.org/stable/\nmodules/generated/sklearn.svm.LinearSVC.\nhtml\n100 200 400 800 1600\nSize\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90Accuracy\nULMFiT\n100 200 400 800 1600\nSize\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90Accuracy\nSVM\nFigure 1: Results for ULMFiT (a) and SVM (b) in terms of accuracy on the test set with varying training set sizes.\nThe boxes represent the deviation among the random subsample per training set size.\n(89.16%) is surpassed by the ULMFiT model\nwhen using only 1600 training instances: all 10\nrandom subsamples for ULMFiT reach an accu-\nracy of at least 89.54% (the left purple box in Fig-\nure 1). This could mean that the pre-trained model\ncaptures many of the required characteristics of\nDutch such that they can be largely used without\nmodiﬁcations.\n6 Conclusions\nPre-trained language models have opened up pos-\nsibilities for classiﬁcation tasks with limited la-\nbelled data. In our experiments we have studied\nthe effects of training set size on the prediction\naccuracy of a ULMFiT classiﬁer based on pre-\ntrained language models for Dutch. In order to\nmake a fair comparison, we have used state-of-the-\nart optimization methods to optimize the hyperpa-\nrameters of each model.\nOur results conﬁrm what had been stated in\n(Howard and Ruder, 2018), but had not been veri-\nﬁed for Dutch or in as much detail. For this partic-\nular dataset and depending on the requirements of\nthe model, satisfactory results might be achieved\nusing training sets that can be manually annotated\nwithin a few hours.\nMoreover, a large part of modeling effort lies\nin the training of a language model on an – in\nthis case – generic corpus, which can be reused\nfor other domains. While the prediction accuracy\ncould be improved by optimizing all network pa-\nrameters on a large dataset, we have shown that\ntraining only the weights of the ﬁnal layer outper-\nforms our SVM models by a large margin.\nULMFiT uses a relatively simple architecture\nthat can be trained on moderately powerful GPUs.\nThis fact, combined with the general availability\nof unlabeled data and the ability to share language\nmodels, suggests that these methods could be ap-\nplied in domains where manual labelling has tra-\nditionally been too expensive. Further research\nshould be conducted to compare how differences\nbetween the source and target datasets affect the\nprediction accuracy and whether more powerful\nnetwork architectures can also be used.\nReferences\nAshutosh Adhikari, Achyudh Ram, Raphael Tang, and\nJimmy Lin. 2019. Rethinking complex neural net-\nwork architectures for document classiﬁcation. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages\n4046–4051, Minneapolis, Minnesota. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation.\narXiv preprint arXiv:1801.06146.\nNelson F Liu, Matt Gardner, Yonatan Belinkov,\nMatthew Peters, and Noah A Smith. 2019. Linguis-\ntic knowledge and transferability of contextual rep-\nresentations. arXiv preprint arXiv:1903.08855.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. arXiv preprint arXiv:1802.05365.\nLeslie N. Smith. 2018. A disciplined approach to neu-\nral network hyper-parameters: Part 1 – learning rate,\nbatch size, momentum, and weight decay. CoRR,\nabs/1803.09820."
}