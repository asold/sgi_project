{
  "title": "VLTinT: Visual-Linguistic Transformer-in-Transformer for Coherent Video Paragraph Captioning",
  "url": "https://openalex.org/W4382240304",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3107137670",
      "name": "Kashu Yamazaki",
      "affiliations": [
        "University of Arkansas at Fayetteville"
      ]
    },
    {
      "id": "https://openalex.org/A2102996568",
      "name": "Khoa Vo",
      "affiliations": [
        "University of Arkansas at Fayetteville"
      ]
    },
    {
      "id": "https://openalex.org/A3192167951",
      "name": "Quang Sang Truong",
      "affiliations": [
        "University of Arkansas at Fayetteville"
      ]
    },
    {
      "id": "https://openalex.org/A2166106843",
      "name": "Bhiksha Raj",
      "affiliations": [
        "Carnegie Mellon University",
        "Zayed University"
      ]
    },
    {
      "id": "https://openalex.org/A2336602421",
      "name": "Ngan Le",
      "affiliations": [
        "University of Arkansas at Fayetteville"
      ]
    },
    {
      "id": "https://openalex.org/A3107137670",
      "name": "Kashu Yamazaki",
      "affiliations": [
        "University of Arkansas at Fayetteville"
      ]
    },
    {
      "id": "https://openalex.org/A2102996568",
      "name": "Khoa Vo",
      "affiliations": [
        "University of Arkansas at Fayetteville"
      ]
    },
    {
      "id": "https://openalex.org/A3192167951",
      "name": "Quang Sang Truong",
      "affiliations": [
        "University of Arkansas at Fayetteville"
      ]
    },
    {
      "id": "https://openalex.org/A2166106843",
      "name": "Bhiksha Raj",
      "affiliations": [
        "Carnegie Mellon University",
        "Ider University"
      ]
    },
    {
      "id": "https://openalex.org/A2336602421",
      "name": "Ngan Le",
      "affiliations": [
        "University of Arkansas at Fayetteville"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3005680577",
    "https://openalex.org/W3020257313",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W3174257385",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3094751268",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W6735463952",
    "https://openalex.org/W6802307560",
    "https://openalex.org/W6775036685",
    "https://openalex.org/W1983364832",
    "https://openalex.org/W6955071965",
    "https://openalex.org/W6737009557",
    "https://openalex.org/W3025136821",
    "https://openalex.org/W2799042952",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W6783870023",
    "https://openalex.org/W6760490472",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2904658088",
    "https://openalex.org/W1815076433",
    "https://openalex.org/W2609138599",
    "https://openalex.org/W3140373187",
    "https://openalex.org/W2795782778",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2973802306",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2951098185",
    "https://openalex.org/W3166366124",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W3210264524",
    "https://openalex.org/W4303648796",
    "https://openalex.org/W3200482744",
    "https://openalex.org/W3160784302",
    "https://openalex.org/W6799914745",
    "https://openalex.org/W3047922786",
    "https://openalex.org/W2925419377",
    "https://openalex.org/W2798991696",
    "https://openalex.org/W2883910824",
    "https://openalex.org/W4283711164",
    "https://openalex.org/W4283809036",
    "https://openalex.org/W6757541033",
    "https://openalex.org/W2784025607",
    "https://openalex.org/W6750401302",
    "https://openalex.org/W4214926101",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2964065937",
    "https://openalex.org/W2968104955",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W4214663214",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2963811641",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2963466731",
    "https://openalex.org/W4235453268",
    "https://openalex.org/W2968101724",
    "https://openalex.org/W3035237998",
    "https://openalex.org/W2985144848",
    "https://openalex.org/W4312535570",
    "https://openalex.org/W4308234126",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4323521100",
    "https://openalex.org/W2963753226",
    "https://openalex.org/W2989322838",
    "https://openalex.org/W3203851412",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W4307730507",
    "https://openalex.org/W2952132648",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2963916161",
    "https://openalex.org/W3034815696",
    "https://openalex.org/W2604178507"
  ],
  "abstract": "Video Paragraph Captioning aims to generate a multi-sentence description of an untrimmed video with multiple temporal event locations in a coherent storytelling. Following the human perception process, where the scene is effectively understood by decomposing it into visual (e.g. human, animal) and non-visual components (e.g. action, relations) under the mutual influence of vision and language, we first propose a visual-linguistic (VL) feature. In the proposed VL feature, the scene is modeled by three modalities including (i) a global visual environment; (ii) local visual main agents; (iii) linguistic scene elements. We then introduce an autoregressive Transformer-in-Transformer (TinT) to simultaneously capture the semantic coherence of intra- and inter-event contents within a video. Finally, we present a new VL contrastive loss function to guarantee the learnt embedding features are consistent with the captions semantics. Comprehensive experiments and extensive ablation studies on the ActivityNet Captions and YouCookII datasets show that the proposed Visual-Linguistic Transformer-in-Transform (VLTinT) outperforms previous state-of-the-art methods in terms of accuracy and diversity. The source code is made publicly available at: https://github.com/UARK-AICV/VLTinT.",
  "full_text": "VLTinT: Visual-Linguistic Transformer-in-Transformer\nfor Coherent Video Paragraph Captioning\nKashu Yamazaki*1, Khoa Vo*1, Quang Sang Truong1, Bhiksha Raj2, 3, Ngan Le1\n1 AICV Lab, University of Arkansas, Fayetteville, Arkansas, USA\n2 Carnegie Mellon University, Pittsburgh, Pennsylvania, USA\n3 Mohammed bin Zayed University of AI\n{ kyamazak, khoavoho, sangt, thile}@uark.edu, bhiksha@cs.cmu.edu\nAbstract\nVideo Paragraph Captioning aims to generate a multi-sentence\ndescription of an untrimmed video with multiple temporal\nevent locations in a coherent storytelling. Following the human\nperception process, where the scene is effectively understood\nby decomposing it into visual (e.g. human, animal) and non-\nvisual components (e.g. action, relations) under the mutual\ninfluence of vision and language, we first propose a visual-\nlinguistic (VL) feature. In the proposed VL feature, the scene\nis modeled by three modalities including (i) a global visual\nenvironment; (ii) local visual main agents; (iii) linguistic scene\nelements. We then introduce an autoregressiveTransformer-\nin-Transformer (TinT) to simultaneously capture the semantic\ncoherence of intra- and inter-event contents within a video.\nFinally, we present a new VL contrastive loss function to\nguarantee the learnt embedding features are consistent with the\ncaptions semantics. Comprehensive experiments and extensive\nablation studies on the ActivityNet Captions and YouCookII\ndatasets show that the proposed Visual-Linguistic Transformer-\nin-Transform (VLTinT) outperforms previous state-of-the-art\nmethods in terms of accuracy and diversity. The source code is\nmade publicly available at: https://github.com/UARK-AICV/\nVLTinT.\nIntroduction\nVideo captioning is the task of automatically generating a\ncaption for a video. An important branch of video captioning\nis dense video captioning (DVC) (Krishna et al. 2017), which\nrequires generating a list of temporal event proposals and the\nassociated sentence description of each event to form a coher-\nent paragraph description of a video. As a simplified version\nof DVC, video paragraph captioning (VPC) (Park et al. 2019)\nfocuses on generating better paragraph captions given a set\nof event segments in a video, which eases the requirement of\nevent proposal generation. In general, a VPC model consists\nof two main components: an encoder to represent each event\nsegment as a feature; and a decoder to generate captions\nwhile maintaining the consistency within each event and the\ncoherence among all sentences of the generated paragraph.\nVideos contain rich semantic knowledge of multiple modal-\nities, e.g., vision, text, speech, and non-speech audio. Under-\nstanding a video involves multiple factors such as a single\n*These authors contributed equally.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nCurrent Event\nPrevious Event\nMen are playing \nvolleyball on a court.\nThe crowd is cheering \naround them.\nTrans \nDecoder\nTrans \nDecoder\nInteraction\nMan, Crowd,\nStadium\nTrans \nDecoder\nTrans. XLMARTVLTinT (ours)\nIntra-event Connection Inter-event Connection\nGlobal Env\nRelevant Lang \nhidden states\nRNN\nhidden states\nhidden states\nHAM \nTrans\nEvent Memory\nGlobal Env\nGlobal Env\n3D CNN\n3D CNN\nLocal Agent\nFigure 1: A high-level comparison between our VLTinT and\nrecent SOTA VPC methods. In the encoder, both Transformer-\nXL (Dai et al. 2019) and MART (Lei et al. 2020) encode\nvisual features by applying 3D CNN-based backbone network\nwhereas our VLTinT encodes visual-linguistic feature by (i)\nglobal visual environment, (ii) local visual main agents, (iii)\nlinguistic scene elements, and a fusion mechanism. In the\ndecoder, Transformer-XL uses recurrence to address context\nfragmentation, MART uses a highly summarized memory to\nremember history information whereas we propose to utilize\na transformer to model the contextual dependencies at both\nintra- and inter-levels.\nhuman actor, group of human actors, non-human actor, and\nphenomenon (V o et al. 2021b; V o-Ho et al. 2021; Hutchinson\nand Gadepally 2021; V o et al. 2023). Examples of non-human\nactors and phenomena performing action include dog chasing,\ncar running, and cloud floating. The existing VPC approaches\n(Zhou et al. 2018; Dai et al. 2019; Lei et al. 2020) employ\nCNN-based networks as a black-box to encode the video\nfeature, which could overlook the contributions of various\nmodalities in the semantic contents of a video. We observe\nthat human perception involves the interaction of vision and\nlanguage and propose VL Encoder to resolve this above\nchallenge. Our VL Encoder is based on two observations:\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n3081\nlanguage influences the basic perceptual process, affecting\nperformance on tasks that might seem to be wholly percep-\ntual in nature (Lupyan et al. 2020); and video content is\neffectively understood by the combination of agents/actors\nand the surrounding environment (V o-Ho et al. 2021; V o et al.\n2021b,a, 2022, 2023).\nOur VL Encoder consists of four modalities: (i) global vi-\nsual environment representing the overall surrounding scene,\n(ii) local visual main agents representing the human agents\ncommitting events, and (iii) linguistic scene elements caption-\ning descriptive details of both visual and non-visual elements,\nand (iv) a fusion module modeling the interaction of those fea-\ntures and combine them into a unified representation. Besides,\nto only focus on the main agents who actually contribute to\nthe event as well as the most relevant scene elements of the\nevent, we make use of a Hybrid Attention Mechanism (HAM)\nfollowing (V o et al. 2021a, 2022, 2023).\nIn VPC, each event is described by one sentence, and they\nall should logically follow each other. Thus, two kinds of\ndependencies have to be modeled in VPC, i.e., intra- and\ninter-event dependencies. In the early days of development,\nRNN-based models were applied to build the caption gen-\nerator to model intra-event coherency (Xiong, Dai, and Lin\n2018; Park et al. 2019). Recently, Transformer-based models\nhave proven to be more effective in generating captions (Dai\net al. 2019; Lei et al. 2020; Ging et al. 2020; Yamazaki et al.\n2022). However, in (Zhou et al. 2018), each event is decoded\nindependently and inter-event coherency is not taken into\naccount. This limitation is later addressed as a context frag-\nmentation (Dai et al. 2019) and RNN-based memory (Lei\net al. 2020; Ging et al. 2020; Yamazaki et al. 2022). How-\never, none of the existing work leverages the success of the\ntransformer in modeling inter-event coherence. To pose this\nchallenge, we propose a novel Transformer-in-Transformer\narchitecture (TinT Decoder). To the best of our knowledge,\nTinT Decoder is the first fully transformer network for VPC,\nwhich simultaneously models both intra- and inter-event in\nan end-to-end framework. The network comparison between\nour VLTinT and the existing SOTA VPC approaches is in\nFig. 1. Furthermore, most prior VPC work makes use of max-\nimum likelihood estimation (MLE) loss to train the model.\nHowever, MLE loss does not guarantee that the learnt event\nembedding features intimately represent the groundtruth cap-\ntions. Thus, we introduce a novel VL contrastive loss, to\nmaintain the learning of both visual and linguistic seman-\ntics during training without adding additional computational\ncosts. The VL Encoder along with TinT Decoder comprises\na novel method, termed Visual-Linguistic Transformer-in-\nTransformer (VLTinT). The main contributions of this paper\nare summarized as follows:\n• A novel VL Encoder, which represents the video content\nby separately modeling (i) global visual feature, (ii) local\nvisual main agents, and (iii) linguistic scene elements; and\ntheir interactions.\n• A novel TinT Decoder to simultaneously model intra-\nand inter-event dependencies in an end-to-end fashion\nproducing a coherent paragraph.\n• A novel VL contrastive loss function to better align both\nvisual and linguistic information.\nRelated Works\nDense Video Captioning\nIn general, video captioning can be divided into either single\nsentence (Pasunuru and Bansal 2017; Wang et al. 2019) for\nshort videos or multiple sentences (Wang et al. 2021b) for\nlong and untrimmed videos. DVC belongs to the second cate-\ngory and it has emerged as a multitask problem that combines\nevent localization and event captioning to generate an infor-\nmative caption for such videos. DVC can be implemented by\nvisual feature only (Krishna et al. 2017; Li et al. 2018; Zhou\net al. 2018; Mun et al. 2019; Deng et al. 2021) or multimodal\nfeatures such as audio (Rahman, Xu, and Sigal 2019), speech\n(Shi et al. 2019; Iashin and Rahtu 2020), and both (Iashin and\nRahtu 2020). Our VPC method shares a common setup with\nDVC with the multimodal feature. Our feature is encoded\nusing both vision and language modalities to better extract\ncontextual scene representation.\nVideo Paragraph Captioning\n(Zhou et al. 2018) first introduced the transformer to the\nVPC task known as Vanilla Transformer, where each event\nis decoded individually without knowing the coherence be-\ntween sentences. To address this limitation, (Lei et al. 2020)\nmodified the Transformer-XL (Dai et al. 2019) and proposed\nMART. MART decodes the caption to learn word-level de-\npendencies by a transformer while modeling the paragraph\ncoherence based on GRU (Chung et al. 2014). Different from\nthe existing VPC methods which utilize pre-trained backbone\nnetworks to extract feature, (Yamazaki et al. 2022) inherits\nthe merits of both vision and language models and proposed\nVLCap. However, all previous works are RNN-based and lim-\nited in capturing long-range dependencies as well as suffers\nfrom the problem of gradient vanishing (Pascanu, Mikolov,\nand Bengio 2013). In this work, we leverage a transformer to\nsimultaneously model the long-range dependencies between\nwords (i.e., intra-event) and sentences (i.e., inter-event).\nTransformer Models\nTransformer (Vaswani et al. 2017) and Vision Transformer\n(ViT) (Dosovitskiy et al. 2021) have recently attracted sig-\nnificant interest in the research community. ViT applies a\npure transformer to the visual recognition task by treat-\ning the image as a composition of\n16 × 16 local patches.\n(Han et al. 2021) presents TNT to further divide them into\nsmaller 4 × 4 patches. Specifically, TNT is built with a non-\nautoregressive inner and outer transformer, which deal with\nlocal sub-patches and sequence of local sub-patches, respec-\ntively. NesT (Zhang et al. 2022) proposes an alternative ap-\nproach to model local and global information by nesting the\ncanonical transformers hierarchically and connecting them\nwith a proposed aggregation function. To model temporal\ncoherency of intra- and inter-event, we propose a novel TinT.\nIn our TinT, the outer transformer is designed as an autore-\ngressive structure to model inter-event coherency whereas\nthe inner transformer handles intra-event coherency.\n3082\nLinguistic scene\nelements\nC3D\nText\nEncoder \nDataset \nVocabulary \nGlobal visual  \nenvironment \nHuman\nDetector\nLocal visual agent\nRoI\nAlign\nHAM\nM2RF\nVision\nEncoder \nHAM\nMulti-modal\nrepresentation fusion\nathlete, \nstanding, \n... \ntrack,  \nuniform\nSnippet\n \nAvg. \nPooling\nMasked Multihead  \nSelf Attention\nChannel MLP\nChannel MLP\nLinear &\nSoftmax\nHAM\nstack\nGT caption \n(shifted right)\ncaption GT caption\nGT caption\nText\nTransformer \n \nGT caption \nembeddings\nEvent  \nEmbeddings\nlatent feature\n+\n+\n+\nSentence \nEmbeddings\nTinT DecoderVL Encoder\nconcat\nMultihead  \nSelf Attention\nEmbedding\nconcat\nevent memory\nouter transinner trans\nFigure 2: Overall network architecture of our proposed VLTinT, which contains two modules, i.e., VL Encoder and TinT\nDecoder. (Left) VL Encoder: given a snippet Xi, the VL Encoder simultaneously extracts local visual features from main agents,\nglobal visual features from the environment, and linguistic relevant scene elements; and models interaction between those three\nmodalities through our M2RF module. (Right) TinT Decoder: the canonical transformer encoder is extended by an autoregressive\nouter transformer that can selectively access the 1st to t − 1th hidden states, which are stored in the event memory, at thetth\nevent captioning step.\nProposed VLTinT\nOur VLTinT consists of two main modules corresponding\nto VL Encoder and TinT Decoder. The VL Encoder aims to\nextract VL representation of each event and the TinT Decoder\naims to generate a caption of each event while simultaneously\nmodeling intra- and inter-event coherency. Both modules are\ntrained in an end-to-end fashion by our proposed VL loss.\nThe over architecture is shown in Fig. 2.\nProblem Setup\nIn VPC, we are given an untrimmed video V = {vi}|V|\ni=1,\nwhere |V| is the number of frames, and a list of its important\nevents E = {ei = (eb\ni , ee\ni )}|E|\ni=1\n, where |E| is the number of\nevents within a video and an event ei is defined by a pair of\nbeginning and ending timestamps (eb\ni , ee\ni )\n. Our objective is to\ngenerate a coherent paragraph that matches the ground truth\nparagraph P = {si}|E|\ni=1 that describes the whole video V. In\nthis setup, ith sentence s = {s1 . . . sN } that consists of N\nwords is the description of its corresponding event ei.\nVisual-Linguistic (VL) Encoder\nOur VL Encoder is responsible for comprehensively repre-\nsenting each snippet Xi of an event into a representative\nfeature to compose a sequence of snippet features for the\ndecoder. Given an event\ne = (eb, ee) and its corresponding\nvideo frames Ve = {vi|eb ≤ i ≤ ee}, we follow the standard\nsettings from existing works (Zhou et al. 2018; Lei et al. 2020;\nSong, Chen, and Jin 2021) and divide Ve into a sequence of\nδ-frame snippets {Xi}L\ni=1. Each snippet Xi consists of δ\nconsecutive frames and Ve has a total of L =\n\u0006|Ve|\nδ\n\u0007\nsnippets.\nThe VL Encoder module encodes each snippet Xi to a VL\nrepresentation fV L\ni as shown in Fig.2 (left). Therefore, video\nsegment Ve is encoded into VL representation {fV L\ni }L\ni=1.\nThe VL Encoder first models a video with the three modali-\nties, (i) global visual environment (ii) local visual main agents\n(iii) linguistic relevant scene elements, and then fuses them\ninto one representation based on the interactions between\nthem. Given a snippetXi, it is encoded into these three modal-\nities, corresponding to fe\ni , fa\ni and fl\ni , respectively. The final\nfeature fV L\ni representing the interaction is extracted by fus-\ning fe\ni , fa\ni and fl\ni through our Multi-modal Representation\nFusion (M2RF) module as follows:\n(i) Global Visual Environment:\nThis modality provides the visual semantic information\nfrom the entire spatial scene of input snippet Xi. To obtain\nsuch target, we adopt a backbone 3D-CNN network (Ji et al.\n2013) to Xi to extract feature mapHi at the last convolutional\nblock of the network. Then, we obtain the global environment\nvisual feature fe\ni ∈ Rdemb by processing Hi with an average\npooling operation to reduce the entire spatial dimension fol-\nlowed by channel MLP. The procedure is summarized as\nfollows:\nfe\ni = MLPθe(Avg.Pooling(Hi)) (1)\n(ii) Local Visual Main Agents:\nThis modality provides the visual features of the main\nhuman agents, who actually contribute to the formation of\nthe event being described. Even though most of the events are\nassociated with agents, not all agents committing movements\nare related to the main content of the event segment. Using a\nsimilar assumption as in (V o-Ho et al. 2021; V o et al. 2021b),\nwe apply a human detector to the center frame of Xi to\n3083\nViT/16 \n \nText Transformer \nCLIP top-k\ndataset vocabulary\ntennis\nracket \nball \nsoccer\neating \noutdoor \nkitchen \nathlete\nchild \nplay\nrunning \ncourt\n...\nHAM\ntennis court\nlinguistic feature\nFigure 3: Illustration of relevant scene elements extraction\nprocess where ViT/16 and Text Transformer are the pre-\ntrained models from CLIP (Radford et al. 2021).\nobtain the bounding boxes of all human agents. Afterward,\nwe align each of the detected bounding boxes Bi onto the\nfeature map Hi, which is obtained by the previous modality,\nusing RoIAlign (He et al. 2017). Then, features overlapped by\neach agent bounding box are averagely pooled into a single\nfeature vector to represent visual information of the agent\ninside that box. Finally, we obtain a set of local agent visual\nfeatures Fa\ni ∈ RNa×da, where Na and da are the number of\ndetected agents and agent embedding dimension, respectively.\nFinally, we apply HAM (detailed later) to adaptively select\nan arbitrary number of main agents from Na detected agents\nand extract their mutual relationships to form a unified agent-\naware visual feature fa\ni ∈ Rdemb as follows:\nfa\ni = HAM(MLPθa(Fa\ni ), fe\ni ) (2)\n(iii) Linguistic Relevant Scene Elements:\nThis modality provides additional contextual details of the\nscene. While the two former modalities capture visual infor-\nmation of spatial appearances and temporal motions, their\nfeatures may overlook some of the scene components because\nof the spacial reduction in the pooling operation from the fea-\nture map Hi. Furthermore, non-visual features could hardly\nbe captured by a normal vision backbone model. Recent\nstudies (Patashnik et al. 2021; Yang, Zhang, and Zou 2022)\nhave shown the extreme zero-shot capability of Contrastive\nLanguage-Image Pre-training (CLIP) where the model can\nestimate the semantic similarity between a set of words and\nan image. Trained on large-scale text-image pairs, CLIP can\ncorrelate not only the visual words but also the non-visual\nwords to the given image. We thus leverage CLIP as a lin-\nguistic feature extractor to obtain top k scene elements (i.e.,\nk texts) that are highly correlated with the middle frame of\nthe input snippet Xi. Specifically, we construct a vocabulary\nW = {w1, . . . wm} based on the groundtruth captions of our\ntraining dataset. Each vocabulary wi ∈ Wis encoded by a\ntransformer network fϕ into a text feature fw\ni . Let Wt be a\ntext projection matrix pre-trained by CLIP, the embedding\ntext vocabulary is computed as\nwe = Wt · fϕ(W) = Wt · fwwhere fw = {fw\ni }m\ni=1. (3)\nLet Wi be an image projection matrix pre-trained by CLIP,\nthe center frame I of the input snippet Xi is first encoded by\na pre-trained ViT gψ to extract visual feature fI, and then\nembedded by Wi as below:\nIe = Wi · gψ(I) = Wi · fI (4)\nThe pairwise cosine similarities between embedded Ie and\nwe are then computed. Top k similarity scores are chosen\nas linguistic categorical concept features Fl\ni ∈ Rk×dl. This\nfeature is also subjected to the HAM module to select only\nthe most relevant representative linguistic features and merge\nthem into a single representation fl\ni ∈ Rdemb as follows:\nfl\ni = HAM(MLPθl(Fl\ni ), fe\ni ) (5)\nThe flowchart of extracting fl\ni is illustrated in Fig.3.\n(iv) Multi-modal Representation Fusion (M2RF):\nThis component aims to fuse features from the three modal-\nities. While concatenation or summation are the two com-\nmon fusion mechanisms, they treat all modalities equally.\nTo better model the impact of each individual modality, we\npropose M2RF as a function gγ, which takes the features fe\ni ,\nfa\ni , and fl\ni as its input. We extract the inter-feature relation-\nships by utilizing a self-attention (SA) layer (Vaswani et al.\n2017) followed by a mean operation. The final representation\nfV L\ni ∈ Rdemb of a given snippet Xi is defined as follows:\nfV L\ni = gγ([fe\ni ; fa\ni ; fl\ni ]) = mean(SA([fe\ni ; fa\ni ; fl\ni ])) (6a)\nwhere [; ] represents the concatenation of features in a new\ndimension, where self-attention is applied on the new di-\nmension and reduced by the mean operation to account for\npermutation invariance.\nTransformer-in-Transformer (TinT) Decoder\nInspired by the recent transformer-based vision-language\nmodels (Chen et al. 2020b; Lei et al. 2020), we adopt the\nunified encoder-decoder transformer structure as a founda-\ntion for the caption generator, i.e., an inner transformer. The\ninner transformer’s input is described as following. In this\nsetup, video features\nFV Lis formed by concatenating all\nfV L\ni obtained by applying VL Encoder into each snippet Xi,\ni.e., FV L= {fV L\ni }L\ni=1 ∈ RL×demb . Textual tokens Ftext is\nencoded by a pre-trained text transformer gϕ from CLIP and\na MLP layer, i.e., Ftext = MLP (gϕ(Shifted GT text)) ∈\nRN×demb , where N is the sequence length of the text tokens.\nFollowing (Lei et al. 2020), learnable token type embeddings\nFtype ∈ R(L+N)×demb are introduced to inform the location\nof the video and the caption representations. Ftype is initial-\nized as 0/1 vectors, i.e., video as 0 and text as 1. For the tth\nevent, an intermediate hidden states ¯Hl\nt ∈ R(L+N)×demb is\ncomputed in Eq. 7b as canonical inner transformer encoder,\nwhere ˜Hl\nt is the internal states after Masked Multihead Self\nAttention (MSA).\nH0\nt = [FV L; Ftext] + Ftype ∈ R(L+N)×demb (7a)\n¯Hl\nt = MLP( ˜Hl\nt) + ˜Hl\nt, ˜Hl\nt = MSA(Hl\nt) + Hl\nt (7b)\nWhile the inner transformer can effectively model intra-\nevent coherency, it cannot handle the contextual relationship\nof inter-event. To address this limitation, we introduce an\n3084\nautoregressive outer transformer. The outer transformer se-\nlectively utilizes the activations of the inner transformer from\nthe previous time steps for generating a coherent paragraph.\nSpecifically, we take advantage of HAM to select only the\nmost relevant hidden states of all previous events stored in\nevent memory with respect to the current one. The outer\ntransformer process is formulated below:\nMl\nt = [Ml\nt−1; ¯Hl\nt] (8a)\nZl\nt = HAM(Ml\nt−1, ¯Hl\nt) (8b)\nHl\nt = MLP(gγ([ ¯Hl\nt; Zl\nt])) + ¯Hl\nt (8c)\nFor the tth event, an intermediate hidden states ¯Hl\nt is\nstacked to the event memory Ml\nt ∈ Rt×(L+N)×demb , where\nMl\n0 = ∅ as in Eq. 8a. Eq. 8b computes the context Zl\nt from\nthe previous states of the event memory and the current in-\ntermediate hidden states ¯Hl\nt using HAM. Finally, in Eq. 8c,\nthe context is integrated with the intermediate hidden states\n¯Hl\nt using gγ, which was introduced in Eq. 6a, and the hidden\nstates are updated via residual connection. After the last layer,\nvideo token positions in HN\nt are ignored, and only the text\ntoken positions are fed to a feed-forward layer followed by\nsoftmax to predict a caption for the tth event.\nHybrid Attention Mechanism (HAM)\nHAM inherits the merits of both hard attention (Patro and\nNamboodiri 2018) and the self-attention (Vaswani et al. 2017)\nto select a rational number of representative features out of\na set of input features and to extract mutual relationships\namong the sub-set of selected features, respectively, and fuse\nthem into a unified representation. HAM was introduced by\n(V o et al. 2021a) and it then has been successfully applied in\nto video analysis i.e. action localization (V o et al. 2022, 2023).\nFig. 4 visualizes the workflow of HAM, which is formulated\nas follows:\nHin = Fin ⊕ fref (9a)\nC = softmax(||Hin||2) (9b)\nM = C > 1\nNin\n(9c)\nfout = gγ(Fin ⊙ M) (9d)\nwhere Fin ∈ RNin×din and fref ∈ Rdin are a set of input fea-\ntures and a reference feature, respectively, whereNin is the\ntotal number of input features and din is the embedding di-\nmension of input and reference features. HAM takes Fin and\nfref as inputs and compute the most relevant feature fout as\nits output.\nVisual-Linguistic (VL) Contrastive Loss\nTypically, the existing VPC methods exploit the MLE loss\nto train their models. The MLE loss serves the objective of\nincreasing the likelihood of predicted captions to be matched\nwith the groundtruths. However, it is unable to address the\nquestion of how well the learnt event embedding features\nrepresent the groundtruth captions. To this end, we leverage\nthe recent advantages of contrastive learning (Wu et al. 2018;\nreference featurefeatures\nHard Attention\n0\nSoft Attention\nfeature \nselection \nmeanfeature \nmerging \nfused feature\nFigure 4: Illustration of HAM. HAM is capable of select-\ning and representing an arbitrary number of representative\nfeatures from the input features Fin with a guidance from\nreference feature fref.\nChen et al. 2020a) and propose Lcon to pull all snippets of\nthe same event and push snippets of different events. Our VL\nLoss consists of two terms corresponding to captioning loss\n(Lcap.) and a contrastive contextual loss (Lcon.). While Lcap.\naims to decode captions that match with groundtruths, Lcon.\nguarantees the learnt latent features are close to the semantic\ninformation encoded in the groundtruth captions.\nCaptioning LossLcap.: Kullback–Leibler (KL) divergence\nis commonly utilized to minimize the divergence between\nempirical distribution p(s|Ve) and predicted distribution\npθ(s|Ve) for a video segment Ve. However, this objective\neasily makes the captioning model overfit high-frequency\ntokens and phrases, which results in repetitive phrases. In\norder to enhance the smoothness of the predicted sentence, a\nregularization term τ is introduced to the training objective\nwith hyper-parameter λ as:\nθ∗ = argmin\nθ\nEs∼p(s)\n\u0014\nlog\n\u0012 p(s)\npθ(s)\n\u0013\n+ λτ(s)\n\u0015\n(10)\nThe second term τ imposes a token-level high-frequency\npenalties as (Song, Chen, and Jin 2021). Based on the obser-\nvation that the model tends to generate words that have been\ngenerated before, we penalize the previously appeared words\nin the regularization term:\nτ(s) = − 1\nN\nNX\ni=1\nX\nc∈{s|s<i}\nlog (1− pθ(c|s<i, E)) (11)\nwhere c is the candidate word at n to be penalized.\nOur Lcap. is defined as follows:\nLcap. = − 1\nN\nNX\ni=1\n(log pθ(si|s<i, Ve)) + λτ(s) (12)\nwhere θ is the model parameters, s1:N is the target ground\ntruth sequence.\nContrastive Contextual LossLcon.: We propose Lcon. to\noptimize the latent feature of the input event to be highly\n3085\nMethods Venue Input B4 ↑ M ↑ C ↑ R ↑ Div2 ↑ R4 ↓\nVanilla\nTrans. (Zhou et al. 2018) CVPR Res200/Flow 9.75 15.64\n22.16 28.90 † 77.40† 7.79\nAdvInf (P\nark et al. 2019) CVPR C3D/Object 10.04 16.60 20.97 – – 5.76\nGVD\n(Zhou et al. 2019) CVPR Res200/Flow/Object 11.04 15.71\n21.95 – – 8.76\nT\nrans.-XL (Dai et al. 2019) ACL Res200/Flow 10.39 15.09\n21.67 30.18 † 75.96† 8.54\nTrans.-XLRG\n(Lei et al. 2020) ACL Res200/Flow 10.17 14.77\n20.40 – – 8.85\nMAR\nT (Lei et al. 2020) ACL Res200/Flow 10.33 15.68\n23.42 30.32 † 75.71† 5.18\nPDVC\n(Wang et al. 2021a) ICCV C3D/Flow 11.80 15.93 27.27 – – –\nVLT\ninT (ours) – C3D/Ling 14.93 18.16\n33.07 36.86 77.72 4.87\nTable 1: Performance comparison of VLTinT with other SOTA models on ActivityNet Captionsae-val. † denotes results by us.\nMethods Venue Input B4 ↑ M ↑ C ↑ R ↑ Div2 ↑ R4 ↓\nVanilla\nTrans. (Zhou et al. 2018) CVPR Res200/Flow 9.31 15.54\n21.33 28.98 † 77.29† 7.45\nTrans.-XL\n(Dai et al. 2019) ACL Res200/Flow 10.25 14.91\n21.71 30.25 † 76.17† 8.79\nTrans.-XLRG\n(Lei et al. 2020) ACL Res200/Flow 10.07 14.58\n20.34 – – 9.37\nMAR\nT (Lei et al. 2020) ACL Res200/Flow 9.78 15.57\n22.16 30.85 † 75.69† 5.44\nMARTCOO\nT (Ging et al. 2020) NIPS COOT 10.85 15.99 28.19 – – 6.64\nMemory\nTrans. (Song, Chen, and Jin 2021) CVPR I3D 11.74 15.64 26.55\n– 83.95 2.75\nVLT\ninT (ours) – C3D/Ling 14.50 17.97\n31.13 36.56 77.72 4.75\nTable 2: Performance comparison of VLTinT with other SOTA models on ActivityNet Captionsae-test. † denotes results by us.\ncorrelated with its groundtruth description. This loss function\nimplicitly encourages our VLTinT to learn better represen-\ntations of the events and enhance its overall performance\nwithout extra computational cost.\nSpecifically, Lcon. processes the entire mini-batch of train-\ning examples B = {(Vb, sb)}|B|\nb=1, where Vb is a set of\nsnippets within the same event and sb is its corresponding\ngroundtruth description sentence. On the one hand, video\nsnippets in Vb are processed through our proposed VLTinT\nto obtain the event embeddings, which corresponds to the\nvideo token position FN\nb ∈ RL×demb of the final hidden state\nHN\nb . On the other hand, we process each groundtruth caption\nsentence sb through the transformer gϕ of CLIP (Radford\net al. 2021) to obtain a representation feature fT\nb ∈ Rdemb .\nThen, Lcon. processes FN\nb and fT\nb as follows:\nLcon. = −\n|B|X\nb1=1\n|B|X\nb2=1\n[1b1=b2 log(eρ(fN\nb1 · fT\nb2 ))\n+ (1 − 1b1=b2 )(1 − log(eρ(fN\nb1 · fT\nb2 )))] (13a)\nwhere fN\nb = mean(FN\nb ). 1b1=b2 returns 1 when samples\ncome from the same event, i.e., b1 = b2 and 0 when samples\ncome from the different events i.e., b1 ̸= b2. ρ is a learnable\ntemperature parameter initialized as log(1/0.07), to prevent\nscaling of the dot product values and stabilize the training.\nFinally, our proposed VL contrastive loss LV Lis defined as:\nLV L= Lcap. + Lcon. (14)\nExperiments\nDatasets and Metrics\nWe benchmark VLTinT on two popular datasets, ActivityNet\nCaptions (Krishna et al. 2017) and YouCookII (Zhou, Xu,\nand Corso 2018). ActivityNet Captions consists of 10,009\ntraining videos and 4,917 validation videos. We follow the\nprevious work (Lei et al. 2020) to split the original validation\nset into two subsets: ae-val with 2,460 videos for validation\nand ae-test with 2,457 videos for testing. YouCookII con-\ntains 1,333 training and 457 validation videos. We report our\nresults on the validation sets. We evaluate the performance\non four standard metrics, i.e., BLEU@4 (B@4) (Papineni\net al. 2002), METEOR (M) (Denkowski and Lavie 2014),\nCIDEr (C) (Vedantam, Zitnick, and Parikh 2015), ROUGE\n(R) (Lin 2004). Whereas to benchmark the diversity of gener-\nated captions, we use two diversity metrics, including 2-gram\ndiversity (Div@2) (Shetty et al. 2017) and 4-gram repetition\n(R@4) (Xiong, Dai, and Lin 2018).\nImplementation Details\nTo extract visual features of the environment, we use C3D (Ji\net al. 2013) pre-trained on Kinetics-400 (Kay et al. 2017) as\nthe backbone network. The agent visual feature is extracted\nby Faster-RCNN (Ren et al. 2015) that is pre-trained on the\nCOCO dataset (Lin et al. 2014). To extract the linguistic scene\nelement features, we employ CLIP (Radford et al. 2021) ViT-\nB/16 model made publically available by OpenAI. We set\nthe hidden size to 768, the number of transformer layers to\n3, and the number of attention heads to 12. Adam optimizer\nwas used to train VLTinT with an initial learning rate of 1e-4,\nβ1 = 0.9, β2 = 0.999, L2 weight decay of 0.01, and learning\nrate warmup over the first 5 epochs. During the training, we\n3086\nMethods Venue Input B@4 ↑ M ↑ C ↑ R ↑ R@4 ↓\nVanilla Trans.(Zhou et al. 2018) CVPR Res200/Flow 4.38 11.55 38.00 – –\nMART (Lei et al. 2020) ACL Res200/Flow 8.00 15.90 35.74 – 4.39\nMARTCOOT (Ging et al. 2020) NIPS COOT 9.44 18.17 46.06 – 6.30\nVLTinT (ours) – C3D/Ling 9.40 17.94 48.70 34.55 4.29\nTable 3: Performance comparison of VLTinT with other SOTA models on YouCookII validation set.\nat-test split ae-val split\nEnv\n. Agt. Ling. B@4 ↑ M ↑ C ↑ R ↑ Div@2\n↑ R@4 ↓ B@4 ↑ M ↑ C ↑ R ↑ Div@2 ↑ R@4 ↓√ × × 13.62 17.41\n29.09 35.96 76.14 5.97 14.02 17.58\n30.31 36.20 76.11 6.08\n× √ × 11.83 16.22\n21.39 33.97 79.20 4.16 12.13 16.57\n24.98 34.36 79.18 4.24\n× × √ 13.38 17.69\n30.30 35.63 80.50 3.32 14.00 17.88\n31.64 35.95 80.44 3.22√\n√ × 13.77 17.52\n30.05 35.93 77.78 4.69 14.12 17.78\n31.15 36.12 78.02 4.56√ × √ 14.53 17.79 30.83 36.67 76.47 5.60 14.84 17.97 31.86 36.80 76.41 5.67√\n√ √ 14.50 17.97 31.13 36.56 77.72 4.75 14.93 18.16\n33.07 36.86 77.72 4.87\nTable 4: Ablation study on the contribution of each modality in VL Encoder on ActivityNet Captions dataset. Env., Agt., and\nLing. denote the global visual environment, local visual main agents, and linguistic relevant scene elements, respectively.\nB@4↑ M ↑ C ↑ R ↑ R@4 ↓\nM RCNN 1.35 9.09 12.29 23.06 18.52\nCLIP 13.38 17.69 30.30 35.63 3.32\nTable 5: Performance comparison between two cases trained\non TinT network without visual feature: (i) scene elements\nextracted by Mask R-CNN (M RCNN) (ii) scene elements\nextracted by CLIP.\nuse the label smoothing with a value of 0.1 and λ = 0.1. We\nran the experiment on a single NVIDIA RTX 3090 (24GB)\nGPU.\nQualitative Analysis\nFig.5 shows comparison between VLTinT and Vanilla Trans-\nformer (VTrans) (Zhou et al. 2018) and MART (Lei et al.\n2020). Overall, VLTinT can generate more descriptive cap-\ntions with fine-grained details. In particular, we noticed that\nVTrans and MART are prone to use high-frequency words\nfor their caption, while VLTinT can use expressive but less\nfrequently appearing words, e.g., ”A man” vs. ”An athletic\nman” in the example. We attribute this improvement to our\nVL Encoder, which incorporates relative scene elements. We\nfurther observe a caption repetitiveness problem in VTrans\nand MART, which is handled our proposed TinT Decoder.\nNotably, with the same action (i.e., run down the track and\njump into a sand pit), our VLTinT can tell when the action\nstarts (i.e., begin) and happens (i.e., then). This is thank to the\nrich spatial information of VL Encoder and strong temporal\ncoherency of TinT Decoder.\nQuantitative Analysis\nWe benchmark and compare VLTinT with the prior SOTA\nVPC works on both ActivityNet Captions ae-val, ae-test, and\nYouCookII as in Tables. 1, 2 and 3, respectively. In those\ntables, we highlight the best and the second-best scores cor-\nresponding each metric. Compared to the SOTA approaches\nAn athletic man is seen standing before a track and leads into him\nrunning down in a pit of sand.  Several more clips are shown of the\nathletes running down the track and landing into a pit.\nAn athletic man is seen standing ready and begins running down a\ntrack and jumping into a pit. The man then runs down the track and\njumps into a sand pit.\nA man runs down a track and jumps into a sand pit. The man runs\ndown the track and jumps into a sand pit.\nA man is running down a track and jumping into a sand pit. He jumps\nover a bar and lands in the sand.\nMART:\nVTrans:\nVLTinT: \nGT:\nFigure 5: Qualitative comparison on ActivityNet Captionsae-\ntest split. Red text indicates the captioning mistakes, purple\ntext indicates repetitive patterns, and blue text indicates some\ndistinct expressions.\nuse of\ninter-event B@4 ↑ M ↑ C ↑ R ↑ling. modeling\nae-val × RNN 11.68 16.79\n25.86 33.97\nTrans. 14.12 17.78\n31.15 36.12\n√ RNN 13.75 17.63\n28.01 36.21\nTrans. 14.93 18.16\n33.07 36.86\nae-test\n× RNN 11.10 15.72\n27.67 31.75\nTrans. 13.77 17.52\n30.05 35.93\n√ RNN 13.45 17.42\n29.68 36.09\nTrans. 14.50 17.97\n31.13 36.56\nTable 6: Comparison between RNN and Transformer to\nmodel inter-event dependencies in TinT decoder on Activi-\ntyNet Captions with C3D (env+agent) is visual feature in the\nencoder. Linguistic feature (Ling.) is considered as an option.\nMART (Lei et al. 2020), MART w/COOT (Ging et al. 2020),\nand PDVC (Wang et al. 2021a), our VLTinT outperforms\nwith large margins on both accuracy and diversity metrics on\nActivityNet Captions. For example on ae-val split, accuracy\ngains 3.13%/1.56%/5.80%6.54% on B@4/M/C/R metrics\n3087\nwhereas diversity increases 0.32% on Div@2 and reduces\n0.32% on R@4 compared to the second-best performance.\nOn ae-test split, accuracy gains 3.65%/1.98%/2.94%5.71%\non B@4/M/C/R metrics whereas diversity increases 0.43%\non Div@2 and reduces 0.67% on R@4 compared to the\nsecond-best performance. On YouCookII, our performance\nis the best on C, R, and R@4 metrics with considerable gaps\nwhile it achieves compatible performance on B@4 and M\nmetrics.\nAblation Studies\n• Contribution of each modality in VL Encoder:We exam-\nine VLTinT on ActivityNet Captions with different modality\nsettings as given in Table 4. The first three rows show the per-\nformance on each individual modality whereas the last three\nrows show the performance on different combinations. Even\nthough the best performance on overall is obtained by com-\nbining all three modalities of both vision (environment and\nagent) and language (scene elements), the performance on\nonly linguistic feature is promising with notable performance,\nespecially on diversity metrics. This should be included in\nour future investigation.\n• Effectiveness of linguistic relevant scene elements:We\ncompare the performance of VLTinT with two cases given in\nTable 5: (i) scene elements extracted by Mask-RCNN trained\non COCO with 80 classes (He et al. 2017) and (ii) scene ele-\nments extracted by CLIP. The ablation study shows the effec-\ntiveness of the scene elements feature extracted by CLIP over\nMask-RCNN. While scene elements consist of human/non-\nhuman (e.g., animals, vehicles) and visual/non-visual (e.g.,\nrelations, activities) elements, Mask R-CNN can only cover\na small portion of them because it was trained on a small\nnumber of visual objects/classes, resulting in poor diversity\nand lower performance on scene understanding compared to\nCLIP.\n• Robustness of TinT Decoder: We examine the TinT\nDecoder with two settings of inter-event modeling, i.e., RNN-\nbased similar to (Lei et al. 2020) and transformer-based\n(ours). The decoder is also considered with two encoder\nfeature settings, i.e., with and without linguistic features\nwhereas C3D (env+agent) is used as visual features. The\nresult is shown in Table 6. Here we observe the substantial\nperformance gain by modeling inter-event relationships by\nour autoregressive outer transformer.\n• Effectiveness of VL LossLV L: The effectiveness of VL\nLoss is examined by replacing LV Lwith MLE loss, which\nis a common loss in VPC. The performance of VLTinT on\nActivityNet Captions ae-test with two loss functions are re-\nported in Table 7.\n•\nComputational Complexity: We compare computational\ncomplexity vs. accuracy of our VLTinT with SOTA VPC\nmodels on the ActivityNet ae-test split. We report trainable\nparams (millions), computation (GFLOPs), average inference\ntime (seconds) over 100 random videos, and accuracy metrics\nin Table 8. In this comparison, we investigate our VLTinT\nwith different settings. Compared to SOTA, our model with\nLoss B@4↑ M ↑ C ↑ R ↑\nMLE 13.80 17.72 30.59 36.11\nLV L(ours) 14.50 17.97 31.13 36.56\nTable 7: Effectiveness ofLV Lcompared to the standard MLE\nloss on ActivityNet Captions ae-test.\nComputational cost Accuracy\nModels Params\n↓ Comp. ↓ Inf.↓ M↑ C↑\nMART 36.25 6.32 0.025 15.57 22.16\nMem T\nrans 29.69 256.44 0.706 16.10 27.36\nE. 36.01 17.69 0.028 17.41 29.09\nE./A. 40.37 22.70 0.032 17.52 30.05\nE./A./L. 43.40 40.37 0.038 17.97 31.13\nTable 8: Computational cost vs. accuracy between VLTinT\n(E./A./L.) with different settings and SOTA VPC models.\nonly env. has compatible params and inference time with\nbetter performance, whereas our model with env. & agent. &\nlang. gain big margins on accuracy while the complexities\nremain plausible.\nConclusion\nIn this work, we have presented VLTinT, a novel model\nfor VPC. The proposed network consists of VL Encoder\nand TinT Decoder. In VL Encoder, the video feature is ex-\ntracted by three modalities, i.e., global visual environment,\nlocal visual main agents, and linguistic relevant scene ele-\nments; and they are fused through M2RF. In TinT Decoder,\nthe intra-event coherency is modeled by the unified inner\ntransformer and inter-event coherency is modeled by the\nautoregressive outer transformer. Our proposed VLTinT is\ndesigned as an end-to-end framework and trained by our pro-\nposed VL contrastive loss LV L. Comprehensive experiments\nand extensive ablation studies on ActivityNet Captions and\nYouCookII datasets have demonstrated the effectiveness of\nVLTinT, which outperforms the existing SOTA approaches\non both accuracy (B@4, M, C, R) and diversity (Div@2,\nR@4) metrics.\nFuture investigations might include further examining lin-\nguistic feature in video understanding and exploring the VL\nEncoder in other video analysis problems.\nAcknowledgments\nThis material is based upon work supported by the Na-\ntional Science Foundation (NSF) under Award No OIA-\n1946391, NSF 1920920, NSF FAIN-2223793 and NIH\n1R01CA277739.\nReferences\nChen, T.; Kornblith, S.; Norouzi, M.; and Hinton, G. 2020a.\nA Simple Framework for Contrastive Learning of Visual\nRepresentations. In Proceedings of the 37th International\nConference on Machine Learning, 1597–1607. PMLR.\nChen, Y .-C.; Li, L.; Yu, L.; El Kholy, A.; Ahmed, F.; Gan, Z.;\nCheng, Y .; and Liu, J. 2020b. UNITER: UNiversal Image-\n3088\nTExt Representation Learning. In Vedaldi, A.; Bischof, H.;\nBrox, T.; and Frahm, J.-M., eds., Computer Vision – ECCV\n2020, 104–120. Cham: Springer International Publishing.\nISBN 978-3-030-58577-8.\nChung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y . 2014. Em-\npirical evaluation of gated recurrent neural networks on se-\nquence modeling. In NIPS 2014 Workshop on Deep Learning,\nDecember 2014.\nDai, Z.; Yang, Z.; Yang, Y .; Carbonell, J.; Le, Q.; and\nSalakhutdinov, R. 2019. Transformer-XL: Attentive Lan-\nguage Models beyond a Fixed-Length Context. In Proceed-\nings of the 57th Annual Meeting of the Association for Com-\nputational Linguistics, 2978–2988. Florence, Italy: Associa-\ntion for Computational Linguistics.\nDeng, C.; Chen, S.; Chen, D.; He, Y .; and Wu, Q. 2021.\nSketch, Ground, and Refine: Top-Down Dense Video Cap-\ntioning. In CVPR, 234–243.\nDenkowski, M.; and Lavie, A. 2014. Meteor Universal: Lan-\nguage Specific Translation Evaluation for Any Target Lan-\nguage. In Proceedings of the Ninth Workshop on Statistical\nMachine Translation, 376–380. Baltimore, Maryland, USA:\nAssociation for Computational Linguistics.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In ICLR.\nGing, S.; Zolfaghari, M.; Pirsiavash, H.; and Brox, T. 2020.\nCOOT: Cooperative Hierarchical Transformer for Video-Text\nRepresentation Learning. In Advances on Neural Information\nProcessing Systems (NeurIPS).\nHan, K.; Xiao, A.; Wu, E.; Guo, J.; XU, C.; and Wang, Y .\n2021. Transformer in Transformer. In Advances in Neural\nInformation Processing Systems, volume 34, 15908–15919.\nCurran Associates, Inc.\nHe, K.; Gkioxari, G.; Dollar, P.; and Girshick, R. 2017. Mask\nR-CNN. In Proceedings of the IEEE International Confer-\nence on Computer Vision (ICCV).\nHutchinson, M. S.; and Gadepally, V . N. 2021. Video Action\nUnderstanding. IEEE Access, 9: 134611–134637.\nIashin, V .; and Rahtu, E. 2020. Multi-Modal Dense Video\nCaptioning. In The IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR) Workshops, 958–959.\nJi, S.; Xu, W.; Yang, M.; and Yu, K. 2013. 3D Convolutional\nNeural Networks for Human Action Recognition. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n35(1): 221–231.\nKay, W.; Carreira, J.; Simonyan, K.; Zhang, B.; Hillier, C.;\nVijayanarasimhan, S.; Viola, F.; Green, T.; Back, T.; Natsev,\nP.; Suleyman, M.; and Zisserman, A. 2017. The Kinetics\nHuman Action Video Dataset. CoRR, abs/1705.06950.\nKrishna, R.; Hata, K.; Ren, F.; Fei-Fei, L.; and Niebles, J. C.\n2017. Dense-Captioning Events in Videos. In International\nConference on Computer Vision (ICCV).\nLei, J.; Wang, L.; Shen, Y .; Yu, D.; Berg, T.; and Bansal, M.\n2020. MART: Memory-Augmented Recurrent Transformer\nfor Coherent Video Paragraph Captioning. In Proceedings\nof the 58th Annual Meeting of the Association for Computa-\ntional Linguistics, 2603–2614. Online: Association for Com-\nputational Linguistics.\nLi, Y .; Yao, T.; Pan, Y .; Chao, H.; and Mei, T. 2018. Jointly\nLocalizing and Describing Events for Dense Video Caption-\ning. In CVPR.\nLin, C.-Y . 2004. ROUGE: A Package for Automatic Evalu-\nation of Summaries. In Text Summarization Branches Out,\n74–81. Barcelona, Spain: Association for Computational Lin-\nguistics.\nLin, T.-Y .; Maire, M.; Belongie, S.; Hays, J.; Perona, P.;\nRamanan, D.; Doll´ar, P.; and Zitnick, C. L. 2014. Microsoft\nCOCO: Common Objects in Context. In Fleet, D.; Pajdla, T.;\nSchiele, B.; and Tuytelaars, T., eds.,Computer Vision – ECCV\n2014, 740–755. Cham: Springer International Publishing.\nISBN 978-3-319-10602-1.\nLupyan, G.; Abdel Rahman, R.; Boroditsky, L.; and Clark,\nA. 2020. Effects of Language on Visual Perception. Trends\nin Cognitive Sciences, 24(11): 930–944.\nMun, J.; Yang, L.; Ren, Z.; Xu, N.; and Han, B. 2019. Stream-\nlined Dense Video Captioning. In CVPR.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBLEU: A Method for Automatic Evaluation of Machine\nTranslation. In Proceedings of the 40th Annual Meeting on As-\nsociation for Computational Linguistics, ACL ’02, 311–318.\nUSA: Association for Computational Linguistics.\nPark, J. S.; Rohrbach, M.; Darrell, T.; and Rohrbach, A. 2019.\nAdversarial Inference for Multi-Sentence Video Description.\nIn CVPR, 6591–6601.\nPascanu, R.; Mikolov, T.; and Bengio, Y . 2013. On the dif-\nficulty of training recurrent neural networks. In Dasgupta,\nS.; and McAllester, D., eds., Proceedings of the 30th Inter-\nnational Conference on Machine Learning, volume 28 of\nProceedings of Machine Learning Research, 1310–1318. At-\nlanta, Georgia, USA: PMLR.\nPasunuru, R.; and Bansal, M. 2017. Multi-Task Video Cap-\ntioning with Video and Entailment Generation. In Proceed-\nings of the 55th Annual Meeting of the Association for Com-\nputational Linguistics (Volume 1: Long Papers), 1273–1283.\nVancouver, Canada: Association for Computational Linguis-\ntics.\nPatashnik, O.; Wu, Z.; Shechtman, E.; Cohen-Or, D.; and\nLischinski, D. 2021. StyleCLIP: Text-Driven Manipulation\nof StyleGAN Imagery. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision (ICCV), 2085–\n2094.\nPatro, B.; and Namboodiri, V . P. 2018. Differential Attention\nfor Visual Question Answering. In CVPR.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nKrueger, G.; and Sutskever, I. 2021. Learning Transferable\nVisual Models From Natural Language Supervision. In Meila,\nM.; and Zhang, T., eds.,Proceedings of the 38th International\nConference on Machine Learning, volume 139 of Proceed-\nings of Machine Learning Research, 8748–8763. PMLR.\n3089\nRahman, T.; Xu, B.; and Sigal, L. 2019. Watch, Listen and\nTell: Multi-Modal Weakly Supervised Dense Event Caption-\ning. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision (ICCV).\nRen, S.; He, K.; Girshick, R.; and Sun, J. 2015. Faster R-\nCNN: Towards Real-Time Object Detection with Region\nProposal Networks. In Cortes, C.; Lawrence, N.; Lee, D.;\nSugiyama, M.; and Garnett, R., eds., Advances in Neural In-\nformation Processing Systems, volume 28. Curran Associates,\nInc.\nShetty, R.; Rohrbach, M.; Anne Hendricks, L.; Fritz, M.; and\nSchiele, B. 2017. Speaking the Same Language: Matching\nMachine to Human Captions by Adversarial Training. InPro-\nceedings of the IEEE International Conference on Computer\nVision (ICCV).\nShi, B.; Ji, L.; Liang, Y .; Duan, N.; Chen, P.; Niu, Z.; and\nZhou, M. 2019. Dense Procedure Captioning in Narrated\nInstructional Videos. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics,\n6382–6391. Florence, Italy: Association for Computational\nLinguistics.\nSong, Y .; Chen, S.; and Jin, Q. 2021. Towards Diverse Para-\ngraph Captioning for Untrimmed Videos. In CVPR, 11245–\n11254.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.\nAttention is All you Need. In Guyon, I.; Luxburg, U. V .;\nBengio, S.; Wallach, H.; Fergus, R.; Vishwanathan, S.; and\nGarnett, R., eds., Advances in Neural Information Processing\nSystems, volume 30. Curran Associates, Inc.\nVedantam, R.; Zitnick, C. L.; and Parikh, D. 2015. CIDEr:\nConsensus-based image description evaluation. In CVPR,\n4566–4575.\nV o, K.; Joo, H.; Yamazaki, K.; Truong, S.; Kitani, K.; Tran,\nM.; and Le, N. 2021a. AEI: Actors-Environment Interaction\nwith Adaptive Attention for Temporal Action Proposals Gen-\neration. In 32nd British Machine Vision Conference 2021,\nBMVC 2021, Online, November 22-25, 2021, 111. BMV A\nPress.\nV o, K.; Truong, S.; Yamazaki, K.; Raj, B.; Tran, M.-T.; and\nLe, N. 2023. AOE-Net: Entities Interactions Modeling with\nAdaptive Attention Mechanism for Temporal Action Propos-\nals Generation. International Journal of Computer Vision,\n131(1): 302–323.\nV o, K.; Yamazaki, K.; Nguyen, P. X.; Nguyen, P.; Luu,\nK.; and Le, N. 2022. Contextual Explainable Video\nRepresentation: Human Perception-based Understanding.\narXiv:2212.06206.\nV o, K.; Yamazaki, K.; Truong, S.; Tran, M.-T.; Sugimoto, A.;\nand Le, N. 2021b. ABN: Agent-Aware Boundary Networks\nfor Temporal Action Proposal Generation. IEEE Access, 9:\n126431–126445.\nV o-Ho, V .-K.; Le, N.; Kamazaki, K.; Sugimoto, A.; and Tran,\nM.-T. 2021. Agent-Environment Network for Temporal Ac-\ntion Proposal Generation. In ICASSP 2021 - 2021 IEEE\nInternational Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), 2160–2164.\nWang, T.; Zhang, R.; Lu, Z.; Zheng, F.; Cheng, R.; and Luo,\nP. 2021a. End-to-End Dense Video Captioning With Parallel\nDecoding. In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV), 6847–6857.\nWang, T.; Zheng, H.; Yu, M.; Tian, Q.; and Hu, H. 2021b.\nEvent-Centric Hierarchical Representation for Dense Video\nCaptioning. IEEE Transactions on Circuits and Systems for\nVideo Technology, 31(5): 1890–1900.\nWang, X.; Wu, J.; Chen, J.; Li, L.; Wang, Y .-F.; and Wang,\nW. Y . 2019. VaTeX: A Large-Scale, High-Quality Multilin-\ngual Dataset for Video-and-Language Research. In Proceed-\nings of the IEEE/CVF International Conference on Computer\nVision (ICCV).\nWu, Z.; Xiong, Y .; Yu, S. X.; and Lin, D. 2018. Unsupervised\nFeature Learning via Non-Parametric Instance Discrimina-\ntion. In CVPR.\nXiong, Y .; Dai, B.; and Lin, D. 2018. Move Forward and Tell:\nA Progressive Generator of Video Descriptions. In Ferrari, V .;\nHebert, M.; Sminchisescu, C.; and Weiss, Y ., eds.,Computer\nVision – ECCV 2018, 489–505. Cham: Springer International\nPublishing. ISBN 978-3-030-01252-6.\nYamazaki, K.; Truong, S.; V o, K.; Kidd, M.; Rainwater, C.;\nLuu, K.; and Le, N. 2022. VLCAP: Vision-Language with\nContrastive Learning for Coherent Video Paragraph Cap-\ntioning. In 2022 IEEE International Conference on Image\nProcessing (ICIP), 3656–3661.\nYang, B.; Zhang, T.; and Zou, Y . 2022. CLIP Meets Video\nCaptioning: Concept-Aware Representation Learning Does\nMatter. arXiv:2111.15162.\nZhang, Z.; Zhang, H.; Zhao, L.; Chen, T.; Arik, S. ; and\nPfister, T. 2022. Nested Hierarchical Transformer: Towards\nAccurate, Data-Efficient and Interpretable Visual Understand-\ning. Proceedings of the AAAI Conference on Artificial Intelli-\ngence, 36(3): 3417–3425.\nZhou, L.; Kalantidis, Y .; Chen, X.; Corso, J. J.; and Rohrbach,\nM. 2019. Grounded Video Description. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR).\nZhou, L.; Xu, C.; and Corso, J. 2018. Towards Automatic\nLearning of Procedures From Web Instructional Videos. Pro-\nceedings of the AAAI Conference on Artificial Intelligence,\n32(1).\nZhou, L.; Zhou, Y .; Corso, J. J.; Socher, R.; and Xiong, C.\n2018. End-to-End Dense Video Captioning With Masked\nTransformer. In CVPR.\n3090",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.8294484615325928
    },
    {
      "name": "Computer science",
      "score": 0.7499361038208008
    },
    {
      "name": "Transformer",
      "score": 0.6457735300064087
    },
    {
      "name": "Paragraph",
      "score": 0.6110374927520752
    },
    {
      "name": "Natural language processing",
      "score": 0.5327513217926025
    },
    {
      "name": "Sentence",
      "score": 0.5312250256538391
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5014159679412842
    },
    {
      "name": "Speech recognition",
      "score": 0.3906790018081665
    },
    {
      "name": "Image (mathematics)",
      "score": 0.10463178157806396
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78715868",
      "name": "University of Arkansas at Fayetteville",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I91044093",
      "name": "Zayed University",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    }
  ]
}