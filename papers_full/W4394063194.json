{
    "title": "Enhancing missense variant pathogenicity prediction with protein language models using VariPred",
    "url": "https://openalex.org/W4394063194",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2104994040",
            "name": "Wei-Ning Lin",
            "affiliations": [
                "Institute of Structural and Molecular Biology",
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A2681899555",
            "name": "Jude Wells",
            "affiliations": [
                "University College London"
            ]
        },
        {
            "id": "https://openalex.org/A2102376419",
            "name": "Zeyuan Wang",
            "affiliations": [
                "Zhejiang University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2619834018",
            "name": "Christine Orengo",
            "affiliations": [
                "University College London",
                "Institute of Structural and Molecular Biology"
            ]
        },
        {
            "id": "https://openalex.org/A2941831677",
            "name": "Andrew C. R. Martin",
            "affiliations": [
                "University College London",
                "Institute of Structural and Molecular Biology"
            ]
        },
        {
            "id": "https://openalex.org/A2104994040",
            "name": "Wei-Ning Lin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2681899555",
            "name": "Jude Wells",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102376419",
            "name": "Zeyuan Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2619834018",
            "name": "Christine Orengo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2941831677",
            "name": "Andrew C. R. Martin",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2164004777",
        "https://openalex.org/W2109372707",
        "https://openalex.org/W2111326065",
        "https://openalex.org/W1980740976",
        "https://openalex.org/W2883972171",
        "https://openalex.org/W2059145105",
        "https://openalex.org/W1970413157",
        "https://openalex.org/W1903713963",
        "https://openalex.org/W2936956377",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W4300861393",
        "https://openalex.org/W2114029728",
        "https://openalex.org/W4280645135",
        "https://openalex.org/W3177622815",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3177500196",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W3179485843",
        "https://openalex.org/W4286500588",
        "https://openalex.org/W4225868104",
        "https://openalex.org/W3161612534",
        "https://openalex.org/W4293530896",
        "https://openalex.org/W4300817677",
        "https://openalex.org/W2017818880",
        "https://openalex.org/W2514779906",
        "https://openalex.org/W2431049581",
        "https://openalex.org/W4224988655",
        "https://openalex.org/W3093098684",
        "https://openalex.org/W4220723906",
        "https://openalex.org/W2074801341",
        "https://openalex.org/W2557981253",
        "https://openalex.org/W1984068087",
        "https://openalex.org/W2159522138",
        "https://openalex.org/W2999309192",
        "https://openalex.org/W3211728297",
        "https://openalex.org/W4385737488",
        "https://openalex.org/W4392044215"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:8136  | https://doi.org/10.1038/s41598-024-51489-7\nwww.nature.com/scientificreports\nEnhancing missense variant \npathogenicity prediction \nwith protein language models \nusing VariPred\nWeining Lin 1, Jude Wells 2, Zeyuan Wang 3, Christine Orengo 1* & \nAndrew C. R. Martin 1*\nComputational approaches for predicting the pathogenicity of genetic variants have advanced in \nrecent years. These methods enable researchers to determine the possible clinical impact of rare and \nnovel variants. Historically these prediction methods used hand-crafted features based on structural, \nevolutionary, or physiochemical properties of the variant. In this study we propose a novel framework \nthat leverages the power of pre-trained protein language models to predict variant pathogenicity. \nWe show that our approach VariPred (Variant impact Predictor) outperforms current state-of-the-art \nmethods by using an end-to-end model that only requires the protein sequence as input. Using one of \nthe best-performing protein language models (ESM-1b), we establish a robust classifier that requires \nno calculation of structural features or multiple sequence alignments. We compare the performance \nof VariPred with other representative models including 3Cnet, Polyphen-2, REVEL, MetaLR, FATHMM \nand ESM variant. VariPred performs as well as, or in most cases better than these other predictors \nusing six variant impact prediction benchmarks despite requiring only sequence data and no pre-\nprocessing of the data.\nA large portion of genetic variation is represented by single nucleotide variants (SNVs). SNVs occur in both \nprotein coding and non-coding regions, while protein-coding SNVs can be further divided into synonymous and \nnon-synonymous (nsSNVs) types. Synonymous SNVs do not change the amino acid sequence of the resulting \nprotein while non-synonymous SNVs (nsSNVs) do.\nMissense mutations, in which a single amino acid is replaced by another, are the most common type of nsSNV \n(the others leading to truncation or extension). There is a long history of using physicochemical and evolutionary \ninformation to predict whether a given missense mutation is disease-causing1–3. Nonetheless it remains a major \nchallenge to predict pathogenicity.\nTo tackle these challenges, many computational tools based on supervised machine learning techniques have \nbeen developed to predict the potential impact of variants. These compute deleterious scores based on dozens of \nbiological properties of variants, such as evolutionary  conservation3–5, biochemical properties of amino  acids6,7, \nand structural features of  proteins8,9.\nHowever, typically only a subset of variants can be annotated with all the features. This is especially true for \ntools that require a protein structure. There are 200 million protein sequences in the UniProt databank dated \n12th Oct 2022 (see: https:// www. ebi. ac. uk/ unipr ot/ TrEMB Lstats), but only 200,000 experimentally-determined \nprotein 3D structures stored in the Protein Data Bank (see: https:// resea  rch. rutge rs. edu/ news/ new- colla borat \nion- betwe en- rcsb- prote in- data- bank- and- amazon- web- servi ces- provi des- expan ded). This indicates that only \napproximately one in a thousand proteins have a reliable, experimentally resolved structure. For example, a com-\nmonly used predictor, Missense3D, can only structurally annotate 1965 and 2134 variants out of 26,884 disease-\nassociated and 563,099 neutral variants, using structures from the  PDB9. Even given the increase in structural \ncoverage using predicted protein structures from  AlphaFold210, the accuracy of AlphaFold2 in predicting the \nstructure of proteins with shallow multiple sequence alignments (MSAs) or orphan proteins, is  questionable11 \nand structure-based predictors may need to be re-trained for different levels of predicted quality obtained from \nAlphaFold2.\nOPEN\n1Division of Biosciences, Institute of Structural and Molecular Biology, University College London, London, \nUK. 2Department of Computer Science, University College London, London, UK. 3College of Computer Science and \nTechnology, Zhejiang University, Zhejiang, China. *email: c.orengo@ucl.ac.uk; andrew.martin@ucl.ac.uk\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:8136  | https://doi.org/10.1038/s41598-024-51489-7\nwww.nature.com/scientificreports/\nTraditional computational variant effect predictors, such as PolyPhen-2 (Polymorphism Phenotyping v2) 6 \nare some of the most popular tools for clinical researchers. PolyPhen-2 was trained on protein sequence align -\nments and exploits known or predicted protein three-dimensional structural information to predict the impact \nof nsSNPs on protein structure and function.\nFATHMM (Functional Analysis Through Hidden Markov Models)12 took a somewhat different approach to \nprevious predictors being trained using Hidden Markov Models, with a range of features including conservation \nand physicochemical properties, to predict the impact of genetic variations on protein function. It is capable of \nhandling a wide range of variations, including those in non-coding regions and is therefore valuable to clinical \nresearchers.\nIn addition to the aforementioned techniques, ensemble methods have also been developed for predicting \nthe pathogenicity of missense variants. REVEL (The Rare Exome Variant Ensemble Learner) and MetaLR, which \nuses Logistic Regression (LR), stand out as two representative ensemble models that were considered the best in \nthe Critical Assessment of Genome Interpretation (CAGI)  competitions13. REVEL, an ensemble method aimed \nat predicting the pathogenicity of missense variants, combines scores from 13 distinct pathogenicity prediction \ntools. MetaLR was developed by integrating scores from 10 different tools.\nIn the latest genome interpretation assessment competition (CAGI-6), a novel predictor named 3Cnet was \nranked top in the SickKids clinical genomes and transcriptomes panel (see: https:// www. 3bill ion. io/ blog/ 3bill ion- \nwins- in- cagi6-a- global- artif cial- intel ligen ce- genome- inter preta tion- conte st/).  3Cnet14 is a deep artifcial neural \nnetwork Long Short-Term Memory (LSTM)-based model, which utilises multiple protein features including \nMSAs, amino acid physicochemical properties, and other features such as motifs and active sites as the  input14. \nAs PolyPhen-2, FATHMM, REVEL, MetaLR and 3Cnet are all trained for predicting the clinical signifcance of \nmissense variants, and 3Cnet has reported state-of-the-art performance in this feld, we selected these as state-\nof-the-art methods against which to benchmark our approach.\nMost recent novel protein data-representation approaches take inspiration from language models that have \nyielded ground-breaking improvements in natural language processing (NLP). In particular, the Transformer \nneural network  architecture15, can learn contextualised word representations from a large amount of unlabelled \ntext data and has achieved state-of-the-art performance for several NLP tasks. In the life sciences, most protein \nlanguage models (pLMs) use Transformer architectures which were developed for NLP , but were subsequently \ntrained on protein sequences with the goal of deciphering the ‘natural language’ of proteins.\npLMs such as  ProtT516, ESM-1b17, ESM-1v18 and ESM-2 19 have been trained on a large corpus of protein \nsequences with the objective of predicting masked amino acids given the context of the non-masked sequence. \nThese pLMs are pre-trained using a masked language modelling objective. During the pre-training, 15% of \nresidues were randomly masked out from the input sequences, and the model predicts which amino acid type \nis most likely to be present at each masked position.\nThis results in a learned feature representation called an ‘embedding’ for each residue position in the protein \nsequence. The embeddings of these sequence-based pre-trained models have been shown to encode useful bio-\nphysical information, such as residue  conservation20 amino acid hydrophobicity, protein structure  class16 and \nprotein functional  properties21.\nEven though these models were pre-trained without using evolutionary information, it has been shown that \nthe methods achieve a similar performance to MSA-based models for various tasks while also reducing the \ncomputational cost.\nRecent studies have used experimental data to evaluate the performance of pLMs in predicting the functional \neffects of  variants17,20,22,23. However, to date, only one study (‘ESM variant’) has used a pLM to predict the clinical \nsignifcance of a  mutation22. ‘ESM variant’ uses the ESM-1b pre-trained pLM without requiring any supervised \ntraining. Given that ESM-1b was trained to predict the likelihood of each amino-acid type at each position, it \nis possible to use these likelihoods as a proxy for how well tolerated an amino-acid change is likely to be at the \nmutation site. ‘ESM variant’ constructs a pathogenicity score for a given mutation by using the ESM-1b likeli-\nhoods for the wildtype and mutant type amino acids at the mutated position (Fig. 1).\nThis study extends the research on pLMs by proposing a novel methodology and architecture. We conduct \na comparative analysis of alternative pLM models including the recent ESM-1v and ESM-2 models which were \ntrained on larger protein datasets. We compare performance of our model against two traditional machine \nlearning-based models including Polyphen-2 and FATHMM, and two novel deep learning based-methods ‘ESM \nvariant’ and 3Cnet using the ClinVar dataset. One downside of supervised machine learning models is that they \ncan be prone to overftting such that they can perform poorly with new sequences not seen during the training \nstage, and where the training data contains genes for which all variants have the same class label (benign/patho-\ngenic). A previous study identifed this as the ‘Type 2 data circularity’  problem24. We investigate whether the \npredictors are prone to bias from Type 2 data circularity using two additional benchmarks: SwissvarFilteredMix \nand VaribenchSelectedPure.\nOur model, VariPred, uses a novel twin-network pLM architecture combined with a trained classifcation \nmodule to achieve the highest classifcation performance on two variant classifcation benchmarks. The twin \nneural network framework (sometimes called a Siamese network) describes an approach where two compara-\nble inputs are each passed through the same network. In our case, we pass the mutant and wildtype sequences \nthrough the pLM to generate embeddings for each residue position. Subsequently, we concatenate the two embed-\ndings for the wildtype and mutant type residues that occur at the mutation position. These paired embeddings are \nused as input features to a lightweight feed-forward classifcation module which is trained on the labelled data. \nAs a result of the transformer network architecture and the pLM’s pre-training objective, contextual information \nfrom the entire sequence is incorporated into the per-residue embedding. At the same time, selecting only the \nembeddings for the residues at the mutation position allows the classi fcation module to focus on information \nwhich is specifc to the mutation site.\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:8136  | https://doi.org/10.1038/s41598-024-51489-7\nwww.nature.com/scientificreports/\nResults\nComparing the performance of LLR, embeddings and LLR + embeddings\nThe ‘ESM variant’  method22 only uses the Log-likelihood ratio (LLR) feature derived from ESM-1b to predict the \nclinical signifcance of missense variants. Here we evaluate the performance for this task, using LLRs derived from \ntwo other pLMs: ESM-1v and ESM-2.\nWe frst tested models using the ClinVar test set. For pLMs which only use the LLR for prediction, ‘ESM \nvariant’ (using ESM-1b) has the best predictive performance, with a Matthews correlation coefficient (MCC) \nscore of 0.600, followed by ESM-1v (0.562) and ESM-2 (0.430) (Fig. 2).\nIn addition to using the LLR threshold to differentiate pathogenic variants from benign, our VariPred predic-\ntor uses a shallow Feed-forward Neural Network (FNN) trained on the pLM embeddings for the wildtype and \nmutant sequence. We observe that this approach signifcantly improves the performance of all protein language \nmodels. ESM1b remains the best performing model, with an MCC score of 0.746, followed by ESM-2 (0.734) \nand ESM1v (0.721) (Fig. 2).\nWhen we combine the LLR together with the embeddings, the performance of all models improved further. \nESM-1b still has the best performance with an MCC score of 0.751, ESM-2 scored 0.748, and ESM-1v achieved \nFigure 1.  Illustration of how to calculate the log-likelihood ratio (LLR) from ESM models. The input for the \npLM is an amino-acid sequence while the output is the log-likelihood ratio calculated based on the probabilities \nof the wildtype amino acid and the mutant amino acid occurring at the given position. Note, the heatmap \nshowing in this fgure was artifcially generated for demonstration purposes. Pr, Probability;  wt148 = P , probability \nthat the residue occurring at the 148th position in the input wildtype (wt) protein sequence is a Proline (P); \n mt148 = G, probability that the 148th residue was mutant type (mt) Glycine (G).\nFigure 2.  Comparison between models under different testing situations. Comparison of the LLR and \nembedding features for three protein language models. The baseline is using ‘Only LLR’ to predict pathogenicity \nof variants; For ‘Only embeddings’ we used amino acid embeddings as input to a shallow Feed-forward neural \nnetwork (FNN) to predict the pathogenicity of variants; ‘LLR + embeddings’ concatenates the LLR feature as \nthe last column of the amino acid embedding matrix, and then performs variant classifcation by using this \nextended matrix as input to the FNN. LLR: Log-likelihood ratio.\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:8136  | https://doi.org/10.1038/s41598-024-51489-7\nwww.nature.com/scientificreports/\n0.729 (Fig. 2). Therefore, in the following experiments to compare the performance with other tools, we chose \nESM-1b as the feature extractor for our model, VariPred. We trained our model with both embeddings and \nLLRs as input.\nComparison between models using the ClinVar test set\nTo evaluate VariPred’s performance on clinical data, we compared the performance of VariPred with other tools \non the ClinVar test set.\nComparing against other methods, VariPred has the best performance with an MCC of 0.714. 3Cnet is closest \nto matching the performance of VariPred with an MCC of 0.673, followed by ‘ESM variant’ (MCC = 0.649), two \nensemble predictors REVEL and Meta-LR2 with MCC scores of 0.537 and 0.513, respectively, and PolyPhen-2 \n(MCC = 0.521), while FATHMM has the lowest performance with an MCC of 0.379 (Fig. 3). In addition, VariPred \nonly requires protein sequence information as input, while 3Cnet requires multiple features including MSAs, \namino acid physicochemical properties, and protein features such as motifs or active sites.\nWe observed that on the ClinVar dataset, 3Cnet’s AUC-ROC is higher than that of VariPred, while its MCC \nis lower. This discrepancy might stem from the inherent differences in how these two metrics evaluate model \nperformance. AUC-ROC is calculated as the area under the curve plotted with True Positive Rate (TPR) and \nFalse Positive Rate (FPR) at various thresholds. However, AUC-ROC does not consider the shape of the curve \nor the threshold used for classifcation. Rather it focuses on the model’s overall performance across all possible \nthresholds. Consequently, for imbalanced datasets, the ROC curve may provide an overly optimistic performance \nestimate. In contrast, MCC considers all four quadrants of the confusion matrix at a single threshold (as would \nbe used in an actual predictor), making it a balanced metric even in cases of class distribution imbalance.\nHowever, another reason could be that the default threshold for 3Cnet was not optimal for these datasets. We \ntherefore assessed the effect of adjusting the 3Cnet threshold to improve performance and observed that when \nthis was done there was no signi fcant difference in the MCC values between VariPred and 3Cnet (with 3Cnet \nobtaining a slightly higher performance, see Supplementary).\nIn addition, to test whether the model’s performance is artifcially inflated by simply learning and predicting \nthe majority class label for certain genes we created an additional ‘balanced label’ subset of the ClinVar valida -\ntion set (see “Methods”: “Additional ‘balanced-label’ test set”) based on the ClinVar validation set, ensuring an \nequal number of positive and negative variants for all genes. Results on this subset showed that both models had \nsimilar AUC-ROC (0.88), while VariPred has a much higher MCC (0.623) compared with that of 3CNet (0.567).\nAdjusting for gene overlap in training and test sets\nThe common standard in the existing literature on supervised models for variant impact prediction is to allow \nthe same gene/protein to occur in the training and test sets as long as there is no overlap in the variant. This is the \napproach taken in the 3Cnet paper. The justifcation is that a clinically useful model should be able to leverage \ninformation from homologous and identical proteins to infer the pathogenicity of previously unseen variants. To \ninvestigate whether VariPred’s prediction accuracy is inflated due to protein homology, we retrained the VariPred \nESM-1b model on a new split of the ClinVar data and evaluated the performance on a non-redundant test set (no \nsequence identity above 30% across the train and test datasets). We fnd that the MCC changes from 0.75 to 0.65 \n(ROC AUC 0.93 to 0.91) when changing from the original train-test split to the no-homology split. This gives a \nmeasure of the model’s expected performance when predicting on genes without any annotation in the training \ndata. While we are unable to benchmark the other supervised models on the non-homology train/test split, as \nFigure 3.  Comparing the performance of pathogenicity predictors using the ClinVar validation set. (A) \nAUC-ROC curve plot for the seven predictors. (B) MCC score for the predictors being tested in this study. The \nerror bars depict the 95% confdence interval for the MCC score, as computed from 10,000 bootstrap sampling \niterations.\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:8136  | https://doi.org/10.1038/s41598-024-51489-7\nwww.nature.com/scientificreports/\nthis would require retraining the other models, we note that VariPred’s performance on the no-homology split \nis still better than the best unsupervised method.\nResults for Type 2 data circularity test\nWe compared all predictors with two public benchmarks, the SwissvarFilteredMix and VaribenchSelectedPure \ntest sets, to evaluate if any of the models are affected by the Type 2 data circularity problem.\nIn this evaluation, FATHMM has the highest accuracy in VaribenchSelectedPure, while having a much higher \nperformance in VaribenchSelectedPure (MCC = 0.327) than SwissvarFilteredMix set (MCC = 0.183), which is \nconsistent with previous  research25 (Fig. 4). In addition, MetaLR’s performance in VaribenchSelectedPure, for \nwhich the MCC score equals 0.221, is higher than SwissvarFilteredMix set (MCC = 0.187). This suggests that \ntraditional machine learning-based methods, FATHMM and MetaLR, are both affected by the Type 2 data \ncircularity problem where the model is learning features of the gene and ignoring the specif cs of the variant \n(see “Methods”: “Additional ‘balanced-label’ test set ” for a more detailed explanation). In contrast, our model, \nVariPred, has the highest performance in SwissvarFilteredMix (MCC = 0.466) (Fig. 4), indicating that VariPred \nis not confounded by the Type 2 data circularity problem.\nWhile PolyPhen-2 demonstrates comparable performance to 3Cnet using the SwissvarFilteredMix set, it is \ncrucial to note that Swiss-Prot forms part of the training data for PolyPhen-2. Consequently, the observed MCC \nscore of 0.291 for PolyPhen-2 applied to SwissvarFilteredMix may be inflated due to train/test overlap.\nAmong the three deep-learning-based tools, all predictors demonstrated superior performance on Swiss-\nvarFilteredMix as compared to VaribenchSelectedPure. Notably, 3Cnet exhibits the lowest MCC score among \nthem, registering a value of 0.331.\nDiscussion\nWe tested three different pLMs (ESM-1b, ESM-1v, ESM-2) and showed that ESM-1b was the best predictor for \npathogenicity of single-position missense variants (Fig.  2). Using the ClinVar test set, our VariPred predictor \nwhich combined LLR and residue embeddings generated by ESM-1b has the best performance achieving an MCC \nof 0.714 and AUC-ROC of 0.928 without using any additional biological features and not being confounded by \nType 2 data circularity (Fig. 3).\nIn principle, since ESM-1v and ESM-2 were pre-trained using a much larger protein sequence dataset, they \nshould have a broader view on the mutability landscape of proteins than ESM-1b. The ESM-1v and ESM-2 \nauthors state that both ESM-1v and ESM-2 are sufficient to conduct the missense mutation pathogenicity predic-\ntion without any further  training18,23. Nonetheless, we observed better performance using the ESM-1b model. \nFigure 4.  Type 2 data circularity problem test. Comparing the performance of pathogenicity predictors using \nthe public test set, SwissvarFilteredMix and VaribenchSelectedPure. Predictors were all tested in the common \ndatasets of both SwissvarFilteredMix and VaribenchSelectedPure. The error bars depict the 95% confdence \ninterval for the MCC score, as computed from 10,000 bootstrap sampling iterations.\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:8136  | https://doi.org/10.1038/s41598-024-51489-7\nwww.nature.com/scientificreports/\nWe speculate that this may be a result of the ESM-1b pre-training dataset being more closely aligned with the \nrelatively narrow set of (human only) proteins that are included in ClinVar.\nReports suggest that for predicting the functional (rather than clinical) effects of variants, which are in the \nform of a continuous scalar value, ESM-1v and ESM-2 have a better  performance18,23. However, recent comments \nsuggest that ESM-1b performs better in some other tasks, such as structure  prediction26. In this study, we showed \nthat ESM-1b outperforms two other state-of-the-art predictors in predicting the binary clinical signi fcance of \nmissense variants.\nIn the comparison between representative traditional machine learning-based methods (PolyPhen-2 and \nFATHMM), ensemble models incorporated with traditional machine learning methods (REVEL and MetaLR) \nand deep learning-based models (VariPred, 3Cnet and ‘ESM variant’) for the task of clinical variant impact \ndetection, the accuracy of models based on deep learning models is far higher than that of models based on \ntraditional machine learning methods (Fig.  3). The lowest MCC score in the deep learning model, ‘ESM vari-\nant’ , is higher than the relatively high MCC score of REVEL in traditional machine learning methods by around \n30%, while supervised deep learning methods (VariPred and 3Cnet) out-perform the unsupervised learning \nmethod (‘ESM variant’).\nIn the Type-2 data circularity test, we noticed that traditional machine learning based models are prone to \nthe bias from the Type-2 data circularity problem. FATHMM, which was not trained on Swiss-Prot dataset, \nhas the highest accuracy in VaribenchSelectedPure (MCC  = 0.327) compared with the SwissvarFilteredMix set \n(MCC = 0.071) while the ensemble model MetaLR also demonstrates a higher performance in VaribenchSelect-\nedPure than SwissvarFilteredMix set, of which the MCC scores are 0.221 and 0.187, respectively (Fig. 4). Mean-\nwhile, none of the deep learning-based methods have a better performance in the VaribenchSelectedPure set. In \nparticular, our model, VariPred, has the highest performance in SwissvarFilteredMix (MCC = 0.466) but a lower \nperformance in VaribenchSelectedPure (MCC = − 0.145) (Fig. 4), indicating that VariPred is not confounded by \nthe Type 2 data circularity problem. This suggests that, compared with deep learning-based methods, traditional \nmachine learning based predictors are more easily affected by the Type 2 data circularity problem where the \nmodel is learning features of the gene and ignoring the specifcs of the variant.\nFrom the test results on the ClinVar ‘balanced label’ subset, we found that VariPred is less susceptible than \n3Cnet to simply predicting the majority class label for genes that have been included in the training data. Accord-\ning to our evaluation on another balanced dataset, SwissvarFilteredMix, 3Cnet’s performance on either MCC or \nAUC-ROC was signifcantly behind VariPred (Figs. 4, 5). Thus VariPred is the top performing method in two \nout of three test sets (ClinVar-class-balanced and SwissVar) and is competitive on a third test set (ClinVar) where \nwe observe no statistically signifcant difference between VariPred and 3Cnet (Fig. 5).\nPreparing the input features for 3Cnet is a difficult task. 3Cnet relies on features based on 85 biophysical \nproperties retrieved from the SNVBox database. However, not all variants can be mapped with features from \nSNVBox as it has not been updated since 2011, resulting in missing sequences and problems with changed \nRefSeq IDs. This may lead to uncertainty in the consistency between the retrieved feature and the data entry. \nAdditionally, only the NP codes which are included in the provided transcript ID list can be transformed into \na 3Cnet prediction dataset.\nIn comparison to other models, VariPred requires only the most fundamental information for each data entry: \nthe wildtype protein sequence and mutation information including which residue is being mutated at which posi‑\ntion in the wildtype sequence into which mutant amino acid. Without the need for further dataset preparation, \nsuch as MSA construction or feature retrieval, making predictions on a dataset of 5000 new variants take 30 min \nwith a 12 GB GPU, such as the Nvidia GTX 1080Ti.\nFigure 5.  AUC-ROC curve plot for VariPred and 3Cnet on SwissvarFilteredMix dataset.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:8136  | https://doi.org/10.1038/s41598-024-51489-7\nwww.nature.com/scientificreports/\nA key feature of VariPred, fne-tuned based on pLMs, is its ability to ft the pathogenicity data extremely well \nusing only sequence data for training. When compared with 3Cnet, VariPred removes dependencies on MSAs \nand externally computed features, requiring only the protein sequence as input. Thus, while other prediction \nmodels directly exploit structural and/or evolutionary information, their performance in pathogenicity predic-\ntion was not as good as that of large language models relying solely on sequence information. Our VariPred \nresults appear to validate previous research suggesting that pLMs pre-trained on a vast array of sequences have \nimplicitly learned the  structural16 and evolutionary  information20 of proteins.\nWe note that performance on the SwissvarFilteredMix and VaribenchSelectedPure test sets was lower than \nperformance on ClinVar for all models, which is consistent with previous  studies25. Comparing the SwissVarFil-\nteredMix dataset with the latest ClinVar dataset (2022-12) we found that 17% of entries have an inconsistent label \n(e.g. ‘uncertain’ in SwissVarFilteredMix versus ‘positive’ in ClinVar) and 81% do not have a label in the ClinVar \ndataset, whilst 2% of variants have the opposite label. Meanwhile, VaribenchSelectedPure also has label discrep-\nancies when compared with ClinVar. Of the 69 variants shared between the ClinVar and VaribenchSelectedPure \ndatasets, 22 have contradictory labels. The reason is not immediately clear, but both VaribenchSelectedPure and \nSwissVarFilteredMix are old datasets. As all models perform badly in these two datasets, we suspect that the \nlabels of the same variants in these two tests may have been changed in ClinVar.\nThis could explain why the performance of all predictors drops in these two datasets, as the inconsistent data \nwould reduce the evaluated prediction performance of the predictors. This could be a result of different criteria \nfor labelling (e.g. how partial penetrance variants are classifed) or labels changing owing to newly available \nevidence. We note that other authors have been critical of using the VariBenchSelectedPure dataset for bench-\nmarking. For example, quoting from Wang and  Wei25: “It is useful to note that the performance of the tools on \nVaribenchSelectedPure cannot be compared directly because this dataset was biased and the labels of variants \nin this dataset were at least partially arti fcial. It was only constructed for testing whether a prediction method \nwas confounded by type 2 circularity. ”25.\nInconclusive and contradictory pathogenicity labels are an argument in favour of unsupervised methods such \nas ‘ESM variant’ . Even though we observed lower performance when compared with supervised methods such \nas VariPred and 3Cnet, it is important to note that the unsupervised methods are not prone to bias introduced \nby training dataset selection and labelling issues.\nVariPred is speci fcally trained for predicting pathogenicity rather than functionality. This distinction is \ncrucial, with pathogenicity being a more downstream concern, while functionality deals with more upstream \nmechanisms. Consequently, our training prioritises pathogenicity data over functionality datasets. Nevertheless, \nwe postulate that our research can be complemented by in vitro functional protein assays, including deep muta-\ntional scanning experiments, which may provide additional insights into disease-causing variants. Additionally, \nour homology tests have demonstrated that when VariPred is retrained by training with sequence similarity level \nless than or equal to 30% compared to that in the test set, it still achieves remarkable performance on the test set \n(MCC = 0.645, AUC-ROC = 0.907). Therefore, we believe that a retrained VariPred can offer valuable insights \nabout disease causing variants on a gene not previously encountered.\nSeveral researchers have posited that variant predictors might yield enhanced accuracy for specif c genes or \n diseases25,27. Thus, in the future, we will evaluate VariPred’s performance on specifc genes associated with various \ndiseases as well as differential pathogenicity prediction—i.e. predicting different pathogenic phenotypes caused \nby mutations in the same  protein27.\nCurrently, VariPred only uses sequence information to predict pathogenicity. In future we will evaluate the \neffect of including structural information in the predictor. A similar strategy of predicting mutation effects \nusing protein embeddings has been implemented for aiding engineering of enzymes by directed evolution and \nfor aiding protein  design28,29, but has not yet been explored in the prediction of the clinical signifcance of mis-\nsense mutations. Incorporating both sequence and structural information is likely to improve VariPred’s ability \nto classify missense variants.\nApplying a more biologically meaningful data augmentation strategy may add more diversity into the training \nset. Conservation information is one of the most powerful features for predicting protein stability and functional \n effects30. In the study of 3Cnet, the artifcial pathogenic-like variants were generated simply by considering the \namino acid frequency and the number of gaps. However, a good conservation scoring scheme depends on mul-\ntiple components, of which the most important include amino acid frequency, residue similarity (biophysical \nproperties), sequence similarity (considering sequence redundancy and MSA depth), the number of gaps in the \nMSA, and the concept of ‘compensated pathogenic mutations’ (CPDs), which refers to mutations occurring in \ndifferent species that are tolerated because of compensating  mutations31. Indeed, it may be possible to exploit the \nLLR output from ESM-1b to suggest compensatory mutations and use the fnal VariPred output to evaluate their \neffect. Therefore, in the future, it may be worth investigating whether such a combination of data augmentation \nand synthetic data strategies can further improve the performance of VariPred.\nIn summary, VariPred only requires the native and mutated sequence and, using protein language model \nencoding, is able to outperform state-of-the-art methods that use features including structural information and \nmultiple sequence alignments.\nMethods\nDataset preparation\nTo avoid having to retrain 3Cnet we opted to use the same train/test split as the 3Cnet authors. The only modif-\ncations that we made were to exclude the 3Cnet simulated data from our model’s training set and remove some \nvariants from the test set which appear to have been inadvertently included by the 3Cnet authors in both the \ntrain and test sets.\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:8136  | https://doi.org/10.1038/s41598-024-51489-7\nwww.nature.com/scientificreports/\nTraining set\nThe training dataset used in 3Cnet consists of three parts: (1) clinical data stored in the clinical database ClinVar, \n(2) common missense variants retrieved from the population database GnomAD, and (3) a set of simulated \npathogenic data generated by the 3Cnet authors. The simulated data are based on amino acid conservation deter-\nmined from MSAs, built using sequences from the RefSeq database. We chose to exclude the simulated data from \nour training dataset and work with the subset of the 3Cnet training data sourced from ClinVar and GnomAD.\nThe ClinVar dataset used by 3Cnet was downloaded from the ClinVar database via the FTP link (version \n2020-4). In total, 72,470 curated missense variants were selected according to the criteria of known molecular \nconsequences and reliable review status. Specifcally, only variants with the GRCh37 assembly version and \nlabelled with ‘missense variants’ were collected, and those with unreliable review status, containing strings with \n‘no assertion’ , ‘Conflicting’ , ‘no interpretation’ , and ‘Uncertain’ were all excluded. Data labelled with ‘pathogenic’ \nor ‘likely pathogenic’ were all considered as pathogenic variants. Similarly, variants with any submission reported \nas either ‘benign’ or ‘likely benign’ were defned as neutral. After fltering out low-quality data, 72,470 variants \n(22,337 pathogenic and 50,133 benign) remained.\nThe GnomAD dataset prepared by the 3Cnet group (fle downloaded using FTP: gnomad.exomes.r2.0.2.sites.\nvcf.gz) consists of 60,614 exome-derived variants. These variants have a minor allele frequency (MAF) higher \nthan 0.1% and each was fltered by requiring a ‘PASS’ annotation, which ensures the quality of the variant, i.e. high \nconfdence genotypes. Since these variants are found in the genome of supposedly healthy people, they are typi-\ncally regarded as benign  variants32. However, even though the 3Cnet authors regard variants with a MAF ≥ 0.1% \nas neutral, we cannot exclude the possibility that some of these variants have undetected (or partial penetrance) \npathogenic effects since a MAF of 1% is usually used in defning a ‘polymorphism’ .\nEach datapoint included in these three datasets was annotated with a specifc RefSeq NP code (protein record \nidentifer in the protein sequence database) and the mutant information in the HGVSp term by the 3Cnet group, \ne.g. NP_689699.2:p.Gly56Ser. For each RefSeq NP code, the corresponding curated wildtype protein sequence \nwas also provided by the 3Cnet group. For each variant in the dataset, the input for the model consists of both \nwild-type and mutant sequences, target mutated position, wildtype amino acid and the mutant amino acid. \nThe fnal output of the model is a binary label, where 0 indicates that the mutation is benign and 1 indicates \npathogenic.\nTest set\nWe noted that some variants (the same gene with the same mutation) were repeated between the 3Cnet train \nand test datasets. This problem arises from splitting the data based on variant information given in the HGVSp \nterm. We found that some proteins annotated with different NP codes are in fact the same isoform, with the \nsame wildtype protein sequence. To avoid having to retrain 3Cnet we chose to remove the 1767 duplicated vari-\nants from the ClinVar test set and a further 900 variants which were duplicated between the GnomAD training \ndataset and the ClinVar test set.\nAs a result of removing these duplicates, the processed training dataset consists of 72,466 variants from \nClinVar and 59,018 variants from GnomAD. In total 17% of variants were labelled as pathogenic. The test set is \ncomprised of data from ClinVar only and consists of 21,125 entries with 45% of variants labelled as pathogenic \n(Supplementary Table 1). Here, we ensure that the training and test sets do not have any variants which are the \nsame.\nWhen fetching pre-computed prediction results from FATHMM, PolyPhen-2, REVEL and MetaLR, we noted \nthat some variants were not available. To ensure all methods were benchmarked with the same test set, we tested \nall predictors on the common test set, consisting of 12,853 variant datapoints in total.\nAdditional ‘balanced‑label’ test set\nIn the comparative analysis of performance metrics, it is observed that the AUC-ROC score of 3Cnet surpasses \nthat of VariPred, whereas the MCC score of 3Cnet is inferior. This divergence in performance metrics might be \nattributed to a specifc characteristic of the dataset used in the full set. Notably, certain genes within this dataset \nexclusively comprise variants of a singular type, exclusively classifed as either benign or pathogenic. This situa-\ntion suggests that the predictive model may predominantly assimilate information at the gene level rather than \nthe variant level. In essence, the model tends to predict all variants of a particular protein as either benign or \npathogenic. This phenomenon aligns with what is known as a type 2 data circularity problem.\nTo test whether model performance was artifcially inflated by learning to predict the majority class for genes \nwith imbalanced class labels, an additional subset was constructed, termed the ‘balanced-label’ subset, derived \nfrom the Clinvar validation set. This subset was constructed to include only those genes that exhibit both classes \nof variants and a strict balance was enforced in the number of each label for these genes. To illustrate, genes that \nsolely consisted of variants labelled as 0 or as 1 were excluded from the test set. In the genes that remained, an \nequilibrium was ensured by taking a random subsample from the majority class of the same size as the minority \nclass.\nTesting for Type 2 data circularity bias\nGrimm et al.24 point out that effective benchmarking of clinical variant prediction can be confounded by circu-\nlarity arising from overlap between the train and test sets. Type 1 data circularity arises when the same variant \nis included in the train and test set. Type 2 circularity arises from the same genes being included in the train \nand test set, even where the individual mutations are distinct. Type 2 data circularity bias is a particular prob -\nlem where the data includes genes where labels are imbalanced towards one class. This scenario can give rise \nto predictors that ignore the specifc details of the mutation merely recognising genes which are oversampled \n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:8136  | https://doi.org/10.1038/s41598-024-51489-7\nwww.nature.com/scientificreports/\nas pathogenic or benign in the training data. In order to assess the models’ propensity to overft to genes in this \nway, we followed the approach of Grimm et al. and tested predictors using the SwissvarFilteredMix dataset and \nthe VaribenchSelectedPure public benchmark. These datasets are used together to test whether performance is \nconfounded by type 2 data circularity.\nThe SwissvarFilteredMix dataset consists of proteins with at least one type of label from each class. By con -\ntrast, in the VaribenchSelectedPure dataset, each protein only has one type of variant class, either all benign or \nall pathogenic. If a model learns to predict based on characteristics of the gene and ignores the speci fcs of the \nvariant, then it will typically show inflated performance on the VaribenchSelectedPure dataset while showing low \nperformance on SwissvarFilteredMix. It is important to emphasise that the performance of tools on Varibench-\nSelectedPure should not be directly compared, as this dataset has inherent biases and its variant labels are, to \nsome extent, artifcially generated. The primary purpose of creating this dataset was to test whether a prediction \nmethod could be confounded by type 2 circularity. The 3Cnet and VariPred training set includes 67% of the genes \nthat were in the VaribenchSelectedPure test set, and therefore have the type 2 circularity that we are trying to test.\nThe VaribenchSelectedPure and SwissvarFilteredMix datasets contain information on chromosome num-\nber, base substitution position, reference nucleotide base, altered nucleotide base, Ensembl protein ID and the \nground-truth label. Some sequences are not consistent with the mutation information, possibly because there \nhas been a new isoform since the two benchmarks were generated in 2016. Therefore, we annotated NM codes \n(mRNA record identifers in the Nucleotide database) for each variant using the latest version of the ANNO -\nV AR  software33, with the transcript-based annotation set for the RefSeq Gene (assembly version hg19; updated \n2020-08-17 at UCSC). We then retrieved the corresponding protein isoform sequences (wildtype sequences) \nusing the Entrez.efetch module included in Biopython (version 1.80) with Python 3.9. Using the original protein \nisoform sequences and the corresponding variant information, we generated the mutant sequences for each \nvariant. This gave a SwissvarFilteredMix test set with 1153 benign variants and 1023 pathogenic variants, and a \nVaribenchSelectedPure set with 3629 benign variants and 2122 pathogenic variants.\nThere are problems with 3Cnet in generating features for some variants from these two benchmark datasets. \nTo predict a variant’s pathogenicity with 3Cnet requires three components: the HGVSp term including the NP \ncode and the mutation information, the NP code corresponding to the wildtype sequence, and 85 biological fea-\ntures retrieved from the SNVBox  database34. However, this information is not recorded in the SNVBox database \nfor some of the variants. Consequently, 3Cnet is not able to give a prediction for these variants. We therefore \ndropped these variants, leaving 1742 variants in the SwissvarFilteredMix and 5159 variants in VaribenchSelect-\nedPure test sets for the evaluation of 3Cnet. We ensured that no variant was repeated between the train and test \nsets i.e. no type 1 circularity.\nMoreover, owing to the absence of some human genes in both the precomputed datasets of PolyPhen-2 and \nFATHMM, we tested all tools using the common variants between SwissvarFilteredMix and VaribenchSelect-\nedPure sets. Eventually, 1603 out of 2176 variants from SwissvarFilteredMix (865 benign vs. 738 pathogenic) \nand 1837 variants from the VaribenchSelectedPure set (1760 benign vs. 77 pathogenic) remained that could be \ntested across all predictors.\nFeature extraction and model setup\nIn order to identify the most suitable pLM for differentiating pathogenic variants from benign, we tested the \nmost widely used pre-trained models including ESM-1b, ESM-1v and ESM-2. ESM-2 has several versions with \ndifferent parameter sizes, ranging from 8 ×  106 to 15 ×  109. According to a previous study, the performance of the \nmodel does not increase with model size, and models with 650 ×  106 parameters appear to have the best ability to \nextract per-residue  features23. Therefore, we chose ESM-2 with 650 ×  106 parameters for our analyses.\nExtract embeddings by pLMs\nESM-1b and ESM-1v are BERT-style encoder-based Transformers, which limit the input length to 1022 amino \nacids. ESM-2 does not have this sequence length limitation, but using longer sequences is computationally pro-\nhibitive. Moreover, the Rotary Position Embedding strategy used in ESM-2 only considers the word embeddings \nand their neighbours, limiting any advantage of larger windows. Therefore, we designed a sequence truncation \nstrategy which is consistent with such encoding methods and limits the maximum length to 1022 in all 3 models.\nThe official ESM tokenizer package pads the length of shorter sequences to 1022 internally, but transforms the \nlength back to the true sequence length during data processing. For sequences longer than 1022, if the mutation \nis within 1022 residues of either the N-terminus or the C-terminus, 1022 residues counting from the end were \nretained; if the mutation index occurs more than 1022 residues from both termini, 510 neighbours from the \nN-terminal side and 511 from the C-terminal side of the mutated residue were selected, resulting in sequences \nhaving 1022 residues.\nTransformer-based pLMs provide features in two forms: the probability of each amino acid type occurring \nat each position in the sequence, and a dense vector-embedded representation of each position in the sequence. \nOwing to the self-attention mechanism of the Transformer architecture, the embedding can incorporate con -\ntextual information from the entire 1022 residue window. We followed a similar approach to ESM-Variant, and \nfor each mutated position we extracted the log likelihood ratio (LLR) and the embedded representation of the \nwild-type and mutant type residue.\nThe LLR was calculated using the ESM likelihood of the mutant and wildtype amino acid at the target posi -\ntion conditioned on the model receiving the wildtype sequence as input, using the formula shown in Fig.  1.\nAmino acids which frequently occur at a target position, typically have a comparatively high likelihood. Thus, \nif the mutant type’s likelihood is signifcantly lower than the wild type, this serves as an indicator that the muta-\ntion is problematic, while mutant residues with high likelihoods typically have similar physiochemical properties \n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:8136  | https://doi.org/10.1038/s41598-024-51489-7\nwww.nature.com/scientificreports/\nand are therefore less likely to affect protein stability or function. The ‘ESM variant’  method22, which uses the LLR \ngenerated by ESM-1b to discriminate variant pathogenicity, suggested that an LLR threshold of − 7.5 is sufficient \nto detect pathogenic variants. Considering the potential variability of ’ cross-points’ i.e. the overlap in distribu-\ntions between two labels in the validation set, across different models and across different test sets, we optimised \nthe performance of models via calculating the speci fc cross-points for the three models: ESM-2, ESM-1v, and \nESM-1b, with respective values of − 7.513, − 6.660 and −8.210—on the training set utilised in our study (Fig. 6). \nThe ESM model with the best performance will then serve as the feature extractor for VariPred. Each position in \nthe sequence is represented by embeddings of a 1280-word dimension, extracted from the chosen ESM model.\nThe schema of data processing and model generation is shown in Fig.  7. To obtain the embeddings of each \nsequence pair (wildtype and mutant protein sequences), all sequence pairs were fed into the pLM. For example, \nif a wildtype sequence consists of 100 amino acids, two embedding matrices (one for amino acids in the wildtype \nsequence, the other for amino acids in the mutant sequence) with dimensions 100 × 1280 would be generated \n(Fig. 7A).\nWe hypothesised that the embedding of the target amino acid at the mutation position would be the most \ninformative. Therefore, we only took the embedding of the amino acid at the mutated position from both wildtype \nand mutant sequences. These two embeddings were then concatenated horizontally such that each data entry is \nrepresented as a vector with dimensions 1 × 2560. Feeding the training dataset (192,575 entries) into the ESM-1b \npre-trained model will generate a wildtype-mutant concatenated amino acid embedding representation matrix \nwith a size of (192,575 × 2560) (Fig.  7B). These embeddings are expected to capture fundamental biological \nfeatures, related to protein function or structural stability.\nTo investigate whether combining LLR and embeddings would increase performance, we appended the \nLLR to the last column of the embedding matrix, which increased the dimension from 2560 to 2561 (Fig.  7C).\nFigure 6.  The intersections of the two LLR distributions among three models. The LLR values obtained from \nESM-2, ESM-1v, and ESM-1b were plotted as a kernel density estimation (KDE) distribution along with their \ncorresponding true values. This visualization enables the differentiation between pathogenic and benign \nvariants.\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:8136  | https://doi.org/10.1038/s41598-024-51489-7\nwww.nature.com/scientificreports/\nFigure 7.  Schema of workflow for training VariPred. (A) In the frst step, each wild-type protein sequence and \nthe corresponding mutant protein sequence are fed into the pLM separately. The pLM generates a per-residue \nembedding for each amino acid. The output is the matrix of sequence embedding, with dimensions sequence \nlength x embedding dimension. (B) Only the embeddings of the amino acids at the mutated position are used \nand joined giving an embedding dimension of 2560. The concatenated embeddings for each observation are \ncombined to give an embedding matrix with dimensions dataset size ×2560. (C) The embedding matrix is fed \nas the input into a Feedforward Neural Network (FNN), and two probabilities are then output identifying if \nthe given variant belongs to the pathogenic or benign group. Note that if the LLR feature is appended the input \nmatrix is dataset size ×2561.\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:8136  | https://doi.org/10.1038/s41598-024-51489-7\nwww.nature.com/scientificreports/\nFeed‑forward neural network\nWe created a classifcation module by including a shallow feed forward neural network (FNN) as the decoder/\nclassifer for the pLM. This was trained on the class labels without updating parameters in the pLM. During \nthe hyperparameter tuning process, we tried increasing the depth of the FNN as well as trying multiple sets of \nlearning rates and drop-out rates. The fnal FNN, which gave optimal performance, consists of one hidden layer, \na LeakyReLu activation function, and one output layer with the dropout rate set at 0.5 and learning rate set at \n0.0001 (Fig. 7C). The input layer of the feed forward neural network has 2560 nodes (2561 if LLR is appended), \nwhile the hidden layer contains 1280 nodes, and the output consists of 2 nodes with a SoftMax layer to ensure the \noutput probabilities sum to 1 for binary classi fcation of benign/pathogenic. Only the pathogenic output node \nwas considered and a value of 0.2 was selected as a threshold for predicting the pathogenic class. This threshold \nwas selected by optimizing the MCC on the validation set during model training and the low value results from \nthe high skewness of the dataset towards neutral variants.\nEvaluation metrics\nAccuracy, Precision-Recall, F1-score, MCC (Matthews correlation coefficient) and AUC-ROC (area under curve \nof the receiver operating characteristic) are some of the most popular metrics for evaluating binary classi fers. \nSince MCC takes all outcomes (true and false positives and negatives) into account (Eq. 1), it is less sensitive to \nclass imbalance and is also more informative about the classifer’s performance at a given  threshold35. In contrast, \nother measures are more sensitive to  imbalance36 and the AUC-ROC gives a view of the overall performance of \na classifer (across a range of thresholds) rather than the actual performance in a classifcation problem.\nTherefore, we applied MCC as the main metric to measure the performance of predictors studied in this \nresearch, while using AUC-ROC as an auxiliary indicator.\nData availability\nAll data and code required to reproduce the model and analysis in this study are available at https:// github. com/ \nwlin16/ VariP red. git.\nReceived: 20 July 2023; Accepted: 5 January 2024\nReferences\n 1. Ng, P . C. & Henikoff, S. Predicting deleterious amino acid substitutions. Genome Res. 11, 863–874 (2001).\n 2. Ramensky, V . Human non-synonymous SNPs: Server and survey. Nucleic Acids Res. 30, 3894–3900 (2002).\n 3. Reva, B., Antipin, Y . & Sander, C. Predicting the functional impact of protein mutations: Application to cancer genomics. Nucleic \nAcids Res. 39, e118 (2011).\n 4. Kumar, P ., Henikoff, S. & Ng, P . C. Predicting the effects of coding non-synonymous variants on protein function using the SIFT \nalgorithm. Nat. Protoc. 4, 1073–1081 (2009).\n 5. Sundaram, L. et al. Predicting the clinical impact of human mutation with deep neural networks. Nat. Genet. 50, 1161–1170 (2018).\n 6. Adzhubei, I. A. et al. A method and server for predicting damaging missense mutations. Nat. Methods 7, 248–249 (2010).\n 7. Schwarz, J. M., Cooper, D. N., Schuelke, M. & Seelow, D. MutationTaster2: Mutation prediction for the deep-sequencing age. Nat. \nMethods 11, 361–362 (2014).\n 8. Al-Numair, N. S. & Martin, A. C. The SAAP pipeline and database: Tools to analyze the impact and predict the pathogenicity of \nmutations. BMC Genomics 14, S4 (2013).\n 9. Ittisoponpisan, S. et al. Can predicted protein 3D structures provide reliable insights into whether missense variants are disease \nassociated?. J. Mol. Biol. 431, 2197–2212 (2019).\n 10. Jumper, J. et al. Highly accurate protein structure prediction with AlphaFold. Nature 596, 583–589 (2021).\n 11. Michaud, J. M., Madani, A. & Fraser, J. S. A language model beats alphafold2 on orphans. Nat. Biotechnol. 40, 1576–1577 (2022).\n 12. Shihab, H. A. et al. Predicting the functional, molecular, and phenotypic consequences of amino acid substitutions using hidden \nMarkov models. Hum. Mutat. 34, 57–65 (2013).\n 13. Consortium, T. C. A. of G. I. CAGI, the Critical Assessment of Genome Interpretation, establishes progress and prospects for \ncomputational genetic variant interpretation methods. Preprint at https:// doi. org/ 10. 48550/ arXiv. 2205. 05897 (2022).\n 14. Won, D.-G., Kim, D.-W ., Woo, J. & Lee, K. 3Cnet: Pathogenicity prediction of human variants using multitask learning with \nevolutionary constraints. Bioinformatics 37, 4626–4634 (2021).\n 15. Vaswani, A. et al. Attention is all you need. In Advances in Neural Information Processing Systems vol. 30 (Curran Associates, Inc., \n2017).\n 16. Elnaggar, A. et al. ProtTrans: Towards cracking the language of lifes code through self-supervised deep learning and high perfor-\nmance computing. IEEE Trans. Pattern Anal. Mach. Intell. https:// doi. org/ 10. 1109/ TPAMI. 2021. 30953 81 (2021).\n 17. Rives, A. et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. \nProc. Natl. Acad. Sci. https:// doi. org/ 10. 1073/ pnas. 20162 39118 (2021).\n 18. Meier, J. et al. Language models enable zero-shot prediction of the effects of mutations on protein function. 2021.07.09.450648 \nPreprint at https:// doi. org/ 10. 1101/ 2021. 07. 09. 450648 (2021).\n 19. Lin, Z. et al. Language models of protein sequences at the scale of evolution enable accurate structure prediction. 2022.07.20.500902 \nPreprint at https:// doi. org/ 10. 1101/ 2022. 07. 20. 500902 (2022).\n 20. Marquet, C. et al. Embeddings from protein language models predict conservation and variant effects. Hum. Genet.  https:// doi. \norg/ 10. 1007/ s00439- 021- 02411-y (2021).\n 21. Littmann, M. et al. Clustering FunFams using sequence embeddings improves EC purity. Bioinformatics 37, 3449–3455 (2021).\n 22. Brandes, N., Goldman, G., Wang, C. H., Y e, C. J. & Ntranos, V . Genome‑wide prediction of disease variants with a deep protein \nlanguage model. (2022) https:// doi. org/ 10. 1101/ 2022. 08. 25. 505311.\n 23. Liu, X. et al. Protein Language Model Predicts Mutation Pathogenicity and Clinical Prognosis. (2022) https:// doi. org/ 10. 1101/ 2022. \n09. 30. 510294.\n(1)MCC = TP× TN − FP× FN√(TP+ FP)(TP+ FN)(TN + FP)(TN + FN)\n13\nVol.:(0123456789)Scientific Reports |         (2024) 14:8136  | https://doi.org/10.1038/s41598-024-51489-7\nwww.nature.com/scientificreports/\n 24. Grimm, D. G. et al. The evaluation of tools used to predict the impact of missense variants is hindered by two types of circularity. \nHum. Mutat. 36, 513–523 (2015).\n 25. Wang, M. & Wei, L. iFish: predicting the pathogenicity of human nonsynonymous variants using gene-speci fc/family-specifc \nattributes and classifers. Sci. Rep. 6, 31321 (2016).\n 26. Meier, J. Personal Communication. https:// github. com/ faceb ookre search/ esm/ discu ssions/ 129 (2019).\n 27. Al-Numair, N. S. et al. The structural effects of mutations can aid in differential phenotype prediction of beta-myosin heavy chain \n(Myosin-7) missense variants. Bioinformatics 32, 2947–2955 (2016).\n 28. Lu, H. et al. Machine learning-aided engineering of hydrolases for PET depolymerization. Nature 604, 662–667 (2022).\n 29. Shroff, R. et al. Discovery of novel gain-of-function mutations guided by structure-based deep learning. ACS Synth. Biol. 9, \n2927–2935 (2020).\n 30. Linder, J. et al. Interpreting neural networks for biological sequences by learning stochastic masks. Nat. Mach. Intell.  https:// doi. \norg/ 10. 1038/ s42256- 021- 00428-6 (2022).\n 31. Barešić, A. & Martin, A. C. R. Compensated pathogenic deviations. Biomol. Concepts 2, 281–292 (2011).\n 32. Karczewski, K. J. et al. The ExAC browser: Displaying reference data information from over 60 000 exomes. Nucleic Acids Res. 45, \nD840–D845 (2017).\n 33. Wang, K., Li, M. & Hakonarson, H. ANNOV AR: Functional annotation of genetic variants from high-throughput sequencing data. \nNucleic Acids Res. 38, e164 (2010).\n 34. Wong, W . C. et al. CHASM and SNVBox: Toolkit for detecting biologically important single nucleotide mutations in cancer. \nBioinformatics 27, 2147–2148 (2011).\n 35. Guilford, J. P . Psychometric Methods, 2nd ed. ix, 597 (McGraw-Hill, 1954).\n 36. Chicco, D. & Jurman, G. The advantages of the Matthews correlation coefficient (MCC) over F1 score and accuracy in binary \nclassifcation evaluation. BMC Genomics 21, 6 (2020).\nAcknowledgements\nThe authors thank Jinyuan Sun, Nicola Bordin, Ian Sillitoe, Marcia Hasenahuer, Shi-Yuan Tong for help with \nBioinformatics tools including HHblits, MMseqs, Blastp, Jackhmmer and Scorecons. The authors thank David \nGregory for help with implementing the SNVBox database and Jin Dai for help with implementing protein \nlanguage models.\nAuthor contributions\nW .L. and J.W . conceived the idea of constructing a twin-network architecture. W .L. designed and performed the \nexperiments. C.O. and A.C.R.M. interpreted the results. J.W . and A.C.R.M. analysed the data. Z.W . provided the \ntechnical advice. W .L. wrote the manuscript. W .L., J.W ., C.O. and A.C.R.M. revised the manuscript.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nSupplementary Information The online version contains supplementary material available at https:// doi. org/ \n10. 1038/ s41598- 024- 51489-7.\nCorrespondence and requests for materials should be addressed to C.O. or A.C.R.M.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024"
}