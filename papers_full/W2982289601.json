{
  "title": "Transfer Learning from Transformers to Fake News Challenge Stance Detection (FNC-1) Task",
  "url": "https://openalex.org/W2982289601",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5068248229",
      "name": "Valeriya Slovikovskaya",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1840435438",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2951714314",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2808554530",
    "https://openalex.org/W2110096996",
    "https://openalex.org/W2159505618",
    "https://openalex.org/W2337875011",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2048658075",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W1880262756",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2147152072",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2735017898",
    "https://openalex.org/W2745013032",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2606964149"
  ],
  "abstract": "In this paper, we report improved results of the Fake News Challenge Stage 1 (FNC-1) stance detection task. This gain in performance is due to the generalization power of large language models based on Transformer architecture, invented, trained and publicly released over the last two years. Specifically (1) we improved the FNC-1 best performing model adding BERT sentence embedding of input sequences as a model feature, (2) we fine-tuned BERT, XLNet, and RoBERTa transformers on FNC-1 extended dataset and obtained state-of-the-art results on FNC-1 task.",
  "full_text": "Transfer Learning from Transformers to Fake\nNews Challenge Stance Detection (FNC-1) Task\nValeriya Slovikovskaya\nUniversity of Pisa\nhttps://www.unipi.it\nAbstract\nIn this paper, we report improved results of the Fake News Challenge Stage 1 (FNC-1)\nstance detection task. This gain in performance is due to the generalization power of large\nlanguage models based on Transformer architecture, invented, trained and publicly released\nover the last two years. Speciﬁcally (1) we improved the FNC-1 best performing model\nadding BERT sentence embedding of input sequences as a model feature, (2) we ﬁne-tuned\nBERT, XLNet, and RoBERTa transformers on FNC-1 extended dataset and obtained state-\nof-the-art results on FNC-1 task.\n1 Introduction\nTwo years ago, the Fake News Challenge Stage 1 (FNC-1) attracted attention of several mem-\nbers of the linguistics research community: ﬁfty teams participated and submitted their re-\nsults, some of them like [Riedel et al., 2017] released their code and published research papers.\nLater, [Hanselowski et al., 2018b] retrospectively examined the top submissions 1, challenge task\nformulation, dataset characteristics as well as baseline model and evaluation metric proposed by\norganizers. Their contribution included revised FNC score, extended dataset and novel classiﬁ-\ncation model that outperforms the winner’s system on class-wise F1 scores. Besides, they also\ndetermined upper bound for FNC-1 data classiﬁcation conducting an annotation experiment that\ninvolved ﬁve human raters, who manually labelled 200 data instances. This detailed analysis of\nthe task, together with the code released by the authors, are highly valuable as a baseline on\nwhich we can rely in order to obtain further improvements.\nIn the last two years since Fake News Challenge Cup, signiﬁcant improvements have oc-\ncurred in NLP technology, in particular with the development of large language models using\ncontextualized word embeddings based on the Google Transformer architecture [Vaswani et al.,\n2017].\nApplying transfer learning techniques to these general purpose pre-trained models helped to\nachieve improvements in a wide range of downstream tasks. Using these models as pre-training\nfor downstream tasks allows lowering the amount of annotated data required for such tasks.\nBERT [Devlin et al., 2018], GPT [Radford et al., 2019], XLNet [Yang et al., 2019], trained on\nthe huge unlabeled datasets on GPUs and Cloud TPUs, attained signiﬁcant accuracy increase\non GLUE [Wang et al., 2018] and RACE [Lai et al., 2017] benchmarks 2.\nThe public releases of pre-trained models led to intensiﬁed experimenting with transfer learn-\ning / ﬁne-tuning for a broader range of linguistic tasks; this stimulated the development of APIs\nthat unify and standardize the access to weights of diﬀerent pre-trained models and made them\naccessible for the greatly extended community of professionals.\nThis paper is organized as follows. In the ﬁrst part, we present Fake New Challenge Stage\n1 (FNC-1) revised task: task formulation, dataset, evaluation metric, and the state-of-the-art\n1including that of Hanselowski team Athene which ranked second\n2GLUE leaderboard: https://gluebenchmark.com/leaderboard/, RACE leaderboard: http://www.qizhexie.\ncom/data/RACE_leaderboard.html\narXiv:1910.14353v1  [cs.CL]  31 Oct 2019\nmodel. In the second part, we introduce new semantic features that allow to improve the model\nperformance. In the third part we present the further improved results obtained as a fruit of\ntransfer learning from BERT, RoBERTa and XLNet transformers applied to the FNC-1 classiﬁ-\ncation task.\n2 FNC-1 task\n2.1 Task formulation\nLet’s suppose we have a statement or a claim which truthfulness is veriﬁed or which source is\ntrusted. How can we then detect liars about such claim in an upcoming news stream? We might\ntry to determine whether the news text is related to the claim, and, if so, to ﬁnd out whether it\ncontradicts or supports it, i.e. its stance on the subject. The Fake News Challenge Stage 1 task\nis formulated in fact as a stance detection problem. The data set provided by organizers consists\nof pairs of a news article headline (as a claim), and a snippet of text taken either from the same\nor from another news article. Each pair in the training part of the data set is labeled with a tag\nattesting the relation between the headline and the snippet. The labels are ”agree”, ”disagree”,\n”discuss”, and ”unrelated”. Table 1 presents a few examples of stance classiﬁcation, taken from\nthe training set of the challenge. The goal of the challenge is to classify the pairs in the test data\nset.\nHeadline Robert Plant Ripped up $800M Led Zeppelin Reunion Contract\nagree Led Zeppelin’s Robert Plant turned down £500 MILLION to reform su-\npergroup\ndisagree No, Robert Plant did not rip up an $800 million deal to get Led Zeppelin\nback together\ndiscuss Robert Plant reportedly tore up an $800 million Led Zeppelin reunion\ndeal\nunrelated Richard Branson’s Virgin Galactic is set to launch SpaceShipTwo today\nTable 1: Headline and text snippets from document bodies with respective\nstances from the FNC-1 dataset\n2.2 Dataset\n2.2.1 FNC-1 data\nThe organizers of the Challenge constructed a news corpus with data from the Emergent dataset\nfor stance classiﬁcation that contains 300 claims, and 2,595 associated article headlines.\nThe claims were collected by journalists from a variety of sources such as rumour sites,\ne.g. Snopes.com, and Twitter accounts such as @Hoaxalizer. Their subjects include world and\nnational U.S. news and technology stories. The journalist also summarises the article into a\nheadline. In parallel to the article-level stance detection, a claim-level veracity judgement is\nreached as more articles associated with the claim are examined.\nThe dataset construction was conducted by Craig Silverman and his colleagues at the Tow\nCenter for Digital Journalism at Columbia University 3 for Emergent Project [Silverman, 2015].\nThen it was used for independent research on rumor debunking and released4 as Emergent dataset\nby [Ferreira and Vlachos, 2016]\nThus the Emergent Project dataset consists of 300 topics, each represented by a claim with\n5–20 possibly related news articles. The labelling of a headline/article pair indicates whether\nthe article is supporting, refuting, or just reporting the claim, respectively.\n3http://towcenter.org/\n4https://github.com/willferreira/mscproject\n2\nDataset headlines documents instances agree disagree discuss unrelated\nFNC-1 2,587 2,587 75,385 7.4% 2.0% 17.7% 72.8%\nTable 2: Corpus statistics and label distribution for the FNC-1 dataset\nOther than for rumor debunking, the FNC-1 organizers match each document with every\nsummarized headline, and then label the pair with one of the four stance labels: ”agree”, ”dis-\nagree”, ”discuss”, and ”unrelated”. To generate the ”unrelated” class, headlines and articles\nbelonging to diﬀerent topics are randomly matched. Headline/article pairs of 200 claims/topics\nare reserved for training, the remaining headline/article pairs of 100 claims/topics for testing.\nClaims, headlines, and articles are therefore not shared between the two data splits. To prevent\nteams from using any unfair means by deriving the labels for the test set from the publicly\navailable Emergent data, the organizers additionally created 266 instances. Table 2 shows the\ncorpus size and label distribution.\n2.2.2 ARC data\nTo test the robustness of their models (i.e. how well they generalize to new datasets), [Hanselowski\net al., 2018b] introduced a novel test data for document-level stance detection based on the Ar-\ngument Reasoning Comprehension (ARC) task proposed by [Habernal et al., 2017].\n[Habernal et al., 2017] manually selected 188 debate topics with popular questions from the\nuser debate section of the New York Times. For each topic, they collected user posts, which are\nhighly ranked by other users, and created two claims representing two opposing views on the\ntopic. Then, they asked crowd workers to decide whether a user post supports either of the two\nopposing claims or does not express a stance at all. This Argument Reasoning Comprehension\n(ARC) dataset consists of typical controversial topics from the news domain, such as immigration,\nschooling issues, or international aﬀairs.\nExample from the original ARC dataset\nTopic Do same-sex colleges play an important role in education, or are they\noutdated?\nUser\npost\nOnly 40 womens colleges are left in the U.S. And, while there are a variety\nof opinions on their value, to the women who have attended . . . them, they\nhave been . . . tremendously valuable. . . .\nClaims 1. Same-sex colleges are outdated 2. Same-sex colleges are still relevant\nLabel Same-sex colleges are still relevant\nGenerated instance in alignment with the FNC-1 problem setting\nStance Headline Document\nagree Same-sex colleges are still relevant Only 40 women’s colleges are left in\nthe U.S. . . .\nTable 3: Example of the original ARC dataset and the generated instance to\nalign with FNC-1 dataset\nWhile this is similar to the FNC-1 dataset, there are signiﬁcant diﬀerences, as a user post is\ntypically a multi-sentence statement representing one viewpoint on the topic. In contrast, the\nnews articles of FNC-1 are longer and usually provide more balanced and detailed perspective\non the issue.\nTo allow using the ARC data for the FNC-1 stance detection setup [Hanselowski et al., 2018b],\nconsider each user post as an article and randomly select one of the two claims as the headline.\nThey label the headline/article pair as ”agree” if the claim has also been chosen by the workers,\nas ”disagree” if the workers chose the opposite claim, and as ”discuss” if the workers selected\nneither claim. Table 3 shows an example of the revised ARC corpus structure.\nIn order to generate the unrelated instances [Hanselowski et al., 2018b], randomly match the\nuser posts with claims, but avoid that a user post is assigned to a claim from the same topic.\nFor training and testing, the authors split the corpus into 80% training/validation set and 20%\ntesting set. Table 4 provides basic corpus statistics.\n3\nDataset headlines documents instances agree disagree discuss unrelated\nARC 4,448 4,448 17,792 8.9% 10.0% 6.1% 75.0%\nTable 4: Corpus statistics and label distribution for the ARC dataset\nFor all our experiments we use the combined FNC + ARC dataset made freely available\nby [Hanselowski et al., 2018a] on GitHub.\n2.3 Evaluation metric\nThe FNC-1 dataset is highly unbalanced with respect to class distribution. The FNC-1 organizers\nproposed the hierarchical evaluation metric, FNC score, which ﬁrst awards .25 points if an\narticle is correctly classiﬁed as related or ”unrelated” to a given headline. If it is related, .75\nadditional points are assigned if the model correctly classiﬁes the headline/article pair as ”agree”,\n”disagree”, or ”discuss”. The goal of this weighting schema is to balance out the large number\nof unrelated instances.\nThis metric was criticized by [Hanselowski et al., 2018b] as the score that, albeit an hierar-\nchical, fails to take into account the imbalanced class distribution of the three related classes\n”agree”, ”disagree”, and ”discuss”.\n[Hanselowski et al., 2018b] reasoned that the models which perform well on the majority\nclass and poorly on the minority classes are favored. Since it is not diﬃcult to separate related\nfrom unrelated instances (the best systems reach about F1 = .99 for the ”unrelated” class), a\nclassiﬁer that just randomly predicts one of the three related classes would already achieve a high\nFNC score. A classiﬁer that always predicts ”discuss” for the related documents even reaches\nFNC = .833, which is even higher than the top-ranked FNC system.\nAuthors therefore argued that the FNC score metric is not appropriate for the task and\nproposed to rely on the class-wise and the macro-averaged F1 scores (F1m) not aﬀected by the\nlarge size of the majority class. The na¨ ıve approach of perfectly classifying ”unrelated” and\nalways predicting ”disagree” for the related classes would achieve only F1m = .444 and would\nreveal a poor ability of the model to distinguish between ”agree”, ”disagree”, and ”discuss”\ncategories.\n2.4 Model architecture\nWith respect to F1m score, the model of team Athene [Hanselowski et al., 2017] outperforms all\nother models. Its architecture, named ”featMLP”, is a multi-layer perceptron (MLP) inspired\nby the work of [Davis and Proctor, 2017]. The model has six hidden and a softmax layers and\nincorporates multiple hand-engineered features. The one group of features includes unigrams,\nand the cosine similarity of word embeddings of nouns and verbs between headline and document\ntokens. The other group consists of topic models based on non-negative matrix factorization,\nlatent Dirichlet allocation, and latent semantic indexing. featMLP also incorporates the FNC-1\nbaseline’s features suggested by the FNC-1 organizers such as word overlap, polarity words,\nrefuting words, and co-occurrence feature that counts how many times word 1-/2-/4-grams,\ncharacter 2-/4-/8-/16-grams, and stop words of the headline appear in the ﬁrst 100, ﬁrst 255\ncharacters of the article, and how often they appear in the article overall. See [Hanselowski et al.,\n2018b] and baseline [Galbraith et al., 2016] for major details. Depending on the feature type,\nthey either form separate feature vectors for document and headline, or a joint feature vector.\n2.5 Lexical features\nAuthors conduct the feature ablation test on the FNC-1 development set with 10-fold cross-\nvalidation to deﬁne the best feature set for their model. This feature set contains bag-of-word\n(BoW) and bag-of-character (BoC) features as well as topic modeling features.\nFor BoW features authors use uni- and bi-grams with 5,000 tokens vocabulary for the headline\nas well as for the article. Based on a technique by [Das and Chen, 2007] they add a negation\ntag ” NEG” as preﬁx to every word between special negation keywords (e.g. ”not”, ”never”,\n4\n”no”) until the next punctuation mark appears. For the BoC, three-grams are chosen with 5,000\ntokens vocabulary, too. For the BoW/BoC feature authors use the TF to extract the vocabulary\nand to build the feature vectors of headline and document. The resulting TF vectors of headline\nand article get concatenated afterwards. In the set of winning Bow/BoC features, authors also\ninclude FNC-1 baseline’s co-occurrence feature.\nAs for topic models, authors use non-negative matrix factorization (NMF) [Lin, 2007], latent\nsemantic indexing (LSI) [Deerwester et al., 1990], and latent Dirichlet allocation (LDA) [Blei\net al., 2003] to create topic models out of which they create independent features. For each\ntopic model, they extract 300 topics out of the headline and article texts. Afterwards, they\ncompute the similarity of headlines and article bodies to the found topics separately and either\nconcatenate the feature vectors (NMF, LSI) or calculate the cosine distance between them as a\nsingle valued feature (NMF, LDA).\nBased on these features the model exploits similarity between the headline and the article in\nterms of rather lexical overlap without really capturing the semantics of the text.\n2.6 Semantic features\nStepping towards taking into account semantic characteristics of the text, [Hanselowski et al.,\n2018b] experiment with ”stackLSTM” model, which combines the best feature set found in the\nablation test with a stacked long short-term memory (LSTM) network [Hermans and Schrauwen,\n2013].\nAuthors use 50-dimensional GloVe word embeddings 5 [Pennington et al., 2014] in order to\ngenerate sequences of word vectors of a headline/article pair. For this, they concatenate a\nmaximum of 100 tokens of the headline and the article. These embedded word sequences are fed\nthrough two stacked LSTMs with a hidden state size of 100 with a dropout of 0.2 each. The\nlast hidden state of the second LSTM is concatenated with the feature set and fed into a 3-layer\nneural network with 600 neurons each. Finally, they add a dense layer with four neurons and\nsoftmax activation function in order to retrieve the class probabilities.\n”stackLSTM” improved performance for the ”disagree” class, which is the most diﬃcult one\nto predict due to the low number of instances.\n3 FNC-1 top model improved\nIn our work on improving FNC-1 results we were looking for semantically sensitive model archi-\ntectures and features. Therefore, we experimented with transfer learning from large pre-trained\nmodels reported as being able to capture semantics of the text.\nThere are two existing strategies for applying pre-trained language representations to down-\nstream tasks: feature-based and ﬁne-tuning. The feature-based approach uses task-speciﬁc ar-\nchitectures that include the pre-trained representations as additional features.\nThe ﬁne-tuning does not require task speciﬁc model, instead it introduces into a general pur-\npose pre-trained model a restricted set of task speciﬁc parameters and trains it on the downstream\ntask dataset, in general for 2–3 epochs, ﬁne-tuning all pre-trained parameters.\nWe apply both approaches to FNC-1 task, and report them both to be eﬀective.\n3.1 InferSent based features\n[Conneau et al., 2017b] indicated the suitability of natural language inference (NLI) for transfer\nlearning to other NLP tasks: NLI is a high-level understanding task that involves reasoning about\nthe semantic relationships within sentences. Authors trained universal sentence representations\nusing the supervised data of the Stanford Natural Language Inference (SNLI) dataset [Bowman\net al., 2015] and made their encoder, based on a bi-directional LSTM architecture with max\npooling, publicly available so that sentence embedding can be easily obtained and transferred as\na feature to other text classiﬁcation tasks.\n5http://nlp.stanford.edu/data/glove.twitter.27B.zip\n5\nThus, for each headline/article body pair of FNC-ARC dataset, we calculated vector repre-\nsentation for every sentence of the article or headline using Facebook InferSent encoder [Conneau\net al., 2017a], then we averaged sentence vectors to obtain vector representation for whole article\nand whole headline, after that we concatenated the article and the headline vectors. We used\nthe headline/article pair vectors as a feature on its own to directly feed into [Hanselowski et al.,\n2018b] ”featMLP” classiﬁer and in addition, as a separate features, we calculated a cosine simi-\nlarity score between headline and article vectors as well as a maximum similarity score between\nheadline and every sentence in the article.\nThese three features together slightly improve F1 macro and F1 -”agree”, -”disagree” and\n-”discuss” scores of ”featMLP” model as shown in Table 5. ”Inf1” column reports the impact of\nInferSent embeddings for input sequence pair, ”Inf3” shows the impact of this feature together\nwith two similarity scores for two sequences in the input pair.\nScore, % Base Inf1 Inf3 BERT1 BERT3 BERT\n+Inf3\nBERT\nonly\nAccuracy 87.18 86.93 87.05 87.57 87.75 87.94 81.45\nFNC score 78.59 77.82 78.16 79.03 79.42 79.72 68.65\nF1 macro 61.61 61.63 62.02 63.32 63.96 63.91 56.27\nF1 agree 50.26 50.66 50.69 53.77 53.15 53.58 43.16\nF1 disagree 27.22 27.81 28.85 31.43 34.04 31.22 31.75\nF1 discuss 72.43 71.70 72.20 71.21 71.61 73.88 58.55\nF1 unrelated 96.54 96.35 96.36 96.89 97.05 96.95 91.61\nTable 5: Performance of featMLP model without and with InferSent- and\nBERT- based features\n3.2 BERT based features\nOur next step was experimenting with transfer learning from pre-trained transformers.\n[Devlin et al., 2018] introduced a new language representation model called BERT, which\nstands for Bidirectional Encoder Representations from Transformers. BERT’s model architec-\nture is a multi-layer bidirectional Transformer encoder based on the original implementation\ndescribed in [Vaswani et al., 2017] and released in the tensor2tensor library [Vaswani et al.,\n2018]. BERT is designed to pre-train deep bidirectional representations from unlabeled text by\njointly conditioning on both left and right context in all layers. BERT uses ”masked language\nmodel” (MLM) pre-training objective, inspired by the Cloze task [Taylor, 1953]. Masked lan-\nguage model randomly masks some of the tokens from the input, and the objective is to predict\nthe masked word based on its context. In addition to the masked language model, BERT also\nuse a ”next sentence prediction” (NSP) task that jointly pre-trains text-pair representations.\nFor the pre-training corpus [Devlin et al., 2018] use the BooksCorpus (800M words) [Zhu\net al., 2015] and English Wikipedia (2,500M words). 6\nSelf attention mechanism in the Transformer allows BERT to model many downstream tasks\n– whether they involve single text or text pairs. For each task, the steps are: (1) simply plug in\nthe task speciﬁc inputs and outputs into BERT and (2) ﬁne-tune all the parameters end-to-end.\nAuthors showed that ﬁne-tuned BERT outperforms all systems on all GLUE tasks by a\nsubstantial margin: BERT-base obtains a 4.5% average accuracy improvement over the prior\nstate of the art, and BERT-large achieves 7.0%.\n[Devlin et al., 2018] report ﬁne-tuning to be relatively inexpensive compared to pre-training:\nat most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the same\npre-trained model. Authors pre-train BERT model of two sizes: BERT-base with 12 trans-\nformer blocks, 12 attention heads, 768 neurons in hidden layers, and 110 million parameters, and\nBERT-large with 24 transformer blocks, 16 attention heads, 1024 neuron in hidden layers, and\n340 million parameters. Both models are made publicly available and their success raised the\n6Authors note that it is critical to use a document-level corpus rather than a shuﬄed sentence-level corpus\nsuch as the Billion Word Benchmark [Chelba et al., 2013] in order to extract long contiguous sequences.\n6\npopularity of libraries like HuggingFace’s Transformers [Wolf et al., 2019] that facilitate access\nto pre-trained models and optimize their integration into NLP pipelines.\nIn our experiments, we follow a feature-based approach ﬁrst to transfer learning and use BERT\nsentence embeddings to create the new features for ”featMLP” model. As with InferSent sentence\nembeddings, we introduce three separate features: we calculate BERT sentence embeddings for\neach sentence of an article and a headline, then we average sentence embeddings to obtain vector\nrepresentation for the whole article and the whole headline, after that we concatenate the article\nand the headline vectors. We use the headline/article pair vectors as a feature on its own, and,\nin addition, as two separate features, we calculate the cosine similarity score between a headline\nand article vectors, and the maximum similarity score between headline and each sentence of the\narticle.\nTo obtain BERT sentence embeddings we leverage the ”bert-as-service” application [Xiao,\n2018]7. ”bert-as-service” is build on pre-trained 12/24-layer BERT models released by Google AI,\nit uses BERT as a sentence encoder and hosts it as a service via ZeroMQ. The application includes\nbuild-in HTTP server and a dashboard; it provides asynchronous encoding and multicasting.\nThe performance of the ”featMLP” model enriched with BERT-based features is shown in\nTable 5. ”BERT1” column shows the impact of BERT vector representation of an input, and\n”BERT3” shows the impact of this feature together with two similarity scores between BERT\nembeddings of two sequences in the input pair. The column ”BERT3 + Inf3” reports InferSent\nand BERT cumulative eﬀect. The most signiﬁcant F1-score increase, that of 7% is attained for\nmost diﬃcult ”disagree” class.\nTo demonstrate the power of BERT embeddings we conduct a kind of ablation test: we\nremove all Bow/BoC and topic modelling features and use only headline/article pair BERT\nvectors (without similarity scores), this way we obtaine the results shown in the ”BERT1 only”\ncolumn of Table 5: it can be seen that the ”disagree” F1 score improvement is relied mostly on\nthe BERT embedding feature.\nThese results pushed us further on the way of experimenting with pre-trained transformers.\nFollowing [Devlin et al., 2018] we ﬁne-tuned on FNC-1 task three pre-trained models: BERT,\nXLNet and RoBERTa, and as expected, received substantially improved F1 scores with respect\nto [Hanselowski et al., 2018b] ”featMLP” model performance.\n4 Transfer learning: from transformers to FNC-1 task\n4.1 XLNet\nBERT predicts all masked positions independently, meaning that during the training, it does\nnot learn to handle dependencies between predicted masked tokens. This reduces the number of\ndependencies BERT learns at once, making the learning signal weaker than it could be.\nAnother problem with BERT is that the [MASK] token – which is at the center of training\nBERT – never appears when ﬁne-tuning BERT on downstream tasks. That means that the\n[MASK] token is a source of train-test skew while ﬁne-tuning.\nXLNet [Yang et al., 2019] incorporates bidirectional context while avoiding the [MASK] tokens\nand independent predictions. It does this by introducing ”permutation language modeling”:\ninstead of predicting the tokens in sequential order, it predicts tokens in some random order. 8\nAside from using permutation language modeling, XLNet improves upon BERT by using the\nTransformer XL as its base architecture.\nBoth BERT and XLNet can take the pair of text sequences as an input. To enable the model\nto distinguish between words in two diﬀerent segments, BERT learns a segment embedding. In\n7with default settings listed here: https://github.com/hanxiao/bert-as-service\n8For example, the sequence order is x1, x2, x3, x4. So for this 4 (N) tokens sentence, there are 24 (N!)\npermutations. If we want to predict the x3, so there are 4 patterns with x3 in the 24 permutations: x3 is at the\n1st position, 2nd position, 3rd position, and 4th position: [ x3, xx, xx, xx], [xx, x3, xx, xx], [xx, xx, x3, xx], [xx,\nxx, xx, x3]. Here we set the position of x3 as t-th position and t-1 tokens are the context words for predicting x3.\nIntuitively, the model will learn to gather information from all positions on both sides.\n7\ncontrast, XLNet learns an embedding that represents whether two words are from the same\nsegment. This embedding is used during attention computation between any two words.\nXLNet integrates the segment recurrence mechanism and relative encoding scheme of Trans-\nformerXL [Dai et al., 2019] into pre-training and enables the model to reuse hidden states from\nprevious segments.9\nXLNet outperforms BERT and achieves state-of-the-art performance across 20 tasks including\ntext classiﬁcation, question answering, natural language inference, duplicate sentence (question)\ndetection, document ranking, coreference resolution.\n4.2 RoBERTa\n[Liu et al., 2019] found that BERT was signiﬁcantly undertrained and proposed an improved\nrecipe for training BERT models, which they call RoBERTa (Robustly optimized BERT ap-\nproach), that can match or exceed the performance of all of the post-BERT methods. The\nrecipe includes: (1) training the model longer, with bigger batches, over more data; (2) removing\nthe ”next sentence prediction” objective; (3) training on longer sequences; and (4) dynamically\nchanging the masking pattern applied to the training data.\nTo train RoBERTa authors use ﬁve English-language corpora of varying sizes and domains,\ntotaling over 160GB of uncompressed text. 10\nThey also demonstrate that removing the ”next sentence prediction” loss together with\nsegment-pair input format matches or slightly improves downstream task performance. RoBERTa\nachieves state-of-the-art results on all 9 of the GLUE task development sets. It consistently out-\nperforms both BERT and XLNet.\n4.3 Transformers ﬁne-tuned on FNC-1 task\nFor our experiments on ﬁne-tuning transformers on FNC-1 task we used the Simple Transformers\n[Rajapakse, 2019] wrapper around Hugging Face Transformers library [Wolf et al., 2019].\nScore, % featMLP BERT XLNet RoBERTa\nAccuracy 87.18 91.32 92.10 93.19\nFNC score 78.59 86.16 87.90 89.17\nF1 macro 61.61 72.82 76.06 78.12\nF1 agree 50.26 64.70 68.61 70.71\nF1 disagree 27.22 47.84 54.85 58.06\nF1 discuss 72.43 80.07 82.10 84.57\nF1 unrelated 96.54 98.68 98.65 99.16\nTable 6: Performance of featMLP, BERT, XLNet, and RoBERTa ﬁne-tuned\nmodels\nThe development of Transformers, a library for natural language processing with transfer\nlearning models, was motivated by the need to share state-of-the-art pre-trained models and to\nincrease the availability of published research code. With this library low-resource users can\nreuse pre-trained models without having to train them from scratch.\nThese models are accessed through a simple and uniﬁed API that follows a classic NLP\npipeline: setting up conﬁguration, processing data with a tokenizer and encoder, and using a\nmodel either for training (adaptation in particular) or inference. The model implementations\nprovided in the library are tested to ensure they match the original author implementations’\n9XLNet is named after TransformerXL\n10These corpora include (1) BOOK CORPUS [Zhu et al., 2015] plus English Wikipedia, the original data used\nto train BERT (16GB); (2) CC-NEWS, which authors collected from the English portion of the CommonCrawl\nNews dataset [Nagel, 2019], the data contains 63 million English news articles crawled between September 2016\nand February 2019 (76GB after ﬁltering); (3) OPEN WEB TEXT [Gokaslan and Cohen, 2019], an open-source\nrecreation of the WebText corpus described in [Radford et al., 2019], the text is web content extracted from URLs\nshared on Reddit with at least three upvotes (38GB); (4) STORIES, a dataset introduced in [Trinh and Le, 2018]\ncontaining a subset of CommonCrawl data ﬁltered to match the story-like style of Winograd schemas (31GB).\n8\nagree disagree discuss unrelated\nagree 1104 1534 1597 1592 152 328 297 265 738 619 474 377 243 24 50 32\ndisagree 306 151 114 129 230 432 517 560 253 136 150 150 280 18 35 21\ndiscuss 617 486 482 487 175 239 205 210 3337 3732 3894 4009 514 222 262 132\nunrelated 129 66 44 29 64 70 50 34 244 156 125 107 20586 20759 20676 20838\nTable 7: Confusion matrix: every cell shows featMLP, BERT, XLNet, RoBERTa\nresult correspondingly\nperformances on various benchmarks. A list of architectures for which reference implementations\nand pre-trained weights are currently provided in Transformers includes BERT [Devlin et al.,\n2018], DistilBERT [Sanh et al., 2019], RoBERTa [Liu et al., 2019], XLNet [Yang et al., 2019],\nGPT and GPT2 [Radford et al., 2019].\nPrecision, % Recall, % F1-score, % Support\n0 53 69 71 71 48 61 66 70 50 65 69 71 2237 2505 2418 2266\n1 38 40 48 52 24 59 63 65 29 48 55 58 1069 737 816 860\n2 73 80 84 86 74 80 80 83 73 80 82 85 4643 4679 4843 4838\n3 95 99 98 99 98 99 99 99 97 99 99 99 21023 21051 20895 21008\nTable 8: Precision, recall, F1-score for featMLP, BERT, XLNet, RoBERTa.\nEvery cell contains 4 results corresponding to four models in order\nThe Simple Transformers [Rajapakse, 2019] library is built on top of the Hugging Face Trans-\nformers. The idea behind it was to make it as simple as possible, abstracting a lot of the\nimplementation details.\nThus, with Simple Transformers on the shoulders of Hugging Face Transformers we could\naccess pre-traines BERT, XLNet and RoBERTa in uniﬁed way without a lot of pre-processing\ncoding.\nWe modiﬁed only limited number of parameters, setting maximum sequence length to be equal\nto 512 tokens 11. We used the base versions of all transformers (”bert-base-uncased”, ”xlnet-\nbase-cased”, ”roberta-base”), those with 12 transformer blocks. We trained every transformers\nfor 5 epochs with batch size of 4, with learning rate of 3e-5 for BERT and 1e-5 for XLNet and\nRoBERTa.12\nOur results, reported in Tables 6, 7, and 8 show that all three models ﬁne-tuned on the Fake\nNews Challenge task outperform [Hanselowski et al., 2018b] ”featMLP” model with signiﬁcant\nmargins varying from 8 to 20% for ”related” classes. The XLNet and RoBERTa demonstrate\nbetter performance than BERT, that corresponds to our expectations; RoBERTa outperforms\nboth BERT and XLNet, that is in line with the results reported in [Liu et al., 2019].\nWe also performed cross domain validation experiments ﬁne-tuning transformers on FNC-1\ndataset and evaluating them on ARC dataset and vice versa. The Table 9 shows that for these test\nRoBERTa can demonstrate worse performance in comparison with BERT and XLNet models.\n5 Conclusion\nWe substantially improved Fake News Challenge Stage 1 results, putting the task in the line\nof Natural Language Processing tasks that are reported to beneﬁt from transfer learning from\npre-trained transformers.\n6 Acknowledgments\nThe author would like to thank Professor Giuseppe Attardi, who provided insight for this work,\nand Professor Serge Sharoﬀ and Dr. Pavel Braslavsky for their helpful advise.\n11The maximum possible value to set given the parameters of pre-trained models\n12Every time ﬁne-tuning was taking several hours on Nvidia Tesla P100 GPU with 16GB of memory.\n9\nFNC - ARC\nF1 score, % featMLP BERT XLNet RoBERTa\nagree 25.71 39.55 43.82 45.61\ndisagree 9.67 28.39 28.31 24.91\ndiscuss 15.42 18.58 08.30 18.23\nunrelated 89.83 93.48 92.53 93.91\nARC - FNC\nF1 score, % featMLP BERT XLNet RoBERTa\nagree 27.57 39.03 41.01 0.73\ndisagree 10.43 8.41 05.44 16.10\ndiscuss 8.03 5.66 14.07 10.40\nunrelated 85.43 95.72 95.59 93.57\nTable 9: Class-wise F1 scores for ”featMLP” model (with BoW/BoC and topic\nmodeling features) and ﬁne-tuned transformers in cross-domain, FNC - ARC\nand ARC - FNC, experiments\nThe experiments were carried out on a server with 4 GPUs Nvidia Tesla Pascal P100, partially\nfunded by the University of Pisa through the grant Grandi Attrezzature 2016.\nReferences\n[Blei et al., 2003] Blei, D. M., Ng, A. Y., and Jordan, M. I. (2003). Latent dirichlet allocation.\nJ. Mach. Learn. Res., 3:993–1022.\n[Bowman et al., 2015] Bowman, S. R., Angeli, G., Potts, C., and Manning, C. D. (2015). A\nlarge annotated corpus for learning natural language inference. In Proceedings of the 2015\nConference on Empirical Methods in Natural Language Processing, pages 632–642, Lisbon,\nPortugal. Association for Computational Linguistics.\n[Chelba et al., 2013] Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., and Koehn, P.\n(2013). One billion word benchmark for measuring progress in statistical language modeling.\nCoRR, abs/1312.3005.\n[Conneau et al., 2017a] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bordes, A.\n(2017a). Infersent. https://github.com/facebookresearch/InferSent.\n[Conneau et al., 2017b] Conneau, A., Kiela, D., Schwenk, H., Barrault, L., and Bordes, A.\n(2017b). Supervised learning of universal sentence representations from natural language infer-\nence data. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language\nProcessing, pages 670–680, Copenhagen, Denmark. Association for Computational Linguistics.\n[Dai et al., 2019] Dai, Z., Yang, Z., Yang, Y., Carbonell, J. G., Le, Q. V., and Salakhutdinov,\nR. (2019). Transformer-xl: Attentive language models beyond a ﬁxed-length context. CoRR,\nabs/1901.02860.\n[Das and Chen, 2007] Das, S. and Chen, M. Y. (2007). Yahoo! for amazon: Sentiment extraction\nfrom small talk on the web. Management Science, 53(9):1375–1388.\n[Davis and Proctor, 2017] Davis, R. and Proctor, C. (2017). Fake news, real consequences: Re-\ncruiting neural networks for the ﬁght against fake news. web.stanford.edu.\n[Deerwester et al., 1990] Deerwester, S., Dumais, S. T., Furnas, G. W., Landauer, T. K., and\nHarshman, R. (1990). Indexing by latent semantic analysis.JOURNAL OF THE AMERICAN\nSOCIETY FOR INFORMATION SCIENCE, 41(6):391–407.\n[Devlin et al., 2018] Devlin, J., Chang, M., Lee, K., and Toutanova, K. (2018). BERT: pre-\ntraining of deep bidirectional transformers for language understanding.CoRR, abs/1810.04805.\n10\n[Ferreira and Vlachos, 2016] Ferreira, W. and Vlachos, A. (2016). Emergent: a novel data-set\nfor stance classiﬁcation. In HLT-NAACL.\n[Galbraith et al., 2016] Galbraith, B., Iqbal, H., van Veen, H., Rao, D., Thorne, J., and\nPan, Y. (2016). Baseline fnc implementation. https://github.com/FakeNewsChallenge/\nfnc-1-baseline.\n[Gokaslan and Cohen, 2019] Gokaslan, A. and Cohen, V. (2019). Openwebtext corpus. http:\n//Skylion007.github.io/OpenWebTextCorpus.\n[Habernal et al., 2017] Habernal, I., Wachsmuth, H., Gurevych, I., and Stein, B. (2017). The\nargument reasoning comprehension task. CoRR, abs/1708.01425.\n[Hanselowski et al., 2017] Hanselowski, A., PVS, A., Schiller, B., and Caspelherr, F. (2017).\nDescription of the system developed by team athene in the fnc-1. https://github.com/\nhanselowski/athene_system/blob/master/system_description_athene.pdf.\n[Hanselowski et al., 2018a] Hanselowski, A., PVS, A., Schiller, B., Caspelherr, F., Chaudhuri,\nD., Meyer, C. M., and Gurevych, I. (2018a). Repository of the coling 2018 paper: A retrospec-\ntive analysis of the fake news challenge stance detection task. https://github.com/UKPLab/\ncoling2018_fake-news-challenge.\n[Hanselowski et al., 2018b] Hanselowski, A., PVS, A., Schiller, B., Caspelherr, F., Chaudhuri,\nD., Meyer, C. M., and Gurevych, I. (2018b). A retrospective analysis of the fake news challenge\nstance-detection task. In Proceedings of the 27th International Conference on Computational\nLinguistics, pages 1859–1874, Santa Fe, New Mexico, USA. Association for Computational\nLinguistics.\n[Hermans and Schrauwen, 2013] Hermans, M. and Schrauwen, B. (2013). Training and analysing\ndeep recurrent neural networks. In Burges, C. J. C., Bottou, L., Welling, M., Ghahramani,\nZ., and Weinberger, K. Q., editors, Advances in Neural Information Processing Systems 26,\npages 190–198. Curran Associates, Inc.\n[Lai et al., 2017] Lai, G., Xie, Q., Liu, H., Yang, Y., and Hovy, E. (2017). RACE: Large-scale\nReAding comprehension dataset from examinations. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing, pages 785–794, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\n[Lin, 2007] Lin, C.-J. (2007). Projected gradient methods for nonnegative matrix factorization.\nNeural computation, 19:2756–79.\n[Liu et al., 2019] Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M.,\nZettlemoyer, L., and Stoyanov, V. (2019). Roberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\n[Nagel, 2019] Nagel, S. (2019). Cc-news. http://chuachinhon.pythonanywhere.com/.\n[Pennington et al., 2014] Pennington, J., Socher, R., and Manning, C. D. (2014). Glove: Global\nvectors for word representation. In Empirical Methods in Natural Language Processing\n(EMNLP), pages 1532–1543.\n[Radford et al., 2019] Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever,\nI. (2019). Language models are unsupervised multitask learners. preprint on webpage at\nhttps://paperswithcode.com/paper/language-models-are-unsupervised-multitask .\n[Rajapakse, 2019] Rajapakse, T. (2019). Simple transformers. https://github.com/\nThilinaRajapakse/simpletransformers.\n[Riedel et al., 2017] Riedel, B., Augenstein, I., Spithourakis, G. P., and Riedel, S. (2017). A\nsimple but tough-to-beat baseline for the Fake News Challenge stance detection task. CoRR,\nabs/1707.03264.\n11\n[Sanh et al., 2019] Sanh, V., Debut, L., Chaumond, J., and Wolf, T. (2019). Distilbert, a distilled\nversion of bert: smaller, faster, cheaper and lighter.\n[Silverman, 2015] Silverman, C. (2015). Emergent: A real-time rumor tracker. http://www.\nemergent.info/.\n[Taylor, 1953] Taylor, W. L. (1953). ”cloze procedure”: A new tool for measuring readability.\nJournalism Bulletin, 30(4):415–433.\n[Trinh and Le, 2018] Trinh, T. H. and Le, Q. V. (2018). A simple method for commonsense\nreasoning. CoRR, abs/1806.02847.\n[Vaswani et al., 2018] Vaswani, A., Bengio, S., Brevdo, E., Chollet, F., Gomez, A. N., Gouws, S.,\nJones, L., Kaiser, L., Kalchbrenner, N., Parmar, N., Sepassi, R., Shazeer, N., and Uszkoreit,\nJ. (2018). Tensor2tensor for neural machine translation. CoRR, abs/1803.07416.\n[Vaswani et al., 2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,\nA. N., Kaiser, L., and Polosukhin, I. (2017). Attention is all you need. CoRR, abs/1706.03762.\n[Wang et al., 2018] Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. R.\n(2018). GLUE: A multi-task benchmark and analysis platform for natural language under-\nstanding. CoRR, abs/1804.07461.\n[Wolf et al., 2019] Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac,\nP., Rault, T., Louf, R., Funtowicz, M., and Brew, J. (2019). Huggingface’s transformers:\nState-of-the-art natural language processing. ArXiv, abs/1910.03771.\n[Xiao, 2018] Xiao, H. (2018). bert-as-service. https://github.com/hanxiao/\nbert-as-service.\n[Yang et al., 2019] Yang, Z., Dai, Z., Yang, Y., Carbonell, J. G., Salakhutdinov, R., and Le, Q. V.\n(2019). Xlnet: Generalized autoregressive pretraining for language understanding. CoRR,\nabs/1906.08237.\n[Zhu et al., 2015] Zhu, Y., Kiros, R., Zemel, R. S., Salakhutdinov, R., Urtasun, R., Torralba, A.,\nand Fidler, S. (2015). Aligning books and movies: Towards story-like visual explanations by\nwatching movies and reading books. CoRR, abs/1506.06724.\n12",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8228157758712769
    },
    {
      "name": "Computer science",
      "score": 0.7341043949127197
    },
    {
      "name": "Embedding",
      "score": 0.679175615310669
    },
    {
      "name": "Sentence",
      "score": 0.5985381603240967
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5109099745750427
    },
    {
      "name": "Transfer of learning",
      "score": 0.4989595413208008
    },
    {
      "name": "Architecture",
      "score": 0.4770504832267761
    },
    {
      "name": "Generalization",
      "score": 0.45189642906188965
    },
    {
      "name": "Language model",
      "score": 0.44996732473373413
    },
    {
      "name": "Natural language processing",
      "score": 0.44359925389289856
    },
    {
      "name": "Speech recognition",
      "score": 0.3329768776893616
    },
    {
      "name": "Electrical engineering",
      "score": 0.14631950855255127
    },
    {
      "name": "Engineering",
      "score": 0.09606686234474182
    },
    {
      "name": "Mathematics",
      "score": 0.073589026927948
    },
    {
      "name": "Art",
      "score": 0.07181879878044128
    },
    {
      "name": "Voltage",
      "score": 0.06699326634407043
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "cited_by": 11
}