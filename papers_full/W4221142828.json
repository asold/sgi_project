{
  "title": "Can Prompt Probe Pretrained Language Models? Understanding the Invisible Risks from a Causal View",
  "url": "https://openalex.org/W4221142828",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3169498763",
      "name": "Boxi Cao",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2098375810",
      "name": "Hongyu Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2171485312",
      "name": "Xianpei Han",
      "affiliations": [
        "Institute of Software",
        "Chinese Academy of Sciences",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2101563253",
      "name": "Fangchao Liu",
      "affiliations": [
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2097220636",
      "name": "Le Sun",
      "affiliations": [
        "Institute of Software",
        "Chinese Academy of Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W3116459227",
    "https://openalex.org/W3173673636",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3213868621",
    "https://openalex.org/W2624319794",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W3104163040",
    "https://openalex.org/W2530395818",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W4312516176",
    "https://openalex.org/W1489525520",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W3154903254",
    "https://openalex.org/W2486285194",
    "https://openalex.org/W2889468083",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2581637843",
    "https://openalex.org/W2251869843",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W3182414949",
    "https://openalex.org/W95780973",
    "https://openalex.org/W2964150944",
    "https://openalex.org/W2509913944",
    "https://openalex.org/W2962781380",
    "https://openalex.org/W2963457723",
    "https://openalex.org/W4287684343",
    "https://openalex.org/W2251919380",
    "https://openalex.org/W2753845591",
    "https://openalex.org/W2798416089",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2951286828",
    "https://openalex.org/W2904443424",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2803125506",
    "https://openalex.org/W3100283070",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3202712981",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4205450747",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W3202415077",
    "https://openalex.org/W3104142662",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2991223644"
  ],
  "abstract": "Prompt-based probing has been widely used in evaluating the abilities of pretrained language models (PLMs). Unfortunately, recent studies have discovered such an evaluation may be inaccurate, inconsistent and unreliable. Furthermore, the lack of understanding its inner workings, combined with its wide applicability, has the potential to lead to unforeseen risks for evaluating and applying PLMs in real-world applications. To discover, understand and quantify the risks, this paper investigates the prompt-based probing from a causal view, highlights three critical biases which could induce biased results and conclusions, and proposes to conduct debiasing via causal intervention. This paper provides valuable insights for the design of unbiased datasets, better probing frameworks and more reliable evaluations of pretrained language models. Furthermore, our conclusions also echo that we need to rethink the criteria for identifying better pretrained language models.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 5796 - 5808\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nCan Prompt Probe Pretrained Language Models? Understanding the\nInvisible Risks from a Causal View\nBoxi Cao1,3, Hongyu Lin1, Xianpei Han1,2,4∗, Fangchao Liu1,3, Le Sun1,2∗\n1Chinese Information Processing Laboratory 2State Key Laboratory of Computer Science\nInstitute of Software, Chinese Academy of Sciences, Beijing, China\n3University of Chinese Academy of Sciences, Beijing, China\n4 Beijing Academy of Artificial Intelligence, Beijing, China\n{boxi2020,hongyu,xianpei,fangchao2017,sunle}@iscas.ac.cn\nAbstract\nPrompt-based probing has been widely used in\nevaluating the abilities of pretrained language\nmodels (PLMs). Unfortunately, recent stud-\nies have discovered such an evaluation may be\ninaccurate, inconsistent and unreliable. Fur-\nthermore, the lack of understanding its inner\nworkings, combined with its wide applicability,\nhas the potential to lead to unforeseen risks for\nevaluating and applying PLMs in real-world\napplications. To discover, understand and quan-\ntify the risks, this paper investigates the prompt-\nbased probing from a causal view, highlights\nthree critical biases which could induce biased\nresults and conclusions, and proposes to con-\nduct debiasing via causal intervention. This pa-\nper provides valuable insights for the design of\nunbiased datasets, better probing frameworks\nand more reliable evaluations of pretrained lan-\nguage models. Furthermore, our conclusions\nalso echo that we need to rethink the criteria for\nidentifying better pretrained language models1.\n1 Introduction\nDuring the past few years, the great success of\npretrained language models (PLMs) (Devlin et al.,\n2019; Liu et al., 2019; Brown et al., 2020; Raffel\net al., 2020) raises extensive attention about eval-\nuating what knowledge do PLMs actually entail.\nOne of the most popular approaches is prompt-\nbased probing (Petroni et al., 2019; Davison et al.,\n2019; Brown et al., 2020; Schick and Schütze,\n2020; Ettinger, 2020; Sun et al., 2021), which\nassesses whether PLMs are knowledgable for a\nspecific task by querying PLMs with task-specific\nprompts. For example, to evaluate whether BERT\nknows the birthplace of Michael Jordan, we could\nquery BERT with “Michael Jordan was born in\n[MASK]”. Recent studies often construct prompt-\nbased probing datasets, and take PLMs’ perfor-\n∗Corresponding Authors\n1We openly released the source code and data at https:\n//github.com/c-box/causalEval.\nEvaluationTarget\nLanguageExpression\nDesirableEffect\nLinguisticCorrelation\nTestData\nPredictorPerformance \nTrainData\nHypothesis ArchitectureAlgorithm…\nVerbalize\nPredictorPLM\nPretrainData\nTestData\nPerformance \nPromptVerbalization\n(b)PLM evaluation via prompt-based probing.Task DistributionalCorrelation\nEvaluationOutcome(a)Conventional evaluationinmachinelearning.\nBundled\nFigure 1: The illustrated procedure for two kinds of\nevaluation criteria.\nmance on these datasets as their abilities for the\ncorresponding tasks. Such a probing evaluation\nhas been wildly used in many benchmarks such\nas SuperGLUE (Wang et al., 2019; Brown et al.,\n2020), LAMA (Petroni et al., 2019), oLMpics (Tal-\nmor et al., 2020), LM diagnostics (Ettinger, 2020),\nCAT (Zhou et al., 2020), X-FACTR (Jiang et al.,\n2020a), BioLAMA (Sung et al., 2021), etc.\nUnfortunately, recent studies have found that\nevaluating PLMs via prompt-based probing could\nbe inaccurate, inconsistent, and unreliable. For\nexample, Poerner et al. (2020) finds that the per-\nformance may be overestimated because many in-\nstances can be easily predicted by only relying\non surface form shortcuts. Elazar et al. (2021)\nshows that semantically equivalent prompts may\nresult in quite different predictions. Cao et al.\n(2021) demonstrates that PLMs often generate un-\nreliable predictions which are prompt-related but\nnot knowledge-related.\nIn these cases, the risks of blindly using prompt-\nbased probing to evaluate PLMs, without under-\nstanding its inherent vulnerabilities, are significant.\nSuch biased evaluations will make us overestimate\nor underestimate the real capabilities of PLMs, mis-\nlead our understanding of models, and result in\n5796\nwrong conclusions. Therefore, to reach a trustwor-\nthy evaluation of PLMs, it is necessary to dive into\nthe probing criteria and understand the following\ntwo critical questions: 1) What biases exist in cur-\nrent evaluation criteria via prompt-based probing?\n2) Where do these biases come from?\nTo this end, we compared PLM evaluation via\nprompt-based probing with conventional evalua-\ntion criteria in machine learning. Figure 1 shows\ntheir divergences. Conventional evaluations aim\nto evaluate different hypotheses (e.g., algorithms\nor model structures) for a specific task. The tested\nhypotheses are raised independently of the train-\ning/test data generation. However, this indepen-\ndence no longer sustains in prompt-based prob-\ning. There exist more complicated implicit con-\nnections between pretrained models, probing data,\nand prompts, mainly due to the bundled pretraining\ndata with specific PLMs. These unaware connec-\ntions serve as invisible hands that can even dom-\ninate the evaluation criteria from both linguistic\nand task aspects. From the linguistic aspect, be-\ncause pretraining data, probing data and prompts\nare all expressed in the form of natural language,\nthere exist inevitable linguistic correlations which\ncan mislead evaluations. From the task aspect, the\npretraining data and the probing data are often sam-\npled from correlated distributions. Such invisible\ntask distributional correlations may significantly\nbias the evaluation. For example, Wikipedia is a\nwidely used pretraining corpus, and many probing\ndata are also sampled from Wikipedia or its exten-\nsions such as Yago, DBPedia or Wikidata (Petroni\net al., 2019; Jiang et al., 2020a; Sung et al., 2021).\nAs a result, such task distributional correlations\nwill inevitably confound evaluations via domain\noverlapping, answer leakage, knowledge coverage,\netc.\nTo theoretically identify how these correlations\nlead to biases, we revisit the prompt-based prob-\ning from a causal view. Specifically, we describe\nthe evaluation procedure using a structural causal\nmodel (Pearl et al., 2000) (SCM), which is shown\nin Figure 2a. Based on the SCM, we find that\nthe linguistic correlation and the task distributional\ncorrelation correspond to three backdoor paths in\nFigure 2b-d, which lead to three critical biases:\n• Prompt Preference Bias, which mainly\nstems from the underlying linguistic corre-\nlations between PLMs and prompts, i.e., the\nperformance may be biased by the fitness of\na prompt to PLMs’ linguistic preference. For\ninstance, semantically equivalent prompts will\nlead to different biased evaluation results.\n• Instance Verbalization Bias, which mainly\nstems from the underlying linguistic correla-\ntions between PLMs and verbalized probing\ndatasets, i.e., the evaluation results are sensi-\ntive and inconsistent to the different verbaliza-\ntions of the same instance (e.g., representing\nthe U.S.A. with the U.S. or America).\n• Sample Disparity Bias, which mainly stems\nfrom the invisible distributional correlation\nbetween pretraining and probing data, i.e.,\nthe performance difference between different\nPLMs may due to the sample disparity of their\npretraining corpus, rather than their ability\ndivergence. Such invisible correlations may\nmislead evaluation results, and thus lead to\nimplicit, unaware risks of applying PLMs in\nreal-world applications.\nWe further propose to conduct causal interven-\ntion via backdoor adjustments, which can reduce\nbias and ensure a more accurate, consistent and re-\nliable probing under given assumptions. Note that\nthis paper not intends to create a “universal cor-\nrect” probing criteria, but to remind the underlying\ninvisible risks, to understand how spurious correla-\ntions lead to biases, and to provide a causal toolkit\nfor debiasing probing under specific assumptions.\nBesides, we believe that our discoveries not only\nexist in prompt-based probing, but will also influ-\nence all prompt-based applications to pretrained\nlanguage models. Consequently, our conclusions\necho that we need to rethink the criteria for identi-\nfying better pretrained language models with the\nabove-mentioned biases.\nGenerally, the main contributions of this paper\nare:\n• We investigate the critical biases and quan-\ntify their risks of evaluating pretrained lan-\nguage models with widely used prompt-based\nprobing, including prompt preference bias, in-\nstance verbalization bias, and sample disparity\nbias.\n• We propose a causal analysis framework,\nwhich can be used to effectively identify,\nunderstand, and eliminate biases in prompt-\nbased probing evaluations.\n5797\n(b)PromptPreferenceBias\nMCL\nEIP\n(c)InstanceVerbalizationBias\nMCLX\nE\n (d)SampleDisparityBias\nMCDa\nE\nTDb\nX\n(a)StructuralCausalModel\nMCDa\nX\nLP\nE\nR\nI TDb\nSCM BackdoorPaths\nCausalRelationTrue Causal Effect\nConfounderVariabletoBlockBackdoorPath\nBackdoorPath\nTreatmentOutcome\nFigure 2: The structural causal model for factual knowledge probing and the three backdoor paths in SCM correspond\nto three biases.\n• We provide valuable insights for the design of\nunbiased datasets, better probing frameworks,\nand more reliable evaluations, and echo that\nwe should rethink the evaluation criteria for\npretrained language models.\n2 Background and Experimental Setup\n2.1 Causal Inference\nCausal inference is a promising technique for iden-\ntifying undesirable biases and fairness concerns in\nbenchmarks (Hardt et al., 2016; Kilbertus et al.,\n2017; Kusner et al., 2017; Vig et al., 2020; Feder\net al., 2021). Causal inference usually describes the\ncausal relations between variables via Structural\nCausal Model (SCM), then recognizes confounders\nand spurious correlations for bias analysis, finally\nidentifies true causal effects by eliminating biases\nusing causal intervention techniques.\nSCM The structural causal model (Pearl et al.,\n2000) describes the relevant features in a system\nand how they interact with each other. Every\nSCM is associated with a graphical causal model\nG = {V, f}, which consists of a set of nodes rep-\nresenting variables V , as well as a set of edges\nbetween the nodes representing the functions f to\ndescribe the causal relations.\nCausal Intervention To identify the true causal\neffects between an ordered pair of variables(X, Y),\nCausal intervention fixes the value of X = x and\nremoves the correlations between X and its prece-\ndent variables, which is denoted as do(X = x). In\nthis way,P(Y = y|do(X = x)) represents the true\ncausal effects of treatment X on outcome Y (Pearl\net al., 2016).\nBackdoor Path When estimating the causal ef-\nfect of X on Y , the backdoor paths are the non-\ncausal paths between X and Y with an arrow into\nX, e.g., X ← Z → Y . Such paths will confound\nthe effect that X has on Y but not transmit causal\ninfluences from X, and therefore introduce spuri-\nous correlations between X and Y .\nBackdoor Criterion The Backdoor Criterion is\nan important tool for causal intervention. Given an\nordered pair of variables (X, Y) in SCM, and a set\nof variables Z where Z contains no descendant of\nX and blocks every backdoor path between X and\nY , then the causal effects of X = x on Y can be\ncalculated by:\nP(Y = y|do(X = x)) =\nX\nz\nP(Y = y|X = x, Z= z)P(Z = z), (1)\nwhere P(Z = z) can be estimated from data or\npriorly given, and is independent of X.\n2.2 Experimental Setup\nTask This paper investigates prompt-based prob-\ning on one of the most representative and well-\nstudied tasks – factual knowledge probing (Liu\net al., 2021b). For example, to evaluate whether\nBERT knows the birthplace of Michael Jordan,\nfactual knowledge probing queries BERT with\n“Michael Jordan was born in [MASK]”, where\nMichael Jordan is the verbalized subject men-\ntion, “was born in”is the verbalized prompt of rela-\ntion birthplace, and [MASK] is a placeholder\nfor the target object.\nData We use LAMA (Petroni et al., 2019) as our\nprimary dataset, which is a set of knowledge triples\nsampled from Wikidata. We remove the N-M rela-\ntions (Elazar et al., 2021) which are unsuitable for\nthe P@1 metric and retain 32 probing relations in\nthe dataset. Please refer to the appendix for detail.\n5798\nPretrained Models We conduct probing experi-\nments on 4 well-known PLMs: BERT (Devlin et al.,\n2019), RoBERTa (Liu et al., 2019), GPT-2 (Rad-\nford et al., 2019) and BART (Lewis et al., 2020),\nwhich correspond to 3 representative PLM archi-\ntectures, including autoencoder (BERT, RoBERTa),\nautoregressive (GPT-2) and denoising autoencoder\n(BART).\n3 Structural Causal Model for Factual\nKnowledge Probing\nIn this section, we formulate the SCM for factual\nknowledge probing procedure and describe the key\nvariables and causal relations.\nThe SCM is shown in Figure 2a, which con-\ntains 11 key variables: 1) Pretraining corpus\ndistribution Da; 2) Pretraining corpus C, e.g.,\nWebtext for GPT2, Wikipedia for BERT; 3) Pre-\ntrained language model M; 4) Linguistic distri-\nbution L, which guides how a concept is verbal-\nized into natural language expression, e.g., rela-\ntion to prompt, entity to mention; 5) Relation R,\ne.g., birthplace, capital, each relation cor-\nresponds to a probing task; 6) Verbalized prompt\nP for each relation , e.g, x was born in y; 7) Task-\nspecific predictor I, which is a PLM combined\nwith a prompt, e.g., <BERT, was born in > as a\nbirthplace predictor; 8) Probing data distri-\nbution Db, e.g., fact distribution in Wikidata; 9)\nSampled probing data T such as LAMA, which\nare sampled entity pairs (e.g., <Q41421, Q18419>\nin Wikidata) of relation R; 10) Verbalized in-\nstances X, (e.g., <Michael Jordan, Brooklyn>\nfrom <Q41421, Q18419>); 11) Performance E of\nthe predictor I on X.\nThe causal paths of the prompt-based probing\nevaluation contains:\n• PLM Pretraining. The path {Da, L} →\nC → M represents the pretraining procedure\nfor language model M, which first samples\npretraining corpus C according to pretraining\ncorpus distribution Da and linguistic distribu-\ntion L, then pretrains M on C.\n• Prompt Selection. The path {R, L} →P\nrepresents the prompt selection procedure,\nwhere each prompt P must exactly express\nthe semantics of relation R, and will be influ-\nenced by the linguistic distribution L.\n• Verbalized Instances Generation. The path\n{Db, R} →T → X ← L represents the\ngeneration procedure of verbalized probing\ninstances X, which first samples probing data\nT of relation R according to data distribution\nDb, then verbalizes the sampled data T into\nX according to the linguistic distribution L.\n• Performance Estimation. The path\n{M, P} →I → E ← X represents the\nperformance estimation procedure, where the\npredictor I is first derived by combining PLM\nM and prompt P, and then the performance\nE is estimated by applying predictor I on ver-\nbalized instances X.\nTo evaluate PLMs’ ability on fact extraction,\nwe need to estimate P(E|do(M = m), R= r).\nSuch true causal effects are represented by the path\nM → I → E in SCM. Unfortunately, there exist\nthree backdoor paths between pretrained language\nmodel M and performance E, as shown in Fig-\nure 2b-d. These spurious correlations make the\nobservation correlation between M and E cannot\nrepresent the true causal effects of M on E, and\nwill inevitably lead to biased evaluations. In the\nfollowing, we identify three critical biases in the\nprompt-based probing evaluation and describe the\nmanifestations, causes, and casual interventions for\neach bias.\n4 Prompt Preference Bias\nIn prompt-based probing, the predictor of a spe-\ncific task (e.g., the knowledge extractor of rela-\ntion birthplace) is a PLM M combined with a\nprompt P (e.g., BERT + was born in). However,\nPLMs are pretrained on specific text corpus, there-\nfore will inevitably prefer prompts sharing the same\nlinguistic regularity with their pretraining corpus.\nSuch implicit prompt preference will confound the\ntrue causal effects of PLMs on evaluation perfor-\nmance, i.e., the performance will be affected by\nboth the task ability of PLMs and the preference\nfitness of a prompt. In the following, we investigate\nprompt preference bias via causal analysis.\n4.1 Prompt Preference Leads to Inconsistent\nPerformance\nIn factual knowledge probing, we commonly as-\nsign one prompt for each relation (e.g., X was\nborn in Y for birthplace). However, dif-\nferent PLMs may prefer different prompts, and it is\nunable to disentangle the influence of prompt pref-\nerence from the final performance. Such invisible\n5799\nLanguageContinentReligionOwned by01020304050607080BERT-largeRoBERTa-largeGPT2-xlBART-large\nFigure 3: The variances of P@1 performance of 4 PLMs\non 4 relations using semantically equivalent prompts.\nWe can see the performance varies significantly.\nprompt preference will therefore lead to inconsis-\ntent conclusions.\nTo demonstrate this problem, we report the\nperformance variance on LAMA using different\nprompts for each PLM. For each relation, we follow\nElazar et al. (2021); Jiang et al. (2020b) and design\nat least 5 prompts that are semantically equivalent\nand faithful but vary in linguistic expressions.\nPrompt selection significantly affects perfor-\nmance. Figure 3 illustrates the performance on\nseveral relations, where the performances of all\nPLMs vary significantly on semantically equiv-\nalent prompts. For instance, by using different\nprompts, the Precision@1 of relation languages\nspoken dramatically changing from 3.90% to\n65.44% on BERT-large, and from 0.22% to 71.94%\non BART-large. This result is shocking, because\nthe same PLM can be assessed from “knowing\nnothing” to “sufficiently good” by only changing\nits prompt. Table 1 further shows the quantitative\nresults, for BERT-large, the averaged standard de-\nviation of Precision@1 of different prompts is 8.75.\nAnd the prompt selection might result in larger per-\nformance variation than model selection: on more\nthan 70% of relations, the best and worst prompts\nwill lead to >10 point variation at Precision@1,\nwhich is larger than the majority of performance\ngaps between different models.\nPrompt preference also leads to inconsistent\ncomparisons. Figure 4 demonstrates an exam-\nple, where the ranks of PLMs are significantly\nchanged when applying diverse prompts. We also\nconduct quantitative experiments, which show that\nthe PLMs’ ranks on 96.88% relations are unstable\nwhen prompt varies.\nAll these results demonstrate that the prompt\npreference bias will result in inconsistent perfor-\nModels LAMA P@1 Worst P@1 Best P@1 Std\nBERT-large 39.08 23.45 46.73 8.75\nRoBERTa-large 32.27 15.64 41.35 9.07\nGPT2-xl 24.19 11.19 33.52 8.56\nBART-large 27.68 16.21 38.93 8.35\nTable 1: The P@1 performance divergence of prompt\nselection averaged over all relations, we can see prompt\npreference results in inconsistent performance.\n0\n15\n30\n45\nX is owned by YX is a goods of YThe owner of X is YX belongs to Y\nBERT-largeRoBERTa-largeGPT2-xlBART-large\nFigure 4: The P@1 performance of 4 PLMs using 4 dif-\nferent prompts of relation owned by, where the rank\nof 4 PLMs is unstable on different prompts: prompt\npreference leads to 3 distinct “best” models and 3 dis-\ntinct “worst” models.\nmance. Such inconsistent performance will fur-\nther lead to unstable comparisons between different\nPLMs, and therefore significantly undermines the\nevaluations via prompt-based probing.\n4.2 Cause of Prompt Preference Bias\nFigure 2b shows the cause of the prompt preference\nbias. When evaluating the ability of PLMs on spe-\ncific tasks, we would like to measure the causal ef-\nfects of path M → I → E. However, because the\nprompt P and the PLM M are all correlated to the\nlinguistic distribution L, there is a backdoor path\nM ← C ← L → P → I → E between PLM M\nand performance E. Consequently, the backdoor\npath will confound the effects of M → I → E\nwith P → I → E.\nBased on the above analysis, the prompt prefer-\nence bias can be eliminated by blocking this back-\ndoor path via backdoor adjustment, which requires\na prior formulation of the distribution P(P). In\nSection 7, we will present one possible causal in-\ntervention formulation which can lead to more con-\nsistent evaluations.\n5 Instance Verbalization Bias\nApart from the prompt preference bias, the under-\nlying linguistic correlation can also induce bias in\nthe instance verbalization process. Specifically, an\ninstance in probing data can be verbalized into dif-\nferent natural language expressions (e.g., verbalize\n5800\nRelation Mention Prediction\nCapital of\nAmerica Chicago\nthe U.S. Washington\nChina Beijing\nCathay Bangkok\nBirthplace\nEinstein Berlin\nAlbert Einstein Vienna\nIsaac Newton London\nSir Isaac Newton town\nTable 2: Different verbalized names of the same entity\nlead to different predictions on BERT-large.\nQ30 in Wikidata into America or the U.S.),\nand different PLMs may prefer different verbaliza-\ntions due to mention coverage, expression prefer-\nence, etc. This will lead to instance verbalization\nbias.\n5.1 Instance Verbalization Brings Unstable\nPredictions\nIn factual knowledge probing, each entity is verbal-\nized to its default name. However, different PLMs\nmay prefer different verbalizations, and such under-\nlying correlation is invisible. Because we couldn’t\nmeasure how this correlation affects probing per-\nformance, the evaluation may be unstable using\ndifferent verbalizations.\nTable 2 shows some intuitive examples. When\nwe query BERT “The capital of the U.S. is\n[MASK]”, the answer is Washington. Mean-\nwhile, BERT would predict Chicago if we re-\nplace the U.S. to its alias America. Such un-\nstable predictions make us unable to obtain reliable\nconclusions on whether or to what degree PLMs\nactually entail the knowledge.\nTo quantify the effect of instance verbalization\nbias, we collect at most 5 verbalizations for each\nsubject entity in LAMA from Wikidata, and cal-\nculate the verbalization stability on each relation,\ni.e., the percentage of relation instances whose pre-\ndictions are unchanged when verbalization varies.\nThe results in Figure 5 show the average verbaliza-\ntion stabilities of all four PLMs are < 40%, which\ndemonstrate that the instance verbalization bias\nwill bring unstable and unreliable evaluation.\n5.2 Cause of Instance Verbalization Bias\nFigure 2c shows the cause of instance verbalization\nbias: the backdoor path M ← C ← L → X → E,\nwhich stems from the confounder of linguistic dis-\ntribution L between pretraining corpus C and ver-\nbalized probing data X. Consequently, the ob-\n20 40 60 80 100\nBERT\nRoBERT a\nGPT2\nBART\nFigure 5: The verbalization stabilities of 4 PLMs on all\nrelations, which is measured by the percentage of rela-\ntion instances whose predictions are unchanged when\nverbalization varies. We can see that the verbalization\nstabilities of all 4 PLMs (BERT-large, RoBERTa-large,\nGPT2-xl, BART-large) are poor.\nserved correlation between M and E couldn’t faith-\nfully represent the true causal effect of M on E,\nbut is also mixed up the spurious correlation caused\nby the backdoor path.\nThe instance verbalization bias can be eliminated\nby blocking this backdoor path via causal interven-\ntion, which requires a distribution formulation of\nthe instance verbalization, i.e., P(X). We will\npresent a possible intervention formulation in Sec-\ntion 7.\n6 Sample Disparity Bias\nBesides the biases induced by linguistic correla-\ntions, the distributional correlations between pre-\ntraining corpus and task-specific probing data can\nalso introduce sample disparity bias. That is, the\nperformance difference between different PLMs\nmay due to the sample disparity of their pretraining\ncorpus, rather than their ability divergence.\nIn conventional evaluation, the evaluated hy-\npotheses are independent of the train/test data gen-\neration, and all the hypotheses are evaluated on\ntraining data and test data generated from the same\ndistribution. Therefore, the impact of correlations\nbetween training data and test data is transparent,\ncontrollable, and equal for all the hypotheses. By\ncontrast, in prompt-based probing, each PLM is\nbundled with a unique pretraining corpus, the corre-\nlation between pretraining corpus distribution and\nprobing data distribution cannot be quantified. In\nthe following we investigate this sample disparity\nbias in detail.\n6.1 Sample Disparity Brings Biased\nPerformance\nIn factual knowledge probing, LAMA (Petroni\net al., 2019), a subset sampled from Wikidata,\n5801\nγ% BERT-base BERT-large GPT2-base GPT2-medium\n0% 30.54 33.08 15.22 22.11\n20% 35.77 39.56 22.02 28.21\n40% 38.68 39.75 24.32 30.29\n60% 38.72 40.68 25.42 31.16\n80% 39.79 41.48 25.65 31.88\n100% 40.15 42.51 26.82 33.12\nNone 37.13 39.08 16.88 22.60\nTable 3: The P@1 on LAMA of PLMs whose further\npretraining data are with different correlation degrees\nγ% with LAMA. The BERT-base and GPT2-base both\ncontain 12 layers, while BERT-large and GPT2-medium\nboth contain 24 layers.\nis commonly used to compare different PLMs.\nPrevious work claims that GPT-style models are\nwith weaker factual knowledge extraction abili-\nties than BERT because they perform worse on\nLAMA (Petroni et al., 2019; Liu et al., 2021c).\nHowever, because PLMs are pretrained on differ-\nent pretraining corpus, the performance divergence\ncan stem from the spurious correlation between\npretraining corpus and LAMA, rather than their\nability difference. For example, BERT’s superior\nperformance to GPT-2 may stem from the diver-\ngence of their pretraining corpus, where BERT’s\npretraining corpus contains Wikipedia, while GPT-\n2’s pretraining corpus doesn’t.\nTo verify the effect of sample disparity bias, we\nfurther pretrain BERT and GPT-2 by constructing\npretraining datasets with different correlation de-\ngrees to LAMA, and report their new performances\non LAMA. Specifically, we use the Wikipedia snip-\npets in LAMA and collect a 99k-sentence dataset,\nnamed WIKI-LAMA. Then we create a series of\npretraining datasets by mixing the sentences from\nWIKI-LAMA with WebText2 (the pretraining cor-\npus of GPT2). That is, we fix all datasets’ size\nto 99k, and a parameter γ is used to control the\nmixture degree: for each dataset, there are γ% in-\nstances sampled from WIKI-LAMA and 1 − γ%\ninstances sampled from WebText. Please refer to\nthe appendix for pretraining detail.\nTable 3 demonstrates the effect of sample dis-\nparity bias. We can see that 1) Sample disparity\nsignificantly influences the PLMs’ performance:\nthe larger correlation degree γ will result in better\nperformance for both BERT and GPT-2; 2) Sample\ndisparity contributes to the performance difference.\nWe can see that the performance gap between GPT-\n2 and BERT significantly narrows down when they\n2http://Skylion007.github.io/\nOpenWebTextCorpus\nare further pretrained using the same data. Besides,\nfurther pretraining BERT on WebText (γ=0) would\nsignificantly undermine its performance. These re-\nsults strongly confirm that the sample disparity will\nsignificantly bias the probing conclusion.\n6.2 Cause of Sample Disparity Bias\nThe cause of sample disparity bias may diverge\nfrom PLMs and scenarios due to the different\ncausal relation between pretraining corpus distribu-\ntion Da and probing data distribution Db. Never-\ntheless, sample disparity bias always exist because\nthe backdoor path will be M ← C ← Da →\nDb → T → X → E when Da is the ancestor of\nDb, or M ← C ← Da ← Db → T → X → E\nwhen Da is the descendant of Db. Figure 2d shows\na common case when the pretraining corpus dis-\ntribution Da is an ancestor of probing data dis-\ntribution Db. For example, the pretraining data\ncontains Wikipedia and probing data is a sampled\nsubset from Wikipedia (e.g., LAMA, X-FACTR,\nBioLAMA). As a result, there is a backdoor path\nbetween M and E, which will mislead the evalua-\ntion.\n7 Bias Elimination via Causal\nIntervention\nThis section describes how to eliminate the above-\nmentioned biases by blocking their corresponding\nbackdoor paths. According to the Backdoor Cri-\nterion in Section 2.1, we need to choose a set of\nvariables Z that can block every path containing an\narrow into M between M and E. Since the linguis-\ntic distribution L, pretraining corpus distribution\nDa and probing data distribution Db are unobserv-\nable, we choose Z = {P, X} as the variable set\nfor blocking all backdoor paths between (M, E) in\nthe SCM by conducting backdoor adjustment:\nP(E|do(M = m), R= r) =\nX\np∈P\nX\nx∈X\nP(p, x)P(E|m, r, p, x). (2)\nEquation 2 provides an intuitive solution. To\neliminate the biases stemming from the spurious\ncorrelations between pretraining corpus, probing\ndata and prompts, we need to consider the natu-\nral distribution of prompts and verbalized probing\ndata regardless of other factors. Consequently, the\noverall causal effects between PLM and evaluation\nresult are the weighted averaged effects on all valid\nprompts and probing data.\n5802\nModel Original Random +Intervention\nBERT-base 56.4 45.4 86.5\nBERT-large 100.0 78.1 100.0\nRoBERTa-base 75.7 44.0 77.8\nRoBERTa-large 56.1 42.2 86.5\nGPT2-medium 63.5 40.7 98.2\nGPT2-xl 74.2 35.7 77.8\nBART-base 63.4 61.6 98.2\nBART-large 97.7 61.3 100.0\nOverall Rank 25.5 5.5 68.5\nTable 4: The rank consistencies over 1000 task samples\n(each task contains 20 relations from LAMA). For a\nPLM, the rank consistency is the percentage of its most\npopular rank in 1000 runtimes. For “Overall Rank”, the\nrank consistency is the percentage of the most popular\nrank of all PLMs in 1000 runtimes, i.e., the rank of all\nPLMs remains the same. “Original” means that we use\nthe LAMA’s original prompts and verbalized names,\n“Random” means that we randomly sample prompts and\nverbalized names every time, “+Intervention” means\nthat we apply causal intervention. We can see that the\nrank consistency is significantly raised after causal in-\ntervention.\nUnfortunately, the exact distribution of P(x, p)\nis intractable , which needs to iterate over all valid\nprompts and all verbalized probing data. To ad-\ndress this problem, we propose a sampling-based\napproximation. Specifically, given a specific as-\nsumption about P(x, p) (we assume uniform distri-\nbution in this paper without the loss of generality),\nwe sample Kp prompts for each relation and Kx\nkinds of verbalization for each instance according\nto P(x, p), and then these samples are used to es-\ntimate the true causal effects between M and E\naccording to Equation 2.\nTo verify whether causal intervention can im-\nprove the evaluation consistency and robustness,\nwe conduct backdoor adjustment experiments on\n8 different PLMs. We randomly sample 1000 sub-\nsets with 20 relations from LAMA, and observe\nwhether the evaluation conclusions were consis-\ntent and stable across the 1000 evaluation runtimes.\nSpecifically, we use rank consistencyas the evalu-\nation metric, which measures the percentage of the\nmost popular rank of each model in 1000 runtimes.\nFor example, if BERT ranks at 3rd place in 800\nof the 1000 runtimes, then the rank consistency of\nBERT will be 80%.\nTable 4 shows the results. We can see that causal\nintervention can significantly improve the evalu-\nation consistency: 1) The consistency of current\nprompt-based probing evaluations is very poor on\nall 8 PLMs: when we randomly select prompts and\nverbalizations during each sampling, the overall\nrank consistency is only 5.5%; 2) Causal interven-\ntion can significantly improve overall rank consis-\ntency: from 5.5% to 68.5%; 3) Casual intervention\ncan consistently improve the rank consistency of\ndifferent PLMs: the rank of most PLMs is very\nstable after backdoor adjustment.\nThe above results verify that causal intervention\nis an effective technique to boost the stability of\nevaluation, and reach more consistent conclusions.\n8 Related Work\nPrompt-based Probing Prompt-based probing\nis popular in recent years (Rogers et al., 2020;\nLiu et al., 2021b) for probing factual knowl-\nedge (Petroni et al., 2019; Jiang et al., 2020a; Sung\net al., 2021), commonsense knowledge (Davison\net al., 2019), semantic knowledge (Ettinger, 2020;\nSun et al., 2021; Brown et al., 2020; Schick and\nSchütze, 2020) and syntactic knowledge (Ettinger,\n2020) in PLMs. And a series of prompt-tuning\nstudies consider optimizing prompts on training\ndatasets with better performance but may under-\nmine interpretability (Jiang et al., 2020b; Shin et al.,\n2020; Haviv et al., 2021; Gao et al., 2021; Qin and\nEisner, 2021; Li and Liang, 2021; Zhong et al.,\n2021). Because such prompt-tuning operations will\nintroduce additional parameters and more unknown\ncorrelations, this paper does not take prompt-tuning\ninto our SCM, delegate this to future work.\nBiases in NLP Evaluations Evaluation is the cor-\nnerstone for NLP progress. In recent years, many\nstudies aim to investigate the underlying biases and\nrisks in evaluations. Related studies include inves-\ntigating inherent bias in current metrics (Cough-\nlin, 2003; Callison-Burch et al., 2006; Li et al.,\n2017; Sai et al., 2019, 2020), exploring dataset\nartifacts in data collection and annotation proce-\ndure (Lai and Hockenmaier, 2014; Marelli et al.,\n2014; Chen et al., 2018; Levy and Dagan, 2016;\nSchwartz et al., 2017; Cirik et al., 2018; McCoy\net al., 2019; Liu et al., 2021a; Branco et al., 2021),\nand identifying the spurious correlations between\ndata and label which might result in catastrophic\nout-of-distribution robustness of models (Poliak\net al., 2018; Rudinger et al., 2018; Rashkin et al.,\n2018).\nMost previous studies demonstrate the evalua-\ntion biases empirically, and interpret the underlying\nreasons intuitively. However, intuitive explanations\nare also difficult to critical and extend. In contrast,\n5803\nthis paper investigates the biases in prompt-based\nprobing evaluations from a causal view. Based on\nthe causal analysis framework, we can identify, un-\nderstand, and eliminate biases theoretically, which\ncan be extended and adapted to other evaluation\nsettings in a principled manner3. We believe both\nthe causal analysis tools and the valuable insights\ncan benefit future researches.\n9 Conclusions and Discussions\nThis paper investigates the critical biases and quan-\ntifies their risks in the widely used prompt-based\nprobing evaluation, including prompt preference\nbias, instance verbalization bias, and sample dispar-\nity bias. A causal analysis framework is proposed\nto provide a unified framework for bias identifica-\ntion, interpretation and elimination with a theoreti-\ncal guarantee. Our studies can promote the under-\nstanding of prompt-based probing, remind the risks\nof current unreliable evaluations, guide the design\nof unbiased datasets, better probing frameworks,\nand more reliable evaluations, and push the bias\nanalysis from empirical to theoretical.\nAnother benefit of this paper is to remind the\nevaluation criteria shifts from conventional ma-\nchine learning algorithms to pretrained language\nmodels. As we demonstrate in Figure 1, in conven-\ntional evaluation, the evaluated hypotheses (e.g.,\nalgorithms, architectures) are raised independently\nof the train/test dataset generation, where the im-\npact of correlations between training data and test\ndata is transparent, controllable, and equal for all\nthe hypotheses. However, in evaluations of pre-\ntrained language models, the pretraining corpus is\nbundled with the model architecture. In this case,\nit is significant to distinguish what you need to as-\nsess (architecture, corpus, or both), as well as the\npotential risks raised by the correlations between\npretraining corpus and test data, which most current\nbenchmarks have ignored. Consequently, this pa-\nper echoes that it is necessary to rethink the criteria\nfor identifying better pretrained language models,\nespecially under the prompt-based paradigm.\nIn the future, we would like to extend our causal\nanalysis framework to fit prompt-tuning based prob-\ning criteria and all PLM-based evaluations.\nAcknowledgments\nWe sincerely thank all anonymous reviewers for\ntheir insightful comments and valuable sugges-\n3Greatly inspired by the reviewer’s valuable comments.\ntions. This research work is supported by the Na-\ntional Natural Science Foundation of China under\nGrants no. 62122077, the Strategic Priority Re-\nsearch Program of Chinese Academy of Sciences\nunder Grant No. XDA27020200, and the National\nNatural Science Foundation of China under Grants\nno. 62106251 and 62076233.\nEthics Consideration\nThis paper has no particular ethic consideration.\nReferences\nRuben Branco, António Branco, João António Ro-\ndrigues, and João Ricardo Silva. 2021. Shortcutted\ncommonsense: Data spuriousness in deep learning\nof commonsense reasoning. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1504–1521, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nChris Callison-Burch, Miles Osborne, and Philipp\nKoehn. 2006. Re-evaluating the role of Bleu in ma-\nchine translation research. In 11th Conference of\nthe European Chapter of the Association for Com-\nputational Linguistics, pages 249–256, Trento, Italy.\nAssociation for Computational Linguistics.\nBoxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-\nong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.\nKnowledgeable or educated guess? revisiting lan-\nguage models as knowledge bases. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1860–1874, Online.\nAssociation for Computational Linguistics.\nQian Chen, Xiaodan Zhu, Zhen-Hua Ling, Diana\nInkpen, and Si Wei. 2018. Neural natural language\ninference models enhanced with external knowledge.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 2406–2417, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\n5804\nV olkan Cirik, Louis-Philippe Morency, and Taylor Berg-\nKirkpatrick. 2018. Visual referring expression recog-\nnition: What do systems actually learn? In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 2 (Short Papers), pages 781–787, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nDeborah Coughlin. 2003. Correlating automated and\nhuman assessments of machine translation quality.\nIn Proceedings of Machine Translation Summit IX:\nPapers, New Orleans, USA.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 1173–1178, Hong Kong, China. Association\nfor Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nYanai Elazar, Nora Kassner, Shauli Ravfogel, Abhi-\nlasha Ravichander, Eduard Hovy, Hinrich Schütze,\nand Yoav Goldberg. 2021. Measuring and improving\nconsistency in pretrained language models. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:1012–1031.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nAmir Feder, Katherine A Keith, Emaad Manzoor, Reid\nPryzant, Dhanya Sridhar, Zach Wood-Doughty, Jacob\nEisenstein, Justin Grimmer, Roi Reichart, Margaret E\nRoberts, et al. 2021. Causal inference in natural lan-\nguage processing: Estimation, prediction, interpreta-\ntion and beyond. ArXiv preprint, abs/2109.00725.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830, Online. Association for Computa-\ntional Linguistics.\nMoritz Hardt, Eric Price, and Nati Srebro. 2016. Equal-\nity of opportunity in supervised learning. In Ad-\nvances in Neural Information Processing Systems 29:\nAnnual Conference on Neural Information Process-\ning Systems 2016, December 5-10, 2016, Barcelona,\nSpain, pages 3315–3323.\nAdi Haviv, Jonathan Berant, and Amir Globerson. 2021.\nBERTese: Learning to speak to BERT. In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 3618–3623, Online. Association\nfor Computational Linguistics.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020a. X-FACTR:\nMultilingual factual knowledge retrieval from pre-\ntrained language models. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 5943–5959, On-\nline. Association for Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020b. How can we know what language\nmodels know? Transactions of the Association for\nComputational Linguistics, 8:423–438.\nNiki Kilbertus, Mateo Rojas-Carulla, Giambattista\nParascandolo, Moritz Hardt, Dominik Janzing, and\nBernhard Schölkopf. 2017. Avoiding discrimination\nthrough causal reasoning. In Advances in Neural\nInformation Processing Systems 30: Annual Confer-\nence on Neural Information Processing Systems 2017,\nDecember 4-9, 2017, Long Beach, CA, USA, pages\n656–666.\nMatt J. Kusner, Joshua R. Loftus, Chris Russell, and\nRicardo Silva. 2017. Counterfactual fairness. In Ad-\nvances in Neural Information Processing Systems 30:\nAnnual Conference on Neural Information Process-\ning Systems 2017, December 4-9, 2017, Long Beach,\nCA, USA, pages 4066–4076.\nAlice Lai and Julia Hockenmaier. 2014. Illinois-LH: A\ndenotational and distributional approach to semantics.\nIn Proceedings of the 8th International Workshop on\nSemantic Evaluation (SemEval 2014) , pages 329–\n334, Dublin, Ireland. Association for Computational\nLinguistics.\nOmer Levy and Ido Dagan. 2016. Annotating rela-\ntion inference in context via question answering. In\nProceedings of the 54th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 249–255, Berlin, Germany. As-\nsociation for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\n5805\nJiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean, Alan\nRitter, and Dan Jurafsky. 2017. Adversarial learning\nfor neural dialogue generation. In Proceedings of\nthe 2017 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2157–2169, Copen-\nhagen, Denmark. Association for Computational Lin-\nguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nFangyu Liu, Emanuele Bugliarello, Edoardo Maria\nPonti, Siva Reddy, Nigel Collier, and Desmond El-\nliott. 2021a. Visually grounded reasoning across\nlanguages and cultures. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 10467–10485, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021b. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nArXiv preprint, abs/2107.13586.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021c. GPT\nunderstands, too. ArXiv preprint, abs/2103.10385.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. ArXiv preprint, abs/1907.11692.\nMarco Marelli, Luisa Bentivogli, Marco Baroni, Raf-\nfaella Bernardi, Stefano Menini, and Roberto Zam-\nparelli. 2014. SemEval-2014 task 1: Evaluation of\ncompositional distributional semantic models on full\nsentences through semantic relatedness and textual\nentailment. In Proceedings of the 8th International\nWorkshop on Semantic Evaluation (SemEval 2014),\npages 1–8, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right\nfor the wrong reasons: Diagnosing syntactic heuris-\ntics in natural language inference. In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3428–3448, Florence,\nItaly. Association for Computational Linguistics.\nJudea Pearl, Madelyn Glymour, and Nicholas P Jewell.\n2016. Causal inference in statistics: A primer. John\nWiley & Sons.\nJudea Pearl et al. 2000. Models, reasoning and infer-\nence. Cambridge, UK: CambridgeUniversityPress,\n19.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze. 2020.\nE-BERT: Efficient-yet-effective entity embeddings\nfor BERT. In Findings of the Association for Compu-\ntational Linguistics: EMNLP 2020, pages 803–818,\nOnline. Association for Computational Linguistics.\nAdam Poliak, Aparajita Haldar, Rachel Rudinger, J. Ed-\nward Hu, Ellie Pavlick, Aaron Steven White, and\nBenjamin Van Durme. 2018. Collecting diverse nat-\nural language inference problems for sentence rep-\nresentation evaluation. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 67–81, Brussels, Belgium.\nAssociation for Computational Linguistics.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203–5212, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nHannah Rashkin, Maarten Sap, Emily Allaway, Noah A.\nSmith, and Yejin Choi. 2018. Event2Mind: Com-\nmonsense inference on events, intents, and reactions.\nIn Proceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1:\nLong Papers), pages 463–473, Melbourne, Australia.\nAssociation for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A primer in BERTology: What we know about\nhow BERT works. Transactions of the Association\nfor Computational Linguistics, 8:842–866.\nRachel Rudinger, Jason Naradowsky, Brian Leonard,\nand Benjamin Van Durme. 2018. Gender bias in\ncoreference resolution. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 2 (Short Papers) ,\npages 8–14, New Orleans, Louisiana. Association for\nComputational Linguistics.\n5806\nAnanya B. Sai, Mithun Das Gupta, Mitesh M. Khapra,\nand Mukundhan Srinivasan. 2019. Re-evaluating\nADEM: A deeper look at scoring dialogue responses.\nIn The Thirty-Third AAAI Conference on Artificial\nIntelligence, AAAI 2019, The Thirty-First Innova-\ntive Applications of Artificial Intelligence Conference,\nIAAI 2019, The Ninth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI 2019,\nHonolulu, Hawaii, USA, January 27 - February 1,\n2019, pages 6220–6227. AAAI Press.\nAnanya B. Sai, Akash Kumar Mohankumar, and\nMitesh M. Khapra. 2020. A survey of evaluation\nmetrics used for NLG systems. ArXiv preprint ,\nabs/2008.12009.\nTimo Schick and Hinrich Schütze. 2020. Few-Shot Text\nGeneration with Pattern-Exploiting Training. ArXiv\npreprint, abs/2012.11926.\nRoy Schwartz, Maarten Sap, Ioannis Konstas, Leila\nZilles, Yejin Choi, and Noah A. Smith. 2017. The\neffect of different writing tasks on linguistic style:\nA case study of the ROC story cloze task. In Pro-\nceedings of the 21st Conference on Computational\nNatural Language Learning (CoNLL 2017) , pages\n15–25, Vancouver, Canada. Association for Compu-\ntational Linguistics.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV , Eric\nWallace, and Sameer Singh. 2020. AutoPrompt: Elic-\niting Knowledge from Language Models with Auto-\nmatically Generated Prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222–4235,\nOnline. Association for Computational Linguistics.\nYu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding,\nChao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen,\nYanbin Zhao, Yuxiang Lu, Weixin Liu, Zhihua Wu,\nWeibao Gong, Jianzhong Liang, Zhizhou Shang,\nPeng Sun, Wei Liu, Xuan Ouyang, Dianhai Yu, Hao\nTian, Hua Wu, and Haifeng Wang. 2021. ERNIE\n3.0: Large-scale knowledge enhanced pre-training\nfor language understanding and generation. ArXiv\npreprint, abs/2107.02137.\nMujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sung-\ndong Kim, and Jaewoo Kang. 2021. Can language\nmodels be biomedical knowledge bases? In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 4723–4734,\nOnline and Punta Cana, Dominican Republic. Asso-\nciation for Computational Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. oLMpics-on what language\nmodel pre-training captures. Transactions of the As-\nsociation for Computational Linguistics, 8:743–758.\nJesse Vig, Sebastian Gehrmann, Yonatan Belinkov,\nSharon Qian, Daniel Nevo, Yaron Singer, and Stu-\nart M. Shieber. 2020. Investigating gender bias in\nlanguage models using causal mediation analysis.\nIn Advances in Neural Information Processing Sys-\ntems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R. Bowman. 2019. Superglue: A stickier\nbenchmark for general-purpose language understand-\ning systems. In Advances in Neural Information\nProcessing Systems 32: Annual Conference on Neu-\nral Information Processing Systems 2019, NeurIPS\n2019, December 8-14, 2019, Vancouver, BC, Canada,\npages 3261–3275.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5017–5033, Online. Association\nfor Computational Linguistics.\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan\nHuang. 2020. Evaluating commonsense in pre-\ntrained language models. In The Thirty-Fourth AAAI\nConference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 9733–9740. AAAI Press.\n5807\nA Datasets Construction Details\nInstance Filtering We follow the data construc-\ntion criteria as LAMA, we remove the instances\nwhose object is multi-token or not in the intersec-\ntion vocabulary of these 4 PLMs.\nRelation Selection We remove all the N-M re-\nlations in LAMA such as “share border with” or\n“twin city”. Because in these relations, there are\nmultiple object entities corresponding to the same\nsubject entity. In that case, the metric Precision@1\nis not suitable for evaluating PLMs in such rela-\ntions. In addition, due to the completeness limita-\ntion of knowledge bases, it’s impossible to find all\nthe correct answers for each subject. Therefore, we\ndo not include these relations in our experiments.\nPrompt Generation Because of the difference\nbetween the pretraining tasks of these 4 PLMs (au-\ntoencoder, autoregressive and denoising autoen-\ncoder), we design prompts where the placeholder\nfor the target object is at the end, e.g., The birth-\nplace of x is y instead of y is the birthplace of x.\nWe follow the instruction from Wikidata, combine\nthe prompts from Elazar et al. (2021) and Jiang\net al. (2020b), and manually filter out the prompts\nwith inappropriate semantics. All the prompts are\ncreated before any experiments and fixed afterward.\nB Further Pretraining Details\nWe further pretrain BERT with masked language\nmodeling (mask probability=15%) and GPT2 with\nautoregressive language modeling task respectively.\nTraining was performed on 8 40G-A100 GPUs for\n3 epochs, with maximum sequence length512. The\nbatch sizes for BERT-base, BERT-large, GPT2-\nbase, GPT2-medium are 256, 96, 128, 64 respec-\ntively. All the models is optimized with Adam\nusing the following parameters: β1 = 0.9, β2 =\n0.999, ϵ= 1e − 8 and the learning rate is 5e − 5\nwith warmup ratio=0.06.\n5808",
  "topic": "Debiasing",
  "concepts": [
    {
      "name": "Debiasing",
      "score": 0.8889000415802002
    },
    {
      "name": "Computer science",
      "score": 0.7541439533233643
    },
    {
      "name": "Language model",
      "score": 0.637830376625061
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4657949209213257
    },
    {
      "name": "Intervention (counseling)",
      "score": 0.4350253939628601
    },
    {
      "name": "Machine learning",
      "score": 0.43271175026893616
    },
    {
      "name": "Language understanding",
      "score": 0.4312078058719635
    },
    {
      "name": "Risk analysis (engineering)",
      "score": 0.3365277647972107
    },
    {
      "name": "Cognitive science",
      "score": 0.19820615649223328
    },
    {
      "name": "Psychology",
      "score": 0.18417483568191528
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    }
  ]
}