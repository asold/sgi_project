{
    "title": "Open challenges and opportunities in federated foundation models towards biomedical healthcare",
    "url": "https://openalex.org/W4406064967",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2136050179",
            "name": "Xingyu Li",
            "affiliations": [
                "Tulane University"
            ]
        },
        {
            "id": "https://openalex.org/A2099460193",
            "name": "Lu Peng",
            "affiliations": [
                "Tulane University"
            ]
        },
        {
            "id": "https://openalex.org/A2103392490",
            "name": "Yu Ping Wang",
            "affiliations": [
                "Tulane University"
            ]
        },
        {
            "id": "https://openalex.org/A2107907725",
            "name": "Weihua Zhang",
            "affiliations": [
                "Fudan University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4387356888",
        "https://openalex.org/W2987741655",
        "https://openalex.org/W3168545914",
        "https://openalex.org/W3168822201",
        "https://openalex.org/W6851592950",
        "https://openalex.org/W2789804061",
        "https://openalex.org/W3182064909",
        "https://openalex.org/W4205561327",
        "https://openalex.org/W1726806267",
        "https://openalex.org/W4323644292",
        "https://openalex.org/W3209696639",
        "https://openalex.org/W3016632787",
        "https://openalex.org/W2911560572",
        "https://openalex.org/W4231698305",
        "https://openalex.org/W4385573131",
        "https://openalex.org/W3045317090",
        "https://openalex.org/W4311706328",
        "https://openalex.org/W3160137267",
        "https://openalex.org/W6631455383",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3144293453",
        "https://openalex.org/W2112796928",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W6780226713",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2962739339",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W6810081322",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W4312933868",
        "https://openalex.org/W4390874575",
        "https://openalex.org/W4288474812",
        "https://openalex.org/W2895763047",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W3125473938",
        "https://openalex.org/W4306694247",
        "https://openalex.org/W4318616691",
        "https://openalex.org/W2979637109",
        "https://openalex.org/W2995022099",
        "https://openalex.org/W3021654819",
        "https://openalex.org/W3006555759",
        "https://openalex.org/W4285554319",
        "https://openalex.org/W4383468656",
        "https://openalex.org/W4323343894",
        "https://openalex.org/W2913570153",
        "https://openalex.org/W3211351785",
        "https://openalex.org/W3097816393",
        "https://openalex.org/W4390323078",
        "https://openalex.org/W3093315565",
        "https://openalex.org/W4379261329",
        "https://openalex.org/W3211237094",
        "https://openalex.org/W4300035993",
        "https://openalex.org/W6600449182",
        "https://openalex.org/W3027472889",
        "https://openalex.org/W4226036112",
        "https://openalex.org/W6605988140",
        "https://openalex.org/W4307534108",
        "https://openalex.org/W6600100092",
        "https://openalex.org/W4385627126",
        "https://openalex.org/W4372260089",
        "https://openalex.org/W4205725149",
        "https://openalex.org/W4303427132",
        "https://openalex.org/W3087893815",
        "https://openalex.org/W2170479554",
        "https://openalex.org/W2911835721",
        "https://openalex.org/W3012711473",
        "https://openalex.org/W4402130781",
        "https://openalex.org/W3164718925",
        "https://openalex.org/W2525360812",
        "https://openalex.org/W4212935479",
        "https://openalex.org/W3083832252",
        "https://openalex.org/W2754650407",
        "https://openalex.org/W2767290858",
        "https://openalex.org/W2158449659",
        "https://openalex.org/W2133297572",
        "https://openalex.org/W4206042814",
        "https://openalex.org/W2968912020",
        "https://openalex.org/W2993519305",
        "https://openalex.org/W3134931716",
        "https://openalex.org/W2951209146",
        "https://openalex.org/W3154403094",
        "https://openalex.org/W3048417086",
        "https://openalex.org/W2914209001",
        "https://openalex.org/W2126598020",
        "https://openalex.org/W3111402128",
        "https://openalex.org/W3192945554",
        "https://openalex.org/W4281400187",
        "https://openalex.org/W2791015340",
        "https://openalex.org/W3112990870",
        "https://openalex.org/W2124354030",
        "https://openalex.org/W3009999522",
        "https://openalex.org/W3037163353",
        "https://openalex.org/W2736137960",
        "https://openalex.org/W2962411443",
        "https://openalex.org/W2161444669",
        "https://openalex.org/W2109555487",
        "https://openalex.org/W2139484679",
        "https://openalex.org/W2068976153",
        "https://openalex.org/W3144696998",
        "https://openalex.org/W4390862266",
        "https://openalex.org/W4283794074",
        "https://openalex.org/W3177464472",
        "https://openalex.org/W2099017849",
        "https://openalex.org/W2913640436",
        "https://openalex.org/W3001688612",
        "https://openalex.org/W3166254754",
        "https://openalex.org/W3183475563",
        "https://openalex.org/W4220927627",
        "https://openalex.org/W4280615754",
        "https://openalex.org/W6632683261",
        "https://openalex.org/W4224227775",
        "https://openalex.org/W4386131770",
        "https://openalex.org/W4295934879",
        "https://openalex.org/W4281728635",
        "https://openalex.org/W6759579507",
        "https://openalex.org/W6796581206",
        "https://openalex.org/W4205991051",
        "https://openalex.org/W2999803881",
        "https://openalex.org/W4387587838",
        "https://openalex.org/W6603139312",
        "https://openalex.org/W4390651644",
        "https://openalex.org/W4401857637",
        "https://openalex.org/W4287887646",
        "https://openalex.org/W4367046615",
        "https://openalex.org/W4392903604",
        "https://openalex.org/W4393148063",
        "https://openalex.org/W4393160079",
        "https://openalex.org/W4393160127",
        "https://openalex.org/W4393161698",
        "https://openalex.org/W4322766882",
        "https://openalex.org/W4285601829",
        "https://openalex.org/W4221156340",
        "https://openalex.org/W4401961994",
        "https://openalex.org/W4366262984",
        "https://openalex.org/W4379984108",
        "https://openalex.org/W4293812377",
        "https://openalex.org/W3011574394",
        "https://openalex.org/W4389267014",
        "https://openalex.org/W2592929672",
        "https://openalex.org/W3098605233",
        "https://openalex.org/W4385567149",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W4320024180",
        "https://openalex.org/W3213348597",
        "https://openalex.org/W4388931647",
        "https://openalex.org/W2036448803",
        "https://openalex.org/W2749701213",
        "https://openalex.org/W3024922541",
        "https://openalex.org/W3000238064",
        "https://openalex.org/W6602174710",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W2119852447",
        "https://openalex.org/W3105801792",
        "https://openalex.org/W4324148013",
        "https://openalex.org/W2995225687",
        "https://openalex.org/W3127238141",
        "https://openalex.org/W2030561475",
        "https://openalex.org/W2752558629",
        "https://openalex.org/W2607804943",
        "https://openalex.org/W4313551944",
        "https://openalex.org/W2112876600",
        "https://openalex.org/W6788864986",
        "https://openalex.org/W6772203101",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W4220967417",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W4366598511",
        "https://openalex.org/W2144578941",
        "https://openalex.org/W6636364444",
        "https://openalex.org/W6600310816",
        "https://openalex.org/W4313197536",
        "https://openalex.org/W4392282268",
        "https://openalex.org/W2025768430",
        "https://openalex.org/W3035060554",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W4313156423",
        "https://openalex.org/W4312804044",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W3104330316",
        "https://openalex.org/W4281485151",
        "https://openalex.org/W2963540523",
        "https://openalex.org/W4225307291",
        "https://openalex.org/W4386072096",
        "https://openalex.org/W2963466845",
        "https://openalex.org/W3104050923",
        "https://openalex.org/W2897980926",
        "https://openalex.org/W3194477403",
        "https://openalex.org/W2592905743",
        "https://openalex.org/W4393146352",
        "https://openalex.org/W4312533035",
        "https://openalex.org/W2912664121",
        "https://openalex.org/W4386076493",
        "https://openalex.org/W3091546937",
        "https://openalex.org/W3201906559",
        "https://openalex.org/W4296027312",
        "https://openalex.org/W4312266387",
        "https://openalex.org/W3128510757",
        "https://openalex.org/W3164654615",
        "https://openalex.org/W4304092062",
        "https://openalex.org/W4382998948",
        "https://openalex.org/W4389237760",
        "https://openalex.org/W4379089507",
        "https://openalex.org/W4388230897",
        "https://openalex.org/W4405081677",
        "https://openalex.org/W2910707576",
        "https://openalex.org/W3019437576",
        "https://openalex.org/W2969881216",
        "https://openalex.org/W3000127051",
        "https://openalex.org/W3087217556",
        "https://openalex.org/W3109762849",
        "https://openalex.org/W2473930607",
        "https://openalex.org/W3137864305",
        "https://openalex.org/W2980946630",
        "https://openalex.org/W3088409176",
        "https://openalex.org/W3106033826"
    ],
    "abstract": "This survey explores the transformative impact of foundation models (FMs) in artificial intelligence, focusing on their integration with federated learning (FL) in biomedical research. Foundation models such as ChatGPT, LLaMa, and CLIP, which are trained on vast datasets through methods including unsupervised pretraining, self-supervised learning, instructed fine-tuning, and reinforcement learning from human feedback, represent significant advancements in machine learning. These models, with their ability to generate coherent text and realistic images, are crucial for biomedical applications that require processing diverse data forms such as clinical reports, diagnostic images, and multimodal patient interactions. The incorporation of FL with these sophisticated models presents a promising strategy to harness their analytical power while safeguarding the privacy of sensitive medical data. This approach not only enhances the capabilities of FMs in medical diagnostics and personalized treatment but also addresses critical concerns about data privacy and security in healthcare. This survey reviews the current applications of FMs in federated settings, underscores the challenges, and identifies future research directions including scaling FMs, managing data diversity, and enhancing communication efficiency within FL frameworks. The objective is to encourage further research into the combined potential of FMs and FL, laying the groundwork for healthcare innovations.",
    "full_text": "Open challenges and opportunities \nin federated foundation models \ntowards biomedical healthcare\nXingyu Li1, Lu Peng1*, Yu‑Ping Wang2 and Weihua Zhang3 \nIntroduction\nFoundation models (FMs) [1, 2] have risen to prominence as pivotal elements in the \nfield of artificial intelligence [3]. These models are distinguished by their deep learning \narchitectures and a vast number of parameters, allowing them to excel in tasks rang -\ning from text generation to video analysis-capabilities that surpass those of previous AI \nsystems. FMs are developed using advanced training techniques, including unsupervised \npretraining [4, 5], self-supervised training [6, 7], instructed fine-tuning [8], and rein -\nforcement human preference feedback [9]. These methodologies equip them to generate \nAbstract \nThis survey explores the transformative impact of foundation models (FMs) in artificial \nintelligence, focusing on their integration with federated learning (FL) in biomedical \nresearch. Foundation models such as ChatGPT, LLaMa, and CLIP , which are trained \non vast datasets through methods including unsupervised pretraining, self‑supervised \nlearning, instructed fine‑tuning, and reinforcement learning from human feedback, \nrepresent significant advancements in machine learning. These models, with their \nability to generate coherent text and realistic images, are crucial for biomedi‑\ncal applications that require processing diverse data forms such as clinical reports, \ndiagnostic images, and multimodal patient interactions. The incorporation of FL \nwith these sophisticated models presents a promising strategy to harness their analyti‑\ncal power while safeguarding the privacy of sensitive medical data. This approach \nnot only enhances the capabilities of FMs in medical diagnostics and personal‑\nized treatment but also addresses critical concerns about data privacy and security \nin healthcare. This survey reviews the current applications of FMs in federated settings, \nunderscores the challenges, and identifies future research directions including scal‑\ning FMs, managing data diversity, and enhancing communication efficiency within FL \nframeworks. The objective is to encourage further research into the combined poten‑\ntial of FMs and FL, laying the groundwork for healthcare innovations.\nKeywords: Foundation model, Federated learning, Healthcare, Biomedical, Large \nlanguage model, Vision language model, Privacy, Multimodal\nOpen Access\n© The Author(s) 2025. Open Access This article is licensed under a Creative Commons Attribution‑NonCommercial‑NoDerivatives 4.0 Inter‑\nnational License, which permits any non‑commercial use, sharing, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified \nthe licensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The \nimages or other third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a \ncredit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by \nstatutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of \nthis licence, visit http://creativecommons.org/licenses/by‑nc‑nd/4.0/.\nREVIEW\nLi et al. BioData Mining            (2025) 18:2  \nhttps://doi.org/10.1186/s13040-024-00414-9\nBioData Mining\n*Correspondence:   \nlpeng3@tulane.edu\n1 Department of Computer \nScience, Tulane University, New \nOrleans, LA, USA\n2 Department of Biomedical \nEngineering, Tulane University, \nNew Orleans, LA, USA\n3 School of Computer Science, \nFudan University, Shanghai, \nChina\nPage 2 of 54Li et al. BioData Mining            (2025) 18:2 \ncoherent text and realistic images with unprecedented accuracy, showcasing their trans -\nformative potential across various domains.\nThe potential of foundation models extends far beyond mere technical capabilities. \nThese models mark a significant paradigm shift from how we utilize artificial intelli -\ngence for cutting-edge scientific problem-solving. As versatile tools, they can be rapidly \nadapted and fine-tuned for specific tasks, eliminating the need to develop new mod -\nels from the ground up. This adaptability is crucial in fields where processing limited \ndatasets to extract meaningful insights is essential. It is particularly transformative in \nbiomedical healthcare, where the efficacy of AI must be balanced with stringent data \nprivacy considerations [10–12]. In this domain, foundation models not only enhance our \nanalytical capabilities but also ensure that sensitive health information is handled with \nthe utmost integrity, thereby aligning technological advancement with ethical standards.\nFederated Learning (FL) [13, 14], a method for training machine learning models \nacross multiple decentralized devices or servers without exchanging local data samples, \naligns well with the capabilities of foundation models in the biomedical healthcare sec -\ntor. In this context, where data privacy and collaborative efforts are essential, FL ena -\nbles the utilization of vast and varied datasets characteristic of the medical field while \nprotecting sensitive patient information. By applying FL, foundation models can access \nand analyze extensive medical data without breaching privacy [15, 16], thus overcoming \nmajor obstacles in deploying AI technologies where data confidentiality is crucial. Exist -\ning applications of FL in conjunction with FMs typically involve training strategies that \nrange from starting from scratch to prompt fine-tuning. FL enhances the application of \nFMs across both large language models and vision-language models, allowing for com -\nprehensive and privacy-conscious analyses.\nIntegrating the privacy-preserving and decentralization features of FL with the robust, \ngeneralizable capabilities of FMs enables researchers to perform in-depth analyses using \ninsights pooled from local datasets. This approach not only broadens the scope and \naccuracy of medical research but also complies with stringent data protection laws such \nas the General Data Protection Regulation (GDPR) [17] in Europe and the Health Insur -\nance Portability and Accountability Act (HIPAA) [18] in the United States. The poten -\ntial for healthcare is profound, facilitating more personalized medicine where treatment \nplans are precisely tailored to individual genetic profiles, lifestyles, and medical histories. \nAdditionally, FMs that are pre-trained or fine-tuned via federated learning on diverse \ndatasets can reveal new biomarkers and therapeutic targets, thereby significantly push -\ning the boundaries of medical research and improving patient care. The synergy between \nfederated learning and foundation models heralds a significant leap forward in the use \nof medical data, driving innovation in medical technologies while rigorously protecting \npatient privacy.\nThis paper presents a comprehensive survey of the latest advancements in foun -\ndation models and federated learning within the biomedical and healthcare sectors, \nhighlighting their implementations and addressing the persistent challenges encoun -\ntered in these fields. A notable application of these technologies involves the use of \nfederated foundation models to train pre-trained vision-language models, such as \nFedClip [19], which enhance both generalization and personalization in image clas -\nsification tasks. Additionally, MedCLIP [20] employs vision-text contrastive learning \nPage 3 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nwith 20K medical datasets to surpass current benchmarks in medical diagnostics. \nFedMed [21], a tailored federated learning framework, effectively counters perfor -\nmance degradation in federated settings, facilitating high-quality collaborative train -\ning. Another groundbreaking model, MedGPT [22], based on the GPT architecture, \nutilizes electronic health records to predict future medical events, offering the poten -\ntial to detect early signs of critical illnesses, such as cancer or cardiovascular diseases \n[23], before they are typically diagnosable through conventional methods. Impor -\ntantly, the utilization of federated learning ensures that sensitive patient data is pro -\ncessed on-site, never leaving the institution’s local environment, thus significantly \nenhancing data security and maintaining strict patient confidentiality.\nThe integration of federated learning (FL) with foundation models (FMs) offers \nunprecedented potential to transform medical diagnostics and personalize treat -\nments, greatly enhancing the capabilities of healthcare systems to deliver exceptional \ncare while adhering to rigorous standards of data privacy and security. This techno -\nlogical advancement not only improves patient outcomes but also strengthens trust in \nthe use of AI within critical sectors such as healthcare. However, deploying federated \nFMs in the biomedical domain comes with significant challenges, including ensuring \ndata privacy and security, achieving model generalization across diverse datasets, and \nmaintaining bias and fairness. Addressing these issues is essential for harnessing the \nfull capabilities of FL FMs in healthcare and biomedical research.\nFurthermore, this paper explores future directions and ongoing challenges in \nthe field, emphasizing the importance of real-time learning and adaptation, foster -\ning collaborative innovation, and the generation of synthetic data for both academic \nand industrial applications within FL frameworks. By overcoming these challenges, \nresearchers and practitioners can fully realize the potential of federated foundation \nmodels, leading to revolutionary advancements in healthcare. These efforts will not \nonly contribute to scientific progress but also to the practical, ethical, and efficient \nimplementation of AI technologies in sensitive environments, ultimately benefiting \nglobal health outcomes.\n• We provide a comprehensive review of existing literature on Federated Learning \n(FL) and Foundation Models (FM) within the biomedical and healthcare domains. \nThis review meticulously categorizes and discusses various aspects such as bio -\nmedical and healthcare data sources, foundation models, federated privacy, and \ndownstream tasks, offering a thorough synthesis of current knowledge and meth -\nodologies.\n• We introduce a taxonomy of biomedical healthcare foundation models, classifying \nthe existing representative FMs from diverse perspectives including model archi -\ntecture, training strategy, and intended application purposes. This taxonomy aids \nin the systematic understanding and comparison of different models.\n• We explore the open challenges and outline future research directions for the \nintegration of FL with FMs in the biomedical and healthcare sectors, providing \ninsights into unresolved issues and potential advancements.\n• To the best of our knowledge, this is the first survey paper to extensively cover \nfoundation models in federated learning specifically tailored for biomedical and \nPage 4 of 54Li et al. BioData Mining            (2025) 18:2 \nhealthcare applications. Our survey uniquely addresses both large-language and \nvision-language models, highlighting their relevance and transformative potential \nin this context.\nHow do we collect papers? In this survey, we collect over two hundred related papers \nin the field of Federated Learning, Foundation Model, and Biomedical healthcare. We \nconsider Google Scholar as our main literature search engine, where the MedPub, Web \nof Science, and IEEE Xplore are also used as essential tools. Moreover, we check most \nof the related top-tier conferences, such as NeurIPS, ICML, ICLR, CVPR, and ECCV, \nand Bioinformatics. The major keywords we use are “Biomedical Federated Learning, \nMedical Pretrained Foundation Model, Healthcare Federated Pretrain Training, etc” . \nThe most representative papers like Med-BERT [24], FedClip [19], and MedClip [20] are \nregarded as seed papers for reference check.\nOrganization The rest of this survey is organized as follows. Background  section \ndescribes the FM and FL literature relevant to our work. Federated learning and foun -\ndation models  section details how to apply FMs with FL. The applications of FM on \nbiomedical and healthcare is summarized in Foundation models in biomedical health -\ncare  section. The challenges and future directions of Federated FMs in the biomedi -\ncal and healthcare sectors are discussed in Open challenges and opportunities in fed -\nerated foundation biomedical research  section. Finally, we conclude our survey in \nConclusions section.\nBackground\nBackground on foundation models\nThe latest wave of AI innovation sees the evolution of a new class of AI models often \nreferred as foundation models (FMs) - a term popularized by the Stanford Institute for \nHuman-Centered AI [25] which can be categorized into two model types: Large-lan -\nguage Model (LLM) and Vision-language Model (VLM). For example, LLMs including \nChatGpt and Gpt-4 [26] from OpenAI demonstrate impressive capabilities to generate \ncoherent text. VLMs such as DALL · E 2 [27] shows the ability to create realistic images \nand art from a text description. These models are trained with pretraining, self-super -\nvised training, and reinforcement-instructed fine-tuning with broad data at immense \nscale and high resource costs, resulting in models with billions of parameters [25]. In \nthis section, we will introduce the backbone of Foundation Models in Backbone net -\nworks in foundation models section, where the pre-trained large-language models and \nvision-language models are discussed in Foundation on text: large language models and \nFoundation beyond text: vision language models sections, respectively.\nBackbone networks in foundation models\nThe significant advancements in foundation models are largely due to the evolution of \ntheir underlying architectures, transitioning from Long Short-Term Memory networks \n(LSTM) [28] to Transformers [29]. Initially, LSTMs served as the basic architecture for \nearly pre-trained models, where the recurrent structure is computationally intensive \nPage 5 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nwhen scaled to deeper layers. In response to these limitations, the Transformer archi -\ntecture was developed and quickly established itself as the standard for modern natu -\nral language processing (NLP) [ 30]. The superiority of Transformers over LSTMs can be \nattributed to two key factors: (1) Efficiency: Transformers eliminate recurrence, enabling \nparallel computation of tokens.(2) Effectiveness: The attention mechanism facilitates \ndynamic spatial interactions between tokens, contingent on the input itself. This section \nprovides a brief overview of the evolution of backbone networks in foundation models, \nhighlighting the transition from LSTMs to Transformers, followed by vision language \nmodel backbones from Convolutional neural networks (CNNs) [ 31] to Vision Trans -\nformers (ViTs) [32].\nBackbone Networks in Texts.  Transformer has become the backbone of most pre-\ntrained language models, such as BERT [ 33], GPT[ 1], and T5[ 34], building upon \nself-attention module and feed-forward networks (FFNs). The self-attention module \nfacilitates token interaction, while FFN refines token representations using non-linear \ntransformations. The Transformer architecture is designed to process tokens efficiently \nin parallel, thanks to the elimination of recurrent units and the use of position embed -\ndings. Additionally, the architecture includes residual connections, layer normalization, \nand other features that prevent saturation issues and enhance expressive power with \nlarge-scale data and deep layers. The input is linearly transformed into query, key, value \n(Q, K, V), and output spaces in the self-attention module: the attention scores between \nthe query and key is computed, which are then used to weight the values. The FFN mod-\nule processes the weighted values to generate the output. The Transformer architecture \nhas proven to be superior in terms of capacity and scalability, enabling the develop -\nment of increasingly sophisticated language models. Considering an input Xm the linear \ntransformation of X into Q, K, V is computed as follows:\nwhere the self-attention module is calculated with a softmax function as follows:\nTo this end, the FFN provides the non-linear features for the transformer architec -\nture. Besides the self-attention and FFN modules, the transformer architecture also \nincludes residual connections [ 35], layer normalization [ 36], and positional encoding \n[37] to enhance the model’s performance. The transformer architecture has been widely \nadopted in various pre-trained language models, such as BERT, GPT, and T5, and has \nbeen instrumental in advancing the field of natural language processing (NLP).\nBackbone Networks in Images.  Convolutional Neural Networks (CNNs) [ 31] have long \nbeen the foundation for many vision-related tasks, characterized by their distinctive \narchitecture comprising convolutional, pooling, activation, and fully connected layers. \nThese layers work in unison where convolutional layers act as trainable filters identifying \nimage patterns like edges and textures, pooling layers reduce data dimensionality, activa-\ntion layers introduce non-linearity, and fully connected layers synthesize these features \n(1)Q = XW Q, K = XW K, V = XW V,\n(2)Attention(Q,K ,V ) = softmax ( QK\ndk\n)V .\nPage 6 of 54Li et al. BioData Mining            (2025) 18:2 \ninto predictions. This architecture has been not only pivotal in vision applications but \nalso adapted for language understanding tasks.\nAs the field evolves, there has been a notable shift towards incorporating Transformer \narchitectures into vision tasks. This integration is exemplified by the development of \nVision Transformers (ViT) [32], which apply the Transformer’s self-attention mecha -\nnisms to image patches for feature extraction, representing a significant evolution from \ntraditional CNN approaches. This concept has similarly impacted computational biol -\nogy, as seen in models like AlphaFold2 [38], which leverages Transformer technology for \nprotein structure prediction. These adaptations underscore the versatility and robust -\nness of Transformer models across different scientific domains.\nFoundation on text: large language models\nIn the field of Natural Language Processing (NLP), the evolution of methods to build \ntoken representations has been marked by significant advancements. Initially, typical \napproaches such as those proposed by [39, 40] focused on creating ’static word embed -\ndings, ’ where a one-to-one mapping between words and their vector representations is \nestablished. These embeddings are termed ’static’ because they do not account for the \ncontext in which a word is used, thus limiting their ability to reflect the diverse meanings \nwords can have in different settings.\nRecognizing the limitations of static embeddings, there has been a shift towards \ndeveloping ’contextualized word embeddings. ’ These representations are dynamic, with \nthe vector for a word varying according to its contextual usage. For instance, the word \n’bank’ would have different embeddings in ’river bank’ compared to ’money bank. ’ This \napproach, exemplified by models like ELMo [41], GPT [42], and BERT [33], signifi -\ncantly enhances the quality of word representations by modeling bi-directional contexts, \nthereby improving performance across various NLP tasks.\nHistorically, neural language models [43, 44] served as foundational frameworks in \nNLP , utilizing relatively shallow neural architectures for efficient training. These mod -\nels were primarily pre-trained on tasks like unidirectional language modeling, which \ninvolves predicting the next word based on previous words. However, subsequent inno -\nvations such as Skip-Gram [39] aimed to enrich word embeddings by predicting sur -\nrounding words or using bidirectional context, respectively. GloVe [40] extended this by \nfocusing on word co-occurrence probabilities.\nThe advent of deep learning brought about more sophisticated approaches for learn -\ning word representations. ELMo [41] introduced a bidirectional language modeling task, \nutilizing both forward and backward context in its pre-training. GPT [42] continued \nwith unidirectional modeling, while BERT [33] innovated with the Masked Language \nModel. This method involves masking words in a sentence and predicting them based \non the remaining unmasked context, allowing for deeper bidirectional context modeling.\nFurther developments like the T5 model [34] introduced an encoder-decoder \nframework for generating text outputs, proving particularly effective in text genera -\ntion tasks such as summarization and question-answering. These advancements have \nbeen integral to the development of versatile language models like OpenAI’s GPT-3, \nPage 7 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nInstructGPT, Codex, and ChatGPT, which not only generate text but also engage in \nconversational exchanges, admit errors, and handle complex user interactions.\nRepresentative Large Language Models Large Language Models (LLMs) have become \npivotal in the evolution of natural language understanding and generation. The progres -\nsion of the GPT series, from GPT-1 [42] to GPT-3 [45], and the subsequent release of \nGPT-4 [26], illustrates a remarkable expansion in model size and versatility. These mod -\nels have profoundly impacted AI research and applications, heralding a new era of com -\nputational linguistics. Concurrently, BERT [33] revolutionized pre-training approaches \nby emphasizing bidirectional training, which significantly enhances language under -\nstanding capabilities. PaLM [46], another notable advancement, has achieved state-of-\nthe-art results across diverse language tasks, highlighting the potential for scalability \nin LLMs. Recent innovations also include Bard [47], which integrates extensive world \nknowledge into a context-aware framework, and LLaMa [2], which prioritizes efficiency \nand practical applicability in language model design. Collectively, these models mark \ncrucial developments in the field, each contributing distinctively to the enrichment and \ncomplexity of machine learning techniques that underpin contemporary AI systems.\nFoundation beyond text: vision language models\nDeep neural networks have exhibited remarkable success across a variety of vision \ntasks, such as image classification, object detection, and instance segmentation, \nlargely attributable to the effectiveness of pre-training. Initially, pre-training in the \nvision domain involved training models on extensive annotated image datasets like \nImageNet [48]. However, to address issues such as generalization errors and spu -\nrious correlations inherent in supervised learning, various self-supervised learning \nmethods have been developed.\nA significant area of advancement in AI research is the integration of vision and \nlanguage models, which aims to develop systems capable of understanding and \ngenerating content that spans visual and textual modalities. The introduction of \nthe Vision Transformer (ViT) [49] marked a pivotal shift by applying the trans -\nformer architecture-originally designed for natural language processing-directly to \nsequences of image patches. This approach fundamentally changed the paradigm \nof how models process visual information. Building on this, CLIP (Contrastive \nLanguage-Image Pre-training) [50] advanced the field by learning visual concepts \nthrough natural language supervision, enabling the model to adeptly handle various \nvision tasks with minimal task-specific training. Further extending these innova -\ntions, Stable Diffusion [51] ventured into generative art, providing tools to create \nintricate images from textual descriptions. The most recent breakthrough, Segment \nAnything [52], tackles image segmentation using deep learning to precisely iden -\ntify and delineate multiple objects within images in ways that are contextually rel -\nevant. Collectively, these developments not only bridge the gap between visual data \nand language processing but also set the stage for more intuitive and interactive AI \nsystems.\nPage 8 of 54Li et al. BioData Mining            (2025) 18:2 \nChallenges of foundation models\nThe paradigm of foundation models (FMs) represents a significant shift from traditional \ntask-specific models that have long dominated the AI landscape. These pre-trained models \nare designed for adaptation to a variety of tasks they were not originally trained for [53]. \nAdaptation techniques include user or engineer prompts, continual learning, and fine-\ntuning-methods that expand their application to fields where data scarcity impedes the \ndevelopment of specialized algorithms. This flexibility introduces exciting possibilities for \nscalable, reusable AI models across diverse domains, including transformative potential in \nhealthcare [54]. However, this shift also presents unique challenges, including the risk of \nover-generalization, the difficulty in fine-tuning for highly specialized tasks, and the ethi-\ncal implications of deploying such versatile technologies in sensitive areas. These challenges \nnecessitate rigorous validation, careful implementation, and ongoing monitoring to ensure \nthat the deployment of foundation models aligns with ethical standards and practical \nrequirements.\nOver‑trusting High Performance & Output Coherence: Ensuring Safe & Reliable \nUse Despite the high accuracy and broad capabilities of larger models, it’s critical to \naddress ethical and legal standards to ensure their use remains safe, fair, and privacy-\nconscious [53]. In healthcare, the necessity for accurate and reliable data for clinical \ndecision-making cannot be overstated. However, verifying the correctness of outputs \nfrom FMs poses a challenge, as demonstrated by systems like ChatGPT, whose outputs \ncan mimic human-like text, potentially leading to automation bias and misuse [55]. The \ncomplexity of these models often precludes a full understanding of their mechanisms, \nnecessitating cautious deployment decisions, especially in sensitive fields like healthcare. \nThis includes designing interfaces that clearly articulate the limitations and probabilistic \nnature of AI outputs and developing robust validation processes to ensure safety and \nfairness.\nBuilding AI in a Vacuum: Decontextualized & Centralized AI development frequently \ntakes place in isolation, focused on technological accuracy before considering real-world \nuser needs [56]. This ’development in a vacuum’ has drawn increasing scrutiny for failing \nto address the actual conditions and requirements of end-users [57]. Foundation models, \nin particular, suffer from this issue as they require significant adaptation to be truly effec-\ntive outside of initial testing environments. A greater emphasis on ethnographic studies \ncould provide deeper insights into the practical applications and challenges of AI within \noperational settings. Moreover, integrating AI technology into everyday use demands an \nunderstanding of specific user contexts, necessitating strategies for risk mitigation and \na move towards more user-centered research directions. Validating the utility of AI in \nreal-world settings and ensuring their integration into clinical practice remain a formi -\ndable challenge, but one that the human-computer interaction (HCI) community is well-\nequipped to tackle by bridging the ’last mile’ of AI in healthcare [58].\nPage 9 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nBackground of federated learning in foundation models\nBackground of conventional federated learning and frameworks\nFederated Learning (FL) is a machine learning paradigm where multiple clients, such as \nmobile devices or entire organizations, collaboratively train a model under the orches -\ntration of a central server, such as a service provider, while keeping the training data \ndecentralized. This method not only adheres to the principles of focused collection and \ndata minimization but also addresses many systemic privacy risks and costs associated \nwith traditional centralized machine learning approaches. The concept of FL, first intro -\nduced by McMahan et  al. in 2016 [13], has grown significantly in interest from both \ntheoretical and practical perspectives. This approach is defined by challenges including \nunbalanced and non-IID data across numerous unreliable devices, limited communi -\ncation bandwidth, and the complexities of model training and implementation across \ndiverse and distributed environments.\nSince its inception, the focus of federated learning has expanded beyond mobile and \nedge devices to include applications involving a small number of more reliable entities, \nsuch as multiple organizations collaborating to train a model. This has led to distin -\nguishing between “cross-device” and “cross-silo” federated learning, each with unique \nchallenges and requirements. In this survey, we delve into the specifics of cross-device \nfederated learning, highlighting its practical aspects, challenges, and its potential to train \nand implement foundation models (FMs) in a distributed fashion.\nGroundbreaking works of FL including McMahan et  al. [13] laid the foundational \nframework for FL systems. Research in FL has since advanced, focusing on enhancing \ndata privacy in applications such as medical image segmentation [59] and addressing \nongoing challenges related to communication efficiency, scalability, and model robust -\nness [60, 61]. Notable developments in FL include SCAFFOLD [62] and FedProx [61], \nwhich tackle issues such as client update variance and client drift in non-IID data envi -\nronments. Further contributions from FedGSam [63], FedLGA [14], and LoMar [64] \nhave advanced FL by developing generalized strategies and adaptive algorithms that \nenhance learning processes in federated settings.\nMoreover, the development of open-source FL frameworks such as TsingTao [65], \nFlower [66], FedML [67], FATE [68], and FederatedScope [69] has significantly advanced \nthe accessibility and standardization of FL practices. Designing specialized FL systems \nand benchmarks is imperative to meet the unique needs and challenges of foundation \nmodels (FM). Although current FL frameworks have made significant strides in both \nacademic and industrial settings [66–71], they may not fully satisfy the specific require -\nments for optimizing memory, communication, and computational demands associated \nwith FMs. Platforms like FedML [67] and FATE [68] are adapted to better support FMs, \nbut extensive research is still needed to thoroughly explore system requirements and \nintegration strategies for these models.\nMotivations of federated learning for foundation models\nScarcity of Compliant Large‑Scale Data The shortage of large-scale, high-quality, \nlegally compliant data has become a critical driver for the adoption of federated learning \nPage 10 of 54Li et al. BioData Mining            (2025) 18:2 \nin the context of foundation models. This scarcity is particularly acute in sectors such as \ntechnology and social media, where data compliance and privacy issues are increasingly \nforegrounded [72–74].\nHigh Computational Resource Demand Training large-scale foundation models \ndemands significant computational resources. For instance, training LLaMa with 65 bil -\nlion parameters required 2048 NVIDIA A100 GPUs over 21 days [2], while the smaller \n1.3 billion parameter GPT-3 model needed 64 Tesla V100 GPUs for a week [45]. The \ndevelopment of GPT-4 also highlighted these intensive demands, utilizing substantial \nresources over several months at considerable financial costs [26]. Federated learning \ncan help alleviate these demands by distributing computational tasks across multiple \ndevices, thereby optimizing resource utilization.\nContinuous Model Updating Challenges As data continually evolves, particularly from \nsources like IoT sensors and edge devices, keeping foundation models updated becomes \na significant challenge [75, 76]. Federated learning offers a dynamic solution by ena -\nbling ongoing, incremental updates to FMs with new data, which allows these models to \nadapt to emerging data landscapes without the need to reinitiate training processes. This \napproach not only enhances the models’ accuracy and relevance but also ensures their \nadaptability to real-world changes [77].\nReducing Response Delays and Enhancing FM Services One of the foremost benefits \nof applying federated learning to foundation models is the potential to deliver nearly \ninstant responses, thus significantly improving user experience. Traditional central \nserver deployments often face latency and privacy issues due to the required network \ncommunications between users and servers [78]. Federated learning addresses these \nconcerns by enabling models to operate directly on local devices, minimizing network \ndependencies, reducing latency, and improving privacy protections. This approach not \nonly enhances response times but also ensures a seamless, privacy-conscious interac -\ntion, maintaining user trust and satisfaction in the services provided by foundation \nmodels.\nMotivations of foundation models for federated learning\nFoundation Models can significantly contribute to enhancing the efficacy of Federated \nLearning. This section explores the motivations behind leveraging FM within FL, exam -\nines the challenges posed by this integration, and discusses the potential opportunities it \noffers to the field.\nData Privacy and Shortage Dilemma in FL In federated settings, clients often grapple \nwith limited or imbalanced datasets, especially in federated few-shot learning contexts \n[79]. Such data scarcity can result in suboptimal model performance, as it may not fully \ncapture the diversity of the data distribution [80]. Moreover, privacy concerns are inten -\nsified due to the potential for sensitive information recovery from model updates in FL \n[81, 82]. These issues are particularly acute in sectors like healthcare or finance, where \ndata privacy regulations or the inherent sensitivity of the data restrict availability, thus \nPage 11 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \ncomplicating the training process and limiting FL’s effectiveness in these crucial areas. \nOne promising solution is the use of synthetic data generated by FMs. Being extensively \npre-trained on vast datasets and further refined through techniques such as fine-tuning \nand prompt engineering, FMs possess a deep understanding of complex data distribu -\ntions, enabling them to produce synthetic data that closely mirrors real-world diversity.\nPerformance Dilemma in FL FL can mitigate issues related to non-IID and biased data \nby leveraging the advanced capabilities of FMs, thus enhancing performance across vari-\nous tasks and domains [83]. FMs can improve FL’s efficiency in several ways. (1). Starting \nPoint Advantage: FMs provide a robust starting point for FL. Clients can begin fine-tun -\ning directly on their local data instead of starting from scratch, leading to faster conver -\ngence and enhanced performance while reducing the need for extensive communication \nrounds [84, 85]. (2). Data Diversity Enhancement: FMs act as powerful generators that \ncan synthesize diverse data, enriching the training dataset in FL. An example is GPT-FL \n[86], which utilizes generative models to produce synthetic data that improves down -\nstream model training on servers. This approach not only boosts test accuracy but also \nenhances communication and client sampling efficiency. (3). Knowledge Distillation: \nFMs can address performance issues in FL by acting as knowledgeable teachers through \ntechniques like knowledge distillation [87].\nNew Sharing Paradigm Empowered by FM Unlike traditional FL, which involves shar -\ning high-dimensional model parameters, FMs use a new paradigm through prompt tun -\ning. PROMPTFL [88] showcases how FM capabilities can be leveraged to efficiently \ncombine global aggregation with local training on sparse data. This approach focuses \non training prompts rather than the entire model, thereby optimizing resource use and \nenhancing performance. Building on this concept, FedPrompt [89] introduces an inno -\nvative prompt tuning method specifically designed for FL, while a recent study FedTPG \n[90] explores a scalable prompt generation network that learns across multiple clients, \naiming to generalize to unseen classes effectively.\nMachine learning in biomedical and health care\nBiomedical ML: data fusion\nData is the cornerstone of sense-making in artificial intelligence (AI), playing a crucial \nrole in various sectors, including healthcare, where they are from diverse sources like \ncare providers, insurers, and academic publications [53, 91]. They vary in form (e.g., \nclinical notes, medical images), scale (e.g., patient versus population level), and style \n(professional versus lay language), posing both opportunities and challenges for the \napplication and training of AI models. Despite the proficiency of machine learning \nmethods in managing and extracting insights from vast, multi-dimensional data [92], \nit is vital to address how societal biases and inequalities are embedded in the data. \nDisparities can manifest in various aspects of healthcare, such as the prioritization of \ncertain medical issues and the exclusion or misrepresentation of specific population \ngroups. These issues often stem from barriers like limited healthcare access, restric -\ntive criteria for clinical trial participation, or the risk of inaccurate data due to doc -\numentation errors and systemic discrimination [93]. For example, in California, the \nPage 12 of 54Li et al. BioData Mining            (2025) 18:2 \nmandate to verify citizenship at hospitals has reduced autism diagnosis rates among \nHispanic children in the context of stringent federal immigration policies.\nHealthcare and biomedicine are major sectors within the U.S. economy, account -\ning for about 17% of the Gross Domestic Product (GDP) [94– 97]. These fields require \nsubstantial financial investments and extensive medical knowledge, encompassing \neverything from patient care to scientific exploration of diseases and the development \nof new therapies [54, 98]. We envision machine learning models as central reposito -\nries of medical knowledge, trained on a diverse array of data sources and modalities \nwithin medicine [99, 100]. These models could serve as dynamic platforms that medi -\ncal professionals and researchers use to access and contribute to the latest findings, \nenhancing their ability to make informed decisions [101].\nBiomedical Data Fusion In the field of biomedical research, a significant challenge lies \nin deciphering the complex interactions within and between the cellular and organismal \nlevels, characterized by diverse components that exhibit emergent behaviors [102]. The \ndata collected through various sensors, while rich, often provide limited insights when \nexamined in isolation due to the specificity of each measurement modality [103]. Data \nfusion, the process of integrating data from multiple views, aims to provide a holistic \nview of biological phenomena by combining disparate data sources that offer unique \nperspectives on the same subject [104]. This approach is generally advantageous in sev -\neral ways, categorized into complementary, redundant, and cooperative features of the \ndata [105]. These features are not mutually exclusive but interact synergistically, enhanc-\ning the robustness and accuracy of the insights gained.\nData fusion requires the use of sophisticated machine learning (ML) methods capable \nof integrating both structured and unstructured data while accommodating their varied \nstatistical properties, sources of non-biological variation, high dimensionality, and dis -\ntinct patterns of missing values [103, 106]. A comprehensive examination of these strat-\negies is presented in a review of multimodal deep learning approaches with potential \nadvancements and methodologies in the medical field [107].\nCategories Summary of Data Fusion The categories of data fusion techniques can \nbe broadly summarized into three main approaches: easy fusion, intermediate fusion, \nand late fusion. Easy fusion typically involves direct modeling techniques where differ -\nent types of neural networks are used to process the input data. This includes fully con -\nnected networks for a straightforward integration of features across modalities [108], \nconvolutional networks that are effective in handling spatial data [109], and recurrent \nnetworks suited for sequential data integration [110]. Autoencoders also play a signifi -\ncant role in easy fusion, with variations such as regular [111], denoising [112], stacked \n[113], and variational autoencoders [114] being employed to refine the fusion process.\nIntermediate fusion, on the other hand, involves branching strategies that can be \nhomogeneous, focusing on either marginal [115] or joint representations [116], or het -\nerogeneous, which also targets both marginal [117] and joint data representations [118]. \nThese strategies optimize the integration by selectively focusing on how data from the \nsame or different modalities are fused.\nPage 13 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nLate fusion utilizes aggregation methods to combine features at a higher level, often \nafter initial independent processing. Techniques in this category include simple averag -\ning [119] and weighted averaging [120], where weights might be assigned based on the \nreliability or importance of each modality. Furthermore, meta-learning approaches are \nutilized to dynamically adjust these weights for optimal performance [121], thus enhanc-\ning the fusion’s effectiveness by incorporating learning-based adjustments. These meth -\nods ensure that the final model output maximally benefits from the diverse characteris -\ntics of all data modalities involved.\nFM in biomedical healthcare\nMotivations Foundation models hold transformative potential for biomedical research, \nparticularly in the realms of drug discovery and disease understanding, thereby enhanc -\ning healthcare solutions [122]. Biomedical discovery processes are currently character -\nized by intensive demands on human resources, lengthy experimental timelines, and \nsubstantial financial outlays. For example, the drug development journey includes stages \nfrom basic research, such as protein target identification and potent molecule discov -\nery, through clinical development involving clinical trials, to the final drug approval \nstage. This extensive process typically spans more than a decade and incurs costs often \nexceeding one billion dollars [123]. Thus, the ability to expedite biomedical discovery \nby harnessing existing data and published findings becomes crucial, especially during \ncritical times like the COVID-19 outbreak, which resulted in significant loss of life and \neconomic damage [124].\nFoundation models contribute to biomedical advancements in two primary ways. \nFirstly, these models exhibit strong generative capabilities, as seen with coherent text \ngeneration in models such as GPT-3. These capabilities can be utilized for generating \nexperimental protocols in clinical trials and in designing novel molecules for drug dis -\ncovery [125, 126]. Secondly, foundation models excel at integrating diverse data modali -\nties in medicine, facilitating the exploration of biomedical concepts across various \nscales-from molecular to patient and population levels-and integrating multiple knowl -\nedge sources, including imaging, textual, and chemical data [127–132]. This integrated \napproach enables discoveries that might be challenging with single-modality data alone.\nAdditionally, foundation models are adept at transferring knowledge across dif -\nferent data modalities. For instance, research by Lu et  al. [133] demonstrated how a \ntransformer model, initially trained on natural language, a data-rich modality, could \nbe adapted for other sequence-based tasks, such as protein folding predictions, a long -\nstanding challenge in biomedicine. These capabilities highlight the potential applications \nof foundation models in addressing complex biomedical tasks.\nApplications Foundation models (FMs) hold significant potential for revolutionizing \nhealthcare applications through their adaptability and efficiency in performing spe -\ncific healthcare and biomedical tasks. They have been proposed for use in a variety of \nareas including disease prediction [24], triage or discharge recommendations [98], and \nhealth administration tasks such as clinical notes summarization [134] and medical text \nPage 14 of 54Li et al. BioData Mining            (2025) 18:2 \nsimplification [1]. These applications leverage the unique capabilities of FMs, such as \nfine-tuning and prompting [45], to tailor solutions to specific needs, enhancing both the \naccuracy and efficiency of medical services.\nFMs are particularly effective in patient-facing roles, such as question-answering sys -\ntems and clinical trial matching applications, benefiting both researchers and patients \nby simplifying access to information and streamlining patient recruitment processes \n[126, 135–137]. As central interfaces, FMs facilitate interactions among data, tasks, and \nindividuals, improving the operational efficiency of healthcare services. This is further \nexplored in subsequent sections focusing on specific healthcare and biomedical tasks.\nAdditionally, FMs serve as repositories of extensive medical knowledge, accessible by \nhealthcare professionals and the public for purposes like medical question-answering \nand interactive chatbot applications. Innovations such as ChatGPT [86] and Bard [138] \nprovide conversational user interfaces that assist users in navigating complex health \ninformation and obtaining relevant health advice.\nThe implementation of FMs also promises to accelerate healthcare application devel -\nopment and research. These models can automate processes such as structured data -\nset generation, data labeling, and synthetic data creation [139]. Looking forward, there \nis considerable scope for developing new FM-enabled capabilities, particularly through \nthe use of multimodal data, a feature characteristic of the healthcare domain. Beyond \nnatural language processing, breakthroughs are already evident in areas like biomedical \nresearch, where tools like AlphaFold [140] have made significant advances in predict -\ning human protein structures to aid drug development. Similarly, innovations in genome \nsequencing are hastening the detection of disease-causing genetic variants [141], and \nnew methods are being developed for optimized clinical trial design [142]. This multi -\ndisciplinary integration highlights the transformative potential of FMs in enhancing and \nexpanding the capabilities of the healthcare sector.\nFederated learning and foundation models\nFederated learning (FL) and foundation models represent two cutting-edge approaches \nin the field of machine learning. Federated learning offers a decentralized approach to \ntraining models across multiple nodes or devices, ensuring privacy and maintaining \ndata locality. In contrast, foundation models, due to their vast size and generalized pre-\ntraining, provide significant adaptability and scalability for a variety of tasks. Integrating \nthese technologies poses unique challenges but also opens up exciting opportunities for \ninnovation in AI training and application. This section explores the relationship between \nfederated learning and foundation models, highlighting key research directions and \nrecent advancements in this domain. Depending on the training paradigm, foundation \nmodels can either be trained from scratch or fine-tuned on top of pre-trained models \nwithin a federated learning framework. Additionally, the application of foundation mod-\nels in federated learning can extend to large language models or vision language models.\nRelated surveys further enrich our understanding of this integration. A recent sur -\nvey by Yu et  al. [83] discusses the intersection of foundation models with federated \nlearning, exploring the motivations behind their integration, the challenges faced, and \nfuture directions for research in this domain. This survey serves as a crucial resource for \nPage 15 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \ncomprehending the current landscape and the potential of combining federated learn -\ning strategies with the robust capabilities of foundation models. Another pivotal work \nby Zhuang et  al. [143] provides an in-depth analysis of how foundational models can \nbe effectively adapted and optimized within a federated learning framework, discussing \nboth the technical hurdles and the potential breakthroughs. Additionally, Kairouz et al. \n[60] offers a comprehensive overview of the advancements and persistent challenges in \nfederated learning, highlighting issues such as algorithm efficiency, data heterogeneity, \nand security concerns. These surveys collectively offer a rich tapestry of insights into the \nevolving field of federated learning and foundation models, emphasizing their complexi -\nties and transformative potential.\nFederated learning and foundation models\nThe integration of pre-training techniques within federated learning (FL) setups, espe -\ncially for large-scale models, is increasingly viewed as essential for boosting model \nperformance and broadening their applicability. Extensive research underscores the \nimportance of pre-training in preparing large models to face the unique challenges pre -\nsented by the decentralized nature of federated datasets. Chen et al. highlight the vital \nrole of pre-training in readying large models for these challenges, emphasizing its neces -\nsity for effective performance within federated learning frameworks [84]. In a similar \nvein, Nguyen et al. investigate how the initial conditions of model training, such as the \nstarting points of pre-training and model initialization, critically influence the effective -\nness and convergence of FL models [144]. These studies stress the importance of meticu-\nlous pre-training phases to ensure that large models are fully equipped to navigate the \ncomplexities of federated learning, thereby maximizing their performance and utility \nin diverse applications. The application of federated learning to foundation models not \nonly addresses these preparatory needs but also leverages the inherent strengths of both \nparadigms to offer several key advantages: (1). Efficient Distributed Learning: Federated \nlearning enables models to learn from data distributed across multiple devices or serv -\ners without needing to centralize the data, thus preserving privacy and reducing data \nmovement costs. (2). Parameter-efficient Training: By utilizing techniques such as model \ncompression and prompt tuning within a federated framework, the training process \nbecomes more parameter-efficient. This is particularly beneficial in environments where \ncomputational resources are limited. (3). Prompt Tuning: This method involves fine-tun-\ning a model on a specific task by adjusting a small set of parameters, and when com -\nbined with federated learning, it allows for personalized model tuning on decentralized \ndata. (4). Model Compression: Techniques like quantization and pruning that reduce the \nmodel size can be effectively applied in federated settings, enhancing the feasibility of \ndeploying large models on edge devices with limited storage and processing capabilities.\nEfficient Distributed Learning Algorithms Efficient distributed learning algorithms are \ncritical for optimizing foundation models within the constraints of limited resources \n[89, 145]. These algorithms are specifically engineered to address the twin challenges of \nenhancing communication and computation efficiency during the training and deploy -\nment of large FMs across a network of devices, which may vary in capabilities and \nPage 16 of 54Li et al. BioData Mining            (2025) 18:2 \nnetwork conditions. Two pivotal techniques in this regard are model parallelism and \npipeline parallelism.\nModel parallelism [146] involves dividing the model into different segments and dis -\ntributing these segments across multiple devices. This allows for simultaneous process -\ning and can significantly expedite the computation process by leveraging the combined \npower of multiple devices. On the other hand, pipeline parallelism [147] focuses on \nenhancing the overall system’s efficiency and scalability by organizing the computation \nprocess in stages. Each stage can be processed on different devices in a pipeline manner, \nthus optimizing the workflow and reducing idle times.\nAn illustrative example of these parallelism strategies in federated learning (FL) for \nFMs is demonstrated in Fig.  1, where participants train distinct layers of a model using \ntheir own private, local data. Note that Fig.  1a is the illustration of model parallelism \nand Fig. 1b demonstrates the pipeline parallelism. This approach not only maintains the \nprivacy of the data but also contributes to the efficiency of the learning process. Recent \nstudies, such as the research conducted by Yuan et  al., validate the practicality of uti -\nlizing pipeline parallelism for decentralized FM training across heterogeneous devices \n[148].\nParameter‑efficient Training Methods Parameter-efficient training methods are \nincreasingly critical in optimizing foundation models for specific domains or tasks. \nThese methods typically involve integrating adapters-a technique where the core \nparameters of the FM are frozen, and only a small, task-specific section of the model \nis fine-tuned. This approach is illustrated in Fig.  1c, which shows how adapters can be \neffectively incorporated into the federated learning framework for FMs. Recent imple -\nmentations such as FedCLIP [50] and FFM [83] utilize this method to fine-tune FMs, \nachieving substantial performance improvements.\nBy focusing adjustments on small adapters rather than the entire model, these train -\ning methods greatly reduce the computational and communication demands typically \nrequired [149, 150]. This is particularly beneficial in FL environments where conserving \nbandwidth and processing power is crucial due to the distributed nature of the data and \nFig. 1 Efficient distributed learning and parameter efficient strategies for foundation models in federated \nlearning\nPage 17 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nthe varying capacities of participating devices. However, despite these efficiencies, the \nunderlying requirement for substantial computational resources to manage the FM and \nexecute the fine-tuning process remains significant.\nPrompt Tuning Prompt tuning has rapidly gained traction as a communication-effi -\ncient alternative to full model tuning, demonstrating effectiveness comparable to more \nresource-intensive methods [151]. This technique involves fine-tuning lightweight, addi-\ntional tokens while keeping the foundational model’s main parameters frozen, which \navoids the necessity of sharing large model parameters across the network. In federated \nlearning scenarios, this approach enables leveraging the collective knowledge from mul -\ntiple participants to refine the prompts used in FM training, potentially enhancing the \nperformance of the FM.\nThe integration of prompt tuning in FL, similar to the parameter efficient approach \ndepicted in Fig.  1c, has been explored in recent research. Studies such as FedPrompt \n[152] and PROMPTFL [88] have shown promising results by improving the quality and \neffectiveness of prompt-based training methods through FL frameworks. These methods \nenable efficient and targeted tuning of model behaviors without requiring extensive data \ntransfer or the deployment of large-scale models on each participant’s device, thereby \nconserving bandwidth and computational resources.\nMoreover, a recent study, FedTPG [90], investigates a scalable prompt generation \nnetwork that learns across multiple clients, aiming to generalize effectively to unseen \nclasses. This approach demonstrates the potential of FL to enhance the sophistication of \nprompt tuning methodologies by distributing the learning process across a wide array of \ndevices and data sources.\nHowever, the implementation of prompt tuning in FL is not without challenges. Con -\ncerns include the assumptions that large FMs are readily available on user devices, which \nmay not always be feasible in resource-constrained environments. Additionally, there are \npotential privacy risks associated with utilizing cloud-based FM APIs, which could com -\npromise the security of sensitive data.\nModel Compression Model compression has emerged as a vital strategy to mitigate the \nsubstantial memory, communication, and computational demands of large foundation \nmodels. By minimizing the size of these models, model compression enables more prac -\ntical deployments within federated learning frameworks without significantly compro -\nmising performance. Prominent compression techniques include knowledge distillation, \nwhere a smaller model is trained to emulate the performance of a larger one [153], and \nquantization, which reduces the numerical precision of model parameters to decrease \nboth size and computational complexity [154]. Additionally, pruning eliminates super -\nfluous or redundant model parameters, significantly lowering the resource requirements \nof the model [155].\nImplementing these compression techniques effectively requires striking a balance \nbetween reducing model size and preserving essential capabilities. This balance ensures \nthat the compressed model performs robustly in real-world applications, maintaining \nthe functionality of the foundation model while reducing operational demands. There -\nfore, research and development in model compression focus not only on shrinking \nPage 18 of 54Li et al. BioData Mining            (2025) 18:2 \nmodel dimensions but also on enhancing efficiency and intelligence, tailored for spe -\ncific deployment scenarios. [156] introduces ResFed, a framework that leverages model \ncompression in federated learning to significantly cut down on bandwidth and storage \nneeds while maintaining high model accuracy. [153] presents the concept of knowledge \ndistillation, which allows a compact “student” model to learn effectively from a larger \n“teacher” model, thus enabling the student to achieve similar performance with much \nlower computational costs. [154] explores quantization techniques for training neural \nnetworks that perform inference using only integer arithmetic, substantially lighten -\ning model load without sacrificing accuracy. [155] provides a thorough review of neural \nnetwork pruning techniques, showing their potential to significantly reduce model size \nwhile maintaining or improving performance. [61] discusses the integration of model \ncompression into federated learning, tackling challenges related to efficiency and scal -\nability in privacy-preserving, decentralized machine learning.\nFL on large language models and vision language models\nIn this section, we delve into the integration of federated learning with foundation mod -\nels on the two main model applications: large language models and vision language \nmodels, exploring the unique challenges and opportunities presented by these advanced \nAI systems.\nFL on large language models\nFederated learning applied to large language models (LLMs) represents a transforma -\ntive approach to harnessing decentralized datasets for model training, while prioritizing \ndata privacy and security [21, 157]. This method is especially crucial for LLMs because \nof their inherent requirement for vast and varied data inputs to accurately capture and \ninterpret the complexities of human language.\nThis nature of FL effectively addresses privacy concerns by ensuring that sensitive \nor proprietary data does not leave its original location, thereby reducing the risk of \ndata breaches. Additionally, this decentralized approach allows LLMs to learn from a \nwider array of linguistic inputs, reflecting regional dialects, colloquialisms, and cultural \nnuances that might not be present in a centralized dataset [158].\nMoreover, the application of FL to LLMs facilitates the development of models that are \nnot only linguistically comprehensive but also more personalized and responsive to local \ncontexts. By training on diverse datasets that are geographically dispersed, LLMs can \ndevelop a deeper understanding of language variations and user-specific preferences, \nleading to improved performance in tasks such as language translation, sentiment analy-\nsis [159], and contextual understanding.\nThis method also helps in mitigating biases that are often present in centralized train -\ning datasets. Since FL involves multiple datasets that are not centrally collected, the \nresulting model is trained on a broader spectrum of data sources, which can contribute \nto more balanced and fair outputs. Thus, federated learning not only enhances the pri -\nvacy and security of data used in training LLMs but also boosts the models’ ability to \ndecipher and utilize the full richness of human language, making them more accurate \nand effective in real-world applications.\nPage 19 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nPractical Applications of FL on LLMs The integration of federated learning with large \nlanguage models is yielding groundbreaking frameworks and methodologies that signifi -\ncantly enhance language model training while adhering to data privacy and security pro-\ntocols. In this survey, we highlight several notable applications and advancements in this \ndomain, including:\n• Privacy-preserving Federated Learning and its application to natural language pro -\ncessing: [157] explores privacy-preserving techniques in federated learning for \ntraining large language models. It particularly focuses on models such as BERT and \nGPT-3, providing insights into how federated learning can be leveraged to maintain \nprivacy without sacrificing the performance of language models in NLP applications.\n• FedMed: A federated learning framework for language modeling: [21] introduces \n“FedMed” , a novel federated learning framework designed specifically for enhancing \nlanguage modeling. The framework addresses the challenge of performance degrada-\ntion commonly encountered in federated settings and showcases effective strategies \nfor collaborative training without compromising on model quality.\n• Efficient Federated Learning with Pre-Trained Large Language Model Using Several \nAdapter Mechanisms: [160] highlights a method to enhance federated learning effi -\nciency by integrating adapter mechanisms into pre-trained large language models. \nThe study emphasizes the benefits of using smaller transformer-based models to alle-\nviate the extensive computational demands typically associated with training large \nmodels in a federated setting. The approach not only preserves data privacy but also \nimproves learning efficiency and adaptation to new tasks.\n• OpenFedLLM: This contribution is a seminal effort in federated learning specifically \ndesigned for large language models. The “OpenFedLLM” framework facilitates the \nfederated training of language models across diverse and geographically distributed \ndatasets. A standout feature of this framework is its capability to ensure data privacy \nduring collaborative model training. It also incorporates federated value alignment, \na novel approach that promotes the alignment of model outputs with human ethi -\ncal standards, ensuring that the trained models adhere to desirable ethical behaviors \n[161]. Moreover, OpenFedLLM is open-source1 , making it accessible to the broader \nresearch community and fostering collaboration in the development of federated lan-\nguage models.\n• Pretrained Models for Multilingual Federated Learning: This study addresses the \ncomplex challenges of utilizing pretrained language models within a federated learn -\ning context across multiple languages. Weller et al. ’s work is crucial for understand -\ning how multilingualism impacts federated learning algorithms, particularly explor -\ning the effects of non-IID (independently and identically distributed) data inherent in \nnatural language processing tasks across different languages. The research explores \nthree main tasks: language modeling, machine translation, and text classification, \nproviding valuable insights into the adaptability of federated learning to diverse lin -\nguistic datasets [162].\n1 https://github.com/rui-ye/OpenFedLLM\nPage 20 of 54Li et al. BioData Mining            (2025) 18:2 \n• GPT-fl: This innovative approach integrates federated learning with prompt-based \ntechniques to train large language models. “GPT-fl” employs prompt learning within \na federated framework, which allows for efficient learning from decentralized data \nsources while maintaining data privacy. This method enhances model adaptability \nand performance across various linguistic tasks, making it a promising solution for \napplications requiring high levels of customization and responsiveness to user-spe -\ncific contexts [86].\nIn summary, LLMs in FL focus on balancing privacy preservation with maintaining high \nperformance in NLP applications. Models like Privacy-preserving Federated Learning, \ne.g., FedMed, explore strategies to mitigate performance degradation in federated set -\ntings and enhance training efficiency using techniques such as adapter mechanisms. \nThese approaches are particularly adept at managing the significant computational over-\nhead associated with LLMs while ensuring that sensitive data remains secure within \nits local environment in FL. OpenFedLLM introduces an open-source framework that \nemphasizes ethical alignment in model outputs, advocating for responsible AI practices \nthat adhere to human ethical standards, which can be crucial as it addresses the grow -\ning concern over AI alignment with societal values. Meanwhile, research on Pretrained \nModels for Multilingual Federated Learning tackles the challenges of multilingual -\nism and non-IID data in federated learning, offering insights into effectively managing \ndiverse linguistic data and enhancing the robustness of language models across different \nlanguages. GPT-fl combines prompt-based techniques within a federated framework, \nimproving model adaptability and customization across linguistic tasks, which allows \nfor personalized and contextually relevant responses that are essential in dynamic real-\nworld applications.\nFL on vision language models\nThe integration of federated learning with vision language models (VLMs) marks a sig -\nnificant advancement in multimodal learning where both visual and textual data are \nprocessed in a privacy-preserving, distributed learning environment. These models are \ncrucial for tasks that necessitate a deep understanding and generation of information \nfrom visual cues and textual descriptions. Federated learning enhances the capability of \nVLMs by enabling them to learn from a diverse set of decentralized data sources, includ-\ning images and associated annotations from various geographic and demographic distri -\nbutions without the need to centralize sensitive data.\nVLMs integrated with FL are particularly beneficial in scenarios where data privacy \nis paramount, such as in healthcare for patient image data or in surveillance where \npersonal data protection is critical. By processing data locally and only sharing model \nupdates, FL preserves the privacy and security of the underlying data, while still benefit -\ning from the diverse data attributes necessary for robust model training.\nThis approach also allows for the training of more personalized and region-specific \nmodels, capturing a wide array of cultural and contextual nuances in visual-textual \ndatasets. For example, a VLM trained via federated learning can better understand and \ngenerate language descriptions for regional landmarks or culturally specific events, \nenhancing its applicability across different global contexts.\nPage 21 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nMoreover, the decentralized nature of FL helps in mitigating dataset bias, a common \nissue in centralized training datasets. Since the training data in FL comes from a wide \nrange of sources, the models are less likely to overfit to the biases present in a single \ndataset, leading to more generalizable and fair VLMs.\nThis section underscores the crucial role of prompt learning in expanding the capabili-\nties of both language and vision models trained in federated environments. By facilitat -\ning efficient task adaptation and maintaining data privacy, prompt learning represents \na significant step forward in the development of AI systems that can operate across \ndiverse and distributed data landscapes.\nPractical Applications of FL on VLMs  \n• FedCLIP: Pioneering the field of federated vision-language models, FedCLIP [19] \nadapts the powerful CLIP (Contrastive Language-Image Pre-training) architecture \n[50] to operate in a federated setting. Unlike traditional learning models that central-\nize data, FedCLIP enables collaborative learning across decentralized image datasets \nwith accompanying text descriptions. Crucially, this approach safeguards data pri -\nvacy by eliminating the need for sensitive user data to leave local devices.\n• PromptFL: [88] demonstrates the power of combining federated learning with \nprompt learning techniques for training models on distributed visual and textual \ndata. Prompt learning injects flexibility into model training. In PromptFL, federated \nlearning preserves privacy while prompt learning improves training effectiveness and \nefficiency across diverse datasets.\n• FedPrompt: Communication-Efficient and Privacy-Preserving Prompt Tuning in \nFederated Learning [152] addresses two critical aspects of federated learning for \nvision-language models: efficiency and privacy. Prompt tuning offers adaptability but \ncan be communication-intensive. FedPrompt explores methods to reduce communi-\ncation overhead while still reaping the benefits of prompt tuning, all while ensuring \nthat sensitive data remains protected.\n• pFedPrompt: [163] addresses personalization challenges in federated vision-language \nmodels. “Personalized Prompt for Vision-Language Models in Federated Learning” \ninvestigates how to learn personalized prompts. These prompts are tailored to indi -\nvidual clients or datasets within the federated system. The aim is to unlock perfor -\nmance gains by having the model adapt its behavior for specific local data distribu -\ntions.\n• FedTPG: (Text-driven Prompt Generation for Vision-Language Models in Federated \nLearning) [90] aims to enhance prompt generation techniques in a federated context. \nIt introduces the idea of learning a prompt generator network which can produce \ncontext-aware prompts that guide vision-language models to tackle a variety of tasks. \nThis has potential benefits for scenarios where a model must adapt to new classes or \ndata it hasn’t encountered previously, aligning well with the distributed nature of fed-\nerated learning.\n• FedMM: In computational pathology, fusing information from multiple modali -\nties can significantly improve diagnostic accuracy. However, centralized training \napproaches raise privacy concerns due to the sensitive nature of medical images. \nPage 22 of 54Li et al. BioData Mining            (2025) 18:2 \nFedMM introduces a federated framework designed specifically to handle multi-\nmodal data in this context. The key idea of [164] is to train individual feature extrac -\ntors for each modality in a federated manner. Because only these learned feature \nextractors are shared, raw image data remains protected within each institution. \nFedMM can accommodate the situation where different institutions or hospitals may \nhave different sets of available modalities. It enables collaborative learning even with \nthis data heterogeneity. Subsequent tasks like classification can be performed locally \nusing the features extracted by the federated models.\n• FedD AT: Foundation models offer impressive performance across many tasks but \noften require substantial amounts of data for finetuning. FedDAT [165] addresses \nthe challenge of finetuning these models in a federated context where the goal is to \nprotect data privacy. To handle heterogeneity, FedDAT leverages a Dual-Adapter \nTeacher technique to regularize how model updates are made on each client. Fur -\nthermore, it employs Mutual Knowledge Distillation to facilitate efficient knowledge \ntransfer across clients in the federated system.\n• CLIP2FL: Real-world data is often messy, and client devices in a federated system \nmight have data with different characteristics or class imbalances. CLIP2FL [166] \ntackles this by using a pre-trained CLIP model as guidance. On the client-side, CLIP \nis used for knowledge distillation to improve the local feature representations. On \nthe server-side, CLIP is employed to generate features which help retrain the server’s \nclassifier, mitigating the negative impact of the long-tailed data problem.\n• FedAPT: [167] introduces FedAPT, a novel method for collaborative learning in fed -\nerated settings where data resides on multiple clients with varying domains (e.g., \ndifferent image styles or categories). FedAPT aims to improve model generaliza -\ntion across these domains while maintaining data privacy. The key innovation lies in \nadaptive prompt tuning within the federated learning framework. Instead of directly \nsharing raw data, FedAPT trains a meta-prompt and adaptive network to personalize \ntext prompts for each specific test sample. This allows the model to better adjust to \ndomain-specific characteristics.\n• General Commerce Intelligence: [168] discusses the development of a novel NLP-\nbased engine designed for commerce applications. This engine leverages federated \nlearning to provide personalized services while ensuring privacy preservation across \nmultiple merchants. The authors focus on creating a “glocally” (globally and locally) \noptimized system that balances global optimization needs with local data privacy \nrequirements.\nIn summary, federated learning models for vision-language tasks have demonstrated \nsignificant innovation in enhancing privacy and efficiency while leveraging the synergy \nbetween textual and visual data. Models like FedCLIP , PromptFL, and FedPrompt focus \non privacy and decentralization, with trade-offs in model accuracy and communication \noverhead. Personalization and adaptability are key in pFedPrompt and FedAPT, which \naim to tailor learning to local datasets but introduce complexity in managing local and \nglobal optimization. FedTPG and CLIP2FL enhance adaptability to new tasks and data \nvariability, although at the cost of increased computational demands. FedMM and Fed -\nDAT tackle challenges in multi-modal and heterogeneous data integration, crucial for \nPage 23 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \napplications like medical diagnostics. Lastly, General Commerce Intelligence optimizes \nfederated learning for commercial applications, balancing local privacy with global opti -\nmization needs.\nFramework for federated foundation models in biomedical\nConceptual framework\nWe simulate a hierarchical multi-tier architecture for the integration of FL and FMs to \nhandle biomedical challenges:\n• Data Centers: Each compute node in FL hosts its private biomedical datasets, which \nis stored locally and only communicated with the server to ensure privacy.\n• Model Host: Foundation Models, pretrained on large-scale, public datasets, serve as \nthe backbone on the server, which can be fine-tuned with the distributed data for \nthe targeted biomedical challenges. Note that the large-scale model can be trained \nor transferred with model distillation or finetuning methods like Parameter-Efficient \nFine-Tuning [169].\n• Aggregation: FL algorithms like FedAvg [13], FedProx [61] are performed to aggre -\ngate updates from the nodes while addressing data heterogeneity and fairness con -\ncerns.\n• Feedback: Explainable metrics and evaluation pipelines will be introduced post-\naggregation to make the system robust and trustworthy.\nAlgorithm\nIn the following algorithm, we introduce the algorithm of Federated Foundation Model \nin biomedical:\nAlgorithm 1 Federated Foundation Model in Biomedical\nPractical applications\nTraining foundation models within a federated learning framework presents distinct \nchallenges, particularly due to the disparate nature of data sources and the varied \nPage 24 of 54Li et al. BioData Mining            (2025) 18:2 \ncomputational resources across participating devices. The overarching goal is to cul -\ntivate effective and inclusive training strategies that can efficiently manage device het -\nerogeneity and ensure data privacy, all while maintaining high model performance.\nTraining foundation models from scratch within a federated learning context is an \nambitious endeavor that involves complex coordination and robust algorithmic strat -\negies. Unlike traditional centralized training environments, federated learning neces -\nsitates handling data that remains on local devices, preventing the direct sharing of \nraw data. This scenario demands sophisticated techniques to efficiently aggregate \nlearning from disparate data sources, which are often uneven in size and diversity. \nThe primary challenge lies in ensuring that the model learns effectively from each \nnode without requiring extensive computational resources or compromising the \nintegrity and privacy of the data. To overcome these hurdles, training strategies must \nbe carefully designed to optimize the learning process across the network, allowing \nfor both model convergence and performance retention. Such strategies often involve \nadvanced algorithms for secure multi-party computation, differential privacy, or \ndecentralized optimization methods. By training foundation models from scratch in \nthis way, the federated approach not only safeguards data privacy but also harnesses \nthe unique insights embedded in local data distributions, leading to more robust and \ngeneralizable models.\nPrompt learning is emerging as a pivotal approach in both natural language pro -\ncessing and computer vision fields, enabling models to adapt to new tasks with mini -\nmal changes to their architecture or weights. This section explores the integration of \nprompt learning with federated learning (FL) across different domains, highlighting \nrecent advancements and unique applications.\nFurthermore, beyond merely fostering participation, it is crucial to consider how \nprofits and costs associated with deploying FMs via APIs are distributed. Ensuring \na fair allocation of rewards and benefits is imperative to maintain trust and promote \nsustained cooperation among stakeholders. Mechanisms need to be established to \ndefine the distribution of profits derived from the use of FMs, guaranteeing a fair \nshare of economic benefits. This equitable distribution is essential not only for foster -\ning a sense of fairness but also for encouraging continued participation and invest -\nment in the FL ecosystem for FMs.\nThe concept of Federated Foundation Models is at the forefront of federated learn -\ning, enabling the training of large-scale models across distributed networks. This \nmethodology is particularly effective in dealing with the challenges related to syn -\nchronizing and updating model parameters in environments where data quality and \nquantity are inconsistent across nodes. It ensures that learning is continuous and \neffective, even when network conditions and data availability vary significantly [83].\nAdditionally, the work titled “Heterogeneous Ensemble Knowledge Transfer for \nTraining Large Models in Federated Learning” by Cho et al. [170] explores innovative \ntechniques for transferring knowledge in federated settings. This study is crucial for \nthe development of robust models capable of performing well across diverse network \nconditions. By facilitating knowledge transfer, this approach allows for the aggre -\ngation of insights from different data distributions and device capabilities, which is \nessential for building comprehensive and resilient models.\nPage 25 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nFurthermore, “No One Left Behind: Inclusive Federated Learning over Heterogene -\nous Devices” by Liu et al. [171] focuses on creating federated learning algorithms that \nintegrate every participating device, regardless of its computational capabilities or the \nquality of the data it holds. This inclusivity ensures that every device contributes to \nand benefits from the collaborative learning process, thus maximizing the utilization \nof available data and enhancing the overall performance of the model. This approach is \nfundamental to achieving equity in model training and ensuring that the advantages of \nsophisticated model learning are universally accessible.\nThese studies provide a foundation for further research into strategies that enhance \nthe combination of foundation models in federated learning frameworks, prioritizing \ninclusivity and efficiency.\nRead‑world applications of federated fm in healthcare\nRecently, the integration of Federated Learning with Foundation Models has begun to \ndemonstrate transformative potential in real-world healthcare applications. In this part, \nwe highlight specific case studies and practical examples where these technologies have \nbeen successfully deployed.\n• Predicting Parkinson’s Disease Progression: [172] applies FL to train explainable AI \n(XAI) [173] models for predicting the progression of Parkinson’s disease. By collabo -\nrating across multiple medical centers without sharing raw patient data, they devel -\noped models that maintained patient privacy while achieving high predictive accu -\nracy.\n• Mammography Analysis: [174] focuses on using FL for mammography analysis, ena -\nbling different healthcare providers to collaboratively train deep learning models \nwithout centralizing sensitive patient data.\n• Intensive Care Unit (ICU) Mortality Prediction: FLICU [175] framework utilizes FL \nto predict ICU mortality rates. By training models on decentralized data from multi -\nple ICUs, the study demonstrated that FL could achieve performance comparable to \ncentralized models.\nFoundation models in biomedical healthcare\nThis section is dedicated to a comprehensive exploration of the application of founda -\ntion models within the biomedical healthcare domain, focusing on both language and \nvision-language models. It will delve into the benchmarks and setups employed to evalu-\nate these models, highlighting the specialized frameworks and metrics used to assess \ntheir performance in healthcare-specific downstream tasks in foundation models.\nBiomedical foundation models\nThe application of foundation models has revolutionized numerous fields, particularly \nin natural language processing (NLP) and vision-language multimodal tasks. This trans -\nformation is largely attributed to several pivotal factors. Firstly, extensive pre-training \non large text corpora allows these models to develop comprehensive universal language \nrepresentations, which significantly enhance performance on various downstream tasks \n[176]. Secondly, such pre-training provides an improved initialization for models, which \nPage 26 of 54Li et al. BioData Mining            (2025) 18:2 \nnot only boosts generalization capabilities but also speeds up convergence on specific \ntarget tasks. Thirdly, this method acts as a powerful form of regularization, crucial for \npreventing overfitting, particularly when training data is scarce.\nMeanwhile, Vision-language multimodal models [50, 177, 178] are emerging as a pow-\nerful subset of foundation models, particularly in the field of image classification tasks in \nbiomedical healthcare. These models synergistically combine the capabilities of image \nprocessing and language understanding to tackle complex tasks that require the inte -\ngration of visual and textual data. In the healthcare sector, this ability is invaluable, as \nit enables the models to interpret medical imagery, such as scans and X-rays, alongside \nassociated clinical notes or diagnostic information [179]. For example, a vision-language \nmodel might analyze an MRI scan while simultaneously considering a patient’s written \nmedical history to provide a more accurate diagnosis. This dual capability enhances the \nmodel’s precision in identifying disease markers, understanding patient symptoms, and \nsuggesting appropriate medical interventions. The integration of these two modalities in \na single model not only streamlines the diagnostic process but also improves the accu -\nracy of treatment recommendations, paving the way for more personalized and effec -\ntive healthcare solutions. By leveraging these advanced models, medical professionals \ncan gain deeper insights into patient conditions, leading to better outcomes and more \nefficient management of healthcare resources.\nThe training of foundation models (FMs) in the biomedical domain involves several \ncrucial phases that enhance their applicability and effectiveness. Initially, unsupervised \npretraining [5] plays a pivotal role, where models learn from large corpora without \nlabeled data. This phase emphasizes the discovery of inherent structures and abstract \nrelationships within the data, without the need for specific predictive tasks, making it \ninvaluable for identifying complex patterns. Subsequently, self-supervised learning \nforms the backbone of foundation models, traditionally utilizing unstructured text from \ngeneral-domain sources such as Wikipedia or web-crawled pages. Recent advancements, \nhowever, have steered the customization of pre-trained FMs towards specific fields to \nbetter meet domain-specific requirements. For instance, CodeBERT [180] is meticu -\nlously trained on programming languages to proficiently comprehend and generate \ncode, whereas SciBERT [58] is tailored for parsing scientific publications and biological \nsequences, addressing the unique challenges of academic and medical research. Follow -\ning this, reinforcement learning from human feedback (RLHF) [9] introduces a novel \nfine-tuning approach where models are adjusted based on rewards derived from human \nfeedback rather than traditional labels. This method significantly aligns model outputs \nwith human values and preferences, essential for applications demanding high engage -\nment and accuracy in user interactions. Lastly, in-context learning [181], especially \neffective in models like GPT, leverages the model’s capacity to generalize from a few \nexamples. By presenting models with specific examples of the desired task at inference \ntime, they dynamically adapt their responses to the context, enhancing their flexibility \nand utility without the need for additional training. This sequence of training methods \ncollectively enhances the adaptability and performance of FMs, making them highly \nsuitable for sophisticated tasks in the biomedical domain.\nFoundation models can greatly benefit from training on expanded, domain-specific \ncorpora [182]. For achieving peak performance in specialized downstream tasks, it is \nPage 27 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nincreasingly recognized that integrating in-domain data during the training phase is \nimperative. This targeted approach not only refines the model’s understanding of com -\nplex biomedical terminologies but also significantly enhances its practical applications \nin healthcare. By tailoring the training process to incorporate specific biomedical vocab -\nulary and contextual nuances, FMs can be transformed into more effective tools, offer -\ning substantial improvements in processing and understanding medical texts, which is \nvital for advancing innovations and solutions within the healthcare industry.\nBiomedical healthcare ML applications and benchmarks\nApplications The application of FMs in the biomedical domain is propelled by a range \nof compelling reasons, each underscoring the unique challenges and opportunities this \nfield presents.\n• Complexity of Sequential Biomedical Data: Biomedical information, including elec -\ntronic health records and biomedical texts, often comes in the form of sequential \ntokens lacking annotations. Historically, this complexity posed significant hurdles for \neffective data modeling. However, advancements in FMs have enabled effective train-\ning on such data in a self-supervised manner, significantly expanding the possibilities \nfor processing and understanding biomedical information using these sophisticated \nmodels.\n• Scarcity of Annotated Data: In the biomedical field, annotated data is typically scarce \nand expensive to produce, often leading to “zero-shot” or “few-shot” learning scenar-\nios. Recent developments in language models, notably GPT-3 [45], have showcased \nremarkable capabilities in few-shot and even zero-shot learning. This evolution \nmeans that a well-trained FM can act as a powerful feature extractor in the biomedi -\ncal domain, reducing the dependency on large volumes of annotated data and easing \nthe barriers to entry for complex biomedical analysis.\n• Knowledge Intensity: The biomedical sector is densely packed with specialized knowl-\nedge, much more so than general domains, often necessitating expert-level under -\nstanding. FMs serve as an accessible, soft knowledge base [183], which can assimi -\nlate and replicate expert knowledge from vast biomedical texts without direct human \nannotation. For example, GPT-3 has shown an impressive ability to recall and apply \nextensive, intricate common knowledge in practical applications [45], demonstrating \nits utility as a tool for knowledge dissemination and decision support in healthcare.\n• Diversity of Biological Data: The scope of biomedical data extends beyond textual \ninformation to include diverse biological sequences, such as proteins and DNA. The \napplication of FMs to these types of data has been notably successful, particularly in \ntasks like protein structure prediction. This success underlines the potential of FM to \ntackle a broader array of biological challenges, suggesting a promising future where \nFMs contribute substantially to critical tasks in genomics, proteomics, and other \nareas of biological research.\n• Speed of Knowledge Synthesis [ 184]: The rapid pace at which biomedical knowl -\nedge evolves makes it challenging to keep up with the latest research and clini -\ncal practices manually. FMs, trained on the latest corpus of literature and clinical \nPage 28 of 54Li et al. BioData Mining            (2025) 18:2 \nguidelines, can quickly synthesize new information, making them invaluable tools \nfor healthcare professionals who need to stay informed about the latest develop -\nments in real-time.\n• Enhanced Predictive Analytics [185]: FMs have the potential to revolutionize predic -\ntive analytics in healthcare by integrating diverse data types-from patient records to \nresearch articles-to predict disease outbreaks, patient outcomes, and treatment effi -\ncacy. This capability can lead to more personalized medicine, where treatments are \ntailored to individual patients based on predictions made by these models.\n• Automated Reasoning and Decision Support: FMs can be employed to automate rea -\nsoning processes and support decision-making in clinical environments. By process -\ning and analyzing large volumes of medical data, these models can suggest diagnostic \noptions, propose treatment plans, and even predict possible complications, thereby \nassisting medical professionals in making better-informed decisions.\n• Reduction in Diagnostic Errors [ 186]: By providing comprehensive, data-driven \ninsights, FMs can help reduce diagnostic errors, one of the significant challenges \nin healthcare. Their ability to learn from vast datasets and identify patterns that \nmay be overlooked by human experts can contribute to more accurate diagnoses \nand, consequently, more effective treatments.\nThese factors collectively motivate the integration of foundation model into biomedi -\ncal research and healthcare operations, indicating a robust pathway for leveraging AI \nto manage and utilize complex biomedical data more effectively.\nBenchmarks The application of pre-trained FMs in the biomedical field exploits a \ndiverse array of unstructured data sources, including electronic health records, scien -\ntific publications, social media texts, biomedical image-text pairs, and various biologi -\ncal sequences such as proteins. For a comprehensive review of mining electronic health \nrecords (EHR), please refer to the previous survey [187]. Discussions on the integration \nof health records and social media texts are explored in [188], while a systematic over -\nview of biomedical textual corpora is presented in [189].\nKey Benchmarks in Biomedical Research:\n• Electronic Health Records (EHR): EHRs encapsulate a comprehensive digital record of \npatient information, including demographics, medical history, medications, labora -\ntory test results, and billing details. They are pivotal for longitudinal studies, allowing \nresearchers to track patient outcomes over time and identify patterns and predictors \nof diseases. The vast amount of data contained within EHRs makes them invaluable \nfor training FMs to recognize and predict medical conditions accurately, although \naccess is tightly regulated to protect patient privacy [190, 191].\n• MIMIC-III (Medical Information Mart for Intensive Care III): This critical care \ndatabase contains detailed information from over 58,976 ICU admissions, includ -\ning 2,083,180 vital signs, medications, laboratory measurements, observations, \nand notes. This richness makes MIMIC-III ideal for developing models that pre -\ndict patient outcomes, tailor treatments, and conduct epidemiological studies in \ncritical care settings [192].\nPage 29 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \n• CPRD (Clinical Practice Research Datalink): A comprehensive dataset that pro -\nvides a complete medical record from GP practices in the UK. It includes diagno -\nses, prescriptions, and clinical events, making it highly suitable for observational \nstudies and clinical trials. The linkage to secondary care data enhances its utility in \ncomprehensive healthcare research [193].\n• Reddit and Tweets: These datasets are increasingly used for public health monitor -\ning and sentiment analysis. Reddit’s COMETA corpus and Twitter’s COVID-twit -\nter-BERT provide real-time data on public health trends, misinformation patterns, \nand community response to health crises, which are crucial for understanding \npublic health behavior and improving communication strategies [194, 195].\n• MIMIC-CXR: This dataset of chest x-rays and accompanying radiological reports \nis crucial for developing automated diagnostic tools that assist radiologists in \ndetecting and diagnosing pathologies from imaging studies. The textual descrip -\ntions help train models to correlate visual signs with diagnostic language [196].\n• DNA Dataset: This genomic dataset facilitates the training of models on genetic \nsequences to predict gene functions, understand genetic variations, and assist in \npersonalized medicine strategies. It is essential for advancing genomics research \nand integrating genetic information with clinical data [197].\n• FMRI datasets: These datasets comprise data from functional magnetic resonance \nimaging (fMRI) studies, which are invaluable in providing detailed insights into \nbrain activity. Utilized extensively in neuroscience, fMRI data helps researchers \nunderstand brain functions, diagnose neurological disorders, and predict out -\ncomes of therapeutic interventions. Notable datasets like the Philadelphia Neu -\nrodevelopmental Cohort (PNC) [198], Autism Brain Imaging Data Exchange \n(ABIDE) [199], and UK Biobank [200] include both functional and structural brain \nimaging data. These resources are critical for advancing our understanding of the \nbrain, enhancing the accuracy of neurological diagnoses, and improving the effi -\ncacy of treatments by enabling a deeper analysis of the brain’s response to various \nstimuli and conditions.\n• The Human Protein Atlas: Contains high-resolution images detailing the spatial \ndistribution of proteins in human tissues and cells [201]. This atlas is used for bio -\ninformatics studies that integrate protein expression with gene expression data to \nelucidate cellular functions and disease mechanisms.\n• GEUVADIS RNA sequencing dataset [ 202]: Provides RNA sequencing data from \nmultiple populations, which is crucial for understanding how genetic variation \naffects gene expression across different human populations. This dataset is instru -\nmental in studying population genetics, evolutionary biology, and disease suscep -\ntibility.\n• ImageCLEFmed [ 203]: A benchmark dataset for multimodal biomedical informa -\ntion retrieval that includes medical images, captions, and text descriptions. It sup -\nports tasks such as medical image classification, annotation, and retrieval, which \nare crucial for medical informatics applications.\nThese datasets exemplify the diverse types of biomedical data available for research, \neach offering unique insights and challenges that can be leveraged to train more \nPage 30 of 54Li et al. BioData Mining            (2025) 18:2 \neffective and nuanced FMs for varied applications in healthcare and medical research. \nNote that the numerical details of the datasets are demonstrated in Table 1 .\nBiomedical healthcare on large language models\nAs introduced in Backbone networks in foundation models  section, the backbone of \nmost pre-trained foundation models, including prominent ones like BERT, GPT, T5, and \ntheir variants, is founded on the Transformer architecture, which framework is charac -\nterized by its reliance on self-attention networks and feed-forward networks (FFNs). The \nbenign enables dynamic interactions between tokens, enhancing the model’s ability to \nhandle complex input relationships, while FFNs perform non-linear transformations to \ndeepen token representations, bolstering feature extraction capabilities.\nIn parallel, the evolution of text representation through FMs has significantly \nadvanced from initial static word embedding methods to sophisticated models capable \nof understanding contextual nuances [204, 205]. Historical neural language models laid \nthe groundwork by predicting word contexts in a unidirectional manner, but modern \napproaches like ELMO [41], GPT [206], and BERT [207] have transformed the land -\nscape with bi-directional and context-aware strategies. These models, through method -\nologies such as bidirectional language modeling and masked language model tasks, offer \ndynamic, context-sensitive word representations that vastly enhance performance across \ndiverse NLP applications, making them fundamental to contemporary language process-\ning tasks.\nHow to tailor LLMs to the biomedical domain\nThe adaptation of large language models to the biomedical domain involves special -\nized methodologies tailored to enhance their functionality for this sector’s unique \ntasks. Initially crafted for general natural language processing (NLP) tasks, mod -\nels like BERT [207] typically undergo a two-stage training process: initial training \nthrough a self-supervised meta-task (such as a masked language model or causal \nlanguage model) on a broad, task-agnostic corpus, followed by fine-tuning on more \nTable 1 Overview of key biomedical healthcare benchmarks\nDataset Size Types\nMIMIC‑III 58,976 admissions Text, Numeric, Categorical\nCPRD 11.3 million patients Text, Numeric, Categorical\nReddit and Tweets 800K reddit posts and up‑to‑date \ntweets\nText\nMIMIC‑CXR 77,110 images Images, Text\nDNA Dataset 106 DNA sequences Genetic Sequences\nPNC 9,500 participants Imaging (Functional, Structural)\nABIDE Over 1,100 individuals Imaging (Functional, Structural)\nUK Biobank Over 500,000 participants Imaging (Functional, Struc‑\ntural), Genetic, Text\nHuman Protein Atlas 12,003 proteins Images, Text\nGEUVADIS RNA sequencing 462 individuals Genetic Sequences\nPage 31 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nspecialized, often smaller-scale, downstream tasks relevant to specific fields. The two \nstrategies have been developed to better integrate LLMs into the biomedical field are \nas follows:\n• Continual Pre-training: This method involves taking general LLMs such as BERT, \ninitially pre-trained on extensive general corpora like Wikipedia or BookCorpus, \nand continuing their training on domain-specific corpora, such as PubMed texts \nand MIMIC-III data. For instance, BioBERT [208] extends BERT’s training to \ninclude PubMed abstracts and articles, while BlueBERT [209] is further trained \non both PubMed and MIMIC-III texts. These adaptations often retain the original \nmodel’s vocabulary, which may not fully capture the specialized terminology of \nbiomedical texts [182].\n• Pre-training from Scratch: Some research advocates starting anew with domain-spe -\ncific corpora to tailor PLMs more closely to biomedical needs [182, 210]. SciBERT \n[210] is an example of this approach, where a novel vocabulary of 30,000 terms spe -\ncific to the domain was developed, and the model was trained on a corpus compris -\ning both computer science (18%) and biomedical (82%) texts. However, recent find -\nings suggest that mixed-domain pre-training might not be optimal for applications \nrequiring high domain specificity. Instead, exclusive pre-training on biomedical cor -\npora is recommended to ensure maximum relevance and efficacy.\nRelated Literatures Before exploring the applications of Large Language Models in the \nbiomedical healthcare domain, it is essential to recognize several representative surveys \nand peer-reviewed publications that have thoroughly reviewed the landscape of bio -\nmedical language models. These resources provide invaluable insights into the develop -\nment, applications, and future prospects of LLMs within this specialized field, laying a \nfoundational understanding for ongoing and future research. Several key surveys and \npublications have extensively discussed the current state and potential advancements of \ntransformer-based biomedical models and general prompting methods in natural lan -\nguage processing:\n• AMMU: A Survey of Transformer-Based Biomedical Pretrained Language Models \n[211]: This comprehensive survey examines the evolution and impact of transformer-\nbased models that have been specifically developed for the biomedical field. The sur -\nvey details various approaches to adapting general language models to address the \nunique challenges posed by biomedical texts, such as the high specificity of vocabu -\nlary and the critical nature of the accuracy needed in medical contexts. The authors \ndiscuss multiple models that have been successfully implemented, highlighting their \nmethodologies, the datasets they were trained on, and their performance on different \nbiomedical NLP tasks. It provides a critical analysis of the strengths and limitations \nof these models, offering insights into how the field might evolve and suggesting \ndirections for future research to enhance model accuracy and applicability.\n• Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in \nNatural Language Processing [ 212]: by Liu et  al. [212]: This survey explores the \nPage 32 of 54Li et al. BioData Mining            (2025) 18:2 \nrelatively new technique of prompting, which adapts pre-trained models to spe -\ncific tasks using minimal task-specific data. Prompting involves modifying the \ninput to pre-trained models in such a way that the task is reformulated to lever -\nage the model’s existing knowledge. The survey systematically categorizes different \ntypes of prompts, discusses their applications in various NLP tasks, and evaluates \ntheir effectiveness across several benchmarks. It provides a detailed look at how \nprompting can reduce the need for large annotated datasets, which is particularly \nbeneficial in domains like biomedicine where acquiring such data can be costly \nand time-consuming. The paper also considers the future of prompting in NLP , \nsuggesting that further refinement of prompting strategies could lead to more \ngeneralizable and efficient NLP systems.\n• Foundation Models in Healthcare: Opportunities, Risks & Strategies Forward [213]: \nThis survey delves into the dual-edged nature of applying foundation models within \nthe healthcare sector. It discusses the substantial opportunities these models present, \nsuch as enhancing diagnostic accuracy, predicting patient outcomes, and personal -\nizing treatment plans. However, it also addresses the significant risks involved, par -\nticularly concerning data privacy, model bias, and the ethical implications of auto -\nmated decision-making in healthcare. The authors propose a framework of strategies \nto mitigate these risks while capitalizing on the potential benefits. These strategies \ninclude developing robust governance frameworks, ensuring transparency in model \nworkings, and engaging with a broad range of stakeholders to ensure that the deploy-\nment of these models in healthcare settings is both ethical and effective.\n• On the Opportunities and Risks of Foundation Models [25]: This broad survey pro -\nvides an extensive overview of the application of foundation models across various \ndomains, with a particular focus on their transformative potential and the risks they \npose. In the context of healthcare, the survey highlights how these models can revo -\nlutionize medical research and practice by providing new insights into disease pat -\nterns and patient care strategies. However, it also raises critical concerns about the \nreliability, fairness, and transparency of these models, especially given their potential \nto impact patient outcomes directly. The paper calls for a balanced approach to har -\nnessing the power of foundation models, advocating for rigorous testing, ethical con-\nsiderations, and regulatory oversight to ensure they benefit society as a whole.\nPractical LLMs in biomedical healthcare\nSince the introduction of BERT, a variety of biomedical pre-trained language models \nhave been developed, enhancing the capabilities of NLP applications within the bio -\nmedical field. These models have been adapted either by further training on specialized \nin-domain corpora or by being built from scratch to cater specifically to the needs of \nmedical and scientific communication. Below is a detailed summary of several exist -\ning pre-trained language models, where the specialized corpora, LLM backbone and \nreleased date are highlighted in Table 2.\n• BioBert [208]: A pioneering work represents a significant advance in the application \nof language models to the biomedical domain. By adapting the BERT architecture, \nPage 33 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \noriginally designed for general language understanding, BioBert is fine-tuned with \nbiomedical texts sourced from extensive databases such as PubMed abstracts and \nPMC full-text articles. This adaptation is not merely a continuation of training but \na targeted effort to align the model’s learning with the intricacies and terminologies \nunique to biomedical literature. As a result, BioBert excels in several biomedical text \nmining tasks including named entity recognition, relation extraction, and question \nanswering over biomedical knowledge bases. The strength of BioBert lies in its abil -\nity to capture deep semantic connections between biomedical concepts, significantly \nimproving the model’s utility for researchers and healthcare professionals who rely \non swift and accurate interpretations of medical texts.\n• MedBert [24]: MedBert is an innovative approach to creating a language model that \nis steeped from the outset in the medical context. Unlike models that are adapted \nfrom general-purpose architectures, MedBert is pre-trained from scratch on a large \nand diverse corpus of medical texts, including electronic health records and other \nclinical documents. This ground-up approach allows MedBert to develop a nuanced \nunderstanding of medical language, including jargon, abbreviations, and the complex \nrelationships between medical concepts. The model has shown significant improve -\nments in tasks such as patient phenotyping and diagnostic prediction, making it a \nvital tool for healthcare analytics. MedBert’s design addresses the challenges of \napplying general language models to medical data, ensuring that the nuances and \ncritical details of medical communication are not lost in translation.\n• ClinicalBERT [216]: Tailored for understanding and processing clinical notes, Clini -\ncalBERT was trained exclusively on data from the MIMIC-III database [192],, which \nincludes around 2 million clinical notes. This specialized training prepares Clinical -\nBERT to handle a variety of clinical documentation styles and medical shorthand, \nmaking it an invaluable tool for applications like patient outcome prediction and \nautomated documentation review, which require a deep understanding of clinical \nnarratives.\n• SciBERT [210]: Developed from scratch, SciBERT focuses on scientific text, primar -\nily from the biomedical field, leveraging a corpus of papers available through the \nTable 2 Overview of pre‑trained language models in biomedicine with release dates\nModel Name Corpora LLM Backbone Release Date\nBioBert PubMed abstracts, PMC articles BERT 2020\nMedBert Medical texts, EHRs BERT 2021\nClinicalBERT MIMIC‑III clinical notes BERT 2019\nSciBERT Scientific papers (82% biomedical) BERT 2019\nCOVID‑twitter‑BERT Tweets about COVID‑19 BERT 2023\nMedGPT Electronic health records (EHRs) GPT 2021\nSCIFIVE Biomedical corpora T5 2021\nLLMBiomedicine Biomedical texts (NER [214] tasks) GPT‑4 2024\nClinicalGPT Diverse medical data GPT 2023\nMultiMedQA Medical QA datasets PaLM [46] 2023\nChatdoctor Patient‑physician conversations LLaMa 2023\nTaiyi Biomedical texts, multilingual Qwen [215] 2024\nPage 34 of 54Li et al. BioData Mining            (2025) 18:2 \nSemantic Scholar database. With 82% of its training corpus composed of biomedi -\ncal research articles, SciBERT is adept at deciphering complex scientific terminol -\nogy and extracting relevant information from scholarly articles, thereby facilitating \nadvanced text mining and information retrieval tasks in scientific research.\n• COVID-twitter-BERT [195]: This model was specifically developed to analyze and \nunderstand discourse about COVID-19 on Twitter. It was trained during the initial \nstages of the pandemic on a dataset comprising approximately 160 million tweets \nrelated to the virus. The model is designed to capture the nuances of public senti -\nment, misinformation, and evolving topics related to COVID-19, providing valuable \ninsights for public health officials and researchers studying communication patterns \nduring health crises.\n• MedGPT [22]: Inspired by the GPT architecture, MedGPT was trained on electronic \nhealth records (EHRs) and is designed to predict future medical events based on \npatients’ medical histories. Its training allows it to model and predict various out -\ncomes, such as diagnoses and complications, making it a potential tool for prognos -\ntic assessments in clinical settings.\n• SCIFIVE [217]: This model is a domain-specific adaptation of the T5 model, trained \nunder the Seq2seq framework on extensive biomedical corpora. SCIFIVE is engi -\nneered to transform complex biomedical queries into concise answers, facilitating \ntasks such as summarizing scientific texts and generating explanatory notes from \ndense medical data.\n• LLMBiomedicine [218]: This research highlights the effectiveness of meticulously \ndesigned prompts and the strategic selection of in-context examples to enhance the \nperformance of LLMs on biomedical NER tasks. By adjusting prompts and exam -\nples to better fit the context of biomedical data, the study demonstrates significant \nimprovements in model performance, making LLMs more adept at identifying and \nclassifying medical entities in text.\n• ClinicalGPT [219]: ClinicalGPT, a model that has been fine-tuned with a diverse set \nof medical data to enhance its performance and reliability in clinical scenarios. The \nmodel undergoes rigorous evaluations to ensure it meets the high standards required \nfor medical applications, focusing particularly on its ability to maintain factual accu -\nracy and provide contextually appropriate responses in simulated clinical interac -\ntions. ClinicalGPT represents a significant advancement in the use of LLMs in medi -\ncine, offering potential improvements in automated patient interaction, diagnostic \nsupport, and personalized treatment planning. By leveraging a vast corpus of medi -\ncal texts for fine-tuning, the model is better equipped to handle the nuanced and \nhighly specialized language found in clinical notes and patient interactions.\n• MultiMedQA [220]: MultiMedQA is a comprehensive benchmark combining six \nexisting medical question answering datasets, which span a variety of contexts from \nprofessional medicine to consumer health inquiries. The benchmark is enhanced by \na newly developed dataset, HealthSearchQA, which consists of medical questions \nfrequently searched online. This diverse collection of datasets is utilized to test the \nLLMs’ ability to understand and process complex medical information across dif -\nferent facets of healthcare and patient inquiries. The authors discuss the significant \nchallenge of assessing LLMs in clinical settings, where the accuracy of information \nPage 35 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nand the models’ understanding of nuanced medical language are crucial. By employ -\ning MultiMedQA, the authors aim to provide a more nuanced and thorough evalua -\ntion of LLMs than previous benchmarks allowed.\n• Chatdoctor [221]: Chatdoctor improves the performance and relevance of responses \nin medical conversational systems. To achieve this, the model was fine-tuned using \na substantial dataset of 100,000 real-world patient-physician conversations sourced \nfrom online medical consultations. This approach ensures that the model not only \nunderstands medical terminology and procedures but also grasps the nuances of \npatient interactions and inquiries.\n• Taiyi [222]: Taiyi highlights the limitations of existing fine-tuned biomedical LLMs, \nwhich are predominantly monolingual and focused on question answering and con -\nversation tasks within the biomedical field. Taiyi, by contrast, is designed to enhance \nperformance across a broader spectrum of NLP applications, including entity extrac-\ntion, relation extraction, and information retrieval, catering to both English and non-\nEnglish texts. The development and evaluation of Taiyi involve rigorous fine-tuning \nprocesses that adjust the model to grasp the nuances and specific terminology used \nin various biomedical contexts, significantly improving its utility and applicability \nin a global healthcare context. This model represents a substantial advancement in \nthe field of biomedical NLP by supporting multilingual capabilities and addressing \nthe critical need for diverse language processing in medical research and healthcare \ndelivery.\nBiomedical LLMs like BioBert, MedBert, and ClinicalBERT have been developed to \nenhance tasks such as named entity recognition, relation extraction, and patient out -\ncome prediction by training on specialized datasets like PubMed and MIMIC-III. While \nthese models excel in capturing deep semantic relationships within specific domains, \nthey often struggle with generalizability outside their specialized fields and require \nrigorous ongoing updates. Models like COVID-twitter-BERT, which focus on spe -\ncific events or datasets, face challenges in maintaining relevance over time due to the \ndynamic nature of their data sources. Innovations like MedGPT and ClinicalGPT show \npromise in clinical settings, yet they must navigate significant challenges related to data \nprivacy and the need for extensive, diverse training data to ensure accuracy and util -\nity across varying medical scenarios. Furthermore, approaches like MultiMedQA and \nTaiyi aim to broaden the applicability of biomedical LLMs across different languages and \nmedical contexts, yet they must balance the breadth of language coverage with the depth \nof medical understanding to be truly effective in global healthcare applications.\nBiomedical healthcare on vision language models\nThis section elaborates on the training of vision language models for biomedical imag -\ning, and their practical applications in the biomedical healthcare sector.\nHow to train vision language models for biomedical imaging\nDeep neural networks demonstrate outstanding performance in various vision tasks, \nincluding image classification, object detection, and instance segmentation. A key to \nPage 36 of 54Li et al. BioData Mining            (2025) 18:2 \nthis success in the foundation model era is the concept of pre-training, which, unlike in \nNLP where it usually involves language models, traditionally meant training on exten -\nsive labeled image datasets like ImageNet [48]. More recently, diverse learning methods \nhave been introduced to overcome limitations of conventional supervised learning, such \nas generalization errors and spurious correlations. We examine several methodologies \nsuitable for imaging applications as follows:\n• Unsupervised Pre-training: Unsupervised pre-training leverages large volumes of \nunlabeled image data to learn rich feature representations without the guidance of \nexplicit annotations. Techniques such as autoencoders [223] and generative adver -\nsarial networks (GANs) [224] train models to generate or reconstruct images, ena -\nbling them to capture the underlying data distributions and learn complex patterns \nwithin the visual inputs. This approach is particularly useful in domains where \nlabeled data is scarce or expensive to obtain.\n• Contrastive Self-supervised Learning: Contrastive self-supervised learning tech -\nniques [225–227] train models to differentiate between various modifications of a \ngiven input image, such as determining whether two images are rotated versions of \neach other or entirely distinct. This method enables the model to develop features \napplicable to diverse vision tasks, including object detection and semantic segmenta -\ntion.\n• Masked Self-supervised Learning: Drawing inspiration from BERT’s approach in NLP , \nmasked self-supervised learning [228–230] is gaining popularity in computer vision. \nThis generative pre-training method trains models to reconstruct images from par -\ntially obscured inputs, aiding in understanding the underlying structure of visual \ndata.\n• Contrastive Language-image Pre-training: An innovative method, contrastive lan -\nguage-image pre-training [50] (CLIP), involves training a vision model using diverse \nimage-text datasets. The model learns to match images with corresponding texts \nwithin a mini-batch through contrastive learning. CLIP shows impressive zero-shot \ncapabilities, performing on par with traditional models like ResNet [35] on ImageNet \nwithout task-specific training. Text descriptions enhance understanding of the visual \ncontent, facilitating the model’s comprehension of visual elements and their interre -\nlations, which is essential for effective learning.\n• Instructed fine-tuning: Instructed fine-tuning involves explicitly guiding the model \nduring the fine-tuning process with task-specific instructions. This method builds \nupon the foundation established during pre-training by aligning the model’s learning \nobjectives closely with the nuances of the target task [50]. For example, in biomedi -\ncal imaging, models can be instructed to identify specific medical conditions from \nimages using detailed descriptions of symptoms or expected imaging features. This \napproach helps the model to focus on relevant aspects of the data, enhancing its per -\nformance on specialized tasks such as diagnosing diseases from medical scans.\nNote that a significant challenge in harnessing FMs for vision-language tasks lies in \novercoming the “task gap” and the “domain gap” . The task gap refers to the differences \nbetween the generic meta-tasks used in FMs, such as masked language modeling in \nPage 37 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nBERT or causal language modeling in GPT, and the specialized requirements of down -\nstream vision-language tasks, such as medical image annotation or diagnostic interpre -\ntation. The domain gap further highlights the disparity between the general training \ncorpora used, and the highly specialized datasets needed for tasks in specific fields like \nbiomedicine. To effectively deploy a pre-trained language model in vision-language \napplications within a specific domain, it is crucial to undertake both domain and task \nadaptations [182, 231–233]. Domain adaptation involves additional training of a model-\noriginally pre-trained on broad, general datasets within a targeted domain, such as \nbiomedicine. This step ensures that the model becomes attuned to the specific termi -\nnologies and data types characteristic of the domain.\nPractical VLMs in biomedical healthcare\nBiomedical vision-and-language models have largely been shaped by influential self-\nsupervised pre-training techniques, such as SimCLR [225] in computer vision and BERT \n[207] in natural language processing. These foundational approaches have paved the way \nfor the adoption of advanced text-to-image diffusion models [27, 51, 234] in the medical \nfield [235, 236], enhancing tasks ranging from diagnostic imaging to patient interaction. \nThis subsection provides a detailed overview of the existing vision-and-language mod -\nels (VLMs) within the biomedical sector and elucidates their functionalities. In this sur -\nvey, VLMs in the biomedical healthcare sector are categorized into three primary types: \ndual-encoder, fusion encoder, and hierarchical structures. Each model type offers dis -\ntinct advantages and limitations, tailored to specific application needs within the health -\ncare context.\nDual-Encoder Models process visual and textual inputs independently through sepa -\nrate encoders before merging the resulting vectors for final task execution. This archi -\ntecture is particularly effective for tasks that require robust single-modal or crossmodal \nrepresentation, such as image classification, image captioning, and cross-modal retrieval. \nHowever, the dual-encoder approach may fall short in fully capturing the intricate inter-\nplays between visual and linguistic elements, which can limit its effectiveness in more \ncomplex multimodal tasks. Fusion-encoder models integrate visual and linguistic data \nearly in the processing pipeline, utilizing a single encoder to manage both modalities. \nThis method facilitates the capture of complex interactions between text and image, \nproving advantageous for tasks that demand a deep multimodal understanding, such \nas visual question answering and complex diagnostic reasoning. While fusion encod -\ners excel in multimodal integration, they may encounter challenges in scenarios where a \nclear distinction between modalities is necessary.\nBesides the dual-encoder and fusion-encoder models, the field also explores innova -\ntive biomedical FMs that combine vision and language, such as hierarchical encoder \nalignment [237, 238] and medical text-to-image diffusion models [234, 239]. Hierarchi-\ncal alignment constructs input pyramids on both visual and linguistic sides, enhancing \nthe model’s ability to match features across modalities at multiple abstraction levels, \nwhich not only improves feature correspondence and model generalization but also \noptimizes the learning process, making it more efficient and adaptable to complex tasks \nlike medical diagnosis from imaging and textual data. Such structured FMs offers sig -\nnificant advantages in terms of computational efficiency, robustness, and scalability, \nPage 38 of 54Li et al. BioData Mining            (2025) 18:2 \ndemonstrating potential for broad applications, especially in the biomedical field. Diffu -\nsion models are generative models inspired by non-equilibrium thermodynamics. They \noperate by defining a Markov chain of diffusion steps that gradually add random noise to \ndata. The model then learns to reverse this diffusion process, reconstructing the desired \ndata samples from the noise. This approach is particularly powerful in medical applica -\ntions where generating high-fidelity images from textual descriptions can assist in diag -\nnostic visualizations and treatment planning.\nWe summarize recent significant developments in adapting vision-language models \nfor biomedical healthcare as follows, where the model type, encoder details, training \ncorpora, and released dates are detailed at Table 3.\n• ConVIRT [253]: ConVIRT utilizes contrastive learning techniques to simultaneously \ntrain ResNet and BERT encoders on paired image and text data. This approach signif-\nicantly improves the model’s performance in image classification tasks by effectively \nreducing the dependency on large volumes of labeled data. By optimizing feature \nextraction and enhancing the semantic alignment between images and their textual \ndescriptions, ConVIRT enables more accurate and efficient classification, making it \nparticularly useful in scenarios where annotated datasets are limited.\n• GLoRIA [254]: GLoRIA advances the field by employing both global and local con -\ntrastive learning strategies to finely align words in radiology reports with corre -\nsponding sub-regions within images. This method enhances local representation \nlearning, allowing for more precise identification and classification of localized fea -\ntures in medical images. Such detailed alignment improves diagnostic accuracy and \naids in the development of more sophisticated automated radiology analysis tools.\n• MedCLIP [20]: MedCLIP leverages the innovative architecture of the CLIP model, \nspecifically tailored for the medical domain. By utilizing pre-computed matching \nTable 3 Overview of vision‑language models in biomedical healthcare\nModel Name Type Image Encoder Text Encoder Training Corpora Release Date\nConVIRT Dual ResNet ClinicalBERT MIMIC‑CXR 2022\nGLoRIA Dual ResNet BioClinicalBERT Chexpert [240] 2021\nMedCLIP Dual ResNet/ViT BioClinicalBERT Chexpert, MIMIC‑CXR 2022\nCheXZero Dual CLIP‑Image CLIP‑Text Chest X‑rays 2022\nLoVT Dual ResNet ClinicalBERT MIMIC‑CXR 2022\nAdapted VLMs Hierarchical Diffusion, VAE [241] Bert, CLIP Chexpert, MIMIC‑CXR 2022\nVisualBERT Fusion Varies BERT MIMIC‑CXR 2020\nMedViLL Fusion ResNet BERT MIMIC‑CXR 2022\nARL Fusion CLIP‑Image RoBERTa [242] MedICaT [243], MIMIC‑CXR, \nROCO[244]\n2022\nLViT Fusion ViT BERT QaTa‑COV19 [245], \nMoNuSeg [246]\n2023\nRoentGen Hierarchical Diffusion CLIP‑Text MIMIC‑CXR 2022\nCLIPSyntel Dual CLIP GPT‑3.5 MMQS [247] 2024\nMed‑unic Dual ResNet/ViT CXR‑BERT [248] MIMIC‑CXR, PadChest [249] 2024\nEchoCLIP Dual ConvNeXt [250] CLIP‑Text Echocardiogram videos 2024\nLlava‑med Fusion Llava [8] Llava PubMed [251], PMC‑15M \n[252]\n2024\nPage 39 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nscores, MedCLIP enhances the alignment between medical images and their cor -\nresponding textual descriptions. This capability facilitates effective zero-shot learn -\ning, allowing MedCLIP to accurately classify medical conditions without the need \nfor extensive fine-tuning on large annotated datasets. The model’s ability to directly \napply learned representations from diverse medical contexts makes it a valuable tool \nfor rapid and efficient disease diagnosis, particularly in environments where labeled \nmedical data is scarce.\n• CheXZero [255]: CheXZero is another adaptation of the CLIP model, focused on \nzero-shot learning for medical imaging, specifically in chest radiography. Unlike \ntraditional models that require detailed annotations for each new disease classifica -\ntion task, CheXZero applies the powerful zero-shot capabilities of the CLIP model \nto accurately identify pathologies in chest X-rays without additional model training. \nThis approach is particularly beneficial for rapidly evolving medical scenarios, such \nas new disease outbreaks or rare conditions, where the availability of comprehen -\nsive labeled datasets might be limited. CheXZero’s innovative use of CLIP for direct \napplication in medical diagnosis demonstrates its potential to significantly streamline \ndiagnostic processes in healthcare settings.\n• LoVT [256]: LoVT specifically targets localized medical imaging tasks by implement-\ning a local contrastive loss that aligns representations of sentences or specific image \nregions. This alignment is crucial for tasks that require detailed understanding of \nsmall, localized anatomical structures or pathological features, enhancing the mod -\nel’s accuracy in specialized medical imaging applications.\n• Adapting Pretrained Vision-Language Foundational Models to Medical Imaging \nDomains [236]: This study explores the effectiveness of adapting general vision-\nlanguage models to the medical imaging domain. It demonstrates how foundational \nmodels, originally designed for broad applications, can be fine-tuned to meet the \nspecific needs of medical diagnostics and research, thus broadening their applicabil -\nity and improving performance in specialized tasks.\n• VisualBERT [257]: This study focuses on adapting general-domain vision-language \nmodels, such as LXMERT and VisualBERT, for the integration of medical images and \ntexts. The effectiveness of these adapted models in disease classification showcases \ntheir potential in clinical settings, where they can support diagnostic processes and \nenhance the accuracy of medical assessments.\n• MedViLL [258]: MedViLL enhances the multimodal interaction between medical \nimages and associated textual data through an innovative vision-language model \nframework by incorporating extensive medical knowledge and utilizing tailored \nmasking schemes, MedViLL is specifically designed to improve understanding and \ngeneration tasks within the medical field, which excels in synthesizing comprehen -\nsive medical reports and generating detailed medical annotations, crucial for assist -\ning healthcare professionals in making informed decisions. The approach to integrat-\ning complex medical datasets ensures a deeper contextual understanding and a more \nnuanced interpretation of both visual and textual medical data.\n• ARL [259]: ARL (Align and Reasoning Language model) introduces a unique align -\nment strategy that specifically targets the challenges of medical imaging and text \nanalysis. By aligning sentence or image region representations through a localized \nPage 40 of 54Li et al. BioData Mining            (2025) 18:2 \ncontrastive loss, ARL effectively bridges the gap between visual features and their \ncorresponding textual annotations. This model is particularly adept at tasks that \nrequire precise localization of medical findings within images, supporting detailed \ndiagnostic processes. ARL’s focus on enhancing the correlation between detailed \nimage regions and descriptive text makes it an invaluable tool for advanced medical \nimaging applications where accuracy and detail are paramount.\n• LViT [260]: LViT leverages medical text annotations to significantly improve seg -\nmentation results, particularly in semi-supervised settings where labeled data may \nbe scarce. By integrating rich textual information, LViT enhances its understanding \nof medical imagery, leading to more accurate segmentation and analysis of medical \nscans.\n• RoentGen [235]: RoentGen introduces a pioneering approach by applying text-to-\nimage diffusion models to medical imaging. This novel methodology holds prom -\nise for generating detailed and accurate medical images from textual descriptions, \npotentially revolutionizing the way medical imagery is produced and understood.\n• CLIPSyntel [247]: CLIPSyntel represents a synergistic application of CLIP and large \nlanguage models to address the challenge of multimodal question summarization in \nhealthcare. This model harnesses the strengths of both visual and textual data pro -\ncessing to provide concise and relevant summaries of complex medical inquiries, \naiding healthcare professionals CLIP and LLM Synergy for Multimodal Question \nSummarization in Healthcare [261].\n• Med-unic [262]: Med-unic presents an approach to enhance the performance of \nmedical vision-language pre-training models across different languages. The authors \nfocus on reducing bias in these models, which often perform better in languages with \nabundant training data (like English) compared to languages with less data. They \nintroduce a unified framework that integrates multilingual textual features and visual \ncontent effectively. Their method involves using a debiasing technique that ensures \nmore equitable learning from visual and textual data across various languages. This \nis achieved by carefully balancing the dataset and incorporating cross-lingual adapta-\ntion [263] techniques to improve model performance uniformly across different lin -\nguistic contexts.\n• EchoCLIP [264]: EchoCLIP is a specialized vision-language foundation model \ndesigned to improve echocardiography interpretation. EchoCLIP leverages the rela -\ntionship between cardiac ultrasound images and expert cardiologist interpretations \nacross diverse patient groups and diagnostic scenarios. The development of this \nmodel addresses the critical challenge of limited availability of annotated clinical \ndata in cardiac imaging. By training on over one million cardiac ultrasound images, \nEchoCLIP aims to enhance the accuracy and efficiency of echocardiogram, offering \na robust tool for cardiac diagnostics that can adapt to various clinical conditions and \nimaging indications.\n• Llava-med [265]: Llava-med presents a novel approach to training a vision-language \nconversational assistant tailored for the biomedical field. The study introduces a cost-\nefficient method for rapidly developing a multimodal conversational AI that can \nunderstand and discuss biomedical images alongside textual data. Unlike previous \nmodels that rely extensively on large-scale image-text pairs from general domains, \nPage 41 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nLlava-med is trained specifically with biomedical data to better address the unique \nneeds of the medical community.\nVLMs in the biomedical domain offer diverse applications but also face certain limita -\ntions. Models like ConVIRT and MedCLIP leverage contrastive and zero-shot learning \nto improve semantic alignment between medical images and texts, reducing reliance on \nextensive labeled datasets and enhancing diagnostic accuracy. However, these models \nmay struggle with generalization outside their training specifics and have limited abil -\nity for continual learning. GLoRIA and ARL focus on fine-grained alignment of medi -\ncal data, improving localized feature identification but potentially lacking in broader \napplication flexibility. Models such as CheXZero and RoentGen introduce innovative \napproaches to medical imaging by applying zero-shot learning and text-to-image dif -\nfusion models, streamlining diagnostic processes and even generating medical images \nfrom textual descriptions. Yet, these models do not fully address the varying complexi -\nties of medical conditions across diverse datasets. EchoCLIP and Llava-med exemplify \nspecialized applications, targeting cardiac imaging and conversational biomedical AI, \nrespectively, but must overcome challenges like limited annotated data and the need for \nspecialized training to ensure widespread applicability.\nOpen challenges and opportunities in federated foundation biomedical \nresearch\nThe integration of AI technologies, particularly large pre-trained foundation mod -\nels in the biomedical field, presents a range of future challenges and opportunities that \nmust be tackled to unlock their full potential. This section delves into the key issues \nand potential avenues for progress concerning the application of federated foundation \nmodels in biomedical research. It underscores the need for robust solutions that ensure \nprivacy, enhance model generalizability, improve computational efficiency, and address \nregulatory and ethical considerations. As we explore these challenges, we also highlight \npromising strategies that may pave the way for more effective and equitable AI-driven \nhealthcare solutions.\nChallenges of foundation models in biomedical healthcare\nResearch on foundation models in the biomedical healthcare domain presents several \nchallenges and directions for future exploration:\n• Data Privacy and Security: The primary challenge in foundation model, especially \nin healthcare, revolves around maintaining patient confidentiality and adhering to \nstringent data protection regulations like HIPAA [266] in the U.S and GDPR [17] in \nEurope. Future research needs to focus on developing robust encryption methods \nand privacy-preserving algorithms that allow for the secure sharing of insights with -\nout exposing sensitive patient data [267, 268].\n• Model Generalization across Diverse Datasets: FMs involve training models on \nhighly heterogeneous data sources, often leading to challenges in model gener -\nalization. Research should explore techniques to enhance the generalizability of \nPage 42 of 54Li et al. BioData Mining            (2025) 18:2 \nfoundation models across diverse healthcare systems and varied patient demo -\ngraphics without compromising performance.\n• Scalability and Computational Efficiency: The computational demand for training \nlarge-scale LLMs and VLMs is significant. Optimizing resource allocation, reduc -\ning communication overhead, and proposing efficient model updating mecha -\nnisms are crucial areas for future development to ensure scalability and practical -\nity in real-world healthcare settings.\n• Bias and Fairness: Ensuring that foundation models do not perpetuate or amplify \nbiases present in downstream tasks is critical [26, 269, 270], especially under the \nbiomedical domain, where the targeted problem for each patient can be narrow. \nFuture research should include developing methodologies for bias detection and \nmitigation in model training and deployment phases. This also involves designing \nfair algorithms that provide equitable healthcare outcomes across different popu -\nlations [271, 272].\n• Interoperability and Standardization: There is a need for standardized protocols \nto ensure interoperability among different healthcare systems participating in \nfoundation model learning.\n• Personalization: Medical treatments and diagnostics often require high degrees \nof personalization. AI models must be capable of adapting to individual patient \nneeds and conditions, which pose challenges in model design and data utilization \nwithout compromising generalizability.\n• Scaling:  Deploying AI solutions on a large scale, particularly in diverse healthcare \nsettings, presents logistical and computational challenges. Scalability involves not \nonly the expansion of AI systems to handle larger datasets but also ensuring these \nsystems are accessible across different regions and healthcare infrastructures.\n• Biomedical Requirements of Accuracy: Biomedical applications demand extremely \nhigh levels of accuracy and reliability. AI models used in diagnostics or treat -\nment recommendation must meet rigorous standards to prevent errors that could \nadversely affect patient health.\n• Robustness to Adversarial Attacks: As the applications of a foundation model in \nbiomedical scenarios can be distributed, they are susceptible to various types of \nadversarial attacks that can compromise model integrity. Enhancing the robust -\nness of foundation models against such attacks, and ensuring secure and reliable \nmodel performance, are a significant direction for ongoing research.\n• Regulatory and Ethical Considerations: As foundation models evolve, there will \nbe increased scrutiny from regulatory bodies concerning their use in clinical set -\ntings. Research must address these regulatory challenges by developing models \nthat are not only effective but also transparent and explainable to satisfy regula -\ntory requirements and maintain public trust.\n• Longitudinal Studies and Continuous Learning: Implementing models that \ncan adapt over time to new data and evolving biomedical conditions is crucial. \nResearch into continuous learning mechanisms that allow FMs to update with -\nout forgetting [273] previously learned knowledge while integrating new insights \nis essential for maintaining the relevance and accuracy of biomedical models.\nPage 43 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \nOpportunities in federated foundation models\nFederated learning offers a unique framework for addressing several challenges asso -\nciated with foundation models, particularly in the sensitive and data-intensive field of \nbiomedical healthcare. In this survey, we would like to highlight how federated learning \ncan help overcome the challenges of FMs and what opportunities it presents in both aca-\ndemic research and industrial applications:\n• Data Privacy and Security: Federated learning enables the collaborative training of \npredictive models by sharing model updates rather than raw data. Each participating \ninstitution retains its data locally, significantly minimizing the risk of data breaches \nand unauthorized access. This method is especially beneficial in healthcare, where \npatient data is highly sensitive and subject to strict privacy regulations. Note that \nupholding data privacy is crucial not only for complying with laws like GDPR and \nHIPAA but also for maintaining patient trust. Federated learning’s ability to train \nmodels without compromising data privacy helps healthcare organizations imple -\nment AI solutions without risking patient confidentiality or facing legal penalties. \nIncreased Model Robustness and Trustworthy: Trust in medical AI systems is \nessential for their acceptance by both medical professionals and patients. Systems \nknown for their reliability and backed by a transparent, accountable training process \nare more likely to be trusted and thus more widely adopted, which makes it crucial to \nfurther investigate the trustworthy federated foundation models.\n• Bias and Fairness: Addressing bias and ensuring fairness is critical for Federated \nFoundation Models, particularly in the healthcare domain where imbalanced data or \nalgorithmic biases can lead to unequal patient outcomes. FL frameworks must con -\nsider demographic diversity and institutional disparities in data availability to create \nequitable models. Techniques such as fairness-aware aggregation methods, adver -\nsarial training for debiasing, and reweighting of underrepresented data during local \ntraining can mitigate biases. Additionally, the use of synthetic data generation to bal-\nance class distributions can further improve trust and fairness in healthcare applica -\ntions.\n• Real-time Learning and Adaptation: In federated learning frameworks, models can \nbe updated continually as new data becomes available across the network. This \ndynamic learning process allows the models to adapt to emerging health trends or \nnew strains of diseases. The ability to update and adapt foundation models in real-\ntime is vital for keeping pace with the fast-evolving nature of diseases and treatments, \nensuring that healthcare providers have the most current tools at their disposal.\n• Collaborative Innovation: By aggregating insights from diverse healthcare environ -\nments and patient demographics, federated learning facilitates the development of \nmodels that perform well across different settings. This heterogeneous data input \nhelps the model learn more comprehensive patterns and reduces the risk of bias \ntowards any particular group or condition. Specifically, how to use federated learn -\ning to efficiently establish a cooperative ecosystem where different healthcare entities \ncan contribute to and benefit from shared AI advancements without compromising \ntheir data sovereignty is worth for real-world application, which can lead to more \nrapid development and refinement of AI technologies.\nPage 44 of 54Li et al. BioData Mining            (2025) 18:2 \n• Multimodality: Medical data are often multimodal, encompassing an extensive \narray of data types-including text, images, videos, databases, and molecular struc -\ntures-across various scales from molecules to populations [129, 274], and presented \nin both professional and/or lay language [275, 276]. While current self-supervised \nmodels excel within individual modalities, such as text [208], images [277], genes \n[197], and proteins [278], they typically lack the capability to integrate and learn from \nthese diverse sources simultaneously. To truly leverage the rich information available \nacross different modalities, there is an urgent need to develop models that can per -\nform both feature-level and semantic-level fusion. Successfully integrating these var -\nied data types could revolutionize how biomedical knowledge is unified and signifi -\ncantly accelerate discovery processes in biomedicine. Federated learning frameworks \ncan capture a richer and more nuanced understanding of patient conditions, which is \ncrucial for multimodal tasks like diagnosing complex diseases that may require cor -\nrelating symptoms, radiology images, and genetic information.\n• Synthetic Data Generation for Further Training: Federated learning can facilitate the \ngeneration of synthetic training data, which helps address the scarcity of annotated \ndatasets, particularly in specialized medical fields. By learning from diverse sources, \nfederated models can generate new, synthetic examples that preserve the statistical \nproperties of real data without revealing any individual patient’s information. This \nsynthetic data can then be used to further train and refine FMs across the network. \nThe generation of synthetic data is a critical solution for overcoming data limitations \nin biomedical research, where privacy concerns and the rarity of certain conditions \ncan significantly constrain the availability of training data.\nConclusions\nThis survey has delved into the transformative potential of foundation models and fed -\nerated learning within the biomedical healthcare domains. Foundation models repre -\nsent a significant advancement in artificial intelligence, offering robust, adaptable tools \nthat can be fine-tuned for specific applications without constructing new models from \nscratch. These models, trained on expansive datasets, are capable of performing a wide \narray of tasks-from text generation to video analysis-that were previously beyond the \nreach of earlier AI systems. In the biomedical and healthcare sectors, where the efficacy \nof AI and the integrity of data privacy are crucial, foundation models play a pivotal role \nby enabling the extraction of valuable insights from constrained datasets. This survey \nhas highlighted the current applications of FMs in these sectors, particularly focusing on \nlarge language models and vision-language models.\nMeanwhile, Federated learning, characterized by its privacy-preserving and decentral -\nized approach, complements the capabilities of foundation models perfectly. By com -\nbining the robust, generalizable nature of FMs with the privacy-centric, decentralized \nattributes of federated learning, researchers can perform deep analyses using globally \npooled insights from locally held datasets. This synergy holds immense potential to meet \nthe specific needs of biomedical AI applications, offering scalable solutions that accom -\nmodate the continuous updating of foundation models with new, relevant data.\nAdditionally, this survey has outlined various challenges and opportunities that arise \nwith the adoption of federated foundation models in healthcare. Federated learning \nPage 45 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \naddresses critical issues such as data privacy, model generalization, scalability, and inher-\nent biases within AI models. By allowing multiple institutions to collaboratively train \nmodels while keeping their data localized, federated learning not only complies with \nstrict data privacy laws but also enhances the diversity and efficacy of medical AI appli -\ncations. Key areas where federated foundation models could notably impact biomedical \nresearch and practice include enhancing model robustness and fairness, enabling real-\ntime model updates and adaptations, and facilitating cross-institutional and interna -\ntional collaborations without compromising data security.\nAcknowledgements\nNot applicable.\nAuthors’ contributions\nXingyu Li wrote the main manuscript, Lu Peng and YuPing Wang revised the main manuscript, prepared figures and \ntables. All authors reviewed the manuscript.\nFunding\nThere was no external source of funding.\nData availability\nNo datasets were generated or analysed during the current study.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 19 May 2024   Accepted: 9 December 2024\nReferences\n 1. Jeblick K, Schachtner B, Dexl J, Mittermeier A, Stüber AT, Topalis J, et al. ChatGPT makes medicine easy to swallow: \nan exploratory case study on simplified radiology reports. Eur Radiol. 2023;34:1–9.\n 2. Touvron H, Lavril T, Izacard G, Martinet X, Lachaux MA, Lacroix T, et al. Llama: Open and efficient foundation lan‑\nguage models. arXiv preprint arXiv:2302.13971. 2023.\n 3. LeCun Y, Bengio Y, Hinton G. Deep learning. New York: IEEE Corporate Headquarters. 2015;521(7553):436–44.\n 4. Erhan D, Courville A, Bengio Y, Vincent P . Why does unsupervised pre‑training help deep learning? In: Proceedings \nof the thirteenth international conference on artificial intelligence and statistics. JMLR Workshop and Conference \nProceedings. Italy: Chia Laguna Resort; 2010. p. 201–8.\n 5. Caron M, Bojanowski P , Mairal J, Joulin A. Unsupervised pre‑training of image features on non‑curated data. In: \nProceedings of the IEEE/CVF International Conference on Computer Vision. 2019. pp. 2959–68.\n 6. Chen T, Frankle J, Chang S, Liu S, Zhang Y, Carbin M, et al. The lottery tickets hypothesis for supervised and self‑\nsupervised pre‑training in computer vision models. In: Proceedings of the IEEE/CVF conference on computer \nvision and pattern recognition. New York: IEEE Corporate Headquarters. 2021. p. 16306–16.\n 7. Wang X, Zhang R, Shen C, Kong T, Li L. Dense contrastive learning for self‑supervised visual pre‑training. In: \nProceedings of the IEEE/CVF conference on computer vision and pattern recognition. New York: IEEE Corporate \nHeadquarters. 2021. p. 3024–33.\n 8. Liu H, Li C, Wu Q, Lee YJ. Visual instruction tuning. Adv Neural Inf Process Sys. 2024;36.\n 9. Christiano PF, Leike J, Brown T, Martic M, Legg S, Amodei D. Deep reinforcement learning from human preferences. \nAdv Neural Inf Process Syst. 2017;30.\n 10. Park C, Took CC, Seong JK. Machine learning in biomedical engineering. Biomed Eng Lett. 2018;8:1–3.\n 11. Habehh H, Gohel S. Machine learning in healthcare. Curr Genomics. 2021;22(4):291.\n 12. Kaur D, Uslu S, Rittichier KJ, Durresi A. Trustworthy artificial intelligence: a review. ACM Comput Surv (CSUR). \n2022;55(2):1–38.\n 13. McMahan B, Moore E, Ramage D, Hampson S, Aguera y Arcas B. Communication‑efficient learning of deep net‑\nworks from decentralized data. In: Artificial intelligence and statistics. PMLR; 2017. pp. 1273–82.\n 14. Li X, Qu Z, Tang B, Lu Z. FedLGA: Toward System‑Heterogeneity of Federated Learning via Local Gradient Approxi‑\nmation. IEEE Trans Cybern. 2023.\nPage 46 of 54Li et al. BioData Mining            (2025) 18:2 \n 15. Li Q, Wen Z, Wu Z, Hu S, Wang N, Li Y, et al. A survey on federated learning systems: Vision, hype and reality for \ndata privacy and protection. IEEE Trans Knowl Data Eng. 2021;35(4):3347–66.\n 16. Wei K, Li J, Ding M, Ma C, Yang HH, Farokhi F, et al. Federated learning with differential privacy: Algorithms and \nperformance analysis. IEEE Trans Inf Forensic Secur. 2020;15:3454–69.\n 17. Li H, Yu L, He W. The impact of GDPR on global technology development. Hershey: Taylor & Francis; 2019.\n 18. of Medicine I. Beyond the HIPAA Privacy Rule: Enhancing Privacy, Improving Health Through Research. Nass \nSJ, Levit LA, Gostin LO, editors. Washington, DC: The National Academies Press; 2009. https:// doi. org/ 10. 17226/  \n12458.\n 19. Lu W, Xixu H, Wang J, Xie X. Fedclip: fast generalization and personalization for clip in federated learning. In: \nICLR 2023 Workshop on Trustworthy and Reliable Large ‑Scale Machine Learning Models. Appleton: Interna‑\ntional Conference on Learning Representations. 2023.\n 20. Wang Z, Wu Z, Agarwal D, Sun J. MedCLIP: Contrastive Learning from Unpaired Medical Images and Text. In: \nGoldberg Y, Kozareva Z, Zhang Y, editors. Proceedings of the 2022 Conference on Empirical Methods in Natural \nLanguage Processing. Abu Dhabi: Association for Computational Linguistics; 2022. pp. 3876–87. https:// doi.  \norg/ 10. 18653/ v1/ 2022. emnlp‑ main. 256.\n 21. Wu X, Liang Z, Wang J. Fedmed: A federated learning framework for language modeling. Sensors. \n2020;20(14):4048.\n 22. Kraljevic Z, Shek A, Bean D, Bendayan R, Teo J, Dobson R. MedGPT: Medical concept prediction from clinical \nnarratives. arXiv preprint arXiv:2107.03134. 2021.\n 23. Subramanian AAV, Venugopal JP . A deep ensemble network model for classifying and predicting breast can‑\ncer. Comput Intell. 2023;39(2):258–82.\n 24. Rasmy L, Xiang Y, Xie Z, Tao C, Zhi D. Med‑BERT: pretrained contextualized embeddings on large ‑scale struc‑\ntured electronic health records for disease prediction. NPJ Digit Med. 2021;4(1):86.\n 25. Bommasani R, Hudson DA, Adeli E, Altman R, Arora S, von Arx S, et al. On the opportunities and risks of foun‑\ndation models. arXiv preprint arXiv:2108.07258. 2021.\n 26. Achiam J, Adler S, Agarwal S, Ahmad L, Akkaya I, Aleman FL, et al. Gpt ‑4 technical report. arXiv pre ‑\nprint arXiv:2303.08774. 2023.\n 27. Ramesh A, Dhariwal P , Nichol A, Chu C, Chen M. Hierarchical text ‑conditional image generation with clip \nlatents. arXiv preprint arXiv:2204.06125. 2022;1(2):3.\n 28. Hochreiter S, Schmidhuber J. Long short ‑term memory. Neural Comput. 1997;9(8):1735–80.\n 29. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. Adv Neural Inf \nProcess Syst. 2017;30.\n 30. Chowdhary K, Chowdhary K. Natural language processing. Fundam Artif Intell. 2020;603–49. https:// dl. acm.  \norg/ doi/ 10. 5555/ 10741 00. 10746 30.\n 31. LeCun Y, Bottou L, Bengio Y, Haffner P . Gradient‑based learning applied to document recognition. Proc IEEE. \n1998;86(11):2278–324.\n 32. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, et al. An Image is Worth 16x16 \nWords: Transformers for Image Recognition at Scale. In: International Conference on Learning Representa‑\ntions. Appleton: International Conference on Learning Representations; 2020.\n 33. Kenton JDMWC, Toutanova LK. BERT: Pre ‑training of Deep Bidirectional Transformers for Language Under ‑\nstanding. In: Proceedings of NAACL ‑HLT. Stroudsburg: Association for Computational Linguistics (ACL). 2019. p. \n4171–86.\n 34. Raffel C, Shazeer N, Roberts A, Lee K, Narang S, Matena M, et al. Exploring the limits of transfer learning with a uni‑\nfied text‑to‑text transformer. J Mach Learn Res. 2020;21(140):1–67.\n 35. He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In: Proceedings of the IEEE conference \non computer vision and pattern recognition. New York: IEEE Corporate Headquarters. 2016. p. 770–8.\n 36. Ba JL, Kiros JR, Hinton GE. Layer Normalization. Stat. 2016;1050:21.\n 37. Shiv V, Quirk C. Novel positional encodings to enable tree‑based transformers. Adv Neural Inf Process Syst. \n2019;32.\n 38. Chen Y. Convolutional neural network for sentence classification. University of Waterloo; 2015.\n 39. Mikolov T. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781. 2013;3781.\n 40. Pennington J, Socher R, Manning CD. Glove: Global vectors for word representation. In: Proceedings of the 2014 \nconference on empirical methods in natural language processing (EMNLP). Stroudsburg: Association for Compu‑\ntational Linguistics (ACL). 2014. p. 1532–43.\n 41. Peters ME, Neumann M, Iyyer M, Gardner M, Clark C, Lee K, et al. Deep Contextualized Word Representations. \nIn: Walker M, Ji H, Stent A, editors. Proceedings of the 2018 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language Technologies. vol. 1 (Long Papers). New Orleans: \nAssociation for Computational Linguistics; 2018. pp. 2227–37. https:// doi. org/ 10. 18653/ v1/ N18‑ 1202.\n 42. Radford A, Wu J, Child R, Luan D, Amodei D, Sutskever I, et al. Language models are unsupervised multitask learn‑\ners. OpenAI Blog. 2019;1(8):9.\n 43. Bengio Y, Ducharme R, Vincent P . A neural probabilistic language model. Adv Neural Inf Process Syst. 2000;13.\n 44. Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations of words and phrases and their \ncompositionality. Adv Neural Inf Process Syst. 2013;26.\n 45. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P , et al. Language models are few‑shot learners. Adv \nNeural Inf Process Syst. 2020;33:1877–901.\n 46. Chowdhery A, Narang S, Devlin J, Bosma M, Mishra G, Roberts A, et al. Palm: Scaling language modeling with \npathways. J Mach Learn Res. 2023;24(240):1–113.\n 47. Manyika J, Hsiao S. An overview of Bard: an early experiment with generative AI. AI Google Static Doc. 2023;2.\n 48. Deng J, Dong W, Socher R, Li LJ, Li K, Fei‑Fei L. Imagenet: A large‑scale hierarchical image database. In: 2009 IEEE \nconference on computer vision and pattern recognition. Washington, DC: IEEE Computer Society. 2009. pp. \n248–55.\nPage 47 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \n 49. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, et al. An Image is Worth 16x16 Words: \nTransformers for Image Recognition at Scale. In: International Conference on Learning Representations. 2021. \nhttps:// openr eview. net/ forum? id= YicbF dNTTy. Accessed 12 Jan 2021.\n 50. Radford A, Kim JW, Hallacy C, Ramesh A, Goh G, Agarwal S, et al. Learning transferable visual models from natural \nlanguage supervision. In: International conference on machine learning. Somerville: Microtome Publishing. 2021. \np. 8748–63.\n 51. Rombach R, Blattmann A, Lorenz D, Esser P , Ommer B. High‑resolution image synthesis with latent diffusion \nmodels. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. New York: IEEE \nCorporate Headquarters. 2022. p. 10684–95.\n 52. Kirillov A, Mintun E, Ravi N, Mao H, Rolland C, Gustafson L, et al. Segment anything. In: Proceedings of the IEEE/CVF \nInternational Conference on Computer Vision. 2023. p. 4015–26.\n 53. Wójcik MA. Foundation Models in Healthcare: Opportunities, Biases and Regulatory Prospects in Europe. In: Inter‑\nnational Conference on Electronic Government and the Information Systems Perspective. Heidelberg: Springer \nNature. 2022. pp. 32–46.\n 54. Yu KH, Beam AL, Kohane IS. Artificial intelligence in healthcare. Nat Biomed Eng. 2018;2(10):719–31.\n 55. Bender EM, Gebru T, McMillan‑Major A, Shmitchell S. On the dangers of stochastic parrots: Can language models \nbe too big? In: Proceedings of the 2021 ACM conference on fairness, accountability, and transparency. New York: \nAssociation for Computing Machinery (ACM). 2021. p. 610–23.\n 56. Osman Andersen T, Nunes F, Wilcox L, Kaziunas E, Matthiesen S, Magrabi F. Realizing AI in healthcare: challenges \nappearing in the wild. In: Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Sys‑\ntems. New York: Association for Computing Machinery (ACM). 2021. p. 1–5.\n 57. Liao QV, Zhang Y, Luss R, Doshi‑Velez F, Dhurandhar A. Connecting algorithmic research and usage contexts: a \nperspective of contextualized evaluation for explainable AI. In: Proceedings of the AAAI Conference on Human \nComputation and Crowdsourcing.Association for the Advancement of Artificial Intelligence (AAAI), vol. 10. Wash‑\nington, DC: Association for the Advancement of Artificial Intelligence (AAAI). 2022. p. 147–59.\n 58. Zając HD, Li D, Dai X, Carlsen JF, Kensing F, Andersen TO. Clinician‑facing AI in the Wild: Taking Stock of the Socio‑\ntechnical Challenges and Opportunities for HCI. ACM Trans Comput Hum Interact. 2023;30(2):1–39.\n 59. Li W, Milletarì F, Xu D, Rieke N, Hancox J, Zhu W, et al. Privacy‑preserving federated brain tumour segmentation. In: \nMachine Learning in Medical Imaging: 10th International Workshop, MLMI 2019, Held in Conjunction with MICCAI \n2019, Shenzhen, China, October 13, 2019, Proceedings 10. Heidelberg: Springer Nature. 2019. pp. 133–41.\n 60. Kairouz P , McMahan HB, Avent B, Bellet A, Bennis M, Nitin Bhagoji A, et al. Advances and Open Problems in Feder‑\nated Learning. Found Trends Mach Learn. 2021;14(1–2):1–210. https:// doi. org/ 10. 1561/ 22000 00083.\n 61. Li T, Sahu AK, Talwalkar A, Smith V. Federated learning: Challenges, methods, and future directions. IEEE Signal \nProcess Mag. 2020;37(3):50–60.\n 62. Karimireddy SP , Kale S, Mohri M, Reddi S, Stich S, Suresh AT. Scaffold: Stochastic controlled averaging for federated \nlearning. In: International conference on machine learning. Somerville: Microtome Publishing. 2020. p. 5132–43.\n 63. Qu Z, Li X, Duan R, Liu Y, Tang B, Lu Z. Generalized federated learning via sharpness aware minimization. In: Inter‑\nnational Conference on Machine Learning. PMLR; 2022. pp. 18250–80.\n 64. Li X, Qu Z, Zhao S, Tang B, Lu Z, Liu Y. Lomar: A local defense against poisoning attack on federated learning. IEEE \nTrans Dependable Secure Comput. 2021;20(1):437–50.\n 65. Zhang J, Hua Y, Wang H, Song T, Xue Z, Ma R, et al. FedCP: Separating Feature Information for Personalized \nFederated Learning via Conditional Policy. In: Proceedings of the 29th ACM SIGKDD Conference on Knowledge \nDiscovery and Data Mining. New York: Association for Computing Machinery (ACM); 2023.\n 66. Beutel DJ, Topal T, Mathur A, Qiu X, Fernandez‑Marques J, Gao Y, et al. Flower: A Friendly Federated Learning \nResearch Framework. arXiv preprint arXiv:2007.14390. 2020.\n 67. He C, Li S, So J, Zeng X, Zhang M, Wang H, et al. Fedml: A research library and benchmark for federated machine \nlearning. arXiv preprint arXiv:2007.13518. 2020.\n 68. Liu Y, Fan T, Chen T, Xu Q, Yang Q. Fate: An industrial grade platform for collaborative learning with data protection. \nJ Mach Learn Res. 2021;22(1):10320–5.\n 69. Xie Y, Wang Z, Gao D, Chen D, Yao L, Kuang W, et al. FederatedScope: A Flexible Federated Learning Platform for \nHeterogeneity. Proc VLDB Endowment. 2023;16(5):1059–72.\n 70. Bonawitz K, Eichner H, Grieskamp W, Huba D, Ingerman A, Ivanov V, et al. Towards Federated Learning at Scale: \nSystem Design. In: Talwalkar A, Smith V, Zaharia M, editors. Proceedings of Machine Learning and Systems. vol. 1. \n2019. pp. 374–88.\n 71. Huba D, Nguyen J, Malik K, Zhu R, Rabbat M, Yousefpour A, et al. Papaya: Practical, private, and scalable federated \nlearning. Proc Mach Learn Syst. 2022;4:814–32.\n 72. Hays K. Elon Musk’s plan to charge for Twitter API access is unraveling — businessinsider.com. https:// www. busin \nessin sider. com/ elon‑ musk‑ plan‑ to‑ charge‑ for‑ twitt er‑ api‑ access‑ unrav eling‑ 2023‑5. Accessed 23 Feb 2024.\n 73. Fung B. Reddit sparks outrage after a popular app developer said it wants him to pay $20 million a year for data \naccess | CNN Business — cnn.com. https:// www. cnn. com/ 2023/ 06/ 01/ tech/ reddit‑ outra ge‑ data‑ access‑ charge/ \nindex. html. Accessed 23 Feb 2024.\n 74. Nast C. Stack Overflow Will Charge AI Giants for Training Data — wired.com. https:// www. wired. com/ story/ stack‑ \noverfl  ow‑ will‑ charge‑ ai‑ giants‑ for‑ train ing‑ data/. Accessed 23 Feb 2024.\n 75. Hadsell R, Rao D, Rusu AA, Pascanu R. Embracing change: Continual learning in deep neural networks. Trends \nCogn Sci. 2020;24(12):1028–40.\n 76. Li X, Tang B, Li H. AdaER: An adaptive experience replay approach for continual lifelong learning. Neurocomputing. \n2024;572:127204.\n 77. Yoon J, Jeong W, Lee G, Yang E, Hwang SJ. Federated continual learning with weighted inter‑client transfer. In: \nInternational Conference on Machine Learning. Somerville: Microtome Publishing. 2021. pp. 12073–86.\n 78. Li Y, Wang H, Jin Q, Hu J, Chemerys P , Fu Y, et al. Snapfusion: Text‑to‑image diffusion model on mobile devices \nwithin two seconds. Adv Neural Inf Process Syst. 2024;36.\nPage 48 of 54Li et al. BioData Mining            (2025) 18:2 \n 79. Shome D, Kar T. FedAffect: Few‑shot federated learning for facial expression recognition. In: Proceedings of the \nIEEE/CVF international conference on computer vision. New York: IEEE Corporate Headquarters. 2021. p. 4168–75.\n 80. Gao D, Yao X, Yang Q. A survey on heterogeneous federated learning. arXiv preprint arXiv:2210.04505. 2022.\n 81. Lyu L, Yu J, Nandakumar K, Li Y, Ma X, Jin J, et al. Towards fair and privacy‑preserving federated deep models. IEEE \nTrans Parallel Distrib Syst. 2020;31(11):2524–41.\n 82. Chen C, Lyu L, Yu H, Chen G. Practical attribute reconstruction attack against federated learning. IEEE Trans Big \nData. 2022;10(6):851–63.\n 83. Yu S, Muñoz JP , Jannesari A. Federated Foundation Models: Privacy‑Preserving and Collaborative Learning for \nLarge Models. arXiv preprint arXiv:2305.11414. 2023.\n 84. Chen HY, Tu CH, Li Z, Shen HW, Chao WL. On the importance and applicability of pre‑training for federated learn‑\ning. In: The Eleventh International Conference on Learning Representations. Appleton: International Conference \non Learning Representations. 2022.\n 85. Tan Y, Long G, Ma J, Liu L, Zhou T, Jiang J. Federated learning from pre‑trained models: A contrastive learning \napproach. Adv Neural Inf Process Syst. 2022;35:19332–44.\n 86. Zhang T, Feng T, Alam S, Dimitriadis D, Lee S, Zhang M, et al. Gpt‑fl: Generative pre‑trained model‑assisted feder‑\nated learning. arXiv preprint arXiv:2306.02210. 2023.\n 87. Li D, Wang J. Fedmd: Heterogenous federated learning via model distillation. arXiv preprint arXiv:1910.03581. \n2019.\n 88. Guo T, Guo S, Wang J, Tang X, Xu W. Promptfl: Let federated participants cooperatively learn prompts instead of \nmodels‑federated learning in age of foundation model. IEEE Trans Mob Comput. 2023;23(5):5179–94.\n 89. Zhao H, Du W, Li F, Li P , Liu G. Reduce communication costs and preserve privacy: Prompt tuning method in feder‑\nated learning. arXiv preprint arXiv:2208.12268. 2022;1(2).\n 90. Qiu C, Li X, Mummadi CK, Ganesh MR, Li Z, Peng L, et al. Federated Text‑driven Prompt Generation for Vision‑Lan‑\nguage Models. In: The Twelfth International Conference on Learning Representations. 2024. https:// openr eview. \nnet/ forum? id= NW31g AylIm. Accessed 16 Jan 2024.\n 91. Crawford K. The atlas of AI: Power, politics, and the planetary costs of artificial intelligence. New Haven: Yale Uni‑\nversity Press; 2021.\n 92. Thieme A, Hanratty M, Lyons M, Palacios J, Marques RF, Morrison C, et al. Designing human‑centered AI for mental \nhealth: Developing clinically relevant applications for online CBT treatment. ACM Trans Comput Hum Interact. \n2023;30(2):1–50.\n 93. Chen IY, Pierson E, Rose S, Joshi S, Ferryman K, Ghassemi M. Ethical machine learning in healthcare. Ann Rev \nBiomed Data Sci. 2021;4:123–44.\n 94. Swensen SJ, Kaplan GS, Meyer GS, Nelson EC, Hunt GC, Pryor DB, et al. Controlling healthcare costs by removing \nwaste: what American doctors can do now. BMJ Qual Saf. 2011;20(6):534–7.\n 95. Van Hartskamp M, Consoli S, Verhaegh W, Petkovic M, Van de Stolpe A, et al. Artificial intelligence in clinical health \ncare applications. Interact J Med Res. 2019;8(2):e12100.\n 96. Keehan SP , Cuckler GA, Poisal JA, Sisko AM, Smith SD, Madison AJ, et al. National Health Expenditure Projections, \n2019–28: Expected Rebound In Prices Drives Rising Spending Growth: National health expenditure projections for \nthe period 2019–2028. Health Affairs. 2020;39(4):704–14.\n 97. Easwaran S, Venugopal JP , Subramanian AAV, Sundaram G, Naseeba B. A comprehensive learning based swarm \noptimization approach for feature selection in gene expression data. Heliyon. 2024;10(17). https:// www. scien cedir \nect. com/ scien ce/ artic le/ pii/ S2405 84402 41319 69.\n 98. Korngiebel DM, Mooney SD. Considering the possibilities and pitfalls of Generative Pre‑trained Transformer 3 \n(GPT‑3) in healthcare delivery. NPJ Digit Med. 2021;4(1):93.\n 99. Krumholz HM, Terry SF, Waldstreicher J. Data acquisition, curation, and use for a continuously learning health \nsystem. Jama. 2016;316(16):1669–70.\n 100. Suresh A, Udendhran R, Vimal S. Deep neural networks for multimodal imaging and biomedical applications. Her‑\nshey: IGI Global; 2020.\n 101. Ionescu D. Deep learning algorithms and big health care data in clinical natural language processing. Linguist \nPhilos Investig. 2020;19:86–92.\n 102. Ma’ayan A. Complex systems biology. J R Soc Interface. 2017;14(134):20170391.\n 103. Ramachandram D, Taylor GW. Deep multimodal learning: A survey on recent advances and trends. IEEE Signal \nProcess Mag. 2017;34(6):96–108.\n 104. Hall DL, Llinas J. An introduction to multisensor data fusion. Proc IEEE. 1997;85(1):6–23.\n 105. Castanedo F, et al. A review of data fusion techniques. Sci World J. 2013;2013.\n 106. Li Y, Wu FX, Ngom A. A review on machine learning principles for multi‑view biological data integration. Brief \nBioinforma. 2018;19(2):325–40.\n 107. Stahlschmidt SR, Ulfenborg B, Synnergren J. Multimodal deep learning for biomedical data fusion: a review. Brief \nBioinforma. 2022;23(2):bbab569.\n 108. Park C, Ha J, Park S. Prediction of Alzheimer’s disease based on deep neural network by integrating gene expres‑\nsion and DNA methylation dataset. Expert Syst Appl. 2020;140:112873.\n 109. Peng C, Zheng Y, Huang DS. Capsule network based modeling of multi‑omics data for discovery of breast cancer‑\nrelated genes. IEEE/ACM Trans Comput Biol Bioinforma. 2019;17(5):1605–12.\n 110. Bichindaritz I, Liu G, Bartlett C. Integrative survival analysis of breast cancer with gene expression and DNA meth‑\nylation data. Bioinformatics. 2021;37(17):2601–8.\n 111. Chaudhary K, Poirion OB, Lu L, Garmire LX. Deep learning‑based multi‑omics integration robustly predicts survival \nin liver cancer. Clin Cancer Res. 2018;24(6):1248–59.\n 112. Franco EF, Rana P , Cruz A, Calderon VV, Azevedo V, Ramos RT, et al. Performance comparison of deep learning \nautoencoders for cancer subtype detection using multi‑omics data. Cancers. 2021;13(9):2013.\n 113. Islam MM, Huang S, Ajwad R, Chi C, Wang Y, Hu P . An integrative deep learning framework for classifying molecular \nsubtypes of breast cancer. Comput Struct Biotechnol J. 2020;18:2185–99.\nPage 49 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \n 114. Albaradei S, Napolitano F, Thafar MA, Gojobori T, Essack M, Gao X. MetaCancer: A deep learning‑based \npan‑cancer metastasis prediction model developed using multi‑ omics data. Comput Struct Biotechnol J. \n2021;54(1):401–14.\n 115. Lee G, Nho K, Kang B, Sohn KA, Kim D. Predicting Alzheimer’s disease progression using multi‑modal deep \nlearning approach. Sci Rep. 2019;9(1):1952.\n 116. Suk HI, Lee SW, Shen D, Initiative ADN, et al. Hierarchical feature representation and multimodal fusion with \ndeep learning for AD/MCI diagnosis. NeuroImage. 2014;101:569–82.\n 117. Xu M, Ouyang L, Han L, Sun K, Yu T, Li Q, et al. Accurately differentiating between patients with COVID ‑19, \npatients with other viral infections, and healthy individuals: multimodal late fusion learning approach. J Med \nInternet Res. 2021;23(1):e25535.\n 118. Wang X, Liu M, Zhang Y, He S, Qin C, Li Y, et al. Deep fusion learning facilitates anatomical therapeutic chemi‑\ncal recognition in drug repurposing and discovery. Brief Bioinforma. 2021;22(6):bbab289.\n 119. Soto JT, Weston Hughes J, Sanchez PA, Perez M, Ouyang D, Ashley EA. Multimodal deep learning enhances \ndiagnostic precision in left ventricular hypertrophy. Eur Heart J Digit Health. 2022;3(3):380–9.\n 120. Sun D, Wang M, Li A. A multimodal deep neural network for human breast cancer prognosis prediction by \nintegrating multi‑ dimensional data. IEEE/ACM Trans Comput Biol Bioinforma. 2018;16(3):841–50.\n 121. Huang SC, Pareek A, Zamanian R, Banerjee I, Lungren MP . Multimodal fusion with deep neural networks for \nleveraging CT imaging and electronic health record: a case ‑study in pulmonary embolism detection. Sci Rep. \n2020;10(1):22147.\n 122. Hanney SR, Castle ‑Clarke S, Grant J, Guthrie S, Henshall C, Mestre ‑Ferrandiz J, et al. How long does biomedi‑\ncal research take? Studying the time taken between biomedical and health research and its translation into \nproducts, policy, and practice. Health Res Policy Syst. 2015;13(1):1–18.\n 123. Wouters OJ, McKee M, Luyten J. Estimated research and development investment needed to bring a new \nmedicine to market, 2009–2018. Jama. 2020;323(9):844–53.\n 124. Lalmuanawma S, Hussain J, Chhakchhuak L. Applications of machine learning and artificial intelligence for \nCovid‑19 (SARS‑ CoV‑2) pandemic: A review. Chaos Solitons Fractals. 2020;139:110059.\n 125. Kadurin A, Nikolenko S, Khrabrov K, Aliper A, Zhavoronkov A. druGAN: an advanced generative adversarial \nautoencoder model for de novo generation of new molecules with desired molecular properties in silico. Mol \nPharm. 2017;14(9):3098–104.\n 126. Harrer S, Shah P , Antony B, Hu J. Artificial intelligence for clinical trial design. Trends Pharmacol Sci. \n2019;40(8):577–91.\n 127. Lanckriet GR, De Bie T, Cristianini N, Jordan MI, Noble WS. A statistical framework for genomic data fusion. \nBioinformatics. 2004;20(16):2626–35.\n 128. Aerts S, Lambrechts D, Maity S, Van Loo P , Coessens B, De Smet F, et al. Gene prioritization through genomic \ndata fusion. Nat Biotechnol. 2006;24(5):537–44.\n 129. Kong J, Cooper LA, Wang F, Gutman DA, Gao J, Chisolm C, et al. Integrative, multimodal analysis of glio ‑\nblastoma using TCGA molecular data, pathology images, and clinical outcomes. IEEE Trans Biomed Eng. \n2011;58(12):3469–74.\n 130. Ribeiro RT, Marinho RT, Sanches JM. Classification and staging of chronic liver disease from multimodal data. \nIEEE Trans Biomed Eng. 2012;60(5):1336–44.\n 131. Wu KE, Yost KE, Chang HY, Zou J. BABEL enables cross‑modality translation between multiomic profiles at \nsingle ‑cell resolution. Proc Natl Acad Sci. 2021;118(15):e2023070118.\n 132. Karthikeyan N, et al. A novel attention‑based cross‑modal transfer learning framework for predicting cardio ‑\nvascular disease. Comput Biol Med. 2024;170:107977.\n 133. Lu K, Grover A, Abbeel P , Mordatch I. Frozen Pretrained Transformers as Universal Computation Engines. Proc \nAAAI Conf Artif Intell. 2022;36(7):7628–36. https:// doi. org/ 10. 1609/ aaai. v36i7. 20729.\n 134. Krishna K, Khosla S, Bigham JP , Lipton ZC. Generating SOAP Notes from Doctor ‑Patient Conversations Using \nModular Summarization Techniques. In: Proceedings of the 59th Annual Meeting of the Association for \nComputational Linguistics and the 11th International Joint Conference on Natural Language Processing. vol. 1 \n(Long Papers). Stroudsburg: Association for Computational Linguistics (ACL). 2021. p. 4958–72.\n 135. Klasnja P , Pratt W. Healthcare in the pocket: mapping the space of mobile ‑phone health interventions. J \nBiomed Inform. 2012;45(1):184–98.\n 136. Zhu M, Ahuja A, Wei W, Reddy CK. A hierarchical attention retrieval model for healthcare question answering. \nIn: The World Wide Web Conference. New York: Association for Computing Machinery (ACM). 2019. p. 2472–82.\n 137. Beck JT, Rammage M, Jackson GP , Preininger AM, Dankwa‑Mullan I, Roebuck MC, et al. Artificial intelligence \ntool for optimizing eligibility screening for clinical trials in a large community cancer center. JCO Clin Cancer \nInform. 2020;4:50–9.\n 138. Pichai S. An important next step on our AI journey — blog.google. 2023. https:// blog. google/ intl/ en‑ africa/  \nprodu cts/ explo re‑ get‑ answe rs/ an‑ impor tant‑ next‑ step‑ on‑ our‑ ai‑ journ ey/. Accessed 06 Feb 2023.\n 139. Chen RJ, Lu MY, Chen TY, Williamson DF, Mahmood F. Synthetic data in machine learning for medicine and \nhealthcare. Nat Biomed Eng. 2021;5(6):493–7.\n 140. Tunyasuvunakool K, Adler J, Wu Z, Green T, Zielinski M, Žídek A, et al. Highly accurate protein structure predic‑\ntion for the human proteome. Nature. 2021;596(7873):590–6.\n 141. Goenka SD, Gorzynski JE, Shafin K, Fisk DG, Pesout T, Jensen TD, et al. Accelerated identification of disease ‑\ncausing variants with ultra‑rapid nanopore genome sequencing. Nat Biotechnol. 2022;40(7):1035–41.\n 142. Chien I, Deliu N, Turner R, Weller A, Villar S, Kilbertus N. Multi‑ disciplinary fairness considerations in machine \nlearning for clinical trials. In: Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Trans‑\nparency. Heidelberg: Springer Nature. 2022. p. 906–24.\n 143. Zhuang W, Chen C, Lyu L. When foundation model meets federated learning: Motivations, challenges, and \nfuture directions. arXiv preprint arXiv:2306.15546. 2023.\nPage 50 of 54Li et al. BioData Mining            (2025) 18:2 \n 144. Nguyen J, Wang J, Malik K, Sanjabi M, Rabbat M. Where to begin? on the impact of pre‑training and initialization in \nfederated learning. In: Workshop on Federated Learning: Recent Advances and New Challenges (in Conjunction \nwith NeurIPS. 2022. https:// openr eview. net/ forum? id= zE9ct lWm5lx.\n 145. Wu C, Wu F, Lyu L, Huang Y, Xie X. Communication‑efficient federated learning via knowledge distillation. Nat \nCommun. 2022;13(1):2032.\n 146. Liao Y, Xu Y, Xu H, Yao Z, Wang L, Qiao C. Accelerating federated learning with data and model parallelism in edge \ncomputing. IEEE/ACM Trans Network. 2023;32(6):904–18.\n 147. Wang Z, Che B, Guo L, Du Y, Chen Y, Zhao J, et al. PipeFL: Hardware/software co‑design of an FPGA accelerator for \nfederated learning. IEEE Access. 2022;10:98649–61.\n 148. Yuan B, He Y, Davis J, Zhang T, Dao T, Chen B, et al. Decentralized training of foundation models in heterogeneous \nenvironments. Adv Neural Inf Process Syst. 2022;35:25464–77.\n 149. Houlsby N, Giurgiu A, Jastrzebski S, Morrone B, De Laroussilhe Q, Gesmundo A, et al. Parameter‑efficient transfer \nlearning for NLP . In: International Conference on Machine Learning. Somerville: Microtome Publishing. 2019. pp. \n2790–9.\n 150. Hu EJ, yelong shen, Wallis P , Allen‑Zhu Z, Li Y, Wang S, et al. LoRA: Low‑Rank Adaptation of Large Language Models. \nIn: International Conference on Learning Representations. 2022. https:// openr eview. net/ forum? id= nZeVK eeFYf9. \nAccessed 28 Jan 2022.\n 151. Lester B, Al‑Rfou R, Constant N. The Power of Scale for Parameter‑Efficient Prompt Tuning. In: Proceedings of the \n2021 Conference on Empirical Methods in Natural Language Processing. Stroudsburg: Association for Computa‑\ntional Linguistics (ACL). 2021. p. 3045–59.\n 152. Zhao H, Du W, Li F, Li P , Liu G. FedPrompt: Communication‑Efficient and Privacy‑Preserving Prompt Tuning in Fed‑\nerated Learning. In: ICASSP 2023‑2023 IEEE International Conference on Acoustics, Speech and Signal Processing \n(ICASSP). New York: IEEE Corporate Headquarters. 2023. pp. 1–5.\n 153. Hinton G. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531. 2015.\n 154. Yang J, Shen X, Xing J, Tian X, Li H, Deng B, et al. Quantization networks. In: Proceedings of the IEEE/CVF Confer‑\nence on Computer Vision and Pattern Recognition. 2019. pp. 7308–16.\n 155. Blalock D, Gonzalez Ortiz JJ, Frankle J, Guttag J. What is the state of neural network pruning? Proc Mach Learn Syst. \n2020;2:129–46.\n 156. Song R, Zhou L, Lyu L, Festag A, Knoll A. Resfed: Communication efficient federated learning with deep com‑\npressed residuals. IEEE Internet Things J. 2023;11(6):9458–72.\n 157. Nagy B, Hegedűs I, Sándor N, Egedi B. Privacy‑preserving Federated Learning and its application to natural lan‑\nguage processing. Knowl Based Syst. 2023;264:109693.\n 158. Liu M, Ho S, Wang M, Gao L, Jin Y, Zhang H. Federated learning meets natural language processing: A survey. \narXiv preprint arXiv:2107.12603. 2021.\n 159. Jothi Prakash V, Arul Antran Vijay S. A multi‑aspect framework for explainable sentiment analysis. Pattern Recogn \nLett. 2024;178:122–9.\n 160. Kim G, Yoo J, Kang S. Efficient Federated Learning with Pre‑Trained Large Language Model Using Several Adapter \nMechanisms. MDPI. 2024;11(21):4479.\n 161. Ye R, Wang W, Chai J, Li D, Li Z, Xu Y, et al. OpenFedLLM: Training Large Language Models on Decentralized Private \nData via Federated Learning. In: Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and \nData Mining; 2024. p. 6137–47.\n 162. Weller O, Marone M, Braverman V, Lawrie D, Van Durme B. Pretrained Models for Multilingual Federated Learning. \nIn: Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies. Stroudsburg: Association for Computational Linguistics (ACL). 2022. p. \n1413–21.\n 163. Guo T, Guo S, Wang J. Pfedprompt: Learning personalized prompt for vision‑language models in federated learn‑\ning. In: Proceedings of the ACM Web Conference 2023. New York: Association for Computing Machinery (ACM). \n2023. p. 1364–74.\n 164. Peng Y, Bian J, Xu J. FedMM: Federated Multi‑Modal Learning with Modality Heterogeneity in Computational \nPathology. In ICASSP 2024‑2024 IEEE International Conference on Acoustics, Speech and Signal Processing \n(ICASSP)}. IEEE. 2024. p. 1696‑700.\n 165. Chen H, Zhang Y, Krompass D, Gu J, Tresp V. Feddat: An approach for foundation model finetuning in multi‑modal \nheterogeneous federated learning. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38. Wash‑\nington, DC: Association for the Advancement of Artificial Intelligence (AAAI); 2024. p. 11285–93.\n 166. Shi J, Zheng S, Yin X, Lu Y, Xie Y, Qu Y. CLIP‑Guided Federated Learning on Heterogeneity and Long‑Tailed Data. \nIn: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38. Washington, DC: Association for the \nAdvancement of Artificial Intelligence (AAAI); 2024. p. 14955–63.\n 167. Su S, Yang M, Li B, Xue X. Federated Adaptive Prompt Tuning for Multi‑Domain Collaborative Learning. In: Proceed‑\nings of the AAAI Conference on Artificial Intelligence. vol. 38. 2024. pp. 15117–25.\n 168. Lee KJ, Jeong B, Kim S, Kim D, Park D. General Commerce Intelligence: Glocally Federated NLP‑Based Engine for \nPrivacy‑Preserving and Sustainable Personalized Services of Multi‑Merchants. In: Proceedings of the AAAI Confer‑\nence on Artificial Intelligence, vol. 38. Washington, DC: Association for the Advancement of Artificial Intelligence \n(AAAI); 2024. p. 22752–60.\n 169. Ding N, Qin Y, Yang G, Wei F, Yang Z, Su Y, et al. Parameter‑efficient fine‑tuning of large‑scale pre‑trained language \nmodels. Nat Mach Intell. 2023;5(3):220–35.\n 170. Cho YJ, Manoel A, Joshi G, Sim R, Dimitriadis D. Heterogeneous Ensemble Knowledge Transfer for Training Large \nModels in Federated Learning. In: Proceedings of the Thirty‑First International Joint Conference on Artificial Intel‑\nligence (IJCAI) Main Track. Vienna: Center for Computer Science; 2022.\n 171. Liu R, Wu F, Wu C, Wang Y, Lyu L, Chen H, et al. No one left behind: Inclusive federated learning over heterogene‑\nous devices. In: Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. New \nYork: Association for Computing Machinery (ACM); 2022. p. 3398–406.\nPage 51 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \n 172. Ducange P , Marcelloni F, Renda A, Ruffini F. Federated Learning of XAI Models in Healthcare: A Case Study on \nParkinson’s Disease. Cogn Comput. 2024;16:1–26.\n 173. Ali S, Abuhmed T, El‑Sappagh S, Muhammad K, Alonso‑Moral JM, Confalonieri R, et al. Explainable Artificial Intelli‑\ngence (XAI): What we know and what is left to attain Trustworthy Artificial Intelligence. Inf Fusion. 2023;99:101805.\n 174. Zielinski K, Kowalczyk N, Kocejko T, Mazur‑Milecka M, Neumann T, Ruminski J. Federated Learning in Healthcare \nIndustry: Mammography Case Study. In: 2023 IEEE International Conference on Industrial Technology (ICIT). New \nYork: IEEE Corporate Headquarters. 2023. pp. 1–6.\n 175. Mondrejevski L, Miliou I, Montanino A, Pitts D, Hollmén J, Papapetrou P . FLICU: A Federated Learning Workflow for \nIntensive Care Unit Mortality Prediction. 2022. https:// arxiv. org/ abs/ 2205. 15104. Accessed 313 Aug 2022.\n 176. Qiu X, Sun T, Xu Y, Shao Y, Dai N, Huang X. Pre‑trained models for natural language processing: A survey. Sci China \nTechnol Sci. 2020;63(10):1872–97.\n 177. Yuan L, Chen D, Chen YL, Codella N, Dai X, Gao J, et al. Florence: A new foundation model for computer vision. \narXiv preprint arXiv:2111.11432. 2021.\n 178. Liu F, Zhu T, Wu X, Yang B, You C, Wang C, et al. A medical multimodal large language model for future pandemics. \nNPJ Digit Med. 2023;6(1):226.\n 179. Litjens G, Kooi T, Bejnordi BE, Setio AAA, Ciompi F, Ghafoorian M, et al. A survey on deep learning in medical image \nanalysis. Med Image Anal. 2017;42:60–88.\n 180. Feng Z, Guo D, Tang D, Duan N, Feng X, Gong M, et al. Codebert: A pre‑trained model for programming and \nnatural languages. In: Findings of the Association for Computational Linguistics: EMNLP 2020; 2020. p. 1536‑47.\n 181. Min S, Lyu X, Holtzman A, Artetxe M, Lewis M, Hajishirzi H, et al. Rethinking the role of demonstrations: What \nmakes in‑context learning work? arXiv preprint arXiv:2202.12837. 2022.\n 182. Gu Y, Tinn R, Cheng H, Lucas M, Usuyama N, Liu X, et al. Domain‑specific language model pretraining for biomedi‑\ncal natural language processing. ACM Trans Comput Healthc (HEALTH). 2021;3(1):1–23.\n 183. Petroni F, Rocktäschel T, Riedel S, Lewis P , Bakhtin A, Wu Y, et al. Language Models as Knowledge Bases? In: Pro‑\nceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International \nJoint Conference on Natural Language Processing (EMNLP‑IJCNLP). Stroudsburg: Association for Computational \nLinguistics (ACL); 2019. p. 2463–73.\n 184. Saxena S, Sangani R, Prasad S, Kumar S, Athale M, Awhad R, et al. Large‑scale knowledge synthesis and com‑\nplex information retrieval from biomedical documents. In: 2022 IEEE International Conference on Big Data (Big \nData). New York: IEEE Corporate Headquarters. 2022. p. 2364–9.\n 185. Gupta V, Choudhary K, Tavazza F, Campbell C, Liao WK, Choudhary A, et al. Cross‑property deep transfer learning \nframework for enhanced predictive analytics on small materials data. Nat Commun. 2021;12(1):6595.\n 186. Horiuchi D, Tatekawa H, Shimono T, Walston SL, Takita H, Matsushita S, et al. Accuracy of ChatGPT gener‑\nated diagnosis from patient’s medical history and imaging findings in neuroradiology cases. Neuroradiology. \n2024;66(1):73–9.\n 187. Eichelberg M, Aden T, Riesmeier J, Dogac A, Laleci GB. A survey and analysis of electronic healthcare record stand‑\nards. Acm Comput Surv (CSUR). 2005;37(4):277–315.\n 188. Gonzalez‑Hernandez G, Sarker A, O’Connor K, Savova G. Capturing the patient’s perspective: a review of advances \nin natural language processing of health‑related text. Yearb Med Inform. 2017;26(01):214–27.\n 189. Kalyan KS, Sangeetha S. SECNLP: A survey of embeddings in clinical natural language processing. J Biomed Inform. \n2020;101:103323.\n 190. Solares JRA, Raimondi FED, Zhu Y, Rahimian F, Canoy D, Tran J, et al. Deep learning for electronic health records: A \ncomparative review of multiple deep neural architectures. J Biomed Inform. 2020;101:103337.\n 191. Weng WH, Szolovits P . Representation learning for electronic health records. arXiv preprint arXiv:1909.09248. 2019.\n 192. Johnson AE, Pollard TJ, Shen L, Lehman LWH, Feng M, Ghassemi M, et al. MIMIC‑III, a freely accessible critical care \ndatabase. Sci Data. 2016;3(1):1–9.\n 193. Herrett E, Gallagher AM, Bhaskaran K, Forbes H, Mathur R, Van Staa T, et al. Data resource profile: clinical practice \nresearch datalink (CPRD). Int J Epidemiol. 2015;44(3):827–36.\n 194. Basaldella M, Liu F, Shareghi E, Collier N. COMETA: A corpus for medical entity linking in the social media. In: \nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020. \np. 3122–37.\n 195. Müller M, Salathé M, Kummervold PE. Covid‑twitter‑bert: A natural language processing model to analyse covid‑\n19 content on twitter. Front Artif Intell. 2023;6:1023281.\n 196. Johnson AE, Pollard TJ, Greenbaum NR, Lungren MP , Deng Cy, Peng Y, et al. MIMIC‑CXR‑JPG, a large publicly avail‑\nable database of labeled chest radiographs. arXiv preprint arXiv:1901.07042. 2019.\n 197. Ji Y, Zhou Z, Liu H, Davuluri RV. DNABERT: pre‑trained Bidirectional Encoder Representations from Transformers \nmodel for DNA‑language in genome. Bioinformatics. 2021;37(15):2112–20.\n 198. Satterthwaite TD, Connolly JJ, Ruparel K, Calkins ME, Jackson C, Elliott MA, et al. The Philadelphia Neurodevelop‑\nmental Cohort: A publicly available resource for the study of normal and abnormal brain development in youth. \nNeuroimage. 2016;124:1115–9.\n 199. Heinsfeld AS, Franco AR, Craddock RC, Buchweitz A, Meneguzzi F. Identification of autism spectrum disorder using \ndeep learning and the ABIDE dataset. NeuroImage Clin. 2018;17:16–23.\n 200. Alfaro‑Almagro F, Jenkinson M, Bangerter NK, Andersson JL, Griffanti L, Douaud G, et al. Image processing and \nQuality Control for the first 10,000 brain imaging datasets from UK Biobank. Neuroimage. 2018;166:400–24.\n 201. Digre A, Lindskog C. The human protein atlas‑Integrated omics for single cell mapping of the human proteome. \nProtein Sci. 2023;32(2):e4562.\n 202. Frazee AC, Jaffe AE, Langmead B, Leek JT. Polyester: simulating RNA‑seq datasets with differential transcript \nexpression. Bioinformatics. 2015;31(17):2778–84.\n 203. Pelka O, Friedrich CM, García Seco de Herrera A, Müller H. Overview of the ImageCLEFmed 2020 concept predic‑\ntion task: Medical image understanding. CLEF2020 Working Notes. 2020;2696.\nPage 52 of 54Li et al. BioData Mining            (2025) 18:2 \n 204. Wang B, Shang L, Lioma C, Jiang X, Yang H, Liu Q, et al. On position embeddings in bert. In: International Confer‑\nence on Learning Representations. Appleton: International Conference on Learning Representations; 2020.\n 205. Wang B, Zhao D, Lioma C, Li Q, Zhang P , Simonsen JG. Encoding word order in complex embeddings. In: Interna‑\ntional Conference on Learning Representations. Appleton: International Conference on Learning Representations; \n2019.\n 206. Radford A, Narasimhan K. Improving Language Understanding by Generative Pre‑Training. Appleton: International \nConference on Learning Representations; 2018. https:// api. seman ticsc holar. org/ Corpu sID: 49313 245.\n 207. Kenton JD, Chang MW, Toutanova LK. Bert: Pre‑training of deep bidirectional transformers for language under‑\nstanding. In: Proceedings of naacL‑HLT. Minneapolis, Minnesota. 2018;1:2.\n 208. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre‑trained biomedical language representation model \nfor biomedical text mining. Bioinformatics. 2020;36(4):1234–40.\n 209. Peng Y, Yan S, Lu Z. Transfer learning in biomedical natural language processing: an evaluation of BERT and ELMo \non ten benchmarking datasets. In: Proceedings of the 18th BioNLP Workshop and Shared Task. 2019. p. 58–65.\n 210. Beltagy I, Lo K, Cohan A. SciBERT: A Pretrained Language Model for Scientific Text. In: Proceedings of the 2019 \nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on \nNatural Language Processing (EMNLP‑IJCNLP). Stroudsburg: Association for Computational Linguistics (ACL); 2019. \np. 3615–20.\n 211. Kalyan KS, Rajasekharan A, Sangeetha S. AMMU: a survey of transformer‑based biomedical pretrained language \nmodels. J Biomed Inform. 2022;126:103982.\n 212. Liu P , Yuan W, Fu J, Jiang Z, Hayashi H, Neubig G. Pre‑train, prompt, and predict: A systematic survey of prompting \nmethods in natural language processing. ACM Comput Surv. 2023;55(9):1–35.\n 213. Thieme A, Nori A, Ghassemi M, Bommasani R, Andersen TO, Luger E. Foundation Models in Healthcare: Opportuni‑\nties, Risks & Strategies Forward. In: Extended Abstracts of the 2023 CHI Conference on Human Factors in Comput‑\ning Systems. CHI EA ’23. New York: Association for Computing Machinery; 2023. https:// doi. org/ 10. 1145/ 35445 49. \n35831 77.\n 214. Sang ETK, De Meulder F. Introduction to the CoNLL‑2003 Shared Task: Language‑Independent Named Entity Rec‑\nognition. In: Proceedings of the Seventh Conference on Natural Language Learning at HLT‑NAACL 2003. Strouds‑\nburg: Association for Computational Linguistics (ACL). 2003. p. 142–7.\n 215. Bai J, Bai S, Chu Y, Cui Z, Dang K, Deng X, et al. Qwen technical report. arXiv preprint arXiv:2309.16609. 2023.\n 216. Huang K, Altosaar J, Ranganath R. Clinicalbert: Modeling clinical notes and predicting hospital readmission. \narXiv preprint arXiv:1904.05342. 2019.\n 217. Phan LN, Anibal JT, Tran H, Chanana S, Bahadroglu E, Peltekian A, et al. Scifive: a text‑to‑text transformer model for \nbiomedical literature. arXiv preprint arXiv:2106.03598. 2021.\n 218. Monajatipoor M, Yang J, Stremmel J, Emami M, Mohaghegh F, Rouhsedaghat M, et al. LLMs in Biomedicine: A \nstudy on clinical Named Entity Recognition. arXiv preprint arXiv:2404.07376. 2024.\n 219. Wang G, Yang G, Du Z, Fan L, Li X. ClinicalGPT: large language models finetuned with diverse medical data and \ncomprehensive evaluation. arXiv preprint arXiv:2306.09968. 2023.\n 220. Singhal K, Azizi S, Tu T, Mahdavi SS, Wei J, Chung HW, et al. Large language models encode clinical knowledge. \nNature. Nature Publishing Group. 2023;620(7972):172–80.\n 221. Yunxiang L, Zihan L, Kai Z, Ruilong D, You Z. Chatdoctor: A medical chat model fine‑tuned on llama model using \nmedical domain knowledge. arXiv preprint arXiv:2303.14070. 2023;2(5):6.\n 222. Luo L, Ning J, Zhao Y, Wang Z, Ding Z, Chen P , et al. Taiyi: a bilingual fine‑tuned large language model for diverse \nbiomedical tasks. J Am Med Inform Assoc. 2024;ocae037. arxiv preprint paper\n 223. Vincent P , Larochelle H, Bengio Y, Manzagol PA. Extracting and composing robust features with denoising autoen‑\ncoders. In: Proceedings of the 25th international conference on Machine learning. New York: Association for \nComputing Machinery (ACM); 2008. p. 1096–103.\n 224. Goodfellow I, Pouget‑Abadie J, Mirza M, Xu B, Warde‑Farley D, Ozair S, et al. Generative adversarial nets. Adv Neural \nInf Process Syst. 2014;27.\n 225. Chen T, Kornblith S, Norouzi M, Hinton G. A simple framework for contrastive learning of visual representations. In: \nInternational conference on machine learning. Somerville: Microtome Publishing. 2020. p. 1597–607.\n 226. Grill JB, Strub F, Altché F, Tallec C, Richemond P , Buchatskaya E, et al. Bootstrap your own latent‑a new approach to \nself‑supervised learning. Adv Neural Inf Process Syst. 2020;33:21271–84.\n 227. He K, Fan H, Wu Y, Xie S, Girshick R. Momentum contrast for unsupervised visual representation learning. In: \nProceedings of the IEEE/CVF conference on computer vision and pattern recognition. New York: IEEE Corporate \nHeadquarters; 2020. p. 9729–38.\n 228. Bao H, Dong L, Piao S, Wei F. BEiT: BERT Pre‑Training of Image Transformers. In: International Conference on Learn‑\ning Representations. Appleton: International Conference on Learning Representations. 2021.\n 229. He K, Chen X, Xie S, Li Y, Dollár P , Girshick R. Masked autoencoders are scalable vision learners. In: Proceedings of \nthe IEEE/CVF conference on computer vision and pattern recognition. New York: IEEE Corporate Headquarters; \n2022. pp. 16000–9.\n 230. Xie Z, Zhang Z, Cao Y, Lin Y, Bao J, Yao Z, et al. Simmim: A simple framework for masked image modeling. In: \nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. New York: IEEE Corporate \nHeadquarters; 2022. p. 9653–63.\n 231. Gururangan S, Marasović A, Swayamdipta S, Lo K, Beltagy I, Downey D, et al. Don’t Stop Pretraining: Adapt \nLanguage Models to Domains and Tasks. In: Proceedings of the 58th Annual Meeting of the Association for Com‑\nputational Linguistics. Stroudsburg: Association for Computational Linguistics (ACL); 2020. p. 8342–60.\n 232. Rongali S, Jagannatha A, Rawat BPS, Yu H. Continual domain‑tuning for pretrained language models. arXiv pre‑\nprint arXiv:2004.02288. 2020.\n 233. Zhang R, Reddy RG, Sultan MA, Castelli V, Ferritto A, Florian R, et al. Multi‑Stage Pre‑training for Low‑Resource \nDomain Adaptation. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Process‑\ning (EMNLP). Stroudsburg: Association for Computational Linguistics (ACL); 2020. p. 5461–8.\nPage 53 of 54\nLi et al. BioData Mining            (2025) 18:2 \n \n 234. Saharia C, Chan W, Saxena S, Li L, Whang J, Denton EL, et al. Photorealistic text‑to‑image diffusion models with \ndeep language understanding. Adv Neural Inf Process Syst. 2022;35:36479–94.\n 235. Chambon P , Bluethgen C, Delbrouck JB, Van der Sluijs R, Połacin M, Chaves JMZ, et al. RoentGen: vision‑language \nfoundation model for chest x‑ray generation. arXiv preprint arXiv:2211.12737. 2022.\n 236. Chambon PJM, Bluethgen C, Langlotz C, Chaudhari A. Adapting Pretrained Vision‑Language Foundational Models \nto Medical Imaging Domains. In: NeurIPS 2022 Foundation Models for Decision Making Workshop. 2022. https:// \nopenr eview. net/ forum? id= QtxbY dJVT8Q. Accesse 04 Oct 2022.\n 237. Nguyen DK, Okatani T. Multi‑task learning of hierarchical vision‑language representation. In: Proceedings of the \nIEEE/CVF Conference on Computer Vision and Pattern Recognition. New York: IEEE Corporate Headquarters; 2019. \np. 10492–501.\n 238. Gao Y, Liu J, Xu Z, Zhang J, Li K, Ji R, et al. Pyramidclip: Hierarchical feature alignment for vision‑language model \npretraining. Adv Neural Inf Process Syst. 2022;35:35959–70.\n 239. Ruiz N, Li Y, Jampani V, Pritch Y, Rubinstein M, Aberman K. Dreambooth: Fine tuning text‑to‑image diffusion mod‑\nels for subject‑driven generation. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern \nRecognition. New York: IEEE Corporate Headquarters; 2023. pp. 22500–10.\n 240. Irvin J, Rajpurkar P , Ko M, Yu Y, Ciurea‑Ilcus S, Chute C, et al. Chexpert: A large chest radiograph dataset with \nuncertainty labels and expert comparison. In: Proceedings of the AAAI conference on artificial intelligence, \nvol. 33. Washington, DC: Association for the Advancement of Artificial Intelligence (AAAI); 2019. p. 590–7.\n 241. Doersch C. Tutorial on variational autoencoders. arXiv preprint arXiv:1606.05908. 2016;364.\n 242. Liu Y. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. 2019.\n 243. Subramanian S, Wang LL, Bogin B, Mehta S, van Zuylen M, Parasa S, et al. Medicat: A dataset of medical images, \ncaptions, and textual references. In: Findings of the Association for Computational Linguistics: EMNLP 2020. 2020. \np. 2112–20.\n 244. Pelka O, Koitka S, Rückert J, Nensa F, Friedrich CM. Radiology objects in context (roco): a multimodal image \ndataset. In: Intravascular Imaging and Computer Assisted Stenting and Large‑Scale Annotation of Biomedical Data \nand Expert Label Synthesis: 7th Joint International Workshop, CVII‑STENT 2018 and Third International Workshop, \nLABELS 2018, Held in Conjunction with MICCAI 2018, Granada, Spain, September 16, 2018, Proceedings 3. Heidel‑\nberg: Springer Nature. 2018. p. 180–9.\n 245. Degerli A, Ahishali M, Kiranyaz S, Chowdhury ME, Gabbouj M. Reliable covid‑19 detection using chest x‑ray \nimages. In: 2021 IEEE International Conference on Image Processing (ICIP). New York: IEEE Corporate Headquarters. \n2021. p. 185–9.\n 246. Kumar N, Verma R, Sharma S, Bhargava S, Vahadane A, Sethi A. A dataset and a technique for generalized nuclear \nsegmentation for computational pathology. IEEE Trans Med Imaging. 2017;36(7):1550–60.\n 247. Ghosh A, Acharya A, Jain R, Saha S, Chadha A, Sinha S. Clipsyntel: clip and llm synergy for multimodal question \nsummarization in healthcare. In: Proceedings of the AAAI Conference on Artificial Intelligence, vol. 38. Washington, \nDC: Association for the Advancement of Artificial Intelligence (AAAI); 2024. p. 22031–9.\n 248. Boecking B, Usuyama N, Bannur S, Castro DC, Schwaighofer A, Hyland S, et al. Making the most of text semantics \nto improve biomedical vision–language processing. In: European conference on computer vision. Heidelberg: \nSpringer Nature. 2022. p. 1–21.\n 249. Bustos A, Pertusa A, Salinas JM, De La Iglesia‑Vaya M. Padchest: A large chest x‑ray image dataset with multi‑label \nannotated reports. Med Image Anal. 2020;66:101797.\n 250. Woo S, Debnath S, Hu R, Chen X, Liu Z, Kweon IS, et al. Convnext v2: Co‑designing and scaling convnets with \nmasked autoencoders. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni‑\ntion. New York: IEEE Corporate Headquarters; 2023. p. 16133–42.\n 251. Zhang S, Xu Y, Usuyama N, Bagga J, Tinn R, Preston S, et al. Large‑scale domain‑specific pretraining for biomedical \nvision‑language processing. arXiv preprint arXiv:2303.00915. 2023;2(3):6.\n 252. Zhang S, Xu Y, Usuyama N, Xu H, Bagga J, Tinn R, et al. BiomedCLIP: a multimodal biomedical foundation model \npretrained from fifteen million scientific image‑text pairs. arXiv preprint arXiv:2303.00915. 2023.\n 253. Zhang Y, Jiang H, Miura Y, Manning CD, Langlotz CP . Contrastive learning of medical visual representations from \npaired images and text. In: Machine Learning for Healthcare Conference. Somerville: Microtome Publishing. 2022. \np. 2–25.\n 254. Huang SC, Shen L, Lungren MP , Yeung S. Gloria: A multimodal global‑local representation learning framework for \nlabel‑efficient medical image recognition. In: Proceedings of the IEEE/CVF International Conference on Computer \nVision. New York: IEEE Corporate Headquarters; 2021. p. 3942–51.\n 255. Tiu E, Talius E, Patel P , Langlotz CP , Ng AY, Rajpurkar P . Expert‑level detection of pathologies from unannotated \nchest X‑ray images via self‑supervised learning. Nat Biomed Eng. 2022;6(12):1399–406.\n 256. Müller P , Kaissis G, Zou C, Rueckert D. Joint learning of localized representations from medical images and reports. \nIn: European Conference on Computer Vision. Heidelberg: Springer Nature. 2022. p. 685–701.\n 257. Li Y, Wang H, Luo Y. A comparison of pre‑trained vision‑and‑language models for multimodal representation learn‑\ning across medical images and reports. In: 2020 IEEE international conference on bioinformatics and biomedicine \n(BIBM). New York: IEEE Corporate Headquarters. 2020. p. 1999–2004.\n 258. Moon JH, Lee H, Shin W, Kim YH, Choi E. Multi‑modal understanding and generation for medical images and text \nvia vision‑language pre‑training. IEEE J Biomed Health Inform. 2022;26(12):6070–80.\n 259. Chen Z, Li G, Wan X. Align, reason and learn: Enhancing medical vision‑and‑language pre‑training with knowl‑\nedge. In: Proceedings of the 30th ACM International Conference on Multimedia. 2022. pp. 5152–61.\n 260. Li Z, Li Y, Li Q, Wang P , Guo D, Lu L, et al. Lvit: language meets vision transformer in medical image segmentation. \nIEEE Trans Med Imaging. 2023.\n 261. Paul A, Nayyar A, et al. A context‑sensitive multi‑tier deep learning framework for multimodal sentiment analysis. \nMultimedia Tools Appl. 2024;83(18):54249–78.\n 262. Wan Z, Liu C, Zhang M, Fu J, Wang B, Cheng S, et al. Med‑unic: Unifying cross‑lingual medical vision‑language pre‑\ntraining by diminishing bias. Adv Neural Inf Process Syst. 2024;36.\nPage 54 of 54Li et al. BioData Mining            (2025) 18:2 \n 263. Prakash J, Vijay AAS. Cross‑lingual Sentiment Analysis of Tamil Language Using a Multi‑stage Deep Learning Archi‑\ntecture. ACM Trans Asian Low Resour Lang Inf Process. 2023;22(12):1–28.\n 264. Christensen M, Vukadinovic M, Yuan N, Ouyang D. Vision–language foundation model for echocardiogram inter‑\npretation. Nat Med. 2024;1–8. https:// www. nature. com/ artic les/ s41591‑ 024‑ 02959‑y# citeas.\n 265. Li C, Wong C, Zhang S, Usuyama N, Liu H, Yang J, et al. Llava‑med: Training a large language‑and‑vision assistant \nfor biomedicine in one day. Adv Neural Inf Process Syst. 2024;36.\n 266. Act A. Health insurance portability and accountability act of 1996. Public Law. 1996;104:191.\n 267. Challen R, Denny J, Pitt M, Gompels L, Edwards T, Tsaneva‑Atanasova K. Artificial intelligence, bias and clinical \nsafety. BMJ Qual Saf. 2019;28(3):231–7.\n 268. Chamikara MAP , Bertok P , Khalil I, Liu D, Camtepe S. Privacy preserving distributed machine learning with feder‑\nated learning. Comput Commun. 2021;171:112–25.\n 269. Wiens J, Saria S, Sendak M, Ghassemi M, Liu VX, Doshi‑Velez F, et al. Do no harm: a roadmap for responsible \nmachine learning for health care. Nat Med. 2019;25(9):1337–40.\n 270. Chen IY, Joshi S, Ghassemi M. Treating health disparities with artificial intelligence. Nat Med. 2020;26(1):16–7.\n 271. Kaushal A, Altman R, Langlotz C. Geographic distribution of US cohorts used to train deep learning algorithms. \nJama. 2020;324(12):1212–3.\n 272. Zhao Q, Adeli E, Pohl KM. Training confounder‑free deep learning models for medical applications. Nat Commun. \n2020;11(1):6010.\n 273. Li Z, Hoiem D. Learning without forgetting. IEEE Trans Pattern Anal Mach Intell. 2017;40(12):2935–47.\n 274. Ruiz C, Zitnik M, Leskovec J. Identification of disease treatment mechanisms through the multiscale interactome. \nNat Commun. 2021;12(1):1796.\n 275. Lavertu A, Altman RB. RedMed: Extending drug lexicons for social media applications. J Biomed Inform. \n2019;99:103307.\n 276. Li I, Yasunaga M, Nuzumlalı MY, Caraballo C, Mahajan S, Krumholz H, et al. A neural topic‑attention model for medi‑\ncal term abbreviation disambiguation. arXiv preprint arXiv:1910.14076. 2019.\n 277. Chaitanya K, Erdil E, Karani N, Konukoglu E. Contrastive learning of global and local features for medical image \nsegmentation with limited annotations. In: Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H, editors. Advances \nin Neural Information Processing Systems. vol. 33. Curran Associates, Inc.; 2020. pp. 12546–58. https:// proce edings. \nneuri ps. cc/ paper_ files/ paper/ 2020/ file/ 94968 6ecef 4ee20 a62d1 6b4a2 d7ccc a3‑ Paper. pdf. Accessed 16 Oct 2020.\n 278. Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Tunyasuvunakool K, et al. High accuracy protein structure predic‑\ntion using deep learning. Fourteenth Crit Assessm Tech Protein Struct Prediction (Abstr Book). 2020;22(24):2.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations."
}