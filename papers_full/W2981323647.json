{
  "title": "Rethinking Exposure Bias In Language Modeling",
  "url": "https://openalex.org/W2981323647",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5067206559",
      "name": "Yifan Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5009330947",
      "name": "Kening Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102019411",
      "name": "Haoyu Dong",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5084505896",
      "name": "Yuezhou Sun",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5080279541",
      "name": "Wenlong Zhao",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5001760915",
      "name": "Zhuowen Tu",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963878748",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2155027007",
    "https://openalex.org/W2099471712",
    "https://openalex.org/W2616969219",
    "https://openalex.org/W2963248296",
    "https://openalex.org/W2174424190",
    "https://openalex.org/W648786980",
    "https://openalex.org/W1757796397",
    "https://openalex.org/W2016589492",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3140968660",
    "https://openalex.org/W2979550185",
    "https://openalex.org/W2963730239",
    "https://openalex.org/W2527819024",
    "https://openalex.org/W2757836268",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2964043796",
    "https://openalex.org/W2487501366",
    "https://openalex.org/W2963456134",
    "https://openalex.org/W2996068536",
    "https://openalex.org/W2898718449",
    "https://openalex.org/W2143612262",
    "https://openalex.org/W2946468379"
  ],
  "abstract": "Exposure bias describes the phenomenon that a language model trained under the teacher forcing schema may perform poorly at the inference stage when its predictions are conditioned on its previous predictions unseen from the training corpus. Recently, several generative adversarial networks (GANs) and reinforcement learning (RL) methods have been introduced to alleviate this problem. Nonetheless, a common issue in RL and GANs training is the sparsity of reward signals. In this paper, we adopt two simple strategies, multi-range reinforcing, and multi-entropy sampling, to amplify and denoise the reward signal. Our model produces an improvement over competing models with regards to BLEU scores and road exam, a new metric we designed to measure the robustness against exposure bias in language models.",
  "full_text": "Rethinking Exposure Bias in Adversarial Language Modeling\nYifan Xu∗, Kening Zhang∗, Haoyu Dong, Yuezhou Sun, Wenlong Zhao & Zhuowen Tu\nUniversity of California San Diego\n{yix081,kez040, had002, yus174, wez094,ztu}@ucsd.edu\nAbstract\nExposure bias refers to the phenomenon that\na language model trained under the teacher\nforcing schema may perform poorly at the in-\nference stage when its predictions are condi-\ntioned on its outputs that diverge from the\ntraining corpus. Although several adversar-\nial training methods have been proposed to\navoid teacher forcing, lacking a clear evalua-\ntion for the exposure bias remains a concern.\nThe contribution of our work is two-fold. (1)\nWe propose to evaluate exposure bias based\non the quality of sentence generated in the\nsentence completion task. (2) We adopt two\nstrategies, multi-range reinforcing and multi-\nentropy sampling, to stabilize adversarial train-\ning, and show an improvement over the com-\npeting models with regards to the sentence\ncompletion task and corpus BLEUs.\n1 Introduction\nLikelihood-based language models with deep neu-\nral networks have been widely adopted to tackle\nthe language modeling tasks (Graves et al., 2013;\nKarpathy and Fei-Fei, 2015; Bahdanau et al., 2014).\nBy far, one of the most popular training strategies is\nteacher forcing, which is derived from the general\nmaximum likelihood estimation (MLE) principle\n(Williams and Zipser, 1989). Under the teacher\nforcing schema, the language model makes predic-\ntions conditioned on the ground-truth inputs. This\nis susceptible to so-called exposure bias: a model\nmay perform poorly at the inference stage, once\nits preﬁx diverges from the previously learned data\n(Bengio et al., 2015). However, there is little work\non how to expose and quantify such performance\ndegeneration in text generation.\nA common strategy to mitigate the exposure bias\nproblem is to impose additional supervision upon\nthe model’s self-generated output via adversarial\n∗Equal contribution.\ntraining. The actor-critic (AC) method (Konda and\nTsitsiklis, 2000) and SeqGAN (Yu et al., 2017)\nintroduce an additional critic network to offer re-\nwards on a language model’s self-generated se-\nquences. Therefore, the language model can later,\nat the inference stage, predict robustly with its pre-\nvious outputs. One issue in adversarial training is\nthat the signal from the critic network is very sparse,\nwhich leads to stability issues. The second issue is\nabout the non-stationary sampled data with strongly\ncorrelated online updates (Pfau and Vinyals, 2016;\nMnih et al., 2016). Due to these problems, existing\nlanguage GANs (Yu et al., 2017; Lin et al., 2017;\nGuo et al., 2017) have a risk of compromising gen-\neration diversity (Caccia et al., 2018). This paper\nmakes the following contributions:\n1. We propose to evaluate the exposure bias for a language\nmodel by performing the sentence completion task using\nthe ground truth preﬁx.\n2. We introduce a new approach, multi-entropy sampling\nand multi-range reinforcing (MEMR), to overcome the\ndifﬁculties during adversarial training, which demon-\nstrates a signiﬁcant improvement over the competing\nmodels in the corpus BLEUs metrics, as well as our\nproposed measures in sentence completion.\n2 Related Works\nA common measure quantifying the exposure bias\nis still absent. Existing works often show perfor-\nmance gains by introducing adversarial training but\nquestions remain if such gains indeed result in the\nreduction of the exposure bias (Bahdanau et al.,\n2016; Yu et al., 2017). Later works add generation\ndiversity into consideration (Shi et al., 2018; Cac-\ncia et al., 2018; Alihosseini et al., 2019) or take\na perspective from traditional language modeling\naspects (Tevet et al., 2018). A closely related work\nto our evaluation measure is (He et al., 2019). The\ndifference is that He et al. (2019) requires inference\nfor ground truth data distribution with experiments\nperformed using synthetic data.\narXiv:1910.11235v2  [cs.CL]  31 Mar 2020\nAn early work addressing the exposure bias prob-\nlem is (Bengio et al., 2015) in which a curriculum\nlearning approach called scheduled sampling is pro-\nposed by gradually replacing the ground-truth to-\nkens with the model’s predictions. In recent RL-\ninspired works, Ranzato et al. (2015) adopt the\nREINFORCE algorithm (Sutton et al., 2000) to di-\nrectly optimize the test-time evaluation score. Bah-\ndanau et al. (2016) employ a similar approach by\ntraining a critic network to predict the metric score\nfor the actor’s generated sequence of tokens. In\nparallel, a language version of generative adver-\nsarial networks (GANs) (Goodfellow et al., 2014),\nSeqGAN, is introduced in (Yu et al., 2017). Se-\nqGAN consists of a generator pre-trained under\nMLE and a discriminator pre-trained to discern the\ngenerator’s distribution from the real data. Follow-\nup works such as RankGAN (Lin et al., 2017) and\nLeakGAN (Guo et al., 2017) alter the training ob-\njectives or model architectures to enhance the guid-\nance. RankGAN (Lin et al., 2017) replaces the\nbinary reward with a relative ranking score. Leak-\nGAN (Guo et al., 2017) allows the discriminator to\n“leak” its internal states to the generator at interme-\ndiate steps. Shi et al. (2018) model a reward func-\ntion using inverse reinforcement learning (IRL).\n3 Exposure Bias Evaluation\n3.1 Exposure Bias\nCross-entropy loss adopted in teacher forcing is\nequivalent to minimizing the forward KL diver-\ngence DKL(P||Qθ) between data distribution P\nand model distribution Qθ. However, during the in-\nference stage, the model is often evaluated based on\nthe quality of its generated samples. The evaluation\nmetrics or human experts can be seen as surrogates\nof the data distribution P, so what they measure is\nthe reverse KL divergence DKL(Qθ||P).\nIn Bayesian inference, there is a well-known\ndifference between DKL(P||Q) and DKL(Q||P)\n(MacKay, 2003). Minimizing DKL(P||Q) encour-\nages the model to cover all the modes in the training\ndata, which will result in over-generalization in the\nextreme case. In contrast, minimizing DKL(Q||P)\nprefers the model to concentrate on the largest\nmode while ignoring the others, which tends to\ncause mode collapse (Husz ´ar, 2015). In our lan-\nguage modeling task, an LSTM strives to cover\nthe entire data distribution at the cost of over-\ngeneralization. It is more likely to produce preﬁxes\ndifferent from those seen at the training stage, and\nthe fact that this model has never learned to pre-\ndict based on these preﬁxes potentially leads to the\nexposure bias .\n3.2 Sentence Completion Task\nIn this section, we form a sentence completion task\nto evaluate the exposure bias. Given a sentence\npreﬁx X1:k of length K drawn from a data distribu-\ntion P, we apply a language model Qθ to perform\nsentence completion until ﬁnal the T step, starting\nfrom such preﬁx.\n• If the preﬁx X1:k is sampled from a seen distribution\nPseen, then the exposure bias for the sentence comple-\ntion task should be relatively low, where\nQθ(Xk:T|Pseen)=EX1:k∼PseenQθ(Xk:T|X1:k)\n• If the preﬁx X1:k comes from an unseen data distribu-\ntion Punseen, then the exposure bias for the task can be\ncritical, where\nQθ(Xk:T|Punseen)=EX1:k∼PunseenQθ(Xk:T|X1:k)\nBased on the deﬁnition for the exposure bias,\nQθ(Xk:T|Punseen) should suffer more from the\ntraining-testing deviation than Qθ(Xk:T|Pseen).\nAlso, such performance degeneration should be\nmore signiﬁcant when preﬁx kgrows longer in both\nscenarios. These two hypotheses are conﬁrmed by\nour result in Figure 1.\nAs a measurement to assess model’s generation\nquality, forward Corpus BLEU, BLEUF, is evalu-\nated. Because precision is the primary concern, we\nset softmax temperature τ = 0.5 to sample high-\nconﬁdence sentences from model’s distribution.\nBased on the task completion task results in Fig-\nure 1, we observe that original SeqGAN (Yu et al.,\n2017) shows more stable result although many text\nGAN variants are proposed later, which is unex-\npected. Therefore, our method MEMR is motivated\nto improve SeqGAN by introducing denser reward\nsignal from the critic network and further stabiliz-\ning the adversarial training.\n4 Method Description\n4.1 Actor-Critic Training\nActor-Critic methods (ACs) formulates language\nmodeling as a generalized Markov Decision Pro-\ncess (MDP) problem, where the actor learns to\noptimize its policy guided by the critic, while the\ncritic learns to optimize its value function based\non the actor’s output and external reward informa-\ntion. As Pfau and Vinyals (2016) points out, GAN\nmethods can be seen as a special case of AC where\nthe critic aims to distinguish the actor’s genera-\ntion from real data and the actor is optimized in an\nopposite direction to the critic.\nIn this work, we use a standard single-layer\nLSTM as the actor network. The training objective\nis to maximize the model’s expected end rewards\nwith policy gradient (Sutton et al., 2000):\nL(θ)=−EX1:T∼πθ\n∑T\nt=1 Qφ(xt,ht) logπθ(xt|ht)\nIn practice, we perform a Monte-Carlo (MC)\nsearch with roll-out policy following Yu et al.\n(2017) to sample complete sentences starting from\neach location in a predicted sequence and compute\ntheir end rewards. Empirically, we found out that\nthe maximum, instead of average, of rewards in\nthe MC search better represents each token’s ac-\ntor value and yields better results during training.\nTherefore, we compute the action value by:\nQφ(xt,ht)=maxXt:T∈MCθ(X1:t,T) Qφ(X1:T)\nThen, We use a convolutional neural network\n(CNN) as the critic to predict the expected rewards\nfor current generated preﬁx:\nL(φ)=−EX1:T∼πθ(r(X1:T)−Qφ(X1:T))2\n4.2 MEMR\nDuring the experiment, we observe a certain level\nof instability for the learned models. In the previ-\nous literature, two major factors behind the training\ninstability are the sparse reward from critic net-\nwork and the update correlation in the sampling\nprocess (Pfau and Vinyals, 2016; Mnih et al., 2016;\nV olodymyr et al., 2013). We address these prob-\nlems using the following strategies:\nMulti-Entropy Sampling: Language GANs\ncan be seen as online RL methods, where the lan-\nguage model is updated from data generated by\na single policy. Most sampled sentences in MC\nsearch are highly correlated. Similar to Xu et al.\n(2019), we empirically observe that increasing the\nrange of the entropy of the actor’s sample distribu-\ntion during training is beneﬁcial to the adversarial\ntraining performance. Speciﬁcally, we alternate\nthe temperature τ in the softmax to generate sam-\nples under different behavior policies. During the\ncritic’s training, the ground-truth sequences are as-\nsigned a perfect target value of 1. The samples\nobtained with τ <1 are supposed to contain lower\nentropy, thus they receive a higher target value\nclose to 1. Those samples obtained with τ >1 con-\ntain higher entropy, and the target value is closer\nto 0. This mechanism decorrelates updates dur-\ning sequential sampling by sampling from multiple\ndiverse entropy distributions synchronously.\nMulti-Range Reinforcing: Our idea of multi-\nrange supervision takes inspiration from deeply-\nsupervised nets (DSNs) (Lee et al., 2015). By\ndesign, lower layers in a CNN have smaller re-\nceptive ﬁelds, allowing them to make better use of\nlocal patterns. Differently from DSNs (Lee et al.,\n2015) which disregard all intermediate predictions\nin the end, we average the reward predictions from\nmultiple intermediate layers of the critic network\nwith the ﬁnal output, which attend to local n-grams\nrather than the whole complete sentence. This is a\nsolution to the reward sparseness, as the language\nmodel can receive averaged reward with more local\ninformation.\n4.3 Effectiveness of Multi-Range Reinforcing\nand Multi-Entropy Sampling\nTable 1 demonstrates the effectiveness of\nmulti-entropy sampling (ME) and multi-range\nreinforcing (MR). We observe that ME improves\nBLEUF5 (precision) signiﬁcantly while MEMR\nfurther enhances BLEUF5 (precision) and BLEUF5\n(recall). Detailed explanations of these metrics can\nbe found in Section 5.2.\nArchitecture BLEU F5 BLEUB5\nTF 15.4 ±0.17 30.5±0.08\nAC 13.8 ±0.16 30.3 ±0.13\nAC (with ME) 22.4 ±0.25 30.0 ±0.09\nAC (with MEMR ) 24.5 ±0.14 31.6 ±0.10\nTable 1: Effectiveness of the proposed ME and MEMR strate-\ngies on EMNLP2017 WMT News Dataset\n5 Experiment\n5.1 Datasets\nWe perform evaluations on two datasets:\nEMNLP2017 WMT News 1 and Google-small, a\nsubset of Google One Billion Words 2.\n• EMNLP2017 WMT News is provided in (Zhu et al.,\n2018), a benchmarking platform for text GANs. The\nentire dataset is split into a training set of 195,010 sen-\ntences, a validation set of 83,576 sentences, and a test\nset of 10,000 sentences. The vocabulary size is 5,254\nand the average sentence length is 27.\n• Google-small is sampled and pre-processed from the\nGoogle One Billion Words. It contains a training set of\n699,967 sentences, a validation set of 200,000 sentences,\nand a test set of 99,985 sentences. The vocabulary size\nis 61,458 and the average sentence length is 29.\n5.2 BLEU metric\nWe adopt three variations of BLEU metric from Shi\net al. (2018). BLEUF, or forward BLEU, is a metric\nfor precision, and BLEUB, or backward BLEU, is a\nmetric for recall. BLEUHA computes the harmonic\nmean of both BLEU. These three metrics take both\n1https://github.com/geek-ai/Texygen\n2http://www.statmt.org/lm-benchmark/\n(a) Train data (Seen preﬁxes) (b) Test data (Unseen preﬁxes)\nFigure 1: Sentence Completion Taskresults based on preﬁxes from training and testing datasets on EMNLP2017 WMT News\n[Higher is better]. In each experiment, the data source for the preﬁxes is used as the reference to calculate BLEUF4.\nEMNLP2017 WMT Google-small\nModel BLEU F5 BLEUB5 BLEUHA5 BLEUF5 BLEUB5 BLEUHA5\nTEACHERFORCING(TF) 15.4 ±0.11 30.5±0.05 20.5±0.10 9.6±0.03 12.9±0.02 11.00±0.02\nSCHEDULEDSAMPLING(SS) (Bengio et al., 2015) 12.1±0.14 30.3±0.06 17.3±0.14 6.2±0.04 10.7±0.02 7.8 ±0.04\nSEQGAN (Yu et al., 2017) 16.6 ±0.09 28.7±0.37 21.0±0.11 20.7±0.02 14.4±0.02 17.0±0.01\nRANKGAN (Lin et al., 2017) 17.7 ±0.14 30.1±0.06 22.3±0.11 21.4±0.06 12.7±0.02 15.9±0.02\nLEAKGAN (Guo et al., 2017) 19.8 ±0.11 31.6±0.04 24.4±0.10 - - -\nMEMR (ours) 24.5±0.08 31.6±0.06 27.9±0.07 22.0±0.07 15.8±0.02 18.4±0.03\nTable 2: Corpus BLEUs Results on EMNLP2017 WMT News and the Google-small dataset. The 95 % conﬁdence intervals\nfrom multiple trials are reported. †the Google-small was not tested in (Guo et al., 2017) and we are unable to train LeakGAN on this dataset using the\nofﬁcial code due to its training complexity (taking 10+ hours per epoch).\ndiversity and quality into consideration. A model\nwith severe mode collapse or diverse but incorrect\noutputs receives low scores.\n5.3 Implementation Details\nWe implement a standard single-layer LSTM as\nthe generator (actor) and a eight-layer CNN as the\ndiscriminator (critic). The LSTM has embedding\ndimension 32 and hidden dimension 256. The CNN\nconsists of 8 layers with ﬁlter size 3, where the 3rd,\n5th, and 8th layers are directly connected to the\noutput layer for multi-range supervision. Other\nparameters are consistent with Zhu et al. (2018).\nAdam optimizer is deployed for both critic and ac-\ntor with learning rate10−4 and 5·10−3 respectively.\nThe target values for the critic network are set to\n[0, 0.2, 0.4, 0.6, 0.8] for samples generated by the\nLSTM with softmax temperatures [0.5, 0.75, 1.0,\n1.25, 1.5].\n6 Results\nBased on the sentence completion results in Figure\n1, all models decrease in precision of generated text\n(reﬂected via BLEUF4) as the fed-in preﬁx length\n(K) increases, but the effect is stronger on the un-\nseen test data, revealing the existence of exposure\nbias. Nonetheless, our model trained under ME and\nMR yields the best sentence quality and a relatively\nmoderate performance decline.\nAlthough TF and SS demonstrate higher\nBLEUF5 performance with shorter preﬁxes, their\nsentence qualities drop drastically on the test\ndataset with longer preﬁxes. On the other hand,\nGANs begin with lower BLEUF4 precision scores\nbut demonstrate less performance decay as the pre-\nﬁx grows longer and gradually outperform TF. This\nrobustness against unseen preﬁxes exhibits that su-\npervision from a learned critic can boost a model’s\nstability in completing unseen sequences. The bet-\nter generative quality in TF and the stronger robust-\nness against exposure bias in GANs are two differ-\nent objectives in language modeling, but they can\nbe pursued at the same time. Our model’s improve-\nment in both perspectives exhibit one possibility to\nachieve the goal.\nWe also report Corpus BLEUs to reﬂect the qual-\nity and diversity of generated text in Table 2 with\ncompeting models on EMNLP2017 WMT News\nand Google-small. Our model, MEMR, outper-\nforms the others in Corpus BLEUs, indicating a\nhigh diversity and quality in its sample distribu-\ntion.\n7 Conclusion\nWe propose to use the sentence completion task to\nreveal exposure bias in text generation. Further, we\novercome the hurdles in adversarial training with\nmulti-range reinforcingand multi-entropy sampling\n(MEMR), which shows an improvement in the sen-\ntence completion task and Corpus BLEUs.\nAcknowledgments\nThe authors are grateful for the supports by NSF\nIIS-1618477, NSF IIS-1717431, and a grant from\nSamsung Research America.\nReferences\nDanial Alihosseini, Ehsan Montahaei, and Mahdieh So-\nleymani Baghshah. 2019. Jointly measuring diver-\nsity and quality in text generation models. In Pro-\nceedings of the Workshop on Methods for Optimiz-\ning and Evaluating Neural Language Generation ,\npages 90–98.\nDzmitry Bahdanau, Philemon Brakel, Kelvin Xu,\nAnirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron\nCourville, and Yoshua Bengio. 2016. An actor-critic\nalgorithm for sequence prediction. arXiv preprint\narXiv:1607.07086.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-\ngio. 2014. Neural machine translation by jointly\nlearning to align and translate. arXiv preprint\narXiv:1409.0473.\nSamy Bengio, Oriol Vinyals, Navdeep Jaitly, and\nNoam Shazeer. 2015. Scheduled sampling for se-\nquence prediction with recurrent neural networks.\nIn Advances in Neural Information Processing Sys-\ntems, pages 1171–1179.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo\nLarochelle, Joelle Pineau, and Laurent Charlin.\n2018. Language gans falling short. arXiv preprint\narXiv:1811.02549.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza,\nBing Xu, David Warde-Farley, Sherjil Ozair, Aaron\nCourville, and Yoshua Bengio. 2014. Generative ad-\nversarial nets. In Advances in neural information\nprocessing systems, pages 2672–2680.\nAlex Graves, Abdel-rahman Mohamed, and Geoffrey\nHinton. 2013. Speech recognition with deep recur-\nrent neural networks. In Acoustics, speech and sig-\nnal processing (icassp), 2013 ieee international con-\nference on, pages 6645–6649. IEEE.\nJiaxian Guo, Sidi Lu, Han Cai, Weinan Zhang, Yong\nYu, and Jun Wang. 2017. Long text generation via\nadversarial training with leaked information. arXiv\npreprint arXiv:1709.08624.\nTianxing He, Jingzhao Zhang, Zhiming Zhou, and\nJames Glass. 2019. Quantifying exposure bias\nfor neural language generation. arXiv preprint\narXiv:1905.10617.\nFerenc Husz´ar. 2015. How (not) to train your genera-\ntive model: Scheduled sampling, likelihood, adver-\nsary? arXiv preprint arXiv:1511.05101.\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages\n3128–3137.\nVijay R Konda and John N Tsitsiklis. 2000. Actor-\ncritic algorithms. In Advances in neural information\nprocessing systems, pages 1008–1014.\nChen-Yu Lee, Saining Xie, Patrick Gallagher,\nZhengyou Zhang, and Zhuowen Tu. 2015. Deeply-\nsupervised nets. In Artiﬁcial Intelligence and\nStatistics, pages 562–570.\nKevin Lin, Dianqi Li, Xiaodong He, Zhengyou Zhang,\nand Ming-Ting Sun. 2017. Adversarial ranking for\nlanguage generation. In Advances in Neural Infor-\nmation Processing Systems, pages 3155–3165.\nDavid JC MacKay. 2003. Information theory, infer-\nence and learning algorithms. Cambridge university\npress.\nV olodymyr Mnih, Adria Puigdomenech Badia, Mehdi\nMirza, Alex Graves, Timothy Lillicrap, Tim Harley,\nDavid Silver, and Koray Kavukcuoglu. 2016. Asyn-\nchronous methods for deep reinforcement learning.\nIn International conference on machine learning ,\npages 1928–1937.\nDavid Pfau and Oriol Vinyals. 2016. Connecting gener-\native adversarial networks and actor-critic methods.\narXiv preprint arXiv:1610.01945.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2015. Sequence level train-\ning with recurrent neural networks. arXiv preprint\narXiv:1511.06732.\nZhan Shi, Xinchi Chen, Xipeng Qiu, and Xuanjing\nHuang. 2018. Towards diverse text generation\nwith inverse reinforcement learning. arXiv preprint\narXiv:1804.11258.\nRichard S Sutton, David A McAllester, Satinder P\nSingh, and Yishay Mansour. 2000. Policy gradient\nmethods for reinforcement learning with function ap-\nproximation. In Advances in neural information pro-\ncessing systems, pages 1057–1063.\nGuy Tevet, Gavriel Habib, Vered Shwartz, and\nJonathan Berant. 2018. Evaluating text gans as lan-\nguage models. arXiv preprint arXiv:1810.12686.\nMnih V olodymyr, Koray Kavukcuoglu, David Silver,\nAlex Graves, and Ioannis Antonoglou. 2013. Play-\ning atari with deep reinforcement learning. In NIPS\nDeep Learning Workshop.\nRonald J Williams and David Zipser. 1989. A learn-\ning algorithm for continually running fully recurrent\nneural networks. Neural computation , 1(2):270–\n280.\nYifan Xu, Lu Dai, Udaikaran Singh, Kening Zhang,\nand Zhuowen Tu. 2019. Neural program synthesis\nby self-learning. arXiv preprint arXiv:1910.05865.\nLantao Yu, Weinan Zhang, Jun Wang, and Yong Yu.\n2017. Seqgan: Sequence generative adversarial nets\nwith policy gradient. In AAAI, pages 2852–2858.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo,\nWeinan Zhang, Jun Wang, and Yong Yu. 2018. Texy-\ngen: A benchmarking platform for text generation\nmodels. arXiv preprint arXiv:1802.01886.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7639387845993042
    },
    {
      "name": "Inference",
      "score": 0.6671175956726074
    },
    {
      "name": "Generative grammar",
      "score": 0.6550078392028809
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6139682531356812
    },
    {
      "name": "Schema (genetic algorithms)",
      "score": 0.5391921997070312
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5093568563461304
    },
    {
      "name": "Adversarial system",
      "score": 0.4979064464569092
    },
    {
      "name": "Machine learning",
      "score": 0.47409045696258545
    },
    {
      "name": "Reinforcement learning",
      "score": 0.4517762064933777
    },
    {
      "name": "Principle of maximum entropy",
      "score": 0.4281437397003174
    },
    {
      "name": "Natural language processing",
      "score": 0.3711860477924347
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 6
}