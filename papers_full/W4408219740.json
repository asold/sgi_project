{
  "title": "LLM Abuse Prevention Tool Using GCG Jailbreak Attack Detection and DistilBERT-Based Ethics Judgment",
  "url": "https://openalex.org/W4408219740",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2153477181",
      "name": "Qiuyu Chen",
      "affiliations": [
        "Yamaguchi University"
      ]
    },
    {
      "id": "https://openalex.org/A2103112766",
      "name": "Shingo YAMAGUCHI",
      "affiliations": [
        "Yamaguchi University"
      ]
    },
    {
      "id": "https://openalex.org/A2323810656",
      "name": "Yudai Yamamoto",
      "affiliations": [
        "Yamaguchi University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6853601813",
    "https://openalex.org/W3034951181",
    "https://openalex.org/W4290943938",
    "https://openalex.org/W4382318449",
    "https://openalex.org/W6675669427",
    "https://openalex.org/W2172090160",
    "https://openalex.org/W2142222368",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W1822239915",
    "https://openalex.org/W7064782555",
    "https://openalex.org/W4404784138",
    "https://openalex.org/W4386339095",
    "https://openalex.org/W4285210452",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4386436506",
    "https://openalex.org/W6745609711",
    "https://openalex.org/W2956090150",
    "https://openalex.org/W4408219740",
    "https://openalex.org/W2106418859",
    "https://openalex.org/W2768348081"
  ],
  "abstract": "In recent years, the misuse of large language models (LLMs) has emerged as a significant issue. This paper focuses on a specific attack method known as the greedy coordinate gradient (GCG) jailbreak attack, which compels LLMs to generate responses beyond ethical boundaries. We have developed a tool to suppress the improper use of LLMs by employing a high-precision detection method that combines syntactic tree analysis with the perplexity of generated text. Furthermore, the tool incorporates one of the small language models (SLMs), the DistilBERT model, to evaluate the harmfulness of sentences, thereby preventing harmful content from entering the LLM. Experimental results demonstrate that the tool effectively detects GCG jailbreak attacks and contributes to the secure usage of LLMs. In the test results, the defense success rate reached 90.8%.",
  "full_text": null,
  "topic": "Psychology",
  "concepts": [
    {
      "name": "Psychology",
      "score": 0.4586801826953888
    },
    {
      "name": "Computer security",
      "score": 0.37074899673461914
    },
    {
      "name": "Criminology",
      "score": 0.3319300711154938
    },
    {
      "name": "Computer science",
      "score": 0.24617251753807068
    }
  ]
}