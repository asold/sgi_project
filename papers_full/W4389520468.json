{
    "title": "Enhancing Retrieval-Augmented Large Language Models with Iterative Retrieval-Generation Synergy",
    "url": "https://openalex.org/W4389520468",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5103100933",
            "name": "Zhihong Shao",
            "affiliations": [
                null,
                "Tsinghua University",
                "Intelligent Health (United Kingdom)",
                "Center for Information Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5041448669",
            "name": "Yeyun Gong",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5101180037",
            "name": "Yelong Shen",
            "affiliations": [
                "Microsoft (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A5044042138",
            "name": "Minlie Huang",
            "affiliations": [
                null,
                "Tsinghua University",
                "Intelligent Health (United Kingdom)",
                "Center for Information Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5042018181",
            "name": "Nan Duan",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5051745436",
            "name": "Weizhu Chen",
            "affiliations": [
                "Microsoft (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4304192721",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3176182290",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4304987220",
        "https://openalex.org/W4313304293",
        "https://openalex.org/W4285307817",
        "https://openalex.org/W3156789018",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4312091845",
        "https://openalex.org/W4304195432",
        "https://openalex.org/W4301243929",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W4376312546",
        "https://openalex.org/W2950681488",
        "https://openalex.org/W3159959439",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4309591663",
        "https://openalex.org/W4287121227",
        "https://openalex.org/W4321177655",
        "https://openalex.org/W4360834937",
        "https://openalex.org/W4320351164",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W2963961878",
        "https://openalex.org/W4318719006",
        "https://openalex.org/W3190126809",
        "https://openalex.org/W3115947671",
        "https://openalex.org/W4309088836",
        "https://openalex.org/W4377372007",
        "https://openalex.org/W4367061137",
        "https://openalex.org/W4312091824",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W4309394153",
        "https://openalex.org/W4320813768",
        "https://openalex.org/W2889787757"
    ],
    "abstract": "Retrieval-augmented generation has raise extensive attention as it is promising to address the limitations of large language models including outdated knowledge and hallucinations. However, retrievers struggle to capture relevance, especially for queries with complex information needs. Recent work has proposed to improve relevance modeling by having large language models actively involved in retrieval, i.e., to guide retrieval with generation. In this paper, we show that strong performance can be achieved by a method we call Iter-RetGen, which synergizes retrieval and generation in an iterative manner: a model's response to a task input shows what might be needed to finish the task, and thus can serve as an informative context for retrieving more relevant knowledge which in turn helps generate a better response in another iteration. Compared with recent work which interleaves retrieval with generation when completing a single output, Iter-RetGen processes all retrieved knowledge as a whole and largely preserves the flexibility in generation without structural constraints. We evaluate Iter-RetGen on multi-hop question answering, fact verification, and commonsense reasoning, and show that it can flexibly leverage parametric knowledge and non-parametric knowledge, and is superior to or competitive with state-of-the-art retrieval-augmented baselines while causing fewer overheads of retrieval and generation. We can further improve performance via generation-augmented retrieval adaptation.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 9248–9274\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nEnhancing Retrieval-Augmented Large Language Models with Iterative\nRetrieval-Generation Synergy\nZhihong Shao1, Yeyun Gong2, yelong shen3, Minlie Huang1∗, Nan Duan2, Weizhu Chen3\n1 The CoAI Group, DCST, Institute for Artificial Intelligence,\n1 State Key Lab of Intelligent Technology and Systems,\n1 Beijing National Research Center for Information Science and Technology,\n1 Tsinghua University, Beijing 100084, China\n2 Microsoft Research Asia 3 Microsoft Azure AI\nszh19@mails.tsinghua.edu.cn aihuang@tsinghua.edu.cn\nAbstract\nRetrieval-augmented generation has raise exten-\nsive attention as it is promising to address the\nlimitations of large language models including\noutdated knowledge and hallucinations. How-\never, retrievers struggle to capture relevance,\nespecially for queries with complex informa-\ntion needs. Recent work has proposed to im-\nprove relevance modeling by having large lan-\nguage models actively involved in retrieval, i.e.,\nto guide retrieval with generation. In this pa-\nper, we show that strong performance can be\nachieved by a method we call ITER -RETGEN,\nwhich synergizes retrieval and generation in an\niterative manner: a model’s response to a task\ninput shows what might be needed to finish\nthe task, and thus can serve as an informative\ncontext for retrieving more relevant knowledge\nwhich in turn helps generate a better response\nin another iteration. Compared with recent\nwork which interleaves retrieval with gener-\nation when completing a single output, I TER -\nRETGEN processes all retrieved knowledge as\na whole and largely preserves the flexibility in\ngeneration without structural constraints. We\nevaluate ITER -RETGEN on multi-hop question\nanswering, fact verification, and commonsense\nreasoning, and show that it can flexibly lever-\nage parametric knowledge and non-parametric\nknowledge, and is superior to or competitive\nwith state-of-the-art retrieval-augmented base-\nlines while causing fewer overheads of retrieval\nand generation. We can further improve per-\nformance via generation-augmented retrieval\nadaptation.\n1 Introduction\nGenerative Large Language Models (LLMs)\nhave powered numerous applications, with well-\nperceived utility. Despite being powerful, LLMs\nlack knowledge that is under-represented in their\ntraining data, and are prone to hallucinations, es-\npecially in open-domain settings (OpenAI, 2023).\n∗*Corresponding author: Minlie Huang.\nRetrieval-augmented LLMs, therefore, have raised\nwidespread attention as LLM outputs can be poten-\ntially grounded on external knowledge.\nPrevious retrieval-augmented LMs (Izacard\net al., 2022b; Shi et al., 2023) typically adopted\none-time retrieval, i.e., to retrieve knowledge us-\ning only the task input (e.g., a user question for\nopen-domain question answering). One-time re-\ntrieval should suffice to fulfill the information needs\nif they are clearly stated in the original input,\nwhich is applicable to factoid question answering\n(Kwiatkowski et al., 2019) and single-hop fact ver-\nification (Thorne et al., 2018), but not to tasks with\ncomplex information needs, e.g., multi-hop rea-\nsoning (Yang et al., 2018) and long-form question\nanswering (Fan et al., 2019).\nTo fulfill complex information needs, recent\nwork proposes to gather required knowledge multi-\nple times throughout the generation process, using\npartial generation (Trivedi et al., 2022a; Press et al.,\n2022)) or forward-looking sentence(s) (Jiang et al.,\n2023) as search queries. However, such structured\nworkflows of interleaving retrieval with generation\nhave the following limitations: (1) as intermediate\ngeneration is conditioned on knowledge retrieved\nbefore, with no awareness of knowledge retrieved\nafterwards, they fail to process all retrieved knowl-\nedge as a whole during the generation process; (2)\nthey require multi-round retrieval to gather a com-\nprehensive set of knowledge, and may frequently\nchange the prompts by updating newly retrieved\nknowledge, thus increasing the overheads of both\nretrieval and generation.\nIn this paper, we find it simple but effective to\nenhance retrieval-augmented LLMs through itera-\ntive retrieval-generation synergy (ITER -RETGEN,\nFig 1). ITER -RETGEN iterates retrieval-augmented\ngeneration and generation-augmented retrieval:\nRetrieval-augmented generation outputs a response\nto a task input based on all retrieved knowledge\n(initially using the task input as the query). This\n9248\noutput shows what might be needed to fulfill the\ntask, and thus can serve as an informative context to\nretrieve more relevant knowledge, i.e., generation-\naugmented retrieval. The newly retrieved knowl-\nedge can benefit another iteration of retrieval-\naugmented generation. We can also leverage model\ngenerations to adapt retrieval, by distilling knowl-\nedge from a re-ranker with access to model genera-\ntions to a dense retriever with access to task inputs\nonly, which may be beneficial in scenarios where\nuser inputs can be easily collected, but relevant\nknowledge or desirable outputs are not annotated.\nWe evaluate our method on three tasks, includ-\ning multi-hop question answering, fact verification,\nand commonsense reasoning. Our method prompts\nan LLM to produce a chain of reasoning steps fol-\nlowed by the final answer under a few-shot set-\nting. For in-context demonstrations, we focus on\nproblem-solving and follow Wei et al. (2022) to\nannotate chains of thoughts, without explicitly con-\nsidering how generation-augmented retrieval might\nbe affected, which makes it conceptually simple\nand easy to implement. Our method achieves up\nto 8.6% absolute gains over previous state-of-the-\nart retrieval-augmented methods on four out of six\ndatasets while being competitive on the remaining\ntwo. According to our experiments, generation gen-\nerally benefits from more iterations, with two itera-\ntions giving the most performance gains. One may\ncustomize the performance-cost tradeoffs by choos-\ning an appropriate number of iterations. We can\nfurther improve performance and also reduce itera-\ntions via the aforementioned generation-augmented\nretrieval adaptation.\nWe summarize our findings as follows:\n• Automatic metrics such as exact match can\nsignificantly underestimate the performance\nof LLMs in question answering tasks. More-\nover, improvements in exact match do not\nalways reflect improvements in generations.\nEvaluation using LLMs may be more reliable.\n• ITER -RETGEN is superior to or competi-\ntive with state-of-the-art retrieval-augmented\nmethods, while being simpler and causing\nfewer overheads of retrieval and generation.\nWith generation-augmented retrieval adapta-\ntion, we can further improve performance\nand also reduce overheads (by reducing itera-\ntions).\n• It is desirable for an LLM to leverage both\nparametric knowledge and non-parametric\nknowledge effectively. I TER -RETGEN con-\nsistently outperforms Self-Ask on question\nanswering tasks, regardless of whether in-\ncontext non-parametric knowledge mentions\nthe answers or not.\n2 Related Work\nIn recent months, there has been a surge in LLM-\npowered applications, such as ChatGPT, Bing Chat,\nand CoPilot (Chen et al., 2021). While showing an\nunprecedented level of performance, LLMs are sub-\nject to the following limitations: (1) Due to a high\ndemand for compute and data, it remains an open\nresearch question to continually update LLMs both\nefficiently and effectively (Scialom et al., 2022);\n(2) LLMs also tend to hallucinate (OpenAI, 2023),\ni.e., generating plausible but non-factual texts. To\nalleviate these issues, there is a growing trend of\naugmenting LLMs with tools (Mialon et al., 2023;\nGou et al., 2023), e.g., a code interpreter (Gao\net al., 2022b; Shao et al., 2023) or a search engine\n(Nakano et al., 2021), in an attempt to offload sub-\ntasks to more qualified experts, or to enrich the\ninput context for LLMs by providing more relevant\ninformation.\nRetrieval augmentation is a mainstream direc-\ntion to connect LLMs to the external world. Previ-\nous retrieval-augmented LMs (Izacard and Grave,\n2021; Shao and Huang, 2022) typically receive re-\ntrieved knowledge in a passive way: knowledge\nis retrieved based on the task inputs without LMs’\nintervention. As it is difficult for a retriever to cap-\nture relevance, especially in the zero-shot setting,\nrecent work shows a shift towards having LLMs\nactively involved in retrieval to improve relevance\nmodeling, e.g., to provide a specific context for\nretrieval with model generations (e.g., generated\nsearch queries (Nakano et al., 2021; Press et al.,\n2022; Yao et al., 2022), partial generation (Trivedi\net al., 2022a), or forward-looking sentences (Jiang\net al., 2023)). Khattab et al. (2022) proposed a\nDSP programming framework that supports vari-\nous retrieval-augmented methods.\nRecent work interleaves retrieval with generation\nwhen completing a single output. Such a structured\nworkflow may reduce the flexibility in generation\n(Yao et al., 2022). ITER -RETGEN avoids interrupt-\ning generation with retrieval, but iterates retrieval\nand generation, i.e., to leverage the complete gen-\neration from the previous iteration to retrieve more\n9249\nQuestion:\nRetrieval:\nRetrieval -Augmented Generation: \nWhat is the height of the player who won the 2015 AFL \nRising Star award?\nTitle: 2015 AFL Rising Star Context: The NAB AFL Rising \nStar award is given annually to a stand out young player in \nthe Australian Football League. The award was won by Jesse \nHogan of Melbourne\nThe 2015 AFL Rising Star award was won by Jesse Hogan of \nMelbourne. Jesse Hogan is a professional Australian rules \nfootballer. He is 198 cm tall. So the answer is 198 cm\nTitle: Jesse Hogan Context: Jesse Hogan … playing for the \nMelbourne Football Club. A key forward, Hogan is 1.95 m \ntall … made his AFL debut in the 2015 season and won the \nRon Evans Medal as the AFL Rising Star\nThe 2015 AFL Rising Star award was won by Jesse Hogan of \nMelbourne. Jesse Hogan is 1.95 m tall.\nSo the answer is 1.95 m\nIteration 1 Iteration 2\nGeneration-Augmented Retrieval:\nRetrieval -Augmented Generation: \nQuestion:\nWhat is the height of the player who won the 2015 AFL \nRising Star award?\nFigure 1: ITER -RETGEN iterates retrieval and generation. In each iteration, I TER -RETGEN leverages the model\noutput from the previous iteration as a specific context to help retrieve more relevant knowledge, which may help\nimprove model generation (e.g., correcting the height of Hesse Hogan in this figure). We only show two iterations\nin this figure for brevity. Solid arrows connect queries to the retrieved knowledge, and dashed arrows denote\nretrieval-augmented generation.\nrelevant information which helps improve genera-\ntion in the next iteration. ITER -RETGEN also has\nthe advantage of processing all retrieved knowl-\nedge as a whole during the generation process, and\nis conceptually simpler and easier-to-implement,\nwhile being empirically strong in multi-hop ques-\ntion answering, fact verification, and commonsense\nreasoning.\nA closely related work called GAR (Mao et al.,\n2021) augments queries with generated background\ninformation. HyDE (Gao et al., 2022a) also shares\na similar spirit, but focuses on zero-shot informa-\ntion retrieval, and proposes to first prompt an LLM\nto produce “hypothetical” paragraphs that cover the\ninformation needed to answer a given question, and\nthen use the generated paragraphs to retrieve the\nreal ones. RepoCoder (Zhang et al., 2023) focuses\non repository-level code completion, and proposes\na 2-iteration retrieval-generation paradigm where\nthe second iteration leverages the intermediate code\ncompletion for retrieval. By contrast, we propose\nto synergize retrieval and generation with I TER -\nRETGEN on various natural language tasks, and\nexplore how we can further adapt retrieval with\nmodel generations.\n3 Iterative Retrieval-Generation Synergy\n3.1 Overview\nGiven a question q and a retrieval corpus D =\n{d}where dis a paragraph, ITER -RETGEN repeats\nretrieval-generation for T iterations; in iteration\nt, we (1) leverage the generation yt−1 from the\nprevious iteration, concatenated with q, to retrieve\ntop-kparagraphs, and then (2) prompt an LLM M\nto produce an output yt, with both the retrieved\nparagraphs (denoted as Dyt−1||q) and qintegrated\ninto the prompt. Therefore, each iteration can be\nformulated as follows:\nyt = M(yt|prompt(Dyt−1||q,q)), ∀1 ≤t≤T (1)\nThe last output yT will be produced as the final\nresponse.\n3.2 Generation-Augmented Retrieval\nThere are many natural language tasks with com-\nplex information needs. For example, in open-\ndomain multi-hop question answering, specific in-\nformation needs may manifest themselves only\nafter correctly answering some prerequisite sub-\nquestions. In other words, there may exist semantic\ngaps between the original question q and its sup-\nporting knowledge, which can not be effectively\naddressed by a retriever with a representation bot-\ntleneck. In the first iteration, we can retrieve knowl-\nedge with only the question q. In later iterations,\nthe LLM output from the previous iteration, though\nhaving no guarantee of correctness, shows what\nmight be needed to answer the question, and thus\ncan be leveraged to bridge the semantic gaps; with\nimproved retrieval, an LLM can potentially pro-\nduce a better output.\n3.3 Retrieval-Augmented Generation\nIn each iteration, we generate an output using\nChain-of-Thought prompting except that we also\nprepend retrieved knowledge to the question q.\nThough there may exist more advanced prompting\n9250\nvariants, e.g., incorporating previous generations\ninto the prompt to enable direct refinements, we\nleave the explorations for future work, and focus\non investigating the synergy between retrieval and\ngeneration in a straightforward manner.\n3.4 Generation-Augmented Retrieval\nAdaptation\nModel generations not only provide specific con-\ntexts for retrieval, but can also be leveraged to op-\ntimize the retriever, so that information needs in a\nquestion can be better captured by the retriever.\nDense Retriever We adopted dense retrieval\nin our experiments. Given a dense retriever\nparametrized by θ= {θq,θd}where θq and θd de-\nnote parameters of the query encoder and the para-\ngraph encoder, respectively, the similarity score\nbetween a query and a paragraph is calculated as\nthe inner product of their encoded vectors:\nsθ(q,d) =⟨E(q; θq),E(d; θd)⟩ (2)\nRe-ranker A re-ranker, parametrized by ϕ, out-\nputs the probability of a paragraph being relevant\nto a query; we denote the probability as sϕ(q,d).\nDistillation A re-ranker is typically better at cap-\nturing relevance between a query and a paragraph\nthan a retriever. Therefore, we distill knowledge\nfrom a re-ranker to a retriever. To help the retriever\nbetter address the semantic gaps between a ques-\ntion and its supporting knowledge, we allow access\nto y1 for the re-ranker (where y1 is the LLM out-\nput from the first iteration). We optimize only the\nquery encoder of the retriever using the following\ntraining objective:\nθ∗\nq = argmin\nθq\nKL(Pϕ(·|y1,q),Pθ(·|q))\nPϕ(d|y1,q) = exp(sϕ(y1||q,d)/τ)∑\nd′∈Dy1||q\nexp(sϕ(y1||q,d′)/τ)\nPθ(d|q) = exp(sθ(q,d)/τ)∑\nd′∈Dy1||q\nexp(sθ(q,d′)/τ)\n(3)\nwhere KL(·,·) denotes the KL divergence between\ntwo probabilistic distributions.\n4 Experiments\n4.1 Datasets\nWe experimented on six datasets of three reason-\ning tasks: (1) Multi-hop question answering ,\nincluding HotPotQA (Yang et al., 2018), 2Wiki-\nMultiHopQA (Ho et al., 2020), MuSiQue (Trivedi\net al., 2022b), and Bamboogle (Press et al., 2022).\nOn MuSiQue, we followed Press et al. (2022) to\nuse only 2-hop questions; (2) Fact Verification,\nincluding Feverous (Aly et al., 2021); (3) Com-\nmonsense reasoning, including StrategyQA (Geva\net al., 2021). Examples are presented in Table 1.\nWe used the October 2017 (Yang et al., 2018)\nand the December 2018 (Karpukhin et al., 2020)\nWikipedia dump as the retrieval corpus for Hot-\nPotQA and 2WikiMultiHopQA, respectively, and\nused the December 2021 Wikipedia dump (Izacard\net al., 2022b) for the other datasets.\n4.2 Evaluation Settings\nWe conducted evaluations on all 125 questions\nfrom Bamboogle, the first 500 questions from the\ntrain set of StrategyQA, and the first 500 questions\nfrom the development sets of the other datasets.\nAll methods are evaluated under the 3-shot setting,\nsharing the same questions in demonstrations.\nEvaluation metrics are exact match (EM) and F1\nfor multi-hop question answering datasets, and ac-\ncuracy for both fact verification and commonsense\nreasoning datasets. For more robust evaluation,\nwe also evaluate the correctness of model outputs\nusing text-davinci-003, the resulting metric de-\nnoted as Acc†. The prompt used for evaluation is\nas follows, where {question}, {model output},\nand {answer}are placeholders.\nPrompt for Evaluating the Correctness of a\nModel Output\nIn the following task, you are given a\nQuestion, a model Prediction for the\nQuestion, and a Ground-truth Answer to the\nQuestion. You should decide whether the\nmodel Prediction implies the Ground-truth\nAnswer.\nQuestion\n{question}\nPrediction\n{model output}\nGround-truth Answer\n{answer}\nDoes the Prediction imply the Ground-truth\nAnswer? Output Yes or No:\n9251\nDatasets Example\nHotPotQA What is the name of this American musician, singer, actor, comedian, and songwriter,\nwho worked with Modern Records and born in December 5, 1932?\n2WikiMultiHopQA Which film came out first, Blind Shaft or The Mask Of Fu Manchu?\nMuSiQue In which year did the publisher of In Cold Blood form?\nBamboogle When did the first prime minister of the Russian Empire come into office?\nFeverous Is it true that Based on the same platform as the Chevrolet Sail, the Baojun 310 was\nlaunched on 2017 Beijing Auto Show where the price ranges from 36.800 yuan to\n60.800 yuan?\nStrategyQA Is it common to see frost during some college commencements?\nTable 1: Example questions from six datasets.\n4.3 Baselines\nDirect Prompting (Brown et al., 2020) prompts an\nLLM to directly generate the final answer without\nan explanation. When augmenting Direct prompt-\ning with retrieval, we used the question to retrieve\nknowledge which will be placed before the ques-\ntion in the prompt.\nCoT Prompting (Wei et al., 2022) prompts an\nLLM to generate natural language reasoning steps\nfollowed by the final answer.\nReAct (Yao et al., 2022) interleaves reasoning, ac-\ntion, and observation steps, until reaching the ac-\ntion of finalizing an answer. An action can be\neither generating a query to search for informa-\ntion or finalizing an answer. An observation is the\nconcatenation of retrieved paragraphs.\nSelf-Ask (Press et al., 2022) interleaves (i) follow-\nup question generation, (ii) retrieval using the\nfollow-up, and (iii) answering the follow-up condi-\ntioned on the retrieved knowledge, until no more\nfollow-up questions are generated and the LLM\ngives an answer to the original question. We fol-\nlowed (Yoran et al., 2023) to prepend newly re-\ntrieved paragraphs to the original question. On our\nevaluated tasks, Self-Ask is conceptually similar\nto ReAct, with the main difference being that Self-\nAsk accumulates retrieved knowledge before the\noriginal question in the prompt, while ReAct places\nretrieved knowledge right after its query. Self-Ask\nand IRCoT (Trivedi et al., 2022a) also share the\nspirit of synergizing reasoning and retrieval.\nDSP (Khattab et al., 2022) comprises a multi-hop\nretrieval stage and an answer prediction stage. For\neach hop within the retrieval stage, the model is\nprompted to generate search queries and to sum-\nmarize retrieve knowledge for subsequent use. In\nthe prediction stage, DSP generates the answer us-\ning CoT based on the summarized knowledge and\nretrieved documents.\n4.4 Implementation Details\nWe used text-davinci-003 version of Instruct-\nGPT (Ouyang et al., 2022) as the backend LLM.\nWe also present experiments using the open-source\nLlama-2 models (Touvron et al., 2023) in Ap-\npendix A. All experiments used greedy decoding.\nContriever-MSMARCO (Izacard et al., 2022a) was\nused for retrieval. We retrieved top-5 paragraphs\nfor each query. We allowed at most 5 interactions\nwith retrieval for ReAct and Self-Ask. We adapted\nthe implementation of DSP 1 to use the same gen-\neration model and retrieval systems as the other\nmethods.\nNote that the first iteration of ITER -RETGEN is\nCoT prompting with retrieval augmentation. There-\nfore, ITER -RETGEN and CoT prompting share the\nsame annotated in-context demonstrations. All\nprompts are presented in the Appendix.\n4.5 Main Results\nAs shown by Table 2, I TER -RETGEN (T ≥ 2)\nachieve significantly higher Acc† than retrieval-\naugmented baselines on HotPotQA, 2WikiMulti-\nHopQA, Bamboogle, and StrategyQA, while being\ncompetitive with the best method (i.e., Self-Ask)\non MuSiQue and Feverous.\nWhen increasing the number of iterations for\nITER -RETGEN, performance generally improves,\nwith the second iteration giving the greatest boost.\n1https://github.com/stanfordnlp/dspy/issues/85\n9252\nMethod HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA\nEM F1 Acc † EM F1 Acc † EM F1 Acc † EM F1 Acc † Acc Acc† Acc Acc†\nWithout Retrieval\nDirect 21.9 36.8 44.8 21.3 29.2 33.9 7.0 18.7 15.8 11.2 24.4 28.0 60.1 60.1 66.5 66.7\nCoT 30.0 44.1 50.0 30.0 39.6 44.0 19.4 30.9 28.6 43.2 51.1 60.0 59.8 59.8 71.0 71.0\nWith Retrieval\nDirect 31.6 44.7 53.3 27.3 35.4 43.6 13.9 28.2 26.5 17.6 31.8 43.2 69.8 69.8 65.6 65.6\nReAct 24.9 44.7 61.1 28.0 38.5 45.9 23.4 37.0 37.9 21.8 31.0 40.3 66.4 66.4 66.9 66.9\nSelf-Ask 36.8 55.2 64.8 37.3 48.8 55.9 27.6 41.5 42.9 31.5 41.2 54.8 70.7 70.7 70.2 70.2\nDSP 43.8 55.0 60.8 - - - - - - - - - - - - -\nITER-RETGEN 1 39.2 53.9 65.5 33.7 45.2 55.4 24.2 38.6 38.1 36.8 47.7 57.6 67.0 67.0 72.0 72.0\nITER-RETGEN 2 44.1 58.6 71.2 34.9 47.0 58.1 26.4 41.1 41.0 38.4 48.7 59.2 68.8 68.8 73.0 73.0\nITER-RETGEN 3 45.2 59.9 71.4 34.8 47.8 58.3 25.7 41.4 40.8 37.6 47.0 59.2 69.0 69.0 72.3 72.3\nITER-RETGEN 4 45.8 61.1 73.4 36.0 47.4 58.5 26.7 41.8 40.8 38.4 49.6 60.0 71.5 71.5 73.8 73.8\nITER-RETGEN 5 45.2 60.3 72.8 35.5 47.5 58.8 25.7 40.7 39.6 39.2 49.7 60.8 70.3 70.3 73.2 73.2\nITER-RETGEN 6 45.9 61.0 73.3 35.5 48.1 59.4 25.9 40.5 39.8 40.0 50.0 59.2 70.9 70.9 72.4 72.4\nITER-RETGEN 7 45.1 60.4 72.9 35.5 47.4 58.4 26.1 42.0 41.0 40.0 50.7 60.8 70.5 70.5 74.1 74.1\nTable 2: Evaluation results on multi-hop question answering, fact verification, and commonsense reasoning datasets.\nAcc† is the accuracy of model outputs evaluated with text-davinci-003. For ITER -RETGEN, we evaluated LLM\noutputs in different iterations (up to 7 iterations). Underlined metric values are higher than those of Self-Ask.\nMethod HotPotQA 2WikiMultiHopQA MuSiQue Bamboogle Feverous StrategyQA\n# API # Doc # API # Doc # API # Doc # API # Doc # API # Doc # API # Doc\nReAct 2.9 14.3 3.0 15.0 2.9 14.4 2.8 14.1 2.1 10.6 2.8 14.2\nSelf-Ask 3.2 16.0 3.2 15.9 3.0 14.8 3.0 14.9 2.3 11.3 3.0 15.1\nTable 3: Average numbers of API calls to text-davinci-003 and retrieved paragraphs for ReAct and Self-Ask.\nNote that ITER -RETGEN (T = 2) achieves significantly higher or competitive Acc† with fewer API calls (i.e., 2)\nand fewer retrieved paragraphs (5 per iteration, 10 in total).\nIt is worth noting that, as shown by Table 3, ITER -\nRETGEN (T = 2) is superior to or competitive with\nReAct and Self-Ask using fewer API calls to the\nLLM (i.e., 2) and fewer retrieved paragraphs (i.e.,\n5 per iteration, 10 in total). ITER -RETGEN is also\nconceptually simple, which is to iterate retrieval-\naugmented CoT, without complex processing.\nWe also compared I TER -RETGEN with DSP\nwhich also generates the answer using CoT based\non retrieved knowledge but differs in information\ncollection and processing. In each iteration, ITER -\nRETGEN retrieves knowledge based on (1) the ques-\ntion and (2) the previous model output which shows\nwhat may be needed to answer the question. With\nthe number of iterations increasing, we tend to\nobtain a more comprehensive and relevant set of\nknowledge. Besides, unlike DSP, we do not summa-\nrize the retrieved documents for answer generation,\nand thus will not introduce summarization errors.\nAs shown in Table 2, ITER -RETGEN outperforms\nDSP significantly. We manually investigate 10 ran-\ndom questions where DSP fails but ITER -RETGEN\nprovides correct answers. On 40% of them, DSP\nfails to retrieve documents that cover the correct\nanswers, while on 50% of them, the summarized\nknowledge is misleading, e.g., for the question\n“What occupation do Chris Menges and Aram\nAvakian share?”, DSP generates a wrong sum-\nmary “Chris Menges and Aram Avakian are\nboth members of the American and British\nSocieties of Cinematographers.”, while the\nretrieved documents mention that Aram Avakian is\na film editor and director, and only Chris Menges\nis with the American and British Societies of Cine-\nmatographers.\nAcc† is a Reliable Metric To investigate how re-\nliable Acc† is, we focused on model outputs where\nEM and Acc† disagree, and manually checked\nwhich metric gives more correct labels. On each\nof the four multi-hop question answering datasets,\n9253\nDataset HotPotQA Feverous\nRetriever Original Distilled w/oy1 Distilled w/y1 Original Distilled w/oy1 Distilled w/y1\nITER-RETGEN1 65.5 67.1 67.7 67.0 67.3 70.7\nITER-RETGEN2 71.2 75.2 75.7 68.8 68.1 69.5\nTable 4: Effect of using LLM generation y1 on optimizing a dense retriever. We evaluated I TER -RETGEN on\nHotPotQA and Feverous in terms of Acc†.\nSubset CoT ✓ CoT/enc-37w/ Answer Retrieved w/o Answer Retrieved\nMethod Self-Ask I TER-RETGEN2 Self-Ask ITER-RETGEN2 Self-Ask ITER-RETGEN2 Self-Ask ITER-RETGEN2\nHotPotQA 77.5 88.0 52.0 54.4 78.1 86.9 29.9 40.8\n2WikiMultiHopQA 68.8 78.2 46.2 42.0 73.1 77.2 30.1 42.3\nMuSiQue 68.5 66.9 32.6 30.7 72.9 78.9 12.2 22.9\nBamboogle 73.0 77.3 28.0 32.0 76.2 82.2 32.8 46.2\nTable 5: Comparisons between Self-Ask and ITER -RETGEN (T = 2) on different subsets, in terms of Acc†. CoT ✓\nis the subset of questions which CoT answers correctly without retrieval; CoT /enc-37is the complement. w/ Answer\nRetrieved is the subset of questions for which a method (Self-Ask or I TER -RETGEN) successfully retrieves\nparagraphs that mention the answers; w/o Answer Retrievedis the complement. ITER -RETGEN tends to be much\nbetter at preserving the LLM’s performance on questions that can be solved using CoT without retrieval, and is\nconsistently more accurate regardless of whether retrieved knowledge mentions the answers or not.\nwe randomly sampled 20 model outputs from the\nsecond iteration of ITER -RETGEN, resulting in 80\nsamples in total. For 98.75% of samples, EM is\n0 and Acc† is 1, while Acc† gives the correct la-\nbels 97.5% of the time, indicating that EM severely\nunderestimates model performance. We also car-\nried out the same evaluation for Self-Ask, andAcc†\ngives the correct labels 98.75% of the time when it\nis inconsistent with EM.\nAcc† offers the advantage of identifying model\noutputs that are semantically correct, even if\ntheir surface forms differ from the annotated an-\nswers. As an illustration, for the question “Which\ncountry Jan Baptist Van Rensselaer’s\nfather is from?”, the annotated answer isDutch,\nwhile the model prediction is Netherlands, which\nis correct in terms of Acc† but is penalized by EM.\nNotably, ITER -RETGEN (T ≥2) consistently\ndemonstrate lower EM but higher Acc† than Self-\nAsk on 2WikiMultiHopQA, suggesting that en-\nhancements in EM do not necessarily reflect im-\nprovements in the quality of generated answers.\nIteration 1 2 3 4 5 6 7\nHotPotQA 49.5 66.1 65.7 66.5 66.7 66.7 67.1\n2WikiMultiHopQA 29.0 45.2 46.2 46.7 45.8 45.8 46.5\nMuSiQue 18.6 32.3 32.3 33.7 32.7 33.5 32.9\nBamboogle 20.8 36.0 36.8 36.0 35.2 36.0 36.0\nTable 6: Answer recall of retrieved paragraphs in differ-\nent iterations for ITER -RETGEN.\nGeneration Benefits Retrieval Adaptation To\ninvestigate how LLM outputs can be leveraged\nfor retrieval adaptation, we experimented on Hot-\nPotQA and Feverous. Specifically, on each dataset,\nwe sampled 9,000 random questions from the train\nset for training, and 1,000 for validation. We ap-\nplied ITER -RETGEN for one iteration, and used the\nmodel outputs y1 for retrieval adaptation as in Sec-\ntion 3.4. We used TART (Asai et al., 2022) as the\nre-ranker, and distilled knowledge from TART to\nthe dense retriever for no more than 1,000 steps.\nBatch size was 32 and learning rate was 1e-5. We\nused the retriever checkpoint with the lowest distil-\nlation loss.\nAs shown by Table 4, retrieval adaptation en-\nables ITER -RETGEN to achieve significantly higher\nAcc† with fewer iterations. We also demonstrated\nthe benefits of using y1 for adaptation by showing\nits improvements over a variant which only dif-\nfers in that the re-ranker has no access to y1; the\ntraining objective of this variant can be obtained by\nremoving all y1 notations in Eq. 3.\n4.6 Ablation Study\n4.6.1 Generation Augments Retrieval\nTable 6 shows the answer recall of retrieval in dif-\nferent iterations. The first iteration uses only the\nquestions for retrieval and suffers from low an-\nswer recall. In the second iteration, retrieval, aug-\nmented with the LLM output from the first iteration,\n9254\nHotPotQA\nQuestion: The arena where the Lewiston Maineiacs played their home games can seat how many people?\nAnswer: 3,677 seated\nIteration 1\nRetrieved Knowledge\n(1) Title: Lewiston Maineiacs Context: The Lewiston Maineiacs were a junior ice hockey team ... The team played its home\ngames at the Androscoggin Bank Colisée ...\n(2) Title: Bangor Auditorium Context: ... a 5,948 seat multi-purpose arena located in Bangor, Maine ...\nGeneration\nThe Lewiston Maineiacs were a junior ice hockey team of the Quebec Major Junior Hockey League based in Lewiston, Maine. The\nteam played its home games at the Androscoggin Bank Colisée.The Androscoggin Bank Colisée can seat up to 5,948 people.\nSo the answer is 5,948\nIteration 2\nRetrieved Knowledge\n(1) Title: Lewiston Maineiacs Context: The Lewiston Maineiacs were a junior ice hockey team ... The team played its home\ngames at the Androscoggin Bank Colisée ...\n(2) Title: Androscoggin Bank Colisée Context: The Androscoggin Bank Colisée (formerly Central Maine Civic Center and\nLewiston Colisee) is a 4,000 capacity (3,677 seated) multi-purpose arena ...\nGeneration\nThe Lewiston Maineiacs were a junior ice hockey team of the Quebec Major Junior Hockey League based in Lewiston, Maine.\nThey played their home games at the Androscoggin Bank Colisée. The Androscoggin Bank Colisée has a capacity of 4,000\n(3,677 seated). So the answer is 3,677\nStrategyQA\nQuestion: Can you get Raclette in YMCA headquarters city?\nAnswer: Yes\nIteration 1\nRetrieved Knowledge\n(1) Title: YMCA Building (Racine, Wisconsin) Context: ... Since the 50’s, headquarters of the publishing house and the\nbookstore are located in 11 rue de la Montagne Sainte-Geneviève 75005 Paris ...\nGeneration\nThe YMCA headquarters is located in Paris, France.Raclette is a traditional Swiss dish, made with melted cheeseserved with\npotatoes, pickles, and other accompaniments. It is not a traditional French dish, so it is unlikely to be found in Paris. So the\nanswer is No\nIteration 2\nRetrieved Knowledge\n(1) Title: Raclette Context: Raclette is a Swiss dish, also popular in France, based on heating cheese and scraping off the melted\npart ...\n(2) Title: YMCA Building (Racine, Wisconsin) Context: ... Since the 50’s, headquarters of the publishing house and the\nbookstore are located in 11 rue de la Montagne Sainte-Geneviève 75005 Paris ...\nGeneration\nYMCA headquarters is located in Paris, France. Raclette is a dish native to parts of Switzerland, but it is also popular in France.\nSo it is likely that Raclette can be found in Paris. So the answer is Yes\nTable 7: Two examples demonstrating retrieval-generation synergy. We underline generated phrases that help\nretrieve relevant knowledge which successfully corrects factual errors (in red) in the second iteration. Irrelevant\nretrieved paragraphs are not shown in the table for brevity.\nachieves significantly higher recall, indicating that\nLLM generations can help bridge the semantic gaps\nbetween complex questions and their supporting\nknowledge. However, performance quickly hits a\nplateau afterwards.\n4.6.2 I TER -RETGEN Leverages Parametric\nand Non-Parametric Knowledge Better\nIdeally, an LLM should flexibly utilize non-\nparametric knowledge or parametric knowledge\ndepending on whether in-context non-parametric\nknowledge is relevant or not. Table 5 presents per-\nformance breakdowns on different subsets of ques-\ntions for investigation. We considered the ability\nof CoT to answer a question correctly without re-\ntrieval as a proxy for assessing an LLM’s capability\nto answer the question using its parametric knowl-\nedge. Compared with Self-Ask, I TER -RETGEN\ntends to be significantly better at preserving the\nLLM’s performance on questions that the LLM can\nsolve using CoT without retrieval, while being com-\npetitive on the complementary subset. This may\nbe because the structural constraints from Self-Ask\nmakes an LLM over-sensitive to the precision and\ncomprehensiveness of follow-up question genera-\ntion and answering, and Self-Ask is also incapable\nof processing all retrieved knowledge as a whole,\nthus reducing the LLM’s flexibility in solving a\nquestion. Moreover, I TER -RETGEN consistently\noutperforms Self-Ask by a large margin, regardless\n9255\nof whether the in-context non-parametric knowl-\nedge mentions the answers or not. This indicates\nthat when the in-context non-parametric knowledge\nis irrelevant or incomplete, ITER -RETGEN exploits\nparametric knowledge better than Self-Ask.\n4.7 Error Analysis\nOn HotPotQA, we manually analyzed 20 random\ncases where ITER -RETGEN (T = 2) fails. 25% of\npredictions are false negatives. On 10% of cases,\nITER -RETGEN retrieves all necessary information\nbut fails to perform correct reasoning. The remain-\ning 65% of error cases are related with retrieval, on\n76.9% of which, retrieval is misled by completely\nwrong reasoning from the first iteration, while on\nthe other cases, reasoning in the first iteration is\npartially correct, but the retriever fails to retrieve\nthe missing pieces in the second iteration. We also\nobserved that, in the first iteration, reasoning can be\nnegatively affected by noisy and possibly distrac-\ntive knowledge retrieved using only the questions\nas the queries.\n5 Case Study\nTable 7 demonstrates retrieval-generation synergy\nwith two examples from HotPotQA and Strate-\ngyQA, respectively. In the first iteration, as both\nquestions need multi-hop reasoning, the retriever\nfails to retrieve all supporting knowledge using only\nthe questions. Despite being affected by distrac-\ntive retrieved knowledge (the capacity of a different\narena in the example from HotPotQA) and show-\ning imperfect parametric knowledge (the generated\nstatement that Raclette is unlikely to be found in\nParis in the example from StrategyQA) in the first\niteration, the LLM generates phrases that help re-\ntrieve relevant knowledge in the second iteration,\nand successfully corrects its outputs.\n6 Conclusion\nWe demonstrate the effectiveness of ITER -RETGEN\nin answering questions with complex information\nneeds. Despite simple, I TER -RETGEN outper-\nforms retrieval-augmented methods that have a\nmore complex workflow, which we believe could\nserve as a strong baseline for future research on\nretrieval-augmented generation. We also show that\ngeneration-augmented retrieval adaptation can fur-\nther improve the performance of I TER -RETGEN\nwhile also reducing overheads.\nLimitations\nIn this work, we propose to enhance retrieval-\naugmented large language models with I TER -\nRETGEN which synergizes retrieval and generation\nin an iterative manner, and demonstrates strong\nperformance compared to more structured prompt-\ning techniques such as Self-Ask. However, it’s\nworth noting that our experiments utilized a fixed\nblack-box large language model, which may not\nhave been equally optimized for various forms of\nprompting. It would be intriguing to investigate the\npotential of prompting-specific (gradient-based) op-\ntimization in pushing the limits further. This could\ninvolve enabling a large language model to leverage\nparametric and non-parametric knowledge more\nflexibly and effectively. By exploring this avenue,\nwe may uncover new insights and advancements\nin the field. Furthermore, our experiments did not\ncover long-form generation which would probably\nbenefit from more fine-grained retrieval than ITER -\nRETGEN does in this work. We acknowledge that\nthis area warrants further exploration, and we leave\nit for future work.\nAcknowledgements\nZhihong Shao and Minlie Huang were supported by\nthe National Science Foundation for Distinguished\nYoung Scholars (with No. 62125604) and the\nNSFC projects (Key project with No. 61936010).\nThey were also supported by the Guoqiang In-\nstitute of Tsinghua University, with Grant No.\n2020GQG0005.\n9256\nReferences\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull,\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, Oana Cocarascu, and Arpit\nMittal. 2021. FEVEROUS: fact extraction and verifi-\ncation over unstructured and structured information.\nIn Proceedings of the Neural Information Processing\nSystems Track on Datasets and Benchmarks 1,\nNeurIPS Datasets and Benchmarks 2021, December\n2021, virtual.\nAkari Asai, Timo Schick, Patrick S. H. Lewis, Xilun\nChen, Gautier Izacard, Sebastian Riedel, Hannaneh\nHajishirzi, and Wen-tau Yih. 2022. Task-aware re-\ntrieval with instructions. CoRR, abs/2211.09260.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. CoRR,\nabs/2005.14165.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Pondé de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nAngela Fan, Yacine Jernite, Ethan Perez, David Grang-\nier, Jason Weston, and Michael Auli. 2019. ELI5:\nlong form question answering. In Proceedings of\nthe 57th Conference of the Association for Compu-\ntational Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers, pages\n3558–3567. Association for Computational Linguis-\ntics.\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan.\n2022a. Precise zero-shot dense retrieval without rele-\nvance labels. CoRR, abs/2212.10496.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022b. PAL: program-aided language\nmodels. CoRR, abs/2211.10435.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? A question answering benchmark with\nimplicit reasoning strategies. Trans. Assoc. Comput.\nLinguistics, 9:346–361.\nZhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen,\nYujiu Yang, Nan Duan, and Weizhu Chen. 2023.\nCritic: Large language models can self-correct with\ntool-interactive critiquing.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing A multi-hop\nQA dataset for comprehensive evaluation of reason-\ning steps. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 6609–6625. International Committee on\nComputational Linguistics.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2022a. Unsupervised dense in-\nformation retrieval with contrastive learning. Trans.\nMach. Learn. Res., 2022.\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\nEACL 2021, Online, April 19 - 23, 2021, pages 874–\n880. Association for Computational Linguistics.\nGautier Izacard, Patrick S. H. Lewis, Maria Lomeli,\nLucas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022b. Few-shot learning with\nretrieval augmented language models. CoRR,\nabs/2208.03299.\nZhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun,\nQian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie\nCallan, and Graham Neubig. 2023. Active retrieval\naugmented generation. CoRR, abs/2305.06983.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nS. H. Lewis, Ledell Wu, Sergey Edunov, Danqi Chen,\nand Wen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 6769–6781. Associa-\ntion for Computational Linguistics.\nOmar Khattab, Keshav Santhanam, Xiang Lisa Li,\nDavid Hall, Percy Liang, Christopher Potts, and\nMatei Zaharia. 2022. Demonstrate-search-predict:\nComposing retrieval and language models for\nknowledge-intensive NLP. CoRR, abs/2212.14024.\n9257\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur P. Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: a benchmark for question answering\nresearch. Trans. Assoc. Comput. Linguistics, 7:452–\n466.\nYuning Mao, Pengcheng He, Xiaodong Liu, Yelong\nShen, Jianfeng Gao, Jiawei Han, and Weizhu Chen.\n2021. Generation-augmented retrieval for open-\ndomain question answering. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual\nEvent, August 1-6, 2021, pages 4089–4100. Associa-\ntion for Computational Linguistics.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ramakanth Pasunuru, Roberta\nRaileanu, Baptiste Rozière, Timo Schick, Jane\nDwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann\nLeCun, and Thomas Scialom. 2023. Augmented\nlanguage models: a survey. CoRR, abs/2302.07842.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\nXu Jiang, Karl Cobbe, Tyna Eloundou, Gretchen\nKrueger, Kevin Button, Matthew Knight, Benjamin\nChess, and John Schulman. 2021. Webgpt: Browser-\nassisted question-answering with human feedback.\nCoRR, abs/2112.09332.\nOpenAI. 2023. GPT-4 technical report. CoRR,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll L. Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray,\nJohn Schulman, Jacob Hilton, Fraser Kelton, Luke\nMiller, Maddie Simens, Amanda Askell, Peter Welin-\nder, Paul F. Christiano, Jan Leike, and Ryan Lowe.\n2022. Training language models to follow instruc-\ntions with human feedback. In NeurIPS.\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. CoRR, abs/2210.03350.\nThomas Scialom, Tuhin Chakrabarty, and Smaranda\nMuresan. 2022. Continual-t0: Progressively instruct-\ning 50+ tasks to language models without forgetting.\nCoRR, abs/2205.12393.\nZhihong Shao, Yeyun Gong, Yelong Shen, Min-\nlie Huang, Nan Duan, and Weizhu Chen. 2023.\nSynthetic prompting: Generating chain-of-thought\ndemonstrations for large language models. CoRR,\nabs/2302.00618.\nZhihong Shao and Minlie Huang. 2022. Answering\nopen-domain multi-answer questions via a recall-\nthen-verify framework. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), ACL 2022,\nDublin, Ireland, May 22-27, 2022, pages 1825–1838.\nAssociation for Computational Linguistics.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon\nSeo, Rich James, Mike Lewis, Luke Zettlemoyer, and\nWen-tau Yih. 2023. REPLUG: retrieval-augmented\nblack-box language models. CoRR, abs/2301.12652.\nJames Thorne, Andreas Vlachos, Christos\nChristodoulopoulos, and Arpit Mittal. 2018.\nFEVER: a large-scale dataset for fact extraction\nand VERification. In Proceedings of the 2018\nConference of the North American Chapter of\nthe Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long\nPapers), pages 809–819, New Orleans, Louisiana.\nAssociation for Computational Linguistics.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, Dan Bikel, Lukas Blecher, Cristian Canton-\nFerrer, Moya Chen, Guillem Cucurull, David Esiobu,\nJude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller,\nCynthia Gao, Vedanuj Goswami, Naman Goyal, An-\nthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Di-\nana Liskovich, Yinghai Lu, Yuning Mao, Xavier Mar-\ntinet, Todor Mihaylov, Pushkar Mishra, Igor Moly-\nbog, Yixin Nie, Andrew Poulton, Jeremy Reizen-\nstein, Rashi Rungta, Kalyan Saladi, Alan Schelten,\nRuan Silva, Eric Michael Smith, Ranjan Subrama-\nnian, Xiaoqing Ellen Tan, Binh Tang, Ross Tay-\nlor, Adina Williams, Jian Xiang Kuan, Puxin Xu,\nZheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan,\nMelanie Kambadur, Sharan Narang, Aurélien Ro-\ndriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-\ntuned chat models. CoRR, abs/2307.09288.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar\nKhot, and Ashish Sabharwal. 2022a. Interleav-\ning retrieval with chain-of-thought reasoning for\nknowledge-intensive multi-step questions. CoRR,\nabs/2212.10509.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022b. MuSiQue: Multi-\nhop questions via single-hop question composition.\nTrans. Assoc. Comput. Linguistics, 10:539–554.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\n9258\nChristopher D. Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. In Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018,\npages 2369–2380. Association for Computational\nLinguistics.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. CoRR, abs/2210.03629.\nOri Yoran, Tomer Wolfson, Ben Bogin, Uri Katz, Daniel\nDeutch, and Jonathan Berant. 2023. Answering\nquestions by meta-reasoning over multiple chains\nof thought. CoRR, abs/2304.13007.\nFengji Zhang, Bei Chen, Yue Zhang, Jin Liu, Daoguang\nZan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.\n2023. Repocoder: Repository-level code completion\nthrough iterative retrieval and generation. CoRR,\nabs/2303.12570.\n9259\nA Experiments Using Llama-2\nTo demonstrate the effectiveness of ITER -RETGEN\non open-source models, we replaced the generation\nmodel text-davinci-003 in Table 2 with Llama-\n2 models (Touvron et al., 2023), and re-ran the\nevaluation. As shown in Table 8, I TER -RETGEN\nconsistently outperforms all baselines significantly.\nB Few-Shot Prompts\nIn this section, we present all few-shot prompts\nused in our experiments. We replace retrieved\nparagraphs with the placeholder {Knowledge} for\nbrevity. CoT prompting shares the same in-context\ndemonstrations with ITER -RETGEN, except that it\nis not augmented with retrieval.\nB.1 HotPotQA\nPrompts for Direct Prompting, ReAct, Self-Ask,\nand ITER -RETGEN are presented in Table 9, Table\n10, Table 11, and Table 12, respectively.\nB.2 2WikiMultiHopQA\nPrompts for Direct Prompting, ReAct, Self-Ask,\nand ITER -RETGEN are presented in Table 13, Table\n14, Table 15, and Table 16, respectively.\nB.3 MuSiQue\nPrompts for Direct Prompting, ReAct, Self-Ask,\nand ITER -RETGEN are presented in Table 17, Table\n18, Table 19, and Table 20, respectively.\nB.4 Bamboogle\nPrompts for Direct Prompting, ReAct, Self-Ask,\nand ITER -RETGEN are presented in Table 21, Table\n22, Table 23, and Table 24, respectively.\nB.5 Feverous\nPrompts for Direct Prompting, ReAct, Self-Ask,\nand ITER -RETGEN are presented in Table 25, Table\n26, Table 27, and Table 28, respectively.\nB.6 StrategyQA\nPrompts for Direct Prompting, ReAct, Self-Ask,\nand ITER -RETGEN are presented in Table 29, Table\n30, Table 31, and Table 32, respectively.\n9260\nModel Llama-2-13B Llama-2-70B\nDataset HotPotQA 2WikiMultiHopQA StrategyQA HotPotQA 2WikiMultiHopQA StrategyQA\nWithout Retrieval\nDirect 36.4 31.6 60.5 47.2 39.0 72.7\nCoT 43.0 33.2 63.7 55.2 46.0 72.7\nWith Retrieval\nDirect 51.8 38.6 63.3 58.6 45.1 73.3\nReAct 36.0 27.5 61.5 42.6 36.8 69.5\nSelf-Ask 45.8 38.5 63.3 58.4 53.2 71.7\nITER-RETGEN 1 53.8 44.6 62.8 64.4 55.1 74.8\nITER-RETGEN 2 57.8 48.0 67.2 67.8 57.9 76.6\nTable 8: Experiments using the open-source Llama-2 models. We usedAcc† as the evaluation metric, i.e., to evaluate\nthe accuracy of model outputs with text-davinci-003.\n{Knowledge}\nQuestion: What is the name of this American musician, singer, actor, comedian, and songwriter, who worked with Modern\nRecords and born in December 5, 1932?\nThe answer is Little Richard\n{Knowledge}\nQuestion: Between Chinua Achebe and Rachel Carson, who had more diverse jobs?\nThe answer is Chinua Achebe\n{Knowledge}\nQuestion: Remember Me Ballin’ is a CD single by Indo G that features an American rapper born in what year?\nThe answer is 1979\nTable 9: 3-Shot Demonstrations for Direct Prompting on HotPotQA.\n9261\nGiven the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,\nyou are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,\nanswer the question directly.\n#\nQuestion: What is the name of this American musician, singer, actor, comedian, and songwriter, who worked with Modern\nRecords and born in December 5, 1932?\nAre follow up questions needed here: Yes.\nFollow up: Who worked with Modern Records?\n{Knowledge}\nIntermediate answer: Artists worked with Modern Records include Etta James, Little Richard, Joe Houston, Ike and Tina Turner\nand John Lee Hooker.\nFollow up: Is Etta James an American musician, singer, actor, comedian, and songwriter, and was born in December 5, 1932?\n{Knowledge}\nIntermediate answer: Etta James was born in January 25, 1938, not December 5, 1932, so the answer is no.\nFollow up: Is Little Richard an American musician, singer, actor, comedian, and songwriter, and was born in December 5, 1932?\n{Knowledge}\nIntermediate answer: Yes, Little Richard, born in December 5, 1932, is an American musician, singer, actor, comedian and\nsongwriter.\nSo the final answer is: Little Richard\n#\nQuestion: Between Chinua Achebe and Rachel Carson, who had more diverse jobs?\nAre follow up questions needed here: Yes.\nFollow up: What jobs did Chinua Achebe have?\n{Knowledge}\nIntermediate answer: Chinua Achebe was a Nigerian (1) novelist, (2) poet, (3) professor, and (4) critic, so Chinua Achebe had 4\njobs.\nFollow up: What jobs did Rachel Carson have?\n{Knowledge}\nIntermediate answer: Rachel Carson was an American (1) marine biologist, (2) author, and (3) conservationist, so Rachel Carson\nhad 3 jobs.\nFollow up: Did Chinua Achebe have more jobs than Rachel Carson?\n{Knowledge}\nIntermediate answer: Chinua Achebe had 4 jobs, while Rachel Carson had 3 jobs. 4 is greater than 3, so yes, Chinua Achebe had\nmore jobs.\nSo the final answer is: Chinua Achebe\n#\nQuestion: Remember Me Ballin’ is a CD single by Indo G that features an American rapper born in what year?\nAre follow up questions needed here: Yes.\nFollow up: Which American rapper is featured by Remember Me Ballin’, a CD single by Indo G?\n{Knowledge}\nIntermediate answer: Gangsta Boo\nFollow up: In which year was Gangsta Boo born?\n{Knowledge}\nIntermediate answer: Gangsta Boo was born in August 7, 1979, so the answer is 1979.\nSo the final answer is: 1979\nTable 10: 3-Shot Demonstrations for ReAct on HotPotQA.\n9262\nGiven the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,\nyou are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,\nanswer the question directly.\n#\n{Knowledge}\nQuestion: What is the name of this American musician, singer, actor, comedian, and songwriter, who worked with Modern\nRecords and born in December 5, 1932?\nAre follow up questions needed here: Yes.\nFollow up: Who worked with Modern Records?\nIntermediate answer: Artists worked with Modern Records include Etta James, Little Richard, Joe Houston, Ike and Tina Turner\nand John Lee Hooker.\nFollow up: Is Etta James an American musician, singer, actor, comedian, and songwriter, and was born in December 5, 1932?\nIntermediate answer: Etta James was born in January 25, 1938, not December 5, 1932, so the answer is no.\nFollow up: Is Little Richard an American musician, singer, actor, comedian, and songwriter, and was born in December 5, 1932?\nIntermediate answer: Yes, Little Richard, born in December 5, 1932, is an American musician, singer, actor, comedian and\nsongwriter.\nSo the final answer is: Little Richard\n#\n{Knowledge}\nQuestion: Between Chinua Achebe and Rachel Carson, who had more diverse jobs?\nAre follow up questions needed here: Yes.\nFollow up: What jobs did Chinua Achebe have?\nIntermediate answer: Chinua Achebe was a Nigerian (1) novelist, (2) poet, (3) professor, and (4) critic, so Chinua Achebe had 4\njobs.\nFollow up: What jobs did Rachel Carson have?\nIntermediate answer: Rachel Carson was an American (1) marine biologist, (2) author, and (3) conservationist, so Rachel Carson\nhad 3 jobs.\nFollow up: Did Chinua Achebe have more jobs than Rachel Carson?\nIntermediate answer: Chinua Achebe had 4 jobs, while Rachel Carson had 3 jobs. 4 is greater than 3, so yes, Chinua Achebe had\nmore jobs.\nSo the final answer is: Chinua Achebe\n#\n{Knowledge}\nQuestion: Remember Me Ballin’ is a CD single by Indo G that features an American rapper born in what year?\nAre follow up questions needed here: Yes.\nFollow up: Which American rapper is featured by Remember Me Ballin’, a CD single by Indo G?\nIntermediate answer: Gangsta Boo\nFollow up: In which year was Gangsta Boo born?\nIntermediate answer: Gangsta Boo was born in August 7, 1979, so the answer is 1979.\nSo the final answer is: 1979\nTable 11: 3-Shot Demonstrations for Self-Ask on HotPotQA.\n{Knowledge}\nQuestion: What is the name of this American musician, singer, actor, comedian, and songwriter, who worked with Modern\nRecords and born in December 5, 1932?\nLet’s think step by step.\nArtists who worked with Modern Records include Etta James, Joe Houston, Little Richard, Ike and Tina Turner and John Lee\nHooker in the 1950s and 1960s. Of these Little Richard, born in December 5, 1932, was an American musician, singer, actor,\ncomedian, and songwriter.\nSo the answer is Little Richard\n{Knowledge}\nQuestion: Between Chinua Achebe and Rachel Carson, who had more diverse jobs?\nLet’s think step by step.\nChinua Achebe was a Nigerian novelist, poet, professor, and critic. Rachel Carson was an American marine biologist, author,\nand conservationist. So Chinua Achebe had 4 jobs, while Rachel Carson had 3 jobs. Chinua Achebe had more diverse jobs than\nRachel Carson.\nSo the answer is Chinua Achebe\n{Knowledge}\nQuestion: Remember Me Ballin’ is a CD single by Indo G that features an American rapper born in what year?\nLet’s think step by step.\nRemember Me Ballin’ is the CD single by Indo G featuring Gangsta Boo. Gangsta Boo is Lola Mitchell’s stage name, who was\nborn in August 7, 1979, and is an American rapper.\nSo the answer is 1979\nTable 12: 3-Shot Demonstrations for ITER -RETGEN on HotPotQA.\n9263\n{Knowledge}\nQuestion: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?\nThe answer is The Mask Of Fu Manchu\n{Knowledge}\nQuestion: When did John V , Prince Of Anhalt-Zerbst’s father die?\nThe answer is 12 June 1516\n{Knowledge}\nQuestion: Which film has the director who was born later, El Extrano Viaje or Love In Pawn?\nThe answer is El Extrano Viaje\nTable 13: 3-Shot Demonstrations for Direct Prompting on 2WikiMultiHopQA.\nGiven the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,\nyou are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,\nanswer the question directly.\n#\nQuestion: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?\nAre follow up questions needed here: Yes.\nFollow up: When did Blind Shaft come out?\n{Knowledge}\nIntermediate answer: Blind Shaft came out in 2003.\nFollow up: When did The Mask Of Fu Manchu come out?\n{Knowledge}\nIntermediate answer: The Mask Of Fu Manchu came out in 1932.\nSo the final answer is: The Mask Of Fu Manchu\n#\nQuestion: When did John V , Prince Of Anhalt-Zerbst’s father die?\nAre follow up questions needed here: Yes.\nFollow up: Who is the father of John V , Prince Of Anhalt-Zerbst?\n{Knowledge}\nIntermediate answer: The father of John V , Prince Of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau.\nFollow up: When did Ernest I, Prince of Anhalt-Dessau die?\n{Knowledge}\nIntermediate answer: Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.\nSo the final answer is: 12 June 1516\n#\nQuestion: Which film has the director who was born later, El Extrano Viaje or Love In Pawn?\nAre follow up questions needed here: Yes.\nFollow up: Who is the director of El Extrano Viaje?\n{Knowledge}\nIntermediate answer: The director of El Extrano Viaje is Fernando Fernan Gomez.\nFollow up: Who is the director of Love in Pawn?\n{Knowledge}\nIntermediate answer: The director of Love in Pawn is Charles Saunders.\nFollow up: When was Fernando Fernan Gomez born?\n{Knowledge}\nIntermediate answer: Fernando Fernan Gomez was born on 28 August 1921.\nFollow up: When was Charles Saunders (director) born?\n{Knowledge}\nIntermediate answer: Charles Saunders was born on 8 April 1904.\nSo the final answer is: El Extrano Viaje\nTable 14: 3-Shot Demonstrations for ReAct on 2WikiMultiHopQA.\n9264\nGiven the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,\nyou are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,\nanswer the question directly.\n#\n{Knowledge}\nQuestion: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?\nAre follow up questions needed here: Yes.\nFollow up: When did Blind Shaft come out?\nIntermediate answer: Blind Shaft came out in 2003.\nFollow up: When did The Mask Of Fu Manchu come out?\nIntermediate answer: The Mask Of Fu Manchu came out in 1932.\nSo the final answer is: The Mask Of Fu Manchu\n#\n{Knowledge}\nQuestion: When did John V , Prince Of Anhalt-Zerbst’s father die?\nAre follow up questions needed here: Yes.\nFollow up: Who is the father of John V , Prince Of Anhalt-Zerbst?\nIntermediate answer: The father of John V , Prince Of Anhalt-Zerbst is Ernest I, Prince of Anhalt-Dessau.\nFollow up: When did Ernest I, Prince of Anhalt-Dessau die?\nIntermediate answer: Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.\nSo the final answer is: 12 June 1516\n#\n{Knowledge}\nQuestion: Which film has the director who was born later, El Extrano Viaje or Love In Pawn?\nAre follow up questions needed here: Yes.\nFollow up: Who is the director of El Extrano Viaje?\nIntermediate answer: The director of El Extrano Viaje is Fernando Fernan Gomez.\nFollow up: Who is the director of Love in Pawn?\nIntermediate answer: The director of Love in Pawn is Charles Saunders.\nFollow up: When was Fernando Fernan Gomez born?\nIntermediate answer: Fernando Fernan Gomez was born on 28 August 1921.\nFollow up: When was Charles Saunders (director) born?\nIntermediate answer: Charles Saunders was born on 8 April 1904.\nSo the final answer is: El Extrano Viaje\nTable 15: 3-Shot Demonstrations for Self-Ask on 2WikiMultiHopQA.\n{Knowledge}\nQuestion: Which film came out first, Blind Shaft or The Mask Of Fu Manchu?\nLet’s think step by step.\nBlind Shaft is a 2003 film, while The Mask Of Fu Manchu opened in New York on December 2, 1932. 2003 comes after 1932.\nTherefore, The Mask Of Fu Manchu came out earlier than Blind Shaft.\nSo the answer is The Mask Of Fu Manchu\n{Knowledge}\nQuestion: When did John V , Prince Of Anhalt-Zerbst’s father die?\nLet’s think step by step.\nJohn was the second son of Ernest I, Prince of Anhalt-Dessau. Ernest I, Prince of Anhalt-Dessau died on 12 June 1516.\nSo the answer is 12 June 1516\n{Knowledge}\nQuestion: Which film has the director who was born later, El Extrano Viaje or Love In Pawn?\nLet’s think step by step.\nThe director of El Extrano Viaje is Fernando Fernan Gomez, who was born on 28 August 1921. The director of Love In Pawn\nis Charles Saunders, who was born on 8 April 1904. 28 August 1921 comes after 8 April 1904. Therefore, Fernando Fernan\nGomez was born later than Charles Saunders.\nSo the answer is El Extrano Viaje\nTable 16: 3-Shot Demonstrations for ITER -RETGEN on 2WikiMultiHopQA.\n9265\n{Knowledge}\nQuestion: In which year did the publisher of In Cold Blood form?\nThe answer is 2001\n{Knowledge}\nQuestion: Who was in charge of the city where The Killing of a Sacred Deer was filmed?\nThe answer is John Cranley\n{Knowledge}\nQuestion: Where on the Avalon Peninsula is the city that Signal Hill overlooks?\nThe answer is eastern tip\nTable 17: 3-Shot Demonstrations for Direct Prompting on MuSiQue.\nGiven the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,\nyou are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,\nanswer the question directly.\n#\nQuestion: In which year did the publisher of In Cold Blood form?\nAre follow up questions needed here: Yes.\nFollow up: What business published In Cold Blood?\n{Knowledge}\nIntermediate answer: In Cold Blood was published in book form by Random House.\nFollow up: Which year witnessed the formation of Random House?\n{Knowledge}\nIntermediate answer: Random House was form in 2001.\nSo the final answer is: 2001\n#\nQuestion: Who was in charge of the city where The Killing of a Sacred Deer was filmed?\nAre follow up questions needed here: Yes.\nFollow up: In which city was The Killing of a Sacred Deer filmed\n{Knowledge}\nIntermediate answer: The Killing of a Sacred Deer was filmed in Cincinnati.\nFollow up: Who was in charge of Cincinnati?\n{Knowledge}\nIntermediate answer: The present Mayor of Cincinnati is John Cranley, so John Cranley is in charge.\nSo the final answer is: John Cranley\n#\nQuestion: Where on the Avalon Peninsula is the city that Signal Hill overlooks?\nAre follow up questions needed here: Yes.\nFollow up: What city does Signal Hill overlook?\n{Knowledge}\nIntermediate answer: Signal Hill is a hill which overlooks the city of St. John’s.\nFollow up: Where on the Avalon Peninsula is St. John’s located?\n{Knowledge}\nIntermediate answer: St. John’s is located on the eastern tip of the Avalon Peninsula.\nSo the final answer is: eastern tip\nTable 18: 3-Shot Demonstrations for ReAct on MuSiQue.\n9266\nGiven the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,\nyou are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,\nanswer the question directly.\n#\n{Knowledge}\nQuestion: In which year did the publisher of In Cold Blood form?\nAre follow up questions needed here: Yes.\nFollow up: What business published In Cold Blood?\nIntermediate answer: In Cold Blood was published in book form by Random House.\nFollow up: Which year witnessed the formation of Random House?\nIntermediate answer: Random House was form in 2001.\nSo the final answer is: 2001\n#\n{Knowledge}\nQuestion: Who was in charge of the city where The Killing of a Sacred Deer was filmed?\nAre follow up questions needed here: Yes.\nFollow up: In which city was The Killing of a Sacred Deer filmed\nIntermediate answer: The Killing of a Sacred Deer was filmed in Cincinnati.\nFollow up: Who was in charge of Cincinnati?\nIntermediate answer: The present Mayor of Cincinnati is John Cranley, so John Cranley is in charge.\nSo the final answer is: John Cranley\n#\n{Knowledge}\nQuestion: Where on the Avalon Peninsula is the city that Signal Hill overlooks?\nAre follow up questions needed here: Yes.\nFollow up: What city does Signal Hill overlook?\nIntermediate answer: Signal Hill is a hill which overlooks the city of St. John’s.\nFollow up: Where on the Avalon Peninsula is St. John’s located?\nIntermediate answer: St. John’s is located on the eastern tip of the Avalon Peninsula.\nSo the final answer is: eastern tip\nTable 19: 3-Shot Demonstrations for Self-Ask on MuSiQue.\n{Knowledge}\nQuestion: In which year did the publisher of In Cold Blood form?\nLet’s think step by step.\nIn Cold Blood was first published in book form by Random House. Random House was form in 2001.\nSo the answer is 2001\n{Knowledge}\nQuestion: Who was in charge of the city where The Killing of a Sacred Deer was filmed?\nLet’s think step by step.\nThe Killing of a Sacred Deer was filmed in Cincinnati. The present Mayor of Cincinnati is John Cranley. Therefore, John\nCranley is in charge of the city.\nSo the answer is John Cranley\n{Knowledge}\nQuestion: Where on the Avalon Peninsula is the city that Signal Hill overlooks?\nLet’s think step by step.\nSignal Hill is a hill which overlooks the city of St. John’s. St. John’s is located on the eastern tip of the Avalon Peninsula.\nSo the answer is eastern tip\nTable 20: 3-Shot Demonstrations for ITER -RETGEN on MuSiQue.\n9267\n{Knowledge}\nQuestion: When did the first prime minister of the Russian Empire come into office?\nThe answer is 1905-11-06 00:00:00\n{Knowledge}\nQuestion: The most populous city in Punjab is how large (area wise)?\nThe answer is 310 square kilometers\n{Knowledge}\nQuestion: What is the capital of the country where yoga originated?\nThe answer is New Delhi\nTable 21: 3-Shot Demonstrations for Direct Prompting on Bamboogle.\nGiven the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,\nyou are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,\nanswer the question directly.\n#\nQuestion: When did the first prime minister of the Russian Empire come into office?\nAre follow up questions needed here: Yes.\nFollow up: Who is the first prime minister of the Russian Empire?\n{Knowledge}\nIntermediate answer: Sergei Witte\nFollow up: When did Sergei Witte come into office?\n{Knowledge}\nIntermediate answer: Sergei Witte was appointed on 6 November 1905.\nSo the final answer is: 1905-11-06 00:00:00\n#\nQuestion: The most populous city in Punjab is how large (area wise)?\nAre follow up questions needed here: Yes.\nFollow up: What is the most populous city in Punjab?\n{Knowledge}\nIntermediate answer: Ludhiana is the most populous and largest city in Punjab.\nFollow up: How large is Ludhiana, the most populous city in Punjab?\n{Knowledge}\nIntermediate answer: The area of Ludhiana is over 310 km2.\nSo the final answer is: 310 square kilometers\n#\nQuestion: What is the capital of the country where yoga originated?\nAre follow up questions needed here: Yes.\nFollow up: Which country was yoga originated?\n{Knowledge}\nIntermediate answer: There is no consensus on yoga’s origin. Suggested origins include India.\nFollow up: What is the capital of India?\n{Knowledge}\nIntermediate answer: The current capital of India is New Delhi.\nSo the final answer is: New Delhi\nTable 22: 3-Shot Demonstrations for ReAct on Bamboogle.\n9268\nGiven the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,\nyou are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,\nanswer the question directly.\n#\n{Knowledge}\nQuestion: When did the first prime minister of the Russian Empire come into office?\nAre follow up questions needed here: Yes.\nFollow up: Who is the first prime minister of the Russian Empire?\nIntermediate answer: Sergei Witte\nFollow up: When did Sergei Witte come into office?\nIntermediate answer: Sergei Witte was appointed on 6 November 1905.\nSo the final answer is: 1905-11-06 00:00:00\n#\n{Knowledge}\nQuestion: The most populous city in Punjab is how large (area wise)?\nAre follow up questions needed here: Yes.\nFollow up: What is the most populous city in Punjab?\nIntermediate answer: Ludhiana is the most populous and largest city in Punjab.\nFollow up: How large is Ludhiana, the most populous city in Punjab?\nIntermediate answer: The area of Ludhiana is over 310 km2.\nSo the final answer is: 310 square kilometers\n#\n{Knowledge}\nQuestion: What is the capital of the country where yoga originated?\nAre follow up questions needed here: Yes.\nFollow up: Which country was yoga originated?\nIntermediate answer: There is no consensus on yoga’s origin. Suggested origins include India.\nFollow up: What is the capital of India?\nIntermediate answer: The current capital of India is New Delhi.\nSo the final answer is: New Delhi\nTable 23: 3-Shot Demonstrations for Self-Ask on Bamboogle.\n{Knowledge}\nQuestion: When did the first prime minister of the Russian Empire come into office?\nLet’s think step by step.\nThe first prime minister of the Russian Empire was Count Sergei Witte. Sergei Witte was appointed on 6 November 1905.\nSo the answer is 1905-11-06 00:00:00\n{Knowledge}\nQuestion: The most populous city in Punjab is how large (area wise)?\nLet’s think step by step.\nLudhiana is the most populous and the largest city in the Indian state of Punjab. The city has an area of over 310 km2.\nSo the answer is 310 square kilometers\n{Knowledge}\nQuestion: What is the capital of the country where yoga originated?\nLet’s think step by step.\nSuggested origins include pre-Vedic Eastern states of India. The current capital of India is New Delhi.\nSo the answer is New Delhi\nTable 24: 3-Shot Demonstrations for ITER -RETGEN on Bamboogle.\n9269\n{Knowledge}\nQuestion: Is it true that Belgrade Race is an annual men’s footrace of around 6 kilometres (5834 metres) that is held in Belgrade,\nSerbia through history, past winners includes Brahim Lahlafi (1st edition), Philip Mosima (3rd) and Josphat Menjo (6th)?\nThe answer is Yes\n{Knowledge}\nQuestion: Is it true that Based on the same platform as the Chevrolet Sail, the Baojun 310 was launched on 2017 Beijing Auto\nShow where the price ranges from 36.800 yuan to 60.800 yuan?\nThe answer is No\n{Knowledge}\nQuestion: Is it true that Florida International University pedestrian bridge collapse was funded with a $19.4 million Transportation\nInvestment Generating Economic Recovery grant from the United States Department of Transportation in 2013, along with state\nagencies and the bridge cost $14.2 million to construct?\nThe answer is No\nTable 25: 3-Shot Demonstrations for Direct Prompting on Feverous.\nGiven the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,\nyou are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,\nanswer the question directly. The final answer should always be either Yes or No, and NOTHING ELSE.\n#\nQuestion: Is it true that Belgrade Race is an annual men’s footrace of around 6 kilometres (5834 metres) that is held in Belgrade,\nSerbia through history, past winners includes Brahim Lahlafi (1st edition), Philip Mosima (3rd) and Josphat Menjo (6th)?\nAre follow up questions needed here: Yes.\nFollow up: What is the Belgrade Race?\n{Knowledge}\nIntermediate answer: The Belgrade Race Through History is an annual men’s footrace of around 6 kilometres (5834 metres) that\nis held in Belgrade, Serbia.\nFollow up: Has Brahim Lahlafi won Belgrade Race?\n{Knowledge}\nIntermediate answer: Yes, Brahim Lahlafi was the winner in 1996.\nFollow up: Has Philip Mosima won Belgrade Race?\n{Knowledge}\nIntermediate answer: Yes, Philip Mosima beat Marathon world record and won in 1998\nFollow up: Has Josphat Menjo won Belgrade Race?\n{Knowledge}\nIntermediate answer: Yes, Josphat Menjo broke the meet record and won the competition.\nSo the final answer is: Yes\n#\nQuestion: Is it true that Based on the same platform as the Chevrolet Sail, the Baojun 310 was launched on 2017 Beijing Auto\nShow where the price ranges from 36.800 yuan to 60.800 yuan?\nAre follow up questions needed here: Yes.\nFollow up: When and where was the Baojun 310 launched?\n{Knowledge}\nIntermediate answer: The Baojun 310 was launched on 2016 Beijing Auto Show, not 2017 Beijing Auto Show.\nSo the final answer is: No\n#\nQuestion: Is it true that Florida International University pedestrian bridge collapse was funded with a $19.4 million Transportation\nInvestment Generating Economic Recovery grant from the United States Department of Transportation in 2013, along with state\nagencies and the bridge cost $14.2 million to construct?\nAre follow up questions needed here: Yes.\nFollow up: How was Florida International University pedestrian bridge collapse funded?\n{Knowledge}\nIntermediate answer: Florida International University pedestrian bridge was a $14.2 million project funded with a $19.4 million\nTransportation Investment Generating Economic Recovery (TIGER) grant from the United States Department of Transportation\nin 2013, along with state agencies, which is consistent with facts in the question.\nFollow up: How much did it cost to construct Florida International University pedestrian bridge?\n{Knowledge}\nIntermediate answer: The bridge cost $9 million to construct, not $14.2 million.\nSo the final answer is: No\nTable 26: 3-Shot Demonstrations for ReAct on Feverous.\n9270\nGiven the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,\nyou are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,\nanswer the question directly. The final answer should always be either Yes or No, and NOTHING ELSE.\n#\n{Knowledge}\nQuestion: Is it true that Belgrade Race is an annual men’s footrace of around 6 kilometres (5834 metres) that is held in Belgrade,\nSerbia through history, past winners includes Brahim Lahlafi (1st edition), Philip Mosima (3rd) and Josphat Menjo (6th)?\nAre follow up questions needed here: Yes.\nFollow up: What is the Belgrade Race?\nIntermediate answer: The Belgrade Race Through History is an annual men’s footrace of around 6 kilometres (5834 metres) that\nis held in Belgrade, Serbia.\nFollow up: Has Brahim Lahlafi won Belgrade Race?\nIntermediate answer: Yes, Brahim Lahlafi was the winner in 1996.\nFollow up: Has Philip Mosima won Belgrade Race?\nIntermediate answer: Yes, Philip Mosima beat Marathon world record and won in 1998\nFollow up: Has Josphat Menjo won Belgrade Race?\nIntermediate answer: Yes, Josphat Menjo broke the meet record and won the competition.\nSo the final answer is: Yes\n#\n{Knowledge}\nQuestion: Is it true that Based on the same platform as the Chevrolet Sail, the Baojun 310 was launched on 2017 Beijing Auto\nShow where the price ranges from 36.800 yuan to 60.800 yuan?\nAre follow up questions needed here: Yes.\nFollow up: When and where was the Baojun 310 launched?\nIntermediate answer: The Baojun 310 was launched on 2016 Beijing Auto Show, not 2017 Beijing Auto Show.\nSo the final answer is: No\n#\n{Knowledge}\nQuestion: Is it true that Florida International University pedestrian bridge collapse was funded with a $19.4 million Transportation\nInvestment Generating Economic Recovery grant from the United States Department of Transportation in 2013, along with state\nagencies and the bridge cost $14.2 million to construct?\nAre follow up questions needed here: Yes.\nFollow up: How was Florida International University pedestrian bridge collapse funded?\nIntermediate answer: Florida International University pedestrian bridge was a $14.2 million project funded with a $19.4 million\nTransportation Investment Generating Economic Recovery (TIGER) grant from the United States Department of Transportation\nin 2013, along with state agencies, which is consistent with facts in the question.\nFollow up: How much did it cost to construct Florida International University pedestrian bridge?\nIntermediate answer: The bridge cost $9 million to construct, not $14.2 million.\nSo the final answer is: No\nTable 27: 3-Shot Demonstrations for Self-Ask on Feverous.\n9271\nYou are required to verify facts in the following questions. The final answer to a question should always be either Yes or No, and\nNOTHING ELSE.\n{Knowledge}\nQuestion: Is it true that Belgrade Race is an annual men’s footrace of around 6 kilometres (5834 metres) that is held in Belgrade,\nSerbia through history, past winners includes Brahim Lahlafi (1st edition), Philip Mosima (3rd) and Josphat Menjo (6th)?\nLet’s think step by step.\nI need to verify facts in the question. The Belgrade Race Through History is an annual men’s footrace of around 6 kilometres\n(5834 metres) that is held in Belgrade, Serbia. In 1996 Brahim Lahlafi was the winner of the competition. Philip Mosima won\nthe competition in 1998, and beat Marathon world record holder Paul Tergat. Josphat Menjo also won the competition and broke\nthe meet record. Therefore, past winners include Brahim Lahlafi, Philip Mosima and Josphat Menjo. All facts are verified.\nSo the answer is Yes\n{Knowledge}\nQuestion: Is it true that Based on the same platform as the Chevrolet Sail, the Baojun 310 was launched on 2017 Beijing Auto\nShow where the price ranges from 36.800 yuan to 60.800 yuan?\nLet’s think step by step.\nI need to verify facts in the question. The Baojun 310 was indeed based on the same platform as the Chevrolet Sail. The Baojun\n310 was launched on 2016 Beijing Auto Show, not 2017 Beijing Auto Show.\nSo the answer is No\n{Knowledge}\nQuestion: Is it true that Florida International University pedestrian bridge collapse was funded with a $19.4 million Transportation\nInvestment Generating Economic Recovery grant from the United States Department of Transportation in 2013, along with state\nagencies and the bridge cost $14.2 million to construct?\nLet’s think step by step.\nI need to verify facts in the question. Florida International University pedestrian bridge was a $14.2 million project funded with a\n$19.4 million Transportation Investment Generating Economic Recovery (TIGER) grant from the United States Department of\nTransportation in 2013, along with state agencies. The Bridge cost $8 million to construct, not $14.2 million.\nSo the answer is No\nTable 28: 3-Shot Demonstrations for ITER -RETGEN on Feverous.\n{Knowledge}\nQuestion: Is it common to see frost during some college commencements?\nThe answer is Yes\n{Knowledge}\nQuestion: Could a llama birth twice during War in Vietnam (1945-46)?\nThe answer is No\n{Knowledge}\nQuestion: Would Richard Dawkins hypothetically refuse an offering of the Last rites?\nThe answer is Yes\nTable 29: 3-Shot Demonstrations for Direct Prompting on StrategyQA.\n9272\nGiven the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,\nyou are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,\nanswer the question directly. The final answer should always be either Yes or No, and NOTHING ELSE.\n#\nQuestion: Is it common to see frost during some college commencements?\nAre follow up questions needed here: Yes.\nFollow up: What seasons can you expect to see frost?\n{Knowledge}\nIntermediate answer: Frost is common during the winter.\nFollow up: When is college commencement?\n{Knowledge}\nIntermediate answer: College commencement ceremonies often happen during the months of December, May, June.\nFollow up: Do any of the months December, May, June occur during the Winter?\n{Knowledge}\nIntermediate answer: December is in the winter.\nSo the final answer is: Yes\n#\nQuestion: Could a llama birth twice during War in Vietnam (1945-46)?\nAre follow up questions needed here: Yes.\nFollow up: How long did the vietnam war (1945-1946) last?\n{Knowledge}\nIntermediate answer: The War in Vietnam (1945-46) lasted around 6 months.\nFollow up: How long is the llama gestation period?\n{Knowledge}\nIntermediate answer: The gestation period for a llama is 11.5 months.\nFollow up: What is 2 times 11.5?\n{Knowledge}\nIntermediate answer: 23, which is longer than 6.\nSo the final answer is: No\n#\nQuestion: Would Richard Dawkins hypothetically refuse an offering of the Last rites?\nAre follow up questions needed here: Yes.\nFollow up: What are the last Rites?\n{Knowledge}\nIntermediate answer: The Last rites, in Catholicism, are the last prayers and ministrations given to an individual of the faith,\nwhen possible, shortly before death.\nFollow up: What are Richard Dawkins religious beliefs?\n{Knowledge}\nIntermediate answer: Richard Dawkins is known as an outspoken atheist, well known for his criticism of creationism and\nintelligent design.\nFollow up: Would an atheist participate in Catholics prayers?\n{Knowledge}\nIntermediate answer: It is unlikely that an atheist would participate in Catholics prayers.\nSo the final answer is: Yes\nTable 30: 3-Shot Demonstrations for ReAct on StrategyQA.\n9273\nGiven the following question, answer it by providing follow up questions and intermediate answers. For each follow up question,\nyou are given a context which is the top returned Wikipedia snippets for the question. If no follow up questions are necessary,\nanswer the question directly. The final answer should always be either Yes or No, and NOTHING ELSE.\n#\n{Knowledge}\nQuestion: Is it common to see frost during some college commencements?\nAre follow up questions needed here: Yes.\nFollow up: What seasons can you expect to see frost?\nIntermediate answer: Frost is common during the winter.\nFollow up: When is college commencement?\nIntermediate answer: College commencement ceremonies often happen during the months of December, May, June.\nFollow up: Do any of the months December, May, June occur during the Winter?\nIntermediate answer: December is in the winter.\nSo the final answer is: Yes\n#\n{Knowledge}\nQuestion: Could a llama birth twice during War in Vietnam (1945-46)?\nAre follow up questions needed here: Yes.\nFollow up: How long did the vietnam war (1945-1946) last?\nIntermediate answer: The War in Vietnam (1945-46) lasted around 6 months.\nFollow up: How long is the llama gestation period?\nIntermediate answer: The gestation period for a llama is 11.5 months.\nFollow up: What is 2 times 11.5?\nIntermediate answer: 23, which is longer than 6.\nSo the final answer is: No\n#\n{Knowledge}\nQuestion: Would Richard Dawkins hypothetically refuse an offering of the Last rites?\nAre follow up questions needed here: Yes.\nFollow up: What are the last Rites?\nIntermediate answer: The Last rites, in Catholicism, are the last prayers and ministrations given to an individual of the faith,\nwhen possible, shortly before death.\nFollow up: What are Richard Dawkins religious beliefs?\nIntermediate answer: Richard Dawkins is known as an outspoken atheist, well known for his criticism of creationism and\nintelligent design.\nFollow up: Would an atheist participate in Catholics prayers?\nIntermediate answer: It is unlikely that an atheist would participate in Catholics prayers.\nSo the final answer is: Yes\nTable 31: 3-Shot Demonstrations for Self-Ask on StrategyQA.\nYou are required to answer the following questions. The final answer to a question should always be either Yes or No, and\nNOTHING ELSE.\n{Knowledge}\nQuestion: Is it common to see frost during some college commencements?\nLet’s think step by step.\nCollege commencement ceremonies often happen during the months of December, May, and sometimes June. Frost isn’t\nuncommon to see during the month of December, as it is the winter.\nSo the answer is Yes\n{Knowledge}\nQuestion: Could a llama birth twice during War in Vietnam (1945-46)?\nLet’s think step by step.\nThe War in Vietnam (1945-46) lasted around 6 months. The gestation period for a llama is 11 months. If a llama birth twice, the\nminimum time needed is 2 times 11 months, which is 22 months, longer than 6 months.\nSo the answer is No\n{Knowledge}\nQuestion: Would Richard Dawkins hypothetically refuse an offering of the Last rites?\nLet’s think step by step.\nRichard Dawkins is known as an outspoken atheist, well known for his criticism of creationism and intelligent design. The Last\nrites, in Catholicism, are the last prayers and ministrations given to an individual of the faith, when possible, shortly before death.\nIt is unlikely that an atheist would participate in Catholics prayers.\nSo the answer is Yes\nTable 32: 3-Shot Demonstrations for ITER -RETGEN on StrategyQA.\n9274"
}