{
  "title": "A Survey on Model Compression and Acceleration for Pretrained Language Models",
  "url": "https://openalex.org/W4382463788",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2970865618",
      "name": "Canwen Xu",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2041520510",
      "name": "Julian McAuley",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2016384870",
    "https://openalex.org/W3212072588",
    "https://openalex.org/W2883671469",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W2604763608",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W2997710335",
    "https://openalex.org/W2747579829",
    "https://openalex.org/W3113057009",
    "https://openalex.org/W2944701285",
    "https://openalex.org/W3092746498",
    "https://openalex.org/W6800876749",
    "https://openalex.org/W2000045479",
    "https://openalex.org/W6677103964",
    "https://openalex.org/W4285269381",
    "https://openalex.org/W6752515464",
    "https://openalex.org/W2771655537",
    "https://openalex.org/W6736780897",
    "https://openalex.org/W3116594510",
    "https://openalex.org/W3022969335",
    "https://openalex.org/W3012638462",
    "https://openalex.org/W6718053083",
    "https://openalex.org/W3119303959",
    "https://openalex.org/W2964910501",
    "https://openalex.org/W6777017071",
    "https://openalex.org/W2460130460",
    "https://openalex.org/W2969515962",
    "https://openalex.org/W2610140147",
    "https://openalex.org/W6600655081",
    "https://openalex.org/W6785331980",
    "https://openalex.org/W6600339457",
    "https://openalex.org/W6784188432",
    "https://openalex.org/W3167796153",
    "https://openalex.org/W4226126941",
    "https://openalex.org/W3154971029",
    "https://openalex.org/W6601449391",
    "https://openalex.org/W3164115288",
    "https://openalex.org/W2138019504",
    "https://openalex.org/W2991340425",
    "https://openalex.org/W4285202066",
    "https://openalex.org/W3176017841",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W4206410067",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W4285204619",
    "https://openalex.org/W4226087293",
    "https://openalex.org/W4243928383",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W3038012435",
    "https://openalex.org/W3034292689",
    "https://openalex.org/W2963928591",
    "https://openalex.org/W3118423943",
    "https://openalex.org/W4280546523",
    "https://openalex.org/W2962677625",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4287370462",
    "https://openalex.org/W3130315600",
    "https://openalex.org/W4286902761",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3035030897",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W3008851394",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2952902402",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W4297068904",
    "https://openalex.org/W3171750540",
    "https://openalex.org/W2887603965",
    "https://openalex.org/W2767785892",
    "https://openalex.org/W3100985894",
    "https://openalex.org/W3101248447",
    "https://openalex.org/W3101163004",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2903810591",
    "https://openalex.org/W4287777801",
    "https://openalex.org/W4287901267",
    "https://openalex.org/W3105648266",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W2963643655",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4309386164",
    "https://openalex.org/W2982041622",
    "https://openalex.org/W3174702398",
    "https://openalex.org/W4385570274",
    "https://openalex.org/W3103754749",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2866343820",
    "https://openalex.org/W3170113752",
    "https://openalex.org/W3099576124",
    "https://openalex.org/W4287887900",
    "https://openalex.org/W4288090629",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W4287208846",
    "https://openalex.org/W3174708387",
    "https://openalex.org/W4310492983",
    "https://openalex.org/W2970418186",
    "https://openalex.org/W4206634569",
    "https://openalex.org/W2996663285",
    "https://openalex.org/W3035038672",
    "https://openalex.org/W2242818861",
    "https://openalex.org/W4287165195",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W4297689207",
    "https://openalex.org/W3207932789",
    "https://openalex.org/W4226498390",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2997666887",
    "https://openalex.org/W3106070274",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W4288256350",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W4287121309",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W2058641082",
    "https://openalex.org/W3095273266",
    "https://openalex.org/W3199960018",
    "https://openalex.org/W3197901717",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3173374050",
    "https://openalex.org/W3174544005",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3098394279",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W3207612418",
    "https://openalex.org/W3154922002",
    "https://openalex.org/W2963828549",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W3174510164",
    "https://openalex.org/W3098873988",
    "https://openalex.org/W2924902521"
  ],
  "abstract": "Despite achieving state-of-the-art performance on many NLP tasks, the high energy cost and long inference delay prevent Transformer-based pretrained language models (PLMs) from seeing broader adoption including for edge and mobile computing. Efficient NLP research aims to comprehensively consider computation, time and carbon emission for the entire life-cycle of NLP, including data preparation, model training and inference. In this survey, we focus on the inference stage and review the current state of model compression and acceleration for pretrained language models, including benchmarks, metrics and methodology.",
  "full_text": "A Survey on Model Compression and Acceleration\nfor Pretrained Language Models\nCanwen Xu, Julian McAuley\nUniversity of California, San Diego\n{cxu, jmcauley}@ucsd.edu\nAbstract\nDespite achieving state-of-the-art performance on many NLP\ntasks, the high energy cost and long inference delay prevent\nTransformer-based pretrained language models (PLMs) from\nseeing broader adoption including for edge and mobile com-\nputing. Efficient NLP research aims to comprehensively con-\nsider computation, time and carbon emission for the entire\nlife-cycle of NLP, including data preparation, model training\nand inference. In this survey, we focus on the inference stage\nand review the current state of model compression and accel-\neration for pretrained language models, including benchmarks,\nmetrics and methodology.\nIntroduction\nThe recent success of applying pretrained deep Transform-\ners (Vaswani et al. 2017) on different NLP tasks (Devlin et al.\n2019; Raffel et al. 2020; Le Scao et al. 2022) has raised con-\ncerns about its efficiency. The high computational cost also\nprevents these pretrained language models (PLMs) from be-\ning deployed in production (Sun et al. 2020). To address this\nproblem, efficient inference refers to techniques that aim to\nmake inference of an ML model faster (time-efficient), con-\nsume fewer computational resources (computation-efficient),\nless memory (memory-efficient) and less disk space (storage-\nefficient). One popular class of techniques is model com-\npression and acceleration, where a large and slow model is\ncompressed to a lightweight model that can be stored with\nlimited disk space on a mobile device, or accelerated to run\nwith low latency (or both). Also, training a large model and\nthen compressing it to a small one can be efficient for training\nand good for generalization (Li et al. 2020).\nIn addition to technical considerations, large models also\nraise environmental and ethical concerns (Bender et al. 2021).\nLarge models have a high carbon footprint which a com-\npressed model can reduce, potentially with little sacrifice in\nperformance. Meanwhile, large models set obstacles for engi-\nneers and researchers from developing countries who cannot\nafford the necessary hardware for running the model (Ben-\nder et al. 2021). Thus, model compression and acceleration\ncan be critical to make state-of-the-art NLP techniques more\naccessible and facilitate inclusiveness.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nWhat’s covered? In this survey, we aim to highlight the\nmost important works in the field of model compression and\nacceleration for PLMs. We review the metrics, benchmarks,\nand methods, organizing these works in a new taxonomy.\nWidely-used techniques, including weight sharing, low-rank\nfactorization, pruning, quantization, knowledge distillation,\nearly exit, and token skipping, are covered with comparative\nanalysis. We also highlight current challenges and future re-\nsearch directions in the field, calling for community efforts to\nbuild an environmentally-friendly, inclusive and sustainable\nfuture of NLP.\nWhat’s not covered? This survey does not cover (1) meth-\nods that design a new architecture for training from scratch\n(e.g., long-range Transformers, Mixture-of-Experts models);\n(2) data-efficient or parameter-efficient model tuning that fo-\ncuses more on the training efficiency rather than inference\nefficiency (e.g., few-shot learning, prompt learning, partial\nmodel tuning); (3) works that use the techniques surveyed in\nthis paper but for other purposes or are application-specific\n(e.g., self-distillation, representation distillation for retrieval).\nThere have been surveys (Qiu et al. 2020; Han et al. 2021;\nXu et al. 2021b) that cover some aspects of this topic. Dif-\nferent from these works, we focus on the latest progress on\nmodel compression and acceleration for pretrained language\nmodels, highlighting the intersection between language tech-\nnology and efficient ML.\nMetrics and Benchmarks\nMetrics\nThere are various metrics to depict inference efficiency in dif-\nferent dimensions. These metrics are often reported together\nwith accuracy to evaluate an NLP model.\nFloating point operations (FLOPs) directly measures the\nnumber of floating points operations needed for executing an\ninstance. FLOPs can serve as a metric for computational effi-\nciency and is somewhat hardware-agnostic. However, FLOPs\ncannot accurately reflect the real runtime since the degree of\nparallelism (DOP) varies for different algorithms.\nInference time (i.e., delay) is used to measure the runtime\nof an algorithm in its inference stage. Inference time can\nvary on different infrastructures. When testing on the same\narchitecture, compared to FLOPs, inference time can bet-\nter approximate the real-world performance of a system by\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n10566\ntaking parallelism into consideration.\nSpeed-up Ratio is the ratio of inference time of the base-\nline model to the accelerated model. Compared to inference\ntime, speed-up ratio draws a relative comparison which can\nbe roughly compared across different hardware. In some\nworks (Zhou et al. 2020; Sun et al. 2021), speed-up ratio\nis approximated by the ratio of the number of Transformer\nlayers in the baseline model to those used in calculation of\nan acceleration method.\nNumber of Parameters and Model Size are often reported\nin NLP studies as metrics that directly reflect the storage\ncost of a model. This can be important for mobile deploy-\nment of an NLP model due to limited storage on a mobile\ndevice. It can also be an indicator for the memory footprint\nand computational cost for training and inference. An excep-\ntion is models with weight sharing. For example, the FLOPs\nand memory use of ALBERT (Lan et al. 2020) is slightly\nhigher than a BERT model (Devlin et al. 2019) with the same\nnumber of layers. However, since all Transformer layers in\nALBERT share the same weights, the model size of n-layer\nALBERT is only 1/n of n-layer BERT.\nCarbon Footprint measures the environmental impact. La-\ncoste et al. (2019) provide a calculator for CO2 by querying a\ndatabase of carbon emission of mainstream cloud computing\nproviders. Alternatively, Experiment Impact Tracker (Hen-\nderson et al. 2020) and CodeCarbon1 are two plugins that can\nrecord the energy use of hardware and estimate the carbon\nemission based on the geolocation.\nLoyalty/Fidelity Recent works Xu et al. (2021a) and Stanton\net al. (2021) propose loyalty and fidelity, respectively. Both\nare similarity metrics calculated between the predicted dis-\ntributions of the teacher and the student, in a teacher-student\ndistillation or compression setting. Loyalty and fidelity can\nreflect how successful the knowledge transfer is from the\nteacher to the student, providing interpretability and relia-\nbility (Xu et al. 2021a; Stanton et al. 2021), and can be an\nindicator of better generalization in distilling large teacher\nmodels and ensembles (Stanton et al. 2021).\nRobustness Su et al. (2018) find that smaller neural networks\nare more vulnerable to adversarial attacks. Xu et al. (2021a)\nsuggest reporting adversarial robustness in addition to accu-\nracy. In addition to adversarial robustness, Du et al. (2021)\nfind compressed pretrained language models are significantly\nless robust on out-of-distribution (OOD) data.\nBenchmarks\nStandard Benchmarks Most studies evaluate on common\nNLP benchmarks. For example, GLUE (Wang et al. 2019b)\nand SuperGLUE (Wang et al. 2019a) are used for natural\nlanguage understanding (NLU). SQuAD (Rajpurkar et al.\n2016) is used for machine reading comprehension (MRC).\nEfficientQA EfficientQA (Min et al. 2020) is an open-\ndomain question answering benchmark encouraging solu-\ntions that efficiently store and access knowledge with the\nsmallest number of bytes. EfficientQA has three resource-\nrestricted tracks, including two tracks with a 6 GB and 500\n1https://codecarbon.io/\nMB cut-off for system size and one track that ranks the sys-\ntems that achieves 25% accuracy with the smallest size.\nSustaiNLP The shared task of SustaiNLP 2020 (Wang and\nWolf 2020) uses SuperGLUE (Wang et al. 2019a) to evaluate\nthe performance of submissions. There are three tracks that\ntarget different accuracy levels and hardware (2 GPU tracks\nand 1 CPU track). Within each track, submissions are ranked\nby lowest energy consumption, measured by Experiment\nImpact Tracker (Henderson et al. 2020).\nELUE Efficient Language Understanding Evaluation (Liu\net al. 2021) is proposed as an attempt to clearly depict the\nPareto Front of FLOPs versus performance. ELUE consists\nof six datasets of three tasks (sentiment analysis, natural\nlanguage inference, and paraphrasing). ELUE has four tracks\nwith a parameter number cut-off of 40M, 55M, 70M and\n110M. The metric used for evaluation is ELEU score, which\ncalculates an average performance advantage over a baseline\n(ElasticBERT) under different FLOPs.\nMethods\nWeight Sharing\nWeight sharing is based on the assumption that large-scale\nmodels, like Transformer (Vaswani et al. 2017), are over-\nparameterized (Li et al. 2020). Weight sharing provides a way\nto decouple computation and parameters by reusing the same\nparameters for multiple computations. Weight sharing can\nreduce inference memory footprint and number of parameters\nand thus is memory- and storage-efficient.\nEncoder-Decoder Sharing In the vanilla Transformer\nmodel (Vaswani et al. 2017) for neural machine translation\n(NMT), there is one encoder for encoding the input into hid-\nden representations, and one decoder for decoding it to the\ntarget language. Tied Transformer (Xia et al. 2019) shares the\nweights of the encoder and decoder of Transformer. The re-\nsults of Tied Transformer are comparable to the vanilla Trans-\nformer. Rothe, Narayan, and Severyn (2020) leverage pre-\ntrained language model checkpoints to initialize a sequence-\nto-sequence model. They experiment with a shared encoder\nand decoder to reduce memory footprint.\nLayer Sharing In Transformer (Vaswani et al. 2017), both\nthe encoder and decoder are stacks of Transformer layers.\nThus, a simple and straightforward way to share the weights\nin a Transformer is to share them across all Transformer\nlayers. Dabre and Fujita (2019) share the weights across all\nTransformer layers for NMT with minimal performance drop.\nUniversal Transformer (Dehghani et al. 2019) shares the\nweights across all layers, allowing for recurrent computation\nwith a dynamic halting mechanism and achieves better per-\nformance than the vanilla Transformer. ALBERT (Lan et al.\n2020) introduces the idea into pretrained language models for\nnatural language understanding (NLU). Although it cannot\nreduce the computational overheads and has an inevitable\nnegative effect on performance, this design saves up to 95%\nof disk space for storing the model, which can be critical for\ndeployment on mobile devices with limited storage. Takase\nand Kiyono (2021) systematically study strategies for shar-\ning weights across layers. Instead of using the weights of\none Transformer layer for all layers, they aim to explore the\n10567\nHessian-based pruning Magnitude pruning L0 regularization Movement pruning\n(1989) (2016) (2018) (2020)\nDecision 2nd order 0th order 1st order 1st order\nObjective L L L + λl0E(L0) L\nScores S −P\nt( ∂2L\n∂W 2i,j\n)(t)W2(t)\ni,j |Wi,j| − P\nt( ∂L\n∂Wi,j\n)(t)W(t)\ni,j f(S\n(t)\ni,j ) −P\nt( ∂L\n∂Wi,j\n)(t)W(t)\ni,j\nTable 1: A summary of various pruning methods. S are saliency scores used to determine which weights to prune. The table style\nis borrowed from Sanh, Wolf, and Rush (2020).\nbest way to use the parameters of M layers for N layers\n(M < N). Reid, Marrese-Taylor, and Matsuo (2021) intro-\nduce a strategy named “sandwich-style” parameter sharing,\nwhich shares the weights for central layers while leaving the\nfirst and last layers independent.\nLow-Rank Factorization\nThe weight matrices in a neural network are often low-rank,\nindicating redundancy in model weights (Sainath et al. 2013).\nThus, a natural idea is to factorize the weight matrices into\ntwo or more smaller matrices to save parameters. A com-\nmon technique for low-rank factorization is singular value\ndecomposition (SVD). For a matrix\nA ∈ Rm×n, there ex-\nists A = UΣV T, where r ≤ min {m, n} is the rank of\nA; U ∈ Rm×r, V ∈ Rn×r are two orthogonal matrices;\nΣ ∈ Rr×r is a diagonal matrix with only the non-zero singu-\nlar values of A. Thus, the space complexity can be effectively\nreduced from O(mn) to O(mr +rn), improving the storage-\nefficiency of the model.\nDecomposing Linear Layers Low-rank factorization can be\napplied to any linear layer. Grachev, Ignatov, and Savchenko\n(2017) factorize the weights of an LSTM language model.\nFollowing that, Winata et al. (2019) exploit SVD for both\nthe LSTM cell in a language modeling task and a pretrained\nLSTM language model, ELMo (Peters et al. 2018). This is\none of the earliest attempts to compress a pretrained lan-\nguage model. Ma et al. (2019) propose a new self-attention\nmodule, namely multi-linear attention, as a substitute for\nthe standard multi-head attention module in a Transformer.\nThey use block-term tensor decomposition (BTD, Lathauwer\n2008) to factorize multi-head attention. Their results demon-\nstrate comparable performance to the vanilla Transformer\nwhile being parameter-efficient. Noach and Goldberg (2020)\npropose a two-stage approach to compress a pretrained lan-\nguage model. In the first stage, they decompose each weight\nmatrix in the pretrained language model with SVD. Then,\nfor the second stage, they fine-tune or use knowledge dis-\ntillation to refine the weights. Chen et al. (2021) propose\ndata-aware low-rank compression (DRONE) by exploiting\nthe prior knowledge of the data distribution. Instead of mini-\nmizing the reconstruction error of the the weight matrix, they\nminimize the approximation error of the outputs. DRONE\nachieves better performance than SVD. Besides, as an alter-\nnative to SVD, Kronecker decomposition retains the rank of\nthe matrix and has shown improvement compressing BERT\nand GPT-2 (Tahaei et al. 2021; Edalati et al. 2022).\nDecomposing Embedding ALBERT (Lan et al. 2020) uses\nfactorization for the embedding layer, which has redundant\nparameters due to its high input and output dimensions. Since\nthe power of Transformer mainly comes from its contextual\nlearning ability, the parameters in the token embedding layer\nare not efficient. It intuitively makes sense to reduce them by\nfactorizing the embedding matrix. Reid, Marrese-Taylor, and\nMatsuo (2021) propose self-attentive factorized embeddings\n(SAFE) by adding a small self-attention layer on the basis of\nlinear projection to achieve better performance.\nPruning\nPruning (LeCun, Denker, and Solla 1989) aims to re-\nmove unimportant weights from a neural network to\nachieve storage-, memory-efficiency, and sometimes also\ncomputation- and time-efficiency while preserving model per-\nformance. There are two key elements in a pruning method:\n(1) A pruning unit is the atomic unit to be removed from a\nmodel; it can be a single weight (unstructured pruning), an\nattention head or even a Transformer layer (structured prun-\ning). (2) A saliency score is the criterion for making pruning\ndecisions. Based on whether it uses a gradient and which\norder of gradient it uses, we can categorize pruning methods\nto zeroth-order (only considering weight magnitude), first-\norder and second-order approaches. We summarize some\nrepresentative pruning methods in Table 1.\nUnstructured Pruning Unstructured pruning removes\n“unimportant” connections in a neural network by setting\nthe corresponding weights to 0. After pruning, the weight\nmatrix often becomes sparse. To exploit the characteristics\nof a sparse matrix to achieve computation- and memory-\nefficiency, specialized hardware (e.g., sparse tensor cores in\nNvidia A100 (Mishra et al. 2021)) and software (e.g., Py-\nTorch Sparse API2) are necessary. See, Luong, and Manning\n(2016) uses magnitude-based pruning with retraining to com-\npress RNN models for NMT. Magnitude-based pruning (Han,\nMao, and Dally 2016) simply prunes weights with smallest\nmagnitude (i.e., absolute values). After pruning, See, Luong,\nand Manning (2016) continue to fine-tune the pruned network\nto obtain better performance. Narang et al. (2017) prune an\nRNN model gradually during training. The magnitude thresh-\nold for pruning is gradually increased with increasing training\nsteps. Wang et al. (2020b) first prunes and retrains NMT mod-\nels with magnitude pruning and then restores the pruned pa-\nrameters to train the entire network again, in order to achieve\nbetter performance than the original model. Zhang and Stadie\n(2020) propose a one-shot pruning technique based on the\n2https://pytorch.org/docs/stable/sparse.html\n10568\nJacobian spectrum. Different from iterative pruning methods,\none-shot pruning techniques only prune a network once and\nthen use standard training to train the sparse network.\nSome recent works target transfer learning as it has be-\ncome the new standard paradigm in NLP. Gordon, Duh, and\nAndrews (2020) aim to reveal how pruning affects transfer\nlearning. They find that low levels of pruning (30%–40%) do\nnot affect pretraining loss or transfer to downstream tasks at\nall. However, further pruning has a negative impact on both\npretraining and transfer learning. A high level of pruning\ncan even prevent the model from fitting downstream datasets.\nSanh, Wolf, and Rush (2020) claim that magnitude pruning\nis suboptimal for transfer learning. They propose movement\npruning as a simple first-order method for fine-tuning of pre-\ntrained language models. Instead of preserving weights that\nare currently far from zero, they retain those that are moving\naway from zero (i.e., gaining larger magnitude) during fine-\ntuning, achieving better performance for pruning BERT. Guo,\nRush, and Kim (2021) propose diff pruning, by learning a\ntask-specific “diff” vector that extends the original pretrained\nparameters. The task-specific “diff” vectors are trained with\nL0 regularization (Louizos, Welling, and Kingma 2018) to\nencourage sparsity. By updating only 0.5 % of parameters,\ndiff pruning achieves similar performance to fine-tuning the\nwhole network.\nStructured Pruning Structured pruning removes weight\nblocks, rows, attention heads, or layers from a model. Com-\npared to unstructured pruning, it can usually achieve acceler-\nation and memory reduction without specialized hardware or\nsoftware. Narang, Undersander, and Diamos (2017) extends\ngradual pruning for RNNs (Narang et al. 2017) to struc-\ntured pruning. They first divide weights into blocks, then\nprune blocks of weights in a layer using group lasso regu-\nlarization (Yuan and Lin 2006) to create blocks of zeros in\nweight matrices. Michel, Levy, and Neubig (2019) and V oita\net al. (2019) find that the multi-head attention in Transformer\nhas redundancy. Both works use a first-order approach to\nremove attention heads from Transformer. Following this,\nFan, Grave, and Joulin (2020) propose a structured dropout\nstrategy named LayerDrop. When training a Transformer, a\nrandom dropout is applied to each layer. After one-off train-\ning, the model can be pruned on-demand to achieve the target\ninference speed. SNIP (Lin et al. 2020) removes unimportant\nnon-linear terms in the residual connections, whose magni-\ntude is below a threshold. Lagunas et al. (2021) introduce\na block pruning approach that extends structured methods\nby considering blocks of any size. They integrate this struc-\nture into movement pruning (Sanh, Wolf, and Rush 2020)\nand find this approach can automatically learn to prune out\nfull components in Transformer, e.g., an attention head. Xia,\nZhong, and Chen (2022) propose CoFi, a pruning method\nthat jointly prunes both coarse-grained units (e.g., layers)\nand fine-grained units (e.g., attention head and hidden units).\nCoFi also introduces a distillation loss to further improve its\nperformance.\nLottery Ticket Hypothesis Frankle and Carbin (2019)\npropose the “lottery ticket hypothesis”: dense, randomly-\ninitialized, feed-forward networks contain subnetworks (win-\nning tickets) that — when trained in isolation — reach test\naccuracy comparable to the original network in a similar\nnumber of iterations. It reveals that retraining from remain-\ning weights (Han, Mao, and Dally 2016) in a pruned network\nis not necessary for the pruned network. In contrary, a “win-\nning ticket” can always learn better, even when training from\nscratch (as long as it is initialized with the same random\nweights). Following this, Chen et al. (2020) verify the lottery\nticket hypothesis on BERT with iterative magnitude pruning.\nThey find that subnetworks found on the pretraining task\n(i.e., masked language modeling, MLM) transfer universally\nto downstream tasks whereas those found on downstream\ntasks do not. Prasanna, Rogers, and Rumshisky (2020) also\nverify the lottery ticket hypothesis with BERT, for both mag-\nnitude and structured pruning. They find that even the worst\nsubnetwork in BERT remains highly trainable, suggesting\nthat the weights of BERT may have relatively low redun-\ndancy. This seems to be consistent with previous finding on\nover-parameterized models (Nakkiran et al. 2020).\nQuantization\nQuantization aims to compress a neural network by reduc-\ning the number of bits (i.e., precision) in the weights of\nthe model, improving storage-, memory-, computation-, and\ntime-efficiency (on most hardware). In general, quantization\ncan be further categorized into post-training quantization\n(PTQ) and quantization-aware training (QAT). PTQ rescales\nthe weights of a trained model whereas QAT introduces the\nrounding error into the training process. Due to the consider-\nable performance drop for PTQ, most works in compressing\nNLP models unanimously use QAT, in order to achieve com-\nparable performance with the full-precision model.\n8-Bit Quantization Quantizing models from full precision\nfloats (FP32) to 8-bit integers (INT8) is a classical setting,\nsince operations including matrix multiplication can be cal-\nculated much faster with INT8 than FP32, especially on\na CPU. Zafrir et al. (2019) use symmetric linear quantiza-\ntion (Jacob et al. 2018) to quantize both weights and activa-\ntions to INT8 dynamically. They also explore quantization-\naware training (QAT) for BERT. They use fake quantiza-\ntion (Jacob et al. 2018) to introduce quantization error into\nthe model during training phase to simulate the rounding\neffect. They use Straight-Through Estimator (STE) (Ben-\ngio, L\n´eonard, and Courville 2013) to estimate the gradient\nof the non-differentiable fake quantization. They find that\ndynamic post-training quantization hurts the downstream\nperformance slightly while QAT achieves comparable per-\nformance to the original model. Similarly, Prato, Charlaix,\nand Rezagholizadeh (2020) apply QAT with STE for Trans-\nformers on neural machine translation and achieve results\nthat are similar to the original model. I-BERT (Kim et al.\n2021) eliminates floating point calculation in activation by\nexploiting lightweight integer-only approximation methods\nfor non-linear operations (e.g., GELU, Softmax and Lay-\nerNorm) in BERT. The resulting I-BERT model is capable\nof doing pure INT8 inference thus has a better acceleration\nratio.\nLower-Bit Quantization Recent works aim to push quanti-\nzation to even lower precision. Lower-bit quantization faces\nmore challenges, including difficulty to optimize, and lack of\n10569\nGT\nSoft loss\nHard loss\nGT\nSoft loss\nHard loss\nGT\nSoft loss\nHard loss\nSoft+Attnloss\nSoft loss\nAttn loss\nMSE\nAttn + VR\nloss\nAttn loss\nKL\nVR loss\nKL\n<latexit sha1_base64=\"DjD4VQN0/0XA7ZS07M6lSbAlqtY=\">AAACQXicbVDPaxNBGJ2tv2r80dQePTgYhHgJu0Vqj4FePFZo0kAmhtnZb5MhszvjzLdiGPb/6qk9CF4Fb7330oMiXr04mxTU1gcDj/e+j+/NS42SDuP4PNq4dfvO3Xub91sPHj56vNXefjJ0urICBkIrbUcpd6BkCQOUqGBkLPAiVXCcLg4a//gDWCd1eYRLA5OCz0qZS8ExSNP2iGkDlqO2JS/AO51jwT/WTEGOXZZbLjwrOM7T3A9rykSmkf4R3jHUpvbMvbfos6lf1HXNrJzN8eW03Yl78Qr0JkmuSKf/zJx8Oft8dDhtf2WZFlUBJQrFnRsnscGJ5xalUFC3WOXAcLHgMxgH2sR1E79qoKYvgpLRXNvwSqQr9e8NzwvnlkUaJpvw7rrXiP/zxhXm+xMvS1MhlGJ9KK8URU2bOmkmLQhUy0C4sDJkpWLOQ20YSm+FEpLrX75Jhru9ZK/36m3S6XfJGpvkKXlOuiQhr0mfvCGHZEAEOSUX5Bv5Hn2KLqMf0c/16EZ0tbND/kH06zeAN7gi</latexit> \nsoftmax\n⇣\nV·V>\npdk\n⌘\nGT\nRandom\nreplacing\nHard loss\n(a) DistilBERT (c) PKD-Last\n(d) PKD-Skip(b) BERT-of-Theseus\n(e) TinyBERT\n(f) MiniLM\nFigure 1: A summary of different KD approaches. “GT” rep-\nresents ground-truth labels.\nmodel expressivity. Shen et al. (2020) propose a group-wise\nquantization scheme and use second-order Hessian-based\nmixed-precision method (Dong et al. 2019) to quantize BERT\ndown to 2 bits. They claim that weights corresponding to each\nneuron could lie in different ranges of real numbers. For ex-\nample, for a multi-head self-attention module, they split the\nweight matrix to 12 groups, with respect to each attention\nhead. Then they further split each group and have a total num-\nber of 128 subgroups, each of which has its own quantization\nrange. GOBO (Zadeh et al. 2020) separates the weights into\ntwo groups — Gaussian and outliers where the former is\nquantized to 3 bits and the latter remains a full-precision\nfloat (FP32). TernaryBERT (Zhang et al. 2020) combines\napproximation-aware and loss-aware ternarization (i.e., using\nonly {−1, 0, 1} for weights) methods with different granu-\nlarity for different components in BERT. They further add\nknowledge distillation to improve the performance of QAT.\nBai et al. (2021) observe a large performance drop from a\nternary network to a binary network when trained directly,\ndue to its loss landscape. They propose ternary weight split-\nting, which initializes BinaryBERT by splitting a half-sized\nternary network into two binary networks. The initialized net-\nwork inherits good performance and can be further fine-tuned\nwithout optimization difficulties. Tao et al. (2022) analyze the\nreasons why quantization is less effective on generative LMs\n(e.g., GPT-2, BART). They conclude that homogeneous word\nembeddings caused by reduced capacity and varied distribu-\ntion of weights are responsible for the failure. They propose\na token-level contrastive distillation and a module-wise dy-\nnamic scaling mechanism to mitigate these two problems.\nKnowledge Distillation\nKnowledge Distillation (Hinton et al. 2015) is a widely used\ntechnique to transfer knowledge from a large model (teacher)\nto a smaller one (student) to achieve all types of efficiency.\nKD usually requires designing a loss function to minimize\nthe distance of the output or intermediate features between\nthe student and the teacher. As illustrated in Figure 1, we\nsummarize the designs of loss functions used in recent works\ndistilling NLP models. Based on the loss function designs,\nwe can further categorize the methods into logit-based KD,\nfeature-based KD, KD with a dynamic target, and module\nreplacing.\nLogit-based KD Following Hinton et al. (2015), logit-based\nKD methods are the first attempts to distill a large pretrained\nlanguage model into a smaller one to improve its efficiency.\nLogit-based KD uses the KL divergence or mean squared\nerror (MSE) to minimize the logits between the student and\nthe teacher. Tang et al. (2019) distills fine-tuned BERT into\na BiLSTM model in a task-specific setting. The resulting\nBiLSTM outperforms its counterpart trained without KD by\na large margin. DistilBERT (Sanh et al. 2019) distills BERT\nin a pretraining setting on the task of masked language mod-\neling (MLM). The loss is a combination of three components:\nthe original MLM loss, cosine distance, and KL divergence.\nAfter distillation, the model can be fine-tuned and perform\ndownstream tasks. Turc et al. (2019) explore the effect of\ninitialization for the student. They find that a student BERT\npretrained with MLM outperforms random initialization and\ntruncated teacher (Sanh et al. 2019; Sun et al. 2019) when\nused to initialize the student model. Liang et al. (2021) use\nMixUp (Zhang et al. 2018) for data augmentation to distill\nBERT.\nFeature-based KD Instead of using only the final output,\nfeature-based KD aims to align the intermediate features be-\ntween the teacher and the student. PKD (Sun et al. 2019)\nintroduces an MSE loss between layer representations. As\nshown in Figure 1(c) and (d), they propose two strategies: one\naligns the student with the last few layers in the teacher (PKD-\nLast) and the other learns the teacher’s representations in ev-\nery 2 layers (PKD-Skip). The latter strategy performs slightly\nbetter in experiments. A similar technique is also presented in\nAguilar et al. (2020). On the basis of PKD, TinyBERT (Jiao\net al. 2020) further introduces an attention loss that aims to\nalign the attention matrices in layers between the teacher\nand the student, as illustrated in Figure 1(e). TinyBERT also\ndemonstrates that performing KD in both pretraining and fine-\ntuning stages can improve the performance of KD. Similarly,\nMiniLM (Wang et al. 2020a, 2021) aligns the attention matrix\nand value-value scaled dot-product (i.e., value relation loss,\nas shown in Figure 1(f)). The added feature complements the\nattention matrix (i.e., queries-keys scaled dot-product) and\nallows the complete transfer of multi-head self-attention. Wu,\nWu, and Huang (2021) propose a multi-teacher distillation\nframework that use both intermediate features and soft la-\nbels from multiple teachers to distill a student and achieve\nbetter performance. DynaBERT (Hou et al. 2020) uses layer-\nwise KD loss to distill a teacher into a student model that\nhas sub-networks of different widths and depths. Thus, the\nsame model can be used on various devices with different\ncomputing budgets. MobileBERT (Sun et al. 2020) redesigns\na BERT architecture that is suitable for mobile devices. In\naddition to the layer-wise feature distillation (Sun et al. 2019)\nand attention distillation (Jiao et al. 2020), they introduce a\nprogressive knowledge transfer mechanism by distilling the\nmodel layer by layer, instead of altogether. Liu et al. (2022)\n10570\nMethod Exit criterion\nDeeBERT entropy < θ\nRightTool calibrated max class probability > θ\nFastBERT entropy < θ\nRomeBERT entropy < θ\nSkipBERT max class probability > θ\nPABEE patience (#consistent prediction > θ)\nV oting accumulated votes > θ\nLeeBERT patience (#consistent prediction > θ)\nPast-Future entropy < θ\nPCEE-BERT patience (#consistent IC confidence > θ)\nBERxiT estimated confidence > θ\nCAT estimated conformity > θ\nTable 2: A summary of three types of early exit methods:\nconfidence estimation, internal ensemble, and learning to\nexit. θ is a predefined threshold for exiting.\nexploit structural features on both token and span levels to\nalign the student with the teacher.\nModule Replacing A special case of KD is BERT-of-\nTheseus (Xu et al. 2020). As shown in Figure 1(b), BERT-of-\nTheseus does not apply any knowledge transfer loss to mini-\nmize the distance between the student and the teacher. Instead,\nthey freeze the teacher modules and train a hybrid model by\nrandomly replacing some modules in the teacher model with\nstudent modules. They further design a linear scheduler to\nincrease the probability of replacement to bridge the gap be-\ntween training and inference. Following this, Sparse Progres-\nsive Distillation (Huang et al. 2022) uses layer-wise KD to\niteratively prune the student modules while randomly replac-\ning each module in the teacher model with its corresponding\nstudent module with a fixed probability. After the target spar-\nsity is hit, the replacing rate is progressively increased to\n1. This method combines feature-based KD, module replac-\ning, and pruning, achieving a super-teacher performance on\nGLUE (Wang et al. 2019b).\nKD with Dynamic Targets In traditional KD, the teacher\nserves as a static target for the student to match, without any\nupdate during distillation. However, this can be suboptimal\nsince the teacher is unaware of the student or its goal to trans-\nfer the knowledge to the student. ProKT (Shi et al. 2020)\nprojects the supervision signals of a teacher model into the\nstudent’s parameter space by decomposing the training objec-\ntive into local intermediate targets with approximate mirror\ndescent (Beck and Teboulle 2003). Zhou, Xu, and McAuley\n(2022) propose a simpler framework with meta learning to\nallow the teacher to adapt itself for better knowledge transfer.\nThe student is evaluated on a “quiz” set after a few training\nsteps and provides feedback to the teacher. Its first-order vari-\nant can further improve training speed and reduces memory\nfootprint (Ma et al. 2022).\nEarly Exit\nEarly exit (EE) does not reduce the size (the total number\nof parameters) of the model. Instead, EE accelerates model\ninference by terminating inference at a particular layer based\non some criteria. Although it does not make a model smaller\nit can reduce computation and achieve acceleration. Early\nexit inserts internal classifiers (which are often simple linear\nlayers) into a large network as triggers for early exiting. The\nkey element in early exit methods is the exit criterion. There\nare three types of exit criteria: confidence estimation, internal\nensemble and learning to exit. We summarize the exit criteria\nin Table 2.\nConfidence Estimation Previous works in computer vi-\nsion (Park et al. 2015; Teerapittayanon, McDanel, and Kung\n2016; Kaya, Hong, and Dumitras 2019) define a metric as\nthe proxy for confidence of prediction. The inference can exit\nearly if the confidence reaches a threshold at an early layer.\nThis idea is then applied to pretrained LMs (Xin et al. 2020).\nFor each Transformer layer, a linear internal classifier (IC) is\ninserted after the Transformer layer. When doing inference,\nthe model exits early when an IC outputs a predicted proba-\nbility with an entropy lower than the threshold. A similar ap-\nproach is proposed in RightTool (Schwartz et al. 2020). The\ntemperature-calibrated maximum class probability is used\nas confidence. FastBERT (Liu et al. 2020) distills the output\nfinal classifier into earlier classifiers for better performance.\nFollowing that, RomeBERT (Geng et al. 2021) proposes gra-\ndient regularization to facilitate the KD. SkipBERT (Wang\net al. 2022) replaces lower BERT layers with pre-computed\nrepresentation of text chunks and uses confidence-based early\nexit for higher layers to achieve maximum acceleration.\nInternal Ensemble A weakness of confidence estimation\nis poor utilization of computation. When confidence of an\ninternal classifier fails to satisfy the exit criterion, all rele-\nvant computation becomes invalid. Reusing the results from\nprevious layers to improve the qualify of early exit can be a\npromising direction. Internal ensemble approaches consider\noutputs and predictions from multiple internal classifiers to\nmake better decisions. This is similar to ensemble learning,\nonly it is within a single model.\nThe first work of internal ensemble, PABEE (Zhou et al.\n2020), draws a comparison between overfitting in training\nand overthinking in inference and adapts early stopping for\ninference. When doing inference, the model will exit once\nmultiple consecutive internal classifiers make the same pre-\ndiction. The threshold, namely patience, is a hyperparameter\nthat can be adjusted to achieve different trade-off between\naccuracy and speed. Besides improvement on performance\nand efficiency, PABEE achieves better adversarial robustness,\nwhich the authors attribute to the ensemble effect of internal\nensemble. Sun et al. (2021) propose a diversity loss that en-\ncourages ICs to have diverse probability distributions. Then,\nthey use a voting mechanism to internally ensemble the clas-\nsifiers. Every IC has one vote in final prediction. The model\nwill exit when one class has accumulates enough votes. Lee-\nBERT (Zhu 2021) promotes consistency of IC predictions by\ndistilling them mutually. Then, it follows PABEE’s patience-\nbased strategy to decide when to exit. Liao et al. (2021)\nintroduce a more elaborate mechanism for internal ensemble.\nThey first train “imitation learners”, which are linear layers\nthat predict the hidden states of future layers based on hid-\nden states that are already calculated. PCEE-BERT (Zhang\net al. 2022) combines patience-based exit with confidence\nestimation and terminates inference when multiple layers are\n10571\nMethod Optimization Feature Operation\nPoWER soft masking attention discarded\nTR-BERT RL hidden states forwarded\nLAT soft masking attention forwarded\nLTP soft masking attention discarded\nTranskimmer re-param. hidden states forwarded\nTable 3: A summary of token skipping methods. This table is\nadapted from Guan et al. (2022).\nconfident.\nLearning to Exit Other works use a learning-based approach\nto make exit decisions. BERxiT (Xin et al. 2021) trains a\nlinear learning-to-exit (LTE) module to predict whether the\ncurrent IC prediction is correct. CAT (Schuster et al. 2021)\nproposes a “meta consistency classifier” to predict whether\nthe current IC prediction matches the final classifier and exits\nwhen the consistency classifier predicts a level of conformity\nhigher than the threshold.\nToken Skipping\nSimilar to early exit, token skipping dynamically accelerates\na PLM without reducing its size. The general idea is to skip\nsome tokens for higher layers based on their importance. A\nsummary of these methods is shown in Table 3.\nPoWER-BERT (Goyal et al. 2020) drops a portion of to-\nkens between each Transformer layer based on the attention\nreceived by each token. The number of tokens to drop at\neach layer (i.e., schedule) is learned by jointly optimizing the\nsparsity of a soft mask layer with the original loss function.\nThis approach obtains better Pareto curves of accuracy-time\ntrade-off. TR-BERT (Ye et al. 2021) introduces a dynamic\nmechanism for making decisions of skipping tokens. It is\ntrained with reinforcement learning with a reward that pro-\nmotes classifier confidence and penalizes the number of re-\ntained tokens. Different from PoWER-BERT, the skipped\ntokens are forwarded to the last layer instead of getting re-\nmoved. Length-Adaptive Transformer (LAT, Kim and Cho\n2021) introduces LengthDrop that randomly skips tokens\nduring pretraining to mitigate the gap between pretraining\nand fine-tuning. The schedule of LAT is searched with an\nevolutionary search algorithm. LTP (Kim et al. 2022) learns\na threshold for each Transformer layer. Instead of following\na schedule to drop a specific number of tokens, LTP simply\ndrops tokens with a saliency score (received attention) lower\nthan the learned threshold. Transkimmer (Guan et al. 2022)\nadds a skim predictor module, consisting of a small MLP and\nGumbel-Softmax reparameterization before each layer. The\nskim predictors output a mask deciding whether to drop a\ntoken. It also employs a skim loss that optimizes the ratio of\nskipped tokens to the total number of tokens to encourage\nsparsity.\nChallenges and Future Directions\nWhich Technique to Use? A common question asked is how\nto decide which technique to use in practice? Unfortunately,\nthere is no silver bullet given that we need to take the task,\nReduce \nmodel size?\nHardware? Longer or \nDeeper?\nUnstructured\nPruning\nQuantization Knowledge\nDistillation\nStructured\nPruning\nSupport\nsparsity Low-\nprecision\nGeneral\nHardware\nEarly Exit\nToken \nSkipping\nBackbone\nis deep\nSequence \nis longSpeed or \naccuracy?\nWeight\nSharing\nNoYes\nLow-Rank\nFactorization\nInference\nSpeed Accuracy\nStatic (input-agnostic) Dynamic (input-aware)\nFigure 2: An oversimplified decision tree for model compres-\nsion and acceleration.\ndata, backbone, and hardware into consideration. We provide\nan oversimplified decision tree (as shown in Figure 2) only\nas a starting point. Note that these techniques can often be\ncombined for better results (to be discussed shortly).\nEvaluation Although there have been benchmarks proposed\nfor evaluating model compression and acceleration as intro-\nduced earlier, there are several drawbacks in current eval-\nuation. First, there is no generally recognized setting for\nevaluation of model compression and acceleration. Different\nstudies often yield models with different speed-up ratio, num-\nber of parameters and accuracy. Thus, it is often difficult to\ndirectly compare them, not to mention differences in hard-\nware. Second, general NLU benchmarks like GLUE (Wang\net al. 2019b) or SuperGLUE (Wang et al. 2019a) may not\nbe the best to represent more common tasks on a mobile\ndevice. Tasks like intention detection, dense retrieval, and\nspam classification could be more representative.\nCombining Techniques Although there have been attempts\nat combining multiple model compression and acceleration\ntechniques (Kim and Awadalla 2020; Sanh, Wolf, and Rush\n2020; Xu et al. 2021a), there is a lack of comprehensive and\nsystematic study for combining compression techniques for\nbetter performance and efficiency. Constructing a best prac-\ntice to compress a large model can be useful for practitioners.\nExplainability and Robustness Recent works (Stanton\net al. 2021; Xu et al. 2021a) cast doubt on the explainability\nof model compression and acceleration. Meanwhile, recent\nworks (Du et al. 2021; Xu et al. 2021a) report negative effects\nof model compression on robustness. Explainable and robust\ncompression methods can be important for applications of\nmodel compression and acceleration. Also, explainable and\nrobust compression minimizes effort to re-evaluate the com-\npressed model, and thus can be reliable and predictable in\nproduction (Stanton et al. 2021; Xu et al. 2021a).\nMinimizing Human Effort Current compression and accel-\neration approaches still largely rely on human heuristics to\nachieve good performance. For example, knowledge distil-\nlation often requires an elaborately designed loss function;\npruning relies on the saliency score; weight sharing and low-\nrank factorization involve expertise to appoint modules for\nsharing or factorization. One promising direction could be\n10572\napplying Meta Learning (Finn, Abbeel, and Levine 2017) or\nNeural Architecture Search (Liu, Simonyan, and Yang 2019)\nto model compression and acceleration, to minimize the need\nfor hyperparameters and human design.\nAcknowledgments\nWe appreciate the insightful comments from the anonymous\nreviewers. We would like to thank Wangchunshu Zhou, Sheng\nShen, Zi Lin for discussion and proofreading. This work is\nsupported by NSF Award #1750063.\nReferences\nAguilar, G.; Ling, Y .; Zhang, Y .; et al. 2020. Knowledge\nDistillation from Internal Representations. In AAAI.\nBai, H.; Zhang, W.; Hou, L.; et al. 2021. BinaryBERT: Push-\ning the Limit of BERT Quantization. In ACL-IJCNLP.\nBeck, A.; and Teboulle, M. 2003. Mirror descent and nonlin-\near projected subgradient methods for convex optimization.\nOperations Research Letters.\nBender, E. M.; Gebru, T.; McMillan-Major, A.; and\nShmitchell, S. 2021. On the Dangers of Stochastic Parrots:\nCan Language Models Be Too Big? In FAccT.\nBengio, Y .; L´eonard, N.; and Courville, A. 2013. Estimat-\ning or propagating gradients through stochastic neurons for\nconditional computation. arXiv preprint arXiv:1308.3432.\nChen, P.-H.; Yu, H.-F.; Dhillon, I.; and Hsieh, C.-J. 2021.\nDRONE: Data-aware Low-rank Compression for Large NLP\nModels. In NeurIPS.\nChen, T.; Frankle, J.; Chang, S.; et al. 2020. The Lot-\ntery Ticket Hypothesis for Pre-trained BERT Networks. In\nNeurIPS.\nDabre, R.; and Fujita, A. 2019. Recurrent Stacking of Layers\nfor Compact Neural Machine Translation Models. In AAAI.\nDehghani, M.; Gouws, S.; Vinyals, O.; et al. 2019. Universal\nTransformers. In ICLR.\nDevlin, J.; Chang, M.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In NAACL-HLT.\nDong, Z.; Yao, Z.; Gholami, A.; et al. 2019. HAWQ: Hes-\nsian AWare Quantization of Neural Networks With Mixed-\nPrecision. In ICCV.\nDu, M.; Mukherjee, S.; Cheng, Y .; et al. 2021. What\ndo Compressed Large Language Models Forget? Robust-\nness Challenges in Model Compression. arXiv preprint\narXiv:2110.08419.\nEdalati, A.; Tahaei, M. S.; Rashid, A.; et al. 2022. Kronecker\nDecomposition for GPT Compression. In ACL.\nFan, A.; Grave, E.; and Joulin, A. 2020. Reducing Trans-\nformer Depth on Demand with Structured Dropout. In ICLR.\nFinn, C.; Abbeel, P.; and Levine, S. 2017. Model-Agnostic\nMeta-Learning for Fast Adaptation of Deep Networks. In\nICML, Proceedings of Machine Learning Research.\nFrankle, J.; and Carbin, M. 2019. The Lottery Ticket Hy-\npothesis: Finding Sparse, Trainable Neural Networks. In\nICLR.\nGeng, S.; Gao, P.; Fu, Z.; and Zhang, Y . 2021. Rome-\nbert: Robust training of multi-exit bert. arXiv preprint\narXiv:2101.09755.\nGordon, M. A.; Duh, K.; and Andrews, N. 2020. Com-\npressing BERT: Studying the Effects of Weight Pruning on\nTransfer Learning. In RepL4NLP@ACL.\nGoyal, S.; Choudhury, A. R.; Raje, S.; et al. 2020. PoWER-\nBERT: Accelerating BERT Inference via Progressive Word-\nvector Elimination. In ICML.\nGrachev, A. M.; Ignatov, D. I.; and Savchenko, A. V . 2017.\nNeural Networks Compression for Language Modeling. In\nPReMI.\nGuan, Y .; Li, Z.; Leng, J.; et al. 2022. Transkimmer: Trans-\nformer Learns to Layer-wise Skim. In ACL.\nGuo, D.; Rush, A. M.; and Kim, Y . 2021. Parameter-Efficient\nTransfer Learning with Diff Pruning. In ACL-IJCNLP.\nHan, S.; Mao, H.; and Dally, W. J. 2016. Deep Compression:\nCompressing Deep Neural Network with Pruning, Trained\nQuantization and Huffman Coding. In ICLR.\nHan, X.; Zhang, Z.; Ding, N.; et al. 2021. Pre-trained models:\nPast, present and future. AI Open.\nHenderson, P.; Hu, J.; Romoff, J.; et al. 2020. Towards the\nsystematic reporting of the energy and carbon footprints of\nmachine learning. Journal of Machine Learning Research.\nHinton, G.; Vinyals, O.; Dean, J.; et al. 2015. Distill-\ning the knowledge in a neural network. arXiv preprint\narXiv:1503.02531.\nHou, L.; Huang, Z.; Shang, L.; et al. 2020. DynaBERT:\nDynamic BERT with Adaptive Width and Depth. InNeurIPS.\nHuang, S.; Xu, D.; Yen, I. E.; et al. 2022. Sparse Progres-\nsive Distillation: Resolving Overfitting under Pretrain-and-\nFinetune Paradigm. In ACL.\nJacob, B.; Kligys, S.; Chen, B.; et al. 2018. Quantization and\nTraining of Neural Networks for Efficient Integer-Arithmetic-\nOnly Inference. In CVPR.\nJiao, X.; Yin, Y .; Shang, L.; et al. 2020. TinyBERT: Distilling\nBERT for Natural Language Understanding. In EMNLP\n(Findings).\nKaya, Y .; Hong, S.; and Dumitras, T. 2019. Shallow-Deep\nNetworks: Understanding and Mitigating Network Overthink-\ning. In ICML.\nKim, G.; and Cho, K. 2021. Length-Adaptive Transformer:\nTrain Once with Length Drop, Use Anytime with Search. In\nACL-IJCNLP.\nKim, S.; Gholami, A.; Yao, Z.; et al. 2021. I-BERT: Integer-\nonly BERT Quantization. In ICML.\nKim, S.; Shen, S.; Thorsley, D.; et al. 2022. Learned Token\nPruning for Transformers. In KDD.\nKim, Y . J.; and Awadalla, H. H. 2020. Fastformers: Highly\nefficient transformer models for natural language understand-\ning. arXiv preprint arXiv:2010.13382.\nLacoste, A.; Luccioni, A.; Schmidt, V .; and Dandres, T. 2019.\nQuantifying the carbon emissions of machine learning. arXiv\npreprint arXiv:1910.09700.\n10573\nLagunas, F.; Charlaix, E.; Sanh, V .; and Rush, A. M. 2021.\nBlock Pruning For Faster Transformers. In EMNLP.\nLan, Z.; Chen, M.; Goodman, S.; et al. 2020. ALBERT: A\nLite BERT for Self-supervised Learning of Language Repre-\nsentations. In ICLR.\nLathauwer, L. D. 2008. Decompositions of a Higher-Order\nTensor in Block Terms - Part II: Definitions and Uniqueness.\nSIAM J. Matrix Anal. Appl.\nLe Scao, T.; Fan, A.; Akiki, C.; et al. 2022. BLOOM: A\n176B-Parameter Open-Access Multilingual Language Model.\narXiv preprint arXiv:2211.05100.\nLeCun, Y .; Denker, J. S.; and Solla, S. A. 1989. Optimal\nBrain Damage. In NIPS.\nLi, Z.; Wallace, E.; Shen, S.; et al. 2020. Train Large, Then\nCompress: Rethinking Model Size for Efficient Training and\nInference of Transformers. In ICML.\nLiang, K. J.; Hao, W.; Shen, D.; et al. 2021. MixKD: Towards\nEfficient Distillation of Large-scale Language Models. In\nICLR.\nLiao, K.; Zhang, Y .; Ren, X.; et al. 2021. A Global Past-\nFuture Early Exit Method for Accelerating Inference of Pre-\ntrained Language Models. In NAACL-HLT.\nLin, Z.; Liu, J. Z.; Yang, Z.; et al. 2020. Pruning Redundant\nMappings in Transformer Models via Spectral-Normalized\nIdentity Prior. In EMNLP (Findings).\nLiu, C.; Tao, C.; Feng, J.; and Zhao, D. 2022. Multi-\nGranularity Structural Knowledge Distillation for Language\nModel Compression. In ACL.\nLiu, H.; Simonyan, K.; and Yang, Y . 2019. DARTS: Differ-\nentiable Architecture Search. In ICLR.\nLiu, W.; Zhou, P.; Wang, Z.; et al. 2020. FastBERT: a Self-\ndistilling BERT with Adaptive Inference Time. In ACL.\nLiu, X.; Sun, T.; He, J.; et al. 2021. Towards Efficient NLP:\nA Standard Evaluation and A Strong Baseline. arXiv preprint\narXiv:2110.07038.\nLouizos, C.; Welling, M.; and Kingma, D. P. 2018. Learning\nSparse Neural Networks through L 0 Regularization. In\nICLR.\nMa, X.; Wang, J.; Yu, L.; and Zhang, X. 2022. Knowledge\nDistillation with Reptile Meta-Learning for Pretrained Lan-\nguage Model Compression. In COLING.\nMa, X.; Zhang, P.; Zhang, S.; et al. 2019. A Tensorized\nTransformer for Language Modeling. In NeurIPS.\nMichel, P.; Levy, O.; and Neubig, G. 2019. Are Sixteen\nHeads Really Better than One? In NeurIPS.\nMin, S.; Boyd-Graber, J. L.; Alberti, C.; et al. 2020. NeurIPS\n2020 EfficientQA Competition: Systems, Analyses and\nLessons Learned. In NeurIPS (Competition and Demos).\nMishra, A.; Latorre, J. A.; Pool, J.; et al. 2021. Ac-\ncelerating sparse deep neural networks. arXiv preprint\narXiv:2104.08378.\nNakkiran, P.; Kaplun, G.; Bansal, Y .; et al. 2020. Deep\nDouble Descent: Where Bigger Models and More Data Hurt.\nIn ICLR.\nNarang, S.; Diamos, G.; Sengupta, S.; and Elsen, E. 2017.\nExploring Sparsity in Recurrent Neural Networks. In ICLR.\nNarang, S.; Undersander, E.; and Diamos, G. 2017.\nBlock-sparse recurrent neural networks. arXiv preprint\narXiv:1711.02782.\nNoach, M. B.; and Goldberg, Y . 2020. Compressing Pre-\ntrained Language Models by Matrix Decomposition. In\nAACL-IJCNLP.\nPark, E.; Kim, D.; Kim, S.; et al. 2015. Big/little deep neural\nnetwork for ultra low power inference. In CODES+ISSS.\nPeters, M. E.; Neumann, M.; Iyyer, M.; et al. 2018. Deep\nContextualized Word Representations. In NAACL-HLT.\nPrasanna, S.; Rogers, A.; and Rumshisky, A. 2020. When\nBERT Plays the Lottery, All Tickets Are Winning. In\nEMNLP.\nPrato, G.; Charlaix, E.; and Rezagholizadeh, M. 2020. Fully\nQuantized Transformer for Machine Translation. In EMNLP\n(Findings).\nQiu, X.; Sun, T.; Xu, Y .; et al. 2020. Pre-trained models\nfor natural language processing: A survey. Science China\nTechnological Sciences.\nRaffel, C.; Shazeer, N.; Roberts, A.; et al. 2020. Exploring\nthe Limits of Transfer Learning with a Unified Text-to-Text\nTransformer. J. Mach. Learn. Res.\nRajpurkar, P.; Zhang, J.; Lopyrev, K.; and Liang, P. 2016.\nSQuAD: 100, 000+ Questions for Machine Comprehension\nof Text. In EMNLP.\nReid, M.; Marrese-Taylor, E.; and Matsuo, Y . 2021. Sub-\nformer: Exploring Weight Sharing for Parameter Efficiency\nin Generative Transformers. In EMNLP (Findings).\nRothe, S.; Narayan, S.; and Severyn, A. 2020. Leveraging Pre-\ntrained Checkpoints for Sequence Generation Tasks. TACL.\nSainath, T. N.; Kingsbury, B.; Sindhwani, V .; et al. 2013. Low-\nrank matrix factorization for Deep Neural Network training\nwith high-dimensional output targets. In ICASSP.\nSanh, V .; Debut, L.; Chaumond, J.; and Wolf, T. 2019. Distil-\nBERT, a distilled version of BERT: smaller, faster, cheaper\nand lighter. arXiv preprint arXiv:1910.01108.\nSanh, V .; Wolf, T.; and Rush, A. M. 2020. Movement Pruning:\nAdaptive Sparsity by Fine-Tuning. In NeurIPS.\nSchuster, T.; Fisch, A.; Jaakkola, T. S.; and Barzilay, R. 2021.\nConsistent accelerated inference via confident adaptive trans-\nformers. arXiv preprint arXiv:2104.08803.\nSchwartz, R.; Stanovsky, G.; Swayamdipta, S.; et al. 2020.\nThe Right Tool for the Job: Matching Model and Instance\nComplexities. In ACL.\nSee, A.; Luong, M.; and Manning, C. D. 2016. Compres-\nsion of Neural Machine Translation Models via Pruning. In\nCoNLL.\nShen, S.; Dong, Z.; Ye, J.; et al. 2020. Q-BERT: Hessian\nBased Ultra Low Precision Quantization of BERT. In AAAI.\nShi, W.; Song, Y .; Zhou, H.; et al. 2020. Learning from deep\nmodel via exploring local targets. OpenReview preprint.\n10574\nStanton, S.; Izmailov, P.; Kirichenko, P.; et al. 2021. Does\nKnowledge Distillation Really Work? arXiv preprint\narXiv:2106.05945.\nSu, D.; Zhang, H.; Chen, H.; et al. 2018. Is Robustness\nthe Cost of Accuracy? - A Comprehensive Study on the\nRobustness of 18 Deep Image Classification Models. In\nECCV.\nSun, S.; Cheng, Y .; Gan, Z.; and Liu, J. 2019. Patient Knowl-\nedge Distillation for BERT Model Compression. In EMNLP-\nIJCNLP.\nSun, T.; Zhou, Y .; Liu, X.; et al. 2021. Early Exiting with En-\nsemble Internal Classifiers. arXiv preprint arXiv:2105.13792.\nSun, Z.; Yu, H.; Song, X.; et al. 2020. MobileBERT: a Com-\npact Task-Agnostic BERT for Resource-Limited Devices. In\nACL.\nTahaei, M. S.; Charlaix, E.; Nia, V . P.; et al. 2021. Kronecker-\nbert: Learning kronecker decomposition for pre-trained lan-\nguage models via knowledge distillation. arXiv preprint\narXiv:2109.06243.\nTakase, S.; and Kiyono, S. 2021. Lessons on parame-\nter sharing across layers in transformers. arXiv preprint\narXiv:2104.06022.\nTang, R.; Lu, Y .; Liu, L.; et al. 2019. Distilling task-specific\nknowledge from bert into simple neural networks. arXiv\npreprint arXiv:1903.12136.\nTao, C.; Hou, L.; Zhang, W.; et al. 2022. Compression of\nGenerative Pre-trained Language Models via Quantization.\nIn ACL.\nTeerapittayanon, S.; McDanel, B.; and Kung, H. T. 2016.\nBranchyNet: Fast inference via early exiting from deep neural\nnetworks. In ICPR. IEEE.\nTurc, I.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nWell-read students learn better: On the importance of pre-\ntraining compact models. arXiv preprint arXiv:1908.08962.\nVaswani, A.; Shazeer, N.; Parmar, N.; et al. 2017. Attention\nis All you Need. In NIPS.\nV oita, E.; Talbot, D.; Moiseev, F.; et al. 2019. Analyzing\nMulti-Head Self-Attention: Specialized Heads Do the Heavy\nLifting, the Rest Can Be Pruned. In ACL.\nWang, A.; Pruksachatkun, Y .; Nangia, N.; et al. 2019a. Super-\nGLUE: A Stickier Benchmark for General-Purpose Language\nUnderstanding Systems. In NeurIPS.\nWang, A.; Singh, A.; Michael, J.; et al. 2019b. GLUE: A\nMulti-Task Benchmark and Analysis Platform for Natural\nLanguage Understanding. In ICLR.\nWang, A.; and Wolf, T. 2020. Overview of the SustaiNLP\n2020 Shared Task. In Proceedings of SustaiNLP: Workshop\non Simple and Efficient Natural Language Processing.\nWang, J.; Chen, K.; Chen, G.; et al. 2022. SkipBERT: Effi-\ncient Inference with Shallow Layer Skipping. In ACL.\nWang, W.; Bao, H.; Huang, S.; et al. 2021. MiniLMv2: Multi-\nHead Self-Attention Relation Distillation for Compressing\nPretrained Transformers. In ACL-IJCNLP (Findings).\nWang, W.; Wei, F.; Dong, L.; et al. 2020a. MiniLM: Deep\nSelf-Attention Distillation for Task-Agnostic Compression\nof Pre-Trained Transformers. In NeurIPS.\nWang, Y .; Wang, L.; Li, V . O. K.; and Tu, Z. 2020b. On the\nSparsity of Neural Machine Translation Models. In EMNLP.\nWinata, G. I.; Madotto, A.; Shin, J.; et al. 2019. On the\neffectiveness of low-rank matrix factorization for lstm model\ncompression. arXiv preprint arXiv:1908.09982.\nWu, C.; Wu, F.; and Huang, Y . 2021. One Teacher is\nEnough? Pre-trained Language Model Distillation from Mul-\ntiple Teachers. In ACL-IJCNLP (Findings).\nXia, M.; Zhong, Z.; and Chen, D. 2022. Structured Pruning\nLearns Compact and Accurate Models. In ACL.\nXia, Y .; He, T.; Tan, X.; et al. 2019. Tied Transformers: Neu-\nral Machine Translation with Shared Encoder and Decoder.\nIn AAAI.\nXin, J.; Tang, R.; Lee, J.; et al. 2020. DeeBERT: Dynamic\nEarly Exiting for Accelerating BERT Inference. In ACL.\nXin, J.; Tang, R.; Yu, Y .; and Lin, J. 2021. BERxiT: Early\nExiting for BERT with Better Fine-Tuning and Extension to\nRegression. In EACL.\nXu, C.; Zhou, W.; Ge, T.; et al. 2020. BERT-of-Theseus:\nCompressing BERT by Progressive Module Replacing. In\nEMNLP.\nXu, C.; Zhou, W.; Ge, T.; et al. 2021a. Beyond Preserved\nAccuracy: Evaluating Loyalty and Robustness of BERT Com-\npression. In EMNLP.\nXu, J.; Zhou, W.; Fu, Z.; et al. 2021b. A Survey on Green\nDeep Learning. arXiv preprint arXiv:2111.05193.\nYe, D.; Lin, Y .; Huang, Y .; and Sun, M. 2021. TR-BERT:\nDynamic Token Reduction for Accelerating BERT Inference.\nIn NAACL-HLT.\nYuan, M.; and Lin, Y . 2006. Model selection and estimation\nin regression with grouped variables. Journal of the Royal\nStatistical Society: Series B (Statistical Methodology).\nZadeh, A. H.; Edo, I.; Awad, O. M.; et al. 2020. GOBO:\nQuantizing Attention-Based NLP Models for Low Latency\nand Energy Efficient Inference. In MICRO.\nZafrir, O.; Boudoukh, G.; Izsak, P.; and Wasserblat, M.\n2019. Q8bert: Quantized 8bit bert. arXiv preprint\narXiv:1910.06188.\nZhang, H.; Ciss ´e, M.; Dauphin, Y . N.; and Lopez-Paz, D.\n2018. mixup: Beyond Empirical Risk Minimization. In\nICLR.\nZhang, M. S.; and Stadie, B. C. 2020. One-Shot Pruning of\nRecurrent Neural Networks by Jacobian Spectrum Evaluation.\nIn ICLR.\nZhang, W.; Hou, L.; Yin, Y .; et al. 2020. TernaryBERT:\nDistillation-aware Ultra-low Bit BERT. In EMNLP.\nZhang, Z.; Zhu, W.; Zhang, J.; et al. 2022. PCEE-BERT:\nAccelerating BERT Inference via Patient and Confident Early\nExiting. In NAACL-HLT (Findings).\nZhou, W.; Xu, C.; Ge, T.; et al. 2020. BERT Loses Patience:\nFast and Robust Inference with Early Exit. In NeurIPS.\nZhou, W.; Xu, C.; and McAuley, J. J. 2022. BERT Learns to\nTeach: Knowledge Distillation with Meta Learning. In ACL.\nZhu, W. 2021. LeeBERT: Learned Early Exit for BERT with\ncross-level optimization. In ACL-IJCNLP.\n10575",
  "topic": "Inference",
  "concepts": [
    {
      "name": "Inference",
      "score": 0.8346692323684692
    },
    {
      "name": "Computer science",
      "score": 0.785885214805603
    },
    {
      "name": "Language model",
      "score": 0.6989613175392151
    },
    {
      "name": "Transformer",
      "score": 0.6942326426506042
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5485223531723022
    },
    {
      "name": "Computation",
      "score": 0.518990695476532
    },
    {
      "name": "Focus (optics)",
      "score": 0.4757675528526306
    },
    {
      "name": "Natural language processing",
      "score": 0.46091440320014954
    },
    {
      "name": "Machine learning",
      "score": 0.42986008524894714
    },
    {
      "name": "Programming language",
      "score": 0.07407748699188232
    },
    {
      "name": "Engineering",
      "score": 0.07346305251121521
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    }
  ],
  "cited_by": 37
}