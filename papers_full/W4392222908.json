{
  "title": "Understanding the Dataset Practitioners Behind Large Language Models",
  "url": "https://openalex.org/W4392222908",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A3013690110",
      "name": "Crystal Qian",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2011782780",
      "name": "Emily Reif",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2238818493",
      "name": "Minsuk Kahng",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4327500636",
    "https://openalex.org/W3118897201",
    "https://openalex.org/W2137406659",
    "https://openalex.org/W2172937406",
    "https://openalex.org/W4396832834",
    "https://openalex.org/W4383681494",
    "https://openalex.org/W4385847553",
    "https://openalex.org/W6696841567",
    "https://openalex.org/W3012094108",
    "https://openalex.org/W4230130496",
    "https://openalex.org/W2087586120",
    "https://openalex.org/W2963847595",
    "https://openalex.org/W2129660502",
    "https://openalex.org/W1991968238",
    "https://openalex.org/W2963214037",
    "https://openalex.org/W2044102377",
    "https://openalex.org/W3016099278",
    "https://openalex.org/W2941629559",
    "https://openalex.org/W2796040126",
    "https://openalex.org/W4366594566",
    "https://openalex.org/W4401042685",
    "https://openalex.org/W3181414820",
    "https://openalex.org/W2963095307",
    "https://openalex.org/W2883424428",
    "https://openalex.org/W2941766203",
    "https://openalex.org/W4284670548",
    "https://openalex.org/W2018493666",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W3118813946",
    "https://openalex.org/W2129987023",
    "https://openalex.org/W3101662419",
    "https://openalex.org/W4383959108",
    "https://openalex.org/W4288086169",
    "https://openalex.org/W2956281901",
    "https://openalex.org/W3029504795"
  ],
  "abstract": "As large language models (LLMs) become more advanced and impactful, it is\\nincreasingly important to scrutinize the data that they rely upon and produce.\\nWhat is it to be a dataset practitioner doing this work? We approach this in\\ntwo parts: first, we define the role of \"dataset practitioners\" by performing a\\nretrospective analysis on the responsibilities of teams contributing to LLM\\ndevelopment at a technology company, Google. Then, we conduct semi-structured\\ninterviews with a cross-section of these practitioners (N=10). We find that\\nalthough data quality is a top priority, there is little consensus around what\\ndata quality is and how to evaluate it. Consequently, practitioners either rely\\non their own intuition or write custom code to evaluate their data. We discuss\\npotential reasons for this phenomenon and opportunities for alignment.\\n",
  "full_text": "Understanding the Dataset Practitioners Behind Large Language\nModel Development\nCrystal Qian∗\ncjqian@google.com\nGoogle Research\nNew York City, NY, USA\nEmily Reif∗\nereif@google.com\nGoogle Research\nSeattle, WA, USA\nMinsuk Kahng\nkahng@google.com\nGoogle Research\nAtlanta, GA, USA\nABSTRACT\nAs large language models (LLMs) become more advanced and im-\npactful, it is increasingly important to scrutinize the data that they\nrely upon and produce. What is it to be a dataset practitioner doing\nthis work? We approach this in two parts: first, we define the role\nof “dataset practitioners” by performing a retrospective analysis on\nthe responsibilities of teams contributing to LLM development at a\ntechnology company, Google. Then, we conduct semi-structured in-\nterviews with a cross-section of these practitioners (N=10). We find\nthat although data quality is a top priority, there is little consensus\naround what data quality is and how to evaluate it. Consequently,\npractitioners either rely on their own intuition or write custom\ncode to evaluate their data. We discuss potential reasons for this\nphenomenon and opportunities for alignment.\nCCS CONCEPTS\n• Human-centered computing →User studies; HCI theory, con-\ncepts and models .\nKEYWORDS\nlarge language models; dataset practitioners; data analysis\n1 INTRODUCTION\nAs the state-of-the-art for large language models (LLMs) advances\n[40, 47], the field of relevant data analysis is rapidly evolving. Be-\ncause the data used and produced by LLMs is largely unstructured,\ntraditional statistical analyses are insufficient for rigorous evalua-\ntion [11, 45, 49]. Furthermore, as applications of these LLMs become\nmore widely adopted and impactful [1, 7], there is a deeper need\nto qualitatively understand these datasets; for instance, to mitigate\nsociological biases, ensure safe outputs, and minimize harm.\nWe aim to identify the needs and challenges of those who want to\nunderstand unstructured, text-based datasets for LLM development:\na group that we define as dataset practitioners. To develop this\ndefinition, we perform a retrospective analysis within Google, a\ntechnology company that is developing LLMs. We then conduct\nsemi-structured interviews with a cross-section of practitioners\n(N=10) to better understand their workflows, tools, and challenges.\nWe find that practitioners increasingly prioritize data quality;\nhowever, there is no consensus on what constitutes “high quality”\ndata. Despite the HCI and visualization researchers’ active efforts\nto deliver relevant sensemaking methods and tools, data practi-\ntioners in aggregate do not appear to be adopting these solutions,\ninstead relying either on cursory visual inspection of spreadsheets\nor custom analyses logic in notebooks to understand their data.\n∗Both authors contributed equally to this research.\nThere is demand for frameworks, consensus, and tooling in this\nspace that is not being met. We discuss hypotheses for this observed\nphenomenon, and conclude with opportunities for further research\nand alignment.\n2 RELATED WORK\n2.1 Analyzing Analyzers\nAs data science has grown as a discipline, so have the amount of\nanalyses [15, 23], surveys [53], and interviews [51] performed to\ncapture the role of those who do this work.\nSome notable highlights include Kandel et al. [27], which classi-\nfies the emerging role of thedata analysts across different industrial\nsectors, such as healthcare and retail. Muller et al. [38] interviewed\ndata scientists at IBM to capture different approaches to their work,\nand Crisan et al. [15] creates a taxonomy of job roles across data\nworkers, such as moonlighters, generalists or evangelists.\nAcross these studies, the definitions ofdata analysts or data work-\ners satisfy the breadth of work that we aim to capture in this inquiry.\nData scientist is too narrow for our population. It does not encom-\npass the specific challenges introduced by the new LLM-centered\ndata regime, such as a rising need for qualitative evaluation meth-\nods or the broader range of job responsibilities within this role.\nThese broader responsibilities might include, for example, creat-\ning new architecture to interpret data, or developing adjudication\nmethods for human-labelled data.\n2.2 Techniques and Tools\nThere have also been existing inquiries into the techniques and\ntools that practitioners use. Many data science workers interact\nwith data in tabular formats, using tools such as Google Sheets or\nMicrosoft Excel [9]. They may also writing code to perform custom\nanalyses, commonly by using Python scripts or notebooks such as\nGoogle Colab or Jupyter [13, 29, 30, 46].\nAs large language models have become more salient, the space of\napplicable techniques and tools has increased. The field of explain-\nable AI (XAI) [16, 17] has yielded new explainability [31, 44] and\nvisualization techniques for natural language processing. These\ntechniques can be packaged into frameworks and tools [2, 6, 27],\nsuch as Language Interpretability Tool [48], What-If Tool [52], and\nAI Fairness 360 [8] among many others [3, 5, 25, 32, 37]. However,\nthese LLM-focused tools are relatively recent, and there is a lack\nof existing research assessing the extent of their adoption across\nindustry and academia.\n1\narXiv:2402.16611v2  [cs.CL]  1 Apr 2024\nCrystal Qian, Emily Reif, and Minsuk Kahng\n2.3 Curation Trends\nDatasets relevant to LLM development have become increasingly\ncomposed of smaller, curated subsets that target address specific\nconcerns, such as safety and fairness [35, 50]. The focus is increas-\ningly on data quality [45] rather than quantity [19], though quanti-\nfying the criteria for data quality is an open problem [18, 24].\n3 RETROSPECTIVE ANALYSIS\nTo define the role ofdata practitioner, we conducted a retrospective\nanalysis of teams working on developing LLMs at Google. This com-\npany’s organizational structure is uniquely positioned to support a\nbroad survey of the landscape as the technology stack is vertically-\nintegrated [22]; that is, the relevant tooling, infrastructure, mod-\neling, evaluation, and research are primary developed in-house.\nFor example, Google has infrastructure teams that build custom\nsoftware to deploy ML experiments on computational resources,\ntooling teams that create applications for interpreting model out-\nputs, data teams that source and clean human data, modeling teams\nthat improve LLM models across different modalities, and safety\nteams that focus on enforcing policies and model quality.\nUsing company-internal organizational charts and employee\ndirectories, we identified projects associated with the development\nof the company’s core LLMs. We also conducted a meta-review\nof company-internal user studies around evaluating tools for data\nexploration. Applying a grounded theory methodology [ 14], we\ninductively applied a relational content analysis and synthesized\ncommon themes to develop a framework around data practitioning.\n3.1 Defining the Dataset Practitioner\nThe dataset practitioner interacts with unstructured, text-based data\nfor the purpose of developing large language models. The practi-\ntioner’s day-to-day work can cover a broad range of tasks tradition-\nally defined in roles such as software developer, machine learning\nengineer, data scientist, research scientist, product manager, or\nproduct counsel. The practitioner may prioritize these responsibil-\nities concurrently, or switch gears along the model development\nlifecycle. They may do any of the following representative tasks:\n• Curating a new dataset from scratch\n• Creating a new benchmark dataset\n• Cleaning a dataset by removing or fixing bad examples\n• Analyzing a dataset (feedback, comments, etc) to find trends\n• Understanding what bias issues might exist in the dataset\n• Making a go/no go decision on whether to use a dataset to\ntrain a model\n• Debugging a specific model error by finding relevant data\n• Finding ways to improve models, try different datasets, and\ncompare model results\n• Identifying key metrics to define “quality” for a use case\nNext, we give examples of datasets that they may explore. The\nterm “dataset” traditionally implies static and well-curated data;\nwe expand this notion to include any set of text examples, which\nmay come from a variety of provenances (e.g. scraped, synthetically\ngenerated, curated by experts). We categorize these broadly:\n(1) Training datasets\n• Pre-training data : LLMs are pre-trained on huge\namounts of data from webscrapes, books, and other\ngiant corpora. The curation of these datasets is hugely\nimpactful on the model’s performance [34].\n• SFT and RLHF data : Supervised Fine-Tuning (SFT)\nand Reinforcement Learning from Human Feedback\n(RLHF) datasets are used to refine pre-trained LLMs [40,\n47]. They are significantly smaller and more special-\nized than pre-training data, and can be used from an\nopen-ended generation model to a specific use case—\nmost notably, the chatbot interface that many produc-\ntionized LLMs employ. LLMs can be fine-tuned for\nother specific products and use cases as well.\n(2) Datasets involved in model evaluation\n• Benchmark evaluation data : Benchmark datasets\nare created to test specific functionalities or behaviors\nof the model. One notable category of these aresafety\nbenchmarks, which test the model’s ability to adhere\nto company policies and safety standards on concepts\nsuch as toxicity, hallucination, etc.\n• Model outputs: Model outputs can be evaluated out-\nside of the context of a specific benchmark. Side-by-\nside analysis of model outputs may be conducted against\ngolden sets or outputs from a baseline model [26].\n• Outputs of in-context learning: These are a specific\nsubset of model outputs. In-context learning has al-\nlowed users to create new models with no golden data\nat all. These may then be evaluated by analyzing the\noutputs from multiple runs of a prompt.\n• Conversational data : User interactions with LLM-\nbased chatbots can be used to evaluate LLMs in the\nwild.\n4 QUALITATIVE STUDY\n4.1 Participants\nUsing our updated definition, we recruited ten dataset practitioners\n(N=10) within Google for our study.1 We selected these participants\nwith the criteria that their current work involves interacting with\ndatasets for the purposes of developing large language models, and\nprioritized sampling participants from a variety of concentrations\nand backgrounds. These participants and their primary focus areas\n(tooling, modeling, or evaluation) are listed in Table 1. We validated\nour observation from Section 3.1 that the domains of their work\nare fluid; participants who identified in one domain during our\nrecruiting cycle demonstrated experience in many adjacent areas\nwithin the interview. For example, a practitioner formerly focused\non modeling shifted priorities to safety and fairness evaluation\nas their models became more scrutinized and regulated, and two\ntool-builders reported being driven to build tooling to address their\nown unmet needs in modeling.\n1Note that Reif et al. [43] uses the same participant sample.\n2\nUnderstanding the Dataset Practitioners Behind Large Language Model Development\nDomain Participant ID Focus Area\nT1 Tools for data annotation\nT2 Tools for data curation\nT3 Tools for data understandingTooling\nT4 Pipeline infrastructure\nM1 Data curation.\nM2 Model architectureModeling\nM3 Model refinement\nR1 Robustness and abuse\nR2 Unsafe and sensitive contentEvaluation\nR3 Annotator ethnography\nTable 1: Study participants and their current focus areas,\ngrouped by domain.\n4.2 Interview Protocol\nFollowing recruitment and an informed consent process, we con-\nducted semi-structured, one-on-one interviews with participants\nover video conferencing. Each 30-minute interview covered the\nfollowing topics:\n(1) Understanding the use case: Background, use case, product\nimpact, research questions\n(2) Tools and techniques : Awareness and usage of existing tools\nand pipelines, decision making, advantages and limitations,\nstatistical and visual interpretability methods\n(3) User challenges : Bottlenecks, unaddressed concerns\nWe curated the interview topics from prior contextual inquiries\nand protocols from similar research studies in defining data work\n[28, 33, 51]. By following a similar interview protocol, we hope to\nisolate the specific challenges faced in LLM-development.\nWe synthesized our findings through a thematic analysis [10].\nEach interview was de-identified, transcribed, broken into excerpts,\nand coded. Thematic elements, behaviors, and representative quotes\nin this paper are saturated [4, 21], with a code repeated in at least\nthree distinct transcriptions.\n4.3 Findings\n4.3.1 Participants prioritize data quality.Corroborating the\nprior work described in Section 2.3, we find that data quality—\ndefining, finding, and identifying high-quality data—was unani-\nmously the biggest user challenge and priority across all use cases\n(Table 2, Challenges ).\nData, historically, has been around volume rather\nthan quality.. we’ve had this big paradigm shift. —\nT2\n“Quality is the big obstacle. . . [You need] a lot of high-\nquality data... there’s no shortcut. ” —E1\nAlthough data quality has always remained an important priority\nfor data scientists, these concerns were addressable through tasks\nsuch as data cleaning [38] or feature engineering [15]. In the con-\ntext of generative modeling, the evaluation metrics and consensus\nframeworks are less straightforward.\n4.3.2 However, practitioners rely largely on their own intu-\nition to validate this data quality.All participants reported that\nthey would evaluate their data by scanning it visually in spreadsheet\nform; that is, they would look at a handful of examples.\n“I’ll read the first 10 examples, and then maybe some\nin the middle. ” —E1\n“I eyeball data.. It’s all my own intuition and kind of\nindividually spot checking examples. ” —M2\nParticipants cited efficiency, customization, a short learning\ncurve, and ease-of-sharing as reasons for their reliance on spread-\nsheets (Table 2, Challenges ).2 While these factors align with prior\nresearch on spreadsheet usage [9], the ease-of-sharing factor may\nparticularly encourage practitioners to use spreadsheets for LLM\ndevelopment. Unlike the data analysts in Kandel et al. [ 27], who\ncollaborated with “hacker”-types with scripting and coding pro-\nficiency, our participants reported needing to share data with a\nlarger and more diverse set of stakeholders, such as directors and\nlegal teams, to review high-stakes safety fine-tuning datasets.\n4.3.3 Or, practitioners will run custom analyses.Seven of the\nnine participants mentioned also writing custom code in Python\nnotebooks to explore their data, and in one instance even to train\nproduction models. Participants liked the customization of these\nnotebooks [29], yet cited reliability, setup, efficiency, code manage-\nment as pain points (Table 2, Challenges ), validating results from\nother studies on Python notebook usage [13, 29, 30, 46].\nThe efficiency concerns around long-running computations in\nPython notebooks [13] may be further exacerbated as LLMs require\nmore computational power; participants mentioned that “getting\nmodel servers up and running takes forever” (R1), “my queries [to\nLLM APIs] take a while” (E1), and they wished they had “infinite\nQPS (Queries Per Second) [for their LLM API]” (R2).\n4.3.4 Practitioners recognize the confirmation biases in their\nexploration practices.The majority—if not all—of the data ex-\nploration is being done between visual inspection in spreadsheets\nand custom logic in Python notebooks, allowing the practitioner to\nlook at whatever they would like. This degree of freedom exacer-\nbates cognitive bias [12, 20, 24, 42]; for example, Miller et al. [36]\nmentions that “explainable AI uses only the researchers’ intuition\nof what constitutes a ‘good’ explanation. ” Indeed, our participants\nadmit to this confirmation bias in their practices:\n““I eyeball that things make sense [in the data]. ” —\nM2\nIn fact, model developers reported that they did not look at\ntraining data unless their model outputs were surprising.\n“When the data is passed to the modeling side, we as-\nsume that the data team has fixed everything. Unless\nwe train and it doesn’t look right, then we’ll [look\nat the data] and give the data team that feedback. ”\n—M3\n2Interestingly and consistent with similar user studies, our participants emphasized\nthat their reliance on visual inspection of spreadsheets were their own behaviors\nand not best practices. They suggested that other practitioners likely used more\nsophisticated tooling [41].\n3\nCrystal Qian, Emily Reif, and Minsuk Kahng\nTable 2: This matrix categorizes our findings (inspired by Kandel et al. [ 27]). An ‘x’ in the cell indicates that a participant\nmentioned this specific topic in their interview. Topics are grouped by Processes, Tools,and Challenges, and participant are\ngrouped by their domain from Table 1. All participants mentioned interacting with spreadsheets and cited data quality as a\nchallenge in their work.\n4.3.5 Participants have not converged upon other tools.Apart\nfrom Google Sheets and Python notebooks like Colab, no other tools\ngarnered consensus among practitioners. Some practitioners em-\nployed additional methods, such as running a binary for calculating\nsafety and toxicity thresholds, kicking off a pipeline to automat-\nically classify their data, and using an user interface to visualize\nembeddings. However, these practices were not prevalent in our\nsample.\n“Everyone is using a different thing, and getting ev-\neryone on the same page is really difficult. ” —M1\nThe lack of alignment in tooling presents an organization chal-\nlenge. As training datasets are increasingly composed of smaller\ndatasets to leverage the expertise of specific subteams, greater col-\nlaboration across groups is necessary. This can lead to increased\nfriction in adopting new tools and exploration patterns [ 27], as\nstakeholders and collaborators must transition to new tooling si-\nmultaneously, or migrate in a manner that preserves data sharing\ncapabilities.\n“With the new generative data— Many people are\ncontributing with many different lenses. In practice,\nthese [subsets] get built by random teams, they get\nadded and nobody really reviews it because you can’t. ”\n—T4\n5 DISCUSSION\nThe reason why practitioners have not aligned on alternative tool-\ning is not obvious. Practitioners across all domains recognize that\nthere is a gap in the workflow:\n“Not having an easy-to-use-tool is a major bottle-\nneck. . . Every time [that I make changes to data], I\nhave to write a custom colab to ingest the new fields. ”\n—M2\n“There are no helpful tools from a qualitative re-\nsearcher’s perspective. I jump between spreadsheets,\na CSV file and a colab. . . The long story short is that\nwe haven’t really found a very useful tool for this. ”\n—E3\n“Right now, if you want to curate high-quality data,\nyou go through [each point] manually as an expert,\nwhich is not scalable [for] thousands of examples. ”\n—T2\nPractitioners are aware of and have tried the existing tools in\nthis space. They are aligned on the properties that they want out\nof this tool (Table 2, Challenges ), and these requests are being com-\nmunicated to tooling teams:\n“The kinds of requests we tend to get nowadays are\nabout larger-scale dataset management, like mixture\nbuilding. When you have a big selection, reviewing\n4\nUnderstanding the Dataset Practitioners Behind Large Language Model Development\n10,000 rows is not what you want to do . . . That is\nmuch more amenable to summary review. ” —T1\nIn response, tooling teams are evaluating and building tools\nto address these requests [ 3, 48, 52]. So, why is there a lack of\nalignment? We discuss hypotheses posed by two different domains\nof practitioners.\n5.0.1 The toolmakers’ hypothesis: the world is new.When\ntool developers (T1-T4) described exploration workflows, they ex-\nplained that there was a lack of alignment because the field is new:\n“The pace is very frenetic right now.. tools are fast-\nchanging. . . ” —T1\n“There’s been a big step function in the NLP world..\nit just takes a while to figure out what tools people\nneed and what all use cases. ” —T2\nTwo observations from our interviews may support this claim.\nFirst, practitioners are using spreadsheets. Perhaps in the absence\nof a ground truth for unstructured data, practitioners prefer to rely\non their own intuition. Similarly, without a definitive framework\nfor qualitative data exploration, practitioners are sticking to the\ntools they know. Adopting new practices takes effort (see Table 2,\nChallenges > Learning curve ), and spreadsheets have been tried-and-\ntrue from the previous state-of-the-art when visually spot-checking\ndata and conducting statistical analyses were sufficient.\nSecond, our participants described a landscape where there was\na lack of alignment [18, 20] across multiple topics such as objectives,\nmetrics, and benchmarks, suggesting that the field and its principles\nare still emerging. The following are representative quotes from\nparticipants:\n• Data quality:\n– T1, on LLM prompts: “There’s so many competing def-\ninitions of prompt quality. . . it’s a research north star\nthat happens to be a major product priority. How can we\nimprove this extremely important data set?’\n– M1, on training data: “The quality of data is subjective;\na lot of people disagree. . . one person thinks it’s really\nhigh-quality data, but there’s no objective. ”\n– T3, on evaluation data: “There’s not a framework for\nevaluating [data].. in a perfect world, there is well-articulated\nbehavior (tone, subject matter, objective results).. ”\n• Metrics:\n– M1: “[Consider] search rankings. . . what makes for a\ngood benchmark, how do we come to an agreement?”\n– E1: “If you’re doing simple classification, it’s easy to\nmeasure accuracy or precision or recall. But with gen-\nerative models, evaluation is very subjective. Even the\noutput of the model is subjective, so then, what’s going\ninto the model- it’s really hard to say, is this better or\nworse?”\n• Safety:\n– T2: “Think about safety data curation. . . people can’t\nagree on criteria, let alone apply that criteria at scale. ”\n• Communication:\n– T3: “What [data practitioners are] actually doing and\nwhat they communicate that they need are two very\ndifferent things. What are they actually trying to do?”\nThis lack of alignment is amplified as teams collaborate more\nclosely [39, 53]. Even if one team in the development pipeline iden-\ntifies their quality evaluation parameters, there needs to be further\nagreement at the inter-team level.\n5.0.2 The model developers’ hypothesis: there’s no tool that\nworks for my use case.Modeling and evaluation practitioners\nspeculated that alignment was unlikely due to custom needs and\nrequirements (Section 3.1 ).\n“I think why [a spreadsheet is] so universal is that it’s\nso basic.. you can customize it to give this affordance\nthat other tools may not give you.. it’s simple. ” —E1\n“We have tried so many [tools]. These tools are lim-\niting is because they offer you exploration on only\none aspect of [the data]. . . For me, they’re too specific. ”\n—M2\nInterestingly, when asked about the custom requirements for\ntheir use cases, practitioners listed similar requirements, which\nsuggest that there may be opportunities for shared methods and\nevaluation frameworks. Some of these requirements include:\n• Summarizing salient features of a dataset and identifying\nthe corresponding data slices (6 participants)\n• Ensuring safety of outputs/respecting toxicity thresholds\n(4 participants)\n• Evaluating numeric distributions on text/token length (3\nparticipants)\nIt is likely the case that both the toolmakers’ and model de-\nvelopers’ hypotheses are true to some extent. There may be select\nopportunities for alignment as the field matures, and there are likely\nother problems that will require custom solutions. For example,\nthere are specific tools being developed to address challenges that\npersist across datasets, such as safety and toxicity classification [8].\n6 CONCLUSIONS AND FUTURE WORK\nIn this study, we aimed to identify the needs of those who are\nexploring unstructured, text-based datasets for the purpose of de-\nveloping LLMs. To define this population ofdataset practitioners,\nwe conducted a retrospective analysis on teams working on LLM\ndevelopment. We then interviewed a broad cross-section of these\npractitioners to better understand their use cases and challenges.\nThrough our retrospective analysis, we found that the dataset\npractitioner takes on a fluid role that is not well-defined in cur-\nrent literature on data workers. We hope that our contribution of\ndefining this population and their use cases will enable the HCI\ncommunity to better assess and support their needs.\nIn our interviews, we found that data quality is unanimously\nthe top priority, but quality is subjective. Further research should\nexplore what data quality means in different contexts, and how\nthe same data can be high-quality or low-quality depending on the\nsituation and perspective. Clarifying subjectivity across conceptual\nframeworks, evaluations, and workflows in this domain remains\na top priority, potentially achieved through standardizing metrics\n(e.g. toxicity, distributions of relevant safety features, data diversity)\nand evaluation criteria.\n5\nCrystal Qian, Emily Reif, and Minsuk Kahng\nTwo primary data exploration patterns emerge: visually inspect-\ning data in spreadsheets, which lacks scalability, and crafting cus-\ntom analyses in Python notebooks, which is high-effort. Both prac-\ntices are susceptible to confirmation bias. However, the community\nhas yet to reach a consensus on alternative best-practices to for\ndata exploration, possibly due to the nascent nature of the field or\nthe custom needs of the practitioners. There are opportunities to\ndetermine the specific areas where prioritizing either flexibility or\nspecificity is most beneficial; these opportunities can be addressed\nby formalizing evaluation frameworks in the evolving landscape,\nand developing flexible tooling for custom analysis.\n“There’s a fundamental chicken and egg problem. . . there’s\nno tooling so people don’t use tooling so tooling doesn’t\ndevelop. ” —T2\nACKNOWLEDGMENTS\nThe authors wish to thank our study participants and Google’s\nPeople + AI Research Team (PAIR), especially James Wexler and\nMichael Terry.\nREFERENCES\n[1] Malak Abdullah, Alia Madain, and Yaser Jararweh. 2022. ChatGPT: Fundamentals,\nApplications and Social Impacts. In 2022 Ninth International Conference on Social\nNetworks Analysis, Management and Security (SNAMS) . 1–8. https://doi.org/10.\n1109/SNAMS58071.2022.10062688\n[2] Namita Agarwal and Saikat Das. 2020. Interpretable Machine Learning Tools: A\nSurvey. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI) . IEEE,\nCanberra, Australia, 1528–1534. https://doi.org/10.1109/SSCI47803.2020.9308260\n[3] Saleema Amershi, Max Chickering, Steven M. Drucker, Bongshin Lee, Patrice\nSimard, and Jina Suh. 2015. ModelTracker: Redesigning Performance Analysis\nTools for Machine Learning. In Proceedings of the 33rd Annual ACM Conference\non Human Factors in Computing Systems (CHI ’15) . ACM, 337–346. https://doi.\norg/10.1145/2702123.2702509\n[4] Hikari Ando, Rosanna Cousins, and Carolyn Young. 2014. Achieving Satu-\nration in Thematic Analysis: Development and Refinement of a Codebook,.\nComprehensive Psychology 3 (2014), 03.CP.3.4. https://doi.org/10.2466/03.CP.3.4\narXiv:https://doi.org/10.2466/03.CP.3.4\n[5] Ian Arawjo, Chelse Swoopes, Priyan Vaithilingam, Martin Wattenberg, and Elena\nGlassman. 2023. ChainForge: A Visual Toolkit for Prompt Engineering and LLM\nHypothesis Testing. arXiv:cs.HC/2309.09128\n[6] Narges Ashtari, Ryan Mullins, Crystal Qian, James Wexler, Ian Tenney, and\nMahima Pushkarna. 2023. From Discovery to Adoption: Understanding the ML\nPractitioners’ Interpretability Journey. In Proceedings of the 2023 ACM Designing\nInteractive Systems Conference (DIS ’23) . Association for Computing Machinery,\nNew York, NY, USA, 2304–2325. https://doi.org/10.1145/3563657.3596046\n[7] Maria Teresa Baldassarre, Danilo Caivano, Berenice Fernandez Nieto, Domenico\nGigante, and Azzurra Ragone. 2023. The Social Impact of Generative AI: An\nAnalysis on ChatGPT. In Proceedings of the 2023 ACM Conference on Information\nTechnology for Social Good (GoodIT ’23) . ACM, 363–373. https://doi.org/10.1145/\n3582515.3609555\n[8] Rachel K. E. Bellamy, Kuntal Dey, Michael Hind, Samuel C. Hoffman, Stephanie\nHoude, Kalapriya Kannan, Pranay Lohia, Jacquelyn Martino, Sameep Mehta,\nAleksandra Mojsilovic, Seema Nagar, Karthikeyan Natesan Ramamurthy, John T.\nRichards, Diptikalyan Saha, Prasanna Sattigeri, Moninder Singh, Kush R. Varsh-\nney, and Yunfeng Zhang. 2018. AI Fairness 360: An Extensible Toolkit for\nDetecting, Understanding, and Mitigating Unwanted Algorithmic Bias. arXiv\npreprint arXiv:1810.01943 (2018). http://arxiv.org/abs/1810.01943\n[9] David Birch, David Lyford-Smith, and Yike Guo. 2018. The Future of Spreadsheets\nin the Big Data Era. arXiv:cs.CY/1801.10231\n[10] Virginia Braun and Victoria Clarke. 2006. Using thematic analysis in psychology.\nQualitative research in psychology 3, 2 (2006), 77–101.\n[11] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-\nplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,\nTom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are\nFew-Shot Learners. In Advances in Neural Information Processing Systems ,\nVol. 33. 1877–1901. https://proceedings.neurips.cc/paper_files/paper/2020/file/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n[12] Aylin Caliskan, Joanna J Bryson, and Arvind Narayanan. 2017. Semantics derived\nautomatically from language corpora contain human-like biases. Science 356,\n6334 (2017), 183–186.\n[13] Souti Chattopadhyay, Ishita Prasad, Austin Z. Henley, Anita Sarma, and Titus\nBarik. 2020. What’s Wrong with Computational Notebooks? Pain Points, Needs,\nand Design Opportunities. In Proceedings of the 2020 CHI Conference on Human\nFactors in Computing Systems (CHI ’20) . ACM, 1–12. https://doi.org/10.1145/\n3313831.3376729\n[14] Juliet M Corbin and Anselm Strauss. 1990. Grounded theory research: Procedures,\ncanons, and evaluative criteria. Qualitative sociology 13, 1 (1990), 3–21.\n[15] Anamaria Crisan, Brittany Fiore-Gartland, and Melanie Tory. 2021. Passing the\nData Baton : A Retrospective Analysis on Data Science Work and Workers. IEEE\nTransactions on Visualization and Computer Graphics 27, 2 (2021), 1860–1870.\nhttps://doi.org/10.1109/TVCG.2020.3030340\n[16] Marina Danilevsky, Kun Qian, Ranit Aharonov, Yannis Katsis, Ban Kawas, and\nPrithviraj Sen. 2020. A Survey of the State of Explainable AI for Natural Language\nProcessing. CoRR abs/2010.00711 (2020). arXiv:2010.00711 https://arxiv.org/abs/\n2010.00711\n6\nUnderstanding the Dataset Practitioners Behind Large Language Model Development\n[17] Arun Das and Paul Rad. 2020. Opportunities and Challenges in Explainable Arti-\nficial Intelligence (XAI): A Survey. CoRR abs/2006.11371 (2020). arXiv:2006.11371\nhttps://arxiv.org/abs/2006.11371\n[18] Finale Doshi-Velez and Been Kim. 2017. Towards A Rigorous Science of Inter-\npretable Machine Learning. arXiv:stat.ML/1702.08608\n[19] Hugh Durrant-Whyte. 2015. Data, Knowledge and Discovery: Machine Learning\nmeets Natural Science. In Proceedings of the 21th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining (KDD ’15) . ACM, 7. https:\n//doi.org/10.1145/2783258.2785467\n[20] Leilani H Gilpin, David Bau, Ben Z Yuan, Ayesha Bajwa, Michael Specter, and\nLalana Kagal. 2018. Explaining explanations: An overview of interpretability of\nmachine learning. In 2018 IEEE 5th International Conference on data science and\nadvanced analytics (DSAA) . IEEE, IEEE, Turin, Italy, 80–89.\n[21] Greg Guest, Arwen Bunce, and Laura Johnson. 2006. How Many Inter-\nviews Are Enough?: An Experiment with Data Saturation and Variability.\nField Methods 18, 1 (2006), 59–82. https://doi.org/10.1177/1525822X05279903\narXiv:https://doi.org/10.1177/1525822X05279903\n[22] Kathryn Rudie Harrigan. 1985. Vertical integration and corporate strategy.\nAcademy of Management journal 28, 2 (1985), 397–425.\n[23] Harlan Harris, Sean Murphy, and Marck Vaisman. 2013. Analyzing the analyzers:\nAn introspective survey of data scientists and their work . O’Reilly Media, Inc.\n[24] Bernease Herman. 2019. The Promise and Peril of Human Evaluation for Model\nInterpretability. arXiv:cs.AI/1711.07414\n[25] Fred Hohman, Minsuk Kahng, Robert Pienta, and Duen Horng Chau. 2018. Visual\nanalytics in deep learning: An interrogative survey for the next frontiers. IEEE\nTransactions on Visualization and Computer Graphics 25, 8 (2018), 2674–2693.\nhttps://doi.org/10.1109/TVCG.2018.2843369\n[26] Minsuk Kahng, Ian Tenney, Mahima Pushkarna, Michael Xieyang Liu, James\nWexler, Emily Reif, Krystal Kallarackal, Minsuk Chang, Michael Terry, and Lucas\nDixon. 2024. LLM Comparator: Visual Analytics for Side-by-Side Evaluation\nof Large Language Models. Extended Abstracts of the 2024 CHI Conference on\nHuman Factors in Computing Systems .\n[27] Sean Kandel, Andreas Paepcke, Joseph M. Hellerstein, and Jeffrey Heer. 2012.\nEnterprise Data Analysis and Visualization: An Interview Study. IEEE Transac-\ntions on Visualization and Computer Graphics 18, 12 (2012), 2917–2926. https:\n//doi.org/10.1109/TVCG.2012.219\n[28] Harmanpreet Kaur, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach,\nand Jennifer Wortman Vaughan. 2020. Interpreting Interpretability: Under-\nstanding Data Scientists’ Use of Interpretability Tools for Machine Learning. In\nProceedings of the 2020 CHI Conference on Human Factors in Computing Systems\n(CHI ’20) . ACM, 1–14. https://doi.org/10.1145/3313831.3376219\n[29] Mary Beth Kery, Bonnie E. John, Patrick O’Flaherty, Amber Horvath, and Brad A.\nMyers. 2019. Towards Effective Foraging by Data Scientists to Find Past Analysis\nChoices. InProceedings of the 2019 CHI Conference on Human Factors in Computing\nSystems (CHI ’19) . ACM, 1–13. https://doi.org/10.1145/3290605.3300322\n[30] Mary Beth Kery, Marissa Radensky, Mahima Arya, Bonnie E. John, and Brad A.\nMyers. 2018. The Story in the Notebook: Exploratory Data Science using a\nLiterate Programming Tool. In Proceedings of the 2018 CHI Conference on Human\nFactors in Computing Systems (CHI ’18) . ACM, 1–11. https://doi.org/10.1145/\n3173574.3173748\n[31] Been Kim, Martin Wattenberg, Justin Gilmer, Carrie Cai, James Wexler, Fernanda\nViegas, and Rory sayres. 2018. Interpretability Beyond Feature Attribution:\nQuantitative Testing with Concept Activation Vectors (TCAV). InProceedings of\nthe 35th International Conference on Machine Learning , Vol. 80. PMLR, 2668–2677.\nhttps://proceedings.mlr.press/v80/kim18d.html\n[32] Biagio La Rosa, Graziano Blasilli, Romain Bourqui, David Auber, Giuseppe San-\ntucci, Roberto Capobianco, Enrico Bertini, Romain Giot, and Marco Angelini.\n2023. State of the art of visual analytics for explainable deep learning. InComputer\nGraphics Forum , Vol. 42. Wiley Online Library, 319–355.\n[33] Catherine Li, Talie Massachi, Jordan Eschler, and Jeff Huang. 2023. Understanding\nthe Needs of Enterprise Users in Collaborative Python Notebooks: This paper\nexamines enterprise user needs in collaborative Python notebooks through a\ndyadic interview study. In Extended Abstracts of the 2023 CHI Conference on\nHuman Factors in Computing Systems (CHI EA ’23) . ACM, Article 402, 7 pages.\nhttps://doi.org/10.1145/3544549.3573843\n[34] Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts,\nBarret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, and Daphne\nIppolito. 2023. A Pretrainer’s Guide to Training Data: Measuring the Effects of\nData Age, Domain Coverage, Quality, & Toxicity. arXiv:cs.CL/2305.13169\n[35] Ninareh Mehrabi, Fred Morstatter, Nripsuta Saxena, Kristina Lerman, and Aram\nGalstyan. 2021. A Survey on Bias and Fairness in Machine Learning. Comput.\nSurveys 54, 6, Article 115 (2021), 35 pages. https://doi.org/10.1145/3457607\n[36] Tim Miller. 2019. Explanation in artificial intelligence: Insights from the social\nsciences. Artificial Intelligence 267 (2019), 1–38. https://doi.org/10.1016/j.artint.\n2018.07.007\n[37] Yao Ming, Huamin Qu, and Enrico Bertini. 2019. RuleMatrix: Visualizing and\nUnderstanding Classifiers with Rules. IEEE Transactions on Visualization and\nComputer Graphics 25, 1 (2019), 342–352. https://doi.org/10.1109/TVCG.2018.\n2864812\n[38] Michael Muller, Ingrid Lange, Dakuo Wang, David Piorkowski, Jason Tsay,\nQ. Vera Liao, Casey Dugan, and Thomas Erickson. 2019. How Data Science\nWorkers Work with Data: Discovery, Capture, Curation, Design, Creation. In\nProceedings of the 2019 CHI Conference on Human Factors in Computing Systems\n(CHI ’19) . ACM, 1–15. https://doi.org/10.1145/3290605.3300356\n[39] Nadia Nahar, Shurui Zhou, Grace Lewis, and Christian Kästner. 2022. Collabora-\ntion challenges in building ML-enabled systems: communication, documentation,\nengineering, and process. In Proceedings of the 44th International Conference on\nSoftware Engineering (ICSE ’22) . ACM, 413–425. https://doi.org/10.1145/3510003.\n3510209\n[40] OpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774 (2023).\n[41] James W. Pennebaker. 2011. The secret life of pronouns. New Scientist 211, 2828\n(2011), 42–45. https://doi.org/10.1016/S0262-4079(11)62167-2\n[42] Peter Pirolli and Stuart Card. 2005. The sensemaking process and leverage points\nfor analyst technology as identified through cognitive task analysis. Proceedings\nof international conference on intelligence analysis 5 (2005), 2–4.\n[43] Emily Reif, Crystal Qian, James Wexler, and Minsuk Kahng. 2024. Automatic\nHistograms: Leveraging Language Models for Text Dataset Exploration.Extended\nAbstracts of the 2024 CHI Conference on Human Factors in Computing Systems .\n[44] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"Why Should I\nTrust You?\": Explaining the Predictions of Any Classifier. InProceedings of the\n22nd ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining (KDD ’16) . ACM, 1135–1144. https://doi.org/10.1145/2939672.2939778\n[45] Nithya Sambasivan, Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen\nParitosh, and Lora M Aroyo. 2021. “Everyone wants to do the model work, not\nthe data work”: Data Cascades in High-Stakes AI. In Proceedings of the 2021 CHI\nConference on Human Factors in Computing Systems (CHI ’21) . ACM, Article 39,\n15 pages. https://doi.org/10.1145/3411764.3445518\n[46] Aurélien Tabard, Wendy E. Mackay, and Evelyn Eastmond. 2008. From individ-\nual to collaborative: the evolution of prism, a hybrid laboratory notebook. In\nProceedings of the 2008 ACM Conference on Computer Supported Cooperative Work\n(CSCW ’08) . ACM, 569–578. https://doi.org/10.1145/1460563.1460653\n[47] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste\nAlayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth,\net al. 2023. Gemini: a family of highly capable multimodal models. arXiv preprint\narXiv:2312.11805 (2023).\n[48] Ian Tenney, James Wexler, Jasmijn Bastings, Tolga Bolukbasi, Andy Coenen, Se-\nbastian Gehrmann, Ellen Jiang, Mahima Pushkarna, Carey Radebaugh, Emily Reif,\nand Ann Yuan. 2020. The Language Interpretability Tool: Extensible, Interactive\nVisualizations and Analysis for NLP Models. arXiv:cs.CL/2008.05122\n[49] Krzysztof Wach, Cong Doanh Duong, Joanna Ejdys, R¯uta Kazlauskait˙e, Pawel\nKorzynski, Grzegorz Mazurek, Joanna Paliszkiewicz, and Ewa Ziemba. 2023. The\ndark side of generative artificial intelligence: A critical analysis of controversies\nand risks of ChatGPT.Entrepreneurial Business and Economics Review 11, 2 (2023),\n7–30.\n[50] Kiri Wagstaff. 2012. Machine Learning that Matters. arXiv:cs.LG/1206.4656\n[51] Dakuo Wang, Justin D. Weisz, Michael Muller, Parikshit Ram, Werner Geyer,\nCasey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray. 2019. Human-\nAI Collaboration in Data Science: Exploring Data Scientists’ Perceptions of\nAutomated AI. Proceedings of the ACM on Human-Computer Interaction 3, CSCW,\nArticle 211 (2019), 24 pages. https://doi.org/10.1145/3359313\n[52] James Wexler, Mahima Pushkarna, Tolga Bolukbasi, Martin Wattenberg, Fer-\nnanda Viégas, and Jimbo Wilson. 2020. The What-If Tool: Interactive Probing\nof Machine Learning Models. IEEE Transactions on Visualization and Computer\nGraphics 26, 1 (2020), 56–65. https://doi.org/10.1109/TVCG.2019.2934619\n[53] Amy X. Zhang, Michael Muller, and Dakuo Wang. 2020. How do Data Science\nWorkers Collaborate? Roles, Workflows, and Tools. Proceedings of the ACM on\nHuman-Computer Interaction 4, CSCW1 (2020), 1–23.\n7",
  "topic": "Intuition",
  "concepts": [
    {
      "name": "Intuition",
      "score": 0.7761709094047546
    },
    {
      "name": "Phenomenon",
      "score": 0.5792309641838074
    },
    {
      "name": "Data science",
      "score": 0.5441362857818604
    },
    {
      "name": "Computer science",
      "score": 0.5268598794937134
    },
    {
      "name": "Data quality",
      "score": 0.515929102897644
    },
    {
      "name": "Work (physics)",
      "score": 0.47691771388053894
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.46972787380218506
    },
    {
      "name": "Knowledge management",
      "score": 0.4327927529811859
    },
    {
      "name": "Public relations",
      "score": 0.3426581025123596
    },
    {
      "name": "Political science",
      "score": 0.2317609190940857
    },
    {
      "name": "Psychology",
      "score": 0.2101978361606598
    },
    {
      "name": "Business",
      "score": 0.20731687545776367
    },
    {
      "name": "Engineering",
      "score": 0.1839827597141266
    },
    {
      "name": "Marketing",
      "score": 0.14843779802322388
    },
    {
      "name": "Epistemology",
      "score": 0.09364873170852661
    },
    {
      "name": "Metric (unit)",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Cognitive science",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}