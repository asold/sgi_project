{
  "title": "Machine Translation with Large Language Models: Prompting, Few-shot Learning, and Fine-tuning with QLoRA",
  "url": "https://openalex.org/W4389519128",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2096474468",
      "name": "Xuan Zhang",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2308310247",
      "name": "Navid Rajabi",
      "affiliations": [
        "George Mason University"
      ]
    },
    {
      "id": "https://openalex.org/A2046721439",
      "name": "Kevin Duh",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A2095651410",
      "name": "Philipp Koehn",
      "affiliations": [
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W2971278086",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W22168010",
    "https://openalex.org/W4318903120",
    "https://openalex.org/W2963842551",
    "https://openalex.org/W4226155321",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4389520065",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W3177323791",
    "https://openalex.org/W2970529093",
    "https://openalex.org/W4300963525",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4385565080",
    "https://openalex.org/W3034700448",
    "https://openalex.org/W2963477238",
    "https://openalex.org/W4285483946",
    "https://openalex.org/W4323323154",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W4385572225",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4367365458",
    "https://openalex.org/W4285077564",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W4321472057",
    "https://openalex.org/W4317547647",
    "https://openalex.org/W4364387438",
    "https://openalex.org/W3105214104",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W4303443398"
  ],
  "abstract": "While large language models have made remarkable advancements in natural language generation, their potential in machine translation, especially when fine-tuned, remains under-explored. In our study, we conduct comprehensive experiments, evaluating 15 publicly available language models on machine translation tasks. We compare the performance across three methodologies: zero-shot prompting, few-shot learning, and fine-tuning. Central to our approach is the use of QLoRA, an efficient fine-tuning method. On French-English, QLoRA fine-tuning outperforms both few-shot learning and models trained from scratch. This superiority is highlighted in both sentence-level and document-level translations, with a significant BLEU score improvement of 28.93 over the prompting method. Impressively, with QLoRA, the enhanced performance is achieved by fine-tuning a mere 0.77% of the model's parameters.",
  "full_text": "Proceedings of the Eighth Conference on Machine Translation (WMT), pages 468–481\nDecember 6–7, 2023. ©2023 Association for Computational Linguistics\n468\nMachine Translation with Large Language Models:\nPrompting, Few-shot Learning, and Fine-tuning with QLoRA\nXuan Zhang\nJohns Hopkins University\nxuanzhang@jhu.edu\nNavid Rajabi\nGeorge Mason University\nnrajabi@gmu.edu\nKevin Duh\nJohns Hopkins University\nkevinduh@cs.jhu.edu\nPhilipp Koehn\nJohns Hopkins University\nphi@jhu.edu\nAbstract\nWhile large language models have made re-\nmarkable advancements in natural language\ngeneration, their potential in machine transla-\ntion, especially when fine-tuned, remains under-\nexplored. In our study, we conduct compre-\nhensive experiments, evaluating 15 publicly\navailable language models on machine transla-\ntion tasks. We compare the performance across\nthree methodologies: zero-shot prompting, few-\nshot learning, and fine-tuning. Central to our\napproach is the use of QLoRA, an efficient fine-\ntuning method. On French-English, QLoRA\nfine-tuning outperforms both few-shot learning\nand models trained from scratch. This superi-\nority is highlighted in both sentence-level and\ndocument-level translations, with a significant\nBLEU score improvement of 28.93 over the\nprompting method. Impressively, with QLoRA,\nthe enhanced performance is achieved by fine-\ntuning a mere 0.77% of the model’s parameters.\n1 Introduction\nThe rapid advancement of large language models\n(LLMs) is reshaping the field of natural language\nprocessing (NLP), marking a potential paradigm\nshift in future development (Zhao et al., 2023). In-\nstead of crafting dedicated task-specific systems,\na growing interest has been focusing on quickly\nadapting LLMs to specific tasks simply through\nprompting (Liu et al., 2023; Sanh et al., 2022). So\nfar, studies have shown that prompting LLMs can\nmatch or even rival the performance of specialized\nsystems on numerous NLP tasks (Radford et al.).\nAmong all the NLP tasks, the application of\nLLMs to machine translation (MT) is understud-\nied. The optimal way to harness LLMs for MT\nremains an open question. While encoder-decoder-\nbased LLMs (Xue et al., 2021; Liu et al., 2020;\nCosta-jussà et al., 2022) are inherently designed\nfor the sequence-to-sequence demands of MT, the\napproach for leveraging decoder-only models is\nless straightforward.\nAlthough there are initial attempts in this di-\nrection (Sia and Duh, 2022; Hendy et al., 2023;\nMoslem et al., 2023; Zhu et al., 2023), these stud-\nies mainly concentrate on prompting and few-shot\nlearning, not exploiting the availability of bitext.\nAdditionally, most work focus on exceptionally\nlarge LLMs like GPT3 (Brown et al., 2020) with\nits staggering 175 billion parameters, which are be-\nyond the reach of non-commercial research groups\nfor local training. This poses a significant hurdle\nfor institutions with constrained computational re-\nsources, rendering the findings less applicable and\nrelevant to many researchers.\nIn this paper, we aim to investigate the perfor-\nmance of LLMs on MT tasks, with a particular\nfocus on decoder-based LLMs, a category less\ncharted for MT applications. Our research fo-\ncuses on a range of publicly available medium-\nsized LLMs. This includes models pretrained on\nEnglish-centric datasets, such as GPT-Neo (Black\net al., 2021), OPT (Zhang et al., 2022), LLaMA2\n(Touvron et al., 2023), as well as those on multilin-\ngual datasets such as XGLM (Lin et al., 2021) and\nBLOOMZ (Muennighoff et al., 2022). We evaluate\nvarious versions of these models, with their param-\neter sizes spanning from 1.3 billion to 13 billion,\ntotaling 15 models.\nIn our experiments, we explore zero-shot prompt-\ning, few-shot learning, and fine-tuning, where our\nemphasis on fine-tuning fills the gap in previous\nstudies. For the fine-tuning process, we employ the\nQLoRA method (Dettmers et al., 2023), which en-\nhances efficiency and minimizes memory usage by\nquantizing the model to 4-bit precision and limiting\nthe number of trainable parameters. To the best of\nour knowledge, this is the first instance of QLoRA\nbeing applied to fine-tuning LLMs for MT tasks.\nWe also evaluate the performance of LLMs in\ndocument-level translation. Standard sequence-to-\nsequence MT models focus on translating one sen-\ntence at a time, overlooking discourse phenom-\n469\nena and the broader context. Existing methods\nfor document-level translation often pivot toward\narchitectural modifications (Tu et al., 2018; Tan\net al., 2019; Xu et al., 2021), leading to specialized\nmodels that need unique designs. Our objective is\nto evaluate the capability of LLMs in preserving\nlong-term contextual coherence and to explore their\npotential in facilitating the development of a robust\ndocument-level translation system.\nWe demonstrate the effectiveness of fine-tuning\non a French-English dataset – this language pair is\nselected due to its accessibility for LLMs, position-\ning it as an ideal starting point for research in this\ndomain. Our experimental results, complemented\nby thorough analysis, reveal that:\n• LLMs, when subjected to fine-tuning, are\npotent MT models. Through fine-tuning,\nthey consistently outperform their zero-shot\nprompting counterparts, achieving an average\nimprovement of 8 BLEU for sentence-level\ntranslation and 16.33 BLEU for document-\nlevel translation. Notably, the model opt-13b\neven sees a remarkable boost of 28.93 BLEU\n(from 4.56 to 33.49).\n• There is a large variation in the performance\nacross different LLMs. LLaMA 2 consistently\noutperforms others for both prompting and\nfine-tuning. BLOOMZ, initially lagging be-\nhind in prompting, ascends to top-tier models\nafter fine-tuning. However, some models, de-\nspite benefiting from fine-tuning, either match\nor fall short of the performance of models\ntrained from scratch. It is also noteworthy that\nlarger models don’t invariably outshine their\nsmaller counterparts.\n• When prompted, LLMs demonstrate supe-\nrior performance in sentence-level translation.\nHowever, the application of fine-tuning yields\nmore substantial enhancements in document\ntranslation, as reflected by both the BLEU and\nCOMET scores. Notably, LLaMA 2 surpasses\nits performance in sentence-level translation\nwhen trained on documents.\n• QLoRA accelerates the fine-tuning process\nwithout compromising model performance.\nTo attain an equivalent BLEU score, it neces-\nsitates 21 times less training time and reduces\nthe trainable parameters by 1370-fold com-\npared to conventional fine-tuning.\n2 Related Work\n2.1 LLM Applications\nLeveraging LLMs across a spectrum of down-\nstream natural language processing (NLP) tasks is\nnow a prevailing approach. However, the optimal\nstrategies for utilizing these models both effectively\nand efficiently remain an open question. Broadly\nspeaking, there are three primary methods to build\napplications based on LLMs:\n• Zero-shot prompting.1 This involves query-\ning LLMs with a prompt that hasn’t been\nseen in the training data of the model. Such\nprompts typically provide specific task instruc-\ntions along with the main query. Given the\nsensitivity of LLMs to the structure and con-\ntent of prompts, careful prompt engineering is\ncrucial to achieve optimal performance.\n• Few-shot learning. Often referred to as in-\ncontext learning, few-shot learning is a tech-\nnique where LLMs are provided with a hand-\nful of examples to guide their responses. Zero-\nshot prompting can be considered a subset of\nthis, where no examples are given. In few-\nshot learning, these examples are integrated\ninto the prompt template, serving as context\nto instruct the model on how to respond.\n• Fine-tuning. The two methods above al-\nlow for task adaptation without the need for\nfurther training on the LLMs. In contrast,\nfine-tuning involves extending the training of\nthe LLMs using additional, task-specific data.\nThis is particularly beneficial when such tai-\nlored datasets are available.\nYang et al. (2023) survey the ‘use cases’ and ‘no\nuse cases’ of LLMs for specific downstream tasks,\nconsidering the three aforementioned methods, and\nconclude that LLMs excel in most NLP tasks.\n2.2 LLMs for MT\nRecent literature has begun to explore the applica-\ntion of LLMs for MT, an area that remained rela-\ntively under-explored until now. Both Hendy et al.\n(2023) and Moslem et al. (2023) underscore the\nsuperiority of GPT3 (Brown et al., 2020), GPT3.5\nand ChatGPT (Bawden and Yvon, 2023) in MT\n1Throughout this paper, we refer to ‘zero-shot prompting’\nsimply as ‘prompting’.\n470\nusing prompting. However, the former also indi-\ncates that these models may not consistently out-\nperform SOTA MT systems and commercial trans-\nlators. In a comparative study, Zhu et al. (2023)\nexperiment with various LLMs, including GLM-\n7.5B (Lin et al., 2021), OPT-175B (Zhang et al.,\n2022), BLOOMZ-7.1B (Muennighoff et al., 2022),\nand ChatGPT. Their findings suggest that while\nthese decoder-only LLMs are competitive, they still\nlag behind when compared to the encoder-decoder-\nbased multilingual language model NLLB (Costa-\njussà et al., 2022). Briakou et al. (2023) studied the\nimpact of LLM data on MT.\nPrompting strategies for MT are studied by Vilar\net al. (2023) for PaLM (Chowdhery et al., 2022)\nand Zhang et al. (2023) for GLM-130B (Zeng et al.,\n2022). They reveal several challenges associated\nwith MT prompting, such as issues with copying,\nmistranslation of entities, and hallucination. These\nchallenges are echoed by Bawden and Yvon (2023),\nwhich identify similar constraints with prompting\non BLOOM (Scao et al., 2022). However, they\nshow these limitations can be mitigated in a few-\nshot learning setting. Sia and Duh (2022) investi-\ngated a light-weight tuning method akin to prefix\ntuning (Li and Liang, 2021). Sia and Duh (2023)\nand Wang et al. (2023) expand the evaluation to\ndocument-level translation.\nWhile prior studies have highlighted the poten-\ntial of LLMs in MT, their focus has been primarily\non in-context learning. A significant gap remains\nin the exploration of fine-tuning LLMs specifically\nfor MT tasks. Additionally, there is an evident ab-\nsence of research that provides a comprehensive\ncomparison among prompting, few-shot learning,\nand fine-tuning methodologies. Recognizing this\noversight, the primary objective of this paper is to\naddress and bridge this research gap.\n3 QLoRA\nQLoRA (Dettmers et al., 2023) is an efficient fine-\ntuning approach that reduces the memory usage of\ntraining without compromising the 16-bit task per-\nformance. The approach involves quantizing a pre-\ntrained model to 4-bit precision. Subsequently, a\ncompact set of learnable Low-rank Adapter (LoRA,\nHu et al. (2021)) weights are added, which can be\ntuned through backpropagation.\nLoRA Motivated by the empirical findings of Li\net al. (2018) and Aghajanyan et al. (2020), which\nsuggest that LLMs possess a notably low intrinsic\ndimension for their parameters, LoRA hypothe-\nsizes a similar low intrinsic rank for weights dur-\ning model adaptation. Thus, LoRA introduces a\nreparameterization aimed at reducing dimensions.\nSpecifically, it employs a low-rank decomposition\nto represent the pretrained weights, resulting in\nnewly-added adapter weight matrices, with the rank\nr anticipated to be considerably smaller than the\noriginal weight matrices’ dimension. During fine-\ntuning, the pretrained weights are frozen, with only\nthe newly incorporated adapter updated via back-\npropagation. A key observation is that as the rank\nr is reduced, there is a corresponding decrease in\nthe number of adaptable parameters.\n4 Experimental Setup\n4.1 Datasets\nIn this study, we focus on the translation direc-\ntion from French to English due to its signifi-\ncant demand for high-quality translation and the\navailability of substantial parallel data. Our fine-\ntuning set includes the commonly used Europarl\n(Koehn, 2005) and News Commentary dataset from\nWMT142. The dev and test sets are the new-\nstest2013 and newstest2014 datasets, respectively,\nfrom WMT14. These datasets are constructed from\ndocuments, thus enabling a natural evaluation of\ndocument-level translation. Table 1 summarizes\nthe statistics of the datasets.\n#sents #docs avg.sents/doc\ntrain 2,366,117 21,430 144\ndev 3000 126 24\ntest 3003 169 18\nTable 1: Dataset statistics.\n4.2 Baseline\nWe compare the performance of systems built upon\nLLMs against an NMT model trained from scratch\nusing the Amazon Sockeye framework (Hieber\net al., 2022). The model architecture is a 12-layer\ntransformer with a model size of 1024, 16 attention\nheads, and 4096 hidden units in the feed-forward\nlayers. We employ byte pair encoding (BPE, Sen-\nnrich et al. (2016)) separately for each language,\nsetting the number of BPE symbols to 30k for both\nlanguages. The model is trained with a batch size\nof 4096, an initial learning rate of 0.0002, and a\n2https://www.statmt.org/wmt14/translation-task.html\n471\nModel Release Time Data Size (B)\nGPT-Neo (Black et al., 2021) Mar, 2021 English-centric 1.3; 2.7\nOPT (Zhang et al., 2022) June, 2022 English-centric 1.3; 2.7; 6.7\nLLaMA2 (Touvron et al., 2023) July, 2023 English-centric 7; 13\nXGLM (Lin et al., 2021) Nov, 2022 Multilingual 1.7; 2.9; 4.5; 7.5\nBLOOMZ (Muennighoff et al., 2022) Nov, 2022 Multilingual 1.7; 3; 7.1\nTable 2: Overview of evaluated LLMs.\nplateau-reduce learning rate scheduler. Addition-\nally, we apply a dropout and label smoothing of\n0.1, use the Adam optimizer with a warm-up of\n10k steps, and set the checkpoint interval to 4000.\nTraining is halted if there is no improvement in per-\nformance on the dev set for 32 consecutive check-\npoints. The model has 4 billion parameters and is\ntrained on a single NVIDIA V100 with 32G GPU\nmemory.\nThis is a relatively standard NMT model, de-\nvoid of advanced techniques such as back transla-\ntion, knowledge distillation, or ensembling, which\ncould potentially elevate the model to state-of-the-\nart performance (Kocmi et al., 2022). However, the\nprimary objective of this study is to compare the ef-\nficacy of using an off-the-shelf machine translation\ntoolkit, which is widely accessible and requires\nminimal effort for machine translation practition-\ners, against building MT systems using LLMs. Im-\nportantly, both methods demand similar levels of\neffort in development, making this a fair compar-\nison to ascertain the most efficient approach for\npractitioners and researchers alike.\n4.3 Pretrained LLMs\nWe investigate a varied collection of pretrained\nLLMs accessible on HuggingFace (Wolf et al.,\n2020), all based on the transformer architecture.\nThis collection comprises five distinct LLMs, each\ntrained on either English-centric or multilingual\ndata and available in multiple versions with vary-\ning parameter sizes. This results in a comprehen-\nsive assortment of 15 models, with parameter sizes\nranging from 1.3 billion to 13 billion. Table 2 sum-\nmarizes the models included in our study.\n• GPT-Neo - a GPT-2 (Radford et al.) like causal\nlanguage model trained on the Pile dataset (Gao\net al., 2020), an 825 GiB English corpus.\n• OPT - a suite of causal language models, where\nthe largest one, OPT-175B, exhibits performance\ncomparable to GPT-3 (Brown et al., 2020).\n• LLAMA 2- pretrained on 2 trillion tokens of\nEnglish-centric data. We used a fine-tuned ver-\nsion of the model, referred to asLLAMA 2-CHAT.\nThis fine-tuned version demonstrates superior\nperformance compared to open-source chat mod-\nels across a wide range of benchmarks.\n• XGLM - a multilingual language model trained\non a balanced corpus covering 30 diverse lan-\nguages with 500B tokens. The XGLM 7.5B\noutperforms GPT-3 on the FLORES-101 (Goyal\net al., 2022) machine translation benchmark in\nfew-shot learning scenarios.\n• BLOOMZ - a multilingual BLOOM model\n(Scao et al., 2022) fine-tuned with the xP3 dataset\n(Muennighoff et al., 2022), which consists of mul-\ntilingual datasets with English prompts, totaling\n95 GiB of text.\nThe selection of these models enables us to as-\nsess the impact of various factors on translation\nperformance, including the type of model (English-\ncentric vs. multilingual) and model size. Addi-\ntionally, the chosen sizes reflect the computational\nresources typically available to research institutes\nwith limited GPU resources, such as university labs.\nThis consideration ensures that our findings are ap-\nplicable and accessible to a broad range of machine\ntranslation researchers and practitioners.\n4.4 Prompted Tuning\nWe fine-tune LLMs using examples that\ninclude specifically formatted prompts\n( French: [fr sent] English: ) and their corre-\nsponding responses ( [en sent] ). The dev set is\nalso formatted in the same way. This approach\ncustomizes the model for the French-English\nmachine translation task.\nSentence-level Prompts The inputs at the sen-\ntence level are formatted as follows:\nFrench: [fr sent] English: [en sent] <eos>\nWe append the special token <eos> at the end\nof each sample to regulate the length of the text\ngenerated by the model. Without this, LLMs tend\n472\nto generate text continuously until they reach a\npredetermined length limit.\nDocument-level Prompts We use the given doc-\nument boundaries to concatenate parallel sentences\ninto document-level sequences. These parallel doc-\numents comprise an equal number of sentences\nin both languages. Our goal is to ensure that the\nmodels generate the same number of output sen-\ntences per document as the number of input sen-\ntences provided, facilitating sentence-level evalu-\nation. We adopt the document mark-up used in\nJunczys-Dowmunt (2019), incorporating symbols\nfor document start ( <BEG> ) and end ( <END> ),\nas well as sentence separators ( <SEP> ). In in-\nstances where documents exceed our sentence limit\nof 10, we substitute the <END> symbol with\na break symbol ( <BRK> ) and commence the\nsubsequent sequence with a continuation symbol\n( <CNT> ) instead of <BEG> . Below is an exam-\nple of a document input:\nFrench: <BEG> [fr sent1] <SEP> [fr sent2]\n<SEP><END> English: <BEG> [en sent1]\n<SEP> [en sent2] <SEP><END>\n4.5 Fine-tuning Setup\nWe configure the learning rate to 2e-4 and employ\nthe Adam optimizer for the training process. A\nbatch size of 32 is used, and the evaluation is per-\nformed every 1000 steps. The fine-tuning process\nis halted if there is no improvement in the model’s\nperformance over 16 consecutive checkpoints. For\nthe LoRA configurations, the rank for the low-rank\napproximation is set to 64, and the scaling factor\nfor the low-rank adaptation is set to 32. The train-\nable parameters are limited to the self-attention\nlayers of the model. Additionally, a dropout rate\nof 0.05 is applied in the LoRA layer. The model\nweights are quantized to 4-bit precision to reduce\nmemory requirements, and mixed-precision train-\ning is enabled, using a combination of float16 and\nfloat32 data types to accelerate the training pro-\ncess. Models with less than 3 billion parameters are\ntrained on a single NVIDIA RTX GPU with 24GB\nof memory, while models with more than 3 billion\nbut less than 7 billion parameters are trained on a\nsingle NVIDIA V100 GPU with 32GB of memory.\nFor models with an even larger number of parame-\nters, we employ multiple V100 GPUs and enable\nmodel parallelism by setting device_map=\"auto\" .\nThis is facilitated by the Accelerate library from\nHugging Face, which automatically distributes the\nmodel across the available GPUs.\n4.6 Evaluation Metrics\nWe use BLEU and COMET (Rei et al., 2020) as\nevaluation metrics to assess the performance of\nour models. For BLEU we use the SacreBLEU\n(Post, 2018) implementation, which standardizes\ntokenization and facilitates reproducibility.\nOn the other hand, unlike BLEU, which de-\npends on the n-gram overlap between the machine-\ngenerated translation and the reference translation,\nCOMET models are trained on a comprehensive\ndataset comprising human translations and human\nquality assessments. This dataset is used to predict\ntranslation quality while also taking the source side\ninto account. This approach enables COMET to\nprovide a more holistic evaluation that includes flu-\nency, adequacy, and preservation of meaning. We\nemploy the latest model, Unbabel/wmt22-comet-\nda, for our evaluation. This model scales the scores\nbetween 0 and 1, where a score approaching 1 indi-\ncates a high-quality translation.\nBy employing both BLEU and COMET, we can\nensure that our evaluation is robust and compre-\nhensive, accounting for not only the lexical similar-\nity between the translations and the references but\nalso the overall quality and preservation of mean-\ning in the translations. Moreover, COMET may\nserve as a superior evaluation metric when assess-\ning the zero-shot performance of LLMs compared\nto BLEU. As we demonstrated in Section 7, the\noutputs from LLMs often excel in preserving mean-\nings but might receive a low score if evaluated\nsolely based on n-gram matching.\n5 Sentence-level Translation\nIn this section, we assess the sentence-level transla-\ntion performance of pretrained LLMs using prompt-\ning versus fine-tuned LLMs (Section 5.1). We in-\nvestigate the effects of incorporating or not incor-\nporating QLoRA during the fine-tuning process\n(Section 5.2). Additionally, we analyze the im-\npact of varying QLoRA hyperparameters (Section\n5.3), including the rank of the low-rank approxima-\ntion (Section 5.3.1), and the trainable parameters\n(Section 5.3.2). We also conduct experiments with\ndifferent sizes of fine-tuning data and compare the\nresults of fine-tuned LLMs with the baseline NMT\nmodel (Section 5.4). Lastly, we explore few-shot\nlearning with varying numbers of shots and diverse\nprompts (Section 5.5).\n473\n0\nFigure 1: Prompting (P) vs. QLoRA fine-tuning (FT) on sentence-level translation using various pretrained LLMs.\nBaseline is the NMT system described in Section 4.2. Rank r for QLoRA is set to 64.3\n5.1 Main Results\nWe present the results of prompting and QLoRA\nfine-tuning in Figure 1. Key observations are:\n• While there is a significant disparity in BLEU\nscores, the same is not observed in COMET. All\nmodels exhibit comparable COMET scores. The\ntop-performing fine-tuned model, llama2-13b,\noutperforms the baseline from 0.837 to 0.862.\nThis indicates that while all models produce\nsemantically coherent translations, their lexical\nchoices, which affect BLEU scores, might differ.\n• In terms of BLEU, the baseline model surpasses\nmost prompted LLMs, with the exception of\nLLAMA 2. Specifically, llama2-7b achieves the\nhighest performance at 34.56 BLEU, marking a\n3.89 BLEU improvement over the baseline.\n• 8 out of the 15 fine-tuned LLMs exceed the base-\nline. This includes both English-centric and mul-\ntilingual models. The standout model is bloomz-\n7.1b achieving a BLEU score of 37.39, a 6.72\nBLEU enhancement compared to the baseline.\n• Fine-tuning invariably boosts LLM performance\non average by 8 BLEU points, with bloomz-7.1b\nwitnessing the most substantial leap of 20.13\nBLEU.\n• No clear advantage is discerned when contrast-\ning prompted multilingual models with English-\ncentric ones. For instance, the multilingual\nbloomz-1.7b scores the lowest at 14.16 BLEU.\nYet, when evaluating the fine-tuning gains over\nprompting, multilingual models average an 11.32\nBLEU improvement, surpassing the 5.02 BLEU\nof their counterparts.\n• Bigger models do not consistently outshine their\nsmaller counterparts. For instance, after fine-\ntuning, bloomz-1.7b trumps the larger opt-13b\n(31.95 vs. 31.29 BLEU). Within the same archi-\ntecture, models with more parameters typically\nfare better, but there are exceptions, like with\nXGLM, where the 4.5b and 7.5b versions lag be-\nhind the 2.9b variant.\nIn conclusion, while directly prompted LLMs\ndo not universally outperform train-from-scratch\nMT models, certain LLMs, such as LLAMA 2,\ndefy this trend. Moreover, fine-tuning consistently\nproves beneficial, with the potential to elevate even\nunderperforming LLMs, like bloomz-7.1b, to top-\ntier performance.\nparams(%) #GPUs time(hrs)\nNo QLoRA 27.40 4 52\nQLoRA 0.02 1 10\nTable 3: Fine-tuning xglm-2.9b with and without\nQLoRA to achieve the BLEU score of 30.05.4Only the\nself-attention layers are tuned. The rank r for QLoRA\napproximation is set to 2.\n3We also report TER in Appendix A.\n4We train the model without QLoRA for 96 hours in total,\nand 30.05 is the BLEU score obtained at the best checkpoint.\n474\nr 2 4 8 16 32 64 128 256 512\ntrain params(%) 0.02 0.05 0.09 0.19 0.39 0.77 1.53 3.01 5.85\nBLEU 31.69 31.72 32.28 32.52 32.80 33.04 30.60 30.09 30.31\nCOMET 0.845 0.846 0.847 0.848 0.849 0.850 0.837 0.835 0.836\nTable 4: QLoRA fine-tuning results on XGLM 2.9B with various rank r choices. All the weights except for\nself-attentions are frozen.\n5.2 QLoRA vs. No QLoRA\nTo assess QLoRA’s efficacy, we contrast it with\nthe original approach, a more resource-intensive\nchoice: fine-tuning without QLoRA, which ex-\ncludes both quantization and low-rank adaptation.\nWe train the xglm-2.9b model using its native 32-\nbit precision, necessitating the use of 4 NVIDIA\nv100s. This is compared against a model fine-tuned\nwith QLoRA set atr = 2. For consistency, only the\nself-attention layers are unfrozen in both models.\nThe comparative results are presented in Table 3.\nAchieving a BLEU score of 30.05, the model\nfine-tuned without QLoRA requires 52 hours\nacross 4 GPUs, totaling 208 GPU hours. In con-\ntrast, the QLoRA-enhanced model completes in\njust 10 hours, marking a 21-fold acceleration and\nutilizing 1370 times fewer trainable parameters\n(0.02% compared to 27.4%).\n5.3 QLoRA Hyperparameters\nWe investigate the impact of selecting different\nranks for LoRA and the unfrozen parameters for\nfine-tuning. We present the results for XGLM 2.9B.\n5.3.1 Rank r\nThe rank r of the decomposition matrices influ-\nences the number of trainable parameters, with a\nlarger r resulting in more trainable parameters. We\nassess the performance associated with different\nchoices of r, ranging from 2 to 512, in Table 4,\nwhile only unfreezing the self-attention layers.\nWith r = 64, the model attains its optimal per-\nformance. However, either reducing or increasing\nthe number of trainable parameters adversely af-\nfects the model’s performance. Interestingly, when\nr = 512, the performance deteriorates even more\nthan when r = 2, despite the fact that the latter\nconverges more quickly due to a smaller number\nof trainable parameters.\n5.3.2 Trainable Parameters\nNext, we aim to determine which part of the model\nshould be fine-tuned. To do this, we unfreeze the\nparameters in different layers of the XGLM 2.9B\nmodel. As illustrated in Table 5, we experiment\nwith unfreezing parameters from various layers, in-\ncluding the self-attention layers, embedding layers,\nfully-connected feed-forward layers, and the LM\nhead layers. The results indicate that fine-tuning\nonly the self-attention layer is sufficient to yield\nthe best performance.\nParams a a+e a+e+f a+e+f+l\nBLEU 31.69 30.09 30.30 28.39\nCOMET 0.845 0.837 0.834 0.826\nTable 5: QLoRA fine-tuning results on XGLM 2.9B\nwith different trainable parameters. a: self-attentions; e:\nembeddings; f : fully-connected feed-forward layers; l:\nlm head. Rank r is set to 2.\nFigure 2: The performance of the baseline system and\nfine-tuned XGLM 2.9B trained with different amounts\nof data.\n5.4 Data Curves\nThe performance of a traditional MT model is\nclosely tied to the volume of its training data, as\nhighlighted by (Koehn and Knowles, 2017). How-\never, for LLMs, which have already benefited from\nvast training datasets, does this correlation still\nhold? To investigate, we compare the responses of\nboth MT model types to varying training data sizes.\nWe incrementally adjust the dataset size from 0.1%\n(2,366 examples) to its entirety and then train the\n475\nPrompt 1 { French: [fr sent] English: [en sent] } x K\nFrench: [fr sent] English:\nPrompt 2 { Translate French to English: French:[fr sent] English: [en sent] } x K\nTranslate French to English: French:[fr sent] English:\nPrompt 3 Translate French to English:{ French: [fr sent] English: [en sent] } x K\nTranslate French to English: French:[fr sent] English:\nPrompt 4 Translate French to English:\nFrench: { [fr sent] } x K English: { [en sent] } x K\nTranslate French to English: French:[fr sent] English:\nPrompt 5 { French: [fr sent] Translate to English:[en sent] } x K\nFrench: [fr sent] Translate to English:\nTable 6: Prompts used in K-shot learning. The substrings within {} are repeated K times.\nBLEU COMET\n0-shot 1-shot 5-shot 10-shot 0-shot 1-shot 5-shot 10-shot\nPrompt 1 27.08 29.15 29.72 29.62 0.814 0.828 0.833 0.834\nPrompt 2 28.36 29.46 29.86 29.95 0.813 0.830 0.836 0.835\nPrompt 3 28.36 29.33 29.86 29.74 0.813 0.831 0.835 0.834\nPrompt 4 28.36 29.46 28.66 27.83 0.813 0.830 0.829 0.825\nPrompt 5 11.82 28.76 29.80 29.70 0.631 0.827 0.834 0.834\nTable 7: Few-shot learning results on XGLM 2.9B.\nbaseline model and fine-tune the LLMs. The out-\ncomes of this experiment are depicted in Figure\n2.\nThe baseline curve validates the assumption that\nperformance improves with increased data avail-\nability. In contrast, LLMs make a robust debut;\neven without additional training data, they achieve\na BLEU score comparable to the baseline trained\non half the dataset. Yet, their performance does not\nconsistently improve with more data. In fact, fine-\ntuning with less than 50% (1.2 million examples)\nof the data seems counterproductive, diminishing\nperformance until the full dataset comes into play.\n5.5 Few-shot Learning\nIn this section, we evaluate the few-shot learning\nperformance of LLMs. Few-shot learning is also\ndenoted as K-shot, with K representing the num-\nber of examples provided before the query, where\nin our case, examples are randomly sampled from\nthe training set. We also compare the impact of 5\nslightly varied prompts, detailed in Table 6. The\nresults of the experiments are presented in Table 7.\nWhen K >= 1, the model consistently out-\nperforms the 0-shot scenario. For prompt 5, 1-\nshot dramatically enhances the model’s capability,\nelevating the BLEU score from 11.82 5 to 28.76.\nHowever, the performance does not exhibit a linear\ngrowth with increasing K; it plateaus. In the case\nof prompt 4, augmenting K even diminishes the\nperformance.\nIn our experiments, the choice of prompt is\nparticularly impactful for 0-shot performance, es-\npecially when comparing prompt 5 to the others.\nHowever, this impact seems to lessen when exam-\nples are presented before the query.\n6 Document-level Translation\nIn this section, we delve into the proficiency of\nLLMs in document-level translation. Our primary\nobservations, contrasting the prompted and fine-\ntuned LLMs, are detailed in Section 6.1. Addition-\nally, we explore the influence of document length,\nmeasured by the number of sentences per docu-\nment, in Section 6.2.\n6.1 Main Results\nFigure 3 presents the results for document-level\ntranslations. Key takeaways include:\n5We observed many empty generations when prompting\nwith Prompt 5. One hypothesis is that the prompt is ambigu-\nous and the model is confused about what to translate.\n476\n0\nFigure 3: Prompting (P) vs. QLoRA fine-tuning (FT) on document-level translation using various pretrained LLMs.\nRank r for QLoRA is set to 64.\n• In contrast to sentence-level translation,\nprompted LLMs face challenges with document-\nlevel translation. 4 out of the 15 LLMs register\nBLEU scores below 10. However, consistent\nwith sentence-level findings, LLAMA 2 continues\nto stand out in zero-shot performance, with the\n7b and 13b versions achieving impressive BLEU\nscores of 35.29 and 36.6, respectively.\n• Fine-tuning demonstrates significant promise\nfor document-level translations, enhancing the\nBLEU scores of their prompted counterparts by\nan average of 16.33. The most notable improve-\nment is seen inopt-13b, which witnesses a BLEU\nincrement of 28.93 (from 4.56 to 33.49).\n• Unlike sentence-level translation, where COMET\nscores remain consistent across all models,\ndocument-level translation displays a more pro-\nnounced variance. This variability is particularly\nevident in prompted models but diminishes in\nfine-tuned ones.\n• Trends observed in sentence-level translation\n(Section 5.1) persist in the document-level con-\ntext: (1) Both English-centric and multilingual\nmodels deliver comparable performance. (2)\nLarger models do not consistently surpass their\nsmaller counterparts. (3) Top-performing mod-\nels in sentence-level translation, such as LLAMA\n2 and BLOOMZ, maintain their dominance in\ndocument-level translation.\n6.2 Input Lengths\nWe construct the examples for fine-tuning by merg-\ning sentences from an original document, as de-\nscribed in Section 4.4. By varying the number of\nsentences per document – specifically, 5, 10, and\n15 – we present our findings in Table 8. Notably,\nfine-tuning with sets of 10 consecutive sentences\nyields the optimal performance, registering a 30.94\nBLEU and a 0.811 COMET score.\n#sents 5 10 15\nBLEU 29.07 30.94 28.32\nCOMET 0.781 0.811 0.742\nTable 8: QLoRA fine-tuning on documents composed of\ndifferent numbers of sentences6. XGLM 2.9B systems\nare fine-tuned with r = 64, and self-attentions and\nembeddings as trainable parameters.\n7 Qualitative Analysis\nFigure 4 shows translations of two French sen-\ntences using various LLMs. In the first example,\nwhen prompted, bloomz-7.1b replicates the source\nsentence verbatim, neglecting to translate. While\nthis does not occur for every test set sample, as\nshown in the second example, similar behavior is\nnoted in other prompted LLMs. On the other hand,\n6We match the length of text documents to that of train\ndocuments.\n477\nFrench\nEnglish\nL'ONU donne un bilan même plus élevé avec 979 morts et 1 902 blessés.\nThe UN has reported even higher numbers with 979 dead and 1,902 injured.\nbloomz-7.1b P L'ONU donne un bilan même plus élevé avec 979 morts et 1 902 blessés.\nbloomz-7.1b FT The UN gives a higher ﬁgure with 979 dead and 1 902 wounded.<eos>.<eos>.<eos>.\nllama2-13b P\nllama2-13b FT\n979 deaths and 1,902 injuries, according to the UN's latest tally.\nThe UN gives an even higher death toll of 979 and 1 902 injured.<eos>The UN gives \nan even higher death toll of 979 and 1 902 injured.<eos>The UN gives an even higher \ndeath toll of 979 and 1 902 injured.<eos>The\nFrench\nEnglish\nL'affaire NSA souligne l'absence totale de débat sur le renseignement\nNSA Affair Emphasizes Complete Lack of Debate on Intelligence\nbloomz-7.1b P French: The NSA case highlights the complete absence of debate on intelligence.\nbloomz-7.1b FT The NSA Case Sheds Light on the Absence of Debate on Intelligence<eos>.<eos>.<eos>.\nllama2-13b P\nllama2-13b FT\nThe NSA case highlights the complete lack of debate on intelligence gathering.\nThe NSA Scandal Highlights the Lack of Intelligence Debate<eos>eos>eos>\nFigure 4: Translations from prompted (P) and fine-tuned (FT) LLMs.\nthe translation using llama2-13b P, though not mir-\nroring the reference verbatim, retains the original\nsentence’s meaning. Both fine-tuned LLMs pro-\nduce proper translations with the initial segment\nof the generated sequences. Bloomz-7.1b appends\na <eos> token post-translation, while llama2b-\n13b reiterates its translation multiple times. Both\noutputs necessitate post-processing, specifically\ntruncating the output at the first occurrence of the\n<eos> token.\nIn the second example, the LLM-generated trans-\nlations retain the meaning of the reference transla-\ntion, showcasing LLMs’ potential in the translation\ntasks.\n8 Conclusions\nIn this study, we investigate the capabilities of\nLLMs in performing machine translation tasks.\nThrough comprehensive experiments, we assess\nthe effectiveness of prompting, few-shot learning,\nand fine-tuning using QLoRA for French-English\ntranslation. Our key findings are:\n1. The proficiency of LLMs in machine trans-\nlation varies. While LLAMA 2consistently\noutperforms its counterparts, other models,\nwhen relying solely on few-shot learning, of-\nten lag behind models trained from scratch.\n2. Fine-tuning invariably enhances performance,\nparticularly for models that struggle with few-\nshot learning and for translating documents. It\ncan transform a seemingly inadequate model\ninto a top-tier translation model, as seen with\nbloomz-7.1b.\n3. QLoRA, due to its efficiency, can be a superior\nalternative to original fine-tuning methods.\n4. Fine-tuning LLMs with QLoRA can be a\npromising and new paradigm for machine\ntranslation practice.\nIn the future, we are interested in exploring two\nprimary avenues. (1) While our current study\ndemonstrates the promise of LLMs trained on\nEnglish-centric data for French-to-English trans-\nlations, it raises intriguing questions: Would these\nresults hold true for other language pairs, espe-\ncially for low-resource languages? And would\nthere be a noticeable difference in performance\nbetween English-centric and multilingual LLMs\nin such scenarios? (2) Our experiments are con-\nfined to decoder-based LLMs. Moving forward,\nwe are also interested in comparing these models\nagainst their encoder-decoder counterparts, such as\nmT5(Xue et al., 2021), mBART (Liu et al., 2020),\nNLLB (Costa-jussà et al., 2022).\n478\nLimitations\nSingle dataset and language pair Our exper-\niments are confined to a single dataset and the\nFrench-English language pair. It remains unclear if\nour findings are generalizable to other datasets and\nlanguage pairs.\nMedium-sized LLMs We have only experi-\nmented with medium-sized LLMs due to computa-\ntional resource constraints. The necessity of fine-\ntuning for significantly larger LLMs remains an\nopen question.\nAcknowledgements\nThis work is supported in part by an Amazon Ini-\ntiative for Artificial Intelligence (AI2AI) Faculty\nResearch Award.\nReferences\nArmen Aghajanyan, Luke Zettlemoyer, and Sonal\nGupta. 2020. Intrinsic dimensionality explains the\neffectiveness of language model fine-tuning. arXiv\npreprint arXiv:2012.13255.\nRachel Bawden and François Yvon. 2023. Investigating\nthe translation performance of a large multilingual\nlanguage model: the case of bloom. arXiv preprint\narXiv:2303.01911.\nSid Black, Leo Gao, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow. If you use this software, please cite it\nusing these metadata.\nEleftheria Briakou, Colin Cherry, and George Foster.\n2023. Searching for needles in a haystack: On the\nrole of incidental bilingualism in PaLM’s translation\ncapability. In Proceedings of the 61st Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 9432–9452, Toronto,\nCanada. Association for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nMarta R Costa-jussà, James Cross, Onur Çelebi, Maha\nElbayad, Kenneth Heafield, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard,\net al. 2022. No language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe pile: An 800gb dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-\nJen Chen, Guillaume Wenzek, Da Ju, Sanjana Kr-\nishnan, Marc’Aurelio Ranzato, Francisco Guzmán,\nand Angela Fan. 2022. The flores-101 evaluation\nbenchmark for low-resource and multilingual ma-\nchine translation. Transactions of the Association for\nComputational Linguistics, 10:522–538.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Hassan\nAwadalla. 2023. How good are gpt models at ma-\nchine translation? a comprehensive evaluation. arXiv\npreprint arXiv:2302.09210.\nFelix Hieber, Michael Denkowski, Tobias Domhan,\nBarbara Darques Barros, Celina Dong Ye, Xing\nNiu, Cuong Hoang, Ke Tran, Benjamin Hsu, Maria\nNadejde, et al. 2022. Sockeye 3: Fast neural\nmachine translation with pytorch. arXiv preprint\narXiv:2207.05851.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685.\nMarcin Junczys-Dowmunt. 2019. Microsoft translator\nat wmt 2019: Towards large-scale document-level\nneural machine translation. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n2: Shared Task Papers, Day 1), pages 225–233.\n479\nTom Kocmi, Rachel Bawden, Ond ˇrej Bojar, Anton\nDvorkovich, Christian Federmann, Mark Fishel,\nThamme Gowda, Yvette Graham, Roman Grund-\nkiewicz, Barry Haddow, Rebecca Knowles, Philipp\nKoehn, Christof Monz, Makoto Morishita, Masaaki\nNagata, Toshiaki Nakazawa, Michal Novák, Martin\nPopel, and Maja Popovi´c. 2022. Findings of the 2022\nconference on machine translation (WMT22). In\nProceedings of the Seventh Conference on Machine\nTranslation (WMT), pages 1–45, Abu Dhabi, United\nArab Emirates (Hybrid). Association for Computa-\ntional Linguistics.\nPhilipp Koehn. 2005. Europarl: A parallel corpus for\nstatistical machine translation. In Proceedings of\nMachine Translation Summit X: Papers, pages 79–86,\nPhuket, Thailand.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. InProceedings\nof the First Workshop on Neural Machine Translation,\npages 28–39.\nChunyuan Li, Heerad Farkhoor, Rosanne Liu, and Ja-\nson Yosinski. 2018. Measuring the intrinsic di-\nmension of objective landscapes. arXiv preprint\narXiv:1804.08838.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021.\nFew-shot learning with multilingual language models.\narXiv preprint arXiv:2112.10668.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2023. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nACM Computing Surveys, 55(9):1–35.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nYasmin Moslem, Rejwanul Haque, and Andy Way. 2023.\nAdaptive machine translation with large language\nmodels. arXiv preprint arXiv:2301.13294.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers, pages 186–\n191, Brussels, Belgium. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. Language mod-\nels are unsupervised multitask learners.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP), pages 2685–2702, Online. Association\nfor Computational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\net al. 2022. Multitask prompted training enables\nzero-shot task generalization. In ICLR 2022-Tenth\nInternational Conference on Learning Representa-\ntions.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In 54th Annual Meeting of\nthe Association for Computational Linguistics, pages\n1715–1725. Association for Computational Linguis-\ntics (ACL).\nSuzanna Sia and Kevin Duh. 2022. Prefix embeddings\nfor in-context machine translation. In Proceedings\nof the 15th biennial conference of the Association\nfor Machine Translation in the Americas (Volume\n1: Research Track) , pages 45–57, Orlando, USA.\nAssociation for Machine Translation in the Americas.\nSuzanna Sia and Kevin Duh. 2023. In-context learn-\ning as maintaining coherency: A study of on-the-\nfly machine translation using large language models.\nIn Proceedings of Machine Translation Summit XIV\n(Volume 1: Research Track).\nMatthew Snover, Bonnie Dorr, Richard Schwartz, Lin-\nnea Micciulla, and John Makhoul. 2006. A study\nof translation edit rate with targeted human annota-\ntion. In Proceedings of the 7th Conference of the\nAssociation for Machine Translation in the Americas:\nTechnical Papers, pages 223–231.\nXin Tan, Longyin Zhang, Deyi Xiong, and Guodong\nZhou. 2019. Hierarchical modeling of global context\nfor document-level neural machine translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the 9th\nInternational Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pages 1576–1585.\n480\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nZhaopeng Tu, Yang Liu, Shuming Shi, and Tong Zhang.\n2018. Learning to remember translation history with\na continuous cache. Transactions of the Association\nfor Computational Linguistics, 6:407–420.\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,\nViresh Ratnakar, and George Foster. 2023. Prompt-\ning PaLM for translation: Assessing strategies and\nperformance. In Proceedings of the 61st Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 15406–\n15427, Toronto, Canada. Association for Computa-\ntional Linguistics.\nLongyue Wang, Chenyang Lyu, Tianbo Ji, Zhirui Zhang,\nDian Yu, Shuming Shi, and Zhaopeng Tu. 2023.\nDocument-level machine translation with large lan-\nguage models. arXiv preprint arXiv:2304.02210.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven\nLe Scao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander Rush. 2020. Huggingface’s\ntransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning: System Demonstrations , pages 38–45, Online.\nAssociation for Computational Linguistics.\nHongfei Xu, Deyi Xiong, Josef Van Genabith, and Qi-\nuhui Liu. 2021. Efficient context-aware neural ma-\nchine translation with layer-wise weighting and input-\naware gating. In Proceedings of the Twenty-Ninth\nInternational Conference on International Joint Con-\nferences on Artificial Intelligence, pages 3933–3940.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mt5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498.\nJingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian\nHan, Qizhang Feng, Haoming Jiang, Bing Yin, and\nXia Hu. 2023. Harnessing the power of llms in prac-\ntice: A survey on chatgpt and beyond. arXiv preprint\narXiv:2304.13712.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022. Glm-130b:\nAn open bilingual pre-trained model. arXiv preprint\narXiv:2210.02414.\nBiao Zhang, Barry Haddow, and Alexandra Birch. 2023.\nPrompting large language model for machine transla-\ntion: A case study. arXiv preprint arXiv:2301.07069.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, et al. 2023. A\nsurvey of large language models. arXiv preprint\narXiv:2303.18223.\nWenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu,\nLingpeng Kong, Jiajun Chen, Lei Li, and Shujian\nHuang. 2023. Multilingual machine translation with\nlarge language models: Empirical results and analy-\nsis. arXiv preprint arXiv:2304.04675.\n481\nA TER on Sentence-level Translations\nThe Translation Edit Rate (TER) is a metric\nintroduced by Snover et al. (2006) to quantify\nthe amount of human editing required to align\na system’s output with a reference translation.\nSpecifically, TER is calculated as the ratio of the\ntotal edits made to the length of the reference\ntranslation. Such edits encompass insertions, dele-\ntions, single-word substitutions, and shifts in word\nsequence. A lower TER indicates better alignment\nwith the reference. As illustrated in Figure 5,\nwhen evaluated using TER, LLMs do not exhibit a\nnoticeable improvement over the baseline model.\nFigure 5: Prompting (P) vs. QLoRA fine-tuning (FT) on sentence-level translation using various pretrained LLMs.\nBaseline is the NMT system described in Section 4.2. Rank r for QLoRA is set to 64.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.847468376159668
    },
    {
      "name": "Machine translation",
      "score": 0.7145010232925415
    },
    {
      "name": "Fine-tuning",
      "score": 0.6630692481994629
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6456074714660645
    },
    {
      "name": "Sentence",
      "score": 0.6207522749900818
    },
    {
      "name": "Scratch",
      "score": 0.6164543032646179
    },
    {
      "name": "Shot (pellet)",
      "score": 0.5638942122459412
    },
    {
      "name": "Language model",
      "score": 0.558351457118988
    },
    {
      "name": "Natural language processing",
      "score": 0.5377666354179382
    },
    {
      "name": "Translation (biology)",
      "score": 0.5366252064704895
    },
    {
      "name": "Natural language generation",
      "score": 0.4683897793292999
    },
    {
      "name": "Natural language",
      "score": 0.3312217593193054
    },
    {
      "name": "Programming language",
      "score": 0.06922069191932678
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Organic chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I162714631",
      "name": "George Mason University",
      "country": "US"
    }
  ],
  "cited_by": 23
}