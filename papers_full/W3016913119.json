{
    "title": "SPECTER: Document-level Representation Learning using Citation-informed Transformers",
    "url": "https://openalex.org/W3016913119",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A1983754593",
            "name": "Arman Cohan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2159323111",
            "name": "Sergey Feldman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2800204358",
            "name": "Iz Beltagy",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098223845",
            "name": "Doug Downey",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2964137255",
            "name": "Daniel Weld",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2897754576",
        "https://openalex.org/W2964311892",
        "https://openalex.org/W1875842236",
        "https://openalex.org/W2883199673",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4210257598",
        "https://openalex.org/W2122763686",
        "https://openalex.org/W2740934577",
        "https://openalex.org/W2964124573",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2467305910",
        "https://openalex.org/W2963918774",
        "https://openalex.org/W2138615112",
        "https://openalex.org/W90362830",
        "https://openalex.org/W2801930304",
        "https://openalex.org/W2963691697",
        "https://openalex.org/W2075322787",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2072840758",
        "https://openalex.org/W2962767366",
        "https://openalex.org/W2624431344",
        "https://openalex.org/W2131744502",
        "https://openalex.org/W2787560479",
        "https://openalex.org/W3102540985",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2963157208",
        "https://openalex.org/W2065108361",
        "https://openalex.org/W2735812278",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2525778437",
        "https://openalex.org/W2752172973",
        "https://openalex.org/W2101234009",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W2963514026",
        "https://openalex.org/W2803873993",
        "https://openalex.org/W2916106175",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2145360759",
        "https://openalex.org/W2796353546",
        "https://openalex.org/W2954058010",
        "https://openalex.org/W1673310716",
        "https://openalex.org/W2962717755",
        "https://openalex.org/W2787905871",
        "https://openalex.org/W2911926823",
        "https://openalex.org/W2786148476",
        "https://openalex.org/W2805482716",
        "https://openalex.org/W2971164983",
        "https://openalex.org/W168564468",
        "https://openalex.org/W1932742904",
        "https://openalex.org/W2922348034",
        "https://openalex.org/W1486649854",
        "https://openalex.org/W2743327679",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2952750383"
    ],
    "abstract": "Representation learning is a critical ingredient for natural language processing systems. Recent Transformer language models like BERT learn powerful textual representations, but these models are targeted towards token- and sentence-level training objectives and do not leverage information on inter-document relatedness, which limits their document-level representation power. For applications on scientific documents, such as classification and recommendation, the embeddings power strong performance on end tasks. We propose SPECTER, a new method to generate document-level embedding of scientific documents based on pretraining a Transformer language model on a powerful signal of document-level relatedness: the citation graph. Unlike existing pretrained language models, SPECTER can be easily applied to downstream applications without task-specific fine-tuning. Additionally, to encourage further research on document-level models, we introduce SciDocs, a new evaluation benchmark consisting of seven document-level tasks ranging from citation prediction, to document classification and recommendation. We show that SPECTER outperforms a variety of competitive baselines on the benchmark.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2270–2282\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n2270\nSPECTER: Document-level Representation Learning using\nCitation-informed Transformers\nArman Cohan†∗ Sergey Feldman†∗ Iz Beltagy† Doug Downey† Daniel S. Weld†,‡\n†Allen Institute for Artiﬁcial Intelligence\n‡Paul G. Allen School of Computer Science & Engineering, University of Washington\n{armanc,sergey,beltagy,dougd,danw}@allenai.org\nAbstract\nRepresentation learning is a critical ingre-\ndient for natural language processing sys-\ntems. Recent Transformer language mod-\nels like BERT learn powerful textual repre-\nsentations, but these models are targeted to-\nwards token- and sentence-level training ob-\njectives and do not leverage information on\ninter-document relatedness, which limits their\ndocument-level representation power. For ap-\nplications on scientiﬁc documents, such as\nclassiﬁcation and recommendation, the em-\nbeddings power strong performance on end\ntasks. We propose SPECTER , a new method to\ngenerate document-level embedding of scien-\ntiﬁc documents based on pretraining a Trans-\nformer language model on a powerful signal\nof document-level relatedness: the citation\ngraph. Unlike existing pretrained language\nmodels, S PECTER can be easily applied to\ndownstream applications without task-speciﬁc\nﬁne-tuning. Additionally, to encourage further\nresearch on document-level models, we intro-\nduce S CIDOCS , a new evaluation benchmark\nconsisting of seven document-level tasks rang-\ning from citation prediction, to document clas-\nsiﬁcation and recommendation. We show that\nSPECTER outperforms a variety of competitive\nbaselines on the benchmark.1\n1 Introduction\nAs the pace of scientiﬁc publication continues to\nincrease, Natural Language Processing (NLP) tools\nthat help users to search, discover and understand\nthe scientiﬁc literature have become critical. In re-\ncent years, substantial improvements in NLP tools\nhave been brought about by pretrained neural lan-\nguage models (LMs) (Radford et al., 2018; Devlin\net al., 2019; Yang et al., 2019). While such models\nare widely used for representing individual words\n∗ Equal contribution\n1 https://github.com/allenai/specter\nor sentences, extensions to whole-document em-\nbeddings are relatively underexplored. Likewise,\nmethods that do use inter-document signals to pro-\nduce whole-document embeddings (Tu et al., 2017;\nChen et al., 2019) have yet to incorporate state-\nof-the-art pretrained LMs. Here, we study how to\nleverage the power of pretrained language models\nto learn embeddings for scientiﬁc documents.\nA paper’s title and abstract provide rich seman-\ntic content about the paper, but, as we show in\nthis work, simply passing these textual ﬁelds to an\n“off-the-shelf” pretrained language model—even a\nstate-of-the-art model tailored to scientiﬁc text like\nthe recent SciBERT (Beltagy et al., 2019)—does\nnot result in accurate paper representations. The\nlanguage modeling objectives used to pretrain the\nmodel do not lead it to output representations that\nare helpful for document-level tasks such as topic\nclassiﬁcation or recommendation.\nIn this paper, we introduce a new method for\nlearning general-purpose vector representations of\nscientiﬁc documents. Our system, SPECTER ,2 in-\ncorporates inter-document context into the Trans-\nformer (Vaswani et al., 2017) language models\n(e.g., SciBERT (Beltagy et al., 2019)) to learn\ndocument representations that are effective across\na wide-variety of downstream tasks, without the\nneed for any task-speciﬁc ﬁne-tuning of the pre-\ntrained language model. We speciﬁcally use cita-\ntions as a naturally occurring, inter-document in-\ncidental supervision signal indicating which docu-\nments are most related and formulate the signal into\na triplet-loss pretraining objective. Unlike many\nprior works, at inference time, our model does not\nrequire any citation information. This is critical\nfor embedding new papers that have not yet been\ncited. In experiments, we show that SPECTER ’s\nrepresentations substantially outperform the state-\n2SPECTER : Scientiﬁc Paper Embeddings using Citation-\ninformed TransformERs\n2271\nof-the-art on a variety of document-level tasks, in-\ncluding topic classiﬁcation, citation prediction, and\nrecommendation.\nAs an additional contribution of this work, we in-\ntroduce and release SCIDOCS 3 , a novel collection\nof data sets and an evaluation suite for document-\nlevel embeddings in the scientiﬁc domain. SCI-\nDOCS covers seven tasks, and includes tens of thou-\nsands of examples of anonymized user signals of\ndocument relatedness. We also release our training\nset (hundreds of thousands of paper titles, abstracts\nand citations), along with our trained embedding\nmodel and its associated code base.\n2 Model\n2.1 Overview\nOur goal is to learn task-independent representa-\ntions of academic papers. Inspired by the recent\nsuccess of pretrained Transformer language models\nacross various NLP tasks, we use the Transformer\nmodel architecture as basis of encoding the input\npaper. Existing LMs such as BERT, however, are\nprimarily based on masked language modeling ob-\njective, only considering intra-document context\nand do not use any inter-document information.\nThis limits their ability to learn optimal document\nrepresentations. To learn high-quality document-\nlevel representations we propose using citations as\nan inter-document relatedness signal and formu-\nlate it as a triplet loss learning objective. We then\npretrain the model on a large corpus of citations\nusing this objective, encouraging it to output rep-\nresentations that are more similar for papers that\nshare a citation link than for those that do not. We\ncall our model SPECTER , which learns Scientiﬁc\nPaper Embeddings using Citation-informed Trans-\nformERs. With respect to the terminology used by\nDevlin et al. (2019), unlike most existing LMs that\nare “ﬁne-tuning based”, our approach results in em-\nbeddings that can be applied to downstream tasks\nin a “feature-based” fashion, meaning the learned\npaper embeddings can be easily used as features,\nwith no need for further task-speciﬁc ﬁne-tuning.\nIn the following, as background information, we\nbrieﬂy describe how pretrained LMs can be applied\nfor document representation and then discuss the\ndetails of SPECTER .\n3https://github.com/allenai/scidocs\nTransformer (initialized with SciBERT)\nRelated paper (P+)Query paper (PQ) Unrelated paper (P−)\nTriplet loss =max\n{(\nd\n(\nPQ, P+)\n−d\n(\nPQ, P−)\n+ m\n)\n, 0\n}\nFigure 1: Overview of SPECTER .\n2.2 Background: Pretrained Transformers\nRecently, pretrained Transformer networks have\ndemonstrated success on various NLP tasks (Rad-\nford et al., 2018; Devlin et al., 2019; Yang et al.,\n2019; Liu et al., 2019); we use these models as\nthe foundation for SPECTER . Speciﬁcally, we use\nSciBERT (Beltagy et al., 2019) which is an adap-\ntation of the original BERT (Devlin et al., 2019)\narchitecture to the scientiﬁc domain. The BERT\nmodel architecture (Devlin et al., 2019) uses multi-\nple layers of Transformers (Vaswani et al., 2017) to\nencode the tokens in a given input sequence. Each\nlayer consists of a self-attention sublayer followed\nby a feedforward sublayer. The ﬁnal hidden state\nassociated with the special [CLS] token is usually\ncalled the “pooled output”, and is commonly used\nas an aggregate representation of the sequence.\nDocument Representation Our goal is to repre-\nsent a given paper Pas a dense vector v that best\nrepresents the paper and can be used in downstream\ntasks. SPECTER builds embeddings from the title\nand abstract of a paper. Intuitively, we would ex-\npect these ﬁelds to be sufﬁcient to produce accurate\nembeddings, since they are written to provide a suc-\ncinct and comprehensive summary of the paper.4\nAs such, we encode the concatenated title and ab-\nstract using a Transformer LM (e.g., SciBERT) and\ntake the ﬁnal representation of the [CLS] token as\nthe output representation of the paper:5\nv = Transformer(input)[CLS], (1)\nwhere Transformer is the Transformer’s for-\nward function, and input is the concatenation of\nthe [CLS] token and WordPieces (Wu et al., 2016)\nof the title and abstract of a paper, separated by\n4We also experimented with additional ﬁelds such as\nvenues and authors but did not ﬁnd any empirical advantage\nin using those (see §6). See §7 for a discussion of using the\nfull text of the paper as input.\n5It is also possible to encode title and abstracts individually\nand then concatenate or combine them to get the ﬁnal embed-\nding. However, in our experiments this resulted in sub-optimal\nperformance.\n2272\nthe [SEP] token. We use SciBERT as our model\ninitialization as it is optimized for scientiﬁc text,\nthough our formulation is general and any Trans-\nformer language model instead of SciBERT. Using\nthe above method with an “off-the-shelf” SciBERT\ndoes not take global inter-document information\ninto account. This is because SciBERT, like other\npretrained language models, is trained via language\nmodeling objectives, which only predict words or\nsentences given their in-document, nearby textual\ncontext. In contrast, we propose to incorporate ci-\ntations into the model as a signal of inter-document\nrelatedness, while still leveraging the model’s ex-\nisting strength in modeling language.\n2.3 Citation-Based Pretraining Objective\nA citation from one document to another suggests\nthat the documents are related. To encode this relat-\nedness signal into our representations, we design\na loss function that trains the Transformer model\nto learn closer representations for papers when one\ncites the other, and more distant representations\notherwise. The high-level overview of the model is\nshown in Figure 1.\nIn particular, each training instance is a triplet of\npapers: a query paper PQ, a positive paper P+ and\na negative paper P−. The positive paper is a paper\nthat the query paper cites, and the negative paper\nis a paper that is not cited by the query paper (but\nthat may be cited by P+). We then train the model\nusing the following triplet margin loss function:\nL= max\n{(\nd\n(\nPQ, P+)\n−d\n(\nPQ, P−)\n+ m\n)\n, 0\n}\n(2)\nwhere d is a distance function and m is the loss\nmargin hyperparameter (we empirically choose\nm = 1). Here, we use the L2 norm distance:\nd(PA, PB) = ∥vA −vB∥2,\nwhere vA is the vector corresponding to the pooled\noutput of the Transformer run on paperA (Equation\n1).6 Starting from the trained SciBERT model, we\npretrain the Transformer parameters on the citation\nobjective to learn paper representations that capture\ndocument relatedness.\n2.4 Selecting Negative Distractors\nThe choice of negative example papers P−is im-\nportant when training the model. We consider two\nsets of negative examples: the ﬁrst set simply con-\nsists of randomly selected papers from the corpus.\n6We also experimented with other distance functions (e..g,\nnormalized cosine), but they underperformed the L2 loss.\nGiven a query paper, intuitively we would expect\nthe model to be able to distinguish between cited\npapers, and uncited papers sampled randomly from\nthe entire corpus. This inductive bias has been\nalso found to be effective in content-based citation\nrecommendation applications (Bhagavatula et al.,\n2018). But, random negatives may be easy for the\nmodel to distinguish from the positives. To provide\na more nuanced training signal, we augment the\nrandomly drawn negatives with a more challenging\nsecond set of negative examples. We denote as\n“hard negatives” the papers that are not cited by the\nquery paper, but are cited by a paper cited by the\nquery paper, i.e. if P1 cite−−→P2 and P2 cite−−→P3\nbut P1 ̸cite−−→P3, then P3 is a candidate hard nega-\ntive example for P1. We expect the hard negatives\nto be somewhat related to the query paper, but typi-\ncally less related than the cited papers. As we show\nin our experiments (§6), including hard negatives\nresults in more accurate embeddings compared to\nusing random negatives alone.\n2.5 Inference\nAt inference time, the model receives one paper, P,\nand it outputs the SPECTER ’s Transfomer pooled\noutput activation as the paper representation for P\n(Equation 1). We note that for inference, SPECTER\nrequires only the title and abstract of the given\ninput paper; the model does not need any citation\ninformation about the input paper. This means that\nSPECTER can produce embeddings even for new\npapers that have yet to be cited, which is critical\nfor applications that target recent scientiﬁc papers.\n3 S CIDOCS Evaluation Framework\nPrevious evaluations of scientiﬁc document repre-\nsentations in the literature tend to focus on small\ndatasets over a limited set of tasks, and extremely\nhigh (99%+) AUC scores are already possible on\nthese data for English documents (Chen et al., 2019;\nWang et al., 2019). New, larger and more diverse\nbenchmark datasets are necessary. Here, we intro-\nduce a new comprehensive evaluation framework\nto measure the effectiveness of scientiﬁc paper em-\nbeddings, which we call SCIDOCS . The framework\nconsists of diverse tasks, ranging from citation pre-\ndiction, to prediction of user activity, to document\nclassiﬁcation and paper recommendation. Note that\nSPECTER will not be further ﬁne-tuned on any of\nthe tasks; we simply plug in the embeddings as fea-\ntures for each task. Below, we describe each of the\n2273\ntasks in detail and the evaluation data associated\nwith it. In addition to our training data, we release\nall the datasets associated with the evaluation tasks.\n3.1 Document Classiﬁcation\nAn important test of a document-level embedding is\nwhether it is predictive of the class of the document.\nHere, we consider two classiﬁcation tasks in the\nscientiﬁc domain:\nMeSH Classiﬁcation In this task, the goals is to\nclassify scientiﬁc papers according to their Medi-\ncal Subject Headings (MeSH) (Lipscomb, 2000).7\nWe construct a dataset consisting of 23K academic\nmedical papers, where each paper is assigned one\nof 11 top-level disease classes such as cardiovas-\ncular diseases, diabetes, digestive diseases derived\nfrom the MeSH vocabulary. The most populated\ncategory is Neoplasms (cancer) with 5.4K instances\n(23.3% of the total dataset) while the category with\nleast number of samples is Hepatitis (1.7% of the\ntotal dataset). We follow the approach of Feldman\net al. (2019) in mapping the MeSH vocabulary to\nthe disease classes.\nPaper Topic Classiﬁcation This task is predict-\ning the topic associated with a paper using the pre-\ndeﬁned topic categories of the Microsoft Academic\nGraph (MAG) (Sinha et al., 2015) 8. MAG pro-\nvides a database of papers, each tagged with a list\nof topics. The topics are organized in a hierarchy\nof 5 levels, where level 1 is the most general and\nlevel 5 is the most speciﬁc. For our evaluation,\nwe derive a document classiﬁcation dataset from\nthe level 1 topics, where a paper is labeled by its\ncorresponding level 1 MAG topic. We construct a\ndataset of 25K papers, almost evenly split over the\n19 different classes of level 1 categories in MAG.\n3.2 Citation Prediction\nAs argued above, citations are a key signal of re-\nlatedness between papers. We test how well differ-\nent paper representations can reproduce this signal\nthrough citation prediction tasks. In particular, we\nfocus on two sub-tasks: predicting direct citations,\nand predicting co-citations. We frame these as\nranking tasks and evaluate performance using MAP\nand nDCG , standard ranking metrics.\n7https://www.nlm.nih.gov/mesh/meshhome.\nhtml\n8https://academic.microsoft.com/\nDirect Citations In this task, the model is asked\nto predict which papers are cited by a given query\npaper from a given set of candidate papers. The\nevaluation dataset includes approximately 30K to-\ntal papers from a held-out pool of papers, con-\nsisting of 1K query papers and a candidate set of\nup to 5 cited papers and 25 (randomly selected)\nuncited papers. The task is to rank the cited papers\nhigher than the uncited papers. For each embed-\nding method, we require only comparing the L2\ndistance between the raw embeddings of the query\nand the candidates, without any additional trainable\nparameters.\nCo-Citations This task is similar to the direct\ncitations but instead of predicting a cited paper,\nthe goal is to predict a highly co-cited paper with\na given paper. Intuitively, if papers A and B are\ncited frequently together by several papers, this\nshows that the papers are likely highly related and\na good paper representation model should be able\nto identify these papers from a given candidate\nset. The dataset consists of 30K total papers and is\nconstructed similar to the direct citationstask.\n3.3 User Activity\nThe embeddings for similar papers should be close\nto each other; we use user activity as a proxy for\nidentifying similar papers and test the model’s abil-\nity to recover this information. Multiple users con-\nsuming the same items as one another is a classic\nrelatedness signal and forms the foundation for rec-\nommender systems and other applications (Schafer\net al., 2007). In our case, we would expect that\nwhen users look for academic papers, the papers\nthey view in a single browsing session tend to be\nrelated. Thus, accurate paper embeddings should,\nall else being equal, be relatively more similar for\npapers that are frequently viewed in the same ses-\nsion than for other papers. To build benchmark\ndatasets to test embeddings on user activity, we\nobtained logs of user sessions from a major aca-\ndemic search engine. We deﬁne the following two\ntasks on which we build benchmark datasets to test\nembeddings:\nCo-Views Our co-views dataset consists of ap-\nproximately 30K papers. To construct it, we take\n1K random papers that are not in our train or de-\nvelopment set and associate with each one up to 5\nfrequently co-viewed papers and 25 randomly se-\nlected papers (similar to the approach for citations).\nThen, we require the embedding model to rank the\n2274\nco-viewed papers higher than the random papers\nby comparing the L2 distances of raw embeddings.\nWe evaluate performance using standard ranking\nmetrics, nDCG and MAP .\nCo-Reads If the user clicks to access the PDF\nof a paper from the paper description page, this\nis a potentially stronger sign of interest in the pa-\nper. In such a case we assume the user will read at\nleast parts of the paper and refer to this as a “read”\naction. Accordingly, we deﬁne a “co-reads” task\nand dataset analogous to the co-views dataset de-\nscribed above. This dataset is also approximately\n30K papers.\n3.4 Recommendation\nIn the recommendation task, we evaluate the abil-\nity of paper embeddings to boost performance in\na production recommendation system. Our rec-\nommendation task aims to help users navigate the\nscientiﬁc literature by ranking a set of “similar pa-\npers” for a given paper. We use a dataset of user\nclickthrough data for this task which consists of\n22K clickthrough events from a public scholarly\nsearch engine. We partitioned the examples tem-\nporally into train (20K examples), validation (1K),\nand test (1K) sets. As is typical in clickthrough data\non ranked lists, the clicks are biased toward the top\nof original ranking presented to the user. To coun-\nteract this effect, we computed propensity scores\nusing a swap experiment (Agarwal et al., 2019).\nThe propensity scores give, for each position in the\nranked list, the relative frequency that the position\nis over-represented in the data due to exposure bias.\nWe can then compute de-biased evaluation metrics\nby dividing the score for each test example by the\npropensity score for the clicked position. We report\npropensity-adjusted versions of the standard rank-\ning metrics Precision@1 ( ˆP@1) and Normalized\nDiscounted Cumulative Gain ( ˆnDCG ).\nWe test different embeddings on the recommen-\ndation task by including cosine embedding dis-\ntance9 as a feature within an existing recommenda-\ntion system that includes several other informative\nfeatures (title/author similarity, reference and ci-\ntation overlap, etc.). Thus, the recommendation\nexperiments measure whether the embeddings can\nboost the performance of a strong baseline system\non an end task. For SPECTER , we also perform an\nonline A/B test to measure whether its advantages\n9Embeddings are L2 normalized and in this case cosine\ndistance is equivalent to L2 distance.\non the ofﬂine dataset translate into improvements\non the online recommendation task (§5).\n4 Experiments\nTraining Data To train our model, we use a\nsubset of the Semantic Scholar corpus (Ammar\net al., 2018) consisting of about 146K query papers\n(around 26.7M tokens) with their corresponding\noutgoing citations, and we use an additional 32K\npapers for validation. For each query paper we con-\nstruct up to 5 training triples comprised of a query,\na positive, and a negative paper. The positive pa-\npers are sampled from the direct citations of the\nquery, while negative papers are chosen either ran-\ndomly or from citations of citations (as discussed in\n§2.4). We empirically found it helpful to use 2 hard\nnegatives (citations of citations) and 3 easy neg-\natives (randomly selected papers) for each query\npaper. This process results in about 684K training\ntriples and 145K validation triples.\nTraining and Implementation We implement\nour model in AllenNLP (Gardner et al., 2018).\nWe initialize the model from SciBERT pretrained\nweights (Beltagy et al., 2019) since it is the state-\nof-the-art pretrained language model on scientiﬁc\ntext. We continue training all model parameters on\nour training objective (Equation 2). We perform\nminimal tuning of our model’s hyperparameters\nbased on the performance on the validation set,\nwhile baselines are extensively tuned. Based on\ninitial experiments, we use a margin m=1 for the\ntriplet loss. For training, we use the Adam opti-\nmizer (Kingma and Ba, 2014) following the sug-\ngested hyperparameters in Devlin et al. (2019) (LR:\n2e-5, Slanted Triangular LR scheduler10 (Howard\nand Ruder, 2018) with number of train steps equal\nto training instances and cut fraction of 0.1). We\ntrain the model on a single Titan V GPU (12G\nmemory) for 2 epochs, with batch size of 4 (the\nmaximum that ﬁt in our GPU memory) and use\ngradient accumulation for an effective batch size of\n32. Each training epoch takes approximately 1-2\ndays to complete on the full dataset. We release\nour code and data to facilitate reproducibility. 11\nTask-Speciﬁc Model Details For the classiﬁca-\ntion tasks, we used a linear SVM where embed-\nding vectors were the only features. The C hyper-\nparameter was tuned via a held-out validation set.\n10Learning rate linear warmup followed by linear decay.\n11https://github.com/allenai/specter\n2275\nFor the recommendation tasks, we use a feed-\nforward ranking neural network that takes as input\nten features designed to capture the similarity be-\ntween each query and candidate paper, including\nthe cosine similarity between the query and candi-\ndate embeddings and manually-designed features\ncomputed from the papers’ citations, titles, authors,\nand publication dates.\nBaseline Methods Our work falls into the inter-\nsection of textual representation, citation mining,\nand graph learning, and we evaluate against state-\nof-the-art baselines from each of these areas. We\ncompare with several strong textual models: SIF\n(Arora et al., 2017), a method for learning docu-\nment representations by removing the ﬁrst prin-\ncipal component of aggregated word-level embed-\ndings which we pretrain on scientiﬁc text; SciBERT\n(Beltagy et al., 2019) a state-of-the-art pretrained\nTransformer LM for scientiﬁc text; and Sent-BERT\n(Reimers and Gurevych, 2019), a model that uses\nnegative sampling to tune BERT for producing op-\ntimal sentence embeddings. We also compare with\nCiteomatic (Bhagavatula et al., 2018), a closely\nrelated paper representation model for citation pre-\ndiction which trains content-based representations\nwith citation graph information via dynamically\nsampled triplets, and SGC (Wu et al., 2019a), a\nstate-of-the-art graph-convolutional approach. For\ncompleteness, additional baselines are also in-\ncluded; due to space constraints we refer to Ap-\npendix A for detailed discussion of all baselines.\nWe tune hyperparameters of baselines to maximize\nperformance on a separate validation set.\n5 Results\nTable 1 presents the main results corresponding\nto our evaluation tasks (described in §3). Overall,\nwe observe substantial improvements across all\ntasks with average performance of 80.0 across all\nmetrics on all tasks which is a 3.1 point absolute\nimprovement over the next-best baseline. We now\ndiscuss the results in detail.\nFor document classiﬁcation, we report macro\nF1, a standard classiﬁcation metric. We observe\nthat the classiﬁer performance when trained on our\nrepresentations is better than when trained on any\nother baseline. Particularly, on the MeSH (MAG)\ndataset, we obtain an 86.4 (82.0) F1 score which is\nabout a ∆= + 2.3 (+1.5) point absolute increase\nover the best baseline on each dataset respectively.\nOur evaluation of the learned representations on\npredicting user activity is shown in the “User activ-\nity” columns of Table 1. SPECTER achieves a MAP\nscore of 83.8 on the co-view task, and 84.5 on co-\nread, improving over the best baseline (Citeomatic\nin this case) by 2.7 and 4.0 points, respectively.\nWe observe similar trends for the “citation” and\n“co-citation” tasks, with our model outperforming\nvirtually all other baselines except for SGC, which\nhas access to the citation graph at training and test\ntime.12 Note that methods like SGC cannot be\nused in real-world setting to embed new papers\nthat are not cited yet. On the other hand, on co-\ncitation data our method is able to achieve the best\nresults with n DCG of 94.8, improving over SGC\nwith 2.3 points. Citeomatic also performs well on\nthe citation tasks, as expected given that its primary\ndesign goal was citation prediction. Nevertheless,\nour method slightly outperforms Citeomatic on the\ndirect citation task, while substantially outperform-\ning it on co-citations (+2.0 nDCG ).\nFinally, for recommendation task, we observe\nthat SPECTER outperforms all other models on this\ntask as well, with n DCG of 53.9. On the recom-\nmendations task, as opposed to previous experi-\nments, the differences in method scores are gen-\nerally smaller. This is because for this task the\nembeddings are used along with several other in-\nformative features in the ranking model (described\nunder task-speciﬁc models in §4), meaning that em-\nbedding variants have less opportunity for impact\non overall performance.\nWe also performed an online study to evaluate\nwhether SPECTER embeddings offer similar advan-\ntages in a live application. We performed an online\nA/B test comparing our SPECTER -based recom-\nmender to an existing production recommender sys-\ntem for similar papers that ranks papers by a textual\nsimilarity measure. In a dataset of 4,113 clicks, we\nfound that SPECTER ranker improved clickthrough\nrate over the baseline by 46.5%, demonstrating its\nsuperiority.\nWe emphasize that our citation-based pretrain-\ning objective is critical for the performance of\nSPECTER ; removing this and using a vanilla SciB-\nERT results in decreased performance on all tasks.\n12For SGC, we remove development and test set citations\nand co-citations during training. We also remove incoming\ncitations from development and test set queries as these would\nnot be available at test time in production.\n2276\nTask → Classiﬁcation User activity prediction Citation prediction Recomm. Avg.Subtask → MAG MeSH Co-View Co-Read Cite Co-Cite\nModel ↓/ Metric → F1 F1 MAP nDCG MAP nDCG MAP nDCG MAP nDCG ˆnDCG ˆP@1\nRandom 4.8 9.4 25.2 51.6 25.6 51.9 25.1 51.5 24.9 51.4 51.3 16.8 32.5\nDoc2vec (2014) 66.2 69.2 67.8 82.9 64.9 81.6 65.3 82.2 67.1 83.4 51.7 16.9 66.6\nFasttext-sum (2017) 78.1 84.1 76.5 87.9 75.3 87.4 74.6 88.1 77.8 89.6 52.5 18.0 74.1\nSIF (2017) 78.4 81.4 79.4 89.4 78.2 88.9 79.4 90.5 80.8 90.9 53.4 19.5 75.9\nELMo (2018) 77.0 75.7 70.3 84.3 67.4 82.6 65.8 82.6 68.5 83.8 52.5 18.2 69.0\nCiteomatic (2018) 67.1 75.7 81.1 90.2 80.5 90.2 86.3 94.1 84.4 92.8 52.5 17.3 76.0\nSGC (2019a) 76.8 82.7 77.2 88.0 75.7 87.5 91.6 96.2 84.1 92.5 52.7 18.2 76.9\nSciBERT (2019) 79.7 80.7 50.7 73.1 47.7 71.1 48.3 71.7 49.7 72.6 52.1 17.9 59.6\nSent-BERT (2019) 80.5 69.1 68.2 83.3 64.8 81.3 63.5 81.6 66.4 82.8 51.6 17.1 67.5\nSPECTER (Ours) 82.0 86.4 83.6 91.5 84.5 92.4 88.3 94.9 88.1 94.8 53.9 20.0 80.0\nTable 1: Results on the SCIDOCS evaluation suite consisting of 7 tasks.\n6 Analysis\nIn this section, we analyze several design deci-\nsions in SPECTER , provide a visualization of its\nembedding space, and experimentally compare\nSPECTER ’s use of ﬁxed embeddings against a ﬁne-\ntuning approach.\nAblation Study We start by analyzing how\nadding or removing metadata ﬁelds from the in-\nput to SPECTER alters performance. The results\nare shown in the top four rows of Table 2 (for\nbrevity, here we only report the average of the met-\nrics from each task). We observe that removing\nthe abstract from the textual input and relying only\non the title results in a substantial decrease in per-\nformance. More surprisingly, adding authors as an\ninput (along with title and abstract) hurts perfor-\nmance.13 One possible explanation is that author\nnames are sparse in the corpus, making it difﬁcult\nfor the model to infer document-level relatedness\nfrom them. As another possible reason of this be-\nhavior, tokenization using Wordpieces might be\nsuboptimal for author names. Many author names\nare out-of-vocabulary for SciBERT and thus, they\nmight be split into sub-words and shared across\nnames that are not semantically related, leading\nto noisy correlation. Finally, we ﬁnd that adding\nvenues slightly decreases performance, 14 except\non document classiﬁcation (which makes sense, as\nwe would expect venues to have high correlation\n13We experimented with both concatenating authors with\nthe title and abstract and also considering them as an additional\nﬁeld. Neither were helpful.\n14Venue information in our data came directly from pub-\nlisher provided metadata and thus was not normalized. Venue\nnormalization could help improve results.\nCLS USR CITE REC Avg.\nSPECTER 84.2 88.4 91.5 36.9 80.0\n−abstract 82.2 72.2 73.6 34.5 68.1\n+ venue 84.5 88.0 91.2 36.7 79.9\n+ author 82.7 72.3 71.0 34.6 67.3\nNo hard negatives 82.4 85.8 89.8 36.8 78.4\nStart w/ BERT-Large 81.7 85.9 87.8 36.1 77.5\nTable 2: Ablations: Numbers are averages of metrics\nfor each evaluation task: CLS: classiﬁcation, USR:\nUser activity, CITE: Citation prediction, REC: Recom-\nmendation, Avg. average over all tasks & metrics.\nwith paper topics). The fact that SPECTER does not\nrequire inputs like authors or venues makes it appli-\ncable in situations where this metadata is not avail-\nable, such as matching reviewers with anonymized\nsubmissions, or performing recommendations of\nanonymized preprints (e.g., on OpenReview).\nOne design decision inSPECTER is to use a set of\nhard negative distractors in the citation-based ﬁne-\ntuning objective. The ﬁfth row of Table 2 shows\nthat this is important—using only easy negatives re-\nduces performance on all tasks. While there could\nbe other potential ways to include hard negatives in\nthe model, our simple approach of including cita-\ntions of citations is effective. The sixth row of the\ntable shows that using a strong general-domain lan-\nguage model (BERT-Large) instead of SciBERT in\nSPECTER reduces performance considerably. This\nis reasonable because unlike BERT-Large, SciB-\nERT is pretrained on scientiﬁc text.\nVisualization Figure 2 shows t-SNE (van der\nMaaten, 2014) projections of our embeddings\n(SPECTER ) compared with the SciBERT baseline\n2277\n(a) SPECTER\n (b) SciBERT\nFigure 2: t-SNE visualization of paper embeddings and\ntheir corresponding MAG topics.\nfor a random set of papers. When comparing\nSPECTER embeddings with SciBERT, we observe\nthat our embeddings are better at encoding topi-\ncal information, as the clusters seem to be more\ncompact. Further, we see some examples of cross-\ntopic relatedness reﬂected in the embedding space\n(e.g., Engineering, Mathematics and Computer\nScience are close to each other, while Business\nand Economics are also close to each other). To\nquantify the comparison of visualized embeddings\nin Figure 2, we use the DBScan clustering algo-\nrithm (Ester et al., 1996) on this 2D projection.\nWe use the completeness and homogeneity cluster-\ning quality measures introduced by Rosenberg and\nHirschberg (2007). For the points corresponding to\nFigure 2, the homogeneity and completeness val-\nues for SPECTER are respectively 0.41 and 0.72\ncompared with SciBERT’s 0.19 and 0.63, a clear\nimprovement on separating topics using the pro-\njected embeddings.\nComparison with Task Speciﬁc Fine-Tuning\nWhile the fact that SPECTER does not require ﬁne-\ntuning makes its paper embeddings less costly to\nuse, often the best performance from pretrained\nTransformers is obtained when the models are ﬁne-\ntuned directly on each end task. We experiment\nwith ﬁne-tuning SciBERT on our tasks, and ﬁnd\nthis to be generally inferior to using our ﬁxed rep-\nresentations from SPECTER . Speciﬁcally, we ﬁne-\ntune SciBERT directly on task-speciﬁc signals in-\nstead of citations. To ﬁne-tune on task-speciﬁc\ndata (e.g., user activity), we used a dataset of co-\nviews with 65K query papers, co-reads with 14K\nquery papers, and co-citations (instead of direct\ncitations) with 83K query papers. As the end tasks\nare ranking tasks, for all datasets we construct up\nto 5 triplets and ﬁne-tune the model using triplet\nranking loss. The positive papers are sampled from\nTraining signal CLS USR CITE REC All\nSPECTER 84.2 88.4 91.5 36.9 80.0\nSciBERT ﬁne-tune on co-view 83.0 84.2 84.1 36.4 76.0\nSciBERT ﬁne-tune on co-read 82.3 85.4 86.7 36.3 77.1\nSciBERT ﬁne-tune on co-citation 82.9 84.3 85.2 36.6 76.4\nSciBERT ﬁne-tune on multitask 83.3 86.1 88.2 36.0 78.0\nTable 3: Comparison with task-speciﬁc ﬁne-tuning.\nthe most co-viewed (co-read, or co-cited) papers\ncorresponding to the query paper. We also include\nboth easy and hard distractors as when training\nSPECTER (for hard negatives we choose the least\nnon-zero co-viewed (co-read, or co-cited) papers).\nWe also consider training jointly on all task-speciﬁc\ntraining data sources in a multitask training process,\nwhere the model samples training triplets from a\ndistribution over the sources. As illustrated in Ta-\nble 3, without any additional ﬁnal task-speciﬁc\nﬁne-tuning, SPECTER still outperforms a SciBERT\nmodel ﬁne-tuned on the end tasks as well as their\nmultitask combination, further demonstrating the\neffectiveness and versatility of SPECTER embed-\ndings.15\n7 Related Work\nRecent representation learning methods in NLP\nrely on training large neural language models on un-\nsupervised data (Peters et al., 2018; Radford et al.,\n2018; Devlin et al., 2019; Beltagy et al., 2019; Liu\net al., 2019). While successful at many sentence-\nand token-level tasks, our focus is on using the\nmodels for document-level representation learning,\nwhich has remained relatively under-explored.\nThere have been other efforts in document repre-\nsentation learning such as extensions of word vec-\ntors to documents (Le and Mikolov, 2014; Ganesh\net al., 2016; Liu et al., 2017; Wu et al., 2018; Gy-\nsel et al., 2017), convolution-based methods (Liu\net al., 2018; Zamani et al., 2018), and variational\nautoencoders (Holmer and Marfurt, 2018; Wang\net al., 2019). Relevant to document embedding, sen-\ntence embedding is a relatively well-studied area of\nresearch. Successful approaches include seq2seq\nmodels (Kiros et al., 2015), BiLSTM Siamese\nnetworks (Williams et al., 2018), leveraging su-\npervised data from other corpora (Conneau et al.,\n2017), and using discourse relations (Nie et al.,\n2019), and BERT-based methods (Reimers and\nGurevych, 2019). Unlike our proposed method,\n15We also experimented with further task-speciﬁc ﬁne-\ntuning of our SPECTER on the end tasks but we did not observe\nadditional improvements.\n2278\nthe majority of these approaches do not consider\nany notion of inter-document relatedness when em-\nbedding documents.\nOther relevant work combines textual features\nwith network structure (Tu et al., 2017; Zhang et al.,\n2018; Bhagavatula et al., 2018; Shen et al., 2018;\nChen et al., 2019; Wang et al., 2019). These works\ntypically do not leverage the recent pretrained con-\ntextual representations and with a few exceptions\nsuch as the recent work by Wang et al. (2019), they\ncannot generalize to unseen documents like our\nSPECTER approach. Context-based citation rec-\nommendation is another related application where\nmodels rely on citation contexts (Jeong et al., 2019)\nto make predictions. These works are orthogonal\nto ours as the input to our model is just paper title\nand abstract. Another related line of work is graph-\nbased representation learning methods (Bruna et al.,\n2014; Kipf and Welling, 2017; Hamilton et al.,\n2017a,b; Wu et al., 2019a,b). Here, we compare to\na graph representation learning model, SGC (Sim-\nple Graph Convolution) (Wu et al., 2019a), which\nis a state-of-the-art graph convolution approach for\nrepresentation learning. SPECTER uses pretrained\nlanguage models in combination with graph-based\ncitation signals, which enables it to outperform the\ngraph-based approaches in our experiments.\nSPECTER embeddings are based on only the title\nand abstract of the paper. Adding the full text of the\npaper would provide a more complete picture of the\npaper’s content and could improve accuracy (Co-\nhen et al., 2010; Lin, 2008; Schuemie et al., 2004).\nHowever, the full text of many academic papers\nis not freely available. Further, modern language\nmodels have strict memory limits on input size,\nwhich means new techniques would be required in\norder to leverage the entirety of the paper within\nthe models. Exploring how to use the full paper\ntext within SPECTER is an item of future work.\nFinally, one pain point in academic paper rec-\nommendation research has been a lack of publicly\navailable datasets (Chen and Lee, 2018; Kanakia\net al., 2019). To address this challenge, we re-\nlease SCIDOCS , our evaluation benchmark which\nincludes an anonymized clickthrough dataset from\nan online recommendations system.\n8 Conclusions and Future Work\nWe present SPECTER , a model for learning repre-\nsentations of scientiﬁc papers, based on a Trans-\nformer language model that is pretrained on cita-\ntions. We achieve substantial improvements over\nthe strongest of a wide variety of baselines, demon-\nstrating the effectiveness of our model. We ad-\nditionally introduce SCIDOCS , a new evaluation\nsuite consisting of seven document-level tasks and\nrelease the corresponding datasets to foster further\nresearch in this area.\nThe landscape of Transformer language models\nis rapidly changing and newer and larger models\nare frequently introduced. It would be interest-\ning to initialize our model weights from more re-\ncent Transformer models to investigate if additional\ngains are possible. Another item of future work is\nto develop better multitask approaches to leverage\nmultiple signals of relatedness information during\ntraining. We used citations to build triplets for our\nloss function, however there are other metrics that\nhave good support from the bibliometrics literature\n(Klavans and Boyack, 2006) that warrant exploring\nas a way to create relatedness graphs. Including\nother information such as outgoing citations as ad-\nditional input to the model would be yet another\narea to explore in future.\nAcknowledgements\nWe thank Kyle Lo, Daniel King and Oren Etzioni\nfor helpful research discussions, Russel Reas for\nsetting up the public API, Field Cady for help in\ninitial data collection and the anonymous reviewers\n(especially Reviewer 1) for comments and sugges-\ntions. This work was supported in part by NSF\nConvergence Accelerator award 1936940, ONR\ngrant N00014-18-1-2193, and the University of\nWashington WRF/Cable Professorship.\nReferences\nAnant K. Agarwal, Ivan Zaitsev, Xuanhui Wang,\nCheng Yen Li, Marc Najork, and Thorsten Joachims.\n2019. Estimating position bias without intrusive in-\nterventions. In WSDM.\nWaleed Ammar, Dirk Groeneveld, Chandra Bha-\ngavatula, Iz Beltagy, Miles Crawford, Doug\nDowney, Jason Dunkelberger, Ahmed Elgohary,\nSergey Feldman, Vu Ha, Rodney Kinney, Sebas-\ntian Kohlmeier, Kyle Lo, Tyler C. Murray, Hsu-\nHan Ooi, Matthew E. Peters, Joanna Power, Sam\nSkjonsberg, Lucy Lu Wang, Christopher Wilhelm,\nZheng Yuan, Madeleine van Zuylen, and Oren Et-\nzioni. 2018. Construction of the literature graph in\nsemantic scholar. In NAACL-HLT.\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\n2279\nA simple but tough-to-beat baseline for sentence em-\nbeddings. In ICLR.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A Pretrained Language Model for Scientiﬁc\nText. In EMNLP.\nChandra Bhagavatula, Sergey Feldman, Russell Power,\nand Waleed Ammar. 2018. Content-Based Citation\nRecommendation. In NAACL-HLT.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. TACL.\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and\nYann LeCun. 2014. Spectral networks and locally\nconnected networks on graphs. ICLR.\nLiqun Chen, Guoyin Wang, Chenyang Tao, Ding-\nhan Shen, Pengyu Cheng, Xinyuan Zhang, Wenlin\nWang, Yizhe Zhang, and Lawrence Carin. 2019. Im-\nproving textual network embedding with global at-\ntention via optimal transport. In ACL.\nTsung Teng Chen and Maria Lee. 2018. Research Pa-\nper Recommender Systems on Big Scholarly Data.\nIn Knowledge Management and Acquisition for In-\ntelligent Systems.\nK. Bretonnel Cohen, Helen L. Johnson, Karin M. Ver-\nspoor, Christophe Roeder, and Lawrence Hunter.\n2010. The structural and content aspects of abstracts\nversus bodies of full text journal articles are different.\nBMC Bioinformatics, 11:492–492.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc\nBarrault, and Antoine Bordes. 2017. Supervised\nLearning of Universal Sentence Representations\nfrom Natural Language Inference Data. In EMNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT.\nMartin Ester, Hans-Peter Kriegel, J¨org Sander, Xiaowei\nXu, et al. 1996. A Density-based Algorithm for Dis-\ncovering Clusters in Large Spatial Databases with\nNoise. In KDD.\nSergey Feldman, Waleed Ammar, Kyle Lo, Elly Trep-\nman, Madeleine van Zuylen, and Oren Etzioni. 2019.\nQuantifying Sex Bias in Clinical Studies at Scale\nWith Automated Data Extraction. JAMA.\nJ Ganesh, Manish Gupta, and Vijay K. Varma. 2016.\nDoc2sent2vec: A novel two-phase approach for\nlearning document representation. In SIGIR.\nMatt Gardner, Joel Grus, Mark Neumann, Oyvind\nTafjord, Pradeep Dasigi, Nelson F. Liu, Matthew Pe-\nters, Michael Schmitz, and Luke Zettlemoyer. 2018.\nAllenNLP: A Deep Semantic Natural Language Pro-\ncessing Platform. In Proceedings of Workshop for\nNLP Open Source Software (NLP-OSS).\nChristophe Van Gysel, Maarten de Rijke, and Evange-\nlos Kanoulas. 2017. Neural Vector Spaces for Un-\nsupervised Information Retrieval. ACM Trans. Inf.\nSyst.\nWill Hamilton, Zhitao Ying, and Jure Leskovec. 2017a.\nInductive Representation Learning on Large Graphs.\nIn NIPS.\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec.\n2017b. Inductive representation learning on large\ngraphs. In NIPS.\nErik Holmer and Andreas Marfurt. 2018. Explaining\naway syntactic structure in semantic document rep-\nresentations. ArXiv, abs/1806.01620.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nLanguage Model Fine-tuning for Text Classiﬁcation.\nIn ACL.\nChanwoo Jeong, Sion Jang, Hyuna Shin, Eun-\njeong Lucy Park, and Sungchul Choi. 2019. A\ncontext-aware citation recommendation model with\nbert and graph convolutional networks. ArXiv,\nabs/1903.06464.\nAnshul Kanakia, Zhihong Shen, Darrin Eide, and\nKuansan Wang. 2019. A Scalable Hybrid Research\nPaper Recommender System for Microsoft Aca-\ndemic. In WWW.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam:\nA Method for Stochastic Optimization. ArXiv,\nabs/1412.6980.\nThomas N Kipf and Max Welling. 2017. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. ICLR.\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S. Zemel, Antonio Torralba, Raquel Urta-\nsun, and Sanja Fidler. 2015. Skip-thought vectors.\nIn NIPS.\nRichard Klavans and Kevin W. Boyack. 2006. Iden-\ntifying a better measure of relatedness for mapping\nscience. Journal of the Association for Information\nScience and Technology, 57:251–263.\nJey Han Lau and Timothy Baldwin. 2016. An\nempirical evaluation of doc2vec with practical in-\nsights into document embedding generation. In\nRep4NLP@ACL.\nQuoc Le and Tomas Mikolov. 2014. Distributed Repre-\nsentations of Sentences and Documents. In ICML.\nJimmy J. Lin. 2008. Is searching full text more effec-\ntive than searching abstracts? BMC Bioinformatics,\n10:46–46.\nCarolyn E Lipscomb. 2000. Medical Subject Headings\n(MeSH). Bulletin of the Medical Library Associa-\ntion.\n2280\nChundi Liu, Shunan Zhao, and Maksims V olkovs.\n2018. Unsupervised Document Embedding with\nCNNs. ArXiv, abs/1711.04168v3.\nPengfei Liu, King Keung Wu, and Helen M. Meng.\n2017. A Model of Extended Paragraph Vector\nfor Document Categorization and Trend Analysis.\nIJCNN.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. ArXiv, abs/1907.11692.\nLaurens van der Maaten. 2014. Accelerating t-SNE\nUsing Tree-based Algorithms. Journal of Machine\nLearning Research.\nAllen Nie, Erin Bennett, and Noah Goodman. 2019.\nDisSent: Learning Sentence Representations from\nExplicit Discourse Relations. In ACL.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research,\n12:2825–2830.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. arXiv.\nRadim ˇReh˚uˇrek and Petr Sojka. 2010. Software Frame-\nwork for Topic Modelling with Large Corpora. In\nLREC.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence Embeddings using Siamese BERT-\nNetworks. In EMNLP.\nAndrew Rosenberg and Julia Hirschberg. 2007. V-\nmeasure: A Conditional Entropy-based External\nCluster Evaluation Measure. In EMNLP.\nJ Ben Schafer, Dan Frankowski, Jon Herlocker, and\nShilad Sen. 2007. Collaborative ﬁltering recom-\nmender systems. In The adaptive web. Springer.\nMartijn J. Schuemie, Marc Weeber, Bob J. A. Schijve-\nnaars, Erik M. van Mulligen, C. Christiaan van der\nEijk, Rob Jelier, Barend Mons, and Jan A. Kors.\n2004. Distribution of information in biomedical ab-\nstracts and full-text publications. Bioinformatics,\n20(16):2597–604.\nDinghan Shen, Xinyuan Zhang, Ricardo Henao, and\nLawrence Carin. 2018. Improved semantic-aware\nnetwork embedding with ﬁne-grained word align-\nment. In EMNLP.\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Dar-\nrin Eide, Bo-June Paul Hsu, and Kuansan Wang.\n2015. An Overview of Microsoft Academic Service\n(MAS) and Applications. In WWW.\nCunchao Tu, Han Liu, Zhiyuan Liu, and Maosong Sun.\n2017. Cane: Context-aware network embedding for\nrelation modeling. In ACL.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In NIPS.\nWenlin Wang, Chenyang Tao, Zhe Gan, Guoyin Wang,\nLiqun Chen, Xinyuan Zhang, Ruiyi Zhang, Qian\nYang, Ricardo Henao, and Lawrence Carin. 2019.\nImproving textual network learning with variational\nhomophilic embeddings. In Advances in Neural In-\nformation Processing Systems, pages 2074–2085.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A Broad-Coverage Challenge Corpus for Sen-\ntence Understanding through Inference. In NAACL-\nHLT.\nFelix Wu, Amauri H. Souza, Tianyi Zhang, Christo-\npher Fifty, Tao Yu, and Kilian Q. Weinberger.\n2019a. Simplifying graph convolutional networks.\nIn ICML.\nLingfei Wu, Ian En-Hsu Yen, Kun Xu, Fangli\nXu, Avinash Balakrishnan, Pin-Yu Chen, Pradeep\nRavikumar, and Michael J Witbrock. 2018. Word\nMover’s Embedding: From Word2Vec to Document\nEmbedding. In EMNLP.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. ArXiv, abs/1609.08144.\nZonghan Wu, Shirui Pan, Fengwen Chen, Guodong\nLong, Chengqi Zhang, and Philip S Yu. 2019b. A\nComprehensive Survey on Graph Neural Networks.\nArXiv, abs/1901.00596.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. ArXiv, abs/1906.08237.\nHamed Zamani, Mostafa Dehghani, W. Bruce Croft,\nErik G. Learned-Miller, and Jaap Kamps. 2018.\nFrom neural re-ranking to neural ranking: Learn-\ning a sparse representation for inverted indexing. In\nCIKM.\nXinyuan Zhang, Yitong Li, Dinghan Shen, and\nLawrence Carin. 2018. Diffusion maps for textual\nnetwork embedding. In NeurIPS.\n2281\nA Appendix A - Baseline Details\n1. Random Zero-mean 25-dimensional vectors\nwere used as representations for each document.\n2. Doc2Vec Doc2Vec is one of the earlier neural\ndocument/paragraph representation methods (Le\nand Mikolov, 2014), and is a natural comparison.\nWe trained Doc2Vec on our training subset using\nGensim ( ˇReh˚uˇrek and Sojka, 2010), and chose the\nhyperparameter grid using suggestions from Lau\nand Baldwin (2016). The hyperparameter grid\nused:\n{’window’: [5, 10, 15],\n’sample’: [0, 10 ** -6, 10 ** -5],\n’epochs’: [50, 100, 200]},\nfor a total of 27 models. The other parameters\nwere set as follows: vector_size=300,\nmin_count=3, alpha=0.025,\nmin_alpha=0.0001, negative=5, dm=0,\ndbow=1, dbow_words=0.\n3. Fasttext-Sum This simple baseline is a\nweighted sum of pretrained word vectors. We\ntrained our own 300 dimensional fasttext embed-\ndings (Bojanowski et al., 2017) on a corpus of\naround 3.1B tokens from scientiﬁc papers which\nis similar in size to the SciBERT corpus (Beltagy\net al., 2019). We found that these pretrained embed-\ndings substantially outperform alternative off-the-\nshelf embeddings. We also use these embeddings in\nother baselines that require pretrained word vectors\n(i.e., SIF and SGC that are described below). The\nsummed bag of words representation has a number\nof weighting options, which are extensively tuned\non a validation set for best performance.\n4. SIF The SIF method of Arora et al. (2017) is\na strong text representation baseline that takes a\nweighted sum of pretrained word vectors (we use\nfasttext embeddings described above), then com-\nputes the ﬁrst principal component of the document\nembedding matrix and subtracts out each document\nembedding’s projection to the ﬁrst principal com-\nponent.\nWe used a held-out validation set to choose a\nfrom the range [1.0e-5, 1.0e-3] spaced evenly\non a log scale. The word probability p(w) was\nestimated on the training set only. When com-\nputing term-frequency values for SIF, we used\nscikit-learn’s TﬁdfVectorizer with the same pa-\nrameters as enumerated in the preceding sec-\ntion. sublinear_tf, binary, use_idf,\nsmooth_idf were all set to False. Since SIF\nis a sum of pretrained fasttext vectors, the resulting\ndimensionality is 300.\n5. ELMo ELMo (Peters et al., 2018) provides con-\ntextualized representations of tokens in a document.\nIt can provide paragraph or document embeddings\nby averaging each token’s representation for all 3\nLSTM layers. We used the 768-dimensional pre-\ntrained ELMo model in AllenNLP (Gardner et al.,\n2018).\n6. Citeomatic The most relevant baseline is Citeo-\nmatic (Bhagavatula et al., 2018), which is an aca-\ndemic paper representation model that is trained on\nthe citation graph via sampled triplets. Citeomatic\nrepresentations are an L2 normalized weighted sum\nof title and abstract embeddings, which are trained\non the citation graph with dynamic negative sam-\npling. Citeomatic embeddings are 75-dimensional.\n7. SGC Since our algorithm is trained on data from\nthe citation graph, we also compare to a state-of-\nthe-art graph representation learning model: SGC\n(Simple Graph Convolution) (Wu et al., 2019a),\nwhich is a graph convolution network. An al-\nternative comparison would have been Graph-\nSAGE (Hamilton et al., 2017b), but SGC (with\nno learning) outperformed an unsupervised variant\nof GraphSAGE on the Reddit dataset16, Note that\nSGC with no learning boils down to graph prop-\nagation on node features (in our case nodes are\nacademic documents). Following Hamilton et al.\n(2017a), we used SIF features as node representa-\ntions, and applied SGC with a range of parameter\nk, which is the number of times the normalized\nadjacency is multiplied by the SIF feature matrix.\nOur range of k was 1 through 8 (inclusive), and was\nchosen with a validation set. For the node features,\nwe chose the SIF model with a = 0.0001, as this\nmodel was observed to be a high-performing one.\nThis baseline is also 300 dimensional.\n8. SciBERT To isolate the advantage of\nSPECTER ’s citation-based ﬁne-tuning objective,\nwe add a controlled comparison with SciBERT\n(Beltagy et al., 2019). Following Devlin et al.\n(2019) we take the last layer hidden state corre-\nsponding to the [CLS] token as the aggregate\ndocument representation.17\n16There were no other direct comparisons in Wu et al.\n(2019a)\n17We also tried the alternative of averaging all token repre-\nsentations, but this resulted in a slight performance decrease\ncompared with the [CLS] pooled token.\n2282\n9. Sentence BERT Sentence BERT (Reimers and\nGurevych, 2019) is a general-domain pretrained\nmodel aimed at embedding sentences. The au-\nthors ﬁne-tuned BERT using a triplet loss, where\npositive sentences were from the same document\nsection as the seed sentence, and distractor sen-\ntences came from other document sections. The\nmodel is designed to encode sentences as opposed\nto paragraphs, so we embed the title and each sen-\ntence in the abstract separately, sum the embed-\ndings, and L2 normalize the result to produce a\nﬁnal 768-dimensional paper embedding.18\nDuring hyperparameter optimization we chose\nhow to compute TF and IDF values weights by\ntaking the following non-redundant combinations\nof scikit-learn’s TﬁdfVectorizer (Pedregosa et al.,\n2011) parameters: sublinear_tf, binary,\nuse_idf, smooth_idf. There were a total\nof 9 parameter combinations. The IDF values\nwere estimated on the training set. The other\nparameters were set as follows: min_df=3,\nmax_df=0.75, strip_accents=’ascii’,\nstop_words=’english’, norm=None,\nlowercase=True. For training of fasttext, we\nused all default parameters with the exception of\nsetting dimension to 300 and minCount was set\nto 25 due to the large corpus.\n18We used the ‘bert-base-wikipedia-sections-mean-tokens’\nmodel released by the authors: https://github.com/\nUKPLab/sentence-transformers"
}