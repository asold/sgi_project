{
  "title": "A Swin transformer and MLP based method for identifying cherry ripeness and decay",
  "url": "https://openalex.org/W4387818231",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2101063952",
      "name": "Ke Song",
      "affiliations": [
        "Xi'an Aeronautical University"
      ]
    },
    {
      "id": "https://openalex.org/A2104807565",
      "name": "Jiwen Yang",
      "affiliations": [
        "Xi'an Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2126258984",
      "name": "Guohui Wang",
      "affiliations": [
        "Xi'an Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2101063952",
      "name": "Ke Song",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2104807565",
      "name": "Jiwen Yang",
      "affiliations": [
        "Xi'an Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2126258984",
      "name": "Guohui Wang",
      "affiliations": [
        "Xi'an Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2903632210",
    "https://openalex.org/W3112325938",
    "https://openalex.org/W2800276543",
    "https://openalex.org/W4387031820",
    "https://openalex.org/W3033469067",
    "https://openalex.org/W4287982304",
    "https://openalex.org/W4205771884",
    "https://openalex.org/W2944777838",
    "https://openalex.org/W4385552942",
    "https://openalex.org/W4376139875",
    "https://openalex.org/W4211073746",
    "https://openalex.org/W2115334369",
    "https://openalex.org/W6755210627",
    "https://openalex.org/W3010655531",
    "https://openalex.org/W3195133177",
    "https://openalex.org/W3022746210",
    "https://openalex.org/W3201606813",
    "https://openalex.org/W2602969294",
    "https://openalex.org/W6774461005",
    "https://openalex.org/W2171943915",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6793119350",
    "https://openalex.org/W4223970271",
    "https://openalex.org/W6792155083",
    "https://openalex.org/W4287092266",
    "https://openalex.org/W2945416315",
    "https://openalex.org/W6795435739",
    "https://openalex.org/W6782027258",
    "https://openalex.org/W3157905703",
    "https://openalex.org/W3113224072",
    "https://openalex.org/W6766263406",
    "https://openalex.org/W4212834205",
    "https://openalex.org/W2155632266",
    "https://openalex.org/W6634249448",
    "https://openalex.org/W2062346783",
    "https://openalex.org/W1980943088",
    "https://openalex.org/W6749613789",
    "https://openalex.org/W1964940342",
    "https://openalex.org/W1908991523",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W6680298704",
    "https://openalex.org/W6674914833",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W6749781174",
    "https://openalex.org/W4214612132",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3108541407",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2794383999",
    "https://openalex.org/W2963651088",
    "https://openalex.org/W4321232185",
    "https://openalex.org/W1573139986",
    "https://openalex.org/W3118308103",
    "https://openalex.org/W2136132422"
  ],
  "abstract": "Cherries are a nutritionally beneficial and economically significant crop, with fruit ripeness and decay (rot or rupture) being critical indicators in the cherry sorting process. Therefore, accurately identifying the maturity and decay of cherries is crucial in cherry processing. With advancements in artificial intelligence technology, many studies have utilized photographs for non-destructive detection of fruit appearance quality. This paper proposes a cherry appearance quality identification method based on the Swin Transformer, which utilizes the Swin Transformer to extract cherry image feature information and then imports the feature information into classifiers such as multi-layer perceptron(MLP) and support vector machine(SVM) for classification. Through the comparison of multiple classifiers, the optimal classifier, namely, MLP, in combination with the Swin Transformer is obtained. Furthermore, performance comparisons are conducted with the original Swin-T method, traditional CNN models, and traditional CNN models combined with MLP. The results demonstrate the following: 1) The proposed method based on the Swin Transformer and MLP achieves an accuracy rate of 98.5%, which is 2.1% higher than the original Swin-T model and 1.0% higher than the best-performing combination of traditional CNN model and MLP. 2) The training time required for the Swin Transformer and MLP is only 78.43 s, significantly faster than other models. The experimental results indicate that the innovative approach of combining the Swin Transformer and MLP shows excellent performance in identifying cherry ripeness and decay. The successful application of this method provides a new solution for determining cherry appearance ripeness and decay. Therefore, this method plays a significant role in promoting the development of cherry sorting machines.",
  "full_text": "A Swin transformer and MLP based\nmethod for identifying cherry\nripeness and decay\nKe Song1, Jiwen Yang2 and Guohui Wang2*\n1School of Electronic Engineering, Xi’an Aeronautical Institute, Xi’an, China,2School of Optoelectronic\nEngineering, Xi’an Technological University, Xi’an, China\nCherries are a nutritionally beneﬁcial and economically signiﬁcant crop, with fruit\nripeness and decay (rot or rupture) being critical indicators in the cherry sorting\nprocess. Therefore, accurately identifying the maturity and decay of cherries is\ncrucial in cherry processing. With advancements in arti ﬁcial intelligence\ntechnology, many studies have utilized photographs for non-destructive\ndetection of fruit appearance quality. This paper proposes a cherry appearance\nquality identiﬁcation method based on the Swin Transformer, which utilizes the\nSwin Transformer to extract cherry image feature information and then imports\nthe feature information into classiﬁers such as multi-layer perceptron(MLP) and\nsupport vector machine(SVM) for classi ﬁcation. Through the comparison of\nmultiple classiﬁers, the optimal classiﬁer, namely, MLP, in combination with the\nSwin Transformer is obtained. Furthermore, performance comparisons are\nconducted with the original Swin-T method, traditional CNN models, and\ntraditional CNN models combined with MLP. The results demonstrate the\nfollowing: 1) The proposed method based on the Swin Transformer and MLP\nachieves an accuracy rate of 98.5%, which is 2.1% higher than the original Swin-T\nmodel and 1.0% higher than the best-performing combination of traditional CNN\nmodel and MLP. 2) The training time required for the Swin Transformer and MLP is\nonly 78.43 s, signiﬁcantly faster than other models. The experimental results\nindicate that the innovative approach of combining the Swin Transformer and\nMLP shows excellent performance in identifying cherry ripeness and decay. The\nsuccessful application of this method provides a new solution for determining\ncherry appearance ripeness and decay. Therefore, this method plays a signiﬁcant\nrole in promoting the development of cherry sorting machines.\nKEYWORDS\ncherry ripeness and decay, CNN, deep features, Swin transformer, muti-layer perceptron\n1 Introduction\nCherry is a highly productive fruit that is widely grown in world wide. Compared to other\nfruits, cherry is high in microelement of iron which can enhance the hematopoietic function\nof the human body and alleviate anemia symptoms. Tieton cherry is a late-ripening cherry\nvarieties, which is not only rich in vitamin A and vitamin C to maintain healthy organ\nfunctioning, but also contain antioxidants to strengthen the immune system, reduce\ninﬂammation. Besides the Tieton cherry also provides calcium to protect bones and\nteeth. Tieton cherry is a seasonal fruit which can usually be made into fresh fruit or\ndried fruit [1]. The ripeness and decay of cherries is one of the important evaluation indexes\nof fruit quality. Agricultural wastage is partly due to the poor marketability of the related\nagricultural products [2]. Cherries with bright colors and regular shapes can attract\nOPEN ACCESS\nEDITED BY\nWenjun Liu,\nBeijing University of Posts and\nTelecommunications (BUPT), China\nREVIEWED BY\nHongyi Bai,\nHeilongjiang University, China\nFuhong Min,\nNanjing Normal University, China\n*CORRESPONDENCE\nGuohui Wang,\nbooler@126.com\nRECEIVED 17 August 2023\nACCEPTED 09 October 2023\nPUBLISHED 20 October 2023\nCITATION\nSong K, Yang J and Wang G (2023), A Swin\ntransformer and MLP based method for\nidentifying cherry ripeness and decay.\nFront. Phys. 11:1278898.\ndoi: 10.3389/fphy.2023.1278898\nCOPYRIGHT\n© 2023 Song, Yang and Wang. This is an\nopen-access article distributed under the\nterms of theCreative Commons\nAttribution License (CC BY). The use,\ndistribution or reproduction in other\nforums is permitted, provided the original\nauthor(s) and the copyright owner(s) are\ncredited and that the original publication\nin this journal is cited, in accordance with\naccepted academic practice. No use,\ndistribution or reproduction is permitted\nwhich does not comply with these terms.\nFrontiers inPhysics frontiersin.org01\nTYPE Original Research\nPUBLISHED 20 October 2023\nDOI 10.3389/fphy.2023.1278898\ncustomers in domestic and foreign markets. Therefore, grading and\nsorting processes play an important role in providing high quality\nfruits to consumers. With the rapid growth of cherry production, the\ndemand for cherry grading and sorting is increasing. Currently,\nmost cherries are sorted manually by workers, which is tedious and\nhave low sorting efﬁciency [3]. The efﬁciency of cherry sorter\noperation affects the rate of sales of products in the market.\nConsequently, it is necessary to develop a high-performance\ncherry sorting machine to improve the ef ﬁciency of cherry\nappearance ripeness and decay identi ﬁcation, and improve the\nspeed of sorting and processing. This would further allow high\nquality cherries to access fruit markets.\nIn recent years, advancementsin computer performance have\ngreatly enhanced deep learning-based object recognition\ntechniques [4– 7]. Concurrently, this has also introduced novel\nsolutions for crop identi ﬁcation. As a result, nowadays, the\ncontrol and monitoring of fruit appearance quality by\nelectronic ways such as machine vision and deep learning has\nbeen increasingly taking the p lace of manual means in some\ndeveloped countries [ 8– 11]. Compared with the manual\ndetection of cherry appearance quality, the advantages of\nmachine vision and deep learning techniques include high\naccuracy and detection speed, high ﬂexibility and low costs,\nprogram-mability. The most important is that electronic ways\ncan achieve non-destructive identiﬁcation. Elmasry et al. (2012)\nutilized visual machine to ident ify irregularly shaped potatoes\nimages, and the average accuracy of this method was 96.2% [12].\nFemling et al. used machine learning to create a system to achieve\nvegetables and fruits identiﬁcation in the retail market [13]. This\nsystem minimizes the number of human computer interactions\nand speeds up the recognition process. Sambasivam et al. achieve\ncassava disease detection and classi ﬁcation with deep\nconvolutional neural networks, a n dr e p o r t e da na c c u r a c ys c o r e\nof over 93% [14] .B a oe ta l .s e tu pal i g h t w e i g h tC N Nm o d e lt o\nidentify wheat ear diseases. They obtained 94.1% accuracy for the\nmodel meanwhile the parameters are only 2.13 M [15]. Gao et al.\nselected spectral features and utilized CNN to classify the ripe\nand early ripe strawberry, which obtained the accuracy of 98.6%\nfor strawberry dataset [16]. Dong et al. proposed a diseases and\npests automatic recognition system based on improved AlexNet\nmodel which has a good performance [17]. In order to maximize\nthe proﬁt of cucumbers fruit, Kheiralipour and Perma (2017)\nproposed graded system of different cucumber forms using image\nprocessing technique and artiﬁcial neural networks. This method\nhas an accuracy of 97.1% for identifying cucumber forms [18].\nAlthough CNN has achieved satisfactory achievement in the\ntask of fruit identiﬁcation, these methods of based on CNN still\nhave some shortcomings, since these methods have limitations in\nthe modeling of global information [19]. When CNN extracting\ntarget features, only if stack many layers can obtain the global\nfeatures. With the emergence of more efﬁcient structures, visual\ntasks with transformer have become a new research orientation in\norder to reduce structural complexity and improve training\nefﬁ\nciency. Transformer captures s patial patterns and non-local\ndependencies by the attention mechanisms [20], which has been\nsuccessfully used to language recognition [21], image generation\n[22], object detection [23], text image synthesis [24], and video\nunderstanding [ 25]. Some transformer-based architectures\ndemonstrate powerful capabilities for visual task processing,\nsuch as Visual Transformer (ViT) [26]. Zheng et al. uses ViT-\nB/32 extract the class token and imports it into the support vector\nmachine to identify the appearance quality of strawberry,\neventually accuracy achieving 98.1% [ 27]. In addition, Swin\nTransformer is an innovative vi sion model for transformers\nthat uses a hierarchical architecture to obtain the ﬂexibility to\nmodel at a variety of scales [28]. Zheng et al. utilizes the Swin\nTransformer to extract image features and import the features\ninto MLP for identifying strawberry, and the accuracy reaches\n98.45% [29].\nAmong the many physical characteristics used as the\nevaluation criteria of agricultural products in the grading\nprocess of cherries, the level of ripeness and decay are the\nmost important. Therefore, one of the requirements set by the\nmarket standards for cherry is the appearance quality of the\nproduct. Therefore, the aim of this research is to propose a\npractical method based on improved Swin Transformer to\nclassify cherry appearance quality with a very high accuracy.\nFirstly, Swin Transformer is u sed to extract cherry image\nfeatures, and then imported into MLP to realize cherry\nrecognition. Compared with other methods, this method can\nachieve higher recognition accuracy.\n2 Materials and methods\n2.1 Materials\n2.1.1 Dataset and experimental environment\nIn this study, the data set consisted of 4,669 cherry images taken\nby a mobile phone (HUAWEI nova 9) from the cherry orchard of\nBailuyuan in Xi ’an. The cherry dataset utilized in this study\nexclusively comprises a single cultivar, namely, Tieton cherry.\nCherries are classi ﬁed into three levels of ripeness: immature,\nmature, and over-mature. Adding broken categories, the cherry\ndataset is divided into four categories. Examples of cherry images\nwith different ripeness and decay are shown in Figure 1. The\nappearance of each class has a very distinct character, such as the\nskin of broken cherries is wrinkled, rotten or cracking (Figure 1A);\nimmature cherries have laurel-green or red-orange skin (Figure 1B);\nthe surface of mature cherry is positive bright red (Figure 1C); it can\nbe seen that if cherries are over mature, they would be dark purple,\neven black hues (Figure 1D); The size of the original image collected\nis large in this experiment, which reduces the accuracy and improves\nthe training time during image analysis and processing [30]. Thus, in\norder to achieve higher recognition accuracy and less training time,\nthe images were resized to 224 × 224. The 4,669 images are divided\ninto training set and test set in the ratio of 8:2. Training and test\nsamples are independent of each other in order to reduce the\ncorrelation between them.\nThe training of all network-models is done in a Personal\nComputer (PC). The experimental hardware environment\nincludes an Intel Core i9-9900X CPU (3.50 GHz) and a NVIDIA\nRTX 2080Ti GPU. The software environment consists of Ubuntu\n18.04 operating system, CUDA10.1 and cuDNN 8.04 for deep\nlearning, Pytorch 1.7 as the neural network framework, and other\nimportant packages such as numpy 1.21.5 and scikit-learn 1.0.2.\nFrontiers inPhysics frontiersin.org02\nSong et al. 10.3389/fphy.2023.1278898\n2.1.2 Images enhancement\nWhen deep learning training network, enough data is needed to\ncomplete the training process to satisfy the training requirements of\nconvolutional neural network. And appropriate expansion of data\nset can improve the accuracy of recognition. Since the number of\nimages broken category is not enough, data enhancement was used\nto increase the size of the data set. In the experiment, seven\nenhancement methods are applied to the images of broken\ncategory: 1) Randomly adjust the brightness of the image; 2)\nRandomly change the image contrast to obtain a new image; 3)\nRotate at random angles; 4) Flip the cherry image vertically or\nhorizontally; 5) Apply an af ﬁne transformation to obtain an\nenhanced image; 6) The dislocation transformation based on the\nhorizontal or vertical direction to realize image geometric\ndeformation; 7) Achieve HSV image enhancement by selecting a\nHue value, a saturation value, and a lightness value. Eventually\n1,302 images of broken category are produced, andFigure 2shows\nthe results of data enhancement.\nThe ﬁnal data distribution is shown inTable 1. The four types of\ndatasets are evenly distributed, which avoids the overﬁtting of single\nFIGURE 1\nExamples of the cherry images.(A) Broken; (B) Immature; (C) Mature; (D) Over-mature.\nFIGURE 2\nOriginal and seven different types of enhanced images.\nTABLE 1 Image amount and resolution.\nItem Training\ndataset\nTest\ndataset\nTotal Resolution\nImmature 958 240 1,198 224 × 224\nMature 838 209 1,047 224 × 224\nOver-\nmature\n930 232 1,162 224 × 224\nBroken 1,042 260 1,302 224 × 224\nFrontiers inPhysics frontiersin.org03\nSong et al. 10.3389/fphy.2023.1278898\nsample data by the network and improves the generalization ability\nof the model.\n2.2 Swin transformer\nSwin Transformer is a deep learning model based on\nTransformer. Unlike the previous Vision Transformer (ViT),\nSwin Transformer is efﬁcient and accurate, and can be used as\nthe backbone of a universal computer vision. As shown inFigure 3A,\nin existing ViT, the feature image size isﬁxed and without being\nsegmented, causing the computational complexity is quadratic to\nimage size. In contrast, Swin Transformer constructs hierarchical\nfeature maps, and the hierarchical feature representation was\nconstructed by small image element and layer by layer\nneighborhood merging as illustrated in Figure 3B. Starting with\nsmall-sized gray patches and gradually merging with adjacent\npatches in deeper layers. The number of patches in red windows\nis ﬁxed, and so the complexity is linear to image size. However, this\napproach will reduce connection between each window. To solve\nthis problem, Swin Transformer adopts that shift of window\npartition, as shown in Figure 3C. The shifted windows connect\nthe windows of the previous layer, providing connections\nbetween them.\nThe basic architecture of the Swin Transformer is shown in\nFigure 4. First, the input RGB image is divided into non-overlapping\npatches through patch splitting module, and each patch is treated as\na “token.” The patch splitting module made up Patch Partition and\nLinear Embedding. Then the feature maps of different scales are\nconstructed through four stages, and each stage includes Swin\nTransformer blocks. Except for the ﬁrst stage, Patch Merging\noperations are required for each stage before Swin Transformer\nBlock. The main purpose is to downsample and generate features of\ndifferent scales.\nFIGURE 3\n(A) The feature maps of ViT;(B) Swin Transformer builds hierarchical feature maps;(C) the shifted window approach for computing self-attention.\nFIGURE 4\n(A) a Swin Transformer architecture(B) Swin Transformer blocks.\nFrontiers inPhysics frontiersin.org04\nSong et al. 10.3389/fphy.2023.1278898\n2.2.1 Swin transformer block\nSwin Transformer is built on the basis of the Transformer\nblock, by replacing the standa rd multi-head self attention\n(MSA) module with a module based on shifted windows\n(W-MSA and SW-MSA) while the other layers remain\nunchanged [ 31]. As observed in Figure 4B ,e a c hS w i n\nTransformer block consists of a window-based multi-head\nself attention (W-MSA) module or a shifted window-based\nmulti-head self attention (SW-MSA) module, followed by a\n2-layer MLP with Gaussian Error Linear Unit (GELU)\nnonlinearity in between. A LN (LayerNorm) layer is added\nbefore each MSA module and each MLP module, and a\nresidual connection is added after each MSA module and\neach MLP. The calculation of feature map in successive Swin\nT r a n s f o r m e rb l o c k si ss h o w nb e l o w :\n^z\nl /equals W − MSA LN zl−1()() + zl−1 (1)\nzl /equals MLP LN ^zl()() + ^zl (2)\n^zl+1 /equals SW − MSA LN zl()() + zl (3)\nzl+1 /equals MLP LN ^zl+1()() + ^zl+1 (4)\nwhere zl denote the output of the MLP module of thel th block,^zl+1\ndenote the output of the (S)W-MSA model.\n2.2.2 W-MSA and SW-MSA\nWhen the conventional transformer block adopts MSA\nmodule, it performs global self-attention computation. As a\nresult, a quadratic increase in the computation of the module\nwith respect to the number of patch tokens. The computational\ncomplexity of the MSA is illustrated in Eq.5.W h e r ea n da r et h e\nheight and width of the input image. For the W-MSA module,\nthe pictures are divided into the windows in an evenly manner.\nThe disadvantage is that the self-attention calculation only be\ncarried out in each window, and information cannot be\ntransferred between Windows. Assuming that each window is\nM in width and height, and then use the MSA module within h\nWindows. The computational complexity of the W-MSA is\nillustrated in Eq. 6. SW-MSA solves the problem of\ninformation communication between different Windows.\nSwin performs self-attention calculations in each window as\nshown in Figure 3B. This method improves the ability of model\ncharacterization.\nΩMSA /equals 4hwc\n2 + 2 hw() 2C (5)\nΩW − MSA /equals 4hwC2 + 2M2hwC (6)\n2.3 Muti-layer perceptron\nMLP is a dynamic classiﬁer based on neural networks. The MLP\nclassiﬁer uses neural networks to deduce a hyperplane that\ndistinguishes between different categories of cherries. The\nhyperplane is then used to perform the classi ﬁcation. In this\nstudy, cherry appearance ripeness and decay were classiﬁed into\nfour categories, and the hyperplane that is farthest from the feature\nvector was chosen as the classiﬁcation plane to classify each feature\nvector into one of the four categories. The MLP mainly consists of an\ninput layer, a hidden layer, and an output layer, with each layer being\nfully connected to the adjacent layers. Its structure is shown in the\nMulti-layer Perceptron module inFigure 5.\nMLP has high recognition accuracy and faster classiﬁcation\nspeed. The hyperparameters of the classiﬁer are the adjustment\nknobs that control the model structure and ef ﬁciency. In this\nexperiment, the optimal parameter details that achieved the best\nrecognition performance are shown inTable 2.\nTo measure the performance of the network during training, a\nloss function is used. Typically, the mean squared error function is\nemployed, as shown in Eq.7.\nL ^y, y() /equals\n1\n2 ^y − y()\n2\n(7)\nHowever, this function is usually non-convex, which can lead to\nﬁnding a local optimal solution rather than a global optimal\nsolution. Therefore, the following function is selected as the loss\nfunction:\nL ^y, y() /equals− y log^y + 1 − y() log 1 − ^y()() (8)\nwhere y is the true value of the sample, and^y is the predicted value.\nThe goal of the training is to minimize the loss function.\nThe average value of the loss function for the entire training\ndataset is the cost function of the training set, as shown in Eq.9:\nJw , b() /equals\n1\nm ∑\nm\ni/equals 1\nL ^y i() ,y i()()\n/equals− 1\nm∑\nm\ni/equals 1\ny i() log ^y i() + 1 − y i()() log 1 − ^y i()()[] (9)\nIt is evident that the cost function is a function ofw and b.\nTherefore, the objective of the training is to iteratively compute the\noptimal values ofw and b, which minimize the cost function and\nachieve the best training results.\n2.4 Proposed method\nIt is well-known that a CNN model can be used as a feature\nextractor by removing the fully connected layers and using the\nremaining layers for feature extraction [32, 33]. Similarly, in this\npaper, the same approach is applied to the Swin Transformer\nmodel. The pre-trained parame ters on ImageNet are used for\nextracting cherry image features, which enhances the model ’s\nreceptive ﬁeld. Liu et al. proposed four models of Swin\nTransformer: Swin-B, Swi n-T, Swin-S, and Swin-L [28]. Swin-\nTh a sas m a l lm o d e ls i z e ,l o wﬂoating-point operations per\nsecond (FLOPs), and high throughput, with values of 29M,\n4.5G, and 755.2 image/s, respectively. Therefore, Swin-T is\nchosen as the feature extractor to avoid high computational\ncomplexity. The overall architecture is shown in Figure 5 ,\nwhere the input cherry image size is 224 × 224. The output\nfeatures from the four stages of the Swin Transformer have a\nresolution of 77 and a channel dimension of 768D. Then, the\nfeatures are ﬂattened into a one-dime nsional feature vector,\nwhich is inputted into an MLP for predicting the ﬁnal cherry\nlabel.\nFrontiers inPhysics frontiersin.org05\nSong et al. 10.3389/fphy.2023.1278898\n2.5 Workﬂow diagram\nStep-1: Swin-T was used as a classiﬁer to extract features of cherry\nimage.\nStep-2: The extracted one-dimensional features are imported\ninto ten classiﬁe r ss u c ha sM L Pa n dS V Mf o rc o m p a r i s o n ,a n d\nthen the best combination Swin transformer and MLP is\nobtained.\nStep-3: Compare the performance of the combination of Swin-\nTransformer and MLP with the combination of CNN and MLP.\n3 Results\n3.1 Evaluation criteria\nIn this paper, the model’s performance is evaluated using six\nmetrics, including accuracy, training time, precision, recall, FPR,\nand F1-score. The speciﬁc formula is shown inTable 3, where TP\nrepresents true positive, TN is true negative, FP is false positive, and\nFN is false negative. Taking binary classiﬁcation task as an example,\nthe structure of the confusion matrix is shown in Table 4.I n\nFIGURE 5\nThe structure of SwinT combined MLP model.\nTABLE 2 Parameters of MLP.\nParameter name Parameter after adjustment\nLearning rate 0.001\nHidden_layer_sizes 105\nActivation ReLU\nSolver Adam\nAlpha 0.0001\nMax_iter 400\nTABLE 3 Deﬁnition of evaluation indicators.\nCriterion Deﬁnition Criterion De ﬁnition\nAccuracy TP+TN\nTP+TN+FP+FN Precision TP\nTP+FP\nRecall TP\nTP+FN F1-score 2× TP\n2× TP+FP+FN\nFPR FP\nTN+FP\nFrontiers inPhysics frontiersin.org06\nSong et al. 10.3389/fphy.2023.1278898\naddition, confusion matrix and ROC curve play a crucial role in\nfurther verifying experimental results.\n3.2 The deep feature visualization of\nnetwork model\nThe purpose of visualizing the deep features of a network model\nis to help us understand how the neural network discriminates\nbetween different object categories, and to gain some insight into\nwhat the neural network relies on to recognize objects [34]. In this\npaper, the Gradient-weighted Class Activation Mapping (Grad-\nCAM) method is used to observe the image features extracted by\nSwin Transformer, and to understand which local regions of the\noriginal image led the model to make itsﬁnal classiﬁcation decision\n[35]. Using the gradient of any target concept, a rough localization\nmap is generated to highlight the important regions of the image\nused for prediction. Figure 6 shows examples of Grad-CAM\ngenerated from four cherry categories. The brightness of the\ngenerated image varies with changes in the visual features of the\nimage [36]. From theﬁgure, we can see that the network is able to\nrecognize the ripeness and decay of the cherries based on their fruit\nfeatures.\n3.3 The performance of Swin transformer\nand different classiﬁers\nSwin Transformer is used as a feature extractor to extract the\ndeep features of cherries, and then the features are imported into the\nclassiﬁer to predict the category labels of cherries. Different\nclassiﬁers have different learning abilities for pre-trained features.\nIn this work, nine classiﬁers such as random forest (RF) [37],\ndecision tree (DT) [38], extremely randomized trees (ET) [39],\nquadratic discriminant analysis (QDA) [40], gaussian naive bayes\n(GNB) [41], SVM [42], linear discriminant analysis (LDA) [43],\nextreme gradient boosting (XGB) [44], and K nearest neighbor\n(KNN) [45] are compared with the MLP classiﬁer proposed in\nthis paper.\n3.3.1 Analyze the accuracy and training time of\ndifferent combinations\nFigure 7shows the comparison of accuracy and training time for\nten classiﬁers combined with Swin Transformer. FromFigure 7A,i t\nis evident that the accuracy of QDA and GNB classiﬁers is much\nlower than the original Swin-T training results, indicating poor\nclassiﬁcation performance. In contrast, SVM, XGB, and MLP\nclassiﬁers have higher accuracy than Swin-T, with improvements\nof 0.2%, 0.3%, and 2.1%, respectively. Through experiments, the\ntraining time of Swin-T was found to be 551.24s, andFigure 7B\nshows that the average training time of the method combining deep\nfeatures with speciﬁc classiﬁers is much shorter than that of Swin-T.\nThe training times for SVM, XGB, and MLP are 78.021, 83.853, and\n78.774 s, respectively. XGB’s longer training time is due to its need to\ntraverse the dataset during node splitting. Through a comprehensive\nanalysis of accuracy and training time, MLP performs the best.\n3.3.2 Analyze the assessment indicators of different\ncombinations\nThe ten classiﬁers were compared based on precision, recall,\nF1-score, and FPR. As shown inTable 5, it can be observed that\nthe QDA and GNB classiﬁers’ results were not satisfactory, as\nTABLE 4 The confusion matrix formed by the parameters of the evaluation index.\nConfusion matrix Predictive\nPositive Negative Totol\nPractical Positive True positive (TP) False negative (FN) Actual positive (TP + FN)\nNegative False positive (FP) True negative (TN) Actual negative (FP + TN)\nTotal Predicted positive (TP + FP) Predicted negative (FN + TN) TP + FP + FN + TN\nFIGURE 6\nGrad-CAM visualization of Swin Transformer.(A) Broken; (B) Immature; (C) Mature; (D) Over-mature.\nFrontiers inPhysics frontiersin.org07\nSong et al. 10.3389/fphy.2023.1278898\ntheir precision and recall values were much lower than the other\nclassi ﬁers, and GNB had the highest FPR value. A high FPR value\nindicates a high false positive rate of the model. SVM and XGB\nclassi ﬁers had very similar precision, recall, and F1-score values,\nwith a difference of only 0.002, 0.001, and 0.002, respectively,\nwhich achieved ideal performance. Compared to SVM and XGB,\nthe MLP classi ﬁer had higher precision and recall values,\nreaching 98.4% and 98.5%, respectively, and an F1-score\nexceeding 0.99. In addition, the MLP classi ﬁer had the lowest\nFPR value among all classiﬁers, which was 0.005.\n3.4 The best performance of Swin\ntransformer and MLP\nThe results demonstrate that the method proposed in this\npaper, which combines Swin-T with MLP, achieves the best\nrecognition performance, with an accuracy of 98.5%, precision\nof 98.4%, recall of 98.5%, F1-score of 0.995, and FPR of 0.005.\nThe high recognition accuracy of the method proposed in this\npaper is attributed to the strong adaptability and self-learning\ncapabilities of MLP. In addition, the ROC curve of Swin\ntransformer and MLP is shown in Figure 8A , where the area\nunder the curve (AUC) is used as a metric for evaluating the\nrecognition performance. The larger the AUC value, the better\nthe recognition performance. As shown in Figure 8A,t h eA U C\nvalues of the four categories and the micro-average are almost\nequal to 1. To further evaluate the performance of this method,\nthe confusion matrix is shown in Figure 8B, which intuitively\ndemonstrates the performance of the model for each category.\nThe performance for each category is represented by the\npredicted labels on the horizontal axis and the true labels on\nthe vertical axis. The results in dicate that the precision for\nbroken, immature, mature, and over-mature are 97.5%,\n98.2%, 99.5%, and 98.5%, respectively. The category with the\nmost misclassi ﬁcation is broken, which was mistakenly\nidenti ﬁed as over-mature. This is possibly because some\ncherries only exhibit slight rot ting, which is characterized by\na darkening of the surface color,and is therefore easily mistaken\nas over-mature. Figure 8B clearly shows that the recognition\nperformance for mature cherries is the best, which is attributed\nto the distinct bright red color of mature cherries. Therefore, the\nmethod proposed in this paper exhibits excellent recognition\nperformance.\n3.5 Proposed method versus other models\nThe proposed method in this paper utilizes Swin-T to extract\ndifferent category features of cherry images and inputs these\nfeatures into a multilayer perceptron (MLP) to predict theﬁnal\nlabels. At the same time, six CNN models including GoogleNet\n[46], VGG13, VGG16, VGG19 [ 47], ResNet101 [ 48], and\nMobileNet_v2 [ 49] are used to recognize the appearance\nquality of cherries, and the features extracted from these CNN\nFIGURE 7\n(A) The accuracy of different combinations(B) The training time of different combinations.\nTABLE 5 The performance of ten classiﬁers.\nClassiﬁers Precision (%) Recall (%) F1 -score FPR\nRF 93.1 93.3 0.931 0.023\nDT 85.4 86.3 0.857 0.049\nFT 95.5 95.7 0.956 0.015\nQDA 63.8 54.9 0.563 0.043\nGNB 77.5 82.9 0.775 0.071\nSVM 96.5 96.6 0.965 0.011\nLDA 96.0 96.1 0.960 0.013\nXGB 96.7 96.7 0.967 0.011\nKNN 92.5 93.0 0.925 0.024\nMLP 98.4 98.5 0.995 0.005\nFrontiers inPhysics frontiersin.org08\nSong et al. 10.3389/fphy.2023.1278898\nmodels are imported into the MLP classiﬁer. The CNN plus MLP\nmethod, traditional CNN method, and the proposed Swin\ntransformer and MLP method are compared in terms of\naccuracy and training time, as shown in Table 6.T h er e s u l t s\nshow that the features plus MLP classiﬁer method has a higher\naverage recognition accuracy than the traditional CNN method,\nespecially the Swin transformer and MLP method achieves the\nbest accuracy of 98.5%. For the proposed method, the training\ntime should be the time for the model to extract features plus the\ntraining time for the classi ﬁer. Table 6 clearly shows that the\ntraining time of the features plus MLP method is much less than\nthat of the traditional CNN models, and the training time of the\nSwin transformer and MLP method is only 78.43 s.\nTo further explain the objective evaluation between accuracy\nand training time, a scatter plot is presented inFigure 9,w h e r e\nthe x-axis and y-axis represent training time and accuracy,\nrespectively, and the x-a x i sv a l u e sr e p r e s e n tt h e\ncomputational resources of the models. We can see from the\nexperiments that the VGG13-MLP, VGG16-MLP, and VGG19-\nMLP methods have low accuracy and long training time, and the\nResNet101-MLP method has a higher accuracy but a long\ntraining time. Therefore, the results demonstrate that the\nmethod proposed in this paper has high accuracy and less\ntraining time, and it has good application value in the\nrecognition of cherry ripeness and decay.\n4 Discussion\n4.1 The advantage of deep features plus MLP\nThe article proposes two advantages of the method: 1) high\naccuracy in recognizing cherr y appearance, 2) short training\ntime. Swin-T with MLP has an accuracy 2.1% higher than\noriginal Swin-T, indicating strong robustness. In addition, the\ntraining time for Swin-T is 551.24 s, while the method of deep\nfeature extraction with MLP has a training time of only 78.43 s.\nThe training time for Swin-T is approximately seven times that of\nFIGURE 8\n(A) Confusion matrix (B) ROC curve.\nTABLE 6 The contrast of traditional models and features plus MLP.\nModel Accuracy (%) Training\ntime\nInference\ntime (s)\nModel + MLP Accuracy (%) Training\ntime\nInference\ntime (s)\nGoogleNet 94.9 516.13 s 0.5485 GoogleNet + MLP 95.9 68.34 s 0.0726\nVGG13 94.4 4236.92 s 4.5026 VGG13 + MLP 95.4 195.63 s 0.2079\nVGG16 93.2 5079.01 s 5.3975 VGG16 + MLP 95.6 227.19 s 0.2414\nVGG19 92.9 6330.21 s 6.7271 VGG19 + MLP 94.5 261.42 s 0.2778\nResNet101 97.6 2025.52 s 2.1525 ResNet101 + MLP 97.5 218.88 s 0.2326\nMobileNet-v2 85.7 250.44 s 0.2661 MobileNet-v2 + MLP 96.8 76.50 s 0.0813\nSwin-T 96.4 551.24 0.5858 Swin-T + MLP 98.5 78.43 s 0.0833\nFrontiers inPhysics frontiersin.org09\nSong et al. 10.3389/fphy.2023.1278898\nSwin transformer and MLP, and the training time for deep\nfeature extraction with MLP is shorter because the network\nonly needs to extract features from cherry images and does\nnot need to continuously optimize the internal parameters of\nthe model. Therefore, the computational complexity of the\nproposed method is much lower than that of Swin-T, and the\nrequired computing power is not large.\n4.2 Potential impact and future work\nIn recent years, the area of cherry cultivation has been\nexpanding year by year, and the yield has been steadily\nincreasing. However, cherry so rting has always been one of\nthe most troublesome problems for growers. Currently,\nmanual sorting during the cherry ripening season is still\ncommon, but this method is expensive, inef ﬁcient, and\ndifﬁcult to ensure the quality of the fruit, which leads to\nsigni ﬁcant quality problems in marketing. Therefore, the\ndevelopment of automatic sorting equipment is particularly\nimportant. According to the di fferent ripening stages of\ncherries, the appearance color of cherries is divided into three\nlevels. In general, to ensure that cherries have a high hardness\nand crisp texture even after several days of packaging and\ntransportation, they should be harvested and sorted before\nthey turn deep red, which is the ripening stage represented in\nthis paper. This ensures that consumers can purchase high-\nquality products. In the cherry sorting process, it is not enough\nto classify the appearance ripeness and decay into four\ncategories. In order to better sort cherries of different\nqualities, it is necessary to further study and add categories\nsuch as semi-ripe and diseases. At the same time, it is very\nimportant to accurately ident ify rotten or damaged cherries\nduring the sorting process. The Swin-T with MLP method\nproposed in this paper has high classi ﬁcation accuracy in the\nidenti ﬁcation of cherry ripeness and decay. This experiment is\nalso applicable to other cherry varieties such as Lapins, Kordia,\nSkeena, etc.\n5 Conclusion\nThis paper proposes a cherry appearance ripeness and decay\nrecognition method based on deep feature extraction combined\nwith an MLP classi ﬁer. The method performs well in cherry\ndetection. In the experimental stage, the features extracted from\nSwin-T were imported into ten classiﬁers for comparison, and\nthe best performing classi ﬁer was the MLP classi ﬁer. In\naddition, the method proposed in this paper, which extracts\nimage features from Swin-T and imports them into MLP, was\ncompared with the method that extracts features from\ntraditional CNN and imports them into MLP. The\nrecognition accuracy of Swin transformer and MLP was as\nhigh as 98.5%, and the training time was only 78.43 s, which\nis an impressive result. Therefore, the proposed method has\nimportant practical value. In addition, this method has\nreference signi ﬁc a n c ef o rt h ei d e n t iﬁcation of other types of\ncherries. If one wishes to identify other cherry varieties, it\nsufﬁces to substitute the dataset. Furthermore, it is\nimperative to emphasize that the improved method presented\nin this article remains applicable in such a scenario, owing to its\nversatility and robustness, enabling it to accommodate the\ndistinct characteristics of various cherry varieties. Thus, it\nfurnishes a ﬂexible and viable solution to the problem of\ncherry cultivar recognition. The focus of future research is to\napply this method to sorting equipment and other mechanical\ndevices to promote the development of future intelligent sorting\nmethods.\nFIGURE 9\nThe accuracy and training time of models.\nFrontiers inPhysics frontiersin.org10\nSong et al. 10.3389/fphy.2023.1278898\nData availability statement\nThe original contributions presented in the study are included in\nthe article/Supplementary Material, further inquiries can be directed\nto the corresponding author.\nAuthor contributions\nKS: Conceptualization, Formal Analysis, Investigation, Methodology,\nResources, Supervision, Validation, Visualization, Writing– original draft,\nWriting– review and editing. JY: Data curation, Software, Validation,\nWriting– original draft. GW: Data cur ation, Funding acquisition,\nInvestigation, Software, Writing– original draft, Writing– review and\nediting.\nFunding\nThe author(s) declare ﬁnancial support was received for the\nresearch, authorship, and/or publication of this article. The work is\npartly supported by the Scientiﬁc Research Program Funded by\nEducation Department of Shaanxi Provincial Government\n(Program No. 22JY025).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial orﬁnancial relationships that could be\nconstrued as a potential conﬂict of interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the authors\nand do not necessarily represent those of their af ﬁliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed or\nendorsed by the publisher.\nReferences\n1. Acero N, Gradillas A, Beltran M, García A, Mingarro DM. Comparison of phenolic\ncompounds proﬁle and antioxidant properties of different sweet cherry (Prunus avium\nL.) varieties. Food Chem (2019) 279:260– 71. doi:10.1016/j.foodchem.2018.12.008\n2. Jahanbakhshi A, Kheiralipour K. Carrot sorting based on shape using image\nprocessing, artiﬁcial neural network and support vector machine.J Agric Machinery\n(2019) 9:295– 307. doi:10.22067/jam.v9i2.70579\n3. Dasari SK, Prasad V. A novel and proposed comprehensive methodology using\ndeep convolutional neural networks forﬂue cured tobacco leaves classiﬁcation. Int J Inf\nTech (2019) 11:107– 17. doi:10.1007/s41870-018-0174-4\n4. Xu S, Zhang L, Tang Y, Han C, Wu H, Song A. Channel attention for sensor-based\nactivity recognition: embedding features into all frequencies in DCT domain.IEEE\nTrans Knowledge Data Eng(2023) 1– 15. doi:10.1109/tkde.2023.3277839\n5. Ge Y, Zhu F, Chen D, Zhao R, Wang X, Li H. Structured domain adaptation with\nonline relation regularization for unsupervised person Re-id. IEEE Trans Neural\nNetworks Learn Syst(2022) 1– 14. doi:10.1109/tnnls.2022.3173489\n6. Huang W, Zhang L, Wang S, Wu H, Song A. Deep ensemble learning for human\nactivity recognition using wearable sensors viaﬁlter activation. ACM Trans Embed\nComput Syst (2022) 22(1):1– 23. doi:10.1145/3551486\n7. Tang Y, Zhang L, Teng Q, Min F, Song A. Triple cross-domain attention on human\nactivity recognition using wearable sensors. IEEE Trans Emerging Top Comput\nIntelligence (2022) 6(5):1167– 76. doi:10.1109/tetci.2021.3136642\n8. Azarmdel H, Mohtasebi SS, Jafari A, Muñoz AR. Developing an orientation and\ncutting point determination algorithm for a troutﬁsh processing system using machine\nvision. Comput Electro Agric(2019) 162:613– 29. doi:10.1016/j.compag.2019.05.005\n9. Yang J, Wang G. Identifying cherry maturity and disease using different fusions of\ndeep features and classiﬁers. J Food Meas Characterization(2023). doi:10.1007/s11694-\n023-02091-4\n10. Wang G, Zheng H, Li X. ResNeXt-SVM: a novel strawberry appearance quality\nidentiﬁcation method based on ResNeXt network and support vector machine.J Food\nMeas Characterization (2023) 17:4345– 56. doi:10.1007/s11694-023-01959-9\n11. Wang G, Zheng H, Zhang X. A robust checkerboard corner detection method for\ncamera calibration based on improved YOLOX.Front Phys (2022) 828. doi:10.3389/\nfphy.2021.819019\n12. ElMasry G, Cubero S, Moltó E, Blasco J. In-line sorting of irregular potatoes by\nusing automated computer-based machine vision system.J Food Eng(2012) 112(1-2):\n60– 8. doi:10.1016/j.jfoodeng.2012.03.027\n13. Femling F, Olsson A, Alonso-Fernandez F. Fruit and vegetable identiﬁcation using\nmachine learning for retail applications. In: Proceedings of the 2018 14th International\nConference on Signal-Image Technology & Internet-Based Systems (SITIS); November\n2018; Las Palmas de Gran Canaria, Spain. IEEE (2018). p. 9–\n15. doi:10.1109/SITIS.2018.\n00013\n14. Sambasivam GAOGD, Opiyo GD. A predictive machine learning application in\nagriculture: cassava disease detection and classiﬁcation with imbalanced dataset using\nconvolutional neural networks. Egypt Inform J (2021) 22(1):27– 34. doi:10.1016/j.eij.\n2020.02.007\n15. Bao W, Yang X, Liang D, Hu G, Yang X. Lightweight convolutional neural\nnetwork model forﬁeld wheat ear disease identiﬁcation. Comput Electro Agric(2021)\n189:106367. doi:10.1016/j.compag.2021.106367\n16. Gao Z, Shao Y, Xuan G, Wang Y, Liu Y, Han X. Real-time hyperspectral imaging\nfor the in-ﬁeld estimation of strawberry ripeness with deep learning.Artif Intelligence\nAgric (2020) 4:31– 8. doi:10.1016/j.aiia.2020.04.003\n17. Dong C, Zhang Z, Yue J, Zhou L. Automatic recognition of strawberry diseases\nand pests using convolutional neural network. Smart Agric Tech (2021) 1:100009.\ndoi:10.1016/j.atech.2021.100009\n18. Kheiralipour K, Pormah A. Introducing new shape features for classiﬁcation of\ncucumber fruit based on image processing technique and artiﬁcial neural networks.\nJ Food process Eng(2017) 40(6):e12558. doi:10.1111/jfpe.12558\n19. Li H, Sui M, Zhao F, Zha Z, Wu F. MVT: mask vision transformer for facial\nexpression recognition in the wild (2021). Available at: https://arxiv.org/abs/2106.\n04520.\n20. Zhou D, Kang B, Jin X, Yang L, Lian X, Jiang Z, et al. Deepvit: towards deeper\nvision transformer (2021). Available at:https://arxiv.org/abs/2103.11886.\n21. Huang CW, Chen YN. Adapting pretrained transformer to lattices for spoken\nlanguage understanding. In: Proceedings of the 2019 IEEE Automatic Speech\nRecognition and Understanding Workshop (ASRU); November-2019; Singapore.\nIEEE (2019). p. 845– 52. doi:10.1109/ASRU46091.2019.9003825\n22. Brostow GJ, Fauqueur J, Cipolla R. Semantic object classes in video: a high-\ndeﬁnition ground truth database.Pattern Recognition Lett(2009) 30(2):88– 97. doi:10.\n1016/j.patrec.2008.04.005\n23. Carion N, Massa F, Synnaeve G, Usunier N, Kirillov A, Zagoruyko S. End-to-end\nobject detection with transformers. In: Proceedings of the Computer Vision– ECCV\n2020: 16th European Conference; August 2020; Glasgow, UK. Springer International\nPublishing (2020). p. 213– 29. doi:10.1007/978-3-030-58452-8_13\n24. Brown T, Mann B, Ryder N, Subbiah M, Kaplan JD, Dhariwal P, et al.\nLanguage models are few-shot learners. Adv Neural Inf Process Syst (2020) 33:\n1877– 901.\n25. Arnab A, Dehghani M, Heigold G, Sun C, Lučić M, Schmid C. Vivit: a video vision\ntransformer. In: Proceedings of the IEEE/CVF international conference on computer\nvision; October 2021; Montreal, QC, Canada (2021). p. 6836– 46.\n26. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T,\net al. An image is worth 16x16 words: transformers for image recognition at scale\n(2020). Available at:https://arxiv.org/abs/2010.11929.\n27. Zheng H, Wang G, Li X. Swin-MLP: a strawberry appearance quality\nidenti ﬁcation method by Swin Transformer and multi-layer perceptron. J Food\nMeas Characterization (2022) 16(4):2789 – 800. doi:10.1007/s11694-022-\n01396-0\nFrontiers inPhysics frontiersin.org11\nSong et al. 10.3389/fphy.2023.1278898\n28. Liu Z, Lin Y, Cao Y, Hu H, Wei Y, Zhang Z, et al. Swin transformer: hierarchical\nvision transformer using shifted windows. In: Proceedings of the IEEE/CVF\ninternational conference on computer vision; October 2021; Montreal, QC, Canada\n(2021). p. 10012– 22.\n29. Zheng H, Wang G, Li X. Identifying strawberry appearance quality by vision\ntransformers and support vector machine.J Food Process Eng(2022) 45(10):e14132.\ndoi:10.1111/jfpe.14132\n30. Jahanbakhshi A, Abbaspour-Gilandeh Y, Ghamari B, Heidarbeigi K. Assessment\nof physical, mechanical, and hydrodynamic properties in reducing postharvest losses of\ncantaloupe (Cucumis melo var. Cantaloupensis). J Food Process Eng (2019) 42(5):\ne13091. doi:10.1111/jfpe.13091\n31. Cao H, Wang Y, Chen J, Jiang D, Zhang X, Tian Q, et al. Swin-unet: unet-like pure\ntransformer for medical image segmentation. In: Proceedings of the European\nConference on Computer Vision; October 2022; Tel Aviv, Israel. Springer Nature\nSwitzerland (2022). p. 205– 18. doi:10.1007/978-3-031-25066-8_9\n32. Takahashi A, Koda Y, Ito K, Aoki T. Fingerprint feature extraction by combining\ntexture, minutiae, and frequency spectrum using multi-task CNN. In: Proceedings of\nthe 2020 IEEE International Joint Conference on Biometrics (IJCB); September 2020;\nHouston, TX, USA. IEEE (2020). p. 1– 8. doi:10.1109/IJCB48548.2020.9304861\n33. Zhu H, Yang L, Fei J, Zhao L, Han Z. Recognition of carrot appearance quality\nbased on deep feature and support vector machine.Comput Electro Agric(2021) 186:\n106185. doi:10.1016/j.compag.2021.106185\n3 4 .N iJ ,G a oJ ,D e n gL ,H a nZ .M o n i t o r i n gt h ec h a n g ep r o c e s so fb a n a n a\nfreshness by GoogLeNet. IEEE Access (2020) 8:228369 – 76. doi:10.1109/access.\n2020.3045394\n35. Selvaraju RR, Cogswell M, Das A, Vedantam R, Parikh D, Batra D. Grad-cam:\nvisual explanations from deep networks via gradient-based localization. In: Proceedings\nof the IEEE international conference on computer vision; October 2017; Venice, Italy\n(2017). p. 618– 26.\n36. Li X, Cai C, Zheng H, Zhu H. Recognizing strawberry appearance quality using\ndifferent combinations of deep feature and classiﬁers. J Food Process Eng(2022) 45(3):\ne13982. doi:10.1111/jfpe.13982\n37. Pal M. Random forest classiﬁer for remote sensing classiﬁcation. Int J remote\nsensing (2005) 26(1):217– 22. doi:10.1080/01431160412331269698\n38. Qin B, Xia Y, Li F. DTU: a decision tree for uncertain data. In: Proceedings of the\nAdvances in Knowledge Discovery and Data Mining: 13th Paciﬁc-Asia Conference,\nPAKDD 2009 Bangkok; April 2009; Thailand. Springer Berlin Heidelberg (2009).\np. 4– 15. doi:10.1007/978-3-642-01307-2_4\n39. Perez A, Larranaga P, Inza I. Supervised classiﬁcation with conditional Gaussian\nnetworks: increasing the structure complexity from naive Bayes.Int J Approximate\nReasoning (2006) 43(1):1– 25. doi:10.1016/j.ijar.2006.01.002\n40. Bose S, Pal A, SahaRay R, Nayak J. Generalized quadratic discriminant analysis.\nPattern Recognition (2015) 48(8):2676– 84. doi:10.1016/j.patcog.2015.02.016\n41. Jahromi AH, Taheri M. A non-parametric mixture of Gaussian naive Bayes\nclassiﬁers based on local independent features. In: Proceedings of the 2017 Artiﬁcial\nintelligence and signal processing conference (AISP); October 2017; Shiraz, Iran. IEEE\n(2017). p. 209– 12. doi:10.1109/AISP.2017.8324083\n42. Noble WS. What is a support vector machine?Nat Biotechnol (2006) 24(12):\n1565– 7. doi:10.1038/nbt1206-1565\n43. Xanthopoulos P, Pardalos PM, Trafalis TB, Xanthopoulos P, Pardalos PM,\nTrafalis TB. Linear discriminant analysis.Robust data mining (2013) 27– 33. doi:10.\n1007/978-1-4419-9878-1_4\n44. Chen T, Guestrin C. XGBoost. Proceedings of the 22nd ACM SIGKDD\ninternational conference on knowledge discovery and data mining . San Francisco,\nCalifornia, USA: Association for Computing Machinery (2016). p. 785– 94.\n45. Guo G, Wang H, Bell D, Bi Y, Greer K. KNN model-based approach in\nclassiﬁcation. In: Proceedings of the On The Move to Meaningful Internet Systems\n2003: CoopIS, DOA, and ODBASE: OTM Confederated International Conferences,\nCoopIS, DOA, and ODBASE 2003; November 2003; Catania, Sicily, Italy. Springer\nBerlin Heidelberg (2003). p. 986– 96. doi:10.1007/978-3-540-39964-3_62\n46. Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, et al. Going deeper with\nconvolutions. In: Proceedings of the IEEE conference on computer vision and pattern\nrecognition; June 2015; Boston, MA, USA (2015). p. 1– 9.\n47. Simonyan K, Zisserman A. Very deep convolutional networks for large-scale\nimage recognition (2014). Availab;e at:https://arxiv.org/abs/1409.1556.\n48. He K, Zhang X, Ren S, Sun J. Deep residual learning for image recognition. In:\nProceedings of the IEEE conference on computer vision and pattern recognition; June\n2016; Las Vegas, NV, USA (2016). p. 770– 8.\n49. Sandler M, Howard A, Zhu M, Zhmoginov A, Chen LC. Mobilenetv2: inverted\nresiduals and linear bottlenecks. In: Proceedings of the IEEE conference on computer\nvision and pattern recognition; June 2018; Salt Lake City, UT, USA (2018). p. 4510– 20.\nFrontiers inPhysics frontiersin.org12\nSong et al. 10.3389/fphy.2023.1278898",
  "topic": "Ripeness",
  "concepts": [
    {
      "name": "Ripeness",
      "score": 0.8955346345901489
    },
    {
      "name": "Support vector machine",
      "score": 0.6535198092460632
    },
    {
      "name": "Transformer",
      "score": 0.6233358979225159
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5688050985336304
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5576545000076294
    },
    {
      "name": "Perceptron",
      "score": 0.47305551171302795
    },
    {
      "name": "Multilayer perceptron",
      "score": 0.45818960666656494
    },
    {
      "name": "Computer science",
      "score": 0.44771403074264526
    },
    {
      "name": "Machine learning",
      "score": 0.33299317955970764
    },
    {
      "name": "Artificial neural network",
      "score": 0.3225047290325165
    },
    {
      "name": "Engineering",
      "score": 0.289446622133255
    },
    {
      "name": "Voltage",
      "score": 0.15677392482757568
    },
    {
      "name": "Horticulture",
      "score": 0.09742465615272522
    },
    {
      "name": "Ripening",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210133467",
      "name": "Xi'an Aeronautical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210110558",
      "name": "Xi'an Technological University",
      "country": "CN"
    }
  ],
  "cited_by": 10
}