{
    "title": "Leveraging Approximate Caching for Faster Retrieval-Augmented Generation",
    "url": "https://openalex.org/W4409047820",
    "year": 2025,
    "authors": [
        {
            "id": null,
            "name": "Shai Aviram Bergman",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A218734128",
            "name": "Zhang Ji",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4220461076",
            "name": "Anne-Marie Kermarrec",
            "affiliations": [
                "Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2123942917",
            "name": "Diana Petrescu",
            "affiliations": [
                "Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2106881632",
            "name": "Rafael Pires",
            "affiliations": [
                "Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A5094263155",
            "name": "Mathis Randl",
            "affiliations": [
                "Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne"
            ]
        },
        {
            "id": "https://openalex.org/A2699327441",
            "name": "Martijn de Vos",
            "affiliations": [
                "Ã‰cole Polytechnique FÃ©dÃ©rale de Lausanne"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4402827393",
        "https://openalex.org/W4389519928",
        "https://openalex.org/W4396822175",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4403899386",
        "https://openalex.org/W4313331827",
        "https://openalex.org/W4392019609",
        "https://openalex.org/W4405942816",
        "https://openalex.org/W4400641571",
        "https://openalex.org/W4226082499",
        "https://openalex.org/W2318810549",
        "https://openalex.org/W6678775411",
        "https://openalex.org/W4385474609",
        "https://openalex.org/W2126356880",
        "https://openalex.org/W2292999625",
        "https://openalex.org/W2168467811",
        "https://openalex.org/W6651372575",
        "https://openalex.org/W4403977136",
        "https://openalex.org/W2806803384",
        "https://openalex.org/W4380322001",
        "https://openalex.org/W2963469388",
        "https://openalex.org/W4398757454",
        "https://openalex.org/W2124509324"
    ],
    "abstract": "Retrieval-augmented generation (RAG) improves the reliability of large language model (LLM) answers by integrating external knowledge. However, RAG increases the end-to-end inference time since looking for relevant documents from large vector databases is computationally expensive. To address this, we introduce Proximity, an approximate key-value cache that optimizes the RAG workflow by leveraging similarities in user queries. Instead of treating each query independently, Proximity reuses previously retrieved documents when similar queries appear, substantially reducing reliance on expensive vector database lookups. To scale efficiently, Proximity employs a locality-sensitive hashing (LSH) scheme that enables fast cache lookups while preserving retrieval accuracy. We evaluate Proximity using the MMLU and MedRAG question answering benchmarks. Our experiments demonstrate that Proximity with our LSH scheme and a realistically skewed MedRAG workload reduces database calls by 78.9% while maintaining database recall and test accuracy. We experiment with different similarity tolerances and cache capacities, and show that the time spent within the Proximity cache remains low and constant (4.8 microseconds) even as the cache grows substantially in size. Our work highlights that approximate caching is a viable and effective strategy for optimizing RAG-based systems.",
    "full_text": "Leveraging Approximate Caching for Faster\nRetrieval-Augmented Generation\nShai Bergman\nHuawei Research\nZurich, Switzerland\nAnne-Marie Kermarrec\nEPFL\nLausanne, Switzerland\nDiana Petrescu\nEPFL\nLausanne, Switzerland\nRafael Pires\nEPFL\nLausanne, Switzerland\nMathis Randlâˆ—\nEPFL\nLausanne, Switzerland\nMartijn de Vos\nEPFL\nLausanne, Switzerland\nJi Zhang\nHuawei Research\nZurich, Switzerland\nAbstract\nRetrieval-augmented generation (RAG) improves the reliability of\nlarge language model (LLM) answers by integrating external knowl-\nedge. However, RAG increases the end-to-end inference time since\nlooking for relevant documents from large vector databases is com-\nputationally expensive. To address this, we introduceProximity, an\napproximate key-value cache that optimizes the RAG workflow by\nleveraging similarities in user queries. Instead of treating each query\nindependently,Proximityreuses previously retrieved documents\nwhen similar queries appear, substantially reducing the reliance on\nexpensive vector database lookups. To efficiently scale,Proximity\nemploys a locality-sensitive hashing (LSH) scheme that enables\nfast cache lookups while preserving retrieval accuracy. We evalu-\nateProximityusing theMMLUandMedRAGquestion-answering\nbenchmarks. Our experiments demonstrate thatProximitywith\nour LSH scheme and a realistically-skewedMedRAGworkload re-\nduces database calls by 77.2% while maintaining database recall and\ntest accuracy. We experiment with different similarity tolerances\nand cache capacities, and show that the time spent within theProx-\nimitycache remains low and constant (4.8 Âµs) even as the cache\ngrows substantially in size. Our results demonstrate that approx-\nimate caching is a practical and effective strategy for optimizing\nRAG-based systems.\nCCS Concepts\nâ€¢Information systems â†’Middleware for databases;Language\nmodels;â€¢Computing methodologies â†’Natural language gener-\nation.\nâˆ—Corresponding author\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nMIDDLEWARE â€™25, Nashville, TN, USA\nÂ©2025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-1554-9/2025/12\nhttps://doi.org/10.1145/3721462.3770776\nKeywords\nRetrieval-Augmented Generation, Large Language Models, Approx-\nimate Caching, Neural Information Retrieval, Vector Databases,\nQuery Optimization, Latency Reduction, Machine Learning Sys-\ntems\nACM Reference Format:\nShai Bergman, Anne-Marie Kermarrec, Diana Petrescu, Rafael Pires, Mathis\nRandl, Martijn de Vos, and Ji Zhang. 2025. Leveraging Approximate Caching\nfor Faster Retrieval-Augmented Generation. In26th International Middleware\nConference (MIDDLEWARE â€™25), December 15â€“19, 2025, Nashville, TN, USA.\nACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3721462.3770776\n1 Introduction\nLarge language models ( LLMs) have revolutionized natural lan-\nguage processing by demonstrating strong capabilities in tasks\nsuch as text generation, translation, and summarization [27]. De-\nspite their increasing adoption, a fundamental challenge is to ensure\nthe reliability of their generated responses [ 19, 56]. A particular\nissue is that LLMs are prone tohallucinationswhere they confi-\ndently generate false or misleading information, which limits their\napplicability in high-stake domains such as healthcare [ 21] and\nfinance [44]. Moreover, their responses can be inconsistent across\nqueries, especially in complex or specialized domains, making it\ndifficult to trust their outputs without extensive verification by\ndomain experts [29, 56].\nRetrieval-augmented generation (RAG) is a popular approach\nto improve the reliability of LLM answers [30]. RAG combines the\nstrengths of neural network-based text generation with external\ninformation retrieval. This technique first retrieves relevant doc-\numents from an external database based on the user query and\nincludes them in the LLM prompt before generating a response.\nBoth user queries and documents are often represented as high-\ndimensional embedding vectors that capture semantic meanings,\nand these embeddings are stored in a vector database. Retrieving\nrelevant documents involves finding embeddings in the database\nthat are the closest to the query embedding, a process known as\nnearest neighbor search (NNS). Thus, RAG enables the LLM to use\nreliable sources of information without the need to modify the\nmodel parameters through retraining or fine-tuning [46].\narXiv:2503.05530v3  [cs.DB]  27 Oct 2025\nMIDDLEWARE â€™25, December 15â€“19, 2025, Nashville, TN, USA Bergman et al.\nAt the same time, the NNS operation that is part of the RAG\nworkflow becomes computationally expensive for large vector\ndatabases [48, 57]. Thus, RAG can significantly prolong the in-\nference end-to-end time [23, 40]. To mitigate the latency increase\nof NNS, we observe that user query patterns to conversational\nagents or search engines often exhibit spatial and temporal locality,\nwhere specific topics may experience heightened interest within a\nshort time span [14] or otherwise exhibit strong bias towards some\nqueries [34]. Similar queries are likely to require and benefit from\nthe same set of retrieved documents in such cases, even if they are\nnot exactly syntactically equal. Building on this observation, we re-\nduce the database load by caching and reusing results from similar\npast user queries. This approach contrasts with conventional RAG\nsystems [42, 57] that treat each query as independent from the oth-\ners without exploiting access patterns. However, exact embedding\nmatching is ineffective when queries are phrased differently, since\neven slight rephrasings of a query typically yield different embed-\nding vectors. To this end, we introduce a novelapproximate caching\nmechanism that uses the semantic information in the embeddings\nof queries, computing similarity in the embedding space and in-\ncorporating a similarity threshold to address the exact matching\nproblem. Approximate caching allows for some level of tolerance\nwhen determining relevant cache entries.\nThis work introducesProximity, a novel approximate key-value\ncache specifically designed for RAG-based LLM systems. By in-\ntercepting queries before they reach the vector database and by\nleveraging previously retrieved results for similar queries,Proxim-\nityreduces the computational cost of NNS and minimizes database\naccesses, effectively lowering the total end-to-end inference latency\nof the RAG pipeline. Specifically, we store past document queries\nin an approximate key-value cache, where each key corresponds\nto the embedding of a previous query, and the associated value is\nthe set of relevant documents retrieved for that query. When a new\nquery is received, the cache checks if there is a cache entry within\nsome similarity threshold ğœ and if so, returns the entry closest to\nthe incoming query. On a cache hit, the corresponding documents\nare returned, bypassing the need for a database lookup. In case of a\ncache miss, the system queries the vector database to retrieve rele-\nvant documents for the new query. The cache is then updated with\nthe new query and retrieved documents, and the RAG pipeline pro-\nceeds as usual. However, to determine whether previously cached\nqueries are sufficiently close to an incoming query, we need to do a\nlinear scan over all cached entries, which becomes computationally\nexpensive as the size of the cache grows. To address this scalability\nconcern, we leverage a locality-sensitive hashing (LSH) scheme and\nintroduceProximity-LSH, a variant of our approximate cache.\nWe implementProximityand evaluate our system using the\nMassive Multitask Language Understanding ( MMLU) [ 17], and\nMedRAG[ 55] benchmarks, which are commonly used to evalu-\nate RAG frameworks. We provide two versions of theMedRAG\nbenchmarks, one where queries are repeated four times each in\nslight variations, and one where queries are repeated according\nto a Zipfian distribution with parameter 0.8. Compared to a RAG\npipeline without caching,Proximitybrings significant speed im-\nprovements while maintaining retrieval accuracy. Specifically, we\nfind thatProximityreduces the latency of document retrieval by up\nto 59% for MMLU and 75% forMedRAGwith no or only a marginal\nDocuments Embedding\nModel\nVector \nDatabase\nLLM\nUser\nQuery\nIndexing\nFigure 1: The RAG workflow.\ndecrease in accuracy. These findings hold across all versions of\nthe experiments, with and without bias towards hot queries.Prox-\nimitywith our LSH scheme and a realistically skewedMedRAG\nworkload reduces database calls by 77.2% and reduces document\nretrieval latency by 72.5% while maintaining test accuracy. These\nresults highlight the viability and effectiveness of using approxi-\nmate caching to improve the speed of RAG-based LLM systems. To\nthe best of our knowledge,Proximityis the first system to apply\napproximate caching to reduce the document retrieval latency in\nRAG pipelines during LLMs inference.\nOur contributions are as follows:\nâ€¢We introduceProximity-FLAT, a novel approximate key-\nvalue cache for RAG pipelines that leverages spatial and\ntemporal similarities in user queries to reduce the overhead\nof document retrieval (Section 3).Proximity-FLAT includes\na similarity-based caching strategy that significantly reduces\nretrieval latency while maintaining high response quality.\nâ€¢We enhance the scalability ofProximity-FLAT by introduc-\ningProximity-LSH, an efficient and scalable variant that\nrelies on locality-sensitive hashing (LSH) to determine suffi-\nciently similar cache entries (Section 3.2).\nâ€¢We benchmark both cache variants using two standard data-\nsets, demonstrating substantial improvements in cache hit\nrates and query latency while maintaining comparable test\naccuracies (Section 4). These improvements are quantified\nrelative to a baseline RAG pipeline without caching. We\nalso analyze the impact of the cache capacity and similarity\ntolerance, providing insight into optimizing retrieval perfor-\nmance for workloads with differing characteristics.\n2 Background and motivation\nWe first detail the RAG workflow in Section 2.1 and then outline\nthe process to retrieve the documents relevant to a user query from\nthe vector database in Section 2.2. We then show in Section 2.3 the\nexistence of skew in search engine queries, which provides ground\nfor caching opportunities.\n2.1 Retrieval-augmented generation (RAG)\nRetrieval-augmented generation (RAG) is a technique that enhances\nthe capabilities of LLMs by integrating information retrieval before\nthe generation process [30]. RAG typically enables higher accuracy\nin benchmarks with a factual ground truth, such as multiple-choice\nquestion answering [9].\nLeveraging Approximate Caching for Faster Retrieval-Augmented Generation MIDDLEWARE â€™25, December 15â€“19, 2025, Nashville, TN, USA\nFigure 1 shows the RAG workflow that consists of the following\neight steps. Before LLM deployment, documents are first converted\ninto high-dimensional embedding vectors using an embedding\nmodel âŠ and stored in a vector database â‹. Optionally, documents\nare divided into smaller chunks before embedding, which improves\nretrieval accuracy by allowing the system to retrieve the most rele-\nvant sections of a document rather than entire documents. These\ntwo steps comprise the indexing phase. When the user sends a\nquery to the LLMâŠ , this query is first converted to an embedding\nvector â‹ using the same embedding model as used for the indexing\nand passed to the retriever. The vector database then searches for\nembeddings close to the query embedding using some distance\nmetric and returns the relevant documents related to this embed-\nding âŒ. These documents and the user query are combined into a\nsingle prompt and passed to the LLMâ . The LLM response is then\nreturned to the user.\n2.2 Vector search\nThe vector search during the RAG workflow obtains relevant em-\nbeddings from a vector database based on the embedding vector\nof the user query. Vector databases are databases that potentially\nstore a vast amount of ğ‘›-dimensional real-valued vectors and are\noptimized to solve the nearest neighbor search (NNS),i.e., finding\nthe ğ‘˜ elements contained in the database that are the closest to\na given query [38]. The similarity metric to be minimized is typi-\ncally L2, cosine, or inner-product, and is fixed before deployment.\nThis lookup returns a ranked list of indices corresponding to re-\nsulting embeddings, and these indices can then be used to obtain\nthe documents that will be sent along with the user prompt to the\nLLM.\nDue to the high dimensionality of embeddings and the sheer\nvolume of data in modern vector databases [4], performing vector\nsearches at scale poses significant computational challenges. NNS\nrequires comparing query embeddings with millions or billions of\nstored vectors, which becomes slow and computationally expen-\nsive as the database grows [7]. Even with optimized index struc-\ntures such as as hierarchical navigable small world (HNSW) [33]\nor quantization-based approaches [ 26], to maintain low-latency\nretrieval while ensuring high recall remains difficult.\nSeveral studies show that vector search can account for a sig-\nnificant portion of the end-to-end latency in RAG-based LLM sys-\ntems [22, 41, 48]. In particular, Shen et al. [48] report that the aver-\nage Time To First Token (TTFT) increases from495 ms to965 ms af-\nter deploying RAG, with a significant share of this overhead (71.8%)\nattributed to the vector database lookup.TTFT captures the latency\nbetween sending a query and receiving the first output token from\nthe LLM. The remaining increase comes from a slightly longerLLM\npre-fill stage caused by processing the additional retrieved docu-\nments. We note that the proportion of TTFT required for vector\nsearch depends on factors such as theLLM size, the vector database\nimplementation, and the number of retrieved vectors. Nevertheless,\nthese findings highlight that vector search latency can become a\nserious bottleneck in RAG systems.\n100 101 102 103 104 105 106\n100\n102\n104\nQuery rank (log scale)\nFrequency (log scale)\nQuery frequency distribution (exact match)\nEmpirical Zipfian (ğ‘  = 0.627)\nFigure 2: The TripClick empirical query frequencies, along\nwith the expected frequencies from the matching Zipfian\ndistribution.\n2.3 Skew in user queries\nRAG pipelines often face heavily skewed user query distributions,\nsimilar to those found in search engines and conversational agents\n[15, 18, 50â€“52]. Specifically, a few popular queries dominate, while\nmost occur rarely or only once. Moreover, LLMs are increasingly\nbeing integrated in search engine ecosystems,e.g., ChatGPT Search\n[37], Google AI Overviews [16] and Microsoft Copilot Search [35].\nThese systems typically rely on RAG pipelines to ground responses\nin knowledge sources. In such systems, the documented skew and\nlocality properties of search queries [ 50, 52] carry over to LLM-\nbased retrieval, making caching particularly appealing.\nTo quantify this skew, we analyze theTripClickdataset, a large\ncollection of user interactions in a health-focused search engine.\nTripClickcomprises approximately 5.2 million user interactions\ncollected from the Trip Database [34] between 2013 and 2020. The\ndataset includes around700 000unique free-text queries and1.3mil-\nlion query-document relevance pairs, making it a valuable resource\nfor studying user search behavior.\nFigure 2 shows the exact-match query frequency distribution\nwith the query rank on the horizontal axis and frequency of the\nquery on the vertical axis (both axis in log scale). The empirical\ndistribution of query frequencies closely matches a Zipfian curve\nwith an exponent of â‰ˆ0.627. This power law distribution is not\nunique to theTripClickdataset. In fact, several studies show that\nuser queries in other domains also often follow a Zipfian power law\ndistribution, with exponent ğ‘  roughly between ğ‘ = 0.6and ğ‘ = 2.5,\nresulting in a strong bias towards a few topics of interest [12, 14, 45].\nThis bias is typical of natural language and brings opportunity for\ncaching: the most frequent queries and their semantic variants\ngenerate a disproportionate share of retrieval workload and their\nresults may be reused, therefore reducing computational load [3].\nBeyond exact query duplicates, many queries differ only slightly\nin their written form but convey highly similar intent. These include\nrephrasings, minor spelling variations, synonym substitutions, or\nchanges in the word order (e.g., â€œbest treatment for asthmaâ€ vs.\nâ€œasthma best therapiesâ€). While these variations are not captured\nby exact matching, they often lie close together in a (latent) em-\nbedding space. To analyze this effect, we embed the queries in the\nTripClickdataset using theMedCPT[ 25] embedding model and\nproject the resulting vectors (which have a dimension of 768) into\ntwo dimensions using a combination of principal component anal-\nysis (PCA) [49] as a preprocessing step, followed by t-distributed\nMIDDLEWARE â€™25, December 15â€“19, 2025, Nashville, TN, USA Bergman et al.\nâˆ’50 0 50\nâˆ’50\n0\n50\nğ‘¥\nğ‘¦\n20\n40\n60\n80\ncount\nFigure 3: The two-dimensional projection of queries in the\nTripClickdataset, each one encoded using theMedCPT\nembedding model.\nstochastic neighbor embedding (t-SNE) [53] for visualization pur-\nposes. We show the two-dimensional embedding space in Figure 3\nas a 100x100 grid. For each point in the grid we count the number\nof embeddings in it and color it accordingly. This figure highlights\nthat many queries cluster based on semantic content, even when\ntheir wording differs.\nThus, we empirically demonstrate using theTripClickdataset\nthat(i)user queries frequently repeat exactly, following a pow-\ner-law distribution; and(ii)that syntactically different queries clus-\nter in the embedding space, indicating semantic relationships be-\ntween distinct queries. These two findings are relevant for RAG\nsystems, which leverage the embedding space. In such systems,\neach user query is mapped to a high-dimensional vector, and the\nnearest neighbors in the document index are retrieved based on\nthis vector. When two queries are semantically similar, not only\nare their embeddings close together, but the resulting retrieved doc-\numents often overlap. In other words, different queries may map\nto the same or similar document sets.This redundancy means that\npreviously retrieved documents can be cached and reused for future\nqueries with similar embeddings, reducing the number of expensive\nNNS operations in the vector database.\n3 Design ofProximity\nGuided by the above insights, we design a caching mechanism that\nreduces the need for repeatedNNSs by reusing previously retrieved\ndocuments for similar queries. More specifically, we leverageap-\nproximate cachingto accelerate RAG document retrieval. Even if\nthe documents retrieved for a given query are not the most optimal\nresults that would have been obtained from a full database lookup,\nthey can still provide valuable context and relevant information\nfor the LLM, allowing the system to maintain good accuracy while\nAlgorithm 1:Cache lookup inProximity-FLAT\nStored state:similarity toleranceğœ, cache capacityğ‘,\nvector databaseD, key-value dictC= {}\n1Procedurelookup(ğ‘):\n2ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ =[(ğ‘˜,distance(ğ‘,ğ‘˜))forğ‘˜inC.keys]\n3(ğ‘˜ğ‘’ğ‘¦,ğ‘šğ‘–ğ‘›_ğ‘‘ğ‘–ğ‘ ğ‘¡)â†min_by_dist(ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ )\n4ifğ‘šğ‘–ğ‘›_ğ‘‘ğ‘–ğ‘ ğ‘¡â‰¤ğœthen\n5returnC[ğ‘˜ğ‘’ğ‘¦]\n6Iâ†D.retrieveDocumentIndices(ğ‘)\n7if|C|â‰¥ğ‘then\n8C.evictOneEntry()\n9C[ğ‘]â†I\n10returnI\nreducing retrieval latency. Among others, our approximate cache is\nparameterized with a similarity thresholdğœ. If the distance between\ntwo query embeddings ğ‘1 and ğ‘2 is equal to or less than ğœ, we con-\nsider these embeddings alike and return similar documents if they\nare available in the cache. Cache hits thus bypass more expensive\nvector database searches. However, to determine whether previous\ncached queries are close to an incoming query, we need to do a\nlinear scan over all cached entries, which becomes computationally\nexpensive as the size of the cache grows. Therefore, the two techni-\ncal challenges are(i)defining an effective set of hyperparameters,\nsuch as the similarity threshold that maximizes cache hits without\ncompromising response relevance, as well as the optimal cache size\nthat enables high cache coverage while still being computationally\nattractive, and(ii)ensuring low computational overhead of each\nquery lookup as the number of cached entries grows.\nWe now present the design ofProximity, an approximate key-\nvalue cache designed to accelerate RAG pipelines.Proximityis\nagnostic of the specific vector database being used but assumes\nthat this database has aretrieveDocumentIndicesfunction that\ntakes as input a query embedding and returns a sorted list of docu-\nment indices whose embeddings are close to the query embedding.\nIn Section 3.1 we first present the high-level process overview of\nretrieving vectors withProximity. Then in Section 3.2 we intro-\nduce locality-sensitive hashing (LSH) in the context of approximate\ncaching, one of the key optimizations implemented to reduce the\nlatency of a search in theProximitycache. Finally, we elaborate in\nSection 3.3 on the parameterization and components of our caching\nmechanism.\n3.1 Retrieving relevant documents with\nProximity-FLAT\nWe first present the basic version of our cache, namedProxim-\nity-FLAT, and later describe an optimized version of our cache in\nSection 3.2. Algorithm 1 shows the high-level algorithm for retriev-\ning relevant documents withProximity-FLAT. We also visualize\ntheProximitycache and workflow in Figure 4 with an example\nwhen processing two subsequent similar query embeddings ğ‘1 and\nğ‘2. Each key in the cache corresponds to an embedding previously\nqueried, while the associated value is a list of the top- ğ‘˜ nearest\nneighbors retrieved from the database during a previous query,\nLeveraging Approximate Caching for Faster Retrieval-Augmented Generation MIDDLEWARE â€™25, December 15â€“19, 2025, Nashville, TN, USA\nVector\nDatabase\nAssets\nQuery \nEmbedding \nNeighborÂ  V ector \nIndices \nq1 \n... ...\n... ...\n... ...\n1 Lookup\nRetriever\nPROXIMITY  cache\nmiss\nhit\nFill cache\n2 \n3 \n4 \n5 \n6 \nFigure 4: The design and workflow of theProximityapprox-\nimate cache when receiving two subsequent, similar query\nembeddings ğ‘1 and ğ‘2. ğ‘1 results in a cache miss whereas ğ‘2\nresults in a hit, returning similar document indices as for ğ‘1.\nwhere ğ‘˜ is a strictly positive integer constant picked at the start\nof the experiment. The cache has a fixed capacity of ğ‘ key entries,\nwhich implies that, when full, an eviction policy is applied to make\nroom for new entries (see Section 3.3.2).\nWhen a user initiates a query, it is first converted into an embed-\nding vector by an embedding model.Proximityis agnostic of the\nspecific embedding model used but we assume that the embedding\nof the user query and the embeddings in the vector database are\ngenerated by the same embedding model, since embeddings from\ndifferent models are not directly comparable. The retriever (left in\nFigure 4) then forwards this query embedding, denoted as ğ‘1, to\ntheProximitycache âŠ.Proximityfirst checks whether a similar\nquery has been recently processed by iterating over each key-value\npair (ğ‘˜, ğ‘£) of cache entries (line 2 in Algorithm 1). If the best match\nis sufficiently close to the query,i.e., the distance between ğ‘and\nğ‘˜ is lower than some threshold ğœ, the associated retrieval results\nare returned immediately (lines 3-5), thus bypassing the vector\ndatabase. Otherwise, the system proceeds with a standard database\nquery (line 6 in Algorithm 1). This is step â‹ in Figure 4, where we\nperform a lookup with ğ‘1 and the vector database. TheProximity\ncache is now updated with the resulting neighbor vector indices\n(in blue) from the database lookup âŒ. Since the number of cache\nentries might exceed the cache size, the eviction policy will remove\na cache entry if necessary (lines 7-8). The cache is now updated\nwith the result obtained from the database (line 9). Finally, the\nvector indices are returned to the retriever â. We adopt the same\ndistance function inProximityas the underlying vector database\nto ensure consistency between the caching mechanism and the\nretrieval process.\nWhen another query embeddingğ‘2 arrives âwith a low distance\nto ğ‘1,Proximityfirst checks if it is sufficiently similar to any stored\nquery embeddings in the cache. Suppose the distance between ğ‘1\nand ğ‘2 is below the predefined similarity threshold ğœ. In that case,\nthe cache returns the previously retrieved document indices associ-\nated with the closest matching query â, thus bypassing a lookup\nin the vector database. We name this first approachProximity-\nFLAT as it scans the entire cache for every incoming query (line 2)\nwithout using any data structure to guide it further. Depending on\nthe specifications of the cache and workload,Proximity-FLAT can\nreduce retrieval latency and computational overhead, especially in\nworkloads with strong spatial or temporal similarities.\nLocality sensitive\n hashing\nQuery \nembedding\nbuckets\nL bits\nFlat caches\n<latexit sha1_base64=\"dGmwunE29SZv5b/Tqu+i3S98UEQ=\">AAACxnicjVHLSsNAFD2Nr1pfVZdugkVwVZIi1WXRTRcuKtoH1CpJOq1D8yKZKKUI/oBb/TTxD/QvvDNOQS2iE5KcOfeeM3PvdWOfp8KyXnPG3PzC4lJ+ubCyura+UdzcaqVRlnis6UV+lHRcJ2U+D1lTcOGzTpwwJ3B91nZHJzLevmVJyqPwQoxj1gucYcgH3HMEUeeVq9PrYskqW2qZs8DWoAS9GlHxBZfoI4KHDAEYQgjCPhyk9HRhw0JMXA8T4hJCXMUZ7lEgbUZZjDIcYkf0HdKuq9mQ9tIzVWqPTvHpTUhpYo80EeUlhOVppopnylmyv3lPlKe825j+rvYKiBW4IfYv3TTzvzpZi8AAR6oGTjXFipHVedolU12RNze/VCXIISZO4j7FE8KeUk77bCpNqmqXvXVU/E1lSlbuPZ2b4V3ekgZs/xznLGhVyna1XD07KNWO9ajz2MEu9mmeh6ihjgaa5D3EI57wbNSN0MiMu89UI6c12/i2jIcPqIGQAA==</latexit>\n2L\nFigure 5: The workflow ofProximity-LSH where each LSH\nbucket maps to aProximity-FLAT cache.\n3.2 Scalability andProximity-LSH\nWhileProximity-FLAT provides caching benefits with minimal\ncomplexity, its linear scan over all cached entries is a scalability\nbottleneck. As the cache size increases, so does the computational\ncost of each query lookup, eventually increasing the end-to-end\ninference time. This linear scan ensures that we find the closest\nmatch in the cache but also results in a lookup cost that is lin-\nearly dependent on ğ‘, the cache capacity. For large caches, this cost\nbecomes prohibitive.\nTo mitigate this scalability bottleneck, we introduceProxim-\nity-LSH, a variant of our approximate cache that supports scalable\nsimilarity-based lookups using LSH, to enable the query to be redi-\nrected to a small bucket of entries that are the most likely to match it.\nThis bounds the amount of query-key comparisons to be performed\nto the size of the bucket. More specifically, we rely on random\nhyperplane LSH [6], a classical method for approximate nearest\nneighbor (ANN) search in high-dimensional spaces. The key idea is\nto compare each embedding to a fixed set of ğ¿randomly generated\nhyperplanes passing through the origin. Each hyperplane defines a\nbinary partition of space: a vector lies on one side or the other. By\nrepeating this for ğ¿hyperplanes, we obtain an ğ¿-bit signature (a\nbinary hash code) that serves as the key for theLSH table. Formally,\ngiven an embedding vector ğ‘âˆˆR ğ‘‘, and ğ¿ random hyperplanes\n(stored as normal vectors ğ‘Ÿ1,ğ‘Ÿ2,...,ğ‘Ÿ ğ¿), we compute its hash code\nâ„(ğ‘)as:\nâ„(ğ‘):{0, 1}ğ¿ =(ğ‘Â·ğ‘Ÿ 1 â‰¥0, â€¦,ğ‘Â·ğ‘Ÿ ğ¿ â‰¥0)\nInProximity-LSH, the hash code â„(ğ‘)determines the bucket to\nwhich an incoming query maps. We show the workflow ofProx-\nimity-LSH in Figure 5. Each bucket is a fixed-sizeProximity-FLAT\ncache with a capacity ofğ‘= 20entries, which we empirically find to\nstrike a good balance between the hit rate and the execution latency\nper query (see Section 4.3.5). When a query arrives, we compute its\nLSH hash, identify its corresponding bucket, and perform a linear\nscan within only that bucket to check for similar embeddings. If no\nsimilar entry is found, the query is forwarded to the vector database,\nand the bucket is updated accordingly. Unlike when using a single\nflat cache, each bucket inProximity-LSH operates its own local\neviction policy such as first-in, first-out ( FIFO) or least-recently\nused (LRU). In particular, there is no global eviction policy that\nis shared acrossProximity-LSH caches as this would require ad-\nditional data structures and state storage, resulting in additional\nMIDDLEWARE â€™25, December 15â€“19, 2025, Nashville, TN, USA Bergman et al.\ncompute and memory costs. Note that this makesProximity-LSH\na ğ‘-way set-associative cache, where the set corresponding to a\ngiven entry is designated by ğ¿-bit LSH. We experimentally show in\nSection 4 that LSH-based partitioning sustains high cache hit rates\nand accuracy.\nAnother benefit of using such buckets is the mitigation of false\npositives,i.e., cases where the similarity tolerance is large enough to\nconsider a query and a cache line as matching, even though the two\nembeddings are actually semantically very different. This mitigation\narises because the tolerance only applies within the bucket to which\nthe query maps, limiting potential false positives to that subset. Due\nto the LSH property of grouping similar embeddings, cache lines\nin this bucket are more likely to be relevant. Moreover, if a query\nis very infrequent, its corresponding bucket may be empty. In such\na case, false positives cannot occur, even though other buckets\ncontain entries. This contrasts withProximity-FLAT, where any\ncache line can potentially be a false positive depending on the global\nsimilarity threshold.\nPerformance gains compared toProximity-FLAT.Proxim-\nity-LSH yields a significant performance boost when dealing with\ncaches with many entries. InProximity-FLAT, every query is com-\npared against all ğ‘ cached embeddings using the chosen distance\nmetric, resulting in a per-query lookup cost of O(ğ‘Â·ğ‘‘) , where ğ‘‘\nis the embedding dimensionality. This linear dependency on the\ncache capacity becomes noticeable in large caches, especially when\nğ‘reaches into the thousands, as discussed in Section 4.5.1.\nIn contrast,Proximity-LSH limits comparisons to a single fixed-\nsize bucket. Each query is hashed using ğ¿random hyperplanes, at\na cost of O(ğ¿Â·ğ‘‘) , and linearly compared only to the ğ‘entries (e.g.,\nğ‘= 20) in the selected bucket, with a comparison cost of O(ğ‘Â·ğ‘‘)\ncorresponding to the distance computations for each of the ğ‘cache\nlines. Both ğ¿and ğ‘are (small) constants, making the total cost of a\nlookup O(ğ‘‘). Most importantly, it is independent of the total cache\ncapacity.\nAs an example, forğ‘= 10 000entries,ğ‘‘= 768, ğ‘= 20, and ğ¿= 10,\naProximity-FLAT lookup performs10 000 Ã—768 = 7.68million\noperations per query. In contrast,Proximity-LSH performs only\n(ğ¿+ğ‘)Â·ğ‘‘= 30 Ã—768 = 23 040operations, a reduction of over\n300Ã—in per-query compute. This performance gap widens with\nincreasing cache size. We show in Section 4 that this speedup does\nnot come at the cost of the quality of the retrieved documents.\n3.3Proximitycache parameters and\ncomponents\nWe now describe the parameters and components of theProximity\ncache, and discuss their impact on performance.\n3.3.1 Cache capacity ğ‘.The cache has a capacity of ğ‘entries, which\ndictates the number of entries it will fill before starting to evict\nold entries (i.e., the number of rows in Figure 4). This parameter\nposes a trade-off between the cache hit rate and the time it takes to\nscan the entire set of keys. A larger cache increases the likelihood\nof cache hits, allowing the system to reuse previously retrieved\ndocuments more frequently and reducing the number of expensive\nvector database lookups. However, increasing the cache size also\nincurs(i)computational overhead as the cache must be searched for\nsimilarity matches on each query and(ii)memory overhead since\nadditional key-value pairs need to be stored. For a small enough ğ‘,\nthis computational overhead is manageable since the cache size is\nlikely to be small compared to the full vector database.\nEffective capacity inProximity-LSH.UnlikeProximity-FLAT,\nwhere the cache capacity ğ‘ is a flat global limit on the number of\ncached embeddings,Proximity-LSH distributes this capacity across\nmultiple buckets, each with a fixed maximum sizeğ‘(e.g., 20 entries).\nThe total cache capacity ofProximity-LSH is thus ğ‘= 2ğ¿ Â·ğ‘, where\nğ‘is the per-bucket size andğ¿the number of random hyperplanes.\nHowever, because LSH partitions the embedding space based\non the distribution of query vectors, in practice not all buckets re-\nceive traffic. Some buckets remain unused, especially in workloads\nwith skewed or clustered query distributions. As a result, the actual\nnumber of stored entries is often much smaller than the theoretical\nmaximum. This sparsity leads to more efficient space usage: we do\nnot allocate memory for a given bucket until an entry is inserted.\nProximity-LSH allocates space only where needed, adapting natu-\nrally to the query distribution. Our experiments in Section 4 show\nthat withProximity-LSH only a fraction of the buckets are actively\npopulated, and the memory footprint scales more gracefully with\nusage compared to flat caches of the same theoretical size.\n3.3.2 Eviction policy.When the cache reaches its maximum capac-\nity, an eviction policy is required to determine which entry should\nbe removed to make space for new ones. The choice of policy can\ninfluence the hit rate of the cache, especially in workloads with\nstrong temporal locality or repeated query patterns. InProxim-\nity-FLAT, we support both FIFO and LRU eviction strategies. FIFO\nevicts the oldest inserted entry regardless of usage frequency. It is\nstraightforward to implement, incurs minimal overhead, and in our\nexperiments, provides comparable accuracy to more sophisticated\nstrategies in many settings. However,FIFO can underperform when\nthere is strong temporal locality,i.e., when recent queries are more\nlikely to be repeated. In such cases, LRU can yield higher hit rates\nby preferentially retaining recently accessed entries.LRU maintains\na usage timestamp or recency ordering, evicting the entry that has\ngone unused the longest. While slightly more expensive to manage,\nwe find that LRU remains efficient for cache sizes considered in\nProximity-FLAT and is particularly effective under bursty query\ntraffic. InProximity-LSH, eviction is managed separately within\neach individual bucket.\n3.3.3 Distance tolerance ğœ.We employ a fuzzy matching strategy\nbased on a predefined similarity threshold ğœ to determine whether\na cached entry can be used for an incoming query. The choice\nof ğœ directly impacts recall and test accuracy, and is therefore a\nkey parameter ofProximity. A low value of ğœ enforces stricter\nmatching, ensuring that retrieved documents are highly relevant\nbut potentially reducing the cache hit rate, thus limiting the benefits\nof caching. We note thatğœ= 0is equivalent to using a cache with\nexact matching. Conversely, a higher value of ğœ increases cache\nutilization by accepting looser matches, improving retrieval speed\nat the potential cost of including less relevant documents. In our\nexperiments, ğœ is treated as a global constant, manually set at the\nstart of each evaluation. In general, determining the appropriate\ntolerance for high-dimensional approximate matching is a challenge\nthat has no standard solution in the literature we reviewed. Most\nLeveraging Approximate Caching for Faster Retrieval-Augmented Generation MIDDLEWARE â€™25, December 15â€“19, 2025, Nashville, TN, USA\nnotably, Friederet al. [14] propose reusing the neighbor information\nto generate a dynamic tolerance per cache line. We found that this\nstill required some arbitrary hand-tuning. In this paper, we leave\nğœ as a hyperparameter to be optimized during deployment. We\nexplore the influence of this parameter on the cache performance\nin Section 4.\n3.3.4 The re-ranking factor ğœŒ.It is common practice for ANN vec-\ntor databases such asDiskANN[ 20] andFAISS[ 10] to retrieve\nmore neighbors than they actually return to the retriever [32]. This\nover-fetching improves recall at the cost of increased latency. We\nadopt a similar strategy inProximity. Specifically, when querying\nthe vector database (line 6 in Algorithm 1), we request a number\nof neighbors significantly exceeding the count required by the re-\ntriever. This does not incur additional latency since the database\ninherently performs this broader search. Upon a subsequent cache\nhit (line 5 in Algorithm 1), rather than immediately returning all\nassociated vectors, we re-rank these vectors to prioritize and select\nonly the ones that are most relevant to the current query. We de-\nfine the re-ranking factor ğœŒâ‰¥ 1as the ratio between the number\nof vectors retrieved from the database and the number of vectors\nexpected by the RAG pipeline.\n4 Evaluation\nWe implement bothProximity-FLAT andProximity-LSH, and\nevaluate our approximate caching approach using two standard\nbenchmarks: Massive Multitask Language Understanding (MMLU)\nandMedRAG. Specifically, our experiments answer the following\nthree questions:\n(i) What is the impact of cache parameters such as cache ca-\npacity ğ‘, similarity tolerance ğœ, LSH hashing granularity ğ¿,\nand per-bucket capacity ğ‘on the end-to-end test accuracy,\nk-recall, hit rate, and latency ofProximity-FLAT andProx-\nimity-LSH (Section 4.3)?\n(ii) How does the cache occupancy ofProximity-LSH change\nwhen varying the LSH hashing granularity ğ¿and similarity\ntoleranceğœ(Section 4.4)?\n(iii) How does the lookup time ofProximityvary with cache oc-\ncupancy and parameters such as cache capacity ğ‘, similarity\ntolerance ğœ, and LSH hashing granularity ğ¿? Furthermore,\nhow well doesProximitymaintain hit rate and recall on\nlarge-scale datasets? (Section 4.5)\n4.1 Implementation details\nWe implementProximityin the Rust programming language and\nmake its implementation available online. 1 We use SIMD CPU\ninstructions for all numerical computations (e.g., the Euclidean dis-\ntance compuation at line 2 of algorithm 1). Our implementation\nusesportable-simd 2, an experimental extension to the Rust lan-\nguage that enables ISA-generic explicit SIMD operations. While\nall LLVM target architectures are technically supported, we only\nconsidered modern x64 and ARM64 platforms for unit testing and\nperformance evaluation.\n1https://github.com/sacs-epfl/proximity\n2https://github.com/rust-lang/portable-simd\nWe expose bindings from the Rust cache implementation to the\nPython machine learning pipeline usingPyO3 3 (Rust-side bindings\ngeneration) andMaturin 4 (Python-side package management).\nThese bindings streamline the integration ofProximityinto exist-\ning RAG pipelines.\n4.2 Experimental setup\nTo evaluateProximity, we adopt and modify two existing end-\nto-end RAG workflows, MMLU andMedRAG, both introduced by\nprevious work on LLM question answering [1, 55]. In our setup,\nall vectors are stored in main memory without serialization to\ndisk, which enables low-latency access to them. We leverage the\nFAISSlibrary [ 10] for efficient ANN search. For the LLM, we use\nthe open-source LLaMA 3.1 Instruct model [11], which is optimized\nfor instruction-following tasks.\n4.2.1 Document source.For the MMLU benchmark, we use the\nwiki_dprdataset as a document source, which contains 21 mil-\nlion passages collected from Wikipedia. The index used isFAISS-\nHNSW, a graph-based indexing structure optimized for fast and\nscalable ANN search. ForMedRAG, we usePubMedas the doc-\nument source, which contains 23.9 million medical publication\nsnippets. The associated vector database is served usingFAISS-\nFlat. For bothwiki_dprandPubMed, we embed each passage as a\n768-dimensional vector. We use theMedCPT[ 25] and theDPR[ 28]\nembedding models for theMedRAGand MMLU benchmarks, re-\nspectively.\n4.2.2 Query workload.We evaluateProximityby using a subset\nof the MMLU andPubMedQAquestion datasets.\nMMLU is a comprehensive benchmark designed to evaluate\nthe knowledge and reasoning of LLMs across a wide array of sub-\njects [17]. It encompasses approximately16 000multiple-choice\nquestions ranging between57topics,e.g., mathematics and history,\nwith question difficulties ranging from elementary to advanced\nprofessional levels. MMLU is frequently used to evaluate the effec-\ntiveness of RAG, and we leverage the subset of questions on the\ntopic of econometrics, containing131total questions. We specifi-\ncally pick this subset because it highly benefits from RAG.\nMedRAGis a benchmark for question answering in the biomed-\nical domain, and we specifically focus on thePubMedQAquestion\nset that contains500questions and answers. Similarly to MMLU,\nwe select at random200questions fromPubMedQAto serve as\nuser queries. To simulate similarity, we generate four variants of\neach question by adding some small textual prefix to them and\nwe randomize the order of the resulting524questions for MMLU\nand800forMedRAG. These two datasets help us to showcase the\nperformance of our cache implementation in the case where there\nis no strong bias in the queries used: every query appears four\ntimes in a slightly different format each time. We denote these two\ndatasets as theuniformdatasets, as they do not display bias in the\nqueries.\nMedRAG-Zipf.Finally, to showcase the capabilities ofProxim-\nityin the context of strong, real-life-like bias, we create a synthetic\nthird dataset based onPubMedQAby drawing10 000queries from\n3https://docs.rs/pyo3/latest/pyo3/.\n4https://www.maturin.rs.\nMIDDLEWARE â€™25, December 15â€“19, 2025, Nashville, TN, USA Bergman et al.\nNo cache 0.5 1 2 5 10\n10\n50\n100\n200\n300\n50.2 50.2 50.2 50.3 48.2 48.8\n50.2 50.2 50 50 47.9 48.1\n50.2 50.2 49.8 49.9 47.9 48.1\n50.2 50.2 49.9 49.6 47.9 48.1\n50.2 50.2 49.8 49.6 47.9 48.1\nCache capacity (ğ‘)\n(a) Test accuracy [%]\n48\n49\n50\nNo cache 0.5 1 2 5 10\n- 0.1 2.6 6.1 81.3 93\n- 0.5 11.8 27.2 92.4 93\n- 1.3 21.6 56.4 92.4 93\n- 2.8 36.9 69.3 92.4 93\n- 3.6 40.1 69.3 92.4 93\n(b) Hit rate [%]\n0\n20\n40\n60\n80\nNo cache 0.5 1 2 5 10\n101.42 103.15 98.68 93.93 25.15 7.94\n88.99 93.97 81.84 70.19 9.95 7.94\n92.96 91.45 70.54 43.94 9.93 7.31\n90.7 88.93 65.59 35.36 10.72 7.31\n89.95 89.81 58.45 36.38 11.16 7.59\n(c) Retrieval latency [ms]\n20\n40\n60\n80\n100\nNo cache 2 5 10\n10\n50\n100\n200\n300\n87.1 87.1 87.5 39.3\n87.1 87.1 87.5 36.6\n87.1 87.1 88.1 36.6\n87.1 87.4 87.5 36.6\n87.1 87.4 87.5 36.6\n 40\n60\n80\nNo cache 2 5 10\n- 0.8 3.8 98\n- 3 20.3 98.4\n- 5.8 42.1 98.4\n- 12 72.6 98.4\n- 15.9 73.3 98.4\nSimilarity tolerance (ğœ)\n0\n20\n40\n60\n80\nNo cache 2 5 10\n4,820 4,905 4,263 59\n4,820 4,334 3,540 72\n4,821 4,641 1,788 51\n4,829 4,323 1,337 80\n5,266 4,424 1,408 86\n2,000\n4,000\nDarker is better\nMMLU\nMedRAG\nFigure 6: The test accuracy (a), cache hit rate (b), and latency of document retrieval (c) ofProximity-FLAT, for different cache\ncapacities and similarity tolerances, for the MMLU (top) andMedRAG(bottom) benchmarks.\n2.5 5.0 7.5 10.0\n0\n50\n100Test accuracy[%]\n(a) Test accuracy [%]\nLSH-LRU LSH-FIFO LRU FIFO\n2.5 5.0 7.5 10.0\n0\n50\n100\nSimilarity tolerance (ğœ)\nRecall [%]\n(b) Recall [%]\n2.5 5 7.5 10\n4\n6\n8\n10\n13.1 45.5 52.6 72.4\n23.6 62.2 68.7 82.8\n33.4 71.8 77.2 84.4\n38.2 73.3 77.4 81.7\nHash bits (ğ¿)\n(c) Hit rate [%]\n20\n40\n60\n80\n2.5 5 7.5 10\n4\n6\n8\n10\n4 2.7 2.4 1.5\n3.5 1.9 1.6 0.8\n3 1.2 1 0.7\n2.8 1.2 1.1 0.9\nHash bits (ğ¿)\n(d) Average latency [ğ‘ ]\n1\n2\n3\n4\nDarker is better\nFigure 7: The test accuracy (a), recall (b), hit rate (c) and average latency (d), for different similarity tolerances, eviction policies,\nwith and without LSH, under theMedRAG-Zipfbenchmark.\nthe original set of500PubMedQAquestions with repetition, fol-\nlowing a Zipf distribution with parameter 0.8, which we estimate to\nbe a reasonable representation of real-world skews, as discussed in\nSection 2.3. Each time a given query appears, we rephrase it by ask-\ning an LLM5 to rephrase the question in a way that is syntactically\ndifferent but semantically equivalent to the original. We verify that\neach generated version of the question is unique across the entire\ndataset, and that the RAG generative system indeed provides the\nsame answer across all rephrasings when run withoutProximity.\nThe most frequently repeated question appears about700times in\nthis dataset while the vast majority of original questions appear\nat most a few times. Every original question appears at least once.\nThis simulates several users asking the same question with various\nwordings, in which caseProximityshould ideally only perform a\nsingle retrieval from the vector database, and rely on the cache for\nfurther queries. As a worst-case scenario for caching, we do not\nsimulate temporal locality of queries: all queries are statistically\nindependent from each other, even though their distribution is se-\nverely biased towards some of the originalPubMedQAquestions.\nWe refer to this third dataset as theMedRAG-Zipfdataset.\n5gpt4o-2024-08-06 by OpenAI\n4.2.3 Hardware.We launch our experiments in Docker containers,\nusing 12 cores of an x64 Intel Xeon Gold 6240 CPU and300 GB of\nallocated RAM per container.\n4.2.4 Metrics.Our evaluation focuses on three performance met-\nrics:(i)Thetest accuracyof the entire RAG system, which is com-\nputed as the percentage of multiple-choice questions in our query\nworkloads answered correctly by the LLM;(ii)Thecache hit rate,\nwhich is defined as the percentage of queries that find a sufficiently\nsimilar match in the cache;(iii)Theretrieval latency, which is the\ntime required to retrieve the relevant documents, including both\ncache lookups and vector database queries where necessary. To\nquantify how well the cache preserves the quality of retrieved con-\ntext, we also measure thedatabase ğ‘˜-recall, defined as the fraction\nof the top-ğ‘˜ documents returned by the cache that are also among\nthe top-ğ‘˜ results retrieved from the vector database for the same\nquery. This metric allows us to assess the extent to which cached\nresults align with the true nearest neighbors, serving as a proxy\nfor retrieval quality. While the ultimate goal is to maintain end-to-\nend accuracy in the RAG pipeline, ğ‘˜-recall provides a more direct\nand inexpensive way to evaluate the effectiveness of approximate\ncaching, independent of downstream LLM variability. To ensure\nLeveraging Approximate Caching for Faster Retrieval-Augmented Generation MIDDLEWARE â€™25, December 15â€“19, 2025, Nashville, TN, USA\nstatistical robustness, we run each experiment five times and with\ndifferent random seeds. We average all results.\n4.3 The impact of cache parameters on test\naccuracy, recall, hit rate, and latency\nWe first examine the impact of the cache capacityğ‘and similarity\ntolerance ğœon the three metrics described above. We evaluate these\nmetrics across different cache capacitiesğ‘âˆˆ {10,50,100,200,300} for\nboth uniform benchmarks using the FIFO cache eviction policy. We\nexperiment with tolerance levels ğœâˆˆ{ 0, 0.5, 1, 2, 5, 10}for MMLU\nand ğœâˆˆ{ 0, 2, 5, 10}forMedRAG. We perform no re-ranking (ğœŒ= 1)\nfor MMLU andMedRAG. For theMedRAG-Zipfdataset, we provide\nthe accuracy and database recall for various cache eviction policies,\nwith and without LSH (see Section 3.2). We also report the hit rate\nand average latency, as in the uniform datasets. We use a re-ranking\nfactor of ğœŒ= 4in experiments onMedRAG-Zipf. Figure 6 shows\nthe results on all uniform datasets and Figure 7 shows the results\nforMedRAG-Zipf.\n4.3.1 Test accuracy.Figure 6(a) shows the end-to-end RAG accu-\nracy for the MMLU andMedRAGbenchmarks for different combi-\nnations of ğ‘ and ğœ and withProximity-FLAT. The figure indicates\nthat accuracy remains relatively stable across different combina-\ntions of ğ‘ and ğœ, with values ranging between 47.9% and 50.2% for\nMMLU (top row). Test accuracy is slightly higher for low similarity\ntolerances ğœ= 0(no cache) and ğœ= 0.5: approximately 50.2%. In-\ncreasing ğœ slightly degrades accuracy, bringing it closer to 48.1%.\nThis is because a higher similarity tolerance increases the likelihood\nof including irrelevant documents in the LLM prompt, negatively\naffecting accuracy. We observe similar behavior inMedRAG(bot-\ntom row), which shows a more pronounced accuracy drop between\nğœ= 5(88%) and ğœ= 10(37%) for the same reason. Increasing ğ‘\ncan lower accuracy,e.g., in MMLU for ğœ= 1.0, accuracy lowers\nfrom 50.2% to 49.8% when increasing ğ‘ from 10 to 300. Interest-\ningly, the highest observed accuracies of 50.3% (for ğœ= 3,ğ‘= 10in\nMMLU) and 88.1% (forğœ= 5,ğ‘= 100inMedRAG) are achieved with\ncaching. This serendipitously occurs because the approximately\nretrieved documents prove more helpful on the MMLU benchmark\nthan the closest neighbors retrieved from the database without\ncaching (ğœ= 0). In both scenarios, the cache rarely decreases ac-\ncuracy to the level of the LLM without RAG (48% for MMLU, 57%\nforMedRAG), except when the value of ğœ becomes too high (e.g.,\nğœ=10forMedRAG).\nFigure 7(a) shows the test accuracy when using theMedRAG-\nZipfbenchmark, for different similarity tolerances, eviction policies,\nand with and without LSH. For ğœâˆˆ{ 2.5, 5, 7.5}, we observe compa-\nrable accuracies across the evaluated eviction policies and between\nProximity-FLAT andProximity-LSH. For ğœ= 10, we observe a\nnotable degradation in accuracy forProximity-FLAT: from 85.7%\nfor LRU with ğœ= 2.5to 77.4% for LRU with ğœ= 10. This same\ndegradation is much less pronounced forProximity-LSH: from\n85.8% forLSH-LRUwith ğœ= 2.5to 84.1% forLSH-LRUwith ğœ= 10.\nThese results show thatProximity-LSH is robust against higher\nvalues ofğœ.\n4.3.2 Recall.Figure 7(b) displays the k-recall for different similar-\nity tolerances, eviction policies, and with and without LSH, and\nwhen using theMedRAG-Zipfbenchmark. This recall indicates the\noverlap between document indices returned by the cache and the\ndocument indices that the databasewould have returnedin the ab-\nsence of caching. We observe that the k-recall is virtually 100% for\ntolerances below 7.5, showing that the cache is perfectly transpar-\nent in this regime: the retrieved documents by the cache are exactly\nthe same as the ones that the database would have selected. For\nğœ= 10, we observe a degradation in the k-recall, which translates to\na similar degradation in the end-to-end accuracy of the model (see\nFigure 7 (a)). This is consistent across all four cache configurations.\nWe note that this correlation between accuracy and k-recall can be\nleveraged to empirically fine-tune the tolerance hyperparameter ğœ\nat a low cost, as k-recall can be evaluated without LLM inference,\nwhich is compute-intensive. Once the optimal tolerance has been\nfound for k-recall, one can extrapolate that it is also optimal for\nend-to-end LLM accuracy.\n4.3.3 Cache hit rate.Figure 6(b) shows the cache hit rate for dif-\nferent values of ğ‘ and ğœ for both benchmarks and withProxim-\nity-FLAT. Increasingğœ increases the hit rate. For ğœ= 0, there are\nno cache hits, as queries need to be equal to any previous query.\nHowever, for ğœâ‰¥ 5, hit rates reach 93% for MMLU and 98.4% for\nMedRAG, demonstrating that higher tolerances allow the cache\nto serve most queries without contacting the database. In this sce-\nnario, the cache prefers to serve possibly irrelevant data rather than\ncontact the database. Nevertheless, even with such high hit rates,\nthere is only a minor decrease in accuracy for MMLU. Similarly, in\nMedRAG, despite the larger drop in accuracy, a hit rate of 72.6%\n(ğœ= 5,ğ‘= 200) sustains an accuracy close to the upper bound.\nIncreasing the cache capacity significantly improves the hit rate,\ni.e., for MMLU, ğœ= 2, and when increasing ğ‘from 10 to 300, the hit\nrate increase from 6.1% to 69.3%.\nFigure 7(c) further illustrates the cache hit rate as a function of\nthe similarity tolerance ğœ and the number of LSH hash bits ğ¿, for\nProximity-LSH and with the skewedMedRAG-Zipfworkload. For\nthis experiments we use the LRU eviction policy. We observe that\nhit rates increase with ğœ, confirming the trends seen Figure 6(b).\nImportantly, even with small hash sizes inProximity-LSH (e.g.,\n4â€“6 bits, corresponding to 16 and 64 buckets, respectively), hit rates\nremain high for tolerances above 5.0, validating the effectiveness\nof LSH in grouping semantically similar queries.\n4.3.4 Query latency.Figure 6(c) shows the retrieval latency for\ndifferent values ofğ‘and ğœ, forProximity-FLAT. Latency reductions\nare significant for configurations with high cache hit rates. Forğœ= 0\n(no cache) and ğ‘= 10, retrieval latency can be as high as101 ms for\nMMLU and4.8 sforMedRAG. Retrieval latency quickly decreases\nas ğœ increases, which aligns with the increase in hit rate; more\nqueries are now answered with results from the cache. Furthermore,\nincreasing ğ‘ also decreases retrieval latency, particularly for higher\nvalues of ğœ. Finally, we remark that the speedup gains byProximity\nincrease as the latency of vector database lookups increases. While\nour implementation keeps all vector indices in main memory, other\ndatabase implementations such asDiskANN(partially) store indices\non the disk, which further increases retrieval latency [20].\nFigure 7(d) shows the average retrieval latency forProximity-\nLSH with the LRU eviction policy. Latency drops sharply as the\nsimilarity tolerance increases and the cache hit rate improves. This\nMIDDLEWARE â€™25, December 15â€“19, 2025, Nashville, TN, USA Bergman et al.\n5 10 15 20 25 30\n50\n75\n100\n60.6\n72.6\n77.5 79.6 80.4 81.1\n85.9 86.0 85.9 85.9 85.9 85.9\nPer-bucket capacity (ğ‘)\n[%]\nHit rate Test accuracy\nFigure 8: The test accuracy and hit rate forProximity-LSH\nas a function of the flat bucket size.\nimprovement in average latency is again linearly increasing with\nthe hit rate, as cache hits dismiss database calls entirely and cache\nmisses are virtually zero-cost when compared to vector database\ncalls.\n4.3.5 Bucket size.We now study the impact of the per-bucket\ncapacity ğ‘ inProximity- LSH on the hit rate and test accuracy.\nIntuitively, larger bucket capacities increase the chance of finding a\nmatch among cached entries, thus improving the hit rate. However,\nthey also increase the number of entries to be scanned per lookup,\nresulting in higher per-query latency.\nWe evaluate this trade-off with theMedRAG-Zipfdataset, using\nProximity-LSH with ğ¿= 8hyperplanes, a tolerance parameter\nğœ= 7.5and the LRU eviction policy. Figure 8 shows the test accuracy\nand hit rate for different bucket sizes ğ‘. Our experimental results\nshow that ğ‘= 20offers the best balance between hit rate and\nlookup cost. Increasing ğ‘from 5 to 20 substantially improves the\nhit rate from 60.6% to 79.6%, while the accuracy remains stable at\napproximately 85.9%. For ğ‘> 20, the hit rate quickly reaches a\nplateau (81.1% at ğ‘= 30), while accuracy remains unchanged. Thus,\nlarger bucket capacities yield negligible benefits in terms of hit rate\nor accuracy, yet impose higher scanning overhead. We therefore\nfixğ‘=20in our experiments withProximity-LSH.\n4.4Proximity-LSH cache occupancy\nWe now determine the cache occupancy ofProximity-LSH, under\nvarying similarity tolerances ğœ and hashing granularity, expressed\nas the number of hash bits ğ¿, after theMedRAG-Zipfworkload\nhas completed. We note that in this experiment, the eviction policy\n(LRU or FIFO) is not relevant, as they both share the same memory\nfootprint. We show these results in Figure 9. Figure 9(a) shows\nrelativeusage, computed as the fraction of the theoretical maximum\ncapacity, while Figure 9(b) reports theabsolutenumber of vectors\nstored in the cache. The maximum capacity ofProximity-LSH is\n2ğ¿ Ã—20, where ğ¿is the number of hash bits and 20 is the per-bucket\nlimit.\nAcross all configurations, we observe a consistent trend: increas-\ning the number of hash bits ğ¿significantly reduces relative cache\nutilization. For instance, with ğ¿= 4(i.e., a total of 16 buckets),\nthe cache achieves over 92% utilization with ğœ= 2.5, filling 295 of\nthe 320 entries. In contrast, for ğ¿= 10(1024 buckets), only 19.1%\nof the cache is used at the same tolerance level, corresponding to\n3920occupied entries out of a maximum of20 480. This reflects the\n2.5 5 7.5 10\n4\n6\n8\n10\n92.2 85 83.4 83.4\n72.1 57.3 54.5 51\n43.1 32.8 30.9 27.4\n19.1 12.1 11 9.2\nSimilarity tolerance (ğœ)\nHash bits (ğ¿)\n(a) Entries used relative to full capacity [%]\n20\n40\n60\n80\n2.5 5 7.5 10\n4\n6\n8\n10\n0.3 0.3 0.3 0.3\n0.9 0.7 0.7 0.6\n2.2 1.6 1.5 1.4\n3.8 2.4 2.2 1.8\n(b) Number of cache lines used (Ã—103)\n1\n2\n3\nDarker is better\nFigure 9: The relative (a) and absolute (b) cache occupancy\nafter completion of theMedRAG-Zipfworkload when using\ntheProximity-LSH cache with a LRU eviction policy, for\nvarying LSH hashing granularities and similarity tolerances.\n20 200 2000 20 000 200 000\n102\n104\nNumber of entries in the cache (ğ‘›)\nLookup time[Âµs]\nLSH FLAT\nFigure 10: The cache lookup time as the number of cache en-\ntries increase, forProximity-FLAT (requiring a linear scan)\nandProximity-LSH. For both cache types we use the LRU\neviction policy.\nsparsity inherent to LSH-based caching: many buckets remain un-\nused unless queries are uniformly distributed across the embedding\nspace, which is not the case for our skewed workload.\nIncreasing the tolerance ğœ slightlyreducesboth absolute and rel-\native cache occupancy: for all configurations of ğ¿, cache occupancy\ndeclines as ğœ increases from 2.5 to 10.0. This is due to the increased\nhit rate at higher tolerances: more queries are matched to existing\nentries, thereby reducing the number of insertions required. For\nexample, at ğ¿= 8, usage decreases from2209entries (43.1%) at\nğœ=2.5to1402entries (27.4%) atğœ=10.0.\nWe argue that this adaptive sparsity is a desirable property of\nProximity-LSH. UnlikeProximity-FLAT that gradually fills up to\nits maximum size, LSH-based caching only allocates memory where\nneeded. In practical deployments with skewed or clustered query\ndistributions, this leads to significantly lower memory usage while\npreserving high cache effectiveness. Consequently,Proximity-LSH\ncan be provisioned with a large theoretical capacity for collision\navoidance, without incurring the cost of fully allocating all cache\nslots.\n4.5Proximityscalability\nFinally, we evaluate the scalability ofProximityby analyzing the\nlookup times for different cache occupancies, and when varying\ncache parameters. We also provide experimental insights on de-\nployingProximityon larger query sets by analyzing the recall and\nhit rate on theTripClickdataset.\nLeveraging Approximate Caching for Faster Retrieval-Augmented Generation MIDDLEWARE â€™25, December 15â€“19, 2025, Nashville, TN, USA\n2.5 5 7.5 10\n20\n50\n100\n200\n6.4 6.5 6.8 7.1\n8.7 8.8 8.8 9.7\n11.7 12.5 12.7 13.4\n18 18.9 19 19.2\nSimilarity tolerance (ğœ)\nCache capacity (ğ‘)\n(a) Latency LRU [ğœ‡ğ‘ ]\n2.5 5 7.5 10\n4\n6\n8\n10\n7 7.4 7.6 7.9\n7.3 8 8.4 9.3\n8.4 8.6 8.8 8.7\n8 7.9 7.9 7.8\nHash bits (ğ¿)\n(b) Latency LSH+LRU [ğœ‡ğ‘ ]\n10\n15\nDarker is better\nFigure 11: The cache lookup time ofMedRAG-Zipfqueries\nsent toProximity-FLAT (a) andProximity-LSH (b) for vary-\ning cache capacities and similarity tolerances.\n4.5.1 Lookup times for increasing cache occupancies.We now evalu-\nate the cache lookup times byProximity-FLAT andProximity-LSH\nby measuring the time spent per query as the cache size increases,\nunder the LRU eviction policy. We vary the number of entries that\nare stored in the cache when the query arrives, denoted by ğ‘›and\nvisualize the results in Figure 10. Figure 10 (dark blue bars) shows\nthe per-query latency of theProximity-FLAT implementation. The\nlookup time increases nearly linearly with the cache capacity ğ‘\n(this holds regardless of the similarity tolerance ğœ, as the amount\nof computation to be done is independent of ğœ). Specifically, this\nincrease is from2.0 Âµs for ğ‘›= 20to13.0 ms for ğ‘›= 200 000. This\nis consistent with the algorithmic complexity ofProximity-FLAT,\nwhich performs a full scan of the cache and computes the distance\nbetween the incoming query and every stored key. Even at rea-\nsonable cache sizes, this linear scan incurs an overhead that can\nbecome a noticeable proportion of the retrieval time budget. In\ncontrast, Figure 10 (light blue bars) displays the scaling capabil-\nities ofProximity-LSH in terms of the amount of vectors that\nare stored in the cache when the user query arrives at the cache.\nWe use ğ¿= 8hashing planes. The lookup time remains constant\nas ğ‘›increases: for all evaluated values of ğ‘›we observe a lookup\ntime of merely4.8 Âµs. This highlights the excellent scalability of the\nProximity-LSH cache.\n4.5.2 Lookup times for varying cache parameters.We runProx-\nimitywith theMedRAG-Zipfbenchmark and analyze the cache\nlookup times of queries sent to bothProximity-FLAT andProx-\nimity-LSH (ğ¿= 8), for varying cache capacities and similarity\ntolerances (Figure 11). Unlike the measurements in Figure 7(d),\nwhich capture the full end-to-end retrieval time including data-\nbase access, for this experiment we only consider the time spent\nperforming cache lookups. Figure 11(a) shows these results for\nProximity-FLAT and highlights that increasing ğ‘ while fixing ğœ\nincreases the cache lookup time. This is because there are more\ncache entries and thus more computational requirements to com-\nplete the lookup. A similar trend is visible when increasing ğœ when\nkeeping ğ‘ fixed. This is because for increasing values of ğœ we are\nleft with more candidates after the linear scan completes (line 2\nin Algorithm 1) and therefore require more time in determining\nthe best candidate (line 3 in Algorithm 1). Figure 11(b) shows the\nlatency ofProximity-LSH for varying tolerance and LSH hashing\ngranularity. Here, the lookup time remains relatively stable across\n1.0 1.5 2.0 2.5\n0\n50\n100\n49.4 50.2 51.4 52.9\n99.4 98.2 95.8 92.2\nSimilarity tolerance (ğœ)\n[%]\nHit rate Database recall\nFigure 12: Hit rate and database k-recall ofProximity-LSH\non theTripClickdataset and with the LRU eviction policy,\nwhile varying the similarity toleranceğœ.\nhashing granularity and similarity tolerances. The time per query\ndepends only on the per-bucket capacity, and not on the total num-\nber of cached entries. This confirms our theoretical expectation\nthat LSH-based cache lookups have constant-time complexity with\nrespect to both total cache size and cache tolerances.\n4.5.3 Large query dataset.To demonstrate the effectiveness of\nProximityon a large-scale query dataset, we useTripClickas\nthe query set (see Section 2.3),PubMedas the document database,\nandDiskANNas the vector database.TripClickcontains approxi-\nmately 5.2 million user queries and we send queries to the system in\nthe same order as they appear in the original dataset. We evaluate\nboth the cache hit rate and the database recall by comparing the\nvectors returned byProximity-LSH( ğ¿= 8) against those retrieved\ndirectly from the database for the same queries. Figure 12 shows\nhow the cache hit rate and database recall behave for different\nvalues of the similarity tolerance ğœ when using the LRU eviction\npolicy.Proximity-LSHmaintains a stable cache hit rate around\n50%across all similarity tolerance levels ğœ. For ğœ= 1.0and ğœ= 1.5,\nthe cache effectively reduces the number of retrieval calls by ap-\nproximately50%while preserving near-perfect k-recall. However,\nk-recall decreases asğœincreases,e.g., from 99.4% for ğœ= 1.0to 92.2%\nfor ğœ= 2.5. This is because a higher similarity tolerance permits\nthe cache to match a larger number of cache lines. These findings\nfurther highlight thatProximitycan achieve significant latency\nreductions while maintaining high retrieval accuracy even when\nusing workloads with a large number of queries and with real-world\nspatial and temporal locality.\n4.6 Experimental conclusion\nOur findings demonstrate thatProximityeffectively reduces re-\ntrieval latency while maintaining competitive accuracy. This makes\napproximate caching a viable optimization for RAG pipelines in\nscenarios where queries exhibit spatial and temporal similarity. In\npractical deployments, however, tuning the tolerance and cache\ncapacity hyperparameters based on workload characteristics will\nbe critical to balancing performance and accuracy.\n5 Related work\nImproving RAG latency.Various strategies have been proposed\nto decrease the retrieval latency of RAG. Zhu et al. propose Sparse\nRAG, an approach that encodes retrieved documents in parallel,\nthereby reducing delays associated with sequential processing [57].\nMIDDLEWARE â€™25, December 15â€“19, 2025, Nashville, TN, USA Bergman et al.\nSparse RAG reduces overhead of theLLM encoding stage.RAGServe\nis a system that dynamically adjusts parameters, such as the num-\nber of retrieved documents, for each query [ 42]. The system bal-\nances response quality and latency by jointly scheduling queries\nand adapting configurations based on individual query require-\nments.PipeRAGintegrates pipeline parallelism and flexible re-\ntrieval intervals to accelerate RAG systems through concurrent\nretrieval and generation processes [23].RAGCacheis a multilevel\ndynamic caching system that organizes intermediate states of re-\ntrieved knowledge into a hierarchical structure, caching them across\nGPU and host memory to reduce overhead [ 24].TurboRAGre-\nduces the latency of the prefill phase by caching and reusing LLM\nkey-value caches [31]. Cache-Augmented Generation is a method\nthat preloads all relevant documents into a language modelâ€™s ex-\ntended context and precomputes key-value caches, thus bypassing\nreal-time retrieval during inference [5]. Speculative RAG improves\naccuracy and reduces latency by using a smaller LLM to generate\nmultiple drafts in parallel from subsets of retrieved documents [54].\nThe above systems optimize different aspects of the RAG workflow\nand many of them are complementary toProximity.\nSimilarity caching.Beyond RAG, caching mechanisms have\nlong been studied to improve efficiency in information retrieval\nsystems [2]. In particular, similarity-based caching techniques aim\nto reduce retrieval latency and improve system throughput by ex-\nploiting patterns in query semantics and distribution [8, 39]. These\napproaches typically leverage the observation that similar queries\ntend to retrieve similar results, enabling cache reuse even in the\nabsence of exact query matches.\nSuch techniques have found applications across a variety of\ndomains, including approximate image retrieval [13], content distri-\nbution networks [36], and recommendation systems [47]. In these\ncontexts, approximate caching is often implemented using distance\nmetrics or clustering methods in embedding spaces to identify and\nreuse semantically close queries.\nHowever, to the best of our knowledge, these approaches have\nnot been applied in the context of question answering, where ap-\nproximate nearest neighbor search (NNS) is used not merely to\nretrieve similar content, but to support factual or task-oriented\nresponses via downstream LLM inference. This makes correctness\nand latency trade-offs more sensitive and requires a finer-grained\ncontrol over recall and relevance.\nIn this work, we extend approximate caching to this domain\nby introducingProximity, a system designed specifically for low-\nlatency, large-scale RAG. Our design incorporates key scalability\noptimizations, including locality-sensitive hashing to reduce lookup\ncomplexity and explicit SIMD instructions to accelerate distance\ncomputations. These architectural decisions enable our system to\noperate efficiently under realistic workloads and tight latency bud-\ngets, making it, to our knowledge, the first practical application of\napproximate similarity caching in RAG-based question answering\npipelines.\nAnswer caching.Finally, Regmi et al. [ 43] propose a semantic\ncaching strategy tailored for LLM applications, where responses\nto previous user queries are stored and reused when semantically\nsimilar queries are detected. In their system, incoming user queries\nare embedded into a high-dimensional space, and the cache returns\na previously generated LLM response if a sufficiently similar query\nhas already been seen. Their approach bypasses both document\nretrieval and answer generation entirely on cache hits, leading to\nsubstantial latency reductions.\nWhile effective in reducing end-to-end response time, this tech-\nnique assumes that semantically close queries always require identi-\ncal responses, which may not hold in practice. This is especially the\ncase when subtle distinctions between queries are relevant. In con-\ntrast,Proximityfocuses on optimizing the RAG document retrieval\nphase by caching references to retrieved documents rather than\nfinal answers. This allows theLLM to regenerate responses tailored\nto the precise query phrasing, preserving potentially important\nnuances that may be lost at the embedding level. As a result, our\nmethod provides greater flexibility and better preserves fidelity to\nthe userâ€™s intent, while still reducing the computational overhead\nof expensive nearest-neighbor lookups.\n6 Conclusion\nWe introducedProximity, a novel caching mechanism designed to\nenhance the efficiency of RAG systems. Our approach significantly\nreduces retrieval latency while maintaining retrieval accuracy by\nleveraging spatial and temporal similarities in user queries toLLMs.\nInstead of treating each query as an independent event,Proximity\ncaches results from previous queries and reuses them when sim-\nilar queries appear. This caching reduces the computational load\non the underlying vector database and decreases the end-to-end\nlatency of the overall RAG pipeline. We also improve the scalabil-\nity of query lookup times by designingProximity-LSH, a cache\nthat uses locality-sensitive hashing (LSH) to dramatically speed up\nthe process of determining similar queries in the cache. Our eval-\nuation with the MMLU andMedRAGbenchmarks demonstrates\nthat bothProximity-FLAT andProximity-LSH provide substan-\ntial performance gains in scenarios where users repeatedly query\nrelated topics. We conclude that our approximate caching strategy\neffectively optimizes RAG pipelines, particularly in workloads with\nsimilar query patterns.\nAcknowledgments\nThis work has been funded by the Swiss National Science Founda-\ntion, under the project â€œFRIDAY: Frugal, Privacy-Aware and Practi-\ncal Decentralized Learningâ€, SNSF grant number 10001796. We are\nalso grateful to MichaÅ‚ Stawarz for helpful discussions and for his\nrecommendations regarding relevant literature.\nReferences\n[1] Vaibhav Adlakha, Parishad BehnamGhader, Xing Han Lu, Nicholas Meade,\nand Siva Reddy. 2024. Evaluating Correctness and Faithfulness of Instruction-\nFollowing Models for Question Answering.Transactions of the Association for\nComputational Linguistics12 (05 2024). https://doi.org/10.1162/tacl_a_00667\n[2] Rafael Alonso, Daniel Barbara, and Hector Garcia-Molina. 1990. Data caching\nissues in an information retrieval system.ACM Transactions on Database Systems\n(TODS)15, 3 (1990). https://doi.org/10.1145/88636.87848\n[3] Ricardo Baeza-Yates, Aristides Gionis, Flavio Junqueira, Vanessa Murdock, Vas-\nsilis Plachouras, and Fabrizio Silvestri. 2007. The impact of caching on search\nengines. InProceedings of the 30th annual international ACM SIGIR conference on\nResearch and development in information retrieval(Amsterdam, The Netherlands)\n(SIGIR â€™07). https://doi.org/10.1145/1277741.1277775\n[4] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Ruther-\nford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bog-\ndan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving\nfrom trillions of tokens. InInternational conference on machine learning (ICML\nâ€™22). PMLR. https://proceedings.mlr.press/v162/borgeaud22a/borgeaud22a.pdf\nLeveraging Approximate Caching for Faster Retrieval-Augmented Generation MIDDLEWARE â€™25, December 15â€“19, 2025, Nashville, TN, USA\n[5] Brian J Chan, Chao-Ting Chen, Jui-Hung Cheng, and Hen-Hsen 25Huang. 2024.\nDonâ€™t Do RAG: When Cache-Augmented Generation is All You Need for Knowl-\nedge Tasks. (2024). arXiv:2412.15605\n[6] Moses S. Charikar. 2002. Similarity estimation techniques from rounding algo-\nrithms. InProceedings of the Thiry-Fourth Annual ACM Symposium on Theory\nof Computing(Montreal, Quebec, Canada)(STOC â€™02). https://doi.org/10.1145/\n509907.509965\n[7] Qi Chen, Bing Zhao, Haidong Wang, Mingqin Li, Chuanjie Liu, Zengzhong\nLi, Mao Yang, and Jingdong Wang. 2021. Spann: Highly-efficient billion-scale\napproximate nearest neighborhood search.Advances in Neural Information\nProcessing Systems34 (2021). https://www.microsoft.com/en-us/research/wp-\ncontent/uploads/2021/11/SPANN_finalversion1.pdf\n[8] Flavio Chierichetti, Ravi Kumar, and Sergei Vassilvitskii. 2009. Similarity caching.\nInProceedings of the Twenty-Eighth ACM SIGMOD-SIGACT-SIGART Symposium\non Principles of Database Systems(Providence, Rhode Island, USA)(PODS â€™09).\nhttps://doi.org/10.1145/1559795.1559815\n[9] Ranul Dayarathne, Uvini Ranaweera, and Upeksha Ganegoda. 2024. Comparing\nthe Performance of LLMs in RAG-Based Question-Answering: A Case Study in\nComputer Science Literature. InInternational Conference on Artificial Intelligence\nin Education Technology. Springer. https://doi.org/10.1007/978-981-97-9255-9_26\n[10] Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy,\nPierre-Emmanuel MazarÃ©, Maria Lomeli, Lucas Hosseini, and HervÃ© JÃ©gou. 2024.\nThe faiss library. (2024). arXiv:2401.08281\n[11] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad\nAl-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan,\net al. 2024. The llama 3 herd of models. (2024). arXiv:2407.21783\n[12] Fabrizio Falchi, Claudio Lucchese, Salvatore Orlando, Raffaele Perego, and Fausto\nRabitti. 2008. A metric cache for similarity search(LSDS-IR â€™08). https://doi.org/\n10.1145/1458469.1458473\n[13] Fabrizio Falchi, Claudio Lucchese, Salvatore Orlando, Raffaele Perego, and Fausto\nRabitti. 2012. Similarity caching in large-scale image retrieval.Information\nprocessing & management48, 5 (2012). https://doi.org/10.1016/j.ipm.2010.12.006\n[14] Ophir Frieder, Ida Mele, Cristina Ioana Muntean, Franco Maria Nardini, Raf-\nfaele Perego, and Nicola Tonellotto. 2024. Caching historical embeddings in\nconversational search.ACM Transactions on the Web18, 4 (2024). https:\n//doi.org/10.1145/3578519\n[15] In Gim, Guojun Chen, Seung-seob Lee, Nikhil Sarda, Anurag Khandelwal, and Lin\nZhong. 2024. Prompt cache: Modular attention reuse for low-latency inference.\nProceedings of Machine Learning and Systems6 (2024), 325â€“338. https://doi.org/\n10.48550/arXiv.2311.04934\n[16] Google. 2025. AI Overviews in Search. https://search.google/ways-to-search/ai-\noverviews Accessed: 2025-09-18.\n[17] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn\nSong, and Jacob Steinhardt. 2020. Measuring massive multitask language under-\nstanding. (2020). arXiv:2009.03300\n[18] Junhao Hu, Wenrui Huang, Weidong Wang, Haoyi Wang, Tiancheng Hu, Qin\nZhang, Hao Feng, Xusheng Chen, Yizhou Shan, and Tao Xie. 2024. EPIC: Effi-\ncient Position-Independent Caching for Serving Large Language Models. (2024).\narXiv:2410.15332\n[19] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian\nWang, Qianglong Chen, Weihua Peng, Xiaocheng Feng, Bing Qin, et al. 2025. A\nsurvey on hallucination in large language models: Principles, taxonomy, chal-\nlenges, and open questions.ACM Transactions on Information Systems43, 2 (2025).\nhttps://doi.org/10.1145/3703155 arXiv:2311.05232\n[20] Suhas Jayaram Subramanya, Fnu Devvrit, Harsha Vardhan Simhadri, Ravishankar\nKrishnawamy, and Rohan Kadekodi. 2019. Diskann: Fast accurate billion-point\nnearest neighbor search on a single node.Advances in Neural Information Pro-\ncessing Systems32 (2019). https://papers.nips.cc/paper_files/paper/2019/file/\n09853c7fb1d3f8ee67a61b6bf4a7f8e6-Paper.pdf\n[21] Ziwei Ji, Tiezheng Yu, Yan Xu, Nayeon Lee, Etsuko Ishii, and Pascale Fung.\n2023. Towards mitigating LLM hallucination via self reflection. InFindings of the\nAssociation for Computational Linguistics: EMNLP 2023. https://doi.org/10.18653/\nv1/2023.findings-emnlp.123 arXiv:2310.06271\n[22] Wenqi Jiang, Suvinay Subramanian, Cat Graves, Gustavo Alonso, Amir Yaz-\ndanbakhsh, and Vidushi Dadu. 2025. RAGO: Systematic Performance Opti-\nmization for Retrieval-Augmented Generation Serving. InProceedings of the\n52nd Annual International Symposium on Computer Architecture (ISCA â€™25). As-\nsociation for Computing Machinery, New York, NY, USA, 974â€“989. https:\n//doi.org/10.1145/3695053.3731093\n[23] Wenqi Jiang, Shuai Zhang, Boran Han, Jie Wang, Bernie Wang, and Tim Kraska.\n2024. Piperag: Fast retrieval-augmented generation via algorithm-system co-\ndesign. (2024). arXiv:2403.05676\n[24] Chao Jin, Zili Zhang, Xuanlin Jiang, Fangyue Liu, Xin Liu, Xuanzhe Liu, and Xin\nJin. 2024. RAGCache: Efficient Knowledge Caching for Retrieval-Augmented\nGeneration. (2024). arXiv:2404.12457\n[25] Qiao Jin, Won Kim, Qingyu Chen, Donald C Comeau, Lana Yeganova, W John\nWilbur, and Zhiyong Lu. 2023. Medcpt: Contrastive pre-trained transformers with\nlarge-scale pubmed search logs for zero-shot biomedical information retrieval.\nBioinformatics39, 11 (2023). https://doi.org/10.1093/bioinformatics/btad651\narXiv:2307.00589\n[26] Herve JÃ©gou, Matthijs Douze, and Cordelia Schmid. 2011. Product Quantization\nfor Nearest Neighbor Search.IEEE Transactions on Pattern Analysis and Machine\nIntelligence33, 1 (2011). https://doi.org/10.1109/TPAMI.2010.57\n[27] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess,\nRewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. (2020). arXiv:2001.08361\n[28] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey\nEdunov, Danqi Chen, and Wen-tau Yih. 2020. Dense Passage Retrieval for Open-\nDomain Question Answering. InProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP). https://doi.org/10.18653/v1/\n2020.emnlp-main.550\n[29] Yoonjoo Lee, Kihoon Son, Tae Soo Kim, Jisu Kim, John Joon Young Chung, Eytan\nAdar, and Juho Kim. 2024. One vs. Many: Comprehending Accurate Information\nfrom Multiple Erroneous and Inconsistent AI Generations. InThe 2024 ACM\nConference on Fairness, Accountability, and Transparency. https://doi.org/10.1145/\n3630106.3662681 arXiv:2405.05581\n[30] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim\nRocktÃ¤schel, et al . 2020. Retrieval-augmented generation for knowledge-\nintensive nlp tasks.Advances in Neural Information Processing Systems\n33 (2020). arXiv:2005.11401 https://proceedings.neurips.cc/paper/2020/file/\n6b493230205f780e1bc26945df7481e5-Paper.pdf\n[31] Songshuo Lu, Hua Wang, Yutian Rong, Zhi Chen, and Yaohua Tang. 2024.\nTurboRAG: Accelerating Retrieval-Augmented Generation with Precomputed\nKV Caches for Chunked Text. (2024). arXiv:2410.07590\n[32] Craig Macdonald and Nicola Tonellotto. 2021. On approximate nearest neigh-\nbour selection for multi-stage dense retrieval. InProceedings of the 30th ACM\nInternational Conference on Information & Knowledge Management. 3318â€“3322.\nhttps://doi.org/10.1145/3459637.3482156\n[33] Yu A Malkov and Dmitry A Yashunin. 2018. Efficient and robust approximate\nnearest neighbor search using hierarchical navigable small world graphs.IEEE\ntransactions on pattern analysis and machine intelligence42, 4 (2018). https:\n//doi.org/10.1109/TPAMI.2018.2889473 arXiv:1603.09320\n[34] Emma Meats, Jon Brassey, Carl Heneghan, and Paul Glasziou. 2007. Using\nthe Turning Research Into Practice (TRIP) database: how do clinicians really\nsearch?Journal of the Medical Library Association95, 2 (2007). pubmed:17443248\nhttps://pmc.ncbi.nlm.nih.gov/articles/PMC1852632/\n[35] Microsoft. 2024. Copilot Search. https://www.microsoft.com/en-us/bing/copilot-\nsearch Accessed: 2025-09-18.\n[36] Ryo Nakamura and Noriaki Kamiyama. 2024. Analysis of Similarity Caching on\nGeneral Cache Networks.IEEE Access(2024). https://doi.org/10.1109/ACCESS.\n2024.3489620\n[37] OpenAI. 2024. ChatGPT Search. https://openai.com/chatgpt/search Accessed:\n2025-09-18.\n[38] James Jie Pan, Jianguo Wang, and Guoliang Li. 2024. Survey of vector database\nmanagement systems.The VLDB Journal33, 5 (2024). https://doi.org/10.1007/\ns00778-024-00864-x arXiv:2310.14021\n[39] Sandeep Pandey, Andrei Broder, Flavio Chierichetti, Vanja Josifovski, Ravi Kumar,\nand Sergei Vassilvitskii. 2009. Nearest-neighbor caching for content-match\napplications. InProceedings of the 18th International Conference on World Wide\nWeb(Madrid, Spain)(WWW â€™09). https://doi.org/10.1145/1526709.1526769\n[40] Derrick Quinn, Mohammad Nouri, Neel Patel, John Salihu, Alireza Salemi,\nSukhan Lee, Hamed Zamani, and Mohammad Alian. 2025. Accelerating Retrieval-\nAugmented Generation. InProceedings of the 30th ACM International Conference on\nArchitectural Support for Programming Languages and Operating Systems, Volume 1\n(Rotterdam, Netherlands)(ASPLOS â€™25). https://doi.org/10.1145/3669940.3707264\n[41] Derrick Quinn, Mohammad Nouri, Neel Patel, John Salihu, Alireza Salemi, Sukhan\nLee, Hamed Zamani, and Mohammad Alian. 2025. Accelerating Retrieval-\nAugmented Generation. InProceedings of the 30th ACM International Confer-\nence on Architectural Support for Programming Languages and Operating Systems,\nVolume 1(Rotterdam, Netherlands)(ASPLOS â€™25). Association for Computing\nMachinery, New York, NY, USA, 15â€“32. https://doi.org/10.1145/3669940.3707264\n[42] Siddhant Ray, Rui Pan, Zhuohan Gu, Kuntai Du, Ganesh Ananthanarayanan, Ravi\nNetravali, and Junchen Jiang. 2024. METIS: Fast Quality-Aware RAG Systems\nwith Configuration Adaptation. (2024). arXiv:2412.10543\n[43] Sajal Regmi and Chetan Phakami Pun. 2024. GPT Semantic Cache: Reducing LLM\nCosts and Latency via Semantic Embedding Caching. (2024). arXiv:2411.05276\n[44] Sohini Roychowdhury. 2024. Journey of hallucination-minimized generative\nai solutions for financial decision makers. InProceedings of the 17th ACM In-\nternational Conference on Web Search and Data Mining. 1180â€“1181. https:\n//doi.org/10.1145/3616855.3635737\n[45] Anirudh Sabnis, Tareq Si Salem, Giovanni Neglia, Michele Garetto, Emilio\nLeonardi, and Ramesh K. Sitaraman. 2023. GRADES: Gradient Descent for Sim-\nilarity Caching.IEEE/ACM Transactions on Networking31, 1 (2023). https:\n//doi.org/10.1109/TNET.2022.3187044\nMIDDLEWARE â€™25, December 15â€“19, 2025, Nashville, TN, USA Bergman et al.\n[46] Tolga Åakar and Hakan Emekci. 2025. Maximizing RAG efficiency: A comparative\nanalysis of RAG methods.Natural Language Processing31, 1 (2025). https:\n//doi.org/10.1017/nlp.2024.53\n[47] Pavlos Sermpezis, Theodoros Giannakas, Thrasyvoulos Spyropoulos, and Luigi\nVigneri. 2018. Soft cache hits: Improving performance through recommendation\nand delivery of related content.IEEE Journal on Selected Areas in Communications\n36, 6 (2018). https://doi.org/10.1109/JSAC.2018.2844983\n[48] Michael Shen, Muhammad Umar, Kiwan Maeng, G Edward Suh, and Udit Gupta.\n2024. Towards Understanding Systems Trade-offs in Retrieval-Augmented Gen-\neration Model Inference. (2024). arXiv:2412.11854\n[49] Jonathon Shlens. 2014. A Tutorial on Principal Component Analysis.\narXiv:1404.1100\n[50] Amanda Spink, Dietmar Wolfram, Major BJ Jansen, and Tefko Saracevic. 2001.\nSearching the web: The public and their queries.Journal of the American society\nfor information science and technology52, 3 (2001). https://doi.org/10.1002/1097-\n4571(2000)9999:9999<::AID-ASI1591>3.0.CO;2-R\n[51] Vikranth Srivatsa, Zijian He, Reyna Abhyankar, Dongming Li, and Yiying Zhang.\n2024. Preble: Efficient distributed prompt scheduling for llm serving. (2024).\narXiv:2407.00023\n[52] Jaime Teevan, Eytan Adar, Rosie Jones, and Michael AS Potts. 2007. Information\nre-retrieval: Repeat queries in Yahooâ€™s logs. InProceedings of the 30th annual\ninternational ACM SIGIR conference on Research and development in information\nretrieval. https://doi.org/10.1145/1277741.1277770\n[53] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.\nJournal of machine learning research9, 11 (2008). https://www.jmlr.org/papers/\nvolume9/vandermaaten08a/vandermaaten08a.pdf\n[54] Zilong Wang, Zifeng Wang, Long Le, Huaixiu Steven Zheng, Swaroop Mishra,\nVincent Perot, Yuwei Zhang, Anush Mattapalli, Ankur Taly, Jingbo Shang, et al.\n2025. Speculative RAG: Enhancing retrieval augmented generation through\ndrafting. (2025). arXiv:2407.08223 https://openreview.net/pdf?id=xgQfWbV6Ey\n[55] Guangzhi Xiong, Qiao Jin, Zhiyong Lu, and Aidong Zhang. 2024. Benchmarking\nRetrieval-Augmented Generation for Medicine. InFindings of the Association for\nComputational Linguistics ACL 2024. https://doi.org/10.18653/v1/2024.findings-\nacl.372\n[56] Lexin Zhou, Wout Schellaert, Fernando MartÃ­nez-Plumed, Yael Moros-Daval,\nCÃ¨sar Ferri, and JosÃ© HernÃ¡ndez-Orallo. 2024. Larger and more instructable\nlanguage models become less reliable.Nature634, 8032 (2024). https://doi.org/\n10.1038/s41586-024-07930-y\n[57] Yun Zhu, Jia-Chen Gu, Caitlin Sikora, Ho Ko, Yinxiao Liu, Chu-Cheng Lin,\nLei Shu, Liangchen Luo, Lei Meng, Bang Liu, et al . 2024. Accelerating Infer-\nence of Retrieval-Augmented Generation via Sparse Context Selection. (2024).\narXiv:2405.16178"
}