{
    "title": "An open-source family of large encoder-decoder foundation models for chemistry",
    "url": "https://openalex.org/W4411857531",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5068590850",
            "name": "Eduardo Soares",
            "affiliations": [
                "IBM Research - Brazil"
            ]
        },
        {
            "id": "https://openalex.org/A5077314929",
            "name": "Emílio Vital Brazil",
            "affiliations": [
                "IBM Research - Brazil"
            ]
        },
        {
            "id": "https://openalex.org/A5092427104",
            "name": "Victor Yukio Shirasuna",
            "affiliations": [
                "IBM Research - Brazil"
            ]
        },
        {
            "id": "https://openalex.org/A5090730388",
            "name": "Dmitry Yu. Zubarev",
            "affiliations": [
                "IBM Research - Almaden"
            ]
        },
        {
            "id": "https://openalex.org/A5043792150",
            "name": "Renato Cerqueira",
            "affiliations": [
                "IBM Research - Brazil"
            ]
        },
        {
            "id": "https://openalex.org/A5085426538",
            "name": "Kristin Schmidt",
            "affiliations": [
                "IBM Research - Almaden"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4317757475",
        "https://openalex.org/W4391561379",
        "https://openalex.org/W3038823641",
        "https://openalex.org/W3113447514",
        "https://openalex.org/W3110901318",
        "https://openalex.org/W4281619372",
        "https://openalex.org/W4385490607",
        "https://openalex.org/W4316507319",
        "https://openalex.org/W4367049415",
        "https://openalex.org/W4398777692",
        "https://openalex.org/W4405599065",
        "https://openalex.org/W4313485929",
        "https://openalex.org/W6800751262",
        "https://openalex.org/W4323651349",
        "https://openalex.org/W4378942305",
        "https://openalex.org/W4297179162",
        "https://openalex.org/W4387023440",
        "https://openalex.org/W3005353977",
        "https://openalex.org/W4320857994",
        "https://openalex.org/W4213070269",
        "https://openalex.org/W4382318938",
        "https://openalex.org/W4307468223",
        "https://openalex.org/W3116865743",
        "https://openalex.org/W2594183968",
        "https://openalex.org/W2785942661",
        "https://openalex.org/W4283331738",
        "https://openalex.org/W4318350716",
        "https://openalex.org/W4396816591",
        "https://openalex.org/W4226145240",
        "https://openalex.org/W4400362104",
        "https://openalex.org/W1505823450",
        "https://openalex.org/W4200030022",
        "https://openalex.org/W2947423323",
        "https://openalex.org/W3155806510",
        "https://openalex.org/W4385569886",
        "https://openalex.org/W2581624817",
        "https://openalex.org/W6860710830",
        "https://openalex.org/W6968085758",
        "https://openalex.org/W3206711231",
        "https://openalex.org/W4213077304",
        "https://openalex.org/W3095883070",
        "https://openalex.org/W6849941170",
        "https://openalex.org/W4281553035",
        "https://openalex.org/W4392817999",
        "https://openalex.org/W2966357564",
        "https://openalex.org/W4310895557",
        "https://openalex.org/W6772452955",
        "https://openalex.org/W4214868967",
        "https://openalex.org/W3080997787",
        "https://openalex.org/W3103092523"
    ],
    "abstract": null,
    "full_text": "communicationschemistry Article\nA Nature Portfolio journal\nhttps://doi.org/10.1038/s42004-025-01585-0\nAn open-source family of large encoder-\ndecoder foundation models for chemistry\nCheck for updates\nEduardo Soares 1 , Emilio Vital Brazil1, Victor Shirasuna 1,D m i t r yZ u b a r e v2,R e n a t oC e r q u e i r a1 &\nKristin Schmidt2\nThe use of foundation models has extended from natural language processing to molecular modeling. In\nthis context, large-scale pre-training strategies have been applied to chemical language models to\nenable representation learning across diverse tasks. Here we introduce a family of encoder-decoder\nchemical foundation models pre-trained on a curated dataset of 91 million molecular sequences from\nPubChem. These models support a range of applications, including property estimation and reaction\noutcome prediction. We evaluate two model variants across several benchmark datasets and show that\nthey match or exceed existing approaches. We also assess the structure of the learned representations\nand ﬁnd that the embedding space supports few-shot learning and separates molecules based on\nchemically relevant features. This structure appears to result from the decoder-based reconstruction\nobjective used during pre-training. Theseﬁndings suggest that the proposed models can serve as\ngeneral-purpose tools for molecular analysis and reasoning with minimal supervision.\nUnderstanding molecular properties is crucial for accelerating discoveries in\ndifferentﬁelds, including drug development and materials science1.T r a d i -\ntional methods rely on labor-intensivetrial-and-error experiments, which\nare both costly and time-consuming2. However, recent advances in deep\nlearning have enabled the use of foundation models to predict molecular\nproperties and generate molecule candidates\n3–8, marking signiﬁcant pro-\ng r e s si ns c i e n t iﬁce x p l o r a t i o n .\nThe introduction of large-scale pre-training methodologies for che-\nmical language models (LMs) represents a signiﬁcant advancement in\ncheminformatics9–11. These methodologies have demonstrated impressive\nresults in challenging molecular tasks such as predicting properties and\ngenerating molecules\n12. The success of these models can be attributed to\ntheir ability to learn contextualized representations of input tokens through\nself-supervised learning on large unlabeled corpora13. This methodological\napproach typically involves two phases: pre-training on unlabeled data\nfollowed byﬁne-tuning on speciﬁc downstream task\n14.B yr e d u c i n gt h e\nreliance on annotated datasets, thisapproach has broadened our under-\nstanding of chemical language representations15.\nSimpliﬁed Molecular-Input Line Entry System, SMILES, provide\nnatural graphs that encode the connectivity information from the line\nannotations of molecular structures16.S M I L E Sd eﬁn e sac h a r a c t e rs t r i n g\nrepresentation of a molecule by performing a depth-ﬁrst pre-order spanning\ntree traversal of the molecular graph, generating symbols for each atom,\nbond, tree-traversal decision, and broken cycles17. Therefore, the resulting\ncharacter string corresponds to aﬂattening of a spanning tree of the\nm o l e c u l a rg r a p h .S M I L E Si sw i d e ly adopted for molecular property\nprediction as SMILES is generally more compact than other methods of\nrepresenting structure, including graphs18. There are billions of SMILES\navailable on different open-sources repositories19. However, most SMILES\nsequences do not belong to well-deﬁned molecules20. Alternative string-\nbased representations exist, such asSELFIES. However, focusing on mole-\ncular optimization tasks on the learned representation space, suggested no\nobvious shortcoming of SMILES with respect to SELFIES in terms of\noptimization ability and sample efﬁciency\n21. The quality of the pre-training\ndata plays a more important role on the outcome of the foundation\nmodel\n7,22.\nTowards this direction, we present anovel family of molecular encoder-\nd e c o d e rf o u n d a t i o nm o d e l s ,d e n o t e da sS M I - T E D289M.O u rS M I - T E D289M\nencoder-decoder foundation model was obtained using a transformer-based\nmolecular tokens encoder model a ligned with an encoder-decoder\nmechanism trained on a large corpus of91 million carefully curated mole-\ncules from PubChem23,S M I - T E D289M introduces novel pooling function that\ndiffers from standard max or mean pooling techniques, and allows SMILES\nreconstruction preserving its properties. Figure1 illustrates the general\narchitecture of SMI-TED289M. Our main contributions are:\n We pre-train a large-scale family of encoder-decoder molecular open-\nsource foundation models, denoted as SMI-TED289M,o no v e r9 1\nmillion molecules carefully curated from PubChem23.\n Our SMI-TED289M family of foundation models encompasses two\ndistinct conﬁgurations: base, which has289 million parameters; and\nthe Mixture-of-OSMI-Experts, MoE-OSMI, characterized by a composi-\ntion of 8 × 289M parameters.\n1IBM Research, Rio de Janeiro, Brazil.2IBM Research, Almaden, CA, USA. e-mail: eduardo.soares@ibm.com\nCommunications Chemistry|           (2025) 8:193 1\n1234567890():,;\n1234567890():,;\n We perform extensive experimentation on several classiﬁcation and\nregression tasks from 11 benchmark datasets, covering quantum\nmechanical, physical, biophysical, and physiological property predic-\ntion of small molecules. We also evaluate the reconstruction capacity of\nour SMI-TED\n289M considering the MOSES benchmarking dataset24.\nWe also conducted high-throughput experiments on Pd-catalyzed\nBuchwald–Hartwig C-N cross-coupling reactions, predicting reaction\nyields. Furthermore, a study investigating the embedding created by\nSMI-TED\n289M and few-shot learning is also provided, indicating\ncompositionality of the learned molecular representations. We also\nprovide a structure–property relationship study over the generated\nembedding space.\nOur results section demonstrates state-of-the-art performance of SMI-\nTED289M on different tasks, molecular properties prediction, molecule\nreconstruction, and an efﬁcient metric for molecular latent space. Com-\npositionality of the latent space suggests strong potential for chemical rea-\nsoning tasks. The SMI-TED289M family consists of two main variants\n(289M, and 8 × 289M), offeringﬂexibility and scalability for different sci-\nentiﬁc applications.\nBenchmarking overview\nTo assess the performance and generalization of SMI-TED, we designed a\nsuite of benchmarks that each target speciﬁch y p o t h e s e sr e g a r d i n go u r\nmodel’s capabilities. In this section, we introduce all the benchmarks in one\nplace, explain the rationale behind their selection, and describe what aspect\nof our model they are intended to validate.\nBenchmark selection and validation hypotheses\nO u re v a l u a t i o nf r a m e w o r ki sb u i l tu p o nf o u rk e yb e n c h m a r k s :\n MoleculeNet Datasets: We employ11 datasets from MoleculeNet25-6\nfor classiﬁcation tasks and 5 for regression tasks-to assess the model’s\npredictive accuracy and generalization across diverse molecular\nproperties. By using the same train/validation/test splits as the original\nbenchmark, we ensure consistency and unbiased evaluation.\n MOSES Benchmarking Dataset: To test the reconstruction and gen-\nerative abilities of SMI-TED, we use the MOSES dataset\n24.T h i s\nbenchmark, with its unique scaffold test set, validates the hypothesis\nthat our model can generate novel molecular scaffolds not seen during\ntraining.\n Pd-Catalyzed Buchwald–Hartwig Reaction Experiments: High-\nthroughput experiments on these reactions26 evaluate the model’s\ncapacity to predict reaction yields over a complex combinatorial space.\nThis benchmark conﬁrms the model’s effectiveness in handling high-\ndimensional experimental data.\n Latent Space Evaluation: By analyzing structure–property relationships-\nsuch as the electron-donating effectso fn i t r o g e ns u b s t i t u e n t so nH O M O\nenergy-we validate the ability of our model to generate a meaningful and\ninterpretable latent space that captures subtle chemical phenomena.\nResults\nExperiments\nTo evaluate the effectiveness of our proposed methodology, we conducted\nexperiments using a set of 11 datasets sourced from MoleculeNet\n25 as\ndetailed in the Supplementary Table 1. Speciﬁcally, we evaluated 6 datasets\nfor classiﬁcation task and 5 datasets for regression tasks. To ensure an\nunbiased assessment, we maintained consistency with the original bench-\nmark by adopting identical train/validation/test splits for all tasks25.W e\nconducted the experiments considering 10 different seeds in other to\nguarantee the robustness of the approach.\nTo assess the reconstruction/decoder capacity of SMI-TED289M we\nconsidered the MOSES benchmarking dataset24. The MOSES dataset con-\ntains 1,936,962 molecular structures.For experiments, we consider the split\nproposed by24, where the dataset was divided into a training, test and scaffold\ntest sets containing around 1.6M, 176k, and 176k molecules respectively.\nThe scaffold test set contains uniqueBemis-Murcko scaffolds that were not\npresent in the training and test sets. We use this set to assess how well the\nmodel can generate previously unobserved scaffolds.\nWe also conducted high-throughput experiments on Pd-catalyzed\nBuchwald–Hartwig C-N cross-coupling reactions, measuring the yields for\neach reaction as described in26. The experiments utilized three 1536-well\nplates, covering a matrix of 15 aryl and heteroaryl halides, four Buchwald\nligands, three bases, and 23 isoxazole additives, resulting in a total of 3955\nreactions. We employed the same data splits as in26 to assess our model’s\nperformance with training sets of varying sizes. An evaluation of the\nembedding space of SMI-TED289M is also provided, it uses the compositional\nmolecules to evaluate the capabilityof the model to generate metric latent\nspaces. An experiment over structure–property relationships in molecular\nlatent space represention is also conducted. In this analysis, we examined the\nelectron-donating effects of nitrogen substituents on HOMO energy, and the\nability of the generated latent space to capture such phenomena.\nComparison with SOTA on MoleculeNet benchmarking tasks\nResults for classiﬁcation tasks. The analysis investigates the com-\nparative efﬁcacy of SMI-TED289M in itsﬁne-tuned and pre-trained states\nversus state-of-the-art algorithms for molecular properties classiﬁcation,\nas demonstrated in Table1.\nTable 1 displays the performance of different advanced methods\non different benchmarking datasets used for molecule classiﬁcation tasks.\nSMI-TED289M consistently shows superior performance in four out of six\ndatasets. Interestingly, using SMI-TED289M with its initial settings provided\ncomparable results to SOTA methods available. However,ﬁne-tuning SMI-\nTED289M further enhances its performance across all datasets. This indicates\nFig. 1 | Overview of the SMI-TED289M model architecture.This ﬁgure illustrates the general architecture of the base SMI-TED289M model.\nhttps://doi.org/10.1038/s42004-025-01585-0 Article\nCommunications Chemistry|           (2025) 8:193 2\nSMI-TED289M’potential for accurate molecule classiﬁcation, with potential\nfor further optimization throughﬁne-tuning.\nResults for regression tasks. Next, we applied SMI-TED289M for pre-\ndiction of chemical properties. The performance results across ﬁve\nchallenging regression benchmarks, namely QM9, QM8, ESOL, FreeSolv,\nand Lipophilicity, are summarized in Table2.\nResults presented in Table2 indicates that SMI-TED289M presents\nsuperior results when compared to the state-of-the-art, outperforming its\ncompetitors in all the 5 datasets considered. Toﬁne-tune SMI-TED\n289M is\nimportant to achieve state-of-the-art results in regression datasets, due to the\ncomplexity of such tasks. Table2 elucidates the superiority of SMI-TED289M\nover the QM9 dataset. The QM9 dataset is composed by 12 tasks regarding\nto the quantum properties of molecules.A detailed overview over the results\nfor QM9 are depicted in the Supplementary Table 9.\nMixture-of-OSMI-Experts perform studies\nThis study compare the results of MoE-OSMI against single SMI-TED289M\nmodels (pre-trained andﬁne-tuned). MoE-OSMI is composed by 8 × 289M\nﬁne-tuned models for each speciﬁct a s k ,w es e tk = 2, which means that 2\nmodels are activated every step. The results for this study are detailed in the\nSupplementary Table 10.\nThe results from the study indicate that MoE-O\nSMI consistently\nachieves higher performance metrics compared to single SMI-TED289M\nmodels (Pre-trained and Fine-Tuned) models across different tasks, espe-\ncially in regression tasks where it improved results in all scenarios. These\nﬁndings suggest that the MoE approach effectively leverages specialized sub-\nmodels to capture diverse patterns in the data, leading to improved accuracy\nin molecular property predictions. The mixture-of-experts approach serves\nas an efﬁcient solution to scale single models and enhance performance for\nvarious tasks due to its ability to allocate speciﬁc tasks to different experts,\noptimizing single model’s overall predictive capabilities.\nDecoder evaluation over MOSES benchmarking dataset\nNext, we compared SMI-TED289M with different baseline models, such as\nthe character-level recurrent neural network (CharRNN)24,S M I L E Sv a r i a -\ntional autoencoder (VAE)24, junction tree VAE (JT-VAE)27, latent incep-\ntionism on molecules (LIMO)28,M o l G e n - 7 b29, and GP-MoLFormer30.A l l\nbaseline performances are reported on their corresponding test set con-\nsisting of 176k molecules. Standard metrics for evaluating model-generated\nmolecules are reported in the Supplementary Table 11.\nWhen compared to baselines, SMI-TED289M is equally performant in\ngenerating unique, valid, and novel molecules that share high cosine simi-\nlarity with the corresponding referen c em o l e c u l e sa tt h ef r a g m e n t( F r a g )\nTable 1 | Methods and performance for the classiﬁcation tasks of MoleculeNet benchmark datasets\nMethod Dataset\nBBBP ClinTox HIV BACE SIDER Tox21\nGraphMVP45 72.4 ± 1.6 79.1 ± 2.8 77.0 ± 1.2 81.2 ± 0.9 63.9 ± 1.2 75.9 ± 0.5\nGEM46 72.4 ± 0.4 90.1 ± 1.3 80.6 ± 0.9 85.6 ± 1.1 67.2 ± 0.4 78.1 ± 0.1\nGROVERLarge\n47 69.5 ± 0.1 76.2 ± 3.7 68.2 ± 1.1 81.0 ±1.4 65.4 ± 0.1 73.5 ± 0.1\nChemBerta48 64.3 90.6 62.2 –––\nChemBerta249 71.94 90.7 – 85.1 ––\nGalatica 30B50 59.6 82.2 75.9 72.7 61.3 68.5\nGalatica 120B50 66.1 82.6 74.5 61.7 63.2 68.9\nUni-Mol51 72.9 ± 0.6 91.9 ±1.8 80.8 ± 0.3 85.7 ± 0.2 65.9 ± 1.3 79.6 ± 0.5\nMolFM51 72.9 ± 0.1 79.7 ± 1.6 78.8 ± 1.1 83.9 ± 1.1 64.2 ± 0.9 77.2 ± 0.7\nMoLFormer52 73.6 ± 0.8 91.2 ± 1.4 80.5 ± 1.65 86.3 ± 0.6 65.5 ± 0.2 80.46 ± 0.2\nSMI-TED289M (Pre-trained) 91.46 ± 0.47 93.49 ± 0.85 80.51 ± 1.34 85.58 ± 0.92 66.01 ± 0.88 81.53 ± 0.45\nSMI-TED289M (Fine-tuned) 92.26 ± 0.57 94.27 ± 1.83 76.85 ± 0.89 88.24 ± 0.50 65.68 ± 0.45 81.85 ± 1.42\nThe best results for each task are presented in bold.\nTable 2 | Methods and performance for the regression tasks of MoleculeNet benchmark datasets\nMethod Dataset\nQM9 QM8 ESOL FreeSolv Lipophilicity\nD-MPNN (Fine-tuned)53 3.241 ± 0.119 0.0143 ± 0.0022 0.98 ± 0.26 2.18 ± 0.91 0.65 ± 0.05\nN-Gram (Fine-tuned)54 2.51 ± 0.19 0.0320 ± 0.003 1.074 ± 0.107 2.688 ± 0.085 0.812 ± 0.028\nPretrainGNN (Pre-trained)55 –– 1.100 ± 0.006 2.764 ± 0.002 0.739 ± 0.003\nGROVERLarge (Fine-tuned)47 –– 0.895 ± 0.017 2.272 ± 0.051 0.823 ± 0.010\nChemBERTa-2 (Pre-trained)49 –– 0.89 – 0.80\nSPMM (Fine-tuned)52 –– 0.818 ± 0.008 1.907 ± 0.058 0.692 ± 0.008\nMolCLRGIN (Fine-tuned)56 2.357 ± 0.118 0.0174 ± 0.0013 1.11 ± 0.01 2.20 ± 0.20 0.65 ± 0.08\nHu et al. (Fine-tuned)57 4.349 ± 0.061 0.0191 ± 0.0003 1.22 ± 0.02 2.83 ± 0.12 0.74 ± 0.00\nMoLFormer (Fine-tuned)12,52 1.5894 ± 0.0567 0.0102 0.880 ± 0.028 2.342 ± 0.052 0.700 ± 0.012\nSMI-TED289M (Pre-trained) 7.4883 ± 0.0659 0.0179 ± 0.0004 0.7045 ± 0.0344 1.668 ± 0.0616 0.6499 ± 0.012\nSMI-TED289M (Fine-tuned) 1.3246 ± 0.0157 0.0095 ± 0.0001 0.6112 ± 0.0096 1.2233 ± 0.0029 0.5522 ± 0.0194\nThe best results for each task are highlighted in bold to indicate top-performing models.\nhttps://doi.org/10.1038/s42004-025-01585-0 Article\nCommunications Chemistry|           (2025) 8:193 3\nlevel, consistent with low Fréchet Che m N e tD i s t a n c e( F C D ) .A tt h es a m e\ntime, SMI-TED289M generates molecules with high internal diversity\n(IntDiv), i.e., average pairwise dissimilarity. The scaffoldcosine similarity\n(Scaf) and similarity to the nearest neighbor in the test set (SNN) of SMI-\nTED289M is superior to the baselinesdemonstrating that SMI-TED289M is\neffective in generating molecules of varying structures and quality compared\nto baseline methods.\nReaction-yield prediction\nP r e v i o u s l y ,w ew e r ea b l et os h o wt h a tt h ep r o p o s e dS M I - T E D289M model\nwas able to perform compared to singletasks transformer-based methods.\nChemical reactions in organic chemistry are described by writing the\nstructural formula of reactants and products separated by an arrow,\nrepresenting the chemical transformation by specifying how the atoms\nrearrange between one or several reactant molecules and one or several\nproduct molecules. Machine learning methods are used to infer chemical\nreaction outcomes, such as yield, from data generated through high-\nthroughput screening experiments.\nWe assessed this architecture against state-of-the-art methods using a\nhigh-throughput dataset of Buchwald–Hartwig cross-coupling reactions,\nfocusing on predicting reaction yields\n26. This involves estimating the per-\ncentage of reactants converted into products. Our evaluation adhered to the\nschema and data divisions outlined in\n26.T a b l e3 presents the results for the\nSMI-TED289M model and compares its performance with existing state-of-\nthe-art approaches.\nThe results presented in Table3 demonstrate the superiority of the\nproposed SMI-TED289M foundation model when benchmarked against\nstate-of-the-art methods, including gradient-boosting andﬁngerprint-\nbased approaches (DRFP)31, a DFT-based random forest model (DFT)31,\nand transformer-basedmodels like Yield-BERT32 and its augmented variant,\nYield-BERT(aug.)32,a n dM S R 2 - R X N33. The performance of the Mamba-\nbased model can be attributed to its pre-training on an expansive dataset of\n91 million curated molecules, which provides a robust foundation of che-\nmical knowledge that signiﬁcantly enhances its predictive capabilities. This\npre-training enables the model to achieve high accuracyeven with limited\ntraining data, as evidenced by its sustained performance when trained on\njust 2.5% of the available samples-a scenario where task-speciﬁcm o d e l s\nexperience a marked decline in accuracy. To ensure the robustness of our\nmodel, we conducted each experiment with 10 different random seeds.\nLatent space study\nMolecular compositionality. We conducted an experiment to investi-\ngate the structure of the latent space created by Large Language Models in\nthe context of Chemistry. Molecular structures are composable from\nfragments, motifs, and functional groups. The composability of structure\noften translates into compositionality of structure–property relations,\nwhich is exempliﬁed by powerful group contribution methods in che-\nmical sciences. Compositionality of the learnt representation, however,\ndoes not follow automatically from the structure of the data and requires\nsome combination of the learning architecture and learning constraints\nto emerge. Our approach was to utilize simple chemical structures that\ncan be easily understood by humans, allowing us to anticipate relation-\nships between elements, and examine the latent space for similar patterns.\nWe constructed a dataset consisting of six families of carbon chains:\nF ¼f CC; CO; CN; CS; CF; CPg. For each family, we generated a\nsequence of molecules by incrementally adding carbon atoms to the end\nof the SMILES string, up to a maximum of ten carbon atoms. For\nexample, the familyCO consists of {CO, CCO, ⋯ , CCCCCCCCCCO}.\nAccording to the domain expert’s intuition consistent with the theory of\nchemical structure, in a metric space, such sequences should exhibit a\nhierarchical distance structure, where the distance between consecutive\nelements is smaller than the distance between elements with a larger\ndifference in carbon count, i.e.,j\nCnFi /C0 Cnþ1Fij < jCnFi /C0 Cnþ2Fij.\nHere, n represents the number of carbon atoms, andSMILES denotes the\nprojection of the SMILES string onto the embedding space.\nFirst, we generated the embeddings for two different encoders, the\nMoLFormer and SMI-TED289M, and used the t-SNE34 projection technique\nto generateﬁgures (Fig.2) for visually inspecting the spaces. It is worth\nnoting that the SMI-TED289M generated an embedding space that creates a\nnice separation of each family and respects the hierarchical distance struc-\nture, almost creating a linear relationship between each family. To quantify\nthis relationship, we created a dataset of triples of SMILES,\nT ¼f ð C\nnFCC; CkFi; CnþkFiÞj 0< n ≤ 4; 0< k ≤ 5g,f o rt h es i xf a m i l i e s\nFi, resulting in six sub-datasets with 20 elements each, e.g.,\n(CC, CCO, CCCCO) is one element of the subset of typeCO where\nn =1 ,k = 2. Then, we randomly selected one triple from each subset to feed a\nlinear regression calculating α, β,a n d B0 such that\nα /C1 CnFCC þ β /C1 CkFi þ B0 ¼ CnþkFi. We validated the linearity using\nthe remaining 114 elements. The linear regression on the MoLFormer\nembeddings resulted inR\n2 =0 . 5 5a n dMSE = 0.237, while on our model\nembeddings, it resulted inR2 =0 . 9 9a n dMSE = 0.002.\nWe evaluated our encoder-decoder model using a few-shot learning\nprocess, where we input a few examples of triples, such as those mentioned\nearlier, to calculateα, β,a n dB\n0.W et h e nu s et h e s ep a r a m e t e r st og e n e r a t e\nembeddings for subsequent SMILES pairs and recreate the SMILES strings.\nTo validate our approach, we tested the process on the same dataset of\nTable 3 | Performance of SMI-TED289M compared with the state of the art in reaction-yield prediction on experimentally\ndetermined yields of Buchwald– Hartwig reactions through HTEs\nSubset/Split DFT RF Yield-BERT Yield-BERT (Aug) DRFP YieldGNN MSR2-RXN SMI-TED 289M\nRand 70/30 0.92 0.95 ± 0.005 0.97 ± 0.003 0.95 ± 0.005 0.96 ± 0.005 0.94 ± 0.005 0.9841 ± 0.0007\nRand 50/50 0.9 0.92 ± 0.01 0.95 ± 0.01 0.93 ± 0.01 – 0.93 ± 0.01 0.982 ± 0.0004\nRand 30/70 0.85 0.88 ± 0.01 0.92 ± 0.01 0.89 ± 0.01 – 0.90 ± 0.01 0.979 ± 0.0013\nRand 20/80 0.81 0.86 ± 0.01 0.89 ± 0.01 0.87 ± 0.01 – 0.87 ± 0.01 0.976 ± 0.0006\nRand 10/90 0.77 0.79 ± 0.02 0.81 ± 0.02 0.81 ± 0.01 – 0.80 ± 0.02 0.961 ± 0.0023\nRand 5/95 0.68 0.61 ± 0.04 0.74 ± 0.03 0.73 ± 0.02 – 0.69 ± 0.03 0.912 ± 0.0043\nRand 2.5/97.5 0.59 0.45 ± 0.05 0.61 ± 0.04 0.62 ± 0.04 – 0.57 ± 0.05 0.875 ± 0.0044\nTest 1 0.8 0.84 ± 0.01 0.80 ± 0.01 0.81 ± 0.01 – 0.83 ± 0.03 0.9832 ± 0.0002\nTest 2 0.77 0.84 ± 0.03 0.88 ± 0.02 0.83 ± 0.003 – 0.83 ± 0.01 0.9820 ± 0.0005\nTest 3 0.64 0.75 ± 0.04 0.56 ± 0.08 0.71 ± 0.001 – 0.69 ± 0.04 0.9827 ± 0.0012\nTest 4 0.54 0.49 ± 0.05 0.43 ± 0.04 0.49 ± 0.004 – 0.51 ± 0.04 0.9825 ± 0.0008\nAverage 1– 4 0.69 0.73 0.58 ± 0.33 0.71 ± 0.16 – 0.72 ± 0.15 0.9826 ± 0.0005\nThe best results for each subset are highlighted in bold.\nhttps://doi.org/10.1038/s42004-025-01585-0 Article\nCommunications Chemistry|           (2025) 8:193 4\ntriples. We calculated the moleculesimilarity between the expected and\ngenerated results using the Tanimoto score (TS)35. We repeated this test with\ndifferent combinations of input triples, yielding similar results.\nFor example, when using the input triples [CC + CCCS = CCCCCS,\nCCCCC + CCCS = CCCCCCCCS] and querying all pairs in our subsets,\nwe obtained a mean TS of 0.52. It is interesting to note that the top two\nsimilar results were CC + CCCCCS = CCCCCS with TS = 0.92 and\nCC + CCCCCO = CCCCCO with TS = 0.92. For more detailed results see\nour Supplementary Information section.\nHistorically, group contribution was introduced in supervised learning\ncontext of structure–property relations. Our simple tests indicate that SMI-\nTED289M derived an equivalent of group contribution method purely from\nself-supervised learning of molecular structure. Signs of the emergence of\ncompositionality of the learned molecular representations suggest strong\npotential of SMI-TED\n289M for reasoning applications. Further studies\nconsistent with methodologies of compositionality analysis in natural lan-\nguages are required to make stronger statements.\nStructure–property relationship. Accurate representation of structure–\nproperty relationships in molecular data remains a signiﬁcant challenge\nin computational chemistry. In particular, it is important to assess\nwhether latent spaces generated by unsupervised pre-training can reﬂect\nchemical phenomena without subsequent task-speciﬁc ﬁne-tuning. To\naddress this issue, we compared two models: SMI-TED, an\nencoder–decoder architecture, and MoLFormer, an encoder-only model,\nusing the QM9 test dataset.\nOur analysis shows that nitrogen-containing molecules comprise\n9.10% of the overall dataset, yet they account for 32.81% of the top 10% of\nmolecules ranked by HOMO energy. This enrichment is consistent with\nclassical chemical theory and indicates that the latent space encodes the\nelectron-donating effects of nitrogen substituents. Figure3 presents t-SNE\nprojections of the latent spaces: in theSMI-TED model, nitrogen-containing\nc o m p o u n d sf o r mc l u s t e r si nr e g i ons associated with elevated HOMO\nvalues, whereas the latent space of MoLFormer displays a different orga-\nnization. Quantitative evaluation using the Davies–Bouldin index yields a\nvalue of 2.82 for SMI-TED compared to 4.28 for MoLFormer, indicating\nthat the SMI-TED latent space exhibits lower intra-cluster variance and\nhigher inter-cluster separation when partitioned by the presence of\nnitrogen-containing groups.\nAp o s s i b l ei n t e r p r e t a t i o ni st h a tt h ei n c l u s i o no fad e c o d e ri nS M I - T E D\nintroduces a reconstruction objective during pre-training, which enforces\nthe encoding of a more comprehensive set of structural features.\nThis additional constraint appearsto promote the formation of a latent\nspace that more effectively links molecular structure to properties such as\nFig. 2 | Comparison of small molecule embedding projections using t-SNE.The ﬁgure shows the t-SNE projection of 60 small molecule embeddings. Color distinguishes\nbetween families, and point size represents the number of carbon atoms in the chain.a MoLFormer embeddings;b SMI-TED289M embeddings.\nFig. 3 | Visualization of HOMO energy embeddings for the top 10% ranked\nmolecules. The ﬁgure shows the t-SNE projection of the top 10% of molecules\nranked by HOMO energy embeddings. Color distinguishes HOMO energy values,\nand the different markers represent the presence or absence of nitrogen atoms.\na MoLFormer embeddings;b SMI-TED289M embeddings.\nhttps://doi.org/10.1038/s42004-025-01585-0 Article\nCommunications Chemistry|           (2025) 8:193 5\nHOMO energy. In contrast, the encoder-only architecture of MoLFormer\nmay not impose the same constraints, resulting in a less organized latent\nrepresentation with respect tofunctional group clustering.\nIn summary, the pre-trained SMI-TED model captures relevant\nstructure–property relationships from SMILES data. The combination of\nquantitative metrics and t-SNE visualizations supports the conclusion that\nan encoder–decoder architecture, by incorporating a reconstruction pro-\ncess, yields a latent space with improved organization.\nDiscussion\nThis paper introduces the SMI-TED289M family of chemical foundation\nmodels, which are pre-trained on a curated dataset of 91 million SMILES\nsamples from PubChem. The SMI-TED\n289M family includes two conﬁg-\nurations: the base model with 289 million parameters and the MoE-OSMI\nmodel, which consists of 8 × 289M parameters.\nThe performance of these models was evaluated through an extensive\nexperimentation on different tasks, including molecular properties classi-\nﬁcation and prediction. Our approach achieved state-of-the-art results in\nmost tasks, particularly in predictingmolecular quantum mechanics, where\nit achieved the best or second-best results in 11 out of 12 tasks of the QM9\ndataset.\nOne key observation is the model’s robustness across various data splits\nfor reaction-yield prediction, particularly in low-resource settings where\nonly a small fraction of the dataset is used for training. This resilience\nunderscores the importance of leveraging large-scale pre-training to encode\ngeneralized chemical knowledge, which can then beﬁne-tuned for speciﬁc\ntasks like reaction yield prediction. In contrast, models that are tailored\nspeciﬁcally for a given task tend to overﬁt to the nuances of the training data\nand struggle to generalize when the training set size is reduced, highlighting\na critical limitation in their design.\nThe comparative analysis between the SMI-TED\n289M model and\nother state-of-the-art methods for reaction-yield prediction also sheds\nlight on the limitations of traditional approaches like DFT-based models,\nwhich, despite their theoretical grounding in quantum chemistry, may\nnot capture the full complexity of reaction mechanisms in practical\nscenarios. Similarly, while transformer-based models like Yield-BERT\nand its augmented variant exhibit strong performance, they fall short of\nthe SMI-TED\n289M model, particularly in low-data regimes, indicating\nthat the sheer scale and diversity of the pre-training data play a pivotal\nrole in achieving superior results.\nWe also investigated the structure of the latent space created by these\nlanguage-based foundation models, using simple chemical structures for\nclarity. SMI-TED\n289M generated an embedding space that creates a nice\nseparation of each family and respects the hierarchical distance structure,\nalmost creating a linear relationship between each family. The encoder-\ndecoder model’s capabilities in few-shot learning were assessed by gen-\nerating embeddings from a few example triples and using them to recreate\nSMILES strings, achieving a Tanimoto score of 0.92 in the best case.\nIn addition to this, our investigation of the latent space structure fur-\nther supports the model’s capability to capture relevant chemical infor-\nmation. Analysis of the QM9 dataset demonstrates that although nitrogen-\ncontaining molecules account for only 9.10% of the overall dataset, they\nconstitute 32.81% of the top decile when molecules are ranked by HOMO\nenergy. t-SNE projections indicate that, in the SMI-TED latent space,\nnitrogen-containing compounds cluster distinctly in regions associated with\nelevated HOMO values. Quantitatively, the Davies–Bouldin index is 2.82 for\nSMI-TED compared to 4.28 for MoLFormer, an encoder-only model,\nindicating that the latent space of SMI-TED exhibits lower intra-cluster\nvariability and greater inter-cluster separation based on the presence of\nnitrogen-containing groups.\nA plausible interpretation of theseﬁndings is that the inclusion of a\ndecoder in the SMI-TED architecture imposes a reconstruction objective\nduring pre-training, which enforcesthe encoding of a more comprehensive\nset of structural features. This additional constraint likely yields a latent\nrepresentation that more effectively correlates molecular structure with\nproperties such as HOMO energy. In contrast, an encoder-only architecture\nmay not capture these details as effectively, resulting in a less organized\nlatent space.\nCollectively, these results suggest that the pre-trained SMI-TED model\neffectively captures relevant structure–property relationships from SMILES\ndata. The integration of quantitative metrics and t-SNE visualizations\nsupports the hypothesis that an encoder-decoder architecture, through the\nincorporation of a reconstruction objective, produces a more organized\nlatent space. This enhanced organization has signiﬁcant implications for the\nd e v e l o p m e n to fc o m p u t a t i o n a lt o o l si nm o l e c u l a rd e s i g n ,a sm o d e l st h a t\ncombine both encoding and reconstruction processes may offer improved\nperformance in downstream tasks such as property prediction and clus-\ntering in chemical space.\nRecent developments in autonomous scienti ﬁc discovery have\ne m p h a s i z e dt h er o l eo ff o u n d a t i o nm o d e l sa sc o m p o n e n t si nm u l t i - a g e n t\nand graph-reasoning systems. In this context, the SMI-TED\n289M model has\nbeen integrated into the dZiner agent, a language model-based system for\ninverse molecular design\n36. dZiner generates candidate molecular structures\nconditioned on desired property proﬁles and employs SMI-TED to infer\nmolecular properties from SMILES representations in real time. The agent\nsupports both closed-loop and human-in-the-loop workﬂows, enabling\niterative screening under property and feasibility constraints.\nMethods\nThis section presents an overview of the proposed SMI-TED289M founda-\ntion model for small molecules. Here,we outline the process of collecting,\ncurating, and pre-processing the pre-train data. Additionally, we describe\nthe token encoder process and the SMILES encoder-decoder process.\nFinally, we explain the Mixture-of-O\nSMI-Experts approach used to scale the\nbase model.\nPre-training data\nThe pretraining data originated fromthe PubChem data repository, a public\ndatabase containing information on chemical substances and their biolo-\ngical activities23. Initially, 113 million SMILES strings were collected from\nPubChem. These molecular strings underwent deduplication and canoni-\ncalization processes to ensure uniqueness\n37. Subsequently, a molecular\ntransformation was conducted to verify the validity of the molecules derived\nfrom the unique SMILES strings, resulting in a set of 91 million unique and\nvalid molecules.\nTo construct the vocabulary, we employed the molecular tokenizer\nproposed by\n38. All 91 million molecules curated from PubChem were uti-\nlized in the tokenization process, resulting in a set of 4 billion molecular\ntokens. The unique tokens extracted from the resulting output provided a\nvocabulary of 2988 tokens plus 5 special tokens. In comparison, MoLFor-\nmer, trained on 1 billion samples with minimal curation, presented a\nvocabulary of 2362 tokens using the same tokenization process\n12.T h i s\nsuggests an improvement in the vocabulary model due to our curation\nprocess.\nTable 4 | SMI-TED289M base architecture speciﬁcity\nHidden size Attention heads Layers Dropout Normalization\n768 12 12 0.2 LayerNorm\nVocab size # SMILES # Mol tokens # Encoder # Decoder Total params\n2993 91M 4T 47M 242M 289M\nhttps://doi.org/10.1038/s42004-025-01585-0 Article\nCommunications Chemistry|           (2025) 8:193 6\nModel architecture\nWe conduct training for SMI-TED 289M model employing a deep-\nbidirectional-transformers-based encoder39 for tokens and an encoder-\ndecoder architecture to compose SMILES. The hyper-parameters of SMI-\nTED289M base model are detailed in Table4.\nTo optimize the relative encoding through position-dependent rota-\ntions Rm of the query and keys at positionm,t h eS M I - T E D289M uses a\nmodiﬁed version of the RoFormer40 attention mechanism. These rotations\ncan be implemented as pointwise multiplications and do not signiﬁcantly\nincrease computational complexity as shown in Eq. (1).\nAttentionmðQ; K; VÞ¼\nPN\nn¼1 φðRmqmÞ; φðRnknÞ\n/C10/C11\nvnPN\nn¼1 φðRmqmÞ; φðRnknÞ\n/C10/C11 ð1Þ\nwhere Q, K, V are the query, key, and value respectively, andφ is a random\nfeature map.\nWe start with a sequence of tokens extracted from SMILES, each\nembedded in a 768-dimensional space. The encoder-decoder layer is\ndesigned to process molecular token embeddings, represented as\nx 2 R\nD × L,w h e r eD denotes the maximum number of tokens andL\nrepresents the embedding space dimension. We limitedD at 202 tokens, as\n99.4% of molecules in the PubChem dataset contain fewer tokens than this\nthreshold.\nIn encoder-only models, a mean pooling layer is typically employed to\nrepresent tokens as SMILES in the latent space. However, this approach is\nlimited by the lack of a natural inversion process for the mean pooling\noperation. To overcome this limitation, we aim to construct a latent space\nrepresentation for SMILES by submersing thex in a latent space, denoted as\nz, as described in Eq. (2).\nz ¼ LayerNorm GELU xW\n1 þ b1\n/C0/C1/C0/C1/C0/C1\nW2; ð2Þ\nwhere z 2 RL, W1 2 RD × L, b1 2 RL, W2 2 RL × L,w i t hL denoting the\nl a t e n ts p a c es i z e( s p e c iﬁcally, L = 768) andD representing the original\nfeature space size (namely,D = 202). Subsequently, we can immersez back\nby calculating Eq. (3).\n^x ¼ LayerNorm GELU zW3 þ b3\n/C0/C1/C0/C1/C0/C1\nW4 ð3Þ\nwhere ^x 2 RD × L, W3 2 RL × L, b3 2 RL, W4 2 RL × D.\nA language layer (decoder) is used to process^x, where it applies non-\nlinearity and normalization, and projects the resulting vector into a set of\nlogits over the vocabulary, which can then be used to predict the next token\nin the molecular\n41.\nPre-training strategies\nPre-training of SMI-TED289M was performed for 40 epochs through the\nentire curated PubChem dataset with aﬁxed learning rate of 1.6e−4a n da\nbatch size of 288 molecules on a total of 24 NVIDIA V100 (16G) GPUs\nparallelized into 4 nodes using DDP andtorch run.I ti n v o l v e st w od i s t i n c t\nphases: i) Learning of token embeddings through a masking process; ii)\nSubsequently, the token embeddings are mapped into a common latent\nspace that encapsulates the entire SMILES string. This latent space not only\nfacilitates the representation of the SMILES but also enables the\nreconstruction of both individual tokens and complete SMILES strings.\nConsequently, the pre-training process involves two separate loss functions:\none for the token embeddings, which is based on the masking process, and\nanother for the encoder-decoder layer, which focuses on the reconstruction\nof tokens. Two pre-training strategies are employed:\n In phase 1, the token encoder is initially pre-trained using 95% of the\navailable samples, while the remaining 5% is reserved for training the\nencoder-decoder layer. This partitioning is necessary as the token\nembeddings may encounter convergence difﬁculties in the initial\nepochs, which could adversely affect the training of the encoder-\ndecoder layer.\n In phase 2, once the token embeddings layer has achieved convergence,\nthe pre-training process is expanded to utilize 100% of the available\nsamples for both phases. This approach leads to an enhancement in the\nperformance of the encoder-decoder layer, particularly in terms of\ntoken reconstruction.\nFor encoder pre-training we use the masked language model method\ndeﬁned in\n39. Initially 15% of the tokens are selected for possible learning.\nFrom that selection, 80% of the tokens are randomly selected and replaced\nwith the [MASK] token, 10% of the tokens are randomly selected to be\nreplaced with a random token, while the remaining 10% of the tokens will be\nunchanged.\nThe adoption of different pre-training strategies has proven instru-\nmental in enhancing the efﬁciency of our model, as evidenced by\nimprovements observed in the loss functions.\nMixture-of-OSMI-Experts\nThe Mixture-of-OSMI-Experts, MoE-OSMI comprises a set ofn “expert\nnetworks” labeled asE1, E2, … , En, augmented through a gating network\ndenoted asG, tasked with generating a sparsen-dimensional embedding\nspace optimized for a downstream task as illustrated by Fig.4.\nHere, we map each SMILES into tokens and then convert the input\ntokens to the latent space. A mean pooling method is applied to all token\nembeddings in order to produce a meaningful embedding of the molecule.\nThe architecture is equipped with a router module responsible for deter-\nmining then experts that will be activated, reﬁning the adaptability and\nspecialization of the system. LetG(x)a n dE\nið^xÞ denote the output of the\ngating network and the output of thei-th expert network, respectively, for a\ngiven input^x of SMILES andx, which is the embeddings space, following a\ns i m i l a rn o t a t i o na sp r o p o s e di nr e f .42. The resulting outputy is deﬁned as\nEq. (4):\ny ¼\nXn\ni¼1\nGðxÞiEið^xÞ ð4Þ\nThe resulting embedding spacey is used to train a task-speciﬁc feed-forward\nnetwork, where the loss function is chosen according to the studied\ndownstream task. The optimization process reﬁnes the parameters ofG(x).\nI ft h eg a t i n gv e c t o ri ss p a r s e ,w ec a nu se softmax over the Top-K logits of a\nlinear layer\n42 deﬁned as Eq. (5).\nGðxÞ :¼ SoftmaxðTopKðx /C1 WgÞÞ ð 5Þ\nFig. 4 | Architecture of the Mixture-of-OSMI-Experts model.Mixture-of-OSMI-Experts for downstream tasks.\nhttps://doi.org/10.1038/s42004-025-01585-0 Article\nCommunications Chemistry|           (2025) 8:193 7\nwhere (TopK(ℓ))i ≔ℓi if ℓi is among theTopK coordinates of logits‘ 2 Rn\nand (TopK(ℓ))i ≔∞otherwise. The router layer retains only the topk values,\nsetting the remaining values to− ∞ (which effectively assigns corre-\nsponding gate values as 0). This sparsity-inducing step serves to optimize\ncomputational efﬁciency43. Here, we deﬁne MoE-OSMI as n =8a n dk =2 ,\nwhich means that MoE-OSMI is composed by 8 × SMI-TED289M models,\nwhich 2 models are activated through the router each round.\nData availability\nThe datasets used and/or analyzed during the current study available from\nthe corresponding author on reasonabl er e q u e s t .T h es o u r c ec o d ei sa v a i l -\nable at: https://github.com/IBM/materials/tree/main/models/smi_tedand\nat https://doi.org/10.5281/zenodo.15603701\n44.\nCode availability\nT h eP y t h o nc o d ef o rt r a i n i n ga n dﬁne-tuning SMI-TED289M,a l o n gw i t h\nscripts for MoE-OSMI, and Jupyter notebooks for using the pre-trained\nSMI-TED289M models, can be accessed at the following repository:https://\ngithub.com/IBM/materials/tree/main/models/smi_ted.P r e - t r a i n e dm o d e l\nweights are hosted on Hugging Face and available at:https://huggingface.co/\nibm/materials.smi-ted. Scripts are also available on Zenodo athttps://doi.\norg/10.5281/zenodo.1560370144.\nReceived: 7 November 2024; Accepted: 11 June 2025;\nReferences\n1. Pan, J. Large language model for molecular chemistry.Nat.\nComputational Sci.3,5 – 5 (2023).\n2. Jablonka, K. M., Schwaller, P., Ortega-Guerrero, A. & Smit, B.\nLeveraging large language models for predictive chemistry.Nat.\nMach. Intell.6, 161– 169 (2024).\n3. Shen, J. & Nicolaou, C. A. Molecular property prediction: recent trends\nin the era of artiﬁcial intelligence.Drug Discov. Today. Technol.32,\n29– 36 (2019).\n4. Walters, W. P. & Barzilay, R. Applications of deep learning in molecule\ngeneration and molecular property prediction.Acc. Chem. Res.54,\n263– 270 (2020).\n5. Wieder, O. et al. A compact review of molecular property prediction\nwith graph neural networks.Drug Discov. Today. Technol.37,1 – 12\n(2020).\n6. Flam-Shepherd, D., Zhu, K. & Aspuru-Guzik, A. Language models can\nlearn complex molecular distributions.Nat. Commun.13, 3293 (2022).\n7. Wang, H. et al. Scientiﬁc discovery in the age of artiﬁcial intelligence.\nNature 620,4 7– 60 (2023).\n8. Wen, M. et al. Chemical reaction networks and opportunities for\nmachine learning.Nat. Computational Sci.3,1 2– 24 (2023).\n9. Sadybekov, A. V. & Katritch, V. Computational approaches\nstreamlining drug discovery.Nature 616, 673– 685 (2023).\n10. Buehler, E. L. & Buehler, M. J. X-lora: Mixture of low-rank adapter\nexperts, aﬂexible framework for large language models with\napplications in protein mechanics and molecular design.APL Mach.\nLearn. 2, 026119 (2024).\n11. Ghafarollahi, A. & Buehler, M. J. Sciagents: Automating scientiﬁc\ndiscovery through bioinspired multi-agent intelligent graph reasoning.\nAdv. Mater.37, 2413523 (2024).\n12. Ross, J. et al. Large-scale chemical language representations capture\nmolecular structure and properties.Nat. Mach. Intell.4, 1256– 1264\n(2022).\n13. Bommasani, R. et al. On the opportunities and risks of foundation\nmodels. Preprint athttps://arxiv.org/abs/2108.07258 (2021).\n14. Yang, S. et al. Foundation models for decision making: problems,\nmethods, and opportunities. Preprint athttps://arxiv.org/abs/2303.\n04129\n(2023).\n15. Guo, T. et al. What can large language models do in chemistry? a\ncomprehensive benchmark on eight tasks.Adv. Neural Inf. Process.\nSyst. 36, 59662– 59688 (2023).\n16. Li, Z., Jiang, M., Wang, S. & Zhang, S. Deep learning methods for\nmolecular representation and property prediction.Drug Discov. Today\n27, 103373 (2022).\n17. Wei, L., Fu, N., Song, Y., Wang, Q. & Hu, J. Probabilistic generative\ntransformer language models for generative design of molecules.J.\nCheminformatics 15, 88 (2023).\n18. Öztürk, H., Özgür, A., Schwaller, P., Laino, T. & Ozkirimli, E. Exploring\nchemical space using natural language processing methodologies for\ndrug discovery.Drug Discov. Today25, 689– 705 (2020).\n19. Tingle, B. I. et al. Zinc 22 a free multi-billion-scale database of tangible\ncompounds for ligand discovery.J. Chem. Inf. Model.63, 1166– 1176\n(2023).\n20. Wigh, D. S., Goodman, J. M. & Lapkin, A. A. A review of molecular\nrepresentation in the age of machine learning.Wiley Interdiscip. Rev.\nComputational Mol. Sci.12, e1603 (2022).\n21. Gao, W., Fu, T., Sun, J. & Coley, C. Sample efﬁciency matters: a\nbenchmark for practical molecular optimization.Adv. Neural Inf.\nProcess. Syst.35, 21342– 21357 (2022).\n2 2 . T a k e d a ,S . ,K i s h i m o t o ,A . ,H a m a d a ,L . ,N a k a n o ,D .&S m i t h ,J .R .\nFoundation model formaterial science. InProceedings of the AAAI\nConference on Artiﬁcial Intelligence, vol. 37, 15376– 15383 (AAAI, 2023).\n23. Kim, S. et al. Pubchem 2023 update.Nucleic Acids Res.51,\nD1373– D1380 (2023).\n24. Polykovskiy, D. et al. Molecular sets (moses): a benchmarking\nplatform for molecular generation models.Front. Pharmacol.11,\n565644 (2020).\n25. Wu, Z. et al. Moleculenet: a benchmark for molecular machine\nlearning. Chem. Sci.9, 513– 530 (2018).\n26. Ahneman, D. T., Estrada, J. G., Lin, S., Dreher, S. D. & Doyle, A. G.\nPredicting reaction performance in c– n cross-coupling using machine\nlearning. Science 360, 186– 190 (2018).\n27. Jin, W., Barzilay, R. & Jaakkola, T. Junction tree variational\nautoencoder for molecular graph generation. InInternational\nconference on machine learning, 2323– 2332 (PMLR, 2018).\n28. Eckmann, P. et al. Limo: Latent inceptionism for targeted molecule\ngeneration. Proc. Mach. Learn. Res.\n162, 5777 (2022).\n29. Fang, Y. et al. Domain-agnostic molecular generation with self-\nfeedback. Preprint athttps://arxiv.org/abs/2301.11259 (2023).\n30. Ross, J. et al. Gp-molformer: a foundation model for molecular\ngeneration. Preprint athttps://arxiv.org/abs/2405.04912 (2024).\n31. Probst, D., Schwaller, P. & Reymond, J.-L. Reaction classiﬁcation and\nyield prediction using the differential reactionﬁngerprint drfp.Digital\nDiscov. 1,9 1– 97 (2022).\n32. Schwaller, P., Vaucher, A. C., Laino, T. & Reymond, J.-L. Prediction of\nchemical reaction yields using deep learning.Mach. Learn.: Sci.\nTechnol. 2, 015016 (2021).\n33. Boulougouri, M., Vandergheynst, P. & Probst, D. Molecular set\nrepresentation learning.Nat. Mach. Intell.6, 754– 763 (2024).\n34. van der Maaten, L. & Hinton, G. Visualizing high-dimensional data\nusing t-sne.J. Mach. Learn. Res.9, 2579– 2605 (2008).\n35. Lipkus, A. H. A proof of the triangle inequality for the tanimoto\ndistance. J. Math. Chem.26, 263– 265 (1999).\n36. Ansari, M., Watchorn, J., Brown, C. E. & Brown, J. S. dziner: rational\ninverse design of materials with ai agents. Preprint athttps://arxiv.org/\nabs/2410.03963 (2024).\n37. Heid, E., Liu, J., Aude, A. & Green, W. H. Inﬂuence of template size,\ncanonicalization, and exclusivity for retrosynthesis and reaction\nprediction applications.J. Chem. Inf. Model.62,1 6– 26 (2021).\n38. Schwaller, P. et al. Molecular transformer: a model for uncertainty-\ncalibrated chemical reaction prediction.ACS Cent. Sci.5, 1572– 1583\n(2019).\nhttps://doi.org/10.1038/s42004-025-01585-0 Article\nCommunications Chemistry|           (2025) 8:193 8\n39. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. InNorth\nAmerican chapter of the association for computational linguistics,\nhttps://api.semanticscholar.org/CorpusID:52967399 (2019).\n40. Su, J. et al. Roformer: enhanced transformer with rotary position\nembedding. Preprint athttps://arxiv.org/abs/2104.09864 (2021).\n41. Ferrando, J., Gállego, G. I., Tsiamas, I. & Costa-jussà, M. R. Explaining\nhow transformers use context to build predictions. Preprint athttps://\narxiv.org/abs/2305.12535 (2023).\n42. Shazeer, N. et al. Outrageously large neural networks: the sparsely-\ngated mixture-of-experts layer. Preprint athttps://arxiv.org/abs/1701.\n06538 (2017).\n43. Jiang, A. Q. et al. Mixtral of experts. Preprint athttps://arxiv.org/abs/\n2401.04088 (2024).\n44. Almeida Soares, E. SMI-TED: an open-source family of large encoder-\ndecoder foundation models for chemistryhttps://doi.org/10.5281/\nzenodo.15603701 (2025).\n45. Liu, S. et al. Pre-training molecular graph representation with 3d\ngeometry. Preprint athttps://arxiv.org/abs/2110.07728 (2021).\n46. Fang, X. et al. Geometry-enhanced molecular representation learning\nfor property prediction.Nat. Mach. Intell.4, 127– 134 (2022).\n47. Rong, Y. et al. Self-supervised graph transformer on large-scale\nmolecular data.Adv. Neural Inf. Process. Syst.33, 12559– 12571\n(2020).\n48. Chithrananda, S., Grand, G. & Ramsundar, B. Chemberta: large-scale\nself-supervised pretraining for molecular property prediction. Preprint\nat https://arxiv.org/abs/2010.09885 (2020).\n49. Ahmad, W., Simon, E., Chithrananda, S., Grand, G. & Ramsundar, B.\nChemberta-2: Towards chemical foundation models. Preprint at\nhttps://arxiv.org/abs/2209.01712 (2022).\n50. Taylor, R. et al. Galactica: a large language model for science. Preprint\nat https://arxiv.org/abs/2211.09085 (2022).\n51. Zhou, G. et al. Uni-mol: a universal 3d molecular representation\nlearning framework. Preprint athttps://chemrxiv.org/engage/\nchemrxiv/article-details/628e5b4d5d948517f5ce6d72 (2023).\n52. Chang, J. & Ye, J. C. Bidirectional generation of structure and\nproperties through a single molecular foundation model.Nat.\nCommun. 15, 2323 (2024).\n53. Yang, K. et al. Analyzing learned molecular representations for\nproperty prediction.J. Chem. Inf. Model.59, 3370– 3388 (2019).\n54. Liu, S., Demirel, M. F. & Liang, Y. N-gram graph: simple unsupervised\nrepresentation for graphs, with applications to molecules. In\nAdvances in neural information processing systems, 32 (NeurIPS,\n2019).\n55. Hu, W. et al. Strategies for pre-training graph neural networks. Preprint\nat https://arxiv.org/abs/1905.12265 (2019).\n56. Wang, Y., Wang, J., Cao, Z. & Barati Farimani, A. Molecular\ncontrastive learning of representations via graph neural networks.\nNat. Mach. Intell.4, 279– 287 (2022).\n57. Hu, Z., Dong, Y., Wang, K., Chang, K.-W. & Sun, Y. Gpt-gnn:\ngenerative pre-training of graph neural networks. InProceedings of\nthe 26th ACM SIGKDD international conference on knowledge\ndiscovery & data mining, 1857– 1867 (ACM, 2020).\nAuthor contributions\nE.S., E.V.B., and D.Z. conceived the computational experiments. E.S. and\nV.S. carried out the experiments, while E.S., E.V.B., and D.Z. analyzed the\nresults. E.V.B., R.C., and K.S. designed and supervised the project. All\nauthors contributed to and reviewed the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s42004-025-01585-0\n.\nCorrespondenceand requests for materials should be addressed to\nEduardo Soares.\nPeer review informationCommunications Chemistrythanks Benoit Da\nMota and the other, anonymous, reviewers for their contribution to the peer\nreview of this work.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to\njurisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s42004-025-01585-0 Article\nCommunications Chemistry|           (2025) 8:193 9"
}