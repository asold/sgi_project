{
  "title": "Can Language Models Serve as Temporal Knowledge Bases?",
  "url": "https://openalex.org/W4385573837",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2144925762",
      "name": "Ruilin Zhao",
      "affiliations": [
        "Huazhong University of Science and Technology",
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A1233410411",
      "name": "Feng Zhao",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2482500164",
      "name": "Guandong Xu",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2179233707",
      "name": "Sixiao Zhang",
      "affiliations": [
        "University of Technology Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2095833031",
      "name": "Hai Jin",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W3176757281",
    "https://openalex.org/W4212964822",
    "https://openalex.org/W4205450747",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W3102844651",
    "https://openalex.org/W3153094109",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4287747814",
    "https://openalex.org/W3176793246",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4309416490",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4287111051",
    "https://openalex.org/W3114916066",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2947337775",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4285239949",
    "https://openalex.org/W4301194718",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3044438666",
    "https://openalex.org/W4206118214",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W3146844750",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2988237903",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W3107969673",
    "https://openalex.org/W2912924812"
  ],
  "abstract": "Recent progress regarding the use of language models (LMs) as knowledge bases (KBs) has shown that language models can act as structured knowledge bases for storing relational facts. However, most existing works only considered the LM-as-KB paradigm in a static setting, which ignores the analysis of temporal dynamics of world knowledge. Furthermore, a basic function of KBs, i.e., the ability to store conflicting information (i.e., 1-N, N-1, and N-M relations), is underexplored. In this paper, we formulate two practical requirements for treating LMs as temporal KBs: (i) The capacity to store temporally-scoped knowledge that contains conflicting information and (ii) the ability to use stored knowledge for temporally-scoped knowledge queries. We introduce a new dataset called LAMA-TK which is aimed at probing temporally-scoped knowledge, and investigate the two above requirements to explore the LM-as-KB paradigm in the temporal domain. On the one hand, experiments show that LMs can memorize millions of temporally-scoped facts with relatively high accuracy and transfer stored knowledge to temporal knowledge queries, thereby expanding the LM-as-KB paradigm to the temporal domain. On the other hand, we show that memorizing conflicting information, which has been neglected by previous works, is still challenging for LMs and hinders the memorization of other unrelated one-to-one relationships.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2024–2037\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nCan Language Models Serve as Temporal Knowledge Bases?\nRuilin Zhao 1,2, Feng Zhao 1∗, Guandong Xu 2, Sixiao Zhang 2, Hai Jin 1\n1National Engineering Research Center for Big Data Technology and System,\nServices Computing Technology and System Lab, Cluster and Grid Computing Lab,\nSchool of Computer Science and Technology, Huazhong University of Science and Technology, China\n2Data Science and Machine Intelligence Lab, University of Technology Sydney, Sydney, Australia\n{ruilinzhao,zhaof,hjin}@hust.edu.cn, guandong.xu@uts.edu.au, zsx57575@gmail.com\nAbstract\nRecent progress regarding the use of language\nmodels (LMs) as knowledge bases (KBs) has\nshown that language models can act as struc-\ntured knowledge bases for storing relational\nfacts. However, most existing works only con-\nsidered the LM-as-KB paradigm in a static\nsetting, which ignores the analysis of tempo-\nral dynamics of world knowledge. Further-\nmore, a basic function of KBs, i.e., the abil-\nity to store conﬂicting information (i.e., 1-N,\nN-1 and N-M relations), is underexplored. In\nthis paper, we formulate two practical require-\nments for treating LMs as temporal KBs: (i)\nthe capacity to store temporally-scoped knowl-\nedge that contains conﬂicting information and\n(ii) the ability to use stored knowledge for\ntemporally-scoped knowledge queries. We in-\ntroduce a new dataset called LAMA-TK which\nis aimed at probing temporally-scoped knowl-\nedge, and investigate the two above require-\nments to explore the LM-as-KB paradigm in\nthe temporal domain. On the one hand, exper-\niments show that LMs can memorize millions\nof temporally-scoped facts with relatively high\naccuracy and transfer stored knowledge to tem-\nporal knowledge queries, thereby expanding\nthe LM-as-KB paradigm to the temporal do-\nmain. On the other hand, we show that memo-\nrizing conﬂicting information, which has been\nneglected by previous works, is still challeng-\ning for LMs and hinders the memorization of\nother unrelated one-to-one relationships.\n1 Introduction\nRecently, language models (LMs) such as BERT\n(Devlin et al. , 2019) and T5 ( Raffel et al. , 2020)\nhave been suggested as an alternative to world\nknowledge bases (KBs) ( Petroni et al. , 2019). The\nparameters of these models appear to store ex-\ntensive real-world knowledge during training and\nstored knowledge can be recalled by ﬁlling cloze\nstatements (e.g. \"Dani Alves plays with [MASK].\n∗*Corresponding author\nMichael Jordan played for [MASK] from 1995 to 1998.\n[MASK] held the position of president of the \nUnited States from 2009 to 2017.\nJohn Bardeen received Nobel Prize in Physics in [MASK].\nChicago Bulls\nBarack Obama\n1956 or 1972\nMichael Jordan joined Washington Wizards in [MASK].\nMichael Jordan played for [MASK] in 2002.\n2001\nWashington Wizards\nThe capacity to store temporally-scoped knowledge.\nThe ability to use stored temporally-scoped knowledge.\nMichael Jordan played for \nWashington Wizards \nfrom 2001 to 2003\nRoBERTa\nRoBERTa\nFigure 1: Expansion of the LM-as-KB paradigm to the\ntemporal domain. We introduce two requirements to\nfurther explore the capability of LMs. (i) The capability\nto store temporal knowledge and (ii) the ability to use\nstored knowledge for temporal knowledge queries.\n–> Barcelona\"). As a result, recent works have con-\nsidered LMs for tasks such as closed-book ques-\ntion answering ( Roberts et al. , 2020), automated\nfact-checking ( Guo et al. , 2021), and knowledge-\ngrounded dialogue systems ( Liu et al. , 2022).\nRelational facts in world knowledge often\nchange with time. For example, \"Michael Jor-\ndan played for Washington Wizards.\" is true only\nfrom 2001 to 2003. However, most existing works\nonly considered the LM-as-KB paradigm in a\n\"static\" setting, ignoring the temporal dynamics\nof world knowledge. However, this temporally-\nscoped knowledge raises several potential chal-\nlenges for the LM-as-KB paradigm.\nConﬂicting Information While training on the\nlarge textual corpus, the model will inevitably\nencounter conﬂicting information (i.e., 1-N, N-\n1, N-M relations), e.g., \"Giannis Antetokounmpo\nplayed for Filathlitikos / Milwaukee Bucks\". Dhin-\ngra et al. (2022) limits the time period of facts\nto reduce the amount of conﬂicting information.\nHowever, conﬂicting information still exists, from\nthe players who played for a team to the politi-\ncian who held multiple positions. These conﬂict-\ning facts will hinder the memorizing process and\n2024\ncause the model to have difﬁculty memorizing all\ncorrect answers.\nCorrelation Between Temporal Scopes Tem-\nporal facts usually contain temporal scopes (e.g.,\na start time and an end time), and a strong corre-\nlation is present between these timestamps. For\nexample, \"Shinzo Abe was the prime minister of\nJapan from 2006 to 2007 .\" and \"Shinzo Abe was\nthe prime minister of Japan from 2012 to 2020 .\"\nare two temporally-scoped facts. These facts have\nthe same subject, object, and predicate but dif-\nferent temporal scopes. As temporal knowledge\nbases, LMs need to memorize not only the times-\ntamps associated with the facts but also the match-\ning relationships between temporal scopes.\nImplicit Temporal Knowledge Temporally-\nscoped facts usually contain implicit facts. For\nexample, \"François Hollande served as president\nof the French Republic from 2012 to 2017\"\ncontains the following facts: \"François Hollande\nserved as president of the French Republic in\n2015\" and \"François Hollande was elected pres-\nident of the French Republic in 2012\". These\nimplicit facts are not directly mentioned in\ntemporally-scoped facts.\nTemporally-scoped knowledge widely exists in\nreal-world knowledge bases like Wikidata. How-\never, existing QA datasets such as LAMA ( Petroni\net al. , 2019), Natural Questions ( Kwiatkowski\net al. , 2019) focus on probing static knowl-\nedge, ignoring the temporal dynamics of world\nknowledge. The temporal dataset TEMPLAMA\n(Dhingra et al. , 2022) focuses on querying fac-\ntual objects with single timestamps, ignoring the\ntemporally-scoped information such as the start\nand end times. Moreover, temporal facts often con-\ntain extensive conﬂicting information, but previ-\nous works did not pay enough attention to these\nconﬂicts. They explored LM-as-KB within 1-1\nrelations (e.g. born in) or discarded facts with\nmultiple objects. Therefore, we propose LAMA-\nTK (short for LAnguage Model Analysis for\nTemporal Knowledge), a new dataset for probing\nLMs for temporally-scoped knowledge. LAMA-\nTK queries temporal knowledge including entity\nnames and special timestamps, and reserves all\ncorrect answers for each query.\nBased on LAMA-TK, we introduce two prac-\ntical questions for LMs as temporal KBs to ex-\nplore the LM-as-KB paradigm in the temporal\nTable 1: Examples from LAMA, TEMPLAMA, and\nour proposed LAMA-TK. LAMA-TK is a novel dataset\nof temporal knowledge statements, which takes into ac-\ncount entities, temporal scopes and multiple answers.\nInput Target(s)\nLAMA\nDante was born in [MASK]. Florence\nBailey Peninsula is located in [MASK]. Antarctica\nTEMPLAMA\nyear: 2013 text: Marina Silva is a member of the _X_. Brazilian Socialist Party\nyear: 2018 text: Marina Silva is a member of the _X_. Sustainability Network\nLAMA-TK\nMichael Jordan played for [MASK] from 1995 to 1998. Chicago Bulls\nMichael Jordan played for [MASK] in 2002. Washington Wizard\nMichael Jordan received NBA Most Valuable Player\nAward in [MASK].\n1988, 1991, 1992,\n1996, 1998\ndomain. We examine LMs on two basic func-\ntions of KBs: the storage capacity and the use of\nstored temporal knowledge, and identify the chal-\nlenges mentioned above during the experiments\n(Section 4).\nFirst question: What is the storage capacity\nof LMs for storing temporal knowledge? Here,\nwe ask the models to memorize millions of tem-\nporal facts and record the storage performance of\nthese LMs (Section 4.1). Results show that LMs\ncan memorize millions of temporal facts with rela-\ntively high accuracy. However, we also show that\nconﬂicting information poses a great challenge to\nthe storage capacity of LMs and hinders the mem-\norizing process of other unrelated facts.\nSecond question: Can LMs use stored tem-\nporal knowledge for temporally-scoped knowl-\nedge queries? Here, we design targeted queries\nto recall stored temporal facts (Section 4.2) and\nfurther explore the ability of LMs to recall im-\nplicit temporal facts (Section 4.3). Results show\nthat pretrained LMs can transfer stored temporally-\nscoped knowledge to new queries with similar se-\nmantics even if the query templates are not ob-\nserved during training. Moreover, we show that\nwith prompts like \" from ST to ET\", LMs can un-\nderstand the difference and continuity of tempo-\nral scopes. These results show that LMs can efﬁ-\nciently handle temporal knowledge.\nContributions: (1) We introduce three chal-\nlenges and two practical requirements for treating\nLMs as temporal KBs, which expands the LM-as-\nKB paradigm ( Petroni et al. , 2019) to the temporal\ndomain. (2) We offer LAMA-TK, a new dataset\nfor probing LMs for temporally-scoped knowl-\nedge. (3) We propose a prompt-based temporal\nscope modeling method to jointly model temporal\nscopes and facts for adapting LMs to temporally-\nscoped facts. (4) We conduct experiments to evalu-\n2025\nate the capacity of LMs to store temporal facts and\nexamine the ability of LMs to use stored knowl-\nedge for temporal queries. (5) We show the nega-\ntive impact of conﬂicting information on the stor-\nage capacity of LMs, which was neglected by pre-\nvious works.\n2 The LAMA-TK Probe\nIn this section, we detail the construction of\nLAMA-TK1, our new temporally-scoped knowl-\nedge probing dataset, including its data sources\nand a set of natural language queries for probing\ntemporal knowledge, as well as the evaluation me-\ntric we use.\n2.1 Knowledge Sources\nCronQuestions CronQuestions ( Saxena et al. ,\n2021) is a KGQA 2 dataset, including a knowledge\ngraph (KG) with associated timestamps and 350K\ntemporal questions. There are 323k facts, 125k en-\ntities, and 203 relations in its KG. We selected the\ntop-5 most frequent temporally rich relations, re-\nsulting in a KG with 226K facts, 96k entities, and\n1322 timestamps.\nWikidata Wikidata3 is a public KB that stores\na massive amount of structured data. We use the\ndump of the January 3rd, 2022 version and re-\ntrieve facts that have both start and end dates using\nSPARQL queries. Following Dhingra et al. (2022),\nwe identify the factual knowledge that has more\nthan one object at the different time periods and\nselect 6 relations with the most such objects. This\nresults in a KG with 497K facts, 260k entities, and\n1132 timestamps.\n2.2 Temporal Knowledge Queries\nAccording to the above knowledge sources, we ﬁ-\nnally construct a KG with 639k facts, 316k enti-\nties, 1539 timestamps, and 7 relations. Follow-\ning Jiang et al. (2020); Dhingra et al. (2022), we\nwrite templates for these relations and convert tem-\nporal knowledge to natural language statements.\nFor example, the temporal knowledge <Giannis\nAntetokounmpo, Member of Sports Team, Filath-\nlitikos B.C., 2011, 2013> was converted into a\nnatural language statement \"Giannis Antetokoun-\nmpo played for Filathlitikos B.C. from 2011 to\n1The LAMA-TK dataset is available at https://github.\ncom/CGCL-codes/LAMA-TK\n2Question Answering over Knowledge Graph.\n3www.wikidata.org\n2013\". Based on these statements, we design tar-\ngeted cloze-style queries and reserve the masked\nentity as the training target. Templates, example\nqueries, and data pre-processing details have been\nshown in Appendix A.\nReal-world knowledge contains extensive con-\nﬂicting information, from the players who played\nfor a team to a politician who held multiple posi-\ntions. However, most previous works tend to ex-\nplore the LM-as-KB paradigm within one-to-one\nrelationships (e.g. \"born in\") or only reserve one\nof the correct answers as the target, without tak-\ning into account whether LMs have similar con-\nﬁdences in other correct answers. Therefore, in\nLAMA-TK, we do not discard conﬂicting infor-\nmation and reserve all correct answers of each\nmasked statement as the answer list (see Table 9\nfor the different between masked entity and an-\nswer list ). Among the 2.48M masked factual state-\nments, there are 379K statements (15%) with mul-\ntiple answers.\n2.3 Evaluation Metric\nAs many queries have multiple answers, we use\nthe top-K accuracy (Acc@K) to measure how well\nthe model performs on these queries. The Acc@k\nis \"query-oriented\". For each query, the top-K ac-\ncuracy is 1 if any of the correct answer is in the top\nk predictions, and is 0 otherwise. In this work, we\nuse both Acc@1 and Acc@5.\nHowever, Acc@K can only measure whether\nthe model can recall at least one correct answer,\nbut it ignores the memorization performance of\nother correct answers of a multi-answer query 4.\nTherefore, we use Hit at top k (Hit@K) to take\ninto account all correct answers to each query. The\nHit@K is \"answer-oriented\". For each correct an-\nswer to the query, if the correct answer is in the\ntop k predictions, Hit@K is 1, otherwise is 0. In\nthis work, we use Hit@5 and Hit@10.\n3 Models\nFollowing Heinzerling and Inui (2021), we use\nRoBERTa (Liu et al. , 2019), the bidirectional LM,\nas the knowledge base. Moreover, we adopt sev-\neral approaches to adapt the original RoBERTa to\ntemporally-scoped knowledge and prepare three\ndifferent RoBERTa models for examination.\n4See Appendix C for more details.\n2026\n3.1 Adaptations\nPrompt-based Temporal Scope Modeling To\njointly model temporal scopes and texts, we manu-\nally write prompt templates for temporal facts and\ndirectly encode temporal scopes in training pro-\ncess. Given a factual sequence of tokens X =\n[x1, x2, .., xn] and its associating temporal scope\n<ST, ET> . We use prompt template \" from ST to\nET\" to convert temporal scope to natural language\ntext and incorporate this text into the factual se-\nquence. In this case, the ﬁnal factual sequence\nX′ = [x1, x2..., xn, from, ST, to, ET]. See Ap-\npendix B for further analysis.\nSymbolic Representation However, the pre-\ntrained masked language model can only handle\nentities whose names are in its vocabulary (e.g.,\nentities like \"English\" and \"Florida\"). This results\nin its inability to predict entities with multiple to-\nkens (e.g., entities like \"Barack Obama\"). In this\nwork, we follow Heinzerling and Inui (2021) to\nstore entities by symbolic representation, i.e., aug-\nmenting the vocabulary of LM and representing all\nthe entities as entries in the vocabulary. The LM\nwill project the ﬁnal hidden state of the [MASK]\ntoken onto the vocabulary and take a softmax over\nall entities ( Heinzerling and Inui , 2021). Although\nsymbolic representation is computationally expen-\nsive, it can memorize entities with high accuracy\nand will not be affected by the length of the entity\nname.\nMemorizing Facts via MLM In this work, we\ntrain the model to memorize factual knowledge\nvia Masked Language Modeling (MLM) ( Devlin\net al. , 2019). We use an entity-level MLM to al-\nlow LMs to memorize entities mentioned in fac-\ntual statements. For example, given an input se-\nquence of tokens X = [x1, x2, ..., xi, xi+1, ..., xn]\nand a two-token entity e = [xi, xi+1]. We con-\nvert the whole tokens of the entity to one mask\ntoken. In this case, the masked sequence of tokens\nX′ = [ x1, x2, ..., xi− 1, [MASK ], xi+2, ..., xn].\nSince we use symbolic representation, the masked\nentity is in the vocabulary of the LM.\n3.2 Employed Models\nRoBERTa(12L) We prepare a RoBERTa model\nwith 12 layers as the temporal knowledge base.\nThe RoBERTa(12L) is initialized from RoBERTa-\nbase ( Liu et al. , 2019).\nRoBERTa(6L) We prepare a 6-layer RoBERTa\nmodel, initialized from DistilRoBERTa-base\n(Sanh et al. , 2019), to investigate how knowledge\nbase capability scales with model size 5.\nRoBERTa-randinit(12L) Heinzerling and Inui\n(2021) shows that LMs without pre-training can\nmemorize more factual statements than pretrained\nmodels. However, it only focuses on memorizing\nstatic and one-to-one relationships. In this work,\nwe also prepare a 12-layer RoBERTa with ran-\ndomly initialized parameters to further explore the\neffect of pre-training in a more practical condition.\n4 Experiments\nStorage and the use of stored knowledge are two\nbasic functions of KBs. To explore the LM-as-KB\nparadigm in the temporal domain, we design sev-\neral experiments to answer the two questions of\nLMs as temporal KBs (Section 1).\nFirst, we conduct a reciting experiment to eval-\nuate the storage capacity of LMs for storing tem-\nporal facts and explore the impact of conﬂicting\ninformation (Section 4.1). Next, we construct tar-\ngeted queries to recall stored temporal knowledge\nin terms of temporal boundaries (Section 4.2) and\nimplicit temporal knowledge (Section 4.3) to ex-\nplore the ability of LMs to use stored knowledge .\n4.1 Storage Capacity\nStorage is the foundation of KB applications.\nHere, we conduct a reciting experiment to inves-\ntigate how much temporal knowledge LMs can\nmemorize (the ﬁrst question) . Firstly, we train\nprepared RoBERTa models to memorize temporal\nknowledge in LAMA-TK. For each fact in LAMA-\nTK, we mask the subject, object, start time, and\nend time, and generate four masked statements.\nThese masked statements then serve as the train-\ning data for LMs to memorize. For example, given\nthe masked statement \"Michael Jordan played for\n[MASK] from 1995 to 1998.\", the model should\npredict the masked entity \"Chicago Bulls\". We\ncall this process as Feeding Temporal Knowl-\nedge into LMs . Then, we test the models to eval-\nuate how many factual statements in training data\nhave been memorized and record the Acc@K and\nHit@K (see Section 2.3). We call this process as\nReciting.\n5DistilRoBERTa-base is the distilled version of\nRoBERTa-base, with 6 layers. Details of the models\nare in Appendix D.\n2027\n1.0\n0.9\n0.8\n0.7\n0.6\n0.5\n100K 500K 1M 1.5M 2M 2.5M\nRoBERTa(6L)\nRoBERTa-rand(12L)\nRoBERTa(12L)\nRoBERTa(6L) non-conflict\nRoBERTa-rand(12L) non-conflict\nRoBERTa(12L) non-conflict\n0.6\n0.5\nAcc@1\nNumber of Masked Statements\n1.0\n0.9\n0.8\n0.7\n0.6\n0.5\n100K 500K 1M 1.5M 2M 2.5M\nRoBERTa(6L)\nRoBERTa-rand(12L)\nRoBERTa(12L)\nRoBERTa(6L) non-conflict\nRoBERTa-rand(12L) non-conflict\nRoBERTa(12L) non-conflict\n0.6\n0.5\nAcc@5\nNumber of Masked Statements\n1.0\n0.95\n0.90\n0.85\n0.6\n0.5\n100K 500K 1M 1.5M 2M 2.5M\nRoBERTa(6L)\nRoBERTa-rand(12L)\nRoBERTa(12L)\nRoBERTa(6L) non-conflict\nRoBERTa-rand(12L) non-conflict\nRoBERTa(12L) non-conflict\n0.80\n0.75 Hit@5\nNumber of Masked Statements\n0.6\n0.5\n100K 500K 1M 1.5M 2M 2.5M\nRoBERTa(6L)\nRoBERTa-rand(12L)\nRoBERTa(12L)\nRoBERTa(6L) non-conflict\nRoBERTa-rand(12L) non-conflict\nRoBERTa(12L) non-conflict\nNumber of Masked Statements\n1.0\n0.95\n0.90\n0.85\n0.80\n0.75 Hit@10\nFigure 2: Overall results of statement memorization. We report Acc@1, Acc@5, Hit@5, and Hit@10 of each\nmodel. The green lines show the performances of models trained on LAMA-TK without conﬂicting information,\nwhile the red lines show the performances of models trained on LAMA-TK with conﬂicting information.\nTable 2: Results of single-answer and multi-answer\nstatement memorization of RoBERTa(12L) trained on\n2.48M masked statements with conﬂicting information.\n#Answer Acc@1 Acc@5 Hit@5 Hit@10\nsingle 0.8441 0.9550 0.9550 0.9623\nmultiple 0.7876 0.9358 0.4748 0.5527\nDifferent from previous works, we examine the\ncapacity of LMs to memorize temporally-scoped\nfacts which often change with time. For ex-\nample, \"Michael Jordan played for Birmingham\nBarons\" is only true from 1994 to 1995. In\n1995, Michael Jordan left Birmingham Barons and\njoined Chicago Bulls. Therefore, to correctly re-\ncall the sports team Michael Jordan played for,\nLMs should additionally take into account the tem-\nporal scopes of facts.\nMoreover, we reserve 1-N, N-1, and N-M rela-\ntions in LAMA-TK. During training, the model\nwill see conﬂicting information (the ﬁrst chal-\nlenge), such as the politician who held multiple\npositions at once and the scientist who received\nmultiple prizes in a year. We call these statements\nas multi-answer statements . Previous works\ndiscarded facts with multiple objects and consid-\nered the LM-as-KB paradigm within 1-1 relations,\nwhich made this task lightweight, but less practi-\ncal. However, storing conﬂicting information is a\nbasic function that a KB should have.\nResult The red lines in Fig 2 show the ac-\ncuracies of statements memorization accuracies\nachieved with different RoBERTa models. The\nrandomly initialized RoBERTa model has the high-\nest recall accuracy for storing temporal knowledge,\ncorrectly answering 86 percent of 2.48 million\nmasked statements; RoBERTa(6L) has the lowest\nrecall accuracy, with 0.76 Acc@1. As the amount\nof training data increases, the storage accuracy of\nall the models gradually decreases. Compared to\nthe RoBERTa(12L), RoBERTa(6L) has more difﬁ-\nTable 3: single-answer statement (1-1 relation) mem-\norization results of RoBERTa(12L) trained on 2.48M\nstatements with and without conﬂicting information.\nTrain Data 1-1 relations\nAcc@1 Acc@5 Hit@5 Hit@10\nnon-conﬂict 0.9700 0.9910 0.9910 0.9930\nconﬂict 0.8441 0.9550 0.9550 0.9623\nculty storing millions of masked statements. This\nresult indicates that LMs with more parameters\nshow better storage capacity. Moreover, we show\nthat the randomly initialized LM exhibits better\nstorage capacity than the pretrained LM. This re-\nsult is the same as previous work ( Heinzerling and\nInui, 2021). The knowledge stored during pretrain-\ning affects the memorization of new knowledge.\nTable 2 shows the memorization results of state-\nments with single and multiple answers. Results\nshow that RoBERTa(12L) can memorize single-\nanswer statements with high Acc@K and Hit@K.\nHowever, memorizing multi-answer statements is\nstill challenging, which shows high Acc@K but\nlow Hit@K. This result shows that LMs can only\nmemorize one of the correct answers, but do not\nhave similar conﬁdence in other correct answers.\nInﬂuence of Conﬂicting Information To ex-\nplore the inﬂuence of conﬂicting information on\nthe storage capacity of LMs, we compare mod-\nels trained on LAMA-TK with and without con-\nﬂicting information. For the version of LAMA-\nTK without conﬂicting information (non-conﬂict),\nwe remove all masked statements with multiple\nanswers. Then, we examine RoBERTa(6L) and\nRoBERTa(12L) on LAMA-TK without conﬂicting\ninformation.\nThe green lines in Fig 2 show the statement\nmemorization performances achieved without con-\nﬂicting information. All models can memorize\n2.48M statements with over 0.95 Acc@1, which\nis much better than memorizing statements with\nconﬂicting information. The accuracy drop indi-\n2028\nTable 4: Performances of RoBERTa models with and without dynamic time masking on 200k time queries in\nzero-shot settings. The models above the midline use original masking, while the ones below the midline use\ndynamic time masking. The green numbers in brackets show the improvement dynamic time masking brings over\nRoBERTa(12L) with original masking. The highest and second-highest scores among all models are boldfaced\nand underlined respectively. Scores with asterisks are the highest among the models with original masking.\nModel Acc@1 Acc@5 Hit@5 Hit@10\nRoBERTa(6L) 0.1890∗ 0.4510∗ 0.3849∗ 0.4944∗\nRoBERTa-rand(12L) 0.1280 0.3260 0.2614 0.3590\nRoBERTa(12L) 0.1226 0.3240 0.2689 0.3596\nRoBERTa(12L) dynamic mask 10% 0.3774(+0.2658) 0.7042(+0.3802) 0.6628(+0.3939) 0.7740(+0.4144)\nRoBERTa(12L) dynamic mask 100% 0.4879(+0.3653) 0.8367(+0.5127) 0.7611(+0.4922) 0.8838(+0.5242)\ncates that the storage capacity of LMs is greatly\naffected by conﬂicting information. The accu-\nracy drop yielded by RoBERTa(6L) is greater than\nRoBERTa(12L), showing that models with fewer\nparameters are more susceptible to conﬂicting in-\nformation.\nMoreover, Table 3 shows the inﬂuence of con-\nﬂicting information on memorizing other one-to-\none relationships. The performance drops indicate\nthat conﬂicting information hinders the memoriz-\ning process of other unrelated 1-1 relations.\n4.2 Temporal Boundary Query\nIn the ﬁrst experiment, we observe that it is possi-\nble for LMs to recite millions of temporal knowl-\nedge. We now turn to investigate whether LMs\ncan use stored knowledge for temporal knowl-\nedge queries or merely recite facts learned by\nrote (the second question) . Firstly, we test\nwhether LMs can differentiate between stored\ntimestamps. For example, if an LM has memo-\nrized \"Barack Obama held the position of pres-\nident of United States from 2009 to 2017\", the\nmodel should recall the start time \"2009\" with the\nquery \"Barack Obama was elected president of the\nUnited States in [MASK]\" or recall the end time\n\"2017\" with the query \"Barack Obama resigned\nfrom president of the United States in [MASK]\".\nTo ensure that the LMs can memorize all re-\nquired knowledge, we ﬁrst sample 100k fact state-\nments with the predicate \"position held\" from\nLAMA-TK and mask their start and end times.\nThis results in 200k masked factual statements.\nThen we train the RoBERTa models to fully mem-\norize all these statements, with 0.99Acc@1.\nNext, we write cloze-style templates to query\nthe start and end times mentioned in stored facts,\nsuch as \"S was elected O in [MASK]\" and \"S re-\nsigned from O in [MASK]\". We use these queries\nto test the capability of the model to understand\nthe difference between temporal scopes. We con-\nRoBERTa\nBarack Obama held the position of \npresident of United States from 2009 to 2017.\nOriginal Masking Dynamic Time Masking\n90%\n10%\ntrain\nquery\nBarack Obama was elected\npresident of United States in [MASK].\nBarack Obama resigned from\npresident of United States in [MASK].\nBarack Obama held the position of \npresident of United States from [MASK] to 2017.\nBarack Obama held the position of \npresident of United States from 2009 to [MASK].\nBarack Obama held the position of \npresident of United States from [MASK] to 2017.\nBarack Obama held the position of \npresident of United States from 2009 to [MASK].\n2009 2017\nFigure 3: Examples of two types of masking and the\nprocess used by LMs for temporal boundary queries.\nThe remaining timestamps are underlined . The predi-\ncates written in red are new query templates.\nduct this experiment in a zero-shot setting; i.e.,\nthe target query templates are not observed during\ntraining. The zero-shot setting can better show the\ncapability of LMs to understand natural language\nqueries and transfer knowledge to targeted queries.\nResult The results are shown in the ﬁrst three\nrows of Table 4. In the case where the model has\nfully memorized all required temporal knowledge,\nthe model with fewer parameters performs better.\nThe performance of RoBERTa(12L) is similar to\nthat of RoBERTa-randinit(12L), but both are lower\nthan that of RoBERTa(6L).\nDynamic Time Masking Through the above ex-\nperiment, we ﬁnd that the model’s capability to\nquery temporal boundaries is not satisfactory (low\nAcc@1). We speculate that this result may be\ndue to the strong correlation between temporal\nscopes (the second challenge) . Original mask-\ning makes the model relies too much on the re-\nmaining timestamp and makes it difﬁcult to query\nthe masked timestamp separately. For example,\nwe use \"Barack Obama held the position of pres-\nident of United State from [MASK] to 2017\" to\ntrain the LMs, which makes the prediction for the\nmasked timestamp \"2009\" excessively rely on the\n2029\nTable 5: Results on 20k queries with original query templates and new query templates (original query templates:\n\"S held the position of O in T\", new query templates: \"S served as O in T\"). We report Acc@1/Acc@5 and\nHit@5/Hit@10 of each model on two template types.\nModel Parameters\nAcc@1 / Acc@5 Hit@5 / Hit@10\nTemplate Type Template Type\nOriginal New Original New\nRoBERTa(6L) 82M 0.4114 / 0.6521 0.2242 / 0.4115 0.6192 / 0.6993 0.3798 / 0.4540\nRoBERTa-rand(12L) 125M 0.4147 / 0.6868 0.0131 / 0.0562 0.6457 / 0.7215 0.0757 / 0.0518\nRoBERTa(12L) 125M 0.3440 / 0.5666 0.3113 / 0.5020 0.5281 / 0.6028 0.4698 / 0.5480\nremaining timestamp \"2017\". This makes it difﬁ-\ncult for LMs to transfer stored timestamps to tem-\nporal boundary queries and results in answering\nthese queries with low accuracy.\nInspired by the dynamic masking of RoBERTa\n(Liu et al. , 2019), we design a dynamic time mask-\ning method to verify this conjecture. As shown in\nFigure 3, while constructing masked factual state-\nments, we only mask the speciﬁc timestamp 1-k%\nof time, and for the other k% of time we mask the\nspeciﬁc timestamp and delete the other time infor-\nmation. To avoid using the same time mask in ev-\nery epoch, we duplicate the training data 10 times\nso that each statement is masked in 10 different\nways over 50 epochs of training. Therefore, the\nmodel will see 10 variations of each statement.\nDynamic time masking reduces the strong cor-\nrelation between temporal scopes by adding per-\nturbation to the other temporal information dur-\ning training. In this experiment, we evaluate\nRoBERTa(12L) with 10% and 100% dynamic\ntime masking. Table 4 shows the performance of\nthese models. By adding 10% perturbation, the ac-\ncuracy of RoBERTa(12L) signiﬁcantly increases\nto 0.3774 Acc@1, 0.7042 Acc@5. The Hit@K of\nRoBERTa(12L) also increases signiﬁcantly. More-\nover, we evaluate RoBERTa with 100% dynamic\ntime masking which completely ignores the corre-\nlation between the start and end times. RoBERTa\nwith 100% dynamic time masking performs the\nbest (both Acc@K and Hit@K). However, 100%\ndynamic time masking causes the model to be un-\nable to associate the start time and the end time\nand to handle facts such as a politician who held\none position several times . These results show\nthat dynamic time masking efﬁciently reduces the\nstrong correlation between temporal scopes and\nhelps LMs recall the stored timestamps.\n4.3 Implicit Temporal Knowledge Query\nPrompt-based temporal scope modeling allows\nLMs to memorize temporal facts with their as-\nsociated temporal scopes. (e.g., \"Michael Jor-\ndan played for Chicago Bulls from 1995 to\n1998\"). Compared with jointly modeling text and\na single timestamp (e.g., \"Michael Jordan played\nfor Chicago Bulls in 1995/1996/1997/1998.\"),\nprompt-based temporal scope modeling intro-\nduces fewer factual statements and less conﬂicting\ninformation, but more implicit temporal knowl-\nedge (the third challenge) . However, can LMs\nuse stored knowledge for implicit temporal\nknowledge queries (the second question)? For\nexample, if an LM has memorized \"François Hol-\nlande held the position of president of the French\nRepublic from 2012 to 2017\", can the LM under-\nstand that François Hollande was the president of\nthe French Republic for each year between 2012\nand 2017? Moreover, can LMs answer the query\n\"François Hollande served as [MASK] in 2015\"\neven if the template \"S served as O in T\" is not\nseen during training?\nA controlled experiment is designed for these\nquestions. We choose one predicate \"position\nheld\" and sample all statements generated by the\ntemplate \"S held the position of O from ST to ET\".\nInspired by Heinzerling and Inui (2021), we add\ndistractors to recognize whether LMs answer these\nqueries by using stored knowledge or simple by\ngeneric association. For a fact <S, P, O, ST, ET>,\nwe add its distractor <S, P, O’, ST’, ET’> which\ninvolves the same subject S and predicate P, but a\ndifferent Object O’. Moreover, we add its distrac-\ntor <S, P’, O’, ST”, ET”> which involves the same\nsubject S but a different predicate P’ and object O’.\nFor example, distractors for <Barack Obama, Posi-\ntion Held, President of United States, 2009, 2017>\nare <Barack Obama, Position Held, United States\nsenator, 2007, 2008> and <Barack Obama, award\nreceived, Nobel Peace Prize, 2009, 2009>. To cor-\nrectly answer the query \"Barack Obama held the\nposition of [MASK] in 2012.\", the model needs\nto consider both the predicate and the temporal\nscopes since there are three distinct entities associ-\nated with \"Barack Obama\". Every fact has at least\none distractor. This results in 20k statements.\n2030\nNext, we train the RoBERTa models to mem-\norize all these fact statements and construct elabo-\nrate queries. For each fact, we randomly select one\nyear between the start and end years as the times-\ntamp of the associated query. We do not consider\nthe start and end years because these boundary\ntimestamps can bring prompts to the query. Then\nwe use two types of templates to generate queries.\nFirst, we use the Original Template \"S held the po-\nsition of O in T\" which is also used to generate\nfact statements for training. Then, we use a New\nTemplate \"S served as O in T\". This template has\nsimilar semantics to the original template, but it\nis not seen during training. We use the New Tem-\nplate to evaluate the robustness of LMs to distinct\ntemplates.\nResult Results are shown in Table 5. For\nOriginal Template , RoBERTa-Randinit(12L) has\nthe highest Acc@K and Hit@K. Compared with\nRoBERTa(12L), RoBERTa(6L) with fewer param-\neters performs slightly better. This result is similar\nto that of the previous experiment, which shows\nthat LMs with fewer parameters have a better ca-\npability to use stored temporal knowledge.\nHowever, the performance achieved on the New\nTemplate shows a distinct result. In the case where\nthe query template is not observed during training,\nthe performance of RoBERTa-randinit(12L) sig-\nniﬁcantly declines, with only 0.0131Acc@1 and\n0.0518Hit@10. Conversely, the performance of\npretrained RoBERTa(12L) drops slightly and re-\nmains at a high level. This result shows that\npretrained LMs contain natural language knowl-\nedge and have strong robustness to new templates.\nCompared to RoBERTa(12L), RoBERTa(6L) has\nlower performance with a more severe drop, show-\ning that LM with more parameters is less affected\nby unseen templates and shows stronger robust-\nness.\n5 Related Work\nRecent research has shown that pretrained lan-\nguage models (PLMs) such as BERT ( Devlin et al. ,\n2019), RoBERTa ( Liu et al. , 2019), GPT ( Rad-\nford et al. , 2018), and T5 ( Raffel et al. , 2020) can\nlearn extensive world knowledge during pretrain-\ning and store these relational facts in their param-\neters. Petroni et al. (2019) constructs LAMA, a\nset of cloze-style queries (e.g., \"Marcello Abbado\nwas born in [MASK]. –> Milan\"), to recall the fac-\ntual knowledge contained in pretrained LMs such\nas ELMo ( Peters et al. , 2018) and BERT ( Devlin\net al. , 2019). Their results show that a PLM con-\ntains relational knowledge and can recall stored\nfacts without ﬁne-tuning. Talmor et al. (2020) pro-\nposes eight cloze-style reasoning tasks to test dif-\nferent types of knowledge in BERT and RoBERTa.\nHeinzerling and Inui (2021) conduct experiments\non RoBERTa to evaluate its ability to store mil-\nlions of facts involving millions of entities and its\nability to query stored facts. Its results provide\na proof-of-concept for LM-as-KB. While these\nworks focus on probing LM in the general do-\nmain, Sung et al. (2021) constructs the BioLAMA,\na biomedical factual knowledge dataset for prob-\ning biomedical LMs, and further explores the ca-\npability of LM to act as a speciﬁc-domain KB.\nMoreover, Wang et al. (2019a); Zhou et al. (2020)\nexamine PLMs on commonsense reasoning tasks,\nshowing that PLM contains commonsense knowl-\nedge. To improve the performance on knowl-\nedge intensive tasks, Wang et al. (2019b) uses a\nTransformer encoder to obtain contextualized en-\ntity and relation embeddings. Yao et al. (2019)\ntreats relational knowledge as textual sequences\nand ﬁnetunes BERT to model these knowledge.\nTo improve the performance of recalling knowl-\nedge, Petroni et al. (2020) augments PLM with\nretrieved relevant context and improved the perfor-\nmance of cloze-style QA. Jiang et al. (2020) pro-\nposes mining-based and paraphrasing-based meth-\nods to generate high-quality prompts, which sig-\nniﬁcantly improves the performance achieved on\nLAMA.\nWithin the current paradigm of the use of\nmasked LMs as KBs, research has focused more\non using generative LMs as KBs. As generative\nLMs can generate text sequences of any length,\nthey are not limited by the length of the given\nknowledge. Roberts et al. (2020) ﬁne-tunes the\npretrained T5 model to three QA datasets Web-\nQuestions ( Berant et al. , 2013), TriviaQA ( Joshi\net al. , 2017), and NaturalQuestions ( Kwiatkowski\net al. , 2019) without any access to external knowl-\nedge to test how much knowledge contained in\nthe LM. The results show that ﬁne-tuned T5 per-\nforms competitively with retrieval-based systems,\nand indicate that large pretrained LMs contain vast\nworld knowledge. Lewis et al. (2021) argues that\nLMs can complete the closed-book QA tasks well,\nmostly due to high test-train overlaps. Wang et al.\n(2021) designs a knowledge memory task and a\n2031\nquestion-answering task on datasets with low test-\ntrain overlaps to evaluate the capability of BART\n(Lewis et al. , 2020) to serve as a KB for closed-\nbook QA. The results show that closed-book QA is\nstill challenging for BART, both in terms of mem-\norizing the knowledge and answering the ques-\ntions. Dhingra et al. (2022) proposes a time-aware\nT5 model which jointly models the text with its\ntimestamp, and constructs a new dataset called\nTEMPLAMA for probing LMs for temporal facts.\nApart from closed-book QA, Dai et al. (2022) ex-\namines cloze tasks for BERT to identify the neu-\nrons that store speciﬁc facts. The results demon-\nstrate the provenance of speciﬁc knowledge in the\nparameters of an LM. Zhu et al. (2020); Cao et al.\n(2021) focus on editing stored knowledge without\naffecting other facts. These works further explore\nthe ability of LMs and expand their functions as\nKBs.\n6 Conclusion\nTemporal knowledge widely exists in real-world\nKBs. In this work, we extend the LM-as-KB\nparadigm to the temporal domain and argue that\npretrained LMs have fairly good capability to\nserve as temporal knowledge bases in terms of\ntheir capacity to store temporal knowledge and\ntheir ability to use stored temporal knowledge.\nHowever, our analysis also shows that conﬂicting\ninformation poses great challenges to the LM-as-\nKB paradigm, such as the drop in storage accuracy\nand the difﬁculty in memorizing multiple answers.\nLimitations\nOur proposed dataset (LAMA-TK) takes the tem-\nporal scopes of temporal facts and N-M relations\ninto account. However, LAMA-TK does not con-\ntain questions that require complex temporal rea-\nsoning, such as \" First-Last: [MASK] was the ﬁrst\npresident of the United States.\" and \" Before-After:\n[MASK] was the president of United States af-\nter Barack Obama.\". ( Saxena et al. , 2021) eval-\nuated BERT, RoBERTa, KnowBERT, and T5 on\nCronQuestions, which contains 232k such com-\nplex questions, but the results showed that these\nlarge pretrained language models perform very\npoorly (lower than 0.01 Hit@1 values).\nIn this work, we propose utilizing the masked\nLM RoBERTa as a temporal KB. Compared to\nT5-cbqa ( Roberts et al. , 2020) (737 million pa-\nrameters), RoBERTa with 12 layers only has 120\nmillion parameters. This makes our experiments\nlightweight. Moreover, we train RoBERTa to\nmemorize temporal facts via MLM ( Devlin et al. ,\n2019). It is possible that incorporating factual\nknowledge into PLMs ( Sun et al. , 2019, 2020) or\naugmented LMs with memory banks ( Févry et al. ,\n2020; Verga et al. , 2020) would allow these LMs\nto memorize factual knowledge more efﬁciently.\nFinally, to explore the capability of an LM to\nmemorize conﬂicting information ( 1-N, N-1, N-M\nrelations), we additionally use Hit@K as the evalu-\nation metric to evaluate how many correct answers\nare contained in the top k predictions. However,\nwe do not consider how to distinguish correct an-\nswers from the predictions of LMs and how many\ncorrect answers should be recalled for a query.\nAcknowledgements\nThis work was supported in part by National Nat-\nural Science Foundation of China under Grants\nNo.62072203 and Australian Research Council\nUnder Grants DP22010371, LE220100078.\nReferences\nJonathan Berant, Andrew Chou, Roy Frostig, and\nPercy Liang. 2013. Semantic parsing on freebase\nfrom question-answer pairs . In Proceedings of the\n2013 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2013, 18-21 Octo-\nber 2013, Grand Hyatt Seattle, Seattle, Washington,\nUSA, pages 1533–1544. ACL.\nNicola De Cao, Wilker Aziz, and Ivan Titov. 2021.\nEditing factual knowledge in language models . In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2021, Virtual Event / Punta Cana, Dominican Re-\npublic, 7-11 November, 2021 , pages 6491–6506. As-\nsociation for Computational Linguistics.\nDamai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao\nChang, and Furu Wei. 2022. Knowledge neurons\nin pretrained transformers . In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), ACL\n2022, Dublin, Ireland, May 22-27, 2022 , pages\n8493–8502. Association for Computational Linguis-\ntics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\n2032\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186. Association for Computa-\ntional Linguistics.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W. Cohen. 2022. Time-Aware Language\nModels as Temporal Knowledge Bases . Transac-\ntions of the Association for Computational Linguis-\ntics, 10:257–273.\nThibault Févry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with en-\ntity supervision . In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing, EMNLP 2020, Online, November 16-20,\n2020, pages 4937–4951. Association for Computa-\ntional Linguistics.\nZhijiang Guo, Michael Sejr Schlichtkrull, and An-\ndreas Vlachos. 2021. A survey on automated fact-\nchecking. CoRR, abs/2108.11896.\nBenjamin Heinzerling and Kentaro Inui. 2021. Lan-\nguage models as knowledge bases: On entity\nrepresentations, storage capacity, and paraphrased\nqueries. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Com-\nputational Linguistics: Main Volume, EACL 2021,\nOnline, April 19 - 23, 2021 , pages 1772–1791. Asso-\nciation for Computational Linguistics.\nZhengbao Jiang, Frank F. Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know . Transactions of the Association for\nComputational Linguistics , 8:423–438.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2017, Vancouver, Canada, July 30 - August 4, Vol-\nume 1: Long Papers , pages 1601–1611. Association\nfor Computational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur P. Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: a benchmark for question answer-\ning research . Transactions of the Association for\nComputational Linguistics , 7:452–466.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension . In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2020, Online, July 5-10, 2020 ,\npages 7871–7880. Association for Computational\nLinguistics.\nPatrick S. H. Lewis, Pontus Stenetorp, and Sebastian\nRiedel. 2021. Question and answer test-train over-\nlap in open-domain question answering datasets . In\nProceedings of the 16th Conference of the European\nChapter of the Association for Computational Lin-\nguistics: Main Volume, EACL 2021, Online, April\n19 - 23, 2021 , pages 1000–1008. Association for\nComputational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining ap-\nproach. CoRR, abs/1907.11692.\nZihan Liu, Mostofa Patwary, Ryan Prenger, Shrimai\nPrabhumoye, Wei Ping, Mohammad Shoeybi, and\nBryan Catanzaro. 2022. Multi-stage prompting for\nknowledgeable dialogue generation . In Findings\nof the Association for Computational Linguistics:\nACL 2022, Dublin, Ireland, May 22-27, 2022 , pages\n1317–1337. Association for Computational Linguis-\ntics.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers) , pages 2227–2237. Association for\nComputational Linguistics.\nFabio Petroni, Patrick S. H. Lewis, Aleksandra Piktus,\nTim Rocktäschel, Yuxiang Wu, Alexander H. Miller,\nand Sebastian Riedel. 2020. How context affects\nlanguage models’ factual predictions . In Proced-\ndings of the Conference on Automated Knowledge\nBase Construction, AKBC 2020, Virtual, June 22-24,\n2020.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language mod-\nels as knowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 2463–2473. Association for\nComputational Linguistics.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training .\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\n2033\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer . Journal of Machine Learning Re-\nsearch, 21:140:1–140:67.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the pa-\nrameters of a language model? In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020 , pages 5418–5426. Associ-\nation for Computational Linguistics.\nGuy D. Rosin, Ido Guy, and Kira Radinsky. 2022.\nTime masking for temporal language models . In\nProceddings of the Fifteenth ACM International\nConference on Web Search and Data Mining, Virtual\nEvent / Tempe, AZ, USA, February 21 - 25, 2022 ,\npages 833–841. ACM.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof BERT: smaller, faster, cheaper and lighter . CoRR,\nabs/1910.01108.\nApoorv Saxena, Soumen Chakrabarti, and Partha P.\nTalukdar. 2021. Question answering over tem-\nporal knowledge graphs . In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Vir-\ntual Event, August 1-6, 2021 , pages 6663–6676. As-\nsociation for Computational Linguistics.\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng\nGuo, Yaru Hu, Xuanjing Huang, and Zheng Zhang.\n2020. Colake: Contextualized language and knowl-\nedge embedding . In Proceedings of the 28th In-\nternational Conference on Computational Linguis-\ntics, COLING 2020, Barcelona, Spain (Online), De-\ncember 8-13, 2020 , pages 3660–3670. International\nCommittee on Computational Linguistics.\nYu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng,\nXuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu,\nHao Tian, and Hua Wu. 2019. ERNIE: en-\nhanced representation through knowledge integra-\ntion. CoRR, abs/1904.09223.\nMujeen Sung, Jinhyuk Lee, Sean S. Yi, Minji Jeon,\nSungdong Kim, and Jaewoo Kang. 2021. Can lan-\nguage models be biomedical knowledge bases? In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2021, Virtual Event / Punta Cana, Dominican Re-\npublic, 7-11 November, 2021 , pages 4723–4734. As-\nsociation for Computational Linguistics.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2020. olmpics - on what language\nmodel pre-training captures . Transactions of the As-\nsociation for Computational Linguistics , 8:743–758.\nPat Verga, Haitian Sun, Livio Baldini Soares, and\nWilliam W. Cohen. 2020. Facts as experts: Adapt-\nable and interpretable neural memory over symbolic\nknowledge. CoRR, abs/2007.00849.\nCunxiang Wang, Shuailong Liang, Yue Zhang, Xiao-\nnan Li, and Tian Gao. 2019a. Does it make sense?\nand why? A pilot study for sense making and ex-\nplanation. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers , pages 4020–4026. Association\nfor Computational Linguistics.\nCunxiang Wang, Pai Liu, and Yue Zhang. 2021. Can\ngenerative pre-trained language models serve as\nknowledge bases for closed-book qa? In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, ACL/IJCNLP 2021, (Volume 1: Long Pa-\npers), Virtual Event, August 1-6, 2021 , pages 3241–\n3251. Association for Computational Linguistics.\nQuan Wang, Pingping Huang, Haifeng Wang, Songtai\nDai, Wenbin Jiang, Jing Liu, Yajuan Lyu, Yong Zhu,\nand Hua Wu. 2019b. Coke: Contextualized knowl-\nedge graph embedding . CoRR, abs/1911.02168.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019.\nKG-BERT: BERT for knowledge graph completion .\nCoRR, abs/1909.03193.\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan\nHuang. 2020. Evaluating commonsense in pre-\ntrained language models. In Proceedings of the\nAAAI Conference of the Artiﬁcial Intelligence , vol-\nume 34, pages 9733–9740.\nChen Zhu, Ankit Singh Rawat, Manzil Zaheer, Srinadh\nBhojanapalli, Daliang Li, Felix X. Yu, and Sanjiv\nKumar. 2020. Modifying memories in transformer\nmodels. CoRR, abs/2012.00363.\n2034\nA Details of LAMA-TK\nKnowledge Sources The knowledge sources of\nLAMA-TK are from CronQuestions( Saxena et al. ,\n2021) and Wikidata. We extracted all facts with\nboth a start date and an end date. Following ( Dhin-\ngra et al. , 2022), we reserved the top 7 most fre-\nquent temporally rich relations from our collected\ntemporal knowledge, namely\n• position held\n• member of sports team\n• employer\n• award received\n• country of citizenship\n• spouse\nThis results in 639k temporally-scoped facts,\nwith 316k entities. Statistics of entities in LAMA-\nTK have been shown in Table 6.\nFact to Text For each fact, we write a template\nto convert it to natural language statements. For\nexample, for the fact <Giannis Antetokounmpo,\nMember of Sports Team, Filathlitikos B.C., 2011,\n2013>, we use the template \"[X] held the position\nof [Y]\" to convert the fact triple <Giannis Ante-\ntokounmpo, Member of Sports Team, Filathlitikos\nB.C.> to text \"Giannis Antetokounmpo played for\nFilathlitikos B.C.\". Templates are shown in Ta-\nble 8.\nFor most relations, we use the prompt template\n\"from ST to ET\" to convert temporal scopes to nat-\nural language texts. However, \"award received\" is\nan exception. It is not a durative relation, the start\ntime of the facts is always equal to the end time.\nTherefore, we use a new prompt template \"in T\" to\nconvert these temporal scopes to texts.\nFinally, we concatenate the factual text and tem-\nporal text, and we get the factual statements.\nConstructing Queries Table 9 shows the mask-\ning process. For each factual statement, we mask\nthe subject, object, start time, and end time, re-\nsulting in four masked statements. We reserve the\nmasked entity and collect all correct answers to the\nmasked statements as the answer list . We train the\nLMs to predict the masked entity , and use both the\nmasked entity and the answer list for evaluation\npurposes. If the masked entity is in the top k an-\nswers of the model, Hit@K is 1. If any of the top k\nanswers of the model is in the answer list, Acc@K\nis 1.\nNote that previous works tended to mask the\nobject of each factual statement only because\nTable 6: Number of entities in LAMA-TK across dif-\nferent types. Please refer to Appendix A.\nPerson Position Sport Team Company\n248463 42006 10953 6768\nPrize Institution Country Time\n3399 3696 152 1539\nTable 7: Examples of Time-aware T5, TempoBERT\nand our proposed prompt-based temporal scope mod-\neling. Our proposed pompt-based temporal scope mod-\neling jointly model text and temporal scopes, which is\nmore suitable for handling temporally-scoped knowl-\nedge.\nTime-aware T5 ( Dhingra et al. , 2022)\nyear 1995: Michael Jordan plays for Chicago Bulls.\nTempoBERT (Rosin et al. , 2022)\n<1995> Michael Jordan plays for Chicago Bulls.\nPrompt-based Temporal Scope Modeling(ours)\nMichael Jordan played for Chicago Bulls from 1995 to\n1998.\nmasking the subject would introduce multi-answer\nstatements. For example, \"[MASK] played for\nChicago Bulls from 1995 to 1998\" has more cor-\nrect answers than \"Michael Jordan played for\n[MASK] from 1995 to 1998\".\nB Further Analysis on Prompt-based\nTemporal Scope Modeling\nSome works have focused on jointly modeling\ntime and text. Time-aware T5( Dhingra et al. ,\n2022) adds a time preﬁx to each text to jointly\nmodel time and text (e.g., \"year:2016 Eden Hazard\nplays for Chelsea F.C\"). TempoBERT( Rosin et al. ,\n2022) adds a time token to the top of the input se-\nquence and designs time masking to encode time\ninto the models (e.g., \"<2022> Joe Biden serves\nas the president of the United States\"). Examples\nhave been shown in Table 7.\nThese approaches focus on modeling text with\na single timestamp. However, the temporal knowl-\nedge stored in knowledge bases usually contains\ntemporal scopes (start and end times). Although\nwe can split temporal scopes into years and jointly\nmodel the years and texts, this splitting process\nwill lead to a massive increase in factual state-\nments that the model needs to memorize, and in-\ntroduce a large amount of conﬂicting information\n(e.g., \"Michael Jordan played for Chicago Bulls\nfrom 1995/1996/1997/1998\"). Section 4.1 has\nshown that conﬂicting information can lead to a\ndecrease in the storage capacity of an LM. There-\n2035\nTable 8: Templates used for converting temporally-scoped knowledge to natural language statements.\nWikidata ID Relation Name # Temporal Knowledge Template\nP54 member of sport team 276633 [X] played for [Y] from [T] to [T]\nP39 position held 227487 [X] held the position of [Y] from [T] to [T]\nP108 employer 25154 [X] worked for [Y] from [T] to [T]\nP166 award received 75027 [X] received [Y] in [T]\nP69 educated at 17842 [X] studied at [Y] from [T] to [T]\nP26 spouse 14645 [X] and [Y] were spouses from [T] to [T]\nP27 country of citizenship 2145 [X] was a citizen of [Y] from [T] to [T]\nTable 9: Example queries for different relations from LAMA-TK. Different from previous work, we mask not only\nthe object, but also the subject and timestamps. Moreover, we reserve all correct answers for each query. [X], [Y],\n[T] refers to the masked subject, object, timestamp respectively. The underlined entities are unmasked entities.\nRelation Name Example Query Masked Entity Answer List\neducated at [X] studied at University of Freiburg from 1928 to 1929 Philip Showalter Hench Philip Showalter Hench, Bernhard Neumann\nposition held Murray Hill held the position of [Y] from 1968 to 1970 Minister for Transport Minister for Transport, Minister of Roads\nemployer Emiliano Aguirre worked for University of Granada from [T] to 1974 1971 1971\nmember of sport team Michael Jordan played for Chicago Bulls from 1984 to [T] 1993 1993\naward received John Bardeen received Nobel Prize in Physics in [T] 1956 1956, 1972\nspouse [X] and Rita Gam were spouses from 1949 to 1955 Sidney Lumet Sidney Lumet\ncountry of citizenship Pasquale Brignoli was a citizen of [Y] from 1861 to 1884 Kingdom of Italy Kingdom of Italy\nfore, we need to ﬁnd a joint modeling method that\ncan preserve the semantic information of temporal\nscopes and reduce the introduction of conﬂicting\ninformation.\nTo this end, we design a prompt-based tem-\nporal scope modeling method. We use prompt\ntemplates such as \" from ST to ET\" and \" in T\"\nto jointly model the temporal scopes and fac-\ntual texts. These prepositions in the prompt tem-\nplates augment the semantic information of times-\ntamps. Section 4.2 shows RoBERTa with prompt-\nbased temporal scope modeling method preserves\nthe temporal boundary of factual knowledge, and\nSection 4.3 shows that with prompt-based tempo-\nral scope modeling method, RoBERTa can under-\nstand the continuity of temporal scopes without\nﬁnetuning. These results provide a proof of con-\ncept that prompt-based template scope modeling\ncan indeed model temporally-scoped knowledge\nwell.\nC Limitations of Top-K Accuracy for\nLM-as-KB Tasks\nThe top-K accuracy metric indicates whether the\ntop k predictions contain correct answers. For ex-\nample, for the query \"John Bardeen received No-\nbel Prize in Physics in [MASK]\", we assume that\nthe model recalls one correct answer \"1956\" in the\ntop 1 and recalls another answer \"1972\" in the top\n100. Even if the model cannot effectively recall\nthe correct answer \"1972\", the Acc@1 and Acc@5\nto this query are still 1. Therefore, for LM-as-\nKB tasks, Acc@K can only indicate whether LMs\ncan correctly answer a query but cannot indicate\nwhether LMs have memorized all correct answers\nto the query.\nIn this paper, we use Hit at top k (Hit@K) to\nevaluate whether LMs have high conﬁdence in all\ncorrect answers. For the above example query, the\nmodel recalls one correct answer \"1956\" at the top\n1 so that Hit@10 for the query \"John Bardeenn\nreceived Nobel Prize in Physics in [MASK]. –>\n1956\" is 1. However, the model recalls another cor-\nrect answer \"1972\" at the top 100 so that Hit@10\nfor the query \"John Bardeen received Nobel Prize\nin Physics in [MASK]. –> 1972\" is 0. Hit@K pro-\nvides a more comprehensive result for queries with\nmultiple answers.\nD Details of Models\nRoBERTa(12L) RoBERTa(12L) has 12 layers,\n768 dimensions, 12 heads, and 125M parameters.\nIts parameters are initialized from huggingface\nRoBERTa-base6.\nRoBERTa(6L) RoBERTa(6L) has 6 layers, 768\ndimensions, 12 heads, and 89M parameters. How-\never, Liu et al. (2019) only provides a 12-layer pre-\ntrained RoBERTa model (RoBERTa-base) and a\n24-layer pretrained RoBERTa model (RoBERTa-\nlarge). Therefore, we initialize RoBERTa(6L)\n6https://huggingface.co/roberta-base\n2036\nwith huggingface DistilRoBERTa-base 7, the dis-\ntilled version of RoBERTa-base. Although\nRoBERTa(6L) is initialized from the distilled ver-\nsion of RoBERTa-base, we do not focus on factual\nknowledge acquired during pre-training. Follow-\ning ( Heinzerling and Inui , 2021), we further train\nLMs on LAMA-TK and only take into account\ntemporal knowledge which is contained in training\ndata.\nRoBERTa-randinit(12L) RoBERTa-\nrandinit(12L) is a randomly initialized 12-layer\nTransformers model, with the same architecture\nas RoBERTa(12L).\nE Reasons for Not Masking Predicate\nIn LAMA-TK, we do not mask the predicate be-\ncause, for most temporal facts, there is a close as-\nsociation between the predicate and the object. For\nexample, given the object \"Nobel Prize in Litera-\nture\", the model will directly predict the masked\nrelation to be \"award received\", since the predic-\ntion for these relations is hardly affected by enti-\nties other than the object.\n7https://huggingface.co/distilroberta-base\n2037",
  "topic": "Memorization",
  "concepts": [
    {
      "name": "Memorization",
      "score": 0.8673502206802368
    },
    {
      "name": "Computer science",
      "score": 0.8301133513450623
    },
    {
      "name": "Domain knowledge",
      "score": 0.6083841323852539
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5380197763442993
    },
    {
      "name": "Function (biology)",
      "score": 0.5000886917114258
    },
    {
      "name": "Knowledge-based systems",
      "score": 0.41520702838897705
    },
    {
      "name": "Knowledge transfer",
      "score": 0.41132861375808716
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3786258101463318
    },
    {
      "name": "Knowledge management",
      "score": 0.2782345712184906
    },
    {
      "name": "Cognitive psychology",
      "score": 0.06594303250312805
    },
    {
      "name": "Psychology",
      "score": 0.06416070461273193
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Evolutionary biology",
      "score": 0.0
    }
  ]
}