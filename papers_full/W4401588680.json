{
    "title": "Targeted Training Data Extractionâ€”Neighborhood Comparison-Based Membership Inference Attacks in Large Language Models",
    "url": "https://openalex.org/W4401588680",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5100775362",
            "name": "Huan Xu",
            "affiliations": [
                "State Grid Corporation of China (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5056503765",
            "name": "Zhanhao Zhang",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5107857964",
            "name": "Xiaodong Yu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5061072868",
            "name": "Yingbo Wu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5060061929",
            "name": "Zhiyong Zha",
            "affiliations": [
                "State Grid Corporation of China (China)"
            ]
        },
        {
            "id": "https://openalex.org/A5108642431",
            "name": "Bo Xu",
            "affiliations": [
                null,
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5084741756",
            "name": "Wenfeng Xu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5058697648",
            "name": "Menglan Hu",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5101861025",
            "name": "Kai Peng",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4401246532",
        "https://openalex.org/W4390547660",
        "https://openalex.org/W4386609108",
        "https://openalex.org/W4401416145",
        "https://openalex.org/W2921459283",
        "https://openalex.org/W6850491769",
        "https://openalex.org/W2945237470",
        "https://openalex.org/W3046764764",
        "https://openalex.org/W4385570888",
        "https://openalex.org/W4383469362",
        "https://openalex.org/W4385573569",
        "https://openalex.org/W6792723844",
        "https://openalex.org/W3188079459",
        "https://openalex.org/W3046102592",
        "https://openalex.org/W4385565597",
        "https://openalex.org/W3178659068",
        "https://openalex.org/W6766503408",
        "https://openalex.org/W3035616549",
        "https://openalex.org/W3082707687",
        "https://openalex.org/W2587741066",
        "https://openalex.org/W2963283805",
        "https://openalex.org/W4317897818",
        "https://openalex.org/W2535690855",
        "https://openalex.org/W2946363484",
        "https://openalex.org/W6635831535",
        "https://openalex.org/W4389564820",
        "https://openalex.org/W1873763122",
        "https://openalex.org/W2983140679",
        "https://openalex.org/W6855185924",
        "https://openalex.org/W3102516861",
        "https://openalex.org/W3138815606",
        "https://openalex.org/W4388049829",
        "https://openalex.org/W3104224589",
        "https://openalex.org/W2965527189"
    ],
    "abstract": "A large language model refers to a deep learning model characterized by extensive parameters and pretraining on a large-scale corpus, utilized for processing natural language text and generating high-quality text output. The increasing deployment of large language models has brought significant attention to their associated privacy and security issues. Recent experiments have demonstrated that training data can be extracted from these models due to their memory effect. Initially, research on large language model training data extraction focused primarily on non-targeted methods. However, following the introduction of targeted training data extraction by Carlini et al., prefix-based extraction methods to generate suffixes have garnered considerable interest, although current extraction precision remains low. This paper focuses on the targeted extraction of training data, employing various methods to enhance the precision and speed of the extraction process. Building on the work of Yu et al., we conduct a comprehensive analysis of the impact of different suffix generation methods on the precision of suffix generation. Additionally, we examine the quality and diversity of text generated by various suffix generation strategies. The study also applies membership inference attacks based on neighborhood comparison to the extraction of training data in large language models, conducting thorough evaluations and comparisons. The effectiveness of membership inference attacks in extracting training data from large language models is assessed, and the performance of different membership inference attacks is compared. Hyperparameter tuning is performed on multiple parameters to enhance the extraction of training data. Experimental results indicate that the proposed method significantly improves extraction precision compared to previous approaches.",
    "full_text": null
}