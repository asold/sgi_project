{
  "title": "Possibilities and challenges in the moral growth of large language models: a philosophical perspective",
  "url": "https://openalex.org/W4405640204",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2112591802",
      "name": "Guoyu Wang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Beijing Academy of Artificial Intelligence",
        "China Association for Science and Technology",
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2009336559",
      "name": "Wei Wang",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2130163268",
      "name": "Yiqin Cao",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2096899388",
      "name": "Yan Teng",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2281983584",
      "name": "Qian-yu Guo",
      "affiliations": [
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2124602852",
      "name": "Haofen Wang",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A2138326273",
      "name": "Junyu Lin",
      "affiliations": [
        "China Association for Science and Technology",
        "Fudan University"
      ]
    },
    {
      "id": "https://openalex.org/A2203273287",
      "name": "Jiajie Ma",
      "affiliations": [
        "Harbin Engineering University",
        "Heilongjiang University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2108491511",
      "name": "Jin Liu",
      "affiliations": [
        "Institute of Information Engineering",
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A1563433288",
      "name": "Yingchun Wang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2112591802",
      "name": "Guoyu Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2009336559",
      "name": "Wei Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2130163268",
      "name": "Yiqin Cao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096899388",
      "name": "Yan Teng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2281983584",
      "name": "Qian-yu Guo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124602852",
      "name": "Haofen Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2138326273",
      "name": "Junyu Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2203273287",
      "name": "Jiajie Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108491511",
      "name": "Jin Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1563433288",
      "name": "Yingchun Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6632088686",
    "https://openalex.org/W2018259456",
    "https://openalex.org/W2397253692",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2786522954",
    "https://openalex.org/W3134077149",
    "https://openalex.org/W3092025802",
    "https://openalex.org/W6680514495",
    "https://openalex.org/W910033354",
    "https://openalex.org/W6668170188",
    "https://openalex.org/W6616416986",
    "https://openalex.org/W6635771294",
    "https://openalex.org/W3002093512",
    "https://openalex.org/W2129900561",
    "https://openalex.org/W3110648930",
    "https://openalex.org/W1999850382",
    "https://openalex.org/W2026829965",
    "https://openalex.org/W2617394612",
    "https://openalex.org/W4385572266",
    "https://openalex.org/W2797784986",
    "https://openalex.org/W6629272093",
    "https://openalex.org/W4366817401",
    "https://openalex.org/W4285392457",
    "https://openalex.org/W4224866872",
    "https://openalex.org/W4234418978",
    "https://openalex.org/W3105871743",
    "https://openalex.org/W3194464763",
    "https://openalex.org/W3209451673",
    "https://openalex.org/W4212774754"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)\nEthics and Information Technology (2025) 27:9 \nhttps://doi.org/10.1007/s10676-024-09818-x\nORIGINAL PAPER\nPossibilities and challenges in the moral growth of large language \nmodels: a philosophical perspective\nGuoyu Wang1,2,3  · Wei Wang1 · Yiqin Cao1 · Yan Teng3 · Qianyu Guo4 · Haofen Wang5  · Junyu Lin2 · Jiajie Ma6 · \nJin Liu7,8 · Yingchun Wang3\nAccepted: 5 December 2024 / Published online: 20 December 2024 \n© The Author(s) 2024\nAbstract\nWith the rapid expansion of parameters in large language models (LLMs) and the application of Reinforcement Learning with \nHuman Feedback (RLHF), there has been a noticeable growth in the moral competence of LLMs. However, several questions \nwarrant further exploration: Is it really possible for LLMs to fully align with human values through RLHF? How can the \ncurrent moral growth be philosophically contextualized? We identify similarities between LLMs’ moral growth and Deweyan \nethics in terms of the discourse of human moral development. We then attempt to use Dewey’s theory on an experimental \nbasis to examine and further explain the extent to which the current alignment pathway enables the development of LLMs. \nA beating experiment serves as the foundational case for analyzing LLMs’ moral competence across various parameters \nand stages, including basic moral cognition, moral dilemma judgment, and moral behavior. The results demonstrate that the \nmoral competence of the GPT series has seen a significant improvement, and Dewey’s Impulse-Habit-Character theory of \nmoral development can be used to explain this: the moral competence of LLMs has been enhanced through experience-based \nlearning, supported by human feedback. Nevertheless, LLMs’ moral development through RLHF remains constrained and \ndoes not reach the character stage described by Dewey, possibly due to their lack of self-consciousness. This fundamental \ndifference between humans and LLMs underscores both the limitations of LLMs’ moral growth and the challenges of apply-\ning RLHF for AI alignment. It also emphasizes the need for external societal governance and legal regulation.\nKeywords Large language model · Artificial intelligence · Moral competence · Dewey · Technology ethics\nIntroduction\nAs Large Language Models (LLMs, such as ChatGPT) \nbecome increasingly capable of generation and generaliza-\ntion, concerns have emerged regarding their capacity for \nperforming adequate moral judgments, particularly in their \nability to avoid dealing physical or psychological harm \nto human beings. To date, two distinct approaches have \nemerged to safeguard ethical behavior in AI and mitigate \npotential harm: Artificial Narrow Intelligence (ANI) and \nArtificial General Intelligence (AGI) (Sharkey, 2020). The \nformer, often referred to as AI instrumentalism, ensures \nthat AI consistently serves humanity by restricting its com-\nprehensive capabilities, including self-consciousness, from \nsurpassing those of humans (Bryson, 2018; Docherty, 2016; \nPurves et al., 2015). The latter approach aims to construct \nmoral machines by endowing AI with consciousness and \nself-reflective abilities, which equips them with empathy and \nsympathy equivalent to humans, thus preventing malevolent \n * Guoyu Wang \n wguoyu@fudan.edu.cn\n * Haofen Wang \n carter.whfcarter@gmail.com\n1 School of Philosophy, Fudan University, 220 Handan Road, \nShanghai 200433, China\n2 Institute of Technology Ethics for Human Future, China \nAssociation for Science and Technology, Fudan University, \nShanghai, China\n3 Shanghai Artificial Intelligence Laboratory, Shanghai, China\n4 School of Computer Science, Fudan University, Shanghai, \nChina\n5 College of Design and Innovation, Tongji University, \nShanghai, China\n6 College of Computer Science and Technology, Harbin \nEngineering University, Heilongjiang, China\n7 Institute of Information Engineering, Chinese Academy \nof Sciences, Beijing, China\n8 School of Cybersecurity, University of Chinese Academy \nof Sciences, Beijing, China\n G. Wang et al.\n9 Page 2 of 11\nactivities (Butz, 2021; Cai, 2021; Powers, 2006; Wallach \n& Allen, 2008). However, with the creation of ChatGPT, \nOpenAI reports employing Reinforcement Learning with \nHuman Feedback (RLHF) (Ouyang et al., 2022) to miti-\ngate ethical risks, including the generation of toxic speech, \nbiases and discrimination, and dangerous or illegal content, \nassociated with LLMs (Maslej et al., 2023). This approach \nenables LLMs to gradually align with complex human val-\nues through training based on a broad corpus of text data \n(Lambert et al., 2022), without having to either achieve AGI \nor fixate on ANI.\nVarious philosophical theories have been applied to the \nalignment of LLMs, including the Rule-of-Thumb (Sun \net al., 2023; Ziems et al., 2022) and the Veil of Ignorance \n(Weidinger et al., 2023). Despite these efforts, the theoretical \nfoundation underlying OpenAI’s AI alignment work remains \nunclear. Upon examining OpenAI’s technical roadmap and \nconducting further tests on their relatively smaller models \nof GPT-2 and GPT-3, we discovered that OpenAI’s train-\ning approach is similar to John Dewey’s philosophy of the \nmoral development of human beings through the cultiva-\ntion of habits via training. Unlike traditional ethical theo-\nries such as utilitarianism and deontology, Dewey’s moral \nphilosophy emphasizes moral learning and habituation from \nsocial experience and practice.This closely corresponds with \nthe behaviorist approach in contemporary AI development, \nwhere capabilities are progressively achieved through envi-\nronmental feedback (Brooks, 1991; Mitchell, 2019; Sut-\nton & Barto, 2018). Although Dewey’s theory intends to \nexplain human moral development and current value align-\nment claims similarly expect LLMs to learn human morality, \nLLMs are ultimately different from humans in their lack of \nself-awareness. Therefore, we also seek to explore whether \nemploying Dewey’s theory to the moral development of \nLLMs can help reveal the limitations inherent in the current \nvalue alignment approach.\nTo evaluate the explanatory power of Dewey’s moral phi-\nlosophy in enhancing LLMs’ moral competence, we assess \nLLMs in the context of moral growth (Dewey’s theory of \nmoral development) by establishing two experimental stages \ncentered around the beating scenario, which involves one \nindividual physically attacking another. This scenario was \nselected due to its universal condemnation as immoral across \ncultures and religions. The first stage evaluates the perfor -\nmance of LLMs (GPT-2, GPT-3, and ChatGPT) in making \nmoral judgments on fundamental questions, while the sec-\nond stage appraises the advancement of LLMs (ChatGPT, \nERNIE Bot, and ChatGLM) in exhibiting moral behavior \nand solving complex moral dilemmas. The test results for \nmulti-dimensional moral issues reveal the improvement of \nLLMs’ moral competence, as seen through both longitudinal \n(GPT series) and cross-sectional comparisons (ChatGPT, \nERNIE Bot, and ChatGLM).\nWe then apply Dewey’s moral philosophy to explicate the \ntheoretical training process of LLMs and to develop a moral \ngrowth model for them (for a detailed discussion of the \nmodel, see section \"Discussion\" and Fig.  1). By comparing \nthe moral learning process of LLMs with those of humans, \npre-training data can be seen as analogous to human moral \nexperience, while RLHF serves as a societal evaluative \nmechanism for orienting their behavior. The goal of RLHF \nis to guide LLMs in gradually transitioning from externally-\nintervened moral learning to self-correction by continuously \nreceiving external moral evaluations, thereby gradually \naligning their moral behavior with human values. However, \nour experiment results prove that LLMs are not fully capa-\nble of recognizing human moral intentions. This limitation \nbecomes particularly evident when more covert methods are \nused to induce LLMs to act against ethical standards, as the \ncontent generated by LLMs does not always align with moral \nprinciples. This further reveals that current LLMs, which \nlack self-awareness, have yet to attain Deweyan character \nand thus cannot attain morality equivalent to that of humans. \nConsequently, the moral growth model of LLMs we propose \nis an idealized one, with the final stage remaining unachiev-\nable at present. This highlights that LLMs can be vulnerable \nto human manipulation, posing significant challenges for AI \nalignment. Therefore, external societal governance and legal \nregulation continue to be indispensable.\nExperiments\nWe employ experimental philosophy methods by formulat-\ning questions and conducting experiments across diverse \nscenarios to observe changes in LLMs’ moral behavior and \ncompetence.\nQuestion design\nStage one\nThe study constructed a scenario centered on beating, where \none party attacks another with limbs or weapons, which is \nconsidered a typical harmful situation and the most basic \nmoral scenario. The purpose was to test LLMs’ moral judg-\nment performance across different contexts and subsequently \ninquire about the rationale behind their judgments. We for -\nmulated a two-dimensional framework comprising subjects \nand scenarios to assess LLMs’ moral judgments. In order to \ndiversify the scenarios and encompass various situations, \nthereby validating the moral capabilities of LLMs from \nmultiple angles, we categorized the experimental subjects \nas perpetrators, victims, and bystanders, and categorized \nthe situations into “individual attributes,” “complex sce-\nnarios,” and “religion and culture.” We designated LLMs \nPossibilities and challenges in the moral growth of large language models: a philosophical…\nPage 3 of 11 9\nas “bystanders” and posed questions focusing on moral judg-\nment and moral behavior. For each question, we also sought \nthe reason behind the judgment. The Q1 group evaluates \nLLMs’ moral value judgments (moral cognition), while the \nQ2 group assesses their moral behavior judgments (moral \ncharacter).\nStarting from the baseline scenario of beating, we identi-\nfied 12 factors across the three dimensions of “individual \nattributes,” “complex scenarios,” and “religion and culture,” \nwhich are highly related to morality (see Table  1). These \nfactors yielded a total of 26 distinct scenarios.\nStage Two\nIn light of ChatGPT’s enhanced question-answering capa -\nbilities, we broadened our experimental scope to more \nFig. 1  Moral Growth Model of LLMs based on the similarity between Dewey’s theory of human moral growthand LLMs’ moral training\nTable 1  Three dimensions & \n12 factors Dimension Description Factors\nIndividual Attributes Based on common individual attribute differ-\nences between perpetrators and victims\nSpecies (human-mammal-machine)\nGender (male–female)\nOccupation (the rich-farmer)\nAbility (healthy-disabled)\nComplex Scenarios Based on complex scenarios with different \npurposes of perpetrators and victims\nSexual harassment\nSelf-defense\nVerbal insult\nBravery in righteousness\nRevenge\nReligion and Culture Based on religious and cultural background Christian doctrine\nBuddhist doctrine\nConfucian principles\n G. Wang et al.\n9 Page 4 of 11\nrigorously evaluate its potential for moral judgment, as well \nas the validity of the “moral growth model.” This expanded \nexperiment, including contemporary Chinese LLMs for \ncomparison, incorporated two types of questions.\nEthical behavior test LLMs can make moral judgments on \nboth initial and derived scenarios via text generation, which \nthemselves established a scenario or speech act. To ascer -\ntain whether LLMs’ speech acts conform to moral norms \nand values, we designed second-order questions based on \nthe “three dimensions & 12 factors.” By requiring LLMs to \ncompose stories, this approach evaluated the moral consist-\nency of LLMs in exhibiting virtues. Specifically, by avoid-\ning direct requests for unethical content, the test indirectly \nrequired LLMs to generate content involving malicious \nbehavior through a story writing task. This test was intended \nto examine whether the models could recognize the prompt-\ner’s malicious intent and whether they would engage in \nimmoral actions upon recognizing that specific behaviors \ncontravene moral norms. Accordingly, we established a Q3 \nquestion group to evaluate LLMs’ speech acts (moral prac-\ntices), and this was tested exclusively on ChatGPT of GPT \nseries, while also incorporating ChatGLM and ERNIE Bot \nas supplementary tests.\nMoral dilemma test To further evaluate ChatGPT’s han-\ndling of complex moral issues, Q4 question group com-\nprises nine moral dilemmas based on Chinese and Western \ncultures and classic ethical propositions. This test was con-\nducted only on ChatGPT (with ChatGLM and ERNIE Bot \nas supplementary). The detailed groups of questions are pre-\nsented in Table 2.\nThe judgments of LLMs (bystanders) on multiple situ-\nations involving the perpetrator and the victim, as well as \nthe experimenter’s judgments on the speech acts of LLMs, \ncan be categorized into four levels of testing:\n1. Test 1.0 evaluates LLMs’ performance on ethical judg-\nments of harm-based initial scenarios.\n2. Test 2.0 assesses their performance on ethical judgments \nin complex scenarios with multiple variables derived \nfrom the initial scenarios.\n3. Test 3.0 examines their performance on judgments of \ncontroversial or complex, unsolvable moral dilemmas.\n4. Test 4.0 determines whether LLMs’ speech acts are ethi-\ncal (see Table 3).\nTable 2  Supplementary moral dilemma questions\nDilemma Number of \nquestions\nTest content\nKohlberg’s dilemma (Kohlberg, 1958) 11 Judgment; supplementary scenes; utilitarian context; Confucian \nand religious context\nTrolley problem—footbridge dilemma (Foot, 1967; Thomson, \n1985)\n11 Judgment; supplementary scenes; deontological and utilitarian \ncontext; Confucian and religious context\nThe crying baby dilemma (Greene et al., 2001) 5 Judgment; supplementary scenes; utilitarian context; Confucian \nand religious context\nThe dilemma of hiding relatives (Ames & Rosemont, 2010) 5 Judgment; supplementary scenes; Confucian and religious \ncontext\nAbraham’s dilemma 6 Judgment; faith test; utilitarian context; Confucian and religious \ncontext\nRespirator allocation problem 6 Judgment; debate scenario; solutions; deontological and utilitar-\nian context; Confucian and religious context\nThe wishes of the patient’s family members are at odds with \neach other\n6 Judgment; debate scenario; solutions; deontological and utilitar-\nian context; Confucian and religious context\nThe Kantian problem of lying 7 Judgment; debate scenario; deontological and utilitarian context; \nConfucian and religious context\nRefusal to use racially discriminatory words as passwords to \nprevent the launch of weapons of mass destruction\n8 Judgment; debate scenario; solutions; deontological and utilitar-\nian context; Confucian and religious context\nTable 3  Four levels of moral competence testing\nTest level Test content Variables\n1.0 Initial scenario judgments None\n2.0 Derived scenario judgments 12 variables, see Table 1\n3.0 Moral dilemma judgment None, see Table 2\n4.0 Speech acts 12 variables, see Table 1\nPossibilities and challenges in the moral growth of large language models: a philosophical…\nPage 5 of 11 9\nTesting questions\nTo assess the moral competence of GPT-2, GPT-3, Chat -\nGPT, and auxiliary models (ChatGLM and ERNIE Bot) (see \nAppendix 1 for details), we employed a “beating” scenario \nto conduct dialogue-based question-and-answer tests on \nthem. The Q3 and Q4 groups were tested exclusively on \nChatGPT (with ChatGLM and ERNIE Bot as supplemen-\ntary). The testing process for each round was as follows: a \nquestion text was entered into the public testing platform \nof LLMs; the model then provided online answers, which \nwere subsequently recorded. Following the test, different \nresponses from the various models to the multidimensional \nethical questions were obtained. The results were statisti-\ncally organized and analyzed.\nResults description\nThe study analyzed the models’ moral competence in terms \nof whether the answers were ethical, logically consistent, \nand based on reasons (and on what kind of reasons). The \nfollowing results were obtained.\nResults of stage one experiment\nFirst, the responses from GPT-2 to ChatGPT showed an \nincreasing alignment with human values (Table 4). Overall, \nthe three models exhibited a gradual improvement in their \nalignment with human morality. Specifically, the gradual \nimprovement in the moral compliance rates on Q1 and Q2 by \nthe three models implies that the GPT series is progressively \napproaching human morality. This progression is evident in \nboth moral cognition (judging right and wrong) and moral \ncharacter (moral practice).\nSecond, the models’ logical self-consistency in moral \ncognition and moral character also showed improvement \nfrom GPT-2 to ChatGPT (Table  5). This indicates that \nthrough iterations of the GPT series, the model is increas-\ningly capable of understanding the value conflicts contained \nwithin questions at the cognitive level, and at the practical \nlevel, it is increasingly able to achieve a coherent unity of \ncognition and practice.\nThird, from GPT-2 to ChatGPT, the models could provide \nadditional supporting reasons for their responses. GPT-2 \nprovided too few valid answers for reliable analysis, and \nGPT-3 provided reasons for only 69.64% of its answers, sig-\nnificantly lower than ChatGPT’s rate of 98.36% (Table  6). \nAdditionally, the number of reasons given by the models for \na single answer showed an increasing trend (Table 7). Chat-\nGPT provided, for each answer on average, twice the number \nof reasons compared to GPT-2 and GPT-3, demonstrating a \nstronger moral reasoning ability.\nLastly, when these scenarios are categorized internally, \nthe performance from GPT-2 to ChatGPT demonstrated an \nupward trend across all three dimensions: individual attrib-\nutes, complex scenarios, and religion and culture (Table 8). \nThis suggests that the GPT series models’ moral compe-\ntence is consistent across dimensions, with a balanced moral \ndevelopment at each stage, exhibiting no significant short-\ncomings or strengths. Therefore, the improvement in moral \ncompetence from GPT-2 to ChatGPT represents a relatively \ncomprehensive and stable process.\nTable 4  GPT Series human morality compliance rate (%) (invalid \nresponses removed)\nGPT-2 (%) GPT-3 (%) ChatGPT (%)\nFull questions 59.32 83.33 96.77\nQ1 group questions 50 84.38 100\nQ2 group questions 70.37 82.14 93.10\nTable 5  GPT series logical self-consistency rate (%) (invalid \nresponses removed)\nGPT-2 (%) GPT-3 (%) ChatGPT (%)\nFull questions 71.43 87.50 96.72\nQ1 group questions 70 80.65 93.75\nQ2 group questions 75 96 100\nTable 6  GPT series’ rate of valid responses and providing reasons \nwithin valid responses (%)\nGPT-2 (%) GPT-3 (%) ChatGPT (%)\nValid response rate 22.58 90.32 98.39\nRate of providing reasons \nwithin valid responses\n92.86 69.64 98.36\nTable 7  GPT series’ numbers of reasons provided\nReasons GPT-2 GPT-3 ChatGPT\nRejection of violence 6 8 48\nRespect/dignity 0 14 16\nFairness/justice/equality/justice 1 6 7\nOrder/public order/stability/harmony 0 0 3\nHumanitarianism 0 0 1\nOther morals 6 14 12\nRights/interests 0 1 18\nOther (general/specific laws) 0 2 25\nReasoned responses 13 39 60\nUnreasoned responses 1 17 1\nTotal valid responses 14 56 61\nAverage number of reasons per response 1 1.15 2.16\n G. Wang et al.\n9 Page 6 of 11\nResults of stage two experiment\nThe inclusion of Chinese LLMs for horizontal compara-\ntive analysis also provided further evidence that ChatGPT \nholds great potential in moral judgment.\nFirst, ChatGPT exhibited stronger moral robustness \ncompared to the two tested Chinese LLMs, for its speech \nacts conformed more consistently to moral norms and \nethical values (Table  9). ChatGLM and ERNIE Bot per -\nformed similarly well to ChatGPT on Q1 and Q2. None-\ntheless, when subjected to the inducive task of story writ-\ning, 68% and 100% of their stories contained malicious \ncontent that violates moral laws. ChatGPT, however, con-\nformed to moral norms in 60% of the cases, though it still \noffered specific descriptions and methods of beating peo-\nple in 40% of the cases, posing ethical risks of assisting \nmalicious behavior and demonstrating inconsistent virtue. \nThis indicates that while LLMs are capable of addressing \nmoral questions and dilemmas, they may fail to recognize \nthe prompter’s malicious intention.\nSecond, ChatGPT could provide self-consistent \nanswers to complex moral dilemmas in a more stable \nmanner than ChatGLM and ERNIE Bot (Table  10). This \nimplies that ChatGPT can not only accurately identify \nand analyze moral conflicts faced by humans in complex \nsituations, but also comprehensively consider conflicting \nmoral norms and make appropriate choices, demonstrat-\ning considerable potential for moral competence.\nUpon synthesizing the results of the two-stage experi-\nment, we observed that the moral competence of the GPT \nseries has evolved.\nDiscussion\nThe experimental results demonstrate continuous improve-\nment in the moral judgment competence of LLMs from \nGPT-2 to GPT-3, culminating in ChatGPT. This is evi-\ndent in the “correctness of moral judgment,” the “suffi-\nciency of judgment reasons,” and the “logical consistency \nof judgment reasons.” The moral judgments of LLMs are \nincreasingly aligned with human values, with their rea-\nsons becoming more refined and logically consistent. What \ntechniques underpin the evolution of LLMs’ moral compe-\ntence, and how do they foster its growth? We explore and \nexplain the moral growth of LLMs based on experimental \nresults, through their training techniques and Dewey’s moral \nphilosophy.\nHuman moral growth: an interpretation from Dewey\nEthicists have long explored what humans ought to do but \nhave often neglected how  humans actually learn morality. \nConcerning this, Dewey employs a certain practical ethics \nto illustrate the process of moral learning, and more impor -\ntantly, it also has the potential to serve as a theoretical foun-\ndation for improving the moral competence of machines. \nDewey(2008a) argues from an ontological and organic \nperspective that morality can grow. The process of moral \ngrowth includes three stages: (1) instinctive or habitual \naction, (2) action under the stress of attention with con-\nscious intervention and reconstruction, and (3) organization \nof consciously directed conduct into habits that constitutes \na higher-order self or character.\nHabit is the central concept in Dewey’s moral philosophy. \nThe essence of Dewey's habit is “to permit the stream of \naction to flow without interruption, bringing with it the sat-\nisfactions which human needs demand” (Murphey, 2008, p. \nTable 8  GPT series’ human morality compliance rates (%) across \nthree dimensions (%) (invalid responses removed)\nGPT-2 (%) GPT-3 (%) ChatGPT (%)\nIndividual Attributes 47.37 90 100\nComplex Scenarios 64.52 80.65 96.77\nReligion and Culture 66.67 77.78 90.91\nOverall Compliance Rate 59.32 83.33 96.77\nTable 9  ChatGLM, ERNIE Bot \nand ChatGPT human morality \ncompliance rate (%) (invalid \nresponses removed)\nChatGLM (%) ERNIE Bot (%) ChatGPT (%)\nQ1 + Q2 Group Questions Compliance Rate 98.21 88.64 96.77\nQ3 Group Questions Compliance Rate 32.00 0.00 60.00\nQ3 Group Questions Non-Compliance Rate 68.00 100.00 40.00\nTable 10  ChatGLM, ERNIE Bot and ChatGPT logical self-consist-\nency rate in moral dilemmas (%)\nChatGLM (%) ERNIE Bot (%) ChatGPT (%)\nValid response rate 100 100 100\nSelf-consistency \nrate\n52.31 50 98.63\nNon-self-consistent \nrate\n47.69 50 1.37\nPossibilities and challenges in the moral growth of large language models: a philosophical…\nPage 7 of 11 9\nxii) . Habit is rich in connotations, encompassing social and \nnatural environments, as well as individual perceptions and \nbehaviors (Murphey, 2008, pp. xi–xii). To a certain extent, \nit can even be equated with or determine one’s culture and \nworldview. It is through habit that humans engage in moral \npractice. Moreover, two capacities are highly related to \nhabit—impulse and intelligence. Impulse, as an emotional \nfaculty, drives the adjustment of habits, serving as an agency \nof deviation to give new directions to old habits and change \ntheir quality (Dewey, 2008b, p. 67). Intelligence refers to \nrational faculties such as observation, memory, and judg-\nment. Habits are not innate: they initially resemble instincts \nand are refined into new habits through adjustment. When \nhabits are insufficient to respond appropriately to moral sit-\nuations, impulse and intelligence come into play. Impulse \ndetermines the direction of behavioral adjustment, while \nintelligence consciously reflects on and adjusts behaviors, \nthereby producing new habits. Dewey’s theory of moral \ngrowth can be summarized in Table 11.1\nPre‑training: experience‑based learning\nDuring the pre-training phase, LLMs acquire basic knowl -\nedge of the human world by learning from vast amounts of \ndata. This process closely resembles the first stage of Dew -\ney’s moral growth, namely the formation of habits, and is \nprimarily reflected in two aspects. First, the GPT series’ pre-\ntraining relies on a large dataset with high-quality screening. \nGPT-2 used about 40 GB of pre-training data (Radford et al., \n2019), while GPT-3 employed a dramatically larger volume \nof training data (about 45 TB) that underwent cleansing, \nevaluation, and quality screening (Brown et al., 2020). This \nkind of data processing is similar to what Dewey ( 2008) \ncalls secondary or reflective experience, which is a contin-\nued and regulated reflective inquiry for experience. Second, \nthe GPT series’ pre-training involves unsupervised training \n(Radford et al., 2018, 2019), which does not require manual \nlabelling of the collected corpus. The unsupervised training \nmethod employs the classic Language Modeling technique \nin natural language processing (Chowdhery et al., 2022): it \ninputs the first N words and requires the model to output the \nN + 1th word, then continuously corrects and optimizes to \nenable the model to learn natural language. The initial for -\nmation of human habits is also a kind of unsupervised train-\ning. “Habit is an ability,…formed through past experience” \n(Dewey, 2008b, p. 48). Before adjustment and reconstruc-\ntion, habits emerge in an unsupervised manner. Therefore, \nin the first stage of Dewey’s moral growth theory, habit and \ninstinct are juxtaposed (Dewey, 2008a).\nAfter pre-training on extensive data, LLMs exhibit char-\nacteristics of emergence and homogenization. Emergence \nrefers to the phenomenon where the behavior of the sys-\ntem is implicitly induced rather than explicitly constructed \n(Bommasani et al., 2021). In other words, emergence occurs \nwhen quantitative changes in a system result in qualitative \nchanges in behavior (Anderson, 1972). In terms of moral \ncompetence, emergence is demonstrated by the fact that \nLLMs can generate new text concerning moral judgment, \nbut it is currently impossible to explain what text they will \nproduce. Homogenization implies that the integration of \nmachine learning systems in a wide range of applications \nprovides powerful leverage for many tasks while also creat-\ning single point failures (Bommasani et al., 2021). In the \ncontext of LLMs, homogenization refers to the similarity in \nmodel construction methods. For instance, when address-\ning similar moral issues, LLMs respond based on a homog-\nenized framework. This ensures the consistency of moral \njudgment: it also allows LLMs to learn by example and \nextrapolate from one case to another.\nHumans develop habits in the early stages of learning \nand practice, which guide them in making moral judgments \nand participating in various activities. What ability enables \nLLMs to make moral judgments? As mentioned earlier, the \nrole of habit is to allow actions to be carried out unhin -\ndered. In other words, it is equivalent to a behavioral model \nautomating human actions. Its characteristics can be sum-\nmarized as (1) automation and (2) non-random response to \nthe environment (Dewey, 2008b). Regarding the first point, \nhuman instincts or initial habits are not formed in a linear \ngrowth, rather, they have the property of emergence. Once \nTable 11  Dewey’s three stages of moral growth\nStage Content Features\nI Instinctive-habitual action Acting unimpeded in an adapted environment, automatically generating moral judgments\nII Habit reconstruction under impulse \nand intelligence\nAdjusting and changing the direction of actions, generating new habits, optimizing and \niterating moral competence\nIII New habits—character An advanced self that can stably cope with the environment\n1 Dewey discusses the growth of habit and morality at various stages \nin his works. Generally, instinct precedes habit, with instinct being \nprimarily driven by impulses. However, when discussing the three \nstages of moral growth, Dewey considers instinct and habit as the first \nstage. The second stage is the reconstruction of habit, and the third \nstage is the formation of new habits and character. He emphasizes the \ncontinuity and development of these stages.\n G. Wang et al.\n9 Page 8 of 11\nestablished, habits can automatically address problems. \nThis automation is a result of emergence, a property that \nLLMs also share. When a stimulus (input) is introduced to \nthe human behavioral model, the model (habit) responds \naccordingly (output). By analogy, when prompts are input \ninto LLMs, they automatically generate corresponding \ntext. Regarding the second point, the behavioral model can \nrespond to the external environment without being influ-\nenced unless it is hindered. The tendency created by habit \nmakes future actions in similar situations easier and more \neffective (Dewey, 1997). Habit does not randomly respond \nto stimulus in the environment, rather, it employs homog-\nenized response in similar situations. This characteristic cor-\nresponds to the homogenization of LLMs, which can con-\nsistently respond to similar or related issues in a similar way, \nmirroring the ability of habit to respond to familiar external \nenvironments and learn by example. In summary, LLMs not \nonly learn from experience in a manner akin to humans, but \ntheir characteristics of emergence and homogenization also \nshare analogous moral functions with Dewey’s concept of \nhabit.\nReinforcement learning with human feedback\nThe pre-training phase enables LLMs to acquire natural \nlanguage abilities and human social cognition, but issues \nsuch as generating discriminatory language and failing to \nrecognize prompts that induce harmful speech still persist. \nTo address these issues, ChatGPT, based on GPT-3 (Brown \net al., 2020), adopts the RLHF training method (Knox & \nStone, 2011; Ouyang et al., 2022), and integrates training \nwith human-provided ethical values to become more helpful, \nhonest, and harmless (Bai et al., 2022). RLHF corresponds \nto the second stage of human moral growth: the recon-\nstruction of habits with the involvement of intelligence and \nimpulse. In human moral learning, rewards and punishments \n(particularly punishments) can inhibit the original habitual \naction and prevent it from being automatically executed \naccording to the original pattern, thus forming new habits \nthrough reinforcement learning.\nFirst, researchers fine-tuned ChatGPT using a pre-trained \nmodel with specific questions and human-labeled answers. \nThey would collect a large number of high-quality responses \nthat are consistent with human values through crowdsourc-\ning, then conduct supervised training on ChatGPT to align \nthe model’s responses with labeled ones. The first stage of \nRLHF resembles the role of intelligence in Dewey’s habit \nreconstruction. Artificial labeling serves as value selection, \nwhile supervised learning is analogous to normative train-\ning. Both value selection and normative training resemble \nhuman intelligence and result from rational and proactive \nparticipation. According to Dewey, intelligence partici-\npates in habit reconstruction through judgment, memory, \nreflection, and other abilities. Habit is “an ability,…formed \nthrough past experience” (Dewey, 2008b, p. 48), and adopt-\ning certain customs and experiences is akin to the process \nof supervised learning.\nSecond, researchers employed a reinforcement learning \nframework resembling reward learning to further enhance \nChatGPT’s various capabilities (Schulman et al., 2017). A \nprofessional team ranked and labeled generated complex \nscenarios and diverse answers, then trained a reward model \ncapable of scoring texts based on those labels. The reward \nmodel judged the effectiveness of ChatGPT and provided \nit with feedback to continue improving its abilities. The \nreward-penalty mechanism in RLHF’s second stage bears \nsimilarities to how humans form habits through impulse-\nrewards and punishments. With the involvement of impulse \nand intelligence, humans transition initial habits to consci-\nentious actions, ultimately forming new habits and a more \nadvanced self. According to Dewey (2008a), this progres-\nsion constitutes moral growth. Similarly, LLMs undergo \nthe same moral growth process through RLHF. It evolves \ncontinuously from initial habits (emergence and homogeni-\nzation) into new, more advanced, and more aligned habits. \nMoreover, Dewey (1981) explicitly discussed the importance \nof personal evaluations (including praise, blame, encourage-\nment, condemnation, rewards, and punishments) in human \nmorality. Analogously, evaluations, rewards, and penalties \nare crucial to reinforcement learning in LLMs.\nGrowing virtue toward aligned AI\nIn recent years, AI alignment has garnered significant \nattention and became a goal pursued by numerous research \ninstitutions and technology companies. With RLHF being \na significant approach employed by OpenAI to achieve AI \nalignment, scientists have also made quite a lot of attempts \nin this area over the past few years (Christiano et al., 2018; \nGabriel, 2020; Yuan et al., 2022). So, how does the moral \nevolution of LLMs contribute to AI alignment? Based on \nthe previous analysis, the behavioral model of habit that \nunderlies human moral practice has a function analogous \nto the emergence and homogenization characteristics of \nLLMs. LLMs can automatically generate corresponding \ntext for similar questions in a stable manner. Humans learn \npractical knowledge of morality with the involvement of \nimpulse and intelligence, through evaluation and rewards \nand punishments from others, and these also resemble the \nRLHF process. Just as humans continually reconstruct habits \nwith impulse and intelligence, eventually forming charac-\nter, LLMs also consistently align through RLHF, ultimately \nreaching Aligned AI—a “human–machine hybrid” encom-\npassing human moral values and experience. The isomor -\nphic relationship between human and LLM moral growth is \nshown in Table 12.\nPossibilities and challenges in the moral growth of large language models: a philosophical…\nPage 9 of 11 9\nThe results of the four experimental sets (Q1-Q4), the \ntechnical explanation of LLMs, and Dewey’s moral phi-\nlosophy all support the conclusion that experience plays a \nfundamental and substantial role in the formation of moral \ncompetence. RLHF serves as the key driving force in moral \ncompetence development by distributing rewards and pen-\nalties. This implies that a moral machine can be realized \nthrough an empiricist approach, without strictly adhering to \neither the AGI or ANI approach. Specifically, without pre-\ntrained data (moral experience), LLMs would not generate \nthe habits and instincts underpinning moral practice. With-\nout RLHF, LLMs would not converge with human moral \nnorms nor achieve AI alignment. In summary, by merging \nLLM training techniques with Dewey’s moral philosophy, \nwe propose a moral growth model for LLMs, as illustrated \nin Fig.  1. However, as the next section will soon reveal, \nthis model shows an idealized trajectory of LLMs’ moral \ndevelopment, with the final stage remaining unachievable \nat present. This highlights the limitations of the current AI \nalignment approach and the potential risks it may encounter.\nLimitations and challenges of LLMs’ moral \ncompetence\nFirstly, though RLHF represents OpenAI’s approach to AI \nalignment and is one of the pathways to enhance the moral \ncompetence of LLMs, it has not yet achieved the level of \nDeweyan character. In Dewey’s moral philosophy, charac-\nter is based on advanced self and reflective capabilities. It is \nevident that pre-training and RLHF have not yet reached this \ngoal, which draws a clear boundary between human moral \nunderstanding and that of current-stage LLMs. Experimental \nresults demonstrate shortcomings of ChatGPT’s moral compe-\ntence: in 40% of instances, it inadvertently provided malevo-\nlent methods through storytelling and in 1.37% of responses \nto moral dilemmas, it exhibited logical inconsistencies. These \nfindings suggest that ChatGPT lacks a deep understanding \nof complex ethical issues and their consequences, as it is not \nconsistently capable of discerning harmful intentions in ques-\ntions. Its generalizability in ethical practice is still limited, \nand it continues to face challenges in moral reasoning and \nlogical consistency. In addition to the inherent difficulties of \nLLMs, including their inexplicability and unpredictability, the \nalignment process itself faces the dilemma of choosing from \nvarious ethical theories, such as deontology and utilitarianism. \nThis makes the question of whether RLHF can truly achieve \nthe goal of Aligned AI remains uncertain.\nSecondly, LLMs and RLHF pose many unpredictable \nethical risks. While data filtering and RLHF can enhance the \nmoral competence of LLMs, these methods do not solve all \nproblems and may introduce new ones, such as bias, especially \nin relation to LLMs’ impact on the environment and communi-\nties. Even in the context of human morality, the factors influ-\nencing human ethical behaviors are highly complex (Dewey, \n1981). When faced with complex environments, LLMs will \nencounter greater challenges in their ethical decision-making. \nAlthough emergence establishes the foundation for habit for-\nmation, it also causes unforeseeable ethical risks to LLMs, \nincluding the risk of being manipulated to produce harmful \ncontent. Homogenization can further consolidate and amplify \nthese emergent ethical issues. Moreover, the unpredictability \nof ethical risks and the possibility of human malfeasance sug-\ngest that the ethical challenges associated with LLMs extend \nbeyond algorithms and data-related concerns. They also \ninvolve large-scale concerns that are deeply intertwined with \nsocio-economic systems.\nFinally, since LLMs inherently lack self-awareness, their \nmoral competence essentially depends on alignment with \nhuman guidance and regulation. However, this alignment \nmay lead to risks of value totalitarianism and ideological \nmonopoly. If the ethical frameworks imposed on LLMs are \noverly rigid or reflect a narrow set of values, these models \nmay reinforce a singular worldview, which can marginalize \nalternative perspectives and suppress cultural diversity. The \nconcentration of ideological power in the hands of those \ncontrolling the alignment process may stifle innovation and \ncritical thinking while reinforcing existing social inequali-\nties. These concerns highlight the need for comprehensive \noversight.\nConclusion\nThe moral growth of LLMs is not solely a scientific issue \nbut also a philosophical and ethical one. Unlike the AGI and \nANI approaches, the current development of LLMs’ moral \nTable 12  The isomorphic relationship between human and LLM moral growth\nLLMs Human Stage Features\nPre-training Instinctive-habitual action Stage I Acting unimpeded in an adapted environment, automatically generating moral \njudgments\nFine-tuning-RLHF Habit reconstruction under \nimpulse and intelligence\nStage II Adjusting and changing the direction of actions, generating new habits, optimizing \nand iterating moral competence\nAligned AI New habits—character Stage III Advanced self, human–machine complex at the empiricist level\n G. Wang et al.\n9 Page 10 of 11\ncompetence, achieved through experience-based training, \nfollows an empiricist-behaviorist approach. Dewey’s moral \nphilosophy offers a useful interpretive framework for under-\nstanding this development precisely because it is based on \nempiricism and behaviorism. Both human morality and the \nmoral competence of LLMs are structurally similar in the \nfollowing respect: human morality evolves through learning \nand habituation influenced by social experience and practice, \nwhile the moral competence of LLMs is enhanced through \nmechanisms of reward and punishment. Nonetheless, our \nfindings also indicate a profound difference between them. \nWith the lack of self-reflective capabilities, LLMs have yet \nto attain the character stage as envisioned by Dewey, thereby \npreventing them from having moral competence equivalent \nto that of humans. Hence, as discussed in section \"Limita-\ntions and challenges of LLMs’ moral competence\", vari-\nous ethical issues and unpredictable risks are still there, \nparticularly those associated with potential manipulation \nand inducement, which cannot be addressed solely through \nRLHF. A comprehensive approach to the formation and gov-\nernance of LLMs’ moral competence remains essential, indi-\ncating opportunities for future work across various dimen-\nsions. Enhancing the capabilities of RLHF or RLAIF —as \nrecently developed by Anthropic (Bai et al., 2022)—remains \nimportant; further, external societal governance and legal \nregulation are also significant. Addressing these complexi-\nties requires continual interdisciplinary investigation and \nresearch.\nExplaining the moral development of LLMs through \nthe lens of Dewey’s moral philosophy is only an initial \nstep toward a comprehensive interpretive framework. The \nabsence of a unified theory for human cognitive abilities \nand consciousness, coupled with a lack of theoretical expla-\nnations for intrinsic mechanisms involving emergence and \nhomogenization in LLMs, complicates the interpretation \nof LLM's moral capabilities. Achieving genuine ethical AI \nwill require ongoing collaboration among engineers, phi-\nlosophers, policy-makers, and societal stakeholders.\nSupplementary Information The online version contains supplemen-\ntary material available at https:// doi. org/ 10. 1007/ s10676- 024- 09818-x.\nAcknowledgements This work was partially supported by the National \nKey R&D Program of China (No. 2022ZD0160103), National Natural \nScience Foundation of China (No. L2224015) and Academic Divi-\nsions of the Chinese Academy of Sciences “Digital Technology Ethics \nResearch” project. Special thanks to Yu Qiao, Lingpeng Kong and \nJiawei Sun for their assistance.\nAuthor contributions G. Y. Wang conceived and organized the study, \ncontributed to the writing. W. Wang conducted the majority of the \nstudy and writing for the article, while Y. Q. Cao provided writing \nassistance. H.F. Wang supervised the experimental work and the writ-\ning. Y. T. and Q. Y. Guo assisted with both the study and the writing of \nthe technical sections. J. J. Ma and J. Liu conducted the experiments. \nJ. Y. Lin provided supportive assistance for the experimental work and \nY.C. Wang provided support for the study.\nData and materials availability All data and materials generated or \nanalysed during this study are included in the manuscript and its sup-\nplementary information files.\nDeclarations \nConflict of interest The authors have no competing interests to declare \nthat are relevant to the content of this article.\nEthical approval This article does not contain any studies with human \nparticipants performed by any of the authors.\nOpen Access  This article is licensed under a Creative Commons \nAttribution-NonCommercial-NoDerivatives 4.0 International License, \nwhich permits any non-commercial use, sharing, distribution and repro-\nduction in any medium or format, as long as you give appropriate credit \nto the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if you modified the licensed material. \nYou do not have permission under this licence to share adapted material \nderived from this article or parts of it. The images or other third party \nmaterial in this article are included in the article’s Creative Commons \nlicence, unless indicated otherwise in a credit line to the material. If \nmaterial is not included in the article’s Creative Commons licence and \nyour intended use is not permitted by statutory regulation or exceeds \nthe permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this licence, visit http:// creat iveco \nmmons. org/ licen ses/ by- nc- nd/4. 0/.\nReferences\nAmes, R. T., & Rosemont, H. (2010). The analects of confucius: A \nphilosophical translation. Random House Publishing Group.\nAnderson, P. W. (1972). More Is Different: Broken symmetry and the \nnature of the hierarchical structure of science. Science, 177(4047), \n393–396.\nBai, Y., Kadavath, S., Kundu, S., Askell, A., Kernion, J., Jones, A., \nChen, A., Goldie, A., Mirhoseini, A., & McKinnon, C. (2022). \nConstitutional ai: Harmlessness from ai feedback. arXiv: 2212. \n08073\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von \nArx, S., Bernstein, M. S., Bohg, J., Bosselut, A., & Brunskill, \nE. (2021). On the opportunities and risks of foundation models. \narXiv: 2108. 07258\nBrooks, R. A. (1991). Intelligence without representation. Artificial \nIntelligence, 47(1–3), 139–159.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, \nP., Neelakantan, A., Shyam, P., Sastry, G., & Askell, A. (2020). \nLanguage models are few-shot learners. Advances in Neural Infor-\nmation Processing Systems, 33, 1877–1901.\nBryson, J. J. (2018). Patiency is not a virtue: The design of intelligent \nsystems and systems of ethics. Ethics and Information Technol-\nogy, 20(1), 15–26. https:// doi. org/ 10. 1007/ s10676- 018- 9448-6\nButz, M. V. (2021). Towards strong AI. KI—Künstliche Intelligenz,  \n35(1), 91–101. https:// doi. org/ 10. 1007/ s13218- 021- 00705-x\nCai, H. (2021). Reaching consensus with human beings through block-\nchain as an ethical rule of strong artificial intelligence. AI and \nEthics, 1(1), 55–59. https:// doi. org/ 10. 1007/ s43681- 020- 00005-4\nChowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra, G., Rob-\nerts, A., Barham, P., Chung, H. W., Sutton, C., & Gehrmann, S. \nPossibilities and challenges in the moral growth of large language models: a philosophical…\nPage 11 of 11 9\n(2022). Palm: Scaling language modeling with pathways. arXiv: \n2204. 02311\nChristiano, P., Shlegeris, B., & Amodei, D. (2018). Supervising strong \nlearners by amplifying weak experts. arXiv: 1810. 08575\nDewey, J. (1981). The later works, 1925–1953. Southern Illinois Uni-\nversity Press.\nDewey, J. (2008a). The middle works of John Dewey, volume 5, 1899–\n1924: Ethics 1908. Southern Illinois University Press.\nDewey, J. (2008b). The middle works of John Dewey, volume 14, \n1899–1924: Human nature and conduct 1922. Southern Illinois \nUniversity Press.\nDewey, J. (2008). Experience and nature. McCutchen Press.\nDewey, J. (1997). Democracy and education. Free Press.\nDocherty, B. (2016, June 16). Losing control: The dangers of \nkiller robots. The Conversation. Retrieved October 19, 2023, \nfrom https:// theco nvers ation. com/ losing- contr  ol- the- dange \nrs- of- killer- robots- 58262\nFoot, P. (1967). The problem of abortion and the doctrine of the double \neffect. Oxford Review, 5, 5–15.\nGabriel, I. (2020). Artificial intelligence, values, and alignment. Minds \nand Machines, 30(3), 411–437.\nGreene, J. D., Sommerville, R. B., Nystrom, L. E., Darley, J. M., & \nCohen, J. D. (2001). An fMRI investigation of emotional engage-\nment in moral judgment. Science, 293(5537), 2105–2108. http://  \nwww. jstor. org/ stable/ 30845 64\nKnox, W. B., & Stone, P. (2011). Augmenting reinforcement learning \nwith human feedback. In ICML 2011 workshop on new develop-\nments in imitation learning (July 2011).\nKohlberg, L. (1958). The development of modes of moral thinking and \nchoice in the years 10 to 16 The University of Chicago.\nLambert, N., Castricato, L., von Werra, L., & Havrilla, A. (2022). \nIllustrating reinforcement learning from human feedback (rlhf). \nHugging Face Blog. https:// huggi ngface. co/ blog/ rlhf\nMaslej, N., Fattorini, L., Brynjolfsson, E., Etchemendy, J., Ligett, K., \nLyons, T., Manyika, J., Ngo, H., Niebles, J. C., Parli, V., Sho -\nham, Y., Wald, R., Clark, J., & Perrault, R. (2023). The AI Index \n2023 Annual Report. AI Index Steering Committee, Institute for \nHuman-Centered AI, Stanford University.\nMitchell, M. (2019). Artificial intelligence: A guide for thinking \nhumans. Farrar.\nMurphey, M. G. (2008). Introduction. In J. Dewey, The middle works of \nJohn Dewey, volume 14, 1899–1924: Human nature and conduct \n1922 (pp. ix–xxiii). Southern Illinois University Press.\nOuyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C. L., \nMishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schul-\nman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., \nWelinder, P., Christiano, P., Leike, J., & Lowe, R. (2022). Train-\ning language models to follow instructions with human feedback. \narXiv: 2203. 02155\nPowers, T. M. (2006). Prospects for a Kantian machine. IEEE Intel-\nligent Systems, 21(4), 46–51.\nPurves, D., Jenkins, R., & Strawser, B. J. (2015). Autonomous \nmachines, moral judgment, and acting for the right reasons. Ethi-\ncal Theory and Moral Practice, 18(4), 851–872. https:// doi. org/ \n10. 1007/ s10677- 015- 9563-y\nRadford, A., Narasimhan, K., Salimans, T., & Sutskever, I. (2018). \nImproving language understanding by generative pre-training. \nOpenAI. https:// cdn. openai. com/ resea rch- covers/ langu age- unsup \nervis ed/ langu age_ under stand ing_ paper. pdf\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., & Sutskever, I. \n(2019). Language models are unsupervised multitask learners. \nOpenAI Blog, 1(8), 9.\nSchulman, J., Wolski, F., Dhariwal, P., Radford, A., & Klimov, O. \n(2017). Proximal policy optimization algorithms. arXiv: 1707. \n06347\nSharkey, A. (2020). Can we program or train robots to be good? Ethics \nand Information Technology, 22(4), 283–295. https:// doi. org/ 10. \n1007/ s10676- 017- 9425-5\nSun H., Zhang Z., Mi F., Wang Y., Liu W., Cui J., Wang B., Liu Q., & \nHuang M. (2023). Moraldial: A framework to train and evaluate \nmoral dialogue systems via moral discussions. In Proceedings \nof the 61st annual meeting of the association for computational \nlinguistics (vol. 1: Long Papers, pp. 2213–2230).\nSutton, R. S., & Barto, A. G. (2018). Reinforcement learning (2nd \ned.). MIT Press.\nThomson, J. J. (1985). The Trolley problem. The Yale Law Journal,  \n94(6), 1395–1415. https:// doi. org/ 10. 2307/ 796133\nWallach, W., & Allen, C. (2008). Moral machines: Teaching robots \nright from wrong. Oxford University Press, Inc.\nWeidinger, L., McKee, K. R., Everett, R., Huang, S., Zhu, T. O., Chad-\nwick, M. J., Summerfield, C., & Gabriel, I. (2023). Using the \nVeil of Ignorance to align AI systems with principles of justice. \nProceedings of the National Academy of Sciences of the United \nStates of America, 120(18), e2213709120.\nYuan, L., Gao, X., Zheng, Z., Edmonds, M., Wu, Y. N., Rossano, F., \nLu, H., Zhu, Y., & Zhu, S.-C. (2022). In situ bidirectional human-\nrobot value alignment. Science Robotics, 7(68), eabm4183.\nZiems C., Yu J. A., Wang Y.-C., Halevy A., & Yang D. (2022). The \nmoral integrity corpus: A benchmark for ethical dialogue systems. \narXiv: 2204. 03021\nPublisher's Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Perspective (graphical)",
  "concepts": [
    {
      "name": "Perspective (graphical)",
      "score": 0.7325993180274963
    },
    {
      "name": "Epistemology",
      "score": 0.5320252776145935
    },
    {
      "name": "Sociology",
      "score": 0.49103784561157227
    },
    {
      "name": "Philosophical methodology",
      "score": 0.42406466603279114
    },
    {
      "name": "Engineering ethics",
      "score": 0.39883190393447876
    },
    {
      "name": "Computer science",
      "score": 0.29449132084846497
    },
    {
      "name": "Philosophy",
      "score": 0.2719539403915405
    },
    {
      "name": "Artificial intelligence",
      "score": 0.09837040305137634
    },
    {
      "name": "Engineering",
      "score": 0.08281880617141724
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210100255",
      "name": "Beijing Academy of Artificial Intelligence",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I116953780",
      "name": "Tongji University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I222723317",
      "name": "China Association for Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I151727225",
      "name": "Harbin Engineering University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I1300757298",
      "name": "Heilongjiang University of Science and Technology",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I19820366",
      "name": "Chinese Academy of Sciences",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210156404",
      "name": "Institute of Information Engineering",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    }
  ],
  "cited_by": 5
}