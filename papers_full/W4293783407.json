{
  "title": "Measuring design compliance using neural language models: an automotive case study",
  "url": "https://openalex.org/W4293783407",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2823284180",
      "name": "Dhasarathy Parthasarathy",
      "affiliations": [
        "Volvo (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A1076095252",
      "name": "Cecilia Ekelin",
      "affiliations": [
        "Volvo (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A5053752132",
      "name": "Anjali Karri",
      "affiliations": [
        "Chalmers University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2650302035",
      "name": "Jiapeng Sun",
      "affiliations": [
        "Chalmers University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2030030098",
      "name": "Panagiotis Moraitis",
      "affiliations": [
        "Chalmers University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963935794",
    "https://openalex.org/W1967581020",
    "https://openalex.org/W2030951679",
    "https://openalex.org/W3112303649",
    "https://openalex.org/W3098605233",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W3146720657",
    "https://openalex.org/W2925517804",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3126866660",
    "https://openalex.org/W2808164261",
    "https://openalex.org/W4206503458",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2474318526",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2951409611",
    "https://openalex.org/W3174082608",
    "https://openalex.org/W3019213264",
    "https://openalex.org/W3117575631",
    "https://openalex.org/W3105625394"
  ],
  "abstract": "As the modern vehicle becomes more software-defined, it is beginning to take significant effort to avoid serious regression in software design. This is because automotive software architects rely largely upon manual review of code to spot deviations from specified design principles. Such an approach is both inefficient and prone to error. In recent days, neural language models pre-trained on source code are beginning to be used for automating a variety of programming tasks. In this work, we extend the application of such a Programming Language Model (PLM) to automate the assessment of design compliance. Using a PLM, we construct a system that assesses whether a set of query programs comply with Controller-Handler, a design pattern specified to ensure hardware abstraction in automotive control software. The assessment is based upon measuring whether the geometrical arrangement of query program embeddings, extracted from the PLM, aligns with that of a set of known implementations of the pattern. The level of alignment is then transformed into an interpretable measure of compliance. Using a controlled experiment, we demonstrate that our technique determines compliance with a precision of 92%. Also, using expert review to calibrate the automated assessment, we introduce a protocol to determine the nature of the violation, helping eventual refactoring. Results from this work indicate that neural language models can provide valuable assistance to human architects in assessing and fixing violations in automotive software design.",
  "full_text": "Measuring design compliance using neural language models â€“\nan automotive case study\nDhasarathy Parthasarathy, Cecilia Ekelin\nVolvo Group, Sweden\nAnjali Karri, Jiapeng Sun, Panagiotis Moraitis\nChalmers University of Technology, Sweden\nPLM \nNeural language model trained on source code\nğ‘‹ \nğ‘Œ \nğ‘’ ğ‘‹ \nğ‘’ ğ‘Œ \nğ‘Ÿ \nğ‘š \nğ‘’ ğ‘‹ , ğ‘’ ğ‘Œ -Embeddings of query programs ğ‘‹ , ğ‘Œ \nğ‘Ÿ -Average offset between controller and handler programs seen in a set of known instances of the pattern\nğ‘’ ğ‘Œ -Embedding of ğ‘Œ expected by ğ‘Ÿ \nğ‘š â€“Alignment between expected and actual embedding as a measure of compliance with the Controller-Handler design pattern\nğ‘’ ğ‘Œ \nQuery programs\nFigure 1: Overview: We demonstrate a system that automatically assesses whether query programs (ğ‘‹,ğ‘Œ )complies with the\nautomotive Controller-Handler design pattern. The heart of the system is a neural language model pre-trained on source code.\nThe assessment process compares geometrical properties of the embeddings of query programs with that of a set of known in-\nstances of the pattern. The comparison is then converted into a score that allows architects to interpret the level of compliance.\nABSTRACT\nAs the modern vehicle becomes more software-defined, it is be-\nginning to take significant effort to avoid serious regression in\nsoftware design. This is because automotive software architects\nrely largely upon manual review of code to spot deviations from\nspecified design principles. Such an approach is both inefficient\nand prone to error. In recent days, neural language models pre-\ntrained on source code are beginning to be used for automating a\nvariety of programming tasks. In this work, we extend the appli-\ncation of such a Programming Language Model (PLM) to automate\nthe assessment of design compliance. Using a PLM, we construct\na system that assesses whether a set of query programs comply\nwith Controller-Handler, a design pattern specified to ensure hard-\nware abstraction in automotive control software. The assessment\nis based upon measuring whether the geometrical arrangement of\nquery program embeddings, extracted from the PLM, aligns with\nthat of a set of known implementations of the pattern. The level\nof alignment is then transformed into an interpretable measure of\ncompliance. Using a controlled experiment, we demonstrate that\nour technique determines compliance with a precision of 92%. Also,\nusing expert review to calibrate the automated assessment, we in-\ntroduce a protocol to determine the nature of the violation, helping\neventual refactoring. Results from this work indicate that neural lan-\nguage models can provide valuable assistance to human architects\nin assessing and fixing violations in automotive software design.\nPROMISE â€™22, November 17, 2022, Singapore, Singapore\nÂ© 2022 Association for Computing Machinery.\nThis is the authorâ€™s version of the work. It is posted here for your personal use. Not\nfor redistribution. The definitive Version of Record was published in Proceedings of\nthe 18th International Conference on Predictive Models and Data Analytics in Software\nEngineering (PROMISE â€™22), November 17, 2022, Singapore, Singapore , https://doi.org/10.\n1145/3558489.3559067.\nCCS CONCEPTS\nâ€¢ Software and its engineering â†’Software architectures ; â€¢\nComputing methodologies â†’Natural language processing .\nKEYWORDS\nneural programming language models, language model evaluation,\nsoftware design patterns\n1 INTRODUCTION\nâ€™If you think good design is expensive, try bad designâ€™, goes the\naphorism. While this observation can headline any design effort, it\nis certainly a prime motivator in the design of software. Very gener-\nally, the process of software design attempts to envision a software\nsolution that meets a given set of requirements [ 14]. The classic\ndesign process achieves this using a combination of tools including\ndesign principles, architecture models, interface specifications, etc.,\nthat parallelly guide, if not instruct top-down, the implementation\nof the solution. Meeting core business requirements may be its pri-\nmary objective, but software design often aspires further to address\nseveral non-functional concerns to increase the likelihood that the\nsolution operates and evolves sustainably [ 4]. The expanded set\nof concerns inevitably complicates the design process, which now\nbecomes an act of trading-off concerns in multiple dimensions,\nunder the shadow of constant uncertainty. In recent years, neural\nlanguage models pre-trained on large source code corpora have\nstarted becoming building blocks for automating a variety of com-\nplex programming tasks like code completion and program repair\n([13] and [ 25], for example). If such programming tasks, which\noften require nuanced judgment, can be automated, can a similar\napproach be applied to automate design tasks? In this work, we\ntake initial steps towards answering this question by investigating\na use case in automotive software design.\narXiv:2208.13215v1  [cs.SE]  28 Aug 2022\nPROMISE â€™22, November 17, 2022, Singapore, Singapore Parthasarathy, et al.\nNeed for assistance in automotive software design â€“ The mod-\nern vehicle is increasingly software driven. Software plays a central\nrole in realizing a variety of in-vehicle functions like preventive\nsafety, driver assistance, energy management, etc. Not only is in-\ncreasing the footprint of software essential to meet the growing\ndemand for functionality, vehicle manufacturers are increasingly\nrealizing that well-designed software is a key requirement to meet\nthis demand sustainably [20]. However, there are several factors\nthat complicate the design and evolution of automotive software. A\nstrict regulatory environment, decades of legacy, complex integra-\ntion chains, the strong influence of non-functional concerns like\nsafety and security, and strong hardware coupling are prominent\namong them. Moreover, since the automotive industry has its roots\nin traditional disciplines like mechanical and electrical engineering,\nknowledge of principles and practices of software engineering is\nless widespread. Therefore, delivering software at high cadence,\nwhile minimizing design compromises and preventing major threats\nto sustainable evolution of the code base, remains a formidable\nchallenge. Currently, the industry relies upon experienced software\narchitects to manually assess the code for design violations and\nintervene with changes when necessary. With violations from spec-\nified design being inevitable in practice, one advantage of manual\nreview is that the expert is able to exercise nuance and judgment\non whether a given violation is acceptable. The disadvantage, of\ncourse, is that manually assessing thousands of lines of code is\neffort-intensive. The intensity of effort alone increases the likeli-\nhood that major violations are left undiscovered and the overall\ndesign regresses, with harmful consequences for system evolution.\nAutomatic assessment of design, with levels of nuance compara-\nble to a human expert, would therefore provide vital assistance in\nincreasing the speed and effectiveness of design intervention.\nNeural language models for software design â€“ The application\nof neural language models for automating programming tasks is\nfundamentally based upon the naturalness hypothesis [1], which\nrecognizes that software is a form of communication. Neural Pro-\ngramming Language Models (PLMs), pre-trained on code corpora,\nexploit such infused elements of human communication to learn a\nstatistical model of programming. Such knowledge lies at the foun-\ndation of its ability to automate complex programming tasks. In\nour attempt to apply PLMs for automating design-related tasks, we\nstart by considering whether design information is also naturally\ncommunicated in code. Generally, programmers choose to augment\nself-explanation in code so that fellow-programmers find it easy\nto extend. A basic explanatory technique like using well-worded\nprogram statements, in a clearly evident sequence, accompanied\nby lucid natural language comments clearly helps code extension\nin relatively local scopes. In parallel, carefully wording and charac-\nterizing entities like methods, modules, or classes, and the ways in\nwhich they relate, interact, and are packaged, promote more global\nextension. Infusing such explanation, which is largely complemen-\ntary to program logic, clearly achieve many of the same objectives\nof a top-down design exercise. In fact, the co-evolution of design and\nsolution â€“ the â€™code as designâ€™ approach â€“ is itself a natural byprod-\nuct of using high-level PLs [26]. Put simply, with software naturally\ncontaining algorithmic and explanatory channels, the latter is likely\nto include information about design. Thus, irrespective of whether it\nemerges bottom-up as a result of programming or top-down as a re-\nsult of a design process,elements of software design occur naturally in\nsource code . Given that (1) PLMs successfully understand statistical\nproperties of natural programming, and (2) elements of design occur\nnaturally in source code, we reason that PLMs pre-trained on large\ncode corpora are likely to understand elements of design . The purpose\nof this study is to both verify this reasoning and exploit its potential.\nProblem statement â€“ We envision a systemSthat uses a PLM F\nto assess whether a set of query programs/filesğ‘„, drawn from a cor-\npus Q, complies with a design patternDspecified for the corpus. A\nscore ğ‘šcalculated by the system provides a measure of compliance.\nğ‘š= S(ğ‘„, D; F), ğ‘„âŠ†Q (1)\nTo construct and evaluate such a system, we pose the following\nresearch questions.\nRQ1 â€“ Can the system Sfor assessing design compliance be con-\nstructed using a neural language model trained on code?\nRQ2 â€“ Does the assessment improve when the PLM is explicitly\nprovided with information relevant to design pattern D?\nRQ3 â€“ Can the measure ğ‘šbe communicated in a way that makes\nit easy for an architect to understand the compliance of ğ‘„ with D?\nResults from our study1 show that it is indeed possible to construct\nsuch a system for measuring design compliance. Such a neural\nlanguage modeling approach to automatically assess design compli-\nance has the potential to improve the chances of quickly identifying\n(and subsequently correcting) design violations, thus promoting\nsustainable evolution of the code base with increased cadence.\n2 CHOOSING A CORPUS AND DESIGN\nPATTERN FOR STUDY\nWe now describe (1) the corpus Q, from which query programs are\ndrawn, and (2) the pattern Dagainst which their design is assessed.\nTruck Application Software corpus â€“ In a modern vehicle, the\noverall system which governs the automatic control of in-vehicle\nfunctions is generally referred to as the Electrical/Electronic (E/E)\nsystem ([15]). In this system, the basic unit of (electronic) hardware\nis the Electronic Control Unit (ECU). It is typically a microcontroller-\nbased platform that brings together the necessary elements of auto-\nmatic control â€“ the control logic, sensors, actuators, and related I/O.\nSoftware â€“ deployed on ECUs to realize the control logic â€“ is our\nfocus here. For this study, we use Truck Application Software (TAS),\na corpus ofâˆ¼5k files of C-language code, that implements in-vehicle\nfunctionality for the Volvo Groupâ€™s truck platforms. Principles of\nsoftware design adopted in TAS stem mainly from the Automotive\nSoftware Architecture (AUTOSAR) industry standard [3]. The basic\nunit of software defined by AUTOSAR is the Software Component\n(SWC), which is an independently deployable unit of functionality.\nAt the core of each SWC is a set of C files, called software mod-\nules, which collectively realize the functionality of the SWC. Upon\ndeployment, SWCs interact with each other to collectively realize\ncontrol applications (Figure 2a). The decomposition of control logic\ninto a set of SWCs is fundamental for achieving several objectives of\n1We release the implementation of our compliance assessment system here. Since the\ntest corpus Qis proprietary, we include examples that illustrate its content.\nMeasuring design compliance using neural language models PROMISE â€™22, November 17, 2022, Singapore, Singapore\nautomotive software design including the management of complex-\nity, separation of concerns, and the promotion of reuse. Therefore,\nthe notion of a SWC is, de facto, also the basic tool for software de-\nsign in TAS. Any design pattern of increased sophistication adopted\nin this corpus, builds upon the idea SWCs. One such design pattern,\nwhich we focus upon in this study, is described below.\n1..* Application 1..* SWC Module \n0..* Controller 1 Handler Device \nHandles \n0..* \n0..* 0..* \nUse Use \nUse \nUse \n(a) \n(b) \nFigure 2: (a) SWCs collaborating to implement a vehicle\nfunction, (b) Controller-Handler software design pattern for\nautomotive control systems\nThe Controller-Handler design pattern â€“ Let us now consider\nan example application2 in TAS â€“ roof hatch control â€“ and its design.\nTrucks are sometimes equipped with a hatch on the roof, which the\ndriver can control to adjust the flow of air and the amount of ambi-\nent light. The hatch is equipped with motors that effect this control.\nOne design principle, used in TAS, to implement such a function is\nthe separation of the core logic for hatch adjustment, theController,\nfrom the logic that handles the motors, the Handler. The main rea-\nson for this separation is that the roof-hatch motors are physically\nwired to specific hardware pins in a specific ECU. This means that\nthe handler needs to be deployed on this particular ECU and use\nthe designated pins to control the motor. In contrast, the applica-\ntion logic in the controller is not bound to a specific ECU, which\nallows more freedom in deciding where it can be deployed. Since\nautomotive ECUs (traditionally) are quite resource constrained, this\nController-Handler (CH) pattern offers a way to efficiently utilize\nthe available resources. Moreover, such hardware abstraction is\nessential to make cost-effective product offerings. A truck typically\nneeds to support a high level of product configuration to be able\nto fit a variety of transport operations and market segments. A\nroof hatch control application that properly implements CH can\nhelp offer trucks with different variants of roof-hatch motors, each\ntuned to meet a certain customer demand. Having separated the\napplication logic, the controller can be reused over all these variants.\nDue to its prevalence in TAS we focus on the CH design pattern\nin this study. Formally (see Figure 2b), the CH pattern advocates\nthe implementation of an in-vehicle control application using a set\nof SWCs ğ‘ƒ = {ğ¶,ğ»1,ğ»2,...ğ»ğ‘}. Here, the Controller component\nğ¶, implements the core control logic, while Handler components\nğ»ğ‘–,ğ‘– = 1...ğ‘ implement hardware-specific logic. In practice, since\nthe handler components are usually independent of each other,\nthe CH design pattern can be defined as applying to each pair\n2Refer to roofHatch in the released code for an illustration\nğ‘ƒ = (ğ¶,ğ»ğ‘–)of controller and handler SWCs used to realize the over-\nall application. Apart from roof hatch control, applications in TAS\nthat adopt this design pattern include washer and wiper control,\nexterior lights control and accelerator pedal control.\nWhile the reasoning behind the CH pattern is intuitive, compliance\nis not always achievable in practice. For instance, the responsibility\nsplit between the controller and handler must be at the right level to,\namong other things, avoid duplication of logic across handler vari-\nants. Roof hatch control includes special logic to ensure electrical\nsafety during actuation. Placing all of this safety logic in the handler\n(and not just motor-specific parts) leads to duplication. If there hap-\npens to be a violation in some handler variant which implements\nmore safety logic than necessary, spotting this is not easy for an ar-\nchitect who was not intimately involved in its design. On the other\nhand, there may be sufficient clues in tokens used in the compro-\nmised handler source code that a neural PLM may find anomalous.\n3 CONSTRUCTING A SYSTEM FOR\nASSESSING DESIGN COMPLIANCE\nHaving fixed the query corpus Qas TAS and the design pattern\nDas Controller-Handler for the study, we restate our objective.\nWe aim to construct a system Sthat assesses whether an ordered\npair ğ‘„ = (ğ‘‹,ğ‘Œ ), ğ‘‹,ğ‘Œ âˆˆQ of SWCs comply with the Controller-\nHandler design pattern. Though either of the SWCs in this pair\ncan be realized using multiple software modules (or programs), at\nthis point it is simpler to consider the case where each SWC is\nrealized as one program. We relax this condition at a later point.\nThe following sections describe the process of constructing the\ncompliance assessment system Sthat we envision in Eq.1.\nPre-training a PLM â€“ In this work, we consider a program ğ‘‹ =\n(ğ‘¡1,ğ‘¡2,...,ğ‘¡ ğ‘)to be a source code file containing a sequence of\ntokens. We then define a PLM to be a language representation\nmodel of the form F(ğ‘‹ğ‘€)â†’ ğ‘‹ pre-trained as a masked language\nmodel, first introduced in BERT [8]. The core task in pre-training\nsuch a model is Masked Reconstruction (MR), shown in Eq.23. In this\ntask, the PLM is provided a masked program ğ‘‹ğ‘€, which produced\nby replacing a fraction of tokens in ğ‘‹ with a mask token t. The\nmodel is then tasked to recover tokens in masked positions, as a\nresult of which it learns contextual meanings of programs.\nğ‘€ğ‘…(ğ‘‹; F)= F(ğ‘‹ğ‘€)[ğ‘—]== ğ‘‹[ğ‘—],\nğ‘— = {ğ‘– : ğ‘¡ğ‘– = t,ğ‘¡ğ‘– âˆˆğ‘‹ğ‘€} (2)\nSince our aim is to assess design compliance in TAS which is a\nC-language corpus, we pre-train a monolingual PLM on C code.\nAs pre-training corpus P, we use âˆ¼75M files of C code derived\nfrom the GitHub public dataset4. The model is then pre-trained by\nminimizing the objective shown in Eq. 3. Prior to being fed into a\nPLM, each program is tokenized and split further into a sequence\nof subwords using the Byte Pair Encoding (BPE) [28] mechanism.\nF:= argmin\nF\nEğ‘‹âˆˆP ğ‘€ğ‘…(ğ‘‹; F) (3)\nThe procedure described in Eq.1 assesses whether a set of programs\nğ‘„ complies with a design pattern D. Practically, however, feeding\nan entire program into the PLM is an issue because C programs\n3In practice, a differentiable cross-entropy loss is used\n4https://console.cloud.google.com/marketplace/details/github/github-repos\nPROMISE â€™22, November 17, 2022, Singapore, Singapore Parthasarathy, et al.\ntend to be long. The average length of a C program isâˆ¼5k subwords\nin the GitHub corpus and âˆ¼7k subwords in the TAS corpus. The\nTransformer architecture [32], which is the mainstay of several\npreviously reported foundational PLMs like CodeBERT [10], is typi-\ncally configured to handle input sequences of length 512-1024. This\nis because the vanilla self-attention mechanism is of quadratic com-\npute and memory complexity, which makes it impractical for longer\ninput sequences. To be able to assess long programs, we therefore\nbase the PLM Fon the more efficient Reformer [17] architecture.\nCombining locality-sensitive-hashing and reversible residual layers,\nthe Reformer handles long sequences much more efficiently. By\nconfiguring the input sequence length to 8192, we are able to feed\naround 80% of programs in the TAS corpus into the Reformer-based\nFintact with manageable memory and computational complexity.\nLonger programs are truncated to this length. The Reformer encoder\nFof âˆ¼180M parameters with 6 self-attention layers (each with 8\nheads) was trained from scratch on 16 Nvidia Tesla V100 GPUs\nuntil the MR accuracy on a validation set of 5k files reached 95.12%.\nAssessing compliance by manual review â€“ Recalling the CH\ndesign pattern described in Section 2, let us now consider how a hu-\nman architect would assess whether a query pair(ğ‘‹,ğ‘Œ )of programs\ncomplies with this pattern. The architect would normally do this by\nreviewing the code (or the â€˜naturalnessâ€™) of the programs and assess\nwhether ğ‘‹and ğ‘Œrespectively embody core principles of a controller\nand its associated handler. It is, however, important to note that\nthe CH pattern defines expectations jointly on the pair and not on\nindividual programs. Therefore, a starting point for the architect\nwould be to juxtapose related parts from the pair (sometimes men-\ntally) and then conduct the assessment. We find it useful to refer to\nsuch a juxtaposition as ğ‘‹ğ‘Œ â€“ the â€˜jointnessâ€™ of the two programs. It\nis on this â€“ at times abstract â€“ representation XY that the architect\nassesses whether principles of the CH pattern are complied with. In\nthe example of roof hatch control, signs of compliance include that\nthe interface for the handler is a pure abstraction of the hardware\ninterface for the hatch motors. That is, the handler does not contain\nextra logic e.g. to protect the motors from over usage. That kind\nof logic would be part of the controller. Similarly, the controller\nshould make use of the handler interface only to interact with the\nmotors. Signs of deviation from the pattern are the opposite of what\nhas been described. That is, the handler contains too much control\nlogic or the controller interacts directly with the motors. Not only is\nmanually assessing the jointness ğ‘‹ğ‘Œ for signs of deviation difficult,\nthere are several factors that complicate the process further. First,\nany instance of the CH pattern is certain to contain code that falls\noutside the purview of the pattern itself. Hence, an architect will\nhave to identify and assess tenets of the pattern in a diluted context.\nSecond, as a relatively loose pattern, it can be realized in several\nstyles. An architect would therefore need to judge whether a given\nstyle of implementation is legitimate. Third, it is practically difficult\nto construct an ideal realization against which the query programs\ncan be assessed. Usually, the architect relies on a subjective mental\nmodel of the pattern, which is not only difficult to explicitly state,\nbut also affects the objectivity of the assessment. Addressing these\nconcerns requires nuanced judgment, which is precisely what a\nhuman expert applies. In using a PLM as an alternative to a human\nexpert, we now describe how we address some of these concerns.\nAssessing compliance using program embeddings â€“ The main\ntool we use for PLM-based compliance assessment is the program\nembedding ğ‘’ğ‘‹, which is a vector representation of the program ğ‘‹\nthat reflects its semantic properties. As shown in [ 24], there are\ndifferent ways to extract embeddings from contextual language\nmodels, each capturing different aspects of information. After some\ntrial and error, we empirically decide to use the normalized output\nof the final (6th) layer of Fas shown below.\nğ‘’ğ‘‹ = F6(ğ‘‹)\n||F6(ğ‘‹)|| (4)\nThe PLM Fis pre-trained on the masked reconstruction task on mil-\nlions of program examples. It is therefore reasonable to expect that\nthe embedding ğ‘’ğ‘‹ is a fairly robust representation of the programğ‘‹\nand is insensitive to minor semantic variations. Thus, the process of\nassessing whether (ğ‘‹,ğ‘Œ )complies with the CH pattern is done, not\nin the code space, but in a vector space using embeddings (ğ‘’ğ‘‹,ğ‘’ğ‘Œ).\nWhile this pair of embeddings sufficiently represent the programs\nindividually, an additional representation is needed to address the\njoint perspective ğ‘‹ğ‘Œ. One simple model to capture the jointness of\na pair of programs would be the offset between their embeddings.\nğ‘Ÿğ‘‹ğ‘Œ = ğ‘’ğ‘Œ âˆ’ğ‘’ğ‘‹ (5)\nShould there exist a benchmark vector ğ‘Ÿ that captures the required\nlevel of jointness as prescribed by the CH design pattern, then the\nassessment of design compliance reduces to checking the align-\nment between ğ‘Ÿğ‘‹ğ‘Œ and ğ‘Ÿ in the embedding space. Put otherwise,\nif ğ‘Ÿ serves as an effective offset vector between the embeddings of\nthe pair of programs (ğ‘‹,ğ‘Œ ), i.e., if Eq.6 is satisfied, then this pair\ncomes close to realizing the principles specified by the CH pattern.\n^ğ‘’ğ‘Œ := ğ‘’ğ‘‹ +ğ‘Ÿ â‰ˆğ‘’ğ‘Œ (6)\nAs noted earlier among concerns in manual assessment, there is no\neasy way to construct an ideal realization of the CH pattern in the\ncode space. This means that access to its embedding equivalent ğ‘Ÿ\nis equally difficult. As a practical alternative, we assess compliance\nwith the average realization of the CH pattern, extracted from a set\nof known instances. That is, given a set ğ‘‰ = {(ğ¶,ğ»)}ğ‘\nğ‘–=1 from the\nTAS corpus that are known to implement the CH pattern, we define\na benchmark of average jointness (Eq. 7), that averages offset vec-\ntors from pairs in ğ‘‰. If this benchmark serves as an effective offset\nfor query programs (ğ‘‹,ğ‘Œ ), satisfying Eq. 6, then this pair comes\nclose to realizing the average implementation of the CH pattern seen\nin |ğ‘‰|known instances. Apart from being an intuitive and practical\nbenchmark, by pooling common traits from known instances,ğ‘Ÿ pro-\nvides a stronger signature for the CH pattern compared to individual\ninstances, where signatures of the pattern are likely to be diluted.\nğ‘Ÿ := 1\n|ğ‘‰|\nâˆ‘ï¸\n(ğ¶,ğ»)âˆˆğ‘‰\nğ‘’ğ» âˆ’ğ‘’ğ¶ (7)\nAs shown using an example in Figure 3, serving as an offset vector\nfrom ğ‘’ğ‘‹, ifğ‘Ÿis able to predict a handler embedding^ğ‘’ğ‘Œ that is reason-\nably close to its actual counterpartğ‘’ğ‘Œ, programs (ğ‘‹,ğ‘Œ )are likely to\ncomply with the CH pattern. Such closeness between ^ğ‘’ğ‘Œ and ğ‘’ğ‘Œ is\neasily measurable using the cosine similarity between these two vec-\ntors. With this method, the assessment system for the CH design pat-\ntern D, originally envisioned as Eq.1, can be re-written as follows.\nMeasuring design compliance using neural language models PROMISE â€™22, November 17, 2022, Singapore, Singapore\nğ‘š= S((ğ‘‹,ğ‘Œ ),D; F,ğ‘‰)= ğ‘’ğ‘Œ Â·(ğ‘’ğ‘‹ +ğ‘Ÿ)\n||ğ‘’ğ‘Œ||2 ||ğ‘’ğ‘‹ +ğ‘Ÿ||2\n(8)\nğ‘’ ğ‘‹ \nğ‘Ÿ \nğ‘’ ğ‘Œ \nğ‘’ ğ‘‹ + ğ‘Ÿ \nğ‘’ ğ‘Š \nğ‘’ ğ‘ \nğ‘š \nFigure 3: The alignment between the actual handler embed-\nding ğ‘’ğ‘Œ and the predicted one ğ‘’ğ‘‹ +ğ‘Ÿreflects compliance. Vec-\ntors ğ‘’ğ‘Š,ğ‘’ğ‘ illustrate embeddings of programs ğ‘Š,ğ‘ âˆˆQ\nUsing cosine similarity as the metric measure â€“ standard practice\nfor comparing language model embeddings â€“ results inâˆ’1 â‰¤ğ‘š â‰¤1.\nThen,ğ‘šâ‰ˆ1 means that the predicted handler embedding^ğ‘’ğ‘Œ closely\naligns with that of the actual handler ğ‘’ğ‘Œ, indicating compliance.\nThus, as a way to assess compliance with the CH design pattern,\nwe substitute a complex code review process with a vastly simpler\ncomparison of embeddings extracted from a neural language model.\nEasing interpretation of compliance â€“ With cosine similarity,\nwhile it is clear that ğ‘š = 1 and ğ‘š = âˆ’1 indicate perfect compli-\nance and non-compliance respectively, such perfect scores are rare.\nScores in between, which are most likely in practice, are difficult to\ninterpret. In order to provide intuitive human-readable assessment,\nwe convert similarity ğ‘šinto a rank ğ‘˜. The discrete rank ğ‘˜ means\nthat the predicted handler embedding is the ğ‘˜ğ‘¡â„ most similar to\nthat of the actual handler, when compared to the embeddings of all\nother programs in the TAS corpus. The best indicator of compliance\nis a rank of ğ‘˜ = 1 when, among all programs in the TAS corpus Q\n(excluding the controllerğ‘‹) there is no better handler thanğ‘Œ for the\ncontroller ğ‘‹, as assessed by the benchmark ğ‘Ÿ. Conversely, a rank of\n|Q|âˆ’1 means that the predicted embedding is least similar and any\nother program in the TAS corpus is a better handler than ğ‘Œ. This is\nthe worst indicator of compliance. While the rank may be a more\ninterpretable measure, its value is now dependent upon the spread\nof embeddings around ğ‘’ğ‘Œ. In the example shown in Figure 3, even\nif the prediction is reasonably good, it is of rank ğ‘˜ = 2, since there\nis another programğ‘ âˆˆQ, whose embedding is closer to that of the\nactual handler program ğ‘Œ. If there is considerable clustering in the\nclose neighborhood ofğ‘’ğ‘Œ, then even a good prediction is unlikely to\nresult in a rank close to 1. We therefore use a simple rule of thumb,\nwhere if the predicted embedding lies within 10% of embeddings\nmost similar toğ‘’ğ‘Œ, we define the assessmentğ‘™ = True that the query\n(ğ‘‹,ğ‘Œ )complies with the CH pattern. If the predicted embedding lies\namong those of 90% of the least similar programs, we label the pair\nas non-compliant. The discrete rank ğ‘˜, in addition to a true/false\nbinary assessment of compliance ğ‘™, eases human comprehension of\nour PLM-based process of assessing design compliance. The com-\nplete process of compliance assessment is described in Procedure 1.\nProcedure 1: Compliance assessment system S\nParameters : Test input (ğ‘‹,ğ‘Œ ), PLM F, TAS corpus Q, known\ninstances of the CH pattern ğ‘‰\n1 Function ğ‘€(ğ‘’ğ´, ğ‘’ğµ):\n2 ğ‘š= ğ‘’ğ´.ğ‘’ğµ\n||ğ‘’ğ´||2 ||ğ‘’ğµ||2\n3 return ğ‘š\n4 Function S(ğ‘‹,ğ‘Œ ; F,ğ‘‰):\n/* Note: ğ‘’ğ‘‹ = F6 (ğ‘‹)/||F6 (ğ‘‹)|| */\n5 ğ‘Ÿ = 1\n|ğ‘‰|\nÃ\n(ğ¶,ğ»)âˆˆğ‘‰ ğ‘’ğ» âˆ’ğ‘’ğ¶\n6 ğ‘ = [ğ‘€(ğ‘’ğ‘, ğ‘’ğ‘‹ +ğ‘Ÿ): ğ‘ âˆˆ Q\\{ğ‘‹}]\n7 ğ‘˜ = indexof (sort(ğ‘), ğ‘€(ğ‘’ğ‘Œ, ğ‘’ğ‘‹ +ğ‘Ÿ)) // rank\n8 ğ‘™ = ğ‘˜ â‰¤0.1 âˆ—|Q| // binary assessment of compliance\n9 return ğ‘˜, ğ‘™\n4 EXPERIMENTS\nThis section describes how we experiment with the system based\nupon parameters identified in Eq. 8.\nQuery (ğ‘‹,ğ‘Œ )and benchmark programs ğ‘‰ â€“ The objective of\nthe assessment process is to check whether a pair of query SWCs\n(ğ‘‹,ğ‘Œ )complies with the average realization of the CH pattern seen\nin a separate set ğ‘‰ of known instances. With the help of architects\nwho are familiar with the TAS corpus, we first identify 21 known\ninstances of the CH pattern and curate them into a set V. Next, we\ndesign a controlled experiment by selecting two types of queries.\nâ€¢The positive query â€“ where the query ğ‘„+ âˆˆ Vis known to\nbe an implementation of the CH pattern that is likely to satisfy\nthe condition specified in Eq.7. The benchmark set in this case is\nğ‘‰ = V\\{ğ‘„+}, which is all known instances of the pattern excluding\nthe instance chosen as the test input.\nâ€¢The negative query â€“ where the queryğ‘„âˆ’âˆˆQ\\V is known to not\nimplement the CH pattern and is therefore unlikely to satisfy Eq.7.\nHere, the benchmark set ğ‘‰ = Vincludes all known instances of\nthe CH pattern in the TAS corpus. Since we expect negative queries\nto perform poorly during the assessment, they help establish a\nbaseline for the evaluating the accuracy of the assessment process.\nConsider a pair of SWCs (ğ¶,ğ»)âˆˆV , that is known to implement\nthe CH pattern. While it is most straightforward to implement\neach SWC in the pair as one program, this is not always practi-\ncal. As shown in Figure 2a, some SWCs include a lot of function-\nality in which case it is necessary to split its code into several\nprograms or files. Practically, therefore, the SWCs are of the form\nğ¶ = {ğ¶1,ğ¶2,...,ğ¶ ğ‘€}and ğ» = {ğ»1,ğ»2,...,ğ» ğ‘}, each of them being\nimplemented using multiple programs. This complicates the assess-\nment process since the systemSis designed only to handle a pair of\nprograms and not a pair of sets. A simple way to circumvent this lim-\nitation is to â€˜unrollâ€™ the setVinto a Cartesian product set as follows.\nVâˆ—= {(ğ‘,â„): ğ‘ âˆˆğ¶,â„ âˆˆğ» : (ğ¶,ğ»)âˆˆV} (9)\nFor every known instance of the CH pattern (ğ¶,ğ»)âˆˆV , the prod-\nuct set Vâˆ—pairs each program in the controller SWC ğ¶ with every\nprogram in the handler componentğ». This process results in a total\nof 63 pairs, which we use as likely queries in our experiments. By\ndrawing queries ğ‘„+âˆˆVâˆ—, the advantage is that we exhaustively\npresent all combinations in a paired form that is suitable for assess-\nment using Eq.8. The disadvantage is that even if at the component\nPROMISE â€™22, November 17, 2022, Singapore, Singapore Parthasarathy, et al.\nlevel every pair (ğ¶,ğ»)âˆˆV is a known instance of the CH pattern,\nnot every pair (ğ‘,â„)âˆˆV âˆ—at the program level is a â€˜trueâ€™ controller-\nhandler pair that implements elementary aspects of the CH pattern.\nConsidering that several instances of the CH pattern are imple-\nmented using multiple programs, and that the assessment system is\ncurrently designed to work only with a pair of programs, we accept\nthe risk and loosen the definition of the CH pattern. Every pair in\nthe product set Vâˆ—is considered as a true pair and is presented\nas a positive case for testing, while also being used to calculate\nğ‘Ÿ. Negative queries ğ‘„âˆ’are simply drawn by picking two random\nprograms from TAS as long as neither of them appear in Vâˆ—.\nThe PLM Fâ€“ As the heart of the automated compliance assess-\nment system, the neural PLM Fcan be seen as the machine coun-\nterpart of a human architect who conducts the same assessment\nmanually. With such an analogy, we now reason about the level of\ninformation with which Fis trained and its relation to the quality\nof assessment. The model pre-trained using Eq.3 on a C-language\ncorpus extracted from GitHub â€“ which we now denote as Fğ´ â€“\nis a C-programming expert. Using this model is akin to asking\na human expert in C-programming, but one who has no experi-\nence in automotive application design and development, to assess\ncompliance with the CH pattern. While it is not impossible for\nsuch an expert to conduct this assessment, it is reasonable that an\nawareness of relevant domain and design concepts would ease the\nprocess. To a C-programming expert, we contend that such aware-\nness can be introduced in three stages. The first stage would be to\nincrease awareness about the automotive-domain, i.e. the pattern of\ntoken usage (its naturalness) in its application code. Second comes\ndesign-related knowledge, mainly the concept of SWCs, which is\nfundamental to the definition of the CH pattern. Third, would be\nthe concept of controllers and handlers, the subjects of assessment.\nLike [12], we achieve the first stage â€“ improving domain-familiarity\nâ€“ by simply continuing to pre-train Fğ´ on code from TAS. The\nsecond stage requires inducing the knowledge of a SWC â€“ a set\nof programs that jointly realize functionality. We do this by first\nassembling a set ğ¶ = {(ğ´,ğ‘ƒ,ğ‘ )}ğ‘€\nğ‘–=1 of programs from TAS, such\nthat ğ´and ğ‘ƒ belong to the same SWC, while ğ‘ belongs to a dif-\nferent SWC. Then, we use the triplet loss to cluster embeddings of\nprograms that belong to a SWC, while keeping those of programs\nfrom different SWCs further apart. To simultaneously ensure that\nthis SWC-based clustering does not majorly disrupt the embedding\ngeometry, and to impart domain familiarity, we combine the MR\ntask on the TAS corpus with SWC-clustering as shown below.\nFğµ = argmin\nF\nE(ğ´,ğ‘ƒ,ğ‘)âˆˆğ¶ ğ‘‡ğ‘…(ğ´,ğ‘ƒ,ğ‘ ; F)+ ğ‘€ğ‘…(ğ´; F)\nğ‘‡ğ‘…(ğ´,ğ‘ƒ,ğ‘ ; F)= (||ğ‘’ğ´ âˆ’ğ‘’ğ‘ƒ||2 âˆ’||ğ‘’ğ´ âˆ’ğ‘’ğ‘||2)\n(10)\nThe resulting fine-tuned model Fğµ is thus more familiar with do-\nmain and design concepts related TAS in comparison to Fğ´. For\nthe third stage of inducing knowledge about controller and handler\nprograms, we follow a similar approach of encouraging the PLM\nto respectively cluster these programs by type. To achieve this, we\nassemble (1) a set ğ·ğ¶ = {(ğ¶1,ğ¶2,ğ´)}ğ‘€\nğ‘–=1 with ğ¶1 and ğ¶2 being con-\ntrollers and ğ´being a non-controller program from the TAS corpus,\nand (2) a set ğ·ğ» = {(ğ»1,ğ»2,ğµ)}ğ‘\nğ‘–=1, with ğ»1 and ğ»2 being handler\nprograms andğµbeing a non-handler program. We then fine-tuneFğµ\nusing the triplet loss on the combined setğ· = ğ·ğ¶âˆªğ·ğ», resulting in\na model Fğ¶ that is aware of the concept of controllers and handlers.\nFğ¶ = argmin\nF\nE(ğ´,ğ‘ƒ,ğ‘)âˆˆğ· ğ‘‡ğ‘…(ğ´,ğ‘ƒ,ğ‘ ; F)+ ğ‘€ğ‘…(ğ´; F) (11)\nBy assessing design compliance using models Fğ´, Fğµ, and Fğ¶, re-\nspectively representing increasing awareness of concepts relevant\nto the assessment, we analyze the influence of such awareness. This\nassessment is conducted on an equal number of positive (ğ‘„+) and\nnegative (ğ‘„âˆ’) queries. For each query, results are collected in terms\nof a discrete rank and a binary label (see Procedure 1).\n5 RESULTS\nThe primary tool which we use for analyzing the results are the\nlabels ğ‘™collected for each query. This binary label indicates whether\nthe query has been evaluated by the systemSto comply with or de-\nviate from the CH pattern. The controlled experiment using positive\nand negative queries, which are known to comply and deviate from\nthe pattern, allows collection of results of each of these cases into\nlists ğ¿+and ğ¿âˆ’respectively. Thus, true positive (TP) assessments are\nthose labels in ğ¿+that evaluate to True and false negatives (FN) are\nthose that evaluate to False. False positive (FP) and true negative\n(TN) assessments are similarly identifiable fromğ¿âˆ’, as shown below.\nğ‘‡ğ‘ƒ : {ğ‘™ |ğ‘™ == ğ‘‡ğ‘Ÿğ‘¢ğ‘’,ğ‘™ âˆˆğ¿+}ğ¹ğ‘ : {ğ‘™ |ğ‘™ == ğ¹ğ‘ğ‘™ğ‘ ğ‘’,ğ‘™ âˆˆğ¿+}\nğ¹ğ‘ƒ : {ğ‘™ |ğ‘™ == ğ‘‡ğ‘Ÿğ‘¢ğ‘’,ğ‘™ âˆˆğ¿âˆ’}ğ‘‡ğ‘ : {ğ‘™ |ğ‘™ == ğ¹ğ‘ğ‘™ğ‘ ğ‘’,ğ‘™ âˆˆğ¿âˆ’} (12)\nUsing this, we build the confusion matrix (Table 1) and performance\nmetrics of the assessment process (Table 2). These metrics help us\nanswer the research questions posed in our problem statement.\nTable 1: Compliance assessment â€“ confusion matrix 1,2\nQueries\nPrediction (ğ‘™) Fğ´ Fğµ Fğ¶\nTrue False True False True False\nPositive (ğ‘„+) - 63 22 (0.35)41 (0.65)37 (0.59)26 (0.41)50 (0.80)13 (0.20)\nNegative (ğ‘„âˆ’) - 63 8 (0.13)55 (0.87)7 (0.11)56 (0.89)4 (0.06)59 (0.94)\n1Confusion matrix on labelsğ¿+andğ¿âˆ’calculated according to Eq.122For definition of each labelğ‘™âˆˆğ¿+ğ‘œğ‘Ÿğ¿âˆ’refer to Procedure 1\nTable 2: Compliance assessment â€“ performance metrics\nMetric with Fğ´ with Fğµ with Fğ¶\nAccuracy 0.611 0.738 0.860\nRecall 0.349 0.587 0.790\nPrecision 0.733 0.840 0.920\nF1 score 0.473 0.691 0.850\nRQ1: assessing design compliance using neural PLMs â€“ Per-\nformance metrics in Table 2 show encouraging signs that a system\nfor assessing compliance of programs (ğ‘‹,ğ‘Œ )with the CH design\npattern can be constructed using a neural language model trained\non nothing but source code. Even with the model Fğ´, which is\npre-trained purely on non-automotive code, the system is capable\nof identifying instances of the CH pattern with a precision of more\nthan 0.70. As also seen in Table 1, with a high True Negative Rate\nMeasuring design compliance using neural language models PROMISE â€™22, November 17, 2022, Singapore, Singapore\n(TNR) (0.87), the system is particularly adept at correctly identify-\ning non-compliant instances of the pattern. The main concern, seen\nfrom the same table, is of course the very high False Negative Rate\n(FNR) of 0.65. That is, the system built using Fğ´ is misclassifying\na majority of known instances of the CH pattern as non-compliant.\nThe high FNR, in turn, lowers the accuracy, precision and F1 score.\nThus, while the performance of design compliance assessment us-\ning Fğ´ is encouraging, it remains unsatisfactory. We reason that\nthere are three main factors that could explain the high FNR. The\nfirst is the product set Vâˆ—, which considers all possible pairs of\nprograms from applications that are known instances of the CH\npattern. The introduction of doubtful pairs could taint both the\naverage jointness benchmark ğ‘Ÿ and whether a positive test input\nis genuinely so. The second reason could be the lack of familiarity\nwith TAS domain and design in Fğ´, due to which program embed-\ndings are arranged in such a way that the benchmark vector ğ‘Ÿ does\nnot serve as a good offset. The third reason could be some weakness\nin assessment using the average jointness benchmark ğ‘Ÿ. Results\nfrom testing with models Fğµ and Fğ¶ shows that it is less likely to\nbe due to a weakness in the assessment approach.\nRQ2: assessment using PLMs with increased knowledge â€“\nHaving been pre-trained only using the GitHub corpus, one weak-\nness in Fğ´ is that it is less aware of domain and design-related spe-\ncializations in the TAS corpus. This is precisely why we train models\nFğµ and Fğ¶ by explicitly providing this information. Assessment us-\ning Fğµ, which learns domain-specific naturalness and the concept of\nSWCs used in the TAS corpus, leads to a strong reduction of the FNR\nto 0.41. The consequent improvement in the F1 score to 0.7 is also\nnoteworthy. This clearly indicates that inducing the knowledge of\nSWCs directly leads to an improvement in the quality of assessment.\nUsing model Fğ¶ â€“ which is trained to understand controller and\nhandler programs â€“ for the assessment leads to yet another strong\nreduction in the FNR to0.2, due to which the precision and F1 score\ncommendably increase to0.92 and 0.85 respectively. The clustering\nobjectives (Eqs. 10 and 11), are therefore likely to have resulted in\nan arrangement of embeddings that better satisfies Eq. 7. These ob-\nservations clearly indicate that using a PLM with an increased level\nof awareness about the domain and its design, results in a much\nmore accurate assessment. Even with a marked improvement in the\nquality of assessment, the FNR remains a concern. To analyze this,\nthere is a need to go beyond binary assessment to a finer method.\nRQ3: easing interpretation of assessment â€“ Analyzing the bi-\nnary labels of compliance (ğ¿+and ğ¿âˆ’), using the confusion matrix\nand metrics derived from it, helps evaluate the performance of the\nassessment system. While this is necessary to build confidence\nin the system, from the perspective of an architect or developer,\nit is equally important to understand why the system assesses a\nquery as complying or deviating from the CH pattern. Since this\nrequires much more nuance than a binary label, we turn to the\nrank ğ‘˜ to gain a finer interpretation of the assessment. Specifically,\nwe analyze the distribution of ğ¾+and ğ¾âˆ’of ranks respectively\ncollected for positive and negative queries. For brevity, we confine\nour analysis to the best performing system that uses the model Fğ¶.\nFirst we begin by visualizing the spread of ranks shown in Figure 4.\nInspecting the spread of ranks for the positive cases ğ¾+, allows us\nto demarcate three intervals of ranks where results cluster. Next, we\nsample queries from each interval and have them assessed by archi-\ntects who are familiar with TAS. The manual assessment of sampled\nqueries follows an approach similar to the one described in Section 3.\nUsing expert review we calibrate the results of the PLM-based com-\npliance assessment system within each interval as described below.\nFigure 4: Calibrating the results of assessment into inter-\npretable intervals using expert review\nâ€¢Interval 1 (ranks 1-100) â€“ The interval where a majority of pos-\nitive cases cluster, it consists mostly of queries that are assessed\nby architects to be good implementations of the CH design pat-\ntern. Some are even judged to be textbook cases with the right\ninterface and responsibility split between controller and handler\nprograms. The best ranking instances in this interval are also those\nwhich exhibit bidirectional exchange of information between the\ntwo programs. The exchange follows standard practice of using\nthe AUTOSAR Runtime Environment (RTE), seen in their use of\nRTE_read and RTE_write methods5. Cases that perform relatively\nworse within this interval (rank close to 100) are observed to im-\nplement unidirectional interaction, where the controller only reads\nfrom the handler, which is perfectly legitimate. Therefore, expert\nreview generally considers test inputs that rank in this interval to\nbe compliant with the CH pattern, with no need for refactoring.\nThis is further strengthened by the fact that not a single negative\ntest case is ranked by the system as being in this interval.\nâ€¢Interval 2 (ranks 100-1000) â€“ Expert review indicates that positive\nqueries in this interval show subtle deviations from the standard\nimplementation of the CH pattern. One deviation is that, while the\nresponsibility split is correct, the controller and handler programs\ndo not interact directly with each other. The actual interaction, in\nthis case, usually happens through some other program in the con-\ntroller SWC, which is excluded due to the constraint that the system\noperates only on pairs of programs. This is, therefore, not a genuine\nviolation and results simply due to a limitation in the system. More\nsignificantly, the other observed deviation is where there is direct\ninteraction, but it does not take place through the AUTOSAR RTE.\nThis is a subtle deviation which could benefit from refactoring. The\nfact that the assessment system consistently places such cases in the\n5Refer to roofHatch in released code for an example\nPROMISE â€™22, November 17, 2022, Singapore, Singapore Parthasarathy, et al.\nsecond interval is an encouraging observation. However, the devia-\ntions observed by expert review in this interval also seem to be char-\nacteristics observable in pairs of programs that are not controllers\nor handlers. For instance, it is plausible that random sampling from\nthe relatively small TAS corpus results in a pair of non-interacting\nprograms, one of which contains some application-like code and the\nother containing some code related to hardware. This could explain\nwhy some negative cases end up being ranked in this interval. In\ngeneral, when the system ranks a query in this interval, it could\nbe a candidate for refactoring. However, it is best if the automated\nassessment is manually verified to ensure that it is a genuine case.\nâ€¢Interval 3 (ranks 1000-5000) â€“ Very few positive cases rank in this\ninterval. In some cases, the test input implements diagnostic rou-\ntines, and not application logic. In others, the controller program is\nvery small, containing only a few lines of code. Generally, therefore\npositive cases seem to rank in this interval because they are marked\noutliers compared to the average CH implementation. A cause for\nconcern are the handful of cases which are genuine false negatives\nand are, in fact, assessed to be good implementations of the CH\npattern. Moreover, a query ranked in this interval seems to deviate\nfrom the average implementation to such an extent that it is barely\ndistinguishable from random queries drawn from TAS. A result in\nthis interval therefore requires manual review by an expert.\nThus, the greatest advantage of the system is its ability to identify\ntrue compliance with the CH pattern. Such cases, as verified by\nexperts, rank in the first interval. Also, its tendency to rank subtle\nvariations â€“ possible candidates for refactoring â€“ in the second\ninterval shows its ability make nuanced judgments. Finding such\ndeviations is a strong indicator of its practical utility. The inconclu-\nsive nature of results in the third interval, and the presence of some\nnegative cases in the second, indicate the boundaries of this process.\nOverall observations â€“ First, queries that fall within the first two\nintervals are remarkably similar in character, meaning that observa-\ntions apply quite consistently to cases within a given interval. This\nreflects the consistency of automated assessment using the average\njointness benchmark. Second, this consistency eases practical use\nbecause when a query ranks within an interval, we have a reason-\nably good idea why this happens. This means that any subsequent\ndesign intervention can be precisely targeted to rectify suspected\ndeviations. Third, the calibration process makes it possible to decide\nthe conditions under which it is necessary for an architect to inter-\nvene. Ranks in the first interval do not require human verification,\nwhile those in the second and (especially) third intervals need active\nintervention. These observations thus point to the ingredients of\na protocol for interpreting the results and, thus, practically using\nthe system. However, we also observe a few caveats in the process\nwhich we now list. First, under the current process, the benchmarkğ‘Ÿ\nneeds to be recalculated whenever there is a new instance of the CH\npattern. Since this is not a computationally heavy process, we do not\nrate this as a major concern. An alternative would be to fix â€˜goldenâ€™\ninstances of the pattern so that the benchmarkğ‘Ÿis itself fixed. While\nchoosing such instances, it is important to ensure that legitimate\nvariations are included. It would also be necessary to periodically\naudit the golden instances to ensure that they are up-to-date with\nthe latest understanding of the pattern. Second, the ranking process\ndepends upon all programs in the TAS corpus, meaning that the\naddition of new programs needs a recalibration of the results. In\nthe worst case, the inclusion of a new set of highly specialized\nprograms could severely disrupt the calibration. However, it is im-\nportant to note that such risks are inherent to any benchmark that\nis derived from an evolving corpus. Third, there is a need to better\nunderstand the relationship between pattern compliance and rank.\nConsider the test input with a rank close to 100 (and thus in interval\n1), but deviates from the textbook implementation because here the\ncontroller only reads from, and does not write to, the handler. Such\na deviation seems sufficient for âˆ¼100 programs in the TAS corpus\nto come in between the predicted and actual handler embeddings.\nWhile the empirical calibration process allows us to circumvent this,\nit is essential to understand the nature of intervening program em-\nbeddings. This is an investigation that we prioritize for future work.\nOverall, results from this study demonstrate a promising method\nto construct an automated system for measuring design pattern\ncompliance using neural language models trained on source code.\n6 DISCUSSION\nRelationship with embedding regularities â€“ The central idea\nof our PLM-based system for design compliance, captured in Eq.7,\nis whether the representation of average jointness ğ‘Ÿ serves as an\neffective offset vector for query embeddings (ğ‘’ğ‘‹,ğ‘’ğ‘Œ). For this con-\ndition to hold across several possible queries, the embeddings of\ncontroller and handler programs need to follow a specific pattern\nof arrangement. The geometrical relationship between the embed-\ndings of semantically related entities has been the focus of extensive\nstudy in neural natural language processing. Most famously, [21]\nstudied regularities in the embeddings of word pairs that are associ-\nated by a similar concept. Using pairs of words (ğ‘€ğ‘ğ‘›,ğ‘Šğ‘œğ‘šğ‘ğ‘›)and\n(ğ¾ğ‘–ğ‘›ğ‘”,ğ‘„ğ‘¢ğ‘’ğ‘’ğ‘› ), the word2vec language model has been shown to\nlearn embeddings such thatğ‘’ğ¾ğ‘–ğ‘›ğ‘”âˆ’ğ‘’ğ‘€ğ‘ğ‘›+ğ‘’ğ‘Šğ‘œğ‘šğ‘ğ‘› â‰ˆğ‘’ğ‘„ğ‘¢ğ‘’ğ‘’ğ‘›. If the\nmodel has a proper understanding of the analogical relationship be-\ntween these pairs of words then, as shown by [21], these four word\nembeddings approximate a parallelogram. Building upon this idea,\n[34] showed that embeddings of pairs of sentences that are related\nby the same concept show a similar parallelogram geometry. Our\nentire approach can be reinterpreted as examining the embedding\nregularities of pairs (ğ‘‹,ğ‘Œ )of programs that are related by the same\nconcept â€“ the CH design pattern. If the neural PLMFused in the as-\nsessment system correctly encodes the jointness that underlies the\nCH design pattern, embeddings of pairs of programs (ğ¶1,ğ»1)and\n(ğ¶2,ğ»2)that implement this pattern should approximate a parallel-\nogram. In which case, ğ‘’ğ¶2 +(ğ‘’ğ»1 âˆ’ğ‘’ğ¶1 )â‰ˆ ğ‘’ğ»2 must hold, which is\na special case of the average jointness benchmark with one known\npattern instance. If this parallelogram geometry consistently holds\nacross several instances of the pattern, the average jointness vector\nğ‘Ÿnaturally serves as an effective offset between the program embed-\ndings of any given instance(ğ‘‹,ğ‘Œ ). Further, since regularity is essen-\ntial for compliance usingğ‘Ÿas the offset, we reason that clustering ob-\njectives (Eqs.10 and 11) strengthens it, improving the quality of the\nassessment process. Additionally, [9] formalized the idea of testing\nthe regularity of one pair of related words using the average offset of\nother pairs of similarly related words â€“ a technique that they refer to\nas 3CosAvg. Their use of the average offset closely reflects our con-\nstruction of the average jointnessğ‘Ÿas the benchmark for assessment.\nMeasuring design compliance using neural language models PROMISE â€™22, November 17, 2022, Singapore, Singapore\nThe fact that the system we design for design compliance assess-\nment is firmly grounded in extensively studied properties of neural\nlanguage models, inspires further confidence in our approach.\nThe quality of program embeddings â€“ The system we construct\nfor assessing compliance with a design pattern is built upon pro-\ngram embeddings, which are vector representations of programs\nextracted from the PLM F. The quality of the assessment process is\ntherefore highly dependent upon the quality of the representation.\nAmong the factors that influence this quality, perhaps the most\nimportant is the objective that is used to train the model. PLMs used\nin our study are primarily trained using the masked reconstruction\ntask shown in Eq.3. The simplicity of the MR task is undoubtedly its\nkey advantage. However, a major shortcoming of the BERT mask-\ning recipe is that, by uniformly choosing 15% of the tokens to be\nmasked, only tokens that are numerically abundant â€“ but semanti-\ncally less significant (like;) â€“ are more likely to be masked. In order\nto successfully reconstruct a token like ; it is often sufficient to\nsimply learn concepts in a local scope, like the likelihood of the end\nof a statement. Thus, with the model rarely being tasked with recon-\nstructing tokens that are semantically significant, it is less equipped\nto learn global concepts like design. This could explain why the base\nmodel Fğ´, which is pre-trained only using MR, performs worst. This\nweakness of MR is well-documented in literature and several inter-\nesting alternatives have been proposed that encourage the model\nto learn more global concepts. One option is to modify the mask-\ning recipe like [29], which masks selected phrases and [16], which\nmasks larger spans of tokens. Another option is to use [6] and [18],\nwhich task a model to detect replaced, permuted, inserted or deleted\ntokens. As tasks that are more complex than reconstructing simple\ntokens, they encourage the model to gain a deeper understanding of\nprogram contexts. Another interesting alternative class of training\nobjectives are those that selectively obfuscate tokens. For instance, a\nde-obfuscation objective proposed by [27] obfuscates class, method,\nand variable names before tasking the model to recover them. Since\nthe successful completion of this task requires a deeper and broader\nunderstanding of the program, they may lead to embeddings that\nare better suited for a design assessment. While we reason the fine-\ntuning objectives that improve domain and design-related aware-\nness (Eqs.10 and 11) are likely to remain important, setting a task\nthat is more complex than MR may result in a much more powerful\nbase model Fğ´. We leave this investigation for future work.\nTraining beyond code â€“ Our results clearly show that it is pos-\nsible to construct a system for assessing design compliance using\nPLMs trained on source code. However, we do not necessarily advo-\ncate a code-only training approach for imparting design knowledge.\nIn addition to source code, automotive software engineering, which\nfollows the AUTOSAR standard, captures additional engineering\ninformation using the standard ARXML modeling language. From\nthe perspective of design awareness, would it therefore be helpful\nto explicitly train PLMs with ARXML models? The answer depends,\nof course, upon whether such models provide additional design\nawareness. If most of the information in ARXML models is likely\nto be replicated in code, then using them for training is unlikely\nto enhance design understanding. Otherwise, if design models do\ncontain some information not discernible in code, it may indeed be\nhelpful to additionally train with such information. Assessing the\nusefulness of engineering information in ARXML for design compli-\nance assessment is an investigation that we leave for future work.\n7 RELATED WORK\nTo the best of our knowledge, our work is the first attempt to apply\nneural language models for measuring design compliance. In soft-\nware engineering, our work closely relates with the task of design\npattern detection. A recent survey of this area [ 33] reveals that\naround 20% of reported methods take a machine learning approach,\nmostly using classical algorithms. Examples include [ 30] which\ncompares pattern instances by modeling them as graphs, and [23]\nand [22] which use artificial neural network and random forest\nmodels respectively to classify pattern instances. We reason that\nthe key advantage of our use of neural language models is the level\nof nuance that it can apply for judging design. A BERT-like PLM,\nwhich has been shown to learn nuanced contextual information,\ncould be vital for assessing design, where firm judgments are rare.\nAlso, unlike the majority focus on pattern detection, we develop\na technique for measuring compliance with a given pattern, includ-\ning steps to identify the source of deviation. Moreover, our study\nfocusing upon embedded control systems would also be a useful\naddition to an area that mostly focuses on object-oriented design.\nAs discussed in detail in Section 6, our approach for compliance\nassessment closely relates to the property of linguistic regularity ob-\nserved in neural natural language models [21]. Most experiments, as\nsurveyed in [2], study this property as a way to evaluate the quality\nof word embedding models. Few of them apply this property in a pre-\ndictive setting by framing an analogy completion task where, given\na triplet (ğ´,ğµ,ğ¶ ), they predictğ·such that (ğ´,ğµ)and (ğ¶,ğ·)are ana-\nlogical pairs. Studies [11] and [19] approach this task respectively\nusing popular word2vec and GloVe embedding models, while [7]\nuses sense embeddings derived from word2vec. An example of the\nproperty being studied in a specialist domain is [5] which fine-tunes\nGloVe on a corpus related to radiology, and uses its embeddings for\nthe analogy completion task. Similar to our departure from word em-\nbedding models, [31] studies this property in pre-trained contextual\nneural language models. The work we survey can therefore be seen\nto relate to parts of our assessment system, but we build a pipeline\nthat not only analyzes embedding regularity, but also interprets it\nwithin the context of software and its design. In doing so, we also\ntie the property of embedding regularities to a concrete application.\n8 CONCLUSIONS\nThis work demonstrates how neural language models trained on\nsource code can be used to measure whether a set of programs\ncomply with desired design properties. Compliance is measured by\ninspecting the geometrical properties â€“ specifically the regularity â€“\nof query program embeddings. Our work also includes techniques\nto significantly improve the accuracy of the assessment by explicitly\nproviding the model with domain and design-related information.\nExperiments performed on an automotive code corpus result in a\nprediction precision of 92%. We also present how the model predic-\ntions can be incorporated into a design review methodology in order\nto provide valuable feedback to automotive software architects.\nPROMISE â€™22, November 17, 2022, Singapore, Singapore Parthasarathy, et al.\nACKNOWLEDGMENTS\nThis work is partially funded by Chalmers AI Research Center\n(CHAIR) project T4AI.\nREFERENCES\n[1] Miltiadis Allamanis, Earl T. Barr, Premkumar T. Devanbu, and Charles Sutton.\n2018. A Survey of Machine Learning for Big Code and Naturalness.ACM Comput.\nSurv. 51, 4 (2018), 81:1â€“81:37. https://doi.org/10.1145/3212695\n[2] Amir Bakarov. 2018. A survey of word embeddings evaluation methods. arXiv\npreprint arXiv:1801.09536 (2018).\n[3] Stefan Bunzel. 2011. AUTOSAR - the Standardized Software Architecture.Inform.\nSpektrum 34, 1 (2011), 79â€“83. https://doi.org/10.1007/s00287-010-0506-7\n[4] Lianping Chen, Muhammad Ali Babar, and Bashar Nuseibeh. 2013. Characterizing\nArchitecturally Significant Requirements. IEEE Software 30, 2 (2013), 38â€“45.\nhttps://doi.org/10.1109/MS.2012.174\n[5] Timothy L Chen, Max Emerling, Gunvant R Chaudhari, Yeshwant R Chillakuru,\nYoungho Seo, Thienkhai H Vu, and Jae Ho Sohn. 2021. Domain specific word\nembeddings for natural language processing in radiology. Journal of biomedical\ninformatics 113 (2021), 103665.\n[6] Kevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020.\nELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.\nIn 8th International Conference on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020 . OpenReview.net. https://openreview.net/\nforum?id=r1xMH1BtvB\n[7] JÃ©ssica Rodrigues da Silva and Helena de Medeiros Caseli. 2020. Generating sense\nembeddings for syntactic and semantic analogy for Portuguese. arXiv preprint\narXiv:2001.07574 (2020).\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:\nPre-training of Deep Bidirectional Transformers for Language Understanding,\nJill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association for Compu-\ntational Linguistics, 4171â€“4186. https://doi.org/10.18653/v1/n19-1423\n[9] Aleksandr Drozd, Anna Gladkova, and Satoshi Matsuoka. 2016. Word Embed-\ndings, Analogies, and Machine Learning: Beyond king - man + woman = queen.\nIn COLING 2016, 26th International Conference on Computational Linguistics, Pro-\nceedings of the Conference: Technical Papers, December 11-16, 2016, Osaka, Japan .\nACL, 3519â€“3530. https://aclanthology.org/C16-1332/\n[10] Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng Feng, Ming Gong,\nLinjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou. 2020. CodeBERT:\nA Pre-Trained Model for Programming and Natural Languages. In Findings of\nthe Association for Computational Linguistics: EMNLP 2020, Online Event, 16-\n20 November 2020 (Findings of ACL) , Trevor Cohn, Yulan He, and Yang Liu\n(Eds.), Vol. EMNLP 2020. Association for Computational Linguistics, 1536â€“1547.\nhttps://doi.org/10.18653/v1/2020.findings-emnlp.139\n[11] Edouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomas\nMikolov. 2018. Learning word vectors for 157 languages. arXiv preprint\narXiv:1802.06893 (2018).\n[12] Suchin Gururangan, Ana Marasovic, Swabha Swayamdipta, Kyle Lo, Iz Beltagy,\nDoug Downey, and Noah A. Smith. 2020. Donâ€™t Stop Pretraining: Adapt Language\nModels to Domains and Tasks. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , Dan\nJurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association\nfor Computational Linguistics, 8342â€“8360. https://doi.org/10.18653/v1/2020.acl-\nmain.740\n[13] Abram Hindle, Earl T. Barr, Zhendong Su, Mark Gabel, and Premkumar T. De-\nvanbu. 2012. On the naturalness of software. In 34th International Conference\non Software Engineering, ICSE 2012, June 2-9, 2012, Zurich, Switzerland , Martin\nGlinz, Gail C. Murphy, and Mauro PezzÃ¨ (Eds.). IEEE Computer Society, 837â€“847.\nhttps://doi.org/10.1109/ICSE.2012.6227135\n[14] ISO/IEC TR 19759:2015 2015. Software Engineering â€” Guide to the software\nengineering body of knowledge (SWEBOK) . Standard. International Organization\nfor Standardization, Geneva, CH.\n[15] Shugang Jiang. 2019. Vehicle E/E Architecture and Its Adaptation to New\nTechnical Trends. In WCX SAE World Congress Experience . SAE International.\nhttps://doi.org/10.4271/2019-01-0862\n[16] Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer, and\nOmer Levy. 2020. SpanBERT: Improving Pre-training by Representing and\nPredicting Spans. Trans. Assoc. Comput. Linguistics 8 (2020), 64â€“77. https:\n//transacl.org/ojs/index.php/tacl/article/view/1853\n[17] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The\nEfficient Transformer. In 8th International Conference on Learning Represen-\ntations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 . OpenReview.net.\nhttps://openreview.net/forum?id=rkgNKkHtvB\n[18] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman\nMohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART:\nDenoising Sequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension. In Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, ACL 2020, Online, July 5-10, 2020 , Dan\nJurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault (Eds.). Association\nfor Computational Linguistics, 7871â€“7880. https://doi.org/10.18653/v1/2020.acl-\nmain.703\n[19] Suryani Lim, Henri Prade, and Gilles Richard. 2021. Classifying and complet-\ning word analogies by machine learning. International Journal of Approximate\nReasoning 132 (2021), 1â€“25.\n[20] Anders Magnusson, Leo Laine, and Johan Lindberg. 2018. Rethink EE architecture\nin automotive to facilitate automation, connectivity, and electro mobility. In\nProceedings of the 40th International Conference on Software Engineering: Software\nEngineering in Practice, ICSE (SEIP) 2018, Gothenburg, Sweden, May 27 - June 03,\n2018, Frances Paulisch and Jan Bosch (Eds.). ACM, 65â€“74. https://doi.org/10.\n1145/3183519.3183526\n[21] TomÃ¡s Mikolov, Wen-tau Yih, and Geoffrey Zweig. 2013. Linguistic Regularities\nin Continuous Space Word Representations. InHuman Language Technologies:\nConference of the North American Chapter of the Association of Computational\nLinguistics, Proceedings, June 9-14, 2013, Westin Peachtree Plaza Hotel, Atlanta,\nGeorgia, USA , Lucy Vanderwende, Hal DaumÃ© III, and Katrin Kirchhoff (Eds.).\nThe Association for Computational Linguistics, 746â€“751. https://aclanthology.\norg/N13-1090/\n[22] Najam Nazar, Aldeida Aleti, and Yaokun Zheng. 2022. Feature-based software\ndesign pattern detection. Journal of Systems and Software 185 (2022), 111179.\nhttps://doi.org/10.1016/j.jss.2021.111179\n[23] Roy Oberhauser. 2020. A Machine Learning Approach Towards Automatic\nSoftware Design Pattern Recognition Across Multiple Programming Languages.\nIn Proceedings of the Fifteenth International Conference on Software Engineering\nAdvances. IARIA, 27â€“32.\n[24] Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher\nClark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep Contextualized Word Rep-\nresentations. In Proceedings of the 2018 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies,\nNAACL-HLT 2018, New Orleans, Louisiana, USA, June 1-6, 2018, Volume 1 (Long\nPapers), Marilyn A. Walker, Heng Ji, and Amanda Stent (Eds.). Association for\nComputational Linguistics, 2227â€“2237. https://doi.org/10.18653/v1/n18-1202\n[25] Yewen Pu, Karthik Narasimhan, Armando Solar-Lezama, and Regina Barzilay.\n2016. sk_p: a neural program corrector for MOOCs. In Companion Proceedings\nof the 2016 ACM SIGPLAN International Conference on Systems, Programming,\nLanguages and Applications: Software for Humanity, SPLASH 2016, Amsterdam,\nNetherlands, October 30 - November 4, 2016 , Eelco Visser (Ed.). ACM, 39â€“40.\nhttps://doi.org/10.1145/2984043.2989222\n[26] Jack W Reeves. 1992. What is software design. C++ Journal 2, 2 (1992), 14â€“12.\n[27] Baptiste RoziÃ¨re, Marie-Anne Lachaux, Marc Szafraniec, and Guillaume Lample.\n2021. DOBF: A Deobfuscation Pre-Training Objective for Programming Lan-\nguages. CoRR abs/2102.07492 (2021). arXiv:2102.07492 https://arxiv.org/abs/2102.\n07492\n[28] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine\nTranslation of Rare Words with Subword Units. InProceedings of the 54th Annual\nMeeting of the Association for Computational Linguistics, ACL 2016, August 7-12,\n2016, Berlin, Germany, Volume 1: Long Papers . The Association for Computer\nLinguistics. https://doi.org/10.18653/v1/p16-1162\n[29] Yu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng, Xuyi Chen, Han Zhang,\nXin Tian, Danxiang Zhu, Hao Tian, and Hua Wu. 2019. ERNIE: Enhanced\nRepresentation through Knowledge Integration. CoRR abs/1904.09223 (2019).\narXiv:1904.09223 http://arxiv.org/abs/1904.09223\n[30] Hannes Thaller, Lukas Linsbauer, and Alexander Egyed. 2019. Feature maps: A\ncomprehensible software representation for design pattern detection. In 2019\nIEEE 26th international conference on software analysis, evolution and reengineering\n(SANER). IEEE, 207â€“217.\n[31] Asahi Ushio, Luis Espinosa-Anke, Steven Schockaert, and Jose Camacho-Collados.\n2021. BERT is to NLP what AlexNet is to CV: can pre-trained language models\nidentify analogies? arXiv preprint arXiv:2105.04949 (2021).\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nAll you Need. In Advances in Neural Information Processing Systems 30: An-\nnual Conference on Neural Information Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA , Isabelle Guyon, Ulrike von Luxburg, Samy\nBengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman\nGarnett (Eds.). 5998â€“6008. https://proceedings.neurips.cc/paper/2017/hash/\n3f5ee243547dee91fbd053c1c4a845aa-Abstract.html\n[33] Hadis Yarahmadi and Seyed Mohammad Hossein Hasheminejad. 2020. Design\npattern detection approaches: a systematic review of the literature. Artif. Intell.\nRev. 53, 8 (2020), 5789â€“5846. https://doi.org/10.1007/s10462-020-09834-5\n[34] Xunjie Zhu and Gerard de Melo. 2020. Sentence Analogies: Linguistic Regularities\nin Sentence Embeddings. In Proceedings of the 28th International Conference on\nComputational Linguistics, COLING 2020, Barcelona, Spain (Online), December\n8-13, 2020 . International Committee on Computational Linguistics, 3389â€“3400.\nhttps://doi.org/10.18653/v1/2020.coling-main.300",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.750667929649353
    },
    {
      "name": "Automotive industry",
      "score": 0.6328458189964294
    },
    {
      "name": "Software",
      "score": 0.5673067569732666
    },
    {
      "name": "Software engineering",
      "score": 0.5442491173744202
    },
    {
      "name": "Construct (python library)",
      "score": 0.532353401184082
    },
    {
      "name": "Software design",
      "score": 0.5213326811790466
    },
    {
      "name": "Code refactoring",
      "score": 0.5092209577560425
    },
    {
      "name": "Implementation",
      "score": 0.48539412021636963
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.47640788555145264
    },
    {
      "name": "Abstraction",
      "score": 0.4115031957626343
    },
    {
      "name": "Programming language",
      "score": 0.3861260414123535
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33328956365585327
    },
    {
      "name": "Software development",
      "score": 0.3121011257171631
    },
    {
      "name": "Engineering",
      "score": 0.15756160020828247
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1340210623",
      "name": "Volvo (Sweden)",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I66862912",
      "name": "Chalmers University of Technology",
      "country": "SE"
    }
  ],
  "cited_by": 6
}