{
  "title": "Hallucination Detection in Foundation Models for Decision-Making: A Flexible Definition and Review of the State of the Art",
  "url": "https://openalex.org/W4393213364",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4227973721",
      "name": "Chakraborty, Neeloy",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2747593485",
      "name": "Ornik, Melkior",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A4223240089",
      "name": "Driggs-Campbell, Katherine",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3204920036",
    "https://openalex.org/W4396821279",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W3043705189",
    "https://openalex.org/W4378465094",
    "https://openalex.org/W2962985038",
    "https://openalex.org/W4377371831",
    "https://openalex.org/W6798182279",
    "https://openalex.org/W4387848774",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W4396814509",
    "https://openalex.org/W6803096969",
    "https://openalex.org/W1480909796",
    "https://openalex.org/W4402670429",
    "https://openalex.org/W6848909144",
    "https://openalex.org/W1995737348",
    "https://openalex.org/W4386184855",
    "https://openalex.org/W2587741066",
    "https://openalex.org/W4390970405",
    "https://openalex.org/W3120376221",
    "https://openalex.org/W2054497239",
    "https://openalex.org/W2016289761",
    "https://openalex.org/W2754517384",
    "https://openalex.org/W4391407102",
    "https://openalex.org/W4388585881",
    "https://openalex.org/W6862028523",
    "https://openalex.org/W3040318838",
    "https://openalex.org/W6810242208",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W4386651449",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2963091658",
    "https://openalex.org/W4391766803",
    "https://openalex.org/W4400716773",
    "https://openalex.org/W6839034341",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W6861442871",
    "https://openalex.org/W6855732556",
    "https://openalex.org/W6849898756",
    "https://openalex.org/W4389519598",
    "https://openalex.org/W4366208220",
    "https://openalex.org/W4381806328",
    "https://openalex.org/W4387835442",
    "https://openalex.org/W4322718421",
    "https://openalex.org/W4393177760",
    "https://openalex.org/W4384648205",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W6856227841",
    "https://openalex.org/W3212580602",
    "https://openalex.org/W4386066536",
    "https://openalex.org/W3209532394",
    "https://openalex.org/W6685112792",
    "https://openalex.org/W6859341508",
    "https://openalex.org/W3148728049",
    "https://openalex.org/W4399768519",
    "https://openalex.org/W4362663195",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W3206968861",
    "https://openalex.org/W2963961878",
    "https://openalex.org/W4400024950",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4383987670",
    "https://openalex.org/W4378505261",
    "https://openalex.org/W4414556916",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W6856594407",
    "https://openalex.org/W4387323020",
    "https://openalex.org/W4367365458",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4387390444",
    "https://openalex.org/W4386755313",
    "https://openalex.org/W6758084534",
    "https://openalex.org/W967544008",
    "https://openalex.org/W6857785731",
    "https://openalex.org/W4388182168",
    "https://openalex.org/W4386501849",
    "https://openalex.org/W4321472314"
  ],
  "abstract": "Autonomous systems are soon to be ubiquitous, spanning manufacturing, agriculture, healthcare, entertainment, and other industries. Most of these systems are developed with modular sub-components for decision-making, planning, and control that may be hand-engineered or learning-based. While these approaches perform well under the situations they were specifically designed for, they can perform especially poorly in out-of-distribution scenarios that will undoubtedly arise at test-time. The rise of foundation models trained on multiple tasks with impressively large datasets has led researchers to believe that these models may provide ‚Äúcommon sense‚Äù reasoning that existing planners are missing, bridging the gap between algorithm development and deployment. While researchers have shown promising results in deploying foundation models to decision-making tasks, these models are known to hallucinate and generate decisions that may sound reasonable but are in fact poor. We argue there is a need to step back and simultaneously design systems that can quantify the certainty of a model‚Äôs decision and detect when it may be hallucinating. In this work, we discuss the current use cases of foundation models for decision-making tasks, provide a general definition for hallucinations with examples, discuss existing approaches to hallucination detection and mitigation with a focus on decision problems, present guidelines, and explore areas for further research in this exciting field.",
  "full_text": "Hallucination Detection in Foundation Models for\nDecision-Making: A Flexible Definition and Review of the\nState of the Art\nNEELOY CHAKRABORTY, University of Illinois Urbana-Champaign, USA\nMELKIOR ORNIK, University of Illinois Urbana-Champaign, USA\nKATHERINE DRIGGS-CAMPBELL, University of Illinois Urbana-Champaign, USA\nAutonomous systems are soon to be ubiquitous, spanning manufacturing, agriculture, healthcare, enter-\ntainment, and other industries. Most of these systems are developed with modular sub-components for\ndecision-making, planning, and control that may be hand-engineered or learning-based. While these ap-\nproaches perform well under the situations they were specifically designed for, they can perform especially\npoorly in out-of-distribution scenarios that will undoubtedly arise at test-time. The rise of foundation models\ntrained on multiple tasks with impressively large datasets has led researchers to believe that these models may\nprovide ‚Äúcommon sense‚Äù reasoning that existing planners are missing, bridging the gap between algorithm\ndevelopment and deployment. While researchers have shown promising results in deploying foundation\nmodels to decision-making tasks, these models are known to hallucinate and generate decisions that may\nsound reasonable, but are in fact poor. We argue there is a need to step back and simultaneously design systems\nthat can quantify the certainty of a model‚Äôs decision, and detect when it may be hallucinating. In this work,\nwe discuss the current use cases of foundation models for decision-making tasks, provide a general definition\nfor hallucinations with examples, discuss existing approaches to hallucination detection and mitigation with a\nfocus on decision problems, present guidelines, and explore areas for further research in this exciting field.\nCCS Concepts: ‚Ä¢ Computing methodologies ‚ÜíArtificial intelligence ; Machine learning ; Natural\nlanguage generation; Machine learning approaches .\nAdditional Key Words and Phrases: Foundation Models, Decision-Making, Hallucination Detection and\nMitigation, Survey\nACM Reference Format:\nNeeloy Chakraborty, Melkior Ornik, and Katherine Driggs-Campbell. 2025. Hallucination Detection in Foun-\ndation Models for Decision-Making: A Flexible Definition and Review of the State of the Art. ACM Comput.\nSurv. 1, 1, Article 1 (January 2025), 55 pages. https://doi.org/10.1145/3716846\n1 Introduction\nA great deal of progress has been made in the last decade and a half with regards to the efficacy\nand efficiency of models for perception, decision-making, planning, and control [88, 195]. Broadly\nspeaking, approaches to these problems fall under one of two umbrellas: hand-engineered model-\nbased systems and data-driven learning-based models [ 61]. With some deployment scenario in\nmind, developers may hand-engineer rules [72] or tune a controller [18] to be tested, or in the case\nThis work is supported by the Office of Naval Research under Grant No.: N00014-23-1-2651.\nAuthors‚Äô Contact Information: Neeloy Chakraborty, neeloyc2@illinois.edu, University of Illinois Urbana-Champaign,\nDepartment of Electrical and Computer Engineering, Coordinated Science Laboratory, Urbana, IL, USA; Melkior Ornik,\nmornik@illinois.edu, University of Illinois Urbana-Champaign, Department of Aerospace Engineering, Talbot Laboratory,\nUrbana, IL, USA; Katherine Driggs-Campbell, krdc@illinois.edu, University of Illinois Urbana-Champaign, Department of\nElectrical and Computer Engineering, Coordinated Science Laboratory, Urbana, IL, USA.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.\n¬© 2025 Copyright held by the owner/author(s).\nACM 1557-7341/2025/1-ART1\nhttps://doi.org/10.1145/3716846\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\narXiv:2403.16527v2  [cs.AI]  11 Feb 2025\n1:2 Chakraborty et al.\nof learning-based models, collect training data and craft some reward function to fit a model to an\nobjective, given said data [75]. In practice, these methods work particularly well in the scenarios that\nthey were specifically designed and trained for, but may produce undesirable results in previously\nunseen out-of-distribution cases [ 221]. Designers may choose to add more rules, re-tune their\ncontroller, fine-tune their model to a more representative dataset, fix the reward function to handle\nedge cases, or even add a detector (which may itself be rule-based or data-driven) at test-time to\nidentify out-of-distribution scenarios before calling on the decision-maker [25, 183, 192]. However,\neven with these changes, there will always be other situations that designers had not previously\nconsidered which will come about during deployment, leading to sub-optimal performance or\ncritical failures. Furthermore, the modifications made to the model may have unforeseen effects at\ntest-time like undesired conflicting rules [57] or catastrophic forgetting of earlier learned skills [95].\nInformally, classical methods and data-driven approaches lack some form of common sense that\nhumans use to adapt in unfamiliar circumstances [64]. More recently, there has been work towards\ndeveloping multi-modal large language models, which take inputs in the form of images, videos,\naudio, and text, to tackle more complex language understanding and reasoning tasks [10, 17]. These\nmodels are developed by collecting and cleaning an enormous natural language dataset, pre-training\nto reconstruct sentences on said dataset, fine-tuning on specific tasks (e.g., question-answering),\nand applying human-in-the-loop reinforcement learning to produce more reasonable responses [1].\nEven though these models are another form of data-driven learning that attempt to maximize the\nlikelihood of generated text conditioned on a given context, researchers have shown that they have\nthe ability to generalize to tasks they have not been trained on, and reason about their decisions.\nMany researchers are specifically exploring the use of large (visual) language models, L(V)LMs,\nto fill the knowledge gap found in earlier works [42]. As such, these foundation models are being\ntested in tasks like simulated decision-making [84] and real-world robotics [247] to take the place\nof perception, planning, and control modules. Even so, foundation models are not without their\nlimitations. Specifically, these models have a tendency to hallucinate, i.e., generate decisions or\nreasoning that sound plausible, but are in fact inaccurate or would result in undesired effects in\nthe world [44, 90, 173]. This phenomenon has led to the beginning of a new research direction\nthat attempts to detect when L(V)LMs hallucinate so as to produce more trustworthy and reliable\nsystems. Before these large black-box systems are applied in safety-critical situations, there need to\nbe methods to detect and mitigate hallucinations. Thus, this survey collects and discusses current\nhallucination mitigation techniques for foundation models in decision-making tasks, and presents\npotential research directions.\nExisting surveys particularly focus on presenting methods for hallucination detection and mitiga-\ntion in question-answering (QA) [90, 173, 240, 253] or object detection tasks [119]. For example, Ji\net al. [90] summarize metrics and hallucination detection and mitigation methods for abstractive\nsummarization, dialogue generation, generative question-answering, data-to-text generation, and\nmachine translation for natural language generation models in general. Ye et al. [240] and Rawte\net al. [173] take this work a step further by classifying hallucination mitigation strategies for\nfoundation models in particular, in generative text and multi-modal settings respectively. Zhang\net al. [253] similarly tackle hallucination detection for language foundation models, but limit their\ndefinition of hallucinations to conflicts between generations and inputs, contexts, and facts. The\nauthors also discuss other possible problems in generated outputs, like ambiguity and bias. In the\ndomain of image captioning, Li et al. [119] provide examples of hallucinations and present a new\nmetric to evaluate object hallucinations. More recent surveys, like the ones from Bai et al . [10]\nand Liu et al. [127], dive deeper into hallucination mitigation methods for multi-modal foundation\nmodels applied to image captioning tasks, but lack a general definition for hallucinations that\nencompasses decision-making applications. The extensive survey from Huang et al. [81] primarily\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:3\nfocuses on methods for evaluating the trustworthiness of language models from the fronts of factu-\nality and faithfulness. Their taxonomy also briefly touches on hallucinations in object detection\nusing LVLMs, but like other surveys, ignores broader decision-making deployments. There are also\nother works that provide examples of current use cases of L(V)LMs in autonomous vehicles [236]\nand robotics [247, 249]. Wang et al. [212] perform a deep analysis of the trustworthiness of a variety\nof foundation models and Chen and Shu [27] provide a taxonomy of hallucinations within LLMs,\nbut both exclude applications to general decision problems. To the best of our knowledge, we are\nthe first to propose a general definition of hallucinations that can be flexibly tuned to any particular\ndeployment setting, including commonly found applications to QA or information retrieval, and\nmore recent developments in planning or control. Furthermore, there is no existing work that\nsummarizes state of the art methods for hallucination detection and mitigation approaches within\ndecision-making and planning tasks. Additionally, to the best of our knowledge, ours is the first\nreview to provide guidelines for choosing and designing hallucination intervention algorithms\nacross different application areas.\nIn the remainder of this work, we discuss the current uses of foundation models for decision-\nmaking tasks in Section 2, define and provide examples of hallucinations in Section 3, identify\ncurrent detection methods in Section 4, present guidelines in Section 5, and explore research\ndirections in Section 6.\n2 Foundation Models Making Decisions\nOriginally coined by Bommasani et al. [17], the term foundation models refers to models that are\n‚Äútrained on broad data at scale such that they can be adapted to a wide range of downstream tasks. ‚Äù\nThis approach is in contrast to works that design and train models on a smaller subset of data for\nthe purpose of being deployed to a specific task [233]. The key difference is that foundation models\nundergo a pre-training procedure on a large-scale dataset containing information from a variety of\npossible deployment fields, through which they are expected to learn more general features and\ncorrespondences that may be useful at test-time on a broader set of tasks [256, 258]. Examples of\nexisting pre-trained foundation models span language [20, 48, 206], vision [24, 99, 150], and multi-\nmodal [1, 167] inputs. In this section, we give a brief overview of existing use cases for foundation\nmodels in robotics and autonomous vehicles, and we provide a discussion of other decision-making\nsystems in Appendix A.2. We also succinctly point out hallucinations found in these works and\nleave a lengthier discussion in Section 3.2. Readers should refer to works from Cui et al. [42], Yang\net al. [236], Zeng et al. [247], and Zhang et al. [249] for a deeper review of application areas.\n2.1 Autonomous Driving\nFor the autonomous vehicle domain, researchers have formulated the use of language foundation\nmodels as a fine-tuning and prompt engineering problem [ 220, 221]. An external sub-system is\nusually designed with (1) a perception module to process signals from raw sensors, (2) a memory\nbank of prior important experiences and its corresponding similarity function to find alike scenarios,\nand (3) a prompt generator to convert current sensor data and relevant memories into natural\nlanguage that can be input to the foundation model. Currently, works either fine-tune LLMs with\na few examples, or directly apply the model in a zero-shot manner, on a QA task with driving\nrelated questions. By framing the task in a QA form, researchers have been able to provide context\nto the L(V)LM to probe for high-level natural language decisions [220, 221], path planning [137,\n191], vehicle tracking and trajectory prediction [96, 224], descriptions of the surroundings of the\nvehicle [29, 231], and low-level control [128]. Figure 1 is an example of how a foundation model\nmay be used in an autonomous driving setting, with possible hallucinations in deployment.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:4 Chakraborty et al.\nFig. 1. Example deployment of an LVLM foundation model in an autonomous driving setting.\nHallucinations (pink) may arise at any point in the decision-making pipeline, including information retrieval,\nplanning, perception, and control. In this example, the LVLM correctly queries the map for possible destinations\nof gas stations, but lists a location that is no longer open. Then, when navigating to one of the locations, the\nmodel predicts a path on the map that goes in the wrong direction of traffic flow. Finally, when applied to\nperception tasks for detecting possible pedestrians in front of the vehicle, the model hallucinates nearby\npeople, causing an improper control action.\nHigh-level Decisions. Wen et al. [220] propose DiLu, a framework consisting of reasoning, reflec-\ntion, and memory modules that support an LLM in producing high-level decisions for autonomous\ndriving, and they test their method within a driving simulator environment. Specifically, the reason-\ning module views the current observation of the vehicle, queries the memory module for any similar\nsituations that were encountered in the past, and converts the experience into a prompt, which is\ninput to the LLM. The prompt is formatted such that it elicits chain-of-thought reasoning [217]\nfrom the LLM, which is shown to improve the accuracy of the model. The generated text output\nby the LLM is summarized by the reflection module, and is used to update the memory bank of\nexperiences. A separate decision decoder model converts the summary into a discrete high-level\ndecision (e.g., idle, turn right, accelerate, etc.) to take in the simulator. The same authors have also\nexperimented with prompting the LVLM model, GPT-4V [1], in a zero-shot manner to describe\nthe surroundings of the vehicle, take high-level decisions, and explain why it believes it would\nbe a good action to take [ 221]. They find that GPT-4V is capable of identifying safety-critical\nscenarios and suggests driving more conservatively in those situations. However, like other existing\nvision models [198], it has difficulty in detecting traffic light states. We discuss other hallucination\nexamples in traffic scenarios in Section 3.2.\nPath Planning. Agent-Driver from Mao et al. [137] utilizes a tool library of functions that com-\nmunicate with neural modules that are responsible for object detection, trajectory prediction,\noccupancy estimation, and mapping. The LLM is asked to reason about what information would be\nhelpful to plan a path of the ego vehicle, and calls on functions from the tool library to build up\nrelevant context. Like Wen et al. [220], the authors use a memory bank of prior driving experiences\nto bolster the context provided to the LLM. With this context, the LLM predicts a suitable path for\nthe ego vehicle to follow. If a collision is detected between the predicted trajectory and surrounding\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:5\nobjects in the scene, the LLM undergoes self-reflection, like Reflexion [188], another hallucination\nmitigation technique, to fine-tune its prediction. Through a short study, the authors test the fre-\nquency of invalid, hallucinated outputs from the model, and find that their self-reflection approach\nresults in zero invalid generations at test-time. Sima et al. [191] build up context before predicting\na path for the ego vehicle by asking a VLM questions about its perception of surrounding vehicles,\npredicting their behaviors, planning high-level vehicle decisions, converting to lower level discrete\nactions, and finally, estimating a coordinate-level trajectory. Their method, DriveLM-Agent, predicts\npaths from images in an end-to-end manner using a multi-modal approach, whereas Agent-Driver\nrequires sensor modules to process context separately.\nTrajectory Prediction. Wu et al. [224] propose PromptTrack as a method to predict bounding boxes\nand trajectories of vehicles in multi-view camera scenes conditioned on a text prompt. PromptTrack\nis another end-to-end method that encodes multi-view images with an image encoder, decodes\npreviously tracked boxes and current detections into new tracks, uses a language embedding\nbranch to predict 3D bounding boxes, and updates the memory of current tracks using past & future\nreasoning branches. Rather than using ego vehicle point of view images for object tracking, Keysan\net al. [96] propose an approach to convert rasterized images from a birds-eye view of the scene into\na prompt describing the past trajectories of vehicles with B√©zier curves. This method combines\nvision and language encoders to generate the B√©zier curve-based scene description, and elicits a\nlanguage model to predict trajectories in a similar format.\nScene Understanding. Works using foundation models for generating scene descriptions given\nmulti-modal information frame the task as a QA problem. For example, Chen et al . [29] use a\nreinforcement learning (RL) agent pre-trained on the driving task in simulation to collect a dataset\ncontaining the vehicle‚Äôs state, environment observation (assumed to be ground truth from simulator,)\nlow-level action, and the ego‚Äôs percentage of attention placed on different surrounding agents (if\nusing an attention-based network). The authors ask ChatGPT [147] to act as a professional driving\ninstructor who generates questions and answers given general driving rules and the vectorized\ndescription of the scene from the RL agent. Then, combining a pre-trained LLM with a vector\nembedder and former model, the architecture is trained end-to-end to answer questions from the\nscene. Examples of questions the architecture is posed at test-time include, ‚ÄúWhat objects are you\nobserving, ‚Äù ‚ÄúHow are you going to drive in this situation and why, ‚Äù and, ‚ÄúWhat are the best tourist\nspots in London‚Äù (the last of which is considered out of scope by the driving model). The authors\nacknowledge that the LLM may generate undesired hallucinated outputs during deployment, so\nthey augment their training instruction dataset with out of scope questions that the model should\nlearn to refuse to answer. DriveGPT4, proposed by Xu et al . [231], is a multi-modal LLM using\nLLaMA 2 [207] which encodes video sequences with an encoder model, and projects question and\nvideo embeddings into a text form to be input to the LLM. A decoder model converts the tokenized\noutput of the LLM into a low-level action with a corresponding high-level reason to take the action.\nLike Chen et al., the authors collect a driving QA dataset to align and fine-tune the architecture to\ndriving tasks.\nControl. Liu et al. [128] tackle low-level control within unsignalized intersections by formulating\nthe problem as a multi-task decision-making process. Specifically, they train expert policies with RL\nto perform individual control tasks (e.g., turning left, going forward, and turning right), with which\nthey collect an expert demonstration dataset. An LLM model, GPT-2 [168], is fine-tuned to predict\nnext actions given past trajectories of state and action pairs from the expert dataset. The authors\nshowcase that their method, MTD-GPT, is able to achieve higher success rates across each of the\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:6 Chakraborty et al.\ncontrol tasks over the original RL policies alone, showcasing the promise of foundation models to\nleverage their general knowledge in a multi-task problem setting.\n2.2 Robotics\nFoundation models have also been used in the robotics domain for object detection, affordance\nprediction, grounding, navigation, and communication. An example of a robot deployed with LVLM\ncapabilities and potential hallucinations is shown in Figure 3 of Appendix A.1. Ichter et al . [86]\nare motivated by the issue of misalignment between the capabilities of a robot and what an LLM\nbelieves it is capable of performing. Because LLMs may not specifically be trained with data from\nthe robot it is to be deployed on, there is a gap in the model‚Äôs understanding and the true capacity\nof the robot, which could lead to hallucinated generations that cannot feasibly be used at runtime.\nThe authors propose SayCan as a method to combine the general knowledge of LLMs with the\nspecific capabilities of a robot in the real-world. Specifically, an LLM is given a task in text form,\nand is asked to output a list of smaller actions to take in order to complete said task successfully. To\nconstrain the LLM to generate possible actions available to the robot, they assume access to (1) the\nprobability distribution of next tokens to generate from the model, and (2) a set of available skills\non the robot, with which they compute the probability of the LLM generating each of the skills next.\nSayCan greedily selects the action that has the highest product of the next token probability from\nthe LLM and the probability of the action actually successfully being executed in the environment,\nuntil the model predicts it has completed the task.\nRather than relying purely on textual context, PaLM-E, proposed by Driess et al. [55], is a multi-\nmodal model that converts various sensor inputs (e.g., images) to a token-space embedding that is\ncombined with instruction embeddings to be input to a PaLM LLM [35]. PaLM is used to either\nanswer questions about the surroundings of the robot, or to plan a sequence of actions to perform\nto complete a task. Driess et al . further acknowledge that the multi-modality of their PaLM-E\narchitecture leads to increased risk of hallucinations.\nInspired by recent promising findings in using foundation models to generate programs [31],\nother works deploy foundation models to write low-level code to be run on robots. Liang et al. [121]\npresent Code as Policies, which uses LLMs to hierarchically generate interactive code and functions\nthat can be called. As the model writes main code to be run on a robot given an instructive prompt\nof the task from the user, it identifies functions to call within the higher level code to complete\nthe task successfully. The authors show that LLMs can leverage third party libraries for existing\nfunctions, or develop their own library of functions dynamically with custom methods for the\ntask. While the functionality of Code as Policies can be tested easily for low-level skill definitions,\nlonger multi-step problems require testing whether all requested conditions have been met by\nrunning the generated code on the robot. As such, Hu et al. [80] propose the RoboEval performance\nbenchmark for testing robot-agnostic LLM-generated code. Specifically, the CodeBotler platform\nprovides an LLM access to abstract functions like ‚Äúpick, ‚Äù ‚Äúplace, ‚Äù and ‚Äúget_current_location‚Äù that\nhave the same external interface regardless of the robot to be deployed on. Like Code as Policies,\nCodeBotler is provided a text instruction from the user and generates code to be tested. Then the\nRoboEval benchmark uses RoboEval Temporal Logic (RTL) to test whether the generated code meets\ntask and temporal ordering constraints provided in the original prompt. Furthermore, they test\nthe robustness of the LLM by passing in several paraphrased prompts to check for consistency\nacross inputs. We discuss similar consistency-checking strategies for identifying hallucinations in\ndecision-making tasks further in Section 4.3.1.\nIn the space of robot navigation, LM-Nav leverages a VLM and attempts to predict a sequence of\nwaypoints for a robot to follow and visit landmarks described within a language command [187].\nHere, the authors use in-context learning [53] to teach GPT-3 [20] to extract desired landmarks\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:7\nfrom a natural language instruction. Assuming there are images of the possible landmarks the robot\ncan navigate to in its environment, LM-Nav uses CLIP [167] to predict the closest matching pairs\nof extracted landmark descriptions and waypoint images. Finally, dynamic programming is applied\non the complete graph of the environment to optimize the path of landmarks to visit. The overall\npredicted path is optimized to maximize the likelihood of successfully completing the instruction\ninput to the model.\n3 Hallucinations\nEven with all their success on a multitude of deployment areas, foundation models still produce\ninconsistent outputs, or hallucinate, at test-time. Here, we provide a general definition for hallucina-\ntions that can be applied to any foundation model deployment task, including various autonomous\nsystems. Additionally, we give examples of hallucinations encountered in literature, and discuss\nhow they come about during testing.\n3.1 What are hallucinations?\nAcross current literature on foundation models, there exist similar patterns and themes that can be\nused to develop a unified definition for hallucinations. With the majority of works studying this\nproblem within QA tasks, where ground truth answers are available, several authors explain halluci-\nnations as producing text that includes details/facts/claims that are fictional/misleading/fabricated\nrather than truthful or reliable [173]. Works making use of a dedicated knowledge-base further\ndescribe hallucinations as generating nonsensical or false claims that are unsubstantiated or incor-\nrectly cited [26, 117, 144, 250]. Varshney et al. [210] also present the idea that foundation models\nmay sound syntactically correct, or coherent, while simultaneously being incorrect. Gallifant et al.\n[66], who perform a peer review of the GPT-4 technical paper, state that hallucinations include\nresponses that are irrelevant to the original prompt. Li et al. [119], who specifically explore hallu-\ncinations of LVLMs in detecting and classifying objects within images, define hallucinations as\ngenerating object descriptions inconsistent with target images. A common theme among existing\nhallucination definitions for QA, information retrieval, and image captioning domains is that,\nwhile the generation may sound coherent, either the output is incorrect, or the model‚Äôs reasoning\nbehind the generated text is incorrect. However, we find these characteristics on their own do\nnot completely encompass the hallucinations found in decision-making tasks in literature, thus\nrequiring additional nuances.\nWithin papers that apply foundation models to decision-making tasks specifically, researchers\nhave encountered similar problems of hallucinations impacting performance. Park et al . [154]\ndescribe hallucinations as predicting an incorrect feasibility of an autonomous system when\ngenerating an explanation behind the uncertainty of an action to take. Similarly, Kwon et al. [104]\nfind that language models may provide incoherent reasoning behind their actions. Wang et al .\n[215] and Ren et al. [174] believe that these generative models also have a sense of high (false)\nconfidence when generating incorrect or unreasonable plans. In the case of robot navigation and\nobject manipulation, Hu et al. [80] and Liang et al. [122] refer to hallucinations as attempting to\ninteract with non-existent locations or objects.\nIn the code generation task, Chen et al. [31] use the term ‚Äúalignment failure, ‚Äù with similar effects\nto those of hallucinations discussed above. More specifically, the authors informally describe an\nalignment failure as an outcome where a model is capable of performing a task, but chooses not to.\nIf a model is able to complete a task successfully within its latent space (perhaps through additional\nprompt engineering or fine-tuning), one may ask, ‚ÄúWhy would the model choose not to?‚Äù As\nfoundation models are trained with the next-token reconstruction objective on a training set, they\nattempt to maximize the likelihood of the next token appearing at test-time as well. Consequently,\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:8 Chakraborty et al.\nTable 1. Definitions for compliance, desirability, relevancy, and plausibility ‚Äî the four characteristics\nof hallucinations.\nCharacteristic Definition\nCompliance Generation meets hard constraints that cannot be ignored\nDesirability Generation attempts to meet soft constraints measured by some cost or reward function\nRelevancy Contents of generation do not fall outside of a defined set of topics\nPlausibility The syntactic similarity of a generation and in-domain normal samples measured via a critic function\nif the test-time prompt includes even minor mistakes, Chen et al. find that LLMs will continue to\ngenerate buggy code to match the input prompt. This issue is further described in Section 3.3.\nWe realize existing definitions for hallucinations are extremely disparate depending on the\ndeployment area. Nevertheless, we have identified three distinct features that are commonly\nmissing within hallucinated generations: compliance, desirability, and relevancy. Additionally, we\nfind hallucinations may seem to be plausible while they are in fact unacceptable. A compliance\nmetric checks that the generation meets hard constraints while the desirability metric measures how\nwell the generation meets soft constraints defined by the model engineer. Irrelevant generations\nrefer to predictions that contain details outside of a set of requested topics. Notice that irrelevant\npredictions can still be considered compliant and desirable depending on how the hard and soft\nconstraints are defined. Plausibility compares the generation‚Äôs syntax to those of a set of known,\nunhallucinated samples. While relevancy is evaluating the content of a prediction, plausibility is\nassessing its phrasing. Table 1 provides complete definitions for each of the four characteristics.\nThen, to bridge definitions from existing QA application areas, decision-making tasks, and all\nother possible test scenarios for foundation models, we combine these findings and define the term\nhallucination as follows:\nDefinition 3.1. A hallucination is a generated output from a model that conflicts with constraints or\ndeviates from desired behavior in actual deployment, or is completely irrelevant to the task at hand,\nbut could be deemed syntactically plausible under the circumstances.\nThere are three key pieces to this definition:\n(1) A generated output from a model.\n(2) A deployment scenario to evaluate model outputs with any of the following:\n‚Ä¢A list of constraints that must be compliant within the generation.\n‚Ä¢A loose interpretation of a desired behavior the generation should meet.\n‚Ä¢A set of topics relevant to the task.\n(3) Metrics measuring compliance, desirability, relevancy, and syntactic soundness (plausibility)\nof generations.\nIn practice, this definition generally encapsulates the qualities of hallucinations discussed earlier.\nFor example, in QA or image captioning tasks, one may define a set of relevant topics that a\ngeneration should not stray from, and constraints may be held in the form a knowledge-base of\nground truth facts. The desired behavior of the generation may be to be phrased in an informative\nmanner, rather than sarcastic. On the other hand, in robot manipulation settings, a developer may\nhave a set of constrained actions feasible on the robot, and the desired behavior could be to complete\na task with as few actions as possible. Relevancy may be measured in relation to the specific task\nto be deployed on ( e.g., a prompt requesting a recipe to make pasta would find it irrelevant if\nthe model also suggested a song to play while cooking). Finally, plausibility informally relates\nto a measure of how believable an output is to a critic. A more realistic generation has a greater\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:9\nTable 2. Examples of applying Definition 3.1 to different tasks.Note that developers may choose to only\ndefine a subset of hallucination characteristics for their deployment depending on evaluation preferences.\nThe table is split into non-decision-making and decision-making applications.\nProblem Setting Characteristic\nCompliance Desired Behavior Relevancy Plausibility\nQuestion-AnsweringGenerations must align withdatabase facts Tone of answer should beinformative Answers should not includereferences to unrelated topicsGeneration is syntacticallysound and believableImage CaptioningObjects in description mustappear in image Censor descriptions forinappropriate images\nDescriptions should not beembellished with details thatcannot be confirmed\nPlanning Predicted sub-task must befeasible to solve Plans should maximizeexpected return Predicted sub-tasks andactions should not stray fromthe end goal with added steps\nGenerated plan is reasonableand seems to attempt toaccomplish goalControl Predicted action must bepossible to perform Predict actions to completeplan efficiently\nchance of deceiving the user into trusting the model, even when the plan may be hallucinated.\nOverall, hallucinated outputs may contain one or more of the core characteristics (noncompliant,\nundesired, irrelevant, and plausible) simultaneously, and our definition can be flexibly applied to\nany deployment scenario in mind by choosing metrics for each characteristic, respectively. We\nshow more examples of applying our definition to various tasks in Table 2.\n3.2 Examples\nDriving Tasks. As discussed in Section 2.1, Wen et al . [221] test GPT-4V on the autonomous\ndriving task and identify failure modes. Regardless of the weather and driving conditions, GPT-4V\nhas difficulty detecting and identifying the traffic light state at an intersection, until the image\nhas zoomed in on the light itself. It also presents additional irrelevant (or completely false) details\nabout other agents, when the prompt had no mention of them in the first place. Furthermore, the\nmodel also has difficulty in describing temporal sequences (i.e., videos) and categorizing images by\ntheir direction within a panoramic view from the vehicle‚Äôs perspective. In their later work, Wen\net al. [220] describe that hallucinations arise in these complex environments because of the high\nvariability in driving scenarios. Even after applying hallucination mitigation techniques like chain-\nof-thought reasoning, the model is not free of these undesired outputs. A similar work evaluating\nthe frequency at which LVLMs hallucinate in their descriptions of images, finds that these models‚Äô\noutputs may include non-existent objects, or additional irrelevant phrases (that may not even\nbe possible to test for accuracy) [ 119]. For example, in a picture of food on a table, an LVLM\nhallucinates a non-existent beverage, and predicts that the ‚Äútable is neatly arranged, showcasing\nthe different food items in an appetizing manner. ‚Äù Although the classification error and irrelevant\ngeneration in this example are not critical, earlier works warn of possible failures with more severe,\nhigh societal impact (e.g., biases in models leading to marginalizing users) [17].\nCode Generation. Chen et al. [31] explore alignment failures of LLMs applied to code completion\ntasks. The authors evaluate the likelihood of these models generating defective code given different\ninput prompts, and discover that in-context learning using examples with buggy code has a\nhigher chance of resulting in poor generations from the model on the actual task at hand. The\nstudy also identifies similar model biases towards race, gender, religion, and other representations.\nFurthermore, the authors find that their model, Codex, is able to generate code that could assist\nwith developing insecure applications or malware, albeit in a limited manner. These findings have\nbeen corroborated by other foundation model code generation works in the robotics domain. For\nexample, Wang et al. [213] describe that Voyager sometimes generates code with references to items\nthat do not exist within MineDojo. Similarly, Hu et al. [80] find that their model has the tendency to\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:10 Chakraborty et al.\ncall functions with invalid objects or locations, pickup objects when it is already holding something,\nask for help when no one is near, and other undesired behaviors.\nQuestion-answering Domain. Several works focus on identifying cases of hallucinations in QA\ntasks. Although this application area is not the direct focus of this work, we present examples of\nhallucinations in this field as we can glean similar failure modes that could arise within decision-\nmaking systems. Common hallucinations in QA result in incorrect answers to questions. For\nexample, Achiam et al. [1] find that GPT-4 ‚Äúhallucinates facts and makes reasoning errors. ‚Äù Achiam\net al. categorize these failures into closed-domain (given context, the model generates irrelevant\ninformation that was not in the context) and open-domain (the model outputs incorrect claims\nwithout any context) hallucinations. After fine-tuning on more data with a hallucination mitigation\nobjective, the model reduces its tendency to hallucinate, but still does not achieve perfect accuracy\n‚Äî a similar trend encountered by Touvron et al. [206]. Another set of works identify hallucinations\nwith contradictions among several sampled generations from an LLM, discussed further in Sec-\ntion 4.3.1 [144, 250]. Intuitively, if a context passed into a model results in conflicting generations,\nthe model must be hallucinating some part of the output. Notice in this example, with relation to\nDefinition 3.1, self-contradiction works test for compliance by checkingconsistency among multiple\n(hallucinated) generations, rather than with respect to a ground-truth knowledge-base that usually\nexists in QA tasks. As such, our definition can flexibly apply to different system setups by describing\ncompliance, desired behavior, and relevancy respectively.\nAdditional examples of hallucinations encountered in image, video, and 3D generation methods,\nand broader impacts faced by medical, legal, and finance industries are discussed in Appendix B.1\nand B.2.\n3.3 Why do they happen?\nThere are several speculations as to how hallucinations come about during deployment. First and\nforemost, like any learning task, foundation models are sensitive to biases in training data [173].\nOnce a model is trained on a given large dataset, some facts may become out-of-date or stale at any\npoint in time [162]. Furthermore, as the training set is embedded into a smaller encoding dimension,\nthe knowledge within an L(V)LM‚Äôs frozen parameters is lossy, and models cannot feasibly be fine-\ntuned every time there is new data [58, 157]. Zhang et al. [250] recommend changing algorithm\nparameters at runtime, such as, temperature (spread of probability distribution of next token),top-ùêæ\nsampling (narrows the set of next tokens to be considered), and beam search (choosing a set of\npossible beams, i.e., trajectories, of next tokens based on high conditional probabilities), but the\nprocess of tuning these parameters is expensive.\nTo combat out-of-date training data, some works provide models with an external knowledge-\nbase of information to pull facts from, with the hope of increasing model accuracy. Even with\nthis up-to-date information, Zhang et al. [251] pose that there may exist a misalignment between\nthe true capabilities of a model, and what a user believes the model is capable of, leading to poor\nprompt engineering. In fact, poor prompting is one of the most significant causes of hallucinations.\nChen et al. [31] find that poor quality prompts lead to poor quality generations, in the context\nof code completion. This phenomenon is attributed to the reconstruction training objective of\nLLMs attempting to maximize the likelihood of next generated tokens, given context and past\noutputs [141], i.e.,\nlog ùëù(ùë†|ùë•)=\nùëÅ‚àëÔ∏Å\nùëñ=1\nlog ùëù(ùúéùëñ |ùë•,ùúéùëñ‚àíùëò ...ùúé ùëñ‚àí1)\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:11\nwhere ùë• is a context input to the model, ùë† is an output sequence of ùëÅ tokens ùúé1 ...ùúé ùëÅ , and any\ngenerated token ùúéùëñ is conditioned on ùëò previously generated tokens. As the public datasets these\nmodels are trained on contain some fraction of undesirable generations (e.g., defective code), the\nmodels become biased to generate similar results under those inputs. Qiu et al . [164] show that\nthis limitation can actually be exploited to push foundation models to generate toxic sentences, or\ncompletely lie, by simply rewording the prompt.\nWhile foundation models condition generated tokens on ground-truth text without hallucina-\ntions at train time, during inference, the model chooses future tokens conditioned on previously\n(possibly hallucinated) generated text. As such, Chen et al. [33] and Varshney et al. [210] state that\ngenerated outputs are more likely to contain hallucinations if prior tokens are hallucinated as well.\nFurthermore, Li et al. [115] find that, even if prompt context provided to a foundation model is\nrelevant, the model may choose to ignore the information and revert to its own (possibly outdated\nor biased) parameterized knowledge.\nOverall, the hallucination detection task is highly complex with several possible sources of\nfailures that need to be considered at test-time. Chen and Shu [27] validate the complexity of the\ndetection problem with studies identifying that human- and machine-based detectors have higher\ndifficulty correctly classifying misinformation generated from LLMs than those written by other\npeople.\n4 Detection and Mitigation Strategies\nHallucination detection and mitigation methods can be classified into three types (white-, grey-,\nand black-box) depending on the available inputs to the algorithm. Generally, given some context,\na foundation model outputs a predicted sequence of tokens, the corresponding probabilities of\neach token, and embeddings of the generation from intermediate layers in the network. White-\nbox hallucination detection methods assume access to all three output types, grey-box require\ntoken probabilities, and black-box only need the predicted sequence of tokens. Because not all\nfoundation models provide access to their hidden states, or even the output probability distribution\nof tokens (e.g., the ChatGPT web interface), black-box algorithms are more flexible during testing.\nIn this section, we present existing detection and mitigation approaches clustered by input type.\nWhile several of these works show promise in QA and image captioning settings, many of them\nrequire further validation on decision-making tasks, and we will point out these methods as they\ncome about. Works in this section are summarized in Table 3. We also provide additional details\nabout the evaluation setups of works deployed on custom datasets, simulators, or the real-world.\nCertain methods that are less related to decision-making are described in Appendix C. Additionally,\nfrequently used metrics, datasets, and simulators are summarized in Appendix D.\n4.1 White-box Methods\nMethods in this section require access to internal weights of the model for hallucination detection.\n4.1.1 Hidden States. Some approaches utilize intermediate embeddings at different network layers.\n(1) Azaria and Mitchell [7] empirically find that language models attempt to correct themselves\nafter outputting an untruthful claim. As such, they hypothesize that the internal states\nof a model must have some understanding of whether the output is correct. Furthermore,\nthe authors stray away from directly using the token probabilities, even though they have\ncorrelation with model accuracy [210], because the complete output sentence‚Äôs probability\nis dependent on the length of the generation and appearance frequency of tokens. Azaria\nand Mitchell present SAPLMA, a simple classifier trained with supervised learning, that\ntakes the activation values of a hidden layer of an LLM as input, and outputs the probability\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:12 Chakraborty et al.\nTable 3. A summary of hallucination detection & mitigation methods discussed in Section 4. Deploy-\nment scenarios are split into question-answering (QA), information retrieval (IR), image captioning (IC), image\ngeneration (IG), & planning (P) tasks. The method ID includes the subsection the method appears in the paper\nand the order in which it appears in the subsection. Bolded method IDs are deployed to decision-making\ntasks specifically. Custom datasets, custom simulators, & real-world experiments for testing are abbreviated\nas CD, CS, & RW, respectively.\nModality Method Type Method ID Application Deployment Evaluation Setting\nDetection Mitigation\nWhite-box\nHidden\nStates\n4.1.1.1 ‚Ä¢ QA CD\n4.1.1.2 ‚Ä¢ QA CD\n4.1.1.3 ‚Ä¢ QA DecodingTrust [212]\nTruthfulQA [125]\nAttention\nWeights 4.1.2.1 ‚Ä¢ ‚Ä¢ IC MSCOCO [126]\nVisual Genome [100]\nHonesty\nAlignment\n4.1.3.1 ‚Ä¢ QA CD\n4.1.3.2 ‚Ä¢ ‚Ä¢ QA TriviaQA [92]\nGrey-box\nConcept\nProbabilities\n4.2.1.1 ‚Ä¢ ‚Ä¢ IR/QA HotpotQA [237]\nCD\n4.2.1.2 ‚Ä¢ IC MSCOCO [126]\n4.2.1.3 ‚Ä¢ P\nRavens [246]\nBabyAI [34]\nVirtualHome [161]\nConformal\nPrediction\n4.2.2.1 ‚Ä¢ ‚Ä¢ IR/QA\nMIMIC-CXR [91]\nCNN/DM [77]\nTriviaQA [92]\n4.2.2.2 ‚Ä¢ ‚Ä¢ QA MMLU [76]\n4.2.2.3 ‚Ä¢ ‚Ä¢ P TableSim [174]\nRW\n4.2.2.4 ‚Ä¢ ‚Ä¢ P TableSim [174]\nCD\n4.2.2.5 ‚Ä¢ ‚Ä¢ P CS\nBlack-box\nAnalyzing\nSamples\n4.3.1.1 ‚Ä¢ IR/QA CD\n4.3.1.2 ‚Ä¢ ‚Ä¢ IR/QA CD\n4.3.1.3 ‚Ä¢ ‚Ä¢ IR/QA\nGSM8K [38]\nMMLU [76]\nCD\n4.3.1.4 ‚Ä¢ ‚Ä¢ P\nSaGC [154]\nCS\nRW\nContinued on next page\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:13\nTable 3 ‚Äì continued from previous page\nModality Method Type Method ID Application Deployment Evaluation Setting\nDetection Mitigation\n4.3.1.5 ‚Ä¢ ‚Ä¢ IR/QA CD\nBlack-box\nAnalyzing\nSamples\n4.3.1.6 ‚Ä¢ ‚Ä¢ IR/QA\nQuest [134]\nMultiSpanQA [116]\nFActScore [142]\nCD\n4.3.1.7 ‚Ä¢ IC/QA\nMSCOCO [126]\nA-OKVQA [185]\nGQA [85]\n4.3.1.8 ‚Ä¢ QA\nBIG-Bench [196]\nGSM8K [38]\nMMLU [76]\n4.3.1.9 ‚Ä¢ QA CD\nAdversarial\nPrompting\n4.3.2.1 ‚Ä¢ IG CD\n4.3.2.2 ‚Ä¢ ‚Ä¢ IR/QA Natural Questions [103]\nCD\n4.3.2.3 ‚Ä¢ QA CD\n4.3.2.4 ‚Ä¢ QA CD\nProxy\nModel\n4.3.3.1 ‚Ä¢ IR/QA CD\n4.3.3.2 ‚Ä¢ IR/QA\nSQuAD [169]\nHotpotQA [237]\nTriviaQA [92]\n4.3.3.3 ‚Ä¢ IR/QA CD\n4.3.3.4 ‚Ä¢ IG CD\nGrounding\nKnowledge\n4.3.4.1 ‚Ä¢ IR/QA\nNatural Questions [103]\nStrategyQA [68]\nQRreCC [5]\n4.3.4.2 ‚Ä¢ ‚Ä¢ IR/QA\nLC-QuAD [208]\nKQA-Pro [23]\nScienceQA [132]\n4.3.4.3 ‚Ä¢ ‚Ä¢ IR/QA FuzzyQA [251]\n4.3.4.4 ‚Ä¢ ‚Ä¢ IR/QA CD\n4.3.4.5 ‚Ä¢ P TextWorld [40]\n4.3.4.6 ‚Ä¢ ‚Ä¢ IG CD\n4.3.4.7 ‚Ä¢ IG I 2P [182]\nConstraint\nSatisfaction\n4.3.5.1 ‚Ä¢ ‚Ä¢ P CD\n4.3.5.2 ‚Ä¢ P RoboEval [80]\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:14 Chakraborty et al.\nof the generated claim being true. The authors choose to collect a new dataset of simple\nstatements with corresponding true/false answers, since popular datasets like FEVER [203]\ndo not partition statements by topic, and some statements cannot be cleanly classified. In\ntotal, their dataset contains 6K sentences spanning six topics. SAPLMA is shown to be able\nto identify untruthful outputs, even when trained on a held-out dataset on a completely\ndifferent topic than evaluated on.\n(2) Yao et al. [238] aim to test the resiliency of foundation models to varying prompts (a form\nof adversarial prompting further discussed in Section 4.3.2). They propose perturbing an\ninput prompt with additional tokens so as to make an LLM under test produce a desired\nhallucination (e.g., modify the original query, ‚ÄúWho won the 2020 US election, ‚Äù to get the\nLLM to generate, ‚ÄúDonald Trump was the victor, ‚Äù while its original response was correctly\nstated as, ‚ÄúJoe Biden was the victor‚Äù). As the search space of possible tokens to add/replace\nwhen developing the adversarial prompt is massive, the work uses a gradient-based token\nreplacing strategy. Specifically, they define an objective that attempts to find trigger tokens\nin the direction of the gradient that maximizes the likelihood of the model outputting the\ndesired hallucination. To evaluate their approach, Yao et al. collect a dataset of truthful facts\nby feeding questions from Wikipedia [62] into an LLM. Then, certain subjects, objects, or\npredicates in the generated answers are replaced to manually create hallucinations. With\nsimple prompt modifications, the authors show that the white-box approach is able to induce\nthe specified hallucinations.\n(3) The work from Song et al. [194] is described in Appendix C.1.1.3.\n4.1.2 Attention Weights. Attention weight matrices, which are prominent within transformer\nmodel architectures, signify the importance the model places on earlier tokens within a generation\nwhen predicting future tokens.\n(1) OPERA, proposed by Huang et al. [82], is a hallucination detection method for LVLMs that\nmakes use of the model‚Äôs internal attention weights. When visualizing the attention matrix,\nthe authors find that there exist peculiar column patterns that align with the beginning of a\nhallucinated phrase. These aggregation patterns usually occur on a non-substantial token like\na period or quotation mark, but are deemed to have a large impact on the prediction of future\ntokens. As such, this finding led Huang et al. to modify the beam search algorithm [63] by\napplying a penalty term to beams wherever an aggregation pattern is detected, and roll back\nthe search to before the pattern arises. Their method is shown to reduce hallucinations, and\neven eliminate possible repetitions in generations.\n4.1.3 Honesty Alignment. In addition to methods that require hidden states or attention matrices,\nwe also include methods that fine-tune foundation models to better communicate their uncertainty\nto questions under white-box algorithms, as they require access to model weights for training.\n(1) For example, Lin et al. [124] collect a calibration dataset of questions and answers from GPT-3\nunder 21 types of arithmetic tasks (e.g., add/subtract and multiply/divide), and record how\noften each task is incorrectly answered. They aim to fine-tune the LLM to also output its\ncertainty that the prediction is correct. Consequently, Lin et al. fine-tune the model with data\npairs of a question and the empirical accuracy on the task that the question originates from\nin the calibration dataset, such that the model is expected to similarly output a probability\nof accuracy at test-time. The authors show that the proposed verbalized probability in\ndeployment does correlate with actual accuracy on the tasks. Specifically, Lin et al. find that\ncertain problems, like multiplication, are more challenging for GPT-3 to answer correctly.\nBy calibrating the model on a set of simpler questions (add/subtract), the model is able to\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:15\ngeneralize its verbal uncertainty to more challenging tasks with multiple possible answer\nchoices.\n(2) The work from Yang et al. [234] is described in Appendix C.1.3.2.\n4.2 Grey-box Methods\nGrey-box approaches leverage the probability distributions of tokens output from the model.\n4.2.1 Concept Probabilities.\n(1) Empirically, Varshney et al. [210] show that there is a negative correlation between halluci-\nnation rate and token probability (i.e., as a token‚Äôs probability decreases within a sentence,\nthe tendency to hallucinate increases). Thus, the authors rely on token probabilities to esti-\nmate uncertainty of concepts within a generated claim, and they check for correctness by\ncross-referencing a knowledge-base. Whenever a concept is found to be conflicting with a\nfact through verification questions, their method attempts to mitigate the error by prompting\nthe LLM to replace the incorrect claim with the evidence. The authors query GPT-3.5 for\nsummaries about 150 different topics sampled from popular Wikipedia subjects, including\nsports, music, and politics. To ensure the accuracy of labels, the authors manually label\nthe truthfulness of the first five sentences in each generated article, rather than relying on\ncrowd-sourced labels. Although effective in the QA setting, Varshney et al. concede that, in\nthe event token probabilities are not available, some form of heuristic must be used to detect\nhallucination candidates.\n(2) Zhou et al. [259] show that external models can be developed to automatically clean halluci-\nnations. The authors tackle the issue of object hallucinations that LVLMs experience when\ndescribing the content of images. Through theoretical formulations, the authors show that\nLVLM responses tend to hallucinate in three settings: when described object classes appear\nfrequently within a description, when a token output has low probability, and when an object\nappears closer to the end of the response. As such, their model, LURE, is a fine-tuned LVLM\ntrained on a denoising objective with a training dataset that is augmented to include objects\nthat appear frequently within responses, and replacing objects with low token probabilities\nor appearing close to the end of the response, with a placeholder tag. At inference time,\ntokens are augmented similarly to how they were changed to generate the training dataset,\nand the LURE LVLM is prompted to denoise hallucinations by filling in uncertain objects.\n(3) SayCanPay, proposed by Hazra et al. [73], builds off of the SayCan framework [86] to im-\nprove the expected payoff of following a plan specified by a language model. Within our\nhallucination definition, this goal translates to increasing the desirability of generations by\nimproving the likelihood of the model achieving higher rewards. The authors propose three\ndifferent strategies for planning: Say, SayCan, and SayCanPay. Say methods greedily choose\nnext actions based only on token probabilities. SayCan approaches also take the success\nrate of the chosen action into consideration. Finally, SayCanPay additionally estimates the\nexpected payoff from following the plan with some heuristic. Hazra et al . learn this Pay\nmodel with regression on an expert trajectory dataset. Combining all three models together\nminimizes the likelihood that a generated plan contains conflicting infeasible action calls,\nwhile maximizing the efficiency of the task completion.\n4.2.2 Conformal Prediction. Another range of works estimate the uncertainty of a model output\nwith conformal prediction so as to provide statistical guarantees on the likelihood of predictions\nbeing correct [186].\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:16 Chakraborty et al.\n(1) Quach et al. [165] propose conformal language modeling to build a set of possible candidate\nresponses to a test prompt, while calibrating algorithm parameters on a held-out dataset of\nindependent prompts and their corresponding admission functions, which check whether a\nmodel output meets the criteria of an input prompt. In their algorithm, the authors calibrate\nthresholds for three separate scoring functions that test for generation quality, similarity\nwith other responses, and model confidence using ‚ÄúLearn then Test‚Äù [6]. At inference time,\ngiven scoring functions, calibrated thresholds, and an input prompt, the method samples\noutputs from the model and adds them to a prediction set if they meet quality and similarity\nthresholds, until the whole set is guaranteed to meet the user-defined confidence parameter.\n(2) The work from Kumar et al. [102] is described in Appendix C.2.2.2.\n(3) While the previous hallucination mitigation works presented using conformal prediction\nare solely applied to QA settings, Ren et al. [174] are the first to apply conformal prediction\nof foundation models to robotic tasks. The authors are motivated by a desire for language-\nconditioned robots to understand when they are uncertain about the next action to take, such\nthat they can ask for help in those cases (while minimizing frequency of clarifications). Because\nLLM generations with different length sequences inherently produce different complete\nsentence probabilities, the authors propose framing the control task as a multiple-choice\nproblem, like Kumar et al. [102]. Their approach, KnowNo, prompts an LLM to generate a\npossible set of next actions to take in multiple choice form. They first collect and hand-label a\ncalibration dataset of pairs of held-out instructions and the probability of the model choosing\nthe best action to take next. At test-time, the model outputs a set of actions to take and\nKnowNo eliminates actions with token probabilities less than a calibrated certainty from a\nuser. If there are still multiple actions left in the prediction set after eliminating uncertain\nactions, the model queries a human for help choosing the next action. Ren et al. show that\nKnowNo deviates from the user-defined error rate least often compared to methods that do\nnot use conformal prediction and has the highest success-to-clarification ratio. Additionally,\nthe authors deploy KnowNo to a real UR5 robot arm, where it is tasked with sorting foods\non a table by order of user preference, and disposing of undesired objects conditioned\non ambiguous instructions. However, several assumptions had to be made to produce the\ndemonstrated results, including having access to next token probabilities, having resources\nto collect a large calibration dataset, presuming people will faithfully provide help when\nasked for it, and using ground-truth vision to fully ground the environmental objects with\nthe text input to the model.\n(4) Liang et al. [122]extend the KnowNo methodology by incorporating an introspective planning\nstep using a previously constructed knowledge-base of experiences, which tends to (1)\nenhance quality of generated plans, and (2) improve interpretability of decisions. Specifically,\nintrospective planning first constructs a knowledge-base containing training pairs of tasks,\nobservations, and valid plans, which the LLM is prompted to generate explanations behind\nwhy they are reasonable. Each experience is stored with a key as an embedding of the original\ninstruction. During inference, given a new test instruction, their method queries the database\nto find the key with the closest embedding to that of the new instruction. This previous\nexperience and reasoning is fed into the model to generate a set of candidate plans to follow.\nFinally, the remainder of the algorithm follows the same process as KnowNo to calibrate\nand narrow down the prediction set to fall within a desired error rate. Liang et al. evaluate\ntheir method on the KnowNo dataset [174] and an augmentation of the original dataset that\nconsiders more safety-critical tasks with ambiguous instructions (e.g., making sure not to\nplace metal objects in a microwave when tasked with heating a bowl).\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:17\n(5) Wang et al. [215] aim to provide additional guarantees on completing the task provided within\nthe natural language instruction. To do so, the authors propose a novel task specification,\nLTL-NL, which combines linear-temporal-logic (LTL) descriptions with natural language from\na user instruction, which the authors claim is easier to define than classical LTL specifications.\nGiven this specification, a symbolic task planner chooses a sub-task to complete next and\nan LLM generates plans for each sub-task, respectively. Like Ren et al. [174] and Liang et al.\n[122], Wang et al. apply conformal prediction to minimize the number possible actions to\ntake next within some desired error rate. However, rather than directly asking a user for\nassistance when there is high uncertainty in the next action to take (or when there are\nenvironmental constraints), their method, HERACLEs, samples a new sub-task to complete\nfrom the task planner. If on the other hand, the task planner is unable to provide a new\nsub-task, HERACLEs requests help from the user. The authors deploy the model and baselines\nin a custom-made 3D mobile manipulator simulator that allows for evaluation of methods that\nuse LTL specifications. For example, the instruction, ‚ÄúDeliver Apple to A‚Äù is converted to LTL\nspecifications and fed into HERACLEs to navigate the robot to (1) pick up the requested apple\nand (2) drop it off at location A. With experimentation, the authors find that their method\nachieves higher task completion rate on missions requiring more sub-tasks, outperforming\nbaseline planners that do not utilize LTL specifications.\n4.3 Black-box Methods\nBlack-box algorithms only rely on the input prompts and output predictions from the model,\nwithout making assumptions on the availability of the hidden state, nor the token probabilities.\n4.3.1 Analyzing Samples from Model. As a result, several works of this type sample multiple\nsentences from an LLM, and measure the similarity of the information present in all samples.\n(1) For example, SelfCheckGPT, proposed by Manakul et al. [136], samples multiple responses\n(each of which may contain many sentences) from an LLM to a single query, and measures\nthe consistency among the varied responses through a study with five different approaches.\nSelfCheckGPT with BERTScore [252], for example, computes a similarity score between two\nsentences from different sampled outputs. Intuitively, a hallucination is detected when the\nsimilarity score for a sentence is low across all other samples. Other consistency-checking\nmethods the authors consider include using an automatic multiple-choice QA system con-\nditioned on the responses, relying on a proxy-LLM which has access to token probabilities,\ntraining an external classifier to predict contradictions (coined SelfCheck-NLI), and directly\nprompting the LLM to evaluate whether any sentence can be supported by another sampled\nresponse. Before evaluating the proposed methods, the authors find that there is no stan-\ndardized hallucination detection dataset for question-answering. Furthermore, the existing\nhallucination dataset from Liu et al. [130] is generated by manually replacing tokens in actual\nfacts, which may not represent generations from real language models. Thus, Manakul et al.\nchoose to curate their own evaluation dataset by querying GPT-3 for articles on topics from\nthe WikiBio dataset [107]. They then manually label the accuracy of each sentence in each\ngenerated article for a total of 1908 sentences across 238 summaries. As expected, Manakul\net al. find that sampling more responses from the model leads to better estimation of the\nvalidity of a claim, but is slower to compute.\n(2) The work from Elaraby et al. [58] is described in Appendix C.3.1.2.\n(3) Rather than analyzing the responses of a single language model, Du et al . [56] take an\nensemble approach. Specifically, they propose pitting multiple instances of an LLM into a\ndebate of the correct answer to a question. In practice, several agents are given the same\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:18 Chakraborty et al.\nquestion and predict a response. Over multiple iterations, all the responses of other agents are\nconcatenated, and fed in as additional context to each model, and a new response is sampled.\nDu et al. first identify whether the proposed debate method can improve the reasoning of\nlanguage models through three offline tasks with increasing difficulty in a custom benchmark:\n(1) simple arithmetic, (2) grade-school math [76], and (3) predicting the next best move to take\nin a game of chess. They additionally evaluate theaccuracy of debate results in (1) generating\nbiographies of famous computer scientists from Wikipedia, (2) general exam knowledge [76],\nand (3) confirming the validity of next moves in chess. The authors show that a combination\nof their iterative ensemble approach with chain-of-thought reasoning mitigates individual\nhallucinations (as agents tend to converge on a single consensus) and increases QA accuracy.\nWhile only tested on offline datasets, the approach could be utilized within a simulation\nframework, like the one presented by Park et al. [155], where, for example, multiple language\nagents may debate about plans to make. It is important to note that Du et al. evaluate the\naccuracy of biography generations by querying ChatGPT for the consistency between a fact\nand generated response. We hypothesize that, like Yu et al. [243], this automatic labeling\nscheme will result in lower quality labels than manual annotation.\n(4) CLARA is a framework engineered by Park et al . [154] that predicts when an instruction\nprovided to a robotic system controlled by an LLM may be ambiguous or infeasible. Intuitively,\nif a language model is uncertain about an instruction, it might output diverse (or conflicting)\nactions to other instructions that hold similar information. Thus, CLARA samples several sets\nof concepts from the original prompt, randomly orders them to assemble multiple inputs to\nthe model, and passes them as input to the language model under test. The method computes\nthe average similarity of pairs of output actions in an embedding space, over all outputs.\nNext, to check for infeasibility of the original goal, the foundation model is provided the\npossible action space of the robot, environmental observation, and goal, and is prompted\nto output whether the desired task is practical. In the event the model is uncertain from\nthe multi-prompting step, but the goal is feasible, CLARA asks for clarification from the\nuser with reasoning for why it is uncertain. In addition to evaluating CLARA on SaGC, a\ncustom dataset described in Appendix D.2.2, the authors also put their model to the test on a\ntabletop manipulator robot in simulation and the real world. While the method achieves a\nreasonable success rate on robotic pick-and-place tasks with real user instructions (where\nother discussed methods are primarily evaluated in QA settings), there are still failure cases\nwhere the model hallucinates during uncertainty reasoning and feasibility prediction.\n(5) Another set of works explicitly identify contradictions among responses, instead of estimating\nsimilarity. Naturally, detecting a self-contradiction is guaranteed to reveal an invalid claim.\nM√ºndler et al. [144] pose that removing detected conflicting information will increase the\nvalidity of a generated response. As such, the authors suggest a solution that finds important\nconcepts within a response to be evaluated, prompts the generation model to generate more\ninformation about each of the concepts, and uses a separate analyzer language model to\nevaluate the consistency of pairs of sentences on the same concept. Any sentences that are\nfound to be conflicting are revised by the analyzer model, before being output to the user.\nThe authors ask four language models to generate 360 summaries for 30 diverse topics from\nWikiBio [107] and Wikipedia [ 62] and have three human annotators manually label any\ninstances of self-contradiction, inaccurate sentences, and unverifiable statements, leading to\nhigh-quality labels. With respect to Definition 3.1, the annotators are identifying cases of\nnoncompliant and irrelevant generations to decide which statements are hallucinations.\n(6) Rather than relying on another language model to analyze the correctness of predictions, Dhu-\nliawala et al. [49] utilize chain-of-thought reasoning to prompt an LLM to generate possible\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:19\nverification questions about its original responses. If there are conflicts in answers to the\nverification questions, the LLM is prompted to regenerate its output with updated context\nand conflicting reasoning. One of the first evaluations the authors perform is on a custom set\nof 56 questions with topics collected from Wikipedia, each of which have multiple correct\nanswers for a total of around 600 ground-truth entities.\n(7) The work from Li et al. [119] is described in Appendix C.3.1.7.\n(8) The work from Xiong et al. [229] is described in Appendix C.3.1.8.\n(9) The work from Yehuda et al. [241] is described in Appendix C.3.1.9.\n4.3.2 Adversarial Prompting. Works specializing in adversarial prompting attempt to test the\nrobustness of models to varying inputs that may coerce the model into producing out-of-distribution\nresults.\n(1) For example, Mehrabi et al. [138] apply adversarial prompting to text-to-image foundation\nmodels, like Stable Diffusion [182], to generate offensive images. With respect to Definition 3.1,\ntheir framework, FLIRT, is essentially testing the tendency of foundation models to hallucinate\nundesired generations in deployment. FLIRT uses an adversarial language model to predict\na prompt to input to the image generator, scores the generated image for the presence of\nundesirable traits using an external classifier, re-prompts the adversary to produce a new\ninstruction conditioned on the findings of the classifier, and repeatedly generates images until\nthe adversary successfully prompts the test model to output an undesirable result. Mehrabi\net al. define objective functions conditioned on the score output by external classifiers to\nmaximize diversity of adversarial prompts and minimize toxicity so as to pass text filters that\ndetect malicious inputs, while improving attack effectiveness. The authors form a large set\nof prompts with varying levels of detail across different sexual, violent, hate, etc. contexts,\ninspired by Schramowski et al. [182]. Each prompt corresponds to one of three test splits that\nchange the type of toxicity, level of detail, and phrasing in the prompt. Tangentially, Mehrabi\net al. evaluate FLIRT on text-to-text models by similarly collecting adversarial prompts with\nvarying vulgarity.\n(2) Another work from Yu et al . [244] presents the AutoDebug framework for automatically\nsampling and updating several prompts for use in adversarial testing of the language model.\nYu et al. argue that evaluating information-retrieval LLMs on popular datasets like Wikipedia\nprovides an over-approximation on the accuracy of these models since they have been overfit\non the same data, possibly leading to memorization. Thus, the authors specifically explore\nadversarial testing under the case that the model predicts a correct response when provided\nrelevant context, but generates an incorrect prediction when the evidence is modified. They\napply two different modification approaches: replacing tokens within the context to provide\nincorrect facts, and adding additional relevant facts to the prompt that may make it difficult\nto pick out the most important details. The authors collect a new dataset by applying their\nadversarial generator to Natural Questions [103] and RealtimeQA [94], with additional human\nfiltering to ensure plausible answers are collected.\n(3) The work from Ramakrishna et al. [170] is described in Appendix C.3.2.3.\n(4) The work from Uluoglakci and Temizel [209] is described in Appendix C.3.2.4.\nAll in all, adversarial prompting is an effective method for identifying robustness of models\nto unseen inputs, which can be used to develop stronger input filters or fine-tune the model for\ndecreased hallucination tendency.\n4.3.3 Proxy Model. Certain black-box works rely on an external, proxy model to detect and mitigate\nhallucinations.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:20 Chakraborty et al.\n(1) One such method is used as a baseline within the SelfCheckGPT article [ 136]. As many\nlanguage foundation models do not provide access to token probabilities, the authors use\nan open-source proxy LLM that does provide token probabilities as an estimate of the\noriginal output‚Äôs probability. They find that using proxy LLMs for probability estimation\nand hallucination detection successfully is highly variable. The accuracy of detection is\ndependent on the complexity of the LLM itself, as well as the training data of the proxy\nLLM (i.e., models trained on independent datasets from the original LLM will have different\ngeneration patterns). Refer to the description of Method ID 4.3.1.1 for a discussion on the\ncustom dataset used for evaluation.\n(2) Within this section, we also include works that use an external trained classifier to detect\nhallucinations. For example, Chen et al . [33] curate a dataset of QA dialogue from LLM\ngenerated responses. They apply a composition of metrics to assess quality of responses,\nincluding a self-assessment from the LLM comparing the ground-truth and predicted text,\nhuman-labeled, and machine metrics ( e.g., BERTScore [ 252], F1 score, BLEU [ 153], etc.).\nTheir hallucination discriminator, RelD, is trained on the dataset in multiple separate phases,\neach using a different objective: regression, multi-class classification, and finally binary\nclassification. Through experiments, they find that RelD closely aligns with human evaluators‚Äô\noriginal predictions.\n(3) The work from Pacchiardi et al. [152] is described in Appendix C.3.3.3.\n(4) The work from Chu et al. [36] is described in Appendix C.3.3.4.\n4.3.4 Grounding Knowledge. In knowledge grounding tasks, a language model is tasked with\nidentifying evidence from an external knowledge-base that supports claims within a summary.\nAlthough seemingly irrelevant to decision-making scenarios, similar methods to ones discussed in\nthis section may be applied in planning tasks to identify observations that are most relevant to\npredicting the next action, or to generate reasoning behind a specified plan.\n(1) PURR, proposed by Chen et al. [26], is a denoising agent, like LURE (discussed in Section 4.2.1),\nthat is trained in an unsupervised fashion given evidence from online sources, a clean (correct)\nsummary, and a noisy (hallucinated) summary. The model learns to denoise the incorrect\nsummary to the clean statement. During deployment, given a possibly hallucinated claim, a\nquestion generation model queries online sources for evidence about the claim, and PURR\ngenerates a cleaned version of the original summary with said evidence.\n(2) The work from Li et al. [118] is described in Appendix C.3.4.2.\n(3) The work from Zhang et al. [251] is described in Appendix C.3.4.3.\n(4) Peng et al. [157] aim to add plug-and-play modules to an LLM to make its outputs more\naccurate, since these large foundation models cannot feasibly be fine-tuned whenever there is\nnew information. Their work formulates the user conversation system as a Markov decision\nprocess (MDP) whose state space is an infinite set of dialogue states which encode the\ninformation stored in a memory bank, and whose discrete action space includes actions\nto call a knowledge consolidator to summarize evidence, to call an LLM prompt engine to\ngenerate responses, and to send its response to the user if it passes verification with a utility\nmodule. The proposed LLM-Augmenter has a memory storing dialogue history, evidence\nfrom the consolidator, set of output responses from an LLM, and utility module results. Its\npolicy is trained in multiple phases with REINFORCE [222] starting with bootstrapping from\na rule-based policy designed from domain experts, then learning from simulators, and finally,\nfrom real users. Peng et al. deploy LLM-Augmenter to two different information retrieval\ndomains: news and customer service. For the news application, the authors retrieve relevant\nnews articles by crawling Reddit news forums for 1.3K articles, following the DSTC7 Track\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:21\n2 [242] approach. Similarly, to emulate the required knowledge of a customer service chat\nbot, the authors use the data from DSTC11 Track 5 [98], which holds 14.7K examples of\nuser reviews and frequently answered questions. The authors find that access to ground-\ntruth knowledge drastically improves QA results, and feedback from the utility module and\nknowledge consolidator help to provide more accurate answers.\n(5) Evaluated in decision-making settings, Introspective Tips [ 30] provide concise, relevant\ninformation to a language planner to learn to solve problems more efficiently. Intuitively,\nsummaries that collect information over all past experiences may be long and contain unnec-\nessary details. In contrast, tips are compact information with high-level guidance that can be\nlearned from one‚Äôs own experiences, from other demonstrations, and from other tasks in a\nsimilar setting. Chen et al. show that providing low-level trajectories is less effective than\ntips on simulated planning tasks. Additionally, with expert demonstrations, the LLM learns\nfaster with fewer number of failed trials than with just past experience alone. However, one\nlimitation identified in the study is that the LLM underperforms in unseen, low-difficulty\nmissions where it has issues generating general tips for zero-shot testing.\n(6) The work from Lim and Shim [123] is described in Appendix C.3.4.6.\n(7) The work from Schramowski et al. [182] is described in Appendix C.3.4.7.\n4.3.5 Constraint Satisfaction. There is also additional work in creating black-box algorithms for\nensuring decision plans generated by foundation models meet user-defined goal specifications and\nsystem constraints, like their grey-box counterpart developed by Wang et al. [215].\n(1) Because these models under test provide their results in text form, it is natural to apply formal\nmethod approaches (e.g., satisfiability modulo theory, SMT, solvers) to verify the satisfaction\nof generated plans. For example, Jha et al . [89] prompt an LLM planner with a problem\nformulated with first order constraints to predict a set of actions to complete the task. The\noutput plan is input to an SMT solver to check for any infeasibilities in the program, and any\ncounterexamples found are used to iteratively update the prompt and generate new plans.\nThis counterexample approach is much faster than relying on combinatorial search methods\nthat find a plan from scratch. However, the quality of generated plans and the number of\niterations before a successful plan is generated are heavily dependent on the LLM generator\nitself, with similar reasons to the proxy-model used by Manakul et al . [136]. In particular,\nthe authors explore the capability and efficiency of state-of-the-art large language models\nto solve block-world planning tasks [69]. For every experiment, each LLM is fed the initial\nrandom setup of a finite number of blocks in a scene, and the desired goal setup in the form\nof first-order constraints. Inoperable plans are detected by the Z3 SMT solver [ 45], who\niteratively works with the LLM to approach a feasible solution using counterexamples.\n(2) Another work from Hu et al. [80] develops a RoboEval benchmark to test generated plans\non real robots, in a black-box manner. Like Wang et al. [215], the authors introduce their\nown extension of LTL formulations, known as RTL, which specifies temporal logic at a\nhigher, scenario-specific, level, while abstracting away constraints that are not dependent on\navailable robot skills. RTL and LTL-NL are easier to read and define than classic LTL methods.\nRoboEval utilizes the provided RTL formulation of a problem, a simulator, and evaluator to\nsystematically check whether the output meets requested goals. Furthermore, to check for\nrobustness of the model to varied instructions, Hu et al. hand-engineer paraphrased sentences\nwithin an offline dataset that should ideally result in the same task completion. Primary\ncauses of failures were found to be a result of generated code syntax errors, attempting to\nexecute infeasible actions on the robot, and failing RTL checks.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:22 Chakraborty et al.\nLike adversarial prompting approaches, testing generated plans on robots in diverse scenarios\nenable researchers to design more robust systems that hallucinate less frequently at test-time.\n5 Guidelines on Current Methodologies\nIn section 4, we present a taxonomy of hallucination detection and mitigation algorithms in various\ndeployment settings. Combining our findings from our extensive review, we now present guidelines\nfor choosing hallucination intervention algorithms and metrics for different environments. As\nsuch, we hope these guidelines will enable developers to follow a standardized procedure to define\nhallucinations for their deployment context, design intervention algorithms, and evaluate efficacy\nbefore integrating hallucination-prone models into systems with humans at risk. Even while the\nfield of LVLMs evolves rapidly, we argue our guidelines are general enough to continue to assist\nresearchers in the near future. Additional considerations when using deep learning methods are\ndiscussed in Appendix E.1.\n5.1 Process of Choosing and Integrating Intervention Algorithms\nFig. 2. Design process of hallucination interven-\ntion methods.\nWe split the design process of hallucination de-\ntection and mitigation methods into nine steps,\nshown in Figure 2.\nDescribe the Model Under Test. The model\nunder test is the specific L(V)LM that has the\npotential to hallucinate in a deployment en-\nvironment. Describing the model requires un-\nderstanding its space of inputs and outputs,\nand whether designers have access to model\nweights and/or generation probability distribu-\ntions. Understanding the model under test is\nimportant for narrowing down possible hallu-\ncination intervention methods.\nChoosing an Evaluation Setting. Evaluation settings are datasets, simulators, or real-world envi-\nronments where the model under test and hallucination intervention algorithm are being tested\nhand in hand. Several possible evaluation settings are described in Appendix D, categorized by\napplication area (e.g., image captioning, conversational QA, control, code generation). Developers\nshould choose an evaluation setting that most closely resembles the intended deployment area to\nminimize the sim-to-real gap between evaluation and final integration [74, 179, 255]. A too large of\na sim-to-real gap results in a poor understanding of the true capabilities of the deployed model.\nHowever, designers need to balance the tradeoffs between the cost of developing a new high-fidelity\nsimulator or collecting representative offline data, and using off the shelf low-fidelity environments\nor datasets that may not truly cover the distribution of the intended deployment area.\nIdentifying Failure Modes in Evaluation Setting. Failure modes are directly related to the charac-\nteristics of hallucinations that need to be detected or mitigated. These modes are identified in one\nof three primary ways. Firstly, designers can choose failure modes based on the hard constraints of\nthe evaluation setting. For example, in decision-making contexts, failure modes may represent a\ncollision space that a robot should not enter, and in QA settings, failure modes could be defined by a\nset of ground-truth answers that a model generates conflicting results against. Second, stakeholders\nimpacted by model decisions will have personal preferences for how the model under test should\nact in deployment. As such, the next set of failure modes are defined by acting outside of the range\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:23\nof desired behaviors. Finally, as discussed in Section 1, it is nearly impossible to account for all\npossible failure modes at inference time. Thus, intervention algorithm designers should also query\nthe L(V)LM under test with different inputs to identify other undesired and irrelevant generations.\nFurthermore, many models are released alongside with a model card with details of failures found\nduring its design stage [66, 140, 148], which developers can use to identify additional modes to\nconsider. This final sampling procedure will not cover all possible remaining failure modes, but will\nimprove coverage rate. Through this three step process, engineers gain an understanding of what\nbehaviors should be considered as hallucinations in the particular evaluation setting. Although\ngeneralized detection/mitigation methods are preferred, identifying these specific subsets of hallu-\ncinations will allow engineers to quantitatively evaluate the efficacy of intervention approaches\nprior to deployment.\nSelecting Hallucination Characteristics. Given that designers have selected a set of failure modes, or\nhallucination behaviors, they can now categorize each mode as one of three content characteristics\ndefined in Section 3.1: compliance, desirability, and relevancy. Examples of categorizing failure\nmodes are shown in Table 2. It might be the case that one or more of the categories do not end up\nwith any failure modes. In this case, remaining unmatched characteristics are deemed irrelevant to\nthe development of the hallucination intervention algorithm at this stage. Finally, if stakeholders\nalso care about cases that the L(V)LM is attempting to deceive them through syntactically plausible\ngenerations, the plausibility characteristic is also relevant to hallucination detection or mitigation.\nPicking Metrics per Characteristic. Intervention algorithm designers can now choose metrics\nto measure the efficacy of model generations and hallucination detection/mitigation methods in\nmeeting the defined requirements of each characteristic. Metrics for each chosen characteristic\nwill differ depending on the deployment context and outputs of the model under test. As such, we\nprovide examples of current metrics in literature categorized by characteristic and application area\nin Table 4 from our review in Section 4.\nSelect Intervention Algorithm Modality. By this point, designers have performed the prerequisites\nof describing the available information from the model under test, its limitations, and defining\nrelevant hallucination characteristics and metrics. The remaining steps are actually developing the\nintervention algorithm. To do so, engineers need to choose which modality the proposed algorithm\nwill fall under ( i.e., white-, grey-, or black-box). As described in Section 4, possible modalities\ndepends on the availability of access to necessary information from the model under test. It is also\nimportant to consider at this stage the desired flexibility of the intervention algorithm (i.e., the ease\nof integrating the algorithm to different test models and evaluation settings). For example, black-box\nmethods are easier to deploy to proprietary models where access to weights and token probabilities\nis limited. However, if the designer only needs to intervene in the hallucination tendency of a\nparticular open-source language model, white-box intervention algorithms can be tuned to the\nlimitations of that specific model.\nIntervention Type. Next engineers can choose whether the intervention algorithm should detect\nand/or mitigate hallucinations. Again, this choice depends on the intended deployment area of\nthe model under test and the needs of the stakeholders in the model design process. As seen in\nTable 3, we find several existing algorithms enable both detection and mitigation of hallucinations.\nDetection of hallucinations enables reacting to failures, while pure mitigation relies on the efficacy\nof the intervention algorithm alone to proactively filter hallucinations prior to final generation. As\nsuch, we recommend safety-critical scenarios be deployed with detection and mitigation algorithms\nto simultaneously reduce chances of failures and inform impacted parties of potential hallucinations\nto increase transparency.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:24 Chakraborty et al.\nTable 4. Common datasets/simulators and metrics across settings and hallucination characteristics.\nEach metric is accompanied by references that use the metric as the specified characteristic. The same metric\ncould be applied as different characteristics.\nSetting CommonDatasets/Simulators Characteristic\nCompliance Desirability Relevancy Plausibility\nQA\n‚Äî TriviaQA [92],‚Äî GSM8K [38],‚Äî HotpotQA [237],‚Äî MMLU [76],‚Äî Natural Questions [103]\n‚Äî Accuracy‚Ä†\n[7,26,33,49,56,58,102,118,136, 144, 152, 157, 229, 241,244, 251],‚Äî Contradictions[49, 56, 58, 144, 152, 241],‚Äî Concept Probabilities[136, 165, 210],‚ÄîBERTScore [136, 157, 170],‚Äî BLEU [118, 157, 170],‚Äî Adversarial Success Rate[238, 244],‚Äî Coverage [102, 251],‚Äî METEOR [157, 170],‚Äî SummaC [58],‚Äî FactScore [49],‚Äî Cosine Similarity [241],‚Äî Sentence-Bert [244],‚Äî AlignScore [170]\n‚Äî Calibration Error[124, 229],‚Äî Succinctness [157, 194],‚Äî Over-Conservativeness[234],‚Äî # of Clarifying Questions[251],‚Äî Prudence [234],‚Äî Preservation [26],‚Äî TOXIGEN Classifier [138]\n‚Äî Perplexity [144, 194],‚Äî Cross Encoder [26]\n‚Äî ROUGE[157, 165, 170],‚Äî Semantic Preciseness[33, 194]\nIC\n‚Äî MSCOCO [126],‚Äî GQA [85],‚Äî Visual Genome [100],‚Äî A-OKVQA [185]\n‚Äî LVLM-Based Scoring[82, 119, 259],‚Äî CHAIR [82, 119, 259],‚Äî POPE‚Ä†[82, 119, 259],‚Äî Co-Occurance [259],‚Äî Concept Probabilities[259],‚Äî BLEU [259],‚Äî BertScore [259],‚Äî CLIP Score [259],‚Äî METEOR [259]\n‚Äî CIDER Human Alignment[259],‚Äî Detailedness [82]\n‚Äî CHAIR [82, 119, 259],‚Äî POPE‚Ä†[82, 119, 259]\n‚Äî Perplexity [82],‚Äî ROUGE [259],‚Äî SPICE [259]\nIG\n‚Äî MSCOCO [126],‚Äî LSUN [243],‚Äî LAION-400M [184],‚Äî I2P [182]\n‚Äî Contradictions [36, 123],‚Äî Accuracy‚Ä†[36],‚Äî LVLM-Based Scoring [36],‚Äî CLIP Score [182]\n‚Äî NudeNet Classifier[138, 182],‚Äî Q16Classifier [138, 182],‚Äî Diversity [138],‚Äî Monetary Cost [36],‚Äî Bias [182],‚Äî Inappropriate Probability[182],‚Äî ExpectedInappropriateness [182],‚Äî COCO FID-30k Fidelity[182]\n‚Äî LVLM-Based Scoring[36],‚Äî CLIP Score [182]\n‚Äî Factual Fabrications[123]\nP\n‚Äî RoboEval [80],‚Äî SaGC [154],‚Äî Ravens [246],‚Äî BabyAI [34],‚Äî TableSim [174]\n‚Äî Feasibility[73, 89, 122, 154, 174, 215],‚Äî Action Probabilities[73, 122, 174],‚Äî Plan/Action Accuracy‚Ä†\n[80, 154, 215],‚Äî Action Variance [182],‚Äî Contradictions [89]\n‚Äî Success Rate[30, 73, 80, 89, 122, 154, 174,215],‚Äî Generalizability[30, 73, 122, 154],‚Äî Clarification/Unsure Rate[122, 154, 174, 215],‚Äî # of Attempts [30, 80, 89],‚Äî Calibration Error[122, 174],‚Äî Coverage [122, 174],‚Äî Estimated Payoff [30, 73],‚Äî Action Set Size [122, 174],‚Äî Plan Readability [215],‚Äî Plan Length [73],‚Äî Unsafe Action Rate [122],‚Äî Monetary Cost [122],‚Äî Inference Speed [215]\n‚Äî Action Probabilities[73, 122, 174]\n‚Äî Non-Compliance Rate[122],‚Äî Preciseness [122]\n‚Ä†Encompasses machine metrics like (balanced) accuracy, precision, recall, F1, AUC, exact match [28], and\npass@1 [31].\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:25\nTable 5. Benefits and limitations of each intervention algorithm type.\nMethod Type Pros Cons\nHidden States ‚Äî tuned for specific model‚Äî hidden states hold useful embeddings‚Äî no need to re-embed text output\n‚Äî reduced model transfer flexibility‚Äî not applicable for proprietary models\nAttention Weights ‚Äî tuned for specific model‚Äî attention weights hold useful info‚Äî reduced model transfer flexibility‚Äî not applicable for proprietary models\nHonesty Alignment ‚Äî directly fine-tunes model under test‚Äî empirically generalizes to new data‚Äî tuned model still susceptible‚Äî test efficacy impacted by data quality\nConcept Probabilities ‚Äî generally model agnostic‚Äî intuitive‚Äî tried in broadest set of deployments\n‚Äî requires access to token probabilities‚Äî correlation may not necessarily hold\nConformal Prediction ‚Äî theoretical guarantees‚Äî applicable to multi-step planning‚Äî provides model uncertainty metric\n‚Äî requires access to token probabilities‚Äî requires collecting calibration dataset‚Äî relies on human intervention\nAnalyzing Samples ‚Äî applicable to proprietary models‚Äî intuitive‚Äî removing conflicts reduces failures\n‚Äî efficiency impacted by # of samples‚Äî affected by compliance metric choice\nAdversarial Prompting ‚Äî applicable to proprietary models‚Äî reveals undesired behaviors‚Äî can lead to better filters‚Äî red-teaming often occurs pre-release\n‚Äî requires covering large input space‚Äî generally does not mitigate‚Äî requires hallucination classifier\nProxy Model ‚Äî applicable to proprietary models‚Äî simple classifier could work well‚Äî proxy has mismatched distribution‚Äî dependent on proxy complexity‚Äî LVLM proxy could hallucinate\nGrounding Knowledge ‚Äî applicable to proprietary models‚Äî provides evidence for responses‚Äî aligns model knowledge with users‚Äô‚Äî can help to learn policies faster\n‚Äî requires ground-truth database‚Äî could reference stale knowledge‚Äî still fails in low-data regimes\nConstraint Satisfaction ‚Äî applicable to proprietary models‚Äî theoretical guarantees‚Äî SMT solvers find failure cases quickly\n‚Äî requires precise constraint definitions‚Äî specification may be hard to parse\nIdentifying an Intervention Sub-Type. Now that the deployment setting, modality, and intervention\ntype have been identified, engineers can choose a method type from Table 3 that falls within\nthose constraints. This step will require some experimentation across method types and specific\nalgorithms to identify the most effective intervention approach using the previously chosen metrics.\nWe list pros and cons of each method type in Table 5 to assist researchers with narrowing down\nthe scope of their algorithm search.\nImplementing the Hallucination Intervention Algorithm. Finally, designers can implement and\nintegrate a chosen algorithm into the specific deployment setting and perform additional tests to\nmeasure its efficacy in detecting/mitigating hallucinations from the model under test.\n6 Future Directions\nHere, we discuss some possible future directions in hallucination detection and mitigation tech-\nniques for foundation models to improve deployments to decision-making tasks.\nEvaluating Methods on Decision-Making Tasks. Most hallucination detection approaches are\ncurrently tested in offline QA settings for information retrieval or knowledge alignment, as seen in\nTable 3. As foundation models are increasingly used for more complex tasks, researchers should\nmake an effort to adapt and evaluate earlier detection/mitigation approaches that were applied to\nQA problems. Although dissimilar in practice from QA settings, planning and control problems\nmay be formulated such that earlier mitigation methods can be evaluated on decision-making tasks.\nFor example, as discussed in Section 2.1, Chen et al. [29] treat the autonomous driving task as a QA\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:26 Chakraborty et al.\nproblem, which could be naturally extended to test other QA hallucination detection methods in\nthe same setting. This evaluation may lead to greater understanding of the general limitations of\nthese models, as we draw parallels across diverse deployments.\nDevelopment of More Black-box Approaches. White- and grey-box detection methods may not\ngenerally be applicable in situations where the internal state or token probabilities are unavailable\nfrom the language model. Thus, we predict black-box approaches will take precedence in the\nnear future, as state-of-the-art LVLMs like GPT-4V already prohibit access to probability outputs.\nHowever, current black-box methods are limited with simplistic sampling techniques to gauge\nuncertainty, and proxy models may not be representative of the true state of the model under\ntest. Works like FLIRT showcase the promise of black-box adversarial prompting approaches in\ngenerating undesirable results [138]. We argue developing more aggressive black-box adversarial\ngenerative models, which explicitly optimize for producing inputs that may perturb the system\noutputs, is key to identifying the limits of a foundation model‚Äôs knowledge.\nPushing Models‚Äô Generalization Capabilities. Currently, foundation models are primarily deployed\nto decision-making tasks that likely have some relation to its training set. For example, although\ncomplex, tasks like multi-agent communication, autonomous driving, and code generation will be\npresent in training datasets. On the other hand, dynamic environments like robot crowd navigation\nrequire identifying nuances in pedestrian behaviors which the model may not have explicitly seen\nduring training. Thus, when models are deployed in decision-making contexts and encounter a\npreviously unseen scenario, or they are utilized in a completely different setting from their training\ndata, it is necessary to consider their generalization capabilities. As discussed in Section 2.1, existing\nexamples of foundation models applied in autonomous driving and robotics utilize external tools\nto retrieve sensor data or memories before planning. Mialon et al. [141] refer to such models that\nextract useful information from databases as augmented language models. As such, the authors\nargue that the combined efforts of using external tools and internal reasoning is critical to the\ngeneralizability of language models to broader tasks ‚Äî also explored by Chen et al. [30], Park et al.\n[155], and Wang et al. [213]. From another perspective, Tong et al. [205] approach the development\nof generalized multi-modal large language models from a vision-centric focus. In particular, the\nauthors find that training LVLMs with heavy consideration on the design of vision encoders and the\nrespective connector between the vision and language models drastically improves the capabilities\nof architectures deployed in vision tasks (e.g., image captioning, QA, depth ordering). This line of\nthinking can bolster the performance of LVLMs deployed in autonomous driving decision-making,\nwhere current methods have failed [221]. Before integrating models into real-world applications, we\nargue that designers should thoroughly explore their generalization limitations to find directions\nfor future growth and to maximize transparency of model capabilities.\nTesting Multi-modal Models. With the explosion of LVLMs, which allow for explicit grounding\nof natural language and vision modalities, further exploration should be performed in evaluating\ntheir effectiveness in decision-making systems. Wen et al. [221] take a step in the right direction\ntowards testing black-box LVLMs in offline driving scenarios, but there is still work to be done in\ndeploying these models in online settings. This direction can shed light on the long-standing debate\nof whether modular or end-to-end systems should be preferred in a particular deployment setting.\nIn fact, while our work has focused on LVLMs, there exist other families of multi-modal foundation\nmodels for the audio [151] and 3D generation [32] spaces, which similarly hallucinate [178, 214]\nand should be evaluated before deployment.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:27\nSummary. We provide a glimpse into the progress of research for evaluating hallucinations of\nfoundation models for decision-making problems. First, we identify existing use cases of founda-\ntion models in decision-making applications (e.g., autonomous driving, robotics) and find several\nworks make note of undesired hallucinated generations in practice. By referencing works that\nencounter hallucinations across diverse domains, we provide a flexible definition for hallucina-\ntions that researchers can leverage, regardless of their deployment scenario. Then, we give a\ntaxonomy of existing hallucination detection and mitigation approaches for decision-making,\nquestion-answering, etc., alongside a list of commonly used metrics, datasets, and simulators for\nevaluation. We find that existing methods range in varying assumptions of inputs and evaluation\nsettings, and believe there is room for growth in general, black-box hallucination detection algo-\nrithms for foundation models. Finally, we present generalized guidelines to assist engineers with\nselecting hallucination intervention algorithms across varied deployment contexts, and suggest\nfuture research directions.\nA Foundation Models Making Decisions\nA.1 Robotics\nFig. 3. Example deployment of an LVLM foundation model in a robotics setting. Hallucinations are\nhighlighted pink. Here, a robot tasked with assembling a sandwich initially identifies an object incorrectly.\nThen, the model comes up with an infeasible plan. Finally, when attempting to perform one of the actions,\nthe robot collides with the human as it did not perceive any danger.\nA.2 Other Areas\nThere are also other works that apply foundation models for decision-making outside of the robotics\nand autonomous vehicle domains. For example, ReAct from Yao et al. [239] identifies that a key\nlimitation of chain-of-thought reasoning [217] is that the model does not update its context or\naction based on observations from an environment. As such, chain-of-thought reasoning relies\npurely on the internal reasoning of the foundation model itself to predict actions to take, missing a\ncrucial step in grounding its actions with their effects on the environment. Given a prompt, ReAct\niterates between an internal reasoning step and acting in the environment to build up context\nrelevant to the task. Yao et al. showcase the promise of the method in a QA setting where the LLM\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:28 Chakraborty et al.\ncan take actions to query information from an external knowledge base, as well as an interactive\ntext-based game, ALFWorld [190]. Chen et al. [30] admit that ReAct is a powerful tool for dynamic\nreasoning and grounding, but is limited by the fact that the updated context from the Act step is\nonly helpful for the particular task the model is currently deployed for. They propose Introspective\nTips to allow an LLM to reason about its past successes and failures in a world to generate general\ntips that will be helpful across diverse instruction-following tasks. Specifically, tips are generated\nfrom the past experience of the model from a similar set of tasks, from expert demonstrations, and\nfrom several games that differ from the target task. By summarizing these experiences into more\nconcise tips, Chen et al. show that Introspective Tips outperform other methods in ALFWorld with\nboth few- and zero-shot contexts.\nPark et al. [155] and Wang et al. [213] apply foundation models in more complex environments to\npush models to their limits to simulate realistic human behaviors and test lifelong learning. Park et al.\npropose generative agents that produce believable, human-like interactions and decisions within a\nsmall town sandbox environment. They develop a module for individual agents in the simulation\nto store and retrieve memories, reflect about past and current experiences, and interact with other\nagents. Their generative agents use similar methods to ReAct and Introspective Tips to act based\non a memory of experiences, but also interact and build relationships with other agents through\ndialogue. The authors show that the agents are able to effectively spread information, recall what\nhas been said to others and stay consistent in future dialogue interactions, and coordinate events\ntogether. Sometimes, however, agents are found to hallucinate andembellish their responses with\nirrelevant details that may be attributed to the training dataset of outside, real-world knowledge.\nVoyager, from Wang et al., deploys GPT-4 to the MineDojo environment [60] to test its in-context\nlifelong learning capabilities. The architecture prompts GPT-4 to generate next high-level tasks to\ncomplete, given the agent‚Äôs current state and results of past tasks ‚Äî a form of automatic curriculum\ngeneration. Voyager then identifies what intermediate general skills would be required to complete\nthe task, and the LLM is used to fill in a skill library with helpful low-level skills in the form of\nprograms that call functions that are available to the simulator. GPT-4 is prompted to generate\nskills that are generalizable to multiple tasks, so that the skill generation step does not have to\nbe called for every task if the skill is already stored in the library. Wang et al. show that Voyager\ncontinuously learns to explore the diverse tech tree available within MineDojo while building and\nleveraging skills. Even so, they find that the LLM hallucinates when generating tasks to tackle and\nwhen writing the code to execute for a particular skill, discussed further in Section 3.2.\nKwon et al. [104] explore the use of LLMs to act as a proxy for a hand-tuned reward function in\nRL tasks. This application is particularly motivated by decision-making tasks that are difficult to\nspecify with a reward function, but can be explained textually with preferences of how a policy\nshould generally act. Specifically, the LLM evaluator first undergoes in-context learning with\nexamples of how it should decide the reward in several cases of the task that the agent will be\ndeployed to. Then, during RL training, the LLM is provided a prompt with the trajectory of the\nagent within the episode, the resulting state from the simulator, and the original task objective\nfrom the user, and is asked to generate a binary reward for the agent (1 if success, 0 else). The\nbinary reward is added to the experience replay, and the agent can be updated using any RL\nalgorithm. Kwon et al. find that a baseline in their work that predicts rewards with no in-context\nlearning especially hallucinates with incoherent reasoning. There has also been work by Suri\net al. [199] on evaluating whether large language models elicit heuristics and biases when making\ndecisions, like humans. Specifically, while it is commonly believed in literature that biases like\nanchoring, representativeness, availability, framing, and endowment are brought about by cognitive\nprocesses, Suri et al. pose that if foundation models generate similar responses, these effects may\nbe partly caused by language, since language models inherently have no cognitive capabilities.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:29\nThrough four user studies comparing the responses of humans and ChatGPT to the same questions\nintending to bring about the decision biases, the authors find that both the model and participants\nshowcased similar effects of partiality. As such, the authors posit that these results imply that\ndecision heuristics in people may actually be influenced by linguistic syntax. We further argue in\nAppendix B.2 that language models deployed in decision-making contexts where they may impact\nhumans‚Äô lives should aim to minimize these biases for fairness.\nB Hallucinations\nB.1 Examples\nImage and Video Generation. Likewise, image and video generation models are not safe from hal-\nlucinations. Recent generative models like DALL¬∑E [15, 171], Stable Diffusion [176], and SORA [149]\nhave shown great promise in producing high quality frames given text input, but they can also\nhallucinate noncompliant, undesirable, and irrelevant features. The DALL¬∑E system cards [143, 148]\nprovide plenty of examples of such characteristics with biased, stereotypical generations, deepfakes\nof public figures, and violent and racy imagery. Betker et al. [15], who train DALL¬∑E 3 to generate\nimages given synthetic captions from a captioning model, find that poor, hallucinated training\ncaptions result in noncompliant image generations at test time that ignore important caption details.\nFurthermore, generations may also include nonfactual contents that misguide users [123]. Even\nwith the development of safety filters that attempt to detect hallucinations before displaying the\ngeneration to the user, Rando et al. [172] find that simple prompt engineering and red-teaming\napproaches can still lead to vulgar content, bypassing the filter. As such, recent efforts have shifted\nfrom solely filtering inappropriate content at test time, and instead providing guidance to the model\nto assist in generating safer features [182]. However, instances of stereotypes are still prevalent in\ngenerations. In a positive light, Huang et al. [83] propose exploiting the hallucination tendency of\nimage generation models to curate a benchmark to evaluate image captioning methods. While the\nresults of image generation models should minimize hallucinations in static frames, video genera-\ntion models also need to produce temporally consistent and plausible clips across a sequence of\nframes. Sora Detector, proposed by Chu et al. [36], attempts to detect static hallucinations (e.g., color\ndistortion, deformations, unrealistic depth of field) and dynamic hallucinations ( e.g., unnatural\noverlapping of objects, implausible motions, temporally illogical generations) within generated\nvideos conditioned on text.\n3D Modeling and Generation. Another field that is quickly gaining the attention of researchers is\n3D generation, where foundation models are generating high fidelity representations of objects\nor scenes conditioned on images or text [9, 114, 129, 227]. These foundation models span a wide\nrange of downstream applications including human avatar creation [230], medical tomography\nunderstanding [16], 3D object generation [32, 70], environment modeling [235], and more. Although\nbillions of labeled pairs of images and text exist across many datasets, as discussed in Appendix D.2,\n3D modality labels have only recently begun to approach a similar scale [232], limiting the number\nof training pairs for 3D foundation models. As such, early 3D generative foundation models\nconditioned on text relied on embeddings from text-image similarity models like CLIP [ 167] to\nserve as a guide for generating accurate scenes [87, 180], or used pre-trained text-to-image diffusion\nmodels to provide feedback for training3D neural radiance field models [160]. Instead of optimizing\nindividual low-level object representations directly, some works assume access to a collection of\n3D assets, which a standard LLM like GPT-4 can query to complete the floor plan of a 3D scene,\nbypassing the need for low-level text-3D datasets [235]. Yang et al. [232] are some of the latest\nresearchers to tackle the data scarcity problem of text-labeled 3D scenes by introducing their\nown procedure for collecting text-scene labels across 40K rooms. When evaluating models with\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:30 Chakraborty et al.\ntheir new 3D-POPE metric, the authors find that several 3D LLMs hallucinate the presence of\nnon-existent objects in the scene, similarly to other LVLMs tested on images. Wang et al . [214]\nfind that 3D generative foundation models hallucinate spatial inconsistencies when rendering\na generated scene from different perspectives. Due to the scarcity of 3D datasets, the authors\npropose Hallo3D: a three-part hallucination detection and mitigation approach leveraging LVLMs\nas advisors. Specifically, Hallo3D samples multiple rendered views of a 3D object with a pre-trained\ndiffusion model conditioned on text. Each rendering is passed through an LVLM which identifies\nabnormalities, and the predicted statements are fed into the diffusion model as negative contexts to\nupdate the original renderings through a prompt-enhanced reconsistency step. The authors use a\ncross-attention mechanism to improve the consistency of frames across different view points.\nB.2 Broader Impacts\nThus far, we have primarily formulated a unified definition for hallucinations and provided examples\nof where they have come up in the research community. However, as this new field advances\nrapidly every day, numerous industries have begun utilizing the technology in their respective\napplications. As such, it is even more imperative that designers develop robust hallucination\ndetection and mitigation methods before deploying models into areas with real humans at risk.\nThree critical industries where LVLMs have shown great promise are the medical, legal, and finance\nsectors [37, 106, 120]. For example, Li et al. [120]and Zhao et al. [254]explain that LLMs are currently\nused in finance for portfolio management, fraud detection, credit scoring, text summarization for\nforecasting, and customer-facing chatbots. In fact, Bloomberg has already designed its own LLM,\nBloombergGPT, trained on a custom curated dataset for investor sentiment analysis given news\ntranscripts, numerical reasoning QA, and named entity recognition [226]. Similarly, LVLMs are\nbeing applied to medical tasks for patient education through QA, assisting physicians when writing\nreports and examining test results, and used in academia for taking medical exams ‚Äî even going\nso far as accomplishing passing results [146]. Likewise, Lai et al. [106] showcase recent examples\nof use cases of LLMs in law, including summarizing legal documents, generating drafts of legal\ndocuments, acting as a legal consult, and making decisions given case facts.\nFor all of their promising results, foundation models have even greater impacts in these critical\napplications. Specifically, designers need to consider problems of data privacy, training with out-of-\ndate data, potential inconsistencies between generations and references, model bias, the ethics of\nusing AI for a particular problem setting, and transparency to stakeholders and impacted parties\nin the decision-making pipeline. In the finance sector, Kang and Liu [93] identify common failure\nmodes of various open-source and proprietary language models in tasks for recognizing financial\nabbreviations and stock symbols, providing explanations for financial terms, and querying a stock\nprice without access to an external database. Each model is shown to have defects at inference time\nwithout using retrieval augmented generation (RAG) [113] methods (similar methods are discussed\nin Section 4.3.4), especially when the training data is out of date. As such, companies like BlackRock,\nInc. are directly applying RAG to existing LLMs for financial QA [ 181]. In the context of legal\napplications, Dahl et al. [43]provide a taxonomy of the complexities of different problems a language\nmodel could be tasked with, each with increasing risk of generating undesired hallucinations.\nNotably, the frequency of hallucinations are found to increase with task complexity and varies with\nthe specified court, jurisdiction, case prominence, and year. In fact, LLM hallucinations have already\nmade their mark in a real court case, where a New York attorney utilized ChatGPT to generate a\nbrief, which in turn referenced non-existent cases. He states the model told him its generations\nwere accurate, underscoring the importance of developing transparent models [218]. Ali et al. [4]\nevaluate the efficacy of foundation models for automatically generating billing codes given medical\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:31\nprocedure descriptions. As the tested models currently perform miserably on the given task, the\nauthors suggest further research before deploying the models in an automated medical context.\nIn an even more generalized medical setting, multiple LLMs are queried to answer novel questions,\nwhere even RAG results in generations that are either inconsistent with sources, or the cited text\ncannot be found in the source [225]. In fact, the AI search engine company Perplexity AI, which uses\nRAG to generate summaries, came under fire during the 2024 United States presidential election\nfor releasing an election hub that generated hallucinated results [39]. One human tester found that,\nwhile the company‚Äôs product centralized information into one source, the model‚Äôs generations\nwrote in varied tones for different candidates, resulting in biased generations. Similarly, LLMs\nhave been found to generate fake citations to support claims queried by users. For example, an\neducation official from the state of Alaska used a generative model to collect citations for a proposed\npolicy [197]. The burden was then left to the policymaker to replace citations to nonexistent\nsources, leading journalists to question the lack of policy on using generative AI to write proposals\nwhich impact the public. The promise of LLMs have also led many educators to directly rely on\nthese models to produce lesson plans, leading to hallucinations when left unchecked [ 12]. One\naudio transcription tool leveraged in medical domains has been found to add irrelevant details\nto generations, and erase the ground-truth audio for security [200]. In an era where foundation\nmodels are being deployed in new, critical areas, we argue ground-truth data should be kept for\nlater model evaluation and redundancies. Overall, it is of the utmost importance to ensure LVLM\ngenerations are free of hallucinations in critical decision-making applications. While these models\nare still far from reaching this milestone, it is even more important to be transparent about model\ncapabilities to users to minimize misalignment of expectations.\nC Detection and Mitigation Strategies\nC.1 White-box Methods\nC.1.1 Hidden States.\n(3) LUNA, introduced by Song et al. [194], is a general framework that measures the trustwor-\nthiness of an LLM output containing four stages of evaluation: model construction, semantic\nbinding, quality metrics, and practical application. The abstract model construction phase\nattempts to profile the LLM using its hidden states with either a discrete time Markov chain\n(DTMC) or a hidden Markov model (HMM) architecture. For example, when fitting a DTMC\nmodel, the authors encode the hidden states of the language model into a lower dimensional\nspace, cluster them into abstract discrete states, and learn a transition function between said\nstates. Semantic binding is used alongside quality metrics to identify the states and transitions\nthat are trustworthy, and which ones are undesired. Finally, at inference time, as the model\ngenerates output tokens to a given prompt, the intermediate network layer embeddings are\niteratively passed through the profiling model to identify when undesired transitions occur.\nThe authors evaluate their framework‚Äôs capability of detecting hallucinations within QA\ndatasets.\nC.1.3 Honesty Alignment.\n(2) Yang et al. [234] take the method one step further by also training the model to refuse to\nanswer questions with high uncertainty.\nC.2 Grey-box Methods\nC.2.2 Conformal Prediction.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:32 Chakraborty et al.\n(2) Kumar et al. [102] similarly apply conformal prediction to LLMs, but for answering multiple\nchoice questions. Specifically, the method first collects a calibration dataset of prompts and\nthe normalized token probabilities of the correct token (i.e., A, B, C, or D) being chosen from\nthe model. Then, during deployment, given a user-defined error rate and a prompt, their\nalgorithm chooses the multiple choice answers with token probabilities that fall within the\ncalibrated score on the held-out dataset.\nC.3 Black-box Methods\nC.3.1 Analyzing Samples from Model.\n(2) A concurrent work from Elaraby et al. [58] rather computes the entailment among responses\nat the sentence-level. Consistency metrics check whether responses contradict one another\nwhile entailment metrics identify if the responses imply one another. The nuanced difference\nbetween SelfCheck-NLI and their method, HaloCheck, is that Elaraby et al . use the Sum-\nmaC [105] entailment estimation method, placing equal weightage among all sentences, in\nall responses, to compute a more balanced prediction score. The authors evaluate HaloCheck\nand hallucination mitigation techniques on a domain-specific, custom-curated dataset, with\nfacts about the US National Basketball Association (NBA). Specifically, Elaraby et al . first\nprompt GPT-4 for questions on topics within the NBA domain and manually filter out low-\nquality generations, resulting in 151 questions. An LLM is then queried for 5 responses to\neach question, and each response is manually annotated for consistency and accuracy of\ngenerations. HaloCheck is also shown to be more efficient at predicting scores. Other com-\nmonly used metrics within the language community for similarity estimation are discussed\nin Appendix D.1.1.\n(7) In the problem space of image captioning, Li et al. [119] estimate the accuracy of a (possibly\nhallucinating) LVLM when describing an image with a text caption. Early metrics, like\nCHAIR [175], fail to provide stable estimates of accuracy when different captions with\nsimilar semantic grounding lead to varying scores. To tackle this stability problem, the\nauthors propose POPE, which curates binary questions about whether an object exists within\nan image scene. Questions to which the foundation model provides conflicting responses\ndescribe objects that the model may be hallucinating. These metrics are further described in\nAppendix D.1.2.\n(8) Yet another recent work by Xiong et al. [229] asks LLMs in a zero-shot manner to verbally\ninclude their uncertainty in the generated output. This desired behavior is elicited through\nadditional prompt engineering (i.e., add a phrase to the prompt like ‚ÄúPlease provide your\nconfidence level as a percentage‚Äù). Unlike Lin et al. [124], the authors do not further fine-tune\nthe model to a calibration dataset of uncertainties. To combat over-confidence in output\nscores, Xiong et al. utilize chain-of-thought reasoning, predict the confidence score of each\nsub-claim, and combine them over the whole response to compute the final belief. The authors\nfind that a hybrid approach, merging verbalized uncertainty with self-contradiction detection,\noutperforms the individual components alone on expected calibration error, comparing\npredicted confidence and actual model accuracy.\n(9) Yehuda et al. [241] present InterrogateLLM, a sampling-based approach that attempts to\nreconstruct an original query given a possibly hallucinated LLM response, and measures\nthe similarity of the two queries. Inspired by human studies [ 19], the authors specifically\nhypothesize that responses that generate queries that differ greatly from the original prompt\npoint to possible hallucinations. However, they also confess that, like human studies, there\nis the possibility of false positive hallucination detections due to the stochastic nature of\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:33\nlanguage models. InterrogateLLM follows a simple procedure: form a prompt with few-\nshot examples containing queries and answers followed lastly by the actual query, generate\na (hallucinated) response from a language model under test, reverse the original prompt\nexamples to provide answers and their queries with the generated response appended last,\nsample generated queries from any language model (not necessarily the one under test),\nand measure the cosine similarity of vector embeddings between the original query and\ngenerated queries. Using public datasets covering a range of trivia knowledge [11, 59, 260], the\nauthors curate a dataset of QA pairs. Most importantly, the authors find that their backward\nfew-shot method is critical to decreasing the false-negative detection rate of hallucinations\nseen in SelfCheckGPT [136], which only compares generated responses from the original\nforward pass. Furthermore, using an ensemble of models for the backward generation and\nmore iterations of query sample generation lead to higher detection accuracy, at the cost of\nefficiency. We expect this backward-query generation approach to hallucination detection\ncan also be applied to planning tasks, where hallucinated plans will reverse-generate task\ndescriptions that do not match well with the original goal.\nC.3.2 Adversarial Prompting.\n(3) Motivated by similar findings that language models are overfitting to QA datasets, Ramakr-\nishna et al. [170] present a novel framework for generating invalid questions to test the\nfrequency of hallucinations output by new LLMs. The authors collect an augmented version\nof the DBpedia dataset [110] replaced with invalid questions to evaluate the hallucination\nrate of language models, and note that any deployment dataset could have been used instead.\nSpecifically, Ramakrishna et al. manually create a list of 24 question templates with tags for\nsubjects and objects that can be filled in. A set of 100 invalid questions using the templates is\ngenerated by sampling subjects and objects from disjoint facts in the original dataset, and\nensuring the questions do not have valid answers in DBpedia. Additional invalid questions\nare produced by replacing dates within questions from TriviaQA [92] with ones that do not\nexist. By passing each test question through various open-source and proprietary LLMs, the\nauthors manually validate that each model has a tendency to hallucinate. Unfortunately, Ra-\nmakrishna et al. find that using automated evaluation metrics (discussed in Appendix D.1)\ndo not correlate well with human annotations on their generated dataset, leaving room for\nfuture growth in aligned automatic evaluation metrics.\n(4) A more recent adversarial method from Uluoglakci and Temizel [209] attempts to automati-\ncally generate hypothetical, invalid questions that language models should reject answering.\nFor example, the hypothetical question, ‚ÄúWhat are the differences between Platypus LLM and\nWolf LLM?‚Äù should be rejected since Wolf LLM is nonexistent, even though Platypus [108] is\na real family of language models. Intuitively, if a model provides a plausible answer to the\ninvalid question, the authors claim that either the fabricated term is not be in the model‚Äôs\ntraining set or the model has a higher tendency to hallucinate overall. As such, the authors\npropose a new framework to generate hypothetical questions, with which they create the\nHypoTermQA dataset. In particular, Uluoglakci and Temizel first query GPT-3.5 for20 popular\ntopics and then 50 hypothetical terms per topic, resulting in 790 filtered fake phrases. To gen-\nerate a diverse set of outputs, the model is set with a high temperature parameter. The authors\nargue that questions produced with only hypothetical terms will be easier to distinguish by\nlanguage models, and thus, generate a set of similar terms with real meanings, in an attempt\nto deceive models. Similar, valid phrases were generated using three distinct approaches:\nquerying GPT-3.5 directly, retrieving titles from Wikipedia with similar vector embeddings to\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:34 Chakraborty et al.\nthe hypothetical terms, and taking the titles of Wikipedia passages whose definition embed-\ndings are similar to those of the hypothetical terms. Finally, GPT-3.5 is instructed to generate\nplausible questions with filtered hypothetical and valid terms to produce a total of 19.5K\nhypothetical and valid questions. Rather than relying on human annotation for identifying\nhallucinated responses, Uluoglakci and Temizel additionally propose HypoTermQA Score ‚Äî\nthe ratio of valid answers to the total number of hypothetical questions, automatically labeled\nby a proxy evaluator LLM agent (like methods in Section 4.3.3). In evaluation, the authors\nfind that both open-source and proprietary models have over 90% frequency of generating\ninvalid responses. However, this frequency differed depending on the chosen evalautor model\ndue to biases. Thus, proxy evaluator agents are also tested by comparing against manual\nannotation. Overall, the proposed dataset generation and automatic hallucination detection\nmethod show great promise in evaluating the factual hallucination tendency of language\nmodels. But, additional work should be done to handle biases in evaluator models, evaluate\nother characteristics of hallucinations, and consider other deployments outside QA.\nC.3.3 Proxy Model.\n(3) Similarly, Pacchiardi et al. [152] develop a black-box lie detector for LLMs. In their case, the\nauthors hypothesize that models that output a lie will produce different behaviors in future\nresponses, like Azaria and Mitchell[7]. As such, at inference time, Pacchiardi et al. prompt the\nLLM with several binary questions (that may be completely unrelated to the original response)\nand collect yes/no answers. All the responses are concatenated into a single embedding that is\ninput to the logistic regression model to predict the likelihood that the response was untruthful.\nTo evaluate their lie detector, the authors assemble over 20K questions from existing data\nsources on topics including general trivia [139, 211, 219], basic arithmetic [156], common\nsense reasoning [202], text translation [204], and self-awareness [158]. They additionally\nsynthetically generate questions with unknowable answers as a control split (e.g., ‚ÄúWhat day\nis it?‚Äù). The authors find that the simple detector is mostly task- and model-agnostic once\ntrained on a single dataset.\n(4) Chu et al. [36] primarily rely on the LVLM GPT-4 to detect hallucinations within AI-generated\nvideos. Video hallucinations are classified as static hallucinations, which occur in individual\nframes, or dynamic hallucinations, which occur across multiple frames. Here, the authors\nemploy the generalized capabilities of GPT-4 to detect objects within keyframes of a video,\nsummarize the video from the keyframes (which might be inconsistent with the original video\ngeneration prompt due to hallucinations), synthesize a temporal knowledge graph represent-\ning the changing relations among detected objects, detect inconsistencies along the knowledge\ngraph, aggregate a hallucination score, and describe the detected hallucinations. Sora Detec-\ntor is evaluated on a custom dataset collected by the authors, T2VHaluBench (described in\nAppendix D.2.4), and outperforms video hallucination detection ablation approaches that\nignore knowledge graph construction, showcasing the usefulness of the representation. As\nsuch the structured approach shows promise in identifying undesired effects in generated\nvideos, but the authors do not provide discussion on the possible hallucinations generated by\nGPT-4 when detecting and describing failures.\nC.3.4 Grounding Knowledge.\n(2) Some knowledge grounding approaches prompt LLMs to generate code to directly query infor-\nmation from databases. Li et al. [118] are motivated by the limitations of existing knowledge-\nbased hallucination mitigation methods; namely that (1) they utilize a fixed knowledge source\nfor all questions, (2) generating retrieval questions with LLMs that interface with a database\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:35\nis not effective because they may not be trained on the particular programming language of\nthe database, and (3) there is no correction capability that handles error propagation between\nknowledge modules. Consequently, the authors propose augmenting LLMs with heteroge-\nneous knowledge sources to assist with summary generation. Specifically, in the event that\nthe model is found to be uncertain about its generated statement through self-contradiction,\ntheir framework, chain-of-knowledge (CoK), chooses subsets of knowledge-bases that may\nbe helpful for answering the original question. Assuming each database has its own query\ngenerator, CoK queries for evidence, and corrects rationales between different sources itera-\ntively. Compared to chain-of-thought reasoning, CoK consistently produces more accurate\nanswers with its iterative corrections.\n(3) Another source of potential conflict that leads to hallucinations, is misalignment between a\nmodel‚Äôs capabilities and the user‚Äôs beliefs about what it can do. Zhang et al. [251] tackle this\nknowledge alignment problem and categorize alignment failures into four types:\n‚Ä¢Semantic ‚Äî an ambiguous term maps to multiple items in a database\n‚Ä¢Contextual ‚Äî the user failing to explicitly provide constraints\n‚Ä¢Structural ‚Äî user provides constraints that are not feasible in the database\n‚Ä¢Logical ‚Äî complex questions that require multiple queries\nTheir proposed MixAlign framework interacts with the user to get clarification when the\nLLM is uncertain about its mapping from the user query to the database. With the original\nquery, knowledge-base evidence, and user clarifications, the LLM formats its final answer to\nthe user.\n(6) Lim and Shim [123] tackle the issue of image generation models producing results that are\nfactually noncompliant with the original prompt, by utilizing an external database to address\ninconsistencies. In particular, the authors confront three forms of image hallucinations:\nfactual inconsistencies, outdated knowledge, and factual fabrications (unlikely generations).\nGiven a prompt and an original (hallucinated) generation from a generator, the authors\nmanually collect a ground-truth image describing the prompt using Google. An LVLM is\nemployed to identify the differences between the retrieved and generated images, resulting in\nan instruction for how to edit the original generation to become factual. Finally, the original\ngeneration and editing instruction is passed through the generator to produce an updated\nresult. Qualitatively, the pipeline produces more factually compliant and plausible generations.\nHowever, a human needs to manually choose the ground-truth image, the prompt correction\ngeneration model can hallucinate improper instructions, and the authors do not compare\ntheir method with other image hallucination mitigation methods.\n(7) Schramowski et al. [182] present Safe Latent Diffusion (SLD), a method to provide additional\nguidance to image generation diffusion models at inference time to reduce the tendency\nof inappropriate generations. Intuitively, diffusion models iteratively remove noise from\na generation until a plausible result is produced. Along the way however, inappropriate\nfeatures like nudity or violence can be denoised, as seen in the training dataset. SLD updates\nthe original prompt guidance encoding by computing the latent encoding of unsafe guidance,\nand shifting the original encoding away from the unsafe encoding given set hyperparameters.\nThe resulting shifted ‚Äúsafe‚Äù guidance is far from unsafe embeddings, but still close enough\nto the original prompt guidance, to produce generations that attempt to follow the original\nprompt without inappropriate content. This augmented guidance is only provided after some\nwarm-up steps of denoising to allow for the model to generate results close to the original\nprompt. The authors provide four sets of hyperparameters to configure the aggressiveness of\nthe content filter. Using a model that detects inappropriate content, Schramowski et al. show\nthat SLD has a lower probability of generating undesired features, and a user study supports\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:36 Chakraborty et al.\nthat the safe generations are still preferred the same as (or more than) the unaugmented\nresults. It is important to note that, while SLD combats undesired generations, there are still\ninstances of inappropriate features present due to their representation in the training dataset.\nFurthermore, the proposed algorithm can be misused by shifting the augmented guidance\ntoward the unsafe vector, producing more inappropriate features.\nD Metrics and Evaluation Platforms\nWe now present common metrics, datasets, and simulation platforms leveraged when developing\nand evaluating the hallucination detection algorithms introduced in Section 4.\nD.1 Metrics\nHere, we list established metrics used for computing language similarity and accuracy of generated\nimage descriptions.\nD.1.1 Language Similarity.\nBERTScore [ 252]. Given a pair of responses, BERTScore computes the BERT [48] embeddings of\nthe sentences and calculates their cosine similarity.\nBARTScore [ 245]. Using a pre-trained BART model [112], which provides access to generated\ntoken probabilities, BARTScore sums over the log probability of each token generated while\nconditioning on context and previously output tokens. Essentially, BARTScore attempts to predict\nthe quality of a generated text using BART as a proxy model.\nSummaC [ 105]. SummaC is a class of natural language inference models that predict entailment,\ncontradiction, and neutral scores between pairs of sentences among a document and its summary.\nEach score is collected into a separate matrix split by metric type. The authors propose two\napproaches, SummaCZS and SummaCConv, for aggregating scores of each sentence in the summary\nwith respect to each sentence in the document.\nGPTScore [ 65]. Like BARTScore, GPTScore relies on a pre-trained language model with access to\ntoken probabilities to estimate quality of outputs, but uses the GPT series of LLMs.\nAlignScore [248]. The creators of AlignScore pose that two pieces of text are aligned when all\ninformation present in one text exists in the other, and the texts do not contradict one another.\nConsequently, they train a classification model on labeled data with three types of labels: a binary\nclassification of aligned or not, a multi-class prediction including a neutral label in addition to the\nbinary classification labels, and a continuous score for a regression task. The AlignScore metric\ncomputes a weighted score across all three prediction heads at test-time.\nSemantic Uncertainty [ 101]. One common method of measuring uncertainty of a model‚Äôs many\ngenerations is computing its entropy over all generated token probabilities. However, in cases where\nmultiple sentences have the same semantic meaning but output different entropies, the aggregated\nmeasurement is not representative of the true uncertainty of the model. Kuhn et al . tackle this\nproblem by clustering sentences into semantic classes and summing entropies of sentences from\nthe same class together.\nD.1.2 Image Captioning.\nCHAIR [ 175]. CHAIRùëñ, used for measuring accuracy of descriptions of images, is the ratio of\nthe number of hallucinated objects to all the objects mentioned in the description. To identify\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:37\nthe hallucinated objects within the description, the authors assume access to ground-truth object\nclasses in the image.\nPOPE [119]. Li et al. recognize that different instructions prompting for a description of an image\nmay lead to different responses from the model with the same semantic meaning. In this case,\nCHAIR gives different scores to both descriptions although they are alike. Instead, their proposed\nmetric, POPE, asks binary questions about the existence of in-domain and out-of-domain objects in\nthe image, which leads to more a more stable metric across different outputs.\nD.2 Offline Datasets\nIn this section, we present relevant offline datasets used for evaluating the performance of halluci-\nnation detection and mitigation techniques in driving, robotic, and QA tasks.\nD.2.1 Driving.\nBDD-X [ 97]. BDD-X is a multi-modal driving dataset consisting of 20K samples (i.e., video clips),\neach consisting of eight images with vehicle control actions and text annotations describing the\nscene and justifying actions.\nDriveGPT4 [231]. Xu et al. augment BDD-X into a QA dataset consisting of questions that ask\nabout the current action of the vehicle, reasoning behind the action, and predicting future control\nsignals. To incorporate other questions a user might ask about the vehicle, surroundings, and other\nmiscellaneous queries, they prompt ChatGPT to generate further questions. In total, the DriveGPT4\ndataset contains 56K samples.\nnuScenes [ 22]. The nuScenes dataset contains 1K driving videos, each running for 20 seconds,\ncollected from roads in Boston and Singapore. Each frame includes six different RGB camera views,\nGPS, annotated 3D bounding boxes of various object classes, and semantically labeled rader, lidar,\nand map representations.\nNuScenes-QA [163]. Like DriveGPT4, NuScenes-QA is a visual QA dataset, but built on top of\nnuScenes. It includes five different types of questions including checking the existence of objects,\ncounting instances, detecting the object being referred to, identifying the action state of an object,\nand comparing two objects. Overall, the dataset holds450K QA pairs across34K scenes in nuScenes.\nTalk2Car [47]. Talk2Car is an earlier extension of the nuScenes dataset which aims to ignite\nfurther research into developing systems that bridge the gap between passengers and an autonomous\nvehicle through natural language. Annotators provided approximately 12K text commands over\n850 videos within the nuScenes training split which refer to an object in the scene.\nRefer-KITTI [ 223]. While Talk2Car is a pioneering work for object referral in real driving scenes\nthrough natural language, each annotated instruction only refers to one object. As such, Wu et al.\npropose a new task definition, referring multi-object tracking (RMOT), which attempts to predict\nall objects that are referred to within a natural language input. They augment the KITTI driving\ndataset [67] with labeled 2D bounding boxes around objects that are referenced within a text\nprompt for 6.5K images.\nNuPrompt [ 224]. NuPrompt is another RMOT-based benchmark, but applied to nuScenes and\nwith 3D bounding box labels. It includes 35K languages prompts, with most prompts referring to\nanywhere between one and ten objects.\nDRAMA [ 135]. Malla et al . argue that, while several datasets exist for anomaly detection or\nidentification on roads, there is a gap in explaining the reason for categorizing an object as being\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:38 Chakraborty et al.\nrisky, i.e., objects the model should pay attention to, like crosswalks, pedestrians, and traffic lights. As\nsuch, DRAMA is a benchmark tackling identification of risky objects in a driving scene conditioned\non natural language. Ding et al. [51] extend DRAMA to further include suggestions on actions the\nego vehicle can take to minimize risk, but the dataset is not public at this time.\nNuInstruct [ 50]. NuInstruct addresses two common limitations in existing driving datasets: they\ncover a limited subset of necessary tasks while driving (e.g., evaluating perception while ignoring\nplanning), and disregard temporal and multi-view representations. Built on top of NuScenes, the\ndataset provides 91K samples of multi-view sequences with corresponding QA pairs spanning 17\nsubtasks within perception, prediction, planning, and risk detection.\nDriveLM [191]. The authors of DriveLM curate a similar comprehensive dataset from nuScenes\nand the CARLA driving simulator [54] with open-ended and factual questions about importance\nrankings of nearby vehicles, planning actions, detecting lanes, and more.\nDriving with LLMs [ 29]. Chen et al. collect a text-based QA dataset from a proprietary driving\nsimulator, generated from ChatGPT with ground-truth observations ( e.g., relative locations of\ndetected vehicles, ego vehicle control actions, etc.) from the simulator.\nD.2.2 Code Generation and Robotics.\nHumanEval [31]. HumanEval is a set of164 handwritten programs, each with a function definition,\ndocstring, program body, and unit tests. The authors find there is great promise in using LLMs for\ncode generation, but output quality is limited by length of context and buggy examples.\nRoboCodeGen [ 121]. Liang et al. build a new code generation benchmark specifically for robot\ntasks with 37 functions focused on spatial reasoning, geometric reasoning, and controls.\nLanguage-Table [133]. The Language-Table dataset contains594K trajectories manually annotated\nwith 198K unique instructions across simulated and real-world manipulator robots. The multi-\nmodal dataset consists of video sequences, corresponding actions at each time step, and language\ninstructions describing the policy of the robot in hindsight.\nSaGC [ 154]. The authors of the CLARA method developed a dataset to identify language goals\nfrom a user that are certain, ambiguous, and infeasible. Collected from three different types of\nrobots (cooking, cleaning, and massage), SaGC is annotated with a floor-plan, descriptions of objects\nand people in view, a text goal, and a label of uncertainty.\nD.2.3 Question-answering.\nHotPotQA [237]. HotPotQA is a question-answering benchmark with 113K multi-hop questions\n(i.e., requiring multiple steps of reasoning to reach answer) collected from Wikipedia. The dataset in-\ncludes both questions that require finding relevant phrases from context paragraphs, and comparing\ntwo entities.\nFEVER [ 203]. In contrast to HotPotQA, the developers of FEVER attempt to answer the question\nof whether a fact is supported by a knowledge-base. The database contains 185K claims with\nannotated labels deciding if each claim is supported, refuted, or indeterminable from Wikipedia\narticles.\nNatural Questions [103]. Natural Questions is yet another QA dataset with sources from Wikipedia.\nThe authors release 307K training and 7K test samples of real (anonymized) queries into the Google\nsearch engine paired with a Wikipedia page and a long and short answer annotated by a person\nbased on said article.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:39\nStrategyQA [68]. Like HotPotQA, StrategyQA aims to develop a dataset of implicit multi-hop\nquestions, but includes a greater variety categories of questions, and with less category imbalance.\nFurthermore, most of the questions in the dataset require three or more steps of decomposition\nand referencing to accurately solve.\nQreCC [5]. Separate from the information retrieval task described in benchmarks above, Anantha\net al. develop a dataset, QreCC, for conversational QA. They focus on reading comprehension,\npassage retrieval, and question rewriting tasks, with a total of 13.7K dialogues paired with 81K\nquestions.\nHA-DPO [ 257]. Zhao et al . present a multi-model visual QA dataset of images, hallucinated\ndescriptions, and non-hallucinated samples from the VG dataset [100].\nD.2.4 Image and Video Generation.\nAVA [145]. Murray et al. tackle curating a dataset to evaluate the aesthetic value of varying\nquality images. AVA contains250K images with aesthetic ratings, textual semantic class labels, and\npictographic style labels. The majority of images contain at least one semantic label, with 150K\ncontaining at least two labels.\nMSCOCO [ 126]. The MSCOCO dataset contains 328K images with 2.5M human-labeled segmen-\ntations of common objects. Each image is accompanied by five text captions. In contrast to earlier\ndatasets like ImageNet [46] and SUN [228], MSCOCO provides more instance labels per class and\nmore non-iconic images containing multiple objects per scene.\nLSUN [ 243]. As vision models continue to evolve and essentially solve existing benchmarks,\nnew datasets need to be curated to provide additional challenges and evaluate generalization\nperformance. Thus, Yu et al. propose a human-in-the-loop data labeling scheme to gather a dataset\nwith one million images for ten scene and 20 object classes, by iterating between manual labeling,\noverfitting a classifier, and automatic labeling. Note that the overall precision of labels is slightly\nlower than pure manual labeling approaches.\nLAION-400M [ 184]. More recently, companies are collecting proprietary datasets to train large\nvision models, outperforming open-sourced models trained on smaller scale datasets. To narrow\nthe gap in training data for generative models, Schuhmann et al. present an image dataset with\n400M images with corresponding metadata, CLIP [167] embeddings, and web crawling resources.\nUnfortunately, works have found that the training split of LAION-400M contains undesired content\nlike racy imagery, slurs, stereotypes, and other biases, which should be filtered out [177].\nI2P [ 182]. The Inappropriate Image Prompts dataset attempts to measure the likelihood of image\ngeneration models to output undesired, inappropriate content given a prompt. In particular, the\ndataset contains 4.5K prompts taken from real users of Stable Diffusion, which have been filtered\nto contain inappropriate details like hate speech, violent imagery, and criminal behavior.\nDrawBench [ 177]. The DrawBench benchmark contains 200 prompts across 11 categories to\nevaluate the quality of generated images on a spectrum of properties ( e.g., colors, quantities,\ngenerated text). Saharia et al. use their benchmark to present 8 samples from two different models\nto human raters, who choose which model they prefer qualitatively.\nT2VHaluBench [ 36]. The developers of the Sora Detector curate a benchmark dataset for text\nto video hallucination detection. In total, the dataset contains 59, 8-second-long videos ‚Äî 53 of\nwhich contain consistency, static, or dynamic hallucinations generated by the Runway-Gen-2 [3]\nand SORA [149] generative models.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:40 Chakraborty et al.\nD.3 Simulation Platforms\nFinally, we introduce common online simulators used to test hallucination detection methods for\ndecision-making tasks.\nD.3.1 Driving.\nHighwayEnv [ 111]. Leurent presents a 2D car simulator, with driving scenarios ranging from a\npassing on a multi-lane highway, merging into a highway, merging and exiting from a roundabout,\nparking, and more. An ego vehicle can be controlled with discrete (e.g., merge left, merge right,\nfaster, etc.) or continuous (e.g., providing an explicit acceleration command) actions.\nSUMO [ 131]. Geared towards microscopic traffic simulation, SUMO allows researchers to design\nroad networks, track traffic flow metrics, and control individual vehicles.\nCARLA [ 54]. CARLA is a 3D driving simulator built on top of Unreal Engine. Existing works\nbenchmark their methods on CARLA for perception, planning, control, and QA tasks for its realism.\nThere is also capability to perform co-simulation with SUMO and CARLA simultaneously [216].\nD.3.2 Robotics.\nRavens [246]. Ravens is a 3D manipulator robot (UR5e) simulator built with PyBullet [41] with\ntasks like block insertion, towers of hanoi, aligning boxes, assembling kits, etc. Each simulated task\nfeatures a manipulator robot with a suction gripper sitting on a table workspace, with three camera\nviews.\nALFWorld [190]. Building on top of the TextWorld simulator, discussed in Appendix D.3.3, ALF-\nWorld aligns perception from the 3D robot simulation benchmark, ALFRED [189], with text-based,\ndiscrete actions like ‚ÄúMoveAhead, ‚Äù ‚ÄúRotateLeft, ‚Äù and ‚ÄúOpen. ‚Äù\nProgPrompt [193]. ProgPrompt is a benchmark of high-fidelity 3D data collected from a virtual\nhome robot. It includes three environments, each with 115 object instances. These simulations are\nfurther used to create a dataset of 70 household robot tasks with a ground-truth set of actions to\nachieve each goal.\nRoboEval [ 80]. RoboEval is a general platform for checking the correctness of code generated for\na robot task. It relies on a simulator, evaluator, and a set of defined tasks to perform evaluations\non a simulated robot. While ProgPrompt captures more realistic scenarios in its high-fidelity 3D\nsimulator, RoboEval is tuned towards verifying code efficiently.\nKnowNo TableSim [ 174]. More recently, the developers of KnowNo also provide a tabletop\nsimulator based on PyBullet, like Zeng et al . [246], for robot manipulation of blocks and bowls.\nProvided instructions vary in ambiguity by attribute, number, and spatial reasoning.\nD.3.3 Other Simulators.\nTextWorld [40]. TextWorld is a suite of text-based games that can be either hand-engineered or\nprocedurally generated, where an agent directly receives text-based observations from an abstract\nworld, and acts with natural language actions to complete a task.\nBabyAI [34]. Chevalier-Boisvert et al. present a 2D top-down, grid-based simulator of instruction-\nfollowing tasks with varying difficulty. Some tasks include simple navigation to a single goal,\npicking and placing objects with ambiguous references, and instructions that implicitly require\nmulti-step reasoning to complete. The simulator provides a partial observation of the space near\nthe agent at every timestep.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:41\nMineDojo [ 60]. The developers of MineDojo attempt to create a benchmark to test the continual\nlearning of agents in an open-world setting. They build an interface on top of Minecraft, a video\ngame, to enable testing with diverse open-ended tasks, and provide access to an external knowledge-\nbase of existing Minecraft tutorials and wiki discussions. MineDojo includes several thousands of\ntasks that are more complex than earlier works (and require multi-step reasoning). As such, task\ncompletion is judged with a learned LVLM, which acts like a human evaluator.\nSmallville [ 155]. Park et al. present a multi-agent conversational simulator where agents are\ncontrolled by language models. Users may set up agents with a defined backstory and provide\ninstructions when desired. Each agent has access to a memory of past experiences, and generates\nnatural language actions to go to certain areas, communicate with others, complete chores, and\nmore.\nE Guidelines on Current Methodologies\nE.1 Beware of Erroneous Hallucination Predictions\nWe additionally point out risks of relying on hallucination intervention algorithms and metrics\nthat use deep learning methods at inference time. Generally speaking, deep neural networks are\nblack boxes with few statistical guarantees and lack interpretability [2, 14, 21, 52, 159, 201]. As such,\nthese models may incorrectly detect hallucinations during deployment ‚Äî effectively hallucinating\nthemselves. In particular, engineers should take precautions when utilizing detection algorithms\nunder method types like hidden states, attention weights, concept probabilities, analyzing samples,\nand proxy model, where neural networks are used frequently. Learned proxy models or classifiers are\nparticularly prone to incorrect predictions because they are trained on a different data distribution\nfrom the model under test [ 8, 166]. Similarly, decreased model complexity may result in poor\npredictions [13, 79, 109]. As such, several works we have listed in Table 4 use LVLM-based evaluators\n(e.g., GPT-4) to increase evaluation model complexity and cover a broader data distribution. However,\nas we have found throughout this work, LVLMs should not be overly relied upon for consistently\naccurate estimates. Instead, learned hallucination detection models that further output a calibrated\nconfidence score (relaying their uncertainty of a prediction) can assist designers with choosing\nhow to utilize the evaluated model‚Äôs decision. Additionally, classic machine metrics like accuracy,\nprecision, recall, false-positive rate (FPR), etc. provide engineers with an understanding of the\neffectiveness of hallucination detection methods prior to deployment. The learned intervention\nalgorithm should be tuned to balance true-positive rate (TPR) and FPR, such that it does not\nmiss critical hallucinations, nor act overly conservative (predicting hallucinations too frequently).\nWe also suggest caution when using learning-based similarity metrics like BERTScore, SummaC,\nAlignScore, CLIP Score, and others listed in Table 4 for similar reasons. Specifically, while these\nmetrics have been shown to reasonably reflect the semantic similarity of varied terms, there are still\ncases of poor alignment [71, 78]. Even adversarial models used in adversarial prompting approaches\nare not safe from learned biases, which could lead to a poor understanding of the uncertainty of\nthe model under test. For example, an adversarial agent tasked with prompting an autonomous\ndriving LVLM could provide incorrect sensor readings, obviously resulting in poor decisions from\nthe LVLM under test. In this case, we have not learned any additional information on the reliability\nof the predicted action. Thus, we argue that deep-learning-based adversarial prompters should\nbe grounded in accurate data to truly understand the uncertainty of model predictions. Overall,\nengineers should take care when using deep learning methods for hallucination intervention ‚Äî or\nas metrics during evaluation ‚Äî because of their tendency to act unpredictably with out-of-domain\ndata, and limited theoretical guarantees.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:42 Chakraborty et al.\nReferences\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,\nJanko Altenschmidt, Sam Altman, Shyamal Anadkat, et al . 2023. GPT-4 Technical Report. arXiv preprint\narXiv:2303.08774 (2023).\n[2] Charu C. Aggarwal. 2023. Neural Networks and Deep Learning . Springer International Publishing, Cham.\n[3] Runway AI. 2023. Gen-2: Generate novel videos with text, images or video clips. Runway blog (2023). https:\n//runwayml.com/research/gen-2\n[4] Soroush Ali, Glicksberg Benjamin S., Zimlichman Eyal, Barash Yiftach, Freeman Robert, Charney Alexander W.,\nNadkarni Girish N, and Klang Eyal. 2024. Large Language Models Are Poor Medical Coders ‚Äî Benchmarking of\nMedical Code Querying. NEJM AI 1, 5 (25 Apr 2024), 13 pages.\n[5] Raviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2021.\nOpen-Domain Question Answering Goes Conversational via Question Rewriting. InProceedings of the 2021 Conference\nof the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Virtual).\nAssociation for Computational Linguistics, 520‚Äì534.\n[6] Anastasios N. Angelopoulos, Stephen Bates, Emmanuel J. Cand√®s, Michael I. Jordan, and Lihua Lei. 2022. Learn then\nTest: Calibrating Predictive Algorithms to Achieve Risk Control. arXiv preprint arXiv:2110.01052 (2022).\n[7] Amos Azaria and Tom Mitchell. 2023. The Internal State of an LLM Knows When It‚Äôs Lying. In Findings of the\n2023 Conference on Empirical Methods in Natural Language Processing (Singapore). Association for Computational\nLinguistics, 967‚Äì976.\n[8] Christina Baek, Yiding Jiang, Aditi Raghunathan, and J. Zico Kolter. 2022. Agreement-on-the-line: Predicting the\nPerformance of Neural Networks under Distribution Shift. In Proceedings of the 2022 Conference on Neural Information\nProcessing Systems (New Orleans, LA, USA). Curran Associates, Inc., 19274‚Äì19289.\n[9] Song Bai and Jie Li. 2024. Progress and Prospects in 3D Generative AI: A Technical Overview including 3D human.\narXiv preprint arXiv:2401.02620 (2024).\n[10] Zechen Bai, Pichao Wang, Tianjun Xiao, Tong He, Zongbo Han, Zheng Zhang, and Mike Zheng Shou. 2024. Halluci-\nnation of Multimodal Large Language Models: A Survey. arXiv preprint arXiv:2404.18930 (2024).\n[11] Rounak Banik. 2017. The Movies Dataset. Kaggle (2017). https://www.kaggle.com/datasets/rounakbanik/the-movies-\ndataset\n[12] Lauren Barack. 2024. Using AI in lesson planning? Beware hallucinations. K-12 Dive (2024). https://www.k12dive.\ncom/news/using-ai-lesson-planning-beware-hallucinations/726660/\n[13] Falco J Bargagli Stoffi, Gustavo Cevolani, and Giorgio Gnecco. 2022. Simple Models in Complex Worlds: Occam‚Äôs\nRazor and Statistical Learning Theory. Minds and Machines 32, 1 (March 2022), 13‚Äì42.\n[14] J.M. Benitez, J.L. Castro, and I. Requena. 1997. Are Artificial Neural Networks Black Boxes? IEEE Transactions on\nNeural Networks 8, 5 (1997), 1156‚Äì1164.\n[15] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee,\nYufei Guo, et al. 2023. Improving Image Generation with Better Captions. OpenAI blog (2023). https://cdn.openai.\ncom/papers/dall-e-3.pdf\n[16] Louis Blankemeier, Joseph Paul Cohen, Ashwin Kumar, Dave Van Veen, Syed Jamal Safdar Gardezi, Magdalini Paschali,\nZhihong Chen, Jean-Benoit Delbrouck, Eduardo Reis, Cesar Truyts, et al. 2024. Merlin: A Vision Language Foundation\nModel for 3D Computed Tomography. arXiv preprint arXiv:2406.06512 (2024).\n[17] Rishi Bommasani, Drew A. Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S. Bernstein,\nJeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. 2022. On the Opportunities and Risks of Foundation Models.\narXiv preprint arXiv:2108.07258 (2022).\n[18] Rakesh P Borase, DK Maghade, SY Sondkar, and SN Pawar. 2021. A review of PID control, tuning methods and\napplications. International Journal of Dynamics and Control 9 (2021), 818‚Äì827.\n[19] Neil Brewer, Rob Potter, Ronald P. Fisher, Nigel Bond, and Mary A. Luszcz. 1999. Beliefs and Data on the Relationship\nBetween Consistency and Accuracy of Eyewitness Testimony. Applied Cognitive Psychology 13, 4 (1999), 297‚Äì313.\n[20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language Models are Few-Shot Learners. In Proceedings of\nthe 2020 Conference on Neural Information Processing Systems (Virtual). Curran Associates, Inc., 1877‚Äì1901.\n[21] Nathan Buskulic, Jalal Fadili, and Yvain Qu√©au. 2024. Convergence and Recovery Guarantees of Unsupervised Neural\nNetworks for Inverse Problems. Journal of Mathematical Imaging and Vision 66, 4 (Aug. 2024), 584‚Äì605.\n[22] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan,\nGiancarlo Baldan, and Oscar Beijbom. 2020. nuScenes: A Multimodal Dataset for Autonomous Driving. In Proceedings\nof the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (Virtual). Institute of Electrical\nand Electronics Engineers, 11618‚Äì11628.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:43\n[23] Shulin Cao, Jiaxin Shi, Liangming Pan, Lunyiu Nie, Yutong Xiang, Lei Hou, Juanzi Li, Bin He, and Hanwang Zhang.\n2022. KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge\nBase. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Dublin, Ireland).\nAssociation for Computational Linguistics, 6101‚Äì6119.\n[24] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin.\n2021. Emerging Properties in Self-Supervised Vision Transformers. In Proceedings of the 2021 IEEE/CVF Conference\non International Conference on Computer Vision (ICCV) (Virtual). Institute of Electrical and Electronics Engineers,\n9630‚Äì9640.\n[25] Neeloy Chakraborty, Aamir Hasan, Shuijing Liu, Tianchen Ji, Weihang Liang, D. Livingston McPherson, and Katherine\nDriggs-Campbell. 2023. Structural Attention-based Recurrent Variational Autoencoder for Highway Vehicle Anomaly\nDetection. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems (London,\nUnited Kingdom). International Foundation for Autonomous Agents and Multiagent Systems, Richland, SC, USA,\n1125‚Äì1134.\n[26] Anthony Chen, Panupong Pasupat, Sameer Singh, Hongrae Lee, and Kelvin Guu. 2023. PURR: Efficiently Editing\nLanguage Model Hallucinations by Denoising Language Model Corruptions. arXiv preprint arXiv:2305.14908 (2023).\n[27] Canyu Chen and Kai Shu. 2024. Can LLM-Generated Misinformation Be Detected?. In Proceedings of the 12th\nInternational Conference on Learning Representations (Vienna, Austria).\n[28] Danqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to Answer Open-Domain\nQuestions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Vancouver,\nCanada). Association for Computational Linguistics, 1870‚Äì1879.\n[29] Long Chen, Oleg Sinavski, Jan H√ºnermann, Alice Karnsund, Andrew James Willmott, Danny Birch, Daniel Maund,\nand Jamie Shotton. 2024. Driving with LLMs: Fusing Object-Level Vector Modality for Explainable Autonomous\nDriving. In Proceedings of the 2024 IEEE International Conference on Robotics and Automation (ICRA) (Yokohama,\nJapan). Institute of Electrical and Electronics Engineers, 14093‚Äì14100.\n[30] Liting Chen, Lu Wang, Hang Dong, Yali Du, Jie Yan, Fangkai Yang, Shuang Li, Pu Zhao, Si Qin, Saravan Rajmohan,\net al. 2023. Introspective Tips: Large Language Model for In-Context Decision Making. arXiv preprint arXiv:2305.11598\n(2023).\n[31] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards,\nYuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating Large Language Models Trained on Code. arXiv\npreprint arXiv:2107.03374 (2021).\n[32] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Zhibin Wang, Jingyi Yu, Gang\nYu, et al. 2024. MeshXL: Neural Coordinate Field for Generative 3D Foundation Models. In Proceedings of the 2024\nConference on Neural Information Processing Systems .\n[33] Yuyan Chen, Qiang Fu, Yichen Yuan, Zhihao Wen, Ge Fan, Dayiheng Liu, Dongmei Zhang, Zhixu Li, and Yanghua\nXiao. 2023. Hallucination Detection: Robustly Discerning Reliable Answers in Large Language Models. In Proceedings\nof the 32nd ACM International Conference on Information and Knowledge Management (Birmingham, United Kingdom).\nAssociation for Computing Machinery, New York, NY, USA, 245‚Äì255.\n[34] Maxime Chevalier-Boisvert, Dzmitry Bahdanau, Salem Lahlou, Lucas Willems, Chitwan Saharia, Thien Huu Nguyen,\nand Yoshua Bengio. 2019. BabyAI: First Steps Towards Grounded Language Learning With a Human In the Loop. In\nProceedings of the 7th International Conference on Learning Representations (New Orleans, LA, USA).\n[35] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham,\nHyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al . 2023. PaLM: Scaling Language Modeling with\nPathways. Journal of Machine Learning Research 24, 240 (2023).\n[36] Zhixuan Chu, Lei Zhang, Yichen Sun, Siqiao Xue, Zhibo Wang, Zhan Qin, and Kui Ren. 2024. Sora Detector: A Unified\nHallucination Detection for Large Text-to-Video Models. arXiv preprint arXiv:2405.04180 (2024).\n[37] Jan Clusmann, Fiona R. Kolbinger, Hannah Sophie Muti, Zunamys I. Carrero, Jan-Niklas Eckardt, Narmin Ghaffari\nLaleh, Chiara Maria Lavinia L√∂ffler, Sophie-Caroline Schwarzkopf, Michaela Unger, Gregory P. Veldhuizen, et al. 2023.\nThe future landscape of large language models in medicine. Communications Medicine 3, 1 (10 Oct 2023), 141.\n[38] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry\nTworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training Verifiers to Solve Math Word Problems. arXiv preprint\narXiv:2110.14168 (2021).\n[39] Tor Constantino. 2024. AI Experts Test Perplexity‚Äôs New Election Hub. Forbes (2024). https://www.forbes.com/sites/\ntorconstantino/2024/11/04/perplexitys-new-election-hub-triggers-reactions-from-ai-experts/\n[40] Marc-Alexandre C√¥t√©, √Åkos K√°d√°r, Xingdi Yuan, Ben Kybartas, Tavian Barnes, Emery Fine, James Moore, Matthew\nHausknecht, Layla El Asri, et al. 2019. TextWorld: A Learning Environment for Text-Based Games. In Proceedings of\nthe 7th Computer Games Workshop at the 27th International Conference on Artificial Intelligence (Stockholm, Sweden).\nSpringer International Publishing, 41‚Äì75.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:44 Chakraborty et al.\n[41] Erwin Coumans and Yunfei Bai. 2016‚Äì2021. PyBullet, a Python module for physics simulation for games, robotics\nand machine learning. http://pybullet.org.\n[42] Can Cui, Yunsheng Ma, Xu Cao, Wenqian Ye, Yang Zhou, Kaizhao Liang, Jintai Chen, Juanwu Lu, Zichong Yang,\nKuei-Da Liao, et al. 2024. A Survey on Multimodal Large Language Models for Autonomous Driving. In Proceedings of\nthe 1st Workshop on Large Language and Vision Models for Autonomous Driving at the 2024 IEEE/CVF Winter Conference\non Applications of Computer Vision (WACV) (Waikoloa, HI, USA). Institute of Electrical and Electronics Engineers,\n958‚Äì979.\n[43] Matthew Dahl, Varun Magesh, Mirac Suzgun, and Daniel E Ho. 2024. Large Legal Fictions: Profiling Legal Hallucina-\ntions in Large Language Models. Journal of Legal Analysis 16, 1 (06 2024), 64‚Äì93.\n[44] Wenliang Dai, Zihan Liu, Ziwei Ji, Dan Su, and Pascale Fung. 2023. Plausible May Not Be Faithful: Probing Object\nHallucination in Vision-Language Pre-training. In Proceedings of the 17th Conference of the European Chapter of the\nAssociation for Computational Linguistics . Association for Computational Linguistics, Dubrovnik, Croatia, 2136‚Äì2148.\n[45] Leonardo de Moura and Nikolaj Bj√∏rner. 2008. Z3: An Efficient SMT Solver. In Proceedings of the 14th International\nConference on Tools and Algorithms for the Construction and Analysis of Systems (Budapest, Hungary). Springer Berlin\nHeidelberg, 337‚Äì340.\n[46] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. 2009. ImageNet: A Large-Scale Hierarchical\nImage Database. In Proceedings of the 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (Miami,\nFL, USA). Institute of Electrical and Electronics Engineers, 248‚Äì255.\n[47] Thierry Deruyttere, Simon Vandenhende, Dusan Grujicic, Luc Van Gool, and Marie-Francine Moens. 2019. Talk2Car:\nTaking Control of Your Self-Driving Car. In Proceedings of the 2019 Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) (Hong\nKong, China). Association for Computational Linguistics, 2088‚Äì2098.\n[48] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. In Proceedings of the 2019 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies (Minneapolis, MN, USA). Association for\nComputational Linguistics, 4171‚Äì4186.\n[49] Shehzaad Dhuliawala, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason E Weston.\n2024. Chain-of-Verification Reduces Hallucination in Large Language Models. In Proceedings of the Workshop on\nReliable and Responsible Foundation Models at the 12th International Conference on Learning Representations (Vienna,\nAustria).\n[50] Xinpeng Ding, Jianhua Han, Hang Xu, Xiaodan Liang, Wei Zhang, and Xiaomeng Li. 2024. Holistic Autonomous\nDriving Understanding by Bird‚Äôs-Eye-View Injected Multi-Modal Large Models. In Proceedings of the 2024 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) (Seattle, WA, USA). Institute of Electrical and Electronics\nEngineers, 13668‚Äì13677.\n[51] Xinpeng Ding, Jianhua Han, Hang Xu, Wei Zhang, and Xiaomeng Li. 2023. HiLM-D: Towards High-Resolution\nUnderstanding in Multimodal Large Language Models for Autonomous Driving. arXiv preprint arXiv:2309.05186\n(2023).\n[52] James E Dobson. 2023. On reading and interpreting black box deep neural networks. International Journal of Digital\nHumanities 5, 2 (Nov. 2023), 431‚Äì449.\n[53] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Zhiyong Wu, Baobao Chang, Xu Sun, Jingjing Xu, Lei Li, and Zhifang\nSui. 2023. A Survey on In-context Learning. arXiv preprint arXiv:2301.00234 (2023).\n[54] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. 2017. CARLA: An Open Urban\nDriving Simulator. In Proceedings of the 1st Conference on Robot Learning (Mountain View, CA, USA) (Proceedings of\nMachine Learning Research, Vol. 78) . PMLR, 1‚Äì16.\n[55] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid,\nJonathan Tompson, Quan Vuong, Tianhe Yu, et al. 2023. PaLM-E: An Embodied Multimodal Language Model. In\nProceedings of the 40th International Conference on Machine Learning (Honolulu, HI, USA) (Proceedings of Machine\nLearning Research, Vol. 202) . PMLR, 8469‚Äì8488.\n[56] Yilun Du, Shuang Li, Antonio Torralba, Joshua B. Tenenbaum, and Igor Mordatch. 2024. Improving Factuality and\nReasoning in Language Models through Multiagent Debate. In Proceedings of the 41st International Conference on\nMachine Learning (Vienna, Austria) (Proceedings of Machine Learning Research, Vol. 235) . PMLR, 11733‚Äì11763.\n[57] L Ekenberg. 2000. The logic of conflicts between decision making agents. Journal of Logic and Computation 10, 4 (08\n2000), 583‚Äì602.\n[58] Mohamed Elaraby, Mengyin Lu, Jacob Dunn, Xueying Zhang, Yu Wang, Shizhu Liu, Pingchuan Tian, Yuping Wang,\nand Yuxuan Wang. 2023. Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language\nModels. arXiv preprint arXiv:2308.11764 (2023).\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:45\n[59] Nidula Elgiriyewithana. 2023. Global Country Information Dataset 2023. Kaggle (2023). https://www.kaggle.com/\ndatasets/nelgiriyewithana/countries-of-the-world-2023\n[60] Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang,\nYuke Zhu, and Anima Anandkumar. 2022. MineDojo: Building Open-Ended Embodied Agents with Internet-Scale\nKnowledge. In Proceedings of the 2022 Conference on Neural Information Processing Systems (New Orleans, LA, USA).\nCurran Associates, Inc., 18343‚Äì18362.\n[61] Simone Formentin, Klaske van Heusden, and Alireza Karimi. 2013. Model-based and data-driven model-reference\ncontrol: A comparative analysis. In Proceedings of the 2013 European Control Conference (ECC) (Zurich, Switzerland).\nInstitute of Electrical and Electronics Engineers, 1410‚Äì1415.\n[62] Wikimedia Foundation. [n. d.]. Wikimedia Downloads . https://dumps.wikimedia.org\n[63] Markus Freitag and Yaser Al-Onaizan. 2017. Beam Search Strategies for Neural Machine Translation. In Proceedings\nof the 1st Workshop on Neural Machine Translation (Vancouver, Canada). Association for Computational Linguistics,\n56‚Äì60.\n[64] Daocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, and Yu Qiao. 2024. Drive Like a Human:\nRethinking Autonomous Driving with Large Language Models. In Proceedings of the 2024 IEEE/CVF Winter Conference\non Applications of Computer Vision Workshops (WACVW) (Waikola, HI, USA). Institute of Electrical and Electronics\nEngineers, 910‚Äì919.\n[65] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2024. GPTScore: Evaluate as You Desire. In Proceedings of\nthe 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies (Mexico City, Mexico). Association for Computational Linguistics, 6556‚Äì6576.\n[66] Jack Gallifant, Amelia Fiske, Yulia A. Levites Strekalova, Juan S. Osorio-Valencia, Rachael Parke, Rogers Mwavu,\nNicole Martinez, Judy Wawira Gichoya, Marzyeh Ghassemi, Dina Demner-Fushman, et al. 2024. Peer review of GPT-4\ntechnical report and systems card. PLOS Digital Health 3, 1 (01 2024), 1‚Äì15.\n[67] Andreas Geiger, Philip Lenz, and Raquel Urtasun. 2012. Are we ready for autonomous driving? The KITTI vision\nbenchmark suite. In Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)\n(Providence, RI, USA). Institute of Electrical and Electronics Engineers, 3354‚Äì3361.\n[68] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle Use a\nLaptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Transactions of the Association for\nComputational Linguistics 9 (04 2021), 346‚Äì361.\n[69] Naresh Gupta and Dana S. Nau. 1992. On the complexity of blocks-world planning. Artificial Intelligence 56, 2 (1992),\n223‚Äì254.\n[70] Junlin Han, Filippos Kokkinos, and Philip Torr. 2025. VFusion3D: Learning Scalable 3D Generative Models from\nVideo Diffusion Models. In Proceedings of the 18th European Conference on Computer Vision (ECCV) (Milan, Italy).\nSpringer Nature Switzerland, 333‚Äì350.\n[71] Michael Hanna and Ond≈ôej Bojar. 2021. A Fine-Grained Analysis of BERTScore. In Proceedings of the Sixth Conference\non Machine Translation (Virtual). Association for Computational Linguistics, Online, 507‚Äì517.\n[72] Frederick Hayes-Roth. 1985. Rule-based systems. Commun. ACM 28, 9 (sep 1985), 921‚Äì932.\n[73] Rishi Hazra, Pedro Zuidberg Dos Martires, and Luc De Raedt. 2024. SayCanPay: Heuristic Planning with Large\nLanguage Models Using Learnable Domain Knowledge. In Proceedings of the 38th AAAI Conference on Artificial\nIntelligence (Vancouver, Canada), Vol. 38. AAAI Press, 20123‚Äì20133.\n[74] Haoran He, Peilin Wu, Chenjia Bai, Hang Lai, Lingxiao Wang, Ling Pan, Xiaolin Hu, and Weinan Zhang. 2024.\nBridging the Sim-to-Real Gap from the Information Bottleneck Perspective. In 8th Annual Conference on Robot\nLearning (Munich, Germany).\n[75] Peter Henderson, Riashat Islam, Philip Bachman, Joelle Pineau, Doina Precup, and David Meger. 2018. Deep\nReinforcement Learning That Matters. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (New\nOrleans, LA, USA). AAAI Press, Article 392, 8 pages.\n[76] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021.\nMeasuring Massive Multitask Language Understanding. In Proceedings of the 9th International Conference on Learning\nRepresentations (Virtual).\n[77] Karl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa Suleyman, and Phil\nBlunsom. 2015. Teaching Machines to Read and Comprehend. In Proceedings of the 2015 Conference on Neural\nInformation Processing Systems (Montreal, Canada). Curran Associates, Inc.\n[78] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: A Reference-free\nEvaluation Metric for Image Captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural\nLanguage Processing (Punta Cana, Dominican Republic). Association for Computational Linguistics, 7514‚Äì7528.\n[79] Xia Hu, Lingyang Chu, Jian Pei, Weiqing Liu, and Jiang Bian. 2021. Model complexity of deep learning: a survey.\nKnowledge and Information Systems 63, 10 (Oct. 2021), 2585‚Äì2619.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:46 Chakraborty et al.\n[80] Zichao Hu, Francesca Lucchetti, Claire Schlesinger, Yash Saxena, Anders Freeman, Sadanand Modak, Arjun Guha,\nand Joydeep Biswas. 2024. Deploying and Evaluating LLMs to Program Service Mobile Robots. IEEE Robotics and\nAutomation Letters 9, 3 (2024), 2853‚Äì2860.\n[81] Lei Huang, Weijiang Yu, Weitao Ma, Weihong Zhong, Zhangyin Feng, Haotian Wang, Qianglong Chen, Weihua Peng,\nXiaocheng Feng, Bing Qin, et al. 2023. A Survey on Hallucination in Large Language Models: Principles, Taxonomy,\nChallenges, and Open Questions. arXiv preprint arXiv:2311.05232 (2023).\n[82] Qidong Huang, Xiaoyi Dong, Pan Zhang, Bin Wang, Conghui He, Jiaqi Wang, Dahua Lin, Weiming Zhang, and Nenghai\nYu. 2024. OPERA: Alleviating Hallucination in Multi-Modal Large Language Models via Over-Trust Penalty and\nRetrospection-Allocation. In Proceedings of the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) (Seattle, WA, USA). Institute of Electrical and Electronics Engineers, 13418‚Äì13427.\n[83] Wen Huang, Hongbin Liu, Minxin Guo, and Neil Zhenqiang Gong. 2024. Visual Hallucinations of Multi-modal Large\nLanguage Models. arXiv preprint arXiv:2402.14683 (2024).\n[84] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and\nEnhong Chen. 2024. Understanding the planning of LLM agents: A survey. arXiv preprint arXiv:2402.02716 (2024).\n[85] Drew A. Hudson and Christopher D. Manning. 2019. GQA: A New Dataset for Real-World Visual Reasoning and\nCompositional Question Answering. In Proceedings of the 2019 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) (Long Beach, CA, USA). Institute of Electrical and Electronics Engineers, 6693‚Äì6702.\n[86] Brian Ichter, Anthony Brohan, Yevgen Chebotar, Chelsea Finn, Karol Hausman, Alexander Herzog, Daniel Ho, Julian\nIbarz, Alex Irpan, Eric Jang, et al. 2023. Do As I Can, Not As I Say: Grounding Language in Robotic Affordances. In\nProceedings of The 6th Conference on Robot Learning (Atlanta, GA, USA) (Proceedings of Machine Learning Research,\nVol. 205). PMLR, 287‚Äì318.\n[87] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter Abbeel, and Ben Poole. 2022. Zero-Shot Text-Guided Object\nGeneration with Dream Fields. In Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR) (New Orleans, LA, USA). Institute of Electrical and Electronics Engineers, 857‚Äì866.\n[88] Joel Janai, Fatma G√ºney, Aseem Behl, and Andreas Geiger. 2020. Computer Vision for Autonomous Vehicles: Problems,\nDatasets and State of the Art. Foundations and Trends in Computer Graphics and Vision 12, 1-3 (2020), 1‚Äì308.\n[89] Sumit Kumar Jha, Susmit Jha, Patrick Lincoln, Nathaniel D. Bastian, Alvaro Velasquez, Rickard Ewetz, and Sandeep\nNeema. 2023. Counterexample Guided Inductive Synthesis Using Large Language Models and Satisfiability Solving. In\nProceedings of the 2023 IEEE Military Communications Conference (MILCOM) (Boston, MA, USA). Institute of Electrical\nand Electronics Engineers, 944‚Äì949.\n[90] Ziwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu, Dan Su, Yan Xu, Etsuko Ishii, Ye Jin Bang, Andrea Madotto, and\nPascale Fung. 2023. Survey of Hallucination in Natural Language Generation. Comput. Surveys 55, 12, Article 248\n(mar 2023), 38 pages.\n[91] Alistair EW Johnson, Tom J Pollard, Seth J Berkowitz, Nathaniel R Greenbaum, Matthew P Lungren, Chih-ying Deng,\nRoger G Mark, and Steven Horng. 2019. MIMIC-CXR, a de-identified publicly available database of chest radiographs\nwith free-text reports. Scientific Data 6, 1 (2019), 317.\n[92] Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. 2017. TriviaQA: A Large Scale Distantly Supervised\nChallenge Dataset for Reading Comprehension. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Vancouver, Canada). Association for Computational Linguistics, 1601‚Äì1611.\n[93] Haoqiang Kang and Xiao-Yang Liu. 2024. Deficiency of Large Language Models in Finance: An Empirical Examination\nof Hallucination. In I Can‚Äôt Believe It‚Äôs Not Better Workshop: Failure Modes in the Age of Foundation Models .\n[94] Jungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi, Ronan Le Bras, Akari Asai, Xinyan Velocity Yu, Dragomir Radev,\nNoah A. Smith, Yejin Choi, and Kentaro Inui. 2023. RealTime QA: What‚Äôs the Answer Right Now?. InProceedings\nof the 37th Conference on Neural Information Processing Systems Datasets and Benchmarks Track (New Orleans, LA,\nUSA).\n[95] Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and Christopher Kanan. 2018. Measuring Catastrophic\nForgetting in Neural Networks. In Proceedings of the 32nd AAAI Conference on Artificial Intelligence (New Orleans, LA,\nUSA). AAAI Press, 3390‚Äì3398.\n[96] Ali Keysan, Andreas Look, Eitan Kosman, Gonca G√ºrsun, J√∂rg Wagner, Yu Yao, and Barbara Rakitsch. 2023. Can you\ntext what is happening? Integrating pre-trained language encoders into trajectory prediction models for autonomous\ndriving. arXiv preprint arXiv:2309.05282 (2023).\n[97] Jinkyu Kim, Anna Rohrbach, Trevor Darrell, John Canny, and Zeynep Akata. 2018. Textual Explanations for Self-\nDriving Vehicles. In Proceedings of the 15th European Conference on Computer Vision (ECCV) (Munich, Germany).\nSpringer International Publishing, 577‚Äì593.\n[98] Seokhwan Kim, Spandana Gella, Chao Zhao, Di Jin, Alexandros Papangelis, Behnam Hedayatnia, Yang Liu, and Dilek\nZ Hakkani-Tur. 2023. Task-Oriented Conversational Modeling with Subjective Knowledge Track in DSTC11. In Pro-\nceedings of The Eleventh Dialog System Technology Challenge (Prague, Czech Republic). Association for Computational\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:47\nLinguistics, 274‚Äì281.\n[99] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer\nWhitehead, Alexander C. Berg, Wan-Yen Lo, et al. 2023. Segment Anything. In Proceedings of the 2023 IEEE/CVF\nInternational Conference on Computer Vision (ICCV) (Paris, France). Institute of Electrical and Electronics Engineers,\n3992‚Äì4003.\n[100] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis,\nLi-Jia Li, David A Shamma, et al. 2017. Visual Genome: Connecting Language and Vision Using Crowdsourced Dense\nImage Annotations. International Journal of Computer Vision 123 (2017), 32‚Äì73.\n[101] Lorenz Kuhn, Yarin Gal, and Sebastian Farquhar. 2023. Semantic Uncertainty: Linguistic Invariances for Uncer-\ntainty Estimation in Natural Language Generation. In Proceedings of the 11th International Conference on Learning\nRepresentations (Kigali, Rwanda).\n[102] Bhawesh Kumar, Charlie Lu, Gauri Gupta, Anil Palepu, David Bellamy, Ramesh Raskar, and Andrew Beam. 2023.\nConformal Prediction with Large Language Models for Multi-Choice Question Answering. InProceedings of the ‚ÄòNeural\nConversational AI Workshop - What‚Äôs left to TEACH (Trustworthy, Enhanced, Adaptable, Capable and Human-centric)\nchatbots?‚Äô at the 40th International Conference on Machine Learning (Honolulu, HI, USA).\n[103] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, Chris Alberti, Danielle\nEpstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al . 2019. Natural Questions: A Benchmark for Question\nAnswering Research. Transactions of the Association for Computational Linguistics 7 (2019), 452‚Äì466.\n[104] Minae Kwon, Sang Michael Xie, Kalesha Bullard, and Dorsa Sadigh. 2023. Reward Design with Language Models. In\nProceedings of the The 11th International Conference on Learning Representations (Kigali, Rwanda).\n[105] Philippe Laban, Tobias Schnabel, Paul N. Bennett, and Marti A. Hearst. 2022. SummaC: Re-Visiting NLI-based Models\nfor Inconsistency Detection in Summarization. Transactions of the Association for Computational Linguistics 10 (2022),\n163‚Äì177.\n[106] Jinqi Lai, Wensheng Gan, Jiayang Wu, Zhenlian Qi, and Philip S Yu. 2023. Large Language Models in Law: A Survey.\narXiv preprint arXiv:2312.03718 (2023).\n[107] R√©mi Lebret, David Grangier, and Michael Auli. 2016. Neural Text Generation from Structured Data with Application\nto the Biography Domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing .\nAssociation for Computational Linguistics, Austin, TX, USA, 1203‚Äì1213.\n[108] Ariel N Lee, Cole J Hunter, and Nataniel Ruiz. 2023. Platypus: Quick, Cheap, and Powerful Refinement of LLMs.arXiv\npreprint arXiv:2308.07317 (2023).\n[109] Yoonho Lee, Juho Lee, Sung Ju Hwang, Eunho Yang, and Seungjin Choi. 2020. Neural Complexity Measures. In\nProceedings of the 2020 Conference on Neural Information Processing Systems (Virtual). Curran Associates, Inc., 9713‚Äì\n9724.\n[110] Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N. Mendes, Sebastian Hellmann,\nMohamed Morsey, Patrick van Kleef, S√∂ren Auer, and Christian Bizer. 2015. DBpedia ‚Äì A Large-scale, Multilingual\nKnowledge Base Extracted from Wikipedia. Semantic Web 6 (2015), 167‚Äì195. 2.\n[111] Edouard Leurent. 2018. An Environment for Autonomous Driving Decision-Making. https://github.com/eleurent/\nhighway-env.\n[112] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov,\nand Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation,\nTranslation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational\nLinguistics (Virtual). Association for Computational Linguistics, 7871‚Äì7880.\n[113] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler,\nMike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. 2020. Retrieval-Augmented Generation for Knowledge-Intensive\nNLP Tasks. InProceedings of the 2020 Conference on Neural Information Processing Systems (Virtual). Curran Associates,\nInc., 9459‚Äì9474.\n[114] Chenghao Li, Chaoning Zhang, Joseph Cho, Atish Waghwase, Lik-Hang Lee, Francois Rameau, Yang Yang, Sung-Ho\nBae, and Choong Seon Hong. 2023. Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era. arXiv preprint\narXiv:2305.06131 (2023).\n[115] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar.\n2023. Large Language Models with Controllable Working Memory. In Findings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Toronto, Canada). Association for Computational Linguistics, 1774‚Äì1793.\n[116] Haonan Li, Martin Tomko, Maria Vasardani, and Timothy Baldwin. 2022. MultiSpanQA: A Dataset for Multi-\nSpan Question Answering. In Proceedings of the 2022 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies (Seattle, WA, USA). Association for Computational\nLinguistics, 1250‚Äì1260.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:48 Chakraborty et al.\n[117] Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluEval: A Large-Scale Hallucination\nEvaluation Benchmark for Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing (Singapore). Association for Computational Linguistics, 6449‚Äì6464.\n[118] Xingxuan Li, Ruochen Zhao, Yew Ken Chia, Bosheng Ding, Shafiq Joty, Soujanya Poria, and Lidong Bing. 2024.\nChain-of-Knowledge: Grounding Large Language Models via Dynamic Knowledge Adapting over Heterogeneous\nSources. In Proceedings of the 12th International Conference on Learning Representations (Vienna, Austria).\n[119] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Xin Zhao, and Ji-Rong Wen. 2023. Evaluating Object Hallucination\nin Large Vision-Language Models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing (Singapore). Association for Computational Linguistics, 292‚Äì305.\n[120] Yinheng Li, Shaofei Wang, Han Ding, and Hang Chen. 2023. Large Language Models in Finance: A Survey. In\nProceedings of the Fourth ACM International Conference on AI in Finance (Brooklyn, NY, USA). Association for\nComputing Machinery, New York, NY, USA, 374‚Äì382.\n[121] Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. 2023.\nCode as Policies: Language Model Programs for Embodied Control. In Proceedings of the 2023 IEEE International\nConference on Robotics and Automation (ICRA) (London, United Kingdom). Institute of Electrical and Electronics\nEngineers, 9493‚Äì9500.\n[122] Kaiqu Liang, Zixu Zhang, and Jaime Fern√°ndez Fisac. 2024. Introspective Planning: Guiding Language-Enabled\nAgents to Refine Their Own Uncertainty. arXiv preprint arXiv:2402.06529 (2024).\n[123] Youngsun Lim and Hyunjung Shim. 2024. Addressing Image Hallucination in Text-to-Image Generation through\nFactual Image Retrieval. arXiv preprint arXiv:2407.10683 (2024).\n[124] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. Teaching Models to Express Their Uncertainty in Words.\nTransactions on Machine Learning Research (2022).\n[125] Stephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods.\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Dublin, Ireland). Association\nfor Computational Linguistics, 3214‚Äì3252.\n[126] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll√°r, and C. Lawrence\nZitnick. 2014. Microsoft COCO: Common Objects in Context. In Proceedings of the 13th European Conference on\nComputer Vision (ECCV) (Zurich, Switzerland). Springer International Publishing, 740‚Äì755.\n[127] Hanchao Liu, Wenyuan Xue, Yifei Chen, Dapeng Chen, Xiutian Zhao, Ke Wang, Liping Hou, Rongjun Li, and Wei\nPeng. 2024. A survey on hallucination in large vision-language models. arXiv preprint arXiv:2402.00253 (2024).\n[128] Jiaqi Liu, Peng Hang, Xiao Qi, Jianqiang Wang, and Jian Sun. 2023. MTD-GPT: A Multi-Task Decision-Making\nGPT Model for Autonomous Driving at Unsignalized Intersections. In Proceedings of the 2023 IEEE International\nConference on Intelligent Transportation Systems (ITSC) (Bilbao, Spain). Institute of Electrical and Electronics Engineers,\n5154‚Äì5161.\n[129] Jian Liu, Xiaoshui Huang, Tianyu Huang, Lu Chen, Yuenan Hou, Shixiang Tang, Ziwei Liu, Wanli Ouyang, Wangmeng\nZuo, Junjun Jiang, and Xianming Liu. 2024. A Comprehensive Survey on 3D Content Generation. arXiv preprint\narXiv:2402.01166 (2024).\n[130] Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu Chen, and Bill Dolan. 2022. A Token-level\nReference-free Hallucination Detection Benchmark for Free-form Text Generation. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Linguistics . Association for Computational Linguistics, Dublin, Ireland,\n6723‚Äì6737.\n[131] Pablo Alvarez Lopez, Michael Behrisch, Laura Bieker-Walz, Jakob Erdmann, Yun-Pang Fl√∂tter√∂d, Robert Hilbrich,\nLeonhard L√ºcken, Johannes Rummel, Peter Wagner, and Evamarie Wiessner. 2018. Microscopic Traffic Simulation\nusing SUMO. In Proceedings of the 2018 International Conference on Intelligent Transportation Systems (ITSC) (Maui,\nHI, USA). Institute of Electrical and Electronics Engineers, 2575‚Äì2582.\n[132] Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and\nAshwin Kalyan. 2022. Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering.\nIn Proceedings of the 2022 Conference on Neural Information Processing Systems (New Orleans, LA, USA). Curran\nAssociates, Inc., 2507‚Äì2521.\n[133] Corey Lynch, Ayzaan Wahid, Jonathan Tompson, Tianli Ding, James Betker, Robert Baruch, Travis Armstrong, and\nPete Florence. 2023. Interactive Language: Talking to Robots in Real Time. IEEE Robotics and Automation Letters\n(2023).\n[134] Chaitanya Malaviya, Peter Shaw, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2023. QUEST: A Retrieval\nDataset of Entity-Seeking Queries with Implicit Set Operations. In Proceedings of the 61st Annual Meeting of the\nAssociation for Computational Linguistics (Toronto, Canada). Association for Computational Linguistics, 14032‚Äì14047.\n[135] Srikanth Malla, Chiho Choi, Isht Dwivedi, Joon Hee Choi, and Jiachen Li. 2023. DRAMA: Joint Risk Localization\nand Captioning in Driving. In Proceedings of the 2023 IEEE/CVF Winter Conference on Applications of Computer Vision\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:49\n(WACV) (Waikola, HI, USA). Institute of Electrical and Electronics Engineers, 1043‚Äì1052.\n[136] Potsawee Manakul, Adian Liusie, and Mark Gales. 2023. SelfCheckGPT: Zero-Resource Black-Box Hallucination\nDetection for Generative Large Language Models. In Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing (Singapore). Association for Computational Linguistics, 9004‚Äì9017.\n[137] Jiageng Mao, Junjie Ye, Yuxi Qian, Marco Pavone, and Yue Wang. 2024. A Language Agent for Autonomous Driving.\nIn Proceedings of the First Conference on Language Modeling .\n[138] Ninareh Mehrabi, Palash Goyal, Christophe Dupuy, Qian Hu, Shalini Ghosh, Richard Zemel, Kai-Wei Chang, Aram\nGalstyan, and Rahul Gupta. 2023. FLIRT: Feedback Loop In-context Red Teaming. arXiv preprint arXiv:2308.04265\n(2023).\n[139] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and Editing Factual Associations in\nGPT. In Proceedings of the 2022 Conference on Neural Information Processing Systems (New Orleans, LA, USA), Vol. 35.\nCurran Associates, Inc., 17359‚Äì17372.\n[140] Meta. 2024. Model Information. GitHub (2024). https://github.com/meta-llama/llama-models/blob/main/models/\nllama3_1/MODEL_CARD.md\n[141] Gr√©goire Mialon, Roberto Dessi, Maria Lomeli, Christoforos Nalmpantis, Ramakanth Pasunuru, Roberta Raileanu,\nBaptiste Roziere, Timo Schick, Jane Dwivedi-Yu, Asli Celikyilmaz, et al. 2023. Augmented Language Models: a Survey.\nTransactions on Machine Learning Research (2023). Survey Certification.\n[142] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and\nHannaneh Hajishirzi. 2023. FActScore: Fine-grained Atomic Evaluation of Factual Precision in Long Form Text\nGeneration. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (Singapore).\nAssociation for Computational Linguistics, 12076‚Äì12100.\n[143] Pamela Mishkin, Lama Ahmad, Miles Brundage, Gretchen Krueger, and Girish Sastry. 2022. DALL¬∑E 2 Preview - Risks\nand Limitations. GitHub (2022). https://github.com/openai/dalle-2-preview/blob/main/system-card.md\n[144] Niels M√ºndler, Jingxuan He, Slobodan Jenko, and Martin Vechev. 2024. Self-contradictory Hallucinations of Large\nLanguage Models: Evaluation, Detection and Mitigation. InProceedings of the 12th International Conference on Learning\nRepresentations (Vienna, Austria).\n[145] Naila Murray, Luca Marchesotti, and Florent Perronnin. 2012. AVA: A Large-Scale Database for Aesthetic Visual\nAnalysis. In Proceedings of the 2012 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) (Providence,\nRI, USA). Institute of Electrical and Electronics Engineers, 2408‚Äì2415.\n[146] Jesutofunmi A. Omiye, Haiwen Gui, Shawheen J. Rezaei, James Zou, and Roxana Daneshjou. 2024. Large Language\nModels in Medicine: The Potentials and Pitfalls. Annals of Internal Medicine 177, 2 (20 Feb 2024), 210‚Äì220.\n[147] OpenAI. 2022. Introducing ChatGPT. OpenAI blog (2022). https://openai.com/blog/chatgpt\n[148] OpenAI. 2023. DALL¬∑E 3 System Card. OpenAI blog (2023). https://cdn.openai.com/papers/DALL_E_3_System_Card.\npdf\n[149] OpenAI. 2024. Video generation models as world simulators. OpenAI blog (2024). https://openai.com/index/video-\ngeneration-models-as-world-simulators/\n[150] Maxime Oquab, Timoth√©e Darcet, Th√©o Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez,\nDaniel HAZIZA, Francisco Massa, Alaaeldin El-Nouby, et al. 2024. DINOv2: Learning Robust Visual Features without\nSupervision. Transactions on Machine Learning Research (2024).\n[151] Gerhard Paa√ü and Sven Giesselbach. 2023. Foundation Models for Speech, Images, Videos, and Control . Springer\nInternational Publishing, Cham, 313‚Äì382.\n[152] Lorenzo Pacchiardi, Alex James Chan, S√∂ren Mindermann, Ilan Moscovitz, Alexa Yue Pan, Yarin Gal, Owain Evans,\nand Jan M. Brauner. 2024. How to Catch an AI Liar: Lie Detection in Black-Box LLMs by Asking Unrelated Questions.\nIn Proceedings of the 12th International Conference on Learning Representations (Vienna, Austria).\n[153] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a Method for Automatic Evaluation\nof Machine Translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics\n(Philadelphia, PA, USA). Association for Computational Linguistics, 311‚Äì318.\n[154] Jeongeun Park, Seungwon Lim, Joonhyung Lee, Sangbeom Park, Minsuk Chang, Youngjae Yu, and Sungjoon Choi.\n2024. CLARA: Classifying and Disambiguating User Commands for Reliable Interactive Robotic Agents. IEEE Robotics\nand Automation Letters 9, 2 (2024), 1059‚Äì1066.\n[155] Joon Sung Park, Joseph O‚ÄôBrien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023.\nGenerative Agents: Interactive Simulacra of Human Behavior. In Proceedings of the 36th Annual ACM Symposium on\nUser Interface Software and Technology (San Francisco, CA, USA). Association for Computing Machinery, New York,\nNY, USA, Article 2, 22 pages.\n[156] Arkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are NLP Models really able to Solve Simple Math Word\nProblems?. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies (Virtual). Association for Computational Linguistics, 2080‚Äì2094.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:50 Chakraborty et al.\n[157] Baolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu,\nWeizhu Chen, and Jianfeng Gao. 2023. Check Your Facts and Try Again: Improving Large Language Models with\nExternal Knowledge and Automated Feedback. arXiv preprint arXiv:2302.12813 (2023).\n[158] Ethan Perez, Sam Ringer, Kamile Lukosiute, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine\nOlsson, Sandipan Kundu, Saurav Kadavath, et al. 2023. Discovering Language Model Behaviors with Model-Written\nEvaluations. In Findings of the 61st Annual Meeting of the Association for Computational Linguistics (Toronto, Canada).\nAssociation for Computational Linguistics, 13387‚Äì13434.\n[159] Tomaso Poggio, Andrzej Banburski, and Qianli Liao. 2020. Theoretical issues in deep networks. Proceedings of the\nNational Academy of Sciences 117, 48 (2020), 30039‚Äì30045.\n[160] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. 2023. DreamFusion: Text-to-3D using 2D Diffusion. In\nProceedings of the 11th International Conference on Learning Representations (Kigali, Rwanda).\n[161] Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja Fidler, and Antonio Torralba. 2018. VirtualHome:\nSimulating Household Activities Via Programs. In Proceedings of the 2018 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) (Salt Lake City, UT, USA). Institute of Electrical and Electronics Engineers, 8494‚Äì8502.\n[162] Gokul Puthumanaillam, Manav Vora, Pranay Thangeda, and Melkior Ornik. 2024. A Moral Imperative: The Need for\nContinual Superalignment of Large Language Models. arXiv preprint arXiv:2403.14683 (2024).\n[163] Tianwen Qian, Jingjing Chen, Linhai Zhuo, Yang Jiao, and Yu-Gang Jiang. 2024. NuScenes-QA: A Multi-Modal Visual\nQuestion Answering Benchmark for Autonomous Driving Scenario. In Proceedings of the 38th AAAI Conference on\nArtificial Intelligence (Vancouver, Canada), Vol. 38. AAAI Press, 4542‚Äì4550.\n[164] Huachuan Qiu, Shuai Zhang, Anqi Li, Hongliang He, and Zhenzhong Lan. 2023. Latent Jailbreak: A Benchmark for\nEvaluating Text Safety and Output Robustness of Large Language Models. arXiv preprint arXiv:2307.08487 (2023).\n[165] Victor Quach, Adam Fisch, Tal Schuster, Adam Yala, Jae Ho Sohn, Tommi S. Jaakkola, and Regina Barzilay. 2024.\nConformal Language Modeling. InProceedings of the 12th International Conference on Learning Representations (Vienna,\nAustria).\n[166] Joaquin Qui√±onero-Candela, Masashi Sugiyama, Anton Schwaighofer, and Neil D. Lawrence. 2008. Dataset Shift in\nMachine Learning . The MIT Press.\n[167] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda\nAskell, Pamela Mishkin, Jack Clark, et al . 2021. Learning Transferable Visual Models From Natural Language\nSupervision. In Proceedings of the 38th International Conference on Machine Learning (Virtual) (Proceedings of Machine\nLearning Research, Vol. 139) . PMLR, 8748‚Äì8763.\n[168] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are\nunsupervised multitask learners. OpenAI blog (2019). https://openai.com/research/better-language-models\n[169] Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. SQuAD: 100,000+ Questions for Machine\nComprehension of Text. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing\n(Austin, TX, USA). Association for Computational Linguistics, 2383‚Äì2392.\n[170] Anil Ramakrishna, Rahul Gupta, Jens Lehmann, and Morteza Ziyadi. 2023. INVITE: a Testbed of Automatically\nGenerated Invalid Questions to Evaluate Large Language Models for Hallucinations. InFindings of the 2023 Conference\non Empirical Methods in Natural Language Processing (Singapore). Association for Computational Linguistics, 5422‚Äì\n5429.\n[171] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical Text-Conditional\nImage Generation with CLIP Latents. arXiv preprint arXiv:2204.06125 (2022).\n[172] Javier Rando, Daniel Paleka, David Lindner, Lennart Heim, and Florian Tramer. 2022. Red-Teaming the Stable\nDiffusion Safety Filter. In NeurIPS ML Safety Workshop .\n[173] Vipula Rawte, Amit Sheth, and Amitava Das. 2023. A Survey of Hallucination in Large Foundation Models. arXiv\npreprint arXiv:2309.05922 (2023).\n[174] Allen Z. Ren, Anushri Dixit, Alexandra Bodrova, Sumeet Singh, Stephen Tu, Noah Brown, Peng Xu, Leila Takayama,\nFei Xia, Jake Varley, et al. 2023. Robots That Ask For Help: Uncertainty Alignment for Large Language Model Planners.\nIn Proceedings of The 7th Conference on Robot Learning (Atlanta, GA, USA) (Proceedings of Machine Learning Research,\nVol. 229). PMLR, 661‚Äì682.\n[175] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018. Object Hallucination\nin Image Captioning. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing\n(Brussels, Belgium). Association for Computational Linguistics, 4035‚Äì4045.\n[176] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. 2022. High-Resolution Image\nSynthesis With Latent Diffusion Models. In Proceedings of the 2022 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR) (New Orleans, LA, USA). Institute of Electrical and Electronics Engineers, 10684‚Äì10695.\n[177] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael\nGontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. 2022. Photorealistic Text-to-Image Diffusion Models with\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:51\nDeep Language Understanding. In Proceedings of the 2022 Conference on Neural Information Processing Systems (New\nOrleans, LA, USA). Curran Associates, Inc., 36479‚Äì36494.\n[178] Pranab Sahoo, Prabhash Meharia, Akash Ghosh, Sriparna Saha, Vinija Jain, and Aman Chadha. 2024. A Comprehensive\nSurvey of Hallucination in Large Language, Image, Video and Audio Foundation Models. In Findings of the 2024\nConference on Empirical Methods in Natural Language Processing (Miami, FL, USA). Association for Computational\nLinguistics, 11709‚Äì11724.\n[179] Erica Salvato, Gianfranco Fenu, Eric Medvet, and Felice Andrea Pellegrino. 2021. Crossing the Reality Gap: A Survey\non Sim-to-Real Transferability of Robot Controllers in Reinforcement Learning. IEEE Access 9 (2021), 153171‚Äì153187.\n[180] Aditya Sanghi, Hang Chu, Joseph G. Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi\nMalekshan. 2022. CLIP-Forge: Towards Zero-Shot Text-to-Shape Generation. In Proceedings of the 2022 IEEE/CVF\nConference on Computer Vision and Pattern Recognition (CVPR) (New Orleans, LA, USA). Institute of Electrical and\nElectronics Engineers, 18582‚Äì18592.\n[181] Bhaskarjit Sarmah, Dhagash Mehta, Stefano Pasquali, and Tianjie Zhu. 2024. Towards reducing hallucination in\nextracting information from financial reports using Large Language Models. In Proceedings of the Third International\nConference on AI-ML Systems (Bangalore, India). Association for Computing Machinery, New York, NY, USA, Article\n39, 5 pages.\n[182] Patrick Schramowski, Manuel Brack, Bj√∂rn Deiseroth, and Kristian Kersting. 2023. Safe Latent Diffusion: Mitigating\nInappropriate Degeneration in Diffusion Models. In Proceedings of the 2023 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR) (Vancouver, Canada). Institute of Electrical and Electronics Engineers, 22522‚Äì22531.\n[183] Andre Schreiber, Tianchen Ji, D. Livingston McPherson, and Katherine Driggs-Campbell. 2023. An Attentional\nRecurrent Neural Network for Occlusion-Aware Proactive Anomaly Detection in Field Robot Navigation. InProceedings\nof the 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (Detroit, MI, USA). Institute of\nElectrical and Electronics Engineers, 8038‚Äì8045.\n[184] Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo\nCoombes, Jenia Jitsev, and Aran Komatsuzaki. 2021. LAION-400M: Open Dataset of CLIP-Filtered 400 Million\nImage-Text Pairs. arXiv preprint arXiv:2111.02114 (2021).\n[185] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. 2022. A-OKVQA:\nA Benchmark for Visual Question Answering Using World Knowledge. InProceedings of the 17th European Conference\non Computer Vision (ECCV) (Tel Aviv, Israel). Springer Nature Switzerland, 146‚Äì162.\n[186] Glenn Shafer and Vladimir Vovk. 2008. A Tutorial on Conformal Prediction. Journal of Machine Learning Research 9,\n12 (2008), 371‚Äì421.\n[187] Dhruv Shah, B≈Ça≈ºej Osi≈Ñski, Brian Ichter, and Sergey Levine. 2023. LM-Nav: Robotic Navigation with Large Pre-\nTrained Models of Language, Vision, and Action. In Proceedings of The 6th Conference on Robot Learning (Atlanta, GA,\nUSA) (Proceedings of Machine Learning Research, Vol. 205) . PMLR, 492‚Äì504.\n[188] Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: language\nagents with verbal reinforcement learning. In Proceedings of the 2023 Conference on Neural Information Processing\nSystems (New Orleans, LA, USA). Curran Associates, Inc., 8634‚Äì8652.\n[189] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and\nDieter Fox. 2020. ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks. In Proceedings\nof the 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (Virtual). Institute of Electrical\nand Electronics Engineers, 10737‚Äì10746.\n[190] Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Cote, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. 2021.\nALFWorld: Aligning Text and Embodied Environments for Interactive Learning. InProceedings of the 9th International\nConference on Learning Representations (Virtual).\n[191] Chonghao Sima, Katrin Renz, Kashyap Chitta, Li Chen, Hanxue Zhang, Chengen Xie, Ping Luo, Andreas Geiger,\nand Hongyang Li. 2023. DriveLM: Driving with Graph Visual Question Answering. arXiv preprint arXiv:2312.14150\n(2023).\n[192] Gonen Singer and Yuval Cohen. 2021. A framework for smart control using machine-learning modeling for processes\nwith closed-loop control in Industry 4.0. Engineering Applications of Artificial Intelligence 102 (2021), 104236.\n[193] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason,\nand Animesh Garg. 2023. ProgPrompt: Generating Situated Robot Task Plans using Large Language Models. In\nProceedings of the 2023 IEEE International Conference on Robotics and Automation (ICRA) (London, United Kingdom).\nInstitute of Electrical and Electronics Engineers, 11523‚Äì11530.\n[194] Da Song, Xuan Xie, Jiayang Song, Derui Zhu, Yuheng Huang, Felix Juefei-Xu, and Lei Ma. 2024. LUNA: A Model-Based\nUniversal Analysis Framework for Large Language Models. IEEE Transactions on Software Engineering 50, 7 (2024),\n1921‚Äì1948.\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:52 Chakraborty et al.\n[195] Mohsen Soori, Behrooz Arezoo, and Roza Dastres. 2023. Artificial intelligence, machine learning and deep learning in\nadvanced robotics, a review. Cognitive Robotics 3 (2023), 54‚Äì70.\n[196] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R.\nBrown, Adam Santoro, Aditya Gupta, Adri√† Garriga-Alonso, et al. 2023. Beyond the Imitation Game: Quantifying and\nextrapolating the capabilities of language models. Transactions on Machine Learning Research (2023).\n[197] Claire Stremple. 2024. False citations show Alaska education official relied on generative AI, raising broader\nquestions. Alaska Beacon (2024). https://alaskabeacon.com/2024/10/28/alaska-education-department-published-\nfalse-ai-generated-academic-citations-in-cell-policy-document/\n[198] Naoki Suganuma and Keisuke Yoneda. 2022. Current Status and Issues of Traffic Light Recognition Technology in\nAutonomous Driving System. IEICE Transactions on Fundamentals of Electronics, Communications and Computer\nSciences E105.A, 5 (2022), 763‚Äì769.\n[199] Gaurav Suri, Lily R. Slater, Ali Ziaee, and Morgan Nguyen. 2024. Do Large Language Models Show Decision Heuristics\nSimilar to Humans? A Case Study Using GPT-3.5. Journal of Experimental Psychology: General 153, 4 (04 2024),\n1066‚Äì1075.\n[200] Gyana Swain. 2024. Patients may suffer from hallucinations of AI medical transcription tools. CIO (2024). https:\n//www.cio.com/article/3593403/patients-may-suffer-from-hallucinations-of-ai-medical-transcription-tools.html\n[201] Mahsa Taheri, Fang Xie, and Johannes Lederer. 2021. Statistical guarantees for regularized neural networks. Neural\nNetworks 142 (2021), 148‚Äì161.\n[202] Alon Talmor, Ori Yoran, Ronan Le Bras, Chandra Bhagavatula, Yoav Goldberg, Yejin Choi, and Jonathan Berant. 2021.\nCommonsenseQA 2.0: Exposing the Limits of AI through Gamification. InProceedings of the 35th Conference on Neural\nInformation Processing Systems Datasets and Benchmarks Track (Virtual), Vol. 1.\n[203] James Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a Large-scale Dataset\nfor Fact Extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies (New Orleans, Louisiana). Association for\nComputational Linguistics, 809‚Äì819.\n[204] J√∂rg Tiedemann. 2012. Parallel Data, Tools and Interfaces in OPUS. InProceedings of the Eighth International Conference\non Language Resources and Evaluation (Istanbul, Turkey). European Language Resources Association, 2214‚Äì2218.\n[205] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha Akula, Jihan Yang,\nShusheng Yang, Adithya Iyer, Xichen Pan, et al. 2024. Cambrian-1: A Fully Open, Vision-Centric Exploration of\nMultimodal LLMs. arXiv preprint arXiv:2406.16860 (2024).\n[206] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth√©e Lacroix, Baptiste\nRozi√®re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. LLaMA: Open and Efficient Foundation Language\nModels. arXiv preprint arXiv:2302.13971 (2023).\n[207] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open Foundation and Fine-Tuned Chat Models.\narXiv preprint arXiv:2307.09288 (2023).\n[208] Priyansh Trivedi, Gaurav Maheshwari, Mohnish Dubey, and Jens Lehmann. 2017. LC-QuAD: A Corpus for Complex\nQuestion Answering over Knowledge Graphs. In Proceedings of the 2017 International Semantic Web Conference\n(Vienna, Austria). Springer International Publishing, 210‚Äì218.\n[209] Cem Uluoglakci and Tugba Temizel. 2024. HypoTermQA: Hypothetical Terms Dataset for Benchmarking Hallucination\nTendency of LLMs. In Proceedings of the Student Research Workshop at the 18th Conference of the European Chapter of\nthe Association for Computational Linguistics (St. Julian‚Äôs, Malta). Association for Computational Linguistics, 95‚Äì136.\n[210] Neeraj Varshney, Wenlin Yao, Hongming Zhang, Jianshu Chen, and Dong Yu. 2023. A Stitch in Time Saves Nine: Detect-\ning and Mitigating Hallucinations of LLMs by Validating Low-Confidence Generation.arXiv preprint arXiv:2307.03987\n(2023).\n[211] Denny Vrandeƒçiƒá and Markus Kr√∂tzsch. 2014. Wikidata: A Free Collaborative Knowledgebase. Commun. ACM 57, 10\n(Sept. 2014), 78‚Äì85.\n[212] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi Xiong, Ritik\nDutta, Rylan Schaeffer, et al. 2023. DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models.\nIn Proceedings of the 37th Conference on Neural Information Processing Systems Datasets and Benchmarks Track (New\nOrleans, LA, USA).\n[213] Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar.\n2024. Voyager: An Open-Ended Embodied Agent with Large Language Models. Transactions on Machine Learning\nResearch (2024).\n[214] Hongbo Wang, Jie Cao, Jin Liu, Xiaoqiang Zhou, Huaibo Huang, and Ran He. 2024. Hallo3D: Multi-Modal Hallucination\nDetection and Mitigation for Consistent 3D Content Generation. In Proceedings of the 2024 Conference on Neural\nInformation Processing Systems .\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:53\n[215] Jun Wang, Jiaming Tong, Kaiyuan Tan, Yevgeniy Vorobeychik, and Yiannis Kantaros. 2024. Conformal Temporal\nLogic Planning using Large Language Models. arXiv preprint arXiv:2309.10092 (2024).\n[216] Axel Wegener, Micha≈Ç Pi√≥rkowski, Maxim Raya, Horst Hellbr√ºck, Stefan Fischer, and Jean-Pierre Hubaux. 2008.\nTraCI: an interface for coupling road traffic and network simulators. In Proceedings of the 11th Communications and\nNetworking Simulation Symposium (Ottawa, Canada). Association for Computing Machinery, New York, NY, USA,\n155‚Äì163.\n[217] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc V Le, and Denny\nZhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. In Proceedings of the 2022\nConference on Neural Information Processing Systems (New Orleans, LA, USA). Curran Associates, Inc., 24824‚Äì24837.\n[218] Benjamin Weiser. 2023. Here‚Äôs What Happens When Your Lawyer Uses ChatGPT. The New York Times (2023).\nhttps://www.nytimes.com/2023/05/27/nyregion/avianca-airline-lawsuit-chatgpt.html\n[219] Johannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing Multiple Choice Science Questions. In\nProceedings of the 3rd Workshop on Noisy User-generated Text . Association for Computational Linguistics, Copenhagen,\nDenmark, 94‚Äì106.\n[220] Licheng Wen, Daocheng Fu, Xin Li, Xinyu Cai, Tao MA, Pinlong Cai, Min Dou, Botian Shi, Liang He, and Yu Qiao.\n2024. DiLu: A Knowledge-Driven Approach to Autonomous Driving with Large Language Models. In Proceedings of\nthe 12th International Conference on Learning Representations (Vienna, Austria).\n[221] Licheng Wen, Xuemeng Yang, Daocheng Fu, Xiaofeng Wang, Pinlong Cai, Xin Li, Tao MA, Yingxuan Li, Linran\nXU, Dengke Shang, et al. 2024. On the Road with GPT-4V(ision): Explorations of Utilizing Visual-Language Model\nas Autonomous Driving Agent. In Proceedings of the Workshop on Large Language Model (LLM) Agents at the 12th\nInternational Conference on Learning Representations (Vienna, Austria).\n[222] Ronald J. Williams. 1992. Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning.\nMachine Learning 8, 3‚Äì4 (may 1992), 229‚Äì256.\n[223] Dongming Wu, Wencheng Han, Tiancai Wang, Xingping Dong, Xiangyu Zhang, and Jianbing Shen. 2023. Referring\nMulti-Object Tracking. In Proceedings of the 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR) (Vancouver, Canada). Institute of Electrical and Electronics Engineers, 14633‚Äì14642.\n[224] Dongming Wu, Wencheng Han, Tiancai Wang, Yingfei Liu, Xiangyu Zhang, and Jianbing Shen. 2023. Language\nPrompt for Autonomous Driving. arXiv preprint arXiv:2309.04379 (2023).\n[225] Kevin Wu, Eric Wu, Ally Cassasola, Angela Zhang, Kevin Wei, Teresa Nguyen, Sith Riantawan, Patricia Shi Riantawan,\nDaniel E Ho, and James Zou. 2024. How well do LLMs cite relevant medical references? An evaluation framework\nand analyses. arXiv preprint arXiv:2402.02008 (2024).\n[226] Shijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur,\nDavid Rosenberg, and Gideon Mann. 2023. BloombergGPT: A Large Language Model for Finance. arXiv preprint\narXiv:2303.17564 (2023).\n[227] Weihao Xia and Jing-Hao Xue. 2023. A Survey on Deep Generative 3D-aware Image Synthesis. Comput. Surveys 56, 4,\nArticle 90 (nov 2023), 34 pages.\n[228] Jianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. 2010. SUN Database: Large-scale\nScene Recognition from Abbey to Zoo. In Proceedings of the 2010 IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) (San Francisco, CA, USA). Institute of Electrical and Electronics Engineers, 3485‚Äì3492.\n[229] Miao Xiong, Zhiyuan Hu, Xinyang Lu, YIFEI LI, Jie Fu, Junxian He, and Bryan Hooi. 2024. Can LLMs Express Their\nUncertainty? An Empirical Evaluation of Confidence Elicitation in LLMs. In Proceedings of the 12th International\nConference on Learning Representations (Vienna, Austria).\n[230] Yuanyou Xu, Zongxin Yang, and Yi Yang. 2023. SEEAvatar: Photorealistic Text-to-3D Avatar Generation with\nConstrained Geometry and Appearance. arXiv preprint arXiv:2312.08889 (2023).\n[231] Zhenhua Xu, Yujia Zhang, Enze Xie, Zhen Zhao, Yong Guo, Kwan-Yee K. Wong, Zhenguo Li, and Hengshuang Zhao.\n2024. DriveGPT4: Interpretable End-to-End Autonomous Driving Via Large Language Model. IEEE Robotics and\nAutomation Letters 9, 10 (2024), 8186‚Äì8193.\n[232] Jianing Yang, Xuweiyi Chen, Nikhil Madaan, Madhavan Iyengar, Shengyi Qian, David F Fouhey, and Joyce Chai. 2024.\n3D-GRAND: A Million-Scale Dataset for 3D-LLMs with Better Grounding and Less Hallucination. arXiv preprint\narXiv:2406.05132 (2024).\n[233] Jingfeng Yang, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Shaochen Zhong, Bing Yin,\nand Xia Hu. 2024. Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond. ACM Transactions\non Knowledge Discovery from Data (feb 2024).\n[234] Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, and Pengfei Liu. 2024. Alignment for Honesty. InProceedings\nof the 2024 Conference on Neural Information Processing Systems (Vancouver, Canada).\n[235] Yue Yang, Fan-Yun Sun, Luca Weihs, Eli Vanderbilt, Alvaro Herrasti, Winson Han, Jiajun Wu, Nick Haber, Ranjay\nKrishna, Lingjie Liu, et al . 2024. Holodeck: Language Guided Generation of 3D Embodied AI Environments. In\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\n1:54 Chakraborty et al.\nProceedings of the 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (Seattle, WA, USA).\nInstitute of Electrical and Electronics Engineers, 16277‚Äì16287.\n[236] Zhenjie Yang, Xiaosong Jia, Hongyang Li, and Junchi Yan. 2024. LLM4Drive: A Survey of Large Language Models\nfor Autonomous Driving. In Proceedings of the Workshop on Open-World Agents at the 2024 Conference on Neural\nInformation Processing Systems (Vancouver, Canada).\n[237] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D.\nManning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings\nof the 2018 Conference on Empirical Methods in Natural Language Processing (Brussels, Belgium). Association for\nComputational Linguistics, 2369‚Äì2380.\n[238] Jia-Yu Yao, Kun-Peng Ning, Zhen-Hui Liu, Mu-Nan Ning, and Li Yuan. 2023. LLM Lies: Hallucinations are not Bugs,\nbut Features as Adversarial Examples. arXiv preprint arXiv:2310.01469 (2023).\n[239] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023. ReAct:\nSynergizing Reasoning and Acting in Language Models. InProceedings of the 11th International Conference on Learning\nRepresentations (Kigali, Rwanda).\n[240] Hongbin Ye, Tong Liu, Aijia Zhang, Wei Hua, and Weiqiang Jia. 2023. Cognitive Mirage: A Review of Hallucinations\nin Large Language Models. arXiv preprint arXiv:2309.06794 (2023).\n[241] Yakir Yehuda, Itzik Malkiel, Oren Barkan, Jonathan Weill, Royi Ronen, and Noam Koenigstein. 2024. InterrogateLLM:\nZero-Resource Hallucination Detection in LLM-Generated Answers. In Proceedings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Bangkok, Thailand). Association for Computational Linguistics, 9333‚Äì9347.\n[242] Koichiro Yoshino, Chiori Hori, Julien Perez, Luis Fernando D‚ÄôHaro, Lazaros Polymenakos, Chulaka Gunasekara,\nWalter S Lasecki, Jonathan K Kummerfeld, Michel Galley, Chris Brockett, et al. 2019. Dialog System Technology\nChallenge 7. arXiv preprint arXiv:1901.03461 (2019).\n[243] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. 2015. LSUN: Construction of\na Large-Scale Image Dataset using Deep Learning with Humans in the Loop. arXiv preprint arXiv:1506.03365 (2015).\n[244] Xiaodong Yu, Hao Cheng, Xiaodong Liu, Dan Roth, and Jianfeng Gao. 2024. ReEval: Automatic Hallucination\nEvaluation for Retrieval-Augmented Large Language Models via Transferable Adversarial Attacks. In Findings of the\n2024 Conference of the North American Chapter of the Association for Computational Linguistics (Mexico City, Mexico).\nAssociation for Computational Linguistics, 1333‚Äì1351.\n[245] Weizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. BARTScore: Evaluating Generated Text as Text Generation.\nIn Proceedings of the 2021 Conference on Neural Information Processing Systems (Virtual). Curran Associates, Inc.,\n27263‚Äì27277.\n[246] Andy Zeng, Pete Florence, Jonathan Tompson, Stefan Welker, Jonathan Chien, Maria Attarian, Travis Armstrong,\nIvan Krasin, Dan Duong, Vikas Sindhwani, and Johnny Lee. 2021. Transporter Networks: Rearranging the Visual\nWorld for Robotic Manipulation. In Proceedings of the 4th Conference on Robot Learning (London, United Kingdom)\n(Proceedings of Machine Learning Research, Vol. 155) . PMLR, 726‚Äì747.\n[247] Fanlong Zeng, Wensheng Gan, Yongheng Wang, Ning Liu, and Philip S. Yu. 2023. Large Language Models for Robotics:\nA Survey. arXiv preprint arXiv:2311.07226 (2023).\n[248] Yuheng Zha, Yichi Yang, Ruichen Li, and Zhiting Hu. 2023. AlignScore: Evaluating Factual Consistency with A\nUnified Alignment Function. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\n(Toronto, Canada). Association for Computational Linguistics, 11328‚Äì11348.\n[249] Ceng Zhang, Junxin Chen, Jiatong Li, Yanhong Peng, and Zebing Mao. 2023. Large language models for human-robot\ninteraction: A review. Biomimetic Intelligence and Robotics 3, 4 (2023), 100131.\n[250] Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. 2024. How Language Model Hallucinations\nCan Snowball. In Proceedings of the 41st International Conference on Machine Learning (Vienna, Austria) (Proceedings\nof Machine Learning Research, Vol. 235) . PMLR, 59670‚Äì59684.\n[251] Shuo Zhang, Liangming Pan, Junzhou Zhao, and William Yang Wang. 2024. The Knowledge Alignment Problem:\nBridging Human and External Knowledge for Large Language Models. In Findings of the 62nd Annual Meeting of the\nAssociation for Computational Linguistics (Bangkok, Thailand). Association for Computational Linguistics, 2025‚Äì2038.\n[252] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. BERTScore: Evaluating Text\nGeneration with BERT. In Proceedings of the 8th International Conference on Learning Representations (Virtual).\n[253] Yue Zhang, Yafu Li, Leyang Cui, Deng Cai, Lemao Liu, Tingchen Fu, Xinting Huang, Enbo Zhao, Yu Zhang, Yulong\nChen, et al. 2023. Siren‚Äôs Song in the AI Ocean: A Survey on Hallucination in Large Language Models. arXiv preprint\narXiv:2309.01219 (2023).\n[254] Huaqin Zhao, Zhengliang Liu, Zihao Wu, Yiwei Li, Tianze Yang, Peng Shu, Shaochen Xu, Haixing Dai, Lin Zhao,\nGengchen Mai, et al. 2024. Revolutionizing Finance with LLMs: An Overview of Applications and Insights. arXiv\npreprint arXiv:2401.11641 (2024).\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.\nHallucination Detection in Foundation Models for Decision-Making 1:55\n[255] Wenshuai Zhao, Jorge Pe√±a Queralta, and Tomi Westerlund. 2020. Sim-to-Real Transfer in Deep Reinforcement\nLearning for Robotics: a Survey. In 2020 IEEE Symposium Series on Computational Intelligence (SSCI) (Canberra, ACT,\nAustralia). 737‚Äì744.\n[256] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie\nZhang, Zican Dong, et al. 2023. A Survey of Large Language Models. arXiv preprint arXiv:2303.18223 (2023).\n[257] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. 2024. Beyond Hallucinations:\nEnhancing LVLMs through Hallucination-Aware Direct Preference Optimization. arXiv preprint arXiv:2311.16839\n(2024).\n[258] Ce Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu, Guangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan, Lifang He, et al.\n2023. A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT. arXiv preprint\narXiv:2302.09419 (2023).\n[259] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao.\n2024. Analyzing and Mitigating Object Hallucination in Large Vision-Language Models. In Proceedings of the 12th\nInternational Conference on Learning Representations (Vienna, Austria).\n[260] Cai-Nicolas Ziegler. 2020. Books Dataset. Kaggle (2020). https://www.kaggle.com/datasets/saurabhbagchi/books-\ndataset\nReceived 29 April 2024; revised 13 January 2025; accepted 3 February 2025\nACM Comput. Surv., Vol. 1, No. 1, Article 1. Publication date: January 2025.",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.8746393322944641
    },
    {
      "name": "State (computer science)",
      "score": 0.6366884708404541
    },
    {
      "name": "Computer science",
      "score": 0.4115844964981079
    },
    {
      "name": "Art",
      "score": 0.36639267206192017
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33836233615875244
    },
    {
      "name": "History",
      "score": 0.2088542878627777
    },
    {
      "name": "Archaeology",
      "score": 0.1908169388771057
    },
    {
      "name": "Algorithm",
      "score": 0.09067785739898682
    }
  ],
  "institutions": []
}