{
  "title": "Direct Output Connection for a High-Rank Language Model",
  "url": "https://openalex.org/W2888799392",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2140893311",
      "name": "Sho Takase",
      "affiliations": [
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A1749670362",
      "name": "Jun Suzuki",
      "affiliations": [
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A2071111304",
      "name": "Masaaki Nagata",
      "affiliations": [
        "Tohoku University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2609482285",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2757047188",
    "https://openalex.org/W2998704965",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W2097927681",
    "https://openalex.org/W2778718264",
    "https://openalex.org/W4300427683",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2798921788",
    "https://openalex.org/W2963073938",
    "https://openalex.org/W1948566616",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2127836646",
    "https://openalex.org/W2293778248",
    "https://openalex.org/W1843891098",
    "https://openalex.org/W2553303224",
    "https://openalex.org/W2963983719",
    "https://openalex.org/W2963462075",
    "https://openalex.org/W4294555862",
    "https://openalex.org/W2963216553",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2963537482",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2564486991",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2549416390",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2571859396",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W2963347649",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2963748792",
    "https://openalex.org/W1535015163",
    "https://openalex.org/W4254816979",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2963084773",
    "https://openalex.org/W2086161653",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2963266340",
    "https://openalex.org/W2962813775",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W4285719527",
    "https://openalex.org/W2097732278",
    "https://openalex.org/W2962964385",
    "https://openalex.org/W2963374479",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W4919037",
    "https://openalex.org/W4299838440"
  ],
  "abstract": "This paper proposes a state-of-the-art recurrent neural network (RNN) language model that combines probability distributions computed not only from a final RNN layer but also middle layers. This method raises the expressive power of a language model based on the matrix factorization interpretation of language modeling introduced by Yang et al. (2018). Our proposed method improves the current state-of-the-art language model and achieves the best score on the Penn Treebank and WikiText-2, which are the standard benchmark datasets. Moreover, we indicate our proposed method contributes to application tasks: machine translation and headline generation.",
  "full_text": "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4599–4609\nBrussels, Belgium, October 31 - November 4, 2018.c⃝2018 Association for Computational Linguistics\n4599\nDirect Output Connection for a High-Rank Language Model\nSho Takase† Jun Suzuki†‡ Masaaki Nagata†\n†NTT Communication Science Laboratories\n‡Tohoku University\n{takase.sho, nagata.masaaki}@lab.ntt.co.jp\njun.suzuki@ecei.tohoku.ac.jp\nAbstract\nThis paper proposes a state-of-the-art recur-\nrent neural network (RNN) language model\nthat combines probability distributions com-\nputed not only from a ﬁnal RNN layer but\nalso from middle layers. Our proposed method\nraises the expressive power of a language\nmodel based on the matrix factorization in-\nterpretation of language modeling introduced\nby Yang et al. (2018). The proposed method\nimproves the current state-of-the-art language\nmodel and achieves the best score on the\nPenn Treebank and WikiText-2, which are\nthe standard benchmark datasets. Moreover,\nwe indicate our proposed method contributes\nto two application tasks: machine translation\nand headline generation. Our code is pub-\nlicly available at: https://github.com/nttcslab-\nnlp/doc lm.\n1 Introduction\nNeural network language models have played a\ncentral role in recent natural language processing\n(NLP) advances. For example, neural encoder-\ndecoder models, which were successfully ap-\nplied to various natural language generation tasks\nincluding machine translation (Sutskever et al.,\n2014), summarization (Rush et al., 2015), and dia-\nlogue (Wen et al., 2015), can be interpreted as con-\nditional neural language models. Neural language\nmodels also positively inﬂuence syntactic pars-\ning (Dyer et al., 2016; Choe and Charniak, 2016).\nMoreover, such word embedding methods as Skip-\ngram (Mikolov et al., 2013) and vLBL (Mnih and\nKavukcuoglu, 2013) originated from neural lan-\nguage models designed to handle much larger vo-\ncabulary and data sizes. Neural language models\ncan also be used as contextualized word representa-\ntions (Peters et al., 2018). Thus, language modeling\nis a good benchmark task for investigating the gen-\neral frameworks of neural methods in NLP ﬁeld.\nIn language modeling, we compute joint prob-\nability using the product of conditional probabili-\nties. Let w1:T be a word sequence with length T:\nw1,...,w T. We obtain the joint probability of word\nsequence w1:T as follows:\np(w1:T) = p(w1)\nT−1∏\nt=1\np(wt+1|w1:t). (1)\np(w1) is generally assumed to be1 in this literature,\nthat is, p(w1) = 1, and thus we can ignore its cal-\nculation. See the implementation of Zaremba et al.\n(2014)1, for an example. RNN language models\nobtain conditional probability p(wt+1|w1:t) from\nthe probability distribution of each word. To com-\npute the probability distribution, RNN language\nmodels encode sequence w1:t into a ﬁxed-length\nvector and apply a transformation matrix and the\nsoftmax function.\nPrevious researches demonstrated that RNN lan-\nguage models achieve high performance by using\nseveral regularizations and selecting appropriate\nhyperparameters (Melis et al., 2018; Merity et al.,\n2018). However, Yang et al. (2018) proved that\nexisting RNN language models have low expres-\nsive power due to the Softmax bottleneck, which\nmeans the output matrix of RNN language mod-\nels is low rank when we interpret the training of\nRNN language models as a matrix factorization\nproblem. To solve the Softmax bottleneck, Yang\net al. (2018) proposed Mixture of Softmaxes(MoS),\nwhich increases the rank of the matrix by com-\nbining multiple probability distributions computed\nfrom the encoded ﬁxed-length vector.\nIn this study, we propose Direct Output Con-\nnection (DOC) as a generalization of MoS. For\nstacked RNNs, DOC computes the probability dis-\ntributions from the middle layers including input\nembeddings. In addition to raising the rank, the\n1https://github.com/wojzaremba/lstm\n4600\nproposed method helps weaken the vanishing gra-\ndient problem in backpropagation because DOC\nprovides a shortcut connection to the output.\nWe conduct experiments on standard benchmark\ndatasets for language modeling: the Penn Treebank\nand WikiText-2. Our experiments demonstrate that\nDOC outperforms MoS and achieves state-of-the-\nart perplexities on each dataset. Moreover, we in-\nvestigate the effect of DOC on two applications:\nmachine translation and headline generation. We\nindicate that DOC can improve the performance of\nan encoder-decoder with an attention mechanism,\nwhich is a strong baseline for such applications. In\naddition, we conduct an experiment on the Penn\nTreebank constituency parsing task to investigate\nthe effectiveness of DOC.\n2 RNN Language Model\nIn this section, we brieﬂy overview RNN language\nmodels. Let V be the vocabulary size and let\nPt ∈RV be the probability distribution of the vo-\ncabulary at timestep t. Moreover, let Dhn be the\ndimension of the hidden state of then-th RNN, and\nlet De be the dimensions of the embedding vectors.\nThen the RNN language models predict probability\ndistribution Pt+1 by the following equation:\nPt+1 = softmax(WhN\nt ), (2)\nhn\nt = f(hn−1\nt ,hn\nt−1), (3)\nh0\nt = Ext, (4)\nwhere W ∈RV×DhN is a weight matrix 2, E ∈\nRDe×V is a word embedding matrix, xt ∈{0,1}V\nis a one-hot vector of input word wt at timestep t,\nand hn\nt ∈RDhn is the hidden state of then-th RNN\nat timestep t. We deﬁne hn\nt at timestep t= 0 as a\nzero vector: hn\n0 = 0. Let f(·) represent an abstract\nfunction of an RNN, which might be the Elman net-\nwork (Elman, 1990), the Long Short-Term Memory\n(LSTM) (Hochreiter and Schmidhuber, 1997), the\nRecurrent Highway Network (RHN) (Zilly et al.,\n2017), or any other RNN variant. In this research,\nwe stack three LSTM layers based on Merity et al.\n(2018) because they achieved high performance.\n3 Language Modeling as Matrix\nFactorization\nYang et al. (2018) indicated that the training of\nlanguage models can be interpreted as a matrix\n2Actually, we apply a bias term in addition to the weight\nmatrix but we omit it to simplify the following discussion.\nfactorization problem. In this section, we brieﬂy\nintroduce their description. Let word sequence\nw1:t be context ct. Then we can regard a nat-\nural language as a ﬁnite set of the pairs of a\ncontext and its conditional probability distribu-\ntion: L= {(c1,P∗(X|c1)),..., (cU,P∗(X|cU))},\nwhere U is the number of possible contexts and\nX ∈ {0,1}V is a variable representing a one-\nhot vector of a word. Here, we consider matrix\nA∈RU×V that represents the true log probability\ndistributions and matrix H ∈RU×DhN that con-\ntains the hidden states of the ﬁnal RNN layer for\neach context ct:\nA=\n\n\nlogP∗(X|c1)\nlogP∗(X|c2)\n...\nlogP∗(X|cU)\n\n; H =\n\n\nhN\nc1\nhN\nc2\n...\nhN\ncU\n\n. (5)\nThen we obtain set of matrices F(A) = {A +\nΛS}, where S ∈RU×V is an all-ones matrix, and\nΛ ∈RU×U is a diagonal matrix. F(A) contains\nmatrices that shifted each row of Aby an arbitrary\nreal number. In other words, if we take a matrix\nfrom F(A) and apply the softmax function to each\nof its rows, we obtain a matrix that consists of true\nprobability distributions. Therefore, for some A′∈\nF(A), training RNN language models is to ﬁnd the\nparameters satisfying the following equation:\nHW⊤= A′. (6)\nEquation 6 indicates that training RNN language\nmodels can also be interpreted as a matrix factor-\nization problem. In most cases, the rank of matrix\nHW⊤is DhN because DhN is smaller than V and\nU in common RNN language models. Thus, an\nRNN language model cannot express true distribu-\ntions if DhN is much smaller than rank(A′).\nYang et al. (2018) also argued that rank(A′) is\nas high as vocabulary sizeV based on the following\ntwo assumptions:\n1. Natural language is highly context-dependent.\nIn addition, since we can imagine many kinds\nof contexts, it is difﬁcult to assume a basis\nthat represents a conditional probability dis-\ntribution for any contexts. In other words,\ncompressing U is difﬁcult.\n2. Since we also have many kinds of semantic\nmeanings, it is difﬁcult to assume basic mean-\nings that can create all other semantic mean-\nings by such simple operations as addition and\nsubtraction; compressing V is difﬁcult.\n4601\nwt\u0001\nxt\u0001\n2nd LSTM layer \u0001\nwt+1\u0001\nxt+1\u0001\n…\u0001\n…\u0001\n…\u0001\n…\nPt+1\u0001\nEquation (7)\u0001\nInput word\u0001\nOne-hot vector\u0001\nEmbedding layer\u0001\n1st LSTM layer \u0001\nEquation (11)\u0001\n \u0001\nFigure 1: Overview of the proposed method: DOC.\nThis ﬁgure represents the example of N = 2 and\ni0 = i1 = i2 = 3.\nIn summary, Yang et al. (2018) indicated thatDhN\nis much smaller than rank(A) because its scale is\nusually 102 and vocabulary size V is at least 104.\n4 Proposed Method: Direct Output\nConnection\nTo construct a high-rank matrix, Yang et al. (2018)\nproposed Mixture of Softmaxes (MoS). MoS com-\nputes multiple probability distributions from the\nhidden state of ﬁnal RNN layer hN and regards\nthe weighted average of the probability distribu-\ntions as the ﬁnal distribution. In this study, we\npropose Direct Output Connection (DOC), which\nis a generalization method of MoS. DOC computes\nprobability distributions from the middle layers in\naddition to the ﬁnal layer. In other words, DOC\ndirectly connects the middle layers to the output.\nFigure 1 shows an overview of DOC, that uses\nthe middle layers (including word embeddings) to\ncompute the probability distributions. Figure 1\ncomputes three probability distributions from all\nthe layers, but we can vary the number of proba-\nbility distributions for each layer and select some\nlayers to avoid. In our experiments, we search for\nthe appropriate number of probability distributions\nfor each layer.\nFormally, instead of Equation 2, DOC computes\nthe output probability distribution at timestep t+ 1\nby the following equation:\nPt+1 =\nJ∑\nj=1\nπj,ct softmax( ˜Wkj,ct), (7)\ns.t.\nJ∑\nj=1\nπj,ct = 1, (8)\nwhere πj,ct is a weight for each probability distri-\nbution, kj,ct ∈Rd is a vector computed from each\nhidden state hn, and ˜W ∈RV×d is a weight matrix.\nThus, Pt+1 is the weighted average ofJprobability\ndistributions. We deﬁne the U×U diagonal matrix\nwhose elements are weight πj,c for each context c\nas Φ. Then we obtain matrix ˜A∈RU×V:\n˜A= log\nJ∑\nj=1\nΦ softmax(Kj ˜W⊤), (9)\nwhere Kj ∈RU×d is a matrix whose rows are vec-\ntor kj,c. ˜Acan be an arbitrary high rank because the\nrighthand side of Equation 9 computes not only the\nmatrix multiplication but also a nonlinear function.\nTherefore, an RNN language model with DOC can\noutput a distribution matrix whose rank is identical\nto one of the true distributions. In other words, ˜A\nis a better approximation of A′than the output of a\nstandard RNN language model.\nNext we describe how to acquire weight πj,ct\nand vector kj,ct. Let πct ∈RJ be a vector whose\nelements are weight πj,ct. Then we compute πct\nfrom the hidden state of the ﬁnal RNN layer:\nπct = softmax(WπhN\nt ), (10)\nwhere Wπ ∈RJ×DhN is a weight matrix. We next\ncompute kj,ct from the hidden state of the n-th\nRNN layer:\nkj,ct = Wjhn\nt, (11)\nwhere Wj ∈Rd×Dhn is a weight matrix. In addi-\ntion, let in be the number of kj,ct from hn\nt. Then\nwe deﬁne the sum of in for all n as J; that is,∑N\nn=0 in = J. In short, DOC computes J proba-\nbility distributions from all the layers, including the\ninput embedding (h0). For iN = J, DOC becomes\nidentical to MoS. In addition to increasing the rank,\nwe expect that DOC weakens the vanishing gra-\ndient problem during backpropagation because a\nmiddle layer is directly connected to the output,\nsuch as with the auxiliary classiﬁers described in\nSzegedy et al. (2015).\nFor a network that computes the weights for sev-\neral vectors, such as Equation 10, Shazeer et al.\n(2017) indicated that it often converges to a state\nwhere it always produces large weights for few\nvectors. In fact, we observed that DOC tends to\nassign large weights to shallow layers. To prevent\nthis phenomenon, we compute the coefﬁcient of\n4602\nvariation of Equation 10 in each mini-batch as a reg-\nularization term following Shazeer et al. (2017). In\nother words, we try to adjust the sum of the weights\nfor each probability distribution with identical val-\nues in each mini-batch. Formally, we compute the\nfollowing equation for a mini-batch consisting of\nwb,wb+1,...,w˜b:\nB =\n˜b∑\nt=b\nπct (12)\nβ =\n(std(B)\navg(B)\n)2\n, (13)\nwhere functions std(·) and avg(·) are functions\nthat respectively return an input’s standard devia-\ntion and its average. In the training step, we add\nλβ multiplied by weight coefﬁcient β to the loss\nfunction.\n5 Experiments on Language Modeling\nWe investigate the effect of DOC on the language\nmodeling task. In detail, we conduct word-level\nprediction experiments and show that DOC im-\nproves the performance of MoS, which only uses\nthe ﬁnal layer to compute the probability distribu-\ntions. Moreover, we evaluate various combinations\nof layers to explore which combination achieves\nthe best score.\n5.1 Datasets\nWe used the Penn Treebank (PTB) (Marcus et al.,\n1993) and WikiText-2 (Merity et al., 2017) datasets,\nwhich are the standard benchmark datasets for\nthe word-level language modeling task. Mikolov\net al. (2010) and Merity et al. (2017) respectively\npublished preprocessed PTB 3 and WikiText-24\ndatasets. Table 1 describes their statistics. We used\nthese preprocessed datasets for fair comparisons\nwith previous studies.\n5.2 Hyperparameters\nOur implementation is based on the averaged\nstochastic gradient descent Weight-Dropped LSTM\n(AWD-LSTM)5 proposed by Merity et al. (2018).\nAWD-LSTM consists of three LSTMs with various\nregularizations. For the hyperparameters, we used\nthe same values as Yang et al. (2018) except for the\n3http://www.ﬁt.vutbr.cz/ imikolov/rnnlm/\n4https://einstein.ai/research/the-wikitext-long-term-\ndependency-language-modeling-dataset\n5https://github.com/salesforce/awd-lstm-lm\nPTB WikiText-2\nV ocab 10,000 33,278\nTrain 929,590 2,088,628\n#Token Valid 73,761 217,646\nTest 82,431 245,569\nTable 1: Statistics of PTB and WikiText-2.\nHyperparameter PTB WikiText-2\nLearning rate 20 15\nBatch size 12 15\nNon-monotone interval 60 60\nDe 280 300\nDh1 960 1150\nDh2 960 1150\nDh3 620 650\nDropout rate for xt 0.1 0.1\nDropout rate for h0\nt 0.4 0.65\nDropout rate for h1\nt,h2\nt 0.225 0.2\nDropout rate for h3\nt 0.4 0.4\nDropout rate for kj,ct 0.6 0.6\nRecurrent weight dropout 0.50 0.50\nTable 2: Hyperparameters used for training DOC.\n#DOC\ni3 i2 i1 i0 λβ Valid Test\n15 0 0 0 0 56.54† 54.44†\n20 0 0 0 0 56.88‡ 54.79‡\n15 0 0 5 0 56.21 54.28\n15 0 5 0 0 55.26 53.52\n15 5 0 0 0 54.87 53.15\n15 5 0 0 0.0001 54.95 53.16\n15 5 0 0 0.001 54.62 52.87\n15 5 0 0 0.01 55.13 53.39\n10 5 0 5 0 56.46 54.18\n10 5 5 0 0 56.00 54.37\nTable 3: Perplexities of AWD-LSTM with DOC on the\nPTB dataset. We varied the number of probability dis-\ntributions from each layer in situation J = 20 except\nfor the top row. The top row (†) represents MoS scores\nreported in Yang et al. (2018) as a baseline.‡represents\nthe perplexity obtained by the implementation of Yang\net al. (2018)6 with identical hyperparameters except for\ni3.\ndropout rate for vector kj,ct and the non-monotone\ninterval. Since we found that the dropout rate for\nvector kj,ct greatly inﬂuences βin Equation 13, we\nvaried it from 0.3 to 0.6 with 0.1 intervals. We\nselected 0.6 because this value achieved the best\nscore on the PTB validation dataset. For the non-\nmonotone interval, we adopted the same value as\nZolna et al. (2018). Table 2 summarizes the hyper-\nparameters of our experiments.\n5.3 Results\nTable 3 shows the perplexities of AWD-LSTM with\nDOC on the PTB dataset. Each value of columnsin\n6https://github.com/zihangdai/mos\n4603\nλβ Valid Test\n0 0.276 0.279\n0.0001 0.254 0.252\n0.001 0.217 0.213\n0.01 0.092 0.086\nTable 4: Coefﬁcient of variation of Equation 10: √βin\nvalidation and test sets of PTB.\nModel Valid Test\nAWD-LSTM 401 401\nAWD-LSTM-MoS 10000 10000\nAWD-LSTM-DOC 10000 10000\nTable 5: Rank of output matrix ( ˜A in Equation 9) on\nthe PTB dataset. D3 of AWD-LSTM is 400.\n0 50 100 150 200 250 300\nEpoch\n50\n60\n70\n80\n90\n100Perplexity\nModel\nAWD-LSTM\nAWD-LSTM-MoS\nProposed: AWD-LSTM-DOC\nFigure 2: Perplexities of each method on the PTB vali-\ndation set.\nrepresents the number of probability distributions\nfrom hidden state hn\nt. To ﬁnd the best combination,\nwe varied the number of probability distributions\nfrom each layer by ﬁxing their total to 20: J = 20.\nMoreover, the top row of Table 3 shows the per-\nplexity of AWD-LSTM with MoS reported in Yang\net al. (2018) for comparison. Table 3 indicates that\nlanguage models using middle layers outperformed\none using only the ﬁnal layer. In addition, Table\n3 shows that increasing the distributions from the\nﬁnal layer (i3 = 20) degraded the score from the\nlanguage model with i3 = 15 (the top row of Ta-\nble 3). Thus, to obtain a superior language model,\nwe should not increase the number of distributions\nfrom the ﬁnal layer; we should instead use the mid-\ndle layers, as with our proposed DOC.\nTable 3 shows that the i3 = 15,i2 = 5 setting\nachieved the best performance and the other set-\ntings with shallow layers have a little effect. This\nresult implies that we need some layers to output ac-\ncurate distributions. In fact, most previous studies\nadopted two LSTM layers for language modeling.\nThis suggests that we need at least two layers to\nobtain high-quality distributions.\nModel Valid Test\nAWD-LSTM† 58.88 56.36\nAWD-LSTM-MoS† 56.36 54.26\nAWD-LSTM-MoS‡ 55.67 53.75\nAWD-LSTM-DOC 54.62 52.87\nAWD-LSTM-DOC (ﬁn) 54.12 52.38\nTable 6: Perplexities of our implementations and re-\nruns on the PTB dataset. We set the non-monotone\ninterval to 60. †represents results obtained by original\nimplementations with identical hyperparameters except\nfor non-monotone interval. ‡indicates the result ob-\ntained by our AWD-LSTM-MoS implementation with\nidentical dropout rates as AWD-LSTM-DOC. For (ﬁn),\nwe repeated ﬁne-tuning until convergence.\nFor the i3 = 15 ,i2 = 5 setting, we explored\nthe effect of λβ in {0,0.01,0.001,0.0001}. Al-\nthough Table 3 shows that λβ = 0.001 achieved\nthe best perplexity, the effect is not consistent. Ta-\nble 4 shows the coefﬁcient of variation of Equa-\ntion 10, i.e., √β in the PTB dataset. This table\ndemonstrates that the coefﬁcient of variation de-\ncreases with growth in λβ. In other words, the\nmodel trained with a large λβ assigns balanced\nweights to each probability distribution. These\nresults indicate that it is not always necessary to\nequally use each probability distribution, but we\ncan acquire a better model in some λβ. Hereafter,\nwe refer to the setting that achieved the best score\n(i3 = 15 ,i2 = 5 ,λβ = 0 .001) as AWD-LSTM-\nDOC.\nTable 5 shows the ranks of matrices containing\nlog probability distributions from each method. In\nother words, Table 5 describes ˜Ain Equation 9 for\neach method. As shown by this table, the output\nof AWD-LSTM is restricted to D37. In contrast,\nAWD-LSTM-MoS (Yang et al., 2018) and AWD-\nLSTM-DOC outputted matrices whose ranks equal\nthe vocabulary size. This fact indicates that DOC\n(including MoS) can output the same matrix as the\ntrue distributions in view of a rank.\nFigure 2 illustrates the learning curves of each\nmethod on PTB. This ﬁgure contains the valida-\ntion scores of AWD-LSTM, AWD-LSTM-MoS,\nand AWD-LSTM-DOC at each training epoch. We\ntrained AWD-LSTM and AWD-LSTM-MoS by\nsetting the non-monotone interval to 60, as with\nAWD-LSTM-DOC. In other words, we used hyper-\nparameters identical to the original ones to train\nAWD-LSTM and AWD-LSTM-MoS, except for\nthe non-monotone interval. We note that the opti-\n7Actually, the maximum rank size of an ordinary RNN\nlanguage model is DN + 1when we use a bias term.\n4604\nModel #Param Valid Test\nLSTM (medium) (Zaremba et al., 2014) 20M 86.2 82.7\nLSTM (large) (Zaremba et al., 2014) 66M 82.2 78.4\nVariational LSTM (medium) (Gal and Ghahramani, 2016) 20M 81.9 ±0.2 79.7 ±0.1\nVariational LSTM (large) (Gal and Ghahramani, 2016) 66M 77.9 ±0.3 75.2 ±0.2\nVariational RHN (Zilly et al., 2017) 32M 71.2 68.5\nVariational RHN + WT (Zilly et al., 2017) 23M 67.9 65.4\nVariational RHN + WT + IOG (Takase et al., 2017) 29M 67.0 64.4\nNeural Architecture Search (Zoph and Le, 2017) 54M - 62.4\nLSTM with skip connections (Melis et al., 2018) 24M 60.9 58.3\nAWD-LSTM (Merity et al., 2018) 24M 60.0 57.3\nAWD-LSTM + Fraternal Dropout (Zolna et al., 2018) 24M 58.9 56.8\nAWD-LSTM-MoS (Yang et al., 2018) 22M 56.54 54.44\nProposed method: AWD-LSTM-DOC 23M 54.62 52.87\nProposed method: AWD-LSTM-DOC (ﬁn) 23M 54.12 52.38\nProposed method (ensemble): AWD-LSTM-DOC ×5 114M 49.99 48.44\nProposed method (ensemble): AWD-LSTM-DOC (ﬁn) ×5 114M 48.63 47.17\nTable 7: Perplexities of each method on the PTB dataset.\nModel #Param Valid Test\nVariational LSTM + IOG (Takase et al., 2017) 70M 95.9 91.0\nVariational LSTM + WT + AL (Inan et al., 2017) 28M 91.5 87.0\nLSTM with skip connections (Melis et al., 2018) 24M 69.1 65.9\nAWD-LSTM (Merity et al., 2018) 33M 68.6 65.8\nAWD-LSTM + Fraternal Dropout (Zolna et al., 2018) 34M 66.8 64.1\nAWD-LSTM-MoS (Yang et al., 2018) 35M 63.88 61.45\nProposed method: AWD-LSTM-DOC 37M 60.97 58.55\nProposed method: AWD-LSTM-DOC (ﬁn) 37M 60.29 58.03\nProposed method (ensemble): AWD-LSTM-DOC ×5 185M 56.14 54.23\nProposed method (ensemble): AWD-LSTM-DOC (ﬁn) ×5 185M 54.91 53.09\nTable 8: Perplexities of each method on the WikiText-2 dataset.\nmization method converts the ordinary stochastic\ngradient descent (SGD) into the averaged SGD at\nthe point where convergence almost occurs. In Fig-\nure 2, the turning point is the epoch when each\nmethod drastically decreases the perplexity. Figure\n2 shows that each method similarly reduces the per-\nplexity at the beginning. AWD-LSTM and AWD-\nLSTM-MoS were slow to decrease the perplexity\nfrom 50 epochs. In contrast, AWD-LSTM-DOC\nconstantly decreased the perplexity and achieved a\nlower value than the other methods with ordinary\nSGD. Therefore, we conclude that DOC positively\naffects the training of language modeling.\nTable 6 shows the AWD-LSTM, AWD-LSTM-\nMoS, and AWD-LSTM-DOC results in our con-\nﬁgurations. For AWD-LSTM-MoS, we trained\nour implementation with the same dropout rates\nas AWD-LSTM-DOC for a fair comparison. AWD-\nLSTM-DOC outperformed both the original AWD-\nLSTM-MoS and our implementation. In other\nwords, DOC outperformed MoS.\nSince the averaged SGD uses the averaged pa-\nrameters from each update step, the parameters\nof the early steps are harmful to the ﬁnal parame-\nters. Therefore, when the model converges, recent\nstudies and ours eliminate the history of and then\nretrains the model. Merity et al. (2018) referred\nto this retraining process as ﬁne-tuning. Although\nmost previous studies only conducted ﬁne-tuning\nonce, Zolna et al. (2018) argued that two ﬁne-\ntunings provided additional improvement. Thus,\nwe repeated ﬁne-tuning until we achieved no more\nimprovements in the validation data. We refer to\nthe model as AWD-LSTM-DOC (ﬁn) in Table 6,\nwhich shows that repeated ﬁne-tunings improved\nthe perplexity by about 0.5.\nTables 7 and 8 respectively show the perplex-\nities of AWD-LSTM-DOC and previous studies\non PTB and WikiText-28. These tables show that\nAWD-LSTM-DOC achieved the best perplexity.\nAWD-LSTM-DOC improved the perplexity by al-\nmost 2.0 on PTB and 3.5 on WikiText-2 from the\nstate-of-the-art scores. The ensemble technique\nprovided further improvement, as described in pre-\n8We exclude models that use the statistics of the test\ndata (Grave et al., 2017; Krause et al., 2017) from these tables\nbecause we regard neural language models as the basis of NLP\napplications and consider it unreasonable to know correct out-\nputs during applications, e.g., machine translation. In other\nwords, we focus on neural language models as the foundation\nof applications although we can combine the method using\nthe statistics of test data with our AWD-LSTM-DOC.\n4605\nvious studies (Zaremba et al., 2014; Takase et al.,\n2017), and improved the perplexity by at least 4\npoints on both datasets. Finally, the ensemble of\nthe repeated ﬁnetuning models achieved 47.17 on\nthe PTB test and 53.09 on the WikiText-2 test.\n6 Experiments on Application Tasks\nAs described in Section 1, a neural encoder-decoder\nmodel can be interpreted as a conditional language\nmodel. To investigate the effect of DOC on an\nencoder-decoder model, we incorporate DOC into\nthe decoder and examine its performance.\n6.1 Dataset\nWe conducted experiments on machine translation\nand headline generation tasks. For machine transla-\ntion, we used two kinds of sentence pairs (English-\nGerman and English-French) in the IWSLT 2016\ndataset9. The training set respectively contains\nabout 189K and 208K sentence pairs of English-\nGerman and English-French. We experimented in\nfour settings: from English to German (En-De), its\nreverse (De-En), from English to French (En-Fr),\nand its reverse (Fr-En).\nHeadline generation is a task that creates a short\nsummarization of an input sentence(Rush et al.,\n2015). Rush et al. (2015) constructed a headline\ngeneration dataset by extracting pairs of ﬁrst sen-\ntences of news articles and their headlines from the\nannotated English Gigaword corpus (Napoles et al.,\n2012). They also divided the extracted sentence-\nheadline pairs into three parts: training, validation,\nand test sets. The training set contains about 3.8M\nsentence-headline pairs. For our evaluation, we\nused the test set constructed by Zhou et al. (2017)\nbecause the one constructed by Rush et al. (2015)\ncontains some invalid instances, as reported in\nZhou et al. (2017).\n6.2 Encoder-Decoder Model\nFor the base model, we adopted an encoder-decoder\nwith an attention mechanism described in Kiyono\net al. (2017). The encoder consists of a 2-layer\nbidirectional LSTM, and the decoder consists of a\n2-layer LSTM with attention proposed by Luong\net al. (2015). We interpreted the layer after com-\nputing the attention as the 3rd layer of the decoder.\nWe refer to this encoder-decoder as EncDec. For\nthe hyperparameters, we followed the setting of\nKiyono et al. (2017) except for the sizes of hidden\n9https://wit3.fbk.eu/\nModel En-De De-En En-Fr Fr-En\nEncDec 23.05 28.18 34.37 34.07\nEncDec+DOC (i3 = 2) 23.62 29.12 36.09 34.41\nEncDec+DOC (i3 = i2 = 2) 23.97 29.33 36.11 34.72\nTable 9: BLEU scores on test sets in the IWSLT 2016\ndataset. We report averages of three runs.\nModel RG-1 RG-2 RG-L\nEncDec 46.77 24.87 43.58\nEncDec+DOC (i3 = 2) 46.91 24.91 43.73\nEncDec+DOC (i3 = i2 = 2) 46.99 25.29 43.83\nABS (Rush et al., 2015) 37.41 15.87 34.70\nSEASS (Zhou et al., 2017) 46.86 24.58 43.53\nKiyono et al. (2017) 46.34 24.85 43.49\nTable 10: ROUGE F1 scores in headline generation\ntest data provided by Zhou et al. (2017). RG in table\ndenotes ROUGE. For our implementations (the upper\npart), we report averages of three runs.\nstates and embeddings. We used 500 for machine\ntranslation and 400 for headline generation. We\nconstructed a vocabulary set by using Byte-Pair-\nEncoding10 (BPE) (Sennrich et al., 2016). We set\nthe number of BPE merge operations at 16K for\nthe machine translation and 5K for the headline\ngeneration.\nIn this experiment, we compare DOC to the base\nEncDec. We prepared two DOC settings: using\nonly the ﬁnal layer, that is, a setting that is identical\nto MoS, and using both the ﬁnal and middle layers.\nWe used the 2nd and 3rd layers in the latter setting\nbecause this case achieved the best performance on\nthe language modeling task in Section 5.3. We set\ni3 = 2 and i2 = 2,i3 = 2. For this experiment,\nwe modiﬁed a publicly available encode-decoder\nimplementation11.\n6.3 Results\nTable 9 shows the BLEU scores of each method.\nSince an initial value often drastically varies the\nresult of a neural encoder-decoder, we reported\nthe average of three models trained from different\ninitial values and random seeds. Table 9 indicates\nthat EncDec+DOC outperformed EncDec.\nTable 10 shows the ROUGE F1 scores of each\nmethod. In addition to the results of our imple-\nmentations (the upper part), the lower part repre-\nsents the published scores reported in previous stud-\nies. For the upper part, we reported the average of\nthree models (as in Table 9). EncDec+DOC outper-\nformed EncDec on all scores. Moreover, EncDec\n10https://github.com/rsennrich/subword-nmt\n11https://github.com/mlpnlp/mlpnlp-nmt/\n4606\noutperformed the state-of-the-art method (Zhou\net al., 2017) on the ROUGE-2 and ROUGE-L F1\nscores. In other words, our baseline is already very\nstrong. We believe that this is because we adopted\na larger embedding size than Zhou et al. (2017). It\nis noteworthy that DOC improved the performance\nof EncDec even though EncDec is very strong.\nThese results indicate that DOC positively inﬂu-\nences a neural encoder-decoder model. Using the\nmiddle layer also yields further improvement be-\ncause EncDec+DOC (i3 = i2 = 2) outperformed\nEncDec+DOC (i3 = 2).\n7 Experiments on Constituency Parsing\nChoe and Charniak (2016) achieved high F1 scores\non the Penn Treebank constituency parsing task\nby transforming candidate trees into a symbol se-\nquence (S-expression) and reranking them based\non the perplexity obtained by a neural language\nmodel. To investigate the effectiveness of DOC,\nwe evaluate our language models following their\nconﬁgurations.\n7.1 Dataset\nWe used the Wall Street Journal of the Penn Tree-\nbank dataset. We used the section 2-21 for train-\ning, 22 for validation, and 23 for testing. We ap-\nplied the preprocessing codes of Choe and Char-\nniak (2016)12 to the dataset and converted a token\nthat appears fewer than ten times in the training\ndataset into a special token unk. For reranking, we\nprepared 500 candidates obtained by the Charniak\nparser (Charniak, 2000).\n7.2 Models\nWe compare AWD-LSTM-DOC with AWD-\nLSTM (Merity et al., 2018) and AWD-LSTM-\nMoS (Yang et al., 2018). We trained each model\nwith the same hyperparameters from our language\nmodeling experiments (Section 5). We selected\nthe model that achieved the best perplexity on the\nvalidation set during the training.\n7.3 Results\nTable 11 shows the bracketing F1 scores on the\nPTB test set. This table is divided into three\nparts by horizontal lines; the upper part describes\nthe scores by single language modeling based\nrerankers, the middle part shows the results by en-\nsembling ﬁve rerankers, and the lower part repre-\n12https://github.com/cdg720/emnlp2016\nF1\nModel Base Rerank\nReranking with single model\nChoe and Charniak (2016) 89.7 92.6\nAWD-LSTM 89.7 93.2\nAWD-LSTM-MoS 89.7 93.2\nAWD-LSTM-DOC 89.7 93.3\nReranking with model ensemble\nAWD-LSTM ×5 (ensemble) 89.7 93.4\nAWD-LSTM-MoS ×5 (ensemble) 89.7 93.4\nAWD-LSTM-DOC ×5 (ensemble) 89.7 93.5\nAWD-LSTM-DOC ×5 (ensemble) 91.2 94.29\nAWD-LSTM-DOC ×5 (ensemble) 93.12 94.47\nState-of-the-art results\nDyer et al. (2016) 91.7 93.3\nFried et al. (2017) (ensemble) 92.72 94.25\nSuzuki et al. (2018) (ensemble) 92.74 94.32\nKitaev and Klein (2018) 95.13 -\nTable 11: Bracketing F1 scores on the PTB test set (Sec-\ntion 23). This table includes reranking models trained\non the PTB without external data.\nsents the current state-of-the-art scores in the set-\nting without external data. The upper part also\ncontains the score reported in Choe and Char-\nniak (2016) that reranked candidates by the simple\nLSTM language model. This part indicates that\nour implemented rerankers outperformed the sim-\nple LSTM language model based reranker, which\nachieved 92.6 F1 score (Choe and Charniak, 2016).\nMoreover, AWD-LSTM-DOC outperformed AWD-\nLSTM and AWD-LSTM-MoS. These results corre-\nspond to the language modeling task.\nThe middle part shows that AWD-LSTM-DOC\nalso outperformed AWD-LSTM and AWD-LSTM-\nMoS in the ensemble setting. In addition, we can\nimprove the performance by exchanging the base\nparser with a stronger one. In fact, we achieved\n94.29 F1 score by reranking the candidates from\nretrained Recurrent Neural Network Grammars\n(RNNG) (Dyer et al., 2016)13, that achieved 91.2\nF1 score in our conﬁguration. Moreover, the low-\nest row of the middle part indicates the result by\nreranking the candidates from the retrained neural\nencoder-decoder based parser (Suzuki et al., 2018).\nOur base parser has two different parts from Suzuki\net al. (2018). First, we used the sum of the hidden\nstates of the forward and backward RNNs as the\nhidden layer for each RNN14. Second, we tied the\nembedding matrix to the weight matrix to compute\n13The output of RNNG is not in descending order because\nit samples candidates based on their scores. Thus, we pre-\npared more candidates (i.e., 700) to be able to obtain correct\ninstances as candidates.\n14We used the deep bidirectional encoder described at\nhttp://opennmt.net/OpenNMT/training/models/ instead of a\nbasic bidirectional encoder.\n4607\nthe probability distributions in the decoder. The\nretrained parser achieved 93.12 F1 score. Finally,\nwe achieved 94.47 F1 score by reranking its candi-\ndates with AWD-LSTM-DOC. We expect that we\ncan achieve even better score by replacing the base\nparser with the current state-of-the-art one (Kitaev\nand Klein, 2018).\n8 Related Work\nBengio et al. (2003) are pioneers of neural language\nmodels. To address the curse of dimensionality\nin language modeling, they proposed a method\nusing word embeddings and a feed-forward neu-\nral network (FFNN). They demonstrated that their\napproach outperformed n-gram language models,\nbut FFNN can only handle ﬁxed-length contexts.\nInstead of FFNN, Mikolov et al. (2010) applied\nRNN (Elman, 1990) to language modeling to ad-\ndress the entire given sequence as a context. Their\nmethod outperformed the Kneser-Ney smoothed\n5-gram language model (Kneser and Ney, 1995;\nChen and Goodman, 1996).\nResearchers continue to try to improve the per-\nformance of RNN language models. Zaremba et al.\n(2014) used LSTM (Hochreiter and Schmidhuber,\n1997) instead of a simple RNN for language mod-\neling and signiﬁcantly improved an RNN language\nmodel by applying dropout (Srivastava et al., 2014)\nto all the connections except for the recurrent con-\nnections. To regularize the recurrent connections,\nGal and Ghahramani (2016) proposed variational\ninference-based dropout. Their method uses the\nsame dropout mask at each timestep. Zolna et al.\n(2018) proposed fraternal dropout, which mini-\nmizes the differences between outputs from dif-\nferent dropout masks to be invariant to the dropout\nmask. Melis et al. (2018) used black-box opti-\nmization to ﬁnd appropriate hyperparameters for\nRNN language models and demonstrated that the\nstandard LSTM with proper regularizations can\noutperform other architectures.\nApart from dropout techniques, Inan et al. (2017)\nand Press and Wolf (2017) proposed the word tying\nmethod (WT), which uniﬁes word embeddings (E\nin Equation 4) with the weight matrix to compute\nprobability distributions (W in Equation 2). In ad-\ndition to quantitative evaluation, Inan et al. (2017)\nprovided a theoretical justiﬁcation for WT and pro-\nposed the augmented loss technique (AL), which\ncomputes an objective probability based on word\nembeddings. In addition to these regularization\ntechniques, Merity et al. (2018) used DropCon-\nnect (Wan et al., 2013) and averaged SGD (Polyak\nand Juditsky, 1992) for an LSTM language model.\nTheir AWD-LSTM achieved lower perplexity than\nMelis et al. (2018) on PTB and WikiText-2.\nPrevious studies also explored superior archi-\ntecture for language modeling. Zilly et al. (2017)\nproposed recurrent highway networks that use high-\nway layers (Srivastava et al., 2015) to deepen re-\ncurrent connections. Zoph and Le (2017) adopted\nreinforcement learning to construct the best RNN\nstructure. However, as mentioned, Melis et al.\n(2018) established that the standard LSTM is supe-\nrior to these architectures. Apart from RNN archi-\ntecture, Takase et al. (2017) proposed the input-to-\noutput gate (IOG), which boosts the performance\nof trained language models.\nAs described in Section 3, Yang et al. (2018) in-\nterpreted training language modeling as matrix fac-\ntorization and improved performance by computing\nmultiple probability distributions. In this study, we\ngeneralized their approach to use the middle lay-\ners of RNNs. Finally, our proposed method, DOC,\nachieved the state-of-the-art score on the standard\nbenchmark datasets.\nSome studies provided methods that boost per-\nformance by using statistics obtained from test data.\nGrave et al. (2017) extended a cache model (Kuhn\nand De Mori, 1990) for RNN language models.\nKrause et al. (2017) proposed dynamic evaluation\nthat updates parameters based on a recent sequence\nduring testing. Although these methods might also\nimprove the performance of DOC, we omitted such\ninvestigation to focus on comparisons among meth-\nods trained only on the training set.\n9 Conclusion\nWe proposed Direct Output Connection(DOC), a\ngeneralization method of MoS introduced by Yang\net al. (2018). DOC raises the expressive power\nof RNN language models and improves quality of\nthe model. DOC outperformed MoS and achieved\nthe best perplexities on the standard benchmark\ndatasets of language modeling: PTB and WikiText-\n2. Moreover, we investigated its effectiveness on\nmachine translation and headline generation. Our\nresults show that DOC also improved the perfor-\nmance of EncDec and using a middle layer posi-\ntively affected such application tasks.\n4608\nReferences\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nEugene Charniak. 2000. A maximum-entropy-inspired\nparser. In 1st Meeting of the North American Chap-\nter of the Association for Computational Linguistics\n(NAACL 2000), pages 132–139.\nStanley F. Chen and Joshua Goodman. 1996. An em-\npirical study of smoothing techniques for language\nmodeling. In Proceedings of the 34th Annual Meet-\ning on Association for Computational Linguistics\n(ACL 1996), pages 310–318.\nDo Kook Choe and Eugene Charniak. 2016. Parsing as\nlanguage modeling. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP 2016), pages 2331–2336.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A. Smith. 2016. Recurrent neural network\ngrammars. In Proceedings of the 2016 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL-HLT 2016), pages 199–209.\nJeffrey L Elman. 1990. Finding Structure in Time.\nCognitive science, 14(2):179–211.\nDaniel Fried, Mitchell Stern, and Dan Klein. 2017. Im-\nproving neural parsing by disentangling model com-\nbination and reranking effects. In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL 2017), pages 161–166.\nYarin Gal and Zoubin Ghahramani. 2016. A Theoreti-\ncally Grounded Application of Dropout in Recurrent\nNeural Networks. In Advances in Neural Informa-\ntion Processing Systems 29 (NIPS 2016).\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017. Improving Neural Language Models with a\nContinuous Cache. In Proceedings of the 5th Inter-\nnational Conference on Learning Representations\n(ICLR 2017).\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong Short-Term Memory. Neural Computation,\n9(8):1735–1780.\nHakan Inan, Khashayar Khosravi, and Richard Socher.\n2017. Tying Word Vectors and Word Classiﬁers:\nA Loss Framework for Language Modeling. In\nProceedings of the 5th International Conference on\nLearning Representations (ICLR 2017).\nNikita Kitaev and Dan Klein. 2018. Constituency pars-\ning with a self-attentive encoder. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (ACL 2018), pages 2676–\n2686.\nShun Kiyono, Sho Takase, Jun Suzuki, Naoaki\nOkazaki, Kentaro Inui, and Masaaki Nagata. 2017.\nSource-side prediction for neural headline genera-\ntion. CoRR.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In\nIn Proceedings of the IEEE International Confer-\nence on Acoustics, Speech and Signal Processing\n(ICASSP 1995), pages 181–184.\nBen Krause, Emmanuel Kahembwe, Iain Murray, and\nSteve Renals. 2017. Dynamic evaluation of neural\nsequence models. CoRR.\nRoland Kuhn and Renato De Mori. 1990. A cache-\nbased natural language model for speech recogni-\ntion. 12:570–583.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP 2015), pages 1412–\n1421.\nMitchell P. Marcus, Mary Ann Marcinkiewicz, and\nBeatrice Santorini. 1993. Building a Large Anno-\ntated Corpus of English: The Penn Treebank. Com-\nputational Linguistics, 19(2):313–330.\nG´abor Melis, Chris Dyer, and Phil Blunsom. 2018. On\nthe state of the art of evaluation in neural language\nmodels. Proceedings of the 6th International Con-\nference on Learning Representations (ICLR 2018).\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018. Regularizing and Optimizing LSTM\nLanguage Models. In Proceedings of the 6th Inter-\nnational Conference on Learning Representations\n(ICLR 2018).\nStephen Merity, Caiming Xiong, James Bradbury,\nand Richard Socher. 2017. Pointer Sentinel Mix-\nture Models. In Proceedings of the 5th Inter-\nnational Conference on Learning Representations\n(ICLR 2017).\nTomas Mikolov, Martin Karaﬁ ´at, Luk ´as Burget, Jan\nCernock´y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Proceed-\nings of the 11th Annual Conference of the Interna-\ntional Speech Communication Association (INTER-\nSPEECH 2010), pages 1045–1048.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed Representa-\ntions of Words and Phrases and their Compositional-\nity. In Advances in Neural Information Processing\nSystems 26 (NIPS 2013), pages 3111–3119.\nAndriy Mnih and Koray Kavukcuoglu. 2013. Learn-\ning Word Embeddings Efﬁciently with Noise-\nContrastive Estimation. In Advances in Neural\nInformation Processing Systems 26 (NIPS 2013),\npages 2265–2273.\n4609\nCourtney Napoles, Matthew Gormley, and Benjamin\nVan Durme. 2012. Annotated gigaword. In Pro-\nceedings of the Joint Workshop on Automatic Knowl-\nedge Base Construction and Web-scale Knowledge\nExtraction, pages 95–100.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies (NAACL 2018), pages 2227–2237.\nBoris T Polyak and Anatoli B Juditsky. 1992. Ac-\nceleration of Stochastic Approximation by Averag-\ning. SIAM Journal on Control and Optimization,\n30(4):838–855.\nOﬁr Press and Lior Wolf. 2017. Using the Output Em-\nbedding to Improve Language Models. In Proceed-\nings of the 15th Conference of the European Chap-\nter of the Association for Computational Linguistics\n(EACL 2017), pages 157–163.\nAlexander M. Rush, Sumit Chopra, and Jason Weston.\n2015. A Neural Attention Model for Abstractive\nSentence Summarization. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP 2015), pages 379–\n389.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation mod-\nels with monolingual data. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (ACL 2016), pages 86–96.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc V . Le, Geoffrey E. Hinton, and\nJeff Dean. 2017. Outrageously large neural net-\nworks: The sparsely-gated mixture-of-experts layer.\nIn Proceedings of the 5th International Conference\non Learning Representations (ICLR 2017).\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overﬁtting. Journal of Machine Learning Re-\nsearch, 15(1):1929–1958.\nRupesh Kumar Srivastava, Klaus Greff, and J ¨urgen\nSchmidhuber. 2015. Highway networks. In Pro-\nceedings of the Deep Learning Workshop in ICML\n15.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to Sequence Learning with Neural Net-\nworks. In Advances in Neural Information Process-\ning Systems 27 (NIPS 2014), pages 3104–3112.\nJun Suzuki, Sho Takase, Hidetaka Kamigaito, Makoto\nMorishita, and Masaaki Nagata. 2018. An empirical\nstudy of building a strong baseline for constituency\nparsing. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL 2018), pages 612–618.\nChristian Szegedy, Wei Liu, Yangqing Jia, Pierre Ser-\nmanet, Scott Reed, Dragomir Anguelov, Dumitru Er-\nhan, Vincent Vanhoucke, and Andrew Rabinovich.\n2015. Going deeper with convolutions. In Proceed-\nings of the 28th IEEE Conference on Computer Vi-\nsion and Pattern Recognition (CVPR 2015), pages\n1–9.\nSho Takase, Jun Suzuki, and Masaaki Nagata. 2017.\nInput-to-output gate to improve rnn language mod-\nels. In Proceedings of the Eighth International Joint\nConference on Natural Language Processing (IJC-\nNLP 2017), pages 43–48.\nLi Wan, Matthew Zeiler, Sixin Zhang, Yann L Cun, and\nRob Fergus. 2013. Regularization of Neural Net-\nworks using DropConnect. In Proceedings of the\n30th International Conference on Machine Learning\n(ICML 2013), pages 1058–1066.\nTsung-Hsien Wen, Milica Gasic, Nikola Mrk ˇsi´c, Pei-\nHao Su, David Vandyke, and Steve Young. 2015. Se-\nmantically Conditioned LSTM-based Natural Lan-\nguage Generation for Spoken Dialogue Systems. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing (EMNLP\n2015), pages 1711–1721.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W. Cohen. 2018. Breaking the softmax\nbottleneck: A high-rank RNN language model. In\nProceedings of the 6th International Conference on\nLearning Representations (ICLR 2018).\nWojciech Zaremba, Ilya Sutskever, and Oriol Vinyals.\n2014. Recurrent neural network regularization. In\nProceedings of the 2nd International Conference on\nLearning Representations (ICLR 2014).\nQingyu Zhou, Nan Yang, Furu Wei, and Ming Zhou.\n2017. Selective encoding for abstractive sentence\nsummarization. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL 2017), pages 1095–1104.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan\nKoutn´ık, and J¨urgen Schmidhuber. 2017. Recurrent\nHighway Networks. Proceedings of the 34th Inter-\nnational Conference on Machine Learning (ICML\n2017), pages 4189–4198.\nKonrad Zolna, Devansh Arpit, Dendi Suhubdy, and\nYoshua Bengio. 2018. Fraternal dropout. In Pro-\nceedings of the 6th International Conference on\nLearning Representations (ICLR 2018).\nBarret Zoph and Quoc V . Le. 2017. Neural Archi-\ntecture Search with Reinforcement Learning. In\nProceedings of the 5th International Conference on\nLearning Representations (ICLR 2017).",
  "topic": "Treebank",
  "concepts": [
    {
      "name": "Treebank",
      "score": 0.9402161836624146
    },
    {
      "name": "Computer science",
      "score": 0.8522053956985474
    },
    {
      "name": "Language model",
      "score": 0.7903103828430176
    },
    {
      "name": "Recurrent neural network",
      "score": 0.7278494238853455
    },
    {
      "name": "Machine translation",
      "score": 0.7176923155784607
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6281964778900146
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5950461030006409
    },
    {
      "name": "Connection (principal bundle)",
      "score": 0.5359718203544617
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.5012106895446777
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.4434024691581726
    },
    {
      "name": "Natural language processing",
      "score": 0.4240611791610718
    },
    {
      "name": "State (computer science)",
      "score": 0.42055660486221313
    },
    {
      "name": "Factorization",
      "score": 0.4151790142059326
    },
    {
      "name": "Headline",
      "score": 0.41401219367980957
    },
    {
      "name": "Artificial neural network",
      "score": 0.36039167642593384
    },
    {
      "name": "Algorithm",
      "score": 0.22011086344718933
    },
    {
      "name": "Programming language",
      "score": 0.1739043891429901
    },
    {
      "name": "Parsing",
      "score": 0.13306209444999695
    },
    {
      "name": "Mathematics",
      "score": 0.07072219252586365
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201537933",
      "name": "Tohoku University",
      "country": "JP"
    }
  ],
  "cited_by": 37
}