{
  "title": "Learn To Remember: Transformer with Recurrent Memory for Document-Level Machine Translation",
  "url": "https://openalex.org/W4229016505",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101199919",
      "name": "Yukun Feng",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A5104225489",
      "name": "Feng Li",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5025834228",
      "name": "Ziang Song",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A5101882088",
      "name": "Boyuan Zheng",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": "https://openalex.org/A5112315093",
      "name": "Philipp Koehn",
      "affiliations": [
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2141895568",
    "https://openalex.org/W2964289193",
    "https://openalex.org/W3204302053",
    "https://openalex.org/W2971347700",
    "https://openalex.org/W3171376244",
    "https://openalex.org/W3174160883",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W3103878009",
    "https://openalex.org/W3035520602",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W2136353104",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2964335437",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W2963506925",
    "https://openalex.org/W4241645538",
    "https://openalex.org/W2963792777",
    "https://openalex.org/W2952446148",
    "https://openalex.org/W2989066524",
    "https://openalex.org/W4298061300",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2962802109",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2962712961",
    "https://openalex.org/W2970529093",
    "https://openalex.org/W2133564696",
    "https://openalex.org/W3034351728",
    "https://openalex.org/W3203460706",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W1851962382",
    "https://openalex.org/W2595715041",
    "https://openalex.org/W2594633041",
    "https://openalex.org/W3173691187",
    "https://openalex.org/W2888159079",
    "https://openalex.org/W2962943802",
    "https://openalex.org/W3100623455",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3106298483"
  ],
  "abstract": "The Transformer architecture has led to significant gains in machine\\ntranslation. However, most studies focus on only sentence-level translation\\nwithout considering the context dependency within documents, leading to the\\ninadequacy of document-level coherence. Some recent research tried to mitigate\\nthis issue by introducing an additional context encoder or translating with\\nmultiple sentences or even the entire document. Such methods may lose the\\ninformation on the target side or have an increasing computational complexity\\nas documents get longer. To address such problems, we introduce a recurrent\\nmemory unit to the vanilla Transformer, which supports the information exchange\\nbetween the sentence and previous context. The memory unit is recurrently\\nupdated by acquiring information from sentences, and passing the aggregated\\nknowledge back to subsequent sentence states. We follow a two-stage training\\nstrategy, in which the model is first trained at the sentence level and then\\nfinetuned for document-level translation. We conduct experiments on three\\npopular datasets for document-level machine translation and our model has an\\naverage improvement of 0.91 s-BLEU over the sentence-level baseline. We also\\nachieve state-of-the-art results on TED and News, outperforming the previous\\nwork by 0.36 s-BLEU and 1.49 d-BLEU on average.\\n",
  "full_text": "Findings of the Association for Computational Linguistics: NAACL 2022, pages 1409 - 1420\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nLearn To Remember: Transformer with Recurrent Memory\nfor Document-Level Machine Translation\nYukun Feng1, Feng Li2, Ziang Song1, Boyuan Zheng1, and Philipp Koehn1\n1Department of Computer Science, Johns Hopkins University\n2Department of Computer Science, University of Illinois Urbana-Champaign\n{yfeng55, zsong17, bzheng12, phi}@jhu.edu, {fengl3}@illinois.edu\nAbstract\nThe Transformer architecture has led to sig-\nniﬁcant gains in machine translation. How-\never, most studies focus on only sentence-level\ntranslation without considering the context de-\npendency within documents, leading to the in-\nadequacy of document-level coherence. Some\nrecent research tried to mitigate this issue by\nintroducing an additional context encoder or\ntranslating with multiple sentences or even the\nentire document. Such methods may lose the\ninformation on the target side or have an in-\ncreasing computational complexity as docu-\nments get longer. To address such problems,\nwe introduce a recurrent memory unit to the\nvanilla Transformer, which supports the infor-\nmation exchange between the sentence and\nprevious context. The memory unit is recur-\nrently updated by acquiring information from\nsentences, and passing the aggregated knowl-\nedge back to subsequent sentence states. We\nfollow a two-stage training strategy, in which\nthe model is ﬁrst trained at the sentence level\nand then ﬁnetuned for document-level transla-\ntion. We conduct experiments on three popu-\nlar datasets for document-level machine trans-\nlation and our model has an average improve-\nment of 0.91 s-BLEU over the sentence-level\nbaseline. We also achieve state-of-the-art re-\nsults on TED and News, outperforming the\nprevious work by 0.36 s-BLEU and 1.49 d-\nBLEU on average.\n1 Introduction\nMost previous machine translation methods are de-\nsigned for sentence-level translation. Recent stud-\nies have shown that the effective use of contextual\ninformation between sentences can achieve better\nperformance in document-level machine translation\n(Garcia et al., 2015; Maruf and Haffari, 2018; Mi-\nculicich et al., 2018; Zhang et al., 2020; Bao et al.,\n2021). Built on the Transformer model (Vaswani\net al., 2017), a general approach is to incorporate\nneighboring sentence states (Tiedemann and Scher-\nrer, 2017; Zheng et al., 2020) into the attention\nmechanism, which has also been widely used in\nmany long sequence modeling methods (Dai et al.,\n2019; Rae et al., 2020; Yang et al., 2019; Belt-\nagy et al., 2020). Zhang et al. (2018); Maruf et al.\n(2019) have introduced an additional context en-\ncoder to solve the limitation of sentence-level trans-\nlation, which, however, is separated from the orig-\ninal translation model and context states is only\napplied on the source side. Other works (Junczys-\nDowmunt, 2019; Scherrer et al., 2019; Zhang et al.,\n2020; Bao et al., 2021) concatenated sentences or\nthe entire document and feed into the attention\nmodule of the Transformer. Since more extended\ncontexts may confound attention on meaningful\nportions of the current sentence, the model is dif-\nﬁcult to select valuable inputs from extra contexts\nto navigate the redundancy of information. Such\nmethods also suffer from the quadratically increas-\ning complexity when documents get longer.\nWe solve such problems by introducing a mem-\nory mechanism to recurrently integrate contextual-\nized knowledge from intermediate state in Trans-\nformer layers. As recurrent memory has been\nwidely researched since RNN (Rumelhart et al.,\n1986), which has been incorporated with Trans-\nformer by Transformer-XL (Dai et al., 2019) and\nfurther extended by Rae et al. (2020) who compress\nprevious states into a two-layer hidden memory. In\nour approach, we update the memory through an at-\ntention module to select practical information from\nsentences and reduce the context space into mul-\ntiple dense vectors in the memory. Besides, we\nuse another attention module to pass the knowl-\nedge retained in the memory back to the sentence\nstate in the next step. Such information exchange\nis expected to convey contextualized dependency\nbetween sentences. This memory mechanism can\nbe applied in each layer for both the source and\ntarget documents, and our study shows that incor-\nporating memory only in the last layer achieves the\n1409\nbest performance. Also, as sentences are ordered\nin documents, our model reads one sentence pair at\neach step, keeping the computational cost as same\nas the sentence-level translation.\nWe experiment across three widely used datasets\nfor document-level translation: TED, NEWS, and\nEuroparl, and evaluate our model with s-BLEU\nand d-BLEU. We ﬁrst train a vanilla Transformer\non sentence-level translation as the baseline and\nﬁnetune the model for the documents by initial-\nizing the memory mechanism to the Transformer.\nOur model outperforms previous SOTA work by\n0.5 s-BLEU and 2.30 d-BLEU on TED, and 0.21\ns-BLEU and 0.57 d-BLEU on News. We do not\nachieve the SOTA result on Europarl, which might\nbe caused by the different results between the base-\nlines for sentence-level translation. However, we\nfurther evaluate the improvement of previous works\nfrom their reported baseline Transformer, and we\nachieve the most relative gain on all three datasets.\nWe also analyze our model from the memory us-\nage, long-range effect, context dependency, and\ncomputational complexity, and demonstrate the ef-\nfectiveness and efﬁciency of our approach in the\ngeneral understanding of the document machine\ntranslation.\nOverall, this paper makes several contributions:\n(i) Our work reduces the contextualized knowledge\nspace of sentences states to multiple dense vectors,\nand considers the sentence dependency for both\nsource and target documents, while keeping com-\nputational complexity in sentence-level. (ii) Our\nmodel signiﬁcantly improves the sentence level\nbaseline by 0.91 s-BLEU average and achieved the\nSOTA results on TED and News. (iii) Our model\nshows the effective use of memory, long-range in-\nﬂuence, context-dependency across sentences, and\ndecoding efﬁciency through convincing analysis.\n2 Related Works\nRecurrent Sequence Modeling RNN (Rumel-\nhart et al., 1986) was the ﬁrst class of models that\nintroduced hidden states as the memory in neural\nmodels. Although improved on sequential-oriented\ntasks, RNN has unsatisfactory learning of long-\nterm information due to gradient vanishing and\nexplosion. LSTM (Hochreiter and Schmidhuber,\n1997) improved RNN by introducing gate mecha-\nnisms to selectively retain knowledge at each step.\nThis RNN variant dominated NLP models until\nthe Transformer replaced the memory unit with a\nself-attention mechanism and achieved great suc-\ncess in a wide range of NLP applications. Although\nwe cannot deny the robustness and effectiveness of\nthe Transformer model, the quadratically increased\ncomputational cost as the increase of token num-\nbers makes Transformer unable to ﬁt the long-range\nsequence. Some studies (Parmar et al., 2018; Child\net al., 2019; Beltagy et al., 2020; Ainslie et al.,\n2020; Qiu et al., 2020; Zaheer et al., 2020; Martins\net al., 2021) try to mitigate this issue by reducing\nthe complexity of the attention module. However,\nsuch work still suffers from the problems by unlim-\nited the document length and the document model-\ning is hard to solve.\nTransformer-XL (Dai et al., 2019) broke this\ndilemma by introducing the recurrent memory\ninto Transformer-based models. It cached previ-\nous hidden sentences computation and mapped\nsuch states to subsequent sentences states. The-\noretically, Transformer-XL could handle inﬁnite\nlength text but storing uncompressed hidden state\nrequires tremendous memory space, which im-\npeded Transformer-XL from good performance\non dealing with practical long-sequence tasks.\nThe Compressive Transformer (Rae et al., 2020)\nfurther addressed this problem by mapping the\nevicted hidden state from cached memory to a more\ncompressed representation. However, two-layer\ncaching still requires a huge memory space and\nmay be improved with trainable memories.\nDocument Machine Translation Machine\nTranslation has been a widely researched area\nfor decades. A series of models have addressed\nvarious translation problems (Koehn et al., 2003;\nKalchbrenner and Blunsom, 2013; Bahdanau\net al., 2015; Luong et al., 2015). As most of\nthem target translation at the sentence level,\ndocument-level translation poses a fundamental\nchallenge requiring models to pass intra-sentential\ninformation throughout consecutive sequences of\nsentences, and it has been addressed by Gong et al.\n(2011); Hardmeier et al. (2013); Pouget-Abadie\net al. (2014); Garcia et al. (2015); Koehn and\nKnowles (2017); Läubli et al. (2018); Agrawal\net al. (2018) among others.\nRecent studies have attempted to incorporate\nadditional contextual information into the Trans-\nformer structure to improve the performance of\nneural machine translation models further. The in-\ntuitive way is to leverage neighboring sentences\nfrom paragraphs or the documents (Tiedemann and\n1410\nFigure 1: An overview of the model architecture, where E and D refers to Encoder and Decoder respectively.\nScherrer, 2017; Maruf and Haffari, 2018; Zheng\net al., 2020), demonstrating the effectiveness of the\nadditional contexts. Speciﬁcally, in the ﬁrst class\nof methodologies for document-level translation,\nindependent from the architecture of vanilla Trans-\nformer processing current sentences, some studies\n(Miculicich et al., 2018; Zhang et al., 2018; Maruf\net al., 2019; V oita et al., 2019a,b; Ma et al., 2020;\nDonato et al., 2021) introduces context-aware com-\nponents only attend to source or target contexts and\nusually jointly train with the rest of the network\nfrom scratch. The second class of models follows\nthe pattern of concatenating multiple sentences for\ntranslation (Agrawal et al., 2018; Scherrer et al.,\n2019; Junczys-Dowmunt, 2019; Zhang et al., 2020).\nSuch a method is expected to capture the contextual\ncorrelations between sentences. However, one of\nits drawbacks is the quadratically increased compu-\ntational complexity in the face of longer contexts se-\nquences. Also, longer sequences usually confound\ndocument-level attention and sometimes even over-\nlook key information on the current sentences. Bao\net al. (2021) uses group masks to introduce local-\nity constraints to reinforce sentence information\nin multi-head attention to resolve the confounding\nissue in long contexts.\nOur work incorporates the idea of the recurrent\nmemory to document-level machine translation. It\nfollows the locality assumptions by reducing the\ncontext space into multiple memory vectors and\npasses dependencies between sentences. The mech-\nanism to update and output memory is similar to\nmodels which store cached bilingual sentence pairs\nin the memory to enhance the sentence-level trans-\nlation (Feng et al., 2017; He et al., 2021; Jiang et al.,\n2021). We believe our approach is intuitive to ef-\nﬁciently store sentence states and transfer context\ninformation across sentences.\n3 Approach\nOur model is shown in Figure 1. Additional to\nthe vanilla Transformer, we introduce a contextual\nmemory unit and two attention modules to manipu-\nlate the memory deﬁned as Update Attention and\nOutput Attention. These modules can be applied at\neach layer in both the encoder and decoder.\nAs input sentences are ordered from left to right\nin the document, our model only reads one sen-\ntence every time. The memory is expected to store\ncontextualized information from the input sentence\nstates and convey such knowledge to the next sen-\ntence. At each step, the Update Attention step\nmaps the contextual information from the sentence\nstate to the memory, and updates the memory to the\nnext step. Meanwhile, the Output Attention step\nfuses the information from the current sentence and\nthe contextual memory, and outputs the aggregated\nknowledge to the remaining modules of the layer.\nFormally, we deﬁneht as the sentence state from\nself-attention module in Transformer layer, and Mt\nrefers to the contextual memory M at step t, where\nt refers to the index oftth sentence in the document.\nMt and ht are updated and outputted as:\nMt+1 = UpdateAttention(Mt,ht)\n˜ht = OutputAttention(Mt,ht)\n(1)\n1411\n3.1 Contextual Memory\nMemory M ∈RdM×dmodel where dmodel refers to\nthe hidden dimension and dM is a hyper-parameter,\nindicating how many vectors will be allocated for\nmemory. To avoid the redundancy of memory\nspace, we set dM to 16. Detailed analysis is dis-\ncussed later.\n3.2 Update Attention\nWe update contextual memory through an atten-\ntion module (Vaswani et al., 2017). Attention is a\nmapping function between input vectors of query\n(Q) and key-value (K-V) pairs. The output is the\nweighted sum of values with corresponding scores.\nAttention(Q,K,V) = Softmax(QKT\n√dk\n)V\nMulti-Head attention extends the vanilla attention\nby projecting input vectors ( Q,K,V) to differ-\nent representation subspaces, and attention is per-\nformed in parallel in each head. Attention outputs\nfrom multiple heads will be concatenated and pro-\njected to the expected space.\nMHA(Q,K,V) = Concat(head1,.., headn)Wo\nheadi = Attention(QWq\ni ,KWk\ni ,VWv\ni )\nwhere dk is the hidden dimension of the K, Wq,\nWk, Wv ∈Rdmodel×dh, and Wo ∈Rn×dh×dmodel\nare learnable parameters. dmodel and dh refer to\nthe hidden dimension of the model and each head.\nTo update the contextual memory Mt to next\nstep, sentence state ht is mapped to Mt through\nthe Multi-Head Attention. Both the memory and\ncontext state are projected into different sub-spaces\nand contextualized knowledge is expected to be\nmapped to each memory vector from different per-\nspectives. The memory at step t is updated as:\n˜Mt = AddNorm(MHA(Mt,ht,ht)) (2)\nA Feed-Forward Network is then used to fur-\nther enhance the memory representation from the\nattention output.\nMt+1 = AddNorm(FeedForward(˜Mt)) (3)\nIn the memory matrixM, each vector is expected\nto select contextualized information from different\nperspectives. However, it is hard to distinguish\nsuch vectors since they do not have actual posi-\ntional meanings, and the same key-value pairs are\nmapped to these vectors in the attention phase re-\nsulting in the same representation in each memory\nvector. To solve such a problem, we use the po-\nsitional encoding PE() as introduced in Vaswani\net al. (2017) to differentiate multiple memory vec-\ntors. M is added by such position-level bias in each\nupdate phase.\nMt = Mt + PE(Mt) (4)\n3.3 Output Attention\nTo map the contextualized knowledge fromMt to\nthe sentence state ht, multi-head attention is used\nto take the representation of ht and Mt as query\nand key-value, respectively.\n˜ht = MHA(ht,Mt,Mt) (5)\n˜ht will be passed to the subsequent modules in the\nTransformer layer.\nSimilar approaches have been discussed in pre-\nvious works. Simply increasing the context space\ndoes not help but introduces a lot of noise. Instead\nof incorporating multiple sentences to the context\nattention, we compress contextualized information\ninto multiple memorized vectors and map such vec-\ntors back to the sentence state at the next step. We\nﬁnd that both the BLEU score and the informa-\ntion gained from the context attention space do not\nincrease when the memory length increases from\n64 to 128. Therefore, a large context space in M\nseems redundant for the model to learn, and we ﬁnd\ndM = 16for the most effectiveness and efﬁciency.\n3.4 Document Neural Machine Translation\nIn the task of document-level machine translation,\nthe source and target documents are represented as\nsequences of sentences X ={xt|1 ≤t ≤n}, and\nY ={yt|1 ≤t ≤n}respectively, where t refers\nto the sentence index. Given a vanilla Transformer\nand its parameters θ, the objective is to maximize\nthe target document probability conditioned on the\nsource document.\nargmax\nθ\nP(Y|X,θ)\nOur approach recurrently translates an ordered doc-\nument sentence by sentence, and the objective is:\nargmax\n˜θ\n∏\nt\nP(Yt|X⩽t,Y<t,˜θ)\nwhere ˜θrefers to Transformer parameters including\nMemory, Update Attention and Output Attention.\n1412\nModel TED News Europarl\ns-BLEU d-BLEU s-BLEU d-BLEU s-BLEU d-BLEU\nVaswani et al. (2017) 23.10 - 22.40 - 29.40 -\nMiculicich et al. (2018) 24.58 - 25.03 - 28.60 -\nMaruf et al. (2019) 24.42 - 24.84 - 29.75 -\nMa et al. (2020) 24.87 - 23.55 - 30.09 -\nZheng et al. (2020) 25.10 - 24.91 - 30.40 -\nBao et al. (2021) 25.12 (+0.30) 27.17 25.52 (+0.33) 27.11 32.39 (+1.02) 34.08\nSentence Baseline 24.73 - 25.18 - 30.13 -\nFinetune on Sentence 25.62 (+0.89) 29.47 25.73 (+0.55) 27.78 31.41 (+1.28) 33.50\nTable 1: Experiments results of BLEU scores on three datasets. The improvement from the Transformer baseline\nfor previous models are also reported as in \"()\". It indicates the score improved from sentence-level translation\nprovided by their implementations. Results are averaged from two runs.\nData # of Docs # of Sents/Doc\nTED 1.7K/93/23 123/98/105\nNews 6.1K/71/155 40/25/20\nEuroparl 118K/240/360 14/15/14\nTable 2: Dataset Statistics for Train/Valid/Test\nAs suggested by Beltagy et al. (2020); Bao et al.\n(2021), context would be better applied in higher\nlayers and keep only local information in lower\nlayers. Therefore we only apply the memory unit\nM in the top layer of encoder and decoder, and\nin lower layers, we keep using the original Trans-\nformer structure. Analysis regarding the location\nof memory is discussed in Section 5.1.\nTraining During training, our model takes an in-\nput of xt and yt, which refer to sequences of tokens\nof the tth sentence in source and target documents.\nMemory unit M is initialized trainable parameters\nbefore the ﬁrst input of each document, and it will\nbe updated after each input sentence pair, which\nare batched as the sentence order in the document.\nFor computational convenience, the gradients are\nonly back-propagated to the current sentence and\nthe most recent sentence in each update step, and\nwe stop the gradient for M before it is passed to the\nnext step.\nInference In the decoding phase, our model\ntranslates the source document sentence by sen-\ntence. In the generation of each sentence, tokens\nare decoded in an auto-regressive order until the\nstop sign or exceeds the max length. The memory\nM will not be updated until the complete sentence\nis generated since the update of M depends on all\ntokens in the current sentence. If M is updated\nafter each token generation, the attention space in\nthe output attention does not represent the com-\nplete contextualized information of the expected\nsentence. The computational complexity keeps in\nsentence-level since we only feed one sentence ev-\nery time, and there is no cache vector besides M.\n4 Experiment\n4.1 Datasets\nWe experiment across three widely used datasets\nfor English→German document translation.\nTED Training data for TED comes from\nIWSLT’17. We use tst2016-2017 as test set and a\nheld-out set from training as valid.\nNews The corpus comes from News Commen-\ntary v11. We use tst2016-2017 as test set and a\nheld-out set from training as valid.\nEuroparl Train, valid and test sets are extracted\nfrom the corpus Europarl v7, as mentioned in\n(Maruf et al., 2019).\nDetailed statistics for the datasets is in Table 2.\nMoses (Koehn et al., 2007) is used for data process-\ning and BPE (Sennrich et al., 2016) is used with\nvocab-size of 30K for all datasets.\n4.2 Settings\nWe adopt Transformer model with the transformer-\nbase conﬁgurations as the baseline, which has six\nlayers with a hidden size of 512 and an interme-\ndiate size of 2048. Token embedding is shared\nfor source and target languages, and token indexes\nare encoded with a learnable embedding matrix.\nWe ﬁrst train a baseline model with vanilla Trans-\nformer architecture for sentence-level translation\nand ﬁnetune our model based on the sentence-level\nbaseline. We use the AdamW optimizer with an ini-\n1413\n8 16 64 128 512\nMemory Size\n0.0\n0.2\n0.4\n0.6\n0.8Information Gain\nOutput-Attention-IG\nUpdate-Attention-IG\ns-BLEU\n25.40\n25.45\n25.50\n25.55\n25.60\n25.65\ns-BLEU Score\nFigure 2: Evaluation on TED with different memory\nsizes.\ntial learning rate of 5 ×10−4 and warm-up steps of\n4000 for training sentence-level baseline. The drop-\nout rate is set to 0.3 for TED and News and 0.1 for\nthe Europarl. As for ﬁnetuning after sentence-level\nTransformer, the learning rate is set as3 ×10−4 for\nnewly initialized parameters and 6 ×10−5 for pre-\ntrained parameters, and warm-up steps of 1000 are\nset for TED and 2000 for News and Europarl. The\ndrop-out rate is set to 0.1 for the Europarl and 0.2\nfor the TED and News during ﬁnetuning. We also\napply gradient accumulation, and detailed studies\nare discussed in the section 5.3. Models are trained\nwith a patience of 5 for both sentence-level and\ndocument-level. We use the beam size of 5 during\ninference and compute the BLEU score in a max\norder of 4 after removing BPE-tokens. s-BLEU\nand d-BLEU are used as evaluation metrics, where\ns-BLEU refers to the BLEU score for sentences,\nand d-BLEU is the score for documents.\n4.3 Results\nExperiment results are shown in Table 1. Our\nmethod shows consistent improvements over three\ndatasets from sentence-level Transformer. We\nachieve the state-of-the-art results of s-BLEU of\n25.62 and d-BLEU of 29.47 on TED and s-BLEU\nof 25.73 and d-BLEU of 27.78 on News . Though\nour results do not outperform G-Transformer (Bao\net al., 2021) on Europarl, we think the difference\nmostly comes from the gap between sentence-\nlevel baselines. Such difference may be caused\nby the implementation framework and computing\nresources, which they use Fairseq library and mul-\ntiple GPUs, while we adopt the code from Hug-\ngingFace and only a single 1080-Ti GPU is used\nfor our training. We further report the score of\nworks gained from their reported baseline, and our\nmodel makes the greatest improvement on all three\nSide Index s-BLEU d-BLEU\nSource+Target 0-1 25.31 29.13\nSource+Target 2-3 25.30 29.23\nSource+Target 4-5 25.43 29.22\nSource+Target 5 25.62 29.47\nSource Only 5 25.42 29.33\nTarget Only 5 25.43 29.25\nTable 3: Evaluation on TED with memory on differ-\nent sides and layers. We adopt the 6-layer Transformer\nmodel ﬁnetued on the sentence-level baseline, and 0\nrefers to the ﬁrst layer, and 5 refers to the last layer.\ndatasets. Overall, the results could demonstrate\nthe advantages of our method in the general under-\nstanding of the document machine translation.\n5 Analysis\nIn this section, we discuss our model from memory\nusage, long-range modeling, context effect, and\ncomputational complexity, respectively. Experi-\nments are conducted with the model ﬁnetuned on\nthe sentence baseline and evaluated on the TED,\nsince TED has the most average sentence number\nper document, which is more likely to reﬂect the\nperformance of our model for long documents.\n5.1 Discussion of Memory\nMemory Size Memory size is evaluated through\ninformation gain (IG) between the random initial-\nized memory and well trained memory. It is cal-\nculated from attention maps in Update Attention\nand Output Attention. IG from Update Attention\nindicates the difference of selected information in\nthe memory, and IG from Output Attention refers\nto how much contextualized knowledge in mem-\nory is mapped to the next sentence state. Figure\n2 shows IG keeps increasing as memory size in-\ncreases from 8 to 64, but it dramatically drops at the\nsize of 128 and 512. While increasing the memory\nsize can ﬁt more contextual information, an exces-\nsively large memory space is likely to introduce\nredundant noise. Therefore, it indicates that con-\ntextualized knowledge should be better distributed\ninto a relatively dense space. Based on the corre-\nsponding s-BLEU score, we set memory size to 16\nin all other experiments for the most effectiveness.\nMemory Side To analyze the effect of the mem-\nory on source and target documents, we set the\nmemory on encoder, decoder and both sides respec-\ntively. We ﬁnd that it is not only necessary to have\n1414\nthe memory to convey the dependency between\nsentences on the source side but also in the decod-\ning process for the target document. As shown in\nTable 3, applying the memory on either side can\noutperform the baseline but the model achieves bet-\nter scores when incorporating the memory on both\nsides. It indicates the necessity of contextualized\ninformation for both source and target documents.\nMemory Position Previous work (Bao et al.,\n2021; Beltagy et al., 2020) has shown that Trans-\nformer lower layers are more likely to have local\ninformation while the context is better incorporated\ninto higher layers. We set the memory in lower, in-\ntermediate, and higher layers respectively. The\nresults as shown in Table 3 are consistent with the\nclaim. Applying memory in higher layers outper-\nforms the others, and it is even better to have it on\nonly the top layer, which satisﬁes that the model is\nmore likely to focus on the locality on lower layers\nand fuse the contextualized information on the top.\n5.2 Discussion of Long Dependency\nMetric Breakdown To ﬁnd out on what kind\nof sentences our model outperforms the sentence-\nlevel Transformer, we evaluate the TED dataset\nwith respect to the sentence index in the document.\nSentences are ordered fed into the model. We com-\npute and average the s-BLEU for sentences at each\nsentence index in the document. We further average\nthe scores for every ten index range. As in Figure\n3, the x-axis refers to the index range of sentences\n(e.g., 20 refers to sentences with indexes from 10\nto 20), and the y-axis indicates the s-BLEU differ-\nence between our model and sentence Transformer.\nOur model has consistently greater performance,\nespecially for sentences in later part of documents,\nindicating our model has the superiority than the\nsentence-level Transformer on longer document\ntranslation and long-range modeling.\nLong-Range Inﬂuence We also analyze the\nlong-range dependency of our model through gra-\ndient attribution test introduced by Ancona et al.\n(2018); Sundararajan et al. (2017). The gradient at-\ntribution test reﬂects the signiﬁcance of the model\ninput feature to its output prediction. We perform\nthis test by calculating the gradients of our well-\ntrained model on the test set of TED. Since sen-\ntences are ordered when fed into the model, evalu-\nating previous sentences’ gradient attribution to the\ncurrent sentence infers if the model supports the\nlong-range dependency. More formally, we deﬁne\n10 20 30 40 50 60 70 80 90 100 100+\nSentence Index\n0.00%\n2.00%\n4.00%\n6.00%\n8.00%\n10.00%\n12.00%\n14.00%\n16.00%Percentage of Data\nPercentage of Data\ns-BLEU Difference\n0.5\n0.0\n0.5\n1.0\n1.5\n2.0\ns-BLEU Difference\nFigure 3: TED datset separated by sentences from dif-\nferent indexes in documents, evaluated with Sentence-\nTransformer and Context-Aware Model.\n10 20 30 40 50 60 70 80 90 90+\nSentence Distance\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2Gradient\n1e 3\nFigure 4: TED dataset evaluated by gradients com-\nputed from different sentences ranges. x-axis refers to\nthe difference between the sentence indexes for gradi-\nent calculation and loss computation.\nthe gradient of the previous sentence i computed\nby the loss propagated from current sentence j as:\nG(Senti,Sentj).\nSpeciﬁcally, the gradient of a certain token in\nprevious sentences is retrieved from its correspond-\ning embedding weight. We conducted experiments\nfor different sentence ranges k for the test with ten\nsentences intervals, and the gradient for each range\nk is computed as:\nScore(k) = Avg(\nD∑\nd=1\nSd∑\ns=1\ns+k+10∑\ni=s+k\nG(Sents,Senti))\nwhere Drefers to number of documents, Sd refers\nto number of sentences in Document d. To prevent\nthe gradient attribution accumulated by the same\ntoken within the evaluated range, only unique to-\nkens within this range are considered. As shown\nin Figure 4, our model has gradients propagated to\nsentence tokens even by 90+ sentences from the\ncomputed loss, indicating our model does have the\nability for long-range sequence modeling.\n1415\n0 1000 2000 3000 4000 5000\nUpdate Steps\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8Loss\nOptimization Window Size: 1-10\n0 1000 2000 3000 4000 5000\nUpdate Steps\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8Loss\nOptimization Window Size: 10-20\n0 1000 2000 3000 4000 5000\nUpdate Steps\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8Loss\nOptimization Window Size: 40-50\n0 1000 2000 3000 4000 5000\nUpdate Steps\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8Loss\nOptimization Window Size: 80-90\n0 1000 2000 3000 4000 5000\nUpdate Steps\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8Loss\nOptimization Window Size: Full\nFigure 5: Training Loss on TED Dataset, with different optimization window sizes\nFigure 6: Attention map from Update Attention, each\ntoken at sentence t is mapped to each memory vector.\nFigure 7: Attention map from Output Attention, mem-\nory vectors are mapped to each token in sentence t+1.\n5.3 Discussion of Context\nConvergence Our model is trained concerning\nthe sentence order in the document. We ﬁnd the\nmodel hard to converge during training as the loss\noscillates within a wide range. Because of the var-\nious distribution of consecutive sentences in doc-\numents, the directions of continuing optimization\nsteps vary greatly, resulting in an unstable con-\nvergence curve. To mitigate this issue, we use\ngroup optimization to update the model, consider-\ning the dependency among sentences. Speciﬁcally,\na number from the optimization window is ran-\ndomly sampled, and the gradients are accumulated.\nThe model will not be updated until the accumu-\nlated steps reach the sampled number. We conduct\nexperiments with different optimization window\nsizes for the update of 5000 steps, and the loss\ncurves are shown in Figure 5, where full means the\ntotal number of sentences in the document. The\nresult shows that the model converges faster and\nmore stable with increasing optimization window\nsize. Such improvement beneﬁts from the grouped\nupdate steps concerning the difference of contextu-\nalized distribution among sentences.\nDependency Across Sentences We evaluate the\nattention maps from Update Attention and Output\nAttention to determine what contextualized infor-\nmation is passed in and out from memory. In Fig-\nure 6, tokens from tth sentence are mapped to each\nmemory vector, and the 8th memory vector has a\nsubstantial attention weight on token \"Frau\". Fig-\nure 7 shows memory vectors are mapped back to\nthe following sentence and the token \"sie\" has a\nhigh probability on the8th memory vector. German\nwords \"Frau\" and \"sie\" refer to \"Mrs\" and \"she\"\nin English. Hence, the memory mechanism has\nthe ability to parse the word dependency between\nsentences at different steps.\n5.4 Discussion of Complexity\nWe further analyze our model’s space and time\ncomplexity during the inference phase. Since we\nonly evaluate the decoding speed and memory ef-\nﬁciency in this case, we use dummy tokens to per-\nform the inference. We randomly generate a se-\nquence of tokens as the source inputs and let the\nmodel decode the same number of tokens as the tar-\nget. We compare our model with both the sentence-\nlevel Transformer and document-level Transformer.\nFor the sentence-level Transformer, we split the\nsequence of tokens into chunks, and each chunk\nhas a length of 100. The decoding complexity is\n1416\n100 200 500 1000 2000 4000 6000 8000 10000\nNumber of T okens\n0\n1000\n2000\n3000\n4000\n5000\n6000\n7000Peak GPU Memory (MB)\n0.6\n0.8\n1.0\n1.2\n1.4\nSeconds Per T oken\n1e 2\nMemory-Ours\nMemory-Transformer-Sent\nMemory-Transformer-Doc\nSpeed-Ours\nSpeed-Transformer-Sent\nSpeed-Transformer-Doc\nFigure 8: Space and Time Complexity for different\nnumber of tokens during inference.\nevaluated over all chunks. For the document-level\nTransformer, we use the entire sequence of tokens\nas the source input and evaluate the complexity of\ndecoding the entire target sequence. Similar to the\nsentence-level Transformer, our model is evaluated\nby the chunk by chunk decoding, and meanwhile,\nwe keep the contextual memory updated. As shown\nin Figure 8, our model keeps the same space com-\nplexity as the sentence-level Transformer and takes\na slightly more time cost because of the update of\ncontextual memory. However, the document-level\nTransformer has an increasing cost for both space\nand time complexity, especially when the target\nsequence has a length greater than 1,000 tokens.\nOverall, results have shown the decoding efﬁciency\nof our model, which keeps the computational com-\nplexity as low as the sentence-level Transformer,\neven in the case of over thousands of tokens.\n6 Conclusion\nThis paper introduces a memory unit that recur-\nrently maps information into and out of Trans-\nformer intermediate states and addresses the lim-\nitation about the context dependency and com-\nputational complexity in document-level machine\ntranslation. We have achieved the SOTA score on\nTED and News and a great improvement from the\nsentence-level baseline. Our model demonstrates\nthe effectiveness and efﬁciency of reduced mem-\nory space, context dependency for both source and\ntarget document, and long range inﬂuence across\ndocuments. The limitation of our work is the train-\ning cost since we accumulate the update steps and\nretain the graph for memory update at each step.\nOur work does not conduct experiments for pre-\ntrained settings due to the time limitation. How-\never, it should be easy to apply our method to any\nTransformer-based pretrained models, such as Liu\net al. (2020). Also, this paper only experiments\non document-level machine translation, and future\nworks may apply this approach for other tasks that\nneed long-range sequence modeling.\nAcknowledgements\nWe sincerely thank the reviewers from ACL Rolling\nReview for their helpful feedback, and the sug-\ngestions regarding ﬁgure plotting from Senior UI\ndesigner Xiangru Chen. We also appreciate the\nsupport of the computing resources from the Cen-\nter for Language and Speech Processing (CLSP) at\nJohns Hopkins University (JHU).\nReferences\nRuchit Agrawal, Marco Turchi, and Matteo Negri.\n2018. Contextual handling in neural machine trans-\nlation: Look behind, ahead and on both sides.\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs\nin transformers. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 268–284, Online. Asso-\nciation for Computational Linguistics.\nMarco Ancona, Enea Ceolini, Cengiz Öztireli, and\nMarkus Gross. 2018. Towards better understanding\nof gradient-based attribution methods for deep neu-\nral networks. In International Conference on Learn-\ning Representations.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2015. Neural machine translation by\njointly learning to align and translate. CoRR,\nabs/1409.0473.\nGuangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing\nChen, and Weihua Luo. 2021. G-transformer for\ndocument-level machine translation. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 3442–3455,\nOnline. Association for Computational Linguistics.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nRewon Child, Scott Gray, Alec Radford, and Ilya\nSutskever. 2019. Generating long sequences with\nsparse transformers.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\n1417\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nDomenic Donato, Lei Yu, and Chris Dyer. 2021. Di-\nverse pretrained context encodings improve docu-\nment translation. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1299–1311, Online. Association for\nComputational Linguistics.\nYang Feng, Shiyue Zhang, Andi Zhang, Dong Wang,\nand Andrew Abel. 2017. Memory-augmented neu-\nral machine translation. In Proceedings of the 2017\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1390–1399, Copenhagen,\nDenmark. Association for Computational Linguis-\ntics.\nEva Martínez Garcia, Cristina España-Bonet, and Lluís\nMàrquez. 2015. Document-level machine transla-\ntion with word vector models. In Proceedings of\nthe 18th Annual Conference of the European Asso-\nciation for Machine Translation , pages 59–66, An-\ntalya, Turkey.\nZhengxian Gong, Min Zhang, and Guodong Zhou.\n2011. Cache-based document-level statistical ma-\nchine translation. In Proceedings of the 2011 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 909–919, Edinburgh, Scotland,\nUK. Association for Computational Linguistics.\nChristian Hardmeier, Sara Stymne, Jörg Tiedemann,\nand Joakim Nivre. 2013. Docent: A document-level\ndecoder for phrase-based statistical machine transla-\ntion. In Proceedings of the 51st Annual Meeting of\nthe Association for Computational Linguistics: Sys-\ntem Demonstrations, pages 193–198, Soﬁa, Bulgaria.\nAssociation for Computational Linguistics.\nQiuxiang He, Guoping Huang, Qu Cui, Li Li, and\nLemao Liu. 2021. Fast and accurate neural machine\ntranslation with translation memory. In Proceed-\nings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 3170–3180,\nOnline. Association for Computational Linguistics.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nShu Jiang, Rui Wang, Zuchao Li, Masao Utiyama, Ke-\nhai Chen, Eiichiro Sumita, Hai Zhao, and Bao liang\nLu. 2021. Document-level neural machine transla-\ntion with associated memory network.\nMarcin Junczys-Dowmunt. 2019. Microsoft translator\nat WMT 2019: Towards large-scale document-level\nneural machine translation. In Proceedings of the\nFourth Conference on Machine Translation (Volume\n2: Shared Task Papers, Day 1), pages 225–233, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nNal Kalchbrenner and Phil Blunsom. 2013. Recurrent\ncontinuous translation models. In Proceedings of\nthe 2013 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1700–1709, Seattle,\nWashington, USA. Association for Computational\nLinguistics.\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris\nCallison-Burch, Marcello Federico, Nicola Bertoldi,\nBrooke Cowan, Wade Shen, Christine Moran,\nRichard Zens, Chris Dyer, Ond ˇrej Bojar, Alexandra\nConstantin, and Evan Herbst. 2007. Moses: Open\nsource toolkit for statistical machine translation. In\nProceedings of the 45th Annual Meeting of the As-\nsociation for Computational Linguistics Companion\nVolume Proceedings of the Demo and Poster Ses-\nsions, pages 177–180, Prague, Czech Republic. As-\nsociation for Computational Linguistics.\nPhilipp Koehn and Rebecca Knowles. 2017. Six chal-\nlenges for neural machine translation. In Proceed-\nings of the First Workshop on Neural Machine Trans-\nlation, pages 28–39, Vancouver. Association for\nComputational Linguistics.\nPhilipp Koehn, Franz J. Och, and Daniel Marcu. 2003.\nStatistical phrase-based translation. In Proceedings\nof the 2003 Human Language Technology Confer-\nence of the North American Chapter of the Associa-\ntion for Computational Linguistics, pages 127–133.\nSamuel Läubli, Rico Sennrich, and Martin V olk. 2018.\nHas machine translation achieved human parity? a\ncase for document-level evaluation. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4791–4796,\nBrussels, Belgium. Association for Computational\nLinguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natu-\nral Language Processing, pages 1412–1421, Lisbon,\nPortugal. Association for Computational Linguistics.\nShuming Ma, Dongdong Zhang, and Ming Zhou. 2020.\nA simple and effective uniﬁed encoder for document-\nlevel machine translation. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 3505–3511, Online. As-\nsociation for Computational Linguistics.\n1418\nPedro Henrique Martins, Zita Marinho, and André F. T.\nMartins. 2021. ∞-former: Inﬁnite memory trans-\nformer.\nSameen Maruf and Gholamreza Haffari. 2018. Docu-\nment context neural machine translation with mem-\nory networks. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers) , pages 1275–\n1284, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nSameen Maruf, André F. T. Martins, and Gholamreza\nHaffari. 2019. Selective attention for context-aware\nneural machine translation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 3092–3102, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas,\nand James Henderson. 2018. Document-level neural\nmachine translation with hierarchical attention net-\nworks. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 2947–2954, Brussels, Belgium. Association\nfor Computational Linguistics.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. 2018. Image transformer. In Proceedings\nof the 35th International Conference on Machine\nLearning, volume 80 of Proceedings of Machine\nLearning Research, pages 4055–4064. PMLR.\nJean Pouget-Abadie, Dzmitry Bahdanau, Bart van Mer-\nriënboer, Kyunghyun Cho, and Yoshua Bengio. 2014.\nOvercoming the curse of sentence length for neu-\nral machine translation using automatic segmenta-\ntion. In Proceedings of SSST-8, Eighth Workshop on\nSyntax, Semantics and Structure in Statistical Trans-\nlation, pages 78–85, Doha, Qatar. Association for\nComputational Linguistics.\nJiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih,\nSinong Wang, and Jie Tang. 2020. Blockwise self-\nattention for long document understanding. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020, pages 2555–2565, Online. Asso-\nciation for Computational Linguistics.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\nmar, Chloe Hillier, and Timothy P. Lillicrap. 2020.\nCompressive transformers for long-range sequence\nmodelling. In International Conference on Learning\nRepresentations.\nDavid E. Rumelhart, Geoffrey E. Hinton, and Ronald J.\nWilliams. 1986. Learning Representations by Back-\npropagating Errors. Nature, 323(6088):533–536.\nYves Scherrer, Jörg Tiedemann, and Sharid Loái-\nciga. 2019. Analysing concatenation approaches to\ndocument-level NMT in two different domains. In\nProceedings of the Fourth Workshop on Discourse in\nMachine Translation (DiscoMT 2019), pages 51–61,\nHong Kong, China. Association for Computational\nLinguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nMukund Sundararajan, Ankur Taly, and Qiqi Yan. 2017.\nAxiomatic attribution for deep networks. InProceed-\nings of the 34th International Conference on Ma-\nchine Learning , volume 70 of Proceedings of Ma-\nchine Learning Research, pages 3319–3328. PMLR.\nJörg Tiedemann and Yves Scherrer. 2017. Neural ma-\nchine translation with extended context. In Proceed-\nings of the Third Workshop on Discourse in Machine\nTranslation, pages 82–92, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019a.\nContext-aware monolingual repair for neural ma-\nchine translation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 877–886, Hong Kong, China. Asso-\nciation for Computational Linguistics.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019b.\nWhen a good translation is wrong in context:\nContext-aware machine translation improves on\ndeixis, ellipsis, and lexical cohesion. In Proceedings\nof the 57th Annual Meeting of the Association for\nComputational Linguistics , pages 1198–1212, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In NeurIPS.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33.\nJiacheng Zhang, Huanbo Luan, Maosong Sun, Feifei\nZhai, Jingfang Xu, Min Zhang, and Yang Liu. 2018.\nImproving the transformer translation model with\ndocument-level context. In Proceedings of the 2018\n1419\nConference on Empirical Methods in Natural Lan-\nguage Processing , pages 533–542, Brussels, Bel-\ngium. Association for Computational Linguistics.\nPei Zhang, Boxing Chen, Niyu Ge, and Kai Fan. 2020.\nLong-short term masking transformer: A simple but\neffective baseline for document-level neural machine\ntranslation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP) , pages 1081–1087, Online. Associa-\ntion for Computational Linguistics.\nZaixiang Zheng, Xiang Yue, Shujian Huang, Jiajun\nChen, and Alexandra Birch. 2020. Towards mak-\ning the most of context in neural machine translation.\nIn Proceedings of the Twenty-Ninth International\nJoint Conference on Artiﬁcial Intelligence, IJCAI-20,\npages 3983–3989. International Joint Conferences\non Artiﬁcial Intelligence Organization. Main track.\n1420",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8980711102485657
    },
    {
      "name": "Machine translation",
      "score": 0.8381112217903137
    },
    {
      "name": "Sentence",
      "score": 0.802733302116394
    },
    {
      "name": "Transformer",
      "score": 0.7150933742523193
    },
    {
      "name": "Natural language processing",
      "score": 0.6923050284385681
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6565958857536316
    },
    {
      "name": "Encoder",
      "score": 0.6015034914016724
    },
    {
      "name": "Speech recognition",
      "score": 0.36185282468795776
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    }
  ],
  "cited_by": 10
}