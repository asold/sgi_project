{
  "title": "MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators",
  "url": "https://openalex.org/W4393153655",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2116590300",
      "name": "Yaqi Zhang",
      "affiliations": [
        "University of Science and Technology of China",
        "National Engineering Research Center of Electromagnetic Radiation Control Materials"
      ]
    },
    {
      "id": "https://openalex.org/A2107737109",
      "name": "Di Huang",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A1974433614",
      "name": "Bin Liu",
      "affiliations": [
        "National Engineering Research Center of Electromagnetic Radiation Control Materials",
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2487432907",
      "name": "Shixiang Tang",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2101265740",
      "name": "Lu Yan",
      "affiliations": [
        "University of Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2098210174",
      "name": "Lu Chen",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "ShangHai JiAi Genetics & IVF Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2103749775",
      "name": "Lei Bai",
      "affiliations": [
        "ShangHai JiAi Genetics & IVF Institute",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2098829900",
      "name": "Qi Chu",
      "affiliations": [
        "National Engineering Research Center of Electromagnetic Radiation Control Materials",
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2150264287",
      "name": "Nenghai Yu",
      "affiliations": [
        "University of Science and Technology of China",
        "National Engineering Research Center of Electromagnetic Radiation Control Materials"
      ]
    },
    {
      "id": "https://openalex.org/A2138640236",
      "name": "Wanli Ouyang",
      "affiliations": [
        "ShangHai JiAi Genetics & IVF Institute",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2116590300",
      "name": "Yaqi Zhang",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A1974433614",
      "name": "Bin Liu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2098210174",
      "name": "Lu Chen",
      "affiliations": [
        "ShangHai JiAi Genetics & IVF Institute",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2103749775",
      "name": "Lei Bai",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "ShangHai JiAi Genetics & IVF Institute"
      ]
    },
    {
      "id": "https://openalex.org/A2098829900",
      "name": "Qi Chu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2150264287",
      "name": "Nenghai Yu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2138640236",
      "name": "Wanli Ouyang",
      "affiliations": [
        "ShangHai JiAi Genetics & IVF Institute",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2769460278",
    "https://openalex.org/W6847631053",
    "https://openalex.org/W4284700810",
    "https://openalex.org/W3045737270",
    "https://openalex.org/W2802441648",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W3189935790",
    "https://openalex.org/W2614034469",
    "https://openalex.org/W3153832461",
    "https://openalex.org/W4225309834",
    "https://openalex.org/W6790035886",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4226317937",
    "https://openalex.org/W4221155299",
    "https://openalex.org/W6844223692",
    "https://openalex.org/W4310997803",
    "https://openalex.org/W6729503815",
    "https://openalex.org/W4317545270",
    "https://openalex.org/W3010876998",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2963324990",
    "https://openalex.org/W3125772723",
    "https://openalex.org/W2964203186",
    "https://openalex.org/W4381568562",
    "https://openalex.org/W4312936899",
    "https://openalex.org/W2963799213",
    "https://openalex.org/W3172698324",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4283388932",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W4281485151",
    "https://openalex.org/W3176693010",
    "https://openalex.org/W4376167553",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287028759",
    "https://openalex.org/W4367061162",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3148944813",
    "https://openalex.org/W4298186886",
    "https://openalex.org/W2963389355",
    "https://openalex.org/W4367367040",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4320085220",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4324299512",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W4386065848",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4312635677",
    "https://openalex.org/W4313145975",
    "https://openalex.org/W4312933868",
    "https://openalex.org/W3204221554",
    "https://openalex.org/W4297981470",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Generating realistic human motion from given action descriptions has experienced significant advancements because of the emerging requirement of digital humans. While recent works have achieved impressive results in generating motion directly from textual action descriptions, they often support only a single modality of the control signal, which limits their application in the real digital human industry. This paper presents a Motion General-Purpose generaTor (MotionGPT) that can use multimodal control signals, e.g., text and single-frame poses, for generating consecutive human motions by treating multimodal signals as special input tokens in large language models (LLMs). Specifically, we first quantize multimodal control signals into discrete codes and then formulate them in a unified prompt instruction to ask the LLMs to generate the motion answer. Our MotionGPT demonstrates a unified human motion generation model with multimodal control signals by tuning a mere 0.4% of LLM parameters. To the best of our knowledge, MotionGPT is the first method to generate human motion by multimodal control signals, which we hope can shed light on this new direction. Visit our webpage at https://qiqiapink.github.io/MotionGPT/.",
  "full_text": "MotionGPT: Finetuned LLMs Are General-Purpose Motion Generators\nYaqi Zhang1,2, Di Huang3, Bin Liu1,2*, Shixiang Tang3, Yan Lu3,\nLu Chen4, Lei Bai4, Qi Chu1,2, Nenghai Yu1,2, Wanli Ouyang4\n1School of Cyber Science and Technology, University of Science and Technology of China\n2CAS Key Laboratory of Electromagnetic Space Information\n3The University of Sydney\n4Shanghai AI Laboratory\nzhangyq99@mail.ustc.edu.cn, flowice@ustc.edu.cn\nAbstract\nGenerating realistic human motion from given action descrip-\ntions has experienced significant advancements because of the\nemerging requirement of digital humans. While recent works\nhave achieved impressive results in generating motion directly\nfrom textual action descriptions, they often support only a\nsingle modality of the control signal, which limits their appli-\ncation in the real digital human industry. This paper presents\na Motion General-Purpose generaTor (MotionGPT) that can\nuse multimodal control signals, e.g., text and single-frame\nposes, for generating consecutive human motions by treating\nmultimodal signals as special input tokens in large language\nmodels (LLMs). Specifically, we first quantize multimodal\ncontrol signals into discrete codes and then formulate them\nin a unified prompt instruction to ask the LLMs to generate\nthe motion answer. Our MotionGPT demonstrates a unified\nhuman motion generation model with multimodal control sig-\nnals by tuning a mere 0.4% of LLM parameters. To the best\nof our knowledge, MotionGPT is the first method to generate\nhuman motion by multimodal control signals, which we hope\ncan shed light on this new direction. Visit our webpage at\nhttps://qiqiapink.github.io/MotionGPT/.\nIntroduction\nHuman motion is pivotal in various applications such as\nvideo gaming, filmmaking, and virtual reality. Recent ad-\nvancements in AI (Saharia et al. 2022; Yu et al. 2022; Ramesh\net al. 2022; Rombach et al. 2022; Ramesh et al. 2021; Ouyang\net al. 2022; Lu et al. 2023) have paved the way for novel\napproaches to motion creation, enabling various control con-\nditions including textual descriptions, music pieces, and hu-\nman poses. However, one significant shortcoming of existing\nworks (Petrovich, Black, and Varol 2022; Zhang et al. 2022;\nTevet et al. 2023; Petrovich, Black, and Varol 2021; Zhuang\net al. 2022) is that they only target a single type of control\ncondition, greatly limiting their applications in the real world,\ne.g., unable to generate motion sequences conditioned on text\ndescriptions and several keyframe human poses. To facilitate\nsuch applications, it is important to develop a unified hu-\nman motion generation framework that can efficiently utilize\nmultiple control signals simultaneously.\n*Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nThis paper proposes a novel and more unified frame-\nwork for text-motion generation. The framework facil-\nitates the generation of human motions using multi-\nple control conditions, formulated as\noutput_motion =\nf(text, task, input_motion). Newly added inputstask and\ninput_motion represent the task and given motion prompts,\nrespectively. Here, task indicates the specific task the model\nshould adapt to, while input_motion provides the keyframe\nposes corresponding to the given task. This framework is a\ndeparture from traditional text-motion generation models as\nthe introduction of input_motion enables more precise con-\ntrol. For example, given an input_motion and set the task\nas \"generate motion given initial poses\", the model should\ncompensate for the subsequent frames of the given frames.\nSuch a framework offers a more practical and comprehensive\nsolution for human motion generation, where task instruc-\ntions and multimodal conditions can flexibly control motion\ngeneration.\nThe challenge of building a model to complete such (text,\nmotion)-motion generation task lies in understanding multi-\nmodal control conditions and generating human motions with\nvarying motion lengths and richer patterns. We argue that\nthese challenges can be naturally resolved by adapting from\nLLMs for the following reasons. First, recent studies have\ndemonstrated that LLMs can understand multimodal inputs,\ne.g., images (Zhu et al. 2023; Du et al. 2023; Li et al. 2023a;\nLiu et al. 2023; Ye et al. 2023) and videos (Li et al. 2023b),\nthrough a lightweight adapter (Hu et al. 2021a). Therefore,\nwe expect the LLMs can also understand motion sequences\nwith an appropriate adapter. Second, LLMs can provide di-\nverse human motion contexts for motion generation because\nthey have encoded diverse motion patterns from extensive\nlarge-scale text data. This enables our motion generator fine-\ntuned from LLMs can produce motions with rich patterns.\nThird, since LLMs output tokens aggressively, producing hu-\nman motion with flexible sequences is no longer an obstacle.\nTo this end, we propose a Motion General-Purpose gen-\neraTor (MotionGPT) by fine-tuning an LLM following de-\nsigned instructions. Specifically, MotionGPT first maps hu-\nman poses into discrete motion codes via the pre-trained\nmotion VQ-V AE and then generates instructions by combin-\ning codes from language prompts and motion prompts. The\nLLMs are fine-tuned by answering the correct human pose se-\nquences to the instructions in an efficient way of well-known\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7368\nLoRA adaptation. The designed motion instruction tuning\nframework can incorporate pose sequence information into\nthe fine-tuned large language model while taking advantage\nof strong motion priors in the original large language model.\nWe conduct extensive experiments on the Hu-\nmanML3D (Guo et al. 2022a) and KIT-ML (Plappert,\nMandery, and Asfour 2016) datasets, demonstrating\nMotionGPT has a strong ability for motion generation\nwith multiple control conditions. Remarkably, MotionGPT\nachieves this with a significantly small set of training\nparameters (33 M), and in less training time (about 4\nhours, or just 10% of the time taken by other methods). We\nobserve that joint training under multiple control instructions\noutperforms training with a single type of control signal,\nshowing the effectiveness of our unified motion generation\ntraining paradigm. Our contributions can be summarized as\nfollows:\n• We introduce a novel model, MotionGPT, for generating\nhuman motions, which allows for multiple types of control\nduring the generation process. To the best of our knowl-\nedge, MotionGPT is the first method for using both text\nand poses as conditions. It supports generating subsequent,\npreceding, or ‘in-betweening’ motions using a single and\nunified model.\n• We demonstrate that a pre-trained LLM can be readily\ntuned to function as a human motion generator, suggesting\nthe potential for directly utilizing LLMs for human motion\ngeneration.\n• We present a comprehensive set of experiments, showcas-\ning the effectiveness of our proposed MotionGPT with\nmultiple types of control signals. Experimental results also\nindicate that using a more powerful LLM results in superior\nmotion generation quality, indicating that further advance-\nments in LLM technology could substantially enhance the\nperformance of MotionGPT in the future.\nRelated Work\nLarge language models Recently, large language mod-\nels (Devlin et al. 2018; Radford et al. 2018, 2019; Brown et al.\n2020; OpenAI 2023; Touvron et al. 2023) have been devel-\noped dramatically,e.g., BERT (Devlin et al. 2018), GPT (Rad-\nford et al. 2018), and Google T5 (Raffel et al. 2020). These\nmodels, such as GPT-4 (OpenAI 2023), demonstrate excep-\ntional performance on various linguistic tasks, thanks to the\nextensive training data (45 gigabytes in the case of GPT-4)\nand the large number of parameters they leverage. Previously,\nlanguage models were task-specific, focusing on areas such\nas translation and sentiment analysis. However, recent devel-\nopments, like ChatGPT, have expanded the capability of these\nmodels. Based on GPT-4, ChatGPT can interact with humans,\nshowcasing its strong natural language understanding abili-\nties. This effectiveness has opened up possibilities for a myr-\niad of downstream tasks achieved through fine-tuning these\nLLMs. However, fine-tuning such models, considering their\nextensive parameters, is a challenging task. To address this\nissue, efficient fine-tuning strategies have been proposed, in-\ncluding prompt tuning (Lester, Al-Rfou, and Constant 2021;\nLiu et al. 2021; Hu et al. 2021b), adapters (Houlsby et al.\n2019; He et al. 2021; Le et al. 2021), and LoRA (Hu et al.\n2021a). Our work draws inspiration from the recent progress\nin LLMs, but it also addresses a distinct problem by introduc-\ning a new modality into the LLMs.\nHuman motion generation Motion generation (Tevet et al.\n2022; Habibie et al. 2017; Petrovich, Black, and Varol 2021;\nLi et al. 2017; Zhang et al. 2022; Guo et al. 2020; Tevet et al.\n2023; Petrovich, Black, and Varol 2022; Li et al. 2021) is\na long-history task that can be conditioned on various con-\nditions, such as motion description, actions, and music. For\ninstance, HP-GAN (Barsoum, Kender, and Liu 2018) and\n(Martinez, Black, and Romero 2017) utilize a sequence-to-\nsequence model to anticipate future poses based on prior\nposes. ACTOR (Petrovich, Black, and Varol 2021) employs\na transformer V AE for both unconditional and action-based\ngeneration. TRAJEV AE (Kania, Kowalski, and Trzci ´nski\n2021), when supplied with an initial pose and a trajectory,\ncan generate a motion sequence that follows the given path. In\nrecent years, text-conditional motion generation has garnered\nsignificant attention. This approach focuses on generating\nhuman motion sequences conditioned on textual descriptions.\nTEMOS (Petrovich, Black, and Varol 2022) proposes a V AE\nmodel that learns a shared latent space for both motion and\ntext. MotionDiffuse (Zhang et al. 2022) integrates a diffu-\nsion model into the text-to-motion generation framework and\naccomplishes impressive results. MDM (Tevet et al. 2023),\naiming to enhance motion-text consistency, uses CLIP (Rad-\nford et al. 2021) as the text encoder to incorporate more\nrobust text priors into the model. In comparison to previous\nmethods, our work, MotionGPT, stands out as the first unified\nmotion generation model that supports multimodal controls.\nMotionGPT: A Motion General-Purpose\nGenerator\nMotionGPT proposes a Motion General-Purpose generaTor\ncontrolled by multimodal conditions, i.e., texts and human\nposes in keyframes. Our motivation is to formulate human\nmotion as a problem of asking the Large Language Model to\ngenerate desirable human motions according to task prompts\nand control conditions. Specifically, we quantize motion con-\ntrols into discrete codes using the widely-used VQ-V AE (Van\nDen Oord, Vinyals et al. 2017). Motion discrete codes, text\ncontrol conditions, and designed task instructions are then\norganized into a unified question template for the LoRA-\nfinetuned LLM to generate a human motion sequence answer.\nFollowing the typical framework of instruction tuning, we\nleverage cross-entropy loss to supervise the LoRA adapter.\nMore importantly, our MotionGPT can address not only exist-\ning human motion generation tasks, e.g., text-to-motion gen-\neration, but also new motion generation tasks by simply ad-\njusting task instructions, showing the potential of MotionGPT\nas a generic baseline framework for motion generation.\nMotion Code Generation\nVQ-V AE proposed in (Van Den Oord, Vinyals et al. 2017)\nenables the model to learn discrete representations for gener-\native models. Given a human pose m, the motion VQ-V AE\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7369\n(Text,Motion)-to-\nmotion\na person walks forward\nText\na person\nwalks forward\nText & Initial Token\nText & Last Token\nText & Key Tokens\na person\nwalks forward\na person\nwalks forward\nText-to-\nmotion\nForecast\nIn-between\na person\nwalks forward\nPrevious Methods\n MotionGPT\nFigure 1: This work proposes a novel human motion generation method via fine-tuned LLMs, named MotionGPT. Compared with\nprevious methods, MotionGPT has the unique ability to accept multiple control conditions and solve various motion generation\ntasks using a unified model.\ncan be trained by the reconstruction loss, the embedding loss\nand the commitment loss, i.e.,\nLVQV AE= ||D(E(m)) − m||2 + ∥sg[E(m)] − e∥2\n2\n+β∥E(m) − sg[e]∥2\n2,\n(1)\nwhere E, D are the motion encoder and the motion decoder,\nrespectively. sg indicates the stop gradient operation. Here,\nthe estimated embedding e after qunatization can be found\nby searching the nearest embedding in a learnable codebook\nB = {b1, b2, ..., bN }, where N is the size of the codebook,\nwhich can be mathematically formulated as\ne = arg min\nbk∈B\n∥E(m) − bk∥2. (2)\nBased on the estimation latent representation e of the motion\nm, the reconstructed human pose ˆ mcan be produced by the\ndecoder of VQ-V AE and the motion codep of human pose\nm can be calculated as the index of its nearest embedding in\nthe codebook, i.e.,\nˆ m= D(e), p = arg min\nk\n∥E(m) − bk∥2. (3)\nInstruction Generation\nIn MotionGPT, we design instructions that combine task\nprompts and control conditions to enable (text, motion)-\nmotion generation tasks. Specifically, given the task prompts\nT = {t1, t2, ..., tnt }, the text control conditions X =\n{x1, x2, ..., xnx } and the pose control conditions P =\n{p1, p2, ..., pnp } where nt, nx and np are the number of\ncodes in T , X and P, the instruction I is formulated as\n% General control conditions format\nControl Conditions: {Text control conditions X\n<x1, x2, ..., xnx >} {Pose control conditions P\n<p1, p2, ..., pnp >}\n% General instruction format\nInstruction I: {Task Prompts T <t1, t2, ..., tnt >}\n{Control Conditions}\nHere, the pose control conditions P = {p1, p2, ..., pnp }\npresents pose codes, generated by using the same motion\nVQ-V AE mentioned earlier. Consequently, the entire instruc-\ntion I can be regarded as a sequence of specialized text inputs.\nBy generating different motion instructions, our MotionGPT\ncan address existing human motion generation tasks and new\nhuman motion generations.\nFine-tuning LLM by Motion Instructions\nInstruction tuning (Wei et al. 2021) enables LLMs to handle\nvarious generation tasks by asking the LLM questions in dif-\nferent instructions. Therefore, we design various instructions\nthat combine both task descriptions and control conditions\nto fine-tune large language model by the widely-used and\nefficient Low-Rank Adaptation (LoRA) (Hu et al. 2021a).\nSpecifically, given a large language model\nF, the general\ntemplate of our instructions I and the answer of the LLM\nˆP = F(I) are formulated as\nBelow is an instruction that describes a task, paired with\nan input that provides further context. Write a response\nthat appropriately completes the request.\n% Task Prompts: Code sequences of Task Prompts\n% Control Conditions: Code sequences of Control Con-\nditions\nInstruction I: {Task PromptsT } {Control Conditions}\nAnswer ˆP: {Sequences of Human Motions}\nThe answer of LLM ˆP = {ˆp1, ˆp2, ...,ˆpnˆp } is a series of\ngenerated motion codes, which can be decoded to human\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7370\nVQV AE\n\"259, 467, ...\"\nMotionGPT: LLM      + LoRA      \nText\n\"a person walks\nstraight forward\"\nPose Tokens\nEncoder\nCodebook\nDecoder\n\"Generate a sequence of motion\ntokens matching the following\nhuman motion description given the\ninitial token\"\nCodebook\n\"259, 494, ...\"\nCE Loss\n\"259\"\nGenerated TokensGround Truth Tokens\nTask Prompt Control Conditions ,\nFigure 2: The pipeline of MotionGPT, a Motion General-Purpose generaTor. Given text and poses as an input example, we\norganize task descriptions (Instruction) and multiple control conditions (Input) within a question template. MotionGPT fine-tunes\nan LLM to generate the corresponding motion answer, which can then be decoded into human motions using a VQ-V AE decoder.\nmotion using Eq. 3.\nSimilar to most language models, we employ cross-entropy\nloss which constrains the similarity between estimated and\nground-truth tokens, to fine-tune LLMs by LoRA, which can\nbe presented as\nLlora = CE(ˆP, ˆPgt), (4)\nwhere ˆPgt is the motion codes of ground-truth motions cal-\nculated by Eq. 3 and ˆP is the motion codes predicted by the\nLLM F.\nGeneralization to Existing and New Tasks\nLeveraging the general template given before, our Mo-\ntionGPT is capable of being a general-purpose motion gen-\nerator, supporting various generation tasks. Specifically, for\nexisting text-to-motion generation setting, MotionGPT ad-\ndress it by constructing following instruction I:\nInstruction (I) :{Task Prompts: \"Generate a sequence\nof motion tokens matching the following human mo-\ntion description.\"} {Control Conditions: Text control\ncondition X}\nBy adjusting instructions, MotionGPT can be easily\nadapted to multiple control conditions, e.g. text and an arbi-\ntrary number of human poses:\nInstruction (I) :{Task Prompts: \"Generate a sequence\nof motion tokens matching the following human mo-\ntion description given the init/last/key pose tokens.\"}\n{Control Conditions: Text control conditionX <Motion\nToken> Pose control conditions P </Motion Token>}\nExperiment\nDatasets and Evaluation Metrics\nDatasets We apply two widely-used datasets, Hu-\nmanML3D (Guo et al. 2022a) and KIT-ML (Plappert, Man-\ndery, and Asfour 2016) for evaluation.\nEvaluation metrics Our evaluation comprises two cate-\ngories of metrics. Firstly, to assess the quality of the gen-\nerated motion, we adopt evaluation metrics consistent with\nprevious methods. These include the Frechet Inception Dis-\ntance (FID),Multi-modal Distance (MM Dist),R-Precision\n(calculating the Top-1/2/3 motion-to-text retrieval accuracy),\nand the Diversity metric. These metrics collectively provide\na robust indication of both the realism and diversity of the\ngenerated motion.\nSecondly, we introduce new metrics tailored to our pro-\nposed motion generation setting, including Reconstruction\nLoss (Recon)and Velocity Loss (Vel). Specifically, these met-\nrics aim to measure the consistency between the provided\npose conditions and the generated motion.\nMore information about datasets, proposed new metrics,\nand implementation details are included in the supplementary\nmaterial (Zhang et al. 2023b).\nComparisons for Motion Generation with Multiple\nControl Conditions\nIn this section, we conduct four different generation experi-\nments with 1) text as the condition, 2) text and initial pose\nas the condition, 3) text and last pose as the condition, and\n4) text and random keyframe pose as the condition. For both\n2) and 3), we use 4 frame poses as the input pose condition;\nWhile for 4), we random sample 12 to 20 frame poses as the\npose condition.\nThe quantitative results of motion quality are depicted in\nTab. 1 and Tab. 2. As illustrated in Tab. 1, our proposed model,\nMotionGPT, exhibits a performance that is competitive with\nstate-of-the-art methods for text-to-motion generation. Specif-\nically, MotionGPT consistently achieves comparable results\nacross all metrics on both HumanML3D (Guo et al. 2022a)\nand KIT-ML (Plappert, Mandery, and Asfour 2016) datasets.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7371\nMethods HumanML3D KIT-ML\nFID ↓ MM Dist ↓ Diversity ↑ FID ↓ MM Dist ↓ Diversity ↑\nReal motion 0.002 2.974 9.503 0.031 2.788 11.08\nTEMOS (Petrovich, Black, and Varol 2022) 3.734 3.703 8.973 3.717 3.417 10.84\nTM2T (Guo et al. 2022b) 1.501 3.467 8.589 1.501 3.467 8.589\nT2M (Guo et al. 2022a) 1.087 3.347 9.175 3.022 3.488 10.72\nMotionDiffuse (Zhang et al. 2022) 0.630 3.113 9.410 1.954 2.958 11.10\nMDM (Tevet et al. 2023) 0.544 5.566 9.559 0.497 9.191 10.85\nMLD (Xin et al. 2023) 0.473 3.196 9.724 0.404 3.204 10.80\nT2M-GPT (Zhang et al. 2023a) 0.116 3.118 9.761 0.514 3.007 10.92\nMotionGPT-13B (Ours) 0.567 3.775 9.006 0.597 3.394 10.54\nTable 1: Comparisons of text-to-motion generation with the state-of-the-art methods on HumanML3D and KIT-ML test set.\nMotionGPT-13B achieves comparable performance on all metrics. Bold and underline indicate the best and the second best\nresult.\nText + Initial Token\na person is doing jumping jacks,\nthen starts jogging in place\nperson went around\nto sit on chair\nText + Last Token\na person walks forward with\nhis arms at his side slowly\nText + Key Tokens\na man steps in a circular motion using\nboth hands simultaneously to point at\nsomeone as if they are having a conversation\na man walks forward, does two kicks to\nthe side and then one kick to the front\nthe figure walks forward walks forward\nthen steps to the side then steps\nbackwards then to the side again\na person is dancing by putting their arms out\nmaking a t-pose and rotating their wrists, then\nmoves their legs up and out one at a time\nFigure 3: Generated motion by MotionGPT with multiple control conditions on HumanML3D.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7372\na standing man leans down to a kneeled\nposition with his left knee contacting\nthe ground and his right leg planted\nfoot down. the man then stands up.\nthe figure steps forward then turns\nslightly right and proceeds\nto walk in that direction\nGT \nMDM \nOurs \nthe person is lifting his dumbbell\nwhile bending his legs\nstaring with arms out in a t, a person brings\ntheir hands together for a clap and proceeds\nto take two steps to sit down to relax\nFigure 4: Qualitative comparison of the state-of-the-art motion generation method MDM with text-only conditions on Hu-\nmanML3D.\nMethods FID ↓ MM Dist ↓ Diversity ↑\nHumanML3D\nText-only 0.567 3.775 9.006\nText + Initial poses 0.520 3.844 9.588\nText + Last poses 0.591 3.718 9.251\nText + Random poses 0.367 3.598 9.176\nKIT-ML\nText-only 0.597 3.394 10.54\nText + Initial poses 0.664 3.445 10.39\nText + Last poses 0.856 3.336 10.58\nText + Random poses 0.671 3.411 10.76\nTable 2: Motion generation quality on HumanML3D and\nKIT-ML test set for diverse control conditions.\nIn addition to text conditions, MotionGPT can also incorpo-\nrate human poses as a secondary control modality and the\nmotion quality results are demonstrated in Tab. 2. The adop-\ntion of additional control conditions, such as initial, last, or\nkey tokens, does not compromise the quality of the generated\nmotions. In some instances, such as when provided with ini-\ntial or key tokens, MotionGPT even outperforms its text-only\ncounterpart from 0.567 to 0.520 or 0.367 under FID metric\non HumanML3D, demonstrating its robustness and flexibil-\nity in handling diverse control modalities. Nevertheless, a\nslight decrease in performance is observed when the model\nis given the final pose as input, which is in line with our\nexpectations, as generating motions with a predetermined\nend pose presents an inherently greater challenge. Despite\nthis, MotionGPT’s performance remains commendable, fur-\nther affirming its capability to generate high-quality, diverse\nmotions under various control conditions.\nWe present visualization results in Fig. 3 and Fig. 4. As\nthe Fig. 3 shown, the motions generated by our model ex-\nhibit a notable alignment with the provided poses, while\nalso displaying a consistent adherence to the textual descrip-\ntions. For the text-to-motion generation task, we compare our\nmodel, MotionGPT, with the MDM, as depicted in Fig. 4.\nOur model demonstrates superior text-consistency and text-\ncompleteness compared to MDM (Tevet et al. 2023). The\nmotions generated by the MDM model often tend to align\nwith only the initial segment of the description, ignoring the\nlatter half. In contrast, our approach exhibits a more compre-\nhensive understanding of the motion descriptions by leverag-\ning the powerful capabilities of LLMs, thus generating more\ncomplete and nuanced motion sequences.\nAblation Study\nAdditionally, extensive ablation studies are conducted on\nHumanML3D (Guo et al. 2022a) dataset to indicate the ef-\nfectiveness of our MotionGPT. More ablation studies are\nincluded in the supplementary material (Zhang et al. 2023b).\nCapability of pre-trained LLM Pre-trained LLMs can\nprovide robust priors about human motion from texts. In\nthis context, we experiment with base models pre-trained to\nvarying degrees, including LLaMA-7B, LLaMA-13B, and\nLLaMA without pre-training. For the un-pretrained LLaMA,\nwe adopt the same network structure as LLaMA-7B without\nloading the pre-trained weights. The randomly initialized\nLLaMA is tuned by LoRA as well, fixing weights during\ntraining. As demonstrated in Tab. 3, our results show a strong\ncorrelation between the level of pre-training in LLMs and the\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7373\nPre-trained Model FID ↓ MM Dist ↓ R-Precision ↑ Diversity ↑Top-1 Top-2 Top-3\nLLaMA w/o pre-trained 26.01 8.445 0.032 0.067 0.106 9.745\nLLaMA-7B 0.590 3.796 0.376 0.553 0.657 9.048\nLLaMA-13B 0.542 3.584 0.411 0.594 0.696 9.311\nTable 3: Evaluation of text-to-motion generation using different pre-trained LLaMA on HumanML3D validation set. Bold\nindicates the best result.\nTask Training FID ↓ MM Dist ↓ R-Precision ↑ Diversity ↑Strategy Top-1 Top-2 Top-3\nText\nSeparate\n0.670 4.267 0.299 0.469 0.577 9.745\n+ Initial token 0.756 3.802 0.374 0.556 0.658 9.148\n+ Last token 1.409 4.516 0.290 0.446 0.564 8.771\n+ Key tokens 0.702 3.690 0.370 0.546 0.668 8.974\nText\nJoint\n0.590−.180 3.796−.471 0.376+.077 0.553+.084 0.657+.080 9.048−.697\n+ Initial token 0.493−.263 3.750−.052 0.384+.010 0.564+.008 0.666+.008 9.378+.230\n+ Last token 0.646−.763 3.675−.841 0.393+.103 0.577+.131 0.681+.117 9.030+.259\n+ Key tokens 0.390−.663 3.492−.198 0.416+.046 0.597+.051 0.713+.045 9.621+.647\nTable 4: Comparisons between separate training for each task and joint training for multiple tasks on HumanML3D validation\nset using MotionGPT-7B. Superscripts indicate the improvement or decrement in the metric. Joint training can achieve better\nperformance for all tasks.\nMethods Recon ↓ Vel ↓\nInitial token\nText-only 24.70 1.095\nText + Initial poses 13.78 0.549\nLast token\nText-only 19.70 1.172\nText + Last poses 6.831 0.397\nKey tokens\nText-only 8.035 3.813\nText + Random poses 5.383 2.423\nTable 5: Evaluation of the effectiveness of pose control condi-\ntions on HumanML3D test set using MotionGPT-13B model.\nperformance of our model in the text-to-motion generation\ntask. This highlights the significant influence of motion prior\nextracted from LLM. Note that the training parameters of\nLoRA are same.\nConsistency with pose control conditions We demon-\nstrate the effectiveness of pose control conditions by assess-\ning the consistency between pose controls and generated\nmotion on the HumanML3D test set. For each task (ini-\ntial/last/key), we generate motion with and without pose con-\ntrols using (text+pose)-to-motion and text-to-motion meth-\nods, respectively. The results are shown in Tab. 5. In compari-\nson to text-only generation, better keyframe pose consistency\narises from generating under pose conditions, showcasing\n(text+pose)-to-motion’s effectiveness with pose control.\nComparison with separate training To further evalu-\nate the effectiveness of our unified motion generation ap-\nproach, we conduct separate training for each task on the\nHumanML3D dataset (Guo et al. 2022a). The aim is to inves-\ntigate if multi-task learning could improve the performance\nof individual control conditions. The comparison results are\ndepicted in Table 4. We find that joint training across all tasks\nyields significant improvements in all metrics. This effect is\nespecially pronounced when text and last poses are used as\nconditions. These findings underscore the utility of our uni-\nfied motion generation approach. It appears that the model’s\nability to generate motions under a specific control type is\nboosted by the knowledge derived from other related control\nconditions.\nConclusion and Limitations\nConclusion This study introduces MotionGPT, a novel\nmethod capable of generating human motion using multi-\nmodal control signals, such as text and single-frame poses.\nThe approach effectively discretizes pose conditions and cre-\nates a unified set of instructions by combining codes from\nboth textual and pose prompts. With MotionGPT, we envision\na path toward more practical and versatile motion generation\nsystems, offering a fresh perspective in the field.\nLimitations Although current MotionGPT may support\nany control modalities beyond current human poses and text,\nthis paper only validates the effectiveness on text and human\nposes. Validating our MotionGPT on a broader spectrum of\npossible modalities, such as music pieces, would be highly\nbeneficial to more applications in the real world.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7374\nAcknowledgments\nThis work is supported by the National Natural Science\nFoundation of China (Grant No. 62121002 and Grant No.\n62272430).\nReferences\nBarsoum, E.; Kender, J.; and Liu, Z. 2018. Hp-gan: Proba-\nbilistic 3d human motion prediction via gan. In Proceedings\nof the IEEE conference on computer vision and pattern recog-\nnition workshops, 1418–1427.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; Neelakantan, A.; Shyam, P.; Sastry, G.; Askell,\nA.; et al. 2020. Language models are few-shot learners.\nAdvances in neural information processing systems, 33: 1877–\n1901.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2018.\nBert: Pre-training of deep bidirectional transformers for lan-\nguage understanding. arXiv:1810.04805.\nDu, Y .; Konyushkova, K.; Denil, M.; Raju, A.; Landon, J.;\nHill, F.; de Freitas, N.; and Cabi, S. 2023. Vision-language\nmodels as success detectors. arXiv:2303.07280.\nGuo, C.; Zou, S.; Zuo, X.; Wang, S.; Ji, W.; Li, X.; and Cheng,\nL. 2022a. Generating diverse and natural 3d human motions\nfrom text. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 5152–5161.\nGuo, C.; Zuo, X.; Wang, S.; and Cheng, L. 2022b. Tm2t:\nStochastic and tokenized modeling for the reciprocal gener-\nation of 3d human motions and texts. In Computer Vision–\nECCV 2022: 17th European Conference, Tel Aviv, Israel,\nOctober 23–27, 2022, Proceedings, Part XXXV, 580–597.\nSpringer.\nGuo, C.; Zuo, X.; Wang, S.; Zou, S.; Sun, Q.; Deng, A.;\nGong, M.; and Cheng, L. 2020. Action2motion: Conditioned\ngeneration of 3d human motions. In Proceedings of the 28th\nACM International Conference on Multimedia, 2021–2029.\nHabibie, I.; Holden, D.; Schwarz, J.; Yearsley, J.; and Komura,\nT. 2017. A recurrent variational autoencoder for human\nmotion synthesis. In Proceedings of the British Machine\nVision Conference (BMVC).\nHe, R.; Liu, L.; Ye, H.; Tan, Q.; Ding, B.; Cheng, L.; Low,\nJ.-W.; Bing, L.; and Si, L. 2021. On the effectiveness of\nadapter-based tuning for pretrained language model adapta-\ntion. arXiv:2106.03164.\nHoulsby, N.; Giurgiu, A.; Jastrzebski, S.; Morrone, B.;\nDe Laroussilhe, Q.; Gesmundo, A.; Attariyan, M.; and Gelly,\nS. 2019. Parameter-efficient transfer learning for NLP. In\nInternational Conference on Machine Learning, 2790–2799.\nPMLR.\nHu, E. J.; Shen, Y .; Wallis, P.; Allen-Zhu, Z.; Li, Y .; Wang, S.;\nWang, L.; and Chen, W. 2021a. Lora: Low-rank adaptation\nof large language models. arXiv:2106.09685.\nHu, S.; Ding, N.; Wang, H.; Liu, Z.; Wang, J.; Li, J.; Wu, W.;\nand Sun, M. 2021b. Knowledgeable prompt-tuning: Incorpo-\nrating knowledge into prompt verbalizer for text classification.\narXiv:2108.02035.\nKania, K.; Kowalski, M.; and Trzci´nski, T. 2021. TrajeV AE:\nControllable Human Motion Generation from Trajectories.\narXiv:2104.00351.\nLe, H.; Pino, J.; Wang, C.; Gu, J.; Schwab, D.; and Besacier,\nL. 2021. Lightweight adapter tuning for multilingual speech\ntranslation. arXiv:2106.01463.\nLester, B.; Al-Rfou, R.; and Constant, N. 2021. The\npower of scale for parameter-efficient prompt tuning.\narXiv:2104.08691.\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023a. Blip-2: Boot-\nstrapping language-image pre-training with frozen image\nencoders and large language models. arXiv:2301.12597.\nLi, K.; He, Y .; Wang, Y .; Li, Y .; Wang, W.; Luo, P.; Wang,\nY .; Wang, L.; and Qiao, Y . 2023b. VideoChat: Chat-Centric\nVideo Understanding. arXiv:2305.06355.\nLi, R.; Yang, S.; Ross, D. A.; and Kanazawa, A. 2021. Ai\nchoreographer: Music conditioned 3d dance generation with\naist++. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, 13401–13412.\nLi, Z.; Zhou, Y .; Xiao, S.; He, C.; Huang, Z.; and Li, H. 2017.\nAuto-conditioned recurrent networks for extended complex\nhuman motion synthesis. arXiv:1707.05363.\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual Instruction\nTuning. arXiv:2304.08485.\nLiu, X.; Ji, K.; Fu, Y .; Tam, W. L.; Du, Z.; Yang, Z.; and\nTang, J. 2021. P-tuning v2: Prompt tuning can be com-\nparable to fine-tuning universally across scales and tasks.\narXiv:2110.07602.\nLu, Z.; Huang, D.; Bai, L.; Liu, X.; Qu, J.; and Ouyang,\nW. 2023. Seeing is not always believing: A Quantita-\ntive Study on Human Perception of AI-Generated Images.\narXiv:2304.13023.\nMartinez, J.; Black, M. J.; and Romero, J. 2017. On human\nmotion prediction using recurrent neural networks. In Pro-\nceedings of the IEEE conference on computer vision and\npattern recognition, 2891–2900.\nOpenAI. 2023. GPT-4 Technical Report. arXiv:2303.08774.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; et al.\n2022. Training language models to follow instructions with\nhuman feedback. Advances in Neural Information Processing\nSystems, 35: 27730–27744.\nPetrovich, M.; Black, M. J.; and Varol, G. 2021. Action-\nconditioned 3D human motion synthesis with transformer\nV AE. InProceedings of the IEEE/CVF International Confer-\nence on Computer Vision, 10985–10995.\nPetrovich, M.; Black, M. J.; and Varol, G. 2022. TEMOS:\nGenerating diverse human motions from textual descriptions.\nIn Computer Vision–ECCV 2022: 17th European Conference,\nTel Aviv, Israel, October 23–27, 2022, Proceedings, Part XXII,\n480–497. Springer.\nPlappert, M.; Mandery, C.; and Asfour, T. 2016. The KIT\nmotion-language dataset. Big data, 4(4): 236–252.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7375\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748–8763. PMLR.\nRadford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;\net al. 2018. Improving language understanding by generative\npre-training.\nRadford, A.; Wu, J.; Child, R.; Luan, D.; Amodei, D.; and\nSutskever, I. 2019. Language Models are Unsupervised Mul-\ntitask Learners.\nRaffel, C.; Shazeer, N.; Roberts, A.; Lee, K.; Narang, S.;\nMatena, M.; Zhou, Y .; Li, W.; and Liu, P. J. 2020. Exploring\nthe limits of transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research, 21(1):\n5485–5551.\nRamesh, A.; Dhariwal, P.; Nichol, A.; Chu, C.; and Chen, M.\n2022. Hierarchical text-conditional image generation with\nclip latents. arXiv:2204.06125.\nRamesh, A.; Pavlov, M.; Goh, G.; Gray, S.; V oss, C.; Radford,\nA.; Chen, M.; and Sutskever, I. 2021. Zero-shot text-to-\nimage generation. In International Conference on Machine\nLearning, 8821–8831. PMLR.\nRombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Om-\nmer, B. 2022. High-resolution image synthesis with latent dif-\nfusion models. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 10684–10695.\nSaharia, C.; Chan, W.; Saxena, S.; Li, L.; Whang, J.; Denton,\nE. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan, B.;\nSalimans, T.; et al. 2022. Photorealistic text-to-image diffu-\nsion models with deep language understanding. Advances in\nNeural Information Processing Systems, 35: 36479–36494.\nTevet, G.; Gordon, B.; Hertz, A.; Bermano, A. H.; and Cohen-\nOr, D. 2022. Motionclip: Exposing human motion generation\nto clip space. In Computer Vision–ECCV 2022: 17th Eu-\nropean Conference, Tel Aviv, Israel, October 23–27, 2022,\nProceedings, Part XXII, 358–374. Springer.\nTevet, G.; Raab, S.; Gordon, B.; Shafir, Y .; Cohen-or, D.; and\nBermano, A. H. 2023. Human Motion Diffusion Model. In\nThe Eleventh International Conference on Learning Repre-\nsentations.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozière, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv:2302.13971.\nVan Den Oord, A.; Vinyals, O.; et al. 2017. Neural discrete\nrepresentation learning. Advances in neural information\nprocessing systems, 30.\nWei, J.; Bosma, M.; Zhao, V . Y .; Guu, K.; Yu, A. W.; Lester,\nB.; Du, N.; Dai, A. M.; and Le, Q. V . 2021. Finetuned\nlanguage models are zero-shot learners. arXiv:2109.01652.\nXin, C.; Jiang, B.; Liu, W.; Huang, Z.; Fu, B.; Chen, T.; Yu,\nJ.; and Yu, G. 2023. Executing your Commands via Motion\nDiffusion in Latent Space. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR).\nYe, Q.; Xu, H.; Xu, G.; Ye, J.; Yan, M.; Zhou, Y .; Wang, J.;\nHu, A.; Shi, P.; Shi, Y .; Jiang, C.; Li, C.; Xu, Y .; Chen, H.;\nTian, J.; Qi, Q.; Zhang, J.; and Huang, F. 2023. mPLUG-\nOwl: Modularization Empowers Large Language Models\nwith Multimodality. arXiv:2304.14178.\nYu, J.; Xu, Y .; Koh, J. Y .; Luong, T.; Baid, G.; Wang, Z.;\nVasudevan, V .; Ku, A.; Yang, Y .; Ayan, B. K.; et al. 2022.\nScaling autoregressive models for content-rich text-to-image\ngeneration. arXiv:2206.10789.\nZhang, J.; Zhang, Y .; Cun, X.; Huang, S.; Zhang, Y .; Zhao,\nH.; Lu, H.; and Shen, X. 2023a. T2M-GPT: Generating\nHuman Motion from Textual Descriptions with Discrete Rep-\nresentations. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR).\nZhang, M.; Cai, Z.; Pan, L.; Hong, F.; Guo, X.; Yang, L.;\nand Liu, Z. 2022. Motiondiffuse: Text-driven human motion\ngeneration with diffusion model. arXiv:2208.15001.\nZhang, Y .; Huang, D.; Liu, B.; Tang, S.; Lu, Y .; Chen, L.; Bai,\nL.; Chu, Q.; Yu, N.; and Ouyang, W. 2023b. MotionGPT:\nFinetuned LLMs are General-Purpose Motion Generators.\narXiv:2306.10900.\nZhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M. 2023.\nMinigpt-4: Enhancing vision-language understanding with\nadvanced large language models. arXiv:2304.10592.\nZhuang, W.; Wang, C.; Chai, J.; Wang, Y .; Shao, M.; and Xia,\nS. 2022. Music2dance: Dancenet for music-driven dance\ngeneration. ACM Transactions on Multimedia Computing,\nCommunications, and Applications (TOMM), 18(2): 1–21.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n7376",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7051031589508057
    },
    {
      "name": "Motion (physics)",
      "score": 0.5731829404830933
    },
    {
      "name": "Computer security",
      "score": 0.40593045949935913
    },
    {
      "name": "Human–computer interaction",
      "score": 0.34157395362854004
    },
    {
      "name": "Artificial intelligence",
      "score": 0.26809853315353394
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I129604602",
      "name": "The University of Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I4210122302",
      "name": "ShangHai JiAi Genetics & IVF Institute",
      "country": "CN"
    }
  ],
  "cited_by": 44
}