{
  "title": "AlphaTuning: Quantization-Aware Parameter-Efficient Adaptation of Large-Scale Pre-Trained Language Models",
  "url": "https://openalex.org/W4385574133",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2106050943",
      "name": "Se Jung Kwon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117141027",
      "name": "Jeong-Hoon Kim",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2164998028",
      "name": "Jeongin Bae",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology",
        "Kootenay Association for Science & Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2261610125",
      "name": "Kang-Min Yoo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150992215",
      "name": "Jin Hwa Kim",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2326935237",
      "name": "Baeseong Park",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2553048246",
      "name": "Byeongwook Kim",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2232041247",
      "name": "Jung-Woo Ha",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2566712049",
      "name": "Nako Sung",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134008861",
      "name": "Dongsoo Lee",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3202028501",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3166859509",
    "https://openalex.org/W3205717164",
    "https://openalex.org/W3160638507",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W4286981949",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3087194612",
    "https://openalex.org/W3131984434",
    "https://openalex.org/W4283157303",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3035855442",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2963122961",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2916954108",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W4312804044",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034340181",
    "https://openalex.org/W2786660442",
    "https://openalex.org/W2806978588",
    "https://openalex.org/W2989743967",
    "https://openalex.org/W3187255235",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W4221167110",
    "https://openalex.org/W4287727281",
    "https://openalex.org/W3120074043",
    "https://openalex.org/W4226146865",
    "https://openalex.org/W2963043696",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W2963576971",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W4229506649",
    "https://openalex.org/W2915478146",
    "https://openalex.org/W4288337707",
    "https://openalex.org/W4385572073",
    "https://openalex.org/W2786951478",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W3007700590",
    "https://openalex.org/W2524428287",
    "https://openalex.org/W2300242332",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2058641082",
    "https://openalex.org/W4287122891",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3101493110",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W3036601975"
  ],
  "abstract": "Se Jung Kwon, Jeonghoon Kim, Jeongin Bae, Kang Min Yoo, Jin-Hwa Kim, Baeseong Park, Byeongwook Kim, Jung-Woo Ha, Nako Sung, Dongsoo Lee. Findings of the Association for Computational Linguistics: EMNLP 2022. 2022.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2022, pages 3288â€“3305\nDecember 7-11, 2022 Â©2022 Association for Computational Linguistics\nAlphaTuning: Quantization-Aware Parameter-Efficient Adaptation\nof Large-Scale Pre-Trained Language Models\nSe Jung Kwon1âˆ—, Jeonghoon Kim1, Jeongin Bae1,4â€ , Kang Min Yoo1,2,3, Jin-Hwa Kim2,3,\nBaeseong Park1, Byeongwook Kim1, Jung-Woo Ha2, Nako Sung1 and Dongsoo Lee1\n1NA VER CLOV A 2NA VER AI Lab 3SNU AIIS 4KAIST\nAbstract\nThere are growing interests in adapting\nlarge-scale language models using parameter-\nefficient fine-tuning methods. However, accel-\nerating the model itself and achieving better\ninference efficiency through model compres-\nsion has not been thoroughly explored yet.\nModel compression could provide the bene-\nfits of reducing memory footprints, enabling\nlow-precision computations, and ultimately\nachieving cost-effective inference. To combine\nparameter-efficient adaptation and model com-\npression, we propose AlphaTuning consisting\nof post-training quantization of the pre-trained\nlanguage model and fine-tuning only some\nparts of quantized parameters for a target task.\nSpecifically, AlphaTuning works by employing\nbinary-coding quantization, which factorizes\nthe full-precision parameters into binary param-\neters and a separate set of scaling factors. Dur-\ning the adaptation phase, the binary values are\nfrozen for all tasks, while the scaling factors are\nfine-tuned for the downstream task. We demon-\nstrate that AlphaTuning, when applied to GPT-\n2 and OPT, performs competitively with full\nfine-tuning on a variety of downstream tasks\nwhile achieving >10Ã—compression ratio under\n4-bit quantization and >1,000 Ã—reduction in\nthe number of trainable parameters.\n1 Introduction\nSelf-supervised learning facilitates the increased\nnumber of parameters to construct pre-trained lan-\nguage models (PLMs) (e.g., Brown et al. (2020);\nDevlin et al. (2019)). We expect the continuation\nof model scaling of the PLMs, especially for the\nTransformers (Vaswani et al., 2017), because their\ngeneral capability follows the power-law in param-\neter size, exhibiting \"the high-level predictability\nand appearance of useful capabilities\" (Ganguli\net al., 2022). Therefore, the Transformer-based\nâˆ—Corresponding author: sejung.kwon@navercorp.com\nâ€ Work done while at NA VER CLOV A\nPLMs have been studied with great enthusiasm for\nvarious applications including natural language pro-\ncessing (Devlin et al., 2019; Radford et al., 2019;\nBrown et al., 2020; Smith et al., 2022; Rae et al.,\n2021; Hoffmann et al., 2022a; Chowdhery et al.,\n2022; Kim et al., 2021a), automatic speech recog-\nnition (Baevski et al., 2020), and computer vision\n(He et al., 2022; Xie et al., 2022).\nDespite the impressive zero or few-shot learning\nperformance of PLMs, additional adaptation steps\n(e.g., fine-tuning on a target task) are required to\nfurther enhance performance on downstream tasks.\nSince each downstream task needs to load/store\nindependent adaptation outcomes, if we aim to de-\nploy multiple instances of distinct tasks, adapting\nPLMs with limited trainable parameters is crucial\nfor the efficient deployment (Li et al., 2018). Thus,\nvarious parameter-efficient adaptation techniques,\nsuch as adapter modules (Houlsby et al., 2019),\nlow-rank adaptation (Hu et al., 2022), prefix-tuning\n(Li and Liang, 2021), prompt tuning (Liu et al.,\n2021a; Gao et al., 2020), and p-tuning (Liu et al.,\n2021b), are proposed.\nAlthough trainable parameters can be signifi-\ncantly reduced by parameter-efficient adaptation\nschemes, we notice that the memory footprints\nfor inference are not reduced compared to those\nof PLMs 1. To enable efficient deployments of\nmultiple downstream tasks, we incorporate model\ncompression and parameter-efficient adaptation.\nWe argue that previous model compression tech-\nniques were not practical solutions in terms of\nparameter-efficiency for adaptations. For example,\nQuantization-Aware Training (QAT) (Jacob et al.,\n2018; Esser et al., 2020) can perform full fine-\ntuning coupled with model compression; however,\neach task needs dedicated memory storage as much\nas that of a compressed PLM. Our key observation\nto achieve a compression-aware parameter-efficient\n1In practice, the adaptation is usually implemented by\nadding small additional parameters to PLMs.\n3288\nA B C C  D\nZero-shot Fine-tuned \nScore on a task\nA B C D\nWeight Size\nLarge LM  \n(LLM)\nA\nFine-tuned \nLLM\nC\nFine-tuned \n& Quantized \nLLM\nDQuantized \nLLM\nB\n(Parameter- \nefficient) \nFine-tuning\nProposed Work:\nAlphaTuning\nQuantization \n(Post-training)\nQuantization \n(Post-training)\nAlphaTuning\nB  D\nFigure 1: Approaches to satisfy both parameter-efficient adaptation and parameter quantization. Our proposed\nAlphaTuning technique can achieve 1) competitive performances to fine-tuned LMs (i.e., A;C) with a remarkably\nreduced parameter size, and 2) significantly better scores than quantized LMs implemented through A;C;D.\nadaptation is that, once a PLM is quantized, only a\nsmall amount of quantization-related parameters is\nneeded to be fine-tuned for each target task. As a\nresult, both the overall memory footprints and the\nnumber of trainable parameters for adaptation can\nbe substantially reduced.\nFigure 1 illustratively compares two differ-\nent approaches enabling both model compression\nand parameter-efficient adaptation. Fine-tuned and\nquantized LMs can be achieved through A;C;D\nor A ;B;D as shown in Figure 1. In the case\nof A ;C;D, we may have a large number of\ntrainable parameters, and/or PTQ may degrade per-\nformance on downstream tasks. To address such\nissues, we investigate A ;B;D scheme, called\nâ€œAlphaTuningâ€ in this work. Specifically, we fac-\ntorize the parameters of large PLMs into binary\nvalues and scaling factors. Then, AlphaTuning con-\nducts the adaptation by training only the scaling\nfactors that occupy a small portion in the quan-\ntization format, while freezing the other binary\nvalues. Note that, to conduct A ;B, we consider\npost-training quantization (PTQ) (Zhao et al., 2019;\nHubara et al., 2020; Li et al., 2020a) because the\nQAT demands significant computational overhead\nfor training from a scratch with the whole dataset.\nIn this paper, our contributions are as follows:\nâ€¢ To the best of our knowledge, this work is the\nfirst successful compression-aware parameter-\nefficient adaptation method.\nâ€¢ We report that once PLMs are quantized by\nPTQ, training scaling factors (less than 0.1%\nof total parameter size) for each task only is\nenough for successful adaptations.\nâ€¢ Throughout various LMs and tasks, we\ndemonstrate that AlphaTuning can achieve\nhigh scores even under 4-bit quantization.\n2 Recent Work\nLarge-Scale Language Models and Quanti-\nzation Pre-trained transformer-based language\nmodels (Devlin et al., 2019; Radford et al., 2019)\nhave shaped the way we design and deploy NLP\nmodels. In recent years, the explosion of availabil-\nity of large-scale (i.e., larger than ten-billion scale)\nlanguage models (Brown et al., 2020; Black et al.,\n2021; Chowdhery et al., 2022; Zhang et al., 2022a;\nHoffmann et al., 2022b) has paved way for a new\nera in the NLP scene, where few-shot learning and\nthe parameter-efficient adaptation for downstream\ntasks will be more important (He et al., 2021). The\nquantization (that we discuss in detail in the next\nsection) is an effective approach to fundamentally\novercome the space and time complexities of the\nlarge-scale language models (Zafrir et al., 2019;\nBondarenko et al., 2021), but existing methods are\nonly applicable to limited domains and task adapt-\nability under the quantized state.\nParameter-Efficient Adaptation of LMs Adapt-\ning language models efficiently for a task and\ndomain-specific data has been at the center of the\ncommunityâ€™s interests since the emergence of large-\nscale language models. One promising approach\nis in-context learning (ICL) (Brown et al., 2020),\nin which the language model learns and predicts\nfrom the given prompt patterns. As the technique\nelicits reasonable few-shot performances from the\nlarge-scale language models without parameter-\ntuning, a plethora of works (Zhao et al., 2021; Lu\net al., 2022; Reynolds and McDonell, 2021; Min\net al., 2022) have investigated the underlying mech-\nanism and proposed various methods to further\nexploit this approach. Another class of techniques\nis to adopt external or partially internal parameters\nsuch as continuous prompt embeddings to enable\n3289\nparameter-efficient LM adaptation, which is based\non the intuition that specific prompt prefixes may\nbetter elicit certain LM behaviors. Earlier works ex-\nplored the discrete prompt token space (Shin et al.,\n2020), but later work showed that optimizing on\nthe continuous word embedding space yielded bet-\nter results (Liu et al., 2021b; Li and Liang, 2021;\nGu et al., 2022), even performing on par with full\nfine-tuning (Lester et al., 2021; Vu et al., 2022).\nAnother similar line of works explored introduc-\ning new parameters within the Transformer blocks\nor partially training existing parameters (Houlsby\net al., 2019; Zhang et al., 2020; Karimi Mahabadi\net al., 2021; Hu et al., 2022). Finally, some works\nhave suggested unifying all existing approaches re-\nlated to parameter-efficient fine-tuning (He et al.,\n2021; Zhang et al., 2022b).\n3 Quantization for AlphaTuning\nEnterprise-scale LMs, such as 175B GPT-3, face\nchallenges in the prohibitive cost of massive de-\nployment mainly resulting from their huge parame-\nter size. To facilitate cost-effective LMs by allevi-\nating memory requirements without noticeable per-\nformance degradation, we can consider compres-\nsion techniques, such as quantization (Jacob et al.,\n2018), pruning (Frankle et al., 2020a), and low-rank\napproximation (N. Sainath et al., 2013). Memory\nreduction by model compression is also useful to\nreduce latency because memory-bound operations\ndominate the overall performance of LMs with a\nsmall batch size (Park et al., 2022). In addition,\nmodel compression can save the number of GPUs\nfor inference because GPUs present highly limited\nmemory capacity (Shoeybi et al., 2019; Narayanan\net al., 2021). In this work, we choose quantization\nas a practical compression technique because of\nits high compression ratio, simple representation\nformat, and the capability to accelerate memory-\nbound workloads (Chung et al., 2020).\nLet us discuss our quantization strategy for LMs\n(see more details in Appendix C). We choose non-\nuniform quantization since uniform quantization\ndemands aggressive activation quantization (to ex-\nploit integer arithmetic units) which is challenged\nby highly non-linear operations (such as softmax\nand layer normalization) of the Transformers (Bon-\ndarenko et al., 2021). Even though uniform quan-\ntization can mitigate performance degradation by\nfrequent activation quantization/dequantization pro-\ncedures (Bhandare et al., 2019) or additional high-\n2.661.05-0.070.65\n-1.82-0.150.410.64\n1.48 0.760.061.36\n1.11\n0.76\n0.91\n1   1  -1  1\n-1  -1  1  1\n1   1   1  1\n1.11 1.11 -1.11 1.11\n-0.76-0.760.760.76\n0.91 0.910.910.91\n1.11\n0.76\n0.91\n1   1  -1  1\n-1  -1  1  1\n1   1   1  1\n1.880.33-0.330.33\n-1.29-0.220.220.22\n1.410.410.411.41\n0.78\n0.53\n0.50\n1   -1  1  -1\n-1  1  -1  -1\n1  -1  -1  1\n1.11\n0.76\n0.91\n1   1  -1  1\n-1  -1  1  1\n1   1   1  1\n2.40 0.850.19 0.85\n-1.590.08 0.530.53\n1.62 0.61 0.2 1.21\n0.78\n0.53\n0.50\n1   -1  1  -1\n-1  1  -1  -1\n1  -1  -1  1\n0.52\n0.30\n0.21\n1   1   1   1\n-1  1  1   1\n1  -1   1   1\nOriginal Weight (W)1-bit Quantized W'(q=1)\n2-bit Quantized W'(q=2)\n3-bit Quantized W'(q=3)\n+\n+\n=\n+\n=\n=\nMSE(W, W'(q=1)) \n = 0.855\nMSE(W,W'(q=2))  \n= 0.550\nMSE(W,W'(q=3))  \n= 0.171\nfrom a set of 21 elements\nfrom a set of 22 elements\nfrom a set of 23 elements\n> >\nFigure 2: BCQ examples with g = 4 and different q\nvalues. As q increases, the MSE between the original\nweight and the quantized weight decreases.\nprecision units (Kim et al., 2021b), such techniques\nare slow and/or expensive. Among various non-\nuniform quantization formats, we choose binary-\ncoding-quantization (BCQ) (Guo et al., 2017; Xu\net al., 2018) which is extended from binary neural\nnetworks (Rastegari et al., 2016) because of high\ncompression ratio (Chung et al., 2020) and efficient\ncomputations (Xu et al., 2018; Jeon et al., 2020).\nBCQ Format Given a full-precision weight vec-\ntor w âˆˆRg, BCQ format approximates wto be\nwâ‰ˆâˆ‘q\ni=1 Î±ibi where qis the number of quantiza-\ntion bits, Î±âˆˆR is a scaling factor to be shared by\ngweights, and bâˆˆ{âˆ’1,+1}g is a binary vector.\nNote that grepresents a group size or the number\nof weights sharing a common scaling factor. Thus,\ngis a hyper-parameter for quantization. When q=1,\nÎ±and bcan be analytically determined to minimize\nthe mean squared error (MSE). If q >1, however,\nÎ±and bneed to be obtained by heuristic methods\nsuch as greedy approximation (Guo et al., 2017)\nand iterative fine-tuning method (Xu et al., 2018).\nFor a weight matrix W âˆˆRhoutÃ—hin, row-wise\nquantization (i.e., g = hin) is a popular choice 2\n(Jeon et al., 2020; Xu et al., 2018) and can be ex-\npressed as follows:\nW â‰ˆ\nqâˆ‘\ni=1\ndiag(Î±i) Â·Bi, (1)\n2On/off-chip memory bandwidth can be maximized by\ncontiguous memory allocation if row-wise quantization is\nadopted. Additionally, for large LLMs (along with a large\nhin), the amount of Î±becomes almost ignorable (i.e., Î±size\nis 32/hin of Bsize) even assuming 32 bits to represent an Î±.\n3290\nLayer W Shape W Size g Î±âˆˆR Bâˆˆ{-1,+1} Quantized W Size (MB)\n(hout, hin) (FP32) Shape Shape q= 1 q= 2 q= 3\nATT_qkv ( 3h,h) 12.58 MB h (q,3h) ( q,3h,h) 0.41 0.81 1.22\nATT_output ( h,h) 4.19 MB h (q,h) ( q,h,h ) 0.14 0.27 0.41\nFFN_h_4h ( 4h,h) 16.78 MB h (q,4h) ( q,4h,h) 0.54 1.08 1.62\nFFN_4h_h ( h,4h) 16.78 MB\n4h (q,h) ( q,h, 4h) 0.52 1.06 1.56\nh (q,4h) ( q,4h,h) 0.54 1.08 1.62\n0.5h (q,8h) ( q,8h,h) 0.56 1.11 1.67\nTable 1: BCQ scheme for q-bit quantization applied to linear layers of the Transformers and examples of BCQ\nformats for GPT-2 medium model (hidden size his 1024). Row-wise quantization is performed when g = hin.\nLower gresults in slightly increased weight size after quantization.\nğµâˆˆ{âˆ’1,+1}!Ã—#Ã—$\nğ›¼âˆˆâ„(Ã—*\nğ‘Šâˆˆâ„(Ã—+\nA weight matrixina pre-trained model\nScaling Factors(â„,Trainable)\nBinary Weights({-1,+1},Frozen)\nNon-uniformk-bit Quantization(Post-training Quantization)\nğ›¼,\nğ›¼-\nğ›¼.\ndataset1\ndataset2\ndatasetk\nQuantizedLinear (hÃ 3h)\nQuantizedLinear (hÃ h)\nQuantizedLinear (hÃ 4h)\nQuantized Linear (4hÃ h)\nDot-product attention \nAttention LayerFeed-forward  Layer\nNormalization\nNormalization\nEmbedding\nTransformer Layer x N\nğ›¼\nğ›¼\nğ›¼\nğ›¼\nB\nB\nB\nBbias\nbias\nbias\nbias\nFrozen ParamsTrainable Params A C DAlphaTuning\ntask1ğµ(fixed & shared)\nğµ(fixed & shared)\nğµ(fixed & shared)\ntask2\ntaskk\nFigure 3: (Left): Quantized Transformer structure in which parameters are categorized into frozen ones and trainable\nones. (Right): Overview of AlphaTuning process that trains scaling factors only for adaptation.\nwhere Î±i âˆˆRhout, Bi âˆˆ{âˆ’1,+1}houtÃ—hin, and\ndiag(Â·) denotes the function of a vector that outputs\na zero-matrix except for the vector elements in its\ndiagonal. A linear operation of Y = XÂ·(W)âŠ¤,\nthen can be approximated as follows:\nY = XÂ·WâŠ¤\nâ‰ˆXÂ·\n( qâˆ‘\ni=1\ndiag (Î±i) Â·Bi\n)âŠ¤\n=\nqâˆ‘\ni=1\n(\n(XÂ·BâŠ¤\ni ) Â·diag (Î±i)\n)\n,\n(2)\nwhere X âˆˆRnbÃ—hin, and Y âˆˆRnbÃ—hout. Note\nthat even though Xis not quantized above, most\ncomplicated floating-point operations are removed\ndue to binary values in B. Since the computational\nadvantages of BCQ have been introduced in the\nliterature (Hubara et al., 2016; Jeon et al., 2020), we\ndo not quantize activations in this work to improve\nquantization quality.\nFigure 2 describes the row-wise BCQ examples\nbased on greedy approximation (Guo et al., 2017)\nwhen q varies. Note that increasing q and/or de-\ncreasing gcan reduce the MSE after quantization\nat the cost of a lower compression ratio.\nTransformer Quantization Table 1 presents our\nBCQ scheme applied to linear layers of the Trans-\nformers while BCQ formats are illustrated for the\nmedium-sized GPT-2 model (that has a hidden size\n(h) of 1024). Note that ifgis large enough such that\neach scaling factor is shared by many weights, the\namount of scaling factors is ignorable compared\nto that of B. In Table 1, hence, 1-bit quantization\nattains almost 32Ã—compression ratio compared to\nFP32 format while lower gslightly increases stor-\nage overhead induced by additional scaling factors.\n4 AlphaTuning: Efficient Fine-Tuning of\nQuantized Models\n4.1 AlphaTuning Principles\nThe key idea of AlphaTuning is identifying param-\neters presenting greater expressive power to mini-\nmize the number of trainable parameters after PTQ.\nNote that training affine parameters (that trans-\nform the activations through operations such as\nscaling, shifting, and rotating) reportedly achieves\nreasonably high accuracy even when all the other\nparameters are fixed to be random (Frankle et al.,\n3291\nModel Method q Trainable Model Size Valid\nLoss\nBLEU (95% Confidence Interval)\nParams CKPT Total Unseen Seen All\nGPT-2\nM\nFT (Fine-Tuning) - 354.9M 1420MB 1420MB 0.79 32.7Â±.6 62.0Â±.4 48.4Â±.3\nâ‡’PTQ(WFT)1 3 - 327MB 327MB 2.03 25.0Â±2.5 58.7Â±1.0 43.2Â±3.3\nLoRA - 0.35M 1.4MB 1420MB 0.81 45.5Â±.4 64.3Â±.2 55.8Â±.3\nâ‡’PTQ(W)+WLoRA\n2 3 - 1.4MB 328MB 2.98 15.8Â±3.0 15.8Â±3.4 15.8Â±3.2\nâ‡’PTQ(W+WLoRA)3 3 - 327MB 327MB 3.36 12.6Â±4.1 16.6Â±6.7 13.6Â±7.5\nAlphaTuning 3 0.22M 0.9MB 327MB 0.81 40.9Â±.5 63.2Â±.5 53.1Â±.4\nAlphaTuning 2 0.22M 0.9MB 289MB 0.84 37.3Â±.5 62.6Â±.5 51.3Â±.5\nGPT-2\nL\nFT (Fine-Tuning) - 774.0M 3096MB 3096MB 0.81 23.8Â±.3 60.8Â±.1 43.0Â±.3\nâ‡’PTQ(WFT) 3 - 535MB 535MB 1.90 23.2Â±.8 62.7Â±.2 43.7Â±.7\nLoRA - 0.77M 3.1MB 3096MB 0.79 48.4Â±.3 64.0Â±.3 57.0Â±.1\nâ‡’PTQ(W)+WLoRA 3 - 3.1MB 538MB 1.97 20.1Â±5.2 27.8Â±4.1 24.1Â±4.5\nâ‡’PTQ(W+WLoRA) 3 - 535MB 535MB 1.97 14.0Â±7.2 26.6Â±11.5 25.8Â±13.0\nAlphaTuning 3 0.42M 1.7MB 535MB 0.84 47.0Â±.6 62.2Â±.2 55.3Â±.3\nAlphaTuning 2 0.42M 1.7MB 445MB 0.82 42.7Â±.4 62.9Â±.4 53.8Â±.1\nAlphaTuning 1 0.42M 1.7MB 355MB 0.87 28.1Â±.3 62.3Â±.7 47.1Â±.4\n1 Fully fine-tuned LMs are quantized by PTQ using the Alternating method.\n2 For inference, quantized PLMs (by the Alternating method) are dequantized to be merged with trainable parameters for\nLoRA. This method is parameter-efficient but we have low scores and dequantization overhead.\n3 After LoRA, frozen weights and trainable weights are merged and then quantized (by the Alternating method). Since PTQ\nis applied to the merged weights, each task needs to store the entire (quantized) model.\nTable 2: Validation loss and test scores on WebNLG with various adaptation methods using GPT-2 models (see\nTable 10 in Appendix for hyper-parameter selections and Table 8 in Appendix additional scores). For full fine-tuning\nand LoRA, we explored learning rates and weight decay factors while the other hyper-parameters are from (Hu\net al., 2022). gis selected to be hin in each layer for row-wise quantization.\n2020b). Interestingly, scaling factors obtained by\nthe BCQ format can be regarded as affine parame-\nters as shown in Eq. 2. Based on such observation,\nFigure 3 presents the overview of AlphaTuning.\nFirst, we quantize the weights of linear layers of\nthe Transformers that dominate the overall memory\nfootprint (Park et al., 2022). Then, the BCQ format\nfactorizes the quantized weights into scaling fac-\ntors and binary values. Finally, the scaling factors\nare trained for a given target task and all the other\nparameters (e.g., biases, binary valuesB, and those\nof the normalization layer and embedding layer)\nare frozen regardless of downstream tasks.\nTraining Algorithm For a linear layer quantized\nby Eq. 1, the forward propagation can be performed\nwithout dequantizing W and be described as Eq. 2.\nSimilarly, the backward propagation can also be\ncomputed in the quantized format and the gradients\nof W and Î±with respect to Y (to conduct the\nchain rule) are obtained as follows:\nâˆ‚X= âˆ‚Y Â·\n( qâˆ‘\ni=1\ndiag(Î±i) Â·Bi\n)\n(3)\nâˆ‚Î±i = (âˆ‚Y)âŠ¤XBâŠ¤\ni Â·1âŠ¤\ngL\n(1 â‰¤iâ‰¤q), (4)\nwhere 1 is an hout-long all-ones vector and gL is\nthe group size of the layer L. Note that dividing\nby gL is empirically introduced in Eq. 4 to prevent\nexcessively large Î± updates and to enhance the\nstability of training. Even if gL Ì¸= hin (i.e., other\nthan row-wise quantization), we still can utilize the\nsame equations by using tiling-based approaches\n(Jeon et al., 2020).\n4.2 AlphaTuning for GPT-2\nWe apply AlphaTuning to GPT-2 medium and large\non WebNLG (Gardent et al., 2017) to explore a\nhyper-parameter space and investigate the effects of\nAlphaTuning as shown in Table 2. Note that in this\npaper, we assume that parameters (including Î±) are\nrepresented as 32-bit floating-point numbers (i.e.,\nFP32 format) unless indicated to be compressed by\nq-bit quantization.\nAdaptation Details PTQ for AlphaTuning is per-\nformed on the pre-trained GPT-2 by the Greedy\nmethod (Guo et al., 2017). Then, for q-bit quanti-\nzation, we train only Î±1 among Î±1 Â·Â·Â·Î±q to max-\nimize parameter-efficiency of adaptation because\ntraining all Î±values provides only marginal gains\nas shown in Table 3. TrainingÎ±1 is performed by a\nlinear decay learning rate schedule without dropout.\nFor each hyper-parameter selection, test scores are\nmeasured at the 5th epoch and averaged over 5 tri-\nals (along with 5 random seeds which are fixed\nfor the experiments in Table 3 justifying our hyper-\n3292\nHyper-Parameter Base Trial Loss Unseen Seen All\nTrainable Params 0.22M ( Î±1) 0.66M ( Î±1,Î±2,Î±3) 0.76 40.6 Â±.4 63 .2 Â±.2 53 .1 Â±.1\nDropout Rate 0.0 0.1 0.81 42.4 Â±.3 61 .2 Â±.4 52 .7 Â±.2\nPTQ Method Greedy Alternating 0.80 41.0 Â±.6 63 .0 Â±.3 53 .0 Â±.3\nLR Warm-up 0 steps 500 steps 0.81 41.0 Â±.2 63 .3 Â±.1 53 .3 Â±.1\nEpochs 5 epochs 3 epochs 0.82 42.2 Â±.6 62 .9 Â±.4 53 .6 Â±.4\nEpochs 5 epochs 10 epochs 0.82 38.5 Â±.7 62 .7 Â±.5 51 .9 Â±.4\nBase Hyper-Parameter Selection (Table 2) 0.81 40.9 Â±.5 63 .2 Â±.5 53 .1 Â±.4\nTable 3: Experimental results on WebNLG to investigate the impact of hyper-parameter selection for AlphaTuning\non GPT-2 medium quantized by PTQ using 3-bit quantization (i.e., q= 3). Test BLEU scores are averaged over 5\ntrials with the same learning rates and weight dacay fectors in Table 2.\n0 2500 5000 7500 10000\nSteps\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8Training Loss\n2000 4000 6000 8000 10000\nSteps\n0.7\n0.8\n0.9\n1.0\n1.1Valid Loss\nFT\nLoRA\nAlphaT(q=2)\nAlphaT(q=3)\n(a) GPT-2 Medium\n0 2500 5000 7500 10000\nSteps\n1.6\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8Training Loss\n2000 4000 6000 8000 10000\nSteps\n0.7\n0.8\n0.9\n1.0\n1.1\n1.2Valid Loss\nFT\nLoRA\nAlphaT(q=1)\nAlphaT(q=2)\nAlphaT(q=3)\n(b) GPT-2 Large\nFigure 4: Training/validation loss on WebNLG by full\nfine-tuning, LoRA, and AlphaTuning (q= 2or 3).\nparameter selections). For all adaptation methods\nconsidered in Table 2, learning rates and weight\ndecay factors are explored to produce the best at\nâ€˜allâ€™ category (see Table 11 for exploration results\non AlphaTuning).\nComparison with Fine-Tuning and LoRA We\ncompare AlphaTuning with full fine-tuning and\nLoRA reproduced by using hyper-parameters (ex-\ncept learning rates and weight decay factors) in\n(Hu et al., 2022) for WebNLG. As shown in Ta-\nble 2, AlphaTuning provides BLUE scores which\nare comparable to that of LoRA and better than that\nof full fine-tuning, while both total memory foot-\nprint and checkpoint (CKPT) 3 memory sizes are\nsignificantly reduced. The different scores can be\npartly explained by Figure 4 showing that the train-\ning process by AlphaTuning or LoRA converges\n3Indicates dedicated storage for each downstream task.\ng Params Loss Unseen Seen All\n64 4.72M 0.78 42.7Â±.3 64.2Â±.2 54.5Â±.2\n256 1.18M 0.77 41.7Â±.4 63.9Â±.3 54.0Â±.2\n512 0.59M 0.77 41.2Â±.7 63.7Â±.1 53.7Â±.3\n1K 0.30M 0.79 40.9Â±.6 63.6Â±.5 53.4Â±.3\nhin 0.22M 0.81 40.9Â±.5 63.2Â±.5 53.1Â±.4\n2K 0.15M 0.84 40.9Â±.4 62.5Â±.7 52.8Â±.4\nTable 4: Impact of g (group size) when AlphaTuning\n(q=3) is applied to GPT-2 medium on WebNLG. When\ng= hin, row-wise quantization is indicated.\nwell while the full fine-tuning causes overfitting.\nInterestingly, even though we train only Î±1 (and\nhence, Î±2 and Î±3 are fixed for all tasks), increasing\nqimproves the validation loss and the test BLEU\nscores. Note that as q increases, â€˜Unseenâ€™ scores\nare enhanced rapidly while â€˜Seenâ€™ scores are not\naffected noticeably. Overall, AlphaTuning with the\n3-bit (i.e., q= 3) quantization can be a successful\nparameter-efficient adaptation with a high compres-\nsion ratio.\nComparison with A;C;D in Figure 1 As po-\ntentially alternative methods of AlphaTuning, we\ninvestigate the following three cases: 1) applying\nPTQ to a fully fine-tuned model (i.e., PTQ(WFT)),\n2) applying PTQ to a PLM and then LoRA parame-\nters are augmented (i.e., PTQ(W)+WLoRA), and 3)\na PLM and LoRA parameters are merged and then\nquantized (i.e., PTQ(W+WLoRA)). Such three cases\ninduce various checkpoint sizes, total model sizes,\nand the number of trainable parameters as shown\nin Table 2. Note that the scores of PTQ(W)+WLoRA\nand PTQ(W+WLoRA) are degraded significantly. In\nother words, model compression techniques and\nparameter-efficient adaptation methods may have\nconflicting properties when combined in a straight-\nforward manner. Even though PTQ(W FT) shows\nbetter scores than the other two cases, the number\nof trainable parameters remains to be the same as\nthat of full fine-tuning and checkpoint size for a\ntask is considerably larger than that of LoRA and\n3293\nModel Method q Trainable Params Valid Loss BLEU METEOR TER\nGPT-2\nMedium\nFine-Tuning - 354.92M - 46.0Â±0.1 0.39 0 .46\nLoRA - 0.35M - 47.1Â±0.2 0.39 0 .46\nAlphaTuning 3 0.22M 1.13 46.6Â±0.2 0.38 0 .48\nAlphaTuning 2 0.22M 1.17 45.7Â±0.2 0.38 0 .49\nGPT-2\nLarge\nFineTuning - 774.03M - 46.5Â±0.1 0.39 0 .45\nLoRA - 0.77M - 47.5Â±0.2 0.38 0 .45\nAlphaTuning 3 0.42M 1.08 47.8Â±0.2 0.39 0 .47\nAlphaTuning 2 0.42M 1.10 47.2Â±0.2 0.38 0 .47\nTable 5: Test scores on DART with various adaptation methods using GPT-2 models (see Table 10 in Appendix for\nhyper-parameter selections). The checkpoint and weight sizes can be found in Table 2. The results of full fine-tuning\nand LoRA are quoted from (Hu et al., 2022). gis selected to be hin in each layer for row-wise quantization. For all\nMETEOR and TER scores in the table, the variances are less than 0.01.\nAlphaTuning. By contrast, AlphaTuning offers ac-\nceptable BLEU scores even with a smaller number\nof trainable parameters and a smaller checkpoint\nsize than those three cases.\nHyper-Parameter Selection A few hyper-\nparameters (such as dropout rate and the number\nof epochs) are related to the trade-off between â€˜Un-\nseenâ€™ score and â€˜Seenâ€™ score as described in Table 3.\nIn the case of PTQ method, even when the Alter-\nnating method (Xu et al., 2018) is employed with\nmany iterations to further reduce MSE, the scores\nbecome similar to that of the Greedy method after\nadaptation. As such, we choose the Greedy method\nfor all tasks in this paper. The learning rate warm-\nup seems to present random effects depending on\nPLM, downstream task, and qselection. The group\nsize gprovides the clear trade-off between the train-\nable parameter size and test scores as shown in Ta-\nble 4. Unless stated otherwise, we choose g= hin\n(i.e., row-wise quantization) in this paper.\n5 Experimental Results\nTo extensively demonstrate the influence of Al-\nphaTuning, we apply detailed adaptation tech-\nniques and hyper-parameter selections that we ex-\nplored by using GPT-2 models on WebNLG (in the\nprevious section) to additional downstream tasks\nand OPT models (Zhang et al., 2022a).\n5.1 GPT-2 Models on DART and E2E\nAdaptations using full fine-tuning, LoRA, and Al-\nphaTuning methods based on pre-trained GPT-2\nmedium/large are performed on DART (Nan et al.,\n2021) and E2E (Novikova et al., 2017). As for\nDART dataset, we observe (in Table 5) AlphaTun-\ning even with an extreme quantization (e.g., q= 2)\ncan maintain test scores to be similar to those of\nLoRA and full fine-tuning, both of which do not\nconsider model compression. In the case of E2E\ndataset (shown in Table 6), we find that 1) full fine-\ntuning suffers from degraded test scores, 2) even\nAlphaTuning with q = 1 is a reasonable choice\nfor GPT-2 large, and 3) quantizing a model (af-\nter being adapted by LoRA) destroys test scores.\nAll in all, when combined with pre-trained GPT-2\nmedium/large on various tasks, AlphaTuning turns\nout to be effective for both a high compression ratio\nand a massive reduction in the number of trainable\nparameters.\n5.2 OPT Models on MNLI and SAMSum\nWe utilize a pre-trained OPT 1.3B model to be\nadapted through full fine-tuning or AlphaTuning\non GLUE-MNLI (Williams et al., 2018) and SAM-\nSum (Gliwa et al., 2019). For text classification\non MNLI, an LM head layer is added on top of\nGPT-2 with randomly initialized weights (Radford\net al., 2019). As evidenced by Table 7, we find\nthe following results: 1) PTQ(WFT) sometimes re-\nsults in severely impaired scores (e.g., on SAMSum\ndataset) even when computations for PTQ are asso-\nciated with a lot of iterations; 2) AlphaTuning out-\nperforms PTQ(WFT) scheme (for the whole tasks\nin this paper), and 3) decreasing gof AlphaTuning\ncan improve scores.\n6 Discussion\nMemory during Adaptation As a compression-\naware parameter-efficient adaptation technique, Al-\nphaTuning reduces not only inference memory foot-\nprints (by quantization) and also training memory\nfootprints during adaptation. Specifically, optimizer\nstates to be stored in GPU memory are derived\nonly by scaling factors that occupy less than 0.1%\nof total weight size if gis large enough. Such re-\nduced GPU memory requirements during training\ncorrespond to increased batch size or a reduced\n3294\nModel Method q Trainable Valid Test Scores\nParams Loss BLEU NIST METEOR ROUGE_L CIDEr\nGPT-2\nM\nFine-Tuning - 354.92M 1.28 67.5Â±.2 8.60Â±.02 46.4Â±.2 70.8Â±.2 2.40Â±.01\nâ‡’PTQ(WFT) 3 - 1.19 67.5Â±.5 8.58Â±.04 46.3Â±.5 70.3Â±.1 2.39Â±.01\nLoRA - 0.35M 1.16 70.2Â±.2 8.80Â±.04 46.8Â±.1 71.7Â±.4 2.53Â±.01\nâ‡’PTQ(W)+WLoRA 3 - 4.10 11.1Â±5.3 2.35Â±1.04 12.4Â±4.2 29.9Â±7.8 0.35Â±.18\nâ‡’PTQ(W+WLoRA) 3 - 4.38 7.5Â±3.2 1.31Â±.21 11.4Â±.8 29.8Â±3.0 0.29Â±.04\nAlphaTuning 3 0.22M 1.18 69.9Â±.3 8.79Â±.05 46.7Â±.2 71.7Â±.3 2.51Â±.01\nAlphaTuning 2 0.22M 1.20 70.0Â±.4 8.80Â±.05 46.7Â±.1 71.6Â±.5 2.51Â±.01\nGPT-2\nL\nFine-Tuning - 774.03M 1.31 67.2Â±.3 8.61Â±.05 46.3Â±.1 70.5Â±.3 2.37Â±.01\nâ‡’PTQ(WFT) 3 - 0.98 66.5Â±1.0 8.45Â±.10 45.7Â±.3 70.3Â±.5 2.37Â±.03\nLoRA - 0.77M 1.13 69.8Â±.2 8.80Â±.03 46.6Â±.1 71.7Â±.1 2.51Â±.01\nâ‡’PTQ(W)+WLoRA 3 - 1.87 50.9Â±3.4 6.63Â±.32 38.7Â±1.9 60.6Â±1.9 1.30Â±.19\nâ‡’PTQ(W+WLoRA) 3 - 1.76 53.7Â±3.1 7.12Â±.37 40.0Â±1.5 61.7Â±1.1 1.5Â±.11\nAlphaTuning 2 0.42M 1.14 69.7Â±.6 8.78Â±.08 46.6Â±.2 71.5Â±.3 2.51Â±.03\nAlphaTuning 1 0.42M 1.18 69.7Â±.3 8.79Â±.03 46.6Â±.1 71.6Â±.2 2.51Â±.02\nTable 6: Validation loss and test scores on E2E with various adaptation methods using GPT-2 models (see Table 10\nin Appendix for hyper-parameter selections). The number of trainable parameters, checkpoint sizes, and weight\nsizes are the same as in Table 2. For full fine-tuning and LoRA, we explored learning rates and weight decay factors\nwhile the other hyper-parameters are quoted from (Hu et al., 2022).gis selected to be hin in each layer for row-wise\nquantization.\nMethod q g\nMNLI SAMSum\nTrainable Wight Accuracy Trainable Weight R1 / R2 / RLParams Size (%) Params Size\nFine-Tuning - - 1315.76M 5.26GB 83.6 1315.75M 5.26GB 49.4 / 25.3 / 40.5\nâ‡’PTQ(WFT) 4 h - 1.04GB 76.7 - 1.04GB 13.0 / 5.5 / 11.4\nAlphaTuning 4 0.5h 1.19M 1.05GB 82.7 1.18M 1.05GB 47.5 / 24.1 / 38.9\nAlphaTuning 4 h 0.60M 1.04GB 82.3 0.59M 1.04GB 47.3 / 23.2 / 38.4\nAlphaTuning 3 0.5h 1.19M 0.90GB 82.4 1.18M 0.90GB 47.4 / 24.2 / 39.0\nAlphaTuning 3 h 0.60M 0.89GB 82.4 0.59M 0.89GB 46.5 / 22.8 / 37.8\nTable 7: Validation scores on MNLI dataset and test scores on SAMSum dataset with full fine-tuning and AlphaTun-\ning using OPT 1.3B model for which hidden size his 2048 (see Appendix B for experimental details).\nminimum number of GPUs performing adaptation.\nEmbedding Layers In this work, we considered\nlinear layers of the Transformers to be quantized\nby BCQ while embedding layers remain to be of\nfull precision. The rationale behind this choice is\nthat as the model scales with a larger hidden size\n(h), the relative size of embedding layers becomes\nsmaller. To be more specific, the space complexi-\nties of linear layers and embedding layers follow\nO(h2) and O(h), respectively. As such, for large-\nscale LMs, we expect quantizing embedding layers\nto produce only marginal improvements on a com-\npression ratio while test scores might be degraded.\nInference Speed As described in Eq. 2, BCQ for-\nmat enables unique computations for matrix mul-\ntiplications even when activations are not quan-\ntized. Recent works (Jeon et al., 2020; Park et al.,\n2022) show that matrix multiplications based on\nBCQ format can be expedited by the following\noperations: 1) compute all possible computations\n(combining partial activations and B) in advance\nand store them in look-up tables (LUTs) and 2) let\nLUT retrievals (using Bvalues as indices) replace\nfloating-point additions in Eq. 2. The major reasons\nfor fast computations are due to byte-level accesses\nof LUTs and increased LUT reuse by increased h\n(Jeon et al., 2020; Park et al., 2022). Such LUT-\nbased matrix multiplications can lead to latency\nimprovement as much as a memory reduction ratio.\n7 Conclusion\nIn this paper, we proposed AlphaTuning as the first\nsuccessful compression-aware parameter-efficient\nadaptation method for large-scale LMs. Through a\nfew representative generative LMs (such as GPT-2),\nwe demonstrated that once linear layers are quan-\ntized by BCQ format, training only scaling factors\ncan obtain reasonably high scores. We also empir-\nically proved that quantizing an already adapted\nLMs would degrade scores significantly. Incorpo-\nrating various model compression techniques and\nparameter-efficient adaptation methods would be\nan interesting research topic in the future.\n3295\nLimitations\nWe believe that the major contributions in this pa-\nper would become more convincing as the size of\nthe PLM increases, whereas the models used for ex-\nperiments in this paper (i.e., GPT-2 and 1.3B OPT)\nmay not be large enough compared to the large-\nscale LMs recently announced ( e.g., OPT 175B).\nConsidering a few reports that larger models tend\nto be compressed by a higher compression ratio\nalong with less performance degradation (Li et al.,\n2020b), we expect AlphaTuning to be effective as\nwell even for larger models, to say, of more than\n10 billion of parameters.\nThe performance of AlphaTuning on 1.3B OPT\nbecomes better than that of PTQ(W FT), but infe-\nrior to that of the full fine-tuning. We suspect such\nresults might result from insufficient search of an\nappropriate training recipe for AlphaTuning. Corre-\nspondingly, exploring learning hyper-parameters of\nAlphaTuning using larger LMs and more datasets\nwould be required to yield general claims on the\ncharacteristics of AlphaTuning.\nEthics Statement\nLarge language models (LMs) such as GPT-\n3 (Brown et al., 2020), Gopher (Rae et al., 2021),\nPaLM (Chowdhery et al., 2022), and Hyper-\nCLOV A (Kim et al., 2021a) have shown surpris-\ning capabilities and performances for natural lan-\nguage understanding and generation, in particular,\nan in-context zero or few-shot manner. They can\nprovide innovative applications such as code gen-\neration (Chen et al., 2021) and text-to-image gen-\neration (Ramesh et al., 2021) via fine-tuning with\nadditional data dedicated to each task. Despite their\nastonishing advantages, it is well known that large\nLMs have severe and challenging limitations for\ndeployment to user applications, such as biased\nand toxic expression, hallucination, too heavy en-\nergy consumption, and carbon emission (Weidinger\net al., 2021). Our work aims to address the energy\nissue of large LMs in terms of inference and de-\nployment. We expect that our method can alleviate\nenergy consumption by reducing practical memory\nfootprints and latency when the large LMs are de-\nployed to various user applications. We might need\nto address the other ethical issues through further\nresearch for safer and better contributions of the\nlarge LMs.\nReferences\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed,\nand Michael Auli. 2020. Wav2vec 2.0: A framework\nfor self-supervised learning of speech representations.\nIn Advances in Neural Information Processing Sys-\ntems, volume 33, pages 12449â€“12460.\nAishwarya Bhandare, Vamsi Sripathi, Deepthi Karkada,\nVivek Menon, Sun Choi, Kushal Datta, and Vikram\nSaletore. 2019. Efficient 8-bit quantization of trans-\nformer neural machine language translation model.\narXiv:1906.00532.\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. GPT-Neo: Large Scale\nAutoregressive Language Modeling with Mesh-\nTensorflow. If you use this software, please cite it\nusing these metadata.\nYelysei Bondarenko, Markus Nagel, and Tijmen\nBlankevoort. 2021. Understanding and overcoming\nthe challenges of efficient transformer quantization.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n7947â€“7969.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877â€“1901.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021. Evaluating large lan-\nguage models trained on code. arXiv preprint\narXiv:2107.03374.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nInsoo Chung, Byeongwook Kim, Yoonjung Choi,\nSe Jung Kwon, Yongkweon Jeon, Baeseong Park,\nSangha Kim, and Dongsoo Lee. 2020. Extremely\nlow bit transformer quantization for on-device neural\nmachine translation. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4812â€“4826.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171â€“4186.\n3296\nSteven K. Esser, Jeffrey L. McKinstry, Deepika Bablani,\nRathinakumar Appuswamy, and Dharmendra S.\nModha. 2020. Learned step size quantization. In\nInternational Conference on Learning Representa-\ntions.\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel\nRoy, and Michael Carbin. 2020a. Pruning neural\nnetworks at initialization: Why are we missing the\nmark? In International Conference on Learning Rep-\nresentations.\nJonathan Frankle, David J Schwab, and Ari S Morcos.\n2020b. Training batchnorm and only batchnorm:\nOn the expressive power of random features in cnns.\narXiv preprint arXiv:2003.00152.\nDeep Ganguli, Danny Hernandez, Liane Lovitt, Nova\nDasSarma, Tom Henighan, Andy Jones, Nicholas\nJoseph, Jackson Kernion, Ben Mann, Amanda Askell,\net al. 2022. Predictability and surprise in large gener-\native models. arXiv preprint arXiv:2202.07785.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020. Mak-\ning pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. The WebNLG\nchallenge: Generating text from RDF data. In Pro-\nceedings of the 10th International Conference on\nNatural Language Generation, pages 124â€“133.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\nsander Wawer. 2019. Samsum corpus: A human-\nannotated dialogue dataset for abstractive summa-\nrization. arXiv preprint arXiv:1911.12237.\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.\n2022. Ppt: Pre-trained prompt tuning for few-shot\nlearning. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 8410â€“8423.\nYiwen Guo, Anbang Yao, Hao Zhao, and Yurong Chen.\n2017. Network sketching: exploiting binary structure\nin deep CNNs. In IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), pages 4040â€“\n4048.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2021. Towards a\nunified view of parameter-efficient transfer learning.\nIn International Conference on Learning Representa-\ntions.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Pi-\notr DollÃ¡r, and Ross Girshick. 2022. Masked autoen-\ncoders are scalable vision learners. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 16000â€“16009.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022a. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022b. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for nlp. In In-\nternational Conference on Machine Learning, pages\n2790â€“2799. PMLR.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nItay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran\nEl-Yaniv, and Yoshua Bengio. 2016. Quantized neu-\nral networks: training neural networks with low pre-\ncision weights and activations. arXiv:1609.07061.\nItay Hubara, Yury Nahshan, Yair Hanani, Ron Banner,\nand Daniel Soudry. 2020. Improving post training\nneural quantization: Layer-wise calibration and inte-\nger programming. arXiv preprint arXiv:2006.10518.\nBenoit Jacob, Skirmantas Kligys, Bo Chen, Meng-\nlong Zhu, Matthew Tang, Andrew Howard, Hartwig\nAdam, and Dmitry Kalenichenko. 2018. Quantiza-\ntion and training of neural networks for efficient\ninteger-arithmetic-only inference. In Proceedings\nof the IEEE Conference on Computer Vision and Pat-\ntern Recognition, pages 2704â€“2713.\nYongkweon Jeon, Baeseong Park, Se Jung Kwon,\nByeongwook Kim, Jeongin Yun, and Dongsoo Lee.\n2020. Biqgemm: matrix multiplication with lookup\ntable for binary-coding-based quantized dnns. In\nSC20: International Conference for High Perfor-\nmance Computing, Networking, Storage and Analy-\nsis, pages 1â€“14. IEEE.\nRabeeh Karimi Mahabadi, James Henderson, and Se-\nbastian Ruder. 2021. Compacter: Efficient low-rank\nhypercomplex adapter layers. Advances in Neural\nInformation Processing Systems, 34:1022â€“1035.\nBoseop Kim, HyoungSeok Kim, Sang-Woo Lee,\nGichang Lee, Donghyun Kwak, Jeon Dong Hyeon,\nSunghyun Park, Sungju Kim, Seonhoon Kim, Dong-\npil Seo, et al. 2021a. What changes can large-scale\nlanguage models bring? intensive study on hyper-\nclova: Billions-scale korean generative pretrained\ntransformers. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 3405â€“3424.\n3297\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W\nMahoney, and Kurt Keutzer. 2021b. I-bert: Integer-\nonly bert quantization. In International conference\non machine learning, pages 5506â€“5518. PMLR.\nDongsoo Lee and Byeongwook Kim. 2018. Retraining-\nbased iterative weight quantization for deep neural\nnetworks. arXiv:1805.11233.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045â€“3059.\nDa Li, Yongxin Yang, Yi-Zhe Song, and Timothy M\nHospedales. 2018. Learning to generalize: Meta-\nlearning for domain generalization. In Thirty-Second\nAAAI Conference on Artificial Intelligence.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582â€“\n4597.\nYuhang Li, Ruihao Gong, Xu Tan, Yang Yang, Peng\nHu, Qi Zhang, Fengwei Yu, Wei Wang, and Shi Gu.\n2020a. Brecq: Pushing the limit of post-training\nquantization by block reconstruction. In Interna-\ntional Conference on Learning Representations.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt\nKeutzer, Dan Klein, and Joey Gonzalez. 2020b. Train\nbig, then compress: Rethinking model size for effi-\ncient training and inference of transformers. In In-\nternational Conference on Machine Learning, pages\n5958â€“5968. PMLR.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021a. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021b. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Confer-\nence on Learning Representations.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n8086â€“8098.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nTara N. Sainath, Brian Kingsbury, Vikas Sindhwani,\nEbru Arisoy, and Bhuvana Ramabhadran. 2013. Low-\nrank matrix factorization for deep neural network\ntraining with high-dimensional output targets. In\nICASSP, pages 6655â€“6659.\nLinyong Nan, Dragomir R Radev, Rui Zhang, Amrit\nRau, Abhinand Sivaprasad, Chiachun Hsieh, Xiangru\nTang, Aadit Vyas, Neha Verma, Pranav Krishna, et al.\n2021. Dart: Open-domain structured data record to\ntext generation. In NAACL-HLT.\nDeepak Narayanan, Mohammad Shoeybi, Jared Casper,\nPatrick LeGresley, Mostofa Patwary, Vijay Kor-\nthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, Julie\nBernauer, Bryan Catanzaro, et al. 2021. Efficient\nlarge-scale language model training on gpu clusters\nusing megatron-lm. In Proceedings of the Interna-\ntional Conference for High Performance Computing,\nNetworking, Storage and Analysis, pages 1â€“15.\nJekaterina Novikova, OndË‡rej DuÅ¡ek, and Verena Rieser.\n2017. The e2e dataset: New challenges for end-to-\nend generation. In Proceedings of the 18th Annual\nSIGdial Meeting on Discourse and Dialogue, pages\n201â€“206.\nGunho Park, Baeseong Park, Se Jung Kwon, Byeong-\nwook Kim, Youngjoo Lee, and Dongsoo Lee. 2022.\nnuqmm: Quantized matmul for efficient inference of\nlarge-scale generative language models.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea V oss, Alec Radford, Mark Chen, and\nIlya Sutskever. 2021. Zero-shot text-to-image gen-\neration. In International Conference on Machine\nLearning, pages 8821â€“8831. PMLR.\nMohammad Rastegari, Vicente Ordonez, Joseph Red-\nmon, and Ali Farhadi. 2016. XNOR-Net: Imagenet\nclassification using binary convolutional neural net-\nworks. In ECCV.\nLaria Reynolds and Kyle McDonell. 2021. Prompt pro-\ngramming for large language models: Beyond the\nfew-shot paradigm. In Extended Abstracts of the\n2021 CHI Conference on Human Factors in Comput-\ning Systems, pages 1â€“7.\n3298\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4222â€“4235.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\narXiv preprint arXiv:1909.08053.\nShaden Smith, Mostofa Patwary, Brandon Norick,\nPatrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George\nZerveas, Vijay Korthikanti, et al. 2022. Using deep-\nspeed and megatron to train megatron-turing nlg\n530b, a large-scale generative language model. arXiv\npreprint arXiv:2201.11990.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nTu Vu, Brian Lester, Noah Constant, Rami Al-Rfou, and\nDaniel Cer. 2022. Spot: Better frozen model adap-\ntation through soft prompt transfer. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 5039â€“5059.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGriffin, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\net al. 2021. Ethical and social risks of harm from\nlanguage models. arXiv preprint arXiv:2112.04359.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112â€“1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38â€“45, Online. Association\nfor Computational Linguistics.\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jian-\nmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. 2022.\nSimmim: A simple framework for masked image\nmodeling. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 9653â€“9663.\nChen Xu, Jianqiang Yao, Zhouchen Lin, Wenwu Ou,\nYuanbin Cao, Zhirong Wang, and Hongbin Zha. 2018.\nAlternating multi-bit quantization for recurrent neural\nnetworks. In International Conference on Learning\nRepresentations (ICLR).\nOfir Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert. In\n2019 Fifth Workshop on Energy Efficient Machine\nLearning and Cognitive Computing-NeurIPS Edition\n(EMC2-NIPS), pages 36â€“39. IEEE.\nAston Zhang, Yi Tay, SHUAI Zhang, Alvin Chan,\nAnh Tuan Luu, Siu Hui, and Jie Fu. 2020. Beyond\nfully-connected layers with quaternions: Parameter-\nization of hypercomplex multiplications with 1/n\nparameters. In International Conference on Learn-\ning Representations.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022a. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nZhengkun Zhang, Wenya Guo, Xiaojun Meng, Yasheng\nWang, Yadao Wang, Xin Jiang, Qun Liu, and\nZhenglu Yang. 2022b. Hyperpelt: Unified parameter-\nefficient language model tuning for both language\nand vision-and-language tasks. arXiv preprint\narXiv:2203.03878.\nRitchie Zhao, Yuwei Hu, Jordan Dotzel, Christopher De\nSa, and Zhiru Zhang. 2019. Improving neural net-\nwork quantization without retraining using outlier\nchannel splitting. In International Conference on\nMachine Learning (ICML), pages 7543â€“7552.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In In-\nternational Conference on Machine Learning, pages\n12697â€“12706. PMLR.\n3299\n0 10000 20000\nSteps\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2Training Loss\n5000 10000 15000 20000 25000\nSteps\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5Valid Loss\nFT\nLoRA\nAlphaT(q=2)\nAlphaT(q=3)\n(a) GPT-2 Medium\n0 10000 20000\nSteps\n1.8\n2.0\n2.2\n2.4\n2.6\n2.8\n3.0\n3.2Training Loss\n5000 10000 15000 20000 25000\nSteps\n1.0\n1.1\n1.2\n1.3\n1.4\n1.5Valid Loss\nFT\nLoRA\nAlphaT(q=1)\nAlphaT(q=2)\n(b) GPT-2 Large\nFigure 5: Training/validation loss on E2E by full fine-\ntuning, LoRA, and AlphaTuning (q=1, 2, or 3)\nA Experimental Details on GPT-2 Models\nFor all the adaptation experiments, we utilize the\npre-trained GPT-2 Medium4/Large5 models pro-\nvided by HuggingFace (Wolf et al., 2020). GPT-2\nMedium consists of 24 Transformer layers with a\nhidden size (h) of 1024 and GPT-2 Large is com-\nposed of 36 Transformer layers with a hidden size\nof 1280. Table 1 includes the types of sublayers\nembedded into a GPT-2 layer.\nA.1 Dataset\nWebNLG Challenge 2017(Gardent et al., 2017)\nconsists of 25,298 (data,text) pairs with 14 cate-\ngories, which can be divided into nine â€œSeenâ€ cate-\ngories and five â€œUnseenâ€ categories. The model\ntakes Resource Description Framework (RDF)\ntriples as inputs and generates natural text descrip-\ntions to perform data-to-text generation task. Since\nthe gradients during adaptation processes are cal-\nculated with only the â€œSeenâ€ categories, the mea-\nsured scores from â€œUnseenâ€ categories are impor-\ntant for evaluating the generation performance of\nmodels. In this paper, we represent three types\nof scores, â€˜Unseenâ€™(U), â€˜Seenâ€™(S), and â€˜Allâ€™(A).\n4Available at https://s3.amazonaws.com/models.\nhuggingface.co/bert/gpt2-medium-pytorch_model.\nbin.\n5Available at https://s3.amazonaws.com/models.\nhuggingface.co/bert/gpt2-large-pytorch_model.\nbin.\nHyper-parameters are selected according to the best\nâ€™Allâ€™ score.\nDART(DAta Record to Text, Nan et al. (2021))\nis an open-domain text generation dataset with 82k\nexamples, which are extracted from several datasets\nincluding WebNLG 2017 and Cleaned E2E.\nE2E (Novikova et al., 2017) was proposed for\ntraining end-to-end and data-driven natural lan-\nguage generation task. It consists of about 50k\ninstances that provide meaning representations\n(MRs) and references for inference in the restaurant\ndomain. Language models should perform data-to-\ntext generation using suggested MRs.\nA.2 Adaptation Details\nFor all the reproduced experiments and AlphaTun-\ning experiments, AdamW (Loshchilov and Hutter,\n2018) optimizer and linear-decaying learning rate\nscheduler were used. The number of epochs for the\nadaptation process is fixed to be 5 epochs and the\nother hyper-parameters are the same as reported in\nLi and Liang (2021); Hu et al. (2022). We did not\ntry to find the best results by evaluating and compar-\ning the checkpoint at every epoch or by adjusting\nthe number of epochs. Instead, we explore the best\nresults under varied learning rates and weight de-\ncay based on the reported list of hyper-parameters\nin Li and Liang (2021) and Hu et al. (2022) (the\nreaders are referred to Table 10).\nTo evaluate the performance of the GPT-2 mod-\nels, we use the beam search algorithm with several\nhyper-parameters listed in Table 9.\nB Experimental Details on OPT models\nTo study performance on downstream tasks of Al-\nphaTuning using larger PLMs (than GPT-2), we uti-\nlize pre-trained OPT models (Zhang et al., 2022a)\non GLUE-MNLI and SAMSum datasets. Due to\nthe limitations on resources, our experiments are\nrestrained to 1.3B model6 with 24 layers (h=2048).\nFine-tuning and Alphatuning are performed under\nthe conditions that we describe in the following\nsubsections.\nB.1 Dataset\nMNLI(Williams et al., 2018)(Multi-Genre Natural\nLanguage Inference) evaluates the sentence under-\nstanding performance. Given a pair of premise and\n6Available at https://huggingface.co/facebook/\nopt-1.3b\n3300\nSize Method q BLEU METEOR TER\nU S A U S A U S A\nGPT\nM\nFT(Fine-Tuning) - 32.7Â±.6 62.0Â±.4 48.4Â±.3 .32 .45 .39 .63 .33 .47\nâ‡’PTQ(WFT) 3 25.0Â±2.5 58.7Â±1.0 43.2Â±3.3 .28 .43 .36 .87 .37 .60\nLoRA - 45.5Â±.4 64.3Â±.2 55.8Â±.3 .38 .45 .42 .47 .32 .39\nâ‡’PTQ(W)+WLoRA 3 15.8Â±3.0 15.8Â±3.4 15.8Â±3.2 .20 .21 .21 1 .03 1 .20 1 .12\nâ‡’PTQ(W+WLoRA) 3 12.6Â±4.1 16.6Â±6.7 13.6Â±7.5 .17 .18 .18 .72 .70 .68\nAlphaTuning 3 40.9Â±.5 63.2Â±.5 53.1Â±.4 .35 .44 .40 .51 .33 .42\nAlphaTuning 2 37.3Â±.5 62.6Â±.5 51.3Â±.5 .33 .44 .39 .55 .33 .43\nGPT\nL\nFT(Fine-Tuning) - 23.8Â±.3 60.8Â±.1 43.0Â±.3 .27 .45 .36 .77 .34 .54\nâ‡’PTQ(WFT) 3 23.2Â±.8 62.7Â±.2 43.7Â±.7 .27 .45 .36 .77 .33 .54\nLoRA - 48.4Â±.3 64.0Â±.3 57.0Â±.1 .39 .45 .42 .45 .32 .38\nâ‡’PTQ(W)+WLoRA 3 20.1Â±5.2 27.8Â±4.1 24.1Â±4.5 .21 .25 .23 .99 .84 .91\nâ‡’PTQ(W+WLoRA) 3 14.0Â±7.2 26.6Â±11.5 25.8Â±13.0 .16 .23 .23 1 .24 .76 .79\nAlphaTuning 3 47.0Â±.6 62.2Â±.2 55.3Â±.3 .38 .43 .41 .46 .33 .39\nAlphaTuning 2 42.7Â±.4 62.9Â±.4 53.8Â±.1 .36 .44 .40 .49 .33 .41\nAlphaTuning 1 28.1Â±.3 62.3Â±.7 47.1Â±.4 .29 .44 .36 .66 .33 .49\nTable 8: Additional scores on WebNLG using GPT-2 (Extended results of Table 2) including METEOR and TER\nscores (U: Unseen, S:Seen, A: All). Lower TER score indicates better generation capability while other scores are\nto be higher for better capability. For METEOR and TAR scores, the variances of all the cases are less than 0.01.\nFT/LoRA AlphaTuning\nBeam size 10 10\nBatch size 16 16\nNo repeat ngram size 4 4\nLength penalty 0.9 0.8\nTable 9: Hyper-parameters for beam search decoding\nhypothesis, the main task is to classify the relation-\nship between the two sentences into one of entail-\nment, contradiction, and neutral. A linear classifier\nhead with three output logits is attached on top\nof the language model and fine-tuned along with\nthe model. The addition of a linear layer slightly\nincreases the overall parameter, unlike other Al-\nphaTuning experiments that only learn Î±.\nSAMSum(Gliwa et al., 2019) is a conversation\ndataset containing 16k summaries. Given a dialog,\nthe goal is to generate summarizations to evaluate\ndialog understanding and natural language genera-\ntion capabilities. For diversity, each conversation\nstyle includes informal, semi-formal, and formal\ntypes with slang words, emoticons, and typos.\nB.2 Adaptation details\nExperimental configurations for adaptation are pre-\nsented in the Table 12. During the training and\nevaluation of the SAMSum dataset, the beam size\nis fixed to be 4 and the generation max length is\nfixed to be 256 (the condition max length is fixed\nto be 192 and the label max length to be 64). Due\nto the decoder-only structure of OPT, the token se-\nquence corresponding to the conditions above was\nput into the input and learned together.\nC Details on BCQ Format\nThis section introduces two popular methods to\nproduce binary codes and scaling factors from full-\nprecision DNN weights. The common objective\nis to minimize mean square error (MSE) between\noriginal data and quantized data in heuristic manner.\nAs introduced in (Rastegari et al., 2016) (i.e.q=1),\na weight vector wis approximated to Î±bwhere Î±\nis a full-precision scaling factor and bis a binary\nvector (bâˆˆ{âˆ’1,+1}n). For one-bit quantization,\nthere is an analytic solution to minimizeâˆ¥wâˆ’Î±bâˆ¥2\nas following:\nbâˆ—= sign(w), Î±âˆ—= wâŠ¤bâˆ—\nn . (5)\nHowever, if we extend this equation to multi-bit\n(q >1) quantization, there is no analytic solution.\nGreedy Approximation first produces Î±1 and\nb1 as in Eq. 5, and then calculates Î±i and bi iter-\natively by minimizing the residual errors ( âˆ¥wâˆ’âˆ‘iâˆ’1\nj=1 Î±jbjâˆ¥2). Then, Î±i and bi are calculated as\nfollows:\nbâˆ—\ni = sign(wâˆ’\niâˆ’1âˆ‘\nj=1\nÎ±jbj). (6)\nÎ±âˆ—\ni =\nwâˆ’âˆ‘iâˆ’1\nj=1 Î±jbâŠ¤\nj bâˆ—\ni\nn . (7)\nAlthough this method is computationally simple,\nit leads to higher MSE by quantization. In spite\nof higher quantization error, AlphaTuning utilizes\nthis Greedy method only for the initial PTQ pro-\ncess. We observe the adapted LMs with AlphaTun-\n3301\nDataset Model Method Learning rate Weight decay\nbest range best range\nWebNLG\n(Table 2)\nGPT\nM\nFT 1e-4 {1e-4, 2e-4, 5e-4} 0.01 {0.0, 0.01,\n0.02} LoRA 5e-4 0.01\nAlphaTuning(q=3) 1e-3 {1e-4, 2e-4, 5e-4,\n1e-3, 2e-3}\n0.05 {0.0, 0.01,\n0.05, 0.1} AlphaTuning(q=2) 1e-3 0.0\nGPT\nL\nFT 1e-4 {1e-4, 2e-4, 5e-4} 0.01 {0.0, 0.01,\n0.02} LoRA 2e-4 0.02\nAlphaTuning(q=3) 1e-4 {1e-4, 2e-4, 5e-4,\n1e-3, 2e-3}\n0.0 {0.0, 0.01,\n0.05, 0.1}AlphaTuning(q=2) 1e-4 0.0\nAlphaTuning(q=1) 1e-3 0.01\nDART\n(Table 5)\nGPT\nM\nAlphaTuning(q=3) 1e-3 {1e-4, 2e-4, 5e-4,\n1e-3, 2e-3}\n0.01 {0.0, 0.01,\n0.05, 0.1} AlphaTuning(q=2) 1e-3 0.05\nGPT\nL\nAlphaTuning(q=3) 5e-4 {1e-4, 2e-4, 5e-4,\n1e-3, 2e-3}\n0.1 {0.0, 0.01,\n0.05, 0.1} AlphaTuning(q=2) 1e-3 0.1\nE2E\n(Table 6)\nGPT\nM\nFT 1e-4 {1e-4, 2e-4, 5e-4} 0.02 {0.0, 0.01,\n0.02} LoRA 2e-4 0.01\nAlphaTuning(q=3) 2e-3 {1e-4, 2e-4, 5e-4,\n1e-3, 2e-3}\n0.0 {0.0, 0.01,\n0.05, 0.1} AlphaTuning(q=2) 5e-3 0.1\nGPT\nL\nFT 5e-4 {1e-4, 2e-4, 5e-4} 0.01 {0.0, 0.01,\n0.02} LoRA 2e-4 0.0\nAlphaTuning(q=2) 1e-3 {1e-4, 2e-4, 5e-4,\n1e-3, 2e-3}\n0.1 {0.0, 0.01,\n0.05, 0.1} AlphaTuning(q=1) 1e-3 0.1\nTable 10: Selected hyper-parameters (learning rates and weight decay) for GPT-M/L results on this paper. We set\nthe hyper-parameter ranges according to the reported parameters in the previous papers (Li and Liang, 2021; Hu\net al., 2022). For each hyper-parameter selection, the test scores are measured at the last epoch and averaged over 5\ntrials, which are performed with fixed 5 random seeds.\nModel q Learning rate Weight decay Loss Unseen Seen All\nGPT-M\n2\n2e-3\n0.00\n0.84 36.7Â±0.9 62.7Â±0.6 51.05Â±0.6\n1e-3 0.84 37 .3Â±0.5 62.6Â±0.5 51.26Â±0.5\n5e-4 0.87 37 .8Â±0.5 61.9Â±0.5 51.08Â±0.3\n2e-4 0.93 38.2Â±0.3 60.1Â±0.2 50.31Â±0.2\n3\n2e-3\n0.05\n0.80 40.1Â±0.6 63.6Â±0.2 53.1Â±0.4\n1e-3 0.81 40 .9Â±0.5 63.2Â±0.5 53.1Â±0.4\n5e-4 0.83 41.2Â±0.4 62.7Â±0.3 53.0Â±0.3\n2e-4 0.87 41 .0Â±0.1 61.3Â±0.2 52.2Â±0.1\nTable 11: BLEU scores of GPT2-M AlphaTuning on WebNLG dataset when the learning rates vary. Higher learning\nrates lead to better â€˜Seenâ€™ scores, but lead to worse â€˜Unseenâ€™ scores (less generative capability). Reversely, lower\nlearning rates lead to better â€˜Unseenâ€™ scores.\nMNLI SAMSum\nMethod FT AlphaT FT AlphaT\nLearning Rate (LR) 5e-6 5e-5 6e-6 1e-4\nWeight Decay 0.01 0.05 0.01 0.05\nOptimizer AdamW Adafactor\nEpoch 3 5\nLR Scheduler Linear decay\nBatch 32\nTable 12: Hyper-parameter selection for the experiments\nusing OPT models\ning (along with Greedy approximation) can reach\nthe comparable scores of full fine-tuning or LoRA\nwhile there is no noticeable improvement by using\nthe Alternating method, an advanced method that\nwe discuss next.\nAlternating Method (Xu et al., 2018) adjusts\nscaling factors and binary values iteratively after\nproducing the initial Î±1..q and b1..q obtained by\nGreedy approximation method. From the initial\nb1..q, Î±1..q can be refined as\n[Î±1,...,Î± q] =\n((\nBâŠ¤\nq Bq\n)âˆ’1\nBâŠ¤\nq w\n)âŠ¤\n, (8)\nwhere Bq = [b1,..., bq] âˆˆ{âˆ’1,+1}nÃ—q. From the\nrefined Î±1..q, the elements in b1..q can be further\nrefined using a binary search algorithm. As we\niterate the process of refining the scaling factor\nand the binary vector repeatedly, the errors due\nto quantization get reduced. When the amount of\nerror is reduced to become close enough to zero,\nwe can stop such iterative processes. It has been\nknown that an appropriate number of iterations is\napproximately between 3 and 15 (Xu et al., 2018;\n3302\nLee and Kim, 2018), and this paper set the number\nof iterations as 15 when the Alternating method is\nselected for PTQ process.\nIn practice, higher scores right after PTQ are\nattainable by the Alternating quantization method\nrather than the Greedy method. Thus, we try pre-\nvious experiments using Alternating method as\nshown in Table 13 and 14 on WebNLG and E2E\ndataset. From those two tables, it should be noted\nthat even when the Alternating algorithm is chosen,\nwe can observe that post-training quantization still\nleads to considerable performance degradation.\ngroup-wise quantization While Eq. 5-7 are de-\nscribed for a weight â€˜vectorâ€™ (w) (for simplicity),\nthe target parameters to be quantized in this paper\nare in the form of weight â€˜matricesâ€™ of LMs. We\nextended the principles of weight vector quantiza-\ntion to a row-wise quantization scheme of a weight\nmatrix (i.e.for each row, q of scaling factors are\nproduced) in Eq. 1. In this case, g should be set\nto hin as described in Table 1. If we assign the g\nto be h(i.e., a hidden size of a model), each row\nwill be divided into hin/hof vectors, and each di-\nvided vector will produce its own scaling factors.\nAlthough we did not explain implementation issues\nof such a group-wise quantization, it has been also\nshown that group-wise quantization has minimal\nimpact on inference latency (Park et al., 2022).\n3303\nModel Method q Loss Unseen Seen All\nGPT\nM\nPTQ(WFT)\n2 5.57 4.9Â±.8 13.9Â±2.1 10.1Â±1.7\n3 2.03 25.0Â±2.5 58.7Â±1.0 43.2Â±3.3\n4 1.70 30.6Â±.9 63.3Â±.3 47.8Â±.7\nPTQ(W)+WLoRA\n3 2.98 15.8Â±3.0 15.8Â±3.4 15.8Â±3.2\n4 2.72 19.8Â±2.4 23.9Â±3.7 22.2Â±3.1\nPTQ(W+WLoRA) 3 3.36 12.6Â±4.1 16.6Â±6.7 13.6Â±7.5\n4 3.10 13.4Â±5.1 15.9Â±6.9 14.4Â±6.0\nGPT\nL\nPTQ(WFT)\n2 2.11 19.5Â±1.9 62.2Â±0.2 41.2Â±2.0\n3 1.91 23.2Â±.8 62.7Â±.2 43.7Â±.7\n4 1.87 23.7Â±.3 61.8Â±.4 43.6Â±.3\nPTQ(W)+WLoRA\n3 1.97 20.1Â±5.2 27.8Â±4.1 24.1Â±4.5\n4 1.67 33.9Â±4.4 45.9Â±3.7 41.7Â±2.1\nPTQ(W+WLoRA) 3 1.97 14.0Â±7.2 26.6Â±11.5 25.8Â±13.0\n4 1.67 28.7Â±7.1 34.3Â±13.1 29.9Â±12.1\nTable 13: BLEU scores on WebNLG dataset with post-training quantization. The fine-tuned models (with WFT) and\nLoRA-tuned models (with frozen W and WLoRA) are quantized by Alternating method (Xu et al., 2018) without\ngradient updates (post-training quantization).\nModel Method q loss BLEU NIST METEOR ROUGE_L CIDEr\nGPT\nM\nPTQ(WFT)\n2 2.687 50.2Â±2.2 5.78Â±1.2 35.8Â±1.5 60.1Â±.9 1.42Â±.11\n3 1.192 67.5Â±.5 8.58Â±.04 46.3Â±.5 70.3Â±.1 2.39Â±.01\n4 0.992 67.2Â±.6 8.52Â±.07 46.4Â±.3 70.3Â±.4 2.38Â±.03\nPTQ(W)+WLoRA\n3 4.095 11.1Â±5.3 2.35Â±1.04 12.4Â±4.2 29.9Â±7.8 0.35Â±.1\n4 1.916 48.3Â±3.8 3.65Â±1.16 32.3Â±.97 59.8Â±1.8 1.22Â±.14\nPTQ(W+WLoRA) 3 4.377 7.5Â±3.2 1.31Â±.21 11.4Â±.8 29.8Â±3.0 0.29Â±.04\n4 3.561 14.6Â±4.8 1.41Â±1.1 15.7Â±1.2 38.7Â±3.2 0.43Â±.03\nGPT\nL\nPTQ(WFT)\n2 0.998 66.1Â±1.0 8.40Â±.11 45.5Â±.4 70.0Â±.4 2.33Â±.03\n3 0.979 66.5Â±1.0 8.45Â±.10 45.7Â±.3 70.3Â±.5 2.37Â±.03\n4 0.976 66.6Â±1.0 8.47Â±.09 45.8Â±.2 70.3Â±.4 2.37Â±.01\nPTQ(W)+WLoRA\n3 1.868 50.9Â±3.4 6.63Â±.32 38.7Â±1.9 60.6Â±1.9 1.30Â±.19\n4 1.398 65.4Â±2.0 8.46Â±0.19 44.9Â±.6 65.3Â±2.2 2.15Â±.11\nPTQ(W+WLoRA) 3 4.377 7.5Â±3.2 1.31Â±.21 11.4Â±.8 29.8Â±3.0 0.29Â±.04\n4 3.561 14.6Â±4.8 1.41Â±1.1 15.7Â±1.2 38.7Â±3.2 0.43Â±.03\nTable 14: Test scores on E2E dataset after post-training quantization (q= 3) performed by Alternating method.\nMethod q Trainable\nParams\nGLUE\nCoLA SST-2 MRPC STS-B QQP MNLI 1 MNLImm\n2 QNLI RTE\nB FT - 108.3M 52.1 93.5 88.9 85.8 71.2 84.6 83.4 90.5 66.4\nAT 3 0.1M 51.0 91.4 91.4 87.4 84.2 80.8 81.1 89.4 69.3\nL\nFT - 333.6M 60.5 94.9 89.3 86.5 72.1 86.7 85.9 92.7 70.1\nAT 2 0.3M 49.1 90.9 88.8 87.0 84.9 82.7 83.5 89.9 66.8\nAT 3 0.3M 55.7 92.3 88.9 87.9 85.6 83.8 84.7 91.4 63.2\n1 MNLI-matched\n2 MNLI-mismatched\nTable 15: BERT-base-cased (B) and BERT-large-cased (L) with full fine-tuning (FT) and AlphaTuning (AT).\nExperiments were evaluated on the GLUE benchmark (Wang et al., 2018). The fine-tuning results used for\ncomparison refer to (Devlin et al., 2019). AlphaTuning follows the accuracy of full fine tuning. The results show\nthat AlphaTuning can be applied to the BERT architecture. Experimental details are in Table 16.\n3304\nModel Configuration GLUE\nCoLA SST-2 MRPC STS-B QQP MNLI 1 MNLImm\n2 QNLI RTE\nBase Batch size 16 32 32 32 32 16 16 16 16\nLearning rate 1e-4 1e-4 1e-4 2e-4 1e-4 5e-5 5e-5 5e-5 1e-4\nLarge\nBatch size 32 16 16 16 32 16 16 16 16\nLearning rate 1e-4 1e-4 1e-4 1e-4 5e-5 5e-5 5e-5 5e-5 1e-4\n1 MNLI-matched\n2 MNLI-mismatched\nTable 16: Hyper-parameter selection for the experiments using BERT-base and BERT-large on GLUE benchmark.\nFor each experiment, the optimizer is selected to be AdamW with a linear decaying learning rate scheduler. The\nnumber of epochs is fixed to be 3, and the weight decay is set to 0.01. The metrics used in the evaluation are\nMatthewâ€™s correlation for CoLA, Pearson correlation for STS-B, F1 score for QQP and MRPC and accuracy for the\nother tasks.\n3305",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5429115295410156
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.5031556487083435
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.46728190779685974
    },
    {
      "name": "Language model",
      "score": 0.461883008480072
    },
    {
      "name": "Scale (ratio)",
      "score": 0.41224777698516846
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38481712341308594
    },
    {
      "name": "Natural language processing",
      "score": 0.37880799174308777
    },
    {
      "name": "Speech recognition",
      "score": 0.3715039789676666
    },
    {
      "name": "Psychology",
      "score": 0.2463369369506836
    },
    {
      "name": "Algorithm",
      "score": 0.23024043440818787
    },
    {
      "name": "Physics",
      "score": 0.1641732156276703
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0833888053894043
    },
    {
      "name": "Neuroscience",
      "score": 0.07902279496192932
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I60922564",
      "name": "Naver (South Korea)",
      "country": "KR"
    },
    {
      "id": "https://openalex.org/I4210099236",
      "name": "Kootenay Association for Science & Technology",
      "country": "CA"
    },
    {
      "id": "https://openalex.org/I157485424",
      "name": "Korea Advanced Institute of Science and Technology",
      "country": "KR"
    }
  ]
}