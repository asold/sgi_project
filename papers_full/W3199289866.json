{
    "title": "Distantly-Supervised Named Entity Recognition with Noise-Robust Learning and Language Model Augmented Self-Training",
    "url": "https://openalex.org/W3199289866",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A1976615132",
            "name": "Yu Meng",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A2124706810",
            "name": "Yun-Yi Zhang",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A2119125168",
            "name": "JiaXin Huang",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A2096156584",
            "name": "Xuan Wang",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A2023856972",
            "name": "Yu Zhang",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A2127420617",
            "name": "Heng Ji",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A2103606203",
            "name": "Jiawei Han",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3106031848",
        "https://openalex.org/W3106109117",
        "https://openalex.org/W2015917093",
        "https://openalex.org/W3034843119",
        "https://openalex.org/W2080133951",
        "https://openalex.org/W3035542229",
        "https://openalex.org/W4300996741",
        "https://openalex.org/W2734693922",
        "https://openalex.org/W2963759070",
        "https://openalex.org/W3125371518",
        "https://openalex.org/W2964074409",
        "https://openalex.org/W4301488752",
        "https://openalex.org/W2995624272",
        "https://openalex.org/W2803187616",
        "https://openalex.org/W2963625095",
        "https://openalex.org/W2011941828",
        "https://openalex.org/W2951970475",
        "https://openalex.org/W2891383691",
        "https://openalex.org/W2530816535",
        "https://openalex.org/W2962902328",
        "https://openalex.org/W3103981637",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W2890931111",
        "https://openalex.org/W2963413667",
        "https://openalex.org/W1940872118",
        "https://openalex.org/W3137298232",
        "https://openalex.org/W2951994466",
        "https://openalex.org/W2963216553",
        "https://openalex.org/W3131870090",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2963697299",
        "https://openalex.org/W2296283641",
        "https://openalex.org/W2889191148",
        "https://openalex.org/W4288095202",
        "https://openalex.org/W2963391789",
        "https://openalex.org/W2962369866",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2020978905",
        "https://openalex.org/W4320013820",
        "https://openalex.org/W2903908313",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2945878859",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W2148540243",
        "https://openalex.org/W2953010755",
        "https://openalex.org/W3115908473",
        "https://openalex.org/W3035097673",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3212769596",
        "https://openalex.org/W2997918300",
        "https://openalex.org/W2808481912"
    ],
    "abstract": "We study the problem of training named entity recognition (NER) models using only distantly-labeled data, which can be automatically obtained by matching entity mentions in the raw text with entity types in a knowledge base. The biggest challenge of distantly-supervised NER is that the distant supervision may induce incomplete and noisy labels, rendering the straightforward application of supervised learning ineffective. In this paper, we propose (1) a noise-robust learning scheme comprised of a new loss function and a noisy label removal step, for training NER models on distantly-labeled data, and (2) a self-training method that uses contextualized augmentations created by pre-trained language models to improve the generalization ability of the NER model. On three benchmark datasets, our method achieves superior performance, outperforming existing distantly-supervised NER models by significant margins.",
    "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10367–10378\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n10367\nDistantly-Supervised Named Entity Recognition with Noise-Robust\nLearning and Language Model Augmented Self-Training\nYu Meng, Yunyi Zhang, Jiaxin Huang, Xuan Wang,\nYu Zhang, Heng Ji, Jiawei Han\nUniversity of Illinois Urbana-Champaign, IL, USA\n{yumeng5, yzhan238, jiaxinh3, xwang174,\nyuz9, hengji, hanj}@illinois.edu\nAbstract\nWe study the problem of training named en-\ntity recognition (NER) models using only\ndistantly-labeled data, which can be automat-\nically obtained by matching entity mentions\nin the raw text with entity types in a knowl-\nedge base. The biggest challenge of distantly-\nsupervised NER is that the distant supervi-\nsion may induce incomplete and noisy labels,\nrendering the straightforward application of\nsupervised learning ineffective. In this pa-\nper, we propose (1) a noise-robust learning\nscheme comprised of a new loss function\nand a noisy label removal step, for training\nNER models on distantly-labeled data, and\n(2) a self-training method that uses contextual-\nized augmentations created by pre-trained lan-\nguage models to improve the generalization\nability of the NER model. On three bench-\nmark datasets, our method achieves superior\nperformance, outperforming existing distantly-\nsupervised NER models by signiﬁcant mar-\ngins1.\n1 Introduction\nNamed entity recognition (NER), which aims at\nidentifying real-world entity mentions ( e.g., per-\nson, location and organization names) from texts,\nis a fundamental task in natural language process-\ning with a wide range of applications, including\nquestion answering (Khalid et al., 2008), knowl-\nedge base construction (Etzioni et al., 2005), text\nsummarization (Aramaki et al., 2009) and dialog\nsystems (Bowden et al., 2018). In recent years,\ndeep neural models (Devlin et al., 2019; Huang\net al., 2015; Lample et al., 2016; Ma and Hovy,\n2016) have achieved enormous success for NER,\nthanks to their strong representation learning power\nthat accurately captures the entity semantics in tex-\ntual contexts. However, a common bottleneck of\napplying deep learning models is the acquisition\n1Code can be found at https://github.com/\nyumeng5/RoSTER.\nLOC\nPER\nORG\nPER\nDistantly-Labeled\nGround Truth\nPER PER\nMiguel Angel Jimenez is a professional golfer.\nCoopers and Lybrand emigrates to Basque Country for ﬁscal reasons.\nMiguel Angel Jimenez is a professional golfer.\nCoopers and Lybrand emigrates to Basque Country for ﬁscal reasons.\nFigure 1: Distant labels obtained with knowledge bases\nmay be incomplete and noisy, resulting in wrongly-\nlabeled tokens.\nof abundant high-quality human annotations, and\nthis is especially the case for training NER models,\nwhich require every entity mention to be labeled in\na sequence.\nTo eliminate the need for human annotations,\none direction is to use distant supervision for au-\ntomatic generation of entity labels. The common\npractice is to match entity mentions in the target\ncorpus with typed entities in external gazetteers\nor knowledge bases. Unfortunately, such a distant\nlabeling process inevitably introduces incomplete\nand noisy entity labels, because (1) the distant su-\npervision source has limited coverage of the entity\nmentions in the target corpus, and (2) some entities\ncan be matched to multiple types in the knowledge\nbases—such ambiguity cannot be resolved by the\ncontext-free matching process. Figure 1 shows that\nsome “person” mentions may be partially labeled\n(or not labeled at all in other cases), and some\nentities with multiple possible types may be misla-\nbeled.\nDue to the existence of such noise, straightfor-\nward application of supervised learning to distantly-\nlabeled data will yield deteriorated performance,\nbecause neural models have the strong capacity to\nﬁt to the given (noisy) data. Some previous studies\non distantly-supervised NER directly treat distant\nlabels as if ground truth for model training and\nrely on simple tricks such as applying early stop-\nping (Liang et al., 2020) and labeling entities with\n10368\nmultiple types (Shang et al., 2018) to handle the\nnoise. Others require an additional manually la-\nbeled training set for building a noise classiﬁcation\nmodel (Onoe and Durrett, 2019).\nIn this paper, we study the distantly-supervised\nNER problem without requiring any human annota-\ntions. Our method consists of two steps: (1) noise-\nrobust learning, and (2) language model augmented\nself-training. In the ﬁrst step, we explicitly address\nthe label noise by using a noise-robust loss func-\ntion and removing noisy labels. In the second step,\nwe use the model’s high-conﬁdence predictions for\nself-training to improve generalization, wherein\na pre-trained language model is used to not only\ninitialize the NER model, but also generate con-\ntextualized augmentations. Our method is named\nRoSTER, for Robust learning and Self-Training\nfor distantly-supervised Entity Recognition.\nThe contributions of this paper are as follows:\n• We propose a noise-robust learning scheme for\ndistantly-supervised NER, comprised of a noise-\nrobust loss function and a noisy label removal\nstep.\n• We propose a new unsupervised contextual-\nized augmentation approach for NER using pre-\ntrained language models. Combined with self-\ntraining, the created augmentations improve the\nmodel’s generalization ability.\n• On three benchmark datasets, RoSTER out-\nperforms existing distantly-supervised NER ap-\nproaches by signiﬁcant margins.\n2 Method\nIn this section, we (1) brieﬂy describe how to ob-\ntain distantly-labeled data, (2) introduce our noise-\nrobust learning scheme and (3) propose a self-\ntraining method with a new contextualized aug-\nmentation generation technique. We assume the\npre-trained RoBERTa (Liu et al., 2019) model is\nused as our backbone model, but our proposed\nmethods can be integrated with other architectures\n(e.g., LSTM-based (Ma and Hovy, 2016)) as well.\n2.1 Distant Label Generation\nGiven an unlabeled corpus, the distant labels are\nusually obtained by matching entities in the cor-\npus with those in the external knowledge bases or\ngazetteers with typing information. In this work,\ninstead of introducing new distant label generation\nmethods, we follow the previous work (Liang et al.,\n2020) for this step: (1) potential entities are de-\ntermined via POS tagging and hand-crafted rules,\n(2) their types are acquired by querying Wikidata\nusing SPARQL (Vrandeˇci´c and Krötzsch, 2014),\nand (3) additional gazetteers from multiple online\nresources are used for matching more entities in\nthe corpus.\n2.2 Noise-Robust Learning\nWe ﬁrst overview the common setup for training\nNER models, and then propose two designs that\nwork jointly for distantly-supervised NER, moti-\nvated by the challenges of learning with noisy la-\nbels: (1) a new loss function, and (2) noisy label\nremoval. Finally, ensembling multiple models is\nhelpful for stabilizing the model performance.\nNER systems are usually trained as a sequence\nlabeling model that classiﬁes every token in\na sequence into a set of entity types or non-\nentity. The label space depends on the tagging\nscheme used ( e.g., the BIO format distinguishes\nbegin/inside/outside of named entities). Specif-\nically, given a sequence x = [ x1,...,x n] of n\ntokens and their corresponding categorical labels\ny= [y1,...,y n], an NER model parameterized by\nθis trained to minimize some classiﬁcation loss\nthat encourages the model to correctly predict the\ngiven labels. The cross entropy (CE) loss is most\ncommonly used for such a purpose:\nLCE = −\nn∑\ni=1\nlog fi,yi (x; θ),\nwhere fi,j(x; θ) is the model’s predicted probabil-\nity of tokenxi belonging to classj(i.e., the softmax\nlayer outputs).\nThe gradient of LCE is (via the chain rule):\n∇θLCE = −\nn∑\ni=1\n∇θfi,yi (x; θ)\nfi,yi (x; θ) . (1)\nDue to the fi,yi (x; θ) term as the denominator,\nthe tokens on which the model’s prediction is less\ncongruent with the provided labels (i.e., fi,yi (x; θ)\nis smaller) will be implicitly weighed more dur-\ning the gradient update. Such a mechanism grants\nbetter model convergence when trained with clean\ndata (i.e., yare ground truth labels), because more\nemphasis is put on difﬁcult tokens. However, when\nthe labels are noisy, training with the cross entropy\nloss can cause overﬁtting to the wrongly-labeled\ntokens (e.g., the two sentences in Figure 1).\n10369\nContrary to cross entropy loss which is sensi-\ntive to noise, the mean absolute error (MAE) loss,\nwhich is commonly used in regression tasks, has\nbeen shown inherently noise-tolerant when used\nfor classiﬁcation (Ghosh et al., 2017) and is deﬁned\nas follows (omitting the constant scale factor 2):\nLMAE =\nn∑\ni=1\n(1 −fi,yi (x; θ)) ,\nand its gradient is given by\n∇θLMAE = −\nn∑\ni=1\n∇θfi,yi (x; θ). (2)\nBy comparing Eq. (2) with Eq. (1), we observe\nthat LMAE is more noise-robust than LCE because\nEq. (2) treats every token equally for gradient up-\ndate, allowing the learning process to be dominated\nby the correct majority in distant labels. However,\nusing LMAE for training deep neural models gener-\nally worsens the convergence efﬁciency and effec-\ntiveness due to the inability of adjusting for chal-\nlenging training samples, and leads to suboptimal\nmodel performance compared to using LCE (Zhang\nand Sabuncu, 2018).\nGeneralized Cross Entropy. To balance be-\ntween model convergence and noise-robustness,\nwe propose to use the generalized cross entropy\n(GCE) loss (Zhang and Sabuncu, 2018) for training\ndistantly-supervised NER models, inspired by the\nq-order entropy (Ferrari et al., 2010), deﬁned as\nfollows:\nLGCE =\nn∑\ni=1\n1 −fi,yi (x; θ)q\nq , (3)\nwhere 0 < q < 1 is a hyperparameter: When\nq →1, LGCE approximates LMAE; when q →0,\nLGCE approximates LCE (using L’Hôpital’s rule;\nsee Appendix A for the derivation). The gradient\nis computed as:\n∇θLGCE = −\nn∑\ni=1\n∇θfi,yi (x; θ)\nfi,yi (x; θ)1−q . (4)\nComparing Eq. (4) to Eq. (1), it can be observed\nthat LGCE is more noise-robust than LCE because\nless weights are given to tokens on which the model\nprediction is less consistent with the given labels\n(note fi,yi (x; θ)1−q > fi,yi (x; θ) for q >0 and\nfi,yi (x; θ) <1). Comparing Eq. (4) to Eq. (2), it\ncan be seen that LGCE facilitates better learning\ndynamics than LMAE because difﬁcult tokens are\ngiven more attention to.\nNoisy Label Removal. Even when a noise-\nrobust loss is used, mislabeled tokens still dete-\nriorate the model performance as long as they are\nincluded in training. Unfortunately, without any\nprior knowledge about which tokens are mislabeled,\nit is challenging to automatically detect them. We\npropose a simple threshold-based strategy to re-\nmove noisy labels: At ﬁrst, all tokens along with\ntheir distant labels will be used for model training;\nlater, those tokens on which the model prediction\ndoes not strongly agree with its distant label (i.e.,\nfi,yi (x; θ) ≤τ where τ is a threshold value) will\nbe excluded from the training set ( i.e., not calcu-\nlated in the loss). The intuition is straightforward:\nSince our loss function is noise-robust, the learned\nmodel will be dominated by the correct majority in\nthe distant labels instead of quickly overﬁtting to\nlabel noise; if the model prediction disagrees with\nsome given labels, they are potentially wrong.\nSpeciﬁcally, we extend Eq.(3) to incorporate the\naforementioned design, as follows:\nLGCE =\nn∑\ni=1\nwi\n1 −fi,yi (x; θ)q\nq , (5)\nwhere wi = 1 at the start of training and is pe-\nriodically updated once every several batches as\nwi = 1 (fi,yi (x; θ) >τ ), where 1 (·) is the indica-\ntor function.\nModel Ensemble for Better Stability. Due to\nthe stochasticity involved in training neural net-\nworks (e.g., dataset shufﬂing, network random ini-\ntialization and dropout), models trained with the\nsame algorithm will have different predictions on\nthe same dataset, and this is especially true when\ninconsistent noisy signals from the distant labels\nmay further disturb model training. As such, model\nensemble is commonly used to suppress the noise\nand provide better stability, by combining the pre-\ndictions of multiple models (Laine and Aila, 2017;\nNguyen et al., 2019). The rationale is that the\nmodel’s predictions are likely to be consistent on\nclean data while inconsistent and oscillating on\nwrongly-labeled data, and ensembling multiple\nmodels enhances consistent predictions and neu-\ntralizes inconsistent ones.\nWe perform model ensemble by simply train-\ning K models {θk}K\nk=1 via Eq. (5) on the same\ndistantly-labeled corpus with different random\nseeds controlling the randomness of the training\nprocess. A new model θENS is ﬁnally trained to ap-\nproximate the average prediction of the Kmodels\n10370\non all tokens by minimizing the Kullback–Leibler\n(KL) divergence loss:\nLENS =\nn∑\ni=1\nKL\n(¯fi\n(\nx; {θk}K\nk=1\n)\n∥fi(x; θENS)\n)\n,\n(6)\nwhere ¯fi\n(\nx; {θk}K\nk=1\n)\n= 1\nK\n∑K\nk=1 fi(x; θk) is\nthe K models’ averaged prediction, and we ﬁnd\nthat K = 5 is sufﬁcient to provide stable ensem-\nbled model performance.\nRemarks. While our methods introduce two ad-\nditional hyperparameters qand τ, their values can\nbe kept same for different datasets to avoid param-\neter tuning. We will also show in Section 3 that\nthe model performance is rather insensitive to these\nhyperparameter values within a reasonable range.\n2.3 Language Model Augmented\nSelf-Training\nAfter the noise-robust learning step, we further ﬁne-\ntune the resulting model (i.e., trained with Eq. (6))\nvia a self-training step on the same corpus, but\nwithout the distant labels, for two reasons: (1) The\nclean signals in the distant supervision have been\nexploited via noise-robust learning, but some to-\nkens may have not been fully leveraged by the\nmodel since they are excluded by the noisy label re-\nmoval step. The self-training step aims to bootstrap\non all tokens using the model’s own predictions\nto improve its generalization ability. Similar self-\ntraining ideas have been explored in classiﬁcation\ntasks (Meng et al., 2018, 2019, 2020). (2) The pre-\ntrained language model (PLM) has only been used\nto initialize the NER model for ﬁne-tuning, while\nPLMs (without ﬁne-tuning) encode factual and re-\nlational knowledge through pre-training (Petroni\net al., 2019) that may complement the NER model\ntraining. The self-training step thus also brings\nadditional pre-trained knowledge for better model\ngeneralization by creating contextualized augmen-\ntations using a PLM. Figure 2 shows an overview.\nContextualized Augmentations with PLMs.\nMany PLMs (Devlin et al., 2019; Lan et al., 2020;\nLiu et al., 2019) are pre-trained with the masked\nlanguage modeling (MLM) task on large-scale\ntext corpora carrying general knowledge like the\nWikipedia. Previous studies (Jiang et al., 2020;\nPetroni et al., 2019) have shown that entity-related\nknowledge can be extracted from a PLM (without\nany ﬁne-tuning) by querying it via cloze templates\nand gathering the PLM’s MLM outputs.\nGiven that the MLM task shares high similar-\nity with the NER task ( i.e., both leverage the\ncontextual information within the sequence for\ntoken-level classiﬁcation) and that the MLM out-\nputs contain general knowledge acquired during\npre-training, we propose to use the pre-trained\nRoBERTa model (without ﬁne-tuning) θPRE for\ncreating label-preserving augmentations (i.e., not\nchanging the entity type label or non-entity label)\nof the original sequences in the corpus in order\nto complement the NER learning with pre-trained\nknowledge. Speciﬁcally, for each sequencexin the\ncorpus, we randomly mask out 15% of its tokens\n(i.e., replace them with the [MASK] token), and\nfeed the partially masked sequence ˆxinto the pre-\ntrained RoBERTa model. Finally, the augmented\nsequence x′= [x′\n1,...,x ′\nn] is created by sampling\nfrom the MLM output probability for each token:\nx′\ni ∼pMLM\ni (ˆx; θPRE) , (7)\nwhere pMLM\ni (ˆx; θPRE) is the MLM probability of\nthe pre-trained RoBERTa model on theith token.\nThe augmented sequence x′ will be semanti-\ncally similar to the original sequence x, and the\nreplaced tokens in x′that are different from those\nin x are likely to be label-preserving because\nPLMs are good at predicting missing words in the\ngiven context, which are usually interchangeable\nto the original ones. To further enforce the label-\npreserving constraint of the augmented sequence,\nwe (1) samplex′\ni only from the top-5 terms given by\npMLM\ni (ˆx; θPRE) to avoid low-quality replacements,\nand (2) require x′\ni to have the same capitalization\nand tokenization with xi (i.e., if xi is capitalized or\nis a subword, so should x′\ni).\nUsing PLMs to perform augmentation for NER\nhas the major beneﬁt of being unsupervised and\ncontextualized. Without PLMs, one may still per-\nform augmentation by replacing an entity in the se-\nquence with another of the same type in the distant\nsupervision source (Dai and Adel, 2020). However,\nsuch an approach requires prior knowledge about\nthe entity type in the sequence (i.e., it does not work\nfor non-entities or entities not matched with distant\nlabels), and the augmentation is context-free, which\nmay create low-quality and invalid sequences (e.g.,\nit does not ﬁt the context to replace a technology\ncompany with a news agency although they both\nbelong to the “organization” entity type).\nSelf-Training. The goals of self-training (ST)\nare two-fold: (1) use the model’s high-conﬁdence\n10371\ndefeated\nsamplingsampling samplingsamplingsampling\nMLM\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\nPre-Trained RoBERTa Model\n(not ﬁne-tuned)\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\nTodd \n<latexit sha1_base64=\"+nXc35ReozH1i536miS7zHWDu+g=\">AAAB+HicbVBNS8NAEN3Ur1o/GvXoZbEInkoiih4rXgQRKtoPSEPZbDft0s0m7E7EGvpLvHhQxKs/xZv/xm2bg7Y+GHi8N8PMvCARXIPjfFuFpeWV1bXiemljc2u7bO/sNnWcKsoaNBaxagdEM8ElawAHwdqJYiQKBGsFw8uJ33pgSvNY3sMoYX5E+pKHnBIwUtcud4A9AkDm3VzcXfvjrl1xqs4UeJG4OamgHPWu/dXpxTSNmAQqiNae6yTgZ0QBp4KNS51Us4TQIekzz1BJIqb9bHr4GB8apYfDWJmSgKfq74mMRFqPosB0RgQGet6biP95XgrhuZ9xmaTAJJ0tClOBIcaTFHCPK0ZBjAwhVHFzK6YDoggFk1XJhODOv7xImsdV97Tq3J5Uaid5HEW0jw7QEXLRGaqhK1RHDURRip7RK3qznqwX6936mLUWrHxmD/2B9fkDxQ2THA==</latexit>\n[MASK]\n<latexit sha1_base64=\"+nXc35ReozH1i536miS7zHWDu+g=\">AAAB+HicbVBNS8NAEN3Ur1o/GvXoZbEInkoiih4rXgQRKtoPSEPZbDft0s0m7E7EGvpLvHhQxKs/xZv/xm2bg7Y+GHi8N8PMvCARXIPjfFuFpeWV1bXiemljc2u7bO/sNnWcKsoaNBaxagdEM8ElawAHwdqJYiQKBGsFw8uJ33pgSvNY3sMoYX5E+pKHnBIwUtcud4A9AkDm3VzcXfvjrl1xqs4UeJG4OamgHPWu/dXpxTSNmAQqiNae6yTgZ0QBp4KNS51Us4TQIekzz1BJIqb9bHr4GB8apYfDWJmSgKfq74mMRFqPosB0RgQGet6biP95XgrhuZ9xmaTAJJ0tClOBIcaTFHCPK0ZBjAwhVHFzK6YDoggFk1XJhODOv7xImsdV97Tq3J5Uaid5HEW0jw7QEXLRGaqhK1RHDURRip7RK3qznqwX6936mLUWrHxmD/2B9fkDxQ2THA==</latexit>\n[MASK]\nTodd Frazier\nwas by\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n15% mask\nTodd Martineliminatedwas by ···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\nOriginal Sequence \n<latexit sha1_base64=\"BQxUj1BI0MhXpYrLPDxmYapDMPU=\">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIosuCG5cV7APasWQymTY0kwxJRi1D/8ONC0Xc+i/u/Bsz7Sy09UDI4Zx7yckJEs60cd1vp7Syura+Ud6sbG3v7O5V9w/aWqaK0BaRXKpugDXlTNCWYYbTbqIojgNOO8H4Ovc7D1RpJsWdmSTUj/FQsIgRbKx03w8kD/Uktlf2NB1Ua27dnQEtE68gNSjQHFS/+qEkaUyFIRxr3fPcxPgZVoYRTqeVfqppgskYD2nPUoFjqv1slnqKTqwSokgqe4RBM/X3RoZjnUezkzE2I73o5eJ/Xi810ZWfMZGkhgoyfyhKOTIS5RWgkClKDJ9YgoliNisiI6wwMbaoii3BW/zyMmmf1b2Lunt7XmucF3WU4QiO4RQ8uIQG3EATWkBAwTO8wpvz6Lw4787HfLTkFDuH8AfO5w9QaJL/</latexit>\nx\n<latexit sha1_base64=\"BQxUj1BI0MhXpYrLPDxmYapDMPU=\">AAAB9XicbVDLSgMxFL1TX7W+qi7dBIvgqsyIosuCG5cV7APasWQymTY0kwxJRi1D/8ONC0Xc+i/u/Bsz7Sy09UDI4Zx7yckJEs60cd1vp7Syura+Ud6sbG3v7O5V9w/aWqaK0BaRXKpugDXlTNCWYYbTbqIojgNOO8H4Ovc7D1RpJsWdmSTUj/FQsIgRbKx03w8kD/Uktlf2NB1Ua27dnQEtE68gNSjQHFS/+qEkaUyFIRxr3fPcxPgZVoYRTqeVfqppgskYD2nPUoFjqv1slnqKTqwSokgqe4RBM/X3RoZjnUezkzE2I73o5eJ/Xi810ZWfMZGkhgoyfyhKOTIS5RWgkClKDJ9YgoliNisiI6wwMbaoii3BW/zyMmmf1b2Lunt7XmucF3WU4QiO4RQ8uIQG3EATWkBAwTO8wpvz6Lw4787HfLTkFDuH8AfO5w9QaJL/</latexit>\nx   \nbydefeatedwas\nNER\nPre-Trained RoBERTa Model\n(ﬁne-tuned)\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\nwas by Todd Frazier\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\nwas eliminated by Todd Martin\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\nAugmented Sequence \n<latexit sha1_base64=\"8G0nGLSTs3/pcwh69xFGY55W9fQ=\">AAAB+HicbVC7TsMwFL0pr1IeDTCyWFQIpipBRTBWYmEsEn1IbVQ5jtNadZzIdhAl6pewMIAQK5/Cxt/gtBmg5UiWj865Vz4+fsKZ0o7zbZXW1jc2t8rblZ3dvf2qfXDYUXEqCW2TmMey52NFORO0rZnmtJdIiiOf064/ucn97gOVisXiXk8T6kV4JFjICNZGGtrVgR/zQE0jc2WPs7OhXXPqzhxolbgFqUGB1tD+GgQxSSMqNOFYqb7rJNrLsNSMcDqrDFJFE0wmeET7hgocUeVl8+AzdGqUAIWxNEdoNFd/b2Q4Unk2MxlhPVbLXi7+5/VTHV57GRNJqqkgi4fClCMdo7wFFDBJieZTQzCRzGRFZIwlJtp0VTEluMtfXiWdi7p7WXfuGrVmo6ijDMdwAufgwhU04RZa0AYCKTzDK7xZT9aL9W59LEZLVrFzBH9gff4ALkmTYQ==</latexit>\nx\n0\n<latexit sha1_base64=\"8G0nGLSTs3/pcwh69xFGY55W9fQ=\">AAAB+HicbVC7TsMwFL0pr1IeDTCyWFQIpipBRTBWYmEsEn1IbVQ5jtNadZzIdhAl6pewMIAQK5/Cxt/gtBmg5UiWj865Vz4+fsKZ0o7zbZXW1jc2t8rblZ3dvf2qfXDYUXEqCW2TmMey52NFORO0rZnmtJdIiiOf064/ucn97gOVisXiXk8T6kV4JFjICNZGGtrVgR/zQE0jc2WPs7OhXXPqzhxolbgFqUGB1tD+GgQxSSMqNOFYqb7rJNrLsNSMcDqrDFJFE0wmeET7hgocUeVl8+AzdGqUAIWxNEdoNFd/b2Q4Unk2MxlhPVbLXi7+5/VTHV57GRNJqqkgi4fClCMdo7wFFDBJieZTQzCRzGRFZIwlJtp0VTEluMtfXiWdi7p7WXfuGrVmo6ijDMdwAufgwhU04RZa0AYCKTzDK7xZT9aL9W59LEZLVrFzBH9gff4ALkmTYQ==</latexit>\nx\n0\n \n<latexit sha1_base64=\"+nXc35ReozH1i536miS7zHWDu+g=\">AAAB+HicbVBNS8NAEN3Ur1o/GvXoZbEInkoiih4rXgQRKtoPSEPZbDft0s0m7E7EGvpLvHhQxKs/xZv/xm2bg7Y+GHi8N8PMvCARXIPjfFuFpeWV1bXiemljc2u7bO/sNnWcKsoaNBaxagdEM8ElawAHwdqJYiQKBGsFw8uJ33pgSvNY3sMoYX5E+pKHnBIwUtcud4A9AkDm3VzcXfvjrl1xqs4UeJG4OamgHPWu/dXpxTSNmAQqiNae6yTgZ0QBp4KNS51Us4TQIekzz1BJIqb9bHr4GB8apYfDWJmSgKfq74mMRFqPosB0RgQGet6biP95XgrhuZ9xmaTAJJ0tClOBIcaTFHCPK0ZBjAwhVHFzK6YDoggFk1XJhODOv7xImsdV97Tq3J5Uaid5HEW0jw7QEXLRGaqhK1RHDURRip7RK3qznqwX6936mLUWrHxmD/2B9fkDxQ2THA==</latexit>\n[MASK]\n<latexit sha1_base64=\"+nXc35ReozH1i536miS7zHWDu+g=\">AAAB+HicbVBNS8NAEN3Ur1o/GvXoZbEInkoiih4rXgQRKtoPSEPZbDft0s0m7E7EGvpLvHhQxKs/xZv/xm2bg7Y+GHi8N8PMvCARXIPjfFuFpeWV1bXiemljc2u7bO/sNnWcKsoaNBaxagdEM8ElawAHwdqJYiQKBGsFw8uJ33pgSvNY3sMoYX5E+pKHnBIwUtcud4A9AkDm3VzcXfvjrl1xqs4UeJG4OamgHPWu/dXpxTSNmAQqiNae6yTgZ0QBp4KNS51Us4TQIekzz1BJIqb9bHr4GB8apYfDWJmSgKfq74mMRFqPosB0RgQGet6biP95XgrhuZ9xmaTAJJ0tClOBIcaTFHCPK0ZBjAwhVHFzK6YDoggFk1XJhODOv7xImsdV97Tq3J5Uaid5HEW0jw7QEXLRGaqhK1RHDURRip7RK3qznqwX6936mLUWrHxmD/2B9fkDxQ2THA==</latexit>\n[MASK]\nPER PEROOO\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\n···\n<latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit><latexit sha1_base64=\"AcPHc2/GmseUvYw39tetylSl3A8=\">AAAB7XicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDabTbt2kw27E6GE/gcvHhTx6v/x5r9x2+agrQ8GHu/NMDMvSKUw6LrfTmltfWNzq7xd2dnd2z+oHh61jco04y2mpNLdgBouRcJbKFDybqo5jQPJO8H4duZ3nrg2QiUPOEm5H9NhIiLBKFqp3WehQjOo1ty6OwdZJV5BalCgOah+9UPFspgnyCQ1pue5Kfo51SiY5NNKPzM8pWxMh7xnaUJjbvx8fu2UnFklJJHSthIkc/X3RE5jYyZxYDtjiiOz7M3E/7xehtG1n4skzZAnbLEoyiRBRWavk1BozlBOLKFMC3srYSOqKUMbUMWG4C2/vEraF3XPJnN/WWvcFHGU4QRO4Rw8uIIG3EETWsDgEZ7hFd4c5bw4787HorXkFDPH8AfO5w+uBY8u</latexit>\nSoft LabelsSoft Labels\nFigure 2: Overview of language model augmented self-training. Only a part of the sequence is shown; the original\nsequence is “Renzo Furlan was eliminated by Todd Martin in the tournament.” We feed the partially masked\noriginal sequence into a pre-trained RoBERTa model and sample from its MLM output probability to obtain an\naugmented sequence (replaced tokens are marked in blue). Then the NER model is trained with both original and\naugmented sequences as inputs to approximate the soft labels.\npredictions that are likely to be reliable for guiding\nthe model reﬁnement on all tokens, and (2) encour-\nage the model to generate consistent predictions\non original sequences and augmented ones, based\non the principle that a generalizable model should\nproduce similar predictions for similar inputs. To\nfulﬁll these goals, we iteratively use the model’s\ncurrent predictions to derive soft labels and grad-\nually update the model so that its predictions on\nboth the original and the augmented sequences ap-\nproximate the soft labels.\nSpeciﬁcally, at the beginning of self-training,\nthe model θ(0) is initialized to be the model trained\nwith Eq. (6). Then at each iterationt, we derive new\nsoft labels y(t+1) that enhance high-conﬁdence\npredictions while demote low-conﬁdence ones via\nsquaring and normalizing the current predictions on\nthe original sequence x, following the soft labeling\nformula by (Xie et al., 2016):\ny(t+1)\ni,j =\nfi,j\n(\nx; θ(t)\n)2\n/gj\n∑\nj′\n(\nfi,j\n(\nx; θ(t)\n)2\n/gj′\n),\ngj =\n∑\ni\nfi,j\n(\nx; θ(t)\n)\n.\n(8)\nThen the model θ(t+1) at the next iteration is\nupdated by approximating the soft labels with both\nthe original sequence and the augmented sequence\nas inputs, via the KL divergence loss:\nLST =\nn∑\ni=1\nKL\n(\ny(t+1)\ni\nfi\n(\nx; θ(t+1)\n))\n+\nn∑\ni=1\nKL\n(\ny(t+1)\ni\nfi\n(\nx′; θ(t+1)\n))\n.\n(9)\nUsing Eq. (9) to train the model not only guides\nthe model learning with its high-conﬁdence predic-\ntions, but also encourages consistent predictions be-\ntween the original and augmented sequences. From\nanother perspective, such a training process grad-\nually propagates conﬁdent label information from\noriginal examples to augmented ones so that the\nmodel is trained with more data for better general-\nization.\nWe note that the soft labels in Eq. (8) are com-\nputed on all entity types, excluding the non-entity\nclass (i.e., the “O” class). This is because the “O”\nclass usually has many more tokens than any en-\ntity type class, while Eq. (8) encourages balanced\nassignments of target soft labels.\n2.4 Overall Algorithm\nWe summarize the entire training procedure in Al-\ngorithm 1. Lines 2-9: We train K models with\ndifferent seeds on the distantly-labeled data using\nthe noise-robust loss. At the end of each iteration,\nnoisy labels are removed based on the model’s pre-\ndictions (Line 9). Line 10: An ensembled model\nis trained. Line 12: Contextualized augmentations\nare created with the pre-trained RoBERTa model.\nLines 16-18: The soft labels are iteratively com-\nputed, and the model is updated to approximate the\nsoft labels on both the original and the augmented\nsequences.\n3 Experiments\n3.1 Datasets\nWe use three benchmark datasets for NER:\nCoNLL03 (Sang and De Meulder, 2003),\nOntoNotes5.0 (Weischedel et al., 2013) which we\n10372\nAlgorithm 1: RoSTERtraining.\nInput: An unlabeled text corpus {x};\nexternal knowledge bases Φ;\npre-trained RoBERTa modelθPRE.\nOutput: A trained NER model θ.\n1 {y}← Distant label generation with Φ ;\n2 // Train Kmodels for ensemble;\n3 for k←0 to K−1 do\n4 θk ←θPRE;\n5 {wi}n\ni=1 ←1;\n6 // Train for M iterations;\n7 for m←1 to M do\n8 θk ←Train with Eq. (5);\n9 {wi ←1 (fi,yi (x; θk) >τ )}n\ni=1;\n10 θENS ←Train with Eq. (6);\n11 // Augmentation;\n12 {x′}← Eq. (7);\n13 // Self-training;\n14 θ(0) ←θENS;\n15 // Train for T iterations;\n16 for t←0 to T −1 do\n17 y(t+1) ←Eq. (8);\n18 θ(t+1) ←Train with Eq. (9);\n19 Return θ= θ(T);\nDataset # Types # Train # Test\nCoNLL03 4 14,041 3,453\nOntoNotes5.0 18 59,924 8,262\nWikigold 4 1,142 274\nTable 1: Dataset statistics with the number of entity\ntypes and the number of training/test sequences.\nfollow the pre-processing of (Chiu and Nichols,\n2016), and Wikigold (Balasuriya et al., 2009). The\ndataset statistics are shown in Table 1. All datasets\nare in English language.\n3.2 Compared Methods\nWe compare with a wide range of state-of-the-art\ndistantly-supervised methods and supervised meth-\nods. Fully supervised methods use the ground\ntruth training set for model training. Distantly-\nsupervised methods use the distantly-labeled train-\ning set obtained as in (Liang et al., 2020). All\nmethods are evaluated on the test set.\nDistantly-supervised methods:\n• Distant Match: This is the baseline that reports\nthe distant supervision quality ( i.e., compares\ndistantly-labeled results with the ground truth).\n• Distant RoBERTa: We ﬁne-tune a pre-trained\nRoBERTa model on distantly-labeled data as if\nthey are ground truth with the standard super-\nvised learning.\n• AutoNER (Shang et al., 2018): It trains the neu-\nral model with a “Tie or Break” tagging scheme.\nAmbiguous tokens are assigned with all possible\nlabels.\n• BOND (Liang et al., 2020): It ﬁrst trains a\nRoBERTa model on distantly-labeled data with\nearly stopping, and then uses a teacher-student\nframework to iteratively self-train the model.\nSupervised methods:\n• BiLSTM-CNN-CRF (Ma and Hovy, 2016): It\nwas one of the state-of-the-art NER models be-\nfore the appearance of pre-trained language mod-\nels, using bidirectional LSTM, CNN and CRF.\nIt is trained from scratch on the training data\nwithout any pre-trained knowledge.\n• RoBERTa: We ﬁne-tune a pre-trained RoBERTa\nmodel on the ground truth training data.\n3.3 Experiment Settings\nWe use the pre-trained RoBERTa-base model as the\nbackbone model (for our method and baselines).\nFor the three datasets CoNLL03, OntoNotes5.0,\nand Wikigold, the maximum sequence lengths are\nset to be 150, 180, and 120 tokens. For all three\ndatasets: The training batch size is 32; the hyper-\nparameters τ and qused by Eq. (5) are both set as\n0.7; the number of models for ensemble K = 5;\nwe use Adam (Kingma and Ba, 2015) as the opti-\nmizer. The peak learning rate is 3e−5, 1e−5 and\n5e−7 for noise-robust training, ensemble model\ntraining and self-training, respectively, with linear\ndecay. The model is run on 2 NVIDIA GeForce\nGTX 1080 Ti GPUs. More implementation details\ncan be found at Appendix C.\n3.4 Main Results\nTable 2 presents the performance of all meth-\nods measured by precision, recall and F1 scores.\nOn all three datasets, RoSTER achieves the best\nperformance among distantly-supervised methods.\nSpeciﬁcally, (1) the Distant RoBERTa baseline\nonly slightly improves the distant labeling results,\n10373\nMethods CoNLL03 OntoNotes5.0 Wikigold\nPre. Rec. F1 Pre. Rec. F1 Pre. Rec. F1\nDistant-Sup.\nDistant Match 0.811 0.638 0.714 0.745 ∗ 0.693∗ 0.718∗ 0.479 0.476 0.478\nDistant RoBERTa 0.837∗ 0.633∗ 0.721∗ 0.760∗ 0.715∗ 0.737∗ 0.603∗ 0.532∗ 0.565∗\nAutoNER 0.752 0.604 0.670 0.731 ∗ 0.712∗ 0.721∗ 0.435 0.524 0.475\nBOND 0.821 0.809 0.815 0.774∗ 0.701∗ 0.736∗ 0.534 0.686 0.600\nRoSTER (Ours) 0.859 0.849 0.854 0.753 0.789 0.771 0.649 0.710 0.678\nSup.\nBiLSTM-CNN-CRF 0.914 0.911 0.912 0.888 ∗ 0.887∗ 0.887∗ 0.554 0.543 0.549\nRoBERTa 0.906∗ 0.917∗ 0.912∗ 0.886∗ 0.890∗ 0.888∗ 0.853 0.876 0.864\nTable 2: Performance of all methods on three datasets measured by precision (Pre.), recall (Rec.) and F1 scores.\nBaseline results marked with ∗ are our own runs; others are reported by (Liang et al., 2020).\nAblations Pre. Rec. F1\nRoSTER 0.859 0.849 0.854\nw/o GCE 0.817 0.843 0.830\nw/o NR 0.830 0.836 0.833\nw/o ST 0.844 0.812 0.828\nTable 3: Ablation study on CoNLL03 dataset. We com-\npare our full method with ablations (see texts for the\nabbreviation meanings).\nAblations Mean (Std.) F1\nw. ensemble 0.828 (0.009)\nw/o ensemble 0.817 (0.025)\nTable 4: Mean and standard deviation (std.) F1 scores\nof 5 runs (before self-training) with and without model\nensemble on CoNLL03 dataset.\nshowing that directly applying supervised learn-\ning to distantly-labeled data will lead to overﬁt-\nting to label noise and poor model generalization;\n(2) RoSTER consistently outperforms AutoNER\nand BOND, demonstrating the superiority of our\nproposed noise-robust learning and self-training\napproach when trained on distantly-labeled data.\nFor further comparison with supervised meth-\nods, we vary the number of ground truth training\nsequences used for supervised RoBERTa, and show\nits performance in Figure 3(a). The performance\nof RoSTER is equivalent to using 1,000 cleanly\nannotated sequences for supervised RoBERTa.\n3.5 Ablation Study\nTo further validate the effectiveness of each com-\nponent, we compare RoSTER with the following\nablations by removing one component at a time: (1)\nreplace the GCE loss in Eq. (5) with cross entropy\nloss (w/o GCE); (2) do not perform noisy label\nremoval (w/o NR); (3) do not perform self-training\n(w/o ST). The results are shown in Table 3. It can\nbe seen that w/o GCEand w/o NRboth lead to\nworse performance than the full model, conﬁrming\nthe necessity of jointly using both designs in noise-\nrobust learning; w/o STalso reduces performance,\nshowing that bootstrapping the model with its own\nhigh-conﬁdence predictions beneﬁts the model’s\ngeneralization.\nWe also study the effect of model ensemble by\nrunning noise-robust training (without subsequent\nself-training) with 5 different seeds and reporting\nthe mean and standard deviation F1 in Table 4.\nEnsembling multiple models slightly improves the\nmean result and greatly reduces the variance.\n3.6 Parameter Study\nWe study the effect of two important hyperparam-\neters q and τ used in Eq. (5) on the model per-\nformance. We separately vary the value of qor τ\nin range [0.1,0.3,0.5,0.7,0.9] while keeping the\nother’s value as default (both use 0.7 as the de-\nfault value). The change in model performance\n(measured by F1) is shown in Figure 3(b). Over-\nall, the performance is rather insensitive to the two\nhyperparameters in the 0.5 −0.9 range. When\nq →1, LGCE approximates LMAE, having good\nnoise-robustness but poor convergence effective-\nness; when q →0, LGCE approximates LCE, hav-\ning good convergence but weak noise-robustness.\nSetting q= 0.7 achieves a good balance between\nconvergence and noise-robustness. When τ →0,\nall distant labels will be used for model training,\nand the model performance will suffer from the\nnoise in them; when τ →1, many correct labels\nwill be removed, and there will be insufﬁcient train-\ning data. Setting τ = 0.7 allows removing noisy\n10374\n500 750 1000 1250 \n# ground truth training sequences\n0.80\n0.82\n0.84\n0.86F1\nSup.RoBERTa\nRoSTER\n(a)\n0.1 0.90.3 0.5 0.7 \nparameter values\n0.80\n0.82\n0.84\n0.86F1\nq\nτ (b)\n0 15050 100 \nself-training steps\n0.65\n0.66\n0.67\n0.68F1\nw/o Augmentation\nw. Augmentation (c)\nFigure 3: (a) (On CoNLL03) Supervised RoBERTa with different number of ground truth training sequences.\nRoSTER trained on distantly-labeled data is equivalent to supervised RoBERTa using around 1,000 ground truth\nsequences. (b) (On CoNLL03) Parameter study. (c) (On Wikigold) Self-training with and without augmentation.\nOriginal: Swiss Bank Corp sets warrants on\nDTB-Bund-Future.\nAugmentation: Swiss Investment Corp sets\nwarrants for HTB-Bund-Future.\nOriginal: Chelsea Clinton was carefully\nshielded from the exposure of public life.\nAugmentation: Hillary Clinton was largely\nshielded from the spotlight of public life.\nTable 5: Original sequences and generated augmenta-\ntions. Replaced words are marked in blue.\nlabels while keeping enough reliable training data.\n3.7 Study of Augmentation\nWe study the effectiveness of the generated con-\ntextualized augmentations for the self-training step.\nWe run the self-training step with and without us-\ning the augmentations (i.e., including or excluding\nthe second term in Eq. (9)) on the Wikigold dataset\nand show the results in Figure 3(c). Even with-\nout augmentations, the self-training improves the\nmodel by using high-conﬁdence predictions for\nself-reﬁnement; with augmentations, the model is\ntrained with more data and eventually generalizes\nbetter with higher test set performance. Two con-\ncrete augmentation examples are shown in Table 5.\n3.8 Case Study\nFinally, we perform case study to understand the\nadvantage of RoSTER with a concrete example\nin Table 6. We show the prediction of AutoNER,\nBOND and RoSTER on a training sequence with\nlabel noise. AutoNER mainly learns from the\ngiven distant labels and slightly generalizes (labels\n“China” separately as a location entity);BOND is\nable to generalize better for more complete entity\ndetection because it has a self-training step that\nbootstraps the model on the training set without\ncompletely overﬁtting to distant labels; however, it\nis still impacted by label noise. RoSTER is able to\ndetect the noisy labels via the noise-robust learning\nstep, and then it further re-estimates the true labels\nin the self-training step with the help of the reliable\nsignals it learns from the clean data as well as the\npre-trained knowledge from PLMs via augmenta-\ntion, instead of relying purely on distant labels.\n4 Related Work\nThe effectiveness of deep neural models for NER\nusually comes with the cost of annotating large\namounts of training data. To alleviate the hu-\nman annotation burden when applying deep mod-\nels, several studies propose to train NER models\nwith weakly/distantly-labeled data. For weakly-\nsupervised NER, previous studies have explored\ncross lingual knowledge transfer from high re-\nsource languages to low resource languages (Feng\net al., 2018; Ni et al., 2017; Xie et al., 2018), ag-\ngregating multiple weak labeling functions (Lison\net al., 2020; Safranchik et al., 2020) or leverag-\ning sentence-level labels (Kruengkrai et al., 2020).\nFew-shot approaches (Huang et al., 2021) have also\nbeen explored to leverage very few labeled data for\nNER model training.\nOur work is more closely related to distantly-\nsupervised NER which uses external gazetteers\nor knowledge bases to automatically derive en-\ntity labels. Along this line, different methods\nhave been proposed to leverage the distant super-\nvision, such as propagating reliable type informa-\ntion on graphs (Ren et al., 2015), designing new\nmodel components to handle multiple possible la-\nbels (Shang et al., 2018), employing additional\nmodels to classify noisy data (Onoe and Durrett,\n10375\nDistant Match: Shanghai-Ek [Chor]PER is jointly owned by the Shanghai Automobile Corporation\nand [Ek Chor]PER China Motorcycle.\nGround Truth: [Shanghai-Ek Chor]ORG is jointly owned by the [Shanghai Automobile Corporation]ORG\nand [Ek Chor China Motorcycle]ORG.\nAutoNER: Shanghai-Ek [Chor]PER is jointly owned by the Shanghai Automobile Corporation\nand [Ek Chor]PER [China]LOC Motorcycle.\nBOND: [Shanghai-Ek Chor]PER is jointly owned by the [Shanghai]LOC [Automobile Corporation]ORG\nand [Ek Chor]PER [China Motorcycle]ORG.\nRoSTER: [Shanghai-Ek Chor]ORG is jointly owned by the [Shanghai Automobile Corporation]ORG\nand [Ek Chor China Motorcycle]ORG.\nTable 6: Case study with RoSTER and baselines. The sentence is from CoNLL03.\n2019), formulating the task as a positive-unlabeled\nlearning problem (Peng et al., 2019), and adopting\nearly stopping to prevent the model from overﬁt-\nting to distant labels (Liang et al., 2020). However,\nprevious methods either do not explicitly address\nthe noise in the distantly-labeled data ( i.e., treat-\ning them as if they are ground truth), or require an\nadditional set of manually-labeled data to train a\ndenoising model. Our method addresses the label\nnoise with a noise-robust learning scheme and a\nself-training step for better generalization, without\nusing any ground truth data.\nOur study is also related to data augmentation\ntechniques. In NLP, data augmentation is well de-\nveloped for text classiﬁcation, by either creating\nreal text sequences (Xie et al., 2020) via back trans-\nlation (Sennrich et al., 2016) or in the hidden states\nof the model via perturbations (Miyato et al., 2017)\nor interpolations by mixing up labels (Chen et al.,\n2020). However, these techniques cannot be read-\nily used for the NER task. (Dai and Adel, 2020)\nstudy a set of simple augmentation methods for\nthe NER task, like synonym replacement, mention\nreplacement or segment shufﬂing. Nevertheless,\nthese augmentations are context-free which may\ngenerate unreasonable sequences or require addi-\ntional sources like the WordNet. Our proposed\naugmentation method is unsupervised and contex-\ntualized, generating high-quality sequences thanks\nto the pre-trained knowledge of PLMs and reliably\nimproving model generalization.\n5 Conclusion and Future Work\nIn this paper, we study the distantly-supervised\nNER problem without using any human annota-\ntions but only distantly-labeled data. For better\nmodel training with noisy data, we propose a noise-\nrobust learning scheme, consisting of a new loss\nfunction and a noisy label removal step. To further\nimprove the model generalization, we propose a\nself-training method that guides model reﬁnement\nwith its own high-conﬁdence predictions and en-\nforces the model to make consistent predictions\non original and augmented sequences generated by\nPLMs. Our method achieves strong performance\non three benchmark datasets, outperforming previ-\nous distantly-supervised NER methods.\nThe techniques proposed in this paper are gener-\nalizable for future studies: The noise-robust learn-\ning scheme may also be applied to other NLP prob-\nlems where labels may contain noise (e.g., obtained\nvia crowdsourcing from non-experts); the augmen-\ntation and self-training method may be helpful\nfor other settings like semi-supervised or few-shot\nlearning. One may also consider exploring larger\npre-trained language models (e.g., RoBERTa-large)\nor more recent pre-trained language models (e.g.,\nCOCO-LM (Meng et al., 2021)) for the distantly-\nsupervised NER task.\nAcknowledgments\nResearch was supported in part by US DARPA\nKAIROS Program No. FA8750-19-2-1004, Social-\nSim Program No. W911NF-17-C-0099, and IN-\nCAS Program No. HR001121C0165, National Sci-\nence Foundation IIS-19-56151, IIS-17-41317, and\nIIS 17-04532, and the Molecule Maker Lab Insti-\ntute: An AI Research Institutes program supported\nby NSF under Award No. 2019897. Any opinions,\nﬁndings, and conclusions or recommendations ex-\npressed herein are those of the authors and do not\nnecessarily represent the views, either expressed\nor implied, of DARPA or the U.S. Government.\nWe thank anonymous reviewers for valuable and\ninsightful feedback.\n10376\nReferences\nEiji Aramaki, Yasuhide Miura, Masatsugu Tonoike,\nTomoko Ohkuma, Hiroshi Masuichi, and Kazuhiko\nOhe. 2009. Text2table: Medical text summariza-\ntion system based on named entity recognition and\nmodality identiﬁcation. In BioNLP.\nDominic Balasuriya, Nicky Ringland, Joel Nothman,\nTara Murphy, and James R Curran. 2009. Named\nentity recognition in wikipedia. In Workshop on The\nPeople’s Web Meets NLP.\nKevin Bowden, Jiaqi Wu, Shereen Oraby, Amita Misra,\nand Marilyn Walker. 2018. SlugNERDS: A named\nentity recognition tool for open domain dialogue sys-\ntems. In LREC 2018.\nJiaao Chen, Zichao Yang, and Diyi Yang. 2020. Mix-\nText: Linguistically-informed interpolation of hid-\nden space for semi-supervised text classiﬁcation. In\nACL.\nJason PC Chiu and Eric Nichols. 2016. Named entity\nrecognition with bidirectional LSTM-CNNs. TACL.\nXiang Dai and Heike Adel. 2020. An analysis of sim-\nple data augmentation for named entity recognition.\nIn COLING.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT.\nOren Etzioni, Michael Cafarella, Doug Downey, Ana-\nMaria Popescu, Tal Shaked, Stephen Soderland,\nDaniel S Weld, and Alexander Yates. 2005. Unsu-\npervised named-entity extraction from the web: An\nexperimental study. Artiﬁcial intelligence.\nXiaocheng Feng, Xiachong Feng, Bing Qin, Zhangyin\nFeng, and Ting Liu. 2018. Improving low resource\nnamed entity recognition using cross-lingual knowl-\nedge transfer. In IJCAI.\nDavide Ferrari, Yuhong Yang, et al. 2010. Maximum\nlq-likelihood estimation. The Annals of Statistics.\nAritra Ghosh, Himanshu Kumar, and PS Sastry. 2017.\nRobust loss functions under label noise for deep neu-\nral networks. In AAAI.\nJiaxin Huang, Chunyuan Li, Krishan Subudhi, Damien\nJose, Shobana Balakrishnan, Weizhu Chen, Baolin\nPeng, Jianfeng Gao, and Jiawei Han. 2021. Few-\nshot named entity recognition: An empirical base-\nline study. In EMNLP.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidi-\nrectional LSTM-CRF models for sequence tagging.\narXiv preprint arXiv:1508.01991.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020. How can we know what language\nmodels know? TACL.\nMahboob Alam Khalid, Valentin Jijkoun, and Maarten\nDe Rijke. 2008. The impact of named entity normal-\nization on information retrieval for question answer-\ning. In ECIR.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In ICLR.\nCanasai Kruengkrai, Thien Hai Nguyen, Sharifah Ma-\nhani Aljunied, and Lidong Bing. 2020. Improving\nlow-resource named entity recognition using joint\nsentence and token labeling. In ACL.\nSamuli Laine and Timo Aila. 2017. Temporal ensem-\nbling for semi-supervised learning. In ICLR.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn NAACL.\nZhen-Zhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite bert for self-supervised learn-\ning of language representations. In ICLR.\nChen Liang, Yue Yu, Haoming Jiang, Siawpeng Er,\nRuijia Wang, Tuo Zhao, and Chao Zhang. 2020.\nBOND: BERT-assisted open-domain named entity\nrecognition with distant supervision. In KDD.\nPierre Lison, Jeremy Barnes, Aliaksandr Hubin, and\nSamia Touileb. 2020. Named entity recognition\nwithout labelled data: A weak supervision approach.\nIn ACL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end\nsequence labeling via bi-directional LSTM-CNNs-\nCRF. In ACL.\nYu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han.\n2018. Weakly-supervised neural text classiﬁcation.\nIn CIKM.\nYu Meng, Jiaming Shen, Chao Zhang, and Jiawei Han.\n2019. Weakly-supervised hierarchical text classiﬁ-\ncation. In AAAI.\nYu Meng, Chenyan Xiong, Payal Bajaj, Saurabh Ti-\nwary, Paul Bennett, Jiawei Han, and Xia Song.\n2021. COCO-LM: Correcting and contrasting text\nsequences for language model pretraining. arXiv\npreprint arXiv:2102.08473.\nYu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan Xiong,\nHeng Ji, Chao Zhang, and Jiawei Han. 2020. Text\nclassiﬁcation using label names only: A language\nmodel self-training approach. In EMNLP.\n10377\nTakeru Miyato, Andrew M. Dai, and Ian J. Goodfel-\nlow. 2017. Adversarial training methods for semi-\nsupervised text classiﬁcation. In ICLR.\nDuc Tam Nguyen, Chaithanya Kumar Mummadi, Thi\nPhuong Nhung Ngo, Thi Hoai Phuong Nguyen,\nLaura Beggel, and Thomas Brox. 2019. SELF:\nLearning to ﬁlter noisy labels with self-ensembling.\nIn ICLR.\nJian Ni, Georgiana Dinu, and Radu Florian. 2017.\nWeakly supervised cross-lingual named entity recog-\nnition via effective annotation and representation\nprojection. In ACL.\nYasumasa Onoe and Greg Durrett. 2019. Learning to\ndenoise distantly-labeled data for entity typing. In\nNAACL.\nMinlong Peng, Xiaoyu Xing, Qi Zhang, Jinlan Fu,\nand Xuan-Jing Huang. 2019. Distantly supervised\nnamed entity recognition using positive-unlabeled\nlearning. In ACL.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, Anton\nBakhtin, Yuxiang Wu, Alexander H. Miller, and Se-\nbastian Riedel. 2019. Language models as knowl-\nedge bases? In EMNLP.\nXiang Ren, Ahmed El-Kishky, Chi Wang, Fangbo Tao,\nClare R V oss, and Jiawei Han. 2015. ClusType:\nEffective entity recognition and typing by relation\nphrase-based clustering. In KDD.\nEsteban Safranchik, Shiying Luo, and Stephen Bach.\n2020. Weakly supervised sequence tagging from\nnoisy rules. In AAAI.\nErik Tjong Kim Sang and Fien De Meulder. 2003. In-\ntroduction to the conll-2003 shared task: Language-\nindependent named entity recognition. In CoNLL.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Improving neural machine translation models\nwith monolingual data. In ACL.\nJingbo Shang, Liyuan Liu, Xiaotao Gu, Xiang Ren,\nTeng Ren, and Jiawei Han. 2018. Learning named\nentity tagger using domain-speciﬁc dictionary. In\nEMNLP.\nDenny Vrande ˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Commu-\nnications of the ACM.\nRalph Weischedel, Martha Palmer, Mitchell Marcus,\nEduard Hovy, Sameer Pradhan, Lance Ramshaw, Ni-\nanwen Xue, Ann Taylor, Jeff Kaufman, Michelle\nFranchini, et al. 2013. Ontonotes release 5.0\nldc2013t19. Linguistic Data Consortium, Philadel-\nphia, PA.\nJiateng Xie, Zhilin Yang, Graham Neubig, Noah A\nSmith, and Jaime G Carbonell. 2018. Neural cross-\nlingual named entity recognition with minimal re-\nsources. In EMNLP.\nJunyuan Xie, Ross B. Girshick, and Ali Farhadi. 2016.\nUnsupervised deep embedding for clustering analy-\nsis. In ICML.\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong,\nand Quoc Le. 2020. Unsupervised data augmenta-\ntion for consistency training. In NeurIPS.\nZhilu Zhang and Mert R Sabuncu. 2018. Generalized\ncross entropy loss for training deep neural networks\nwith noisy labels. In NeurIPS.\nA Generalized Cross Entropy\nThe Generalized Cross Entropy (GCE) loss\n(Eq. (3), also shown below),\nLGCE =\nn∑\ni=1\n1 −fi,yi (x; θ)q\nq ,\nis a generalized version of the cross entropy (CE)\nloss, as q→0, LGCE →LCE, shown as follows:\nlim\nq→0\nLGCE = lim\nq→0\nn∑\ni=1\n1 −fi,yi (x; θ)q\nq\n= lim\nq→0\nn∑\ni=1\nd\ndq (1 −fi,yi (x; θ)q)\nd\ndq (q)\n= lim\nq→0\nn∑\ni=1\n−fi,yi (x; θ)q log fi,yi (x; θ)\n1\n= −\nn∑\ni=1\nlog fi,yi (x; θ)\n= LCE.\nThe second line is obtained by applying L’Hôpital’s\nrule.\nB Baseline Sources\nWe use the following sources for baseline imple-\nmentation:\n• Distant RoBERTa: We use the Huggingface\nTransformer library for the RoBERTa model:\nhttps://huggingface.co/transformers/.\n• AutoNER: We use the authors’ released code:\nhttps://github.com/shangjingbo1226/\nAutoNER.\n• BOND: We use the authors’ released code:\nhttps://github.com/cliang1453/BOND/.\nThe results reported in Table 2 are obtained by\ntaking the higher value of (1) our own run and (2)\nthe corresponding performance reported in (Liang\net al., 2020).\n10378\nC Implementation Details\nTagging Scheme for Distantly-Supervised NER.\nInstead of using BIO/BIOES tagging scheme, we\nuse the binary IO format ( i.e., only distinguish\nwhether a token is a part of an entity or not) fol-\nlowing previous work (Peng et al., 2019), mainly\nbecause the distant labeling process may induce\npartially matched entities (e.g., the ﬁrst sentence in\nFigure 1), and the beginning/ending token of the\nentity can be inaccurate.\nDropping Non-Entity Tokens From Distant La-\nbels. We ﬁnd it beneﬁcial to randomly exclude\na portion of distantly-labeled non-entity tokens\n(we dropped 50% non-entity tokens for all three\ndatasets in the experiments) from training. This is\nprobably because the distant labeling process fails\nto detect some entities which will be mislabeled as\nthe “O” class, and randomly dropping non-entity\ntokens reduces the number of such false negative\nlabels.\nImplementation of the NER Head. Different\nfrom the common setup of ﬁne-tuning PLMs for\nNER, we implement the NER head in Figure 2 as\ntwo linear layers instead of one: One linear layer\nclassiﬁes entity tokens against non-entity ones (i.e.,\nbinary classiﬁcation), and the other linear layer\nclassiﬁes all entity type classes. During the self-\ntraining step, the former is trained to maintain its\npredictions, while the latter is trained to approxi-\nmate the soft labels.\nNoisy Label Removal for Minority Types.\nSome minority types may have very few labeled\ntokens and the model will output low-conﬁdence\npredictions on them due to insufﬁcient training. To\nmake sure those tokens are not treated as noisy\nones and removed from training, we do not per-\nform noisy label removal on entity type classes\n(i.e., keep all tokens labeled as those classes) of\nwhich >90% tokens are predicted with conﬁdence\nlower than the threshold."
}