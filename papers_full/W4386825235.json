{
  "title": "Brain-Inspired Remote Sensing Foundation Models and Open Problems: A Comprehensive Survey",
  "url": "https://openalex.org/W4386825235",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2166558591",
      "name": "Licheng Jiao",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2993179985",
      "name": "Zhongjian Huang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2574030705",
      "name": "Xiaoqiang LU",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2100229437",
      "name": "Xu Liu",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2125455967",
      "name": "YuTing Yang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2752439777",
      "name": "Jiaxuan Zhao",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2185415415",
      "name": "Jinyue Zhang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2144442131",
      "name": "Biao Hou",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2145130265",
      "name": "Shuyuan Yang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2099661209",
      "name": "Fang Liu",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2107876346",
      "name": "Wenping Ma",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2095721230",
      "name": "Lingling Li",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2130877461",
      "name": "Xiangrong Zhang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2537253677",
      "name": "Puhua Chen",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2162225490",
      "name": "Zhixi Feng",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A1978905009",
      "name": "Xu Tang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2099282363",
      "name": "Yuwei Guo",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2348045240",
      "name": "Dou Quan",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2099309572",
      "name": "Shuang Wang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2126991721",
      "name": "Weibin Li",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A1972075961",
      "name": "Jing Bai",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2100793049",
      "name": "Yangyang Li",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A1985245930",
      "name": "Ronghua Shang",
      "affiliations": [
        "Xidian University"
      ]
    },
    {
      "id": "https://openalex.org/A2008450384",
      "name": "Jie Feng",
      "affiliations": [
        "Xidian University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3172942063",
    "https://openalex.org/W6849928207",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W6800751262",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W6802987763",
    "https://openalex.org/W6791353385",
    "https://openalex.org/W4321487954",
    "https://openalex.org/W4225630686",
    "https://openalex.org/W4382138923",
    "https://openalex.org/W6852403328",
    "https://openalex.org/W6851549221",
    "https://openalex.org/W3016495922",
    "https://openalex.org/W4312795296",
    "https://openalex.org/W6720561100",
    "https://openalex.org/W6851815880",
    "https://openalex.org/W6840202747",
    "https://openalex.org/W6848503226",
    "https://openalex.org/W6779977557",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W4312804044",
    "https://openalex.org/W6798805250",
    "https://openalex.org/W6811072154",
    "https://openalex.org/W6849177959",
    "https://openalex.org/W4386071687",
    "https://openalex.org/W6851578965",
    "https://openalex.org/W6810574017",
    "https://openalex.org/W4214644404",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W3176641147",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W6676497082",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W6790019176",
    "https://openalex.org/W3152083889",
    "https://openalex.org/W2963785576",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W6852253433",
    "https://openalex.org/W4297990271",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6809923571",
    "https://openalex.org/W2944851425",
    "https://openalex.org/W2558787362",
    "https://openalex.org/W4213019189",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W6799370299",
    "https://openalex.org/W4376113940",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W4321499901",
    "https://openalex.org/W4366823237",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W4206706211",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W3144293453",
    "https://openalex.org/W1663984431",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W4320495408",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W4313034430",
    "https://openalex.org/W3023371261",
    "https://openalex.org/W6797132756",
    "https://openalex.org/W6851949647",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6851800889",
    "https://openalex.org/W4244207479",
    "https://openalex.org/W4283697304",
    "https://openalex.org/W3096831136",
    "https://openalex.org/W6640963894",
    "https://openalex.org/W2963420272",
    "https://openalex.org/W6755312952",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W6774314701",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W6774670964",
    "https://openalex.org/W2883725317",
    "https://openalex.org/W6779997284",
    "https://openalex.org/W6791742336",
    "https://openalex.org/W2321533354",
    "https://openalex.org/W6747899497",
    "https://openalex.org/W2326925005",
    "https://openalex.org/W3113662382",
    "https://openalex.org/W3204957802",
    "https://openalex.org/W6807144946",
    "https://openalex.org/W4292794000",
    "https://openalex.org/W4285128126",
    "https://openalex.org/W4285157526",
    "https://openalex.org/W3205123177",
    "https://openalex.org/W6851629679",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4312651322",
    "https://openalex.org/W4386071547",
    "https://openalex.org/W4386083033",
    "https://openalex.org/W6759579507",
    "https://openalex.org/W6738045163",
    "https://openalex.org/W6796581206",
    "https://openalex.org/W6853251322",
    "https://openalex.org/W6769627184",
    "https://openalex.org/W6810932996",
    "https://openalex.org/W6798182279",
    "https://openalex.org/W6850625674",
    "https://openalex.org/W6846002521",
    "https://openalex.org/W6847118041",
    "https://openalex.org/W6847076894",
    "https://openalex.org/W6810081322",
    "https://openalex.org/W6810738896",
    "https://openalex.org/W4386076222",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W3216270236",
    "https://openalex.org/W4386057769",
    "https://openalex.org/W6851590661",
    "https://openalex.org/W6810334672",
    "https://openalex.org/W6803872405",
    "https://openalex.org/W6846867676",
    "https://openalex.org/W6809885388",
    "https://openalex.org/W6842585177",
    "https://openalex.org/W6849307739",
    "https://openalex.org/W6855055401",
    "https://openalex.org/W6852490124",
    "https://openalex.org/W4206424103",
    "https://openalex.org/W4362714880",
    "https://openalex.org/W3119125170",
    "https://openalex.org/W4206470192",
    "https://openalex.org/W3040777427",
    "https://openalex.org/W4281250306",
    "https://openalex.org/W4285992154",
    "https://openalex.org/W4285168619",
    "https://openalex.org/W4382998925",
    "https://openalex.org/W4382193053",
    "https://openalex.org/W4206554021",
    "https://openalex.org/W3086451439",
    "https://openalex.org/W3134794785",
    "https://openalex.org/W4213253308",
    "https://openalex.org/W4389104749",
    "https://openalex.org/W4367721790",
    "https://openalex.org/W3174575212",
    "https://openalex.org/W4304757906",
    "https://openalex.org/W4378194802",
    "https://openalex.org/W4285296445",
    "https://openalex.org/W4312981890",
    "https://openalex.org/W4312325112",
    "https://openalex.org/W4303980685",
    "https://openalex.org/W3011248632",
    "https://openalex.org/W4285234694",
    "https://openalex.org/W4285725586",
    "https://openalex.org/W4291727297",
    "https://openalex.org/W4210588976",
    "https://openalex.org/W3128055887",
    "https://openalex.org/W4226458380",
    "https://openalex.org/W4288391342",
    "https://openalex.org/W4362683587",
    "https://openalex.org/W3034208233",
    "https://openalex.org/W3211995028",
    "https://openalex.org/W4214928390",
    "https://openalex.org/W4225648150",
    "https://openalex.org/W4292258945",
    "https://openalex.org/W4226228401",
    "https://openalex.org/W3193414609",
    "https://openalex.org/W4226417358",
    "https://openalex.org/W4312336436",
    "https://openalex.org/W6795827248",
    "https://openalex.org/W4224212608",
    "https://openalex.org/W4365801655",
    "https://openalex.org/W4319083668",
    "https://openalex.org/W3164897814",
    "https://openalex.org/W6802189453",
    "https://openalex.org/W6803421043",
    "https://openalex.org/W3208099041",
    "https://openalex.org/W4206144626",
    "https://openalex.org/W4285293152",
    "https://openalex.org/W4207064056",
    "https://openalex.org/W4311796646",
    "https://openalex.org/W4319289169",
    "https://openalex.org/W4317761551",
    "https://openalex.org/W4380534865",
    "https://openalex.org/W4365790484",
    "https://openalex.org/W2969711198",
    "https://openalex.org/W3011098089",
    "https://openalex.org/W3161959336",
    "https://openalex.org/W6791175494",
    "https://openalex.org/W4285106403",
    "https://openalex.org/W3210764906",
    "https://openalex.org/W4323338432",
    "https://openalex.org/W3127571973",
    "https://openalex.org/W4290716747",
    "https://openalex.org/W6798190242",
    "https://openalex.org/W3200540539",
    "https://openalex.org/W4285143409",
    "https://openalex.org/W4294643336",
    "https://openalex.org/W4289821511",
    "https://openalex.org/W4285222391",
    "https://openalex.org/W4312466966",
    "https://openalex.org/W4284880006",
    "https://openalex.org/W4362654594",
    "https://openalex.org/W4367663071",
    "https://openalex.org/W2979315537",
    "https://openalex.org/W4212841652",
    "https://openalex.org/W4315778389",
    "https://openalex.org/W4380053227",
    "https://openalex.org/W4304084012",
    "https://openalex.org/W4322707256",
    "https://openalex.org/W6853655421",
    "https://openalex.org/W3089915566",
    "https://openalex.org/W4312389717",
    "https://openalex.org/W3194015448",
    "https://openalex.org/W4200272616",
    "https://openalex.org/W3011916860",
    "https://openalex.org/W4285744637",
    "https://openalex.org/W4312915946",
    "https://openalex.org/W4214587440",
    "https://openalex.org/W4213449032",
    "https://openalex.org/W4220716465",
    "https://openalex.org/W4317433994",
    "https://openalex.org/W4362598957",
    "https://openalex.org/W4316661266",
    "https://openalex.org/W4292828962",
    "https://openalex.org/W4229439310",
    "https://openalex.org/W4285818301",
    "https://openalex.org/W4312920811",
    "https://openalex.org/W4312719444",
    "https://openalex.org/W4225991573",
    "https://openalex.org/W4360930321",
    "https://openalex.org/W3034971973",
    "https://openalex.org/W3127131334",
    "https://openalex.org/W4294311189",
    "https://openalex.org/W2067178723",
    "https://openalex.org/W1980038761",
    "https://openalex.org/W2515866431",
    "https://openalex.org/W2592962403",
    "https://openalex.org/W2884822772",
    "https://openalex.org/W4225108153",
    "https://openalex.org/W3129983770",
    "https://openalex.org/W4213305896",
    "https://openalex.org/W2991488782",
    "https://openalex.org/W2964309882",
    "https://openalex.org/W4312443924",
    "https://openalex.org/W4385346076",
    "https://openalex.org/W4312769131",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W4312349930",
    "https://openalex.org/W4226058394",
    "https://openalex.org/W4214648418",
    "https://openalex.org/W2962749812",
    "https://openalex.org/W3203608457",
    "https://openalex.org/W2992240579",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W4312820606",
    "https://openalex.org/W3206817059",
    "https://openalex.org/W1966084915",
    "https://openalex.org/W4224261208",
    "https://openalex.org/W2898323475",
    "https://openalex.org/W2022424636",
    "https://openalex.org/W3202498768",
    "https://openalex.org/W3204722966",
    "https://openalex.org/W4378901443",
    "https://openalex.org/W4319866562",
    "https://openalex.org/W2803816323",
    "https://openalex.org/W2105464873",
    "https://openalex.org/W2004834775",
    "https://openalex.org/W3157638739",
    "https://openalex.org/W2788231085",
    "https://openalex.org/W2058853524",
    "https://openalex.org/W3182972866",
    "https://openalex.org/W6761628794",
    "https://openalex.org/W6731404398",
    "https://openalex.org/W3174155187",
    "https://openalex.org/W2010228425",
    "https://openalex.org/W1498436455",
    "https://openalex.org/W2045908891",
    "https://openalex.org/W6843103369",
    "https://openalex.org/W2998470023",
    "https://openalex.org/W4210371477",
    "https://openalex.org/W2031534534",
    "https://openalex.org/W2168462263",
    "https://openalex.org/W2012133058",
    "https://openalex.org/W2167422691",
    "https://openalex.org/W3112358871",
    "https://openalex.org/W2744751690",
    "https://openalex.org/W3192206917",
    "https://openalex.org/W3109365969",
    "https://openalex.org/W6801514204",
    "https://openalex.org/W1995341919",
    "https://openalex.org/W2006370340",
    "https://openalex.org/W6743446608",
    "https://openalex.org/W4312853765",
    "https://openalex.org/W4285065070",
    "https://openalex.org/W6779879114",
    "https://openalex.org/W6853384840",
    "https://openalex.org/W6852755768",
    "https://openalex.org/W2395811491",
    "https://openalex.org/W6849168099",
    "https://openalex.org/W6854027915",
    "https://openalex.org/W4379929801",
    "https://openalex.org/W6848908594",
    "https://openalex.org/W6853440210",
    "https://openalex.org/W3035821888",
    "https://openalex.org/W4310854133",
    "https://openalex.org/W3009561768",
    "https://openalex.org/W4206307542",
    "https://openalex.org/W4221166201",
    "https://openalex.org/W4404594861",
    "https://openalex.org/W3183501258",
    "https://openalex.org/W4392397297",
    "https://openalex.org/W4367859967",
    "https://openalex.org/W3213833596",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W4396760778",
    "https://openalex.org/W4367000428",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3159812033",
    "https://openalex.org/W3103318792",
    "https://openalex.org/W4226359564",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4297948910",
    "https://openalex.org/W4392172801",
    "https://openalex.org/W4224035735",
    "https://openalex.org/W4380356267",
    "https://openalex.org/W4287025617",
    "https://openalex.org/W4390692489",
    "https://openalex.org/W4402671897",
    "https://openalex.org/W1959608418",
    "https://openalex.org/W3200975211",
    "https://openalex.org/W3173151551",
    "https://openalex.org/W4255630607",
    "https://openalex.org/W4366196934",
    "https://openalex.org/W2486285194",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W2565239651",
    "https://openalex.org/W4394966885",
    "https://openalex.org/W4396982199",
    "https://openalex.org/W4385004765",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4390874379",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W1566309885",
    "https://openalex.org/W3215626407",
    "https://openalex.org/W4288391486",
    "https://openalex.org/W4366208220",
    "https://openalex.org/W4313483388",
    "https://openalex.org/W4319049530",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3135922489",
    "https://openalex.org/W3105577662",
    "https://openalex.org/W4361230739",
    "https://openalex.org/W4310921506",
    "https://openalex.org/W4378509449"
  ],
  "abstract": "The foundation model (FM) has garnered significant attention for its remarkable transfer performance in downstream tasks. Typically, it undergoes task-agnostic pretraining on a large dataset and can be efficiently adapted to various downstream applications through fine-tuning. While FMs have been extensively explored in language and other domains, their potential in remote sensing has also begun to attract scholarly interest. However, comprehensive investigations and performance comparisons of these models on remote sensing tasks are currently lacking. In this survey, we provide essential background knowledge by introducing key technologies and recent developments in FMs. Subsequently, we explore essential downstream applications in remote sensing, covering classification, localization, and understanding. Our analysis encompasses over 30 FMs in both natural and remote sensing fields, and we conduct extensive experiments on more than 10 datasets, evaluating global feature representation, local feature representation, and target localization. Through quantitative assessments, we highlight the distinctions among various FMs and confirm that pretrained large-scale natural FMs can also deliver outstanding performance in remote sensing tasks. After that, we systematically presented a brain-inspired framework for remote sensing foundation models (RSFMs). We delve into the brain-inspired characteristics in this framework, including structure, perception, learning, and cognition. To conclude, we summarize 12 open problems in RSFMs, providing potential research directions. Our survey offers valuable insights into the burgeoning field of RSFMs and aims to foster further advancements in this exciting area.",
  "full_text": "1\nBrain-inspired Remote Sensing Foundation Models\nand Open Problems: A Comprehensive Survey\nLicheng Jiao, Fellow, IEEE, Zhongjian Huang, Xiaoqiang Lu, Xu Liu , Member, IEEE, Yuting Yang,\nJiaxuan Zhao, Jinyue Zhang, Biao Hou , Member, IEEE, Shuyuan Yang, Senior Member, IEEE , Fang Liu , Senior\nMember, IEEE, Wenping Ma, Senior Member, IEEE , Lingling Li , Senior Member, IEEE ,\nXiangrong Zhang, Senior Member, IEEE , Puhua Chen , Senior Member, IEEE , Zhixi Feng , Member, IEEE,\nXu Tang, Senior Member, IEEE , Yuwei Guo, Senior Member, IEEE , Dou Quan , Member, IEEE,\nShuang Wang, Senior Member, IEEE , Weibin Li, Jing Bai , Senior Member, IEEE , Yangyang Li, Senior\nMember, IEEE, Ronghua Shang , Senior Member, IEEE , Jie Feng , Senior Member, IEEE\nAbstract—The foundation model (FM) has garnered significant\nattention for its remarkable transfer performance in downstream\ntasks. Typically, it undergoes task-agnostic pre-training on a large\ndataset and can be efficiently adapted to various downstream ap-\nplications through fine-tuning. While FMs have been extensively\nexplored in language and other domains, their potential in remote\nsensing has also begun to attract scholarly interest. However,\ncomprehensive investigations and performance comparisons of\nthese models on remote sensing tasks are currently lacking. In\nthis survey, we provide essential background knowledge by intro-\nducing key technologies and recent developments in FMs. Subse-\nquently, we explore essential downstream applications in remote\nsensing, covering classification, localization, and understanding.\nOur analysis encompasses over thirty FMs in both natural and\nremote sensing fields, and we conduct extensive experiments on\nmore than ten datasets, evaluating global feature representation,\nlocal feature representation, and target localization. Through\nquantitative assessments, we highlight the distinctions among\nvarious foundation models and confirm that pre-trained large-\nscale natural FMs can also deliver outstanding performance in\nremote sensing tasks. After that, we systematically presented a\nbrain-inspired framework for remote sensing foundation models\n(RSFMs). We delve into the brain-inspired characteristics in\nthis framework, including structure, perception, learning, and\ncognition. To conclude, we summarize twelve open problems\nin RSFMs, providing potential research directions. Our survey\noffers valuable insights into the burgeoning field of RSFMs and\naims to foster further advancements in this exciting area.\nI. I NTRODUCTION\nThe rapid advancements in data and model parameters\nhave catalyzed the emergence of a new paradigm in artifi-\nThis work was supported in part by the Key Scientific Technological\nInnovation Research Project by Ministry of Education, the State Key Program\nand the Foundation for Innovative Research Groups of the National Natural\nScience Foundation of China (61836009), the National Natural Science\nFoundation of China (U22B2054, 6207619262006177, 61902298, 61573267,\n61906150and 62276199), the 111 Project, the Program for Cheung Kong\nScholars and Innovative Research Team in University (IRT 15R53), the ST\nInnovation Project from the Chinese Ministry of Education, the Key Research\nand Development Program in Shaanxi Province of China(2019ZDLGY03-\n06), the National Science Basic Research Plan in Shaanxi Province of\nChina(2022JQ-607), the China Postdoctoral fund(2022T150506), the Scien-\ntific Research Project of Education Department In Shaanxi Province of China\n(No.20JY023).\nThe authors are with the Key Laboratory of Intelligent Perception and Image\nUnderstanding of the Ministry of Education of China, International Research\nCenter of Intelligent Perception and Computation, School of Artificial Intel-\nligence, Xidian University, Xi’an, China (e-mail: huangzj@stu.xidian.edu.cn;\nlchjiao@mail.xidian.edu.cn).\ncial intelligence [1, 2, 3]. Through large-scale pre-training\nof neural networks, we witness the manifestation of novel\ncharacteristics, enabling a previously unprecedented level of\nunderstanding and reasoning [4]. The models trained on broad\ndata can be adapted to a wide range of downstream tasks.\nThese models are called foundation models (FMs) to under-\nscore their critically central yet incomplete character [5].\nDifferent from the non-foundation models designed for a\nspecific task or domain, foundation models are a new paradigm\nthat can be adapted to many different tasks and domains.\nAs shown in Fig. 1, the main characteristics of FMs can be\nsummarized into three aspects: data and model Size, learning\nstrategies, and adaptation. (1) Data and model size: FMs are\ntrained on large amounts of unlabeled or weakly labeled data,\nsuch as text, images, audio, or video, that cover a broad range\nof topics and domains. For example, visual training datasets\ninclude ImageNet-22K [6] and JFT-300M [7], and multi-modal\ndatasets include Laion [8]. As for model size, flexible and\nexpandable models like ViT can be scaled up from ViT-\nsmall with 50 million parameters to ViT-22B with 22 billion\nparameters [3]. (2) Learning strategies: Foundation models\nuse self-supervised or semi-supervised learning to learn from\nthe data without human supervision or with minimal human\nguidance. Numerous self-supervised algorithms are employed\nfor pre-training, including contrast learning [9], generative\nmasked image modeling [1], and multi-modal contrast learning\n[10]. (3) Adaption: Foundation models can be adapted or fine-\ntuned to various downstream tasks or domains by adding a\nsmall amount of task-specific data or parameters.\nThe interpretation of remote sensing (RS) is a crucial\nmethod for observing the Earth [11, 12, 13], and FMs have\ngarnered significant attention and increasingly play a vital role\nin this domain [14, 15]. RS images, acquired through satellites,\nare generated at a substantial scale, reaching PB scale [16].\nDue to the complexity of RS data and the requirement for\nprofessional knowledge, labeled RS data is scarce. The pre-\ntraining approach of the FM can mine the value of RS data\nand enable the utilization of a significant amount of unlabeled\ndata.\nInspired by the FMs developed for natural images, the field\nof RS has also seen the emergence of FMs, garnering attention\n[17, 18, 19, 20, 21, 22]. The typical remote sensing foundation\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n2\nPanchromatic True color\nMultispectral Hyperspectral\nSAR Satellite Video \nData\n Applications\nObject \nDetection\nSemantic \nSegmentation\nObject \nTracking\n...\nChange \nDetection\nImage Caption\nFoundation Model\nTransformer Encoder\nLinear Projection of Flattened Patches\nLarge-Scale Model\n1 2 3 4 5 6 7 8 90 *\nLearning Algorithms\nContrastive \nLearningModel\nMasked Image \nModeling\nAdaptionAdaption\nAdaption\nVQA\nFig. 1. The framework of the remote sensing foundation models. The models are trained with large-scale multi-modality data and can be adapted to downstream\napplications.\nTABLE I\nA SUMMARY OF EXISTING REPRESENTATIVE FOUNDATION MODELS IN NATURAL AND REMOTE SENSING FIELDS . WE SUMMARIZE THEIR MODALITY ,\nVISUAL /TEXT ENCODER , MODEL PARAMETERS , TRAINING DATASET AND DATA NUMBER .\nField Method Modality Visual Encoder Text Encoder Parameters Training Dataset Data Number\nBYOL [23] V ResNet200 - 375M IN1K 1.28M Images\nSimCLR v2 [24] V ResNet152 - 795M IN1K 1.28M Images\nDINO [25] V ViT Base - 85M IN1K 1.28M Images\nMAE [1] V ViT Huge - 632M IN1K 1.28M Images\nSimMIM [26] V SwinV2 Large - 197M IN22K 14M Images\nScaling ViT [2] V ViT Giant - 1.8B JFT-300M/3B 3B Images\nViT 22B [3] V ViT 22B - 22B JFT-4B 4B Images\nCLIP [10] VL ViT Large Transformer 307M/63M WebImageText 400M ITPs\nALBEF [27] VL ViT-B BERT base 85.8M/123.7M CC, SBU,\nCOCO, VG\n4M Images\n14M ITPs\nCoCa [28] VL Transformer Transformer 1B/1.1B JFT-3B, ALIGN\nBLIP-2 [29] VL ViT Giant FlanT5-XXL 1.8B/11B\nCOCO, VG,\nCC3M, CC12M, SBU,\nsubset of LAION400M\n129M ITPs\nBEiT-v3 [30] VL Multiway\nTransformers\nMultiway\nTransformers 1.9B\nCC12M, CC3M, SBU, COCO,\nVG, IN21K,\nEnglish Wikipedia, BookCorpus,\nOpenWebText, CC-News, Stories\n21M ITPs,\n14M Images,\n160G documents\nNatural\nSAM [31] VL ViT-Huge - 636M SA-1B 11M Images\nRSP [32] V Swin-Tiny\nViTAEv2-S - 27M MillionAID 1M Images\nRVSA [17] V ViTAE-Base - 89M MillionAID 1M Images\nRingMo [18] V Swin-T Base - 88M self-collected 2M Images\nGeograph [33] V ResNet50 - 24M fMoW, GeoImageNet 0.9M Images\nSatMAE [20] V ViT Large - 307M fMoW 0.7M Images\nScale MAE [22] V ViT Large - 307M fMow 0.4M Images\nBillion [19] V ViT G12 - 2.4B MillionAID 1M Images\nRS\nGFM [21] V Swin - 80M GeoPile 0.6M Images\nNOTE: The V and VL in Modality represent the model is designed for vision and vision-language, respectively.\nA single number shown in Parameter represent the overall parameter number of the model. The union like (307M/63M) means the model consist of a\nvisual encoder with 307 millions parameters and a text encoder with 63 millions.\nThe ITP shown in Data Number is the abbreviation of Image-Text pair.\nThe training dataset used for foundation models are ImageNet 1K (IN1K), ImageNet 22K (IN22K) [6], JFT [2], WebImageText [10], COCO [34],\nConceptual 12M (CC12M) [35], Conceptual Captions (CC3M) [36], SBU Captions (SBU) [37], Visual Genome (VG) [38], ALIGN [39], LAION [8],\nSegment Anything 1B (SA-1B) [31], MillionAID [40], fMoW [41], GeoImageNet [21], GeoPile [21].\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n3\nRemote Sensing \nFoundation Model \n• Model Structure :      Transformer\n• Learning Algorithm: Self-supervised Learning\n• Quick Adaptation:     Parameter-efficient Tuning \nKey Technology of FMs\nDevelopments of FMs\nApplications of RSFMs\nEffectiveness of RSFMs\nOpen Problems\n• Language Foundation Models\n• Vision Foundation Models\n• Vision-Language Foundation Models\n• Remote Sensing Foundation Models\n• Global Representation: KNN Linear Experiments\n• Local Representation:   Semantic Segmentation\n• Object Localization:      Object Detection\n• Scence Classification \n• Semantic Segmentation \n• Change Detection \n• Object Detection \n• Visual Question Answering\n• Moving Object Detection\n• Object Tracking \n• Visual Grounding \n• Image Caption \nBrain-Inspried RSFMs\n• Structure:   Spiking, Diversity, Geometry\n• Perception: Sparsity, Selectivity, Directionality\n• Learning:    Hebbian, Error-correction, Competitive\n• Cognition:   Memory, Forgetting, Reasoning\n• Brain-Inspired\n• Learning Theory\n• Interpretability\n• Knowledge\n• Efficiency\n• Multi-modal and Cross-modal\n• Friendliness Interfaces\n• Physics-Informed\n• Causal Inference\n• Robustness\n• Spatiotemporal Forecasting\n• Security\nFig. 2. The organizational structure of this survey.\nmodel (RSFM) is pre-trained using a substantial number of\noptical images, validating the feasibility of training FMs in\nthe RS domain. Additionally, scholars have considered factors\nsuch as multi-spectral images, time-series images [20], and\ngeographical resolution [22] to build more robust RS models.\nThe advancements in remote sensing foundation models\nhave been impressive. However, there is still a noticeable gap\nbetween the scale of remote sensing data and the models,\nespecially when compared to natural foundation models. Table\nI summarizes the basic information of foundation models\nin both natural and remote sensing domains, highlighting\nthe disparity in dataset size and model parameters. RSFMs\ntypically rely on data-driven approaches, training large-scale\nparameters from limited remote sensing datasets like Million\nAID [40], which contains only 1 million images. In contrast,\nnatural foundation models benefit from much larger datasets,\nsuch as ImageNet-1K, containing millions of images.\nBeside the scale of models, most RSFMs follow the\nparadigm of the nature FMs. It has been demonstrated that\nnatural FMs suffer from brittle, unchangeable structures.\nModels-based generation is prone to hallucinate unintended\nresults [42]. These unstable results limit the application of\nthe FMs in the field of remote sensing, which requires high\naccuracy and robustness to guarantee security. To bridge this\ngap, brain-inspired RSFMs will be a new potential research\ndirection [11]. Jiao et al. [43] have conducted systematic\nanalyses of algorithms inspired by brain and biological mecha-\nnisms, including neural networks, natural computing, machine\nlearning, and compression. Their work has provided valuable\ninsights into how brain-inspired approaches can be applied\nto enhance the capabilities of foundation models. Similarly,\nSchmidgall et al. [44] have explored the integration of more\nbiologically plausible mechanisms into current brain-inspired\nlearning representations, with the goal of further enhancing\nthe capabilities of these networks. Zou et al. [45] focuses\non reviewing brain-inspired models with an emphasis on the\nspatio-temporal nature of visual signals.\nIn this paper, we have drawn insights from brain charac-\nteristics to propose a brain-inspired framework for remote\nsensing foundation models. The exploration of brain-inspired\nalgorithms in the context of remote sensing holds great\npromise and offers exciting opportunities for future research\nand advancements in the field.\nWe investigate the progress of current RSFMs as shown\nin Fig. 2. In section II, we describe the key technologies\nunderlying these models, including the essential Transformer\nstructure of FMs and self-supervised pre-training methods.\nFurthermore, we introduce common methods for efficient\nparameter optimization, taking into account the application\nparadigm of the latest FMs. Section III covers the latest\ndevelopments in various FMs, including language FMs, visual\nFMs, visual-language FMs, and RSFMs. Section IV delves\ninto several core applications of RS interpretation, focusing on\nclassification, localization, and understanding tasks. To address\nthe lack of systematic comparison between current RSFMs and\nnatural FMs, experiments are conducted in remote sensing\ninterpretation from three perspectives: global representation,\nlocal representation, and object localization in section V .\nThese experiments provide a fair comparison of the proposed\nRSFMs. In section VI, a framework of brain-inspired RSFM\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n4\nis proposed and the key characteristics of brain are discussed.\nFinally, twelve open problems of RS is discussed in section\nVII.\nOur contributions can be summarized as follows:\n(1) We have comprehensively investigated the key tech-\nnologies and latest advancements in FMs. This provides a\ncomprehensive overview of FM research.\n(2) To the best of our knowledge, this is the first systematic\nsummary and analysis of the performance of existing RSFMs\ncompared to natural FMs. The experimental results can serve\nas a guide.\n(3) We propose a framework of brain-inspired RSFM and\ninvestigate the key characteristics of the brain. In addition,\ntwelve open problems in the construction of RSFMs are\ndiscussed.\nOverall, our work provides valuable insights into the FM\nlandscape, offers performance comparisons, and highlights im-\nportant characteristics and challenges in the realm of RSFMs.\nII. T HE KEY TECHNOLOGY OF FOUNDATION MODELS\nThe key technology of FMs consist of the model structures,\nlearning algorithms and finetuning. In this section, we firstly\nintroduce the important structure, Transformer. Then, the de-\nvelopment of self-supervised learning and parameter-effcient\ntuning are discussed.\nA. Transformer\nTransformer [46] is a neural network model based on a self-\nattention mechanism, which is often used in natural language\nprocessing tasks. Due to the non-locality and the natural\nrelationship of language, this long-term and self-attention\nbehavior makes Transformer an effective tool [47].\nThe main idea of Transformer is to calculate the context-\nrelated representation through the self-attention mechanism.\nConvolutional neural networks (CNN) of the traditional re-\ncurrent neural network (RNN) [48] have some difficulties in\nprocessing long sequence data when processing long sequence\ndata [49]. The entire network structure of Transformer is com-\nposed of attention mechanisms, abandoning traditional CNN\nor RNN, and obtaining context information by calculating the\ncorrelation between each word and all other words, thereby\navoiding the problem of traditional models [50].\nThe core component of the Transformer includes a multi-\nhead self-attention mechanism and forward feedback network.\nIn the multi-head self-attention mechanism, the input text\nsequence will be split into multiple vectors. Then, a series of\nlinear transformations, attention calculation, and concatenation\noperations will be followed to generate an output vector. This\noutput vector contains the information in each position of\nthe input sequence, and the information at each position is\nconsidered equally. Therefore, Transformer is more suitable\nfor processing long text sequences compared to the circulating\nneural network [51, 52, 53].\n1) The villian Transformer: The main structure is shown\nin Fig. 3. The core of the Transformer consists of the encoder\nand decoder. It shows the overall encoder-decoder structure\nof the Transformer model. The encoder converts the input\nMasked \nMulti-head \nattention\nAdd & Norm\nFeedforward \nnetwork\nMulti-head \nattention\nInput \nencoding\nOutput \nencoding\nAdd & Norm Add & Norm\nMulti-head \nattention\nAdd & Norm\nAdd & Norm\nFeedforward \nnetwork\nPosition \nembeddings\nPosition \nembeddings\nLinear\nSoftmax\nOutput\nNx Nx\nFig. 3. The architecture of Transformer.\nsequence into the context vector, and the decoder uses the\ncontext vector to generate the output sequence. The encoder\nis mainly composed of two layers of the self-attention head\nand a two-layer feed-forward neural network. There is also\na decoder that has the self-attention layer and feed-forward\nlayer. In addition, there is also a self-attention layer between\nthese two layers to pay attention to the relevant parts of the\ninput sentence, which is similar to the attention of the Seq2Seq\nmodel [54].\nSpecifically, Transformer usually contains multiple contin-\nuous encoders. Each encoder consists of multiple layers. Each\nlayer contains two sub-layers: multi-head attention and feed-\nforward network. In the multi-head self-attention mechanism,\nthe input sequence is divided into multiple heads. Each head\nperforms a self-attention calculation, and the attention weight\nand weighted the input vector to obtain the relationship of\neach word with other words. And then, the output vector can\nbe calculated. The output vectors of all heads are merged\ninto a longer vector through the concatenation operation. It\ncontains information on each position in the input sequence,\nand the information at each position is considered equally.\nThis is why Transformer is more suitable for handling long text\nsequences. Multi-head attention and feedforward networks use\nresidual connections and layers among the sub-layers. In the\nfeedforward network, the two neural networks of the RELU\nactivation function use the two layers of neural networks for\nnon-linear transformation.\nCompared with the encoder, each decoder block is added\nwith a multi-head crossing attention layer to embed the de-\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n5\nPreprint. Under review.\nTransformer \nEncoder\nMLP \nHead\n Vision Transformer (ViT)\n*\nLinear \nProjection \nof \nFlattened \nPatches\n*\n \nExtra \nlearnable\n     \n[class]\n \nembedding\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\nPatch \n+ \nPosition \nEmbedding\nClass\nBird\nBall\nCar\n...\nEmbedded \nPatches\nMulti-Head \nAttention\nNorm\nMLP\nNorm\n+\nL\n \nx\n+\n Transformer Encoder\nFigure 1: Model overview. We split an image into ﬁxed-size patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence of vectors to a standard Transformer\nencoder. In order to perform classiﬁcation, we use the standard approach of adding an extra learnable\n“classiﬁcation token” to the sequence. The illustration of the Transformer encoder was inspired by\nVaswani et al. (2017).\n3.1 V ISION TRANSFORMER (VIT)\nAn overview of the model is depicted in Figure 1. The standard Transformer receives as input a 1D\nsequence of token embeddings. To handle 2D images, we reshape the image x ∈RH×W×C into a\nsequence of ﬂattened 2D patches xp ∈RN×(P2·C), where (H,W ) is the resolution of the original\nimage, Cis the number of channels,(P,P ) is the resolution of each image patch, andN = HW/P2\nis the resulting number of patches, which also serves as the effective input sequence length for the\nTransformer. The Transformer uses constant latent vector size D through all of its layers, so we\nﬂatten the patches and map to Ddimensions with a trainable linear projection (Eq. 1). We refer to\nthe output of this projection as the patch embeddings.\nSimilar to BERT’s[class] token, we prepend a learnable embedding to the sequence of embed-\nded patches (z0\n0 = xclass), whose state at the output of the Transformer encoder ( z0\nL) serves as the\nimage representation y (Eq. 4). Both during pre-training and ﬁne-tuning, a classiﬁcation head is at-\ntached to z0\nL. The classiﬁcation head is implemented by a MLP with one hidden layer at pre-training\ntime and by a single linear layer at ﬁne-tuning time.\nPosition embeddings are added to the patch embeddings to retain positional information. We use\nstandard learnable 1D position embeddings, since we have not observed signiﬁcant performance\ngains from using more advanced 2D-aware position embeddings (Appendix D.3). The resulting\nsequence of embedding vectors serves as input to the encoder.\nThe Transformer encoder (Vaswani et al., 2017) consists of alternating layers of multiheaded self-\nattention (MSA, see Appendix A) and MLP blocks (Eq. 2, 3). Layernorm (LN) is applied before\nevery block, and residual connections after every block (Wang et al., 2019; Baevski & Auli, 2019).\nThe MLP contains two layers with a GELU non-linearity.\nz0 = [xclass; x1\npE; x2\npE; ··· ; xN\np E] +Epos, E ∈R(P2·C)×D, Epos ∈R(N+1)×D (1)\nz′\nℓ = MSA(LN(zℓ−1)) +zℓ−1, ℓ = 1...L (2)\nzℓ = MLP(LN(z′\nℓ)) +z′\nℓ, ℓ = 1...L (3)\ny = LN(z0\nL) (4)\nHybrid Architecture. As an alternative to raw image patches, the input sequence can be formed\nfrom feature maps of a CNN (LeCun et al., 1989). In this hybrid model, the patch embedding\nprojection E (Eq. 1) is applied to patches extracted from a CNN feature map. As a special case,\nthe patches can have spatial size 1x1, which means that the input sequence is obtained by simply\n3\nFig. 4. The architecture of Vision Transformer(ViT).\ncoder into the encoder output. In addition, all sublayers in\nthe encoder and decoder use the remaining connections and\nlayers to improve the scalability of the Transformer. To record\nthe sequential information, each input embedding is attached\nto the start of the encoder and decoder stack with a position\nencoding. Finally, a linear layer and a softmax operation are\nadapted to predict the next word.\nCompared with the traditional RNN, the Transformer model\ncan directly obtain global information. It is one of the advan-\ntages of high computing efficiency, parallel computing, and\nprocessing long text sequences. Therefore, it is widely used\nin NLP. In ChatGPT [55, 56, 57], Transformer technology is\nused to generate text and text classification tasks. Its efficient\ncomputing power and accurate prediction results have been\nverified in practical applications.\n2) Visual Transformer: Transformer has achieved great\nsuccess in NLP. Subsequently, it was extended to computer\nvision and showed good performance in computer visual tasks,\nincluding image recognition, classification, segmentation, and\nso on. It has proven to be a simple and scalable framework.\nCompared with traditional methods, it has obvious training ef-\nficiency advantages. It can use a pure transformer architecture\nor combined with CNN to achieve better results.\nViT. The overall framework of ViT [58] is shown in Figure\n4. First of all, the image is divided into 16 × 16 patches\nand then the flatted patches of the flatted linear mapping.\nThe obtained patch and position encoding is sent to the\nTransformer encoder for encoding. Finally, send the encoded\nfeatures into the MLP head for classification. Among them, the\nTransformer encoder is mainly the position encoder structure\nproposed in Transformer. The appearance of ViT is a prelimi-\nnary attempt by Transformer in computer visual tasks [59]. It\nis convolution-free and highly recognized by researchers for\nits excellent long-distance modeling capabilities.\nSwin Transformer. Swin Transformer [60] adapts the hier-\narchical construction method, similar to convolutional neural\nnetworks. The sizes of the feature maps decreased with the fea-\nture layer deepened. ViT’s feature maps’ sizes are unchanged,\nwith 16 times downsampling. Unlike ViT, its feature maps\nchange four times, eight times, and 16 times downsampling.\nIn detail, its overall framework is figured as Fig. 5 (a). It\ncomprises patch partition, linear embedding, patch merging,\nand SWin Transformer blocks. The overall architecture of two\nImages\nSwin\nTransformer\nBlock\nLinear Embedding\nSwin\nTransformer\nBlock\nPatch Merging\nSwin\nTransformer\nBlock\nPatch Merging\nSwin\nTransformer\nBlock\nPatch Merging\nStage 1 Stage 2 Stage 3 Stage 4\n2 2 6 2\nPatch Partition\nFigure 3. (a) The architecture of a Swin Transformer (Swin-T); (b) two successive Swin Transformer\nEq. (3)). W-MSA and SW-MSA are multi-head self attention modules with regular and shifted windowing conﬁgurations, respectively.\ncedure is repeated twice, as “Stage 3” and “Stage 4”, with\noutput resolutions of H\n16 ×W\n16 and H\n32 ×W\n32 , respectively.\nThese stages jointly produce a hierarchical representation,\nwith the same feature map resolutions as those of typical\nconvolutional networks, e.g., VGG [48] and ResNet [27].\nAs a result, the proposed architecture can conveniently re-\nplace the backbone networks in existing methods for vari-\nous vision tasks.\nSwin Transformer block Swin Transformer is built by\nreplacing the standard multi-head self attention (MSA)\nmodule in a Transformer block by a module based on\nshifted windows (described in Section 3.2), with other lay-\ners kept the same. As illustrated in Figure 3(b), a Swin\nTransformer block consists of a shifted window based MSA\nmodule, followed by a 2-layer MLP with GELU non-\nlinearity in between. A LayerNorm (LN) layer is applied\nbefore each MSA module and each MLP, and a residual\nconnection is applied after each module.\n3.2. Shifted Window based Self-Attention\nThe standard Transformer architecture [58] and its adap-\ntation for image classiﬁcation [19] both conduct global self-\nattention, where the relationships between a token and all\nother tokens are computed. The global computation leads to\nquadratic complexity with respect to the number of tokens,\nmaking it unsuitable for many vision problems requiring an\nimmense set of tokens for dense prediction or to represent a\nhigh-resolution image.\nSelf-attention in non-overlapped windows For efﬁcient\nmodeling, we propose to compute self-attention within lo-\ncal windows. The windows are arranged to evenly partition\nthe image in a non-overlapping manner. Supposing each\nwindow contains M ×M patches, the computational com-\nplexity of a global MSA module and a window based one\non an image of h×wpatches are3:\nΩ(MSA) = 4hwC2 + 2(hw)2C, (1)\nΩ(W-MSA) = 4hwC2 + 2M2hwC, (2)\nwhere the former is quadratic to patch number hw, and the\nlatter is linear when M is ﬁxed (set to 7 by default). Global\nself-attention computation is generally unaffordable for a\nlarge hw, while the window based self-attention is scalable.\nShifted window partitioning in successive blocks The\nwindow-based self-attention module lacks connections\nacross windows, which limits its modeling power. To intro-\nduce cross-window connections while maintaining the efﬁ-\ncient computation of non-overlapping windows, we propose\na shifted window partitioning approach which alternates be-\ntween two partitioning conﬁgurations in consecutive Swin\nTransformer blocks.\nAs illustrated in Figure 2, the ﬁrst module uses a regular\nwindow partitioning strategy which starts from the top-left\npixel, and the 8 ×8 feature map is evenly partitioned into\n2 ×2 windows of size 4 ×4 (M = 4). Then, the next mod-\nule adopts a windowing conﬁguration that is shifted from\nthat of the preceding layer, by displacing the windows by\n(⌊M\n2 ⌋,⌊M\n2 ⌋) pixels from the regularly partitioned windows.\nWith the shifted window partitioning approach, consec-\nutive Swin Transformer blocks are computed as\nˆzl = W-MSA\n(\nLN\n(\nzl−1))\n+ zl−1,\nzl = MLP\n(\nLN\n(ˆzl))\n+ ˆzl,\nˆzl+1 = SW-MSA\n(\nLN\n(\nzl))\n+ zl,\nzl+1 = MLP\n(\nLN\n(ˆzl+1))\n+ ˆzl+1, (3)\nwhere ˆzl and zl denote the output features of the (S)W-\nMSA module and the MLP module for blockl, respectively;\n3We omit SoftMax computation in determining complexity.\n4\n10015\nFig. 5. The architecture of Swin Transformer.\nBottleneck Transformers for Visual Recognition\nAra\nvind Srinivas 1 Tsung-Yi Lin 2 Niki Parmar 2 Jonathon Shlens 2 Pieter Abbeel 1 Ashish Vaswani2\n1UC Berkeley 2Google Research\n{aravind}@cs.berkeley.edu\nAbstract\nWe present BoTNet, a conceptually simple yet powerful\nbackbone architecture that incorporates self-attention for\nmultiple computer vision tasks including image classiﬁca-\ntion, object detection and instance segmentation. By just\nreplacing the spatial convolutions with global self-attention\nin the ﬁnal three bottleneck blocks of a ResNet and no other\nchanges, our approach improves upon the baselines signiﬁ-\ncantly on instance segmentation and object detection while\nalso reducing the parameters, with minimal overhead in la-\ntency. Through the design of BoTNet, we also point out how\nResNet bottleneck blocks with self-attention can be viewed as\nTransformer blocks. Without any bells and whistles, BoTNet\nachieves 44.4% Mask AP and 49.7% Box AP on the COCO\nInstance Segmentation benchmark using the Mask R-CNN\nframework; surpassing the previous best published single\nmodel and single scale results of ResNeSt [\n67] evaluated\non the COCO validation set. Finally, we present a simple\nadaptation of the BoTNet design for image classiﬁcation,\nresulting in models that achieve a strong performance of\n84.7% top-1 accuracy on the ImageNet benchmark while\nbeing up to 1.64x faster in “compute” 1 time than the popu-\nlar EfﬁcientNet models on TPU-v3 hardware. We hope our\nsimple and effective approach will serve as a strong baseline\nfor future research in self-attention models for vision. 2\n1. Introduction\nDeep convolutional backbone architectures [ 37, 54, 28,\n66, 56] have enabled signiﬁcant progress in image classiﬁ-\ncation [ 52], object detection [ 17, 40, 21, 20, 50], instance\nsegmentation [ 25, 13, 27]. Most landmark backbone archi-\ntectures [ 37, 54, 28] use multiple layers of 3×3 convolutions.\nWhile the convolution operation can effectively capture\nlocal information, vision tasks such as object detection, in-\nstance segmentation, keypoint detection require modeling\nlong range dependencies. For example, in instance segmen-\n1F orward and backward propagation for batch size 32\n2Please refer to https://arxiv.org/abs/2101.11605 for a\nlonger version.\nFigure 1: Left: A ResNet Bottleneck Block, Right: A Bot-\ntleneck Transformer (BoT) block. The only difference is\nthe replacement of the spatial 3 × 3 convolution layer with\nMulti-Head Self-Attention (MHSA). The structure of the\nself-attention layer is described in Figure 4.\ntation, being able to collect and associate scene information\nfrom a large neighborhood can be useful in learning relation-\nships across objects [ 32]. In order to globally aggregate the\nlocally captured ﬁlter responses, convolution based archi-\ntectures require stacking multiple layers [ 54, 28]. Although\nstacking more layers indeed improves the performance of\nthese backbones [ 67], an explicit mechanism to model global\n(non-local) dependencies could be a more powerful and scal-\nable solution without requiring as many layers.\nModeling long-range dependencies is critical to natural\nlanguage processing (NLP) tasks as well. Self-attention\nis a computational primitive [ 61] that implements pairwise\nentity interactions with a content-based addressing mecha-\nnism, thereby learning a rich hierarchy of associative features\nacross long sequences. This has now become a standard tool\nin the form of Transformer [ 61] blocks in NLP with promi-\nnent examples being GPT [ 46, 5] and BERT [ 14, 42] models.\nA simple approach to using self-attention in vision is to\nreplace spatial convolutional layers with the multi-head self-\nattention (MHSA) layer proposed in the Transformer [ 61]\n(Figure 1). This approach has seen progress on two seem-\ningly\ndifferent approaches in the recent past. On the one\nhand, we have models such as SASA [ 49], AACN [ 4],\n16519\n（a） （b)\nFig. 6. Compare architecture of ResNet Bottleneck and Bottleneck Trans-\nformer.\nsuccessive Swin Transformer blocks is shown as Fig. 5 (b).\nThe hierarchical characteristics of the Swin Transformer have\nan essential role in visual recognition.\nBotNet. BotNet [61] is a simple but effective backbone\nfor visual representation. It introduces self-attention to many\nvisual tasks, including image classification, object detection,\nand instance segmentation. BotNet consists of Bottleneck\nTransformer blocks. In detail, its framework is shown in Fig.\n6. For 2048-dimension input, the ResNet bottleneck contains\nthe convolutional operations, including 1×1×512, 3×3×512,\nand 1 × 1 × 2048. The skipping connection of ResNet is\nstill maintained. Compared with the ResNet bottleneck, The\nBottleneck Transformer only replaces the original second\nconvolutional operation with multi-head attention (MHSA)\n(shown as Fig. 6). In the Bottleneck Transformer, MHSA is the\ncentral core novelty. It enables the model to capture different\ncharacteristics and modes in the input data. In addition, BotNet\nreplaces the spatial convolutions in the last three Bottleneck\nblocks of ResNet with global self-attention. It has significantly\nimproved the baseline regarding instance segmentation and\ntarget detection and reduces the parameters to minimize delay.\n3) Advantages and Disadvantages: NLP was a latecomer\nin the past 10 years of a deep learning revolution. Anna\nRumshisky, a computer scientist at the University of Mas-\nsachusetts, said that NLP [62, 63] was behind the computer\nin a sense. Vision Transformer breaks the restrictions of\nincompetence computing in the RNN model. Note that the\nmechanism provides context information for any location in\nthe input sequence. It is of the advantages of parallelism,\nunlimited positioning operations, strong global characteristics,\nstrong versatility, and strong scalability so that the generative\npre-trained (GPT) model [64, 65] has excellent performance.\nSpecifically, the advantages are listed as follows.\n1) Design innovation. It abandoned the most fundamental\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n6\nRNN or CNN in NLP and achieved excellent results. The\ndesign of it is very inspiring and worthy of in-depth research.\n2) The key to Transformer’s design is that the distance\nbetween any two words is 1, which is effective for solving\nthe difficult long-term dependencies in NLP.\n3) Transformer can not only be applied in machine transla-\ntion in NLP but is not even limited to the NLP field. It is a\ndirection of very scientific research potential.\n4) The parallelism of the algorithm is good, which is in line\nwith the current hardware environment.\nOf course, its model still has some limitations such as\nfollows.\n1) Although the rough abandonment of RNN and CNN is\nvery dazzling, it also causes the model to lose the ability to\ncapture local characteristics. The combination of RNN, CNN,\nand Transformer may bring better results [66].\n2) The lost location information that Transformer is impor-\ntant in NLP, and adding position embedding to the feature\nvector is just a suitable measure, and it does not change the\ninherent defects in the Transformer structure.\n3) Though Transformer helps integrate and improve artificial\nintelligence (AI) tools [67]. As with other emerging technolo-\ngies, Transformer also has expensive costs. A Transformer\nmodel requires a lot of computing power during the pre-\ntraining stage to defeat the previous competitors [68, 47].\n4) From the perspective of the Transformer, there are\nproblems of large memory occupation and high delay in\narchitecture based on the Transformer, which hinders their\nefficient deployment and reasoning. Recently, many studies\nhave improved computing and memory efficiency around\nthe original Transformer architecture, but most of them are\nconcentrated in the semi-supervised field [25, 69].\nB. Self-supervised Learning\nSelf-supervised learning (SSL) takes a crucial role in train-\ning foundation models. Many state-of-the-art foundation mod-\nels utilize SSL in the pre-training phase. This pre-training\nphase allows the foundation model to acquire rich features\nand representations, and then use the labeled data to fine-tune\nfor specific downstream tasks. SSL is a form of unsupervised\nlearning that aims to extract useful and generalizable feature\nrepresentations from a large amount of unlabeled data for\ndownstream tasks [70, 71, 72]. Referred to as the ”dark\nmatter” of intelligence, self-supervised learning differs from\nsupervised learning, which is constrained by the availability\nof labeled data. Instead, self-supervised methods leverage a\n”semi-automatic” process to obtain ”labels” directly from the\ndata itself, saving significant manpower and time costs [71].\nIn recent years, SSL has achieved remarkable success in\nthe field of deep learning, particularly in natural language\nprocessing, with the emergence of influential language models\nsuch as BERT [4] and GPT-3 [73]. In computer vision, models\nlike MAE [1] and DINOv2 [74] have been able to match\nor even surpass supervised models in certain scenarios. The\ngeneral workflow of SSL in computer vision is illustrated\nin Fig. 7. SSL defines a pretext task based on unlabeled\ninputs to generate descriptive and interpretable representations\nRepresentation\nPretext\nTask\nDownstream\nTask\nRepresentation\nUnlabeled-Data\nPre-train\nTransfer\nLabeled-Data\nFig. 7. General approach of SSL. Firstly, an auxiliary task is trained using an\nunlabeled dataset to apply the SSL scheme. Subsequently, the learned network\nweights are transferred from the pretext task to the downstream task, enabling\ntraining on a small amount of data with labels.\n SSL\n Predictive\n Spatial\n CFN \n RotNet \n Generative\n GAN\n GAN\n BigGAN\n SRGAN\n Autoencoder\n VAE\n MAE\n Contrastive\n Negative sampling\n SimCLR\n MoCo\n Clustering\n DeepCluster\n SwAV\n Knowledge distillation\n BYOL\n DINO\n Redundancy reduction  Barlow Twins\n Spectral  Colorful Image Colorization\nFig. 8. Summary of popular self-supervision methods and typical models.\n[75, 70, 72, 76]. The pretext task is a pre-designed task in\nthe pre-training phase, where the objective function is learned\nby inputting unlabeled data. Typically, pretext tasks can be\nprediction-based, context-based, or generation-based, and the\nsupervision signal is generated from the data itself [75]. After\ntraining on the pretext task, the learned representations are\ntransferred as initial weights to downstream tasks to achieve\ntheir intended objectives.\n1) SSL for natural images: Based on different pretext task\napproaches, three different SSL methods can be identified:\ngenerative, contrastive, and predictive [70, 77], as shown in\nFig. 8.\nGenerative methods: Generative methods aim to learn\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n7\nMasked Autoencoders Are Scalable Vision Learners\nKaiming He∗,† Xinlei Chen∗ Saining Xie Yanghao Li Piotr Doll ´ar Ross Girshick\n∗equal technical contribution †project lead\nFacebook AI Research (FAIR)\nAbstract\nThis paper shows that masked autoencoders (MAE) are\nscalable self-supervised learners for computer vision. Our\nMAE approach is simple: we mask random patches of the\ninput image and reconstruct the missing pixels. It is based\non two core designs. First, we develop an asymmetric\nencoder-decoder architecture, with an encoder that oper-\nates only on the visible subset of patches (without mask to-\nkens), along with a lightweight decoder that reconstructs\nthe original image from the latent representation and mask\ntokens. Second, we ﬁnd that masking a high proportion\nof the input image, e.g., 75%, yields a nontrivial and\nmeaningful self-supervisory task. Coupling these two de-\nsigns enables us to train large models efﬁciently and ef-\nfectively: we accelerate training (by 3 ×or more) and im-\nprove accuracy. Our scalable approach allows for learning\nhigh-capacity models that generalize well: e.g., a vanilla\nViT-Huge model achieves the best accuracy (87.8%) among\nmethods that use only ImageNet-1K data. Transfer per-\nformance in downstream tasks outperforms supervised pre-\ntraining and shows promising scaling behavior.\n1. Introduction\nDeep learning has witnessed an explosion of archi-\ntectures of continuously growing capability and capacity\n[33, 25, 57]. Aided by the rapid gains in hardware, mod-\nels today can easily overﬁt one million images [13] and\nbegin to demand hundreds of millions of—often publicly\ninaccessible— labeled images [16].\nThis appetite for data has been successfully addressed in\nnatural language processing (NLP) by self-supervised pre-\ntraining. The solutions, based on autoregressive language\nmodeling in GPT [47, 48, 4] and masked autoencoding in\nBERT [14], are conceptually simple: they remove a portion\nof the data and learn to predict the removed content. These\nmethods now enable training of generalizable NLP models\ncontaining over one hundred billion parameters [4].\nThe idea of masked autoencoders, a form of more gen-\neral denoising autoencoders [58], is natural and applicable\nin computer vision as well. Indeed, closely related research\nencoder\n....\n....\ndecoder\ninput target\nFigure 1. Our MAE architecture . During pre-training, a large\nrandom subset of image patches ( e.g., 75%) is masked out. The\nencoder is applied to the small subset of visible patches. Mask\ntokens are introduced after the encoder, and the full set of en-\ncoded patches and mask tokens is processed by a small decoder\nthat reconstructs the original image in pixels. After pre-training,\nthe decoder is discarded and the encoder is applied to uncorrupted\nimages (full sets of patches) for recognition tasks.\nin vision [59, 46] preceded BERT. However, despite signif-\nicant interest in this idea following the success of BERT,\nprogress of autoencoding methods in vision lags behind\nNLP. We ask: what makes masked autoencoding different\nbetween vision and language ? We attempt to answer this\nquestion from the following perspectives:\n(i) Until recently, architectures were different. In vision,\nconvolutional networks [34] were dominant over the last\ndecade [33]. Convolutions typically operate on regular grids\nand it is not straightforward to integrate ‘indicators’ such as\nmask tokens [14] or positional embeddings [57] into con-\nvolutional networks. This architectural gap, however, has\nbeen addressed with the introduction of Vision Transform-\ners (ViT) [16] and should no longer present an obstacle.\n(ii) Information density is different between language\nand vision. Languages are human-generated signals that\nare highly semantic and information-dense. When training\na model to predict only a few missing words per sentence,\nthis task appears to induce sophisticated language under-\nstanding. Images, on the contrary, are natural signals with\nheavy spatial redundancy— e.g., a missing patch can be re-\ncovered from neighboring patches with little high-level un-\n1\narXiv:2111.06377v3  [cs.CV]  19 Dec 2021\nFig. 9. The training procedure of the MAE. (Image from [1].)\nrepresentations by reconstructing or generating input data. The\nbasic idea is to model the underlying data distribution to\ncapture the statistical properties and dependencies of the input\ndata. Generative methods can implicitly capture meaningful\nfeatures and structures in the data without relying on explicit\nlabels. These methods often utilize generative models such as\nautoencoders and generative adversarial networks (GANs) [78]\nto perform reconstruction or generation tasks.\nAn autoencoder consists of an encoder network that maps\ninput data to a latent space representation and a decoder\nnetwork that reconstructs the data from the latent space.\nBased on the autoencoder, several variant methods have\nemerged. For example, Variational Autoencoder (V AE) [79]\ncombines the encoder-decoder structure of an autoencoder\nwith probabilistic modeling. It assumes the existence of a\nprior distribution P(Z) over the latent space and models the\nconditional distribution P (X|Z). The encoder approximates\nthe posterior distribution P (Z|X) by inferring the model\nQ(Z|X). Recently, generative methods have been commonly\nused for information recovery tasks, such as inpainting, where\na portion of an image is removed, and the network’s context\nencoder is trained to restore the missing pixel values based on\nthe surrounding context [80]. This idea has evolved into the\nMasked Autoencoders (MAE) task [1], where random masks\nare applied to input image patches, and the missing pixels are\nsubsequently reconstructed (as shown in Fig. 9). In order to\ncorrectly reconstruct each pixel, the model needs to understand\nthe different objects and relevant components present in the\nimage. Therefore, the learned feature representations are useful\nfor other downstream tasks.\nGAN [78] consists of two networks: a generator G : ZX\nand a discriminator D : X[0,1]. The generator synthesizes\nfake samples from random noise in the latent space, while\nthe discriminator attempts to distinguish between real and\nfake samples. These methods are primarily used for image\ngeneration and image super-resolution tasks. For example,\nBigGAN [81] introduces innovations such as a large-scale\nGAN architecture, conditional batch normalization, category\nconditioning, and orthogonal regularization. These advance-\nments enable BigGAN to generate high-quality, diverse, and\ncategory-specific images. SRGAN [82] is capable of recov-\nering high-resolution textures in super-resolution tasks by\nlearning from a large set of downsampled images.\nContrastive learning methods: Contrastive learning meth-\nods aim to learn representations by maximizing the similarity\nbetween semantically related samples and minimizing the sim-\nilarity between unrelated samples. The key idea of contrastive\nlearning is to create a contrastive objective function that\nencourages the model to bring similar samples closer together\nin the feature space and separate dissimilar samples. Classical\ncontrastive learning methods can be categorized into four\ntypes: negative sampling, clustering, knowledge distillation,\nand redundancy reduction.\nThe Negative sampling method involves creating a set of\nnegative samples dissimilar to the anchor samples. The objec-\ntive is to encourage the model to differentiate and separate pos-\nitive and negative examples in the learned representation space,\nthereby learning valuable feature representations and avoiding\nmodel collapse. In SSL with negative sampling, models like\nSimCLR [83] and the MoCo series are typical approaches.\nSimCLR uses a convolutional neural network as a feature\nextractor and employs various data augmentation strategies to\nincrease data diversity. As shown in Fig. 10, the MoCo model\n[84] uses a momentum-based update strategy to update model\nparameters and performs contrastive learning by comparing a\nset of samples with a larger queue of negative samples. MoCo\nand its variants, such as MoCov2 [85] and MoCov3 [86],\nhave demonstrated strong performance in SSL tasks, proving\nthe effectiveness of negative sampling in training robust and\ndiscriminative representations.\nThe Clustering approach utilizes clustering algorithms to\ngroup similar samples in an unsupervised manner. One repre-\nsentative method is DeepCluster, which first clusters images\ninto different clusters and then trains a convolutional neural\nnetwork to recognize assignments [87]. Another example is the\nSwA V [88] model, which introduces the concept of grouping\nsimilar representations into a cluster. It generates multiple\nviews of an image using data augmentation and then groups\nthese views into clusters based on their similarity. SwA V\nencourages similar representations within each cluster and\nmakes representations from different clusters dissimilar.\nThe Knowledge distillation method refers to the process\nof transferring knowledge from a teacher model to a student\nmodel. The teacher model is usually a pretrained model with\nhigh performance, while the student model is trained using\nSSL. In SSL based on Knowledge distillation, several notable\nmodels have been proposed. One classic method is BYOL [23]\nand the DINO (DINO [25], DINOV2 [74]) series. BYOL fo-\ncuses on predicting the representation of one augmented view\nfrom another view. It uses an online network as the student\nnetwork and a target network as the teacher network, updating\nthe target network using a momentum-based update strategy.\nThe DINO model combines the Transformer architecture and\nlearns representations by maximizing consistency between two\nviews of the same image, where one view is used as a query\nand the other view is used as a key to predict the output of\nthe teacher network by the student network. Building upon\nDINO, DINOv2 further incorporates a clustering objective\nto encourage samples that are semantically similar to cluster\ntogether in the learned representation. This helps the model\ncapture finer-grained and meaningful structures in the learned\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n8\nMomentum Contrast for Unsupervised Visual Representation Learning\nKaiming He Haoqi Fan Yuxin Wu Saining Xie Ross Girshick\nFacebook AI Research (FAIR)\nAbstract\nWe present Momentum Contrast (MoCo) for unsuper-\nvised visual representation learning. From a perspective on\ncontrastive learning [29] as dictionary look-up, we build\na dynamic dictionary with a queue and a moving-averaged\nencoder. This enables building a large and consistent dic-\ntionary on-the-ﬂy that facilitates contrastive unsupervised\nlearning. MoCo provides competitive results under the\ncommon linear protocol on ImageNet classiﬁcation. More\nimportantly, the representations learned by MoCo transfer\nwell to downstream tasks. MoCo can outperform its super-\nvised pre-training counterpart in 7 detection/segmentation\ntasks on PASCAL VOC, COCO, and other datasets, some-\ntimes surpassing it by large margins. This suggests that\nthe gap between unsupervised and supervised representa-\ntion learning has been largely closed in many vision tasks.\nCode: https://github.com/facebookresearch/moco\n1. Introduction\nUnsupervised representation learning is highly success-\nful in natural language processing, e.g., as shown by GPT\n[50, 51] and BERT [12]. But supervised pre-training is still\ndominant in computer vision, where unsupervised meth-\nods generally lag behind. The reason may stem from dif-\nferences in their respective signal spaces. Language tasks\nhave discrete signal spaces (words, sub-word units, etc.)\nfor building tokenized dictionaries, on which unsupervised\nlearning can be based. Computer vision, in contrast, further\nconcerns dictionary building [54, 9, 5], as the raw signal is\nin a continuous, high-dimensional space and is not struc-\ntured for human communication (e.g., unlike words).\nSeveral recent studies [61, 46, 36, 66, 35, 56, 2] present\npromising results on unsupervised visual representation\nlearning using approaches related to the contrastive loss\n[29]. Though driven by various motivations, these methods\ncan be thought of as building dynamic dictionaries. The\n“keys” (tokens) in the dictionary are sampled from data\n(e.g., images or patches) and are represented by an encoder\nnetwork. Unsupervised learning trains encoders to perform\ndictionary look-up: an encoded “query” should be similar\nto its matching key and dissimilar to others. Learning is\nformulated as minimizing a contrastive loss [29].\nencoder momentum\nencoder\nq\ncontrastive loss\nsimilarity\nqueue\nk0 k1 k2 ...\nx query x key\n0 x key\n1 x key\n2 ...\nFigure 1. Momentum Contrast (MoCo) trains a visual represen-\ntation encoder by matching an encoded query q to a dictionary\nof encoded keys using a contrastive loss. The dictionary keys\n{k0,k1,k2,...}are deﬁned on-the-ﬂy by a set of data samples.\nThe dictionary is built as a queue, with the current mini-batch en-\nqueued and the oldest mini-batch dequeued, decoupling it from\nthe mini-batch size. The keys are encoded by a slowly progressing\nencoder, driven by a momentum update with the query encoder.\nThis method enables a large and consistent dictionary for learning\nvisual representations.\nFrom this perspective, we hypothesize that it is desirable\nto build dictionaries that are: (i) large and (ii) consistent\nas they evolve during training. Intuitively, a larger dictio-\nnary may better sample the underlying continuous, high-\ndimensional visual space, while the keys in the dictionary\nshould be represented by the same or similar encoder so that\ntheir comparisons to the query are consistent. However, ex-\nisting methods that use contrastive losses can be limited in\none of these two aspects (discussed later in context).\nWe present Momentum Contrast (MoCo) as a way of\nbuilding large and consistent dictionaries for unsupervised\nlearning with a contrastive loss (Figure 1). We maintain the\ndictionary as a queue of data samples: the encoded repre-\nsentations of the current mini-batch are enqueued, and the\noldest are dequeued. The queue decouples the dictionary\nsize from the mini-batch size, allowing it to be large. More-\nover, as the dictionary keys come from the preceding sev-\neral mini-batches, a slowly progressing key encoder, imple-\nmented as a momentum-based moving average of the query\nencoder, is proposed to maintain consistency.\n1\narXiv:1911.05722v3  [cs.CV]  23 Mar 2020\nFig. 10. The training procedure of the MoCo. (Image from [84].)\nrepresentation.\nRedundancy reduction refers to the process of reducing\nredundant information in learned representations. The goal is\nto ensure that the representations capture the essential and\ndiscriminative features of the data. For example, the Barlow\nTwins model [89] maximizes the cross-correlation matrix of\nthe representations while minimizing its diagonal elements.\nThis encourages the representations to capture statistical de-\npendencies between different parts of the input data while\nreducing redundancy. By maximizing cross-correlation, the\nmodel learns to encode useful information across different\nviews.\nPredictive methods: Predictive methods aim to learn useful\nrepresentations by training models to perform prediction tasks.\nPopular image prediction tasks involve methods based on both\nspatial and spectral aspects of the image. In the spatial-based\napproach, the image prediction task involves predicting the\nrelative positions of two patches from the same image or\nidentifying the random order of a sequence of patches from the\nsame image. The former trains convolutional neural networks\nto predict the relative positions of two randomly sampled\npatches in an image. The latter constructs image puzzles\nby decomposing the image into a series of non-overlapping\npatches and predicts the relative positions of each patch to\nreconstruct the image. Image puzzle tasks require learning how\nparts are assembled within an object, the relative positions of\ndifferent parts, and the shape of the object. Therefore, these\nrepresentations are useful for downstream classification and\ndetection tasks. For example, Noroozi et al. proposed CFN\n[90], where a neural network is trained to solve the Jigsaw\npuzzle, learning both feature mappings of object parts and\ntheir correct spatial arrangements. Geometric transformation\nrecognition tasks are used to identify the rotation angle of\nthe entire image. This task requires the network to learn to\nlocate salient objects in the image, recognize their orientations\nand object types, and then associate object orientations with\nthe original image. RotNet [91], proposed by Gidaris et al.,\nperforms unsupervised representation learning by predicting\nimage rotations. Counting tasks aim to train models to count\nvisual primitives in an image and learn the representation of\nthe image by outputting the number of objects in the image,\neffectively learning spatial and object information in the\nimage. Whereas spectral-based methods aim to automatically\nadd realistic colors to grayscale images, which is referred to\nas the image coloring task (IC). For example, CNN is used\nfor IC followed by classification, detection, and segmentation\ndownstream task validation in [92].\n2) SSL for remote sensing images: SSL in remote sensing\nis a method that utilizes unlabeled information in remote\nsensing data to learn useful representations. In the field of\nremote sensing, SSL has been widely applied to multispectral\nimagery, hyperspectral imagery, and synthetic aperture radar\n(SAR) imagery. Similar to the domain of natural images,\nremote sensing SSL can also be categorized into three different\napproaches: generative, contrastive, and predictive.\nGenerative methods: Generative methods in remote sens-\ning imagery often rely on techniques such as autoencoders\nand generative adversarial networks (GANs). However, gener-\native methods in remote sensing involve additional tasks. For\ninstance, tasks like Urban Flood Mapping and Hyperspectral\nUnmixing are included. Specifically, Peng et al. [93] proposed\nthe SSL framework for patch-based urban flood mapping using\nmulti-temporal multispectral satellite imagery. They utilized\npatch-level change vector analysis with features learned by\na self-supervised autoencoder to generate patch-level change\nmaps highlighting potential flood-affected areas. Jin et al.\n[94] introduced AAENet, a novel technique network for un-\nsupervised hyperspectral unmixing. The proposed approach\nimproved model performance and robustness by incorporating\nuntied-weighted autoencoder, discrimination network, adver-\nsarial processes, and adding abundance priors to the frame-\nwork.\nContrastive methods: In contrastive methods, Negative\nsampling remains a popular approach. For example, Hou\net al. [95] proposed a contrastive learning-based algorithm\nfor hyperspectral image classification, which consists of a\npretraining phase and a fine-tuning phase. In the first phase,\nthe model is pretrained by constructing positive and negative\nsample pairs to learn to discriminate between them. In the\nsecond phase, based on the pretrained model, features are\nextracted from hyperspectral images for classification, and a\nsmall number of labeled samples are used for fine-tuning the\nfeatures. Similarly, [96] adopts a two-stage approach where\nthe model is trained to predict whether two image patches\ncome from the same image. Swin Transformer is combined\nwith SSL for land cover classification and segmentation. In\nclustering-based methods, Contrastive Learning-Based Dual\nDynamic GCN for SAR Image Scene Classification proposes a\nclustering-based contrastive learning approach using Dual Dy-\nnamic GCN (Graph Convolutional Network) for SAR image\nscene classification. The proposed clustering-based contrastive\nSSL model is used to transform SAR images into a higher-\nlevel embedding space as richer representations without any\nlabels, aiding subsequent node representations and information\npropagation in GCN.Knowledge distillation methods are often\ninspired by models in the natural image domain. For instance,\nMuhtar et al. [97] proposed a method called IndexNet for\nsemantic segmentation. The proposed IndexNet consists of\ntwo branches: the Index Contrastive Branch and the Instance\nContrastive Branch. The Index Contrastive Branch learns\npixel-level representations by tracking object positions and\nmaintaining sensitivity to changes in object positions to ensure\nconsistency. The Instance Contrastive Branch follows the stan-\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n9\nPretrained \nWeights\nA\nB\nX\nH\nW\nMulti-Head \nAttention\nAdd&Norm\nFeed \nForward\nAdd&Norm\nAdaptor\nAdd&Norm\n(a) (b)\nFig. 11. The illustration of the Parameter-effcient tuning algorithm including\nadapter and LoRA. (a) A simple example of the adapter with Transformer\nblock. (b) The illustration of the LoRA algorithm.\ndard BYOL learning process, learning image-level represen-\ntations by combining image-level and pixel-level contrastive\nlearning to capture spatiotemporal invariant features.\nPredictive methods: remote sensing prediction methods\ncan also utilize other downstream tasks by constructing a\npretext task based on rotation prediction. For example, Ji\net al. [98] used rotation prediction to identify the input’s\ntwo-dimensional rotation to guide the learning of transfer-\nable knowledge across categories. They combined contrastive\nlearning to bring positive sample pairs closer and push away\nnegative sample pairs, promoting intra-class consistency and\ninter-class inconsistency. These pretraining tasks are jointly\noptimized in an end-to-end manner with semantic category\nprediction tasks, ultimately achieving remote sensing image\nscene classification. Additionally, in [99], IC is used as a\npretexting task to learn feature representations, which are\nthen transferred to a U-Net model for predicting the semantic\nsegmentation of remote sense urban scenes.\nC. Parameter-effcient Tuning for FMs\nFine-tuning is a crucial method for applying pre-trained\nmodels to downstream tasks. However, it involves updating\nparameters for both the entire model and each task model.\nFine-tuning a large FM poses significant challenges in terms\nof computing resources and storage. To address this, the\ntechnology of parameter efficient fine-tuning (PEFT) has been\nexplored and implemented. The primary objective of PEFT\nis to enhance the performance of pre-trained models on new\ntasks by minimizing the number of fine-tuning parameters and\nreducing computational complexity. This, in turn, mitigates the\ntraining cost associated with large pre-trained models. In most\ncases, PEFT only requires the addition or updating of a small\nnumber of parameters in the model to facilitate its application\non downstream tasks. Remarkably, these techniques achieve\ncomparable accuracy compared to full fine-tuning. In this\nsection, we introduce various parameter-efficient fine-tuning\nstrategies, such as Prompt Tuning, Adapter Tuning, and LoRA.\nFor more parameter-efficient tuning strategies, interested read-\ners can refer to the literature [100].\n1) Prompt Tuning [101]: Prompting refers to construct\na language instruction to the input text of LLM so that\nthe LLM can solve the downstream tasks without finetune\nthe whole models [73]. To construct better prompting texts,\nPrompt Tuning treats the prompts as task-specific continuous\nvectors and directly optimize them via gradients during fine-\ntuning [101]. Prompt Tuning only adds trainable vectors to\nthe input embedding layer, initializing them with text. This\napproach allows fine-tuning with smaller learning parameters\nand offers higher computational efficiency. Despite fine-tuning\nfewer model parameters, Prompt Tuning achieves accuracy\ncomparable to full fine-tuning.\nPrefix Tuning [102] is a kind of Prompt Tuning method.\nIt uses a series of “virtual tokens” to create a prefix of\ntokens, providing “implicit” hints to the model. As a result,\nthe parameters of the large language model remain frozen,\nand for each layer’s input, a set of continuous task-related\nprefix tokens is learned to prompt the model. To ensure stable\ntraining, an MLP layer is added to reparameterize these prefix\ntokens. Once training is complete, only the parameters of the\nprefix layer need to be saved to obtain a fine-tuned model for\na specific task.\nInspired by the Prompt Tuning, some researches try to\napply these efficient tuning methods to the visual domain.\nVisual prompt tuning (VPT) [103] prepends a set of learnable\nparameters to the pre-trained vision transformer and conducts\nexperiments on a wide variety of downstream recognition\ntasks. It shows that VPT achieves significant performance\ngains compared to other parameter efficient tuning protocols.\nMulti-modal Prompt Learning (MaPLe) [104] learn prompts\non both text and vision branches to ensure mutual synergy. In\naddition, branch-aware hierarchical prompts are also designed\nto progressively model the stage-wise feature relationships. Oh\net al. [105] proposed the black-box visual prompting, which\nefficiently adapts the large-scale pre-trained models without\nknowledge about model architectures and parameters.\n2) Adapter Tuning [106, 107]: Adapter Tuning is a\nmore general fine-tuning strategy designed for large models.\nIt involves adding an adapter module within each layer or\nbetween layers while keeping the main parameters of the pre-\ntrained model fixed, as shown in Fig. 11 (a). During the fine-\ntuning process, only the parameters in the adapter are trained\nto adapt to downstream tasks, reducing the computational\noverhead of training. The advantage of Adapter Tuning lies\nin its ability to retain the model’s general knowledge while\nlearning specific knowledge for downstream tasks, avoiding\ncatastrophic forgetting and task interference. This approach is\nnow widely used in various applications.\n3) LoRA [108]: Low-Rank Adaptation (LoRA) is a well-\nknown technology for fine-tuning the model. Fine-tuning\ntypically involves modifying all parameters in the model, but\ntoday’s large language models with Billion-scale parameters\nmake conventional fine-tuning strategies computationally ex-\npensive. Previous research has shown that neural networks are\nover-parameterized, and their parameters can be represented\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n10\nseparately using low-rank representation. Unlike fine-tuning\nall parameters, the LoRA algorithm suggests that we can learn\na low-rank weight residual parameter, denoted as ∆W, to\nachieve the fine-tuning effect.\nAs shown in Fig. 11 (b), for a weight W, rather than\nadjusting all parameters fully, we only need to learn the\nresidual ∆W of this parameter. Moreover, this residual can\nbe decomposed into two low-rank matrices, ∆W = AB. As a\nresult, we only need to fine-tune matrices A and B to achieve\nthe fine-tuning effect, and the performance after fine-tuning is\nequivalent to that of full fine-tuning.\nDue to its ease of use and effectiveness, LoRA has prompted\nthe development of various methods to improve it, enabling\nfine-tuning models with more parameters, even on smaller\ncomputers. For example, AdaLoRA [109] considers the im-\nportance of parameters, adaptively allocates the budget for pa-\nrameter optimization, and parameterizes incremental updates\nin the form of singular value decomposition. QLoRA [110] en-\nhances Vallian LoRA from a quantitative perspective. QLoRA\nbackpropagates gradients through a frozen, 4-bit quantized\npretrained language model into Low-Rank Adapters (LoRA).\nThis enables fine-tuning of 65B parameter models on a single\n48G GPU while maintaining comparable performance to 16-\nbit fine-tuned models.\nIII. T HE DEVELOPMENT OF FOUNDATION MODELS\nA. Language Foundation Model\nLanguage foundation models, also known as Large Lan-\nguage Models, have gained significant attention in recent\nyears. These models utilize a vast amount of text for unsu-\npervised training and excel in text representation and under-\nstanding. Some of the notable language foundation models are\nsummarized in Table II.\nOne of the pioneering models is Deep Bidirectional Trans-\nformers (BERT) [4], which achieves deep representation pre-\ntraining by joint conditioning on unlabeled samples in a\nbidirectional manner. BERT achieves state-of-the-art results on\n7 NLP tasks. Raffel et al. [111] proposed a method to convert\nall text-based language problems into a text-to-text format and\ntrained a T5 model with 11B parameters. Subsequently, the\nwell-known Generative Pre-trained Transformer (GPT) series\nmodels were introduced. GPT-3 [73], in particular, demon-\nstrated the potential of large-scale parametric language models\nand inspired a wide range of applications. For instance, We-\nbGPT [112] implemented question answering in a web browser\nenvironment, while Codex [113] performed fine-tuning on the\nbasis of GPT to enable the model to master Python code-\nwriting capabilities. Following these developments, several\nlanguage foundation models with large-scale parameters were\nproposed [114, 115, 116, 117, 118], showcasing impressive\nperformance in language translation, summarization, question\nanswering, and text completion. These models have displayed\nunprecedented capabilities [118].\nMoreover, advancements like InstructGPT [119] have fur-\nther improved the control and flexibility of large language\nmodels, ensuring the logic and values of the models align with\nhuman understanding. This has opened up new possibilities for\nutilizing language foundation models in a more human-like\nmanner.\nB. Vision Foundation Model\nDrawing inspiration from the construction of language foun-\ndation models, the field of computer vision has also delved into\nlarge-parameter foundation models. Table II summarizes some\nrepresentative vision foundation models.\nThe exploration of vision foundation models can be cat-\negorized into three main aspects: training methods, param-\neter number, and tasks. BYOL [23] enables self-supervised\nlearning by interacting between two networks. SimCLR [24]\nproposes a semi-supervised learning method that combines\nunsupervised, distillation, and few-shot supervision to enhance\nthe model’s capabilities. SimMIM [26] simplifies the training\nprocess of MAE while maintaining accuracy.\nResearchers have also explored breakthroughs in the number\nof parameters in visual models. Leveraging the scalability of\nthe ViT model, Zhai et al. [2] scaled the ViT model to 1.8\nbillion parameters. Taking it a step further, Google proposed a\nmodel [3] with 22 billion parameters, demonstrating the visual\nscaling potential akin to large language models. Additionally,\nInternImage [120] implements a large-scale CNN foundation\nmodel, achieving performance improvements similar to ViT.\nWhile most of these models are focused on image-based\ntasks, the extension of natural image foundation models to\nvideo domains has also been explored [121, 122, 123]. These\nadvancements have opened up new possibilities for utilizing\nlarge-parameter foundation models in various visual tasks,\nboth for images and videos.\nC. Vision-Language Foundation Model\nIn the current landscape, foundation models have evolved\nto encompass more than just deep models; they now focus\non utilizing vast amounts of data and computational power\nto tackle diverse problems. The goal is to use a unified\nmodel capable of addressing multiple modalities and tasks.\nConsequently, there is a growing emphasis on training vision-\nlanguage models [124]. Table II summarizes some of the\nrepresentative vision-language foundation models.\nCLIP [10] is a prominent example that leverages a large-\nscale collection of image-text pairs from the internet for\ncontrastive learning, enabling the creation of a unified repre-\nsentation of multimodal data. Inspired by CLIP, various multi-\nmodal image foundation models have been proposed, differing\nin their model structures [125, 30], feature representations\n[126, 127], multimodal feature fusion approaches [27], feature\nalignment loss functions [28], pre-training methods [29], and\nmore. These advancements have significantly improved the\nperformance of multimodal foundation models.\nIn addition, DALL-E [128] combines the diffusion model\nwith multimodal foundation models to generate images from\ntext. SAM [31] introduces a promptable model with training\nstrategies that enable the segmentation of objects using text,\npoints, and lines as prompts. GPT-4 [129] exhibits superhuman\ncapabilities on various professional and academic datasets.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n11\nTABLE II\nSUMMERY OF LANGUAGE , VISION AND VISION -LANGUAGE FOUNDATION MODELS .\nModel Name Publish\nTime\nParameter\nNumber Contribution\nLanguage Foundation Models\nBERT [4] 2018 336M Propose a transformer-based language representation model using a bidirectional approach to train on\nlarge amounts of text data.\nT5 [111] 2019 11B Convert all text-based language problems into a text-to-text format, allowing us to use the same\nmodel, loss function, and hyperparameters on any NLP task.\nGPT-3 [73] 2020 175B Demonstrate that scaling up language models greatly improves task-agnostic, few-shot performance.\nWebGPT [112] 2021 175B Fine-tune GPT-3 to answer long-form questions using a text-based web-browsing environment.\nCodex [113] 2021 175B Propose a GPT language model finetuned on publicly available code from GitHub, and study its Python\ncode-writing capabilities.\nBLOOM [116] 2022 176B Propose a language model capable of generating text in 46 natural languages and 13 programming\nlanguages.\nGLM [115] 2022 130B Propose a bilingual (English and Chinese) pre-trained language model with 130 billion parameters.\nFlan-T5 [117] 2022 11B Explore instruction finetuning with a particular focus on scaling the number of tasks, scaling the model\nsize, and finetuning on chain-of-thought data.\nInstructGPT [119] 2022 1.3B Propose an avenue for aligning language models with user intent on a wide range of tasks by fine-\ntuning with human feedback.\nPaLM [118] 2022 540B Further explore the impact of scale and train a 540-billion parameter, densely activated, Transformer\nlanguage model.\nLLaMA [114] 2023 65B Release a collection of foundation language models ranging from 7B to 65B parameters publicly.\nVision Foundation Models\nBYOL [23] 2020 375M Propose a approach to self-supervised image representation learning through the interaction and learn\nbetween online and target network.\nSimCLR v2 [24] 2020 795M\nPropose a semi-supervised learning algorithm contraining unsupervised pretraining of a big model,\nsupervised fine-tuning on a few labeled examples, and distillation with unlabeled examples for refining\nand transferring the task-specific knowledge.\nDINO [25] 2021 85M Show the potential of self-supervised pretraining a standard ViT model, achieving performance that are\ncomparable with the best convnets specifically designed.\nMAE [1] 2022 632M Propose a masked autoencoders which are scalable self-supervised learners for computer vision.\nSimMIM [26] 2022 197M Propose a simplified framework for masked image modeling without the need for special designs, such\nas block-wise masking and tokenization via discrete V AE or clustering.\nScaling ViT [2] 2022 1.8B Demonstrate that the performance-compute frontier for ViT models with enough training data roughly\nfollows a (saturating) power law.\nInternImage [120] 2023 1.08B Propose a new large-scale CNN-based foundation model, which can obtain the gain from increasing\nparameters and training data like ViTs\nViT 22B [3] 2023 22B Present a recipe for highly efficient and stable training of a 22B-parameter ViT.\nVideoSwin [121] 2022 200M Instead an inductive bias of locality in video transformers based on the Swin Transfomrmer.\nBEVT [122] 2022 88M Conduct masked image modeling on image data and masked video modeling on video data jointly.\nVideoMAE [123] 2023 2B Propose a scalable and general self-supervised pre-trainer for building video foundation models.\nVision-Language Foundation Models\nCLIP [10] 2021 370M\nDemonstrate that the simple pre-training task of predicting which caption goes with which image is\nan efficient and scalable way to learn SOTA image representations from scratch on a dataset of 400\nmillion (image, text) pairs collected from the internet.\nALBEF [27] 2021 209.5M Propose a contrastive loss to align the image and text representations before fusing them through\ncross-modal attention.\nFlorence [126] 2021 893M Propose a new computer vision foundation model to expand the representations from coarse (scene) to\nfine (object), from static (images) to dynamic (videos), and from RGB to multiple modalities.\nCoCa [28] 2022 2.1B Propose a minimalist design to pretrain an image-text encoder-decoder foundation model jointly with\ncontrastive loss and captioning loss.\nDALL-E 2 [128] 2022 6.5B A diffusion model can generate images from text descriptions based on the embedding space of CLIP.\nFlamingo [125] 2022 80B\nPropose an architecture to bridge powerful pretrained vision-only and language-only models, handle\nsequences of arbitrarily interleaved visual and textual data, and seamlessly ingest images or videos as\ninputs.\nInternVideo [127] 2022 -\nExplores masked video modeling and video-language contrastive learning as the pretraining objectives,\nand selectively coordinates video representations of these two complementary frameworks in a\nlearnable manner.\nBLIP-2 [29] 2023 12.8B Propose a generic and efficient pretraining strategy that bootstraps vision-language pre-training from\noff-the-shelf frozen pre-trained image encoders and frozen large language models.\nBEiT-v3 [30] 2023 1.9B Propose a Multiway Transformers for general-purpose modeling, where the modular architecture\nenables both deep fusion and modality-specific encoding.\nPaLI [130] 2023 17B Scale up a model to the joint modeling of language and vision with flexible task interface.\nSAM [31] 2023 - Propose a promptable model, which can transfer zero-shot to new image distributions and tasks.\nGPT-4 [129] 2023 - Propose a large-scale, multimodal model which exhibits human-level performance on various\nprofessional and academic benchmarks.\nmPLUG-2 [131] 2023 - Introduce a multi-module composition network by sharing common universal modules for modality\ncollaboration and disentangling different modality modules to deal with modality entanglement.\nMeta-Transformer\n[132] 2023 -\nPropose a unified framework can handle a wide range of tasks including fundamental perception\n(text, image, point cloud, audio, video), practical application (X-Ray, infrared, hyperspectral,\nand IMU), and data mining (graph, tabular, and time-series).\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n12\nTABLE III\nSUMMERY OF REMOTE SENSING FOUNDATION MODELS .\nModel Name Publish\nTime\nParameter\nNumber Contribution\nRSP [32] 2022 27M Conduct an empirical study of remote sensing pretraining on aerial images.\nRVSA [17] 2022 89M Train a plain ViTs with about 100 million parameters and propose a new rotated varied-size window\nattention.\nRingMo [18] 2022 88M Construct a large-scale dataset by collecting two million RS images and propose a training method\ndesigned for dense and small objects.\nGeograph [133] 2023 24M Propose a contrastive learning methods exploiting the spatio-temporal structure of remote sensing data.\nSatMAE [20] 2023 307M Propose a pre-training framework for temporal or multi-spectral satellite imagery based on MAE.\nScaleMAE [22] 2023 307M Propose a masked image modeling method using the area of the Earth covered by the image to\ndetermine the scale of the positional encoding.\nBillion [19] 2023 2.4B Conduct an empirical study of the effect of increasing the number of model parameters in RS.\nGFM [21] 2023 80M Investigate the potential of continual pretraining from largescale ImageNet-22k models and propose a\nmulti-objective continual pretraining paradigm.\nCSP [134] 2023 - Propose a contrastive spatial pre-training framework leveraging the abundant geospatial information\nassociated with images.\nmPLUG-2 [131] introduce a multi-module composition net-\nwork including text, image and video. Meta-Transformer [132]\nproposed a unified framework performing learning across 12\nmodalities with unpaired data (e.g., natural language, 2D\nimages, 3D point clouds, audio, video, time series, tabular\ndata).\nThese models are all built upon large-scale training data and\nself-supervised methods, harnessing the potential of unlabeled\nmulti-modal data encompassing both vision and language to\ntrain the foundation models. As a result, these models can\neffectively perform a wide range of tasks that involve both\nvision and language processing.\nD. RS Foundation Model\nThe research on natural image foundation models has seen\nsignificant progress, and the field of remote sensing has also\ngarnered substantial attention in this regard. However, due to\nthe inherent domain gap between natural images and remote\nsensing images, directly applying pre-trained models from nat-\nural images to remote sensing images often leads to suboptimal\nresults. To address this challenge, the construction of remote\nsensing foundation models can be divided into two approaches:\ntraining from scratch and continuous training using pre-trained\nnatural image models. The RSFMs are summarized in Tabel\nIII.\n1) Training from scratch: The training from scratch ap-\nproach involves collecting a large number of remote sensing\nimages and using the training methods employed in natural\nimage foundation models. Wang et al. [32] conducted exper-\niments on remote sensing pre-training models, showing that\npre-training methods can effectively alleviate data differences,\nbut may still be influenced by task differences as downstream\ntasks require distinct representations from scene recognition\ntasks. Sun et al. [18] collected 2 million remote sensing images\nto build a large-scale dataset covering diverse scenes and\nobjects worldwide for pre-training. They proposed the RingMo\nmasked image modeling method, addressing the problem of\ndense small targets often overlooked in complex remote sens-\ning scenes. Wang et al. [17] trained a visual remote sensing\nTransformer model with approximately 100 million parameters\nand introduced a new rotating variable-size window attention\nmethod to accommodate the characteristics of dense remote\nsensing targets. Addressing the issue of large differences in\nthe scale of remote sensing targets, Reed et al. proposed the\nScale-MAE method [22]. This method explicitly learns the\nrelationship between data at different known scales, enabling\nrobust multi-scale representations. Furthermore, SatMAE [20]\nestablished a masked autoencoder (MAE)-based pre-training\nframework for temporal or multispectral satellite imagery,\nextending the paradigm to multispectral imagery as well as\ntemporal dimensions. Geograph [133] introduced a method for\ncomparative learning of spatio-temporal structure for remote\nsensing data. Additionally, CSP [134] utilized geospatial in-\nformation in the image to construct a pre-training framework\nfor contrastive learning.\n2) Continuous training: While training from scratch has\npropelled the development of remote sensing foundation mod-\nels, it can be resource-intensive and challenging for large-scale\nmodels. As a result, some researchers have turned their focus\nto the method of continuous training, which utilizes existing\npre-trained natural image foundation models. Cha et al. [19]\nimplemented a Billion-level remote sensing image foundation\nmodel based on Wang et al.’s [17] work. Mendieta et al. [21]\nconstructed a compact yet diverse dataset called GeoPile to\nincrease the amount of information in the pre-training data.\nThey introduced the GFM model and carried out continuous\npre-training based on the large-scale ImageNet-22k pre-trained\nmodel to achieve an efficient geospatial foundation model with\nminimal resource cost and carbon impact. These continuous\ntraining approaches offer a cost-effective and efficient way\nto leverage existing pre-trained models in the remote sensing\ndomain.\nIV. A PPLICATIONS OF RS F OUNDATION MODEL\nIn this section, we introduce the important applications of\nRS foundation models. The applications are shown in Fig. 12.\nThe applications are divided into three types: Classification\nTask, Location Task and Understand Task. The classification\ntasks classify the image into a certain category at the image\nlevel or pixel level. Location tasks locate the target with boxes\nor masks. The understand tasks involve in the process of\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n13\nVision FM\nCity\nThere are two \ntennis courts in \nthe grass.\nTwo.\nLanguage FM\nObject Description:\nTennis court on the left.\nQuestion:\nHow many tennis courts \nare there in the scene?\nFusion Module\nScene\nClassification\nSemantic \nSegmentation\nChange\nDetection\nObject\nDetection\nMoving Object\nDetection\nObject\nTracking\nVisual \nGrounding\nImage\nCaption\nVisual Question\nAnswer \nClassification Location Understanding\nSingle Images Pair Images Video\nFig. 12. The application of Remote Sensing Foundation Models. The applications are divided into three types: Classification Task, Location Task and\nUnderstand Task. The classification task and location task are pure-visual tasks. The understand tasks involve in the process of language.\nlanguage. The representative algorithms in recent years are\nsummarized in Table IV and Table V.\nA. Classification Task\n1) Scence Classification: Scene classification is an image\nclassification task similar to natural images. Given a remote\nsensing image, it needs to be classified into a specific category\naccording to the category settings [135]. In the whole remote\nsensing image, the scene information contained is complex,\nso the picture used for scene classification is usually cut out\nfrom the whole image according to a specific target to obtain\na single-category image.\nScene classification tasks mainly require model representa-\ntion capabilities, so many new algorithms are also applied to\nthis field. Yang et al. [136] proposed an explainable multi-\nscale spatial-spectral Transformer. Spatial-attention is a pop-\nular algorithm to achieve multi-scale fusion [137, 138, 139].\nContrastive learning is wildly applied to learn robust represen-\ntation in remote sensing [140, 141, 142, 143, 144]. In addi-\ntion, generative adversarial network [145], neural architecture\nsearch [146], auto-encoder [147], knowledge distillation [148]\nand collaborative framework [149] have also been applied to\nscene classification.\n2) Semantic Segmentation: Semantic segmentation, also\nknown as land-cover and land-use classification in remote\nsensing, is one of the most important and widely used tasks\nin remote sensing interpretation. This task involves classifying\neach pixel of an image into specific categories representing\ndifferent ground objects. The classification process must take\ninto account various factors, such as multi-scale characteristics\nof ground objects, texture features, and spectral characteristics.\nHowever, the ground resolution of remote sensing images can\nbe limiting, as often a single pixel contains spectral infor-\nmation from multiple ground objects, making it challenging\nto accurately distinguish the boundaries of individual ground\nobjects. Nevertheless, with advancements in high-resolution\nremote sensing imaging technology, the accuracy of ground\nobject segmentation has significantly improved and become\nwidely used in various applications.\nNumerous studies have been conducted in the field of\nsemantic segmentation in remote sensing. To leverage the po-\ntential of large amounts of unlabeled data, various algorithms\nsuch as semi-supervised [150], self-supervised [151, 152, 153,\n154, 155], and self-training [156, 157] methods are widely\nemployed in remote sensing semantic segmentation. These\nmethods make effective use of unlabeled data to improve\nthe accuracy of segmentation. Additionally, factors such as\nmulti-scale [158, 159], spatial-temporal [160, 161, 162], and\nboundary [163] information, which are crucial for semantic\nsegmentation, are often carefully considered and incorporated\ninto the design of algorithms. Furthermore, there are investiga-\ntions into the interaction-based segmentation of remote sensing\nimages, which explores methods that involve interactions\nbetween pixels for more accurate segmentation results [164].\n3) Change Detection: Remote sensing change detection\n(RSCD) is a process that involves identifying and extracting\ndifferences between multi-temporal remote sensing images\ncaptured of the same geographic area. The typical workflow of\nRSCD methods includes several steps, such as remote sensing\nimage pre-processing (alignment, correction, noise reduction,\netc.), selecting appropriate change detection methods, and\nanalyzing the results.\nDue to the limited availability of labeled data for training,\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n14\nTABLE IV\nA SUMMARY OF THE REPRESENTATIVE ALGORITHMS OF CLASSIFICATION APPLICATION . THE CLASSIFICATION APPLICATION INCLUDES SCENE\nCLASSIFICATION , SEMANTIC SEGMENTATION AND CHANGE DETECTION .\nMethod Publication Modality Key Characteristics\nScene classification\nTang et al. [137] JSTARS2021 Optical Local features; Attention consistent network\nPeng et al. [146] TGRS2021 Optical Neural architecture search; Architecture regularization\nLiu et al. [140] TNNLS2022 SAR Contrastive learning; Dual dynamic GCN\nZeng et al. [141] ISPRS2022 Optical Few-shot learning; Task-specific contrastive loss; Self-attention and mutual-attention\nEMTCAL [165] TGRS2022 Optical Multiscale; Cross-level attention; Transformer.\nQian et al. [147] TGRS2022 SAR G0-based convolutional variational auto-encoder; Hybrid network; Structural constraint\nMiao et al. [145] TGRS2022 Optical Involution-generative adversarial network; Siamese network; Semi-supervised\nXu et al. [148] TGRS2022 Optical Knowledge distillation; ViT\nGCSANet [138] JSTARS2022 Optical Global context spatial attention; Multiscale\nHuang et al. [142] GRSL2022 Optical Spatial-temporal invariant contrastive learning; Optimal transport\nLi et al. [139] RS2022 Optical Multiscale; Self-adaptive attention\nLi et al. [143] TGRS2023 Optical Contrastive learning; Hard feature transformation\nYang et al. [136] TGRS2023 Optical Explainable; Multiscale representation; Spatial-frequency;\nTexture-enhanced encoder; Transformer\nZhao et al. [149] TGRS2023 Optical Local and long-range collaborative framework; Cross-feature calibration;\nSemi-supervised; ViT\nXu et al. [144] JSTARS2023 Optical Contrastive learning; Occlusion images; Out-of-distribution\nSemantic Segmentation\nShang et al. [158] RS2020 MSI Multi-scale context; Adaptive fusion; Channel attention\nLi et al. [151] JSTARS2021 Optical Selfsupervised learning; Multitask loss; Triplet Siamese network;\nHigh-level and low-level image features\nLuo et al. [160] ISPRS2022 Optical Domain adaptation; Cross-spatiotemporal\nLi et al. [163] TGRS2022 Optical Boundary attention; Multilevel aggregation; Multitask learning\nMANet [159] TGRS2022 Optical Aware relation; Collaborative learning; Feature refinement; Multi-scale\nLu et al. [156] TGRS2022 Optical Data augmentation; Pseudolabeling-based self-training;\nSemisupervised learning; Linear sampling\nBai et al. [161] TGRS2022 HSI Multibranch prediction; Self-attention; Spatial attention; Transformer\nXue et al. [154] TGRS2022 Optical, HSI Generative selfsupervised; Multimodal data\nXue et al. [155] TGRS2022 Optical, HSI Multiview learning; Contrastive learning; Multitask learning; Multimodal images\nYang et al. [152] TGRS2022 Optical Contrastive learning; Self-supervised learning\nJAGAN [162] JSTARS2022 HSI Channel-space attention; Generative adversarial network\nDIAL [164] JSTARS2022 Optical Active learning; Interactive segmentation\nLi et al. [157] RS2022 Optical Unsupervised domain adaptation; Self-training;\nGradual Class Weights; Local Dynamic Quality\nLu et al. [150] TGRS2023 Optical Semisupervised; Weak-to-strong consistency learning;\nSparse dual-view cross-sample image generation\nGhanbari et al. [166] JSTARS2023 SAR Local and global spatial dependency; Superpixels; Graph-based learning\nMarsocci et al. [153] JSTARS2023 Optical Continual learning; Selfsupervised learning; Barlow twins\nChange Detection\nDong et al. [167] RS2020 Optical, SAR Generative adversarial networks; Deep belief networks; Self-supervised learning\nChen et al. [168] RS2021 Optical Siamese network; Attention mechanism\nZhang et al. [169] TIP2022 SAR Contourlet fusion; Non-local clustering; Fuzzy clustering\nLi et al. [170] TGRS2022 Optical Densely attentive refinement network; Hybrid attention\nLv et al. [171] TGRS2022 MSI Convolutional block attention module; Multiscale dilation convolution module\nSwinSUNet [172] TGRS2022 Optical Siamese U-shaped structure; Swin transformer\nDing et al. [173] TGRS2022 Optical Bi-temporal semantic reasoning network; CNN\nZhang et al. [174] TGRS2022 SAR Multiobjective sparse feature learning; Cross-entropy clustering loss\nDong et al. [175] TGRS2022 SAR Deep clustering; Noise-robustness loss; Shearlet transform\nDong et al. [176] TGRS2022 SAR Deep clustering; Multiscale fusion; Octave convolution; Self-attention\nHFA-Net [177] PR2022 Optical High frequency; Spatial-wise attention\nSSCFNet [178] JSTARS2023 Optical Combined enhancement; Spatial-Spectral Cross Fusion\nZhang et al. [179] RS2023 Optical Siamese network; Attention module; Transformer module; Multi-scale feature fusion\nclustering algorithms remain the mainstream approach for\nchange detection [169, 174, 175, 176]. Attention modules\nhave also emerged as a noteworthy technique in this field\n[168, 179]. Notable attention modules include hybrid attention\n[170], convolutional block attention module [171], and spatial-\nwise attention [177], which have been proposed to enhance the\nperformance of change detection algorithms. Additionally, the\ncharacteristics of Generative Adversarial Networks (GANs)\n[167] and the Swin-Transformer model [172] have been in-\ntegrated into change detection algorithms, further improving\ntheir effectiveness.\nResearchers have also proposed innovative methods to rea-\nson about both single-temporal and cross-temporal semantic\ncorrelations for change detection [173], and spatial-spectral\ncross fusion approaches such as SSCFNet [178] have been\nintroduced to improve change detection performance. Further-\nmore, multi-scale geometric techniques like Shearlet [175]\nand contourlet [169] have been applied to change detection\nto provide multi-scale and multi-directional features, leading\nto better performance in detecting changes in remote sensing\nimages.\nB. Location Task\n1) Object Detection: Object detection is a crucial task\nin remote sensing interpretation that involves identifying and\nlocalizing objects of interest within a scene. However, ob-\nject detection in remote sensing presents unique challenges\ncompared to natural scenes. Remote sensing targets are often\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n15\nTABLE V\nA SUMMARY OF THE REPRESENTATIVE ALGORITHMS OF LOCATION AND UNDERSTANDING APPLICATIONS . THE LOCATION APPLICATION INCLUDES\nOBJECT DETECTION , MOVING OBJECT DETECTION , TARGET TRACKING AND VISUAL GROUNDING (VG). T HE UNDERSTANDING APPLICATION INCLUDES\nVISUAL GROUNDING , IMAGE CAPTION AND VISUAL QUESTION ANSWERING (VQA).\nMethod Publication Modality Key Characteristics\nObject Detection\nLi et al. [180] JSTARS2021 Optical Point-based; Progressive candidate bounding box mining; Weakly Supervised\nZhang et al. [181] TGRS2022 Optical Foreground modeling; Foreground anchor reweighting loss\nZhang et al. [182] TGRS2022 Optical Laplacian feature pyramid; Trainable Laplacian operator\nBai et al. [183] TGRS2022 Optical Wavelet decomposition; Discrete wavelet multiscale attention mechanism;\nReinforcement learning; Time-frequency analysis\nLiu et al. [184] TGRS2022 Optical Feature distillation structure; Global perception; Axial attention; Salient object detection\nCheng et al. [185] TGRS2022 Optical Weakly supervised; Self-guided Proposal Generation\nYe et al. [186] RS2022 Optical Adaptive attention fusion; EfficientDet; Spatial attention; Complete Intersection over Union\nZhang et al. [187] ISPRS2023 Optical Generalized few-shot; Transfer-learning; Metric learning; Representation compensation\nMOL [188] ISPRS2023 Optical Weakly supervised; Noisy learning; Multi-view learning; Temporal consistency\nLi et al. [189] TGRS2023 Optical Instance-aware distillation; Relation-based; Parameter-free masking module\nZhang et al. [190] JSTARS2023 Optical Object-centric; Attention-guided mask generator; Masked image modeling;\nSelf-supervised learning; Vision transformer\nHan et al. [191] JSTARS2023 Optical Capsule reasoning; Transformer; DETR-based; Multilevel feature fusion\nMoving Object Detection\nZhang et al. [192] TGRS2020 Optical Background subtraction; Low-rank matrix decomposition; Structured sparsity regularization\nZhang et al. [193] TGRS2020 Optical Background subtraction; Online robust principal component analysis;\nStructured sparsityinducing norm\nFeng et al. [194] ISPRS2021 Optical Keypoint-based detection; Spatial motion information-guided;\nRelative spatial relationship; MOT\nZhang et al. [195] TPAMI2022 Optical Moving-confidence-assisted matrix decomposition;\nMoving-Confidence-Assisted Matrix Decomposition\nPi et al. [196] TGRS2022 Optical Motion information; Low-resolution; Transformer\nDSFNet [197] GRSL2022 Optical Two-stream detection network; Dynamic and static information fusion\nSDANet [198] TIP2023 Optical Anchor-free detector; Semantic-embedded; Weakly supervised learning;\nDensity matching; Road information; Bi-directional conv-RNN\nObject Tracking\nHRSiam [199] TIP2021 Optical SOT; Siamese network; High spatial-resolution representation; Gaussian mixture model\nDFAT [200] TGRS2022 Optical SOT; Dynamic feature-adaptive; Candidate experts\nCui et al. [201] TGRS2022 Optical SOT; Deep reinforcement learning; Occlusion; Temporal and spatial context\nMBLT [202] TGRS2022 Optical SOT; Motion and background learning; Location probability; Segmentation\nSong et al. [203] TGRS2022 Optical SOT; Channel and spatial attention; Cross-attention; Composite feature combine\nLi et al. [204] TGRS2022 Optical SOT; Correlation particle filter; Kalman filter\nChen et al. [205] JSTARS2022 Optical SOT; Historical model; Correlation filter; Antidrift tracker correction\nLi et al. [206] GRSL2022 Optical SOT; Correlation filter; Interacting multiple model\nNie et al. [207] GRSL2022 Optical SOT; Temporal motion compensation; Multidimensional information-aware; Siamese\nLi et al. [208] TCYB2023 Optical SOT; Dual-branch spatial-channel co-attention;\nCollaborative learning; Geometric constraint\nZhang et al. [209] TEVC2023 Optical SOT; Quantum evolution; Rotation operator; Trajectory inference;\nBalanced Intersection over Union\nSiamMDM [210] TGRS2023 Optical SOT; Dynamic template update; Multiple response map fusion;\nScore-guided target motion trajectory prediction\nAo et al. [211] TIP2020 Optical MOT; Probabilistic noise modeling; Multi-morphological; Evaluation protocols\nHe et al. [212] TGRS2022 Optical MOT; Graph reasoning; Multitask learning; Spatiotemporal; AIR-MOT dataset\nZhang et al. [213] TGRS2023 Optical MOT; Bidirectional; Invalid fragment trajectory backtracking;\nTrajectory criteria; Integrate with SOT\nCFTracker [214] TGRS2023 Optical MOT; Cross-frame feature update; Cross-frame training flow; Joint detection and tracking\nVG\nSun et al. [215] ACM MM2022 Optical RSVG dataset; Numerical geospatial relations; Geospatial relation graph\nZhan et al. [216] TGRS2023 Optical DIOR-RSVG dataset; Transformer; Multigranularity visual language fusion\nYuan et al. [217] arXiv2023 Optical RefSegRS dataset; Language-guided cross-scale enhancement; Segmentation\nImage Caption\nWu et al. [218] IJCNN2020 Optical Long short-term memory network; Scene attention\nYe et al. [219] TGRS2022 Optical Joint training; Multilabel attributes; Multilabel classification;\nDifferentiable sampling operator; Dynamic contrast loss\nLi et al. [220] TGRS2022 Optical Recurrent attention; High-level attentive maps; Long short-term memory; Semantic gate\nWang et al. [221] ISPRS2022 Optical Multi-label; Semantic attribute extractor; Cross-modal semantic feature fusion operators\nLi et al. [222] RS2022 Optical Multi-level attention mechanism; Encoder-decoder\nTypeFormer [223] GRSL2022 Optical Captiontype controller; Multiscale vision transformer\nWang et al. [224] IGARSS2022 Optical Pure transformer\nYang et al. [225] ISPRS2022 Optical Meta learning; Support tasks\nWang et al. [226] JSTARS2022 Optical Multiscale; Multiinteraction\nZia et al. [227] IJAEOG2022 Optical Multi-scale; Adaptive attention; Topic sensitive word embedding\nZhang et al. [228] RS2023 Optical Multi-source interactive; Stair attention\nChg2Ca [229] arXiv2023 Optical Change caption; Siamese CNN-based; Hierarchical self-attention\nVQA\nZhang et al. [230] TGRS2023 Optical Hash-based spatial multiscale visual representation; Spatial hierarchical reasoning\nPrompt-RSVQA [231] CVPRW2022 Optical Prompt; DistilBERT\nYuan et al. [232] TGRS2022 Optical Self-paced curriculum learning; Spatial transformer; Language-guided; RNN\nBazi et al. [233] TGRS2022 Optical CLIP; Co-attention; Transformer\nAl et al. [234] IJRS2022 Optical Open-set dataset; VQA-TextRS dataset; Transformer\nChappuis et al. [235] IGARSS2022 Optical Recurrent neural network; BERT\nYuan et al. [236] TGRS2022 Optical Multitemporal; Change detection; CDVQA dataset\nBashmal et al. [237] JSTARS2023 Optical Visual question generation; Language transformers; GPT-2; TextRS-VQA dataset\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n16\ndensely distributed, making it difficult for conventional hor-\nizontal bounding boxes to effectively capture and surround\nthe targets. Moreover, the significant variation in scale among\nremote sensing objects adds complexity to the detection task.\nTo address these challenges, researchers have proposed\nvarious approaches to improve object detection in remote\nsensing imagery. For example, Zhang et al. [181] introduced\nthe Laplacian feature pyramid to capture multi-scale features,\nenhancing the detection performance. Ye et al. [186] de-\nveloped an adaptive attention fusion method in conjunction\nwith EfficientDet [238] to better handle multi-scale objects.\nBai et al. [183] leveraged time-frequency analysis and deep\nreinforcement learning to reduce computational complexity\nwhile ensuring detection accuracy.\nIn addition to the traditional target detection methods,\nrecent efforts have focused on advancing learning algorithms\nfor object detection. Weakly supervised learning [188, 180,\n185], self-supervised learning [190], and distillation strategies\n[189, 184] have gained attention as effective approaches for\nimproving object detection performance in remote sensing\nimagery. These learning methods help enhance the model’s\nability to learn from limited labeled data, leading to better\nobject detection results in remote sensing applications.\n2) Moving Object Detection: The advancement of re-\nmote sensing photography technology has allowed satellites to\ncapture continuous videos by staring at specific areas [239].\nMoving object detection in satellite videos involves extracting\nobjects in motion, such as airplanes, ships, and cars. However,\ndue to factors like changes in lighting, weather, and viewing\nangles, the moving targets often occupy only a small portion of\nthe scene. As a result, moving target detection faces challenges\nof false detections and missed detections caused by image blur\nand shaking.\nOne popular approach for moving object detection in remote\nsensing is background subtraction [192, 193]. Zhang et al.\n[195] explored background modeling and incorporated sparse\nconstraints to achieve accurate extraction of moving objects.\nHowever, in low-quality videos, background subtraction meth-\nods tend to produce a large number of false alarms and may\nmiss many positive targets. To address these limitations, sev-\neral methods based on temporal and appearance features have\nbeen proposed [194, 196]. For example, DSFNet [197] is a\ntwo-stream detection network that considers both dynamic and\nstatic information. SDANet [198] is an anchor-free detector\nthat utilizes road information to suppress false alarms. These\napproaches aim to improve the accuracy and robustness of\nmoving object detection in remote sensing videos, making it an\nactive area of research with promising applications in various\ndomains.\n3) Object Tracking: Object tracking in satellite videos\ninvolves continuously tracking single or multiple objects [240].\nHowever, due to the limited ground resolution of satellite\nimaging, the targets in satellite images are usually very small,\nproviding limited detail information, which can lead to track-\ning deviations or difficulty in distinguishing targets from the\nbackground. Additionally, the presence of clouds and buildings\ncan further hinder object tracking.\nIn single object tracking (SOT), correlation filter-based\nalgorithms remain comparable [204, 205, 206]. To address\nchallenges in high-spatial resolution representation, HRSiam\n[199] is proposed. Cui et al. [201] explore reinforcement learn-\ning to tackle occlusion problems during tracking. MBLT [202]\nleverages motion and background information to improve\nobject tracking in satellite videos. Spatial-channel attention\nis also utilized in SOT for remote sensing [203, 208], and\ndynamic information in videos is explored in [200, 207, 210].\nIn multiple object tracking (MOT), Ao et al. [211] pro-\npose probabilistic noise modeling algorithms and evaluation\nprotocols for MOT. He et al. [212] develop graph reasoning\nalgorithms that leverage the relations between objects and\nintroduce the AIR-MOT dataset. Zhang et al. [213] utilize\nbidirectional tracking for trajectory verification to mitigate\nthe influence of similar objects. CFTracker [214] introduces\na cross-frame feature update and training flow to enhance\ntracking performance. These advancements in object tracking\ntechniques for satellite videos hold significant promise for\nimproving the accuracy and robustness of remote sensing\napplications.\nC. Understanding Task\nUnderstanding tasks in remote sensing interpretation en-\ncompass tasks involving linguistic descriptions, including vi-\nsual grounding (VG), image captioning, and visual question\nanswering (VQA).\n1) Visual Grounding: Visual grounding, also known as\nreferring location, is a derivative task of target detection.\nIn contrast to target detection, where the category of the\ntarget is pre-defined, visual grounding requires locating the\ntarget in the image based on a given linguistic expression.\nThis task demands not only language understanding but also\ncomprehension of the categories and relationships of targets\nin remote sensing images to achieve accurate localization.\nRemote sensing visual grounding is still in its early stages of\ndevelopment, and datasets like RSVG [215] and DIOR-RSVG\n[216], RefSegRS [217] have been introduced.\nZhan et al. [216] proposed the MGVLF module, which\ncombines image features extracted from CNNs and text fea-\ntures obtained using BERT to achieve target localization.\nSimilarly, Sun et al. [215] proposed the GeoVG model, which\nalso utilizes BERT to encode text. Moreover, the geospatial\nrelations of the target are taken into consideration to improve\naccuracy.\n2) Image Caption: Image captioning involves summarizing\nthe text describing an image based on the information in the\nimage. In the case of remote sensing images, which are taken\nfrom a high altitude, the targets are small and numerous. As\na result, remote sensing image caption algorithms tend to\nfocus on describing the dominant content in the scene while\noverlooking smaller objects of interest. Additionally, current\nremote sensing image description datasets suffer from issues\nsuch as small image sizes and limited richness of content.\nExisting algorithms face difficulties when applied to large-\nscale images and struggle to fully describe the content using\nrich, hierarchical, and coherent language.\nNumerous studies have been conducted on remote sensing\nimage captioning, with attention mechanisms and semantic\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n17\ninformation often employed [222, 218, 220, 221, 227, 228].\nWang et al. [224] proposed a pure Transformer for image cap-\ntioning in remote sensing. TypeFormer [223] was introduced\nto control the type of generated captions, while Chg2Ca [229]\nextended captioning to change descriptions in remote sensing.\nAdditionally, multi-label [219] and meta-learning [225] have\nalso been considered.\n3) Visual Question Answering: Visual question answering\ninvolves answering questions based on image information. In\nremote sensing, visual question answering is mainly divided\ninto three types: (1) determining whether a specific target\nis present in the image; (2) identifying the target within\nthe question description area; and (3) counting the number\nof targets. Current remote sensing visual question answering\nmodels often overlook information in the image space, leading\nto lower accuracy in information answering. Moreover, the\ndesign of current visual question answering tasks is rela-\ntively simple and cannot handle more complex questions.\nFurthermore, these tasks do not consider the role of landmark\nbuildings in question answering, necessitating the integration\nof real geographic information to enhance the practicality of\nvisual answering tasks.\nZhang et al. [230] propose a spatial hierarchical reason-\ning network to model and reason the relationships between\nentities. Yuan et al. [232] introduce a self-paced curriculum\nlearning approach for VQA in remote sensing. Models like\nBERT [231, 235], CLIP [233], and GPT [237] have been\nwidely applied, and addressing the open-set problem of VQA\nin remote sensing is also a focus of research [234]. The\nVQA task related to change detection is an emerging research\ndirection [236]. Furthermore, visual question generation is also\na valuable area for the development of VQA [237].\nV. E XPLORATION THE EFFECTIVENESS OF EXISTING FMS\nON VARIOUS RS APPLICATIONS\nRemote sensing interpretation is a technology utilized for\nanalyzing and comprehending images. Numerous foundation\nmodels have been developed for image interpretation. In this\nsection, we conduct systematic experiments to compare the\nperformance of remote sensing foundation models with natural\nfoundation models in remote sensing applications. The exper-\niments cover three key aspects: global representation (scene\nclassification), local representation (semantic segmentation),\nand object localization (object detection). Furthermore, we\nprovide a detailed discussion on the strengths and weaknesses\nof the currently existing foundation models.\nA. Scene Classification\nWe conducted experiments on representation capabilities\nusing five commonly used scene classification datasets in\nremote sensing.\n1) Dataset: We conducted experiments on scene classifi-\ncation using a total of five datasets.\nWHU-RS19 [241]: This dataset consists of 1,005 images\ncollected from Google Earth imagery, covering 19 categories\nof remote sensing scenes. The images are fixed at a size of\n600 × 600 pixels with a spatial resolution of 0.5 m. Each\ncategory contains approximately 50 images.\nUCMerced [242]: UCMerced is obtained from the United\nStates Geological Survey National Map and comprises 21\nclasses, with 100 images per category. Compared to WHU-\nRS19, UCMerced offers a higher spatial resolution of 0.3 m.\nThe images in UCMerced are cropped into smaller regions of\n256 × 256 pixels.\nAID [243]: AID is an aerial imagery dataset that includes\n10,000 images with 30 categories of remote sensing scenes.\nThe number of sample images varies from 220 to 420 for\neach class. The spatial resolutions of the images range from\n8 m to 0.5 m, presenting a challenge in scene classification.\nAID dataset was constructed to consider higher intraclass vari-\nations and smaller interclass dissimilarity for comprehensive\ncomparisons.\nRESISC [244]: Also known as NWPU-RESISC45, this\ndataset consists of 31,500 images extracted from remote sens-\ning images in Google Earth. It comprises 45 categories, with\n700 images in each class. The spatial resolution of RESISC\nvaries from 30 m to 0.2 m. The image size is standardized\nto 256 × 256 pixels. RESISC covers over 100 countries\nand regions worldwide, providing rich image variations, high\nwithin-class diversity, and between-class similarity.\nfMoW [41]: The Functional Map of the World (fMoW) is\na large-scale remote sensing dataset used for training machine\nlearning models. It enables the prediction of building functions\nand land use based on the time series of satellite imagery. The\ndataset contains over 1 million images from more than 200\ncountries, annotated with 63 categories. There are two ver-\nsions available: fMoW-full, consisting of 4-band and 8-band\nmultispectral images, and fMoW-rgb, which is in JPEG format\nand contains RGB images converted from multispectral data.\nSince most foundation models only work with RGB images,\nwe utilized the fMoW-RGB dataset for our experiments.\nThese datasets provide a diverse range of remote sensing\nscenes and facilitate a thorough evaluation of the representa-\ntion capabilities of foundation models.\n2) Experiment Analysis: Except for the fMoW dataset, the\nother datasets were not initially divided into training and test\nsets. Therefore, we randomly divided these datasets into train-\ning and test sets using different training ratios. Specifically,\nwe selected 50% of the WHU-RS19 dataset, 20% and 50%\nof the AID dataset, 50% and 80% of the UCMerced dataset,\nand 10% and 20% of the RESISC dataset as the training\nsets, respectively. The remaining images were used as the test\nsets. To mitigate the impact of randomness, each separation\nwith a specific training ratio was performed three times. For\nthe fMoW dataset, we used the official training and test sets\nprovided by the dataset creators. The experimental results are\npresented in Table VI. In this experiment, four remote sensing\nfoundation models are selected for comparisons, including\nRSP [32], RVSA [17], SatMAE [20], ScaleMAE [22]. In\naddition, SwinV1 [60], SwinV2 [26], CLIP [10], ALBEF\n[27], BEiT-v3 [30], BLIP-2 [29], SAM [31] are involved in\nexperiments.\nAmong the foundation models for remote sensing, the\nRSP model has demonstrated promising results on multiple\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n18\nTABLE VI\nNUMERICAL RESULTS OF COMPARISONS WITH FOUNDATION MODELS ON FIVE SCENE CLASSIFICATION DATASETS WITH DIFFERENT TRAINING RATIO .\nTHE TOP THREE RESULTS ARE MASKED IN RED , GREEN AND BLUE .\nWHU-RS19 AID UCMerced RESISC fMoW\nMethod Name Tag 0.5 0.2 0.5 0.5 0.8 0.1 0.2\nSwin-T 83.2 ±1.23 82.4 ±0.23 86.2 ±0.06 93.8±0.55 95.0±0.67 67.7 ±0.31 71.7 ±0.20 36.3RSP ViTAEv2-S 86.1 ±1.13 86.6 ±0.40 89.6 ±0.22 95.7±0.59 96.1 ±0.40 74.5±0.33 77.7 ±0.32 37.8\nViT 66.6 ±1.77 65.2 ±0.76 74.2 ±0.23 69.9 ±0.84 79.3 ±1.21 57.2 ±0.50 64.2 ±0.55 34.4RVSA ViTAE 67.9 ±0.74 67.7 ±0.40 76.2 ±0.33 72.6 ±1.70 81.4 ±1.27 59.5 ±0.41 66.1 ±0.24 34.5\nSatMAE default 82.7 ±1.66 77.6 ±0.53 82.9 ±0.48 79.3 ±0.84 82.9 ±0.19 64.2 ±0.41 69.3 ±0.17 32.7\nScaleMAE default 84.7 ±0.37 86.5 ±0.39 90.6 ±0.31 75.6 ±1.83 83.4 ±1.46 75.8 ±0.16 80.6 ±0.12 43.8\nSwin-B-IN22K-224 85.6 ±1.03 77.6 ±0.45 82.1 ±0.42 88.2 ±1.27 90.4 ±0.30 67.0 ±0.52 72.0 ±0.19 34.2SwinV1 Swin-L-IN22K-224 89.4 ±0.52 87.6 ±0.16 90.7 ±0.13 91.9 ±0.90 93.8 ±0.97 77.9 ±0.48 81.3 ±0.39 38.9\nSwinV2-B-IN1K-256 90.9 ±0.58 89.7 ±0.37 92.4 ±0.38 93.5 ±1.18 95.4 ±0.30 80.1 ±0.19 83.8 ±0.32 40.6\nSwinV2-B-IN22K-192 90.1 ±0.32 87.3 ±0.07 90.4 ±0.42 92.7 ±1.13 95.1 ±0.11 76.4 ±0.57 79.9 ±0.23 38.8SwinV2\nSwinV2-L-IN22K-192 90.6 ±0.67 88.5 ±0.06 91.6 ±0.30 92.9 ±1.09 95.5 ±0.85 79.0 ±0.11 82.2 ±0.19 39.8\nViT-B/32 88.9 ±0.86 92.5 ±0.37 94.3 ±0.50 89.6 ±0.39 93.8 ±1.36 85.4 ±0.06 87.5 ±0.13 42.7\nViT-B/16 87.1 ±0.37 89.7 ±0.40 92.2 ±0.28 88.9 ±0.92 93.0 ±1.19 81.4 ±0.32 83.8 ±0.14 38.0\nViT-L/14 91.2±0.49 93.9 ±0.24 95.8 ±0.08 92.2±0.68 95.5 ±1.21 89.2±0.04 90.8 ±0.10 50.0CLIP\nViT-L/14-336 91.9±0.93 94.0 ±0.26 96.1 ±0.11 92.5±0.47 95.9±0.81 89.6 ±0.19 91.2 ±0.08 52.2\ndefault 84.9 ±0.49 83.8 ±0.31 86.8 ±0.38 87.5 ±1.28 92.1 ±0.85 72.3 ±0.47 76.1 ±0.16 34.2ALBEF 4M 89.3 ±0.28 89.0 ±0.38 90.7 ±0.49 88.6 ±1.13 92.8 ±1.11 76.6 ±0.16 79.6 ±0.09 35.4\nBEiT-v3 BEiT3-L-p16-224 77.6 ±1.26 78.2 ±0.63 83.7 ±0.53 83.6 ±0.70 88.6 ±1.01 69.2 ±0.29 73.9 ±0.17 31.7\nBLIP-2 default 93.1±0.58 95.6 ±0.17 96.8 ±0.02 93.9 ±0.91 96.8 ±0.74 91.8 ±0.24 93.1 ±0.06 53.0\nSAM ViT-B 74.1 ±0.43 58.7 ±0.36 64.4 ±0.36 66.6 ±0.88 71.6 ±1.81 39.9 ±0.22 44.3 ±0.28 -\nRSP BLIP-2CLIPScaleMAESatMAERVSA\n(a) UCMerced\n(b) RESISC\nRSP BLIP-2CLIPScaleMAESatMAERVSA\nFig. 13. Feature visualizations of six foundation models. (a) The UCMerced dataset. (b) RESISC dataset.\ndatasets. This can be attributed to the model’s utilization of\nMillionAID’s classification labels during pre-training, which\nenables better performance on scene classification datasets. On\nthe other hand, RVSA, SatMAE, and ScaleMAE are founda-\ntion models trained using label-free self-supervised algorithms.\nSatMAE exhibits superior performance on the WHU-RS19\nand UCM datasets. The ScaleMAE model achieves the best\nresults on AID, RESISC, and fMow datasets, while also\nperforming competitively with other remote sensing founda-\ntion models on WHU-RS19 and UCMerced datasets. The\nsuccess of ScaleMAE can be attributed to its consideration\nof different Ground Sampling Distances (GSDs) during the\ntraining process, leading to effective adaptation to datasets\nwith multiresolution characteristics.\nAmong the natural foundation models, the BLIP2 and CLIP\nseries models have achieved remarkable results. The BLIP2\nmodel utilizes the image encoder from CLIP, resulting in\nsimilar performance to CLIP. The CLIP model demonstrates\nexcellent representation capabilities for remote sensing im-\nages, thanks to the inclusion of remote sensing-related data\nin its dataset. Furthermore, compared to models trained with\nmasked image modeling techniques like Swin-Transformer,\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n19\nTABLE VII\nNUMERICAL RESULTS OF COMPARISONS WITH FOUNDATION MODELS ON FIVE SEMANTIC SEGMENTATION DATASETS . THE TOP THREE RESULTS ARE\nMASKED IN RED , GREEN AND BLUE .\nDFC22 Vaihingen MER GID-15 PotsdamModel Backbone Pretrained Data mIoU mAcc mIoU mAcc mIoU mAcc mIoU mAcc mIoU mAcc\nSatMAE ViT-L fMoW 46.3 58.7 69.3 77.0 64.1 73.0 88.1 92.0 75.7 83.8\nScaleMAE ViT-L fMoW 50.2 63.2 72.3 79.7 64.3 73.5 90.7 93.5 76.9 84.5\nViT-B kvdiff 49.0 61.1 71.6 79.7 64.3 74.0 92.6 95.6 77.6 84.7RVSA ViTAE-B kvdiff MillionAID 49.3 61.4 71.4 79.3 64.4 74.5 92.2 95.7 77.7 84.6\nDeeplabv3+ R50 IN1K 48.1 59.6 73.1 80.2 63.9 74.6 94.6 96.7 78.3 84.9\nConvNeXt-T 50.1 63.4 73.7 80.7 64.0 74.7 91.6 95.3 79.0 85.9\nConvNeXt-S 49.0 61.3 73.5 81.1 64.5 74.8 92.1 95.4 79.3 86.0\nConvNeXt-B\nIN1K\n49.7 62.8 74.2 81.4 64.9 75.4 94.1 96.7 78.9 85.5ConvNeXt\nConvNeXt-B IN22K 50.0 63.0 75.3 82.3 65.0 75.1 94.5 96.9 79.8 86.3\nV AN V AN-B IN1K 49.4 61.6 73.9 81.1 65.3 75.7 94.2 96.5 79.9 86.4\nMViTv2-T 50.1 62.4 75.4 83.7 68.7 77.0 93.0 95.6 79.0 85.7\nMViTv2-S 50.5 62.6 75.7 82.6 68.3 77.0 93.5 96.3 79.3 86.4MViTv2\nMViTv2-B\nIN1K\n50.5 62.5 75.6 82.9 67.2 76.7 94.3 96.5 80.0 86.3\nSegFormer SegFormer-B2 IN1K 40.0 52.0 62.0 70.9 56.1 65.4 81.3 87.5 73.1 81.6\nViT-B 49.3 60.8 71.4 78.8 66.5 75.0 92.5 95.2 77.9 84.9\nViT-L IN1K 48.9 61.2 71.2 78.3 66.3 74.6 92.4 95.0 77.4 84.6ViT\nViT-L IN22K 48.3 60.5 70.7 78.2 65.1 74.1 89.8 92.8 77.3 85.1\nSwin-T 50.2 62.7 73.8 80.9 64.1 74.5 92.0 95.3 79.1 86.3\nSwin-S 51.1 64.1 74.2 81.4 65.9 74.9 94.3 96.8 79.3 85.9\nSwin-B\nIN1K\n49.9 62.6 73.2 80.4 64.5 75.4 95.0 97.1 79.3 86.2SwinV1\nSwin-B IN22K 51.6 65.6 75.1 81.9 66.7 75.9 94.8 97.1 80.1 86.8\nSwinv2-T-p4w8 50.6 63.2 73.5 80.8 64.7 74.9 94.0 96.5 79.0 85.6\nSwinv2-S-p4w8 50.3 62.1 73.4 80.7 65.7 76.3 94.4 96.8 79.1 85.7\nSwinv2-B-p4w8\nIN1K\n50.1 62.4 73.6 80.4 65.4 75.7 94.7 96.8 79.4 85.7SwinV2\nSwinv2-B-p4w12 IN22K 50.7 63.3 74.9 81.9 66.0 76.1 95.0 96.9 79.3 85.9\nCLIP CLIP-ViT-B WebImageText 49.2 60.2 74.1 80.8 67.9 76.3 - - - -\nDenseCLIP DenseCLIP-ViT-B WebImageText 49.9 63.9 74.1 81.0 67.2 76.5 - - - -\nfoundation models trained using image-language pairs training\nplace greater emphasis on capturing high-level semantic infor-\nmation. As a result, they exhibit superior representation abil-\nities. On the other hand, the underwhelming results of SAM\ncan be attributed to its design for segmentation tasks. The\nlarge feature map causes background information to interfere\nwith target information during average pooling, consequently\nimpacting classification outcomes.\nMoreover, when comparing all foundation models, we ob-\nserve that natural foundation models consistently outperform\ncurrent remote sensing foundation models. Even when Swin-\nTransformer is trained without remote sensing images, it still\ndemonstrates superiority on WHU-RS19, UCM, AID, and\nRESISC datasets. Further more, we use the T-SNE algorithm\nto visualize features as shown in Fig. 13. The RSP shows\na good features in UCMerced dataset, but perform worse in\nthe complex dataset, RESISC. The CLIP and BLIP-2 perform\nwell across these datasets. This further corroborates the notion\nthat large-scale pre-trained natural foundation models remain\nhighly competitive in the field of remote sensing. Additionally,\nthis insight inspires us to leverage natural foundation models to\nenhance the development of remote sensing foundation models\nin terms of efficiency and performance.\nB. Semantic Segmentation\nSemantic segmentation, also known as land-cover classifi-\ncation in remote sensing, differs from scene classification as it\ninvolves classifying pixels of an image on a pixel level. This\nallows for the evaluation of the local representation capabilities\nof foundation models. In our experiments, we combine the\nUperNet [245] with the foundation models and fine-tune them\nusing five semantic segmentation datasets to compare their\nperformance.\n1) Datasets: We conducted experiments on semantic seg-\nmentation using a total of five datasets.\nDFC22 [246]: The DFC22 dataset is based on the\nMiniFrance (MF) dataset [247] and is designed for train-\ning semisupervised semantic segmentation models for land\nuse/land cover mapping. It contains 766 labeled images with a\nresolution of approximately 2000 ×2000. In our experiments,\nwe compared the fine-tune performance of each foundation\nmodel. We uniformly resized all images to 512 × 512 pixels\nand randomly split them into training and test sets with a 4:1\nratio. No unlabeled images were used.\nVaihingen [248]: The Vaihingen dataset, released by the\nInternational Society for Photogrammetry and Remote Sensing\n(ISPRS) Commission, consists of remote sensing images of\nthe Vaihingen village. It is divided into 33 patches, with 17\npatches used as the test set and the remaining 16 patches as\nthe training set. The patch sizes range from 1996×1995 pixels\nto 3816×2550 pixels. In our experiments, we resized all data\nto 512 × 512 pixels with an overlap size of 128 pixels.\nMER [249]: The Mars-Seg (MER) dataset consists of 4155\nRGB images and 1024 grayscale images, capturing the Martian\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n20\nlandscape. The RGB images have a resolution of 560 × 500,\nwhile the grayscale images have a resolution of 1024 × 1024.\nTo conduct our experiments, we uniformly resized the MER\nimages to 512 ×512 pixels. We randomly split both the RGB\nand grayscale images into training and test sets with a 4:1\nratio.\nGID-15 [250]: The Gaofen Image dataset (GID-15) is\na large dataset for land use and land cover classification. It\ncontains ten high-quality Gaofen-2 images from different cities\nin China, with a resolution of 7200 × 6800 pixels. In our\nexperiments, we applied the sliding window method with a\nwindow size of 512 × 512 and a stride size of 384 to extract\npatches from the images. The resulting patches were then\nrandomly divided into training and test sets using a 4:1 ratio.\nPotsdam [251]: The ISPRS Potsdam dataset comprises\n38 high-resolution aerial images with a resolution of 0.5m. It\nis annotated with 6 categories, including impervious surfaces,\nbuildings, low vegetation, trees, cars, and clutter. The dataset\nis divided into 24 training images and 14 testing images, each\nwith a size of 6000×6000 pixels. In our experiments, we used\nthe sliding window method with a window size of 512 × 512\nand a stride size of 384 to extract image patches.\n2) Experiment Analysis: The experiment results are pre-\nsented in Table VII. In this experiment, three remote sensing\nfoundation models are selected for comparisons, including\nSatMAE [20], ScaleMAE [22] and RVSA [17]. In addition,\nDeeplabv3+ [252], ConvNeXt [253], V AN [254], MViTv2\n[255], SegFormer [256], ViT [58], SwinV1 [60], SwinV2\n[257], CLIP [10], DenseCLIP [258] are involved in experi-\nments.\nFrom the table, we can observe that different remote sensing\nfoundation models yield varying effects on semantic segmen-\ntation. ScaleMAE performs well on DFC22 and Vaihingen\ndatasets, achieving mIoU scores of 50.2 and 72.3, respectively.\nRVSA shows better performance on the larger datasets GID-\n15 and Potsdam. The three remote sensing foundation models\nperform similarly on the MER dataset.\nAmong the natural base models, Swin-Transformer achieves\nthe best results on DFC22, GID-15, and Potsdam datasets. This\ncan be attributed to two factors: its ability to represent remote\nsensing images and the structure of the model. Our scene\nclassification experiments have already demonstrated Swin-\nTransformer’s strong representation capability for remote sens-\ning images. In semantic segmentation, Swin-Transformer pro-\nvides multi-scale features, effectively improving accuracy.\nMViTv2 also performs admirably, particularly excelling on\nthe Vaihingen and MER datasets. It is worth noting that\nMViTv2 is only pre-trained on ImageNet 1K, indicating that\nits performance gains are primarily derived from its excellent\nmulti-scale design and improved representation ability of local\nfeatures. Furthermore, the Tiny and Small versions of the\nMViTv2 series models are also competitive. This highlights\nthe fact that the number of parameters cannot solely determine\nthe performance of a model when it is applied to a specific\nremote sensing dataset.\nBy comparing all the foundation models, it becomes evident\nthat natural foundation models outperform current remote\nsensing foundation models in terms of performance. This can\nbe attributed to two main factors. Firstly, the current remote\nsensing foundation models are pre-trained on the fMoW and\nMillionAID datasets, which do not comprehensively cover\nall remote sensing datasets. Consequently, these models do\nnot exhibit significant advantages in the local representation\nsegmentation. Additionally, the current foundation models\npredominantly utilize ViT as the underlying structure, which\nalso affects their performance. Therefore, designing a remote\nsensing foundation model requires not only a well-suited pre-\ntraining algorithm but also an excellent multi-scale structure\nthat enables the model to meet the requirements of diverse\napplications.\nC. Object Detection\nObject detection is a crucial task in remote sensing interpre-\ntation, as it necessitates the model’s ability to handle objects\nwith significant size variations while also performing accurate\nclassification. In our experiments, we integrate the Oriental-\nRCNN [259] with the foundation models and fine-tune them\non two widely used remote sensing object detection datasets\nto assess their performance.\n1) Datasets: We utilized two object detection datasets for\nour experiments: DOTA v1.0 and DIOR-R.\nDOTA v1.0 [260]:DOTA is a renowned dataset widely used\nfor rotated object detection in the remote sensing domain. We\nemployed DOTA v1.0 for evaluation purposes. This dataset\nconsists of 15 common categories, 2,806 images, and 188,282\ninstances, gathered from various sensors and platforms. The\nimage sizes in DOTA v1.0 range from 800 × 800 to 4,000 ×\n4,000 pixels. The labels for the training and validation sets\nare publicly available. The categories in DOTA v1.0 include\nplane (PL), baseball-diamond (BD), bridge (BG), ground-\ntrack-field (GTF), small-vehicle (SV), large-vehicle (LV), ship\n(SH), tennis-court (TC), basketball-court (BC), storage-tank\n(ST), soccer-ball-field (SBF), roundabout (RA), harbor (HB),\nswimming-pool (SP), and helicopter (HC). For our experiment,\nwe trained the model on the train set and evaluated its\nperformance on the validation set.\nDIOR-R [261]: The DIOR-R dataset is an extension of the\nprevious DIOR dataset [262]. It comprises 23,463 images and\n192,518 instances, encompassing a wide range of scenes and\n20 common object classes. The images in DIOR-R have a fixed\nsize of 800 pixels. The object categories in DIOR-R include\nairplane (APL), airport (APO), baseball field (BF), basketball\ncourt (BC), bridge (BR), chimney (CH), dam (DAM), express-\nway service area (ESA), expressway toll station (ETS), golf\nfield (GF), ground track field (GTF), harbor (HA), overpass\n(OP), ship (SH), stadium (STA), storage tank (STO), tennis\ncourt (TC), train station (TS), vehicle (VE), and windmill\n(WM).\n2) Experiment Analysis: DOTA: The experimental results\nfor DOTA are presented in Table VIII. In this experiment,\nResNet [263], PVTv2 [264], Poolformer [265], V AN [254],\nMViTv2 [255], ConvNeXt [253], SwinV1 [60], SwinV2 [257]\nand RVSA [17] are involved in experiments.\nOverall, there is no significant gap between the foundation\nmodels designed for natural images and those tailored for\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n21\nTABLE VIII\nNUMERICAL RESULTS OF COMPARISONS WITH FOUNDATION MODELS ON DOTA V1.0 DATASETS WITH DIFFERENT TRAINING RATIO . THE TOP THREE\nRESULTS ARE MASKED IN RED , GREEN AND BLUE .\nModel Pretrained Data mAP PL BD BG GTF SV LV SH TC BC ST SBF RA HB SP HC\nR50 R50 IN1K 66.6 88.7 73.5 45.3 63.8 61.3 82.8 87.7 90.4 61.2 62.0 56.9 61.8 63.5 54.5 45.8\npvtv2-b0 71.9 89.4 75.7 51.9 76.7 66.2 84.6 88.5 90.6 67.7 69.2 63.9 66.1 70.9 57.6 58.9\npvtv2-b1 74.6 89.6 83.1 55.5 77.9 68.9 85.7 89.2 90.7 75.3 69.4 65.9 70.1 74.9 59.5 62.8\npvtv2-b2 74.6 89.9 84.6 58.1 80.0 68.2 86.0 89.3 90.7 74.8 69.8 64.2 71.0 76.0 60.1 56.6\nPVTv2\npvtv2-b3\nIN1K\n74.6 89.9 78.9 59.1 78.8 67.6 86.1 89.4 90.6 76.8 70.8 65.4 69.0 76.1 60.6 60.1\npoolformer-s12 70.7 88.8 77.4 48.8 77.5 59.3 83.4 88.5 90.5 67.2 62.4 62.2 67.2 66.6 58.0 61.9\npoolformer-s24 73.5 89.6 77.5 56.6 78.6 65.5 84.5 88.8 90.7 74.0 62.4 66.4 66.3 75.1 58.8 67.3\nPoolformerpoolformer-s36\nIN1K\n72.8 89.6 77.8 50.6 74.7 60.1 84.2 88.9 90.7 72.2 62.6 63.4 71.4 74.2 59.7 71.7\nvan-b0 72.7 89.2 77.6 51.4 77.6 65.5 83.6 88.2 90.5 73.6 69.4 64.5 65.8 73.6 58.2 61.7\nvan-b1 73.7 89.6 76.6 53.0 81.0 63.4 84.8 89.0 90.7 74.5 69.5 67.0 71.7 75.2 58.8 60.6\nvan-b2 74.6 89.8 84.0 50.7 79.4 67.7 85.7 88.9 90.7 74.6 62.7 71.2 67.8 76.7 59.2 70.6\nvan-b3 74.9 89.8 77.6 55.7 81.6 62.0 85.0 89.2 90.8 75.0 70.4 68.9 66.7 76.5 62.4 72.5\nV AN\nvan-b4\nIN1K\n74.0 89.8 77.1 52.8 83.2 62.3 79.2 89.1 90.7 76.9 62.7 70.6 67.5 76.4 60.0 71.1\nMViTv2-T 75.2 89.7 86.4 57.6 82.0 68.3 86.5 89.3 90.7 74.4 70.2 69.1 73.5 75.4 59.6 55.7\nMViTv2MViTv2-S\nIN1K\n75.1 90.0 84.9 59.0 77.9 69.2 86.6 89.3 90.6 74.2 70.1 67.4 74.4 76.2 55.2 61.7\nConvNext-T 74.1 89.4 81.6 54.0 81.2 66.2 84.9 89.2 90.8 74.9 69.8 64.7 71.0 74.2 57.5 62.7\nConvNext-S 73.9 89.5 84.1 57.7 79.4 61.1 85.0 89.2 90.8 75.2 62.4 65.7 73.0 76.0 59.3 60.5\nConvNext-B 75.0 89.7 77.6 55.3 81.1 63.2 85.7 89.2 90.8 76.8 69.5 69.4 72.8 76.5 58.7 68.6\nConvNext-L\nIN1K\n75.6 89.8 77.8 59.1 82.2 63.4 85.4 89.2 90.8 77.0 70.1 69.6 73.3 76.5 59.2 70.6\nConvNext-B 75.2 89.8 78.1 58.6 83.5 70.3 85.9 89.4 90.8 76.8 70.4 71.8 73.6 76.6 59.1 52.8\nConvNext-L 76.3 89.8 78.0 60.0 82.0 70.6 86.0 89.4 90.8 76.0 70.7 69.2 74.3 76.9 60.0 70.7\nConvNext\nConvNext-XL\nIN22K\n75.6 90.1 78.7 60.5 81.0 63.0 85.4 89.3 90.8 76.8 70.8 70.3 73.3 76.4 59.5 68.8\nSwin-T 74.2 89.6 77.1 57.3 81.8 66.2 85.3 89.1 90.7 73.2 62.3 68.3 73.7 74.3 57.0 67.2\nSwin-S 75.1 89.5 85.4 58.4 76.1 61.9 85.2 88.8 90.6 74.7 69.8 67.1 75.4 75.4 57.7 70.9\nSwin-B\nIN1K\n74.9 89.7 78.3 57.3 80.4 61.2 85.5 88.8 90.7 73.9 69.8 68.8 68.0 76.0 61.2 73.5\nSwin-B 75.5 89.7 86.5 59.1 80.7 67.2 86.0 88.9 90.7 74.8 70.2 69.7 67.7 76.1 58.5 66.9\nSwin-B w12 76.0 89.7 84.8 57.0 78.1 66.9 86.0 89.0 90.6 73.3 70.1 69.2 73.5 75.4 60.0 76.3\nSwin-L 75.8 89.9 83.2 59.3 80.1 67.6 86.1 89.2 90.7 76.1 70.3 68.8 67.7 76.2 58.9 72.1\nSwinV1\nSwin-L w12\nIN22K\n76.5 89.7 86.2 58.1 78.8 66.8 86.0 89.1 90.7 77.1 70.2 69.3 75.3 76.2 58.9 75.4\nSwinv2-T 73.7 89.8 77.6 56.7 81.3 60.2 85.1 89.3 90.7 72.6 62.7 67.9 71.6 75.2 58.0 67.3\nSwinv2-S 73.0 89.7 78.7 57.4 75.9 61.7 79.2 89.0 90.7 73.3 62.7 66.5 68.3 76.1 59.4 66.3\nSwinv2-B\nIN1K\n73.9 89.9 77.8 58.5 76.4 60.2 79.3 88.9 90.7 75.9 62.9 71.3 67.2 76.2 59.3 73.7\nSwinv2-B w12 74.9 89.9 77.4 56.2 80.6 61.5 86.4 89.3 90.8 75.9 70.3 69.9 67.5 76.3 60.4 71.0\nSwinV2\nSwinv2-L w16\nIN22K\n75.0 90.0 77.0 53.2 82.6 64.3 79.4 89.3 90.8 76.0 70.3 71.9 68.7 76.6 60.2 74.3\nViT-B 74.8 90.1 71.9 56.9 73.3 65.7 84.4 88.6 90.9 69.1 89.0 69.5 65.2 76.6 66.0 64.2\nViT-B kvdiff 75.2 90.1 72.1 57.1 74.5 65.9 84.6 88.3 90.8 71.8 88.7 71.3 68.1 76.0 65.1 63.6\nViTAE-B 76.3 90.2 74.6 56.5 73.6 65.5 84.9 88.4 90.9 71.6 88.8 77.8 68.6 76.0 67.0 70.0\nRVSA\nViTAE-B kvdiff\nMillionAID\n74.0 90.1 71.4 54.7 75.3 65.0 85.1 89.4 90.8 69.9 88.2 70.9 62.7 75.5 67.0 53.4\nremote sensing. Swin-Transformer achieved an mAP of 76.5,\nwhile ConvNext and RVSA both achieved 76.3 mAP. However,\nSwin-Transformer and ConvNext yielded comparable results\nto the ViTAE-Base model in RVSA when using a large\nparameter model. Consequently, RVSA demonstrates superior\nperformance in remote sensing target detection. This can\nbe attributed to the incorporation of a rotating variable-size\nwindow attention method in the RVSA’s structural design,\neffectively enhancing the model’s accuracy in object detection.\nFrom a category standpoint, Swin-Transformer exhibits a\nsignificant advantage over RVSA’s ViTAE-B in the baseball-\ndiamond and roundabout categories, achieving 11.6 and 6.7\nmAP scores higher, respectively. These two object classes\nare less sensitive to rotating boxes, rendering the design of\nRVSA less beneficial in terms of performance improvement.\nConversely, RVSA surpasses Swin-Transformer by 8.5 mAP\nin the soccer-ball-field category. Therefore, incorporating the\nfoundation model can enhance the accuracy of the downstream\napplications. Certain modules specifically designed for down-\nstream tasks remain crucial even in foundation models and can\neffectively improve model performance.\nDIOR-R: Similarly, we can observe similar patterns in the\nDIOR-R dataset experiments as in the DOTA dataset. The\nexperimental results on the DIOR-R dataset are presented\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n22\nTABLE IX\nNUMERICAL RESULTS OF COMPARISONS WITH FOUNDATION MODELS ON DIOR-R DATASETS WITH DIFFERENT TRAINING RATIO . THE TOP THREE\nRESULTS ARE MASKED IN RED , GREEN AND BLUE .\nMethod Model Pretrained Data mAP APL APO BF BC BR CH DAM ESA ETS GF\nR50 R50 IN1K 52.7 60.9 14.7 71.0 80.5 27.2 72.1 18.8 56.5 48.0 56.3\npvtv2-b1 63.5 70.3 35.8 80.1 81.4 40.5 72.5 29.1 77.9 67.3 71.6PVTv2 pvtv2-b2 IN1K 66.7 71.8 46.9 79.9 81.3 44.2 80.0 36.5 79.8 70.0 77.1\nV AN van-b2 IN1K 66.2 79.6 42.3 80.5 81.4 44.6 80.1 36.1 79.2 70.5 69.4\nMViTv2-T 67.1 79.6 49.8 80.5 81.3 44.3 80.5 37.4 79.1 68.8 77.6MViTv2 MViTv2-S IN1K 68.1 80.7 50.3 80.6 81.1 44.3 81.1 37.3 79.9 70.4 79.2\nConvNext-T 63.6 71.5 29.8 79.7 81.4 41.1 72.5 27.0 75.8 67.7 67.9\nConvNext-S 66.2 71.1 40.6 79.5 81.3 43.6 80.4 36.5 79.3 69.3 75.9\nConvNext-B 67.0 71.9 42.1 80.2 88.8 44.5 72.6 36.5 84.1 70.8 78.6\nConvNext-L\nIN1K\n68.1 80.9 42.6 80.3 89.4 44.8 80.4 39.2 79.4 71.2 78.4\nConvNext-B 68.5 80.7 50.0 80.3 81.3 45.6 81.3 37.9 85.2 71.5 77.6\nConvNext\nConvNext-L IN22K 70.1 81.4 50.9 80.6 89.6 47.4 81.3 38.6 86.8 77.7 78.5\nSwin-T 65.2 79.9 37.7 80.1 88.0 38.9 72.5 35.1 77.7 68.1 75.3\nSwin-S 66.8 71.5 44.3 80.5 89.2 44.3 72.7 36.5 80.0 70.5 78.4\nSwin-B\nIN1K\n67.1 71.9 43.3 80.0 89.0 45.1 80.3 38.3 79.7 70.8 78.2\nSwin-B 69.2 80.5 50.9 80.8 88.7 45.4 81.3 41.2 87.5 71.1 77.8\nSwin-B w12 69.0 80.8 51.0 80.8 81.2 47.1 81.4 41.2 87.4 76.0 78.6\nSwin-L 70.1 80.7 52.8 81.1 89.0 46.4 81.6 40.8 88.6 76.2 79.1\nSwinV1\nSwin-L w12\nIN22K\n69.8 79.7 58.2 80.9 81.2 46.4 81.4 40.6 88.4 75.7 78.8\nSwinv2-T 64.1 71.1 33.7 79.0 81.4 41.8 72.6 34.3 78.3 67.1 76.4\nSwinv2-S 66.9 80.2 44.7 79.7 81.3 44.2 79.3 38.4 80.0 70.4 78.5\nSwinv2-B\nIN1K\n66.6 72.1 41.9 80.6 88.4 45.0 72.7 39.0 79.8 70.9 77.4\nSwinv2-B w12 67.2 72.3 43.9 80.4 89.2 45.5 80.9 37.2 85.6 71.6 76.3\nSwinV2\nSwinv2-L w16 IN22K 67.8 80.5 43.9 80.8 81.3 45.3 81.0 34.4 86.2 71.5 77.2\nViT-B 67.3 80.4 41.3 80.6 81.3 46.1 72.6 32.7 86.7 70.0 72.6\nViT-B kvdiff 67.0 80.4 40.7 80.5 81.4 46.4 72.6 29.1 84.4 69.2 74.1\nViTAE-B 69.9 81.2 50.3 80.9 86.9 50.3 79.1 36.5 88.2 73.8 76.6RVSA\nViTAE-B kvdiff\nMillionAID\n68.5 80.3 41.0 80.1 81.3 47.3 77.4 36.7 87.3 70.8 75.6\nGTF HA OP SH STA STO TC TS VE WM\nR50 R50 IN1K 72.3 16.9 45.9 80.3 49.8 61.3 81.0 38.8 41.5 60.8\npvtv2-b1 83.1 33.6 56.8 81.0 78.5 61.5 81.5 54.8 46.8 65.2PVTv2 pvtv2-b2 IN1K 82.7 41.6 58.5 81.1 79.8 68.8 81.3 59.0 48.1 65.3\nV AN van-b2 IN1K 83.5 40.7 58.2 81.1 76.3 62.1 81.4 62.9 49.3 65.6\nMViTv2-T 83.2 38.8 58.5 81.1 80.8 60.9 81.4 64.0 48.3 65.2MViTv2 MViTv2-S IN1K 84.0 41.9 58.6 81.1 81.4 68.7 81.4 64.2 49.3 65.4\nConvNext-T 84.1 36.7 55.6 81.0 80.3 70.7 81.3 56.1 47.2 64.9\nConvNext-S 84.2 41.1 58.4 81.1 76.9 70.5 81.3 63.2 44.0 66.1\nConvNext-B 84.4 41.3 58.6 81.1 77.4 70.0 81.4 64.1 44.1 66.5\nConvNext-L\nIN1K\n84.2 43.2 58.8 81.2 78.2 70.5 81.4 62.3 49.9 66.2\nConvNext-B 83.7 43.9 59.7 81.1 77.0 70.6 81.4 65.9 49.2 65.9\nConvNext\nConvNext-L IN22K 83.2 45.3 59.7 89.3 76.9 70.7 81.5 67.1 50.0 66.2\nSwin-T 83.2 35.0 56.1 81.0 79.8 70.7 81.5 55.6 43.3 63.8\nSwin-S 83.8 42.3 57.9 81.1 77.9 70.6 81.4 65.8 43.6 64.2\nSwin-B\nIN1K\n84.7 43.1 58.5 81.0 82.9 62.6 81.4 62.0 43.7 65.1\nSwin-B 83.3 44.5 59.1 81.0 77.3 71.0 81.4 66.9 48.7 65.0\nSwin-B w12 82.8 43.7 59.2 81.0 76.7 70.8 81.4 66.0 47.8 65.2\nSwin-L 83.6 44.0 60.0 81.0 82.9 71.1 81.4 65.7 49.5 65.6\nSwin\nSwin-L w12\nIN22K\n83.4 45.4 59.6 81.0 82.6 70.9 81.3 66.8 49.1 65.2\nSwinv2-T 81.7 37.5 56.1 80.9 77.2 70.4 81.4 54.6 43.4 63.9\nSwinv2-S 82.6 40.0 58.5 81.0 77.3 70.0 81.5 62.8 43.7 64.5\nSwinv2-B\nIN1K\n83.1 40.4 58.7 81.0 77.4 70.4 81.4 63.3 43.8 65.0\nSwinv2-B w12 76.0 42.9 59.1 81.0 77.7 70.6 81.5 62.7 43.9 65.3\nSwinV2\nSwinv2-L w16 IN22K 82.7 44.0 59.1 81.2 82.8 70.3 81.5 63.5 44.0 65.2\nViT-B 82.1 41.3 60.3 81.2 80.3 71.1 89.9 62.2 49.7 64.1\nViT-B kvdiff 81.9 41.3 59.7 81.2 81.6 70.5 89.9 61.0 49.8 64.8\nViTAE-B 83.3 44.5 60.8 81.2 83.6 71.1 89.9 63.2 50.3 65.4RVSA\nViTAE-B kvdiff\nMillionAID\n82.3 44.9 60.4 81.2 82.8 70.8 89.4 66.7 50.2 62.8\nin Table IX. Swin-Transformer, ConvNext, and RVSA all\nachieved great performance. RVSA demonstrated excellent\nperformance in both the tennis court and vehicle categories.\nSwin-Transformer employs a multi-scale Transformer struc-\nture, enabling better detection of super large airport. Con-\nvNext, with its convolutional structure, exhibited superior\ndetection performance for ships.\nIn this section, we conduct experiments focusing on three\nessential aspects: global representation (scene classification),\nlocal representation (semantic segmentation), and object local-\nization (object detection). Our findings from these experiments\nreveal that the foundational models trained with natural images\nexhibit comparable performance to the ones developed for\nremote sensing. The CLIP model stands out for its remarkable\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n23\nDiversity \nStructure\nSpiking\nRemote Sensing Foundation Model\nSparsity\nSelectivity\nPerception\nDirectionality\nHebbian\nError-correction\nLearning\nCompetitive\nMemory\nForgetting\nCognition\nReasoning\nGeometry\nBrain-Inspired Properties \nPriori Knowledge \nSpectral Signature\n Road Network \nMulti-Modal Data\nImage Video\nPoint Cloud Text\nHow many baseball \ndiamonds are there in this \npicture?\nSeveral buildings with a \nswinmming pool.\nTerrain Geographical Location\nFig. 14. The overall framework of the Brain-inspired RSFM.\nperformance in scene classification; however, it doesn’t per-\nform as well in semantic segmentation. At present, no single\nfoundation model can excel across all these applications in\nour experiments. This underscores the necessity for further\ndevelopment of foundation models to suit a wide array of\napplications in the remote sensing field.\nVI. B RAIN -INSPIRED REMOTE SENSING FOUNDATION\nMODEL\nA. Overall architecture of the Brain-inspired RSFM\nThe foundation model aims to address multiple modalities\nand tasks with a unified approach. However, based on the\nexperiments conducted in the previous section, we have iden-\ntified several shortcomings in the current foundation models’\nperformance. Particularly, there is a lack of remote sensing\nfoundation models that can effectively handle multimodal\ndata. These existing data-driven foundation models still have\nlimitations in terms of data size, model structure, and learning\nstrategies.\nTo address these challenges, we propose a brain-inspired\nframework for a remote sensing foundation model, as illus-\ntrated in Fig. 14. This framework aims to integrate multi-\nmodal data in remote sensing, such as image, video, point\ncloud and text, and represent them uniformly for data-driven\nlearning. Moreover, it incorporates prior knowledge, such as\nobject spectral signature, road network information, terrain and\ngeographical location, into the model for knowledge-driven\nlearning. By combining both data-driven and knowledge-\ndriven approaches, we expect to enhance the model’s perfor-\nmance and adaptability. More importantly, the brain-inspired\nproperties can guide us to construct the model, represent\nthe data, build learning algorithms and process reasoning.\nThus, in the following sections, we will delve into the key\nbrain-inspired properties, focusing on four aspects: structure,\nperception, learning, and cognition.\nInput Spikes\nx1\n∑ F(· ) yx2\nxn\nw1\nw2\nwn\n......\nδ1\n∑δ2\nδn\nw1\nw2\nwn\n......\nInput Feature Output \nFeature\nOutput \nSpikes\nTime\nTransfer \nFunction\nIntegrate & \nLeakyUmem\n（a）\n（b）\nFig. 15. The illustration of (a) a classical artificial neural unit and (b) spiking\nneurons. The image is re-produced following [266].\nB. Basic properties of Brain-inspired RSFM\n1) Structure: The foundation of a functional model lies\nin its structure. Just as the human brain possesses a complex\narchitecture to enable its comprehensive functions, we seek to\ndesign a model with similar characteristics. In this section, we\nexplore the brain’s spiking structure, diversity, and geometry.\nSpiking: The human brain, consisting of 86 billion neurons,\ncommunicates through highly structured connections called\nsynapses [267]. Neurons exchange information in a sparse\nand asynchronous manner through discrete action potentials,\nor “spikes” [268]. To emulate this essential characteristic\nof the human brain, the spiking neural network (SNN) was\nintroduced (as shown in Fig. 15). Unlike traditional neural\nnetworks, SNN processes sparse spatio-temporal signals by\nsimulating the excitation and inhibition of neurons. Spiking\nneurons receive spikes developing the membrane potential\nthrough time following differential equations. A spike is\nemitted when the membrane potential crosses a threshold\n[266]. This approach offers advantages in terms of analog\ncomputing, low power consumption, fast reasoning, event-\ndriven processing, online learning, and large-scale parallelism,\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n24\nas it has demonstrated superior performance [269].\nDiversity: The brain’s composition is not uniform; it relies\non various neuron types to achieve its complex functions. The\ncerebral cortex, for example, is organized into four major\nstructures: the occipital lobe, temporal lobe, parietal lobe,\nand frontal lobe [270]. Recent studies have revealed a rich\ndiversity of neurons in the brain. For instance, Yao et al.\n[271] analyzed over 500,000 individual cells in the mouse\nprimary motor cortex and identified 56 highly replicable neural\ntypes. Ed Lein et al. demonstrated the abundance of neuronal\nspecies in the cerebral cortex [272] using techniques such as\npatch clamp. This diversity is essential for the realization of\ndifferent modalities and functional differentiations in the brain.\nSimilarly, our neural network needs to incorporate a variety\nof neuron types to facilitate the realization of different tasks\nand modalities in the model.\nGeometry: Traditionally, the brain’s complex functions\nwere thought to arise from intricate interregional connections.\nHowever, neural field theory suggests that the brain’s geom-\netry may represent a more fundamental dynamical constraint.\nStudies by Caucheteux et al. [273] using human magnetic reso-\nnance imaging data demonstrated that cortical and subcortical\nactivity can be understood as arising from the excitation of\nfundamental resonant modes of the brain’s geometry (i.e., its\nshape). This implies that geometric constraints play a crucial\nrole in shaping brain functions, in addition to neural connec-\ntions. Therefore, our model should also take into account the\npotential role of geometric constraints in shaping functions.\nDiscussion: Spiking neural networks have been studied a lot\nin deep learning. Yao et al. [274] proposed attention spiking\nneural network. It integrates the attention mechanism into\na million-scale spiking neural network. On the ImageNet-\n1K dataset, it has achieved performance equivalent to that\nof traditional artificial neural networks for the first time,\nand its theoretical energy efficiency is 31.8 times that of\nartificial neural networks with the same structure. Therefore,\nbrain-inspired spiking neural networks have many potentials.\nFor large-scale basic models, spiking neural networks will\nhave more potential. However, neuron diversity and geometric\nconstraints have not been studied in the current model. The\nmodel’s functional design of different neurons and geometric\nconstraints combined with dynamics research will help im-\nprove the robustness of the basic model.\n2) Perception: Perception is the process through which\nhumans obtain information from the external world. For the\nbrain, this input information includes visual, auditory, tactile,\nand other sensory data. Similarly, in remote sensing, different\ndata such as visible light and SAR provide multimodal in-\nformation. To design an effective foundation model, we need\nto mimic the human brain’s characteristics such as sparsity,\nselectivity, and directionality to enhance the model’s efficiency\nin perceiving information.\nSparsity: The brain exhibits a hierarchical, sparse, and peri-\nodic structure [275]. Sparsity plays a crucial role in biological\nbrains as it allows for the representation and processing of\ninformation using only a small number of activated neurons\nor saliences. This sparsity is a property of neural coding\nthat enhances the brain’s efficiency, robustness, and flexibility.\nStudies have shown that sparse representations in the cerebral\ncortex’s V1 may satisfy the optimality criteria of information\ntheory [276]. As we move to higher levels of neurons, the\nreceptive fields become larger, and sparsity becomes stronger.\nRecent research has also indicated that neural circuits are\norganized in a sparse yet efficient manner. Higher levels of\nintelligence have been associated with more direct informa-\ntion processing and less cortical activity during reasoning,\nhighlighting the importance of sparsity in the brain’s efficient\nperception [275].\nSelectivity: Selectivity, often referred to as attention mech-\nanism, is a key feature of the brain’s ability to focus on\nspecific objects and control areas of attention [277]. The brain\nreceives a vast amount of information simultaneously but\ncannot process it with equal priority. Therefore, it employs\nselective attention to filter and prioritize information [278].\nSelective attention exists widely in the human visual system\nand is regulated by both bottom-up and top-down mechanisms.\nBottom-up selectivity responds to salient stimuli from the\nenvironment, such as changes in target brightness or motion.\nOn the other hand, top-down selectivity allows humans to\nprocess relevant information based on current behavior and\nintentions while ignoring irrelevant information, forming a\nclose integration between attention and cognition [279].\nDirectionality: Directionality is the brain’s ability to per-\nceive its own position and orientation. The brain has az-\nimuthal and oblique angle cells that provide orientation and\nposition information. When the head faces a particular di-\nrection, the corresponding direction cells are activated [280].\nNeural coding patterns of egocentric spatial orientation have\nbeen discovered in the medial temporal lobe of the human\nbrain, supporting vector representations of egocentric spaces\nby encoding distances to reference points [281]. Building a\nmulti-scale directional network in the foundation model aligns\nwith the biological basis and significance of directionality in\nperception.\nDiscussion: Overall, the application of sparsity, directional-\nity, and selectivity in deep learning can lead to more efficient\nand effective neural networks that are better able to generalize\nand learn from data. Child et al. [282] sparse transformers\ncan be used in long sequences Better performance in density\nmodeling. Networks based on multi-scale geometric structures,\nsuch as Ridgelet neural network, Contourlet neural network,\netc., all use characteristics such as directionality to achieve a\nmore sparse representation [43]. These methods can provide\ntheoretical support for the perception of brain-inspired foun-\ndation models.\n3) Learning: Learning is a fundamental process for hu-\nmans to acquire memories, knowledge, and practical skills.\nFrom a neuroscience perspective, neurons possess plasticity,\nenabling them to learn by modifying their connections and\nweights [283]. In this section, we introduce three brain-\ninspired learning models: Hebbian learning, Error-correction\nlearning, and Competitive learning.\nHebbian Learning: In 1949, Hebbian presents a postulate:\n“cells that fire together wire together”. This means that when\nan axon of cell is near enough to excite other cell or repeatedly\nor persistently takes part in firing it, the connection weight\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n25\nand w in different optimization loops, and this allows to use the\nhyperparameter optimization technique41 for optimizing θ (see\nMethods). In this manner, we expect toﬁrst provide an effective\nﬂexible modeling strategy that supports the modeling of versatile\nlocal learning rules and can learn to optimize both the underlying\nweights and the local learning rules in different optimization\nloops, thereby facilitating a generic integration of the two types of\nlearning.\nFurthermore, we want to incorporate diverse spiking dynamics,\nLP, and global learning into one uni ﬁed temporal credit\nassignment problem. To this end, we jointly derive from\nmembrane potential dynamics and ion-channel dynamics and\nobtain a differentiable signal propagation equation of subthres-\nhold dynamics. We use a memory gate (Fig. 1d) and make a\ncareful choice of derivative approximation\n8,42 for handling the\ndiscontinuous dynamics of spiking neurons (see Methods). Then,\nwe adopt the backpropagation through time (BPTT) algorithm as\nglobal learning for training SNNs. Because LP has an independent\ncorrelation-based updating manner, directly integrating the local\nmodules with handcrafted parameters and global learning\ntogether is difﬁcult to ensure convergence. To incorporate the\nimpact of local weight updates into the entire optimization\nframework, we take a parametric modeling strategy (Fig.1c) to\ntransform the local synaptic incrementsΔwLP into a parametric\nfunction related to presynaptic spike activity, pre, postsynaptic\nspike activity,post, and local hyperparametersθ, consisting of the\nlocal learning rate, sliding threshold and some other hyperpara-\nmeters determined by speciﬁc local learning rules. By doing so,\nwe not only maintain the underlying weight directly optimized by\nBPTT but also exclusively model local weight updated behaviors\nas a temporal-based function concerning adjacent spiking\nactivities and local meta-parameters.\nThe adopted decoupling optimization strategy is also inspired\nby a variant of synaptic dynamics. Speciﬁcally, derived from the\nion-channel differential dynamics, the synaptic weightswtðÞ have\ntwo terms\nwtðÞ¼ wt n\n/C0/C1\ne\ntn/C0t\nτw þ ePt ; pre; post;θ\n/C0/C1\n≜ wGP þ wLP; ð1Þ\nwhere wt n\n/C0/C1\ndenotes the phase value of the synaptic dynamics at\ndiscrete time tn, ktðÞ¼ e\ntn/C0t\nτw denotes the synaptic decay function,\nand τw denotes the synaptic constant. Here ePt ; pre; post;θ\n/C0/C1\ndenotes the generic local modiﬁcations. If we further assume the\ntop-down signals to modify the statewt n\n/C0/C1\nat the speciﬁc timetn\nwhen supervision signals are provided, and assume that\nePt ; pre; post;θ\n/C0/C1\nrepresents the LP driven by neuron activities\nand meta-parameters θ, then Eq. (1) can be used to accordingly\ndecompose the weight into two parts,wGP and wLP.\nFinally, we develop a global-local synergic learning model that\nintegrates LP and GP into one neuromorphic model by\nexclusively allocating them to act on different weight parts and\ntime scales. Here, each HP unit has two weight branches (Fig.1d):\nwGP, being directly updated by BPTT when receiving supervision\ninformation, and wLP, being updated in an event-driven manner\nby the meta-learned spiking LP. To fully utilize various learning\nfeatures of the LP and GP, we use the meta-learning circuit and\nbilevel optimization technique to decouple the optimization\nprocess of wLP and wGP: The HP model provides aﬂexible and\nconﬁgurable solution that allows the construction of different\nFig. 1 Illustration of hybrid synergic learning model. aAn illustration of biological synaptic plasticity mechanisms and neuronal dynamics. The\nneuromodulators exhibit a radically different evolutionary (learning) process from synaptic plasticity and can encode top-down supervision information to\nmodulate versatile synaptic plasticity behaviors.b Motivated by the neural circuitry, we develop a multiscale meta-learning paradigm to integrate these two\ntypes of learning in one neuromorphic model. It models the parameters of local learning as a type of meta-learning parameters and decouples the learning\nprocess of weights (solid blue lines) and these local meta-parameters (dashed blue lines) by using a bilevel optimization technique, enablingﬂexible\nmultiscale learning.c Parametric biological short-term plasticity. Local weight modiﬁcations can be equivalently modeled as a parametric function related\nto the initial weight valuewt0\n, the presynaptic spike activityxt\n1, the postsynaptic spike activity,st\n1, and the hyperparametersθ of the Hebbian rule.d The HP\nunit divides the weights into two branches,wGP and wLP. wGP is updated based on global errors, andwLP is updated based on adjacent neuron activities and\nmeta-parameters. In each HP unit, we use a memory gate that controls the reset or leaky integration behaviors of membrane potentialut using the spike\nﬁring statest.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-021-27653-2 ARTICLE\nNATURE COMMUNICATIONS|           (2022) 13:65 | https://doi.org/10.1038/s41467-021-27653-2 | www.nature.com/naturecommunications 3\nFig. 16. A learning paradigm integrating global error-driven learning (called\n“global plasticity”, GP) and local correlation-driven learning (called “local\nplasticity”, LP). Image from [284].\nbetween these two cell will be increased. Inspired by this\npostulate, the correlation-based learning rules are generally\ncalled Hebbian learning [285]. Hebbian learning is a form of\nunsupervised learning, as it doesn’t rely on external feedback\nor error signals. Instead, it captures statistical correlations\nbetween inputs and outputs, forming associative memories.\nError-correction Learning: Error-correction learning fol-\nlows the principle that neurons should learn from their mis-\ntakes. In this form of learning, the connection weight between\ntwo neurons is adjusted based on the difference between the\nactual output and the desired output (i.e., the error). Error-\ncorrection learning is a form of supervised learning, as it\nrequires a teacher or target to provide the desired output.\nBy minimizing the error, this learning mechanism improves\nthe performance of the network. The most popular learning\nalgorithm for use with error-correction learning is the back-\npropagation algorithm [286]. Through setting the target loss,\nthe gradient can be backpropagated across the neural network\nand the weight can be updated [287].\nCompetitive Learning: Competitive learning operates on\nthe principle that neurons should compete for activation. Only\na few neurons in a layer are allowed to be active at a time,\nwhile the others are inhibited. The weights of connections be-\ntween inputs and active neurons are strengthened, whereas the\nweights of connections between inputs and inactive neurons\nare weakened [288]. Similar to Hebbian learning, competitive\nlearning is unsupervised and doesn’t rely on external feedback.\nIt enables the discovery of features or clusters in the input data,\nleading to the formation of sparse representations. Activation\nLearning [289] is a type of competitive learning . It injects\ncompetition within and among neurons and show capacity\nof learning plentiful local features from few shots of input\npatterns.\nDiscussion: The learning method represented by error\ncorrection learning of backpropagation has excellent perfor-\nmance on specific tasks. Therefore, it occupies the mainstream\nposition in the current model learning. But its supervised\nlearning is still far from the way the brain learns. Wu et al.\n[284] proposed a neuromorphic global-local synergic learning\nmodel (as shown in Fig. 16). It can meta-learn local plasticity\nand receive top-down supervision information. By combining\ndifferent learning methods, the ability of the model in few-shot\nlearning, continual learning and other scenarios is improved.\nTherefore, organically combining a variety of brain-inspired\nlearning methods into the learning of the foundation model\nwill help improve the learning efficiency.\n4) Cognition: Cognition refers to the process through\nwhich human beings acquire knowledge, apply that knowl-\nedge, and process information, representing the most funda-\nmental psychological process in human beings. The human\nbrain receives information input from the external world,\nprocesses it, converts it into internal psychological activities,\nand then controls actions accordingly. In this section, we\nwill primarily introduce the mechanisms of brain memory,\nforgetting, and reasoning.\nMemory: Memory is fundamental to human beings’ ability\nto solve complex problems. The human brain can encode\nand store past experiences to form memories, which can be\nsearched and retrieved when needed. Engram cells serve as\nfundamental evidence that the brain forms memories, provid-\ning the necessary conditions [290] for their emergence. Based\non their differentiation and actions, memories can be divided\ninto short-term memory and long-term memory.\nShort-term memory, also known as working memory, usu-\nally lasts only seconds or minutes [291]. Two theories about\nshort-term memory are ”activity-silent neural networks” and\n”sustained activity.” Activity-dependent synaptic plasticity en-\nables the formation of a transient nervous system [292],\nleading to transient increases or decreases in neurotransmitter\nsignals on synapses, forming a dynamic neural network [293].\nShort-term memory is formed through the strengthening or\nweakening of these transient signals. The theory of sustained\nactivity suggests that short-term memory is maintained by\ncontinuous action potential discharge. Studies by Courtney et\nal. [294] and Foster et al. [295] have also shown functional\nmagnetic resonance imaging (fMRI) and electroencephalog-\nraphy (EEG) results indicating a sustained increase in brain\nactivity during the delayed period of memory tasks.\nThe hippocampus, located in the temporal lobe, is a crucial\npart of the limbic system responsible for forming new memo-\nries and converting short-term memory into long-term memory\nin the human brain. This process, often called consolidation,\ninvolves gene activation and the formation of new synaptic\nconnections between neurons in the brain. Research by Yap\net al. [296] revealed that the hippocampus expresses sparse\npopulations of neurons activated by novel experiences. These\nneurons may fine-tune their inputs to form persistent networks\nthat provide a coordinated response to an experience, leading\nto long-term memory consolidation.\nForgetting: Forgetting is the opposite process of remem-\nbering. Forgetting occurs for different reasons and occurs at\ndifferent stages of memory formation, storage and retrieval.\nCurrently known forgetting mechanisms include passive for-\ngetting and active forgetting [297]. Passive forgetting is the\nnon-spontaneous memory loss process of the human brain.\nOver time, memories become difficult to retrieve without\ncontext. In addition, similar memories can form interference\nand thus be lost. Finally, the instability of the biological\nmemory mechanism will cause the memory to fade naturally\nover time. In contrast, active forgetting is considered to be\na spontaneous memory extinction process. Active forgetting\nusually includes Motivated forgetting and Retrieval-induced\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n26\nforgetting. Motivated forgetting refers to the forgetting process\nthat is under our own cognitive control. For example, when\ncertain memories affect one’s positive image or are inconsis-\ntent with beliefs, motivational forgetting will actively abandon\nthis part of the memory. Retrieval-induced forgetting describes\nthat when memory is consolidated through learning, it may\nweaken the same type of memory that has not been practiced,\nand only retrieval of certain memories may cause the forgetting\nof other memories. All in all, forgetting plays a vital role in\nhuman beings. It allows us to focus on what is retained in\nmemory and to protect ourselves from adverse memories.\nReasoning: As a complex intelligent system, the brain’s\ncausal inference ability is one of the main manifestations\nof its intelligence. When the brain processes multi-sensor\ninformation, it exhibits the ability to make causal inferences.\nIn particular, in situations where information from multiple\nsensors differs, the brain is able to infer whether the signals\ncome from the same source or independent sources, and\ndoes not integrate signals that are unlikely to come from\nthe same source. Reuben et al. [298] show how interactions\nbetween different types of neurons lead to optimal integration\nand causal inference. Many neurons that receive input from\nboth modalities are congruent neurons with similar tuning\nfor both modalities, enabling multisensory integration. There\nare also heterotropic neurons, capable of detecting signals\nfrom different sources. The collaboration of coherent and\nheterotropic neurons may be what enables the brain to form\ncausal inferences.\nDiscussion: Memory and forgetting play an important role\nin the cognitive process. Memorizing can help the model form\nknowledge, while forgetting can clear unnecessary informa-\ntion. Memory-based models have been heavily proposed [299]\nin current time series modeling. In addition, reasoning has also\nreceived extensive attention, and a large number of models\nwith reasoning have been proposed [300]. However, the current\nbasic model still does not involve the exploration of these\ncognitive abilities, and its performance on complex tasks is\nstill very limited.\nVII. O PEN PROBLEMS\nAs the basis of remote sensing field, the foundation models\nhave received extensive attention and research. There are still\nmany challenging open problems to be solved in this field. In\nthis section, we analyze twelve open problems about RSFMs\nand propose potential solutions (as shown in Fig. 17).\nA. Brain-Inspired FMs\nNeural networks have their roots in the study of the brain’s\nneuron structure [301]. While artificial intelligence research\nhas made significant progress, the current FMs still fall short\nof capturing the brain’s remarkable capabilities. The brain\nis exceptionally complex, yet it can achieve functions like\nrecognition, cognition, and decision-making while consuming\nminimal power. Developing brain-inspired FMs can drive\nartificial intelligence towards higher performance and lower\ncosts.\nThe design of brain-inspired FMs can be approached from\ntwo main perspectives: brain structure and brain characteris-\ntics. The spiking neural network, for example, emulates the\nhuman brain’s activation and inhibition of signals through\naccumulation [302]. Another approach, the capsule network,\nsimulates a set of neurons and uses vectors to represent feature\npose information [303]. The brain’s inherent characteristics,\nsuch as sparseness, selectivity, and directionality, offer valu-\nable insights for designing foundational models that can better\nmimic the brain’s efficiency and effectiveness.\nB. Physics-Informed FMs\nIn tasks related to remote sensing interpretation, data are\ncollected from the physical world. However, current FMs\noften focus solely on data-driven approaches, neglecting the\nunderlying physical characteristics of the data. By incorporat-\ning these physical characteristics, we can effectively unearth\nlatent features within the data and extract robust and sparse\nrepresentations.\nA prime example of physics-informed data interpretation\nis the use of the Fourier transform for spectrum analysis\nof one-dimensional signals and wavelet transform for two-\ndimensional signals. These methods realize a sparse repre-\nsentation of a complex signal. Yang et al. [136] employed\nlifting wavelet to extract multi-scale frequency domain fea-\ntures, leading to more robust feature representations. Wave-\nMLP represents each token as a wave function with phase\nand amplitude in quantum mechanics, which enhances token\nrepresentation [304]. Additionally, Li et al. [305] proposed\nutilizing electromagnetic scattering information for synthetic\naperture radar automatic target recognition. This approach\ncharacterizes the electromagnetic scattering properties of the\ntarget, providing crucial physical structure information. By\nintegrating existing physical research, it becomes possible\nto effectively describe target characteristics and improve FM\nperformance.\nC. The Learning Theory of FMs\nA typical characteristic of the foundation model is that it\nis trained using self-supervised learning, so the model is task-\nagnostic [5]. However, the learning theory of the foundation\nmodels is not well studied. Self-supervised learning is a type\nof unsupervised learning. When a model is optimized for a\ncertain goal (such as image reconstruction), the model can also\nachieve excellent performance on an unoptimized goal (such as\nimage classification). The results obtained in this experiment\ndo not have as good theoretical support. Therefore, it is an\nimportant issue to study the learning theory of the foundation\nmodel.\nSutskever et al. [306] proposed the use of compression to\nexplain unsupervised learning. He regarded the information\nobtained by unsupervised learning as the common structure\nnoticed by the compressor. The better the compressor, the more\ncommon structures it can extract. Correspondingly, the com-\nmon patterns learned by a model in the data of unsupervised\ntasks can be used to help perform supervised tasks. Chen et\nal. [307] demonstrated that a good sequence predictor can also\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n27\nOpen\nProblems\nBrain-Inspired FMs  \nThe brain provides guidance for efficient algorithm design\n• brain structure\n• brain-inspired properties\nPhysics-Informed FMs  \nPhysical theory provides evidence for data analysis\n• Modeling based on physical theory\nThe Learning Theory of FMs  \nExplore the theories of unsupervised learning\n• Understanding from the compression\nThe Causal Inference of FMs  \nMining the potential correlations between data\n• Language model-driven reasoning\n• The theory of causal inference\nThe Interpretability of FMs  \nImproving the interpretability\n• Designing with Interpretable structures\n• Visual analysis\nKnowledge-Based RSFMs  \nIntegrating remote sensing knowledge\n• Embeding with RS knowledge graph\nThe Spatiotemporal Forecasting Ability of RSFMs  \nThe ability to predict spatio-temporal relationships\n• Spatio-temporal data construction\n• Spatio-temporal architecture\nMulti-modal and Cross-modal RSFMs  \nDealing with arbitrary modal RS data\n• Multimodal data representation learning \n• Multimodal  align and fusion\nThe Efficiency of the RSFMs  \nThe expansion of the parameters affects the efficiency.\n• Efficient parameter optimization, \nquantization and pruning\nThe Security of the RSFMs  \nModel accuracy is affected by adversarial examples\n• Adversarial sample defense\nThe Robustness of FMs  \nImproving robustness to data distribution\n• Data augmentation and adaptation methods\n• Incorporating external knowledge\nThe  Friendliness of the RSFM Interface\nDesigning simple and user-friendly system interfaces\n• Dataset Management, Task Setting, Model \nFine-tuning, Efficient Model Deployment\nFig. 17. Top twelve open problems of foundation models.\nachieve powerful unsupervised learning in the image domain.\nThe current theoretical research on foundation models is still\nin the initial stage of development, but there is still a long way\nto go to theoretically guide the design of learning algorithms.\nD. The Causal Inference of FMs\nDespite the significant breakthroughs of FMs, the current\npre-training models face challenges in learning potential causal\nrelationships within data. Existing research reveals a critical\nshortcoming of correlation-based FMs in terms of causal in-\nference [308]. However, systems like AutoGPT and BMTools\n[309] have shown promising potential by enabling models to\nlearn how to call various tools to complete tasks.\nIncorporating the underlying theory of causal inference into\nfoundation models can greatly enhance their understanding.\nFor instance, the structural causal model allows for counterfac-\ntual reasoning, enabling inferences about how other variables\nwould be affected if certain variables were to change. Addi-\ntionally, the structural causal model can determine the con-\nditional independence between variables, assessing whether\nother variables remain independent when certain variables’\nvalues are given [310]. By combining FMs with the reasoning\ncapabilities of the structural causal model, we can significantly\nimprove the model’s ability to analyze potential variable\nrelationships.\nE. The Interpretability of FMs\nImproving model interpretability is a crucial step towards\nunderstanding the internal algorithm logic of FMs. Utilizing\ninterpretable model structures can be an effective approach\nin this regard. One such technique is wavelet decomposition,\nwhich can decompose features from the frequency domain.\nThis allows for the learning of interpretable geometric tex-\nture features, making it easier to understand how the model\nprocesses and represents different patterns in the data [311].\nBy incorporating wavelet decomposition into the model, re-\nsearchers can gain insights into the specific features and char-\nacteristics that the model focuses on during the interpretation\nprocess.\nFurthermore, constructing models based on existing physi-\ncal principles can also enhance interpretability. Wu et al. [312]\ntreat the forward process of neural network calculations as a\ndiffusion process from a given initial state, explicitly utilizing\ndependencies between samples during forward calculations.\nBy doing so, they explicitly utilize dependencies between\nsamples during forward calculations, which can help in better\nunderstanding how the model propagates information and\nmakes decisions based on the input data.\nIn addition, the attention correlations in the Transformer\nstructure can be leveraged to visualize feature associations.\nBy visualizing these associations, researchers can gain insights\ninto which parts of the input data are most relevant to the\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n28\nmodel’s predictions. This enhances the interpretability of the\nmodel to some extent, providing a glimpse into the decision-\nmaking process of the foundation model.\nF .The Robustness of FMs\nThe training of foundation models on large and diverse\nunlabeled datasets brings advantages in terms of generalization\nand feature extraction capabilities. However, it is practically\nimpossible to collect a dataset that covers all possible distri-\nbutions of scenes, making the robustness of the model closely\nrelated to the distribution of the training data [5].\nTo enhance the robustness of foundation models, data aug-\nmentation is a simple yet effective method. Applying various\ntransformations to the training data can introduce diversity\ninto the training dataset and help the model learn to handle\ndifferent variations of the data. The model can avoid overfitting\nto specific patterns and improves the ability to handle unseen\ndata [83].\nMoreover, adaptation methods are valuable for enhancing\nthe robustness of foundation models when applied to down-\nstream tasks. Fine-tuning a small number of parameters while\nfreezing others in the model can improve the model’s per-\nformance on out-of-distribution samples. This process allows\nthe model to adapt to the specific characteristics of the target\ntask while retaining the general knowledge learned during pre-\ntraining [5].\nAdditionally, incorporating external knowledge into the\nmodel’s input can further boost its robustness. By providing\nrelevant information as additional input, such as prior knowl-\nedge about the scene, domain-specific attributes, or environ-\nmental context, the model gains a broader understanding of\nthe data and handles diverse and complex scenes well.\nG. Knowledge-Based RSFMs\nThe capability of foundation models in visual language\nhas significantly advanced due to large-scale image and text\nalignment data, enhancing their understanding and analysis\nabilities. However, in the remote sensing field, collecting abun-\ndant image and text-paired data for training is challenging.\nTo address this, Deng et al. [313] created a dataset called\nGeoSignal, enabling fine-tuning of large language models\nspecifically for earth science-related queries. Similarly, fusing\nremote sensing interpretation with large language models re-\nquires constructing fine-tuning datasets with expert knowledge\nto enhance the model’s accuracy.\nWhile underlying models demonstrate impressive perfor-\nmance, they are often limited in capturing and exploiting\ncommon-sense errors, leading to potential risks. To improve\nthe reliability and interpretability of foundation models, in-\ncorporating knowledge graphs can be advantageous [314].\nKnowledge graphs serve as structured knowledge databases\nthat provide additional information for model reasoning. By\ncombining the foundation model with knowledge graphs, the\nmodel gains access to key task-related knowledge through in-\ncontextual learning without requiring retraining. The founda-\ntion model’s robust knowledge understanding and processing\ncapabilities can further enhance its accuracy and performance.\nH. The Spatiotemporal Forecasting Ability of RSFMs\nFoundation models driven by large-scale data have shown\nremarkable capabilities in feature extraction and understanding\ncomplex images. However, current remote sensing foundation\nmodels, using Masked image modeling for self-supervised\nlearning, fail to fully exploit the potential time-varying re-\nlationships in time-series data. Consequently, a significant\namount of data remains untapped. To address this limita-\ntion, exploring a multi-temporal foundation model pre-training\nmethod can be beneficial. By training the model with input\nfrom remote sensing time series data, we can analyze the\ndynamic temporal information within the data. This approach\nenables the foundation model to perform spatiotemporal analy-\nsis and prediction, significantly enhancing its ability to monitor\ndisasters and other time-sensitive phenomena.\nI. Multi-modal and Cross-modal RSFMs\nRemote sensing data exhibits a wide variety, with different\nsensors providing data with varying temporal and spatial\nreferences and formats. This diversity makes it challenging\nto collect and align high-quality multi-modal sample data,\nthereby limiting the development of multi-modal RSFMs. One\npotential solution is to establish a standardized interface, which\nwould enhance the efficiency of data aggregation.\nFurthermore, different tasks often exploit distinct physical\nproperties. For example, sound separation requires frequency\ninformation, while sound content recognition relies on timing\ninformation. Therefore, the data feature extraction models\nfor different modalities need to be constructed based on\ntheir unique characteristics [315]. Introducing texture features,\nscattering center features, phase coherence features, spectral\ncorrelation features, etc., can effectively process multi-modal\ndata. Additionally, selecting suitable feature spaces, such as\nEuclidean space, Hilbert space, Unitary space, etc., can help\nmitigate information loss.\nTo address the challenge of inconsistent data space-time\nbenchmarks and alignment difficulties, an independent feature\nextraction network can be designed for each modality [132].\nSubsequently, a small set of alignment features can be uti-\nlized to align the features, enabling FMs to work effectively\nacross different data modalities. This approach ensures that the\nextracted features retain their unique properties while being\ncompatible across different modalities.\nJ. The Efficiency of RSFMs\nAs the amount of data and model parameters increases,\nthere is growing concern about the training and reasoning\nefficiency of foundation models. Large-scale language models\nhave already achieved training with billion-scale parameters,\nand the number of parameters in remote sensing vision\nfoundation models is expected to continue rising. However,\nthe processing of vast amounts of remote sensing image\ndata requires significant computational resources. Therefore,\nthe parameter-efficient fine-tuning algorithms can help reduce\ntraining overhead.\nQuantization and pruning are also essential techniques to\nconsider. By reducing the precision of individual weight\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n29\nnumerical representations, the model’s operating efficiency can\nbe significantly improved. For instance, Transformer models\ncan be quantized from FP32 to INT8, a widely adopted\npractice in training large language models. Pruning, on the\nother hand, involves removing elements of the network, rang-\ning from individual weights to higher granularity component\nchannels. Researchers, like Spyrison et al. [316], have pro-\nposed sparse pruning methods, enabling GPT series models\nto achieve 50% sparsity in a single pass without the need\nfor retraining. Additionally, research has demonstrated full\nparameter fine-tuning of a 65 Billion parameter model on\neight 3090 GPUs [317]. As such, continuously integrating\nthe latest technology with remote sensing foundation models\ncan improve training and reasoning efficiency while saving\nvaluable computational resources.\nIntegrating the latest technology advancements with RSFMs\ncan lead to improved training and reasoning efficiency, allow-\ning for more efficient use of valuable computational resources.\nBy continually exploring and implementing parameter-efficient\ntechniques, researchers can ensure that remote sensing foun-\ndation models remain scalable, powerful, and capable of\nhandling the ever-increasing volume of remote sensing data\nwith minimal computational overhead.\nK. The Security of RSFMs\nThe issue of hallucination in large language models is\nindeed a significant concern [42], especially in the visual\ndomain, where it can resemble the phenomenon of adversarial\nexamples. In certain cases, the model may confidently predict\nincorrect information or assign high probabilities to non-target\nareas, which can lead to serious consequences in practical\napplications, particularly in remote sensing, where model\nmisidentification can result in critical decision-making errors.\nAdversarial sample defense has been explored by re-\nsearchers to enhance the security of models, both in gen-\neral contexts [318] and specifically in remote sensing [319].\nHowever, given the large number of parameters in foundation\nmodels, the effectiveness of conventional adversarial attack\nmitigation strategies may require further investigation and\ndevelopment.\nMoreover, while adversarial attacks are commonly studied\nand implemented on digital images, it is vital to consider the\npotential impact of such attacks on high-resolution remote\nsensing images. In remote sensing, even minor local changes\nin real scenes could lead to significant model misidentification,\nhighlighting the need for robust defense mechanisms that can\nhandle such real-world variations.\nL. The Friendliness of RSFM Interfaces\nRSFMs offer a significant advancement in the application\nof remote sensing interpretation. However, their adoption still\npresents a high threshold for users. To promote the widespread\nuse of RSFMs, it is crucial to design simple and user-friendly\nsystem interfaces. These interfaces should facilitate various\nremote sensing interpretation applications and enable RSFMs\nto adapt with only a small number of samples.\nThe RSFM system should support the following key func-\ntionalities to enhance user experience: (1) Dataset Manage-\nment: The system should allow users to easily upload and\nconstruct datasets. This feature enables users to input their\nown data for specific tasks, making it convenient to work with\ntheir own remote sensing data. (2) Task Setting: Users should\nbe able to set up interpretation tasks effortlessly. The interface\nshould provide intuitive options to define the specifics of the\ntasks they want to perform using RSFMs. (3) Model Fine-\ntuning: RSFMs should offer users the ability to fine-tune mod-\nels with their dataset to achieve better performance on specific\ntasks. This fine-tuning process should be straightforward and\nrequire minimal expertise. (4) Efficient Model Deployment:\nOnce the model is ready, the system should enable high-\nprecision and efficient deployment. Users should be able to\ndeploy their customized RSFM models quickly and easily for\npractical applications.\nBy providing a user-friendly interface, RSFMs can be\nreadily applied to a broader range of scenarios and tasks.\nUsers, even those with limited expertise in remote sensing or\ndeep learning, can utilize RSFMs to accomplish their interpre-\ntation objectives through simple and efficient operations. This\nwill significantly enhance the practicality and accessibility of\nRSFMs in the field of remote sensing interpretation.\nVIII. C ONCLUDING REMARKS AND DISCUSSION\nFoundation models have emerged as a promising direction\nin remote sensing research. In this paper, we provided a\ncomprehensive survey of the current development of remote\nsensing foundation models. We started by explaining the\nkey technologies underlying foundation models, including\nTransformer structures, self-supervised pre-training methods,\nand efficient parameter optimization techniques. Then latest\ndevelopments in foundation models across various domains\nare presented, including language, vision, visual-language and\nremote sensing foundation models. We explored core applica-\ntions in remote sensing interpretation, including classification,\nlocation, and understanding tasks.\nAfter that, performance comparison experiments are con-\nducted from three aspects: global representation, local rep-\nresentation and target localization. Through the experiments,\nwe observed that while remote sensing foundation models\ndemonstrate potential, they still face challenges in achieving\nsignificant advantages over natural foundation models due\nto limited remote sensing data and certain structural design\nlimitations.\nThrough the above research and analysis, this paper sum-\nmarizes the research and development of the remote sensing\nfoundation model. From the perspective of the development\nprocess of the foundation model, models are updating quickly.\nAs mentioned in this paper, a model like Meta-Transformer\n[132] applies the data with 12 modalities to one foundation\nmodel, and similar research will quickly follow up in the field\nof remote sensing. In the near future, multi-modal and cross-\nmodal remote sensing foundation models will receive a lot of\nresearch. A large amount of data in the field of remote sensing\nwill be more fully mined. The barriers of multi-modal data will\nalso be gradually broken down.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n30\nHowever, the large-scale computing behind these studies\nmeans that this form of research is difficult to follow. A\nlarge number of calculations and lack of theoretical research\nsupport will be the shortcomings of the current remote sensing\nfoundation model. To this end, this paper further elaborates a\nvaluable research direction of the foundation model, that is,\nthe brain-inspired remote sensing foundation model. Different\nfrom the current research ideas of the foundation model,\nthe brain-inspired properties will provide foundation models\nwith a theoretical foundation from a biological background,\nreliable performance and higher data utilization efficiency.\nThis framework provides a novel perspective to guide the\ndevelopment of future models and applications in remote\nsensing interpretation.\nFinally, we identified twelve open problems in remote sens-\ning foundation model research, encompassing areas such as\nbrain-inspired modeling, physical information integration, and\nknowledge-based learning. Addressing these open problems\nwill drive the proposal and adoption of innovative methods in\nremote sensing interpretation.\nIn summary, remote sensing foundation models hold im-\nmense potential and continue to be an active area of research.\nBy addressing the identified challenges and exploring new\navenues inspired by brain characteristics, we can unlock the\nfull potential of remote sensing foundation models.\nREFERENCES\n[1] K. He, X. Chen, S. Xie, Y . Li, P. Doll ´ar, and R. Girshick, “Masked autoencoders\nare scalable vision learners,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2022, pp. 16 000–16 009.\n[2] X. Zhai, A. Kolesnikov, N. Houlsby, and L. Beyer, “Scaling vision transformers,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022, pp. 12 104–12 113.\n[3] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer,\nA. Steiner, M. Caron, R. Geirhos, I. Alabdulmohsin et al. , “Scaling vision\ntransformers to 22 billion parameters,” arXiv preprint arXiv:2302.05442 , 2023.\n[4] J. D. M.-W. C. Kenton and L. K. Toutanova, “Bert: Pre-training of deep bidirec-\ntional transformers for language understanding,” in Proceedings of NAACL-HLT,\n2019, pp. 4171–4186.\n[5] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx, M. S.\nBernstein, J. Bohg, A. Bosselut, E. Brunskill et al. , “On the opportunities and\nrisks of foundation models,” arXiv preprint arXiv:2108.07258 , 2021.\n[6] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet: A large-\nscale hierarchical image database,” in 2009 IEEE conference on computer vision\nand pattern recognition . Ieee, 2009, pp. 248–255.\n[7] C. Sun, A. Shrivastava, S. Singh, and A. Gupta, “Revisiting unreasonable effec-\ntiveness of data in deep learning era,” in Proceedings of the IEEE international\nconference on computer vision , 2017, pp. 843–852.\n[8] C. Schuhmann, R. Kaczmarczyk, A. Komatsuzaki, A. Katta, R. Vencu, R. Beau-\nmont, J. Jitsev, T. Coombes, and C. Mullis, “Laion-400m: Open dataset of clip-\nfiltered 400 million image-text pairs,” in NeurIPS Workshop Datacentric AI , no.\nFZJ-2022-00923. J ¨ulich Supercomputing Center, 2021.\n[9] C. Xinlei, X. Saining, and H. Kaiming, “An empirical study of training self-\nsupervised visual transformers,” arXiv preprint arXiv:2104.02057 , vol. 8, 2021.\n[10] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry,\nA. Askell, P. Mishkin, J. Clark et al., “Learning transferable visual models from\nnatural language supervision,” in International conference on machine learning .\nPMLR, 2021, pp. 8748–8763.\n[11] L. Jiao, Z. Huang, X. Liu, Y . Yang, M. Ma, J. Zhao, C. You, B. Hou, S. Yang,\nF. Liu et al. , “Brain-inspired remote sensing interpretation: A comprehensive\nsurvey,” IEEE Journal of Selected Topics in Applied Earth Observations and\nRemote Sensing, 2023.\n[12] L. Zhang and L. Zhang, “Artificial intelligence for remote sensing data analysis:\nA review of challenges and opportunities,” IEEE Geoscience and Remote Sensing\nMagazine, 2022.\n[13] L. Jiao, X. Zhang, X. Liu, F. Liu, S. Yang, W. Ma, L. Li, P. Chen, Z. Feng,\nY . Guoet al., “Transformer meets remote sensing video detection and tracking:\nA comprehensive survey,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , 2023.\n[14] C. Wen, Y . Hu, X. Li, Z. Yuan, and X. X. Zhu, “Vision-language models in remote\nsensing: Current progress and future trends,” arXiv preprint arXiv:2305.05726 ,\n2023.\n[15] G. Mai, W. Huang, J. Sun, S. Song, D. Mishra, N. Liu, S. Gao, T. Liu, G. Cong,\nY . Hu et al. , “On the opportunities and challenges of foundation models for\ngeospatial artificial intelligence,” arXiv preprint arXiv:2304.06798 , 2023.\n[16] V . C. Gomes, G. R. Queiroz, and K. R. Ferreira, “An overview of platforms for\nbig earth observation data management and analysis,” Remote Sensing, vol. 12,\nno. 8, p. 1253, 2020.\n[17] D. Wang, Q. Zhang, Y . Xu, J. Zhang, B. Du, D. Tao, and L. Zhang, “Advancing\nPlain Vision Transformer Toward Remote Sensing Foundation Model,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 61, pp. 1–15, 2023.\n[18] X. Sun, P. Wang, W. Lu, Z. Zhu, X. Lu, Q. He, J. Li, X. Rong, Z. Yang,\nH. Chang, Q. He, G. Yang, R. Wang, J. Lu, and K. Fu, “RingMo: A Remote\nSensing Foundation Model with Masked Image Modeling,” IEEE Transactions\non Geoscience and Remote Sensing , pp. 1–1, 2022.\n[19] K. Cha, J. Seo, and T. Lee, “A Billion-scale Foundation Model for Remote\nSensing Images,” Apr. 2023.\n[20] Y . Cong, S. Khanna, C. Meng, P. Liu, E. Rozi, Y . He, M. Burke, D. Lobell, and\nS. Ermon, “Satmae: Pre-training transformers for temporal and multi-spectral\nsatellite imagery,” Advances in Neural Information Processing Systems , vol. 35,\npp. 197–211, 2022.\n[21] M. Mendieta, B. Han, X. Shi, Y . Zhu, and C. Chen, “GFM: Building Geospatial\nFoundation Models via Continual Pretraining,” Mar. 2023.\n[22] C. J. Reed, R. Gupta, S. Li, S. Brockman, C. Funk, B. Clipp, K. Keutzer,\nS. Candido, M. Uyttendaele, and T. Darrell, “Scale-MAE: A Scale-Aware Masked\nAutoencoder for Multiscale Geospatial Representation Learning,” Apr. 2023.\n[23] J.-B. Grill, F. Strub, F. Altch ´e, C. Tallec, P. Richemond, E. Buchatskaya,\nC. Doersch, B. Avila Pires, Z. Guo, M. Gheshlaghi Azar et al. , “Bootstrap\nyour own latent-a new approach to self-supervised learning,” Advances in neural\ninformation processing systems , vol. 33, pp. 21 271–21 284, 2020.\n[24] T. Chen, S. Kornblith, K. Swersky, M. Norouzi, and G. E. Hinton, “Big self-\nsupervised models are strong semi-supervised learners,” Advances in neural\ninformation processing systems , vol. 33, pp. 22 243–22 255, 2020.\n[25] M. Caron, H. Touvron, I. Misra, H. J ´egou, J. Mairal, P. Bojanowski, and A. Joulin,\n“Emerging properties in self-supervised vision transformers,” in Proceedings of\nthe IEEE/CVF international conference on computer vision , 2021, pp. 9650–\n9660.\n[26] Z. Xie, Z. Zhang, Y . Cao, Y . Lin, J. Bao, Z. Yao, Q. Dai, and H. Hu, “Simmim: A\nsimple framework for masked image modeling,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2022, pp. 9653–9663.\n[27] J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. C. H. Hoi, “Align before\nfuse: Vision and language representation learning with momentum distillation,”\nAdvances in neural information processing systems , vol. 34, pp. 9694–9705,\n2021.\n[28] J. Yu, Z. Wang, V . Vasudevan, L. Yeung, M. Seyedhosseini, and Y . Wu,\n“Coca: Contrastive captioners are image-text foundation models,” Transactions\non Machine Learning Research , 2022. [Online]. Available: https://openreview.\nnet/forum?id=Ee277P3AYC\n[29] J. Li, D. Li, S. Savarese, and S. Hoi, “Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large language models,” in International\nConference on Machine Learning , 2023.\n[30] W. Wang, H. Bao, L. Dong, J. Bjorck, Z. Peng, Q. Liu, K. Aggarwal, O. K.\nMohammed, S. Singhal, S. Som et al., “Image as a foreign language: Beit pre-\ntraining for vision and vision-language tasks,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2023, pp. 19 175–\n19 186.\n[31] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,\nS. Whitehead, A. C. Berg, W.-Y . Lo et al., “Segment anything,” arXiv preprint\narXiv:2304.02643, 2023.\n[32] D. Wang, J. Zhang, B. Du, G.-S. Xia, and D. Tao, “An Empirical Study of Remote\nSensing Pretraining,” IEEE Transactions on Geoscience and Remote Sensing, pp.\n1–1, 2022.\n[33] K. Ayush, B. Uzkent, C. Meng, K. Tanmay, M. Burke, D. Lobell, and S. Ermon,\n“Geography-aware self-supervised learning,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , 2021, pp. 10 181–10 190.\n[34] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll ´ar,\nand C. L. Zitnick, “Microsoft coco: Common objects in context,” in Computer\nVision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September\n6-12, 2014, Proceedings, Part V 13 . Springer, 2014, pp. 740–755.\n[35] S. Changpinyo, P. Sharma, N. Ding, and R. Soricut, “Conceptual 12m: Pushing\nweb-scale image-text pre-training to recognize long-tail visual concepts,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2021, pp. 3558–3568.\n[36] P. Sharma, N. Ding, S. Goodman, and R. Soricut, “Conceptual captions: A\ncleaned, hypernymed, image alt-text dataset for automatic image captioning,”\nin Proceedings of the 56th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , 2018, pp. 2556–2565.\n[37] V . Ordonez, G. Kulkarni, and T. Berg, “Im2text: Describing images using\n1 million captioned photographs,” Advances in neural information processing\nsystems, vol. 24, 2011.\n[38] R. Krishna, Y . Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,\nY . Kalantidis, L.-J. Li, D. A. Shamma et al. , “Visual genome: Connecting\nlanguage and vision using crowdsourced dense image annotations,” International\njournal of computer vision , vol. 123, pp. 32–73, 2017.\n[39] C. Jia, Y . Yang, Y . Xia, Y .-T. Chen, Z. Parekh, H. Pham, Q. Le, Y .-H. Sung, Z. Li,\nand T. Duerig, “Scaling up visual and vision-language representation learning\nwith noisy text supervision,” in International conference on machine learning .\nPMLR, 2021, pp. 4904–4916.\n[40] Y . Long, G.-S. Xia, S. Li, W. Yang, M. Y . Yang, X. X. Zhu, L. Zhang, and\nD. Li, “On creating benchmark dataset for aerial image interpretation: Reviews,\nguidances, and million-aid,” IEEE Journal of selected topics in applied earth\nobservations and remote sensing , vol. 14, pp. 4205–4230, 2021.\n[41] G. Christie, N. Fendley, J. Wilson, and R. Mukherjee, “Functional map of the\nworld,” in Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018, pp. 6172–6180.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n31\n[42] Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y . Xu, E. Ishii, Y . J. Bang, A. Madotto,\nand P. Fung, “Survey of hallucination in natural language generation,” ACM\nComputing Surveys, vol. 55, no. 12, pp. 1–38, 2023.\n[43] L. Jiao, R. Shang, F. Liu, and W. Zhang, Brain and nature-inspired learning,\ncomputation and recognition . Elsevier, 2020.\n[44] S. Schmidgall, J. Achterberg, T. Miconi, L. Kirsch, R. Ziaei, S. Hajiseyedrazi, and\nJ. Eshraghian, “Brain-inspired learning in artificial neural networks: a review,”\narXiv preprint arXiv:2305.11252 , 2023.\n[45] X.-L. Zou, T.-J. Huang, and S. Wu, “Towards a new paradigm for brain-inspired\ncomputer vision,” Machine Intelligence Research , vol. 19, no. 5, pp. 412–424,\n2022.\n[46] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in neural\ninformation processing systems , vol. 30, 2017.\n[47] Y . Yang, L. Jiao, X. Liu, F. Liu, S. Yang, Z. Feng, and X. Tang, “Transformers\nmeet visual learning understanding: A comprehensive review,” arXiv preprint\narXiv:2203.12944, 2022.\n[48] Y . Yu, X. Si, C. Hu, and J. Zhang, “A review of recurrent neural networks:\nLstm cells and network architectures,” Neural computation , vol. 31, no. 7, pp.\n1235–1270, 2019.\n[49] D. Zhang and D. Wang, “Relation classification: Cnn or rnn?” in Natural\nLanguage Understanding and Intelligent Applications: 5th CCF Conference\non Natural Language Processing and Chinese Computing, NLPCC 2016, and\n24th International Conference on Computer Processing of Oriental Languages,\nICCPOL 2016, Kunming, China, December 2–6, 2016, Proceedings 24. Springer,\n2016, pp. 665–675.\n[50] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao, C. Xu,\nY . Xu et al. , “A survey on vision transformer,” IEEE transactions on pattern\nanalysis and machine intelligence , vol. 45, no. 1, pp. 87–110, 2022.\n[51] T. Wolf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,\nT. Rault, R. Louf, M. Funtowicz et al. , “Transformers: State-of-the-art natural\nlanguage processing,” in Proceedings of the 2020 conference on empirical\nmethods in natural language processing: system demonstrations , 2020, pp. 38–\n45.\n[52] K. S. Kalyan, A. Rajasekharan, and S. Sangeetha, “Ammus: A survey of\ntransformer-based pretrained models in natural language processing,” arXiv\npreprint arXiv:2108.05542, 2021.\n[53] E. Yang, M. D. Li, S. Raghavan, F. Deng, M. Lang, M. D. Succi, A. J. Huang, and\nJ. Kalpathy-Cramer, “Transformer versus traditional natural language processing:\nhow much data is enough for automated radiology report classification?” The\nBritish Journal of Radiology , vol. 96, p. 20220769, 2023.\n[54] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning with\nneural networks,” Advances in neural information processing systems , vol. 27,\n2014.\n[55] E. A. Van Dis, J. Bollen, W. Zuidema, R. van Rooij, and C. L. Bockting, “Chatgpt:\nfive priorities for research,” Nature, vol. 614, no. 7947, pp. 224–226, 2023.\n[56] A. Tlili, B. Shehata, M. A. Adarkwah, A. Bozkurt, D. T. Hickey, R. Huang, and\nB. Agyemang, “What if the devil is my guardian angel: Chatgpt as a case study\nof using chatbots in education,” Smart Learning Environments , vol. 10, no. 1,\np. 15, 2023.\n[57] N. Savage, “Drug discovery companies are customizing chatgpt: heres how,”\nNature Biotechnology, 2023.\n[58] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,\nM. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., “An image is worth 16x16\nwords: Transformers for image recognition at scale,” in International Conference\non Learning Representations , 2020.\n[59] S. Khan, M. Naseer, M. Hayat, S. W. Zamir, F. S. Khan, and M. Shah,\n“Transformers in vision: A survey,” ACM computing surveys (CSUR) , vol. 54,\nno. 10s, pp. 1–41, 2022.\n[60] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,\n“Swin transformer: Hierarchical vision transformer using shifted windows,” in\nProceedings of the IEEE/CVF international conference on computer vision, 2021,\npp. 10 012–10 022.\n[61] A. Srinivas, T.-Y . Lin, N. Parmar, J. Shlens, P. Abbeel, and A. Vaswani,\n“Bottleneck transformers for visual recognition,” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, 2021, pp. 16 519–16 529.\n[62] K. Chowdhary and K. Chowdhary, “Natural language processing,” Fundamentals\nof artificial intelligence , pp. 603–649, 2020.\n[63] J. Hirschberg and C. D. Manning, “Advances in natural language processing,”\nScience, vol. 349, no. 6245, pp. 261–266, 2015.\n[64] L. Floridi and M. Chiriatti, “Gpt-3: Its nature, scope, limits, and consequences,”\nMinds and Machines , vol. 30, pp. 681–694, 2020.\n[65] B. D. Lund and T. Wang, “Chatting about chatgpt: how may ai and gpt impact\nacademia and libraries?” Library Hi Tech News, vol. 40, no. 3, pp. 26–29, 2023.\n[66] D. Y . Wu, D. Lin, V . Chen, and H.-H. Chen, “Associated learning: an alternative\nto end-to-end backpropagation that works on cnn, rnn, and transformer,” in\nInternational Conference on Learning Representations , 2021.\n[67] H. Mohapatra and S. R. Mishra, “Exploring the sector-specific influence and\nresponse of ai tools: A critical review,” arXiv preprint arXiv:2307.05909 , 2023.\n[68] H. Chen, Y . Wang, T. Guo, C. Xu, Y . Deng, Z. Liu, S. Ma, C. Xu, C. Xu,\nand W. Gao, “Pre-trained image processing transformer,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition , 2021, pp.\n12 299–12 310.\n[69] Z. Weng, X. Yang, A. Li, Z. Wu, and Y .-G. Jiang, “Semi-supervised vision\ntransformers,” in European Conference on Computer Vision . Springer, 2022,\npp. 605–620.\n[70] L. Jing and Y . Tian, “Self-supervised visual feature learning with deep neural\nnetworks: A survey,” IEEE transactions on pattern analysis and machine intel-\nligence, vol. 43, no. 11, pp. 4037–4058, 2020.\n[71] X. Liu, F. Zhang, Z. Hou, L. Mian, Z. Wang, J. Zhang, and J. Tang, “Self-\nsupervised learning: Generative or contrastive,”IEEE Transactions on Knowledge\nand Data Engineering , vol. 35, no. 1, pp. 857–876, 2021.\n[72] R. Balestriero, M. Ibrahim, V . Sobal, A. Morcos, S. Shekhar, T. Goldstein,\nF. Bordes, A. Bardes, G. Mialon, Y . Tian et al., “A cookbook of self-supervised\nlearning,” arXiv preprint arXiv:2304.12210 , 2023.\n[73] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Nee-\nlakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models are few-shot\nlearners,” Advances in neural information processing systems , vol. 33, pp. 1877–\n1901, 2020.\n[74] M. Oquab, T. Darcet, T. Moutakanni, H. V o, M. Szafraniec, V . Khalidov,\nP. Fernandez, D. Haziza, F. Massa, A. El-Nouby et al., “Dinov2: Learning robust\nvisual features without supervision,” arXiv preprint arXiv:2304.07193 , 2023.\n[75] T. Hastie, R. Tibshirani, J. Friedman, T. Hastie, R. Tibshirani, and J. Friedman,\n“Overview of supervised learning,” The elements of statistical learning: Data\nmining, inference, and prediction , pp. 9–41, 2009.\n[76] I. Goodfellow, Y . Bengio, and A. Courville, Deep learning. MIT press, 2016.\n[77] Y . Wang, C. Albrecht, N. A. A. Braham, L. Mou, and X. Zhu, “Self-supervised\nlearning in remote sensing: A review,” IEEE Geoscience and Remote Sensing\nMagazine, 2022.\n[78] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,\nA. Courville, and Y . Bengio, “Generative adversarial networks,”Communications\nof the ACM , vol. 63, no. 11, pp. 139–144, 2020.\n[79] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” arXiv preprint\narXiv:1312.6114, 2013.\n[80] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros, “Context\nencoders: Feature learning by inpainting,” in Proceedings of the IEEE conference\non computer vision and pattern recognition , 2016, pp. 2536–2544.\n[81] A. Brock, J. Donahue, and K. Simonyan, “Large scale gan training for high\nfidelity natural image synthesis,” in International Conference on Learning Rep-\nresentations, 2018.\n[82] C. Ledig, L. Theis, F. Husz ´ar, J. Caballero, A. Cunningham, A. Acosta, A. Aitken,\nA. Tejani, J. Totz, Z. Wang et al., “Photo-realistic single image super-resolution\nusing a generative adversarial network,” in Proceedings of the IEEE conference\non computer vision and pattern recognition , 2017, pp. 4681–4690.\n[83] T. Chen, S. Kornblith, M. Norouzi, and G. Hinton, “A simple framework for\ncontrastive learning of visual representations,” in International conference on\nmachine learning. PMLR, 2020, pp. 1597–1607.\n[84] K. He, H. Fan, Y . Wu, S. Xie, and R. Girshick, “Momentum contrast for\nunsupervised visual representation learning,” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , 2020, pp. 9729–9738.\n[85] X. Chen, H. Fan, R. Girshick, and K. He, “Improved baselines with momentum\ncontrastive learning,” arXiv preprint arXiv:2003.04297 , 2020.\n[86] X. Chen, S. Xie, and K. He, “An empirical study of training self-supervised\nvision transformers,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision , 2021, pp. 9640–9649.\n[87] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clustering for unsu-\npervised learning of visual features,” in Proceedings of the European conference\non computer vision (ECCV) , 2018, pp. 132–149.\n[88] M. Caron, I. Misra, J. Mairal, P. Goyal, P. Bojanowski, and A. Joulin, “Unsuper-\nvised learning of visual features by contrasting cluster assignments,” Advances\nin neural information processing systems , vol. 33, pp. 9912–9924, 2020.\n[89] J. Zbontar, L. Jing, I. Misra, Y . LeCun, and S. Deny, “Barlow twins: Self-\nsupervised learning via redundancy reduction,” in International Conference on\nMachine Learning. PMLR, 2021, pp. 12 310–12 320.\n[90] M. Noroozi and P. Favaro, “Unsupervised learning of visual representations\nby solving jigsaw puzzles,” in Computer Vision–ECCV 2016: 14th European\nConference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings,\nPart VI. Springer, 2016, pp. 69–84.\n[91] S. Gidaris, P. Singh, and N. Komodakis, “Unsupervised representation learning\nby predicting image rotations,” in ICLR 2018, 2018.\n[92] R. Zhang, P. Isola, and A. A. Efros, “Colorful image colorization,” in Computer\nVision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands,\nOctober 11-14, 2016, Proceedings, Part III 14 . Springer, 2016, pp. 649–666.\n[93] B. Peng, Q. Huang, J. V ongkusolkit, S. Gao, D. B. Wright, Z. N. Fang, and\nY . Qiang, “Urban flood mapping with bitemporal multispectral imagery via a\nself-supervised learning framework,” IEEE Journal of Selected Topics in Applied\nEarth Observations and Remote Sensing , vol. 14, pp. 2001–2016, 2020.\n[94] Q. Jin, Y . Ma, F. Fan, J. Huang, X. Mei, and J. Ma, “Adversarial autoencoder\nnetwork for hyperspectral unmixing,” IEEE Transactions on Neural Networks\nand Learning Systems , 2021.\n[95] S. Hou, H. Shi, X. Cao, X. Zhang, and L. Jiao, “Hyperspectral imagery\nclassification based on contrastive learning,” IEEE Transactions on Geoscience\nand Remote Sensing , vol. 60, pp. 1–13, 2021.\n[96] L. Scheibenreif, J. Hanna, M. Mommert, and D. Borth, “Self-supervised vision\ntransformers for land-cover segmentation and classification,” in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition , 2022,\npp. 1422–1431.\n[97] D. Muhtar, X. Zhang, and P. Xiao, “Index your position: A novel self-supervised\nlearning method for remote sensing images semantic segmentation,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 60, pp. 1–11, 2022.\n[98] H. Ji, Z. Gao, Y . Zhang, Y . Wan, C. Li, and T. Mei, “Few-shot scene classification\nof optical remote sensing images leveraging calibrated pretext tasks,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 60, pp. 1–13, 2022.\n[99] J. Gonz ´alez-Santiago, F. Schenkel, and W. Middelmann, “Self-supervised image\ncolorization for semantic segmentation of urban land cover,” in 2021 IEEE\nInternational Geoscience and Remote Sensing Symposium IGARSS. IEEE, 2021,\npp. 3468–3471.\n[100] V . Lialin, V . Deshpande, and A. Rumshisky, “Scaling down to scale up: A guide\nto parameter-efficient fine-tuning,” arXiv preprint arXiv:2303.15647 , 2023.\n[101] B. Lester, R. Al-Rfou, and N. Constant, “The power of scale for parameter-\nefficient prompt tuning,” in Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing , 2021, pp. 3045–3059.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n32\n[102] X. L. Li and P. Liang, “Prefix-tuning: Optimizing continuous prompts for\ngeneration,” in Proceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International Joint Conference on\nNatural Language Processing (Volume 1: Long Papers) , 2021, pp. 4582–4597.\n[103] M. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie, B. Hariharan, and S.-N. Lim,\n“Visual prompt tuning,” in European Conference on Computer Vision. Springer,\n2022, pp. 709–727.\n[104] M. U. Khattak, H. Rasheed, M. Maaz, S. Khan, and F. S. Khan, “Maple:\nMulti-modal prompt learning,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2023, pp. 19 113–19 122.\n[105] C. Oh, H. Hwang, H.-y. Lee, Y . Lim, G. Jung, J. Jung, H. Choi, and K. Song,\n“Blackvip: Black-box visual prompting for robust transfer learning,” in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2023, pp. 24 224–24 235.\n[106] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Ges-\nmundo, M. Attariyan, and S. Gelly, “Parameter-efficient transfer learning for nlp,”\nin International Conference on Machine Learning. PMLR, 2019, pp. 2790–2799.\n[107] S.-A. Rebuffi, H. Bilen, and A. Vedaldi, “Learning multiple visual domains with\nresidual adapters,” Advances in neural information processing systems , vol. 30,\n2017.\n[108] E. J. Hu, P. Wallis, Z. Allen-Zhu, Y . Li, S. Wang, L. Wang, W. Chenet al., “Lora:\nLow-rank adaptation of large language models,” in International Conference on\nLearning Representations, 2021.\n[109] Q. Zhang, M. Chen, A. Bukharin, P. He, Y . Cheng, W. Chen, and T. Zhao,\n“Adaptive budget allocation for parameter-efficient fine-tuning,” arXiv preprint\narXiv:2303.10512, 2023.\n[110] T. Dettmers, A. Pagnoni, A. Holtzman, and L. Zettlemoyer, “Qlora: Efficient\nfinetuning of quantized llms,” arXiv preprint arXiv:2305.14314 , 2023.\n[111] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y . Zhou, W. Li,\nand P. J. Liu, “Exploring the limits of transfer learning with a unified text-to-\ntext transformer,” The Journal of Machine Learning Research , vol. 21, no. 1, pp.\n5485–5551, 2020.\n[112] R. Nakano, J. Hilton, S. Balaji, J. Wu, L. Ouyang, C. Kim, C. Hesse, S. Jain,\nV . Kosaraju, W. Saunders et al., “Webgpt: Browser-assisted question-answering\nwith human feedback,” arXiv preprint arXiv:2112.09332 , 2021.\n[113] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards,\nY . Burda, N. Joseph, G. Brockman et al. , “Evaluating large language models\ntrained on code,” arXiv preprint arXiv:2107.03374 , 2021.\n[114] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,\nB. Rozi `ere, N. Goyal, E. Hambro, F. Azhar et al. , “Llama: Open and efficient\nfoundation language models,” arXiv preprint arXiv:2302.13971 , 2023.\n[115] A. Zeng, X. Liu, Z. Du, Z. Wang, H. Lai, M. Ding, Z. Yang, Y . Xu, W. Zheng,\nX. Xia et al., “Glm-130b: An open bilingual pre-trained model,” in The Eleventh\nInternational Conference on Learning Representations , 2022.\n[116] T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ili ´c, D. Hesslow, R. Castagn ´e,\nA. S. Luccioni, F. Yvon, M. Gall ´e et al., “Bloom: A 176b-parameter open-access\nmultilingual language model,” arXiv preprint arXiv:2211.05100 , 2022.\n[117] H. W. Chung, L. Hou, S. Longpre, B. Zoph, Y . Tay, W. Fedus, E. Li, X. Wang,\nM. Dehghani, S. Brahma et al., “Scaling instruction-finetuned language models,”\narXiv preprint arXiv:2210.11416 , 2022.\n[118] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,\nP. Barham, H. W. Chung, C. Sutton, S. Gehrmann et al., “Palm: Scaling language\nmodeling with pathways,” arXiv preprint arXiv:2204.02311 , 2022.\n[119] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang,\nS. Agarwal, K. Slama, A. Ray et al. , “Training language models to follow\ninstructions with human feedback,” Advances in Neural Information Processing\nSystems, vol. 35, pp. 27 730–27 744, 2022.\n[120] W. Wang, J. Dai, Z. Chen, Z. Huang, Z. Li, X. Zhu, X. Hu, T. Lu, L. Lu,\nH. Li et al. , “Internimage: Exploring large-scale vision foundation models\nwith deformable convolutions,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 2023, pp. 14 408–14 419.\n[121] Z. Liu, J. Ning, Y . Cao, Y . Wei, Z. Zhang, S. Lin, and H. Hu, “Video swin\ntransformer,” in Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition , 2022, pp. 3202–3211.\n[122] R. Wang, D. Chen, Z. Wu, Y . Chen, X. Dai, M. Liu, Y .-G. Jiang, L. Zhou, and\nL. Yuan, “Bevt: Bert pretraining of video transformers,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition , 2022, pp.\n14 733–14 743.\n[123] L. Wang, B. Huang, Z. Zhao, Z. Tong, Y . He, Y . Wang, Y . Wang, and Y . Qiao,\n“Videomae v2: Scaling video masked autoencoders with dual masking,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2023, pp. 14 549–14 560.\n[124] J. Zhang, J. Huang, S. Jin, and S. Lu, “Vision-language models for vision tasks:\nA survey,” arXiv preprint arXiv:2304.00685 , 2023.\n[125] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y . Hasson, K. Lenc,\nA. Mensch, K. Millican, M. Reynolds et al., “Flamingo: a visual language model\nfor few-shot learning,” Advances in Neural Information Processing Systems ,\nvol. 35, pp. 23 716–23 736, 2022.\n[126] L. Yuan, D. Chen, Y .-L. Chen, N. Codella, X. Dai, J. Gao, H. Hu, X. Huang,\nB. Li, C. Li et al. , “Florence: A new foundation model for computer vision,”\narXiv preprint arXiv:2111.11432 , 2021.\n[127] Y . Wang, K. Li, Y . Li, Y . He, B. Huang, Z. Zhao, H. Zhang, J. Xu, Y . Liu,\nZ. Wang et al. , “Internvideo: General video foundation models via generative\nand discriminative learning,” arXiv preprint arXiv:2212.03191 , 2022.\n[128] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, “Hierarchical text-\nconditional image generation with clip latents,” arXiv preprint arXiv:2204.06125,\n2022.\n[129] OpenAI. (2023) Gpt-4 technical report.\n[130] X. Chen, X. Wang, S. Changpinyo, A. Piergiovanni, P. Padlewski, D. Salz,\nS. Goodman, A. Grycner, B. Mustafa, L. Beyer, A. Kolesnikov, J. Puigcerver,\nN. Ding, K. Rong, H. Akbari, G. Mishra, L. Xue, A. V . Thapliyal,\nJ. Bradbury, W. Kuo, M. Seyedhosseini, C. Jia, B. K. Ayan, C. R.\nRuiz, A. P. Steiner, A. Angelova, X. Zhai, N. Houlsby, and R. Soricut,\n“PaLI: A jointly-scaled multilingual language-image model,” in The Eleventh\nInternational Conference on Learning Representations, 2023. [Online]. Available:\nhttps://openreview.net/forum?id=mWV oBz4W0u\n[131] H. Xu, Q. Ye, M. Yan, Y . Shi, J. Ye, Y . Xu, C. Li, B. Bi, Q. Qian, W. Wanget al.,\n“mplug-2: A modularized multi-modal foundation model across text, image and\nvideo,” arXiv preprint arXiv:2302.00402 , 2023.\n[132] Y . Zhang, K. Gong, K. Zhang, H. Li, Y . Qiao, W. Ouyang, and X. Yue,\n“Meta-transformer: A unified framework for multimodal learning,” arXiv preprint\narXiv:2307.10802, 2023.\n[133] K. Ayush, B. Uzkent, C. Meng, K. Tanmay, M. Burke, D. Lobell, and S. Ermon,\n“Geography-aware self-supervised learning,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision , 2021, pp. 10 181–10 190.\n[134] G. Mai, N. Lao, Y . He, J. Song, and S. Ermon, “Csp: Self-supervised con-\ntrastive spatial pre-training for geospatial-visual representations,” arXiv preprint\narXiv:2305.01118, 2023.\n[135] L. Jiao, J. Gao, X. Liu, F. Liu, S. Yang, and B. Hou, “Multi-scale representation\nlearning for image classification: A survey,” IEEE Transactions on Artificial\nIntelligence, 2021.\n[136] Y . Yang, L. Jiao, F. Liu, X. Liu, L. Li, P. Chen, and S. Yang, “An explainable\nspatial-frequency multi-scale transformer for remote sensing scene classification,”\nIEEE Transactions on Geoscience and Remote Sensing , 2023.\n[137] X. Tang, Q. Ma, X. Zhang, F. Liu, J. Ma, and L. Jiao, “Attention consistent\nnetwork for remote sensing scene classification,” IEEE Journal of Selected Topics\nin Applied Earth Observations and Remote Sensing , vol. 14, pp. 2030–2045,\n2021.\n[138] W. Chen, S. Ouyang, W. Tong, X. Li, X. Zheng, and L. Wang, “Gcsanet: A\nglobal context spatial attention deep learning network for remote sensing scene\nclassification,” IEEE Journal of Selected Topics in Applied Earth Observations\nand Remote Sensing , vol. 15, pp. 1150–1162, 2022.\n[139] L. Li, P. Liang, J. Ma, L. Jiao, X. Guo, F. Liu, and C. Sun, “A multiscale\nself-adaptive attention network for remote sensing scene classification,” Remote\nSensing, vol. 12, no. 14, p. 2209, 2020.\n[140] F. Liu, X. Qian, L. Jiao, X. Zhang, L. Li, and Y . Cui, “Contrastive learning-\nbased dual dynamic gcn for sar image scene classification,” IEEE Transactions\non Neural Networks and Learning Systems , 2022.\n[141] Q. Zeng and J. Geng, “Task-specific contrastive learning for few-shot remote\nsensing image scene classification,” ISPRS Journal of Photogrammetry and\nRemote Sensing, vol. 191, pp. 143–154, 2022.\n[142] H. Huang, Z. Mou, Y . Li, Q. Li, J. Chen, and H. Li, “Spatial-temporal invariant\ncontrastive learning for remote sensing scene classification,” IEEE Geoscience\nand Remote Sensing Letters , vol. 19, pp. 1–5, 2022.\n[143] Z. Li, B. Hou, X. Guo, S. Ma, Y . Cui, S. Wang, and L. Jiao, “Contrastive learning\nbased on multi-scale hard features for remote sensing image scene classification,”\nIEEE Transactions on Geoscience and Remote Sensing , no. 99, pp. 1–1, 2023.\n[144] J. Xu, Y . Li, Q. Shi, and L. He, “Occluded scene classification via cascade\nsupervised contrastive learning,” IEEE Journal of Selected Topics in Applied\nEarth Observations and Remote Sensing , 2023.\n[145] W. Miao, J. Geng, and W. Jiang, “Semi-supervised remote-sensing image\nscene classification using representation consistency siamese network,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 60, pp. 1–14, 2022.\n[146] C. Peng, Y . Li, L. Jiao, and R. Shang, “Efficient convolutional neural architecture\nsearch for remote sensing image scene classification,” IEEE Transactions on\nGeoscience and Remote Sensing , vol. 59, no. 7, pp. 6092–6105, 2020.\n[147] X. Qian, F. Liu, L. Jiao, X. Zhang, P. Chen, L. Li, J. Gu, and Y . Cui, “A\nhybrid network with structural constraints for sar image scene classification,”\nIEEE Transactions on Geoscience and Remote Sensing , vol. 60, pp. 1–17, 2021.\n[148] K. Xu, P. Deng, and H. Huang, “Vision transformer: An excellent teacher for\nguiding small networks in remote sensing image scene classification,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 60, pp. 1–15, 2022.\n[149] M. Zhao, Q. Meng, L. Zhang, X. Hu, and L. Bruzzone, “Local and long-range\ncollaborative learning for remote sensing scene classification,” IEEE Transactions\non Geoscience and Remote Sensing , 2023.\n[150] X. Lu, L. Jiao, L. Li, F. Liu, X. Liu, S. Yang, Z. Feng, and P. Chen, “Weak-\nto-strong consistency learning for semisupervised image segmentation,” IEEE\nTransactions on Geoscience and Remote Sensing , 2023.\n[151] W. Li, H. Chen, and Z. Shi, “Semantic segmentation of remote sensing images\nwith self-supervised multitask representation learning,” IEEE Journal of Selected\nTopics in Applied Earth Observations and Remote Sensing , vol. 14, pp. 6438–\n6450, 2021.\n[152] M. Yang, L. Jiao, F. Liu, B. Hou, S. Yang, Y . Zhang, and J. Wang, “Coarse-to-\nfine contrastive self-supervised feature learning for land-cover classification in\nsar images with limited labeled data,” IEEE Transactions on Image Processing ,\nvol. 31, pp. 6502–6516, 2022.\n[153] V . Marsocci and S. Scardapane, “Continual barlow twins: continual self-\nsupervised learning for remote sensing semantic segmentation,” IEEE Journal\nof Selected Topics in Applied Earth Observations and Remote Sensing , 2023.\n[154] Z. Xue, X. Yu, A. Yu, B. Liu, P. Zhang, and S. Wu, “Self-supervised feature\nlearning for multimodal remote sensing image land cover classification,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 60, pp. 1–15, 2022.\n[155] Z. Xue, B. Liu, A. Yu, X. Yu, P. Zhang, and X. Tan, “Self-supervised feature\nrepresentation and few-shot land cover classification of multimodal remote\nsensing images,” IEEE Transactions on Geoscience and Remote Sensing, vol. 60,\npp. 1–18, 2022.\n[156] X. Lu, L. Jiao, F. Liu, S. Yang, X. Liu, Z. Feng, L. Li, and P. Chen, “Simple\nand efficient: A semisupervised learning framework for remote sensing image\nsemantic segmentation,” IEEE Transactions on Geoscience and Remote Sensing ,\nvol. 60, pp. 1–16, 2022.\n[157] W. Li, H. Gao, Y . Su, and B. M. Momanyi, “Unsupervised domain adaptation\nfor remote sensing semantic segmentation with transformer,” Remote Sensing ,\nvol. 14, no. 19, p. 4942, 2022.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n33\n[158] R. Shang, J. Zhang, L. Jiao, Y . Li, N. Marturi, and R. Stolkin, “Multi-scale\nadaptive feature fusion network for semantic segmentation in remote sensing\nimages,” Remote Sensing, vol. 12, no. 5, p. 872, 2020.\n[159] P. He, L. Jiao, R. Shang, S. Wang, X. Liu, D. Quan, K. Yang, and D. Zhao,\n“Manet: Multi-scale aware-relation network for semantic segmentation in aerial\nscenes,” IEEE Transactions on Geoscience and Remote Sensing , vol. 60, pp.\n1–15, 2022.\n[160] M. Luo and S. Ji, “Cross-spatiotemporal land-cover classification from vhr remote\nsensing images with deep learning based domain adaptation,” ISPRS Journal of\nPhotogrammetry and Remote Sensing , vol. 191, pp. 105–128, 2022.\n[161] J. Bai, Z. Wen, Z. Xiao, F. Ye, Y . Zhu, M. Alazab, and L. Jiao, “Hyperspectral\nimage classification based on multibranch attention transformer networks,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 60, pp. 1–17, 2022.\n[162] W. Chen, S. Ouyang, J. Yang, X. Li, G. Zhou, and L. Wang, “Jagan: A framework\nfor complex land cover classification using gaofen-5 ahsi images,” IEEE Journal\nof Selected Topics in Applied Earth Observations and Remote Sensing , vol. 15,\npp. 1591–1603, 2022.\n[163] A. Li, L. Jiao, H. Zhu, L. Li, and F. Liu, “Multitask semantic boundary\nawareness network for remote sensing image segmentation,” IEEE Transactions\non Geoscience and Remote Sensing , vol. 60, pp. 1–14, 2021.\n[164] G. Lenczner, A. Chan-Hon-Tong, B. Le Saux, N. Luminari, and G. Le Besnerais,\n“Dial: Deep interactive and active learning for semantic segmentation in remote\nsensing,” IEEE Journal of Selected Topics in Applied Earth Observations and\nRemote Sensing, vol. 15, pp. 3376–3389, 2022.\n[165] X. Tang, M. Li, J. Ma, X. Zhang, F. Liu, and L. Jiao, “Emtcal: Efficient\nmultiscale transformer and cross-level attention learning for remote sensing scene\nclassification,” IEEE Transactions on Geoscience and Remote Sensing , vol. 60,\npp. 1–15, 2022.\n[166] M. Ghanbari, L. Xu, and D. A. Clausi, “Local and global spatial information for\nland cover semi-supervised classification of complex polarimetric sar data,” IEEE\nJournal of Selected Topics in Applied Earth Observations and Remote Sensing ,\n2023.\n[167] H. Dong, W. Ma, Y . Wu, J. Zhang, and L. Jiao, “Self-supervised representation\nlearning for remote sensing image change detection based on temporal predic-\ntion,” Remote Sensing, vol. 12, no. 11, p. 1868, 2020.\n[168] P. Chen, L. Guo, X. Zhang, K. Qin, W. Ma, and L. Jiao, “Attention-guided\nsiamese fusion network for change detection of remote sensing images,” Remote\nSensing, vol. 13, no. 22, p. 4597, 2021.\n[169] W. Zhang, L. Jiao, F. Liu, S. Yang, and J. Liu, “Adaptive contourlet fusion clus-\ntering for sar image change detection,” IEEE Transactions on Image Processing,\nvol. 31, pp. 2295–2308, 2022.\n[170] Z. Li, C. Yan, Y . Sun, and Q. Xin, “A densely attentive refinement network\nfor change detection based on very-high-resolution bitemporal remote sensing\nimages,” IEEE Transactions on Geoscience and Remote Sensing , vol. 60, pp.\n1–18, 2022.\n[171] Z. Lv, F. Wang, G. Cui, J. A. Benediktsson, T. Lei, and W. Sun, “Spatial–spectral\nattention network guided with change magnitude image for land cover change\ndetection using remote sensing images,” IEEE Transactions on Geoscience and\nRemote Sensing, vol. 60, pp. 1–12, 2022.\n[172] C. Zhang, L. Wang, S. Cheng, and Y . Li, “Swinsunet: Pure transformer network\nfor remote sensing image change detection,” IEEE Transactions on Geoscience\nand Remote Sensing , vol. 60, pp. 1–13, 2022.\n[173] L. Ding, H. Guo, S. Liu, L. Mou, J. Zhang, and L. Bruzzone, “Bi-temporal\nsemantic reasoning for the semantic change detection in hr remote sensing\nimages,” IEEE Transactions on Geoscience and Remote Sensing , vol. 60, pp.\n1–14, 2022.\n[174] W. Zhang, L. Jiao, F. Liu, S. Yang, W. Song, and J. Liu, “Sparse feature clustering\nnetwork for unsupervised sar image change detection,” IEEE Transactions on\nGeoscience and Remote Sensing , vol. 60, pp. 1–13, 2022.\n[175] H. Dong, L. Jiao, W. Ma, F. Liu, X. Liu, L. Li, and S. Yang, “Deep shearlet\nnetwork for change detection in sar images,” IEEE Transactions on Geoscience\nand Remote Sensing , vol. 60, pp. 1–15, 2022.\n[176] H. Dong, W. Ma, L. Jiao, F. Liu, and L. Li, “A multiscale self-attention deep\nclustering for change detection in sar images,” IEEE Transactions on Geoscience\nand Remote Sensing , vol. 60, pp. 1–16, 2021.\n[177] H. Zheng, M. Gong, T. Liu, F. Jiang, T. Zhan, D. Lu, and M. Zhang, “Hfa-net:\nHigh frequency attention siamese network for building change detection in vhr\nremote sensing images,” Pattern Recognition, vol. 129, p. 108717, 2022.\n[178] J. Wang, F. Liu, L. Jiao, H. Wang, H. Yang, X. Liu, L. Li, and P. Chen, “Sscfnet: A\nspatial-spectral cross fusion network for remote sensing change detection,” IEEE\nJournal of Selected Topics in Applied Earth Observations and Remote Sensing ,\n2023.\n[179] M. Zhang, Z. Liu, J. Feng, L. Liu, and L. Jiao, “Remote sensing image change\ndetection based on deep multi-scale multi-attention siamese transformer network,”\nRemote Sensing, vol. 15, no. 3, p. 842, 2023.\n[180] Y . Li, B. He, F. Melgani, and T. Long, “Point-based weakly supervised learning\nfor object detection in high spatial resolution remote sensing images,” IEEE\nJournal of Selected Topics in Applied Earth Observations and Remote Sensing ,\nvol. 14, pp. 5361–5371, 2021.\n[181] T. Zhang, X. Zhang, P. Zhu, P. Chen, X. Tang, C. Li, and L. Jiao, “Foreground\nrefinement network for rotated object detection in remote sensing images,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 60, pp. 1–13, 2021.\n[182] W. Zhang, L. Jiao, Y . Li, Z. Huang, and H. Wang, “Laplacian feature pyramid\nnetwork for object detection in vhr optical remote sensing images,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 60, pp. 1–14, 2021.\n[183] J. Bai, J. Ren, Y . Yang, Z. Xiao, W. Yu, V . Havyarimana, and L. Jiao, “Object\ndetection in large-scale remote-sensing images based on time-frequency analysis\nand feature optimization,” IEEE Transactions on Geoscience and Remote Sensing,\nvol. 60, pp. 1–16, 2021.\n[184] Y . Liu, S. Zhang, Z. Wang, B. Zhao, and L. Zou, “Global perception network\nfor salient object detection in remote sensing images,” IEEE Transactions on\nGeoscience and Remote Sensing , vol. 60, pp. 1–12, 2022.\n[185] G. Cheng, X. Xie, W. Chen, X. Feng, X. Yao, and J. Han, “Self-guided\nproposal generation for weakly supervised object detection,” IEEE Transactions\non Geoscience and Remote Sensing , vol. 60, pp. 1–11, 2022.\n[186] Y . Ye, X. Ren, B. Zhu, T. Tang, X. Tan, Y . Gui, and Q. Yao, “An adaptive\nattention fusion mechanism convolutional network for object detection in remote\nsensing images,” Remote Sensing, vol. 14, no. 3, p. 516, 2022.\n[187] T. Zhang, X. Zhang, P. Zhu, X. Jia, X. Tang, and L. Jiao, “Generalized few-shot\nobject detection in remote sensing images,” ISPRS Journal of Photogrammetry\nand Remote Sensing , vol. 195, pp. 353–364, 2023.\n[188] G. Wang, X. Zhang, Z. Peng, X. Jia, X. Tang, and L. Jiao, “Mol: Towards\naccurate weakly supervised remote sensing object detection via multi-view noisy\nlearning,” ISPRS Journal of Photogrammetry and Remote Sensing , vol. 196, pp.\n457–470, 2023.\n[189] C. Li, G. Cheng, G. Wang, P. Zhou, and J. Han, “Instance-aware distillation\nfor efficient object detection in remote sensing images,” IEEE Transactions on\nGeoscience and Remote Sensing , vol. 61, pp. 1–11, 2023.\n[190] T. Zhang, Y . Zhuang, H. Chen, L. Chen, G. Wang, P. Gao, and H. Dong, “Object-\ncentric masked image modeling based self-supervised pretraining for remote\nsensing object detection,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , 2023.\n[191] Y . Han, W. Meng, W. Tang, and L. Tang, “Capsule-inferenced object detection\nfor remote sensing images,” IEEE Journal of Selected Topics in Applied Earth\nObservations and Remote Sensing , 2023.\n[192] J. Zhang, X. Jia, and J. Hu, “Error bounded foreground and background modeling\nfor moving object detection in satellite videos,”IEEE Transactions on Geoscience\nand Remote Sensing , vol. 58, no. 4, pp. 2659–2669, 2019.\n[193] J. Zhang, X. Jia, J. Hu, and J. Chanussot, “Online structured sparsity-based\nmoving-object detection from satellite videos,” IEEE Transactions on Geoscience\nand Remote Sensing , vol. 58, no. 9, pp. 6420–6433, 2020.\n[194] J. Feng, D. Zeng, X. Jia, X. Zhang, J. Li, Y . Liang, and L. Jiao, “Cross-frame\nkeypoint-based and spatial motion information-guided networks for moving vehi-\ncle detection and tracking in satellite videos,” ISPRS Journal of Photogrammetry\nand Remote Sensing , vol. 177, pp. 116–130, 2021.\n[195] J. Zhang, X. Jia, J. Hu, and K. Tan, “Moving vehicle detection for remote sensing\nvideo surveillance with nonstationary satellite platform,” IEEE transactions on\npattern analysis and machine intelligence , vol. 44, no. 9, pp. 5185–5198, 2021.\n[196] Z. Pi, L. Jiao, F. Liu, X. Liu, L. Li, B. Hou, and S. Yang, “Very low-resolution\nmoving vehicle detection in satellite videos,” IEEE Transactions on Geoscience\nand Remote Sensing , vol. 60, pp. 1–17, 2022.\n[197] C. Xiao, Q. Yin, X. Ying, R. Li, S. Wu, M. Li, L. Liu, W. An, and Z. Chen,\n“Dsfnet: Dynamic and static fusion network for moving object detection in\nsatellite videos,” IEEE Geoscience and Remote Sensing Letters , vol. 19, pp. 1–5,\n2021.\n[198] J. Feng, Y . Liang, X. Zhang, J. Zhang, and L. Jiao, “Sdanet: semantic-embedded\ndensity adaptive network for moving vehicle detection in satellite videos,” IEEE\ntransactions on image processing , vol. 32, pp. 1788–1801, 2023.\n[199] J. Shao, B. Du, C. Wu, M. Gong, and T. Liu, “Hrsiam: High-resolution siamese\nnetwork, towards space-borne satellite video tracking,” IEEE Transactions on\nImage Processing, vol. 30, pp. 3056–3068, 2021.\n[200] W. Zhang, L. Jiao, F. Liu, S. Yang, and J. Liu, “Dfat: Dynamic feature-adaptive\ntracking,” IEEE Transactions on Circuits and Systems for Video Technology ,\nvol. 33, no. 1, pp. 43–58, 2022.\n[201] Y . Cui, B. Hou, Q. Wu, B. Ren, S. Wang, and L. Jiao, “Remote sensing object\ntracking with deep reinforcement learning under occlusion,” IEEE transactions\non geoscience and remote sensing , vol. 60, pp. 1–13, 2021.\n[202] W. Zhang, L. Jiao, F. Liu, L. Li, X. Liu, and J. Liu, “Mblt: Learning motion\nand background for vehicle tracking in satellite videos,” IEEE Transactions on\nGeoscience and Remote Sensing , vol. 60, pp. 1–15, 2021.\n[203] W. Song, L. Jiao, F. Liu, X. Liu, L. Li, S. Yang, B. Hou, and W. Zhang, “A joint\nsiamese attention-aware network for vehicle object tracking in satellite videos,”\nIEEE Transactions on Geoscience and Remote Sensing , vol. 60, pp. 1–17, 2022.\n[204] Y . Li, C. Bian, and H. Chen, “Object tracking in satellite videos: correlation\nparticle filter tracking method with motion estimation by kalman filter,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 60, pp. 1–12, 2022.\n[205] S. Chen, T. Wang, H. Wang, Y . Wang, J. Hong, T. Dong, and Z. Li, “Vehicle\ntracking on satellite video based on historical model,” IEEE Journal of Selected\nTopics in Applied Earth Observations and Remote Sensing , vol. 15, pp. 7784–\n7796, 2022.\n[206] Y . Li and C. Bian, “Object tracking in satellite videos: A spatial-temporal\nregularized correlation filter tracking method with interacting multiple model,”\nIEEE Geoscience and Remote Sensing Letters , vol. 19, pp. 1–5, 2022.\n[207] Y . Nie, C. Bian, and L. Li, “Object tracking in satellite videos based on\nsiamese network with multidimensional information-aware and temporal motion\ncompensation,” IEEE Geoscience and Remote Sensing Letters , vol. 19, pp. 1–5,\n2022.\n[208] X. Li, L. Jiao, H. Zhu, F. Liu, S. Yang, X. Zhang, S. Wang, and R. Qu,\n“A collaborative learning tracking network for remote sensing videos,” IEEE\nTransactions on Cybernetics , vol. 53, no. 3, pp. 1954–1967, 2022.\n[209] R. Zhang, L. Jiao, L. Li, X. Liu, F. Liu, and S. Yang, “A quantum evolutionary\nlearning tracker for video,” IEEE Transactions on Evolutionary Computation ,\n2023.\n[210] J. Yang, Z. Pan, Z. Wang, B. Lei, and Y . Hu, “Siammdm: An adaptive\nfusion network with dynamic template for real-time satellite video single object\ntracking,” IEEE Transactions on Geoscience and Remote Sensing , 2023.\n[211] W. Ao, Y . Fu, X. Hou, and F. Xu, “Needles in a haystack: Tracking city-\nscale moving vehicles from continuously moving satellite,” IEEE Transactions\non Image Processing , vol. 29, pp. 1944–1957, 2019.\n[212] Q. He, X. Sun, Z. Yan, B. Li, and K. Fu, “Multi-object tracking in satellite\nvideos with graph-based multitask modeling,” IEEE Transactions on Geoscience\nand Remote Sensing , vol. 60, pp. 1–13, 2022.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n34\n[213] J. Zhang, X. Zhang, Z. Huang, X. Cheng, J. Feng, and L. Jiao, “Bidirectional\nmultiple object tracking based on trajectory criteria in satellite videos,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 61, pp. 1–14, 2023.\n[214] L. Kong, Z. Yan, Y . Zhang, W. Diao, Z. Zhu, and L. Wang, “Cftracker:\nMulti-object tracking with cross-frame connections in satellite videos,” IEEE\nTransactions on Geoscience and Remote Sensing , 2023.\n[215] Y . Sun, S. Feng, X. Li, Y . Ye, J. Kang, and X. Huang, “Visual grounding in remote\nsensing images,” in Proceedings of the 30th ACM International Conference on\nMultimedia, 2022, pp. 404–412.\n[216] Y . Zhan, Z. Xiong, and Y . Yuan, “Rsvg: Exploring data and models for visual\ngrounding on remote sensing data,” IEEE Transactions on Geoscience and\nRemote Sensing, vol. 61, pp. 1–13, 2023.\n[217] Z. Yuan, L. Mou, Y . Hua, and X. X. Zhu, “Rrsis: Referring remote sensing image\nsegmentation,” arXiv preprint arXiv:2306.08625 , 2023.\n[218] S. Wu, X. Zhang, X. Wang, C. Li, and L. Jiao, “Scene attention mechanism for\nremote sensing image caption generation,” in 2020 International Joint Conference\non Neural Networks (IJCNN) . IEEE, 2020, pp. 1–7.\n[219] X. Ye, S. Wang, Y . Gu, J. Wang, R. Wang, B. Hou, F. Giunchiglia, and L. Jiao,\n“A joint-training two-stage method for remote sensing image captioning,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 60, pp. 1–16, 2022.\n[220] Y . Li, X. Zhang, J. Gu, C. Li, X. Wang, X. Tang, and L. Jiao, “Recurrent attention\nand semantic gate for remote sensing image captioning,” IEEE Transactions on\nGeoscience and Remote Sensing , vol. 60, pp. 1–16, 2021.\n[221] S. Wang, X. Ye, Y . Gu, J. Wang, Y . Meng, J. Tian, B. Hou, and L. Jiao,\n“Multi-label semantic feature fusion for remote sensing image captioning,” ISPRS\nJournal of Photogrammetry and Remote Sensing , vol. 184, pp. 1–18, 2022.\n[222] Y . Li, S. Fang, L. Jiao, R. Liu, and R. Shang, “A multi-level attention model for\nremote sensing image captions,” Remote Sensing, vol. 12, no. 6, p. 939, 2020.\n[223] Z. Chen, J. Wang, A. Ma, and Y . Zhong, “Typeformer: Multiscale transformer\nwith type controller for remote sensing image caption,” IEEE Geoscience and\nRemote Sensing Letters , vol. 19, pp. 1–5, 2022.\n[224] J. Wang, Z. Chen, A. Ma, and Y . Zhong, “Capformer: Pure transformer for remote\nsensing image caption,” in IGARSS 2022-2022 IEEE International Geoscience\nand Remote Sensing Symposium . IEEE, 2022, pp. 7996–7999.\n[225] Q. Yang, Z. Ni, and P. Ren, “Meta captioning: A meta learning based remote\nsensing image captioning framework,” ISPRS Journal of Photogrammetry and\nRemote Sensing, vol. 186, pp. 190–200, 2022.\n[226] Y . Wang, W. Zhang, Z. Zhang, X. Gao, and X. Sun, “Multiscale multiinteraction\nnetwork for remote sensing image captioning,” IEEE Journal of Selected Topics\nin Applied Earth Observations and Remote Sensing , vol. 15, pp. 2154–2165,\n2022.\n[227] U. Zia, M. M. Riaz, and A. Ghafoor, “Transforming remote sensing images to\ntextual descriptions,” International Journal of Applied Earth Observation and\nGeoinformation, vol. 108, p. 102741, 2022.\n[228] X. Zhang, Y . Li, X. Wang, F. Liu, Z. Wu, X. Cheng, and L. Jiao, “Multi-source\ninteractive stair attention for remote sensing image captioning,” Remote Sensing,\nvol. 15, no. 3, p. 579, 2023.\n[229] S. Chang and P. Ghamisi, “Changes to captions: An attentive network for remote\nsensing change captioning,” arXiv preprint arXiv:2304.01091 , 2023.\n[230] Z. Zhang, L. Jiao, L. Li, X. Liu, P. Chen, F. Liu, Y . Li, and Z. Guo, “A spatial\nhierarchical reasoning network for remote sensing visual question answering,”\nIEEE Transactions on Geoscience and Remote Sensing , vol. 61, pp. 1–15, 2023.\n[231] C. Chappuis, V . Zermatten, S. Lobry, B. Le Saux, and D. Tuia, “Prompt-rsvqa:\nPrompting visual context to a language model for remote sensing visual question\nanswering,” in Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 2022, pp. 1372–1381.\n[232] Z. Yuan, L. Mou, Q. Wang, and X. X. Zhu, “From easy to hard: Learning\nlanguage-guided curriculum for visual question answering on remote sensing\ndata,” IEEE Transactions on Geoscience and Remote Sensing , vol. 60, pp. 1–11,\n2022.\n[233] Y . Bazi, M. M. Al Rahhal, M. L. Mekhalfi, M. A. Al Zuair, and F. Melgani,\n“Bi-modal transformer-based approach for visual question answering in remote\nsensing imagery,”IEEE Transactions on Geoscience and Remote Sensing, vol. 60,\npp. 1–11, 2022.\n[234] M. M. Al Rahhal, Y . Bazi, S. O. Alsaleh, M. Al-Razgan, M. L. Mekhalfi,\nM. Al Zuair, and N. Alajlan, “Open-ended remote sensing visual question\nanswering with transformers,” International Journal of Remote Sensing , vol. 43,\nno. 18, pp. 6809–6823, 2022.\n[235] C. Chappuis, V . Mendez, E. Walt, S. Lobry, B. Le Saux, and D. Tuia, “Language\ntransformers for remote sensing visual question answering,” in IGARSS 2022-\n2022 IEEE International Geoscience and Remote Sensing Symposium . IEEE,\n2022, pp. 4855–4858.\n[236] Z. Yuan, L. Mou, Z. Xiong, and X. X. Zhu, “Change detection meets visual\nquestion answering,” IEEE Transactions on Geoscience and Remote Sensing ,\nvol. 60, pp. 1–13, 2022.\n[237] L. Bashmal, Y . Bazi, F. Melgani, R. Ricci, M. M. Al Rahhal, and M. Zuair,\n“Visual question generation from remote sensing images,” IEEE Journal of\nSelected Topics in Applied Earth Observations and Remote Sensing , vol. 16,\npp. 3279–3293, 2023.\n[238] M. Tan, R. Pang, and Q. V . Le, “Efficientdet: Scalable and efficient object\ndetection,” in Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2020, pp. 10 781–10 790.\n[239] L. Jiao, R. Zhang, F. Liu, S. Yang, B. Hou, L. Li, and X. Tang, “New generation\ndeep learning for video object detection: A survey,” IEEE Transactions on Neural\nNetworks and Learning Systems , vol. 33, no. 8, pp. 3195–3215, 2021.\n[240] Y . Li, J. Licheng, Z. Huang, X. Zhang, R. Zhang, X. Song, C. Tian, Z. Zhang,\nF. Liu, Y . Shuyuanet al., “Deep learning-based object tracking in satellite videos:\nA comprehensive survey with a new dataset,” IEEE Geoscience and Remote\nSensing Magazine, 2022.\n[241] G. Sheng, W. Yang, T. Xu, and H. Sun, “High-resolution satellite scene classifi-\ncation using a sparse coding based multiple feature combination,” International\njournal of remote sensing , vol. 33, no. 8, pp. 2395–2412, 2012.\n[242] Y . Yang and S. Newsam, “Bag-of-visual-words and spatial extensions for land-use\nclassification,” in Proceedings of the 18th SIGSPATIAL international conference\non advances in geographic information systems , 2010, pp. 270–279.\n[243] G.-S. Xia, J. Hu, F. Hu, B. Shi, X. Bai, Y . Zhong, L. Zhang, and X. Lu, “Aid:\nA benchmark data set for performance evaluation of aerial scene classification,”\nIEEE Transactions on Geoscience and Remote Sensing , vol. 55, no. 7, pp. 3965–\n3981, 2017.\n[244] G. Cheng, J. Han, and X. Lu, “Remote sensing image scene classification:\nBenchmark and state of the art,” Proceedings of the IEEE , vol. 105, no. 10,\npp. 1865–1883, 2017.\n[245] T. Xiao, Y . Liu, B. Zhou, Y . Jiang, and J. Sun, “Unified perceptual parsing for\nscene understanding,” in Proceedings of the European conference on computer\nvision (ECCV), 2018, pp. 418–434.\n[246] R. H ¨ansch, C. Persello, G. Vivone, J. C. Navarro, A. Boulch, S. Lefevre, and\nB. Saux, “The 2022 ieee grss data fusion contest: Semisupervised learning\n[technical committees],” IEEE geoscience and remote sensing magazine , vol. 10,\nno. 1, pp. 334–337, 2022.\n[247] J. Castillo-Navarro, B. Le Saux, A. Boulch, N. Audebert, and S. Lef `evre, “Semi-\nsupervised semantic segmentation in earth observation: The minifrance suite,\ndataset analysis and multi-task network study,” Machine Learning , pp. 1–36,\n2021.\n[248] Isprs, 2016. international society for photogrammetry and remote sensing.\n2d semantic labeling challenge. [Online]. Available: https://www.isprs.org/\neducation/benchmarks/UrbanSemLab/2d-sem-label-vaihingen.aspx\n[249] J. Li, S. Zi, R. Song, Y . Li, Y . Hu, and Q. Du, “A stepwise domain adaptive seg-\nmentation network with covariate shift alleviation for remote sensing imagery,”\nIEEE Transactions on Geoscience and Remote Sensing , vol. 60, pp. 1–15, 2022.\n[250] X.-Y . Tong, G.-S. Xia, Q. Lu, H. Shen, S. Li, S. You, and L. Zhang, “Land-cover\nclassification with high-resolution remote sensing images using transferable deep\nmodels,” Remote Sensing of Environment , vol. 237, p. 111322, 2020.\n[251] Isprs, 2016. international society for photogrammetry and remote sensing.\n2d semantic labeling challenge. [Online]. Available: https://www.isprs.org/\neducation/benchmarks/UrbanSemLab/2d-sem-label-potsdam.aspx\n[252] L.-C. Chen, Y . Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-\ndecoder with atrous separable convolution for semantic image segmentation,”\nin Proceedings of the European conference on computer vision (ECCV) , 2018,\npp. 801–818.\n[253] Z. Liu, H. Mao, C.-Y . Wu, C. Feichtenhofer, T. Darrell, and S. Xie, “A convnet\nfor the 2020s,” in Proceedings of the IEEE/CVF conference on computer vision\nand pattern recognition , 2022, pp. 11 976–11 986.\n[254] M. Guo, C. Lu, Z. Liu, M. Cheng, and S. Hu, “Visual attention network,”\nComputational Visual Media , pp. 1–20, 2023.\n[255] Y . Li, C.-Y . Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and C. Feichten-\nhofer, “Mvitv2: Improved multiscale vision transformers for classification and\ndetection,” in Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2022, pp. 4804–4814.\n[256] E. Xie, W. Wang, Z. Yu, A. Anandkumar, J. M. Alvarez, and P. Luo, “Segformer:\nSimple and efficient design for semantic segmentation with transformers,” Ad-\nvances in Neural Information Processing Systems , vol. 34, pp. 12 077–12 090,\n2021.\n[257] Z. Liu, H. Hu, Y . Lin, Z. Yao, Z. Xie, Y . Wei, J. Ning, Y . Cao, Z. Zhang, L. Dong\net al., “Swin transformer v2: Scaling up capacity and resolution,” in Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition , 2022,\npp. 12 009–12 019.\n[258] Y . Rao, W. Zhao, G. Chen, Y . Tang, Z. Zhu, G. Huang, J. Zhou, and J. Lu,\n“Denseclip: Language-guided dense prediction with context-aware prompting,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022, pp. 18 082–18 091.\n[259] X. Xie, G. Cheng, J. Wang, X. Yao, and J. Han, “Oriented r-cnn for object de-\ntection,” in Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 3520–3529.\n[260] G. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu, M. Pelillo, and\nL. Zhang, “Dota: A large-scale dataset for object detection in aerial images,” in\nProceedings of the IEEE conference on computer vision and pattern recognition ,\n2018, pp. 3974–3983.\n[261] G. Cheng, J. Wang, K. Li, X. Xie, C. Lang, Y . Yao, and J. Han, “Anchor-\nfree oriented proposal generator for object detection,” IEEE Transactions on\nGeoscience and Remote Sensing , vol. 60, pp. 1–11, 2022.\n[262] K. Li, G. Wan, G. Cheng, L. Meng, and J. Han, “Object detection in optical\nremote sensing images: A survey and a new benchmark,” ISPRS journal of\nphotogrammetry and remote sensing , vol. 159, pp. 296–307, 2020.\n[263] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2016, pp. 770–778.\n[264] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo, and L. Shao,\n“Pvt v2: Improved baselines with pyramid vision transformer,” Computational\nVisual Media, vol. 8, no. 3, pp. 415–424, 2022.\n[265] W. Yu, M. Luo, P. Zhou, C. Si, Y . Zhou, X. Wang, J. Feng, and S. Yan,\n“Metaformer is actually what you need for vision,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition , 2022, pp.\n10 819–10 829.\n[266] B. Yin, F. Corradi, and S. M. Boht ´e, “Accurate and efficient time-domain\nclassification with adaptive spiking recurrent neural networks,” Nature Machine\nIntelligence, vol. 3, no. 10, pp. 905–913, 2021.\n[267] S. Herculano-Houzel, “The remarkable, yet not extraordinary, human brain as\na scaled-up primate brain and its associated cost,” Proceedings of the National\nAcademy of Sciences , vol. 109, no. supplement 1, pp. 10 661–10 668, 2012.\n[268] S. Zheng, L. Qian, P. Li, C. He, X. Qin, and X. Li, “An introductory review of\nspiking neural network and artificial neural network: from biological intelligence\nto artificial intelligence,” arXiv preprint arXiv:2204.07519 , 2022.\n[269] M. Pfeiffer and T. Pfeil, “Deep learning with spiking neurons: Opportunities and\nchallenges,” Frontiers in neuroscience, vol. 12, p. 774, 2018.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n35\n[270] W. Penfield and E. Boldrey, “Somatic motor and sensory representation in the\ncerebral cortex of man as studied by electrical stimulation,” Brain, vol. 60, no. 4,\npp. 389–443, 1937.\n[271] Z. Yao, H. Liu, F. Xie, S. Fischer, R. S. Adkins, A. I. Aldridge, S. A. Ament,\nA. Bartlett, M. M. Behrens, K. Van den Berge et al. , “A transcriptomic and\nepigenomic cell atlas of the mouse primary motor cortex,” Nature, vol. 598, no.\n7879, pp. 103–110, 2021.\n[272] J. Berg, S. A. Sorensen, J. T. Ting, J. A. Miller, T. Chartrand, A. Buchin, T. E.\nBakken, A. Budzillo, N. Dee, S.-L. Ding et al. , “Human neocortical expansion\ninvolves glutamatergic neuron diversification,” Nature, vol. 598, no. 7879, pp.\n151–158, 2021.\n[273] J. C. Pang, K. M. Aquino, M. Oldehinkel, P. A. Robinson, B. D. Fulcher,\nM. Breakspear, and A. Fornito, “Geometric constraints on human brain function,”\nNature, pp. 1–9, 2023.\n[274] M. Yao, G. Zhao, H. Zhang, Y . Hu, L. Deng, Y . Tian, B. Xu, and G. Li, “Attention\nspiking neural networks,” IEEE transactions on pattern analysis and machine\nintelligence, 2023.\n[275] E. Genc ¸, C. Fraenz, C. Schl ¨uter, P. Friedrich, R. Hossiep, M. C. V oelkle,\nJ. M. Ling, O. G ¨unt¨urk¨un, and R. E. Jung, “Diffusion markers of dendritic\ndensity and arborization in gray matter predict differences in intelligence,” Nature\ncommunications, vol. 9, no. 1, pp. 1–11, 2018.\n[276] B. A. Olshausen and D. J. Field, “Sparse coding with an overcomplete basis set:\nA strategy employed by v1?” Vision research, vol. 37, no. 23, pp. 3311–3325,\n1997.\n[277] P. R. Roelfsema, “Attentionvoluntary control of brain cells,” Science, vol. 332,\nno. 6037, pp. 1512–1513, 2011.\n[278] D. E. L. Lockhofen and C. Mulert, “Neurochemistry of visual attention,” Frontiers\nin neuroscience, vol. 15, p. 643597, 2021.\n[279] A. Thiele and M. A. Bellgrove, “Neuromodulation of attention,” Neuron, vol. 97,\nno. 4, pp. 769–785, 2018.\n[280] A. Finkelstein, D. Derdikman, A. Rubin, J. N. Foerster, L. Las, and N. Ulanovsky,\n“Three-dimensional head-direction coding in the bat brain,” Nature, vol. 517, no.\n7533, pp. 159–164, 2015.\n[281] L. Kunz, A. Brandt, P. C. Reinacher, B. P. Staresina, E. T. Reifenstein, C. T.\nWeidemann, N. A. Herweg, A. Patel, M. Tsitsiklis, R. Kempter et al., “A neural\ncode for egocentric spatial maps in the human medial temporal lobe,” Neuron,\nvol. 109, no. 17, pp. 2781–2796, 2021.\n[282] R. Child, S. Gray, A. Radford, and I. Sutskever, “Generating long sequences with\nsparse transformers,” arXiv preprint arXiv:1904.10509 , 2019.\n[283] Satoshi and Ikeda, “Functional recovery on stroke, stroke model and rehabili-\ntation(progress in regenerative medicine),” Japanese Journal of Psychosomatic\nMedicine, vol. 53, no. 8, pp. 742–747, 2013.\n[284] Y . Wu, R. Zhao, J. Zhu, F. Chen, M. Xu, G. Li, S. Song, L. Deng, G. Wang,\nH. Zheng et al. , “Brain-inspired global-local learning incorporated with neuro-\nmorphic computing,” Nature Communications, vol. 13, no. 1, p. 65, 2022.\n[285] W. Gerstner and W. M. Kistler, “Mathematical formulations of hebbian learning,”\nBiological cybernetics, vol. 87, no. 5, pp. 404–415, 2002.\n[286] P. J. Werbos, The roots of backpropagation: from ordered derivatives to neural\nnetworks and political forecasting . John Wiley & Sons, 1994, vol. 1.\n[287] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, “Learning representations by\nback-propagating errors,” nature, vol. 323, no. 6088, pp. 533–536, 1986.\n[288] D. E. Rumelhart and D. Zipser, “Feature discovery by competitive learning,”\nCognitive science, vol. 9, no. 1, pp. 75–112, 1985.\n[289] H. Zhou, “Activation learning by local competitions,” arXiv preprint\narXiv:2209.13400, 2022.\n[290] S. A. Josselyn and S. Tonegawa, “Memory engrams: Recalling the past and\nimagining the future,” Science, vol. 367, no. 6473, p. eaaw4325, 2020.\n[291] R. Douglas Fields, “The enigma of working memory: Changing views,” The\nNeuroscientist, vol. 28, no. 5, pp. 420–424, 2022.\n[292] P. S. Goldman-Rakic, “Cellular basis of working memory,” Neuron, vol. 14, no. 3,\npp. 477–485, 1995.\n[293] G. Mongillo, O. Barak, and M. Tsodyks, “Synaptic theory of working memory,”\nScience, vol. 319, no. 5869, pp. 1543–1546, 2008.\n[294] S. M. Courtney, L. G. Ungerleider, K. Keil, and J. V . Haxby, “Transient and\nsustained activity in a distributed neural system for human working memory,”\nNature, vol. 386, no. 6625, pp. 608–611, 1997.\n[295] J. J. Foster, D. W. Sutterer, J. T. Serences, E. K. V ogel, and E. Awh, “The\ntopography of alpha-band activity tracks the content of spatial working memory,”\nJournal of neurophysiology, vol. 115, no. 1, pp. 168–177, 2016.\n[296] E.-L. Yap, N. L. Pettit, C. P. Davis, M. A. Nagy, D. A. Harmin, E. Golden,\nO. Dagliyan, C. Lin, S. Rudolph, N. Sharma et al. , “Bidirectional perisomatic\ninhibitory plasticity of a fos neuronal network,” Nature, vol. 590, no. 7844, pp.\n115–121, 2021.\n[297] R. L. Davis and Y . Zhong, “The biology of forgettinga perspective,” Neuron,\nvol. 95, no. 3, pp. 490–503, 2017.\n[298] R. Rideauxa, K. R. Storrsc, G. Maielloc, and A. E. Welchmanb, “How multisen-\nsory neurons solve causal inference,” PNAS, vol. 118, no. 32, p. e2106235118,\n2021.\n[299] J. F. Torres, D. Hadjout, A. Sebaa, F. Mart ´ınez- ´Alvarez, and A. Troncoso, “Deep\nlearning for time series forecasting: a survey,” Big Data, vol. 9, no. 1, pp. 3–21,\n2021.\n[300] C. Helwe, C. Clavel, and F. Suchanek, “Reasoning with transformer-based\nmodels: Deep learning, but shallow reasoning,” in International Conference on\nAutomated Knowledge Base Construction (AKBC) , 2021.\n[301] W. S. McCulloch and W. Pitts, “A logical calculus of the ideas immanent in\nnervous activity,” The bulletin of mathematical biophysics , vol. 5, pp. 115–133,\n1943.\n[302] W. Maass, “Networks of spiking neurons: the third generation of neural network\nmodels,” Neural networks, vol. 10, no. 9, pp. 1659–1671, 1997.\n[303] S. Sabour, N. Frosst, and G. E. Hinton, “Dynamic routing between capsules,”\nAdvances in neural information processing systems , vol. 30, 2017.\n[304] Y . Tang, K. Han, J. Guo, C. Xu, Y . Li, and Y . Wang, “An image patch is a\nwave: Phase-aware vision mlp,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , 06 2022, pp. 10 925–10 934.\n[305] C. Li, L. Du, Y . Li, and J. Song, “A novel sar target recognition method combining\nelectromagnetic scattering information and gcn,” IEEE Geoscience and Remote\nSensing Letters, vol. 19, pp. 1–5, 2022.\n[306] I. Sutskever. (2023) An observation on generalization. [Online]. Available:\nhttps://simons.berkeley.edu/talks/ilya-sutskever-openai-2023-08-14\n[307] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever,\n“Generative pretraining from pixels,” in International conference on machine\nlearning. PMLR, 2020, pp. 1691–1703.\n[308] Z. Jin, J. Liu, Z. Lyu, S. Poff, M. Sachan, R. Mihalcea, M. Diab, and\nB. Sch ¨olkopf, “Can large language models infer causation from correlation?”\narXiv preprint arXiv:2306.05836 , 2023.\n[309] Y . Qin, S. Hu, Y . Lin, W. Chen, N. Ding, G. Cui, Z. Zeng, Y . Huang,\nC. Xiao, C. Han et al., “Tool learning with foundation models,” arXiv preprint\narXiv:2304.08354, 2023.\n[310] M. Glymour, J. Pearl, and N. P. Jewell, Causal inference in statistics: A primer .\nJohn Wiley & Sons, 2016.\n[311] W. Wang, N. Yang, Y . Zhang, F. Wang, T. Cao, and P. Eklund, “A review of road\nextraction from remote sensing images,” Journal of traffic and transportation\nengineering (english edition) , vol. 3, no. 3, pp. 271–282, 2016.\n[312] Q. Wu, C. Yang, W. Zhao, Y . He, D. Wipf, and J. Yan, “DIFFormer:\nScalable (graph) transformers induced by energy constrained diffusion,” in The\nEleventh International Conference on Learning Representations , 2023. [Online].\nAvailable: https://openreview.net/forum?id=j6zUzrapY3L\n[313] C. Deng, T. Zhang, Z. He, Q. Chen, Y . Shi, L. Zhou, L. Fu, W. Zhang, X. Wang,\nC. Zhou et al., “Learning a foundation language model for geoscience knowledge\nunderstanding and utilization,” arXiv preprint arXiv:2306.05064 , 2023.\n[314] S. Pan, L. Luo, Y . Wang, C. Chen, J. Wang, and X. Wu, “Unifying large language\nmodels and knowledge graphs: A roadmap,” arXiv preprint arXiv:2306.08302 ,\n2023.\n[315] X. Wang, G. Chen, G. Qian, P. Gao, X.-Y . Wei, Y . Wang, Y . Tian, and W. Gao,\n“Large-scale multi-modal pre-trained models: A comprehensive survey,”Machine\nIntelligence Research, pp. 1–36, 2023.\n[316] N. Spyrison, D. Cook, and K. Marriott, “A study on a user-controlled ra-\ndial tour for variable importance in high-dimensional data,” arXiv preprint\narXiv:2301.00077, 2022.\n[317] K. Lv, Y . Yang, T. Liu, Q. Gao, Q. Guo, and X. Qiu, “Full parameter\nfine-tuning for large language models with limited resources,” arXiv preprint\narXiv:2306.09782, 2023.\n[318] Y . Xu, B. Du, and L. Zhang, “Assessing the threat of adversarial examples on deep\nneural networks for remote sensing scene classification: Attacks and defenses,”\nIEEE Transactions on Geoscience and Remote Sensing , vol. 59, no. 2, pp. 1604–\n1617, 2020.\n[319] B. Peng, B. Peng, J. Zhou, J. Xie, and L. Liu, “Scattering model guided\nadversarial examples for sar target recognition: Attack and defense,” IEEE\nTransactions on Geoscience and Remote Sensing , vol. 60, pp. 1–17, 2022.\nLicheng Jiao (Fellow, IEEE) received the\nB.S.degree from Shanghai Jiaotong University,\nShanghai, China, in 1982 and the M.S. and PhD\ndegree from Xian Jiaotong University, Xian, China,\nin 1984 and 1990, respectively.\nSince 1992, he has been a distinguished professor\nwith the school of Electronic Engineering, Xidian\nUniversity, Xian, where he is currently the Director\nof Key Laboratory of Intelligent Perception and\nImage Understanding of the Ministry of Education\nof China. He has been a foreign member of the\nacademia European and the Russian academy of natural sciences. His research\ninterests include machine learning, deep learnig, natural computation, remote\nsensing, image processing, and intelligent information processing.\nProf. Jiao is the Chairman of the Awards and Recognition Committee, the\nVice Board Chairperson of the Chinese Association of Artificial Intelligence,\nthe fellow of IEEE/IET/CAAI/CIE/CCF/CAA, a Councilor of the Chinese\nInstitute of Electronics, a committee member of the Chinese Committee of\nNeural Networks, and an expert of the Academic Degrees Committee of the\nState Council.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n36\nZhongjian Huang (Student Member, IEEE) re-\nceived the B.S. degree in intelligent science and\ntechnology from Xidian University, Xian, China in\n2018. He is currently pursuing the Ph.D. degree\nin computer science and technology from Xidian\nUniversity, Xi’an China. He is currently a member of\nKey Laboratory of Intelligent Perception and Image\nUnderstanding of Ministry of Education, Interna-\ntional Research Center for Intelligent Perception and\nComputation, and Joint International Research Lab-\noratory of Intelligent Perception and Computation,\nXidian University, Xi’an, China. His current research interests include video\ntracking and satellite videos analysis.\nXiaoqiang Lu (Student Member, IEEE) received\nthe B.S. degree in information countermeasure tech-\nnique from Xidian University, Xian, China, in 2020,\nwhere he is currently pursuing the Ph.D. degree with\nthe Key Laboratory of Intelligent Perception and\nImage Understanding of the Ministry of Education,\nSchool of Artificial Intelligence.\nHis research interests include machine learning,\ndeep learning, object detection, and semantic seg-\nmentation.\nXu Liu (Member, IEEE) received the B.S. degrees\nin Mathematics and applied mathematics from North\nUniversity of China, Taiyuan, China in 2013. He\nreceived the Ph.D. degrees from Xidian University,\nXian, China, in 2019. He is currently associate pro-\nfessor of Huashan elite and postdoctoral researcher\nof Key Laboratory of Intelligent Perception and Im-\nage Understanding of Ministry of Education, School\nof Artificial Intelligence, Xidian University, Xi’an,\nChina. He is the chair of IEEE Xidian university\nstudent branch(2015-2019). His current research in-\nterests include machine learning and image processing.\nYuting Yang (Student Member, IEEE) received the\nB.S. degree in electronic information science and\ntechnology from Northwest University, Xian, China,\nin 2018. She is majored in computer science and\ntechnology as a Ph.D. candidate of Xidian Univer-\nsity, Xian, China.\nShe is currently a member of Key Laboratory\nof Intelligent Perception and Image Understanding\nof Ministry of Education, International Research\nCenter for Intelligent Perception and Computation,\nand Joint International Research Laboratory of In-\ntelligent Perception and Computation, Xidian University, Xi’an, China. Her\nresearch interests include computer vision, the interpretability of deep learning\nand multiscale geometric analysis.\nJiaxuan Zhao (Student Member, IEEE) received\nthe B.S. degree in materials science and engineering\nfrom Xidian University, Xian, China, in 2019. She\nis currently pursuing the Ph.D. degree with the Key\nLaboratory of Intelligent Perception and Image Un-\nderstanding of Ministry of Education, School of Ar-\ntificial Intelligence Xidian University, Xian, China.\nHer research interests include multimodal fusion,\nevolutionary computing, and image understanding.\nJinyue Zhang Jinyue Zhang received the B.S. de-\ngree in intelligent science and technology from Xid-\nian University, Xian, China in 2018. She is currently\npursuing the PhD degree in computer science and\ntechnology from Xidian University, Xian China. Her\ncurrent research interests include video tracking and\nsatellite videos analysis.\nBiao Hou (Member, IEEE) received the B.S. and\nM.S. degrees in mathematics from Northwest Uni-\nversity, Xian, China, in 1996 and 1999, respectively,\nand the Ph.D. degree in circuits and systems from\nXidian University, Xian, in 2003. Since 2003, he\nhas been with the Key Laboratory of Intelligent\nPerception and Image Understanding of the Min-\nistry of Education, Xidian University, where he is\ncurrently a Professor. His research interests include\ncompressive sensing and synthetic aperture radar\nimage interpretation.\nShuyuan Yang (Senior Member, IEEE) received\nthe B.A. degree in electrical engineering and the\nM.S. and Ph.D. degrees in circuit and system from\nXidian University, Xian, China, in 2000, 2003, and\n2005, respectively. She has been a Professor with the\nSchool of Artificial Intelligence, Xidian University.\nHer research interests include machine learning and\nmultiscale geometric analysis.\nFang Liu (Senior Member, IEEE) received the B.S.\ndegree in computer science and technology from\nXi’an Jiaotong University, Xi’an, China, in 1984 and\nthe M.S. degree in computer science and technology\nfrom Xidian University, Xi’an, in 1995.\nShe is currently a Professor with the School of\nComputer Science, Xidian University. Her research\ninterests include signal and image processing, syn-\nthetic aperture radar image processing, multiscale\ngeometry analysis, learning theory and algorithms,\noptimization problems, and data mining.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n37\nWenping Ma (Senior Member, IEEE) received the\nB.S. degree in computer science and technology and\nthe Ph.D. degree in pattern recognition and intelli-\ngent systems from Xidian University, Xi’an, China,\nin 2003 and 2008, respectively. She is currently an\nAssociate Professor with the School of Artificial\nIntelligence, Xidian University. Her research inter-\nests include natural computing and intelligent image\nprocessing. Dr. Ma is a member of CIE.\nLingling Li (Senior Member, IEEE) received the\nB.S. and Ph.D. degrees from Xidian University,\nXian, China, in 2011 and 2017, respectively. From\n2013 to 2014, she was an Exchange Ph.D. Student\nwith the Intelligent Systems Group, Department of\nComputer Science and Artificial Intelligence, Uni-\nversity of the Basque Country UPV/EHU, Leioa,\nSpain. She is currently an Associate Professor with\nthe School of Artificial Intelligence, Xidian Univer-\nsity. Her research interests include quantum evolu-\ntionary optimization, and deep learning.\nXiangrong Zhang (Senior Member, IEEE) received\nthe B.S. and M.S. degrees in computer application\ntechnology from the School of Computer Science,\nXidian University, Xian, China, in 1999 and 2003,\nrespectively, and the Ph.D. degree in pattern recog-\nnition and intelligent system from the School of\nElectronic Engineering, Xidian University, in 2006.\nShe is currently a Professor with the Key Laboratory\nof Intelligent Perception and Image Understanding\nof the Ministry of Education, Xidian University.\nFrom January 2015 to March 2016, she was a\nVisiting Scientist with the Computer Science and Artificial Intelligence\nLaboratory, Massachusetts Institute of Technology. Her research interests\ninclude pattern recognition, machine learning, and remote sensing image\nanalysis and understanding.\nPuhua Chen (Senior Member, IEEE) received the\nB.S. degree in environmental engineering from the\nUniversity of Electronic Science and Technology\nof China, Chengdu, China, in 2009, and the Ph.D.\ndegree in circuit and system from Xidian University,\nXian, China, in 2016. She is currently a Lecturer\nwith the School of Artificial Intelligence, Xidian\nUniversity. Her research interests include machine\nlearning, pattern recognition, and remote sensing\nimage interpretation.\nZhixi Feng (Member, IEEE) received the B.A.\ndegree in automation from Lanzhou University of\nTechnology, in 2012, and the Ph.D. degree in intelli-\ngent information processing from Xidian University,\nin 2018. He is currently an Associate Professor in\nartificial intelligence with Xidian University. His re-\nsearch interests include machine learning and remote\nsensing information processing.\nXu Tang (Senior Member, IEEE) received the B.Sc.,\nM.Sc., and Ph.D. degrees in electronic circuit and\nsystem from Xidian University, Xian, China, in\n2007, 2010, and 2017, respectively. From 2015 to\n2016, he was a Joint Ph.D. Student along with Prof.\nW. J. Emery with the University of Colorado at\nBoulder, Boulder, CO, USA. He is currently an As-\nsociate Professor with the Key Laboratory of Intelli-\ngent Perception and Image Understanding, Ministry\nof Education, Xidian University. His research in-\nterests include remote sensing image content-based\nretrieval and reranking, hyperspectral image processing, remote sensing scene\nclassification, object detection, etc.\nYuwei Guo (Senior Member, IEEE) was born in\nShaanxi, China, in March 1988. She is currently\npursuing the M.S. and Ph.D. degrees in circuit and\nsystem with Xidian University, Xian, China. She is\nalso an Associate Professor with the Key Laboratory\nof Intelligent Perception and Image Understanding\nof Ministry of Education of China, Xidian Univer-\nsity. Her research interests include rough set theory,\ndata mining, and image processing.\nDou Quan (Member, IEEE) received the B.S. degree\nin 2015 and the Ph.D. degree in 2021, from Xidian\nUniversity, Xian, China. From 2019 to 2020 she was\na Joint Ph.D. along with Prof. Jocelyn Chanussot\nat the Research center of Inria Grenoble-Rhone-\nAlpes, France. She is currently a lecturer with the\nKey Laboratory of Intelligent Perception and Image\nUnderstanding of Ministry of Education of China,\nXidian University.\nHer research interests include machine learning,\ndeep learning and metric learning, image matching,\nimage registration, and image classification.\nShuang Wang (Senior Member, IEEE) received\nthe B.S., M.S., and Ph.D. degrees in circuits and\nsystems from Xidian University, Xian, China, in\n2000, 2003, and 2007, respectively. She is currently\na Professor with the Key Laboratory of Intelligent\nPerception and Image Understanding of Ministry of\nEducation of China, Xidian University. Her research\ninterests include sparse representation, image pro-\ncessing, synthetic aperture radar (SAR) automatic\ntarget recognition, remote sensing image captioning,\nand polarimetric SAR data analysis.\nWeibin Li is currently professor with Key Lab-\noratory of Intelligent Perception and Image Un-\nderstanding of Ministry of Education of China at\nXidian University, Xian, China. He received his\nPh.D. Degree from Xidian University, Xian, China,\nin 2004. His research interests are in the area of\nSpatio-temporal intelligence. His research interest\nincludes GNSS navigation system, Remote sensing\nimage rocessing, Industrial Internet and industrial\nintelligence.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\n38\nJing Bai (Senior Member, IEEE) received the B.S.\ndegree in electronic and information engineering\nfrom Zhengzhou University, Zhengzhou, China, in\n2004, and the Ph.D. degree in pattern recogni-\ntion and intelligent systems from Xidian University,\nXian, China, in 2009. She is currently a Profes-\nsor with Xidian University. Her research interests\ninclude image processing, machine learning, and\nintelligent information processing.\nYangyang Li (Senior Member, IEEE) received the\nB.S. and M.S. degrees in computer science and tech-\nnology and the Ph.D. degree in pattern recognition\nand intelligent system from Xidian University, Xian,\nChina, in 2001, 2004, and 2007, respectively. She is\ncurrently a Professor with the School of Artificial In-\ntelligence, Xidian University. Her research interests\ninclude quantum-inspired evolutionary computation,\nartificial immune systems, and deep learning.\nRonghua Shang (Senior Member, IEEE) received\nthe B.S. degree in information and computation\nscience and the Ph.D. degree in pattern recogni-\ntion and intelligent systems from Xidian University,\nXian, China, in 2003 and 2008, respectively. She\nis currently a Professor with Xidian University. Her\nresearch interests include evolutionary computation,\nimage processing, and data mining.\nJie Feng (Senior Member, IEEE) received the B.S.\ndegree from Changan University, Xian, China, in\n2008, and the Ph.D. degree from Xidian Univer-\nsity, Xian, in 2014. She is currently an Associate\nProfessor with the Key Laboratory of Intelligent\nPerception and Image Understanding of Ministry of\nEducation, Xidian University. Her interests include\nremote sensing image processing, deep learning, and\nmachine learning.\nThis article has been accepted for publication in IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/JSTARS.2023.3316302\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7571582198143005
    },
    {
      "name": "Foundation (evidence)",
      "score": 0.5332218408584595
    },
    {
      "name": "Field (mathematics)",
      "score": 0.5154346227645874
    },
    {
      "name": "Representation (politics)",
      "score": 0.5107033848762512
    },
    {
      "name": "Task (project management)",
      "score": 0.48283156752586365
    },
    {
      "name": "Open research",
      "score": 0.46510300040245056
    },
    {
      "name": "Data science",
      "score": 0.4617680609226227
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.45088258385658264
    },
    {
      "name": "Perception",
      "score": 0.4455488622188568
    },
    {
      "name": "Feature learning",
      "score": 0.4109017252922058
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3928194046020508
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3786778450012207
    },
    {
      "name": "Remote sensing",
      "score": 0.36370187997817993
    },
    {
      "name": "Systems engineering",
      "score": 0.2150023877620697
    },
    {
      "name": "Engineering",
      "score": 0.1312379240989685
    },
    {
      "name": "World Wide Web",
      "score": 0.10351622104644775
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "History",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Pure mathematics",
      "score": 0.0
    }
  ]
}