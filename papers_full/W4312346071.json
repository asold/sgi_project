{
  "title": "Measuring Media Bias via Masked Language Modeling",
  "url": "https://openalex.org/W4312346071",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5101610717",
      "name": "Xiaobo Guo",
      "affiliations": [
        "Dartmouth Hospital",
        "Dartmouth College"
      ]
    },
    {
      "id": "https://openalex.org/A5043402333",
      "name": "Weicheng Ma",
      "affiliations": [
        "Dartmouth Hospital",
        "Dartmouth College"
      ]
    },
    {
      "id": "https://openalex.org/A5035399743",
      "name": "Soroush Vosoughi",
      "affiliations": [
        "Dartmouth Hospital",
        "Dartmouth College"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1119807432",
    "https://openalex.org/W3093726604",
    "https://openalex.org/W1602390003",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2149636593",
    "https://openalex.org/W2970879410",
    "https://openalex.org/W3093937844",
    "https://openalex.org/W2134625031",
    "https://openalex.org/W6650830430",
    "https://openalex.org/W6763669421",
    "https://openalex.org/W2991203612",
    "https://openalex.org/W6683619503",
    "https://openalex.org/W6727369040",
    "https://openalex.org/W2810359804",
    "https://openalex.org/W3046684368",
    "https://openalex.org/W3127474531",
    "https://openalex.org/W3130090378",
    "https://openalex.org/W2217796615",
    "https://openalex.org/W2523934214",
    "https://openalex.org/W4287179421",
    "https://openalex.org/W3100201982",
    "https://openalex.org/W2891649320",
    "https://openalex.org/W2949555168",
    "https://openalex.org/W3126073260"
  ],
  "abstract": "Bias in news reporting can lead to tribalism and division on important issues. Scalable and reliable measurement of such biases is an important first step in addressing them. In this work, based on the intuition that media bias is captured by the tone and word choices in articles, we propose a framework for modeling the relative bias of media outlets through masked token prediction via large-scale pretrained masked language models fine-tuned on articles form news outlets. Through experiments on five diverse and politically polarized topics we show that our framework can capture media bias towards these topics with high reliability. Additionally, our experiments show that our framework is general, in that language models fine-tuned on one topic can be applied to other topics with little drop in performance.",
  "full_text": "Measuring Media Bias via Masked Language Modeling\nXiaobo Guo, Weicheng Ma, Soroush Vosoughi\nDepartment of Computer Science, Dartmouth College Hanover, New Hampshire\n{xiaobo.guo.gr, weicheng.ma.gr, soroush.vosoughi}@dartmouth.edu\nAbstract\nBias in news reporting can lead to tribalism and division on\nimportant issues. Scalable and reliable measurement of such\nbiases is an important first step in addressing them. In this\nwork, based on the intuition that media bias is captured by\nthe tone and word choices in articles, we propose a frame-\nwork for modeling the relative bias of media outlets through\nmasked token prediction via large-scale pretrained masked\nlanguage models fine-tuned on articles form news outlets.\nThrough experiments on five diverse and politically polar-\nized topics we show that our framework can capture media\nbias towards these topics with high reliability. Additionally,\nour experiments show that our framework is general, in that\nlanguage models fine-tuned on one topic can be applied to\nother topics with little drop in performance.\nIntroduction\nPartisanship in news outlets has been shown to heavily sway\npublic opinion (Eveland Jr and Shah 2003; Heimlich 2011;\nMorales 2011). Such bias operates via two mechanisms: se-\nlective coverage of issues, known as issue filtering, and pre-\nsentation of issues, known as issue framing (Budak, Goel,\nand Rao 2016). Compared to issue filtering, bias via issue\nframing is more nuanced and difficult to identify. Further-\nmore, framing bias falls under different categories: bias rela-\ntive to “neutral” opinion, median preference of citizens, and\nother media outlets (Puglisi and Snyder Jr 2015). Among\nthese categories, bias relative to other media outlets is one\nthat can be objectively quantified. Thus, in this paper, we\nfocus on measuring the relative bias of media outlets with\nrespect to issue framing.\nMost prior work examining media bias use qualitative\nmethods, however these methods are subjective, expensive\nand cannot be easily reproduced. Others have used quan-\ntitative methods, specifically audience-based and content-\nbased approaches. The audience-based method is premised\non the assumption that readers prefer news outlets closer to\ntheir ideology (Mullainathan and Shleifer 2005). Although\nthis method has produced an ideological ordering of out-\nlets (Zhou, Resnick, and Mei 2011; Ribeiro et al. 2018;\nGentzkow and Shapiro 2011; Mitchell 2016; Spinde et al.\nCopyright © 2022, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n2021a), it relies on readership surveys which introduces its\nown subjective bias, is costly and not easily scalable.\nContent-based methods on the other hand measure me-\ndia bias directly from the published content. These meth-\nods usually utilize hand-crafted features such as the tone\nor the choice of phrases when referring to the same events\nor entities (e.g., undocumented immigrant vs. illegal alien).\nThese methods either rely fully on human annotation (Lim,\nJatowt, and Yoshikawa 2018; Golez and Karapandza 2020;\nF¨arber et al. 2020; Gentzkow and Shapiro 2010; Budak,\nGoel, and Rao 2016; Fan et al. 2019; Spinde et al. 2021c)–\nwhich makes them not scalable and prone to human bias– or\nutilize (semi-) automatic methods by representing the prob-\nlem as a standard supervised learning task (Spinde, Ham-\nborg, and Gipp 2020a,b; Spinde et al. 2021b; Chen et al.\n2020), relying on topic-specific hand-crafted features.\nTo address these challenges, we utilize language models\n(LMs) to capture the opinions “hidden” in news articles.\nSpecifically, we capture the tone and word preference of me-\ndia outlets by fine-tuning BERT models (Devlin et al. 2019)\non corpora from different outlets and topics collected from\nMedia Cloud, a publicly available news API (see Section\nfor more detail).\nThrough the masked language modeling (MLM) objec-\ntive, we then use these fine-tuned models to predict the\nchoice of word by individual media outlets for different\ncontexts, such as different issues or events. In other words,\nprompting these models with sentences such as “the great-\nest threat to the immigration system is\n”, they will output\nthe word most likely to complete the sentence given the as-\nsociations learned from their fine-tuning texts. Comparing\noutputs from models fine-tuned on different outlets can thus\nilluminate key attitudinal differences between. The prompts\nused in this paper come directly from the development set of\nour corpora.\nTo validate the relative bias between outlets as reported\nby our method, we compare the results to three independent\nnews bias datasets: two based on surveys of the US populace\nconducted by Pew Research (Jurkowitz et al. 2020), and an-\nother based on expert curation of bias in media outlets from\nthe website Allsides.com. Our proposed framework for esti-\nmating relative bias between media outlets through prompt-\ning fine-tuned masked language models is scalable in that it\neliminates the need for data annotation or hand-crafted fea-\nProceedings of the Sixteenth International AAAI Conference on Web and Social Media (ICWSM2022)\n1404\ntures.\nDatasets\nWe collect articles from Media Cloud 1 to construct our\ndataset. Specifically, we sample news under five diverse top-\nics: “Climate Change”, “Corporate Tax”, “Drug Policy”,\n“Gay Marriage” and “The Affordable Care Act”, from 10\nnews outlets in the US: Breitbart News Network, CBS News,\nCNN, Fox News, HuffPost, New York Times, NPR, USA\nToday, Wall Street Journal, and Washington Post.\nWe divide the dataset into train and development (dev)\nsets with a 90/10 split. Due to the length limit of BERT, we\nbreak the news articles down to paragraphs with no more\nthan 256 words and label the paragraphs with their respec-\ntive media outlets. The paragraphs are split at sentence level\nto ensure the completeness of sentences.\nThe dataset contains 107,121 instances for “Climate\nChange”, 104,316 instances for “Corporate Tax”, 104,188\ninstances for “Drug Policy”, 109,486 instances for “Gay\nMarriage”, and 106,287 instances for “The Affordable Care\nAct”. Each instance is one paragraph. In Table 1, we list the\nstatistics of the train and dev sets for each topic and media\noutlet.\nAdditionally, we construct three ground truth datasets for\nevaluation purposes. Two of the datasets are based on sur-\nvey data from Pew Research (Jurkowitz et al. 2020). These\ndatasets are annotated on a 6-point scale based on the survey\nresults regarding how much the respondents (1) trust a par-\nticular new outlet, and (2) use the outlet for political news.\nWe call these datasets SoA-t and SoA-s, capturing the share\nof Americans who trust and receives news from each out-\nlet respectively. The third dataset (called MBR) is based on\nexpert curation of media bias by the wesbite Allsides.com\n2. This dataset labels each media outlet with five political\nleanings, from left to right, based on editorial review, third-\nparty analysis, independent review, surveys and community\nfeedback.\nMethodology\nAs shown in Figure 1, our framework is comprised of three\nsteps: (1) We first fine-tune an LM for each media outlet\n(2) We then leverage the fine-tuned LMs to create attitudinal\nrepresentations of the media outlets via prompt-based mask\ntoken prediction (3) We utilize the generated representations\nto measure the relative bias of the outlets.\nLanguage Model Fine-tuning\nWe fine-tune bert-base-cased (henceforth referred to as\nBERT) using the MLM task as done in its pre-training stage.\nWhile we leverage BERT in our experiments because of its\npopularity and high performance, any MLM-based language\nmodel can potentially be utilized.\n1https://mediacloud.org/\n2https://www.allsides.com/media-bias/media-bias-ratings\nMedia Attitude Representation\nTo generate the attitudinal representation of each media out-\nlet, we use prompt-based mask token prediction in which we\nmask one word in a selected prompt and apply the fine-tuned\nLMs to predict it.\nInspired by work on authorship attribution (e.g., (Coyotl-\nMorales et al. 2006)), we pick the prompts and the word to\nbe masked using the following approach: (1) We first create\na list of bigrams that appear in the dev set of all ten media\noutlets (219 total) (2) For each instance in dev set contain-\ning these bigrams (11,500 total) we generate two masked\nprompts (per bigram), one where the mask is applied to the\nword preceding the bigram and one with the mask applied\nto the word following the bigram. We experiment with other\nmethods to pick the prompts and the tokens to be masked,\nand the method described here was the best performing in\nour small-scale experiments.\nWe then use the fine-tuned LMs for each outlet to predict\nthe masked token in our prompts. We retrieve the the top-10\ncandidate words with the highest probability. The attitude of\neach media outlet with respect to the masked prompt is then\nrepresented as a vector of the probability of these words.\nNote that the vectors for the outlets all have the same length\nand correspond to the same words, i.e., the union of all the\ntop-10 candidate words from the outlets (if a word was not\nin the top-10 candidate of an outlet, its probability is set to\n0). This allows for cross media comparisons. In Figure 2,\nwe show an example of how the media attitudinal represen-\ntations are generated for two media outlets using the top-3\ncandidate words (for simplicity).\nAt the end, for each topic t we have nt vectors for each\nof the 10 media outlets, where nt is the number of masked\nprompts for each topic. The set of these vectors represent\nthe attitude of each media outlet with respect to different\ntopics. In Table 2, we show the number of bigrams, instances\nin the dev set, and masked prompts (note that each instance\nproduces one or more masked prompts) for each topic for the\nbigram outer method. We observe that other than the “The\nAffordable Care Act” all topics have a similar data size.\nMeasuring Relative Bias\nWe measure the relative bias of the media outlets for specific\ntopics by first calculating the distance between each pair of\noutlets. This is done by calculating the mean of the euclidean\ndistance across all aligned (by prompt) attitudinal represen-\ntation vectors for the specified topics for the outlet pair.\nNext, for each outlet we create a ranking of other media\noutlets based on their distance. Note that these ranking do\nnot have to be symmetric, in that if outlet A has outlet B as\nthe closest outlet, it doesn’t necessarily follow that B will\nhave A as its closest outlet. These rankings corresponding to\nrelative attitudinal bias of each outlet with respect to other\noutlets. Outlets that are ranked closer to each other should\nhave more similar attitudes towards the specified topics and\nvice versa.\n1405\nClimate Change Corporate Tax Drug Policy Gay Marriage The Affordable Care Act\nTrain Dev Train Dev Train Dev Train Dev Train Dev\nBreitbart 8,267 807 8,357 756 7,765 934 7,663 1,016 6,665 732\nCBS 11,515 1,680 9,002 1,026 9,313 1,273 12,320 2,268 7,968 799\nCNN 12,587 1,414 12,982 1,529 13,158 1,565 13,949 1,841 16,030 1,805\nFox 8,487 450 8,526 1,284 9,445 602 6,473 695 7,272 735\nHuffPost 10,272 1,102 11,044 1,087 9,780 1,117 9,385 1,054 10,138 1,132\nNPR 15,260 1,509 14,730 1,997 14,934 1,503 17,285 2,036 13,057 1,281\nNYTimes 3,113 322 2,678 297 3,077 271 3,803 289 4,936 452\nUSA Today 12,288 1,842 13,464 1,476 12,432 1,718 12,436 1,340 12,261 1,208\nWallstreet 4,914 447 4,040 433 4,403 469 6,469 635 3,003 407\nWashington 9,747 1,098 8,788 816 9,545 911 7,666 861 14,459 1,947\nTable 1: The statistics of the train and dev sets for each topic and media outlet.“NYTimes” : “New York Times”, “Wallstreet” :\n“The Wall Street Journal”, “Washington” : “Washington Post”\nFigure 1: An overview of our framework. For simplicity, only two sample media outlets are shown here.\nClimate Cor Drug SSM Care Total\n# of bigrams 39 37 37 31 75 219\n# of instances 1,856 2,067 1,992 1,514 4,071 11,500\n# of masked prompts 4,299 5,152 5,161 3,550 13,160 31,322\nTable 2: The statistics of the number of bigrams, instances in the dev set, and masked prompts for the BO prompt and mask\nselection method. “Climate” : “Climate Change”, “Cor” : “Corporate Tax”, “Drug” : “Drug policy”, “SSM” : “Gay Marriage”,\n“Care” : “The Affordable Care Act”.\nFigure 2: An example generation of media attitudinal representations for two outlets. The bigram is “health insurance” (shown\nin orange), and we mask the tokens “market” and “individual” separately to generate two masked instance. For simplicity, we\ngenerate the representations using the top-3 predicted words.\nIn-domain Performance Out-of-domain Performance\nClimate Cor Drug SSM Care Climate Cor Drug SSM Care\nSoA-t 0.24 0.35 0.31 0.31 0.35 0.33 (0.08) 0.25 (0.07) 0.29 (0.07) 0.24 (0.05) 0.25 (0.01)\nSoA-s 0.28 0.28 0.31 0.30 0.33 0.32 (0.08) 0.21 (0.05) 0.28 (0.07) 0.23 (0.06) 0.26 (0.02)\nMBR 0.27 0.26 0.44 0.30 0.41 0.37 (0.07) 0.21 (0.03) 0.35 (0.09) 0.26 (0.05) 0.30 (0.05)\nTable 3: Agreement between media similarity ranking using our framework and ground truth datasets. Agreement is calculated\nusing Kendall’sτ (higher magnitude is better). “Climate” : “Climate Change”, “Cor” : “Corporate Tax”, “Drug” : “Drug policy”,\n“SSM” : “Gay Marriage”, “Care” : “The Affordable Care Act”. The out-of-domain results show the average performance of\nmodels trained on 4 different topics and tested on the remaining topic. The standard deviations are shown in the brackets.\n1406\nExperiments and Analysis\nTo validate our method for estimating the relative attitudinal\nbias of news outlets, we run two sets of experiments: the first\nexperiment trains and evaluates the relative bias of outlets on\nthe same topic. while the other trains the model on one topic\nand evaluates it on other topics to assess the generalizabil-\nity of the trained model. The two settings are respectively\nreferred to as in-domain and out-of-domain media bias esti-\nmation tasks.\nWe compare the relative attitudinal bias of the outlets pre-\ndicted by our framework with ground truth labels in the\nSoA-s, SoA-t, and MBR datasets. Specifically, we mea-\nsure the relative bias of the outlets using these ground truth\ndatasets using a similar procedure as described in Section\n: for each outlet we calculate the distance to the other out-\nlets (based on their ground truth political ideology labels)\nand create a ranked list. For SoA-s and SoA-t we use cosine\ndistance (since these datasets provide a distribution over ide-\nologies for each outlet), while for MBR we look at the abso-\nlute distance between the outlets (since this dataset provides\na single ideological score for each outlet).\nWe then calculate the similarity between the predicted and\nground truth similarity rankings via Kendall rank correlation\ncoefficients (Kendall’s τ). As a Kendall’s τ is calculated for\neach media outlet; we use the mean of all theτ’s as the eval-\nuation score. Note that τ ranges from −1 to 1 correspond-\ning to perfect misalignment and alignment of ranked lists\nrespectively. A τ of 0 corresponds to completely random\nalignment.\nIn Table 3, we show the in-domain and out-of-domain\nrelative media bias estimation evaluations using the three\nground truth datasets. We observe that the agreements as\nreported by τ are all significantly above random chance\n(p < 0.05 for all), with the ‘The Affordable Care Act”\ndataset achieving the highest agreement in in-domain exper-\niments (likely due to the extremely polarized conversation\naround this topic). As expected, the out-of-domain experi-\nments on average have lower performance, but the drop is\nminimal and all results are still significantly above random\nchance. This results show that our framework can capture\nboth the topic-specific and general attitudinal bias of outlets.\nWe also compare the performance of our models against\ntwo classic baseline methods used in prior media bias esti-\nmation experiments, namely models based on Latent Dirich-\nlet allocation (LDA) and Term Frequency-Inverse Document\nFrequency (TF-IDF) features. Both baseline models are fine-\ntuned on the train set for deciding the hyper-parameters and\nencode the instances in the dev set3. The encodings are then\nused to calculate cosine distances between pairs of media\noutlets in a manner similar to what was done in Section .\nSame as before, for each outlet we use the distance to other\noutlets to create a ranked list and use Kendall’sτ to measure\nagreement with the ground truth rankings.\nWe show the mean τ across all topics in Table 4. Our\nmodel outperforms the baselines on all three ground truth\ndatasets, with the differences all being statistically signifi-\ncant (p <0.05).\n3We treat each instance as one document.\nSoA-t SoA-s MBR\nLDA 0.15(0.08) 0.16(0.09) 0.22(0.11)\nTF-IDF 0.23(0.07) 0.26(0.08) 0.29(0.07)\nOurs 0.31(0.05) 0.30(0.02) 0.34(0.08)\nTable 4: Comparison of agreement between ground truth\ndatasets and the baselines and our method. Agreement is cal-\nculated using Kendall’sτ. Results are averaged across 5 top-\nics and the standard deviations are reported in the brackets.\np <0.05 for all values.\nConclusion and Future Work\nWe presented a framework for capturing attitudinal bias of\nmedia outlets using the MLM objective through masked\nprompting of fine-tuned large-scale pretrained language\nmodels (BERT specifically). Our experiments show that the\npredicted relative bias of outlets match 3 different ground\ntruth datasets. Future work can explore other strategies for\nprompt and masked token selection.\nCode & Data Availability: We will make the code and\ndata publicly available here. 4\nEthics Statement\nThis paper proposes a novel method for studying the rela-\ntive bias of news outlets with respect to different issues us-\ning masked language modeling. Though the issue of bias\nin news is controversial, this paper only studies the rela-\ntive bias between outlets. We do not make any judgements\nas to whether certain outlets are biased, our method only\nreports the relative difference between the outlets. All text\nused in this paper come from public news outlets and were\ncollected using the publicly available API of Media Cloud.\nAs such, the data does not contains any private information.\nSince we use mainstream news outlets for our data collec-\ntion we believe there is less risk of overtly unethical infor-\nmation (though we cannot be sure given the current sociopo-\nlitical climate). Given the relatively large size of our dataset,\nwe cannot manually examine all articles, however, the pub-\nlicly released dataset will warn users of the possibility of\nthe dataset containing unethical information and will allows\nusers to flag unethical articles in our dataset.\nFinally, as we use pre-trained language models in our pa-\nper, we must be aware of the inherent bias in such models\n(based on their pre-training data) and again err on the side\nof caution when utilizing such models for consequential ap-\nplications.\nReferences\nBudak, C.; Goel, S.; and Rao, J. M. 2016. Fair and balanced?\nQuantifying media bias through crowdsourced content anal-\nysis. Public Opinion Quarterly, 80(S1): 250–271.\nChen, W.-F.; Al Khatib, K.; Wachsmuth, H.; and Stein, B.\n2020. Analyzing Political Bias and Unfairness in News Ar-\nticles at Different Levels of Granularity. In Proceedings of\n4https://github.com/guoxiaobo96/media-bias\n1407\nthe Fourth Workshop on Natural Language Processing and\nComputational Social Science, 149–154.\nCoyotl-Morales, R. M.; Villase ˜nor-Pineda, L.; Montes-y\nG´omez, M.; and Rosso, P. 2006. Authorship attribution us-\ning word sequences. In Iberoamerican Congress on Pattern\nRecognition, 844–853. Springer.\nDevlin, J.; Chang, M.-W.; Lee, K.; and Toutanova, K. 2019.\nBERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding. In Proceedings of the 2019 Con-\nference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), 4171–4186.\nEveland Jr, W. P.; and Shah, D. V . 2003. The impact of in-\ndividual and interpersonal factors on perceived news media\nbias. Political psychology, 24(1): 101–117.\nFan, L.; White, M.; Sharma, E.; Su, R.; Choubey, P. K.;\nHuang, R.; and Wang, L. 2019. In Plain Sight: Media Bias\nThrough the Lens of Factual Reporting. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-IJCNLP) ,\n6343–6349.\nF¨arber, M.; Burkard, V .; Jatowt, A.; and Lim, S. 2020. A\nMultidimensional Dataset Based on Crowdsourcing for An-\nalyzing and Detecting News Bias. InProceedings of the 29th\nACM International Conference on Information & Knowl-\nedge Management, 3007–3014.\nGentzkow, M.; and Shapiro, J. M. 2010. What drives media\nslant? Evidence from US daily newspapers. Econometrica,\n78(1): 35–71.\nGentzkow, M.; and Shapiro, J. M. 2011. Ideological segrega-\ntion online and offline.The Quarterly Journal of Economics,\n126(4): 1799–1839.\nGolez, B.; and Karapandza, R. 2020. Home-country media\nslant and equity prices. Available at SSRN 3396726.\nHeimlich, R. 2011. Press widely criticized, but trusted more\nthan other information sources. Pew Research Center.\nJurkowitz, M.; Mitchel, A.; Shearer, E.; and Walker, M.\n2020. U.S. Media Polarization and the 2020 Election: A\nNation Divided. Pew Research Center.\nLim, S.; Jatowt, A.; and Yoshikawa, M. 2018. Understand-\ning Characteristics of Biased Sentences in News Articles. In\nCIKM workshops.\nMitchell, A. 2016. Key findings on the traits and habits of\nthe modern news consumer. Pew Research Center.\nMorales, L. 2011. Majority in US continues to distrust the\nmedia, perceive bias. Gallop Politics.\nMullainathan, S.; and Shleifer, A. 2005. The market for\nnews. American economic review, 95(4): 1031–1053.\nPuglisi, R.; and Snyder Jr, J. M. 2015. Empirical studies of\nmedia bias. In Handbook of media economics, volume 1,\n647–667. Elsevier.\nRibeiro, F. N.; Henrique, L.; Benevenuto, F.; Chakraborty,\nA.; Kulshrestha, J.; Babaei, M.; and Gummadi, K. P. 2018.\nMedia bias monitor: Quantifying biases of social media\nnews outlets at large-scale. In Twelfth international AAAI\nconference on web and social media.\nSpinde, T.; Hamborg, F.; and Gipp, B. 2020a. An integrated\napproach to detect media bias in German news articles. In\nProceedings of the ACM/IEEE joint conference on digital\nlibraries in 2020, 505–506.\nSpinde, T.; Hamborg, F.; and Gipp, B. 2020b. Media bias\nin german news articles: A combined approach. In Joint\nEuropean Conference on Machine Learning and Knowledge\nDiscovery in Databases, 581–590. Springer.\nSpinde, T.; Hamborg, F.; Kreuter, C.; Gipp, B.; Gaissmaier,\nW.; and Giese, H. 2021a. How Can the Perception of Media\nBias in News Articles Be Objectively Measured? Best Prac-\ntices and Recommendations Using User Studies. In Pro-\nceedings of the ACM/IEEE Joint Conference on Digital Li-\nbraries (JCDL).\nSpinde, T.; Rudnitckaia, L.; Mitrovi´c, J.; Hamborg, F.; Gran-\nitzer, M.; Gipp, B.; and Donnay, K. 2021b. Automated iden-\ntification of bias inducing words in news articles using lin-\nguistic and context-oriented features. Information Process-\ning & Management, 58(3): 102505.\nSpinde, T.; Rudnitckaia, L.; Sinha, K.; Hamborg, F.; Gipp,\nB.; and Donnay, K. 2021c. MBIC–A Media Bias Annotation\nDataset Including Annotator Characteristics. arXiv preprint\narXiv:2105.11910.\nZhou, D. X.; Resnick, P.; and Mei, Q. 2011. Classifying the\npolitical leaning of news articles and users from user votes.\nIn Fifth International AAAI Conference on Weblogs and So-\ncial Media.\n1408",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7337561845779419
    },
    {
      "name": "Language model",
      "score": 0.6675592064857483
    },
    {
      "name": "Media bias",
      "score": 0.6348066329956055
    },
    {
      "name": "Scalability",
      "score": 0.5434922575950623
    },
    {
      "name": "Natural language processing",
      "score": 0.4124550521373749
    },
    {
      "name": "Artificial intelligence",
      "score": 0.37833279371261597
    },
    {
      "name": "Data science",
      "score": 0.3549860119819641
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    }
  ]
}