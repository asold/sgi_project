{
  "title": "CalibNet: Geometrically Supervised Extrinsic Calibration using 3D Spatial Transformer Networks",
  "url": "https://openalex.org/W2908599206",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A1961219115",
      "name": "Ganesh Iyer",
      "affiliations": [
        "International Institute of Information Technology, Hyderabad"
      ]
    },
    {
      "id": "https://openalex.org/A2798970843",
      "name": "R, Karnik Ram",
      "affiliations": [
        "International Institute of Information Technology, Hyderabad"
      ]
    },
    {
      "id": "https://openalex.org/A2097602063",
      "name": "J. Krishna Murthy",
      "affiliations": [
        "Université de Montréal"
      ]
    },
    {
      "id": "https://openalex.org/A2167589289",
      "name": "K. Madhava Krishna",
      "affiliations": [
        "International Institute of Information Technology, Hyderabad"
      ]
    },
    {
      "id": "https://openalex.org/A1961219115",
      "name": "Ganesh Iyer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2798970843",
      "name": "R, Karnik Ram",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097602063",
      "name": "J. Krishna Murthy",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2167589289",
      "name": "K. Madhava Krishna",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2200124539",
    "https://openalex.org/W6764386301",
    "https://openalex.org/W6763422710",
    "https://openalex.org/W6739778489",
    "https://openalex.org/W2963270286",
    "https://openalex.org/W2609883120",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6638667902",
    "https://openalex.org/W2560722161",
    "https://openalex.org/W6688774849",
    "https://openalex.org/W2295149141",
    "https://openalex.org/W6684191040",
    "https://openalex.org/W1575087183",
    "https://openalex.org/W2964186846",
    "https://openalex.org/W6722453979",
    "https://openalex.org/W1990398405",
    "https://openalex.org/W2770841999",
    "https://openalex.org/W6697925102",
    "https://openalex.org/W2963896595",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2115579991",
    "https://openalex.org/W2217105717",
    "https://openalex.org/W2246782745",
    "https://openalex.org/W2435623039",
    "https://openalex.org/W2783032472",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2612827728",
    "https://openalex.org/W2963121255",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2950974964",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W2951336016",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2620406343",
    "https://openalex.org/W2734619798",
    "https://openalex.org/W2950762923",
    "https://openalex.org/W2489710028",
    "https://openalex.org/W2560609797",
    "https://openalex.org/W2964314455"
  ],
  "abstract": "3D LiDARs and 2D cameras are increasingly being used alongside each other in sensor rigs for perception tasks. Before these sensors can be used to gather meaningful data, however, their extrinsics (and intrinsics) need to be accurately calibrated, as the performance of the sensor rig is extremely sensitive to these calibration parameters. A vast majority of existing calibration techniques require significant amounts of data and/or calibration targets and human effort, severely impacting their applicability in large-scale production systems. We address this gap with CalibNet: a self-supervised deep network capable of automatically estimating the 6-DoF rigid body transformation between a 3D LiDAR and a 2D camera in real-time. CalibNet alleviates the need for calibration targets, thereby resulting in significant savings in calibration efforts. During training, the network only takes as input a LiDAR point cloud, the corresponding monocular image, and the camera calibration matrix K. At train time, we do not impose direct supervision (i.e., we do not directly regress to the calibration parameters, for example). Instead, we train the network to predict calibration parameters that maximize the geometric and photometric consistency of the input images and point clouds. CalibNet learns to iteratively solve the underlying geometric problem and accurately predicts extrinsic calibration parameters for a wide range of mis-calibrations, without requiring retraining or domain adaptation. The project page is hosted at https://epiception.github.io/CalibNet",
  "full_text": "CalibNet: Geometrically Supervised Extrinsic Calibration using 3D\nSpatial Transformer Networks\nGanesh Iyer, R. Karnik Ram, J. Krishna Murthy, and K. Madhava Krishna\nFig. 1: CalibNet estimates the extrinsic calibration parameters between a 3D LiDAR and a 2D camera. It takes as input an\nRGB image (a) from a calibrated camera, a raw LiDAR point cloud (b), and outputs a 6-DoF rigid-body transformation\nT that best aligns the two inputs. (c) shows the colorized point cloud output for a mis-calibrated setup, and (d) shows the\noutput after calibration using our network. As shown, using the mis-calibrated point cloud to recover a colorized 3D map\nof the world results in an incoherent reconstruction. Notice how the 3D structures highlighted in (c) using red rectangles\nfail to project to their 2D counterparts. However, using the extrinsic calibration parameters predicted by CalibNet produces\nmore consistent and accurate reconstructions (d), even for large initial mis-calibrations.\nc⃝2018 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media,\nincluding reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers\nor lists, or reuse of any copyrighted component of this work in other works. DOI: 10.1109/IROS.2018.8593693\nAbstract— 3D LiDARs and 2D cameras are increasingly being\nused alongside each other in sensor rigs for perception tasks.\nBefore these sensors can be used to gather meaningful data,\nhowever, their extrinsics (and intrinsics) need to be accurately\ncalibrated, as the performance of the sensor rig is extremely\nsensitive to these calibration parameters. A vast majority of\nexisting calibration techniques require signiﬁcant amounts of\ndata and/or calibration targets and human effort, severely\nimpacting their applicability in large-scale production systems.\nWe address this gap with CalibNet: a geometrically supervised\ndeep network capable of automatically estimating the 6-DoF\nrigid body transformation between a 3D LiDAR and a 2D\ncamera in real-time. CalibNet alleviates the need for calibration\ntargets, thereby resulting in signiﬁcant savings in calibration\nefforts. During training, the network only takes as input a\nLiDAR point cloud, the corresponding monocular image, and\nthe camera calibration matrix K. At train time, we do not\nGanesh Iyer, R. Karnik Ram, and K. Madhava Krishna are\nwith the Robotics Research Center at the International Institute\nof Information Technology, Hyderabad, India. J. Krishna Murthy\nis with Mila, Universit ´e de Montr ´eal, Quebec, Canada. Authors’\nemail: giyer2309@gmail.com, karnikram@gmail.com,\nmkrishna@iiit.ac.in\nimpose direct supervision (i.e., we do not directly regress to\nthe calibration parameters, for example). Instead, we train the\nnetwork to predict calibration parameters that maximize the\ngeometric and photometric consistency of the input images and\npoint clouds. CalibNet learns to iteratively solve the underlying\ngeometric problem and accurately predicts extrinsic calibration\nparameters for a wide range of mis-calibrations, without\nrequiring retraining or domain adaptation. The project page is\nhosted at https://epiception.github.io/CalibNet\nI. INTRODUCTION\nPerception of visual cues and objects in the environment\nis an important aspect of autonomous robot navigation.\nA successful perception system relies on various on-board\nsensors. A growing number of sensors of various modalities\nare being used in robots. An autonomous car, for instance,\nuses a 3D LiDAR in combination with 2D cameras as the\ndense color information of the latter complements the sparse\ndistance information of the former. With their increasing use,\ncalibration techniques to estimate accurate extrinsic param-\neters of the sensors are becoming increasingly important. In\nthe same example of an autonomous car, without accurate ex-\narXiv:1803.08181v2  [cs.RO]  4 Aug 2019\ntrinsic parameters, the laser distance measurements cannot be\naccurately projected onto the camera images, and thus color\npixels in the images cannot be accurately associated with\ndistance information. Over the past several years, a number\nof calibration techniques have been proposed, speciﬁcally\nfor the LiDAR-camera calibration problem [1], [2], [3], [4],\n[5] Yet, the vast majority of these techniques depend on\nspeciﬁc calibration targets such as checkerboards, and require\nsigniﬁcant amounts of manual effort [1], [2]. In addition, they\nare unable to correct for any deviation due to environmental\nchanges or vibrations during live operation, often rendering\nthe robots inoperable. Thus there is an imperative need for\nautomatic and online calibration techniques which can signif-\nicantly extend the ﬂexibility and adaptability of these robots.\nThere have been some previously published techniques in\nthis area [3], [4], [5] but most of these techniques still depend\non accurate initialization for the calibration parameters [3],\n[4], or require signﬁcant amounts of ego-motion data [5].\nOur work tries to tackle this problem of LiDAR-camera\ncalibration i.e. estimating the 6-DoF rigid body transfor-\nmation between a 3D LiDAR and a 2D camera, without\nany assumptions about the existence of any speciﬁc features\nor landmarks in the scenes, without any initial estimate\nfor the extrinsic parameters, and in real-time. We leverage\nthe recent success of deep neural networks in classical\ncomputer vision tasks such as visual recognition [6], in our\nsolution. Our network only takes as input a LiDAR point\ncloud, the corresponding monocular image, and the camera\ncalibration matrix K, and is able to accurately estimate\nthe extrinsic parameters for a wide range of possible mis-\ncalibrations about and along any of the axes respectively.\nFurther, our training method employs geometric supervision\nby directly reducing the dense photometric error and dense\npoint cloud distance error measures, to regress the correct\nextrinsic calibration parameters, thus eliminating the need of\nan existing calibrated sensor setup to collect training data. To\nthe best of our knowledge, we believe this is the ﬁrst deep\nneural network to estimate extrinsic calibration parameters\nin this manner.\nWe showcase the following contributions with CalibNet:\n1) CalibNet is the ﬁrst geometrically supervised deep\nlearning approach to tackle the problem of multi-sensor\nself-calibration in LiDAR-camera rigs.\n2) We use a novel architecture based on 3D Spatial\nTransformers [7] that learns to solve the underlying\nphysical problem, by using geometric and photometric\nconsistency to guide the learning process. This makes\nCalibNet agnostic to non-invariant parameters such as\ndependence on camera intrinsics, and the approach\neffortlessly generalizes to data collected from multiple\nsensor rigs, without requiring retraining or ﬁne-tuning.\n3) In the larger scheme of things, the paper presents a\nfundamentally different approach for model-to-image\nregistration to leverage the best of both worlds (viz.\ndeep representation learning and geometric computer\nvision) in building an accurate and viable system for\nthe task.\nThe paper is organized as follows: We review related\nLiDAR-camera calibration techniques in the next section. We\ndetail our technique and the network architecture in Sec. III.\nIn Sec. IV we experimentally evaluate our technique, and\nconclude the paper in Sec. V .\nII. RELATED WORK\nThe LiDAR-camera extrinsic calibration problem has been\nwell-studied for several years. Existing approaches for cross-\nsensor calibration broadly fall into two categories, viz. target-\nbased and target-less techniques. The taxonomy can be\nextended further based on whether they work automatically\nor require manually labeled correspondences. Usually, there\nis a trade-off between the effort required in experimental\nsetup and the amount of data needed for effective calibration.\nExpensive calibration setups have the advantage that they\nwork with very little data. Inexpensive calibration setups\ncompensate for the lack of sophistication by gathering a\nlarger volume of data.\nGeiger et al. [2] proposed an automatic system for accurate\ncamera-camera and LiDAR-camera calibration using just a\nsingle image per sensor. They require a speciﬁc calibration\nsetup with multiple checkerboard targets. Simpler techniques\nthat can solve for the LiDAR-camera extrinsic parameters,\nusing simple easy-to-make targets and lesser number of\ncorrespondences have been recently proposed in [8],[9].\nAlthough accurate, these techniques are slow, labor intensive,\nand require careful tuning of several hyperparameters.\nLevinson and Thrun proposed one of the ﬁrst target-less\ntechniques in [3]. Their underlying assumption, is that depth\ndiscontinuities in laser data should project onto edges in\nimages for an accurate extrinsic calibration. The two feature\nclasses are combined and a grid search is used to estimate\nthe optimal extrinsic parameters. Pandey et al. proposed a\nvery similar method in [4] where they try to maximize the\nmutual information between the LiDAR’s intensity of return\nand the intensity of the corresponding points in the camera’s\nimage. Although the assumptions hold even in the presence\nof measurement noise, the formulated cost functions are only\nlocally convex and hence rely on a good initialization for the\noptimization to converge.\nAnother class of target-less techniques exists where in-\ndependent motion estimates from the sensors are used and\nmatched to obtain the extrinsic calibration parameters, as\nshown in [5]. They do not rely on any initialization for the\nextrinsics, or require any overlapping ﬁelds of view for the\nsensors. However, they still require large amounts of data\nand good motion estimates for accurate results which limits\ntheir applicability to ofﬂine scenarios.\nRecently, deep neural networks have shown tremendous\nsuccess in classical computer vision tasks such as visual\nrecognition [6], localization [10], and correspondence esti-\nmation [11]. Deep networks have also shown their effec-\ntiveness in dealing with unordered point clouds for tasks\nsuch as 3D Object Detection and Segmentation [12], [13].\nYet, surprisingly, only a few deep learning based approaches\nhave been applied to the calibration problem. The ﬁrst deep\nFig. 3: Network architecture\nconvolutional neural network for LiDAR-camera calibration\nwas proposed in [14] by Schneider et al. Using a Network-in-\nNetwork based pre-trained supervised network, they aim to\nregress the transformation parameters that accurately aligns\nthe LiDAR point cloud to the image, by training the network\nwith large amounts of annotated “decalibration” data. While\nfeasible and real-time, the training process is agnostic of\nthe underlying geometry of the problem. Schneider et al.\n[14] regress to the calibration parameters, conditioned on the\ninput image. Since it doesn’t take geometry into account, it\nhas to be retrained each time sensor intrinsics change. In\ncontrast, our method leverages the recent success of self-\nsupervised networks [15], [16] and attempts to solve the\nproblem by attempting to reduce the dense photometric error\nand dense point cloud distance error between the misaligned\nand target depth maps. While we use transformed depth\nmaps as targets, such a map could be found by any stereo\nreconstruction method and be used for training. Further,\nsince our model only requires camera intrinsics for applying\nthe spatial transformations during training, any intrinsically\ncalibrated camera system can be used for extrinsic calibration\nusing our architecture.\nIII. OUR APPROACH\nIn this section we present the theory behind our approach,\nthe network architecture, training methodology, and loss\nfunctions.\nA. Network Architecture\nInput Preprocessing:The network takes as input an RGB\nimage, the corresponding mis-calibrated LiDAR point cloud,\nand the camera calibration matrix K.\nThe point cloud is ﬁrst converted into a sparse depth map\nas a pre-processing step. This is done by projecting the\nLiDAR point cloud onto the image plane. Since the initial\nmis-calibration is inaccurate, projecting the mis-calibrated\npoints to the image plane results in a sparse depth map that\nis (grossly) inconsistent with the image (see Fig. 1(c)). We\nnormalize both the RGB input image and the sparse depth\nmap to the range of ±1. The sparse depth maps are then\nmax-pooled to create semi-dense depth maps using a 5 x 5\nmax-pooling window. The resulting semi-dense depth maps\nlook similar to the inputs shown in Fig.4(b).\nArchitectural Details: The network primarily consists\nof 2 asymmetric branches, each performing a series of\nconvolutions (see Fig. 3. For the RGB branch we use the\nconvolutional layers of a pre-trained ResNet-18 network\n[17]. For the depth branch, we use a similar architecture\nas for the RGB stream, but with half the number of ﬁlters\nat each stage. Like in [14], this architecture has several\nadvantages for feature extraction. The use of pre-trained\nweights for the RGB input prevents learning the relevant\nfeatures from scratch. However, since the parameters of the\ndepth stream are learned from scratch, the ﬁlters for the depth\nstream are reduced at each stage. The outputs of the two\nbranches are then concatenated along the channel dimension\nand passed through a series of additional fully convolutional\nlayers, for global feature aggregation. BatchNorm [18] is\nused throughout the network, after every convolutional block.\nWe decouple the output streams for rotations and translations\nto capture differences in modalities that might exist between\nrotations and translations. The output of the network is a 1\nx 6 vector ξ = (v,ω) ∈se(3) where v is the translational\nvelocity vector, and ω is the rotational velocity vector.\nSO(3) layer: While translations are directly predicted at\nthe output of the network, we need to convert the output\nrotation vector in so(3) to its corresponding rotation matrix.\nAn element ω∈so(3) can be converted to SO(3) by using\nthe Exponential Map. The exponential map is simply the\nmatrix exponential over a linear combination of the group\ngenerators. Given ω= (ω1,ω2,ω3)T, the exponential map is\ndeﬁned as follows.\nexp: so(3) →SO(3); ˆω↦→eˆω\nHere, ˆω is the skew-symmetric matrix from of ω (also\nreferred to as the hat operator), and eˆω computed using\nTaylor series expansion for the matrix exponential function.\nA closed form solution to the above expression yields the\nwell-known Rodrigues formula.\nR= eˆω = I+ ˆω\n∥ω∥sin ∥ω∥+ ˆω2\n∥ω∥2 (1 −cos(∥ω∥))\nThis gives us the rotation in Rin SO(3). Combining with\ntranslation predicted by the network gives us a 3D rigid body\ntransformation T ∈SE(3) deﬁned as\nT =\n( R t\n0 1\n)\nwhere R∈SO(3) and t∈R3\n3D Spatial Transformer Layer: Once we convert the\ncalibration parameters predicted by the network to a rigid-\nbody transform in T ∈ SE(3), we use a 3D Spatial\nTransformer Layer that transforms the input depth map by\nthe predicted transformation T. We extend the original 3D\nSpatial Transformer layer [7] in this work to handle sparse\nor semi-dense input depth maps.\nKnowing the camera intrinsics (fx,fy,cx,cy) allows\nback-projection of the max-pooled depth image to a sparse\n(or in some cases, semi-dense) point cloud using the mapping\nπ−1 : R2 →R3.\nπ−1(x,y,Z ) =\n((x−cx\nfx\n)\n,\n(y−cy\nfy\n)\n,Z\n)\nWe now transform the obtained point cloud by the extrinsic\ncalibration T predicted by the network, and then project the\ntransformed point cloud back to the image plane using the\ncamera intrinsics.\n(x\ny\n)\n= π\n\nR\n\n\nX\nY\nZ\n\n+ t\n\n (1)\nHere, Rand tare the rotation and translation components of\nt, and π is the perspective projection operator (note that the\ncamera intrinsics are subsumed into π, for sake of brevity).\nThis operation is carried out in a differentiable man-\nner using the 3D Grid Generator. To obtain an image of\nsimilar dimensions to that of the input image, we scale\n(fx,fy,cx,cy) accordingly.\nLoss Functions:The use of dense methods for registration\nis even more requisite in the case of extrinsic calibration.\nWe use two types of loss terms during training:\n1. Photometric Loss: After transforming the depth map\nby the predicted T, we check for the dense pixel-wise\nerror (since each pixel is encoded with the depth intensity)\nbetween the predicted depth map and the correct depth map.\nThe error term is deﬁned as,\nLphoto = 1\n2\nN∑\n1\n(\nDgt −KTπ−1[Dmiscalib]\n)2\n(2)\nwhere Dgt is the target depth map and Dmiscalib is the\ninitial mis-calibrated depth map. While we use max-pooled\ndepth maps during training, note that this could be further\ngeneralized by using a stereo pair to estimate depth maps,\nand use the same for training as well.\n2. Point Cloud Distance Loss:The 3D Spatial Transformer\nLayer allows the transformation of a point cloud after\nbackprojection. At this stage, we utilize the unregistered\ntransformed and target point clouds and try to minimize the\n3D-3D point distances between them in metric scale. Note\nthat we don’t know correspondences since we are working\nwith unordered point sets, making it a more difﬁcult\ntask. Therefore, we considered various distance metrics\nthat would be an accurate measure of the error between\nunregistered sets in the world-coordinate frame. Following\nthe recent success of [19] for point cloud generation, we\nconsidered the following distance measures.\nChamfer Distance: The Chamfer Distance between two\npoint clouds S1,S2 ⊆R3, is deﬁned as the sum of squared\ndistances of the nearest points between the two clouds.\ndCD(S1,S2) =\n∑\nx∈S1\nmin\ny∈S2\n∥x−y∥2\n2 +\n∑\ny∈S2\nmin\nx∈S1\n∥x−y∥2\n2\n(3)\nEarth Mover’s Distance:The Earth Mover’s Distance is\noriginally a measure of dissimilarity between two multi-\ndimensional distributions. Since the distance between the\nindividual points of the clouds can be calculated, we try to\nsolve for a metric that signiﬁes the overall distance measure\nbetween the clouds. Speciﬁcally, given two point clouds\nS1,S2 ⊆R3, the optimization essentially tries to solve the\nassignment problem for each point. In particular, if φ is a\nmapping between the two point sets, then we minimize the\ndistance as follows,\ndEMD(S1,S2) = min\nφ:S1→S2\n∑\nx∈S1\n∥x−φ(x)∥2 (4)\nwhere φ: S1 →S2 is a bijection.\nCentroid ICP Distance:Similar to the loss term used for\nIterative Closest Point based alignment, we try to directly\nminimize the distance between the target point clouds and\na point cloud transformed by the predicted transformation.\nWhile various methods are used to establish an initial corre-\nspondence, such as the use of Kd-trees or minimum distance\nbetween points, we directly minimize the total distance\nbetween point cloud cluster centres. Each centre is computed\nas a centroid of intermittent points.\ndicp(S1,S2) =1\n2\nN∑\n1\ni∑\n1\nXi\nexp −(RXi\nmiscalib + t)\n2\n(5)\nwhere Xexp is a possible cluster center of the expected\npoint cloud, and Xmiscalib is a cluster center from the\nmis-calibrated point cloud projected to the world coordinate\nframe.\nOur ﬁnal loss function consists of a weighted sum of the\nphotometric loss and point cloud distance loss,\nLfinal = αph(Lphoto) +βdist(d(S1,S2)) (6)\nB. Layer Implementation Details\nA major bottleneck when dealing with semi-dense depth\nmaps is that we cannot utilize operations that apply updates\nto all tensor locations. Since the input maps would contain\nmultiple pixel locations with zero depth as intensity, there is a\nneed to implement layers that apply mathematical operations\nonly at sparse tensor locations. It is also a prerequisite that\nthese layers are differentiable in an end-to-end fashion.\nWe use the Tensorﬂow library [20] to implement the\nvarious sub-modules of our pipeline. We frequently utilize\nthe scatter nd operation, based on advantages mentioned in\n[21], since it allows for the sparse update of pixel intensities\nat various tensor locations. We use this operation in the\nBilinear Sampling Layer, such that, when interpolating pixel\nlocations we only consider sparse neighbor locations of\nwhere depth intensities are available.\nWe also contribute to an operation that prevents duplicate\nupdates at the same pixel locations in eq. 5. In order to\nprevent rewriting pixel value updates to the same index\nlocations, we use a Cantor Pairing function, and retain only\nunique index locations before updating depth intensity value.\nC. Iterative Re-alignment\nSo far we have presented a solution where an input\nmis-calibrated depth map is only transformed once before\nchecking for photometric and distance errors. While some\nearlier works employ this method externally by resupplying\nthe inputs [14], our method is capable of applying iterative\nrealignment in an end-to-end fashion within the network.\nFor this, once the network predicts an initial transforma-\ntion T, we transform the input mis-calibrated depth map\nby the predicted transformation. We now feed this newly\ntransformed depth map as input to the network, in order\nto predict a residual transform T′. The ﬁnal transformation\nis now computed after computing the product of predicted\ntransformations at each stage.\nˆT = (T)(T′)(T′′)(T′′′)... (7)\nAt each step, the error is computed against the target depth\nmap and target point cloud. When unrolled, the network\nresembles a recurrent unit similar to the work in [22], where\nthe gradient ﬂow at each iteration is against the transformed\npoint cloud, and its resultant depth map.\nIV. EXPERIMENTS AND DISCUSSION\nWe thoroughly analyze the proposed approach and present\nqualitative and quantitative results on LiDAR-camera data\nmade available as part of the popular KITTI [23] autonomous\ndriving benchmark. In this section, we detail the experi-\nmental setup, training procedure, and analyze the results\nobtained.\nA. Dataset Preparation\nWe use raw recordings from the KITTI dataset [23],\nspeciﬁcally the color image and velodyne point cloud record-\nings, to prepare our dataset. We use the 26 09 driving\nsequences for training, since they consist of a high number\nof sequences with good scene variation. To obtain training\ndata in the form of mis-calibrated depth maps, we ﬁrst apply\nrandom transformations Trandom to the calibrated point\nclouds in order to decalibrate them. To capture a wide range\nof input misalignments, we sample (vx,vy,vz,ωx,ωy,ωz)\nrandomly from a uniform distribution in the range of ±10◦\nrotation and ±0.2 m translation in any of the axes. We believe\nthis is a good range of values to correctly emulate possible\ngross calibration errors that may not be directly corrected by\ntarget-based methods or measurement devices. By projecting\nthese point clouds onto the camera plane, we obtain our input\nmis-calibrated sparse depth maps. The network takes as input\nthese mis-calibrated depth maps and a corresponding RGB\nimage.\nThe target depth maps are obtained by recalibrating the\nmisaligned depth maps. We apply the inverse of the original\ntransform used to mis-calibrate the depth map, T−1\nrandom, and\nuse this newly obtained depth map as the target depth map\nfor photometric error reduction. While we could have directly\nused the calibrated point clouds to obtain ground truth depth\nmaps during training, we noticed that for very large mis-\ncalibrations, the point cloud often deviates outside the ﬁeld-\nof-view of the camera, leading to huge variations between\na calibrated depth map and a re-calibrated depth map. By\nthis method, therefore, we ensure any there are no major\ndifferences between the predicted and ground truth maps due\nto wider transformations when calculating the photometric\nerror.\nUsing the above method, we generate a total of 24000\npairs for training and 6000 pairs for testing. To demonstrate\nthe generalization capability of our architecture even with\ndifferent camera intrinsic parameters, we also use different\ninstances from the 30 09 driving sequences during testing,\nas shown in Figure 5.\nB. Evaluation Metrics\nNote that, for the task of multi-sensor extrinsic calibra-\ntion, while the translation measure between the camera and\nLiDAR coordinate frames can be roughly estimated using\nmeasuring devices, it is very difﬁcult to even roughly mea-\nsure the estimates of the yaw, pitch, and roll angles. Stressing\non the difﬁculty of estimating the rotation components, we\nshow a strong validation in using dense photometric error\nbetween the predicted and target depth maps to ﬁnd the\nrotation matrix through our experiments.\nSince we are not training in a supervised manner, i.e.\nregressing by checking the error against the target transfor-\nmation, we observed that while photometric loss helps in\nFig. 4: Examples of calibration results for different scenes from the KITTI dataset. The ﬁrst row shows the input RGB images,\nand the second row shows the corresponding mis-calibrated LiDAR point cloud projected onto the images. The third row\nshows LiDAR point clouds projected using the network’s predicted transforms, and the last row shows the corresponding\nground truth results. The red rectangles in the second row indicate the mis-alignment, and in the third row they denote the\nproper alignment after calibration.\nFig. 5: An example of a severely mis-calibrated input. Notice\nhow the input mis-calibration causes the depth map to be\nskewed along the road. (a) is the RGB input image, and (b) is\nthe mis-calibrated depth map. (c) is the calibrated depth map\nusing our network’s estimate, and (d) is the corresponding\nground truth result. As shown, our network is able to recover\nthe calibration even in this case.\nestimating better rotation values progressively over training,\nthere is no such bound on the predicted translation values.\nThis leads to erroneous initial translation values during\ntraining. We observed that point cloud distance measures\nserve as an important bound during training to ensure that\ntranslation estimates also slowly achieve the requisite target\nvalues. While we found the original end-to-end architecture\neffective in estimating the correct rotation values during\ntesting, we found it difﬁcult to estimate translation values,\nsince residual values are hardly affected by small translation\nchanges, but highly sensitive to erroneous rotation values. To\nremedy this, we train in an iterative fashion to re-estimate\ntranslation values. We experiment with freezing the initial\nmodel, and use the rotation predictions to transform the mis-\ncalibrated point cloud. We then use only the point cloud\ndistance measures to train for the translation values. We\ntrain this model with both ground truth rotations, and our\nestimated rotation values with frozen weights. We use earth\nmover’s distance as our cloud distance metric of choice,\nsince we observed that it scales better when estimating large\ntranslation values. Since the training time would increase\ndrastically when calculating earth mover’s distance for semi-\ndense point sets, we use the sparse depth maps without\nmax-pooling to project sparse point clouds. We also further\nsparsify the clouds by ﬁnding centroids of local clusters,\nsimilar to the process mentioned for dicp. We ﬁnd 4096\nintermediate centroid locations in the predicted and ground\ntruth point clouds, and use these centroids to calculate the\nearth mover’s distance. We found this to improve training\ntime without compromising on translation accuracy loss\nduring training.\nWe evaluate overall rotation error as the geodesic distance\nover SO(3), given by,\ndg(Ri,Rj) = 1√\n2∥log(RT\ni Rj)∥F (8)\nand, absolute translation error as the overall error for trans-\nlations,\ndtr= ∥Xi −Xj∥ (9)\nwhere Ri is the predicted rotation, Rj is the ground truth\nrotation, Xi is the predicted translation, and Xj is the ground\ntruth translation.\nFig. 6: Global error statistics of our test set. Plots in red, green, and blue denote the absolute error values of CalibNet along\nthe X, Y , and Z axis respectively. (a) shows the change in absolute error for the range of initial mis-calibration in degrees\nacross our test set. (b),(c) show the change in absolute error for the range of mis-calibration in meters. (b) demonstrates\nthe error values upon training with ground truth rotations, while (c) demonstrates the error values when translation model is\ntrained on prediction from the frozen weights of the previous iteration, (d) geodesic distance error distribution (in radians)\nover the test set instance pairs, (e) absolute translation error distribution (in meters) over the test set instance pairs.\nC. Training Details\nFor training the network we use the Adam Optimizer [24],\nwith an initial learning rate of 1e−4, and momentum equal\nto 0.9. We decrease the learning rate by a factor 0.5 every\nfew epochs. We train for a total of 28 epochs. Using Earth\nMover Distance in the cost function, we set αph equal to 1.0\nand βdist equal to 0.15 and slowly increase its value to 1.75.\nTo prevent over-ﬁtting, we also apply a dropout of 0.7 at the\nfully connected layers.\nD. Results\nWe show results of our base architecture for rotation values\nand the iterative re-alignment based model for translation.\n1) Rotation estimation: Our network performs exceed-\ningly well for rotation estimation. We report a mean absolute\nerror (MAE) value for rotation angles on the test set: (Yaw:\n0.15◦, Pitch: 0.9◦, Roll: 0.18◦). Figure 6(a) further illustrates\nthe low absolute error values, against a widespread variation\nin mis-calibrations. We also show, in Figure 6(d), the per-\nformance of our network in predicting the overall rotation\nvalue as a function of the geodesic distance. Speciﬁcally, we\ndemonstrate how the geodesic distance is close to 0 for the\nbulk of the instance pairs in the test set.\n2) Translations given ground truth: We observed that a\nsingle iteration fails to correctly estimate translation, since a\nsigniﬁcant photometric error reduction can lead to correctly\nestimated rotation parameters, but the point cloud distance\nloss has to decrease to very low values to correctly estimate\ntranslation. Since the value could not be minimized in a\nsingle iteration, we decided to further ﬁne tune on translation\nvalues. During training for translation values, we rotate the\ninput depth map with the ground truth rotation values and\nuse the spatial transformer to apply the new estimate for\ntranslation. For this, we only use EMD as the error metric.\nWe report mean absolute error in translation: (X: 4.2 cm, Y:\n1.6 cm, Z: 7.22 cm). Figure 6(b), and 6(d) correspond to the\noutputs based on this training scheme. Figure 6(b) shows the\nabsolute error in translation over the mis-calibration range in\nthe test set, while 6(d) shows the overall absolute translation\nerror against a varying set, which, despite high initial mis-\ncalibrations, is bounded in the range of cms.\n3) Translations given CalibNet rotation estimates: To\ndemonstrate the capability of our geometric supervision\nmethod, we use the iterative re-alignment methodology in\nour network. After CalibNet regresses an initial estimate\nof rotation, we use the spatial transformer to apply the\nestimated rotation on the mis-calibrated depth map and\ntrain for translation using earth movers distance as the loss\nfunction. Figure 6(c) shows the overall absolute error for the\nvarious translation components. We observed that in the case\nof when mis-calibration in X-axis is higher than±0.25 m, the\nabsolute translation errors often deviate sharply. However,\nour model still works well for translation in the range of\n±0.24 m. We report mean absolute error in translations: (X:\n12.1 cm, Y: 3.49 cm, Z: 7.87 cm).\n4) Qualitative Results: Our network is able to accurately\nestimate the calibration, over a wide variety of scenes and\nwide range of initial mis-calibrations. Figures 4 and 5 show\nsome of these results. Even with very high initial errors in\nall the 6 axes, the spatial transformer successfully aligns the\nmis-calibrated depth map to the RGB frame, achieving close\nto ground truth re-alignment. Each of the columns corre-\nsponds to a particular scene chosen carefully to showcase\nthe versatility of the network. For example the ﬁrst column\nshows a scenario devoid of vehicles and less on features.\nThe second column shows a scene with several vehicles and\nwith changes in illumination in the scene. The third column\nshows a dense scene with a higher number of objects and\nlow visibility of the underlying plane. In each of the cases,\nthe network portrays its efﬁcacy. More qualitative results are\nshown in the accompanying video.\nV. CONCLUSION\nIn this paper, we presented a novel self-supervised deep\nnetwork that can be used to estimate the 6-DoF rigid body\ntransformation between a 3D LiDAR and a 2D camera, a\ncritical sensor combination that’s typically found in most\nautonomous cars for perception. Unlike existing techniques,\nwe do not rely on special targets in the scene, or any human\nintervention, thereby enabling true in-situ calibration. The\nnetwork is able to correct for mis-calibrations up to ±20◦\nin rotation and ±0.2 m in translation, with a mean absolute\nerror of 0.41◦ in rotation and 4.34 cm in translation.\nThe estimates of the network can serve as a good ini-\ntialization for other optimization techniques with locally\nconvex cost functions, which can further improve the es-\ntimates. Furthermore, the network seems to have learned the\nunderlying geometry of the problem, which sets it up for\nother interesting applications in RGB-D localization, visual\nSLAM, stereo calibration, and is also a viable step in self-\nsupervising architectures extended to non-symmetric inputs.\nIn the future, we also wish to explore other priors to solve the\nregistration problem between point clouds and RGB frames,\nsuch as the correspondence between depth maps and color\nframes, and ground-plane constraints.\nREFERENCES\n[1] R. Unnikrishnan and M. Hebert, “Fast extrinsic calibration of a laser\nrangeﬁnder to a camera,” 2005.\n[2] A. Geiger, F. Moosmann, ¨O. Car, and B. Schuster, “Automatic camera\nand range sensor calibration using a single shot,” in Robotics and\nAutomation (ICRA), 2012 IEEE International Conference on. IEEE,\n2012.\n[3] J. Levinson and S. Thrun, “Automatic online calibration of cameras\nand lasers.” in Robotics: Science and Systems, 2013.\n[4] G. Pandey, J. R. McBride, S. Savarese, and R. M. Eustice, “Automatic\ntargetless extrinsic calibration of a 3d lidar and camera by maximizing\nmutual information.” in AAAI, 2012.\n[5] Z. Taylor and J. Nieto, “Motion-based calibration of multimodal sensor\narrays,” in Robotics and Automation (ICRA), 2015 IEEE International\nConference on. IEEE, 2015.\n[6] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classiﬁcation\nwith deep convolutional neural networks,” in Advances in neural\ninformation processing systems, 2012.\n[7] A. Handa, M. Bloesch, V . P ˘atr˘aucean, S. Stent, J. McCormac, and\nA. Davison, “gvnn: Neural network library for geometric computer\nvision,” in European Conference on Computer Vision. Springer, 2016.\n[8] C. Guindel, J. Beltrn, D. Martn, and F. Garca, “Automatic extrinsic cal-\nibration for lidar-stereo vehicle sensor setups,” in IEEE International\nConference on Intelligent Transportation Systems (ITSC), 674679 .\nIEEE, 2017.\n[9] Z. Pusztai and L. Hajder, “Accurate calibration of lidar-camera systems\nusing ordinary boxes,” 2017.\n[10] A. Kendall, M. Grimes, and R. Cipolla, “Posenet: A convolutional\nnetwork for real-time 6-dof camera relocalization,” inComputer Vision\n(ICCV), 2015 IEEE International Conference on. IEEE, 2015.\n[11] C. B. Choy, J. Gwak, S. Savarese, and M. Chandraker, “Universal cor-\nrespondence network,” in Advances in Neural Information Processing\nSystems 30, 2016.\n[12] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning\non point sets for 3d classiﬁcation and segmentation,” Proc. Computer\nVision and Pattern Recognition (CVPR), IEEE, 2017.\n[13] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierar-\nchical feature learning on point sets in a metric space,” in Advances\nin Neural Information Processing Systems, 2017.\n[14] N. Schneider, F. Piewak, C. Stiller, and U. Franke, “Regnet: Multi-\nmodal sensor registration using deep neural networks,” in Intelligent\nVehicles Symposium (IV), 2017 IEEE. IEEE, 2017.\n[15] T. Zhou, M. Brown, N. Snavely, and D. G. Lowe, “Unsupervised\nlearning of depth and ego-motion from video,” in CVPR, 2017.\n[16] R. Li, S. Wang, Z. Long, and D. Gu, “Undeepvo: Monocular vi-\nsual odometry through unsupervised deep learning,” arXiv preprint\narXiv:1709.06841, 2017.\n[17] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2016.\n[18] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” in International\nconference on machine learning, 2015.\n[19] H. Fan, H. Su, and L. Guibas, “A point set generation network for\n3d object reconstruction from a single image,” in Conference on\nComputer Vision and Pattern Recognition (CVPR), 2017.\n[20] M. Abadi and A. A. et al., “TensorFlow: Large-scale machine\nlearning on heterogeneous systems,” 2015, software available from\ntensorﬂow.org. [Online]. Available: https://www.tensorﬂow.org/\n[21] M. Ren, A. Pokrovsky, B. Yang, and R. Urtasun, “Sbnet: Sparse blocks\nnetwork for fast inference,” arXiv preprint arXiv:1801.02108, 2018.\n[22] A. Newell, K. Yang, and J. Deng, “Stacked hourglass networks for\nhuman pose estimation,” in European Conference on Computer Vision.\nSpringer, 2016.\n[23] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics:\nThe kitti dataset,” The International Journal of Robotics Research,\n2013.\n[24] D. P. Kingma and J. Ba, “Adam: A method for stochastic\noptimization,” CoRR, 2014. [Online]. Available: http://arxiv.org/abs/\n1412.6980",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7645268440246582
    },
    {
      "name": "Calibration",
      "score": 0.7229254841804504
    },
    {
      "name": "Point cloud",
      "score": 0.6577924489974976
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6198201179504395
    },
    {
      "name": "Computer vision",
      "score": 0.5999166965484619
    },
    {
      "name": "Lidar",
      "score": 0.5773462057113647
    },
    {
      "name": "Intrinsics",
      "score": 0.5471512079238892
    },
    {
      "name": "Remote sensing",
      "score": 0.31298837065696716
    },
    {
      "name": "Mathematics",
      "score": 0.11714518070220947
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I64189192",
      "name": "International Institute of Information Technology, Hyderabad",
      "country": "IN"
    },
    {
      "id": "https://openalex.org/I70931966",
      "name": "Université de Montréal",
      "country": "CA"
    }
  ],
  "cited_by": 198
}