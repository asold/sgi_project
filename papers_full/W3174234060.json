{
  "title": "Common Sense Beyond English: Evaluating and Improving Multilingual Language Models for Commonsense Reasoning",
  "url": "https://openalex.org/W3174234060",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2563984569",
      "name": "Bill Yuchen Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2142831431",
      "name": "Seyeon Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2302865608",
      "name": "Qiao Xiaoyang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2898695519",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3099771192",
    "https://openalex.org/W3104163040",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2964343359",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W3086339196",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W3101449015",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W4287827771",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3045462440",
    "https://openalex.org/W3097986428",
    "https://openalex.org/W2107901333",
    "https://openalex.org/W2799146523",
    "https://openalex.org/W3102483398",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2996403597",
    "https://openalex.org/W2073302931",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3088396740",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W3102999298",
    "https://openalex.org/W3105752438"
  ],
  "abstract": "Bill Yuchen Lin, Seyeon Lee, Xiaoyang Qiao, Xiang Ren. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 1274‚Äì1287\nAugust 1‚Äì6, 2021. ¬©2021 Association for Computational Linguistics\n1274\nCommon Sense Beyond English: Evaluating and Improving\nMultilingual Language Models for Commonsense Reasoning\nBill Yuchen Lin Seyeon Lee Xiaoyang Qiao Xiang Ren\n{yuchen.lin, seyeonle, xiaoyanq, xiangren}@usc.edu\nDepartment of Computer Science and Information Sciences Institute,\nUniversity of Southern California\nAbstract\nCommonsense reasoning research has so far\nbeen limited to English. We aim to evalu-\nate and improve popular multilingual language\nmodels (ML-LMs) to help advance common-\nsense reasoning (CSR) beyond English. We\ncollect the Mickey corpus, consisting of 561k\nsentences in 11 different languages, which\ncan be used foranalyzing and improving ML-\nLMs. We propose Mickey Probe, alanguage-\nagnostic probing task for fairly evaluating the\ncommon sense of popular ML-LMs across dif-\nferent languages. In addition, we also create\ntwo new datasets, X-CSQA and X-CODAH,\nby translating their English versions to 15\nother languages, so that we can evaluate pop-\nular ML-LMs for cross-lingual commonsense\nreasoning. To improve the performance be-\nyond English, we propose a simple yet effec-\ntive method ‚Äî multilingual contrastive pre-\ntraining (MCP). It signiÔ¨Åcantly enhances sen-\ntence representations, yielding a large perfor-\nmance gain on both benchmarks (e.g., +2.7%\naccuracy for X-CSQA over XLM-RL)1.\n1 Introduction\nUnderstanding natural language relies heavily on\ncommonsense reasoning (CSR), which is the pro-\ncess of making inferences with commonsense\nknowledge. Commonsense knowledge is the set of\ngeneral facts that reÔ¨Çect our natural understanding\nof the physical world and human behavior, which\nare usually seen as an implicit background when\npeople communicate with each other using lan-\nguages. It is thus of vital importance to evalu-\nate and improve the commonsense reasoning ca-\npability of language models (LMs), towards build-\ning general natural language understanding (NLU)\nsystems (Davis and Marcus, 2015).\n1We release our code and data at the project website:\nhttps://inklab.usc.edu/XCSR/.\nBirds have [mask] .\nLAMA Probe\nWhere do adults usually use glue sticks?\nA) school B) drawer C) office\nCommonsenseQASWAG/CODAHThe chef drops the piece of shrimp in the fryer.‚Üí\nA) The chef chops the pan.B) The chef watches it sizzle.C) The chef likes fried chicken.\nEnglish LM\nMultilingual LM\nEN\nAR\nZH\nFR\nHI\nIT\nDE\nES\nJA\nPL\nNL\nPT\nUR\nVI\nRU\nSW\nwing\nC B\nFigure 1: Commonsense reasoning is well-studied with\nbenchmarks and LMs in English. Can we advance\ncommonsense reasoning beyond English?\nMany recent benchmark datasets and probing\nmethods have been proposed to evaluate ma-\nchine common sense. As shown in Figure 1,\nthe LAMA probe (Petroni et al., 2019) is for an-\nalyzing LMs‚Äô zero-shot commonsense recalling\nability; CommonsenseQA (CSQA) (Talmor et al.,\n2019) is instead a multiple-choice QA task that\nneeds Ô¨Åne-tuning; CODAH (Chen et al., 2019)\nand SWAG (Zellers et al., 2018) focus on the abil-\nity to complete the most plausible scenes. How-\never, all these works have been limited only to\nEnglish. Consequently, follow-up analysis and\nreasoning methods developed (Lin et al., 2019;\nFeng et al., 2020; Lin et al., 2020) also focus only\non English LMs like BERT (Devlin et al., 2019).\nSuch English-centric trend of commonsense rea-\nsoning studies not only limits our research scope,\nbut also tends to exacerbate English-speciÔ¨Åc bias\nthat might prevent future methods from generaliz-\ning beyond English (Ponti et al., 2020).\n1275\nIt is of pressing urgency for the community to\ndevelop NLU systems that can serveall languages\nin the world to bridge the gap between different\ncultures and eliminate language barriers (Hu et al.,\n2020), and multilingual language models (ML-\nLMs), such as XLM-R (Conneau et al., 2020),\nare among the most promising tools to achieve\nthis ambitious goal. Although ML-LMs have been\nevaluated in a few NLU tasks, e.g., XNLI (Con-\nneau et al., 2018) and XTEMRE (Hu et al., 2020),\nit is still relatively unclear how ML-LMs per-\nform in commonsense reasoning tasks, due to the\nlack of 1) dedicated methods for probing common\nsense in ML-LMs and 2) multilingual benchmark\ndatasets for commonsense reasoning.\nTo analyze how much common sense ML-\nLMs already have without any tuning, we pro-\npose MICKEY PROBE , a zero-shot probing task. It\ntasks a ML-LM to rank a set ofcontrastive as-\nsertions (i.e., declarative sentences) in the same\nlanguage by their commonsense plausibility, for\nwhich we usepseudo-likelihood (PLL) ( Salazar\net al., 2020) as a proxy. Unlike the LAMA probe,\nit can studymulti-token conceptswhich are ubiq-\nuitous in some non-English languages. In addi-\ntion, it fairly compares performance across differ-\nent languages via alanguage-invariant evaluation\nprotocol. Alongside the probing task, we also cre-\nate MickeyCorpus, a large-scale multilingual\ndataset, consisting of 561k sentences in 11 differ-\nent languages. Our experiments reveal that there\nare always large discrepancies across different lan-\nguages in the tested ML-LMs, and different ML-\nLMs show very different language preferences.\nBeyond supervision-free analysis of ML-LMs,\nwe also study their performance in commonsense\nreasoning tasks, such as CSQA and CODAH,\nwithin across-lingual transfersetting (i.e., trained\non English data and tested on other languages).\nWe Ô¨Ånd that existing ML-LMs tend to have much\nlower accuracy in commonsense reasoning beyond\nEnglish. We conjecture a major common weak-\nness of existing ML-LMs is that their pretrain-\ning stages do not have a propersentence-level ob-\njective. Therefore, we proposemultilingual con-\ntrastive pre-training (MCP), which tasks a ML-\nLM to select the correct assertion out of a set\nof N contrastive assertions in N different lan-\nguages. We re-formatMickeyCorpus by sam-\npling across languages and thus form a dedicated\npre-training corpus for the MCP task. To fairly\nevaluate different ML-LMs and validate the ef-\nfectiveness of MCP, we create X-CSQA and X-\nCODAH, two cross-lingual commonsense reason-\ning datasets by translating their English versions to\n15 other languages2, including low-resource ones\nsuch as Swahili (sw) and Urdu (ur). Experiments\nshow that the proposed MCP objective indeed sig-\nniÔ¨Åcantly improves the performance of state-of-\nthe-art ML-LMs in cross-lingual commonsense\nreasoning. Our contributions are as follows:\n‚Ä¢ Resources. We collect a large multilin-\ngual parallel corpus,MickeyCorpus, con-\nsisting of 561k sentences in 11 languages,\nwhich can be used foranalyzing and improv-\ning ML-LMs. We also createX-CSQA and\nX-CODAH, two cross-lingual CSR bench-\nmarks in 16 languages, for question answer-\ning and scene completion, respectively.\n‚Ä¢ Evaluation and analysis.We analyze mul-\ntiple popular ML-LMs with MICKEY PROBE ,\na language-invariant, zero-shot task for prob-\ning common sense in ML-LMs; We also eval-\nuate them on X-CSQA and X-CODAH in a\ncross-lingual transfer setting.\n‚Ä¢ Method to improve ML-LMs.We propose\nmultilingual contrastive pretraining, a sim-\nple and effective sentence-level pretext task\nfor enhancing ML-LMs in cross-lingual com-\nmonsense reasoning, which signiÔ¨Åcantly im-\nproves the state-of-the-art ML-LMs in cross-\nlingual commonsense reasoning.\n2 Background and Related Work\nIn this section, we introduce important concepts,\nbackground knowledge, and related work before\nwe present our work in following sections.\n2.1 Multilingual Language Models\nA multilingual language model (ML-LM) aims\nto produce text representations for multiple lan-\nguages in a uniÔ¨Åed embedding space. One of\nthe unique advantages of ML-LMs is their po-\ntential ability to performzero-shot cross-lingual\ntransfer ‚Äî a model trained (or Ô¨Åne-tuned) on\ndata in one language (usually English) can be di-\nrectly used in other languages as well without fur-\nther Ô¨Åne-tuning. Improving ML-LMs is thus be-\nlieved as one of the most promising approach to-\nwards multilingual NLU at scale. mBERT (Devlin\n2The 16 languagesfor X-CSQA and X-CODAH: {en, zh,\nde, es, fr, it, jap, nl, pl, pt, ru, ar, vi, hi,sw, ur}.\n1276\net al., 2019) is simply the BERT model (Devlin\net al., 2019) trained on multilingual corpora with-\nout speciÔ¨Åc designs about multilinguality. The\ndistil-mBERT (d-mBERT) (Sanh et al., 2019) is\na smaller mBERT trained by knowledge distil-\nlation. Conneau and Lample (2019) proposed\nXLM(-100), which is pretrained with both masked\nlanguage modeling (MLM) and translation lan-\nguage modeling (TLM). Conneau et al. (2020)\nfurther proposed XLM-R, which improves the\nXLM with a better sub-token vocabulary and high-\nquality multilingual corpora (CC100). We leave\nthe analysis of recent seq2seq ML-LMs, such as\nmBART (Liu et al., 2020) and mT5 (Xue et al.,\n2021), as future work, because their architectures\nare signiÔ¨Åcantly different from the other ML-LMs.\nNote that the above ML-LMs are pretrained\nonly with token-level training objectives such as\nMLM (i.e., recovering masked tokens in monolin-\ngual text) and TLM (i.e., recovering masked to-\nkens in a pair of parallel sentences in two differ-\nent languages). However, most NLU tasks, in-\ncluding commonsense reasoning, highly rely on\nsentence-level representations. We argue that a\nwell-designed sentence-level pre-training objec-\ntive should improve ML-LMs for NLU tasks. This\nintuition motivates us to propose a sentence-level\npre-training objective ‚Äî MCP (Section5).\n2.2 Cross-lingual Language Understanding\nThere are a few recent multilingual benchmarks\nfor NLU tasks, e.g., XTREME(Hu et al., 2020),\nTyDi QA(Clark et al., 2020), and XGLUE(Liang\net al., 2020). XTREME and XGLUE are uniÔ¨Åed\nlarge-scale multilingual multitask benchmarks,\nwhile Ty-Di QA focuses on the QA. These existing\ncross-lingual benchmarks have not coveredcom-\nmonsense reasoning tasks, such as CSQA (Talmor\net al., 2019), SWAG (Zellers et al., 2018), and CO-\nDAH (Chen et al., 2019).\nCSQA is a question answering task and the\nother two are scene completion tasks, while all\nhave a multiple-choice selection objective, as\nshown in Figure1. These benchmarks are widely\nused to evaluate LMs for commonsense reasoning.\nUnfortunately, they are limited to English, not ap-\nplicable to evaluate models of multilingual com-\nmonsense knowledge, which motivates us to cre-\nate X-CSQA and X-CODAH. The goal of the re-\ncent XCOPA (Ponti et al., 2020) dataset shares a\nsimilar goal, but it only focused on event-based\ncausal reasoning in the scope of humans‚Äô social\nbehavior, which is thus arguably more culturally\nbiased. In contrast, the X-CSQA and X-CODAH\nare mainly for evaluating general world knowl-\nedge and cover more Ô¨Åne-grained types of reason-\ning (e.g., quantitative, negation), and thus engage\na more language-agnostic, comprehensive under-\nstanding of ML-LMs about common sense.\n2.3 The LAMA Probe and Its Limitations\nThe LAMA Probe (Petroni et al., 2019) is the\nseminal work on probing for common sense in\n(English) language models. It has a straightfor-\nward intuition: if a pretrained language model\ncontains more commonsense knowledge, then\nit should be better at recalling a masked to-\nken in a commonsense assertion (e.g.,‚Äúbirds have\n[mask]‚Äù). SpeciÔ¨Åcally, given a LAMA-probe sen-\ntence s and its masked token wt, a LM under\ntesting uses all past and future tokens ‚Äîs\\t :=\u0000\nw1,‚äø‚äø‚äø,w t\u00001,w t+1,‚äø‚äø‚äø,w |s|\n\u0000\n. as the input to\nrank all tokens in the vocabulary with the prob-\nability P\n\u0000\nwt | s\\t\n\u0000\nvia zero-shot inference. One\ncan evaluate the performance of recalling common\nsense by measuring the position of a correct to-\nken ‚Äúwing‚Äù in the ranked list. That is, the LAMA\nprobe method uses token-level probability as a\nproxy to probe for common sense in LMs via rank-\ning all tokens in their vocabularies.\nThis intuitive method, however, has several in-\nherent limitations. First, in many other languages,\nmulti-token concepts are ubiquitous, for exam-\nple, ‚ÄúÀõf√ú‚Äù (‚Äúlibrary‚Äù in SimpliÔ¨Åed Chinese).\nJiang et al.(2020) present several methods to de-\ncode multi-token entities so that they can adapt the\nLAMA probe to probe a LM forlanguage-speciÔ¨Åc\nanalysis. It is however infeasible to use token-\nlevel probing tasks if we want to analyze ML-LMs\nacross languages. In addition, the evaluation met-\nric of the LAMA probe could be unfair, because\nthere can be many correct words for a masked\nposition (e.g., ‚Äúbirds have legs/eyes‚Äù). The rank-\ning metrics of the LAMA probe, however, tend to\nignore these facts, resulting in a less trustworthy\nanalysis. The vocabulary-speciÔ¨Åc ranking is un-\nfair when comparing across different languages,\nso they can have very different label space. These\nlimitations of the LAMA Probe prevent us from\nanalyzing common sense in ML-LM across topo-\nlogically diverse languages.\n1277\n3 The Mickey Probe\nThe challenges of using the LAMA Probe for\nprobing common sense in ML-LMs motivate us to\npropose a more suitable method for analyzing ML-\nLMs, one that can fairly compare across a diverse\nset of languages. We present M ICKEY PROBE ,\na Multilingual task for probing commonsense\nknowledge and analysis. We design a language-\nagnostic probing task with a sentence-selection\nobjective for analyzing common sense of a ML-\nLM: given a set of assertions (i.e., declarative sen-\ntences) that have similar words and syntactic fea-\ntures, select the one with highest commonsense\nplausibility. We present the task formulation in\nthis section and then introduce how we collect the\ndedicated dataset in Section4.\nNotations. We deÔ¨Åne a Mickey probeM as a set\nof K assertions in the same language, where one\nand only one of them (say,Mi) is the truth asser-\ntion with better commonsense plausibility than the\nother K \u00001 ones. Each Mickey probeM has mul-\ntiple semantically equivalent versions in different\nlanguages. Let us denote a language by l 2L\nwhere L = {en, fr, ru, zh, ‚äø ‚äø ‚äø} and |L| is the\nnumber of languages of interest. Then,Ml is the\nprobe M in the languagel. For example,Men and\nMfr denote the probes with the same meaning but\nin English (en) and French (fr) respectively. We\nuse M to denote a multilingual parallel dataset for\nMICKEY PROBE , which consists ofT ‚á•|L|‚á•K as-\nsertions. T is the number of MICKEY PROBE items\nand each item hasK assertions and|L| language.\nFinally, we can formally describe a multilingual\nparallel datasetM for MICKEY PROBE :\n8M 2M , 8(lx,l y) 2L 2, 8i 2 NÔ£øK,\nMlx\ni ./ Mly\ni ‚äø\n(1)\nWe use the notation./ to indicate two assertions\nin different languages (e.g.,lx and ly) are semanti-\ncally equivalent to each other. We leave the details\nof creating such anM in Section4.\nCommonsense Probing Task. Given a Micky\nProbe M in the datasetM, and suppose the index\nof the truth assertion to bet, a perfect multilingual\nlanguage model would produce sentence probabil-\nities such that it always gives the truth assertion\nMl\nt the highest probability among other candidates\nfor every language.\n8l 2L , 8i 2 NÔ£øK,P (Ml\ni ) Ô£ø P (Ml\nt )‚äø (2)\nRanking \nby PLLs \nMickeyProbeML-LM\nThe effectof readingthe newsis lyingabout the world.\n‚Ä¶ of interviewingthe deceasedis learningabout the world.\n‚Ä¶ oftrackingthe dragonis learningabout the world. \n‚Ä¶ ofreadingthe newsis learningabout the world.\n‚Ä¶ of readingthe newsis sayingabout the world.\nÈòÖËØªÊñ∞ÈóªÁöÑÊïàÊûúÊòØÂØπ‰∏ñÁïåÊííË∞é„ÄÇ\nÈááËÆøÊ≠ªËÄÖÁöÑÊïàÊûúÊòØ‰∫ÜËß£‰∏ñÁïå„ÄÇ\nËøΩË∏™ÈæôÁöÑÊïàÊûúÊòØ‰∫ÜËß£‰∏ñÁïå„ÄÇ\nÈòÖËØªÊñ∞ÈóªÁöÑÊïàÊûúÊòØ‰∫ÜËß£‰∏ñÁïå„ÄÇ\nÈòÖËØªÊñ∞ÈóªÁöÑÊïàÊûúÊòØÊèèËø∞‰∏ñÁïå„ÄÇ\nen: [4,3,1,5,2]‚Ä¶.\nzh: [2,4,3,1,5]\nùëÄ4ùëíùëõ\nùëÄ4ùëß‚Ñé\nFigure 2: A Mickey Probe exampleM has a set of\nprobes in different languages (e.g.,Men/zh), and each\nof them is a set of 5 assertions. We rank assertions\nin the same language by their PLLs to probe common\nsense in ML-LMs across different languages.\nIt is still an open problem to properly com-\npute sentence probabilities from masked lan-\nguage models, the recently proposedpseudo-log-\nlikelihood scoring (PLLs) (Salazar et al., 2020)\nhas shown promising results in many downstream\nNLP applications that need sentence re-ranking\n(e.g., speech recognition, and translation), sug-\ngesting it is a promising proxy of sentence prob-\nability. Given a sentences, its PLL is deÔ¨Åned as:\nlog P (s) = PLL(s): =\n|s|X\ni=1\nlog P\n\u0000\nwi | s\\i\n\u0000\n(3)\nThat is, we individually mask each tokenwi at a\ntime and use the remaining contexts\\i to get the\nprobability of a wordwi in the sentences. Finally,\nwe aggregate them to approximateP (s).\nEvaluation Metric. The evaluation met-\nric for M ICKEY PROBE over a multilingual\nparallel dataset M in a speciÔ¨Åc language\nl is deÔ¨Åned as the overall hit@k accu-\nracy of the selection results hit@ k (l)=P\nM2M 1{truth-rank(Ml) Ô£ø k} ‚óÅ |M| where\ntruth-rank(Ml) means the the position of the\ntruth assertion Ml\nt in Ml sorted by their prob-\nabilities deÔ¨Åned in Eq. (3). The hit@1 is just\nequivalent to the conventionalaccuracy.\nAdvantages of MICKEY PROBE . There are two\nkey advantages of the MICKEY PROBE for evalu-\nating ML-LMs: (1) Thesentence-level probabil-\nity can be more generally applied in languages be-\nsides English, comparing with the LAMA probe\nwhich only studies single-token English words.\n1278\nModels \\L en de it es fr nl ru bg vi zh hi avg\nBT-Cosine 1.0 0.937 0.936 0.935 0.934 0.933 0.901 0.901 0.882 0.879 0.869 0.919\nCC-size (GB) 300.8 66.6 30.2 53.3 56.8 29.3 278.0 57.5 137.3 46.9 20.2 97.9\nShortest 23.17 27.21 29.93 31.00 35.84 31.68 18.55 22.01 15.46 25.07 20.66 25.51\nd-mBERT 62.95 34.56 25.26 34.85 50.46 32.39 21.49 29.14 19.77 32.57 25.88 33.57\nmBERT 63.56 35.58 29.13 44.70 42.58 35.15 28.30 36.03 24.04 28.15 27.85 35.92\nXLM-100 60.57 36.33 26.49 43.39 32.53 36.24 32.90 39.71 25.79 33.01 31.49 36.22\nXLM-RB 89.69 58.94 53.45 60.88 49.12 59.99 45.74 45.26 41.65 51.02 40.73 54.22\nXLM-RL 90.03 61.98 53.42 63.68 59.47 63.12 50.03 47.01 45.30 55.93 43.98 57.63\nTable 1: The hit@1 accuracy (%) of the Ô¨Åve ML-LMs for the MICKEY PROBE task.\n(2) The task formulation creates a relatively\nclosed-ended setting, such that we can use a\nlanguage-independent evaluation metricto fairly\ncompare across various languages within a ML-\nLM and compare across various ML-LMs for\na particular language. In addition, we can see\nLAMA Probe as amonolingual, word-level ver-\nsion of the more general MICKEY PROBE : the\nLAMA Probe is whenL = {en}, and{Men} =\nM 2M is a huge number of K assertions (i.e.,\nthe vocabulary size) ‚Äî a Ô¨Åxed [mask] is re-\nplaced by all tokens in the vocabulary.\n4 The Mickey Corpus and Evaluation\nWe present a procedure for automatically creat-\ning a multilingual parallel datasetM for the prob-\ning task MICKEY PROBE . Our collected corpus,\nnamed MickeyCorpus , has 561k sentences in\n11 languages (T =10.2k, K=5, |L|=11).\n4.1 Creating English Probes\nFor the correct commonsense assertions in En-\nglish, we have an existing resource, the OMCS\ncorpus (Singh et al., 2002) which contains human-\nwritten sentences in English that describe com-\nmonsense facts. Each assertion can be used as a\nMen\nt and we perform perturbations on it to cre-\nate the otherK \u00001 distractor assertions (i.e., false\ncandidates), yielding anMen example.\nInspired by BERT-attack method (Li et al.,\n2020), we use a simple method to generate false\nassertions that are semantically related and syn-\ntactically similar to the truth assertions. Given a\ncorrect assertion, we Ô¨Årst randomly sample a few\n(1 ‚á† 3) words with a part-of-speech tag as noun,\nverb, or adjective, and replace them with [mask].\nThen, we use a beam-search style method to de-\ncode the [mask] tokens one by one from left to\nright. To ensure that the distractors are less plau-\nFigure 3: The MICKEY PROBE results in hit@1-acc. A\nlarger version of this Ô¨Ågure is in Appendix (Fig.6).\nsible, we limit the decoding steps to only sam-\nple tokens that ranks between 200th‚á†300th. We\nrepeat the above procedure multiple times with\ndifferent sets of[mask] tokens. Then, we use\nStanza (Qi et al., 2020) to remove distractors that\nhave sequences of POS tags or morphological fea-\ntures different from the truth assertions. Finally,\nwe sampleK \u0000 1 of them as the distractors.\n4.2 Scaling to Ten Other Languages.\nWe use bidirectional translation with the Mar-\nianMT models (Junczys-Dowmunt et al., 2018)\npretrained on the OPUS corpora ( Tiedemann,\n2016). We translate all English probes to the 25\nlanguages that has models in both directions and\nthen translate them back to English. As the outputs\nfrom these models might contain noise and errors,\nwe compute the semantic similarities (i.e., cosine\nsimilarity) between the originalMen and the back-\ntranslated Mx-en via the SentenceBERT (Reimers\nand Gurevych, 2019) model.\nTo ensure the quality and fair comparisons, we\nset a similarity threshold as 0.75 and keep the\nintersections of probes in all languages. Con-\nsidering some languages tend to have transla-\ntions of lower quality, we Ô¨Ånally choose the\nbest 10 languages to build the Mickey Probe\ndataset for our analysis, yielding 10k exam-\nples in each language and 10.2k*5*11 ‚á° 561k\nsentences in total. The language set L =\n1279\n{en, de, fr, ru, es, hi, vi, bg, zh, nl, it}.\nNote that our purpose of checking the back-\ntranslation quality here is mainly to only keep\nthe high-quality translations for all language pairs\nthat we considered. Conventional metrics, e.g.,\nBLUE score (Papineni et al., 2002), which focus\non the exact word match, are thus less suitable:\ngiven the original sentence ‚ÄúI have a book‚Äù, the\ntranslation results ‚ÄúI have a novel‚Äù and ‚ÄúI have a\ntool‚Äù will be seen as equally wrong. Inspired by\nBERTScore (Zhang et al., 2020), the BT-cosine is\nbased on SentenceBERT, which efÔ¨Åciently gives a\nhigher score for the former and a lower score for\nthe latter, due to the semantic relatedness between\n‚Äúnovel‚Äù and ‚Äúbook.‚Äù We observed that most of our\nback-translations are in similar situations, and thus\ndecide to use BT-cosine instead of others.\n4.3 Analyzing ML-LMs with Mickey Probes\nWe now use theMickeyCorpus to evaluate the\n5 pre-trained ML-LMs introduced in Section2.1:\nd-mBERT (Sanh et al., 2019), mBERT (Devlin\net al., 2019), XLM (Conneau and Lample, 2019),\nXLM-RBase, and XLM-R Large (Conneau et al.,\n2020). All these ML-LMs pretraining objectives\ncontain masked-word-prediction tasks, so we can\neasily use PPLs (Eq. 3) to probe them a zero-\nshot, supervision-free manner with hit@1 accu-\nracy. (The hit@2 results are shown in Appendix.)\nWe present a histogram in Figure3 and show the\nconcrete results in Table1. We Ô¨Ånd that there\nare always large discrepancies across different lan-\nguages in all tested ML-LMs, which motivates us\nto analyze the following questions.\nQ1: Do different ML-LMs have similar lan-\nguage preferences? No. We arrange the lan-\nguages in all ML-LMs with the same order for\nFigure 3 ‚Äî the monotonically descending order\nof XLM-RL. Interestingly, we Ô¨Ånd that different\nML-LMs are good for different languages, result-\ning in a very diverse set of trends. For example,\nXLM-RB, has a higher performance init than zh\nand fr, unlike XLM-R\u0000L which are pre-trained\non the same corpora with the same objectives.\nmBERT and d-mBERT has stronger performance\nin fr than nl and de, unlike XLM and XLM-R.\nQ2: Does length inÔ¨Çuence PLL ranking?Not\nmuch. The PLL computation indeed tends to pre-\nfer shorter sequences (see Eq.3), so one may won-\nder if the length of assertions would inÔ¨Çuence the\nprobing results. The ‚ÄúShortest‚Äù row in Table1\npresents the results when we always select the\nshortest assertion within a probe, instead of PLL\nranking. The gaps between these scores and XLM-\nR-L‚Äôs suggest that the probing task indeed uses\nPLL as a valid proxy for evaluating common sense\nbased on sentence-level semantics.\nQ3: Is the translation quality a key factor?We\nshow ‚ÄúBT-Cosine‚Äù, the mean of the cosine scores\nbetween the original English sentences and the\nback-translated ones, and sort the table by these\nnumbers. The Ô¨Årst 5 languages, {de, it, es, fr, nl}\nhave the largest BT-Cosine, i.e., the best transla-\ntion quality, and they indeed have better perfor-\nmances in general for XLM-R models. However,\nalthough zh has a worse BT-score thanvi, all ML-\nLMs perform better inzh than vi. Thus, we be-\nlieve the translation quality ofMickeyCorpus\nwill not be a factor to inÔ¨Çuence our understanding\nof ML-LMs. Consequently, this suggests that fur-\nther study must depend on pre-training corpora of\neach ML-LM in different languages.\nQ4: Does the size of pre-training corpora mat-\nter? We list the size of the monolingual corpus\nin each language for CC-100 that XLM-R are pre-\ntrained on (i.e., the CC-size row). Althoughru has\na much larger corpus thande, it, etc., the XLM-\nR performance inru is much worse. In addition,\nfr and nl have almost the same translation quality\nwhile fr‚Äôs CC-size is twice the size ofnl, but the\nperformance infr is still much worse thannl.W e\nconjecture this would be either due to the design of\nsub-token vocabulary or the text quality (instead of\nthe size) of the CC-100 corpora.\nFurther implications. The benchmark results\nof Ô¨Åve popular ML-LMs on the MICKEY PROBE\ntask over the MickeyCorpus offer the initial\nand valuable understanding with a closer look\nat the commonsense knowledge of ML-LMs by\nprobing them in a uniÔ¨Åed evaluation protocol. One\ncan either compare a ML-LM across different lan-\nguages or compare a certain language across ML-\nLMs in Table1. These comparable results sup-\nport further analysis that can beneÔ¨Åt the develop-\nment of ML-LMs in the future. After all, even\nthe best ML-LM XLM-RL also degrades much in\nother languages, and also perform slightly worse\nthan RoBERTaL in en (93.4%). We argue (culture-\ninvariant) common sense knowledge should be\nseen as an important way to connect multiple lan-\nguages and thus better align them in a shared em-\nbedding space induced by a ML-LM.\n1280\n5 Multilingual Contrastive Pre-Training\nIn this section, we reformat the MICKEY PROBE\nso that we can reuse the MickeyCorpus for\nimproving the pre-trained ML-LMs for common-\nsense reasoning beyond English. We propose a\nmultilingual contrastive pre-training(MCP) task\nthat focuses on enhancing the sentence-level rep-\nresentation of ML-LMs. MCP improves a ML-\nLM in a multilingual, contrastive environment,\nwhere the model learns to select the assertion with\nthe best commonsense plausibility from a set of\ncontrastive sentencesin different languages. Each\nMCP example is a set ofmultilingual assertions\nwhile each Mickey probe is amonolingual set.\nMCP Dataset Creation from M. We create\npretraining examples for the MCP task by con-\nverting MICKEY PROBE examples, as shown in the\nsteps illustrated in Algorithm1. Simply put, we\nreformat aK-way Mickey ProbeM (K ‚á•| L |as-\nsertions) to a MCP example by sampling a set of\nV candidate assertions inV different languages.\nWe convert all examples in theMickeyCorpus\nM to build a newcross-lingual sentence-selection\ndataset C for learning the MCP task.\nMCP Learning. Given a MCP exampleC 2C ,\nwe append one dense linear layerf on top of a\nML-LM with parameters denoted as‚á•ML-LM for\nlearning to predict thecommonsense plausibility\nscore of each assertionCi 2 C as follows:\nhi = ML-LM(Ci)‚äø[CLS] (4)\noi = f(hi;‚á•f ) (5)\nzi = eoi\nPV =|C|\nj=1 eoj\n(6)\n‚á¢ =\nVX\ni=1\n\u00001i log (zi) (7)\nWe Ô¨Årst get the logitoi of each assertion by pro-\njecting its[CLS] embeddings hi to a logitoi via\na dense layer f with parameters ‚á•f ; Then, we\nuse SoftMax to normalize the logits as plausibility\nscores zi; Finally, we compute the cross-entropy\nloss ‚á¢ where 1i=1 ifCi is a correct assertion and\n0 otherwise. We Ô¨Åne-tune{‚á•ML-LM, ‚á•f } to mini-\nmize the overall loss over the MCP datasetC.\n6 Evaluation for Cross-lingual CSR\nIn this section, we introduce the datasets, experi-\nmental setup, results, and our analysis.\nAlgorithm 1:Convert a Mickey ProbeM\nto an example for the MCP task.\nIn: M 2M /* is a probe that has|L| sub-sets;\neach sub-setMlx is a set ofK assertions in the\nsame languagelx 2L . Mlx\nt is always the truth. */\nOut: C /* A set ofV assertions in different\nlanguages. */\nRemarks: \u0000n(X) is a function to randomly\nsample n unique elements from a setX.\n1 la  \u0000\u00001(L) /* Pick an anchor language. */\n2 C  \u0000 {Mla\nt } /* Initiate w/ the truth assertion. */\n/* Iterate each sampled distractor languageli. */\n3 foreach li 2 \u0000V \u00001(L\u0000 la) do\n/* Sample an index of distractor assertion. */\n4 j  \u0000\u00001(NÔ£øK \u0000{ t})\n/* Add a distractor assertion as a candidate. */\n5 C.add(Mli\nj )\n6.1 X-CSQA & X-CODAH: Two New\nBenchmarks for Evaluating XCSR\nTo evaluate ML-LMs for commonsense reason-\ning in a cross-lingual zero-shot transfer setting, we\ncreate two benchmark datasets, namely X-CSQA\nand X-CODAH. Table3 shows the statistics of the\ntwo datasets. SpeciÔ¨Åcally, we use online commer-\ncial services such asDeepL Pro Translateto col-\nlect high-quality translations of the examples in\nCSQA and CODAH for15 languagesother than\nEnglish. The size of CODAH is small (only 2.7k),\nso we use 7k SWAG validation examples as addi-\ntional training data which share the same formu-\nlation. We discuss the reduction ofcultural dif-\nferences and quality control of automatic transla-\ntions as well as other details inEthical Consider-\nations (the paragraph for cultural bias reduction)\nand Appendix (A). As our goal is to evaluate differ-\nent ML-LMs (instead of different languages) in a\nuniÔ¨Åed evaluation protocol for cross-lingual com-\nmonsense reasoning, we argue that such automati-\ncally translated examples, although might contain\nnoise, can serve as a starting benchmark for us to\nobtain meaningful analysis before more human-\ntranslated datasets will be available in the future.\n6.2 Setup\nWe focus on 4 popular ML-LMs that we intro-\nduced in Section2.1: mBERT, XLM-100, XLM-\nRB and XLM-RL as well as our proposed MCP\nmethod. For both tasks, we concatenate each\nprompt (the question or Ô¨Årst sentence) and each\n1281\nen de it es fr nl ru vi zh hi pl ar ja pt sw ur avg\nCC-size (GB)300.866.6 30.2 53.3 56.8 29.3 278.0137.346.9 20.2 44.6 28.0 69.3 49.1 1.6 5.7 76.10\nX-CODAH[Task:Scene Completion;Random Guess:25.0;RoBERTaLfor en: 81.6]\nmBERT 42.9 33.1 33.5 33.8 35.2 33.7 31.9 22.8 38.0 26.531.0 34.8 34.0 37.2 30.8 31.533.2\nXLM-100 42.7 31.5 32.2 30.7 34.9 32.6 30.9 24.7 31.4 26.827.0 30.0 27.4 33.2 25.3 24.930.4\nXLM-R-B 50.1 45.8 44.4 44.2 45.2 42.0 44.1 43.2 44.6 38.141.9 37.8 42.0 44.1 35.6 34.642.4\nXLM-R-L 66.4 59.6 59.9 60.9 60.1 59.3 56.3 57.4 57.3 49.157.5 51.2 53.8 58.2 42.2 46.656.0\nMCP(XLM-RB) 52.2 47.6 46.2 44.4 48.1 44.8 42.9 43.2 45.7 37.841.8 41.8 42.9 44.7 37.2 36.443.6\nMCP(XLM-RL) 69.9 60.7 61.9 60.7 61.4 60.7 58.6 62.3 61.9 53.759.0 54.1 54.7 60.8 44.6 48.058.3\n\u0000(XLM-RL) +3.5 +1.1 +2.0 -0.2 +1.3 +1.4 +2.3 +4.9 +4.6 +4.6 +1.5 +2.9 +0.9 +2.6 +2.4 +1.4 +2.3\nX-CSQA[Task:Question Answering;Random Guess:20.0;RoBERTaLfor en:70.4 ]\nmBERT 38.8 29.6 36.4 35.3 33.8 32.6 32.7 22.2 37.8 21.127.2 27.7 31.4 34.1 21.8 23.730.4\nXLM-100 34.3 26.7 28.5 29.3 28.3 27.2 29.9 21.1 28.6 22.126.6 26.3 25.1 30.9 20.1 21.726.7\nXLM-RB 51.5 44.1 42.1 44.8 44.0 43.3 39.5 42.6 40.6 34.640.2 38.4 37.5 43.4 29.6 33.040.6\nXLM-RL 66.7 56.1 58.2 59.5 60.3 56.8 52.1 51.4 52.7 48.753.9 48.4 50.0 59.9 41.6 45.253.8\nMCP(XLM-RB) 52.1 46.2 45.6 44.3 44.7 45.3 42.8 45.3 44.3 36.841.4 36.8 37.5 44.9 28.1 33.441.9\nMCP(XLM-RL) 69.5 59.3 60.3 61.4 60.0 61.1 57.5 55.7 56.7 51.356.1 52.3 50.2 60.7 43.3 48.856.5\n\u0000(XLM-RL) +2.8 +3.3 +2.2 +1.9 -0.4 +4.3 +5.4 +4.3 +4.0 +2.6 +2.1 +3.9 +0.2 +0.8 +1.7 +3.6 +2.7\nTable 2: Benchmark results for different ML-LMs and MCP-enhanced models for X-CSQA and X-CODAH in a\nzero-shot cross-lingual setting.\u0000 is the improvement of MCP. {pl,ar,ja,pt,sw,ur} are unseen in MCP.\nStat.#Dataset! X-CSQA X-CODAH\nTask Format QA SceneComp.\n# Languages 15 15\n# Options per Example 5 4\n# Training (en) 8,888 8,476\n# Dev per Lang. 1,000 300\n# Test per Lang. 1,074 1,000\n# Total Instances 80,550 60,000\nTable 3: Statistics of the two X-CSR datasets.\nof its options individually in the form of ‚Äú[CLS]\nprompt [SEP] optioni [SEP]‚Äù. Then, we Ô¨Åne-tune\nML-LMs over the English training dataset and test\nthem on other languages.\nWhy zero-shot cross-lingual transfer?It is al-\nmost impossible to collect data inall languages\nthat an NLU system might be used for. There-\nfore, prior works mainly focus on zero-shot cross-\nlingual transfer (Conneau et al., 2018), which is\nmore meaningful and can offerlower-bound per-\nformance analysis. It is also an ideal setting\nfor studying CSR because most commonsense\nfacts are language-invariant. Thus, an English-\nÔ¨Ånetuned ML-LM for CSR should be able to trans-\nfer its ability to a wide range of other languages as\nwell. Furthermore, our goal of this paper is to eval-\nuate and improve ML-LMs, so translating back to\nEnglish and then use an English-only LM is also\nnot helpful towards to this end.\nFigure 4: Categorized accuracy in for MCP(XLM-RL)\non X-CODAH. Each box is for 15 languages.\n6.3 Experiments for Cross-lingual CSR\nIn Table2, we present the empirical results over\nX-CODAH and X-CSQA for the ML-LMs as well\nas two models enhanced by our proposed MCP\nmethod. On both tasks, the XLM-RL performs the\nbest with a large margin. Enhanced by the MCP\nmethod, both XLM-RB and XLM-RL see signif-\nicant improvement (e.g., 2.7% absolute improve-\nment for XLM-RL on X-CSQA-avg).\nCan MCP‚Äôs improvement generalize to un-\nseen, low-resource languages? Note that MCP\ndataset only involves 9 languages here, and there\nare 6 languages that are totallyunseen in the MCP\ntraining (i.e., {pl, ar, ja, pt, sw, ur}). The largest\nperformance gain is inru on X-CSQA andvi on X-\nCODAH. Surprisingly, we Ô¨Ånd the improvements\non them are also large for XLM-RL (e.g., 48.4!\n52.3 forar). In addition, for the twolow-resource\nlanguages sw and ur, MCP also brings2 ‚á† 3 per-\ncentage points of improvement for XLM-RL. It is,\nhowever, not always the case forXLM-RB, which\nwe conjecture tends to be more likely to overÔ¨Åt.\n1282\nMCP(XLM-R-B)MCP(XLM-R-L)XLM-R-LXLM-R-B\n100200300400500600700800Step0.20.30.40.50.60.7\nFigure 5: Dev acc v.s. learning steps on X-CSQA.\nAlthough ML-LMs enjoy the merits of zero-\nshot cross-lingual transfer, their performances are\nusually worse than the English-only RoBERTaL\non the en-test (70.4% vs 66.7% for X-CSQA).\nAlthough MCP can mitigate the gap (70.4% vs\n69.5%) for X-CSQA, there is still a large gap\n(81.6% vs 69.9%) for X-CODAH. We use Fig.4\nto analyze how different categories of common-\nsense reasoning in CODAH (Chen et al., 2019)\nare diverse in different languages. We Ô¨Ånd that\nothers, reference, and negation have relatively\nsmaller variances across different languages, as\nthey are more language-invariant. However, a\nfew polysemous, idioms examples can be English-\nspeciÔ¨Åc which may not generalize to other lan-\nguages. More detailed analysis is in Appendix.\nFrom the curve of dev accuracy in Figure5,\nwe see that MCP-enhanced XLM-R models are\nmuch more sample efÔ¨Åcient and converge much\nfaster than vanilla versions. This suggests that the\nMCP, if used on a larger corpus with broader top-\nics, can potentially produce a better ML-LM with\nmore general usage, especially when only limited\nlabelled is available. Our results on XNLI-10%\n(using 10% of the training data) (Conneau et al.,\n2018) show that MCP-enhanced XLM-RL has 1.2\npercent accuracy improvement on the average of\n15 languages. As our focus in this paper is com-\nmonsense reasoning, we leave the study on other\ncross-lingual NLU tasks as future work. Impor-\ntantly, our experiments imply that a proper (con-\ntinual) pre-training task that has a (contrastive)\nsentence-level objective could improve both the Ô¨Å-\nnal performance as well as learning efÔ¨Åciency.\n7 Conclusion\nWe evaluate and improve popular multilingual lan-\nguage models (ML-LMs) for advancing common-\nsense reasoning beyond English. We propose\nthe MICKEY PROBE ,a language-agnostic probing\ntask for analyzing common sense of ML-LMs in a\nzero-shot manner. With our proposed new bench-\nmark datasets via automatic translation, X-CSQA\nand X-CODAH, we evaluate ML-LMs in a cross-\nlingual transfer setting for commonsense reason-\ning. We also improve the state-of-the-art ML-LM\nwith a simple yet effective method ‚Äî multilingual\ncontrastive pre-training, which uses a sentence-\nlevel objective to enhance sentence representa-\ntions, yielding a signiÔ¨Åcant performance gain. All\nabove work is based onMickeyCorpus, which\ncan be used as both a probing dataset and a pre-\ntraining corpus for analyzing and improving ML-\nLMs. We hope our resources and pre-training\nmethod for ML-LMs can help the community ad-\nvance commonsense reasoning beyond English.\nAcknowledgements\nThis research is supported in part by the OfÔ¨Åce\nof the Director of National Intelligence (ODNI),\nIntelligence Advanced Research Projects Activity\n(IARPA), via Contract No. 2019-19051600007,\nthe DARPA MCS program under Contract No.\nN660011924033 with the United States OfÔ¨Åce Of\nNaval Research, the Defense Advanced Research\nProjects Agency with award W911NF-19-20271,\nand NSF SMA 18-29268. The views and con-\nclusions contained herein are those of the authors\nand should not be interpreted as necessarily rep-\nresenting the ofÔ¨Åcial policies, either expressed or\nimplied, of ODNI, IARPA, or the U.S. Govern-\nment. We would like to thank all the collabora-\ntors in USC INK research lab and the reviewers\nfor their constructive feedback on the work.\n* Ethical Considerations\nResource Copyright This work presents three\nnew resources: MickeyCorpus, X-CODAH,\nand X-CSQA, which are multilingual extension of\nthe OMCS (Singh et al., 2002) 3, CSQA (Talmor\net al., 2019)4, and CODAH (Chen et al., 2019)5 re-\nspectively. All these three original sources of the\ndata are publicly available for free, and we do not\nadd any additional requirement for accessing our\nresources. We will highlight the original sources\nof our data and ask users to cite the original papers\nwhen they use our extended versions for research.\n3https://github.com/commonsense/\nconceptnet5/wiki/Downloads4https://www.tau-nlp.org/commonsenseqa5https://github.com/Websail-NU/CODAH\n1283\nCultural Bias Reduction Like most most mul-\ntilingual parallel resources, especially in general\nNLU domain, there exists potential data bias due\nto the barrier of languages as well ascultural dif-\nferences (Acharya et al., 2020; Lin et al., 2018),\nwhich could induce the labeling differences on the\nsame situation. For example, a question like ‚Äúwhat\ndo people usually drink in the morning? (cof-\nfee/tea/milk)‚Äù or ‚Äúwhen does a wedding usually\nstart? (morning/afternoon/evening)‚Äù might be an-\nswered very differently by people from different\nbackgrounds and cultures, not to mention differ-\nent languages. The prior English commonsense\nresources which our datasets are built on are al-\nready possess such inherent bias, even with in the\nEnglish language. Therefore, before we translate\nCSQA and CODAH, we intentionally remove the\nexamples that are either labeled as non-neutral by\na pre-trained sentiment classiÔ¨Åer, or contained any\nkeywords that are relevant to social behavior (e.g.,\nweddings). We manually inspect test examples in\nX-CSQA and X-CODAH in the English and Chi-\nnese versions and have a strong conÔ¨Ådence there is\nfew strongly controversial example. However, we\nadmit that such reduction of cultural differences in\ncommon sense has not been systematically mea-\nsured in this work for other languages.\nApplication Risks of Cross-lingual CSR.\nThe work also evaluates a few multilingual lan-\nguage models (ML-LMs) for cross-lingual com-\nmonsense reasoning (XCSR), and introduced a\nnew model which outperforms them. This raises\nthe question of whether harm might arise from\napplications of XCSR‚Äîor more generally, since\nXCSR is intended as a step toward making\nEnglish-only CSR more applicable in other lan-\nguages, whether harm might arise more generally\nfrom existing ML-LMs. Among the risks that need\nto be considered in any deployment of NLP tech-\nnology are that responses may be wrong or biased,\nin ways that would lead to improperly justiÔ¨Åed de-\ncisions. Although in our view the current technol-\nogy is still relatively immature, and unlikely to be\nÔ¨Åelded in applications that would cause harm of\nthis sort, it is desirable that ML-LMs provide au-\ndit trails, and recourse so that their predictions can\nbe explained to and critiqued by affected parties.\nReferences\nA. Acharya, Kartik Talamadupula, and Mark A. Fin-\nlayson. 2020. An atlas of cultural commonsense for\nmachine reasoning.ArXiv, abs/2009.05664.\nMichael Chen, Mike D‚ÄôArcy, Alisa Liu, Jared Fer-\nnandez, and Doug Downey. 2019. CODAH: An\nadversarially-authored question answering dataset\nfor common sense. InProceedings of the 3rd Work-\nshop on Evaluating Vector Space Representations\nfor NLP, pages 63‚Äì69, Minneapolis, USA. Associa-\ntion for Computational Linguistics.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A bench-\nmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the\nAssociation for Computational Linguistics, 8:454‚Äì\n470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm√°n, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020.Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440‚Äì\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019.Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems 32: Annual\nConference on Neural Information Processing Sys-\ntems 2019, NeurIPS 2019, December 8-14, 2019,\nVancouver, BC, Canada, pages 7057‚Äì7067.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475‚Äì2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nErnest Davis and Gary Marcus. 2015. Commonsense\nreasoning and commonsense knowledge in artiÔ¨Å-\ncial intelligence. Communications of the ACM,\n58(9):92‚Äì103.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171‚Äì4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nYanlin Feng, Xinyue Chen, Bill Yuchen Lin, Peifeng\nWang, Jun Yan, and Xiang Ren. 2020. Scalable\nmulti-hop relational reasoning for knowledge-aware\n1284\nquestion answering. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1295‚Äì1309, On-\nline. Association for Computational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A Massively Multilingual Multi-\ntask Benchmark for Evaluating Cross-lingual Gen-\neralization. Technical report.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020.X-FACTR:\nMultilingual factual knowledge retrieval from pre-\ntrained language models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5943‚Äì5959,\nOnline. Association for Computational Linguistics.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz,\nTomasz Dwojak, Hieu Hoang, Kenneth HeaÔ¨Åeld,\nTom Neckermann, Frank Seide, Ulrich Germann,\nAlham Fikri Aji, Nikolay Bogoychev, Andr√© F. T.\nMartins, and Alexandra Birch. 2018.Marian: Fast\nneural machine translation in C++. In Proceedings\nof ACL 2018, System Demonstrations, pages 116‚Äì\n121, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,\nand Xipeng Qiu. 2020.BERT-ATTACK: Adversar-\nial attack against BERT using BERT. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n6193‚Äì6202, Online. Association for Computational\nLinguistics.\nYaobo Liang, Nan Duan, Yeyun Gong, Ning Wu, Fen-\nfei Guo, Weizhen Qi, Ming Gong, Linjun Shou,\nDaxin Jiang, Guihong Cao, Xiaodong Fan, Ruofei\nZhang, Rahul Agrawal, Edward Cui, Sining Wei,\nTaroon Bharti, Ying Qiao, Jiun-Hung Chen, Win-\nnie Wu, Shuguang Liu, Fan Yang, Daniel Cam-\npos, Rangan Majumder, and Ming Zhou. 2020.\nXGLUE: A new benchmark datasetfor cross-lingual\npre-training, understanding and generation. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP) ,\npages 6008‚Äì6018, Online. Association for Compu-\ntational Linguistics.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang\nRen. 2019. KagNet: Knowledge-aware graph net-\nworks for commonsense reasoning. In Proceedings\nof the 2019 Conference on Empirical Methods in\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2829‚Äì2839, Hong\nKong, China. Association for Computational Lin-\nguistics.\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xi-\nang Ren. 2020.Birds have four legs?! NumerSense:\nProbing Numerical Commonsense Knowledge of\nPre-Trained Language Models. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 6862‚Äì\n6868, Online. Association for Computational Lin-\nguistics.\nBill Yuchen Lin, Frank F. Xu, Kenny Zhu, and Seung-\nwon Hwang. 2018. Mining cross-cultural differ-\nences and similarities in social media. In Proceed-\nings of the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 709‚Äì719, Melbourne, Australia. Asso-\nciation for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation. Trans-\nactions of the Association for Computational Lin-\nguistics, 8:726‚Äì742.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002.Bleu: a method for automatic eval-\nuation of machine translation. InProceedings of the\n40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311‚Äì318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nFabio Petroni, Tim Rockt√§schel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as\nknowledge bases? In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463‚Äì2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nEdoardo Maria Ponti, Goran Glava≈°, Olga Majewska,\nQianchu Liu, Ivan Vuli¬¥c, and Anna Korhonen. 2020.\nXCOPA: A multilingual dataset for causal com-\nmonsense reasoning. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 2362‚Äì2376, On-\nline. Association for Computational Linguistics.\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton,\nand Christopher D. Manning. 2020. Stanza: A\npython natural language processing toolkit for many\nhuman languages. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations , pages 101‚Äì\n108, Online. Association for Computational Lin-\nguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. InProceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP),\npages 3982‚Äì3992, Hong Kong, China. Association\nfor Computational Linguistics.\n1285\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020.Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics,\npages 2699‚Äì2712, Online. Association for Compu-\ntational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter.ArXiv,\nabs/1910.01108.\nPush Singh, Thomas Lin, Erik T Mueller, Grace Lim,\nTravell Perkins, and Wan Li Zhu. 2002. Open mind\ncommon sense: Knowledge acquisition from the\ngeneral public. InOTM Confederated International\nConferences\" On the Move to Meaningful Internet\nSystems\", pages 1223‚Äì1237. Springer.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019.CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4149‚Äì4158, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nJ√∂rg Tiedemann. 2016. OPUS ‚Äì parallel corpora for\neveryone. In Proceedings of the 19th Annual Con-\nference of the European Association for Machine\nTranslation: Projects/Products, Riga, Latvia. Baltic\nJournal of Modern Computing.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021.mT5: A massively\nmultilingual pre-trained text-to-text transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 483‚Äì498, Online. Association for Computa-\ntional Linguistics.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and\nYejin Choi. 2018. SWAG: A large-scale adversar-\nial dataset for grounded commonsense inference.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n93‚Äì104, Brussels, Belgium. Association for Compu-\ntational Linguistics.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020.Bertscore: Eval-\nuating text generation with BERT. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\n1286\nAppendix\nA Details for Dataset Construction\nBefore we start the translation procedure, we Ô¨Årst\nre-split the datasets of CSQA and CODAH such\nthat the test set examples in the English language\ndo not contain controversial examples or culture-\nrelated examples that would potentially cause cul-\ntural bias in our dataset. Please refer to the section\nof Ethical Considerations (following the Conclu-\nsion) in the main paper for more details. Then, we\nuse the DeepL Pro translation service to translate\nthe 10 languages: {de, fr, es, pt, it, nl, pl, ru, jap,\nzh} and use Google Translation API to translate\nthe others {ar, sw, ur, vi, hi}.\nWe agree that ideally we should use human ex-\nperts to translate the examples in CSQA and CO-\nDAH, but the cost or building a large-scale multi-\nlingual dataset with the same scale of our datasets\nis extremely high ‚Äì around 10k USD. As a mat-\nter of fact, most of the examples in CSQA and\nCODAH are very easy and short sentences, and\nmost of them can be well translated by modern\ncommercial translation APIs, because they usually\nhave a hybrid system. Moreover, we choose the\nDeepL online service because it has been reported\nby many individual media as the best choice. To\nensure the quality of the translation, we perform\nthe translation for both directions and then use\nthe same quality control method as we discussed\nin Section4 for removing the examples that have\nlower cosine similarity between original English\nversion and back-translated examples. During the\nprocess, we manually went through the Chinese\nversions to Ô¨Ånd a suitable threshold for taking the\nintersection ‚Äî 0.85, which results in a comparable\nBT-cosine mean to the XNLI dataset6.\nModels #lgs tnz L Hm Hff A V #para\nmBERT 104 WP 12 768 3072 12 110k 172M\nXLM-100100 BPE 16 1280 5120 16 200k 570M\nXLM-RB 100 SPM 12 768 3072 12 250k 270M\nXLM-RL 100 SPM 24 1024 4096 16 250k 550M\nTable 4: Model Architectures.\nB Hyper-parameters\nWe summarize hyper-parameters that we used for\ntraining ML-LMs on X-CODAH and X-CSQA in\n6We sampled 1k examples in the test set and follow the\nsame procedure for the intersection language set.\nTable 7. Evaluation Steps are equally 100 for all\nmodels and datasets.Maximum Sequence Length\nis 100 for X-CODAH and 64 for X-CSQA. The\nbatch size here refers to ‚Äútrain batch size per de-\nvice ‚á• # GPUs‚á• # gradient accumulation steps‚Äù.\nNote that the MCP methods use the exactly the\nsame hyper-parameters which we have found op-\ntimal by tuning over the dev set. The learning rates\nwe tried for all models are from the range {3e-5,\n2e-5, 1e-5, 8e-6, 6e-6, 5e-6}. The warm up steps\nare selected from {50, 100, 200, 300, 500}.\nC Details of ML-LMs\nTable 4 shows the model architectures and sizes\nthat we used from (Conneau et al., 2020). We\nshow the tokenization (tnz) used by each Trans-\nformer model, the number of layersL, the number\nof hidden states of the model Hm, the dimension\nof the feed-forward layer Hff , the number of at-\ntention headsA, the size of the vocabularyV and\nthe total number of parameters #params.\nD Additional Experimental Results\nD.1 Hit@1 Accuracy in Histogram\nD.2 Hit@k Accuracy of Mickey Probes\nTable5 shows the Hit@2 Accuracy of the Ô¨Åve ML-\nLMs for theMickeyProbe. Hit@2 Accuracy eval-\nuates whether the models can rank the correct as-\nsertion within top 2. Unlike Hit@1 which only\naccepts best predictions, Hit@2 is more Ô¨Çexible.\nThus, the performances in Hit@2 increase com-\npared to the ones in Hit@1. We can see that the\ndiscrepancies across languages still exist.\nD.3 Categorized X-CODAH Analysis\nPlease refer the CODAH (Chen et al., 2019) pa-\nper for the deÔ¨Ånition and concrete examples in\neach category. We show benchmark results of\nMCP(XLM-RL) on X-CODAH within different\ncarriages in Table6. The RB stands for using the\nRoBERTa-Large model to Ô¨Åne-tune on the English\nX-CODAH dataset. We Ô¨Ånd that the largest gaps\nin En are in the Idioms and the Others. Interest-\ningly, we Ô¨Ånd that the quantities category is where\nMCP performs better than the RoBERTa large.\n1287\nFigure 6: The MICKEY PROBE results in hit@1-acc. (An enlarged version of Figure3.)\nModels \\L en de it es fr nl ru bg vi zh hi avg\nShortest 42.20 50.91 52.49 56.06 57.30 55.95 40.96 45.86 35.64 47.67 43.81 48.08\nd-mBERT 87.06 61.48 47.70 62.30 76.17 59.03 45.71 55.47 42.53 60.24 52.56 59.11\nmBERT 87.38 62.30 52.02 73.01 70.41 62.42 56.83 62.34 49.77 53.81 53.99 62.21\nXLM-100 85.17 63.96 47.05 71.61 55.99 63.14 58.73 65.89 50.29 60.53 58.08 61.86\nXLM-RB 97.77 83.64 78.21 84.73 72.77 84.08 74.04 71.67 68.79 77.89 68.27 78.35\nXLM-RL 97.83 85.57 76.73 85.56 83.71 86.09 77.74 72.55 72.01 81.32 70.78 80.90\nTable 5: The hit@2 accuracy of the Ô¨Åve ML-LMs for the Mickey Probe task.\nCategoryRB en de it es fr nl ru vi zh hi pl ar ja pt sw ur avg\nIdioms 79.52 69.8861.45 56.63 60.24 73.49 60.24 57.83 50.6 55.42 45.7859.04 50.6 50.6 56.63 44.58 40.9655.87Neg. 75.61 75.6165.85 65.85 70.73 70.73 58.54 70.73 65.85 70.73 63.4165.85 60.98 58.54 70.73 41.46 58.5464.63Poly. 79.17 75.0058.33 66.67 68.75 70.83 60.42 66.67 68.75 56.25 54.1760.42 45.83 66.67 68.75 45.83 5061.46Ref. 86.49 78.3862.16 67.57 67.57 64.86 64.86 67.57 62.16 54.05 67.5772.97 75.68 45.95 54.05 62.16 56.7664.02Quant. 61.29 67.7445.16 45.16 51.61 54.84 61.29 51.61 61.29 45.16 54.8458.06 41.94 41.94 54.84 51.61 51.6152.42Others 82.89 68.9561.05 62.37 59.74 59.08 60.66 57.37 63.03 63.55 53.2957.89 54.08 55.13 60.79 43.55 47.558.00\nTable 6: Benchmark results for MCP(XLM-R-L) on X-CODAH in different categories. RB = RoBERTa-Large.\nModel lr # epoch # wus bsz\nX-CODAH\nmBERT 3E-05 20 100 128\nXLM-100 1E-05 20 100 64\nXLM-R-B 1E-05 20 100 128\nXLM-R-L 6E-06 10 100 64\nMCP(XLM-R-B)1E-05 20 100 128\nMCP(XLM-R-L)6E-06 10 100 64\nX-CSQA\nmBERT 3E-05 30 100 64\nXLM-100 1E-05 20 300 64\nXLM-R-B 1E-05 30 100 144\nXLM-R-L 6E-06 10 100 64\nMCP(XLM-R-B)1E-05 30 100 144\nMCP(XLM-R-L)6E-06 10 100 64\nTable 7: The optimal hyper-parameters for Ô¨Åne-tuning.\n(lr represents ‚Äòlearning rate‚Äô; training# epoch; # wus\n= ‚Äò# warm up steps‚Äô;bsz = ‚Äòbatch size‚Äô)",
  "topic": "Commonsense reasoning",
  "concepts": [
    {
      "name": "Commonsense reasoning",
      "score": 0.7429032325744629
    },
    {
      "name": "Computer science",
      "score": 0.6818262934684753
    },
    {
      "name": "Computational linguistics",
      "score": 0.5960784554481506
    },
    {
      "name": "Natural language processing",
      "score": 0.5839523673057556
    },
    {
      "name": "Linguistics",
      "score": 0.4971025288105011
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.4957233667373657
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49298426508903503
    },
    {
      "name": "Joint (building)",
      "score": 0.468833863735199
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.4226382076740265
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.153714120388031
    },
    {
      "name": "Philosophy",
      "score": 0.15338391065597534
    },
    {
      "name": "Engineering",
      "score": 0.12135624885559082
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Architectural engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}