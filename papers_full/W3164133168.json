{
  "title": "Unit Test Case Generation with Transformers and Focal Context",
  "url": "https://openalex.org/W3164133168",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5020154435",
      "name": "Michele Tufano",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5059021264",
      "name": "Dawn Drain",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5105672745",
      "name": "A. Svyatkovskiy",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5004928013",
      "name": "Shao Kun Deng",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5085284883",
      "name": "Neel Sundaresan",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2959222533",
    "https://openalex.org/W2395052532",
    "https://openalex.org/W2470841477",
    "https://openalex.org/W2882984136",
    "https://openalex.org/W2077273779",
    "https://openalex.org/W1977080779",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2072275383",
    "https://openalex.org/W2972082064",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W1964730672",
    "https://openalex.org/W1971650562",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2973529529",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1965194038",
    "https://openalex.org/W2156723666",
    "https://openalex.org/W2907705732",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W1505962635",
    "https://openalex.org/W3100026183",
    "https://openalex.org/W3101506519",
    "https://openalex.org/W2514713644",
    "https://openalex.org/W2884276923",
    "https://openalex.org/W2736762043",
    "https://openalex.org/W3105903381",
    "https://openalex.org/W2682664750",
    "https://openalex.org/W2962027130",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Automated unit test case generation tools facilitate test-driven development and support developers by suggesting tests intended to identify flaws in their code. Existing approaches are usually guided by the test coverage criteria, generating synthetic test cases that are often difficult for developers to read or understand. In this paper we propose AthenaTest, an approach that aims to generate unit test cases by learning from real-world focal methods and developer-written testcases. We formulate unit test case generation as a sequence-to-sequence learning task, adopting a two-step training procedure consisting of denoising pretraining on a large unsupervised Java corpus, and supervised finetuning for a downstream translation task of generating unit tests. We investigate the impact of natural language and source code pretraining, as well as the focal context information surrounding the focal method. Both techniques provide improvements in terms of validation loss, with pretraining yielding 25% relative improvement and focal context providing additional 11.1% improvement. We also introduce Methods2Test, the largest publicly available supervised parallel corpus of unit test case methods and corresponding focal methods in Java, which comprises 780K test cases mined from 91K open-source repositories from GitHub. We evaluate AthenaTest on five defects4j projects, generating 25K passing test cases covering 43.7% of the focal methods with only 30 attempts. We execute the test cases, collect test coverage information, and compare them with test cases generated by EvoSuite and GPT-3, finding that our approach outperforms GPT-3 and has comparable coverage w.r.t. EvoSuite. Finally, we survey professional developers on their preference in terms of readability, understandability, and testing effectiveness of the generated tests, showing overwhelmingly preference towards AthenaTest.",
  "full_text": "1\nUnit Test Case Generation with Transformers\nand Focal Context\nMichele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, Neel Sundaresan\nAbstract—Software testing is a critical part of software development life cycle which helps identify potential regressions and reduce\nmaintenance costs, yet it is often neglected by developers. Automated unit test case generation tools facilitate test-driven development\nand support developers by suggesting tests intended to identify ﬂaws in their code. Existing approaches are usually guided by the test\ncoverage criteria, generating synthetic test cases that are often difﬁcult for developers to read or understand. In this paper we propose\nATHENA TEST , an approach that aims to generate unit test cases by learning from real-world focal methods and developer-written test\ncases. We formulate unit test case generation as a sequence-to-sequence learning task, adopting a two-step training procedure\nconsisting of denoising pretraining on a large unsupervised Java corpus, and supervised ﬁnetuning for a downstream translation task of\ngenerating unit tests. We investigate the impact of natural language and source code pretraining, as well as the focal context information\nsurrounding the focal method. We found that both techniques provide improvements in terms of validation loss, with pretraining yielding\n25% relative improvement and focal context providing additional 11.1% improvement. We also introduceMETHODS 2TEST , the largest\npublicly available supervised parallel corpus of unit test case methods and corresponding focal methods in Java, which comprises 780K\ntest cases mined from 91K open-source repositories hosted on GitHub. We evaluate ATHENA TEST on ﬁve defects4j projects, generating\n∼25K passing test cases covering 43.7% of the focal methods with only 30 attempts. We execute the test cases, collect test coverage\ninformation, and compare them with test cases generated by EvoSuite and GPT -3, ﬁnding that our approach outperforms GPT -3 and has\ncomparable coverage w.r.t. EvoSuite. Finally, we survey professional developers on their preference in terms of readability,\nunderstandability, and testing effectiveness of the generated test cases. The results show that developers overwhelmingly prefer test\ncases generated by ATHENA TEST over EvoSuite’s, suggesting that our approach could signiﬁcantly improve developer productivity.\nIndex Terms—Automated Software Testing, Deep Learning\n!\n1 I NTRODUCTION\nSoftware testing is widely acknowledged as one of the most\ncritical, challenging, and expensive phases of the software\ndevelopment lifecycle. Technology companies are constantly\nlooking into ways to deliver their software faster, without\nsacriﬁcing its quality and correctness. To succeed, these\ncompanies often rely on continuous integration and delivery\nof software, which allows for fast and reliable deployment of\nsoftware into production. In this context, automated testing\nrepresents a fundamental piece of the pipeline, providing\ndevelopers with the conﬁdence they need to iterate quickly,\nand integrate new features without regressions.\nUnit testing lays as the foundational basis of the testing\npyramid, beneath integration and end-to-end testing [1]. This\nprominent visual metaphor intends to provide a guidance\non the adequate amount of effort that should be allocated\nfor each of the testing layers. Thus, the largest amount of\ntests should be at the unit test layer, where individual units\nof software (e.g., a single method) are tested in isolation to\nensure that they behave as intended.\nUnit Test frameworks, such as JUnit [2], offer an en-\nvironment and APIs that facilitate writing and executing\nrepeatable test cases. JUnit provides methods such as asser-\ntions which support the developers in checking conditions,\noutputs, or states in a software program, assessing its\n• M. Tufano, D. Drain, A. Svyatkovskiy, S. K. Deng, N. Sundaresan are\nwith Microsoft, Redmond, WA, USA.\nE-mail: {mitufano, dadrain, alsvyatk, shade, neels}@microsoft.com\nexpected behavior. Several other frameworks have been built\non top of JUnit, such as Cactus [3] and TestnNG [4]. Others\ncan be integrated with JUnit to support different scenarios\nor testing methodologies, such as Mockito [5], which allows\nmocking of objects by replacing functionalities with dummy\nimplementations that emulate real code, focusing the testing\non the method under test.\nOn top of these frameworks, researchers have proposed\nseveral techniques that aim to automate the generation of\nunit test cases. EvoSuite [6], Randoop [7], and Agitar [8] are\namong the most popular and widely used examples of such\ntechniques. EvoSuite relies on an evolutionary approach\nbased on a genetic algorithm to generate unit test cases,\ntargeting code coverage criteria such as branch and line\ncoverage. Speciﬁcally, it introduces mutants ( i.e., modiﬁed\nversions of methods or classes under test) and iteratively\ngenerates assert statements to kill such mutants. During\nthis process, EvoSuite minimizes the number of asserts\nwhile trying to maximize the number of detected mutants.\nRandoop is a different automated test generation tool that\nrelies on feedback-directed random testing, a technique\nthat uses execution traces to guide the selection of method\nsequences which are then checked against a set of user-\nspeciﬁed contracts ( i.e., user-speciﬁed program logic).\nA major weakness and criticism of these approaches\nis related to the poor readability and understandability of\nthe generated test cases [9], [10], which clearly appear as\nmachine-generated code. Other studies have highlighted\ndifferent limitations of these automation tools, such as\nunsatisfactory code quality [11]–[13], poor fault-detection\narXiv:2009.05617v2  [cs.SE]  20 May 2021\n2\nJava Repositories\nData Collection\nTest Cases\nFocal Methods\nMapping\nModel Finetuning\nDECODERENCODER\nBART Transformer\nEnglish\nPretraining\nCode\nPretraining\nInput\nExpected output\nDECODERENCODER\nModel Pretraining\nTest Case\nMining\nFocal Method\nMapping\nFig. 1: Overview of A THENA TEST – We mine test cases from GitHub and map them to the corresponding focal methods,\nwhich we collect in M ETHODS 2TEST , then pretrain a BART Transformer model on both English and Source Code corpora,\nﬁnally we ﬁnetune the model on the unit test case generation task.\ncapability [14], and the inability to adequately meet the soft-\nware testing needs of industrial developers [15], [16]. These\nlimitations stem from the fact that these approaches mainly\nfocus on code coverage as unique objective, disregarding\nother factors that may be relevant for developers.\nDeep learning techniques have shown the potential of\nlearning from real-world examples, and have been employed\nin several software engineering tasks, such as code comple-\ntion [17], automated patch generation [18], [19], comment\ngeneration [20], and many others [21]. Recent advancements\nin transformer models, such as OpenAI’s GPT-3 [22], have\nmade headlines and shown impressive results in realistic text\ngeneration and question answering tasks.\nIn this paper, we present an approach that aims to\nlearn from developer-written test cases how to generate\ncorrect and readable tests.Our approach relies on a large\nsequence-to-sequence transformer model pretrained both on\nEnglish and Java source code, then ﬁnetuned on the task of\ngenerating unit test cases. For this task, we mine thousands\nof real-world test cases and map them to the corresponding\nfocal methods, then use this parallel corpus for training and\nevaluation.\nTo summarize, our contributions are as follows:\n• ATHENA TEST : an automated test case generation ap-\nproach based on a sequence-to-sequence transformer\nmodel. The approach is able to generate thousands\nof syntactically correct, compilable, and passing test\ncases for Defects4j projects, that invoke a variety of\ntesting APIs. The generated test cases have comparable\ntest coverage w.r.t. EvoSuite and they are preferred\nby professional developers in terms of readability, un-\nderstandability, and testing effectiveness. These test\ncases appear to be: (i) realistic – similar to developer-\nwritten test cases; (ii) accurate – correctly asserting the\nexpected behavior of a focal method; (iii) human-readable\n– readable and understandable code, with good variable\nand method names.\n• METHODS 2TEST : the largest publicly available1 parallel\ncorpus of test cases mapped to the corresponding focal\nmethods [23]. This dataset enlists 780K mapped test\ncases, extracted from 91K open source Java projects.\n2 A PPROACH\nFigure 1 provides an overview of our approach. Starting with\na dataset of Java open-source projects obtained from GitHub,\n1. https://github.com/microsoft/methods2test\nwe mine test cases and map them to the corresponding\nfocal methods (Sec. 2.1). Next, we consider a transformer\nmodel (Sec. 2.2), which has been pretrained on English and\nsource code corpora (Sec. 2.3), select the best focal context\nsurrounding the focal method (Sec. 2.4), and ﬁnetune for the\ntask of generating unit test cases (Sec. 2.5).\n2.1 Data Collection\nThe goal of this stage is to mine test cases and their\ncorresponding focal methods (i.e., the method tested by the\ntest case) from a set of Java projects. We select a 91K sample\nof all the public GitHub Java repositories declaring an open\nsource license, which have been updated within the last ﬁve\nyears, and are not forks.\nFirst, we parse each project to obtain classes and methods\nwith their associated metadata. Next, we identify each test\nclass and its corresponding focal class. Finally, for each test\ncase within a test class, we map it to the related focal method\nobtaining a set of mapped test cases.\nParsing\nWe parse each project under analysis with thetree-sitter\nparser [24]. During the parsing, we automatically collect\nmetadata associated with the classes and methods identiﬁed\nwithin the project. Speciﬁcally, we extract information such as\nmethod and class names, signatures, bodies, annotations, and\nvariables. The parsed code will be used to identify test cases\nand corresponding focal methods, as well as augmenting the\nfocal methods with focal context.\nFind Test Classes\nIn this stage, we identify all the test classes, which are classes\nthat contain a test case. To do so, we mark a class as a\ntest class if it contains at least one method with the @Test\nannotation. This annotation informs JUnit that the method\nto which it is attached can be run as a test case.\nFind Focal Classes\nFor each test class we aim to identify the focal class which\nrepresents the class under test. To this aim, we employ the\nfollowing two heuristics, in sequence:\n• Path Matching : best practices for JUnit testing sug-\ngests placing code and corresponding test cases\nin mirrored folder structure. Speciﬁcally, given the\nclass src/main/java/Foo.java the corresponding\n3\nJUnit test cases should be placed in the class\nsrc/test/java/FooTest.java. Our ﬁrst heuristic\ntries to identify the folder where the focal class is deﬁned,\nby following the path of the test class but starting with\nthe src/main folder (i.e., production code).\n• Name Matching : the name of a test class is usually\ncomposed of the name of the focal class, along with\na \"Test\" preﬁx or sufﬁx. For example, the test case\nfor the class Foo.java would probably be named\nFooTest.java. Thus, following the path matching\nheuristic, we perform name matching to identify the\nfocal class by matching the name of the test case without\nthe (optional) \"Test\" preﬁx/sufﬁx.\nFind Focal Method\nFor each test case ( i.e., method within a test class with the\n@Test annotation) we attempt to identify the corresponding\nfocal method within the focal class. To this aim, we employ\nthe following heuristics:\n• Name Matching : following the best practices for nam-\ning classes, test case names are often similar to the\ncorresponding focal methods. Thus, the ﬁrst heuristic\nattempts to match the test cases with a focal method\nhaving a name that matches, after removing possible\nTest preﬁx/sufﬁx.\n• Unique Method Call : if the previous heuristic did not\nidentify any focal method, we compute the intersection\nbetween (i) the list of method invocations within the\ntest case and (ii) the list of methods deﬁned within the\nfocal class. If the intersection yields a unique method,\nthen we select the method as the focal method. The\nrationale behind this approach is as follows: since we\nhave already matched the test class with the focal class\n(with very high conﬁdence heuristics), if the test case\ninvokes a single method within that focal class, it is very\nlikely testing that single method.\nMapped Test Cases\nThe result of the data collection phase is a set of mapped test\ncases, where each test case is mapped to the corresponding\nfocal method. It is important to note that we discard test\ncases for which we were not able to identify the focal method\nusing our heuristics. We designed these heuristics to be based\non testing best practices, and obtain a correct mapping with\nvery high conﬁdence. This allows us to train our model on\ntest cases that follow best practices, and likely excluding test\ncases that have been automatically generated.\nWe collect an initial set of 887,646 mapped test case pairs.\nFrom this set, we exclude duplicates, remaining with a total\nof 780,944 unique mapped test case pairs. Next, we split\nthe dataset into training ( ∼80% - 624,022 pairs), validation\n(∼10% - 78,534 pairs), and test ( ∼10% - 78,388 pairs) sets. We\nperformed this split by carefully taking into account possible\ndata leakage. Speciﬁcally, during the split we enforce the\nconstraint that any two data points belonging to the same\nrepository cannot be placed in two different sets ( e.g., one\nin training and the other in test). That is, all the data points\nbelonging to the same repository will be placed in the same\nset.\nTable 1 reports the details of the dataset split, with\nnumber of repositories and mapped test cases. The set of\nTABLE 1: METHODS 2TEST Dataset\nSet Repositories Mapped Test Cases\nTraining 72,188 624,022\nValidation 9,104 78,534\nTest 10,093 78,388\nTotal 91,385 780,944\nmapped test cases will be used to train our model to generate\na test case given the focal method. We publicly release the\ndataset METHODS 2TEST [23].\n2.2 BART Transformer\nATHENA TEST is based on a BART transformer model.\nBART [25] is a denoising autoencoder which utilizes the\nstandard sequence-to-sequence transformer architecture\nfrom [26], substituting ReLUs with GeLU activation func-\ntions.\nWe select the BART model architecture because it fa-\ncilitates ﬁnetuning for the downstream translation task of\ntest case generation, providing a more advanced set of\nnoising transformations, which include token masking, token\ndeletion, inﬁlling and statement permutation. The model is\npretrained by corrupting documents and optimizing the\ncross-entropy loss between the decoder’s output and the\noriginal input sequence.\nWe pretrain the BART large model architecture, which has\n12 encoder layers and 12 decoder layers. The model is trained\nin mixed-precision, using Adam stochastic optimization\nprocedure with ϵ= 10−6, and β1 = 0.9, β2 = 0.98 optimizer\nparameters; we apply inverse square root learning rate\nschedule with the base learning rate of 0.0001, a warmup\nperiod of 5000 update steps, and local gradient accumulation\nwith a frequency of 4 update steps.\n2.3 Pretraining\nWe employ two pretraining stages: English Pretraining,\nwhere we perform semi-supervised pretraining on a large\ncorpus of English text, and Code Pretraining, where the\nmodel is pretrained on Java source code.\nEnglish Pretraining\nIn this stage we pretrain a model in a semi-supervised fashion\non a large corpus of English text, with the goal of learning\nsemantic and statistical properties of natural language. The\npretraining is performed for 40 epochs on 160GB of English\ntext extracted from books, Wikipedia, and news articles [27].\nBART is trained in an unsupervised manner. Given\ncorrupted text, its objective is to reconstruct the original\ntext. The particular type of noise used in this work involves\nmasking 30% of all tokens, with masks covering spans\nof tokens with lengths following a Poisson distribution\nparameterized by λ= 3, as well as permuting all sentences.\nCode Pretraining\nIn this stage we pretrain a model on source code corpus\nwritten in Java language, with the goal of learning syntax\nand properties of source code.\n4\nWe collect this code corpus dataset by crawling all public,\nnon-fork Java repositories on GitHub with at least 50 stars.\nWe then deduplicate at the ﬁle-level using a hash function.\nAfter ﬁltering for permissive licenses and ﬁltering out based\non heuristics like the fraction of non-ASCII characters, we\nare left with 25GB of training data from the 26,000 reposi-\ntories. For pretraining validation, we use the 239 test Java\nrepositories from the CodeSearchNet [28], which comprise\n600MB.\nA similar pretraining strategy to English pretraining is\nemployed. The source code ﬁles are corrupted by deleting\n20% of all tokens independently and rotating half of all\ndocuments. This pretraining is performed for 10 epochs.\nModel Pretraining Variants\nAt the end of these stages, we obtain four different variants\nof the model, based on the level of pretraining performed:\n• BART_Scratch: a model which has not been pretrained\non any corpus but directly ﬁnetuned on the test case\ngeneration task.\n• BART_English: a model which has been pretrained on\nthe English corpus and then ﬁnetuned for the test case\ngeneration task.\n• BART_Code: a model pretrained on the source code\ncorpus, then ﬁnetuned on the test case generation task.\n• BART_English+Code: a model pretrained ﬁrst on English\nand further pretrained on source code corpus, then\nﬁnetuned on the test case generation task.\n2.4 Focal Context\nIn this section we describe the code representation we build\nfor the input to the model. The goal of this phase is to\nconstruct an input which contains the necessary information\nthat the model can leverage to generate correct and useful\ntest cases. Intuitively, the focal method (i.e., the method under\ntest) represents the core information to feed to the model.\nHowever, additional contextual information can provide\nimportant clues for the model to better understand the focal\nmethod nature and its context, improving the likelihood of\ngenerating test cases that compile and properly test the focal\nmethod.\nWe build different versions of the code input representa-\ntion – with diverse degree of focal context – with the aim of\nempirically evaluating these code representations. We begin\nwith the core information (i.e., focal method) and iteratively\nadd contextual information such as class name, constructors,\nother method signatures, and ﬁelds.\nFigure 2 provides an overview of the different levels\nof context we generate for the focal method add in the\nCalculator class. The left side corresponds to the textual\nrepresentation, while the right side delineates the context\nwhich is indicated with a focal context ID, which we describe\nin the following:\n• fm: this representation incorporates exclusively the\nsource code of the focal method. Intuitively, this contains\nthe most important information for generating accurate\ntest cases for the given method.\n• fm+fc: this representations adds the focal class name,\nwhich can provide meaningful semantic information to\nthe model.\nFig. 2: Focal Context\nfm +fc +c +m +f\n• fm+fc+c: this representation adds the signatures of the\nconstructor methods of the focal class. The idea behind\nthis augmentation is that the test case may require\ninstantiating an object of the focal class in order to\nproperly test the focal method.\n• fm+fc+c+m: this representation adds the signatures of\nthe other public methods in the focal class. The rationale\nwhich motivated this inclusion is that the test case may\nneed to invoke other auxiliary methods within the class\n(e.g., getters, setters) to set up or tear down the testing\nenvironment.\n• fm+fc+c+m+f : this representation adds the public ﬁelds\nof the focal class. The motivation is that test cases may\nneed to inspect the status of the public ﬁelds to properly\ntest a focal method.\nWhile constructing these representations we face two\nopposing goals: (i) include as many tokens as possible, given\ntheir powerful expressiveness discussed above (ii) keep a\nconcise representation that ﬁts into GPU memory.\nIntuitively, having a representation that includes many\ntokens from the focal context allows the model to attend to\ndifferent parts of the input and leverage these information\nto generate a correct and meaningful test case. On the other\nhand, irrelevant tokens could represent noise for the learning\nprocess, which could lead to worse performances, as well as\nwasting GPU memory that could be use for more informative\ntokens.\nIt is important to highlight that in our representation,\nthe order of inclusion of a particular focal context, for\nexample the constructors’ signatures (fm+fc+c) before other\nmethods’ signatures (fm+fc+c+m), is important, since the\ntextual representation could be truncated if it exceeds 1024\ntokens (i.e., maximum sequence length in our model).\nThis order of inclusion has been deﬁned by the authors\nbased on their understanding and intuition of the meaningful\nclues for test case generation within the focal class. We\nempirically evaluate these design decision in our empirical\nstudy.\nModel Context Variants\nSimilarly to what has been discussed for model pretraining,\nwe train different model variants for each level of focal\n5\ncontext. Speciﬁcally, we obtain ﬁve different models which\nwe refer to with the corresponding focal context ID.\n2.5 Finetuning\nIn this stage we ﬁnetune a model on the task of generating\nunit test cases for a given method. Speciﬁcally, we represent\nthis task as a translation task, where the source is a focal\nmethod ( i.e., the method we would like to test), and the\ntarget is the corresponding test case originally written by a\nsoftware developer.\nThe ﬁnetuning training is performed using the collected\nmapped test cases (Sec. 2.1), where a mapped test case\nmtci can be seen as a pair mtci = {tci,fmi}comprising\nthe test case tci and the corresponding focal method fmi.\nThe ﬁnetuning process is a translation task, with a training\nobjective to learn the mapping fmi →tci as a conditional\nprobability P(tci|fmi). Note that we refer with fmi to the\nfocal method and its available focal context, depending on\nthe model variant.\nDuring training, we use the cross entropy loss and the\nAdam optimizer, monitoring the loss on the validation set\nfor early stopping. We use shared vocabulary embeddings\nbetween Encoder and Decoder for optimization reasons [26],\n[29] and because our input and output language is the same\n(i.e., Java source code).\n3 E XPERIMENTAL DESIGN\nThe goal of our empirical study is to determine if our\napproach can generate accurate and useful unit test case\ngiven a method. Our experiments aim at answering the\nresearch questions described in the following paragraphs.\nWe begin by evaluating the impact of English and code\npretraining to select our base model (RQ 1), next we consider\ndifferent levels of focal context and select the best model for\nunit test generation (RQ2). ATHENA TEST is then evaluated in\nterms of code-speciﬁc metrics for the test cases it generates\n(RQ3) and a large-scale analysis is performed on Defects4j\nprojects (RQ4). Finally, we compare ATHENA TEST against\nEvoSuite and GPT-3 in terms of code coverage (RQ 5) and in\nterms of developers’ preferences (RQ6).\nRQ1: Does model pretraining impact the performances\nof Unit Test Case Generation?As a ﬁrst step towards the\ncreation of a unit test generation model, we intend to select\na base model that we will specialize on our downstream task.\nThe available options (described in Sec. 2.3) include a scratch\nmodel (randomly initialized, with no pretraining), and model\nvariants with English and/or code pretraining.\nIn this research question we aim at evaluating the impact\nof the pretraining process on the performances of our\ndownstream task. With this aim, we ﬁnetune the four model\nvariants on the unit test generation task, letting the models\nconverge independently till no major improvements on the\nvalidation loss is observed, and for a maximum of 50k steps.\nThe ﬁntuning at this stage is performed using the minimal\nlevel of focal context ( fm) for all the model variants, since\nwe are only interested in observing the pretraining effect at\nthis point.\nWe evaluate the models by observing the validation loss\nduring model training. A low validation loss means the\nmodel is effectively learning meaningful representations\nduring training and is able to generalize how to generate\ntest cases on a different set of input methods ( i.e., validation\nset). Speciﬁcally, we analyze three key metrics: (i) the initial\nvalidation loss during ﬁnetuning, which indicates the impact\nof the pretraining process; (ii) the best validation loss, which\nhighlights the model achieving the best performance; (iii) the\nnumber of steps needed to reach the best validation loss, as\na measure of how fast the ﬁnetuning process converges.\nAt the end of this experiment, we select the model with\nthe best validation loss, which will be used for further\ninvestigation in the following research questions.\nRQ2: How does focal context impact the training for\nUnit Test Case Generation?\nIn this research question we aim at empirically evaluating\nthe impact of the focal context to the performances of our\nmodels on the unit test case generation task. Speciﬁcally, the\ngoal is to quantify the effect of each level of focal context,\nwhich we add incrementally starting from the focal method.\nTo do so, we perform a preliminary token-based analysis as\nwell as validation loss comparison among the model variants.\nIngredient Space Analysis\nUnit test cases may contain tokens that are shared with\nthe focal context, such as variable names, method calls,\nliterals, and so on. We refer to such tokens as ingredients\nthat can be selected from the focal context to build a test\ncase candidate. This metaphor has also been used in the\nliterature to characterize tokens necessary to perform bug-\nﬁxing activities [30], [31].\nIn order to understand whether different levels of focal\ncontext provide possibly more ingredients that the model can\nleverage to generate a test case, we perform an ingredient\nspace analysis. Speciﬁcally, given a focal method fm, its\ncorresponding ﬁve different levels of focal context ( i.e., fm,\nfm+fc, fm+fc+c, fm+fc+c+m, fm+fc+c+m+f ), and the target test\ncase tc, we compute the overlap between the set of tokens\nin the tc and each of the focal context. During this process\nwe properly tokenize the source code and disregard Java\nkeywords and separators. We compare the distributions of\nnumber of shared tokens over the training set for the ﬁve\nvariants of focal context.\nValidation Loss\nWhile a token-based analysis can provide meaningful ev-\nidence to support the decision to incorporate a particular\nfocal context, such an analysis is limited in its nature, since\nit requires perfect token matching. On the other hand, some\ntokens carry signiﬁcant semantic value that can provide\npowerful clues to the generation of test cases, even whew\nsuch token does not appear in the test case. For example, the\nname of the focal class Calculator provides to the model\nthe domain where the focal method belongs to, even in the\ncase that the token Calculator is never used in the actual\ntest case.\nFor this reason, we complement the ingredient space\nanalysis with a validation loss analysis, where we train ﬁve\nmodels to generate test cases, each of them taking as input a\ndifferent version of focal context. Note that in this experiment,\nwhile the input source is different, the output target is the\nsame and consistent among the variants. The training is\n6\nperformed starting, for all the variants, from the pretrained\nmodel that achieved the best results in the ﬁrst research\nquestion.\nAt the end of this research question, the results of the\nanalyses will inform us on the variant of focal context that\nmakes the best use of the limited token window ( i.e., 1024\ntokens). This model is the ﬁnal model for ATHENA TEST\nwhich will be deployed and evaluated in the subsequent\nresearch questions.\nRQ3: What is the quality of the generated Test Cases?\nIn this research question we further analyze the test cases\ngenerated by the model selected in RQ 2. The focus of this\nanalysis is to scrutinize the generated model’s predictions\nlooking for speciﬁc properties that unit test cases should\nhave.\nSyntactic Correctness\nWe begin by verifying that the sequence of tokens generated\nby the model represents a syntactically correct source code\nmethod conforming to the Java speciﬁcations. To this aim,\nwe parse all the predictions generated by the model using a\nJava parser, which determines the syntactic correctness.\nTesting APIs\nFor a method to be considered as a test case, it needs to\nexhibit some basic properties, such as:\n• Test Annotation: the test case should declare the @Test\nannotation.\n• Focal Method Invocation: to properly test a focal method,\nthe test case should invoke the focal method.\n• Testing APIs : the test case should check the proper\nbehavior of the focal method using testing APIs, such\nas assert statements and mocking methods. Speciﬁcally,\nwe consider two testing framework APIs: JUnit Assert\nAPIs (e.g., assertTrue, assertEqual) as well as the\nMockito Framework APIs ( e.g., mock, verify). We\nchose these testing framework for their popularity and\napplicability in many different contexts and domains.\nWe plan to incorporate more domain-speciﬁc testing\nframeworks, such as Selenium [32] or REST Assured [33]\nin future work.\nWe check compliance to these properties using a Java\nparser, extracting annotations and method calls. We also\ncompare the distribution of testing APIs between the original\ntest cases and the ones generated by the model.\nRQ4: Can ATHENA TEST generate Test Cases for De-\nfects4j projects?\nIn this research question we are interested in evaluating\nthe performances of ATHENA TEST on a widely common\nbenchmark dataset such as defects4j. We rely on defects4j\nsince it provides a reliable infrastructure to generate, compile,\nexecute, and evaluate test cases for several popular open\nsource software projects.\nThe goal of this research question is to understand the\nreal-world performance of our approach when used on large\nand complex systems. In particular, whether ATHENA TEST\nis able to generate test cases that are compilable, executable,\nand correct w.r.t. the given project and focal method.\nTable 2 lists the defects4j projects representing the scope\nof our empirical analysis. Speciﬁcally, we select ﬁve popular\nTABLE 2: Defects4j Projects Analyzed\nProject Revisions Focal Methods\nLang 63 2,712\nChart 26 1,328\nCli 38 645\nCsv 16 373\nGson 18 220\nTotal 161 5,278\nand commonly used projects: Apache Commons Lang [34],\nJFreeChart [35], Apache Common Cli [36], Apache Common\nCsv [37], Google Gson [38]. We selected these projects as\nrepresentative of different domains, sizes, and organizations.\nOur experimental design consists of three main phases:\n(i) generation; (ii) execution; and (iii) evaluation of the test\ncases.\nGeneration\nFor each project pand revision rev, we checkout the ﬁxed\nversion revf , since in our experimental scenario we are\ngenerating test cases assuming a correct project. Next, we\nidentify the focal class(es) fc (i.e., the class where the bug\nwas identiﬁed) for the speciﬁc rev using defects4j APIs.\nSubsequently, we parse the focal classes and extract the list\nof every public method. Each one of these public methods\nrepresents a focal method fm for which we aim to generate\ntest cases. For each focal methodfmwe invoke ATHENA TEST\nand generate 30 candidate test cases using beam search.\nExecution\nEach candidate test casetcis then injected into a test class that\ncontains the appropriate imports and scaffolding necessary\nto be executed for the particular project pand revision rev.\nNext, the test class is compressed into a format supported by\ndefects4j API and the test case is executed. During execution\nwe collect coverage information with Cobertura, which\nproduces an xml ﬁle specifying the lines and conditions\ncovered for each Java ﬁle and method.\nEvaluation\nAfter the execution phase, we perform the evaluation by\nanalyzing multiple output ﬁles and logs, including build\nlogs, test execution outputs, and coverage ﬁles.\nWe classify each candidate test case tc into these cate-\ngories:\n• Syntax Error: the test has syntax errors;\n• Build Error: the test has correct syntax but fails to build;\n• Failing Test : the test builds but fails due to wrong\nassertions or expected behavior;\n• Passing Test: the test builds and passes;\n• Correct Test: the test passes and covers the correct focal\nmethod;\nThe ﬁrst four categories are mutually exclusive, that is\na tc can either be classiﬁed in Syntax Error or Failed Build\nand so on, while the last category ( Correct Test) represents a\nmore stringent subset of the Passing Test one. Speciﬁcally, we\nconsider a test case tcto be correct only if it builds properly,\nexecutes without failing, and covers the correct focal method\ngiven as input.\n7\nWe report statistics for all the generated test cases and\ndeﬁned categories. Additionally, we report method-level\nstatistics considering the percentage of methods successfully\ntested, that is, those that have at least one correct test case\nout of the 30 candidates.\nIn total, we consider 5 projects, 161 different revisions,\nand generate test cases for 5,278 focal methods.\nRQ5: How does our approach compare to EvoSuite and\nGPT-3? The goal of this research question is to provide a\npreliminary quantitative and qualitative comparison between\nthe test cases generated by our model and those generated\nby two alternative approaches: EvoSuite and GPT-3. We\nchose these two approaches as representative of two different\nclasses of techniques: (i) evolutionary-based automated test\ncase generation; (ii) transformer-based language models.\nEvoSuite\nEvoSuite [6] is a widely known tool that automatically\ngenerates unit tests for Java software. EvoSuite uses an\nevolutionary algorithm to generate JUnit tests, targeting\ncode coverage criteria. Speciﬁcally, it introduces mutants and\niteratively generates assert statements to kill such mutants.\nDuring this process, EvoSuite minimizes the number of\nasserts while trying to maximize the number of detected\nmutants.\nGPT -3\nGenerative Pre-trained Transformer 3 (GPT-3) is an autore-\ngressive language model introduced by OpenAI [22]. GPT-\n3 is a transformer decoder-only architecture having 175\nbillion trainable parameters. It has been pre-trained on the\nCommon Crawl dataset [39] constituting nearly a trillion\nwords, an expanded version of the WebText [40] dataset,\ntwo internet-based books corpora (Books1 and Books2),\nand English-language Wikipedia. GPT-3 has demonstrated\nan impressive task-agnostic few-shot performance on text\ngeneration, translation and question-answering, as well as\ncloze tasks. The few-shot learning assumes an extended\ncontext supplied to the model during inference as a task\ndescription, and requires no gradient updates.\nExperiment’s Design\nIn this experiment, we aim at assessing two main qualities of\nthe generated test cases: (i) correctness – tests that accurately\nassert the behavior of the focal method; (ii) code coverage –\nnumber of lines and conditions covered by the test cases.\nFor this comparison we select a small but reproducible\ntestbed using defects4j [41]. We rely on defects4j since\nit provides a reliable infrastructure to generate, compile,\nexecute, and evaluate test cases. Speciﬁcally, we select Lang-\n1-f, which represents the ﬁxed version of the ﬁrst bug in\nthe defects4j collection belonging to the project Apache\nCommons Lang [34]. Note that these projects are not included\nin our pretraining or ﬁnetuning datasets. We generate unit\ntest cases for all the public methods of the class impacted\nby the bug, NumberUtils, using our model, EvoSuite, and\nGPT-3. Next, we compile and execute the test cases and\nmanually assess their correctness. Speciﬁcally, to be deﬁned\nas correct, the test case needs not only to be able to execute\nand pass, but also must specify at least one assert that is\nsemantically accurate w.r.t the focal method. Subsequently,\nwe compute test coverage using defects4j (which, in turn,\nrelies on Cobertura [42]) singularly for each unit test case\ngenerated by the three approaches.\nEvoSuite - Generation\nTo generate test cases with EvoSuite, we use the de-\nfects4j built-in command gen_tests.pl -g evosuite\n-p Lang -v 1f. This command invokes EvoSuite test\ngeneration on the ﬁrst ﬁxed revision of Lang, which\nwill generate test cases for the class affected by the bug\n(i.e., NumberUtils). We let EvoSuite generate test cases for\n500 seconds (∼8 minutes). Then, we test every unit test case\ngenerated and select the best test case for each focal method.\nGPT -3 - Generation\nTo generate test cases with GPT-3 we rely on few-shot\nlearning. Speciﬁcally, we provide two examples of input\nfocal method and corresponding test case taken from the\ntraining set, then feed one of the public methods in the\nNumberUtils class, and expect GPT-3 to answer with the\ncorresponding test case.\nWe use the OpenAI API and davinci-msft serving\nendpoint to perform inference on the model. We experiment\nwith two different sets of prompts ( i.e., focal methods and\ntest cases) from the supervised training set for our target\ndownstream task as conditioning, varying the sampling\ntemperature parameter from 0.1 to 0.9 with 0.1 increments\n(i.e., the higher the temperature, the more risky or creative\nare the outputs). We generate ten candidate output sequences\nfor each focal method, selecting the best test case for each\nfocal method. Note, we fall back to one-shot learning if the\nexamples and the current focal method exceed the maximum\nsequence length for GPT-3 (i.e., 2048 tokens), which happened\nonly once.\nATHENA TEST - Generation\nThe generation process is similar to what is described in the\nprevious RQ4, except that we select the best prediction from\nthe top-10 candidates, rather than the top-30.\nWe are aware that this represents only a small-scale\npreliminary evaluation, however, given the signiﬁcant man-\nual effort assessing the correctness, we believe this is an\nimportant ﬁrst step. We discuss this in the threats to validity\nsection.\nRQ6: Do developers prefer ATHENA TEST ’s test cases\nover EvoSuite’s?In this research question we aim at analyz-\ning the developer’s perspective and preferences regarding\ntest cases. In particular, we are interested in developers’\nview of different aspects of test cases, such as readability,\nunderstandability, and testing effectiveness.\nTo this aim, we designed a survey with developers where\nwe show them a focal method under test and two alternative\ntest cases: one generated with ATHENA TEST , and the other\nwith EvoSuite. We then pose the developers three questions,\nasking that they rely on their personal preferences when\nevaluating these factors:\n• Q1: Which test case is more readable and understandable?\n• Q2: Which test case is testing the method more appropri-\nately?\n8\nFig. 3: Pretraining Models - Validation Loss\nEnglish and Code pretraining provide positive effect\n5000 10000 15000 20000 25000 30000\nTraining Step\n2 × 100\n3 × 100\n4 × 100\nValidation Loss (log scale)\nBART_Scratch\nBART_Code\nBART_English\nBART_English+Code\n• Q3: Which test case would you prefer to have in your\nproject?\nThe ﬁrst two questions are designed to evaluate two\ndifferent factors, namely understandability and testing effec-\ntiveness of the test cases. These questions can be answered\nby choosing: (i) Test Case A; (ii) Test Case B; (ii) Equally\n(i.e., same degree of understandability and testing effective-\nness). The third question is designed to break possible ties,\nand asks for overall preference between the two test cases\n(choose A or B). This will provide some clues as to whether\ndevelopers prefer one factor over the other.\nThe survey consists of two background questions, asking\nabout Java and JUnit experience, followed by 14 testing\nscenarios to review. Each scenario is formed by a focal\nmethod, and two test cases (one fromATHENA TEST , the other\nfrom EvoSuite), randomly assigned with label A or B. The\n14 focal methods have been selected from the experiment in\nRQ5 and all the test cases selected are compilable and correct.\nWe simply instruct the developer to answer the questions\nbased on their personal preferences, without providing any\nclues about which test case was generated by our approach.\n4 E XPERIMENTAL RESULTS\nIn this section we report and discuss the results of our\nempirical study.\nRQ1: Does model pretraining impact the performances\nof Unit Test Case Generation?Figure 3 shows the cross-\nentropy loss on the validation set during training for the\nfour model variations. We note a substantial gap between\nthe model without pretraining ( BART_Scratch) compared\nto the models with English ( BART_English), source code\n(BART_Code) and both ( BART_English+Code) pretraining.\nComparing the English only and the English+Code models,\nthe additional pretraining on source code has three evident\neffects: (i) lower initial loss (1.89 versus 1.66); (ii) lower best\nloss (1.56 versus 1.51); (iii) faster convergence ( ∼20k training\nsteps earlier).\nWe conclude that English and Code pretraining are\nbeneﬁcial for our downstream task, thus we select the\nBART_English+Code as our starting model for the subsequent\nﬁnetuning steps.\nFig. 4: Focal Context Models - Validation Loss\nAdditional focal context improves task loss\n10000 12500 15000 17500 20000 22500 25000 27500 30000\nTraining Step\n1.35\n1.40\n1.45\n1.50\n1.55\n1.60\n1.65Validation Loss\nFM\nFM+FC\nFM+FC+C\nFM+FC+C+M\nFM+FC+C+M+F\nSummary for RQ 1. Pretraining on both English and\nsource code has a signiﬁcant positive effect on the task\nof generating Test Cases. The model BART_English+Code\nachieves the best validation loss.\nRQ2: How does focal context impact the training for\nUnit Test Case Generation?In this section we report the\nresults of our experiments aiming at investigating the impact\nof the focal context on the test case generation task.\nIngredient Space Analysis\nFigure 5 shows the distribution of number of tokens in the\ntarget test case that are shared with the input code repre-\nsentations. The distributions are represented with boxplots,\nwhere the vertical line represent the median and the red\ntriangle the mean.\nThe ﬁrst representation ( fm) shares 3 tokens on median\nand 4.15 tokens on average with the target test case, while\nthe largest representation (fm+fc+c+m+f ) shares 5 tokens on\nmedian and 5.69 tokens on average with the corresponding\ntest case.\nFrom the boxplots we can notice that the focal method\nrepresents the major contribution to the test case, in terms\nof ingredients. The focal class name and the constructors\nboost signiﬁcantly the shared tokens, while the subsequent\nadditions to the focal context have diminishing returns.\nThis preliminary analysis conﬁrms the intuition that\nadditional focal context can provide useful ingredients to the\nmodel when generating test cases.\nValidation Loss\nFigure 4 shows the cross-entropy loss on the validation set\nduring training for the ﬁve focal context model variants.\nAll the model variants have been ﬁnetuned starting from\nthe BART_English+Code, which was selected as the best\npretrained model in the previous research question.\nThe model variant fm depicted with a red line in Fig.\n4 corresponds to the red line in Fig. 3, which is the\nBART_English+Code model trained with the minimal focal\ncontext (fm).\nThe model variants with additional focal context show\nimproved validation loss over the base fm model. Speciﬁcally,\nthe biggest delta improvement is observed when adding\n9\nFig. 5: Focal Context - Ingredient Analysis\nIngredients for tests are available in the focal context\n0 2 4 6 8 10 12\nTokens in Test Case\nFM\nFM+FC\nFM+FC+C\nFM+FC+C+M\nFM+FC+C+M+F\nFig. 6: Testing APIs Distribution\nGenerated tests contains similar number of testing APIs\n0 1 2 3 4 5 6\n#Testing APIs\nOriginal\nPredicted\nthe focal class name ( fm+fc). This representation has only\nfew additional tokens compared to the fm model, however\nthey appear to provide signiﬁcant boost during training. We\nhypothesize that the focal class name is a strong semantic\nclue that can be leveraged by the model when generating\ntests.\nThe next three model variants fm+fc+c, fm+fc+c+m, and\nfm+fc+c+m+f cluster together towards the bottom of the\ngraph, with signiﬁcant improvement over the ﬁrst two\nvariants (fm and fm+fc). Overall, the best performing model\nis the fm+fc+c+m+f, which has the largest available focal\ncontext.\nThese results conﬁrm that focal context, in addition to\nthe focal method, provides informative tokens upon which\nthe model can attend while generating unit test cases. The\ningredient analysis complemented with the validation loss\nanalysis corroborates the intuition that information from the\nfocal class, such as its constructors, methods, and ﬁelds, are\nbeneﬁcial to the downstream task.\nWe select the model BART_English+Code pretrained on\nEnglish and code, then ﬁnetuned with the representation\nfm+fc+c+m+f, as our target model for A THENA TEST .\nSummary for RQ2. Focal context improves the perfor-\nmances of the model. It provides token ingredients that\ncan be used during the generation of unit test cases. The\nmodel fm+fc+c+m+f, with the largest available focal context,\nachieves the best validation loss.\nFig. 7: Testing APIs Breakdown Distribution\nGenerated tests contains similar API distribution to original\nassertEquals\nassertThat\nwhen\nassertTrue\nthenReturn\nverify\nassertFalse\nmock\nassertNotNull\nassertNull\nfail\nexpect\nassertSame\nassertArrayEquals\nassertThrows\nexpectMessage\n0\n5000\n10000\n15000\n20000\n25000\n30000\n35000\n40000 Original\nPredicted\nRQ3: What is quality of the generated Test Cases?\nSyntactic Correctness\nThe model generates syntactically correct Java methods for\n84% of the top predictions in the test set. We manually\ninvestigated the reasons behind the syntactic errors for some\nof the predictions, and found that they were mostly due to\ntruncated sequences when generating long test cases. We\ndevised a simple approach that attempts to recover these\npredictions by deleting the last truncated statement, and\nadding a closing parenthesis. With this simple approach,\nthe syntactic correctness reaches 95%. These results show\nthat our approach is able to generate syntactically correct\nJava methods in most of the cases, and with simple post-\nprocessing it achieves extremely high levels of correctness.\nFurthermore, an incorrect prediction could be replaced with\nanother prediction generated by the model (on the same focal\nmethod) using beam search or sampling.\nTesting APIs\nThe model generates methods that declare the @Test an-\nnotation in 99.99% of the cases, correctly learning the JUnit\nstandard for test cases. Furthermore, 94.9% of the generated\ntest cases invoke the correct focal method which is supposed\nto test.\nFigure 6 shows the distribution of testing API calls within\neach test cases in the test set, both for the original test cases\nand for the predictions of the model. From the boxplot we\ncan notice that the two distributions have the same quartiles\nwith, on median, one testing API call in each test case. Note\nthat outliers are not reported in this ﬁgure. The mean (shown\nas a red triangle) indicates that the original test cases tend\nto contain slightly more testing APIs compared to the ones\ngenerated by the model.\nFigure 7 shows the breakdown distribution of the top-\n16 testing API found in the test set. These include JUnit\nAPIs such as assertEquals and Mockito APIs such as\nmock and verify. The plot clearly shows that the generated\ntest cases invoke a variety of different testing APIs, closely\n10\nfollowing the distribution of the original test cases. However,\nwe do observe a gap between the number of APIs in the\noriginal and predicted test cases. In our future work we plan\nto incorporate techniques to augment the number of assert\nstatements in the test cases.\nWe conclude this research question with qualitative\nexamples of test cases generated by the model. Figure 8\nshows the focal method review belonging to the class\nCSConference. This example was written by one of the\nauthors with the goal of demonstrating the generalizability\nof our model on novel and unusual input methods (i.e., not\nbelonging to the training distribution). Figure 8 also shows\nthree examples of generated test cases selected from the\ntop-10 predictions of the model. In the test case #1 the\nmodel creates a new instance of Submission, followed by\nan assert statement that contains the focal method invocation.\nThe model generates a correct method invocation passing\nthe correct number of arguments and types. The assert\nstatement interestingly inspects the reason of the Response\nby matching the same string used within the focal method.\nIn the test case #2 the model ﬁrst creates a realistic instance of\na paper submission, then it invokes the focal method saving\nits response, and ﬁnally asserts that the response is of type\nAcceptance. Finally, in test case #3 the model relies on\nmocking APIs from Mockito to mock the Submission class,\nas well as using assert statements on the response. These\nexamples showcase the ability of our model in generating\nrealistic test cases, which resemble test cases written by\ndevelopers.\nSummary for RQ3. Our approach generates syntactically\ncorrect test cases that conform to the Unit Test Case\nstandards. The generated test cases invoke a variety of\nTesting APIs, following a similar distribution to the original\ntest cases.\nRQ4: Can ATHENA TEST generate Test Cases for De-\nfects4j projects?\nIn this section we report the results of using ATHENA TEST\nfor generating unit test cases for ﬁve defects4j projects.\nFor each focal method, we allow ATHENA TEST to generate\n30 candidate test cases using beam search, and evaluate\neach candidate based on syntax correctness, compilability,\nexecution, coverage and correctness.\nTable 3 reports the results of our experiments, where the\nleft side of the table provides focal method-level statistics,\nwhile the right side test case-level statistics.\nWe begin by discussing the Test Case statistics from right\nto left. In our experiments ATHENA TEST generated a total of\n158k test cases for 5,278 focal methods.\nThe syntactically incorrect test cases account for 9.49% of\nthe total generated tests. While this represents an opportunity\nand future direction for improving our model, in practice,\nthese candidates could be easily and quickly discarded using\na syntax checker.\nA signiﬁcant portion of the generated tests (42.41%)\nare syntactically correct but fail to build. In our manual\ninvestigation, we found that these build errors are often due\nto incorrect usage of classes and methods outside the focal\nclass. For example, a test case may need to instantiate an\nobject of a different class that is used by the focal method,\nFocal Method\npublic class CSConference {\npublic static Response review(Submission paper, List<Reviewer> revs) {\nif(paper.citations < 50){\nreturn new Rejection(\"Discuss related works\");\n}\nif(!paper.hasComparison()){\nreturn new Rejection(\"Compare with state of the art\");\n}\nreturn new Acceptance(\"Congrats!\");\n}}\nModel Generated Test Case #1 - Asserts\n@Test public void conference() {\nSubmission paper = new Submission();\nassertTrue(CSConference.review(paper, new ArrayList<Reviewer>()).getReason()\n.equalsIgnoreCase(\"Discuss related works\"));\n}\nModel Generated Test Case #2 - Asserts\n@Test public void mentionedStates() {\nSubmission paper = new Submission(\"Yang et al. 2012\",\n\"Elements of Information Sciences\", 61, 3);\nResponse response = CSConference.review(paper, new ArrayList<Reviewer>());\nassertTrue(response instanceof Acceptance);\n}\nModel Generated Test Case #3 - Mockito & Asserts\n@Test public void reference() {\nSubmission paper = mock(Submission.class);\nwhen(paper.getNumberOfCitations()).thenReturn(3);\nwhen(paper.citations()).thenReturn(60);\nwhen(paper.hasComparison()).thenReturn( true);\nResponse response = Conference.review(paper, new ArrayList<Reviewer>());\nassertEquals( new Acceptance(\"Congrats!\"), response);\nwhen(paper.citations()).thenReturn(99);\nresponse = Conference.review(paper, new ArrayList<Reviewer>());\nassertEquals( new Rejection(\"Discuss related works\"), response);\n}\nFig. 8: Examples of Generated Test Cases\nand an incorrect usage of the object ( e.g., wrong method\nname or parameter) may lead to a build error.\nFailing tests, which are compilable but fail during their\nexecution, represent 26.71% of the generated tests. These\ntests usually fail for incorrect assertions or wrong expected\nbehavior ( e.g., the test expects an exception which is not\nraised).\nPassing tests account for 21.35% of the tests generated by\nATHENA TEST . These tests are syntactically correct, compil-\nable, and execute without failing.\nFinally, when analyzing the coverage information of the\npassing test cases, we classify 16.21% of all the generated test\ncases, ∼25K tests, as correct. These test cases are a subset of\nthe passing tests which cover the correct focal method given\nas input. Note that the remaining passing test cases that are\nnot covering the focal method, could potentially still be used\nto test other parts of the project under test.\nConsidering the focal method-level statistics, ATHENA T-\nEST was able to generate at least one correct test case for\n43.75% of all the focal methods, for a total of ∼2k different\nmethods. We believe that this percentage could be increase\nby allowing the model to generate additional test cases over\nthe ﬁrst 30 candidates.\nOverall, the results of our experiments demonstrate that\nATHENA TEST is able to correctly test a large number of\ndifferent focal methods belonging to a diverse set of projects.\nWhile a 16% correct rate for candidate tests could be\nperceived as an underwhelming result, it is worth noting that\nwe are disclosing and analyzing every single attempt by our\nmodel. Common automated test generation approaches often\ncreate many internal candidates that are mutated, analyzed,\nand discarded before the correct ones are presented to the\nuser. For example, EvoSuite can generate a large offspring set,\nwhere descendants are mutated, evaluated, and discarded\n11TABLE 3: Defects4j Results – Test cases generated by ATHENA TEST are classiﬁed in ﬁve categories based on syntax\ncorrectness, compilability, test execution, and coverage. Overall, 16.21% of the generated tests are correct ( i.e., compile, pass,\nand cover the correct focal method), with A THENA TEST being able to correctly tests 43.75% of the focal methods.\nProject Focal Methods Test Cases\nTested Total Correct Passing Failing Build Error Syntax Error Total\nLang 1,545 (56.97%) 2,712 18,997 (23.35%) 25,563 (31.42%) 26,515 (32.58%) 19,450 (23.90%) 9,832 (12.08%) 81,360\nChart 425 (32.00%) 1,328 3,443 (8.64%) 4,663 (11.70%) 6567 (16.48%) 28,318 (71.07%) 292 (0.73%) 39,840\nCli 190 (29.46%) 645 2,142 (11.07%) 2,399 (12.39%) 5,492 (28.38%) 10,545 (54.49%) 914 (4.72%) 19,350\nCsv 128 (34.31%) 373 912 (8.15%) 1,005 (8.98%) 2,160 (19.30%) 5,370 (47.99%) 2,655 (23.72%) 11,190\nGson 21 (9.54%) 220 186 (2.80%) 186 (2.80%) 1,573 (23.71%) 3,497 (52.71%) 1,344 (20.26%) 6,634\nTotal 2,309 (43.75%) 5,278 25,680 (16.21%) 33,816 (21.35%) 42,307 (26.71%) 67,180 (42.41%) 15,037 (9.49%) 158,374\nduring the evolution.\nIn a deployment setting, ATHENA TEST could be used to\ngenerate a large set of candidates which are then analyzed\nand ﬁltered before being introduce in the project under test.\nWe publicly release all 25K correct test cases generated\nby ATHENA TEST in this experiment [43]. Test cases and\nassociated metadata ( e.g., coverage ﬁles) are available for\nbrowsing and downloading at our dedicated website. 2\nSummary for RQ4. ATHENA TEST is able to generate correct\ntest cases for different defects4j projects. When generating\nup to 30 candidates, ATHENA TEST was able to correctly\ntests 43% of all the focal methods, with 16% of the candidate\ntests being correct.\nRQ5: How does our approach compare to EvoSuite and\nGPT-3?\nTable 4 reports the results of our test coverage analysis\ncomparing EvoSuite, GPT-3, and ATHENA TEST on the class\nNumberUtils of Lang-1-f. The table reports the absolute\n(and percentage) line and condition coverage at class-level,\nfor each of the 18 unique public methods in the class (without\nconsidering overloading), marking in bold the best coverage\nvalue. From the results in Table 4 we notice: (i) EvoSuite was\nable to successfully test all the methods; (ii) GPT-3 correctly\ntested only 6 out of 18 methods; (iii) ATHENA TEST generated\ncorrect test cases for all the methods, while achieving the\nbest coverage in most cases.\nFor GPT-3 we explored several sampling temperatures,\nand settled on the 0.5 value which appeared to provide good\ndiversity of the samples while still generating realistic code.\nWe found that, in most of the cases where GPT-3 was not\nable to generate a correct test case, it generated code that\nonly invoked the focal method without correctly asserting\nits behavior. However, in those 6 cases reported in the table,\nwe found the test cases to be correct and readable code, and\nsometimes also obtaining the best coverage. While GPT-3\nachieved the lowest overall performances of the three, we\nwould consider this still a positive result for GPT-3, given\nthe fact that it was not ﬁnetuned on test case generation.\nRegarding our approach, ATHENA TEST was able to\ngenerate correct test cases for all the focal methods. Overall,\nthe results indicate that ATHENA TEST is able to generate\ncorrect test cases with adequate test coverage, often achieving\nbetter coverage than EvoSuite.\nWe now provide a qualitative comparison of the test\ncases generated by the three approaches. Figure 9 shows\n2. https://athenatestdemowebsite.azurewebsites.net\nthe generated test cases for the focal method createFloat.\nEvoSuite creates a test case that assert that the return value\nof the method is null, when providing a null string as\ninput, covering the ﬁrst condition in the focal method.\nGPT-3 creates a test case that simply invokes the focal\nmethod multiple times (limited in the ﬁgure), with correct\narguments, but without asserting the correct behaviour of\nthe method. ATHENA TEST generated a test case that checks\n(i) the focal method correctly creates a the ﬂoat 1.2; (ii) the\nfocal method returns null on a null string. Speciﬁcally, it\ncovers both conditions of the focal method. We can also\nnotice that the generated test case has a very idiomatic\nname testCreateFloat (similar to GPT-3), compared to\nEvoSuite’stest044.\nFigure 10 shows the test cases for the focal method\nisDigits. EvoSuite’s test case checks whether the empty\nstring is correctly identiﬁed as not being a numerical digit.\nGPT-3 accurately asserts the behavior of the method by\ntesting a string containing only digits ( e.g., \"100\") and\none that contains a non-digit character ( e.g., \"1a\"). Our\napproach, generates a test case which tests six different\nstrings, one of which only contains digits, while the other ﬁve\ncontains also non-digit characters, for example a negative\nnumber (e.g., \"-123\") as well as comma-separated numbers\n(e.g., \"1,2,3\"). Interestingly, in this case, combining EvoSuite\nand ATHENA TEST would lead to better overall code coverage.\nCompared to EvoSuite, our approach is able to generate\ncorrect test cases which obtain comparable (if not better) test\ncoverage for most of the focal methods. ATHENA TEST ’s test\ncases appear to be more readable and similar to developers-\nwritten code w.r.t. those generated by EvoSuite. Our ap-\nproach outperforms GPT-3 in terms of number of correct\ntest cases generated. While GPT-3 was not ﬁnetuned on the\ntest case generation task, it is a substantially larger model\n(175 billion parameters) compared to our transformer-based\nmodel (400 million parameters).\nSummary for RQ3. Our approach generates test cases that\naccurately test the focal methods and obtain comparable\ntest coverage w.r.t. EvoSuite, as well as outperforming GPT-\n3. These test cases appear to be similar to developer-written\ntest cases with readable and understandable code.\nRQ6: Do developers prefer ATHENA TEST ’s test cases\nover EvoSuite’s?\nWe received responses from 12 Microsoft developers,\nnone of them involved in this work. All the developers had\nJava experience (4 with one year or less, 7 with 1-3 years, 1\nwith 4 or more years). Eight of them claimed to have JUnit\n12TABLE 4: Test Coverage Analysis – Test cases generated by EvoSuite, GPT-3, and ATHENA TEST are executed and their\ncoverage is analyzed in terms of line and condition covered. A THENA TEST has a comparable coverage w.r.t. EvoSuite.\nFocal Method EvoSuite GPT-3 A THENA TEST\nLines Conditions Lines Conditions Lines Conditions\ntoInt(String, int) 21 (5.6%) 1 (0.3%) - - 23 (6.1%) 2 (0.6%)\ntoLong(String, long) 20 (5.3%) 1 (0.3%) - - 20 (5.3%) 1 (0.3%)\ntoFloat(String, float) 20 (5.3%) 1 (0.3%) - - 22 (5.9%) 1 (0.3%)\ntoDouble(String, double) 20 (5.3%) 1 (0.3%) - - 20 (5.3%) 1 (0.3%)\ntoByte(String, byte) 20 (5.3%) 1 (0.3%) - - 23 (6.1%) 2 (0.6%)\ntoShort(String, short) 20 (5.3%) 1 (0.3%) - - 22 (5.9%) 1 (0.3%)\ncreateFloat(String) 20 (5.3%) 1 (0.3%) - - 21 (5.6%) 2 (0.6%)\ncreateDouble(String) 20 (5.3%) 1 (0.3%) - - 21 (5.6%) 2 (0.6%)\ncreateInteger(String) 20 (5.3%) 1 (0.3%) - - 21 (5.5%) 2 (0.6%)\ncreateLong(String) 20 (5.3%) 1 (0.3%) 20 (5.3%) 1 (0.3%) 21 (5.6%) 2 (0.6%)\ncreateBigInteger(String) 28 (7.5%) 8 (2.4%) 30 (8.7%) 7 (2.1%) 20 (5.3%) 1 (0.3%)\ncreateBigDecimal(String) 22 (5.9%) 3 (0.9%) - - 22 (5.9%) 3 (0.9%)\nmin(long[]) 27 (7.2%) 6 (1.8%) 26 (6.9%) 5 (1.5%) 22 (5.9%) 2 (0.6%)\nmin(int, int, int) 22 (5.9%) 2 (0.6%) 23 (6.1%) 2 (0.6%) 22 (5.9%) 2 (0.6%)\nmax(float[]) 28 (7.5%) 7 (2.1%) - - 22 (5.8%) 2 (0.6%)\nmax(byte, byte, byte) 23 (6.1%) 2 (0.6%) 21 (5.6%) 2 (0.6%) 22 (5.9%) 2 (0.6%)\nisDigits(String) 20 (5.3%) 1 (0.3%) 23 (6.1%) 5 (1.5%) 23 (6.1%) 5 (1.5%)\nisNumber(String) 44 (11.7%) 29 (8.6%) - - 51 (13.6%) 41 (12.1%)\nFocal Method\npublic static Float createFloat( final String str) {\nif (str == null) {\nreturn null ;\n}\nreturn Float.valueOf(str);\n}\nEvoSuite\n@Test(timeout = 4000)\npublic void test044() throws Throwable {\nFloat float0 = NumberUtils.createFloat((String) null);\nassertNull(float0);\n}\nGPT-3\n@Test public void testCreateFloat() {\nNumberUtils.createFloat(\"1.0\");\nNumberUtils.createFloat(\"1.0f\");\nNumberUtils.createFloat(\"1.0E+1\");\nNumberUtils.createFloat(\"1.0E-1\");\n...\nATHENA TEST\n@Test public void testCreateFloat() {\nassertEquals( new Float(1.2f), NumberUtils.createFloat(\"1.2\"));\nassertNull(NumberUtils.createFloat( null));\n}\nFig. 9: Test Cases Generated for createFloat\nexperience.\nFigure 11 reports the answers to the three survey ques-\ntions in a likert-style plot, where the y-axis represents the\ntesting scenario instance, and the x-axis the number of re-\nsponses for EvoSuite (in red, towards left), for ATHENA TEST\n(in blue, towards right), and neutral answer (middle green).\nRegarding Q 1, we found that 61% of the responses\nfavored ATHENA TEST ’s test cases in terms of readability and\nunderstandability, while in 29% of the cases the developers\nthought both test cases were equally readable, and only in\n10% of the cases they preferred EvoSuite’s.\nFor Q2, 70% of the responses selected ATHENA TEST ’s test\ncases as testing the focal method more appropriately than\nEvoSuite’s counterpart. In 12% of the cases they were deemed\nas equally appropriate, and only in 18% the developers\npreferred EvoSuite’s test case.\nFinally in Q3, when asked to choose which test case they\npreferred overall, they overwhelmingly elected ATHENA T-\nEST ’s test cases, in 82% of the cases, and only 18% EvoSuite.\nInterestingly, we found that in 12 instances (∼7%), devel-\nFocal Method\npublic static boolean isDigits( final String str) {\nif (StringUtils.isEmpty(str)) {\nreturn false ;\n}\nfor (int i = 0; i < str.length(); i++) {\nif (!Character.isDigit(str.charAt(i))) {\nreturn false ;\n}\n}\nreturn true ;\n}\nEvoSuite\n@Test(timeout = 4000)\npublic void test092() throws Throwable {\nboolean boolean0 = NumberUtils.isDigits(\"\");\nassertFalse(boolean0);\n}\nGPT-3\n@Test public void testIsDigits() {\nassertTrue(NumberUtils.isDigits(\"100\"));\nassertFalse(NumberUtils.isDigits(\"1a\"));\n}\nATHENA TEST\n@Test public void isDigits() {\nassertTrue(NumberUtils.isDigits(\"123\"));\nassertFalse(NumberUtils.isDigits(\"-123\"));\nassertFalse(NumberUtils.isDigits(\"1a2\"));\nassertFalse(NumberUtils.isDigits(\"a0b-2\"));\nassertFalse(NumberUtils.isDigits(\"abcax\"));\nassertFalse(NumberUtils.isDigits(\"1,2,3\"));\n}\nFig. 10: Test Cases Generated for isDigits\nopers picked one test case in Q1 and the other test case in Q2.\nA deep dive in these cases revealed that developers mostly\npreferred ATHENA TEST test cases in terms of readability, but\nEvoSuite in terms of testing effectiveness.\nSummary for RQ4. Developers prefer test cases generated\nby ATHENA TEST over those generated by EvoSuite, in\nterms of readability, understandability, and testing effec-\ntiveness.\n5 D ISCUSSION & FUTURE WORK\nOur preliminary evaluation shows encouraging results in\nmany different aspects. Our approach is able to generate\nsyntactically correct test cases that conform to the test case\n13\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nQ1: Which test case is more readable and understandable?\nEvoSuite\nEqual\nAthenaTest\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nQ2: Which test case is testing the method more appropriately?\nEvoSuite\nEqual\nAthenaTest\n024681012 0 2 4 6 8 10 12\nNumber of Responses\n13\n12\n11\n10\n9\n8\n7\n6\n5\n4\n3\n2\n1\n0\nQ3: Which test case would you prefer to have in your project?\nEvoSuite\nAthenaTest\nFig. 11: Survey results with professional developers\nstandards and invoke a variety of testing APIs. While further\nanalyses should be performed, this preliminary evaluation\nshows that the generated test cases appear to be (i) realistic –\nsimilar to developer-written test cases; (ii) accurate – correctly\nasserting the expected behavior of a focal method; (iii) human-\nreadable – readable and understandable code, with good\nvariable and method names.\nWe believe this work represents a stepping stone towards\na new category of automated test case generation tools,\nshifting away from coverage-guided approaches towards\nmodels that aim at code understanding. These learning\napproaches have the potential of generating natural test\ncases that better integrate with the existing code base, and\ndo not appear like machine-written code.\nDuring our manual investigation of the generated test\ncases, we also observed several weaknesses and pitfalls of\nthe model, which we will discuss in this section. These\nweaknesses serve us as inspiration for future work, with\nthe goal of improving our model.\n5.1 Project-Level Context\nWhen providing focal context limited to the focal method and\nclass, the model is forced to perform a series of reasonable\nguesses on the composition of other classes and methods\noutside the scope of the focal class. For example, if the focal\nmethod takes as argument an object of a speciﬁc Class, the\nmodel doesn’t currently have knowledge about the behavior\nand available methods of the Class. In those instances, the\nmodel relies on the past pretraining (on a large amount of\nsource code) to infer the behavior of such classes.\nWe plan to incorporate project-level context, pertinent\nwith the given focal method and class, in our input repre-\nsentation to the model. A static analysis tool could be used\nto collect information about the classes involved in the focal\nmethod (e.g., instantiated, invoked or passed as argument),\nand a skeleton of such classes summarizing the APIs could\nbe used to augment the focal context input.\nFurthermore, semi-supervised pretraining on the projects\nwhere the model will be used to generate test cases could\nhelp the model to familiarize with the code base and be more\naccurate when generating statements and method calls.\n5.2 Testing Frameworks\nNumerous testing frameworks are available for Java devel-\nopers which aim at supporting domain-speciﬁc applications\nor different testing scenarios and methodologies. Our current\napproach does not take into consideration the speciﬁc testing\nframework used by the developer, and thus could propose\na test case using a different testing API which is not being\nused in the current project.\nIn our future work we plan to train our model to support\nmultiple testing frameworks and to allow the developer to\nspecify the particular testing APIs to be used. This could be\nachieved using control codes (i.e., special reserved keywords)\nto inform the model about the particular testing APIs used\nin the test case, both during training and inference.\n5.3 Deployment\nDeployment of large neural models to production represents\na major engineering challenge. In this section, we discuss\nthe possible deployment scenario in Visual Studio Code IDE\nbacked by the Azure cloud compute.\nWe propose to design the ATHENA TEST system as a\ntwo-layer service, consisting of the server-side inference\nmodule and the client-side unit test case provider module.\nWith the model size exceeding 100 MB, the cloud-based\ndeployment is the only viable option, which also offers\ncontrol over the hardware setup and can guarantee resource\navailability. Introducing the client-side unit test case provider\nmodule would allow to minimize the inference time for the\nbest user experience. The server-side module is deployed\nas a containerized web application to Azure Kubernetes\nService [44] listening on a HTTPS endpoint. It processes\ncompletion requests and returns the model output, which is\nimplemented in PyTorch.\n6 T HREATS TO VALIDITY\nThreats to construct validity concern the relationship between\ntheory and observation and are mainly related to the mea-\nsurements we performed. In our context, the threat arises by\ntraining our models on potentially noisy data, speciﬁcally,\nlow quality test cases or incorrect mapping between focal\nmethods and tests. We attempt to mitigate this threat by\nrelying on safe and accurate heuristics to mine test cases and\nfocal methods, following best practices.\nInternal validity threats concern factors internal to our\nstudy that could inﬂuence our results. The performance of\nour approach depends on the hyperparameter conﬁguration\nand pretraining process. We did not perform hyperparameter\nsearch since these large models require substantial training\ntime, however, we reuse conﬁgurations suggested in the\nliterature. We experiment with different pretraining stages\nand report the results of our experiments.\nThreats to external validity concern the generalization of\nour ﬁndings. In this paper the threat arises in RQ 3, given the\nsmall-scale evaluation, we cannot claim generalizability of\n14\nthe results. We clearly state that this represents a preliminary\nevaluation and more experiments should be conducted to\nassess the quality of our approach. We also acknowledge the\nfact that additional analyses should be performed to evaluate\nthe fault detection capability of the generated test cases. We\nare actively working on addressing these limitations in our\ncontinuing work.\n7 R ELATED WORK\nOur work is related to several existing approaches in the\narea of automated software testing. In particular, there is\na class of approaches that aims at generating tests cases,\nsuch as Evosuite [6], Randoop [7], and Agitar [8]. The\nmain differentiating factor between these techniques and\nour approach is the learning component. ATHENA TEST is\nbased on transformer model which aims at learning, from\ndeveloper-written test cases, the best practices on how\nto write readable and accurate test cases. On the other\nhand, most of the existing techniques in the literature rely\non handcrafted rules or heuristics to generate test cases,\noptimizing towards code coverage.\nSeveral existing works in the literature have proposed\ndeep learning based approaches for software engineering\ntasks, such as code completion [17], automated patch gener-\nation [18], [19], comment generation [20], and many others\n[21]. While we share with these approaches the process\nof learning from examples, we also introduce signiﬁcant\nnovelty in this process. Speciﬁcally, we are among the ﬁrst to\ntrain large, state-of-the-art sequence-to-sequence transformer\nmodels applied to software engineering tasks. Additionally,\nwe pretrain these models on both English and source code\nshowing the beneﬁts of both types of pretraining on the\ngeneration of test cases.\nOur work is also related to a broad set of literature on\ntransfer learning [39], unsupervised language model pretrain-\ning [45], [46], and denoising pretraining [25], [47], [48]. In this\npaper, we extend these ideas to source code as a language,\ncombining English and source code pretraining modes, ﬁne-\ntuning on a downstream translation task from the automated\nsoftware engineering domain. We compare this approach to\nthe task-agnostic few-short learning approach introduced in\nGPT-3 [22]. We ﬁnd and discuss certain limitations of the\nfew-shot learning approach as compared to ﬁnetuning using\na translation task.\n8 C ONCLUSION\nIn this paper we presented ATHENA TEST , an approach that\naims at generating unit test cases by learning from real-\nworld, developer-written test cases. Our approach relies\non a sequence-to-sequence transformer model which was\npretrained both on English and Java source code, then\nﬁnetuned on the task of generating test cases given a method\nunder test. We train the model using a supervised parallel\ncorpus of 630k test cases and corresponding focal methods\nin Java, which we publicly release as M ETHODS 2TEST [23].\nOur evaluation shows that ATHENA TEST is able to\ngenerate syntactically correct test cases that invoke a variety\nof testing APIs. We compiled and executed these test cases,\ncomparing them with EvoSuite and GPT-3, ﬁnding that we\nachieve comparable or better test coverage. Finally, in a\nstudy with professional developers, we found that they\nprefer ATHENA TEST ’s test cases in terms of readability,\nunderstandability, and testing effectiveness.\nREFERENCES\n[1] M. Cohn, Succeeding with agile: software development using Scrum .\nPearson Education, 2010.\n[2] “Junit,” https://junit.org.\n[3] “Apache jakarta cactus,” http://jakarta.apache.org/cactus.\n[4] “Testng,” https://testng.org.\n[5] “Mockito,” https://site.mockito.org.\n[6] G. Fraser and A. Arcuri, “Evosuite: automatic test suite generation\nfor object-oriented software,” in Proceedings of the 19th ACM\nSIGSOFT symposium and the 13th European conference on Foundations\nof software engineering, 2011, pp. 416–419.\n[7] C. Pacheco and M. D. Ernst, “Randoop: feedback-directed random\ntesting for java,” in Companion to the 22nd ACM SIGPLAN conference\non Object-oriented programming systems and applications companion ,\n2007, pp. 815–816.\n[8] Agitar, “Utilizing Fast Testing to Transform Java Development into\nan Agile, Quick Release, Low Risk Process,” http://www.agitar.\ncom/, 2020.\n[9] E. Daka, J. Campos, G. Fraser, J. Dorn, and W. Weimer, “Modeling\nreadability to improve unit tests,” inProceedings of the 2015 10th Joint\nMeeting on Foundations of Software Engineering , 2015, pp. 107–118.\n[10] G. Grano, S. Scalabrino, H. C. Gall, and R. Oliveto, “An empirical\ninvestigation on the readability of manual and generated test\ncases,” in 2018 IEEE/ACM 26th International Conference on Program\nComprehension (ICPC). IEEE, 2018, pp. 348–3483.\n[11] F. Palomba, D. Di Nucci, A. Panichella, R. Oliveto, and A. De Lucia,\n“On the diffusion of test smells in automatically generated test code:\nAn empirical study,” in 2016 IEEE/ACM 9th International Workshop\non Search-Based Software Testing (SBST). IEEE, 2016, pp. 5–14.\n[12] F. Palomba, A. Panichella, A. Zaidman, R. Oliveto, and A. De Lucia,\n“Automatic test case generation: What if test code quality matters?”\nin Proceedings of the 25th International Symposium on Software Testing\nand Analysis, 2016, pp. 130–141.\n[13] G. Grano, F. Palomba, D. Di Nucci, A. De Lucia, and H. C. Gall,\n“Scented since the beginning: On the diffuseness of test smells in\nautomatically generated test code,” Journal of Systems and Software,\nvol. 156, pp. 312–327, 2019.\n[14] G. H. Pinto and S. R. Vergilio, “A multi-objective genetic algorithm\nto test data generation,” in 2010 22nd IEEE International Conference\non Tools with Artiﬁcial Intelligence, vol. 1. IEEE, 2010, pp. 129–134.\n[15] M. M. Almasi, H. Hemmati, G. Fraser, A. Arcuri, and J. Benefelds,\n“An industrial evaluation of unit test generation: Finding real faults\nin a ﬁnancial application,” in 2017 IEEE/ACM 39th International\nConference on Software Engineering: Software Engineering in Practice\nTrack (ICSE-SEIP). IEEE, 2017, pp. 263–272.\n[16] S. Shamshiri, “Automated unit test generation for evolving soft-\nware,” in Proceedings of the 2015 10th Joint Meeting on Foundations of\nSoftware Engineering, 2015, pp. 1038–1041.\n[17] A. Svyatkovskiy, Y. Zhao, S. Fu, and N. Sundaresan, “Pythia: ai-\nassisted code completion system,” in Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge Discovery & Data\nMining, 2019, pp. 2727–2735.\n[18] M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, and\nD. Poshyvanyk, “An empirical study on learning bug-ﬁxing patches\nin the wild via neural machine translation,” ACM Transactions on\nSoftware Engineering and Methodology (TOSEM) , vol. 28, no. 4, pp.\n1–29, 2019.\n[19] Z. Chen, S. J. Kommrusch, M. Tufano, L.-N. Pouchet, D. Poshy-\nvanyk, and M. Monperrus, “Sequencer: Sequence-to-sequence\nlearning for end-to-end program repair,” IEEE Transactions on\nSoftware Engineering, 2019.\n[20] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment\ngeneration,” in 2018 IEEE/ACM 26th International Conference on\nProgram Comprehension (ICPC). IEEE, 2018, pp. 200–20 010.\n[21] C. Watson, M. Tufano, K. Moran, G. Bavota, and D. Poshyvanyk,\n“On learning meaningful assert statements for unit test cases,”arXiv\npreprint arXiv:2002.05800, 2020.\n15\n[22] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P . Dhariwal,\nA. Neelakantan, P . Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei, “Language models are\nfew-shot learners,” 2020.\n[23] Microsoft, “methods2test,” https://github.com/microsoft/\nmethods2test, 2020.\n[24] “Tree-sitter,” http://tree-sitter.github.io/tree-sitter.\n[25] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed,\nO. Levy, V . Stoyanov, and L. Zettlemoyer, “Bart: Denoising\nsequence-to-sequence pre-training for natural language generation,\ntranslation, and comprehension,” 2019.\n[26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, and I. Polosukhin, “Attention is all\nyou need,” CoRR, vol. abs/1706.03762, 2017. [Online]. Available:\nhttp://arxiv.org/abs/1706.03762\n[27] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis,\nL. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly optimized\nbert pretraining approach,” arXiv preprint arXiv:1907.11692, 2019.\n[28] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\n“Codesearchnet challenge: Evaluating the state of semantic code\nsearch,” arXiv preprint arXiv:1909.09436, 2019.\n[29] O. Press and L. Wolf, “Using the output embedding to improve\nlanguage models,” CoRR, vol. abs/1608.05859, 2016. [Online].\nAvailable: http://arxiv.org/abs/1608.05859\n[30] M. Martinez, W. Weimer, and M. Monperrus, “Do the ﬁx ingredients\nalready exist? an empirical inquiry into the redundancy assump-\ntions of program repair approaches,” in Companion Proceedings of\nthe 36th international conference on software engineering , 2014, pp.\n492–495.\n[31] M. White, M. Tufano, M. Martinez, M. Monperrus, and D. Poshy-\nvanyk, “Sorting and transforming program repair ingredients via\ndeep learning code similarities,” in 2019 IEEE 26th International\nConference on Software Analysis, Evolution and Reengineering (SANER).\nIEEE, 2019, pp. 479–490.\n[32] A. Bruns, A. Kornstadt, and D. Wichmann, “Web application tests\nwith selenium,” IEEE software, vol. 26, no. 5, pp. 88–91, 2009.\n[33] “Rest assured,” http://rest-assured.io, 2020.\n[34] “Apache commons lang,” https://commons.apache.org/proper/\ncommons-lang, 2021.\n[35] “Jfreechart,” https://jfree.org/jfreechart, 2021.\n[36] “Apache commons cli,” https://commons.apache.org/proper/\ncommons-cli, 2021.\n[37] “Apache commons csv,” https://commons.apache.org/proper/\ncommons-csv, 2021.\n[38] “Google gson,” https://github.com/google/gson, 2021.\n[39] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P . J. Liu, “Exploring the limits of transfer\nlearning with a uniﬁed text-to-text transformer,” 2019.\n[40] A. Gokaslan and V . Cohen, “Openwebtext corpus,” http://\nSkylion007.github.io/OpenWebTextCorpus, 2019.\n[41] R. Just, D. Jalali, and M. D. Ernst, “Defects4j: A database of existing\nfaults to enable controlled testing studies for java programs,” in\nProceedings of the 2014 International Symposium on Software Testing\nand Analysis, 2014, pp. 437–440.\n[42] “Cobertura,” https://cobertura.github.io/cobertura, 2020.\n[43] Microsoft, “Athenatest website,” https://athenatestdemowebsite.\nazurewebsites.net, 2021.\n[44] ——, “Azure kubernetes service,” https://azure.microsoft.com/\nen-us/services/kubernetes-service, 2020.\n[45] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and\nI. Sutskever, “Language models are unsupervised multitask\nlearners,” 2018. [Online]. Available: https://d4mucfpksywv.\ncloudfront.net/better-language-models/language-models.pdf\n[46] ——, “Language models are unsupervised multitask learners,”\n2019.\n[47] J. Devlin, M. Chang, K. Lee, and K. Toutanova, “BERT:\npre-training of deep bidirectional transformers for language\nunderstanding,” CoRR, vol. abs/1810.04805, 2018. [Online].\nAvailable: http://arxiv.org/abs/1810.04805\n[48] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “Roberta: A robustly\noptimized BERT pretraining approach,” CoRR, vol. abs/1907.11692,\n2019. [Online]. Available: http://arxiv.org/abs/1907.11692",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8127719759941101
    },
    {
      "name": "Unit testing",
      "score": 0.805832028388977
    },
    {
      "name": "Java",
      "score": 0.7018837332725525
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6386836767196655
    },
    {
      "name": "Test (biology)",
      "score": 0.6086703538894653
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5690419673919678
    },
    {
      "name": "Machine learning",
      "score": 0.5306921005249023
    },
    {
      "name": "Test case",
      "score": 0.5149944424629211
    },
    {
      "name": "Source code",
      "score": 0.5024905204772949
    },
    {
      "name": "Task (project management)",
      "score": 0.45426103472709656
    },
    {
      "name": "Code coverage",
      "score": 0.4349888265132904
    },
    {
      "name": "Data mining",
      "score": 0.3954378366470337
    },
    {
      "name": "Natural language processing",
      "score": 0.367737352848053
    },
    {
      "name": "Programming language",
      "score": 0.24585622549057007
    },
    {
      "name": "Software",
      "score": 0.17993620038032532
    },
    {
      "name": "Engineering",
      "score": 0.10956946015357971
    },
    {
      "name": "Systems engineering",
      "score": 0.07486739754676819
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Regression analysis",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": []
}