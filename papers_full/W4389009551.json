{
  "title": "Analogy Generation by Prompting Large Language Models: A Case Study of InstructGPT",
  "url": "https://openalex.org/W4389009551",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2690126050",
      "name": "Bhavya Bhavya",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A2187349416",
      "name": "Jinjun Xiong",
      "affiliations": [
        "University at Buffalo, State University of New York"
      ]
    },
    {
      "id": "https://openalex.org/A2152766206",
      "name": "ChengXiang Zhai",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2470921995",
    "https://openalex.org/W3168771811",
    "https://openalex.org/W2947415936",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3177100526",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W1489525520",
    "https://openalex.org/W4239181501",
    "https://openalex.org/W4307653761",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3103161287",
    "https://openalex.org/W3177879172",
    "https://openalex.org/W3035252911",
    "https://openalex.org/W3107855336",
    "https://openalex.org/W2000226110",
    "https://openalex.org/W3169841173",
    "https://openalex.org/W2912178159",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3095771422",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W2950824039",
    "https://openalex.org/W3174082608",
    "https://openalex.org/W1975879668",
    "https://openalex.org/W3092786817",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2964132420"
  ],
  "abstract": "We propose a novel application of prompting Pre-trained Language Models (PLMs) to generate analogies and study how to design effective prompts for two task settings: generating a source concept analogous to a given target concept (aka Analogous Concept Generation or ACG), and generating an explanation of the similarity between a given pair of target concept and source concept (aka Analogous Explanation Generation or AEG).We found that it is feasible to prompt InstructGPT to generate meaningful analogies and the best prompts tend to be precise imperative statements especially with a low temperature setting.We also systematically analyzed the sensitivity of the InstructGPT model to prompt design, temperature, and injected spelling errors, and found that the model is particularly sensitive to certain variations (e.g., questions vs. imperative statements).Further, we conducted human evaluation on 1.4k of the generated analogies and found that the quality of generations varies substantially by model size.The largest Instruct-GPT model can achieve human-level performance at generating meaningful analogies for a given target while there is still room for improvement on the AEG task. 1",
  "full_text": "Proceedings of the 15th International Conference on Natural Language Generation, pages 298 - 312\nJuly 17-22, 2022c⃝2022 Association for Computational Linguistics\nAnalogy Generation by Prompting Large Language Models:\nA Case Study of InstructGPT\nBhavya Bhavya1, Jinjun Xiong2 and ChengXiang Zhai1\n1Department of Computer Science, University of Illinois at Urbana-Champaign\n1{bhavya2, czhai}@illinois.edu\n2Department of Computer Science and Engineering, University at Buffalo\n2jinjun@buffalo.edu\nAbstract\nWe propose a novel application of prompting\nPre-trained Language Models (PLMs) to gen-\nerate analogies and study how to design effec-\ntive prompts for two task settings: generating\na source concept analogous to a given target\nconcept (aka Analogous Concept Generation\nor ACG), and generating an explanation of the\nsimilarity between a given pair of target con-\ncept and source concept (aka Analogous Ex-\nplanation Generation or AEG ). We found that\nit is feasible to prompt InstructGPT to gener-\nate meaningful analogies and the best prompts\ntend to be precise imperative statements espe-\ncially with a low temperature setting. We also\nsystematically analyzed the sensitivity of the\nInstructGPT model to prompt design, temper-\nature, and injected spelling errors, and found\nthat the model is particularly sensitive to cer-\ntain variations (e.g., questions vs. imperative\nstatements). Further, we conducted human eval-\nuation on 1.4k of the generated analogies and\nfound that the quality of generations varies sub-\nstantially by model size. The largest Instruct-\nGPT model can achieve human-level perfor-\nmance at generating meaningful analogies for\na given target while there is still room for im-\nprovement on the AEG task.1\n1 Introduction\nLarge Pre-trained Language Models (PLMs) such\nas BERT(Devlin et al., 2018) and GPT(Brown et al.,\n2020) have been applied to many tasks of text gen-\neration (e.g., summarization, dialogue system) with\npromising results (Li et al., 2021). However, no\nexisting work has studied how to apply PLMs to\ngenerate different kinds of textual analogies, such\nas conceptual metaphors (e.g.,“Life is a journey2”),\nand instructional analogies (e.g., “A red blood cell\nis like a truck in that they both transport essential\nsupplies”(Newby et al., 1995)).\n1Our code and datasets are available for public use:\nhttps://github.com/Bhaavya/InstructGPT-Analogies\n2https://en.wikipedia.org/wiki/Conceptual_metaphor\nTable 1: Selected prompts and InstructGPT-generated\nanalogies for natural selection\nPrompt\n(P7):\nWhat is analogous to natural se-\nlection?\nInstructGPT\nOutput:\nThe analogous process to natural\nselection is artificial selection. (9\nwords)\nPrompt\n(P2):\nExplain natural selection using a\nwell-known analogy.\nInstructGPT\nOutput:\nImagine that you have a jar of\nmixed nuts ... If you shake the jar\n...the big nuts will fall out first ...\nanalogy is that natural selection\nis like a sieve that separates the\nfit from the unfit... (136 words)\n.Generating analogies has a wide range of appli-\ncations, such as explaining concepts and scientific\ninnovation, and analogies play a crucial role in hu-\nman cognition. Analogical matching and reasoning\nenables humans to understand and learn unfamiliar\nconcepts (aka target concepts) by means of familiar\nones (aka source concepts) and to make scientific\ninnovations. Unsurprisingly, analogy modeling and\ngeneration has been a long-standing goal of AI\n(Mitchell, 2021). This is a challenging problem\nbecause it often requires computing deep semantic\nsimilarities that are beyond the surface-level simi-\nlarity. For example, the Bohr’s atom model and the\nsolar system are analogous due to their structural\nand relational similarities (i.e., atoms orbit around\nthe nucleus like planets around the sun).\nMuch work has been done to compute such ana-\nlogical similarities between concepts. However,\nexisting approaches mostly rely on structured rep-\nresentations, thus, they can only where such repre-\nsentations already exist. For example, one of the\nmost popular models is Structural Mapping Engine\n(SME) (Forbus et al., 2017), which aligns struc-\ntured representations of the target and source con-\ncepts using predicate logic. Moreover, they cannot\ngenerate analogies in natural language.\n298\nInspired by the recent success in applying PLMs\nto many NLP tasks (e.g., (Li et al., 2021)), we\npropose and study the application of PLMs to anal-\nogy generation. We consider two typical applica-\ntion scenarios of analogy generation: 1) Analogous\nConcept Generation (ACG): given a target concept\n(e.,g, bohr’s model), generate a source concept anal-\nogous to the target concept (e.g., solar system), pos-\nsibly with an explanation of their similarities; 2)\nAnalogy Explanation Generation ( AEG ): given a\ntarget concept and an analogous source concept,\ngenerate an explanation of their similarities.\nBy noting the similarity of the two tasks defined\nabove to other text generation problems, and being\ninspired by the recent success of using prompted\nPLMs for text generation, we propose analogy\ngeneration by using a PLM with appropriately de-\nsigned prompts. We adopt the promising emerg-\ning paradigm of prompting language models (Liu\net al., 2021) that uses textual prompts with unfilled\nslots and directly leverages the language models to\nfill those slots and obtain the desired output. For\nexample, Table 1 shows sample prompts and PLM-\ngenerated outputs for ACG from our experiments.\nSpecifically, we study the following main re-\nsearch questions: RQ1) How effective is a modern\nPLM such as InstructGPT in generating meaning-\nful analogies? RQ2) How sensitive are the gener-\nated analogies to prompt design, the temperature\nhyperparameter, and spelling errors? RQ3) How\ndoes the model size impact the quality of generated\nanalogies?\nTo study these questions, we design several ex-\nperiments on analogies generated from the Instruct-\nGPT (Ouyang et al., 2022) model. First, we man-\nually validate whether InstructGPT can generate\nmeaningful analogies for ten well-known analo-\ngies in the science domain. Next, we design and\nsystematically vary prompt variants (e.g., imper-\native statements vs. questions) and temperature,\nand investigate the corresponding variations in the\ngenerated text by comparing them to a reference\ndataset of science analogies. Finally, we study the\nimpact of model size on the quality of generated\nanalogies both by automatically comparing against\nthe reference data and using human evaluation.\nOur experimental results show that PLMs\n(specifically, InstructGPT) offer a promising gen-\neral approach to generating analogies with properly\ndesigned prompts. Furthermore, the InstructGPT\nmodel is found to be sensitive to the prompt design,\ntemperature, and spelling errors for this task, par-\nticularly to the prompt style (i.e., question vs. im-\nperative statement). Precise imperative statements\nin low-temperature setting are found to be the best\nprompts. Finally, the quality of the generated analo-\ngies depends heavily on the model size. While\nthe largest model can achieve human-level perfor-\nmance on the ACG task, the smallest model barely\ngenerates any meaningful analogies. The AEG task\nproved to be more challenging based on human\nevaluation and could be a better test of the analogi-\ncal reasoning capabilities of PLMs especially for\nexplaining analogies not seen during training.\n2 Related Work\n2.1 Computational Models of Analogies\nThere has been a lot of work on computational\nmodeling of analogies (Mitchell, 2021). The SME\nmodel (Forbus et al., 2017) is one of the most\npopular symbolic model that finds the mapping,\nor connections between structured representations\nof source and target concepts and their attributes.\nHowever, such methods cannot generate new analo-\ngous source concepts with analogical explanation.\nThe recent deep learning-based approaches,\nincluding using pre-trained language models\n(Mikolov et al., 2013; Rossiello et al., 2019; Ushio\net al., 2021), are able to generate analogies to\nsome extent, but are currently limited to simple\nword-level and proportional analogies, such as (os-\ntrich:bird :: lion:?). In contrast, we aim to generate\nand explain more complex analogies of concepts,\ne.g. instructional analogies (Newby et al., 1995).\nAnother line of work is on finding analogous\ndocuments for scientific innovation, such as prod-\nuct descriptions and research papers, based on their\nsemantic similarities (Kittur et al., 2019). In con-\ntrast, we operate in a generative task setup.\nTo the best of our knowledge, none of the exist-\ning work has studied the problem of automatically\ngenerating complex analogies in natural language.\nRecently, research on more “generative” analogy-\nmaking tasks has been recommended (Mitchell,\n2021). Along this direction, we believe that our\nproposed task is challenging and more practically\nuseful than the existing text-based generative ana-\nlogical tasks including letter-string (e.g., if “abc”\nchanges “abd”), what does “pqrs” change to?) and\nword-level analogies.\n.\n299\n2.2 Prompting Language Models\nRecently, prompts have been either manually cre-\nated or learned to successfully leverage PLMs\nfor several natural language tasks (Liu et al.,\n2021). Our work is closest to prompting for lex-\nical and proportional analogy generation (Ushio\net al., 2021). But, none of the existing work has\nperformed an in-depth study on prompting PLMs\nfor both generating analogous concepts given a sin-\ngle query concept and explaining the analogical\nsimilarities between two query concepts.\n3 Problem Formulation\nMotivated by the practical applications of this task\n(e.g., explaining concepts), we study analogy gen-\neration in the following settings.\n1. Analogous Concept Generation (ACG) or No\nSource (NO_SRC ): Here, only the target concept\nis provided as the input. The goal is to generate\nan analogous source concept or scenario, along\nwith some explanation to justify the analogy. For\nexample, “Explain Bohr’s atomic model using an\nanalogy.”\n2. Analogy Explanation Generation ( AEG ) or\nWith Source (WSRC ): Here, in addition to the tar-\nget, the source concept is also a part of input. The\ngoal is to generate an explanation of how the target\nand source are analogous. For example, “Explain\nhow Bohr’s atomic model is analogous to the solar\nsystem.”\nOur problem setup is similar to the use of PLMs\nfor text generation (Li et al., 2021), and is most\nclosely related to single-relation analogy genera-\ntion (e.g., ostrich : bird :: animal : lion) (Ushio\net al., 2021), where the input is a pair of query con-\ncept (e.g., ostrich : bird), and the task is to choose\nan analogical pair from a pre-defined list of candi-\ndate pairs. But, our proposed task is still different\nin nature and much more challenging (e.g., requir-\ning more creativity in some cases). First, both of\nour inputs and outputs are different. For example,\nin the proposed ACG setup, our input is a single\nconcept (e.g., “bohr’s model”), not a pair of con-\ncepts. Our task is to identify another concept (or\nscenario) that has an equivalence to the query con-\ncept based on their deep and non-trivial semantic\nsimilarities. No previous work has studied this kind\nof “single-concept-based” analogy generation with\npre-trained language models. Even in the proposed\nAEG setup where we also use a pair of concepts\nas input, they are different from the pair used in\nthe previous work. For example, our input could\nbe a pair (e.g., “bohr’s model” and “solar system”)\nand the output is an explanation of their analogi-\ncal relations (e.g., how their structures are similar).\nSecond, we do not have a pre-defined finite list of\ncandidates to choose from, which is a more realis-\ntic and interesting setting than previous work from\napplication perspectives, and is also much more\nchallenging for evaluation.\n4 Experiment Setup\nIn this section, we discuss InstructGPT PLM and\ndatasets used in our experiments.\nInstructGPT Model: Recently, several PLMs\nhave been developed and trained on massive web\ndata (Devlin et al., 2018; Brown et al., 2020; Raffel\net al., 2019). In this study, we probe the aligned\nGPT-3 models, InstructGPT. These are GPT-3 mod-\nels that have been optimized to follow instructions\nbetter (Ouyang et al., 2022). InstructGPT has four\nvariants depending on the model size (number of\nparameters), namely Ada (350 M), Babbage (1.3\nB), Curie (6.7 B) , and Davinci (175 B) 3. Unless\notherwise mentioned, we use the Davinci model\nfor the experiments as it is expected to have the\nbest performance.\nWe used the Open AI API4 to generate all analo-\ngies. Main hyperparameters are described in Sec-\ntion 5.2.2 and rest in the Appendix A\nDataset: As the task of analogy generation, as de-\nfined in this paper, has not been previously studied,\nthere is no existing data set available to use directly\nfor evaluation. We thus opted to create new data\nsets for evaluation. Table 2 shows sample data from\nthese datasets.\nStandard Science Analogies (STD ): As far as we\ncould find, the closest dataset consisting of concep-\ntual analogies is from (Turney, 2008). It consists\nof ten standard science analogies. However, these\nonly contain the source and target concepts but not\nany explanation in natural language.\nScience analogies from academic Q&A sites\nSAQA : We searched for quiz questions that asked\nto create analogies on academic Q&A sites like\nChegg.com, Study.com 5 by using search queries\n3https://blog.eleuther.ai/gpt3-model-sizes/\n4https://beta.openai.com/docs/api-\nreference/completions/create\n5https://chegg.com/, https://study.com/. We manually in-\nspected the data and found no personal identifiers or offensive\ncontent. We manually compiled the datasets, no scraping was\ndone.\n300\nlike ‘create an analogy’, ‘analogy to explain’, and\nmanually downloaded the relevant questions and\nanswers. After manually removing irrelevant data,\n75 unique question-answer pairs were obtained.\nNext, we manually extracted the analogies from\nanswers, i.e., target and source concepts, and the\nexplanation of the analogical similarity.\nThere are total 109 concepts (about high-school\nscience) with 148 English analogies. The average\nword length of analogies is 62.25 words.\nTable 2: Sample analogies from STD and SAQA .\nDataset Target Source Explanation\nSTD atom solar\nsystem\n-\nSAQA ligase sewing\nma-\nchine\n... Ligase is simi-\nlar to a sewing ma-\nchine, as it binds\ntwo elements ...\n(25 words)\n5 Experiment Results\nIn this section, we present our experiment results\nand examine each of the three research questions\nintroduced earlier.\n5.1 Feasibility Analysis\nWe first examine RQ1 and investigate whether In-\nstructGPT is capable of generating analogies with\nsimple prompts by looking at the results on the\nsmaller STD dataset which contains well-known\nanalogies. Here, we seek standard analogies, so\nwe designed prompts with keywords such as \"well-\nknown analogy\", \"often used to explain\", etc. The\nfull list of prompts is in Table 17, Appendix C).\nWe observed that all the prompts were success-\nful in retrieving natural language analogies to some\nextent but they differed in several aspects. Table 1\nshows sample analogies generated by two of our\nprompts (P7 and P2, Table 17) for the target con-\ncept “natural selection.” In this case, the reference\nanswer in the STD dataset is “artifical selection,”\nwhich P7 successfully retrieved, while P2 gener-\nated a different but also valid analogy. Such varia-\ntions indicate both the potential of using different\nprompts to generate (multiple) different analogies\nand the model sensitivity to prompt design, which\nwe further investigate in Section 5.2.\nTo quantify the effectiveness of different\nprompts, we manually evaluated the source con-\ncepts mentioned in the generated analogies (if any).\nTable 3 shows the number of exact matches of gen-\nerated source concepts to those in the referenceSTD\ndataset, along with the number of “valid” source\nconcepts generated. Valid means a reasonable anal-\nogy that is either commonly known (e.g., easily\navailable on the internet 6) or contains a mean-\ningful justification. All prompts generated valid\nanalogies in most cases, even if they didn’t exactly\nmatch the reference source concept further sug-\ngesting the promise of InstructGPT for generating\nmeaningful analogies. Note that the low number\nof exact matches with the reference dataset is ex-\npected to some extent because there are several\npossible “valid” analogies for a given source con-\ncept and so there is a small chance that the model\nwould generate exactly the same analogous concept\nas in the reference.\nTable 3: Number of analogies that match the ground\ntruth or are otherwise meaningful, out of the total ten\nanalogies generated for STD target concepts by the seven\nprompts (P1-P7).\nP1 P2 P3 P4 P5 P6 P7\n# Match 3 3 6 4 3 5 3\n# Valid 6 9 9 8 7 10 10\n5.2 Robustness analyses\nAs observed in many other applications of\nprompted PLMs, the performance of a task tends\nto be sensitive to the prompts used and the temper-\nature parameter (Lu et al., 2021; Zhao et al., 2021).\nMoreover, many PLMs are known to be vulnera-\nble to the presence of spelling errors (Pruthi et al.,\n2019; Ma et al., 2020). Thus, it is important to\nexperiment with variations of both the prompts and\nthe temperature parameter (with frequency_penalty,\nSection 5.2.2 ), and spelling errors and study how\nthey impact the generated analogy (RQ2).\nFor these analyses, we need to compare the\nmodel performance in a large number of configura-\ntions, which makes human evaluation impossible.\nThus, we rely on automatic metrics. Automatic\nevaluation of natural language generation is known\nto be challenging (e.g., long-form question answer-\ning (Krishna et al., 2021)) and automatic metrics\ngenerally have low correlation with human judg-\nment (Callison-Burch et al., 2006; Raffel et al.,\n2019). Evaluation of analogies is even more chal-\n6Note that commonly known does not necessarily mean\navailable on the internet. We use it only as a proxy here since\nthere is no good way to determine what is common knowledge.\n301\nlenging especially because a target concept could\nhave several valid analogies with seemingly differ-\nent meanings (e.g., “artificial selection” vs. “sieve”\nfrom Section 5.1). Thus, before using existing\nmethods, we designed sanity checks and found that\nthose methods behave as we expect (e.g., analogies\nhave a higher score than non-analogies, see Ap-\npendix B). We note that our sanity checks are only\nthe necessary and not the sufficient requirements\nof a good metric for evaluating analogies as they\ndo not evaluate creativity or reasoning. However,\nwe use them as an approximation only for relative\ncomparison between methods on the same task as\nthey are unlikely to favor any single method.\nWe use three representative measures of auto-\nmatic evaluation of generated text: BLEURT (Sel-\nlam et al., 2020) (B), METEOR (Lavie and Agarwal,\n2007) (M), ROUGE -L (R)7 (Lin, 2004)8. BLEURT\n(B) is used as the primary metric for evaluation\nsince it is a recent machine learning-based metric\nthat has been shown to capture semantic similari-\nties between texts (Sellam et al., 2020).\nSimilar average BLEURT values would indicate\nthat the prompts are equally good (or bad) on a\ntask, but not necessarily in the same way. On the\nother hand, Kendall’s Tau (Kendall, 1938) indicates\nhow well the ranks of two variables are correlated.\nThis would suggest that those prompts have similar\nstrengths and weaknesses. Thus, we analyze both\nscores to get a more complete picture of hyperpa-\nrameter sensitivity.\n5.2.1 Analysis of prompts\nTo study the effectiveness and robustness of dif-\nferent prompts for analogy generation in the un-\nsupervised setting, we manually designed several\nprompts for all the problem settings. The different\nprompt variants are all paraphrases that are seman-\ntically similar. The main ways they differ are: 1.\nQuestions vs. Imperative Statements (e.g., P5 vs.\nP2, Table 5); 2. Synonyms (e.g., P2 vs. P3, Table\n5); 3. Word Ordering (e.g., P1 vs. P3, Table 4).\nWe only study the zero-shot setting mainly because\nthe choice/number of examples in few-shot could\nmake an impact on the generated analogies and\nmake it harder to interpret our experiment results.\nPrompts for the NO_SRC and WSRC settings are\nin Tables 4,5, respectively. Here, <target>, <src>\nare target and source concept placeholders.\nOur major findings are as follows:\n7https://pypi.org/project/rouge-score/\n8https://www.nltk.org/api/nltk.translate.meteor_score.html\nTable 4: Prompts for NO_SRC\nId Prompt\nP1 Explain <target> using an analogy.\nP2 Create an analogy to explain <target>.\nP3 Using an analogy, explain <target>.\nP4 What analogy is used to explain <target>?\nP5 Use an analogy to explain <target>.\nTable 5: Prompts for WSRC\nId Prompt\nP1 Explain <target> using an analogy involv-\ning <src>.\nP2 Explain how <target> is analogous to\n<src>.\nP3 Explain how <target> is like <src>.\nP4 Explain how <target> is similar to <src>.\nP5 How is <target> analogous to <src>?\nP6 How is <target> like <src>?\nP7 How is <target> similar to <src>?\nQuestions and statements are significantly dif-\nferent: The question prompts are P4, Table 4 and\nP5-P7, Table 5. From Tables 6 and 7, questions\nhave significantly different and lower scores than\nstatements. This could be an artifact of how the\nInstructGPT models were trained and should be\nfurther investigated.\nTable 6: Comparison of performances of different\nprompts and temperatures in NO_SRC . ∗and †mean\nstatistically significant compared to the best performing\nsetting at p<0.1 and p<0.05 respectively based on a two-\ntailed t-test.\nB R M\nP1tl 0.46 0.187 0.154\nP1th 0.448† 0.181† 0.167\nP2tl 0.451 0.193 0.154\nP2th 0.45∗ 0.184 0.161\nP3tl 0.462 0.196 0.164\nP3th 0.452 0.188 0.171\nP4tl 0.427† 0.170† 0.126†\nP4th 0.431† 0.179† 0.156\nP5tl 0.451 0.188 0.154\nP5th 0.449∗ 0.183∗ 0.163\nImpact of synonyms and word order:Prompt per-\nformances vary based on synonyms and word order.\nFor example, some synonymous prompt pairs (e.g,\nP2-P4, P5-P7 in WSRC ) are more correlated than\nothers (e.g., P2-P3, P5-P6 in WSRC ). This could\nbe because “analogous to” and “similar to” share a\nword unlike the other synonym “like”. As expected,\n302\nprompts with the most different meanings (e.g., P1\nin WSRC – involving <src> is not necessarily the\nsame as analogous to <src>) are least correlated\nwith others. However, from Table 7, the average\nperformances of synonymous prompts (e.g., P2tl\nand P3tl, P2tl and P5tl) are not significantly dif-\nferent. Overall, this suggests that InstructGPT is\nmore robust to synonyms/word-order than to the\nprompt style (question/imperative statements) for\nthis task. The overall best-performing prompts (P3\nin NO_SRC , P2 in WSRC ) contain some form of the\nword “analogy” rather than its synonyms, confirm-\ning that precise and direct prompts are better.\nTable 7: Comparison of performances of different\nprompts and temperatures in WSRC . ∗and †mean sta-\ntistically significant at p<0.1 and p<0.05 compared to\nthe best performing setting respectively based on a two-\ntailed t-test.\nB R M\nP1tl 0.504 0.223 0.187†\nP1th 0.497† 0.212† 0.199\nP2tl 0.515 0.217 0.203\nP2th 0.502∗ 0.210† 0.208\nP3tl 0.504 0.229 0.191\nP3th 0.504 0.216 0.203\nP4tl 0.506 0.214 0.197\nP4th 0.497† 0.206† 0.2\nP5tl 0.499∗ 0.217 0.18†\nP5th 0.496† 0.211† 0.191∗\nP6tl 0.500∗ 0.216 0.176†\nP6th 0.494† 0.212† 0.183†\nP7tl 0.497† 0.208† 0.179†\nP7th 0.492† 0.204† 0.186†\nFigure 1: Kendall’s Tau correlation between BLEURT\nscores of various prompts and temperatures in WSRC\n5.2.2 Analysis of temperature\nHigher temperature increases the randomness in\nthe generated text and is often suggested for cre-\native tasks (Lucy and Bamman, 2021). Since some\nanalogies require creativity, we are especially inter-\nested in studying the impact of this hyperparameter.\nWe explore two settings. Low Temperature (tl):\nthis is a deterministic setting, where temperature =\nfrequency_penalty = presence_penalty = 0. High\nTemperature (th): Here temperature is set to 0.85.\nTo avoid repetition of words and topics, we set\nfrequency_penalty = 1.24 and presence_penalty =\n1.71. These hyperparameters were selected based\non initial qualitative exploration. To account for the\nrandomness, we set best_n = 3, i.e., select the best\nresponse out of three generated responses, and gen-\nerate 5 such best responses. In all experiments, we\nreport the average performance of all 5 responses.\nFrom the results in lower-right corner of Figures\n1 and 3 (in Appendix D), we make the following\ntwo observations:\nFirstly, at high temperature, prompts are gen-\nerally well-correlated with each other suggesting\nlesser sensitivity to prompt design at high tempera-\ntures. This requires further investigation because\nwe expect higher randomness to generate a variety\nof different analogies, and thus have lower correla-\ntions in general.\nSecondly, the overall performances of the high-\ntemperature variants are generally lower than their\nlow-temperature counterparts. To investigate when\nhigh temperature could help, we further looked\ninto a case in the WSRC setting where the high-\ntemperature version of the best prompt, ( P2th),\nperformed much better. The results are shown in\nTable 8. In this case, unlike P2hl, P2tl fails on\nidentifying the target and also generates incorrect\nfacts, (“rubber of your lungs”). This shows some\nevidence of high temperature prompts working bet-\nter for more complex and creative analogies, which\nshould be investigated further in the future.\n5.2.3 Analysis of synthetic spelling errors\nSince obtaining natural spelling errors can be diffi-\ncult, injecting synthetic errors is a common way to\ntest the robustness of models (e.g., (Jayanthi et al.,\n2020). Thus, following previous work (Sakaguchi\net al., 2017; Jayanthi et al., 2020), we injected the\nfollowing four types of character-level errors to\nthe internal characters of the target concept in the\nprompt: Delete (delete one randomly chosen char-\nacter), Permute (switch two randomly chosen ad-\n303\nTable 8: Analogies generated in the high and low-temperature setting by the prompt: Explain how breathing\nmechanism of frogs is analogous to blowing of the balloon.\nTemp. Generated Analogy\nHigh ... When you blow into a balloon, the air pressure ... The frog’s respiratory system works in a\nsimilar way. When you breathe in, you increase the air pressure inside your lungs. This\nincreased air pressure causes the walls ...\nLow When you blow up a balloon, the air inside the balloon is pushing outwards on the rubber of\nthe balloon. This is the same thing that happens when you breathe in: the air inside your\nlungs is pushing outwards on the rubber of your lungs.\njacent characters in the string), Insert (insert one\nrandom alphabet at a random position), and Re-\nplace (replace one randomly chosen character in\nthe string with a random alphabet). Target concepts\nwith length less than 3 were kept unchanged.\nAverage BLEURT scores from three different\nruns for all prompts in the low-temperature set-\nting in NO_SRC are shown in Table 9. Overall, the\nperformance decreases, indicating the sensitivity\nof language models to spelling errors. Further, Re-\nplace generally leads to the biggest performance\ndrop for all prompts (∼3 −7% relative decrease).\nThe model is generally most robust to Insert, simi-\nlar to the results reported in previous work on word\nrecognition using neural networks (Sakaguchi et al.,\n2017).\nTable 9: Impact of injecting Delete (D), Permute (P),\nInsert (I) and Replace (R) errors to the target concept in\nthe prompt compared to the original (O) prompt based\non BLEURT scores. ∗and †mean statistically significant\nat p<0.1 and p<0.05 respectively based on a two-tailed\nt-test.\nD P I R O\nP1 0.438† 0.437† 0.436† 0.429† 0.46\nP2 0.431† 0.434† 0.442 0.427† 0.451\nP3 0.444† 0.445† 0.447∗ 0.44† 0.462\nP4 0.423 0.424 0.428 0.416 0.427\nP5 0.438∗ 0.437∗ 0.441 0.435† 0.451\n5.3 Analysis of model size\nFinally, we examine RQ3, i.e., how does the model\nsize impact the quality of the generated analogies.\nIn general, models with more parameters can be\nexpected to perform better. We now study whether\nthe same holds for this task and how much the\nmodel size impacts the performance.\nFigure 2 shows the BLEURT scores of various\nmodels on both the task setups. As expected, the\nperformance increases significantly with model\nsize in both WSRC and NO_SRC , suggesting that\nlarger models are better at generating analogy-like\ntext for the given targets. Further, the biggest im-\nprovement is seen as the number of parameters in-\ncreases from 0.3B to 1.3B in both settings (19.17%\nand 15.34% relative improvements, respectively).\nSimilar to what we observed in the case of the\n175B Davinci model, the performance in WSRC is\nhigher than that in NO_SRC for other models too.\nThis confirms that all models have some capacity\nto incorporate the source provided in the prompt.\nBLEURT Score\nModel Size\nFigure 2: Average performances of various InstructGPT\nmodels based on BLEURT scores.\n5.4 Human evaluation\nTo further validate the generated analogies more\ncomprehensively, we also conducted human evalu-\nation as described below.\n5.4.1 Annotation Setup\nWe conducted the study on Amazon Mechanical\nTurk. Based on manual evaluation of responses\nto screening tests (Appendix E), we selected 17\nworkers for the main study.\nFurther, we created a sample dataset for evalu-\nating analogies generated both in the NO_SRC and\nWSRC settings. In total, we generated 13k analo-\ngies 9 in NO_SRC and 18k analogies 10 in WSRC .\n96 analogies (5 in high temperature and 1 low temperature)\n*109 target concepts*5 prompts*4 models\n106 analogies (5 in high temperature and 1 low temperature)\n*109 target concepts*7 prompts*4 models\n304\nFrom this data, we randomly selected 42 concepts\nfor the NO_SRC setup and 21 of them were selected\nfor the wsrc setup (to have comparabale number\nof analogies in both settings). The analogies for\nthe selected concepts, generated by all the models\nusing all the prompts in the low temperature setting\nwere selected for evaluation since low temperature\nwas better based on automatic evaluation.\nIn total, 1407 unique analogies (576 from WSRC ,\n770 from NO_SRC , and 61 human-generated from\nSAQA ) were evaluated by 3 workers each, which\nis common in previous work on evaluation of auto-\nmatically generated text (van der Lee et al., 2021).\nThe main study had one question asking workers to\nevaluate whether the shown candidate analogy was\nmeaningful for the target concept (Yes/No/Can’t\ndecide) and provide a text input for explaining their\nchoice (Figure 6, Appendix F). Please refer to Ap-\npendix E for more details of the study design.\n5.4.2 Quantitative Results\nTable 10 shows the percentage of analogies rated\nas meaningful, based on majority vote, for the vari-\nous models and the human references from SAQA .\nThere were <2% ties or cases with ‘Can’t decide’\nas the majority, which were discarded. The Fleiss’\nkappa (Fleiss, 1971) inter-annotator agreement was\n0.347 in case of WSRC (plus human references for\nthe selected concepts for wsrc concepts), indicat-\ning fair agreement and 0.553 in case of NO_SRC\n(plus human references for the selected concepts\nfor NO_SRC concepts) indicating moderate agree-\nment.\nWe observe that the percentage of meaningful\nanalogies increases with model size, again confirm-\ning that larger models have a higher capacity to\ngenerate analogies. Interestingly, in the NO_SRC\nsetting, the largest model has comparable perfor-\nmance to humans. We note that this doesn’t neces-\nsarily mean that those models are creative or have\ncommonsense reasoning skills as they could have\nsimply memorized those analogies, which a known\nproblem of such models (Bender et al., 2021). It\nrequires further research to test whether the models\ngenerate novel analogies unseen during training.\nMoreover, upon inspection, we found that the\nhuman-generated analogies sometimes had minor\nissues, such as grammatical errors, which could\nimpact their rating by annotators. So, it is possible\nthat analogies written by experts, such as science\ninstructors proficient in English, might be rated\nhigher. Nevertheless, these results are quite en-\ncouraging as the model seems to have comparable\nperformance to general online users who wrote the\nanalogies in our reference dataset.\nIn the WSRC setting, the performance of Instruct-\nGPT is lower than human performance. This could\nbe because there is a lesser likelihood of seeing the\nexact same analogy, i.e., the one asked to explain\nin the prompt, during training, compared to seeing\nany analogies for the target concept as required\nin the NO_SRC setting. So, WSRC might require\nmore “analogical reasoning” from the models, es-\npecially for explaining analogies not seen during\ntraining. This highlights the importance of human\nevaluations for such tasks because otherwise, based\non automatic evaluation alone, we would conclude\nthat this is an easier setting. This is because met-\nrics like BLEURT cannot assess the soundness of\nthe generated reasoning.\nWe also compute the NO_SRC performance on\nthe 21 shared concepts (NO_SRC 21 , Table 10) for\na fair comparison between the two settings. It is\ninteresting to note that the performances of smaller\nmodels increase while that of larger models go\ndown in the WSRC setting. This could be because\nthe provided source in the prompt helps provide\nsome guidance to the smaller models. For example,\neven by copying parts of the prompt (i.e., source\nand target), they could generate meaningful analo-\ngies (e.g., <source> is like <target>) in a few cases.\nSince their performance in the NO_SRC setting is\nvery poor, even minor help or “tricks” would lead\nto performance improvement. On the other hand,\nthe larger models that already performed very well,\nlikely do not have much to gain from such help\nand, in fact, perform worse due to the analogical\nreasoning argument made above.\nOverall, this highlights some limitations of the\nInstructGPT model for analogical reasoning, which\nrequires further research for improvement.\n5.4.3 Error Analysis\nThe annotators were also asked to explain their an-\nswer choice (i.e, meaningful analogy or not). By in-\nspection, we identified the following major themes\nbased on the workers’ explanations for choosing\n“not meaningful” across all models/tasks. These\nthemes are not mutually exclusive and multiple\nthemes were often found for one wrong generation.\n1. No Analogy:This is one of the most com-\nmon cases where the model failed to generate any\nanalogy at all. Instead, it mostly generated a simple\ndescription/definition of the target concept. In a\n305\nTable 10: Percentage of meaningful analogies generated\nby various InstructGPT models and humans based on\nhuman evaluation. Highest value per row is underlined.\n0.3B 1.3B 6.7B 175B Human\nNO_SRC 1.90 15.61 48.29 70.05 66.67\nWSRC 8.97 29.05 38.46 53.79 71.88\nNO_SRC 21 0 12.0 47.0 66.99 71.88\nfew cases, it also generated a tautology or an exam-\nple. For example, “The b-lymphocytes are similar\nto the white blood cells. ”\n2. Irrelevant to target:The generated text con-\ntained little to none relevant information pertaining\nto the target. One interesting reason behind this\nwas capitalization for abbreviations. For example,\nsince the targets in the prompt were lowercased\n(e.g., nadh), smaller models were unable to identify\nabbreviations, while the larger models succeeded\nat this. Another reason observed was that of an\nambiguous target, e.g., computer “mouse” misiden-\ntified as a rodent. In more insidious cases, the text\nlooked correct but presented incorrect facts.\n3. Incorrect source or explanation:Here, im-\nportant details about the source concept were either\nincorrect or missing, or the provided explanation\nwas insufficient, making the analogy completely\nwrong or weak at best. For example, “A molecule\nof DNA is like a drop of water. It has a specific\nshape and size, and it can carry the genetic instruc-\ntions for making a particular organism. ”\nSome error types found in other natural language\ngenerations from GPT-3 (Dou et al., 2021), e.g.,\nincoherence and grammar, were also found in our\ntask. Further research is required to quantify them\nfor analogical generation and attempt to fix them.\n6 Limitations\nA major limitation of our study is that we only stud-\nied analogies on a small reference dataset in one\ndomain (high-school science). Our newly created\nreference data sets are relatively small due to lim-\nited resources found online. But, the sample size\nof the automatically generated analogies we eval-\nuated was large ( ∼31k automatically evaluated,\nand ∼1.4k manually evaluated) thereby mitigat-\ning some concerns about bias due to small dataset\nsize. Moreover, as our research questions study an\nopen-ended generation task, having a pre-defined\nlist of reference candidates is not ideal for evalu-\nation. Thus, future research is required to more\nthoroughly evaluate the generated analogies and\ninvestigate the generalizability of the findings to\nother domains.\nFurther, the manual evaluation was conducted\nby a selected group of people in the US and might\nnot reflect the opinions of a more diverse group.\nMoreover, our kappa scores of 0.3-0.5, although\ncommon in previous NLG evaluation work (van der\nLee et al., 2021), are not on the higher end. In gen-\neral, thresholds to determine what counts as high or\nlow kappa scores tend to be open to interpretation\n(van der Lee et al., 2021). Thus, we’ve released\nour annotated and full datasets online, as also sug-\ngested in (van der Lee et al., 2021), and invite other\nresearchers to further investigate them.\n7 Conclusion\nIn this study, we proposed and studied the novel\ntask of generating analogies by prompting Instruct-\nGPT. Our experiments showed that the InstructGPT\nis effective on this task when precise prompts are\nused, thus offering a promising new way to gener-\nate analogies, which can break the limitation of the\ntraditional analogy generation methods in requiring\na pre-generated structured representation.\nBy evaluating the performances of the various\ndesigned prompts in multiple temperature settings\nand in the presence of synthetic spelling errors, we\nfound that the InstructGPT model is sensitive to\nthose variations (e.g., question vs. imperative-style\nprompts). Additionally, based on human evalua-\ntion, we found that the quality of the generated\nanalogies substantially depends on the model size.\nThe largest model was found to achieve human-\nlevel performance at generating analogies for given\ntarget concepts. There is still much room for im-\nprovement at the challenging task of explaining the\nanalogical similarity between the given target and\nsource concepts.\nOur work opens up many exciting opportunities\nfor future work both for application-oriented and\nfoundational research on PLMs for analogy gener-\nation. For example, conducting more robustness\nanalyses based on prompt perturbations (e.g., natu-\nral spelling mistakes, grammar, length, etc.). Also,\nin addition to the unsupervised approaches we ex-\nplored in this paper, it is interesting to develop\nsupervised approaches for this task including by\nfine-tuning PLMs on our created datasets.\n8 Acknowledgments\nThis work is supported in part by the IBM-Illinois\nCenter for Cognitive Computing Systems Research\n(C3SR) as an IBM AI Horizon’s Network.\n306\n9 Ethical Considerations\nThe risks associated with using PLMs for analogy\ngeneration are similar to those of NLG tasks, such\nas bias, toxicity, and misinformation (Bender et al.,\n2021; Weidinger et al., 2021). Accordingly, these\nshould be carefully evaluated before deploying the\nmodels for any practical applications, such as edu-\ncation.\nFurthermore, there is a steep monetary and en-\nvironmental cost associated with using the GPT-\n3 models, especially Davinci. The OpenAI API\ncharges $0.06 /1K tokens. Including early exper-\niments, analogy generation in this study costed a\ntotal of about $240. Since we conducted multiple\nruns with the same prompt account for randomness\n(e.g., in the high temperature setting), the costs rose\nsharply.\nReferences\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of the 2021 ACM Confer-\nence on Fairness, Accountability, and Transparency,\npages 610–623.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nChris Callison-Burch, Miles Osborne, and Philipp\nKoehn. 2006. Re-evaluating the role of bleu in ma-\nchine translation research. In 11th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nYao Dou, Maxwell Forbes, Rik Koncel-Kedziorski,\nNoah A Smith, and Yejin Choi. 2021. Scarecrow:\nA framework for scrutinizing machine text. arXiv\npreprint arXiv:2107.01294.\nJoseph L Fleiss. 1971. Measuring nominal scale agree-\nment among many raters. Psychological bulletin,\n76(5):378.\nKenneth D Forbus, Ronald W Ferguson, Andrew Lovett,\nand Dedre Gentner. 2017. Extending sme to handle\nlarge-scale cognitive modeling. Cognitive Science,\n41(5):1152–1201.\nSai Muralidhar Jayanthi, Danish Pruthi, and Graham\nNeubig. 2020. Neuspell: A neural spelling correction\ntoolkit. arXiv preprint arXiv:2010.11085.\nMaurice G Kendall. 1938. A new measure of rank\ncorrelation. Biometrika, 30(1/2):81–93.\nAniket Kittur, Lixiu Yu, Tom Hope, Joel Chan, Hila\nLifshitz-Assaf, Karni Gilon, Felicia Ng, Robert E\nKraut, and Dafna Shahaf. 2019. Scaling up analogi-\ncal innovation with crowds and ai.Proceedings of the\nNational Academy of Sciences, 116(6):1870–1877.\nKalpesh Krishna, Aurko Roy, and Mohit Iyyer. 2021.\nHurdles to progress in long-form question answering.\narXiv preprint arXiv:2103.06332.\nAlon Lavie and Abhaya Agarwal. 2007. Meteor: An\nautomatic metric for mt evaluation with high levels\nof correlation with human judgments. In Proceed-\nings of the second workshop on statistical machine\ntranslation, pages 228–231.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong\nWen. 2021. Pretrained language models for text gen-\neration: A survey. arXiv preprint arXiv:2105.10311.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2021. Fantastically ordered\nprompts and where to find them: Overcoming\nfew-shot prompt order sensitivity. arXiv preprint\narXiv:2104.08786.\nLi Lucy and David Bamman. 2021. Gender and repre-\nsentation bias in gpt-3 generated stories. In Proceed-\nings of the Third Workshop on Narrative Understand-\ning, pages 48–55.\nWentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shijin\nWang, and Guoping Hu. 2020. Charbert: character-\naware pre-trained language model. arXiv preprint\narXiv:2011.01513.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efficient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nMelanie Mitchell. 2021. Abstraction and analogy-\nmaking in artificial intelligence. arXiv preprint\narXiv:2102.10717.\nTimothy J Newby, Peggy A Ertmer, and Donald A\nStepich. 1995. Instructional analogies and the learn-\ning of concepts. Educational Technology Research\nand Development, 43(1):5–18.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n307\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nDanish Pruthi, Bhuwan Dhingra, and Zachary C Lip-\nton. 2019. Combating adversarial misspellings\nwith robust word recognition. arXiv preprint\narXiv:1905.11268.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nGaetano Rossiello, Alfio Gliozzo, Robert Farrell, Nico-\nlas R Fauceglia, and Michael Glass. 2019. Learning\nrelational representations by analogy using hierarchi-\ncal siamese networks. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long and Short\nPapers), pages 3235–3245.\nKeisuke Sakaguchi, Kevin Duh, Matt Post, and Ben-\njamin Van Durme. 2017. Robsut wrod reocgini-\nton via semi-character recurrent neural network. In\nThirty-first AAAI conference on artificial intelligence.\nThibault Sellam, Dipanjan Das, and Ankur P Parikh.\n2020. Bleurt: Learning robust metrics for text gener-\nation. In Proceedings of ACL.\nPeter D Turney. 2008. The latent relation mapping\nengine: Algorithm and experiments. Journal of Arti-\nficial Intelligence Research, 33:615–655.\nAsahi Ushio, Luis Espinosa-Anke, Steven Schock-\naert, and Jose Camacho-Collados. 2021. Bert is\nto nlp what alexnet is to cv: Can pre-trained lan-\nguage models identify analogies? arXiv preprint\narXiv:2105.04949.\nChris van der Lee, Albert Gatt, Emiel van Miltenburg,\nand Emiel Krahmer. 2021. Human evaluation of\nautomatically generated text: Current trends and best\npractice guidelines. Computer Speech & Language,\n67:101151.\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor\nGriffin, Jonathan Uesato, Po-Sen Huang, Myra\nCheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh,\net al. 2021. Ethical and social risks of harm from\nlanguage models. arXiv preprint arXiv:2112.04359.\nTony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. arXiv\npreprint arXiv:2102.09690.\nA Hyperparameters\nBased on initial explorations, where we varied the\nnumber of maximum tokens between 0 and 1000 in\nincrements of 100, and then from 935-955 in incre-\nments of 1, we noticed that setting a high number of\nmaximum tokens worked better in generating more\ncomprehensive analogies that were not abruptly\ncut-off and there was little sensitivity to higher val-\nues around 950. So, we randomly chose one value\nin that range (939). The default value of top_p = 1\nwas used.\nB Suitability of existing evaluation\nmetrics\nTo first investigate the suitability of existing eval-\nuation metrics for generated analogies before we\ncan trust any evaluation results using them, we de-\nsigned two testers to examine whether the existing\nmetrics behave as expected: 1) Ordering Tester\nOT: This tester is to see if an evaluation metric\ncan order a set of methods that have known orders\nbetween them correctly as expected. 2) Random\nPerturbation TesterRPT : This tester checks if\nan evaluation metric responds to a random pertur-\nbation to the ground truth data used for evaluation.\nA reasonable metric is expected to generate lower\nperformance figures after perturbation.\nWe use those two testers to study the suitability\nof three popular and representative measures of\nautomatic evaluation of generated text: BLEURT\n(Sellam et al., 2020), METEOR (Lavie and Agarwal,\n2007), ROUGE -L (Lin, 2004).\nBLEURT (B) is a recent machine learning-based\nmetric that has been shown to capture semantic\nsimilarities between text. ROUGE -L (R)11 measures\nlongest matching subsequence of words. We use\nits F1-score. METEOR (M) 12 matches word stems\nand synonyms also.\nDesign of testers:We design an OT and a RPT\nbased on the following baseline methods:\nNo Analogy baseline ( NO_ANLGY ): Here, the\nprompts instruct the model to generate an expla-\nnation or description of the target concept and do\nnot ask for an analogy explicitly. Thus, we expect\nthe generated text to be in a different “style” than\nanalogies and the overall performance to be lower.\nHowever, the generation would still contain other\nrelevant keywords describing the target. Thus, it is\na good baseline to test if the metrics can distinguish\nbetween analogies and other descriptions.\nRandom baselines: For each of the three\nsetups, we introduced random baselines\n11https://pypi.org/project/rouge-score/\n12https://www.nltk.org/api/nltk.translate.meteor_score.html\n308\n(NO_ANLGY _RAND , NO_SRC _RAND , and\nWSRC _RAND , respectively) where a generated\nstring is evaluated against a random analogy\n(excluding the correct matching analogy) in\nthe reference dataset (i.e., applying a random\nperturbation to the ground truth). These baselines\npreserve the “style” of the text but not the content.\nWe expect these methods to perform worse than\ntheir non-random counterparts.\nAdditionally, NO_SRC setting is expected to\nperform worse than WSRC because in WSRC , the\nmodel has more information (i.e., the source con-\ncept) and thus has better chances of generating the\ncorrect analogical explanation. Thus, the expected\norder is NO_ANLGY < NO_SRC < WSRC .\nMetric testing results:Table 11 shows the overall\nresults of experiments on the SAQA dataset using\nthe Davinci model. Each row shows the highest\naverage scores given by a metric in various setups\n(performances of each prompt are in Section 5.2\nand at the end of this section.).\nWe can see that all the three metrics order the\nsetups as expected, i.e., random baselines are as-\nsigned a lower score than non-random setups, and\nscores for NO_ANLGY < NO_SRC < WSRC . This\nsuggests that all the three metrics have “passed\"\nour two testers and thus can be reasonably used\nto evaluate whether the automatically generated\nanalogies are similar to those generated by humans.\nIn other words, they should help assess whether\nthe generated text is relevant to the target concept\nand discuss properties of the concept that could\nbe explained using analogies (because they passed\nRPT ), and written in an analogical style (because\nthey passed OT).\nMoreover, the results also indicates that the In-\nstructGPT model is able to follow the prompts\nin the three settings to some extent and generate\nnon-analogical descriptions, general analogies, and\nanalogies containing the source concepts, in those\nsettings respectively.\nIn terms of discernment power, all metrics have\nsmall gaps between the scores of random and non-\nrandom settings. Similar results were previously\nreported in (Krishna et al., 2021) for ROUGE scores\non long-form question-answering. Out of the three\nmetrics, the BLEURT score has the largest gaps\nin all the settings, both between the random and\nnon-random baselines and also between settings. It\nis also shown to capture semantic similarity well\n(Sellam et al., 2020). Thus, we use it as the main\nmetric in the rest of the experiments.\nTable 12: Prompts for NO_ANLGY\nId Prompt\nP1 Explain <target>.\nP2 What is <target>?\nP3 Explain <target> in plain language to a\nsecond grader.\nTable 13: Comparison of performances of different\nprompts and temperatures in NO_ANLGY .\nB R M\nP1tl 0.434 0.183 0.149\nP1th 0.432 0.18 0.158\nP2tl 0.43 0.175 0.129\nP2th 0.425 0.172 0.136\nP3tl 0.445 0.180 0.132\nP3th 0.444 0.179 0.144\nTable 14: Comparison of performances of different\nprompts and temperatures in NO_SRC _RAND .\nB R M\nP1tl 0.375 0.132 0.103\nP1th 0.367 0.123 0.108\nP2tl 0.359 0.116 0.092\nP2th 0.366 0.127 0.105\nP3tl 0.362 0.124 0.099\nP3th 0.364 0.126 0.109\nP4tl 0.338 0.115 0.084\nP4th 0.348 0.121 0.1\nP5tl 0.358 0.121 0.097\nP5th 0.348 0.122 0.107\nTable 15: Comparison of performances of different\nprompts and temperatures in WSRC _RAND .\nB R M\nP1tl 0.37 0.120 0.094\nP1th 0.363 0.122 0.107\nP2tl 0.385 0.117 0.096\nP2th 0.381 0.12 0.109\nP3tl 0.358 0.117 0.095\nP3th 0.359 0.115 0.1\nP4tl 0.367 0.113 0.096\nP4th 0.37 0.115 0.105\nP5tl 0.36 0.113 0.09\nP5th 0.356 0.117 0.094\nP6tl 0.346 0.111 0.086\nP6th 0.347 0.113 0.091\nP7tl 0.353 0.114 0.092\nP7th 0.352 0.109 0.093\n309\nTable 11: Testing results using OT and RPT . The higher score between the random baseline and the non-random\nsetup is bolded. Highest score in a row in underlined.\nNO_ANLGY _RAND NO_ANLGY NO_SRC _RAND NO_SRC WSRC _RAND WSRC\nB 0.349 0.445 0.375 0.462 0.385 0.515\nR 0.122 0.183 0.132 0.196 0.122 0.229\nM 0.099 0.158 0.109 0.171 0.109 0.208\nTable 16: Comparison of performances of different\nprompts and temperatures in NO_ANLGY _RAND .\nB R M\nP1tl 0.346 0.115 0.087\nP1th 0.349 0.122 0.099\nP2tl 0.322 0.116 0.077\nP2th 0.327 0.113 0.081\nP3tl 0.334 0.111 0.079\nP3th 0.336 0.11 0.081\nC Experiments on STD dataset\nTable 17: Prompts for STD analogies\nId Prompt\nP1 Explain <target> using an analogy.\nP2 Explain <target> using a well-known anal-\nogy.\nP3 What analogy is often used to explain <tar-\nget>?\nP4 Using a well-known analogy, explain <tar-\nget>.\nP5 Using an analogy, explain <target>.\nP6 What is a well-known analogy to explain\n<target>?\nP7 What is analogous to <target>?\nTable 18: Most common analogies generated for each\ntarget concept in the STD dataset. #Pmt. means number\nof prompts that generated the shown analogy.\nTarget Most common src. # Pmt.\nmind computer 7\natom solar system 6\nheat transfer fluid/water flow 4\nsounds wave 4\nrespiration combustion 3\nlight river 3\nplanet rock 2\nbacterial mutation game of telephone 3\nnatural selection sieve 2\ngas molecules balls 2\nHigh TemperatureLow Temperature\nLow TemperatureHigh Temperature\nFigure 3: Kendall’s Tau correlation between BLEURT\nscores of various prompts and temperatures in NO_SRC\nD Experiments on SAQA dataset\nTable 19: Comparison of lengths of generated responses\nby question (Q) vs. statement (S) in the WSRC setting.\nQuestion versions of the prompts generate fewer words\non average, than their statement counterparts.\nPrompt Pair Avg. Len. (S) Avg. Len. (Q)\nP2-P5 43.93 34.53\nP3-P6 32.55 31.4\nP4-P7 42.51 32.72\nTable 20: Comparison of lengths of generated responses\nby low and high temperatures in the NO_SRC setting.\nHigh temperature generates consistently longer analo-\ngies. Same trend is observed in other settings also.\nPrompt Avg. Length (tl) Avg. Length (th)\nP1 39.74 47.62\nP2 32.67 40.71\nP3 40.06 46.62\nP4 32.51 40.13\nP5 36.53 38.50\nE Mturk study details\nFor identifying qualified workers on Amazon Me-\nchanical Turk, we designed a pre-screening test\n(Mturk Qualification) asking them to identify the\nmeaningful analogy for a target concept (Figure 4,\nAppendix F). Further, we used the following ad-\nditional qualifications: workers should have com-\n310\npleted at least 5k tasks with >98% approval rate\nand be located in the US since the task requires\nproficiency in english (this way of filtering is not\nperfect but there is currently no good way to iden-\ntify native english speakers via Mturk). We did\nnot collect any other demographic or geographic\ninformation about the workers.\nThose who passed these qualifications worked\non a small test batch of analogies asking detailed\nquestions about their quality (Figure 5, Appendix\nF). The questions consisted of both Likert-style\nor Binary choice questions and text inputs ask-\ning them to explain their choices. We manually\nassessed their responses, especially paying close\nattention to their reasoning to identify qualified\nworkers for the main study.\nFor both the main study and the screening, a\nsimple definition of the target from sites like Simple\nEnglish Wikipedia 13 was provided to workers as\nreference and they were encouraged to refer to the\ninternet to learn more about the shown concepts.\nWe also provided several sample annotations as\npart of the instructions to guide workers. Moreover,\nwe were available to answer clarification questions\nvia a shared chatroom.\nAnnotators were paid at the rate of $50/hr. The\nrate was decided based on open discussions with\nthem and is above the minimum wage. They were\ninformed that the data generated would be used\nfor research purposes. We consulted with our uni-\nversity ethics board and found that IRB was not\nrequired for this study.\nF Human evaluation interface\n13https://simple.wikipedia.org/wiki/Main_Page\n311\nFigure 4: Pre-screening question for identifying qualified workers.\nFigure 5: Sample interface for screening qualified workers.\nFigure 6: Sample interface for human evaluation of the analogies.\n312",
  "topic": "Analogy",
  "concepts": [
    {
      "name": "Analogy",
      "score": 0.813307523727417
    },
    {
      "name": "Computer science",
      "score": 0.6819185018539429
    },
    {
      "name": "Natural language processing",
      "score": 0.47911953926086426
    },
    {
      "name": "Programming language",
      "score": 0.38415828347206116
    },
    {
      "name": "Linguistics",
      "score": 0.35716673731803894
    },
    {
      "name": "Philosophy",
      "score": 0.11243021488189697
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I63190737",
      "name": "University at Buffalo, State University of New York",
      "country": "US"
    }
  ]
}