{
    "title": "Rewire-then-Probe: A Contrastive Recipe for Probing Biomedical Knowledge of Pre-trained Language Models",
    "url": "https://openalex.org/W3205810519",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2512114965",
            "name": "Zaiqiao Meng",
            "affiliations": [
                "University of Glasgow"
            ]
        },
        {
            "id": "https://openalex.org/A2031764755",
            "name": "Fangyu. Liu",
            "affiliations": [
                "University of Glasgow"
            ]
        },
        {
            "id": "https://openalex.org/A108414181",
            "name": "Ehsan Shareghi",
            "affiliations": [
                "University of Glasgow"
            ]
        },
        {
            "id": "https://openalex.org/A2481891025",
            "name": "Yixuan Su",
            "affiliations": [
                "University of Glasgow"
            ]
        },
        {
            "id": "https://openalex.org/A2108358372",
            "name": "Charlotte Collins",
            "affiliations": [
                "University of Glasgow"
            ]
        },
        {
            "id": "https://openalex.org/A2098750953",
            "name": "Nigel Collier",
            "affiliations": [
                "University of Glasgow"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3164540570",
        "https://openalex.org/W2988975212",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2970476646",
        "https://openalex.org/W3173673636",
        "https://openalex.org/W2159583324",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W3201193395",
        "https://openalex.org/W3166846774",
        "https://openalex.org/W3098267758",
        "https://openalex.org/W3167361718",
        "https://openalex.org/W4297808394",
        "https://openalex.org/W3199709353",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3104163040",
        "https://openalex.org/W3100283070",
        "https://openalex.org/W3169283738",
        "https://openalex.org/W2925863688",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W4295288972",
        "https://openalex.org/W2964207259",
        "https://openalex.org/W3213730158",
        "https://openalex.org/W3105066976",
        "https://openalex.org/W4226099034",
        "https://openalex.org/W3155682407",
        "https://openalex.org/W3091432621",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2842511635",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3168090480",
        "https://openalex.org/W2561529111",
        "https://openalex.org/W3172427031",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W3044438666",
        "https://openalex.org/W3166986030",
        "https://openalex.org/W3156170450",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3119164154",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W3152497014",
        "https://openalex.org/W4205450747",
        "https://openalex.org/W3169602049",
        "https://openalex.org/W4304206638",
        "https://openalex.org/W2108325777",
        "https://openalex.org/W2937845937",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3096403953"
    ],
    "abstract": "Knowledge probing is crucial for understanding the knowledge transfer mechanism behind the pre-trained language models (PLMs). Despite the growing progress of probing knowledge for PLMs in the general domain, specialised areas such as biomedical domain are vastly under-explored. To catalyse the research in this direction, we release a well-curated biomedical knowledge probing benchmark, MedLAMA, which is constructed based on the Unified Medical Language System (UMLS) Metathesaurus. We test a wide spectrum of state-of-the-art PLMs and probing approaches on our benchmark, reaching at most 3% of acc@10. While highlighting various sources of domain-specific challenges that amount to this underwhelming performance, we illustrate that the underlying PLMs have a higher potential for probing tasks. To achieve this, we propose Contrastive-Probe, a novel self-supervised contrastive probing approach, that adjusts the underlying PLMs without using any probing data. While Contrastive-Probe pushes the acc@10 to 28%, the performance gap still remains notable. Our human expert evaluation suggests that the probing performance of our Contrastive-Probe is still under-estimated as UMLS still does not include the full spectrum of factual knowledge. We hope MedLAMA and Contrastive-Probe facilitate further developments of more suited probing techniques for this domain.",
    "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 4798 - 4810\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nRewire-then-Probe: A Contrastive Recipe for Probing Biomedical\nKnowledge of Pre-trained Language Models\nZaiqiao Meng♠♦∗ Fangyu Liu♠∗ Ehsan Shareghi♣♠\nYixuan Su♠ Charlotte Collins♠ Nigel Collier♠\n♠Language Technology Lab, University of Cambridge\n♦Department of Computing Science, University of Glasgow\n♣Department of Data Science and AI, Monash University\n♠{zm324, fl399, ys484, cac74, nhc30}@cam.ac.uk\n♣ehsan.shareghi@monash.edu\nAbstract\nKnowledge probing is crucial for understand-\ning the knowledge transfer mechanism behind\nthe pre-trained language models (PLMs). De-\nspite the growing progress of probing knowl-\nedge for PLMs in the general domain, spe-\ncialised areas such as biomedical domain are\nvastly under-explored. To facilitate this, we\nrelease a well-curated biomedical knowledge\nprobing benchmark, MedLAMA, constructed\nbased on the Uniﬁed Medical Language Sys-\ntem (UMLS) Metathesaurus. We test a wide\nspectrum of state-of-the-art PLMs and prob-\ning approaches on our benchmark, reaching at\nmost 3% of acc@10. While highlighting vari-\nous sources of domain-speciﬁc challenges that\namount to this underwhelming performance,\nwe illustrate that the underlying PLMs have a\nhigher potential for probing tasks. To achieve\nthis, we propose C ontrastive-Probe, a novel\nself-supervised contrastive probing approach,\nthat adjusts the underlying PLMs without us-\ning any probing data. While C ontrastive-\nProbe pushes the acc@10 to 24%, the perfor-\nmance gap remains notable. Our human ex-\npert evaluation suggests that the probing per-\nformance of our C ontrastive-Probe is under-\nestimated as UMLS does not comprehensively\ncover all existing factual knowledge. We\nhope MedLAMA and Contrastive-Probe facili-\ntate further developments of more suited prob-\ning techniques for this domain.1\n1 Introduction\nPre-trained language models (PLMs; Devlin et al.\n2019; Liu et al. 2020) have orchestrated incredi-\nble progress on myriads of few- or zero-shot lan-\nguage understanding tasks, by pre-training model\nparameters in a task-agnostic way and transferring\nknowledge to speciﬁc downstream tasks via ﬁne-\ntuning (Brown et al., 2020; Petroni et al., 2021).\n1The data and code implementation are available at\nhttps://github.com/cambridgeltl/medlama.\n∗Equal contribution. This work was done at the Univer-\nsity of Cambridge.\nQuery Answer(s)\nHard Queries\nRiociguathas physiologic eﬀect[Mask]. Vasodilation\nEntecavirmay prevent[Mask]. Hepatitis B\nInvasive Papillary Breast Carcinomadisease mapped to gene[Mask]. [ERBB2 Gene, CCND1 Gene]\nEasy Queries\nPosttraumatic arteriovenous ﬁstulais associated morphology of[Mask]. Traumatic arteriovenousﬁstula\nAcute Myeloid Leukemia with Mutated RUNX1disease mapped to gene[Mask]. RUNX1 Gene\nMagnesium Chloridemay prevent[Mask]. Magnesium Deﬁciency\nTable 1: Example probing queries fromMedLAMA. Bold\nfont denotes UMLS relation.\nTo better understand the underlying knowledge\ntransfer mechanism behind these achievements,\nmany knowledge probing approaches and bench-\nmark datasets have been proposed (Petroni et al.,\n2019; Jiang et al., 2020a; Kassner et al., 2021;\nZhong et al., 2021). This is typically done by for-\nmulating knowledge triples as cloze-style queries\nwith the objects being masked (see Table 1) and\nusing the PLM to ﬁll the single (Petroni et al.,\n2019) or multiple (Ghazvininejad et al., 2019)\n[Mask] token(s) without further ﬁne-tuning.\nIn parallel, it has been shown that specialised\nPLMs (e.g., BioBERT; Lee et al. 2020, Blue-\nBERT; Peng et al. 2019 and PubMedBERT; Gu\net al. 2020) substantially improve the performance\nin several biomedical tasks (Gu et al., 2020). The\nbiomedical domain is an interesting testbed for in-\nvestigating knowledge probing for its unique chal-\nlenges (including vocabulary size, multi-token en-\ntities), and the practical beneﬁt of potentially dis-\nposing the expensive knowledge base construction\nprocess. However, research on knowledge probing\nin this domain is largely under-explored.\nTo facilitate research in this direction, we\npresent a well-curated biomedical knowledge\nprobing benchmark, MedLAMA, that consists of\n19 thoroughly selected relations. Each relation\ncontains 1 k queries (19 k queries in total with at\nmost 10 answers each), which are extracted from\n4798\nID Relation Manual Prompt\n1 disease may have associated disease The disease [X] might have the associated disease [Y] .\n2 gene product plays role in biological processThe gene product [X] plays role in biological process [Y] .\n3 gene product encoded by gene The gene product [X] is encoded by gene [Y] .\n4 gene product has associated anatomy The gene product [X] has the associated anatomy [Y] .\n5 gene associated with disease The gene [X] is associatied with disease [Y] .\n6 disease has abnormal cell [X] has the abnormal cell [Y] .\n7 occurs after [X] occurs after [Y] .\n8 gene product has biochemical function [X] has biochemical function [Y] .\n9 disease may have molecular abnormality The disease [X] may have molecular abnormality [Y] .\n10 disease has associated anatomic site The disease [X] can stem from the associated anatomic site [Y] .\n11 associated morphology of [X] is associated morphology of [Y] .\n12 disease has normal tissue origin The disease [X] stems from the normal tissue [Y] .\n13 gene encodes gene product The gene [X] encodes gene product [Y] .\n14 has physiologic effect [X] has physiologic eﬀect of [Y] .\n15 may treat [X] might treat [Y] .\n16 disease mapped to gene The disease [X] is mapped to gene [Y] .\n17 may prevent [X] may be able to prevent [Y] .\n18 disease may have finding [X] may have [Y] .\n19 disease has normal cell origin The disease [X] stems from the normal cell [Y] .\nTable 2: The 19 relations and their corresponding manual prompts in MedLAMA.\nthe large UMLS (Bodenreider, 2004) biomedical\nknowledge graph and veriﬁed by domain experts.\nWe use automatic metrics to identify the hard ex-\namples based on the hardness of exposing answers\nfrom their query tokens. See Table 1 for a sample\nof easy and hard examples from MedLAMA.\nA considerable challenge in probing in biomed-\nical domain is handling multi-token encoding of\nthe answers (e.g. in MedLAMA only 2.6% of the\nanswers are single-token, while in the English set\nof mLAMA; Kassner et al. 2021, 98% are single-\ntoken), where all existing approaches (i.e., mask\npredict; Petroni et al. 2019, retrieval-based; Dufter\net al. 2021, and generation-based; Gao et al.\n2020) struggle to be e ﬀective.2 For example, the\nmask predict approach (Jiang et al., 2020a) which\nperforms well in probing multilingual knowledge\nachieves less than 1% accuracy on MedLAMA.\nTo address the aforementioned challenge, we\npropose a new method, Contrastive-Probe, that\nﬁrst adjusts the representation space of the under-\nlying PLMs by using a retrieval-based contrastive\nlearning objective (like ‘rewiring’ the switchboard\nto the target appliances Liu et al. 2021c) then re-\ntrieves answers based on their representation sim-\nilarities to the queries. Notably, our C ontrastive-\nProbe does not require using the MLM heads dur-\ning probing, which avoids the vocabulary bias\nacross di ﬀerent models. Additionally, retrieval-\nbased probe is e ﬀective for addressing the multi-\ntoken challenge, as it avoids the need to gener-\nate multiple tokens from the MLM vocabulary.\nWe show that Contrastive-Probe facilitates abso-\n2Prompt-based probing approaches such as Auto-\nPrompt (Shin et al., 2020a), SoftPrompt (Qin and Eisner,\n2021), and OptiPrompt (Zhong et al., 2021) need additional\nlabelled data for ﬁne-tuning prompts, but we restrict the scope\nof our investigation to methods that do not require task data.\nlute improvements of up-to ∼5% and ∼21% on the\nacc@1 and acc@10 probing performance com-\npared with the existing approaches.\nWe further highlight that the elicited knowl-\nedge by Contrastive-Probe is not gained from the\nadditional random sentences, but from the origi-\nnal pre-trained parameters, which echos the pre-\nvious ﬁnding of Liu et al. (2021b); Glavaš and\nVuli´c (2021); Su et al. (2021, 2022). Addition-\nally, we demonstrate that diﬀerent state-of-the-art\nPLMs and transformer layers are suited for diﬀer-\nent types of relational knowledge, and diﬀerent re-\nlations requires diﬀerent depth of tuning, suggest-\ning that both the layers and tuning depth should\nbe considered when infusing knowledge over dif-\nferent relations. Furthermore, expert evaluation of\nPLM responses on a subset of MedLAMA highlights\nthat expert-crafted resources such as UMLS still\ndo not include the full spectrum of factual knowl-\nedge, indicating that the factual information en-\ncoded in PLMs is richer than what is reﬂected by\nthe automatic evaluation.\nThe ﬁndings of our work, along with the pro-\nposed MedLAMA and Contrastive-Probe, highlight\nboth the unique challenges of the biomedical do-\nmain and the unexploited potential of PLMs. We\nhope our research to shed light on what domain-\nspecialised PLMs capture and how it could be bet-\nter resurfaced, with minimum cost, for probing.\n2 MedLAMA\nTo facilitate research of knowledge probing in\nthe biomedical domain, we create the MedLAMA\nbenchmark based on the largest biomedical knowl-\nedge graph UMLS (Bodenreider, 2004). UMLS 3\n3Release version 2021AA: https://download.nlm.\nnih.gov/umls/kss/2021AA/umls-2021AA-full.zip\n4799\n1 2 3 4 5 6 7 8 9 10111213141516171819\nRelation ID\n0\n500\n1000Count\nset\nfull\nhard\n1 2 3 4 5 6 7 8 9>9\nNumber of Tokens\n0\n10\n20Percent\nFigure 1: Left: Count over full and hard sets. Right:\nPercentage of answers over number of tokens.\nis a comprehensive metathesaurus containing 3.6\nmillion entities and more than 35.2 million knowl-\nedge triples over 818 relation types which are\nintegrated from various ontologies, including\nSNOMED CT, MeSH and the NCBI taxonomy.\nCreating a LAMA-style (Petroni et al., 2019)\nprobing benchmark from such a knowledge graph\nposes its own challenges: (1) UMLS is a col-\nlection of knowledge graphs with more than 150\nontologies constructed by di ﬀerent organisations\nwith very di ﬀerent schemata and emphasis; (2)\na signiﬁcant amount of entity names (from cer-\ntain vocabularies) are unnatural language (e.g.,\nt(8;21)(q22;q22) denoting an observed karyotypic\nabnormality) which can hardly be understood by\nthe existing PLMs, with tokenisation tailored for\nnatural language; (3) some queries (constructed\nfrom knowledge triples) can have up to hundreds\nof answers (i.e., 1-to-N relations), complicating\nthe interpretation of probing performance; and (4)\nsome queries may expose answers in themselves\n(e.g., answer within queries), making it challeng-\ning to interpret relative accuracy scores.\nSelection of Relationship Types. In order to\nobtain high-quality knowledge queries, we con-\nducted multiple rounds of manual ﬁltering on the\nrelation level to exclude uninformative relations or\nrelations that are only important in the ontolog-\nical context but do not contain interesting seman-\ntics as a natural language (e.g, taxonomy and mea-\nsurement relations). We also excluded relations\nwith insuﬃcient triples/entities. Then, we manu-\nally checked the knowledge triples for each rela-\ntion to ﬁlter out those that contain unnatural lan-\nguage entities and ensure that their queries are se-\nmantically meaningful. Additionally, in the cases\nof 1-to-N relations where there are multiple gold\nanswers for the same query, we constrained all the\nqueries to contain at most 10 gold answers. These\nsteps resulted in 19 relations with each containing\n1k randomly sampled knowledge queries. Table 2\nshows the detailed relation names and their corre-\nsponding prompts.\nEasy vs. Hard Queries.Recent works (Poerner\net al., 2020; Shwartz et al., 2020) have discovered\nApproach Type Answer space MLM\nFill-mask (Petroni et al., 2019) MP PLM V ocab\u0013\nX-FACTR (Jiang et al., 2020a) MP PLM V ocab\u0013\nGenerative PLMs (Lewis et al., 2020) GB PLM V ocab\u0017\nMask average (Kassner et al., 2021) RB KG Entities\u0013\nContrastive-Probe(Ours) RB KG Entities \u0017\nTable 3: Comparison of di ﬀerent approaches. Types\nof probing approaches: Mask predict (MP), Retrieval-\nbased (RB) and Generation-based (GB).\nthat PLMs are overly reliant on the surface form\nof entities to guess the correct answer of a knowl-\nedge query. The PLMs “cheat” by detecting lex-\nical overlaps between the query and answer sur-\nface forms instead of exercising their abilities of\npredicting factual knowledge. For instance, PLMs\ncan easily deal with the triple <Dengue virus live\nantigen CYD serotype 1, may-prevent, Dengue>\nsince the answer is part of the query. To miti-\ngate such bias, we also create a hard query set\nfor each relation by selecting a subset of their cor-\nresponding 1k queries using token and matching\nmetrics (i.e., exact matching and ROUGE-L (Lin\nand Och, 2004)). For more details see the Ap-\npendix. We refer to the ﬁnal ﬁltered and original\nqueries as the hard setsand full sets, respectively.\nFigure 1 (left) shows the count of hard vs. full sets.\nThe Multi-token Issue. One of the key chal-\nlenges for probing MedLAMA is the multi-token de-\ncoding of its entity names. In MedLAMA there are\nonly 2.6% of the entity names that are single-\ntoken4 while in the English set of mLAMA (Kass-\nner et al., 2021) and LAMA (Petroni et al., 2019)\nthe percentage of single-token answers are 98%\nand 100%, respectively. Figure 1 (right) shows the\npercentage of answers by diﬀerent token numbers.\n3 Existing Multi-token Knowledge\nProbing Approaches\nWhile the pioneer works in PLM knowledge prob-\ning mainly focused on the single-token entities,\nmany recent works have started exploring the so-\nlutions for the multi-token scenario (Kassner et al.,\n2021; Jiang et al., 2020a; De Cao et al., 2021).\nThese knowledge probing approaches can be cat-\negorised, based on answer search space and re-\nliance on MLM head, into three categories: mask\npredict, generation-based, and retrieval-based.\nTable 3 summarises their key diﬀerences.\nMask Predict.Mask predict (Petroni et al., 2019;\nJiang et al., 2020a) is one of the most commonly\n4Tokenized by Bert-base-uncased.\n4800\nHIV + In + ##fect + ##ions\n0.45 \n0.2 \n0.6 \n0.35shock\n(d)Contrastive probe(b) Generative PLMs(a)Mask predict\nDextran 40 may \ntreat [Mask] .\nElvitegravir may prevent \n[Mask][Mask][Mask][Mask] .\nSingle-token Multi-token\nEpistaxis\nBERT\nBART/T5\nElvitegravir may prevent [Mask]\n<s>    HIV    In    ##fect   ##ions\nBERT\n[CLS] Elvitegravir \nmay prevent [Mask].\nBERT\nElvitegravir may prevent [Mask].\nBERT\nMLM\nMLM\nMLM\nNasal Mass\nPain\nEpistaxis \nRhinorrhea\n…\nEntities\nAutoregressive\nDecoder\n(c)Mask average\nNasal Mass\nPain\nEpistaxis  \nRhinorrhea\n…\nEntities\nEpistaxis\nQuery \nembedding\nEntity \nembeddings\n…\n[CLS]\nMulti-token \nDecoding\nNearest \nNeighbor\nSearch\nFigure 2: Comparison of diﬀerent probing approaches. (d) is our proposed Contrastive-Probe.\nused approaches to probe knowledge for masked\nPLMs (e.g. BERT). The mask predict approach\nuses the MLM head to ﬁll a single mask token\nfor a cloze-style query, and the output token is\nsubjected to the PLM vocabulary (Petroni et al.,\n2019). Since many real-world entity names are\nencoded with multiple tokens, the mask predict\napproach has also been extended to predict multi-\ntoken answers using the conditional masked lan-\nguage model (Jiang et al., 2020a; Ghazvininejad\net al., 2019). Figure 2(a) shows the prediction pro-\ncess. Speciﬁcally, given a query, the probing task\nis formulated as: 1) ﬁlling masks in parallel in-\ndependently (Independent); 2) ﬁlling masks from\nleft to right autoregressively (Order); 3) ﬁlling to-\nkens sorted by the maximum conﬁdence greed-\nily ( Conﬁdence). After all mask tokens are re-\nplaced with the initial predictions, the predictions\ncan be further reﬁned by iteratively modifying one\ntoken at a time until convergence or until the max-\nimum number of iterations is reached (Jiang et al.,\n2020a). For example, Order+Order represents\nthat the answers are initially predicted by Order\nand then reﬁned by Order. In this paper we exam-\nined two of these approaches, i.e.Independent and\nOrder+Order, based on our initial exploration.\nGeneration-based. Recently, many generation\nbased PLMs have been presented for text gener-\nation tasks, such as BART (Lewis et al., 2020) and\nT5 (Raﬀel et al., 2020). These generative PLMs\nare trained with a de-noising objective to restore\nits original form autoregressively (Lewis et al.,\n2020; Raﬀel et al., 2020). Such an autoregressive\ngeneration process is analogous to theOrder prob-\ning approach, thus the generative PLMs can be\ndirectly used to generate answers for each query.\nSpeciﬁcally, we utilize the cloze-style query with\na single [M ask] token as the model input. The\nmodel then predicts the answer entities that cor-\nrespond to the [M ask] token in an autoregressive\nmanner. An illustration is provided in Figure 2(b).\nRetrieval-based. Mask predict and Generation-\nbased approaches need to use the PLM vocabulary\nas their search spaces for answer tokens, which\nmay generate answers that are not in the answer\nset. In particular, when probing the masked PLMs\nusing their MLM heads, the predicted result might\nnot be a good indicator for measuring the amount\nof knowledge captured by these PLMs. This is\nmainly because the MLM head will be eventually\ndropped during the downstream task ﬁne-tuning\nwhile the MLM head normally accounts for more\nthan 20% of the total PLM parameters. Alterna-\ntively, the retrieval-based probing (Dufter et al.,\n2021; Kassner et al., 2021) are applied to address\nthis issue. Instead of generating answers based on\nthe PLM vocabulary, the retrieval-based approach\nﬁnds answers by ranking the knowledge graph\ncandidate entities based on the query and entity\nrepresentations, or the entity generating scores.\nTo probe PLMs on MedLAMA, we use mask aver-\nage (Kassner et al., 2021), an approach that takes\nthe average log probabilities of entity’s individual\ntokens to rank the candidates. The retrieval-based\napproaches address the multi-token issue by re-\nstricting the output space to the valid answer set\nand can be used to probe knowledge in di ﬀerent\ntypes of PLMs (e.g. BERT vs. fastText; Dufter\net al. 2021). However, previous works (Kassner\net al., 2021; Dufter et al., 2021) only report results\nbased on the type-restricted candidate set (e.g. re-\nlation) which we observed to decay drastically un-\nder the full entity set.\n4 C ontrastive-Probe: Cloze-style Task as\na Self-retrieving Game\nTo better transform the PLM encoders for the\ncloze-style probing task, we propose Contrastive-\n4801\nProbe which pre-trains on a small number\nof sentences sampled from the PLM’s origi-\nnal pre-training corpora with a contrastive self-\nsupervising objective, inspired by the Mirror-\nBERT (Liu et al., 2021b). Our contrastive pre-\ntraining does not require the MLM head or any ad-\nditional external knowledge, and can be completed\nin less than one minute on 2 ×2080Ti GPUs.\nSelf-supervised Contrastive Rewiring.We ran-\ndomly sample a small set of sentences (e.g. 10k,\nsee §5.2 for stability analysis of C ontrastive-\nProbe on several randomly sampled sets), and re-\nplace their tail tokens (e.g. the last 50% exclud-\ning the full stop) with a [Mask] token. Then these\ntransformed sentences are taken as the queries of\nthe cloze-style self-retrieving game. In the follow-\ning we show an example of transforming a sen-\ntence into a cloze-style query:\nSentence: Social-distancing largely reduces coron-\navirus infections.\nQuery: Social-distancing largely [Mask].\nwhere “reduces coronavirus infections” is marked\nas a positive answer of this query.\nGiven a batch, the cloze-style self-retrieving\ngame is to ask the PLMs to retrieve the positive an-\nswer from all the queries and answers in the same\nbatch. Our C ontrastive-Probe tackles this by op-\ntimising an InfoNCE objective (Oord et al., 2018),\nL= −\nN∑\ni=1\nlog exp(cos( f (xi), f (xp))/τ)∑\nxj∈Ni\nexp(cos( f (xi), f (xj))/τ)\n, (1)\nwhere f (·) is the PLM encoder (with the MLM\nhead chopped-oﬀ and [CLS] as the contextual rep-\nresentation), N is batch size, xi and xp are from\na query-answer pair (i.e., xi and xp are from the\nsame sentence), Ni contains queries and answers\nin the batch, and τis the temperature. This objec-\ntive function encourages f to create similar rep-\nresentations for any query-answer pairs from the\nsame sentence and dissimilar representations for\nqueries/answers belonging to diﬀerent sentences.\nRetrieval-based Probing. For probing step, the\nquery is created based on the prompt-based tem-\nplate for each knowledge triple , as shown in the\nfollowing:\nTriple: <Elvitegravir, may-prevent, Epistaxis>\nQuery: Elvitegravir may prevent [Mask].\nand we search for nearest neighbours from all the\nentity representations encoded by the same model.\nApproach PLM Full Set\nacc@1 acc@10\nGenerative PLMs\nBART-base 0.16 1.39\nSciFive-base 0.53 2.02\nSciFive-large 0.55 2.03\nT5-small 0.70 1.72\nT5-base 0.06 0.19\nX-FACTR(Conﬁdence)\nBERT 0.05 -\nBlueBERT 0.74 -\nBioBERT 0.17 -\nX-FACTR(Order+Order)\nBERT 0.06 -\nBlueBERT 0.50 -\nBioBERT 0.11 -\nMask average\nBERT 0.06 0.73\nBlueBERT 0.05 1.39\nBioBERT 0.28 3.03\nContrastive-Probe(Ours)\nBERT 1.95 6.96\nBlueBERT 4.87 19.87\nBioBERT 3.28 15.46\nPubMedBERT5.71 24.31\nTable 4: Performance of di ﬀerent probing approaches\non the full set of MedLAMA. Since the MLM head of\nPubMedBERT is not available, the mask predict and\nmask average approaches cannot be applied. Best re-\nsults are in bold and the second bests are underlined.\n5 Experiments\nIn this section we conduct extensive experiments\nto verify whether C ontrastive-Probe is eﬀective\nfor probing biomedical PLMs. First, we experi-\nment with C ontrastive-Probe and existing prob-\ning approaches on MedLAMA benchmark (§5.1).\nThen, we conduct in-depth analysis of the stability\nand applicability of C ontrastive-Probe in prob-\ning biomedical PLMs (§5.2). Finally, we report an\nevaluation of a biomedical expert on the probing\npredictions and highlight our ﬁndings (§5.3).\nContrastive-Probe Rewiring. We train our\nContrastive-Probe based on 10k sentences which\nare randomly sampled from the PubMed texts5 us-\ning a mask ratio of 0.5. The best hyperparameters\nand their tuning options are provided in Appendix.\nProbing Baselines. For the mask predict ap-\nproach, we use the original implementation of X-\nFACTR (Jiang et al., 2020a), and set the beam size\nand the number of masks to 5. Both mask pre-\ndict and retrieval-based approaches are tested un-\nder both the general domain and biomedical do-\nmain BERT models, i.e. Bert-based-uncased (De-\nvlin et al., 2019), BlueBERT (Peng et al., 2019),\nBioBERT (Lee et al., 2020), PubMedBERT (Gu\net al., 2020).6 For generation-based baselines, we\ntest ﬁve PLMs, namely BART-base (Lewis et al.,\n5We sampled the sentences from a PubMed corpus used\nin the pre-training of BlueBERT (Peng et al., 2019).\n6The MLM head of PubMedBERT is not publicly avail-\nable and cannot be evaluated by X-FACTR andmask average.\n4802\n2020), T5-small and T5-base (Ra ﬀel et al., 2020)\nthat are general domain generation PLMs, and\nSciFive-base & SciFive-large (Phan et al., 2021)\nthat are pre-trained on large biomedical corpora.\n5.1 Benchmarking on MedLAMA\nComparing Various Probing Approaches.Ta-\nble 4 shows the overall results of various probing\nbaselines on MedLAMA. It can be seen that the per-\nformances of all the existing probing approaches\n(i.e. generative PLMs, X-FACTR and mask pre-\ndict) are very low ( <1% for acc@1 and <4% for\nacc@10) regardless of the underlying PLM, which\nare not e ﬀective indicators for measuring knowl-\nedge captured. In contrast, our C ontrastive-\nProbe obtains absolute improvements by up-to ∼\n5% and ∼21% on acc@1 and acc10 respectively\ncomparing with the three existing approaches,\nwhich validates its eﬀectiveness on measuring the\nknowledge probing performance. In particular,\nPubMedBERT model obtains the best probing per-\nformance (5.71% in accuracy) for these biomedi-\ncal queries, validating its e ﬀectiveness of captur-\ning biomedical knowledge comparing with other\nPLMs (i.e. BERT, BlueBERT and BioBERT).\nBenchmarking with Contrastive-Probe. To fur-\nther examine the eﬀectiveness of PLMs in captur-\ning biomedical knowledge, we benchmarked sev-\neral state-of-the-art biomedical PLMs (including\npure pre-trained and knowledge-enhanced mod-\nels) on MedLAMA through our Contrastive-Probe.\nTable 5 shows the probing results over the full\nand hard sets. In general, we can observe that\nthese biomedical PLMs always perform better\nthan general-domain PLMs (i.e., BERT). Also,\nwe observe the decay of performance of all these\nmodels on the more challenging hard set queries.\nWhile PubMedBERT performs the best among all\nthe pure pre-trained models, SapBERT (Liu et al.,\n2021a) and CoderBERT (Yuan et al., 2020) (which\nare the knowledge infused PubMedBERT) further\npush performance to 8% and 30.41% on acc@1\nand acc@10 metrics respectively, highlighting the\nbeneﬁts of knowledge infusion pre-training.\nComparison per Answer Length.Since diﬀerent\nPLMs use diﬀerent tokenizers, we use char length\nof the query answers to split MedLAMA into dif-\nferent bins and test the probing performance over\nvarious answer lengths. Figure 3 shows the re-\nsult. We can see that the performance of retrieval-\nbased probing in C ontrastive-Probe increases as\nModel acc@1/acc@10\nFull Set Hard Set\nBERT (Devlin et al., 2019) 1.95±0.40/6.96±0.96 0.67±0.19/3.27±0.54BlueBERT (Peng et al., 2019) 4.87±0.43/19.87±0.62 4.12±0.46/18.18±0.77BioBERT (Lee et al., 2020) 3.28±0.20/15.46±0.93 2.14±0.23/12.59±1.19ClinicalBERT (Alsentzer et al., 2019) 1.83±0.15/8.64±0.79 0.71±0.13/5.45±1.06SciBERT (Beltagy et al., 2019) 3.64±0.33/18.11±1.95 2.14±0.30/14.64±2.01PubMedBERT (Gu et al., 2020) 5.71±0.58/24.31±1.29 4.49±0.49/21.74±1.21\nUmlsBERT (Yuan et al., 2020) 2.94±0.21/11.64±0.46 1.80±0.11/7.75±0.42SapBERT (Liu et al., 2021a) 7.80±0.38/30.41±1.23 5.15±0.27/26.09±1.17CoderBERT (Michalopoulos et al., 2021)8.00±0.60/26.41±1.086.08±0.52/22.69±1.10\nTable 5: Benchmarking biomedical PLMs on\nMedLAMA (Full and Hard) via Contrastive-Probe. The\nbottom panel are knowledge-enhanced PLMs. The av-\nerage performance and their standard deviation are re-\nported based on rewiring over 10 diﬀerent random sets.\n1 10 20 30 40 >50\nNumber of chars\n2\n4\n6acc@1 in %\nContrastive-probe\n1 10 20 30 40 >50\nNumber of chars\n0.25\n0.50\n0.75\n1.00\n1.25\nMask predict\nModel\nBluebert\nBioBert\nFigure 3: Performance over answer lengths.\nthe answer length increase while the performance\nof mask predict dropped signiﬁcantly. This result\nvalidates that our C ontrastive-Probe (retrieval-\nbased) are more reliable at predicting longer an-\nswers than the mask predict approach since the lat-\nter heavily relies on the MLM head.7\n5.2 In-depth Analysis of Contrastive-Probe\nSince our C ontrastive-Probe involves many hy-\nperparameters and stochastic factors during self-\nretrieving pre-training, it is critical to verify if it\nbehaves consistently under (1) diﬀerent randomly\nsampled sentence sets; (2) di ﬀerent types of rela-\ntions; and (3) diﬀerent pre-training steps.\nStability of Contrastive-Probe. To conduct this\nveriﬁcation, we sampled 10 di ﬀerent sets of 10k\nsentences from the PubMed corpus 8 and probed\nthe PubMedBERT model using our C ontrastive-\nProbe on the full set. Figure 4 shows the acc@1\nperformance over top 9 relations and the micro\naverage performance of all the 19 relations. We\ncan see that the standard deviations are small and\nthe performance over di ﬀerent sets of samples\nshows the similar trend. This further highlights\n7For the single-token answer probing scenario,\nContrastive-Probe does not outperform the mask pre-\ndict approach, particularly in the general domain. This is\nexpected since most of the masked PLMs are pre-trained by\na single-token-ﬁlling objective.\n8The tuning corpus itself is unimportant, since we can ob-\ntain the similar results even using Wikipedia.\n4803\n100 200 300 400 500\nsteps\n0\n5\n10\n15\n20acc@1\n100 200 300 400 500\nsteps\n10\n20\n30\n40\n50\n60acc@10\nmicro average (all 19 relations)\ngene product plays role in biological process\ngene product encoded by gene\noccurs after\ndisease may have molecular abnormality\nassociated morphology of\ndisease has normal tissue origin\ngene encodes gene product\nmay prevent\ndisease has normal cell origin\nFigure 4: Performance over training steps on full set.\nThe shaded regions are the standard deviations.\nthat the probing success of C ontrastive-Probe is\nnot due the selected pre-training sentences. In-\ntuitively, the contrastive self-retrieving game (§4)\nis equivalent to the formulation of the cloze-style\nﬁlling task, hence tuning the underlying PLMs\nmakes them better suited for knowledge elicita-\ntion needed during probing (like ‘rewiring’ the\nswitchboards). Additionally, from Figure 4 we\ncan also observe that di ﬀerent relations exhibit\nvery diﬀerent trends during pre-training steps of\nContrastive-Probe and peak under diﬀerent steps,\nsuggesting that we need to treat di ﬀerent types of\nrelational knowledge with di ﬀerent tuning depths\nwhen infusing knowledge. We leave further explo-\nration of this to future work.\nProbing by Relations. To further analyse the\nprobing variance over di ﬀerent relations, we also\nplot the probing performance of various PLMs\nover diﬀerent relations of MedLAMA in Figure 5.\nWe can observe that di ﬀerent PLMs exhibit dif-\nferent performance rankings over di ﬀerent types\nof relational knowledge (e.g. BlueBERT peaks at\nrelation 12 while PubMedBERT peaks at relation\n3). This result demonstrates that di ﬀerent PLMs\nare suited for di ﬀerent types of relational knowl-\nedge. We speculate this to be reﬂective of their\ntraining corpora.\nProbing by Layer. To investigate how much\nknowledge is stored in each Transformer layer,\nwe chopped the last layers of PLMs and applied\nContrastive-Probe to evaluate the probing perfor-\nmance based on the ﬁrst L ∈{3,5,7,9,11,12}lay-\ners on MedLAMA. In general, we can see in Fig-\nure 6 that the model performance drops signiﬁ-\ncantly after chopping the last 3 layers, while its\naccuracy is still high when dropping only last one\nlayer. In Figure 7, we further plot the layer-wise\nprobing performance of PubMedBERT over dif-\nferent relations. Surprisingly, we ﬁnd that di ﬀer-\nent relations do not show the same probing per-\nformance trends over layers. For example, with\nonly the ﬁrst 3 layers, PubMedBERT achieves the\nbest accuracy (>15%) on relation 11 queries. This\nresult demonstrates that both relation types and\nPLM layers are confounding variables in captur-\ning factual knowledge, which helps to explain the\ndiﬀerence of training steps over relations in Fig-\nure 4. This result also suggests that layer-wise\nand relation-wise training could be the key to ef-\nfectively infuse factual knowledge for PLMs.\n5.3 Expert Evaluation on Predictions\nTo assess whether the actual probing performance\ncould be possibly higher than what is reﬂected\nby the commonly used automatic evaluation, we\nconducted a human evaluation on the prediction\nresult. Speciﬁcally, we sample 15 queries and\npredict their top-10 answers using C ontrastive-\nProbe based on PubMedBERT and ask the asses-\nsor9 to rate the predictions on a scale of [1,5]. Fig-\nure 8 shows the confusion matrices.10 We observe\nthe followings: (1) There are 3 UMLS answers\nthat are annotated with score level 1-4 (precisely,\nlevel 3), which indicates UMLS answers might not\nalways be the perfect answers. (2) There are 20\nannotated perfect answers (score 5) in the top 10\npredictions that are not marked as the gold an-\nswers in the UMLS, which suggests the UMLS\ndoes not include all the expected gold knowledge.\n(3) In general, PubMedBERT achieves an 8.67%\n(13/150) acc@10 under gold answers, but under\nthe expert annotation the acc@10 is 22% (33/150),\nwhich means the probing performance is higher\nthan what evaluated using the automatically ex-\ntracted answers.\nBenchmark # Rel. # Queries Avg. # Answer % Single-Tokens\nLAMA 41 41k 1 100%\nBioLAMA 36 49k 1 2.2%\nMedLAMA 19 19k 2.3 2.6%\nTable 6: Statistics comparison among LAMA, Bio-\nLAMA and our MedLAMA.\n5.4 Comparing with BioLAMA\nDuring the writing of this work, we noticed a con-\ncurrent work to ours that also released a biomed-\nical knowledge probing benchmark, called Bio-\nLAMA Sung et al. (2021). In Table 6, we com-\n9A senior Ph.D. graduate in Cell Biology.\n10In the Appendix, we provide examples with their UMLS\ngold answers, human annotated answers and probing predic-\ntions of diﬀerent probing approaches.\n4804\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19\nRelation ID\n0\n10\n20acc@1\nmodel\nBERT-base\nSciBERT\nPubMedBERT\nBioBERT\nBlueBERT\nFigure 5: Performance of PLMs over diﬀerent relations.\n3 5 7 9 11 12\nlayer\n0\n2\n4\n6\n8marco acc@1\nmodel\nBlueBERT\nBioBERT\nPubMedBERT\n3 5 7 9 11 12\nlayer\n0\n2\n4\n6macro acc@1\nset\nHard\nFull\nFigure 6: Performance over diﬀerent layers.\npare MedLAMA with LAMA (Petroni et al., 2019)\nand BioLAMA in terms of data statistics. We\nfound that there is only 1 overlapped relation (i.e.,\nmay treat) between BioLAMA and MedLAMA, and\nno overlap exists on the queries. We can see\nthat, without additional training data from the\nbiomedical knowledge facts, C ontrastive-Probe\nreaches a promising performance compared with\nOptiPrompt approach, which needs further train-\ning data. Additionally, since Mask Predict and\nOptiPrompt require using the MLM head, it is im-\npossible to compare a model without MLM head\nbeing released (e.g. PubMedBERT). In contrast,\nour Contrastive-Probe not only provides a good\nindicator of comparing these models in terms of\ntheir captured knowledge, but also makes layer-\nwise knowledge probing possible.\n5.5 Limitations of Contrastive-Probe\nHow to early stop?For fair comparison of diﬀer-\nent PLMs, we currently use checkpoints after con-\ntrastive tuning for a ﬁxed number of steps (200,\nspeciﬁcally). However, we have noticed that dif-\nferent models and di ﬀerent probing datasets have\ndiﬀerent optimal training steps. To truly ‘rewire’\nthe most knowledge out of each PLMs, we need\na uniﬁed validation set for checkpoint selection.\nWhat the validation set should be and how to guar-\nantee its fairness require further investigation.\nPerformance not very stable.We have noticed\nthat using di ﬀerent contrastive tuning corpus as\nwell as diﬀerent random seeds can lead to a certain\nvariance of their probing performances (see Table\n5). To mitigate such issue, we use average perfor-\nProbe Model CTD wikidata UMLS\nacc@1 acc@5 acc@1 acc@5 acc@1 acc@5\nMask PredictBERT 0.06 1.20 1.16 6.04 0.82 1.99BioBERT 0.42 3.25 3.67 11.20 1.16 3.82Bio-LM 1.17 7.3011.9725.92 3.44 8.88\nOptiPromptBERT 3.56 6.97 3.29 8.13 1.44 3.65BioBERT4.82 9.74 4.21 12.91 5.0813.28Bio-LM 2.9910.1910.6025.158.25 20.19\nContrastive-ProbeBlueBERT 1.62 5.84 6.64 25.972.63 11.46BioBERT 0.20 0.99 1.04 4.51 0.89 3.89Bio-LM 1.70 4.26 4.32 18.74 1.27 5.01PubMedBERT 2.60 8.87 10.2035.144.93 18.33\nTable 7: Performance on BioLAMA benchmark. Note\nthat both the mask predict and opti-prompt require us-\ning the MLM head and opti-prompt needs further train-\ning data, so it is impossible to compare a model with-\nout MLM head being released (e.g. PubMedBERT). In\ncontrast, our Contrastive-Probe make all these models\ncomparable in terms of their captured knowledge.\nmance of 10 runs on 10 randomly sampled corpus.\nImproving the stability of Contrastive-Probe and\ninvestigating its nature is a future challenge.\n6 Related Work and Discussion\nKnowledge Probing Benchmarks for PLMs.\nLAMA (Petroni et al., 2019), which starts this line\nof work, is a collection of single-token knowledge\ntriples extracted from sources including Wikidata\nand ConceptNet (Speer et al., 2017). To miti-\ngate the problem of information leakage from the\nhead entity, Poerner et al. (2019) propose LAMA-\nUHN, which is a hard subset of LAMA that has\nless token overlaps in head and tail entities. X-\nFACTR (Jiang et al., 2020a) and mLAMA (Kass-\nner et al., 2021) extend knowledge probing to the\nmultilingual scenario and introduce multi-token\nanswers. They each propose decoding methods\nthat generate multi-token answers, which we have\nshown to work poorly on MedLAMA. BioLAMA\n(Sung et al., 2021) is a concurrent work that also\nreleases a benchmark for biomedical knowledge\nprobing.\nProbing via Prompt Engineering. Knowledge\nprobing is sensitive to what prompt is used (Jiang\net al., 2020b). To bootstrap the probing perfor-\nmance, Jiang et al. (2020b) mine more prompts\nand ensemble them during inference. Later works\nparameterised the prompts and made them train-\n4805\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19\nRelation ID\n0\n10\n20\n30acc@1\nlayer\n3\n5\n7\n9\n11\n12\nFigure 7: Performance of PubMedBERT over layers.\nTrue False\n5 1-4\n4\n26.67%\n1\n6.67%\n1\n6.67%\n9\n60.00%\nTop 1\nTrue False\n13\n8.67%\n20\n13.33%\n3\n2.00%\n114\n76.00%\nUMLS answersUMLS answers\nHuman scores\nTop 10\n50 100\nFigure 8: Confusion matrices of expert annotated\nscores versus the extracted UMLS answers. Five an-\nnotation score levels: 5- Perfectly answer the query; 4-\nSimilar to the gold answer, could somehow be the an-\nswer; 3- Related to the query but not correct ; 2- Same\ndomain or slight relation; 1-Completely unrelated.\nable (Shin et al., 2020b; Fichtel et al., 2021; Qin\nand Eisner, 2021). We have opted out prompt-\nengineering methods that require training data in\nthis work, as tuning the prompts are essentially\ntuning an additional (parameterised) model on top\nof PLMs. As pointed out by Fichtel et al. (2021),\nprompt tuning requires large amounts of training\ndata from the task. Since task training data is used,\nthe additional model parameters are exposed to the\ntarget data distribution and can solve the set set by\noverﬁtting to such biases (Cao et al., 2021). In\nour work, by adaptively ﬁnetuning the model with\na small set of raw sentences, we elicit the knowl-\nedge out from PLMs but do not expose the data\nbiases from the benchmark (MedLAMA).\nBiomedical Knowledge Probing.Nadkarni et al.\n(2021) train PLMs as KB completion models and\ntest on the same task to understand how much\nknowledge is in biomedical PLMs. BioLAMA fo-\ncuses on the continuous prompt learning method\nOptiPrompt (Zhong et al., 2021), which also re-\nquires ground-truth training data from the task.\nOverall, compared to BioLAMA, we have pro-\nvided a more comprehensive set of probing exper-\niments and analysis, including proposing a novel\nprobing technique and providing human evalua-\ntions of model predictions.\n7 Conclusion\nIn this work, we created a carefully curated\nbiomedical probing benchmark, MedLAMA, from\nthe UMLS knowledge graph. We illustrated that\nstate-of-the-art probing techniques and biomedi-\ncal pre-trained languages models (PLMs) struggle\nto cope with the challenging nature (e.g. multi-\ntoken answers) of this specialised domain, reach-\ning only an underwhelming 3% of acc@10. To\nreduce the gap, we further proposed a novel con-\ntrastive recipe which rewires the underlying PLMs\nwithout using any probing-speciﬁc data and illus-\ntrated that with a lightweight pre-training their ac-\ncuracies could be pushed to 24%.\nOur experiments also revealed that diﬀerent lay-\ners of transformers encode di ﬀerent types of in-\nformation, reﬂected by their individual success at\nhandling certain types of prompts. Additionally,\nusing a human expert, we showed that the existing\nevaluation criteria could overpenalise the models\nas many valid responses that PLMs produce are\nnot in the ground truth UMLS knowledge graph.\nThis further highlights the importance of having a\nhuman in the loop to better understand the poten-\ntials and limitations of PLMs in encoding domain\nspeciﬁc factual knowledge.\nOur ﬁndings indicate that the real lower bound\non the amount of factual knowledge encoded by\nPLMs is higher than we estimated, since such\nbound can be continuously improved by optimis-\ning both the encoding space (e.g. using our self-\nsupervised contrastive learning technique) and the\ninput space (e.g. using the prompt optimising\ntechniques (Shin et al., 2020a; Qin and Eisner,\n2021)). We leave further exploration of integrat-\ning the two possibilities to future work.\nAcknowledgements\nNigel Collier and Zaiqiao Meng kindly acknowl-\nedges grant-in-aid support from the UK ESRC for\nproject EPI-AI (ES/T012277/1).\n4806\nReferences\nEmily Alsentzer, John Murphy, William Boag, Wei-\nHung Weng, Di Jindi, Tristan Naumann, and\nMatthew McDermott. 2019. Publicly available clin-\nical bert embeddings. In NAACL HLT, pages 72–78.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn EMNLP, pages 3606–3611.\nOlivier Bodenreider. 2004. The uniﬁed medical lan-\nguage system (umls): integrating biomedical termi-\nnology. Nucleic acids research, 32(suppl_1):D267–\nD270.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In EACL.\nBoxi Cao, Hongyu Lin, Xianpei Han, Le Sun, Lingy-\nong Yan, Meng Liao, Tong Xue, and Jin Xu. 2021.\nKnowledgeable or educated guess? revisiting lan-\nguage models as knowledge bases. In ACL, pages\n1860–1874.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2021. Autoregressive entity retrieval.\nIn ICLR.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL, pages 4171–4186.\nPhilipp Dufter, Nora Kassner, and Hinrich Schütze.\n2021. Static embeddings as e ﬃcient knowledge\nbases? In NAACL, pages 2353–2363.\nLeandra Fichtel, Jan-Christoph Kalo, and Wolf-Tilo\nBalke. 2021. Prompt tuning or ﬁne-tuning-\ninvestigating relational knowledge in pre-trained\nlanguage models. In AKBC.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2020.\nMaking pre-trained language models better few-shot\nlearners. In ACL.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel de-\ncoding of conditional masked language models. In\nEMNLP, pages 6112–6121.\nGoran Glavaš and Ivan Vuli´c. 2021. Is supervised syn-\ntactic parsing beneﬁcial for language understanding\ntasks? an empirical investigation. In ACL, pages\n3090–3104.\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas,\nNaoto Usuyama, Xiaodong Liu, Tristan Naumann,\nJianfeng Gao, and Hoifung Poon. 2020. Domain-\nspeciﬁc language model pretraining for biomedical\nnatural language processing. ACM Transactions on\nComputing for Healthcare.\nZhengbao Jiang, Antonios Anastasopoulos, Jun Araki,\nHaibo Ding, and Graham Neubig. 2020a. X-factr:\nMultilingual factual knowledge retrieval from pre-\ntrained language models. In EMNLP, pages 5943–\n5959.\nZhengbao Jiang, Frank F Xu, Jun Araki, and Graham\nNeubig. 2020b. How can we know what language\nmodels know? TACL, 8:423–438.\nNora Kassner, Philipp Dufter, and Hinrich Schütze.\n2021. Multilingual lama: Investigating knowledge\nin multilingual pretrained language models. In ACL,\npages 3250–3258.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2020. BioBERT: a pre-\ntrained biomedical language representation model\nfor biomedical text mining. Bioinformatics,\n36(4):1234–1240.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In ACL, pages 7871–7880.\nChin-Yew Lin and Franz Josef Och. 2004. Auto-\nmatic evaluation of machine translation quality us-\ning longest common subsequence and skip-bigram\nstatistics. In ACL, pages 605–612.\nFangyu Liu, Ehsan Shareghi, Zaiqiao Meng, Marco\nBasaldella, and Nigel Collier. 2021a. Self-\nalignment pre-training for biomedical entity repre-\nsentations. NAACL.\nFangyu Liu, Ivan Vuli ´c, Anna Korhonen, and Nigel\nCollier. 2021b. Fast, e ﬀective, and self-supervised:\nTransforming masked language models into univer-\nsal lexical and sentence encoders. EMNLP.\nQianchu Liu, Fangyu Liu, Nigel Collier, Anna Korho-\nnen, and Ivan Vuli´c. 2021c. Mirrorwic: On eliciting\nword-in-context representations from pretrained lan-\nguage models. In CoNLL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2020.\nRoberta: A robustly optimized bert pretraining ap-\nproach. In ICLR.\nGeorge Michalopoulos, Yuanxin Wang, Hussam Kaka,\nHelen Chen, and Alexander Wong. 2021. Umlsbert:\nClinical domain knowledge augmentation of contex-\ntual embeddings using the uniﬁed medical language\nsystem metathesaurus. In NAACL, pages 1744–\n1753.\nRahul Nadkarni, David Wadden, Iz Beltagy, Noah A\nSmith, Hannaneh Hajishirzi, and Tom Hope. 2021.\nScientiﬁc language models for biomedical knowl-\nedge base completion: An empirical study. arXiv\npreprint arXiv:2106.09700.\n4807\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. arXiv preprint arXiv:1807.03748.\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019.\nTransfer learning in biomedical natural language\nprocessing: An evaluation of BERT and ELMo on\nten benchmarking datasets. In BioNLP Workshop,\npages 58–65.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James\nThorne, Yacine Jernite, Vladimir Karpukhin, Jean\nMaillard, et al. 2021. KILT: a benchmark for knowl-\nedge intensive language tasks. NAACL.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as\nknowledge bases? In EMNLP, pages 2463–2473.\nLong N Phan, James T Anibal, Hieu Tran, Shaurya\nChanana, Erol Bahadroglu, Alec Peltekian, and Gré-\ngoire Altan-Bonnet. 2021. Sciﬁve: a text-to-text\ntransformer model for biomedical literature. arXiv\npreprint arXiv:2106.03598.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2019. E-bert: E ﬃcient-yet-eﬀective entity embed-\ndings for bert. arXiv preprint arXiv:1911.03681.\nNina Poerner, Ulli Waltinger, and Hinrich Schütze.\n2020. E-bert: E ﬃcient-yet-eﬀective entity embed-\ndings for bert. In EMNLP: Findings, pages 803–\n818.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying lms with mixtures of soft prompts.\nIn NAACL.\nColin Raﬀel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020a. Auto-\nprompt: Eliciting knowledge from language models\nwith automatically generated prompts. In EMNLP.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020b. Eliciting\nknowledge from language models using automati-\ncally generated prompts. In EMNLP, pages 4222–\n4235.\nVered Shwartz, Rachel Rudinger, and Oyvind Tafjord.\n2020. “you are grounded!”: Latent name artifacts\nin pre-trained language models. In EMNLP, pages\n6850–6861.\nRobyn Speer, Joshua Chin, and Catherine Havasi.\n2017. Conceptnet 5.5: An open multilingual graph\nof general knowledge. In AAAI.\nYixuan Su, Tian Lan, Yan Wang, Dani Yogatama, Ling-\npeng Kong, and Nigel Collier. 2022. A contrastive\nframework for neural text generation. CoRR,\nabs/2202.06417.\nYixuan Su, Fangyu Liu, Zaiqiao Meng, Tian Lan,\nLei Shu, Ehsan Shareghi, and Nigel Collier. 2021.\nTacl: Improving BERT pre-training with token-\naware contrastive learning. CoRR, abs/2111.04198.\nMujeen Sung, Jinhyuk Lee, Sean Yi, Minji Jeon, Sung-\ndong Kim, and Jaewoo Kang. 2021. Can lan-\nguage models be biomedical knowledge bases? In\nEMNLP.\nZheng Yuan, Zhengyun Zhao, and Sheng Yu. 2020.\nCoder: Knowledge infused cross-lingual medical\nterm embedding for term normalization. arXiv\npreprint arXiv:2011.02947.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [mask]: Learning vs. learning to\nrecall. In NAACL.\n4808\nA Appendix\nA.1 Details of the Hardness Metrics\nIn this paper, we use two automatic metrics to dis-\ntinguish hard and easy queries. In particular, we\nﬁrst ﬁlter out easy queries by an exact matching\nmetric (i.e. the exactly matching all the words of\nanswer from queries). Since our MedLAMA con-\ntains multiple answers for queries, we use a thresh-\nold on the average exact matching score, i.e. avg-\nmatch>0.1, to ﬁlter out easy examples, where\navg-match is calculated by:\navg-match = Count(matched answers)\nCount(total answers) .\nThis metric can remove all the queries that match\nthe whole string of answers. However, some\ncommon sub-strings between queries and answers\nalso prone to reveal answers, particularly ben-\neﬁting those retrieval-based probing approaches.\nE.g. <Magnesium Chloride, may-prevent, Mag-\nnesium Deﬁciency >. Therefore, we further cal-\nculate the ROUGE-L score (Lin and Och, 2004)\nfor all the queries by regarding <query, answers>\npairs as the<hypothesis, reference> pairs, and fur-\nther ﬁlter out the ROUGE-L>0.1 queries.\nA.2 Hyperparameters Tuning\nWe train our C ontrastive-Probe based on 10k\nsentences which are randomly sampled from the\noriginal pre-training corpora of the correspond-\ning PLMs. Since most of the biomedical BERTs\nuse PubMed texts as their pre-training corpora,\nfor all biomedical PLMs we sampled random sen-\ntences from a version of PubMed corpus used\nby BlueBERT model (Peng et al., 2019), while\nfor BERT we sampled sentences from its original\nWikitext corpora. For the hyperparamters of our\nContrastive-Probe, Table 8 lists our search op-\ntions and the best parameters used in our paper.\nA.3 The Impact of Mask Ratios\nTo further investigate the impact of the mask ra-\ntio to the probing performance, we also test our\nContrastive-Probe based on PubMedBERT over\ndiﬀerent mask ratios ({0.1, 0.2, 0.3, 0.4, 0.5})\nunder the 10 random sentence sets, the result of\nwhich is shown in Figure 9. We can see that over\ndiﬀerent mask ratios the C ontrastive-Probe al-\nways reaches their best performance under certain\npre-training steps. And the performance curves\nof mask ratios are di ﬀerent over the full and hard\nsets, but they all achieves a generally good per-\nformance when the mask ratio is 0.5, which val-\nidates that di ﬀerent mask ratios favour di ﬀerent\ntypes queries.\n100 200 300 400 500\nsteps\n3.5\n4.0\n4.5\n5.0\n5.5\n6.0\n6.5\n7.0micro acc@1\nFull set\n0.1\n0.2\n0.3\n0.4\n0.5\n100 200 300 400 500\nsteps\n16\n18\n20\n22\n24\n26\n28\n30micro acc@10\nFull set\n0.1\n0.2\n0.3\n0.4\n0.5\n100 200 300 400 500\nsteps\n3.2\n3.5\n3.8\n4.0\n4.2\n4.5\n4.8\n5.0micro acc@1\nHard set\n0.1\n0.2\n0.3\n0.4\n0.5\n100 200 300 400 500\nsteps\n16\n18\n20\n22\n24micro acc@10\nHard set\n0.1\n0.2\n0.3\n0.4\n0.5\nFigure 9: Performance of Contrastive-Probe based on\nPubMedBERT over diﬀerent mask ratios. The shaded\nregions are the standard deviations under 10 di ﬀerent\nrandom sentence sets sampled from the PubMed cor-\npus.\n4809\nHyperparameters Search space\nrewire training learning rate { 1e-5, 2e-5∗, 5e-5}\nrewire training steps 500\nrewire training mask ratio {0.1, 0.2, 0.3, 0.4 ∗, 0.5∗}\nτin InfoNCE of rewire training {0.02,0.03 ∗,0.04,0.05}\nrewire training data size {1k, 10k ∗, 20k,100k}\nstep of checkpoint for probing {50, 150, 200 ∗, 250, 300, 350}\nmax_seq_length of tokeniser for queries 50\nmax_seq_length of tokeniser for answers 25\nTable 8: Hyperparameters along with their search grid. ∗marks the values used to obtain the reported results.\nQuery 1:The gene productHLA Class II Histocompatibility Antigen, DP(W4) Beta Chainis encoded by gene [Y].\nUMLS Answers:MHC Class II Gene,HLA-DPB1 Gene,Immunoprotein Gene\nHuman Answers:MHC Class II Gene,HLA-DPB1 Gene\nModel Contrastive-Probe(PubMedBERT) X-FACTR (BlueBERT) Generative PLMs (SciFive-large)\nTop-5\nMHC Class II Gene b HLA-DRB1\nMHC Class I Gene hla encoding HLA\nHLA-A Gene dqb1 DP(W)\nHLA-DPB1 Gene locus dqb1 HLA-B\nHLA-F Gene 2 , dq beta 2 HLA-DQ\nQuery 2:The gene productTuberinis encoded by gene [Y].\nUMLS Answers:TSC2 Gene,Signaling Pathway Gene\nHuman Answers:TSC2 Gene, Tuberin\nModel Contrastive-Probe(PubMedBERT) X-FACTR (BlueBERT) Generative PLMs (SciFive-large)\nTop-5\nTSC2 Gene family of tuberins “”\nSKA2 Gene ##t1 TUB\nTSPY1 Gene symbol tuber Tuberin\nTuberin ( tuber ) TUBE\nTSC1 Gene a TUBB\nQuery 3:Refractory Monomorphic Post-Transplant Lymphoproliferative Disordermay have [Y].\nUMLS Answers:Lymphadenopathy,Aggressive Clinical Course,Extranodal Disease\nHuman Answers:Early post-transplant lymphoproliferative disorder, Lymphoproliferative disorder following transplantation,\nRefractory Polymorphic Post-Transplant Lymphoproliferative Disorder,Aggressive Clinical Course,Post transplant lymphoproliferative disorder\nNeoplastic Post-Transplant Lymphoproliferative Disorder,Refractory Monomorphic Post-Transplant Lymphoproliferative Disorder\nModel Contrastive-Probe(PubMedBERT) X-FACTR (BlueBERT) Generative PLMs (SciFive-large)\nTop-5\nEarly post-transplant lymphoproliferative disorder manifestations similar to this\nLymphoproliferative disorder following transplantation relapses in this study\nRefractory Polymorphic Post-Transplant Lymphoproliferative Disorder phenotype similar to our case\nAggressive Clinical Course - speciﬁc phenotype similar to ours\nPost transplant lymphoproliferative disorder features similar to this case\nQuery 4:moexiprilmight treat [Y].\nUMLS Answers:Diabetic Nephropathies,Heart Failure,Hypertension,Ventricular Dysfunction, Left\nHuman Answers:Essential Hypertension, Hypertension\nModel Contrastive-Probe(PubMedBERT) X-FACTR (BlueBERT) Generative PLMs (SciFive-large)\nTop-5\nEssential Hypertension hypertension “”\nPosttransplant hyperlipidemia diabetes mellitus this\nHypertension essential hypertension them\nAtherosclerotic Cardiovascular Disease diabetes migraine\nType 1 Diabetes Mellitus in patients with hypertension patients\nTable 9: Example predictions of di ﬀerent probing approaches. The human answers are annotated based on the\nContrastive-Probe predictions.\n4810"
}