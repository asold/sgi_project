{
    "title": "On the Robustness of Vision Transformers to Adversarial Examples",
    "url": "https://openalex.org/W3143373604",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2222458738",
            "name": "Kaleel Mahmood",
            "affiliations": [
                "University of Connecticut"
            ]
        },
        {
            "id": "https://openalex.org/A2323625981",
            "name": "Rigel Mahmood",
            "affiliations": [
                "University of Connecticut"
            ]
        },
        {
            "id": "https://openalex.org/A2113223833",
            "name": "Marten van Dijk",
            "affiliations": [
                "Centrum Wiskunde & Informatica"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6773424267",
        "https://openalex.org/W6754390751",
        "https://openalex.org/W6771748797",
        "https://openalex.org/W6714069269",
        "https://openalex.org/W3035160371",
        "https://openalex.org/W6775170262",
        "https://openalex.org/W6774549192",
        "https://openalex.org/W6774469542",
        "https://openalex.org/W2774644650",
        "https://openalex.org/W6784333009",
        "https://openalex.org/W6640425456",
        "https://openalex.org/W6777014282",
        "https://openalex.org/W6698183232",
        "https://openalex.org/W3034408878",
        "https://openalex.org/W2947469743",
        "https://openalex.org/W3097217077",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W6758779121",
        "https://openalex.org/W6746608116",
        "https://openalex.org/W2963857521",
        "https://openalex.org/W2603766943",
        "https://openalex.org/W3080297477",
        "https://openalex.org/W6726114608",
        "https://openalex.org/W6748475379",
        "https://openalex.org/W6769955919",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W2147800946",
        "https://openalex.org/W6731927902",
        "https://openalex.org/W6739868092",
        "https://openalex.org/W2979170146",
        "https://openalex.org/W6779931059",
        "https://openalex.org/W3034758058",
        "https://openalex.org/W3034994123",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W4293846201",
        "https://openalex.org/W3097065222",
        "https://openalex.org/W2570685808",
        "https://openalex.org/W3173787059",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2995368830",
        "https://openalex.org/W3009542902",
        "https://openalex.org/W3205959904",
        "https://openalex.org/W2408141691",
        "https://openalex.org/W3020482686",
        "https://openalex.org/W2913848079",
        "https://openalex.org/W2912070915",
        "https://openalex.org/W2963207607",
        "https://openalex.org/W2963143631",
        "https://openalex.org/W1945616565",
        "https://openalex.org/W2994759459",
        "https://openalex.org/W2964253222",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W3103340107",
        "https://openalex.org/W3006834354",
        "https://openalex.org/W2302255633",
        "https://openalex.org/W1673923490",
        "https://openalex.org/W2891584299",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3006622081",
        "https://openalex.org/W2950864148",
        "https://openalex.org/W3036553362",
        "https://openalex.org/W2964153729",
        "https://openalex.org/W2963070423",
        "https://openalex.org/W4299802238"
    ],
    "abstract": "Recent advances in attention-based networks have&#13;\\nshown that Vision Transformers can achieve state-of-the-art&#13;\\nor near state-of-the-art results on many image classification&#13;\\ntasks. This puts transformers in the unique position of being&#13;\\na promising alternative to traditional convolutional neural&#13;\\nnetworks (CNNs). While CNNs have been carefully stud-&#13;\\nied with respect to adversarial attacks, the same cannot be&#13;\\nsaid of Vision Transformers. In this paper, we study the&#13;\\nrobustness of Vision Transformers to adversarial examples.&#13;\\nOur analyses of transformer security is divided into three&#13;\\nparts. First, we test the transformer under standard white-&#13;\\nbox and black-box attacks. Second, we study the transfer-&#13;\\nability of adversarial examples between CNNs and trans-&#13;\\nformers. We show that adversarial examples do not readily&#13;\\ntransfer between CNNs and transformers. Based on this&#13;\\nfinding, we analyze the security of a simple ensemble de-&#13;\\nfense of CNNs and transformers. By creating a new attack,&#13;\\nthe self-attention blended gradient attack, we show that&#13;\\nsuch an ensemble is not secure under a white-box adver-&#13;\\nsary. However, under a black-box adversary, we show that&#13;\\nan ensemble can achieve unprecedented robustness without&#13;\\nsacrificing clean accuracy. Our analysis for this work is&#13;\\ndone using six types of white-box attacks and two types of&#13;\\nblack-box attacks. Our study encompasses multiple Vision&#13;\\nTransformers, Big Transfer Models and CNN architectures&#13;\\ntrained on CIFAR-10, CIFAR-100 and ImageNet.",
    "full_text": null
}