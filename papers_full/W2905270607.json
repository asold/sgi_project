{
    "title": "Attending to Mathematical Language with Transformers",
    "url": "https://openalex.org/W2905270607",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4297710380",
            "name": "Wangperawong, Artit",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1525961042",
        "https://openalex.org/W2784011873",
        "https://openalex.org/W2335930829",
        "https://openalex.org/W2548137223",
        "https://openalex.org/W2593413655",
        "https://openalex.org/W2963418779",
        "https://openalex.org/W2883812034",
        "https://openalex.org/W2173051530",
        "https://openalex.org/W2325237720",
        "https://openalex.org/W2097182554",
        "https://openalex.org/W2141373061",
        "https://openalex.org/W1773179558",
        "https://openalex.org/W2402671427",
        "https://openalex.org/W1581407678",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W1771459135"
    ],
    "abstract": "Mathematical expressions were generated, evaluated and used to train neural network models based on the transformer architecture. The expressions and their targets were analyzed as a character-level sequence transduction task in which the encoder and decoder are built on attention mechanisms. Three models were trained to understand and evaluate symbolic variables and expressions in mathematics: (1) the self-attentive and feed-forward transformer without recurrence or convolution, (2) the universal transformer with recurrence, and (3) the adaptive universal transformer with recurrence and adaptive computation time. The models respectively achieved test accuracies as high as 76.1%, 78.8% and 84.9% in evaluating the expressions to match the target values. For the cases inferred incorrectly, the results differed from the targets by only one or two characters. The models notably learned to add, subtract and multiply both positive and negative decimal numbers of variable digits assigned to symbolic variables.",
    "full_text": "Attending to Mathematical Language with Transformers\nArtit Wangperawong\nartit.wangperawong@usbank.com\nU.S. Bank\n1095 Avenue of the Americas\nNew York, NY 10036\nAbstract\nMathematical expressions were generated, evaluated and\nused to train neural network models based on the transformer\narchitecture. The expressions and their targets were analyzed\nas a character-level sequence transduction task in which the\nencoder and decoder are built on attention mechanisms. Three\nmodels were trained with common hyperparameters to un-\nderstand and evaluate symbolic variables and expressions in\nmathematics: (1) the self-attentive and feed-forward trans-\nformer without recurrence or convolution, (2) the universal\ntransformer with recurrence, and (3) the adaptive universal\ntransformer with recurrence and adaptive computation time.\nThe models respectively achieved test accuracies as high as\n76.1%, 78.8% and 84.9% in evaluating the expressions to\nmatch the target values. For the cases inferred incorrectly, the\nresults differed from the targets by only one or two charac-\nters. The models notably learned to add, subtract and multiply\nboth positive and negative decimal numbers of variable digits\nassigned to symbolic variables.\nIntroduction\nArithmetic and algebra are important mathematical skills\nthat should be acquired by one’s adolescence (Carraher et\nal. 2006). Therefore, we should expect that an artiﬁcially in-\ntelligent agent or system can at least master such problems\nwithout predetermined algorithms. Arithmetic involves the\nstudy of numbers and the effect on them of operators such\nas addition (+), subtraction (−), multiplication (×), and di-\nvision (÷). Algebra at the basic level involves the study of\nmathematical symbols and the rules governing how such\nsymbols are manipulated. A mathematical expression is a\nphrase constructed with a ﬁnite arrangement of numbers,\nsymbols and operators according to the rules of mathemat-\nics. Such rules are typically pre-programmed into computers\nand execute with ideally perfect accuracy. Here we describe\nneural network models trained to read mathematical phrases\nat the character level and evaluate the expressions for a result\nwithout any pre-programmed or hard-coded math rules.\nBackground and Related Work\nPrior studies related to this work have used multilayer per-\nceptrons (Hoshen and Peleg 2016), recurrent neural net-\nworks (RNN) (Zaremba, Kurach, and Fergus 2014; Mickey\nand McClelland 2014), long short-term memory (LSTM)\n(Zaremba and Sutskever 2014; Kalchbrenner, Danihelka,\nand Graves 2015), Neural GPUs (Kaiser and Sutskever\n2015; Price, Zaremba, and Sutskever 2016; Freivalds and\nLiepins 2017), and transformers (Vaswani et al. 2017; De-\nhghani et al. 2018). These studies were mostly restricted\nto addition of integers with the same number of digits\nand did not involve symbolic variables or expressions. The\nstudy involving mathematical expressions sought to dis-\ncover efﬁcient mathematical identities (Zaremba, Kurach,\nand Fergus 2014). For the studies that considered mul-\ntiplication, accuracy for the multiplication tasks were ei-\nther not explicitly reported (Zaremba and Sutskever 2014;\nDehghani et al. 2018) or involved binary representations\n(Kaiser and Sutskever 2015; Price, Zaremba, and Sutskever\n2016; Freivalds and Liepins 2017).\nIn this work, we report results for directly evaluating\nmathematical expressions involving addition, subtraction\nand multiplication of both positive and negative decimal\nnumbers with variable digits assigned to symbolic variables.\nThe end-to-end process described below does not include\nany curriculum training or intermediary non-decimal repre-\nsentations.\nExperimental Methods\nThe training and test data were generated by assigning sym-\nbolic variables either positive or negative decimal integers\nand then describing the algebraic operation to perform. Such\nexpressions were generated as input strings as shown in the\nexample:\nx= 85,y = −523,x ×y\n−44455\nWe restrict our variable assignments to the range x,y ∈\n[−1000,1000) and the operations to the set {+,−,×}. To\nensure that the model embraces symbolic variables, the or-\nder in which and appears in the expression is randomly cho-\nsen. For instance, an input string contrasting from the exam-\nple shown above might be y = 129,x = 531,x −y. Each\ninput string is accompanied by its target string, which is the\nevaluation of the mathematical expression. For this study,\narXiv:1812.02825v5  [cs.CL]  14 Sep 2019\nTable 1: Example input obfuscation table.\nx = 8 5 , y = − 5 2 3 , x × y\ng r f d n p r w d q e n g k p\nTable 2: Example output obfuscation table.\n− 4 4 4 5 5\nw m m m d d\nall targets considered are decimal integers represented at the\ncharacter level. About 12 million unique samples were thus\ngenerated and randomly split into training and test sets at an\napproximate ratio of 9:1, respectively.\nThe entirety of each input is read and encoded at the char-\nacter level. The entirety of each output is decoded at the\ncharacter level. Only after training do the models come to\ninterpret meaning behind the character sequences. One can\nimagine that a different character mapping be used to ob-\nfuscate the meaning assigned by mathematical practice but\nstill be trainable for the models described here to capture\nthe relationships between the individual characters (Table 1).\nMapping such results back to the representations familiar\nin mathematical practice would yield the same results (Ta-\nble 2).\nThe input-target pairs were ﬁrst used to train a self-\nattentive and feed-forward transformer without recurrence\nor convolution in a similar manner as the base model pre-\nviously reported (Vaswani et al. 2017). The self-attention\nmechanism used is the scaled dot-product attention accord-\ning to\nAttention(K,Q,V ) =softmax\n(QKT\n√\nd\n)\nV, (1)\nwhere dis the dimension (number of columns) of the input\nqueries Q, keys K, and values V. By using self-attention,\ntransformers can account for the whole sequence in its en-\ntirety and bi-directionally. For multi-head attention with h\nheads that jointly attend to different representation sub-\nspaces at different positions given a sequence of length m\nand the matrix H ∈Rm×d, the result is\nMultiHead(H) =Concat(head1,...,head h) WO,\nheadi = Attention\n(\nHW\ni ,HK\ni ,HV\ni\n)\n,\n(2)\nwhere the projections are learned parameter matrices\nHW\ni ,HK\ni ,HV\ni ∈R(d×d)/h and WO ∈R(d×d).\nThe same hyperparameters were used as the standard\ntransformer above except for the details that follow. The\ntransformer used in this study is smaller. The encoder con-\nsisted of two identical layers, each of which consists of\ntwo sub-layers: a multi-head self-attention layer and a fully-\nconnected feed-forward network layer. Layer normalization\nwas used to preprocess the sub-layer inputs. The decoder\nconsisted of two identical layers, each of which consists of\nTable 3: Test performance comparison of inferring mathe-\nmatical expressions at the character level for different types\nof expressions for the transformers studied in this work: T -\nTransformer; UT - Universal Transformer; AUT - Adaptive\nUniversal Transformer.\nType a + a a −a a ×a a + b a −b a + b\nT 1.0 1.0 1.0 0.98 0.49 0.09\nUT 1.0 1.0 1.0 1.0 0.50 0.23\nAUT 1.0 1.0 1.0 0.99 0.99 0.15\nthree sub-layers: a multi-head self-attention layer, a multi-\nhead attention layer over the output of the encoder stack, and\na fully-connected feed-forward network layer. Each multi-\nheaded attention layer consisted of 4 heads. Each fully-\nconnected feed-forward network consisted of 128 neurons.\nA dropout rate of 0.1 was used to postprocess the output of\neach sub-layer before it is added to the sub-layer input by\nthe residual connection. The number of training steps was\nset to 100,000.\nResults and Discussion\nThe transformer model achieved an accuracy on the test set\nof 76.1%. When we analyze the performance by the type\nof expression, however, we ﬁnd that the model infers with\nperfect accuracy symmetric a(op)aexpressions such as x+\nx, y−y, and x∗x. Slightly less perfect were asymmetric\na+baddition tasks, such as x+yand y+x, which had 98%\naccuracy. The next challenging tasks involved asymmetric\na−bsubtraction, such as x−yand y−x, which had 49%\naccuracy. The model struggled most with asymmetric a×\nb multiplication tasks, such as x×y or y ×x, which had\nonly 9% accuracy. Note that this is a single model trained to\nperform all the different types of tasks. A summary of the\nresults are shown in Table 3.\nThe results demonstrate that the transformer can learn to\ninterpret and evaluate symbolic variables and expressions as\nrepresented by character strings, performing addition, sub-\ntraction and multiplication of both positive and negative dec-\nimal numbers. The transformer can correctly utilize the val-\nues assigned to symbolic variables for inference. Consider-\ning the example input string y= 568,x = −867,y ×y, the\nmodel correctly ignores the value assigned to x and com-\nputes 322624 as the output. The attention visualizations for\nthe encoder’s self-attention and decoder’s attention on the ﬁ-\nnal layer of the encoder shown in Figs. 1 and 2, respectively,\nillustrate that the output characters attend almost exclusively\non the characters representing the assignment to y. Further-\nmore, the order in which xand y assignments occur in the\nstring are handled well, since the accuracy is high despite\nour data including random variations as mentioned above.\nFor the cases inferred incorrectly, the results are very\nclose to the targets. As an example, the value produced for\nthe input sequence y = −440,x = 687,y ×yis −300280,\nwhich is very close to the actual target value of −302280\nconsidering the character match accuracy at each position.\nOnly the thousandth place character is incorrect, which is\nrepresentative of our general observation that one or two of\nthe middle positions are most difﬁcult to correctly infer. In-\nterestingly, the ﬁrst and last positions of the output attend\nprimarily to the ﬁrst and last positions representing the as-\nsignment to y, whereas the output positions in between do\nnot exhibit such selective attention (Fig. 3). This confusion\ncould be the reason for the faulty inference of the characters\nin the middle of the output.\nFigure 1: Encoder’s self-attention on the last character for\none of the transformer layers. The different attention heads\nare color coded.\nFigure 2: Transformer attention distributions for the de-\ncoder’s attention on the ﬁnal layer of the encoder for dif-\nferent decoder layers and attention heads (color-coded). See\nFig. 3 for more visualizations.\nIn order to improve the performance for evaluating non-\nsymmetric subtraction and multiplication expressions, the\ntransformer can be augmented with recurrent inductive bias\nas described by prior work on universal transformers (De-\nhghani et al. 2018). Unlike the standard transformer, the uni-\nversal transformer can be computationally universal given\nsufﬁcient memory. At training step t, the universal trans-\nformer iterates to improve its representations Ht ∈R(m×d)\nfor the m input positions in parallel with a self-attention\nmechanism, followed by a recurrent transformation using a\ndepth-wise separable convolution or a position-wise fully-\nconnected layer. The universal transformer was thus re-\nported to achieve state-of-the-art results on translation, nat-\nural language understanding, and learning-to-execute tasks\nsimilar to this study, outperforming both LSTM RNNs and\nthe standard transformer given the same hyperparameters.\nUsing the same hyperparameters and dataset described\nabove for the standard transformer, the universal transformer\nachieved better results on all types of asymmetrica(op)bex-\npressions as shown in Table 3. The overall accuracy on the\ntasks is 78.8%. The most improvement occurred for thea×b\nmultiplication tasks, which more than doubled in accuracy. It\ntherefore appears that the recurrent inductive bias as imple-\nmented in the universal transformer successfully addresses\nsome of the shortcomings of the standard transformer model\nwhen using the same hyperparameters.\nSince only a−band a×btasks can be improved upon\nany further, we next add adaptive computation time (ACT)\n(Graves 2016) to the universal transformer (Dehghani et al.\n2018) in order to devote more processing resources to sym-\nbols not interpreted well by the model. For a neural network\nRwith parametric state transition model S, output weights\nWy, output bias by, input sequence xt, state sequence st, in-\ntermediate update numbern, intermediate state sequencesn\nt ,\nintermediate output sequence yn\nt , augmented input sequence\nxn\nt , ACT can be implemented by iterating through each step\nof the sequence as follows:\nxn\nt = (xt,δn,1) , (3)\nsn\nt =\n{S(st−1,x1\nt ) if n= 1\nS(sn−1\nt ,xn\nt ) if n̸= 1 (4)\nyn\nt = (Wt ·sn\nt ,by) , (5)\nwhere δn,1 is a binary indicator of whether the input step\nhas been incremented at update n. An extra sigmoidal halt-\ning unit hand its associated weightsWh and bias bh is added\nto the network to calculate the halting probability pn\nt at in-\ntermediate steps up to the total number of updates N(t) ac-\ncording to\nhn\nt = σ(Wh ·sn\nt + bn) , (6)\nN(t) = min{n′:\nn′\n∑\nn=1\nhn\nt >= 1−ϵ} (7)\nR(t) = 1−\nN(t)−1∑\nn=1\nhn\nt (8)\nFigure 3: Extra attention visualizations corresponding to Fig. 2 for a decoder layer’s attention on the ﬁnal layer of the encoder.\nThe different attention heads are color coded. Attentions are displayed for each output character individually. The ﬁrst and last\ncharacters of the output attend primarily to the ﬁrst and last characters representing the assignment to y, whereas the characters\nin between do not exhibit such selective attention.\npn\nt =\n{R(t) if n= N(t)\nhn\nt if n̸= N(t) , (9)\nwhere a small threshold ϵ= 0.01 is used to halt after a sin-\ngle update and an upper bound on updates N(t) ≤ 24 is\nimposed. The state and output sequences and are calculated\nas\nst =\nN(t)∑\nn=1\npn\nt sn\nt yt =\nN(t)∑\nn=1\npn\nt yn\nt . (10)\nAs shown in Table 3, the adaptive universal transformer\nimproves on thea−btasks almost to perfection but performs\nmuch worse on the a×btasks, producing an overall higher\naccuracy of 84.9%. The adaptive universal transformer may\nhave focused only to improve the a−btasks because it is\nmore attainable than thea×btasks in improving overall efﬁ-\nciency. For 100,000 training steps on a 6-core CPU, the uni-\nversal transformer requires about 3.5 times the training dura-\ntion of the vanilla transformer, whereas the adaptive univer-\nsal transformer requires only 2 times the training duration.\nThis indicates that although improvements can be attributed\nto more training computational costs, augmenting the uni-\nversal transformer with ACT improves training efﬁcacy.\nConclusion and Future Work\nThe mathematical language understanding demonstrated in\nthis study is foundational for an artiﬁcially intelligent agent.\nThe framework and ﬁndings discussed should also be trans-\nferable to natural language understanding. The symbolic\nvariable assignment is analogous to supporting facts in the\nbAbi story, question and answering tasks (Weston et al.\n2015). Symmetric a(op)atasks only utilize one of the sup-\nporting facts, whereas asymmetric a(op)btasks utilizes two\nsupporting facts. The symbolic expressions and their evalu-\nation studied here can thus be considered a simpliﬁed ver-\nsion of story, question and answering tasks that can be stud-\nied and analyzed more expediently and concretely. We ex-\npect that future studies will involve more types of symbolic\nexpressions and variables, further elucidating how to im-\nprove the shortcomings of existing models to the beneﬁt\nof more complex natural language understanding problems.\nMore training steps would also improve accuracy. The re-\nsults described here have been made reproducible with open-\nsourced software hosted on GitHub (Wangperawong 2019).\nThe transformer model has been shown to work well for a\nmyriad of applications beyond what we typically consider as\nsequence transduction tasks, e.g. image processing (Parmar\net al. 2018). More generally, transformers can be applied to\nproblems involving tensors as inputs and tensors as outputs,\nwhich is the motivation behind the Tensor2Tensor library\nused in this study (Vaswani et al. 2018). The attention mech-\nanism of the transformer architecture can be interpreted as a\nglobal receptive ﬁeld that can analyze more than the lim-\nited receptive ﬁelds, which are often referred to as ﬁlters,\nin convolutional neural networks. We therefore expect that\nthe transformer can serve as a uniﬁed model to incorporate\nand improve upon previous work in churn prediction (Wang-\nperawong et al. 2016), information retrieval (Wangperawong\net al. 2018), and collaborative ﬁltering (Liu and Wangpera-\nwong 2018). The customer’s history can be the story or in-\nput sequence, and the question can be whether they churn or\nwhat item they would choose from recommendations pro-\nvided.\nReferences\n[Carraher et al. 2006] Carraher, D. W.; Schliemann, A. D.;\nBrizuela, B. M.; and Earnest, D. 2006. Arithmetic and alge-\nbra in early mathematics education. Journal for Research in\nMathematics education 87–115.\n[Dehghani et al. 2018] Dehghani, M.; Gouws, S.; Vinyals,\nO.; Uszkoreit, J.; and Kaiser, Ł. 2018. Universal transform-\ners. arXiv preprint arXiv:1807.03819.\n[Freivalds and Liepins 2017] Freivalds, K., and Liepins, R.\n2017. Improving the neural gpu architecture for algorithm\nlearning. arXiv preprint arXiv:1702.08727.\n[Graves 2016] Graves, A. 2016. Adaptive computa-\ntion time for recurrent neural networks. arXiv preprint\narXiv:1603.08983.\n[Hoshen and Peleg 2016] Hoshen, Y ., and Peleg, S. 2016.\nVisual learning of arithmetic operation. In Thirtieth AAAI\nConference on Artiﬁcial Intelligence.\n[Kaiser and Sutskever 2015] Kaiser, Ł., and Sutskever, I.\n2015. Neural gpus learn algorithms. arXiv preprint\narXiv:1511.08228.\n[Kalchbrenner, Danihelka, and Graves 2015] Kalchbrenner,\nN.; Danihelka, I.; and Graves, A. 2015. Grid long short-term\nmemory. arXiv preprint arXiv:1507.01526.\n[Liu and Wangperawong 2018] Liu, X., and Wangperawong,\nA. 2018. A collaborative approach to angel and ven-\nture capital investment recommendations. arXiv preprint\narXiv:1807.09967.\n[Mickey and McClelland 2014] Mickey, K. W., and McClel-\nland, J. L. 2014. A neural network model of learning math-\nematical equivalence. In Proceedings of the Annual Meeting\nof the Cognitive Science Society, volume 36.\n[Parmar et al. 2018] Parmar, N.; Vaswani, A.; Uszkoreit, J.;\nKaiser, Ł.; Shazeer, N.; Ku, A.; and Tran, D. 2018. Image\ntransformer. arXiv preprint arXiv:1802.05751.\n[Price, Zaremba, and Sutskever 2016] Price, E.; Zaremba,\nW.; and Sutskever, I. 2016. Extensions and limitations of\nthe neural gpu. arXiv preprint arXiv:1611.00736.\n[Vaswani et al. 2017] Vaswani, A.; Shazeer, N.; Parmar, N.;\nUszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and\nPolosukhin, I. 2017. Attention is all you need. CoRR\nabs/1706.03762.\n[Vaswani et al. 2018] Vaswani, A.; Bengio, S.; Brevdo, E.;\nChollet, F.; Gomez, A. N.; Gouws, S.; Jones, L.; Kaiser,\nŁ.; Kalchbrenner, N.; Parmar, N.; et al. 2018. Ten-\nsor2tensor for neural machine translation. arXiv preprint\narXiv:1803.07416.\n[Wangperawong et al. 2016] Wangperawong, A.; Brun, C.;\nLaudy, O.; and Pavasuthipaisit, R. 2016. Churn analysis\nusing deep convolutional neural networks and autoencoders.\narXiv preprint arXiv:1604.05377.\n[Wangperawong et al. 2018] Wangperawong, A.; Kri-\nangchaivech, K.; Lanari, A.; Lam, S.; and Wangperawong,\nP. 2018. Comparing heterogeneous entities using artiﬁcial\nneural networks of trainable weighted structural compo-\nnents and machine-learned activation functions. arXiv\npreprint arXiv:1801.03143.\n[Wangperawong 2019] Wangperawong, A. 2019. Ten-\nsor2tensor. https://github.com/artitw/tensor2tensor.\n[Weston et al. 2015] Weston, J.; Bordes, A.; Chopra, S.;\nRush, A. M.; van Merri¨enboer, B.; Joulin, A.; and Mikolov,\nT. 2015. Towards ai-complete question answering: A set of\nprerequisite toy tasks. arXiv preprint arXiv:1502.05698.\n[Zaremba and Sutskever 2014] Zaremba, W., and Sutskever,\nI. 2014. Learning to execute. arXiv preprint\narXiv:1410.4615.\n[Zaremba, Kurach, and Fergus 2014] Zaremba, W.; Kurach,\nK.; and Fergus, R. 2014. Learning to discover efﬁcient\nmathematical identities. In Advances in Neural Information\nProcessing Systems, 1278–1286."
}