{
  "title": "ProtGPT2 is a deep unsupervised language model for protein design",
  "url": "https://openalex.org/W4288066876",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5004684799",
      "name": "Noelia Ferruz",
      "affiliations": [
        "University of Bayreuth"
      ]
    },
    {
      "id": "https://openalex.org/A5084421039",
      "name": "Steffen Schmidt",
      "affiliations": [
        "University of Bayreuth"
      ]
    },
    {
      "id": "https://openalex.org/A5063262156",
      "name": "Birte Höcker",
      "affiliations": [
        "University of Bayreuth"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2956569764",
    "https://openalex.org/W4283390570",
    "https://openalex.org/W3166142427",
    "https://openalex.org/W2980789587",
    "https://openalex.org/W3203741962",
    "https://openalex.org/W2972223935",
    "https://openalex.org/W4405188194",
    "https://openalex.org/W3217516687",
    "https://openalex.org/W4214916617",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W4281287617",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W3125063344",
    "https://openalex.org/W3010387158",
    "https://openalex.org/W3186612807",
    "https://openalex.org/W4283733033",
    "https://openalex.org/W4280491725",
    "https://openalex.org/W4225080234",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2577723078",
    "https://openalex.org/W4224988655",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2173591891",
    "https://openalex.org/W2738675921",
    "https://openalex.org/W3003243900",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3165745490",
    "https://openalex.org/W4200472293",
    "https://openalex.org/W2941112903",
    "https://openalex.org/W2145268834",
    "https://openalex.org/W2557595285",
    "https://openalex.org/W2101220662",
    "https://openalex.org/W4281790889",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W3183475563",
    "https://openalex.org/W2107520340",
    "https://openalex.org/W3005012312",
    "https://openalex.org/W2606439133",
    "https://openalex.org/W2898933261",
    "https://openalex.org/W3157345013",
    "https://openalex.org/W1965827393",
    "https://openalex.org/W3106745904",
    "https://openalex.org/W2161311439",
    "https://openalex.org/W1985007883",
    "https://openalex.org/W2009247363",
    "https://openalex.org/W3018526736",
    "https://openalex.org/W3195621306",
    "https://openalex.org/W2519539312",
    "https://openalex.org/W3159318882",
    "https://openalex.org/W4210840673",
    "https://openalex.org/W2898459031",
    "https://openalex.org/W3137550543",
    "https://openalex.org/W3081133783",
    "https://openalex.org/W3179499565",
    "https://openalex.org/W2173280871",
    "https://openalex.org/W4210861939",
    "https://openalex.org/W4200574022",
    "https://openalex.org/W3139132287",
    "https://openalex.org/W2999044305",
    "https://openalex.org/W4206964505",
    "https://openalex.org/W3081168214",
    "https://openalex.org/W2051210555",
    "https://openalex.org/W2296686360",
    "https://openalex.org/W2989052455",
    "https://openalex.org/W2082586732",
    "https://openalex.org/W2407399801",
    "https://openalex.org/W2971227267",
    "https://openalex.org/W4388024559",
    "https://openalex.org/W4392302569"
  ],
  "abstract": null,
  "full_text": "nature communications\nArticle https://doi.org/10.1038/s41467-022-32007-7\nProtGPT2 is a deep unsupervised language\nmodel for protein design\nNoelia Ferruz 1,3 , Steffen Schmidt 2 &B i r t eH ö c k e r1\nProtein design aims to build novel proteins customized for speciﬁc purposes,\nthereby holding the potential to tackle many environmental and biomedical\nproblems. Recent progress in Transformer-based architectures has enabled\nthe implementation of language models capable of generating text with\nhuman-like capabilities. Here, motivated by this success, we describe\nProtGPT2, a language model trained onthe protein space that generates de\nnovo protein sequences following the principles of natural ones. The gener-\nated proteins display natural amino acid propensities, while disorder predic-\ntions indicate that 88% of ProtGPT2-generated proteins are globular, in line\nwith natural sequences. Sensitive sequence searches in protein databases\nshow that ProtGPT2 sequences are distantly related to natural ones, and\nsimilarity networks further demonstrate that ProtGPT2 is sampling unex-\nplored regions of protein space. AlphaFold prediction of ProtGPT2-sequences\nyields well-folded non-idealized structures with embodiments and large loops\nand reveals topologies not captured in current structure databases. ProtGPT2\ngenerates sequences in a matter of seconds and is freely available.\nNatural language processing (NLP) has seen extraordinary advances in\nrecent years. Large pre-trained language models have drastically\ntransformed the NLPﬁeld and with it, many of the tools we use in our\ndaily lives, such as chatbots, smart assistants, or translation machines.\nAnalogies between protein sequences and human languages have long\nbeen noted by us and others\n1,2. Protein sequences can be described as a\nconcatenation of letters from a chemically deﬁned alphabet, the nat-\nural amino acids, and like human languages, these letters arrange to\nform secondary structural elements (“words”), which assemble to form\ndomains (“sentences”) that undertake a function (“meaning”). One of\nthe most attractive similarities is that protein sequences, like natural\nlanguages, are information-complete: they store structure and func-\ntion entirely in their amino acid order with extreme efﬁciency. With the\nextraordinary advances in the NLPﬁeld in understanding and gen-\nerating language with near-human capabilities, we hypothesized that\nthese methods open a new door to approach protein-related problems\nfrom sequence alone, such as protein design.\nAlthough protein sequences and human languages are not with-\nout dissimilarities, their analogies have stimulated applying NLP\nmethods to solve protein research problems for decades\n2.S u p e r v i s e d\nNLP methods, where the input sequences are trained jointly with their\nlabels to produce predictive models, have been applied to various\ntasks, such as detecting structural similarity or predicting stability\n3,4.A\nremarkable collection of supervised language models applied to bio-\nmolecules is available in the BioSeq-BLM platform\n5,6. Nevertheless,\nsince the inception of the Transformer7, unsupervised learning, where\nthe training occurs on unlabeled data, has emerged as a versatile tool\nfor language modeling. Several Transformer-based models, such as\nTCR-BERT\n8,e p i B E R T o p e9,E S M10,P r o t T r a n s11,o rP r o t e i n B E R T12, have\nshown to be very competitive with other methods13,14.M o s to ft h e s e\nmodels use BERT-like15 architectures and denoising autoencoding\ntraining objectives, i.e., they are pre-trained by corrupting the input\ntokens in some way and trying to reconstruct the original sentence\n2.\nAlthough these models could be adjusted for generation16, their most\ndirect application is sequence embedding.\nAnother important branch of language models beneﬁts from\nautoregressive training, i.e., models are trained to predict subsequent\nwords given a context. These models, the most well-known of which\nReceived: 1 April 2022\nAccepted: 13 July 2022\nCheck for updates\n1Department of Biochemistry, University of Bayreuth, Bayreuth, Germany.2Computational Biochemistry, University of Bayreuth, 95447 Bayreuth, Germany.\n3Present address: Institute of Informatics and Applications, University of Girona, Girona, Spain.e-mail: noelia.ferruz-capapey@uni-bayreuth.de\nNature Communications|         (2022) 13:4348 1\n1234567890():,;\n1234567890():,;\nare possibly the GPT-x series17, excel at generating long, coherent text—\nsometimes to the extent that much debate has been raised about their\npotential misuse\n18. Protein autoregressive language models, such as\nProGen19–21, RITA22,a n dD A R K23 have also been studied, and show the\npotential of autoregressive Transformers for protein design. Moti-\nvated by these works and the ever-increasing capabilities of English-\nspeaking models such as the GPT-x series, we wondered whether we\ncould train a generative model to (i) effectively learn the protein lan-\nguage, (ii) generateﬁt, stable proteins, and (iii) understand how these\nsequences relate to natural ones, including whether they sample\nunseen regions of the protein space.\nHere, we introduce ProtGPT2, an autoregressive Transformer\nmodel with 738 million parameters capable of generating de novo\nprotein sequences in a high-throughput fashion. ProtGPT2 has effec-\ntively learned the protein language upon being trained on about 50\nnon-annotated million sequences spanning the entire protein space.\nProtGPT2 generates protein sequences with amino acid and disorder\npropensities on par with natural ones while being“evolutionarily”\ndistant from the current protein space. Secondary structure prediction\ncalculates 88% of the sequences to be globular, in line with natural\nproteins. Representation of the protein space using similarity networks\nreveals that ProtGPT2 sequences explore‘dark’ areas of the protein\nspace by expanding natural superfamilies. The generated sequences\nshow predicted stabilities and dynamic properties akin to their natural\ncounterparts. Since ProtGPT2 has been already pre-trained, it can be\nused to generate sequences on standard workstations in a matter of\nseconds or be furtherﬁnetuned on sequence sets of a user’s choice to\naugment speciﬁc protein families. The model and datasets are avail-\nable in the HuggingFace repository\n24 at (https://huggingface.co/\nnferruz/ProtGPT2). Since protein design has an enormous potential\nto solve problems inﬁelds ranging from biomedical to environmental\nsciences25,26, we believe that ProtGPT2 is a timely advance towards\nefﬁcient high-throughput protein engineering and design.\nResults\nLearning the protein language\nThe major advances in the NLPﬁeld can be partially attributed to the\nscale-up of unsupervised language models. Unlike supervised learning,\nwhich requires the labeling of each data point, self-supervised (or\noften named unsupervised) methods do not require annotated data,\nthus promoting the use of ever-growing datasets such as Wikipedia or\nthe C4 Corpus\n27. Given both the growth of protein sequence databases\nand the lack of annotation for a signiﬁcant part of the protein space,\nprotein sequences have become great candidates for unsupervised\ntraining\n4,10,11 and now offer the opportunity to encode and generate\nprotein sequences.\nTo achieve this goal, we trained a Transformer7 to produce a\nmodel that generates protein sequences. Language models are\nstatistical models that assign probabilities to words and sentences.\nWe are interested in a model that assigns high probability to sen-\ntences (W) that are semantically and syntactically correct orﬁta n d\nfunctional, in the case of proteins. Because we are interested in a\ngenerative language model, we trained the model using an auto-\nregressive strategy. In autoregressive models, the probability of a\nparticular token or word (w\ni) in a sequence depends solely on its\ncontext, namely the previous tokens in the sequence. The total\nprobability of a sentence (W) is the combination of the individual\nprobabilities for each word (w\ni):\npWðÞ =\nYn\ni\npw i∣w<i\n/C0/C1\nð1Þ\nWe trained the Transformer by minimizing the negative log-\nlikelihood over the entire dataset. More intuitively, the model must\nlearn the relationships between a word wi —or amino acid—and all the\nprevious ones in the sequence, and must do so for each sequencek in\ndataset (D):\nLCLM = /C0 ∑\nD\nk =1\nlog pθ wk\ni ∣wk\n<i\n/C16/C17\nð2Þ\nTo learn the protein language, we used UniRef50 (UR50) (version\n2021_04), a clustering of UniProt at 50% identity. We chose this dataset\nversus larger versions of UniParc (such as UR100) as it was previously\nshown to improve generalization and performance for the ESM\nTransformers\n10.U n i r e f 5 0’s sequences populate the entire protein\nspace, including the dark proteome, regions of the protein space\nwhose structure is not accessible via experimental methods or\nhomology modeling\n28,29. For evaluation, we randomly excluded 10% of\nthe dataset sequences—these sequences are not seen by ProtGPT2\nduring the training process. Theﬁnal training datasets contained 44.9\nand 4.9 million sequences for training and evaluation, respectively. We\ntokenized our dataset using the BPE algorithm30.T h eﬁnal model is a\ndecoder-only architecture of 36 layers and 738 million parameters.\nAnalogous to the GLUE benchmark31—a collection of tools that\ncomputational linguists use to evaluate language models on different\ntasks such as question answering or translation—we also developed a\nseries of extrinsic tests to assess the quality of ProtGPT2-generated\nsequences. The following sections elaborate on how ProtGPT2 gen-\nerates de novo sequences with properties that resemble modern\nprotein space.\nStatistical sampling of natural amino acid propensities\nAutoregressive language generation is based on the assumption that\nthe probability distribution of a sequence can be decomposed into the\nproduct of conditional next-word distributions (Eq.1). However, there\nis still considerable debate about the best decoding strategy to emit\nsequences from a model\n32. It is not uncommon that well-trained gen-\neric language models that perform well in GLUE tasks generate inco-\nherent gibberish or repetitive text depending on the sampling\nprocedure\n32. We brieﬂy summarize here the most used sampling\nstrategies for language generation that we applied in this study.\nGreedy search strategy selects the word with the highest prob-\nability at each timestep. Although algorithmically simple, the gener-\nated sequences are deterministic and soon also become repetitive\n(Fig. 1a). Beam search tries to alleviate this problem by retaining the\nmost probable candidates, although the resulting texts still suffer from\nrepetitiveness and are not as surprising as those from humans, which\ntend to alternate low and high probability tokens\n32 (Fig. 1b). Lastly,\nrandom sampling moves away from deterministic sampling by ran-\ndomly picking a word out of the top-k most probable ones (Fig.1c, d).\nIn a recent study, Holtzman et al.\n32 investigated several sampling\nstrategies toﬁnd the best parameters for text generation. Inspired by\nthis work, we systematically generated sequences following different\nsampling strategies and parameters (Fig.1). To assess what sampling\nprocedure generates the most natural-like sequences, we compared\nthe amino acid propensities of the generated set to that found in\nnatural protein sequences (Methods). As stated by Hoffmann et al., we\nalso observe greedy and beam search to produce repetitive, determi-\nnistic sequences, while random sampling dramatically improves the\ngenerated propensities (Fig.1). Moreover, we also observe that high\nvalues of k are needed to generate sequences that resemble natural\nones, i.e., our best results occur in the range ofk > 800 and we speci-\nﬁcally chose k = 950 in this work (Fig.1h). As observed with other\ngenerative models\n33,34, our sampling improves when applying a repe-\ntition penalty of 1.2. Consequently, we used these sampling parameters\nfor the rest of this work.\nArticle https://doi.org/10.1038/s41467-022-32007-7\nNature Communications|         (2022) 13:4348 2\nProtGPT2 sequences encode globular proteins\nIn order to evaluate ProtGPT2’s generated sequences in the context of\nsequence and structural properties, we created two datasets, one with\nsequences generated from ProtGPT2 using the previously described\ninference parameters, and the other with randomly chosen sequences\nfrom UR50. Each dataset consists of 10,000 sequences. Since\nProtGPT2 was trained in an unsupervised manner, i.e., without\nincluding functional annotations, our analyses focus on validating the\nstructural and biochemical properties of ProtGPT2 sequences.\nWe ﬁrst studied disordered and secondary structural content in\nthe datasets. It has been previously shown that approximately 14% of\nthe proteins found in bacteria and archaea are disordered\n28.T ot h i s\nend, we ran IUPred335 to analyze if the ProtGPT2-generated sequences\nare more prone to be disordered than a set of natural sequences.\nInterestingly, our analysis shows a similar number of globular domains\namong the ProtGPT2-generated sequences (87.59%) and natural\nsequences (88.40%). Several methods have been reported that detect\nshort intrinsically disorder regions\n36. Since our goal is to provide high-\nlevel comparisons of globularity and prevalent disorder across data-\nsets, we further performed an analysis of the protein sequences at the\namino acid level using IUPred3. Remarkably, our results show a similar\ndistribution of ordered/disordered regions for the two datasets, with\n79.71 and 82.59% of ordered amino acids in the ProtGPT2 and natural\ndatasets, respectively (Table1).\nWe next investigated whether the similarities in disorder are a\nconsequence of equivalent secondary structure element content.\nTo this end, we computed PSIPRED\n37 predictions for the ProtGPT2\nand natural sequence datasets. The natural sequences display alpha-\nhelical, beta-sheet, and coil contents of 45.19, 41.87, and 12.93%,\nrespectively. The ProtGPT2 dataset presented percentages of 48.64,\n39.70, and 11.66%, respectively.\nThese results indicate that ProtGPT2 generates sequences that\nresemble globular domains whose secondary structure contents are\ncomparable to those found in the natural space.\nProtGPT2 sequences are similar yet distant to natural ones\nProteins have diversiﬁed immensely in the course of evolution via\npoint mutations as well as duplication and recombination. Using\nsequence comparisons, it is, however, possible to detect similarities\nbetween two proteins even when their sequences have signiﬁcantly\ndiverged. We wondered how related ProtGPT2 sequences are to nat-\nural ones. To this end, we utilized HHblits, a sensitive remote homol-\nogy detection tool that uses proﬁle hidden Markov models to search\nquery sequences against a database\n38. We searched for homologs of\nthe 10,000 sequences in ProtGPT2’s dataset against the Uniclust30\ndatabase39. For comparison purposes, we also performed the same\nsearch with the natural dataset using the same settings. In addition, to\nanalyze how completely random sequences would compare against\nProtGPT2 ones, we also crafted a third dataset by randomly con-\ncatenating the 25 letters in the vocabulary.\nBecause we want to provide a quantitative comparison of the\ndatasets’ relatedness to modern protein space, we produced identity\nvs sequence length plots (Fig.2). In detail, for each of the alignments\nfound in Uniclust30, we depict the one with the highest identity and\nlength. As a reference point in this sequence identity-length space, we\nuse the HSSP curve\n40, a boundary set to deﬁne the conﬁdence of\nFig. 1 | Examples with different sampling parameters for GPT2-large after the\ncontext input:‘ten best things to do in Lisbon’ (a– d) and ProtGPT2 without\ncontext (e– h). While greedy and beam search produce repetitive sentences (a, b)\nand protein sequences (e, f), sampling generates creative texts, which, however,\ncan be degenerate (c) or not sample natural sequence propensities (g) for small\nvalues of k. Larger values of k produce quality text (d) and sequences whose pro-\npensities match natural ones. Repetitive and degenerative text are shown in blue\nand orange, respectively.\nTable 1 | Disorder and secondary structure predictions of the\nnatural and ProtGPT2 dataset\nNatural dataset ProtGPT2 dataset\nIUPred3 (globular domains) 88.40% 87.59%\nOrdered content 79.71% 82.59%\nAlpha-helical content 45.19% 48.64%\nBeta-sheet content 41.87% 39.70%\nCoil content 12.93% 11.66%\n(n = 10,000 independent sequences/dataset).\nArticle https://doi.org/10.1038/s41467-022-32007-7\nNature Communications|         (2022) 13:4348 3\nprotein sequence relatedness. Proteins whose identity falls below this\ncurve, an area known as the“twilight zone”, do not necessarily have\nsimilar 3D structures nor are likely homologous. Since the sequences in\nthe ProtGPT2 and random datasets are not the consequence of protein\nevolution, we use the curve as a well-known threshold to compare the\ndatasets.\nWhen looking at the distribution of hits above and below the\ncurve, we observe that HHblitsﬁnds many hits in the Uniclust30\ndatabase that are related to the dataset of natural sequences (Fig.2a).\nSpeciﬁcally, out of the 10,000 dataset sequences, 9621 (96.2%) showed\nidentities above the HSSP curve. Similarly, 9295 ProtGPT2-generated\nsequences (93%) also have counterparts in the Uniclust30 database\nthat align above the HSSP curve (Fig.2b). Conversely, 93% of the ran-\ndomly generated sequences fall below this threshold (Fig.2c). Despite\nthese similar patterns for the natural and ProtGPT2 datasets, the two\ndatasets show differences in their distribution of hits. With a one-\nstandard-deviation range of 31.5–69.7%, the natural dataset has a\nhigher mean identity than the ProtGPT2 set, with a range of 32.9–64.1%\n(Fig. 2a, b). The differences between the natural and ProtGPT2 se-\nquence distributions are not statistically signiﬁcant (p value <0.05\nKolmogorov–Smirnoff). However, substantial differences between the\nnatural and ProtGPT2 datasets occur in the high-identity range (>90%).\nAlthough 365 sequences in the ProtGPT2 dataset have high-identity\nsequences in Uniclust30, they correspond in all cases to alignments\nbelow 15 amino acids, whereas the natural dataset displays\n760 sequences over 90% with an alignment length in the one-standard-\ndeviation range of 14.8–77.3 amino acids. These results suggest that\nProtGPT2 effectively generates sequences that are distantly related to\nnatural ones but are not a consequence of memorization and\nrepetition.\nProtGPT2 generates ordered structures\nOne of the most important features when designing de novo sequen-\nces is their ability to fold into stable ordered structures. We have\nevaluated the potentialﬁtness of ProtGPT2 sequences in comparison\nto natural and random sequences in the context of AlphaFold pre-\ndictions, Rosetta Relax scores, and molecular dynamics (MD)\nsimulations.\nAlphaFold\n41,42 produces a per-residue estimate of its conﬁdence\non a scale from 0–100 (pLDDT). This score has been shown to correlate\nwith order43: Low scores (pLDDT > 50) tend to appear in disordered\nregions, while excellent scores (pLDDT > 90) appear in ordered ones43.\nHere we producedﬁve structure predictions per sequence. The mean\npLDDT of the dataset is 63.2 when taking the best-scoring structure\nper sequence and 59.6 when averaging across allﬁve predictions\nper sequence. Moreover, 37% of sequences show pLDDT values over\n70, in agreement with other recent studies\n23. A representation of all\ndata points is shown in Supplementary Fig. 2a. Since pLDDT scores are\na proxy for structural order, we turned to the natural and random\ndatasets to see how they compare to ProtGPT2 sequences. In agree-\nment with previous works, 66% of the sequences in the natural dataset\nwere predicted with pLDDT values greater than 70\n43, giving an average\nvalue of 75.3 for the whole dataset (Supplementary Fig. 2b). In contrast,\nthe predictions in the random dataset revealed a mean pLDDT value of\n44, with only 7.4% of sequences with pLDDT values over 70 (Supple-\nmentary Fig. 2c).\nTo further validate the quality of the model, we performed\nRosetta-RelaxBB runs on the three datasets\n44. Rosetta Relax performs a\nMonte Carlo optimization over the Rosetta energy function, which\nresults in different backbone and rotamer conformations. Lower\nRosetta Energy conformers correlate with more relaxed structures\n45.\nThe most recent Rosetta Energy Forceﬁeld (REF2015) strongly corre-\nlates with experimental variables such as heat capacity, density, and\nenthalpy\n46. This scoring function reﬂects the thermodynamic stability\nof one static protein conformation. Here we have performed Rosetta\nRelax experiments for the 30,000 sequences of the three datasets\n(Fig. 3a). A broad rule of thumb is that the total score (Rosetta Energy\nUnits, REU) should lie between−1a n d−3 per residue\n47. We observe\nsuch distribution in the natural and ProtGPT2 datasets, with averages\nof 1.90 and 1.73 REU/residue, respectively. As expected, the dataset of\nrandom sequences showed an average value of 0.13 REU/residue.\nWe further tested if ProtGPT2 sequences show similar dynamic\nproperties as natural sequences. Proteins are dynamic entities; without\ntheir inherentﬂexibility, they would not be capable of interacting with\nother biomolecules and performing their functions in the cell\n48.T o\nevaluate whether ProtGPT2 sequences showﬂexibility patterns in the\nsame range as natural proteins, we randomly selected 12 sequences per\ndataset and ran three replicas of molecular dynamics (MD) of 100 ns\neach, totaling 108 trajectories and an aggregate time of 10.8 micro-\nseconds (Methods). To ensure that the dynamics observed during the\nsimulations were not an artifact of different pLDDT values—and hence\npossible different disorder predictions—we made sure that differences\namong dataset-pLDDT mean values were not statistically different\n(Supplementary Fig. 3). The Root Mean Square Deviation means for\nFig. 2 | Pairwise sequence identities vs. alignment length for each of the data-\nsets (a: natural (yellow), b: ProtGPT2 (green), and c: random (red)) as com-\nputed with HHblits against the Uniclust30 database.The lines depicted in red on\neach plot represent the HSSP curve, which we use as a reference to compare the\nthree datasets\n40. Each plot shows a hexbin compartmentalization of the best-\nscoring identities and their distributions. While natural (a) and protGPT2 (b)\nsequences show similar percentages below the curve, 93% of the sequences in the\nrandom dataset (c) do not have signiﬁcantly similar sequences in the Uniclust30\ndatabase. Natural and ProtGPT2 datasets show signiﬁcant differences in the high-\nidentity range (n = 10,000 independent sequences/dataset).\nArticle https://doi.org/10.1038/s41467-022-32007-7\nNature Communications|         (2022) 13:4348 4\neach of the trajectories in the natural and ProtGPT2 datasets resulted\nin average values of 2.93 and 3.12 Å, respectively (Fig.3b). As expected,\nthe random sequences showed signiﬁcant deviations during the tra-\njectories, with an average of 9.41 Å. While ProtGPT2 sequences showed\nhigher values than the natural ones, the distributions are not sig-\nniﬁcantly different (Mann–Whitney U-test, p value 0.39). The results\nindicate that ProtGPT2 sequences might have similar dynamic prop-\nerties as proteins found in nature. The complete list of the trajectories’\nRMSD is presented in Supplementary Figs. 4, 5.\nProtGPT2 transcends the boundaries of the current pro-\ntein space\nSeveral studies tried to reduce the large dimensionality of protein\nsequences into a few discernible dimensions for their analysis. Most\nrepresentation methods consist of (i) hierarchical classiﬁcations of\nprotein structures such as the ECOD and CATH databases\n49,50, (ii)\nCartesian representations51, and similarity networks52,53.W er e c e n t l y\nrepresented the structural space in a network that showed proteins as\nnodes, linked when they have a homologous and structurally-similar\nfragment in common\n54 and made the results available in the Fuzzle\ndatabase55. The network represented 25,000 domains from the seven\nmajor SCOP classes and showed that the modern known protein space\nhas both connected and“island-like” regions.\nIt is implausible that evolution has explored all possible\nprotein sequences\n56. Therefore, the challenge has been posed\nwhether we can design proteins that populate unexplored —or\ndark—regions of the protein space and if, by doing so, we can design\nnovel topologies and functions 56. Here, we integrated the\nProtGPT2 sequences into our network representation of the protein\nspace. To this end, we generated an HMM pro ﬁle for each\nSCOPe2.07 and ProtGPT2 sequence, compared them in an all-\nagainst-all fashion using HHsearch and represented the networks\nwith Protlego\n57. To avoid that speci ﬁc sequences with several\nalignments end up represented by the same node in the network, we\nduplicate entries with two non-overlapping alignments, as pre-\nviously described\n54.\nThe network contains 59,612 vertices and 427,378 edges, com-\nprising 1847 components or‘island-like’ clusters (Fig.4). The major\ncomponent accumulates more than half of the nodes (30,690)—a\nnumber signiﬁcantly higher than the number observed in a network\nproduced with the same settings but excluding ProtGPT2 sequences\n(Supplementary Fig. 6)— strongly suggesting that ProtGPT2 generates\nsequences that bridge separate islands in protein space. We select six\nexamples across different areas of the network from topologically\ndifferent SCOPe classes to showcase ProtGPT2 sequences at the\nstructural level (Fig.4). In particular, we report an all-β (751), twoα/β\n(4266, 1068), one membrane protein (4307), anα + β (486)a n da l l -α\n(785) structures. These structures illustrate ProtGPT2’s versatility at\ngenerating de novo structures. For each case, we searched the most\nsimilar protein structure found in the PDB database using FoldSeek\n58.\nProtGPT2 generates well-folded all-β structures (751, 4307), which\ndespite recent impressive advances59, have for long remained very\nchallenging60. ProtGPT2 also produces membrane proteins (4307),\nwhich pose a difﬁcult target for protein design due to the challenges at\nspecifying structure within the membrane and the laborious experi-\nmental characterizations61. Besides the generation of natural fold\nrepresentatives, ProtGPT2 also produces previously unreported\ntopologies. For example, we report protein4266, whose topology\ndoes not match any of the currently reported structures in the PDB,\nwith a low DALI Z-score of 5.4 and an RMSD of 3.0 Å to PDB5B48 over\n67 residues (identity 9%).\nNevertheless, possibly the most remarkable property of\nProtGPT2 sequences is their signiﬁcant deviation from all previously\ndesigned de novo structures, which often feature idealized topologies\nwith loops and minimal structural elements. De novo proteins have the\nadvantage of not carrying any evolutionary history and are thus\namenable as a scaffold for virtually any function, but in practice, the\nlack of embodiments and longer loops hamper the design of crevices,\nsurfaces, and cavities—necessary for the interaction with other mole-\ncules and function realization. ProtGPT2 sequences resemble the\ncomplexity of natural proteins, with multifaceted surfaces capable of\nallocating interacting molecules and substrates, thus paving the way\nfor functionalization. In Fig.4, we show structures486 and 1060,t w o\nexamples of such complex structures. In particular,1068 shows a TIM-\nbarrel fold, a topology which to date has met impressive success in de\nnovo design\n62–64, but whose idealized structure has nevertheless pro-\nven challenging to extend via additional secondary elements and\nlonger loops65,66.\nPreserved functional hotspots\nVisual inspection of the structural superimposition of the best hits\nfound with FoldSeek revealed several instances where the sidechains\nof ligand-interacting residues are conserved. Two examples are shown\nin Fig.5. The natural structure most similar to sequence357 (Fig. 5a)\ncorresponds to PDB code1X0P (chain A), a blue-light sensor domain\nthat binds FAD. When superimposing the structures, we observe that\n357 has retained the sidechain binding hotspots, with three residues\nidentical (D169, Q150, and N131) and two different but capable of\nforming the same interactions, Lysine at position R165 and Histidine at\nposition K127. Sequence475 (Fig.5b) is most similar to PDB code5M1T\n(chain A), a phosphodiesterase that folds into a TIM-barrel and binds to\nthe bacterial second messenger cyclic di-3′,5′-guanosine monopho-\nsphate (PDB three-letter code C2E). Out of the ﬁve sidechain-\ninteracting residues, the ProtGPT2 sequence preserves three resi-\ndues (Q455, R473, and E469), and includes one substitution for\nanother residue capable of hydrogen-bonding (aspartic acid for Q513).\nIt is remarkable to note that ProtGPT2 has generated these sequences\nin a zero-shot fashion, i.e., without furtherﬁnetuning in these two\nparticular folds. These results have impactful consequences for pro-\ntein engineering because ProtGPT2 appears to preserve binding\npositions in the generated sequences, despite the low identities (31.1\nand 29.2% for 357 and 45, respectively), and can be used to augment\nthe repertoires of speciﬁc folds and families.\nFig. 3 | Comparison of Rosetta and molecular dynamics calculations among the\nthree datasets. aAverage Rosetta energy units per residue for the three datasets.\nAlphaFold prediction structures were used as input for the Rosetta RelaxBB pro-\ntocol. 10,000 structures were run per dataset, one replica per system.b Root mean\nsquare deviation (RMSD) distribution for each MD dataset as computed by aver-\naging RMSDs independently for each trajectory, represented as a boxplot. Twelve\nstructures were simulated per dataset, three replicas per system. In both plots, the\nmedian is indicated as a black line; boxes depict the interquartile range (IQR), and\nwhiskers represent 1.5 x IQR. Points outside this range are displayed as individual\ndata points.\nArticle https://doi.org/10.1038/s41467-022-32007-7\nNature Communications|         (2022) 13:4348 5\nDiscussion\nThe design of de novo proteins harnessing artiﬁcial intelligence\nmethods has been meeting incredible success in the last 2 years10,67,68.\nMotivated by the unprecedented advances in NLP, we have imple-\nmented a generative language model, ProtGPT2, which has effectively\nlearned the protein language. ProtGPT2 can generate sequences that\nare distantly related to natural ones and whose structures resemble the\nknown structural space, with non-idealized complex structures. Since\nProtGPT2 has been trained on the entire sequence space, the\nsequences produced by the model can sample any region, including\nthe dark proteome and areas traditionally regarded as very challenging\nin the protein designﬁeld, such as all-β structures and membrane\nproteins. Visual superimposition of ProtGPT2 proteins with distantly\nrelated natural protein structures reveals that ProtGPT2 has also cap-\ntured functional determinants, preserving ligand-binding interactions.\nAs the design of artiﬁcial proteins can solve many biomedical and\nenvironmental problems, we see extraordinary potential in our protein\nlanguage model. ProtGPT2 designsﬁt globular proteins in a matter of\nseconds without requiring further training on a standard workstation.\nProtGPT2 can be conditioned towards a particular family, function, or\na: All alpha proteins\nb: All beta proteins f: Membrane/cell surface proteins\ng: Small proteinsc: ( / ) proteins e: Multi-domain proteins\nd: (  + ) proteins\n751 \n4GZV_H (13 %)\npLDDT: 81.4\n4266 \n*5B48_C (9%, 67aa)\npLDDT: 72.5\n785 \n6EOU_A (26 %)\npLDDT: 94.8\n4307 \n3QRC_B (18 %)\npLDDT: 70.4\n486 \n2HIY_B (23.2 %)\npLDDT: 94.8\n1068 \n3CT7_A (28%)\npLDDT: 90.2\nProtGPT2\nFig. 4 | An overview of the protein space and examples of proteins generated by\nProtGPT2.Each node represents a sequence. Two nodes are linked when they have\nan alignment of at least 20 amino acids and 70% HHsearch probability. Colors\ndepict the different SCOPe classes, and ProtGPT2 sequences are shown in white. As\nexamples, we select proteins of each of the majorﬁve SCOP classes: all-β structures\n(751), α/β (4266 and 1068), membrane protein (4307),α+β (486), and all-α (785).\nThe selected structures are colored according to the class of their most similar hit.\nThe structures were predicted with AlphaFold, and we indicate the code of the\nmost similar structure in the PDB as found by FoldSeek\n58, except for protein 4266,\nwhere no structures were found.\nArticle https://doi.org/10.1038/s41467-022-32007-7\nNature Communications|         (2022) 13:4348 6\nfold byﬁnetuning the model on a set of sequences of a user’s choice. In\nthis context, ProtGPT2 will enable the screening for proteins with\nsimilarities to natural proteins in order to improve,ﬁne-tune or alter a\nspeciﬁc biochemical function of a natural protein. Large-scale\nscreening of ProtGPT2-designed protein libraries might identify pro-\nteins with folds not captured in structural databases and functions that\nhave no related counterpart in the natural space. ProtGPT2 constitutes\na big step forward towards efﬁcient protein design and generation, and\nlays the groundwork for future experimental studies exploring the\nstructural and functional parameters of designed proteins, and their\nsubsequent real-world applications. Future efforts include the inclu-\nsion of conditional tags, which will enable the controlled generation of\nspeciﬁc functions.\nMethods\nVocabulary encoding\nWe use a BPE30 tokenizer to train the vocabulary of our dataset. BPE is a\nsub-word tokenization algorithm thatﬁnds the most frequently used\nword roots, ensuring better performance than one-hot tokenization\nand avoiding the out-of-vocabulary problem. Given the size of Uni-\nref50, we used Swiss-Prot (2021_04) containing >0.5 M sequences to\ntrain our tokenizer. Following the training strategy of GPT2\n17,o u rﬁnal\nvocabulary contained 50,256 tokens that correspond to the most\nwidely reused oligomers in protein space, with an average size of four\namino acids per token (Supplementary Fig. 1). Learned positional\nembeddings were used as in the original GPT2.\nDataset preparation\nWe took Uniref50 version 2021_04 as the dataset for training, con-\ntaining 49,874,565 sequences. 10% of the sequences were randomly\nselected to produce the validation dataset. Theﬁnal training and\nvalidation datasets contained 44.88 and 4.99 million sequences,\nrespectively. We produced two datasets, one using a block size\nof 512 tokens, and another one with 1024 tokens. The results shown\nin this work correspond to a model trained with a block size of\n512 tokens.\nModel pre-training\nWe use a Transformer decoder model as architecture for our training\nwhich processes input sequences tokenized with a BPE strategy. The\nmodel uses during training the original dot-scale self-attention as\nintroduced by ref.7. The model consist of 36 layers with a model\ndimensionality of 1280. The architecture matches that of the pre-\nviously released GPT2-large Transformer\n17, which was downloaded\nfrom HuggingFace24. Model weights were reinitialized prior to training.\nThe model was optimized using Adam (β1 =0 . 9 ,β2 = 0.999) with a\nlearning rate of 1e-03. For our main model, we trained 65,536 tokens\nper batch (128 GPUs × 512 tokens). A batch size of 8 per device was\nused, totaling 1024. The model trained on 128 NVIDIA A100s in 4 days.\nParallelism of the model was handled with DeepSpeed\n69.\nModel inference\nWe systematically sampled sequences using our main model using\ndifferent inference parameters. In particular, we varied the repetition\npenalty from a range of 1.1 to 3.0 at each 0.1 units, top_k from 250 to\n1000 sampling every 50 units, and a top_p from 0.7 to 1.0 with a\nwindow of 0.05 units. 100 sequences were produced for each sam-\npling parameter set and the frequency of their amino acids compared\nto natural sequences. We observed which parameters produced\nfewer differences in the set of the seven most common amino acids in\nnatural sequences. We also explored the beam search algorithm for\nbeams in the range 50 to 100 using a window of 1 unit but it produced\nworse matches in all cases. To determine amino acid frequencies in\nnatural sequences for comparison to ProtGPT2 samples, we\na\n1X0P_A\n357\nFAD\nQ150\nN131\nK127\nR165\nD169\n5M1T_A\n475\nR473\nQ513\nR614\nQ455E469\nC2E\nb\nFig. 5 | Superimposition of the predicted structures for sequences 357 and 475\nand the respective top scoring proteins in FoldSeek. aStructural alignment of\n357 with pdb1X0P (chain A, blue). Shown areﬁve residues in 1X0P that interact via\ntheir sidechains with the ligand FAD. Of these, three are identical in357,a n d\nanother two correspond to substitutions to the same amino acid type (R165 to\nlysine and Q150 to histidine).b Structural alignment of475 with pdb5M1T(chain A)\ndepictingﬁve sidechain-interacting residues with ligand C2E. All amino acids in475\nare conserved except for residue R614, which was substituted by a glycine. The PDB\nstructures are shown in color with their sidechains in a thinner representation.\nArticle https://doi.org/10.1038/s41467-022-32007-7\nNature Communications|         (2022) 13:4348 7\nrandomly picked 1 million sequences from the Uniref50 dataset. The\nbest matching parameters were further downsampled with ﬁner\nwindows and their frequencies compared with radar plots, as shown\nin Fig. 1 in the main text. The best performing parameters in our\ndataset were top_k 950, repetition penalty of 1.2, and default tem-\nperature and top_p values of 1.\nSequence dataset generation\nThree sequence datasets were produced to compare their properties.\nThe ProtGPT2 dataset was generated by sampling 1000 batches of\n100 sequences, each with the selected inference parameters and a\nwindow context of 250 tokens. This step produced 100,000 sequen-\nces. We ﬁltered from this set those sequences whose length had\nbeen cut due to the window context, giving a total of 29,876 sequen-\nces. From this set, we randomly selected 10,000 sequences.\nTheir average length is 149.2 ± 50.9 amino acids. The natural dataset\nwas created by randomly sampling 100,000 sequences from Uni-\nref50. 10,000 of these sequences were further chosen to ensure their\naverage and standard deviation lengths matched that of the ProtGPT2\ndataset sequences. The random dataset was created by concatenating\nthe 25 amino acids that appear in UniRef50, which includes the\n20 standard amino acids and other IUPAC codes such as“X”, “B”, “U”,\n“O”,a n d“Z”, by randomly concatenating them into sequences with a\nlength taken from a normal distribution between 5 and 267\namino acids.\nHomology detection\nEach sequence in the three 10k datasets was searched for similarity\nagainst the PDB70 and uniclust30 databases using HHblits\n70.W eu s e d\nthe Uniclust30 database version 2018_08 and the pdb70 version\n2021_04. As HHblits produces a list of alignments we selected all those\nover the HSSP curve as possible matches, and from these, selected the\nlargest alignment. Thus, for each sequence in each dataset, the longest\nand the highest identity scoring alignment was selected and repre-\nsented in Fig.2.\nDisorder prediction\nIUPred3 was run on ProtGPT2 and natural datasets using all\nthree possible options to detect shorter (“short”)o rl o n g e r(“longer”)\nunstructured regions, as well as structured regions (“glob”)35. Ordered\ncontent was determined with the“short” option. The output of the\n“glob” analysis also reports if any structured, globular domain was\nfound, as shown in Table1. We ran secondary structure prediction\nusing PSIPRED v4.0 for each sequence in natural and ProtGPT2\ndatasets\n37. The alignments of the abovementioned HHblits searches\nwere used as multiple sequence alignments. We computed the per-\ncentages for each secondary element by dividing the number of amino\nacids with a certain prediction by the total number of amino acids with\nac o nﬁdence value of 5 or more.\nAlphaFold2 structure prediction\nWe predicted ﬁve structures for each sequence in the ProtGPT2\ndataset using AlphaFold ColabFold batch v1.241.\nNetwork construction\nSequences in the ProtGPT2 and SCOP 2.07ﬁltered at 95% datasets were\njoined. For each sequence, we produced a multiple sequence align-\nment (MSA) using HHblits against the database Uniclust 2018_08.\nHidden Markov model proﬁles were produced for each MSA using\nHHblits\n70, and an all-against-all search for each proﬁle was performed\nusing HHsearch38. The network was constructed by representing every\nsequence as a node, and linking two nodes whenever they have an\nalignment of at least 20 amino acids with 70% HHsearch probability.\nExtensive details on the all-against-all comparison and network con-\nstruction, and tools to generate the networks can be found in our\nprevious works Fuzzle\n54,55 and Protlego57. Detection of similar topolo-\ngies was determined with FoldSeek58.\nMolecular dynamics simulations\nSimulation systems were built and run with the software HTMD71.I na l l\ncases, systems comprised solvated all-atom cubic boxes. Simulation\nboxes consisted of a protein centered at the origin of coordinates and\nexplicit solvent molecules and neutralizing NaCl ions were added to\neach box. The Amber 19SB forceﬁeld was used\n72. Three replicas were\nconstructed per sequence. All systems were minimized, equilibrated,\nand run with ACEMD\n73 using default parameters: each system was\nminimized and relaxed under NPT conditions for 1 ns at 1 atm and\n3 0 0Ku s i n gat i m e - s t e po f4f s ,r i g i db o n d s ,c u t o f fo f9Å ,a n dP M Ef o r\nlong-range electrostatics. Heavy protein and ligand atoms were con-\nstrained by a 10 kcal/mol/Å2 spring constant. Production simulations\nwere run in the NVT ensemble using a Langevin thermostat with a\ndamping of 0.1 ps\n−1 and a hydrogen mass repartitioning scheme to\nachieve timesteps of 4 fs74.\nRosetta calculations\nRosetta Relax runs were produced with the Rosetta Software Suite\nv3.12\n44 using as input structure the best-scoring prediction from\nAlphaFold.\nReporting summary\nFurther information on research design is available in the Nature\nResearch Reporting Summary linked to this article.\nData availability\nThe model weights are publicly available in the HuggingFace\nrepository: https://huggingface.co/nferruz/ProtGPT2 and Zenodo:\nhttps://doi.org/10.5281/zenodo.6796843[https://zenodo.org/record/\n6796843#.YswB9XbMIVA]. The dataset for training is available at:\nhttps://huggingface.co/datasets/nferruz/UR50_2021_04.T h et h r e e\nsequence datasets in this work are available at:https://huggingface.co/\ndatasets/nferruz/dataset_fastas. The AlphaFold predictions for the\nthree datasets are available at https://huggingface.co/datasets/\nnferruz/dataset_alphafold. The Uniref50 original database version\n21_04 is available at https://ftp.uniprot.org/pub/databases/uniprot/\nprevious_releases/release-2021_04/. The Uniclust30 database version\n2018_08 is available at http://gwdu111.gwdg.de/~compbiol/uniclust/\n2018_08/uniclust30_2018_08_hhsuite.tar.gz.\nCode availability\nThe model was trained with the HugginFace transformers Trainer\nversion 4.14.1. The code and documentation are available here:https://\nhuggingface.co/docs/transformers/main_classes/trainer.\nReferences\n1. Yang, K. K., Wu, Z. & Arnold, F. H. Machine-learning-guided directed\nevolution for protein engineering.Nat. Methods16,6 8 7–694 (2019).\n2. Ferruz, N. & Höcker, B. Controllable protein design with language\nmodels. Nat. Mach. Intell.4,5 2 1–532 (2022).\n3. Bepler, T. & Berger, B. Learning the protein language: evolution,\nstructure, and function.Cell Syst.12,6 5 4–669.e3 (2021).\n4. Alley, E. C., Khimulya, G., Biswas, S., AlQuraishi, M. & Church, G. M.\nUniﬁed rational protein engineering with sequence-based deep\nrepresentation learning.Nat. Methods16,1 3 1 5–1322 (2019).\n5. Li, H. L., Pang, Y. H. & Liu, B. BioSeq-BLM: a platform for analyzing\nDNA, RNA and protein sequences based on biological language\nmodels. Nucleic Acids Res.49,e 1 2 9–e129 (2021).\n6. Liu, B., Gao, X. & Zhang, H. BioSeq-Analysis2.0: an updated plat-\nform for analyzing DNA, RNA and protein sequences at sequence\nlevel and residue level based on machine learning approaches.\nNucleic Acids Res.47,e 1 2 7–e127 (2019).\nArticle https://doi.org/10.1038/s41467-022-32007-7\nNature Communications|         (2022) 13:4348 8\n7. Vaswani, A. et al. Transformer: attention is all you need. InAdvances\nin Neural Information Processing Systems5999–6009 (2017).\n8. Wu, K. et al. TCR-BERT: learning the grammar of T-cell receptors for\nﬂexible antigen-xbinding analyses. Preprint atbioRxiv https://doi.\norg/10.1101/2021.11.18.469186(2021).\n9. Park, M., Seo, S., Park, E. & Kim, J. EpiBERTope: a sequence-based\npre-trained BERT model improves linear and structural epitope\nprediction by learning long-distance protein interactions effec-\ntively. Preprint atbioRxiv https://doi.org/10.1101/2022.02.27.\n481241 (2022).\n10. Rives, A. et al. Biological structure and function emerge from scal-\ning unsupervised learning to 250 million protein sequences.Proc.\nNatl. Acad. Sci. 118, e2016239118 (2021).\n11. Elnaggar, A. et al. ProtTrans: Towards Cracking the Language of\nLifes Code Through Self-Supervised Deep Learning and High Per-\nformance Computing. InIEEE Transactions on Pattern Analysis and\nMachine Intelligence. https://doi.org/10.1109/TPAMI.2021.3095381.\n1 2 . B r a n d e s ,N . ,O f e r ,D . ,P e l e g ,Y . ,R a p p o p o r t ,N .&L i n i a l ,M .P r o -\nteinBERT: a universal deep-learning model of protein sequence and\nfunction.Bioinformatics38,2 1 0 2–2110 (2022).\n13. Yang, K. K., Lu, A. X. & Fusi, N. K. Convolutions are competitive with\ntransformers for protein sequence pretraining. Preprint atbioRxiv\nhttps://doi.org/10.1101/2022.05.19.492714(2022).\n14. Rao, R. et al. Evaluating protein transfer learning with TAPE.Adv.\nNeural Inf. Process. Syst. 32,9 6 8 9–9701 (2019).\n1 5 . D e v l i n ,J . ,C h a n g ,M . - W . ,L e e ,K .&T o u t a n o v a ,K .B E R T :p r e - t r a i n i n g\nof deep bidirectional transformers for language understanding.\nPreprint at arXiv:1810.04805 (2018).\n16. Johnson, S. R., Monaco, S., Massie, K. & Syed, Z. Generating novel\nprotein sequences using Gibbssampling of masked language\nmodels. Preprint atbioRxiv https://doi.org/10.1101/2021.01.26.\n428322 (2021).\n17. Radford, A. et al. Language models are unsupervised multitask\nlearners.https://github.com/codelucas/newspaper(2018).\n18. OpenAI says its text-generating algorithm GPT-2 is too dangerous\nto release.https://slate.com/technology/2019/02/openai-gpt2-\ntext-generating-algorithm-ai-dangerous.html(2019).\n19. Madani, A. et al. ProGen: language modeling for protein genera-\ntion. (2020).\n20. Madani, A. et al. Deep neural language modeling enables func-\ntional protein generation across families. Preprint atbioRxiv\nhttps://doi.org/10.1101/2021.07.18.452833(2021).\n21. Nijkamp, E. et al. ProGen2: exploring the boundaries of protein\nlanguage models. Preprint at arxivhttps://doi.org/10.48550/arxiv.\n2206.13517(2022).\n22. Hesslow, D. et al. RITA: a Study on Scaling Up Generative Protein\nSequence Models. Preprint at arXiv 2205.05789 (2022).\n23. Moffat, L., Kandathil, S. M. & Jones, D. T. Design in the DARK:\nlearning deep generative models for de novo protein design. Pre-\nprint atbioRxiv https://doi.org/10.1101/2022.01.27.478087(2022).\n24. Wolf, T. et al. HuggingFace’s transformers: state-of-the-art natural\nlanguage processing. Preprint at arXiv 1910.03771 (2019).\n25. Campeotto, I. et al. One-step design of a stable variant of the\nmalaria invasion protein RH5 for use as a vaccine immunogen.\nP r o c .N a t lA c a d .S c i .U S A114,9 9 8–1002 (2017).\n26. Lu, H. et al. Machine learning-aided engineering of hydrolases for\nPET depolymerization.Nature 604,6 6 2–667 (2022).\n27. Raffel, C. et al. Exploring the limits of transfer learning with a uni-\nﬁed text-to-text transformer.J. Mach. Learn. Res.21,1 –67 (2020).\n28. Perdigão, N. et al. Unexpected features of the dark proteome.Proc.\nNatl Acad. Sci. USA112,1 5 8 9 8–15903 (2015).\n29. Perdigão, N., Rosa, A. C. & O’Donoghue, S. I. The Dark Proteome\nDatabase.BioData Min. 10,2 4( 2 0 1 7 ) .\n30. Gage, P. A new algorithm for data compression.https://doi.org/10.\n5555/177910.177914.\n31. Wang, A. et al. GLUE: a multi-task benchmark and analysis platform\nfor natural language understanding. Preprint atarXiv (2018).\n32. Holtzman, A., Buys, J., Du, L., Forbes, M. & Choi, Y. The curious case\nof neural text degeneration.CEUR Workshop Proc. 2540,( 2 0 1 9 ) .\n33. Keskar, N. S., McCann, B., Varshney, L. R., Xiong, C. & Socher, R.\nCTRL: a conditional transformer language model for controllable\ngeneration. Preprint at arxiv (2019).\n34. Madani, A. et al. ProGen: language modeling for protein genera-\ntion. Preprint atbioRxiv https://doi.org/10.1101/2020.03.07.\n982272 (2020).\n35. Abor Erd, G. ´, Os,˝, Atyásaty´atyás Pajkos, M. ´, Dosztányi, Z. &\nDosztányi, D. IUPred3: prediction of protein disorder enhanced\nwith unambiguous experimental annotation and visualization of\nevolutionary conservation.Nucleic Acids Res.49,\nW297–W303 (2021).\n36. Tang, Y. J., Pang, Y. H. & Liu, B. DeepIDP-2L: protein intrinsically\ndisordered region prediction bycombining convolutional atten-\ntion network and hierarchical attention network.Bioinformatics38,\n1252–1260 (2022).\n37. Buchan, D. W. A. & Jones, D. T. The PSIPRED protein analysis\nworkbench: 20 years on.Nucleic Acids Res.47,\nW402–W407 (2019).\n38. Söding, J. Protein homology detection by HMM-HMM comparison.\nBioinformatics21,9 5 1–960 (2005).\n39. Mirdita, M. et al. Uniclust databases of clustered and deeply\nannotated protein sequences and alignments.Nucleic Acids Res.\n45\n, D170 (2017).\n40. Rost, B. Twilight zone of protein sequence alignments.Protein Eng.\nDes. Sel.12,8 5–94 (1999).\n41. Mirdita, M. et al. ColabFold: making protein folding accessible to\nall. Nat. Methods19,6 7 9–682 (2022).\n42. Jumper, J. et al. Highly accurate protein structure prediction with\nAlphaFold.Nature 596,5 8 3–589 (2021).\n43. Tunyasuvunakool, K. et al. Highly accurate protein structure pre-\ndiction for the human proteome.Nature 596,5 9 0–596 (2021).\n4 4 . D i M a i o ,F . ,L e a v e r - F a y ,A . ,B r a d l e y ,P . ,B a k e r ,D .&A n d r é ,I .M o d -\neling symmetric macromolecular structures in Rosetta3.PLoS\nONE 6, e20450 (2011).\n4 5 . S a u e r ,M .F . ,S e v y ,A .M . ,C r o w e ,J .E .&M e i l e r ,J .M u l t i - s t a t ed e s i g n\nof ﬂexible proteins predicts sequences optimal for conformational\nchange. PLOS Comput. Biol.16, e1007339 (2020).\n46. Alford, R. F. et al. The Rosetta all-atom energy function for mac-\nromolecular modeling and design.J. Chem. Theory Comput.13,\n3031 (2017).\n47. Wedemeyer, M. J., Mueller, B. K., Bender, B. J., Meiler, J. & Volkman,\nB. F. Modeling the complete chemokine-receptor interaction.\nMethods Cell Biol.149,2 8 9–314 (2019).\n48. Miller, M. D. & Phillips, G. N. Moving beyond static snapshots:\nprotein dynamics and the protein Data Bank.J. Biol. Chem. 296,\n100749 (2021).\n49. Cheng, H. et al. ECOD: an evolutionary classiﬁcation of protein\ndomains.PLoS Comput. Biol.10, e1003926 (2014).\n50. Sillitoe, I. et al. CATH: increased structural coverage of functional\nspace. Nucleic Acids Res.49,D 2 6 6–D273 (2021).\n51. Osadchy, M. & Kolodny, R. Maps of protein structure space reveal a\nfundamental relationship between protein structure and function.\nP r o c .N a t lA c a d .S c i .U S A108,1 2 3 0 1–12306 (2011).\n5 2 . A l v a ,V . ,R e m m e r t ,M . ,B i e g e r t ,A . ,L u p a s ,A .N .&S ö d i n g ,J .Ag a l a x y\nof folds.Protein Sci.19,1 2 4–130 (2010).\n53. Nepomnyachiy, S., Ben-Tal, N.& Kolodny, R. Global view of the\nprotein universe.Proc. Natl Acad. Sci. USA111,1 1 6 9 1–11696\n(2014).\n54. Ferruz, N. et al. Identiﬁcation and analysis of natural building\nblocks for evolution-guided fragment-based protein design.J. Mol.\nBiol. 432,3 8 9 8–3914 (2020).\nArticle https://doi.org/10.1038/s41467-022-32007-7\nNature Communications|         (2022) 13:4348 9\n55. Ferruz, N., Michel, F., Lobos, F., Schmidt, S. & Höcker, B. Fuzzle 2.0:\nligand binding in natural protein building blocks.Front. Mol. Biosci.\n8, 805 (2021).\n5 6 . H u a n g ,P .S . ,B o y k e n ,S .E .&B a k e r ,D .T h ec o m i n go fa g eo fd en o v o\nprotein design.Nature 537,3 2 0–327 (2016).\n5 7 . F e r r u z ,N . ,N o s k e ,J .&H ö c k e r ,B .P r o t l e g o :aP y t h o np a c k a g ef o rt h e\nanalysis and design of chimeric proteins.Bioinformaticshttps://\ndoi.org/10.1093/bioinformatics/btab253(2021).\n58. Kempen, M. van et al. Foldseek: fast and accurate protein structure\nsearch. Preprint atbioRxiv https://doi.org/10.1101/2022.02.07.\n479398 (2022).\n59. Marcos, E. et al. De novo design of a non-localβ-sheet protein with\nhigh stability and accuracy.Nat. Struct. Mol. Biol.25,\n1028–1034 (2018).\n60. Pan, X. & Kortemme, T. Recent advances in de novo protein design:\nPrinciples, methods, and applications.J. Biol. Chem.296,\n100558 (2021).\n61. Xu, C. et al. Computational design of transmembrane pores.Nature\n585,1 2 9–134 (2020).\n62. Romero-Romero, S. et al. The Stability Landscape of de novo TIM\nBarrels Explored by a Modular Design Approach.J. Mol. Biol. 433,\n167153 (2021).\n63. Huang, P. S. et al. De novo design of a four-fold symmetric TIM-\nbarrel protein with atomic-level accuracy.Nat. Chem. Biol.12,\n29–34 (2016).\n64. Anand, N. et al. Protein sequence design with a learned potential.\nNat. Commun.13,1 –11 (2022).\n6 5 . K o r d e s ,S . ,R o m e r o - R o m e r o ,S . ,L u t z ,L .&H ö c k e r ,B .An e w l y\nintroduced salt bridge cluster improves structural and biophysical\nproperties of de novo TIM barrels.Protein Sci.31,5 1 3–527\n(2022).\n6 6 . W i e s e ,J .G . ,S h a n m u g a r a t n a m ,S .&H ö c k e r ,B .E x t e n s i o no fad e\nnovo TIM barrel with a rationally designed secondary structure\nelement.Protein Sci.30,9 8 2–989 (2021).\n67. Senior, A. W. et al. Improved protein structure prediction using\npotentials from deep learning.Nature 577,7 0 6–710 (2020).\n68. Ferruz, N. & Höcker, B. Dreaming ideal protein structures.Nat.\nBiotechnol.40,1 7 1–172 (2022).\n6 9 . R a s l e y ,J . ,R a j b h a n d a r i ,S . ,R u w a s e ,O .&H e ,Y .D e e p S p e e d :\nsystem optimizations enable training deep learning models\nwith over 100 billion parameters. InProc. 26th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data\nMining.3 5 0 5\n–3506 (Association for Computing Machin-\nery, 2020).\n70. Remmert, M., Biegert, A., Hauser, A. & Söding, J. HHblits: lightning-\nfast iterative protein sequence searching by HMM-HMM align-\nment. Nat. Methods9,1 7 3–175 (2011).\n7 1 . D o e r r ,S . ,H a r v e y ,M .J . ,N o é ,F .&D eF a b r i t i i s ,G .H T M D :h i g h -\nthroughput molecular dynamics for molecular discovery.J. Chem.\nTheory Comput.12,1 8 4 5–1852 (2016).\n72. Tian, C. et al. Ff19SB: amino-acid-speciﬁc protein backbone para-\nmeters trained against quantum mechanics energy surfaces in\nsolution.J. Chem. Theory Comput.16,5 2 8–552 (2020).\n73. Harvey, M. J., Giupponi, G. & De Fabritiis, G. ACEMD: accelerating\nbiomolecular dynamics in the microsecond time scale.J. Chem.\nTheory Comput.5,1 6 3 2–1639 (2009).\n7 4 . F e r r u z ,N . ,H a r v e y ,M .J . ,M e s t r e s ,J .&D eF a b r i t i i s ,G .I n s i g h t sf r o m\nfragment hit binding assays by molecular simulations.J. Chem. Inf.\nModel. 55, 2200–2205 (2015).\nAcknowledgements\nThe authors gratefully acknowledge the scientiﬁcs u p p o r ta n dH P C\nresources provided by the Erlangen National High-Performance Com-\nputing Center (NHR@FAU) of the Friedrich-Alexander-Universität\nErlangen-Nürnberg (FAU) under an early-access NHR project. NHR\nfunding is provided by federal and Bavarian state authorities. NHR@FAU\nhardware is partially funded by the German Research Foundation (DFG)\n— 440719683. We thank Thomas Zeiser for his considerate support and\nSurbhi Dhingra for feedback on the manuscript. N.F. acknowledges\nsupport from an AGAUR Beatriu de Pinós MSCA-COFUND Fellowship\n(project 2020-BP-00130). The authors thank funding from the German\nResearch Foundation (DFG) - 491183248 and the Open Access Publish-\ni n gF u n do ft h eU n i v e r s i t yo fB a y r e u t h .\nAuthor contributions\nN.F conceived the work, trained the model, analyzed the data, and wrote\nthe manuscript. S.S produced the IUPred3 disorder predictions and\nanalysis and wrote the manuscript. B.H analyzed the data and wrote the\nmanuscript. The three authors discussed the results and supervised\nthe work.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains supplementary\nmaterial available at\nhttps://doi.org/10.1038/s41467-022-32007-7.\nCorrespondenceand requests for materials should be addressed to\nNoelia Ferruz.\nPeer review informationNature Communicationsthanks the anon-\nymous reviewer(s) for their contribution to the peer review of this\nwork. Peer reviewer reports are available.\nReprints and permission informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons license, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons license and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2022\nArticle https://doi.org/10.1038/s41467-022-32007-7\nNature Communications|         (2022) 13:4348 10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7096803188323975
    },
    {
      "name": "Protein design",
      "score": 0.49959897994995117
    },
    {
      "name": "Computational biology",
      "score": 0.4901643693447113
    },
    {
      "name": "Natural language",
      "score": 0.4679011106491089
    },
    {
      "name": "Protein structure",
      "score": 0.45659881830215454
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.45216572284698486
    },
    {
      "name": "Bridging (networking)",
      "score": 0.44826966524124146
    },
    {
      "name": "Protein sequencing",
      "score": 0.43854403495788574
    },
    {
      "name": "Protein evolution",
      "score": 0.42500782012939453
    },
    {
      "name": "Sequence (biology)",
      "score": 0.410049706697464
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40112870931625366
    },
    {
      "name": "Peptide sequence",
      "score": 0.2927696406841278
    },
    {
      "name": "Biology",
      "score": 0.2424488365650177
    },
    {
      "name": "Genetics",
      "score": 0.13089275360107422
    },
    {
      "name": "Gene",
      "score": 0.09126999974250793
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I54009628",
      "name": "University of Bayreuth",
      "country": "DE"
    }
  ],
  "cited_by": 627
}