{
  "title": "The transformer earthquake alerting model: a new versatile approach to earthquake early warning",
  "url": "https://openalex.org/W3084541745",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4222765634",
      "name": "Münchmeyer, Jannes",
      "affiliations": [
        "Humboldt-Universität zu Berlin",
        "Helmholtz Centre Potsdam - GFZ German Research Centre for Geosciences"
      ]
    },
    {
      "id": null,
      "name": "Bindi, Dino",
      "affiliations": [
        "Helmholtz Centre Potsdam - GFZ German Research Centre for Geosciences"
      ]
    },
    {
      "id": "https://openalex.org/A2247720008",
      "name": "Leser, Ulf",
      "affiliations": [
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A3091305792",
      "name": "Tilmann, Frederik",
      "affiliations": [
        "Freie Universität Berlin",
        "Helmholtz Centre Potsdam - GFZ German Research Centre for Geosciences"
      ]
    },
    {
      "id": null,
      "name": "M\\\"unchmeyer, Jannes",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2914736117",
    "https://openalex.org/W2098941059",
    "https://openalex.org/W2963518130",
    "https://openalex.org/W6634817459",
    "https://openalex.org/W2761021936",
    "https://openalex.org/W2910713906",
    "https://openalex.org/W2950689189",
    "https://openalex.org/W6630476760",
    "https://openalex.org/W2799508466",
    "https://openalex.org/W3006351417",
    "https://openalex.org/W1987940947",
    "https://openalex.org/W2792048852",
    "https://openalex.org/W2026338384",
    "https://openalex.org/W2903747695",
    "https://openalex.org/W1540953370",
    "https://openalex.org/W2913447624",
    "https://openalex.org/W2740234317",
    "https://openalex.org/W2999573551",
    "https://openalex.org/W2789621234",
    "https://openalex.org/W2916673431",
    "https://openalex.org/W2992786058",
    "https://openalex.org/W2987173766",
    "https://openalex.org/W3014316192",
    "https://openalex.org/W3080809230",
    "https://openalex.org/W2069252613",
    "https://openalex.org/W2948194985",
    "https://openalex.org/W6741441694",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2135566483",
    "https://openalex.org/W1579853615",
    "https://openalex.org/W4301409532",
    "https://openalex.org/W3021034800",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2962843773",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W1512572083",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970859221",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W3102041920"
  ],
  "abstract": "SUMMARY Earthquakes are major hazards to humans, buildings and infrastructure. Early warning methods aim to provide advance note of incoming strong shaking to enable preventive action and mitigate seismic risk. Their usefulness depends on accuracy, the relation between true, missed and false alerts and timeliness, the time between a warning and the arrival of strong shaking. Current approaches suffer from apparent aleatoric uncertainties due to simplified modelling or short warning times. Here we propose a novel early warning method, the deep-learning based transformer earthquake alerting model (TEAM), to mitigate these limitations. TEAM analyses raw, strong motion waveforms of an arbitrary number of stations at arbitrary locations in real-time, making it easily adaptable to changing seismic networks and warning targets. We evaluate TEAM on two regions with high seismic hazard, Japan and Italy, that are complementary in their seismicity. On both data sets TEAM outperforms existing early warning methods considerably, offering accurate and timely warnings. Using domain adaptation, TEAM even provides reliable alerts for events larger than any in the training data, a property of highest importance as records from very large events are rare in many regions.",
  "full_text": "THE TRANSFORMER EARTHQUAKE ALERTING MODEL : A NEW\nVERSATILE APPROACH TO EARTHQUAKE EARLY WARNING\nJannes Münchmeyer1,2,∗, Dino Bindi1, Ulf Leser2, Frederik Tilmann1,3\n1 Deutsches GeoForschungsZentrum GFZ, Potsdam, Germany\n2 Institut für Informatik, Humboldt-Universität zu Berlin, Berlin, Germany\n3 Insitut für geologische Wissenschaften, Freie Universität Berlin, Berlin, Germany\n∗To whom correspondence should be addressed: munchmej@gfz-potsdam.de\nJanuary 12, 2021\nABSTRACT\nEarthquakes are major hazards to humans, buildings and infrastructure. Early warning methods\naim to provide advance notice of incoming strong shaking to enable preventive action and mitigate\nseismic risk. Their usefulness depends on accuracy, the relation between true, missed and false alerts,\nand timeliness, the time between a warning and the arrival of strong shaking. Current approaches\nsuffer from apparent aleatoric uncertainties due to simpliﬁed modelling or short warning times. Here\nwe propose a novel early warning method, the deep-learning based transformer earthquake alerting\nmodel (TEAM), to mitigate these limitations. TEAM analyzes raw, strong motion waveforms of an\narbitrary number of stations at arbitrary locations in real-time, making it easily adaptable to changing\nseismic networks and warning targets. We evaluate TEAM on two regions with high seismic hazard,\nJapan and Italy, that are complementary in their seismicity. On both datasets TEAM outperforms\nexisting early warning methods considerably, offering accurate and timely warnings. Using domain\nadaptation, TEAM even provides reliable alerts for events larger than any in the training data, a\nproperty of highest importance as records from very large events are rare in many regions.\n1 Introduction\nThe concept of earthquake early warning has been around for over a century, but the necessary instrumentation and\nmethodologies have only been developed in the last three decades [1, 2]. Early warning systems aim to raise alerts\nif shaking levels likely to cause damage are going to occur. Existing methods split into two main classes: source\nestimation based and propagation based. The former, like EPIC [3] or FINDER [4], estimate the source properties of an\nevent, i.e., its location or fault extent and magnitude, and then use a ground motion prediction equation (GMPE) to\ninfer shaking at target sites. They provide long warning times, but incur a large apparent aleatoric uncertainty due to\nsimpliﬁed assumptions in the source estimation and in the GMPE [ 5]. Propagation based methods, like PLUM [ 5],\ninfer the shaking at a given location from measurements at nearby seismic stations. Predictions are more accurate, but\nwarning times are reduced, as warnings require measurements of strong shaking at nearby stations [6].\nRecently, machine learning methods, particularly deep learning methods, have emerged as a tool for fast assessment of\nearthquakes. Under certain circumstances, they led to improvements in various tasks, e.g., estimation of magnitude\n[7, 8], location [ 9, 10] or peak ground acceleration (PGA) [ 11]. Nonetheless, no existing method is applicable to\nearly warning because they lack real-time capabilities, instead requiring ﬁxed waveform windows after the P arrival.\nFurthermore, the existing methods are restricted in terms of their input stations, as they use either a single seismic station\nas input [7, 8] or a ﬁxed set of seismic stations, that needs to be deﬁned at training time [9, 11, 12]. While single station\napproaches miss out on a considerable amount of information obtainable from combining waveforms from different\nsources, ﬁxed stations approaches have limited adaptability to changing networks. The latter is of particular concern as\nfor large, dense networks the stations of interest, i.e., the stations closest to an event, will change on a per-event basis.\nFinally, existing methods systematically underestimate the strongest shaking and the highest magnitudes, as these are\narXiv:2009.06316v4  [physics.geo-ph]  11 Jan 2021\nThe transformer earthquake alerting model\nFigure 1: Map of the station (left) and event (right) distribution in the Japan dataset. Stations are shown as black\ntriangles, events as dots. The event color encodes the event magnitude. There are ∼20 additional events far offshore,\nwhich are outside the displayed map region in the catalog.\nrare and therefore underrepresented in the training data (Fig. 6, 8 in [ 11], Fig. 3, 4 in [ 8]). However, early warning\nsystems must also be able to provide reliable warnings for earthquakes larger than any previously seen in a region.\nHere, we present the transformer earthquake alerting model (TEAM), a deep learning method for early warning,\ncombining the advantages of both classical early warning strategies while avoiding the deﬁciencies of prior deep\nlearning approaches. We evaluate TEAM on two data sets from regions with high seismic hazard, namely Japan and\nItaly. Due to their complementary seismicity, this allows to evaluate the capabilities of TEAM across scenarios. We\ncompare TEAM to two state-of-the-art warning methods, of which one is prototypical for source based warning and\none for propagation based warning.\n2 Data and Methods\n2.1 Data\nFor our study we use two nation scale datasets from highly seismically active regions with dense seismic networks,\nnamely Japan (13,512 events, years 1997-2018, Figure 1) and Italy (7,055 events, years 2008-2019, Figure 2). Their\nseismicity is complementary, with predominantly subduction plate interface or Wadati-Benioff zone events for Japan,\nmany of them offshore, and shallow, crustal events for Italy. We split both datasets into training, development and test\nsets with ratios of 60:10:30. We employ an event-wise split, i.e., all records for a particular event will be assigned to the\nsame subset. We do not explicitly split station-wise but due to temporary deployments there are a few stations in the\ntest set which have no records in the training set (Figure 2). We use the training set for model training, the development\nset for model selection, and the test set only for the ﬁnal evaluation. We split the Japan dataset chronologically, yielding\nthe events between August 2013 and December 2018 as test set. For Italy, we test on all events in 2016, as these are of\nparticular interest, encompassing most of the central Italy sequence with the MW=6.2 and Mw=6.5 Norcia events [13].\nEspecially the latter event is notably larger than any in the training set (Mw = 6.1 L’Aquila event in 2007), thereby\nchallenging the extrapolation capabilities of TEAM.\nBoth datasets consist of strong motion waveforms. For Japan each station comprises two sensors, one at the surface and\none borehole sensor, while for Italy only surface recordings are available. As the instrument response in the frequency\nband of interest is ﬂat, we do not restitute the waveforms, but only apply a gain correction. This has the advantage that\nit can trivially be done in real-time. The data and preprocessing are further described in the supplement text S1.\n2\nThe transformer earthquake alerting model\nFigure 2: Map of the station (left) and event (right) distribution in the Italy dataset. Stations present in the training set\nare shown as black triangles, while stations only present in the test set are shown as yellow triangles. Events are shown\nas dots with the color encoding the event magnitude. All events with magnitudes above 5.5 are shown as stars. The red\nstars indicate large training events, while the yellow stars indicate large test events. The inset shows the central Italy\nregion with intense seismicity and high station density in the test set. Moment magnitudes for the largest test events are\ngiven in the inset.\n2.2 The transformer earthquake alerting model\nThe early warning workﬂow with TEAM encompasses three separate steps (Figure 3): event detection, PGA estimation\nand thresholding. We do not further consider the event detection task here, as it forms the base of all methods discussed\nand affects them similarly. The PGA estimation, resulting in PGA probability densities for a given set of target locations,\nis the heart of TEAM and described in detail below. In the last step, thresholding, TEAM issues warnings for each target\nlocations where the predicted exceedance probability pfor ﬁxed PGA thresholds surpasses a predeﬁned probability α.\nTEAM conducts end-to-end PGA estimation: its input are raw waveforms, its output predicted PGA probability\ndensities. There are no intermediate representations in TEAM that warrant an immediate geophysical interpretation.\nThe PGA assessment can be subdivided into three components: feature extraction, feature combination, and density\nestimation (Figure S1). Input to TEAM are three, respectively six (3 surface, 3 borehole), component waveforms at 100\nHz sampling rate from multiple stations and the corresponding station coordinates. Furthermore, the model is provided\nwith a set of output locations, at which the PGA should be predicted. These can be anywhere within the spatial domain\nof the model and need not be identical with station locations in the training set.\nTEAM extracts features from input waveforms using a convolutional neural network (CNN). The feature extraction is\napplied separately to each station, but is identical for all stations. CNNs are well established for feature extraction from\nseismic waveforms, as they are able to recognize complex features independent of their position in the trace. On the\nother hand, CNN based feature extraction usually requires a ﬁxed input length, inhibiting real-time processing. We\nallow real-time processing through the alignment of the waveforms and zero-padding: we align all input waveforms in\ntime i.e., all start at the same time t0 and end at the same time t1. We deﬁne t0 to be 5 s before the ﬁrst P wave arrival at\nany station, allowing the model to understand the noise characteristics. For t1 we use the current time, i.e., the amount\nof available waveforms. We obtain constant length input, by padding all waveforms after t1 with zeros up to a total\nlength of 30 s. The feature extraction is described in more detail in supplementary text S2.\nTEAM combines the feature vectors and maps them to representations at the targets using a transformer [14]. Trans-\nformers are attention-based neural networks for combining information from a ﬂexible number of input vectors in a\nlearnable way. To encode the location of the recording stations as well as of the prediction targets, we use sinusoidal\nvector representations. For input stations, we add these representations component-wise to the feature vectors, for\ntarget stations we directly use them as inputs to the transformer. This architecture, processing a varying number of\ninputs, together with the explicitly encoded locations, allows TEAM to handle dynamically varying sets of stations and\n3\nThe transformer earthquake alerting model\nFigure 3: Schematic view of TEAM’s early warning workﬂow for the October 2016 Norcia event (Mw = 6.5) 2.5 s\nafter the ﬁrst P wave pick (∼3.5 s after origin time). a. An event is detected through triggering at multiple seismic\nstations. The waveform colors correspond to the stations highlighted with orange to magenta outlines. The circles\nindicate the approximate current position of P (dashed) and S (solid) wavefronts. b. TEAM’s input are raw waveforms\nand station coordinates; it estimates probability densities for the PGA at a target set. A more detailed TEAM overview\nis given in Figure S1. c. The exceedance probabilities for a ﬁxed set of PGA thresholds are calculated based on the\nestimated PGA probability densities. If the probability exceeds a threshold α, a warning is issued. The ﬁgure visualizes\na 10%g PGA level with α = 0.4, resulting in warnings for the stations highlighted. The colors correspond to the\nstations with green outlines in a. d. The real-time shake map shows the highest PGA levels for which a warning is\nissued. Stations are colored according to their current warning level. The table lists all stations for which warnings have\nalready been issued.\ntargets. The transformer returns one vector for each target representing predictions at this target. Details on the feature\ncombinations can be found in supplementary text S3.\nFrom each of the vectors returned by the transformer, TEAM calculates the PGA predictions at one target. Similar to\nthe feature extraction, the PGA prediction network is applied separately to each target, but is identical for all targets.\nTEAM uses mixture density networks [15] returning Gaussian mixtures, to computes PGA densities. Gaussian mixtures\nallow TEAM to predict more complex distributions and better capture realistic uncertainties than a point estimate or a\nsingle Gaussian. The full speciﬁcations for the ﬁnal PGA estimation are provided in supplementary text S4.\nTEAM is trained end-to-end using a negative log-likelihood loss. To increase the ﬂexibility of TEAM and allow for\nreal-time processing, we use training data augmentation. We randomly select the stations used as inputs and targets in\neach training iteration. In addition, again in each training iteration, we randomly replace all waveforms after a time t\nwith zeros, matching the input representation of real time data, to train TEAM for real-time application. These data\naugmentations as well as the complete training procedure are further described in supplementary text S5.\nTo mitigate the systematic underestimation of high PGA values observed in previous machine learning models, TEAM\noversamples large events and PGA targets close to the epicenter during training, which reduces the inherent bias in\ndata towards smaller PGAs. When learning from small catalogs or when applied to regions where events substantially\nlarger than all training events can be expected, e.g., because of known locked fault patches or historic records, TEAM\nadditionally can use domain adaptation. To this end the training procedure is modiﬁed to include large events from\nother regions that are similar to the expected events in the target region. While records from those events will differ in\ncertain aspects, e.g., site responses or the exact propagation patterns, other aspects, e.g., the average extent of strong\n4\nThe transformer earthquake alerting model\nshaking or the duration of events of a certain size, will mostly be independent of the region in question. The domain\nadaptation aims to enable the model to transfer the region immanent aspects of large events, at the cost of a certain\nblurring of the speciﬁc regional aspects of the target region. TEAM aims to mitigate the blurring of regional aspects by\nthe choice of training procedure.\nOur Italy dataset is an example of this situation. Accordingly, TEAM applies domain adaptation to this case: It ﬁrst\ntrains a joint model using data from Japan and from Italy, which is then ﬁne-tuned using the Italy data on its own,\nexcept for the addition of a few large, shallow, onshore events from Japan. We chose these events, as for Italy one also\nexpects large, shallow, crustal events due to its tectonic setting and earthquake history. As we use events from Italy in\nboth training steps and in particular in the second step the overwhelming number of events are from Italy, we expect\nthat this scheme only results in a small degradation in the modelling of the regional speciﬁcs of the Italy region.\n2.3 Baselines\nWe compare TEAM to two state-of-the-art early warning methods, one using source estimation and one propagation\nbased. As source estimation based method we use the estimated point source approach (EPS), which estimates\nmagnitudes from peak displacement during the P-wave onset [16] and then applies a GMPE [17] to predict the PGA.\nFor simplicity, our implementation assumes knowledge of the ﬁnal catalog epicentre, which is impossible in real-time,\nleading to overoptimistic results for EPS. As propagation based method we chose an adaptation of PLUM [5], which\nissues warnings if a station within a radius rof the target exceeds the level of shaking. In contrast to the original PLUM,\nwhich operates on the Japanese seismic intensity scale,IJMA [18], our adaptation applies the concept of PLUM to PGA,\nthereby making it comparable to the other approaches. Whereas IJMA is also a measure of the strongest acceleration\nand is thus strongly correlated with PGA, it considers a narrower frequency band and imposes a mimum duration of\nstrong shaking. As such, although the perfomance might vary slightly for our PLUM-like approach compared to the\noriginal PLUM, it still exhibits its key features, in particular the effects of the localized warning strategy. Additionally\nwe apply the GMPE used in EPS to catalog location and magnitude as an approximate upper accuracy bound for point\nsource algorithms (Catalog-GMPE or C-GMPE). C-CMPE is a theoretical bound that can not be realized in real-time.\nIt can be considered as an estimate of the modeling error for point source approaches. A detailed description of the\nbaseline methods can be found in supplementary text S6.\n3 Results\n3.1 Alert performance\nWe compare the alert performance of all methods for PGA thresholds from light (1%g) to very strong (20%g) shaking,\nregarding precision, the fraction of alerts actually exceeding the PGA threshold, and recall, the fraction of issued\nalerts among all cases where the PGA threshold was exceeded [ 19, 20]. Precision and recall trade-off against each\nother depending on α. While the PGA predictions of TEAM, EPS and the C-GMPE are probabilistic, the thresholding\ntransforms the predictions into alerts or non-alerts. The probability distribution describes the uncertainty of the models,\ne.g., for the GMPE the apparent aleatoric uncertainty from aspects not accounted for, which makes false and missed\nalerts inevitable. The threshold value controls the trade-off between both types of errors, and its appropriate value\nwill depend on user needs, speciﬁcally the costs associated with false and missed alerts. Therefore, to analyze the\nperformance of the models across different user requirements, we look at the precision recall curves for different\nthresholds α. In addition to precision and recall, we use two summary metrics: F1 score, the harmonic mean of\nprecision and recall, and AUC, the area under the precision recall curve. The evaluation metrics and full setup of the\nevaluation are deﬁned in detail in supplement text S7.\nTEAM outperforms both EPS and the PLUM-like approach for both datasets and all PGA thresholds, indicated by the\nprecision-recall-curves of TEAM lying to the top-right of the baseline curves (Figure 4a). For any baseline method\nconﬁguration, there is a TEAM conﬁguration surpassing it both in precision and in recall. Improvements are larger for\nJapan, but still substantial for Italy. To compare the performance at ﬁxed α, we selected αvalues yielding the highest\nF1 score separately for each PGA threshold and method. Again, TEAM outperforms both baselines on both datasets,\nirrespective of the PGA level (Figure 4b). Performance statistics in numerical form are available in tables S1 and S2.\nAll methods degrade with increasing PGA levels, particularly for Japan. This degradation is intrinsic to early warning\nfor high thresholds due to the very low prior probability of strong shaking [6, 20, 19]. Furthermore, shortage of training\ndata with high PGA values results in less well constrained model parameters.\nUsing domain adaptation techniques, TEAM copes well with the Italy data, even though the largest test event (Mw = 6.5)\nis signiﬁcantly larger than the largest training event ( Mw = 6.1), and three further test events have MW ≥ 5.8. To\nassess the impact of this technique, we compared TEAM’s results to a model trained without it (Figures S2, S3).\n5\nThe transformer earthquake alerting model\nFigure 4: Warning statistics for the three early-warning models (TEAM, EPS, PLUM) for the Japan and Italy datasets.\nIn addition, statistics are provided for C-GMPE, which can only be evaluated post-event due to its reliance on catalog\nmagnitude and location. a. Precision and recall curves across different thresholds α= 0.05,0.1,0.2,..., 0.8,0.9,0.95.\nAs the PLUM-like approach has no tuning parameter, its performance is shown as a point. Enlarged markers show\nthe conﬁgurations yielding the highest F1 scores. Numbers in the corner give the area under the precision recall curve\n(AUC), a standard measure quantifying the predictive performance across thresholds. b. Precision, recall and F1 score\nat different PGA thresholds using the F1 optimal value α. Threshold probabilities αwere chosen independently for\neach method and PGA threshold. c. Number of events and traces exceeding each PGA threshold for training and test\nset. Training set numbers include development events and show the numbers before oversampling is applied. For Italy\ntraining and test event curve are overlapping due to similar numbers of events.\n6\nThe transformer earthquake alerting model\nFigure 5: Warning time statistics. a. Area under the precision recall curve for different minimum warning times. All\nalerts with shorter warning times are counted as false negatives. b. Warning time histograms showing the distribution\ntrue alerts across distances for the different methods. Please note that the total number of true alerts differs by method\nand is not shown in this subplot. Therefore the values of different methods can not be directly compared, but only the\ndifferences in the distributions. TEAM and EPS are shown at F1-optimal α, chosen separately for each threshold and\nmethod. Warning time dependence on hypocentral distance is shown in Figure S4.\nWhile for low PGA thresholds differences are small, at high PGA levels they grow to more than 20 points F1 score.\nInterestingly, for large events, TEAM strongly outperforms TEAM without domain adaptation even for low PGA\nthresholds. This shows that domain adaptation does not only allow the model to predict higher PGA values, but also to\naccurately assess the region of lighter shaking for large events. Domain adaptation therefore helps TEAM to remain\naccurate even for events quite far from the training distribution.\n3.2 Warning times\nIn application scenarios, a user will usually require a certain warning time, which is the time between issuing of\nthe warning and ﬁrst exceedance of the level of shaking, as this time is necessary for taking action. As the previous\nevaluation considered prediction accuracy irrespective of the warning time, we now compare the methods while\nimposing a certain minimum warning time. Actually, TEAM consistently outperforms both baselines across different\nrequired warning times and irrespective of the PGA threshold (Figure 5a). While the margin for TEAM compared to\nthe baselines is smaller for Italy than for Japan, TEAM shows consistently strong performance across different warning\ntimes. In contrast, EPS performs clearly worse at short warning times, the PLUM-based approach at longer warning\ntimes. The latter is inherent to the key idea of PLUM and makes the method only competitive at high PGA thresholds,\nwhere potential maximum warning times are naturally short due to the proximity between stations with strong shaking\nand the epicenter [21]. We further note that while the PLUM-like approach shows slightly higher AUC than TEAM for\nshort warning times at 20 %g, this is only a hypothetical result. As PLUM does not have a tuning parameter between\nprecision and recall, this performance can actually only be realised for a speciﬁc precision/recall threshold, where it\nperforms slightly superior to TEAM (Figure 4a bottom right).\n7\nThe transformer earthquake alerting model\nWarning times depend on α: a lower αvalue naturally leads to longer warning times but also to more false positive\nwarnings. At F1-optimal thresholds α, EPS and TEAM have similar warning time distributions (Figure 5b, Table S3),\nbut lowering αleads to stronger increases in warning times for TEAM. For instance, at 10%g, lowering αfrom 0.5 to\n0.2 increases average warning times of TEAM by 2.3 s/1.2 s (Japan/Italy), but only by 1.1 s/0.1 s for EPS. Short times\nas measured here are critical in real applications: First, they reduce the time available for counter measures. Second,\nreal warning times will be shorter than reported here due to telemetry and compute delays. However, compute delays\nfor TEAM are very mild: analysing the Norcia event (25 input stations, 246 target sites) for one time step took only\n0.15 s on a standard workstation using non-optimized code.\n4 Discussion\n4.1 Calibration of probabilities\nEven though TEAM and EPS give probabilistic predictions, it is not clear whether these predictions are well-calibrated,\ni.e., if the predicted conﬁdence values actually correspond to observed probabilities. Calibrated probabilities are\nessential for threshold selection, as they are required to balance expected costs of taking action versus expected costs of\nnot taking action. We note that while good calibration is a necessary condition for a good model, it is not sufﬁcient, as a\nmodel constantly predicting the marginal distribution of the labels would be always perfectly calibrated, yet not very\nuseful.\nTo assess the calibration, we use calibration diagrams (Figures S9 and S10) for Japan and Italy at different times after\nthe ﬁrst P arrival. These diagrams compare the predicted probabilities to the actually observed fraction of occurrences.\nIn general, both models are well calibrated, with a slightly better calibration for TEAM. Calibration is generally better\nfor Japan, where only EPS is slightly underconﬁdent at earlier times for the highest PGA thresholds. For Italy, EPS\nis generally slightly overconﬁdent, while TEAM is well calibrated, except for a certain overconﬁdence at 20%g. We\nsuspect that the worse calibration for the largest events is caused by the domain adaptation strategy, but the better\nperformance in terms of accuracy clearly weighs out this downside of domain adaptation.\n4.2 Insights into the transformer earthquake alerting model\nWe analyze differences between the methods using one example event from each dataset (Japan: Figure 6, Italy: Figure\nS5). All methods underestimate the shaking in the ﬁrst seconds (left column Figures 6, S5). However, TEAM is\nthe quickest to detect the correct extent of the shaking. Additionally, it estimates even ﬁne-grained regional shaking\ndetails in real-time (middle and right columns). In contrast, shake maps for EPS remain overly simpliﬁed due to the\nassumptions inherent to GMPEs (right column and bottom left panel). For the Japan example, even late predictions of\nEPS understimate the shaking, due to an underestimation of the magnitude. The PLUM-based approach produces very\ngood PGA estimates, but exhibits the worst warning times.\nNotably, TEAM predictions at later times correspond even better to the measured PGA than C-GMPE estimates,\nalthough these are based on the ﬁnal magnitude (top right and bottom left panels). For the Japan data, this is not only\nthe case for the example at hand, but also visible in Figure 4, showing higher accuracy of TEAM’s prediction compared\nto C-GMPE for all thresholds except 20%g on the full Japan dataset. We assume TEAM’s superior performance is\nrooted in both global and local aspects. Global aspects are the abilities to exploit variations in the waveforms, e.g.,\nfrequency content, to model complex event characteristics, such as stress drop, radiation pattern or directivity, and to\ncompare to events in the training set. Local aspects include understanding regional effects, e.g., frequency dependent\nsite responses, and the ability to consider shaking at proximal stations. We note that for our Italy experiments, the\nmodelling of local aspects resulting from regional characteristics might be slightly degraded by the domain adaptation.\nHowever, the ﬁrst-order propagation effects such as, e.g., amplitude decay due to geometric spreading, are similar\nbetween regions and therefore not negatively affected by the domain adaptation. In conclusion, combining a global\nevent view with propagation aspects, TEAM can be seen as a hybrid model between source estimation and propagation.\n4.3 TEAM performance on the Tohoku sequence\nWe evaluated TEAM for Japan on a chronological train/dev/test split, as this split ensures the evaluation closest to the\nactual application scenario. On the other hand, this split put the M = 9.1 Tohoku event in March 2011 into the training\nset. To evaluate the performance for this very large event and its aftershocks, we trained another TEAM instance using\nthe year 2011 as test set and the remainder of the data for training and validation. Figure 7 shows the precision recall\ncurves for the chronological split and the year 2011 as test set. In general, the performance of all models stays similar\nwhen evaluated on the alternative split. A key difference between the curves is, that TEAM, in particular for high\nPGA thresholds, does not reach similar levels of recall for 2011 as for the chronological split, while achieving higher\n8\nThe transformer earthquake alerting model\nFigure 6: Scenario analysis of the 22nd November 2016 MJ = 7.4 Fukushima earthquake, the largest test event located\nclose to shore. Maps show the warning levels for each method (top three rows) at different times (shown in the corner,\nt = 0 s corresponds to P arrival at closest station). Dots represent stations and are colored according to the PGA\nrecorded during the full event, i.e., the prediction target. The bottom row shows (left to right), the catalog based GMPE\npredictions, the warning time distributions, and the true positives (TP), false negatives (FN) and false positives (FP) for\neach method, both at a 2%g PGA threshold. EPS and GMPE shake map predictions do not include station terms, but\nthey are included for the bottom row histograms.\n9\nThe transformer earthquake alerting model\nFigure 7: Precision recall curves for the Japanese dataset using the chronological split (top) and using the events in\n2011 as test set (bottom). The year 2011 contains the Mw = 9.1 Tohoku event as well as its aftershocks.\nprecision. As we will describe in the next paragraph, this trend probably results from a tendency to underestimate true\nPGA amplitudes, which will naturally reduce recall and boost precision. Nevertheless, the performance of TEAM\nas quantiﬁed by the AUC actually improves, and signiﬁcantly so for the highest thresholds. We suspect that this\ntendency for underestimation is either caused by the higher number of large events in the 2011 test set compared to the\nchronological split, or by the lower number of high PGA events in the training set without 2011.\nFigure S6 presents a scenario analysis for the Tohoku event. All models underestimate the event considerably, with the\nstrongest underestimation for the EPS method. Even 20 s after the ﬁrst P wave arrival, all methods underestimate both\nthe severity and the extent of shaking. Do to its localized approach, the PLUM-based model achieves the highest number\nof true warnings, albeit at short warning times and a certain number of false positives, which due to the underestimation\nare totally absent from TEAM and EPS predictions. The performance of both EPS and TEAM is likely degraded by the\nslow onset of the Tohoku event as described by [22]. According to [22] the main subevent with a displacement of 36 m\nonly initiated 20 s after the onset of the Tohoku event. Therefore only the ﬁrst P waves for EPS or at most the ﬁrst 25 s\nof waveforms for TEAM is most likely insufﬁcient to correctly estimate the size of the Tohoku event.\nFor Italy, we showed that underestimation for large events can be mitigated using transfer learning. However, the\nTohoku event clearly shows the limitations of this strategy, as nearly no training data for events of comparable size\nare available, even when using events across the globe. Therefore, for the largest events alternative strategies need to\nbe developed, e.g., training using simulated data. Furthermore, the 25 s of waveforms used by TEAM in the current\nimplementation may, for a very large event, not capture the largest subevent. While we decided to use only 25 s of event\nwaveforms, as there is only insufﬁcient training data of longer events, this window could be extended when developing\ntraining strategies and models for the largest events.\n5 Conclusion\nIn this study we presented the transformer earthquake alerting model (TEAM). TEAM outperforms existing early\nwarning methods in terms of both alert performance and warning time. Using a ﬂexible machine learning model, TEAM\nis able to extract information about an event from raw waveforms and leverage the information to model the complex\ndependencies of ground motion. We point out two further aspects that make TEAM appealing to users. First, TEAM\ncan adapt to various user requirements by combining two thresholds, one for shake level and one for the exceedance\nprobability. As TEAM outputs probability density functions over the PGA, these thresholds can easily be adjusted by\nindividual users on the ﬂy, e.g., by setting sliders in an early warning system. Second, deep learning models typically\nexhibit large performance improvements from larger training datasets [23] due to the high number of model parameters.\nIn our study this reﬂects in the better performance on the twofold larger Japan dataset. This indicates that TEAM’s\nperformance can be improved just by collecting more comprehensive catalogs, which happens automatically over time.\n10\nThe transformer earthquake alerting model\nAcknowledgements\nWe thank the National Research Institute for Earth Science and Disaster Resilience (NIED) for providing the catalog and\nwaveform data for our Japan dataset. We thank the Istituto Nazionale di Geoﬁsica e Vulcanologia and the Dipartimento\ndella Protezione Civile for providing the catalog and waveform data for our Italy dataset. J. M. acknowledges the\nsupport of the Helmholtz Einstein International Berlin Research School in Data Science (HEIBRiDS). We thank Matteo\nPicozzi for discussions on earthquake early warning that helped improve the study design. We thank Hiroyuki Goto and\nan anonymous reviewer for their comments which helped to improve the manuscript. An implementation of TEAM and\nthe baselines is available at https://github.com/yetinam/TEAM. The Italy dataset has been published as [24]. The\nJapan dataset can be obtained using the scripts in the code repository.\nReferences\n[1] R. M. Allen, P. Gasparini, O. Kamigaichi, and M. Bose. The Status of Earthquake Early Warning around the\nWorld: An Introductory Overview. Seismological Research Letters, 80(5):682–693, September 2009.\n[2] Richard M. Allen and Diego Melgar. Earthquake Early Warning: Advances, Scientiﬁc Challenges, and Societal\nNeeds. Annual Review of Earth and Planetary Sciences, 47(1):361–388, 2019.\n[3] Angela I. Chung, Ivan Henson, and Richard M. Allen. Optimizing Earthquake Early Warning Performance:\nElarmS-3. Seismological Research Letters, 90(2A):727–743, March 2019.\n[4] M. Böse, D. E. Smith, C. Felizardo, M.-A. Meier, T. H. Heaton, and J. F. Clinton. FinDer v.2: Improved\nreal-time ground-motion predictions for M2–M9 with seismic ﬁnite-source characterization. Geophysical Journal\nInternational, 212(1):725–742, January 2018.\n[5] Yuki Kodera, Yasuyuki Yamada, Kazuyuki Hirano, Koji Tamaribuchi, Shimpei Adachi, Naoki Hayashimoto,\nMasahiko Morimoto, Masaki Nakamura, and Mitsuyuki Hoshiba. The Propagation of Local Undamped Motion\n(PLUM) Method: A Simple and Robust Seismic Waveﬁeld Estimation Approach for Earthquake Early WarningThe\nPropagation of Local Undamped Motion (PLUM) Method. Bulletin of the Seismological Society of America ,\n108(2):983–1003, April 2018.\n[6] Men-Andrin Meier, Yuki Kodera, Maren Böse, Angela Chung, Mitsuyuki Hoshiba, Elizabeth Cochran, Sarah\nMinson, Egill Hauksson, and Thomas Heaton. How Often Can Earthquake Early Warning Systems Alert Sites\nWith High-Intensity Ground Motion? Journal of Geophysical Research: Solid Earth, 125(2):e2019JB017718,\n2020.\n[7] Anthony Lomax, Alberto Michelini, and Dario Jozinovi´c. An Investigation of Rapid Earthquake Characteriza-\ntion Using Single-Station Waveforms and a Convolutional Neural Network. Seismological Research Letters,\n90(2A):517–529, 2019.\n[8] S. Mostafa Mousavi and Gregory C. Beroza. A Machine-Learning Approach for Earthquake Magnitude Estimation.\nGeophysical Research Letters, 47(1):e2019GL085976, 2020.\n[9] Marius Kriegerowski, Gesa M. Petersen, Hannes Vasyura-Bathke, and Matthias Ohrnberger. A Deep Convolutional\nNeural Network for Localization of Clustered Earthquakes Based on Multistation Full Waveforms. Seismological\nResearch Letters, 90(2A):510–516, 2019.\n[10] S. Mostafa Mousavi and Gregory C. Beroza. Bayesian-Deep-Learning Estimation of Earthquake Location from\nSingle-Station Observations. arXiv:1912.01144 [physics], December 2019.\n[11] Dario Jozinovi´c, Anthony Lomax, Ivan Štajduhar, and Alberto Michelini. Rapid prediction of earthquake ground\nshaking intensity using raw waveform data and a convolutional neural network.Geophysical Journal International,\n222(2):1379–1389, 2020.\n[12] Ryota Otake, Jun Kurima, Hiroyuki Goto, and Sumio Sawada. Deep Learning Model for Spatial Interpolation of\nReal-Time Seismic Intensity. Seismological Research Letters, 91(6):3433–3443, November 2020.\n[13] Mauro Dolce and Daniela Di Bucci. The 2016–2017 Central Apennines Seismic Sequence: Analogies and\nDifferences with Recent Italian Earthquakes. In Kyriazis Pitilakis, editor, Recent Advances in Earthquake\nEngineering in Europe: 16th European Conference on Earthquake Engineering-Thessaloniki 2018, Geotechnical,\nGeological and Earthquake Engineering, pages 603–638. Springer International Publishing, Cham, 2018.\n[14] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser,\nand Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008, 2017.\n[15] Christopher M Bishop. Mixture density networks. Technical report, Aston University, 1994.\n11\nThe transformer earthquake alerting model\n[16] Huseyin Serdar Kuyuk and Richard M. Allen. A global approach to provide magnitude estimates for earthquake\nearly warning alerts. Geophysical Research Letters, 40(24):6329–6333, 2013.\n[17] Georgia Cua and Thomas H. Heaton. Characterizing Average Properties of Southern California Ground Motion\nAmplitudes and Envelopes. EERL Report, Earthquake Engineering Research Laboratory, Pasadena, CA, 2009.\n[18] Khosrow T Shabestari and Fumio Yamazaki. A proposal of instrumental seismic intensity scale compatible with\nmmi evaluated from three-component acceleration records. Earthquake Spectra, 17(4):711–723, 2001.\n[19] Men-Andrin Meier. How “good” are real-time ground motion predictions from Earthquake Early Warning\nsystems? Journal of Geophysical Research: Solid Earth, 122(7):5561–5577, 2017.\n[20] Sarah E. Minson, Annemarie S. Baltay, Elizabeth S. Cochran, Thomas C. Hanks, Morgan T. Page, Sara K.\nMcBride, Kevin R. Milner, and Men-Andrin Meier. The Limits of Earthquake Early Warning Accuracy and Best\nAlerting Strategy. Scientiﬁc Reports, 9(1):2478, February 2019.\n[21] Sarah E. Minson, Men-Andrin Meier, Annemarie S. Baltay, Thomas C. Hanks, and Elizabeth S. Cochran. The\nlimits of earthquake early warning: Timeliness of ground motion estimates. Science Advances, 4(3):eaaq0504,\nMarch 2018.\n[22] Kazuki Koketsu, Yusuke Yokota, Naoki Nishimura, Yuji Yagi, Shin’ichi Miyazaki, Kenji Satake, Yushiro Fujii,\nHiroe Miyake, Shin’ichi Sakai, Yoshiko Yamanaka, et al. A uniﬁed source model for the 2011 tohoku earthquake.\nEarth and Planetary Science Letters, 310(3-4):480–487, 2011.\n[23] Chen Sun, Abhinav Shrivastava, Saurabh Singh, and Abhinav Gupta. Revisiting unreasonable effectiveness of data\nin deep learning era. In Proceedings of the IEEE international conference on computer vision, pages 843–852,\n2017.\n[24] Jannes Münchmeyer, Dino Bindi, Ulf Leser, and Frederik Tilmann. Fast earthquake assessment and earthquake\nearly warning dataset for italy, 2020.\n[25] Jasper Snoek, Yaniv Ovadia, Emily Fertig, Balaji Lakshminarayanan, Sebastian Nowozin, D Sculley, Joshua\nDillon, Jie Ren, and Zachary Nado. Can you trust your model’s uncertainty? evaluating predictive uncertainty\nunder dataset shift. In Advances in Neural Information Processing Systems, pages 13969–13980, 2019.\n[26] Kazi R. Karim and Fumio Yamazaki. Correlation of JMA instrumental seismic intensity with strong motion\nparameters. Earthquake Engineering & Structural Dynamics, 31(5):1191–1212, 2002.\n[27] Elizabeth S. Cochran, Julian Bunn, Sarah E. Minson, Annemarie S. Baltay, Deborah L. Kilb, Yuki Kodera, and\nMitsuyuki Hoshiba. Event Detection Performance of the PLUM Earthquake Early Warning Algorithm in Southern\nCalifornia. Bulletin of the Seismological Society of America, 109(4):1524–1541, August 2019.\n[28] National Research Institute For Earth Science And Disaster Resilience. Nied k-net, kik-net, 2019.\n[29] Istituto Nazionale di Geoﬁsica e Vulcanologia (INGV), Istituto di Geologia Ambientale e Geoingegneria (CNR-\nIGAG), Istituto per la Dinamica dei Processi Ambientali (CNR-IDPA), Istituto di Metodologie per l’Analisi\nAmbientale (CNR-IMAA), and Agenzia Nazionale per le nuove tecnologie, l’energia e lo sviluppo economico\nsostenibile (ENEA). Centro di microzonazione sismica network, 2016 central italy seismic sequence (centromz),\n2018.\n[30] Universita della Basilicata. Unibas, 2005.\n[31] RESIF - Réseau Sismologique et géodésique Français. Resif-rlbp french broad-band network, resif-rap strong\nmotion network and other seismic stations in metropolitan france, 1995.\n[32] University of Genova. Regional seismic network of north western italy. international federation of digital\nseismograph networks, 1967.\n[33] Presidency of Counsil of Ministers - Civil Protection Department. Italian strong motion network (ran), 1972.\n[34] Istituto Nazionale di Geoﬁsica e Vulcanologia (INGV), Italy. Rete sismica nazionale (rsn), 2006.\n[35] Dipartimento di Fisica, Università degli studi di Napoli Federico II. Irpinia seismic network (isnet), 2005.\n[36] MedNet Project Partner Institutions. Mediterranean very broadband seismographic network (mednet), 1990.\n[37] OGS (Istituto Nazionale di Oceanograﬁa e di Geoﬁsica Sperimentale) and University of Trieste. North-east italy\nbroadband network (ni), 2002.\n[38] OGS (Istituto Nazionale di Oceanograﬁa e di Geoﬁsica Sperimentale). North-east italy seismic network (nei),\n2016.\n[39] RESIF - Réseau Sismologique et géodésique Français. Réseau accélérométrique permanent (french ac-\ncelerometrique network) (rap), 1995.\n12\nThe transformer earthquake alerting model\n[40] Geological Survey-Provincia Autonoma di Trento. Trentino seismic network, 1981.\n[41] Istituto Nazionale di Geoﬁsica e Vulcanologia (INGV). Ingv experiments network, 2008.\n[42] EMERSITO Working Group. Seismic network for site effect studies in amatrice area (central italy) (sesaa), 2018.\n[43] David J. Wald, Vincent Quitoriano, Thomas H. Heaton, and Hiroo Kanamori. Relationships between Peak\nGround Acceleration, Peak Ground Velocity, and Modiﬁed Mercalli Intensity in California. Earthquake Spectra,\n15(3):557–564, August 1999.\n[44] Mikhail Belkin, Daniel Hsu, Siyuan Ma, and Soumik Mandal. Reconciling modern machine-learning practice and\nthe classical bias–variance trade-off. Proceedings of the National Academy of Sciences, 116(32):15849–15854,\n2019.\n[45] Vidya Muthukumar, Kailas V odrahalli, Vignesh Subramanian, and Anant Sahai. Harmless interpolation of noisy\ndata in regression. IEEE Journal on Selected Areas in Information Theory, 2020.\n13\nThe transformer earthquake alerting model\nA Data and Preprocessing\nFor our study we use two datasets, one from Japan, one from Italy. The Japan dataset consists of 13,512 events between\n1997 and 2018 from the NIED KiK-net catalog [ 28]. The data was obtained from NIED and consists of triggered\nstrong motion records. Each trace contains 15 s of data before the trigger and has a total length of 120 s. Each station\nconsists of two three component strong motion sensors, one at the surface and one borehole sensor. We split the dataset\nchronologically with ratios of 60:10:30 between training, development and test set. The training set ends in March 2012,\nthe test set begins in August 2013. Events in between are used as development set. We decided to use a chronological\nsplit to ensure a scenario most similar to the actual application in an early warning setting.\nThe Italy dataset consists of 7,055 events between 2008 and 2019 from the INGV catalog. We use data from the 3A\n[29], BA [30], FR [31], GU [32], IT [33], IV [34], IX [35], MN [36], NI [37], OX [38], RA [39], ST [40], TV [41]\nand XO [42] networks. We use all events from 2016 as test set and the remaining events as training and development\nsets. The test set consists of 31% of the events, a similar fraction as in the Japan dataset. We shufﬂe events between\ntraining and development set. While a chronological split would have been the default choice, we decided to use 2016\nfor testing, as it contains a long seismic sequence in central Italy containing several very large events in August and\nOctober. Further details on the statistics of both datasets can be found in Table S4.\nBefore training we extract, align and preprocess the waveforms and store them in hdf5 format. As alignment requires\nthe ﬁrst P pick, we need approximate picks for the datasets. For Japan we use the trigger times provided by NIED.\nOur preprocessing accounts for misassociated triggers. For Italy we use an STA/LTA trigger around the predicted P\narrival. While triggering needs to be handled differently in an application scenario, we use this simpliﬁed approach as\nour evaluation metrics depend only very weakly on the precision of the picks.\nB TEAM - Feature extraction network\nThe feature extraction of TEAM is conducted separately for each station. Nonetheless the same convolutional neural\nnetwork (CNN) for feature extraction is applied at all stations, i.e., the same model with the same model weights.\nAs amplitudes of seismic waveforms can span several orders of magnitude, the ﬁrst layer of the network normalizes\nthe traces by dividing through their peak value observed so far. All components of one station are normalized jointly,\nsuch that the amplitude ratio between the components stays unaltered. Notably, we only use the peak value observed\nso far, i.e., the waveforms after t1, which have been blinded with zeros, are not considered, as this would introduce a\nknowledge leak. As the peak amplitude of the trace is likely a key predictor, we logarithmize the value and concatenate\nit to the feature vector after passing through all the convolutional layers, prior to the fully connected layers.\nWe apply a set of convolutional and max-pooling layers to the waveforms. We use convolutional layers as this allows\nthe model to extract translation invariant features and as convolutional kernels can be interpreted as modeling frequency\nfeatures. We concatenate the output of the convolutions and the logarithm of the peak amplitude. This vector is fed into\na multi-layer perceptron to generate the ﬁnal features vector for the station. All layers use ReLu activations. A detailed\noverview of the number and speciﬁcations of the layers in the feature extraction model can be found in Table S5.\nC TEAM - Feature combination network\nThe feature extraction provides one feature vector per input station representing the waveforms. As additional input the\nmodel is provided with the location of the stations, represented by latitude, longitude and elevation. The targets for the\nPGA estimation are speciﬁed by the latitude, longitude and elevation as well.\nWe use a transformer network [14] for the feature combination. Given a set of ninput vectors, a transformer produces\nnoutput vectors capturing combined information from all the vectors in a learnable way. We use transformers for two\nmain reasons. First, they are permutation equivariant, i.e., changing the order of input or output stations does not have\nany impact on the output. This is essential, as there exists no natural ordering on the input stations. Second, they can\nhandle variable input sizes, as the number of parameters of transformers is independent of the number of input vectors.\nThis property allows application of the model to different sets of stations and a ﬂexible number of target locations.\nTo incorporate the locations of the stations we use predeﬁned position embeddings. As proposed by [ 14], we use\npairs of sinusoidal functions, sin(2π\nλi\nx) and cos(2π\nλi\nx), with different wavelength λi. We use 200 dimensions for\nlatitude and longitude, respectively, and the remaining 100 dimensions for elevation. We anticipate two advantages\nof sinusoidal embeddings for representing the station position. First, keeping the position embeddings ﬁxed instead\nof learnable reduces the parameters and therefore likely provides better representations for stations with only few\ninput measurements or sites not contained in the training set. Second, sinusoidal embeddings guarantee that shifts\n14\nThe transformer earthquake alerting model\ncan be represented by linear transformations, independent of the location it applies to. As the attention mechanism in\ntransformers is built on linear projections and dot products, this should allow for more efﬁcient attention scores at least\nin the ﬁrst transformer layers. As proposed in the original transformer paper [14], the position embeddings are added\nelement-wise to the feature vectors to form the input of the transformer. We calculate position embeddings of the target\nlocations in the same way.\nAs in our model input and output size of the transformer are identical, we only use the transformer encoder stack\n[14] with six encoder layers. Inputs are the feature vectors with position embeddings from all input stations and the\nposition embeddings of the output locations. By applying masking to the attention we ensure that no attention weight is\nput on the vectors corresponding to the output locations. This guarantees that each target only affects its own PGA\nvalue and not any other PGA values. As the self-attention mechanism of the transformer has quadratic computational\ncomplexity in the number of inputs, we restrict the maximum number of input stations to 25 (see training details for the\nselection procedure). Further details on the hyperparameters can be found in Table S6. The transformer returns one\noutput vector for each input vector. We discard the vectors corresponding to the input stations and only keep the vectors\ncorresponding to the targets.\nD TEAM - Mixture density output\nSimilar to the feature extraction, the output calculation is conducted separately for each target, while sharing the same\nmodel and weights between all targets. We use a mixture density network to predict probability densities for the PGA\n[15]. We model the probability as a mixture of m = 5Gaussian random variables. Using a mixture of Gaussians\ninstead of a single Gaussian allows the model to predict more complex distribution, like non-Gaussian distributions,\ne.g., asymmetric distributions. The functional form of the Gaussian mixture is ∑m\ni=1 αiϕµi,σi (x). We write ϕµi,σi for\nthe density of a standard normal with mean µi and standard deviation σi. The values αi are non-negative weights for\nthe different Gaussians with the property ∑m\ni=1 αi = 1. The mixture density network uses a multi-layer percepton to\npredict the parameters αi, µi and σi. The hidden dimensions are 150, 100, 50, 30, 10. The activation function is ReLu\nfor the hidden layers, linear for the µand σoutputs and softmax for the αoutput.\nE TEAM - Training details\nWe train the model end-to-end using negative log-likelihood as loss function. All components are trained jointly\nend-to-end. The model has about 13.3 million parameters in total. To increase the amount of training data and to\ntrain the model on shorter segments of data we apply various forms of data augmentation. Each data augmentation is\ncalculated separately each time a particular waveform sample is shown, such that the effective training samples vary.\nFirst, if our dataset contains more stations for an event than the maximum number of 25 allowed by the model, we\nsubsample. We introduce a bias to the subsampling to favor stations closer to the event. We use up to twenty targets for\nPGA prediction. Similarly to the input station, we subsample if more targets are available and bias the subsampling to\nstations close to the event. This bias ensures that targets with higher PGA values are shown more often during training.\nSecond, we apply station blinding, meaning we zero out a set of stations in terms of both waveforms and coordinates.\nThe number of stations to blind is uniformly distributed between zero and the total number of stations available minus\none. In combination with the ﬁrst point this guarantees that the model also learns to predict PGA values at sites where\nno waveform inputs are available.\nThird, we apply temporal blinding. We uniformly select a time tthat is between 1 sbefore the ﬁrst P pick and 25 s\nafter. All waveforms are set to zero after time t. The model therefore only uses data available at time t. Even though we\nnever apply TEAM to times before the ﬁrst P pick, we include these in the training process to ensure TEAM learns a\nsensible prior distribution. We observed that this leads to better early predictions. As information about the triggering\nstation distribution would introduce a knowledge leak if available from the beginning, we zero out all waveforms and\ncoordinates from stations that did not trigger until time t.\nFourth, we oversample large magnitude events. As large magnitude events are rare, we artiﬁcially increase their number\nin the training set. An event with magnitude M ≥ M0 is used λM−M0 times in each training epoch with λ= 1.5 and\nM0 = 5for Japan and M0 = 4for Italy. This event-based oversampling implicitly increases the number of high PGA\nvalues in the training set, too.\nWe apply all data augmentation on the training and the development set, to ensure that the development set properly\nrepresents the task we are interested in. As this introduces stochasticity into the development set metrics, we evaluate\nthe development set three times after each epoch and average the result. In contrast, at test time we do not apply any\n15\nThe transformer earthquake alerting model\ndata augmentation, except temporal blinding for modelling real-time application. If more than 25 stations are available\nfor a test set event, we select the 25 station with the earliest arrivals for evaluation.\nWe train our model using the Adam optimizer. We emphasize that the model is only trained on predicting the PGA\nprobability density and does not use any information on the PGA thresholds used for evaluation. We start with a\nlearning rate of 10−4 and decrease the learning rate by a factor of 3 after 5 epochs without a decrease in validation loss.\nFor the ﬁnal evaluation we use the model from the epoch with lowest loss on the development set. We apply gradient\nclipping with a value of 1.0. We use a batch size of 64. We train the model for 100 epochs.\nTo improve the calibration of the predicted probability densities we use ensembles [ 25]. We use an ensemble size\nof 10 models and average the predicted probability densities. We weight each ensemble member identically. To\nincrease the entropy between the ensembles, we also modify the position encodings between the ensemble members\nby rotating the latitude and longitude values of stations and targets. The rotations for the 10 ensemble members are\n0◦,5◦,..., 40◦,45◦.\nFor the Italy model we use domain adaptation by modifying the training procedure. We ﬁrst train a model jointly on\nthe Italy and Japan data set, according to the conﬁguration described above. We use the resulting model weights as\ninitialization for the Italy model. For this training we reduce the number of PGA targets to 4, leading to a higher fraction\nof high PGA values in the training data, and the learning rate to 10−5. In addition, we train jointly on an auxilary data\nset, comprised of 77 events from Japan. The events were chosen to be shallow, crustal and onshore, having a magnitude\nbetween 5.0 and 7.2. We shift the coordinates of the stations to lie in Italy. We use 85% of the auxiliary events in the\ntraining set and 15% in the development set.\nWe implemented the model using Tensorﬂow. We trained each model on one GeForce RTX 2080 Ti or Tesla V100.\nTraining of a single model takes approximately 5 h for the Japan dataset, 10 h for the joint model and 1 h for the Italy\ndata set. We benchmarked the inference performance of TEAM on a common workstation with GPU acceleration\n(Intel i7-7700, Nvidia Quadro P2000). Running TEAM with ensembling at a single timestep took 0.15 s for all\n246 PGA targets of the Norcia event. As our implementation is not optimized for run time, we expect an optimized\nimplementation to yield multifold lower run times, enabling a real-time application of TEAM with high update rate and\nlow compute latency.\nFigure S7 shows the training and validation loss curves for the Japan TEAM model and the ﬁne-tuning step of the Italy\nTEAM model. While there is some variation between the ensemble members, all show similar characteristics. We note,\nthat the early appearance of the optima for the Italy ﬁne-tuning is expected, because of the transfer learning applied. We\nvalidated through a comparison of the ﬁne-tuned and the non-ﬁne-tuned models, that the ﬁne-tuning step still leads to\nconsiderable improvement in the model performance.\nAs visible from the by far lower training than validation loss, all models exhibit overﬁtting. This is expected, as\nthe number of model parameters (13.3M) is very high in comparison to the number of training examples (<10,000).\nHowever, multiple publications (e.g. [44, 45]) have provided theoretical and empirical evidence, that overﬁtting for\ndeep learning is not problematic and can even lead to considerably better performance than a not overﬁtted model, if\nproper model selection on a validation set is employed. We conduct this model selection by using the model with the\nlowest validation score.\nF Baseline methods\nWe compare TEAM to two baseline methods, EPS and a PLUM-based approach. We do not compare to any deep\nlearning baseline, because we are not aware of any published deep learning method for early warning that can actually\nbe applied in real-time. For the EPS method we use a GMPE based on the functional form by [17] and add a quadratic\nmagnitude term as proposed by [19]. We make further minor adjustments to accommodate the wider range of magnitudes\nin our datasets. The functional form of the GMPE is:\nlog(pga) =a1M + a2 max(M − M0,0)2 + b(Rd + C(M)) +dlog(Rd + C(M)) +e+ δS + N (0,σ2) (1)\nC(M) :=c1 exp(c2 max(0,M − 5))(arctan(M − 5) +π/2) (2)\nRd :=\n√\nR2 + H2\nd (3)\nWe write M for magnitude, Rfor epicentral distance, δS for the station bias, and efor an error term. We use m/s2\nas unit for PGA and km as unit for all length measurements. We use a pseudo-depth Hd, depending on the event\ndepth and the dataset. This allows to model the stronger attenuation with distance for shallow events. For Italy we set\nHd = 5km for events shallower than 20 km and Hd = 50km for all other events. For Japan we set Hd = 5km for\nevents shallower than 20 km, Hd = 40km for events between 20 km and 200 km and set Hd to the actual depth for all\ndeeper events, to account for a few very deep events. We setM0 = 4for Italy and M0 = 6for Japan.\n16\nThe transformer earthquake alerting model\nWe ﬁx c1 = 1.48 and c2 = 1.11, as proposed by [ 17], and optimize the other parameters using linear regression.\nWe perform the optimization iteratively to obtain station bias terms, using the union of training and development\nset. To avoid noise samples in calibration we only use stations for which Rd <(M − 3.5) ∗ 200 km for Japan and\nRd <(M − 3) ∗ 50 km for Italy. The calibrated GMPEs have residual values σof 0.29 for Italy and 0.33 for Japan,\nmatching the value of ∼0.3 proposed as the approximate current optimum for GMPEs [20]. Residual plots can be found\nin Figure S8.\nWe note that our GMPE model is using a point source assumption, which is incorrect for larger events. We chose this\nsimpliﬁcation, as it is common in source based early warning and makes the GMPE performance an upper bound for\nany method relying on magnitude and location estimate. While there are early warning methods based on extended fault\nmodels [4], they perform equally well as point source approaches for all but the largest events [6]. As lower thresholds\nare dominated by smaller events, for which the point source approximation is valid, the inferior performance of the\nGMPE compared to TEAM is not an artifact of the point source assumption, but probably related to its inability to\naccount for systematic propagation effects caused by regional structure, and variability of the earthquake source (focal\nmechanism, stress drop) not captured by the magnitude and location.\nFor magnitude estimation we use the peak displacement based method proposed by [16]. We bandpass ﬁlter the signal\nbetween 0.5 Hz and 3 Hz and discard traces with insufﬁcient signal to noise ratio. We extract peak displacement from\nthe horizontal components in the ﬁrst 6 s of the P wave. We stop the time window at latest at the S onset. We use the\nrelationship\nM = c1 log(PD) +c2 log(R) +c3 + N (0,σ2) (4)\nto estimate magnitudes from peak displacement. We use c1 = 1.23, c2 = 1.38, c3 = 5.69 (Italy) / c3 = 5.89 (Japan)\nand σ = 0.31. These are the values from [ 16], except for c3 which needed to be adjusted as we do not use moment\nmagnitude. We combine the predictions in probability space assuming independence between the predictions from\ndifferent stations. We weight stations based on the length of the P wave window recorded so far. We use the mean value\nof the single-station magnitude estimates for PGA estimation. For both the application of the GMPE and the magnitude\nestimation we use the catalog hypocenters. As the quality of real-time location estimates will be worse, this leads to\ninﬂated performance measures for EPS.\nAs second baseline, we adapted the PLUM algorithm [5]. While the original paper applies PLUM to seismic intensities,\nwe apply it to PGA values. This adaptation is possible, as approximate linear and especially monotonic relations\nexist between intensity and PGA [26]. However, as seismic intensiy incorporates a narrower frequency band and also\nconsiders the duration of strong shaking [18], the PLUM adaptation to PGA might exhibit slightly different performance.\nThe PGA prediction ˆpgas\nt at a station sat time tis the maximum of all observed PGA values pgas′\nt at stations s′within\na radius rof s. Therefore a warning for a certain threshold for a station is issued once the threshold has been exceeded\nat any station within the radius r. Due to different station densities in Italy and Japan we used different values for r. For\nItaly we used r= 15km for Japan we used r= 30km. Following the ﬁndings of Cochran et al [27], we do not use site\ncorrection terms in our implementation of PLUM as they only have minor impact on the performance.\nG Evaluation metrics\nWe analyze the performance of the early warning algorithms using PGA thresholds of 1%g, 2%g, 5%g, 10%g and\n20%g, approximately matching Modiﬁed Mercalli Intensity (MMI) III (light) to VII (very strong) [43]. We calculate\nPGA from the absolute value of the two horizontal components. For the PGA values for the Japanese data, we use the\nsurface stations and not the borehole stations.\nA warning at a site should be issued if anytime during the event the PGA threshold is exceeded at the site. We consider\na warning correct (true positive, TP), if a warning for a certain threshold was issued and the threshold was actually\nexceeded later during the event. Missed warnings (false negative, FN) are all cases, where the PGA threshold was\nexceeded, but no warning was issued or the warning was issued after the PGA threshold was ﬁrst exceeded. We consider\na warning false (false positive, FP), if a warning was issued, but the threshold was not exceeded. All remaining cases\nare true negatives (TN).\nAs the number of true negatives depends strongly on the inclusion criteria of the catalog, we use metrics independent of\nthe true negatives. As summary statistics we use precision, TP/(TP+FP), measuring the fraction of correct warnings\namong all warnings, and recall, TP/(TP+FN), measuring the fraction of possible correct warnings that was issued. We\nuse the F1 score = 2∗precision∗recall/(precision+recall) as a combined statistic. Any analysis using a ﬁxedαuses the\nvalue maximizing the F1 score, which is speciﬁc to each method and PGA threshold. For an analysis independent of the\nthreshold αwe use the area under the precision recall curve (AUC). We use valuesα= 0.05,0.1,0.2,..., 0.8,0.9,0.95\nand add additional points at (0,1) and (1,0) to the precision recall curve to approximate the AUC. For comparison of\n17\nThe transformer earthquake alerting model\nthe PLUM-based model using AUC in Figure 3, we introduce an artiﬁcial precision recall line for PLUM with a slope\nof −1 going through the observed precision and recall values.\nWe deﬁne the warning time as the time between the issuance of a warning and the ﬁrst exceedance of the threshold.\nWe consider a zero latency system and do not impose a minimum warning time. For comparing warning times\nbetween methods or different parameter combinations, we only use the subset of station event pairs, where both\nmethods/parameter combinations issued correct warnings.\nWe evaluate our PLUM-based implementation continuously, i.e., warnings are issued immediately at the exceedance of\na threshold. TEAM and EPS are evaluated every 0.1 s, starting 1 s after the ﬁrst P arrival for EPS and 0.5 s after the ﬁrst\nP arrival for TEAM. We use a longer time before the ﬁrst prediction for EPS as the early results of EPS are unstable.\nWarnings are not retracted, i.e., even if the model later estimates a shake level below the warning threshold, the warning\nstay active.\n18\nThe transformer earthquake alerting model\nFigure S1: Overview of the transformer earthquake alerting model, showing the input, the feature extraction, the feature\ncombination, the PGA estimation and the output. For simplicity, not all layers are shown, but only their order and\ncombination is visualized schematically. For the exact number of layers and the size of each layer please refer to\ntables S5 and S6. Please note that the number of input stations and the number of targets are both variable, due to the\nself-attention mechanism in the feature combination. Ten instances of this network are trained independently and the\nresults ensemble-averaged.\n19\nThe transformer earthquake alerting model\nFigure S2: True positives (TP), false negatives (FN) and false positives (FP) for the events in the Italy test sets causing\nthe largest shaking. The methods are the transformer earthquake alerting model without domain adaptation (TEAM\nbase), the transformer earthquake alerting model (TEAM), the estimated point source algorithm (EPS) and PLUM-based\napproach. In addition, a GMPE with full catalog information is included for reference. Values αwere chosen separately\nfor each threshold and method to yield the highest F1 score for the whole test set, but are kept constant across all events.\nTEAM with domain adaptation outperforms TEAM without domain adaptation consistently across all thresholds. This\nindicates that the domain adaptation not only allows TEAM to better predict higher levels of shaking, but also to better\nassess large events in general.\n20\nThe transformer earthquake alerting model\nFigure S3: Precision, recall and F1 score at different PGA thresholds for Italy including TEAM without domain\nadaptation. Threshold values αwere chosen independently for each method and PGA threshold to yield the highest\nF1 score. The methods are the transformer earthquake alerting model without domain adaptation (TEAM Base), the\ntransformer earthquake alerting model (TEAM), the estimated point source (EPS) model and the PLUM-based model.\nIn addition the graph shows the performance of C-GMPE, a GMPE with full catalog information for reference.\nFigure S4: Warning time and hypocentral distance between station and event for each true alert at F1-optimal α. The\nwhite area corresponds roughly to the range of possible warning times and is bounded by the 90 th percentile of the\ntimes between ﬁrst detection of an event (i.e., arrival of P wave at the closest station) and ﬁrst exceedance of the PGA\nthreshold in recordings at that approximate distance.\n21\nThe transformer earthquake alerting model\nFigure S5: Scenario analysis of the 30th October 2016 Mw = 6.5 Norcia earthquake, the largest event in the Italy test\nset. See Fig. 4 in the main paper for further explanations. The bottom row diagrams for this scenario analysis use a\n10%g PGA threshold.\n22\nThe transformer earthquake alerting model\nFigure S6: Scenario analysis of the 11th March 2011 Mw = 9.1 Tohoku earthquake, the largest event in the Japan\ndataset. See Fig. 4 in the main paper for further explanations. The bottom row diagrams for this scenario analysis use a\n2%g PGA threshold.\n23\nThe transformer earthquake alerting model\nFigure S7: Training and validation loss curves for the Japan TEAM model and the ﬁne-tuning step of the Italy TEAM\nmodel. Each line shows the loss curve for one ensemble member with colors matching between training and validation\ncurves. The models used are determined by the minimum validation loss and are denoted by black crosses. The models\nwere evaluated after the training epoch indicated on the x-axis, i.e., the leftmost point of each curve already includes\none epoch of training.\n24\nThe transformer earthquake alerting model\nFigure S8: Predictions and residuals of the GMPEs derived in this study. All PGA values are given as log units using\nm/s2. Every point refers to one recording. Solid lines indicate running means, dashed lines denote the running standard\ndeviation around the running mean. Orange crosses denote mean and standard deviations for magnitude ranges with\ninsufﬁcient data to infer a continuous line. Window sizes are 0.24 m.u./10 km (Italy) and 0.44 m.u./53 km (Japan).\nOverall σis 0.29 for Italy and 0.33 for Japan. The plotted magnitude values have been offset by random values between\n-0.05 and 0.05 m.u. for increased visibility.\n25\nThe transformer earthquake alerting model\nFigure S9: Calibration diagrams for Japan at different times after the ﬁrst P detection and different PGA thresholds. The\nconﬁdence is deﬁned as the probability of exceeding the PGA threshold as predicted by the model. Each bar represents\nthe traces with a conﬁdence value inside the limits of the bar. Its height is given by the accuracy, the fraction of traces\nactually exceeding the threshold among all traces in the bar. For a perfectly calibrated model, the conﬁdence equals\nthe accuracy. This is indicated by the dashed line. We note that accuracy estimations for the high PGA thresholds are\nstrongly impacted by stochasticity due to the small number of samples.\n26\nThe transformer earthquake alerting model\nFigure S10: Calibration diagrams for Italy at different times after the ﬁrst P detection and different PGA thresholds. For\na further description see the caption of ﬁgure S9.\n27\nThe transformer earthquake alerting model\nTable S1: Performance statistics for Japan. Probability thresholds αwere chosen to maximize F1 scores and are shown\nin the last column. The AUC value does not depend on the threshold α. PGA indicates the used PGA threshold.\nPGA [g] Precision Recall F1 AUC α\nTEAM\n1% 0.70 0.77 0.73 0.82 0.60\n2% 0.69 0.69 0.69 0.76 0.60\n5% 0.59 0.67 0.63 0.68 0.50\n10% 0.50 0.60 0.54 0.56 0.40\n20% 0.33 0.48 0.39 0.35 0.30\nEPS\n1% 0.50 0.63 0.56 0.57 0.40\n2% 0.48 0.48 0.48 0.48 0.40\n5% 0.40 0.40 0.40 0.34 0.30\n10% 0.27 0.36 0.31 0.25 0.20\n20% 0.20 0.26 0.22 0.15 0.20\nPLUM\n1% 0.39 0.56 0.46 - -\n2% 0.30 0.50 0.38 - -\n5% 0.22 0.42 0.29 - -\n10% 0.18 0.39 0.25 - -\n20% 0.11 0.28 0.16 - -\nC-GMPE\n1% 0.58 0.74 0.65 0.69 0.30\n2% 0.47 0.71 0.56 0.60 0.20\n5% 0.44 0.54 0.48 0.48 0.20\n10% 0.44 0.46 0.45 0.43 0.20\n20% 0.56 0.38 0.45 0.42 0.30\nTable S2: Performance statistics for Italy. Probability thresholds αwere chosen to maximize F1 scores and are shown\nin the last column. The AUC value does not depend on the threshold α. PGA indicates the used PGA threshold.\nPGA [g] Precision Recall F1 AUC α\nTEAM\n1% 0.64 0.64 0.64 0.68 0.60\n2% 0.55 0.65 0.60 0.63 0.50\n5% 0.58 0.52 0.55 0.54 0.50\n10% 0.50 0.46 0.48 0.43 0.40\n20% 0.51 0.35 0.42 0.36 0.30\nEPS\n1% 0.44 0.37 0.40 0.37 0.30\n2% 0.44 0.36 0.40 0.36 0.40\n5% 0.41 0.40 0.40 0.33 0.40\n10% 0.39 0.38 0.38 0.30 0.40\n20% 0.39 0.39 0.39 0.25 0.40\nPLUM\n1% 0.25 0.66 0.37 - -\n2% 0.21 0.61 0.31 - -\n5% 0.18 0.60 0.28 - -\n10% 0.22 0.64 0.32 - -\n20% 0.19 0.65 0.30 - -\nC-GMPE\n1% 0.62 0.69 0.65 0.71 0.30\n2% 0.61 0.65 0.63 0.68 0.30\n5% 0.60 0.59 0.60 0.63 0.30\n10% 0.64 0.57 0.60 0.59 0.30\n20% 0.59 0.54 0.56 0.54 0.30\nTable S3: Relative warning times of the algorithms in seconds. Positive values indicate longer average warning times\nfor the second method, negative values shorter warning times. The difference in average warning times is calculated\nfrom all event station pairs, where both methods issued correct warnings. No value is reported if this set is empty. We\nset αfor TEAM and EPS to the optimal value in terms of F1 score.\nJapan Italy\nPGA [g] 1% 2% 5% 10% 20% 1% 2% 5% 10% 20%\nEPS TEAM 0.39 0.43 0.70 0.31 0.61 0.18 0.26 -0.49 -0.65 -1.19\nPLUM TEAM 8.98 8.24 6.35 5.01 0.55 1.49 1.60 1.03 -0.03 0.03\nPLUM EPS 8.53 7.74 5.29 3.08 -0.04 2.95 3.11 2.35 0.81 1.08\n28\nThe transformer earthquake alerting model\nTable S4: Data set statistics for the full data set and the test set. The lower boundary of the magnitude category is the\n5th percentile of the magnitude; this limit is chosen as each data set contains a small number of unrepresentative very\nsmall events. The upper boundary is the maximum magnitude. The lower part of the table shows how often each PGA\nthreshold was exceeded. An event is counted as exceeding a threshold if at least one station exceeded this threshold\nduring the event. The number of exceedances in the test set for Italy is disproportionally high compared to the number\nof events in the test set. This is caused by the high seismic activity and the higher station density in 2016. Traces for\nJapan always refer to 6 component traces, while for Italy it refers to 3 component traces.\nJapan Italy\nFull Test Full Test\nYears 1997 - 2018 08/2013 - 12/2018 2008 - 2019 01/2016 - 12/2016\nMagnitudes 2.7 - 9.0 2.7 - 8.1 2.7 - 6.5 2.7 - 6.5\nEvents 13,512 4,054 7,055 2,123\nUnique stations 697 632 1,080 621\nTraces 372,661 104,573 494,183 253,454\nAvg. traces per event 27.6 25.9 70.3 119.4\nPGA [g] Events Traces Events Traces Events Traces Events Traces\n1% 8,761 55,618 2,710 15,215 1,841 6,379 923 3,826\n2% 5,324 24,396 1,601 6,489 1,013 2,921 503 1,771\n5% 2,026 6,802 583 1,712 348 888 171 563\n10% 782 2,223 216 506 120 330 58 223\n20% 238 631 62 100 40 107 20 82\nTable S5: Architecture of the feature extraction network. The input dimensions of the waveform data are (time,\nchannels). FC denotes fully connected layers. As FC layers can be regarded as 0D convolutions, we write the output\ndimensionality in the ﬁlters column. The “Concatenate scale” layer concatenates the log of the peak amplitude to the\noutput of the convolutions. Depending on the existence of borehole data the number of input ﬁlters for the ﬁrst Conv1D\nlayer is 64 instead of 32 in the non-borehole case.\nLayer Filters Kernel size Stride\nConv2D 8 5, 1 5, 1\nConv2D 32 16, 3 1, 3\nFlatten to 1D\nConv1D 64 16 1\nMaxPool1D 2 2\nConv1D 128 16 1\nMaxPool1D 2 2\nConv1D 32 8 1\nMaxPool1D 2 2\nConv1D 32 8 1\nConv1D 16 4 1\nFlatten to 0D\nConcatenate scale\nFC 500\nFC 500\nFC 500\nTable S6: Architecture of the transformer network. Please note that even though the transformer in TEAM does not\napply dropout, we explicitly state this in the table, as transformers commonly use dropout.\nFeature Value\n# Layers 6\nDimension 500\nFeed forward dimension 1000\n# Heads 10\nMaximum number of stations 25\nDropout 0\nActivation GeLu\n29",
  "topic": "Earthquake warning system",
  "concepts": [
    {
      "name": "Earthquake warning system",
      "score": 0.8502798080444336
    },
    {
      "name": "Warning system",
      "score": 0.7308322191238403
    },
    {
      "name": "Notice",
      "score": 0.6130704879760742
    },
    {
      "name": "Computer science",
      "score": 0.5163635015487671
    },
    {
      "name": "Induced seismicity",
      "score": 0.4941747486591339
    },
    {
      "name": "Seismic hazard",
      "score": 0.48754948377609253
    },
    {
      "name": "Earthquake prediction",
      "score": 0.48117193579673767
    },
    {
      "name": "Seismology",
      "score": 0.47803860902786255
    },
    {
      "name": "Earthquake scenario",
      "score": 0.45711493492126465
    },
    {
      "name": "Earthquake simulation",
      "score": 0.44159188866615295
    },
    {
      "name": "Geology",
      "score": 0.3256267309188843
    },
    {
      "name": "Telecommunications",
      "score": 0.07854422926902771
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210152878",
      "name": "GFZ Helmholtz Centre for Geosciences",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I39343248",
      "name": "Humboldt-Universität zu Berlin",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I75951250",
      "name": "Freie Universität Berlin",
      "country": "DE"
    }
  ],
  "cited_by": 90
}