{
  "title": "Keeping Your Eye on the Ball: Trajectory Attention in Video Transformers",
  "url": "https://openalex.org/W3172908893",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287301988",
      "name": "Patrick, Mandela",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202112620",
      "name": "Campbell, Dylan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223134852",
      "name": "Asano, Yuki M.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221790083",
      "name": "Misra, Ishan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3088819828",
      "name": "Metze, Florian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3160965359",
      "name": "Feichtenhofer, Christoph",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750622813",
      "name": "Vedaldi, Andrea",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226543447",
      "name": "Henriques, João F.",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Henriques, Jo\\~ao F.",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2157939923",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2154889144",
    "https://openalex.org/W2092262344",
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W3034572008",
    "https://openalex.org/W2625366777",
    "https://openalex.org/W3182683290",
    "https://openalex.org/W2770804203",
    "https://openalex.org/W2971680695",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3014619463",
    "https://openalex.org/W3102696055",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2931316642",
    "https://openalex.org/W2990152177",
    "https://openalex.org/W2950971447",
    "https://openalex.org/W3109304426",
    "https://openalex.org/W2984287396",
    "https://openalex.org/W2891446678",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2963155035",
    "https://openalex.org/W3154596443",
    "https://openalex.org/W2963820951",
    "https://openalex.org/W2964091144",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W3168294587",
    "https://openalex.org/W3204588463",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2964491122",
    "https://openalex.org/W2964700958",
    "https://openalex.org/W3122640483",
    "https://openalex.org/W3116298410",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2975761646",
    "https://openalex.org/W1797109199",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W2963112338",
    "https://openalex.org/W2962722947",
    "https://openalex.org/W3171927989",
    "https://openalex.org/W2105101328",
    "https://openalex.org/W3101065397",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2108333036",
    "https://openalex.org/W3037916678",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3108319047",
    "https://openalex.org/W2953296820",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2995460200",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W2963782415",
    "https://openalex.org/W3204032499",
    "https://openalex.org/W3109908659",
    "https://openalex.org/W3104662889",
    "https://openalex.org/W3043840704",
    "https://openalex.org/W3035265375",
    "https://openalex.org/W3034915791",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W2949178656",
    "https://openalex.org/W2883429621",
    "https://openalex.org/W3121592593",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3179019763",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2948048211",
    "https://openalex.org/W3096833468",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W3109931228",
    "https://openalex.org/W3173365702",
    "https://openalex.org/W2024868105",
    "https://openalex.org/W3195108980",
    "https://openalex.org/W3204419213",
    "https://openalex.org/W2955874753",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3099495704",
    "https://openalex.org/W3121735241"
  ],
  "abstract": "In video transformers, the time dimension is often treated in the same way as the two spatial dimensions. However, in a scene where objects or the camera may move, a physical point imaged at one location in frame $t$ may be entirely unrelated to what is found at that location in frame $t+k$. These temporal correspondences should be modeled to facilitate learning about dynamic scenes. To this end, we propose a new drop-in block for video transformers -- trajectory attention -- that aggregates information along implicitly determined motion paths. We additionally propose a new method to address the quadratic dependence of computation and memory on the input size, which is particularly important for high resolution or long videos. While these ideas are useful in a range of settings, we apply them to the specific task of video action recognition with a transformer model and obtain state-of-the-art results on the Kinetics, Something--Something V2, and Epic-Kitchens datasets. Code and models are available at: https://github.com/facebookresearch/Motionformer",
  "full_text": "Keeping Your Eye on the Ball:\nTrajectory Attention in Video Transformers\nMandela Patrick∗\nFacebook AI\nmandelapatrick@fb.com\nDylan Campbell∗\nUniversity of Oxford\ndylan@robots.ox.ac.uk\nYuki Asano∗\nUniversity of Oxford\nyuki@robots.ox.ac.uk\nIshan Misra\nFacebook AI\nimisra@fb.com\nFlorian Metze\nFacebook AI\nfmetze@fb.com\nChristoph Feichtenhofer\nFacebook AI\nfeichtenhofer@fb.com\nAndrea Vedaldi\nFacebook AI\nvedaldi@fb.com\nJoão F. Henriques\nUniversity of Oxford\njoao@robots.ox.ac.uk\nAbstract\nIn video transformers, the time dimension is often treated in the same way as the\ntwo spatial dimensions. However, in a scene where objects or the camera may move,\na physical point imaged at one location in framet may be entirely unrelated to what\nis found at that location in frame t + k. These temporal correspondences should\nbe modeled to facilitate learning about dynamic scenes. To this end, we propose a\nnew drop-in block for video transformers— trajectory attention—that aggregates\ninformation along implicitly determined motion paths. We additionally propose a\nnew method to address the quadratic dependence of computation and memory on\nthe input size, which is particularly important for high resolution or long videos.\nWhile these ideas are useful in a range of settings, we apply them to the speciﬁc task\nof video action recognition with a transformer model and obtain state-of-the-art\nresults on the Kinetics, Something–Something V2, and Epic-Kitchens datasets.\nCode and models are available at: https://github.com/facebookresearch/\nMotionformer.\n1 Introduction\nTransformers [83] have become a popular architecture across NLP [35], vision [22] and speech [5].\nThe self-attention mechanism in the transformer works well for different types of data and across\ndomains. However, its generic nature and its lack of inductive biases also mean that transformers\ntypically require extremely large amounts of data for training [63, 9], or aggressive domain-speciﬁc\naugmentations [79]. This is particularly true for video data, for which transformers are also appli-\ncable [56], but where statistical inefﬁciencies are exacerbated. While videos carry rich temporal\ninformation, they can also contain redundant spatial information from neighboring frames. Vanilla\nself-attention applied to videos compares pairs of image patches extracted at all possible spatial\nlocations and frames. This can lead it to focus on the redundant spatial information rather than the\ntemporal information, as we show by comparing normalization strategies in our experiments.\nWe therefore contribute a variant of self-attention, called trajectory attention, which is better able\nto characterize the temporal information contained in videos. For the analysis of still images,\n∗Equal contribution.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2106.05392v2  [cs.CV]  23 Oct 2021\ntime\nFigure 1: Trajectory attention. In this sequence of frames from the Kinetics-400 dataset, depicting\nthe action ‘kicking soccer ball’, the ball does not remain stationary with respect to the camera, but\ninstead moves to different locations in each frame. Trajectory attention aims to share information\nalong the motion path of the ball, a more natural inductive bias for video data than pooling axially\nalong the temporal dimension or over the entire space-time feature volume. This allows the network\nto aggregate information from multiple views of the ball, to reason about its motion characteristics,\nand to be less sensitive to camera motion.\nspatial locality is perhaps the most important inductive bias, motivating the design of convolutional\nnetworks [46] and the use of spatial encodings in vision transformers [22]. This is a direct consequence\nof the local structure of the physical world: points that belong to the same 3D object tend to project\nto pixels that are close to each other in the image. By studying the correlation of nearby pixels, we\ncan thus learn about the objects.\nVideos are similar, except that 3D points move over time, thus projecting on different parts of the\nimage along certain 2D trajectories. Existing video transformer methods [8, 3, 56] disregard these\ntrajectories, pooling information over the entire 3D space-time feature volume [3, 56], or pooling\naxially across the temporal dimension [8]. We contend that pooling along motion trajectories would\nprovide a more natural inductive bias for video data, allowing the network to aggregate information\nfrom multiple views of the same object or region, to reason about how the object or region is moving\n(for example, the linear and angular velocities), and to be invariant to camera motion.\nWe leverage attention itself as a mechanism to ﬁnd these trajectories. This is inspired by methods\nsuch as RAFT [ 78], which showed that excellent estimates of optical ﬂow can be obtained from\nthe correlation volume obtained by comparing local features across space and time. We observe\nthat the joint attention mechanism for video transformers computes such a correlation volume as an\nintermediate result. However, subsequent processing collapses the volume without consideration\nfor its particular structure. In this work, we seek instead to use the correlation volume to guide the\nnetwork to pool information along motion paths.\nWe also note that visual transformers operate on image patches which, differently from individual\npixels, cannot be assumed to correspond to individual 3D points and thus to move along simple 1D\ntrajectories. For example, in Figure 1, depicting the action ‘kicking soccer ball’, the ball spans up to\nfour patches, depending on the speciﬁc video frame. Furthermore, these patches contain a mix of\nforeground (the ball) and background objects, thus at least two distinct motions. Fortunately, we are\nnot forced to select a single putative motion: the attention mechanism allows us to assemble a motion\nfeature from all relevant ‘ball regions’.\nInspired by Nyströmformer [ 95], we also propose a principled approximation to self-attention,\nOrthoformer. Our approximation sets state-of-the-art performance on the recent Long Range Arena\n(LRA) benchmark [77] for evaluating efﬁcient attention approximations and generalizes beyond the\nvideo domain to long text and high resolution images, with lower FLOPS and memory requirements\ncompared to alternatives, Nyströmformer and Performer [16]. Combining our approximation with\ntrajectory attention allows us to signiﬁcantly improve its computational and memory efﬁciency. With\nour contributions, we set state-of-the-art results on four video action recognition benchmarks.\n2\n2 Related Work\nVideo representations and 3D-CNNs. Hand-crafted features were originally used to convert video\ndata into a representation amenable to analysis by a shallow linear model. Such representations\ninclude SIFT-3D [67], HOG3D [42], and IDT [84]. Since the breakthrough of AlexNet [43] on the\nImageNet classiﬁcation benchmark [65], which demonstrated the empirical beneﬁts of deep neural\nnetworks to learn representations end-to-end, there have been many attempts to do the same for video.\nArchitectures with 3D convolutions—3D-CNNs—were originally proposed to learn deep video\nrepresentations [80]. Since then, improvements to this paradigm include the use of ImageNet-inﬂated\nweights [12], the space-time decomposition of 3D convolutions [ 61, 82, 94], channel-separated\nconvolutions [81], non-local blocks [87], and attention layers [14]. Optical ﬂow-based pooling can\nbe used instead of temporal convolutions to improve the representation’s robustness to camera and\nobject motions [1]. Our approach shares this motivation.\nVision transformers. The transformer architecture [83], originally proposed for natural language\nprocessing, has recently gained traction in the computer vision domain. The vision transformer\n(ViT) [22] decomposes an image into a sequence of 16 ×16 words and uses a multi-layer transformer\nto perform image classiﬁcation. To improve ViT’s data efﬁciency, DeiT [79] used distillation from\na strong teacher model and aggressive data augmentation. Transformers have also been used in\na variety of vision image tasks, such as image representation learning [ 13, 92, 20, 66], image\ngeneration [58], object detection [52, 10], video question-answering [38], few-shot learning [21], and\nimage–text [54, 70, 75, 48, 76], video-text [72, 71, 99, 28, 60, 2, 6], and video-audio [47, 59, 32]\nrepresentation learning.\nAttention for video recognition. The self-attention operation proposed in the transformer [ 83]\nhave been adapted to video recognition tasks. Wang et al. [87] propose the non-local mean operation\nfor video action recognition, which is equivalent to the standard transformer self-attention applied\nuniformly across space and time, while our proposed trajectory attention does not treat the space\nand time dimensions equivalently. Zhao et al. [ 97] propose a CNN architecture that explicitly\npredicts trajectories and aggregates information along them using a convolution operation. In\ncontrast, our transformer architecture does not explicitly predict trajectories, but instead provides an\ninductive bias that encourages the network to consider motion trajectories where useful. Concurrent\nworks [8, 3, 56, 24] have also adapted the self-attention operation to the spatio-temporal nature\nof videos, however, these approaches do not have a mechanism for reasoning about motion paths,\ntreating time as just another dimension, unlike our approach.\nEfﬁcient attention. Due to the quadratic complexity of self-attention, there has been a signiﬁcant\namount of research on how to reduce its computational complexity with respect to time and memory\nuse. Sparse attention mechanisms [15] were used to reduce self-attention complexity to O(n√n),\nand locality-sensitivity hashing was used by Reformer [41] to further reduce this to O(n log n). More\nrecently, linear attention mechanisms have been introduced, namely Longformer [7], Linformer [86],\nPerformer [16] and Nyströmformer [ 95]. The Long Range Arena benchmark [ 77] was recently\nintroduced to compare these different attention mechanisms.\nTemporal correspondences and optical ﬂow. There are many approaches that aim to establish\nexplicit correspondences between video frames as a way to reason about camera and object motion.\nFor short-range correspondences across time, optical ﬂow algorithms [33, 73, 78] are highly effective.\nIn particular, RAFT [78] showed the effectiveness of an all-pairs inter-frame correlation volume as an\nencoding, which is essentially an attention map. All-pairs intra-frame correlations were subsequently\nshown to help resolve correspondence ambiguities [37]. For longer-range correspondences, object\ntracking by repeated detection [64] and data association can be used. In contrast to these approaches,\nour work does not explicitly establish temporal correspondences, but facilitates implicit correspon-\ndence learning via trajectory attention. Jabri et al. [34] estimate correspondences in a similar way,\nframing the problem as a contrastive random walk on a graph and apply explicit guidance via a cycle\nconsistency loss. Incorporating such guidance into a video transformer is an interesting direction.\n3 Trajectory Attention for Video Data\nOur goal is to modify the attention mechanism in transformers to better capture the information\ncontained in videos. Consider an input video I ∈RT′×3×H×W consisting of T′frames of resolution\n3\nFigure 2: Trajectory attention ﬂowchart. We divide the attention operation into two stages: the ﬁrst\nforming a set of ST trajectory tokens for every space-time location st—a spatial attention operation\nbetween pairs of frames—and the second pooling along these trajectories with a 1D temporal attention\noperation. In this way, we accumulate information along the motion paths of objects in the video.\nThe softmax operations are computed over the last dimension.\nH ×W. As in existing video transformer models [8, 3], we pre-process the video into a sequence\nof ST tokens xst ∈RD, for a spatial resolution of S and a temporal resolution of T. We use a\ncuboid embedding [3, 24], where disjoint spatio-temporal cubes from the input volume are linearly\nprojected to RD (equivalent to a 3D convolution with downsampling). We also test an embedding\nof disjoint image patches [ 22]. A learnable positional encoding e ∈RD is added to the video\nembeddings for spatial and temporal dimensions separately, resulting in the codezst = xst + es\ns + et\nt.\nFinally, a learnable classiﬁcation token zcls is added to the sequence of tokens, like in the BERT\nTransformer [35], to reason about the video as a whole. For clarity, we elide the classiﬁcation token\nfrom our treatment in the sequel.\nWe now have a set of tokens that form the input to a sequence of transformer layers that, as in\nViT [22], consist of Layer Norm (LN) operations [ 4], multi-head attention (MHA) [ 83], residual\nconnections [30], and a feed-forward network (MLP):\ny = MHA(LN(z)) + z; z′= MLP(LN(y)) + y. (1)\nIn the next section, we shall focus on a single head of the attention operation, and demonstrate\nhow self-attention can realize a suitable inductive bias for video data. For clarity of exposition, we\nabuse the notation slightly, neglecting the layer norm operation and using the same dimensions for\nsingle-head attention as for multi-head attention.\n3.1 Video self-attention\nThe self-attention operation begins by forming a set of query-key-value vectors qst, kst, vst ∈RD,\none for each space-time location st in the video. These are computed as linear projections of the input\nzst, that is, qst = Wqzst, kst = Wkzst, and vst = Wvzst, for projection matrices Wi ∈RD×D.\nA direct application of attention across space-time (called joint space-time attention[8, 3]) computes:\nyst =\n∑\ns′t′\nvs′t′ · exp⟨qst, ks′t′ ⟩∑\n¯s¯t exp⟨qst, k¯s¯t⟩. (2)\nIn this way, each queryqst is compared to all keys ks′t′ using dot products, the results are normalized\nusing the softmax operator, and the weights thus obtained are used to average the values corresponding\nto the keys. Compared to a standard transformer, we have omitted for brevity the softmax temperature\nparameter D1/2 and instead assume that the queries and keys have been divided by D1/4.\nOne issue with this formulation is that it has quadratic complexity in both space and time, i.e.,\nO(S2T2). An alternative is to restrict attention to either space or time (called divided space-time\nattention):\nyst =\n∑\ns′\nvs′t · exp⟨qst, ks′t⟩∑\n¯s exp⟨qst, k¯st⟩ (space); yst =\n∑\nt′\nvst′ · exp⟨qst, kst′ ⟩∑\n¯t exp⟨qst, ks¯t⟩ (time). (3)\n4\nThis reduces the complexity to O(S2T) and O(ST 2), respectively, but only allows the model to\nanalyse time and space independently. This is usually addressed by interleaving [8] or stacking [3]\nthe two attention modules in a sequence.\nDifferent to both of these approaches, we perform attention along trajectories, the probabilistic\npath of a token between frames.2 For each space-time location st (the trajectory ‘reference point’)\nand corresponding query qst, we construct a set of trajectory tokens ˜ystt′ , representing the pooled\ninformation weighted by the trajectory probability. The trajectory extends for the duration of the\nvideo sequence and its tokens ˜ystt′ ∈RD at different times t′are given by:\n˜ystt′ =\n∑\ns′\nvs′t′ · exp⟨qst, ks′t′ ⟩∑\n¯s exp⟨qst, k¯st′ ⟩. (4)\nNote that the attention in this formula is applied spatially (index s) and independently for each\nframe. Intuitively, this pooling operation implicitly seeks the location of the trajectory at time t′by\ncomparing the trajectory query qst to the keys ks′t′ at time t′.\nOnce the trajectories are computed, we further pool them across time to reason about intra-frame\ninformation/connections. To do so, the trajectory tokens are projected to a new set of queries, keys\nand values as usual:\n˜qst = ˜Wq ˜ystt, ˜kstt′ = ˜Wk ˜ystt′ , ˜vstt′ = ˜Wv ˜ystt′ . (5)\nLike qst before, the updated reference query ˜qst corresponds to the trajectory reference point st and\ncontains information spatially-pooled from across the reference frame t. This new query is used to\npool across the new time (trajectory) dimension by applying 1D attention:\nyst =\n∑\nt′\n˜vstt′ · exp⟨˜qst, ˜kstt′ ⟩∑\n¯t exp⟨˜qst, ˜kst¯t⟩\n. (6)\nLike joint space-time attention, our approach has quadratic complexity in both space and time,\nO(S2T2), so has no computational advantage and is in fact slower than divided space-time attention.\nHowever, we demonstrate better accuracy than both joint and divided space-time attention mecha-\nnisms. We also provide fast approximations in Section 3.2. A ﬂowchart of the full trajectory attention\noperation is shown in tensor form in Figure 2.\n3.2 Approximating attention\nTo complement our trajectory attention, we also propose an approximation scheme to speed up\ncalculations. This scheme is generic and applies to any attention-like pooling mechanism. We thus\nswitch to a generic transformer-like notation to describe it. Namely, consider query-key-value matrices\nQ, K, V ∈RD×N such that the query-key-value vectors are stored as columns qi, ki, vi ∈RD in\nthese matrices.\nIn order to obtain an efﬁcient decomposition of the attention operator, we will rewrite it using a\nprobabilistic formulation. Let Aij ∈{0, 1}be a categorical random variable indicating whether the\njth input (with key vector kj ∈RD) is assigned to the ith output (with query vector qi ∈RD), with∑\nj Aij = 1. The attention operator uses a parametric model of the probability of this event based on\nthe multinomial logistic function, i.e., the softmax operator S(·):3\nP(Ai:) = S(qT\ni K), (7)\nwhere the subscript : denotes a full slice of the input tensor in that dimension. We now introduce\nthe latent variables Uℓj ∈{0, 1}, which similarly indicate whether the jth input is assigned to the\nℓth prototype, an auxiliary vector which we denote by pℓ ∈RD. We can use the laws of total and\nconditional probability to obtain:\nP(Aij) =\n∑\nℓ\nP(Aij |Uℓj)P(Uℓj). (8)\nNote that the latent variables that we chose are independent of the inputs (keys). They use the\nsame parametric model, but with parameters P ∈RD×R (the concatenated prototype vectors pℓ):\n2Here, we refer to the trajectory as the motion between pairs of frames, rather than a multi-frame path.\n3I.e. [S(z)]i = exp(zi/\n√\nD)/ ∑\nj exp(zj/\n√\nD). For matrix inputs, the sum is over the columns.\n5\nP(U) = S(PTK). Eq. 8 is exact, even under the parametric model for P(U), though the corre-\nsponding true distribution P(A |U) is intractable. We now approximate the conditional probability\nP(A |U) with a similar parametric model:\n˜P(A |U) = S(QTP), (9)\nwhere Q ∈RD×N concatenates all query vectors horizontally. Substituting equations 7–9 we\nwrite the full approximate attention ˜A, multiplied by an arbitrary matrix V (which in the case of a\ntransformer contains the values of the key–value pairs stacked as rows):\n˜P(A)V = S(QTP)\n(\nS(PTK)V\n)\n. (10)\nComputational efﬁciency. One important feature of the approximation in eq. 10 is that it can\nbe computed in two steps. First the values V are multiplied by a prototypes-keys attention matrix\nS(PTK) ∈RR×N , which can be much smaller than the full attention matrix S(QTK) ∈RN×N\n(eq. 7), i.e., R ≪N. Finally, this product is multiplied by a queries-prototypes attention matrix\nS(QTP) ∈RN×R, which is also small. This allows us to sidestep the quadratic dependency of full\nattention over the input and output size (O(N2)), replacing it with linear complexity (O(N)) as long\nas R is kept constant.\nPrototype selection. The aim for prototype-based attention approximation schemes is to use as\nfew prototypes as possible while reconstructing the attention operation as accurately as possible. As\nsuch, it behooves us to select prototypes efﬁciently. We have two priorities for the prototypes: to\ndynamically adjust to the query and key vectors so that their region of space is well-reconstructed,\nand to minimize redundancy. The latter is important because the relative probability of a query–key\npair may be over-estimated if many prototypes are clustered near that query and key. To address these\ncriteria, we incrementally build a set of prototypes from the set of queries and keys such that a new\nprototype is maximally orthogonal to the prototypes already selected, starting with a query or key at\nrandom. This greedy strategy is dynamic, since it selects prototypes from the current set of queries\nand keys, and has high entropy, since it preferences well-separated prototypes. Moreover, it balances\nspeed and performance by using a greedy strategy, rather than ﬁnding a globally-optimal solution to\nthe maximum entropy sampling problem [68], making it suitable for use in a transformer.\nNaïvely applying prototype-based attention approximation techniques to video transformers would\ninvolve creating a unique set of prototypes for each frame in the video. However, additional\nmemory savings can be realized by sharing prototypes across time. Since there is signiﬁcant\ninformation redundancy between frames, video data is opportune for compression via temporally-\nshared prototypes.\nOrthoformer algorithm. The proposed approximation algorithm is outlined in Algorithm 1. The\nattention matrix is approximated using intermediate prototypes, selected as the most orthogonal\nsubset of the queries and keys, given a desired number of prototypes R. To avoid a linear dependence\non the sequence length N, we ﬁrst randomly subsample cR queries and keys, for a constant c, before\nselecting the most orthogonal subset, resulting in a complexity quadratic in the number of prototypes\nO(R2). The algorithm then computes two attention matrices, much smaller than the original problem,\nand multiplies them with the values. The most related approach in the literature is Nyströmformer [95]\nattention, outlined in Algorithm 2. This approach involves a pseudoinverse to attenuate the effect of\nnear-parallel prototypes, has more operations, and a greater memory footprint.\nAlgorithm 1 Orthoformer (proposed) attention\n1: P ←MostOrthogonalSubset(Q, K, R)\n2: Ω1 = S(QTP/\n√\nD)\n3: Ω2 = S(PTK/\n√\nD)\n4: Y = Ω1(Ω2V)\nAlgorithm 2 Nyströmformer [95] attention\n1: Pq, Pk ←SegmentMeans(Q, K, R)\n2: Ω1 = S(QTPk/\n√\nD)\n3: Ω−1\n2 = IterativeInverse(S(PT\nq Pk/\n√\nD), Niter)\n4: Ω3 = S(PT\nq K/\n√\nD)\n5: Y = Ω1\n(\nΩ−1\n2 (Ω3V)\n)\n3.3 The Motionformer model\nOur full video transformer model builds on previous work, as shown in Table 1. In particular, we use\nthe ViT image transformer model [22] as the base architecture, the separate space and time positional\nencodings of TimeSformer [8], and the cubic image tokenization strategy as in ViViT [ 3]. These\ndesign choices are ablated in Section 4. The crucial difference for our model is the trajectory attention\nmechanism, with which we demonstrate greater empirical performance than the other models.\n6\nTable 1: Comparison of recent video transformer models. We show the different design choices\nof recent video transformer models and how they compare to our proposed Motionformer model.\nModel Base Model Attention Pos. Encoding Tokenization\nTimeSformer [8] ViT-B Divided Space–Time Separate Square\nViViT [3] ViT-L Joint/Divided Space–Time Joint Cubic\nMotionformer ViT-B Trajectory Separate Cubic\n4 Experiments\nDatasets. Kinetics [39] is a large-scale video classiﬁcation dataset consisting of short clips\ncollected from YouTube, licensed by Google under Creative Commons. As it is a dataset of human\nactions, it potentially contains personally identiﬁable information such as faces, names and license\nplates. Something–Something V2 [29] is a video dataset containing more than 200,000 videos\nacross 174 classes, with a greater emphasis on short temporal clips. In contrast to Kinetics, the\nbackground and objects remain consistent across different classes, and therefore models have to\nreason about ﬁne-grained motion signals. We veriﬁed the importance of temporal reasoning on this\ndataset by showing that a single frame model gets signiﬁcantly worse results, a decrease of 39%\ntop-1 accuracy. In contrast, a drop of only 7% is seen on the Kinetics-400 dataset, showing that\ntemporal information is much less relevant there. We obtained a research license for this data from\nhttps://20bn.com; the data was collected with consent. Epic Kitchens-100 [18] is an egocentric\nvideo dataset capturing daily kitchen activities. The highest scoring verb and action pair predicted by\nthe network constitutes an action, for which we report top-1 accuracy. The data is licensed under\nCreative Commons and was collected with consent by the Epic Kitchens teams.\nImplementation details. We follow a standard training and augmentation pipeline [3], as detailed\nin the appendix. For ablations, our default Motionformer model is the Vision Transformer Base\narchitecture [22] (ViT/B), pretrained on ImageNet-21K [19], patch-size 2×16×16 with central frame\ninitialization [3], separate space-time positional embedding and our trajectory attention. The base\narchitecture has 12 layers, 12 attention heads, and an embedding dimension of 768. Our default\nMotionformer model operates on 16×224×224 videos with temporal stride 4 i.e. temporal extent of\n2s. For comparisons with state-of-the-art, we report results on two additional variants: Motionformer-\nHR, which has a high spatial resolution (16×336×336 videos with temporal stride 4 i.e. temporal\nextent of 2s), and Motionformer-L, which has a long temporal range ( 32×224×224 videos with\ntemporal stride 3 i.e. temporal extent of 3s). Experiments with the large ViT architecture are deferred\nto the appendix.\n4.1 Ablation studies\nInput: tokenization. We consider the effect of different input tokenization approaches for both joint\nand trajectory attention on Kinetics-400 (K-400) and Something–Something V2 (SSv2) in Table 2b.\nFor patch tokenization ( 1×16×16), we use inputs of size 8×224×224, while for cubic [ 3, 24]\ntokenization (2×16×16), we use inputs of size 16×224×224 to ensure that the model has the same\nnumber of input tokens over the same temporal range of 2 seconds. For both attention types, we see\nthat cubic tokenization gives a 1% accuracy improvement over square tokenization on SSv2, a dataset\nfor which temporal information is critical. Furthermore, our proposed trajectory attention using cubic\ntokenization outperforms joint space-time attention on both datasets.\nInput: positional encoding. Here, we ablate using a joint or separate [ 24] (default) space-time\npositional encoding in Table 2b. Similar to the results for input tokenization, the choice of positional\nencoding is particularly important for the ﬁne-grained motion dataset, SSv2. Since joint space-time\nattention treats tokens in the space-time volume equally, it beneﬁts particularly from separating the\npositional encodings, allowing it to differentiate between space and time dimensions, with a 4%\nimprovement on SSv2 over joint space-time encoding. Our proposed trajectory attention elicits a more\nmodest improvement of 1% from using separated positional encodings on SSv2, and outperforms\njoint space-time attention in both settings on both datasets.\nAttention block: comparisons. We compare our proposed trajectory attention to joint space-time\nattention [3], and divided space-time attention [ 8] in Table 4. Our trajectory attention (bottom\nrow) outperforms both alternatives on the K-400 and SSv2 datasets. While we see only modest\n7\nTable 2: Input encoding ablations: Comparison of input tokenization and positional encoding\ndesign choices. We report GFLOPS and top-1 accuracy (%) on K-400 and SSv2.\n(a) Cubic tokenization works best for trajectory attn.\nAttention Tokenization GFlops K-400 SSv2\nJoint ST Square (1 ×162) 179.7 79.4 63.0\nCubic (2×162) 180.6 79.2 64.0\nTrajectory Square (1×162) 368.5 79.4 65.8\nCubic (2×162) 369.5 79.7 66.5\n(b) Trajectory attn. works well with both encodings.\nAttention Pos. Encoding GFlops K-400 SSv2\nJoint ST Joint ST 180.6 79.1 60.8\nSeparate ST [24] 180.6 79.2 64.0\nTrajectory Joint ST 369.5 79.6 65.8\nSeparate ST [24] 369.5 79.7 66.5\nTable 3: Orthoformer ablations: We ablate various aspects of our Orthoformer approximation.\nE denotes exact attention and A denotes approximate attention. We report max CUDA memory\nconsumption (GB) and top-1 accuracy (%) on K-400 and SSv2.\n(a) Orthoformer is competitive with Nyström.\nAttention Approx. Mem. K-400 SSv2\nTrajectory (E) N/A 7.4 79.7 66.5\nTrajectory (A) Performer 5.1 72.9 52.7\nNyströmformer 3.8 77.5 64.0\nOrthoformer 3.6 77.5 63.8\n(b) Selecting orthogonal prototypes is the best strategy.\nAttention Selection Mem. K-400 SSv2\nTrajectory (E) N/A 7.4 79.7 66.5\nTrajectory (A) Seg-Means 3.6 75.8 60.3\nRandom 3.6 76.5 62.5\nOrthogonal 3.6 77.5 63.8\n(c) Approximation improves with more prototypes.\nAttention # Prototypes Mem. K-400 SSv2\nTrajectory (E) N/A 7.4 79.7 66.5\nTrajectory (A) 16 3.1 73.9 59.2\n64 3.3 74.9 63.0\n128 3.6 77.5 63.8\n(d) Temporal sharing is the best strategy.\nAttention Sharing Mem. K-400 SSv2\nTrajectory (E) N/A 7.4 79.7 66.5\nTrajectory (A) \u0017 16.5 77.3 61.5\n✓ 3.6 77.5 63.8\nimprovements on the appearance cue-reliant K-400 dataset, our trajectory attention signiﬁcantly\noutperforms (+2%) the other approaches on the motion cue-reliant SSv2 dataset. This dataset requires\nﬁne-grained motion understanding, something explicitly singled out by previous video transformer\nworks [3, 8] as a challenge for their models. In contrast, our trajectory attention excels on this dataset,\nindicating that its motion-based design is able to capture some of this information.\nAttention block: trajectory attention design. We ablate two design choices for our trajectory\nattention: the per-frame softmax normalization and the 1D temporal attention. Unlike joint space-time\nattention, which normalizes the attention map over all tokens in space and time, trajectory attention\nnormalizes independently per frame, allowing us to implicitly track the trajectories of query patches\nin time. In row 5 of Table 4, we ablate the beneﬁts of this design choice. We observe a reduction of\n2.5% on K-400 and 5.6% on SSv2 by normalizing over space and time (NormST ) compared with\nnormalizing over space alone (NormS). In row 4, we show the beneﬁt of using 1D temporal attention\n(AttT ) to aggregate temporal features, compared to average pooling (AvgT ). We observe reductions of\n3.7% on K-400 and 6.5% on SSv2 when using average pooling instead of temporal attention applied\nto the motion trajectories, although it saves computing the additional query/key/value projections.\n4.2 Orthoformer approximated attention\nApproximation comparisons. In Table 3a, we compare our Orthoformer algorithm to alternative\nstrategies: Nyströmformer [ 95] and Performer [ 16]. Our algorithm performs comparably with\nNyströmformer with a reduced memory footprint. In Table 5, we also compare these attention\nmechanisms on the Long Range Arena benchmark [77] to show applicability to other tasks and data\ntypes. Orthoformer is able to effectively approximate self-attention, outperforming the state-of-the-art\ndespite using far fewer prototypes (64) and so gaining signiﬁcant computational and memory beneﬁts.\nPrototype selection. A key part of our Orthoformer algorithm is the prototype selection procedure.\nIn Table 3b, we ablate three prototype selection strategies: segment-means, random, and greedy\nmost-orthogonal selection. Segment-means, the strategy used in Nyströmformer, performs poorly\nbecause it can generate multiple parallel prototypes, which will over-estimate the relative probability\n8\nTable 4: Attention ablations: We compare trajectory attention with alternatives and ablate its design\nchoices. We report GFLOPS and top-1 accuracy (%) on K-400 and SSv2. AttT : temporal attention,\nAvgT : temporal averaging, NormST : space-time normalization, NormS: spatial normalization.\nAttention AttT AvgT NormS NormST GFLOPS K-400 SSv2\nJoint Space-Time – – – – 180.6 79.2 64.0\nDivided Space-Time – – – – 185.8 78.5 64.2\n\u0017 ✓ ✓ \u0017 180.6 76.0 60.0\n✓ \u0017 \u0017 ✓ 369.5 77.2 60.9\nTrajectory ✓ \u0017 ✓ \u0017 369.5 79.7 66.5\nTable 5: Comparison to the state-of-the-art on Long Range Arena benchmark. GFLOPS and\nCUDA maximum Memory (MB) are reported for the ListOps task. Note that our algorithm achieves\nthe best overall results with far fewer prototypes (64) than the other methods.\nModel ListOps Text Retrieval Image Pathﬁnder Avg↑ GFLOPS↓ Mem.↓\nExact [83] 36.69 63.09 78.22 31.47 66.35 55.16 1.21 4579\nPerformer-256 [16] 36.69 63.22 78.98 29.39 66.55 54.97 0.49 885\nNyströmformer-128 [95] 36.90 64.17 78.67 36.16 52.32 53.64 0.62 745\nOrthoformer-64 33.87 64.42 78.36 33.26 66.41 55.26 0.24 344\nof query–key pairs near those redundant prototypes. In contrast, our proposed strategy of selecting the\nmost orthogonal prototypes from the query and key set works the best across both datasets, because it\nexplicitly minimises prototype redundancy with respect to direction.\nNumber of prototypes. In Table 3c, we show that Orthoformer improves monotonically as the\nnumber of prototypes is increased. In particular, we see an average performance improvement of 4%\non both datasets as we increase the number of prototypes from 16 to 128.\nTemporally-shared prototypes. In Table 3d, we demonstrate the memory savings and perfor-\nmance beneﬁts of sharing prototypes across time. On SSv2, we observe a 2% improvement in\nperformance and a 5×decrease in memory usage. These gains may be attributed to the regularization\neffect of having prototypes leverage redundant information across frames.\nScaling transformer models with approximated trajectory attention. The Orthoformer atten-\ntion approximation algorithm allows us to train larger models and higher resolution inputs for a\ngiven GPU memory budget. Here, we verify this, by training a large vision transformer model\n(ViT-L/16) [22] with a higher resolution input (336 ×336 pixels) on the Kinetics-400 dataset, using\nthe Orthoformer approximation with 196 temporally-shared prototypes and the same schedule as\nthe base model. We use a ﬁxed patch size (in pixels) for all models, and so the number of input\ntokens to the transformer scales with the square of the image resolution. As shown in Table 7, this\nmodel achieves a competitive accuracy without ﬁne-tuning the training schedule, hyperparameters\nor data augmentation strategy. We expect that ﬁne-tuning these on a validation set would greatly\nimprove the model’s performance, based on results from contemporary work [3]. Obviously such a\nparameter sweep is more time-consuming for these large models, however, these preliminary results\nare indicative that higher accuracies are attainable if these parameters were to be optimized.\n4.3 Comparison to the state-of-the-art\nIn Table 6, we compare our method against the current state-of-the-art on four common benchmarking\ndatasets: Kinetics-400, Kinetics-600, Something–Something v2 and Epic-Kitchens. We ﬁnd that our\nmethod performs favorably against current methods, even when compared against much larger models\nsuch as ViViT-L. In particular, it achieves strong top-1 accuracy improvements of1.0% and 2.3% for\nSSv2 and Epic-Kitchen Nouns, respectively. These datasets require greater motion reasoning than\nKinetics and so are a more challenging benchmark for video action recognition.\n5 Conclusion\nWe have presented a new general-purpose attention block for video data that aggregates information\nalong implicitly determined motion trajectories, lending a realistic inductive bias to the model.\nWe further address its quadratic dependence on the input size with a new attention approximation\n9\nTable 6: Comparison to the state-of-the-art on video action recognition. We report GFLOPS\nand top-1 ( %) and top-5 ( %) video action recognition accuracy on K-400/600, and SSv2. On\nEpic-Kitchens, we report top-1 (%) action (A), verb (V), and noun (N) accuracy.\n(a) Something–Something V2\nModel Pretrain Top-1 Top-5 GFLOPs ×views\nSlowFast [27] K-400 61.7 - 65.7 ×3×1\nTSM [51] K-400 63.4 88.5 62.4 ×3×2\nSTM [36] IN-1K 64.2 89.8 66.5 ×3×10\nMSNet [44] IN-1K 64.7 89.4 67 ×1×1\nTEA [50] IN-1K 65.1 - 70 ×3×10\nbLVNet [25] IN-1K 65.2 90.3 128.6 ×3×10\nVidTr-L [49] IN-21K+K-400 60.2 - 351 ×3×10\nTformer-L [8] IN-21K 62.5 - 1703 ×3×1\nViViT-L [3] IN-21K+K-400 65.4 89.8 3992 ×4×3\nMViT-B [24] K-400 67.1 90.8 170 ×3×1\nMformer IN-21K+K-400 66.5 90.1 369.5 ×3×1\nMformer-L IN-21K+K-400 68.1 91.2 1185.1×3×1\nMformer-HR IN-21K+K-400 67.1 90.6 958.8 ×3×1\n(b) Kinetics-400\nMethod Pretrain Top-1 Top-5 GFLOPs ×views\nI3D [12] IN-1K 72.1 89.3 108 ×N/A\nR(2+1)D [82] - 72.0 90.0 152 ×5×23\nS3D-G [94] IN-1K 74.7 93.4 142.8 ×N/A\nX3D-XL [26] - 79.1 93.9 48.4 ×3×10\nSlowFast [27] - 79.8 93.9 234 ×3×10\nVTN [56] IN-21K 78.6 93.7 4218 ×1×1\nVidTr-L [49] IN-21K 79.1 93.9 392 ×3×10\nTformer-L[8] IN-21K 80.7 94.7 2380 ×3×1\nMViT-B [24] - 81.2 95.1 455 ×3×3\nViViT-L [3] IN-21K 81.3 94.7 3992 ×3×4\nMformer IN-21K 79.7 94.2 369.5 ×3×10\nMformer-L IN-21K 80.2 94.8 1185.1 ×3×10\nMformer-HR IN-21K 81.1 95.2 958.8×3×10\n(c) Epic-Kitchens\nMethod Pretrain A V N\nTSN [85] IN-1K 33.2 60.2 46.0\nTRN [98] IN-1K 35.3 65.9 45.4\nTBN [40] IN-1K 36.7 66.0 47.2\nTSM [51] IN-1K 38.3 67.9 49.0\nSlowFast [27] K-400 38.5 65.6 50.0\nViViT-L [3] IN-21K+K-400 44.0 66.4 56.8\nMformer IN-21K+K-400 43.1 66.7 56.5\nMformer-L IN-21K+K-400 44.1 67.1 57.6\nMformer-HR IN-21K+K-400 44.5 67.0 58.5\n(d) Kinetics-600\nModel Pretrain Top-1 Top-5 GFLOPs ×views\nAttnNAS [89] - 79.8 94.4 -\nLGD-3D [62] IN-1K 81.5 95.6 -\nSlowFast [27] - 81.8 95.1 234 ×3×10\nX3D-XL [26] - 81.9 95.5 48.4 ×3×10\nTformer-HR [8] IN-21K 82.4 96.0 1703 ×3×1\nViViT-L [3] IN-21K 83.0 95.7 3992 ×3×4\nMViT-B-24 [24] - 83.8 96.3 236×1×5\nMformer IN-21K 81.6 95.6 369.5 ×3×10\nMformer-L IN-21K 82.2 96.0 1185.1 ×3×10\nMformer-HR IN-21K 82.7 96.1 958.8 ×3×10\n-\nTable 7: Can we train larger models using approximated trajectory attention? We report top-1\nand top-5 accuracy (%) on the Kinetics-400 dataset of two variants of our Motionformer model:\nMotionformer-B and Motionformer-H. The former uses the base model with exact (E) trajectory\nattention, while the latter uses a much larger model (ViT-L) and a higher resolution input (336 ×336\npixels) with approximate (A) trajectory attention, i.e., using Orthoformer. We reduce this to a linear\nrelationship with the Orthoformer approximation, which allows us to ﬁt the model on the GPU.\nModel Base model Params Attention Max memory (GB) Top-1 Top-5\nMformer-B ViT-B/224 109.1M Trajectory (E) 7.3 79.7 94.2\nMformer-H ViT-L/336 381.9M Trajectory (A) 22.2 80.0 94.5\nalgorithm that signiﬁcantly reduces the memory requirements, the largest bottleneck for transformer\nmodels. With these contributions, we obtain state-of-the-art results on several benchmark datasets.\nNonetheless, our approach inherits many of the limitations of transformer models, including poor\ndata efﬁciency and slow training. Speciﬁc to this work, trajectory attention has higher computational\ncomplexity than alternative attention operations used for video data. This is attenuated by the\nproposed approximation algorithm, with signiﬁcantly reduced memory and computation requirements.\nHowever, its runtime is bottlenecked by prototype selection, which is not easily parallelized.\nFuture work. There are many applications of trajectory attention beyond video action classiﬁcation,\nsuch as those tasks where temporal context is highly important. We see signiﬁcant potential for\nusing trajectory attention for tracking [31], temporal action localization [93, 90] and online action\ndetection [96, 69], among other settings, and leave these as avenues for future work.\nPotential negative societal impacts. One negative impact of this research is the signiﬁcant envi-\nronmental impact associated with training transformers, which are large and compute-expensive\nmodels. Compared to 3D-CNNs where the compute scales linearly with the sequence length, video\ntransformers scale quadratically. To mitigate this, we proposed an approximation algorithm with\nlinear complexity that greatly reduces the computational requirements. There is also potential for\nvideo action recognition models to be misused, such as for unauthorized surveillance.\n10\nAcknowledgments and Disclosure of Funding\nWe are grateful for support from the Rhodes Trust (M.P.), the European Research Council Starting\nGrant (IDIU 638009, D.C.), Qualcomm Innovation Fellowship (Y .A.), the Royal Academy of\nEngineering (RF201819/18/163, J.H.), and EPSRC Centre for Doctoral Training in Autonomous\nIntelligent Machines & Systems (EP/L015897/1, M.P. and Y .A.). Funding for M.P. was received under\nhis Oxford afﬁliation. We thank Bernie Huang, Dong Guo, Rose Kanjirathinkal, Gedas Bertasius,\nMike Zheng Shou, Mathilde Caron, Hugo Touvron, Benjamin Lefaudeux, Haoqi Fan, and Geoffrey\nZweig from Facebook AI for their help, support, and discussion around this project. We also thank\nMax Bain and Tengda Han from VGG for fruitful discussions.\n11\nReferences\n[1] Triantafyllos Afouras, Andrew Owens, Joon Son Chung, and Andrew Zisserman. Self-supervised learning\nof audio-visual objects from video. In ECCV, 2020.\n[2] Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu Chang, Yin Cui, and Boqing Gong.\nVatt: Transformers for multimodal self-supervised learning from raw video, audio and text, 2021.\n[3] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario Luˇci´c, and Cordelia Schmid. Vivit: A\nvideo vision transformer, 2021.\n[4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016.\n[5] Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework for\nself-supervised learning of speech representations. In NeurIPS, 2020.\n[6] Max Bain, Arsha Nagrani, Gül Varol, and Andrew Zisserman. Frozen in time: A joint video and image\nencoder for end-to-end retrieval, 2021.\n[7] Iz Beltagy, Matthew E. Peters, and Arman Cohan. Longformer: The long-document transformer. 2020.\n[8] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video\nunderstanding? In ICML, 2021.\n[9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark,\nChristopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models\nare few-shot learners. In NeurIPS, 2020.\n[10] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\n[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand\nJoulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.\n[12] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset.\nIn CVPR, 2017.\n[13] Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, Prafulla Dhariwal, David Luan, and Ilya\nSutskever. Generative pretraining from pixels. In ICML, 2020.\n[14] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and Jiashi Feng.a2-nets: Double attention\nnetworks. In NeurIPS, 2018.\n[15] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse\ntransformers. URL https://openai.com/blog/sparse-transformers, 2019.\n[16] Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin\nBelanger, Lucy J Colwell, and Adrian Weller. Rethinking attention with performers. In ICLR, 2021.\n[17] E. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V . Le. Randaugment: Practical data augmentation with\nno separate search. In CVPRW, 2020.\n[18] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Antonino Furnari, Evangelos Kazakos, Jian Ma,\nDavide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, et al. Rescaling egocentric vision. In ECCV,\n2020.\n[19] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical\nimage database. In CVPR, 2009.\n[20] Karan Desai and Justin Johnson. VirTex: Learning Visual Representations from Textual Annotations. In\nCVPR, 2021.\n[21] Carl Doersch, Ankush Gupta, and Andrew Zisserman. Crosstransformers: spatially-aware few-shot transfer.\nIn NeurIPS, 2020.\n[22] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and\nNeil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR,\n2021.\n[23] Haoqi Fan, Yanghao Li, Bo Xiong, Wan-Yen Lo, and Christoph Feichtenhofer. Pyslowfast. https:\n//github.com/facebookresearch/slowfast, 2020.\n[24] Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan, Jitendra Malik, and Christoph\nFeichtenhofer. Multiscale vision transformers, 2021.\n[25] Quanfu Fan, Chun-Fu (Ricarhd) Chen, Hilde Kuehne, Marco Pistoia, and David Cox. More Is Less:\nLearning Efﬁcient Video Representations by Temporal Aggregation Modules. In NeurIPS, 2019.\n[26] Christoph Feichtenhofer. X3d: Expanding architectures for efﬁcient video recognition. In CVPR, 2020.\n[27] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video\nrecognition. In ICCV, 2019.\n[28] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. Multi-modal transformer for video\nretrieval. In ECCV, 2020.\n[29] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michalski, Joanna Materzynska, Susanne Westphal,\nHeuna Kim, Valentin Haenel, Ingo Fruend, Peter Yianilos, Moritz Mueller-Freitag, and et al. The\n“something something” video database for learning and evaluating visual common sense. In ICCV, 2017.\n[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\nIn CVPR, 2016.\n12\n[31] João F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. High-speed tracking with kernelized\ncorrelation ﬁlters. IEEE transactions on pattern analysis and machine intelligence, 37(3):583–596, 2014.\n[32] Po-Yao Huang, Mandela Patrick, Junjie Hu, Graham Neubig, Florian Metze, and Alexander Hauptmann.\nMultilingual multimodal pre-training for zero-shot cross-lingual transfer of vision-language models. In\nNAACL, 2021.\n[33] Eddy Ilg, Nikolaus Mayer, Tonmoy Saikia, Margret Keuper, Alexey Dosovitskiy, and Thomas Brox.\nFlownet 2.0: Evolution of optical ﬂow estimation with deep networks. In CVPR, 2016.\n[34] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as a contrastive random walk.\nIn NeurIPS, 2020.\n[35] Kenton Lee Jacob Devlin, Ming-Wei Chang and Kristina Toutanova. BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. In NAACL, 2018.\n[36] Boyuan Jiang, Mengmeng Wang, Weihao Gan, Wei Wu, and Junjie Yan. Stm: Spatiotemporal and motion\nencoding for action recognition. In ICCV, 2019.\n[37] Shihao Jiang, Dylan Campbell, Yao Lu, Hongdong Li, and Richard Hartley. Learning to estimate hidden\nmotions with global motion aggregation. In ICCV, 2021.\n[38] Yash Kant, Dhruv Batra, Peter Anderson, Alex Schwing, Devi Parikh, Jiasen Lu, and Harsh Agrawal.\nSpatially aware multimodal transformers for textvqa. In ECCV, 2020.\n[39] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan,\nFabio Viola, Tim Green, Trevor Back, Paul Natsev, et al. The kinetics human action video dataset, 2017.\n[40] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and Dima Damen. Epic-fusion: Audio-visual\ntemporal binding for egocentric action recognition. In ICCV, 2019.\n[41] Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In ICLR, 2020.\n[42] Alexander Klaser, Marcin Marszałek, and Cordelia Schmid. A spatio-temporal descriptor based on\n3d-gradients. In BMVC, 2008.\n[43] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. In NeurIPS, 2012.\n[44] Heeseung Kwon, Manjin Kim, Suha Kwak, and Minsu Cho. Motionsqueeze: Neural motion feature\nlearning for video understanding. In ECCV, 2020.\n[45] Zihang Lai, Erika Lu, and Weidi Xie. Mast: A memory-augmented self-supervised tracker. In IEEE Conf.\nComput. Vis. Pattern Recog., 2020.\n[46] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to\ndocument recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.\n[47] Sangho Lee, Youngjae Yu, Gunhee Kim, Thomas Breuel, Jan Kautz, and Yale Song. Parameter efﬁcient\nmultimodal transformers for video representation learning. In ICLR, 2021.\n[48] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. Visualbert: A simple and\nperformant baseline for vision and language, 2019.\n[49] Xinyu Li, Yanyi Zhang, Chunhui Liu, Bing Shuai, Yi Zhu, Biagio Brattoli, Hao Chen, Ivan Marsic, and\nJoseph Tighe. Vidtr: Video transformer without convolutions, 2021.\n[50] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and Limin Wang. Tea: Temporal excitation and\naggregation for action recognition. In CVPR, 2020.\n[51] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift module for efﬁcient video understanding. 2019.\n[52] Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob\nUszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-centric learning with slot attention. In NeurIPS,\n2020.\n[53] Ilya Loshchilov and Frank Hutter. Fixing weight decay regularization in adam, 2018.\n[54] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic\nrepresentations for vision-and-language tasks. In NeurIPS, 2019.\n[55] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris\nGinsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, and Hao Wu. Mixed precision training.\nIn ICLR, 2018.\n[56] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan Asselmann. Video transformer network, 2021.\n[57] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using\nspace-time memory networks. In Int. Conf. Comput. Vis., 2019.\n[58] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Łukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin\nTran. Image transformer. In ICML, 2018.\n[59] Mandela Patrick, Yuki M. Asano, Bernie Huang, Ishan Misra, Florian Metze, Joao Henriques, and Andrea\nVedaldi. Space-time crop & attend: Improving cross-modal video representation learning, 2021.\n[60] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander G Hauptmann, Joao F. Henriques,\nand Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. In ICLR, 2021.\n[61] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d residual\nnetworks. In ICCV, 2017.\n[62] Zhaofan Qiu, Ting Yao, Chong-Wah Ngo, Xinmei Tian, and Tao Mei. Learning spatio-temporal representa-\ntion with local and global diffusion. CVPR, 2019.\n[63] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. OpenAI Blog, 1(8):9, 2019.\n[64] Deva Ramanan, David A. Forsyth, and Andrew Zisserman. Strike a pose: Tracking people by ﬁnding\nstylized poses. In Proc. CVPR, 2005.\n13\n[65] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large\nScale Visual Recognition Challenge. IJCV, 2015.\n[66] Mert Bulent Sariyildiz, Julien Perez, and Diane Larlus. Learning visual representations with caption\nannotations. In ECCV, 2020.\n[67] P. Scovanner, S. Ali, and M. Shah. A 3-dimensional sift descriptor and its application to action recognition.\nIn ACM MM, 2007.\n[68] Michael C Shewry and Henry P Wynn. Maximum entropy sampling. Journal of Applied Statistics,\n14(2):165–170, 1987.\n[69] Gurkirt Singh, Suman Saha, Michael Sapienza, Philip HS Torr, and Fabio Cuzzolin. Online real-time\nmultiple spatiotemporal action localisation and prediction. In Proceedings of the IEEE International\nConference on Computer Vision, pages 3637–3646, 2017.\n[70] Weijie Su, Xizhou Zhu, Yue Cao, Bin Li, Lewei Lu, Furu Wei, and Jifeng Dai. Vl-bert: Pre-training of\ngeneric visual-linguistic representations. In ICLR, 2020.\n[71] Chen Sun, Fabien Baradel, Kevin Murphy, and Cordelia Schmid. Contrastive bidirectional transformer for\ntemporal representation learning, 2019.\n[72] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy, and Cordelia Schmid. Videobert: A joint model\nfor video and language representation learning. In ICCV, 2019.\n[73] Deqing Sun, Xiaodong Yang, Ming-Yu Liu, and Jan Kautz. Pwc-net: Cnns for optical ﬂow using pyramid,\nwarping, and cost volume. In CVPR, 2018.\n[74] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking\nthe inception architecture for computer vision. In CVPR, 2016.\n[75] Hao Tan and Mohit Bansal. Lxmert: Learning cross-modality encoder representations from transformers.\nIn EMNLP, 2019.\n[76] Hao Tan and Mohit Bansal. V okenization: Improving language understanding with contextualized,\nvisual-grounded supervision. In EMNLP, 2020.\n[77] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang,\nSebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efﬁcient transformers. In ICLR,\n2021.\n[78] Zachary Teed and Jia Deng. RAFT: recurrent all-pairs ﬁeld transforms for optical ﬂow. In Proc. ECCV,\n2020.\n[79] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé\nJégou. Training data-efﬁcient image transformers & distillation through attention. In ICML, 2021.\n[80] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal\nfeatures with 3d convolutional networks. In ICCV, 2015.\n[81] Du Tran, Heng Wang, Matt Feiszli, and Lorenzo Torresani. Video classiﬁcation with channel-separated\nconvolutional networks. In ICCV, 2019.\n[82] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at\nspatiotemporal convolutions for action recognition. In CVPR, 2018.\n[83] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\n[84] Heng Wang and Cordelia Schmid. Action recognition with improved trajectories. In ICCV, 2013.\n[85] Limin Wang, Yuanjun Xiong, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment\nnetworks: Towards good practices for deep action recognition. In ECCV, 2016.\n[86] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear\ncomplexity. In NeurIPS, 2020.\n[87] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR,\n2018.\n[88] Xiaolong Wang, Allan Jabri, and Alexei A Efros. Learning correspondence from the cycle-consistency of\ntime. In CVPR, 2019.\n[89] Xiaofang Wang, Xuehan Xiong, Maxim Neumann, AJ Piergiovanni, Michael S Ryoo, Anelia Angelova,\nKris M Kitani, and Wei Hua. Attentionnas: Spatiotemporal attention cell search for video classiﬁcation. In\nECCV, 2020.\n[90] Philippe Weinzaepfel, Zaid Harchaoui, and Cordelia Schmid. Learning to track for spatio-temporal action\nlocalization. In Proceedings of the IEEE international conference on computer vision, pages 3164–3172,\n2015.\n[91] Ross Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models,\n2019.\n[92] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer,\nand Peter Vajda. Visual transformers: Token-based image representation and processing for computer\nvision, 2020.\n[93] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaiming He, Philipp Krahenbuhl, and Ross Girshick.\nLong-term feature banks for detailed video understanding. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 284–293, 2019.\n[94] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal\nfeature learning: Speed-accuracy trade-offs in video classiﬁcation. In ECCV, 2018.\n[95] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas\nSingh. Nyströmformer: A nyström-based algorithm for approximating self-attention. In AAAI, 2021.\n14\n[96] Mingze Xu, Yuanjun Xiong, Hao Chen, Xinyu Li, Wei Xia, Zhuowen Tu, and Stefano Soatto. Long\nshort-term transformer for online action detection. arXiv preprint arXiv:2107.03377, 2021.\n[97] Yue Zhao, Yuanjun Xiong, and Dahua Lin. Trajectory convolution for action recognition. In Proceedings\nof the 32nd International Conference on Neural Information Processing Systems, pages 2208–2219, 2018.\n[98] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Torralba. Temporal relational reasoning in videos.\nIn ECCV, 2018.\n[99] Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. In CVPR, 2020.\n15\n6 Appendix\n6.1 Further experimental analysis and results\n6.1.1 Does trajectory attention make better use of motion cues?\nIn the main paper (and below in Section 6.1.2), we provide evidence that action classiﬁcation on\nthe Something–Something V2 (SSv2) dataset [29] is more reliant on motion cues than the Kinetics\ndataset [39], where appearance cues dominate and a single-frame model achieves high accuracy.\nImproved performance on SSv2 is one way to infer that our model makes better use of temporal\ninformation, however, here we consider another way. We artiﬁcially adjust the speed of the video\nclips by changing the temporal stride of the input. A larger stride simulates faster motions, with\nadjacent frames being more different. If our trajectory attention is able to make better use of the\ntemporal information in the video than the other attention mechanisms, we expect the margin of\nimprovement to increase as the temporal stride increases. As shown in Figure 3, this is indeed what\nwe observe, with the lines diverging as temporal stride increases, especially for the motion cue-reliant\nSSv2 dataset. Since the same number of frames are used as input in all cases, the larger the stride, the\nmore of the video clip is seen by the model. This provides additional conﬁrmation that seeing a small\npart of a Kinetics video is usually enough to classify it accurately, as shown on the bottom left, where\nthe absolute accuracy is reported.\n6.1.2 How important are motion cues for classifying videos from the Kinetics-400 and\nSomething–Something V2 datasets?\nTo determine the relative importance of motion cues compared to appearance cues for classifying\nvideos on two of the major video action recognition datasets (Kinetics-400 and Something–Something\nV2), we trained a single frame vision transformer model and compare the results to a multi-frame\nmodel that can reason about motion. The single frame was sampled from the video at random.\nTable 8 shows that single-frame action classiﬁers can do almost as well as video action classiﬁers\non the Kinetics-400 dataset, implying that the motion information is much less relevant. In contrast,\nclassifying videos from the Something-Something V2 dataset clearly requires this motion information.\nTherefore, to excel on the SSv2 dataset, a model must reason about motion information. Our model,\nwhich introduces an inductive bias that favors pooling along motion trajectories, is able to do this and\nsees corresponding performance gains.\nTable 8: Importance of motion cues for the K-400 and SSv2 datasets. A classiﬁer for the K-400\ndataset performs well when all motion information is removed (1 frame model), while a classiﬁer for\nthe SSv2 dataset performs very poorly. Therefore, SSv2 is a better dataset for evaluating video action\nclassiﬁcation, where the combination of appearance and motion is critical.\nDataset Top-1 accuracy (1 frame) Top-1 accuracy (8 frames) ∆\nKinetics-400 73.2 79.7 6.5\nSomething–Something V2 27.1 66.5 39.4\n6.1.3 Which classes is the performance difference larger with and without the trajectory\nattention?\nThe class labels with the largest performance increase (given in parentheses) on the Something-\nSomething v2 dataset are: “Spilling [something] next to [something]” (18%), “Pretending to put\n[something] underneath [something]” (15%), and “Trying to pour [something] into [something],\nbut missing so it spills next to it” (14%). The classes with the largest performance decrease are:\n“Putting [something] that can’t roll onto a slanted surface, so it stays where it is” (10%), “Putting\n[something] on a ﬂat surface without letting it roll” (9%), and “Showing a photo of [something] to the\ncamera” (8%). It is apparent that classes involving predominantly stationary objects do not beneﬁt\nfrom trajectory attention, as we would expect.\n16\nTemporal stride\nTop-1 accuracy margin\n-1.25\n-1.00\n-0.75\n-0.50\n-0.25\n0.00\n0.25\n2 4 6 8 10 12 14 16\nJoint Divided Trajectory\n(a) K-400: top-1 accuracy margin\nTemporal stride (xS)\nTop-1 accuracy margin\n-3\n-2\n-1\n0\n1\n1/4 1/2 3/4 1\nJoint Divided Trajectory (b) SSv2: top-1 accuracy margin\nTemporal stride\nTop-1 accuracy\n76\n77\n78\n79\n80\n2 4 6 8 10 12 14 16\nJoint Divided Trajectory\n(c) K-400: top-1 accuracy\nTemporal stride (xS)\nTop-1 accuracy\n15\n35\n55\n75\n1/4 1/2 3/4 1\nJoint Divided Trajectory (d) SSv2: top-1 accuracy\nFigure 3: Does trajectory attention make better use of motion cues? Performance of transformer\nmodels with joint space-time attention, divided space-time attention, and trajectory attention, as\nthe temporal stride increases, on the Kinetics-400 dataset (left) and the Something–Something V2\ndataset (right). Top: top-1 accuracy margin relative to trajectory attention (difference of accuracy\nand trajectory accuracy). Bottom: absolute top-1 accuracy shown for reference. If our trajectory\nattention is able to make better use of the temporal information in the video than the other attention\nmechanisms, we expect the accuracy margin between the methods to increase as the temporal stride\nincreases. This is indeed the observed behaviour, especially for the motion cue-reliant SSv2 dataset.\nA larger stride simulates greater motion between input frames, which trajectory attention is better\nable to model and reason about. Note that the larger the stride, the more of the video clip is seen by\nthe model; for all plots, the rightmost side of the axis corresponds to the entire video clip. Note also\nthat the strides for SSv2 are written as multiples of S, the stride needed to evenly sample the entire\nvideo clip.\n6.1.4 Trajectory attention maps\nIn Figure 4, we show qualitative results of the intermediate attention maps of our trajectory attention\noperation. The learned attention maps appear to implicitly track the query points across time, a\nstrategy that is easier to learn with the inductive bias instilled by trajectory attention.\n6.1.5 How long does it take to train Motionformer model?\nFor Table 3c, using Motionformer with orthoformer approximation, 16 prototypes take 384 GPU\nhours, 64 prototypes take 800 GPU hours, and 128 prototypes take 1216 GPU hours. For the Kinetics-\n400 state-of-the-art table, the Mformer-B model took 384 GPU hours, the Mformer-L took 1334\nGPU hours, and Mformer-HR model took 1376 GPU hours to train. Our baseline Mformer-B model,\nwhich outperforms TimeSformer-B by over 1%, takes similar GPU hours (416 (ours) vs the 416 GPU\nhours reported in Table 2 of TimeSformer). We cannot directly compare to ViViT because they didn’t\n17\nquery\nquery\nFigure 4: Trajectory attention maps. In this sequence of frames from Kinetics-400 (row 1) and\nSomething-Something V2 (row 3), we show the attention maps at each frame given an initial query\npoint (red point). We see that the model learns to implicitly track along motion paths (yellow arrow)\nusing our trajectory attention module.\nreport training time, but they used a very large transformer (24 layers) compared to ours (12 layers)\nand so we expect the training time for their approach to be signiﬁcantly greater.\n6.1.6 Semi-supervised Video Object Segmentation on DA VIS 2017\nWe evaluate our baseline Motionformer Kinetics-pretrained model (16x16 with Trajectory Attention)\non the semi-supervised video object segmentation task on the DA VIS 2017 dataset as in Jabri et\nal. [34] in Table 9. We directly use the attention maps of our Motionformer model in the label\npropagation setting, as in [34]. We report mean (m) of standard boundary alignment (F) and region\nsimilarity (J) metrics. We attain a competitive J&F-Mean of 60.6. For comparison, DINO [ 11]\nobtains J&F-Mean of 62.3 with the same architecture (ViT-B/16x16), but by using a self-supervised\nlearning task on IM-1K. We expect that we could signiﬁcantly improve the performance by using an\n8x8 patch size, as this was shown to be highly effective for the task [11].\n6.2 Implementation details\nPreprocessing. During training, we randomly sample clips of size 16×224×224 at a rate of 1/4\nfrom 30 FPS videos, thereby giving an effective temporal resolution of just over 2 seconds. We\nnormalize the inputs with mean and standard deviation 0.5, rescaling in the range [−1, 1]. We use\nstandard video augmentations such as random scale jittering, random horizontal ﬂips and color\njittering. For smaller datasets such as Something–Something V2 and Epic-Kitchens, we additionally\napply rand-augment [17]. During testing, we uniformly sample 10 clips per video and apply a 3 crop\nevaluation [27].\nTraining. For all datasets, we use the AdamW [ 53] optimizer with weight decay 5 ×10−2, a\nbatch size per GPU of 4, label smoothing [74] with alpha 0.2 and mixed precision training [55]. For\n18\nTable 9: DA VIS 2017 Video object segmentation. We evaluate the quality of frozen features on\nvideo instance tracking. We report mean region similarity Jm and mean contour-based accuracy Fm.\nMethod Data Arch. (J&F)m Jm Fm\nSupervised\nImageNet INet ViT-S/8 66.0 63.9 68.1\nSTM [57] I/D/Y RN50 81.8 79.2 84.3\nOurs K-400 Mformer-B/16 60.6 58.3 62.9\nSelf-supervised\nCT [88] VLOG RN50 48.7 46.4 50.0\nMAST [45] YT-VOS RN18 65.5 63.3 67.6\nSTC [34] Kinetics RN18 67.6 64.8 70.2\nDINO [11] INet ViT-B/16 62.3 60.7 63.9\nKinetics-400/600 and Something-Something V2, we train for 35 epochs, with an initial learning rate\nof 10−4, which we decay by 10 at epochs 20, 30. As Epic-Kitchens is a smaller dataset, we use a\nlonger schedule and train for 50 epochs with decay at 30 and 40.\nLong Range Arena benchmark details. For the Long-Range Arena benchmark [77], we used the\ntraining, validation, and testing code and parameters from the Nyströmformer Github repository. The\nPerformer [16] implementation was ported over to PyTorch from the ofﬁcial Github repo, and the\nNyströmformer [95] implementation was used directly from its Github repository.\nComputing resources. Ablation experiments were run on a GPU cluster using 4 nodes (32 GPUs)\nwith an average training time of 12 hours. Experiments for comparing with state-of-the-art models\nused 8 nodes (64 GPUs), with an average training time of 7 hours.\nLibraries. For our code implementation, we used the timm [91] library for our base vision trans-\nformer implementation, and the PySlowFast [23] library for training, data processing, and the\nevaluation pipeline.\n19",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.712466835975647
    },
    {
      "name": "Computation",
      "score": 0.5679747462272644
    },
    {
      "name": "Transformer",
      "score": 0.5644658803939819
    },
    {
      "name": "Artificial intelligence",
      "score": 0.563732385635376
    },
    {
      "name": "Computer vision",
      "score": 0.552070677280426
    },
    {
      "name": "Frame rate",
      "score": 0.4419933557510376
    },
    {
      "name": "Computer graphics (images)",
      "score": 0.35419243574142456
    },
    {
      "name": "Algorithm",
      "score": 0.20535951852798462
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}