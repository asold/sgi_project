{
  "title": "An Efficient Transformer Decoder with Compressed Sub-layers",
  "url": "https://openalex.org/W3174339925",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2148565904",
      "name": "Yanyang Li",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2098738808",
      "name": "Ye Lin",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A1983914940",
      "name": "Tong Xiao",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2496766346",
      "name": "Jingbo Zhu",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2148565904",
      "name": "Yanyang Li",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2098738808",
      "name": "Ye Lin",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A1983914940",
      "name": "Tong Xiao",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2496766346",
      "name": "Jingbo Zhu",
      "affiliations": [
        "Northeastern University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2894175714",
    "https://openalex.org/W2964045208",
    "https://openalex.org/W2975381464",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W6746208923",
    "https://openalex.org/W4240508093",
    "https://openalex.org/W2890964657",
    "https://openalex.org/W6638523607",
    "https://openalex.org/W3035812575",
    "https://openalex.org/W2963736842",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W3102816807",
    "https://openalex.org/W6772305528",
    "https://openalex.org/W3035083896",
    "https://openalex.org/W6745245109",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6772381481",
    "https://openalex.org/W2970290486",
    "https://openalex.org/W2964213727",
    "https://openalex.org/W6787920355",
    "https://openalex.org/W2888520903",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2767206889",
    "https://openalex.org/W2763421725",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2970731908",
    "https://openalex.org/W2965046076",
    "https://openalex.org/W2997751980",
    "https://openalex.org/W2963295570",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W2194775991"
  ],
  "abstract": "The large attention-based encoder-decoder network (Transformer) has become prevailing recently due to its effectiveness. But the high computation complexity of its decoder raises the inefficiency issue. By examining the mathematic formulation of the decoder, we show that under some mild conditions, the architecture could be simplified by compressing its sub-layers, the basic building block of Transformer, and achieves a higher parallelism. We thereby propose Compressed Attention Network, whose decoder layer consists of only one sub-layer instead of three. Extensive experiments on 14 WMT machine translation tasks show that our model is 1.42x faster with performance on par with a strong baseline. This strong baseline is already 2x faster than the widely used standard baseline without loss in performance.",
  "full_text": "An Efﬁcient Transformer Decoder with Compressed Sub-layers\nYanyang Li1*, Ye Lin1\u0003, Tong Xiao1,2† , Jingbo Zhu1,2\n1NLP Lab, School of Computer Science and Engineering, Northeastern University, Shenyang, China\n2NiuTrans Research, Shenyang, China\nfblamedrlee, linye2015g@outlook.com, fxiaotong,zhujingbog@mail.neu.edu.cn\nAbstract\nThe large attention-based encoder-decoder network (Trans-\nformer) has become prevailing recently due to its effective-\nness. But the high computation complexity of its decoder\nraises the inefﬁciency issue. By examining the mathematic\nformulation of the decoder, we show that under some mild\nconditions, the architecture could be simpliﬁed by compress-\ning its sub-layers, the basic building block of Transformer,\nand achieves a higher parallelism. We thereby propose Com-\npressed Attention Network, whose decoder layer consists of\nonly one sub-layer instead of three. Extensive experiments\non 14 WMT machine translation tasks show that our model is\n1.42\u0002faster with performance on par with a strong baseline.\nThis strong baseline is already 2\u0002faster than the widely used\nstandard baseline without loss in performance.\nIntroduction\nTransformer is an attention-based encoder-decoder model\n(Vaswani et al. 2017). It has shown promising results in\nmachine translation tasks recently (Wang et al. 2019; Li\net al. 2020a; Zhang et al. 2020; Li et al. 2020b). Nonethe-\nless, Transformer suffers from the inefﬁciency issue at infer-\nence. This problem is attributed to the Transformer decoder\nfor two reasons: 1) the decoder is deep (Kasai et al. 2020).\nIt consists of multiple layers and each layer contains three\nsub-layers, including two attentions and a feed-forward net-\nwork; 2) the attention has a high (quadratic time) complexity\n(Zhang, Xiong, and Su 2018), as it needs to compute the cor-\nrelation between any two input words.\nPrevious work has focused on improving the complex-\nity of the attention in the decoder to accelerate the in-\nference. For example, A AN uses the averaging operation\nto avoid computing the correlation between input words\n(Zhang, Xiong, and Su 2018). S AN share the attention re-\nsults among layers (Xiao et al. 2019). On the other hand,\nwe learn that vanilla attention runs faster in training than\nin inference thanks to its parallelism. This offers a new di-\nrection: a higher degree of parallelism could speed up the\ninference. The most representative work of this type is the\nnon-autoregressive approach (Gu et al. 2018). Its decoder\n*Authors contributed equally.\n†Corresponding author.\nCopyright © 2021, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\npredicts all words in parallel, but fails to model the word de-\npendencies. Despite of their successes, all these systems still\nhave a deep decoder.\nIn this work, we propose to parallelize the sub-layers to\nobtain a shallow autoregressive decoder. This way does not\nsuffer from the poor result of directly reducing depths and\navoids the limitation of non-autoregressive approaches. We\nprove that the two attention sub-layers in a decoder layer\ncould be parallelized if we assume their inputs are close to\neach other. This assumption holds and thereby we compress\nthese two attentions into one. Furthermore, we show that\nthe remaining feed-forward network could also be merged\ninto the attention due to their linearity. To the end, we pro-\npose Compressed Attention Network (CAN for short). The\ndecoder layer of CAN possesses a single attention sub-layer\nthat does the previous three sub-layers’ jobs in parallel.\nAs another “bonus”, C AN is simple and easy to be imple-\nmented.\nIn addition, Kasai et al. (2020) empirically discover that\nexisting systems are not well balancing the encoder and de-\ncoder depths. Based on their work, we build a system with a\ndeep encoder and a shallow decoder, which is 2\u0002faster than\nthe widely used standard baseline without loss in perfor-\nmance. It requires neither the architecture modiﬁcation nor\nadding extra parameters. This system serves as a stronger\nbaseline for a more convincing comparison.\nWe evaluate CAN and the stronger baseline in 14 machine\ntranslation tasks, including WMT14 English$fGerman,\nFrenchg(En$fDe, Frg) and WMT17 English$fGerman,\nFinnish, Latvian, Russian, Czechg (En$fDe, Fi, Lv, Ru,\nCsg). The experiments show that CAN is up to 2.82\u0002faster\nthan the standard baseline with almost no loss in perfor-\nmance. Even comparing to our stronger baseline, C AN still\nhas a 1.42\u0002 speed-up, while other acceleration techniques\nsuch as SAN and AAN are 1.12\u00181.16\u0002in the same case.\nTo summarize, our contributions are as follows:\n• We propose C AN, a novel architecture that accelerates\nTransformer by compressing its sub-layers for a higher\ndegree of parallelism. CAN is easy to be implemented.\n• Our work is based on a stronger baseline, which is 2\u0002\nfaster than the widely used standard baseline.\n• The extensive experiments on 14 WMT machine transla-\ntion tasks show that CAN is 1.42\u0002faster than the stronger\nTheThi rty-Fi fth AAA ICon ferenceon A rti fi ci al Intellig ence(AAAI-21)\n13315\n\u0002N\nwˇo hˇen hˇao .\nEncoder\nI am ﬁne .\nam ﬁne . heosi\nSelf-Attention\nCross-Attention\nFFNTarget Token\nSource Token\nDecoder\n(a) Transformer\n\u0002N\nwˇo hˇen hˇao .\nEncoder I am ﬁne .\nam ﬁne . heosi\nCompressed-Attention\nTarget Token\nSource Token\nDecoder\n(b) C AN\nFigure 1: Transformer vs. CAN (Chinese pinyin!English: “w ˇo hˇen hˇao . ”!“I am ﬁne . ”).\nbaseline and 2.82\u0002 for the standard baseline. C AN also\noutperforms other approaches such as SAN and AAN.\nBackground: Transformer\nTransformer is one of the state-of-the-art neural models in\nmachine translation. It consists of a N-layer encoder and a\nN-layer decoder, where N = 6 in most cases. The encoder\nmaps the source sentence to a sequence of continuous rep-\nresentations and the decoder maps these representations to\nthe target sentence. All layers in the encoder or decoder are\nidentical to each other.\nThe layer in the decoder consists of three sub-layers, in-\ncluding the self-attention, the cross-attention and the feed-\nforward network (FFN). The self-attention takes the output\nX of the previous sub-layer as its input and produces a ten-\nsor with the same size as its output. It computes the attention\ndistribution Ax and then averages X by Ax. We denote the\nself-attention as Yx = Self( X), where X 2Rt\u0002d, t is the\ntarget sentence length and d is the dimension of the hidden\nrepresentation:\nAx = SoftMax(XWq1WT\nk1XT\np\nd\n) (1)\nYx = AxXWv1 (2)\nwhere Wq1; Wk1; Wv1 2Rd\u0002d.\nThe cross-attention is similar to the self-attention, except\nthat it takes the encoder output H as an additional input.\nWe denote the cross-attention as Yh = Cross(X; H), where\nH 2Rs\u0002d, s is the source sentence length:\nAh = SoftMax(XWq2WT\nk2HT\np\nd\n) (3)\nYh = AhHWv2 (4)\nwhere Wq2; Wk2; Wv2 2Rd\u0002d.\nThe FFN applies non-linear transformation to its inputX.\nWe denote the FFN as Yf = FFN(X):\nYf = ReLU(XW1 + b1)W2 + b2 (5)\n1 2 3 4 5 6\n1\n2\n3\n4\n5\n6\nCross-Attention\nSelf-Attention\n1 2 3 4 5 6\n1\n2\n3\n4\n5\n6\nFFN\nCross-Attention\nFigure 2:\nThe cosine similarity of inputs for every two ad-\njacent sub-layers on WMT14 En-De translation task (a dark\ncell means the inputs are dissimilar).\nwhere W1 2Rd\u00024d, b1 2R4d, W2 2R4d\u0002d and b2 2Rd.\nAll sub-layers are coupled with the residual connection\n(He et al. 2016a), i.e.,Y = f(X)+ X where f could be any\nsub-layer. Their inputs are also preprocessed by the layer\nnormalization ﬁrst (Ba, Kiros, and Hinton 2016). Fig. 1(a)\nshows the architecture of Transformer decoder. For more de-\ntails, we refer the reader to Vaswani et al. (2017).\nCompressed Attention Network\nCompressing Self-Attention and Cross-Attention\nAs suggested by Huang et al. (2016), the output of one layer\nin the residual network can be decomposed into the sum\nof all outputs from previous layers. For the adjacent self-\nattention and cross-attention, we can write their ﬁnal output\nas Y = X + Self(X) + Cross(X0; H), where X is the in-\nput of self-attention and X0 = X + Self(X) is the input\nof cross-attention. If X and X0are identical, we are able to\naccelerate the computation of Y by parallelizing these two\nattentions, as X0do not need to wait Self(X) to ﬁnish.\nPrevious work (He et al. 2016b) has shown that inputs\nof adjacent layers are similar. This implies that X and X0\nare close and the parallelization is possible. We empirically\nverify this in the left part of Fig. 2 by examining the cosine\nsimilarity between inputs of every self-attention and cross-\nattention pairs. It shows that X and X0are indeed close to\n13316\n[H; X]\nY\nd 4d\nA\nd\nY=\nh\nHfWv2;XfWv1\ni\nEq. 11\nY = AY\nY = XW1 + Y + b1\nY = ReLU(Y )W2 + b2\nInput\nOutputTarget TokenX\nSource Token H\nFigure 3: Compressed-Attention.\neach other (a high similarity > 0:9 for the diagonal entries).\nTherefore we could assumeX and X0are identical (we omit\nthe layer normalization for simplicity):\nY = X + Self(X) + Cross(X; H) (6)\nBy observing that Eq. 2 and Eq. 4 are essentially matrix\nmultiplications, we could rewrite Self(X) + Cross(X; H)\nas a single matrix multiplication:\nA =\n\u0002\nAT\nx ; AT\nh\n\u0003T\n(7)\nSelf(X) + Cross(X; H) = A [XWv1; HWv2] (8)\n[\u0001] is the concatenation operation along the ﬁrst dimension.\nXiao et al. (2019) shows that some attention distributions\nAx and Ah are duplicate. This means that there exists a cer-\ntain redundancy in fWq1; Wk1gand fWq2; Wk2g. Thus we\ncould safely share Wq1 and Wq2 to parallelize the computa-\ntion of the attention distribution A:\n\u0016A =\n\u0010\nXWq [XWk1; HWk2]T\n\u0011\n=\np\nd (9)\nA =\n\u0002\nSoftMax( \u0016AT\n\u0001;1:::t); SoftMax( \u0016AT\n\u0001;t+1:::t+s)\n\u0003T\n(10)\nHowever, A consists of two SoftMax distributions and is\nused in Eq. 8 without normalization. The output variance\nis then doubled and leads to poor optimization (Glorot and\nBengio 2010). It is advised to divide A by\np\n2 to preserve\nthe variance. This way resembles a single distribution. So\nwe use one SoftMax instead and this works well:\nA = SoftMax(XWq [XWk1; HWk2]T\np\nd\n) (11)\nNow, we can compute Y in Eq. 6 efﬁciently by using Eq.\n11 as well as Eq. 8 to compute Self(X) + Cross(X; H).\nCompressing Attention and FFN\nIt is natural to consider to merge the attention and FFN with\nthe same approach for further speed-up. As suggested by\nthe right part of Fig. 2, the similarities between inputs of\n6/6 9/4 12/2 14/1\n26\n27\n28\n# of Encoder Layers/# of Decoder Layers\nBLEU [%]\n100\n200\n300\nSpeed\nBLEU Speed\nFigure 4: Performance (BLEU) and translation speed (to-\nken/sec) vs. the numbers of encoder and decoder layers on\nWMT14 En-De translation task.\nthe adjacent cross-attention and FFN are low (dark diagonal\nentries). This implies that it is not ideal to make the identical\ninput assumption to parallelize the cross-attention and FFN.\nHere we provide another solution. Given that attention is\nmerely a weighted sum and FFN performs a linear projection\nﬁrst, we can merge them by exploiting the linearity. This\nway not only parallelizes the computation of attention and\nFFN but also removes redundant matrix multiplications.\nWe substitute X in Eq. 5 by Y in Eq. 6:\nYf = ReLU(XW1 + A [XWv1; HWv2] W1 + b1)W2 + b2\n(12)\nWe can combine W1 with Wv1 as well as Wv2 into\nfWv1; fWv2 2Rd\u00024d, as these matrices are learnable and ma-\ntrix multiplied together:\nYf = ReLU(XW1 + A\nh\nXfWv1; HfWv2\ni\n+ b1)W2 + b2\n(13)\nFurthermore, XW1 can be computed in parallel with other\ntransformations such as XWq.\nThis eventually gives us an more efﬁcient decoder layer\narchitecture, named Compressed-Attention. The whole com-\nputation process is shown in Fig. 3: it ﬁrst computes the at-\ntention distribution A by Eq. 11, then performs the attention\noperation via Eq. 13, and produces Yf as the ﬁnal result.\nThe proposed Compressed Attention Network (CAN) stacks\ncompressed-attentions to form its decoder. Fig. 1 shows the\ndifference between Transformer and CAN.\nBalancing Encoder and Decoder Depths\nBased on the ﬁndings of Kasai et al. (2020), we learn that\na shallow decoder could offer a great speed gain, while a\ndeep encoder could make up of the loss of a shallow decoder\nwithout adding a heavy computation overhead. Since their\nwork is based on knowledge distillation (Hinton, Vinyals,\nand Dean 2015), here we re-examine this idea under the\nstandard training setting (without knowledge distillation).\nFig. 4 shows the performance and speed if we gradually\nreduce the decoder depth while adding more encoder layers.\nWe see that although the overall number of parameters re-\nmains the same, the baseline can be 2\u0002faster without losing\nany performance (12/2 vs. 6/6). This justiﬁes the previous\n13317\nSource Lang. Train Valid Test\nsent. word sent. word sent. word\nWMT14 En$De 4.5M 220M 3000 110K 3003 114K\nEn$Fr 35M 2.2B 26K 1.7M 3003 155K\nWMT17\nEn$De 5.9M 276M 8171 356K 3004 128K\nEn$Fi 2.6M 108M 8870 330K 3002 110K\nEn$Lv 4.5M 115M 2003 90K 2001 88K\nEn$Ru 25M 1.2B 8819 391K 3001 132K\nEn$Cs 52M 1.2B 8658 354K 3005 118K\nTable 1: Data statistics (# of sentences and # of words).\nidea. We thereby choose a stronger baseline with a 12-layer\nencoder and a 2-layer decoder for a more convincing com-\nparison. This setting is also applied to CAN.\nExperiments\nExperimental Setup\nDatasets We evaluate our methods on 14 machine trans-\nlation tasks (7 datasets \u00022 translation directions each), in-\ncluding WMT14 En$fDe, Frgand WMT17 En$fDe, Fi,\nLv, Ru, Csg.\nWMT14 En$fDe, Frgdatasets are tokenized by a script\nfrom Moses1. We apply BPE (Sennrich, Haddow, and Birch\n2016) with 32K merge operations to segment words into\nsubword units. Sentences with more than 250 subword units\nare removed. The ﬁrst two rows of Table 1 are the detailed\nstatistics of these two datasets. For En-De, we share the\nsource and target vocabularies. We choosenewstest-2013 as\nthe validation set and newstest-2014 as the test set. For En-\nFr, we validate the system on the combination of newstest-\n2012 and newstest-2013, and test it on newstest-2014.\nAll WMT17 datasets are the ofﬁcial preprocessed version\nfrom WMT17 website2. BPE with 32K merge operations is\nsimilarly applied to these datasets. We use the concatena-\ntion of all available preprocessed validation sets in WMT17\ndatasets as our validation set:\n• En$De. We use the concatenation of newstest2014, new-\nstest2015 and newstest2016 as the validation set.\n• En$Fi. We use the concatenation ofnewstest2015, news-\ndev2015, newstest2016 and newstestB2016 as the valida-\ntion set.\n• En$Lv. We use newsdev2016 as the validation set.\n• En$Ru. We use the concatenation of newstest2014, new-\nstest2015 and newstest2016 as the validation set.\n• En$Cs. We use the concatenation of newstest2014, new-\nstest2015 and newstest2016 as the validation set.\nWe use newstest2017 as the test set for all WMT17 datasets.\nDetailed statistics of these datasets are shown in Table 1. For\nall 14 translation tasks, we report case-sensitive tokenized\nBLEU scores3.\n1https://github.com/moses-smt/mosesdecoder/blob/master/\nscripts/tokenizer/tokenizer.perl\n2http://data.statmt.org/wmt17/translation-task/preprocessed/\n3https://github.com/moses-smt/mosesdecoder/blob/master/\nscripts/generic/multi-bleu.perl\nSystem Test \u0001BLEU Valid Speed \u0001Speed\nEn-De\nBaseline 27.32 - 26.56 104.27 -\nBalanced 27.46 0.00 26.81 219.53 0.00%\nSAN 26.91 -0.55 26.04 229.89 +4.72%\nAAN 27.36 -0.10 26.11 233.58 +6.40%\nCAN 27.32 -0.14 26.47 290.08 +32.14%\nDe-En\nBaseline 30.50 - 30.34 103.97 -\nBalanced 30.76 0.00 30.37 206.00 0.00%\nSAN 30.09 -0.67 30.11 240.52 +16.76%\nAAN 30.15 -0.61 30.07 232.08 +12.66%\nCAN 30.37 -0.39 30.17 293.16 +42.31%\nEn-Fr\nBaseline 40.82 - 46.80 104.65 -\nBalanced 40.55 0.00 46.87 206.54 0.00%\nSAN 40.45 -0.10 46.69 208.68 +1.04%\nAAN 40.50 -0.05 46.57 210.29 +1.82%\nCAN 40.25 -0.30 46.56 263.83 +27.74%\nFr-En\nBaseline 36.33 - 47.03 105.85 -\nBalanced 36.86 0.00 46.89 201.13 0.00%\nSAN 36.73 -0.13 46.82 213.30 +6.05%\nAAN 36.52 -0.34 46.74 215.97 +7.38%\nCAN 36.67 -0.19 46.63 266.63 +32.57%\nTable 2: Comparison of BLEU scores [%] and translation\nspeeds (token/sec) of different attention models on WMT14\nEn$fDe, Frgtranslation tasks.\nModel Setup Our baseline system is based on the open-\nsource implementation of the Transformer model presented\nin Ott et al. (2019). For all machine translation tasks, the\nstandard Transformer baseline (Baseline) consists of a 6-\nlayer encoder and a 6-layer decoder. The embedding size\nis set to 512. The number of attention heads is 8. The FFN\nhidden size equals to 4\u0002embedding size. Dropout with the\nvalue of 0.1 is used for regularization. We adopt the inverse\nsquare root learning rate schedule with 8,000 warmup steps\nand 0:0007 learning rate. We stop training until the model\nstops improving on the validation set. All systems are trained\non 8 NVIDIA TITIAN V GPUs with mixed-precision train-\ning (Micikevicius et al. 2018) and a batch size of 4,096 to-\nkens per GPU. We average model parameters in the last 5\nepochs for better performance. At test time, the model is de-\ncoded with a beam of width 4 and half-precision. For an ac-\ncurate speed comparison, we decode with a batch size of 1 to\navoid paddings. The stronger balanced baseline (Balanced)\nshares the setting with this standard baseline, except that its\nencoder depth is 12 and decoder depth is 2.\nWe compare C AN and other model acceleration ap-\nproaches with our baselines. We choose Sharing Attention\nNetwork (S AN) (Xiao et al. 2019) and Average Attention\nNetwork (A AN) (Zhang, Xiong, and Su 2018) for compar-\nison, as they have been proven to be effective in various\nmachine translation tasks (Birch et al. 2018). All hyper-\nparameters of C AN, SAN and AAN are identical to the bal-\nanced baseline system. Results are the average of 3 runs.\nResults\nTable 2 shows the results of various systems on WMT14\nEn$fDe, Frg. Our balanced baseline has nearly the same\n13318\nSystem Test \u0001BLEU Valid Speed \u0001Speed\nEn-De\nBaseline 28.40 - 31.30 106.58 -\nBalanced 28.65 0.00 31.39 218.35 0.00%\nCAN 28.30 -0.35 30.94 280.57 +28.50%\nDe-En\nBaseline 34.48 - 35.36 103.04 -\nBalanced 34.38 0.00 35.16 220.05 0.00%\nCAN 33.99 -0.39 34.82 286.23 +30.07%\nEn-Fi\nBaseline 21.28 - 18.31 103.84 -\nBalanced 21.38 0.00 18.67 207.73 0.00%\nCAN 21.14 -0.24 18.19 286.36 +37.85%\nFi-En\nBaseline 25.54 - 21.32 106.59 -\nBalanced 25.63 0.00 21.29 209.88 0.00%\nCAN 25.25 -0.38 21.31 287.57 +37.02%\nEn-Lv\nBaseline 16.14 - 21.33 107.20 -\nBalanced 15.98 0.00 21.21 219.02 0.00%\nCAN 15.90 -0.08 20.75 287.33 +31.19%\nLv-En\nBaseline 18.74 - 24.79 106.25 -\nBalanced 18.69 0.00 24.54 216.06 0.00%\nCAN 18.21 -0.48 24.16 275.89 +27.69%\nEn-Ru\nBaseline 30.44 - 30.67 106.46 -\nBalanced 30.28 0.00 30.59 214.52 0.00%\nCAN 29.89 -0.39 30.28 287.13 +33.85%\nRu-En\nBaseline 34.44 - 32.39 107.24 -\nBalanced 34.24 0.00 32.22 213.78 0.00%\nCAN 33.95 -0.29 31.92 287.86 +34.65%\nEn-Cs\nBaseline 24.00 - 28.09 106.18 -\nBalanced 23.69 0.00 28.03 212.65 0.00%\nCAN 23.59 -0.10 27.71 272.37 +28.08%\nCs-En\nBaseline 30.00 - 33.01 104.00 -\nBalanced 30.06 0.00 32.86 202.96 0.00%\nCAN 29.87 -0.19 32.99 269.70 +32.88%\nTable 3: BLEU scores [%] and translation speeds (token/sec)\non WMT17 En$fDe, Fi, Lv, Ru, Csgtranslation tasks.\nperformance as the standard baseline, but its speed is 2\u0002\nfaster on average. A similar phenomenon is also observed\nfrom WMT17 experiments in Table 3. This observation in-\ndicates that existing systems do not well balance the encoder\nand decoder depths. We also report the performance of AAN,\nSAN and the proposed CAN. All three approaches have sim-\nilar BLEU scores and slightly underperform the balanced\nbaseline. C AN is more stable than the others, as its maxi-\nmum \u0001BLEU is -0.39, while SAN is -0.67 and AAN is -0.61.\nFor speeds of these systems, S AN and A AN have a similar\nlevel of acceleration (1\u001816%) over the balanced baseline.\nCAN, on the other hand, provides a higher level of accel-\neration (27\u001842%). Interestingly, we ﬁnd that the accelera-\ntion is more obvious in De-En than in others, e.g., 42% in\nDe-En and 27% in En-Fr for C AN. We ﬁnd that the length\nratio between the translation and the source sentence in De-\nEn is higher than others, e.g., 1.0 for De-En and 0.981 for\nEn-Fr. In this case the decoder tends to predict more words\nand consumes more time in De-En, and thus acceleration\napproaches that work on the decoder are more effective. In\naddition, though not reported in Table 2, we ﬁnd that ap-\nplying C AN on the standard baseline hurt the performance\nSystem Before KD After KD\nTest \u0001BLEU Test \u0001BLEU\nBalanced 27.46 0.00 27.82 0.00\nSAN 26.91 -0.55 27.76 -0.06\nAAN 27.36 -0.10 27.85 +0.03\nCAN 27.32 -0.14 28.08 +0.26\nTable 4: BLEU scores [%] of applying knowledge distilla-\ntion (KD) on WMT14 En-De translation task.\nSystem Test \u0001BLEU Speed \u0001Speed\nBalanced 27.46 0.00 219.53 0.00%\n+ Compress Attention 27.09 -0.37 263.64 +20.09%\n+ Compress FFN 27.69 +0.23 233.17 +6.21%\n+ Compress All 27.32 -0.14 290.08 +32.14%\nTable 5: Ablation study on WMT14 En-De translation task\n(Compress Attention: compress the self-attention and cross-\nattention only; Compress FFN: compress the cross-attention\nand FFN only; Compress All: compress the self-attention,\ncross-attention and FFN).\nless (-0.09 BLEU points on average) than on the balanced\nbaseline (-0.25 BLEU points on average).\nMore experimental results to justify the effectiveness of\nCAN are presented in Table 3. We evaluate the balanced\nbaseline as well as C AN on ﬁve WMT17 language pairs.\nThe results again show that the balanced baseline is indeed a\nstrong baseline with BLEU scores close to the standard base-\nline and is consistently 2\u0002 faster. CAN also shows a simi-\nlar trend that it slightly underperforms the balanced baseline\n(< 0:5 BLEU scores) but is > 27% faster.\nAnalysis\nKnowledge Distillation\nAlthough SAN, AAN and CAN offer considerable speed gain\nover the balanced baseline, they all suffer from the perfor-\nmance degradation as shown in Table 2 and Table 3. The\npopular solution to this is knowledge distillation (KD). Here\nwe choose sequence-level knowledge distillation (Kim and\nRush 2016) for better performance in machine translation\ntasks. The balanced baseline is used to generate the pseudo\ndata for KD.\nTable 4 shows that KD closes the performance gap be-\ntween the fast attention models (S AN, A AN and C AN) and\nthe balanced baseline. This fact suggests that all three sys-\ntems have enough capacity for a good performance, but\ntraining from scratch is not able to reach a good conver-\ngence state. It suggests that these systems might require a\nmore careful hyper-parameters tuning or a better optimiza-\ntion method.\nAblation Study\nTo investigate in which part CAN contributes the most to the\nacceleration as well as the performance loss, we only com-\npress the self-attention and cross-attention or compress the\ncross-attention and FFN for study. Table 5 shows the results\n13319\n1 4 16 64 256\n0\n200\n400\nBeam Size\nSpeed\nBalanced\nCAN\n10 20 30 40 50+\n50\n100\n150\n200\nLength\nSpeed\nBalanced\nCAN\nCAN SAN AAN\n26:9\n27:0\n27:1\n30:3\n30:4\n30:4\nTranslation\nLength\nEn-De En-Fr\nFigure 5: Translation speed (token/sec) vs. beam size and translation length on WMT14 En-De translation task.\nof this ablation study. We can see that compressing the two\nattentions provides a 20.09% speed-up, while only 6.21%\nfor compressing attention and FFN. This is because FFN is\nalready highly parallelized and accelerating itself does not\nbring much gain. Note that the second row of Table 5 is\nexactly Zhang, Titov, and Sennrich (2019)’s work (without\nAAN). We see that CAN is faster and performs better. On the\nother hand, compressing attentions brings the most perfor-\nmance loss, which shows that the identical input assumption\nis strong. Fig. 2 shows that inputs of the adjacent layers are\nnot very similar in lower layers. Therefore using CAN in low\nlayers might bring a great loss. We also ﬁnd that compress-\ning attention and FFN has an even better result. This might\nbe that we remove the redundant parameters in the model.\nSensitivity Analysis\nWe study how the speed could be affected by other factors\nin Fig. 5, e.g., the beam size and the translation length. The\nleft of Fig. 5 shows that C AN is consistently faster than the\nbalanced baseline with different beam size. As the accelera-\ntion provided by CAN is constantly proportional to the speed\nof the baseline, it becomes less obvious when the baseline is\nslow, i.e., translating with a large beam. An opposite trend\nhappens in the middle of Fig. 5 for the translation length.\nThis is because overheads such as data preparation domi-\nnate the translation time of short sentences. This way results\nin a slow speed even when the translation time is short. As\nboth the baseline and CAN get faster when generating longer\ntranslations, one might suspect that the superior acceleration\nof CAN over other approaches comes from the fact that CAN\ngenerates longer translations. Further analysis is conducted\nand shown in the right of Fig. 5. We see that C AN, S AN\nand A AN generate translations with similar lengths in two\nWMT14 translation tasks. This observation justiﬁes that the\nsuperior acceleration brought by CAN did come from its de-\nsign rather than translation lengths.\nError Analysis\nAs shown in Table 2 and Table 3, the acceleration of C AN\ncomes at the cost of performance. Here we conduct experi-\nments to better understand in which aspect CAN scariﬁes for\nspeed-up. We ﬁrst evaluate the sentence-level BLEU score\nfor each translation, then cluster these translations accord-\ning to their averaged word frequencies or lengths.\n6 12 18 24 30+\n35\n40\n45\nFrequency (\u0002105)\nBLEU [%]\nBalanced\nCAN\n10 20 30 40 50+\n32\n34\n36\n38\n40\nLength\nBLEU [%]\nBalanced\nCAN\nFigure 6: BLEU score [%] vs. word frequency (\u000210 5) and\ntranslation length on WMT14 En-De translation task.\nFig. 6 shows the results. The left of Fig. 6 indicates that\nCAN did well on sentences with low frequencies, but not\non those with high frequencies. The right of Fig. 6 shows\nthat CAN does not translate short sentences well but is quite\ngood at translating long sentences. These facts are counter-\nintuitive as one might expect a poor model could do well\non easy samples (high frequency and short sentences) but\nnot on hard ones (low frequency and long sentences). This\nmight due to the identical input assumptions we used to de-\nrive C AN are critical to easy samples. We left this for the\nfuture exploration.\nParallelism Study\nA simple approach to obtain a higher parallelism without\nmodifying the architecture is to increase the batch size at in-\nference. Fig. 7 compares the inference time of the balanced\nbaseline and CAN by varying the batch size. We can see that\nboth systems run faster with a larger batch size and C AN is\nconsistently faster than the balanced baseline. But the accel-\neration of C AN over the baseline \u0001Speed diminishes when\nthe batch size gets larger. In this case we observe that C AN\nreaches the highest parallelism (a nearly 100% GPU utility)\nin a smaller batch size (\u001532) than the baseline (> 64). This\nmeans that enlarging the batch size no longer provides ac-\nceleration for C AN, while the baseline can still be further\nspeeded up. We expect C AN could be faster if more tensor\ncores are available in the future.\n13320\n0 20 40 60\n0\n2;000\n4;000\n6;000\n8;000\n10;000\nBatch Size\nSpeed\n0\n10\n20\n30\n40\n50\n\u0001Speed [%]\nBalanced CAN\n\u0001Speed\nFigure 7: Speed (token/sec) and \u0001Speed [%] vs. batch size\non WMT14 En-De translation task.\nTraining Study\nWe plot the training and validation loss curve of the stan-\ndard baseline, the balanced baseline and C AN in Fig. 8 for\nstudying their convergence. We can see that all systems con-\nverge stably. The balanced baseline has a higher loss than\nthe standard baseline in both the training and validation sets,\nbut their BLEU scores are close as shown in Table 2. This\nis due to the shallow decoder in the balanced baseline. Since\nthe loss is determined by the decoder, a shallow decoder with\nless capacity would have a higher loss. Wang et al. (2019) in-\ndicates that the encoder depth has a greater impact than the\ndecoder on BLEU scores, therefore the deep encoder makes\nup the performance loss of the shallow decoder. We also see\nthat C AN has a higher loss than the balanced baseline be-\ncause we compress the decoder. Since we do not enhance\nthe encoder, the BLEU score drops accordingly.\nRelated Work\nModel Acceleration\nLarge Transformer has demonstrated its effectiveness on\nvarious natural language processing tasks, including ma-\nchine translation (Vaswani et al. 2017), language modelling\n(Baevski and Auli 2019) and etc. The by-product brought by\nthis huge network is the slow inference speed. Previous work\nfocuses on improving model efﬁciency from different per-\nspectives. For example, knowledge distillation approaches\ntreat the large network output as the ground truth to train\na small network (Kim and Rush 2016). Low-bit quantiza-\ntion approaches represent and run the model with 8-bit in-\nteger (Lin et al. 2020). Our work follows another line of re-\nsearches, which purses a more efﬁcient architecture.\nChen et al. (2018) show that the attention of Transformer\nbeneﬁts the encoder the most and the decoder could be\nsafely replaced by a recurrent network. This way reduces\nthe complexity of the decoder to linear time but incurs a\nhigh cost in training. Zhang, Xiong, and Su (2018) show\nthat the self-attention is not necessary and a simple aver-\naging is enough. Xiao et al. (2019) indicate that most atten-\ntion distributions are redundant and thus share these distri-\nbutions among layers. Kitaev, Kaiser, and Levskaya (2020)\nuse locality-sensitive hashing to select a constant number\nof words and perform attention on them. Fan, Grave, and\n0 3 6 9 12 15 18 21\n4\n6\n8\n#Epoch\nLoss\nBaseline\nBalanced\nCAN\nFigure 8: Loss vs. # of epochs on WMT14 En-De translation\ntask (solid lines are the training losses, dashed lines are the\nvalidation losses).\nJoulin (2020) train a large Transformer and drop some lay-\ners at testing for fast inference. Gu et al. (2018) use a non-\nautoregressive decoder to predict the whole sentence at one\ntime instead of generating it word by word. This approach\nmakes a linear time translation process to constant time via\nthe parallel computation. Perhaps the most related works are\nHe et al. (2018); Zhang, Titov, and Sennrich (2019). They\nmerge the self-attention and cross-attention and share their\nparameters. We, on the other hand, use different sets of pa-\nrameters for each attention and mathematically prove that\nthis way is equivalent to the standard Transformer under\nsome mild conditions. We further show that the attention and\nFFN could also be merged together due to their linearity.\nDeep Transformer\nRecent studies have shown that deepening the Transformer\nencoder is more beneﬁcial than widening the encoder or\ndeepening the decoder (Bapna et al. 2018). Wang et al.\n(2019) show that placing the layer normalization before\n(Pre-Norm) rather than behind (Post-Norm) the sub-layer al-\nlows us to train deep Transformer. Xiong et al. (2020) prove\nthat the success of the Pre-Norm network relies on its well-\nbehaved gradient. Zhang, Titov, and Sennrich (2019) suggest\nthat a proper initialization is enough to train a deep Post-\nNorm network. Kasai et al. (2020) similarly exploit this ob-\nservation but to build a faster instead of a better model. They\nshow that using knowledge distillation, a deep encoder and\nshallow decoder model could run much faster without losing\nany performance. Based on their work, we use this model as\nour baseline system and evaluate it on extensive machine\ntranslation tasks without knowledge distillation.\nConclusion\nIn this work, we propose CAN, whose decoder layer consists\nof only one attention. CAN offers consistent acceleration by\nproviding a high degree of parallelism. Experiments on 14\nWMT machine translation tasks show that C AN is 2.82\u0002\nfaster than the baseline. We also use a stronger baseline for\ncomparison. It employs a deep encoder and a shallow de-\ncoder, and is 2\u0002faster than the standard Transformer base-\nline without loss in performance.\n13321\nAcknowledgments\nThis work was supported in part by the National Sci-\nence Foundation of China (Nos. 61876035 and 61732005),\nand the National Key R&D Program of China (No.\n2019QY1801). The authors would like to thank anonymous\nreviewers for their comments.\nReferences\nBa, L. J.; Kiros, J. R.; and Hinton, G. E. 2016. Layer Nor-\nmalization. CoRR abs/1607.06450. URL http://arxiv.org/\nabs/1607.06450.\nBaevski, A.; and Auli, M. 2019. Adaptive Input Represen-\ntations for Neural Language Modeling. In 7th International\nConference on Learning Representations, ICLR 2019, New\nOrleans, LA, USA, May 6-9, 2019. OpenReview.net. URL\nhttps://openreview.net/forum?id=ByxZX20qFQ.\nBapna, A.; Chen, M. X.; Firat, O.; Cao, Y .; and Wu, Y . 2018.\nTraining Deeper Neural Machine Translation Models with\nTransparent Attention. In Riloff, E.; Chiang, D.; Hocken-\nmaier, J.; and Tsujii, J., eds., Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing, Brussels, Belgium, October 31 - November 4, 2018,\n3028–3033. Association for Computational Linguistics. doi:\n10.18653/v1/d18-1338. URL https://doi.org/10.18653/v1/\nd18-1338.\nBirch, A.; Finch, A. M.; Luong, M.; Neubig, G.; and Oda, Y .\n2018. Findings of the Second Workshop on Neural Machine\nTranslation and Generation. In Birch, A.; Finch, A. M.;\nLuong, M.; Neubig, G.; and Oda, Y ., eds., Proceedings of\nthe 2nd Workshop on Neural Machine Translation and Gen-\neration, NMT@ACL 2018, Melbourne, Australia, July 20,\n2018, 1–10. Association for Computational Linguistics. doi:\n10.18653/v1/w18-2701. URL https://doi.org/10.18653/v1/\nw18-2701.\nChen, M. X.; Firat, O.; Bapna, A.; Johnson, M.; Macherey,\nW.; Foster, G. F.; Jones, L.; Schuster, M.; Shazeer, N.; Par-\nmar, N.; Vaswani, A.; Uszkoreit, J.; Kaiser, L.; Chen, Z.;\nWu, Y .; and Hughes, M. 2018. The Best of Both Worlds:\nCombining Recent Advances in Neural Machine Transla-\ntion. In Gurevych, I.; and Miyao, Y ., eds., Proceedings\nof the 56th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2018, Melbourne, Australia, July\n15-20, 2018, Volume 1: Long Papers, 76–86. Association\nfor Computational Linguistics. doi:10.18653/v1/P18-1008.\nURL https://www.aclweb.org/anthology/P18-1008/.\nFan, A.; Grave, E.; and Joulin, A. 2020. Reducing Trans-\nformer Depth on Demand with Structured Dropout. In\n8th International Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020 .\nOpenReview.net. URL https://openreview.net/forum?id=\nSylO2yStDr.\nGlorot, X.; and Bengio, Y . 2010. Understanding the dif-\nﬁculty of training deep feedforward neural networks. In\nTeh, Y . W.; and Titterington, D. M., eds., Proceedings of\nthe Thirteenth International Conference on Artiﬁcial Intel-\nligence and Statistics, AISTATS 2010, Chia Laguna Resort,\nSardinia, Italy, May 13-15, 2010, volume 9 of JMLR Pro-\nceedings, 249–256. JMLR.org. URL http://proceedings.mlr.\npress/v9/glorot10a.html.\nGu, J.; Bradbury, J.; Xiong, C.; Li, V . O. K.; and Socher,\nR. 2018. Non-Autoregressive Neural Machine Translation.\nIn 6th International Conference on Learning Representa-\ntions, ICLR 2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Conference Track Proceedings. OpenReview.net.\nURL https://openreview.net/forum?id=B1l8BtlCb.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016a. Deep Resid-\nual Learning for Image Recognition. In 2016 IEEE Confer-\nence on Computer Vision and Pattern Recognition, CVPR\n2016, Las Vegas, NV , USA, June 27-30, 2016, 770–778.\nIEEE Computer Society. doi:10.1109/CVPR.2016.90. URL\nhttps://doi.org/10.1109/CVPR.2016.90.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016b. Iden-\ntity Mappings in Deep Residual Networks. In Leibe, B.;\nMatas, J.; Sebe, N.; and Welling, M., eds., Computer Vision\n- ECCV 2016 - 14th European Conference, Amsterdam, The\nNetherlands, October 11-14, 2016, Proceedings, Part IV ,\nvolume 9908 of Lecture Notes in Computer Science, 630–\n645. Springer. doi:10.1007/978-3-319-46493-0n\n38. URL\nhttps://doi.org/10.1007/978-3-319-46493-0n 38.\nHe, T.; Tan, X.; Xia, Y .; He, D.; Qin, T.; Chen, Z.; and\nLiu, T. 2018. Layer-Wise Coordination between Encoder\nand Decoder for Neural Machine Translation. In Bengio,\nS.; Wallach, H. M.; Larochelle, H.; Grauman, K.; Cesa-\nBianchi, N.; and Garnett, R., eds., Advances in Neural\nInformation Processing Systems 31: Annual Conference\non Neural Information Processing Systems 2018, NeurIPS\n2018, 3-8 December 2018, Montr ´eal, Canada, 7955–\n7965. URL http://papers.nips.cc/paper/8019-layer-wise-\ncoordination-between-encoder-and-decoder-for-neural-\nmachine-translation.\nHinton, G. E.; Vinyals, O.; and Dean, J. 2015. Distilling the\nKnowledge in a Neural Network. CoRR abs/1503.02531.\nURL http://arxiv.org/abs/1503.02531.\nHuang, G.; Sun, Y .; Liu, Z.; Sedra, D.; and Weinberger,\nK. Q. 2016. Deep Networks with Stochastic Depth. In Leibe,\nB.; Matas, J.; Sebe, N.; and Welling, M., eds., Computer\nVision - ECCV 2016 - 14th European Conference, Amster-\ndam, The Netherlands, October 11-14, 2016, Proceedings,\nPart IV, volume 9908 ofLecture Notes in Computer Science,\n646–661. Springer. doi:10.1007/978-3-319-46493-0n\n39.\nURL https://doi.org/10.1007/978-3-319-46493-0n 39.\nKasai, J.; Pappas, N.; Peng, H.; Cross, J.; and Smith, N. A.\n2020. Deep Encoder, Shallow Decoder: Reevaluating the\nSpeed-Quality Tradeoff in Machine Translation. CoRR\nabs/2006.10369. URL https://arxiv.org/abs/2006.10369.\nKim, Y .; and Rush, A. M. 2016. Sequence-Level Knowledge\nDistillation. In Su, J.; Carreras, X.; and Duh, K., eds., Pro-\nceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2016, Austin, Texas,\nUSA, November 1-4, 2016, 1317–1327. The Association\nfor Computational Linguistics. doi:10.18653/v1/d16-1139.\nURL https://doi.org/10.18653/v1/d16-1139.\n13322\nKitaev, N.; Kaiser, L.; and Levskaya, A. 2020. Reformer:\nThe Efﬁcient Transformer. In 8th International Conference\non Learning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net. URL https:\n//openreview.net/forum?id=rkgNKkHtvB.\nLi, B.; Wang, Z.; Liu, H.; Jiang, Y .; Du, Q.; Xiao, T.;\nWang, H.; and Zhu, J. 2020a. Shallow-to-Deep Training\nfor Neural Machine Translation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Lan-\nguage Processing, EMNLP 2020, Online, November 16-20,\n2020, 995–1005. Association for Computational Linguis-\ntics. doi:10.18653/v1/2020.emnlp-main.72. URL https:\n//doi.org/10.18653/v1/2020.emnlp-main.72.\nLi, Y .; Wang, Q.; Xiao, T.; Liu, T.; and Zhu, J. 2020b. Neu-\nral Machine Translation with Joint Representation. In The\nThirty-Fourth AAAI Conference on Artiﬁcial Intelligence,\nAAAI 2020, The Thirty-Second Innovative Applications of\nArtiﬁcial Intelligence Conference, IAAI 2020, The Tenth\nAAAI Symposium on Educational Advances in Artiﬁcial In-\ntelligence, EAAI 2020, New York, NY, USA, February 7-12,\n2020, 8285–8292. AAAI Press. URL https://aaai.org/ojs/\nindex.php/AAAI/article/view/6344.\nLin, Y .; Li, Y .; Liu, T.; Xiao, T.; Liu, T.; and Zhu, J. 2020.\nTowards Fully 8-bit Integer Inference for the Transformer\nModel. In Bessiere, C., ed.,Proceedings of the Twenty-Ninth\nInternational Joint Conference on Artiﬁcial Intelligence, IJ-\nCAI 2020, 3759–3765. ijcai.org. doi:10.24963/ijcai.2020/\n520. URL https://doi.org/10.24963/ijcai.2020/520.\nMicikevicius, P.; Narang, S.; Alben, J.; Diamos, G. F.; Elsen,\nE.; Garc ´ıa, D.; Ginsburg, B.; Houston, M.; Kuchaiev, O.;\nVenkatesh, G.; and Wu, H. 2018. Mixed Precision Train-\ning. In 6th International Conference on Learning Represen-\ntations, ICLR 2018, Vancouver, BC, Canada, April 30 - May\n3, 2018, Conference Track Proceedings. OpenReview.net.\nURL https://openreview.net/forum?id=r1gs9JgRZ.\nOtt, M.; Edunov, S.; Baevski, A.; Fan, A.; Gross, S.; Ng, N.;\nGrangier, D.; and Auli, M. 2019. fairseq: A Fast, Extensi-\nble Toolkit for Sequence Modeling. In Ammar, W.; Louis,\nA.; and Mostafazadeh, N., eds., Proceedings of the 2019\nConference of the North American Chapter of the Associa-\ntion for Computational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA, June\n2-7, 2019, Demonstrations, 48–53. Association for Com-\nputational Linguistics. doi:10.18653/v1/n19-4009. URL\nhttps://doi.org/10.18653/v1/n19-4009.\nSennrich, R.; Haddow, B.; and Birch, A. 2016. Neural Ma-\nchine Translation of Rare Words with Subword Units. In\nProceedings of the 54th Annual Meeting of the Associa-\ntion for Computational Linguistics, ACL 2016, August 7-12,\n2016, Berlin, Germany, Volume 1: Long Papers. The Associ-\nation for Computer Linguistics. doi:10.18653/v1/p16-1162.\nURL https://doi.org/10.18653/v1/p16-1162.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L.; and Polosukhin, I. 2017. At-\ntention is All you Need. In Guyon, I.; von Luxburg, U.;\nBengio, S.; Wallach, H. M.; Fergus, R.; Vishwanathan, S.\nV . N.; and Garnett, R., eds., Advances in Neural Informa-\ntion Processing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 December 2017,\nLong Beach, CA, USA, 5998–6008. URL http://papers.nips.\ncc/paper/7181-attention-is-all-you-need.\nWang, Q.; Li, B.; Xiao, T.; Zhu, J.; Li, C.; Wong, D. F.; and\nChao, L. S. 2019. Learning Deep Transformer Models for\nMachine Translation. In Korhonen, A.; Traum, D. R.; and\nM`arquez, L., eds., Proceedings of the 57th Conference of the\nAssociation for Computational Linguistics, ACL 2019, Flo-\nrence, Italy, July 28- August 2, 2019, Volume 1: Long Papers,\n1810–1822. Association for Computational Linguistics. doi:\n10.18653/v1/p19-1176. URL https://doi.org/10.18653/v1/\np19-1176.\nXiao, T.; Li, Y .; Zhu, J.; Yu, Z.; and Liu, T. 2019. Shar-\ning Attention Weights for Fast Transformer. In Kraus, S.,\ned., Proceedings of the Twenty-Eighth International Joint\nConference on Artiﬁcial Intelligence, IJCAI 2019, Macao,\nChina, August 10-16, 2019, 5292–5298. ijcai.org. doi:10.\n24963/ijcai.2019/735. URL https://doi.org/10.24963/ijcai.\n2019/735.\nXiong, R.; Yang, Y .; He, D.; Zheng, K.; Zheng, S.; Xing,\nC.; Zhang, H.; Lan, Y .; Wang, L.; and Liu, T. 2020. On\nLayer Normalization in the Transformer Architecture.CoRR\nabs/2002.04745. URL https://arxiv.org/abs/2002.04745.\nZhang, B.; Titov, I.; and Sennrich, R. 2019. Improving Deep\nTransformer with Depth-Scaled Initialization and Merged\nAttention. In Inui, K.; Jiang, J.; Ng, V .; and Wan, X., eds.,\nProceedings of the 2019 Conference on Empirical Meth-\nods in Natural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, November 3-7,\n2019, 898–909. Association for Computational Linguistics.\ndoi:10.18653/v1/D19-1083. URL https://doi.org/10.18653/\nv1/D19-1083.\nZhang, B.; Xiong, D.; and Su, J. 2018. Accelerating Neu-\nral Transformer via an Average Attention Network. In\nGurevych, I.; and Miyao, Y ., eds., Proceedings of the 56th\nAnnual Meeting of the Association for Computational Lin-\nguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018,\nVolume 1: Long Papers, 1789–1798. Association for Com-\nputational Linguistics. doi:10.18653/v1/P18-1166. URL\nhttps://www.aclweb.org/anthology/P18-1166/.\nZhang, Y .; Wang, Z.; Cao, R.; Wei, B.; Shan, W.; Zhou,\nS.; Reheman, A.; Zhou, T.; Zeng, X.; Wang, L.; Mu, Y .;\nZhang, J.; Liu, X.; Zhou, X.; Li, Y .; Li, B.; Xiao, T.; and\nZhu, J. 2020. The NiuTrans Machine Translation Systems\nfor WMT20. In Proceedings of the Fifth Conference on Ma-\nchine Translation, WMT@EMNLP 2020, Online, Novem-\nber 19-20, 2020, 338–345. Association for Computational\nLinguistics. URL https://www.aclweb.org/anthology/2020.\nwmt-1.37/.\n13323",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.736868143081665
    },
    {
      "name": "Transformer",
      "score": 0.7115172147750854
    },
    {
      "name": "Encoder",
      "score": 0.6207850575447083
    },
    {
      "name": "Computation",
      "score": 0.6049526333808899
    },
    {
      "name": "Decoding methods",
      "score": 0.49217405915260315
    },
    {
      "name": "Parallel computing",
      "score": 0.4520528018474579
    },
    {
      "name": "Computer engineering",
      "score": 0.3984071612358093
    },
    {
      "name": "Algorithm",
      "score": 0.37218111753463745
    },
    {
      "name": "Engineering",
      "score": 0.10736781358718872
    },
    {
      "name": "Voltage",
      "score": 0.10659182071685791
    },
    {
      "name": "Electrical engineering",
      "score": 0.08182412385940552
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I9224756",
      "name": "Northeastern University",
      "country": "CN"
    }
  ],
  "cited_by": 26
}