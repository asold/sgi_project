{
  "title": "A visual–omics foundation model to bridge histopathology with spatial transcriptomics",
  "url": "https://openalex.org/W4410866390",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A885599202",
      "name": "Chen Wei-qing",
      "affiliations": [
        "Houston Methodist",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2364413073",
      "name": "Zhang Pengzhi",
      "affiliations": [
        "Cornell University",
        "Houston Methodist"
      ]
    },
    {
      "id": null,
      "name": "Tran, Tu N",
      "affiliations": [
        "Houston Methodist",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2652127172",
      "name": "Xiao Yi-wei",
      "affiliations": [
        "Houston Methodist",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2359194697",
      "name": "Li, Shengyu",
      "affiliations": [
        "Cornell University",
        "Houston Methodist"
      ]
    },
    {
      "id": null,
      "name": "Shah, Vrutant V.",
      "affiliations": [
        "Houston Methodist"
      ]
    },
    {
      "id": "https://openalex.org/A2035056196",
      "name": "Cheng Hao",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": null,
      "name": "Brannan, Kristopher W.",
      "affiliations": [
        "Houston Methodist"
      ]
    },
    {
      "id": null,
      "name": "Youker, Keith",
      "affiliations": [
        "Cornell University",
        "Houston Methodist"
      ]
    },
    {
      "id": "https://openalex.org/A2140676656",
      "name": "Lai Li",
      "affiliations": [
        "Cornell University",
        "Houston Methodist"
      ]
    },
    {
      "id": null,
      "name": "Fang, Longhou",
      "affiliations": [
        "Houston Methodist",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A1935332823",
      "name": "Yang Yu",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A2165315827",
      "name": "Le Nhat Tu",
      "affiliations": [
        "Houston Methodist",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A4301516823",
      "name": "Abe Jun'ichi",
      "affiliations": [
        "The University of Texas MD Anderson Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A4300404344",
      "name": "Chen, Shu-Hsia",
      "affiliations": [
        "Houston Methodist"
      ]
    },
    {
      "id": "https://openalex.org/A2111701804",
      "name": "Ma Qin",
      "affiliations": [
        "The Ohio State University"
      ]
    },
    {
      "id": "https://openalex.org/A2039326416",
      "name": "Chen, Ken",
      "affiliations": [
        "The University of Texas MD Anderson Cancer Center"
      ]
    },
    {
      "id": "https://openalex.org/A2338862789",
      "name": "Song, Qianqian",
      "affiliations": [
        "University of Florida"
      ]
    },
    {
      "id": "https://openalex.org/A4262399447",
      "name": "Cooke, John P",
      "affiliations": [
        "Houston Methodist",
        "Cornell University"
      ]
    },
    {
      "id": "https://openalex.org/A2168252407",
      "name": "Wang Guangyu",
      "affiliations": [
        "Houston Methodist",
        "Cornell University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2912854782",
    "https://openalex.org/W4304084212",
    "https://openalex.org/W4318220659",
    "https://openalex.org/W3037083567",
    "https://openalex.org/W4312199864",
    "https://openalex.org/W4291021272",
    "https://openalex.org/W4392947521",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W4229042118",
    "https://openalex.org/W3207750165",
    "https://openalex.org/W4392947532",
    "https://openalex.org/W4385948838",
    "https://openalex.org/W4379093466",
    "https://openalex.org/W4384198290",
    "https://openalex.org/W4390725375",
    "https://openalex.org/W4386309054",
    "https://openalex.org/W2989928339",
    "https://openalex.org/W4210403261",
    "https://openalex.org/W4362521742",
    "https://openalex.org/W4385684094",
    "https://openalex.org/W4285490847",
    "https://openalex.org/W3014807564",
    "https://openalex.org/W4405114593",
    "https://openalex.org/W4386791134",
    "https://openalex.org/W2980897730",
    "https://openalex.org/W4378838672",
    "https://openalex.org/W4399387478",
    "https://openalex.org/W2951158909",
    "https://openalex.org/W2085487226",
    "https://openalex.org/W4398201291",
    "https://openalex.org/W2134236847",
    "https://openalex.org/W4280494993",
    "https://openalex.org/W4385955242",
    "https://openalex.org/W4318617280",
    "https://openalex.org/W4200565305",
    "https://openalex.org/W4402634766",
    "https://openalex.org/W4389938094",
    "https://openalex.org/W2152909439",
    "https://openalex.org/W3197498361",
    "https://openalex.org/W3135547872",
    "https://openalex.org/W2914568698",
    "https://openalex.org/W4223937445",
    "https://openalex.org/W2806857275",
    "https://openalex.org/W2998010746",
    "https://openalex.org/W3208940117",
    "https://openalex.org/W4323320361",
    "https://openalex.org/W2772283936",
    "https://openalex.org/W4399365283",
    "https://openalex.org/W2950976066",
    "https://openalex.org/W4205817967",
    "https://openalex.org/W2889213939",
    "https://openalex.org/W4280533477",
    "https://openalex.org/W2949177718",
    "https://openalex.org/W4225272983",
    "https://openalex.org/W3161009639",
    "https://openalex.org/W3129476455",
    "https://openalex.org/W4392168151",
    "https://openalex.org/W2144506857",
    "https://openalex.org/W3216067893",
    "https://openalex.org/W4285803722",
    "https://openalex.org/W4403853489",
    "https://openalex.org/W4200189327",
    "https://openalex.org/W3216310288",
    "https://openalex.org/W4396780833",
    "https://openalex.org/W4399970333",
    "https://openalex.org/W4396494945",
    "https://openalex.org/W4404824622",
    "https://openalex.org/W4393153069",
    "https://openalex.org/W4378212544",
    "https://openalex.org/W2800392236",
    "https://openalex.org/W4309989446",
    "https://openalex.org/W4290188763",
    "https://openalex.org/W4375956111",
    "https://openalex.org/W4226106494",
    "https://openalex.org/W4387310664",
    "https://openalex.org/W4316507588",
    "https://openalex.org/W4389429489",
    "https://openalex.org/W4382394552",
    "https://openalex.org/W4290989128",
    "https://openalex.org/W4384023988",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4306820534",
    "https://openalex.org/W3211074487",
    "https://openalex.org/W3159302505",
    "https://openalex.org/W3098027819",
    "https://openalex.org/W4226172937"
  ],
  "abstract": "Abstract Artificial intelligence has revolutionized computational biology. Recent developments in omics technologies, including single-cell RNA sequencing and spatial transcriptomics, provide detailed genomic data alongside tissue histology. However, current computational models focus on either omics or image analysis, lacking their integration. To address this, we developed OmiCLIP, a visual–omics foundation model linking hematoxylin and eosin images and transcriptomics using tissue patches from Visium data. We transformed transcriptomic data into ‘sentences’ by concatenating top-expressed gene symbols from each patch. We curated a dataset of 2.2 million paired tissue images and transcriptomic data across 32 organs to train OmiCLIP integrating histology and transcriptomics. Building on OmiCLIP, our Loki platform offers five key functions: tissue alignment, annotation via bulk RNA sequencing or marker genes, cell-type decomposition, image–transcriptomics retrieval and spatial transcriptomics gene expression prediction from hematoxylin and eosin-stained images. Compared with 22 state-of-the-art models on 5 simulations, and 19 public and 4 in-house experimental datasets, Loki demonstrated consistent accuracy and robustness.",
  "full_text": "Nature Methods | Volume 22 | July 2025 | 1568–1582 1568\nnature methods\nArticle\nhttps://doi.org/10.1038/s41592-025-02707-1\nA visual–omics foundation model to bridge \nhistopathology with spatial transcriptomics\n \nWeiqing Chen    1,2,12, Pengzhi Zhang    1,3,4,5,12, Tu N Tran1,3,4,5, Yiwei Xiao1,3,4,5, \nShengyu Li1,3,4,5, Vrutant V . Shah4, Hao Cheng6, Kristopher W. Brannan4, \nKeith Youker    3,5, Li Lai    3,5, Longhou Fang    3,5, Yu Yang7, Nhat-Tu Le3,5, \nJun-ichi Abe    8, Shu-Hsia Chen9, Qin Ma    6, Ken Chen    10, Qianqian Song    11, \nJohn P . Cooke2,3,4,5 & Guangyu Wang    1,3,4,5 \nArtificial intelligence has revolutionized computational biology. Recent \ndevelopments in omics technologies, including single-cell RNA sequencing \nand spatial transcriptomics, provide detailed genomic data alongside \ntissue histology. However, current computational models focus on \neither omics or image analysis, lacking their integration. T o address \nthis, we developed OmiCLIP, a visual–omics foundation model linking \nhematoxylin and eosin images and transcriptomics using tissue patches \nfrom Visium data. We transformed transcriptomic data into ‘sentences’ \nby concatenating top-expressed gene symbols from each patch. We \ncurated a dataset of 2.2 million paired tissue images and transcriptomic \ndata across 32 organs to train OmiCLIP integrating histology and \ntranscriptomics. Building on OmiCLIP, our Loki platform offers five key \nfunctions: tissue alignment, annotation via bulk RNA sequencing or marker \ngenes, cell-type decomposition, image–transcriptomics retrieval and \nspatial transcriptomics gene expression prediction from hematoxylin \nand eosin-stained images. Compared with 22 state-of-the-art models on \n5 simulations, and 19 public and 4 in-house experimental datasets, Loki \ndemonstrated consistent accuracy and robustness.\nComputational biology has advanced notably with artificial intelli -\ngence (AI) for tasks such as gene expression enhancement, single-cell \nperturbation prediction, tissue annotation, diagnosis, primary tumor \norigin predictions and image retrieval from hematoxylin and eosin \n(H&E)-stained images1–7. Recently, foundation models like CLIP8, CoCa9 \nand DeCLIP10 have been adapted to the field, fine-tuned with pathology \nimages and captions, as seen in PLIP and CONCH11,12. These visual–lan-\nguage foundation models support applications like text-to-image and \nimage-to-text retrieval, histology image classification, captioning and \ndiagnosis improvement.\nOmics data, including transcriptomics and genetics, provide \ncrucial insights into cell types in health and disease, enhancing our \nunderstanding of cellular heterogeneity, lineage tracing and disease \nmechanisms13–22. Combining omics data with histology images offers \ncomplementary information for both research and clinical applica -\ntions, and has been used for predicting cancer outcomes, prognosis \nand response to neoadjuvant chemotherapy3. However, existing meth-\nods remain task specific and lack a unified multimodal AI model to \nintegrate histology and omics data. Additionally, challenges remain in \ndeveloping infrastructure to efficiently analyze sequencing data and \npathology images together.\nT o address these gaps, we introduce omics and image pretraining, \nOmiCLIP, a transcriptomic–image dual-encoder foundation model and \nLoki platform, an infrastructure of multimodal analysis using OmiCLIP \nas a backbone. T o train OmiCLIP, we curated the ST-bank dataset with \n2.2 million tissue patches from 1,007 samples across 32 organs with \npaired whole-slide images (WSIs) and 10x Visium spatial transcriptom-\nics (ST) data. Inspired by large language model-based single-cell models \nReceived: 30 September 2024\nAccepted: 15 April 2025\nPublished online: 29 May 2025\n Check for updates\nA full list of affiliations appears at the end of the paper.  e-mail: gwang2@houstonmethodist.org\nNature Methods | Volume 22 | July 2025 | 1568–1582\n 1569\nArticle https://doi.org/10.1038/s41592-025-02707-1\ndepth, medium-to-low sequencing depth and high-to-low sequencing \ndepth. We compared similarity scores between paired images and \noriginal transcriptomic embeddings, with paired images and downsam-\npled transcriptomic embeddings. These embeddings were encoded \nusing OmiCLIP’s image and transcriptomic encoders, using PLIP and \nOpenAI CLIP as benchmarks (Extended Data Fig. 3c). Results demon-\nstrated OmiCLIP’s robustness across sequencing depths, highlighting \nits adaptability to datasets generated across different technologies.\nThe key advantage of contrastive-aligned visual–transcriptom -\nics pretraining is its unique capability to drive the development of \ncross-modality tissue analysis tools. As a proof of concept, we devel-\noped Loki, a unified AI platform for multimodal analysis. In Loki, five \nmodules were implemented, including Loki Align for multi-section \ntissue alignment, Loki Annotate for multimodal tissue annotation, Loki \nDecompose for cell-type decomposition from transcriptomics or his-\ntology, Loki Retrieve for histology image–transcriptomics retrieval and \nLoki PredEx for ST gene expression prediction from histology images \n(Fig. 1b). While these initial modules demonstrate its potential, Loki \nis designed to expand, supporting the development of more tools to \nfurther enhance multimodal tissue reconstruction and analysis. Loki \ncould serve as the infrastructure that efficiently transfers transcrip -\ntomics such as scRNA-seq, bulk RNA-seq data and even marker genes \ninto pathology image analysis via the pretrained model (OmiCLIP) \n(Fig. 1d), streamlining workflows, accelerating analysis and minimiz-\ning sequencing cost in research areas such as three-dimensional (3D) \ntissue studies and pathology diagnosis.\nOmiCLIP improves image and transcriptomics representations\nOmiCLIP’s image embeddings capture the morphology of tissues, while \nits transcriptomic embeddings represent genomic characteristics. \nSince OmiCLIP includes both transcriptomics and image encoders, \nhere we evaluated whether contrastive learning enhances the ability \nof each encoder to represent tissue types better than the initial encod-\ners. T o assess clustering performance, we moved beyond qualitative \nvisualizations and introduced quantitative metrics to assess the quality \nof the clustering. The uniform manifold approximation and projection \n(UMAP) visualizations showed that both embeddings clustered similar \ntissue types (Extended Data Fig. 2); however, the results were limited \nin their ability to quantify clustering quality and may have appeared \nunstable in some cases. Therefore, we computed the Calinski–Harabasz \n(CH) score29, a widely used clustering validation metric, which bal -\nances the dispersion between clusters with the cohesion within clusters \n(Methods). Higher CH scores reflect better clustering performance by \nindicating more distinct and internally consistent clusters.\nFirst, we calculated CH scores across 95 tissue samples from the \nST-bank dataset, which included expert-annotated cell types from \nbreast, healthy heart, kidney cancer and lung tissues and heart tissue \nwith myocardial infarction (Supplementary Table 2). These annotated \ncell types served as ground-truth cluster labels. Our results showed a \nsignificant increase (P value < 0.001; Extended Data Fig. 1) in CH scores \nfor embeddings after contrastive learning compared to before, dem-\nonstrating improved clustering performance.\nSecond, we expanded the CH score calculations to the rest of the \nST-bank samples, where no cell-type annotations are directly available. \nFor these samples, the clusters were identified by the Leiden algorithm \non the ST (Methods). After contrastive learning, CH scores significantly \nincreased in all organ types (P  value < 0.05; Extended Data Fig. 2). \nOmiCLIP’s image embeddings also outperformed SOTA models like \nUNI7 and GigaPath30 by aligning image and transcriptomic data, not \njust image–image interactions. The results demonstrated OmiCLIP’s \nability to capture tissue heterogeneity.\nLoki Align aligns ST-to-ST and H&E image-to-ST data\nResearchers recently began investigating spatial biology in 3D, reveal-\ning new insights into tissue organization and cellular interactions. \nlike GenePT23 and Cell2Sentence24, we represented transcriptomics of \na tissue patch by a ‘sentence’ of top-ranking highly expressed genes, \nseparated by spaces (‘ ’). Using this large-scale set of transcriptomics–\nhistology image pairs, we trained the CLIP-based foundation model, \nintegrating both genomic and image data. Building upon OmiCLIP, \nthe Loki platform offers five core functions: tissue alignment, tissue \nannotation, cell-type decomposition, image–transcriptomics retrieval \nand ST gene expression prediction (Fig. 1). Loki provides several distinc-\ntive features, including aligning H&E images with ST data, annotating \ntissue H&E images based on bulk RNA sequencing (RNA-seq) or marker \ngenes and decomposing cell types from H&E images with reference to \nsingle-cell RNA sequencing (scRNA-seq). We evaluated Loki’s functions \nagainst 22 state-of-the-art (SOTA) methods on 5 simulation datasets, 19 \npublicly available experimental datasets and 4 in-house experimental \ndatasets, showing Loki’s consistent accuracy and robustness across \ntasks. We also investigated OmiCLIP’s embeddings for clustering and \nannotating scRNA-seq data and predicting The Cancer Genome Atlas \n(TCGA) participants’ risk levels (Supplementary Notes 1 and 2).\nResults\nLoki platform powered by contrastive-aligned visual–omics\nTranscriptomics provides insights into cellular diversity within tis-\nsues, making it a natural indicator of tissue diversity25. ST technologies \nbridge histopathology images and transcriptomics data, enabling the \ndevelopment of a foundation model that integrates both. We intro-\nduce OmiCLIP, a visual–transcriptomics foundation model trained \non ST-bank, which includes diverse histopathology images and over  \n2.2 million paired transcriptomics from 113 studies (Fig. 1a –c and \nSupplementary Table 1). ST-bank covers 32 organ types, including \nconditions like health, cancer, heart failure and Alzheimer’s disease \n(Fig. 1b,c). We applied a quality-control pipeline to retain ST data with \nhigh-resolution H&E images. As the batch effects may strongly affect \nthe generalization ability of the model, the adopted rank-based strat-\negies inspired by recent single-cell foundation models such as Gen-\neFormer26 and scFoundation27 successfully eliminate batch effects \nthrough rank-based approaches rather than relying directly on raw \nread counts or normalized gene expression values. Specifically, we \nstandardized text descriptions of the associated images by convert-\ning all Ensembl gene IDs to gene symbols and removing housekeeping \ngenes. T o format transcriptomics for language models, genes symbols \nwere ranked from high to low by expression levels and structured into \nsentences for the text encoder (Fig. 1a).\nOmiCLIP was fine-tuned using CoCa 9, a SOTA visual–language \nfoundation framework, comprising an image encoder, a text encoder \nand a multimodal fusion decoder. The image and transcriptomics \nmodalities were aligned in a common representation space utilizing \ncontrastive learning (Fig. 1a and Extended Data Figs. 1 and 2). In this \ndual-modality space, paired image and transcriptomic embedding \nvectors were optimized to be similar.\nT o evaluate OmiCLIP’s reliability to image quality variability across \nsamples due to technological limitations, we simulated low-quality H&E \nimages by adding Gaussian noise and compared the similarity scores \nbetween the paired transcriptomic and original image embeddings, \nwith paired transcriptomic and simulated low-quality image embed-\ndings, which were encoded by OmiCLIP’s image and transcriptomic \nencoders. PLIP and OpenAI CLIP served as benchmarks (Extended \nData Fig. 3a,b), and results demonstrated that OmiCLIP is robust to \nvariations in image quality.\nFor sequencing depth variability across technologies, we first ana-\nlyzed the sequencing depth ranges in ST-bank and categorized samples \ninto high, medium and low sequencing depth groups, identified as \n11,792 unique molecular identifier (UMI) counts, 4,512 UMI counts and \n615 UMI counts, respectively. Second, we generated low sequencing \ndepth ST simulations using the downsampling function implemented \nin scuttle28. We evaluated transitions from high-to-medium sequencing \nNature Methods | Volume 22 | July 2025 | 1568–1582 1570\nArticle https://doi.org/10.1038/s41592-025-02707-1\na\nb\nSpinal cord\n110,000\nBrain\n297,000\nSkin\n185,000\nBreast\n194,000\nLiver 166,000\nColon 71,000 Prostate\n123,000\nKidney\n166,000\nHeart\n217,000\n2.2 million pairs\nTranscriptomics\nBrain\nHeart\nBreast\nSkin\nLiver\nKidney\nEmbryo\nProstate\nSpinal cord\nColon\nPancreas\nOvary\nUterus\nIntestine\nAdipose\nOthers\nStomach\nTonsil\nLung\nSimilarity\nTissue\nDisease\nStudy\nH&E images\nNormal\nCancer\nSkin disease\nHeart failure\nHCM\nAlzheimer’s disease\nDead brain\nDental disease\nAcute kidney injury\nDiabetes\nSteatotic liver\nParkinson’s disease\nHBV\nProstatic hyperplasia\nTonsillitis\nSleep apnea\nSquamous papillomas\nGranulomatous myositis\nDisease\nHigh\nLow\nOmiCLIP\nfoundation\nmodel\nc. Tissue \nannotation\nb. Cell-type \ndecomposition\n1. scRNA-seq to \nH&E image\n2. scRNA-seq to ST\n1. Annotation by \nbulk RNA-seq \nd. Histology image–transcriptomics \nretrieval\ne. ST gene expression prediction \nby H&E image \na. Tissue \nalignment 2. ST to ST \nalignment\n1. H&E image to \nST alignment\nText \nencoder\nImage \nencoder\nContrastive\nlearning\nVisual–transcriptomic embeddings\nMYH7 TNNT2 ACTC1 NPPA … CATA4\nPPARG SREBF1 … ADIPOQ\nSpot1Spot2Spot3Spot4\nLoki platform\nTissue alignment\nVisium\nH&E image\nBulk RNA-seq\nMarker genes\n’MYH7 TNNT2 … CATA4’\nd\nscRNA-seq Transfer learning\nc\n2. Annotation by \nmarker genes\nExon ExonIntron\n30\n20\n10\n–10\n–20\n–30\n–30 –20 –10 0 10\nt-SNE1\nt-SNE2\n20 30\n0\n(Spot1) ‘SNAP25 ENO2 CKB GRIN2C CAMK4 … MTOR VPS13D‘\n(Spot2) ‘KLK3 KLK2 TFF3 SPON2 FABP5 … FOS NDUFA13‘\n(Spot3) ‘S100A6 PRDX5 KRT8 PHGR1 TIMP1 … SNHG6 ZNF706‘\n(Spot4) ‘COL1A1 SPARC COL1A2 COL3A1 … FOSL2 TMEM127‘\nFig. 1 | Overview of the study. a, The workflow of pretraining the OmiCLIP \nmodel with paired image–transcriptomics dataset via contrastive learning. \nb, Workflow of the Loki platform using the OmiCLIP foundation model as \nan engine. Left diagram illustrates the size of the training data in different \norgans. Right diagram lists the existing modules of the Loki platform, \nincluding tissue alignment, cell-type decomposition, tissue annotation, ST \ngene expression prediction and histology image–transcriptomics retrieval. \nCreated in BioRender.com. c, The heat map represents image embeddings \nand transcriptomic embeddings similarity across various organs and disease \nconditions. The color of the heat map reflects the OmiCLIP’s embedding \nsimilarities, with red indicating high similarity and blue indicating low \nsimilarity. HCM, hypertrophic cardiomyopathy; HBV, hepatitis B virus \ninfection. d, Schematic illustration of Loki platform with transfer learning for \n3D tissue analysis. Created in BioRender.com.\nNature Methods | Volume 22 | July 2025 | 1568–1582\n 1571\nArticle https://doi.org/10.1038/s41592-025-02707-1\nThis requires tools to align multiple H&E images or ST sections, and \neven cross-align H&E images with ST slides. However, spatial distor-\ntions and biological variations between sections make alignment chal-\nlenging. T o address this, we developed the module Loki Align to align \nST-to-ST data, H&E image-to-H&E image, and H&E image-to-ST data. \nLoki Align first embeds patch-level transcriptomics or H&E images into \na 768-dimension space using OmiCLIP, and then applies the adapted \ncoherent point drift (CPD) method31 to align two embeddings, pre-\nserving probability distribution and topology (Fig. 2a and Methods). \nWe evaluated Loki Align on four datasets including two simulation \ndatasets, a set of eight adjacent small intestine tissue sections, and a \nset of two adjacent ovarian carcinosarcoma sections. T o ensure com-\npatibility with datasets that may not be represented in the ST-bank, we \nused fine-tuning as a default setting for the Loki Align in the alignment \ntasks. Fine-tuning minimized contrastive loss between image embed-\ndings and the paired text embeddings of the top-expressed gene name \nsentence (Methods). We further evaluated the zero-shot performance \non an ovarian carcinosarcoma dataset.\nFirst, we simulated paired H&E images and ST data by perturb -\ning gene expression and spatial locations with varying noise levels, \ncovering diverse tissue types and disease types (Methods). We meas-\nured the distance between Loki-aligned data and the ground truth, \nand compared Loki Align with PASTE and GPSA, which are designed \nfor ST section alignment 32,33. At both high and low noise levels, Loki \nST-to-ST alignment and Loki image-to-ST alignment ranked first and \nsecond, respectively, among the four methods (Fig. 2b), significantly \noutperforming PASTE and GPSA (P values < 0.001, Wilcoxon test). This \nsuperiority likely stems from PASTE’s design for linear transforma -\ntions, which maintains topological integrity but struggles with spatial \nwarping32, while GPSA aims to map readouts to a common coordinate \nsystem, risking topological fidelity33.\nSecond, we tested Loki Align on eight adjacent human small intes-\ntine tissues sections34. Real-world datasets often present challenges \ndue to distortions such as rotation, tilt, uneven slicing and missing \nfragments. For better performance, we fine-tuned OmiCLIP using \nthe target slide’s H&E image and ST data. We aligned seven source \nST datasets to target ST data and seven source H&E images to target \nST data using Loki Align and applied PASTE and GPSA to align seven \nsource ST datasets to target ST data. Loki Align successfully aligned all \nsource sections to the target section. T o evaluate the performance, we \ncalculated the Pearson correlation coefficient (PCC) and Kendall’s tau \ncoefficient. For ST-to-ST scenarios, we compared the aligned ST data \nand the target ST data. For image-to-ST scenarios, after aligning the \nH&E image to the target ST dataset, we compared the paired ST data \ncorresponding to the H&E image with the target ST dataset. The median \nPCC for Loki’s image-to-ST and ST-to-ST alignment ranged from 0.67 to \n0.80 and 0.62 to 0.83, respectively (Fig. 2c). The median Kendall’s tau \ncoefficient ranged from 0.16 to 0.27 for Loki’s image-to-ST and 0.18 to \n0.27 for ST-to-ST alignment (Supplementary Fig. 1a). On the vertical \nplane, Loki correctly aligned the same tissue types by image-to-ST and \nST-to-ST alignment, while PASTE and GPSA twisted the tissues. PASTE \nrotated three source sections (sources 1–3; Fig. 2c ) and the PCC and \nKendall’s tau coefficient ranged from −0.25 to 0.39 and −0.06 to 0.13, \nrespectively. GPSA found common coordinates in six of the seven slices \nbut introduced tremendous distortions, resulting in a PCC of 0.27 to \n0.56 and Kendall’s tau coefficient of 0.06 to 0.13. Overall, Loki ST-to-ST \nand image-to-ST alignments outperformed the SOTA methods. T o \nisolate the contributions of OmiCLIP embeddings versus the superior \nregistration method (CPD), we applied CPD to both OmiCLIP embed-\ndings and transcriptomic embeddings that was reduced to two prin-\ncipal components using principal component analysis (PCA; Fig. 2c). \nOmiCLIP embeddings significantly improved the performance of align-\nment compared to PCA embeddings (P value < 0.001, Wilcoxon test).\nThird, we assessed Loki Align’s performance on two adjacent \nhuman ovarian carcinosarcoma sections35 (Fig. 2d). With fine-tuning, \nLoki’s ST-to-ST and image-to-ST achieved the best performance, with \nmedian PCCs of 0.88 and 0.86, and Kendall’s tau coefficients of 0.21 and \n0.18, respectively. PASTE, GPSA and CAST36 had median PCCs of 0.26, \n0.43 and 0.71 and median Kendall’s tau coefficients of 0.03, 0.04 and \n0.09, respectively (P value < 0.01; Fig. 2e and Supplementary Fig. 1b). \nThe spatial expression patterns of representative genes are shown in \nSupplementary Fig. 2.\nFourth, we evaluated Loki Align on a human breast cancer dataset37 \nwith paired 10x Visium and Xenium slides (Extended Data Fig. 4). We \ngenerated simulation data by performing rotation and translation \nof Xenium data. T o perform the alignment, we first calculated tran -\nscriptomic embeddings for the Visium slide using gene sentences \nderived from Visium transcriptomic data. For the Xenium slide, we \ncreated pseudo-Visium data by averaging gene expression values \nacross pseudo-spots. These pseudo-Visium data were then used to \ncalculate transcriptomic embeddings via the transcriptomic encoder \nof OmiCLIP. Finally, Loki Align was applied to align the transcriptomic \nembeddings of the Xenium slide with those of the Visium slide, with \nperformance measured by the mean distance between the aligned \nand target spots. The resulting distance between the aligned Xenium \nslide and the target Visium slide was 0.08 mm, demonstrating that Loki \nAlign effectively aligns Visium and Xenium slides with high precision.\nFifth, we evaluated the performance of three training strategies: \npretraining plus fine-tuning, pure pretraining and pure training from \nscratch on ovarian carcinosarcoma samples (Supplementary Fig. 3). \nThe best performance was achieved with pretraining plus fine-tuning, \nresulting in a median PCC of 0.86 and a Kendall’s tau coefficient of 0.17. \nPure pretraining showed comparable performance, with a median \nPCC of 0.85 and a Kendall’s tau coefficient of 0.18. In contrast, training \nFig. 2 | Tissue alignment. a, Schematic illustration of tissue alignment using ST \nand histology image with Loki Align. Created in BioRender.com. b, Performance \ncomparison of tissue alignment on 100 low-noise and 100 high-noise simulated \ndatasets, represented by the distance between ground truth and aligned \nsimulated sample using Loki (ST-to-ST and image-to-ST) and baseline methods \nPASTE (ST-to-ST) and GPSA (ST-to-ST), respectively. P values were calculated \nusing a one-sided Wilcoxon test. c, Alignment results on eight adjacent normal \nhuman small intestine samples using Loki (ST-to-ST and image-to-ST) and \nbaseline methods PASTE (ST-to-ST), GPSA (ST-to-ST) and CPD (ST-to-ST), \nrespectively. We colored the samples using the top three PCA components of \nOmiCLIP transcriptomic embeddings, mapped to red, green and blue color \nchannels, respectively. For visualization, we stacked the eight samples together \nalong the perpendicular axis before and after different alignment methods, \nrespectively, and visualized from the side view. The source2 that has no spatial \nvariable gene selected by GPSA to run it is marked as ‘not applicable’ (NA). \nBox plots show the comparison of tissue alignment performances on these \nseven source samples respectively and combined, represented by the PCC \n(and Kendall’s tau coefficient in Supplementary Fig. 1) of highly variable gene \nexpression between target and source samples after alignment at the same \nlocation, using Loki and baseline methods (PASTE, GPSA and CPD using PCA \nembeddings as input), respectively. In the box plots, the middle line represents \nthe median, the box boundaries indicate the interquartile range, and the whiskers \nextend to data points within 1.5 times the interquartile range. d, Tissue alignment \nof two adjacent human ovarian carcinosarcoma samples using Loki (ST-to-ST and \nimage-to-ST) and baseline methods PASTE (ST-to-ST), GPSA (ST-to-ST) and CAST \n(ST-to-ST), respectively. We colored the samples as described in c. e, Alignment \nperformance comparison using PCC and Kendall’s tau coefficient of the highly \nexpressed gene expression between the target sample and the source sample at \naligned locations, using Loki (ST-to-ST and image-to-ST) and baseline methods \nPASTE (ST-to-ST), GPSA (ST-to-ST) and CAST (ST-to-ST), respectively. In the box \nplots, the middle line represents the median, the box boundaries indicate the \ninterquartile range, and the whiskers extend to data points within 1.5 times the \ninterquartile range; n = 147.\nNature Methods | Volume 22 | July 2025 | 1568–1582 1572\nArticle https://doi.org/10.1038/s41592-025-02707-1\n1.00\n0.75\n0\n–0.25\n0.50\n0.25\n–0.50\nba\nTarget\nSource\nd\nAlign two modalities by CPD \nOmiCLIP \nimage encoder\nOmiCLIP \ntext encoder\nPASTE ST to ST\nP = 2.9 × 10\n–34\nLoki\nBaseline\nLoki\nBaseline\nLoki\nBaseline\nDistance (mm)\nHigh-noiseLow-noise\nST to ST\nGPSA ST to ST\nImage to ST\nP = 5.0 × 10\n–5\nP = 1.5 × 10\n–34\nP = 1.1 × 10\n–8\nSimulation\nPASTE ST to ST\nST to ST\nGPSA ST to ST\nImage to ST\nLoki image to ST\nLoki\nST to ST\nLoki\nimage to ST\nPASTE\nST to ST\nGPSA\nST to ST\nBefore\nalignment\nTarget Source1 Source2 Source3 Source4 Source5 Source6 Source7All samples\n(side view)\nNA\n2 mm\nTargetSource\nc\nPASTE ST to STGPSA ST to ST\nImage to ST\nST to ST\nCPD (PCA) ST to ST\nPCC\nST1 (source) ST2 (target) Loki\nimage to ST\nGround truth \n(manual alignment)\nPCC\nKendall’s tau\nPASTE GPSA\n2 mm\nPASTE ST to STGPSA ST to ST\nImage to ST\nST to ST\nCPD (PCA) ST to ST\nPASTE ST to STGPSA ST to ST\nImage to ST\nST to ST\nCPD (PCA) ST to ST\nPASTE ST to STGPSA ST to ST\nImage to ST\nST to ST\nCPD (PCA) ST to ST\nPASTE ST to STGPSA ST to ST\nImage to ST\nST to ST\nCPD (PCA) ST to ST\nPASTE ST to STGPSA ST to ST\nImage to ST\nST to ST\nCPD (PCA) ST to ST\nPASTE ST to STGPSA ST to ST\nImage to ST\nST to ST\nCPD (PCA) ST to ST\nPASTE ST to STGPSA ST to ST\nImage to ST\nST to ST\nCPD (PCA) ST to ST\nPASTE ST to STGPSA ST to ST\nImage to ST\nST to ST\nCAST ST to ST\n2 mm\n2 mm\n2 mm 2 mm\n2 mm\nCAST\n2 mm\ne\nPASTE ST to STGPSA ST to ST\nImage to ST\nST to ST\nCAST ST to ST\n1.0\n0.5\n0\n–0.5\n1.00\n0.75\n0\n–0.25\n0.50\n0.25\n–0.50\n–0.75\n1.00\n0.75\n0\n–0.25\n0.50\n0.25\n–0.50\n–0.75\n0.4\n1.0\n0.8\n0.2\n0\n0.6\n–0.2\n–0.4\n0.4\n1.0\n0.8\n0.2\n0\n0.6\n–0.2\n0.4\n1.0\n0.8\n0.2\n0\n0.6\n–0.2\n–0.4\n1.00\n0.75\n0\n–0.25\n0.50\n0.25\n–0.50\n1.00\n0.75\n0\n–0.25\n0.50\n0.25\n–0.50\n0.6\n0.5\n0.2\n0.1\n0.4\n0.3\n0\n–0.1\n3.0\n2.5\n2.0\n1.5\n1.0\n0.5\n0\nNature Methods | Volume 22 | July 2025 | 1568–1582\n 1573\nArticle https://doi.org/10.1038/s41592-025-02707-1\nfrom scratch exhibited the lowest performance, with a median PCC of \n0.53 and a Kendall’s tau coefficient of 0.06. Overall, we recommend \nfine-tuning as a default setting for Loki Align, as it ensures compatibility \nwith datasets underrepresented in the ST-bank.\nLastly, we examined whether Loki Align could leverage both \nmodalities simultaneously for alignment over a single modality. T o \nevaluate this, we integrated image embeddings and transcriptomic \nembeddings by averaging them and used the combined embeddings \nto align two adjacent ovarian carcinosarcoma samples. We then calcu-\nlated the PCC and Kendall’s tau coefficient for the image embeddings, \ntranscriptomic embeddings and averaged embeddings to assess per-\nformance (Supplementary Fig. 4). The results indicated that the aver-\naged embeddings did not outperform single-modality embeddings. \nAltogether, by addressing spatial distortions and biological variability, \nLoki Align enables the accurate alignment of multiple H&E images and \nST sections, thereby supporting advanced 3D reconstructions of tissue \norganization, particularly for cross-modality studies that combine \nH&E images and ST data.\nLoki Annotate deciphers H&E images with bulk RNA-seq data\nNext, we evaluated Loki’s capability to analyze H&E images using \nbulk RNA-seq data, which is commonly used in both basic research \nand clinical practice. During OmiCLIP pretraining, the cosine simi -\nlarities between paired ST and histology images were maximized, \nallowing the similarity between the H&E image of tissue patches \nand tissue-type-specific bulk RNA-seq data to indicate tissue-type \nenrichment. We developed Loki Annotate to annotate H&E images \nusing tissue-type-specific bulk RNA-seq data as a reference. We used \nOmiCLIP to encode tissue patches from a WSI and the tissue-specific \nbulk RNA-seq data, then calculated the cosine similarity between the \nencoded embeddings (Fig. 3a). Higher similarity values indicate greater \npresence of the tissue type.\nWe evaluated Loki Annotate on breast cancer, normal breast, \nand heart failure tissues. In three breast cancer tissues, H&E regions \ncorresponding to tumor tissue showed high similarity with the bulk \nRNA-seq data from tumor biopsies, which include tumor-related mark-\ners such as COL1A1 (ref. 38) and ACTB 39 (Fig. 3b and Supplementary \na\nOmiCLIP \nimage encoder\nOmiCLIP \ntext encoder\nBulk RNA-seq from\ndiﬀerent tissue types\nb\nSimilarity heat map of diﬀerent tissue types\nTumor\nT cells\nFibroblast\nAdipose\nSmooth muscle\nStroma\nTumor similarity\nLow HighBreast cancer\n2 mm\n2 mm\n2 mm\nP value < 0.01\nP value < 0.01\nP value < 0.01\nCLAM attention\nLow High Heart failure\n1 mm\nP value < 0.01\nP value < 0.01\nP value < 0.01\nFibroblast similarity\nLow High\n1 mm\n1 mm\nNormal breast\nP value < 0.01\nP value < 0.01\nP value < 0.01\nAdipose similarity\nLow High\n1 mm\n1 mm\n1 mm\nFig. 3 | Tissue annotation using bulk RNA-seq data. a, Schematic illustration \nof tissue annotation using H&E image and reference bulk RNA-seq data from \ndifferent sources, with OmiCLIP paired image and transcriptomic embeddings. \nb, Histology WSIs of breast cancer, heart failure and normal breast samples. The \nmajor tumor regions, fibroblast cell-enriched regions and adipose regions are \nannotated by pathology experts in black lines. Heat map shows the similarity \nof WSIs to the corresponding reference bulk RNA-seq of tumor, fibroblast and \nadipose, respectively. The color of the heat map reflects the similarities between \nWSIs and reference bulk RNA-seq data, with red indicating high similarity and \nblue indicating low similarity. CLAM attention heat maps were generated using \nCLAM with default parameters.\nNature Methods | Volume 22 | July 2025 | 1568–1582 1574\nArticle https://doi.org/10.1038/s41592-025-02707-1\na\nCRC7K WSSS4\nLUAD\nPatch\nCamelyon LC25000\nLoki 0.59 0.79 0.60 0.96\nPLIP 0.50 0.78 0.58 0.93\nLoki and\nPLIP 0.72 0.83 0.62 0.97\nb c\nADI\nTUM\nMUS\n0.42 0.17\n0.41 0.22\n0.43 0.21\nLoki OpenAI CLIP\nSimilarity\n(smooth muscle)\nOmiCLIP \nimage encoder\nPLIP \nimage encoder\nOmiCLIP\ntext encoder\n0.5\n0.2\n0.1\nSimilarity\nPLIP \ntext encoder\n0.4\n0.3\n0.1‘An H&E image patch of smooth muscle’\n‘An H&E image patch of adipose’\n‘An H&E image patch of tumor’\nd e\nf\nF1 score\nCRC7K WSSS4LUAD PatchCamelyon LC25000\nLoki\nOpenAI CLIP\nADI DEB LYM MUC MUS NOR STR TUM\nOpenAI CLIP\nLoki\nPLIP\nLoki & PLIP\nLoki and PLIPLoki PLIP\nF1 score\nPredicted label Predicted label Predicted label\nTrue label\nTrue label\nTrue label\n0.8\n0.6\n0.4\n0.2\n0\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nADI\nDEB\nLYM\nMUC\nMUS\nNOR\nSTR\nTUM\nADI\n1,200\n1,000\n800\n600\n400\n200\n0\n1,200\n1,000\n800\n600\n400\n200\n0\n800\n600\n400\n200\n0\nDEB\nLYM\nMUC\nMUS\nNOR\nSTR\nTUM\nADI\nDEB\nLYM\nMUC\nMUS\nNOR\nSTR\nTUM\nADI DEB LYM MUC MUS NOR STR TUMADI DEB LYM MUC MUS NOR STR TUM ADI DEB LYM MUC MUS NOR STR TUM\n1,337 0 0 1 0 0 0 0 906 120 0 3 0 0 309 0 1,206 35 0 2 0 0 95 0\n1 168 1 29 27 0 3 110\n6 1 556 1 0 3 43 24\n44 19 3 387 55 0 525 2\n0 0 0 2 550 0 30 10\n5 0 0 35 0 352 17 332\n2 1 0 8 162 0 186 62\n5 0 0 49 2 29 7 1,141\n134 1 1 6 00 197 0\n0 340 0 0 00 294 0\n0 0 2 371 00 218 1\n0 0 32 0 3311 181 196\n0 0 2 104 00 314 1\n1 0 15 7 300 772 408\n3 0 20 27 01 984 0\n3810 0 81 62 0 0 148\n1080 388 0 0 53 0 103\n11229 13 667 93 16 0 6\n033 3 0 533 0 0 23\n044 0 58 2 112 0 525\n1589 3 29 146 0 4\n07 1 41 19 179 0\n135\n986\n‘TP53 ERCAM KRAS ... DSP ’ (tumor)\n‘FABP4 ADIPOQ LEP ... ADIPOR2’ (adipose)\n‘MYL6 MYL9 TPM2 ... CASQ2’\n‘FABP4 ADIPOQ LEP ... ADIPOR2’\n‘TP53 EPCAM KRAS ... DSP ’\n‘MYL6 MYL9 TPM2 ... CASQ2’\nFig. 4 | Tissue annotation using marker genes. a, Schematic illustration of tissue \nannotation using H&E image and reference marker genes. The annotation result \nis decided by choosing the candidate texts with the highest similarity score to the \ninput image query. For Loki, we used the text content of marker gene symbols of \neach tissue type. For the PLIP model, we used the text content of natural language \ndescription of each tissue type. b, Examples of similarity scores of images and \ntexts calculated by Loki and OpenAI CLIP model, respectively. c, Comparison \nof zero-shot performances, represented by weighted F1 scores, across four \ndatasets using Loki and OpenAI CLIP, respectively. Number of test samples for \neach dataset: CRC7K (n = 6,333); WSSS4LUAD (n = 10,091); LC25000 (n = 15,000); \nand PatchCamelyon (n = 32,768). d, Comparison of zero-shot performances, \nrepresented by weighted F1 scores, across four datasets using Loki, PLIP and \nincorporating Loki and PLIP models by average similarity (shown in a; Methods), \nrespectively. e, Comparison of zero-shot performances, represented by \nweighted F1 scores of each tissue type in the CRC7K dataset using OpenAI CLIP \nmodel, Loki, PLIP model and incorporating Loki and PLIP models, respectively. \nf, Confusion matrix of the CRC7K dataset using Loki (left), PLIP model (middle) \nand incorporating Loki and PLIP models (right), respectively. The ground-truth \nlabels are presented in rows and the predicted labels are presented in columns. \nADI, adipose tissue; NOR, normal colon mucosa; TUM, colorectal carcinoma \nepithelium; LYM, lymphocytes; MUC, mucus; DEB, debris; MUS, smooth muscle; \nSTR, cancer-associated stroma.\nNature Methods | Volume 22 | July 2025 | 1568–1582\n 1575\nArticle https://doi.org/10.1038/s41592-025-02707-1\nFig. 5). Similarity scores within the tumor regions were significantly \nhigher than those outside (P  value < 0.05, Wilcoxon test). Addition-\nally, higher similarity scores were consistent with higher diagnos -\ntic values of tumors calculated by clustering-constrained-attention \nmultiple-instance learning40 (CLAM, a SOTA WSI tumor analysis model; \nFig. 3b). Next, we tested the similarity between H&E images of heart \nfailure tissues and fibroblast RNA-seq data, as well as between H&E \nimages of normal breast tissues and adipose RNA-seq data. The simi-\nlarity scores in the corresponding pathology annotated regions were \nremarkably higher than the non-corresponding regions (Fig. 3b  and \nSupplementary Fig. 5). In summary, Loki Annotate effectively anno -\ntates H&E images by using tissue-type-specific bulk RNA-seq data as \na reference.\nLoki Annotate annotates H&E images based on marker genes\nWhen bulk RNA-seq is unavailable, Loki Annotate can also annotate \ntissues using predefined marker genes, similar to the workflow of using \nbulk RNA-seq data without fine-tuning. We created tissue-specific \ngene lists using well-established markers, such as ‘TP53, EPCAM, KRAS, \n…, DSP’ for tumors (Fig. 4a and Supplementary Table 3). As with the \nbulk RNA-seq approach, we used OmiCLIP to encode tissue patches \nfrom histology images and the gene name sentence composed from \nthe marker gene list. We applied Loki Annotate to four benchmark \nhistopathology datasets including CRC7K41 (eight tissue types), WSSS-\n4LUAD42 (normal and tumor), PatchCamelyon43 (normal and tumor) and \nLC2500044 (benign and malignant). Tissue-type annotation was deter-\nmined by cosine similarity derived from the dot product of normalized \ntext embeddings and H&E image embeddings, with the highest cosine \nsimilarity score assigned as the predicted tissue to the query image. \nBased on these annotations, precision was defined as the proportion \nof correctly predicted tissues (true positives) of all predicted tissues, \nwhile recall was defined as the proportion of correctly predicted tis-\nsues of all actual tissues. The F1 score was calculated as the harmonic \nmean of precision and recall, which was used to measure classification \nperformance. We measured annotation performance using F1 score and \ncompared our results to the OpenAI CLIP model. Our analysis showed \nthat Loki consistently outperformed OpenAI CLIP across all four data-\nsets (Fig. 4b,c). The F1 scores of Loki ranged from 0.59 to 0.96, while the \nF1 scores of OpenAI CLIP ranged from 0.03 to 0.34 (Fig. 4c).\nSeveral studies have developed visual–language foundation mod-\nels using paired histopathology images and captions 11,12. Given that \ntranscriptomics and natural language provide complementary infor-\nmation, we investigated whether their combination could improve \nannotation performance without additional training. We applied PLIP, \na visual–language foundation model for pathology image analysis, to \nannotate the tissue images by descriptive prompts, such as convert-\ning ‘tumor’ to ‘an H&E image patch of colorectal adenocarcinoma \nepithelium’ in the CRC7K dataset. Overall, PLIP performed comparably \nto Loki, with F1 scores ranging from 0.5 to 0.93 (Fig. 4d). We then com-\nbined Loki and PLIP by averaging their similarity scores of an H&E image \nand a given tissue type (Fig. 4a and Methods), resulting in the best per-\nformance across all four benchmark datasets (Fig. 4d,e). In CRC7K, PLIP \nmisclassified 63% of colorectal adenocarcinoma epithelium images as \ncancer-associated stroma, while Loki misclassified 15% of tumor images \nas normal colon mucosa. Notably, combining Loki and PLIP achieved \na 93% recall rate, demonstrating that combining transcriptomic and \nnatural language enhances overall performance compared to each \nmodality alone (Fig. 4f).\nLoki Decompose maps cell types in H&E image using \nscRNA-seq\nSince OmiCLIP can project the Visium ST data and H&E images to a \nshared embedding space, we developed Loki Decompose, a feature to \ndecompose cell types in both ST data and H&E images, using scRNA-seq \nas a reference. Inspired by ST decomposition models like Tangram and \nCytoSPACE45,46, we used OmiCLIP to encode the patches (the same size \nas a Visium spot) of an H&E image and scRNA-seq transcriptomic profile \ninto this embedding space. As an application of Tangram with OmiCLIP \nembeddings instead of gene expression data, Loki Decompose applied \nTangram’s nonconvex optimization algorithm47 to deconvolute the \nOmiCLIP embeddings of an H&E image patch or the embeddings of \na Visium spot’s transcriptomic profile rather than raw gene expres-\nsion data, providing the cell-type composition of an image patch or \na Visium spot (Fig. 5a). We assessed Loki Decompose on our in-house \ntriple-negative breast cancer (TNBC) dataset, a human colorectal can-\ncer dataset48 and a brain dataset49,50.\nFirst, we performed a Xenium experiment on the in-house TNBC \nsample and captured paired H&E images. We generated pseudo-Visium \ndata from the Xenium data as a benchmark for evaluating Loki Decom-\npose, using publicly available scRNA-seq data as a ref. 51. The Xenium \ndata classified tissue into three main cell types: cancer epithelial cells, \nimmune cells and stromal cells (Fig. 5b and Extended Data Fig. 5a,b). \nWe used Loki to decompose pseudo-Visium spots and H&E images, \nusing paired sequencing and image data from one-fourth of a WSI for \nfine-tuning followed by cross-validation (Methods). Decomposition \naccuracy was evaluated using Jensen–Shannon ( JS) divergence and \nthe structural similarity index measure (SSIM). These metrics were \ncalculated by comparing the predicted cell-type proportions to the \nground truth derived from the Xenium data. Since JS divergence and \nSSIM operate on different scales, we standardized their values by cal-\nculating z-scores among different methods (details in Methods). The \nz-score for JS divergence was inverted (that is, multiplied by −1), as lower \nvalues indicate better performance. Finally, we averaged the z-scores \nof JS divergence and SSIM to calculate an overall impact score, which \nprovides a unified metric for comparison across methods. Loki Decom-\npose in ST mode and image mode ranked as the top two methods with \nFig. 5 | Cell-type decomposition. a, Schematic illustration of tissue alignment \nusing ST, reference scRNA-seq data and histology images with OmiCLIP paired \ntranscriptomic and image embeddings after fine-tuning. b, H&E image of our \nin-house TNBC sample, characterized by Xenium into three major cell types: \ncancer epithelial, immune and stromal cells. c, Performance comparison of 12 \ndecomposition methods using JS divergence, SSIM and impact scores. z-scores \nof JS divergence (or SSIM) across methods were calculated based on the average \nJS divergence (or SSIM) among cell types. The impact score of each method is \nthe average of the z-score of JS divergence and SSIM (Methods). The green color \nindicates decomposition tools. The blue color indicates the performance of \nreplacing OmiCLIP embeddings with other transcriptomic foundation models’ \nembeddings. d, Cell-type decomposition results on three major cell types of the \nTNBC sample using the image by Loki and using ST by Tangram, with Xenium \ndata as ground truth. The color of the heat map reflects the z-score, calculated \nby the probability distribution of each cell type. e, H&E image of the human \ncolorectal cancer sample and cell-type distribution within the Visium-HD capture \narea. f, Bar plot shows the accuracy of decomposition on four major cell types \nby Loki using ST or image mode, and by Tangram using ST. Error bars indicate \nthe standard deviation and the center values represent the mean. For both JS \ndivergence and SSIM, adjusted P value > 0.1 using a two-sided Wilcoxon test.  \ng, Whole-slide (20 mm × 13 mm) human colorectal cancer cell-type \ndecomposition. Different tissue regions are annotated by the pathologist as \nground truth. Heat map shows the cell-type distribution of fibroblast, tumor, \nintestinal epithelial, smooth muscle and immune/inflammatory cells, with \ncolor reflecting the density of each cell type. CLAM attention heat maps were \ngenerated using CLAM with default parameters. h, Cell-type decomposition \nresults on the brain sample. Left, brain anatomic references with zoom-in \nH&E image patches of L1 (VLMCs, astrocytes), L2/3, L4/5, L6 and white matter \n(WM; oligodendrocytes), respectively. Created in BioRender.com. Right, heat \nmap shows the cell-type distribution of VLMCs, astrocytes, L2/3, L4/5, L6 and \noligodendrocytes, with color reflecting the distribution of each cell type.\nNature Methods | Volume 22 | July 2025 | 1568–1582 1576\nArticle https://doi.org/10.1038/s41592-025-02707-1\ng Colorectal cancer pathology annotation\nTumor\nIntestinal epithelial\nSmooth muscle\nFibroblast\nImmune/inflammatory\nAdipose\n5 mm\na\nc\nImmune Epithelial Stroma\nd Xenium Loki image Tangram\nb\nXenium \nregion\n2 mm\nEpithelial\nImmune\nStroma\nXenium \n(in-house patient)\n–1 1\nz- score\ne\nSSIM\nJS divergence\nImage TangramST\nVisium-HD \ncapture area\n5 mm\nCell-type distribution \nB cells\nEndothelial\nFibroblast\nIntestinal epithelial\nMyeloid\nNeuronal\nSmooth muscle\nT cells\nTumor\nf\nAstrocytes L2/3VLMCs\nL4/5 L6 Oligodendrocytes\nDeconvolution from image\nL4/5\nL2/3\nL6\nL1 (VLMCs, astrocytes)\nWM (oligodendrocytes)\nh\nDeconvolution from image\nSmooth muscle\nFibroblast\nIntestinal epithelial\nCLAM attention\nAttention\nLow High\nTumor\nLow High\nDensity\nImmune/inflammatory \nLow High\nDensity\nLow High\nDensity\nLow High\nDensity\nLow High\nDensity\n1.0\n0.8\n0.6\n0.4\n0.2\n0\n1.0\n0.8\n0.6\n0.4\n0.2\n0\nImage TangramST\nLow High\nDensity\nLow High\nDensity\nNS NS\nNS NS\nLow High\nDensity\nLow High\nDensity\nLow High\nDensity\nLow High\nDensity\nEpithelial Immune Stroma z-score Epithelial Immune Stroma z-score\nLoki ST 0.51 0.28 0.13 1.82 0.17 0.45 0.61 0.83 1.32\nLoki image 0.52 0.40 –0.03 1.76 0.18 0.42 0.73 0.46 1.11\nRCTD 0.38 0.17 –0.01 0.88 0.20 0.38 0.64 0.86 0.87\nCARD 0.08 0.02 0.00 –0.21 0.22 0.43 0.60 0.78 0.28\nTangram –0.02 0.03 0.15 –0.10 0.23 0.57 0.52 0.52 0.21\nscGPT –0.07 0.00 0.19 –0.17 0.23 0.56 0.52 0.52 0.18\nSpatial Seurat 0.08 0.03 0.02 –0.15 0.22 0.65 0.56 0.09 –0.03\nscFoundation –0.21 –0.11 0.19 –0.81 0.24 0.59 0.52 0.40 –0.20\nGeneFormer 0.00 0.00 0.01 –0.44 0.23 0.59 0.66 –0.08 –0.26\nCytoSPACE 0.04 0.02 0.06 –0.18 0.27 0.72 0.74 –1.03 –0.61\nCell2location –0.03 –0.14 –0.13 –1.24 0.49 0.57 0.63 –0.88 –1.06\nspatialDWLS –0.24 0.03 –0.07 –1.15 0.63 0.71 0.78 –2.49 –1.82\nImpact scoreSSIM JS divergence\nStep 1\nST fine-tuning\nHistology image \n(adjacent/serial section)\nprediction\nOmiCLIP \nimage\nencoder\nOmiCLIP \ntext encoder\nStep 2\nGene expression\nDimension 1\nDimension 2\nscRNA-seq\nCell 1\nCell 2\nCell N\nCell-type A Cell-type B Cell-type C Cell-type D Patch\n0.75 0.06 0.17 0.02\n0.08 0.83 0.05 0.04\n0.13 0.08 0.68 0.11\n0.05 0.01 0.15 0.79\n...\n...\n...\n...\n...\n\nNature Methods | Volume 22 | July 2025 | 1568–1582\n 1577\nArticle https://doi.org/10.1038/s41592-025-02707-1\nimpact scores of 1.32 and 1.11, respectively, outperforming other SOTA \nmethods52 including Tangram, Spatial Seurat53, CARD54, CytoSPACE, \nCell2location50, SpatialDWLS55 and RCTD56, with impact scores ranging \nfrom 0.87 to −1.82 (Fig. 5c,d and Extended Data Fig. 5c). As single-cell \nfoundation models such as GeneFormer26, scGPT57 and scFoundation27 \ncan also provide the transcriptomic embeddings, to further evaluate \nthe approach, we replaced OmiCLIP gene expression embeddings \nwith those from single-cell foundation models GeneFormer, scGPT \nand scFoundation. Results showed that scGPT, scFoundation and Gen-\neFormer ranked 6th, 8th and 9th, respectively (Fig. 5c and Extended \nData Fig. 5c).\nSecond, we evaluated Loki Decompose using pseudo-Visium \ndata generated from whole-genome sequencing Visium-HD data of \nhuman colorectal cancer as a benchmark (Fig. 5e). We fine-tuned \nOmiCLIP on regions with paired sequencing and image data (Meth -\nods). Remarkably, the transcriptomic embeddings for scRNA-seq \ndata effectively captured cell heterogeneity, even without training on \nscRNA-seq data (Extended Data Fig. 6a and Supplementary Note 1). \nLoki Decompose successfully predicted the spatial distribution of key \ncell types (Extended Data Fig. 6b). We developed a technique inspired \nby non-maximum suppression (NMS)58 to refine spatial probabilistic \nmaps, enhancing decomposition performance by reducing ambigu -\nity in complex spatial scenarios and focusing predictions on the most \nconfident cell-type assignments. Using JS divergence and SSIM scores, \nLoki Decompose based on either the ST data or the H&E images was \ncomparable to Tangram, which used gene expression as input (Fig. 5f).\nThird, we extended the analysis to the entire WSI (20 mm) of the \nsame human colorectal cancer tissue (Fig. 5g), segmenting it into \nimage patches matching Visium spot size. Similarly, we used OmiCLIP \nto encode image patches and transcriptomics of scRNA-seq and then \ndecomposed those using scRNA-seq data. Loki Decompose accurately \npredicted densities of tumor, fibroblast, intestinal epithelial, smooth \nmuscle, immune and inflammatory cells, aligning closely with pathol-\nogy annotations (Fig. 5g). Additionally, our predicted tumor cell density \nmatched that of CLAM40, further validating Loki Decompose’s robust-\nness (Fig. 5g).\nFourth, to test Loki Decompose in a more challenging scenario, \nwe applied it to a brain tissue, where neurons share similar morphol-\nogy. Our dataset included vascular and leptomeningeal cells (VLMCs), \nastrocytes, and neurons from layers 2/3 (L2/3), layers 4/5 (L4/5) and \nlayer 6 (L6), as well as oligodendrocytes (Fig. 5h and Supplementary \nFig. 6). VLMCs and astrocytes are concentrated near the cortical surface \nand pial borders (for example, layer 1), while oligodendrocytes are \nmore prevalent in deeper layers and within white matter tracts 49.  To \ndecompose the mouse brain cortex slice, we applied a workflow similar \nto the one for other decomposition tasks. First, we fine-tuned OmiCLIP \nusing adjacent Visium data and H&E images, then segmented the WSI \ninto patches, corresponding to Visium spot size. The transcriptomic \nencoder of OmiCLIP was used to encode the scRNA-seq data from the \nAllen Institute atlas49, while the image encoder was used to encode the \nH&E image. Finally, Loki Decompose was applied to predict cell-type \ndistributions within the brain cortex H&E image. Loki Decompose \naccurately predicted the distribution of VLMCs, astrocytes, neurons \nfrom L2/3, L4/5 and L6 and oligodendrocytes, aligning closely with \nbrain anatomic ref. 49.\nLastly, we tested the performance of decomposition using three \ntraining strategies: pretraining plus fine-tuning, pure pretraining and \npure training from scratch on TNBC samples (Extended Data Fig. 7). The \nanalysis showed that pretraining plus fine-tuning had the best perfor-\nmance, achieving a mean SSIM score of 0.30 and a mean JS divergence \nof 0.40. In contrast, pure pretraining resulted in a mean SSIM score of \n0.13 and a mean JS divergence of 0.43, while pure training from scratch \nperformed the worst, with a mean SSIM score of 0.00070 and a mean JS \ndivergence of 0.44. Although pure pretraining achieved a comparable \nJS divergence score to the pretraining plus fine-tuning method (0.43 \nversus 0.40), it showed a notable decline in the SSIM (0.13 versus 0.30), \nunderscoring the importance of fine-tuning for optimal performance. \nTherefore, we strongly recommend fine-tuning the model for this task \nto achieve optimal results.\nAltogether, Loki Decompose effectively inferred cell-type frac -\ntions from H&E images and ST data, demonstrating its potential to \nenhance spatial tissue analysis by utilizing H&E images to reduce \nexperimental costs and processing time, particularly in multi-section \ntissue studies.\nLoki Retrieve enables H&E image-to-transcriptomics retrieval\nOne of the basic functions of contrastive learning models is retrieval. \nLeveraging such ability of OmiCLIP, we developed Loki Retrieve to \nidentify and retrieve transcriptomics data corresponding to a given H&E \nimage. Using OmiCLIP’s image encoder, query images were encoded to \nembeddings to retrieve the most similar transcriptomic entries from \nthe ST-bank dataset in the aligned latent space (Fig. 6a). We presented \nthe top 50 most similar transcriptomics results, as demonstrated by \nthe ST-paired images from the ST-bank dataset (Fig. 6b). Then, we sys-\ntematically evaluated our model on diverse datasets including four \nindependent histopathology datasets of colorectal cancer, lung cancer \nand lymph node metastasis, along with eight in-house tissues of heart \nfailure, Alzheimer’s disease and breast cancer human tissues (Sup-\nplementary Fig. 7). Because ground-truth transcriptomics data were \nunavailable, retrieval accuracy was assessed by measuring similarity \nbetween the query image and the retrieved transcriptomics-paired \nimages. Overall, Loki Retrieve significantly outperformed OpenAI CLIP \nand PLIP by a large margin (Fig. 6c,d; P value < 0.05), achieving median \nsimilarity scores ranging from 0.7 to 0.9.\nWe further evaluated image-to-transcriptomics retrieval perfor-\nmance by calculating the rank of the correct pair using Recall@K (5% \nand 10%). This metric measures the proportion of correctly retrieved \ndata within the samples retrieved using the top-K quantile (Methods). \nWe used four reserved samples from ST-bank as validation datasets \nincluding brain, heart, kidney and breast tissue samples and four inde-\npendent ST studies as a test dataset, including desmoplastic small \nround cell tumor, colorectal cancer, vascular and colon samples (Sup-\nplementary Table 4). Results demonstrated that Loki notably out -\nperformed both OpenAI CLIP and PLIP across all validation datasets. \nSpecifically, Loki achieved Recall@5% of 0.125 and Recall@10% of \n0.227 for brain (average 2.3-fold higher than OpenAI CLIP and 2.5-fold \nFig. 6 | Image-to-transcriptomics retrieval. a, Schematic illustration of image-\nto-transcriptomics retrieval on the ST-bank dataset. b, Example image-to-\ntranscriptomics retrieval results. For each example image from adipose tissue, \ncolorectal adenocarcinoma epithelium, lymphocytes, smooth muscle and \nnormal colon mucosa, the retrieved top 50 most similar transcriptomics are \nshown by the paired image from the ST-bank dataset. c, Image-to-transcriptomics \nretrieval similarity scores across the four validation datasets—CRC7K, WSSS4LUAD,  \nLC25000 and PatchCamelyon—using Loki, OpenAI CLIP and PLIP. In the box \nplots, the middle line represents the median, the box boundaries indicate the \ninterquartile range, and the whiskers extend to data points within 1.5 times the \ninterquartile range. d, Image-to-transcriptomics retrieval similarity scores across \nthe eight in-house human tissues: heart failure (HF), Alzheimer’s disease (AD), \nmetaplastic breast cancer (MPBC) and TNBC, using Loki, OpenAI CLIP and PLIP. In \nthe box plots, the middle line represents the median, the box boundaries indicate \nthe interquartile range, and the whiskers extend to data points within 1.5 times \nthe interquartile range. e, Image-to-transcriptomics retrieval evaluation across \nfour validation datasets and one test dataset using Loki, OpenAI CLIP and PLIP, \nwith random baseline. The top-K quantile most similar transcriptomics were \nretrieved. We report Recall@K for K ∈ {5%, 10%} (Methods). f, Example image-to-\ntranscriptomics retrieval results. The retrieved transcriptomics are shown by  \nthe paired image.\nNature Methods | Volume 22 | July 2025 | 1568–1582 1578\nArticle https://doi.org/10.1038/s41592-025-02707-1\na\nADI\nTUM\nLYM\nMUS\nNOR\nc\nPatchCamelyon \nCRC7K\nADI DEB LYM MUC MUS NOR STR TUM\nWSSS4LUAD LC25000\nSimilaritySimilarity\nHF1 HF2 AD1 MPBCHF3 HF4 AD2 TNBC\nIn-house dataset\nSimilarity\nd\nb\nQuery image 1 \nQuery image 2 \nST-bank dataset\nLoki\nOpenAI CLIP\nPLIP\nLoki\nOpenAI CLIP\nPLIP\ne f\nQuery\nimage\nRetrieved \ntranscriptomics Dataset Metric Loki OpenAI CLIP PLIP Random\nFold change\n(Loki versus\nPLIP)\nFold change\n(Loki versus\nOpenAI CLIP)\nRecall@5% 0.125 0.051 0.048 0.052 2.4 2.6\n0.227 0.103 0.095 0.101 2.2\n0.186 0.052 0.057 0.049 3.6\n0.291 0.104 0.103 0.098 2.8\n3.3\n2.8\n0.173 0.052 0.053 0.053 3.4\n0.297 0.100 0.097 0.101 3.0\n3.3\n3.0\n0.140 0.049 0.050 0.047 2.9\n0.240 0.100 0.096 0.094 2.4\n2.8\n2.5\n0.117 0.033 0.042 0.025 3.5\n0.208 0.075 0.067 0.067 2.8\n2.8\n3.1\n2.4Recall@10%\nRecall@5%\nRecall@10%\nRecall@5%\nRecall@10%\nRecall@5%\nRecall@10%\nRecall@5%\nRecall@10%\nTest dataset\nValidation brain dataset\nValidation heart dataset\nValidation kidney dataset\nValidation breast dataset\n1.0\n0.8\n0.2\n0.6\n0.4\n1.0\n0.8\n0.2\nNormal Tumor Normal TumorNormal Tumor\n0.6\n0.4\n1.0\n0.8\n0.2\n0.6\n0.4\n1.0\n0.8\n0.2\n0.6\n0.4\n1.0\n0.8\n0.2\n0.6\n0.4\nMYH7 TNNT2 ACTC1 ... CATA4\nPPARG LEP ... ADIPOQ\n. . .\nNature Methods | Volume 22 | July 2025 | 1568–1582\n 1579\nArticle https://doi.org/10.1038/s41592-025-02707-1\nhigher than PLIP), Recall@5% of 0.186 and Recall@10% of 0.291 for \nheart (average 3.2-fold higher than OpenAI CLIP and 3.1-fold higher \nthan PLIP), Recall@5% of 0.173 and Recall@10% of 0.297 for kidney \n(average 3.2-fold higher than OpenAI CLIP and PLIP) and Recall@5% \nof 0.140 and Recall@10% of 0.240 for breast (average 2.6-fold higher \nthan OpenAI CLIP and PLIP; Fig. 6e). On the test dataset, Loki further \ndemonstrated substantial improvements, achieving Recall@5% of 0.117 \nand Recall@10% of 0.208 (average 3.1-fold higher than OpenAI CLIP and \n3.0-fold higher than PLIP; Fig. 6e and Supplementary Table 4). T ogether, \nthese results confirm Loki’s superior performance in accurately retriev-\ning paired transcriptomic information from images.\nLoki PredEx predicts ST gene expression from H&E images\nBuilding on the success of Loki Align, Annotate and Decompose in \nanalyzing tissue across the H&E image and transcriptomics data, we \ndeveloped Loki PredEx to predict gene expression for image patches. \nLoki PredEx computes a weighted sum of gene expression from ref-\nerence ST spots where weights are determined by similarity scores \nbetween the query image and ST data, both encoded by OmiCLIP \n(Supplementary Fig. 8 and Methods). Several studies have explored \npredicting gene expression from H&E images using AI models 59–62. \nWe compared Loki PredEx with them on a normal human heart data-\nset comprising 39 samples. Loki accurately predicted highly variable \ngene expression, as demonstrated by the spatial distribution of the \npredicted gene expression (Extended Data Fig. 8). T o evaluate the per-\nformance, we used mean squared error (MSE) and PCC as two metrics. \nLoki PredEx demonstrated superior performance, achieving the best \nresults based on MSE scores in 28 of 39 cases, and ranking as the best \nin 16 of 39 samples based on PCC compared to Hist2ST, HisT oGene, \nBLEEP and mclSTExp (Extended Data Fig. 9a). These results showed the \nrobustness of OmiCLIP in predicting ST data across diverse datasets \n(Extended Data Fig. 9b). A major limitation of deep learning models \nlike HisT oGene is their heavy hardware requirements. Models like His-\nT oGene and Hist2ST were optimized for smaller legacy ST datasets, \nwith fewer spots. For instance, HisT oGene is typically trained on less \nthan 7,000 spot–image pairs. However, with modern ST technolo-\ngies such as Visium, slides contain over 4,000 spots, pushing memory \ndemands above 300 GB and complicating GPU-based training. In our \nexperiments, training HisT oGene on over 80,000 spots from 35 tissues \nrequired 4 h on 16 2.60 GHz Intel Xeon Gold 6348 CPUs for 100 epochs \nand Hist2ST took 31 h under similar conditions. Loki PredEx avoids \nthese resource-intensive training needs, providing a more efficient \nalternative. T ogether, Loki PredEx delivers accurate ST gene expres-\nsion predictions, and avoids these resource-intensive training needs, \nproviding a more efficient alternative based upon the use of pretrained \nweights, highlighting its potential as a scalable infrastructure.\nDiscussion\nExisting dual-modality foundation models in computational biology11,12 \nprimarily combine images with textual descriptions, proving their \nutility in histopathology annotation and analysis. However, the natural \nlanguage descriptions lack molecular insights for disease characteriza-\ntion. Our study first suggests that publicly available ST datasets provide \nsufficient volume and diversity to pretrain a foundation model bridging \ntissue morphology with genomics. The success of the development \nof our foundation model could represent a substantial step toward \nunderstanding molecular mechanisms regulating tissue phenotypes \nin health and disease.\nWe presented OmiCLIP, a high-performance histopathology \nimage–omics foundation model by contrastive learning. Unlike \nvisual–language foundation models, OmiCLIP integrates molecular \ninsights with pathology images, complementing language descrip -\ntions. Benchmark results indicate that OmiCLIP performs comparably \nto, and in some cases surpasses visual–language foundation models \nin tissue annotation, suggesting that marker genes could serve as \neffective tissue labels independent of language. Notably, our anno -\ntation of tissue types incorporating both language description and \nmarker genes shows promise for triple-modal foundation modeling \nof image, transcriptomics and language. Using marker genes as a label \ncould potentially facilitate molecular investigation-related studies \nsuch as drug repurposing, immune response prediction and disease \nmechanism discovery.\nA key question is whether OmiCLIP’s transcriptomic encoder \ngeneralizes to other sequencing techniques like bulk RNA-seq and \nscRNA-seq. We evaluated the information of transcriptomic embed-\ndings by cell annotation of scRNA-seq data (Supplementary Note 1) \nand tumor classification of bulk RNA-seq data (Supplementary Note 2).  \nOur results show that OmiCLIP’s transcriptomic embeddings efficiently \ncluster participants with cancer without specific training and accu-\nrately annotate cell types with even 1% of labeled cells.\nLoki could potentially enhance 3D tissue analysis by integrating \nimaging and molecular modalities in a scalable and efficient manner. \nEmerging 3D histology and omics techniques already show prom -\nise in improving diagnostic accuracy by preserving native 3D tissue \nmorphology, leading to better prognostic predictions and ultimately \nimproved patient care63–66. However, challenges remain in spatial dis-\ntortions and aligning molecular data across different modalities. Loki \naddresses these by aligning tissue slices and integrating ST, histology \nand scRNA-seq data, enabling a more comprehensive understanding \nof tissue architecture and cellular interactions, which is crucial for 3D \ntissue analysis. Incorporating Loki into workflows facilitates detailed \nmolecular and spatial features analysis across tissue sections, sup -\nporting automated, scalable and high-resolution 3D tissue analysis.\nLoki provides an AI-powered platform supporting the expansion of \nadditional tools in a unified framework. Among the existing modules, \nLoki Annotate automates annotation and interpretation of molecular \nand spatial tissue features using associated or external RNA-seq data or \nmarker genes. Loki PredEx predicts spatial gene expression from histol-\nogy images, reducing reliance on costly and laborious ST experiments. \nThese modules, leveraging contrastively aligned embeddings, enable \nefficient multimodal tissue reconstruction and analysis, providing a \nscalable solution to the growing demand for high-resolution tissue \nstudies. Loki’s ability to integrate diverse data types across tissue sec-\ntions minimizes cost and complexity while accelerating workflows in \nenabling deeper insights into biological systems.\nCompared to billion-scale datasets for developing visual–language \nmodels in the general machine-learning domain, the major limitation \nof this study is pretraining data size. We expect that continued use of \ntraining datasets may further improve the zero-shot performance. \nHowever, several biomedical multimodal foundation models were \nefficiently trained on million-scale datasets by removing duplicates \nand noise11,12,67, a strategy we used to optimize training efficiency.\nNotably, as a contrastive learning framework, OmiCLIP is not \ngenerative and cannot directly generate the accurate transcriptomic \nprofile of the query image. Instead, it retrieves tissues with the most \nsimilar transcriptomic profiles to the query tissue. While it effectively \nembeds transcriptomic and histology data at the patch level, it does \nnot inherently generate new data, such as reconstructing a WSI with \ngene expression patterns. However, OmiCLIP’s patch-level embed-\ndings could support generative approaches, such as diffusion models, \nto reconstruct WSIs with ST details. Future studies could refine the \ntranscriptomic encoder using RNA-seq datasets like scRNA-seq and \nbulk RNA-seq data. Although ST-bank includes 32 organ types, rare \nconditions may be underrepresented. We suggest fine-tuning align -\nment and decomposition tasks to ensure compatibility with datasets \nthat are not covered in ST-bank (Extended Data Fig. 10).\nUnlike single-cell foundation models like scGPT57, Geneformer26 \nand scFoundation27, our approach models omics data as text, effectively \nbridging molecular and visual modalities. Representing gene expres-\nsion data as text leverages natural language processing models to \nNature Methods | Volume 22 | July 2025 | 1568–1582 1580\nArticle https://doi.org/10.1038/s41592-025-02707-1\nembed biological information into a high-dimensional space, offering \nseveral advantages over using gene expression values directly. First, \ntext embeddings integrate omics data with various biological entities \nsuch as pathways, functional annotations68 and cell types69, extending \nthe model’s capabilities beyond tissue alignment and decomposition, \nmaking it adaptable to a broader range of biological tasks. Second, \nthis approach aligns with other multimodal foundation models, and \nallows incorporation of proteomics, metabolomics and DAPI images \ninto the same unified space. In contrast, raw gene expression values \nlack flexibility for such integrations and require additional preprocess-\ning. Third, text-based foundation models trained on billions of tokens \nprovided robust text embeddings, like GenePT23, demonstrating that \ngene embeddings from textual descriptions can match or surpass \nmodels trained on extensive gene expression datasets. This supports \nour approach of utilizing text-based embeddings to capture rich bio-\nlogical information efficiently.\nWhile integrating two modalities enhances information capture, \nit may also introduce noise or misalignment, potentially overshad -\nowing benefits. If one modality dominates, performance gains from \ndual-modality fusion may be minimal.\nLoki Decompose is valuable in scenarios where sequencing costs \nlimit transcriptomic profiling. By estimating cell-type proportions \nfrom images, researchers can preselect, screen or perform batch \nprocessing of samples cost-effectively for exploratory studies and \nlarge-scale screenings. Loki Retrieve utilizes curated reference images \nfor ground-truth comparisons, aiding validation and interpretation, \nespecially when training data for prediction models like Loki PredEx \nare scarce. T ogether, our approach contributes to a unified, scalable \nframework for multimodal analysis.\nT o conclude, we created ST-bank, a dataset of over 2 million \npathology-specific image–transcriptomics pairs. We developed \nOmiCLIP to integrate these data, forming a visual–omics foundation \nmodel. Leveraging OmiCLIP, we built Loki, an infrastructure enabling \nmultimodal analysis for tissue alignment, tissue annotation, cell-type \ndecomposition, histology image–transcriptomics retrieval and ST gene \nexpression prediction. These capabilities represent a fundamental \nstep toward bridging and applying foundation models in genomics \nfor histopathology.\nOnline content\nAny methods, additional references, Nature Portfolio reporting sum-\nmaries, source data, extended data, supplementary information, \nacknowledgements, peer review information; details of author con -\ntributions and competing interests; and statements of data and code \navailability are available at https://doi.org/10.1038/s41592-025-02707-1.\nReferences\n1. Hegde, N. et al. Similar image search for histopathology: SMILY. \nNPJ Digit. Med. 2, 56 (2019).\n2. Chen, C. et al. Fast and scalable search of whole-slide images via \nself-supervised deep learning. Nat. Biomed. Eng. 6, 1420–1434 \n(2022).\n3. Huang, Z. et al. Artificial intelligence reveals features associated \nwith breast cancer neoadjuvant chemotherapy responses from \nmulti-stain histopathologic images. NPJ Precis. Oncol. 7, 14 (2023).\n4. Lu, M. Y. et al. AI-based pathology predicts origins for cancers of \nunknown primary. Nature 594, 106–110 (2021).\n5. Zhu, L. et al. An accurate prediction of the origin for bone \nmetastatic cancer using deep learning on digital pathological \nimages. EBioMedicine 87, 104426 (2023).\n6. Chen, R. J. et al. Pan-cancer integrative histology-genomic \nanalysis via multimodal deep learning. Cancer Cell 40, 865–878 \ne866 (2022).\n7. Chen, R. J. et al. Towards a general-purpose foundation model for \ncomputational pathology. Nat. Med. 30, 850–862 (2024).\n8. Radford, A. et al. Learning transferable visual models from natural \nlanguage supervision. In Proc. 38th International Conference on \nMachine Learning 8748–8763 (PMLR, 2021).\n9. Yu, J. et al. CoCa: contrastive captioners are image-text \nfoundation models. Trans. Mach. Learn. Res. (2022).\n10. Li, Y. et al. Supervision exists everywhere: a data efficient \ncontrastive language-image pre-training paradigm. In Proc. \n10th International Conference on Learning Representations \n(OpenReview.net, 2022).\n11. Lu, M. Y. et al. A visual-language foundation model for \ncomputational pathology. Nat. Med. 30, 863–874 (2024).\n12. Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T. J. & Zou, J. A \nvisual–language foundation model for pathology image analysis \nusing medical twitter. Nat. Med. 29, 2307–2316 (2023).\n13. Abe, J.-I. et al. An ERK5-NRF2 axis mediates \nsenescence-associated stemness and atherosclerosis. Circ. Res. \n133, 25–44 (2023).\n14. Mao, H. et al. CRAT links cholesterol metabolism to innate \nimmune responses in the heart. Nat. Metab. 5, 1382–1394 (2023).\n15. Chau, K. M. et al. TNIK regulation of interferon signaling and \nendothelial cell response to virus infection. Front. Cardiovasc. \nMed. 10, 1213428 (2024).\n16. Nguyen, M. T. et al. Endothelial activation and fibrotic changes are \nimpeded by laminar flow-induced CHK1-SENP2 activity through \nmechanisms distinct from endothelial-to-mesenchymal cell \ntransition. Front. Cardiovasc. Med. 10, 1187490 (2023).\n17. Silverman, A. D., Karim, A. S. & Jewett, M. C. Cell-free gene \nexpression: an expanded repertoire of applications. Nat. Rev. \nGenet. 21, 151–170 (2020).\n18. Ding, J., Sharon, N. & Bar-Joseph, Z. Temporal modelling using \nsingle-cell transcriptomics. Nat. Rev. Genet. 23, 355–368 (2022).\n19. Li, S. et al. A relay velocity model infers cell-dependent RNA \nvelocity. Nat. Biotechnol. 42, 99–108 (2024).\n20. Chakraborty, A. et al. Epigenetic induction of smooth muscle \ncell phenotypic alterations in aortic aneurysms and dissections. \nCirculation 148, 959–977 (2023).\n21. Preissl, S., Gaulton, K. J. & Ren, B. Characterizing cis-regulatory \nelements using single-cell epigenomics. Nat. Rev. Genet. 24, \n21–43 (2023).\n22. Wagner, D. E. & Klein, A. M. Lineage tracing meets single-cell \nomics: opportunities and challenges. Nat. Rev. Genet. 21, 410–427 \n(2020).\n23. Chen, Y. & Zou, J. Simple and effective embedding model for \nsingle-cell biology built from ChatGPT. Nat. Biomed. Eng. 9, \n483–493 (2024).\n24. Levine, D. et al. Cell2Sentence: teaching large language models \nthe language of biology. In Proc. 41st International Conference on \nMachine Learning 27299–27325 (PMLR, 2024).\n25. Szabo, P. A. et al. Single-cell transcriptomics of human T cells \nreveals tissue and activation signatures in health and disease. Nat. \nCommun. 10, 4706 (2019).\n26. Theodoris, C. V. et al. Transfer learning enables predictions in \nnetwork biology. Nature 618, 616–624 (2023).\n27. Hao, M. et al. Large-scale foundation model on single-cell \ntranscriptomics. Nat. Methods 21, 1481–1491 (2024).\n28. McCarthy, D. J., Campbell, K. R., Lun, A. T. & Wills, Q. F. Scater: \npre-processing, quality control, normalization and visualization of \nsingle-cell RNA-seq data in R. Bioinformatics 33, 1179–1186 (2017).\n29. Caliński, T. & Harabasz, J. A dendrite method for cluster analysis. \nCommun. Stat. Theory Methods 3, 1–27 (1974).\n30. Xu, H. et al. A whole-slide foundation model for digital pathology \nfrom real-world data. Nature 630, 181–188 (2024).\n31. Myronenko, A. & Song, X. Point set registration: coherent point \ndrift. IEEE Trans. Pattern Anal. Mach. Intell. 32, 2262–2275  \n(2010).\nNature Methods | Volume 22 | July 2025 | 1568–1582\n 1581\nArticle https://doi.org/10.1038/s41592-025-02707-1\n32. Zeira, R., Land, M., Strzalkowski, A. & Raphael, B. J. Alignment \nand integration of spatial transcriptomics data. Nat. Methods 19, \n567–575 (2022).\n33. Jones, A., Townes, F. W., Li, D. & Engelhardt, B. E. Alignment of \nspatial genomics data using deep Gaussian processes. Nat. \nMethods 20, 1379–1387 (2023).\n34. Mirzazadeh, R. et al. Spatially resolved transcriptomic profiling of \ndegraded and challenging fresh frozen samples. Nat. Commun. \n14, 509 (2023).\n35. Villacampa, E. G. et al. Genome-wide spatial expression  \nprofiling in formalin-fixed tissues. Cell Genom. 1, 100065  \n(2021).\n36. Tang, Z. et al. Search and match across spatial omics samples at \nsingle-cell resolution. Nat. Methods 21, 1818–1829 (2024).\n37. Janesick, A. et al. High resolution mapping of the tumor \nmicroenvironment using integrated single-cell, spatial and in situ \nanalysis. Nat. Commun. 14, 8353 (2023).\n38. Hayashi, M. et al. Identification of the collagen type 1 alpha 1 gene \n(COL1A1) as a candidate survival-related factor associated with \nhepatocellular carcinoma. BMC Cancer 14, 108 (2014).\n39. Gu, Y. et al. A pan-cancer analysis of the prognostic and \nimmunological role of β-actin (ACTB) in human cancers. \nBioengineered 12, 6166–6185 (2021).\n40. Lu, M. Y. et al. Data-efficient and weakly supervised computational \npathology on whole-slide images. Nat. Biomed. Eng. 5, 555–570 \n(2021).\n41. Kather, J. N. et al. Predicting survival from colorectal cancer \nhistology slides using deep learning: a retrospective multicenter \nstudy. PLoS Med. 16, e1002730 (2019).\n42. Han, C. et al. WSSS4LUAD: grand challenge on weakly-supervised \ntissue semantic segmentation for lung adenocarcinoma. Preprint \nat https://arxiv.org/abs/2204.06455 (2022).\n43. Veeling, B. S., Linmans, J., Winkens, J., Cohen, T. & Welling, M. \nRotation equivariant CNNs for digital pathology. In Medical Image \nComputing and Computer Assisted Intervention – MICCAI 2018: \n21st International Conference, Granada, Spain, September 16–20, \n2018, Proceedings, Part II 210–218 (Springer, 2018).\n44. Borkowski, A. et al. Lung and colon cancer histopathological \nimage dataset (LC25000). Preprint at https://arxiv.org/\nabs/1912.12142 (2019).\n45. Biancalani, T. et al. Deep learning and alignment of spatially \nresolved single-cell transcriptomes with Tangram. Nat. Methods \n18, 1352–1362 (2021).\n46. Vahid, M. R. et al. High-resolution alignment of single-cell and \nspatial transcriptomes with CytoSPACE. Nat. Biotechnol. 41, \n1543–1548 (2023).\n47. Jain, P. & Kar, P. Non-convex optimization for machine learning. \nFound. Trends Mach. Learn. 10, 142–363 (2017).\n48. Oliveira, M. F. et al. Characterization of immune cell populations \nin the tumor microenvironment of colorectal cancer using high \ndefinition spatial profiling. Preprint at bioRxiv https://doi.org/ \n10.1101/2024.06.04.597233 (2024).\n49. Tasic, B. et al. Shared and distinct transcriptomic cell types across \nneocortical areas. Nature 563, 72–78 (2018).\n50. Kleshchevnikov, V. et al. Cell2location maps fine-grained cell \ntypes in spatial transcriptomics. Nat. Biotechnol. 40, 661–671 \n(2022).\n51. Karaayvaz, M. et al. Unravelling subclonal heterogeneity and \naggressive disease states in TNBC through single-cell RNA-seq. \nNat. Commun. 9, 3588 (2018).\n52. Li, B. et al. Benchmarking spatial and single-cell transcriptomics \nintegration methods for transcript distribution prediction and cell \ntype deconvolution. Nat. Methods 19, 662–670 (2022).\n53. Stuart, T. et al. Comprehensive integration of single-cell data. Cell \n177, 1888–1902 (2019).\n54. Ma, Y. & Zhou, X. Spatially informed cell-type deconvolution for \nspatial transcriptomics. Nat. Biotechnol. 40, 1349–1359 (2022).\n55. Dong, R. & Yuan, G.-C. SpatialDWLS: accurate deconvolution of \nspatial transcriptomic data. Genome Biol. 22, 145 (2021).\n56. Cable, D. M. et al. Robust decomposition of cell type mixtures in \nspatial transcriptomics. Nat. Biotechnol. 40, 517–526 (2022).\n57. Cui, H. et al. scGPT: toward building a foundation model for \nsingle-cell multi-omics using generative AI. Nat. Methods 21, \n1470–1480 (2024).\n58. Neubeck, A. & Van Gool, L. Efficient non-maximum suppression. \nIn 18th International Conference on Pattern Recognition (ICPR’06) \n850–855 (IEEE, 2006).\n59. Pang, M., Su, K. & Li, M. Leveraging information in spatial \ntranscriptomics to predict super-resolution gene expression from \nhistology images in tumors. Preprint at bioRxiv https://doi.org/ \n10.1101/2021.11.28.470212 (2021).\n60. Zeng, Y. et al. Spatial transcriptomics prediction from histology \njointly through transformer and graph neural networks. Brief. \nBioinform. 23, bbac297 (2022).\n61. Xie, R. et al. Spatially resolved gene expression prediction from \nhistology images via bi-modal contrastive learning. Advances in \nNeural Information Processing Systems 36 (2024).\n62. Min, W., Shi, Z., Zhang, J., Wan, J. & Wang, C. Multimodal \ncontrastive learning for spatial gene expression prediction using \nhistology images. Brief. Bioinform. 25, bbae551 (2024).\n63. Kuett, L. et al. Three-dimensional imaging mass cytometry for \nhighly multiplexed molecular and cellular mapping of tissues and \nthe tumor microenvironment. Nat. Cancer 3, 122–133 (2022).\n64. Xie, W. et al. Prostate cancer risk stratification via nondestructive \n3D pathology with deep learning–assisted gland analysis. Cancer \nRes. 82, 334–345 (2022).\n65. Song, A. H. et al. Analysis of 3D pathology samples using weakly \nsupervised AI. Cell 187, 2502–2520 (2024).\n66. Schott, M. et al. Open-ST: high-resolution spatial transcriptomics \nin 3D. Cell 187, 3953–3972 (2024).\n67. Christensen, M., Vukadinovic, M., Yuan, N. & Ouyang, D. Vision–\nlanguage foundation model for echocardiogram interpretation. \nNat. Med. 30, 1481–1488 (2024).\n68. Hu, M. et al. Evaluation of large language models for discovery of \ngene set function. Nat. Methods 22, 82–91 (2024).\n69. Hou, W. & Ji, Z. Assessing GPT-4 for cell type annotation in \nsingle-cell RNA-seq analysis. Nat. Methods 21, 1462–1465 (2024).\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons licence, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this licence, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2025\nNature Methods | Volume 22 | July 2025 | 1568–1582 1582\nArticle https://doi.org/10.1038/s41592-025-02707-1\n1Center for Bioinformatics and Computational Biology, Houston Methodist Research Institute, Houston, TX, USA. 2Department of Physiology, Biophysics \n& Systems Biology, Weill Cornell Graduate School of Medical Science, Cornell University, New York, NY, USA. 3Center for Cardiovascular Regeneration, \nHouston Methodist Research Institute, Houston, TX, USA. 4Center for RNA Therapeutics, Houston Methodist Research Institute, Houston, TX, USA. \n5Department of Cardiothoracic Surgery, Weill Cornell Medicine, Cornell University, New York, NY, USA. 6Department of Biomedical Informatics, College \nof Medicine, The Ohio State University, Columbus, OH, USA. 7Department of Pathology, Immunology and Laboratory Medicine, College of Medicine, \nUniversity of Florida, Gainesville, FL, USA. 8Department of Cardiology, The University of Texas MD Anderson Cancer Center, Houston, TX, USA. 9Center \nfor Immunotherapy, Neal Cancer Center, Houston Methodist Research Institute, Houston, TX, USA. 10Department of Bioinformatics and Computational \nBiology, The University of Texas MD Anderson Cancer Center, Houston, TX, USA. 11Department of Health Outcomes and Biomedical Informatics,  \nCollege of Medicine, University of Florida, Gainesville, FL, USA. 12These authors contributed equally: Weiqing Chen, Pengzhi Zhang.  \n e-mail: gwang2@houstonmethodist.org\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\nMethods\nTraining dataset curation\nWe curated a large dataset of histopathology image–transcriptom-\nics pairs using publicly available 10x Visium datasets (Supplementary \nTable 1). H&E images were cropped to match ST spot sizes, and text sen-\ntences were generated by combining the top 50 expressed genes per spot \ninto sentences. For example, the top-expressed genes in one spot, for \nexample, SNAP25, ENO2, CKB, GRIN2C and CAMK4, will be combined into a \nsentence: ‘SNAP25 ENO2 CKB GRIN2C CAMK4 … MTOR VPS13D’ . Data pre-\nprocessing involved removing duplicates and excluding low-resolution \nH&E images (<2,000 × 2,000 pixels), and normalizing raw count matrices \nfollowing standard protocols using Seurat70 and Scanpy71. For datasets in \ntranscripts per million or fragments per kilobase of transcript per million \nfragments mapped formats, which cannot be normalized to standard \ngene expression profiles, were retained unchanged. Quality control was \napplied to filter out contaminated, extremely low-quality or damaged \ncells, retaining only those with over 200 expressed genes. Ensembl gene \nIDs were converted to gene symbols for consistency. Housekeeping genes \nwere removed to ensure a more biologically relevant analysis. These steps \nresulted in ST-bank, a pathology-specific image–transcriptomics caption \ndataset of 2,185,571 pairs.\nDownstream evaluation datasets (details in Supplementary \nNote 3)\nTissue alignment. Simulated datasets were generated from ten human \ntissue slices including two breast cancer 72,73, one colorectal liver \ncancer74, one liver cancer75, one prostate cancer76, one 10x Genomics \nprostate cancer, one 10x Genomics colon cancer, one embryonic lung77, \none normal small intestine34 and one sleep apnea tonsil sample78. We \nsimulated new ST experiments by perturbing both gene expression and \nspatial locations at different levels of noise, generating 10 simulated \ndatasets per real dataset, totaling 200 datasets (100 low-noise, 100 \nhigh-noise). Real-world data tests used a normal human small intestine \nVisium dataset34 of eight adjacent tissue slices, a human ovarian carci-\nnosarcoma Visium dataset35 of two adjacent tissue slices and a human \nbreast cancer Visium and Xenium dataset37.\nTissue annotation . Bulk RNA-seq data-based annotation used \nthree normal human breast and three human heart failure histology \nimages79,80 and three breast cancer histology images from TCGA. Pathol-\nogy experts annotated different tissue regions. Bulk RNA-seq datasets \nincluding 663 human adipose and 504 fibroblast samples from the \nGenotype-Tissue Expression Portal and three paired tumor biopsy \nsamples from TCGA. Marker gene-based annotation included four data-\nsets: CRC7K (6,333 colorectal adenocarcinoma images), WSSS4LUAD \n(10,091 LUAD images), LC25000 (25,000 lung and colon images) and \nPatchCamelyon (32,768 lymph node images).\nCell-type decomposition. We downloaded a human colorectal cancer \ndataset48 to create pseudo-Visium spots in the Visium-HD capture area. \nPathology experts annotated different tissue regions. We collected an \nin-house TNBC patient-derived xenograft for processing on Xenium \nslides, to create pseudo-Visium spots with an external scRNA-seq refer-\nence of TNBC51 for decomposition. We also downloaded a mouse brain \nVisium dataset50 and a scRNA-seq dataset49 from the Allen Institute.\nH&E image-to-ST retrieval. We collected our in-house heart failure \npatient tissue, paraffin-embedded Alzheimer’s disease patient tissue, \nand metaplastic breast cancer and TNBC patient-derived xenografts. \nThe validation datasets included brain, heart, kidney and breast sam-\nples, and the test dataset included desmoplastic small round cell tumor, \ncolorectal cancer, vascular and colon samples (Supplementary Table 4).\nST gene expression prediction. We used a normal human heart sam-\nple dataset81 of paired ST data and H&E images including 39 samples.\nOmiCLIP model training\nOmiCLIP consisted of an image encoder and a text encoder following \nCoCa9 settings. The image encoder was based on a standard vision \ntransformer (ViT)82 with an input image size of 224 × 224 pixels. The \ntext encoder was based on a causal masking transformer with input \ntext length of 76 tokens. Regarding the initial embeddings of ST data, \nthe initial text encoder was not trained from scratch but on LAION-5B83, \nincluding biological literature, which may explain its tendency to \ncluster similar tissue patches. The model was trained for 20 epochs, \nusing one NVIDIA A100 80-GB GPU with a local batch size of 64. The \noutput vectors of the image and text encoders with dimensions of 768 \nwere optimized by minimizing the contrastive loss on a given batch. \nAll experiments were run in Python v.3.9. Detailed software versions \nare: CUDA v.12.2; torch v.2.3.1; torchvision v.0.18.1; scipy v.1.13.1; pil-\nlow v.10.4.0; scikit-learn v.1.5.2; pandas v.2.2.3; numpy v.1.25.0; and \nscanpy v.1.10.3.\nOmiCLIP model fine-tuning\nT o improve performance on downstream tasks, OmiCLIP allows \nfine-tuning with user datasets. The fine-tuning dataset is created by \npreprocessing Visium data using a standard 10x Space Ranger pipeline \nand generating gene name sentences as describe in ‘Training dataset \ncuration’ , ensuring compatibility with the pretraining dataset format. \nFine-tuning is done using contrastive loss9 between image embeddings \nand paired text embeddings of the top-expressed gene sentences. The \ncontrastive loss is calculated according to equation (1):\nLCon =− 1\nN\n⎛\n⎜⎜\n⎝\n∑\nN\ni log\nexp(\nxT\ni yi\nσ\n)\n∑\nN\nj=1exp(\nxT\ni y j\nσ\n)\n+∑\nN\ni log\nexp(\nyT\ni xi\nσ\n)\n∑\nN\nj=1exp(\nyT\ni x j\nσ\n)\n⎞\n⎟⎟\n⎠\n, (1)\nwhere xi and yj denote the normalized image and text embeddings, \nrespectively. N denotes the batch size, while σ represents the tempera-\nture parameter. The pretrained model was fine-tuned for ten epochs \nfor the tissue alignment task and five epochs for the cell-type decom-\nposition task, using a local batch size of 64, minimizing the contrastive \nloss.\nLoki Align\nWe first fine-tuned OmiCLIP using paired ST data and H&E image of \nthe target sample. The fine-tuned OmiCLIP text encoder and image \nencoder then encoded ST data and image, respectively. We used a \nnonrigid point set registration algorithm based on the CPD method31, \nwhich iteratively aligns two point sets by minimizing the statistical \ndiscrepancies.\nThe algorithm initializes the transformation matrix W to zero and \nsets the variance σ2 of point displacements as shown in equation (2):\nσ2 = 1\nDNM\nM,N\n∑\nm,n=1\n‖xn −ym‖2. (2)\nWhere D is the point’s dimensionality, M,N are the number of points in \neach set, and x,y are the source and target points in sets X and Y, respec-\ntively. Point sets are modeled as Gaussian mixture samples, with cor-\nrespondence probability matrix G computed as shown in equation (3):\ngij = exp\n−\n1\n2β2 ‖yyyi−yyyj‖2\n. (3)\nThis forms the basis for expectation–maximization steps, which \niterate until convergence. During the E-step, posterior probabilities P \nof correspondences update as given by equation (4):\nPmn = exp−\n1\n2σ2 ‖xxxn−(yyym+G(m,⋅)W)‖\n2\n∑\nM\nk=1exp−\n1\n2σ2 ‖xxxn−(yyyk+G(k,⋅)W)‖\n2\n+\nw\n1−w\n(2πσ2)\nD/2\nM\nN\n. (4)\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\nIn the M-step, W updates according to equations (5)–(7):\n(G+λσ2d(P1)\n−1\n)W= d(P1)\n−1\nPX−Y, (5)\nNp = 1TP1,T= Y+GW, (6)\nσ2 = 1\nNpD(tr(XTd(PT1)X)−2tr((PX)\nT\nT)+tr(TTd(P1)T)), (7)\nwhere the transformation weights W are constrained to 0≤ W≤ 1. \nParameters β> 0 controls transformation stiffness and the trade-off \nbetween data fidelity and smoothness, respectively.\nWe optimized CPD by adding the first two principal components \nof embeddings generated by OmiCLIP image encoder or text encoder, \nalong with the original two-dimensional coordinates. The M-step was \noptimized by updating only the coordinates to minimize loss. We fur-\nther calculated the homography matrix with translation and rotation \nbetween spots before and after alignment to avoid tremendous distor-\ntion. For PASTE, GPSA and CAST, we used their default configuration \nfor tissue preparation and alignment in Visium data.\nLoki Annotate\nBulk RNA-seq data. OmiCLIP enables zero-shot annotation by learn-\ning an aligned latent space for image and transcriptomic embeddings, \neliminating the need for retraining. We used OmiCLIP text encoder \nto encode bulk RNA-seq data and image encoder for H&E images, \nthen calculated cosine similarity between transcriptomic and image \nembeddings at spot level.\nMarker genes. Annotation was determined by selecting candidate \ntexts with the highest similarity score to image query. We evaluate \nthis using four datasets: CRC7K, LC25000, PatchCamelyon and WSSS-\n4LUAD. For Loki, text candidates were generated according to marker \ngenes of each tissue type (Supplementary Table 3). For the PLIP model, \ntext candidates were generated from tissue-type descriptions (Sup -\nplementary Table 3). The OmiCLIP image encoder encoded images \nresized to 20 × 20 pixels, consistent with its pretraining. OpenAI CLIP \nand PLIP models used their default configuration and functions for \nimage and text processing.\nMultimodal annotation. For jointly using Loki and PLIP, we summed \ntheir normalized similarity scores. Let, sLoki(I,T) and sPLIP(I,T) represent \nthe similarity scores between an image I and text T computed by Loki \nand PLIP, respectively. Normalized scores were obtained according to \nequations (8)–(10):\n̂sLoki(I,T) =\nsLoki(I,T)−min\nT′\nsLoki(I,T′)\nmax\nT′\nsLoki(I,T′)−min\nT′\nsLoki(I,T′), (8)\n̂sPLIP(I,T) =\nsPLIP(I,T)−min\nT′\nsPLIP(I,T′)\nmax\nT′\nsPLIP(I,T′)−min\nT′\nsPLIP(I,T′), (9)\nscombine(I,T) = ̂sLoki(I,T)+ ̂sPLIP(I,T). (10)\nThe candidate text T∗ with the highest combined similarity score \nwas identified as given by equation (11):\nT∗ = argmax\nT\nscombine(I,T). (11)\nLoki Decompose\nT o decompose human colorectal cancer slices, we fine-tuned OmiCLIP \nusing paired Visium ST data and H&E images. We then used fine-tuned \nOmiCLIP text encoder to encode scRNA-seq data and pseudo-Visium \nST data, and image encoder to encode H&E images. For in-house TNBC \nhuman samples, we fine-tuned OmiCLIP using a quarter of a region \n(top-right, top-left, bottom-right or bottom-left) of pseudo-Visium \nST data and H&E images, then encoded scRNA-seq data and ST data \nvia the text encoder and H&E images via the image encoder. Similarly, \nfor mouse brain cortex slices, we fine-tuned OmiCLIP using adjacent \nVisium ST data and H&E images, then encoded scRNA-seq data and \nH&E images accordingly.\nWe used a nonconvex optimization algorithm implemented by \nTangram to co-register OmiCLIP embeddings of scRNA-seq data with \nthose of ST data or H&E images. We aimed to obtain a probabilistic \nmapping matrix M aligning single cells to specific spots based on \nembedding similarities between scRNA-seq and ST data or scRNA-seq \nand H&E images. The mapping matrix M of dimensions spots-by-  \ncells quantifies the likelihood that a given single cell is located  \nwithin a particular spot. The scRNA-seq data matrix S is structured as \ncells-by-embeddings, while the ST data or H&E image matrix G is for-\nmatted as spots-by-embeddings. The optimal mapping matrix  \nM is derived by minimizing the loss function L(S,M) as shown in  \nequation (12):\nL(S,M) =\nnembeddings\n∑\nk\ncosdistance((MTS)∗,k,G∗,k). (12)\nHere, cosdistance denotes the cosine distance between OmiCLIP \nembeddings of the mapped single cells and those of ST data or H&E \nimages. The loss function aims to minimize the cosine distance \nbetween the projected single-cell embeddings MTS and the embed-\ndings of ST data or H&E images G, thereby ensuring that the embed-\ndings of the single cells, when mapped, resemble those observed in \nthe spatial data as closely as possible. Each element Mij in the matrix \nrepresents the probability that celli correspond to spotj, integrating \nthe cellular composition of the spatial spot. For Tangram, we used a \nuniform density prior for each spot without target count, aligning \nwith Loki Decompose. T o enhance efficiency, we adapted the mapping \nat the cell cluster level. The same settings were used for Loki, while \nSpatial Seurat, CARD, CytoSPACE, RCTD, Cell2location and spatialD-\nWLS utilized their default configurations and tissue preparation and \ndecomposition functions. For scGPT, scFoundation and GeneFormer, \nwe used default configuration and tissue preparation functions \nbefore using the Tangram method with same default configurations \nto decompose cell types. T o evaluate their performance, we used \ncell-type information from Xenium, Visium-HD and pathology annota-\ntion as ground truth.\nT o improve decomposition performance in regions with complex \ncellular heterogeneity, we developed a refinement strategy inspired \nby NMS 58. This method prioritizes the most probable cell type  \nwithin each spot, reducing overlapping or ambiguous assignments \nwhen multiple cell types have comparable probabilities. This  \nrefined method is recommended in complex spatial scenarios, such \nas colorectal cancer. For N total spots (indexed by i= 1,…, N), and C \ncell types, we defined Pi,c as the original probability of cell type  \nc at spot i . The NMS-based refinement follows two steps:  \nselecting the highest probability cell type and suppressing others. \nThe most likely cell type at each spot i  was determined as given  \nby equation (13):\nc∗\ni = argmax\nc∈C\nPi,c. (13)\nThen refined probabilities P(NMS)\ni,c  was defined according to equa-\ntion (14):\nP(NMS)\ni,c = {\nPi,c,ifc = c∗\ni ,\n0,otherwise.\n(14)\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\nThis NMS-based refinement ensured that only the cell type with \nthe highest likelihood remained at each spot, eliminating competing \nprobabilities and improving spatial decomposition accuracy.\nLoki Retrieve\nSimilarly to Loki Annotate, the retrieval results were decided by choos-\ning candidate transcriptomics with the highest similarity score to the \nimage query.\nk∗ = argmax\nk∈K\nsim(Iq,Ik). (15)\nHere, as shown in equation (15), K indicates the set of all pairs, Iq \nindicates the image embeddings of a given query, Ik indicates the \ntranscriptomics embeddings and k∗ indicates the candidate transcrip-\ntomics with the highest similarity score. We then calculated the similar-\nity between the embeddings of the query image and the image that is \npaired with the retrieved transcriptomics as the ground truth.\nLoki PredEx\nWe applied 10-fold cross-validation to evaluate Loki PredEx’s perfor-\nmance. In each fold, OmiCLIP was fine-tuned on the training set for \nten epochs, and then we used the fine-tuned OmiCLIP text encoder to \nencode the ST data of training sets and the image encoder to encode \nthe image of validation sets. For each spot in the validation set, cosine \nsimilarity between its image embeddings and all the transcriptomic \nembeddings in the training set was computed, and these weights were \nused to generate ST gene expression prediction for validation set spots \nvia a weighted average as given by equation (16):\nXi =\n∑jϵTwi, j ⋅Xj\n∑jϵTwi, j\n, (16)\nwhere T is the set of all spots in the training set, Xi is the predicted gene \nexpression for validation spot i , wi,j  is the similarity score between \nvalidation spot i and training spot j, and Xj is the gene expression for \ntraining spot j.\nT o benchmark performance, we compared Loki PredEx against \nHisT oGene, Hist2ST, BLEEP and mclSTExp, on the same dataset. In each \nfold, the top 300 expressed genes in the validation set were selected \nfor prediction. We followed default training settings: 100 epochs \nfor HisT oGene, 4 epochs for BLEEP, 90 epochs for mclSTExp and 110 \nepochs reduced from 350 due to computational resource constraints \nfor Hist2ST. By applying the same cross-validation procedure and \nevaluating the top 300 expressed genes in each fold, we ensured a fair \ncomparison between Loki PredEx and baseline models.\nEvaluation metrics and statistical analysis\nIn ‘OmiCLIP improves image and transcriptomics representations’ , we \nused the Leiden algorithm in Scanpy71 to cluster ST with default param-\neters including a resolution of 1 and a sparse adjacency matrix derived \nfrom neighbor connectivity. We then calculated the UMAP embeddings \nwith an effective minimum distance of 0.5 and three dimensions.\nThe CH score, also referred to as the variance ratio criterion, was used \nto evaluate clustering quality for a given dataset by comparing \nbetween-cluster dispersion and within-cluster dispersion. It was com-\nputed using two sets of ground truth, a benchmarked dataset containing \n95 tissue samples from the ST-bank, which included expert-annotated \ncell types (Supplementary Table 2) and the Leiden clustering (described \nabove) labels for samples without cell-type annotations. For a dataset \nwith n points {x1,…, xn} divided into k clusters {C1,…, Ck}, CH score is the \nratio normalized by the number of degrees of freedom for between-cluster \nand within-cluster dispersions, respectively, as given by equation (17):\nCH= BCSS/(k−1)\nWCSS/(n−k). (17)\nBetween-cluster sum of squares (BCSS) is calculated as the \nweighted sum of squared Euclidean distances from each cluster’s \ncentroid to overall centroid, as given by equation (18):\nBCSS=\nk\n∑\ni=1\nni‖ci −c‖2. (18)\nHere, ni is the number of points in cluster Ci, ci is the centroid of \ncluster Ci, and c is the overall centroid. BCSS quantifies separation \nbetween clusters, with higher value indicating better separation. \nWithin-cluster sum of squares (WCSS) measures the cohesion of the \nclusters with smaller values indicating tighter clustering and is the total \nsquared Euclidean distances from each data point to its cluster cen -\ntroid, as given by equation (19):\nWCSS=\nk\n∑\ni=1\n∑\nx∈Ci\n‖x−ci‖2. (19)\nThe PCC, which ranges from −1 to 1, assessed tissue alignment and \ngene expression prediction. Given paired data {(x1,y1),…, (xn,yn)} con-\nsisting of n pairs, PCC represented by rxy is defined in equation (20):\nrxy = n∑xiyi −∑xi∑yi\n√n∑x2\ni −(∑xi)\n2\n√n∑y2\ni −(∑yi)\n2\n, (20)\nwhere n is the sample size, and xi,yi are the individual sample points \nindexed with i.\nKendall’s tau coefficient, which ranges from −1 to 1, assessed tissue \nalignment, as given by equation (21):\nτ= P−Q\n√(P+Q+T)(P+Q+U)\n, (21)\nwhere P denotes the number of concordant pairs, Q is the number of \ndiscordant pairs, while T and U represent ties occurring solely in x or \nsolely in y, respectively.\nJS divergence, which ranges from 0 to 1, assessed cell-type decom-\nposition. T o calculate JS divergence between two probability distribu-\ntions P and Q, we first computed the pointwise average distribution, \nas given by equation (22):\nM= 1\n2(P+Q). (22)\nThen, we calculated Kullback–Leibler (KL) divergence of each \ndistribution with respect to M: DKL(P||M) and DKL(Q||M). KL divergence \nis a measure of how one probability distribution diverges from a second \ndistribution, as given by equation (23):\nKL(P||Q) = ∑P(x)log(P(x)\nQ(x)). (23)\nJS divergence is the average of these two KL divergences as given \nby equation (24):\nDJS(P||Q) = 1\n2DKL(P||M)+ 1\n2DKL(Q||M). (24)\nThe SSIM, which ranges from −1 to 1, assessed cell-type decomposi-\ntion, where we considered the cell-type distribution in spatial as image. \nFor two images x and y, as shown in equation (25):\nSSIM(x,y) =\n(2μxμy +C1)(2σxy +C2)\n(μ2\nx +μ2\ny +C1)(σ2\nx +σ2\ny +C2)\n. (25)\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\nwhere ux and uy are the mean intensities of images x and y, σ2\nx and σ2\ny are \nthe variances of x and y, σxy is the covariance between x and y, C1 and \nC2 are small constants to stabilize the division when the denominators \nare close to zero.\nMSE assessed ST gene expression prediction by comparing the \nEuclidean distance of the highly expressed gene expression between \nground truth and prediction for each method within the same location.\nThe impact score assessed the performance of cell-type decom-\nposition. For each decomposition method m, we computed the mean \nJS divergence, JSm, and the mean SSIM, SSIMm, across all cell types as \ngiven by equations (26) and (27):\nSSIMm = 1\nN\nN\n∑\nc=1\nSSIM( pc,qc), (26)\nJSm = 1\nN\nN\n∑\nc=1\nJS( pc,qc), (27)\nwhere pc and qc represent the ground truth and predicted proportions, \nrespectively. N represents the total number of cell types. We standard-\nized SSIM and JS divergence across methods to enable direct compari-\nson, as they operate on different scales. The standardized metrics ZSSIMm \nand ZJSm\n are calculated according to equation (28):\nZSSIMm = SSIMm −μSSIM\nσSSIM\n, (28)\nwhere μSSIM and σSSIM are the mean and standard deviation of SSIM \nacross methods. Because lower JS divergence indicates better perfor-\nmance, we inverted the standardized JS divergence values by multiply-\ning them by −1, as given by equation (29):\nZJSm\n=−\nJSm −μJS\nσJS\n, (29)\nwhere μJS and σJS are the mean and standard deviation of JS divergence \nacross methods. T o generate a unified metric for decomposition accu-\nracy, we averaged the inverted JS divergence z -scores and the SSIM \nz-scores for each method as given by equation (30):\nImpactscorem =\nZJSm\n+ZSSIMm\n2 . (30)\nF1 score, which ranges from 0 to 1, assessed zero-shot and linear \nprobing methods as given by equation (31):\nF1= 2×precision×recall\nprecision+recall = 2×TP\n2×TP+FP+FN. (31)\nHere, TP represents true positives, FP represents false positives \nand FN represents false negatives. A higher F1 score indicates better \noverall performance in classification tasks. The weighted F1 score was \ncalculated by averaging the F1 scores for each class, with each class’s \ncontribution weighted based on its frequency in the data.\nRecall@K assessed image-to-transcriptomics retrieval. Let Q be \nthe set of all queries, and N be the total number of queries. For each \nquery q∈ Q, the retrieval model outputs a ranked list of candidate \ntargets as given by equation (32):\nRq = [cq,1,cq,2,…, cq,i], (32)\nwhere cq,i is the ith highest-ranked candidate for query q based on  \ncosine similarity, and quantile(q) is the quantile of the smallest index i \nof the ground-truth target. Recall@K is defined as the fraction of  \nqueries for which the ground-truth target occurs at rank K or better as \ngiven by equation (33):\nRecall@K= 1\nN ∑\nq∈Q\nI[quantile(q) ≤ K], (33)\nwhere I[•] is an indicator function that takes the value of 1 if \nquantile(q) ≤ K and 0 otherwise.\nTwo-sided Student’s t-test and Wilcoxon rank-sum test were used \nto assess statistical significance between models.\nReporting summary\nFurther information on research design is available in the Nature \nPortfolio Reporting Summary linked to this article.\nData availability\nThe normal human small intestine dataset used for the tissue align -\nment task can be found in https://doi.org/10.1038/s41467-023-\n36071-5 (ref. 34). The human ovarian carcinosarcoma dataset used \nfor the tissue alignment task can be found at https://doi.org/10.1016/j.\nxgen.2021.100065 (ref. 35). The human breast cancer dataset used \nfor the tissue alignment task can be found at https://doi.org/10.1038/\ns41467-023-43458-x (ref. 37). The human colorectal cancer dataset \nincluding Visium, Visium-HD and scRNA-seq data of serial slices \nused for cell-type decomposition task can be found at https://doi.\norg/10.1101/2024.06.04.597233 (ref. 48). The TNBC scRNA-seq data \nused for the cell-type decomposition task can be found at https://doi.\norg/10.1038/s41467-018-06052-0 (ref. 51 ). The TNBC Xenium data \ngenerated in this study have been deposited in the Gene Expression \nOmnibus database under accession code GSE293199. The brain dataset \nincluding Visium data of serial slices used for cell-type decomposition \ntask can be found at https://doi.org/10.1038/s41587-021-01139-4 (ref. \n50). The brain scRNA-seq dataset used for cell-type decomposition \ntask can be found at https://doi.org/10.1038/s41586-018-0654-5 (ref. \n49). The histology images of the heart failure patient dataset used for \nthe tissue annotation task can be found at https://doi.org/10.1038/\ns41586-022-05060-x (ref. 80 ). The histology images of the normal \nhuman breast dataset used for the tissue annotation task can be found \nat https://doi.org/10.1038/s41586-023-06252-9 (ref. 79). The histology \nimages of TCGA BRCA dataset used for the tissue annotation task are \navailable from the NIH Genomic Data Commons (https://portal.gdc.\ncancer.gov/). The bulk RNA-seq data used for tissue annotation task \nare available from the Genotype-Tissue Expression Portal (https://\ngtexportal.org/home/) and TCGA (https://portal.gdc.cancer.gov/). \nCRC7k image patch data and labels can be found at Zenodo via https://\ndoi.org/10.5281/zenodo.1214456 (ref. 84). WSSS4LUAD image patches \nand labels can be found at https://wsss4luad.grand-challenge.org/. \nLC25000 image patches and labels can be found at https://github.com/\ntampapath/lung_colon_image_set/. PatchCamelyon image patches \nand labels can be found at https://patchcamelyon.grand-challenge.\norg/. The validation and test datasets used for the image–transcrip -\ntomics retrieval task can be found in Supplementary Table 4. The \nnormal human heart samples used for the ST gene expression predic-\ntion task can be found at https://doi.org/10.1038/s41586-023-06311-\n1 (ref. 81). The ST-bank database is available at https://github.com/\nGuangyuWangLab2021/Loki/.\nCode availability\nLoki is implemented in Python and is available via https://github.com/\nGuangyuWangLab2021/Loki/. The pretrained OmiCLIP weights are \navailable via https://huggingface.co/WangGuangyuLab/Loki/.\nReferences\n70. Hao, Y. et al. Dictionary learning for integrative, multimodal and \nscalable single-cell analysis. Nat. Biotechnol. 42, 293–304 (2024).\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\n71. Wolf, F. A., Angerer, P. & Theis, F. J. SCANPY: large-scale single-cell \ngene expression data analysis. Genome Biol. 19, 15 (2018).\n72. He, S. et al. Starfysh reveals heterogeneous spatial dynamics in \nthe breast tumor microenvironment. Preprint at bioRxiv  \nhttps://doi.org/10.1101/2022.11.21.517420 (2022).\n73. Barkley, D. et al. Cancer cell states recur across tumor types and \nform specific interactions with the tumor microenvironment. Nat. \nGenet. 54, 1192–1201 (2022).\n74. Garbarino, O. et al. Spatial resolution of cellular senescence \ndynamics in human colorectal liver metastasis. Aging Cell 22, \ne13853 (2023).\n75. Wu, R. et al. Comprehensive analysis of spatial architecture in \nprimary liver cancer. Sci. Adv. 7, eabg3750 (2021).\n76. Figiel, S. et al. Spatial transcriptomic analysis of virtual prostate \nbiopsy reveals confounding effect of tissue heterogeneity on \ngenomic signatures. Mol. Cancer 22, 162 (2023).\n77. Sountoulidis, A. et al. A topographic atlas defines developmental \norigins of cell heterogeneity in the human embryonic lung. Nat. \nCell Biol. 25, 351–365 (2023).\n78. Engblom, C. et al. Spatial transcriptomics of B cell and T cell \nreceptors reveals lymphocyte clonal dynamics. Science 382, \neadf8486 (2023).\n79. Kumar, T. et al. A spatially resolved single-cell genomic atlas of \nthe adult human breast. Nature 620, 181–191 (2023).\n80. Kuppe, C. et al. Spatial multi-omic map of human myocardial \ninfarction. Nature 608, 766–777 (2022).\n81. Kanemaru, K. et al. Spatially resolved multiomics of human \ncardiac niches. Nature 619, 801–810 (2023).\n82. Dosovitskiy, A. et al. An image is worth 16x16 words: transformers \nfor image recognition at scale. In International Conference on \nLearning Representations (OpenReview.net, 2020).\n83. Schuhmann, C. et al. Laion-5b: an open large-scale dataset for \ntraining next generation image-text models. Adv. Neural Inf. \nProcess. Syst. 35, 25278–25294 (2022).\n84. Kather, J. N., Halama, N. & Marx, A. 100,000 histological images of \nhuman colorectal cancer and healthy tissue. Zenodo https://doi.org/ \n10.5281/zenodo.1214456 (2018).\nAcknowledgements\nThis work was supported in part by the grant R35GM150460 (to G.W.) \nfrom the National Institute of General Medical Sciences (NIGMS) \nand grant R01HL169204-01A1 (to L.L.) from the National Institutes of \nHealth (NIH)-National Heart, Lung, and Blood Institute (NHLBI). K.W.B. \nis supported by NIH/National Institute of Neurological Disorders and \nStroke (NINDS) award K22 NS112678, NIH/National Cancer Institute \n(NCI) award R01 CA284315 and Cancer Prevention and Research \nInstitute of Texas (CPRIT) award RR220017. We acknowledge J. Chang \nin Houston Methodist Research Institute for support and assistance in \nfacilitating access to clinical resources essential to this study.\nAuthor contributions\nG.W. supervised the study. W.C. and G.W. designed and developed \nthe visual–omics foundation model and platform. W.C., P.Z., Y.X., T.T. \nand H.C. analyzed the data. G.W., W.C. and P.Z. wrote the manuscript. \nV.V.S., K.W.B., L.L., K.Y. and L.F. provided in-house patient tissues. K.Y. \nand Y.Y. annotated the pathology images. J.P.C., K.W.B., L.L., K.Y., L.F., \nQ.S., Q.M., K.C., N.-T.L., J.-i.A., Y.X., S.-H.C. and S.L. were involved in the \ndiscussion and helped improve the manuscript. All authors approved \nthe final version of the manuscript.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nExtended data is available for this paper at  \nhttps://doi.org/10.1038/s41592-025-02707-1.\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s41592-025-02707-1.\nCorrespondence and requests for materials should be addressed to \nGuangyu Wang.\nPeer review information Nature Methods thanks Spencer Krieger \nand the other, anonymous, reviewer(s) for their contribution to the \npeer review of this work. Peer reviewer reports are available. Primary \nHandling Editor: Rita Strack, in collaboration with the Nature Methods \nteam.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\nAirway smooth muscle\nArterial vessel\nCartilage\nGlands\nMultilayer epitheliu m\nNerve\nPerichondr ium\nWeird morphol ogy\nTranscriptomic embeddings Image embeddings\nCalinski-Harabasz score\nCalinski-Harabasz score\nBefore contrastive learning\nAfter contrastive learning\nBreast 1 (n=4)\nBreast 2 (n=4)\nHealthy Heart (n=39)\nKidney Cancer (n=5)\nKidney Cancer (Interface) (n=9)\nLung 1 (n=4)\nLung 2 (n=5)\nMI Heart (Control) (n=3) \nMI Heart (Fibrotic Zone) (n=5) \nMI Heart (Ischaemic Zone) (n=5) \nMI Heart (Remote Zone) (n=8) \nLung 3 (n=4)\na\nb\nImage embeddi ngsH&E staining image Spatial transcr iptomics Transcr iptomic embeddi ngs\nBefore contrastive learning\nLung\nCell type\nAfter contrastive learning Before contrastive learning After contrastive learning\nHealthy heart Kidney cancerMI heart\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nCell type Cell type Cell type Cell type\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nCell type Cell type Cell type Cell typeCell type\nCell type\nCell type\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nCell type Cell type Cell type Cell type\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nCell type Cell type Cell type Cell type\nBreast 1 (n=4)\nBreast 2 (n=4)\nHealthy Heart (n=39)\nKidney Cancer (n=5)\nKidney Cancer (Interface) (n=9)\nLung 1 (n=4)\nLung 2 (n=5)\nMI Heart (Control) (n=3) \nMI Heart (Fibrotic Zone) (n=5) \nMI Heart (Ischaemic Zone) (n=5) \nMI Heart (Remote Zone) (n=8) \nLung 3 (n=4)\n1000\n800\n200\n600\n400\n0\n1000\n800\n200\n600\n400\n0\n1200\nT cell\nCollagen secreting cell\nEndot helia l cell\nEpithelia l cell\nFibroblast\nMacrophage\nMast cell\nMonocyt e\nNative cell\nPlasma cell\nPodocyt e\nPrincipal cell\nAdipocyt e\nCardiomyocyte\nCycling cells\nEndot helia l\nFibroblast\nLymphoi d\nMast\nMyeloid\nNeuronal\nPericyte\nVSMCs\nCardiac skeleton\nFibrosis\nMyocardium atria\nMyocardium ventricular\nNode\nVessel\nExtended Data Fig. 1 | Image and transcriptomic representations.  \na, Clustering performance on ST-bank data with cell type annotation. Left: \nclustering performance using transcriptomic embeddings generated from \nOmiCLIP model before and after training. Right: clustering performance \nusings image embeddings from OmiCLIP model before and after training. The \nCalinski-Harabasz scores were calculated on the embeddings (Methods) using \nthe pretrained OmiCLIP transcriptomic (left) and image (right) encoders, \nevaluated for each organ type. Higher Calinski-Harabasz scores indicate better \nseparation capability between clusters of the embeddings. In the box plots, the \nmiddle line represents the median, the box boundaries indicate the interquartile \nrange, and the whiskers extend to data points within 1.5× the interquartile range. \nb, Image and transcriptomic embeddings of the lung, kidney cancer, healthy \nheart, and Myocardial Infarction (MI) heart samples. Each row corresponds to a \nWSI and showcases information from two modalities. The first column are H&E \nimages showing tissue morphology; the second column are the heatmaps of ST \ndata with the colors indicating the cell types; the third column are the UMAP of \nimage embeddings colored by cell types before and after contrastive learning; \nthe fourth column are the UMAP of transcriptomics embeddings colored by cell \ntypes before and after contrastive learning.\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\nSpina\nl Cord\nSmall intestine\nSkin\nHea\nrt\nOther\ns\nKidne\ny\nBrea\nst\nBrain\nLun\ng\nEmbryo\nLiver\nOvary\nColon\nProstate\nAdipo\nse\nUterus\nPan\ncrea\ns\nTon\nsil\nStomach\nCalinski-Harabasz scoreCalinski-Harabasz score\nTranscriptomic embeddings\nImage embeddings Before contrastive learning\nAfter contrastive learning\nProv-GigaPath\nUNI\na\nb Image embeddi ngsH&E staining image Spatial transcr iptomics Transcr iptomic embeddi ngs\nBefore contrastive learning\nSpinal\nCord\nLiver\nCancer\nBrain\nCancer\nKidney\nCancer\nLeiden\nclusters\nUMAP2\nUMAP1\nSkin\nCancer\nAfter contrastive learning Before contrastive learning After contrastive learning\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nUMAP2\nUMAP1\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\nLeiden\nclusters\n103\n102\n101\n100\n103\n102\n101\n100\nExtended Data Fig. 2 | See next page for caption.\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\nExtended Data Fig. 2 | Image and transcriptomic representations analysis. \na, Clustering performance on all ST-bank data. T op: clustering performance \nusing transcriptomic embeddings generated from OmiCLIP model before and \nafter training. Bottom: clustering performance usings image embeddings from \nOmiCLIP model before and after training, and image embeddings generated \nfrom UNI and Pro-GigaPath, respectively. The Calinski-Harabasz scores were \ncalculated on the embeddings using the pre-trained OmiCLIP transcriptomic \n(top) and image (bottom) encoders, evaluated for each organ type. Higher \nCalinski-Harabasz scores indicate better separation capability between clusters \nof the embeddings. In the box plots, the middle line represents the median, the \nbox boundaries indicate the interquartile range, and the whiskers extend to \ndata points within 1.5× the interquartile range. Sample sizes are skin: 163, brain: \n119, breast: 97, heart: 73, kidney: 73, embryo: 73, others: 64, liver: 57, prostate: \n49, spinal cord: 44, ovary: 32, colon: 29, pancreas: 25, lung: 22, tonsil: 18, uterus: \n17, adipose: 15, small intestine: 14, and stomach: 12. b, Image and transcriptomic \nembeddings of the spinal cord, liver cancer, brain cancer, kidney cancer and skin \ncancer samples. Each row corresponds to a WSI and showcases information from \ntwo modalities. The first column are H&E images showing tissue morphology; the \nsecond column are the heatmaps of ST data with the colors indicating the ST data \nclustering using Leiden algorithm (Methods); the third column are the UMAP of \nimage embeddings colored by ST Leiden clusters before and after contrastive \nlearning; the fourth column are the UMAP of transcriptomics embeddings \ncolored by ST Leiden clusters before and after contrastive learning.\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\nLow quality region\nOriginal image\nSimulated low quality image\nLoki (original image)\nSimilarity\nLoki (simulated low quality image)\nPLIP\nOpenAI CLIP\na b\nLoki (original transcriptome)\nLoki (down sampled from high to middle)\nPLIP\nOpenAI CLIP\nSimilarity\nLoki (down sampled from middle to low)\nLoki (down sampled from high to low)\nc\n0.5\n0.4\n0.3\n0.2\n0.50\n0.45\n0.30\n0.25\n0.40\n0.35\n0.20\n0.15\nExtended Data Fig. 3 | OmiCLIP’s robustness for image quality and sequencing \ndepth. a, Example image with low-quality region marked in red line and \nsimulated low-quality image by adding Gaussian noise. b, Cosine similarity of \npaired transcriptomic and image embeddings using OmiCLIP (original image \nand simulated low-quality image), PLIP (original image), and OpenAI CLIP \n(original image). In the box plots, the middle line represents the median, the \nbox boundaries indicate the interquartile range, and the whiskers extend to \ndata points within 1.5× the interquartile range. Sample sizes are 10 for each \nsimulated condition. c, Cosine similarity of the paired image with transcriptomic \nembeddings using OmiCLIP (original transcriptomes and down sampled \ntranscriptome from high sequencing depth to middle sequencing depth, middle \nsequencing depth to low sequencing depth, and high sequencing depth to low \nsequencing depth, respectively), PLIP (original transcriptome), and OpenAI CLIP \n(original transcriptome). In the box plots, the middle line represents the median, \nthe box boundaries indicate the interquartile range, and the whiskers extend to \ndata points within 1.5× the interquartile range, n = 500.\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\n2 mm\nXenium data (source) Visium data (target) Xenium data (Loki Align)\n2 mm 2 mm\na b c\nExtended Data Fig. 4 | Visium and Xenium tissue alignment. Tissue alignment results on breast cancer sample using Loki Align. a, Source Xenium ST data. b, Target \nVisium ST data. c, Xenium ST data after Loki alignment.\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\nKRT7H&E image\n ACTG2 cell type\nRORCb\na\nRCTD\ncEpithelialImmuneStroma\nGene expression\nEpithelia l\nImmune\nStroma\n-1 1\nZ-score\nCARD scGPT Spatial Seurat scFoundation GeneFormer CytoSpace Cell2location spatialDWLS\nlouvain cell type\nEpithelial\nImmune\nStroma\nEpithelia l\nImmune\nStroma\n1.0\n0.8\n0.2\n0.6\n0.4\n0.0\nKLF5\nEPC AM\nEGFR\nCDH1\nTPD52\nERBB2\nCCND1\nACTA2\nACTG2\nLYZ\nMMP 2\nZEB2\nPDGFRB\nPPAR G\nMZB1\nRORC\nFOXP3\nCXCL12\nPRDM 1\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nEPC AM\nEGFR\nCDH1\nKLF5\nCCND1\nTPD52\nERBB2\nKRT7\nACTA2\nLYZ\nMMP 2\nPDGFRB\nCD4\nIL2R A\nCD3 E\nMZB1\nITGAX\nITGAM\nEpithelia l\nT Cell\nStroma\nB Cell\nSER PINA3\nACTG2\nZEB2\nMacrophage\nGene expression\nlow high\nFraction of cells \nin group (%)\n0.0 0.5 1.0\nMean expression \nin group\n20 40 60 80100\nExtended Data Fig. 5 | Cell type decomposition of TNBC case study. a, Xenium \ndata from our in-house TNBC patient sample, colored by Louvain clusters and cell \ntypes, respectively. b, H&E image, marker gene expression (KRT7, ATCG2, RORC), \nand cell type distribution in an example zoom-in region of the TNBC sample.  \nc, Cell type decomposition results on 3 major cell types of the TNBC sample \nusing ST by RCTD, CARD, scGPT, Spatial Seurat, scFoundation, GeneFormer, \nCytoSPACE, Cell2location, and SpatialDWLS, respectively. The color of the \nheatmap reflects the z-score, calculated by the enrichment of each cell type.\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\nCRC scRNA-seq OmiCLIP embedding\nb Tumor Fibroblast Smooth muscle Intestinal epithelial\nGround truthLoki image decomposition\nLow High\nProbability\nLoki ST decomposition\nUMAP2\nUMAP1\na\nB cells\nEndothelial\nFibroblast\nIntestinal Epithelial\nMyeloid\nNeuronal\nSmooth Muscle\nT cells\nTumor\nExtended Data Fig. 6 | Cell type decomposition of colorectal case study. a, \nUMAP representation of the OmiCLIP transcriptomic embeddings colored by \ncell types, where each dot represents a spot. b, Cell type decomposition result \nusing Loki ST decomposition and Loki image decomposition respectively on \nhuman colorectal sample within the Visium HD capture area, and ground truth. \nHeatmap shows the cell type distribution of tumor, fibroblast, smooth muscle, \nand intestinal epithelial, respectively, with color reflecting the probability of \neach cell type.\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\nSSIM JS Divergence\nLoki Decompose\n Image to ST \n(fine-tuning)\na\nb\nLoki Decompose\n Image to ST \n(pre-training)\nLoki Decompose\n Image to ST\n(train from scratch)\nImage\n to ST (fine-tuning)\nImage\n to ST (pre-training)\nImage\n to ST (train from scratch)\nEpithelial Immune Stroma\nImage\n to ST (fine-tuning)\nImage\n to ST (pre-training)\nImage\n to ST (train from scratch)\n-1 1\nZ-score\n0.35\n0.30\n0.15\n0.10\n0.25\n0.20\n0.00\n-0.05\n0.05\n0.5\n0.2\n0.1\n0.4\n0.3\n0.0\nExtended Data Fig. 7 | Cell type decomposition of fine-tuning, pre-training, \nand train from scratch. a, Cell type decomposition results on 3 major cell types \nof the TNBC sample using Loki Decompose Image-to-ST (fine-tuning, pre-\ntraining, and train from scratch). The color of the heatmap reflects the z-score, \ncalculated by the enrichment of each cell type. b, Bar plot shows the accuracy \nof decomposition of 3 major cell types by Loki Decompose Image-to-ST (fine-\ntuning, pre-training, and train from scratch). Error bar is standard deviation with \ncenter measured by mean.\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\nGround Truth Loki Hist2ST HisToGene\nTAGLNAPODMYH7C1QA\nH&E Image\nGene expression\nLow High\nmclSTExp BLEEP\nExtended Data Fig. 8 | Examples of ST gene expression prediction. H&E images, ground truth ST gene expression, and ST gene expression predicted by Loki, Hist2ST, \nHisT oGene, BLEEP, and mclSTExp, respectively.\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1 MSE\nLoki HisToGene mclSTExp BLEEP Hist2ST\nPCC\nLoki HisToGene mclSTExp BLEEP Hist2ST\na\nBLEEP\nHist2ST\nHisToGene\nmclSTExp\nLoki\nMSE\nPCC\nSample 1\nSample 2\nSample 3\nSample 4\nSample 5\nSample 6\nSample 7\nSample 8\nSample 9\nSample 10\nSample 11\nSample 12\nSample 13\nSample 14\nSample 15\nSample 16\nSample 17\nSample 18\nSample 19\nSample 20\nSample 21\nSample 22\nSample 23\nSample 24\nSample 25\nSample 26\nSample 27\nSample 28\nSample 29\nSample 30\nSample 31\nSample 32\nSample 33\nSample 34\nSample 35\nSample 36\nSample 37\nSample 38\nSample 39\nb\n1.0\n0.8\n0.2\n0.6\n0.4\n0.0\n1.0\n0.5\n-1.0\n0.0\n-0.5\n1.00\n0.75\n0.00\n-0.25\n0.50\n0.25\n-0.50\n-0.75\n-1.00\n0.7\n0.6\n0.3\n-0.2\n0.5\n0.4\n-0.1\n0.0\nExtended Data Fig. 9 | Comparison of ST gene expression prediction \nperformances. a, Comparison of ST gene expression prediction performances, \nrepresented by MSE and PCC respectively on 39 normal heart tissues using Loki, \nHist2ST, HisT oGene, BLEEP, and mclSTExp, respectively. In the box plots, the \nmiddle line represents the median, the box boundaries indicate the interquartile \nrange, and the whiskers extend to data points within 1.5× the interquartile range. \nb, Summarized comparison of ST gene expression prediction performances, \nrepresented by MSE and PCC respectively across all samples using Loki, \nHisT oGene, mclSTExp, BLEEP, and Hist2ST respectively. In the box plots, the \nmiddle line represents the median, the box boundaries indicate the interquartile \nrange, and the whiskers extend to data points within 1.5× the interquartile range.\nNature Methods\nArticle https://doi.org/10.1038/s41592-025-02707-1\nFunction Software Fine-tune\nLoki Align H&E image to ST \nalignment Loki only Suggested\nLoki\nPASTE\nLoki Decompose scRNA-seq to H&E image Loki only Suggested\nLoki\nTangram\nLoki Annotate Tissue annotation by Bulk \nRNA-seq Loki only No need\nLoki Annotate Tissue annotation by \nmarker genes Loki only No need\nLoki Retrieve H&E image-to-ST retrieval Loki only No need\nLoki\nHisToGeneLoki PredEx ST gene expression \nprediction by H&E image Suggested\nLoki Align ST to ST alignment No need\nLoki Decompose scRNA-seq to ST mapping No need\nExtended Data Fig. 10 | Summary of the fine-tuning settings for downstream tasks. Recommendation settings for downstream tasks.\n\n\n",
  "topic": "Transcriptome",
  "concepts": [
    {
      "name": "Transcriptome",
      "score": 0.8407529592514038
    },
    {
      "name": "Annotation",
      "score": 0.5981900095939636
    },
    {
      "name": "Computational biology",
      "score": 0.5588515996932983
    },
    {
      "name": "Biology",
      "score": 0.519398033618927
    },
    {
      "name": "RNA-Seq",
      "score": 0.5065058469772339
    },
    {
      "name": "Snapshot (computer storage)",
      "score": 0.49759986996650696
    },
    {
      "name": "Genomics",
      "score": 0.42267942428588867
    },
    {
      "name": "Computer science",
      "score": 0.3743579387664795
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3544861078262329
    },
    {
      "name": "Bioinformatics",
      "score": 0.3455617427825928
    },
    {
      "name": "Gene",
      "score": 0.3235176205635071
    },
    {
      "name": "Genome",
      "score": 0.25572705268859863
    },
    {
      "name": "Gene expression",
      "score": 0.22348365187644958
    },
    {
      "name": "Genetics",
      "score": 0.17994263768196106
    },
    {
      "name": "Database",
      "score": 0.0912962257862091
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1295876152",
      "name": "Houston Methodist",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I52357470",
      "name": "The Ohio State University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I33213144",
      "name": "University of Florida",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1343551460",
      "name": "The University of Texas MD Anderson Cancer Center",
      "country": "US"
    }
  ],
  "cited_by": 16
}