{
  "title": "Evaluating the effectiveness of XAI techniques for encoder-based language models",
  "url": "https://openalex.org/W4406751855",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5093470856",
      "name": "Melkamu Abay Mersha",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4208318287",
      "name": "Mesay Gemeda Yigezu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2344103259",
      "name": "Jugal Kalita",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2981731882",
    "https://openalex.org/W4230575913",
    "https://openalex.org/W3131457744",
    "https://openalex.org/W6911428550",
    "https://openalex.org/W4294559022",
    "https://openalex.org/W2282821441",
    "https://openalex.org/W6639204139",
    "https://openalex.org/W6734194636",
    "https://openalex.org/W1787224781",
    "https://openalex.org/W6754655096",
    "https://openalex.org/W4386142022",
    "https://openalex.org/W4401009060",
    "https://openalex.org/W6783721678",
    "https://openalex.org/W4392305894",
    "https://openalex.org/W2962858109",
    "https://openalex.org/W3208147779",
    "https://openalex.org/W4393950444",
    "https://openalex.org/W2195388612",
    "https://openalex.org/W6810032775",
    "https://openalex.org/W6753160485",
    "https://openalex.org/W3035574324",
    "https://openalex.org/W2979200397",
    "https://openalex.org/W6745730051",
    "https://openalex.org/W3163443091",
    "https://openalex.org/W3212053370",
    "https://openalex.org/W4391741433",
    "https://openalex.org/W6762319049",
    "https://openalex.org/W4321786089",
    "https://openalex.org/W4399919514",
    "https://openalex.org/W6749067469",
    "https://openalex.org/W4297252763",
    "https://openalex.org/W3125997628",
    "https://openalex.org/W1965555277",
    "https://openalex.org/W3176196997",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W4299959846",
    "https://openalex.org/W4240030094",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W3035503910",
    "https://openalex.org/W3036453007",
    "https://openalex.org/W3034917890",
    "https://openalex.org/W2970014349",
    "https://openalex.org/W2997416769",
    "https://openalex.org/W2951936329",
    "https://openalex.org/W4392933350",
    "https://openalex.org/W2962851944",
    "https://openalex.org/W2963125461",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4391709753",
    "https://openalex.org/W3176820529",
    "https://openalex.org/W4403884126",
    "https://openalex.org/W4393976865",
    "https://openalex.org/W2773497437"
  ],
  "abstract": null,
  "full_text": "Evaluating the Effectiveness of XAI Techniques for Encoder-Based Language Models\nMelkamu Abay Mershaa, Mesay Gemeda Yigezub, Jugal Kalitaa\naCollege of Engineering and Applied Science, University of Colorado Colorado Springs, , 80918, CO, USA\nbInstituto Polit´ ecnico Nacional (IPN), Centro de Investigaci´ on en Computaci´ on (CIC), , 07738, Mexico city, Mexico\nAbstract\nThe black-box nature of large language models (LLMs) necessitates the development of eXplainable AI (XAI) techniques for trans-\nparency and trustworthiness. However, evaluating these techniques remains a challenge. This study presents a general evaluation\nframework using four key metrics: Human-reasoning Agreement (HA), Robustness, Consistency, and Contrastivity. We assess the\neffectiveness of six explainability techniques from five different XAI categories—model simplification (LIME), perturbation-based\nmethods (SHAP), gradient-based approaches (InputXGradient, Grad-CAM), Layer-wise Relevance Propagation (LRP), and atten-\ntion mechanisms-based explainability methods (Attention Mechanism Visualization, AMV)—across five encoder-based language\nmodels: TinyBERT, BERTbase, BERTlarge, XLM-R large, and DeBERTa-xlarge, using the IMDB Movie Reviews and Tweet\nSentiment Extraction (TSE) datasets. Our findings show that the model simplification-based XAI method (LIME) consistently out-\nperforms across multiple metrics and models, significantly excelling in HA with a score of 0.9685 on DeBERTa-xlarge, robustness,\nand consistency as the complexity of large language models increases. AMV demonstrates the best Robustness, with scores as low\nas 0.0020. It also excels in Consistency, achieving near-perfect scores of 0.9999 across all models. Regarding Contrastivity, LRP\nperforms the best, particularly on more complex models, with scores up to 0.9371.\nKeywords: XAI, explainable artificial intelligence, interpretable deep learning, explainable machine learning, evaluation\nframework, evaluation metrics, large language models, LLMs, interpretability, natural language processing, NLP, explainability\ntechniques, black-box models.\n1. Introduction\nThe exponential growth in the capabilities of powerful large\nlanguage models (LLMs) such as GPT [1], BERT [2], and\ntheir derivatives has revolutionized various domains, including\nsafety-critical applications. However, these models are com-\nplex and opaque, posing significant challenges in understanding\ntheir decision-making processes and their internal workings. It\nis critically important to comprehend the underlying principles\nbehind the decisions of these architecturally complex models.\nAs these models become more sophisticated and are deployed\nacross broader applications, the need for clear and interpretable\nexplanations of their decision-making processes becomes in-\ncreasingly essential for discovering potential flaws or biases [3],\nenhancing user trust [4], facilitating regulatory compliance [5],\nand guiding the responsible integration of AI models into di-\nverse sectors [6], [4].\nExplainable Artificial Intelligence (XAI) techniques are the\nkey to unlocking the reasons behind a model’s decision-making\nprocess. They provide crucial insights into how and why a\nmodel arrives at a particular decision, bridging the gap between\ncomplex AI models and human understanding [7], [8]. Post-\nhoc XAI methods can be employed to analyze and interpret\ntrained models, providing explanations for their decisions with-\nout modifying the underlying model structure. Based on their\ndesign and functionality, these post-hoc XAI techniques can be\ncategorized into model simplification approaches, which cre-\nate interpretable forms of complex models [9]; perturbation-\nbased methods, which alter inputs to identify influential fea-\ntures [10]; gradient-based approaches, which use gradients to\nassess feature contributions [11]; Layer-wise Relevance Propa-\ngation (LRP) approaches, which trace decisions back through\nmodel layers to assign relevance scores [12]; and attention\nmechanisms techniques, which highlight the key input fea-\ntures affecting model predictions based on the attention weights\n[13]. In addition to these categories, several other post-hoc\nXAI methods exist, such as generating natural language expla-\nnations, which provide narrative interpretations of model deci-\nsions [14].\nThe growing number of explainability techniques in each\nXAI category often produce varying and sometimes contradic-\ntory explanations for the same input and model, complicating\nthe determination of accuracy [15]. Explanations may lack con-\nsistency across different situations due to changes in model be-\nhavior, making validation and verification challenging. Not all\nXAI methods suit every model architecture or complexity, high-\nlighting the need for systematic assessment and automated se-\nlection of suitable techniques [16]. Although previous studies\nhave used various metrics to evaluate XAI e ffectiveness [17],\ninconsistencies among these studies result in a lack of standard-\nized criteria for comparison [16]. Additionally, these studies\noften overlook crucial factors like model complexity, language\ndiversity, and application domain [18, 19], and most current\nmetrics emphasize feature salience scores, potentially leading\nto misleading results when identical scores are assigned to dif-\n1\narXiv:2501.15374v1  [cs.CL]  26 Jan 2025\nferent features.\nIn our study, we introduce new text ranking-based met-\nrics alongside saliency scores and develop a robust evalua-\ntion framework that integrates the strengths of existing meth-\nods while addressing their limitations. This framework focuses\non critical factors such as model complexity, a broad range of\ndownstream tasks, diverse explainability categories, and the dy-\nnamic behavior of models over time. Unlike most existing stud-\nies, which primarily focus on a limited set of aspects, our ap-\nproach comprehensively considers various explainability meth-\nods, downstream tasks, model architectures, language diversity,\nand domain-specific requirements to ensure a meaningful eval-\nuation of XAI techniques.\nWe establish a general evaluation framework to assess XAI\nmethods across di fferent AI models, proposing metrics that\naccount for the strengths and limitations of XAI methods\nin encoder-based language models. The framework includes\nwidely applicable XAI methods from five distinct categories,\nevaluated on five models with varying complexities across\ntwo text classification tasks—short and long text inputs. Our\nevaluation, conducted on five encoder-only language mod-\nels—TinyBERT, BERTbase, BERTlarge, XLM-R large, and\nDeBERTa-xlarge—provides a nuanced understanding of how\ndifferent XAI techniques perform across various models and\nscenarios. The framework and metrics are designed to be ap-\nplicable to a range of XAI techniques, encoder-based language\nmodels, and downstream tasks.\nThe contributions of this study are:\n• We propose a comprehensive end-to-end evaluation frame-\nwork to assess the effectiveness of XAI techniques across\nvarious machine learning, including transformer-based\nmodels.\n• We develop four evaluation metrics based on saliency to-\nken ranking and saliency score approaches for evaluating\nXAI techniques in encoder-based language models.\n• We analyze and compare five XAI categories across five\nencoder-based language models using these metrics on\ntwo downstream tasks.\n• We evaluate our XAI framework on two text classification\ntasks under five XAI categories and five encoder-based\nlanguage models.\n• We compare and contrast explanations with human ratio-\nnales to assess alignment with models’ decision-making\nprocesses.\n• We provide comparative analyses to guide the selection of\nsuitable XAI techniques for di fferent encoder-based lan-\nguage models in downstream tasks.\nThe remainder of this paper is organized as follows: Section\n2 reviews the related work on XAI techniques and evaluation\nmethods. In Section 3, we present the methodology, detail-\ning the proposed evaluation framework and metrics. Section\n4 describes the experimental setup, including datasets, mod-\nels, and the selected XAI techniques. In Section 5, we discuss\nthe results of the evaluation, providing insights into the e ffec-\ntiveness of different XAI methods across various encoder-based\nlanguage models. Section 6 presents the limitations and future\nwork. Finally, Section 7 concludes the paper, summarizing our\nfindings and suggesting directions for future research.\n2. Background and Related Works\n2.1. Background\nExplainable AI (XAI) has become essential to artificial intel-\nligence and machine learning, especially with the rise of com-\nplex models like Large Language Models such as GPT [1],\nBERT [2], and various transformer-based architectures. These\nmodels have performed exceptionally well in diverse natural\nlanguage processing tasks. However, their complexity makes\nthem operate largely as black boxes, presenting considerable\nchallenges in understanding their internal decision-making pro-\ncesses. This opacity has critical implications for trustworthi-\nness, fairness, and accountability, especially in high-stakes do-\nmains such as healthcare, finance, and law. To address these\nissues, XAI techniques have been developed to provide insights\ninto AI models’ decision-making processes, o ffering explana-\ntions that enhance transparency and trust.\nThe urgency for evaluating XAI techniques on LLMs stems\nfrom the need to ensure that explanations are accurate and prac-\ntically useful across di fferent languages and NLP tasks. How-\never, most existing XAI techniques were not developed with\nLLMs in mind. Consequently, they may struggle to provide\ncoherent and comprehensive interpretations for these complex\narchitectures, potentially requiring adaptations or entirely new\nmethodologies that can handle the unique requirements of LLM\ninterpretability. The need for new methodologies to handle the\nunique requirements of LLM interpretability is urgent and can-\nnot be overstated.\n2.2. XAI Techniques\nXAI techniques are generally categorized into two cate-\ngories: ante-hoc methods, which aim to be applied during\nmodel training, and post-hoc methods, which are applied after\ndeployment to explain model predictions. Given LLMs’ com-\nplexity, post-hoc techniques are often preferred as they o ffer\nflexibility for explaining pre-trained models. In post-hoc ex-\nplainability, methods are further divided into model-agnostic\ntechniques, which can be applied to any black-box model, and\nmodel-specific techniques, designed for specific architectures.\nPost-hoc explainability techniques are broadly categorized\ninto five main groups based on their functionalities and design\nmethodologies. Model simplification techniques, such as LIME\n[9], are model-agnostic, and they simplify complex models\ninto more interpretable ones. Perturbation-based techniques,\nlike SHAP [20], are model-agnostic and modify feature val-\nues to measure their impact on predictions. Gradient-based\ntechniques, such as Integrated Gradients [11], Grad-CAM [21],\nand Saliency Maps [22], explain models by analyzing gradients.\nGradient-based XAI methods for Transformer models in NLP\ntasks analyze the influence of input tokens on predictions by\ncomputing gradients of the model’s output with respect to in-\nput tokens [23], [24], [25]. Layer-wise Relevance Propagation\n(LRP) techniques assign predictions to input features by redis-\ntributing scores backward through the model’s layers [26], [27].\n2\nLRP, originally developed for image processing tasks, has been\nsuccessfully adapted to NLP tasks to enhance interpretability\nin models like BERT. In the context of NLP, LRP identifies\nthe contributions of individual input tokens to a model’s final\nprediction by tracing their influence through the model’s lay-\ners [28], [29], [24]. Attention mechanism techniques , such as\nAttention Rollout [30] and Attention Mechanism Visualization\n[13], visualize influential input features by analyzing attention\nweights.\n2.3. Related Works\nVarious XAI methods have been proposed to improve inter-\npretability in AI models, yet their e ffectiveness varies widely\nacross models, requiring systematic assessment. Some ap-\nproaches, like counterfactual explanations, aim to expose model\nreasoning by revealing instances that would lead to di fferent\npredictions [31, 32]. However, these methods often fall short in\nproviding holistic model explanations [33]. Other studies have\nrelied on human evaluation to assess the quality of XAI out-\nputs, but this is often subjective and can vary significantly be-\ntween users [34, 35]. Alternative evaluation approaches, such\nas Ground Truth Correlation, have emerged, comparing human-\nidentified salient features with those generated by XAI methods\n[36, 17]. Another method evaluates the su fficiency of expla-\nnations by removing the most salient tokens identified by an\nXAI technique and observing the impact on model performance\n[17]. While insightful, such metrics may not fully capture\nan XAI method’s effectiveness for highly parameterized LLMs\ndue to their limited scalability and focus on simpler models like\nrandom forests and SVMs [37]. Studies on CNN, LSTM, and\nBERT have further evaluated model simplification, perturba-\ntion, and gradient-based techniques, demonstrating their inter-\npretative potential [18].\nMost existing studies on evaluating explainability techniques\nvary widely in scope, covering di fferent model architectures,\nmethods, and tasks. Some focus on single architectures with\nmultiple explainability methods [17], while others examine a\nrange of models like CNN, LSTM, and BERT, using vari-\nous techniques such as model simplification, perturbation, and\ngradient-based approaches [18], [38]. Many studies are also\ndataset-specific [39], limiting the generalizability of their find-\nings across domains and languages. This emphasis on high-\nresource languages highlights a gap in understanding XAI tech-\nniques for under-resourced languages, impacting the fairness\nand accessibility of AI globally. Additionally, most research\noverlooks the influence of model complexity on the e ffective-\nness of explainability methods. Transformer models, for in-\nstance, vary significantly in complexity, from millions to bil-\nlions of parameters [40], [41]. Current research does not suf-\nficiently explore how explainability methods perform across\nthese complexities or which methods are best suited for dif-\nferent levels of model complexity.\nThese limitations highlight the necessity of our comprehen-\nsive study, which systematically evaluates the e ffectiveness of\nXAI methods across LLMs with varying complexities. Our re-\nsearch addresses gaps in previous studies by integrating four\nrobust evaluation metrics to thoroughly assess how di fferent\nlevels of encoder-based language model complexity influence\nthe performance of XAI techniques. By adopting this holistic\napproach, we aim to identify and recommend the most e ffec-\ntive XAI categories and techniques tailored to specific encoder-\nbased language model complexities, ensuring their optimal ap-\nplication across diverse models and tasks.\n3. Methodology\nAn XAI evaluation framework has manifold applications, of-\nfering significant benefits across various AI models and XAI\ntechniques development and deployment dimensions. Exist-\ning XAI evaluation frameworks are often simple and typically\nfocus on limited tasks, a narrow range of AI models and ex-\nplainability techniques, and evaluation metrics [37], [18], [17].\nThese frameworks lack a comprehensive and standardized ap-\nproach for evaluating the e ffectiveness of XAI methods across\ndiverse contexts and applications. No general and standardized\nevaluation framework systematically assesses the performance\nof XAI techniques in a way that meets the needs of di fferent\nstakeholders. Our new comprehensive XAI evaluation frame-\nwork overcomes these limitations by incorporating a diverse\narray of datasets, covering a wide range of tasks, supporting\nvarious AI architectures, including neural networks and trans-\nformer models, and employing a variety of XAI methods and\nvarious evaluation metrics; see Figure 1. This comprehen-\nsive framework is used to rigorously evaluate and compare XAI\nmethods across different scenarios. The framework is designed\nto be easy to understand and extend, allowing for the incor-\nporation of new datasets, tasks, AI models, XAI methods, and\nevaluation metrics as the field of XAI evolves. This holistic ap-\nproach makes our framework exceptionally suited for in-depth\nevaluations of XAI techniques.\nFigure 1: An overview of our comprehensive XAI evaluation framework for as-\nsessing the effectiveness of explainability techniques across different scenarios.\n3.1. Evaluation Metrics\nEvaluation metrics for XAI techniques are crucial as they\nprovide quantitative measures to assess the quality and reliabil-\nity of explanations [42], [43]. These metrics ensure that expla-\nnations accurately reflect the model’s behavior. They also en-\nable the systematic comparison and improvement of XAI meth-\nods, ensuring that AI models’ decisions are transparent and re-\nliable across various applications. We build a comprehensive\nevaluation framework to evaluate various XAI categories by\nadopting and enhancing the existing metrics, and introducing\nnew metrics.\n3\n3.1.1. Human-reasoning Agreement (HA)\nThe HA metric measures the degree of alignment between\nthe explanations provided by an explainability technique and\nhuman intuition or reasoning [17], [18]. It evaluates how\nclosely a model’s reasoning or explanation matches human\njudgment, with a human-annotated dataset serving as the base-\nline for this metric. However, it is important to note that the as-\nsumptions regarding the high degree of agreement between the\nfeature importance scores (such as word saliency scores in this\nstudy) provided by explainability techniques and those from\nhuman-annotated datasets are not always valid. For instance,\nwhile the saliency scores for words may be similar, the specific\nwords identified as salient may differ, which is a significant lim-\nitation of previous studies [18]. Using measurements like co-\nsine similarity, Pearson correlation, and intersection-over-union\nto compute the agreement between human and explainability\nword saliency scores with this limitation may not always be\npractical. This saliency approach does not adequately reflect the\nrank or relevance of words to the decision-making process. To\naddress this limitation, we employed token /word ranking and\nMean Average Precision (MAP) methods to assess the level of\nagreement between human-annotated and explainability-based\nexplanations. Initially, we ranked the significant tokens /words\nfor the model’s decision-making process based on saliency\nscores from both perspectives to compute the Average Preci-\nsion (AP) and MAP.\nMean Average Precision (MAP)During our evaluation, we\nutilize the AP and MAP of the ranked words/tokens to precisely\nmeasure the agreement between human rationales and the ex-\nplanations generated by the explainability technique and math-\nematically represented by Equations 1 and 2, respectively, this\nmetric provides a clear understanding of the alignment between\nhuman-annotated and explainability explanations. Average pre-\ncision for a single instance evaluates the importance of salient\nwords identified in the explanation compared to those identi-\nfied by human annotators, aiding decision-making processes. It\nis computed as:\nAP =\nPn\nk=1(P(k) ×rel(k))\nNumber of relevant tokens (n) (1)\nWhere k represents the rank or position of a word in the se-\nquence of retrieved words, n is the total number of retrieved\nwords, P(k) is the precision at rank k in the ranked word list,\nand rel(k) is a binary indicator function. rel(k) = 1 if the word at\nrank or position k from the explainability explanation matches\nthe corresponding word in the human annotation, otherwise\nrel(k) = 0, Equation 3. The ranking of words from the explain-\nability method is automatically determined based on relevance\nscores computed by the explainability technique. In contrast,\nhuman annotators provide a ranked order of words directly in\ntheir explanation rather than assigning explicit relevance scores\nonly. An example is available in Appendix A.\nAP measures the precision of the explanation provided by the\nexplainability technique for a single instance. If the AP score\nis high (closer to 1), it indicates that the explainability of that\ninstance strongly aligns with the human rationale. A lower AP\nscore (closer to 0) suggests that the agreement is poor in that\nparticular instance.\nMAP is the mean of the AP scores for all instances, as calcu-\nlated by Equation (2).\nMAP =\nPN\nn=1 APn\nN (2)\nwhere N is the total number of instances evaluated, APn is the\nAverage Precision calculated for the nth instance.\nMAP evaluates the alignment between explanations provided\nby an explainability technique and human rationale across a\nset of instances or documents rather than measuring the pre-\ncision of the explanation for each individual instance. A high\nMAP score (closer to 1) indicates that the explainability tech-\nnique consistently performs well across diverse instances and\nstrongly agrees with human rationales. Conversely, a low MAP\nscore (closer to 0) suggests that the explainability technique’s\nperformance varies across instances, indicating poor agreement\nwith human rationales.\n3.1.2. Robustness\nThis metric measures the robustness of explanations pro-\nvided by explainability techniques in response to changes in the\ninput data and the model [44]. It evaluates how explanations\nvary under various conditions, such as real-time applications\nand adversarial perturbations. It also assesses the model’s con-\nsistency when its parameters are altered or when it is retrained\nwith modified or augmented datasets. This helps understand\nthe robustness and reliability of the explanations across di ffer-\nent scenarios, thereby enhancing comprehension of the model’s\nbehavior under diverse conditions [45]. We assess how stable\nthe explanation provided for an original input instance X re-\nmains when the input is slightly modified to X′. In previous\nstudies, the robustness of explanations is measured directly by\nevaluating the differences in top-k saliency token /word scores\nbetween the original input instance and the perturbed or modi-\nfied input instance [37]. However, this approach does not fully\ncapture the robustness of explanations or the model’s behav-\nior, as different tokens/words at the same rank may have similar\nscores. To address this, we employ element-wise di fferences\nand averaging techniques to quantify robustness metrics at both\nthe token/word and instance levels based on relevance compu-\ntation.\nRelevance Function:The relevance function, rel(k), serves\nas a binary indicator to determine the relevance of each word k\nin a model’s decision-making process. If the wordk is included\nin both X and X′, it returns 1; otherwise, it returns 0.\nrel(k) =\n\n1 if k ∈X\n0 otherwise (3)\nwhere k is a relevant word to the model’s decision-making pro-\ncess.\nTo create a modified instance X′, a perturbation δi is applied\nsuch that X′=X +δi. This perturbation δi typically involves var-\nious techniques such as masking, replacing words with syn-\nonyms, removing words, or applying other modifications to\nwords with high or low salience scores.\n4\nElement-wise Difference d(k): For each word k, the func-\ntion d(k) precisely quantifies the change in the saliency scores\nof a word k between X and X′, as identified by rel(k). d(k) is\ncomputed as:\nd(k) = ∥X[k] −\u0000X′[k] ×rel(k)\u0001∥ (4)\nwhere X[k] represents the saliency score of wordk in the expla-\nnation derived from the original input, while X′[k] represents\nthe saliency score of word k in the explanation derived from\nthe modified input. The product with rel(k) ensures that differ-\nences are calculated only for words that appear in both sets ( X\nand X′), avoiding distortion from irrelevant words.\nAverage Difference (AD):The AD aggregates the individ-\nual differences d(k) for all relevant words and provides a single\nmetric for each instance. AD reflects the average magnitude of\nchange in an individual explanation due to an input modifica-\ntion. AD is essential for understanding the robustness of expla-\nnations at an instance level. Mathematically, it is described by\nEquation 5.\nAD = 1\nK\nKX\nk=1\nd(k) (5)\nwhere K represents the total number of words in an explanation\nfor a given instance.\nMean Average Difference (MAD):MAD is a dataset-wide\nmetric that averages the AD values across all instances, provid-\ning a global measure of the robustness of the explanations in\nresponse to changes in input data throughout the dataset. Math-\nematically, it is represented by Equation 7\nMAD = 1\nN\nNX\nn=1\nAD (6)\nMAD =\nPN\nn=1\n\u0010 1\nK\nPK\nk=1 d(k)\n\u0011\nN (7)\nwhere N is the total number of instances.\nLower AD and MAD scores indicate that the explainability\ntechnique performs robustly well both at the instance level and\nacross diverse instances, respectively.\n3.1.3. Consistency\nModels with diverse architectures tend to have low explana-\ntion consistency and vice versa [46]. We are interested in prov-\ning the similarity of attention reasoning mechanisms rather than\nsimilar predictions for similar model architecture. We focus on\na set of models with the same architecture, trained with different\nrandom seeds and randomly initialized weights. Our interest is\nto discover the attention-based reasoning mechanisms instead\nof model prediction outputs since similarities of prediction out-\nputs are not always guaranteed in models with similar reason-\ning. Different models can arrive at the same prediction through\ndifferent reasoning processes.\nLet Ma and Mb be two distinct models with similar architectures\nand trained with di fferent seeds, and xi be an input instance.\nDA(Ma,Mb,xi) and DE(Ma,Mb,xi) are the distances between\nattention weights and explanation scores, respectively, and they\ncan be computed using some distance measurements such as\nCosine similarity or Euclidean distance.\nMulti-Layer Attention Weights Distance for inputxi:\nThe similarity of their reasoning mechanisms can be e ffec-\ntively measured by computing the distance between the atten-\ntion weights generated by the two models (Equation 10). First,\nwe extracted and averaged attention weights. For a model M\nwith L attention layers, attention weights at lth layer can be rep-\nresented by Al(M, xi) for the input xi, ( M can be model Ma\nor Mb). Then, we compute the weighted average of attention\nweights by Equation 8.\nA(M,xi) = 1\nL\nLX\nl=1\nAl(M,xi) (8)\nHere, A(M,xi) represents the averaged attention weights over\nall layers for a given instance xi. We obtain the average atten-\ntion weight vector A(M,xi). This average provides a consoli-\ndated view of how the model attends to di fferent parts of the\ninput across all layers, making it easier to compare attention\nmechanisms between models.\nThe distance between model Ma and Mb, denoted as\n(DA(Ma,Mb,xi)) is computed using the equation below.\nDA(Ma,Mb,xi) = DA(Ma(xi),Mb(xi)) (9)\nDA(Ma,Mb,xi) establishes a way to quantify the di fferences or\nsimilarities in how two models process the same input xi. This\nsets the basis for comparing models based on their responses to\nthe same data point rather than only on their output predictions.\nA(Ma,xi) and A(Mb,xi) are the averaged attention weights of\nmodels Ma and Mb for the input xi, as shown by Equation\n8.Then DA(Ma,Mb,xi) computed as:\nDA(Ma,Mb,xi) = DA(A(Ma,xi),A(Mb,xi)). (10)\nConsiderable attention weight similarities or differences sug-\ngest that the models attend to di fferent aspects of the input be-\ning trained with different seeds. This is not just a technical de-\ntail but a crucial aspect in assessing whether the models main-\ntain consistent focus and importance on the same input features\n(words), which is our main interest in evaluating the consis-\ntency and effectiveness of different explainability techniques.\nExplanation Score Distance for Inputxi: The explanation\nscores are derived from an explainability technique that we are\ninterested in to evaluate its e ffectiveness and consistency. By\nmeasuring the distance between the explanation scores, we can\nevaluate how similarly or di fferently the explainability tech-\nniques explain the two models’ predictions for the same input.\nDE(Ma,Mb,xi) = DE(Ma(xi),Mb(xi)) (11)\nThis equation computes the difference in explanation scores be-\ntween the two models for the same input, emphasizing the mag-\nnitude of differences.\n5\nConsistency in Instance Levelxi: We can assess e ffective-\nness and consistency at the instance level by comparing the dis-\ntances between attention weights ( DA(Ma,Mb,xi)) and expla-\nnation scores ( DE(Ma,Mb,xi)) for individual inputs. If these\ndistances are similar or close to similar, the explainability tech-\nnique is considered effective, indicating that models with simi-\nlar attention weights provide consistent explanations.\nConsistency across the Dataset:For multiple instances X =\n{x1,x2,..., xN }, we computed the correlation of the distances of\nattention weights and explanation scores using Spearman’s rank\ncorrelation coefficient ρ[47], (Equation 12).\nρ= Spearman’scorr\n\u0010\n{DA(Ma,Mb,xi)}N\ni=1,{DE(Ma,Mb,xi)}N\ni=1\n\u0011\n(12)\nwhere ρ is the overall Spearman’s correlation of the attention\nweights and explanations of the entire input instances, and N is\nthe total number of input instances.\nρprovides the overall correlation between the attention weights\nand explainability explanations of the global trend across all\ndata points. The higher the positive correlation, the more con-\nsistent the explainable technique is.ρmeasures the strength and\ndirection of the monotonic relationship between the distances in\nattention weights and the distances in explanation scores across\nmultiple inputs. A high correlation indicates that models with\nsimilar attention weights also tend to have similar explanation\nscores, suggesting consistency in their reasoning mechanisms.\nBy using this attention-weight approach, we can determine\nwhether models trained with di fferent random seeds exhibit\nconsistent reasoning mechanisms and focus on similar input\nwords, thereby evaluating the e ffectiveness and robustness of\nexplainability techniques.\n3.1.4. Contrastivity\nContrastivity is a critical evaluation metric for assessing\nthe effectiveness of XAI methods, particularly in classification\ntasks [48]. It focuses on how well an XAI method can di ffer-\nentiate between different classes through its explanations, pro-\nviding insight into why a model chooses one class over another.\nFor example, to assess the di fference between the two classes\n(positive or negative), we can compare the explanations for dif-\nferent class predictions and see if the explanations for one class\nare distinct from those for another. This practical use of con-\ntrastivity helps us to understand the effectiveness of XAI meth-\nods. We used Kullback-Leibler Divergence (KL Divergence) to\nquantify the contrastivity metric in feature importance distribu-\ntions. KL Divergence is ideal for its sensitivity to differences in\nfeature importance distributions and its focus on the direction\nof divergence, making it perfect for analyzing and comparing\ntokens/words (feature) importance across different classes [49].\nKL(P ∥Q) =\nnX\ni=1\nP(i) log\n P(i)\nQ(i)\n!\n(13)\nwhere P(i) and Q(i) represent the importance of feature i in one\nclass and in a di fferent class, respectively, and n is the total\nnumber of tokens/words in the given instance. P and Q are the\ndistribution of feature importance for the two different classes.\nHigh contrastivity means that the XAI method e ffectively\nhighlights di fferent features for di fferent classes. The posi-\ntive attributions should be associated with the target label, and\nthe negative attributions should be associated with the opposite\nclass.\n4. Experiments and Setups\nWe have proposed a comprehensive XAI evaluation frame-\nwork as a benchmark for assessing the effectiveness of explain-\nability techniques across different scenarios (Fig. 1). Our study\nused five di fferent encoder-only language models with varied\nlevels of complexity to focus on text data and a classification\ntask. To provide clear insights into our experiment, we included\nsix distinct XAI methods, each representing five different cate-\ngories of XAI techniques (two methods for gradient-based cat-\negories). We then evaluated these methods using four specific\nmetrics.\nWe conducted our experiments on Google Colab, utilizing\nan NVIDIA A100-SXM4-40GB GPU with 40 GB of VRAM\npowered by CUDA Version 12.2. This setup provided robust\ncomputational resources, enabling e fficient handling of high-\ndemand tasks such as deep learning model training and large-\nscale data processing.\n4.1. Datasets\nWe utilized two distinct datasets for our study: IMDB 1\nMovie Reviews and Tweet Sentiment Extraction. The IMDB\nMovie Reviews dataset consists of 50,000 movie reviews, each\nlabeled as either positive or negative. The Tweet Sentiment Ex-\ntraction (TSE) 2 dataset consists of 31,016 tweets labeled with\nsentiments such as positive, negative, or neutral. Tweets are\ntypically short texts. We randomly split each dataset into 80%\nfor training and 20% for testing.\n4.2. Models\nWe conducted experiments using commonly used\ntransformer-based models, including TinyBERT [50], BERT-\nbase-uncased [2], BERT-large-uncased [2], XLM-R large\n[51], and DeBERTa-xlarge [52]. These models were chosen\nprimarily for their varying levels of complexity and parameter\nsizes. This baseline model selection enables a comprehensive\ncomparison and analysis of explainability techniques across\nmodels with diverse complexities. Fig 2 illustrates the selected\ntransformer-based models and their respective parameter sizes,\nhighlighting the differences that impact their performance and\nsuitability for various tasks. By evaluating the e ffectiveness\nof explainability techniques, we aim to understand how model\ncomplexity and size influence the interpretability and trans-\nparency of these models in practical applications. We fine-tune\nall the selected pre-trained models by adding a linear layer on\ntop of them. The size of this linear layer corresponds to the\nnumber of classes in the given classification task.\n1https://www.kaggle.com/datasets/columbine/imdb-dataset-sentiment-\nanalysis-in-csv-format\n2https://www.kaggle.com/c/tweet-sentiment-extraction\n6\nFigure 2: The selected transformer-based models by complexity band parame-\nter size.\n4.3. Explainability Techniques\nWe grouped explainability techniques into five categories\nbased on their design principles and functionality to provide\na comprehensive evaluation by category: model simplification\n[9], perturbation [10], gradient [11], layer-wise relevance prop-\nagation [12], and attention mechanism [13]. We chose the most\nrepresentative and commonly used explainability techniques\nfrom each category.\nFrom the model simplification category, we selected LIME,\na model-agnostic explainer. LIME trains a linear model to ap-\nproximate the local decision boundary for each instance and\nprovides explanations for individual predictions of a complex\nmodel [9].\nWe selected SHAP, another model-agnostic explainer, from\nthe perturbation category. SHAP uses the perturbation tech-\nnique to determine the importance of each feature and pro-\nvides interpretable explanations for the model’s predictions\n[11], [20].\nFor the Gradient category, we employed InputXGradient and\nGradient-weighted Class Activation Mapping (Grad-CAM),\nmodel-specific techniques. These methods leverage the gradi-\nents of the model’s output with respect to its input features to\ninterpret the predictions of deep learning models [53], [21].\nWe used LRP-ϵ (LRP for simplicity for this study) methods\nfrom the Layer-wise Relevance Propagation category due to its\nbalanced approach between simplicity and stability. LRP- ϵ is\na model-specific technique that assigns a model’s prediction to\nits input features by systematically redistributing the prediction\nscore of the model backward through each neuron to the previ-\nous layer based on the contribution of each neuron to the output\n[12], [29].\nLastly, we selected an Attention Mechanism Visualization\n(AMV) explainability technique that is also model-specific.\nThis technique visualizes and interprets the most influential\ninput features for a model’s prediction based on the attention\nweights assigned by the model’s attention mechanism [13],\n[54], [28].\n5. Results and Discussion\nWe present the results of each evaluation metric on various\nXAI methods and encoder-based language models across the\nIMDB (long texts) and TSE (short texts) datasets, considering\nmodel complexity from TinyBERT (14.5 million parameters) to\nDeBERTa-large (1.5 billion parameters). The primary focus of\nthis study is not on the performance of the models themselves\nbut on the effectiveness of the XAI methods on these encoder-\nbased language models. The quantitative results for each eval-\nuation metric are presented below. Additionally, Appendix B\nincludes sample visualizations of the outputs from each XAI\ntechnique, offering deeper insights and enabling thorough com-\nparisons.\nHuman-reasoning Agreement (HA):Table 1 presents the\nHA metric results. We randomly selected 100 instances from\neach dataset. We used three machine learning experts to anno-\ntate the datasets as the baseline to evaluate the alignment be-\ntween explanations provided by XAI methods and human judg-\nment.\nOn the IMDB dataset, which contains longer texts, LIME\nconsistently performs exceptionally well across all model sizes,\nwith HA scores improving as model complexity increases. This\nsuggests that LIME e ffectively captures human-like explana-\ntions irrespective of model size, achieving its highest score of\n0.9685 with DeBERTa-xlarge. SHAP shows moderate perfor-\nmance, with its effectiveness significantly improving for larger\nmodels, indicating that it benefits from increased model com-\nplexity or larger parameter sizes. LRP struggles with larger\nmodels, displaying a decline in performance as model com-\nplexity increases. It performs better with simpler models like\nTinyBERT, where it achieved its highest score of 0.6427. In-\nputXGradient, despite showing low agreement with human ra-\ntionales, holds promise as it improves with larger models, sug-\ngesting that it benefits from increased model complexity. Grad-\nCAM demonstrates moderate performance, with improvements\nseen as model size increases, while AMV shows consistently\npoor performance across all models, with a slight decrease as\nmodel complexity increases.\nFor the TSE dataset, which contains shorter texts, LIME\nagain shows high agreement with human rationales, with per-\nformance improving as model complexity increases, achieving\na top score of 0.9118 with DeBERTa-xlarge. SHAP, demon-\nstrating its adaptability, performs better on TSE compared to\nIMDB, particularly for larger models, indicating its e ffective-\nness with increased model complexity. LRP performs better on\nTSE (short text) than IMDB but still shows a decline in perfor-\nmance with larger models. InputXGradient improves on TSE\ncompared to IMDB, with better performance for larger models,\nwhile Grad-CAM maintains moderate performance, improving\nwith larger models. AMV continues to show low performance\nacross all models, with a slight decrease as model complexity\nincreases.\nLIME stands out as the best-performing explainability tech-\nnique, providing strong and reliable alignment with human ra-\ntionales across both datasets and various models. Its perfor-\nmance has improved with increased model complexity. SHAP\nand Grad-CAM, on the other hand, provide a balance of perfor-\nmance, benefiting significantly from larger models. In contrast,\nAMV and LRP are the least effective in aligning with human ra-\ntionales and become significantly less e ffective as model com-\nplexity increases. InputXGradient is also the least e ffective but\nbenefits from increasing model complexity.\n7\nTable 1: The quantitative results of the Human-reasoning Agreement metric on various XAI methods and encoder-based language models for IMDB and TSE\ndatasets. Higher scores indicate better agreement.\nIMDB TSE\nTinyBERT BERTbase BERTlarge XLM-R DeBERTa xlarge TinyBERT BERTbase BERTlarge XLM-R DeBERTa xlarge\nLIME 0.8774 0.6981 0.8903 0.9445 0.9685 0.7566 0.4689 0.8023 0.8869 0.9118\nSHAP 0.4135 0.4354 0.5012 0.5634 0.6625 0.5231 0.5728 0.6002 0.6894 0.7254\nLRP 0.6427 0.2736 0.2078 0.2011 0.1984 0.7223 0.5986 0.5023 0.4533 0.3689\nInputXGradient 0.0782 0.0659 0.1775 0.2356 0.3133 0.1691 0.0965 0.2647 0.3562 0.3959\nGrad-CAM 0.1437 0.1936 0.5229 0.6118 0.6497 0.2563 0.4125 0.4565 0.5001 0.5926\nAMV 0.1658 0.1459 0.1001 0.0859 0.0653 0.2136 0.1759 0.1325 0.0962 0.0593\nTable 2: The quantitative results of the Robustness metric on various XAI methods and encoder-based language models for IMDB and TSE datasets. Lower scores\nindicate better robustness.\nIMDB TSE\nTinyBERT BERTbase BERTlarge XLM-R DeBERTa xlarge TinyBERT BERTbase BERTlarge XLM-R DeBERTa xlarge\nLIME 0.0056 0.0058 0.0061 0.0078 0.0092 0.0043 0.0049 0.0053 0.0060 0.0068\nSHAP 0.0356 0.0387 0.1258 0.1547 0.2139 0.0258 0.0301 0.03662 0.1321 0.1965\nLRP 0.3214 0.5431 0.7621 0.8549 0.9124 0.3392 0.4895 0.5684 0.6855 0.8215\nInputXGradient 0.1108 0.1546 0.2391 0.2769 0.3012 0.0953 0.1258 0.2011 0.2547 0.2958\nGrad-CAM 0.0237 0.0161 0.0273 0.0312 0.0367 0.0189 0.0123 0.0232 0.0259 0.0291\nAMV 0.0020 0.0023 0.0056 0.0058 0.0073 0.0014 0.0019 0.0032 0.0041 0.0051\nTable 3: The quantitative results of Consistency metric on various XAI methods and encoder-based language models for IMDB and TSE datasets. Higher scores\nindicate better consistency.\nIMDB TSE\nTinyBERT BERTbase BERTlarge XLM-R DeBERTa xlarge TinyBERT BERTbase BERTlarge XLM-R DeBERTa xlarge\nLIME 0.9665 0.9741 0.9800 0.9837 0.9895 0.7568 0.8425 0.8962 0.9228 0.9556\nSHAP 0.9002 0.9368 0.9487 0.9569 0.9775 0.8322 0.8598 0.8896 0.9002 0.09324\nLRP 0.9417 0.9642 0.9754 0.9801 0.9887 0.8556 0.8901 0.9223 0.9596 0.9713\nInputXGradient 0.9341 0.9447 0.9555 0.9599 0.9799 0.7583 0.7759 0.7952 0.8235 0.8596\nGrad-CAM -0.9593 -0.9677 -0.9698 -0.9700 -0.9749 -0.8259 -0.8556 -0.8718 -0.8987 -0.9325\nAMV 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999 0.9999\nTable 4: The quantitative results of Contrastivity metric on various XAI methods and encoder-based language models for IMDB and TSE datasets. Higher scores\nindicate better.\nIMDB TSE\nTinyBERT BERTbase BERTlarge XLM-R DeBERTa xlarge TinyBERT BERTbase BERTlarge XLM-R DeBERTa xlarge\nLIME 0.7065 0.7226 0.6545 0.6288 0.5766 0.6863 0.7145 0.6631 0.6251 0.5838\nSHAP 0.6449 0.6694 0.6208 0.5565 0.5448 0.6552 0.6727 0.6075 0.5709 0.5356\nLRP 0.7723 0.5797 0.7958 0.8456 0.9371 0.8598 0.7126 0.8540 0.8797 0.9367\nInputXGradient 0.7488 0.5921 0.4493 0.4289 0.3942 0.7968 0.6833 0.5288 0.4516 0.3661\nGrad-CAM 0.5689 0.4695 0.4163 0.3915 0.3641 0.6623 0.5965 0.4471 0.4034 0.3371\nAMV 0.1546 0.1168 0.1012 0.0761 0.0384 0.1841 0.1616 0.1243 0.1050 0.0680\nRobustness: Table 2 presents the quantitative results of\nthe robustness metric. LIME demonstrates very good robust-\nness across all models for the IMDB dataset, with scores rang-\ning from 0.0056 for TinyBERT to 0.0092 for DeBERTa-xlarge.\nThis suggests that LIME’s explanations are stable and consis-\ntent even for complex models handling long texts. SHAP, on\nthe other hand, shows moderate robustness, with scores rang-\ning from 0.0356 for TinyBERT to 0.2139 for DeBERTa-xlarge.\nThis indicates that SHAP’s explanations are less robust for\nlarger models. LRP exhibits poor robustness, especially for\nlarger models, with scores ranging from 0.3214 for TinyBERT\nto 0.9124 for DeBERTa-xlarge, suggesting highly variable and\nunstable explanations. InputXGradient shows moderate robust-\nness, with scores ranging from 0.1108 for TinyBERT to 0.3012\nfor DeBERTa-xlarge, indicating that while it is more robust\nthan LRP, it is less effective than LIME and Grad-CAM. Grad-\nCAM demonstrates good robustness, with scores ranging from\n0.0161 for BERTbase to 0.0367 for DeBERTa-xlarge, providing\nrelatively stable explanations even for larger models handling\nlong texts. AMV shows the best robustness across all models,\nwith scores ranging from 0.0020 for TinyBERT to 0.0073 for\nDeBERTa-xlarge, indicating highly stable and consistent expla-\nnations regardless of model complexity.\nLIME maintains high robustness across different models for the\nTSE dataset, with scores ranging from 0.0043 for TinyBERT to\n0.0068 for DeBERTa-xlarge, suggesting stable and consistent\nexplanations for short texts. SHAP again demonstrates moder-\nate robustness, with scores ranging from 0.0258 for TinyBERT\nto 0.1965 for DeBERTa-xlarge, indicating less stable explana-\ntions for larger models. LRP continues to show poor robust-\nness, especially for larger models, with scores ranging from\n0.3392 for TinyBERT to 0.8215 for DeBERTa-xlarge. InputX-\n8\nGradient shows moderate robustness, with scores ranging from\n0.0953 for TinyBERT to 0.2958 for DeBERTa-xlarge, provid-\ning more stable explanations than LRP but less e ffective than\nLIME and Grad-CAM. Grad-CAM demonstrates good robust-\nness, with scores ranging from 0.0123 for BERTbase to 0.0291\nfor DeBERTa-xlarge, providing relatively stable explanations\neven for larger models with short texts. AMV continues to\nshow the best robustness across all models, with scores rang-\ning from 0.0014 for TinyBERT to 0.0051 for DeBERTa-xlarge,\nindicating highly stable and consistent explanations regardless\nof model complexity.\nLIME and AMV are consistently the most robust XAI meth-\nods, providing stable and consistent explanations across both\ndatasets and various models, regardless of text length and model\ncomplexity. Grad-CAM o ffers a balance of performance, pro-\nviding good robustness across different models, particularly for\nlarger models. LRP and SHAP are the least robust, with SHAP\nshowing significant performance decreases for larger models\nand LRP exhibiting high variability and instability.\nConsistency: Table 3 presents the quantitative results of\nthe Consistency metric. LIME demonstrates excellent consis-\ntency across both datasets, with scores improving as model\ncomplexity increases. For IMDB, scores range from 0.9665\nfor TinyBERT to 0.9895 for DeBERTa-xlarge, while for TSE,\nscores range from 0.7568 to 0.9556, indicating highly reliable\nexplanations for both long and short texts. SHAP also ex-\nhibits strong consistency, with scores increasing from 0.9002\nfor TinyBERT to 0.9775 for DeBERTa-xlarge on IMDB, and\nfrom 0.8322 to 0.9324 on TSE, suggesting that SHAP’s expla-\nnations become more reliable as model complexity increases.\nLRP shows very good consistency, with scores improving from\n0.9417 for TinyBERT to 0.9887 for DeBERTa-xlarge on IMDB,\nand from 0.8556 to 0.9713 on TSE, indicating stable and reli-\nable explanations for more complex models. InputXGradient\ndemonstrates strong consistency, with scores increasing from\n0.9341 for TinyBERT to 0.9799 for DeBERTa-xlarge on IMDB,\nand from 0.7583 to 0.8596 on TSE, highlighting its e ffective-\nness in providing consistent explanations for larger models.\nConversely, Grad-CAM shows poor consistency across both\ndatasets, with negative scores ranging from -0.9593 for Tiny-\nBERT to -0.9749 for DeBERTa-xlarge on IMDB, and from -\n0.8259 to -0.9325 on TSE, indicating significant inconsistency\nin its explanations. In contrast, AMV achieves perfect consis-\ntency with scores consistently at 0.9999 across all models, re-\ngardless of complexity, providing identical explanations across\nall instances and models.\nAMV and LIME are the most robust XAI methods, consis-\ntently providing stable and reliable explanations across both\ndatasets and various models, irrespective of text length and\nmodel complexity. SHAP, LRP, and InputXGradient o ffer a\nbalance of performance with reasonable consistency, particu-\nlarly for larger models. However, Grad-CAM remains the least\nconsistent method, with significant variability and instability in\nits explanations.\nContrastivity: Table 4 presents the quantitative results of\nthe Contrastivity metric. For both the IMDB and TSE datasets,\nLIME demonstrates strong contrastivity for smaller models,\nwith scores of 0.7065 for TinyBERT on IMDB and 0.6863 on\nTSE. However, LIME’s performance decreases as model com-\nplexity increases, with scores dropping to 0.5766 for DeBERTa-\nxlarge on IMDB and 0.5838 on TSE, suggesting that its ability\nto highlight contrasting features diminishes with larger mod-\nels. SHAP exhibits moderate contrastivity across both datasets,\nwith scores such as 0.6449 for TinyBERT on IMDB and 0.6552\non TSE. However, as model complexity increases, SHAP’s\nscores decrease, dropping to 0.5448 for DeBERTa-xlarge on\nIMDB and 0.5356 on TSE, indicating reduced effectiveness for\nlarger models. LRP shows varying performance on the IMDB\ndataset, with a lower score of 0.5797 for BERTbase but signif-\nicantly higher scores for more complex models like DeBERTa-\nxlarge, which achieves 0.9371. LRP maintains strong con-\ntrastivity on the TSE dataset with high scores, such as 0.8540\nfor BERTlarge and 0.9367 for DeBERTa-xlarge, suggesting\nit e ffectively highlights contrasting features as model com-\nplexity increases. InputXGradient displays good contrastiv-\nity for smaller models, with scores of 0.7488 for TinyBERT\non IMDB and 0.7968 on TSE. However, its performance de-\nclines sharply with increasing model complexity, as seen in the\nscores of 0.3942 for DeBERTa-xlarge on IMDB and 0.3661\non TSE. Grad-CAM demonstrates weak contrastivity across\nboth datasets, with low scores such as 0.5689 for TinyBERT\non IMDB and 0.6623 on TSE, further declining with more\ncomplex models like DeBERTa-xlarge, which scores 0.3641 on\nIMDB and 0.3371 on TSE, indicating less e ffective highlight-\ning of contrasting features. In contrast, AMV shows poor con-\ntrastivity across all models on both datasets, with particularly\nlow scores of 0.1546 for TinyBERT and 0.0384 for DeBERTa-\nxlarge on IMDB, and similarly low scores on TSE, indicating\nminimal effectiveness in highlighting contrasting features.\nLRP emerges as the most e ffective XAI method in terms\nof contrastivity, especially for complex models like DeBERTa-\nxlarge. This suggests that LRP is particularly adept at high-\nlighting differences in model predictions based on contrasting\nfeatures. LIME and SHAP offer a balance of performance, pro-\nviding moderate contrastivity, although their e ffectiveness de-\ncreases as model complexity increases. Grad-CAM and AMV\nshow poor contrastivity, with significant variability and lower\nscores across different models, indicating less reliable explana-\ntions for highlighting contrasting features.\nOverall, The study highlights that no single XAI technique\nexcels universally across all metrics and models. However, our\nrigorous evaluation process has identified some reliable XAI\ntechniques. A model simplification-based approach, LIME,\nconsistently performs well across multiple evaluation metrics,\nmaking it a reliable choice for generating explanations that\nalign with human reasoning, and are robust and consistent. De-\nspite its limitations in contrastivity, AMV , an attention mech-\nanism approach, excels in robustness and consistency, mak-\ning it suitable for applications where stability and reliability\nare paramount. A layer-wise relevance propagation approach,\nLRP, shows promise in contrastivity, particularly for complex\nmodels, indicating its potential for tasks requiring identifying\ncontrasting features. Perturbation-based techniques (such as\nSHAP) and gradient-based techniques (such as InputXGradi-\n9\nTable 5: The quantitative results of the combined weighted metrics scores on various XAI methods and encoder-based language models for IMDB and TSE datasets.\nHigher scores indicate better overall performance.\nIMDB TSE\nTinyBERT BERTbase BERTlarge XLM-R DeBERTa xlarge TinyBERT BERTbase BERTlarge XLM-R DeBERTa xlarge\nLIME 0.8862 0.8755 0.8797 0.8873 0.8611 0.7989 0.7468 0.7785 0.8572 0.8611\nSHAP 0.7308 0.7507 0.7427 0.7504 0.7427 0.7308 0.7507 0.7427 0.7504 0.7516\nLRP 0.7588 0.5686 0.7329 0.6741 0.6809 0.6779 0.6562 0.6677 0.7055 0.7285\nInputXGradient 0.6626 0.6569 0.6741 0.6865 0.7062 0.6572 0.6457 0.6635 0.6859 0.6978\nGrad-CAM 0.6621 0.2355 0.6906 0.6934 0.6965 0.1824 0.6817 0.6906 0.7197 0.7250\nAMV 0.5796 0.5241 0.5637 0.5561 0.5442 0.5991 0.5920 0.5733 0.5593 0.5436\nent) demonstrate moderate performance across all metrics and\nmodels.\nCombined Weighted-metrics Scores (CWS): A combined\nweighted metrics approach is employed to assess the perfor-\nmance of various XAI methods across di fferent encoder-only\nlanguage models. The evaluation is based on four key met-\nrics: Human-reasoning Agreement (HA), Robustness (R), Con-\nsistency (Cn), and Contrastivity (Ct), each assesses various as-\npects of XAI performance. Higher scores (closer to 1) indicate\nbetter performance for Human-reasoning Agreement, Consis-\ntency, and Contrastivity. However, lower scores (closer to 0)\nare preferable for robustness as they reflect an excellent expla-\nnation of stability under perturbations. To align this with the\nother metrics, we normalize the Robustness score by subtract-\ning it from 1, ensuring all metrics are positively oriented. Each\nmetric is considered equally important, so they are all assigned\nan equal weight ( ω) of 0.25. However, if specific metrics are\nmore critical to the evaluation context, larger weights can be\nassigned to those metrics accordingly. The CWS for each XAI\nmethod on each encoder-based language model is computed us-\ning the formula:\nCWS = ωHA ·HA + ωCn ·Cn + ωCt ·Ct + ωR ·(1 −R) (14)\nwhere ωHA + ωCn + ωCt + ωR =1, and ω≥0.\nCWS provides a comprehensive evaluation, with higher\nscores indicating the superior overall performance of XAI\ntechniques across di fferent encoder-based language models,\nas shown in Table 5. Based on combined weighted metric\nscores, LIME, a model simplification-based technique, con-\nsistently demonstrates strong performance across all models\non the IMDB and TSE datasets. SHAP, a perturbation-based\nmethod, shows balanced and reliable results across models. The\nLRP approach exhibits more variability in performance, with\nsignificant fluctuations depending on the model. InputXGra-\ndient, a gradient-based method, maintains mid-range scores,\noffering consistent reliability but falling short of top-tier per-\nformance. Grad-CAM, also a gradient-based method, shows\nsignificant variability, with performance varying greatly across\nmodels. Finally, AMV consistently scores modestly across both\ndatasets, suggesting it may be less e ffective than other XAI\nmethods.\nLIME aligns more e ffectively with human reasoning than\nother XAI methods because it uses localized, interpretable ap-\nproximations tailored to each prediction instance. By selec-\ntively isolating and analyzing the influence of specific words or\ntokens, LIME approximates complex model behavior in a way\nthat resonates with human judgment. With their sophisticated\nattention mechanisms that process multi-dimensional relation-\nships across tokens, transformer-based models are often opaque\nand challenging to interpret globally. LIME’s localized focus\neffectively addresses this issue by concentrating only on the\nmost relevant tokens within each instance, avoiding the need to\nexplain the entire model’s behavior. In contrast, other explain-\nability methods, such as gradient-based techniques like Grad-\nCAM and InputXGradient, often lack the strong alignment with\nhuman reasoning that LIME provides. Gradient-based meth-\nods depend on gradients or feature attributions derived from\nthe entire model behavior, which may introduce noise in highly\nparameterized transformer models. These methods frequently\nstruggle to capture localized patterns and may highlight features\nthat do not align with human interpretative strategies, especially\nas model complexity increases.\n6. Limitation and Future Work\nDue to experimental complexity, our study is limited to\nselected metrics, encoder-based language models, XAI tech-\nniques, and classification tasks. Future research could broaden\nthe scope by incorporating a wider range of XAI techniques,\nexploring more diverse and complex transformer-based models\nsuch as LLaMA, including under-resourced languages, and ex-\namining a broader set of downstream tasks. This extended work\nwould allow for more refined evaluation metrics and increased\napplicability in real-world contexts. Furthermore, addressing\nthe computational complexity of XAI techniques will be cru-\ncial to improving the scalability and feasibility of our evaluation\nframework for large-scale applications.\n7. Conclusion\nOur proposed comprehensive evaluation framework, with a\ndetailed set of metrics, serves as a structured approach to as-\nsess the e ffectiveness of various explainability techniques ap-\nplied to encoder-based language models. This systematic eval-\nuation, which rigorously tests these techniques across key eval-\nuation metrics such as Human-reasoning Agreement, Robust-\nness, Consistency, and Contrastivity, is a significant step for-\nward. By evaluating diverse datasets and models, we provide an\nin-depth analysis of how well each technique aligns with human\njudgment, remains robust under perturbations, provides consis-\ntent explanations, and highlights contrasting features. Our find-\nings indicate that model simplification approaches like LIME\nperform well across multiple metrics, regardless of model com-\nplexity. Although it has limitations in contrastivity, the atten-\ntion mechanism approach (e.g., AMV) excels in robustness and\n10\nconsistency metrics, making it ideal for applications requir-\ning stability and reliability. The layer-wise relevance propa-\ngation (LRP) technique shows strong potential in contrastivity,\nparticularly for complex models, suggesting its potential ad-\nvantage in tasks that require identifying contrasting features.\nPerturbation-based techniques (e.g., SHAP) and gradient-based\ntechniques (e.g., InputXGradient) demonstrate moderate per-\nformance across all metrics and models. This evaluation en-\nhances our understanding of the strengths and limitations of\ncurrent XAI techniques in encoder-based language models. It\nlays a foundation for future research to improve the reliability of\nexplanations provided by explainability techniques in language\nmodels in real-world applications.\nReferences\n[1] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, et al., Improving\nlanguage understanding by generative pre-training (2018).\n[2] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep\nbidirectional transformers for language understanding, arXiv preprint\narXiv:1810.04805 (2018).\n[3] A. Das, P. Rad, Opportunities and challenges in explainable artificial in-\ntelligence (xai): A survey, arXiv preprint arXiv:2006.11371 (2020).\n[4] A. B. Arrieta, N. D ´ıaz-Rodr´ıguez, J. Del Ser, A. Bennetot, S. Tabik,\nA. Barbado, S. Garc ´ıa, S. Gil-L ´opez, D. Molina, R. Benjamins, et al.,\nExplainable artificial intelligence (xai): Concepts, taxonomies, opportu-\nnities and challenges toward responsible ai, Information fusion 58 (2020)\n82–115.\n[5] P. Regulation, Regulation (eu) 2016 /679 of the european parliament and\nof the council, Regulation (eu) 679 (2016) 2016.\n[6] M. Langer, D. Oster, T. Speith, H. Hermanns, L. K ¨astner, E. Schmidt,\nA. Sesing, K. Baum, What do we want from explainable artificial intel-\nligence (xai)?–a stakeholder perspective on xai and a conceptual model\nguiding interdisciplinary xai research, Artificial Intelligence 296 (2021)\n103473.\n[7] V . Shah, S. R. Konda, Neural networks and explainable ai: Bridging the\ngap between models and interpretability, INTERNATIONAL JOURNAL\nOF COMPUTER SCIENCE AND TECHNOLOGY 5 (2) (2021) 163–\n176.\n[8] R. Dwivedi, D. Dave, H. Naik, S. Singhal, R. Omer, P. Patel, B. Qian,\nZ. Wen, T. Shah, G. Morgan, et al., Explainable ai (xai): Core ideas,\ntechniques, and solutions, ACM Computing Surveys 55 (9) (2023) 1–33.\n[9] M. T. Ribeiro, S. Singh, C. Guestrin, ” why should i trust you?” explain-\ning the predictions of any classifier, in: Proceedings of the 22nd ACM\nSIGKDD international conference on knowledge discovery and data min-\ning, 2016, pp. 1135–1144.\n[10] M. D. Zeiler, R. Fergus, Visualizing and understanding convolutional\nnetworks, in: Computer Vision–ECCV 2014: 13th European Confer-\nence, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I 13,\nSpringer, 2014, pp. 818–833.\n[11] M. Sundararajan, A. Taly, Q. Yan, Axiomatic attribution for deep net-\nworks, in: International conference on machine learning, PMLR, 2017,\npp. 3319–3328.\n[12] S. Bach, A. Binder, G. Montavon, F. Klauschen, K.-R. M¨uller, W. Samek,\nOn pixel-wise explanations for non-linear classifier decisions by layer-\nwise relevance propagation, PloS one 10 (7) (2015) e0130140.\n[13] M. Honnibal, I. Montani, spacy 2: Natural language understanding with\nbloom embeddings, convolutional neural networks and incremental pars-\ning, To appear 7 (1) (2017) 411–420.\n[14] O.-M. Camburu, T. Rockt ¨aschel, T. Lukasiewicz, P. Blunsom, e-snli: Nat-\nural language inference with natural language explanations, Advances in\nNeural Information Processing Systems 31 (2018).\n[15] V . Hassija, V . Chamola, A. Mahapatra, A. Singal, D. Goel, K. Huang,\nS. Scardapane, I. Spinelli, M. Mahmud, A. Hussain, Interpreting black-\nbox models: a review on explainable artificial intelligence, Cognitive\nComputation 16 (1) (2024) 45–74.\n[16] M. Pawlicki, A. Pawlicka, F. Uccello, S. Szelest, S. D’Antonio, R. Kozik,\nM. Chora´s, Evaluating the necessity of the multiple metrics for assessing\nexplainable ai: A critical examination, Neurocomputing (2024) 128282.\n[17] J. DeYoung, S. Jain, N. F. Rajani, E. Lehman, C. Xiong, R. Socher, B. C.\nWallace, Eraser: A benchmark to evaluate rationalized nlp models, arXiv\npreprint arXiv:1911.03429 (2019).\n[18] P. Atanasova, A diagnostic study of explainability techniques for text clas-\nsification, in: Accountable and Explainable Methods for Complex Rea-\nsoning over Text, Springer, 2024, pp. 155–187.\n[19] O. Luk ´as, S. Garc´ıa, Bridging the explanation gap in ai security: A task-\ndriven approach to xai methods evaluation., in: ICAART (3), 2024, pp.\n1370–1377.\n[20] L. S. Shapley, et al., A value for n-person games (1953).\n[21] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, D. Batra,\nGrad-cam: Visual explanations from deep networks via gradient-based lo-\ncalization, in: Proceedings of the IEEE international conference on com-\nputer vision, 2017, pp. 618–626.\n[22] K. Simonyan, A. Vedaldi, A. Zisserman, Deep inside convolutional net-\nworks: Visualising image classification models and saliency maps, arXiv\npreprint arXiv:1312.6034 (2013).\n[23] O. Barkan, E. Hauon, A. Caciularu, O. Katz, I. Malkiel, O. Armstrong,\nN. Koenigstein, Grad-sam: Explaining transformers via gradient self-\nattention maps, in: Proceedings of the 30th ACM International Confer-\nence on Information & Knowledge Management, 2021, pp. 2882–2887.\n[24] P. Fantozzi, M. Naldi, The explainability of transformers: Current status\nand directions, Computers 13 (4) (2024) 92.\n[25] Y . Wang, T. Zhang, X. Guo, Z. Shen, Gradient based feature attribution\nin explainable ai: A technical review, arXiv preprint arXiv:2403.10415\n(2024).\n[26] G. Montavon, S. Lapuschkin, A. Binder, W. Samek, K.-R. M ¨uller, Ex-\nplaining nonlinear classification decisions with deep taylor decomposi-\ntion, Pattern recognition 65 (2017) 211–222.\n[27] R. Achtibat, S. M. V . Hatefi, M. Dreyer, A. Jain, T. Wiegand, S. La-\npuschkin, W. Samek, Attnlrp: attention-aware layer-wise relevance prop-\nagation for transformers, arXiv preprint arXiv:2402.05602 (2024).\n[28] A. Ali, T. Schnake, O. Eberle, G. Montavon, K.-R. M ¨uller, L. Wolf, Xai\nfor transformers: Better explanations through conservative propagation,\nin: International Conference on Machine Learning, PMLR, 2022, pp.\n435–451.\n[29] Y . Yang, V . Tresp, M. Wunderle, P. A. Fasching, Explaining therapy pre-\ndictions with layer-wise relevance propagation in neural networks, in:\n2018 IEEE International Conference on Healthcare Informatics (ICHI),\nIEEE, 2018, pp. 152–162.\n[30] T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, T. Aila, Analyz-\ning and improving the image quality of stylegan, in: Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, 2020,\npp. 8110–8119.\n[31] P.-J. Kindermans, S. Hooker, J. Adebayo, M. Alber, K. T. Sch ¨utt,\nS. D ¨ahne, D. Erhan, B. Kim, The (un) reliability of saliency methods,\nExplainable AI: Interpreting, explaining and visualizing deep learning\n(2019) 267–280.\n[32] S. Wachter, B. Mittelstadt, C. Russell, Counterfactual explanations with-\nout opening the black box: Automated decisions and the gdpr, Harv. JL\n& Tech. 31 (2017) 841.\n[33] A. Jacovi, Y . Goldberg, Towards faithfully interpretable nlp sys-\ntems: How should we define and evaluate faithfulness?, arXiv preprint\narXiv:2004.03685 (2020).\n[34] G. Bansal, T. Wu, J. Zhou, R. Fok, B. Nushi, E. Kamar, M. T. Ribeiro,\nD. Weld, Does the whole exceed its parts? the e ffect of ai explanations\non complementary team performance, in: Proceedings of the 2021 CHI\nconference on human factors in computing systems, 2021, pp. 1–16.\n[35] P. Lertvittayakumjorn, F. Toni, Human-grounded evaluations of expla-\nnation methods for text classification, arXiv preprint arXiv:1908.11355\n(2019).\n[36] L. Arras, A. Osman, W. Samek, Clevr-xai: A benchmark dataset for the\nground truth evaluation of neural network explanations, Information Fu-\nsion 81 (2022) 14–40.\n[37] O. Arreche, T. R. Guntur, J. W. Roberts, M. Abdallah, E-xai: Evaluat-\ning black-box explainable ai frameworks for network intrusion detection,\nIEEE Access (2024).\n[38] M. Mersha, M. Bitewa, T. Abay, J. Kalita, Explainability in neu-\nral networks for natural language processing tasks, arXiv preprint\narXiv:2412.18036 (2024).\n[39] C. Guan, X. Wang, Q. Zhang, R. Chen, D. He, X. Xie, Towards a deep\n11\nand unified understanding of deep neural models in nlp, in: International\nconference on machine learning, PMLR, 2019, pp. 2454–2463.\n[40] M. G. Yigezu, M. A. Mersha, G. Y . Bade, J. Kalita, O. Kolesnikova,\nA. Gelbukh, Ethio-fake: Cutting-edge approaches to combat fake\nnews in under-resourced languages using explainable ai, arXiv preprint\narXiv:2410.02609 (2024).\n[41] A. L. Tonja, M. Mersha, A. Kalita, O. Kolesnikova, J. Kalita, First attempt\nat building parallel corpora for machine translation of northeast india’s\nvery low-resource languages, arXiv preprint arXiv:2312.04764 (2023).\n[42] M. Nauta, J. Trienes, S. Pathak, E. Nguyen, M. Peters, Y . Schmitt,\nJ. Schl ¨otterer, M. van Keulen, C. Seifert, From anecdotal evidence to\nquantitative evaluation methods: A systematic review on evaluating ex-\nplainable ai, ACM Computing Surveys 55 (13s) (2023) 1–42.\n[43] M. Mersha, K. Lam, J. Wood, A. AlShami, J. Kalita, Explainable artifi-\ncial intelligence: A survey of needs, techniques, applications, and future\ndirection, Neurocomputing (2024) 128111.\n[44] A. Rosenfeld, Better metrics for evaluating explainable artificial intelli-\ngence (2021).\n[45] S. Nogueira, K. Sechidis, G. Brown, On the stability of feature selection\nalgorithms, Journal of Machine Learning Research 18 (174) (2018) 1–54.\n[46] J. Huang, A. Mishra, B. C. Kwon, C. Bryan, Conceptexplainer: Inter-\nactive explanation for deep neural networks from a concept perspective,\nIEEE Transactions on Visualization and Computer Graphics 29 (1) (2022)\n831–841.\n[47] C. Spearman, The proof and measurement of association between two\nthings. (1961).\n[48] I. Stepin, J. M. Alonso, A. Catala, M. Pereira-Fari ˜na, A survey of con-\ntrastive and counterfactual explanation generation methods for explain-\nable artificial intelligence, IEEE Access 9 (2021) 11974–12001.\n[49] S. Kullback, R. A. Leibler, On information and su fficiency, The annals of\nmathematical statistics 22 (1) (1951) 79–86.\n[50] X. Jiao, Y . Yin, L. Shang, X. Jiang, X. Chen, L. Li, F. Wang, Q. Liu, Tiny-\nbert: Distilling bert for natural language understanding, arXiv preprint\narXiv:1909.10351 (2019).\n[51] A. Conneau, K. Khandelwal, N. Goyal, V . Chaudhary, G. Wenzek,\nF. Guzm ´an, E. Grave, M. Ott, L. Zettlemoyer, V . Stoyanov, Unsu-\npervised cross-lingual representation learning at scale, arXiv preprint\narXiv:1911.02116 (2019).\n[52] P. He, X. Liu, J. Gao, W. Chen, Deberta: Decoding-enhanced bert with\ndisentangled attention, arXiv preprint arXiv:2006.03654 (2020).\n[53] P.-J. Kindermans, K. T. Sch¨utt, M. Alber, K.-R. M¨uller, D. Erhan, B. Kim,\nS. D¨ahne, Learning how to explain neural networks: Patternnet and pat-\nternattribution, arXiv preprint arXiv:1705.05598 (2017).\n[54] H. Chefer, S. Gur, L. Wolf, Transformer interpretability beyond attention\nvisualization, in: Proceedings of the IEEE /CVF conference on computer\nvision and pattern recognition, 2021, pp. 782–791.\nAppendix A. Human-reasoning Agreement Example\nWe proposed four evaluation metrics: Human-reasoning\nAgreement, Consistency, Robustness, and Contrastivity. In this\nframework, human annotators rank only the most important\nwords, while precision and rel (k) are computed automatically\nwithout human intervention.\nThe primary objective of the Human-reasoning Agreement\n(HA) metric is to rigorously evaluate XAI explanations, en-\nsuring they align accurately with human judgment, particu-\nlarly in safety-critical domains such as healthcare, finance, au-\ntonomous vehicles, aerospace, nuclear energy, defense, trans-\nportation safety, and legal compliance. HA’s restricted rank-\ning approach is crucial for achieving exact interpretability and\nalignment with human priorities in these fields, where misinter-\npreting AI predictions can lead to severe consequences. HA en-\nsures that XAI explanations support reliable and safe decision-\nmaking by focusing on strict ranking alignment, reducing the\nrisk of errors, and enhancing the AI model’s capacity to operate\nwithin strict safety and regulatory standards.\nAverage Precision (AP) for a Single Instance\nAP =\nPn\nk=1(P(k) ×rel(k))\nNumber of relevant tokens (n) (A.1)\nwhere:\n• k is the rank in the sequence of retrieved relevant words,\n• n is the total number of relevant words (as determined by\nhuman annotations),\n• P(k) is the precision at rank k,\n• rel(k) is an indicator function that equals 1 if the word at\nrank k matches the human annotation; otherwise, it equals\n0.\nMean Average Precision (MAP) across Multiple Instances\nMAP = 1\nN\nNX\nn=1\nAPn (A.2)\nwhere:\n• N is the total number of instances evaluated,\n• APn is the Average Precision for the n-th instance.\nExample\n(Note: This example is not actual program output; it is pro-\nvided solely to illustrate how this metric functions.)\nInput instance: ” The movie was absolutely fantastic, fasci-\nnating, and delightful.”\nStep 1: Human Annotation and Ranking (Gold Standard)\nHuman annotators rank the words by importance for positive\nsentiment:\n{fantastic, fascinating, absolutely, delightful, movie}\nStep 2: XAI Explanation and Ranking\nWe rank important words based on the XAI explanation\nscores as follows:\n{fantastic, fascinating, absolutely, movie, delightful}\nStep 3: Computation of Relevance rel(k) and Precision P(k)\nRank 1(XAI: “fantastic”, Human: “fantastic”)\nRelevance rel(1): ”fantastic” matches in both lists, so rel(1) =\n1.\nPrecision P(1): Computed as the number of relevant words up\nto rank 1 divided by 1: P(1) = 1\n1 = 1\nRank 2(XAI: “fascinating”, Human: “fascinating”)\nRelevance rel(2): ”Fascinating” also matches, so rel(2) = 1.\nPrecision P(2): Computed as the number of relevant words up\nto rank 2 divided by 2: P(2) = 2\n2 = 1\nRank 3(XAI: “absolutely”, Human: “absolutely”)\nRelevance rel(3): ”Absolutely” matches, so rel(3) = 1.\nPrecision P(3): Computed as the number of relevant words up\nto rank 3 divided by 3: P(3) = 3\n3 = 1\nRank 4(XAI: “movie”, Human: “delightful”)\nRelevance rel(4): ”Movie” does not match with ”delightful,”\nso rel(4) = 0.\n12\nPrecision P(4): Computed as the number of relevant words up\nto rank 4 divided by 4: P(4) = 3\n4 = 0.75\nRank 5(XAI: “delightful”, Human: “movie”)\nRelevance rel(5): ”Delightful” does not match with ”movie,”\nso rel(5) = 0.\nPrecision P(5): Computed as the number of relevant words up\nto rank 5 divided by 5: P(5) = 3\n5 = 0.6\nSummary\nRank XAI Word Human Word rel(k) P(k)\n1 fantastic fantastic 1 1\n2 fascinating fascinating 1 1\n3 absolutely absolutely 1 1\n4 movie delightful 0 0.75\n5 delightful movie 0 0.6\nAverage Precision (AP):\nAP = (1 ×1) + (1 ×1) + (1 ×1) + (0.75 ×0) + (0.6 ×0)\n5\n= 1 + 1 + 1 + 0 + 0\n5 = 3\n5 = 0.6\nThus, for this single instance, AP = 0.6.\nMean Average Precision (MAP)\nIf we evaluate multiple instances, MAP would be calculated as\neach instance’s mean of AP scores. Since this example uses\nonly one instance, MAP = AP = 0.6.\nHence, The higher the Average Precision (AP) and Mean Av-\nerage Precision (MAP) scores, the stronger the alignment or\nagreement between the XAI explanations and human reason-\ning.\n13\nAppendix B. Sample XAI visualization Outputs\nThis appendix provides a comprehensive perspective on how different models and explainability methods handle sentiment analy-\nsis. we present sample visualizations of the decision-making process for sentiment predictions on different datasets, utilizing various\nexplainability methods, including SHAP, LIME, InputXGradient, Grad-CAM, Attention Visualization, and Layer-wise Relevance\nPropagation (LRP). Each figure highlights the words contributing most significantly to the sentiment predictions, o ffering insight\ninto how models like TinyBERT, BERT base, BERT-large, XLMR, and DeBERTa interpret input text data. For this visualization,\nwe selected two sample texts from the IMDB and TSE datasets, applying various models and explanation methods to compare how\neach interprets and attributes sentiment. This visualization comparison offers insight into model behavior and reveals how different\nexplainability techniques illustrate the model decision-making process.\nFigure B.3: SHAP explanation of BERT base model’s IMDB movie sentiment prediction. Positive words are highlighted in red and negative words in blue.\nFigure B.4: LIME explanation of BERT large model’s IMDB movie sentiment prediction. Positive words are highlighted in orange and negative words in blue.\nFigure B.5: Grad-CAM explanation of DeBERTa model’s positive sentiment prediction on IMDB movie review (Yellow indicates high importance and dark\npurple indicates less importance).\n14\nFigure B.6: Attention Mechanism visualizationexplanation of XLMR model’s positive sentiment prediction on IMDB movie review (Yellow indicates high\nimportance and dark purple indicates less importance).\nFigure B.7: InputXGradient explanation of TinyBERT model’s positive sentiment prediction on IMDB movie review, highlighting the words that contribute most\nto the model’s prediction. Stronger red intensity indicates higher relevance to the prediction, with words like ’beautiful,’ ’fantastic,’ ’wonderful,’ and ’relationship’\nsuggesting a positive sentiment.\nFigure B.8: LIME explanation of BERT large model’s TSE dataset sentiment prediction. Positive words are highlighted in orange, and negative words in blue.\nFigure B.9: InputXGradient explanation of XLMR model’s positive sentiment prediction on Tweet Sentiment Extraction (TSE) dataset, highlighting the words\nthat contribute most to the model’s prediction. Stronger red intensity indicates higher relevance to the prediction, with words like ’misses,’ ’today,’ ’even though,’\n’gone,’ ’several years,’ ’still,’ ’dearly,’ and ’happy’ suggesting a positive sentiment.\nFigure B.10: LRP explanation of BERT large model’s TSE dataset sentiment prediction. Higher-importance words are highlighted with greater intensities of\ngreen, indicating their strong positive contribution to the BERT large model’s prediction\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.617129385471344
    },
    {
      "name": "Natural language processing",
      "score": 0.38538211584091187
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33561110496520996
    }
  ],
  "institutions": [],
  "cited_by": 7
}