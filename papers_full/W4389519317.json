{
    "title": "A Comparative Analysis of Task-Agnostic Distillation Methods for Compressing Transformer Language Models",
    "url": "https://openalex.org/W4389519317",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5067751383",
            "name": "Takuma Udagawa",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5080640298",
            "name": "Aashka Trivedi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5068061267",
            "name": "Michele Merler",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5069125151",
            "name": "Bishwaranjan Bhattacharjee",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4285204619",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W3174544005",
        "https://openalex.org/W3177265267",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3170233084",
        "https://openalex.org/W4385567464",
        "https://openalex.org/W3177378457",
        "https://openalex.org/W4382463788",
        "https://openalex.org/W3034457371",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2998183051",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4283789538",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W3154971029",
        "https://openalex.org/W2997006708",
        "https://openalex.org/W2975381464",
        "https://openalex.org/W3152607317",
        "https://openalex.org/W2975429091",
        "https://openalex.org/W3116594510",
        "https://openalex.org/W4287854320",
        "https://openalex.org/W3213180921",
        "https://openalex.org/W3200786561",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2970454332",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W4385571003",
        "https://openalex.org/W4226126941",
        "https://openalex.org/W4386576838",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W3212072588",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W4386566583",
        "https://openalex.org/W4206634569",
        "https://openalex.org/W3184738308",
        "https://openalex.org/W3115511229",
        "https://openalex.org/W3008374555",
        "https://openalex.org/W4327989871",
        "https://openalex.org/W3103884771",
        "https://openalex.org/W3203532272",
        "https://openalex.org/W3173374050",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W3199246732",
        "https://openalex.org/W3095273266",
        "https://openalex.org/W3170113752",
        "https://openalex.org/W3169008558",
        "https://openalex.org/W3101248447",
        "https://openalex.org/W3035030897",
        "https://openalex.org/W4285269381",
        "https://openalex.org/W4285202066",
        "https://openalex.org/W4292779060"
    ],
    "abstract": "Large language models have become a vital component in modern NLP, achieving state of the art performance in a variety of tasks. However, they are often inefficient for real-world deployment due to their expensive inference costs. Knowledge distillation is a promising technique to improve their efficiency while retaining most of their effectiveness. In this paper, we reproduce, compare and analyze several representative methods for task-agnostic (general-purpose) distillation of Transformer language models. Our target of study includes Output Distribution (OD) transfer, Hidden State (HS) transfer with various layer mapping strategies, and Multi-Head Attention (MHA) transfer based on MiniLMv2. Through our extensive experiments, we study the effectiveness of each method for various student architectures in both monolingual (English) and multilingual settings. Overall, we show that MHA transfer based on MiniLMv2 is generally the best option for distillation and explain the potential reasons behind its success. Moreover, we show that HS transfer remains as a competitive baseline, especially under a sophisticated layer mapping strategy, while OD transfer consistently lags behind other approaches. Findings from this study helped us deploy efficient yet effective student models for latency-critical applications.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track, pages 20–31\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nA Comparative Analysis of Task-Agnostic Distillation Methods\nfor Compressing Transformer Language Models\nTakuma Udagawa, Aashka Trivedi, Michele Merler, Bishwaranjan Bhattacharjee\nIBM Research AI\n{takuma.udagawa@, aashka.trivedi@, mimerler@us., bhatta@us.}ibm.com\nAbstract\nLarge language models have become a vital\ncomponent in modern NLP, achieving state of\nthe art performance in a variety of tasks. How-\never, they are often inefﬁcient for real-world\ndeployment due to their expensive inference\ncosts. Knowledge distillation is a promising\ntechnique to improve their efﬁciency while re-\ntaining most of their effectiveness. In this pa-\nper, we reproduce, compare and analyze sev-\neral representative methods for task-agnostic\n(general-purpose) distillation of Transformer\nlanguage models. Our target of study in-\ncludes Output Distribution (OD) transfer, Hid-\nden State (HS) transfer with various layer\nmapping strategies, and Multi-Head Attention\n(MHA) transfer based on MiniLMv2. Through\nour extensive experiments, we study the effec-\ntiveness of each method for various student ar-\nchitectures in both monolingual (English) and\nmultilingual settings. Overall, we show that\nMHA transfer based on MiniLMv2 is gener-\nally the best option for distillation and ex-\nplain the potential reasons behind its success.\nMoreover, we show that HS transfer remains\nas a competitive baseline, especially under\na sophisticated layer mapping strategy, while\nOD transfer consistently lags behind other ap-\nproaches. Findings from this study helped us\ndeploy efﬁcient yet effective student models\nfor latency-critical applications.\n1 Introduction\nLarge language models have become a crucial com-\nponent in modern NLP. They have achieved excep-\ntional performance on various downstream tasks\n(Devlin et al., 2019; Liu et al., 2019; Lewis et al.,\n2020) and their capability shows consistent im-\nprovement with more compute, data, and model\nparameters (Kaplan et al., 2020; Brown et al., 2020;\nTouvron et al., 2023). On the downside, it is becom-\ning increasingly difﬁcult to deploy such models in\nreal-world environments due to their inefﬁciency,\ni.e. high computation, memory, latency and storage\ncosts (Xu and McAuley, 2023).\nKnowledge distillation (Hinton et al., 2015) is a\npromising technique to overcome this challenge by\ntransferring the knowledge of the original model\n(teacher) to a smaller, more efﬁcient model (stu-\ndent). This can be conducted in either task-speciﬁc\n(Turc et al., 2019; Jiao et al., 2020) ortask-agnostic\nmanner (Sanh et al., 2019; Wang et al., 2020).\nThe latter only requires distilling a single general-\npurpose student which can be directly ﬁnetuned on\nany downstream task. Due to its high convenience,\nwe focus on this latter approach in this study.\nIn recent years, there have been various meth-\nods proposed for task-agnostic distillation of Trans-\nformer language models. The aim of this paper is\nto reproduce, compare and analyze the most rep-\nresentative methods in this area. We generally fo-\ncus on the architecture-agnostic distillation which\nimposes no or minimal restriction on the student\narchitecture1: the representative methods include\nOutput Distribution (OD) transfer (Hinton et al.,\n2015), Hidden State (HS) transfer based on linear\nmapping (Jiao et al., 2020; Mukherjee et al., 2021)\nand Multi-Head Attention (MHA) transfer based\non MiniLMv2 (Wang et al., 2021).\nFor HS transfer, the layer mapping strategy be-\ntween teacher and student layers plays a signiﬁ-\ncant role in overall performance, however, the op-\ntimal strategy remains unknown or controversial\n(Sun et al., 2019; Wu et al., 2020; Ko et al., 2023).\nTherefore, we explore a diverse range of strategies\nto empirically evaluate each technique.\nFor MHA transfer, the MiniLMv2 approach has\nbeen shown to achieve state-of-the-art performance,\nhowever, there is relatively little understanding be-\nhind its success. Therefore, we develop a novel\nvariant named DirectMiniLM which is useful for\n1By architecture-agnostic, we mean that the student and\nteacher can have different architectural parameters (e.g. num-\nber of layers, attention heads, hidden state size, etc).\n20\nIBM Research\nFeed-Forward LayerMulti-Head Attention Layer\nStudentTeacher\n×L\nStudentTeacherStudentTeacher\nStudentTeacherStudentTeacherStudentTeacher\n(a) Transformer LM\nIBM Research\nFeed-Forward LayerMulti-Head Attention Layer\nStudentTeacher\n×L\nStudentTeacherStudentTeacher\nStudentTeacherStudentTeacherStudentTeacher (b) OD Transfer\nIBM Research\nFeed-Forward LayerMulti-Head Attention Layer\nStudentTeacher\n×L\nStudentTeacherStudentTeacher\nStudentTeacherStudentTeacherStudentTeacher (c) HS Transfer\nIBM Research\nFeed-Forward LayerMulti-Head Attention Layer\nStudentTeacher\n×L\nStudentTeacherStudentTeacher\nStudentTeacherStudentTeacherStudentTeacher (d) MHA Transfer\nFigure 1: A high-level illustration of (a) the Transformer architecture and (b-d) representative distillation methods.\n(b-d) denote Output Distribution (OD), Hidden State (HS), and Multi-Head Attention (MHA) transfer, respectively.\nLines between the student and teacher depict which level of information is transferred in each method.\nunderstanding the effectiveness behind MiniLMv2\nboth theoretically and empirically.\nIn contrast to most previous studies, all methods\nare reproduced on a single uniﬁed codebase for fair\nand consistent comparison. We also conduct distil-\nlation on 4 different student architectures, reducing\nthe model size in various dimensions to ﬁt various\nparameter and latency budgets. Finally, all experi-\nments are conducted on both monolingual and mul-\ntilingual settings, distilled from open-source BERT\n(Devlin et al., 2019) and in-house XLM-RoBERTa\n(Conneau et al., 2020), respectively.\nThrough our extensive experiments, we criti-\ncally analyze the effectiveness of each distillation\nmethod and provide practical advice for both re-\nsearchers and practitioners working in this area. In\nsummary, our key ﬁndings are:\n• MHA transfer is generally the best option for\nvarious student architectures and language set-\ntings. By comparison with DirectMiniLM, we\nprovide novel insights underlying its success.\n• While the effectiveness of HS transfer depends\non the layer mapping strategy, it remains as a\ncompetitive baseline. More sophisticated layer\nmapping strategy can provide a boost in perfor-\nmance, esp. in the multilingual setting.\n• Methods relying on OD transfer consistently lag\nbehind other methods. This shows that classical\nOD distillation can be less effective when dis-\ntilling complex language models on a general-\npurpose objective.\n2 Transformer Language Models\nFirst, we brieﬂy review the standard architecture\nof Transformer language models (Vaswani et al.,\n2017; Devlin et al., 2019). A Transformer consists\nof a stack of L Transformer layers, where each\nlayer comprises two sub-layers: a Multi-Head At-\ntention (MHA) layer followed by a fully connected\nFeed-Forward (FF) layer (Figure 1, (a)).\nFormally, let x denote the input sequence, dh\nthe hidden state size, and Hi ∈R|x|×dh the hid-\nden state of the ith Transformer layer (H0 denotes\nthe input sequence embeddings). Given Hi, the\nMHA layer ﬁrst computes the query, key, and value\nmappings Qi,a, Ki,a, Vi,a for each attention head\na∈[1,Ah], which are combined to obtain the at-\ntention head output Oi,a:\nQi,a = HiWQ,i,a (1)\nKi,a = HiWK,i,a (2)\nVi,a = HiWV,i,a (3)\nOi,a = softmax(\nQi,aKT\ni,a√dk\n)Vi,a (4)\nHere, dk denotes the attention head size (typically\nset to dh\nAh\n) and WQ,i,a,WK,i,a,WV,i,a ∈Rdh×dk\nare the learnt weight matrices. The output of the\nMHA layer is the concatenation of Oi,a, namely\nMHA(Hi) =⨁Ah\na=1 Oi,a.\nNext, the MHA layer output is followed by a\nposition-wise FF layer with an intermediate size\nof df and a non-linear activation (we use GELU\n(Hendrycks and Gimpel, 2016) in all models). The\nhidden state of the next Transformer layer is com-\nputed as Hi+1 = FF(MHA(Hi)).2\nFinally, to predict the output distribution over the\nentire vocabulary V, a linear layer WO ∈Rdh×|V|\nis applied on top of the last hidden state to compute\nthe logits z = HLWO ∈R|x|×|V|. The output dis-\ntribution can be obtained by applying the softmax\nfunction over z, denoted as softmax(z).\nThroughout this paper, we assume that both the\nstudent and teacher are Transformer language mod-\nels with LS and LT layers, respectively.\n2Both MHA and FF sub-layers have a residual connection\n(He et al., 2016) and are followed by layer normalization (Ba\net al., 2016), which are omitted for brevity.\n21\n3 Distillation Methods\nNext, we introduce the representative task-agnostic\ndistillation methods illustrated in Figure 1, (b-d).\nFor Multi-Head Attention (MHA) transfer, we con-\nsider two approaches: MiniLMv2 and its novel\nvariant DirectMiniLM. For a survey of advanced\nmethods and topics we could not cover in this study,\nplease refer to Appendix A.\nOutput Distribution (OD) TransferThe output\ndistribution of the teacher contains useful infor-\nmation on the relative probabilities of plausible\n(even if incorrect) predictions (Hinton et al., 2015).\nIn OD transfer, the student is trained to replicate\nthe teacher’s output distribution. This is achieved\nby optimizing the following loss function, where\nzS,zT denote the student/teacher logits, CE(.) the\ncross entropy loss and T the output temperature:\nLOD = T2 ·CE\n(\nsoftmax\n(zT\nT\n)\n,softmax\n(zS\nT\n))\n(5)\nHidden State (HS) Transfer Transformer lan-\nguage models progressively learn useful and gen-\neralizable features layer by layer. In HS transfer,\nthe student is trained to predict such useful features\nrepresented in the teacher’s hidden states.\nFormally, each student layer is mapped to a set\nof teacher layers to be predicted. Let φ(i) denote\nthe set mapped from the ith student layer, where\n∅⊆ φ(i) ⊆[1,LT]. For each j ∈φ(i), the hid-\nden state of the ith student layer HS\ni ∈R|x|×dS\nh\nis linearly transformed to predict the hidden state\nof the jth teacher layer HT\nj ∈R|x|×dT\nh .3 This is\nrepresented by the following loss function, where\nWj\ni ∈RdS\nh×dT\nh denotes the linear transformation\nweight and MSE(.) the mean squared error loss:\nLHS =\nLS\n∑\ni=1\n∑\nj∈φ(i)\nMSE\n(\nHS\ni Wj\ni,HT\nj\n)\n(6)\nOne open problem in this approach is the choice\nof layer mapping strategy φ. We conduct extensive\nexperiments to compare a diverse range of strate-\ngies, which will be discussed in §4.\nMiniLMv2 The MHA layer is a key component\nin Transformer language models which controls the\nlong-range dependencies and interactions within\ninput texts. MiniLMv2 (Wang et al., 2021) is an\n3Note that dS\nh and dT\nh are the student and teacher hidden\nstate sizes which can take different values.\neffective method to deeply transfer this module\nwhile allowing different number of attention heads\nAS\nh and AT\nh for the student and teacher. Their main\nidea is to distil the attentionrelation matrices (Q-Q,\nK-K and V-V) obtained by ﬁrst concatenating the\nquery (Q), key (K), and value (V) mappings from\nall attention heads and re-splitting them into the\nsame number of attention relation heads Ar.\nFormally, let AS\nQ,i,a,AS\nK,i,a,AS\nV,i,a ∈R|x|×dS\nr\ndenote the concatenated and re-split queries, keys,\nand values for the ith student layer, where a ∈\n[1,Ar] and dS\nr = dS\nh\nAr . For instance, ⨁AS\nh\na=1 QS\ni,a =⨁Ar\na=1 AS\nQ,i,a, i.e. original queries from AS\nh atten-\ntion heads are simply concatenated and then re-\nsplit into Ar matrices. We use the same notation\nfor the jth teacher layer, AT\nQ,j,a,AT\nK,j,a,AT\nV,j,a ∈\nR|x|×dT\nr , where dT\nr = dT\nh\nAr . Then, the loss function\nof MiniLMv2 can be deﬁned as follows:\nLMHA =\n∑\nα∈{Q,K,V}\nAr∑\na=1\nCE\n(\nRT\nα,j,a,RS\nα,i,a\n)\n(7)\nRT\nα,j,a = softmax\n(AT\nα,j,aATT\nα,j,a√\ndTr\n)\n(8)\nRS\nα,i,a = softmax\n(AS\nα,i,aAST\nα,i,a√\ndSr\n)\n(9)\nHere, RT\nα,j,a,RS\nα,i,a ∈R|x|×|x| denote the atten-\ntion relation matrices which are computed based\non the matrix products of AT\nα,i,a,AS\nα,i,a in eq. (8),\n(9), respectively. Intuitively, this aims to transfer\nthe teacher’s queries (Q), keys (K) and values (V)\nin a somewhat indirect way through their matrix\nproducts (Q-Q, K-K and V-V).\nHowever, there is minimal justiﬁcation for why\nthis method works effectively. It is also difﬁcult\nto directly compare the method against HS trans-\nfer since the losses are computed differently. To\nbetter understand MiniLMv2, we propose its novel\nvariant named DirectMiniLM for our analysis.\nDirectMiniLM In DirectMiniLM, we aim to\ntransfer the teacher’s Q/K/V mappings more di-\nrectly through the linear transformation of the stu-\ndent’s ones, just as we did in HS transfer. Speciﬁ-\ncally, we use the following loss function with the\nlinear transformation Wα,a ∈RdS\nr ×dT\nr :\nLDirect\nMHA =\n∑\nα∈\n{Q,K,V}\nAr∑\na=1\nMSE\n(\nAS\nα,i,aWα,a,AT\nα,j,a\n)\n(10)\n22\nDirectMiniLM is important in two aspects. First,\nthis approach is directly comparable to HS trans-\nfer based on eq. (6) with the only difference in\nwhich information you transfer: the hidden states\nHT\ni → HS\nj or the Q/K/V mappings AT\nα,i,a →\nAS\nα,j,a. From this comparison, we can quantify the\nprecise advantage of transferring each knowledge\nin an apples-to-apples manner.\nSecond, DirectMiniLM is also closely relevant to\nMiniLMv2: if we constrain Wα,a to be orthogonal\n(i.e. Wα,aWT\nα,a = I) and take the matrix product\nfor each term within the MSE loss in eq. (10), we\nobtain the following loss function:\n∑\nα∈\n{Q,K,V}\nAr∑\na=1\nMSE\n(\nAS\nα,i,aAST\nα,i,a,AT\nα,j,aATT\nα,i,a\n)\n(11)\nThis loss closely resembles MiniLMv2 from eq. (7)\nwith a minor difference of using MSE loss instead\nof CE loss with softmax. Therefore, DirectMiniLM\nwith certain constraints naturally corresponds to\nMiniLMv2. The major difference is in whether\nAT\nα,i,a is transferred directly (with linear mappings)\nor indirectly (with relation matrices): by comparing\nthese two approaches, we can precisely quantify\nthe advantage of each optimization technique.\n4 Experimental Setup\nWe explore the task-agnostic knowledge distillation\nmethods under two settings:4\n1. Monolingual Distillation: We train English\nstudents using the open-source BERT (Devlin\net al., 2019) as the teacher. These models are\ndistilled on the same corpus used for pretrain-\ning BERT, i.e., English Wikipedia (Devlin et al.,\n2019) and BookCorpus (Zhu et al., 2015).\n2. Multilingual Distillation: We train multilingual\nstudents using our in-house XLM-RoBERTa\n(Conneau et al., 2020) as the teacher, and distill\non the CC100 dataset (Conneau et al., 2020),\nwhich consists of data in more than 100 lan-\nguages. We only use a small subset of the cor-\npus to conduct our experiments within a reason-\nable computation budget while maintaining the\nlanguage-wise distribution.\nIn both settings, we use the Base (12 layer) archi-\ntecture for the teacher, as shown in Table 1. For\n4Note that we limit our study to encoder-only models and\nleave the distillation of decoder-only (Radford et al., 2019) or\nencoder-decoder (Lewis et al., 2020) models as future work.\nmore details on each distillation setup (e.g. hyper-\nparameters), please refer to Appendix B.\nStudent Models To conduct a strong comparison\nof the representative knowledge distillation meth-\nods, we train 4 students of varying architectures\nand latency/parameter budgets. A summary of the\nstudent architectures, with their parameters and\nlatency of inference, are shown in Table 1.\nOur largest student is a 6 layer model that fol-\nlows the same architecture as DistilBERT (Sanh\net al., 2019). We also use the 6 layer model used\nin Mukherjee et al. (2021), which has a smaller\nhidden size than the teacher. Our smaller 4 and 3\nlayer students were obtained as recommendations\nfrom a Neural Architecture Search process (Trivedi\net al., 2023) to ﬁnd good student architectures for\ndistillation from the XLM-RoBERTa teacher, con-\nditioned to minimize the latency on CPU. Please\nrefer to Appendix C for more details.\nLayer Mapping Strategies The layer mapping\nstrategy φis a parameter that needs to be consid-\nered for both HS and MHA transfer. For HS trans-\nfer, we explore the following three settings:\n1. Single Mapping: We only distil the last (LTth)\nteacher layer into the last student layer, which\nhas been shown to be a simple yet competitive\nbaseline (Ko et al., 2023).\n2. 1-to-1 Mapping: Prior work shows that map-\nping not only the last layer but also the inter-\nmediate layers improves distillation (Sun et al.,\n2019). In 1-to-1 mapping, we distil one teacher\nlayer into each student layer by choosing:\n• Last LS teacher layers, i.e. φ(i) ={LT −\nLS + i} (i ∈ [1,LS]). Empirically, last\nteacher layers capture more high-level (e.g.\nsemantic) knowledge in their representations\n(Tenney et al., 2019; Jawahar et al., 2019).\n• A Uniform selection of teacher layers which\nchooses every kth teacher layer, i.e. φ(i) =\n{ki}, where k = ⌈LT/LS⌉.5 This method\ncan also transfer the lower teacher layers,\nwhich empirically captures local (e.g. syn-\ntactic) knowledge (Tenney et al., 2019).\n3. 1-to-N Mapping: Some works even show that\nmapping each student layer to multiple teacher\nlayers can avoid the loss of information and fa-\ncilitate student learning (Wu et al., 2020; Pass-\nban et al., 2021). For 1-to-N Mapping, we ex-\n5This strategy is used in DistilBERT (Sanh et al., 2019)\nand also known as the \"skip\" strategy (Sun et al., 2019).\n23\nModel Architecture Monolingual Multilingual Monolingual Latency Multilingual Latency\nParams Params GPU CPU GPU CPU\n6L-DistilBERT 6, 12, 768, 3072 66 234 5.98 (0.03) 33.28 (0.09) 6.01 (0.06) 34.02(0.06)\n6L 6, 12, 384, 1536 23 106 5.69 (0.02) 11.98 (0.07) 5.99 (0.07) 12.52 (0.06)\n4L 4, 12, 576, 768 27 153 3.66 (0.01) 9.53 (0.04) 3.98 (0.02) 9.66 (0.05)\n3L 3, 12, 384, 1024 16 100 3.02 (0.01) 5.41 (0.08) 3.25 (0.01) 6.01 (0.06)\nTeacher 12, 12, 768, 3072 110 277 8.69 (0.08) 64.91 (0.61) 9.47 (0.01) 66.31 (0.57)\nTable 1: Model Architectures displayed as [ L, Ah, dh, df ]. All parameters are in millions, with the difference\nin the monolingual and multilingual parameters due to the vocabulary sizes (30K for monolingual and 252K for\nmultilingual). All latencies are in milliseconds, measured over 5 runs, with standard deviation in parenthesis.\nDistillation Method Layer Mapping Strategies\nSingle:LTth\nHS Transfer 1-to-1: Last, Uniform\n1-to-N: Uniform-Cons., Uniform+Last\nMHA Transfer Single:LTth, (LT−1)th, (LT−2)th\nTable 2: Layer mapping strategies explored in each\ndistillation method. The same strategies are explored\nfor MiniLMv2 and DirectMiniLM in MHA Transfer.\nplore the following choices of teacher layers:\n• A uniform selection of kconsecutive layers\n(Uniform-Cons.), i.e. φ(i) = [k(i−1),ki],\nwhere k = ⌈LT/LS⌉. This avoids the loss\nof information since all teacher layers are\nmapped to at least one student layer.\n• Combining the Uniform and Last strategies\nfrom the 1-to-1 mapping ( Uniform+Last).\nThis selects 2 teacher layers per student layer\nbased on each 1-to-1 strategy, expecting to\ntake the best out of both approaches.\nFor MHA transfer, we always take the single\nmapping strategy and distill a single teacher layer\ninto the last student layer, following Wang et al.\n(2021). Speciﬁcally, we experiment with the last\nthree teacher layers as a choice for distillation for\nboth MiniLMv2 and DirectMiniLM. Table 2 sum-\nmarizes our layer selection options.\nWhile OD transfer can be conducted from\nscratch, we found this converges slowly and does\nnot perform competitively.6 Therefore, we take the\nstyle of multi-stage distillation (Mukherjee et al.,\n2021) and conduct OD transfer after HS transfer,\nusing the distilled checkpoint from HS transfer.\nThis approach converges much faster with better\nﬁnal performance, hence we take this approach as\nthe representative OD transfer method.\n6Our 6L monolingual student takes 49 hours on 30 V100\nGPUs to reach acceptable performance, while the same model\nachieves better scores in only 10.5 hours when initialized from\nthe HS transferred checkpoint.\n5 Evaluation and Results\nFor both our monolingual and multilingual models,\nwe measure performance on the English GLUE\nBenchmark (Wang et al., 2019) and report the av-\nerage score of all tasks (without CoLA7). For mul-\ntilingual models, we provide evaluations on the\nXNLI dataset (Conneau et al., 2018), a set of in-\nference tasks which evaluates the model’s perfor-\nmance on 15 languages after being ﬁnetuned on\nonly English training data. We report the average\nscore of all languages for XNLI.\nTable 3 summarizes the performance of each\ndistillation method on 4 student architectures. For\ndetailed evaluations of each method based on the\nbest conﬁguration, please refer to Appendix D.\nWe also provide a comparison against DistilBERT\n(Sanh et al., 2019), a representative architecture-\nconstrained method, in Appendix E.\nHS Transfer From Table 3, we can verify that\nthe performance of HS transfer varies with different\nlayer mapping strategies, and no strategy dominates\nthe others in all settings. In the monolingual setting,\nwe found that the single mapping strategy performs\ncompetitively, which is in line with the ﬁndings of\nKo et al. (2023). However, in the multilingual set-\nting, more sophisticated 1-to-N strategies generally\nshow superiority over the simpler baselines. This\nindicates that more supervision from the teacher\ncan be helpful (and at worst harmless), hence we\nadvocate for the adoption 1-to-N strategies, esp. in\nthe challenging multilingual distillation.\nOD Transfer As mentioned in §4, we initialize\nthe model from the HS transferred checkpoints with\neach layer mapping strategy. Interestingly, we see a\nslight degradation in performance on downstream\ntasks compared to only HS transfer, with a signiﬁ-\n7Distilled models often perform poorly on CoLA: We hy-\npothesize this is because CoLA is the only syntactic task in\nthe benchmark as opposed to the other semantic tasks (Xu\net al., 2022). We include the results of CoLA in Appendix D.\n24\nDistillation\nMethod\nLayer Avg. GLUE (Monolingual)Avg. GLUE (Multilingual)Avg. XNLI (Multilingual)\nMapping 6L- 6L 4L 3L 6L- 6L 4L 3L 6L- 6L 4L 3LStrategy DistilBERT DistilBERT DistilBERT\nLTth 84.1 79.4 80.2 78.9 80.8 77.1 78.0 74.7 56.2 55.1 51.6 50.6\nLast 83.2 80.4 79.3 77.7 81.7 77.0 78.3 72.6 63.1 61.0 60.3 54.4\nHS Transfer Uniform 82.9 80.6 79.6 76.6 81.6 78.2 78.3 73.5 59.9 59.9 59.7 59.9\nUniform-Cons.83.9 80.6 80.6 77.7 82.4 78.8 78.0 75.9 65.5 62.2 60.4 58.6\nUniform+Last 84.1 80.4 80.4 77.7 83.1 78.7 79.2 75.0 67.0 62.7 62.5 57.9\nLTth 84.1 78.1 79.4 76.6 78.5 75.1 75.2 67.9 50.5 48.2 51.6 43.8\nOD Transfer Last 83.1 80.4 79.3 76.4 80.7 76.9 76.1 69.8 62.6 57.0 54.1 42.7\n(init. from Uniform 83.4 79.8 79.8 77.1 79.9 78.0 77.9 65.4 60.4 54.1 52.0 42.8\nHS Transfer)Uniform-Cons.83.7 80.3 79.5 76.7 81.7 78.7 76.4 70.1 63.1 61.0 56.5 48.2\nUniform+Last 84.1 80.5 79.9 77.1 82.1 78.4 76.4 72.3 66.0 60.9 60.0 48.6\nMiniLMv2\nLTth 84.2 81.9 79.9 77.6 82.3 80.1 79.3 74.4 67.0 66.7 63.1 59.3\n(LT−1)th 84.2 82.5 80.0 78.2 83.1 81.0 80.2 75.8 69.1 67.5 65.6 62.0\n(LT−2)th 84.4 82.2 80.7 78.3 82.9 80.5 78.3 73.4 67.5 66.9 63.5 61.5\nDirectMiniLM\nLTth 84.0 81.3 79.7 78.2 83.2 80.8 79.0 75.1 66.3 66.1 64.7 60.7\n(LT−1)th 84.4 81.7 79.6 78.0 81.9 81.1 80.3 73.8 66.9 65.9 64.8 61.0\n(LT−2)th 84.3 81.7 80.4 78.3 83.4 80.9 79.7 75.6 66.3 64.8 65.4 60.5\nTeacher 85.5 84.8 70.9\nTable 3: Performance of the representative distillation methods evaluated on avg. GLUE and XNLI. Results based\non the best layer mapping strategy for each method is underlined, and the best overall result is shown in bold.\ncant loss observed for smaller students. This indi-\ncates that learning effective representations from\nthe output distribution signals is difﬁcult, especially\nfor students with lower capacity. Moreover, given\nhow computationally expensive OD transfer can\nbe, HS transfer is a cheaper and more effective\nalternative for knowledge transfer.\nMHA Transfer For both MiniLMv2 and Direct-\nMiniLM, we found distilling the upper-middle\nteacher layer, i.e. (LT−1)th or (LT−2)th strategy,\nled to the best performance, in line with the orig-\ninal ﬁndings of Wang et al. (2021). Importantly,\nwe found that both MHA transfer methods gener-\nally outperform HS transfer, which points to the\nbeneﬁt of transferring the Q/K/V knowledge over\nthe hidden state knowledge. This is consistent with\nthe latest comparative study by Wang et al. (2023),\nalthough they only evaluate on the 6L-DistilBERT\narchitecture in the monolingual setting.\nWe also note that MiniLMv2 and DirectMiniLM\nperform equivalently, with the notable exception\non XNLI. We attribute this to two factors:\n1. MiniLMv2 transfers relational representations\nconditioned on the whole input, while Direct-\nMiniLM transfers absolute position-wise rep-\nresentations. The former may be more seman-\ntically informative, as the contextual represen-\ntations often exhibit rich relational structures\n(Park et al., 2021; Liu et al., 2022a).\n2. DirectMiniLM requires learning the linear trans-\nformation weight Wα,a, while MiniLMv2 does\nnot incur any additional parameters.\nFrom these observations, we generally expect\nMiniLMv2 to be the best distillation method and\nhave adopted it in our latency-critical applications.8\nHowever, DirectMiniLM performs comparably and\nprovides meaningful insights on the beneﬁt of each\noptimization technique, which can be useful for\ndebugging and analyzing MiniLMv2. Therefore,\nwe recommend its comparison for both reseachers\nand practitioners in future studies.\n6 Conclusion\nThis study critically analyzes the representative\nmethods for task-agnostic distillation of language\nmodels. Speciﬁcally, we compare Output Distri-\nbution (OD), Hidden State (HS), and Multi-Head\nAttention (MHA) transfer for different student ar-\nchitectures, language settings, and layer mapping\nstrategies. Through our extensive experiments, we\nshow that MHA transfer based on MiniLMv2 is the\nbest option across many settings, followed by HS\ntransfer with sophisticated 1-to-N mapping strate-\ngies. Meanwhile, we did not ﬁnd OD transfer to\nbe an effective alternative. Finally, we propose Di-\nrectMiniLM to demistify the precise advantage of\nthe indirect (i.e. relation matrix based) optimiza-\ntion technique proposed in MiniLMv2. Overall,\nwe hope this study will be a useful guide for both\nresearchers and practitioners working in this area.\n8Speciﬁcally, the 4L monolingual and multilingual stu-\ndents with 7x speedup on CPU have been deployed for various\nNLP applications, such as entity extraction, document classiﬁ-\ncation and relation detection, while maintaining 93% of the\nteacher’s performance on average (Trivedi et al., 2023).\n25\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nHaoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jin Jin,\nXin Jiang, Qun Liu, Michael Lyu, and Irwin King.\n2021. BinaryBERT: Pushing the limit of BERT\nquantization. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 4334–4348, Online. Association for\nComputational Linguistics.\nMatan Ben Noach and Yoav Goldberg. 2020. Com-\npressing pre-trained language models by matrix de-\ncomposition. In Proceedings of the 1st Confer-\nence of the Asia-Paciﬁc Chapter of the Association\nfor Computational Linguistics and the 10th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing, pages 884–889, Suzhou, China. Associa-\ntion for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nAdvances in Neural Information Processing Systems,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nPatrick Chen, Hsiang-Fu Yu, Inderjit Dhillon, and Cho-\nJui Hsieh. 2021. Drone: Data-aware low-rank com-\npression for large nlp models. InAdvances in Neural\nInformation Processing Systems , volume 34, pages\n29321–29334. Curran Associates, Inc.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale.\nAlexis Conneau, Guillaume Lample, Ruty Rinott, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAngela Fan, Edouard Grave, and Armand Joulin. 2020.\nReducing transformer depth on demand with struc-\ntured dropout. In International Conference on\nLearning Representations.\nMd Akmal Haidar, Nithin Anchuri, Mehdi Reza-\ngholizadeh, Abbas Ghaddar, Philippe Langlais, and\nPascal Poupart. 2022. RAIL-KD: RAndom interme-\ndiate layer mapping for knowledge distillation. In\nFindings of the Association for Computational Lin-\nguistics: NAACL 2022 , pages 1389–1400, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nDan Hendrycks and Kevin Gimpel. 2016. Gaus-\nsian error linear units (gelus). arXiv preprint\narXiv:1606.08415.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nAref Jafari, Mehdi Rezagholizadeh, Pranav Sharma,\nand Ali Ghodsi. 2021. Annealing knowledge distil-\nlation. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, pages 2493–2504,\nOnline. Association for Computational Linguistics.\nGanesh Jawahar, Benoît Sagot, and Djamé Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657, Florence, Italy. Associa-\ntion for Computational Linguistics.\nXiaoqi Jiao, Huating Chang, Yichun Yin, Lifeng Shang,\nXin Jiang, Xiao Chen, Linlin Li, Fang Wang, and\nQun Liu. 2021. Improving task-agnostic bert distil-\nlation with layer mapping search. Neurocomputing,\n461:194–203.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang,\nXiao Chen, Linlin Li, Fang Wang, and Qun Liu.\n2020. TinyBERT: Distilling BERT for natural lan-\nguage understanding. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4163–4174, Online. Association for Computational\nLinguistics.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei.\n2020. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361.\nSehoon Kim, Amir Gholami, Zhewei Yao, Michael W\nMahoney, and Kurt Keutzer. 2021. I-bert: Integer-\nonly bert quantization. In International conference\non machine learning, pages 5506–5518. PMLR.\n26\nJongwoo Ko, Seungjoon Park, Minchan Jeong, Sukjin\nHong, Euijai Ahn, Du-Seong Chang, and Se-Young\nYun. 2023. Revisiting intermediate layer distillation\nfor compressing language models: An overﬁtting\nperspective. In Findings of the Association for Com-\nputational Linguistics: EACL 2023, pages 158–175,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nFrançois Lagunas, Ella Charlaix, Victor Sanh, and\nAlexander Rush. 2021. Block pruning for faster\ntransformers. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language\nProcessing, pages 10619–10629, Online and Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nJianquan Li, Xiaokang Liu, Honghong Zhao, Ruifeng\nXu, Min Yang, and Yaohong Jin. 2020. BERT-\nEMD: Many-to-many layer mapping for BERT com-\npression with earth mover’s distance. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n3009–3018, Online. Association for Computational\nLinguistics.\nKevin J Liang, Weituo Hao, Dinghan Shen, Yufan\nZhou, Weizhu Chen, Changyou Chen, and Lawrence\nCarin. 2021. Mix{kd}: Towards efﬁcient distillation\nof large-scale language models. In International\nConference on Learning Representations.\nKaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su,\nXu Sun, and Bin He. 2021. A global past-future\nearly exit method for accelerating inference of pre-\ntrained language models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2013–2023, Online.\nAssociation for Computational Linguistics.\nChang Liu, Chongyang Tao, Jiazhan Feng, and\nDongyan Zhao. 2022a. Multi-granularity structural\nknowledge distillation for language model compres-\nsion. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1001–1011, Dublin, Ire-\nland. Association for Computational Linguistics.\nChang Liu, Chongyang Tao, Jianxin Liang, Tao Shen,\nJiazhan Feng, Quzhe Huang, and Dongyan Zhao.\n2022b. Rethinking task-speciﬁc knowledge distilla-\ntion: Contextualized corpus as better textbook. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing , pages\n10652–10658, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nWeijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao,\nHaotang Deng, and Qi Ju. 2020. FastBERT: a self-\ndistilling BERT with adaptive inference time. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 6035–\n6044, Online. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\nXinge Ma, Jin Wang, Liang-Chih Yu, and Xuejie\nZhang. 2022. Knowledge distillation with reptile\nmeta-learning for pretrained language model com-\npression. In Proceedings of the 29th International\nConference on Computational Linguistics , pages\n4907–4917, Gyeongju, Republic of Korea. Interna-\ntional Committee on Computational Linguistics.\nYihuan Mao, Yujing Wang, Chufan Wu, Chen Zhang,\nYang Wang, Quanlu Zhang, Yaming Yang, Yunhai\nTong, and Jing Bai. 2020. LadaBERT: Lightweight\nadaptation of BERT through hybrid model compres-\nsion. In Proceedings of the 28th International Con-\nference on Computational Linguistics , pages 3225–\n3234, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, Ang\nLi, Nir Levine, Akihiro Matsukawa, and Hassan\nGhasemzadeh. 2020. Improved knowledge distil-\nlation via teacher assistant. In Proceedings of\nthe AAAI conference on artiﬁcial intelligence , vol-\nume 34, pages 5191–5198.\nSubhabrata Mukherjee, Ahmed Hassan Awadallah, and\nJianfeng Gao. 2021. Xtremedistiltransformers: Task\ntransfer for task-agnostic distillation. arXiv preprint\narXiv:2106.04563.\nGeondo Park, Gyeongman Kim, and Eunho Yang.\n2021. Distilling linguistic context for language\nmodel compression. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 364–378, Online and Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nPeyman Passban, Yimeng Wu, Mehdi Rezagholizadeh,\nand Qun Liu. 2021. Alp-kd: Attention-based layer\nprojection for knowledge distillation. In Proceed-\nings of the AAAI Conference on artiﬁcial intelli-\ngence, volume 35, pages 13657–13665.\n27\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI blog, 1(8):9.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nSheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma, Zhewei\nYao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. 2020. Q-bert: Hessian based ultra low\nprecision quantization of bert. In Proceedings of\nthe AAAI Conference on Artiﬁcial Intelligence , vol-\nume 34, pages 8815–8821.\nWonchul Son, Jaemin Na, Junyong Choi, and Wonjun\nHwang. 2021. Densely guided knowledge distilla-\ntion using multiple teacher assistants. In Proceed-\nings of the IEEE/CVF International Conference on\nComputer Vision, pages 9395–9404.\nSiqi Sun, Yu Cheng, Zhe Gan, and Jingjing Liu. 2019.\nPatient knowledge distillation for BERT model com-\npression. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4323–4332, Hong Kong, China. Association for\nComputational Linguistics.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. MobileBERT:\na compact task-agnostic BERT for resource-limited\ndevices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2158–2170, Online. Association for Computa-\ntional Linguistics.\nMarzieh Tahaei, Ella Charlaix, Vahid Nia, Ali Gh-\nodsi, and Mehdi Rezagholizadeh. 2022. Kronecker-\nBERT: Signiﬁcant compression of pre-trained lan-\nguage models through kronecker decomposition and\nknowledge distillation. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2116–2127, Seattle,\nUnited States. Association for Computational Lin-\nguistics.\nWeiting Tan, Kevin Heffernan, Holger Schwenk, and\nPhilipp Koehn. 2023. Multilingual representation\ndistillation with contrastive learning. In Proceed-\nings of the 17th Conference of the European Chap-\nter of the Association for Computational Linguistics,\npages 1477–1490, Dubrovnik, Croatia. Association\nfor Computational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and efﬁ-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAashka Trivedi, Takuma Udagawa, Michele Merler,\nRameswar Panda, Yousef El-Kurdi, and Bishwaran-\njan Bhattacharjee. 2023. Neural architecture search\nfor effective teacher-student knowledge transfer in\nlanguage models. arXiv preprint arXiv:2303.09639.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\narXiv preprint arXiv:1908.08962.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Inter-\nnational Conference on Learning Representations.\nJue Wang, Ke Chen, Gang Chen, Lidan Shou, and Ju-\nlian McAuley. 2022. SkipBERT: Efﬁcient inference\nwith shallow layer skipping. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n7287–7301, Dublin, Ireland. Association for Com-\nputational Linguistics.\nWenhui Wang, Hangbo Bao, Shaohan Huang, Li Dong,\nand Furu Wei. 2021. MiniLMv2: Multi-head self-\nattention relation distillation for compressing pre-\ntrained transformers. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021 ,\npages 2140–2151, Online. Association for Computa-\ntional Linguistics.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. In Advances in Neural\nInformation Processing Systems , volume 33, pages\n5776–5788. Curran Associates, Inc.\nXinpeng Wang, Leonie Weissweiler, Hinrich Schütze,\nand Barbara Plank. 2023. How to distill your BERT:\nAn empirical study on the impact of weight initiali-\nsation and distillation objectives. In Proceedings of\nthe 61st Annual Meeting of the Association for Com-\nputational Linguistics (Volume 2: Short Papers) ,\npages 1843–1852, Toronto, Canada. Association for\nComputational Linguistics.\nYimeng Wu, Peyman Passban, Mehdi Rezagholizadeh,\nand Qun Liu. 2020. Why skip if you can combine: A\nsimple knowledge distillation technique for interme-\ndiate layers. In Proceedings of the 2020 Conference\n28\non Empirical Methods in Natural Language Process-\ning (EMNLP) , pages 1016–1021, Online. Associa-\ntion for Computational Linguistics.\nYimeng Wu, Mehdi Rezagholizadeh, Abbas Ghaddar,\nMd Akmal Haidar, and Ali Ghodsi. 2021. Universal-\nKD: Attention-based output-grounded intermediate\nlayer knowledge distillation. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7649–7661, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nMengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022.\nStructured pruning learns compact and accurate\nmodels. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 1513–1528, Dublin,\nIreland. Association for Computational Linguistics.\nJi Xin, Raphael Tang, Yaoliang Yu, and Jimmy Lin.\n2021. BERxiT: Early exiting for BERT with better\nﬁne-tuning and extension to regression. In Proceed-\nings of the 16th Conference of the European Chap-\nter of the Association for Computational Linguistics:\nMain Volume, pages 91–104, Online. Association for\nComputational Linguistics.\nCanwen Xu and Julian McAuley. 2023. A survey on\nmodel compression and acceleration for pretrained\nlanguage models. In Proceedings of the AAAI Con-\nference on Artiﬁcial Intelligence , volume 37, pages\n10566–10575.\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Furu Wei,\nand Ming Zhou. 2020. BERT-of-theseus: Com-\npressing BERT by progressive module replacing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7859–7869, Online. Association for Computa-\ntional Linguistics.\nDongkuan (DK) Xu, Subhabrata Mukherjee, Xiaodong\nLiu, Debadeepta Dey, Wenhui Wang, Xiang Zhang,\nAhmed Awadallah, and Jianfeng Gao. 2022. Few-\nshot task-agnostic neural architecture search for dis-\ntilling large language models. In Advances in\nNeural Information Processing Systems, volume 35,\npages 28644–28656. Curran Associates, Inc.\nOﬁr Zafrir, Guy Boudoukh, Peter Izsak, and Moshe\nWasserblat. 2019. Q8bert: Quantized 8bit bert. In\n2019 Fifth Workshop on Energy Efﬁcient Machine\nLearning and Cognitive Computing-NeurIPS Edi-\ntion (EMC2-NIPS), pages 36–39. IEEE.\nMinjia Zhang, Niranjan Uma Naresh, and Yuxiong\nHe. 2022. Adversarial data augmentation for task-\nspeciﬁc knowledge distillation of pre-trained trans-\nformers. Proceedings of the AAAI Conference on\nArtiﬁcial Intelligence, 36(10):11685–11693.\nWangchunshu Zhou, Canwen Xu, and Julian McAuley.\n2022. BERT learns to teach: Knowledge distil-\nlation with meta learning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n7037–7049, Dublin, Ireland. Association for Com-\nputational Linguistics.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In 2015 IEEE International Con-\nference on Computer Vision (ICCV), pages 19–27.\nA Related Work\nMobileBERT (Sun et al., 2020) is an effective tech-\nnique to compress BERT into a specially designed\nstudent with a bottleneck architecture. In BERT-\nof-Theseus (Xu et al., 2020), the modules of the\nteacher are progressively replaced with smaller\nones to improve efﬁciency. However, these ap-\nproaches constrain the architecture of the students.\nIn contrast, we focus on the architecture-agnostic\ndistillation methods for better ﬂexibility.\nImprovements on distillation objectives are also\nmade, e.g. transferring the relational, structural\nor holistic representations of the language models\nmay provide more useful signals for students (Park\net al., 2021; Liu et al., 2022a; Tan et al., 2023).\nWhen the transfer set is limited, various methods\nof data augmentation (Liang et al., 2021; Zhang\net al., 2022; Liu et al., 2022b) can be applied suc-\ncessfully. To alleviate the capacity gap between\nthe teacher and student, previous works proposed\nscheduled annealing in OD transfer (Jafari et al.,\n2021), multi-stage distillation with intermediate-\nsized teacher assistants (Mirzadeh et al., 2020; Son\net al., 2021), and meta-learning to optimize the\nteacher for student distillation (Zhou et al., 2022;\nMa et al., 2022). We leave the exploration of such\nadvanced techniques as future work.\nLayer mapping strategies for HS transfer have\nalso been studied extensively. Jiao et al. (2021)\nproposed an evolutionary search process to obtain\nthe optimal layer mapping for speciﬁc downstream\ntasks. Li et al. (2020) applied Earth Mover’s Dis-\ntance to prioritize mappings with smaller cost (i.e.\ndistillation loss). The attention mechanism can also\nbe applied to map student layers to similar teacher\nlayers, where the similarity is computed based on\nthe cosine similarity (Passban et al., 2021) or the\npredictions of internal classiﬁers (Wu et al., 2021).\nFinally, random mapping has been shown to work\nsurprisingly well, potentially working as a regu-\nlarizer to prevent overﬁtting (Haidar et al., 2022).\n29\nIn this study, we focus instead on the carefully de-\nsigned and easily applicable heuristic strategies.\nFinally, there are different approaches to reduc-\ning the inference costs of large language models,\nsuch as quantization (Zafrir et al., 2019; Shen et al.,\n2020; Kim et al., 2021; Bai et al., 2021), pruning\n(Fan et al., 2020; Lagunas et al., 2021; Xia et al.,\n2022), early exit mechanisms (Liu et al., 2020; Xin\net al., 2021; Liao et al., 2021; Wang et al., 2022),\nand matrix decomposition (Ben Noach and Gold-\nberg, 2020; Mao et al., 2020; Chen et al., 2021;\nTahaei et al., 2022). Many of these approaches are\ncomplementary to our distillation methods and can\nbe combined for further efﬁciency.\nB Distillation Setup\nWe train our monolingual students on the entire\nWikipedia and BookCorpus using the AdamW Op-\ntimizer (Loshchilov and Hutter, 2019) with β1 =\n0.9,β2 = 0.98. For HS and MHA transfer, stu-\ndents are trained for 7 epoch with a peak learning\nrate (LR) of 5e−4. For OD transfer, we train for\n3 epochs with a peak LR of 3e−4 after HS trans-\nfer. We use a linear LR warmup over the ﬁrst 5%\nof the training steps and then a linear decay. We\nuse a batch size of 32 with the maximum sequence\nlength set to 256 and train on 30 V100 GPUs.\nFor multilingual distillation, we use a small sub-\nset of CC-100 containing 7M sentences, which we\nfound to be sufﬁcient for developing competitive\nstudents. We generally use the same setup as mono-\nlingual distillation, except we use the peak LR of\n8e−4 for MHA transfer. Multilingual students are\ntrained on 2 A100-80GB GPUs.\nFinally, the method-speciﬁc hyperparameters\n(§3) are as follows. For OD transfer, we set the\noutput temperature T to the default value of 1. For\nMiniLMv2, we use Ar > Ah to transfer more\nﬁne-grained knowledge in the Q/K/V mappings:\nspeciﬁcally, we set Ar = 48, which is also used in\nWang et al. (2021). For DirectMiniLM, we found\nusing Ar = Ah without the orthogonal constraints\non Wα,a led to the best performance and used this\nsetting throughout our experiments.\nC Finding Smaller Student Models\nOur smallest students, a 4 layer and a 3 layer model,\nwere obtained as recommendations from a Neural\nArchitecture Search process to ﬁnd good student\narchitectures for task-agnostic distillation from an\nXLM-RoBERTa teacher, conditioned to minimize\nthe latency of inference on a CPU. Speciﬁcally, we\nfollow the KD-NAS method of Trivedi et al. (2023)\nand modify the reward to reduce the distillation\nloss LHS deﬁned in Eq. (6), along with the CPU\nlatency of the student (lat(S)) normalized by the\nteacher’s latency (lat(T)):\nreward(S) = (1−LHS) ∗\n( lat(S)\n0.6 ∗lat(T)\n)−0.06\n(12)\nPlease refer to their original paper for more details.\nD Evaluation Results for Best Models\nWe include detailed results of each distillation\nmethod for the best conﬁguration (i.e. layer map-\nping strategy). Speciﬁcally, we show the results of\neach GLUE task for monolingual and multilingual\ndistillation in Table 5 and 6. We show language-\nwise performance on XNLI in Table 7. All down-\nstream tasks are evaluated on 3 random seeds.\nFor the sake of efﬁcient evaluation, we did not\nconduct expensive grid search for ﬁnetuning hyper-\nparameters. After some manual tuning, we used\nthe same LR of 2e−5 and batch size of 32 for ﬁne-\ntuning all models on all tasks. We used 3 epochs of\nﬁnetuning for GLUE tasks (except CoLA, where\nwe used 6 and 10 epochs for monolingual and mul-\ntilingual models) and 5 epochs for XNLI.\nE Architecture Constrained Distillation:\nDistilBERT\nDistilBERT (Sanh et al., 2019) is one of the earli-\nest and most widely used baseline. This method\ncomprises (1) layer initialization from the teacher\nlayers, (2) HS transfer based on cosine similarity\nloss, and (3) OD transfer. The ﬁrst two techniques\nrestrict the architecture of each student layer to be\nidentical to the teacher model, which limits our\nanalysis to the 6L-DistilBERT student architecture.\n6L-DsitilBERT Teacher\nAvg. GLUE (Monolingual) 82.9 (0.5) 85.5 (0.6)\nAvg. GLUE (Multilingual) 79.7 (0.5) 84.8 (0.3)\nAvg. XNLI (Multilingual) 61.8 (0.5) 70.9 (0.8)\nTable 4: DistilBERT Performance. Average GLUE\nscores reported for all tasks w/o CoLA. Average XNLI\nscores reported for all languages. Average taken over 3\nrandom seeds with standard deviation in parenthesis.\nAs shown in the results of Table 4, the perfor-\nmance of DistilBERT is generally not competitive\nwith our distillation methods from Table 3, espe-\ncially in the multilingual setting.\n30\nModel Distillation Best GLUE Performance Avg. Avg.\nMethod Strategy MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE(-CoLA)\n6L-DistilBERT\nHS TransferUniform+Last82.6 86.2 88.7 90.8 45.9 85.9 89.7 65.1 79.4 (0.5) 84.1 (0.4)\nOD TransferUniform+Last82.7 86.5 88.3 91.3 50.8 85.5 89.7 64.4 79.9 (0.3) 84.1 (0.2)\nMiniLMv2 (LT−2)th 83.0 86.6 90.1 91.6 53.1 86.7 89.0 64.2 80.5 (0.4) 84.4 (0.3)\nDirectMiniLM(LT−1)th 82.9 86.6 90.0 91.4 52.7 86.4 89.0 64.9 80.5 (0.5) 84.4 (0.4)\n6L\nHS TransferUniform-Cons.78.3 85.0 85.9 90.9 31.2 83.2 84.4 56.3 74.4 (0.4) 80.6 (0.3)\nOD TransferUniform+Last79.1 84.6 86.3 89.7 38.6 82.3 83.7 57.9 75.3 (0.6) 80.5 (0.3)\nMiniLMv2 (LT−1)th 80.8 84.9 88.0 90.3 36.2 84.5 86.2 62.5 76.7 (0.1) 82.5 (0.1)\nDirectMiniLM(LT−1)th 80.0 85.1 87.2 90.9 36.1 83.3 85.9 59.7 76.0 (0.2) 81.7 (0.2)\n4L\nHS TransferUniform-Cons.77.3 84.9 85.7 90.0 26.9 83.4 83.0 60.1 73.9 (0.4) 80.6 (0.3)\nOD TransferUniform+Last78.2 84.6 85.1 90.1 32.2 83.3 83.2 55.1 74.0 (0.2) 79.9 (0.4)\nMiniLMv2 (LT−2)th 78.8 83.8 86.0 90.8 30.9 83.0 84.3 58.2 74.5 (0.2) 80.7 (0.3)\nDirectMiniLM(LT−2)th 79.0 84.2 85.7 90.0 29.7 82.5 84.9 56.6 74.1 (0.4) 80.4 (0.4)\n3L\nHS Transfer LTth 74.3 82.8 84.0 89.4 20.0 80.8 83.4 57.5 71.5 (0.1) 78.9 (0.3)\nOD TransferUniform+Last73.8 81.9 83.4 86.6 15.1 78.8 82.7 52.8 69.4 (0.3) 77.1 (0.4)\nMiniLMv2 (LT−2)th 75.1 81.9 84.8 87.3 13.3 81.6 82.0 55.1 70.1 (0.4) 78.3 (0.2)\nDirectMiniLM(LT−2)th 75.7 82.2 84.0 88.5 16.8 81.0 83.3 53.5 70.6 (0.2) 78.3 (0.3)\nTeacher 84.4 88.0 91.5 92.9 57.4 88.0 89.0 64.8 82.0 (0.6) 85.5 (0.6)\nTable 5: Monolingual Student GLUE Performance for all tasks. Each row shows performance based on the best\nlayer mapping strategy. Each score reported as an average over 3 random seeds (standard deviation in parenthesis).\nModel Distillation Best GLUE Performance Avg. Avg.\nMethod Strategy MNLI QQP QNLI SST-2 CoLA STS-B MRPC RTE(-CoLA)\n6L-DistilBERT\nHS TransferUniform+Last80.8 86.8 87.9 90.2 32.3 84.7 88.5 62.6 76.7 (0.6) 83.1 (0.3)\nOD TransferUniform+Last80.1 86.4 86.2 89.8 33.1 84.1 87.5 60.5 76.0 (1.0) 82.1 (0.5)\nMiniLMv2 (LT−1)th 81.3 85.8 88.8 89.6 40.2 85.9 89.3 61.0 77.7 (0.5)83.1 (0.3)\nDirectMiniLM(LT−2)th 81.0 86.4 89.2 89.8 37.8 85.9 90.1 61.7 77.7 (0.7) 83.4 (0.6)\n6L\nHS TransferUniform-Cons.75.0 82.8 83.0 86.7 16.9 80.8 84.6 58.5 71.1 (0.6) 78.8 (0.4)\nOD TransferUniform-Cons.76.2 83.7 83.6 87.5 16.9 78.1 85.0 55.9 71.1 (0.6) 78.7 (0.5)\nMiniLMv2 (LT−1)th 78.3 83.7 86.9 89.1 29.2 83.6 85.1 60.3 74.5 (0.5)81.0 (0.4)\nDirectMiniLM(LT−1)th 78.3 84.3 86.1 89.4 25.5 84.5 86.9 58.0 74.1 (0.6)81.1 (0.5)\n4L\nHS TransferUniform+Last75.6 83.7 83.8 87.8 18.3 81.2 83.3 59.0 71.6 (0.7) 79.2 (0.5)\nOD Transfer Uniform 73.4 83.8 81.2 85.2 17.0 80.0 82.8 58.6 70.3 (0.7) 77.9 (0.7)\nMiniLMv2 (LT−1)th 76.8 83.4 85.2 87.6 17.1 83.9 86.0 58.1 72.3 (0.7)80.2 (0.5)\nDirectMiniLM(LT−1)th 77.0 83.6 85.2 88.5 19.2 83.5 85.2 59.1 72.7 (0.6)80.3 (0.4)\n3L\nHS TransferUniform-Cons.71.0 80.7 82.1 84.6 11.0 75.8 82.2 54.9 67.8 (0.4)75.9 (0.4)\nOD TransferUniform+Last68.1 79.4 79.7 81.9 2.6 61.5 81.2 54.6 63.6 (0.5) 72.3 (0.6)\nMiniLMv2 (LT−1)th 72.7 80.6 83.2 84.6 9.7 70.6 81.7 57.4 67.6 (0.6) 75.8 (0.5)\nDirectMiniLM(LT−2)th 72.2 81.2 83.4 84.8 15.9 67.9 82.0 58.0 68.2 (1.1)75.6 (1.1)\nTeacher 84.1 87.9 90.2 91.9 51.7 86.6 91.4 61.4 80.6 (0.3) 84.8 (0.3)\nTable 6: Multilingual Student GLUE Performance for all tasks. Each row shows performance based on the best\nlayer mapping strategy. Each score reported as an average over 3 random seeds (standard deviation in parenthesis).\nModel Distillation Best XNLI Performance Avg.Method Strategyar bg de el en es fr hi ru sw th tr ur vi zh\n6L-DistilBERT\nHS TransferUniform+Last64.7 69.7 69.6 69.2 80.7 72.0 70.2 64.6 67.7 51.2 65.3 62.5 58.9 70.4 68.667.0 (0.4)OD TransferUniform+Last63.7 69.1 69.4 67.0 78.6 70.7 68.9 60.0 69.0 51.2 65.4 61.9 57.9 68.5 68.866.0 (0.6)MiniLMv2(LT−1)th 65.5 71.6 72.1 71.5 81.4 75.0 73.5 65.3 70.6 58.1 65.1 67.1 60.9 69.7 69.369.1 (0.5)DirectMiniLM(LT−1)th 63.8 69.4 69.3 68.5 79.2 73.2 71.2 64.1 67.2 55.1 63.9 65.6 59.7 66.6 67.066.9 (0.4)\n6L\nHS TransferUniform+Last59.7 67.2 63.4 65.6 75.9 68.7 66.8 58.3 62.4 48.9 62.7 59.1 53.4 63.2 65.162.7 (0.4)OD TransferUniform+Last55.7 62.6 63.7 59.2 76.5 66.9 63.7 54.1 62.0 45.7 57.9 56.3 51.0 62.8 62.261.0 (0.5)MiniLMv2(LT−1)th 65.0 69.7 70.4 68.8 80.3 73.1 71.5 62.9 69.3 53.8 65.0 65.7 59.6 69.2 68.067.5 (0.5)\nDirectMiniLMLTth 63.2 68.8 70.1 68.1 78.4 70.5 70.0 62.2 66.6 52.4 64.6 64.0 59.1 66.2 66.966.1 (0.5)\n4L\nHS TransferUniform+Last56.9 64.5 66.2 66.3 77.3 68.2 63.9 57.9 63.9 49.2 61.8 59.2 54.0 64.2 64.262.5 (0.5)OD TransferUniform+Last55.7 62.6 63.7 59.2 76.5 66.9 63.7 54.1 62.0 45.7 57.9 56.3 51.0 62.8 62.260.0 (0.5)MiniLMv2(LT−1)th 62.9 67.5 67.8 68.2 77.8 70.7 68.2 62.4 67.0 51.0 63.6 64.7 57.7 67.2 67.465.6 (0.8)DirectMiniLM(LT−2)th 63.2 68.3 67.9 67.6 78.3 69.7 69.6 63.1 64.9 49.0 64.2 62.4 58.6 67.2 66.365.4 (0.7)\n3L\nHS TransferUniform58.3 63.4 60.5 60.6 74.1 65.6 61.6 56.6 61.4 46.7 57.3 55.9 51.8 61.1 63.159.9 (0.5)OD TransferUniform+Last45.6 52.3 48.7 47.8 69.9 55.0 49.4 42.9 47.3 40.9 46.3 44.4 41.6 49.7 47.848.6 (0.5)MiniLMv2(LT−1)th 60.0 64.9 63.6 64.3 74.1 66.7 64.2 58.2 61.8 49.4 59.7 60.7 55.3 64.2 62.462.0 (0.8)DirectMiniLM(LT−1)th 57.4 63.0 64.1 63.3 74.3 66.1 65.1 57.2 62.1 46.7 56.7 58.1 55.2 63.6 61.861.0 (0.4)\nTeacher 69.1 73.2 74.1 72.2 83.4 75.1 73.1 69 71.3 57.3 69.7 67.7 64.1 70.8 73.370.9 (0.8)\nTable 7: Multilingual Student XNLI Performance for 15 languages. Each row shows performance based on\nthe best layer mapping strategy. Each score reported as an average over 3 random seeds (standard deviation in\nparenthesis).\n31"
}