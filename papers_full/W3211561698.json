{
  "title": "On the Effects of Transformer Size on In- and Out-of-Domain Calibration",
  "url": "https://openalex.org/W3211561698",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5047523675",
      "name": "Soham Dan",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A5023802054",
      "name": "Dan Roth",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2964212410",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3104939451",
    "https://openalex.org/W2948210185",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2626967530",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963508788",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3016339201",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2975429091",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3118608099",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W3094508337",
    "https://openalex.org/W3034257552",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2978670439"
  ],
  "abstract": "Large, pre-trained transformer language models, which are pervasive in natural language processing tasks, are notoriously expensive to train. To reduce the cost of training such large models, prior work has developed smaller, more compact models which achieves a significant speedup in training time while maintaining competitive accuracy to the original model on downstream tasks. Though these smaller pre-trained models have been widely adopted by the community, it is not known how well are they calibrated compared to their larger counterparts. In this paper, focusing on a wide range of tasks, we thoroughly investigate the calibration properties of pre-trained transformers, as a function of their size. We demonstrate that when evaluated in-domain, smaller models are able to achieve competitive, and often better, calibration compared to larger models, while achieving significant speedup in training time. Post-hoc calibration techniques further reduce calibration error for all models in-domain. However, when evaluated out-of-domain, larger models tend to be better calibrated, and label-smoothing instead is an effective strategy to calibrate models in this setting.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2096–2101\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n2096\nOn the Effects of Transformer Size on In- and Out-of-Domain Calibration\nSoham Dan\nUniversity of Pennsylvania\nsohamdan@seas.upenn.edu\nDan Roth\nUniversity of Pennsylvania\ndanroth@seas.upenn.edu\nAbstract\nLarge, pre-trained transformer language mod-\nels, which are pervasive in natural language\nprocessing tasks, are notoriously expensive to\ntrain. To reduce the cost of training such large\nmodels, prior work has developed smaller,\nmore compact models which achieves a signif-\nicant speedup in training time while maintain-\ning competitive accuracy to the original model\non downstream tasks. Though these smaller\npre-trained models have been widely adopted\nby the community, it is not known how well\nare they calibrated compared to their larger\ncounterparts. In this paper, focusing on a wide\nrange of tasks, we thoroughly investigate the\ncalibration properties of pre-trained transform-\ners, as a function of their size. We demonstrate\nthat when evaluated in-domain, smaller mod-\nels are able to achieve competitive, and often\nbetter, calibration compared to larger models,\nwhile achieving signiﬁcant speedup in train-\ning time. Post-hoc calibration techniques fur-\nther reduce calibration error for all models\nin-domain. However, when evaluated out-of-\ndomain, larger models tend to be better cali-\nbrated, and label-smoothing instead is an effec-\ntive strategy to calibrate models in this setting.\n.\n1 Introduction\nLarge pre-trained transformer language models like\nBERT (Devlin et al., 2019; Liu et al., 2019) have\nrevolutionized natural language processing, achiev-\ning state-of-the-art results in several tasks. The pro-\ncess of applying these models on a downstream task\nconsists of two components: (1) Self-supervised\npre-training on a large amount of text corpora and\n(2) Supervised ﬁne-tuning on the downstream task.\nDue to the very large number of parameters of such\ntransformer based architectures, the high down-\nstream accuracies comes at a large computational\ncost (Sharir et al., 2020; Bender et al., 2021) during\nhttp://cogcomp.org/page/publication_view/953\nthe pre-training stage and also to a lesser extent,\nwhile ﬁne-tuning. To alleviate this computational\ncost, several models with fewer parameters have\nbeen proposed that signiﬁcantly speed-up both the\npre-training and the ﬁne-tuning stages (Turc et al.,\n2019; Lan et al., 2020; Sanh et al., 2019; Sun et al.,\n2020). For example, the smallest model in (Turc\net al., 2019) consists of only 4 million parameters\ncompared to BERT-base which has 110 million\nparameters; this leads to a 65x speedup for pre-\ntraining time. It has been widely observed (Turc\net al., 2019; Lan et al., 2020) that smaller models\nachieve comparable downstream task performance\nwith a very signiﬁcant speedup in training time.\nA second issue with pre-trained models with a\nmassive number of parameters, is their lack of cali-\nbration, which measures how well the model con-\nﬁdences (posterior probabilities) are aligned with\nthe empirical likelihoods. In other words, for a cal-\nibrated model the probability associated with the\npredicted class label should reﬂect its ground truth\ncorrectness likelihood. Importantly, in the seminal\nwork of (Guo et al., 2017), the authors demonstrate\nthat for deep neural architectures increasing model\nsize negatively affects its calibration, even though\nclassiﬁcation accuracy increases. In this paper, we\nextend this to investigate the dependence of cali-\nbration on model size for pre-trained transformer\nmodels. Since miscalibrated models can make very\nconﬁdent predictions even when they make errors,\nespecially on out-of-distribution data (Gupta et al.),\nit is crucial to carefully study model calibration.\nRecently, there has been some progress on study-\ning the calibration of deep neural networks and\nspeciﬁcally, pre-trained transformers (Guo et al.,\n2017; Desai and Durrett, 2020; Kong et al., 2020;\nJagannatha et al., 2020). However, a careful study\nof how the size of the pre-trained model inﬂuences\ncalibration is lacking. With the computational con-\nstraints of training large transformers like BERT\nand the increasingly wide adoption of smaller mod-\n2097\nels, it becomes essential to study the calibration\nof these variants. In this work, we make a thor-\nough empirical study of the calibration properties\nof smaller transformer architectures of the BERT\nfamily, for a wide set of classiﬁcation tasks. The\nset of models have rich variations over number of\nlayers, number of hidden neurons and embedding\nrepresentation. Additionally, we analyze the effects\nof techniques designed to help calibrate models:\nduring training (eg: label smoothing) and post-hoc\n(eg: temperature scaling), on the smaller models,\nfor both in- and out-of-domain datasets.\nWe establish the following results in this paper:\n1. When evaluated in-domain, smaller models are\nas well calibrated as BERT-base, both with and\nwithout temperature scaling.\n2. When evaluated out-of-domain, smaller models\nare worse calibrated than BERT-base. This persists,\nto a lesser extent, even after temperature scaling.\n3. Label Smoothing, on the other hand, is not ef-\nfective in-domain, but helps smaller models attain\nbetter calibration than BERT-base out-of-domain.\nIt also helps improve accuracy as compared to the\nnon-smoothed models, on out-of-domain data.\n2 Background\nIn this section, we describe how we measure cali-\nbration and two techniques that help calibrate mod-\nels: Temperature Scalingand Label Smoothing.\nCalibration Metric: Let us deﬁne the following\nnotation: Kis the number of classes,zi denotes the\nraw logits from the model for the ith example and\nσ(k) denotes the kth value of the softmax layer σ,\ncorresponding to the probability for the kth class\n(for k ∈[1,...,K ]). Then, the conﬁdence on the\nith example is pi = maxkσ(zi)(k).\nA model is well calibrated if the conﬁdence on\na prediction is aligned with the accuracy on that\nprediction, in expectation. The widely adopted\nExpected Calibration Error (ECE)metric (Guo\net al., 2017) measures exactly this: difference in\nexpectation between conﬁdence and accuracy. Em-\npirically this is approximated by dividing the data\ninto Mconﬁdence based bins, i.e.,Bm(where m∈\n{1,2,...,M }) contains all datapoints ifor which\npi lies in (m−1\nM , m\nM]. If acc(Bm) and conf(Bm)\ndenotes the average accuracy and prediction conﬁ-\ndence for the points in Bm, ECE is deﬁned as:\nECE =\nM∑\nm=1\n|Bm|\nn |acc(Bm) −conf(Bm)|,\nwhere, |Bm|denotes the number of datapoints\nin Bm and n is the total number of samples\n(∑M\nm=1 Bm). In our experiments we set M = 10.\nReliability diagramsare a popular graphical rep-\nresentation of calibration. It plots the bucket-\nwise accuracies acc(Bm) versus the conﬁdences\nconf(Bm). The identity line denotes perfect cali-\nbration. The greater the deviation from the identity\nline, higher is the mis-calibration of the model.\nPost-hoc calibration: The calibration properties\nof a model can be evaluated directly out-of-box\n(OOB) based on the softmax scores of the model’s\npredictions. Temperature scalingis designed to\nimprove the calibration of a model after training.\nIt rescales the logits zi by a factor of T, before\napplying softmax σ. On the ith example, the new\nconﬁdence prediction is qi = maxkσ(zi\nT )(k) Thus,\nas T →∞, qi → 1\nK, ∀i, which is the uniform\ndistribution with maximum uncertainty. As T →0,\nthe probability collapses to a point mass (qi = 1)\nand if T = 1, pi = qi. The optimal temperature T\nis tuned on the dev-set by a line search algorithm.\nLabel Smoothing(Szegedy et al., 2016) leads to a\nmodiﬁed ﬁne-tuning procedure to address overcon-\nﬁdent predictions. While Maximum Likelihood Es-\ntimation (MLE), sharpens the model’s posterior dis-\ntribution around the target labels, label smoothing\nintroduces uncertainty to smoothen the posterior\nover the labels. Label smoothing constructs a new\ntarget vector from the one-hot target vector, with a\nprobability of 1 −αon the target label and α\nK−1 on\nall the other labels. Then, in the standard manner,\nthe cross entropy loss is minimized between the\nmodel predictions and the modiﬁed target vectors.\nLabel smoothing has been shown to implicitly cal-\nibrate neural networks (Müller et al., 2019) and\n(Desai and Durrett, 2020) have shown it is effective\nfor calibrating models on out-of-distribution data.\n3 Experiments\n3.1 Models\nWe consider a family of smaller pre-trained\ntransformer models from (Turc et al., 2019) with\nthe number of layers (L) ranging from 2 to 12 and\nthe number of hidden neurons (H) ranging from\n128 to 768. This family of models allows us to\ncarefully study calibration as a function of L and\nH, since the other parameters like training data\nand architecture type are constant across them.\nWe focus on 5 models: Tiny (L=2, H=128), Mini\n(L=4, H=256), Small (L=4, H=512), Medium\n2098\nModel SNLI MNLI COLA\n(L/H) Acc.(↑) OOB (↓) TS(↓) Acc.(↑) OOB(↓) TS (↓) Acc.(↑) OOB(↓) TS (↓)\n2/128 82.05 2.61 1.14 69.72 3.61 1.80 69.39 2.25 0.95\n4/256 86.67 3.64 1.23 76.05 4.75 1.95 70.54 4.25 2.37\n4/512 87.24 3.63 0.80 78.01 4.28 0.95 74.38 7.42 2.06\n8/512 88.72 4.46 1.41 80.15 4.79 1.35 76.58 4.78 3.03\nAlbert 89.07 0.86 0.91 83.62 3.29 0.94 79.08 4.90 2.47\nBERTbase 89.29 2.70 1.30 84.02 4.72 0.82 80.80 4.31 2.08\nModel SST-2 QQP TwitterPPDB\n(L/H) Acc.(↑) OOB (↓) TS(↓) Acc.(↑) OOB(↓) TS (↓) Acc.(↑) OOB(↓) TS (↓)\n2/128 80.04 4.49 2.46 84.21 3.06 1.44 84.62 6.27 3.99\n4/256 85.55 7.07 1.67 88.28 2.79 1.47 88.99 5.06 2.29\n4/512 88.53 7.64 4.61 88.56 3.87 0.68 88.36 5.74 2.73\n8/512 89.22 7.83 4.14 89.51 3.08 1.14 87.85 6.66 3.14\nAlbert 91.97 4.73 1.49 89.03 1.03 0.70 90.21 3.17 2.14\nBERTbase 90.60 8.07 4.45 89.47 1.54 0.74 88.77 5.73 3.40\nTable 1: Variation of Acc. (Accuracy) and ECE (deﬁned in Sec. 2) as a function of model size (L/H denotes the\nnumber of layers/number of hidden neurons) across 6 different tasks. Acc. is in % (↑denotes higher is better ) and\nOOB, TS are in ECE (↓denotes lower is better). The results are average over 5 iterations with random initialization.\nThe best results in each column are bolded. BERTbase and Albert (uses parameter-sharing) have L=12 and H=768.\n(L=8, H=512), and Base (L=12, H=768). Note that\nthe ﬁrst 4 models have far fewer parameters than\nBERT; the Tiny model has only 4m parameters\ncompared to the 110m parameters in BERT-Base.\nTo investigate the effect of other types of parameter\nreduction techniques beyond reducing the number\nof neurons or layers, we also experiment with\nAlbert (Lan et al., 2020). Albert uses factorized\nembeddings and cross layer parameter sharing\nto reduce the number of parameters to only 12\nmillion. We use the 12 layer Albert Base model\nwhich is architecturally comparable to BERT Base.\nFor all models, we experiment with three settings:\nOut-of-box (OOB) Calibration: We directly use\nthe conﬁdences pi (on the ith example) from the\nmodel to compute ECE. No specialized techniques\nare used to explicitly calibrate the model.\nTemperature Scaling (TS) (Guo et al., 2017):\nWe use this post-hoc (does not require model-\nretraining) calibration technique that ﬁnds the\noptimal temperature T as that which achieves the\nlowest ECE on the dev-set, using line-search.\nLabel Smoothing (LS): We train a label-smoothed\nmodel with hyper-parameter α= 0.1. This model\ncan be used out-of-box or with temperature scaling.\n3.2 Tasks\nWe perform experiments on various NLP tasks:\nNatural Language Inference: The Stanford Nat-\nural Language Inference (SNLI) (Bowman et al.,\n2015) and the Multi-Genre Natural Language In-\nference (MNLI) (Williams et al., 2017) datasets are\nused. Each of them have three classes correspond-\ning to the relations between the hypothesis and the\npremise: entailment, contradiction and neutral.\nParaphrase Detection: The Quora Question Pairs\n(QQP) (Iyer et al., 2017) and the TwitterPPDB (Lan\net al., 2017) datasets are used, where the former\ncontains semantically equivalent questions from\nQuora and the latter contains semantically equiva-\nlent tweets from Twitter. Both datasets have two\nclasses corresponding to similar/ dis-similar pairs.\nGrammaticality Detection: The Corpus of Lin-\nguistic Acceptability (COLA) (Warstadt et al.,\n2018) is used. It contains two classes correspond-\ning to whether sentences are grammatical or not.\nSentiment Analysis: The Stanford Sentiment\nTreebank (SST-2) (Socher et al., 2013) is used in\nthe binary classiﬁcation setting, where movie re-\nviews are assigned positive or negative labels.\nFor details on the datasets, refer to Appendix A.\n3.3 In Domain Calibration\nFor each of the different datasets, we ﬁne-tune1 the\nvarious models on the train-set and evaluate their\ncalibration error on the test-set. Additionally, we\ncalibrate the model in-domain through temperature\nscaling, where the optimal T is tuned on the dev-\n1Refer to Appendix B for details on hyper-parameter\nchoices: ﬁne-tuning epochs, learning rate and batch size.\n2099\nFigure 1: Reliability Diagram for In-Domain MNLI\ntest-set, evaluated out-of-box. The closer the lines are\nto the identity line, better is the calibration. Note the\nﬁrst 3 bins with conﬁdences from [0.0-0.3] do not con-\ntain any points and thus, we start from the 4th bin.\nset. 2 Table 1 shows the accuracies and the ECE\nfor the various models on the different datasets.\nWe see that models with far fewer parameters than\nBERT-base, have competitive accuracy as well as\ncompetitive (and sometimes better) calibration as\ncompared to BERT-base. This holds even after\ntemperature scaling, which reduces the ECE for all\nthe models. Fig. 1 shows the reliability diagram\nfor MNLI, where we see that the different smaller\nmodels are as well calibrated as BERT-base.\n3.4 Out-of-domain calibration\nWe further investigate the effect of model size on\ncalibration for out-of-domain data. For Natural\nLanguage Inference, all models are ﬁne-tuned on\nthe SNLI train-set and evaluated on the MNLI test-\nset. For Paraphrase Detection, all models are ﬁne-\ntuned on the QQP train-set and evaluated on the\nTwitterPPDB test-set. We also investigate the ef-\nfect of (1) Temperature Scaling (where the optimal\ntemperature is chosen based on performance on the\ndev-set for the source domain: SNLI or QQP) and\n(2) Label Smoothing with α= 0.1, on calibration.\nIn the reliability diagram in Fig. 2 and in Table 2,\nwe see that smaller models suffer from higher cali-\nbration error (ECE) on out-of-domain data, when\nevaluated out-of-box (OOB) or with temperature\nscaling (TS). The gap in ECE between smaller mod-\nels and BERT-base is more severe for the SNLI\nto MNLI transfer. However, Label Smoothing is\nvery effective in the out-of-domain setting. It sig-\nniﬁcantly reduces calibration error of all models\n2We also try label-smoothing, but it gives worse results\nthan temperature scaling for in-domain data, across all models.\nModel SNLI→MNLI\n(L/H) Acc. OOB TS Acc. LS\n2/128 47.73 19.64 18.34 56.57 3.65\n4/256 56.57 15.61 12.92 61.83 6.17\n4/512 57.61 14.55 11.16 63.91 6.82\n8/512 63.13 15.43 9.38 66.76 6.91\nAlbert 67.09 8.36 8.13 68.59 4.18\nBERTbase 69.88 7.25 4.06 71.35 4.98\nModel QQP→TwitterPPDB\n(L/H) Acc. OOB TS Acc. LS\n2/128 85.95 8.89 7.70 85.57 5.07\n4/256 86.34 10.03 8.07 88.08 5.28\n4/512 86.94 9.013 7.50 88.32 6.32\n8/512 86.58 8.84 7.62 89.24 5.37\nAlbert 86.86 8.05 7.69 87.97 6.78\nBERTbase 87.35 7.59 7.09 90.22 7.06\nTable 2: Variation of accuracy and ECE as a function\nof model size for the domain shift from SNLI to MNLI\n(above) and from QQP to TwitterPPDB (below). Acc.\n(Accuracy) is in % (higher is better) and OOB,TS,LS\nare in ECE (lower is better).\nFigure 2: Reliability Diagram for Out-of-Domain\nMNLI test-set, evaluated out-of-box.\nin general, but helps more for smaller models, as\nseen in both the transfer tasks. Additionally, label\nsmoothing helps improve accuracy for all models\nwhen compared to their OOB counterparts.\n4 Conclusion\nWe presented a thorough empirical study of the ef-\nfects of model size (number of parameters) on cali-\nbration. Through experiments on a number of tasks,\nwe demonstrated that smaller transformer models\nare as well, and sometimes better, calibrated com-\npared to BERT-Base, when evaluated in-domain.\nOn out-of-domain evaluation, larger models are bet-\nter calibrated, out-of-box. Label-smoothed models\nare better calibrated and more accurate, on out-of-\n2100\ndomain data, with smaller models beneﬁting more\nfrom Label Smoothing.\nAcknowledgements\nResearch was sponsored by the Army Research\nOfﬁce and was accomplished under Grant Number\nW911NF-20-1-0080. This work was supported by\nContract FA8750-19-2-0201 with the US Defense\nAdvanced Research Projects Agency (DARPA).\nThe views expressed are those of the authors and\ndo not reﬂect the ofﬁcial policy or position of the\nDepartment of Defense, the Army Research Ofﬁce\nor the U.S. Government.\nReferences\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big?. In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, pages 610–623.\nSamuel R Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP.\nShrey Desai and Greg Durrett. 2020. Calibration of\npre-trained transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 295–302.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Wein-\nberger. 2017. On calibration of modern neural net-\nworks. In International Conference on Machine\nLearning, pages 1321–1330. PMLR.\nAshim Gupta, Giorgi Kvernadze, and Vivek Srikumar.\nBert & family eat word salad: Experiments with text\nunderstanding. arXiv preprint arXiv:2101.03453.\nShankar Iyer, Nikhil Dandekar, and Kornél Csernai.\n2017. Quora question pairs.\nAbhyuday Jagannatha et al. 2020. Calibrating struc-\ntured output predictors for natural language process-\ning. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics,\npages 2078–2092.\nLingkai Kong, Haoming Jiang, Yuchen Zhuang, Jie\nLyu, Tuo Zhao, and Chao Zhang. 2020. Cali-\nbrated language model ﬁne-tuning for in-and out-of-\ndistribution data. arXiv preprint arXiv:2010.11506.\nWuwei Lan, Siyu Qiu, Hua He, and Wei Xu. 2017.\nA continuously growing dataset of sentential para-\nphrases. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1224–1234.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. Albert: A lite bert for self-supervised learn-\ning of language representations.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nIlya Loshchilov and Frank Hutter. 2018. Decoupled\nweight decay regularization. In International Con-\nference on Learning Representations.\nRafael Müller, Simon Kornblith, and Geoffrey Hinton.\n2019. When does label smoothing help? arXiv\npreprint arXiv:1906.02629.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nOr Sharir, Barak Peleg, and Yoav Shoham. 2020. The\ncost of training nlp models: A concise overview.\narXiv e-prints, pages arXiv–2004.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. Mobilebert: a\ncompact task-agnostic bert for resource-limited de-\nvices. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2158–2170.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethinking\nthe inception architecture for computer vision. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 2818–2826.\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. 2019. Well-read students learn better:\nOn the importance of pre-training compact models.\n2101\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP,\npages 353–355.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018. Neural network acceptability judgments.\narXiv preprint arXiv:1805.12471.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Fun-\ntowicz, et al. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. arXiv\npreprint arXiv:1910.03771.\nA Dataset Details\nSince the GLUE tasks (Wang et al., 2018) do not\nhave an annotated public test-set, we split the dev-\nset equally such that one half forms the new dev-set\nand the other half forms the test-set. The dev-set is\nused for hyper-parameter selection. Table 3 shows\nthe details for each of the datasets considered.\nB Hyper-parameter Selection\nAll models are used from the HuggingFace Trans-\nformers Library (Wolf et al., 2019). All models are\nﬁne-tuned for 2 to 4 epochs with the best value cho-\nsen on the basis of the accuracy on the dev set. We\nset the batch size as 16 with a learning rate of 2e-5,\ngradient clip of 1.0, and no weight decay. All mod-\nels are optimized using AdamW (Loshchilov and\nHutter, 2018). All the experiments are performed\non NVIDIA 24GB GPUs (although most models\ncan be run on 11GB GPUs).\nDataset Train Dev Test\nSNLI 549,368 4,922 4,923\nMNLI 392,702 4907 4908\nSST-2 67,349 910 911\nQQP 363,871 20,216 20,217\nTwitterPPDB 46,667 5,060 5,060\nCOLA 8,551 531 532\nTable 3: Number of training, development and test ex-\namples for the various datasets we experiment with.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8087983131408691
    },
    {
      "name": "Speedup",
      "score": 0.7660311460494995
    },
    {
      "name": "Transformer",
      "score": 0.6729205250740051
    },
    {
      "name": "Calibration",
      "score": 0.6315463781356812
    },
    {
      "name": "Smoothing",
      "score": 0.583142876625061
    },
    {
      "name": "Language model",
      "score": 0.5618654489517212
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3894166052341461
    },
    {
      "name": "Machine learning",
      "score": 0.35888832807540894
    },
    {
      "name": "Voltage",
      "score": 0.1621149182319641
    },
    {
      "name": "Parallel computing",
      "score": 0.10171270370483398
    },
    {
      "name": "Engineering",
      "score": 0.08341321349143982
    },
    {
      "name": "Statistics",
      "score": 0.07802525162696838
    },
    {
      "name": "Mathematics",
      "score": 0.07527866959571838
    },
    {
      "name": "Computer vision",
      "score": 0.07085815072059631
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36788626",
      "name": "California University of Pennsylvania",
      "country": "US"
    }
  ]
}