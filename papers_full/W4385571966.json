{
  "title": "Grokking of Hierarchical Structure in Vanilla Transformers",
  "url": "https://openalex.org/W4385571966",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2741844230",
      "name": "Shikhar Murty",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2896488246",
      "name": "Pratyusha Sharma",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2558501541",
      "name": "Jacob Andreas",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2151390485",
      "name": "Christopher D. Manning",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2087327436",
    "https://openalex.org/W3202908475",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W3197009789",
    "https://openalex.org/W2963355447",
    "https://openalex.org/W2952744660",
    "https://openalex.org/W4290994975",
    "https://openalex.org/W2962911926",
    "https://openalex.org/W4285174271",
    "https://openalex.org/W4226434736",
    "https://openalex.org/W2087165009",
    "https://openalex.org/W3214210379",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3014415613",
    "https://openalex.org/W4302013344",
    "https://openalex.org/W4308168008",
    "https://openalex.org/W4299785371",
    "https://openalex.org/W2963347649"
  ],
  "abstract": "For humans, language production and comprehension is sensitive to the hierarchical structure of sentences. In natural language processing, past work has questioned how effectively neural sequence models like transformers capture this hierarchical structure when generalizing to structurally novel inputs. We show that transformer language models can learn to generalize hierarchically after training for extremely long periods—far beyond the point when in-domain accuracy has saturated. We call this phenomenon structural grokking. On multiple datasets, structural grokking exhibits inverted U-shaped scaling in model depth: intermediate-depth models generalize better than both very deep and very shallow transformers. When analyzing the relationship between model-internal properties and grokking, we find that optimal depth for grokking can be identified using the tree-structuredness metric of CITATION. Overall, our work provides strong evidence that, with extended training, vanilla transformers discover and use hierarchical structure.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 439–448\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nGrokking of Hierarchical Structure in Vanilla Transformers\nShikhar Murty† Pratyusha Sharma‡ Jacob Andreas‡ Christopher D. Manning†\n†Computer Science Department, Stanford University ‡MIT CSAIL\n{smurty, manning}@cs.stanford.edu, {pratyusha, jda}@mit.edu\nAbstract\nFor humans, language production and compre-\nhension are sensitive to the hierarchical struc-\nture of sentences. In natural language process-\ning, past work has questioned how effectively\nneural sequence models like transformers cap-\nture this hierarchical structure when generaliz-\ning to structurally novel inputs. We show that\ntransformer language models can learn to gener-\nalize hierarchically after training for extremely\nlong periods—far beyond the point when in-\ndomain accuracy has saturated. We call this\nphenomenon structural grokking. On multiple\ndatasets, structural grokking exhibits inverted\nU-shaped scaling in model depth: intermediate-\ndepth models generalize better than both very\ndeep and very shallow transformers. When ana-\nlyzing the relationship between model-internal\nproperties and grokking, we find that opti-\nmal depth for grokking can be identified using\nthe tree-structuredness metric of Murty et al.\n(2023). Overall, our work provides strong evi-\ndence that, with extended training, vanilla trans-\nformers discover and use hierarchical structure.\n1 Introduction\nAlthough human language is produced as a linear\nsequence, it is hierarchically organized. Smaller\nunits compose to form larger constituents. The abil-\nity to infer this hierarchical structure underlies our\nability to produce and understand new sentences\n(Chomsky, 1965; Crain and Nakayama, 1987). In\nthis paper, we investigate whether standard neural\ntransformer models (Vaswani et al., 2017) can also\ngeneralize hierarchically when trained on language\nprocessing tasks (Fig 1). Our main finding is that\nhierarchical generalization in transformers does oc-\ncur, but very slowly: performance on structurally\nnovel sentences increases gradually, long after per-\nformance on sentences from the training distribu-\ntion has plateaued. We term this phenomenonstruc-\ntural grokking, by analogy to existing findings on\nsimple classification tasks (Power et al., 2022).\nExample Non-hierarchical Rule (fits train set but does not generalize): \nHierarchical Rule (fits train set and also generalizes): \n(b) Question-Formation\nMove the first auxiliary in the sentence.\nMove the auxiliary verb for the matrix subject.\n(a) Dyck-LM\nTraining\n[ ( ( ( { { [ ] } } ) ) ) ]\n( [ ( [ ] ) ] ) { { ( [ ] ) } }\n( { { [ ] } } ) { { ( [ ] ) } }\n[ ( ( ) ) { } { } ( ) [ ] { { } } ]\n{ {  } ( ( ( ) ) ) [ ]\nMy walrus does move. Does my walrus move?\nHer vultures don't comfort the dogs that do wait. \nDon’t her vultures comfort the dogs that do wait?\nYour xylophone who doesn't eat does swim. Does \nyour xylophone who doesn’t eat swim?\nOutput the most frequent \nbracket at this index.\nMatch the most recent \nunmatched open bracket.\nTraining\nGeneralization Generalization\nFigure 1: Examples from language modeling datasets\nwe use to assess hierarchical generalization in vanilla\ntransformers. These datasets are constructed so that\nboth a non-hierarchical as well as a hierarchical rule can\nperfectly fit the training set, but only the hierarchical\nrule generalizes to structurally novel inputs.\nOn two datasets, we show that structural\ngrokking exhibits inverted U-shaped scaling be-\nhavior as a function of model depth: hierarchi-\ncal generalization improves, then declines, as we\ntrain deeper models. Prior work suggests that a\nnumber of model-internal properties might track\nthe emergence of hierarchical structure in trans-\nformers, including weight norms (Merrill et al.,\n2021; Liu et al., 2022; Power et al., 2022), attention\nsparsity (Merrill et al., 2021), and functional tree-\nstructuredness (Murty et al., 2023). We find that\nfunctional tree-structuredness is uniquely able to\npredict structural grokking—while weight norms\nand attention sparsity increase monotonically in\nmodel depth, tree-structuredness is highest for mod-\nels of the optimal depth for structural grokking.\nOur results challenge findings from prior work\n(Mueller et al., 2022; Petty and Frank, 2021) claim-\ning that ordinary transformers completely fail on\nthe tests of hierarchical generalization that we study.\nWe attribute these failures to early stopping based\non in-domain validation performance, which signif-\n439\nicantly underestimates hierarchical generalization\ndue to structural grokking. On the datasets where\nthis prior work reports generalization accuracies\nbelow 20%, simply by training for longer , mean\naccuracy across random seeds reaches 80%, and\nseveral seeds achieve near-perfect generalization\nperformance. Past findings are also partially ex-\nplained by U-shaped scaling: this work uses mod-\nels that are too shallow (Mueller et al., 2022; Petty\nand Frank, 2021) or too deep (Mueller et al., 2022).\nOur results align with past findings on the role\nof extended training in other language processing\nproblems (Csordás et al., 2021; Hoffmann et al.,\n2022).\n2 Background\nTransformers Given a sequence of tokens\nw≤i = w1,w2,...,w i, where each token is drawn\nfrom a fixed vocabulary V, an L-layer transformer\nlanguage model (LM) fL\nθ outputs a distribution\nover the next token wi+1 ∈V, fL\nθ (w≤i) ∈R|V|.\nA key part of the architecture is a sequence of L\nself-attention layers, where layer pcomputes con-\ntextual vectors of token kas a non-linear paramet-\nric function of a convex combination of contex-\ntual vectors of tokens w≤k from the previous layer,\nwhere coefficients ap\nk ∈Rk are known as the at-\ntention distribution. The LM weights are learned\nby maximizing the log probability of the correct\ncontinuation wk+1, given prefix w≤k.\nHierarchical structure in transformersWhile\nunsupervised pre-training of transformers has led\nto state-of-the-art transfer learning results across\nNLP, the architecture itself has been claimed to lack\nhuman-like inductive biases toward hierarchical\nstructure (Tran et al., 2018; Hahn, 2020; Petty and\nFrank, 2021; Mueller et al., 2022). We revisit these\nclaims in this work.\nTo understand whether a given model has a bias\nfor acquiring hierarchical structure, we follow Mc-\nCoy et al. (2020) and evaluate generalization in\nmodels trained on ambiguous tasks in which train-\ning data is consistent with both a “hierarchical rule”\nas well as a “non-hierarchical rule” (Fig 1). To test\nif the hierarchical rule has been acquired, we test\ngeneralization on a separate out-of-distribution test\nset, constructed such that only learners that have\nacquired the hierarchical rule are successful.\nGrokking Power et al. (2022) identify the phe-\nnomenon of grokking on small algorithmic datasets\nwhere they find that test performance improves\nlong after training performance has saturated. We\nhypothesize a similar structural grokking, where\nthe model groks hierarchical structure long after\nin-domain validation performance has saturated,\nand consequently, hierarchical generalization can\ncontinue to improve with extended training.\n3 Experiments\nDatasets Since our goal is to understand hier-\narchical generalization in transformers, we use\ntwo datasets from (McCoy et al., 2020) and ad-\nditionally evaluate on a simple bracket-tracking\ntask. For Dyck, models are trained to predict next\ntokens in strings drawn from Dyck20,10, the lan-\nguage of well-nested brackets with 20 types and\nmax nesting depth of 10. We evaluate generaliza-\ntion to structurally unobserved strings inDyck20,10\n(see Fig 1 for examples and Appendix A for de-\ntails). For the McCoy et al. (2020) datasets, in\nQuestion-Formation, models must convert English\nsentences into questions and, in Tense-Inflection,\nmodels must map from sentences and tense markers\nto appropriately re-inflected sentences. We evalu-\nate generalization on the out-of-distribution test set\nfrom McCoy et al. (2020).\nModel We train transformer LMs with {2, 4, 6, 8,\n10} layers (see Appendix B for more details). For\neach depth, we train models with 10 random seeds\nfor 300k (400k for Dyck) steps. Given the input\nsentence (or prefix in the case of Dyck) we decode\ngreedily from the model at test time. For Dyck, we\nreport the accuracy of generating the correct clos-\ning bracket type by ranking among closing brack-\nets, given an input prefix from the language. As\ndone in prior work (McCoy et al., 2020; Petty and\nFrank, 2021; Mueller et al., 2022), for Question-\nFormation, we report first word accuracy of the\ndecoded question, and for Tense-Inflection, we re-\nport the fraction of test inputs for which the target\nverb is correctly inflected.\n3.1 Main Results\nTransformers exhibit structural grokkingWe\nfirst present results obtained with the best model\ndepth on all datasets in Fig 2. We find clear evi-\ndence of structural grokking: Across datasets, gen-\neralization improves many training steps after in-\ndistribution accuracy has saturated, sometimes ap-\nproaching perfect accuracy.\n440\nEarly stopping on in-domain validation accuracy Training till structural grokking occurs\n(a) Dyck (6 layers)\nin-domain val accuracy\ngeneralization accuracy\n(b) Question-Formation (6 layers)\nin-domain val accuracy\ngeneralization accuracy\n(c) Tense-Inflection (4 layers)\nin-domain val accuracy\ngeneralization accuracy\nthousand steps thousand steps thousand steps\nAccuracy\n∆ ≈ 2%\n∆ ≈ 65%\n∆ ≈ 50%\n∆  ≈ 20%\n∆ ≈ 8% ∆ ≈ 0%\nFigure 2: Average accuracy across 10 random seeds on the in-domain val set (solid) and generalization set (dashed)\nfor all datasets. Generalization performance improves even after in-domain accuracies have saturated, showing\nstructural grokking. We highlight with orange and blue lines the gap between in-domain and generalization\naccuracies at the point of early stopping based on the in-domain val set performance vs. at the end of training, noting\nthat prior work stops training at the orange line. Stopping training prior to structural grokking can result in a vast\nunderestimation of generalization performance.\nTense-Inflection\n(a) Inverted U-shaped scaling (b) Dynamics of model internal properties\nQuestion-FormationAverage success Average success\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ntree structurednesstree structuredness\nweight norm weight norm\nattention sparsity attention sparsity\nthousand steps thousand steps thousand steps\n10 layer > 6 layer > 2 layer\n 6 layer > 10 layer > 2 layer\n10 layer > 6 layer > 2 layer\n6 layer >  4 layer  > 2 layer\n 6 layer >  4 layer  > 2 layer\n 4 layer >  6 layer  > 2 layer\nFigure 3: (a) Inverted U-shaped laws for grokking: On Question-Formation (top) and Tense-Inflection (bottom),\nwe find that both very small and very deep models either fail to exhibit structural grokking or do so infrequently,\ncompared to an in-between optimal model depth. (b) While weight norms and attention sparsity increase for all\nmodels and do not differentiate between different sizes, tree-structuredness is highest for the optimal model depth.\nEarly stopping considered harmful Next, we\ncompare generalization accuracy obtained by early\nstopping on in-domain validation accuracy (as done\nin Petty and Frank (2021); Mueller et al. (2022)) to\nlonger training runs (Fig 2). Early stopping leads to\nvastly underestimating generalization. For instance,\naverage generalization goes up from <40%, <50%\nto <90%, <80% on Question-Formation and Tense-\nInflection, respectively.\nInverted U-shaped scaling On Question-\nFormation and Tense-Inflection, we train models\nof increasing depths from 2 to 10 layers. For\neach depth, we report the fraction of seeds (out\nof 10) where generalization accuracy eventually\ncrosses 80%, in Fig 3a. We find an inverted\nU-shaped scaling behavior—very shallow and very\ndeep models are unsuccessful, while most seeds\ngeneralize in models of intermediate depth. This\nmay also explain why prior work that either used\nvery shallow models (1–3-layer transformers in\nPetty and Frank (2021); Mueller et al. (2022))\nor very deep models (12-layer transformers in\nMueller et al. (2022)) failed to generalize well.\n4 Analysis\nGiven that structural grokking occurs only in a sub-\nset of model architectures, can we identify when it\nhas happened (or predict when it will occur)? Sev-\neral model-internal properties have been claimed to\nrelate to either grokking or emergent hierarchical\n441\nstructure in transformers.\nWeight Norms Recent work (Power et al., 2022;\nLiu et al., 2022) identifies the L2 norm of parame-\nter weights as an important quantity for grokking.\nFor instance, Power et al. (2022) find weight decay\nto improve grokking speed and Liu et al. (2022)\nidentify a “goldilocks zone” in weight norm space\nwhere grokking occurs. More generally, norm\ngrowth over the course of training has been studied\nas a key factor in neural network generalization\n(Soudry et al., 2018).\nAttention Sparsity Merrill et al. (2021) prove\nthat norm growth in transformers leads to attention\nsaturation, an important property for emergent lin-\nguistic structure (Merrill et al., 2022). As a proxy\nfor attention sparsity of fL\nθ , we compute the nega-\ntive mean entropy of all distributions {ap\nk}.\nTree-structuredness McCoy et al. (2020) show\nthat tree-structured encoders such as Tai et al.\n(2015) show near perfect hierarchical general-\nization. While transformers are relatively un-\nconstrained, recent evidence suggests that, when\ntrained on language data, they implictly implement\n(approximately) tree-structured computations. In\nparticular, the tree projection method of Murty\net al. (2023) precisely characterizes the extent to\nwhich a transformer’s internal computation on an\ninput can be approximated with a tree-structured\nneural encoding, providing a tree-structuredness\nscore (tscore) for any transformer, and a binary tree\nthat best approximates its computation on an input\nstring (see Appendix C for details). To evaluate\nwhether these trees correspond to human notions\nof syntax, we additionally compare recovered trees\nto gold-standard ones (tparseval, Black et al., 1991).\n4.1 Results\nWe characterize the dynamics of weight norms\n(normalized by number of layers to compare dif-\nferent model depths), attention sparsity, and tree-\nstructuredness, by computing these quantities every\n3k gradient updates for Question-Formation and\nTense-Inflection. For data-dependent properties\nsuch as attention sparsity and tree-structuredness,\nwe sample 10k examples from the training data.\nWe plot these quantities for the smallest model, the\nlargest model for which at least one run shows suc-\ncessful grokking, and for the optimal model depth,\nin Fig 3b.\nTense-Inflection\n(a) Inverted U-shaped scaling (b) Dynamics of model internal properties\nQuestion-FormationAverage success Average success\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\ntree structurednesstree structuredness\nweight norm weight norm\nattention sparsity attention sparsity\nthousand steps thousand steps thousand steps\n10 layer > 6 layer > 2 layer\n 6 layer > 10 layer > 2 layer\n10 layer > 6 layer > 2 layer\n6 layer >  4 layer  > 2 layer\n 6 layer >  4 layer  > 2 layer\n 4 layer >  6 layer  > 2 layer\nthousand stepsthousand steps\n(a) Question-Formation (b) Tense-Inflection\nFigure 4: While some models fail to generalize hierar-\nchically, all models are effective at learning computa-\ntions whose closest tree structures progressively evolve\ntowards ground truth syntax, matching or outperforming\na right branching baseline (in dashed red).\nOptimal models are most tree-structured\nWeight norms and attention sparsity grow for all\nmodel settings in both datasets. However, these\nproperties by themselves are unable to predict that\nboth shallow and deep models fail—shallow mod-\nels learn the sparsest solutions as well as solutions\nwith largest weight norms, but never generalize\nhierarchically. As noted by Murty et al. (2023),\ntscore improves over time for all models, indicat-\ning increased tree-structuredness over time. For\nboth datasets, the “optimal” model learns the most\ntree-structured solution compared to both deep and\nshallow models. Liu et al. (2022) note that, on\nalgorithmic tasks, grokking “coincides with the\nemergence of structure in embeddings”. Similarly,\nfor language tasks, we find that structural grokking\ncoincides with the emergence of tree structured\ninternal computations.\nTransformers are surprisingly effective at struc-\nture induction From the dynamics of tparseval\nin Fig 4, we note that all models, regardless of\nwhether they generalize or not, learn structures that\nare close to ground truth syntax, sometimes outper-\nforming a right-branching baseline. McCoy et al.\n(2020) note that tree-structured encoders only gen-\neralize when structured according to correct parse\ntrees. Here, we find that all transformers learn cor-\nrect tree structures, but only the ones that are the\nmost tree-structured generalize best.\n5 Conclusion\nThis work shows that transformers are capable of\nexhibiting structure-sensitive “hierarchical general-\nization” via a grokking mechanism. Their overall\nlearning behavior gradually shifts from memoriza-\ntion (high in-domain accuracy, poor out-of-domain\naccuracy) to generalization (high in-domain and\n442\nout-of-domain accuracy). While we show such be-\nhavior on relatively small datasets with small mod-\nels, we believe these results may have broader im-\nplications, as training for longer has been shown to\nhelp even for web-scale language modeling (Hoff-\nmann et al., 2022) and on compositional gener-\nalization tasks (Csordás et al., 2021). Structural\ngrokking happens most often at “medium-sized”\nmodel depths, and both very shallow and very deep\nmodels fail to exhibit it. While properties pre-\nviously connected with linguistic generalization\nin transformers such as weight norms and atten-\ntion sparsity do not differentiate good architectures\nfrom bad ones, functional tree-structuredness of\nthe transformer can well predict the optimal model\ndepth. While there are clear limitations to the trans-\nformer architecture (such as the inability to imple-\nment unbounded recursion), our results show that it\nmay have stronger inductive biases than previously\nbelieved: With sufficient training, transformers can\nrepresent hierarchical sentence structure and use\nthis structure to generalize correctly.\n6 Reproducibility\nAll code and data for these experiments is avail-\nable at https://github.com/MurtyShikhar/\nstructural-grokking.git.\n7 Acknowledgements\nSM was funded by a gift from Apple Inc. CM is\na fellow in the CIFAR Learning in Machines and\nBrains program. We thank John Hewitt, Belinda\nLi, Rishi Bommasani and members of the Stanford\nNLP group for feedback on the paper.\nLimitations\nOur work has the following limitations. First, we\nonly evaluate generalization on datasets based on\nEnglish language. Second, we show structural\ngrokking on three datasets, and while we believe\nthis to be a general phenomenon, we leave investi-\ngating similar behavior on other datasets for future\nwork. Next, we also do not study the effect of train-\ning data size on structural grokking, and do not\ninvestigate whether transformers learn to grok hier-\narchical structure in low data regimes. Finally, all\ndatasets here are based on context-free grammars,\neither similar to or taken directly from prior work,\nand we believe constructing similar generalization\nbenchmarks on real language data is a good avenue\nfor future work.\nReferences\nE. Black, S. Abney, D. Flickenger, C. Gdaniec, R. Gr-\nishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,\nJ. Klavans, M. Liberman, M. Marcus, S. Roukos,\nB. Santorini, and T. Strzalkowski. 1991. A procedure\nfor quantitatively comparing the syntactic coverage\nof English grammars. In Speech and Natural Lan-\nguage: Proceedings of a Workshop Held at Pacific\nGrove, California, February 19-22, 1991.\nNoam Chomsky. 1965. Aspects of the Theory of Syntax.\nThe MIT Press, Cambridge.\nStephen Crain and Mineharu Nakayama. 1987. Struc-\nture dependence in grammar formation. Language,\n63(3):522–543.\nRóbert Csordás, Kazuki Irie, and Juergen Schmidhuber.\n2021. The devil is in the detail: Simple tricks im-\nprove systematic generalization of transformers. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, pages 619–\n634, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nMichael Hahn. 2020. Theoretical limitations of self-\nattention in neural sequence models. Transactions of\nthe Association for Computational Linguistics, 8:156–\n171.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de Las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, et al. 2022. Training compute-\noptimal large language models. In Proceedings of the\n36th Conference on Neural Information Processing\nSystems (NeurIPS 2022).\nZiming Liu, Eric J Michaud, and Max Tegmark. 2022.\nOmnigrok: Grokking beyond algorithmic data. In\nThe Eleventh International Conference on Learning\nRepresentations.\nR. Thomas McCoy, Robert Frank, and Tal Linzen. 2020.\nDoes syntax need to grow on trees? Sources of hier-\narchical inductive bias in sequence-to-sequence net-\nworks. Transactions of the Association for Computa-\ntional Linguistics.\nWilliam Merrill, Vivek Ramanujan, Yoav Goldberg, Roy\nSchwartz, and Noah A. Smith. 2021. Effects of pa-\nrameter norm growth during transformer training:\nInductive bias from gradient descent. In Proceedings\nof the 2021 Conference on Empirical Methods in Nat-\nural Language Processing, pages 1766–1781, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nWilliam Merrill, Ashish Sabharwal, and Noah A. Smith.\n2022. Saturated transformers are constant-depth\nthreshold circuits. Transactions of the Association\nfor Computational Linguistics, 10:843–856.\nAaron Mueller, Robert Frank, Tal Linzen, Luheng Wang,\nand Sebastian Schuster. 2022. Coloring the blank\n443\nslate: Pre-training imparts a hierarchical inductive\nbias to sequence-to-sequence models. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 1352–1368, Dublin, Ireland. Association\nfor Computational Linguistics.\nShikhar Murty, Pratyusha Sharma, Jacob Andreas, and\nChristopher D Manning. 2023. Characterizing intrin-\nsic compositionality in transformers with tree projec-\ntions. In The Eleventh International Conference on\nLearning Representations.\nJackson Petty and Robert Frank. 2021. Trans-\nformers generalize linearly. arXiv preprint\narXiv:2109.12036.\nAlethea Power, Yuri Burda, Harri Edwards, Igor\nBabuschkin, and Vedant Misra. 2022. Grokking:\nGeneralization beyond overfitting on small algorith-\nmic datasets. arXiv preprint arXiv:2201.02177.\nOfir Press and Lior Wolf. 2017. Using the output em-\nbedding to improve language models. InProceedings\nof the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Vol-\nume 2, Short Papers, pages 157–163, Valencia, Spain.\nAssociation for Computational Linguistics.\nDaniel Soudry, Elad Hoffer, Mor Shpigel Nacson,\nSuriya Gunasekar, and Nathan Srebro. 2018. The im-\nplicit bias of gradient descent on separable data. The\nJournal of Machine Learning Research, 19(1):2822–\n2878.\nKai Sheng Tai, Richard Socher, and Christopher D. Man-\nning. 2015. Improved semantic representations from\ntree-structured long short-term memory networks. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1556–\n1566, Beijing, China. Association for Computational\nLinguistics.\nKe M Tran, Arianna Bisazza, and Christof Monz. 2018.\nThe importance of being recurrent for modeling hier-\narchical structure. In Proceedings of the 2018 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 4731–4736.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30.\n444\nDataset Train In-Domain Val Generalization\nDyck 200000 20000 20000\nQuestion-Formation 100000 1000 10000\nTense-Inflection 100000 1000 10000\nTable 1: Statistics for all datasets used in this work.\nA Dataset Details\nAll statistics are in Table 1. For Question-\nFormation and Tense-Inflection, we use splits as\ngiven in McCoy et al. (2020) with no additional\npreprocessing. We give details of Dyck below.\nDyck Details We construct our Dyck dataset by\nsampling 200k strings from Dyck20,10, the lan-\nguage of well-nested brackets with 20 different\nbracket types and nesting depth atmost 10. For\neach string, we define its structure as a binary vec-\ntor of 0s and 1s. For instance the structure of “(([]))”\nis “11110000”. To construct a generalization set,\nwe sample strings with unobserved structures i.e.\nstrings whose 0-1 structure does not match the\nstructure of any of the training strings. Since the\nobjective at test time is to measure closing bracket\naccuracy, we only rank model probability among\nall closing brackets, and only evaluate on prefixes\nwhere the opening bracket is atleast 10 tokens away\nfrom its corresponding closing bracket.\nB Model Details\nWe use a transformer language model with the fol-\nlowing hyperparameters:\n• Number of attention heads = 4\n• Hidden dimensionality = 512\n• Tied input and output matrices as done in\nPress and Wolf (2017)\nNext, we use the following hyperpameters for\noptimization:\n• AdamW (β1: 0.9, β2: 0.999, ϵ: 1e-7), with\nlearning rates in {1e-4, 5e-5, 1e-5}, noting\nthat 1e-4 works best for all experiments. We\nuse a linear warmup scheduler warming up\nfrom 0 to the final learning rate over 10k gra-\ndient steps.\n• We clip gradients to have a max L2 norm of\n10.\n• We use a batch size of 8.\nC Functional Tree-Structuredness\nTree Projections (TP; Murty et al. (2023)) measure\nhow well computations performed by a given trans-\nformer f can be approximated with tree-structured\nencoders. To do this, TP solves the following opti-\nmization problem:\nϕproj,Tproj ≜ arg min\nϕ,T\nL(f,gϕ,T), (1)\nwhere gϕ is the class of tree structured encoders\nthat processes sentence Saccording to bottom-up\ntrees T(S), and Lis a distance function between\nvector outputs of fand gϕ on spans from the binary\ntree T. TP minimizes Equation 1 approximately,\nand recovers an approximate ˆTproj. The tree score\nover a dataset Dis defined as\ntscore ≜\n∑\nS∈DETSCI(S,T) −SCI(S, ˆTproj(S))\n|D| ,\n(2)\nwhere SCI (span contextual invariance) is the dis-\ntance between contextual and context-free vector\nrepresentations of all spans pin T (for more details,\nsee Murty et al. (2023)). In particular, SCI score\nfor a sentence Sstructured according to T(S) is\nSCI(S,T) ≜\n∑\ns∈T\nd(vS\np,˜vp) (3)\nfor some suitably chosen distance function d(here,\ncosine similarity). To measure the bracketing F1\nscore (PARSEV AL; Black et al. (1991)) of the\ninduced tree projection of the transformer ˆTproj\nagainst ground truth gold syntax trees, Tg, when\navailable, Murty et al. (2023) define\ntparseval ≜ PARSEVAL ( ˆTproj,Tg,D). (4)\nD Training Loss Curves\nWe explore the hypothesis that syntactic grokking\nis simply a result of the training loss continuing to\ndecrease, even after in-domain validation perfor-\nmance has saturated in Fig 5. We note that training\nlosses generally saturate before in-domain valida-\ntion performance saturates (also noted in Power\net al. (2022)). Next, we also find that all models,\nregardless of whether they grok or not, eventually\nget to comparable training losses. We conclude\nthat the inverted U-shaped trend is not an artifact\nof poorly optimized models.\n445\n(a) 2 layer\n(e) 10 layer\n(d) 8 layer\n(c) 6 layer\n(b) 4 layer\nFigure 5: We plot average training loss for all model depths on the Question-Formation dataset. We note that (1)\ngrokking happens even though training losses fully flat line around 10k gradient steps, and that (2) the inverse\nU-shaped scaling is not a result of poor optimization of small / large models since all models eventually have the\nsame stable training loss as the optimal model depth.\n446\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection-6\n□\u0017 A2. Did you discuss any potential risks of your work?\nLeft blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nYes (in Section-1 and Abstract)\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nWe trained several transformer models, the checkpoints of which will be available upon de-anonymization\n□\u0013 B1. Did you cite the creators of artifacts you used?\nWe cite all prior work whose datasets we use in Sections-2 and 3\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nLeft blank.\nC □\u0013 Did you run computational experiments?\nLeft blank.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nLeft blank.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n447\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nLeft blank.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nLeft blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n448",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7563856840133667
    },
    {
      "name": "Computer science",
      "score": 0.6604397296905518
    },
    {
      "name": "Tree structure",
      "score": 0.5477133393287659
    },
    {
      "name": "Comprehension",
      "score": 0.5348374247550964
    },
    {
      "name": "Language model",
      "score": 0.5301331877708435
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5148874521255493
    },
    {
      "name": "Scaling",
      "score": 0.5111399292945862
    },
    {
      "name": "Natural language processing",
      "score": 0.41953301429748535
    },
    {
      "name": "High fidelity",
      "score": 0.41420912742614746
    },
    {
      "name": "Algorithm",
      "score": 0.29005271196365356
    },
    {
      "name": "Voltage",
      "score": 0.1391737461090088
    },
    {
      "name": "Mathematics",
      "score": 0.13712817430496216
    },
    {
      "name": "Engineering",
      "score": 0.11911731958389282
    },
    {
      "name": "Binary tree",
      "score": 0.11519438028335571
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210156583",
      "name": "Laboratoire d'Informatique de Paris-Nord",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    }
  ],
  "cited_by": 10
}