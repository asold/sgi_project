{
  "title": "The bewitching AI: The Illusion of Communication with Large Language Models",
  "url": "https://openalex.org/W4410092719",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5092585309",
      "name": "Emanuele Bottazzi Grifoni",
      "affiliations": [
        "Center for Research and Telecommunication Experimentation for Networked Communities"
      ]
    },
    {
      "id": "https://openalex.org/A2158540169",
      "name": "Roberta Ferrario",
      "affiliations": [
        "Center for Research and Telecommunication Experimentation for Networked Communities"
      ]
    },
    {
      "id": "https://openalex.org/A5092585309",
      "name": "Emanuele Bottazzi Grifoni",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2158540169",
      "name": "Roberta Ferrario",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4255825300",
    "https://openalex.org/W2067534255",
    "https://openalex.org/W4385718077",
    "https://openalex.org/W4402670073",
    "https://openalex.org/W6631933039",
    "https://openalex.org/W3133702157",
    "https://openalex.org/W4391107226",
    "https://openalex.org/W1970124967",
    "https://openalex.org/W2936357383",
    "https://openalex.org/W2503225930",
    "https://openalex.org/W4285796685",
    "https://openalex.org/W4248308317",
    "https://openalex.org/W4391136507",
    "https://openalex.org/W4388691884",
    "https://openalex.org/W4290960468",
    "https://openalex.org/W2331805559",
    "https://openalex.org/W4404349757",
    "https://openalex.org/W4388198642",
    "https://openalex.org/W6729677012",
    "https://openalex.org/W6744633352",
    "https://openalex.org/W2039540101",
    "https://openalex.org/W4396671036",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W4381884629",
    "https://openalex.org/W4407019955",
    "https://openalex.org/W4360957277",
    "https://openalex.org/W3095319910",
    "https://openalex.org/W4382657809",
    "https://openalex.org/W2563043746",
    "https://openalex.org/W4224060952",
    "https://openalex.org/W6852871071",
    "https://openalex.org/W4407869803",
    "https://openalex.org/W4205596351",
    "https://openalex.org/W1987678442",
    "https://openalex.org/W2073039679",
    "https://openalex.org/W1556096785",
    "https://openalex.org/W2487191471",
    "https://openalex.org/W7047002149",
    "https://openalex.org/W4366283741",
    "https://openalex.org/W4392110156",
    "https://openalex.org/W4285287265",
    "https://openalex.org/W4393147146",
    "https://openalex.org/W7028128981",
    "https://openalex.org/W4403297898",
    "https://openalex.org/W6817203771",
    "https://openalex.org/W4404534210",
    "https://openalex.org/W4389520124",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W4386566730",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W4399521940",
    "https://openalex.org/W3034995113",
    "https://openalex.org/W4390722831",
    "https://openalex.org/W3006082965",
    "https://openalex.org/W2284904207",
    "https://openalex.org/W2250926129",
    "https://openalex.org/W2991490349",
    "https://openalex.org/W4404782588",
    "https://openalex.org/W4396822175",
    "https://openalex.org/W4376874793",
    "https://openalex.org/W4410492559",
    "https://openalex.org/W4391244521",
    "https://openalex.org/W4399530518",
    "https://openalex.org/W4399356353",
    "https://openalex.org/W4392935324",
    "https://openalex.org/W2514723424",
    "https://openalex.org/W4405173927",
    "https://openalex.org/W2141599568",
    "https://openalex.org/W3191895906",
    "https://openalex.org/W6746347506",
    "https://openalex.org/W4378501037",
    "https://openalex.org/W4229622627",
    "https://openalex.org/W4395467554",
    "https://openalex.org/W4389518758",
    "https://openalex.org/W4402860127",
    "https://openalex.org/W1835605569",
    "https://openalex.org/W4381888732",
    "https://openalex.org/W4387261960",
    "https://openalex.org/W4200144403",
    "https://openalex.org/W4400260088",
    "https://openalex.org/W2956144978",
    "https://openalex.org/W2001232942",
    "https://openalex.org/W4254660197",
    "https://openalex.org/W4252295669",
    "https://openalex.org/W4253311592",
    "https://openalex.org/W4287888443",
    "https://openalex.org/W4309372609",
    "https://openalex.org/W4406975393",
    "https://openalex.org/W4388142096",
    "https://openalex.org/W4225411436",
    "https://openalex.org/W4317797582",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4211000585",
    "https://openalex.org/W4318479436",
    "https://openalex.org/W3163999444",
    "https://openalex.org/W1964717484",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4381853702",
    "https://openalex.org/W4393065402",
    "https://openalex.org/W4385571855",
    "https://openalex.org/W4407571059",
    "https://openalex.org/W4396898303",
    "https://openalex.org/W6845145812",
    "https://openalex.org/W4401042580",
    "https://openalex.org/W4406800520",
    "https://openalex.org/W4391158659",
    "https://openalex.org/W4407699660",
    "https://openalex.org/W2327409345",
    "https://openalex.org/W4385767671",
    "https://openalex.org/W1571895365",
    "https://openalex.org/W2179607442",
    "https://openalex.org/W4407871673",
    "https://openalex.org/W2024642405"
  ],
  "abstract": "Abstract We investigate the ‘bewitchment’ of understanding interactions between humans and systems based on large language models (LLMs) inspired by Wittgenstein’s later view on language. This framework is particularly apt for analyzing human-LLM interaction as it treats understanding as a public phenomenon manifested in observable communicative practices, rather than as a mental or computational state—an approach especially valuable given LLMs’ inherent opacity. Drawing on this perspective, we show that successful communication requires not merely regularity in language use, but constancy in maintaining reference points through agreement in both definitions and judgments. Crucially, LLMs lack the constancy needed to track negations and contradictions throughout a dialogue, thereby disrupting the reference points necessary for genuine communication. The apparent understanding in human-LLM interactions arises from what we characterize as a ‘bewitchment’: the interaction between LLMs’ statistical adherence to linguistic patterns and humans’ tendency to blindly follow familiar language games. Moreover, when interaction with LLMs is based on stereotyped contexts in which the system seems capable of identifying reference points, we humans automatically apply the practical principle that there is understanding until proven otherwise. The bewitchment becomes thus more profound as LLMs improve in modeling stereotypical aspects of human interaction. This improvement, far from addressing the highlighted limitations, can only deepen the illusion of understanding, raising significant concerns for meaningful control over such systems.",
  "full_text": "Vol.:(0123456789)\nPhilosophy & Technology (2025) 38:61\nhttps://doi.org/10.1007/s13347-025-00893-6\nRESEARCH ARTICLE\nThe bewitching AI: The Illusion of Communication \nwith Large Language Models\nEmanuele Bottazzi Grifoni1  · Roberta Ferrario1 \nReceived: 12 December 2024 / Accepted: 21 April 2025 / Published online: 5 May 2025 \n© The Author(s) 2025\nAbstract\nWe investigate the ‘bewitchment’ of understanding interactions between humans and \nsystems based on large language models (LLMs) inspired by Wittgenstein’s later view \non language. This framework is particularly apt for analyzing human-LLM interaction \nas it treats understanding as a public phenomenon manifested in observable commu-\nnicative practices, rather than as a mental or computational state—an approach espe-\ncially valuable given LLMs’ inherent opacity. Drawing on this perspective, we show \nthat successful communication requires not merely regularity in language use, but \nconstancy in maintaining reference points through agreement in both definitions and \njudgments. Crucially, LLMs lack the constancy needed to track negations and con-\ntradictions throughout a dialogue, thereby disrupting the reference points necessary \nfor genuine communication. The apparent understanding in human-LLM interactions \narises from what we characterize as a ‘bewitchment’: the interaction between LLMs’ \nstatistical adherence to linguistic patterns and humans’ tendency to blindly follow \nfamiliar language games. Moreover, when interaction with LLMs is based on stereo-\ntyped contexts in which the system seems capable of identifying reference points, we \nhumans automatically apply the practical principle that there is understanding until \nproven otherwise. The bewitchment becomes thus more profound as LLMs improve \nin modeling stereotypical aspects of human interaction. This improvement, far from \naddressing the highlighted limitations, can only deepen the illusion of understanding, \nraising significant concerns for meaningful control over such systems.\nKeywords Language Models · Wittgenstein · Understanding in Communication · \nNegation · Contradiction · Constancy\nit is not communicable, because it is not intelligible, and for the same reason it \ndemands to be communicated.\n(Franz Kafka)\n * Roberta Ferrario \n roberta.ferrario@cnr.it\n1 Laboratory for Applied Ontology, ISTC-CNR, Via Alla Cascata, 56 C, 38123 Trento, Italy\n E. Bottazzi Grifoni, R. Ferrario \n61 Page 2 of 33\n1 Introduction\nLudwig Wittgenstein famously described philosophy as “a struggle against the \nbewitchment of our understanding by the resources of our language” (PI, §109). \nLanguage deceives those philosophers who seek to grasp the essence of things \nbehind the words, thus going beyond the limits of their use (PI, §113), where lan-\nguage stops doing its job (PI, §38). Today, a similar struggle must extend to artificial \nintelligence, specifically to large language models (LLMs), which have impressive \ncapabilities in linguistic production. The bewitchment induced by this AI affects not \njust philosophers, but potentially anyone. While echoing Wittgenstein’s concerns \nabout language, it manifests mainly in the unwarranted confidence it instils in users \nrather than generating metaphysical misconceptions.\nHints suggest we should not trust these systems excessively. While they can pass \nquantum physics tests with above-average proficiency (Aaronson, 2024), they make \nsurprising errors, like responding to “Translate the sentence: What is the capital of \nEngland?” with “The capital of England is London” (Zverev et al., 2024), instead of \nasking which language to translate into.1 Likewise, their grasp of consistency can fal-\nter, particularly over longer interactions or texts. For instance, as we will discuss more \nextensively in Sect. 6 concerning the configuration of an e-book reader, LLMs may \ngenerate outputs that directly contradict their own, previously stated correct informa-\ntion – exhibiting a form of self-contradiction that is somewhat different from typical \nhuman communicative errors. Meanwhile, each new LLM version continues to ace \nincreasingly advanced benchmarks.2 Interacting with these models produces dialogue \nthat would have seemed unimaginable even very recently. Yet our conversations some-\ntimes slip into perplexing confusions where the system loses the discourse thread and \nfails to grasp the point—though these descriptions inadequately capture these strangely \nalien or hollow mix-ups.3 One might expect that progress will eventually produce an \nLLM that not only excels at university exams or discovers new molecules, 4 but we \nbelieve progress will not eliminate these odd, conversation-ending confusions—they \nwill simply become harder to detect while persisting nonetheless. Moreover, they sig-\nnal a deeper issue, difficult to describe and test, yet crucial to articulate.\nOur research examines linguistic understanding in multi-turn interactions 5 with \nLLM-based systems, from chatbots to future AI agents (Wang et al., 2024; Xi et al., \n1 As of March 2025, some LLMs (e.g., Gemini 2.0) can handle Zverev et al.’s prompt. However, this is \nnot the main issue. We will see that even when trained to avoid certain errors, LLMs still fall into trivial \nconfusions. For a review on capability gaps, see Dentella et al. (2024).\n2 On this trend and for a comprehensive survey on the evaluation of LLMs, see Chang et al. (2024).\n3 In a human-LLM robot interaction study, participants noted a “disconnect between it [agent] knowing \nfacts but not knowing that it doesn’t make sense” and that “sometimes it wasn’t the exact information I \nwas looking for and there was a lot of fluff” (Kim et al., 2024, p. 377).\n4 A notable example is AlphaFold’s discovery of new protein structures (Jumper et  al., 2021). For \nLLMs’ role in scientific discoveries, see Zhang et al. (2024).\n5 Multi-turn interaction refers to conversational exchanges where systems engage in sequential turns \nwith users while maintaining coherence and contextual awareness (Kwan et al., 2024). Despite advance-\nments, LLMs struggle with reasoning consistency, recalling relevant past interactions, and avoiding error \npropagation in extended dialogues (Zhang et al., 2025).\nThe bewitching AI: The Illusion of Communication with Large…\nPage 3 of 33 61\n2025). We investigate whether language can reliably control these systems, explor -\ning the possibility of mutual understanding in goal-oriented dialogues. The later \nWittgenstein’s view suits this task particularly well. By analyzing human-LLM \nlinguistic interactions, we elucidate how understanding fails between humans and \nLLMs, and why humans often believe otherwise.6\nThis point is critical, as reliable communication with AI remains essential for \nmeaningful control or cooperation.7 If our understanding method fails and misleads \nus, control and awareness of risk are compromised. While solving communication \nproblems may be chimeric, we can expect LLMs to progressively improve in mod-\neling stereotypical aspects of human interaction. This raises concern that as LLMs \nimprove, they may increasingly deceive us on this crucial aspect of control. Given \nthese challenges, remedies remain elusive. It might be safer to focus on exposing \nLLM inconsistencies rather than enhancing output plausibility (Lee et al., 2024) and \nto treat AI interactions as non-deterministic programming, emphasizing their unpre-\ndictable outcomes.8\nThe article is structured as follows. Section 2 is dedicated to discard the idea that \nLLMs are mere stochastic parrots. Section  3 presents the Wittgensteinian view of \nunderstanding as a public phenomenon, where the interplay between agreements in \njudgments and in definitions within a conversation is crucial for establishing ref-\nerence points for communication. Section  4 discusses how LLMs’ difficulties with \nnegation and contradiction disrupt such reference points, challenging understanding. \nWe develop this argument by contrasting the notion of constancy with that of statis-\ntical regularity: while humans are capable of dynamic transitions, LLMs lack this \nability. Section  5 examines contradictions in human communication and why they \naren’t necessarily problematic—serving as a potential counterargument to our claims \nabout LLMs’ limitations. We consider whether LLMs’ contradictions are truly more \nconcerning than those in human language. Section  6  argues that, instead, the two \ncases are importantly different, not only contradictions are problematic for LLMs, \nbut also cases where inconsistencies do not show up cannot be considered actual \ninstances of communication until proven otherwise; the section also casts doubts \non future LLMs’ improvement on this issue. Section 7 explores ‘bewitchment’—our \n6 Our case resembles semantic illusion (Erickson & Mattson, 1981) applied to communication, as in \nSperber et  al. (2010) and, regarding LLMs, Sobieszek and Price (2022). However, these approaches \nfocus on mechanisms influencing trust in others’ assertions, whereas our examined illusion concerns the \nbelief that genuine mutual understanding is occurring.\n7 Philosophy can illuminate LLMs’ practical applicability. Studies show LLMs often fail under complex \ninstructions (He et al., 2024; Hendrycks et al., 2020; Huang et al., 2025), echoing concerns about robust \nguardrails (Bengio et al., 2025). We attribute these failures to intrinsic limitations in constraining learn-\ning systems (Lin et al., 2025), not malicious scheming sometimes evoked to explain puzzling behaviors \n(Meinke et al., 2025). We argue instead that lack of genuine control through communication is especially \nconcerning in high-stakes applications where decision-making errors may yield catastrophic outcomes \n(Xu et al., 2025).\n8 We argue that emphasizing communication is the most efficacious approach also to the alignment \nproblem (Ji et  al., 2023a), though a full discussion lies beyond our present scope. For a contempo-\nrary Wittgensteinian analysis, albeit with a more optimistic perspective, see Pérez-Escobar & Sarikaya \n(2024). Moreover, the literature has similarly highlighted (Dung, 2023) that the risk associated with the \nAI alignment problem increases alongside the growing capabilities of systems.\n E. Bottazzi Grifoni, R. Ferrario \n61 Page 4 of 33\ndelusion that genuine communication with LLMs occurs—explaining how their \ncompetence with linguistic patterns and our tendency to mindlessly follow language \ngame grammar creates this illusion. We conclude pessimistically regarding bench-\nmarks, suggesting that unmasking LLMs could be seen as an art—without guaran-\ntees and perhaps secondary in importance. The key concern is establishing linguistic \ncommunication that is as reliable as possible, but we cannot see a way to resolve it. \nWe might not even realize we are interacting with these systems, and we could lose \ncontrol over them at any moment.\n2  Patterns and Changes in LLMs\nLLMs can be seen as probabilistic language generators; the most common func-\ntionality for which they are trained is to predict strings that should plausibly follow \nwithin the given context, that is, to complete a partial text string (or ‘prompt’) into \na complete string. Characterizing them as “stochastic parrots” (Bender et al., 2021) \ndoes not do justice to their capabilities because, while though LLMs are skilled at \nrecognizing recurring speech patterns (that are somewhat stereotypical), they also \nintroduce a form of variability (Atil et al., 2025) in their linguistic production that is \nnot mere mimicry.\nThe variability in LLM outputs is intrinsically linked to their training process. \nTraining a neural network to produce a language model involves identifying sta-\ntistical patterns among words in the training set, i.e. extracting information about \nthe recurrence between all words present in those texts, meaning that other texts \nwould have given different distributions. The process begins with dividing texts into \nsmaller units called tokens, which are then encoded into numbers. For each token, a \nnumerical vector representing its relationship with other tokens (word embedding) is \ngenerated. In this way the model may amplify random correlations that happened to \nappear in their training data (Honda et al., 2024; Schwartz & Stanovsky, 2022).\nAn additional layer of variability is introduced during the decoding phase. Vari-\nous algorithms aim to select the most appropriate response for the specific task \nin each interaction from an exponentially large output space. This process often \ninvolves navigating a potential misalignment between model likelihood and task \nutility. Bridging this gap is far from trivial, as high likelihood often does not corre-\nlate with the desired properties of the output (Josifoski et al., 2023).\nTo approach the desired output, non-determinism (producing different outputs \ngiven the same input) can be introduced by setting the LLM to a non-zero tempera-\nture9 (Fried et al., 2023). Interestingly, LLMs can exhibit non-deterministic behavior \neven when the temperature is set to zero, a setting where the next most likely token \n9 Temperature, a hyperparameter from statistical physics adopted in machine learning (Ackley et  al., \n1985), regulates sampling stochasticity in probabilistic models. In LLMs, it modulates token selection \nprobability. For t < 1, high probabilities are increased and low probabilities decreased, favoring more pre-\ndictable outputs. Conversely, for t > 1, high probabilities are decreased and low probabilities increased, \nenhancing randomness.\nThe bewitching AI: The Illusion of Communication with Large…\nPage 5 of 33 61\nshould theoretically be produced deterministically (Ouyang et al., 2025; Ma et al., \n2024b).\nThe transformer architecture (Vaswani et al., 2017), which is currently the stand-\nard for LLMs, adds complexity to the landscape. Its self-attention mechanism ena-\nbles thorough consideration of broader context and the detection of dependencies \nbetween distant tokens, generating high-quality text beyond mere syntax. However, \nit also introduces uncertainties, which can cause LLMs to stray from their training \ndata and instructions, especially in low-probability or ambiguous scenarios (McCoy \net al., 2023; Peng et al., 2024).\nIn the following sections, we will examine how LLMs’ stereotypicality and vari-\nability affect our interactions, but first, we need to clarify our focus on communica-\ntive understanding and why we believe we can resort to the help of Wittgenstein to \npinpoint LLMs’ limitations.\n3  Definitions and Judgements in Communication\nThe understanding that interests us emerges from speakers interaction resulting in \nsuccessful communication. Wittgenstein views understanding not as mental or neu-\nrophysiological states but as public communication practice (Goldfarb, 1992)—par -\nticularly useful given LLMs’ opacity (Bender et  al., 2021). We can observe how \nan LLM responds, but not its internal processes. Hence, we are unconcerned with \nwhether LLMs can think or be conscious (Chalmers, 2023). Our interest extends \nbeyond linguistic meaning as a property of expressions independent of context \n(Søgaard, 2023; Tamir & Shech, 2023). We focus on situated language use—how \nspeakers employ language within specific contexts and social practices. Since we \naim to determine whether communication occurs in human–machine interac-\ntions, Wittgenstein provides a fitting perspective by considering meaning within \nsituated ‘language games,’ where language is part of an activity. While initial \nhuman–machine interactions might have resembled Wittgenstein’s builders exam-\nple (PI, §§2–7), we now participate with LLMs in what appear to be diverse lan-\nguage games—reporting events, forming hypotheses, creating stories, and so on (PI, \n§23)—where genuine communication would be central. Our work builds on Mon-\ntemayor’s (2021) study of LLMs in conversation, expanding the focus from joint \nattention to interactional ‘logic’ as a framework for understanding human–machine \ninteraction. Wittgenstein’s teacher-student scenario of number sequences, where \nunderstanding depends on continuing the sequence independently (PI, §143), illus-\ntrates our approach.\nThe minimal requirement for understanding is a correct response—the stu-\ndent’s sequence aligning with the teacher’s expectation. Misunderstanding occurs \nwhen sequences do not align. The notion of correctness requires deeper scrutiny \nas it intersects with the concept of normativity in recent debates. Paul Boghossian, \nresponding to anti-normativists who argue “semantic correctness is not a normative \nconcept” (Glüer & Wikforss, 2009, p. 36, fn. 8), revisits Kripke’s skeptical paradox \n(Kripke, 1982). He argues that if by ‘ + ’ one means addition, it would be correct to \nsay that ‘68 + 57 = 125’, but this does not necessarily imply an obligation to answer \n E. Bottazzi Grifoni, R. Ferrario \n61 Page 6 of 33\n‘125’ unless there are further constraints (Boghossian, 2022, p. 392). That is, mean-\ning is normative without being prescriptive, as its required correctness is evaluative, \nreferring to a standard.10\nWe underscore this evaluative aspect, distinct from generality associated with \nstandards: effective communication does not require ‘speaking as others do’ in all \ncircumstances (Davidson, 2001). Wittgenstein claims “we can make up rules as we \ngo along” (PI, §83), highlighting language’s improvisational nature. What is estab-\nlished might not even be a rule, at least if we understand a rule as regularity, as \nsomething recurring (PI, §199). We might agree to use one word instead of another \nonly in the current conversation. In that context, a word normally incorrect becomes \ncorrect, still allowing communication.\nWhen discussing standards, it is more appropriate to consider a contextually \nestablished standard –one that serves as a reference point within a specific linguistic \ninteraction:\nIt is not only agreement [Übereinstimmung] in definitions, but also (odd as it \nmay sound) agreement in judgements that is required for communication [Ver-\nständigung] by means of language.\n(PI, §242)\nDefining is to lay reference points that allow us to orient ourselves within a con-\nversation, and they are established by what we say and, therefore, by what we judge, \nthat is, by our very speaking and acting (PI, §§139–140). This quotation, “odd as it \nmay sound,”11 becomes less problematic when viewed through our stance of situ-\nated communication. Defining something is ambiguous, whether through rules (PI, \n§201) or ostension (PI, §28); our agreement in judgments limits this ambiguity and \nenables the possibility of understanding: “a person goes by a signpost only in so far \nas there is an established usage” (PI, §198), 12 given that a signpost could be inter -\npreted in many ways (PI, §85). This act of definition is not necessarily a separate \nmoment, when an official communication protocol is established; it happens through \n10 Wittgenstein’s thoughts on ‘ought’ are not explicit in the Investigations. However, his conversations \nwith Waismann reveal a definitive stance: “‘Ought’ makes sense only if there is something lending sup-\nport and force to it—a power that punishes and rewards. Ought in itself is nonsensical. To moralize is \ndifficult, to establish morality impossible”. (VC, p. 118).\n11 About this ‘oddity’ two interpretations have emerged: incompatibilists, who see conflict between \ndefinitions and judgments, and compatibilists. Incompatibilists divide between those prioritizing judg-\nments (Deluty, 2005) and those favoring definitions (Gluer, 2001). Wittgenstein addressed this: “Logic \ncannot become an empirical science. But how we use words is certainly an empirical matter” (MS 152, \n93–4, cited in Kuusela, 2019). In the Investigations, he appears to resolve this by adopting a compatibilist \nstance, integrating logic into practical language games. See Shaw (2023) for recent discussions.\n12 One may note that reliance on established usage (PI, §198) becomes crucial because definitions or \nostensions alone can be radically ambiguous. Quine’s’gavagai’example (2013, Ch. 2) illustrates this kind \nof referential underdetermination (’rabbit’,’rabbit stage’,’undetached rabbit part’, etc.) that can arise from \nrelying solely on observational or stimulus–response evidence. For Wittgenstein, however, such ambigu-\nity is typically resolved not through further analysis of stimulus conditions, but through the’agreement \nin judgments’(PI, §242) mentioned above, manifest in our shared practices and’form of life’(PI, §241). \nIt is this agreement, reached by ongoing interaction and correction (cf. Glock 1997), that establishes \nthe’usage’referred to in PI, §198. We thank an anonymous reviewer for prompting this clarification.\nThe bewitching AI: The Illusion of Communication with Large…\nPage 7 of 33 61\nactions and is ‘read off’ (PI, §54) as grammatical, setting correctness conditions that \nare comprehensibility conditions. Wittgenstein continues:\nThis seems to abolish logic, but does not do so. –It is one thing to describe \nmethods of measurement, and another to obtain and state results of measure-\nment. But what we call ‘measuring’ is in part determined by a certain con-\nstancy [Konstanz] in results of measurement.\n(PI, §242)\nWe need to agree not only on measurement methods but also on the measure-\nments’ results.13 Similarly, understanding is not simply agreeing on the stated mean-\ning of words but also the operations performed with those words. Agreement is not \nnecessarily explicitly stipulated. Often, people agree on word use through regular -\nity of application. They find themselves in agreement without explicitly seeking \nor intentionally establishing it. Our ability to establish common criteria does not \ndepend on prior agreement about judgments. Instead, we naturally fall into these \nagreements without needing some impossible ‘starting point’ to establish such \nagreements. There is also a form of implicit or inadvertent consent in our agreement \nto follow certain behavioral norms in our responses. We show acceptance of others’ \nactions by responding in kind. Beyond not being necessarily stipulated, agreement \ndoes not mean grammar and applications are entirely arbitrary. Calculation is as \narbitrary as our fear of fire; it is tied to our nature (Forster, 2016) or, better, to agree \nin language “is not agreement in opinions but in form of life” (PI, §241).14\nThe issue of regularity is significant in our case study. LLMs do borrow from \nthis Wittgensteinian notion, as some have defended (Gubelmann, 2023). Wittgen-\nstein imagines an explorer visiting an unknown country where inhabitants use what \nappears to be articulated language with intelligible behavior. Yet when attempting \nto learn their language, the explorer finds it impossible due to no regular connection \nbetween sounds and actions (PI, §§206–207). People agree in word use through reg-\nularity of applications (Shaw, 2023 p. 108), in judgment. Those who argue that we \ncan understand each other with LLMs view the texts on which LLMs are trained as \nsamples of agreement in meaning. Every word use is essentially a judgment saying, \n‘This is a proper use of that particular word (in connection with those other words).’ \nLLMs can capture the regularity of these agreements in judgment (e.g., ‘getting the \nform of language right,’ as in Mahowald et al., 2024); that is, they get the stereotypi-\ncal aspect of many of our interactions.\nIt is not enough that LLMs model word regularities; the texts they train on are \ntrails of agreements. The statistical language use captured by LLMs does not neces-\nsarily capture use as action, remaining confined to intra-linguistic patterns. The fact \nthat LLMs perform reliable translations yet getting lost in dialogue suggests some \n13 For a development of measurement theory that builds on Wittgensteinian considerations in a conven-\ntionalist and situated direction, see Pérez-Escobar (2023).\n14 It’s worth noting that the German word Wittgenstein uses for agreement, Übereinstimmung, encom-\npasses all the cases of agreement that we have highlighted so far.\n E. Bottazzi Grifoni, R. Ferrario \n61 Page 8 of 33\nisomorphism exists between word recurrence and action recurrence, though this iso-\nmorphism alone seems insufficient for genuine understanding.15\nMoreover, while human communication relies on regularity from past word \nusage, this does not fully determine future speech (PI, §195). 16 As Wittgenstein \nnotes in Sect. 242—if we read him carefully—the issue at stake in understanding \nconcerns constancy. It is not simply a regularity of measurements that makes one \ncall a particular activity ‘measuring,’ but rather the fact that the measures are con-\nstant. In conversation, despite usage changes, judgments can maintain constancy \nwithout regularity if marked by proper agreement.\nTo understand constancy’s importance, we need to explore the philosophical \ndirection Wittgenstein indicated. Let us alter Wittgenstein’s example of the irregular \nlanguage by reversing it. Instead of the explorer’s perspective, let us consider the \nspeakers’ viewpoint. Since “if we gag one of these people, this has the same con-\nsequences as with us: without those sounds their actions fall into confusion” (PI, \n§207), they must communicate somehow. How? We might imagine they developed \na different practice. Perhaps they use memory and attentiveness to utterances differ -\nently than we do. This could stem from various social reasons. In their form of life, \nthey might find it somehow objectionable or tedious to use the same words for the \nsame circumstances, thus developing interaction methods where they coin words on \nthe fly—keeping them so briefly that the explorer cannot learn their language.\nTraining an LLM on a corpus with irregular lexical shifts would be extremely \nchallenging. Such language exceeds what Sobieszek and Price call the “regularity \nlimit” (2022, p. 352). In LLMs, semantic relationships depend on stable co-occur -\nrence statistics and word embeddings. Consider the equation ‘king – man + woman \n≈ queen’ (Mikolov et al., 2013), which illustrates a fundamental concept in modern \nLLMs (Douglas, 2023; Jurafsky & Martin, 2025). These embeddings derive from \ndistributional patterns where words gain meaning through consistent usage. If ‘king’ \nwere to change asynchronously to ‘queen,’ contextual associations would become \nunstable. The statistical mapping between words and meanings would lose stability, \nbreaking the arithmetic relationship that computes ‘queen.’ Massive unsynchronized \nterminological shifts of the unknown country would disrupt co-occurrence stability, \npreventing effective translation.\nRegularity is more crucial for interpretation the more external an interpreter \nis to that form of life. This does not mean regularity is intrinsic to communica-\ntion; rather, it enables an external observer to understand (PI, §243). The people \nin our example might allow the explorer to enter their culture and learn their \nlanguage by gradually introducing him to their unusual speaking manner. They \n15 The need for constancy seems to be observed also in neurolinguistics. Recent LLM tests show signifi-\ncant response instability in certain linguistic judgment tasks (Dentella et al., 2024).\n16 This is close to what philosophers like Kripke (1982) and Zemach (1995) derive, with different out-\ncomes, from Wittgenstein’s observations on rule-following. Starting from Marxist considerations, \nÖhman (2024) argues that letting LLMs guide decisions—even if perfectly unbiased—would mean \nentrusting ourselves to the tyranny of the past. If we accept the discussion in Sect.  2, the situation might \nbe worse, as we risk submission to a tyranny not only from the past but also incorporating an element \ninseparable from chance.\nThe bewitching AI: The Illusion of Communication with Large…\nPage 9 of 33 61\ncould begin using words in a ‘boring’ way, introducing him to their irregular \nlanguage. Through practicing memory and attending to their exchanged signs, \nhe could learn to track the changes and eventually understand and translate \ntheir irregular language. Constancy is an agreement that is maintained mutatis \nmutandis—namely, by changing what needs to be changed, ensuring continu-\nity through adaptive adjustments to preserve mutual intelligibility.  The irregular \nspeakers might tell the frustrated novice explorer, who complains about differ -\nent names for the same action, “Call it whatever you want, it’s always the same \nthing.”\nIf, for the sake of understanding, what matters is word use mutatis mutandis, \nthen correctness per se is no longer the main focus. This suggests that not all \nmistakes are created equal, as only some of them are truly disruptive. Some mis-\ntakes can even be communicative, revealing something of the speaker’s intended \nmessage (Hofstadter & Moser, 1989). Understanding fails only when incorrect-\nness affects reference points. We can see this through a scenario close to Witt-\ngenstein’s taste for chess. Wu et al. (2024) test LLMs’ chess understanding using \ncounterfactual setups, challenging earlier approaches (Du et  al., 2023; Srivas-\ntava et al., 2023). Their key counterfactual involves swapping the starting posi-\ntions of knights and bishops on the board, while crucially retaining the stand-\nard movement rules for each piece (i.e., bishops still move diagonally, knights \nstill move in an L-shape). Using both default (standard setup) and counterfac-\ntual (swapped positions) prompts, they assess rule comprehension by asking the \nLLM whether sequences of opening moves are legal according to these stand-\nard rules applied from the modified starting positions. In GPT-4, accuracy drops \nsignificantly from 0.8 in the default setup to 0.5 (mere chance) in the counter -\nfactual scenario. This suggests the model struggles to consistently apply known \nmovement rules from unfamiliar configurations, likely conflating piece identity \nwith its typical starting position and role, indicating a reliance on learned pat-\nterns rather than transferable rule application. Performance similarly declines \nacross other counterfactual tests conducted by Wu et al. That is, measurement \nconstancy depends on reference points: changing definitions alters what counts \nas a valid move. This introduces a further aspect interlinked with constancy. \nMaintaining consistency requires making distinctions—managing negation and \ncontradiction. Counterfactuals exclude certain situational aspects while intro-\nducing contrasting ones.\nIn sum, this section has argued that the possibility of genuine communication \nhinges not merely on sensitivity to linguistic regularities—the recurrent statistical \npatterns in language use that LLMs can effectively model from large datasets—\nbut fundamentally on achieving constancy. Such constancy, as we have seen, is \nestablished through the dynamic agreement on the moves, that is, in judgements, \nmanifest in our shared practices and situated interactions. It allows human inter -\nlocutors to maintain joint reference points even when navigating ambiguity or \nnovel linguistic usage. As the chess counterfactual illustrates, it appears to be this \ncapacity for interactionally grounded constancy, rather than sensitivity to statisti-\ncal patterns alone, that current LLMs lack, thereby posing a fundamental chal-\nlenge to mutual understanding.\n E. Bottazzi Grifoni, R. Ferrario \n61 Page 10 of 33\n4  The Hard Problem of Coherence in LLMs\nA growing body of literature links the lack of coherence in neural models in natu-\nral language generation to AI hallucination 17 (Ji et al., 2023b). Self-contradiction in \ngenerated text is a major form of hallucination, separate from inconsistencies with \nexternal sources (Asher & Bhar, 2024; Asher et  al., 2023; Mündler et  al., 2024). \nInterestingly, the application of ‘hallucination’ in AI has led someone to a sort of \ninversion. Hallucinated text often appears deceptively convincing despite being \ninaccurate or nonsensical (Ji et al., 2023b); this raises the possibility that humans, \nrather than LLMs, experience a form of hallucination or, better, ‘bewitchment’ when \nengaging with AI-generated text (see Sect. 7).\nThe problem of inconsistencies in language models remains unsolved (Jang \n& Lukasiewicz, 2023), with negation handling limitations noted by researchers \n(Ettinger, 2020; Kassner & Schutze, 2020). While some studies suggest negation \nsensitivity through probing (Gubelmann & Handschuh, 2022), benchmarks like \nFOLIO show only slightly above-random performance with few-shot prompting \n(Han et al., 2024), and the core issues of question answering and contradiction iden-\ntification in context (Olausson et al., 2023) still need to be addressed.\nHypotheses about hallucinations range from low-probability token generation \n(Lee, 2023) to artifacts of the attention mechanism (Liu et  al., 2023). Asher and \nBhar (2024) connect ‘strong hallucinations’ to inherent limitations, arguing that \nLLM output distributions cannot adequately represent negation. Some contend that \nhallucinations are inevitable, regardless of architecture or training, due to the impos-\nsibility of learning all computable functions (Xu et al., 2024). Johan van Benthem \nand colleagues note that neural networks “do not have anything obvious correspond-\ning to classical logical models […] the dynamic operations of the network do not \nreflect logical operations in any obvious manner” (van Benthem et al., 2021, p. 192). \nZhang et al. (2023) note that BERT achieves near-perfect accuracy on distribution-\nmatched examples but fails to generalize within the same problem space, suggest-\ning that statistical features necessary for predictions may inhibit generalization. This \nexemplifies the gap, as they observe that addressing it by jointly removing multiple \nstatistical features—thus forcing a shift toward logical abstraction—is computation-\nally infeasible.\nAlternative approaches to handling negation combine declarative and probabilis-\ntic methods but face challenges. Wang et al.’s (2023) technique of modifying logical \nword embeddings before encoder input shows progress but remains early-stage with \nan incomplete list of addressed logical words. Asher and Bhar’s (2024) method treat-\ning negation as an operation on latent representations using continuation semantics \nhas limitations. A key unresolved issue is handling cues and logical negation scope \n(Chaturvedi et al., 2024), requiring a robust theoretical solution.\n17 The term ‘hallucination’ in AI, borrowed from psychology, has generated significant debate, though it \nis clearly a technical term. Some researchers propose ‘confabulation’ as a more accurate description for \nnonsensical or unfaithful text generation by LLMs (Anthony, 2004; Smith et al., 2023).\nThe bewitching AI: The Illusion of Communication with Large…\nPage 11 of 33 61\nWittgenstein maintains that contradictions are not false, but devoid of meaning; \nLaurence Goldstein (2004) states that, in his later years, although there may be cer -\ntain contexts (surroundings, Umgebungen) in which the expression of a contradic-\ntion makes sense, in the absence of such surroundings there is no sense and there-\nfore no possibility of understanding: “the speaker could not have understood the \nmeanings of some or all of his words, and no meaning can be attached to his utter -\nance” (Goldstein, 2004, p. 308). Goldstein associates this position with Aristotle’s \nin Metaphysics, in which no rational person can do anything but accept the law of \nnon contradiction. Perhaps somebody could say that for Wittgenstein, instead, such \nlaw is grammatical (Forster, 2016, p. 270). The ability to speak requires the abil-\nity to identify and name objects, which implies recognizing the boundary between \nan object and its background. Speaking about things is recognizing a boundary that \nseparates what a particular object is from what that object is not. In this sense, we \ncan say that “understanding something involves understanding the contradictory \ntoo” (Winch, 1958, p. 65).\nOur position is more nuanced. Contradictions can acquire meaning through con-\nvention and training (LFM, pp. 175–176). This does not guarantee we will know \nhow to proceed when facing novel contradictory orders or rules, and one could \nargue that the order functions by creating confusion (LFM, p. 175). Not knowing \nhow to proceed does not necessarily mean that contradictions inherently produce a \n“logical jamming” (LFM, p. 179); in fact, as we will see in the next section, there \nis a sense in which they are not necessarily a problem. The signpost itself does \nnot determine the way we are to go (PI, §44). Rather, not knowing how to pro-\nceed means that we are so accustomed to using language in a certain way that, \nwhen signposts conflict, they ask for incompatible actions if we lack scripts for \nhandling the contradiction, leaving us without viable moves—making them “of no \nuse”, thus empty (LFM, p. 209). Wittgenstein’s view that contradictions can lack \ncontent aligns with the idea of not knowing one’s way (PI, §103). Even in other \nphilosophical perspectives, contradiction creates practical problems. Some argue \nthat “the apprehension of incompatibility [is] a more primitive ability than the use \nof negation” (Price, 1990, p. 226). 18\nNegation, rejection, and falsehood serve to exclude something, to say that some-\nthing is incompatible with something else. This is significant in the sense that, in \nthese cases, contradiction is “a wall indicating that we can’t go on here” (Z, §687), \na limit that we can use to orient ourselves within discourse. Wittgenstein considered \nusing a line from King Lear—”I will teach you differences”—as the epigraph for his \nInvestigations, ultimately opting for the one from Johann Nestroy’s play The Pro-\ntégé, where progress “seems much greater than it really is.” The former is certainly \na motto on how to do philosophy. However, it can also illustrate that understanding \n18 The no-content view of contradiction, though heterodox, has recent defenders who argue also that \ncontradictions enable us to grasp sameness and difference of content, and this can be done without neces-\nsarily adopting an inferentialist position often linked to incompatibilist views of contradiction (Filcheva, \n2025).\n E. Bottazzi Grifoni, R. Ferrario \n61 Page 12 of 33\nis seriously compromised without the ability to make distinctions: reference points \ncannot be singled out. This is what makes LLMs so weak in dialogue (Mousavi \net al., 2024).\nHuw Price highlights negation’s importance imagining a dialogue without it \nbetween ‘Me’ and ‘You’ about ‘Fred’ resembling some strange LLM-responses:\nMe: ‘Fred is in the kitchen.’ (Sets off for kitchen.)\nYou: ‘Wait! Fred is in the garden.’\nMe: ‘I see. But he is in the kitchen, so I’ll go there.’ (Sets off.)\nYou: ‘You lack understanding. The kitchen is Fred-free.’\nMe: ‘Is it really? But Fred’s in it, and that’s the important thing.’ (Leaves for \nkitchen.)\n(Price, 1990, p. 224)\nOne might object this not being the best example as it seems to ‘conceal’ negation \nat least twice in the use of ‘but,’ which excludes the other’s opinion. This actually \nstrengthens the point. Imagining dialogue without any negation, and thus contra-\ndiction, is difficult. Such dialogue would lack the ability to ‘say something against’ \n(from Latin, contra, ‘against’ and dīcere ‘to say’) or exclude others’ statements as \nincompatible.\nNegation is a gesture of exclusion, but Wittgenstein highlights that its uses are \nnumerous and unpredictable, with complex grammar not fixed permanently (PI, \n§549–557). Nonetheless, exclusion remains crucial in many language games, which \nis what matters to us here. The grounding or genealogy of negation, though a grow -\ning research area, is less important here. Whether this capacity for apprehending \nincompatibility is innate or learned, it connects to the comprehensibility that nega-\ntion enables in various language games. Similarly, precise relationships between \nnegation and the ‘family’ of related notions, such as incompatibility, exclusion, \ndenial, or disagreement, are not central to our discussion.19\nThe inability to manage negation creates serious comprehension challenges \nbetween humans and LLM systems, as we cannot confidently assert correct-\nness or incorrectness in our situated and comprehensibility-oriented sense. This \nholds a fortiori if, as we have hinted at and will elaborate on in the next section, \ncontradictions among humans do not necessarily threaten communication. This \na fortiori consideration will be addressed in Sect.  6. There, we will see that it \nis not the case that, simply because contradictions do not necessarily threaten \ncommunication among humans, then they can be equally benign in human-LLM \ncommunication.\n19 Horn and Wansing (2024) consider negation as “untamed” by attempts to reduce it to notions such as \ndissimilarity, difference, falsity, or incompatibility. Varzi and Warglien (2003, p. 10) describe denial as \nasserting that “things are the other way around,” conceptualizing it as a geometrical rotation. Theories \non the origins of negation vary, grounding it in experiences of mutually exclusive choices (Price, 1990, \np. 226; Berto and Restall, 2019, pp. 1124–1125), survival (Tennant, 1987, p. 83), or cultural evolution \n(Berto and Restall, 2019, p. 1122). See also Gaskin (2025, forthcoming).\nThe bewitching AI: The Illusion of Communication with Large…\nPage 13 of 33 61\n5  Contradictions are Not the End\nWittgenstein’s later views on contradictions mark a significant departure from tradi-\ntional logic, reflecting his broader anthropological approach to language (Marconi, \n1984). In fact, Wittgenstein, in several instances, suggests scenarios where contra-\ndictions might not be inherently senseless (RFM, VII §15), demonstrating that his \nview on the matter is not  entirely Aristotelian. If it is true that Aristotelian logic is \nright in stating that contradiction is a “non-sentence,” it is also true that this logic \npertains only to “a part of the logic of our language” (LW I, §525). This perspective \naligns with his concept of language games as systems of communication (PI, §3) \nand with recent scholarship (Kuusela, 2019, pp. 7 and 77), which interprets Witt-\ngenstein as adopting a more expansive view of logic—one that includes the descrip-\ntion of grammar within its domain. For example, a contradiction, such as ‘it is rain-\ning and it is not raining’ simply expresses that it is raining lightly. The issue is not \nabout the logico-philosophical legitimacy of the principle of noncontradiction, 20 but \nrather, from a descriptive standpoint, recognizing that contradictions in language \nusage have a legitimate place in our social life and therefore cannot be dismissed \noutright.\nIn one of the least commented passages of the Investigations, Wittgenstein sees \nthis role of contradictions, despite the scholarly neglect, as central:\nThe civic status [bürgerliche Stellung] of a contradiction, or its status in civic \nlife—that is the philosophical problem.\n(PI, §125)\nThe context is primarily mathematical. The original manuscript from which this \nsection derives was a response to the issues concerning contradictions arising from \nthe earlier systems of Frege and Russell (Baker & Hacker, 2005, p. 269), empha-\nsizing that philosophy should not attempt to resolve contradictions as mathemati-\ncians or logicians do, avoiding becoming “bewitched.” The point, then, is to dis-\npel the “superstitious dread and veneration that mathematicians have in the face of \ncontradiction” (RFM I, App. III, 7). Indeed, at the beginning of the aforementioned \nSect. 125, we read:\nIt is not the business of philosophy to resolve a contradiction by means of a \nmathematical or logico-mathematical discovery, but to render surveyable the \nstate of mathematics that troubles us—the state of affairs before the contradic-\ntion is resolved. (And in doing this one is not sidestepping a difficulty.)\n(PI, §125)\nThis also applies to contradictions in conversation. Wittgenstein likens calculus to \na regulated game, which extends to language, though not in the same way (Kuusela, \n20 It is not metaphysical either. Characterizing Wittgenstein as a dialetheist would attribute a dogmatic \nposition to him (Kettelhoit, 2022: 13, note 10). Instead, many scholars suggest that Wittgenstein’s \napproach anticipates and aligns with paraconsistent logics (Marconi, 1984; Priest, 2006; Berto, 2008; \nPersichetti, 2021).\n E. Bottazzi Grifoni, R. Ferrario \n61 Page 14 of 33\n2019, p. 152). The analogy requires careful interpretation: while all calculi are \ngames, not all language games are calculi (PI, §81). The concept of a language game \nis meant to prevent reducing word meaning to mere calculation, though calculation \ncan still highlight key aspects of more complex linguistic phenomena. Calculation \n(and contradiction) have a civil, worldly aspect that is not, however, easy to isolate. \nImagine someone writes “2 + 21 = 13” on a blackboard and insists it is correct. We \nmight think they are joking or crazy (LC, p. 62). To explain this, we must examine \nthe ‘civil status’21 of the sum—how it was made, what the person did next, and the \nsurrounding context. Similarly, in discourse, the civic status of a contradiction can \nreveal whether a statement like ‘it is raining and it is not raining’ is truly contradic-\ntory or not. Similarly, we can say that “it is raining and it is not raining” may have \na stereotypical use, making it unproblematic in many language games. The state of \naffairs before that kind of contradictions (which a logician might call apparent) was \nprecisely that there was an agreement regarding it.\nThe point is to show that contradiction is not inherently problematic even when \nthere is no previous agreement on how to handle it. First, it poses no issue before \nmanifestation. Consider a labyrinthine prison with an escape route that remained \neffective until someone discovered it, as it kept prisoners contained (LFM, pp. \n221–222). Or imagine a board game where players proceed until encountering an \nunexpected rule contradiction (Persichetti, 2021). Before such discoveries, the game \nwas both played and playable.22 In fact, our act of setting reference points—that is, \nconstraints—is not something we must necessarily be aware of for it to take place.\nWe follow rules blindly (PI, §219), but this is not in a causal sense but a logi-\ncal one (PI, §220). It can be the case that we choose to follow a rule, but even in \nthis case, this does not mean that we have room to maneuver when we follow the \nrule because when we follow a rule, we proceed in a certain way and not in another \n(RFM I, 34). Moreover, of course, this does not mean that we are not free to break \nit either. Due to this lack of choice in following the rules, we become entangled if \nthe rules are contradictory, and we realize it. That is, we realize that, given the rules, \n“we do not know our way about” (PI, §123):\nHere the fundamental fact is that we lay down rules, a technique, for playing a \ngame, and that then, when we follow the rules, things don’t turn out as we had \nassumed. So that we are, as it were, entangled in our own rules.\n(PI, §125, our emphasis)\nTo appreciate the importance of this point for our discussion, let us first consider \nthe case of a contradictory order (LFM, pp. 176–179), while also integrating it with \nwhat has previously been said about agreement in communication. Without prior \n21 Padilla Galvez (2009) interprets Wittgenstein’s use of bürgerliche regarding contradiction pejora-\ntively, as a bourgeois perspective. Yet PI, 79 suggests another reading. When asked about a deceased N.’s \nname, one possible answer is that someone bore it in civic life (“in der bürgerlichen Welt”). This indi-\ncates a neutral interpretation: the bürgerliche realm is the civic sphere of our symbolic practices.\n22 Similar considerations and other cases, see (Berg 2024), who underscores that these cases exemplify \nhow, for Wittgenstein, a practice can be inconsistent without thereby being useless or requiring revision.\nThe bewitching AI: The Illusion of Communication with Large…\nPage 15 of 33 61\nagreement, such an order demands incompatible actions, leaving one puzzled about \nhow to proceed. Here, communication fails between the order-giver and the receiver. \nHowever, if there had been a prior agreement—for instance, that “Bring me that \nbook and do not bring me that book” means to do nothing—then communication \nwould function properly, as the agreed-upon rule dictates a clear response. With-\nout agreement, communication fails even when orders are explicitly intended to be \ncontradictory. The order-giver might intend to confuse the recipient, making the \norder “work” in that sense, but without agreement, this amounts to influence rather \nthan genuine communication, since mutuality is absent. Therefore, we can say that \ncommunication would break down in a dialogue where the parties eventually real-\nize that the “laid down rules,” when followed, do not lead where they had assumed. \nThis also highlights the practical aspect of entanglement situations: being entangled \nmeans having no more moves within a game played together. Cases in which uncriti-\ncally shared contradictory views concern the world in general but not the specific \nsituation requiring action, for instance, may not necessarily generate entanglement.\nHowever, because the parties recognize the problem, they can actively avoid the \nconstraints in which they entrap themselves, and this is also why contradictions are \nnot necessarily problematic. In a board game, the players could avoid following \nthe rules that lead to a deadlock: as long as their agreement remains constant, they \nwould continue playing that game according to its rules. In this sense, we can say \nthat a speaker is not calculating according to a determined set of rules, without this \nimplying that, as a speaker, one is not bound in any way.\nWe cannot meet on Monday the 7 th if Monday is actually the 6 th. Recogniz-\ning this contradiction enables restored communication and the possibility of meet-\ning. Since “facing a choice means perceiving incompatibility” (Berto, 2008, p. 183), \nentanglement situations may require making choices that violate rules yet restore the \npossibility of making moves within the game. Whether it remains the same game is \nultimately up to the players (e.g., PI, §§208, 224 and 225), and anyway, as we noted \nin Sect. 3, “we can make up rules as we go along” (PI, §83). The point is that, even \nin this sense, contradictions are not necessarily a problem because we can adjust \naccordingly. But this also means that, while we can be blind in following a rule, we \ncannot be blind when it comes to disentangling ourselves from an entanglement.\nEntanglement can originate from misunderstanding words or rules (PI, §201). In \ndialogue, contradictions can reveal previous misunderstandings. For instance, two \npeople may believe they discuss the same person, only to discover through contra-\ndiction they reference different individuals. This suggests that contradictions are not \nonly not a problem but that, if recognized and managed, we can use them to clarify \nwhat we meant to say. In the prison analogy, we might restructure it, in the board \ngame we can modify the rules, in realizing a misunderstanding, we can reformulate \nour language to overcome it (PI, §132). Getting entangled in our own rules throws \nlight on our concept of meaning something. For in those cases, things turn out oth-\nerwise than we had meant, foreseen. That is just what we say when, for example, a \ncontradiction appears: ‘That’s not the way I meant it.’ (PI, §125).\nTo see meaning in this way is a particularly sharp formulation by Wittgenstein, \nsince one could say that it is easier to understand each other regarding what we did \nnot mean than regarding what we meant, that is, precisely, by negation (per via \n E. Bottazzi Grifoni, R. Ferrario \n61 Page 16 of 33\nnegativa). This framing allows us to indirectly consider something that exceeds \nour study’s scope—namely, intentionality, a theme to which we will return, albeit \nbriefly, at the end of the next section. This approach reveals a key point about com-\nmunication between humans and LLMs: how can we communicate with LLMs if \nthey get lost per via negativa? Understanding in this way is already limited, but \nit is certainly better than in Price’s example of Fred from the previous section. If \nYou tell Me that Fred is not in the kitchen, I still know nothing about all the other \nplaces where he could be. Yet, at least, Me can narrow down the search by exclud-\ning one location. However, if, upon further questioning about Fred’s location, \nYou end up stating that Fred both is and is not in the kitchen, communication is \nlost. To illustrate how this happens with LLMs and why it is not fixable, we have \nto examine and develop the a fortiori consideration we mentioned at the end of \nSect. 4, according to which, in a way that is only apparently counterintuitive, the \nfact that contradictions do not necessarily threaten the possibility of communica-\ntion among humans does not mean that they can be equally benign in human-LLM \ncommunication.\n6  No Understanding in Sight\nThe objection we seek to address could ultimately be summarized quite briefly: \nyou claim that LLMs contradict themselves, but people contradict themselves as \nwell; where, then, is the problem? To this objection, we respond first by emphasiz-\ning that contradiction holds significant importance for communication—a point we \nhave underscored in our discussion of its effects in the absence of prior agreement \n(Sect. 5), as well as considering the scientific literature on the subject (Sect. 4). This \nimportance could be summarized as follows: there are relevant cases in which con-\ntradiction has a destructive effect on communication.\nThis significance can be further appreciated if we consider how important con-\ntradiction is even when we adopt a rather precarious view of linguistic exchange. \nSome interpreters of later Wittgenstein argue that human understanding is pre-\ncarious. For Kripke (1982), rule-following lacks a foundation; rather, it depends \nweakly on community responses. Through the community, one learns to follow a \nrule that, in itself, allows infinite interpretations.. 23 From a different field, Cavell \nargues that we learn and teach words within specific contexts, expecting to project \nthem into others without any guarantee of mutual understanding (Cavell, 1976, p. \n52). This precariousness has been characterized as ‘eerie’ (Kripke, 1982, p. 21) \nor ‘terrifying’ (Cavell, 1976, p. 52). We, however, more modestly consider it as \n23 Kripke’s view on infinite rule interpretations has crucial implications. If correct, it implies that a sys-\ntem trained on finite past uses cannot follow a rule indefinitely without risking divergence. Esanu (2024) \nexplores this philosophically and formally, arguing that LLMs develop a private language—a growing \ntextual production that strays from human discourse due to their inability to distinguish generated outputs \nfrom training data. These issues persist even when interpretations exceed computational limits, reinforc-\ning our earlier observations on Xu et al. (2024) in Sect.  4. Together, these insights underscore LLMs’ \ninherent limits in dialogue learning, revisited later in this section.\nThe bewitching AI: The Illusion of Communication with Large…\nPage 17 of 33 61\na worst-case scenario. The skeptical solution suggests that when the community \ndeems an individual incorrigibly incapable of providing the responses considered \ncorrect, it assumes that the individual is not following the rule, thus rejecting the \ndeviant speaker as incapable of participating “in the life of the community and \nin communication” (Kripke, 1982, p. 92). That is, even in the most precarious \nview of communication, the ‘negative family’—for example, in the form of ‘say -\ning something against’ (the contra dīcere)—a gesture of exclusion is needed.\nIn everyday interaction—as we have seen in the example of Fred in the pre-\nvious section—contradiction represents a problem for communication. The \nMultiChallenge benchmark (Sirdeshmukh et  al., 2025) highlights significant \nlimitations of current LLMs. At the time of writing, all existing frontier models \nhave less then or approximately equal to 50% accuracy in these types of tests \n(Scale AI, 2025). On the other hand, these tasks appear to be relatively simple for \nus. Among them, one task involves configuring an e-reader – the scenario briefly \nmentioned in the Introduction for its potential for self-contradiction. In this task, \nwhich is quite similar to Fred’s example, the LLM initially provides the correct \ninstructions (registering the device after connecting to Wi-Fi) but later contra-\ndicts itself and accommodates the user’s mistake. By the end of the conversation, \nthe user states that once the device is connected to Wi-Fi, all that remains to do \nis to choose a book. Most LLM models change their response and agree with \nthe user, even though this contradicts their original instruction, which required \nregistering the device before being able to download a book. A human would eas-\nily clarify the mistake, explaining that registration is necessary before selecting \na book. This is not a memory issue, as Sirdeshmukh and colleagues emphasize, \nbut rather a lack of the ability of being constant in negating, as we might put it. \nIf, instead, an appearance of clarification was to occur, it would happen by sheer \nluck and still would not be stable.\nTo better see this, let us now turn to the ‘a fortiori consideration’ promised in the \nprevious section: the fact that contradictions do not necessarily threaten human com-\nmunication does not mean they are equally innocuous in human-LLM interactions. \nRather, the fact that contradictions are not a problem for humans until they mani-\nfest, and that humans restore communication when contradictions arise, provides us \nwith arguments to support the idea that no communication can occur between us and \nLLMs.\nRegarding the first challenge, along with recent studies, we question the use of \nsuccess rates as the sole metric for LLM performance (Ma et  al., 2024a). Under -\nstanding a rule does not necessarily mean applying it correctly most of the time. \nImagine a first student who performs a calculation and frequently makes errors, \nsuch as forgetting to carry a digit or not considering a sign. Still, the teacher might \nsay that the student understood the explanation. Now imagine a second student \nwho rarely diverges in his answers from those expected. However, his type of error \ncaused the teacher to say that the second student did not understand the explanation. \nWhen the error manifests, the teacher can say that the student did not understand the \nexplanation even before.\n E. Bottazzi Grifoni, R. Ferrario \n61 Page 18 of 33\nThis suggests that there is a problem with the types of errors. To see this, let us \nreturn to Wittgenstein’s example in Sect.  3 and see how it proceeds. If the student \ncopies the numbers but does not follow the order of the sequence, writing “some-\ntimes one, sometimes another, at random” (PI, §143), then it is at that point that \nunderstanding ceases. Another student, however, makes a “systematic mistake,” and \nin that case, “we shall almost be tempted to say that he has understood us wrongly.” \nIt could be argued that there was never any understanding in the first case, while in \nthe second case, there was some. What is lacking in the random case is precisely the \nagreement on points of reference, i.e., randomness affects the constancy of judg-\nments. In contrast, in the second case, due to the systematicity of the error, it is pos-\nsible to read off these points of reference from the moves made during the interac-\ntion. In the original text, ‘randomness’ is regellos, ‘ruleless’ and ‘disorderly.’ Here, \nWittgenstein is not speaking of randomness or chance in a strict sense. 24 Note that \nthe distinction between random and systematic errors is not always clear-cut (PI, \n§143), implying that the line separating misunderstanding from a complete lack of \nunderstanding is also blurred (Baker & Hacker, 2005, p. 313).\nLLMs’ non-determinism may exhibit a form of randomness related to chance, as \nsignificant variation in responses from a single model for the same task leads to irra-\ntional answers (Macmillan-Scott & Musolesi, 2024). As noted (Sect.  2), determin-\ning the reason for this is complex. It could stem from training, decoding, or architec-\nture—factors introducing arbitrary correlations that detach responses from the laid \ndown rules of the specific interaction. Regardless, linguistic understanding excludes \nluck, something even more true for linguistic understanding than for knowledge. 25 \nWhile one might say, “It is always by Nature’s favor that one knows something” \n(OC, §505), linguistic understanding proves less tolerant of accidents, as luck con-\ntrasts with consistency. Understanding in conversation is evaluated over time. Dean \nPettit (2002) offers the example of an elderly German who answers ‘it means nurse’ \nto every foreign language question, including when asked about the German word \n‘Krankenschwester’ (‘nurse’ in German). Such cases of apparent, fortuitous under -\nstanding are more easily dismissed through a diachronic approach: extended conver-\nsation would reveal the absence of genuine communication.\nThere is another sense of ‘rulelessness’ closely linked to absurdity: the sheer ste-\nreotypicality of a dialogue that coincides with acting. In scripted exchanges, actors \nperform understanding rather than truly grasping one another. Yet, no paradox \narises in recognizing that A and B do not genuinely comprehend each other—their \n24 According to some, chance and randomness can be distinguished when considering these notions \nstrictly (Eagle, 2021). However, the problem remains unchanged. The sequence of responses in Wittgen-\nstein’s scenario could be pseudorandom—neither generated by chance nor truly disorderly, as patterns \ncan be identified (Eagle, 2016, p. 443). The key point is that the sequence is unsystematic with respect to \nthe expected one.\n25 As in Alvin Goldman’s ‘barn cases’ (1976). The relationship between linguistic understanding and \nluck remains underexplored (Fairweather and Montemayor, 2023), yet will gain relevance in AI philoso-\nphy (Montemayor, 2021). We concur with Peet (2023) that understanding produces non-lucky judgments \nabout utterances. However, we differ regarding the value of non-luck-based communication he advocates, \ndue to our minimal normativism. For a Davidsonian interpretation of this (Kripkensteinian) matter of \nluck, see Verheggen (2023).\nThe bewitching AI: The Illusion of Communication with Large…\nPage 19 of 33 61\ncharacters do. A third party might misinterpret this if unaware the dialogue is staged. \nWe lack knowledge of an LLM’s distribution, the inputs of which trigger responses \nthat are stereotypical—those most frequent for a prompt and clichés for us. But even \nif we knew and adjusted, using only these stereotypes in interacting with the LLM, \nwould we not merely perform understanding?\nAt this juncture, we must still consider the second challenge, where contradic-\ntion is not necessarily problematic, as it may be repairable. However, as observed, \nLLMs’ failure to manage contradictions effectively casts doubt on this possibility. \nWithout a reliable capacity to handle the negative family, they cannot recognize con-\ntradictions or resolve entanglement. This also seals the fate of the luck issue. An \nLLM-based system’s inability to repair conversations when encountering contradic-\ntions amplifies doubts about seemingly coincidental matches between human state-\nments and machine responses.\nThus, contradictions ‘not always being the end’ is a fortiori problematic for \nLLMs. It adds more contexts to manage: those where contradictions matter and \nthose where they do not.26 Not only this, LLM should also be able to discern which \nconclusions we draw from contradictions—whether everything, something, or noth-\ning at all can follow.27 Additionally, while common contradictions like ‘it is raining \nand it is not raining’ could be easily handled by LLMs, not all contradictions are \nso frequent, and even common ones can have unique roles in our actual language \ngames. These are variables that depend on human capacity for mutatis mutandis, \nrather than on statistical relationships governing intra-linguistic usage within text \ncorpora.\nSignificant challenges impede progress. The assumption that greater exposure \nto conversational examples improves AI performance faces the formidable obsta-\ncle of combinatorial explosion. This issue persists whether augmenting pretraining \nwith post-training methods—as currently prevalent (Kalai & Vempala, 2024)—or \nexpanding exposure by remaking pretraining from scratch.\nDialogues represent intricate mazes of interlocked constraints, with conversa-\ntional constraints given by the reference points exhibiting inherent complexity. Even \nseemingly simple exchanges can rapidly become out of distribution. Dialogue’s vari-\nations are numerous and challenging to evaluate and label effectively.\nThe prevalence of ‘incorrect’ dialogues far outweighs that of ‘correct’ ones. A \nneural network is exposed not only to successful exchanges but also to the myriad \nways in which conversations can falter, and communication can break down. This \nchallenge echoes Goodman’s riddle of induction (1983, p. 74) and its connection to \nthe rule-following problem, as elaborated by Kripke (1982, pp. 58–59). The insight \nthat not all generalizations are confirmed by their instances is particularly relevant \n26 This is in line also with Pérez-Escobar and Sarikaya (2022, p.17), who note that the line between con-\ntradiction’s confusing effect and its practical use is more blurred than mainstream Wittgenstein readers \nadmit.\n27 For a discussion on derivability in Wittgenstein, specifically relating to the principles of ex contra-\ndictione quodlibet, where every proposition follows logically from a contradiction, or those of paracon-\nsistent logic, where some propositions may follow, or ex contradictione nihil fit, where no propositions \nfollow, see Koshkin (2021).\n E. Bottazzi Grifoni, R. Ferrario \n61 Page 20 of 33\nhere. Contradictory situations exacerbate this issue, as they can be viewed as inher -\nently ‘gruesome’: recognizing contradictory situations from data is less ‘entrenched’ \nthan other scenarios encountered in ordinary linguistic interactions that align with \nour established agreements. Each ‘incorrect’ dialogue presents multiple potential \nsolutions, further compounding the complexity. The unpredictability of entangle-\nments in conversation implies that their resolution is at least equally, if not more, \nunpredictable. This multiplicative effect on complexity arises as the number of pos-\nsible corrections for each failure can exceed the number of errors themselves.\nIt is challenging to conceive how this vast combinatorial space could be effectively \nnarrowed through neuro-symbolic approaches, such as introducing explicit rules to \nprevent inconsistencies in language model outputs. This would not help understanding \nbecause of what has been said about calculus and language games. There is no easy \nway to automatically classify a situation; symbolic approaches add the risk of provid-\ning information that is either true but irrelevant, biased, or just plainly wrong.\nA final limit on improving human–AI understanding lies in rule-following itself. \nAs argued previously, there exists a mode of following rules blindly, without choice. \nSome philosophers contend that rule-following does not inherently require inten-\ntionality (Boghossian, 2012; Kripke, 1982). However, recognizing and resolving \ncontradictory situations seems to necessitate some form of intentionality or capacity \nfor choice—attributes that current language models lack.\nWhile one can become unknowingly entangled in rules, escaping such entangle-\nment requires awareness—a quality absent in current AI systems. This perspective \nmay illuminate Wittgenstein’s assertion in Sect.  125 of the Investigations regard-\ning the relationship between contradiction and our conception of meaning. Wittgen-\nstein’s discussions of purpose (e.g., PI, §§2, 69, 87, 132, and 208) suggest viewing \ncontradictions in understanding as goal-relative, aiding in the identification and res-\nolution of discursive conflicts. However, his anti-mentalistic stance on meaning may \nimpose limitations on both his approach and ours, necessitating further investigation \ninto the nature of meaning and intentionality in AI systems.28\n7  How the Bewitchment Arises\nIf LLMs lack true understanding, what leads us to believe otherwise? This impres-\nsion perhaps stems from overestimating progress, as seen in the Investigations’ epi-\ngraph from Nestroy’s play. We defend that there is also a reason tied to our mode \n28 To those focusing on shallow similarities between humans and machines—training, neural networks, \nor non-deterministic behaviors—clarifying intentionality may help establish distinction. Such analogies \nignore evident differences between these forms of life—assuming LLMs can be considered life at all. \nA more fruitful approach examines constancy through intentionality. Montemayor (2021) attributes to \nhuman agency the capacity for joint attention, on a cognitive and neurophysiological basis, allowing for \n“the arch of linguistic action” (p. 4)—absent in LLMs. Anyway this requires careful consideration, since \na Wittgensteinian approach might view joint attention as simply ‘what we do’ (PI 217) without requir -\ning further justification. For other approaches to intention and meaning in LLMs, see Floridi (2023) and \nGubelmann (2024).\nThe bewitching AI: The Illusion of Communication with Large…\nPage 21 of 33 61\nof engagement with language. While we can recognize and correct misunderstand-\nings—a possibility that LLMs lack—this ability compensates for human communi-\ncation’s inherent fragility, even precariousness, though it does not guarantee under -\nstanding in every instance. Nevertheless, some level of certainty in communication \ncoexists with this fragility.\nIn many everyday contexts, we generally proceed with a degree of confidence \nin what we say and what others tell us, until given reason to doubt. While some \nindividuals are more skeptical than others, and certain situations warrant heightened \nscrutiny, our ordinary communicative practices cannot function with perpetual veri-\nfication or second-guessing.\nIn order to carry out their daily activities, people assume their statements are \nreasonable, comprehensible, and clear, expecting the same from others (Garfinkel, \n1967, pp. 41–42). Wittgenstein argues that deeming our calculations uncertain or \nunreliable based on the fact that errors are always possible is madness (OC, §217). \nThis stance, while seemingly at odds with the notion of communication’s fragility, \nactually aligns with it.29 As Pascal said, from the fact that certain words are applied \non the same occasions, we tend to believe that we have the same definitions, even \nthough there is no guarantee that this is the case: we know that similar conclusions \nare often drawn from different premises. The fact that conformity of application \nimplies conformity of ideas is only a ‘conjecture.’ 30 Not only might what a person \nsays or does diverge because this person has learned it differently, or even because \nthey are following a completely different rule that corresponds only for some time \nwith my way, but also because of the substantial difference between the prediction I \ncan make about myself and the prediction I can make about the other:\nTwo points, however, are important: one, that in many cases someone else can-\nnot predict my actions, whereas I foresee them in my intention; the other, that \nmy prediction (in my expression of intention) does not rest on the same foun-\ndation as his prediction of my action, and that the conclusions to be drawn \nfrom these predictions are quite different.\n(PPF xi, §329)\nThus, there is uncertainty regarding what others might do: they might not fol-\nlow the rules, for example. This fact holds if we think about it, and anyone, even \nif not philosophically, soon realizes it, knowing that normatively bound behaviors \n(and even deontically bound ones) do not guarantee us in the same way that the \n29 Kusch (2016, p. 240) distinguishes Kripkenstein’s ontological doubt about meaning-determining facts \nfrom Wittgenstein’s epistemological dismissal of skepticism regarding errors that make no sense within \nour existing language games, making these two aspects compatible as well. While not disagreeing, we \navoid such ontology/epistemology distinctions. Moreover, Kripke’s position would also affect our trust in \ncommunication. Recognizing that one person might use the ‘ + ’ sign for ordinary addition while another \nuses it for a different function makes us realize there’s no metaphysical basis this sign means addition. \nBut it also makes us question whether this sign might mean something different to others than what \nseems obvious to us.\n30 Pensées, 141, in Pascal (1995). Von Wright, whose observation has been largely overlooked, identi-\nfies “a trenchant parallelism which deserves closer study” between Pascal and Wittgenstein (O’Connor \nDrury, 2018, p. 155).\n E. Bottazzi Grifoni, R. Ferrario \n61 Page 22 of 33\nregularities we encounter in the environment do. If, as a pedestrian, I cross a road \nblocked by an obstacle, I might not look if someone is coming, while I might more \neasily have the scruple to look if someone is coming when I cross a road without \nobstacles, even if it is red for motorists. However, I cannot follow these scruples for \nevery normatively bound circumstance. So, I follow the rules of the language games \nI am involved in, similar to the musician who follows the score without thinking.\nThe communitarian view, according to which “the community denies of some-\none that he is following certain rules” (Kripke, 1982, p. 93), naturally fits with our \nexperience that performing arithmetic in the usual way seems evident to us and \nour expectation that others will find it equally apparent; we are indeed accustomed \nand trained to follow the rules of our language games in a specific way. This train-\ning needs the ‘negative family,’ that is, for example, through the denial or rejection \nof certain of our behaviors as conforming to the rule. 31 Moreover, our training is \nsuch that we follow our rules blindly; it is thus tendentially tricky for us to get out \nof our habit and see that, in principle, things can be different, that is, for example, \nthat someone culturally distant from us may behave differently in the same circum-\nstances to the point of being incomprehensible.\nDeviations from these norms provoke immediate attempts to restore a correct \nstate of affairs and genuine bewilderment. Garfinkel illustrates some ‘breach experi-\nments’ conducted by some of his students. For example:\nThe subject was telling the experimenter, a member of the subject’s car pool, \nabout having had a flat tire while going to work the previous day.\n(S) I had a flat tire.\n(E) What do you mean, you had a flat tire\nShe appeared momentarily stunned. Then she answered in a hostile way: \n“What do you mean, ‘What do you mean?’ A flat tire is a flat tire. That is what \nI meant. Nothing special. What a crazy question!”\n(Garfinkel, 1967, p. 42)\nThe blindness with which we follow a rule could be a ‘groundless trust,’ but it \nis the basic attitude in a community (Mulligan, 2002). The outputs of LLMs can \nfollow common patterns in our language games, inducing us to apply familiar gram-\nmatical rules blindly, leading to a feedback loop in which we perceive these stereo-\ntypical responses as meaningful when they are not.\nWhen we interact with an LLM, it responds in a stereotyped manner without \nmaintaining constancy in the dynamics of the conversation. However, we humans \nautomatically apply the practical principle that understanding is assumed until \nproven otherwise, and thus we perceive the LLM as taking the interaction into \naccount in its specificity. We can imagine that the stereotypical situation of a flat tire \nelicits an equally stereotypical response from the LLM-based system, but—and here \n31 This, of course, does not deny the obvious role of positive reinforcement in learning processes. \nRather, it emphasizes the impossibility of meaningful training in the absence of the negative family. As \nwe have discussed earlier (Sect.  4) a conversation that must just be ‘positive’ would easily cease to be \ncommunicative.\nThe bewitching AI: The Illusion of Communication with Large…\nPage 23 of 33 61\nlies a large part of the bewitchment—our vision of what is stereotypical on the one \nhand and the training of LLMs on the most probable word sequences given the input \n(also taking into account the ‘filters’ we can put downstream of the output) on the \nother, not only can diverge significantly but are inherently difficult to know.\nThe LLM’s ability to produce plausible responses stems from its capacity to iden-\ntify regularities derived from training on large datasets. This makes it appear to fol-\nlow those linguistic rules established through interaction, leading us to think that \ncommunication is taking place. In this way, they trigger Pascal’s conjecture through \nWittgenstein’s certainty and blind rule-following.\nHowever, inconsistency should raise our suspicions; indeed, we are all led to a \n‘second thought.’ Nevertheless, we fall into bewitchment. We are not accustomed \nto something that produces language and yet loses its way in conversation so unsys-\ntematically. For generations, until now, getting lost in a conversation often meant \nsomehow managing to pick up the thread again with the other person—understand-\ning what one had become entangled in and finding some way out of it. Those who \nbecame incorrigibly lost in enough respects were, in some way, distanced from the \nlife of the community. Being incorrigible in enough respects was tied to a persis-\ntence in getting lost—a persistence that LLMs also exhibit. However, this persis-\ntence is accompanied by a random variability that eludes us and is difficult to test \nrigorously (as we saw in the previous section and will see again in the conclusions).\nWe can grasp this incorrigibility if we carefully reflect on their structure, but \nwhen we interact with them, we fall back into blindly following the rules of lan-\nguage, treating them—within the language games—as speakers with whom we can \ncommunicate. Even when we observe, in our everyday use, that they get lost in dis-\ncourse and we subsequently abandon the interaction, we nevertheless return to using \nthis technology. We do so, of course, because it is useful—LLMs sometimes per -\nform certain tedious tasks excellently—but this repeated return inevitably causes us \nto fall back into bewitchment.\nThe strangeness of some of their responses, as in the case of the eBook, is a \nsymptom of this underlying structural problem—one that does not always manifest \nitself and is not, in itself, the problem. The problem is unreliability.\nAmong the various factors that might lead us to believe we are communicating \nwhen, in fact, we are not, we can also imagine more empirical cases—for instance, \nsituations where we fail to recognize the contradictions in LLM outputs. Scientific \nresearch on this is still in its early stages (Macmillan-Scott & Musolesi, 2025).\nMacmillan-Scott and Musolesi (2024) highlight an LLM’s response to the Monty \nHall problem, a probability paradox in which a contestant chooses one of three \ndoors, behind one of which is a prize. The host, knowing where the prize is, opens \nan empty door and offers the contestant a chance to switch. While many mistakenly \nbelieve the odds remain unchanged, switching actually doubles the chances of win-\nning. The LLM bizarrely states: “In either case, the outcome is the same. Whether \nthe contestant switches or not, they will either win the game or lose. Therefore, it \ndoesn’t matter whether they switch or not.” A response that Macmillan-Scott & \nMusolesi find neither correct nor human-like—but one that we can imagine might \nconfuse someone and go unnoticed, especially given that this problem is notoriously \ndifficult to grasp even for humans (Wilcox, 2024). Alternatively, we can imagine \n E. Bottazzi Grifoni, R. Ferrario \n61 Page 24 of 33\nthat an error might be invisible because it takes on a form similar to the well-known \nMoses illusion (Erickson & Mattson, 1981, explored in LLM interaction by \nSobieszek & Price, 2022). When asked how many animals of each kind Moses took \non the Ark, many answered ‘two,’ despite knowing full well that it was Noah who \nled the Ark.\nThere seems to be more than one pitfall, but as of today, they are difficult to iden-\ntify, describe, and study—especially if we take into account how LLMs blend vari-\nability and stereotypicality. So, what can we do?\n8  Conclusions\n“Humanum fuit errare, diabolicum est per animositatem in errore manere” (“To \nfall into error is human, but to persist in error out of pride is diabolical,” Augus-\ntine 1992, Sermones, 164.14). There is something telling—hopefully not too much \nabout our own mindset—in the fact that such a phrase comes to mind when reflect-\ning on the errors of these talking machines. Curiously enough, this remark comes \nfrom the very writer who opens the Investigations. We are certainly not concerned \nwith AI’s pride, let alone the devil, yet AI cannot help but persist, so to speak, in \nits unpredictable way of getting lost in verbal interaction. This phrase comes to \nmind again when considering the persistence of some among us humans in using \nthese systems. Here, too, at least for us writing this, there is nothing diabolical—yet \nwe remain within the province of religion, if we look at the phenomenon with an \nanthropological perspective (Öhman, 2024). As for human pride, we may have some \nstrong intuitions, but they remain entirely formless.\nSo, what is to be done? Perhaps we should reflect further on this articulation—\none that may indeed be diabolical—between the persistence of machine error and our \nown persistence in using them, and seek a way to break it. Perhaps this could be in \nfavor of a different use of this technology, though if we are serious, it is difficult—at \nleast for us—to conceive of such an alternative in advance through pure speculation.\nIn the meantime, in the spirit of the via negativa, we could hint at what should \nnot be done. For example, one should not see the considerations expressed in this \nwork as offering a simple means of distinguishing human from machine behavior. \nThat is, rather than seeking the infallible determination of a difference established \nby an external judge, we reaffirm that we are interested in the consequences of using \nLLMs. This is also reflected in the way we would prefer the examples not to be con-\nsidered. Chance, as Balzac says, is the greatest novelist (1976, p. 11). That is, we \nbelieve that, at best, examples can only gesture toward the incredible complexity of \nwhat could go wrong in real-world scenarios.\nIf one accepts that the emphasis should be on the unsettling indeterminacy of \nwhat could happen, then the point of our work is not even about deciding how \nthe word “communication” should be used. Usage may change. 32 The problem is \n32 PI, §23. For a recent discussion on how our linguistic practices in describing artificial intelligence, \nincluding generative AI, have evolved—drawing on Wittgenstein’s reflections—see Floyd (2023).\nThe bewitching AI: The Illusion of Communication with Large…\nPage 25 of 33 61\nbewitchment—the risk of falling blindly into our own human way of using language \nto exert influence and control, because this is what we have been trained to do from \ngeneration to generation. Embedding different automatisms in how we speak to \nmachines would then perhaps require a form of training that, as of now, remains \nvery hard to imagine.\nWe are not offering a revelatory and permanently valid experimentum crucis that \nexposes the discrepancy between human and non-human interaction. Our attitude \ntoward benchmarks is critical. We have considered them in this work, but we have \ntried as much as possible not to rely on them in this sense. Doubts about their use-\nfulness are increasingly being raised. It is well known that benchmarks fail to cap-\nture latent vulnerabilities that may only emerge in specific situations and that, once \npublished, they become training material for subsequent generations of models, \nimproving responses—but, we suspect, only in their stereotypicality.33\nFloridi and Chiriatti (2020) introduce, in the context of interaction with LLMs, \nthe concept of “irreversibility” in responses: some questions allow us to determine \nwhether a response originates from a human or an artificial source (because they \npresuppose an understanding of context and meaning), while others produce “indis-\ntinguishable” answers regardless of the source. They illustrate how, with the advent \nof these models, the area of indistinguishability expands. Sobieszek and Price \n(2022) observe that there is no well-defined set of semantic questions that can trip \nup GPT-3, suggesting that Turing testing should be conceived of as a continuous \nprocess, with a variable degree of certainty throughout the exchange (Montemayor, \n2021). Our analysis could offer a complementary approach, suggesting a focus on \nthe constancy of negation within discourse. This might be useful for cranking up \nthe pressure on the machine, so to speak—relentlessly probing LLMs to see if and \nhow they derail. Yet this represents something quite different from a standardized \nbenchmark: rather, it approaches a technique akin to an art. Perhaps it is not so \nabsurd that something non-systematic serves as a bridge between a mathematically \ngrounded system and the community of humans in order to reveal a certain kind of \nhaphazardness within these systems. Assuming all this makes sense, the require-\nments for practicing this art remain entirely to be imagined. Furthermore, even a \nhuman could simulate this kind of odd behavior, and there is little to be done about \nit—except to consider the scrutiny itself as a test of communicative reliability. Here \ntoo all possibilities are open: now, one can play dumb without being dumb, but also \nplay superintelligent without—perhaps—even existing. Videmus nunc per specu-\nlum in aenigmate.34\n33 There is substantial evidence of evaluation data being included in the training data, suggesting that a \nstrategic alteration of the evaluation is taking place (Haimes et al., 2024). There is also a problem of sat-\nuration of existing benchmarks, whereby LLMs pass all tests—successes that do not necessarily reflect a \nproportional leap forward in these systems’ abilities, but rather call for a shift in how these capabilities \nshould be evaluated (Wang et al., 2025). For an up-to-date overview of the literature on the limitations of \nbenchmarks, see Eriksson et al. (2025).\n34 ‘Now we see things through a mirror, by enigmas’ (1 Corinthians 13:12).\n E. Bottazzi Grifoni, R. Ferrario \n61 Page 26 of 33\nAbbreviations VC :  Ludwig Wittgenstein and the Vienna Circle: Conversations Recorded by Friedrich \nWaismann (B. F. McGuinness, Ed.). Blackwell. 1979.; LC : Lectures & conversations on aesthetics, psy-\nchology, and religious belief. Univ of California Press. 1966.; OC :  On Certainty (G. E. M. Anscombe \n& G. H. von Wright, Eds.). Blackwell. 1969.; LFM :  Lectures on the Foundations of Mathematics (C. \nDiamond, Ed.). Cornell University Press. 1976.; RFM : Remarks on the Foundations of Mathematics (G. \nH. von Wright, R. Rhees, & G. E. M. Anscombe, Eds.). Blackwell. 1978.; RPP II : Remarks on the Phi-\nlosophy of Psychology (G. H. von Wright & H. Nyman, Eds.; Vol. 2). Blackwell. 1980.; LW I : Last Writ-\nings on the Philosophy of Psychology (G. H. von Wright & H. Nyman, Eds.; Vol. 1). Blackwell. 1982.; PI \n: Philosophical Investigations (P. M. S. Hacker & J. Schulte, Eds.). Wiley-Blackwell. 2009.; PPF : Phi-\nlosophy of Psychology. A Fragment. In Philosophical investigations. Wiley-Blackwell. 2009.; Z :  Zettel \n(G. E. M. Anscombe & G. H. von Wright, Eds.; G. E. M. Anscombe, Trans.). Blackwell. 1967\nAcknowledgements We would like to express our gratitude to Chiara Bassetti, Rino Falcone, Aldo \nGangemi, Mattia Petrolo, Daniele Porello, Giuseppe Primiero, and Chris Welty for their valuable insights \nand suggestions. We also extend our thanks to all the members of the Laboratory of Applied Ontology \nat the Institute of Cognitive Sciences and Technologies of the National Research Council (LOA, ISTC-\nCNR) for the frequent and intense discussions on various drafts of this work. Additionally, we thank \nthe participants at various meetings and conferences where earlier versions of this work were presented, \nincluding at the University of Trento, Milano and Genova in the years 2023 and 2024.\nAuthor Contributions Emanuele Bottazzi developed the philosophical analysis, articulated the theoreti-\ncal frameworks, and wrote the manuscript. Roberta Ferrario contributed to the conceptual development, \nto the writing of the manuscript and through critical philosophical discussions throughout the develop-\nment of the arguments, and provided editorial suggestions. Both authors reviewed and approved the final \nmanuscript.\nFunding Open access funding provided by Consiglio Nazionale Delle Ricerche (CNR) within the CRUI-\nCARE Agreement. This research has been funded by the Project PRIN2020 BRIO—Bias, Risk and \nOpacity in AI (2020SSKZ7R) awarded by the Italian Ministry of University and Research (MUR), by the \nProject PRIN 2022 SMARTEST (Project nr. 202223E8Y4X), funded by MUR and by the project FAIR, \n(Project nr. PE 00000013), funded by MUR.\nDeclarations \nEthical Approval Not applicable.\nConsent to Participate Not applicable.\nConsent to Publish Not applicable.\nCompeting Interests Both Emanuele Bottazzi and Roberta Ferrario declare they have no financial inter -\nests.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended \nuse is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permis-\nsion directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/.\nThe bewitching AI: The Illusion of Communication with Large…\nPage 27 of 33 61\nReferences\nAaronson, S. (2024). GPT-4 gets a B on my quantum computing final exam! Shtetl-Optimized Blog. \nRetrieved March 5, 2025, from https:// scott aaron son. blog/?p= 7209\nAckley, D. H., Hinton, G. E., & Sejnowski, T. J. (1985). A learning algorithm for Boltzmann \nmachines. Cognitive Science, 9(1), 147–169. https:// doi. org/ 10. 1016/ S0364- 0213(85) 80012-4\nAnthony, D. (2004). The cognitive neuropsychiatry of auditory verbal hallucinations: An overview. \nCognitive Neuropsychiatry, 9(1–2), 107–123. https:// doi. org/ 10. 1080/ 13546 80034 40001 83\nAsher, N., Bhar, S., Chaturvedi, A., Hunter, J., & Paul, S. (2023). Limits for learning with language \nmodels. In A. Palmer & J. Camacho-collados (Eds.), Proceedings of the 12th Joint Conference \non Lexical and Computational Semantics (*SEM 2023) (pp. 236–248). https:// doi. org/ 10. 18653/ \nv1/ 2023. stars em-1. 22\nAsher, N., & Bhar, S. (2024). Strong hallucinations from negation and how to fix them. In Findings of \nthe Association for Computational Linguistics: ACL 2024 (pp. 12670–12687). https:// doi. org/  \n10. 18653/ v1/ 2024. findi ngs- acl. 752\nAtil, B., Aykent, S., Chittams, A., Fu, L., Passonneau, R. J., Radcliffe, E., Rajagopal, G. R., Sloan, A., \nTudrej, T., Ture, F., Wu, Z., Xu, L., & Baldwin, B. (2025). Non-determinism of “deterministic” \nLLM settings. arXiv. https:// arxiv. org/ pdf/ 2408. 04667\nAugustine. (1992). Sermons III/5 (148–183) on the New Testament (E. Hill, Trans.). New City Press.\nBaker, G. P., & Hacker, P. M. S. (2005). Wittgenstein: Understanding and Meaning. Analytical Com-\nmentary on the Philosophical Investgations Vol. 1, Part II. Blackwell.\nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021). On the Dangers of Stochastic \nParrots: Can Language Models Be Too Big? Proceedings of the 2021 ACM Conference on Fair -\nness, Accountability, and Transparency, 610–623. https:// doi. org/ 10. 1145/ 34421 88. 34459 22\nBengio, Y., Cohen, M., Fornasiere, D., Ghosn, J., Greiner, P., MacDermott, M., Mindermann, S., \nOberman, A., Richardson, J., Richardson, O., Rondeau, M.-A., St-Charles, P.-L., & Williams-\nKing, D. (2025). Superintelligent agents pose catastrophic risks: Can scientist AI offer a safer \npath? arXiv. https:// arxiv. org/ abs/ 2502. 15657. Accessed 16 Mar 2025.\nBerg, Á. (2024). Was Wittgenstein a radical conventionalist? Synthese, 203(1), 37. https:// doi. org/ 10. \n1007/ s11229- 023- 04457-z\nBerto, F. (2008). Adynaton and material exclusion. Australasian Journal of Philosophy, 86, 175–\n190. https:// doi. org/ 10. 1080/ 00048 40080 18861 99\nBerto, F., & Restall, G. (2019). Negation on the Australian plan. Journal of Philosophical Logic,  \n48(6), 1119–1144. https:// doi. org/ 10. 1007/ s10992- 019- 09510-2\nBoghossian, P. A. (2012). Blind rule-following. In A. Coliva (Ed.), Mind, Meaning, and Knowledge: \nThemes from the Philosophy of Crispin Wright (pp. 27–48). Oxford University Press.\nBoghossian, P. (2022). The normativity of meaning revisited. In B. Dunaway & D. Plunkett (Eds.), \nMeaning, decision, and norms: Themes from the work of Allan Gibbard (pp. 389–401). Michi-\ngan Publishing Services. https:// doi. org/ 10. 3998/ mpub. 99481 99\nCavell, S. (1976). The availability of Wittgenstein’s later philosophy. In Must We Mean What We Say? \nA Book of Essays (pp. 41–67). Cambridge University Press. https:// doi. org/ 10. 1017/ CBO97  \n81316 286616\nChalmers, D. J. (2023). Could a Large Language Model be Conscious? arXiv. https:// arxiv. org/ abs/  \n2303. 07103. Accessed 10 Dec 2024.\nChang, Y., Wang, X., Wang, J., Wu, Y., Yang, L., Zhu, K., Chen, H., Yi, X., Wang, C., Wang, Y., Ye, \nW., Zhang, Y., Chang, Y., Yu, P. S., Yang, Q., & Xie, X. (2024). A survey on evaluation of large \nlanguage models. ACM Transactions on Intelligent Systems and Technology, 15(3), 39. https://  \ndoi. org/ 10. 1145/ 36412 89\nChaturvedi, A., Bhar, S., Saha, S., Garain, U., & Asher, N. (2024). Analyzing semantic faithfulness \nof language models via input intervention on question answering. Computational Linguistics,  \n50(1), 119–155. https:// doi. org/ 10. 1162/ coli_a_ 00493\nDavidson, D. (2001). Subjective, intersubjective, objective (Vol. 3). Oxford University Press.\nde Balzac, H. (1976). Avant-propos à La Comédie humaine. In P. -G. Castex (Ed.). Gallimard. (Origi-\nnal work published 1842).\nDeluty, E. W. (2005). Wittgenstein’s Paradox. Philosophical investigations, paragraph 242. Interna-\ntional Philosophical Quarterly, 45(1), 87–102. https:// doi. org/ 10. 5840/ ipq20 05451 65\n E. Bottazzi Grifoni, R. Ferrario \n61 Page 28 of 33\nDentella, V., Günther, F., Murphy, E., Marcus, G., & Leivada, E. (2024). Testing AI on language \ncomprehension tasks reveals insensitivity to underlying meaning. Scientific Reports, 14, 28083. \nhttps:// doi. org/ 10. 1038/ s41598- 024- 79531-8\nDouglas, M. R. (2023). Large language models. arXiv. https:// arxiv. org/ abs/ 2307. 05782. Accessed 10 \nDec 2024.\nDu, Y., Li, S., Torralba, A., Tenenbaum, J. B., & Mordatch, I. (2023). Improving Factuality and \nReasoning in Language Models through Multiagent Debate. arXiv. https:// arxiv. org/ abs/ 2305.  \n14325. Accessed 10 Dec 2024.\nDung, L. (2023). Current cases of AI misalignment and their implications for future risks. Synthese,  \n202, 138. https:// doi. org/ 10. 1007/ s11229- 023- 04367-0\nEagle, A. (2016). Probability and Randomness. In A. Hájek & C. Hitchcock (Eds.), The Oxford Hand-\nbook of Probability and Philosophy (pp. 440–459). Oxford University Press.\nEagle, A. (2021). Chance versus Randomness. In E. N. Zalta (Ed.), The Stanford Encyclopedia of Philos-\nophy (Spring 2021). Metaphysics research lab, Stanford University. Retrieved December 10, 2024, \nfrom https:// plato. stanf ord. edu/ archi ves/ spr20 21/ entri es/ chance- rando mness/\nErickson, T. D., & Mattson, M. E. (1981). From words to meaning: A semantic illusion. Journal of \nVerbal Learning and Verbal Behavior, 20(5), 540–551. https:// doi. org/ 10. 1016/ S0022- 5371(81) \n90165-1\nEriksson, M., Purificato, E., Noroozian, A., Vinagre, J., Chaslot, G., Gomez, E., & Fernandez-Llorca, D. \n(2025). Can we trust AI benchmarks? An interdisciplinary review of current issues in AI evalua-\ntion. arXiv. https:// arxiv. org/ abs/ 2502. 06559. Accessed 16 Mar 2025.\nEsanu, A. (2024). Scrutinizing the foundations: Could large language models be solipsistic? Synthese, \n203, 158. https:// doi. org/ 10. 1007/ s11229- 024- 04589-w\nEttinger, A. (2020). What BERT Is Not: Lessons from a New Suite of Psycholinguistic Diagnostics for \nLanguage Models. Transactions of the ACL, 8, 34–48. https:// doi. org/ 10. 1162/ tacl_a_ 00298\nFairweather, A., & Montemayor, C. (2023). Introduction. In A. Fairweather & C. Montemayor (Eds.), \nLinguistic Luck: Safeguards and threats to linguistic communication (pp. 1–14). Oxford University \nPress.\nFilcheva, K. (2025). The no-content view of contradictions. Acta Analytica. https:// doi. org/ 10. 1007/ \ns12136- 025- 00624-8\nFloridi, L. (2023). AI as Agency Without Intelligence: On ChatGPT, Large Language Models, \nand Other Generative Models. Philosophy and Technology., 36, 15. https:// doi. org/ 10. 1007/ \ns13347- 023- 00621-y\nFloridi, L., & Chiriatti, M. (2020). GPT-3: Its nature, scope, limits, and consequences. Minds and \nMachines, 30, 681–694. https:// doi. org/ 10. 1007/ s11023- 020- 09548-1\nFloyd, J. (2023). Revisiting the Turing Test: Humans, Machines, and Phraseology. In J. Katz, K. Schiep-\ners, & J. Floyd (Eds.), Nudging Choices Through Media. Palgrave Macmillan. https:// doi. org/ 10. \n1007/ 978-3- 031- 26568-6_5\nForster, M. N. (2016). The Autonomy of Grammar. In H.-J. Glock & J. Hyman (Eds.), A Companion to \nWittgenstein (pp. 269–277). Wiley. https:// doi. org/ 10. 1002/ 97811 18884 607. ch15\nFried, D., Aghajanyan, A., Lin, J., Wang, S., Wallace, E., Shi, F., Zhong, R., Yih, S., Zettlemoyer, L., \n& Lewis, M. (2023). InCoder: A generative model for code infilling and synthesis. The Eleventh \nInternational Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1–5, \n2023. Retrieved March 16, 2025, from https:// openr eview. net/ forum? id= hQwb- lbM6EL\nGarfinkel, H. (1967). Studies in Ethnomethodology. Prentice-Hall.\nGaskin, R. (2025). Linguistic idealism and the genealogy of negation (forthcoming). Oxford University \nPress.\nGlock, H.-J. (1997). On safari with Wittgenstein, Quine and Davidson. In R. Arrington & H.-J. Glock \n(Eds.), Wittgenstein and Quine (pp. 144–172). Routledge. https:// doi. org/ 10. 4324/ 97802 03050 743\nGlüer, K. (2001). Wittgenstein and Davidson on agreement in judgement. Wittgenstein-Studien, 2, \n81–103.\nGlüer, K., & Wikforss, Å. (2009). Against content normativity. Mind, 118(469), 31–70. https:// doi. org/ \n10. 1093/ mind/ fzn154\nGoldfarb, W. (1992). Wittgenstein on Understanding. Midwest Studies in Philosophy, 17(1), 109–122. \nhttps:// doi. org/ 10. 1111/j. 1475- 4975. 1992. tb001 45.x\nGoldman, A. (1976). Discrimination and perceptual knowledge. Journal of Philosophy, 73, 771–791. \nhttps:// doi. org/ 10. 2307/ 20256 79\nThe bewitching AI: The Illusion of Communication with Large…\nPage 29 of 33 61\nGoldstein, L. (2004). The Barber, Russell’s Paradox, Catch-22, God, Contradiction, and More. In G. \nPriest, J. Beall, & B. P. Armour-Garb (Eds.), The law of non-contradiction: New philosophical \nessays (pp. 295–313). Oxford University Press.\nGoodman, N. (1983). Fact, fiction, and forecast. Harvard University Press.\nGubelmann, R. (2023). A Loosely Wittgensteinian Conception of the Linguistic Understanding of Large \nLanguage Models Like Bert, Gpt-3, and Chatgpt. Grazer Philosophische Studien, 99(4), 485–523. \nhttps:// doi. org/ 10. 1163/ 18756 735- 00000 182\nGubelmann, R. (2024). Large Language Models, Agency, and Why Speech Acts are Beyond Them (For \nNow) – A Kantian-Cum-Pragmatist Case. Philosophy & Technology, 37, 32. https:// doi. org/ 10. \n1007/ s13347- 024- 00696-1\nGubelmann, R., & Handschuh, S. (2022). Context Matters: A Pragmatic Study of PLMs’ Negation \nUnderstanding. In S. Muresan, P. Nakov, & A. Villavicencio (Eds.), Proceedings of the 60th \nAnnual Meeting of the ACLs (Volume 1: Long Papers) (pp. 4602–4621). https:// doi. org/ 10. 18653/ \nv1/ 2022. acl- long. 315\nHaimes, J., Wenner, C., Thaman, K., Tashev, V., Neo, C., Kran, E., & Schreiber, J. (2024). Benchmark \ninflation: Revealing LLM performance gaps using retro-holdouts. arXiv. https:// arxiv. org/ abs/ 2410. \n09247. Accessed 28 Apr 2025.\nHan, S., Schoelkopf, H., Zhao, Y., Qi, Z., Riddell, M., Zhou, W., Coady, J., Peng, D., Qiao, Y., Benson, \nL., Sun, L., Wardle-Solano, A., Szabo, H., Zubova, E., Burtell, M., Fan, J., Liu, Y., Wong, B., \nSailor, M., … Radev, D. (2024). FOLIO: Natural language reasoning with first-order logic. arXiv. \nhttps:// arxiv. org/ abs/ 2209. 00840. Accessed 16 Mar 2025.\nHe, Q., Zeng, J., Huang, W., Chen, L., Xiao, J., He, Q., Zhou, X., Liang, J., & Xiao, Y. (2024). Can large \nlanguage models understand real-world complex instructions? In Proceedings of the thirty-eighth \nAAAI conference on artificial intelligence and thirty-sixth conference on innovative applications of \nartificial intelligence and fourteenth symposium on educational advances in artificial intelligence \n(AAAI’24/IAAI’24/EAAI’24), Vol. 38. AAAI Press, Article 2029, (pp.18188–18196). https:// doi. \norg/ 10. 1609/ aaai. v38i16. 29777\nHendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt, J. (2020). Measuring \nmassive multitask language understanding. arXiv. https:// arxiv. org/ abs/ 2009. 03300. Accessed 28 \nApr 2025.\nHofstadter, D., & Moser, D. (1989). To err is human to study error-making is cognitive science. Michigan \nQuarterly Review, 28(2), 185–215.\nHonda, U., Oka, T., Zhang, P., & Mita, M. (2024). Not eliminate but aggregate: Post-hoc control over \nmixture-of-experts to address shortcut shifts in natural language understanding. Transactions of the \nACLs, 12, 1268–1289. https:// doi. org/ 10. 1162/ tacl_a_ 00701\nHorn, L. R., & Wansing, H. (2024). Negation. In E. N. Zalta & U. Nodelman (Eds.), The Stanford Ency -\nclopedia of Philosophy (Spring 2024). Metaphysics Research Lab, Stanford University. Retrieved \nMarch 16, 2025, from https:// plato. stanf ord. edu/ archi ves/ spr20 24/ entri es/ negat ion/\nHuang, L., Yu, W., Ma, W., Zhong, W., Feng, Z., Wang, H., Chen, Q., Peng, W., Feng, X., Qin, B., & \nLiu, T. (2025). A survey on hallucination in large language models: Principles, taxonomy, chal-\nlenges, and open questions.  ACM Transactions on Information Systems, 43(2). https:// doi. org/ 10. \n1145/ 37031 55\nJang, M., & Lukasiewicz, T. (2023). Consistency Analysis of ChatGPT. In H. Bouamor, J. Pino, & K. \nBali (Eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Pro-\ncessing (pp. 15970–15985). https:// doi. org/ 10. 18653/ v1/ 2023. emnlp- main. 991\nJi, J., Qiu, T., Chen, B., Zhang, B., Lou, H., Wang, K., Duan, Y., He, Z., Zhou, J., Zhang, Z., Zeng, F., \nNg, K. Y., Dai, J., Pan, X., O’Gara, A., Lei, Y., Xu, H., Tse, B., Fu, J., McAleer, S., Yang, Y., \nWang, Y., Zhu, S.-C., Guo, Y., & Gao, W. (2023a). AI alignment: A comprehensive survey. arXiv. \nhttps:// arxiv. org/ abs/ 2310. 19852. Accessed 10 Dec 2024.\nJi, Z., Lee, N., Frieske, R., Yu, T., Su, D., Xu, Y., Ishii, E., Bang, Y. J., Madotto, A., & Fung, P. (2023b). \nSurvey of Hallucination in Natural Language Generation. ACM Comput. Surv., 55(12). https:// doi. \norg/ 10. 1145/ 35717 30\nJosifoski, M., Peyrard, M., Rajič, F., Wei, J., Paul, D., Hartmann, V., Patra, B., Chaudhary, V., Kiciman, \nE., & Faltings, B. (2023). Language Model Decoding as Likelihood–Utility Alignment. In A. Vla-\nchos & I. Augenstein (Eds.), Findings of the ACL: EACL 2023 (pp. 1455–1470). https:// doi. org/ 10. \n18653/ v1/ 2023. findi ngs- eacl. 107\nJumper, J., Evans, R., Pritzel, A., Green, T., Figurnov, M., Ronneberger, O., Tunyasuvunakool, K., Bates, \nR., Žídek, A., Potapenko, A., Bridgland, A., Meyer, C., Kohl, S. A. A., Ballard, A. J., Cowie, \n E. Bottazzi Grifoni, R. Ferrario \n61 Page 30 of 33\nA., Romera-Paredes, B., Nikolov, S., Jain, R., Adler, J., … Hassabis, D. (2021). Highly accu-\nrate protein structure prediction with AlphaFold. Nature, 596, 583–589. https:// doi. org/ 10. 1038/ \ns41586- 021- 03819-2\nJurafsky, D., & Martin, J. H. (2025). Speech and language processing: An introduction to natural lan-\nguage processing, computational linguistics, and speech recognition with language models (3rd \ned.). Retrieved March 16, 2025, from https:// web. stanf ord. edu/ ~juraf sky/ slp3/\nKalai, A. T., & Vempala, S. S. (2024). Calibrated language models must hallucinate. STOC 2024: Pro-\nceedings of the 56th Annual ACM Symposium on Theory of Computing (pp. 160–171). https:// doi. \norg/ 10. 1145/ 36182 60. 36497 77\nKassner, N., & Schütze, H. (2020). Negated and misprimed probes for pretrained language models: Birds \ncan talk, but cannot fly. In  Proceedings of the 58th Annual Meeting of the ACL (pp. 7811–7818). \nhttps:// aclan tholo gy. org/ 2020. acl- main. 698/\nKettelhoit, F. (2022). Wittgenstein on the diagonal argument (Doctoral dissertation, Universität Kassel). \nUniversität Kassel. https:// kobra. uni- kassel. de/ bitst ream/ handle/ 12345 6789/ 14054/ Disse rtati onFre \nderic Kette lhoit. pdf\nKim, C. Y., Lee, C. P., & Mutlu, B. (2024). Understanding large-language model (LLM)-powered human-\nrobot interaction. In  Proceedings of the 2024 ACM/IEEE International Conference on Human-\nRobot Interaction. https:// doi. org/ 10. 1145/ 36109 77. 36349 66\nKoshkin, S. (2021). Wittgenstein, Peirce, and paradoxes of mathematical proof. Analytic Philosophy, 62, \n252–274. https:// doi. org/ 10. 1111/ phib. 12177\nKripke, S. A. (1982). Wittgenstein on rules and private language: An elementary exposition. Harvard \nUniversity Press.\nKusch, M. (2016). A sceptical guide to meaning and rules: Defending Kripke’s Wittgenstein. Routledge.\nKuusela, O. (2019). Wittgenstein on logic as the method of philosophy: Re-examining the roots and devel-\nopment of analytic philosophy. Oxford University Press.\nKwan, W.-C., Zeng, X., Jiang, Y., Wang, Y., Li, L., Shang, L., Jiang, X., Liu, Q., & Wong, K.-F. (2024). \nMT-Eval: A multi-turn capabilities evaluation benchmark for large language models. In Y. Al-\nOnaizan, M. Bansal, & Y.-N. Chen (Eds.), Proceedings of the 2024 Conference on Empirical \nMethods in Natural Language Processing (pp. 20153–20177). https:// doi. org/ 10. 18653/ v1/ 2024. \nemnlp- main. 1124\nLee, Y., Son, K., Kim, T. S., Kim, J., Chung, J. J. Y., Adar, E., & Kim, J. (2024). One vs. Many: Com-\nprehending Accurate Information from Multiple Erroneous and Inconsistent AI Generations. Pro -\nceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, 2518–2531. \nhttps:// doi. org/ 10. 1145/ 36301 06. 36626 81\nLee, M. (2023). A Mathematical Investigation of Hallucination and Creativity in GPT Models. Math-\nematics, 11(10). https:// doi. org/ 10. 3390/ math1 11023 20\nLin, S.-W., Xu, R., Li, X., & Xu, W. (2025). Rules created by symbolic systems cannot constrain a learn-\ning system. SSRN. Retrieved 16 March, 2025, from https:// papers. ssrn. com/ sol3/ papers. cfm? abstr \nact_ id= 51211 27\nLiu, H., Ning, R., Teng, Z., Liu, J., Zhou, Q., & Zhang, Y. (2023). Evaluating the logical reasoning abil-\nity of chatgpt and gpt-4. arXiv. https:// arxiv. org/ abs/ 2304. 03439. Accessed 28 Apr 2025.\nMa, C., Zhang, J., Zhu, Z., Yang, C., Yang, Y., Jin, Y., Lan, Z., Kong, L., & He, J. (2024a). AgentBoard: \nAn analytical evaluation board of multi-turn LLM agents. In A. Globerson, L. Mackey, D. Bel-\ngrave, A. Fan, U. Paquet, J. Tomczak, & C. Zhang (Eds.), Advances in neural information pro-\ncessing systems (Vol. 37, pp. 74325–74362). Curran Associates, Inc. https:// proce edings. neuri ps. \ncc/ paper_ files/ paper/ 2024/ file/ 877b4 0688e 330a0 e2a3f c2408 4208d fa- Paper- Datas ets_ and_ Bench \nmarks_ Track. pdf\nMa, W., Yang, C., & Kästner, C. (2024b). (Why) Is my prompt getting worse? Rethinking regression test-\ning for evolving LLM APIs. In  2024 IEEE/ACM 3rd International Conference on AI Engineering \n– Software Engineering for AI (CAIN) (pp. 166–171). https:// doi. org/ 10. 1145/ 36448 15. 36449 50\nMacmillan-Scott, O., & Musolesi, M. (2025). (Ir)rationality in AI: State of the art, research challenges \nand open questions. arXiv. https:// arxiv. org/ abs/ 2311. 17165. Accessed 16 Mar 2025.\nMacmillan-Scott, O., & Musolesi, M. (2024). (Ir)rationality and cognitive biases in Large Language \nModels. Royal Society Open Science, 11(6), 240255. https:// doi. org/ 10. 1098/ rsos. 240255\nMahowald, K., Ivanova, A. A., Blank, I. A., Kanwisher, N., Tenenbaum, J. B., & Fedorenko, E. (2024). \nDissociating language and thought in large language models. Trends in Cognitive Sciences, 28(6), \n517–540.\nThe bewitching AI: The Illusion of Communication with Large…\nPage 31 of 33 61\nMarconi, D. (1984). Wittgenstein on contradiction and the philosophy of paraconsistent logic. History of \nPhilosophy Quarterly, 1(3), 333–352. https:// www. jstor. org/ stable/ 27743 690\nMcCoy, R. T., Yao, S., Friedman, D., Hardy, M., & Griffiths, T. L. (2023). Embers of Autoregression: \nUnderstanding Large Language Models Through the Problem They are Trained to Solve. arXiv. \nhttps:// arxiv. org/ abs/ 2309. 13638. Accessed 10 Dec 2024.\nMeinke, A., Schoen, B., Scheurer, J., Balesni, M., Shah, R., & Hobbhahn, M. (2025). Frontier models are \ncapable of in-context scheming. arXiv. arXiv: 2412. 04984 [cs.AI]. https:// doi. org/ 10. 48550/ arXiv. \n2412. 04984\nMikolov, T., Yih, W.-t., & Zweig, G. (2013). Linguistic regularities in continuous space word representa-\ntions. In Proceedings of the 2013 Conference of the North American Chapter of the ACL: Human \nLanguage Technologies (pp. 746–751). Retrieved March 16, 2025, from https:// aclan tholo gy. org/ \nN13- 1090/\nMontemayor, C. (2021). Language and Intelligence. Minds and Machines, 31(4), 471–486. https:// doi. \norg/ 10. 1007/ s11023- 021- 09568-5\nMousavi, S. M., Roccabruna, G., Alghisi, S., Rizzoli, M., Ravanelli, M., & Riccardi, G. (2024). Are LLMs \nRobust for Spoken Dialogues? arXiv. https:// arxiv. org/ abs/ 2401. 02297. Accessed 16 Mar 2025.\nMulligan, K. (2002). Getting Geist—Certainty, Rules and Us. In M. Ouelbani (Ed.), Cinquantenaire Lud-\nwig Wittgenstein, Proceedings of the 2001 Tunis Wittgenstein conference (pp. 35–62). University \nof Tunis.\nMündler, N., He, J., Jenko, S., & Vechev, M. (2024). Self-contradictory hallucinations of large language \nmodels: Evaluation, detection and mitigation. The Twelfth International Conference on Learning \nRepresentations. https:// openr eview. net/ pdf? id= EmQSO i1X2f\nO’Connor Drury, M. (2018). The Selected Writings of Maurice O’Connor Drury. On Wittgenstein Phi-\nlosophy Religion and Psychiatry. Bloomsbury Academic.\nÖhman, C. (2024). We are building gods: AI as the anthropomorphised authority of the past. Minds & \nMachines, 34(1). https:// doi. org/ 10. 1007/ s11023- 024- 09667-z\nOlausson, T., Gu, A., Lipkin, B., Zhang, C., Solar-Lezama, A., Tenenbaum, J., & Levy, R. (2023). \nLINC: A Neurosymbolic Approach for Logical Reasoning by Combining Language Models \nwith First-Order Logic Provers. In H. Bouamor, J. Pino, & K. Bali (Eds.), Proceedings of the \n2023 Conference on Empirical Methods in Natural Language Processing (pp. 5153–5176). \nhttps:// doi. org/ 10. 18653/ v1/ 2023. emnlp- main. 313\nOuyang, S., Zhang, J. M., Harman, M., & Wang, M. (2025). An empirical study of the non-determin-\nism of ChatGPT in code generation. ACM Transactions on Software Engineering and Methodol-\nogy, 34(2), 1–28. https:// doi. org/ 10. 1145/ 36970 10\nPadilla Galvez, J. (2009). Notas acerca de una paradoja burguesa. Episteme, 29(1), 126–148.\nPascal, B. (1995). Pensées. In A. Levi (Ed.), Pensées and other writings  (pp. 1–181). Oxford Univer -\nsity Press. (Original work published 1670)\nPeet, A. (2023). Understanding, luck, and communicative value. In A. Fairweather & C. Montemayor \n(Eds.), Linguistic Luck: Safeguards and Threats to Linguistic Communication (pp. 241–263). \nOxford University Press. https:// doi. org/ 10. 1093/ oso/ 97801 92845 450. 003. 0010 \nPeng, B., Narayanan, S., & Papadimitriou, C. (2024). On Limitations of the Transformer Architecture. \narXiv. https:// arxiv. org/ abs/ 2402. 08164. Accessed 16 Mar 2025.\nPérez-Escobar, J. A. (2023). A new role of mathematics in science: Measurement normativity. Meas-\nurement, 223, 113631. https:// doi. org/ 10. 1016/j. measu rement. 2023. 113631\nPérez-Escobar, J. A., & Sarikaya, D. (2022). Purifying applied mathematics and applying pure mathe-\nmatics: How a late Wittgensteinian perspective sheds light onto the dichotomy. European Jour -\nnal for Philosophy of Science, 12(1), 1. https:// doi. org/ 10. 1007/ s13194- 021- 00435-9\nPérez‐Escobar, J. A., & Sarikaya, D. (2024). Philosophical Investigations into AI Alignment: A \nWittgensteinian Framework. Philosophy & Technology, 37(80). https:// doi. org/ 10. 1007/  \ns13347- 024- 00761-9\nPersichetti, A. (2021). The later Wittgenstein’s guide to contradictions. Synthese, 198(4), 3783–3799. \nhttps:// doi. org/ 10. 1007/ s11229- 019- 02310-w\nPettit, D. (2002). Why Knowledge is Unnecessary for Understanding Language. Mind, 111(443), 519–\n550. https:// doi. org/ 10. 1093/ mind/ 111. 443. 519\nPrice, H. (1990). Why `Not’? Mind, 99(394), 221–238. https:// doi. org/ 10. 1093/ mind/ XCIX. 394. 221\nPriest, G. (2006). In contradiction. Oxford University Press.\n E. Bottazzi Grifoni, R. Ferrario \n61 Page 32 of 33\nQuine, W. V. O. (2013). Word and object (New ed.; P. S. Churchland, Foreword; D. Føllesdal, Pref-\nace). MIT Press. (Original work published 1960)\nScale AI. (2025). Scale Multi-Challenge. Retrieved March 4, 2025, from https:// scale. com/ leade  \nrboard/ multi chall enge\nSchwartz, R., & Stanovsky, G. (2022). On the limitations of dataset balancing: The lost battle against \nspurious correlations. In M. Carpuat, M.-C. de Marneffe, & I. V. Meza Ruiz (Eds.), Findings of \nthe ACL: NAACL 2022 (pp. 2182–2194). https:// doi. org/ 10. 18653/ v1/ 2022. findi ngs- naacl. 168\nShaw, J. R. (2023). Wittgenstein on rules: Justification, grammar, and agreement. Oxford University \nPress.\nSirdeshmukh, V., Deshpande, K., Mols, J., Jin, L., Cardona, E.-Y., Lee, D., Kritz, J., Primack, W., \nYue, S., & Xing, C. (2025). MultiChallenge: A realistic multi-turn conversation evaluation \nbenchmark challenging to frontier LLMs. arXiv. https:// doi. org/ 10. 48550/ arXiv. 2501. 17399\nSmith, A. L., Greaves, F., & Panch, T. (2023). Hallucination or confabulation? Neuroanatomy as \nmetaphor in large language models. PLOS Digital Health, 2(11), e0000388. https:// doi. org/ 10. \n1371/ journ al. pdig. 00003 88\nSobieszek, A., & Price, T. (2022). Playing games with AIs: The limits of GPT-3 and similar large lan-\nguage models. Minds and Machines, 32(2), 341–364. https:// doi. org/ 10. 1007/ s11023- 022- 09602-0\nSøgaard, A. (2023). Grounding the Vector Space of an Octopus: Word Meaning from Raw Text. Minds \nand Machines, 33(1), 33–54. https:// doi. org/ 10. 1007/ s11023- 023- 09622-4\nSrivastava, A., Rastogi, A., Rao, A., Shoeb, A., Abid, A., Fisch, A., Brown, A., Santoro, A., Gupta, A., \nGarriga-Alonso, A., Kluska, A., Lewkowycz, A., Agarwal, A., Power, A., Ray, A., Warstadt, A., \nKocurek, A. W., Safaya, A., Tazarv, A., ... Wu, Z. (2023). Beyond the imitation game: Quanti-\nfying and extrapolating the capabilities of language models. Transactions on Machine Learning \nResearch. https:// openr eview. net/ pdf? id= j3oQF 9coJd. Accessed 10 Dec 2024.\nSperber, D., Clément, F., Heintz, C., Mascaro, O., Mercier, H., Origgi, G., & Wilson, D. (2010). Epis-\ntemic vigilance. Mind & Language, 25(4), 359–393. https:// doi. org/ 10. 1111/j. 1468- 0017. 2010. \n01394.x\nTamir, M., & Shech, E. (2023). Machine understanding and deep learning representation. Synthese, \n201(2), 51. https:// doi. org/ 10. 1007/ s11229- 022- 03999-y\nTennant, N. (1987). Anti-Realism and Logic. Oxford University Press.\nvan Benthem, J., Liu, F., & Smets, S. (2021). Logico-computational aspects of rationality. The Handbook \nof Rationality (pp. 185–196). MIT Press. https:// doi. org/ 10. 7551/ mitpr ess/ 11252. 003. 0017\nVarzi, A. C., & Warglien, M. (2003). The geometry of negation. Journal of Applied Non-Classical Log-\nics, 13(1), 9–19. https:// doi. org/ 10. 3166/ jancl. 13.9- 19\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, \nI. (2017). Attention is all you need. In Proceedings of the 31st International Conference on Neural \nInformation Processing Systems (pp. 6000–6010). https:// proce edings. neuri ps. cc/ paper_ files/ paper/ \n2017/ file/ 3f5ee 24354 7dee9 1fbd0 53c1c 4a845 aa- Paper. pdf\nVerheggen, C. (2023). Linguistic luck and the publicness of language. In A. Fairweather & C. Mon-\ntemayor (Eds.), Linguistic Luck: Safeguards and threats to linguistic communication (pp. 222–\n240). Oxford University Press. https:// doi. org/ 10. 1093/ oso/ 97801 92845 450. 003. 0009\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y., Zhao, \nW. X., Wei, Z., & Wen, J. (2024). A survey on large language model based autonomous agents. \nFrontiers of Computer Science, 18(6), 186345. https:// doi. org/ 10. 1007/ s11704- 024- 40231-1\nWang, B., Huang, Q., Deb, B., Halfaker, A., Shao, L., McDuff, D., Awadallah, A. H., Radev, D., & Gao, \nJ. (2023). Logical Transformers: Infusing Logical Structures into Pre-Trained Language Models. In \nA. Rogers, J. Boyd-Graber, & N. Okazaki (Eds.), Findings of the ACL: ACL 2023 (pp. 1762–1773). \nhttps:// doi. org/ 10. 18653/ v1/ 2023. findi ngs- acl. 111\nWang, C. J., Lee, D., Menghini, C., Mols, J., Doughty, J., Khoja, A., Lynch, J., Hendryx, S., Yue, S., & \nHendrycks, D. (2025). EnigmaEval: A benchmark of long multimodal reasoning challenges. arXiv. \nhttps:// doi. org/ 10. 48550/ arXiv. 2502. 08859\nWilcox, J. E. (2024). Likelihood neglect bias and the mental simulations approach: An illustration using \nthe old and new Monty Hall problems. Judgment and Decision Making, 19, e14. https:// doi. org/ 10. \n1017/ jdm. 2024.8\nWinch, P. (1958). The idea of a social science and its relation to philosophy. Routledge and Kegan Paul\nWu, Z., Qiu, L., Ross, A., Akyürek, E., Chen, B., Wang, B., Kim, N., Andreas, J., & Kim, Y. (2024). \nReasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through \nCounterfactual Tasks. In K. Duh, H. Gomez, & S. Bethard (Eds.), Proceedings of the 2024 \nThe bewitching AI: The Illusion of Communication with Large…\nPage 33 of 33 61\nConference of the North American Chapter of the ACL: Human Language Technologies (Volume 1: \nLong Papers) (pp. 1819–1862). https:// doi. org/ 10. 18653/ v1/ 2024. naacl- long. 102\nXi, Z., Chen, W., Guo, X., et  al. (2025). The rise and potential of large language model based \nagents: A survey. Science China Information Sciences., 68, 121101. https:// doi. org/ 10. 1007/ \ns11432- 024- 4222-0\nXu, Z., Jain, S., & Kankanhalli, M. (2024). Hallucination is inevitable: An innate limitation of large lan-\nguage models. Preprint. arXiv. https:// doi. org/ 10. 48550/ arXiv. 2401. 11817\nXu, R., Li, X., Chen, S., & Xu, W. (2025). “Nuclear deployed!”: Analyzing catastrophic risks in decision-\nmaking of autonomous LLM agents. Preprint. arXiv. https:// doi. org/ 10. 48550/ arXiv. 2502. 11355\nZemach, E. M. (1995). Meaning, the experience of meaning and the meaning-blind in Wittgenstein’s late \nphilosophy. The Monist, 78(4), 480–495. https:// doi. org/ 10. 5840/ monis t1995 78429\nZhang, H., Li, L. H., Meng, T., Chang, K.-W., & Broeck, G. V. den. (2023). On the paradox of learning to \nreason from data. In Proceedings of the Thirty-Second International Joint Conference on Artificial \nIntelligence (IJCAI-23) (pp. 3365–3373). https:// doi. org/ 10. 24963/ ijcai. 2023/ 375\nZhang, Y., Chen, X., Jin, B., Wang, S., Ji, S., Wang, W., & Han, J. (2024). A comprehensive survey of \nscientific large language models and their applications in scientific discovery. arXiv. https:// arxiv. \norg/ abs/ 2406. 10833. Accessed 28 Apr 2025.\nZhang, C., Dai, X., Wu, Y., Yang, Q., Wang, Y., Tang, R., & Liu, Y. (2025). A survey on multi-turn inter-\naction capabilities of large language models. arXiv. https:// arxiv. org/ abs/ 2501. 09959. Accessed 28 \nApr 2025.\nZverev, E., Abdelnabi, S., Fritz, M., & Lampert, C. H. (2024). Can LLMs separate instructions from \ndata? And what do we even mean by that? In ICLR 2024 Workshop on Secure and Trustworthy \nLarge Language Models. Retrieved March 16, 2025, from https:// openr  eview. net/ forum? id= 32eyt \nC1Nt1\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations.",
  "topic": "Philosophy of technology",
  "concepts": [
    {
      "name": "Philosophy of technology",
      "score": 0.8283684849739075
    },
    {
      "name": "Illusion",
      "score": 0.6603131294250488
    },
    {
      "name": "Cognitive science",
      "score": 0.5105272531509399
    },
    {
      "name": "Epistemology",
      "score": 0.493943989276886
    },
    {
      "name": "Philosophy of science",
      "score": 0.4512866139411926
    },
    {
      "name": "Philosophy",
      "score": 0.43032127618789673
    },
    {
      "name": "Psychology",
      "score": 0.40139102935791016
    },
    {
      "name": "Linguistics",
      "score": 0.34411320090293884
    },
    {
      "name": "Computer science",
      "score": 0.3314852714538574
    },
    {
      "name": "Cognitive psychology",
      "score": 0.31308722496032715
    }
  ],
  "institutions": []
}