{
  "title": "KaLM at SemEval-2020 Task 4: Knowledge-aware Language Models for Comprehension and Generation",
  "url": "https://openalex.org/W3117841010",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A5079416324",
      "name": "Jiajing Wan",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    },
    {
      "id": "https://openalex.org/A5102524460",
      "name": "Xinting Huang",
      "affiliations": [
        "Sun Yat-sen University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2073302931",
    "https://openalex.org/W2998374885",
    "https://openalex.org/W2982346747",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W2987669390",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2947337775",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2972851234",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3113425182",
    "https://openalex.org/W2898695519"
  ],
  "abstract": "This paper presents our strategies in SemEval 2020 Task 4: Commonsense Validation and Explanation. We propose a novel way to search for evidence and choose the different large-scale pre-trained models as the backbone for three subtasks. The results show that our evidence-searching approach improves model performance on commonsense explanation task. Our team ranks 2nd in subtask C according to human evaluation score.",
  "full_text": "Proceedings of the 14th International Workshop on Semantic Evaluation, pages 543–550\nBarcelona, Spain (Online), December 12, 2020.\n543\nKaLM at SemEval-2020 Task 4: Knowledge-aware Language Models for\nComprehension And Generation\nJiajing Wan* Xinting Huang*\nSun Yat-Sen University\nGuangzhou, China\n{wanjj, huangxt39}@mail2.sysu.edu.cn\nAbstract\nThis paper presents our strategies in SemEval 2020 Task 4: Commonsense Validation and\nExplanation. We propose a novel way to search for evidence and choose the different large-scale\npre-trained models as the backbone for three subtasks. The results show that our evidence-\nsearching approach improves model performance on commonsense explanation task. Our team\nranks 2nd in subtask C according to human evaluation score.1\n1 Introduction\nCommonsense reasoning has been seen as one of the key ability for intelligent machines to perform\nvarious activities (Davis and Marcus, 2015). SemEval 2020 Task 4 (Wang et al., 2020) is a commonsense\nvalidation and explanation task which is inspired by Wang et al. (2019). This task consists of three\nsubtasks. The ﬁrst subtask is to choose the one which makes sense from two natural language statements\nwith similar wordings; The second subtask is to decide among three options the most crucial reason why a\ngiven statement does not make sense; The third subtask requires the machine to generate the reasons.\nTo make predictions or generate reasons, background knowledge is essential. A simple way to\nsupplement that knowledge is utilizing plain texts from natural language databases, e.g. Wikipedia.\nIntuitively, for a speciﬁc given statement, plain texts can be provided by searching similar sentences in\nthe database. In other words, each evidence sentences contain the keywords of the given statement. This\nmethod is used in some of the state-of-the-art models (Lv et al., 2019). However, for the purpose of\nexplaining the reason, evidence which has similar wording with the given statement may lack information,\nand sometimes can be misleading when the given statement does not make sense.\nIn this work, we propose a novel way for evidence-searching using plain texts. We obtain evidence\nby searching for the meaning of the keywords in the given statement. In other words, using evidence\ncontaining the meanings of the keywords rather than containing the keywords themselves. The reason for\nour method is that these meanings may provide important information to explain why a statement makes\nsense or does not make sense. For example in Figure 1, the deﬁnition of ”aircraft carrier”—”A warship\ndesigned to carry aircraft”—is given by the evidence obtained from the database. Such a deﬁnition well\nexplains why the given statement is wrong and can be used when generating the reason. In contrast,\nevidence containing both ”aircraft carrier” and ”human” will not be helpful.\nWe conduct experiments on subtask A, B, and C. Results show that our evidence-searching method\nboosts the performance on subtask C. Our team achieves accuracy of 95.3 (9th place) in subtask A and\n93.2 (7th place) in subtask B. In subtask C, Our approach achieves the BLEU score of 18.5 (3rd place)\nand human evaluation score of 2.08 (2nd place). Moreover, the best BLEU score in our experiments (20.4)\neven outperforms the score we obtain in competition.\n*denotes equal contribution.\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\n1The code is available at https://github.com/huangxt39/KaLM\n544\nTask Description Question example\nSubtask A Choose the one which makes\nsense\nStatement1: He put a turkey into the fridge.\nStatement2: He put an elephant into the fridge.\nSubtask B\nSelect the most corresponding\nreason why this statement is\nagainst common sense\nStatement: He put an elephant into the fridge.\nA: An elephant is much bigger than a fridge.\nB: Elephants are usually white while fridges\nare usually white.\nC: An elephant cannot eat a fridge.\nSubtask C\nGenerate the reason why this\nstatement is against common\nsense\nStatement: He put an elephant into the fridge.\nTable 1: A brief summarization of Subtask A, B, and C in SemEval 2020 Task 4.\n2 Related Work\nPre-trained Language models have been proved to be essential while dealing with sequence-to-sequence\ntask. We present the language model we utilized in this section and demonstrate our consideration. We\nuse RoBERTa (Liu et al., 2019) for language comprehension task. The structure of RoBERTa is based\non BERT (Devlin et al., 2018). We also use BART (Lewis et al., 2019) for generation task. BART is a\ndenoising autoencoder for pre-training sequence-to-sequence models. It uses the standard sequence-to-\nsequence Transformer architecture except the activation functions which is replaced by GeLUs. We ﬁnd\nthe structure of autoregressive decoder make the model can be directly ﬁne tuned for sequence generation\ntasks. BART is also enable to apply any type of document corruption which make the additional knowledge\navailable to the model.\nMachine common sense has long been acknowledged as a critical component for natural language\nunderstanding. The challenge for the model has turned to abstractive knowledge comprehension. A task\nthat is closely related to SemEval 2020 Task 4 is CommonsenseQA (Talmor et al., 2018), in which the\ncommonsense knowledge is required to make the correct prediction. In CommonsenseQA task, large-scale\npre-trained models have brought signiﬁcant performance gains. These gains are obtained by developing\ntraining strategies and enlarging training data (Liu et al., 2019), or improving parameter efﬁciency (Lan et\nal., 2019).\nWhile some of the improvement is achieved by developing the pre-trained model itself, some other\napproaches resort to external modules, e.g. knowledge extraction and graph-based reasoning in (Lv et\nal., 2019). In our work, we are interested in the knowledge extraction method because developing the\npre-trained model itself will be computationally expensive.\nMoreover, different from recent common strategy (Lin et al., 2019; Lv et al., 2019; Ma et al., 2019) to\nuse external knowledge which involves structured knowledge like ConceptNet (Speer et al., 2016), we\nextract external knowledge from plain texts.\n3 System Description\nWe ﬁrst describe our evidence-searching approach which can be utilized in downstream tasks. Then we\ndescribe our systems for three subtasks.\n3.1 Evidence-Searching Approach\nAs shown in Figure 1, we ﬁrst extract more than 1M two-element tuples (word, gloss) from Wiktionary2\nwith the help of Wiktextract package3 and adopt Elastic Search tools4 to index these tuples (note that the\nsame word can have many meanings, thus in practice the tuples actually consist of 3 elements, adding\n2Wiktionary version: enwiktionary-20200220\n3https://github.com/tatuylonen/wiktextract\n4https://www.elastic.co/\n545\nFigure 1: The Evidence-Searching Procedure\nan element which indicates the importance of this meaning). Then for each given statement, we extract\nits keywords with the help of Spacy5. For each keyword, we search for those tuples whose ”word” ﬁeld\nmatches the keyword. The Elastic Search engine ranks the matching score for tuples. We select top K\ntuples for each keyword. Thus the number of evidence tuples for a given statement is K*M (M denotes the\nnumber of keywords). In short, we search for the meaning of the keywords. Finally, the input sentences\nare produced by concatenating the original statement and corresponding evidence together. The detailed\nformat is varied according to different subtasks and will be discussed later.\n3.2 Systems for Subtask A&B\nBoth of subtask A&B are to select one among several choices. Thus we implement two of the same model\nfor subtaskA&B. In a nutshell, our model is RoBERTa (A Robustly Optimized BERT Pretraining Ap-\nproach)(Liu et al., 2019) since it has been found that large-scale pre-trained contextualized representation\nmasters a certain degree of commonsense knowledge (Zhou et al., 2019). This is also supported by our\nexperimental results.\nFor subtask A, we adapt pre-trained RoBERTaLARGE to subtask A dataset. We denote the hidden size\nfor each layer (transformer blocks) as H, aggregate sequence representation as C ∈RH (ﬁnal hidden\nstate corresponding to the special [CLS] word embedding). The task-speciﬁc parameters we introduce is\na vector V ∈RH. For each example, we simply input the two choice respectively and obtain the ﬁnal\naggregate representation Ci ∈RH for each choice iwhose dot product with the vector V denotes a score\nfor choice i. Thus the probability distribution is the softmax over the two choices:\nPi = eV ·Ci\n∑n\nj=1 eV ·Cj\n(1)\nndenotes the number of choices, which is 2 in subtask A. At testing time, the model’s predictionˆiis the\nchoice with the highest probability:\nˆi= arg max\ni\nPi (2)\nThe model is trained with back propagation, using negative log-likelihood as loss function.\nIn subtask B, given a statement that does not make sense, select the key reason from three options to\nexplain why it does not make sense. Adapting RoBERTa to subtask B dataset is similar to the adaptation\nfor subtask A. For each example, we construct three input sentences by concatenating statements with\nthree choices respectively. Then input these sentences respectively. We compute the score for each of\nchoice according to Equation 1 where n is 3 in subtask B. Then we follow the procedure described in\nsubtask A.\n5https://spacy.io/\n546\n3.3 System for Subtask C\nSubtask C is an NLG (Natural Language Generation) task, which is quite different from subtask A&B.\nMeanwhile, this subtask also requires commonsense knowledge and reasoning ability, which makes it\nmore challenging. Our model for subtask C is BART (Denoising Sequence-to-Sequence Pre-training\nfor Natural Language Generation, Translation, and Comprehension) (Lewis et al., 2019). BART is a\nsequence-to-sequence model using a standard Transformer-based neural machine translation architecture.\nIt is pre-trained by learning a model to reconstruct the original text from the corrupted text. It uses the\nsame pre-training data as RoBERTa (Liu et al., 2019), consisting of 160Gb of news, books, stories, and\nweb text.\nSpeciﬁcally, we adapt pre-trained BARTLARGE to subtask C dataset. For each ex-\nample, we follow our aforementioned evidence-searching approach to obtain evidence for\nthe given statement. In the subtask C dataset, each statement has 3 referential rea-\nsons. During training, we construct 3 new example for each example in original\ndataset (e.g., for one original example <statement,evidence,reason1,reason2,reason3 > we\nconstruct three new example <statement,evidence,reason1 >, <statement,evidence,reason2 >,\n<statement,evidence,reason3 >). Thus the total number of training examples is 3*N (N denotes\nthe number of training examples originally). We denote this method as Multi −target training since the\nsame input has multiple different targets. For each new example, the input sentence is the concatenation\nof the statement and evidence. Because BART has an autoregressive decoder, it can be directly ﬁne-tuned\nfor such a sequence generation task and can generate outputs autoregressively.\n4 Experiments\nWe use the ofﬁcially released dataset and standard train/trial/dev split of SemEval 2020 task 4 for\nexperiments. We will give the performance of the best settings on test split. Note that we compare\ndifferent settings through performance obtained by training the model on train&trial split and testing it on\ndev split since testing on test split is inconvenient. We will also give our conﬁguration of ﬁnal submissions\nfor subtask A, B, C in section 4.1, 4.2, 4.3\n4.1 Experiments for Subtask A\nWe implement RoBERTa in FAIRSEQ (Ott et al., 2019). RoBERTa is optimized with Adam (Kingma and\nBa, 2014) with the following parameters: β1 = 0.9,β2 = 0.98,ϵ = 1e−6 and L2 weight decay of 0.01.\nThe learning rate is warmed up over the ﬁrst 800 steps to a peak value of 1e-5, and then polynomially\ndecayed. The clipping threshold of gradients is 0.1. RoBERTa is ﬁne-tuned with a dropout of 0.1 on\nall layers and attention weights. It is ﬁne-tuned for S=8,000 updates, with mini-batches containing B=8\nsequences of maximum length T=512 tokens.\nIn experiments where we add evidence, for each statement we have several tuples (word, gloss).\nThe evidence for the statement is in such format: ” <word1 >: <gloss1 >\\<word2 >: <gloss2 >\n\\···”. We construct two input sentences for each example in such format: ”<Statement1 >Context:\n<Evidence1 >”, ”<Statement2 >Context: <Evidence2 >”. Note that due to the unavoidable mem-\nory limitation problem, we use memory efﬁcient ﬂoating point numbers option provided by FAIRSEQ\nwhen we add evidence.\nWe train the model on 1 ×11GB GeForce RTX 2080 GPU for around 15 minutes when we input\nstatements without evidence and 2 ×GPUs for around 100 minutes when we add evidence.\nWe notice that there are some statements whose letters are all capitalized (e.g., A GIRL WON THE\nRACE WITH HER FRIEND) in Subtask A dataset. We capitalize the ﬁrst letter and make other letters in\nlowercase. We denote this operation as Lowercase in Table 2.\nAs shown in Table 2, we can see that the performance is slightly improved after we make some letter\nlowercase, since otherwise different forms of a word can be mapped to different embeddings while they\nhave the same meaning. Then we explore the effect of evidence. By adding evidence to the input statement,\nwe obtain a slightly better result in the development dataset while performance degradation is found in the\ntest dataset. We hypothesize this discrepancy is because the amount of noise data in evidence is unstable.\n547\nDev Acc Test Acc\nRoBERTaLARGE (our implementation) 95.4 -\n+ Lowercase 95.7 95.3\n+ Lowercase + Evidence 96.3 94.1\nTable 2: Model performance on the ofﬁcial subtask A development set and test set. Lowercase: make all\nthe letters in lowercase except for the ﬁrst letter. Evidence: add evidence from wiktionary\nDev Acc Test Acc\nRoBERTaLARGE (our implementation) 91.6 -\n+ Extra Words 93.2 93.2\n+ Extra Words + Reasonable Statement 93.2 -\n+ Extra Words + Wiktionary 92.4 -\n+ Extra Words + Reasonable Statement + Wiktionary 93.1 -\nTable 3: Model performance on the ofﬁcial subtask B development set and test set. Extra Words:\nextra words indicating given statement does not make sense. Reasonable Statement: the corresponding\nreasonable statement. Wiktionary: the obtained evidence from wiktionary\nOur conﬁguration of the ﬁnal submission of subtask A has the same setting discussed in the ﬁrst\nparagraph, with the lowercase operation, and without additional evidence. Our approach achieves the\naccuracy of 95.3% on subtask A test dataset\n4.2 Experiments for Subtask B\nWe primarily follow the optimization hyperparameters, given in Section 4.1, except for the batch size,\nnumber of warmup steps, and number of total updates which are 4, 500, and 10,000 separately. In the\nfollowing experiments, we also follow the lowercase operation in Section 4.1 as part of the default setting.\nIn subtask A, given two similar statements, one makes sense while another one does not. In subtask B,\nthe one that does not make sense is given, thus the other one—the statement that makes sense—can be\nused as a kind of evidence since the different words between two statements may be the keywords for\nexplaining why given statement does not make sense. We denote this evidence as Reasonable Statement,\nthe evidence from wiktionary as Wiktionary.\nIn experiments where we do not add any evidence, we construct input sentences in such format ”The\nstatement ’<Statement >’ is absurd. Because<Choicei >” in which we concatenate some additional\nwords (”The statement ’···’ is absurd. Because···”) to the sentence and denote this technique as\nExtra Words. Moreover, in experiments where we add the reasonable statement, the input format for each\nchoice is ”Reasonable statement:<Reasonable Statement>\\The statement ’<Statement >’ is ab-\nsurd. Because<Choicei >”. If the evidence is added, then the format will be”Context: <Wiktionary >\nReasonable statement: <Reasonable Statement>\\The statement ’<Statement >’ is absurd. Be-\ncause <Choicei >”\nIn Table 3 we present the result of different settings. We see that the extra words can bring 1.6%\nabsolute improvement since they indicate that the given statement does not make sense and the choice is\nthe reason for that. We also see that using corresponding reasonable statement and wiktionary evidence\nachieve comparable performance while they involve extra computational cost. Therefore, we only use\nextra words in the ﬁnal submission of subtask B and achieve accuracy of 93.2%\n4.3 Experiments for Subtask C\nBART is also implemented in FAIRSEQ (Ott et al., 2019). It is optimized with Adam (Kingma and Ba,\n2014) with the following parameters: β1 = 0.9,β2 = 0.999,ϵ = 1e−8 and L2 weight decay of 0.01.\nThe learning rate is warmed up over the ﬁrst 500 steps to a peak value of 3e-5, and then polynomially\ndecayed. The clipping threshold of gradients is 0.1. BART is ﬁne-tuned with a dropout of 0.1 on all\n548\nDev BLEU Test BLEU\nBARTLARGE (our implementation) 15.15 -\n+Multi-target 19.66 19.43\n+Multi-target + Extra Words 18.10 -\n+Multi-target + Wiktionary 18.98 -\n+Multi-target + Extra Words + Wiktionary 19.50 -\n+Multi-target + Extra Words + Reasonable Statement 18.98 -\n+Multi-target + Extra Words + Reasonable Statement + Wiktionary 20.03 20.39\nTable 4: Model performance on the ofﬁcial subtask C development set and test set. Multi-target: the same\ninput has multiple different targets. Extra Words: extra words indicating given statement does not make\nsense. Reasonable Statement: the corresponding reasonable statement. Wiktionary: the obtained evidence\nfrom wiktionary\nlayers and attention weights. It is ﬁne-tuned for S=1,200 updates, with mini-batches containing B=32\nsequences of maximum length T=512 tokens. Note that we keep the same experiment settings in the\nfollowing experiments. We train the model on 4 ×11GB GeForce RTX 2080 GPU, for 15 minutes to 1\nhour according to different settings.\nIn the following experiments, we follow the input format described in Section 4.2, removing the\n”<Choicei >” only. We conduct experiments on different combinations of the methods described above\n(Multi-target: Section 3.3; Extra Words, Reasonable Statement, and Wiktionary: Section 4.2) and explore\nthe effect of them.\nAs shown in Table 4, by using Multi-target training, we can obtain a 4.51 improvement on BLEU score.\nCompare to the baseline where we simply use the ﬁrst referential answer of each example in training\ndata as the target of the model output, the Multi-target method provides a larger amount of training data\nand thus helps the model get better performance. From Table 4 we can see performance degradation\nappears as we add some extra material but then performance improved as we add more material. We\nhypothesize the degradation is because the complexity of the input sentence increases as we add extra\nmaterial. When Extra Words, Reasonable Statement, and Wiktionary are all added, the beneﬁts outweigh\nthe disadvantages they bring. Therefore, we use all of them in the ﬁnal submission of subtask C and\nachieve the BLEU score of 18.5 (3rd place) and human evaluation score of 2.08 (2nd place), which obtains\na 0.14 gain over 3rd place and only 0.02 less than 1st place.\nThe best score (20.39) on the test set in Table 4 outperforms the score we achieved during the competition\n(18.5). Note that we might achieve a better human evaluation score accordingly. There are two reasons for\nthat. Firstly, we optimize our evidence-searching approach after the competition, improving the quality\nof the evidence (All the experiments with evidence added in this paper are using the improved version.\nDetails of optimizing method are shown in Appendices). Secondly, we observe that during the training\nprocess, the model performs well at the beginning but turns to mess later. Thus it’s difﬁcult to choose the\nbest model during the training process when we cannot evaluate it on the test set. When the competition\nhas ended, however, we can evaluate our models and choose the best one during the training process.\n5 Conclusion\nIn this work, we choose the different large-scale pre-trained models as the backbone for three subtasks\nand propose a novel way to search for evidence, which aims to obtain the meaning of the keywords in\nthe given statement. Our experiments demonstrate the importance of additional knowledge for language\nmodels to understand the content. The results show that our evidence-searching approach is helpful to\ncommonsense explanation task.\n549\nReferences\nErnest Davis and Gary Marcus. 2015. Commonsense reasoning and commonsense knowledge in artiﬁcial intelli-\ngence. September.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019.\nAlbert: A lite bert for self-supervised learning of language representations.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoy-\nanov, and Luke Zettlemoyer. 2019. Bart: Denoising sequence-to-sequence pre-training for natural language\ngeneration, translation, and comprehension.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xiang Ren. 2019. Kagnet: Knowledge-aware graph networks for\ncommonsense reasoning.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR,\nabs/1907.11692.\nShangwen Lv, Daya Guo, Jingjing Xu, Duyu Tang, Nan Duan, Ming Gong, Linjun Shou, Daxin Jiang, Guihong\nCao, and Songlin Hu. 2019. Graph-based reasoning over heterogeneous external knowledge for commonsense\nquestion answering.\nKaixin Ma, Jonathan Francis, Quanyang Lu, Eric Nyberg, and Alessandro Oltramari. 2019. Towards generalizable\nneuro-symbolic systems for commonsense question answering.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli.\n2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48–53,\nMinneapolis, Minnesota, June. Association for Computational Linguistics.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2016. Conceptnet 5.5: An open multilingual graph of general\nknowledge. CoRR, abs/1612.03975.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2018. Commonsenseqa: A question an-\nswering challenge targeting commonsense knowledge. CoRR, abs/1811.00937.\nCunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian Gao. 2019. Does it make sense? and why?\nA pilot study for sense making and explanation. CoRR, abs/1906.00363.\nCunxiang Wang, Shuailong Liang, Yili Jin, Yilong Wang, Xiaodan Zhu, and Yue Zhang. 2020. SemEval-2020 task\n4: Commonsense validation and explanation. In Proceedings of The 14th International Workshop on Semantic\nEvaluation. Association for Computational Linguistics.\nXuhui Zhou, Yue Zhang, Leyang Cui, and Dandan Huang. 2019. Evaluating commonsense in pre-trained language\nmodels.\nAppendices\nWe optimize our evidence-searching approach after the competition. That is, ﬁltering out some meaning-\nless word-gloss pairs while keeping the framework unchanged. We observe two kinds of meaningless\npairs in the evidence. First, pairs contain misleading gloss, e.g., when we search the gloss for the key-\nword ”car”, we obtain ”CAR: initialism of ’Central African Republic’ CAR: (uncountable chemistry)\nabbreviation of ’carnitine’”; when we search the gloss for ”like”, ”like like: (slang) To fancy; to be\nattracted to” is obtained. To address this problem, we remove the pairs whose gloss contains ”initial-\nism/historical/obsolete/abbreviation/(dated)/slang/acronym/(US)/synonym/archaic/surname/(rare)”, and\nremove the pairs whose word contains ”-” or capital letters. Replacing these with other pairs. Second,\nwe also observe many pairs only contain prototype information. For example: ”watermelons: plural of\n’watermelon’”, ”concentrated: past of ’concentrate’”, ”facebook: alternative form of ’Facebook’”. To\n550\nobtain more meaningful gloss, we search the pairs for the prototype word, regarding the prototype word as\none of the keywords. Speciﬁcally, we detect whether the gloss contain words like ”plural of/past of/third\nperson singular of/clipping of/alternative form of/alternative spelling of” which indicate the gloss points\nto a prototype word. In that case, we use the same search function to acquire evidence for prototype word\nand incorporate the new evidence. (note that to avoid inﬁnite loop, we do not detect the prototype in\nsub-search). Besides, we also adjust the number of evidence tuples for a keyword dynamically. For the\nstatement contains less keywords, we obtain more evidence tuples for each keywords. Thus the length of\nthe evidence for statements will be more stable.",
  "topic": "SemEval",
  "concepts": [
    {
      "name": "SemEval",
      "score": 0.9319223165512085
    },
    {
      "name": "Computer science",
      "score": 0.84058678150177
    },
    {
      "name": "Task (project management)",
      "score": 0.8055523037910461
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6466578841209412
    },
    {
      "name": "Natural language processing",
      "score": 0.6361472010612488
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.631858766078949
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.6083584427833557
    },
    {
      "name": "Comprehension",
      "score": 0.5781358480453491
    },
    {
      "name": "Language model",
      "score": 0.5215111374855042
    },
    {
      "name": "Scale (ratio)",
      "score": 0.43694064021110535
    },
    {
      "name": "Programming language",
      "score": 0.12432718276977539
    },
    {
      "name": "Domain knowledge",
      "score": 0.11847639083862305
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157773358",
      "name": "Sun Yat-sen University",
      "country": "CN"
    }
  ],
  "cited_by": 6
}