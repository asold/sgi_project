{
    "title": "JUSTers at SemEval-2020 Task 4: Evaluating Transformer Models against Commonsense Validation and Explanation",
    "url": "https://openalex.org/W3115452684",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A5056848826",
            "name": "Ali Fadel",
            "affiliations": [
                "Jordan University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5073474306",
            "name": "Mahmoud Al‐Ayyoub",
            "affiliations": [
                "Jordan University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5100752356",
            "name": "Erik Cambria",
            "affiliations": [
                "Jordan University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2794557536",
        "https://openalex.org/W2897722020",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W4388152766",
        "https://openalex.org/W3094173182",
        "https://openalex.org/W2947337775",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963626623",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W1511480444",
        "https://openalex.org/W2788810909",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2102381086",
        "https://openalex.org/W3113425182",
        "https://openalex.org/W2911964244",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2970785793"
    ],
    "abstract": "In this paper, we describe our team's (JUSTers) effort in the Commonsense Validation and Explanation (ComVE) task, which is part of SemEval2020. We evaluate five pre-trained Transformer-based language models with various sizes against the three proposed subtasks. For the first two subtasks, the best accuracy levels achieved by our models are 92.90% and 92.30%, respectively, placing our team in the 12th and 9th places, respectively. As for the last subtask, our models reach 16.10 BLEU score and 1.94 human evaluation score placing our team in the 5th and 3rd places according to these two metrics, respectively. The latter is only 0.16 away from the 1st place human evaluation score.",
    "full_text": "Proceedings of the 14th International Workshop on Semantic Evaluation, pages 535–542\nBarcelona, Spain (Online), December 12, 2020.\n535\nJUSTers at SemEval-2020 Task 4: Evaluating Transformer Models\nAgainst Commonsense Validation and Explanation\nAli Fadel\nJordan University of Science & Tech\nIrbid, Jordan\naliosm1997@gmail.com\nMahmoud Al-Ayyoub\nJordan University of Science & Tech\nIrbid, Jordan\nmaalshbool@just.edu.jo\nErik Cambria\nNanyang Technological University, Singapore\ncambria@ntu.edu.sg\nAbstract\nIn this paper, we describe our team’s (JUSTers) effort in the Commonsense Validation and Explana-\ntion (ComVE) task, which is part of SemEval2020. We evaluate ﬁve pre-trained Transformer-based\nlanguage models with various sizes against the three proposed subtasks. For the ﬁrst two subtasks,\nthe best accuracy levels achieved by our models are 92.90% and 92.30%, respectively, placing our\nteam in the 12th and 9th places, respectively. As for the last subtask, our models reach 16.10 BLEU\nscore and 1.94 human evaluation score placing our team in the 5th and 3rd places according to\nthese two metrics, respectively. The latter is only 0.16 away from the 1st place human evaluation\nscore.\n1 Introduction\nAddressing the issue of commonsense understanding using deep learning algorithms and models has\nattracted increasing attention from the research community. Building a natural language processing\n(NLP) system that can understand both explicit and implicit knowledge, validate it and correct it before\nextracting important information is a task that is both hard and complex. At the same time, this task is\nessential to a large number of tasks like language modeling, word sense disambiguation and sentiment\nanalysis (Cambria et al., 2009).\nApproaches like KnowBERT (Peters et al., 2019) leverage knowledge bases like WordNet (Miller et\nal., 1990) and DBPedia. Such knowledge bases provide a rich source of high quality, human-curated\nknowledge to ﬁne-tune the internal hidden states of large language models like BERT (Devlin et al., 2018).\nOther approaches leverage external knowledge to enhance classiﬁcation, e.g., Sentic LSTM (Ma et al.,\n2018) tackled the problem of targeted aspect-based sentiment analysis by embedding the knowledge from\nSenticNet (Cambria et al., 2020) into an attentive long short-term memory (LSTM) network.\nThe Commonsense Validation and Explanation (ComVE) task (Wang et al., 2020) at SemEval2020 was\nproposed based on a dataset built by Wang et al. (Wang et al., 2019) to evaluate deep learning algorithms\nand models against commonsense tasks. The general purpose of the dataset is to test whether an NLP\nsystem can differentiate statements that make sense from those that do not. It consists of three subtasks:\n• Task A: Validation(Sentence Classiﬁcation) In this subtask, the system is given two natural language\nstatements with similar wordings and it is expected to distinguish the statement that makes sense\nfrom the one that does not. For example:\nStatement 1: He put a turkey into the fridge. correct\nStatement 2: He put an elephant into the fridge.\n• Task B: Explanation(Multiple Choice) In this subtask, a natural language statement that contradicts\ncommonsense is given along with three reasons why it does not make sense. The system is expected\nto choose the correct justiﬁcation. For example:\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/.\n536\nStatement: He put an elephant into the fridge.\nOptions:\n– A: An elephant is much bigger than a fridge. correct\n– B: Elephants are usually white while fridges are usually white.\n– C: An elephant cannot eat a fridge.\n• Task C: Explanation (Text Generation) This subtask is similar to the second one since the input to\nboth is a single natural language statement that contradicts commonsense and the goal is to determine\nwhy is that. However, in this subtask, the system is expected to generate the justiﬁcation from scratch.\nThe generated text is evaluated against three correct reference justiﬁcations. For example:\nStatement: He put an elephant into the fridge.\nReferential Reasons:\n– An elephant is much bigger than a fridge.\n– A fridge is much smaller than an elephant.\n– Most of the fridges aren’t large enough to contain an elephant.\nThe evaluation metric for Subtasks A and B is accuracy. As for Subtask C, the evaluation involves the\nBLEU score as an automatic evaluation metric in addition to a human evaluation score. For more detailed\ndiscussion and analysis of the dataset, please refer to (Wang et al., 2019).\nIn this work, we evaluate and ﬁne-tune ﬁve pre-trained Transformer-based (Vaswani et al., 2017)\nlanguage models with various sizes for the previously mentioned commonsense subtasks. For the ﬁrst\nsubtask, we use BERT (Devlin et al., 2018), RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2019)\nlanguage models. The best we achieve is 92.90% placing our team in the 12th place among 39 teams.\nWhile in the second subtask, we utilize XLNet (Yang et al., 2019) in addition to BERT and RoBERTa\nlanguage models to achieve 92.30% accuracy, which places our team in the 9th place out of 27 teams.\nFinally, in the last subtask, we used GPT-2 (Radford et al., 2019) language model to tackle the text\ngeneration task. Our system achieves 16.10 BLEU score and 1.94 human evaluation score, which places\nour team in the 5th place (out of 17 teams) and the 3rd place, respectively. Our system’s human evaluation\nscore is only 0.16 away from the top score in the competition. Our code and experimental results are\npublicly available in a GitHub repository.1\nThe rest of this paper is organized as follows. The following section demonstrates how we utilize the\npre-trained Transformer-based language models to build our systems for each subtask. In Section 3, we\npresent our experimental results and discuss some insights. Finally, we provide concluding remarks in\nSection 4.\n2 System Overview\nIn the following subsections we describe how we utilize the pre-trained Transformer-based language\nmodels to build our subtasks’ systems.\n2.1 Task A: Validation (Sentence Classiﬁcation)\nIn this subtask, we evaluate three Transformer-based language models, namely: BERT, RoBERTa and\nALBERT. All of these models were built on top of BERT with different training procedures and datasets\nand architectural enhancements in order to improve the resulting models.\nWe treat this task as a binary classiﬁcation problem. I.e., given two statements, one conforms with\ncommonsense and the other against it, the correct ones are labeled with 1s and the rest with 0s. Based on\nthis dataset, we ﬁne-tune the language models to perform binary classiﬁcation. To produce an inference\nusing this model, we input the correct and the wrong statements and get their probabilities of being correct\nor not from the model independently. After that, the statement with the lower probability is considered as\nthe one that is against commonsense. Figure 1a shows the task’s training and inference procedures.\n1https://github.com/AliOsm/SemEval2020-Task4-ComVE\n537\n2.2 Task B: Explanation (Multiple Choice)\nWe evaluate XLNet in addition to BERT and RoBERTa language models against the multiple choice\ntask. The approach is straightforward. Each one of the three given options (reasons) is concatenated\nindependently with the given statement (that is against commonsense). The concatenation is entered to\nthe model, and the model is ﬁne-tuned to select one of the three options as a multi-class classiﬁcation\nproblem.\nIn the inference phase, the same procedure in training is used to predict the correct reason based on the\nprobabilities given by the model using a Softmax layer. Figure 1b shows the task’s training and inference\nprocedures.\n2.3 Task C: Explanation (Text Generation)\nFor text generation task, we utilize GPT-2 language model to tackle the problem. The idea is to ﬁne-tune\nthe model to generate a reason that clariﬁes why the given statement is incorrect. The model is trained\nto generate the next word given the previous sequence of words. To train it, given the task dataset, we\nconcatenate the wrong statement with each of the given referential reasons independently (with a separator\nin-between). Thus, from each example, we construct three training instances.\nTo generate a reason why the given statement is against commonsense, we input the wrong statement\ninto the model followed by the same separator used in the training phase and ask the model to generate\ntext token-by-token until reaching the end of text token. Figure 1c shows the task’s training and inference\nprocedures.\n3 Experimental Results\nTo run our experiments with the targeted pre-trained Transformer-based language models, we use the\nGoogle Colab platform (Carneiro et al., 2018) and two open source Python packages, which are Trans-\nformers (Wolf et al., 2019)2 and SimpleTransformers.3 For each subtask, we report the development and\ntest sets results, training time and model size. More experimental results can be found in our GitHub\nrepository.\nIn the following subsections, we describe the models’ types and sizes that are used for each subtask,\ntheir results and some insights learned from our experiments.\n3.1 Task A: Validation (Sentence Classiﬁcation)\nFor this subtask, we use Nvidia Tesla K80 GPU from Google Colab platform to perform the experiments\nreported in Table 1. The learning rate is set to 4 ×10−5 and the maximum sequence length to 30 tokens in\nall experiments, while using training and evaluation batch sizes of 32. The models in Experiments A1 and\nA2 are trained for ten epochs, while the models in Experiments A3-A7 and A9 are trained for 15 epochs.\nFinally, Experiment A8’s model is trained for 20 epochs.\nAs shown in the table, using larger model size consistently leads to better results as expected. A\ncomparison between BERT’s cased and uncased models (Experiments A1-A6) shows that cased models\noutperforms uncased models signiﬁcantly, which means that the problem at hand is case-sensitive.\nRoBERTa base model (Experiment A7) outperforms all BERT models (base and large versions) on the\ndevelopment and test sets except for bert-large-cased model (Experiment A3), which is behind it\nby only 0.1%. RoBERTa large model (Experiment A8) is the best model in terms of accuracy on the test\nset. These results imply that RoBERTa models are more suitable than BERT models if we want to treat\neach of them as a knowledge base and use it to extract or validate facts. Finally, ALBERT Xxlarge model\n(Experiment A9) results are not as good as RoBERTa models, however, it does help when performing the\nensemble models.\nOur submission (Experiment A11) is based on majority voting ensemble between four models (Exper-\niments A3, A7, A8 and A9). Note that we use ALBERT Xxlarge model results twice in the ensemble\n2https://github.com/huggingface/transformers\n3https://github.com/ThilinaRajapakse/simpletransformers\n538\n(a) Task A\n(b) Task B\n(c) Task C\nFigure 1: Subtasks training and inference procedures.\n539\nprocess, because it catches more challenging examples and improves the ensemble results on the de-\nvelopment set. This ensemble performs better than any other model on the development set. However,\nRoBERTa large model outperforms it on the test set by 0.1%.\nOther attempts we made to obtain better results include experimenting with different machine learning\ntechniques, such as random forests (Breiman, 2001) with term frequency-inverse document frequency\n(TF-IDF) features, fastText classiﬁcation (Joulin et al., 2016) and Universal Sentence Encoder (Cer et\nal., 2018) representations with either random forests or a simple feed-forward neural network. Among\nthese, the best technique is using Universal Sentence Encoder representations with random forests to do\nthe classiﬁcation. This leads to 80% accuracy on the test set, which is comparable to the results of the\nBERT base uncased model (Experiment A2). Finally, it is worth mentioning that we also try to use paired\nsentences classiﬁcation (Devlin et al., 2018) instead of sentence classiﬁcation. However, the results do not\nimprove.\nExpr # Model Dev Acc. Test Acc. Train Time Size\nA1 bert-base-cased 85.56% 87.30% 43.64m 433.3 MB\nA2 bert-base-uncased 81.54% 81.40% 44.98m 438 MB\nA3 bert-large-cased 89.87% 89.40% 202.92m 1.3 GB\nA4 bert-large-uncased 85.36% 83.30% 213.02m 1.3 GB\nA5 bert-large-cased-whole-word-masking 88.97% 86.70% 208.60m 1.3 GB\nA6 bert-large-uncased-whole-word-masking 83.05% 83.30% 199.93m 1.3 GB\nA7 roberta-base 89.77% 89.30% 67.03m 501 MB\nA8 roberta-large 92.78% 93.00% 280.51m 1.4 GB\nA9 albert-xxlarge-v2 86.96% 86.60% 1050m 891.2 MB\nA10 ensemble 3+6+7+8+9 92.98% 92.80% N/A N/A\nA11 ensemble 3+7+8+9+9 93.08% 92.90% N/A N/A\nTable 1: Task A: Validation (Sentence Classiﬁcation) Experimental Results\n3.2 Task B: Explanation (Multiple Choice)\nAll experiments related to this subtask (Table 2) are run on Nvidia Tesla P100 GPU from Google Colab\nplatform. The learning rate is set to 4 × 10−5 for all experiments except for Experiments B8 and B10,\nwhere we use 1 ×10−5 as the learning rate. The maximum sequence length is set to 40 for all experiments,\nwhile using training and evaluation batch sizes of 32. The models in Experiments B1, B2, B7 and B9 are\ntrained for ﬁve epochs, while models in Experiments B3-B6 are trained for ten epochs. Finally, models in\nExperiments B8 and B10 are trained for 20 epochs.\nConsistent with the observation from Task A, large models achieve better results than the base versions.\nHowever, in contrast with Task A, the cased and uncased versions of BERT models (Experiments B1-B6)\nalmost converge to the same results. Comparing RoBERTa base model (Experiment B7) results with BERT\nmodels results shows that it is on par with or better than all BERT models (base and large versions), while\nRoBERTa large model (Experiment B8) performs better than the other models on both the development\nand test sets. Hence, our submission is based on its predictions. This is consistent with our observations\nfrom Task A results. Finally, the XLNet models (Experiments B9 and B10) results are not as good as\nRoBERTa models, but it outperforms BERT models by signiﬁcant margin on the development and test\nsets.\n3.3 Task C: Explanation (Text Generation)\nSimilarly with Task B, we use Nvidia Tesla P100 GPU from Google Colab platform to run the experiments\nof Task C whose results are reported in Table 4. We use5 ×10−5 as the learning rate and set the maximum\nsequence length to 128. We use training batch size of 64, while predicting reasons for each example\nindependently (one example at a time). The model in Experiment C1 is trained for 15 epochs, while the\n540\nExpr # Model Dev Acc. Test Acc. Train Time Size\nB1 bert-base-cased 81.95% 80.10% 10.5m 433.5 MB\nB2 bert-base-uncased 82.65% 81.50% 10.5m 438.2 MB\nB3 bert-large-cased 85.46% 85.80% 76m 1.3 GB\nB4 bert-large-uncased 85.46% 85.30% 74.5m 1.3 GB\nB5 bert-large-cased-whole-word-masking 87.26% 87.10% 74.5m 1.3 GB\nB6 bert-large-uncased-whole-word-masking 88.16% 88.30% 74.5m 1.3 GB\nB7 roberta-base 86.86% 87.50% 11.16m 500 MB\nB8 roberta-large 92.38% 92.30% 149m 1.4 GB\nB9 xlnet-base-cased 84.25% 85.40% 10m 470.1 MB\nB10 xlnet-large-cased 90.27% 88.70% 177m 1.4 GB\nTable 2: Task B: Explanation (Multiple Choice) Experimental Results\nmodels in Experiments C2 and C3 are trained for ﬁve epochs. Finally, the model in Experiment C4 is\ntrained for one epoch only.\nUsing a small model, like distilled GPT-2 (Experiment C1) from HuggingFace (Sanh et al., 2019),4\ngives good results compared with the larger versions of GPT-2 models. We notice that better BLEU\nscores are achieved as we use larger models. This trend changes when we reach the GPT-2 large model\n(Experiment C4), which produces lower BLEU score on development and test sets compared with GPT-2\nmedium model (Experiment C3). This decrease may have been caused by the huge amount of parameters\nin GPT-2 large model (∼774M) and the small size of the dataset. Even ﬁne-tuning the model for less than\none entire epoch did not lead to better results. We do not use GPT-2 xlarge model because of memory\nissues on a single 16GB GPU. Our submission is based on GPT-2 medium model predictions.\nTable 3 shows some examples from development dataset paired with their generated reasons. In the\nﬁrst ﬁve examples, there is a trend where the model negates the entered sentence without producing a\nfactual reason. We can see that clearly in the ﬁfth example, where the model negates the two parts of\nthe statement even if the second part “and can roll” is correct and this trend appears frequently in the\ngenerated reasons. The bias for negating the entered sentence that appears in the generated reasons could\nbe attributed to a bias in the training dataset. On the other hand, for the last ﬁve examples, the model\ngenerates very clear reasons why the entered statement is against commonsense. For example, in “you\ncan eat mercury” statement, the model generated “mercury is poisonous”. So, we can safely say that the\nmodel knows what mercury is and knows some of its properties.\nAgainst Commonsense Statement Generated Reason\nChicken can swim in water. Chicken can’t swim.\nshoes can ﬂy Shoes are not able to ﬂy.\nChocolate can be used to make a coffee pot Chocolate is not used to make coffee pots.\nyou can also buy tickets online with an identity card You can’t buy tickets with an identity card.\na ball is square and can roll A ball is round and cannot roll.\nYou can use detergent to dye your hair. Detergent is used to wash clothes.\nyou can eat mercury mercury is poisonous\nA gardener can follow a suspect gardener is not a police ofﬁcer\ncars can ﬂoat in the ocean just like a boat Cars are too heavy to ﬂoat in the ocean.\nI am going to work so I can lose money. Working is not a way to lose money.\nTable 3: Against Commonsense Statements with Generated Reasons\nIt is worth mentioning that we try to do an ensemble between the best three models (Experiments\nC2-C4) by entering the reasons generated by GPT-2 model into the best model from Task B and using it\n4https://huggingface.co\n541\nto select the best reason. However, the results do not improve.\nExpr # Model Dev BLEU Test BLEU Train Time Size\nC1 distilgpt2 13.7582 13.8026 29m 354.2 MB\nC2 gpt2 14.0547 13.6534 15m 549.5 MB\nC3 gpt2-medium 16.7153 16.1187 39.33m 1.5 GB\nC4 gpt2-large 16.5110 15.9299 35.66m 3.2 GB\nTable 4: Task C: Explanation (Text Generation) Experimental Results\nWe note that the results we have discussed so far for Task C are based on the BLEU metric. Despite its\noverwhelming popularity in text generation tasks, this metric is known to have many ﬂaws (Zhao et al.,\n2019). Fortunately, for the task at hand, the organizers provide human evaluation scores for subsets of the\nreasons generated by the participating systems computed as follows. They asked three human annotators\nto evaluate 100 randomly selected samples of the test set for each system. The rubrics are as follows.5\n0. The reason is not grammatically correct, or not comprehensible at all, or not related to the statement\nat all.\n1. The reason is just the negation of the statement, or a simple paraphrase. Obviously, a better\nexplanation can be made.\n2. The reason is relevant and appropriate, though it may contains a few grammatical errors or irrelevant\nparts. Or, it might be like case 1, but it is hard to write a proper reason.\n3. The reason is appropriate and is a solid explanation of why the statement does not make sense.\nOur system obtains a human evaluation score of 1.94, which means that the expected reasons outputted\nfrom it are not perfect, but they are relevant and appropriate (if the given statement is not difﬁcult to\njustify why it is against commonsense). On the other hand, there are systems that achieved higher BLEU\nscores than our system, but their human evaluation scores are much lower than our systems. They are\neven close to 1, which means that the expected reasons outputted from them are not much better than a\nsimple negation or paraphrasing of the given statements. At the end of the day, a model as simple as ours\nwith its ability to achieve competitive results in the ﬁrst two tasks and very satisfactory results for the\nlast task, represents an appealing option for a production system for the task at hand. To further aid the\nreproducibility and practicality of our work, we provide our Task C ﬁne-tuned models at HuggingFace\nmodels hub, 6 where anyone can use them using four lines of code.\n4 Conclusion\nIn this work, we evaluated pre-trained Transformer-based language models against three commonsense\ntasks as part of the Commonsense Validation and Explanation (ComVE) task, which is part of Sem-\nEval2020. Our experiments showed that pre-trained language models can be treated as powerful knowledge\nbases to extract and validate facts. We were ranked in the 12th and 9th for ﬁrst and second subtasks,\nrespectively. As for third subtask, we were ranked in the 5th and 3rd places using automatic (BLEU) and\nhuman evaluation metrics, respectively. We provide the code and the experimental results through our\nGitHub repository.\nAcknowledgments\nWe thank Google Colaboratory7 for providing such platform that can be used to run machine learning and\ndeep learning experiments on accelerated hardware like Nvidia Tesla GPUs for free.\n5https://bit.ly/2W8NF0W\n6https://huggingface.co/models?search=ComVE\n7https://colab.research.google.com\n542\nReferences\nLeo Breiman. 2001. Random forests. Machine learning, 45(1):5–32.\nErik Cambria, Amir Hussain, Catherine Havasi, and Chris Eckl. 2009. Common sense computing: From the soci-\nety of mind to digital intuition and beyond. In Julian Fierrez, Javier Ortega, Anna Esposito, Andrzej Drygajlo,\nand Marcos Faundez-Zanuy, editors, Biometric ID Management and Multimodal Communication, volume 5707\nof Lecture Notes in Computer Science, pages 252–259. Springer, Berlin Heidelberg.\nErik Cambria, Yang Li, Frank Z Xing, Soujanya Poria, and Kenneth Kwok. 2020. Senticnet 6: Ensemble applica-\ntion of symbolic and subsymbolic ai for sentiment analysis. In CIKM.\nTiago Carneiro, Raul Victor Medeiros Da N´obrega, Thiago Nepomuceno, Gui-Bin Bian, Victor Hugo C De Albu-\nquerque, and Pedro Pedrosa Reboucas Filho. 2018. Performance analysis of google colaboratory as a tool for\naccelerating deep learning applications. IEEE Access, 6:61677–61685.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario\nGuajardo-Cespedes, Steve Yuan, Chris Tar, et al. 2018. Universal sentence encoder. arXiv preprint\narXiv:1803.11175.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirec-\ntional transformers for language understanding. arXiv preprint arXiv:1810.04805.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. Bag of tricks for efﬁcient text\nclassiﬁcation. arXiv preprint arXiv:1607.01759.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2019.\nAlbert: A lite bert for self-supervised learning of language representations. arXiv preprint arXiv:1909.11942.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nYukun Ma, Haiyun Peng, and Erik Cambria. 2018. Targeted aspect-based sentiment analysis via embedding\ncommonsense knowledge into an attentive LSTM. In AAAI, pages 5876–5883.\nGeorge A Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J Miller. 1990. Introduction\nto wordnet: An on-line lexical database. International journal of lexicography, 3(4):235–244.\nMatthew E Peters, Mark Neumann, IV Logan, L Robert, Roy Schwartz, Vidur Joshi, Sameer Singh, and Noah A\nSmith. 2019. Knowledge enhanced contextual word representations. arXiv preprint arXiv:1909.04164.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners. OpenAI Blog, 1(8):9.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of bert:\nsmaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and\nIllia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages\n5998–6008.\nCunxiang Wang, Shuailong Liang, Yue Zhang, Xiaonan Li, and Tian Gao. 2019. Does it make sense? and why? a\npilot study for sense making and explanation. arXiv preprint arXiv:1906.00363.\nCunxiang Wang, Shuailong Liang, Yili Jin, Yilong Wang, Xiaodan Zhu, and Yue Zhang. 2020. SemEval-2020 task\n4: Commonsense validation and explanation. In Proceedings of The 14th International Workshop on Semantic\nEvaluation. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-\nart natural language processing. ArXiv, abs/1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ R Salakhutdinov, and Quoc V Le. 2019. Xlnet:\nGeneralized autoregressive pretraining for language understanding. In Advances in neural information process-\ning systems, pages 5754–5764.\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. 2019. Mover-\nscore: Text generation evaluating with contextualized embeddings and earth mover distance. arXiv preprint\narXiv:1909.02622."
}