{
  "title": "Generating Natural Language Adversarial Examples on a Large Scale with Generative Models",
  "url": "https://openalex.org/W3012613466",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5084848859",
      "name": "Yankun Ren",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102161232",
      "name": "Jianbin Lin",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5063062444",
      "name": "Siliang Tang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5045140292",
      "name": "Jun Zhou",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102831864",
      "name": "Shuang Yang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5063815625",
      "name": "Yuan Qi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5009408707",
      "name": "Xiang Ren",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2243397390",
    "https://openalex.org/W2798966449",
    "https://openalex.org/W2149741699",
    "https://openalex.org/W2949128310",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2963857521",
    "https://openalex.org/W2963834268",
    "https://openalex.org/W2799420921",
    "https://openalex.org/W2963619462",
    "https://openalex.org/W2962897886",
    "https://openalex.org/W2097726431",
    "https://openalex.org/W2172140247",
    "https://openalex.org/W2591788621",
    "https://openalex.org/W2940009958",
    "https://openalex.org/W2964153729",
    "https://openalex.org/W2951004968",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W2950352427",
    "https://openalex.org/W2902788804",
    "https://openalex.org/W2963859254",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W2593759704",
    "https://openalex.org/W1832693441",
    "https://openalex.org/W2963223306",
    "https://openalex.org/W2791941932",
    "https://openalex.org/W2963207607",
    "https://openalex.org/W2784815861",
    "https://openalex.org/W2306941105",
    "https://openalex.org/W2785681938",
    "https://openalex.org/W2277519914",
    "https://openalex.org/W2905446659",
    "https://openalex.org/W2609368435",
    "https://openalex.org/W2735135478"
  ],
  "abstract": "Today text classification models have been widely used. However, these classifiers are found to be easily fooled by adversarial examples. Fortunately, standard attacking methods generate adversarial texts in a pair-wise way, that is, an adversarial text can only be created from a real-world text by replacing a few words. In many applications, these texts are limited in numbers, therefore their corresponding adversarial examples are often not diverse enough and sometimes hard to read, thus can be easily detected by humans and cannot create chaos at a large scale. In this paper, we propose an end to end solution to efficiently generate adversarial texts from scratch using generative models, which are not restricted to perturbing the given texts. We call it unrestricted adversarial text generation. Specifically, we train a conditional variational autoencoder (VAE) with an additional adversarial loss to guide the generation of adversarial examples. Moreover, to improve the validity of adversarial texts, we utilize discrimators and the training framework of generative adversarial networks (GANs) to make adversarial texts consistent with real data. Experimental results on sentiment analysis demonstrate the scalability and efficiency of our method. It can attack text classification models with a higher success rate than existing methods, and provide acceptable quality for humans in the meantime.",
  "full_text": "Generating Natural Language Adversarial Examples on\na Large Scale with Generative Models\nYankun Ren1 , Jianbin Lin1 , Siliang Tang‚àó,2 , Jun Zhou1 , Shuang Yang1 , Yuan Qi1 and Xiang Ren3\nAbstract. Today text classiÔ¨Åcation models have been widely used.\nHowever, these classiÔ¨Åers are found to be easily fooled by adversar-\nial examples. Fortunately, standard attacking methods generate ad-\nversarial texts in a pair-wise way, that is, an adversarial text can only\nbe created from a real-world text by replacing a few words. In many\napplications, these texts are limited in numbers, therefore their cor-\nresponding adversarial examples are often not diverse enough and\nsometimes hard to read, thus can be easily detected by humans and\ncannot create chaos at a large scale. In this paper, we propose an end\nto end solution to efÔ¨Åciently generate adversarial texts from scratch\nusing generative models, which are not restricted to perturbing the\ngiven texts. We call it unrestricted adversarial text generation. Specif-\nically, we train a conditional variational autoencoder (V AE) with an\nadditional adversarial loss to guide the generation of adversarial ex-\namples. Moreover, to improve the validity of adversarial texts, we\nutilize discrimators and the training framework of generative adver-\nsarial networks (GANs) to make adversarial texts consistent with\nreal data. Experimental results on sentiment analysis demonstrate the\nscalability and efÔ¨Åciency of our method. It can attack text classiÔ¨Åca-\ntion models with a higher success rate than existing methods, and\nprovide acceptable quality for humans in the meantime.\n1 Introduction\nToday machine learning classiÔ¨Åers have been widely used to pro-\nvide key services such as information Ô¨Åltering, sentiment analysis.\nHowever, recently researchers have found that these ML classiÔ¨Åers,\neven deep learning classiÔ¨Åers are vulnerable to adversarial attacks.\nThey demonstrate that image classiÔ¨Åer [10] and now even text clas-\nsiÔ¨Åer [26] can be fooled easily by adversarial examples that are de-\nliberately crafted by attacking algorithms. Their algorithms gener-\nate adversarial examples in a pair-wise way. That is, given one input\nx ‚ààX , they aim to generate one corresponding adversarial exam-\nple x‚Ä≤ ‚ààX by adding small imperceptible perturbations to x. The\nadversarial examples must maintain the semantics of the original in-\nputs, that is, x‚Ä≤must be still classiÔ¨Åed as the same class as xby hu-\nmans. On the other hand, adversarial training is shown to be a useful\ndefense method to resist adversarial examples [31, 10]. Trained on a\nmixture of adversarial and clean examples, classiÔ¨Åers can be resistant\nto adversarial examples.\nIn the area of natural language processing (NLP), existing meth-\nods are pair-wise, thus heavily depend on input data x. If attackers\n1 Ant Financial Services Group, Emails: {yankun.ryk, jianbin.ljb,\njun.zhoujun, shuang.yang, yuan.qi}@anÔ¨Ån.com\n2 Zhejiang University, Email: siliang@zju.edu.cn\n3 University of Southern California, Email: xiangren@usc.edu\n* Corresponding author.\nhuman: negative. ML model: positive\nS1\nS2\n‚Ä¶\nyou‚Äôll trudge out of the theater a movie \nthat is a matter of a dangerous form\njust plain silly and cloying cinema\n‚Ä¶\nhuman: negative. ML model: positive\nS1\nnot gives you considered\nunnerving feeling, no great actors \nand/or expensive production\nhuman: negative. ML model: negative\nstill gives you an eerie feeling, no\ngreat actors or expensive production\nhuman: negative. \nno need for real-world texts\nPair-wise \nreplacement\nattack methods\n(a)\nOur attack \nmethod(b)\nOriginal\nInputs:\nAdversarial\nTexts:\nFigure 1. An illustration of adversarial text generation. (a) Given one nega-\ntive text which is also classiÔ¨Åed as negative by a ML model, traditional meth-\nods replace a few words (yellow background) in the original text to get one\npaired adversarial text, which is still negative for humans, but the model pre-\ndiction changes to positive. (b) Our unrestricted method does not need in-\nput texts. We only assign a ground-truth class - negative, then our method\ncan generate large-scale adversarial texts. which are negative for humans, but\nclassiÔ¨Åed as positive by the ML model.\nwant to generate adversarial texts which should be classiÔ¨Åed as a\nchosen class with pair-wise methods, they must Ô¨Årst collect texts la-\nbeled as the chosen class, then transform these labeled texts to the\ncorresponding adversarial examples by replacing a few words. As\nthe amount of labeled data is always small, the number of generated\nadversarial examples is limited. These adversarial examples are of-\nten not diverse enough and sometimes hard to read, thus can be easily\ndetected by humans. Moreover, in practice, if attackers aim to attack\na public opinion monitoring system, they must collect a large number\nof high-quality labeled samples to generate a vast amount of adver-\nsarial examples, otherwise, they can hardly create an impact on the\ntargeted system. Therefore, pair-wise methods only demonstrate the\nfeasibility of the attack but cannot create chaos on a large scale.\nIn this paper, we propose an unrestricted end to end solution to ef-\nÔ¨Åciently generate adversarial texts, where adversarial examples can\nbe generated from scratch without real-world texts and are still mean-\ningful for humans. We argue that adversarial examples do not need\nto be generated by perturbing existing inputs. For example, we can\ngenerate a movie review that does not stem from any examples in\nthe dataset at hand. If the movie review is thought to be a positive\nreview by humans but classiÔ¨Åed as a negative review by the targeted\nmodel, the movie review is also an adversarial example. Adversarial\nexamples generated in this way can break the limit of input number,\nthus we can get large scale adversarial examples. On the other hand,\nthe proposed method can also be used to create more adversarial ex-\namples for defense. Trained with more adversarial examples often\nmeans more robustness for these key services.\nThe proposed method leverages a conditional variational autoen-\ncoder (V AE) to be the generator which can generate texts of a desired\narXiv:2003.10388v1  [cs.CL]  10 Mar 2020\nclass. To guide the generator to generate texts that mislead the tar-\ngeted model, we access the targeted model in a white-box setting and\nuse an adversarial loss to make the targeted model make a wrong pre-\ndiction. In order to make the generated texts consistent with human\ncognition, we use discrimators and the training framework of gener-\native adversarial networks (GANs) to make generated texts similar\nas real data of the desired class. After the whole model is trained, we\ncan sample from the latent space of V AE and generate inÔ¨Ånite ad-\nversarial examples without accessing the targeted model. The model\ncan also transforms a given input to an adversarial one.\nWe evaluate the performance of our attack method on a sentiment\nanalysis task. Experiments show the scalability of generation. The\nadversarial examples generated from scratch achieve a high attack\nsuccess rate and have acceptable quality. As the model can gener-\nate texts only with feed-forwards in parallel, the generation speed is\nquite fast compared with other methods. Additional ablation stud-\nies verify the effectiveness of discrimators, and data augmentation\nexperiments demonstrate that our method can generate large-scale\nadversarial examples with higher quality than other methods. When\nexisting data at hand is limited, our method is superior over the pair-\nwise generation.\nIn summary, the major contributions of this paper are as follows:\n‚Ä¢ Unlike the existing literature in text attacks, we aim to construct\nadversarial examples not by transforming given texts. Instead, we\ntrain a model to generate text adversarial examples from scratch.\nIn this way, adversarial examples are not restricted to existing in-\nputs at hand but can be generated from scratch on a large-scale.\n‚Ä¢ We propose a novel method based on the vanilla conditional V AE.\nTo generate adversarial examples, we incorporate an adversarial\nloss to guide the vanilla V AE‚Äôs generation process.\n‚Ä¢ We adopt one discrimator for each class of data. When training,\nwe train the discrimators and the conditional V AE in a min-max\ngame like GANs, which can make generated texts more consistent\nwith real data of the desired class.\n‚Ä¢ We conduct attack experiments on a sentiment analysis task. Ex-\nperimental results show that our method is scalable and achieves a\nhigher attack success rate at a higher speed than recent baselines.\nThe quality of generated texts is also acceptable. Further ablation\nstudies and data augmentation experiments verify our intuitions\nand demonstrate the superiority of scalable text adversarial exam-\nple generation.\n2 Related Work\nThere has been extensive studies on adversarial machine leaning, es-\npecially on deep neural models [31, 10, 16, 28, 1]. Much work fo-\ncuses on image classiÔ¨Åcation tasks [31, 10, 5, 11, 33]. [31] solves the\nattack problem as an optimization problem with a box-constrained L-\nBFGS. [10] proposes the fast gradient sign method (FGSM), which\nperturbs images with noise computed as the gradients of the inputs.\nIn NLP, perturbing texts is more difÔ¨Åcult than images, because\nwords in sentences are discrete, on which we can not directly perform\ngradient-based attacks like continuous image space. Most methods\nadapt the pair-wise methods of image attacks to text attacks. They\nperturb texts by replacing a few words in texts. [24, 9, 6] calculate\ngradients with respect to the word vectors and perturb word embed-\nding vectors with gradients. They Ô¨Ånd the word vector nearest to\nthe perturbed vector. In this way, the perturbed vector can be map\nto a discrete word to replace the original one. These methods are\ngradient-based replacement methods.\nEncoder\nùëû\"\nz Decoder\nùëù$\nLatent Space\nOriginal\nText\nAdversarial\nText\nGeneratorùí¢\nùë• ùë•'\nDiscrimator\nùíü)\nTarget Model\nùëì\nc\nground-truth class\nDiscrimator\nùíü+\nDiscrimator\nùíü ùí¥-+\n‚Ä¶‚Ä¶\nFigure 2. The architecture of the whole model. In the training phase,Ggen-\nerates an adversarial textx‚Ä≤to reconstruct the original textx, and feedx‚Ä≤to D\nand f to make f predict differently on xand x‚Ä≤. After trained, the model can\ngenerate large-scale adversarial texts based on sampled latent space vector z\nand a chosen class cwithout original texts x.\nOther attacks on texts can be summarized as gradient-free replace-\nment methods. They replace words in texts with typos or synonyms.\n[16] proposes to edit words with tricks like insertion, deletion and\nreplacement. They choose appropriate words to replace by calcu-\nlating the word frequency and the highest gradient magnitude. [15]\nproposes Ô¨Åve automatic word replacement methods, and use mag-\nnitude of gradients of the word embedding vectors to choose the\nmost important words to replace. [26] is based on synonyms sub-\nstitution strategy. Authors introduce a new word replacement order\ndetermined by both the word saliency and the classiÔ¨Åcation proba-\nbility. However, these replacement methods still generate adversar-\nial texts in a pair-wise way, which restrict the adversarial texts to\nthe variants of given real-world texts. Besides, the substitute words\nsometimes change text meanings. Thus existing adversarial text gen-\neration methods only demonstrate the feasibility of the attack but\ncannot create chaos on a large scale.\nIn order to tackle the above problems, we propose an unrestricted\nend to end solution to generate diverse adversarial texts on a large\nscale with no need of given texts.\n3 Methodology\nIn this section, we propose a novel method to generate adversarial\ntexts for the text classiÔ¨Åcation model on a large scale. Though trained\nwith labeled data in a pair-wise way, after it is trained, our model\ncan generate an unlimited number of adversarial examples without\nany input data. Moreover, like other traditional pair-wise generation\nmethods, our model can also transform a given text into an adversar-\nial one. Unlike the existing methods, our model generates adversarial\ntexts without querying the attacked model, thus the generation pro-\ncedure is quite fast.\n3.1 Overview\nFigure 2 illustrates the overall architecture of our model. The model\nhas three components: a generator G, discrimators D, and a targeted\nmodel f. Gand Dform a generative adversarial network (GAN).\nWhen training, we feed an original input to the generator G, which\ntransforms xto an adversarial output x‚Ä≤. The procedure can be de-\nÔ¨Åned as follows:\nG(x) :x‚ààX‚Üí x‚Ä≤ (1)\nGaims to generate x‚Ä≤to reconstruct x. Then, we feed the gener-\nated x‚Ä≤to the targeted model f, and f will classify x‚Ä≤as a certain\nclass, which we hope is a wrong label. Thus we have the following\nequation:\nf(x‚Ä≤) =yt ‚ààY (2)\nGRU \nEncoder\nGRU \nEncoder\nLinear\nLinear \nLinearz\nfilm depressing film<GO>\ngrim film <EOS>Latent\nSpace\nc\nOriginal Text\nAdversarial Text\nground-truth classdepressing\nGRU \nDecoder\nGRU\nDecoder\nGRU \nDecoder\nùúá\nùúé\nUnrestricted Generation Components \nFigure 3. The generator G. When training, we need input texts to train G\nAfter Gis trained, we only need to sample z from the latent space, and use\nthe decoder to generate adversarial texts unrestrictedly without original texts.\nwhere yt Ã∏= f(x) and Yis the label space of the targeted classiÔ¨Åca-\ntion model.\nIn order to keep x‚Ä≤being classiÔ¨Åed as the same class as xby hu-\nman, we add one discrimator for each class y ‚ààY . With the help\nof the min-max training strategy of GAN framework, each class y‚Äôs\ndiscrimator can make x‚Ä≤close to the distribution of real class ydata,\nthus x‚Ä≤is made to be compatible with human congnition.\nWe now proceed by introducing these components in further de-\ntails.\n3.2 Generator\nIn this subsection, we describe the generator Gfor text generation.\nWe use the variational autoencoder (V AE) [14, 27] as the generator.\nThe V AE is a generative model based on a regularized version of the\nstandard autoencoder. This model supposes the latent variable z is\nsampled from a prior distribution.\nAs shown in Figure 2, the V AE is composed of the encoder\nqŒ∏(z|x) and the decoder pœÑ(x|z), where œÑ is the parameters of p\nand Œ∏is the parameters of q. qŒ∏ is a neural network. Its input is a text\nx, its output is a latent code z. qŒ∏ encodes xinto a latent represen-\ntation space Z, which is a lower-dimensional space than the input\nspace. pœÑ is another neural network. Its input is the codez, it outputs\nan adversarial text x‚Ä≤to the probability distribution of the input data\nx.\nIn our model, we adopt the gated recurrent unit (GRU) [7] as the\nencoder and the decoder. As in Figure 3, The input xis a sentence\nof words, we formulate the input for neural networks as follows: for\na word at the position i in a sentence, we Ô¨Årst transform it into a\nword vector vi by looking up a word embedding table. The word\nembedding table is randomly initialized and is updated during the\nmodel training. Then the word embedding vectors are fed into the\nGRU encoder. In the i-th GRU cell, a hidden state hi is emitted.\nWe use hN to denote the last GRU cell‚Äôs hidden state, where N is\nthe length of the encoder input. In order to get latent code z, we feed\nhN into two linear layers to get ¬µand œÉrespectively. Following the\nGaussian reparameterization trick [14], we sample a random sample\nŒµfrom a standard Gaussian (¬µ= ‚àí ‚Üí0 , œÉ= ‚àí ‚Üí1 ), and compute zas:\nz= ¬µ+ œÉ‚ó¶Œµ (3)\nComputed in this way,zis guaranteed to be sampled from a Gaussian\ndistribution N(¬µ,œÉ2).\nThen, we can decode z to generate an adversarial text x‚Ä≤. Before\nfeeding zto the decoder, we adopt a condition embeddingckto guide\nthe decoder to generate text x‚Ä≤of a certain class yk, which can be\nchosen arbitrarily. Suppose in a text classiÔ¨Åcation task, there are |Y|\nclasses. SpeciÔ¨Åcally, we randomly initialize a class embedding table\nas a matrix C ‚ààR|Y|√ód and look up Cto get the corresponding em-\nbedding ck of class yk. Then, we feed [z,ck] into a linear layer to get\nanother vector representation. The vector encodes the information of\nthe input text and a desired class.\nThe decoder GRU uses this vector as the initial state to generate\nthe output text. Each GRU cell generates one word. The computation\nprocess is similar to that of the GRU encoder, except the output layer\nof each cell. The output Oi of the i-th GRU cell is computed as:\nui = Wh ¬∑hi (4)\nOi,k = eui,wk\n‚àë|V|\nj=1 e\nui,wj\n(5)\nwhere Wh ‚ààRdhi√ó|V|is the transformation weights, Vis the word\nvocabulary, wk ‚ààV and ui,Oi ‚ààR|V|. Oi,k is the probability of\nthe i-th GRU cell emitting the k-th word wk in the vocabulary.\nIn the training phase, the GRU cell chooses the word index with\nthe highest probability to emit:\nwk = arg max\nk\nOi,k (6)\nWhen training, the loss function of the V AE is calculated as:\nLVAE(Œ∏,œÜ) =‚àíEqŒ∏(z|x)(log pœÑ(x|z))\n+ Œ±KL(qŒ∏(z|x)||p(z)) (7)\nThe Ô¨Årst term is the reconstruction loss, or expected negative log-\nlikelihood. This term encourages the decoder to learn to reconstruct\nthe data. So the output text is made to be similar to the input text.\nThe second term is the Kullback-Leibler divergence between the la-\ntent vector distribution qŒ∏(z|x) and p(x). If the V AE were trained\nwith only the reconstruction objective, it would learn to encode its in-\nputs deterministically by making the variances inq(z|x) vanishingly\nsmall [25]. Instead, the V AE uses the second term to encourages the\nmodel to keep its posterior distributions close to a prior p(z), which\nis generally set as a standard Gaussian.\nIn the training phase, the input to the GRU decoder is the input\ntext, appended with a special<GO>token as the start word. We add\na special <EOS>token to the input text as the ground truth of the\noutput text. The <EOS>token represents the end of the sentence.\nWhen training the GRU decoder to generate texts, the GRU decoder\ntends to ignore the latent code zand only relies on the input to emit\noutput text. It actually degenerates into a language model. This situ-\nation is called KL-vanishing. To tackle the KL-vanishing problem in\ntraining GRU decoder, we adopt the KL-annealing mechanism [2].\nKL-annealing mechanism gradually increase the KL weight Œ±from\n0 to 1. This can be thought of as annealing from a vanilla autoencoder\nto a V AE. Also, we randomly drop the input words into the decoder\nwith a Ô¨Åxed keep rate k ‚àà[0,1], to make the decoder depend on the\nlatent code zto generate output text.\nNotably, if we randomly sample z from a standard Gaussian, the\ndecoder can also generate output text based on z. The difference is\nthat there is no input to the GRU decoder, but we can send the word\ngenerated by the i-th GRU cell to the (i+ 1)-th GRU cell as the\n(i+ 1)-th input word. SpeciÔ¨Åcally, in the inference phase, we use\nbeam-search to generate words. The initial input word to the Ô¨Årst\nGRU cell is the <GO>token. When the decoder emits the <EOS>\ntoken, the decoder stops generating new words, and the generation\nof one complete sentence is Ô¨Ånished.\nIn this way, afterGis trained, theoretically, we can sample inÔ¨Ånite\nz from the latent space and generate inÔ¨Ånite output texts based on\nthese z. This is part of the superiority of our method.\nAlgorithm 1Text Adversarial Examples Generation\nInput: Training data of different classes X0, ..., X|Y|‚àí1\nOutput: Text Adversarial Examples\n1: Train a V AE by minimizingLVAE on X0, ..., X|Y|‚àí1 with KL-\nannealing mechanism and word drop\n2: Initialize Gwith the pretrained V AE\n3: Initialize the targeted model with a pretrained TextCNN\n4: Freeze the weights of the targeted model\n5: repeat\n6: for yk = y0,y1,...,y |Y|‚àí1 do\n7: sample a batch of ntexts {xi}n\ni=0 of class yk from Xk\n8: Ggenerates {x‚Ä≤}n\ni=0 with condition ck\n9: Compute Lk\ndisc = 1\nn\nn‚àë\ni=1\nlog Dk(x) +\n1\nn\nn‚àë\ni=1\nlog(1 ‚àíDk(x‚Ä≤))\n10: end for\n11: Update weights of D0, D1, ..., D|Y|‚àí1 by minimizing\n‚àí‚àë|Y|‚àí1\nk=1 Lk\ndisc\n12: Update weights of Gby minimizing Ljoint\n13: until convergence\n14: if With inputs for the encoder then\n15: Encode inputs and decode the corresponding adversarial\ntexts\n16: else\n17: Randomly sample z‚ààN(0,1) and choose a class yk ‚ààY\n18: The decoder takes [z,ck] and generates the adversarial\ntext from scratch\n3.3 Targeted Model\nSince the TextCNN model has good performances and is quite fast,\nit is one of the most widely used methods for text classiÔ¨Åcation task\nin industrial applications [34]. As we aim to attack models used in\npractice, we take the TextCNN model [13] as our targeted model.\nSuppose we set the condition of the V AE to be yk, the decoder\ngenerates the output text x‚Ä≤, then we feed the text into the targeted\nmodel, and the targeted model will predict a probability Ptarget(yi)\nfor each candidate class yi. We conduct targeted attack and aim to\ncheat the targeted model to classify x‚Ä≤as class yt (yt Ã∏= yk), we can\nget the following adversarial loss function:\nLadv = ‚àíEpœÑ(x|z)(log Ptarget(yt)) (8)\nThis is a cross entropy loss that maximize the probability of classyt.\nRecall that words in the adversarial text x‚Ä≤are computed in Equa-\ntion 6, in which Function arg maxis not derivative. So we can not\ndirectly feed the word index computed in Equation 6 into the targeted\nmodel. In this paper, we utilize the Gumbel-Softmax [12] to make\ncontinuous value approximate discrete word index. The embedding\nmatrix W fed to TextCNN is calculated as:\nÀúOi,k = exp(log((ui,wk) +gk)/t)‚àë|V|\nj=1 exp(log((ui,wj) +gk)/t)\n(9)\nWi = ÀúOi ¬∑E (10)\nwhere E ‚ààR|V|√ódw is the whole vocabulary embedding matrix, ui\nis from Equation 4,gk is drawn from Gumbel(0,1) distribution [12]\nand tis the temperature.\n3.4 Discrimator Model\nUntil this point, ideally, we suppose the generated x‚Ä≤ should have\nmany same words as xof class yk (thus be classiÔ¨Åed as yk by hu-\nmans) and be classiÔ¨Åed as class yt by the targeted model. But this\nassumption is not rigorous. Most of the time,x‚Ä≤is not classiÔ¨Åed as yk\nby humans. In natural language texts, even a single word change may\nchange the whole meaning of a sentence. A valid adversarial exam-\nple must be imperceptible to humans. That is, humans must classify\nx‚Ä≤as class yk.\nSuppose Xk is the distribution of real data of class yk and Xk\n‚Ä≤is\nthe distribution of generated adversarial data transformed from x ‚àà\nX. We utilize the idea of GAN framework to makex‚Ä≤similar to data\nfrom Xk. Thus x‚Ä≤will be classiÔ¨Åed as yk by humans and classiÔ¨Åed\nas yt at the same time.\nSpeciÔ¨Åcally, we adopt one discrimator Dk for each class yk ‚ààY.\nDk aims to distinguish the data distribution of real labeled data xof\nclass yk and adversarial datax‚Ä≤generated by Gwith desired class yk:\nLk\ndisc = Ex‚àºXk[log(Dk(x))] +Ex‚Ä≤‚àºXk‚Ä≤ [log(1 ‚àíDk(x‚Ä≤))] (11)\nThe overall training objective is a min-max game played between\nthe generator Gand the discrimators L0\ndisc, L1\ndisc, ..., L|Y|‚àí1\ndisc , where\n|Y|is the total number of classes:\nmin\nG\nmax\nDk\nLk\ndisc (12)\nDk tries to distinguish Xk and Xk\n‚Ä≤, while Gtries to fool Dk to\nmake x‚Ä≤‚ààXk\n‚Ä≤be classiÔ¨Åed as real data byDk. Trained in this adver-\nsarial way, the generated adversarial text distribution Xk\n‚Ä≤is drawn\nclose to distributionXk, which is of classyk. Thus x‚Ä≤is mostly likely\nto be similar to data from Xk and is classÔ¨Åed as yk by human as a\nresult.\nWe implement the discrimators with multi-layer perceptions\n(MLPs). Because arg maxfunction is not derivable, similar to Equa-\ntion 9 and 10 in Section 3.3, we Ô¨Årst use Gumbel-Softmax to trans-\nform the decoder output ui from Equation 4 into a Ô¨Åxed-sized matrix\nV = [w1,w2,...,w m]T. Then, Dk calculate the probability of a\ntext being true data of class yk as:\nDk(x) =MLP(V) (13)\n3.5 Model Training\nCombining Equations 7, 8, 12, we obtain the joint loss function for\nmodel training:\nLjoint = LVAE + œÜLadv +\n|Y|‚àí1‚àë\nk=1\nLk\ndisc (14)\nWe Ô¨Årst train the V AE and the targeted modelfwith training data.\nThen we freeze weights of the targeted model and initialize the G‚Äôs\nweights with the pretrained V AE‚Äôs weights. At last, the generator\nGand all the discrimators L0\ndisc, L1\ndisc, ..., L|Y|‚àí1\ndisc are trained in a\nmin-max game with loss Ljoint. The whole training process is sum-\nmarized in Algorithm 1.\n4 Experiments\nWe report the performances of our method on attacking TextCNN on\nsentiment analysis task, which is an important text classiÔ¨Åcation task.\nSentiment analysis is widely applied to helping a business understand\nthe social sentiment of their products or services by monitoring on-\nline user reviews and comments [23, 4, 21]. In several experiments,\nwe evaluate the quality of the text adversarial examples for sentiment\nanalysis generated by the proposed method.\nExperiments are conducted from two aspects. SpeciÔ¨Åcally, we Ô¨Årst\nfollow the popular settings and evaluate our model‚Äôs performances\nof transforming an existing input text into an adversarial one. We\nobserve that our method has higher attack success rate, generates Ô¨Çu-\nent texts and is efÔ¨Åcient. Besides, we also evaluate our method on\ngenerating adversarial texts from scratch unrestrictedly. Experimen-\ntal results show that we can generate large-scale diverse examples.\nThe generated adversarial texts are mostly valid, and can be utilized\nto substantially improve the robustness of text classiÔ¨Åcation models.\nWe further report ablation studies, which veriÔ¨Åes the effectiveness\nof the discrimators. Defense experiment results demonstrate that gen-\nerating large-scale can help to make model more robust.\n4.1 Experiment Setup and Details\nExperiments are conducted on two popular public benchmark\ndatasets. They are both widely used in sentiment analysis [32, 19, 8]\nand adversarial example generation [15, 29, 30].\nRotten Tomatoes Movie Reviews (RT) [22].This dataset consists\nof 5,331 positive and 5,331 negative processed movie reviews. We\ndivide 80% of the dataset as the training set,10% as the development\nset and 10% as the test set.\nIMDB [17]. This dataset contains 50,000 movie reviews from on-\nline movie websites. It consists of positive and negative paragraphs.\n25,000 samples are for training and 25,000 are for testing. We held\nout 20% of the training set as a validation set as [15].\n4.2 Comparing With Pair-wise Methods\nIn most of the existing work [26, 18, 1], text adversarial examples\nare generated through a pair-wise way. That is, Ô¨Årst we should take a\ntext example, and then transform it into an adversarial instance.\nTo compare with the current methods fairly, we limit our method to\npair-wise generation. In this experiment, we set œÜ= 9. SpeciÔ¨Åcally,\nwe Ô¨Årst feed an input text into the GRU encoder, and set the condition\nck as the ground-truth class of the text. After that, the decoder can\ndecode [z,ck] to get the adversarial output text.\nWe choose four representative methods as baselines:\n‚Ä¢ Random: Select 10% words randomly and modify them.\n‚Ä¢ Fast Gradient Sign Method (FGSM)[10]: First, perturbation is\ncomputed as Œµsign(‚ñΩxJ), where J is the loss function and x is\nthe word vectors. Then, search in the word embedding table to\nÔ¨Ånd the nearest word vector to the perturbed word vector. FGSM\nis the fastest among gradient-based replacement methods.\n‚Ä¢ DeepFool [20]: This is also a gradient-based replacement method.\nIt aims to Ô¨Ånd out the best direction, towards which it takes the\nshortest distance to cross the decision boundary. The perturbation\nis also applied to the word vectors. After that, nearest neighbor\nsearch is used to generate adversarial texts.\n‚Ä¢ TextBugger [15]: TextBugger is a gradient-free replacement\nmethod. It proposes strategies such as changing the word‚Äôs\nspelling and replacing a word with its synonym, to change a word\nslightly to create adversarial texts. Gradients are only computed to\nÔ¨Ånd the most important words to change.\nAttack Success Rate.Following the existing literature [10, 20, 15],\nwe evaluate the attack success rate of our method and four baseline\nmethods.\nTable 1. Attack success rate of transforming given texts in a pair-wise way.\nMethod RT IMDB\nRandom 1.5% 1.3%\nFGSM+NNS 25.7% 36.2%\nDeepFool+NNS 28.5% 23.9%\nTextBugger 85.1% 90.5%\nOurs (œÜ= 5) 87.1% 92.8%\nWe summarize the performances of of our method and all base-\nlines in Table 1. From Table 1, we can observe that randomly chang-\ning 10% words is not enough to fool the classiÔ¨Åer. This implies the\ndifÔ¨Åculty of attack. TextBugger and our method both achieve quite\nhigh attack success rate. While our method performs even better than\nTextBugger, which is the state-of-the-art method.\nWe show some adversarial examples generated by our method and\nTextBugger to demonstrate the differences in Figure 4.\nWe can observe that TextBugger mainly changes the spelling of\nwords. The generated text becomes not Ô¨Çuent and easy to be detected\nby grammar checking systems. Also, though humans may guess the\noriginal meanings, the changed words are treated as out of vocabu-\nlary words by models. For example, TextBugger changes the spelling\nof ‚Äòawful‚Äô, ‚Äòcliches‚Äô and ‚Äòfoolish‚Äô in Figure 4. These words are im-\nportant negative sentiment words for a negative sentence. It is natural\nthat changing these words to unknown words can change the predic-\ntion of models. Unlike TextBugger, our method generates meaningful\nand Ô¨Çuent contents. For example, in the Ô¨Årst example of Figure 4, we\nreplace ‚Äòread the novel‚Äô with ‚Äòlove the book‚Äô, the substitution is still\nÔ¨Çuent and make sense to both humans and models.\nGeneration Speed. It takes about one hour and about 3 hours to\ntrain our model on RT dataset and IMDB dataset respectively. We\nalso evaluate the time cost of generating one adversarial example.\nWe take the FGSM method as the representative of gradient-based\nmethods, as FGSM is the fastest among them. We measure the time\ncost of generating 1,000 adversarial examples and calculate the av-\nerage time of generating one. Results are shown in Table 2.\nTable 2. Time cost of generating one adversarial text.\nMethod FGSM+NNS TextBugger Ours (œÜ= 5)\nTime 0.7s 0.05s 0.014s\nWe can observe that our method is much faster than others. That\nis mainly because our generative model is trained beforehand. After\nthe model is trained, the generation of one batch just requires one\nfeed-forward.\n4.3 Unrestricted Adversarial Text Generation\nAs mentioned in Section 3.2, after our model is trained, we can ran-\ndomly sample z from latent space, choose a desired class yk ‚ààY ,\nget the embedding vector ck of yk, then feed [z,ck] to the decoder to\ngenerate adversarial texts unrestrictedly with no need of labeled text.\nAttack Success Rate.When training, we can tune œÜin Equation 14\nto affect the model. After trained with different œÜ, we observe the\ngenerated texts are different. We randomly generate 50,000 exam-\nples and compute the proportion of adversarial examples with differ-\nent œÜ. The results are shown in Figure 5(a). Notice if we set œÜ = 0,\nDataset: RT. Method: Ours(ùúô= 9). Ground-truth: Positive. Original prediction: 0.95 Positive.  Adversarial prediction: 0.68 Negative. \nText: Inside one the films conflict like powered plot there is a decent moral trying to get out but its not that it‚Äôs the tension first that keeps makes you in feel your seat affleck and jackson are \ngood is magnificent sparring partners\nDataset: IMDB. Method: Ours(ùúô= 9). Ground-truth: Negative. Original prediction: 0.98 Negative.  Adversarial prediction: 0.94 Positive. \nText: i read the novel love the book some years ago and i liked loved it a lot when i saw the read this movie i couldnt believe was cared it they changed was thrown everything i liked\nexpected about the novel book even the plot i wonder what if did isabel allende author did say about the this movie but i think it sucks\nDataset: IMDB. Method: TextBugger. Ground-truth: Negative. Original prediction: 0.99 Negative.  Adversarial prediction: 0.81 Positive. \nText: I love these awful awf ul 80's summer camp movies. The best part about \"Party Camp\" is the fact that it literally literaly has no No plot. The cliches clichs here are limitless: the nerds vs. \nthe jocks, ‚Ä¶, the secretly horny camp administrators, and the embarrassingly embarrassing1y foolish fo0lish sexual innuendo littered throughout. This movie will make you laugh, but never \nintentionally. I repeat, never.\nFigure 4. Adversarial texts generated in a pair-wise way. In texts, the crossed out contents are from the original texts, while the red texts are the substitute\ncontents in the adversarial examples.\n0 1 2 3 4 5 6 7\nœï\n0\n20\n40\n60\n80\n100Attack Success Rate (%)\n10.3\n24.5\n58.6\n69.4\n78.0\n84.2 88.6 90.2\n20.1\n30.7\n60.1\n74.5\n80.6\n88.6 90.1 93.9\nRT\nIMDB\n(a) Attack Success Rate\n0 1 2 3 4 5 6 7\nœï\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\n3.00\n3.25\n3.50Perplexity2.40\n2.54 2.6 2.68 2.71 2.75 2.77 2.792.53\n2.65 2.71 2.78 2.82 2.84 2.87 2.88\nRT\nIMDB (b) Perplexity\n0 1 2 3 4 5 6 7\nœï\n0\n20\n40\n60\n80\n100Validity Rate (%)\n31\n74 70\n76 75 72 73 75\n21\n73 72 75 72 74 75 73\nRT\nIMDB (c) Validity Rate\nFigure 5. The attack success rate, perplexity and validity of unrestricted adversarial text generation from scratch. Randomly sample zto generate adversarial\ntexts from scratch with different œÜ. Note that when œÜ= 0, the model is a vanilla V AE\nDataset: RT. Method: Vanilla VAE (ùúô= 0). Chosen Emotional Class: Negative. Model prediction: 0.53 Positive.\nText: this is the kind of movie that might have been benefited from a movie that is not more than a movie\nDataset: IMDB. Method: Vinilla VAE (ùúô= 0). Chosen Emotional Class : Positive. Model prediction: 0.54 Negative.  \nText: i had never heard of this movie until the end of the first half hour or minutes we were glued to the edge of your seat throughout the entire movie i thought it was going to be a good \nidea to see a movie about a bunch of people trying to find out what happened to their ‚Ä¶ see if you want to see a movie that is going to happen next to the end\nDataset: RT. Method: Ours (ùúô= 1). Chosen Emotional Class : Positive. Model prediction: 0.99 Negative\nText: theres no reason to be disappointed\nDataset: RT. Method: Ours (ùúô= 7). Chosen Emotional Class : Negative. Model prediction: 0.89 Positive\nText: sandra bullock fish out into a dark and poorly executed story about about about which he doesnt manage to be a joyful teacher\nDataset: IMDB. Method: Ours (ùúô= 1). Chosen Emotional Class : Negative. Model prediction: 0.97 Positive\nText: this was the first time i saw this movie when i was a kid i was expecting it to be the first time i saw this movie i was thoroughly impressed with this movie was that it was so bad \nDataset: IMDB. Method: Ours (ùúô= 7). Chosen Emotional Class : Positive. Model prediction: 0.93 Negative\nText: a lot of fun to watch this movie is about a virus who crashes in the himalayas unlucky enough to take a trip to the old house in the woods in the himalayas unlucky enough to be a \nphotographer and wanted to prevent the freezing man in a limb in a limb in a limb in a limb in a limb in his assignment to stop him he decides to take him out of his apartment with his wife\nFigure 6. Adversarial examples generated from scratch unrestrictedly. Humans should classify adversarial texts as the chosen emotional classyk.\nthe model is a vanilla V AE and it is not trained continually after pre-\ntrained.\nFrom Figure 5(a), we can observe that the attack success rate of the\nvanilla V AE is only10.3% and 20.1% respectively, this implies that\nonly randomly generating texts can hardly fool the targeted model.\nWhen œÜis greater than 0, the attack success rate is consistently better\nthan the vanilla V AE. This reÔ¨Çects the importance ofLadv.\nAlso, the attack success rate increases as œÜbecomes larger. It is\nbecause the larger œÜis, the more important role Ladv will plays in\nthe Ô¨Ånal joint loss Ljoint. So, the text generator Gis more easily\nguided by the Ladv to generate an adversarial example.\nTo evaluate the quality of the generated adversarial texts with dif-\nferent œÜ, we adopt three metrics : perplexity, validity and diversity.\nPerplexity.Perplexity [3] is a measurement of how well a probability\nmodel predicts a sample. A low perplexity indicates the language\nmodel is good at predicting the sample. Given a pretrained language\nmodel, it can also be used to evaluate the quality of texts. Similarly,\na low perplexity indicates the text is more Ô¨Çuent for the language\nmodel. We compute perplexity as:\nPerplexity = ‚àí 1\n|word_num|\n‚àë\nx‚ààX‚Ä≤\nV‚àë\nj=1\nlog P(x‚Ä≤\nj|x‚Ä≤\n0,...,x ‚Ä≤\nj‚àí1)\n(15)\nwhere V is the number of words in one sentence. P(x‚Ä≤\nj) is the prob-\nability of j-th word in x‚Ä≤computed by the language model.\nWe train a language model with the training data of IMDB and RT,\nand use it asP in Equation 15. We measure and compare the perplex-\nity of the generated 50,000 texts and data of the original training set.\nResults are shown in Figure 5(b). We can observe that the perplex-\nity is only a bit higher than the original data‚Äôs, which means that the\nquality of generated texts are acceptable. Also, as œÜgets larger, the\nperplexity gets bigger. This is perhaps because Ladv can distort the\ngenerated texts.\nw/o data\naugmentation\n1 2 3 4 5 6 7\nœï\n0\n20\n40\n60\n80\n100Accuracy (%)\n0.0\n95.2 97.1 94.4 96.3 96.1 95.1 94.9\n70.6 70.5 71.3 71.2 70.9 71.6 70.9 71.1\nOriginal Test Data Acc.\nAdversarial Test Data Acc.\n(a) RT\nw/o data\naugmentation\n1 2 3 4 5 6 7\nœï\n0\n20\n40\n60\n80\n100Accuracy (%)\n0.0\n94.3 95.6 96.0 95.8 94.2 93.9 95.1\n81.7 80.8 81.9 80.3 81.1 80.9 81.4 81.0\nOriginal Test Data Acc.\nAdversarial Test Data Acc. (b) IMDB\n1.5k 3.6k 4.2k 5k 5.4k\nAugmentation Data Amount\n30\n40\n50\n60\n70\n80\n90\n100Accuracy (%) 81.6\n85.4\n88.3 90.1\n70.4 70.4 70.8\n71.2 71.171.8 71.3 71.6\n70.7 70.4\n78.1\n82.4\n85.9\n89.2 90.6\nOriginal Test Data Acc. (Pair-wise)\nAdversarial Test Data Acc. (Pair-wise)\nOriginal Test Data Acc. (From Scratch)\nAdversarial Test Data Acc. (From Scratch) (c) Data augmentation compare\nFigure 7. Defense with adversarial training in different settings. (a) and (b) On RT and IMDB datasets, data augmentation with adversarial data generated\nfrom scratch under different œÜ. (c) On RT dataset, accuracy of models trained with equal size of augmentation adversarial data, which is generated in pair-wise\nway and unrestricted generation way respectively.\nValidity. If we feed [z,ck] to the decoder, then a valid generated\nadversarial text is supposed to be classiÔ¨Åed as classyk by humans but\nbe classiÔ¨Åed as class yt Ã∏= yk by the targeted model. We randomly\nselect 100 generated texts for each œÜ and manually evaluate their\nvalidity. The results are shown in Figure 5(c).\nFrom Figures 5(c), we can observe that the validity rates of our\nmethod on both datasets are higher than 70% and much higher than\nthat of the vanilla V AE. This implies our methods can generate high-\nquality and high-validity texts with high attack success rate.\nDiversity. We Ô¨Årst generate one million adversarial texts. To com-\npare generated texts with train data, we extract all 4-grams of train\ndata and generated texts. On average, for each generated text, less\nthan 18% of 4-grams can be found in all 4-grams of train data on all\ndatasets. This shows that there exists some similarity and our model\ncan also generate texts with different words combinations. To com-\npare generated texts with each other, we suppose that if over 20% of\n4-grams of one generated text don‚Äôt exist at the same time in any one\nof the other generated texts, the text is one unique text. We observe\nmore than 70% of generated texts are unique. This proved that the\ngenerated texts are diverse.\nAdversarial Examples.We show some valid adversarial examples\ngenerated by our method in Figure 6. We can view that the adversar-\nial examples generated by the vanilla V AE is more likely neutral, and\nthe conÔ¨Ådence of the targeted model is not huge. On the contrary, the\ngenerated examples of our method have high conÔ¨Ådence of the tar-\ngeted model. This shows Ladv is important to attack success rate.\nBesides, the Ô¨Çuency and validity of texts generated by our method\nare acceptable.\n4.4 Ablation Study\nIn this section, we further demonstrate the effectiveness of discrima-\ntors. We now report the ablation study.\nWe Ô¨Årst remove discrimators and Ldisc, then train our model. We\ncompare it with the model trained with Ljoint in a min-max game.\nWe evaluate their attack success rate, perplexity and validity. Results\nare show in Table 3.\nTable 3. Performance of our model trained with and without Ldisc.\nDataset Method Attack Success\nRate Perplexity Validity\nRT withLdisc 90.2% 2.79 75%\nwithoutLdisc 94.1% 7.32 15%\nIMDB withLdisc 93.9% 2.88 73%\nwithoutLdisc 94.3% 7.41 12%\nThe attack success rates of models trained with and without Ldisc\nare close. But the validity of the model trained withoutLdiscis much\nlower than that of the model with Lfilter. The reason of this phe-\nnomenon is as follows. When training the generator Gwith only\nLVAE and Ladv, suppose we want to generate positive adversarial\ntexts and the targeted model must classify it as negative, the easiest\nway to achieve this goal is to change a few words in the generated text\nto negative words, such as \"bad\". But texts generated this way can not\nfool humans. If we add discrimators to draw distribution of adversar-\nial texts close to the distribution of real data, this phenomenon can\nbe controlled. This shows that discrimators and the min-max game\nminmax Lk\ndisc can improve the validity greatly.\n4.5 Defense With Adversarial Training\nUsing the adversarial examples to augment the training data can\nmake models more robust, this is called adversarial training.\nOn RT dataset, we randomly generate 4k adversarial texts to aug-\nment the training data and 1k to test the model. On IMDB dataset, we\nrandomly generate 10k, of which 8k for training and 2k for testing.\nResults are shown in Figure 7(a) and Figure 7(b).\nThrough adversarial data augmentation, test accuracy on the orig-\ninal test data is stable. Also, the accuracy on the adversarial data is\nimproved greatly (from0 to >90%). It implies that adversarial train-\ning can make models more robust without hurting its effectiveness.\nThen, on RT dataset, we Ô¨Årst augment training data with adversar-\nial examples generated by pair-wise generation. The adversarial ex-\namples are generated through transforming training data. Note that\nwe have 8k training data in RT dataset. When we set bigger œÜ, the\nattack success rate is higher, so we can generate more adversarial ex-\namples in the pair-wise way. But with any œÜ, unrestricted generation\nfrom scratch can result in inÔ¨Ånite adversarial data. We compare the\nadversarial data augmentation performances of pair-wise and unre-\nstricted generation from scratch. We use the same number of adver-\nsarial examples generated by the two modes, and hold out 20% of\ngenerated data for testing. Results are shown in Figure 7(c).\nWe can see that with pair-wise generation, if training data is lim-\nited, we need to generate more adversarial examples to improve the\nadversarial test accuracy. Higher adversarial test accuracy requires\nhigher œÜ. But higher œÜresults in bigger perplexity, which means low\ntext quality. Differently, with unrestricted generation from scratch,\nwe can generate inÔ¨Ånite adversarial texts using very small œÜ, with\nhigh Ô¨Çuency and similar adversarial test accuracy. Thus, under simi-\nlar adversarial test accuracy, the text Ô¨Çuency of pair-wise generation\nis worse than that of unrestricted generation from scratch. This indi-\ncates the advantage of our proposed method.\n5 Conclusion\nIn this paper, we have proposed a scalable method to generate ad-\nversarial texts from scratch attacking a text classiÔ¨Åcation model. We\nadd an adversarial loss to enforce the generated text to mislead the\ntargeted model. Besides, we use discrimators and GAN-like train-\ning strategy to make adversarial texts mimic real data of the desired\nclass. After the generator is trained, it can generate diverse adver-\nsarial examples of a desired class on a large scale without real-world\ntexts. Experiments show that the proposed method is scalable and can\nachieve higher attack success rate at a higher speed compared with\nrecent methods. In addition, it is also demonstrated that the gener-\nated texts are of good quality and mostly valid. We further conduct\nablation experiments to verify effects of discrimators. Experiments\nof data augmentation indicate that our method generates more di-\nverse adversarial texts with higher quality than pair-wise generation,\nwhich can make the targeted model more robust.\nREFERENCES\n[1] Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho,\nMani Srivastava, and Kai-Wei Chang, ‚ÄòGenerating natural language ad-\nversarial examples‚Äô,arXiv preprint arXiv:1804.07998, (2018).\n[2] Samuel Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal\nJozefowicz, and Samy Bengio, ‚ÄòGenerating sentences from a continu-\nous space.‚Äô, in Proceedings of the Twentieth Conference on Computa-\ntional Natural Language Learning (CoNLL)., (2016).\n[3] Peter F Brown, Vincent J Della Pietra, Robert L Mercer, Stephen\nA Della Pietra, and Jennifer C Lai, ‚ÄòAn estimate of an upper bound\nfor the entropy of english‚Äô, Computational Linguistics, 18(1), 31‚Äì40,\n(1992).\n[4] Erik Cambria, ‚ÄòAffective computing and sentiment analysis‚Äô, IEEE In-\ntelligent Systems, 31(2), 102‚Äì107, (2016).\n[5] Nicholas Carlini and David Wagner, ‚ÄòTowards evaluating the robustness\nof neural networks‚Äô, in2017 IEEE Symposium on Security and Privacy\n(SP), pp. 39‚Äì57. IEEE, (2017).\n[6] Minhao Cheng, Jinfeng Yi, Huan Zhang, Pin-Yu Chen, and Cho-Jui\nHsieh, ‚ÄòSeq2sick: Evaluating the robustness of sequence-to-sequence\nmodels with adversarial examples‚Äô, arXiv preprint arXiv:1803.01128,\n(2018).\n[7] Kyunghyun Cho, Bart Van Merri√´nboer, Dzmitry Bahdanau, and\nYoshua Bengio, ‚ÄòOn the properties of neural machine translation:\nEncoder-decoder approaches‚Äô,arXiv preprint arXiv:1409.1259, (2014).\n[8] Ari Firmanto, Riyanarto Sarno, et al., ‚ÄòPrediction of movie sentiment\nbased on reviews and score on rotten tomatoes using sentiwordnet‚Äô, in\n2018 International Seminar on Application for Technology of Informa-\ntion and Communication, pp. 202‚Äì206. IEEE, (2018).\n[9] Zhitao Gong, Wenlu Wang, Bo Li, Dawn Song, and Wei-Shinn\nKu, ‚ÄòAdversarial texts with gradient methods‚Äô, arXiv preprint\narXiv:1801.07175, (2018).\n[10] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy, ‚ÄòExplaining\nand harnessing adversarial examples‚Äô,arXiv preprint arXiv:1412.6572,\n(2014).\n[11] Weiwei Hu and Ying Tan, ‚ÄòGenerating adversarial malware examples\nfor black-box attacks based on gan‚Äô,arXiv preprint arXiv:1702.05983,\n(2017).\n[12] Eric Jang, Shixiang Gu, and Ben Poole, ‚ÄòCategorical reparameterization\nwith gumbel-softmax‚Äô,arXiv preprint arXiv:1611.01144, (2016).\n[13] Yoon Kim, ‚ÄòConvolutional neural networks for sentence classiÔ¨Åcation‚Äô,\nin Proceedings of the 2014 Conference on Empirical Methods in Natu-\nral Language Processing (EMNLP), pp. 1746‚Äì1751, (2014).\n[14] Diederik P Kingma and Max Welling, ‚ÄòAuto-encoding variational\nbayes‚Äô,arXiv preprint arXiv:1312.6114, (2013).\n[15] J Li, S Ji, T Du, B Li, and T Wang, ‚ÄòTextbugger: Generating adversar-\nial text against real-world applications‚Äô, in 26th Annual Network and\nDistributed System Security Symposium, (2019).\n[16] Bin Liang, Hongcheng Li, Miaoqiang Su, Pan Bian, Xirong Li, and\nWenchang Shi, ‚ÄòDeep text classiÔ¨Åcation can be fooled‚Äô, inProceedings\nof the 27th International Joint Conference on ArtiÔ¨Åcial Intelligence, pp.\n4208‚Äì4215. AAAI Press, (2018).\n[17] Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, An-\ndrew Y Ng, and Christopher Potts, ‚ÄòLearning word vectors for senti-\nment analysis‚Äô, in Proceedings of the 49th annual meeting of the asso-\nciation for computational linguistics: Human language technologies-\nvolume 1 , pp. 142‚Äì150. Association for Computational Linguistics,\n(2011).\n[18] Paul Michel, Xian Li, Graham Neubig, and Juan Miguel Pino, ‚ÄòOn eval-\nuation of adversarial perturbations for sequence-to-sequence models‚Äô,\narXiv preprint arXiv:1903.06620, (2019).\n[19] Melody Moh, Abhiteja Gajjala, Siva Charan Reddy Gangireddy, and\nTeng-Sheng Moh, ‚ÄòOn multi-tier sentiment analysis using supervised\nmachine learning‚Äô, in 2015 IEEE/WIC/ACM International Conference\non Web Intelligence and Intelligent Agent Technology (WI-IAT) , vol-\nume 1, pp. 341‚Äì344. IEEE, (2015).\n[20] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal\nFrossard, ‚ÄòDeepfool: a simple and accurate method to fool deep neural\nnetworks‚Äô, in Proceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 2574‚Äì2582, (2016).\n[21] Juan Antonio Morente-Molinera, Gang Kou, Konstantin Samuylov,\nRaquel Ure√±a, and Enrique Herrera-Viedma, ‚ÄòCarrying out consensual\ngroup decision making processes under social networks using senti-\nment analysis over comparative expressions‚Äô, Knowledge-Based Sys-\ntems, 165, 335‚Äì345, (2019).\n[22] Bo Pang and Lillian Lee, ‚ÄòSeeing stars: Exploiting class relationships\nfor sentiment categorization with respect to rating scales‚Äô, in Proceed-\nings of the 43rd annual meeting on association for computational\nlinguistics, pp. 115‚Äì124. Association for Computational Linguistics,\n(2005).\n[23] Bo Pang, Lillian Lee, et al., ‚ÄòOpinion mining and sentiment analysis‚Äô,\nFoundations and Trends R‚Éùin Information Retrieval , 2(1‚Äì2), 1‚Äì135,\n(2008).\n[24] Nicolas Papernot, Patrick McDaniel, Ananthram Swami, and Richard\nHarang, ‚ÄòCrafting adversarial input sequences for recurrent neural net-\nworks‚Äô, in MILCOM 2016-2016 IEEE Military Communications Con-\nference, pp. 49‚Äì54. IEEE, (2016).\n[25] Tapani Raiko, Mathias Berglund, Guillaume Alain, and Laurent Dinh,\n‚ÄòTechniques for learning binary stochastic feedforward neural net-\nworks‚Äô,arXiv preprint arXiv:1406.2989, (2014).\n[26] Shuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che, ‚ÄòGenerating nat-\nural language adversarial examples through probability weighted word\nsaliency‚Äô, inProceedings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pp. 1085‚Äì1097, (2019).\n[27] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra,\n‚ÄòStochastic backpropagation and approximate inference in deep gen-\nerative models‚Äô, inInternational Conference on Machine Learning, pp.\n1278‚Äì1286, (2014).\n[28] Suranjana Samanta and Sameep Mehta, ‚ÄòTowards crafting text adver-\nsarial samples‚Äô,arXiv preprint arXiv:1707.02812, (2017).\n[29] Motoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji Matsumoto, ‚ÄòIn-\nterpretable adversarial perturbation in input embedding space for text‚Äô,\narXiv preprint arXiv:1805.02917, (2018).\n[30] Congzheng Song and Vitaly Shmatikov, ‚ÄòFooling ocr systems with ad-\nversarial text images‚Äô,arXiv preprint arXiv:1802.05385, (2018).\n[31] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Du-\nmitru Erhan, Ian Goodfellow, and Rob Fergus, ‚ÄòIntriguing properties of\nneural networks‚Äô,arXiv preprint arXiv:1312.6199, (2013).\n[32] Prayag Tiwari, Brojo Kishore Mishra, Sachin Kumar, and Vivek Ku-\nmar, ‚ÄòImplementation of n-gram methodology for rotten tomatoes re-\nview dataset sentiment analysis‚Äô, International Journal of Knowledge\nDiscovery in Bioinformatics (IJKDB), 7(1), 30‚Äì41, (2017).\n[33] Chaowei Xiao, Jun-Yan Zhu, Bo Li, Warren He, Mingyan Liu,\nand Dawn Song, ‚ÄòSpatially transformed adversarial examples‚Äô, arXiv\npreprint arXiv:1801.02612, (2018).\n[34] Wei Emma Zhang, Quan Z Sheng, A Alhazmi, and C Li, ‚ÄòAdversar-\nial attacks on deep learning models in natural language processing: A\nsurvey‚Äô,arXiv preprint arXiv:1901.06796, (2019).",
  "topic": "Adversarial system",
  "concepts": [
    {
      "name": "Adversarial system",
      "score": 0.9495513439178467
    },
    {
      "name": "Generative grammar",
      "score": 0.7704084515571594
    },
    {
      "name": "Computer science",
      "score": 0.7554680705070496
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6048505306243896
    },
    {
      "name": "Scalability",
      "score": 0.5730009078979492
    },
    {
      "name": "Autoencoder",
      "score": 0.5600669980049133
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5301666259765625
    },
    {
      "name": "Language model",
      "score": 0.4426631033420563
    },
    {
      "name": "Natural language",
      "score": 0.4204968810081482
    },
    {
      "name": "Machine learning",
      "score": 0.40666866302490234
    },
    {
      "name": "Natural language processing",
      "score": 0.3925767242908478
    },
    {
      "name": "Deep learning",
      "score": 0.33833497762680054
    },
    {
      "name": "Theoretical computer science",
      "score": 0.32346296310424805
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": []
}