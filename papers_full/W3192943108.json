{
    "title": "Robust Transfer Learning with Pretrained Language Models through Adapters",
    "url": "https://openalex.org/W3192943108",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2015695805",
            "name": "Han, Wenjuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2170727223",
            "name": "Pang Bo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2223320997",
            "name": "Wu Yingnian",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3034199299",
        "https://openalex.org/W2970352191",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W3011279327",
        "https://openalex.org/W3018458867",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W3099793224",
        "https://openalex.org/W3105721709",
        "https://openalex.org/W3153675281",
        "https://openalex.org/W3023528699",
        "https://openalex.org/W2975185270",
        "https://openalex.org/W2963777589",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3013571468",
        "https://openalex.org/W3167810126",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3098824823",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W3120490999",
        "https://openalex.org/W4322614701",
        "https://openalex.org/W3037854022",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2243397390",
        "https://openalex.org/W2979736514",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2949128310",
        "https://openalex.org/W3171293857",
        "https://openalex.org/W3101449015",
        "https://openalex.org/W3176693010",
        "https://openalex.org/W2995998574",
        "https://openalex.org/W3104186312",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2898700502"
    ],
    "abstract": "Transfer learning with large pretrained transformer-based language models like BERT has become a dominating approach for most NLP tasks. Simply fine-tuning those large language models on downstream tasks or combining it with task-specific pretraining is often not robust. In particular, the performance considerably varies as the random seed changes or the number of pretraining and/or fine-tuning iterations varies, and the fine-tuned model is vulnerable to adversarial attack. We propose a simple yet effective adapter-based approach to mitigate these issues. Specifically, we insert small bottleneck layers (i.e., adapter) within each layer of a pretrained model, then fix the pretrained layers and train the adapter layers on the downstream task data, with (1) task-specific unsupervised pretraining and then (2) task-specific supervised training (e.g., classification, sequence labeling). Our experiments demonstrate that such a training scheme leads to improved stability and adversarial robustness in transfer learning to various downstream tasks.",
    "full_text": "Robust Transfer Learning with Pretrained Language Models through\nAdapters\nWenjuan Han1∗†, Bo Pang2∗, Yingnian Wu2\n1 Beijing Institute for General Artiﬁcial Intelligence, Beijing, China\n2 Department of Statistics, University of California, Los Angeles\nhanwenjuan@bigai.ai\n{bopang, ywu}@ucla.edu\nAbstract\nTransfer learning with large pretrained\ntransformer-based language models like\nBERT has become a dominating approach for\nmost NLP tasks. Simply ﬁne-tuning those\nlarge language models on downstream tasks\nor combining it with task-speciﬁc pretraining\nis often not robust. In particular, the perfor-\nmance considerably varies as the random seed\nchanges or the number of pretraining and/or\nﬁne-tuning iterations varies, and the ﬁne-tuned\nmodel is vulnerable to adversarial attack. We\npropose a simple yet effective adapter-based\napproach to mitigate these issues. Speciﬁcally,\nwe insert small bottleneck layers (i.e., adapter)\nwithin each layer of a pretrained model, then\nﬁx the pretrained layers and train the adapter\nlayers on the downstream task data, with (1)\ntask-speciﬁc unsupervised pretraining and\nthen (2) task-speciﬁc supervised training\n(e.g., classiﬁcation, sequence labeling). Our\nexperiments demonstrate that such a training\nscheme leads to improved stability and\nadversarial robustness in transfer learning to\nvarious downstream tasks. 1\n1 Introduction\nPretrained transformer-based language models like\nBERT (Devlin et al., 2019), RoBERTa (Liu et al.,\n2019) have demonstrated impressive performance\non various NLP tasks such as sentiment analysis,\nquestion answering, text generation, just to name a\nfew. Their successes are achieved through sequen-\ntial transfer learning (Ruder, 2019): pretrain a lan-\nguage model on large-scale unlabeled data and then\nﬁne-tune it on downstream tasks with labeled data.\nThe most commonly used ﬁne-tuning approach is\nto optimize all parameters of the pretrained model\n*Equal contributions.\n†Corresponding author.\n1https://github.com/WinnieHAN/\nAdapter-Robustness.git\nFigure 1: Learning curves of ﬁne-tuning with the task-\nspeciﬁc pretraining iterations varied. The curve with triangles\nrepresents the model that has converged in the 8000-th pre-\ntraining iteration.\nwith regard to the downstream-task-speciﬁc loss.\nThis training scheme is widely adopted due to its\nsimplicity and ﬂexibility (Phang et al., 2018; Peters\net al., 2019; Lan et al., 2019; Raffel et al., 2020;\nClark et al., 2020; Nijkamp et al., 2021; Lewis et al.,\n2020).\nDespite the success of the standard sequential\ntransfer learning approach, recent works (Guru-\nrangan et al., 2020; Lee et al., 2020; Nguyen\net al., 2020) have explored domain-speciﬁc or task-\nspeciﬁc unsupervised pretraining, that is, masked\nlanguage model training on the downstream task\ndata before the ﬁnal supervised ﬁne-tuning on it.\nAnd they demonstrated beneﬁts of task-speciﬁc\npretraining on transfer learning performance. How-\never, both standard sequential transfer learning and\nthat with task-speciﬁc pretraining are unstable in\nthe sense that downstream task performance is sub-\nject to considerable ﬂuctuation while the random\nseed is changed or the number of pretraining and/or\nﬁne-tuning iterations is varied even after the train-\ning has converged (see Section 2 and Section 3 for\narXiv:2108.02340v1  [cs.CL]  5 Aug 2021\nWNLI RTE MRPC STS-B CoLA SST-2 QNLI QQP MNLI\nMetrics Acc. Acc. F1/Acc. P/S corr. M corr. Acc. Acc. Acc./F1 M acc.\nWO. 56.34 65.7 88.85/84.07 88.64/88.48 56.53 92.32 90.66 90.71/87.49 84.10\nW. F . 45.07 61.73 89.47/85.29 83.95/83.70 49.23 91.97 87.46 88.40/84.31 81.08\nTSP .+F .56.34 68.59 89.76/86.37 89.24/88.87 64.87 92.78 91.12 90.92/87.88 84.14\nTable 1: Performance on the development dataset of GLUE. Results of W.(F.) are reported in Adapter-Hub. We\nreport results of WO. using the implementation from Wolf et al. (2020). Acc.: Accuracy. M acc.: Mismatched\nAcc. P/S acc.: Person/Spearman corr. M corr.: Matthew’s corr. TSP.: Task-Speciﬁc Pretrain. F.: Finetune. WO.:\nWithout adapter. W.: With adapter.\ndetails). Inspired by He et al. (2021), analyzing the\ninstability w.r.t different random seeds, we analyze\nother aspects of the stability: the stability to pre-\ntraining and ﬁne-tuning iterations and the gradient\nvanishing issue. For instance, as observed in Fig. 1,\nas the number of task-speciﬁc pretraining iteration\nvaries, CoLA’s performance is severely unstable in\nﬁne-tuning. Besides instability, we also observe\nthat task-speciﬁc pretraining is vulnerable to ad-\nversarial attack. Last but not least, task-speciﬁc\npretraining and/or ﬁne-tuning on the entire model\nis highly parameter-inefﬁcient given the large size\nof these models (e.g., the smallest BERT has 110\nmillion parameters).\nIn this work, we propose a simple yet effective\nadapter-based approach to mitigate these issues.\nAdapters are some small bottleneck layers inserted\nwithin each layer of a pretrained model (Houlsby\net al., 2019; Pfeiffer et al., 2020a,b). The adapter\nlayers are much smaller than the pretrained model\nin terms of the number of parameters. For instance,\nthe adapter used in Houlsby et al. (2019) only adds\n3.6% parameters per task. In our approach, we\nadapt the pretrained model to a downstream task\nthrough 1) task-speciﬁc pretraining and 2) task-\nspeciﬁc supervised training (namely, ﬁne-tuning)\non the downstream task (e.g., classiﬁcation, se-\nquence labeling) by only optimizing the adapters\nand keeping all other layers ﬁxed. Our approach is\nparameter-efﬁcient given that only a small number\nof parameters are learned in the adaptation.\nThe adapted model learned through our approach\ncan be viewed as a residual form of the original\npretrained model. Suppose x is an input sequence\nand horiginal is the features of x computed by the\noriginal model. Then the feature computed by the\nadapted model is,\nhadapted = horiginal + fadapter(x), (1)\nwhere fadapter(x) is the residual feature in addi-\ntion to horiginal and fadapter is the adapter learned\nin the adaptation process. horiginal extracts general\nfeatures that are shared across tasks, while fadapter\nis learned to extract task-speciﬁc features. In prior\nwork (Houlsby et al., 2019; Pfeiffer et al., 2020b),\nfadapter is learned with task-speciﬁc supervised\nlearning objective, distinctive from the unsuper-\nvised pretraining objective, and might not be com-\npatible with horiginal, as evidenced in our experi-\nments. In our approach, fadapter is ﬁrst trained with\nthe same pretraining objective2 on the task-speciﬁc\ndata before being adapted with the supervised train-\ning objective, encouraging the compatibility be-\ntween horiginal and fadapter, which is shown to im-\nprove the downstream task performance in our ex-\nperiments (see Table 1).\nSome prior works have examined the potential\ncauses of the instability of pretrained language\nmodels in transfer learning. Lee et al. (2019)\nproposed that catastrophic forgetting in sequential\ntransfer learning underlined the instability, while\nMosbach et al. (2020) proposed that gradient van-\nishing in ﬁne-tuning caused it. Pinpointing the\ncause of transfer learning instability is not the fo-\ncus of the current work, but our proposed method\nseems to able to enhance transfer learning on both\naspects.\nThe standard sequential transfer learning or that\nwith task-speciﬁc pretraining updates all model pa-\nrameters in ﬁne-tuning. In contrast, our approach\nkeeps the pretrained parameters unchanged and\nonly updates the parameters in the adapter layers,\nwhich are a small amount compared to the pre-\ntrained parameters. Therefore, our approach natu-\nrally alleviates catastrophic forgetting considering\nthe close distance between the original pretrained\nmodel and the adapted model. On the other hand,\nwe do not observe gradient vanishing with our trans-\nfer learning scheme (see Section 2 for more details).\n2In this work, we conduct experiments with the most\nwidely used pretraining objective, masked language modeling.\nThe same training scheme can be extended to other pretraining\nobjectives.\nFigure 2: Distribution of dev scores on RTE from 10 random\nseed restarts when ﬁnetuning (1) BERT (Devlin et al., 2019)\nand (2) BERT with the adapter architecture.\nThis might be because optimizing over a much\nsmaller parameter space in our approach, compared\nto the standard sequential transfer learning scheme\nwhere all parameters are trained, renders the op-\ntimization easier. We leave it to future work for\nfurther theoretical analysis.\nIn addition to its improved stability, the proposed\ntransfer learning scheme is also likely to be more\nrobust to adversarial attack. Given that it updates\nthe entire model, the standard transfer learning ap-\nproach might suffer from overﬁtting to the down-\nstream task, and thus a small perturbation in the\ninput might result in consequential change in the\nmodel prediction. In turn, it might be susceptible\nto adversarial attack. Our approach only updates\na much smaller portion of parameters, and hence\nmight be more robust to these attacks, which is con-\nﬁrmed in our empirical analysis (see Section 4).\nContributions. In summary our work has the\nfollowing contributions. (1) We propose a sim-\nple and parameter-efﬁcient approach for trans-\nfer learning. (2) We demonstrate that our ap-\nproach improves the stability of the adaptation\ntraining and adversarial robustness in downstream\ntasks. (3) We show the improved performance of\nour approach over strong baselines. Our source\ncode is publicly available at https://github.\ncom/WinnieHAN/Adapter-Robustness.git.\n2 Instability to Different Random Seeds\nWe ﬁrst evaluate the training instability with re-\nspect to multiple random seeds: ﬁne-tuning the\nmodel multiple times in the same setting, vary-\ning only the random seed. We conduct the experi-\nments on RTE (Wang et al., 2018) when ﬁne-tuning\n1) BERT-base-uncased (Devlin et al., 2019) and\n2) BERT-base-uncased with the adapter (Houlsby\n0.01\n0.05\n0.1\n0.5\n0 100 200 300 400\nLayer-1  (WO.) Layer-4  (WO.) Layer-8  (WO.)\nLayer-12   (WO.) Pooler Layer (WO.)\nClassification Layer  (WO.) Classification Layer (W.)\nFigure 3: Gradient norms (on log scale) of intermediate layer\nand classiﬁcation layer on RTE for with/without-adapter ﬁne-\ntuning run. WO.: Without adapter. W.: With adapter.\nFigure 4: Box plots showing the TSP. stability of BERT\nwith/without adapter on CoLA.\nFigure 5: Attack success rate of BERT with/without adapter\nduring task-speciﬁc pretraining. WO.: Without adapter. W.:\nWith adapter.\n(a) Pretraining Iteration 0\n (b) Pretraining Iteration 10000\n (c) Pretraining Iteration 20000\nFigure 9: Box plots showing the ﬁne-tuning stability of BERT with/without adapter for different TSP. iterations on CoLA. WO.:\nWithout adapter. W.: With adapter.\net al., 2019) 3. As shown in Figure 2, the model\nwithout adapter leads to a large standard deviation\non the ﬁne-tuning accuracy, while the one with\nadapter results in a much smaller variance on the\ntask performance.\nGradient Vanishing Mosbach et al. (2020) ar-\ngues that the ﬁne-tuning instability can be ex-\nplained by optimization difﬁculty and gradient van-\nishing. In order to inspect if the adapter-based\napproach suffers from this optimization problem,\nwe plot the L2 gradient norm with respect to differ-\nent layers of BERT, pooler layer and classiﬁcation\nlayer, for ﬁne-tuning with or without adapter in\nFigure 3.\nIn traditional ﬁne-tuning (without adapter), we\nsee vanishing gradients for not only the top layers\nbut also the pooler layer and classiﬁcation layer.\nThis is in large contrast to the with-adapter ﬁne-\ntuning. The gradient norm in the with-adapter ﬁne-\ntuning does not decrease signiﬁcantly in the train-\ning process. These results imply that the adaptation\nwith adapter does not exhibit gradient vanishing\nand presents a less difﬁcult optimization problem,\nwhich in turn might explain the improved stability\nof our approach.\n3 Instability to Pretraining and\nFine-tuning Iterations\nFine-tuning with all parameters also exhibits an-\nother instability issue. In particular, ﬁne-tuning a\nmodel multiple times on the pretrained language\nmodel, varying the task-speciﬁc pretraining itera-\ntions and ﬁne-tuning iterations, leads to a large stan-\ndard deviation in downstream task performance. As\n3For all the experiments, we use the implementa-\ntion of Pfeiffer et al. (2020b): https://github.com/\nAdapter-Hub/adapter-transformers.git.\nobserved in Figure 1, CoLA’s performance when\nvarying the task-speciﬁc pretraining iterations is\nseverely unstable during pretraining iterations and\nﬁne-tuning iterations. The model has converged at\nthe pretraining iteration of 8000. However, ﬁne-\ntuning based on this model does not obtain the best\nperformance.\nPretraining Iterations. Figure 4 displays the\nperformance on CoLA of 10 ﬁne-tuning runs with\nand without the adapter. For each run, we vary only\nthe number of pretraining iterations from 2000 to\n20000 with an interval of 2000 and ﬁx the ﬁne-\ntuning epochs to 10. We clearly observe that most\nruns for BERT with adapter outperforms the one\nwithout adapter. Moreover, the adapter makes pre-\ntraining BERT signiﬁcantly more stable than the\nstandard approach (without adapter).\nFine-tuning Iterations. We then study the sta-\nbility with regard to the number of ﬁne-tuning\niterations. We show box plots for BERT using\nvarious pretraining iterations and ﬁne-tuning iter-\nations, with and without adapter in Figure 9. The\nthree sub-ﬁgures represent the early, mid, and late\nstages of pretraining, corresponding to the 0-th,\n10000-th, and 20000-th iteration respectively. The\n0-th iteration represents the original model without\ntask-speciﬁc pretraining. The model suffers from\nunderﬁtting in the 0-th iteration and overﬁtting in\nthe 20000-th iteration.\nIn Figure 9 (a), we plot the distributions of the de-\nvelopment scores from 100 runs when ﬁne-tuning\nBERT with various ﬁne-tuning epochs ranging\nfrom 1 to 100. In the early stage, the average de-\nvelopment score of the model with the adapter is a\nlittle lower than the baseline model while the sta-\nbility is better. After several epochs of pretraining,\nthe adapter gradually shows improved performance\nin terms of the mean, minimum and maximum as\ndemonstrated in Figure 9 (b). In the end of the pre-\ntrainig, there exists an over-ﬁtting problem for the\ntraditional BERT models. Pretraining transfers the\nmodel to a speciﬁc domain and fails to maintain the\noriginal knowledge. In contrast, the performance\nwith the adapter still grows as training continues\nand consistently beneﬁt from pretraining. Besides,\nwe observe that the adapter leads to a small vari-\nance in the ﬁne-tuning performance, especially in\nthe late stage. Additional plots and learning curves\ncan be found in the Appendix.\n4 Adversarial Robustness\nWhile successfully applied to many domains, the\npredictions of Transformers (Vaswani et al., 2017)\nbecome unreliable in the presence of small adver-\nsarial perturbations to the input (Sun et al., 2020;\nLi et al., 2020). Therefore, the adversarial attacker\nhas become an important tool (Moosavi-Dezfooli\net al., 2016) to verify the robustness of models.\nThe robustness is usually evaluated from attack\neffectiveness (i.e., attack success rate). We use a\nSOTA adversarial attack approach to assess the ro-\nbustness: PWWS attacker (Ren et al., 2019). 4.\nFigure 5 shows the attack success rate of BERT\nwith/without adapter during task-speciﬁc pretrain-\ning on SST-2. The x-axis is the number of epochs\nfor task-speciﬁc pretraining. It can be observed that\nthe model with the adapter has better adversarial\nrobustness.\n5 Conclusion\nWe propose a simple yet effective transfer learning\nscheme for large-scale pretrained language model.\nWe insert small bottleneck layers (i.e., adapter)\nwithin each block of the pretrained model and then\noptimize the adapter layers in task-speciﬁc unsu-\npervised pretraining and supervised training (i.e.,\nﬁne-tuning) while ﬁxing the pretrained layers. Ex-\ntensive experiments demonstrate that our approach\nleads to improved stability with respect to different\nrandom seeds and different number of iterations in\ntask-speciﬁc pretraining and ﬁne-tuning, enhanced\nadversarial robustness, and better transfer learning\n4We use the implementation in OpenAttack toolkit\nhttps://github.com/thunlp/OpenAttack.git.\nIt generates adversarial examples and evaluation the adver-\nsarial robustness of the victime model using thee adversarial\nexamples. We use the default settings including all the\nhyper-parameter values.\ntask performance. We therefore consider the pro-\nposed training scheme as a robust and parameter-\nefﬁcient transfer learning approach.\nAcknowledgments\nY . W. is partially supported by NSF DMS 2015577.\nReferences\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. arXiv\npreprint arXiv:2004.10964.\nRuidan He, Linlin Liu, Hai Ye, Qingyu Tan, Bosheng\nDing, Liying Cheng, Jia-Wei Low, Lidong Bing,\nand Luo Si. 2021. On the effectiveness of adapter-\nbased tuning for pretrained language model adapta-\ntion. arXiv preprint arXiv:2106.03164.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for nlp.\nIn International Conference on Machine Learning,\npages 2790–2799. PMLR.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2019. Mixout: Effective regularization to ﬁnetune\nlarge-scale pretrained language models. In Interna-\ntional Conference on Learning Representations.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2020. Biobert: a pre-trained biomed-\nical language representation model for biomedical\ntext mining. Bioinformatics, 36(4):1234–1240.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettlemoyer.\n2020. Pre-training via paraphrasing. Advances in\nNeural Information Processing Systems, 33.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang\nXue, and Xipeng Qiu. 2020. Bert-attack: Adver-\nsarial attack against bert using bert. arXiv preprint\narXiv:2004.09984.\nHairong Liu, Mingbo Ma, Liang Huang, Hao Xiong,\nand Zhongjun He. 2019. Robust neural machine\ntranslation with joint textual and phonetic embed-\nding. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics,\npages 3044–3049, Florence, Italy. Association for\nComputational Linguistics.\nSeyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,\nand Pascal Frossard. 2016. Deepfool: a simple and\naccurate method to fool deep neural networks. In\nProceedings of the IEEE conference on computer vi-\nsion and pattern recognition, pages 2574–2582.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2020. On the stability of ﬁne-tuning\nbert: Misconceptions, explanations, and strong base-\nlines. In International Conference on Learning Rep-\nresentations.\nDat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.\n2020. BERTweet: A pre-trained language model for\nEnglish Tweets. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, pages 9–14.\nErik Nijkamp, Bo Pang, Ying Nian Wu, and Caiming\nXiong. 2021. SCRIPT: Self-critic PreTraining of\ntransformers. In Proceedings of the 2021 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 5196–5202, Online. As-\nsociation for Computational Linguistics.\nMatthew E Peters, Sebastian Ruder, and Noah A Smith.\n2019. To tune or not to tune? adapting pretrained\nrepresentations to diverse tasks. In Proceedings of\nthe 4th Workshop on Representation Learning for\nNLP (RepL4NLP-2019), pages 7–14.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e,\nKyunghyun Cho, and Iryna Gurevych. 2020a.\nAdapterfusion: Non-destructive task composi-\ntion for transfer learning. arXiv preprint\narXiv:2005.00247.\nJonas Pfeiffer, Andreas R ¨uckl´e, Clifton Poth, Aish-\nwarya Kamath, Ivan Vuli ´c, Sebastian Ruder,\nKyunghyun Cho, and Iryna Gurevych. 2020b.\nAdapterhub: A framework for adapting transform-\ners. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 46–54.\nJason Phang, Thibault F ´evry, and Samuel R Bowman.\n2018. Sentence encoders on stilts: Supplementary\ntraining on intermediate labeled-data tasks. arXiv\npreprint arXiv:1811.01088.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nShuhuai Ren, Yihe Deng, Kun He, and Wanxiang Che.\n2019. Generating natural language adversarial ex-\namples through probability weighted word saliency.\nIn Proceedings of the 57th annual meeting of the as-\nsociation for computational linguistics, pages 1085–\n1097.\nSebastian Ruder. 2019. Neural transfer learning for\nnatural language processing. Ph.D. thesis, NUI Gal-\nway.\nLichao Sun, Kazuma Hashimoto, Wenpeng Yin, Akari\nAsai, Jia Li, Philip Yu, and Caiming Xiong. 2020.\nAdv-bert: Bert is not robust on misspellings! gen-\nerating nature adversarial samples on bert. arXiv\npreprint arXiv:2003.04985.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP,\npages 353–355.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nA Hyper-Parameters Setting\nWe conduct the experiments on the task of\nGLUE tasks (Wang et al., 2018) when ﬁne-\ntuning 1) BERT-base-uncased (Devlin et al.,\n2019) and 2) BERT-base-uncased with the\nadapter architecture (Houlsby et al., 2019). For\nall the experiments, we use the implementa-\ntion from https://github.com/Adapter-Hub/\nadapter-transformers.git. For the model with\nadapter, we follows the setup from Mosbach et al.\n(2020). For all experiments, we use the default\nhyper-parameters except for the number of epochs.\nPlease refer to the provided link.\nThe main hyper-parameters are listed in Table 2\nand Table 3.\nMax Sequence Length 256\nBatch Size 32\nLearning rate 1e-4\nNumber of Epochs 20\nTable 2: Hyper-parameters for BERT with Adapter.\nMax Sequence Length 128\nBatch Size 32\nLearning rate 2e-5\nNumber of Epochs 10\nTable 3: Hyper-parameters for BERT without Adapter.\nB Instability to Pretraining and\nFine-tuning Iterations\nWe provide box plots for BERT using various pre-\ntraining iterations and ﬁne-tuning iterations, with\nand without adapter on CoLA in Figure 10. The\ncorresponding learning curves are in Figure 13.\nC Instability for Large Dataset\nIn contrast to relatively large datasets, smaller data\nis more suitable and convincing as an example\nto analyze stability. Small dataset is easier to en-\ncounter over-ﬁtting problems and often not stable\n(Devlin et al., 2019). We use MNLI to evaluate the\ntraining instability in terms of 5 random seeds with\nthe same setup in Figure 2. The interquartile range\nof BERT with adapter on the distribution of dev\nscores is smaller than BERT without adapter. It\nshows that the model without adapter consistently\nleads to the instability issue on the ﬁne-tuning ac-\ncuracy, while the adapter architecture brings less\nbeneﬁt with larger dataset.\nFigure 10: Box plots showing the ﬁne-tuning stability of BERT with/without adapter for different pretraining\niteration from 0 to 20000.\nEpoch\nAcc.\n0.45\n0.50\n0.55\n0.60\n0.65\n20 40 60 80 100\nIteration-0\nIteration-2000\nIteration-4000\nIteration-6000\nIteration-8000\nIteration-10000 \nIteration-12000\nIteration-14000\nIteration-16000\nIteration-18000\nIteration-20000\n(a) BERT with adapter.\nEpoch\nAcc.\n0.45\n0.50\n0.55\n0.60\n0.65\n0 25 50 75 100\nIteration-0\nIteration-2000\nIteration-4000\nIteration-6000\nIteration-8000\nIteration-10000 \nIteration-12000\nIteration-14000\nIteration-16000\nIteration-18000\nIteration-20000 (b) BERT without adapter.\nFigure 13: Learning curves of ﬁne-tuning when varying the pretraining iterations."
}