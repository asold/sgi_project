{
  "title": "End-to-End Temporal Action Detection With Transformer",
  "url": "https://openalex.org/W3173459793",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2033601844",
      "name": "Liu, Xiaolong",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2645498951",
      "name": "Wang Qimeng",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2146297392",
      "name": "Hu Yao",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2157396789",
      "name": "Tang Xu",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1967282524",
      "name": "Zhang Shi-wei",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2111813221",
      "name": "Bai Song",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166164994",
      "name": "Bai, Xiang",
      "affiliations": [
        "Huazhong University of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2964214371",
    "https://openalex.org/W3034623254",
    "https://openalex.org/W2472970127",
    "https://openalex.org/W2463824207",
    "https://openalex.org/W2471143248",
    "https://openalex.org/W2963247196",
    "https://openalex.org/W2962876901",
    "https://openalex.org/W2766402183",
    "https://openalex.org/W2607566495",
    "https://openalex.org/W2964216549",
    "https://openalex.org/W2983918066",
    "https://openalex.org/W2550143307",
    "https://openalex.org/W2986407524",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2964008341",
    "https://openalex.org/W2883915488",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2994508843",
    "https://openalex.org/W2336403884",
    "https://openalex.org/W1927052826",
    "https://openalex.org/W2529163075",
    "https://openalex.org/W2751832138",
    "https://openalex.org/W3099758941",
    "https://openalex.org/W3172347581",
    "https://openalex.org/W2597958930",
    "https://openalex.org/W2962677524",
    "https://openalex.org/W2884969173",
    "https://openalex.org/W2964274041",
    "https://openalex.org/W2519328139",
    "https://openalex.org/W2755876276",
    "https://openalex.org/W2952435096",
    "https://openalex.org/W3176444885",
    "https://openalex.org/W3069380482",
    "https://openalex.org/W3111420154",
    "https://openalex.org/W2963321993",
    "https://openalex.org/W3095669214",
    "https://openalex.org/W2962709777",
    "https://openalex.org/W2884293275",
    "https://openalex.org/W2948229620",
    "https://openalex.org/W2895240652",
    "https://openalex.org/W2989042503",
    "https://openalex.org/W3189800722",
    "https://openalex.org/W3177433885",
    "https://openalex.org/W2950534130",
    "https://openalex.org/W2998601171",
    "https://openalex.org/W3173212682",
    "https://openalex.org/W3109715102",
    "https://openalex.org/W3203848195",
    "https://openalex.org/W3185799881",
    "https://openalex.org/W4312747676",
    "https://openalex.org/W4225264236",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W6790307280",
    "https://openalex.org/W6839769395",
    "https://openalex.org/W3035265375",
    "https://openalex.org/W2963351113",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W6789619165",
    "https://openalex.org/W3128626728",
    "https://openalex.org/W6795672903",
    "https://openalex.org/W3106041614",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W2507009361",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W6745476028",
    "https://openalex.org/W3110589170",
    "https://openalex.org/W3208474254",
    "https://openalex.org/W2593722617",
    "https://openalex.org/W4313050661",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2805042136",
    "https://openalex.org/W3092462694",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W3168294587",
    "https://openalex.org/W2604113307",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W4312294051",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2765277449",
    "https://openalex.org/W3123394884",
    "https://openalex.org/W4226142324",
    "https://openalex.org/W2599765304",
    "https://openalex.org/W2486996822",
    "https://openalex.org/W3100481960",
    "https://openalex.org/W3164690902",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W4287551379",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Temporal action detection (TAD) aims to determine the semantic label and the temporal interval of every action instance in an untrimmed video. It is a fundamental and challenging task in video understanding. Previous methods tackle this task with complicated pipelines. They often need to train multiple networks and involve hand-designed operations, such as non-maximal suppression and anchor generation, which limit the flexibility and prevent end-to-end learning. In this paper, we propose an end-to-end Transformer-based method for TAD, termed TadTR. Given a small set of learnable embeddings called action queries, TadTR adaptively extracts temporal context information from the video for each query and directly predicts action instances with the context. To adapt Transformer to TAD, we propose three improvements to enhance its locality awareness. The core is a temporal deformable attention module that selectively attends to a sparse set of key snippets in a video. A segment refinement mechanism and an actionness regression head are designed to refine the boundaries and confidence of the predicted instances, respectively. With such a simple pipeline, TadTR requires lower computation cost than previous detectors, while preserving remarkable performance. As a self-contained detector, it achieves state-of-the-art performance on THUMOS14 (56.7% mAP) and HACS Segments (32.09% mAP). Combined with an extra action classifier, it obtains 36.75% mAP on ActivityNet-1.3. Code is available at https://github.com/xlliu7/TadTR.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 1\nEnd-to-end Temporal Action Detection with\nTransformer\nXiaolong Liu, Qimeng Wang, Yao Hu, Xu Tang, Shiwei Zhang, Song Bai, and Xiang Bai, Senior Member, IEEE\nAbstractâ€”Temporal action detection (TAD) aims to determine\nthe semantic label and the temporal interval of every action\ninstance in an untrimmed video. It is a fundamental and\nchallenging task in video understanding. Previous methods tackle\nthis task with complicated pipelines. They often need to train\nmultiple networks and involve hand-designed operations, such\nas non-maximal suppression and anchor generation, which limit\nthe ï¬‚exibility and prevent end-to-end learning. In this paper,\nwe propose an end-to-end Transformer-based method for TAD,\ntermed TadTR. Given a small set of learnable embeddings\ncalled action queries, TadTR adaptively extracts temporal context\ninformation from the video for each query and directly predicts\naction instances with the context. To adapt Transformer to\nTAD, we propose three improvements to enhance its locality\nawareness. The core is a temporal deformable attention module\nthat selectively attends to a sparse set of key snippets in a video. A\nsegment reï¬nement mechanism and an actionness regression head\nare designed to reï¬ne the boundaries and conï¬dence of the pre-\ndicted instances, respectively. With such a simple pipeline, TadTR\nrequires lower computation cost than previous detectors, while\npreserving remarkable performance. As a self-contained detector,\nit achieves state-of-the-art performance on THUMOS14 (56.7%\nmAP) and HACS Segments (32.09% mAP). Combined with an\nextra action classiï¬er, it obtains 36.75% mAP on ActivityNet-1.3.\nCode is available at https://github.com/xlliu7/TadTR.\nIndex Termsâ€”Transformer, Temporal Action Detection, Tem-\nporal Action Localization, Action Recognition.\nI. I NTRODUCTION\nV\nIDEO understanding has become more important than\never as the rapid growth of media prompts the generation,\nsharing, and consumption of videos. As a fundamental task in\nvideo understanding, temporal action detection (TAD) aims to\npredict the semantic label, the start time, and the end time\nof every action instance in an untrimmed and possibly long\nvideo. For its wide range of applications, including security\nsurveillance, home care, video editing, video recommendation,\nand so on, temporal action detection has gained increasing\nattention from the community in recent years [1]â€“[5].\nPrevious methods for TAD can be roughly categorized into\ntwo groups. Top-down methods [1], [10], [11] perform classiï¬-\nThis work was supported by National Key R&D Program of China (No.\n2018YFB1004600). (Corresponding author: Xiang Bai.)\nX. Liu (email: brucelio@outlook.com) and Q. Wang are with the School of\nElectronic Information and Communications, Huazhong University of Science\nand Technology. X. Bai (email: xbai@hust.edu.cn) is with the School of\nArtiï¬cial Intelligence and Automation, Huazhong University of Science and\nTechnology. Y . Hu, X. Tang, and S. Zhang are with Alibaba Group. S. Bai is\nwith ByteDance Inc. Part of this work was done when X. Liu was an intern\nat Alibaba Group.\nThis paper has supplementary downloadable material available at\nhttp://ieeexplore.ieee.org., provided by the author.\n(a)\n(b)\n(c)\n(e)\n(d)\nTop-down methods\nAnchors\nVideo Data Dense \nDetections\nSparse \nDetections\nNMS\nClass-aware\nCls. & Reg.\nVideo Data Frame-level \nPredictions\nSparse \nDetections\nGrouping and/or NMS\nFrame-level\nCls. & Reg.\nSparse \nDetections\nLearned \nQueries\nVideo Data\nActionness\nRegression\n(Optional) Sparse \nDetections\nEncoding & \nDecoding\nClass-agnostic\nCls. & Reg.\nProposals\nClass-aware\nCls. & Reg. Dense \nDetections\nSparse\nDetections\nAnchors\nVideo Data\nNMS\nProposals\nStandalone \nClassifier Dense \nDetections\nSparse \nDetections\nAnchors\nVideo Data\nClass-agnostic\nCls. & Reg. NMS\nDifferentiable step Indifferentiable step\nFig. 1: Comparison of different pipelines of temporal action\ndetection. (a) Multi-stage pipeline in [1], [2], etc.; (b) Two-\nstage pipeline in [6], [7]; (c) Top-down one-stage pipeline\nin [8], (d) Bottom-up pipeline in [9] (e) The set prediction\npipeline in this work.\ncation and regression on a large amount of candidate segments.\nBottom-up methods [9], [12] perform per-frame classiï¬cation\nand group these predictions into segment-level predictions.\nWhile these methods achieve state-of-the-art performance on\nstandard benchmarks, they have complex pipelines.\nAs shown in Fig. 1, these pipelines involve post-processing\noperations, such as non-maximum suppression (NMS) and\ngrouping. These operations, together with anchor setting in\nmany top-down methods, are hand-designed with prior knowl-\nedge about this task and not learnable, which restricts the ï¬‚ex-\nibility. Besides, most proposal-based methods [2], [11], [13],\nrequires a standalone classiï¬er to classify action proposals.\nThese issues block the gradient ï¬‚ow and prevent end-to-end\nlearning. Thus, it is necessary to develop a simple end-to-\nend method that directly predicts action instances in a single\ndifferentiable network1 without hand-crafted components.\nIn this paper, we introduce an end-to-end temporal action\ndetection framework to address the above issues. Inspired by\nthe object detection Transformer (DETR) [15], we directly\nmap a set of learnable embeddings, called action queries,\nto action instances in parallel. As the queries do not di-\nrectly indicate the initial locations of actions like anchors or\nproposals, we are unable to extract features for each query\n1The single network means the detection network upon the video encoder.\nTraining the video encoders along with the detection head for long videos\noften requires excessive computing resources. Therefore, most methods use\nofï¬‚ine features ( e.g. I3D [14]) trained separately with large mini-batch size\non large amounts of short (around 2 seconds) videos.\n0000â€“0000/00$00.00 Â© 2021 IEEE\narXiv:2106.10271v4  [cs.CV]  11 Aug 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 2\nfrom speciï¬c locations like previous methods. The detector\nis required to extract sufï¬cient long-term context information\nbefore knowing which interval an action falls in. Besides, the\ncontext should be adaptive and relevant with each query, in\norder to differentiate between these queries. Traditional 1D\nconvolutional neural networks cannot easily achieve them, due\nto a ï¬xed receptive ï¬eld and ï¬xed weights. Recently, Trans-\nformers [16] have shown great power in sequence modeling. It\nis able to reason the relations between sequence elements and\nadaptively capture long-term context with the self-attention\nmodule. A video is naturally a sequence of frames and there\nis abundant context in it [2], [13], [17]. Therefore Transformer\nis a desirable choice for the above goals.\nBased on the above motivations, we propose a Temporal\nAction Detection TRansformer (TadTR) that predicts actions\nby extracting relevant context for each action query. It has an\nencoder-decoder structure. The encoder models inter-snippet\nrelations to capture snippet-level context. The decoder models\naction-snippet relations to enhance each action query with\nsnippet-level context, and inter-action relations to capture\ninstance-level context from the other action queries. In this\nway, we can exploit richer context than previous methods\nthat only exploit snippet-level context [2] or instance-level\ncontext [13]. Upon the decoder, two feed-forward networks\n(FFNs) predict the class and the segment for each action query.\nDuring training, an action matching module dynamically de-\ntermines a one-to-one ground truth assignment according to\nthe predictions. Owing to this, our detector avoids duplicate\ndetections and NMS is unnecessary. It produces a very sparse\nset of action detections ( 10 âˆ¼102), orders of magnitude fewer\nthan previous methods ( 103 âˆ¼104).\nHowever, due to the intrinsic difference between space and\ntime, a direct application of Transformer is not appropriate. We\nobserve that different frames in a video of actions are highly\nsimilar, because of the temporal redundancy and the slow\nchanges in backgrounds or actors. Besides, the boundaries of\nactions are less clear than those of objects [18]. Therefore,\nto precisely detect actions, a detector needs to be locality-\naware, which means being aware of the subtle local changes in\nthe temporal domain. The dense attention module in primitive\nTransformer that attends to all elements in a sequence, is less\nsensitive to such local changes by design. To mitigate this\nissue, we draw inspiration from [19] and propose a temporal\ndeformable attention (TDA) module as the basic building\nblock of Transformer. It selectively attends to a sparse set of\nkey elements around a reference location in the input sequence,\nwhere the sampling locations and attention weights are learned\nand dynamically adjusted in accordance with the inputs. In\nthis way, it can adaptively extract context information while\npreserving locality awareness.\nBesides TDA, we make two additional improvements to en-\nhance locality awareness. First, a segment reï¬nement mecha-\nnism is employed to reï¬ne the boundaries of predicted actions.\nTo be concrete, we iteratively re-attend to the video according\nto the previous predictions and reï¬ne the boundaries with\nthe newly extracted context. Second, we attach an actionness\nregression head to Transformer to predict a reliable conï¬dence\nscore called actionness for detection ranking. It extracts the\nlocal features with RoIAlign [20] for each predicted action and\nestimates its IoU with the best-matched ground truth action.\nThis is more reliable than simply using classiï¬cation scores,\nas the classiï¬cation branch may ï¬nd a shortcut from context\nbut ignore the complete local details. Despite being seemingly\nsmall changes, they signiï¬cantly improve performance.\nWe conduct comprehensive experiments on three datasets to\nevaluate TadTR. With a surprisingly simple pipeline, TadTR\nachieves remarkable performance with a low computation\ncost. Without any extra classiï¬er, it achieves state-of-the-art\nperformance on HACS Segments [21] and THUMOS14 [22].\nWhen combined an extra classiï¬er, it reaches 36.75% mAP\non ActivityNet-1.3 [23], outperforming strong competitors\nsuch as G-TAD [2] and BMN [11]. In terms of run time, it\ntakes only 155 ms per video on THUMOS14, which is much\nfaster than recent state-of-the-art methods, as shown in Fig. 2.\nWe believe that the simplicity, the ï¬‚exibility, and the strong\nperformance of the new method will beneï¬t and ease future\nresearch on temporal action detection.\nThe contributions of this work are as follows:\nâ€¢ We introduce an end-to-end set prediction (SP) frame-\nwork that simpliï¬es the pipeline for temporal action\ndetection (TAD). It can detect actions in a single dif-\nferentiable network without hand-crafted components.\nâ€¢ We propose a Transformer architecture that is enhanced\nwith locality awareness to better adapt to the TAD task.\nThe core is a temporal deformable attention (TDA) mod-\nule that selectively attends to a sparse set of key snippets\nin a video. We show that TDA is crucial for the success\nof the SP framework for TAD.\nâ€¢ Different from previous works that ignore context or only\nexploit snippet-level or instance-level context, we model\ninter-snippet, inter-action, and action-snippet relations to\ncapture both levels of context for more accurate temporal\naction detection.\nâ€¢ Our method achieves state-of-the-art performance of\nself-contained detectors on HACS Segments and THU-\nMOS14, and competitive results on ActivityNet-1.3. Be-\nsides, it requires a lower computation cost than its com-\npetitors.\nII. R ELATED WORK\nTemporal Action Detection. Previous TAD methods can be\nroughly categorized into top-down methods and bottom-up\nmethods according to the pipeline. Top-down methods can\nbe further categorized into multi-stage, two-stage, and one-\nstage methods. (a) Multi-stage methods [2], [11], [24]â€“[27]\nï¬rst generate candidate segments and train a binary classiï¬er\nthat associates each segment with a conï¬dence score, resulting\nin proposals. Those proposals with high scores are fed to\na multi-class classiï¬er to classify the actions. The candidate\nsegments are generated by dense uniform sampling [1], [28]\nor grouping local frames that may contain actions [29]. Some\nmethods [30], [31] combine multiple schemes for complemen-\ntarity. (b) Two-stage methods [6], [7], [32], [33] simplify\nthe multi-stage pipeline by adopting a one-stage proposal\ngenerator, which directly predicts the scores and boundaries of\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 3\n/uni00000013/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019/uni0000001a\n/uni00000006/uni00000059/uni0000004c/uni00000047/uni00000048/uni00000052/uni00000056/uni00000003/uni00000053/uni00000048/uni00000055/uni00000003/uni00000056/uni00000048/uni00000046/uni00000052/uni00000051/uni00000047\n/uni00000017/uni00000013\n/uni00000017/uni00000018\n/uni00000018/uni00000013\n/uni00000018/uni00000018/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000050/uni00000024/uni00000033\n/uni00000037/uni00000044/uni00000047/uni00000037/uni00000035\n/uni00000037/uni00000044/uni00000047/uni00000037/uni00000035/uni00000010/uni0000004f/uni0000004c/uni00000057/uni00000048\n/uni00000024/uni00000015/uni00000031/uni00000048/uni00000057\n/uni00000033/uni0000002a/uni00000026/uni00000031\n/uni0000002a/uni00000037/uni00000024/uni00000027\n/uni00000025/uni00000036/uni00000031\n/uni00000025/uni00000030/uni00000031\n/uni00000030/uni00000035\n/uni00000024/uni00000029/uni00000036/uni00000027\n/uni00000030/uni00000038/uni00000036/uni00000028/uni00000036\nFig. 2: Comparison of recent temporal action detection meth-\nods on THUMOS14, in terms of both performance (average\nmAP) and speed. Our method achieves state-of-the-art perfor-\nmance while running signiï¬cantly faster.\npre-deï¬ned multi-scale anchors associated with each temporal\nlocation. These methods need to manually set multiple anchor\nscales, which restricts the ï¬‚exibility. Note that multi-stage\nmethods can also be seen as generalized two-stage methods.\n(c) Top-down one-stage methods [8], [34] can be seen as the\nclass-aware variant of the one-stage proposal generator. (d)\nBottom-up methods perform frame-level action classiï¬cation\nand merge the frame-level results to segment-level predic-\ntions. For example, [9] ï¬rst predicts the action and boundary\nprobabilities and then groups frames with maximal structured\nsum as actions. Recent anchor-free methods ( e.g., AFSD [35]\nand A2Net [36]) also belong to this group. Besides these\nmethods, a few works ( e.g., CTAP [30] and PCG-TAL [37])\ncombine different pipelines to enhance the performance. All\nthe above methods require post-processing steps such as NMS\nor grouping, which prevent end-to-end learning. An early\nwork by Yeung et al. [38] also proposes a TAD method\nwithout hand-crafted components. Based on recurrent neural\nnetworks (RNN) and reinforcement learning (RL), it learns\naction detection by training an agent that iteratively picks an\nobservation location and deciding whether to emit or reï¬ne\na candidate action after observation. However, its reward\nfunction is not differetiable. Therefore it does not meet the\ncriteria of end-to-end in this paper. All the above methods\nare fully-supervised. There are also some weakly-supervised\nmethods that only utilize single-frame supervision [39] or\nvideo-level supervision [40]â€“[51] during training.\nTransformers and Context in Video Understanding. Trans-\nformers have achieved great success in natural language pro-\ncessing [16] and image understanding [52]â€“[54]. The core\nof Transformer is the self-attention mechanism that aggre-\ngates non-local cues through a weighted sum of features at\nattended locations. Compared with convolutions, self-attention\ncan capture long-range context and dynamically adjust weights\naccording to the input. Recently, many works have revealed\nthe great potential of Transformers in video understanding\ntasks [55]â€“[57]. For example, VideoBERT [55] and Act-\nBERT [58] utilize Transformers to learn an joint representation\nfor video and text. TimeSformer [56] decouples spatial and\ntemporal self-attention for video classiï¬cation. Zhou et al. [59]\ncapture the temporal dependency with Transformer for video\ncaptioning. Girdhar et al. [60] apply Transformer to model\nthe relationship between spatial proposals for spatio-temporal\naction detection.\nIn this paper, Transformer is used to capture temporal\ncontext information for temporal action detection. Speciï¬-\ncally, we employ attention modules to model the relations\nbetween video snippets, the relations actions and snippets,\nand the relations between actions. Several concurrent works\nalso employ Transformer for context modeling in temporal\naction detection (AGT [61]) and temporal action proposal\ngeneration (RTD-Net [62] and TAPG [63]). However, these\nworks either adopt a traditional TAD pipeline or have difï¬culty\nin training. TAPG still relies on hand-crafted anchors and\npost-processing steps. RTD-Net requires a three-step training\nscheme to optimize different parts of the network separately\nand relies on extra action classiï¬ers to classify the proposals.\nAGT suffers from slow training convergence (1000 Ã—more\niterations than TadTR). In addition, different from these works\nthat exploit the vanilla attention module, TadTR introduces\na more efï¬cient temporal deformable attention module that\nadaptively attends to a sparse set of key snippets in a video. As\na result, it enjoys lower computation costs and easier training.\nTherefore, TadTR is more practical.\nIn the ï¬eld of temporal action detection, some previous\nworks also exploit context in other ways. For example, increas-\ning the receptive ï¬eld by a ï¬xed ratio [7], [17]. However, this\nis not ï¬‚exible enough and may introduce irrelevant information\nfrom unrelated frames. Another line of works exploit context\nby modeling the relations between different snippets [2], [64]\nor the relations between different proposals [13] with graph.\nThe attention modules in this work are alternatives to them.\nMoreover, we model different kinds of relations and can\ncapture richer context of different levels.\nDETR and Deformable DETR. Temporal action detection\nmethods [6], [8], [36] often draw inspiration from object\ndetection methods. This work is inspired by DETR [15] and\nDeformable DETR [19]. DETR proposes a Transformer-based\nSet Prediction (SP) framework to achieve end-to-end ob-\nject detection without hand-crafted components. Deformable\nDETR proposes multi-scale deformable attention to address\nthe issues of slow convergence and limited feature resolution\nof DETR. While extending them for direct TAD is intuitive,\nthe effectiveness remains unclear. Our main contribution over\nDETR and Deformable DETR is that we adapt the SP frame-\nwork and deformable attention for direct TAD and validate\ntheir effectiveness. Although the high-level design of TadTR is\nsimilar to Deformable DETR, the implementation is different\nas TadTR aims to temporally localize actions in videos while\nDeformable DETR is designed for object detection in images.\nBesides, we reveal that deformable attention is crucial for the\nsuccess of the SP framework for TAD and segment reï¬nement\nis also important. Furthermore, directly extending Deformable\nDETR to TAD does not achieve satisfactory performance as\nthe conï¬dence scores predicted by the decoder are not reliable.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 4\nCNNTime\nâ€¦\nAction Queries Decoder & \nSegment Refinement\nPos. Embed.\nğ‘‹ğ‘ƒ âˆˆ â„ğ‘‡ğ‘†Ã—ğ¶\nVideo Features \nğ‘‹ğ‘‰ âˆˆ â„ğ‘‡ğ‘†Ã—ğ¶\nActionness{à·ğ’ˆğ’Š}\nRoIAlign\nFFN\nActionness Regression\n49.2s\n52.5s\n54.7s\n57.7s\nBaseball pitch\nScore: 0.83\nBaseball pitch\nScore: 0.85\nTime\nOutput\nTDA\nFFN\nAdd & Norm\nAdd & Norm\nEncoder\nğ¿ğ¸ Ã—\nğ‘‹ğ¸\nDec. Layer-l\nÆ¸ğ‘¡(ğ‘™) áˆ˜ğ‘‘(ğ‘™)\nFFN\náˆ˜ğ‘‘(ğ‘™âˆ’1)\nğ¿ğ· Ã—\nğ‘§(ğ‘™âˆ’1)\nà·œğ‘(ğ‘™)\nÆ¸ğ‘¡(ğ‘™âˆ’1)\nÆ¸ğ‘¡(0)\nğ‘§(ğ‘™)\nğ‘§(0)\nSelf Attention\nAdd & Norm\nTDA\nAdd & Norm\nFFN\nAdd & Norm\nğ‘‹ğ¸\nDec. Layer details\nFFN\nÆ¸ğ‘¡(ğ‘™âˆ’1)\nğ‘§(ğ‘™âˆ’1)\nğ‘§(ğ‘™)\nDetection à·ğ’šğ’Š:\nSegment \nÆ¸ğ‘ ğ‘– = ( Æ¸ğ‘¡ğ‘–, áˆ˜ğ‘‘ğ‘–)\nClass \nÆ¸ğ‘ğ‘– = argmax à·œğ‘ğ‘–\nConfidence score\nà·œğ‘ğ‘–( Æ¸ğ‘ğ‘–)âˆ— à·œğ‘”ğ‘– TDA: Temporal Deformable Attention                   \nFFN: Feed-Forward Network                   \nPos. Embed.:  Position embedding\nğ‘“ğ¼ğ‘… : Initial Reference Points Projection\nâŠ•ï¼šSegment Refinement Function\nFFN\nSegments {à·œğ’”ğ’Š}\nClass Scores {à·ğ’‘ğ’Š}\nNon-action\nğ‘“ğ¼ğ‘…\nFig. 3: The architecture of TadTR. It takes the video features extracted with a CNN and a set of learnable action queries as\ninput and decodes a set of action predictions in parallel via a Transformer. The encoder captures the long-term context in the\ninput feature sequence. The decoder extracts relevant context from the encoder for each action query and models the relations\nbetween action queries. Upon the decoder, we use feed-forward networks to predict the segments and classes of output actions.\nA segment reï¬nement mechanism (the blue box) and an actionness regression head (the orange box) are utilized to reï¬ne the\nboundaries and the conï¬dence scores of the predicted actions, respectively.\nTo relieve this issue, we add a simple yet effective actionness\nregression head to reï¬ne the conï¬dence scores. To sum up,\nthe adaptation and improvement over DETR and Deformable\nDETR make the SP framework practical for TAD and TadTR\ncan serve as a strong baseline for SP-based TAD.\nIII. T ADTR\nTadTR is constructed on video features encoded with a pre-\ntrained video classiï¬cation network ( e.g., I3D [14]). Fig. 3\nshows the overall architecture of TadTR. TadTR takes as input\nthe video features and a set of learnable action queries. Then\nit outputs a set of action predictions. Each action prediction is\nrepresented as a tuple of the temporal segment, the conï¬dence\nscore, and the semantic label. It consists of a Transformer\nencoder to model the interactions between video snippets,\na Transformer decoder to predict action segments, and an\nextra actionness regression head to estimate the conï¬dence\nscore of the predicted segments. During training, an action\nmatching module is used to determine a one-to-one ground\ntruth assignment to the action predictions.\nA. Architecture\nEncoder. Let XV âˆˆ RTSÃ—C denotes the video feature\nsequence, where TS and C are the length and dimension,\nrespectively. Each frame in the feature sequence is a feature\nvector extracted from a certain snippet in the video. Here, a\nsnippet means a sequence of a few (e.g., 8) consecutive frames.\nWe use linear projection to make C = 256 . The encoder\nmodels the relations between different snippets and outputs\na feature sequence XE âˆˆ RTSÃ—C enhanced with temporal\ncontext. As depicted in Fig. 3, it consists of LE Transformer\nencoder layers of the homogeneous architecture. Each encoder\nlayer has two sub-layers, i.e., a temporal deformable attention\n(TDA) module, and a feed-forward network (FFN). Layer\nnormalization [65] is used after each sub-layer and a residual\nconnection is added between the input of each sub-layer and\nthe output of the follow-up normalization layer. Except for\nTDA, all the other components are identical to the primitive\nTransformer [16].\nTDA is an alternative to the dense attention module in [16].\nThe high similarities between different frames and the vague-\nness of action boundaries require a detector to possess local-\nity awareness. In other words, the detector should be more\nsensitive to local changes in the temporal domain. The dense\nattention module that attends to all locations in an input feature\nsequence, is less sensitive to such local changes. Besides, it\nsuffers from high computation cost and slow convergence [19].\nTo better ï¬t the TAD task, we draw inspiration from [19] and\npropose a temporal deformable attention (TDA) module that\nadaptively attends to a sparse set of temporal locations around\na reference location in the input feature sequence.\nLet zq âˆˆRC be the feature of query qand tq âˆˆ[0,1] be the\nnormalized coordinate of the corresponding reference point.\nGiven an input feature sequence X âˆˆ RTSÃ—C, the output\nhm âˆˆRTSÃ—(C/M) of the m-th (mâˆˆ{1,2,...,M }) head of a\nTDA module is computed by an weighted sum of a set of key\nelements sampled from X:\nhm =\nKâˆ‘\nk=1\namqkW V\nmX((tq + âˆ†tmqk)TS), (1)\nwhere K is the number of sampling points, amqk âˆˆ [0,1]\nis the normalized attention weight, and âˆ†tmqk âˆˆ [0,1] is\nthe sampling offset relative to tq. X((tq + âˆ†tmqk)TS) is\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 5\nthe linear interpolated feature at (tq + âˆ†tmqk)TS as it is\nfractional. Following [19], the attention weight amqk and the\nsampling offset âˆ†tmqk are predicted from the query feature zq\nby linear projection. We normalize the attention weight with\nsoftmax to make âˆ‘K\nk=1 amqk = 1 . W V\nm âˆˆRCÃ—(C/M) is a\nlearnable weight. The output of TDA is computed by a linear\ncombination of the outputs of different heads:\nTDA(zq,tq,X) = W OConcat(h1,h2,...,h m), (2)\nwhere W O âˆˆRCÃ—C is a learnable weight.\nWhen computing the Ï„-th frame in the output sequence, the\nquery and the reference point are both the Ï„-th frame in the\ninput sequence. Therefore, we refer to TDA in the encoder as\ntemporal deformable self-attention (TDSA). The query feature\nis the summation of the input feature of that frame and the\nposition embedding at that location. The position embedding\nis used to differentiate between different locations in the\ninput sequence. In this paper, we use the sinusoidal position\nembedding following [16].\nXP(Ï„,Î³) =\n{\nsin Ï„\n10000Î³/C Î³ is even\ncos Ï„\n10000(Î³âˆ’1)/C Î³ is odd . (3)\nThe feed-forward network consists of two fully connected\n(FC) layers and a ReLU activation in between. It is the same\nacross different positions and can be viewed as a stack of two\n1D convolution layers with kernel size 1. The dimensions of\nthe two FC layers are CF = 2048 and C = 256, respectively.\nDecoder. The decoder takes as input the encoder features\nXE and Nq action queries with learnable embeddings Ë†z(0) =\n{Ë†z(0)\ni }Nq\ni=1. It transforms these embeddings to Nq action predic-\ntions Ë†Y = {Ë†yi}. As illustrated in Fig. 3, the decoder consists\nof LD sequential decoder layers. Each decoder layer has\nthree major sub-layers: a self-attention module, a temporal de-\nformable cross-attention (TDCA) module, and a feed-forward\nnetwork. Similar to each encoder layer, we add a residual\nconnection between each sub-layer and the following layer\nnormalization function. The output of the l-th decoder layer is\ndenoted by z(l). The self-attention module models the relation\nbetween action queries and updates their embeddings. The\nmotivation here is that multiple actions in one video are often\nrelated. For example, a cricket shot action often appears after\na cricket bowling action. To make an action prediction, each\nquery extracts relevant context information from the video via\nthe TDCA module. Given the encoder features XE and the\ninput embedding Ë†zi âˆˆRC, the output query embedding of\nTDCA is formulated as TDA(Ë†zi,Ë†ti,XE). Here, Ë†ti is the\ncoordinate of the reference point in XE. By default, it is\npredicted by a projection function fIR from Ë†z(0)\ni . fIR is\nimplemented with a linear layer and a follow-up sigmoid\nfunction for normalization. The reference point can be seen\nas the initial estimation of the center of the corresponding\naction segment. FFNs in the decoder layers have the same\narchitecture as those in the encoder layers.\nDifferent from TDSA in the encoder, the query embedding\nË†z(0)\ni and the reference point are learnable and shared by all\ninput videos. This allows the network to learn the global\ndistribution of the action locations in the training dataset,\nwhich is more ï¬‚exible than hand-crafted anchor setting or\nproposal sampling. An analysis is given in Sec. IV-D.\nPrediction Heads. Upon the output (the updated query em-\nbeddings) of each decoder layer, we apply FFNs to predict\nthe classiï¬cation probabilities Ë†pi and the temporal segment\nË†si = (Ë†ti, Ë†di) of the action instance Ë†yi corresponding to each\nquery. Both Ë†ti and Ë†di are normalized. To make the boundaries\nof the instances more accurate, a segment reï¬nement mecha-\nnism is proposed. Besides, an additional actionness regression\nhead is employed to reï¬ne the conï¬dence score. They are\ndetailed below.\nSegment Reï¬nement. Transformer is able to capture long-\nrange context information. However, the predicted action\nboundaries might be unsatisfactory for lack of locality. In-\nspired by [19], we introduce a reï¬nement mechanism to\nenhance locality awareness and improve localization perfor-\nmance. It involves two strategies. The ï¬rst is the incremental\nreï¬nement of segments. Instead of predicting the segments\nindependently at each decoder layer, we adjust the segments\naccording to previously predicted segments layer by layer.\nFormally, given each action segment Ë†s(lâˆ’1)\ni = (Ë†t(lâˆ’1)\ni , Ë†d(lâˆ’1)\ni )\npredicted at the (lâˆ’1)-th decoder layer, the l-th decoder layer\npredicts the location offsets (âˆ†Ë†t(l)\ni ,âˆ† Ë†d(l)\ni ) relative to Ë†s(lâˆ’1)\ni .\nThe corresponding reï¬ned segment Ë†s(l)\ni = (Ë†t(l)\ni , Ë†d(l)\ni ) is then\ncomputed by:\nË†t(l)\ni = Ïƒ(âˆ†Ë†t(l)\ni + Ïƒâˆ’1(Ë†t(lâˆ’1)\ni )),l âˆˆ{1,2,...,L D} (4)\nË†d(l)\ni = Ïƒ(âˆ† Ë†d(l)\ni + Ïƒâˆ’1( Ë†d(lâˆ’1)\ni )),l âˆˆ{2,3,...,L D}, (5)\nwhere Ïƒ(Â·) and Ïƒâˆ’1(Â·) are the sigmoid and the inverse sigmoid\nfunction, respectively. Specially, Ë†t(0)\ni , is the initial reference\npoint Ë†ti predicted by fIR. The initial value of Ë†d(l)\ni is Ë†d(1)\ni\npredicted at the ï¬rst decoder layer. The second is iterative\nreference point adjustment. We update the reference points\nof TDCA in each decoder layer instead of always using Ë†t(0)\ni .\nSpeciï¬cally, Ë†t(lâˆ’1)\ni , the reï¬ned segment center at the (lâˆ’1)-\nth decoder layer, is used as the reference point of TDCA at\nthe l-th decoder layer. In this way, TDCA can be adaptive to\nthe input video and better aligned with the local features of\nthe action instances. We validate the effectiveness of the two\nstrategies in the experiments.\nActionness Regression. One challenge of temporal action\ndetection is to generate reliable conï¬dence scores for ranking.\nTypically, classiï¬cation scores are used. However, the classiï¬-\ncation task focuses more on discriminative features and is less\nsensitive to the localization quality of an action. As a result,\nthe classiï¬cation score of the detections may be unreliable for\nranking. An example is shown in Fig. 4.\nTo mitigate this issue, we employ an actionness regression\nhead that extracts context aligned with the interval of a pre-\ndicted segment and predicts an actionness score upon it. Given\nthe encoder feature sequence XE and a predicted segment\nË†si by the decoder, we ï¬rst apply temporal RoIAlign [20]\nupon XE to obtain the aligned features Xsi âˆˆ RTRÃ—C\nwithin the interval deï¬ned by si from XE. Here, TR is the\nnumber of bins for RoIAlign. To include a certain amount of\ncontext information around the boundaries, we slightly expand\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 6\n1.0s 8.0s\n54.8s5.5s\nCurling: 0.50 \nCurling: 0.39 \n0.3s 52.0s\nCurling\nGround Truth Detections Time\nFig. 4: The Transformer may generate unreliable conï¬dence\nscores. Here, a prediction with a lower overlap with the curling\naction has a higher score than a more accurate prediction\n(0.50 vs. 0.39).\nthe segment by a factor of Ïµ when applying RoIAlign. The\nexpanded segment can be expressed as (Ë†ti,Ïµ Ë†di). Then, a feed-\nforward network is used to predict the actionness score Ë†gi\nfrom the aligned feature. Ë†gi is supervised by the maximal\nIoU gi (intersection over union) between si and all ground\ntruth actions. In this way, the detector is enforced to be more\nsensitive to local features in order to differentiate between\ndifferent segments.\nDiscussion. The actionness regression head is somewhat sim-\nilar to the second stage of the traditional two-stage method,\nas they can both reï¬ne the conï¬dence scores. However, the\nset prediction pipeline adopted by TadTR is signiï¬cantly\ndifferent from traditional pipelines. It is hard to categorize\nTadTR and its variants into one-stage or two-stage methods,\nas it does not need anchor setting and post-processing steps\nlike traditional one-stage or two-stage methods. Besides, the\nactionness regression head is very lightweight. It only needs\nto predict class-agnostic conï¬dence scores. The computation\ncost (in FLOPs) of this head is 6.59% of that of the full model.\nDifferently, the second stage of a two-stage method often con-\ntributes to a major amount of computation cost ( e.g., 99.98%\nin BMN [11]). We also note that TadTR can already make\nsparse and complete detections without actionness regression.\nB. Training and Inference\nAction Matching. The action matching module determines\nthe targets assigned to each detection during training. Inspired\nby DETR [15] in object detection, we frame it as a set-to-\nset bipartite matching problem to ensure a one-to-one ground\ntruth assignment.\nLet Y = {yj}Nq\nj=1 be a set of ground truth actions padded\nwith âˆ… (no action) and Ï€ be the permutation that assigns\neach target yj to the corresponding detection Ë†yÏ€(j). Bipartie\nmatching aims to ï¬nd a permutation that minimizes the overall\nmatching cost:\nË†Ï€= arg min\nNqâˆ‘\nj=1\nC(yj,Ë†yÏ€(j)). (6)\nThe matching cost considers the classiï¬cation probabilities and\nthe distance between ground truth and predicated segments.\nSpeciï¬cally, C(yj,Ë†yÏ€(j)) is deï¬ned as\n1 cjÌ¸=âˆ…[Lcls(pÏ€(j),cj) + Lseg(sj,Ë†sÏ€(j))], (7)\nwhere cj and sj are the class label and the temporal segment\nof yj. Lcls(pÏ€(j),cj) is the classiï¬cation term. We use cross-\nentropy loss by default. Lseg(sj,Ë†sÏ€(j)) is the distance between\nthe predicted location and the ground truth location, deï¬ned\nas\nÎ»iouLiou(sj,Ë†sÏ€(j)) + Î»coordLL1(sj,Ë†sÏ€(j)), (8)\nwhere LL1 is the L1 distance and Liou is the IoU loss. IoU\nloss is deï¬ned as the the opposite number of the IoU. Î»iou and\nÎ»coord are hyper-parameters. The matching problem is solved\nwith the Hungarian algorithm.\nThrough the set-based action matching, each ground truth\nwill be assigned to only one prediction, thus avoiding duplicate\npredictions. This brings two merits. First, TadTR does not rely\non the non-differentiable non-maximal suppression (NMS) for\npost-processing and enjoys end-to-end training. Second, we\ncan make sparse predictions with limited queries ( e.g. 10)\ninstead of dense predictions in many previous works ( e.g. tens\nof thousands for BMN [11] and G-TAD [2]), which saves the\ncomputation cost.\nIn a way, the action matching module performs a learnable\nNMS. The matching cost takes the classiï¬cation scores of\nthe detections into account. In this way, those detections with\nlower scores are more likely to be assigned with a non-action\ntarget. As a result, their classiï¬cation scores will be suppressed\nin the training process.\nLoss Functions. Once the ground truth assignment is deter-\nmined, we optimize the network by minimizing the following\nmulti-part loss functions:\nL=\nNqâˆ‘\nj=1\n[Lcls(pË†Ï€(j),cj) + 1 cjÌ¸=âˆ…Lseg(sj,Ë†sË†Ï€(j))\n+Î»actLL1(Ë†gË†Ï€(j),gË†Ï€(j))],\n(9)\nwhere the ï¬rst two items optimize the detections from the\ndecoder and the last one optimizes the outputs of actionness\nregression. Lcls uses focal loss [66]. Ë†Ï€ is the solution of\nEquation 6. Î»act is a hyper-parameter.\nInference. During inference, we ignore the action predictions\nfrom all but the last decoder layer. The conï¬dence score for\na detection Ë†yi is computed by\nâˆš\nË†pi(Ë†ci) Â·Ë†gi, where Ë†ci is the\npredicted action label.\nIV. E XPERIMENTS\nA. Experimental Setup\nDatasets and Evaluation Metrics. We conduct experiments\non THUMOS14 [22], HACS Segments [21], and ActivityNet-\n1.3 [23]. THUMOS14 is built on videos from 20 sports\naction classes. It contains 200 and 213 untrimmed videos\nfor training and testing. There are 3007 and 3358 action\ninstances on the two sets. The average length of actions is\n5 seconds. ActivityNet-1.3 and HACS Segments share the\nsame 200 classes of daily activities. Both datasets are split\ninto three sets: training, validation, and testing. The numbers\nof videos in these sets are 10024, 4926, and 5044 respectively\non ActivityNet-1.3, and 37613, 5981, and 5987 on HACS\nSegments. The average length of actions is 48 seconds on\nActivityNet-1.3 and 33 seconds on HACS Segments. On both\ndatasets, the annotations on the testing set are reserved by the\norganizers. Therefore, we evaluate on the validation set.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 7\nFollowing conventions, the mean average precision (mAP)\nat different IoU thresholds is used for performance evaluation.\nOn THUMOS14, the IoU thresholds for computing mAPs are\n[0.3 : 0 .7 : 0 .1]. On the other two datasets, we report mAPs\nat the thresholds {0.5,0.75,0.95}and the average mAP at\nthe thresholds [0.5 : 0 .95 : 0 .05]. For simplicity, we denote\nmAP at the IoU threshold Î± as mAPÎ± and the average mAP\nis referred to as mAP unless specially noted.\nVideo Feature Extraction. Most TAD methods are based on\nofï¬‚ine extracted video features. For easier comparison with\nthem, we also use video features as the input of our method.\nFor experiments on HACS Segments, we directly use the\nofï¬cial I3D features2, which are extracted with I3D trained on\nKinetics at 2FPS. On the other datasets, we use the commonly\nused features in previous works. On THUMOS14, the two-\nstream I3D [14] networks pre-trained on Kinetics [14] are\ntaken as the video encoder, and the features are extracted every\n8 frames. On ActivityNet-1.3, we use the two-stream TSN [67]\nfeatures extracted at 5FPS. Following previous works [2], [11],\nwe resize the video features to a ï¬xed length of 100 via linear\ninterpolation on ActivityNet-1.3 and HACS Segments. Since\nthe videos are long on THUMOS14, we follow [11] to crop\neach video feature sequence with windows of length 128 and\nstride 64 for training. In each window, we reserve the instances\ncontained in it and clip the instances that partially overlap\nwith it. We ignore the instances that have less than 1-second\noverlap with the window during training. This strategy is called\nlength-based instance ï¬ltering. During inference, the stride is\nincreased to 96 and the duplicate detections in the overlapped\nregion are merged with NMS. This strategy is called cross-\nwindow fusion (CWF). We also report the performance of\nTadTR tested on non-overlapping windows (with a stride of\n128). In this case, we simply take the union of detections from\nall windows of a video.\nImplementation Details. LE and LD are set to 2 and 4,\nrespectively. The loss weights Î»iou, Î»coord and Î»act are set\nto 2, 5 and 5 respectively. The numbers of attention heads\nM and sampling points K are set to 8 and 4, respectively.\nThe parameters of the linear layers that predict attention\nweights are initialized to zero. We initialize the linear lay-\ners that predict sampling offsets to make {âˆ†pmqk}8\nm=1 =\n(k,0,âˆ’k,0,k, 0,âˆ’k,0) at initialization. The expanding factor\nÏµ and the number of bins TR for RoIAlign in the actionness\nregression head are 1.5 and 16 respectively.\nTadTR is trained using AdamW [68] optimizer. The initial\nlearning rate is 2 Ã—10âˆ’4 and scaled by a factor of 0.1 after\ntraining for a certain number of epochs. The learning rates of\nthe linear projection layers for predicting attention weights and\nsampling offsets are multiplied by 0.1. We train the models\nfor 30, 15, and 30 epochs and decrease the learning rates after\n25, 12, and 25 epochs on THUMOS14, ActivityNet-1.3, and\nHACS Segments respectively. The batch size is set to 16.\nIn the experiments, we also explore an improved training\nsetting. Following RetinaNet [66] and AFSD [35], we use\nfocal loss [66] for the classiï¬cation term in the matching cost\n2http://hacs.csail.mit.edu/hacs_segments_features.zip\n(Eq. 7). We ï¬nd that this modiï¬cation speeds up convergence.\nTherefore the total numbers of training epochs are reduced to\n16, 12, and 20 on the three datasets, respectively. The learning\nrates are decreased after 14, 9, and 18 epochs, respectively.\nBesides, we use the integrity-based instance ï¬ltering (IBIF)\nstrategy in G-TAD [2] and AFSD [35] to replace the default\nlength-based instance ï¬ltering strategy on THUMOS14. To be\nspeciï¬c, in each window, we only keep those ground truth\ninstances whose integrity exceeds 0.75. Here, the integrity of\nan instance sg in a window sw is deï¬ned as |sg âˆ©sw|/|sg|,\nwhere |Â·| means the length. It is similar to IoU but has a\ndifferent denominator. Those windows without such instances\nare ignored during training.\nThe experiments are conducted on a workstation with a\nsingle Tesla P100 GPU card, and Intel(R) Xeon(R) CPU\nE5-2682 v4 @ 2.50GHz. It takes around 10 minutes, 36\nminutes, and 150 minutes to ï¬nish training on THUMOS14,\nActivityNet-1.3, and HACS Segments, respectively.\nB. Main Results\nTHUMOS14. Table I demonstrates the temporal action de-\ntection performance and run time comparison on the testing\nset of THUMOS14. We measure the run time of these meth-\nods with publicly available implementations under the same\nenvironment (a single P100 GPU). We run methods on the\nfull testing set with batch size set to 1 and report the average\ntime and FLOPs per video. The average length of videos on\nTHUMOS14 is 217 seconds. BMN [11] and G-TAD [2] use\ntwo-stream TSN [67] features originally. For a fair compari-\nson, we also report their performance with I3D features. For\nAFSD [35], we have excluded the computation cost of the\nfeature extractor. For TadTR, we report the performance with\ndifferent training and inference settings. The entry with Â§is\nwith integrity-based instance ï¬ltering (IBIF). The entries with\nâˆ—are with IBIF and focal loss. TadTR-lite is the variant that\ndoes not use cross-window fusion (CWF) during inference.\nWe observe that:\n1) TadTR* achieves the best performance among all the\ncompared methods in terms of mAP at all IoU thresholds.\nEven the variant without CWF can achieve state-of-the-art\nperformance. TadTR* is slightly better than TadTR Â§ owing\nto focal loss.\n2) TadTR* outperforms the second-best method MUSES [27]\nby 3.3% in terms of average mAP, which demonstrates the\nadvantage of our method. Compared with the competitive\nsingle-network method A2Net [36], TadTR achieves 15.1%\nhigher average mAP.\n3) Compared with the concurrent Transformer-based methods\nAGT [61] and RTD-Net [62], TadTR* achieves better perfor-\nmance. It surpasses AGT by 9.7% in terms of mAP 0.5. It also\noutperforms RTD-Net by 7.7% (56.7% vs. 49.0%) in terms\nof average mAP. Besides the advantage in accuracy, TadTR is\neasier to train. Differently, AGT requires 1000Ã—more training\niterations (3000k vs. 3k for TadTR) due to slow convergence of\ndense attention. RTD-Net requires a three-step training scheme\nto optimize different parts of the network.\n4) Our results are achieved at a low computation cost. TadTR\nis around 11 Ã—faster than the second-best method MUSES,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 8\nTABLE I: Comparison with state-of-the-art methods on THUMOS14. Run time is the average inference time per video,\nincluding post-processing operations, such as NMS. SN: single-network. E2E: end-to-end. TS: two-stream. â™¯For proposal\ngeneration methods, the computation cost of the extra classiï¬ers is not included (marked with >). â€ Results copied from [36].\nâ€¡Our implementation. * With focal loss and IBIF. Â§ With IBIF.\nMethod Feature SN E2E mAP0.3 mAP0.4 mAP0.5 mAP0.6 mAP0.7 mAP Time/ms GFLOPs\nYeung et al. [38] VGG16 \u0013 - 36.0 26.4 17.1 - - - - -\nYuan et al. [9] TS - - 36.5 27.8 17.8 - - - - -\nSSAD [8] TS \u0013 - 43.0 35.0 24.6 - - - - -\nR-C3D [6] C3D \u0013 - 44.8 35.6 28.9 - - - - -\nSSN [69] TS - - 51.9 41.0 29.8 - - - - -\nTAL-Net [7] I3D \u0013 - 53.2 48.5 42.8 33.8 20.8 39.8 - -\nBSN [29] TS - - 53.5 45.0 36.9 28.4 20.0 36.8 >2065 >3.4\nMGG [31] TS - - 53.9 46.8 37.4 29.5 21.3 37.8 - -\nBMN [11] TS - - 56.0 47.4 38.8 29.7 20.5 38.5 >483 >171.0\nBC-GNN [64] TS - - 57.1 49.1 40.4 31.2 23.1 40.2 - -\nG-TAD [2] TS - - 54.5 47.6 40.2 30.8 23.4 39.3 >4440 >639.8\nBMNâ€  [11] I3D - - 56.4 47.9 39.2 30.2 21.2 39.0 - -\nG-TADâ€¡ [2] I3D - - 58.7 52.7 44.9 33.6 23.8 42.7 >3552 >368.9\nMR [70] I3D - - 53.9 50.7 45.4 38.0 28.5 43.3 >644 >36.8\nA2Net [36] I3D \u0013 - 58.6 54.1 45.5 32.5 17.2 41.6 1554 30.4\nP-GCN [13] I3D - - 63.6 57.8 49.1 - - - 7298 4.4\nP-GCNâ€¡ [13] I3D - - 64.9 59.0 49.4 36.7 22.6 46.5 7298 4.4\nG-TAD [2]+P-GCN [13] I3D - - 66.4 60.4 51.6 37.6 22.9 47.8 - -\nAGT [61] I3D \u0013 \u0013 65.0 58.1 50.2 - - - - -\nPCG-TAL [37] I3D - - 64.2 57.3 48.3 - - - - -\nRTD-Net [62] I3D - - 68.3 62.3 51.9 38.8 23.7 49.0 >211 >32.1\nAFSD [35] I3D - - 67.3 62.4 55.5 43.7 31.1 52.0 3245 84.1\nMUSES [27] I3D - - 68.9 64.0 56.9 46.3 31.0 53.4 2101 34.1\nTadTR* (Ours) I3D \u0013 \u0013 74.8 69.1 60.1 46.6 32.8 56.7 195 1.07\nTadTR-lite* (Ours) I3D \u0013 \u0013 71.3 65.9 57.0 44.6 30.4 53.8 155 0.85\nTadTRÂ§ (Ours) I3D \u0013 \u0013 70.3 64.3 55.7 44.0 30.0 52.9 195 1.07\nTadTR (Ours) I3D \u0013 \u0013 67.1 61.1 52.0 39.9 26.2 49.3 195 1.07\nand 8 Ã—faster than the competitive single-network detector\nA2Net. It also requires much fewer FLOPs. The efï¬ciency of\nour method is owing to the simple framework and the sparsity\nof predictions.\nThe above results indicate that our method is both accurate\nand efï¬cient. We also note that many other methods are\ncomposed of multiple independently trained networks. Those\nproposal generation methods (BSN, MGG, BMN, G-TAD,\nMR, BC-GNN, and RTD-Net) are not self-contained, as they\nrely on an extra classiï¬er (such as P-GCN) to accomplish\nthe TAD task. Differently, TadTR can achieve action detection\nwith only a single uniï¬ed network.\nWe note that the computation cost of TadTR is not com-\nparable with those methods that directly take video frames as\ninput, such as R-C3D. Most previous works use video features\nas input and focus on the design of detection networks.\nHACS Segments. We report the performance of TadTR,\nSSN [10], and the state-of-the-art method G-TAD [2] in\nTable II. Our method achieves an average mAP of 30.83%,\nwhich outperforms SSN (+11.86% mAP) and G-TAD (+3.35%\nmAP). Besides, our method requires 455 Ã— fewer GFLOPs\nthan G-TAD. As for run time, the network inference and post-\nprocessing step of G-TAD take 33 ms and 908 ms per video,\nrespectively. The total run time is 941 ms, 49.5 Ã— that of\nTadTR (19 ms). We also try the improved training setting,\nwhich results in 32.09% mAP. The results again illustrate the\nsuperiority of TadTR.\nActivityNet-1.3. Table III compares the performance of dif-\nferent methods on the validation set of ActivityNet-1.3. Some\nTABLE II: Comparison of different methods on the validation\nset of HACS Segments. The results of SSN are from [21]. *\nWith focal loss.\nMethod mAP0.5 mAP0.75 mAP0.95 mAP Time/ms GFLOPs\nSSN [69] 28.82 18.80 5.32 18.97 - -\nG-TAD [2] 41.08 27.59 8.34 27.48 941 45.7\nTadTR 45.16 30.70 11.78 30.83 19 0.1\nTadTR* 47.14 32.11 10.94 32.09 19 0.1\nmethods ( e.g., G-TAD [2]) only implement action proposal\ngeneration and cannot produce action detections without ex-\nternal action classiï¬ers. We divide the methods into two groups\naccording to whether external action classiï¬ers are used. Being\nsimple and end-to-end trainable, TadTR achieves an average\nmAP of 28.21%, which is stronger than all the other methods.\nWith the improved training setting, TadTR achieves 1.69%\nhigher mAP. This variant is 2.56% better than the second-best\nmethod PCG-TAL [37] in terms of mAP. Compared with the\nsecond-best single-network detector TAL-Net, we improve the\nperformance by 9.68%.\nFor comparison with previous methods [2], [11], [36] that\nare combined with an ensemble of classiï¬ers [69], we also try\nsuch a combination. To be concrete, we pass the detections\nby TadTR to the classiï¬ers and fuse the classiï¬cation scores\nof TadTR and the classiï¬ers by multiplication. When fused\nwith [10], TadTR enjoys a signiï¬cant performance boost,\nachieving an average mAP of 34.64%. It is better than the\nother compared methods in terms of average mAP, although\nsome methods use the stronger I3D features. When using the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 9\nTABLE III: Comparison of different methods on ActivityNet-1.3. Methods in the second group are combined with an ensemble\nof action classiï¬ers [69]. The computation costs (in FLOPs) of the action classiï¬ers are not included. The results of BMN and\nG-TAD with TSP [71] features are from [71]. TS: two-stream. SN: single-network. * With focal loss.\nMethod Feature SN E2E mAP0.5 mAP0.75 mAP0.95 mAP GFLOPs\nSelf-contained methods\nR-C3D [6] C3D \u0013 - 26.80 - - - -\nSSN [10] TS - - 39.12 23.48 5.49 23.98 -\nTAL-Net [7] I3D \u0013 - 38.23 18.30 1.30 20.22 -\nP-GCN [13] I3D - - 42.90 28.14 2.47 26.99 5.0\nPCG-TAL [37] I3D - - 42.14 28.34 6.12 27.34 -\nTadTR (Ours) TS \u0013 \u0013 41.40 28.85 7.86 28.21 0.038\nTadTR* (Ours) TS \u0013 \u0013 43.67 30.58 8.32 29.90 0.038\nCombined with an ensemble of action classiï¬ers [69]\nCDC [72] C3D - - 43.83 25.88 0.21 22.77 -\nBMN [11] TS - - 50.07 34.78 8.29 33.85 45.6\nG-TAD [2] TS - - 50.36 34.60 9.02 34.09 45.7\nP-GCN [13] I3D - - 48.26 33.16 3.27 31.11 5.0\nMR [70] I3D - - 43.47 33.91 9.21 30.12 -\nA2Net [36] I3D - - 43.55 28.69 3.70 27.75 1.2\nPCG-TAL [37] I3D - - 50.24 35.21 7.84 34.01 -\nRTD-Net [62] I3D - - 47.21 30.68 8.61 30.83 3.1\nAFSD [35] I3D - - 52.38 35.27 6.47 34.39 3.3\nTadTR* (Ours) TS - - 51.29 34.99 9.49 34.64 0.038\nTadTR+BMN (Ours) TS - - 50.51 35.35 8.18 34.55 45.6\nTadTR* (Ours) I3D - - 52.83 37.05 10.83 36.11 0.038\nBMN [11] TSP - - 51.23 36.78 9.50 35.67 45.6\nG-TAD [2] TSP - - 51.26 37.12 9.29 35.81 45.7\nTadTR* (Ours) TSP - - 53.62 37.52 10.56 36.75 0.038\nTABLE IV: Comparison of different variants of TadTR.\nMethod HACS Segments THUMOS14 ActivityNet\nmAP0.5 mAP0.75 mAP0.95 mAP MFLOPs mAP mAP\nTadTR 45.16 30.70 11.78 30.83 100.5 47.92 28.21\nTadTR w/o encoder 39.65 26.99 9.08 26.94 95.3 40.99 27.34\nTadTR w/o instance-level context 43.11 29.97 10.43 29.70 66.8 45.26 26.23\nTadTR with dense attention 22.76 12.52 4.19 13.58 564.5 24.15 23.79\nTadTR w/o actionness regression 42.10 28.44 10.23 28.51 99.4 45.09 26.13\nTadTR w/o segment reï¬nement 39.89 28.03 9.54 27.65 99.8 44.07 27.40\nI3D features, the average mAP is further boosted to 36.11%,\noutperforming the second-best method AFSD by 1.72%. Be-\nsides, the performance is achieved at a low computation cost,\nas indicated by the smaller FLOPs.\nTadTR can also be combined with BMN. This is imple-\nmented by connecting the encoder of TadTR to the detection\nhead of BMN. Owing to the adaptive context captured by the\nTransformer encoder, TadTR+BMN achieves an improvement\nof 0.7% over BMN, reaching 34.55% mAP. It also outperforms\nthe recent method G-TAD. This indicates the advantage of\nTransformer in temporal action detection. Note that 99.98% of\nthe computation cost is on the proposal classiï¬cation branch of\nBMN. Therefore the total computation cost of TadTR+BMN\nis close to that of BMN.\nWith the stronger TSP [71] features, the performance of\nTadTR reaches 36.75% mAP. It outperforms G-TAD by 0.94%\nand BMN by 1.08%. This again demonstrates the superiority\nof TadTR.\nC. Ablation Study\nIn this subsection, we validate the effectiveness of different\ncomponents of TadTR and evaluate the effects of various\nhyper-parameters. Unless specially noted, all reported results\nare with the default training and setting. TadTR-lite is used\nfor THUMOS14.\nThe importance of context information. The key of Trans-\nformer is the self-attention mechanism that incorporates the\ncontext in a video sequence. In TadTR, we leverage two\nkinds of context, snippet-level context from related snippets\nand instance-level context from related action queries, which\nare captured by Transformer encoder and the self-attention\nmodule in Transformer decoder respectively. By removing\nTransformer encoder, we get a variant â€œTadTR w/o encoderâ€.\nBy removing the self-attention module in the decoder, we get\na variant â€œTadTR w/o instance-level contextâ€. We report the\nperformance of the two variants in Table IV. It is observed\nthat removing the encoder leads to a 3.89% drop on HACS\nSegments, 6.93% drop on THUMOS14, and 0.87% drop on\nActivityNet in terms of average mAP. It indicates that the\nTransformer encoder is crucial for our model, as the decoder\nrequires long-range and adaptive context to reason the rela-\ntions between the actions and the video. Removing instance-\nlevel context, the average mAP drops by 1.13% on HACS\nSegments, 2.66% on THUMOS14, and 1.98% on ActivityNet.\nWe conclude that the context information from other action\ninstances is also helpful for action detection.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 10\nTABLE V: Comparison of the variants of TadTR with Trans-\nformer encoder and 1D CNN encoder on HACS Segments.\nEncoder Average mAP MFLOPsw/o NMS w/ NMS\nTransformer 30.83 30.53 100.5\n1D CNN 27.95 29.12 134.8\n/uni00000036/uni0000004b/uni00000052/uni00000055/uni00000057/uni00000030/uni00000048/uni00000047/uni0000004c/uni00000058/uni00000050/uni0000002f/uni00000052/uni00000051/uni0000004a\n/uni00000014/uni00000013\n/uni00000015/uni00000013\n/uni00000016/uni00000013\n/uni00000017/uni00000013\n/uni00000018/uni00000013\n/uni00000019/uni00000013/uni00000024/uni00000059/uni00000048/uni00000055/uni00000044/uni0000004a/uni00000048/uni00000003/uni00000050/uni00000024/uni00000033/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000014/uni0000001a/uni00000011/uni00000015/uni00000015/uni00000014/uni00000019/uni00000011/uni0000001a/uni0000001b\n/uni00000015/uni0000001b/uni00000011/uni0000001a/uni0000001a\n/uni00000016/uni00000014/uni00000011/uni00000019/uni00000016\n/uni00000017/uni0000001b/uni00000011/uni00000015/uni0000001c\n/uni00000018/uni00000017/uni00000011/uni00000016/uni0000001b/uni00000037/uni00000044/uni00000047/uni00000037/uni00000035/uni00000003/uni0000005a/uni00000012/uni00000003/uni00000014/uni00000027/uni00000003/uni00000026/uni00000031/uni00000031/uni00000003/uni00000048/uni00000051/uni00000046/uni00000052/uni00000047/uni00000048/uni00000055\n/uni00000037/uni00000044/uni00000047/uni00000037/uni00000035\nFig. 5: Comparison of the performance of TadTR with 1D\nCNN encoder and TadTR (with Transformer encoder) for\nactions with different durations on HACS Segments, measured\nby average mAP.\nTransformer encoder v.s. CNN encoder. We try replacing\nthe Transformer encoder with a 1D CNN encoder, which is\ncommon for temporal modeling in previous TAD methods. The\n1D CNN encoder is composed of two 1D convolutional layers\nwith 256 ï¬lters of kernel size 3 and ReLU activation. As can\nbe observed in Table V, using 1D CNN encoder leads to 2.88%\naverage mAP drop when NMS is not applied (the default\noption), and 1.41% average mAP drop when NMS is applied.\nInterestingly, the performance of this variant with NMS is\nimproved by 1.17% over that without NMS. It indicates that\nthere are many duplicate detections. One possible reason is\nthat CNN features are locally correlated. Therefore, it is hard\nto differentiate between close predictions as they have similar\nfeatures.\nTo further dissect the performance gap between TadTR with\n1D CNN encoder (equipped with NMS) and TadTR, we divide\nthe ground truth instances into 3 groups according to the\nnormalized duration: short ( 0 âˆ¼0.1), medium (0.1 âˆ¼0.2) and\nlong ( 0.2 âˆ¼1) and report the average mAP for each group\nin Fig. 5. Here we use the normalized duration because we\nresize the video features into a ï¬xed length. For reference, the\naverage duration per video is 148 seconds. As can be observed,\nTadTR with Transformer encoder achieves better performance\nfor medium-length and long actions. 1D CNN encoder is\nslightly better for short actions. The result is reasonable, as\n1D CNN is good at modeling short-term dependency but poor\nat modeling long-term dependency.\nWe also explore deeper CNNs and larger convolution ker-\nnels, but no improvement is observed. In terms of computation\ncost, this variant has much higher FLOPs than TadTR with\nTransformer encoder. The results show that 1D CNN is inferior\nto Transformer encoder.\nDense attention v.s. temporal deformable attention. We try\nreplacing all temporal deformable attention modules in TadTR\n1.0s 8.0s\n54.8s5.5s\nCurling: 0.50â†’0.47\nCurling: 0.39â†’0.60\n0.3s 52.0s\nGround Truth Detections Time\n11.0s\n76.6s\n19.6s 49.0s\nCleaning Windows: 0.24â†’0.42\n13.7s 71.5s\nCleaning Windows: 0.22â†’0.46\nCleaning Windows\nCurling\nFig. 6: Actionness regression improves the ranking of de-\ntections. In each of the above cases, the initial scores are\nunreliable. The more accurate detection obtains a lower score\nthan the less accurate one before rescoring. With the new\nscores (on the right of the arrows) applied, the ranking order\nturns satisfactory. Best viewed in color.\nTABLE VI: Ablation study of segment reï¬nement (SR) on\nActivityNet. The results are with focal loss.\nVariants 0.5 0.75 0.95 mAP\nStandard SR 43.67 30.58 8.32 29.90\nSR w/o incremental reï¬nement 42.72 29.00 6.92 28.59\nSR w/o reference point adjustment 42.61 28.62 6.28 28.05\nwith vanilla dense attention modules. This variant is called\nâ€œTadTR with dense attentionâ€. As depicted in Table IV, the\nperformance of this variant is far behind TadTR, especially on\nTHUMOS14 (-23.77% mAP), even if the model is trained\nfor 180 epochs. It means temporal deformable attention is\ncrucial for the success of TadTR. The main reason is that the\ndense attention lacks locality awareness. As different frames\nare usually similar in background, a dense attention module\ntends to over-smooth input sequence at initialization (see an\nexample in the supplementary material). As a result, it is\nhard to localize temporal segments with different semantics.\nBesides, the variant with dense attention has 5.7 Ã— higher\ncomputation cost than that of TadTR. Therefore, temporal\ndeformable attention is a better choice.\nThe effects of actionness regression and segment re-\nï¬nement. We study the effects of the two components by\nremoving them individually, resulting in 2 different model\nvariants. Their results are presented in Table IV. Comparing\nTadTR w/o actionness regression and TadTR, we observe that\nactionness regression leads to improvements of all metrics.\nSpeciï¬cally, the improvements are 3.06%, 2.26%, 1.55%,\nand 2.32% in terms of mAP at IoU 0.5, 0.75, 0.95 and\nthe average mAP on HACS Segments. On THUMOS14 and\nActivityNet, the average mAP improves by 2.83% and 2.08%.\nSeveral qualitative examples are presented in Fig. 6 to show\nhow actionness regression helps. It can produce more reliable\nconï¬dence scores for the action predictions.\nComparing TadTR w/o segment reï¬nement and TadTR, we\nï¬nd this component helpful for improving the localization\naccuracy. On HACS Segments, it improves the mAP at the\nstrict threshold of 0.95 by a large margin of 2.24%. The\nmAPs at other thresholds are also consistently improved. On\nTHUMOS14 and ActivityNet, the average mAP improves by\n3.85% and 0.81%.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 11\nTABLE VII: Effects of the numbers of encoder layers and\ndecoder layers on HACS Segments.\nLE LD mAP0.5 mAP0.75 mAP0.95 mAP MFLOPs\n2 4 45.16 30.70 11.78 30.83 100.5\n4 4 44.63 30.39 10.76 30.39 105.6\n6 4 40.55 27.55 9.88 27.63 110.8\n2 2 42.10 29.05 9.57 28.84 79.6\n2 4 45.16 30.70 11.78 30.83 100.5\n2 6 45.20 30.82 10.67 30.74 121.3\nTABLE VIII: Effect of the number of action queries on\ndifferent datasets. The average mAPs are reported.\n#queries Nq 10 20 30 40 50\nTHUMOS14 44.06 46.48 46.94 47.92 46.78\nHACS Segments 29.63 30.73 30.83 29.99 29.47\nActivityNet-1.3 28.21 26.47 26.27 26.29 22.96\nTo investigate how the segment reï¬nement mechanism im-\nproves the performance, we evaluate the effect of incremental\nreï¬nement and reference point adjustment. As can be observed\nin Table VI, disabling incremental reï¬nement leads to a\n1.31% decrease in mAP, which suggests that the incremental\nreï¬nement strategy is better than independent prediction at\neach decoder layer. Disabling reference point adjustment in\nstandard segment reï¬nement mechanism also leads to a 1.85%\ndecrease in mAP. This is reasonable, as the initial referent\npoints associated with the queries are invariant for different\ninput samples. The adjustment strategy makes them adaptive\nto the input and aligned with the local features of the target\nactions.\nBesides the detection performance, another important aspect\nis the computation cost. In terms of FLOPs, adding or remov-\ning the two components has little impact, which is shown\nin Table IV. In terms of run time, the average time cost\nper video on THUMOS14 is 130 ms for TadTR-lite without\nthe two components. Adding the actionness regression head\nwill increase the average time cost to 141 ms. With segment\nreï¬nement enabled, the average time cost per video becomes\n155 ms, which is still very efï¬cient compared with state-of-\nthe-art methods.\nThe effects of the number of action queries. Table VIII\ncompares the performance of TadTR using different number\nof action queries ( Nq). The best performance is achieved at\nNq = 40 on THUMOS14, Nq = 30 on HACS Segments and\nNq = 10 on ActivityNet-1.3. The results are reasonable, as the\naverage number of action instances per video on THUMOS14\n(15.4) is larger than that on ActivityNet-1.3 (1.5) and HACS\nSegments (2.8).\nThe effects of the numbers of encoder layers and decoder\nlayers. We evaluate TadTR with different numbers of encoder\nlayers ( LE) and decoder layers ( LD) and report results in\nTable VII. With LD ï¬xed, the best performance is achieved\nwhen LE is 2. Larger LE gives inferior results probably due to\nthe difï¬culty of training. Therefore, we set LE to 2. With LE\nï¬xed, the average mAP increases by 2.03% when LD increases\nfrom 2 to 4. Larger LD gives a slightly lower performance.\nTABLE IX: Effect of the number of sampling points K on\nthe performance and computation cost on HACS Segments.\nThe models are trained for half of the full training cycle (15\nepochs).\n#points K mAP0.5 mAP0.75 mAP0.95 mAP MFLOPs\n1 39.22 27.07 9.80 27.03 99.1\n2 40.22 27.62 10.15 27.61 99.6\n4 41.20 28.52 10.63 28.49 100.5\n8 39.77 27.15 9.86 27.20 102.4\nTABLE X: Effect of the number of attention heads M on\nthe performance and computation cost on HACS Segments.\nThe models are trained for half of the full training cycle (15\nepochs).\n#heads M mAP0.5 mAP0.75 mAP0.95 mAP MFLOPs\n1 38.62 25.25 7.94 25.57 100.2\n2 40.30 26.37 8.09 26.52 100.2\n4 40.74 28.41 10.51 28.30 100.3\n8 41.20 28.52 10.63 28.49 100.5\n16 41.07 28.35 10.23 28.31 100.9\nTherefore we suggest setting LD to 4.\nThe effect of the number of sampling points. Table IX\ncompares the performance and computation cost using differ-\nent numbers of sampling points K in TDA modules on HACS\nSegments. We observe that moderately increasing K improves\nthe performance, as more temporal details can be captured.\nThe best performance is achieved at K = 4. The number of\nsampling points has little impact on the computation cost.\nThe effect of the number of attention heads. Table X com-\npares the performance and computation cost using different\nnumber of attention heads M on HACS Segments. It is ob-\nserved that moderately increasing M boosts the performance,\nas more diverse features can be learned. The performance\nis saturated at M = 8 . Similar to the number of sampling\npoints, the number of attention heads has little impact on the\ncomputation cost.\nThe effects of the hyper-parameters in actionness re-\ngression. In Table XI, we compare different choices of the\nexpanding factor Ïµ and the number of bins TR for actionness\nregression. We see that the best performance is achieved when\nÏµ = 1.5. This result is 0.56% higher than that of the variant\nwithout RoI expansion ( Ïµ = 1), showing the effectiveness of\nRoI expansion. Among different choices of TR, TR = 16\nresults in the best performance.\nTABLE XI: Effect of the expanding factor Ïµ and the number\nof bins TR for actionness regression on HACS Segments.\nÏµ 1 1.25 1.5 2\nmAP 30.27 30.40 30.83 30.01\nTR 8 16 32 -\nmAP 30.39 30.83 30.26 -\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 12\nEncoder reference point Encoder sampling points\n504 506 508 510 512\nt/sec\nGT: HighJump Pred: HighJump Decoder reference point Decoder sampling points\nlow\nhigh\nFig. 7: Visualization of temporal deformable attention. The ï¬rst row is uniformly sampled video frames. The second row\nvisualizes the attention at two randomly picked reference points in the last encoder layer. The third row visualizes the attention\nfor the predicted action in the last decoder layer. We use different markers to represent sampling points in different heads and\nseparate the points from different heads vertically. The color of a point indicates the attention weight. Best viewed in color.\nMore examples are given in the supplementary material.\n260 265 270 275 280 285 290\nt/sec\nJavelinThrow Non-action\nCricketBowling CricketShot\n0 5 10 15 20 25 30\nt/sec\n#28\n#7#4 #36#24\n#37\n#20 #38\n#4\n#12\nFig. 8: Visualization of self-attention between action queries\nin the last decoder layer. We average the attention over all\nheads. The queries are represented by the predicted instances.\nThe arrows indicate the attention that the topmost query casts\non the other four queries with the largest weights. The attention\nweight is encoded by the color. The darker the arrow, the larger\nthe attention weight. In the ï¬rst example, the four attended\ninstances are semantically related to the topmost instance.\nIn the second example, the topmost instance casts the most\nattention to a nearby instance (#37) with the same class. The\nother three non-action instances are also attended, probability\nfor context from the background. Best viewed in color.\nD. Analysis\nVisualization of attention. Fig. 12 visualizes temporal de-\nformable attention of the last encoder layer and the last\ndecoder layer. We observe that: (1) Different attention heads\nfocus on different temporal regions and scales. For example,\nthe sampling points marked with left-triangle and up-triangle\nare distributed on the left side of the reference point. In some\nheads, the sampling points that are farthest away from the\nreference point have relatively higher attention weights, to\ncapture useful cues for action boundaries. (2) The encoder\nand the decoder have different preferences for context. The\nCenter Æ¸ğ‘¡\nLength áˆ˜ğ‘‘\nFig. 9: Visualization of all action predictions on all videos\nfrom HACS Segments validation set from 30 action query\nslots. In each 1-by-1 square, we use scattered points to\nrepresent all predictions from this query. The horizontal and\nthe vertical coordinates are the coordinate of the center and\nthe normalized length of these predictions, respectively. We\nobserve that each query is responsible for action predictions\nin certain locations and lengths.\nsampling points in the decoder almost cover the full extent\nof an action prediction, providing a large receptive ï¬eld. Dif-\nferently, the sampling points in the encoder have a relatively\nshort temporal extent, capturing a moderate amount of context.\nFig. 8 visualizes self-attention between action queries in the\nlast decoder layer. We ï¬nd that the topmost query in each\nexample cast the most attention on the queries whose predicted\ninstances are semantically related to that of it. It suggests that\nthe self-attention layer in the decoder can model the relations\nbetween action queries (or instances).\nVisualization of action queries. Fig. 9 illustrates the dis-\ntribution of locations and scales (lengths) of output actions\nassociated with each action query. We observe that each query\nproduces action predictions in certain locations and scales.\nDifferent locations and scales are covered by a small number\nof queries. It means that the detector learns the distribution of\nactions in the training dataset. This is more ï¬‚exible than the\nhand-crafted anchor design in previous methods.\nLimitations. Although TadTR achieves strong overall per-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 13\n10.8s 12.5s\n10.9s 11.6s\n101.6s 103.1sGT\n101.4s 102.6sMUSES\nTadTR (missed)\nGT\nMUSES\nTadTR (missed)\n(a) BasketballDunk\n(b) VolleyballSpiking\nFig. 10: Failure cases. TadTR misses the two short actions that are hard to detect while the 1D CNN-based method MUSES\npartially detects them.\nformance, it may fail on some short actions, as depicted in\nFig. 10. The quantitative results in Fig. 5 also illustrate the\nlower performance of TadTR on short actions. One possible\nreason is that Transformer is inferior to 1D CNN in modeling\nshort-term dependency. Combing Transformer and 1D CNN\nmight improve the performance on short actions. Another\nlimitation is that TadTR will miss actions when the number of\ntrue actions in a video is larger than the number of queries Nq,\nalthough such cases might be rare. This is a common issue of\nDETR-alike detectors. How to maintain the performance while\nincreasing Nq is worth studying in future works.\nV. C ONCLUSION\nWe propose TadTR, a simple end-to-end method for tem-\nporal action detection (TAD) based on Transformer. It views\nthe TAD task as a direct set prediction problem and maps a\nseries of learnable embeddings to action instances in parallel\nby adaptively extracting temporal context in the video. It\nsimpliï¬es the pipeline of TAD and removes hand-crafted\ncomponents such as anchor setting and post-processing. We\nmake three improvements to enhance the Transformer with\nlocality awareness to better adapt to the TAD task. Extensive\nexperiments validate the remarkable performance and efï¬-\nciency of TadTR and the effectiveness of different components.\nTadTR achieves state-of-the-art or competitive performance\non HACS Segments, THUMOS14, and ActivityNet-1.3 with\nlower computation costs. We hope that this work could trigger\nthe development of Transformers and efï¬cient models for\ntemporal action detection. The current implementation of\nTadTR is based on ofï¬‚ine extracted CNN features for a fair\ncomparison with previous methods. In the future, we plan to\nexplore joint learning of the video encoder and TadTR [73],\nand temporal action detectors purely based on Transformers.\nAPPENDIX\nIn this supplement, we present several visualization results.\nFig. 11 illustrates the smoothing effect of dense attention.\nFig. 12 supplements Fig. 7 in the main document and gives\nmore examples to demonstrate temporal deformable attention.\nREFERENCES\n[1] Z. Shou, D. Wang, and S.-F. Chang, â€œTemporal action localization in\nuntrimmed videos via multi-stage cnns,â€ inCVPR, 2016, pp. 1049â€“1058.\n[2] M. Xu, C. Zhao, D. S. Rojas, A. Thabet, and B. Ghanem, â€œG-TAD:\nSub-graph localization for temporal action detection,â€ in CVPR, 2020,\npp. 10 156â€“10 165.\n[3] S. Ma, L. Sigal, and S. Sclaroff, â€œLearning activity progression in lstms\nfor activity detection and early detection,â€ in CVPR, June 2016, pp.\n1942â€“1950.\n[4] A. Richard and J. Gall, â€œTemporal action detection using a statistical\nlanguage model,â€ in CVPR, 2016, pp. 3131â€“3140.\n[5] F. Caba Heilbron, J. Carlos Niebles, and B. Ghanem, â€œFast temporal\nactivity proposals for efï¬cient detection of human actions in untrimmed\nvideos,â€ in CVPR, 2016, pp. 1914â€“1923.\n[6] H. Xu, A. Das, and K. Saenko, â€œR-c3d: region convolutional 3d network\nfor temporal activity detection,â€ in ICCV, 2017, pp. 5794â€“5803.\n[7] Y .-W. Chao, S. Vijayanarasimhan, B. Seybold, D. A. Ross, J. Deng, and\nR. Sukthankar, â€œRethinking the faster r-cnn architecture for temporal\naction localization,â€ in CVPR, 2018, pp. 1130â€“1139.\n[8] T. Lin, X. Zhao, and Z. Shou, â€œSingle shot temporal action detection,â€\nin ACM MM, 2017, pp. 988â€“996.\n[9] Z.-H. Yuan, J. C. Stroud, T. Lu, and J. Deng, â€œTemporal action\nlocalization by structured maximal sums,â€ in CVPR, 2017, pp. 3684â€“\n3692.\n[10] Y . Zhao, Y . Xiong, L. Wang, Z. Wu, X. Tang, and D. Lin, â€œTemporal\naction detection with structured segment networks,â€ ICCV, pp. 2914â€“\n2923, 2017.\n[11] T. Lin, X. Liu, X. Li, E. Ding, and S. Wen, â€œBmn: Boundary-matching\nnetwork for temporal action proposal generation,â€ in ICCV, 2019, pp.\n3889â€“3898.\n[12] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager, â€œTemporal\nconvolutional networks for action segmentation and detection,â€ inCVPR,\n2017, pp. 156â€“165.\n[13] R. Zeng, W. Huang, M. Tan, Y . Rong, P. Zhao, J. Huang, and C. Gan,\nâ€œGraph convolutional networks for temporal action localization,â€ in\nICCV, 2019, pp. 7094â€“7103.\n[14] J. Carreira and A. Zisserman, â€œQuo vadis, action recognition? a new\nmodel and the kinetics dataset,â€ in CVPR, 2017, pp. 4724â€“4733.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 14\n0 20 40 60 80 100 120\nT emporal dimension\n0\n20\n40\n60\n80\n100\n120\nT emporal dimension\nSimilarity matrix of input feature\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 20 40 60 80 100 120\nSource locations\n0\n20\n40\n60\n80\n100\n120\nOutput locations\nAttention weights\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 20 40 60 80 100 120\nT emporal dimension\n0\n20\n40\n60\n80\n100\n120\nT emporal dimension\nSimilarity matrix of dense attention encoded feature\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFig. 11: Different frames in a video is usually highly similar. The dense attention tends to cast uniform attention to different\nlocations in the input sequence at initialization. Left: The similarity matrix of each pair of snippets in CNN features of a\nrandomly selected video. Middle: The attention weight. Right: The similarity matrix of the output feature of the dense-attention.\nBest viewed in color.\nEncoder reference point Encoder sampling points\n0 25 50 75 100 125 150 175 200\nt/sec\nGT: Futsal Pred: Futsal Decoder reference point Decoder sampling points\nlow\nhigh\n(a) Futsal\nEncoder reference point Encoder sampling points\n163 164 165 166 167 168 169 170\nt/sec\nGT: Diving Pred: Diving Decoder reference point Decoder sampling points\nlow\nhigh\n(b) Diving\nFig. 12: Visualization of temporal deformable attention. The ï¬rst row is uniformly sampled video frames. The second row\nvisualizes the attention at two randomly picked reference points in the last encoder layer. The third row visualizes the attention\nfor the predicted action in the last decoder layer. We use different markers to represent sampling points in different heads and\nseparate points from different heads vertically. The color of a point indicates the attention weight. Best viewed in color.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 15\n[15] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, â€œEnd-to-end object detection with transformers,â€ in\nECCV, 2020, pp. 213â€“229.\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, â€œAttention is all you need,â€ in NIPS, 2017,\npp. 5998â€“6008.\n[17] X. Dai, B. Singh, G. Zhang, L. S. Davis, and Y . Q. Chen, â€œTemporal\ncontext network for activity localization in videos,â€ in ICCV, 2017, pp.\n5727â€“5736.\n[18] H. Alwassel, F. Caba Heilbron, V . Escorcia, and B. Ghanem, â€œDiagnos-\ning error in temporal action detectors,â€ in ECCV, 2018, pp. 256â€“272.\n[19] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, â€œDeformable\ndetr: Deformable transformers for end-to-end object detection,â€ in ICLR,\n2021.\n[20] K. He, G. Gkioxari, P. DollÃ¡r, and R. Girshick, â€œMask r-cnn,â€ in ICCV,\n2017, pp. 2961â€“2969.\n[21] H. Zhao, A. Torralba, L. Torresani, and Z. Yan, â€œHACS: human action\nclips and segments dataset for recognition and temporal localization,â€\nin ICCV, 2019, pp. 8667â€“8677.\n[22] H. Idrees, A. R. Zamir, Y .-G. Jiang, A. Gorban, I. Laptev, R. Sukthankar,\nand M. Shah, â€œThe THUMOS challenge on action recognition for videos\nâ€œin the wildâ€,â€ pp. 1â€“23, 2017.\n[23] F. Caba Heilbron, V . Escorcia, B. Ghanem, and J. Carlos Niebles,\nâ€œActivityNet: A large-scale video benchmark for human activity under-\nstanding,â€ in CVPR, 2015, pp. 961â€“970.\n[24] J. Yuan, B. Ni, X. Yang, and A. A. Kassim, â€œTemporal action localization\nwith pyramid of score distribution features,â€ in CVPR, 2016, pp. 3093â€“\n3102.\n[25] F. C. Heilbron, W. Barrios, V . Escorcia, and B. Ghanem, â€œScc: Semantic\ncontext cascade for efï¬cient action detection.â€ inCVPR, 2017, pp. 3175â€“\n3184.\n[26] X. Liu, Y . Sun, J. Lu, C. Yao, and Y . Zhou, â€œSelf-similarity action\nproposal,â€ IEEE Signal Processing Letters, vol. 27, pp. 2064â€“2068,\n2020.\n[27] X. Liu, Y . Hu, S. Bai, F. Ding, X. Bai, and P. H. S. Torr, â€œMulti-shot\ntemporal event localization: A benchmark,â€ in CVPR, June 2021, pp.\n12 596â€“12 606.\n[28] J. Gao, Z. Yang, C. Sun, K. Chen, and R. Nevatia, â€œTurn tap: Temporal\nunit regression network for temporal action proposals,â€ in ICCV, 2017,\npp. 3648â€“3656.\n[29] T. Lin, X. Zhao, H. Su, C. Wang, and M. Yang, â€œBsn: Boundary sensitive\nnetwork for temporal action proposal generation,â€ in ECCV, September\n2018, pp. 3â€“21.\n[30] J. Gao, K. Chen, and R. Nevatia, â€œCtap: Complementary temporal action\nproposal generation,â€ in ECCV, September 2018, pp. 70â€“85.\n[31] Y . Liu, L. Ma, Y . Zhang, W. Liu, and S.-F. Chang, â€œMulti-granularity\ngenerator for temporal action proposal,â€ in CVPR, 2019, pp. 3604â€“3613.\n[32] V . Escorcia, F. C. Heilbron, J. C. Niebles, and B. Ghanem, â€œDaps: Deep\naction proposals for action understanding,â€ inECCV, 2016, pp. 768â€“784.\n[33] S. Buch, V . Escorcia, C. Shen, B. Ghanem, and J. C. Niebles, â€œSst:\nSingle-stream temporal action proposals,â€ in CVPR, 2017, pp. 6373â€“\n6382.\n[34] F. Long, T. Yao, Z. Qiu, X. Tian, J. Luo, and T. Mei, â€œGaussian temporal\nawareness networks for action localization,â€ in CVPR, 2019, pp. 344â€“\n353.\n[35] C. Lin, C. Xu, D. Luo, Y . Wang, Y . Tai, C. Wang, J. Li, F. Huang,\nand Y . Fu, â€œLearning salient boundary feature for anchor-free temporal\naction localization,â€ in CVPR, 2021, pp. 3320â€“3329.\n[36] L. Yang, H. Peng, D. Zhang, J. Fu, and J. Han, â€œRevisiting anchor\nmechanisms for temporal action localization,â€ IEEE Transactions on\nImage Processing, vol. 29, pp. 8535â€“8548, 2020.\n[37] R. Su, D. Xu, L. Sheng, and W. Ouyang, â€œPcg-tal: Progressive cross-\ngranularity cooperation for temporal action localization,â€ IEEE Trans-\nactions on Image Processing, vol. 30, pp. 2103â€“2113, 2021.\n[38] S. Yeung, O. Russakovsky, G. Mori, and L. Fei-Fei, â€œEnd-to-end learning\nof action detection from frame glimpses in videos,â€ in CVPR, 2016, pp.\n2678â€“2687.\n[39] F. Ma, L. Zhu, Y . Yang, S. Zha, G. Kundu, M. Feiszli, and Z. Shou,\nâ€œSf-net: Single-frame supervision for temporal action localization,â€ in\nECCV. Springer, 2020, pp. 420â€“437.\n[40] P. Nguyen, T. Liu, G. Prasad, and B. Han, â€œWeakly supervised action\nlocalization by sparse temporal pooling network,â€ in CVPR, 2018, pp.\n6752â€“6761.\n[41] S. Paul, S. Roy, and A. K. Roy-Chowdhury, â€œW-talc: Weakly-supervised\ntemporal activity localization and classiï¬cation,â€ in ECCV, September\n2018, pp. 588â€“607.\n[42] D. Liu, T. Jiang, and Y . Wang, â€œCompleteness modeling and context\nseparation for weakly supervised temporal action localization,â€ inCVPR,\n2019, pp. 1298â€“1307.\n[43] Z. Shou, H. Gao, L. Zhang, K. Miyazawa, and S.-F. Chang, â€œAutoloc:\nWeakly-supervised temporal action localization in untrimmed videos,â€\nin ECCV, 2018, pp. 154â€“171.\n[44] T. Yu, Z. Ren, Y . Li, E. Yan, N. Xu, and J. Yuan, â€œTemporal structure\nmining for weakly supervised action detection,â€ in ICCV, 2019, pp.\n5522â€“5531.\n[45] L. Huang, Y . Huang, W. Ouyang, and L. Wang, â€œModeling sub-actions\nfor weakly supervised temporal action localization,â€ IEEE Transactions\non Image Processing, vol. 30, pp. 5154â€“5167, 2021.\n[46] W. Yang, T. Zhang, Z. Mao, Y . Zhang, Q. Tian, and F. Wu, â€œMulti-\nscale structure-aware network for weakly supervised temporal action\ndetection,â€ IEEE Transactions on Image Processing, vol. 30, pp. 5848â€“\n5861, 2021.\n[47] R. Zeng, C. Gan, P. Chen, W. Huang, Q. Wu, and M. Tan, â€œBreaking\nwinner-takes-all: Iterative-winners-out networks for weakly supervised\ntemporal action localization,â€ IEEE Transactions on Image Processing,\nvol. 28, no. 12, pp. 5797â€“5808, 2019.\n[48] L. Huang, Y . Huang, W. Ouyang, and L. Wang, â€œRelational prototypical\nnetwork for weakly supervised temporal action localization,â€ in AAAI,\nvol. 34, no. 07, 2020, pp. 11 053â€“11 060.\n[49] A. Islam, C. Long, and R. Radke, â€œA hybrid attention mechanism for\nweakly-supervised temporal action localization,â€ in AAAI, vol. 35, no. 2,\n2021, pp. 1637â€“1645.\n[50] Y . Zhai, L. Wang, W. Tang, Q. Zhang, J. Yuan, and G. Hua, â€œTwo-stream\nconsensus network for weakly-supervised temporal action localization,â€\nin ECCV. Springer, 2020, pp. 37â€“54.\n[51] L. Huang, L. Wang, and H. Li, â€œForeground-action consistency network\nfor weakly supervised temporal action localization,â€ in ICCV, 2021, pp.\n8002â€“8011.\n[52] B. Tan, N. Xue, S. Bai, T. Wu, and G.-S. Xia, â€œPlanetr: Structure-guided\ntransformers for 3d plane recovery,â€ in CVPR, 2021, pp. 4186â€“4195.\n[53] J.-N. Chen, S. Sun, J. He, P. H. Torr, A. Yuille, and S. Bai, â€œTransmix:\nAttend to mix for vision transformers,â€ in CVPR, 2022, pp. 12 135â€“\n12 144.\n[54] D. Liang, X. Chen, W. Xu, Y . Zhou, and X. Bai, â€œTranscrowd:\nweakly-supervised crowd counting with transformers,â€ Science China\nInformation Sciences, vol. 65, no. 6, pp. 1â€“14, 2022.\n[55] C. Sun, A. Myers, C. V ondrick, K. Murphy, and C. Schmid, â€œVideobert:\nA joint model for video and language representation learning,â€ in ICCV,\n2019, pp. 7464â€“7473.\n[56] G. Bertasius, H. Wang, and L. Torresani, â€œIs space-time attention all\nyou need for video understanding?â€ in ICML, July 2021, pp. 813â€“824.\n[57] J. Wu, Y . Jiang, W. Zhang, X. Bai, and S. Bai, â€œSeqformer: a frus-\ntratingly simple model for video instance segmentation,â€ arXiv preprint\narXiv:2112.08275, 2021.\n[58] L. Zhu and Y . Yang, â€œActbert: Learning global-local video-text repre-\nsentations,â€ in CVPR, 2020, pp. 8746â€“8755.\n[59] L. Zhou, Y . Zhou, J. J. Corso, R. Socher, and C. Xiong, â€œEnd-to-end\ndense video captioning with masked transformer,â€ in CVPR, 2018, pp.\n8739â€“8748.\n[60] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, â€œVideo action\ntransformer network,â€ in CVPR, 2019, pp. 244â€“253.\n[61] M. Nawhal and G. Mori, â€œActivity graph transformer for temporal action\nlocalization,â€ arXiv preprint arXiv:2101.08540, 2021.\n[62] J. Tan, J. Tang, L. Wang, and G. Wu, â€œRelaxed transformer decoders for\ndirect action proposal generation,â€ in ICCV, October 2021, pp. 13 526â€“\n13 535.\n[63] L. Wang, H. Yang, W. Wu, H. Yao, and H. Huang, â€œTemporal action pro-\nposal generation with transformers,â€ arXiv preprint arXiv:2105.12043,\n2021.\n[64] Y . Bai, Y . Wang, Y . Tong, Y . Yang, Q. Liu, and J. Liu, â€œBoundary\ncontent graph neural network for temporal action proposal generation,â€\nin ECCV, 2020, pp. 121â€“137.\n[65] J. L. Ba, J. R. Kiros, and G. E. Hinton, â€œLayer normalization,â€ arXiv\npreprint arXiv:1607.06450, 2016.\n[66] T.-Y . Lin, P. Goyal, R. Girshick, K. He, and P. DollÃ¡r, â€œFocal loss for\ndense object detection,â€ in ICCV, 2017, pp. 2980â€“2988.\n[67] L. Wang, Y . Xiong, Z. Wang, Y . Qiao, D. Lin, X. Tang, and L. Van Gool,\nâ€œTemporal segment networks: Towards good practices for deep action\nrecognition,â€ in ECCV, 2016, pp. 20â€“36.\n[68] I. Loshchilov and F. Hutter, â€œDecoupled weight decay regularization,â€\nin ICLR, 2017, pp. 1â€“18.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021 16\n[69] Y . Zhao, B. Zhang, Z. Wu, S. Yang, L. Zhou, S. Yan, L. Wang,\nY . Xiong, W. Yali, D. Lin, Y . Qiao, and X. Tang, â€œCUHK & ETHZ\n& SIAT submission to ActivityNet challenge 2017,â€ arXiv preprint\narXiv:1710.08011, pp. 20â€“24, 2017.\n[70] P. Zhao, L. Xie, C. Ju, Y . Zhang, Y . Wang, and Q. Tian, â€œBottom-up\ntemporal action localization with mutual regularization,â€ inECCV, 2020.\n[71] H. Alwassel, S. Giancola, and B. Ghanem, â€œTsp: Temporally-sensitive\npretraining of video encoders for localization tasks,â€ inICCV Workshops,\n2021, pp. 3166â€“3176.\n[72] Z. Shou, J. Chan, A. Zareian, K. Miyazawa, and S.-F. Chang, â€œCdc:\nConvolutional-de-convolutional networks for precise temporal action\nlocalization in untrimmed videos,â€ in ICCV, 2017, pp. 1417â€“1426.\n[73] X. Liu, S. Bai, and X. Bai, â€œAn empirical study of end-to-end temporal\naction detection,â€ in CVPR, June 2022, pp. 20 010â€“20 019.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.826916515827179
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6086388230323792
    },
    {
      "name": "Transformer",
      "score": 0.5662881135940552
    },
    {
      "name": "Locality",
      "score": 0.48929545283317566
    },
    {
      "name": "Action recognition",
      "score": 0.4725192189216614
    },
    {
      "name": "Classifier (UML)",
      "score": 0.4579460024833679
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4322115182876587
    },
    {
      "name": "Machine learning",
      "score": 0.35275423526763916
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Class (philosophy)",
      "score": 0.0
    }
  ]
}