{
  "title": "FPM: A Collection of Large-scale Foundation Pre-trained Language Models",
  "url": "https://openalex.org/W3212188742",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5032930326",
      "name": "Dezhou Shen",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2799054028",
    "https://openalex.org/W3015253856",
    "https://openalex.org/W3169113923",
    "https://openalex.org/W3107315802",
    "https://openalex.org/W4293454876",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3134582802",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W4287319715",
    "https://openalex.org/W3158631574",
    "https://openalex.org/W2963963856",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3176617251",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3187018546"
  ],
  "abstract": "Abstract Recent work in language modeling has shown that train- ing large-scale Transformer models has promoted the lat- est developments in natural language processing applica- tions. However, there is very little work to unify the cur- rent effective models. In this work, we use the current ef- fective model structure to launch a model set through the current most mainstream technology. We think this will become the basic model in the future. For Chinese, us- ing the GPT-2[9] model, a 10.3 billion parameter language model was trained on the Chinese dataset, and, in particu- lar, a 2.9 billion parameter language model based on dia- logue data was trained; the BERT model was trained on the Chinese dataset with 495 million parameters; the Trans- former model has trained a language model with 5.6 bil- lion parameters on the Chinese dataset. In English, cor- responding training work has also been done. Using the GPT-2 model, a language model with 6.4 billion param- eters was trained on the English dataset; the BERT[3] model trained a language model with 1.24 billion param- eters on the English dataset, and in particular, it trained a 688 million parameter based on single card training tech- nology Language model; Transformer model trained a lan- guage model with 5.6 billion parameters on the English dataset. In the TNEWS classification task evaluated by CLUE[13], the BERT-C model exceeded the 59.46% accu- racy of ALBERT-xxlarge with an accuracy rate of 59.99%, an increase of 0.53%. In the QQP classification task evalu- ated by GLUE[11], the accuracy rate of 78.95% surpassed the accuracy rate of BERT-Large of 72.1%, an increase of 6.85%. Compared with the current accuracy rate of ERNIE, the first place in the GLUE evaluation of 75.2%, an increase of 3.75%.",
  "full_text": "FPM: A Collection of Large-scale Foundation Pre-\ntrained Language Models\nDezhou Shen  (  shendezhou@rct.ai )\nrct ai https://orcid.org/0000-0001-5514-507X\nArticle\nKeywords: language modelling, large scale modelling, language processing\nPosted Date: November 8th, 2021\nDOI: https://doi.org/10.21203/rs.3.rs-1061146/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nFPM: A Collection of Large-scale Foundation Pre-trained Language Models\nDezhou Shen\nDepartment of Computer Science\nrct ai\nBeijing, CN 101300\nshendezhou@rct.ai\nAbstract\nRecent work in language modeling has shown that train-\ning large-scale Transformer models has promoted the lat-\nest developments in natural language processing applica-\ntions. However , there is very little work to unify the cur-\nrent effective models. In this work, we use the current ef-\nfective model structure to launch a model set through the\ncurrent most mainstream technology. W e think this will\nbecome the basic model in the future. F or Chinese, us-\ning the GPT -2[\n9] model, a 10.3 billion parameter language\nmodel was trained on the Chinese dataset, and, in particu-\nlar , a 2.9 billion parameter language model based on dia-\nlogue data was trained; the BERT model was trained on the\nChinese dataset with 495 million parameters; the Trans-\nformer model has trained a language model with 5.6 bil-\nlion parameters on the Chinese dataset. In English, cor-\nresponding training work has also been done. Using the\nGPT -2 model, a language model with 6.4 billion param-\neters was trained on the English dataset; the BERT[\n3]\nmodel trained a language model with 1.24 billion param-\neters on the English dataset, and in particular , it trained a\n688 million parameter based on single card training tech-\nnology Language model; Transformer model trained a lan-\nguage model with 5.6 billion parameters on the English\ndataset. In the TNEWS classiﬁcation task evaluated by\nCLUE[\n13], the BERT -C model exceeded the 59.46% accu-\nracy of ALBERT -xxlarge with an accuracy rate of 59.99%,\nan increase of 0.53%. In the QQP classiﬁcation task evalu-\nated by GLUE[\n11], the accuracy rate of 78.95% surpassed\nthe accuracy rate of BERT -Large of 72.1%, an increase of\n6.85%. Compared with the current accuracy rate of ERNIE,\nthe ﬁrst place in the GLUE evaluation of 75.2%, an increase\nof 3.75%.\n1. Introduction\nNatural language processing is developing rapidly , partly\nbecause of the increase in available calculations and the size\nof datasets. Abundant calculations and data make it possible\nto train larger and larger language models through unsuper-\nvised pre-training. As these models become larger, they ex-\nceed the memory limits of modern processors. Fortunately ,\nit is possible to train a language model with hundreds of mil-\nlions or even tens of billions of parameters on the NVIDIA\nA100 40GB GPU. By partitioning the model so that the\nweights and their associated optimizer states do not need\nto reside on the processor at the same time, several model\nparallelism methods overcome this limitation. By deploy-\ning multiple GPUs on one machine, we can train larger and\nmore effective models. And now there are many methods\nto overcome model degradation, such as the technology of\novercoming model degradation caused by model size scal-\ning by rearranging the layer normalization and residual con-\nnection in the Transformer layer.\nIn summary , the contributions of this article are as fol-\nlows:\nFor Chinese, using the GPT -2 model, a 10.3 billion pa-\nrameter language model was trained on the Chinese dataset,\nwhich is the largest known Chinese generative model. And,\nin particular, trained a 2.9 billion parameter language model\nbased on dialogue data. The BER T model has trained\na 495 million parameter language model on the Chinese\ndataset and is the largest known Chinese coding model. The\nTransformer model trained a 5.6 billion parameter language\nmodel on the Chinese dataset.\nIn English, using the GPT -2 model, a language model\nwith 6.4 billion parameters was trained on the English\ndataset; the BER T model trained a language model with\n1.24 billion parameters on the English dataset. In particular,\na language model with 688 million parameters trained based\non single-card training technology is the largest known En-\nglish coding model. The Transformer model trained a 2.9\nbillion parameter language model on the English dataset.\nIn the TNEWS classiﬁcation task evaluated by CLUE,\nthe BER T -C model exceeded the 59.46% accuracy of\nALBER T -xxlarge[\n6] with an accuracy rate of 59.99%, an\nincrease of 0.53%.\n1\nIn the QQP classiﬁcation task evaluated by GLUE, the\naccuracy rate of 78.95% surpassed the accuracy rate of\nBER T -Large of 72.1%, an increase of 6.85%. Compared\nwith the current accuracy rate of ERNIE, the ﬁrst place in\nthe GLUE evaluation of 75.2%, an increase of 3.75%.\n2. Related W ork\n2.1. Pre­trained Models Review\nIn terms of generative models, Tsinghua trained a 2.6\nbillion parameter CPM[\n17] model; Eleuther trained a 2.7\nbillion parameter GPT -Neo and a 6 billion parameter GPT -\nJ[\n12] model; OpenAI trained a 175 billion parameter GPT -\n3[1] model.\nIn terms of coding models, iFL YTEK has trained a 330\nmillion parameter Roberta[ 2] model, Alibaba trained a 27\nbillion parameter PLUG[ 7] model, Huawei trained a 110\nbillion parameter Pangu[ 15] model, and Saleforce trained a\n1.6 billion parameter CTRL[ 5] model.\nIn terms of coding and decoding models, Tsinghua has\ntrained a CPM2[ 16] model with 11 billion parameters.\n3. GPT\nIn the experiment of this article, a multi-layer trans-\nformer decoder is used as the language model, which is a\nvariant of the transformer. The model applies a multi-head\nself-attention operation to the input context label, and then\na position feedforward layer to generate the output distribu-\ntion on the target label. The decoder also consists of a stack\nof N=6 identical layers. In addition to the two sublayers in\neach encoder layer, the decoder also inserts a third sublayer\nthat performs multi-head attention on the output of the en-\ncoder stack. Similar to the encoder, residual connections are\nused around each sub-layer, and then layer normalization is\nperformed. The self-attention sublayer in the decoder stack\nis also modiﬁed to prevent the position from paying atten-\ntion to subsequent positions. This masking, combined with\nthe fact that the output embedding is offset by one position,\nensures that the prediction of the position only depends on\nthe known output at positions less than one.\n3.1. Applications of Attention\nTransformer uses multi-head attention in three different\nways:\n1. In the ”encoder-decoder attention” layer, the query\ncomes from the previous decoder layer, and the mem-\nory keys and values come from the encoder’s output.\nThis allows every position in the decoder to partici-\npate in all positions in the input sequence. This mim-\nics the typical encoder-decoder attention mechanism in\nthe sequence-to-sequence model.\n2. The encoder includes a self-attention layer. In the self-\nattention layer, all keys, values, and queries come from\nthe same place, in this case, the output of the previous\nlayer in the encoder. Every position in the encoder can\nfocus on all positions in the previous layer of the en-\ncoder.\n3. Similarly , the self-attention layer in the decoder allows\neach position in the decoder to focus on all positions\nin the decoder up to and including that position. It is\nnecessary to prevent leftward information ﬂow in the\ndecoder to preserve the autoregressive characteristics.\nThis is achieved by masking all values corresponding\nto illegal connections in the softmax input.\n4. BERT\nBER T is a pre-trained Transformer network that sets up\nthe latest results for various NLP tasks, including question\nanswering, sentence classiﬁcation, and sentence pair regres-\nsion. The BER T input used for sentence pair regression\nconsists of two sentences separated by a special [SEP] to-\nken. Apply more than 12 layers (base model) or 24 layers\n(large model) of multi-head attention, and pass the output\nto a simple regression function to derive the ﬁnal label.\nRoBER T a[\n8] shows that the performance of BER T can\nbe further improved through small adjustments to the pre-\ntraining process. BER T and its detailed implementation will\nbe introduced in this section. The framework has two steps:\npre-training and ﬁne-tuning. During pre-training, the model\nis trained on unlabeled data on different pre-training tasks.\nFor ﬁne-tuning, the BER T model is ﬁrst initialized with pre-\ntrained parameters, and then all parameters are ﬁne-tuned\nusing labeled data from downstream tasks. Each down-\nstream task has a separate ﬁne-tuning model, even if they\nare initialized with the same pre-training parameters.\n5. T ransformer\nCPM-2 is a standard Transformer-based model that com-\nbines a two-way encoder and a one-way decoder. In order to\neffectively store model parameters on the GPU, model par-\nallelism is used, which separates the self-attention layer and\nthe feedforward layer along the width dimension, and ﬁ-\nnally distributes the partitions of a model on multiple GPUs.\nIn order to reduce memory requirements and accelerate pre-\ntraining, mixed-precision training, gradient checkpointing,\nand zero stage 1 optimization are used.\nMost competitive neural sequence transduction models\nhave an encoder-decoder structure. Here, the encoder maps\nthe input sequence of symbolic representation (x1,. . . ,xn)\nto the continuous representation sequence z=(z1,. . . ,zn).\nGiven z, the decoder then generates the symbols of an out-\nput sequence (y1,. . . ,ym), one element at a time. At each\nstep, the model is automatically regressed, using previously\n2\ngenerated symbols as additional input when generating the\nnext one. Transformer follows this overall architecture,\nusing stacked self-attention and point-by-point, fully con-\nnected encoder and decoder layers, respectively .\nEncoder: The encoder consists of a stack of N identi-\ncal layers. Each layer has two sub-layers. The ﬁrst is a\nmulti-head self-attention mechanism, and the second is a\nsimple, position-wise fully connected feedforward network.\nA residual connection is used around each of the two sub-\nlayers, followed by layer normalization. That is, the output\nof each sublayer is LayerNorm(x+Sublayer(x)), where Sub-\nlayer(x) is a function implemented by the sublayer itself. In\norder to facilitate these residual connections, all sub-layers\nand embedding layers in the model have produced an output\nof dimensional model.\nDecoder: The decoder also consists of a stack of N iden-\ntical layers. In addition to the two sublayers in each en-\ncoder layer, the decoder also inserts a third sublayer that\nperforms multi-head attention on the output of the encoder\nstack. Similar to the encoder, residual connections are used\naround each sub-layer, and then layer normalization is per-\nformed. The self-attention sublayer in the decoder stack has\nalso been modiﬁed to prevent locations from paying atten-\ntion to subsequent locations. This masking, combined with\nthe fact that the output embedding is offset by one position,\nensures that the prediction of position only depends on the\nknown output at positions less than 1.\n6. Setup\nThe pre-trained language understanding model is the\ncore task of natural language processing and language un-\nderstanding. There are several forms of language model-\ning. In this work, we focus on GPT -2, a language model\nbased on a left-to-right generative transformer; BER T , a\nbidirectional transformer model based on language model\nshielding; and CPM-2, a fusion editor Decoded transformer\nlanguage model. The conﬁguration of these models is ex-\nplained in the next section, and refer to the original paper\nfor more details.\n6.1. T raining Dataset\nThis work only focuses on model training, and uses\nthe existing large-scale diversiﬁed dataset English dataset\nPile[\n4] and Chinese dataset Wudao[ 14] on the dataset.\nThe GPT -English word segmenter uses the GPT -2 En-\nglish vocabulary , which contains 30,000 symbols. It is\ntrained by the OpenAI team using the 40GB text and 8 mil-\nlion documents collected by the W ebT ext corpus. W ebT ext\nis a web page curated and ﬁltered by OpenAI humans. All\noutbound links with a rating of at least 3karma are crawled\nfrom Reddit. The generated dataset W ebT ext contains a text\nsubset of 45 million links. In the cleanup phase, links cre-\nated after December 2017 were removed, and after dedupli-\ncation and some heuristic-based cleanup, slightly more than\n8 million documents were left, a total of 40GB of text, and\nall Wikipedia documents were deleted.\nThe GPT -Chinese word segmenter uses CPM’s Chinese\nvocabulary , which contains 30,000 symbols. It is trained by\nthe Tsinghua team using 100G multi-category texts, includ-\ning encyclopedias, news, novels and questions and answers.\nThe CPM Chinese vocabulary uses the unigram language\nmodel to build a new sub-word vocabulary based on the\nsub-word corpus, and since the length of the input sequence\nis usually greater than the length of a single document,\ndifferent documents are connected by adding the ”end-of-\ndocument” symbol after each document. T ogether to make\nfull use of the input length. In the vocabulary construc-\ntion process, a new sub-word vocabulary is constructed, in-\ncluding commonly used words and characters. Considering\nthat the original BER T word segmentation will introduce\nan additional splitter between words, the CPM team set up\na special token as a splitter to make the sub-word process\nreversible.\nTransformer-Chinese word segmenter, using CPM-2 to\nChinese vocabulary , it contains 26240 symbols and is\ntrained by the Tsinghua team using 2.3TB of cleaned Chi-\nnese data. The CPM-2 vocabulary is modiﬁed based on\nthe Chinese BPE. The original BPE inserts many redundant\nspace marks ”\n” in the word segmentation sequence. The\nTsinghua team replaces the sentence segmentation device\nwith a combination of the word segmentation device and\nthe stammering word segmentation, and deletes it Inserted\nspaces. Since it does not matter whether the symbols in the\nvocabulary appear at the beginning of the word, tags like\n”happy” (happy) and ”\nhappy” ( happy) are merged into a\nsingle tag ”happy” to simplify the vocabulary . Chinese data\ncomes from multiple ﬁelds, including encyclopedias, nov-\nels, question and answer, scientiﬁc literature, e-books, news\nand reviews.\nTransformer-English word segmentation, using CPM-\n2 to the English vocabulary , it contains 29,752 symbols,\nwhich is trained by the Tsinghua team using 300GB cleaned\nEnglish data. The CPM-2 vocabulary is modiﬁed based on\nthe English BPE. The original BPE inserts many redun-\ndant space marks ”\n” in the word segmentation sequence.\nThe Tsinghua team replaced the sentence segmenter with\nthe word segmenter combined with nltk word segmentation,\nand deleted Inserted spaces. English data comes from multi-\nple ﬁelds, including encyclopedias, novels, Q&A, scientiﬁc\nliterature, e-books, news, and reviews.\nBER T -English word segmentation uses BER T’s En-\nglish vocabulary , which contains 30,000 symbols. It was\ntrained by the Google team using W ordPiece embedding\non BooksCorpus (800M words) and English Wikipedia\n(2500M words).\nThe BER T -Chinese word segmentation period uses\n3\nBER T’s Chinese vocabulary , which contains 21128 sym-\nbols and was trained by the Harbin Institute of T echnology\nteam using 13.6 million lines of Chinese Wikidata. In the\nprocess of generating the BER T vocabulary , Harbin Insti-\ntute of T echnology downloaded the latest Wikipediadump\nand used WikiExtractor.py to preprocess it according to the\nrecommendations of Devlin et al ., resulting in 1,307 ex-\ntracted ﬁles, and used L TP for Chinese word segmentation.\nEnglish corpus, using the 800GB corpus collected by\nThe Pile. In the process of using The Pile, due to the limi-\ntation of computing resources, not all the data was selected.\nOnly the four blocks of 02, 03, 04, 17 about 213G data were\nused and removed. T wo types of corpus, StackExchange\nand Github, which contain a lot of impurities, are included,\nand the entire dataset is about 200G. The Pile dataset is\na new dataset collected by EleutherAI from PubMedCen-\ntral, ArXiv , GitHub, FreeLaw Project, StackExchange, U.S.\nPatent and Trademark Ofﬁce, PubMed, UbuntuIRC, Hack-\nerNews, Y ouTube, PhilPapers and NIHExPorter, and in-\ntroduces OpenW ebT ext2 and BookCorpus2, they They are\nextensions of the original OpenW ebT ext and BookCorpus\ndatasets.\nChinese corpus, using 3TB Chinese corpus collected by\nWudao. In the process of using Wudao corpus, due to the\nlimitation of computing resources, 200G corpus was used.\nThe Wudao dataset uses 3 billion web pages as the origi-\nnal data source to extract text content from web pages with\nhigh text density . Evaluate the quality of each data source\nbefore extracting the text, and ignore web pages with text\ndensity below 70%. The author uses the simhash algorithm\nto remove those repetitive content, and ﬁlters out those that\ndo not contain meaningful sentences, few words, sensitive\ninformation, high-frequency garbled characters, fragments\ncontaining more than ten consecutive non-Chinese charac-\nters, HTML, Cascading Style Sheets (CSS) and Javascript\nW eb page, remove private information, use punctuation\nmarks (ie period, exclamation mark, question mark, ellip-\nsis) to split the extracted text and delete the last paragraph,\nwhich may sometimes be incomplete. Convert traditional\ncharacters to simpliﬁed characters, and remove abnormal\nsymbols (ie Emoticons, signs, etc.), delete all spaces in each\nsentence.\nChinese dialogue data, from the STC[\n10]-680M cor-\npus dataset. A corpus of approximately 4.4 million con-\nversations on W eibo. In order to build this million-scale\ndataset, the STC dataset ﬁrst grabs hundreds of millions of\nresponse pairs, and then ﬁlters out potential responses by\ndeleting trivial responses such as ”wow”. Advertisements,\nand delete the content after the ﬁrst 30 responses to keep the\ntheme consistent to clean up the original data.\nname corpus language\nBERT -C 200G CN\nBERT -E-S 200G EN\nBERT -E-M 200G EN\nBERT -E-L 200G EN\nBERT -E-X 200G EN\nBERT -E-E 200G EN\nBERT -X-CN-S 200G CN\nBERT -X-EN-S 200G EN\nBERT -X-EN-M 200G EN\nCPM-X-S 200G CN\nCPM-X-M 200G CN\nCPM-X-L 200G CN\nEV A-X 684M CN\nEPM-X-S 200G EN\nEPM-X-M 200G EN\nEPM-X-L 200G EN\nEPM-X-X 200G EN\nCPM-2-X-S 200G CN\nCPM-2-X-M 200G CN\nEPM-2-X-S 200G EN\nT able 1. Corpus and language for the FPM models.\n6.2. T raining Optimization and Hyperparameters\nIn order to effectively train the model, in some experi-\nments, the mixed precision training and dynamic loss scal-\ning are removed to use the tensor core of A100, but in some\nexperiments, the training fails to converge due to accuracy\nreasons, and the mixed precision training is removed. First\ninitialize the weights with a simple normal distribution,\nand then scale the weights immediately before the residual\nlayer, where N is the number of transformer layers com-\nposed of self-attention and MLP blocks. For the optimizer,\nuse Adam with a weight decay of 0.01. In addition, a 1.0\nglobal gradient norm crop is used to improve the stability\nof training large models. In all cases, use a dropout of 0.1.\nFinally , in order to better manage the memory footprint, ac-\ntivation checkpoints are used after each transformer layer.\nFor the GPT -2 model, all training is performed using\n1024 symbol sequences, the batch size is 512, and 300k iter-\nations are performed. The 1.5e-4 learning rate is preheated\nfor 3k iterations, and then a single-loop cosine decay is per-\nformed in the remaining iterations. Stop attenuation at the\nminimum learning rate of 1e-5.\nFor the BER T model, the training process described in\nthe original text is mainly followed. Using the original\nBER T dictionary , the vocabulary size is 30,522. In addition,\nfollow the suggested sentence order prediction to reposition\nthe next sentence prediction head, and use the whole word\nn-gram mask. For all cases, set the batch size to 1024 and\nuse a learning rate of 1.0e-4, warm up in 10,000 iterations\n4\nmodel-name param n-layer\nBERT -C 330M 24\nBERT -E-S 687.5M 50\nBERT -E-M 825M 60\nBERT -E-L 962.5M 70\nBERT -E-X 1.1B 80\nBERT -E-E 1.24B 90\nBERT -X-CN-S 495M 36\nBERT -X-EN-S 495M 36\nBERT -X-EN-M 687.5M 48\nCPM-X-S 2.9B 36\nCPM-X-M 5.1B 64\nCPM-X-L 10.3B 128\nEV A-X 2.9B 36\nEPM-X-S 2.9B 36\nEPM-X-M 4B 50\nEPM-X-L 5.1B 64\nEPM-X-X 6.4B 80\nCPM-2-X-S 2.9B 12\nCPM-2-X-M 5.6B 24\nEPM-2-X-S 2.9B 12\nT able 2. The FPM models parameters and layers.\nand decay linearly in the remaining iterations. Other train-\ning parameters remain unchanged.\n7. Experiments\nAll experiments used up to 2 DGX servers (a total of\n16 A100 SXM3 40GB GPUs). The infrastructure is opti-\nmized for multi-node deep learning applications. Through\nNVSwitch, a bandwidth of 300GB/sec is achieved be-\ntween GPUs in the server, and a bandwidth of 10GB/sec\nis achieved between servers that use 1 InﬁniBand adapter\nper server. . For Chinese, using the GPT -2 model, a 10.3\nbillion parameter language model was trained on the Chi-\nnese dataset, and, in particular, a 2.9 billion parameter lan-\nguage model based on dialogue data was trained; the BER T\nmodel was trained on the Chinese dataset with 495 million\nparameters; the Transformer model has trained a language\nmodel with 5.6 billion parameters on the Chinese dataset.\nIn English, corresponding training work has also been done.\nUsing the GPT -2 model, a language model with 6.4 billion\nparameters was trained on the English dataset; the BER T\nmodel trained a language model with 1.24 billion parame-\nters on the English dataset, and in particular, it trained a 688\nmillion parameter based on single card training technology\nLanguage model; Transformer model trained a language\nmodel with 2.9 billion parameters on the English dataset.\n7.1. EPM­X\nBased on the Transformer model, we built the en-\ncoding and decoding language model EPM-2-X, using\nthe Transformer-English tokenizer, and trained a language\nmodel with 2.9 billion parameters. It has a 12-layer network\nstructure, 6 encoding layers and 6 decoding layers.\n7.2. EPM­2­X\nBased on the Transformer model, we built the en-\ncoding and decoding language model EPM-2-X, using\nthe Transformer-English tokenizer, and trained a language\nmodel with 2.9 billion parameters. It has a 12-layer network\nstructure, 6 encoding layers and 6 decoding layers.\n7.3. BERT ­E\nBased on the BER T model, we built the coded language\nmodel BER T -E. Using the BER T -English word segmenter,\nwe trained 5 models with different layers. The largest model\nis a language model with 1.24 billion parameters. It has 90\nlayers of Transformer coding layers. The network is cas-\ncaded, and we named it BER T -EE.\n7.4. BERT ­X­EN\nBased on the BER T model, we built the coded language\nmodel BER T -X-EN. Using the BER T -English word seg-\nmentation, we trained 2 models with different layers. The\nlargest model is 690 million parameters, including 48 lay-\ners of Transformer coding layer network stack W e named it\nBER T -X-EN-M.\n7.5. CPM­X\nBased on the GPT -2 model, we built a generative lan-\nguage model CPM-X, using GPT -Chinese word segmen-\ntation, trained 3 models with different levels, the largest\nmodel is a 10.3 billion parameter language model, it has\n128 layers Transformer decoding layer network is stacked,\nwe named it CPM-XL. Based on the GPT -2 model, we built\na generative language model EV A-X, using GPT -Chinese\nword segmentation, using STC dialogue data, a language\nmodel of 2.9 billion parameters, it has 36 layers of Trans-\nformer decoding layer network stacked, we will It was\nnamed EV A-X.\n7.6. CPM­2­X\nBased on the Transformer model, we built the codec lan-\nguage model CPM-2-X, we trained 2 models, the largest\nof which uses Transformer-Chinese word segmentation,\ntrained a 5.6 billion parameter language model, it has a 24-\nlayer network structure , 12 encoding layers and 12 decod-\ning layers, we named it CPM-2-XM.\n5\nmodel-name d-n-hidden n-heads d-h-hidden\nBERT -C 1024 16 64\nBERT -E-S 1024 16 64\nBERT -E-M 1024 16 64\nBERT -E-L 1024 16 64\nBERT -E-X 1024 16 64\nBERT -E-E 1024 16 64\nBERT -X-CN-S 1024 16 64\nBERT -X-EN-S 1024 16 64\nBERT -X-EN-M 1024 16 64\nCPM-X-S 2560 32 80\nCPM-X-M 2560 32 80\nCPM-X-L 2560 32 80\nEV A-X 2560 32 80\nEPM-X-S 2560 32 80\nEPM-X-M 2560 32 80\nEPM-X-L 2560 32 80\nEPM-X-X 2560 32 80\nCPM-2-X-S 4096 64 64\nCPM-2-X-M 4096 64 64\nEPM-2-X-S 4096 64 64\nT able 3. The FPM models’ archetecture conﬁgurations.\n7.7. BERT ­C\nBased on the BER T model, we built the coded language\nmodel BER T -C. Using the BER T -Chinese word segmenter,\nwe trained a language model with 330 million parameters.\nIt has a 24-layer Transformer coding layer network cas-\ncaded, and we named it BER T -C.\n7.8. BERT ­X­CN\nBased on the BER T model, we built the coded language\nmodel BER T -X-CN. Using the BER T -Chinese word seg-\nmenter, we trained a language model with 495 million pa-\nrameters, including 36 layers of Transformer coding layers.\nW e named it BER T -X-CN-S.\n8. Evaluation\nIn the evaluation stage, we evaluated the QQP classiﬁca-\ntion task evaluated by GLUE on the BER T -E-S model.\n8.1. GLUE­QQP\nW e use a Nvidia 3090 graphics card to ﬁne-tune the QQP\ndataset on the BER T -E-S model. The BER T -E-S model has\n50 layers, 1024 hidden dimensions, 16 attention heads, each\nhead contains 64 hidden layers, and 687 million parameters.\nIn the QQP classiﬁcation task evaluated by GLUE, after 18\nhours of ﬁne-tuning 270,000 steps on the 109M corpus, the\naccuracy rate of 78.95% exceeded the accuracy of BER T -\nLarge of 72.1%, an increase of 6.85%. Compared with the\nmodel-name time step gpu Flops\nBERT -C 81h 750k 8 727EFlops\nBERT -E-S 34h 500k 8 305EFlops\nBERT -E-M 38h51m 500k 8 349EFlops\nBERT -E-L 45h38m 500k 8 410EFlops\nBERT -E-X 54h24m 500k 8 488EFlops\nBERT -E-E 63h 500k 8 566EFlops\nBERT -X-CN-S 96h 880k 1 107EFlops\nBERT -X-EN-S 315h 2800k 1 353EFlops\nBERT -X-EN-M 315h 2800k 1 353EFlops\nCPM-X-S 3h 10k 8 26.9EFlops\nCPM-X-M 12h 60k 8 108EFlops\nCPM-X-L 24h 100k 8 216EFlops\nEV A-X 25h 160k 2 54EFlops\nEPM-X-S 20h 320k 4 92EFlops\nEPM-X-M 24h 320k 4 110EFlops\nEPM-X-L 27h 320k 4 121EFlops\nEPM-X-X 30h 320k 4 135EFlops\nCPM-2-X-S 60h 200k 2 135EFlops\nCPM-2-X-M 138h 80k 8 1240EFlops\nEPM-2-X-S 110h 200k 2 247EFlops\nT able 4. The cost for the FPM models.\ncurrent 75.2% accuracy rate of ERNIE, the ﬁrst place in the\nGLUE evaluation, this is an increase of 3.75%.\n8.2. CLUE1.0­TNEWS\nW e use an Nvidia 3090 graphics card to ﬁne-tune the\nTNEWS dataset using the BER T -C model. The BER T -C\nmodel has 24 layers, 1024 hidden dimensions, 16 attention\nheads, each head contains 64 hidden layers, and 330 million\nparameters. In the TNEWS classiﬁcation task evaluated by\nCLUE, after ﬁne-tuning the 9.7M corpus for 8 hours and\n70,000 steps, the accuracy rate of 59.99% surpassed the ac-\ncuracy rate of 59.46% of ALBER T -xxlarge, an increase of\n0.53%.\n9. Discussion\nW e will discuss from the perspective of the language,\nscale, network conﬁguration, and cost of the training mod-\nels.\n9.1. Language\nFrom T able-\n6.1, Chinese is the single country with the\nlargest number of users, and English is the most widely used\nlanguage. This job has trained a large number of Chinese\nand English corpora and conducted detailed training.\n9.2. Scale\nFrom T able-\n6.2, the models with the least number of lay-\ners are CPM-2-X and EPM-2-X, with only 12 layers. The\n6\nmodel with the largest number of layers is CPM-X, with\n128 layers trained. From the perspective of the difﬁculty of\ntraining, the GPT structure is easier to cascade the number\nof layers; while the Transformer codec structure is not easy\nto increase the number of layers, and it is easier to touch the\nmemory limit of the hardware.\nFrom the perspective of model parameters, the model\nwith the least amount of parameters is BER T -C, with only\n330 million, and the largest parameter is CPM-X-L, with\nparameters reaching 10.3 billion.\n9.3. Architecture Conﬁguration\nFrom T able-\n7.8, for BER T , GPT , and Transformer, the\nnetwork structure parameters used in this work are ﬁxed,\nand the parameters in the original paper are used more with-\nout major modiﬁcations.\n9.4. Cost\nIt can be seen from T able-\n8.2 that in terms of time con-\nsumption, the shortest training time is CPM-XS, and the\nlongest model is CPM-X-EN; from the training step num-\nber is CPM-XS, only 100,000 steps, the most training It is\nBER T -X-EN with 2.8 million steps. From the perspective\nof computing power, the least computing power is CPM-\nXS, which uses 26.9EFlops, and the most computing power\nis CPM-2-XM, which uses 1240EFlops.\n10. Conclusion\nIn this work, the latest technology for training super-\nlarge converter models is used, and a simple and efﬁcient\nintra-layer model parallel method is used to train dozens of\nconverter models with hundreds of millions or even tens of\nbillions of parameters.\nFor Chinese, using the GPT -2 model, a 10.3 billion pa-\nrameter language model was trained on the Chinese dataset,\nand, in particular, a 2.9 billion parameter language model\nbased on dialogue data was trained; the BER T model was\ntrained on the Chinese dataset with 495 million parameters;\nthe Transformer model has trained a language model with\n5.6 billion parameters on the Chinese dataset.\nIn English, corresponding training work has also been\ndone. Using the GPT -2 model, a language model with 6.4\nbillion parameters was trained on the English dataset; the\nBER T model trained a language model with 1.24 billion pa-\nrameters on the English dataset, and in particular, it trained\na 688 million parameter based on single card training tech-\nnology Language model; Transformer model trained a lan-\nguage model with 2.9 billion parameters on the English\ndataset.\nIn the TNEWS classiﬁcation task evaluated by CLUE,\nthe BER T -C model exceeded the 59.46% accuracy of\nALBER T -xxlarge with an accuracy rate of 59.99%, an in-\ncrease of 0.53%.\nIn the QQP classiﬁcation task evaluated by GLUE, the\naccuracy rate of 78.95% surpassed the accuracy rate of\nBER T -Large of 72.1%, an increase of 6.85%. Compared\nwith the current accuracy rate of ERNIE, the ﬁrst place in\nthe GLUE evaluation of 75.2%, an increase of 3.75%.\nReferences\n[1] T om B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry , Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\n2\n[2] Yiming Cui, W anxiang Che, Ting Liu, Bing Qin, Ziqing\nY ang, Shijin W ang, and Guoping Hu. Pre-training with\nwhole word masking for chinese bert. arXiv preprint\narXiv:1906.08101, 2019.\n2\n[3] Jacob Devlin, Ming-W ei Chang, Kenton Lee, and Kristina\nT outanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In Proceedings of the\n2019 Conference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human Language\nT echnologies, V olume 1 (Long and Short P apers) , pages\n4171–4186, 2019.\n1\n[4] Leo Gao, Stella Biderman, Sid Black, Laurence Golding,\nTravis Hoppe, Charles Foster, Jason Phang, Horace He, An-\nish Thite, Noa Nabeshima, et al. The pile: An 800gb\ndataset of diverse text for language modeling. arXiv preprint\narXiv:2101.00027, 2020.\n3\n[5] Nitish Shirish Keskar, Bryan McCann, Lav R V arshney ,\nCaiming Xiong, and Richard Socher. Ctrl: A condi-\ntional transformer language model for controllable genera-\ntion. arXiv preprint arXiv:1909.05858 , 2019.\n2\n[6] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin\nGimpel, Piyush Sharma, and Radu Soricut. Albert: A lite\nbert for self-supervised learning of language representations.\narXiv preprint arXiv:1909.11942 , 2019.\n1\n[7] Junyang Lin, Rui Men, An Y ang, Chang Zhou, Ming Ding,\nYichang Zhang, Peng W ang, Ang W ang, Le Jiang, Xianyan\nJia, et al. M6: A chinese multimodal pretrainer. arXiv\npreprint arXiv:2103.00823 , 2021.\n2\n[8] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar\nJoshi, Danqi Chen, Omer Levy , Mike Lewis, Luke Zettle-\nmoyer, and V eselin Stoyanov . Roberta: A robustly optimized\nbert pretraining approach. arXiv preprint arXiv:1907.11692 ,\n2019.\n2\n[9] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, and Ilya Sutskever. Language models are unsuper-\nvised multitask learners. T echnical report, 2020.\n1\n[10] Lifeng Shang, Zhengdong Lu, and Hang Li. Neural re-\nsponding machine for short-text conversation. arXiv preprint\narXiv:1503.02364, 2015. 4\n[11] Alex W ang, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy , and Samuel Bowman. Glue: A multi-task\nbenchmark and analysis platform for natural language un-\nderstanding. In Proceedings of the 2018 EMNLP W orkshop\nBlackboxNLP: Analyzing and Interpreting Neural Networks\nfor NLP , pages 353–355, 2018.\n1\n7\n[12] Ben W ang. Mesh-Transformer-J AX: Model-Parallel\nImplementation of Transformer Language Model with\nJ AX.\nhttps://github.com/kingoflolz/mesh-\ntransformer-jax, May 2021. 2\n[13] Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao,\nY udong Li, Y echen Xu, Kai Sun, Dian Y u, Cong Y u, et al.\nClue: A chinese language understanding evaluation bench-\nmark. In Proceedings of the 28th International Conference\non Computational Linguistics , pages 4762–4772, 2020.\n1\n[14] Sha Y uan, Hanyu Zhao, Zhengxiao Du, Ming Ding, Xiao\nLiu, Y ukuo Cen, Xu Zou, Zhilin Y ang, and Jie T ang. Wudao-\ncorpora: A super large-scale chinese corpora for pre-training\nlanguage models. AI Open , 2021.\n3\n[15] W ei Zeng, Xiaozhe Ren, T eng Su, Hui W ang, Yi Liao, Zhi-\nwei W ang, Xin Jiang, ZhenZhang Y ang, Kaisheng W ang, Xi-\naoda Zhang, et al. Pangu- alpha: Large-scale autoregressive\npretrained chinese language models with auto-parallel com-\nputation. arXiv preprint arXiv:2104.12369 , 2021.\n2\n[16] Zhengyan Zhang, Y uxian Gu, Xu Han, Shengqi Chen, Chao-\njun Xiao, Zhenbo Sun, Y uan Y ao, Fanchao Qi, Jian Guan,\nPei Ke, et al. Cpm-2: Large-scale cost-effective pre-trained\nlanguage models. arXiv preprint arXiv:2106.10715 , 2021.\n2\n[17] Zhengyan Zhang, Xu Han, Hao Zhou, Pei Ke, Y uxian Gu,\nDeming Y e, Y ujia Qin, Y usheng Su, Haozhe Ji, Jian Guan,\net al. Cpm: A large-scale generative chinese pre-trained lan-\nguage model. AI Open , 2:93–99, 2021.\n2\n8",
  "topic": "Foundation (evidence)",
  "concepts": [
    {
      "name": "Foundation (evidence)",
      "score": 0.7968837022781372
    },
    {
      "name": "Scale (ratio)",
      "score": 0.6588302254676819
    },
    {
      "name": "Computer science",
      "score": 0.5337487459182739
    },
    {
      "name": "Natural language processing",
      "score": 0.39103028178215027
    },
    {
      "name": "Artificial intelligence",
      "score": 0.35461878776550293
    },
    {
      "name": "History",
      "score": 0.14020559191703796
    },
    {
      "name": "Geography",
      "score": 0.1289932131767273
    },
    {
      "name": "Cartography",
      "score": 0.09848672151565552
    },
    {
      "name": "Archaeology",
      "score": 0.086009681224823
    }
  ],
  "institutions": [],
  "cited_by": 1
}