{
    "title": "DC-Former: Diverse and Compact Transformer for Person Re-identification",
    "url": "https://openalex.org/W4382457808",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A1966168618",
            "name": "Wen Li",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2071287572",
            "name": "Cheng Zou",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2102035757",
            "name": "Meng Wang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2147866624",
            "name": "Fu-Rong Xu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2307018167",
            "name": "Jianan Zhao",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2809599241",
            "name": "Ruobing Zheng",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A2102674343",
            "name": "Yuan Cheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097149163",
            "name": "Wei Chu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A1966168618",
            "name": "Wen Li",
            "affiliations": [
                "Inception Institute of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2071287572",
            "name": "Cheng Zou",
            "affiliations": [
                "Inception Institute of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2102035757",
            "name": "Meng Wang",
            "affiliations": [
                "Inception Institute of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2147866624",
            "name": "Fu-Rong Xu",
            "affiliations": [
                "Inception Institute of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2307018167",
            "name": "Jianan Zhao",
            "affiliations": [
                "Inception Institute of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2809599241",
            "name": "Ruobing Zheng",
            "affiliations": [
                "Inception Institute of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2102674343",
            "name": "Yuan Cheng",
            "affiliations": [
                "Inception Institute of Artificial Intelligence"
            ]
        },
        {
            "id": "https://openalex.org/A2097149163",
            "name": "Wei Chu",
            "affiliations": [
                "Inception Institute of Artificial Intelligence"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3159481202",
        "https://openalex.org/W2989923292",
        "https://openalex.org/W2769088658",
        "https://openalex.org/W6786714330",
        "https://openalex.org/W3122006793",
        "https://openalex.org/W4312297495",
        "https://openalex.org/W3097870364",
        "https://openalex.org/W3128723389",
        "https://openalex.org/W3035655772",
        "https://openalex.org/W3190739399",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W3208382980",
        "https://openalex.org/W3156464220",
        "https://openalex.org/W1982925187",
        "https://openalex.org/W3166362606",
        "https://openalex.org/W2747359207",
        "https://openalex.org/W2604463754",
        "https://openalex.org/W3100555577",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2922510913",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2796664602",
        "https://openalex.org/W3008625353",
        "https://openalex.org/W2783855081",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3100506510",
        "https://openalex.org/W4200635168",
        "https://openalex.org/W2769994766",
        "https://openalex.org/W3176285518",
        "https://openalex.org/W3211333906",
        "https://openalex.org/W3217568419",
        "https://openalex.org/W3178838461",
        "https://openalex.org/W3168146779",
        "https://openalex.org/W6687888618",
        "https://openalex.org/W6703133677",
        "https://openalex.org/W3098711604",
        "https://openalex.org/W6743440100",
        "https://openalex.org/W3175823695",
        "https://openalex.org/W2963000559",
        "https://openalex.org/W3143016713",
        "https://openalex.org/W3205959870",
        "https://openalex.org/W2204750386",
        "https://openalex.org/W2962691289",
        "https://openalex.org/W3217250086",
        "https://openalex.org/W2998508940",
        "https://openalex.org/W2963047834",
        "https://openalex.org/W4353103813",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W3200038036",
        "https://openalex.org/W2598634450",
        "https://openalex.org/W2963901085",
        "https://openalex.org/W4287330514",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3175445198",
        "https://openalex.org/W3204665409",
        "https://openalex.org/W2967515867",
        "https://openalex.org/W4287236877",
        "https://openalex.org/W4304098207",
        "https://openalex.org/W3009561768",
        "https://openalex.org/W2963842104",
        "https://openalex.org/W3170874841"
    ],
    "abstract": "In person re-identification (ReID) task, it is still challenging to learn discriminative representation by deep learning, due to limited data. Generally speaking, the model will get better performance when increasing the amount of data. The addition of similar classes strengthens the ability of the classifier to identify similar identities, thereby improving the discrimination of representation. In this paper, we propose a Diverse and Compact Transformer (DC-Former) that can achieve a similar effect by splitting embedding space into multiple diverse and compact subspaces. Compact embedding subspace helps model learn more robust and discriminative embedding to identify similar classes. And the fusion of these diverse embeddings containing more fine-grained information can further improve the effect of ReID. Specifically, multiple class tokens are used in vision transformer to represent multiple embedding spaces. Then, a self-diverse constraint (SDC) is applied to these spaces to push them away from each other, which makes each embedding space diverse and compact. Further, a dynamic weight controller (DWC) is further designed for balancing the relative importance among them during training. The experimental results of our method are promising, which surpass previous state-of-the-art methods on several commonly used person ReID benchmarks. Our code is available at https://github.com/ant-research/Diverse-and-Compact-Transformer.",
    "full_text": "DC-Former: Diverse and Compact Transformer for Person Re-identification\nWen Li1, Cheng Zou1, Meng Wang1, Furong Xu1, Jianan Zhao1,\nRuobing Zheng1, Yuan Cheng2*, Wei Chu1\n1Ant Group\n2Artificial Intelligence Innovation and Incubation (AI3) Institute, Fudan University\n{yinian.lw,wuyou.zc,darren.wm,booyoungxu.xfr,zhaojianan.zjn,zhengruobing.zrb,\nweichu.cw}@antgroup.com, cheng yuan@fudan.edu.cn\nAbstract\nIn person re-identification (ReID) task, it is still challenging\nto learn discriminative representation by deep learning, due\nto limited data. Generally speaking, the model will get better\nperformance when increasing the amount of data. The addi-\ntion of similar classes strengthens the ability of the classifier\nto identify similar identities, thereby improving the discrimi-\nnation of representation. In this paper, we propose a Diverse\nand Compact Transformer (DC-Former) that can achieve a\nsimilar effect by splitting embedding space into multiple di-\nverse and compact subspaces. Compact embedding subspace\nhelps model learn more robust and discriminative embed-\nding to identify similar classes. And the fusion of these di-\nverse embeddings containing more fine-grained information\ncan further improve the effect of ReID. Specifically, multi-\nple class tokens are used in vision transformer to represent\nmultiple embedding spaces. Then, a self-diverse constraint\n(SDC) is applied to these spaces to push them away from\neach other, which makes each embedding space diverse and\ncompact. Further, a dynamic weight controller (DWC) is fur-\nther designed for balancing the relative importance among\nthem during training. The experimental results of our method\nare promising, which surpass previous state-of-the-art meth-\nods on several commonly used person ReID benchmarks. Our\ncode is available at https://github.com/ant-research/Diverse-\nand-Compact-Transformer.\nIntroduction\nPerson re-identification (ReID) aims at identifying person\nacross different camera views, which is very important in\nmany applications, such as intelligent surveillance, cross\ncamera tracking and smart city. While ReID has attracted\ngreat research interest and gained considerable development\nin recent years, there still exist some challenges (Yan et al.\n2021; Yang et al. 2021), such as blur, low resolution, oc-\nclusion, illumination and viewpoint variation. These factors\ncause the intra-class distance of samples to be larger than the\ninter-class distance, which makes it challenging to retrieve\npedestrians of correct identities.\nPlenty of efforts (He et al. 2021; Gu et al. 2022; Tan et al.\n2022) have been made recently to improve the performance\n*Corresponding author\nCopyright ¬© 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Illustration of DC-Former for representation learn-\ning. On the left, each circle with dots denotes an embed-\nding space. DC-Former uses multiple embeddings to repre-\nsent each sample, and a self-diverse constraint is imposed on\nthese embeddings to push them away. Finally, DC-Former\nobtains multiple diverse embedding subspaces for represen-\ntation. And each subspace is more compact than the original\nspace, which increases the identity density of embedding\nspace to help model improve its discrimination for iden-\ntifying similar classes. Figures on the right visualized by\nGrad-Cam (Selvaraju et al. 2017) show that diverse embed-\ndings from DC-Former focus on multiple different discrim-\ninative regions. And the fusion of them can provide more\nfine-grained information.\nof ReID, and among which increasing the amount of train-\ning data may be the most powerful way. On the one hand,\nincreasing more instances for each identity helps to recog-\nnize one person under different circumstances, extracting the\nmost common and discriminative features for the same class,\nthus reducing intra-class distance. On the other hand, in-\ncreasing more identities means that there is a higher chance\nto place more similar (easy-to-confuse) classes in the em-\nbedding space, which would help the model to extract dis-\ncriminative features with larger inter-class distance for sim-\nilar classes. Therefore, more identities and more instances\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n1415\nfor each identity, resulting in larger inter-class distance and\nsmaller intra-class distance, make ReID task easier.\nAs mentioned above, great potential lies in data. How-\never, due to limited data, existing works concentrate more on\nother aspects, such as stronger backbone (Liu et al. 2021),\nmetric loss (Sun et al. 2020), pre-trained model (Fu et al.\n2021), etc. As far as we know, few studies consider ReID\ntask from the perspective of data except for (Gong et al.\n2021; Zhong et al. 2020), all of which are data augmenta-\ntions, finally increasing the instances of each identity, but\nno study tries to increase the number of identity. Directly\nincreasing the number of identity is impractical because it\nequals to adding more labeled data, which is expensive, but\nif there is a certain way that can simulate to increase the\nnumber of identity, it will also improve the performance\nof ReID. Here we hypothesize that increasing the number\nof identity is equal to increasing the identity density in a\ngiven space. Reducing the size of the embedding space can\nmake all the identities become more compact, so the relative\namount of identity (identity density) increases. In a more\ncompact embedding space, the reduction of inter-class dis-\ntance makes it more difficult for model to distinguish sam-\nples from similar classes. Therefore, more discriminative\nand robust information is extracted to ensure that the clas-\nsifier correctly identifies similar classes.\nIn this paper, we propose a Diverse and Compact Trans-\nformer (DC-Former) that can achieve a similar effect of in-\ncreasing identities of training data. As shown in Figure 1,\nit compresses the embedding space by dividing the orig-\ninal space into multiple subspaces. More compact repre-\nsentations in the compressed embedding subspace helps\nmodel extract more discriminative feature to identify sim-\nilar classes. And the embeddings of different subspaces\nare diverse, the fusion of them contains more information\nthat can further improve performance. Specifically, multi-\nple class tokens (CLSes) are used in vision transformer, and\neach CLS is supervised by an identity loss to obtain mul-\ntiple representations. Then, a self-diverse constraint (SDC)\nis applied to CLSes to make the distribution of them as\nfar as possible. In this way, the original space is divided\ninto multiple subspaces. Due to the different learning status\nof different CLSes when dividing the space, some CLSes\nare pushed away while others are very close. A dynamic\nweight controller (DWC) is further designed for balancing\nthe relative importance among them during training. Finally,\neach of compact subspaces learns a more robust representa-\ntion. The experimental results of our method are promising,\nwhich surpass previous state-of-the-art methods on three\ncommonly used person ReID benchmarks.\nThe contributions of this paper are summarized as fol-\nlows:\n‚Ä¢ We propose a DC-Former to get multiple diverse and\ncompact embedding subspaces, each embedding of these\ncompact subspaces is more robust and discriminative to\nidentify similar classes. And the fusion of these diverse\nembeddings can further improve the effect of ReID.\n‚Ä¢ We propose a self-diverse constraint (SDC) to make em-\nbedding subspaces presented by each class token do not\noverlap. And a dynamic weight controller (DWC) is de-\nvised to balance the relative importance among multiple\nclass tokens during training.\n‚Ä¢ Our method surpasses previous methods and sets state-\nof-the-art on three person ReID benchmarks including\nMSMT17, Market-1501 and CUHK03.\nRelated Work\nImage-Based ReID\nReID can be conducted on either images (He et al. 2021)\nor videos (Zhao et al. 2021) . Recent image-based tasks\nmainly focus on person ReID and vehicle ReID. The stud-\nies of person ReID have paid attention to feature represen-\ntation learning (Chen et al. 2019; Suh et al. 2018) and deep\nmetric learning (Zheng et al. 2017; Deng et al. 2018). For\nthe feature learning strategy, Typically, there are three main\ncategories for the feature learning strategy, including global\nfeature (Chen et al. 2019), local feature (Suh et al. 2018) and\nauxiliary feature (e.g., domain information (Lin et al. 2017),\nsemantic attributes (Lin et al. 2019), viewpoint information\n(Zhu et al. 2020)). As for the deep metric learning, many ex-\nisting works have designed loss functions to guide the fea-\nture representation learning, which can be divided into three\ngroups, i.e., identity loss (Zheng et al. 2017), verification\nloss (Deng et al. 2018) and triplet loss (Hermans, Beyer, and\nLeibe 2017).\nRepresentation Learning In ReID\nReID is one of fine-grained task which has far intra-class\ndistance and close inter-class distance. Extracting discrimi-\nnative features to distinguish similar classes is challenging.\nTo minimize intra-class distance and maximize inter-class\ndistance, proxy-based loss (Zheng, Zheng, and Yang 2017;\nKrizhevsky, Sutskever, and Hinton 2012) and pair-based loss\n(Liu et al. 2017; Hermans, Beyer, and Leibe 2017) are com-\nmonly used to push the different class away and pull the\nsame class close. Some works use attention-base method\n(Huynh and Elhamifar 2020) to discover discriminative fea-\ntures or enhance low-discriminative features, which helps\ndisciminative feature representation. For example, DAM\n(Xu et al. 2021) iteratively identifies insufficiently trained\nelements and improves them. And some works (Zhong et al.\n2020; Srivastava et al. 2014) impose some regulation oper-\nations (e.g., random erasing and dropout) to prevent overfit-\nting. Self-supervised representation learning (He et al. 2019;\nChen et al. 2020; Caron et al. 2021; Isobe et al. 2021) use\ncontrastive loss to maximize the agreement of features from\nsame image with different augmentations.\nTransformer-Based ReID\nBefore vision transformer, CNN-based methods have\nachieved absolute advantages on ReID task. Methods like\nPCB (Sun et al. 2018), MGN (Wang et al. 2018), etc., par-\ntition an image into several stripes to obtain local feature\nrepresentations for each stripe with multiple granularities.\nWith the success of transformer in the field of computer ver-\nsion, it‚Äôs also widely used in person ReID. Compared with\n1416\nTransformer Layers\nTransformer Layers\n3 09 58 46‚Ä¶ ‚Ä¶\n‚Ä¶\nPosition \nEmbedding\nMultiple Class TokensLinear Projection of Flatted Patches\n‚Ä¶\nReID Head\n‚Ä¶\nSelf-diverse Constraint\nSpace B\nùëì!\nSpace A\nùëì\"\nSpace C\nùëì#\nOriginal Feature Space\nùëì\" ùëì!\nùëì#\n‚Ä¶\n‚Ä¶\n‚Ä¶\nLoss Loss Lossùúî! ‚àó ùúî\" ‚àóùúî# ‚àó\n‚Ä¶\nDynamic Weight Controller\n: original class tokens\n: self-diverse class tokens\n(a) (b)\nFigure 2: The framework of DC-Former. Multiple class tokens concatenated with patch embeddings, adding positional embed-\ndings, are fed into transformer encoder. Self-diverse constraint is employed on these class tokens in the last transformer layer\nto push them far way from each other, leading to diverse representation spaces. Then they are each supervised by a ReID head,\nwhich contains a triplet loss and a classification loss. During training, a dynamic weight controller is used to dynamically adjust\nthe constraint loss for easier optimization. fa, fb and fc are different representations of the same object.\nCNN-based method, ViT can keep more detailed informa-\ntion because these is no convolution and dowmsampling op-\nerators, which is more suited for fine-grained task like ReID.\nTransReID (He et al. 2021) is the first pure transformer-\nbased method on ReID, it proposes a jigsaw patches mod-\nule (JPM) which shuffles patch embeddings and re-groups\nthem for further feature learning to extract several local fea-\ntures and aggregates them to get robust feature with global\ncontext. TransReID-SSL (Luo et al. 2021) uses a massive\nperson ReID dataset LUPerson (Fu et al. 2021) to train a\nstronger pre-trained model by DINO (Caron et al. 2021).\nMany works (Zhu et al. 2021; Li et al. 2021; Chen et al.\n2021; Lai, Chai, and Wei 2021) devote to extract partial re-\ngion representation by transformer. For example, AAformer\n(Zhu et al. 2021) uses the additional learnable vectors of\n‚Äòpart tokens‚Äô to learn the part representations by clustering\nthe patch embeddings into several groups and integrates the\npart into the self-attention for alignment. Some methods (Li\net al. 2021; Lai, Chai, and Wei 2021) use transformer to\nlearn several different part-aware masks to get partial fea-\nture. And some works aim to fuse features at different gran-\nularities. For example, HAT (Zhang et al. 2021) put hier-\narchical features at different granularities from CNN back-\nbone into transformer to aggregate them. These methods\nhave one thing in common that multiple different features\nare extracted and integrated to obtain more robust represen-\ntation. However, they impose constraints on specific goals\nlike focusing only on local receptive fields or different gran-\nularity which are limited to extract representation that the\nmodel really needs. It is insightful to learn multiple feature\nspace by model itself through certain implicit constraint.\nMethodology\nOverview\nFor enhancing the identity density to increase the ability of\ndiscriminating similar classes, the original embedding space\nis divided into multiple diverse and compact subspaces. The\nproposed method consists of three parts: a ViT-based net-\nwork with multiple class tokens to produce multiple em-\nbedding spaces, a self-diverse constraint loss (SDC) to push\nthese embedding spaces far away from each other, and a dy-\nnamic weight controller (DWC) to balance the relative im-\nportance among class tokens during training as the token\nnumber increases.\nNetwork Architecture\nTo construct multiple different embedding spaces for one\nsingle input image, an architecture with multiple parallel\noutput features is required. Most multi-feature representa-\ntion methods usually extract different features from dif-\nferent granularities or different partial regions by multiple\nbranches (Wang et al. 2018; Sun et al. 2018). The multiple\nfeatures generated by these branches are often individual and\nhave no interactions with each other during train, which may\n1417\n(a) w/o SDC\n (b) with SDC\nFigure 3: Feature distributions visualized by t-SNE of two\nclass tokens. Different colors represent different classes,\nmarker ‚ãÜ stands for feature points from one class token\nand ‚Éù from the other. The data points are sampled from\nMSMT17 test set. (a) Without SDC, the feature points from\ntwo class tokens are heavily overlapped, which implies that\nthe embedding spaces of these two class tokens are very\nclose. (b) With SDC, the feature points from two class to-\nkens can be easily separated into two parts without any over-\nlap, and each embedding space become more compact.\ncause multiple embedding spaces overlapping and homoge-\nnous.\nIn vision transformer, thanks to stacked self-attention\nmodules, information flows from patch embeddings to one\nclass token, layer by layer gradually and autonomously. The\nclass token acts as an information collector here, it receives\ninformation from each patch and outputs the summary ac-\ncording to its prior knowledge (learnable parameters). It‚Äôs\nnatural to come up with an idea that if there exist multiple\nclass tokens acting as multiple information collectors, there\nwill be a chance to gather different information from patch\nembeddings. Then, multiple different embedding spaces can\nbe supported by multiple class tokens. Therefore, ViT with\nadditional class tokens is chosen to be the main achitecture.\nFigure 2 illustrates the proposed framework. The input\nimage is divided into H √ó W patches, and then each patch\nis mapped to a vector of dimension D through a trainable\nlinear projection. The output of this projection is denoted\nas patch embeddings PC√óD, where C is the number of\npatches. Then multiple learnable embeddings of dimension\nD are concatenated as a sequence, denoted as class tokens\nfN√óD, where N is the number of class tokens. After that,\nclass tokens and patch embeddings are concatenated to get a\nvector sequence X(N+C)√óD = [fN√óD; PC√óD]. Next, this\nsequence added with a learnable positional embedding se-\nquence is fed into transformer layers to get multiple repre-\nsentations.\nNotably in our design, multiple class tokens are mutually\nvisible in self-attention layers, which helps the model to con-\nverge because of sharing intermediate features. For one cer-\ntain class token, it receives information not only from all the\npatch embeddings but also from other class tokens, which\nimproves the information acquisition efficiency.\nSelf-Diverse Constraint Loss\nIf there are multiple different embeddings to represent each\nsamples, the embedding space will become more compact,\nwhich helps model improve embedding‚Äôs discrimination of\nsimilar classes. With multiple class tokens, the proposed\nframework can output multiple embeddings in parallel. But\nin practice, the learned embedding spaces overlap with each\nother because these class tokens are homogeneous in struc-\nture, their learning goals are the same, and there is no exter-\nnal energy to push them to be different.\nOne way to learn different embedding spaces is to change\nthe learning objectives, DeiT (Touvron et al. 2021) proposed\na distillation token to learn the output logits of another CNN\nmodel, which requires training two models and much man-\nual efforts, making the training procedure complex. Con-\nsidering that class token is computed as a weighted sum of\nV , although multiple class tokens share the same K-V pairs\nfrom patch embeddings PC√óD, they can still get different\nattentions from PC√óD if the query Q is different. In other\nwords, class tokens can learn different information as long\nas they are different.\nHere we humbly hypothesize that the more difference be-\ntween class tokens, the farther the distance between their\nembedding space, which pushes overlapping embedding\nspaces away from each other. Not only do we want the class\ntokens to be different from each other, but we want to max-\nimize the difference between them. In the metric of cosine\nsimilarity, if two vectors are orthogonal, i.e., cos(‚àó, ‚àó) = 0,\nthey are irrelevant, and the distance between them are ex-\ntremely large. If the class tokens are orthogonal to each\nother, each embedding space of them is compressed more\ntightly in the finite space to ensure their distance is maxi-\nmized.\nWe propose a self-diverse constraint loss (SDC) to con-\nstrain the relationship between class tokens. It forces the\nclass tokens to be orthogonal to each other and can be ex-\npressed as:\nLsdc = 1\nC2\nN\nP\ni\nP\njŒΩij, i < j, i, j= 1, ..., N (1)\nwhere ŒΩij = |cos(fi, fj)|, and fi, fj indicates any two class\ntokens of fN√óD. Self-diverse constraint loss is employed\non these class tokens to make sure that these embedding sub-\nspaces is far apart from each other. It makes each embedding\nsubspace compact and helps model extract more discrimina-\ntive feature to identify similar classes.\nThe representations from different embedding subspaces\ncontain not only identity information, but also features from\ndifferent perspectives, i.e., coarse/fine-grained granularity,\nglobal/local region, and other unrecognized complementary\naspects. The fusion of them can further facilitate robust and\nperturbation-invariant feature representation for ReID tasks.\nDynamic Weight Controller\nTo obtain more compact and robust features, more class to-\nkens are needed to get more different embedding subspaces.\nHowever, as the number of class tokens increases, it be-\ncomes harder to optimize because there are more pairs of\nclass tokens, each of which is required to be orthogonal.\n1418\nMethods Publications MSMT17 Market-1501 CUHK03-l CUHK03-d\nmAP R1 mAP R1 mAP R1 mAP R1\nPFD (Wang et al. 2022) AAAI 64.4 83.8 89.7 95.5 - - - -\nTransReID (He et al. 2021) ICCV 67.4 85.3 88.9 95.2 - - - -\nDPM (Tan et al. 2022) ACM‚ÄôMM - - 89.7 95.5 - - - -\nGASM384‚Üë (He and Liu 2020) ECCV 52.5 79.5 84.7 95.3 - - - -\nCDNet384‚Üë (Li, Wu, and Zheng 2021) CVPR 54.7 78.9 86.0 95.1 - - - -\nAutoLoss384‚Üë (Gu et al. 2022) CVPR 63.0 83.7 90.1 96.2 74.3 75.6 - -\nAAformer384‚Üë (Zhu et al. 2021) - 63.2 83.6 87.7 95.4 77.8 79.9 74.8 77.6\nOH-former368‚Üë (Chen et al. 2021) - 69.2 86.6 88.7 95.0 - - - -\nTransReID384‚Üë (He et al. 2021) ICCV 69.4 86.2 89.5 95.2 - - - -\nDC-Former - 69.8 86.2 90.4 96.0 79.4 81.6 77.5 80.1\nDC-Former384‚Üë - 70.7 86.9 90.6 96.0 83.3 84.4 77.5 79.6\nTable 1: Comparisons with state-of-the-art methods on person ReID benchmarks. 368 ‚Üë and 384 ‚Üë denote the input images are\nresized to 368 √ó 128 and 384 √ó 128, otherwise 256 √ó 128. Best results for previous methods are underlined and best of our\nmethods are labeled in bold.\nIn experiments, we find that when the number of class\ntokens increases to a certain number, the loss in Eq. 1 can-\nnot be minimized as expected. Although some of the pairs\nhave low cosine similarity, others are still very similar (co-\nsine similarity close to 1). Actually it only learns less embed-\nding space than we expect, and those class token pairs with\nhigher self-diverse constraint loss are not optimized well in\ntraining. The reason for this is that randomness in the train-\ning process makes it easier for some pairs to be pushed far-\nther apart while others harder. Therefore, it‚Äôs necessary to\nchange the relative importance among class token pairs for\nself-diverse constraint loss while training.\nWe propose a dynamic weight controller (DWC) to dy-\nnamically adjust the loss weight of each pair during train-\ning on the fly. Instead of simply averaging these pair losses\nas in Eq. 1, the loss of each pair is re-weighted by its own\nsoftmax-normalized loss. The weight of each loss can be de-\nfined as:\nœâij = exp(ŒΩij)\nP\nm\nP\nnexp(ŒΩmn), m < n, m, n= 1, ..., N(2)\nSo, the balanced self-diverse constraint loss is defined as:\nLSDC = P\ni\nP\njœâijŒΩij, i < j, i, j= 1, ..., N (3)\nThose pairs with smaller cosine similarities are given\nsmaller weights, while larger ones are given larger weights\nto make the model focus more on similar pairs. In this way,\npairs can be learned more evenly so that they can be all or-\nthogonal to each other.\nObjective Function\nIn training, in order to ensure that each class token has the\nability to distinguish identities, they are each supervised by\ncross-entropy loss for classification (ID loss) after normal-\nized by BNNeck (Luo et al. 2019). To pull the samples of the\nsame class closer and push the samples of different classes\nfar away, the triplet loss with soft-margin is used to mine\nhard example in each embedding subspace and can be cal-\nculated as:\nLtriplet = log[1 +exp(||fa ‚àí fp||2\n2 ‚àí ||fa ‚àí fn||2\n2)] (4)\nThe overall objective function is:\nLtotal = 1\nN\nPN\ni=1(Li\nID + Li\ntriplet) +ŒªLSDC (5)\nDuring inference, all the class tokens are concatenated to\nrepresent an image.\nExperiments\nExperimental Settings\nImplementation Details. We apply VIT-B/16 (Dosovitskiy\net al. 2020) as our backbone, it contains 12 transformer lay-\ners with hidden size of 768 dimensions. Overlapping patch\nembedding (step size = 12) and SIE (He et al. 2021) are\nalso used in our experiments. All the images are resized to\n256 √ó 128 unless other specified. The training images are\naugmented with random horizontal flipping, padding, ran-\ndom cropping, random erasing (Zhong et al. 2020), and ran-\ndom grayscale (Gong et al. 2021). The initial weights of the\nmodels are pre-trained on ImageNet.\nThe batch size is set to 64 with 4 images per ID. SGD opti-\nmizer is employed with a momentum of 0.9 and a weight de-\ncay of 1e-4. The learning rate is initialized as 0.032 with co-\nsine learning rate decay. All the experiments are performed\non 4 Nvidia Tesla V100 GPUs.\nDatasets and Evaluations. The proposed method is eval-\nuated on four widely used person ReID benchmarks, i.e.,\nMSMT17 (Wei et al. 2018),Market-1501 (Zheng et al. 2015)\nand CUHK03 (Li et al. 2014). Mean Average Precision\n(mAP) and Cumulative Matching Curve (CMC) are used to\nevaluate the performance of ReID tasks.\nComparisons with State-of-the-Arts\nTo verify the effectiveness of the proposed method, experi-\nments are conducted on three commonly used person ReID\nbenchmarks in Table 1. On MSMT17, our method outper-\nforms previous SOTA methods (e.g., TransReID) by a large\n1419\nFigure 4: Grad-Cam visualization of attention map on\nMarket-1501. (a) Input image. (b) Baseline. (c)-(d) Two\nclass tokens without SDC. (e)-(f) Two class tokens with\nSDC. As can be seen, self-diverse constraint loss makes\nmultiple class tokens to focus on different discriminative re-\ngions.\nmAP Rank-1\nT1 T2 Cat. T 1 T2 Cat.\nBaseline - - 66.1 - - 84.6\nw/o SDC 66.9 66.9 66.9 84.8 84.8 84.8\nwith SDC 67.9 68.1 68.3 85.3 85.4 85.5\nTable 2: The ablation study of SDC on MSMT17. T1 and T2\ndenote two class tokens, Cat. denotes the concatenation of\nthe two class tokens.\nmargin especially on mAP (+2.4%) with 256 √ó 128 resolu-\ntion, and also achieves the best performance with higher res-\nolution 384 √ó 128. On Market-1501, our method achieves\nthe best performance on mAP and comparable perfor-\nmance on Rank-1. On CUHK03, our method achieves abso-\nlute superiority which outperforms previous SOTA method\n(AAformer) by a large margin both on mAP (+4.2%) and\nRank-1 (+2.5%).\nAblation Study\nExperiments are conducted to study the effectiveness of self-\ndiverse constraint loss (SDC), intra-class and inter-class dis-\ntance of compressed embedding space, the hyper-parameters\nŒª and token number N, the effectiveness of dynamic weight\ncontroller (DWC), and the effectiveness of training with a\nsmaller amount of identity.\nImpact of Multiple Class Tokens and SDC. The effec-\ntiveness of multiple class tokens is validated on MSMT17\nin Table 2. The baseline has only one class token. Adding\none additional class token to the baseline (totally two class\ntokens) provides +0.8% mAP improvement on MSMT17.\nDuring training, each class token aggregates information not\nonly from patch embeddings but also from other class to-\nkens, which improves the information acquisition efficiency.\nBut in further study, we find that the similarity between these\ntwo class tokens is very high, i.e., 0.999, which implies that\nthere is no difference between them, so there is no improve-\nment after fusion. And continuing to increase the number of\ntokens cannot continue to improve the performance.\nWith SDC imposed on these two class tokens, the co-\nsine similarity of them becomes 0.007, which is very close\nto 0.0, implying that these two class tokens have learned\n(a) Baseline\n (b) Cat(T1, T2)\n(c) T1\n (d) T2\nFigure 5: The distance of positive and negative pairs in\nMSMT17 test set. The positive/negative pair denotes that\ntwo sample from Query and Gallery are the same/differ-\nent class(es). The ordinate is the number of pairs. For better\nvisualization, only partial negative pairs sampled randomly\nare shown in this figure.\nConfusion ‚Üì mAP/R1 (%) ‚Üë\nBaseline 26,469 66.1/84.6\nT1 25,738 67.9/85.3\nT2 25,064 68.1/85.4\nCat. 24,390 68.3/85.5\nTable 3: The confusion of positive and negative pairs on\nMSMT17. Confusion means the number of overlapped\npairs in Figure 5.\ntwo non-overlapping representation subspaces. Figure 3 il-\nlustrates the feature distributions of two class tokens, and\nthey are separated to each other. Also, the performance of\nthese two class tokens are both improved by a large margin\nespecially on mAP, more than +1.0% improvement, which\nmeans both tokens have learned more robust features. Af-\nter concatenation, feature fusion further improves the per-\nformance, reaching 68.3% mAP and 85.5% Rank-1, which\nis +2.2% mAP and +0.9% Rank-1 higher than the baseline.\nThe attention map visualized in Figure 4 shows that both\ntwo tokens correctly represent the foreground part of the ob-\nject. Moreover, the two tokens represent different embed-\nding spaces, and the information they represent is also dif-\nferent. Compared with the baseline, two class tokens with\nSDC has captured more fine-grained features. The fusion of\nmultiple class tokens helps the model to learn more discrim-\ninative representation.\nIntra-class and inter-class distance. DC-Former divides\nthe original embedding space into multiple compact sub-\nspaces, reducing intra-class distance but also reducing inter-\nclass distance. To verify the effectiveness of the compact\nspace proposed by DC-Former, the intra-class and inter-\nclass distance of DC-Former‚Äôs embeddings are calculated\nand visualized in Figure 5. And the confusion of positive\nand negative pairs in Figure 5 is further calculated in Table 3.\n1420\n(a) Impact of Œª\n (b) Impact of N\nFigure 6: Visualization of ablation studies on MSMT17. Im-\npact of two hyper-parameters of SDC.\n(a) w/o DWC\n (b) with DWC\n(c) mAP\n (d) Rank-1\nFigure 7: The evaluation of DWC on MSMT17. N denotes\nthe number of class tokens. (a-b) DC-Former‚Äôs SDC loss\nwhen training with/without DWC. (c-d) The mAP and Rank-\n1 of DC-Former with/without DWC.\nCompared to baseline, the embedding space of each token in\nDC-Former is smaller, as is their concatenation. Moreover,\nthe embedding space of T2 is more compact than that of T1,\nso T2 achieves higher performance. And the confusion of\nDC-Former is less than baseline, which means that compact\nembedding space pushes the embeddings of the same class\nmore tightly than the embeddings of different classes.\nHyper-parameters of SDC. SDC has two hyper-\nparameters which are the weight of lossŒª and the number of\nclass tokens N. We analyze the influence ofŒª on the perfor-\nmance in Figure 6(a). When Œª = 0, baseline achieves 66.9%\nmAP and 84.8% Rank-1 on MSMT17. When Œª = 0.1, SDC\ndoesn‚Äôt work because it‚Äôs too small. As Œª increases, the per-\nformance increases. When Œª = 1, the mAP and Rank-1 are\nimproved to 68.3% and 85.5%, respectively. When continu-\ning to increase Œª, the performance is degraded because ex-\ncessive weights make the model pull apart two features at\nthe beginning, making it difficult to optimize the classifica-\ntion loss. Therefore, Œª = 1 is the best beneficial for learning\nmultiple diverse features.\nThe experiments on the number of class tokens N is in\nFigure 6(b). Increasing the number of class tokens improves\nthe performance of model. When N is between 3 and 6,\nmAP and Rank-1 are higher. And N = 4 reaches the best\nperformance at 69.2% mAP and 86.2% Rank-1. Continuing\n(a) mAP\n (b) Rank-1\nFigure 8: Performance under training with a smaller amount\nof identity on MSMT17. Randomly select partial categories\nas training set, and keep the test set unchanged.\nto increase N, performance is degraded because finding too\nmuch embedding space makes training difficult. As we can\nsee SDC loss in Figure 7(a), too much tokens cannot opti-\nmize model well because the subspaces cannot be separated\ndue to competition between tokens. N could be a little dif-\nferent in different datasets. In the SOTA experimental con-\nfiguration (Table 1),N is [6,5,4] for MSMT17, Market-1501\nand CUHK03, respectively.\nDynamic Weight Controller. The effectiveness of the\nproposed DWC module is validated in Figure 7. The SDC\nloss in training phase illustrated in Figure 7(a-b) shows that\nDWC can make each pairs learn evenly so that they are all\northogonal to each other, which shows effectiveness on bal-\nancing the relative importance among class tokens during\ntraining as the token number increases. The performance in\nFigure 7(c-d) shows that DWC has limited effect when N\nis small (less than 5). While it provides about +1.0% im-\nprovement when N is large. When N ‚â• 7, the performance\ndecreases because the number of subspaces in a finite em-\nbedding space has reached its limit. Too small space makes\nthe embeddings lose their discrimination.\nEffectiveness with a smaller amount of identity. We\nevaluate the effectiveness of DC-Former with a smaller\namount of identity in Figure 8. DC-Former achieves com-\nparable results by using less than 20% identities of baseline.\nAnd the smaller the amount of identity, the more obvious the\nadvantages of DC-Former. Increasing the amount of identity\nstrengthens the ability of the model to identify similar iden-\ntities. And more compact embedding space of DC-Former\nalso enhances the discrimination of similar classes‚Äô repre-\nsentations, which is similar to the effect of increasing the\namount of identity.\nConclusions\nIn this paper, we propose a transformer-based network for\nReID to learn multiple diverse and compact embedding sub-\nspaces, which improves the robustness of representation by\nincreasing identity density of embedding space. And the fu-\nsion of these representations from different subspaces fur-\nther improves performance. Our method outperforms pre-\nvious state-of-the-arts on three person ReID benchmarks.\nBased on this promising results, we believe our method has\ngreat potential to be further explored in other area, and we\nhope it can bring new insights to the community.\n1421\nReferences\nCaron, M.; Touvron, H.; Misra, I.; J¬¥egou, H.; Mairal, J.; Bo-\njanowski, P.; and Joulin, A. 2021. Emerging Properties in\nSelf-Supervised Vision Transformers. In Proceedings of the\nInternational Conference on Computer Vision (ICCV).\nChen, G.; Lin, C.; Ren, L.; Lu, J.; and Zhou, J. 2019. Self-\ncritical attention learning for person re-identification. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, 9637‚Äì9646.\nChen, X.; Fan, H.; Girshick, R.; and He, K. 2020. Improved\nBaselines with Momentum Contrastive Learning. arXiv\npreprint arXiv:2003.04297.\nChen, X.; Xu, J.; Xu, J.; and Gao, S. 2021. OH-Former:\nOmni-Relational High-Order Transformer for Person Re-\nIdentification. arXiv preprint arXiv:2109.11159.\nDeng, W.; Zheng, L.; Ye, Q.; Kang, G.; Yang, Y .; and\nJiao, J. 2018. Image-image domain adaptation with pre-\nserved self-similarity and domain-dissimilarity for person\nre-identification. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 994‚Äì1003.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nFu, D.; Chen, D.; Bao, J.; Yang, H.; Yuan, L.; Zhang, L.;\nLi, H.; and Chen, D. 2021. Unsupervised pre-training for\nperson re-identification. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n14750‚Äì14759.\nGong, Y .; Zeng, Z.; Chen, L.; Luo, Y .; Weng, B.; and\nYe, F. 2021. A Person Re-identification Data Augmen-\ntation Method with Adversarial Defense Effect. CoRR,\nabs/2101.08783.\nGu, H.; Li, J.; Fu, G.; Wong, C.; Chen, X.; and Zhu,\nJ. 2022. AutoLoss-GMS: Searching Generalized Margin-\nBased Softmax Loss Function for Person Re-Identification.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 4744‚Äì4753.\nHe, K.; Fan, H.; Wu, Y .; Xie, S.; and Girshick, R. 2019.\nMomentum Contrast for Unsupervised Visual Representa-\ntion Learning. arXiv preprint arXiv:1911.05722.\nHe, L.; and Liu, W. 2020. Guided saliency feature learning\nfor person re-identification in crowded scenes. In European\nConference on Computer Vision, 357‚Äì373. Springer.\nHe, S.; Luo, H.; Wang, P.; Wang, F.; Li, H.; and Jiang,\nW. 2021. TransReID: Transformer-Based Object Re-\nIdentification. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision (ICCV), 15013‚Äì\n15022.\nHermans, A.; Beyer, L.; and Leibe, B. 2017. In defense of\nthe triplet loss for person re-identification. arXiv preprint\narXiv:1703.07737.\nHuynh, D.; and Elhamifar, E. 2020. Fine-Grained General-\nized Zero-Shot Learning via Dense Attribute-Based Atten-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition (CVPR).\nIsobe, T.; Li, D.; Tian, L.; Chen, W.; Shan, Y .; and Wang,\nS. 2021. Towards Discriminative Representation Learning\nfor Unsupervised Person Re-Identification. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion (ICCV), 8526‚Äì8536.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nagenet classification with deep convolutional neural net-\nworks. Advances in neural information processing systems,\n25.\nLai, S.; Chai, Z.; and Wei, X. 2021. Transformer Meets Part\nModel: Adaptive Part Division for Person Re-Identification.\nIn Proceedings of the IEEE/CVF International Conference\non Computer Vision (ICCV) Workshops, 4150‚Äì4157.\nLi, H.; Wu, G.; and Zheng, W.-S. 2021. Combined\nDepth Space Based Architecture Search for Person Re-\nIdentification. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n6729‚Äì6738.\nLi, W.; Zhao, R.; Xiao, T.; and Wang, X. 2014. Deep-\nReID: Deep Filter Pairing Neural Network for Person Re-\nidentification. In CVPR.\nLi, Y .; He, J.; Zhang, T.; Liu, X.; Zhang, Y .; and Wu,\nF. 2021. Diverse Part Discovery: Occluded Person Re-\nIdentification With Part-Aware Transformer. InProceedings\nof the IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition (CVPR), 2898‚Äì2907.\nLin, J.; Ren, L.; Lu, J.; Feng, J.; and Zhou, J. 2017.\nConsistent-aware deep learning for person re-identification\nin a camera network. In Proceedings of the IEEE conference\non computer vision and pattern recognition, 5771‚Äì5780.\nLin, Y .; Zheng, L.; Zheng, Z.; Wu, Y .; Hu, Z.; Yan, C.; and\nYang, Y . 2019. Improving person re-identification by at-\ntribute and identity learning. Pattern Recognition, 95: 151‚Äì\n161.\nLiu, H.; Feng, J.; Qi, M.; Jiang, J.; and Yan, S. 2017.\nEnd-to-end comparative attention networks for person re-\nidentification. IEEE Transactions on Image Processing,\n26(7): 3492‚Äì3506.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin,\nS.; and Guo, B. 2021. Swin transformer: Hierarchical vi-\nsion transformer using shifted windows. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vi-\nsion, 10012‚Äì10022.\nLuo, H.; Gu, Y .; Liao, X.; Lai, S.; and Jiang, W. 2019.\nBag of Tricks and a Strong Baseline for Deep Person Re-\nIdentification. In The IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) Workshops.\nLuo, H.; Wang, P.; Xu, Y .; Ding, F.; Zhou, Y .; Wang, F.;\nLi, H.; and Jin, R. 2021. Self-Supervised Pre-Training for\nTransformer-Based Person Re-Identification. arXiv preprint\narXiv:2111.12084.\nSelvaraju, R. R.; Cogswell, M.; Das, A.; Vedantam, R.;\nParikh, D.; and Batra, D. 2017. Grad-cam: Visual explana-\ntions from deep networks via gradient-based localization. In\nProceedings of the IEEE international conference on com-\nputer vision, 618‚Äì626.\n1422\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2014. Dropout: a simple way to prevent\nneural networks from overfitting. The journal of machine\nlearning research, 15(1): 1929‚Äì1958.\nSuh, Y .; Wang, J.; Tang, S.; Mei, T.; and Lee, K. M.\n2018. Part-aligned bilinear representations for person re-\nidentification. In Proceedings of the European conference\non computer vision (ECCV), 402‚Äì419.\nSun, Y .; Cheng, C.; Zhang, Y .; Zhang, C.; Zheng, L.; Wang,\nZ.; and Wei, Y . 2020. Circle loss: A unified perspec-\ntive of pair similarity optimization. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 6398‚Äì6407.\nSun, Y .; Zheng, L.; Yang, Y .; Tian, Q.; and Wang, S. 2018.\nBeyond part models: Person retrieval with refined part pool-\ning (and a strong convolutional baseline). In Proceedings of\nthe European conference on computer vision (ECCV), 480‚Äì\n496.\nTan, L.; Dai, P.; Ji, R.; and Wu, Y . 2022. Dynamic Prototype\nMask for Occluded Person Re-Identification. arXiv preprint\narXiv:2207.09046.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J¬¥egou, H. 2021. Training data-efficient image trans-\nformers & distillation through attention. In International\nConference on Machine Learning, 10347‚Äì10357. PMLR.\nWang, G.; Yuan, Y .; Chen, X.; Li, J.; and Zhou, X. 2018.\nLearning discriminative features with multiple granularities\nfor person re-identification. In Proceedings of the 26th ACM\ninternational conference on Multimedia, 274‚Äì282.\nWang, T.; Liu, H.; Song, P.; Guo, T.; and Shi, W. 2022.\nPose-guided Feature Disentangling for Occluded Person Re-\nidentification Based on Transformer. AAAI.\nWei, L.; Zhang, S.; Gao, W.; and Tian, Q. 2018. Per-\nson transfer gan to bridge domain gap for person re-\nidentification. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, 79‚Äì88.\nXu, F.; Wang, M.; Zhang, W.; Cheng, Y .; and Chu, W.\n2021. Discrimination-Aware Mechanism for Fine-Grained\nRepresentation Learning. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 813‚Äì822.\nYan, C.; Pang, G.; Jiao, J.; Bai, X.; Feng, X.; and Shen, C.\n2021. Occluded person re-identification with single-scale\nglobal representations. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, 11875‚Äì11884.\nYang, J.; Zhang, J.; Yu, F.; Jiang, X.; Zhang, M.; Sun,\nX.; Chen, Y .-C.; and Zheng, W.-S. 2021. Learning To\nKnow Where To See: A Visibility-Aware Approach for Oc-\ncluded Person Re-Identification. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision ,\n11885‚Äì11894.\nZhang, G.; Zhang, P.; Qi, J.; and Lu, H. 2021. HAT:\nHierarchical Aggregation Transformers for Person Re-\nIdentification, 516‚Äì525. New York, NY , USA: Association\nfor Computing Machinery. ISBN 9781450386517.\nZhao, J.; Qi, F.; Ren, G.; and Xu, L. 2021. PhD Learn-\ning: Learning with Pompeiu-hausdorff Distances for Video-\nbased Vehicle Re-Identification. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2225‚Äì2235.\nZheng, L.; Shen, L.; Tian, L.; Wang, S.; Wang, J.; and Tian,\nQ. 2015. Scalable Person Re-identification: A Benchmark.\nIn Computer Vision, IEEE International Conference on.\nZheng, L.; Zhang, H.; Sun, S.; Chandraker, M.; Yang, Y .;\nand Tian, Q. 2017. Person re-identification in the wild. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, 1367‚Äì1376.\nZheng, Z.; Zheng, L.; and Yang, Y . 2017. A discrimina-\ntively learned cnn embedding for person reidentification.\nACM transactions on multimedia computing, communica-\ntions, and applications (TOMM), 14(1): 1‚Äì20.\nZhong, Z.; Zheng, L.; Kang, G.; Li, S.; and Yang, Y . 2020.\nRandom erasing data augmentation. In Proceedings of\nthe AAAI conference on artificial intelligence, volume 34,\n13001‚Äì13008.\nZhu, K.; Guo, H.; Zhang, S.; Wang, Y .; Huang, G.; Qiao,\nH.; Liu, J.; Wang, J.; and Tang, M. 2021. Aaformer:\nAuto-aligned transformer for person re-identification. arXiv\npreprint arXiv:2104.00921.\nZhu, Z.; Jiang, X.; Zheng, F.; Guo, X.; Huang, F.; Sun, X.;\nand Zheng, W. 2020. Aware loss with angular regularization\nfor person re-identification. In Proceedings of the AAAI con-\nference on artificial intelligence, volume 34, 13114‚Äì13121.\n1423"
}