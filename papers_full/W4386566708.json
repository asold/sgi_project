{
  "title": "Multi-Modal Bias: Introducing a Framework for Stereotypical Bias Assessment beyond Gender and Race in Vision–Language Models",
  "url": "https://openalex.org/W4386566708",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2569342686",
      "name": "Sepehr Janghorbani",
      "affiliations": [
        "Rutgers, The State University of New Jersey"
      ]
    },
    {
      "id": "https://openalex.org/A2134233121",
      "name": "Gerard de Melo",
      "affiliations": [
        "University of Potsdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4287121067",
    "https://openalex.org/W2152499249",
    "https://openalex.org/W2145073540",
    "https://openalex.org/W83732997",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W4404752324",
    "https://openalex.org/W4312983864",
    "https://openalex.org/W4245789282",
    "https://openalex.org/W3199396412",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3154222058",
    "https://openalex.org/W4225667278",
    "https://openalex.org/W3185212449",
    "https://openalex.org/W3095351420",
    "https://openalex.org/W2140534852",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2963612262",
    "https://openalex.org/W3190434222",
    "https://openalex.org/W2119750321",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W2990751682",
    "https://openalex.org/W3170344956",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W4288007632",
    "https://openalex.org/W3172141633",
    "https://openalex.org/W1988783896",
    "https://openalex.org/W2954275542",
    "https://openalex.org/W4241220173",
    "https://openalex.org/W2145574511",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W2018811141",
    "https://openalex.org/W3035296331",
    "https://openalex.org/W3126792443"
  ],
  "abstract": "Recent breakthroughs in self-supervised training have led to a new class of pretrained vision–language models. While there have been investigations of bias in multimodal models, they have mostly focused on gender and racial bias, giving much less attention to other relevant groups, such as minorities with regard to religion, nationality, sexual orientation, or disabilities. This is mainly due to lack of suitable benchmarks for such groups. We seek to address this gap by providing a visual and textual bias benchmark called MMBias, consisting of around 3,800 images and phrases covering 14 population subgroups. We utilize this dataset to assess bias in several prominent self-supervised multimodal models, including CLIP, ALBEF, and ViLT. Our results show that these models demonstrate meaningful bias favoring certain groups. Finally, we introduce a debiasing method designed specifically for such large pretrained models that can be applied as a post-processing step to mitigate bias, while preserving the remaining accuracy of the model.",
  "full_text": "Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1725–1735\nMay 2-6, 2023 ©2023 Association for Computational Linguistics\nMulti-Modal Bias: Introducing a Framework for Stereotypical Bias\nAssessment beyond Gender and Race in Vision–Language Models\nSepehr Janghorbani\nRutgers University\nNew Brunswick, NJ, USA\nsepehr.janghorbani@rutgers.edu\nGerard de Melo\nHPI / University of Potsdam\nPotsdam, Germany\ngdm@demelo.org\nAbstract\nRecent breakthroughs in self-supervised train-\ning have led to a new class of pretrained vi-\nsion–language models. While there have been\ninvestigations of bias in multimodal models,\nthey have mostly focused on gender and racial\nbias, giving much less attention to other rel-\nevant groups, such as minorities with regard\nto religion, nationality, sexual orientation, or\ndisabilities. This is mainly due to lack of suit-\nable benchmarks for such groups. We seek\nto address this gap by providing a visual and\ntextual bias benchmark called MMBias, con-\nsisting of around 3,800 images and phrases cov-\nering 14 population subgroups. We utilize this\ndataset to assess bias in several prominent self-\nsupervised multimodal models, including CLIP,\nALBEF, and ViLT. Our results show that these\nmodels demonstrate meaningful bias favoring\ncertain groups. Finally, we introduce a debi-\nasing method designed specifically for such\nlarge pretrained models that can be applied as a\npost-processing step to mitigate bias, while pre-\nserving the remaining accuracy of the model.\n1 Introduction\nThe recent emergence of large pretrained vision–\nlanguage models has revolutionized many multi-\nmodal tasks previously considered impractical to\nsolve. Although architectures capable of jointly\naddressing computer vision and NLP tasks using a\nsingle unified model have been around for a while\n(Lu et al., 2019; Tan and Bansal, 2019; Li et al.,\n2019), recent advances in self-supervised training\nmethods have amplified the significance and appli-\ncability of such models. The sheer power of these\nmethods is highly dependent on the scale of the\nmodel and the diversity and distributional proper-\nties of the dataset on which they are trained. Due\nto their wide range of diverse applications (Eslami\net al., 2021), it is of utmost importance to be aware\nof the shortcomings of vision–language pretrained\n(VLP) models as well as their capabilities.\nOne such limitation is that, like any other ma-\nchine learning system, multimodal models may\nbe prone to exhibiting human-like stereotypical\nbiases such as gender or race-related stereotypes\n(Nadeem et al., 2020; Garrido-Muñoz et al., 2021).\nFor instance, pretrained language models have been\nshown to associate male-gendered phrases and sen-\ntences to a greater extent with certain high-paying\nprofessions and even with individual traits such\nas intelligence, in comparison to female-gendered\nphrases (Wang et al., 2021a). Similarly, it has been\nfound that Hispanic and African American names\nmay be tied to words representing danger and crime\nmore often than Caucasian names (Manzini et al.,\n2019). Certain biases have also been identified in\ncomputer vision models as well (Wang et al., 2019).\nSuch biases are discriminatory towards affected\npopulation groups and can be extremely harmful\nto society the more these models are deployed in\nreal-world applications.\nWhile there has been some research aimed\nat identifying and addressing biases in vision–\nlanguage models, most such studies have focused\non gender and racial biases, while other notable\ngroups such as religious minorities, national mi-\nnorities, LGBTQ people, and people with disabili-\nties have received much less attention, despite their\nlegal status as protected groups in the US. This is\nalarming considering the fact that the potentially\naffected groups together constitute a considerable\npart of the global population. For instance, the US\nCensus Bureau reported approximately 40 million\npeople identifying as immigrants in the US and 244\nmillion world-wide as of 2015.1 Furthermore, ap-\nproximately 40 million people in the US and about\n1 billion people in the world suffer from some sort\nof disability.2 One of the main obstacles for bias\nanalysis of these relevant population groups has\nbeen the lack of standardized benchmark datasets\n1www.un.org/en/development/desa/population/migration\n2www.worldbank.org/en/topic/disability\n1725\nFigure 1: Experiment pipeline. We feed target and attribute data to the model. The embeddings obtained from\npretrained encoders are then used to measure bias metrics between visual and textual stimuli following Eq. 1.\nthat specifically enable an analysis of how they\nmay be affected. In this paper, we attempt to ad-\ndress this problem by gathering and releasing a\nvisual and textual bias benchmark called MMBias,\nconsisting of approximately 3,500 images and 350\nphrases covering over 14 minority subgroups. Fur-\nthermore, we utilize the dataset to measure stereo-\ntypical bias in several prominent self-supervised\nmultimodal VLP models that have attracted sig-\nnificant attention recently, namely OpenAI CLIP\n(Radford et al., 2021), ALBEF (Li et al., 2021), and\nViLT (Kim et al., 2021). In our experiments, we\nquantify the bias present in these models, including\nboth cross-modal and intra-modal bias. Our results\nconfirm that these models harbor meaningful bi-\nases favoring certain groups. Finally, we introduce\na novel debiasing method designed for such large\npretrained models that can be applied as a post-\nprocessing step to mitigate bias, and we show that\nthis step does not adversely affect the performance\nin a substantial way.\n2 Related Work\nThe majority of work on language models only fo-\ncuses on gender and racial bias assessment (Guo\nand Caliskan, 2021; Bordia and Bowman, 2019).\nHowever, there have been some studies that con-\nsider bias with regard to other categories such as\nprofession, religion, and disability as well (Nadeem\net al., 2020; Hutchinson et al., 2020). However,\nthese forms of bias are not just exclusive to the lan-\nguage domain, and image classifiers as well as mul-\ntimodal models have also been shown to demon-\nstrate such biases (Srinivasan and Bisk, 2021; Ross\net al., 2020).\nThus far, there has been rather limited work\non multimodal bias assessment of self-supervised\nmodels such as CLIP, and prior work considers\nonly gender and racial biases. Wang et al. (2021b)\nmeasures the gender and racial bias in CLIP’s im-\nage classification module using the Fairface dataset,\nwhile Wang et al. (2021a) further show that CLIP\nassociates male-gendered phrases to high-paying\nprofessions more than female-gendered phrases.\nAgarwal et al. (2021) provide insights towards po-\ntential applications of the CLIP model and further\nstudy and evaluate its gender/racial bias as well\nas measuring the misclassification differences be-\ntween different subgroups. Bhargava and Forsyth\n(2019) measure and propose solutions to gender\nbias in several image captioning systems.\nThe only work that addresses other relevant\ngroups such as religion, sexual orientation, and\ndisability in the image space is Steed and Caliskan\n(2021). However, the data considered in the study\nis limited, consisting of only around 600 images,\nout of which around 500 again correspond to gen-\nder and racial biases. This leaves only around 100\nfor other protected groups, i.e., fewer than 20 im-\nages for each protected group study. Sirotkin et al.\n(2022) use this limited dataset to measure bias in\nseveral self-supervised visual models but the au-\nthors do not explore multimodal models such as\nCLIP. An orthogonal line of research has been pur-\nsued in Zhou et al. (2022), where several multi-\nmodal vision-language models are analyzed to mea-\nsure these models’ tendency to pick stereotypical\nstatements as captions for anti-stereotypical images\nin pre-trained vision-language models. With MM-\nBias, we thus hope to enable further research on\ndiverse forms of bias in vision–language models.\n1726\nFigure 2: Sample images from the MMBias dataset. Each row corresponds to one of the target classes: religion,\nnationality, disability, and sexual orientation. Images are compiled from the image sharing service Flickr.\n3 Methodology\nBias and Fairness. In conventional social stud-\nies, one of the most well-established and widely-\ninvestigated forms of biases is what is known as\nimplicit bias, or as social stereotypes, defined and\ninvestigated in Greenwald and Banaji (1995). This\ntype of bias is usually measured using Implicit\nAssociate Testing (IAT), introduced in the semi-\nnal work of (Greenwald et al., 1998), and has so\nfar been widely used to describe and account for\na wide range of implicit prejudices (Kiefer and\nSekaquaptewa, 2007). IAT experiments quantify\nhuman implicit bias by measuring response times\ndifferences when human subjects are asked to pair\nsimilar concepts and different concepts. In its orig-\ninal form, IAT was used to to measure the degree\nof pleasantness (a.k.a. valence in psychology), of\nentities such as “flowers” and “insects” by pairing\nthem with abstract attributes such as pleasant and\nunpleasant (Russell, 2003). Caliskan et al. (2017)\nshowed that a similar IAT testing paradigm can be\napplied to bias measurement in deep embeddings.\nIn this approach, instead of subject reaction time,\nthe proximity of embeddings of a basket of words\nthat represent a concept is measured. Furthermore,\nword sentiment is usually used to represent valence,\ndue to well-established studies linking word sen-\ntiment with the psychological concept of valence\n(Mohammad, 2016). The experimental methodol-\nogy used in our study follows similar principles.\nMore generally, a machine learning system may\nbe deemed unbiased or fair when its predictions\ndo not favor members of any relevant population\ngroup or discriminate against any other (Garrido-\nMuñoz et al., 2021). For instance, suppose that\nthe class under consideration is religion and we are\nevaluating pleasantness / unpleasantness scores a\nsystem would assign to each considered religious\nsubgroup. A machine learning system is assessed\nas fair if and only if the scores it assigns to different\nreligious subgroups do not differ substantially.\nMore formally, in a bias study, the two sub-\ngroups under study, also known as target entities,\nmay be represented as sets of instances X =\n{x1,x2,...,x N}and Y = {y1,y2,...,y N}. For\nexample, X may be images corresponding to Is-\nlam and Y to Christianity. Furthermore, the at-\ntributes towards which the bias is being measured\nmay be given as sets A = {a1,a2,...,a M}and\nB = {b1,b2,...,b M}. For example, A could be\na set of words representing pleasantness, while\nB represents unpleasantness. Similarly, many\ngender-bias studies consider sets for high paying\nvs. low paying professions as attribute sets. A ma-\nchine learning model is then said to be fair towards\nsubgroups X and Y with respect to attributes A\nand B if and only if ϕ(X,A,B) ≈ϕ(Y ,A,B),\nwhere ϕis some scoring function that scores the\nsimilarity of the sets of attributes A, B to a target\nentity X or Y .\nScoring Functions. There can be different\nchoices for the scoring function ϕabove. Caliskan\net al. (2017) introduced the Caliskan test shown\nbelow in Eq. 1, with ϕcapturing the difference of\nthe mean of cosine distances between targets and\nattributes. This method is ideal for the analysis of\nmodels such as CLIP, since they operate directly\non entity embeddings. The effect size represented\nby d(X,Y,A,B ) is a measure of the magnitude\nof the bias. Larger numbers indicate a stronger\nbias, while the sign reflects which target entity the\nattributes show a stronger bias towards.\nHowever, for vision–language fusion models\nthat do not provide explicit access to separate im-\n1727\nage/text embeddings, an alternative scoring func-\ntion can be defined as the difference in the image–\ntext matching probabilities, as in the last row of\nEq. 1. Sets X and Y as well as A and B are usu-\nally constructed to have equal number of samples.\nd=\nmean\nx∈X\nϕ(x,A,B ) −mean\ny∈Y\nϕ(y,A,B )\nstd-dev\nw∈X∪Y\nϕ(w,A,B ) (1)\nϕ(w,A,B ) = mean\na∈A\ncos(w,a) −mean\nb∈B\ncos(w,b)\nϕ(w,A,B ) = mean\na∈A\nσ(w,a) −mean\nb∈B\nσ(w,b)\nHere, cos(·,·) denotes the cosine similarity of vec-\ntors, while σ(·,·) denotes the probability of a text\nand image pair being a match.\nEvaluation Pipeline Fig. 1 shows the pipeline\nfollowed in our experiments. The target and at-\ntribute stimuli are fed into the model and embed-\ndings it emits are used to compute the bias score.\n4 The MMBias Dataset\nThe majority of the work on societal bias analy-\nsis so far focuses on unimodal language models.\nAlthough there has been some limited work on\nmultimodal models, these studies mainly focus on\ngender and racial disparities. As a result, bias with\nregard to other classes, including religion, national-\nity, sexual orientation, and disability have largely\nbeen unexplored. This has been mainly due to the\nlack of standardized benchmark datasets that specif-\nically target these minority groups. To address this\nconcern, we gather and release the first multimodal\ndataset of this size in this line of research that spans\nover a wider range of groups. We hope that this\ndataset can serve as a benchmark in future research.\nOur dataset, referred to as MMBias, contains\n3,500 target images and 350 English phrases corre-\nsponding to different target concepts. Each target\nTarget ConceptX Target Values{x1,...,xN}\nReligion Islam, Christianity, Judaism,\nBuddhism, Hinduism\nNationality American, Arab,\nChinese, Mexican\nDisability Physical disability,\nMental disability, No disability\nSexual Orientation Homosexual, Heterosexual\nTable 1: MMBias spans over 4 target classes and 14\ntarget groups including 5 major religions, 4 nationalities,\n2 forms of disability and sexual orientations.\nFigure 3: Gaussian distribution of the image sizes scaled\nby a factor of 1,000. Most images are sized around\n340x340 pixels.\ncategory has 250 corresponding images obtained\nfrom the popular image uploading website Flickr.\nOur dataset also contains 20 textual phrases related\nto each target concept, used for bias experiments in\nthe textual domain.\nData Compilation For gathering the image data,\nwe invoke the Flickr API and retrieve 1,000 most\nrelevant images for each target concept using 10–\n12 search keywords for each. The keywords are\nchosen to be as diverse as possible to minimize\nany potential bias in the data gathering process to\nthe extent possible. Then human annotators are\nused to filter out noisy images. Annotators were in-\nstructed to manually eliminate irrelevant or explicit\ncontent as well as images that contained private in-\nformation or names/addresses. In order to balance\nthe dataset, 250 images were randomly chosen for\neach concept out of the filtered images, and we only\nconsider images with a Creative Commons license.\nThe processing pipeline and quality control is sim-\nilar to the one used for the creation of Flickr30k\n(Young et al., 2014). Furthermore, the textual part\nof the dataset contains phrases such as “This is X.”\nreplacing X with a “Muslim person\", “Christian\nperson\", etc. The same aforementioned keywords\nwere used to retrieve textual data for each concept\nusing the RelatedWords site3 followed by a similar\ndata cleaning and noise filtering process.\nTable 1 shows the classes MMBias covers as\nwell as the considered groups in each class. MM-\nBias spans over 4 target classes, including religion,\nnational origin, disability, and sexual orientation.\nIn this study, we did not include gender and race,\nas there is already a large body of work focusing\non them. For religion, our dataset includes the 5\n3relatedwords.org/\n1728\nmajor religions in the world today: Islam, Chris-\ntianity, Judaism, Buddhism, and Hinduism. As\nfor the national origin, MMBias includes images\ncorresponding to the four nationalities: American\n(USA), Chinese, Arab4 (collectively), and Mexican.\nFurthermore, another two additional nationalities,\nFrench and Italian, are also included in the textual\nphrases in addition to the former. As for disability,\nMMBias contains images for two common forms\nof disability: physical disability, mental disability\nas well as people with no disability. In addition\nto these, the textual data includes phrases corre-\nsponding to visual disability and hearing disability\nas well. Finally, the two most common types of\nsexual orientations, homosexuality and heterosex-\nuality, are included in MMBias. The selection of\nsubgroups as well as their pairings was a result of\nconsulting several social studies that show present\nbias against people with disabilities (Dovidio et al.,\n2011), homosexuals (Hebl et al., 2002), certain na-\ntionalities (Park et al., 2007; Buriel and Vasquez,\n1982) and religions (Abid et al., 2021; Rowatt et al.,\n2005; Rudman and Ashmore, 2007). However, we\nplan to extend our data to a larger pool of classes\nand respective subgroups in the future.\nFinally, in order to conduct intra-visual bias stud-\nies, MMBias also contains two sets of images cor-\nresponding to visual pleasantness and unpleasant-\nness, called the valence dataset. These sets were\nconstructed by following a similar method to Steed\nand Caliskan (2021), retrieving images correspond-\ning to pleasant concepts such wealth, peace, babies,\nlove, butterflies, etc. and unpleasant concepts such\nas death, injury, prison, fear, etc.\nAnalysis. Fig. 2 provides some sample images\ntaken from the dataset. Each row shows a different\ntarget class. Fig. 3 provides deeper insights into the\nsizes of the crawled images. The x-axis reflects the\nsurface area of the image in pixels, scaled by a fac-\ntor of 1,000. As can be seen, image sizes in most\nclasses follow a normal distribution with a mean\nof around 110,000 pixels, translating to approxi-\nmately 340x340 images, with the exception of im-\nages corresponding to the nationality class, which\nhave a slightly higher mean of around 350x350.\nThe height and width of images does not vary sub-\nstantially across the dataset.\n4Arab collectively refers to a number of Arab countries\n(each also having other cultural groups). We hope that more\nspecific nationalities and cultural groups can be added in the\nfuture.\nFigure 4: t-SNE representation of image embeddings.\nLeft shows embedding clusters before bias mitigation.\nRight shows embedding clusters after bias mitigation.\nBoth cases show well-separated clusters, suggesting bias\nmitigation has negligible effects on cluster separability.\nFurthermore, we analyze the separability of our\ndataset with regards to image classes. The images\nare fed into the CLIP model and the first 100 prin-\ncipal components are extracted from the resulting\nembeddings, and then t-SNE is applied. Fig. 4\nshows the t-SNE representation of the images. We\nobserve that the dataset can be well-separated form-\ning clearly-defined clusters. For instance, we notice\nthat different religions form well-separated clusters.\nInterestingly the clusters that are more intertwined\ncorrespond to correlated subjects such as the reli-\ngion Islam and the Arab nationality designation.\nThis is not surprising given Islam is particularly\nprevalent in Arab countries and thus many of the\nimages share similar features.\n5 Experimental Evaluation\nWe have conducted three sets of experiments to\nassess and quantify the bias in the aforementioned\nmodels: CLIP, ALBEF, and ViLT. The following\nsections explain the details of each setting.\n5.1 OpenAI CLIP\nCLIP is a multimodal vision–language embedding\nmodel originally devised for zero-shot classifica-\ntion of images. It utilizes a self-supervised con-\ntrastive loss to learn a joint embedding space for\nboth images and text. The model is jointly trained\non the WebImageText dataset, a set of 400 million\npaired image–text pairs crawled form the web. Al-\nthough primarily designed for image classification,\nCLIP embeddings have been used in numerous\nother downstream applications, making it a prime\ncandidate for our analysis. The architecture of\nCLIP has independent visual and textual encoding\nmodules providing explicit access to each modal-\nity’s embedding. As a result, it is possible to not\n1729\nFigure 5: Top 15 closest attributes returned by CLIP for each target group. Red colors represent negative sentiment\nwhile blue represents positive sentiment. Stereotypical patterns can be seen among different groups.\nonly analyze the bias across domains but conduct\nablation studies for each module separately as well.\nWe used the “ViT-B/32” model with the official\nCLIP code. Our experiments are as follows.\n5.1.1 Cross-Modal Zero-Shot Classification\nCLIP was originally introduced as a means for zero-\nshot image classification. In this experiment, we\nmeasure bias for this task across different modal-\nities. Given a set of target concept images XI\nand Y I, and a set of textual attribute phrases AT\nand BT, we use CLIP to perform zero-shot clas-\nsification of target images to attribute words. For\neach image group, the top 15 classified words are\nreturned. The attributes are two sets of 60 words\nconveying positive5 or negative6 sentiment, many\nof which were also included in the original IAT\nstudies (Bellezza et al., 1986).\nThe resulting correlation scores are provided in\nFig. 5. Each row shows the top 15 words returned\nby the model for each of the target classes. Words\nwith positive sentiment are blue-colored while ad-\njectives with negative sentiment are given in red.\nThe number inside each bar as well as the color\nintensity represent the degree to which the model\nassociates that target class with that word. Fig. 5\nshows stereotypical patterns emerging, e.g., the\n5ptrckprry.com/course/ssd/data/positive-words.txt\n6ptrckprry.com/course/ssd/data/negative-words.txt\nmost associated attributes to Islam and Judaism are\nwords related to poverty, terror, and extremism such\nas: “impoverished”, “vagrant”, ”terrorist“, ”oppres-\nsion“, “outcast”, “extremist”, etc., which carry a\nhighly negative sentiment.\nHowever, unlike Islam and Judaism, in the case\nof Buddhism and Christianity, 12 of the 15 top\nattributes have positive sentiment. The most asso-\nciated attributes are words resembling peace and\nhappiness such as: “peace”, “blessing”, “compas-\nsionate”, “admirable”, etc., carrying a highly posi-\ntive sentiment. This is aligned with societal stereo-\ntypes that certain religions are looked upon less\nfavorably than others. Similar patterns can be ob-\nserved for other classes such as nationality as well.\nCertain nationalities such as Americans are viewed\nas more favorably by the model compared to Arab,\nMexican, and Chinese people. Interestingly, biases\nagainst the Arab category are very similar to biases\nagainst Islam, e.g., both obtaining high scores for\n“terrorism”, “extremist”, and “impoverished”. This\nlikely stems from the the fact that most Arab coun-\ntries are predominantly Muslim and the model may\nhave acquired latent correlations among the two.\nFor the target class “Mexican”, the highest-scoring\nwords are “undocumented”, “greed”, and “illegal”,\nfollowed by “impoverished”, which reflects the\ntypical right-wing media portrayal of this group in\nthe US. Similarly, Chinese nationals are associated\n1730\nwith negative attributes relating to poverty and dic-\ntatorship. The next category that exhibits a bias is\nsexual orientation, where the LGBTQ community\nis mostly associated with words such as “offend-\ning”,“vulgar”, “hateful”, “perverse”, etc. Finally,\nwe can see the large negative sentiment the model\nexhibits towards people with disabilities.\n5.1.2 Cross-Modal Bias Assessment\nThis experiment quantifies the bias in CLIP using\nCaliskan cosine similarity metric in Eq. 1. Given a\nset of target concept images XI and Y I and a set\nof textual attribute phrases AT and BT , the goal\nis to measure the effect size,d(XI,Y I,AT ,BT ),\ndistance between image target concepts XI, Y I\nand textual pleasantness attributes. The results are\nprovided in the first column of Table 2. Positive\nnumbers reflect a negative bias towards the first\ntarget X, while negative numbers indicate a posi-\ntive bias towards X. The magnitude represents the\nintensity to which the bias is present in the model\nwith regard to test data. The results in Table 2 are\nconsistent with the results in the zero-shot clas-\nsification experiment, confirming certain societal\nstereotypes. For instance, in the case of religion,\nwe have already observed that Islam and Judaism\nare tied to negative words much more frequently,\ncompared to Christianity and Buddhism. Similarly,\nhere, we observe that bias scores for “Islam vs.\nChristianity” and “Judaism vs. Christianity” are\nfairly high as well. In the case of “Islam vs. Ju-\ndaism”, Islam is viewed more unfavorably, reflect-\ning the surge of Islamophobic tendencies in recent\ndecades. In this regard, the most favorably assessed\nreligions are Christianity and Buddhism. Similar\ntrends can be seen when considering nationality\nas well. The model shows a negative bias towards\nArab, Chinese, and Mexican people compared to\nAmericans. This is again consistent with previous\nobservations in the zero-shot classification experi-\nment. Finally, we find that people with disabilities\nas well as the LGBTQ community are viewed more\nnegatively.\n5.1.3 Ablation: Intra-Modal Encoder Bias\nSince CLIP provides explicit access to textual\nand visual embeddings, we can run ablation\nstudies by measuring the bias in each module\nindependently. In order to do so, we mea-\nsure the effect size using the Caliskan formula\nd(XT ,Y T ,AT ,BT ) for textual data and Image\nAssociation test d(XI,Y I,AI,BI) for images,\nTarget Target CLIP CLIP CLIP ALBEFViLT\nX Y CrossTextualVisual\nReligion\nMuslimChristian1.72 1.48 1.61 0.37 0.45\nJewish Christian1.69 1.24 1.43 0.34 0.51\nMuslim Jewish 0.47 0.41 0.75 0.03 -0.04\nMuslimBuddhist1.62 0.69 1.53 0.23 0.26\nChristianBuddhist-0.75 -0.99 -0.35 -0.14 -0.21\nHindu Buddhist-0.52 0.06 -0.11 0.01 0.01\nJewish Buddhist1.61 0.31 1.28 0.20 0.30\nMuslim Hindu 1.65 0.64 1.49 0.24 0.25\nNationality\nArab American1.28 1.79 1.56 0.11 -0.03\nArab French – 1.79 – – –\nArab Italian – 1.25 – – –\nMexican Arab -0.32 0.24 -0.92 -0.04 0.06\nChineseAmerican0.89 1.30 1.20 0.03 -0.07\nMexicanAmerican1.13 1.75 1.20 0.07 0.03\nDisability\nVisual Abled – 1.25 – – –\nHearing Abled – 1.13 – – –\nMental Abled 1.48 1.04 1.05 0.37 0.13\nPhysicalAbled 1.25 1.03 1.35 0.02 -0.01\nLGBTQLGBTQHetero. 1.67 0.93 1.46 0.07 0.10\nTable 2: Bias assessment for CLIP, ALBEF, and ViLT\nmodels. CLIP-Cross has numbers for cross-modal bias\nassessment experiment, while CLIP-Textual and CLIP-\nVisual show effect sizes for intra-modality ablation stud-\nies. Positive numbers favor target Y while negative\nnumbers favor target X.\nwhere unlike the cross-modal experiment, both tar-\nget concepts as well as attributes have the same\nmodality. These experiments can provide insights\nas to which module is more heavily responsible\nfor the observed bias. Columns 2 and 3 in Ta-\nble 2 present the findings. For the image modal-\nity, we have images with positive and negative va-\nlence, analogous to positive and negative-sentiment\nwords. As we can see, the model demonstrates sim-\nilar bias to the cross-modal case. Similarly, we\nnotice that “Islam” and “Judaism” attract more neg-\native bias in comparison with “Christianity” and\n“Buddhism”. In some cases the effect size is slightly\ndifferent, which can be explained by the fact that\nthe number of samples in the case of textual data\nis smaller, entailing a greater standard deviation,\nwhich in turn alters the effect size.\n5.2 Fusion-based Models\nWe next evaluate two fusion-based models. Al-\nthough these models typically have independent\ntextual and visual encoding modules in their lower\nlayers, their architecture is complemented by a fu-\nsion module in higher levels to combine the infor-\nmation in different modalities, enabling them to\nlearn joint embeddings of the visual and textual\ndomains. This has been shown to be essential for\nmore complex tasks such as VQA and NLVR that\nrequire more complex reasoning. The first such\n1731\nmodel we consider is ALBEF. Similar to CLIP, AL-\nBEF (Li et al., 2021) first learns separate visual and\ntextual embeddings using Transformer-based im-\nage and text encoders coupled with contrastive loss.\nHowever, unlike CLIP, ALBEF further combines\nthese embeddings by adopting an attention-based\nfusion architecture to model more complex interac-\ntions between these modalities, and directly aims\nto address several vision–language objectives, in-\ncluding image–text matching and masked language\nmodeling. This model is pretrained on conceptual\ncaptions and SBU captions (Sharma et al., 2018;\nOrdonez et al., 2011). Furthermore, the model is\ntrained using momentum distillation to facilitate\nlearning by adding an auxiliary learning network\nto stabilize the leaning process.\nViLT (Kim et al., 2021) is another recent VLP\nmodel that is devised as a more computationally\nefficient alternative to CLIP and ALBEF. Unlike\nlarge and computationally-heavy image and text\nencoders in CLIP and ALBEF, ViLT utilizes only\nshallow linear layers to process the sequence of\nword embeddings and image patches of the text–\nimage input pair. Furthermore, in order to enable\nthe model to solve complex vision–language tasks\nsuch as VQA, NLVR, and ITM, a Transformer-\nbased architecture is employed on top to capture the\ncomplex dynamics between the modalities. This\nmodel is trained using a combination of image–\ntext matching, word patch alignment, and masked\nlanguage modelling objectives.\nWith regard to bias assessment, unlike CLIP,\nfusion-based models do not provide explicit access\nto separate visual and textual embeddings but rather\nprovide a combined embedding of the pair. As a\nresult, computing the Caliskan distance in Eq. 1\nis not possible. However, interestingly one of the\nobjectives these models optimize for is the image–\ntext matching (ITM) objective. ITM is the problem\nof estimating the probability that a given image–\ntext pair is a match. This task is directly related to\nour bias evaluation problem. We can argue that a\nmodel is fair if the probability of assigning pleas-\nantness scores is similar across different concepts.\nIn other words, the following should hold for the\nITM scores:\nPITM(A|X) −PITM(B|X) ≈\nPITM(A|Y ) −PITM(B|Y )\nColumns 4 and 5 in Table 2 include the results\nfor ALBEF and ViLT. The numbers provided are\nprobabilistic differences and are not comparable to\nthe Caliskan scores provided for CLIP. In order to\nreduce irrelevant noise only the top 15 most signif-\nicant matches are considered. Again, we see that\nthese models exhibit strong biases favoring Chris-\ntianity vs. Islam and Judaism, matching Christian\nimages to positive words 45% more than Muslim\nand 51% more than Judaism. However, in case\nof nationality, these models show fewer signs of\nbias. Furthermore, ViLT and ALBEF show less\nbias towards physical disabilities compared to men-\ntal disability.\nAlgorithm 1 Bias Mitigation Algorithm\nRequire: Image Embedding VI,\nText Embedding VT,\nFeatures to remove N,\nClassification Labels L\nX ←∅\nΨ ←Compute_Bias(VI,V T)\nfor d←1 to len(VI) do\nVI ←VI \\vI\nd\nVT ←VT \\vT\nd\nif MI(vI\nd,L) <Θ then\nψd ←Compute_Bias(VI,VT)\nif ψd <Ψ then\nX ←X∪{(d,ψd)}\nend if\nend if\nend for\nZ ←sortψd (X)[0 : N] // Dimensions to remove\nX ←X \\Z\nreturn X\n6 Bias Mitigation Algorithm\nBias mitigation methods typically fall into one of\nthree categories: data augmentation (fair resam-\npling), model adjustment, and embedding post pro-\ncessing algorithms. Each of these alternatives have\ntheir own benefits and drawbacks, but a major lim-\nitation of the first two is that they require retrain-\ning the models. This can be burdensome in many\ncases. In particular, we often lack access to the\ndataset, the model’s training procedure, or in the\ncase of large pretrained models, retraining may\nalso be computationally infeasible on typical hard-\nware and cost budgets. Post-processing methods,\non the other hand, may be invoked as a fast and\n1732\nefficient plug-and-play method to modify learned\nembeddings without the need for retraining. Since\nvision–language tasks are complex, VLP models\nusually have large architectures to be able to cap-\nture all the complex dynamics. However, this can\ncause them to learn redundant or highly correlated\nfeatures, since they are not optimally compressed.\nThese features are not only computationally waste-\nful but can also amplify model bias. Due to the\nhigh correlation among some features, we can re-\nmove some without affecting performance, while\nsimultaneously reducing bias. In order to identify\nthose features, we directly optimize for the objec-\ntive in Eq. 1 by removing features in a greedy man-\nner, pruning N dimensions that cause the largest\ndecrease in bias effect size. However, we only\nconsider the features that exhibit a small mutual\ninformation with respect to classification labels. Θ\ncan be set empirically and this ensures only redun-\ndant dimensions are removed. Algorithm 1 details\nthe steps of our technique.\nBias Before After Reduction\nMuslim vs Christian 1.72 0.57 66%\nJewish vs Christian 1.69 0.75 55%\nMuslim vs Buddhist 1.62 0.11 93%\nJewish vs Buddhist 1.61 0.30 82%\nMuslim vs Hindu 1.65 0.71 57%\nArab vs American 1.28 0.33 74%\nMexican vs American 1.13 0.85 26%\nChinese vs American 0.89 0.56 38%\nMental Dis. vs No Dis. 1.48 0.49 66%\nLGBTQ vs Heterosexual1.67 0.92 45%\nTable 3: Bias Mitigation Results. Our algorithm is\nable to significantly reduce bias without substantially\naffecting performance.\nThe results of this debiasing method are pre-\nsented in Table 3. We removed 54 dimensions\n(10% of all dimensions), which leads to up to 93%\nbias reduction in some cases. This however only\nminimally affects the model’s classification accu-\nracy. We have tested the accuracy of the model\non the MMbias dataset as well as the CIFAR-100\ndataset. On MMbias, the accuracy dropped by only\n1.1% , and on CIFAR100 by only 1.3% from 80.1%\nto 78.8%. Furthermore, Fig. 4b shows that even\nafter removing the aforementioned dimensions, the\nembeddings still remain well-separable, confirming\nthe redundancy of some of the embedding features.\nRegarding the choice of N (number of features\nremoved) in the bias mitigation algorithm, a larger\nN will affect the performance of the model more\nnegatively, as previously observed in other dimen-\nsionality reduction algorithm. In order to find a rea-\nsonable N we can plot the bias reduction as well as\nperformance reduction as a function of N. Inspect-\ning this graph allows us to consider the trade-off\nbetween greater bias removal and the loss of ac-\ncuracy, allowing us to choose an N that decreases\nthe bias in a meaningful way while not affecting\nperformance significantly.\n7 Conclusion\nMost bias analysis studies focus on gender and\nracial biases, which is primarily due to a lack of\nsuitable data to consider further important forms\nof bias. In this study, we have compiled a new\nmultimodal bias assessment dataset called MM-\nBias enabling the study of bias affecting popula-\ntion groups largely neglected in prior studies. Our\ndataset consists of around 3,500 images and hun-\ndreds of phrases covering over 14 minority sub-\ngroups. Furthermore, based on a formulation of\nthe bias-fairness problem, we draw on this data to\nassess the level of bias in several prominent self-\nsupervised multimodal models, including CLIP,\nALBEF, and ViLT. Our results show that these mod-\nels demonstrate meaningful bias towards certain\ngroups. Finally, we introduce a novel bias mit-\nigation technique designed specifically for large\npretrained models that can be applied as a post-\nprocessing step to reduce bias, and show that it\nhas negligible effects on classification performance\nas well as data separability. Our data and code is\navailable at github.com/sepehrjng92/MMBias.\nLimitations\nThis work seeks to make a contribution towards\nvision–language models that exhibit less biased\nbehavior. To this end, we provide a large new\ndataset, new experimental results, and also investi-\ngate a bias mitigation method for pre-trained vision–\nlanguage models. Yet, bias measurement data as\nwell are prone to biases, most notably in the se-\nlection of classes and groups, but also with regard\nto the particular data instances. We envision that\nMMBias will grow to encompass further groups\nand additional data in the future, e.g., further ethnic\nminorities, sexual identities, and gender identities.\nWe also hope that our dataset can serve as a starting\npoint for research on additional natural languages.\nClearly, our bias mitigation algorithm can only\nmitigate certain fairly overt expressions of bias in\n1733\nvision–language models. Large pre-trained models\nhave millions of parameters that affect the model\nbehavior. As vision–language models necessarily\nneed to rate ties between images and text, they will\ncontinue to prefer or disprefer certain associations,\nleading to remnant biases. Still, we hope that our\nwork will enable the community to pay closer atten-\ntion to these challenges and work towards models\nthat behave in more equitable ways.\nEthics Statement\nWith our work, we wish to encourage further ana-\nlysis of bias in machine learning models. To this\nend, we provide data that enables an assessment of\na number of potential manifestations of bias. We\nacknowledge that the images harbor a multitude\nof different stereotypes that cannot be taken to be\nrepresentative of the various groups. Moreover, we\nacknowledge that the pairings of classes of peo-\nple adopted thus far in our work leaves out other\ngroups of people, e.g., further forms of faith and\nbelief, and also further pairings. We view our work\nas a step towards a more inclusive bias assessment\nresource that should keep growing in the future.\nReferences\nAbubakar Abid, Maheen Farooqi, and James Zou. 2021.\nLarge language models associate muslims with vio-\nlence. Nature Machine Intelligence, 3(6):461–463.\nSandhini Agarwal, Gretchen Krueger, Jack Clark, Alec\nRadford, Jong Wook Kim, and Miles Brundage. 2021.\nEvaluating clip: towards characterization of broader\ncapabilities and downstream implications. arXiv\npreprint arXiv:2108.02818.\nFrancis S Bellezza, Anthony G Greenwald, and\nMahzarin R Banaji. 1986. Words high and low in\npleasantness as rated by male and female college stu-\ndents. Behavior Research Methods, Instruments, &\nComputers, 18:299–303.\nShruti Bhargava and David Forsyth. 2019. Ex-\nposing and correcting the gender bias in image\ncaptioning datasets and models. arXiv preprint\narXiv:1912.00578.\nShikha Bordia and Samuel R Bowman. 2019. Identify-\ning and reducing gender bias in word-level language\nmodels. arXiv preprint arXiv:1904.03035.\nRaymond Buriel and Richard Vasquez. 1982. Stereo-\ntypes of mexican descent persons: Attitudes of\nthree generations of mexican americans and anglo-\namerican adolescents. Journal of Cross-Cultural Psy-\nchology, 13(1):59–70.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan.\n2017. Semantics derived automatically from lan-\nguage corpora contain human-like biases. Science,\n356(6334):183–186.\nJohn F Dovidio, Lisa Pagotto, and Michelle R Hebl.\n2011. Implicit attitudes and discrimination against\npeople with physical disabilities. Disability and ag-\ning discrimination: Perspectives in law and psychol-\nogy, pages 157–183.\nSedigheh Eslami, Gerard de Melo, and Christoph\nMeinel. 2021. Does CLIP benefit visual question\nanswering in the medical domain as much as it does\nin the general domain? CoRR, abs/2112.13906.\nIsmael Garrido-Muñoz, Arturo Montejo-Ráez, Fer-\nnando Martínez-Santiago, and L Alfonso Ureña-\nLópez. 2021. A survey on bias in deep nlp. Applied\nSciences, 11(7):3184.\nAnthony G Greenwald and Mahzarin R Banaji. 1995.\nImplicit social cognition: attitudes, self-esteem, and\nstereotypes. Psychological review, 102(1):4.\nAnthony G Greenwald, Debbie E McGhee, and Jor-\ndan LK Schwartz. 1998. Measuring individual differ-\nences in implicit cognition: the implicit association\ntest. Journal of personality and social psychology ,\n74(6):1464.\nWei Guo and Aylin Caliskan. 2021. Detecting emergent\nintersectional biases: Contextualized word embed-\ndings contain a distribution of human-like biases. In\nProceedings of the 2021 AAAI/ACM Conference on\nAI, Ethics, and Society, pages 122–133.\nMichelle R Hebl, Jessica Bigazzi Foster, Laura M Man-\nnix, and John F Dovidio. 2002. Formal and interper-\nsonal discrimination: A field study of bias toward\nhomosexual applicants. Personality and social psy-\nchology bulletin, 28(6):815–825.\nBen Hutchinson, Vinodkumar Prabhakaran, Emily Den-\nton, Kellie Webster, Yu Zhong, and Stephen De-\nnuyl. 2020. Social biases in nlp models as bar-\nriers for persons with disabilities. arXiv preprint\narXiv:2005.00813.\nAmy K Kiefer and Denise Sekaquaptewa. 2007. Im-\nplicit stereotypes and women’s math performance:\nHow implicit gender-math stereotypes influence\nwomen’s susceptibility to stereotype threat. Journal\nof experimental social psychology, 43(5):825–832.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:\nVision-and-language transformer without convolu-\ntion or region supervision. In International Con-\nference on Machine Learning , pages 5583–5594.\nPMLR.\nJunnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,\nShafiq Joty, Caiming Xiong, and Steven Chu Hong\nHoi. 2021. Align before fuse: Vision and language\nrepresentation learning with momentum distillation.\nAdvances in neural information processing systems,\n34:9694–9705.\n1734\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert: A sim-\nple and performant baseline for vision and language.\narXiv preprint arXiv:1908.03557.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks. Ad-\nvances in neural information processing systems, 32.\nThomas Manzini, Yao Chong Lim, Yulia Tsvetkov, and\nAlan W Black. 2019. Black is to criminal as cau-\ncasian is to police: Detecting and removing mul-\nticlass bias in word embeddings. arXiv preprint\narXiv:1904.04047.\nSaif M Mohammad. 2016. Sentiment analysis: De-\ntecting valence, emotions, and other affectual states\nfrom text. In Emotion measurement, pages 201–237.\nElsevier.\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2020.\nStereoset: Measuring stereotypical bias in pretrained\nlanguage models. arXiv preprint arXiv:2004.09456.\nVicente Ordonez, Girish Kulkarni, and Tamara L. Berg.\n2011. Im2text: Describing images using 1 million\ncaptioned photographs. In Neural Information Pro-\ncessing Systems (NIPS).\nJaihyun Park, Karla Felix, and Grace Lee. 2007. Im-\nplicit attitudes toward arab-muslims and the moderat-\ning effects of social information. Basic and Applied\nSocial Psychology, 29(1):35–45.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models\nfrom natural language supervision. In International\nConference on Machine Learning, pages 8748–8763.\nPMLR.\nCandace Ross, Boris Katz, and Andrei Barbu.\n2020. Measuring social biases in grounded vi-\nsion and language embeddings. arXiv preprint\narXiv:2002.08911.\nWade C Rowatt, Lewis M Franklin, and Marla Cotton.\n2005. Patterns and personality correlates of implicit\nand explicit attitudes toward christians and muslims.\nJournal for the scientific study of religion, 44(1):29–\n43.\nLaurie A Rudman and Richard D Ashmore. 2007. Dis-\ncrimination and the implicit association test. Group\nProcesses & Intergroup Relations, 10(3):359–372.\nJames A Russell. 2003. Core affect and the psycholog-\nical construction of emotion. Psychological review,\n110(1):145.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In Proceedings of ACL.\nKirill Sirotkin, Pablo Carballeira, and Marcos Escudero-\nViñolo. 2022. A study on the distribution of social\nbiases in self-supervised learning visual models. In\nProceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 10442–\n10451.\nTejas Srinivasan and Yonatan Bisk. 2021. Worst of both\nworlds: Biases compound in pre-trained vision-and-\nlanguage models. arXiv preprint arXiv:2104.08666.\nRyan Steed and Aylin Caliskan. 2021. Image representa-\ntions learned with unsupervised pre-training contain\nhuman-like biases. In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, pages 701–713.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers. arXiv preprint arXiv:1908.07490.\nJialu Wang, Yang Liu, and Xin Eric Wang. 2021a. Are\ngender-neutral queries really gender-neutral? miti-\ngating gender bias in image search. arXiv preprint\narXiv:2109.05433.\nJialu Wang, Yang Liu, and Xin Eric Wang.\n2021b. Assessing multilingual fairness in pre-\ntrained multimodal representations. arXiv preprint\narXiv:2106.06683.\nTianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei\nChang, and Vicente Ordonez. 2019. Balanced\ndatasets are not enough: Estimating and mitigating\ngender bias in deep image representations. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5310–5319.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67–78.\nKankan Zhou, Yibin LAI, and Jing Jiang. 2022. Vl-\nstereoset: A study of stereotypical bias in pre-trained\nvision-language models. Association for Computa-\ntional Linguistics.\n1735",
  "topic": "Debiasing",
  "concepts": [
    {
      "name": "Debiasing",
      "score": 0.8387680053710938
    },
    {
      "name": "Computer science",
      "score": 0.6696222424507141
    },
    {
      "name": "Sexual orientation",
      "score": 0.5911005735397339
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.572695255279541
    },
    {
      "name": "Race (biology)",
      "score": 0.549475371837616
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5423397421836853
    },
    {
      "name": "Gender bias",
      "score": 0.5157852172851562
    },
    {
      "name": "Machine learning",
      "score": 0.4420029819011688
    },
    {
      "name": "Modal",
      "score": 0.4311652183532715
    },
    {
      "name": "Natural language processing",
      "score": 0.3871695399284363
    },
    {
      "name": "Cognitive psychology",
      "score": 0.36926668882369995
    },
    {
      "name": "Psychology",
      "score": 0.2786610722541809
    },
    {
      "name": "Social psychology",
      "score": 0.14122804999351501
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I102322142",
      "name": "Rutgers, The State University of New Jersey",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I176453806",
      "name": "University of Potsdam",
      "country": "DE"
    }
  ]
}