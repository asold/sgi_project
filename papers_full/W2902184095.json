{
  "title": "Learning RoI Transformer for Detecting Oriented Objects in Aerial Images",
  "url": "https://openalex.org/W2902184095",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1959255347",
      "name": "Ding Jian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2139583801",
      "name": "Xue-nan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978356363",
      "name": "Long Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4200861725",
      "name": "Xia, Gui-Song",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Lu, Qikai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2577537809",
    "https://openalex.org/W2613825824",
    "https://openalex.org/W2607013913"
  ],
  "abstract": "Object detection in aerial images is an active yet challenging task in computer vision because of the birdview perspective, the highly complex backgrounds, and the variant appearances of objects. Especially when detecting densely packed objects in aerial images, methods relying on horizontal proposals for common object detection often introduce mismatches between the Region of Interests (RoIs) and objects. This leads to the common misalignment between the final object classification confidence and localization accuracy. Although rotated anchors have been used to tackle this problem, the design of them always multiplies the number of anchors and dramatically increases the computational complexity. In this paper, we propose a RoI Transformer to address these problems. More precisely, to improve the quality of region proposals, we first designed a Rotated RoI (RRoI) learner to transform a Horizontal Region of Interest (HRoI) into a Rotated Region of Interest (RRoI). Based on the RRoIs, we then proposed a Rotated Position Sensitive RoI Align (RPS-RoI-Align) module to extract rotation-invariant features from them for boosting subsequent classification and regression. Our RoI Transformer is with light weight and can be easily embedded into detectors for oriented object detection. A simple implementation of the RoI Transformer has achieved state-of-the-art performances on two common and challenging aerial datasets, i.e., DOTA and HRSC2016, with a neglectable reduction to detection speed. Our RoI Transformer exceeds the deformable Position Sensitive RoI pooling when oriented bounding-box annotations are available. Extensive experiments have also validated the flexibility and effectiveness of our RoI Transformer. The results demonstrate that it can be easily integrated with other detector architectures and significantly improve the performances.",
  "full_text": "Learning RoI Transformer for Detecting\nOriented Objects in Aerial Images\nJian Ding, Nan Xue, Yang Long, Gui-Song Xia∗, Qikai Lu\nLIESMARS-CAPTAIN, Wuhan University, Wuhan, 430079, China\n{jian.ding, xuenan, longyang, guisong.xia, qikai lu}@whu.edu.cn\nDecember 4, 2018\nAbstract\nObject detection in aerial images is an active yet challenging task in computer vision because of\nthe birdview perspective, the highly complex backgrounds, and the variant appearances of objects.\nEspecially when detecting densely packed objects in aerial images, methods relying on horizontal\nproposals for common object detection often introduce mismatches between the Region of Interests\n(RoIs) and objects. This leads to the common misalignment between the ﬁnal object classiﬁcation\nconﬁdence and localization accuracy. Although rotated anchors have been used to tackle this\nproblem, the design of them always multiplies the number of anchors and dramatically increases\nthe computational complexity. In this paper, we propose a RoI Transformer to address these\nproblems. More precisely, to improve the quality of region proposals, we ﬁrst designed a Rotated\nRoI (RRoI) learner to transform a Horizontal Region of Interest (HRoI) into a Rotated Region of\nInterest (RRoI). Based on the RRoIs, we then proposed a Rotated Position Sensitive RoI Align\n(RPS-RoI-Align) module to extract rotation-invariant features from them for boosting subsequent\nclassiﬁcation and regression. Our RoI Transformer is with light weight and can be easily embedded\ninto detectors for oriented object detection. A simple implementation of the RoI Transformer has\nachieved state-of-the-art performances on two common and challenging aerial datasets, i.e., DOTA\nand HRSC2016, with a neglectable reduction to detection speed. Our RoI Transformer exceeds the\ndeformable Position Sensitive RoI pooling when oriented bounding-box annotations are available.\nExtensive experiments have also validated the ﬂexibility and eﬀectiveness of our RoI Transformer.\nThe results demonstrate that it can be easily integrated with other detector architectures and\nsigniﬁcantly improve the performances.\n1 Introduction\nObject detection in aerial images aims at locating objects of interest (e.g., vehicles, airplanes) on the\nground and identifying their categories. With more and more aerial images being available, object\ndetection in aerial images has been a speciﬁc but active topic in computer vision [1–4]. However, unlike\nnatural images that are often taken from horizontal perspectives, aerial images are typically taken with\nbirdviews, which implies that objects in aerial images are always arbitrary oriented. Moreover, the\nhighly complex background and variant appearances of objects further increase the diﬃculty of object\n∗Corresponding author: guisong.xia@whu.edu.cn.\n1\narXiv:1812.00155v1  [cs.CV]  1 Dec 2018\nFigure 1: Horizontal (top) v.s. Rotated RoI warping (bottom) illustrated in an image with\nmany densely packed objects. One horizontal RoI often contains several instances, which leads ambi-\nguity to the subsequent classiﬁcation and location task. By contrast, a rotated RoI warping usually\nprovides more accurate regions for instances and enables to better extract discriminative features for\nobject detection.\ndetection in aerial images. These problems have been often approached by an oriented and densely\npacked object detectiontask [5–7], which is new while well-grounded and have attracted much attention\nin the past decade [8–12].\nMany of recent progresses on object detection in aerial images have beneﬁted a lot from the R-\nCNN frameworks [2, 4, 7, 13–18]. These methods have reported promising detection performances, by\nusing horizontal bounding boxes as region of interests (RoIs) and then relying on region-based features\nfor category identiﬁcation [2, 4, 16]. However, as observed in [5, 19], these horizontal RoIs (HROIs)\ntypically lead to misalignments between the bounding boxes and objects. For instance, as shown in\nFig. 1, due to the oriented and densely-distributed properties of objects in aerial images, several object\ninstances are often crowded and contained by one HRoI. As a result, it usually turns to be diﬃcult to\ntrain a detector for extracting object features and identifying the object’s accurate localization.\nInstead of using horizontal bounding boxes, oriented bounding boxes have been alternatively em-\nployed to eliminate the mismatching between RRoIs and corresponding objects [5, 19, 20]. In order\nto achieve high recalls at the phase of RRoI generation, a large number of anchors are required with\ndiﬀerent angles, scales and aspect ratios. These methods have demonstrated promising potentials\non detecting sparsely distributed objects [8–10, 21]. However, due to the highly diverse directions of\nobjects in aerial images, it is often intractable to acquire accurate RRoIs to pair with all the objects in\nan aerial image by using RRoIs with limited directions. Consequently, the elaborate design of RRoIs\nwith as many directions and scales as possible usually suﬀers from its high computational complexity\n2\nat region classiﬁcation and localization phases.\nAs the regular operations in conventional networks for object detection [14] have limited gener-\nalization to rotation and scale variations, it is required of some orientation and scale-invariant in\nthe design of RoIs and corresponding extracted features. To this end, Spatial Transformer [22] and\ndeformable convolution and RoI pooling [23] layers have been proposed to model the geometry vari-\nations. However, they are mainly designed for the general geometric deformation without using the\noriented bounding box annotation. In the ﬁeld of aerial images, there is only rigid deformation, and\noriented bounding box annotation is available. Thus, it is natural to argue that it is important to\nextract rotation-invariant region features and to eliminate the misalignment between region features\nand objects especially for densely packed ones.\nIn this paper, we propose a module called RoI Transformer, targeting to achieve detection of\noriented and densely-packed objects, by supervised RRoI learning and feature extraction based on\nposition sensitive alignment through a two-stage framework [13–15, 24, 25]. It consists of two parts.\nThe ﬁrst is the RRoI Learner, which learns the transformation from HRoIs to RRoIs. The second is the\nRotated Position Sensitive RoI Align, which extract the rotation-invariant feature extraction from the\nRRoI for subsequent objects classiﬁcation and location regression. To further improve the eﬃciency,\nwe adopt a light head structure for all RoI-wise operations. We extensively test and evaluate the\nproposed RoI Transformer on two public datasets for object detection in aerial images i.e.DOTA [5]\nand HRSC2016 [19], and compare it with state-of-the-art approaches, such as deformable PS RoI\npooling [23]. In summary, our contributions are in three-fold:\n•We propose a supervised rotated RoI leaner, which is a learnable module that can transform\nHorizontal RoIs to RRoIs. This design can not only eﬀectively alleviate the misalignment be-\ntween RoIs and objects, but also avoid a large amount of RRoIs designed for oriented object\ndetection.\n•We designe a Rotated Position Sensitive RoI Alignment module for spatially invariant feature\nextraction, which can eﬀectively boost the object classiﬁcation and location regression. The\nmodule is a crucial design when using light-head RoI-wise operation, which grantees the eﬃciency\nand low complexity.\n•We achieve state-of-the-art performance on several public large-scale datasets for oriented object\ndetection in aerial images. Experiments also show that the proposed RoI Transformer can\nbe easily embedded into other detector architectures with signiﬁcant detection performance\nimprovements.\n2 Related Work\n2.1 Oriented Bounding Box Regression\nDetecting oriented objects is an extension of general horizontal object detection. The objective of\nthis problem is to locate and classify an object with orientation information, which is mainly tackled\nwith methods based on region proposals. The HRoI based methods [5, 26] usually use a normal RoI\nWarping to extract feature from a HRoI, and regress position oﬀsets relative to the ground truths.\nThe HRoI based method exists a problem of misalignment between region feature and instance. The\n3\nRRoI based methods [9, 10] usually use a Rotated RoI Warping to extract feature from a RRoI, and\nregress position oﬀsets relative to the RRoI, which can avoid the problem of misalignment in a certain.\nHowever, the RRoI based method involves generating a lot of rotated proposals. The [10] adopted\nthe method in [8] for rotated proposals. The SRBBS [8] is diﬃcult to be embedded in the neural\nnetwork, which would cost extra time for rotated proposal generation. The [9, 12, 21, 27] used a\ndesign of rotated anchor in RPN [15]. However, the design is still time-consuming due to the dramatic\nincrease in the number of anchors ( num scales×num aspect ratios×num angles). For example,\n3×5×6 = 90 anchors at a location. A large amount of anchors increases the computation of parameters\nin the network, while also degrades the eﬃciency of matching between proposals and ground truths\nat the same time. Furthermore, directly matching between oriented bounding boxes (OBBs) is harder\nthan that between horizontal bounding boxes(HBBs) because of the existence of plenty of redundant\nrotated anchors. Therefore, in the design of rotated anchors, both the [9,28] used a relaxed matching\nstrategy. There are some anchors that do not achieve an IoU above 0.5 with any ground truth, but\nthey are assigned to be True Positive samples, which can still cause the problem of misalignment. In\nthis work, we still use the horizontal anchors. The diﬀerence is that when the HRoIs are generated, we\ntransform them into RRoIs by a light fully connected layer. Based on this strategy, it is unnecessary\nto increase the number of anchors. And a lot of precisely RRoIs can be acquired, which will boost\nthe matching process. So we directly use the IoU between OBBs as a matching criterion, which can\neﬀectively avoid the problem of misalignment.\n2.2 Spatial-invariant Feature Extraction\nCNN frameworks have good properties for the generalization of translation-invariant features while\nshowing poor performance on rotation and scale variations. For image feature extraction, the Spatial\nTransformer [22] and deformable convolution [23] are proposed for the modeling of arbitrary deforma-\ntion. They are learned from the target tasks without extra supervision. For region feature extraction,\nthe deformable RoI pooling [23] is proposed, which is achieved by oﬀset learning for sampling grid of\nRoI pooling. It can better model the deformation at instance level compared to regular RoI warp-\ning [14,24,25]. The STN and deformable modules are widely used for recognition in the ﬁeld of scene\ntext and aerial images [29–33]. As for object detection in aerial images, there are more rotation and\nscale variations, but hardly nonrigid deformation. Therefore, our RoI Transformer only models the\nrigid spatial transformation, which is learned in the format of ( dx,dy,dw,dh,dθ). However, diﬀerent\nfrom deformable RoI pooling, our RoI Transformer learns the oﬀset with the supervision of ground\ntruth. And the RRoIs can also be used for further rotated bounding box regression, which can also\ncontribute to the object localization performance.\n2.3 Light RoI-wise Operations\nRoI-wise operation is the bottleneck of eﬃciency on two-stage algorithms because the computation\nare not shared. The Light-head R-CNN [34] is proposed to address this problem by using a larger\nseparable convolution to get a thin feature. It also employs the PS RoI pooling [24] to further reduce\nthe dimensionality of feature maps. A single fully connected layer is applied on the pooled features\nwith the dimensionality of 10, which can signiﬁcantly improve the speed of two-stage algorithms.\nIn aerial images, there exist scenes where the number of instances is large. For example, over 800\ninstances are densely packed on a single 1024 ×1024 image. Our approach is similar to Deformable\n4\nFigure 2: The architecture of RoI Transformer. For each HRoI, it is passed to a RRoI learner.\nThe RRoI learner in our network is a PS RoI Align followed by a fully connected layer with the\ndimension of 5 which regresses the oﬀsets of RGT relative to HRoI. The Box decoder is at the end of\nRRoI Learner, which takes the HRoI and the oﬀsets as input and outputs the decoded RRoIs. Then\nthe feature map and the RRoI are passed to the RRoI warping for geometry robust feature extraction.\nThe combination of RRoI Learner and RRoI warping form a RoI Transformer (RT). The geometry\nrobust pooled feature from the RoI Transformer is then used for classiﬁcation and RRoI regression.\nFigure 3: An example explaining the relative oﬀset. There are three coordinate systems. The\nXOY is bound to the image. The x1O1y1 and x2O2y2 are bound to two RRoIs (blue rectangle)\nrespectively. The yellow rectangle represents the RGT. The right two rectangles are obtained from\nthe left two rectangles by translation and rotation while keeping the relative position unchanged. The\n(∆x1,∆y1) is not equal to (∆X2,∆y2) if they are all in theXOY . They are the same if (∆x1,∆y1) falls\nin x1O1y1 and (∆X2,∆y2) in (x2O2y2). The α1 and α2 denote the angles of two RRoIs respectively.\nRoI pooling [23] where the RoI-wise operations are conducted twice. The light-head design is also\nemployed for eﬃciency guarantee.\n5\n3 RoI Transformer\nIn this section, we present details of our proposed ROI transformer, which contains a trainable fully\nconnected layer termed as RRoI Learner and a RRoI warping layer for learning the rotated RoIs from\nthe estimated horizontal RoIs and then warping the feature maps to maintain the rotation invariance of\ndeep features. Both of these two layers are diﬀerentiable for the end-to-end training. The architecture\nis shown in Fig.2.\n3.1 RRoI Learner\nThe RRoI learner aims at learning rotated RoIs from the feature map of horizontal RoIs. Suppose\nwe have obtained n horizontal RoIs denoted by {Hi}with the format of ( x,y,w,h ) for predicted 2D\nlocations, width and height of a HRoI, the corresponding feature maps can be denoted as {Fi}with\nthe same index. Since every HRoI is the external rectangle of a RRoI in ideal scenarios, we are trying\nto infer the geometry of RRoIs from every feature map Fi using the fully connected layers. We follow\nthe oﬀset learning for object detection to devise the regression target as\nt∗\nx = 1\nwr\n(\n(x∗ −xr) cosθr + (y∗ −yr) sinθr\n)\n,\nt∗\ny = 1\nhr\n(\n(y∗ −yr) cosθr −(x∗ −xr) sinθr)\n)\n,\nt∗\nw = log w∗\nwr , t ∗\nh = log h∗\nhr ,\nt∗\nθ = 1\n2π\n(\n(θ∗ −θr) mod 2 π\n)\n,\n(1)\nwhere (xr,yr,wr,hr,θr) is a stacked vector for representing location, width, height and orientation\nof a RRoI, respectively. ( x∗,y∗,w∗,h∗,θ∗) is the ground truth parameters of an oriented bounding\nbox. The modular operation is used to adjust the angle oﬀset target t∗\nθ that falls in [0 ,2π) for the\nconvenience of computation. Indeed, the target for HRoI regression is a special case of Eq. (1) if\nθ∗ = 3π\n2 . The relative oﬀsets are illustrated in Fig. 3 as explanation. Mathematically, the fully\nconnected layer outputs a vector ( tx,ty,tw,th,tθ) for every feature map Fi by\nt = G(F; Θ), (2)\nwhere Grepresents the fully connected layer and Θ is the weight parameters of Gand Fis the feature\nmap for every HRoI.\nWhile training the layerG, we are about to match the input HRoIs and the ground truth of oriented\nbounding boxes (OBBs). For the consideration of computational eﬃciency , the matching is between\nthe HRoIs and axis-aligned bounding boxes over original ground truth. Once an HRoI is matched, we\nset the t∗\nθ directly by the deﬁnition in Eq. (1). The loss function for optimization is used as Smooth\nL1 loss [13]. For the predicted t in every forward pass, we decode it from oﬀset to the parameters of\nRRoI. That is to say, our proposed RRoI learner can learn the parameters of RRoI from the HRoI\nfeature map F.\n3.2 Rotated Position Sensitive RoI Align\nOnce the parameters of RRoI are obtained, we are able to extract the rotation-invariant deep features\nfor Oriented Object Detection. Here, we propose the module of Rotated Position Sensitive (RPS) RoI\nAlign to extract the rotation-invariant features within a network.\n6\nFigure 4: Rotated RoI warping The shape of the warped feature is a horizontal rectangle (we use\n3×3 for example here.) The sampling grid for RoI warping is determined by the RRoI (xr,yr,w,h,θ ).\nWe employ the image instead of feature map for better explanation. After RRoI warping, the extracted\nfeatures are geometry robust. (The orientations of all the vehicles are the same).\nGiven the input feature map Dwith H ×W ×C channels and a RRoI ( xr,yr,wr,hr,θr), where\n(xr,yr) denotes the center of the RRoI and (wr,hr) denotes the width and height of the RRoI. The θr\ngives the orientation of the RRoI. The RPS RoI pooling divides the Rotated RoI into K×K bins and\noutputs a feature map Ywith the shape of ( K×K×C). For the bin with index ( i,j) (0 ≤i,j <K)\nof the output channel c(0 ≤c<C ), we have\nYc(i,j) =\n∑\n(x,y)∈bin(i,j)\nDi,j,c(Tθ(x,y))/nij, (3)\nwhere the Di,j,c is a feature map out of the K×K×C feature maps. The channel mapping is the\nsame as the original Position Sensitive RoI pooling [24]. The nij is the number of sampling locations\nin the bin. The bin(i,j) denotes the coordinates set {iwr\nk + (sx + 0.5) wr\nk×n; sx = 0,1,...n −1}×{jhr\nk +\n(sy+ 0.5) hr\nk×n; sy = 0,1,...n−1}. And for each ( x,y) ∈bin(i,j), it is converted to (x\n′\n,y\n′\n) by Tθ, where\n(x\n′\ny\n′\n)\n=\n(cosθ −sinθ\nsinθ cosθ\n)(x−wr/2\ny−hr/2\n)\n+\n(xr\nyr\n)\n(4)\nTypically, Eq. (3) is implemented by bilinear interpolation.\n3.3 RoI Transformer for Oriented Object Detection\nThe combination of RRoI Learner, and RPS RoI Align forms a RoI Transformer(RT) module. It\ncan be used to replace the normal RoI warping operation. The pooled feature from RT is rotation-\ninvariant. And the RRoIs provide better initialization for later regression because the matched RRoI\n7\nis closer to the RGT compared to the matched HRoI. As mentioned before, a RRoI is a tuple with 5\nelements (xr,yr,wr,hr,θr). In order to eliminate ambiguity, we use h to denote the short side and w\nthe long side of a RRoI. The orientation vertical tohand falling in [0,π] is chosen as the ﬁnal direction\nof a RRoI. After all these operations, the ambiguity can be eﬀectively avoided. And the operations\nare required to reduce the rotation variations.\nIoU between OBBs In common deep learning based detectors, there are two cases that IoU\ncalculation is needed. The ﬁrst lies in the matching process while the second is conducted for (Non-\nMaximum Suppression) NMS. The IoU between two OBBs can be calculated by Equation 5:\nIoU = area(B1 ∩B2)\narea(B1 ∪B2) (5)\nwhere the B1 and B2 represent two OBBs, say, a RRoI and a RGT. The calculation of IoU between\nOBBs is similar with that between horizontal bounding boxes (HBBs). The only diﬀerence is that the\nIoU calculation for OBBs is performed within polygons as illustrated in Fig. 5. In our model, during\nthe matching process, each RRoI is assigned to be True Positive if the IoU with any RGT is over 0.5.\nIt is worth noting that although RRoI and RGT are both quadrilaterals, their intersection may be\ndiverse polygons, e.g. a hexagon as shown in Fig 5(a). For the long and thin bounding boxes, a slight\njitter in the angle may cause the IoU of the two predicted OBBs to be very low, which would make\nthe NMS diﬃcult as can be seen in Fig. 5(b).\nRRoI\nRGT\nscore: 0.94\nscore: 0.53\n(a) (b)\nFigure 5: Examples of IoU between oriented bounding boxes(OBBs). (a) IoU between a\nRRoI and a matched RGT. The red hexagon indicates the intersection area between RRoI and RGT.\n(b) The intersection between two long and thin bounding boxes. For long and thin bounding boxes,\na slight jitter in the angle may lead to a very low IoU of the two boxes. The red quadrilateral is the\nintersection area. In such case, the predicted OBB with score of 0.53 can not be suppressed since the\nIoU is very low.\nTargets Calculation After RRoI warping, the rotation-invariant feature can be acquired. Consis-\ntently, the oﬀsets also need to be rotation-invariant. To achieve this goal, we use the relative oﬀsets\nas explained in Fig. 3. The main idea is to employ the coordinate system binding to the RRoI rather\nthan the image for oﬀsets calculation. The Eq. (1) is the derived formulation for relative oﬀsets.\n8\n4 Experiments and Analysis\n4.1 Datasets\nFor experiments, we choose two datasets, known as DOTA [5] and HRSC2016 [19], for oriented object\ndetection in aerial images.\n•DOTA [5]. This is the largest dataset for object detection in aerial images with oriented\nbounding box annotations. It contains 2806 large size images. There are objects of 15 categories,\nincluding Baseball diamond (BD), Ground track ﬁeld (GTF), Small vehicle (SV), Large vehicle\n(LV), Tennis court (TC), Basketball court (BC), Storage tank (ST), Soccer-ball ﬁeld (SBF),\nRoundabout (RA), Swimming pool (SP), and Helicopter (HC). The fully annotated DOTA images\ncontain 188, 282 instances. The instances in this data set vary greatly in scale, orientation, and\naspect ratio. As shown in [5], the algorithms designed for regular horizontal object detection\nget modest performance on it. Like PASCAL VOC [35] and COCO [36], the DOTA provides\nthe evaluation server1.\nWe use both the training and validation sets for training, the testing set for test. We do a limited\ndata augmentation. Speciﬁcally, we resize the image at two scales(1.0 and 0.5) for training and\ntesting. After image rescaling, we crop a series of 1024 ×1024 patches from the original images\nwith a stride of 824. For those categories with a small number of samples, we do a rotation\naugmentation randomly from 4 angles (0,90,180,270) to simply avoid the eﬀect of an imbalance\nbetween diﬀerent categories. With all these processes, we obtain 37373 patches, which are much\nless than that in the oﬃcial baseline implements (150, 342 patches) [5]). For testing experiments,\nthe 1024 ×1024 patches are also employed. None of the other tricks is utilized except the stride\nfor image sampling is set to 512.\n•HRSC2016 [19]. The HRSC2016 [19] is a challenging dataset for ship detection in aerial\nimages. The images are collected from Google Earth. It contains 1061 images and more than 20\ncategories of ships in various appearances. The image size ranges from 300 ×300 to 1500 ×900.\nThe training, validation and test set include 436 images, 181 images and 444 images, respectively.\nFor data augmentation, we only adopt the horizontal ﬂipping. And the images are resized to\n(512,800), where 512 represents the length of the short side and 800 the maximum length of an\nimage.\n4.2 Implementation details\nBaseline Framework. For the experiments, we build the baseline network inspired from Light-\nHead R-CNN [34] with backbone ResNet101 [39]. Our ﬁnal detection performance is based on the\nFPN [40] network, while it is not employed in the ablation experiments for simplicity.\n•Light-Head R-CNN OBB: We modiﬁed the regression of fully-connected layer on the second\nstage to enable it to predict OBBs, similar to work in DOTA [5]. The only diﬀerence is that we\nreplace ((xi,yi),i = 1,2,3,4) with ( x,y,w,h,θ ) for the representation of an OBB. Since there\n1http://captain.whu.edu.cn/DOTAweb/\n9\nTable 1: Results of ablation studies. We used the Light-Head R-CNN OBB detector as our baseline.\nThe leftmost column represents the optional settings for the RoI Transformer. In the right four\nexperiments, we explored the appropriate setting for RoI Transformer.\nBaseline Baseline + diﬀerent settings\nRoI Transformer? ✓ ✓ ✓ ✓\nLight RRoI Learner? ✓ ✓ ✓\nContext region enlarge? ✓ ✓\nNMS on RRoIS? ✓ ✓ ✓\nmAP 58.3 63.17 63.39 66.25 67.74\nTable 2: Comparisons with the state-of-the-art methods on HRSC2016.\nmethod CP [10] BL2 [10] RC1 [10] RC2 [10] R2PN [21] RRD [37] RoI Trans.\nmAP 55.7 69.6 75.7 75.7 79.6 84.3 86.2\nTable 3: Comparisons with state-of-the-art detectors on DOTA [5]. The short names for each category\ncan be found in Section 4.1. The FR-O indicates the Faster R-CNN OBB detector, which is the oﬃcial\nbaseline provided by DOTA [5]. The RRPN indicates the Rotation Region Proposal Networks, which\nused a design of rotated anchor. The R2CNN means Rotational Region CNN, which is a HRoI-based\nmethod without using the RRoI warping operation. The RDFPN means the Rotation Dense Feature\nPyramid Netowrks. It also used a design of Rotated anchors, and used a variation of FPN. The work\nin Yang et al. [38] is an extension of R-DFPN.\nmethod FPN Plane BD Bridge GTF SV LV Ship TC BC ST SBF RA Harbor SP HC mAP\nFR-O [5] – 79.42 77.13 17.7 64.05 35.3 38.02 37.16 89.41 69.64 59.28 50.3 52.91 47.89 47.4 46.3 54.13\nRRPN [9] – 80.94 65.75 35.34 67.44 59.92 50.91 55.81 90.67 66.92 72.39 55.06 52.23 55.14 53.35 48.22 61.01\nR2CNN [26] – 88.52 71.2 31.66 59.3 51.85 56.19 57.25 90.81 72.84 67.38 56.69 52.84 53.08 51.94 53.58 60.67\nR-DFPN [27] ✓ 80.92 65.82 33.77 58.94 55.77 50.94 54.78 90.33 66.34 68.66 48.73 51.76 55.1 51.32 35.88 57.94\nYang et al. [38] ✓ 81.25 71.41 36.53 67.44 61.16 50.91 56.6 90.67 68.09 72.39 55.06 55.6 62.44 53.35 51.47 62.29\nBaseline – 81.06 76.81 27.22 69.75 38.99 39.07 38.3 89.97 75.53 65.74 63.48 59.37 48.11 56.86 44.46 58.31\nDPSRP – 81.18 77.42 35.48 70.41 56.74 50.42 53.56 89.97 79.68 76.48 61.99 59.94 53.34 64.04 47.76 63.89\nRoI Transformer – 88.53 77.91 37.63 74.08 66.53 62.97 66.57 90.5 79.46 76.75 59.04 56.73 62.54 61.29 55.56 67.74\nBaseline ✓ 88.02 76.99 36.7 72.54 70.15 61.79 75.77 90.14 73.81 85.04 56.57 62.63 53.3 59.54 41.91 66.95\nRoI Transformer ✓ 88.64 78.52 43.44 75.92 68.81 73.68 83.59 90.74 77.27 81.46 58.39 53.54 62.83 58.93 47.67 69.56\nTable 4: Comparison of our RoI Transformer with deformable PS RoI pooling and Light-Head R-CNN\nOBB on accuracy, speed and memory. All the speed are tested on images with size of 1024 ×1024\non a single TITAN X (Pascal). The time of post process ( i.e.NMS) was not included. The LR-O,\nDPSRP and RT denote the Light-Head R-CNN OBB, deformable Position Sensitive RoI pooling and\nRoI Transformer, respectively.\nmethod mAP train speed test speed param\nLR-O 58.3 0.403 s 0.141s 273MB\nDPSRP 63.89 0.445s 0.206s 273.2MB\nRT 67.74 0.475s 0.17s 273MB\nis an additional param θ, we do not double the regression loss as the original Light-Head R-\n10\nFigure 6: Visualization of detection results from RoI Transformer in DOTA.\nCNN [34] does. The hyperparameters of large separable convolutions we set is k= 15,Cmid =\n256,Cout = 490. And the OHEM [41] is not employed for sampling at the training phase.\nFor RPN, we used 15 anchors same as original Light-Head R-CNN [34]. And the batch size of\nRPN [15] is set to 512. Finally, there are 6000 RoIs from RPN before Non-maximum Suppression\n(NMS) and 800 RoIs after using NMS. Then 512 RoIs are sampled for the training of R-CNN.\nThe learning rate is set to 0.0005 for the ﬁrst 14 epochs and then divided by 10 for the last 4\nepochs. For testing, we adopt 6000 RoIs before NMS and 1000 after NMS processing.\n•Light-Head R-CNN OBB with FPN: The Light-Head R-CNN OBB with FPN uses the\nFPN [40] as a backbone network. Since no source code was publicly available for Light-Head\nR-CNN based on FPN, our implementation details could be diﬀerent. We simply added the\nlarge separable convolution on the feature of every level P2,P3,P4,P5. The hyperparameters of\nlarge separable convolution we set is k= 15,Cmid = 64,Cout = 490. The batch size of RPN is\nset to be 512. There are 6000 RoIs from RPN before NMS and 600 RoIs after NMS processing.\nThen 512 RoIs are sampled for the training of R-CNN. The learning rate is set to 0.005 for the\nﬁrst 5 epochs and divided by a factor of 10 for the last 2 epochs.\n11\n4.3 Comparison with Deformable PS RoI Pooling\nIn order to validate that the performance is not from extra computation, we compared our performance\nwith that of deformable PS RoI pooling, since both of them employed RoI warping operation to model\nthe geometry variations. For experiments, we use the Light-Head R-CNN OBB as our baseline. The\ndeformable PS RoI pooling and RoI Transformer are used to replace the PS RoI Align in the Light-\nHead R-CNN [34].\nComplexity. Both RoI Transformer and deformable RoI pooling have a light localisation network,\nwhich is a fully connected layer followed by the normal pooled feature. In our RoI Transformer, only\n5 parameters(tx,ty,tw,th,tθ) are learned. The deformable PS RoI pooling learns oﬀsets for each bin,\nwhere the number of parameters is 7 ×7 ×2 . So our module is designed lighter than deformable\nPS RoI pooling. As can be seen in Tab. 4, our RoI Transformer model uses less memory (273MB\ncompared to 273.2MB) and runs faster at the inference phase (0.17s compared to 0.206s per image).\nBecause we use the light-head design, the memory savings are not obvious compared to deformable\nPS RoI pooling. However, RoI Transformer runs slower than deformable PS RoI pooling on training\ntime (0.475s compared to 0.445s) since there is an extra matching process between the RRoIs and\nRGTs in training.\nDetection Accuracy. The comparison results are shown in Tab. 4. The deformable PS RoI pooling\noutperforms the Light-Head R-CNN OBB Baseline by 5.6 percents. While there is only 1.4 points\nimprovement for R-FCN [24] on Pascal VOC [35] as pointed out in [23]. It shows that the geometry\nmodeling is more important for object detection in aerial images. But the deformable PS RoI pooling\nis much lower than our RoI Transformer by 3.85 points. We argue that there are two reasons: 1)\nOur RoI Transformer can better model the geometry variations in aerial images. 2) The regression\ntargets of deformable PS RoI pooling are still relative to the HRoI rather than using the boundary\nof the oﬀsets. Our regression targets are relative to the RRoI, which gives a better initialization for\nregression. The visualization of some detection results based on Light-Head R-CNN OBB Baseline,\nDeformable Position Sensitive RoI pooling and RoI Transformer are shown in Fig. 7, Fig. 8 and Fig. 9,\nrespectively. The results in Fig. 7 and the ﬁrst column of Fig. 8 are taken from the same large image.\nIt shows that RoI Transformer can precisely locate the instances in scenes with densely packed ones.\nAnd the Light-Head R-CNN OBB baseline and the deformable RoI pooling show worse accuracy\nperformance on the localization of instances. It is worth noting that the head of truck is misclassiﬁed\nto be small vehicle (the blue bounding box) for the three methods as shown in Fig. 7 and Fig. 8. While\nour proposed RoI Transformer has the least number of misclassiﬁed instances. The second column\nin Fig 8 is a complex scene containing long and thin instances, where both Light-Head R-CNN OBB\nbaseline and deformable PS RoI pooling generate many False Negatives. And these False Negatives\nare hard to be suppressed by NMS due to the reason as explained in Fig. 5(b). Beneﬁting from\nthe consistency between region feature and instance, the detection results based on RoI Transformer\ngenerate much fewer False Negatives.\n4.4 Ablation Studies\nWe conduct a serial of ablation experiments on DOTA to analyze the accuracy of our proposed RoI\nTransformer. We use the Light-Head R-CNN OBB as our baseline. Then gradually change the\n12\nFigure 7: Visualization of detection on the scene where many densely packed instances exist. We\nselect the predicted bounding boxes with scores above 0.1, and a NMS with threshold 0.1 is applied\nfor duplicate removal.\nsettings. When simply add the RoI Transformer, there is a 4.87 point improvement in mAP. The\nother settings are discussed in the following.\nLight RRoI Learner. In order to guarantee the eﬃciency, we directly apply a fully connected layer\nwith output dimension of 5 on the pooled features from the HRoI warping. As a comparison, we also\ntried more fully connected layers for the RRoI learner, as shown at the ﬁrst and second columns in\nTab. 1. We ﬁnd there is little drop (0.22 point) on mAP when we add on more fully connected layer\nwith output dimension of 2048 for the RRoI leaner. The little accuracy degradation should be due\nto the fact that the additional fully connected layer with higher dimensionality requires a longer time\nfor convergence.\nContextual RRoI. As pointed in [9, 42], appropriate enlargement of the RoI will promote the\nperformance. A horizontal RoI may contain much background while a precisely RRoI hardly contains\nredundant background as explained in the Fig. 10. Complete abandon of contextual information will\nmake it diﬃcult to classify and locate the instance even for the human. Therefore, it is necessary to\nenlarge the region of the feature with an appropriate degree. Here, we enlarge the long side of RRoI\nby a factor of 1.2 and the short side by 1.4. The enlargement of RRoI improves AP by 2.86 points, as\nshown in Tab. 1\nNMS on RRoIs. Since the obtained RoIs are rotated, there is ﬂexibility for us to decide whether\nto conduct another NMS on the RRoIs transformed from the HRoIs. This comparison is shown in\nthe last two columns of Tab. 1. We ﬁnd there is ∼1.5 points improvement in mAP if we remove the\nNMS. This is reasonable because there are more RoIs without additional NMS, which could increase\nthe recall.\n4.5 Comparisons with the State-of-the-art\nWe compared the performance of our proposed RoI Transformer with the state-of-the-art algorithms\non two datasets DOTA [5] and HRSC2016 [19]. The settings are described in Sec. 4.2, and we just\n13\nFigure 8: Visualization of detection results in DOTA. The ﬁrst row shows the results from RoT\nTransformer. The second ros shows the results from Light-Head R-CNN OBB baseline. The last\nrow shows the results from deformable PS RoI pooling. In the visualization, We select the predicted\nbounding boxes with scores above 0.1, and a NMS with threshold 0.1 is applied for duplicate removal.\nreplace the Position Sensitive RoI Align with our proposed RoI Transformer. Our baseline and RoI\nTransformer results are obtained without using ohem [41] at the training phase.\nResults on DOTA. We compared our results with the state-of-the-arts in DOTA. Note the RRPN [9]\nand R2CNN [26] are originally used for text scene detection. The results are a re-implemented version\nfor DOTA by a third-party 2. As can be seen in Tab. 3, our RoI Transformer achieved the mAP of\n67.74 for DOTA , it outperforms the previous the state-of-the-art without FPN (61.01) by 6.71 points.\nAnd it even outperforms the previous FPN based method by 5.45 points. With FPN, the Light-Head\nOBB Baseline achieved mAP of 66.95, which outperforms the previous state-of-the-art detectors, but\nstill slightly lower than RoI Transformer. When RoI Transformer is added on Light-Head OBB FPN\nBaseline, it gets improvement by 2.6 points in mAP reaching the peak at 69.56. This indicates that the\nproposed RoI Transformer can be easily embedded in other frameworks and signiﬁcantly improve the\n2https://github.com/DetectionTeamUCAS/RRPN_Faster-RCNN_Tensorflow\n14\nFigure 9: Visualization of detection results in DOTA. The ﬁrst row shows the results from RoT\nTransformer. The second ros shows the results from Light-Head R-CNN OBB baseline. The last\nrow shows the results from deformable PS RoI pooling. In the visualization, We select the predicted\nbounding boxes with scores above 0.1, and a NMS with threshold 0.1 is applied for duplicate removal.\ndetection performance. Besides, there is a signiﬁcant improvement in densely packed small instances.\n(e.g. the small vehicles, large vehicles, and ships). For example, the detection performance for the ship\ncategory gains an improvement of 26.34 points compared to the previous best result (57.25) achieved\nby R2CNN [26]. Some qualitative results of RoI Transformer on DOTA are given in Fig 6.\nResults on HRSC2016. The HRSC2016 contains a lot of thin and long ship instances with arbi-\ntrary orientation. We use 4 scales {642,1282,2562,5122}and 5 aspect ratios {1/3,1/2,1,2,3}, yielding\nk= 20 anchors for RPN initialization. This is because there is more aspect ratio variations in HRSC,\n15\nFigure 10: Comparison of 3 kinds of region for feature extraction. (a) The Horizontal Region. (b) The\nrectiﬁed Region after RRoI Warping. (c) The rectiﬁed Region with appropriate context after RRoI\nwarping.\nbut relatively fewer scale changes. The other settings are the same as those in 4.2. We conduct\nthe experiments without FPN which still achieves the best performance on mAP. Speciﬁcally, based\non our proposed method, the mAP can reach 86.16, 1.86 higher than that of RRD [37]. Note that\nthe RRD is designed using SSD [43] for oriented object detection, which utilizes multi-layers for fea-\nture extraction with 13 diﬀerent aspect ratios of boxes {1,2,3,5,7,9,15,1/2,1/3,1/5,1/7,1/9,1/15}.\nWhile our proposed framework just employs the ﬁnal output features with only 5 aspect ratios of\nboxes. In Fig. 11, we visualize some detection results in HRSC2016. The orientation of the ship is\nevenly distributed over 2 π. In the last row, there are closely arranged ships, which are diﬃcult to\ndistinguish by horizontal rectangles. While our proposed RoI Transformer can handle the above men-\ntioned problems eﬀectively. The detected incomplete ship in the third picture of the last row proves\nthe strong stability of our proposed RoI Transformer detection method.\n5 Conclusion\nIn this paper, we proposed a module called RoI Transformer to model the geometry transformation and\nsolve the problem of misalignment between region feature and objects. The design brings signiﬁcant\nimprovements for oriented object detection on the challenging DOTA and HRSC with negligible\ncomputation cost increase. While the deformable module is a well-designed structure to model the\ngeometry transformation, which is widely used for oriented object detection. The comprehensive\ncomparisons with deformable RoI pooling solidly veriﬁed that our model is more reasonable when\noriented bounding box annotations are available. So, it can be inferred that our module can be an\noptional substitution of deformable RoI pooling for oriented object detection.\nReferences\n[1] G. Cheng, P. Zhou, and J. Han, “Rifd-cnn: Rotation-invariant and ﬁsher discriminative convolutional\nneural networks for object detection,” in CVPR, pp. 2884–2893, 2016.\n[2] Y. Long, Y. Gong, Z. Xiao, and Q. Liu, “Accurate object localization in remote sensing images based on\nconvolutional neural networks,” IEEE Trans. Geosci. Remote Sens. , vol. 55, no. 5, pp. 2486–2498, 2017.\n[3] G. Wang, X. Wang, B. Fan, and C. Pan, “Feature extraction by rotation-invariant matrix representation\nfor object detection in aerial image,” IEEE Geosci.Remote Sensing Lett. , 2017.\n[4] Z. Deng, H. Sun, S. Zhou, J. Zhao, and H. Zou, “Toward fast and accurate vehicle detection in aerial images\nusing coupled region-based convolutional neural networks,” J-STARS, vol. 10, no. 8, pp. 3652–3664, 2017.\n16\nFigure 11: Visualization of detection results from RoI Transformer in HRSC2016 . We\nselect the predicted bounding boxes with scores above 0.1, and a NMS with threshold 0.1 is applied\nfor duplicate removal.\n[5] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu, M. Pelillo, and L. Zhang, “DOTA: A\nlarge-scale dataset for object detection in aerial images,” in Proc. CVPR, 2018.\n[6] S. Razakarivony and F. Jurie, “Vehicle detection in aerial imagery: A small target detection benchmark,”\nJ Vis. Commun. Image R. , vol. 34, pp. 187–203, 2016.\n[7] M.-R. Hsieh, Y.-L. Lin, and W. H. Hsu, “Drone-based object counting by spatially regularized regional\nproposal network,” in ICCV, vol. 1, 2017.\n17\n[8] Z. Liu, H. Wang, L. Weng, and Y. Yang, “Ship rotated bounding box space for ship extraction from\nhigh-resolution optical satellite images with complex backgrounds,” IEEE Geosci. Remote Sensing Lett. ,\nvol. 13, no. 8, pp. 1074–1078, 2016.\n[9] J. Ma, W. Shao, H. Ye, L. Wang, H. Wang, Y. Zheng, and X. Xue, “Arbitrary-oriented scene text detection\nvia rotation proposals,” TMM, 2018.\n[10] Z. Liu, J. Hu, L. Weng, and Y. Yang, “Rotated region based cnn for ship detection,” in ICIP, pp. 900–904,\nIEEE, 2017.\n[11] M. Liao, B. Shi, and X. Bai, “Textboxes++: A single-shot oriented scene text detector,” CoRR,\nvol. abs/1801.02765, 2018.\n[12] S. M. Azimi, E. Vig, R. Bahmanyar, M. K¨ orner, and P. Reinartz, “Towards multi-class object detection in\nunconstrained remote sensing imagery,” arXiv:1807.02700, 2018.\n[13] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies for accurate object detection\nand semantic segmentation,” in CVPR, pp. 580–587, 2014.\n[14] R. Girshick, “Fast r-cnn,” in CVPR, pp. 1440–1448, 2015.\n[15] S. Ren, K. He, R. B. Girshick, and J. Sun, “Faster R-CNN: towards real-time object detection with region\nproposal networks,” IEEE TPAMI, vol. 39, no. 6, pp. 1137–1149, 2017.\n[16] G. Cheng, P. Zhou, and J. Han, “Learning rotation-invariant convolutional neural networks for object\ndetection in VHR optical remote sensing images,” IEEE Trans. Geosci. Remote Sens. , vol. 54, no. 12,\npp. 7405–7415, 2016.\n[17] Z. Xiao, Y. Gong, Y. Long, D. Li, X. Wang, and H. Liu, “Airport detection based on a multiscale fusion\nfeature for optical remote sensing images,” IEEE Geosci. Remote Sensing Lett. , vol. 14, no. 9, pp. 1469–\n1473, 2017.\n[18] X. Li and S. Wang, “Object detection using convolutional neural networks in a coarse-to-ﬁne manner,”\nIEEE Geosci. Remote Sensing Lett. , vol. 14, no. 11, pp. 2037–2041, 2017.\n[19] Z. Liu, H. Wang, L. Weng, and Y. Yang, “Ship rotated bounding box space for ship extraction from\nhigh-resolution optical satellite images with complex backgrounds,” IEEE Geosci. Remote Sensing Lett. ,\nvol. 13, no. 8, pp. 1074–1078, 2016.\n[20] K. Liu and G. M´ attyus, “Fast multiclass vehicle detection on aerial images,”IEEE Geosci. Remote Sensing\nLett., vol. 12, no. 9, pp. 1938–1942, 2015.\n[21] Z. Zhang, W. Guo, S. Zhu, and W. Yu, “Toward arbitrary-oriented ship detection with rotated region\nproposal and discrimination networks,” IEEE Geosci. Remote Sensing Lett. , no. 99, pp. 1–5, 2018.\n[22] M. Jaderberg, K. Simonyan, A. Zisserman, et al., “Spatial transformer networks,” in NIPS, pp. 2017–2025,\n2015.\n[23] J. Dai, H. Qi, Y. Xiong, Y. Li, G. Zhang, H. Hu, and Y. Wei, “Deformable convolutional networks,” CoRR,\nabs/1703.06211, vol. 1, no. 2, p. 3, 2017.\n[24] J. Dai, Y. Li, K. He, and J. Sun, “R-FCN: object detection via region-based fully convolutional networks,”\nin NIPS, pp. 379–387, 2016.\n[25] K. He, G. Gkioxari, P. Doll´ ar, and R. Girshick, “Mask r-cnn,” in ICCV, pp. 2980–2988, IEEE, 2017.\n[26] Y. Jiang, X. Zhu, X. Wang, S. Yang, W. Li, H. Wang, P. Fu, and Z. Luo, “R2cnn: rotational region cnn\nfor orientation robust scene text detection,” arXiv:1706.09579, 2017.\n18\n[27] X. Yang, H. Sun, K. Fu, J. Yang, X. Sun, M. Yan, and Z. Guo, “Automatic ship detection in remote\nsensing images from google earth of complex scenes based on multiscale rotation dense feature pyramid\nnetworks,” Remote Sensing, vol. 10, no. 1, p. 132, 2018.\n[28] L. Liu, Z. Pan, and B. Lei, “Learning a rotation invariant detector with rotatable bounding box,”\narXiv:1711.09405, 2017.\n[29] Q. Yang, M. Cheng, W. Zhou, Y. Chen, M. Qiu, and W. Lin, “Inceptext: A new inception-text module\nwith deformable psroi pooling for multi-oriented scene text detection,” arXiv:1805.01167, 2018.\n[30] Y. Ren, C. Zhu, and S. Xiao, “Deformable faster r-cnn with aggregating multi-layer features for partially\noccluded object detection in optical remote sensing images,” Remote Sensing, vol. 10, no. 9, p. 1470, 2018.\n[31] M. Liao, J. Zhang, Z. Wan, F. Xie, J. Liang, P. Lyu, C. Yao, and X. Bai, “Scene text recognition from\ntwo-dimensional perspective,” arXiv:1809.06508, 2018.\n[32] B. Shi, X. Wang, P. Lyu, C. Yao, and X. Bai, “Robust scene text recognition with automatic rectiﬁcation,”\nin CVPR, pp. 4168–4176, 2016.\n[33] Z. Xu, X. Xu, L. Wang, R. Yang, and F. Pu, “Deformable convnet with aspect ratio constrained nms for\nobject detection in remote sensing imagery,” Remote Sensing, vol. 9, no. 12, p. 1312, 2017.\n[34] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun, “Light-head r-cnn: In defense of two-stage object\ndetector,” arXiv:1711.07264, 2017.\n[35] M. Everingham, L. V. Gool, C. Williams, J. Winn, and A. Zisserman, “The pascal visual object classes\n(VOC) challenge,” IJCV, vol. 88, no. 2, pp. 303–338, 2010.\n[36] T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll´ ar, and C. L. Zitnick, “Microsoft\nCOCO: common objects in context,” in ECCV, pp. 740–755, 2014.\n[37] M. Liao, Z. Zhu, B. Shi, G.-S. Xia, and X. Bai, “Rotation-sensitive regression for oriented scene text\ndetection,” in CVPR, pp. 5909–5918, 2018.\n[38] X. Yang, H. Sun, X. Sun, M. Yan, Z. Guo, and K. Fu, “Position detection and direction prediction for\narbitrary-oriented ships via multiscale rotation region convolutional neural network,” arXiv:1806.04828,\n2018.\n[39] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in CVPR, June 2016.\n[40] T.-Y. Lin, P. Doll´ ar, R. B. Girshick, K. He, B. Hariharan, and S. J. Belongie, “Feature pyramid networks\nfor object detection.,” in CVPR, vol. 1, p. 3, 2017.\n[41] A. Shrivastava, A. Gupta, and R. Girshick, “Training region-based object detectors with online hard\nexample mining,” in CVPR, 2016.\n[42] P. Hu and D. Ramanan, “Finding tiny faces,” in CVPR, pp. 1522–1530, IEEE, 2017.\n[43] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. E. Reed, C. Fu, and A. C. Berg, “SSD: single shot multibox\ndetector,” in ECCV, pp. 21–37, 2016.\n19",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7717218995094299
    },
    {
      "name": "Region of interest",
      "score": 0.7605160474777222
    },
    {
      "name": "Computer vision",
      "score": 0.7112922668457031
    },
    {
      "name": "Computer science",
      "score": 0.7104515433311462
    },
    {
      "name": "Object detection",
      "score": 0.530969500541687
    },
    {
      "name": "Transformer",
      "score": 0.49244293570518494
    },
    {
      "name": "Minimum bounding box",
      "score": 0.44169723987579346
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3898579478263855
    },
    {
      "name": "Engineering",
      "score": 0.1610296070575714
    },
    {
      "name": "Image (mathematics)",
      "score": 0.13326594233512878
    },
    {
      "name": "Voltage",
      "score": 0.11542847752571106
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}