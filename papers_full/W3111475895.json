{
  "title": "Preordering Encoding on Transformer for Translation",
  "url": "https://openalex.org/W3111475895",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2804240156",
      "name": "Yuki Kawara",
      "affiliations": [
        "Osaka University"
      ]
    },
    {
      "id": "https://openalex.org/A2168877387",
      "name": "Chenhui Chu",
      "affiliations": [
        "Kyoto University"
      ]
    },
    {
      "id": "https://openalex.org/A2037395121",
      "name": "Yuki Arase",
      "affiliations": [
        "Osaka University"
      ]
    },
    {
      "id": "https://openalex.org/A2804240156",
      "name": "Yuki Kawara",
      "affiliations": [
        "Osaka Gakuin University",
        "Kyoto International University",
        "Osaka University"
      ]
    },
    {
      "id": "https://openalex.org/A2168877387",
      "name": "Chenhui Chu",
      "affiliations": [
        "Osaka Gakuin University",
        "Osaka University"
      ]
    },
    {
      "id": "https://openalex.org/A2037395121",
      "name": "Yuki Arase",
      "affiliations": [
        "Kyoto International University",
        "Osaka University",
        "Osaka Gakuin University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6608792757",
    "https://openalex.org/W6681703963",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W6691304694",
    "https://openalex.org/W6732150968",
    "https://openalex.org/W6682508316",
    "https://openalex.org/W6713150159",
    "https://openalex.org/W2168184608",
    "https://openalex.org/W2008961349",
    "https://openalex.org/W6690770833",
    "https://openalex.org/W2108460050",
    "https://openalex.org/W6680267823",
    "https://openalex.org/W2036291627",
    "https://openalex.org/W2118206542",
    "https://openalex.org/W2623037479",
    "https://openalex.org/W2964091381",
    "https://openalex.org/W2251670534",
    "https://openalex.org/W2963922633",
    "https://openalex.org/W6676241556",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6752057903",
    "https://openalex.org/W6691409493",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W4254408171",
    "https://openalex.org/W6681235753",
    "https://openalex.org/W6691351942",
    "https://openalex.org/W6685154543",
    "https://openalex.org/W2741986820",
    "https://openalex.org/W2984274107",
    "https://openalex.org/W2949745489",
    "https://openalex.org/W2251843378",
    "https://openalex.org/W2251997274",
    "https://openalex.org/W2175969983",
    "https://openalex.org/W2576482813",
    "https://openalex.org/W1969974515",
    "https://openalex.org/W2251167004",
    "https://openalex.org/W2250307129",
    "https://openalex.org/W1793412061",
    "https://openalex.org/W2242975712",
    "https://openalex.org/W2144900797",
    "https://openalex.org/W2977975856",
    "https://openalex.org/W2250947025",
    "https://openalex.org/W2807352297",
    "https://openalex.org/W222053410",
    "https://openalex.org/W2401082558",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2109614908",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2153508793",
    "https://openalex.org/W2136544838"
  ],
  "abstract": "The difference in word orders between source and target languages is a serious hurdle for machine translation. Preordering methods, which reorder the words in a source sentence before translation to obtain a similar word ordering with a target language, significantly improve the quality in statistical machine translation. While the information on the preordering position improved the translation quality in recurrent neural network-based models, questions such as how to use preordering information and whether it is helpful for the Transformer model remain unaddressed. In this article, we successfully employed preordering techniques in the Transformer-based neural machine translation. Specifically, we proposed a novel preordering encoding that exploits the reordering information of the source and target sentences as positional encoding in the Transformer model. Experimental results on ASPEC Japanese-English and WMT 2015 English-German, English-Czech, and English-Russian translation tasks confirmed that the proposed method significantly improved the translation quality evaluated by the BLEU scores of the Transformer model by 1.34 points in the Japanese-to-English task, 2.19 points in the English-to-German task, 0.15 points in the Czech-to-English task, and 1.48 points in the English-to-Russian task.",
  "full_text": "644 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021\nPreordering Encoding on Transformer for Translation\nYuki Kawara , Chenhui Chu, and Yuki Arase\nAbstract—The difference in word orders between source and\ntarget languages is a serious hurdle for machine translation. Pre-\nordering methods, which reorder the words in a source sentence\nbefore translation to obtain a similar word ordering with a target\nlanguage, signiﬁcantly improve the quality in statistical machine\ntranslation. While the information on the preordering position im-\nproved the translation quality in recurrent neural network-based\nmodels, questions such as how to use preordering information\nand whether it is helpful for the Transformer model remain un-\naddressed. In this article, we successfully employed preordering\ntechniques in the Transformer-based neural machine translation.\nSpeciﬁcally, we proposed a novel preordering encodingthat exploits\nthe reordering information of the source and target sentences as po-\nsitional encoding in the Transformer model. Experimental results\non ASPEC Japanese–English and WMT 2015 English–German,\nEnglish–Czech, and English–Russian translation tasks conﬁrmed\nthat the proposed method signiﬁcantly improved the translation\nquality evaluated by the BLEU scores of the Transformer model\nby 1.34 points in the Japanese–to–English task, 2.19 points in the\nEnglish–to–German task, 0.15 points in the Czech–to–English task,\nand 1.48 points in the English–to–Russian task.\nIndex Terms—Neural machine translation, preordering, word-\norder, transformer.\nI. INTRODUCTION\nT\nHE difference between the word orders in the source and\ntarget languages signiﬁcantly inﬂuences the translation\nquality in statistical machine translation (SMT) [1]–[3]. An\neffective approach to address this issue is preordering, which\nreorders the words in a source sentence before it is translated.\nIt is performed either through rule-based methods [4], [5], or\nby extracting the reordering rules automatically from a parallel\ncorpus using machine learning-based methods [3], [6]. These\nmethods improve translation quality in SMT, especially in case\nwhere the word orders in the source and target languages are\nhighly dissimilar, such as between SVO and SOV languages. Re-\ncently, Zhaoet al.[7] exploited preordering index embeddings\nfor a recurrent neural network (RNN) - based neural machine\ntranslation (NMT) model to improve the translation quality.\nHowever, questions such as whether or not the preordering\nManuscript received February 7, 2020; revised June 7, 2020, September 24,\n2020, and November 16, 2020; accepted November 18, 2020. Date of publication\nDecember 10, 2020; date of current version January 15, 2021. This work was\nsupported in part by NTT Communication Science Laboratories and Grant-in-\nAid for Young Scientists #19K20343, JSPS. The associate editor coordinating\nthe review of this article and approving it for publication was Dr. T. Watanabe.\n(Corresponding author: Yuki Kawara.)\nYuki Kawara and Yuki Arase are with the Graduate school of Information\nScience and Technology, Osaka University, Suita 565-0871, Japan (e-mail:\nkawara.yuki@ist.osaka-u.ac.jp; arase@ist.osaka-u.ac.jp).\nChenhui Chu is with Kyoto University, Kyoto 606-8501, Japan (e-mail:\nchu@i.kyoto-u.ac.jp).\nDigital Object Identiﬁer 10.1109/TASLP.2020.3042001\nFig. 1. Preordering example for English to Japanese translation. “I like the\npen” was preordered to “I the pen like.” The model knows the preordering\nindices, and therefore, it can consider reordering the information.\nis helpful for the Transformer model [8] and the appropriate\nprocedure that needs to be followed for using the preordering\ninformation remain unaddressed.\nCompared the RNN-based models, the Transformer model\nprovides a signiﬁcantly improved translation quality. However,\nit cannot handle the order of the tokens because it calculates each\ntoken representation independently. Therefore, this model uses\nposition encoding [8], which is added to token representations\nbefore the model collects token embeddings as inputs. Another\nmethod for considering the positions of tokens is relative en-\ncoding [9], wherein the relative position is calculated and added\nto token representations at each layer to determine the order of\ntokens. Due to these encodings, the Transformer model consid-\ners the token order at each side. However, it cannot consider\nthe token orders for both the source and the target sentences\nsimultaneously because these encodings are used separately on\neach side.\nTo exploit both the source and target order information in the\nTransformer model, we proposepreordering encoding, which\nencodes the positions of preordered tokens using absolute [8]\nand relative encoding [9] approaches. We considered an attention\nmechanism that considers preordering information, as depicted\nin Fig. 1. Specially, when the model calculates token representa-\ntions, we add preordering encoding to the token representation.\nFurthermore, our preordering encoding allows the model to\nconsider the source and target sentence orders simultaneously\nwhile it outputs the translation.\nWe conducted English–Japanese, English–German, English–\nCzech, and English–Russian translation experiments. As a re-\nsult, we conﬁrmed that the proposed method improved 1.34\nand 1.01 BLEU points by preordering encoding for relative\nand absolute encoding, respectively, on the Japanese–to–English\ntranslation task; 1.84 and 2.19 BLEU points by preordering\nencoding for relative and absolute encoding, respectively, on\nthe English–to–German translation task, 0.15 BLEU points by\npreordering encoding for relative encoding on the Czech–to–\nEnglish translation task, and 1.26 BLEU points by preordering\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nKAWARAet al.: PREORDERING ENCODING ON TRANSFORMER FOR TRANSLATION 645\nfor relative encoding and 1.48 BLEU points by preordering for\nabsolute encoding on the English–to–Russian translation task.\nII. RELATED WORK\nA. Preordering for SMT\nTo date, the word order is a major problem in SMT [1]–[3].\nModels that adjust orders of translated phrases in a decoder\nhave been proposed to solve this issue [1], [10], [11]; however,\nsuch reordering models do not perform well for long-distance\nreordering. In addition, they are computationally expensive. To\naddress these drawbacks, preordering [3], [12]–[16] and post-\nordering [2], [17], [18] models have been proposed. Preordering\nreorders the words in a source sentence before translation is\nperformed, whereas post-ordering reorders the words in the\ntranslated sentence without considering the word order after\ntranslation. In particular, preordering improves the translation\nquality effectively because it solves long-distance reordering\nand computational complexity issues [3], [19].\nRule-based preordering methods either manually create re-\nordering rules [13]–[16] or extract reordering rules from a\ncorpus [12], [20]. On the other hand, the references [3], [6],\n[21]–[23] applied machine learning to the preordering problem.\nSpeciﬁcally, Hoshinoet al.[23] proposed a method that learns\nwhether child nodes should be swapped at each node of a\nsyntax tree. Furthermore, Neubig et al. [21] and Nakagawa\n[3] proposed methods that construct a binary tree and reorder\nsimultaneously from a source sentence. Moreover, Kawaraet al.\n[6] used a recursive neural network for preordering and improved\nthe translation quality in the SMT.\nB. Usage of Reordering Information in NMT\nZhang et al.[24] proposed a method that exploits the distortion\nmodel used in SMT for RNN-based NMT. This distortion model\ndetermines the index of target tokens in the source sentence.\nBased on this model, they shifted the weight for source tokens\nand calculated the weight for the next token; this approach\nsigniﬁcantly improved the translation quality. This indicates that\nthe token order information of the target sentence in the source\nsentence contributes to NMT.\nChen et al. [25], [26] proposed a method of learning rep-\nresentation based on the order information in the Transformer\nmodel. This method learns the representations from the order\ninformation of the source tokens in the encoder and that of\nthe target tokens in the decoder. Although they calculate the\nrepresentation from both the orders of source and target tokens,\nthey do not use preordering. Speciﬁcally, their approach only\nutilizes the representations calculated from the position of each\ntoken and those of each encoder and decoder. Therefore, this\nmethod cannot consider reordering information.\nMurthy et al. [27] exploited preordering for low-resource\nNMT with transfer learning. They ﬁrst trained the translation\nmodel on languages with an abundant parallel corpus. Later, they\nreordered the source sentences in low-resource languages to be\nsimilar to those in high-resource languages. Finally, they trained\nthe translation model on a low-resource parallel corpus with\nFig. 2. Schematic of the Transformer model. The left side is an encoder, which\nreads source tokens, and the right side is the decoder, which reads target tokens\nwith masks and predicts the next tokens.\ntransfer learning, and reported that the translation quality gained\nsigniﬁcant improvement compared to that obtained without pre-\nordering. This result indicates that token order information can\nalso boost translation quality in multilingual NMT.\nIn the case of an RNN-based factored NMT, Du and Way [28]\nreported that the position of preordered source sentences can\nimprove the translation quality using as the extracted features.\nMoreover, for an RNN-based model, Zhaoet al.[7] reported that\nthe translation quality was improved when preordering index\nembeddings were used. This result indicates that preordering\ninformation is also useful for RNN-based NMT.\nIII. PRELIMINARIES\nThis section brieﬂy describes the Transformer model on which\nour preordering encoding was employed.\nA. Transformer Model\nAs mentioned earlier, the Transformer model calculates each\ntoken representation using self-attention networks. Fig. 2 il-\nlustrates the overall architecture of the Transformer model. It\nconsists of an encoder (left side) and a decoder (right side). The\nencoder takes a source sentence as input and converts it into\ntoken representations. Further, the decoder takes the predicted\ntokens in the target sentence and the token representations of\nthe encoder as input, and predicts the next token of the target\nsentence. This model is trained to minimise the following loss\nfunction:\nlmle = −logp(y|x,θ)\n= −\n∑\nt\nlogp(yt|x,y<t,θ),\n646 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021\nFig. 3. Multi-head attention architecture, which consists ofhattention heads.\nwhere, θ represents the parameters of this model, x =\n{x1,x2,...,x n} is a source sentence of length n, and y =\n{y1,y2,...,y m}is a target sentence of lengthm.\nThis model consists of a stacked multi-head attention model,\nlayer normalization, and a point-wise feed-forward network\nin both the encoder and the decoder. In the encoder, each\ntoken representation is aggregated at the multi-head attention.\nSubsequently, each representation of the tokens output from\nthe multi-head attention was normalized and transformed by\nthe position-wise feed-forward network. In the decoder, each\ntoken representation was calculated by a masked multi-head\nattention because even if the model could know all tokens of the\ntarget sentence at the training, it could only know the predicted\nprevious tokens of the target sentence at the inference. Once each\nrepresentation of the previous tokens is calculated, the model\npredicts the next token of the target sentence using a softmax\nfunction.\nB. Multi-Head Attention for Encoder\nMulti-head attention employsh attention heads. Fig. 3 illus-\ntrates this model. Each head takes a source token representation\ne = {e1,e2,..., en} (ei ∈ Rde ) provided as inputs to a linear\nfunction that transforms them and outputs the token represen-\ntation z = {z1,z2,··· ,zn} (zi ∈ Rdz ), which is used further\nas the input for the next layer. Each output representationzi\nis calculated as a weighted sum of input representations and is\ntransformed by a linear function:\nzi =\nn∑\nj=1\nαij(ejWV +bV ), (1)\nwhere, WV ∈ Rde×dz is a weight matrix for transforming the\nrepresentation, andbV is a bias. Moreover,αij is the weight of\nthe j-th token of thei-th token representation and is calculated\nas:\nαij = exp(sij)∑n\nk=1 exp(sik),\nsij = (eiWQ +bQ)(ejWK +bK)T\n√dz\n, (2)\nwhere, WQ,W K ∈ Rde×dz are weight matrices for transform-\ning the representation; moreover,bQ and bK indicate the bias\nterm.\nFig. 4. Scaled dot-production attention with relative encoding.\nAfter calculating representations zl at the l-th head, the\nj-th token representations of each head are concatenated\nwith each other. Each concatenated representation z′\nj =\nConcat(z1\nj ,z2\nj ,..., zh\nj ) is transformed by a point-wise feed-\nforward network and used as the input by the next layer.\nC. Absolute Encoding and Relative Encoding\nVaswani et al. [8] used the absolute position information of\ntokens obtained using absolute encoding, which encodes a token\nposition in a deterministic manner with sine and cosine functions\nas follows:\nPE(pos,2q)= s i n (pos/100002q/dz ),\nPE(pos,2q +1)= cos( pos/100002q/dz ),\nwhere, pos is the position of tokens, andq is the dimension of\nthe absolute encoding.\nShaw et al. [9] proposed an alternative method that exploits\nthe relative position of tokens. They extended self-attention with\nrelative encodings, which are exploited when the model calcu-\nlates attention. Speciﬁcally, the relative encodingsaK\nij and aV\nij,\nwhich are the position representations of tokens, are calculated\nas follows:\naK\nij = relclip(j−i,k)EK\na , (3)\naV\nij = relclip(j−i,k)EV\na ,\nclip(x,k)=m a x (−k,min(k,x)), (4)\nwhere, clip(·,k) determines the relative distance based on the\nmaximum distancek, and this model considers2k +1 unique\nlabels for around i-th tokens. A reli ∈ R2k+1 is a one-hot\nvector, wherein the dimension corresponding toi is 1 and the\nothers are 0.EK\na and EV\na ∈ R(2k+1)×dz are embedding matrices\nfor relative encoding. These embedding matrices are learned\nthrough training.\nFig. 4 depicts the Transformer model with relative encoding.\naK\nij and aV\nij are added in (1) and (2) as follows:\nzi =\nn∑\nj=1\nαij(ejWV +bV +aV\nij), (5)\nsij = (eiWQ +bQ)(ejWK +bK +aK\nij )T\n√dz\n. (6)\nKAWARAet al.: PREORDERING ENCODING ON TRANSFORMER FOR TRANSLATION 647\nRelative encodings are added to token representations, and thus,\nzi becomes a token representation that is considered in each\ntoken position.\nIV . PROPOSED METHOD\nA. Preordering Methods\nWe used preordering methods based on bracketing transduc-\ntion grammar (BTG) [3] and recursive neural network (RvNN)\n[6] because both models are state-of-the-art in SMT and opti-\nmized them for Kendall’sτ function (7).\nτ =\n4∑|y|−1\ni=1\n∑|y|\nj=i+1 δ(yi < yj)\n|y|(|y|− 1) −1,\nδ(x)=\n{\n1( x is true),\n0( otherwise), (7)\nwhere, y is a vector of target word indexes that are aligned with\nsource words. The value of Kendall’sτ is [−1,1]. When it is 1,\nit means that the sequence ofy follows a complete ascending\norder, that is, the target sentence has the same word order as\nwith the source in terms of word alignment.\n1) BTG Model: The BTG model learns to parse sentences\nand perform preordering jointly using the latent variable percep-\ntron [29]. This model simultaneously parses and assigns either\ninverted (I)or straight (S) labels at each node in the top-down\nprocess. Aninverted (I)indicates reordering the child nodes, and\na straight (S)indicates that their order is unchanged. It is trained\nusing word classes, part-of-speech tags, and word alignments.\nDuring the test, the model parses source sentences in top-down\nrules and assigns labels at each node.\nThis model reorders the words in the sentences bases on the\nfollowing equations:\nˆz =a r gm a x\nz∈Z(x)\n∑\nm∈Nodes(z)\nΛ·Φ(m),\nx′= Proj(ˆz),\nwhere, Φ(m)is a feature function for the BTG tree nodem, and\nΛis the vector of feature weights.Z(x)is the set of all possible\nBTG trees for sentencex, Nodes(z) is the set of all nodes in\nthe treez, andProj(z) is a function that generates a reordered\nsentence x′from BTG treez.\n2) RvNN Model: In the RvNN model, we ﬁrst parse source\nsentences to obtain their syntax trees with an external parser.\nSubsequently, we assign either theinverted (I) or straight (S)\nlabels at each node of the source syntax tree. Gold labels are\nautomatically determined to achieve the highest Kendall’sτ,\nwhich are based on word alignment links. The RvNN predicts\nlabels at the node in the test time and outputs the reordered\nindices of the source-side tokens.\nThe RvNN is constructed using a binary syntax tree. It predicts\nthe label determined by (7). Speciﬁcally, it decides whether the\nchild nodes should be reordered by considering the subtree,\nwhose vector is calculated in a bottom-up manner from the\nleaf nodes. Fig. 5 shows an example of preordering an English\nsentence “My father is a teacher.” At the VP node corresponding\nFig. 5. Preordering an English sentence “My father is a teacher” with RvNN\nfor Japanese. (I indicates reordering the child nodes, andS indicates not reorder-\ning the child nodes.)\nto “is a teacher,” the vector of the node is calculated by (8),\nconsidering its child nodes correspond to “is” and “a teacher”\nas:\np = f([pl;pr]W +b), (8)\ns = pWs +bs, (9)\nwhere, f is a rectiﬁer function,W ∈ R2λ×λ is a weight matrix,\npl and pr are vector representations of the left and right child\nnodes, respectively (λ is a hyper parameter for the size ofp).\n[·;·] denotes the concatenation of two vectors.Ws ∈ Rλ×2 is\na weight matrix for the output layer, andb ∈ Rλ,bs ∈ R2 are\nthe biases.s ∈ R2, calculated using (9), is a weight vector for\neach label, which is fed into a softmax function to calculate the\nprobabilities of thestraight (S)and inverted (I)labels.\nB. Preordering Encoding\nAfter obtaining the reordered positions of the source tokens\nusing preordering methods, we compute the preordering encod-\ning. For absolute encoding of reordering positions, we calculate\npreordering encoding in the same way as the absolute encoding\nof original sentence positions using preordered sentence posi-\ntions, and we add both absolute encoding of original sentence\npositions and reordering positions to the word embedding.\nFor the relative encoding of the reordering positions, we calcu-\nlate preordering encoding in the following manner: we modify\n(3) and (4) to capture the reordering information. When pre-\nordering encoding is calculated, we use the reordered positions\nof source tokens obtained by preordering methods. Fig. 6 shows\nan example of a sentence “I like the pen that my father bought\nyesterday.” This sentence is reordered to “I my father yesterday\nbought that the pen like” in English to Japanese translation.\nThe ordered sentence indices of the original source sentence are\n{0,8,6,7,5,1,2,4,3}. When we consider the representation of\n“bought,” each of the clipped reordering relative indices results\nin {−4,4,2,3,1,−3,−2,0,−1}.1 For example, the index of\n“pen” in the reordered sentence is 7 and “bought” is 4; therefore,\nthe reordering relative index of “pen” is7−4=3 . Another\nexample is: the index of “yesterday” in reordered sentences is\n3, and therefore, the reordering relative index of “yesterday” is\n3−4= −1.\n1Relative indices become{−7,−6,−5,−4,−3,−2,−1,0,1}, which are\nsubtraction of the absolute positions.\n648 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021\nFig. 6. This is the example of calculating the representation of “bought.” The top sentence is a source sentence “I like the pen that my father bought yesterday,”\nand the bottom sentence is the reordered sentence for English to Japanese translation tasks. At the bottom right of each token, the number indicates the relative\nposition not clipped of a token before and after reordering. Attentions for tokens are calculated considering each reordered position.\nWe calculate preordering encoding as relative encodingrK\nij\nand rV\nij as:\nrK\nij = relclip(pj −pi,k)EK\nr ,\nrV\nij = relclip(pj −pi,k)EV\nr ,\nwhere, pj and pi are reordering positions of thej-th and i-th\nsource side tokens, respectively.EK\nr and EV\nr are embedding\nmatrices for preordering encoding.rK\nij , rV\nij ∈ Rdz are the repre-\nsentations of the preordering-based relative positions. The model\ncan calculate more optimized representations by considering the\nreordering positions.\nLater, we add the preordering encodingrK\nij and rV\nij to (5) and\n(6) as\nzi =\nn∑\nj=1\nαij(ejWV +bV +aV\nij +rV\nij),\nsij = (eiWQ +bQ)(ejWK +bK +aK\nij +rK\nij )T\n√dz\n.\nAs a result,zi becomes a representation considering the order\nof the source and target tokens.\nV. EVA L UAT I O N\nA. Corpus and Preprocessing\nWe conducted English–Japanese, English–German, English–\nCzech, and English–Russian translation experiments. We used\nthe ASPEC corpus [30] for the English–Japanese tasks, the Com-\nmon Crawl Corpus2 for the English–German tasks and English–\nRussian tasks, and the Common Crawl Corpus and CzEng\n1.03[31] for the English–Czech tasks.4 The English–Japanese\ncorpus consists of approximately 2M sentence pairs as train-\ning data, 1,790 sentence pairs as development data, and 1,812\nsentence pairs as test data. Furthermore, the English–German\ntraining corpus consists of approximately 2.4M sentence pairs\nas training data. Speciﬁcally, we used “newstest 2014”, which\nconsists of 3,003 sentence pairs, as development data, and “new-\nstest 2015”, which contains 2,169 sentence pairs, as test data.\n2[Online]. Available: http://www.statmt.org/wmt15/translation-task.html\n3[Online]. Available: http://ufal.mff.cuni.cz/czeng/czeng10\n4We used the Common Crawl Corpus as well as CzEng 1.0 for English–Czech\ntasks to avoid being low resource setting because Common Crawl Corpus only\nconsists of 161,838 sentence pairs.\nThe English–Czech training corpus consists of 808,443 sentence\npairs as training data. We used “newstest 2014”, which consists\nof 3,003 sentence pairs, as development data, and “newstest\n2015”, which contains 2,656 sentence pairs, as test data. The\nEnglish–Russian training corpus consists of 878,386 sentence\npairs as training data. In particular, we used “newstest 2014”,\nwhich consists of 3,003 sentence pairs, as development data,\nand “newstest 2015”, which contains 2,818 sentence pairs, as\ntest data.\nIt should be noted that we excluded the part of the sen-\ntence pairs whose lengths were longer than 50 words, or if\nthe source–to–target length ratio exceeded 9. Furthermore, we\nused the remaining 1.8M, 2.2M, 745,782, and 817,256 source\nand target sentences as training data for English–Japanese,\nEnglish–German, English–Czech, and English–Russian tasks,\nrespectively. We used the Stanford Core NLP5 for tokenization\nand pos-tagging of English, Enju6 for parsing of English, Juman7\nfor tokenization of Japanese, Ckylark8 for parsing of Japanese,\nMoses tokenizer9 for the tokenization of German, the Berkeley\nparser10 for parsing German, and the Stanza11 for tokenization\nand pos-tagging of Czech and Russian.12\nB. Training of Preordering Models\nWe used the BTG and RvNN models for preordering source\nsentences in our experiments. To obtain the word alignments for\nboth models, we used MGIZA.13\nFor the BTG model, we used the implementation by the\nauthors.14 In particular, we trained the model for 20 iterations\non 100k sentences sampled from the training dataset. The size\nof the word clustering was set to 256 for the BTG.\n5[Online]. Available: https://stanfordnlp.github.io/CoreNLP/\n6[Online]. Available: https://github.com/mynlp/enju\n7[Online]. Available: http://nlp.ist.i.kyoto-u.ac.jp/index.php?JUMAN\n8[Online]. Available: https://github.com/odashi/ckylark\n9[Online]. Available: https://github.com/moses-smt/mosesdecoder/blob/\nmaster/scripts/tokenizer/ tokenizer.perl\n10[Online]. Available: https://github.com/slavpetrov/berkeleyparser\n11[Online]. Available: https://stanfordnlp.github.io/stanza/\n12We did not apply subword segmentation by byte pair encoding [32] owing\nto its inconsistency to the unit of parsing, which makes RvNN preordering\nunavailable. Moreover, for Czech–to–English and Russia–to–English tasks, we\ndid not conduct experiments with preordering encoding by RvNN because we\ncould not ﬁnd any existing constituency parsers.\n13[Online]. Available: http://github.com/moses-smt/giza-pp\n14[Online]. Available: https://github.com/google/topdown-btg-preordering\nKAWARAet al.: PREORDERING ENCODING ON TRANSFORMER FOR TRANSLATION 649\nTABLE I\nBLEU SCORES WITH DIFFERENT CLIPPING DISTANCE k,M EASURED ON\nDEVELOPMENT DATA ONENGLISH–TO–JAPANESE TASK\nThe RvNN model was trained for 5 epochs on the same\nsentences of the BTG model. In this case, the vocabulary size\nand mini-batch sizes were set to 50k and 500, respectively, while\nthe size of word and POS-tag embeddings were set to 200.\nC. Training of NMT Models\nWe implemented our approach on OpenNMT-py.15 For ex-\nperiments, we used the Transformer model with absolute and\nrelative encodings as the baseline.16\nFor all the models compared, we used the common settings\nas listed below. The size of the vocabulary is 50k, the number\nof both encoder and decoder layers is 6, the dimensions of both\nde and dz were set to 512, the number of heads was 8, and the\nclipping distancek for relative and preordering encodings were\nboth set to 4. Moreover, the clipping distance for preordering\nencoding was tuned using the development sets. We used the\nAdam [33] optimizer with an initial learning rate of 0.001. It was\nensured that the learning rate was decayed every 10k iterations\nafter the model trained 50k iterations. We trained 250k iterations\nand translated the test set with the best model, i.e., the one that\nshowed minimum perplexity on the validation sets.\nTo tune the clipping distancek, we conducted a preliminary\nexperiment using development data on the English–to–Japanese\ntask; Table I presents the corresponding results. Speciﬁcally, for\nthe value ofkwas up to 4, BLEU scores improved askincreased\nat both baseline and relative encoding with BTG. However, when\nkwas set to 8, the baseline BLEU score deteriorated. On the other\nhand, the BLEU score of relative reordering encoding by BTG\nstill improved in this case, but with negligible difference between\nthe BLEU scores obtained when the values ofk were 4 and 8.\nTherefore, we setk =4 for all the models in our experiments.\nD. Overall Results\nThe translation quality was evaluated by BLEU [34] and\nRIBES [35]. RIBES is an evaluation score that focuses on\nreordering based on precision between reference sentences and\nsystem outputs. We ran each experiment 3 times and calculated\nthe average scores on each task to remove the inﬂuence of\nrandomly set initialization values.\nTable II presents the BLEU and RIBES scores of the MT tasks\nwhere English is the source language. Compared to the baseline,\npreordering encoding using the BTG (“+BTG” in Table II) and\nRvNN (“+RvNN” in Table II) methods improved BLEU scores\nby 1.84 and 1.82 points, respectively, for relative encoding and\n15[Online]. Available: https://github.com/OpenNMT/OpenNMT-py\n16Shaw et al.[9] reported that combination of position and relative encoding\ndid not improve translation accuracy. However, we found that translation quality\nimproved in English–Japanese and English–German translation tasks; therefore,\nwe used both position encoding and relative encoding as the baseline.\nby 2.19 and 1.92 points, respectively, for absolute encoding,\nin the English–to–German task. Moreover, these methods im-\nproved RIBES scores by 0.99 and 0.85 points, respectively, for\nrelative encoding and by 1.28 and 0.64 points, respectively, for\nabsolute encoding. In the English–to–Russian task, while BTG\ndid not exhibit any improvement in the BLEU scores, whereas\nRvNN improved BLEU scores by 1.26 and 1.48 points, and\nRIBES scores by 1.64 and 1.80 points for relative encoding and\nabsolute encoding, respectively.\nHowever, in the English–to–Japanese task, preordering en-\ncoding by BTG and RvNN deteriorated each BLEU score by\n1.03 and 0.48 points for relative encoding and 1.12 and 0.97\npoints for absolute encoding, respectively. It also degraded the\nRIBES scores by 0.33 and 0.26 points for relative encoding and\n0.38 and 0.45 points for absolute encoding, respectively. In the\nEnglish–to–Czech task, RvNN deteriorated the BLEU scores by\n1.56 and 1.55 points for relative encoding and absolute encoding,\nrespectively, and RIBES scores by 2.00 and 2.29 points for\nrelative encoding and absolute encoding, respectively.\nTable III shows the BLEU and RIBES scores of the MT\ntasks where English is the target language. Speciﬁcally, in\nthe Japanese–to–English task, preordering encoding by BTG\n(“+BTG” in Table III) and RvNN (“+RvNN” in Table III)\nimproved BLEU scores by 1.34 and 1.12 points for relative\nencoding, and 1.01 and 0.73 points for absolute encoding com-\npared to the baseline, respectively. In the Czech–to–English\ntask, preordering encoding by BTG improved BLEU scores by\n0.15 points for relative encoding and 0.06 points for absolute\nencoding.\nHowever, BLEU scores with BTG and RvNN deteriorated\nby 0.59 and 0.59 points for relative encoding, and 0.32 and\n0.38 points for absolute encoding, respectively, in the German–\nto–English task. Degradations were also observed for RIBES\nscores: by 0.33 and 0.40 points using BTG and RvNN, re-\nspectively, for relative encoding, and by 0.22 points using\nRvNN for absolute encoding. In the Russian–to–English task,\nthe BLEU and RIBES scores of relative encoding with BTG\ndeteriorated by 0.30 and 0.35 points, respectively; on the other\nhand, the BLEU score of absolute reordering encoding with BTG\nimproved by 0.01 points, whereas the RIBES scores deterio-\nrated by 0.09 points. Our investigation of the relation between\nqualities of preordering and translation is detailed in the next\nsection.\nWe conducted experiments with only preordered sentences to\ninvestigate their effect. In these experiments, we used preordered\nsentences and preordered positions, but we did not use the\noriginal positions. Table IV shows the results when we directly\ninput preordered sentences (“Only RvNN” and “Only BTG”) to\nthe Transformer model for English-Japanese translation tasks.\nThe results conﬁrmed that using preordered sentences as inputs\ndirectly deteriorates the translation quality. These results agree\nwith Du and Way [28], Kawaraet al.[6], Chenet al.[25], and\nWang et al. [36], who reported that using directly preordered\nsource sentences deteriorated translation quality in all tasks by\n1 to 3 BLEU points and 2 to 3 RIBES points compared to the\nbaseline system results. This indicates that the original position\nis important for exploiting the preordering information.\n650 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021\nTABLE II\nKENDALL’S τ, BLEUAND RIBES SCORES ONTEST SET OFMT TASKS WHERE ENGLISH IST H ESOURCE LANGUAGE.N UMBERS INBOLD INDICATE THEBEST\nRESULT OFEACH TASK AND THESYSTEMS THAT ARE STATISTICALLYINSIGNIFICANT ATp< 0.05 FROM THEBEST SYSTEMS,W HILE EACH ↑AND ↓INDICATES\nTHAT THEIMPROVED ANDDETERIORATED GAP OFSCORE BYPROPOSED METHOD IS STATISTICALLYSIGNIFICANT ATp< 0.05 FROM “BASELINE”. “+BTG”AND\n“+RVNN” INDICATE THAT OUR METHOD IS USED WITH BTG OR RVNN\nTABLE III\nKENDALL’S τ, BLEUAND RIBES SCORES ONTEST SET OFMT TASKS WHERE ENGLISH IST H ETARGET LANGUAGE.N UMBERS INBOLD INDICATE THEBEST\nRESULT OFEACH TASK AND THESYSTEMS THAT ARE STATISTICALLYINSIGNIFICANT AT p< 0.05 FROM THEBEST SYSTEMS,W HILE EACH ↑AND ↓INDICATES\nTHAT THEIMPROVED ANDDETERIORATED GAP OFSCORE BYPROPOSED METHOD IS STATISTICALLYSIGNIFICANT ATp< 0.05 FROM “BASELINE”. “+BTG”AND\n“+RVNN” INDICATE THAT OUR METHOD IS USED WITH BTG OR RVNN\nTABLE IV\nBLEU AND RIBES SCORES ONEACH TEST SETS WITH PREORDERED\nSENTENCES.“ ONLY BTG” AND “ONLY RVNN” INDICATE THAT THE\nTRANSLATION SYSTEM WAS TRAINED ONPREORDERED SENTENCES\nVI. ANALYSIS\nWe tried to answer the following four research questions to\nanswer in order to further understand the features of preordering\nencoding:\nQ1 What is the upper-bound of improvements possible by\npreordering encoding? (Section VI-A)\nQ2 How does preordering encoding quality affect translation\nquality? (Section VI-B)\nQ3 How does preordering encoding affect to the under- and\nover-generation problems?17 (Section VI-C)\nQ4 Does the effect of preordering encoding depend on sen-\ntence length? (Section VI-D)\nThe tendencies of preordering encoding for absolute and\nrelative encoding are similar, and therefore, in the remainder\nof this section, we analysed preordering encoding for relative\nencoding to answer these questions.\nA. Upper-Bound of Preordering Encoding\nFirst, we conducted an experiment using gold-standard pre-\nordering for preordering encoding to investigate the upper bound\n17“Under-generation” means the translated sentence loses information of the\nsource sentence, whereas “Over-generation” implies the translated sentence\nrepeats the same phrase.\nTABLE V\nBLEU AND RIBES SCORES ON THETEST SET.“ GOLD-STANDARD”I NDICATES\nTHE RESULT OFGOLD-STANDARD PREORDERING ENCODING\nof our method. To obtain the gold-standard preordering, we\nstarted by calculating the word alignments by MGIZA using\nthe training, development, and test datasets. Later, we reordered\nthe sentence to decrease cross alignment using heuristics.\nAccording to the results provided in Table V, compared to the\nbaseline system, using the indices of gold-standard preorder-\ning improved translation quality signiﬁcantly, onall language\npairs. This result indicates that the order information of the\ntarget sentence has a signiﬁcant inﬂuence for machine trans-\nlation. The measured improvements in translation quality with\ngold-standard preordering of MT tasks, where English is the\nsource language, were+9.31, +7.26, +3.58, and+3.08 in the\nEnglish–to–Japanese, English–to–German, English–to–Czech,\nand in English–to–Russian tasks, respectively.\nThe improvements in translation quality with gold-standard\npreordering of MT tasks, where English is the target language,\nKAWARAet al.: PREORDERING ENCODING ON TRANSFORMER FOR TRANSLATION 651\nTABLE VI\n1, 2, 3,AND 4-GRAM PRECISION OFTRANSLATIONS BY THEBASELINE AND THEGOLD-STANDARD PREORDERING ON THETEST SET\nTABLE VII\nTHE AVERAGE NUMBER OFDELETION ANDINSERTION PER SENTENCE ONMT TASKS WHERE ENGLISH IST H ESOURCE LANGUAGE.N UMBERS WITH ↑AND ↓\nINDICATE THESCORE IMPROVED ANDDETERIORATED BYPROPOSED METHOD IS STATISTICALLYSIGNIFICANT ATp< 0.05 FROM “BASELINE”. “+BTG”,\n“+RVNN”, AND “+GOLD-STANDARD”I NDICATE TRANSLATION BYOUR METHOD WITH BTG, RVNN, AND GOLD-STANDARD\nTABLE VIII\nTHE AVERAGE NUMBER OFDELETION ANDINSERTION PER SENTENCE ONMT TASKS WHERE ENGLISH IST H ETARGET LANGUAGE.N UMBERS WITH ↑AND ↓\nINDICATE THESCORE IMPROVED ANDDETERIORATED BYPROPOSED METHOD IS STATISTICALLYSIGNIFICANT ATp< 0.05 FROM “BASELINE”. “+BTG”,\n“+RVNN”, AND “+GOLD-STANDARD”I NDICATE TRANSLATION BYOUR METHOD WITH BTG AND RVNN, AND GOLD-STANDARD\nwere +12.51, +4.80, +3.94, and +3.42 in the Japanese–to–\nEnglish, German–to–English, Czech–to–English, and Russian–\nto–English tasks, respectively. These results indicate that pre-\nordering encoding has a greater effect on the Japanese–to–\nEnglish task of this corpus than on the other three tasks.\nIn order to analyze the improvements achieved by preordering\nin the translation quality, we evaluated then-gram precision of\nthe baseline and gold-standard. Table VI shows the 1, 2, 3, and\n4-gram precision of each method measured on the test sets. All of\nthe n-gram precision values of the gold-standard are higher than\nthe baseline. These results indicate that preordering information\nis helpful not only for improving word-level (1-gram) but also\nfor phrase-level (2 to 4-gram) translations.\nB. Relation Between Preordering and Translation Qualities\nFurthermore, we investigated the quality of preordering by\nevaluating the extent of similarity of the original and preordered\nsentences with the gold-standard preordering. This was done\nusing Kendall’sτ score, which is a rank correlation coefﬁcient\nof word order between two sentences.\nTable II details the results of the MT tasks considering\nEnglish as the source language. In the English–to–Japanese\ntask, the Kendall’sτ improved (+0.32 and +0.17 points by\nusing BTG and RvNN, respectively), whereas the BLEU scores\ndeteriorated (−1.03and −0.48points through BTG and RvNN,\nrespectively). However, for the English–to–German task, the\nKendall’sτ deteriorated (−0.03 and ±0 points by using BTG\nand RvNN, respectively), the BLEU scores improved (+1.84\nand +1.82 points by using BTG and RvNN, respectively, with\nrelative reordering encoding; and by+2.19 and +1.92 points\nthrough BTG and RvNN, respectively, with absolute reordering\nencoding). In the case of English–to–Czech task, the Kendall’s\nτ did not change when RvNN was utilized, whereas the BLEU\nscores deteriorated (−1.56 and −1.55 points with relative and\nabsolute reordering encoding, respectively, when performed us-\ning RvNN). Moreover, it improved (+1.26 and +1.48 points\nwith relative and absolute reordering encoding, respectively,\nby using RvNN) in the case of English–to–Russian task. We\n652 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021\nTABLE IX\nTRANSLATION ANDREORDERING EXAMPLE INJAPANESE TOENGLISH TASKS.( LITERAL TRANSLATIONS ARE GIVEN IN THE\nPARENTHESIS UNDER THEJAPANESE SENTENCES)\nhave conducted various analyses: BLEU scores at different tau\nvalues, BLEU scores regarding word distances between original\nand reordered positions, and BLEU scores by tau differences\nbetween original and reordered sentences. However, none of\nthese analyses found a clear correlation for Kendall’sτ and\nBLEU score in the tasks where English is a source language.\nThe results provided in Table III indicate that, in MT tasks\nwhere English was the target language, Kendall’sτ improved\nin the Japanese–to–English task (+0.16and +0.01points using\nBTG and RvNN, respectively) and in the Czech–to–English task\n(+0.01points by BTG); whereas, it deteriorated in the German–\nto–English task (−0.02and ±0points through BTG and RvNN,\nrespectively) and in the Russian–to–English task (−0.01points\nby BTG). In particular, the domains of German–to–English,\nCzech–to–English, and Russian–to–English tasks were news\ncorpora, and there exists a correlation between Kendall’sτ and\nBLEU score in these corpora. Therefore, it can be concluded that\npreordering encoding could prove to be helpful for translation.\nC. Effects to Under- and Over-Generation\nWe hypothesized that preordering encoding would decrease\nunder-generations because it can consider the target side order\nand realize the lack of translation. To conﬁrm this hypothesis,\nwe conducted an automatic evaluation of over- and under-\ngenerations following Takebayashiet al. [37]. Speciﬁcally, we\nevaluated the number of “insertion” and “deletion” operations\nper sentence using Translation Edit Rate (TER) [38] as under-\nand over-generations, respectively.\nTables VII and VIII present the average numbers of each\noperation, for all of which, the signiﬁcance of differences was\ntested using the bootstrapping method [39]. Compared to the\nKAWARAet al.: PREORDERING ENCODING ON TRANSFORMER FOR TRANSLATION 653\nbaseline, our method, when combined with RvNN, signiﬁcantly\ndecreased under-generations in the Japanese–to–English task.\nOne reason for this is that absolute and relative encoding can\nidentify the position of source tokens but not the position of\ntarget tokens, and therefore, absolute and relative encoding\ncannot consider reordering information. In contrast, preordering\nencoding can see the predicted positions in the target sentence\nusing preordering models. Therefore, the proposed model can\ncapture the token order of the target sentence and consider the\ndropping of tokens in the target sentence in the Japanese–to–\nEnglish task.\nThe number of under-generations by gold-standard decreased\ncompared to the baseline on all tasks. Simultaneously, the num-\nber of over-generations by gold-standard increased on English–\nto–German, English–to–Czech, English–to–Russian, German–\nto–English, Czech–to–English, and Russian–to–English tasks\nas compared to the corresponding baselines. These results\nindicate that preordering encoding performed with gold-\nstandard reordering can improve translation by decreasing\nunder-generations; however, it tends to generate unnecessary\ntokens.\nTable IX shows the translation examples in the Japanese–\nto–English task. In the ﬁrst example, the translation of the\nbaseline system lacks the translation “electroconductive table\nmat and ﬂoor mat and list strap are listed” compared to the\nreference sentence. The sentence information was eliminated\nby the baseline system, whereas, “electroconductive table mat\nand ﬂoor mat and list strap are listed” is successfully output by\nour method (“this article mentions table mat and ﬂoor mat of the\nconductivity” in the RvNN model and “antistatic table mat and\nﬂoor mat of electroconductive are mentioned as countermea-\nsure” in the BTG model). In the second example, the translation\nusing the baseline system lacks the translation of the phrase\n“since the survey was limited”, due to which, the translated\nsentence lacks this information available in the source sentence.\nHowever, translation using our methods can output translations\nthat do not lack any source information.\nIn the third example, the translation of the baseline system\noutputs a translation that is almost similar to the reference.\nHowever, the translation of our method with RvNN lacks the\ninformation “cardiac and respiratory arrest were caused.” In\nthe fourth example, all translations using the baseline and our\nmethod lack some information from the source sentence. We\nassume this attributes to the quality of preordering, as discussed\nin the previous section. Accordingly, ensuring further improve-\nments in the preordering quality is our future work.\nD. Relation to Sentence Lengths\nIt is known that the longer the source sentence is, the poorer\nits machine translation output becomes. The same phenomenon\nlikely happens on preordering, too. Hence, we investigated the\nrelation between sentence lengths and qualities of preordering\nand translation. Figs. 7, 8, 9, and 10 show the average BLEU\nscores and Kendall’sτs of different source sentence lengths in\nJapanese–to–English, Czech–to–English, English–to–German,\nFig. 7. BLEU scores and Kendall’sτs for each length of source sentence in\nthe Japanese to English task.\nFig. 8. BLEU scores and Kendall’sτs for each length of source sentence in\nthe Czech to English task.\nand English–to–Russian tasks, respectively. It was observed that\nthe proposed method improved the overall BLEU scores.\nIn the Japanese–to–English task, Kendall’sτ decreased as\nsentences became longer; speciﬁcally, Kendall’sτ of BTG and\nRvNN were inferior to the baseline (without preordering) for\nsentences with lengths equal to or more than 60 words. However,\nthe BLEU scores of sentences with lengths equal to or more\nthan 20 words were higher than those translated by the baseline.\nIn the Czech–to–English task, the Kendall’sτ improved when\nthe length of source sentence was longer than 20 words, and\nthe BLEU scores also improved when the length of source\nwas longer than 30 words. These results imply that when the\n654 IEEE/ACM TRANSACTIONS ON AUDIO, SPEECH, AND LANGUAGE PROCESSING, VOL. 29, 2021\nFig. 9. BLEU scores and Kendall’sτs for each length of source sentence in\nthe English to German task.\nFig. 10. BLEU scores and Kendall’sτs for each length of source sentence in\nthe English to Russian task.\ntarget language is English, the effect of preordering is more\npronounced in middle to longer sentences.\nIn the English–to–German task, the Kendall’sτ of BTG and\nRvNN was found to be always lower than that of the baseline.\nHowever, the BLEU scores of the proposed method were higher\nthan those of the baseline for longer sentences. Similarly, in the\nEnglish–to–Russian task, Kendall’sτ of BTG and RvNN was\nobserved to be consistently lower than that of the baseline. How-\never, the BLEU scores of the proposed method by RvNN were\nhigher than the baseline when the source sentence was longer\nthan 10 words; moreover, it by BTG was higher than the baseline\nwhen the source sentence was longer than 50 words. These\nresults are not directly comparable to Japanese–to–English and\nCzech–to–English tasks because of the differences in the target\nlanguages, as reﬂected in the largely different characteristics of\nKendall’sτ. We suspect that the sentence length is one of the\nfactors that affect the translation quality; however, we consider\nfurther investigation of the effects of preordering encoding in\ntarget languages other than English as our future work to.\nVII. CONCLUSION\nIn this article, we proposed preordering encoding that exploits\nreordering information in the Transformer model. The proposed\nmethod allows the Transformer model to consider the source\nand target sentence orders simultaneously. To the best of our\nknowledge, this is the ﬁrst time that reordering information is\nexploited for machine translation with the Transformer model.\nThe experiment conﬁrmed that the proposed method improves\ntranslation quality using Transformer models.\nAs our future work, ﬁrst, we plan to improve the quality of\npreordering because it is currently not satisfactorily high when\ncompared with gold-standard preordering. Second, we plan to\nintegrate the preordering model into the NMT model to avoid\nerror propagation in the current pipeline approach.\nREFERENCES\n[1] C. Tillmann, “A unigram orientation model for statistical machine trans-\nlation,” inProc. Hum. Lang. Technol. Conf. North Amer . Chapter Assoc.\nComput. Linguistics, Boston, MA, USA, May 2004, pp. 101–104.\n[2] K. Hayashi, K. Sudoh, H. Tsukada, J. Suzuki, and M. Nagata, “Shift-\nreduce word reordering for machine translation,” inProc. Conf. Empirical\nMethods Natural Lang. Process., Seattle, WA, USA, Oct. 2013, pp. 1382–\n1386.\n[3] T. Nakagawa, “Efﬁcient top-down BTG parsing for machine translation\npreordering,” inProc. Annu. Meet. Assoc. Comput. Linguistics Int. Joint\nConf. Natural Lang. Process., Beijing, China, Jul. 2015, pp. 208–218.\n[4] H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh, “Head ﬁnalization: A\nsimple reordering rule for SOV languages,” inProc. Workshop Stat. Mach.\nTransl. MetricsMATR, Uppsala, Sweden, Jul. 2010, pp. 244–251.\n[5] A. Gojun and A. Fraser, “Determining the placement of German verbs in\nEnglish–to–German SMT,” inProc. Conf. Eur . Chapter Assoc. Comput.\nLinguistics, Avignon, France, Apr. 2012, pp. 726–735.\n[6] Y . Kawara, C. Chu, and Y . Arase, “Recursive neural network based\npreordering for english-to-japanese machine translation,” inProc. Annu.\nMeet. Assoc. Comput. Linguistics, Student Res. Workshop, Melbourne,\nAustralia, Jul. 2018, pp. 21–27.\n[7] Y . Zhao, J. Zhang, and C. Zong, “Exploiting pre-ordering for neural\nmachine translation,” inProc. Int. Conf. Lang. Resour . Eval., Miyazaki,\nJapan, May 2018.\n[8] A. Vaswaniet al., “Attention is all you need,” inProc. Int. Conf. Neural\nInf. Process. Syst., Long Beach, USA, Dec. 2017, pp. 5998–6008.\n[9] P. Shaw, J. Uszkoreit, and A. Vaswani, “Self-attention with relative posi-\ntion representations,” inProc. Conf. North Amer . Chapter Assoc. Comput.\nLinguistics: Hum. Lang. Technol., New Orleans, Louisiana, Jun. 2018,\npp. 464–468.\n[10] P. Koehn, A. Axelrod, A. Birch-Mayne, C. Callison-Burch, M. Osborne,\nand D. Talbot, “Edinburgh system description for the 2005 IWSLT speech\ntranslation evaluation,” in Proc. Int. Workshop Spoken Lang. Transl.,\nPittsburgh, PA, USA, Oct. 2005, pp. 68–75.\n[11] M. Nagata, K. Saito, K. Yamamoto, and K. Ohashi, “A clustered global\nphrase reordering model for statistical machine translation,” inProc.\nInt. Conf. Comput. Linguistics Annu. Meet. Assoc. Comput. Linguistics,\nSydney, Australia, Jul. 2006, pp. 713–720.\n[12] F. Xia and M. McCord, “Improving a statistical MT system with automat-\nically learned rewrite patterns,” inProc. Int. Conf. Comput. Linguistics\n(COLING), Geneva, Switzerland, Aug. 2004, pp. 508–514.\n[13] C. Wang, M. Collins, and P. Koehn, “Chinese syntactic reordering for\nstatistical machine translation,” inProc. Conf. Empirical Methods Natural\nLang. Process. Comput. Natural Lang. Learn., Prague, Czech Republic,\nJun. 2007, pp. 737–745.\nKAWARAet al.: PREORDERING ENCODING ON TRANSFORMER FOR TRANSLATION 655\n[14] P. Xu, J. Kang, M. Ringgaard, and F. Och, “Using a dependency parser\nto improve SMT for subject-object-verb languages,” inProc. Hum. Lang.\nTechnol.: Annu. Conf. North Amer . Chapter Assoc. Comput. Linguistics,\nBoulder, USA, Jun. 2009, pp. 245–253.\n[15] H. Isozaki, K. Sudoh, H. Tsukada, and K. Duh, “Head ﬁnalization: A\nsimple reordering rule for SOV languages,” inProc. Workshop Stat. Mach.\nTransl. MetricsMATR, Uppsala, Sweden, Jul. 2010, pp. 244–251.\n[16] A. Gojun and A. Fraser, “Determining the placement of German verbs in\nEnglish–to–German SMT,” inProc. Conf. Eur . Chapter Assoc. Comput.\nLinguistics, Avignon, France, Apr. 2012, pp. 726–735.\n[17] I. Goto, M. Utiyama, and E. Sumita, “Post-ordering by parsing for\nJapanese-English statistical machine translation,” inProc. Annu. Meet.\nAssoc. Comput. Linguistics, Jeju Island, Korea, Jul. 2012, pp. 311–316.\n[18] I. Goto, M. Utiyama, and E. Sumita, “Post-ordering by parsing with ITG\nfor Japanese-English statistical machine translation,”ACM Trans. Asian\nLang. Inf. Process., vol. 12, no. 4, pp. 17:1–17:22, Oct. 2013.\n[19] L. Jehl, A. de Gispert, M. Hopkins, and B. Byrne, “Source-side preordering\nfor translation using logistic regression and depth-ﬁrst branch-and-bound\nsearch,” inProc. Conf. Eur . Chapter Assoc. Comput. Linguistics, Gothen-\nburg, Sweden, Apr. 2014, pp. 239–248.\n[20] D. Genzel, “Automatically learning source-side reordering rules for large\nscale machine translation,” inProc. Int. Conf. Comput. Linguistics, Bei-\njing, China, Aug. 2010, pp. 376–384.\n[21] G. Neubig, T. Watanabe, and S. Mori, “Inducing a discriminative parser to\noptimize machine translation reordering,” inProc. Conf. Empirical Meth-\nods Natural Lang. Process. Comput. Natural Lang. Learn, Jeju Island,\nKorea, Jul. 2012, pp. 843–853.\n[22] U. Lerner and S. Petrov, “Source-side classiﬁer preordering for machine\ntranslation,” inProc. Conf. Empirical Methods Natural Lang. Process.,\nSeattle, WA, USA, Oct. 2013, pp. 513–523.\n[23] S. Hoshino, Y . Miyao, K. Sudoh, K. Hayashi, and M. Nagata, “Dis-\ncriminative preordering meets Kendall’sτ maximization,” inProc. Annu.\nMeet. Assoc. Comput. Linguistics Int. Joint Conf. Natural Lang. Process.,\nBeijing, China, Jul. 2015, pp. 139–144.\n[24] J. Zhang, M. Wang, Q. Liu, and J. Zhou, “Incorporating word reorder-\ning knowledge into attention-based neural machine translation,” inProc.\nAnnu. Meet. Assoc. Comput. Linguistics, Vancouver, Canada, Jul. 2017,\npp. 1524–1534.\n[25] K. Chen, R. Wang, M. Utiyama, and E. Sumita, “Neural machine transla-\ntion with reordering embeddings,” inProc. Annu. Meet. Assoc. Comput.\nLinguistics, Florence, Italy, Jul. 2019, pp. 1787–1799.\n[26] K. Chen, R. Wang, M. Utiyama, and E. Sumita, “Recurrent positional\nembedding for neural machine translation,” inProc. Conf. Empirical\nMethods Natural Lang. Process. Int. Joint Conf. Natural Lang. Process.,\nHong Kong, China, Nov. 2019, pp. 1361–1367.\n[27] R. Murthy, A. Kunchukuttan, and P. Bhattacharyya, “Addressing word-\norder divergence in multilingual neural machine translation for extremely\nlow resource languages,” inProc. Conf. North Amer . Chapter Assoc.\nfor Comput. Linguistics: Hum. Lang. Technol., Minneapolis, Minnesota,\nJun. 2019, pp. 3868–3873.\n[28] J. Du and A. Way, “Pre-reordering for neural machine translation: Helpful\nor harmful?,”The Prague Bull. Math. Linguistics, vol. 108, pp. 171–182,\nJun. 2017.\n[29] X. Sun, T. Matsuzaki, D. Okanohara, and J. Tsujii, “Latent variable\nperceptron algorithm for structured classiﬁcation,” inProc. Int. Joint Conf.\nArtif. Intell., Pasadena, California, USA, Jul. 2009, pp. 1236–1242.\n[30] T. Nakazawa et al., “ASPEC: Asian scientiﬁc paper excerpt corpus,”\nin Proc. Int. Conf. Lang. Resour . Eval., Portorož., Slovenia, May 2016,\npp. 2204–2208.\n[31] O. Bojar et al., “The joy of parallelism with CzEng 1.0,” inProc. Int.\nConf. Lang. Resour . Eval,. Istanbul, Turkey: Eur. Lang. Resour. Assoc.,\nMay 2012, pp. 3921–3928.\n[32] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of\nrare words with subword units,” inProc. Annu. Meet. Assoc. Comput.\nLinguistics, Berlin, Germany, Aug. 2016, pp. 1715–1725.\n[33] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,”\nin Proc. Int. Conf. Learn. Representations, San Diego, USA, Dec. 2015.\n[34] P. Kishore, R. Salim, W. Todd, and Z. Wei-Jing, “BLEU: A method for\nautomatic evvaluation of machine translation,” inProc. Annu. Meet. Assoc.\nComput. Linguistics, Philadelphia, USA, Jul. 2002, pp. 311–318.\n[35] H. Isozaki, T. Hirao, K. Duh, K. Sudoh, and H. Tsukada, “Automatic\nevaluation of translation quality for distant language pairs,” inProc. Conf.\nEmpirical Methods Natural Lang. Process., Cambridge, USA, Oct. 2010,\npp. 944–952.\n[36] R. Wang, C. Ding, M. Utiyama, and E. Sumita, “English-Myanmar NMT\nand SMT with pre-ordering: NICT’s machine translation systems at WAT-\n2018,” inProc. Paciﬁc Asia Conf. Lang., Inf. Comput. (PACLIC): Workshop\nAsian Transl., Honkong, China, Dec. 2018.\n[37] Y . Takebayashi, C. Chenhui, Y . Arase, and M. Nagata, “Word rewarding\nfor adequate neural machine translation,” inProc. Int. Workshop Spoken\nLang. Transl., Bruges, Belgium, Oct. 2018, pp. 14–22.\n[38] M. Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul, “A study\nof translation edit rate with targeted human annotation,” inProc. Assoc.\nMach. Transl. Amer ., Cambridge, USA, Aug. 2006, pp. 223–231.\n[39] P. Koehn, “Statistical signiﬁcance tests for machine translation evaluation,”\nin Proc. Conf. Empirical Methods Natural Lang. Process., Barcelona,\nSpain, Jul. 2004, pp. 388–395.\nYuki Kawara received the M.S. degree in in-\nformation science and technology in 2018. He is\ncurrently working toward the doctoral degree with\nOsaka University. His research interests include nat-\nural language processing, and in particular, machine\ntranslation.\nChenhui Chu received the B.S. degree in software\nengineering from Chongqing University in 2008, and\nthe M.S. and Ph.D. degrees in informatics from Ky-\noto University in 2012 and 2015, respectively. He\nis currently a program-speciﬁc Associate Professor\nwith Kyoto University. His research interests include\nnatural language processing, particularly machine\ntranslation and multimodal machine learning.\nYuki Arase is an Associate Professor with the Grad-\nuate School of Information Science and Technology,\nOsaka University, Japan. She was previously an Asso-\nciate Researcher with the natural language computing\ngroup of Microsoft Research Asia. Her primary re-\nsearch interest focuses on English/Japanese machine\ntranslation, paraphrasing, conversation systems, and\neducational applications for language learners.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5871566534042358
    },
    {
      "name": "Translation (biology)",
      "score": 0.5022890567779541
    },
    {
      "name": "Computer science",
      "score": 0.43110209703445435
    },
    {
      "name": "Engineering",
      "score": 0.12054422497749329
    },
    {
      "name": "Electrical engineering",
      "score": 0.11621043086051941
    },
    {
      "name": "Voltage",
      "score": 0.0927489697933197
    },
    {
      "name": "Biology",
      "score": 0.06959205865859985
    },
    {
      "name": "Genetics",
      "score": 0.06671026349067688
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I98285908",
      "name": "The University of Osaka",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I22299242",
      "name": "Kyoto University",
      "country": "JP"
    }
  ],
  "cited_by": 18
}