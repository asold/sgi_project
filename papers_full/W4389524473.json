{
  "title": "Adapting Language Models to Compress Contexts",
  "url": "https://openalex.org/W4389524473",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1981036797",
      "name": "Alexis Chevalier",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3200543743",
      "name": "Alexander Wettig",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5000037569",
      "name": "Anirudh Ajith",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2095803999",
      "name": "Danqi Chen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2251939518",
    "https://openalex.org/W4320813768",
    "https://openalex.org/W4386185625",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4300978888",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W4385567182",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4389524599",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4281758439",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W4223530498",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W2338908902",
    "https://openalex.org/W4288087322",
    "https://openalex.org/W4361230739",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3166986030",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W4285595435",
    "https://openalex.org/W4296932804",
    "https://openalex.org/W4308902180",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W4366330736",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4388979610",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3167167634",
    "https://openalex.org/W4385573057",
    "https://openalex.org/W4287019748",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2952186591",
    "https://openalex.org/W2151290962",
    "https://openalex.org/W3172943453"
  ],
  "abstract": "Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These language models are capable of compressing long contexts into summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments, and summary vectors from all previous segments are used in language modeling. We fine-tune OPT and Llama-2 models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations and find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference costs. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling and a passage re-ranking task. Overall, AutoCompressors emerge as a simple and inexpensive solution to extend the context window of LMs while speeding up inference over long contexts.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3829–3846\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAdapting Language Models to Compress Contexts\nAlexis Chevalier∗ Alexander Wettig∗ Anirudh Ajith Danqi Chen\nDepartment of Computer Science & Princeton Language and Intelligence\nPrinceton University\n{achevalier,anirudh.ajith}@princeton.edu\n{awettig, danqic}@cs.princeton.edu\nAbstract\nTransformer-based language models (LMs)\nare powerful and widely-applicable tools, but\ntheir usefulness is constrained by a finite\ncontext window and the expensive computa-\ntional cost of processing long text documents.\nWe propose to adapt pre-trained LMs into\nAutoCompressors. These language models are\ncapable of compressing long contexts into com-\npact summary vectors, which are then acces-\nsible to the model as soft prompts. Summary\nvectors are trained with an unsupervised objec-\ntive, whereby long documents are processed in\nsegments, and summary vectors from all previ-\nous segments are used in language modeling.\nWe fine-tune OPT and Llama-2 models on se-\nquences of up to 30,720 tokens and show that\nAutoCompressors can utilize long contexts to\nimprove perplexity. We evaluate AutoCompres-\nsors on in-context learning by compressing task\ndemonstrations and find that summary vectors\nare good substitutes for plain-text demonstra-\ntions, increasing accuracy while reducing infer-\nence costs. Finally, we explore the benefits of\npre-computing summary vectors for large cor-\npora by applying summary vectors to retrieval-\naugmented language modeling and a passage\nre-ranking task. Overall, AutoCompressors\nemerge as a simple and inexpensive solution\nto extend the context window of LMs while\nspeeding up inference over long contexts.1\n1 Introduction\nTransformer-based (Vaswani et al., 2017) language\nmodels (LMs) have recently seen a sharp rise\nin popularity and are now receiving millions of\nqueries, processing billions of tokens, and generat-\ning text for a wide variety of applications (Brown\net al., 2020; Touvron et al., 2023; Zhang et al.,\n*AC and AW contributed equally. This work was done\nwhen AC was at the Institute for Advanced Study and visited\nthe Princeton NLP group.\n1Our code and models are publicly available at\nhttps://github.com/princeton-nlp/AutoCompressors.\nuse for language modeling summary vectors\nrandomly \nsegmented input\nsummary\ntokens\nsummary\nvectors\nLM\nLM\nLM\nFigure 1: AutoCompressors process long documents\nby recursively generating summary vectors which are\npassed as soft prompts to all subsequent segments.\n2022). With this rise in popularity comes the chal-\nlenge for researchers to make LMs more efficient,\nto speed up inference and to deploy LMs at scale,\nwhile increasing their versatility, thus allowing\nusers to process more data in new ways.\nWith these goals in mind, we propose to teach\npre-trained LMs the ability to compress text into\nsummary vectors. Summary vectors are short soft\nprompts (Lester et al., 2021), one or two orders of\nmagnitude shorter than the pre-compressed plain\ntext, that are obtained from the output states of a\nlanguage model. Summary vectors serve two gen-\neral purposes: they can help extend the language\nmodel’s context window to very long documents\nwith minimal computational overhead, and they\nhelp speed up inference on text for which summary\nvectors have been pre-computed and cached.\nOur models, which we call AutoCompressors,\nare trained with a simple unsupervised learning\nobjective that encourages the model to store essen-\ntial information in the summary vectors. Summary\nvectors are produced segment by segment from\nlong documents and are used to improve language\nmodeling in future segments (Figure 1). Our work\n3829\nbuilds on the recently proposed RMT architecture\n(Bulatov et al., 2022) with a crucial difference: we\nintroduce summary accumulation, in which sum-\nmary vectors from all segments are concatenated\nto produce the summary of the entire document.\nWe also train AutoCompressors with randomly seg-\nmented inputs so they can better compress contexts\nof variable lengths in downstream tasks. We show\nthat these innovations improve long-range informa-\ntion retention and enable new ways of reasoning\nover multiple passages.\nAutoCompressors can be initialized with pre-\ntrained LMs to produce powerful and versatile\nmodels. We fine-tune AutoCompressors from OPT-\n2.7B (Zhang et al., 2022) and Llama-2-7B (Tou-\nvron et al., 2023) models on sequences from 6,144\nup to 30,720 tokens with a single NVIDIA A100\nGPU of 80GB memory. We show that summary\nvectors are effective for improving perplexity over\nlong documents and that these compression capa-\nbilities are robust to domain generalization. Our\nanalysis suggests that AutoCompressors are able to\nreason over summary vectors, making them useful\nfor a diverse set of downstream applications.\nWe apply AutoCompressors to in-context learn-\ning (ICL) by compressing up to 90 in-context\ndemonstrations. We consider 11 classification\ntasks, including 7 SuperGLUE tasks (Wang et al.,\n2019), and we find that summary vectors outper-\nform few-shot ICL with a comparable number of\nin-context tokens on 8 out of 11 tasks.\nFinally, we explore two applications where\nAutoCompressors can reduce inference costs by\npre-computing summary vectors for large corpora.\nFirst, we adopt a setting for retrieval-augmented\nlanguage modeling (Shi et al., 2023). We find that\nfor equal sequence lengths, using summary vec-\ntors achieves 1.5×the perplexity gains compared\nto plain-text passages, and outperforms retrieval-\naugmented methods for similar computational bud-\ngets. Secondly, we consider a zero-shot passage\nre-ranking task (Sachan et al., 2022). We estab-\nlish that re-ranking passages based on their sum-\nmary vectors achieves the best trade-off between\nre-ranking performance and inference throughput.\nIn summary, our main contributions are the fol-\nlowing: (1) We introduce a method for extending\nLMs to long context windows under small-scale\ncomputational requirements by learning to generate\nsummary vectors. We propose summary accumula-\ntion and training with randomized segmenting as\nkey features of AutoCompressors. (2) We show\nthat summary vectors encode useful information\nfor downstream tasks and can be used to reduce\nthe inference cost of in-context learning. (3) We\ndemonstrate the benefits of pre-computing sum-\nmary vectors for large corpora and using Auto-\nCompressors in conjunction with retrievers.\n2 Related Work\nSoft prompts Soft prompt tuning is an effective\nmethod to adapt pre-trained Transformers without\nupdating existing parameters (Lester et al., 2021;\nZhong et al., 2021; Liu et al., 2022). Newly ini-\ntialized embeddings are prepended to the input se-\nquence (the “soft prompt”), and optimization is\nperformed with respect to these new parameters\nwhile the rest of the model is frozen. It is one\nof many parameter-efficient fine-tuning methods\n(Lialin et al., 2023) and is related to prefix tuning,\nwhere newly initialized parameters are prepended\nto the attention states instead (Li and Liang, 2021).\nPrompt compression Wingate et al. (2022) pro-\npose to learn a soft prompt σ to compress the in-\nformation contained in a context x. Given a pre-\ntrained language model pLM, they draw continu-\nations y ∼pLM(·| x) based on xand use a dis-\ntillation objective to align the model’s predictions\nconditioned on the soft prompt pLM(y |σ) to the\npredictions conditioned on the context pLM(y|x).\nWingate et al. (2022) find that soft prompts retain\nhigh-level information and facilitate controllable\ngeneration. However, the approach requires run-\nning the optimization for every new contextx, with\nno knowledge transfer between similar contexts.\nIn contrast, our AutoCompressors learn to predict\ntheir own soft prompts σas a function of x.\nContext distillation A related line of work (Askell\net al., 2021; Snell et al., 2022) aims to distill in-\ncontext information, e.g., instructions, into an un-\nprompted student model. In concurrent work, Mu\net al. (2023) teach models to compress instructions\ninto short key-value attention prefixes. Our ap-\nproach differs by learning to compress any context\ninformation, including long documents, and results\nin more compact soft prompts.\nLong-range Transformers A number of archi-\ntectural modifications have been proposed to scale\nTransformers to longer context lengths while reduc-\ning the high memory costs of full attention. These\ninclude restricting and sparsifying the attention\nwindow (Dai et al., 2019; Child et al., 2019), ap-\n3830\nproximating the attention (Rae et al., 2020; Zheng\net al., 2022; Choromanski et al., 2021), as well as\nintroducing recurrent elements (Ma et al., 2022; Bu-\nlatov et al., 2022), conditional computation (Ainslie\net al., 2023), and retrieving previous tokens from\nthe context at the output layer (Zhong et al., 2022).\nSee Tay et al. (2022) for a comprehensive survey\nof efficient long-range architectures.\nMost of these architectures typically require ex-\npensive training from scratch, or will deviate sub-\nstantially from a pre-trained initialization.2 More-\nover, many language models lack the inductive\nbias to extrapolate to longer sequences (Press et al.,\n2022). While AutoCompressors could in principle\nbe trained from scratch, we show that they offer a\nstraightforward solution for extending the context\nwindow of pre-trained models to longer sequences.\n3 Method\nWe describe how we adapt a pre-trained language\nmodel to compress text into summary vectors. An\noverview of our architecture is shown in Figure 1.\nSummary vectors The AutoCompressor builds\non the RMT architecture (Bulatov et al., 2022). We\nextend the input vocabulary of the base model by\nκspecial summary tokens <Sum>i and initialize κ\nnew input embeddings.3 When we append the se-\nquence <Sum>1 ... <Sum>κ to an input, it signals to\nthe model to output special summary vectors of\nthe preceding context. These vectors can then be\npassed to the next text segment as a soft prompt\nof length κ. Since the embedding spaces of pre-\ntrained language models can span thousands of\ndimensions, we expect that this mechanism has\na high capacity for passing information to subse-\nquent segments. Furthermore, a soft prompt can\ninterpolate between many token embeddings, and\ntherefore represent more abstract concepts than a\nsingle discrete token (Wingate et al., 2022).\nSummary accumulation We split long docu-\nments into segments S1,...,S n and process them\nsequentially. Bulatov et al. (2022) incorporate in-\nformation from previous segments by prepending\nthe compressed summary σi−1 produced from Si−1\nto the embedded inputs ofSi. We proposesummary\n2In our pre-liminary experiments, even fine-tuning a pre-\ntrained OPT-2.7b model with Transformer-XL-style training\n(Dai et al., 2019) caused optimization difficulties and deterior-\niated the pre-trained model quality.\n3When fine-tuning OPT models, we observe benefits with\ninitializing the embeddings of the summary tokens with the\npre-trained embedding for the end-of-sequence token </s>.\naccumulation, which allows for a direct informa-\ntion pathway between each segment and all seg-\nments preceding it: we concatenate the summary\nvectors σ1 ...,σ i−1 to form σ<i and prepend σ<i\nto Si. Note that the length of σ<i is now (i−1)κ,\nwhich grows linearly with the document length.\nPositional embeddings When using a base Trans-\nformer architecture with absolute positional embed-\ndings, such as the OPT architecture (Zhang et al.,\n2022), we do not add positional embeddings to the\nsummary tokens <Sum>i, nor to the summary vec-\ntors. This allows us to use all pre-trained position\nembeddings as context tokens and makes it possi-\nble to scale the model to an arbitrary number of\ncompression steps during training. The model still\npreserves the order of summary tokens due to their\nseparate token embeddings.\nIf the base Transformer uses relative positional\nembeddings, such as RoPE (Su et al., 2022), we\napply the positional embedding to the summary to-\nkens and vectors without any further modification.\n3.1 Training Summary Vectors\nWe use a simple unsupervised training approach\nwhich encourages the model to learn to compress\ncontexts over multiple steps.\nTraining objective Write (xi\n1,...,x i\nmi ) for the\nsegment Si for every i ≤ n, where mi is the\nnumber of tokens in Si. Conditioning on the\nconcatenated summary vectors σ<i, we project\nthe Transformer outputs with the language mod-\neling head to obtain the next-token probabilities\np(xi\nt |xi\n1,...,x i\nt−1,σ<i). We minimize the cross-\nentropy loss over the entire document:\nL= −1\nN\nn∑\ni=1\nmi∑\nt=1\nlog p(xi\nt |xi\n1,...,x i\nt−1,σ<i).\nwhere N is the total number of tokens. This ob-\njective retains the pre-trained language model’s\nabilities on the first segment S1 and it incentivizes\nthe model to store useful information in the sum-\nmary vectors, which future segments can leverage\nto make better token predictions.\nUnlike Wingate et al. (2022), we do not train\nwith a knowledge distillation objective, since the\npre-trained LM has a limited context window as\na teacher, whereas the AutoCompressor student\nlearns to process much longer documents.\nRandomized segmenting We randomly vary the\nlengths mi of the segments Si during training, sub-\nject to the condition that each segment fits into\n3831\nthe model’s context window. This allows Auto-\nCompressors to compress documents of different\nlengths and improves performance under evalua-\ntion with fixed-length segments (see Figure 2).\nBPTT with stop-gradients We employ backprop-\nagation through time (BPTT) and gradient check-\npointing (Chen et al., 2016) for each segment to\nreduce the size of the computational graph. In addi-\ntion, we compute and cache summary vectors and\nstop their gradients after 2 compression steps, simi-\nlar to caching past attention states in Transformer-\nXL training (Dai et al., 2019). This assumes that\nfor learning to compress the useful information in\nSi, it is sufficient to predict the tokens in the adja-\ncent Si+1. In Figure 2, we confirm that this incurs\nno penalty when predicting long segments, while\nfurther reducing GPU memory requirements.\n4 Language Modeling Evaluation\nIn this section, we train AutoCompressors and eval-\nuate their long-range language modeling capabil-\nities by sampling long sequences which we split\ninto segments of 2,048 tokens. We fix the final\nsegment and compress the previous n segments.\nWe track the perplexity of the final segment when\nconditioning on the summary vectors for each n.\nWe conduct our main experiments and ablations\nwith OPT models (Zhang et al., 2022) of 1.3B or\n2.7B parameters, fine-tuned on 2B tokens from the\nPile (Gao et al., 2020). In Section 4.1, we evaluate\nan AutoCompressor on sequences of 8,000 tokens\nand compare to an equivalent RMT model and an\nExtended Full Attention baseline. In Section 4.2,\nwe fine-tune an AutoCompressor on sequences of\n30,000 tokens to demonstrate the feasibility on very\nlong sequences. Finally, in Section 4.3, we scale\nup AutoCompressors by fine-tuning a Llama-2-7B\nmodel on 15B tokens from RedPajama (Togeth-\nerAI, 2023). Full model hyperparameters and data\ninformation can be found in Appendix A.\n4.1 Experiments on 8K-Token Sequences\nSetting We initialize all models with the 2.7B-\nparameter OPT model and fine-tune on 2B tokens\nfrom 4 domains form the Pile (Gao et al., 2020).\nOur AutoCompressor uses κ = 50 summary to-\nkens and is fine-tuned with summary accumulation\nover four segments, each ranging from 1,024 to\n2,048 tokens. Compressing 2,048 tokens into 50\nsummary vectors achieves a compression rate of 40\ntokens per summary vector. We use the following\nbaselines:\n1. We fine-tune an OPT-2.7B baseline on our data.\nThis model is limited to sequences of 2,048\ntokens due to pre-training.\n2. Extended full attention: We fine-tune OPT-2.7B\non sequences of up to 4,096 tokens by extending\nthe model’s positional embeddings. We initial-\nize the embeddings for positions [2049..4096]\nwith the embeddings for positions[1..2048]. We\nare not able to extend the context beyond 4,096\ntokens due to GPU memory limitations.\n3. RMT-2.7B: We fine-tune an RMT model on our\ndata with κ= 50summary vectors.\nWe evaluate on documents of 8,192 tokens,\ndrawn from the 4 training domains or 4 held-out\ndomains. We generate summary vectors for up\nto 3 segments of 2,048 tokens, but also for single\nsegments as short as 128 tokens. For the extended\nfull-attention baseline we prepend the previous con-\ntext tokens to the context window.\nResults We show the results in Table 1. We find\nthat the AutoCompressor benefits from long con-\ntexts of 6,144 tokens and consistently outperforms\nthe RMT model.\nWe also find that the AutoCompressor benefits\nfrom much shorter sequences than seen during\ntraining, unlike RMT. See also Figure 2 and Ta-\nble 6 for the usefulness of randomized segmenting.\nWhile extended full attention performs the best\non 4,096-long sequences, we observe a trade-off for\nshorter contexts where AutoCompressors achieve\nthe best performance. We also stress that the\nAutoCompressor attends to at most 150 additional\nsoft prompts during evaluation, whereas the full at-\ntention model is given an additional 2,048 tokens.\nThese trends hold for both in-domain and out-\nof-domain evaluation. However, the gap between\nthe AutoCompressor and the full-attention baseline\nincreases in the out-of-domain setting, suggesting\nthat the summary vectors generalize slightly less\nthan pre-trained attention heads.\n4.2 Experiments on 30K-Token Sequences\nSetting We fine-tune OPT-1.3B and OPT-2.7B as\nAutoCompressors on 2B tokens but train on se-\nquences of 30,720 tokens with 20 compression\nsteps.4 We use 50 summary tokens, randomized\nsegmenting, and stop-gradients as before. We also\n4Due to the scarcity of very long sequences in the Pile,\nwe only train on data from the Books3 domain, and use the\nGutenberg domain as out-of-domain evaluation.\n3832\nIn-domain Out-of-domain\nSegments ———– 1 ———– – 2 – – 3 – ———– 1 ———– – 2 – – 3 –\nContext tokens 128 512 2048 4096 6144 128 512 2048 4096 6144\nExtended FA† 6.33†↑1.0% 6.15†↓2.1% 5.94†↓5.4% - - 8.57 †↑0.5% 8.28†↓2.9% 7.93†↓7.0% - -\nRMT 6.42 ↑2.2% 6.19↓1.4% 6.02↓4.1% 6.02↓4.1% 6.01↓4.3% 8.76↑2.7% 8.44↓1.1% 8.21↓3.8% 8.20↓3.9% 8.20↓3.9%\nAutoCompressor 6.14↓2.2% 6.04↓3.8% 5.98↓4.8% 5.94↓5.4% 5.93↓5.6% 8.39↓1.6% 8.26↓3.2% 8.17↓4.2% 8.12↓4.8% 8.10↓5.0%\nTable 1: Held-out perplexity on 2,048 tokens, while varying the length of the preceding context (all the experiments\nare based on OPT-2.7B models). For RMT and AutoCompressor, we condition on summary vectors. We also report\nthe perplexity gains compared to the fine-tuned OPT baseline without extra context, which achieves 6.28 in-domain\nand 8.53 out-of-domain (gains shown in colored numbers). †: Although the extended full attention (Extended\nFA) achieves similar or slightly better perplexity, it uses up to 2,048 additional tokens and cannot extend further.\nHowever, the AutoCompressor uses only 50 ×3 = 150summary vectors to process 6,144 context tokens.\nSegments – 0 – – 7 – – 14 – CUDA\nContext tokens 0 14336 28672 memory\nRMT-1.3B 13.18 12.50 12.50 54GB\nAutoCompressor-1.3B 13.21 12.4912.47 38GB\nRMT-2.7B - - - OOM\nAutoCompressor-2.7B 11.86 11.2111.18 75GB\nTable 2: Evaluation results for AutoCompressors trained\non sequences of 30,720 tokens and evaluated on Books3\n(in-domain) and Gutenberg (out-of-domain). We train\nwith a single NVIDIA A100 GPU and report the CUDA\nmemory required for fine-tuning using a single sequence\nper batch. AutoCompressors require less memory be-\ncause we stop gradients after two segments.\nSegments – 0 – ––––––– 1 –––––– – 2 – – 3 –\nContext tokens 0 128 512 2048 4096 6144\nLlama-2 5.52 5.30 5.15 4.98 - -\nExtended FA 5.40 5.19 5.06 4.88 4.80 4.76\nAutoCompressor 5.40 5.23 5.16 5.11 5.08 5.07\nTable 3: Evaluation results for our AutoCompressor\ntrained from Llama-2 7B on sequences of 6,144 tokens.\nFor the AutoCompressor, we condition on summary\nvectors. For Llama-2 and the Extended Full Attention\n(Extended FA), we condition on plain text tokens.\nfine-tune an RMT model from OPT-1.3B, to use\nas a baseline. We are not able to fine-tune a 2.7B-\nparameter RMT baseline because the RMT method\nleads to an out-of-memory error.\nAll models are evaluated on the final 2,048 held-\nout tokens of documents of size 30,720 tokens by\ncompressing all previous 2,048-token segments.\nResults We collect our results in Table 2. The\nevaluation shows that both AutoCompressor mod-\nels learn to utilize the entire 28K tokens to reduce\nperplexity, while the RMT baseline does not benefit\nfrom doubling the number of context tokens from\n14K to 28K. This shows that summary accumula-\ntion effectively captures long-range dependencies\nin documents. We also report the CUDA memory\nrequirements for fine-tuning each model in Table 2.\nWe train with one NVIDIA A100 GPU with 80GB\nof memory. Stopping gradients reduces CUDA\nmemory and makes it possible to fine-tune an Au-\ntoCompressor from OPT-2.7B, while fine-tuning\nwith RMT leads to out-of-memory at that scale.\n4.3 Scaling Up AutoCompressors to Llama-2\nSetting We fine-tune a 7B-parameter Llama-2\nmodel as an AutoCompressor on a single GPU\nby freezing the model and optimizing only the sum-\nmary token embeddings and the attention weights\nvia LoRA (Hu et al., 2022). The model is trained on\n15B tokens from RedPajama (TogetherAI, 2023),\nsplit into sequences of 6,144 tokens, and we use\n50 summary tokens, randomized segmenting, and\nstop-gradients. We also fine-tune an Extended Full\nAttention baseline on the same dataset. The context\nwindow of the pre-trained model is extended by in-\ncreasing the θvalue in RoPE following (Rozière\net al., 2023).\nWe compare both models to the pre-trained\nLlama-2-7B model, which has a context window\nof 4,096 tokens. All models are evaluated on the\nfinal 2,048 tokens of 8,192-token documents.\nResults We collect our results in Table 3. The\nAutoCompressor benefits from the entire context\nto reduce perplexity: compressing a 4,096-token\ncontext into 100 summary vectors achieves simi-\nlar perplexity to the Extended Full Attention base-\nline with 512 plain text tokens, and compressing\na 6,144-token context into 150 summary vectors\nfurther improves perplexity slightly. Moreover, we\nfind that summary vectors preserve perplexity when\nshort contexts are compressed.\nHowever, Llama-2 and the Extended Full At-\n3833\n0 1000 2000 3000 4000 5000 6000\nNumber of context tokens\n6.8\n7.0\n7.2\n7.4\n7.6\nPPL\nAutoCompressor\nAutoCompressor w/o summary accumulation\nAutoCompressor w/o randomized segmenting\nAutoCompressor w/o stop-gradients\nRMT\nFigure 2: Perplexity on 2048 held-out tokens given dif-\nferent numbers of compressed tokens. Compression is\nperformed on up to 3 segments of 2048 tokens. Abla-\ntions show that the different components of our fine-\ntuning strategy help boost performance and that stop-\ngradients do not impact performance.\ntention baseline outperform the AutoCompressor\nwhen longer contexts are provided. Further re-\nsearch is needed to construct summary vectors that\npreserve all of the context information.\n4.4 Analysis\nAblations We train OPT-2.7B models without ran-\ndomized segmenting, summary accumulation, or\nstop gradients. The results are shown in Figure 2.\nWe find that randomized segmenting leads to better\ncompression of short segments, but still improves\nperplexity when compressing multiple 2048 token\nsegments. As expected, summary accumulation\nhelps improve perplexity beyond one compressed\nsegment. We also confirm that stopping gradients\ndoes not impact performance despite reducing GPU\nmemory requirements. In Table 2, we also show\nthat stopping gradients helps reduce GPU memory.\nWe also train AutoCompressors with κ = 20,\n50, 70 or 100 summary tokens and report the held-\nout perplexity results in Table 7 in the Appendix.\nSurprisingly, we find that performance does not\nincrease with longer soft prompts, and κ = 50\nperforms the best overall. We hypothesize that\nlearning a larger number of summary vectors may\nrequire a larger training budget.\nToken-level analysis We seek to better understand\nhow summary vectors benefit individual token pre-\ndictions. In Figure 5 in the Appendix, we show\nperplexity gains at each token position for the Au-\ntoCompressor with summary vectors and for the\nextended full-attention baseline.\nWe find that conditioning on summary vectors\nimproves perplexity over all 2048 token positions.\nWe observe that the extended full attention baseline\noutperforms the AutoCompressor at the start of the\nsequence, whereas the AutoCompressor achieves\nthe best performance towards the end of the se-\nquence. This shows that summary vectors effec-\ntively capture long-range textual dependencies.\nIn Appendix D, we show examples of sentences\nand tokens which benefit the most from summary\nvectors. We find that summary vectors contain\nsalient information, such as names or dates, and\nthat the model can reason over summary vectors.\nThis confirms that summary vectors are useful sum-\nmaries of the compressed text.\n5 Compressing Demonstrations for\nIn-Context Learning\nIn this section, we study the usefulness of summary\nvectors for performing downstream tasks. We show\nthat in-context demonstrations can reliably be com-\npressed down into summary vectors to improve\nperformance while also increasing efficiency on a\ndiverse set of NLP benchmarks.\nEvaluation We evaluate the in-context learning\nabilities of the AutoCompressor based on Llama-\n2-7B from Section 4.3 on eleven classification and\nmultiple-choice question-answering datasets. For\neach dataset, we evaluate the effect of compressing\n1, 2 or 3 segments of demonstrations into 50, 100\nor 150 summary vectors. For each segment, we\ninclude as many demonstrations as possible until\nwe reach 750 tokens. For SST-2, this corresponds\nto 30 demonstrations per segment on average. We\ncompare this compression approach with the results\nobtained by prompting the model using 150 and\n750 tokens’ worth of plain-text demonstrations.\nWe use contextual calibration (Zhao et al., 2021)\nand class-balanced sampling when these techniques\nimprove performance on a validation set. For each\ndataset, we report the mean accuracy and standard\ndeviation over 7 random seeds. The detailed set-\ntings for each dataset can be found in Table 11.\nIn Table 12 in the Appendix, we also compare\nthe ICL performance of our OPT-2.7B based Au-\ntoCompressor models against the RMT baseline\nand a pre-trained OPT-2.7B, and include the per-\nformance of the pre-trained Llama-2-7B model.\nResults We show evaluation results in Table 4.\nResults show that summary vectors consistently\nimprove performance over the zero-shot baseline.\nFurthermore, summary vectors increase accuracy\n3834\nAG News SST-2 BoolQ WIC WSC RTE CB COPA MultiRC MR Subj\nZero-shot63.3(0.0) 67.7(0.0) 67.4(0.0) 50.8(0.0) 43.3(0.0) 58.8(0.0) 42.9(0.0) 52.5(0.0) 52.5(0.0) 57.4(0.0) 49.3(0.0)\n50 summary vecs.79.6(4.9) 94.2(1.6) 70.1(3.3) 51.6(2.1) 47.7(8.7) 66.3(7.0) 46.4(18.7) 84.5(1.0) 52.6(2.8) 91.5(1.0) 53.5(3.6)\n100 summary vecs.87.6(1.2) 92.6(3.3) 66.3(2.8) 52.5(2.2) 42.9(2.5) 63.5(6.6) 64.5(5.9) 85.9(0.4) 56.1(1.2) 90.7(2.6) 57.0(5.6)\n150 summary vecs.85.4(3.4) 92.3(2.9) 68.0(1.8) 52.8(1.5) 49.9(7.6) 65.3(6.6) 54.8(5.8) 86.1(0.6) 54.8(2.2) 91.1(2.2) 56.6(7.9)\nICL (150 tokens)74.5(2.2) 92.4(3.1) 67.4(0.0) 52.4(2.7) 51.8(6.9) 69.1(2.1) 46.4(23.0) 80.0(1.9) 52.5(0.0) 79.7(15.7) 57.9(10.7)\nICL (750 tokens)81.2(4.1) 93.8(1.2) 67.7(2.7) 52.4(2.0) 40.0(5.7) 73.1(3.5) 50.3(2.8) 82.6(1.6) 47.0(3.2) 91.6(0.8) 60.7(14.8)\nTable 4: Evaluation of the ICL performance of the Llama-2 7B model. Each summary is 50 tokens-long and\ncorresponds to a segment of 750 tokens’ worth of demonstrations. We also report accuracies when prompting the\nAutoCompressor with 150 and 750 tokens’ worth of plaintext demonstrations as baselines. Note that for BoolQ and\nMultiRC, demonstrations are too long to fit into 150 tokens.\ncompared to 150 tokens worth of plain demonstra-\ntions on 8/11 tasks. On 8 tasks (AG News, SST-2,\nBoolQ, WiC, WSC, CB, COPA and MultiRC), sum-\nmary vectors also out-perform ICL with 750 tokens’\nworth of plain text demonstrations. Summary vec-\ntors emerge as a strong alternative to plain text\ndemonstrations, as they increase accuracy while\nreducing inference cost.\nIn Table 12 (Appendix E), we find that the OPT-\n2.7B AutoCompressor achieves higher accuracies\nthan the RMT baseline on 8 out of 11 tasks and\nthat the RMT model does not benefit from multi-\nple compression steps. This shows that summary\naccumulation is an effective mechanism for com-\npressing in-context demonstrations. We also ob-\nserve that our fine-tuned Llama-2 AutoCompressor\nhas substantially worse zero-shot accuracy on some\ntasks compared to the Llama-2 initialization, and\nslightly worse ICL performance. We suspect that\nthis is due to domain mismatch in our fine-tuning\ndata and the Llama-2 pre-training corpus.\n6 Compressing Retrieval Corpora for\nEfficient Inference\nWe study the usefulness of pre-computing sum-\nmary vectors for large collections of documents.\nThese can be stored and later retrieved for efficient\ninference. Since inference is typically more expen-\nsive than storage, this approach has the potential to\nachieve good practical trade-offs.\n6.1 Retrieval-augmented Language Modeling\nRetrieval-augmented language models improve to-\nken predictions by retrieving information from a\ndata store. A number of approaches have been pro-\nposed to infuse external knowledge in the input\nlayer (Guu et al., 2020; Shi et al., 2023), intermedi-\nate layers (Borgeaud et al., 2022) or at the output\nlayer (Khandelwal et al., 2020; Zhong et al., 2022).\nPre-process Corpus\nAutoCompressor\nAutoCompressor\nO�f-the-shelf\nRetriever\nRetrieval-Augmented LM\nPrompt\nGenerate...\npre-computed\nsummary\nvectorsFetch \ntop-k\nFigure 3: Efficient retrieval-augmented language mod-\neling with AutoCompressors. Large corpora can be\npre-processed into compressed summary vectors which\ncan be stored cheaply. Upon retrieval, compressed sum-\nmaries are fused for efficient access to multiple docu-\nments in a single forward pass.\nREPLUG Our case study focuses on REPLUG\n(Shi et al., 2023), which is a simple method for com-\nbining a pre-trained language model with an off-\nthe-shelf retriever to improve language modeling\nperformance. Given access to an external corpus C,\nREPLUG retrieves kpassages D= {d1,...,d k}\nbased on a segment xto score the next segment y.\nThe overall probability foryis computed by ensem-\nbling the predictions based on different passages:\np(y|x,D) =\n∑\nd∈D\nλ(d,x) ·p(y|CONCAT (d,x)),\nwhere λ(d,x) are the normalized similarity scores\nfrom the retriever and CONCAT (d,x) denotes con-\ncatenation of pand x. This method incurs a sub-\nstantial overhead, since it requireskforward passes\nover sequences CONCAT (d,x,y ).\nFused Summaries We introduce a setting for\nretrieval-augmented language modeling close to\nfusion-in-decoder (Izacard and Grave, 2021). We\nconcatenate the summary vectors of retrieved pas-\nsages Dto form the fused summary vectors, σD=\nCONCAT (σdk ,...,σ d1 ), where dk,...,d 1 are or-\ndered from least-to-most relevant. This resembles\n3835\nPerplexity Gain(%) Throughput(examples/s)\nPassages top- 1 top-2 top-5 top-10 top-1 top-2 top-5 top-10\n50 tokens REPLUG -0.64 0.58 1.68 2.35 51 38 16 9\n50 tokens Fused Passages 0.71 1.01 1.70 2.60 28 27 23 17\n512 tokens→50 sum. vecs. Fused Summaries 1.04 1.67 2.63 3.74 28 27 23 17\n512 tokens REPLUG -1.47 2.24 5.25 8.30 18 10 6 3\nTable 5: PPL gains (%) from different retrieval-augmented language modeling settings, over the no-retrieval baseline.\nWe evaluate the OPT-2.7B AutoCompressor and we report throughput on a single NVIDIA A100 GPU for each\nmethod without batching examples. Fused Summaries outperforms Fused Passages and REPLUG with 50-token\npassages. Moreover, Fused Summaries top-10 outperforms REPLUG top-2 with 512-token passages while also\ngaining a 1.7×throughput increase.\nsummary accumulation as described in Section 3.\nWe also find it useful to smooth probability scores\nand re-order the retrieved passages based on their\nsummary vectors (Appendix F). Figure 3 gives an\noverview of our approach.\nFused Passages We establish a baseline for fus-\ning summary vectors by concatenating the plain-\ntext passages and computing smoothed probabili-\nties, see Appendix F. Unlike summary vectors, this\nmethod is limited by the model’s context window.\nExperiments We evaluate the OPT-2.7B Auto-\nCompressor introduced in Section 4.1 without\nany additional fine-tuning. Similar to Shi et al.\n(2023), we retrieve from the Pile. We use Books3,\nFreeLaw, GitHub, Wikipedia, Gutenberg, ArXiv,\nHackerNews, and YoutubeSubtitles. We index 10B\ntokens for each domain, which are split into pas-\nsages of 512 or 50 tokens.\nWe sample segments of 256 tokens from the Pile\nvalidation data, using the first 128 tokens as context\nxfor retrieval and the last 128 tokens yfor evalua-\ntion. We use the Contriever model (Izacard et al.,\n2022) for retrieval, and retrieve the top 10 passages.\nWe also deduplicate our data by removing passages\nthat overlap with xby 64 tokens.\nResults Results are shown in Table 5. We find that\nFused Summaries outperforms Fused Passages and\nREPLUG when 50-token passages are retrieved.\nWe measure throughput empirically and show that\nfor 10 retrieved documents, Fused Summary Vec-\ntors remains inexpensive. We note that compress-\ning the 10B token datasets results in disk space of\n5TB per domain when stored in half-precision for-\nmat.5 Therefore Fused Summaries achieves a good\ntrade-off between storage costs and throughput.\n5For comparison, storing the transformer output at every\nsingle token (e.g., in an encoder-decoder setting) would take\nup 51 TB, and storing all attention states would be 3,276 TB.\nMoreover, Fused Summaries outperforms RE-\nPLUG top-2 with 512-token passages and sees a\n1.7x throughput increase, which shows that the\nmodel benefits from the diversity of compressed\ndocuments. However, REPLUG top-10 outper-\nforms Fused Summaries. We leave it as future work\nto explore how to produce higher quality summary\nvectors to better utilize the compressed passages.\nWe note that fusing summary vectors is effec-\ntive despite a mismatch in training since we draw\nindependent summary vectors from separate docu-\nments. Furthermore, our AutoCompressor model\nis only ever trained to accumulate 3 sets of sum-\nmary vectors, and yet it benefits from fusing the\nsummary vectors of up to 10 documents.\n6.2 Unsupervised Passage Re-ranking\nFinally, we consider the case study of passage re-\nranking, in which a fast off-the-shelf retriever like\nBM25 retrieves a large set of candidate passages,\nand a more capable re-ranker refines the ranking to\nincrease the rank of the most relevant passages.\nMethod Sachan et al. (2022) introduce an effec-\ntive method for leveraging language models as\nre-rankers with no additional supervision or fine-\ntuning. Given a query qand a set of candidate pas-\nsages {p1,...,p k}, the language model scores the\nlikelihood of the queryqconditioned on the prompt\n“Passage: {pi}. Please write a question\nbased on this passage.” for each passage pi\nand re-ranks the passages based on the scores.\nExperiments We consider the task of re-ranking\nBM25 passages on the NQ test set (Balachandran\net al., 2021) and compare out-of-the-box AutoCom-\npressors with 20 and 50 summary tokens to pre-\ntrained OPT models from 125M to 2.7B parameters.\nWe pre-compute summary vectors for 21M pas-\nsages from a Wikipedia corpus (Karpukhin et al.,\n3836\n0 2 4 6 8 10 12\nRe-ranking throughput (queries/s)\n68\n69\n70\n71\n72\n73\n74\n75Recall@20\n50\n60\n70\n100\n250\n50\n100\n150\n250\n1000\n50\n100\n250\n1000\n50\n100\n250\n1000\n50\n100\n250\n50\n100\n250\nPareto front\nAutoCompressorκ=20\nAutoCompressorκ=50\nOPT-2.7B\nOPT-1.3B\nOPT-350M\nOPT-125M\nFigure 4: We compare AutoCompressors (squares) in an\nunsupervised passage re-ranking setting to pre-trained\nlanguage models (circles). The number on each data\npoint shows how many passages retrieved by BM25 are\nre-ranked, and the vertical axis shows the Recall@20\nperformance of the re-ranking system on the NQ test\nset. We consider the throughput on a single NVIDIA\nA100 GPU and assume that multiple queries cannot be\nbatched. By leveraging pre-computed summary vectors\nfor passages, AutoCompressors lead to re-ranking solu-\ntions that lie on the Pareto front of recall vs. compute.\n2020), which requires 2.1TB and 5.4TB disk space\nin half precision for 20 and 50 summary vectors re-\nspectively. We measure the quality of the re-ranked\nresults using Recall@20.\nResults The results are shown in Figure 4. We\nmeasure throughput for individual un-batched\nqueries on a single NVIDIA A100 80GB GPU\nand assume that the latency of loading summary\nvectors is negligible. Although the passages are\nonly 100 words long, resulting in low compres-\nsion rates, summary vectors substantially speed up\nthe inference, while sacrificing on performance less\nthan smaller models. This leads to a Pareto-optimal\ntrade-off between compute and performance and\ndemonstrates that summary vectors often retain\nsufficient information from a passage to assess its\nrelevance for a particular query.\n7 Conclusion\nWe have introduced a training strategy for adapting\npre-trained LMs into AutoCompressors, which re-\ncursively compress contexts into summary vectors.\nOur experiments indicate that summary vectors re-\ntain important contextual information, that they can\nencode in-context demonstrations, and that they\ncan be used in retrieval settings. Summary vec-\ntors can also be pre-computed, cached and re-used.\nThis offers practical efficiency gains by reducing\nthe size of the attention window. Significant future\nwork remains in scaling AutoCompressors to big-\nger models and improving the quality of summary\nvectors to further close the gap with full attention\nover long-range contexts.\nLimitations\n1. We only apply AutoCompressors to OPT mod-\nels of up to 2.7B parameters and a Llama model\nof 7B parameters. Future work needs to estab-\nlish how AutoCompressors perform for even\nlarger models. As the summary vector dimen-\nsion grows, there is promise for retaining more\ninformation per vector.\n2. Our results suggest that summary vectors ig-\nnore some useful information that is accessible\nvia full attention. Additionally, models do not\nalways benefit from increasing the number of\nsummary vectors. We suspect that the training\nsignal for learning summary vectors efficiently\nmight be limited by pre-trained models being\nvery good at making predictions from the plain-\ntext tokens in the current segment. Future work\nis needed to improve this optimization.\n3. Summary accumulation still leads to quadratic\ncomplexity with increasing number of segments,\nalbeit at a much lower rate than full attention.\nFuture work may explore ways to combine\nmany summary vectors more efficiently.\nAcknowledgments\nWe thank Mengzhou Xia, Howard Chen, Vishvak\nMurahari, Aatmik Gupta, Zirui Wang, Jiatong Yu,\nand the members of the Princeton NLP group for\nhelpful discussion and valuable feedback. This re-\nsearch is supported by an NSF CAREER award\n(IIS-2239290), a Sloan Research Fellowship, and\na Data Science Research Award from Adobe. AC\nalso gratefully acknowledges support from the Min-\nerva Research Foundation.\nReferences\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago On-\ntañón, Siddhartha Brahma, Yury Zemlyanskiy, David\nUthus, Mandy Guo, James Lee-Thorp, Yi Tay, Yun-\nHsuan Sung, and Sumit Sanghai. 2023. CoLT5:\nFaster long-range transformers with conditional com-\nputation.\n3837\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Ben Mann, Nova DasSarma, et al. 2021. A\ngeneral language assistant as a laboratory for align-\nment. arXiv preprint arXiv:2112.00861.\nVidhisha Balachandran, Bhuwan Dhingra, Haitian Sun,\nMichael Collins, and William Cohen. 2021. Inves-\ntigating the effect of background knowledge on nat-\nural questions. In Proceedings of Deep Learning\nInside Out (DeeLIO): The 2nd Workshop on Knowl-\nedge Extraction and Integration for Deep Learning\nArchitectures, pages 25–30, Online. Association for\nComputational Linguistics.\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The fifth PASCAL recognizing\ntextual entailment challenge. In TAC.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International conference on ma-\nchine learning, pages 2206–2240. PMLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nAydar Bulatov, Yuri Kuratov, and Mikhail Burtsev. 2022.\nRecurrent memory transformer. In Advances in Neu-\nral Information Processing Systems.\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos\nGuestrin. 2016. Training deep nets with sublinear\nmemory cost. arXiv preprint arXiv:1604.06174.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Be-\nlanger, Lucy J Colwell, and Adrian Weller. 2021.\nRethinking attention with Performers. In Interna-\ntional Conference on Learning Representations.\nChristopher Clark, Kenton Lee, Ming-Wei Chang,\nTom Kwiatkowski, Michael Collins, and Kristina\nToutanova. 2019. BoolQ: Exploring the surprising\ndifficulty of natural yes/no questions. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2924–2936, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nIdo Dagan, Oren Glickman, and Bernardo Magnini.\n2005. The PASCAL recognising textual entailment\nchallenge. In the First International Conference on\nMachine Learning Challenges: Evaluating Predic-\ntive Uncertainty Visual Object Classification, and\nRecognizing Textual Entailment.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na fixed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2978–2988, Florence, Italy. Asso-\nciation for Computational Linguistics.\nTri Dao, Daniel Y Fu, Stefano Ermon, Atri Rudra, and\nChristopher Re. 2022. FlashAttention: Fast and\nmemory-efficient exact attention with IO-awareness.\nIn Advances in Neural Information Processing Sys-\ntems.\nMarie-Catherine de Marneffe, Mandy Simons, and Ju-\ndith Tonhauser. 2019. The CommitmentBank: Inves-\ntigating projection in naturally occurring discourse.\nProceedings of Sinn und Bedeutung, 23(2):107–124.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe Pile: An 800GB dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In Proceedings of the\n37th International Conference on Machine Learning,\nvolume 119 of Proceedings of Machine Learning\nResearch, pages 3929–3938. PMLR.\nR Bar Haim, Ido Dagan, Bill Dolan, Lisa Ferro, Danilo\nGiampiccolo, Bernardo Magnini, and Idan Szpektor.\n2006. The second pascal recognising textual entail-\nment challenge. In Proceedings of the Second PAS-\nCAL Challenges Workshop on Recognising Textual\nEntailment, volume 7, pages 785–794.\nEdward J Hu, yelong shen, Phillip Wallis, Zeyuan Allen-\nZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu\nChen. 2022. LoRA: Low-rank adaptation of large\nlanguage models. In International Conference on\nLearning Representations.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Sebas-\ntian Riedel, Piotr Bojanowski, Armand Joulin, and\nEdouard Grave. 2022. Unsupervised dense informa-\ntion retrieval with contrastive learning. Transactions\non Machine Learning Research.\n3838\nGautier Izacard and Edouard Grave. 2021. Leveraging\npassage retrieval with generative models for open do-\nmain question answering. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume,\npages 874–880, Online. Association for Computa-\ntional Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2020. Generalization\nthrough memorization: Nearest neighbor language\nmodels. In International Conference on Learning\nRepresentations.\nDaniel Khashabi, Snigdha Chaturvedi, Michael Roth,\nShyam Upadhyay, and Dan Roth. 2018. Looking\nbeyond the surface: A challenge set for reading com-\nprehension over multiple sentences. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers), pages 252–262, New Orleans, Louisiana. As-\nsociation for Computational Linguistics.\nDiederik Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In International\nConference on Learning Representations (ICLR), San\nDiega, CA, USA.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nHector J. Levesque, Ernest Davis, and Leora Morgen-\nstern. 2012. The winograd schema challenge. In 13th\nInternational Conference on the Principles of Knowl-\nedge Representation and Reasoning, KR 2012, Pro-\nceedings of the International Conference on Knowl-\nedge Representation and Reasoning, pages 552–561.\nInstitute of Electrical and Electronics Engineers Inc.\n13th International Conference on the Principles of\nKnowledge Representation and Reasoning, KR 2012\n; Conference date: 10-06-2012 Through 14-06-2012.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 4582–\n4597, Online. Association for Computational Lin-\nguistics.\nVladislav Lialin, Vijeta Deshpande, and Anna\nRumshisky. 2023. Scaling down to scale up: A guide\nto parameter-efficient fine-tuning. arXiv preprint\narXiv:2303.15647.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 61–68,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nXuezhe Ma, Chunting Zhou, Xiang Kong, Junxian\nHe, Liangke Gui, Graham Neubig, Jonathan May,\nand Luke Zettlemoyer. 2022. Mega: moving av-\nerage equipped gated attention. arXiv preprint\narXiv:2209.10655.\nJesse Mu, Xiang Lisa Li, and Noah Goodman. 2023.\nLearning to compress prompts with gist tokens.\narXiv preprint arXiv:2304.08467.\nBo Pang and Lillian Lee. 2004. A sentimental education:\nSentiment analysis using subjectivity. InProceedings\nof ACL, pages 271–278.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploiting\nclass relationships for sentiment categorization with\nrespect to rating scales. In Proceedings of ACL, pages\n115–124.\nMohammad Taher Pilehvar and Jose Camacho-Collados.\n2019. WiC: the word-in-context dataset for evalu-\nating context-sensitive meaning representations. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 1267–1273,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nOfir Press, Noah Smith, and Mike Lewis. 2022. Train\nshort, test long: Attention with linear biases enables\ninput length extrapolation. In International Confer-\nence on Learning Representations.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\nmar, Chloe Hillier, and Timothy P. Lillicrap. 2020.\nCompressive transformers for long-range sequence\nmodelling. In International Conference on Learning\nRepresentations.\nMelissa Roemmele, Cosmin Adrian Bejan, and An-\ndrew S Gordon. 2011. Choice of plausible alter-\nnatives: An evaluation of commonsense causal rea-\nsoning. In AAAI Spring Symposium: Logical Formal-\nizations of Commonsense Reasoning.\nBaptiste Rozière, Jonas Gehring, Fabian Gloeckle,\nSten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi\nAdi, Jingyu Liu, Tal Remez, Jérémy Rapin, Artyom\nKozhevnikov, Ivan Evtimov, Joanna Bitton, Manish\nBhatt, Cristian Canton Ferrer, Aaron Grattafiori, Wen-\nhan Xiong, Alexandre Défossez, Jade Copet, Faisal\n3839\nAzhar, Hugo Touvron, Louis Martin, Nicolas Usunier,\nThomas Scialom, and Gabriel Synnaeve. 2023. Code\nllama: Open foundation models for code.\nDevendra Sachan, Mike Lewis, Mandar Joshi, Armen\nAghajanyan, Wen-tau Yih, Joelle Pineau, and Luke\nZettlemoyer. 2022. Improving passage retrieval with\nzero-shot question generation. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3781–3797, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen-tau Yih. 2023. REPLUG: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nCharlie Snell, Dan Klein, and Ruiqi Zhong. 2022.\nLearning by distilling context. arXiv preprint\narXiv:2209.15189.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha,\nBo Wen, and Yunfeng Liu. 2022. Roformer: En-\nhanced transformer with rotary position embedding.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald Met-\nzler. 2022. Efficient transformers: A survey. ACM\nComput. Surv., 55(6).\nTogetherAI. 2023. RedPajama: An open source recipe\nto reproduce llama training dataset.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. LLaMA: Open and ef-\nficient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel Bowman. 2019. SuperGLUE: A stickier\nbenchmark for general-purpose language understand-\ning systems. In Advances in Neural Information\nProcessing Systems, volume 32. Curran Associates,\nInc.\nDavid Wingate, Mohammad Shoeybi, and Taylor\nSorensen. 2022. Prompt compression and contrastive\nconditioning for controllability and toxicity reduction\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2022, pages\n5621–5634, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOPT: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems, volume 28. Curran Associates, Inc.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference\non Machine Learning, volume 139 of Proceedings\nof Machine Learning Research, pages 12697–12706.\nPMLR.\nLin Zheng, Chong Wang, and Lingpeng Kong. 2022.\nLinear complexity randomized self-attention mech-\nanism. In Proceedings of the 39th International\nConference on Machine Learning , volume 162 of\nProceedings of Machine Learning Research, pages\n27011–27041. PMLR.\nZexuan Zhong, Dan Friedman, and Danqi Chen. 2021.\nFactual probing is [MASK]: Learning vs. learning\nto recall. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 5017–5033, Online. Association\nfor Computational Linguistics.\nZexuan Zhong, Tao Lei, and Danqi Chen. 2022. Train-\ning language models with memory augmentation.\nIn Proceedings of the 2022 Conference on Empir-\nical Methods in Natural Language Processing, pages\n5657–5673, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\n3840\nA Models and Data\nAll models are fine-tuned from OPT models on\nthe Pile. We conduct our experiments using a sin-\ngle NVIDIA A100 80GB GPU and we use Flash\nAttention (Dao et al., 2022) as an efficient imple-\nmentation of exact attention over long sequences.\nWe also use gradient checkpointing between com-\npressed segments to reduce GPU memory.\nA.1 OPT Experiments on 8K Tokens\nWe fine-tune our models on 2B tokens from the\nPile. We sample 500M tokens from the following\nPile subdomains: Books3, FreeLaw, GitHub and\nWikipedia.\nThe following models use a learning rate of 2e-5,\na batch size of 130K tokens, 1,00 warm-up steps,\nand the Adam optimizer (Kingma and Ba, 2015):\n1. The fine-tuned OPT-2.7B baseline is fine-tuned\non documents of up to 2,048 tokens.\n2. The extended full-attention baseline is fine-\ntuned on documents of up to 4,096 tokens by ex-\ntending the positional embeddings of OPT-2.7B\nto 4,096 positions. We initialize the embeddings\nfor positions [2049..4096] with the embeddings\nfor positions [1..2048].\n3. The RMT baseline is fine-tuned on documents\nof up to 8,192 tokens. Each document is seg-\nmented into four segments of 2,048 tokens. We\nuse κ= 50summary vectors but we do not use\nsummary accumulation, randomized segment-\ning, or stop-gradients.\n4. Our AutoCompressor is fine-tuned on docu-\nments of up to 6,144 tokens. Each document is\nrandomly segmented into four segments such\nthat the first two segments add up to 3,072 to-\nkens. The length of each segments ranges from\n1,024 to 2,048 tokens. We use κ= 50summary\nvectors and summary accumulation. We stop\ngradients every two compression steps.\nAll models are evaluated on documents sam-\npled from the Pile with a fixed length of 8,192\ntokens. We sample 610 documents from each of\nthe following domains: Books3, FreeLaw, GitHub,\nWikipedia (in-domain), and ArXiv, Gutenberg,\nHackerNews, YoutubeSubtitles (out-of-domain).\nExamples of documents from each of those do-\nmains can be found in Tables 9 and 10.\nA.2 OPT Experiments on 30K Tokens\nWe fine-tune our models on 2 billion tokens from\nthe Books3 subdomain of the Pile. All models are\nfine-tuned on documents of up to 30,720 tokens.\nWe use a learning rate of 2e-5, a batch size of\n130k tokens, 1,000 warm-up steps and the Adam\noptimizer.\n1. RMT-1.3B uses κ= 50summary vectors and is\nfine-tuned without summary accumulation, ran-\ndomized segmenting, or stop-gradients. Each\ndocument is split into 15 segments of 2,048 to-\nkens Even with gradient checkpointing, attempt-\ning to fine-tune a 2.7B parameter RMT model\non this dataset leads to an out-of-memory error.\n2. The AutoCompressor models are fine-tuned\nfrom OPT-1.3B and 2.7B on documents of up\nto 30,720 tokens. Each document is split into\n20 segments such that segment 2iand segment\n2i+1 add up to 3,072 tokens. The length of each\nsegment is randomly sampled between 1,024\nand 2,048. We use κ = 50 summary vectors\nwith summary accumulation and we stop gradi-\nents every two compression steps.\nAll models are evaluated on documents of\n30,720 tokens from the Pile. We use 1,000 doc-\numents from Books3 (in-domain) and 1,000 docu-\nments from Gutenberg (out-of-domain).\nA.3 Llama-2 Experiments on 8K Tokens\nWe fine-tune our Llama-2 models on 15B tokens\nfrom RedPajama. We sample 1B tokens from long\ndocuments in ArXiv, Books, C4, GitHub, as well\nas 10B tokens from CommonCrawl, 800M from\nWikipedia and 70M tokens from StackExchange.\nBoth our AutoCompressor and our Extended\nFull Attention baseline are fine-tuned from Llama-\n2-7B on sequences of 6,144 tokens with LoRA (Hu\net al., 2022) parameter efficient fine-tuning applied\nto the attention heads. We use a LoRA dimension\nof 16 applied to the QKV- and Out-projections. We\nuse a learning rate of 4e-4, a batch size of 200K\ntokens, 5,000 warm-up steps and the Adam opti-\nmizer. For the AutoCompressor, we also optimize\nthe newly initialized summary token embeddings.\nWe train our AutoCompressor in the same way\nas the OPT-2.7B AutoCompressor, with κ = 50,\nrandomly segmenting each sequence into four sem-\ngents, and stopping gradients every two compres-\nsion steps. The Extended Full Attention baseline is\nfine-tuned with a RoPE θvalue of 80,000.\nWe evaluate our models on 500 sequences of\n8,192 tokens from each of ArXiv, Books, C4,\nGitHub, StackExchange, and 5,000 sequences from\nCommonCrawl.\n3841\nB No-context Language Modeling\nIn Table 6, we verify that our fine-tuning strategy\ndoes not significantly affect the language modeling\ncapabilities of the OPT AutoCompressors when no\nsummary tokens are given. We find that the Auto-\nCompressor performs slightly better than the RMT\nmodel and significantly better than the extended\nfull attention model when no additional context\nis given. Moreover, the AutoCompressor almost\nmatches the OPT02.7B fine-tuned baseline, with\nperplexity increasing by less than 1%.\nIn-domain Out-of-domain\nOPT-2.7B 7.53 ↑19.9% 9.19↑7.7%\nOPT-2.7B fine-tuned 6.28 8.53\nAutoCompressor-2.7B 6.31↑0.5% 8.60↑0.8%\nRMT-2.7B 6.34 ↑1.0% 8.62↑1.1%\nExtended full attention 6.57↑6.4% 8.94↑4.8%\nTable 6: Held-out perplexity of all models on 2048\ntokens without summary vectors or additional context.\nC AutoCompressor Ablations\nWe train OPT AutoCompressor models as in Sec-\ntion 4.1 while varying κ = 20,50,70,100. In\nTable 7, we report the perplexity evaluation on\ndocuments of 8192 tokens across all evaluation\ndomains.\nCompressed tokens\nκ 0 2048 4096 6144\n20 7.36 7.05 7.01 7.00\n50 7.37 6.99 6.94 6.93\n70 7.41 7.01 6.97 6.95\n100 7.48 7.07 7.01 7.00\nTable 7: Held-out perplexity across all evaluation do-\nmains for AutoCompressors based on OPT-2.7B trained\nwith different numbers of summary tokens κ. We ob-\nserve that κ= 50performs the best overall.\nD Token-level AutoCompressor Analysis\nIn Figure 5, we plot the perplexity gains achieved\nby the OPT AutoCompressor and the extended\nfull attention baseline from Section 4.1 over the\npre-trained OPT-2.7B model. We plot the gains\nachieved by the AutoCompressor both without any\nadditional context and with the summary vectors\nobtained from 2048 compressed tokens.\nResults show that the summary vectors help re-\nduce perplexity over the entire 2,048-token seg-\nment. This shows that summary vectors do not\nonly contain information which helps continue the\nprevious sequence.\nFigure 5 also shows that the extended full-\nattention baseline benefits more from the additional\n2,048 context tokens than the AutoCompressor at\nthe start of the sequence, but that the AutoCom-\npressor achieves stronger gains at the end of the\nsequence. This shows that summary vectors ef-\nfectively capture long-range textual dependencies\nand that fine-tuning AutoCompressors produces\nmore robust models than fine-tuning extended full-\nattention models.\n0 400 800 1200 1600 2000\nToken position\n0\n10\n20\n30\n40\n50\n60\n70\n80PPL gain (%) →\nExtended full attn. (2048 tokens)\nAutoCompressor (2048 tokens)\nAutoCompressor (no context)\nFigure 5: We plot the perplexity gain over OPT-2.7B\nfor our AutoCompressor model and the 4096-extended\nattention baseline. We track the perplexity at each to-\nken position in sequences of 2048 tokens. The Auto-\nCompressor model almost matches the strong extended-\nattention baseline at the start of sequences and outper-\nforms it at the end of sequences.\nIn Tables 9 and 10, we give hand-picked exam-\nples of sequences from each evaluation domain,\nhighlighting which tokens benefit the most from\nthe compressed context. We compress the first 300\ntokens in every document from the evaluation set\nand evaluate on the following 100 tokens. In the\nnotation of Section 3.1, we measure the perplexity\ngain of each token as\np(x2\nt |x2\n1,...,x 2\nt−1,σ1)\np(x2\nt |x2\n1,...,x 2\nt−1) .\nFor each example, we record the top 3-5 most im-\nproved token predictions.\nWe find that the tokens which benefit the most\nfrom the summary vectors are often interpretable.\nNames of characters, dates, and locations are often\n3842\ncopied through the summary vectors (see the ex-\namples for Wikipedia, FreeLaw, or HackerNews).\nWe also find that the model is able to reason over\nthe summary vectors, as the tokens which benefit\nthe most are sometimes not explicitly present in\nthe compressed context, but are closely associated\nwith the domain of speech (see the examples for\nBooks3, Gutenberg and YoutubeSubtitles.). Finally,\nwe find that summary vectors are often useful for\ncontinuing the previous sentence (see the GitHub\nexample.)\nE In-Context Learning Details\nWe evaluate on in-context examples of the follow-\ning datasets: AG News (topic classification, Zhang\net al. (2015)), SST-2 (sentiment analysis, Socher\net al. (2013)), BoolQ (Boolean Questions, Clark\net al. (2019)), WiC (Word-in-Context, word sense\ndismabiguation, Pilehvar and Camacho-Collados\n(2019)), WSC (Winograd Schema Challenge, coref-\nerence resolution, Levesque et al. (2012)), RTE\n(Recognizing Textual Entailment, Dagan et al.\n(2005); Haim et al. (2006); Bentivogli et al. (2009)),\nCB (CommitmentBank, de Marneffe et al. (2019)),\nCOPA (Choice of Plausible Alternatives, Roem-\nmele et al. (2011)), MultiRC (Multi-Sentence Read-\ning Comprehension, Khashabi et al. (2018)), MR\n(Movie Reviews, Pang and Lee (2005)), Subj (Sub-\njectivity, Pang and Lee (2004). We follow the GPT-\n3 prompt templates (Brown et al., 2020) and detail\nour evaluation setting for OPT and Llama-2 in Ta-\nble 11.\nIn Table 12, we compile evaluation results for\nOPT-2.7B, Llama-2-7B, as well as our AutoCom-\npressor and RMT models.\nF Fused Retrieval-augmented Language\nModeling\nPerplexity Gain (%)\nPassages top- 1 top-2 top-5 top-10\nFused Summaries 1.04 1.67 2.63 3.74\nFused Summaries w/o re-ranking 1.04 1.52 2.02 2.63\nTable 8: PPL gains ( %) over the no-retrieval baseline\nfor Fused Summary with and without re-ranking. In\nre-ranking, we order the passages based on theℓ2 norms\nof their summary vectors before concatenating the sum-\nmary vectors, whereas w/o re-ranking we use the re-\ntrieval scores from the Contriever model. Re-ranking\nconsistently produces higher perplexities.\nWe provide details and ablations for our pro-\nposed REPLUG alternative. Inspired by fusion-in-\ndecoder (Izacard and Grave, 2021), we fuse sum-\nmary vectors or passages in a single forward pass.\nFused Summary Vectors The summary vectors\nof retrieved passages Dare concatenated in order\nof increasing retrieval scores to form fused sum-\nmary vectors, σD = Concat[σdk ,...,σ d1 ]. This\nresembles summary accumulation as described in\nSection 3, but differs in that the retrieved summary\nvectors were produced independently rather than\nrecursively. Nevertheless, we find that AutoCom-\npressors transfer well to this setting.\nFurthermore, we find it beneficial to smooth the\nconditioned probabilities with the unconditioned\nprobabilities p(y|x), and compute\np(y|x,D) =p(y|Concat[σD,x]) +p(y|x)\n2 .\nWe also show that language-modeling perfor-\nmance improves when Dis re-ordered based on the\nsmallest ℓ2 distance between the summary vectors\n{σ(d1),...,σ (dk)}and σx. This incurs negligible\noverhead since σx can be constructed during the\nsame forward pass which computes p(y|x). The\nablation for this is shown in Table 8\nFused Passages We establish a baseline for Fusing\nSummary Vectors by concatenating the correspond-\ning plain-text passages D= Concat[dk,...,d 1]\nand computing\np(y|x,D) =p(y|Concat[D,x]) +p(y|x)\n2 .\nNote that this approach is quickly limited by the\nsize of the pre-trained language model’s context\nwindow, especially when retrieving many long pas-\nsages.\n3843\nDomain Compressed context Evaluation sequence Mostimprovedtokens\nBooks3Surrealism—not for Breton’s depreciation of \"Red Front,\" but for a seemingly insignificantaside. In early March, before sending the text to press, Breton showed it to Aragon. Thelatter consented to the publication, with one exception: a footnote in which Breton quoted thePCF official’s remark (which Aragon had earlier reported to him) about \"complicating thesimple, healthy relations between men and women\"—a clear illustration, Breton felt, of \"justhow much bad faith or mental indigence we were up against.\" Aragon considered internalParty statements to be confidential, and asked that the footnote be removed; according tohim, Breton \"spontaneously crossed out the note on the galleys with a delete mark that I canstill recall... saying that he wanted to give the Party no excuse for expelling me.\" But when_The Poverty of Poetry_came off press the next day, the incriminating footnote was stillthere.Whether Breton retained the note as a test of Aragon’s loyalty, or whether he deemed thisexample of PCF stupidity too good to waste, or whether the printer simply neglected to makethe correction, no one has ever established. But the result was that this single act came torepresent for Aragon every philosophical difference, stricture, and humiliation that had everdarkened his long friendship with Breton. On March 10, he responded to the tract via ananonymous note in\n_L’Humanité_: \"Our comrade Aragon in-forms us that he has absolutely nothing todo with the publication of a pamphlet entitled_The Poverty of Poetry_... He wishes to makeit clear that he entirely disavows both the con-tents of this pamphlet and the attention it hasdrawn to his name, every Communist beingduty-bound to condemn the attacks containedin this pamphlet as incompatible with the classstruggle.\" This short paragraph was the onlynotice he ever saw fit to give of\nPovertyPoAragonHuman\nWikipedia </s>Shi CeShi Ce (; born 15 December 1985) is a Chinese deaf female table tennis player. She hasrepresented China at the Deaflympics four times from 2005-2017. Shi Ce has been regardedas one of the finest athletes to have represented China at the Deaflympics, having won 14medals at the event since making her debut in the 2005 Summer Deaflympics.BiographyShi Ce was born in Yichun, Heilongjiang on 15 December 1985. She was born with anear condition that impaired her hearing which resulted in her deafness and has congenitalmalformation in her right ear. Her parents decided to consult a doctor and took her to anhospital in the Zhejiang Province in order to cure her ear impairment when she was justfive years old. The doctor suggested that surgery would cause facial paralysis after Shi Ce’sparents demanded for a surgery. Shi Ce took the sport of Table tennis and started playing itat the age of nine.CareerShi Ce has won 14 medals in her Deaflympic career as a Table tennis player including 11gold medals. Shi Ce was eligible to compete at the National Games of China despite herdeafness, in 2015. In the competition, she secured gold medals in singles, doubles, mixeddoubles and in the team events.2005 Summer Deaflympics Shi Ce made her first appearance at an international sports\nevent during the 2005 Summer Deaflympicsand excelled on her debut Deaflympic eventafter winning gold medals in the women’s sin-gles, doubles and in the mixed doubles. Shewas also the part of the Chinese Table ten-nis team which secured the silver medal inthe 2005 Deaflympics. In the same year, shereceived the Deaf Sportswoman of the Yearaward from the ICSD for her remarkable per-formances at the 2005 Summer Deaflympics.Shi Ce\nCeDe2005SummerShi\nGithub </s> import sysimport datetimedef basic(arguments):import apicritic = api.critic.startSession(for_testing=True)repository = api.repository.fetch(critic, name=\"critic\")branch = api.branch.fetch(critic,repository=repository, name=arguments.review)review = api.review.fetch(critic, branch=branch)alice = api.user.fetch(critic, name=\"alice\")bob = api.user.fetch(critic, name=\"bob\")dave = api.user.fetch(critic, name=\"dave\")erin = api.user.fetch(critic, name=\"erin\")all_comments = api.comment.fetchAll(critic)assert isinstance(all_comments, list)EXPECTED={0:{\"text\": \"This is a general issue.\", \"location\": None,\n\"type\": \"issue\", \"state\": \"open\"},1 :{\"text\": \"This is a general note.\",\"location\": None,\"type\": \"issue\",\n1location}textissue\nFreeLaw 8By the end of 1975, Farmers National was insolvent and under investigation by the FloridaDepartment of Insurance. The Miami newspapers published a series of articles describingthe relationship between Hauser and the company. Lawrence Lee, an attorney for an Arizonaunion group, investigated Farmers National in connection with an Old Security-FarmersNational proposal. He was told by the Florida insurance department that Farmers Nationalwas connected with Hauser, that it had been injected with questionable assets which werebeing investigated by the department, and that it had been fined $5,000 for failing to discloseboth Hauser’s ownership and a loan to one of its directors. Lee contacted Richard Halford,vice-president at Old Security in charge of union group insurance, and related the informationhe had received. Halford assured Lee that he was aware of Hauser’s reputation, but thatHauser was no longer involved with Farmers National. Halford then called Kavanagh,who told him that Hauser had no official capacity with the company, and that the financialproblems had been cleared up. Halford did not attempt to check the accuracy of Kavanagh’srepresentations with the Florida Department of Insurance.9Hauser controlled a second company, Family Provider Life Insurance Company (\"FamilyProvider\"). In 1975, the company had no business, no office, and assets of $50,000. Becauseof Farmers National’s insolvency, Hauser decided to activate\nFamily Provider, and its assets were increasedto $250,000, the minimum required to con-duct business in Arizona, where the companywas licensed. In January 1976, Boden andKavanagh met with Halford and Robert Bar-ton, president of Old Security, to proposea new agreement between Old Security andFamily Provider for the purpose of obtainingthe Fund business. Both Barton and Halfordconsidered Family Provider and Farmers Na-tional to be \"synonymous\" and believed thatKavanagh and Boden\nSecurityOldFamilyavanassets\nTable 9: Examples of sequences from in-domain test Pile domains. We highlight the tokens from the evaluation\nsequence which benefit the most from the summary vectors. In Books3, L’Humanité is prominent French newspaper\nassociated with Breton and his circle. In GitHub, the summary vectors carry information about the logical and\nsyntactical continuation of the context.\n3844\nDomain Compressed context Evaluation sequence Mostimprovedtokens\nHackerNewsHackers steer Tesla into oncoming traffic by placing three stickers on the road- velmu https://www.businessinsider.com/tesla-hackers-steer-into-oncoming-traffic-with-stickers-on-the-road-2019-4====== chrisbolt From yesterday:[https://news.ycombinator.com/item?id=19536375](https://news.ycombinator.com/item?id=19536375)——gregmacWhile I’m hugely skeptical of the current state of self-driving cars, you could probably gethuman drivers to make the same mistake if you were to repaint the lines. However, humanswill also notice the oncoming cars (if there are any) and avoid getting in a head-on collision.The thing missing from this test is that critical practical piece: if there was an oncomingcar, will the Tesla do something to avoid the collision? I would assume that not getting in ahead-on crash is higher priority than staying in the lane markings.Without oncoming traffic, all this is testing is what the Tesla considers valid line markings.I’m sure there’s room for improvement here (such as checking where the other lane is, raisingthe requirement for how well-defined the lines have to be, etc), but\nthose are also going to involve trade-offswhere there are legitimate situations that willstop working.I think you could just as easily title this video\"Tesla auto-pilot follows road markings evenif they’re really bad\".Edit: The best shot I could get from the video[1] makes me even more upset at this test:these look like the temporary markings of-ten used during construction, just before theycome and paint the normal lines using the big\nTeslatestmarkingsauto\nArXivzk=hk(xk) +vk, vk∼N(0,Rk)In the above equations, we see that the transition matrixFk,k−1has been replaced by thenonlinear vector-valued functionfk,k−1(·), and similarly, the matrixHk, which transformsa vector from the state space into the measurement space, has been replaced by the nonlinearvector-valued functionhk(·). The method proposed by the Extended Kalman Filter is tolinearize the nonlinearities about the current state prediction (or estimate). That is, wechooseFk,k−1as the Jacobian offk,k−1evaluated atˆxk−1|k−1, andHkas the Jacobian of\nhkevaluated atˆxk|k−1and proceed as in the linear Kalman Filter of Sectionsec::kf.[ˆ18]Numerical accuracy of these methods tends to depend heavily on the nonlinear functions. Ifwe have linear constraints but\na nonlinearfk,k−1(·)andhk(·), we can adaptthe Extended Kalman Filter to fit into theframework of the methods described thus far.Nonlinear Equality and Inequality Constraints———————————————Since equality and inequality constraints wemodel are often times nonlinear, it is importantto make the extension to nonlinear equality andinequality constrained Kalman Fil\nExtendedlinearhkKal\nGutenbergeight or nine cents. Telegrams in foreign languages are sent within the empire for five senper word, with a minimum charge of twenty-five sen for five words or a fraction thereof. Nocharge is made for delivery within a radius of 2-1/2 miles of the telegraph office.There are no private telegraph corporations. The government builds, owns, and operates thelines just as it does the mails. The postal and 101 telegraph systems are intimately connected,and the same office does service for both.The first telegraph line in Japan was opened in 1869. The venture proving a success, thefollowing year the line was extended and a general telegraphic system for the whole countrydecided upon. The rapid construction of telegraph lines began in 1872, from which year ithas gone forward uninterruptedly. At present the lines extend to every corner of the empire.The first lines were surveyed, built, and operated under foreign experts; but the natives havelearned so rapidly that they have been enabled to do away with all foreign employees. All ofthe materials and instruments in use, with the exception of submarine cables and the mostdelicate electrical measuring apparatus, are made in Japan.MAILS.–The Japanese mail system was modeled after the American in 1871.\nAt first it was limited to postal service betweenthe three large cities of Tokyo, Kyoto, and Os-aka; but in 1872 it was extended to the wholecountry, with the exception of a certain part ofthe Hokkaido, which was without roads andalmost without population. To-day there is novillage or hamlet in the whole land which doesnot enjoy the convenience of a good postal sys-tem. The mails are sent with promptness and\nlimitedpostalTokyo\nYoutubeSubtitles te que no voy a esa escuela.\"Johnny GaleckiEl Dr. Leonard Hofstadter obtuvo su doctorado a los 24 años, pero el actor que lo interpretasólo llegó a medio camino de la secundaria. En una entrevista con Time Out Chicago en el2009, Johnny Galecki reveló que abandonó la escuela a mediados del octavo grado luego deaños de evitar ir a clases a toda costa. Le dijo a Time Out, \"Una vez que las divisiones largasaparecieron en tercer grado, iba al baño por 45 minutos y nadie lo notaba, todos los días ala misma hora del día, sólo para escapar de ellas.\" Puede que Galecki no tenga un cerebromatemático, pero siempre tuvo inteligencia callejera. \"El conocimiento es el mejor y másseguro tesoro... Vaya, me aburro a mí mismo.\" A los 14 años, vivió solo en un apartamentitoen Burbank, California, mient\nras trabajaba en la comedia AmericanDreamer, su primer gran trabajo. Su familiapasó nueve meses en Long Beach antes deregresar a Chicago, y él se quedó para concen-trarse en su carrera como actor.Jim ParsonsEl Dr. Sheldon Cooper fue un niño prodigio.Comenzó la universidad cuando tenía 11 años\nParsonsabaJimDr\nTable 10: Examples of sequences from out-of-domain test Pile domains. We highlight the tokens from the evaluation\nsequence which benefit the most from the summary vectors. In Gutenberg, ‘Tokyo’ is not copied over from the\ncompressed context but is inferred from the discussion of Japan. In YoutubeSubtitles, ‘Jim Parsons’ benefits the\nmost from the summary vectors because the context discusses his co-star John Galecki in The Big Bang Theory.\n3845\nDataset Prompt template OPT-based models Llama-2-based models\nToks. / dem. Cal. Bal. Toks. / dem. Cal. Bal.\nAG NewsArticle: {text}\\nTopic: {label} 65 ✓ 75 ✓\nSST-2Sentence: {sentence}\\nSentiment: {label} 22 ✓ ✓ 25 ✓ ✓\nBoolQ{passage}\\nquestion: {question}?\\nanswer: {label}165 ✓ 170 ✓\nWiC {sentence1}\\n{sentence2}\\nquestion: Is the word ’{word}’45 ✓ 45 ✓used the same way in the two sentences above?\\nanswer: {label} ✓\nWSC Question: In the sentence \"{text}\", does the pronoun ’{span2_text}’61 50 ✓refer to {span1_text}?\\nAnswer: {label}\nRTE {premise}\\nquestion: {hypothesis} True or False?\\nanswer: {label}75 85\nCB {premise}\\nquestion: hypothesis. true, false or neither?\\nanswer: {label}98 ✓ 95 ✓\nCOPAContext: {premise}\\nAnswer: {answer} 21 ✓ 22 ✓ ✓\nMultiRCContext: {paragraph}\\n{question}\\n{answer}\\nanswer: {label}350 ✓ ✓ 350 ✓ ✓\nMR Review: {text}\\nSentiment: {label} 36 ✓ 40 ✓ ✓\nSubj input: {text}\\ntype: {label} 40 ✓ 40 ✓ ✓\nTable 11: Details of the datasets and prompts used for the ICL evaluation of our OPT-2.7B and Llama-2-7B\nAutoCompressors and baselines. “Toks / dem.” ( Tokens per demonstration) denotes how long demonstrations\nare for the average example. “Cal.” (Calibration) denotes whether we use calibration (Sachan et al., 2022), and\n“Bal.” (Balanced) means whether we enforce class-balanced sampling. We decide the ticks based on which method\nperforms best on a held-out validation set.\nAG News SST-2 BoolQ WiC WSC RTE CB COPA MultiRC MR Subj\nOPT-2.7B AutoCompressor\nZero-shot68.2(0.0) 78.0(0.0) 60.2(0.0) 49.5(0.0) 60.6(0.0) 55.2(0.0) 43.6(0.0) 69.0(0.0) 43.8(0.0) 60.0(0.0) 56.7(0.0)\n50 summary vecs.72.7(1.4) 84.3(9.2) 55.8(4.2) 50.4(1.0) 61.3(5.8) 54.8(3.4) 55.9(5.4) 71.6(0.6) 44.1(1.1) 70.4(10.2) 63.2(7.7)\n100 summary vecs.71.2(3.8) 87.0(3.5) 57.5(4.6) 50.7(1.0) 60.2(6.7) 55.5(2.5) 54.4(4.0) 71.9(0.4) 45.6(2.8) 73.1(12.9) 62.2(5.8)\n150 summary vecs.68.2(3.3) 82.6(5.6) 59.8(1.8) 51.8(1.1) 63.5(0.0) 55.8(1.8) 58.3(5.1) 71.4(0.5) 46.7(2.1) 67.0(11.9) 58.5(6.7)\nICL (150 tokens)72.5(2.5) 70.8(12.6) 60.2(0.0) 50.4(1.1) 52.3(13.9) 57.6(4.3) 51.1(7.1) 71.3(1.5) 43.8(0.0) 86.4(4.2) 61.7(11.2)\nICL (750 tokens)67.3(3.4) 87.5(5.0) 69.1(1.0) 51.0(1.7) 62.9(0.8) 57.4(4.4) 49.0(1.1) 72.0(0.7) 52.0(5.4) 86.7(5.9) 73.6(13.9)\nOPT-2.7B RMT\nZero-shot66.9(0.0) 72.8(0.0) 58.4(0.0) 50.3(0.0) 64.4(0.0) 55.2(0.0) 42.2(0.0) 68.8(0.0) 43.9(0.0) 62.5(0.0) 69.8(0.0)\n1-step summary vecs.66.3(5.5) 86.5(5.1) 49.6(8.1) 51.0(1.00 57.7(6.6) 51.3(1.2) 53.3(3.8) 67.4(1.1) 44.9(1.2) 52.6(2.8) 63.3(11.2)\n2-step summary vecs.65.2(7.2) 88.6(2.3) 54.8(4.1) 50.3(0.8) 58.6(6.7) 50.2(1.4) 49.5(4.8) 68.2(1.2) 45.5(1.8) 54.1(1.9) 54.6(1.7)\n3-step summary vecs.63.9(3.3) 84.5(6.6) 41.8(9.7) 50.6(0.6) 54.3(7.9) 50.2(1.4) 49.5(3.6) 68.0(0.9) 45.5(1.0) 52.8(1.6) 58.4(8.6)\nICL (150 tokens)70.8(1.9) 75.1(13.3) 58.4(0.0) 51.7(2.8) 52.5(13.1) 57.2(3.6) 46.5(3.6) 69.3(1.5) 43.9(0.0) 89.0(1.4) 60.7(12.1)\nICL (750 tokens)65.8(4.2) 85.7(9.7) 57.2(7.6) 51.5(2.7) 59.2(8.5) 57.8(2.0) 48.2(0.7) 70.9(0.7) 54.6(3.6) 87.5(4.6) 71.6(12.6)\nOPT-2.7B Pre-trained\nZero-shot65.1(0.0) 79.1(0.0) 55.8(0.0) 49.4(0.0) 53.9(0.0) 51.2(0.0) 21.2(0.0) 66.8(0.0) 43.7(0.0) 59.0(0.0) 66.2(0.0)\nICL (150 tokens)71.6(2.6) 68.56(14.9) 55.8(0.0) 50.6(1.0) 53.30(11.1) 56.1(2.4) 46.2(6.4) 71.7(1.2) 43.7(0.0) 86.7(4.3) 61.9(10.9)\nICL (750 tokens)63.3(5.1) 91.0(3.2) 63.0(1.3) 50.0(0.4) 63.5(0.6) 54.7(3.0) 52.1(4.8) 73.4(1.0) 53.5(6.2) 89.9(2.2) 64.4(10.7)\nLlama-2-7B AutoCompressor\nZero-shot63.3(0.0) 67.7(0.0) 67.4(0.0) 50.8(0.0) 43.3(0.0) 58.8(0.0) 42.9(0.0) 52.5(0.0) 52.5(0.0) 57.4(0.0) 49.3(0.0)\n50 summary vecs.79.6(4.9) 94.2(1.6) 70.1(3.3) 51.6(2.1) 47.7(8.7) 66.3(7.0) 46.4(18.7) 84.5(1.0) 52.6(2.8) 91.5(1.0) 53.5(3.6)\n100 summary vecs.87.6(1.2) 92.6(3.3) 66.3(2.8) 52.5(2.2) 42.9(2.5) 63.5(6.6) 64.5(5.9) 85.9(0.4) 56.1(1.2) 90.7(2.6) 57.0(5.6)\n150 summary vecs.85.4(3.4) 92.3(2.9) 68.0(1.8) 52.8(1.5) 49.9(7.6) 65.3(6.6) 54.8(5.8) 86.1(0.6) 54.8(2.2) 91.1(2.2) 56.6(7.9)\nICL (150 tokens)74.5(2.2) 92.4(3.1) 67.4(0.0) 52.4(2.7) 51.8(6.9) 69.1(2.1) 46.4(23.0) 80.0(1.9) 52.5(0.0) 79.7(15.7) 57.9(10.7)\nICL (750 tokens)81.2(4.1) 93.8(1.2) 67.7(2.7) 52.4(2.0) 40.0(5.7) 73.1(3.5) 50.3(2.8) 82.6(1.6) 47.0(3.2) 91.6(0.8) 60.7(14.8)\nLlama-2-7B Pre-trained\nZero-shot68.8(0.0) 87.2(0.0) 70.0(0.0) 51.4(0.0) 65.4(0.0) 62.8(0.0) 32.1(0.0) 75.5(0.0) 54.5(0.0) 84.1(0.0) 48.9(0.0)\nICL (150 tokens)71.9(3.8) 91.6(2.9) 70.0(0.0) 51.0(1.9) 55.4(3.2) 70.9(1.7) 39.3(21.2) 84.2(1.3) 54.5(0.0) 90.6(3.3) 63.6(10.8)\nICL (750 tokens)78.2(3.8) 94.5(0.8) 70.3(6.1) 54.9(1.9) 42.2(5.0) 71.3(4.4) 51.3(3.5) 85.3(0.7) 47.0(1.5) 92.9(0.5) 65.4(14.5)\nTable 12: We evaluate the following models on 11 in-context learning tasks: The OPT-2.7B AutoCompressor and\nRMT model, the Llama-2-7B AutoCompressor, and the respective pre-trained models. For each fine-tuned model,\nnumbers in bold are the highest evaluation results using at most 150 additional tokens. When using summary\nvectors, the OPT-2.7B AutoCompressor outperforms the RMT model on 8/11 tasks. Moreover, the OPT-2.7B\nAutoCompressor benefits from multiple compression steps on most tasks whereas the RMT model performs best\nwithout summary vectors on 7/11 tasks and benefits from 3-step summary vectors on none of the above tasks. The\nLlama-2 AutoCompressor achieves the absolute highest accuracy using summary vectors on 7/11 tasks. It also\nachieves the highest accuracy with summary vectors on 9/11 tasks using at most 150 additional tokens.\n3846",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8885097503662109
    },
    {
      "name": "Perplexity",
      "score": 0.8726294040679932
    },
    {
      "name": "Language model",
      "score": 0.7588064074516296
    },
    {
      "name": "Inference",
      "score": 0.6316943764686584
    },
    {
      "name": "Transformer",
      "score": 0.585780680179596
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5698222517967224
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.5282182693481445
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4908652901649475
    },
    {
      "name": "Natural language processing",
      "score": 0.4904343783855438
    },
    {
      "name": "Context model",
      "score": 0.4795232117176056
    },
    {
      "name": "Task (project management)",
      "score": 0.4753263294696808
    },
    {
      "name": "Window (computing)",
      "score": 0.47075513005256653
    },
    {
      "name": "Machine learning",
      "score": 0.4101431369781494
    },
    {
      "name": "World Wide Web",
      "score": 0.06685596704483032
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Object (grammar)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}