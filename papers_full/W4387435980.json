{
    "title": "Improving Sentiment Analysis in Election-Based Conversations on Twitter with ElecBERT Language Model",
    "url": "https://openalex.org/W4387435980",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5005807085",
            "name": "Asif Khan",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5084300721",
            "name": "Huaping Zhang",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5048521716",
            "name": "Nada Boudjellal",
            "affiliations": [
                "Université Constantine 2"
            ]
        },
        {
            "id": "https://openalex.org/A5066501193",
            "name": "Arshad Ahmad",
            "affiliations": [
                "Pak-Austria Fachhochschule: Institute of Applied Sciences and Technology",
                "Pakistan Institute of Engineering and Applied Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A5031928709",
            "name": "Maqbool Khan",
            "affiliations": [
                "Pak-Austria Fachhochschule: Institute of Applied Sciences and Technology",
                "Pakistan Institute of Engineering and Applied Sciences"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4361225479",
        "https://openalex.org/W4210320212",
        "https://openalex.org/W3046950317",
        "https://openalex.org/W4205177338",
        "https://openalex.org/W3196289802",
        "https://openalex.org/W6851092927",
        "https://openalex.org/W6703170100",
        "https://openalex.org/W6794665542",
        "https://openalex.org/W6798430783",
        "https://openalex.org/W3197264344",
        "https://openalex.org/W4284965264",
        "https://openalex.org/W4376274173",
        "https://openalex.org/W4311632592",
        "https://openalex.org/W3000501558",
        "https://openalex.org/W4366549364",
        "https://openalex.org/W6772752637",
        "https://openalex.org/W6774526564",
        "https://openalex.org/W6843017590",
        "https://openalex.org/W3081987387",
        "https://openalex.org/W3201286234",
        "https://openalex.org/W3091788708",
        "https://openalex.org/W2761453955",
        "https://openalex.org/W6770749020",
        "https://openalex.org/W6756508097",
        "https://openalex.org/W3081991753",
        "https://openalex.org/W2068515881",
        "https://openalex.org/W6662613254",
        "https://openalex.org/W6750757439",
        "https://openalex.org/W6841989486",
        "https://openalex.org/W2791200561",
        "https://openalex.org/W2914556987",
        "https://openalex.org/W3017090629",
        "https://openalex.org/W6778038236",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W4298110867",
        "https://openalex.org/W3136657289",
        "https://openalex.org/W4386701122",
        "https://openalex.org/W4298009900",
        "https://openalex.org/W3199146084",
        "https://openalex.org/W3045495327",
        "https://openalex.org/W4293113218",
        "https://openalex.org/W2148143831",
        "https://openalex.org/W6842425476",
        "https://openalex.org/W4224011737",
        "https://openalex.org/W4237970149",
        "https://openalex.org/W4360605428",
        "https://openalex.org/W3008110149",
        "https://openalex.org/W2903329043",
        "https://openalex.org/W2799668929",
        "https://openalex.org/W4291470761",
        "https://openalex.org/W3178466251",
        "https://openalex.org/W3167958315",
        "https://openalex.org/W2991207823",
        "https://openalex.org/W4205640832",
        "https://openalex.org/W4292388317",
        "https://openalex.org/W4297324160",
        "https://openalex.org/W3159019313",
        "https://openalex.org/W3104186312",
        "https://openalex.org/W2048188343"
    ],
    "abstract": "Sentiment analysis plays a vital role in understanding public opinions and sentiments toward various topics. In recent years, the rise of social media platforms (SMPs) has provided a rich source of data for analyzing public opinions, particularly in the context of election-related conversations. Nevertheless, sentiment analysis of election-related tweets presents unique challenges due to the complex language used, including figurative expressions, sarcasm, and the spread of misinformation. To address these challenges, this paper proposes Election-focused Bidirectional Encoder Representations from Transformers (ElecBERT), a new model for sentiment analysis in the context of election-related tweets. Election-related tweets pose unique challenges for sentiment analysis due to their complex language, sarcasm, and misinformation. ElecBERT is based on the Bidirectional Encoder Representations from Transformers (BERT) language model and is fine-tuned on two datasets: Election-Related Sentiment-Annotated Tweets (ElecSent)-Multi-Languages, containing 5.31 million labeled tweets in multiple languages, and ElecSent-English, containing 4.75 million labeled tweets in English. The model outperforms other machine learning models such as Support Vector Machines (SVM), Naïve Bayes (NB), and eXtreme Gradient Boosting (XGBoost), with an accuracy of 0.9905 and F1-score of 0.9816 on ElecSent-Multi-Languages, and an accuracy of 0.9930 and F1-score of 0.9899 on ElecSent-English. The performance of different models was compared using the 2020 United States (US) Presidential Election as a case study. The ElecBERT-English and ElecBERT-Multi-Languages models outperformed BERTweet, with the ElecBERT-English model achieving a Mean Absolute Error (MAE) of 6.13. This paper presents a valuable contribution to sentiment analysis in the context of election-related tweets, with potential applications in political analysis, social media management, and policymaking.",
    "full_text": "This work is licensed under a Creative Commons Attribution 4.0 International License,\nwhich permits unrestricted use, distribution, and reproduction in any medium, provided the\noriginal work is properly cited.\nechT PressScience\nDOI:10.32604/cmc.2023.041520\nARTICLE\nImproving Sentiment Analysis in Election-Based Conversations on Twitter\nwith ElecBERT Language Model\nAsif Khan1,H u a p i n gZ h a n g1,*,N a d aB o u d j e l l a l2, Arshad Ahmad3 and Maqbool Khan3\n1SchoolofComputerScienceandTechnology,BeijingInstituteofTechnology,Beijing,100081,China\n2TheFacultyofNewInformationandCommunicationTechnologies,UniversityAbdel-HamidMehriConstantine2,Constantine,\n25000,Algeria\n3DepartmentofITandComputerScience,Pak-AustriaFachhochschule:InstituteofAppliedSciencesandTechnology,Haripur,\n22620,Pakistan\n*CorrespondingAuthor:HuapingZhang.Email:kevinzhang@bit.edu.cn\nReceived:26April2023 Accepted:28June2023 Published:08October2023\nABSTRACT\nSentiment analysis plays a vital role in understanding public opinions and sentiments toward various topics. In\nrecent years, the rise of social media platforms (SMPs) has provided a rich source of data for analyzing public\nopinions,particularlyinthecontextofelection-relatedconversations.Nevertheless,sentimentanalysisofelection-\nrelated tweets presents unique challenges due to the complex language used, including figurative expressions,\nsarcasm, and the spread of misinformation. To address these challenges, this paper proposes Election-focused\nBidirectionalEncoderRepresentationsfromTransformers(ElecBERT),anewmodelforsentimentanalysisinthe\ncontextofelection-relatedtweets.Election-relatedtweetsposeuniquechallengesforsentimentanalysisduetotheir\ncomplexlanguage,sarcasm,andmisinformation.ElecBERTisbasedontheBidirectionalEncoderRepresentations\nfrom Transformers (BERT) language model and is fine-tuned on two datasets: Election-Related Sentiment-\nAnnotated Tweets (ElecSent)-Multi-Languages, containing 5.31 million labeled tweets in multiple languages, and\nElecSent-English,containing4.75millionlabeledtweetsinEnglish.Themodeloutperformsothermachinelearning\nmodels such as Support Vector Machines (SVM), Naïve Bayes (NB), and eXtreme Gradient Boosting (XGBoost),\nwith an accuracy of 0.9905 and F1-score of 0.9816 on ElecSent-Multi-Languages, and an accuracy of 0.9930 and\nF1-scoreof0.9899onElecSent-English.Theperformanceofdifferentmodelswascomparedusingthe2020United\nStates (US) Presidential Election as a case study. The ElecBERT-English and ElecBERT-Multi-Languages models\noutperformedBERTweet,withtheElecBERT-EnglishmodelachievingaMeanAbsoluteError(MAE)of6.13.This\npaperpresentsavaluablecontributiontosentimentanalysisinthecontextofelection-relatedtweets,withpotential\napplicationsinpoliticalanalysis,socialmediamanagement,andpolicymaking.\nKEYWORDS\nSentimentanalysis;socialmedia;electionprediction;machinelearning;transformers\n3346 CMC, 2023, vol.76, no.3\n1 Introduction\nIn recent years, social media has emerged as a powerful tool for public discourse, particularly\nin the context of politics and elections [1]. As a result, sentiment analysis has become a crucial tool\nfor understanding public opinion and sentiment during elections [2]. However, sentiment analysis of\nelection-related tweets poses unique challenges due to the complex nature of political language and\nthe nuances of social dynamics involved [3].\nOne of the main challenges in sentiment analysis is the lack of dependable and extensive datasets\nsuitable for training machine learning models.\nPrior studies have utilized diverse datasets to address this challenge, including 3 million tweets\nrelated to the US presidential election [4], a dataset of 38,432,811 tweets from the US 2020 Presidential\nelection [5], and a dataset consisting of 5,299 tweets from the 2022 Philippines national election [6].\nAdditionally, other datasets have been used for sentiment analysis, such as 1,302,388 tweets from the\nEcuadorian presidential elections of 2021 [7] and 50K election-related tweets from the Indian General\nElection 2019 [8]. Moreover, a study explored the influence of tweet sentiment on opinions and retweet\nlikelihood using datasets focused on various events, including a 2017 demonetization in India dataset\n(14,940 tweets), a 2016 US election dataset (397,629 tweets), and a 2018 American Music Awards\ndataset (27,556 tweets) [9].\nFurthermore, studies have focused on specific elections, such as Pakistan’s general election in 2018\n[10], the 2020 US presidential election [11], the Nigeria 2023 presidential election [12], and recent\npresidential elections in Latin America, utilizing over 65,000 posts from social media platforms [13].\nAdditional datasets include 9,157 tweets from the 2017 Punjab assembly elections [14], a dataset\nof 5,000 messages from Twitter and Facebook annotated as neutral/partisan [15], a 100K #Politics\ndataset [16], and 29,462 tweets related to the West Bengal election in India [17]. Despite the existing\ndatasets utilized for sentiment analysis in elections, there are limitations in terms of their size and\ncomprehensiveness. These datasets may not encompass the wide spectrum of political scenarios or\nprovide a sufficient representation of sentiment variations.\nTo address the scarcity of dependable datasets, the development of the US Presidential Election\nTweets Dataset (UPETD) was undertaken. This dataset comprises 5.3 million election-related tweets\nand has been labeled with positive, negative, and neutral sentiments using the “Valence Aware Dic-\ntionary for sEntiment Reasoning (V ADER)” technique. The resulting dataset, named the “ElecSent\ndataset,” serves as a valuable resource for training machine learning models like BERT, enabling more\nprecise and effective sentiment analysis of election-related tweets [18–20].\nTo further improve the accuracy of sentiment analysis in election-related tweets, this study pro-\nposes ElecBERT, a new sentiment analysis model specifically designed for political tweets. ElecBERT\nis fine-tuned on the ElecSent dataset and utilizes the BERT language model, taking into account the\ncontext and nuances of political language and social dynamics for more accurate sentiment analysis.\nThis study implemented ElecBERT to predict the sentiment of election-related tweets during the\nUS 2020 Presidential Election as a case study. The effectiveness of ElecBERT in analyzing election-\nrelated tweets and predicting public sentiment was evaluated by comparing the results with the actual\nelection outcome.\nThe implications of this study are far-reaching in terms of understanding public opinion in\nelection-related situations. Accurate sentiment analysis can help political campaigns and policymakers\nto gauge public opinion, identify areas of concern, and design policies accordingly. This study\nCMC, 2023, vol.76, no.3 3347\nprovides a valuable contribution to sentiment analysis in the context of election-related tweets and\nhas implications for a wide range of applications in the political and social domains.\nThe main contributions of this study are as follows:\n1. ElecSent dataset: 5.3 million tweets related to politics, labeled with a positive, negative, and\nneutral sentiments.\n2. ElecBERT: a new sentiment analysis model specifically designed for political tweets, utilizing\nthe BERT language model and taking into account the complexities of political language and\nsocial dynamics for improved accuracy.\nThe paper is structured as follows: InSection 2, related work is presented.Section 3introduces the\nElecSent dataset and the proposed ElecBERT model methodology.Section 4describes the experiments\nand presents the results, followed by a discussion. Finally, the study is concluded in the conclusion\nsection.\n2 Related Work\nSentiment analysis on social media data has gained significant attention in recent years due to\nthe increasing importance of understanding public opinion in various domains. Several studies have\nexplored different approaches to sentiment analysis, including rule-based methods such as V ADER,\nand machine learning techniques such as logistic regression, SVMs, and NB. However, the field of\nsentiment analysis has also seen significant advancements in deep learning-based methods for text\nanalytics. Deep learning techniques, such as Convolutional Neural Networks (CNNs) and Recurrent\nNeural Networks (RNNs) [21], have demonstrated remarkable performance in this domain. CNNs\nexcel at capturing local features and patterns, while RNNs are effective in modeling sequential\ndependencies in text data [22].\nFurthermore, a recent study by [23] introduced a novel approach that utilizes capsule networks\nfor sentiment analysis, with a specific focus on social media content from platforms like Twitter.\nThe study findings demonstrate the effectiveness of capsule networks in sentiment analysis tasks,\nparticularly when analyzing Twitter data. Another study [24] has introduced a novel neural network\nmodel for sentiment analysis, combining multi-head self-attention and character-level embedding.\nThis model effectively tackles the challenges of sentiment word extraction and the out-of-vocabulary\nproblem commonly encountered in existing methods. By employing an encoder-decoder architecture\nwith Bidirectional Long Short-Term Memory (BiLSTM), the model captures contextual semantic\ninformation and extracts deeper emotional features, enhancing its ability to analyze sentiment in text.\nDomain-specific sentiment analysis, such as election prediction, has also been explored. For\ninstance, in a study [25], the authors employed tools such as Natural Language Toolkit (NLTK),\nTweet Natural Language Processing (TweetNLP) toolkit, Scikit-learn, and Statistical Package for the\nSocial Sciences (SPSS) statistical package to predict ideological orientation (conservative or right-\nleaning, progressive or left-leaning) of 24,900 tweets collected over 9 h during an election, achieving an\noverall accuracy of 99.8% using Random Forest. Similarly, in [26], Scikit-learn was utilized to analyze\n46,705 Greek tweets over 20 days during an election, achieving a Random Forest accuracy of 0.80 and\nprecision values of Negative= 0.74, Neutral= 0.83, and Positive= 1.\nIn another study [27], Textblob was used to analyze 277,509 tweets from three states (Florida,\nOhio, and North Carolina) over a month for sentiment analysis during the election, achieving\nNB accuracy of over 75%. Furthermore, in [28], the authors employed SVM, NB, and K-Nearest\nNeighbors (KNN) on 2018 Pakistani election data to predict the support for each political leader.\n3348 CMC, 2023, vol.76, no.3\nThey found that SVM outperformed other models with an accuracy of 79.89%. Similarly, in [29],\nSVM with a hybrid (unigram+ bigram) was used on 100K tweets during the U.S. Election 2012 and\nKarnataka (India) Elections 2013, achieving accuracies of 88% and 68%, respectively, using NLTK\nand Stanford part-of-speech (POS) tagger.\nAdditionally, study [30] utilized Waikato Environment for Knowledge Analysis (WEKA) to\nanalyze 3,52,730 tweets over a month for sentiment analysis on political parties in India, while study\n[14] employed the Syuzhet package in R-language, WEKA, and Gephi were used to analyze 9,157\ntweets over approximately a month regarding political parties, achieving an SVM accuracy of 78.63%.\nFurthermore, study [31] utilized KNN to analyze election-related data, achieving an average accuracy\nof 92.19%.\nMoreover, several studies have explored the use of Deep Learning and large language models for\nsentiment analysis in various domains. For example in [32], the authors analyzed US 2020 Presidential\nelection using BERT and V ADER, finding that V ADER outperformed BERT. Additionally, in [33],\nScikit-learn, NLTK, and V ADER were used to analyze 121,594 tweets over two days about a candidate\nwith an SVM accuracy of 0.99. Furthermore, study [34] employed Textblob, OpLexicon (Portuguese\nsentiment lexicon), and Sentilex (Portuguese sentiment lexicon) to analyze 158,279 tweets over 16 days\nabout a candidate with SVM accuracy of 0.93 and 0.98 for OpLexicon/Sentilex. Moreover, study [35]\nused Long short-term memory (LSTM) to analyze 3,896 tweets over approximately three months,\nexamining election trends, party, and candidate sentiment analysis, yielding precision= 0.76, recall=\n0.75, and F1-score= 0.74.\nHowever, these models often struggle to capture the in-depth nature of political language\nand social dynamics involved in election-related tweets. Recently, deep learning models such as\nTransformer-based models have shown remarkable performance in various natural language process-\ning (NLP) tasks, including sentiment analysis. One of the most popular deep learning models for\nNLP tasks is BERT, which has achieved state-of-the-art performance on several benchmark datasets.\nHowever, fine-tuning BERT for specific domains, such as election-related tweets, can improve its\nperformance and make it more effective for sentiment analysis.\nSeveral studies have utilized large language models for different domain-specific tasks. For\ninstance, BERTweet is a pre-trained language model for English Tweets, trained on 850 million Tweets\nusing the Robustly Optimized BERT Pretraining Approach (RoBERTa) pre-training procedure [36].\nBERTweet-COVID19 models were pre-trained on a corpus of 23 million COVID-19 English Tweets.\nIt outperforms strong baselines on three Tweet NLP tasks and also achieved good results on several\nbenchmarks, such as SemEval2017 where it achieved 0.732 AvgRec andF\nNP\n1 0.728, and accuracy 0.72,\nwhile on SemEval2018 it achievedFPos\n1 0.746 and accuracy 0.782. On WNUT16 it achieved 0.521 F1-\nscore, and on WNUT17 it achieved 0.551 F1-score.\nMoreover, PoliBERTweet, a pre-trained language model trained on over 83M US 2020 election-\nrelated English tweets [37]. The model is specifically designed to address the nuances of political\nlanguage and can be used for a variety of downstream tasks such as political misinformation analysis\nand election public opinion analysis. The authors used a stance detection dataset to check the\nperformance of PoliBERTweet [1]. The F1-score for BIDEN using RoBERTa (RB) is 0.6, RB/P-\nM is 0.663 TweetEval (TE) is 0.624, TE/P-M is 0.653, BERTweet (BT) is 0.650, BT/P-M is 0.673,\npoliBERTweet (PoliBERT) 0.708, sentiment knowledge-enhanced pre-training (SKEP) is 0.746, and\nknowledge-enhanced masked language modeling (KEMLM) is 0.758. While the F1-score for TRUMP\nusing RoBERTa (RB) is 0.771, RB/P-M is 0.779 TweetEval (TE) 0.809, TE/P-M is 0.811, BERTweet\nCMC, 2023, vol.76, no.3 3349\n(BT) is 0.828, BT/P-M is 0.831, poliBERTweet (PoliBERT) 0.848, sentiment knowledge-enhanced pre-\ntraining (SKEP) is 0.772, and knowledge-enhanced masked language modeling (KEMLM) is 0.788.\nP-M indicates Poli-Medium.\nNumerous other studies utilized large language models for different tasks such as BioBERT, a\npre-trained BERT model that has been specifically trained on biomedical text. It can be fine-tuned for\nvarious biomedical NLP tasks [38]. FinBERT is a pre-trained BERT model that has been specifically\ndesigned for financial text. It can be fine-tuned for various financial NLP tasks [39]. AraBERT is\nanother pre-trained BERT model that has been specifically trained on Arabic text. It can be fine-\ntuned for various Arabic NLP tasks [19]. ABioNER: A BERT-based model for identifying disease and\ntreatment-named entities in Arabic biomedical text [40]. MEDBERT.de, a pre-trained German BERT\nmodel designed specifically for the medical domain. Trained on a large corpus of 4.7 million German\nmedical documents, it achieves state-of-the-art performance on eight different medical benchmarks\n[41]. MolRoPE-BERT, an end-to-end deep learning framework for molecular property prediction that\nefficiently encodes the position information of SMILES sequences using Rotary Position Embedding\n(RoPE). The framework combines a pre-trained BERT model with RoPE for extracting potential\nmolecular substructure information. The model is trained on four million unlabeled drug SMILES\nand is evaluated on four datasets, demonstrating comparable or superior performance to conventional\nand state-of-the-art baselines [42].\nHowever, to the best of our knowledge, no study has explored the use of BERT specifically for\nsentiment analysis on election-related tweets, accounting for the complexities of political language\nand social dynamics. Therefore, this study proposes ElecBERT, a fine-tuned BERT model tailored for\nsentiment analysis on election-related tweets.\n3 Materials and Methods\nThis section presents the methodology employed in this study, including the ElecSent dataset and\nthe ElecBERT sentiment analysis model.\n3.1 Dataset\nTwitter using its Application Programming Interface (APIs) allow for data collection. They\nprovide developers with access to the platform’s vast collection of public data, including tweets, user\nprofiles, and search results. Numerous tools can be employed to collect tweets, for instance, Twitter-\nTap, Tweepy, TWurl, twarc, streamR, TweetMapper, Twitonomy, NodeXL, and Twython. This study\nutilized the Twitter Search API, which enables the retrieval of tweets based on specific criteria, such\nas keywords, hashtags, and dates. To collect tweets, the Tweepy Python library was applied, which is\na widely used tool for interacting with the Twitter Search API. A substantial number of tweets were\ncollected in JSON (JavaScript Object Notation) format, a lightweight data-interchange format that\nis both human-readable and machine-parseable. Each tweet contained various attributes, including\nuser_id (the unique identifier of the user who posted the tweet), lang (the language of the tweet), id\n(the unique identifier of the tweet), created_at (the date and time that the tweet was posted), text (the\ntext of the tweet), coordinates (the geographic coordinates of the tweet, if available), and others.\n3.1.1 UPETD Dataset\nThe UPETD dataset comprised nearly 5.31 million tweets related to Joe Biden, Donald Trump,\nDemocratic, Republican, and USElection2020 (during Timeline 1 (T1): 5th Dec 2019 and 30th Nov\n3350 CMC, 2023, vol.76, no.3\n2020, and Timeline 2 (T2):= 1st Aug 2020 to 30th Nov 2020).Table 1shows the statistics of the UPETD\ndataset.\nTable 1: Statistics for UPETD\nStats Joe biden Donald trump Democratic Republicans Election Total\nMulti-languages\ntweets\n1,594,370 835,119 248,988 1,724,198 908,508 5,311,183\nEnglish tweets 1,400,843 684,093 245,747 1,637,150 785,469 4,753,302\nCollection\ntimeline\nT2 T2 T1 T1 T2 –\n3.1.2 ElecSent Dataset\nThe ElecSent dataset is based on the UPETD dataset. Sentiments were assigned to each tweet in\nthe UPETD dataset. Several studies used V ADER to classify the tweets and later use other machine\nlearning approaches for sentiment analysis [43–45]. V ADER is a lexicon and rule-based sentiment\nanalysis tool specifically designed for social media text. It uses a sentiment lexicon that contains a\nlist of words with their associated sentiment scores (positive, negative, or neutral), along with a set of\ngrammatical rules and heuristics to analyze the sentiment of a given text. This study also employed\nV ADER to assign the sentiments to each tweet in the UPETD dataset. This dataset was subsequently\nnamed the ElecSent dataset. The ElecSent dataset is presented in two forms, (i) ElecSent-Multi-\nLanguage dataset, and (ii) ElecSent-English. ElecSent-Multi-Language contains tweets in multiple\nlanguages, including English, and has a total of 5.31 million tweets. ElecSent-English includes only\nEnglish-written tweets and has a total of 4.753 million tweets, which is almost 89.5% of the total\ndataset.\nTo analyze the distribution of sentiments in the dataset, the percentage of positive, negative,\nand neutral tweets was calculated.Fig. 1shows that both ElecSent-Multi-Language and ElecSent-\nEnglish have a similar distribution, with positive tweets being the highest, followed by negative and\nneutral tweets. ElecSent-Multi-Language has 41% positive, 32% negative, and 27% neutral tweets,\nwhile ElecSent-English has 43% positive, 31% negative, and 26% neutral tweets. ElecSent can be further\nused in machine learning election prediction models based on the public’s sentiment towards various\ncandidates and political parties.\nThe ElecSent dataset has an imbalanced distribution of labels in both its ElecSent-Multi-\nLanguages and ElecSent-English versions, with the majority of samples being Positive, followed\nby Negative and Neutral. This imbalance can lead to overfitting issues in models trained on this\ndataset. To address this problem, this study applied the Synthetic Minority Over-sampling Technique\n(SMOTE), which creates synthetic samples for the minority classes. with an equal distribution of\nsamples among all three classes [46–48]. Specifically,Fig. 2shows the balanced dataset that contains\n34% Positive, 33% Negative, and 33% Neutral samples.\nCMC, 2023, vol.76, no.3 3351\nFigure 1:Sentiment distribution of the ElecSent dataset (Multi-Languages & English-only)\nFigure 2:The balanced ElecSent dataset\n3.2 Building ElecBERT: Architecture and Fine-Tuning Approach\nBERT is a pre-trained language model that is trained on a large corpus of text data, and it is\nvery effective for various natural language processing tasks, including sentiment analysis. The BERT\nmodel (bert-base-uncased) was fine-tuned on the ElecSent dataset, represented asD, with labelsL=\n{positive, negative, neutral}, and obtained a new model called“ElecBERT”.Fig. 3shows the process of\nfine-tuning the ElecBERT model. Fine-tuning was performed by minimizing the loss functionL(D,\nθ),w h e r eθ is the parameters of BERT, and the output of the fine-tuned model for an inputx is denoted\nas f(x; θ∗). The statement can be defined as:\nLet D denote the “ElecSent” dataset, which consists of a set of tweets labeled as either positive,\nnegative, or neutral. LetX denote the set of input features andY denotes the corresponding set of\nlabels inD. The dataset is split into two parts: a training set and a validation set, with a ratio of 80:20.\nThen, the datasetD can be represented as follows:\nD = {(x\n1, y1), (x2, y2), ... , (xn, yn)}\nwhere eachxi is a feature vector and eachyi is a label inL.\nBefore feeding the text data to BERT, the tweets in the “ElecSent” dataset have undergone pre-\nprocessed. The BERT tokenizer is utilized to tokenize the pre-processed text data. The tokenizer adds\nspecial tokens like [CLS] and [SEP] to the start and end of each sentence, truncates/pads the sentences\nto a maximum length of 64 tokens, maps the tokens to their IDs, and creates an attention mask. To\nprepare the data for training, the training set and validation set are concatenated, and the tokenizer is\nemployed to encode the concatenated data.\n3352 CMC, 2023, vol.76, no.3\nFigure 3:Overview of the ElecBERT model\nThe BERT model (bert-base-uncased) is then fine-tuned on the ElecSent dataset using cross-\nentropy loss and theAdamW optimizer. The loss functionL(D, θ) is defined as:\nL (D, θ) = Σi = 1 t onl(f (xi; θ), yi)\nwhere θ denotes the parameters of BERT,f(x; θ) is the output of the BERT model for inputx with\nparameters θ,a n dl is the cross-entropy loss function that measures the discrepancy between the\npredicted output and the ground-truth label.\nThe TensorDataset and DataLoader classes are utilized from thePyTorch library to create data\nloaders for the training set and the validation set. This study uses theRandomSampler class to sample\nthe data randomly during training and theSequentialSampler class to sample the data sequentially\nduring validation. During training, the training loss, the validation loss, and the F1-score for each\nlabel (positive, negative, and neutral) are monitored using the validation set.\nThe model undergoes training for 6 epochs with a batch size of 32 and a learning rate of2e-5.\nAfter each epoch, the model is evaluated on the validation set, and metrics such as the validation\nloss, accuracy, precision, recall, and the F1-score are computed. The resulting model is named\n“ElecBERT”, which is denoted as:\nElecBERT = BERT (D, θ∗)\nwhere θ∗ denotes the optimal parameters obtained after fine-tuning.\nCMC, 2023, vol.76, no.3 3353\n4 Experiments and Results\nThe experiments for SVM, NB, and XGBoost were conducted in Google Colab (python 3.7).\nFurthermore, for ElecBERT, NVIDIA TITAN XP 12 GB with 128 GB RAM was used.\nThe proposed ElecBERT model was evaluated through extensive experimentation on two datasets:\nElecSent-Multi-Language and ElecSent-English. The former comprises 5.3 million election-related\ntweets in various languages, while the latter has 4.753 million English-written tweets. The datasets\nwere split into training-validation sets in an 80:20 ratio. The bert-base-uncased pre-trained model\nwas utilized as the initial BERT model. The ElecBERT model was fine-tuned for six epochs with a\nbatch size of 32 and a learning rate of 2e-5. The experiments used the AdamW optimizer with default\nparameters and the cross-entropy loss function.\nThe performance metrics for the proposed ElecBERT model are impressive. Specifically, the\nElecBERT-Multi-Languages model achieved an accuracy of 0.9905, precision of 0.9813, recall of\n0.9819, and an F1-score of 0.9816 during its 5th and 6th epochs. The validation loss at the 6th epoch\nwas 0.140. Comparatively, ElecBERT-English performed better with an accuracy of 0.9930, precision\nof 0.9906, recall of 0.9893, and an F1-score of 0.9899 during the 6th epoch.Figs. 4aand 4b represents\nthe training metrics of both models. Furthermore,Fig. 5shows the validation loss for both models.\nElecBERT-English may have outperformed ElecBERT-Multi-Languages due to the use of V ADER\nto label the ElecSent dataset. V ADER is known to perform better in English than in other languages,\nand this could have led to a higher quality of labeled data for the ElecBERT-English model to train\non, resulting in its better performance on the English dataset. These metrics indicate that ElecBERT\nhas achieved excellent performance on the ElecSent dataset.\nFigure 4: (a) ElecBERT-Multi-Languages | Evaluation Metrics. (b) ElecBERT-English | Evaluation\nMetrics\n0.1364\n0.1184 0.1273 0.1375 0.1409 0.1409\n0.1064 0.0984 0.0873 0.0866 0.0875 0.0811\n123456\nValidation loss\nEpochs\nElecBERT-Multi-Languages ElecBERT-English\nFigure 5:Validation loss for ElecBERT-Multi-Languages and ElecBERT-English\n3354 CMC, 2023, vol.76, no.3\nIn addition, the experiments on XGBoost, SVM, and NB were conducted using the ElecSent\ndataset. Interestingly SVM and Naive Bayes (NB) achieved lower F1-scores 0.802 and 0.826, and\nXGBoost achieved 0.8964 F1-score.Fig. 6shows the evaluation matric, F1-score for the ElecBERT\nmodel, as well as XGBoost, SVM, and NB. This suggests that ElecBERT was able to capture the\nnuances of sentiment in political tweets better than the traditional machine learning models, leading\nto superior performance on this task.\nFigure 6:F1-score | ElecBERT-Multi-Lang, ElecBERT-English, SVM, XGBoost, and NB\n4.1 Leveraging ElecBERT on the 2020 US Presidential Election\nThis section presents a case study using ElecBERT to analyze sentiment in election-related tweets\nduring the US 2020 Presidential Election and predict election outcomes. The study aims to explore\nthe effectiveness of ElecBERT in predicting public sentiment and election outcomes. The results were\ncompared with the actual election outcomes to evaluate the performance of the model.\n4.1.1 Data\nThe data in this study was gathered from December 2019 to November 2020 using hashtags related\nto the Democratic and Republican Parties (#Democratic, #TheDemocrats, #GOP, and #Republican).\nThe dataset consists of 1,637,150 tweets from Republican Party and 245,757 tweets from Democratic\nParty.\n4.1.2 Analyzing the Elections\nFig. 7displays sentiment analysis results for tweets about the Democratic and Republican Parties\nusing three different language models: BERTweet, ElecBERT-Multi-Lang, and ElecBERT-English.\nThe majority of tweets in all categories are classified as neutral sentiments towards both Democratic\nand Republican politicians. This is likely because political tweets often contain objective statements of\nfact or news updates, which may not express a clear sentiment towards a particular politician. However,\nthe results also indicate that both ElecBERT-Multi-Lang and ElecBERT-English are more effective\nthan BERTweet in identifying positive sentiment towards both Democratic and Republican politicians.\nFor example, ElecBERT-Multi-Lang had the highest percentage of positive sentiment classification\nfor Republican politicians at 42.36%, which is significantly higher than BERTweet’s 7.94% positive\nsentiment classification. Similarly, ElecBERT-English had the highest percentage of positive sentiment\nclassification for Democratic politicians at 39.62%, which is also significantly higher than BERTweet’s\n13.43% positive sentiment classification. On the other hand, BERTweet had the highest percentage\nof negative sentiment classification for both Democratic and Republican politicians. For instance,\nCMC, 2023, vol.76, no.3 3355\nBERTweet classified 44.67% of Republican tweets as negative sentiment, which is considerably higher\nthan ElecBERT-Multi-Lang’s 33.36% negative sentiment classification.\nFigure 7:Sentiment analysis for democratic and republican parties using BERTweet, ElecBERT-Multi-\nLang, and ElecBERT-English\nIn addition, two equations (Eqs. (1)and (2)) were utilized in this study to forecast the vote share for\neach political party. The approach involved assuming that positive sentiments expressed towards the\nDemocratic Party and negative sentiments expressed towards the Republican Party represent support\nfor the Democratic Party, and conversely for the Republican Party.\nVote share for Democratic= Dem.Pos. + Rep.Neg.\nDem.Pos + Dem.Neg. + Rep.Pos. + Rep.Neg. (1)\nVote share for Republican= Rep.Pos. + DemNeg.\nDem.Pos + Dem.Neg. + Rep.Pos. + Rep.Neg. (2)\nTable 2presents the vote share percentages for BERTweet, ElecBERT-Multi-Lang, and ElecBERT-\nEnglish, as well as the actual US Election 2020 results. Additionally, the table includes the normalized\nDemocratic and Republican results. The reason for providing the normalized results is that the study\nonly focused on the Republican and Democratic parties and excluded tweets about other political\nparties. Therefore, the sum of the vote share percentages for the two parties in the actual US Election\n2020 results does not add up to 100. To address this, the actual results were normalized to add up to\n100, which facilitates comparison with the results obtained using the three language models.\nTable 2: Vote shares for BERTweet, ElecBERT-Multi-Lang, ElecBERT-English, and actual US\nElection 2020 results along with normalized results\nElection (Prediction/Actual) Democratic Republicans Democratic\nnormalized\nRepublican\nnormalized\nBERTweet 78.25 21.75 78.25 21.75\nElecBERT-Multi-Languages 45.98 54.02 45.98 54.02\nElecBERT-English 46.12 53.88 46.12 53.88\nUS presidential election 2020 51.4 46.9 52.28 47.72\n3356 CMC, 2023, vol.76, no.3\nThe results show that BERTweet predicted a significantly higher vote share for the Democratic\nParty (78.25%) than for the Republican Party (21.75%). In contrast, both ElecBERT-Multi-Lang\nand ElecBERT-English predicted a higher vote share for the Republican Party (54.02% and 53.88%,\nrespectively) than for the Democratic Party (45.98% and 46.12%, respectively). These results indicate\nthat the two ElecBERT models were more accurate in predicting the actual vote share distribution\nbetween the two parties. The actual US Presidential Election 2020 results indicate that the Democratic\nParty received 51.4% of the vote share, while the Republican Party received 46.9%. However, when\nnormalized to 100, the Democratic and Republican results are 52.28% and 47.72%, respectively. This\nnormalization facilitates comparison with the results obtained using the three language models.\nThe Mean absolute error (MAE) and root mean square error (RMSE) was used to compare the\npredicted vote shares with the actual results of the US Presidential Election 2020. MAE measures the\naverage absolute difference between the predicted and actual values, while RMSE measures the square\nroot of the average squared difference between the predicted and actual values. The lower the MAE\nand RMSE values, the closer the predicted results are to the actual election results. The MAE and\nRMSE values for BERTweet, ElecBERT-Multi-Lang, and ElecBERT-English were calculated using\nEqs. (3)and (4), respectively.\nMAE = 1\nN\n∑ N\ni=1\n|Predictedi − Actuali| (3)\nRMSE =\n√∑N\ni=1(Predictedi − Actuali)2\nN (4)\nIn Table 3, both ElecBERT-Multi-Languages and ElecBERT-English outperform BERTweet in\nterms of MAE and RMSE. This suggests that the two models are better at predicting the vote\nshare for the two major parties in the US election. In particular, the MAE for Democratic and\nRepublican parties for both ElecBERT-Multi-Languages and ElecBERT-English is significantly lower\nthan BERTweet. For instance, the MAE for Democratic party prediction using ElecBERT-Multi-\nLanguages and ElecBERT-English are 5.42 and 5.28, respectively, while the MAE for BERTweet is\n26.85. Similarly, the MAEs for Republican party prediction using ElecBERT-Multi-Languages and\nElecBERT-English are 7.12 and 6.98, respectively, while the MAE for BERTweet is 25.15.\nTable 3: MAE, and RMSE for BERTweet, ElecBERT-Multi-Languages, and ElecBERT-English\nModel MAE Dem. MAE rep. MAE RMSE MAE normalized\nBERTweet 26.85 25.15 26 26.014 25.98\nElecBERT-Multi-\nLanguages\n5.42 7.12 6.27 6.327 6.34\nElecBERT-English 5.28 6.98 6.13 6.189 6.16\nMoreover, the RMSE values for both ElecBERT-Multi-Languages and ElecBERT-English are\nalso significantly lower than BERTweet for both Democratic and Republican parties, indicating that\nthe predicted vote shares are closer to the actual vote shares. Finally, the MAE Normalized value in\nthe table shows the average difference between the predicted and actual normalized vote share, which\nis equal to 100 for both parties. Here too, ElecBERT-English models perform better than ElecBERT-\nMulti-Languages BERTweet, with lower MAE values.\nCMC, 2023, vol.76, no.3 3357\nOn the whole, the results suggest that ElecBERT models, both ElecBERT-Multi-Lang and\nElecBERT-English, can perform well in analyzing election-related tweets and predicting election\noutcomes. These models outperformed BERTweet in terms of sentiment analysis and vote share\nprediction. Additionally, the MAE and RMSE values indicate that the ElecBERT models have a\nlower prediction error than BERTweet, especially when it comes to the Democratic party’s vote share\nprediction. Therefore, it is reasonable to assume that ElecBERT models can help analyze and predict\nfuture elections by analyzing large volumes of social media data.\n4.2 Practical Usage of the ElecBERT\nThe proposed ElecBERT model has several practical applications in the field of natural language\nprocessing (NLP) and sentiment analysis. Here are some potential practical usages of ElecBERT:\n1. Sentiment Analysis: ElecBERT can be utilized for sentiment analysis tasks related to election-\nrelated tweets. By leveraging its fine-tuned knowledge of a large corpus of election tweets,\nElecBERT can effectively classify the sentiment of new, unseen election-related tweets as\npositive, negative, or neutral. This can provide valuable insights into public opinion, sentiment\ntrends, and the overall sentiment surrounding political candidates and election events.\n2. Election Monitoring: With its ability to analyze sentiment, ElecBERT can be used for real-\ntime monitoring of elections. By processing a stream of tweets in real time, ElecBERT can help\ngauge the sentiment of the public towards candidates, parties, or specific election issues. This\ncan be valuable for political campaigns, media outlets, and researchers seeking to understand\npublic sentiment and adjust their strategies accordingly.\n3. Social Media Analytics: ElecBERT can contribute to social media analytics by providing a\ndeep understanding of election-related conversations happening on platforms like Twitter.\nBy applying ElecBERT to large volumes of election tweets, analysts can identify emerging\ntopics, detect patterns, and gain insights into voter behavior, public opinion, and the sentiment\ndynamics throughout an election campaign.\n4. Opinion Mining: ElecBERT can assist in extracting and analyzing opinions expressed in\nelection tweets. By leveraging its fine-tuned language understanding capabilities, ElecBERT\ncan help identify and categorize different aspects of political discourse, such as policy issues,\ncandidate attributes, or sentiment towards specific campaign promises. This can support\nopinion-mining tasks and provide a nuanced understanding of voter opinions.\n5. Election Prediction: With its fine-tuned knowledge of election-related tweets, ElecBERT can\npotentially contribute to election outcome prediction models. By analyzing sentiment patterns,\ntrends, and public opinion expressed in tweets, ElecBERT can provide additional insights\nto complement traditional polling methods, enabling more accurate predictions of election\nresults.\n6. Social Listening and Crisis Management: During elections, social media can be a breeding\nground for misinformation, rumors, and crises. ElecBERT can be used as a tool for social\nlistening and crisis management by monitoring election-related conversations on platforms\nlike Twitter. It can help identify potentially problematic content, detect the spread of misinfor-\nmation, and provide real-time sentiment analysis to assist in managing and addressing crises\neffectively.\n3358 CMC, 2023, vol.76, no.3\n5 Conclusion and Future Work\nThis paper presented ElecBERT, a new model for sentiment analysis in the context of election-\nrelated tweets. The model was fine-tuned on two datasets: ElecSent-Multi-Languages, containing 5.31\nmillion labeled tweets in multiple languages, and ElecSent-English, containing 4.75 million labeled\ntweets in English. The ElecSent dataset is labeled (positive, negative, and neutral) using V ADER.\nNotably, ElecBERT showcased superior performance when compared to SVM, NB, and XGBoost,\nachieving an accuracy of 0.9905 and an F1-score of 0.9816 on ElecSent-Multi-Languages, as well\nas an accuracy of 0.9930, and an F1-score of 0.9899 on ElecSent-English. Furthermore, this study\nconducted a comprehensive analysis of the 2020 US Presidential Election as a case study, comparing\nthe performance of different models. Among them, both the ElecBERT-English and ElecBERT-Multi-\nLanguages models outperformed BERTweet, with the ElecBERT-English model achieving an MAE of\n6.13. This paper presents a valuable contribution to sentiment analysis in the context of election-related\ntweets, with potential applications in political analysis, social media management, and policymaking.\nThe ElecBERT model was trained on the 2020 US Presidential Election data only, and its\nperformance on other elections or political events may vary. Additionally, the sentiment labels for the\nElecSent dataset were generated using V ADER, an automated tool, without manual human reviewing\nand verification. In the future, more data from other elections should be added to make the model more\nrobust and generalizable. Moreover, using other pre-trained models like PoliBERT can be explored to\nfurther improve the accuracy of sentiment analysis on election-related tweets. Finally, expanding the\nmodel to incorporate more complex features of political languages, such as sarcasm and irony, could\nlead to a more nuanced understanding of election-related sentiment on social media.\nAcknowledgement: The authors would like to express their sincere gratitude to the Beijing Municipal\nNatural Science Foundation, and the Foundation Enhancement Program for their generous for their\nfinancial support. The authors are deeply appreciative of the support and resources provided by these\norganizations.\nFunding Statement:The research work was funded by the Beijing Municipal Natural Science Founda-\ntion (Grant No. 4212026), and Foundation Enhancement Program (Grant No. 2021-JCJQ-JJ-0059).\nAuthor Contributions:The authors confirm contribution to the paper as follows: Conceptualization:\nA.K., N.B.; methodology: A.K., N.B., and H.Z; software: A.K., and N.B.; validation: A.K., and\nN.B.; formal analysis: H.Z., N.B., A.A., and M.K.; investigation: A.K., N.B., A.A., and M.K.;\nresources: A.K., H.Z., and N.B.; data curation: A.K., and N.B.; writing—original draft preparation:\nA.K.; writing—review and editing: A.K., H.Z, N.B., A.A, and M.K.; visualization: A.K., and N.B.;\nsupervision: H.Z.; project administration: H.Z.; funding acquisition: H.Z. All authors reviewed the\nresults and approved the final version of the manuscript.\nAvailability of Data and Materials:The data used in this study is available at “https://doi.org/10.57967/h\nf/0813”.\nConflicts of Interest:The authors declare that they have no conflicts of interest to report regarding the\npresent study.\nCMC, 2023, vol.76, no.3 3359\nReferences\n[1] R. Rizk, D. Rizk, F . Rizk and S. Hsu, “280 characters to the White House: Predicting 2020 U.S. presidential\nelections from Twitter data,”Computational and Mathematical Organization Theory, 2023. https://doi.\norg/10.1007/s10588-023-09376-5\n[2] A. Oussous, Z. Boulouard and B. F . Zahra, “Prediction and analysis of Moroccan elections using sentiment\nanalysis,”AI and IoT for Sustainable Development in Emerging Countries: Challenges and Opportunities,v o l .\n105, pp. 597–609, 2022.\n[3] P . Chauhan, N. Sharma and G. Sikka, “The emergence of social media data and sentiment analysis in\nelection prediction,”Journal of Ambient Intelligence and Humanized Computing, vol. 12, no. 2, pp. 2601–\n2627, 2021.\n[4] L. Hagemann and O. Abramova, “Crafting audience engagement in social media conversations: Evidence\nfrom the U.S. 2020 presidential elections,” inProc. of the 55th Hawaii Int. Conf. on System Sciences, Hawaii,\nUnited States, pp. 1–10, 2022.\n[5] H. N. Chaudhry, Y . Javed, F . Kulsoom, Z. Mehmood, Z. I. Khanet al.,“Sentiment analysis of before and\nafter elections: Twitter data of U.S. election 2020,”Electronics, vol. 10, no. 17, pp. 2082, 2021.\n[6] R. E. Demillo, G. Solano and N. Oco, “Philippine national elections 2022: Voter preferences and topics\nof discussion on Twitter,” in5th Int. Conf. on Artificial Intelligence in Information and Communication,\nICAIIC, Bali, Indonesia, pp. 724–729, 2023.\n[7] S. Lopez-Fierro, C. Chiriboga-Calderon and R. Pacheco-Villamar, “If it looks, retweets and follows like\na troll; Is it a troll?: Targeting the 2021 ecuadorian presidential elections trolls,” inProc. 2021 IEEE Int.\nConf. on Big Data, Florida, USA, pp. 2503–2509, 2021.\n[8] S. Chatterjee and S. Gupta, “Incremental real-time learning framework for sentiment classification: Indian\ngeneral election 2019, a case study,” in2021 IEEE 6th Int. Conf. on Big Data Analytics, ICBDA 2021,\nXiamen, China, pp. 198–203, 2021.\n[9] A. Ahmad, E. Furey and J. Blue, “An investigation into the impact and predictability of emotional polarity\non the virality of Twitter tweets,” in32nd Irish Signals and Systems Conf., ISSC 202, Athlone, Ireland, pp.\n1–5, 2021.\n[10] H. Ali, H. Farman, H. Yar, Z. Khan, S. Habibet al.,“Deep learning-based election results prediction using\nTwitter activity,”Soft Computing, vol. 26, no. 16, pp. 7535–7543, 2022.\n[ 1 1 ] A .K a r a m i ,S .B .C l a r k ,A .M a c k e n z i e ,D .L e e ,M .Z h uet al.,“2020 U.S. presidential election in swing\nstates: Gender differences in Twitter conversations,”International Journal of Information Management Data\nInsights, vol. 2, no. 2, pp. 100097, 2020.\n[12] O. Olabanjo, A. Wusu, O. Afisi, M. Asokere, R. Padonuet al.,“From twitter to Aso-Rock: A sentiment\nanalysis framework for understanding Nigeria 2023 presidential election,”Heliyon, vol. 9, no. 5, pp. e16085,\n2023.\n[13] K. Brito and P . J. L. Adeodato, “Machine learning for predicting elections in Latin America based on social\nmedia engagement and polls,”Government Information Quarterly, vol. 40, no. 1, pp. 101782, 2023.\n[14] P . Singh, Y . K. Dwivedi, K. S. Kahlon, A. Pathania and R. S. Sawhney, “Can Twitter analytics predict\nelection outcome? An insight from 2017 Punjab assembly elections,”Government Information Quarterly,\nvol. 37, no. 2, pp. 101444, 2020.\n[15] Kaggle, “Political social media posts,” 2023. https://www.kaggle.com/datasets/crowdflower/political-\nsocial-media-posts\n[16] Kaggle, “Global political tweets,” 2023. https://www.kaggle.com/datasets/kaushiksuresh147/political-\ntweets\n[17] A. Chakraborty and N. Mukherjee, “Analysis and mining of an election-based network using large-scale\nTwitter data: A retrospective study,”Social Network Analysis and Mining, vol. 13, no. 1, pp. 74, 2023.\n[18] L. Zhao, L. Li, X. Zheng and J. Zhang, “A BERT based sentiment analysis and key entity detection\napproach for online financial texts,” inProc. of the 2021 IEEE 24th Int. Conf. on Computer Supported\nCooperative Work in Design, CSCWD, Dalian, China, pp. 1233–1238, 2021.\n3360 CMC, 2023, vol.76, no.3\n[19] W . Antoun, F . Baly and H. Hajj, “AraBERT: Transformer-based model for Arabic language understand-\ning,” inProc. of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task\non Offensive Language Detection, Marseille, France, pp. 9–15, 2020.\n[20] D. A. Kristiyanti, A. H. Umam, M. Wahyudi, R. Amin and L. Marlinda, “Comparison of SVM naïve bayes\nalgorithm for sentiment analysis toward West Java Governor candidate period 2018-2023 based on public\nopinion on Twitter,” in2018 6th Int. Conf. on Cyber and IT Service Management, CITSM 2018,P a r a p a t ,\nIndonesia, pp. 1–6, 2019.\n[21] T. V . Cherian, G. J. L. Paulraj, I. J. Jebadurai and J. Jebadurai, “Experimental comparative analysis on\nconvolutional neural network (CNN) and recurrent neural network (RNN) on aspect-level sentiment\nanalysis,” in4th EAI Int. Conf. on Big Data Innovation for Sustainable Cognitive Computing, Coimbatore,\nIndia, pp. 17–27, 2023.\n[22] M. E. Basiri, S. Nemati, M. Abdar, E. Cambria and U. R. Acharya, “ABCDM: An attention-based\nbidirectional CNN-RNN deep model for sentiment analysis,”Future Generation Computer Systems,v o l .\n115, no. 3, pp. 279–294, 2021.\n[23] P . Demotte, K. Wijegunarathna, D. Meedeniya and I. Perera, “Enhanced sentiment extraction architecture\nfor social media content analysis using capsule networks,”Multimedia Tools and Applications, vol. 82, no.\n6, pp. 8665–8690, 2023.\n[24] H. Xia, C. Ding and Y . Liu, “Sentiment analysis model based on self-attention and character-level\nembedding,” IEEE Access, vol. 8, pp. 184614–184620, 2020.\n[25] R. C. Prati and E. Said-Hung, “Predicting the ideological orientation during the Spanish 24M elections in\nTwitter using machine learning,”AI and Society, vol. 34, no. 3, pp. 589–598, 2019.\n[26] D. Beleveslis, C. Tjortjis, D. Psaradelis and D. Nikoglou, “A hybrid method for sentiment analysis of\nelection related tweets,” in4th South-East Europe Design Automation, Computer Engineering, Computer\nNetworks and Social Media Conf., SEEDA-CECNSM 2019, Piraeus, Greece, pp. 1–6, 2019.\n[27] L. Oikonomou and C. Tjortjis, “A method for predicting the winner of the USA presidential elections using\ndata extracted from Twitter,” inSouth-East Europe Design Automation, Computer Engineering, Computer\nNetworks and Social Media Conf., SEEDA_CECNSM, Kastoria, Greece, pp. 1–8, 2018.\n[28] A. Khan, H. Zhang, J. Shang, N. Boudjellal, A. Ahmadet al.,“Predicting politician’s supporters’ network\non Twitter using social network analysis and semantic analysis,”Scientific Programming, vol. 2020, no. 1,\npp. 9353120, 2020.\n[29] M. Anjaria and R. M. R. Guddeti, “A novel sentiment analysis of social networks using supervised\nlearning,” Social Network Analysis and Mining, vol. 4, no. 1, pp. 1–15, 2014.\n[30] R. Srivastava, M. P . S. Bhatia, H. Kumar and S. Jain, “Analyzing Delhi assembly election 2015 using textual\ncontent of social network,” inA C MI n t .C o n f .P r o c .S e r i e s, New Y ork, NY , USA, pp. 78–85, 2015.\n[31] S. Sharma and N. P . Shetty, “Determining the popularity of political parties using Twitter sentiment\nanalysis,” inProc. of the 6th Int. Conf. on FICTA, Odisha, India, pp. 21–29, 2018.\n[32] A. Khan, H. Zhang, N. Boudjellal, L. Dai, A. Ahmadet al.,“A comparative study between rule-based and\ntransformer-based election prediction approaches: 2020 US presidential election as a use case,” inDatabase\nand Expert Systems Applications-DEXA 2022 Workshops, Vienna, Austria, pp. 32–43, 2022.\n[33] K. Jaidka, S. Ahmed, M. Skoric and M. Hilbert, “Predicting elections from social media: A three-country,\nthree-method comparative study,”Asian Journal of Communication, vol. 29, no. 3, pp. 252–273, 2019.\n[34] B. Justino Garcia Praciano, J. P . Carvalho Lustosa Da Costa, J. P . Abreu Maranhao, F . L. Lopes de\nMendonca, R. T. De Sousa Junioret al.,“Spatio-temporal trend analysis of the Brazilian elections based\non Twitter data,” inIEEE Int. Conf. on Data Mining Workshops, ICDMW, Beijing, China, pp. 1355–1360,\n2019.\n[35] M. Z. Ansari, M. B. Aziz, M. O. Siddiqui, H. Mehra and K. P . Singh, “Analysis of political sentiment\norientations on Twitter,”Procedia Computer Science, vol. 167, no. 1, pp. 1821–1828, 2020.\n[36] D. Q. Nguyen, T. Vu and A. Tuan Nguyen, “BERTweet: A pre-trained language model for English Tweets,”\nin Proc. of the 2020 Conf. on Empirical Methods in Natural Language Processing: System Demonstrations,\npp. 9–14, 2020.https://aclanthology.org/2020.emnlp-demos.0/\nCMC, 2023, vol.76, no.3 3361\n[37] K. Kawintiranon and L. Singh, “PoliBERTweet: A pre-trained language model for analyzing political\ncontent on twitter,” in2022 Language Resources and Evaluation Conf., LREC, Marseille, France, pp. 7360–\n7367, 2022.\n[38] J. Lee, W . Y oon, S. Kim, D. Kim, S. Kimet al.,“BioBERT: A pre-trained biomedical language representa-\ntion model for biomedical text mining,”Bioinformatics, vol. 36, no. 4, pp. 1234–1240, 2020.\n[39] A. H. Huang, H. Wang and Y . Yang, “FinBERT: A large language model for extracting information from\nfinancial text∗,” Contemporary Accounting Research, vol. 40, no. 2, pp. 806–841, 2022.\n[40] N. Boudjellal, H. Zhang, A. Khan, A. Ahmad, R. Naseemet al.,“ABioNER: A bert-based model for\nArabic biomedical named-entity recognition,”Complexity, vol. 2021, pp. 1–6, 2021.\n[41] K. K. Bressem, J. M. Papaioannou, P . Grundmann, F . Borchert, L. C. Adamset al.,“MEDBERT.de: A\ncomprehensive german bert model for the medical domain,” arXiv:2303.08179, 2023.\n[42] Y . Liu, R. Zhang, T. Li, J. Jiang, J. Maet al.,“MolRoPE-BERT: An enhanced molecular representation\nwith rotary position embedding for molecular property prediction,”Journal of Molecular Graphics and\nModelling, vol. 118, no. 5, pp. 108344, 2023.\n[43] M. Mujahid, E. Lee, F . Rustam, P . B. Washington, S. Ullahet al.,“Sentiment analysis and topic modeling\non tweets about online education during COVID-19,”Applied Sciences, vol. 11, no. 18, pp. 8438, 2021.\n[44] A. Borg and M. Boldt, “Using V ADER sentiment and SVM for predicting customer response sentiment,”\nExpert Systems with Applications, vol. 162, pp. 113746, 2020.\n[45] A. Vohra and R. Garg, “Deep learning based sentiment analysis of public perception of working from home\nthrough tweets,”Journal of Intelligent Information Systems, vol. 60, no. 1, pp. 255–274, 2022.\n[46] N. V . Chawla, K. W . Bowyer, L. O. Hall and W . P . Kegelmeyer, “SMOTE: Synthetic minority over-sampling\ntechnique,” Journal of Artificial Intelligence Research, vol. 16, pp. 321–357, 2002.\n[47] M. R. Pribadi, D. Manongga, H. D. Purnomo, I. Setyawan and Hendry, “Sentiment analysis of the\npedulilindungi on google play using the random forest algorithm with SMOTE,” in2022 Int. Seminar\non Intelligent Technology and Its Applications: Advanced Innovations of Electrical Systems for Humanity,\nISITIA, 2022-Proc., Surabaya, Indonesia, pp. 115–119, 2022.\n[48] W . Jiang, K. Zhou, C. Xiong, G. Du, C. Ouet al.,“KSCB: A novel unsupervised method for text sentiment\nanalysis,” Applied Intelligence, vol. 53, no. 1, pp. 301–311, 2023."
}