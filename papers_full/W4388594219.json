{
  "title": "Retrieval for Extremely Long Queries and Documents with RPRS: A Highly Efficient and Effective Transformer-based Re-Ranker",
  "url": "https://openalex.org/W4388594219",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2890486346",
      "name": "Arian Askari",
      "affiliations": [
        "Leiden University"
      ]
    },
    {
      "id": "https://openalex.org/A2262246974",
      "name": "Suzan Verberne",
      "affiliations": [
        "Leiden University"
      ]
    },
    {
      "id": "https://openalex.org/A2221537955",
      "name": "Amin Abolghasemi",
      "affiliations": [
        "Leiden University"
      ]
    },
    {
      "id": "https://openalex.org/A74964845",
      "name": "Wessel Kraaij",
      "affiliations": [
        "Leiden University"
      ]
    },
    {
      "id": "https://openalex.org/A280681230",
      "name": "Gabriella Pasi",
      "affiliations": [
        "University of Milano-Bicocca"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4293248102",
    "https://openalex.org/W4225500303",
    "https://openalex.org/W2971209824",
    "https://openalex.org/W3152269143",
    "https://openalex.org/W2997051142",
    "https://openalex.org/W2004902747",
    "https://openalex.org/W6784117431",
    "https://openalex.org/W4225726644",
    "https://openalex.org/W2074122739",
    "https://openalex.org/W3016913119",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3174821968",
    "https://openalex.org/W3017961061",
    "https://openalex.org/W2138621090",
    "https://openalex.org/W3156413894",
    "https://openalex.org/W2929149158",
    "https://openalex.org/W2969574947",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W6771626834",
    "https://openalex.org/W3128431233",
    "https://openalex.org/W3163300289",
    "https://openalex.org/W6785645804",
    "https://openalex.org/W3214374241",
    "https://openalex.org/W2059097316",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2767834779",
    "https://openalex.org/W2168979393",
    "https://openalex.org/W2032854262",
    "https://openalex.org/W2044413982",
    "https://openalex.org/W3214405796",
    "https://openalex.org/W3137130464",
    "https://openalex.org/W4313257365",
    "https://openalex.org/W6786353593",
    "https://openalex.org/W2558203065",
    "https://openalex.org/W6758015726",
    "https://openalex.org/W2942607211",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2967853179",
    "https://openalex.org/W195748530",
    "https://openalex.org/W3037109418",
    "https://openalex.org/W4213191780",
    "https://openalex.org/W3175102705",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4252076394",
    "https://openalex.org/W4251326898",
    "https://openalex.org/W3162626524",
    "https://openalex.org/W2963995225",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3087644421",
    "https://openalex.org/W2570926664",
    "https://openalex.org/W3034707327",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W3001873397",
    "https://openalex.org/W2510561604",
    "https://openalex.org/W3153414861",
    "https://openalex.org/W4200635123",
    "https://openalex.org/W3008374555",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2111593814",
    "https://openalex.org/W3162659806",
    "https://openalex.org/W3124384256",
    "https://openalex.org/W4287199807",
    "https://openalex.org/W3175987672",
    "https://openalex.org/W4300427681",
    "https://openalex.org/W4287814941",
    "https://openalex.org/W4287645952",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W3103587400",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W4214484423",
    "https://openalex.org/W4288253152",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W4287256190",
    "https://openalex.org/W3128525963",
    "https://openalex.org/W4225516685",
    "https://openalex.org/W4287812619",
    "https://openalex.org/W4239019441",
    "https://openalex.org/W3106268368",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W4295312788"
  ],
  "abstract": "Retrieval with extremely long queries and documents is a well-known and challenging task in information retrieval and is commonly known as Query-by-Document (QBD) retrieval. Specifically designed Transformer models that can handle long input sequences have not shown high effectiveness in QBD tasks in previous work. We propose a Re-Ranker based on the novel Proportional Relevance Score (RPRS) to compute the relevance score between a query and the top- k candidate documents. Our extensive evaluation shows RPRS obtains significantly better results than the state-of-the-art models on five different datasets. Furthermore, RPRS is highly efficient, since all documents can be pre-processed, embedded, and indexed before query time that gives our re-ranker the advantage of having a complexity of O(N) , where N is the total number of sentences in the query and candidate documents. Furthermore, our method solves the problem of the low-resource training in QBD retrieval tasks as it does not need large amounts of training data and has only three parameters with a limited range that can be optimized with a grid search even if a small amount of labeled data is available. Our detailed analysis shows that RPRS benefits from covering the full length of candidate documents and queries.",
  "full_text": "Retrieval for extremely long queries and documents with\nRPRS: a highly efficient and effective transformer-based re-\nranker\nAskari, A.; Verberne, S.; Abolghasemi, M.A.; Kraaij, W.; Pasi, G.\nCitation\nAskari, A., Verberne, S., Abolghasemi, M. A., Kraaij, W., & Pasi, G. (2023).\nRetrieval for extremely long queries and documents with RPRS: a highly\nefficient and effective transformer-based re-ranker. Acm Transactions On\nInformation Systems, 42(5). doi:10.1145/3631938\n \nVersion: Publisher's Version\nLicense: Licensed under Article 25fa Copyright Act/Law\n(Amendment Taverne)\nDownloaded from: https://hdl.handle.net/1887/3718733\n \nNote: To cite this publication please use the final published version (if\napplicable).\n115\nRetrieval for Extremely Long Queries and Documents with\nRPRS: A Highly Efficient and Effective Transformer-based\nRe-Ranker\nARIAN ASKARI, SUZAN VERBERNE, AMIN ABOLGHASEMI,a n dWESSEL KRAAIJ,\nLeiden University, The Netherlands\nGABRIELLA PASI,University of Milano-Bicocca, Italy\nRetrieval with extremely long queries and documents is a well-known and challenging task in information\nretrievalandiscommonlyknownasQuery-by-Document(QBD)retrieval.SpecificallydesignedTransformer\nmodels that can handle long input sequences have not shown high effectiveness in QBD tasks in previous\nwork. We propose a Re-Ranker based on the novel Proportional Relevance Score (RPRS) to compute the\nrelevance score between a query and the top-k candidate documents. Our extensive evaluation shows RPRS\nobtains significantly better results than the state-of-the-art models on five different datasets. Furthermore,\nRPRSishighlyefficient,sincealldocumentscanbepre-processed,embedded,andindexedbeforequerytime\nthat gives our re-ranker the advantage of having a complexity ofO(N),w h e r eN is the total number of\nsentences in the query and candidate documents. Furthermore, our method solves the problem of the low-\nresourcetraininginQBDretrievaltasksasitdoesnotneedlargeamountsoftrainingdataandhasonlythree\nparameters with a limited range that can be optimized with a grid search even if a small amount of labeled\ndata is available. Our detailed analysis shows that RPRS benefits from covering the full length of candidate\ndocuments and queries.\nCCS Concepts: •Information systems→Novelty in information retrieval;\nAdditional Key Words and Phrases: Query-by-document retrieval·Sentence-BERT based ranking, Neural\ninformation retrieval\nACM Reference format:\nArian Askari, Suzan Verberne, Amin Abolghasemi, Wessel Kraaij, and Gabriella Pasi. 2024. Retrieval for Ex-\ntremely Long Queries and Documents with RPRS: A Highly Efficient and Effective Transformer-based Re-\nRanker.ACM Trans. Inf. Syst.42, 5, Article 115 (April 2024), 32 pages.\nhttps://doi.org/10.1145/3631938\n1 INTRODUCTION\nQuery-by-document(QBD) retrievalisataskinwhichaseeddocumentactsasaquery—instead\nof a few keywords—with the aim of finding similar (relevant) documents from a document collec-\ntion [31, 79, 80]. Examples of QBD tasks are professional, domain-specific retrieval tasks such\nAuthors’ addresses: A. Askari, S. Verberne, A. Abolghasemi, and W. Kraaij, Leiden University, The Netherlands; e-mails:\na.askari@liacs.leidenuniv.nl, s.verberne@liacs.leidenuniv.nl, m.a.abolghasemi@liacs.leidenuniv.nl, w.kraaij@liacs.\nleidenuniv.nl; G. Pasi, University of Milano-Bicocca, Italy; e-mail: gabriella.pasi@unimib.it.\nPermission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee\nprovided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and\nthe full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be\nhonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish,topostonserversortoredistributetolists,\nrequires prior specific permission and/or a fee. Request permissions frompermissions@acm.org.\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\n1046-8188/2024/04-ART115 $15.00\nhttps://doi.org/10.1145/3631938\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n\n115:2 A. Askari et al.\nas legal case retrieval [5, 65, 68, 83], patent prior art retrieval [19, 49, 51], and scientific literature\nretrieval[15,42].Intheliterature,threeothertermsareusedtorefertothistypeoftask:Query-by-\nExample retrieval, document similarity ranking, and document-to-document similarity ranking.\nTransformer-based ranking models [69], such as BERT-based rankers [46, 78] have yielded im-\nprovements in many IR tasks. However, the time and memory complexity of the self-attention\nmechanism in these architectures isO(L2) for a sequence of lengthL [28]. For that reason, archi-\ntectures based on BERT have an input length limitation of 512 tokens. This causes challenges in\nQBDtaskswherewehavelongqueriesanddocuments.Forinstance,theaveragelengthofqueries\nand documents in the legal case retrieval task of theCompetition on Legal Information Ex-\ntraction and Entailment(COLIEE) 2021 [5]i sm o r et h a n5k words. Variants of Transformers\nthat aim to cover long sequences such as LongFormer [7] and Big Bird [84] have not shown high\neffectiveness, which could be due to either their scattered attention mechanism [63]o rt ot h el i m -\nited number of training instances for QBD tasks [5]. To overcome these limitations, in this article\nwe propose an effective and efficient Transformer-based re-ranker for QBD retrieval that covers\nthewholelengthofthequeryandcandidatedocumentswithoutmemorylimitations.Wefocuson\nre-ranking in a two-stage retrieval pipeline following prior studies that address retrieval tasks in\na multi-stage retrieval pipeline [2,13, 46,47].\nIndomain-specifictasks,thereisoftentheneedforexpensiveprofessionalsearchersasannota-\ntorstocreateQBDtestcollections,e.g.,lawyerannotatorsforcaselawretrievaltasks.Thismakes\nthe annotation process for QBD tasks expensive and, as a result, low-resource training sets are a\nsignificantissueinthesetasks[ 5].OnesuchexampleisthetrainingsetoftheCOLIEE2021dataset\nforcaselawretrievalthatconsistsofonly3,297relevantdocumentsfor900queriesastrainingin-\nstances, which is very few compared to general web search datasets, e.g., MS MARCO [45]h a s\napproximately 161 times more training instances (532,761 queries). This limitation is overcome\nby our proposedmethod, aRe-ranker based on Proportional Relevance Score computation\n(RPRS),whichdoesnotneedlargeamountsoftrainingdata,becauseithasthreeparameterswith\na limited range that can be—but do not necessarily need to be—optimized using a grid search if a\nsmallamountoflabelleddataisavailable.ComparedtoBM25,ourproposedmethodhasonlyone\nmore parameter, which we optimize over 10 different possible values. The two other parameters’\nranges are equal to those of BM25.\nGiven a query document and a set of candidate documents, we split the query and document\ntext into sentences and useSentence-BERT (SBERT)bi-encoders [56] to embed the individual\nsentences. Then, a relevance score is computed by the RPRS relevance model we propose in this\narticle, which is based on the similarity of individual sentences between a query and a document.\nThe intuition behind the proposedmethod is basedon the following assumption: a candidate doc-\nument from the ranked list of documents in response to a query is more likely to be relevant if\nit contains a relatively high number of sentences that are similar to query sentences compared\nto other documents’ sentences in the ranked list. Since RPRS has been designed based on this as-\nsumption,weconductanexperimentalinvestigationtoassessthequalityofourproposedmethod\nby addressing the following research questions:\nRQ1: What is the effectiveness of RPRS compared to the state-of-the-art models for QBD\nretrieval?\nTo answer this question we evaluate the effectiveness of RPRS on legal case retrieval (i.e., given a\nlegalcase,findtherelatedcases)usingtheCOLIEE2021dataset[ 53].AsRPRSusesaSBERTmodel\nto embed query and document sentences, we first run a series of experiments to find the SBERT\nmodelwithwhichRPRSachievesthehighesteffectiveness.Additionally,weinvestigatetwodiffer-\nent unsupervised and self-supervised domain adaption approaches on the top-two most effective\nSBERT models without domain adaption. We conclude that RPRS achieves higher effectiveness\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:3\nwithdomain-specificTransformersintheSBERTarchitecturewhenadaptingthemtothedomain\nby using TSDA [73]. We continue the experiments based on this finding. We also compare RPRS\nto two prior models: (1) the state-of-the-art model on the COLIEE dataset, MTFT-BERT [2], and\n(2) Self-Supervised Document Similarity Ranking (SDR)[21], which also uses bi-encoders\nfor sentence-level representation to compute relevance scores and is the state of the art on two\nWikipedia document similarity datasets.\nFor relevance estimation with RPRS, we split both the query and document content into sen-\ntences.However,thereareotherwaysforsplittingqueryanddocument.Therefore,weinvestigate\nif sentences are the most appropriate and informative textual units for RPRS by addressing:\nRQ2: How effective is RPRS with shorter or longer text units instead of sentences?\nNext, for analyzing the cross-data generalizability of the proposed method we address the follow-\ning question:\nRQ3: What is the effectiveness of RPRS with parameters that were tuned on a different dataset\nin the same domain?\nTo address this question, we have evaluated the proposed method on theCaselaw dataset [36]\nwith the parameters that were tuned on the COLIEE dataset without doing domain adaption on\nthe collection for the Transformer model. In other words, our goal is to analyze how much the\nthree optimized parameters of RPRS on the COLIEE dataset are transferable to another dataset in\nthesame(i.e.,legal)domain.Next,weassessthegeneralizabilityofourmethodbyaddressingRQ4.\nRQ4:TowhatextentisRPRSeffectiveandgeneralizableacrossdifferentdomainswithdifferent\ntype of documents?\nIn this regard, we evaluate our method on two different domains using datasets of patents and\nWikipedia webpages. For patents, we use CLEF-IP 2011 [51], a patent prior art retrieval dataset,\nand for Wikipedia we assess the effectiveness of our method on WWG andWikipedia wine\narticles (WWA) datasets, which are two datasets for similarity ranking between Wikipedia\npages in the wine and video game domains. We show that the proposed method achieves higher\nranking effectiveness over the state of the art in all five datasets (COLIEE, Caselaw, CLEF-IP 2011,\nWWG,WWA).1\nIn summary, our contributions are as follows:\n(1) Weproposeaneffectiveandhighlyefficientre-ranker(RPRS)forQBDtasksthatcoversthe\nfull text of query and candidate documents without any length limitation. The efficiency of\nourmethodisjustifiedbyusingbi-encodersanditseffectivenessisevaluatedonfivedatasets.\n(2) RPRS is suitable for QBD retrieval datasets with low-resource training data, since it has\nonly three parameters with limited range that can be optimized using a grid search even if\na relatively low amount of labelled data is available.\n(3) We show how the use of various SBERT models and adapting them on domain-specific\ndatasets affects the effectiveness of RPRS. This results in three SBERT models in the legal,\npatent, and Wikipedia domains, which we will make publicly available on Huggingface.\n(4) Ourproposedmodeloutperformsthestate-of-the-artmodelsforeachofthefivebenchmarks,\nincluding SDR [21], which was also proposed as a sentence-based ranker for QBD tasks.\n(5) An ablation study shows that Document Proportion—the proportion of the document that\nis relevant to the query—is the most important component in RPRS. In addition, we found\nthe crucial role of covering the full length of queries and documents for QBD tasks by\nstudying the effect of feeding RPRS with truncated queries and documents.\nThe structure of the article is as follows: After a discussion of related work in Section2,w ed e -\nscribetheproposedmethodinSection 3andthedetailsoftheBM25andSDRbaselinesinSection 4.\n1The implementation is available onhttps://github.com/arian-askari/rprs\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:4 A. Askari et al.\nThe experiments and implementation details are covered in Section5. The results are examined\nandtheresearchquestionsareaddressedinSection 6.InSection 7,weinvestigateanddiscussour\nproposed method in more-depth. Finally, the conclusion is described in Section8.\n2 RELATED WORK\nIn the following, we first introduce QBD retrieval tasks. Then, we provide an overview on prior\nmethodsforsentenceembeddingandpre-training.Finally,wegiveanoverviewofpreviousworks\non sentence-level retrieval methods.\n2.1 QBD Tasks\nLegal case retrieval.In countries withcommon lawsystems, finding supporting precedents to a\nnew case, is vital for a lawyer to fulfill their responsibilities to the court. However, with the large\namount of digital legal records—the number of filings in the U.S. district courts for total cases\nand criminal defendants was and is 544,460 in 20202—it takes a significant amount of time for\nlegal professionals to scan for specific cases and retrieve the relevant sections manually. Studies\nhave shown that attorneys spend approximately 15 hours in a week seeking case law [30]. This\nworkloadnecessitatestheneedforinformationretrieval(IR)systemsspecificallydesignedforthe\nlegal domain. The goal of these systems is to assist lawyers in their duties by exploiting AI and\ntraditional information retrieval methods. One particular legal IR task is case law retrieval.\nThe majority of the work on case law retrieval takes place in COLIEE [53, 55]. Askari and Ver-\nberne[5] combine lexicaland neuralrankingmodels for legalcaseretrieval.Theyoptimize BM25\nandobtainstate-of-the-artresultswithlexicalmodels.However,recently,ontopoftheoptimized\nBM25, Abolghasemi et al. [2] present multi-task learning as a re-ranker for QBD retrieval and set\nthenewstateoftheart.ThelimitationofthemethodinReference[ 2]isthattheinputislimitedby\ntheBERTarchitecture(512tokens)[ 17].Therefore,itcannotcoverthefulltextoflongdocuments\nin QBD datasets like COLIEE.\nPatent prior art retrieval.Patents serve as stand-ins for various domains including technologi-\ncal, economic, and even social activities. TheIntellectual Property (IP)system encourages the\ndisclosure of innovative technology and ideas by granting inventors exclusive monopoly rights\nover the economic value of their inventions. Therefore, according to Piroi et al. [51], patents have\na considerable impact on the market value of companies. With the number of patent applications\nfiled each year continuing to rise, the need for effective and efficient solutions for handling such\nhugeamountsofinformationgrowsmoreessential.Therearevariouspatentanalysistasks.Inthis\nwork, we focus on patent prior art retrieval, the aim of which is to find patent documents in the\ntarget collection that may invalidate a specific patent application [49,51].\nPatentretrievalisachallengingtask:Thedocumentsarelengthy,andthelanguageisformal,le-\ngal,andtechnicalwithlongsentences[ 70].Shalabyetal.[ 64]provideadetailedreviewofresearch\non patent retrieval that shows that the most successful methods on the CLEF-IP [51] benchmarks\naretraditionallexical-basedmethods[ 50,71],sometimesextendedwithsyntacticinformation[ 18].\nThere are more recent studies that focus on patent retrieval in passage-level assessment [4, 25].\nHowever, recent studies do not address document-level patent retrieval due to the limitation of\nTransformer-basedmethodsontakingintoaccountthefulllengthofthedocuments.Mahdabiand\nCrestani[40]proposeaneffectiveapproachforpatentpriorartretrieval,whichisbasedoncollect-\ning a citation network using a specific API that is not provided by the organizers of the CLEF-IP\ndataset. Taking into account the fact that this setup is different from the original setup, we do not\nconsider this approach as our baseline for a fair comparison.\n2https://www.uscourts.gov/statistics-reports/judicial-business-2020\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:5\nDocument similarity for Wikipedia-based datasets. Estimating the similarity within\nWikipedia pages is useful for many applications such as clustering, categorization, finding rele-\nvantwebpages,andsoon.Ginzburgetal.[ 21]proposetwonewdatasetsonWikipediaannotated\nby experts. The documents come from the wine and video game domains; we refer to the collec-\nt i o n sa s( 1 )Wikipedia video games (WVG)and (2) WWA. For both datasets, the task is finding\nrelevant Wikipedia pages given a seed page.\n2.2 Sentence Embedding\nSentence embedding is a well-studied topic and it is suitable for measuring sentence similarity,\nclustering, information retrieval via semantic search, and so on. BERT-based sentence embedding\nmethodsemployTransformermodelstoeffectivelyandefficientlyembedsentences.Thechallenge\nis that BERT’s architecture makes it inappropriate for semantic similarity search in its original\nconfiguration, since it requires both sentences to be concatenated into the network and applies\nmultiple attention layers between all tokens of both sentences, which has a significant computa-\ntionaloverhead.Forinstance,tofindthemostsimilarpairinacollectionof10,000sentencesBERT\nneeds roughly 50 million inference calculations, which takes around 65 hours [56].\nHumeau et al. [26] introduce poly-encoders to tackle the runtime overhead of the BERT cross-\nencoder and present a method to calculate a score employing attention between the context vec-\ntorsandpre-computedcandidateembeddings.However,Poly-encodershavethedisadvantagethat\ntheirscorefunctionisnotsymmetricandthattheircomputingcostisexcessivelyhighandrequires\nO(n2) scorecalculations.ReimersandGurevych[ 56]thereforeproposedSBERTasavariantofthe\npre-trained BERT model that uses siamese and triplet network architectures to produce semanti-\ncally meaningful embeddings that can be compared using cosine similarity. SBERT decreases the\ntimeittakestofindthemostsimilarpairfrom65hourstoaround5secondswhilekeepingBERT’s\nperformance [56]. In this work, we utilize SBERT as our embedding model, making the efficiency\nofRPRSmuchhigherthancross-encoderbasedre-rankers.WeexploitaselectionofSBERTmodels\nthat are tuned on different datasets.\nItisnoteworthytomentionthatwhileourproposedmethodsleveragebi-encoders,specifically\nSentence-BERT Reimers and Gurevych [56], to re-rank the first-stage retriever’s ranked list, our\napproach fundamentally diverges from dense passage retrievers that employ bi-encoders as first\nstage retrievers in terms of both methodology and application [27]. Dense retrievers emphasize\non the alignment of query and relevant document representations by optimizing a targeted loss\nfunction, such as the mean square error between the query and relevant document vectors while\nwedonothavesuchtraininginourproposed.Moreover,optimizingdenseretrieversonthequery-\nby-document task with extremely lengthy queries or documents is not computationally possible\ndue to BERT model’s maximum word limit of 512 words, which prevents them from representing\neach query or document by a single pass to a bi-encoder and, as a result, prevents them from\nbeingabletooptimizingtherepresentationofqueryanddocument,astheydonothaveoneunion\nrepresentation. There could be future work on studying how applying dense passage retrieval on\nquery by document task is out of the scope of this study.\n2.3 Unsupervised Sentence Embeddings Training\nPrevious works on unsupervised sentence embeddings learning have achieved promising re-\nsults on semantic textual similarity tasks by combining pre-trained Transformers with various\ntraining objectives. Carlsson et al. [10] propose Contrastive Tension, which views identical and\ndifferent sentences as positive and negative examples, respectively, and trains two independent\nencoders.BERT-flow[ 32]debiasestheembeddingdistributiontowardGaussiantotrainthemodel.\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:6 A. Askari et al.\nSimCSE[20]usescontrastivelearning[ 12,23]toclassifyidenticalsentenceswithdifferentdropout\nmasks as positive instances.\nWang et al. [73]proposeanunsupervisedstate-of-the-artmethodcalledTSDAEthatisbasedon\nTransformersanddenoisingauto-encodersthatencodedamagedsentencesintofixed-sizedvectors\nandrequirethedecodertoreconstructtheoriginalsentencesfromthesesentenceembeddings[ 73].\nWang et al. [74]propose GenerativePseudoLabeling(GPL) ,whichcombinesaquerygenerator\nwithpseudolabelingfromacross-encoder.However,theGPLmethodologyisnotsuitableforQBD\ntasks as it relies on cross-encoders that are limited both in length and efficiency. We evaluate the\neffect of two state-of-the-art pre-training methods in Section6.\n2.4 Sentence-level Retrieval\nAddressingrelevanceofcandidatedocumentsbyleveragingsentence-levelevidencehasbeenstud-\nied for long documents retrieval in the past years [3,24,33,81,82]. However, there is no work on\nsentence-basedrelevancescorecomputationusingTransformer-basedmodels(i.e,SBERT[ 56])on\nQBDretrievaltaskssimilartoourapproach.Yilmazetal.[ 81]applyinferenceatthesentencelevel\nfor each document and aggregate the sentence-level inference by learning a weight for each top-\nscoringsentenceineachcandidatedocument.Zhangetal.[ 85]observethatthe“best”sentenceor\nparagraphinadocumentgivesadecentproxyfordocumentrelevance,whichwastheinspiration\nf o rY i l m a ze ta l .[81] as well. We also use this intuition in our approach to QBD Retrieval, where\nboththequeryandthedocumentsarelongtexts.Mysoreetal.[ 41]proposeascientificdocument\nsimilaritymodelbasedonsentence-levelsimilaritythatleveragesco-citationsentencesasasource\nof document similarity.\nRecently, Ginzburg et al. [21] proposed an unsupervised ranker called SDR that computes the\nfinal relevance score by computing two sentence- and paragraph-level matrices. They evaluate\nSDR’s effectiveness on two new datasets annotated by human experts. We replicate SDR [21]a s\nthe most recent and comparable methodology to RPRS, because (1) although SDR’s mechanism\nis dissimilar to RPRS fundamentally, it also is a sentence-level relevance scoring model designed\nfor QBD tasks that covers the full length of both queries and candidate document texts using\nsentenceembeddings,and(2)similarlytoRPRS,SDRhasacomplexityof O(N),where N iscountof\nsentences—duetotheutilizationofbi-encodersentenceembeddings.Therefore,besidescomparing\nRPRS with the state-of-the-art model on each dataset, we compare its effectiveness to SDR. SDR’s\narchitecture makes it suitable for both full-ranking and re-ranking settings.\n3 PROPOSED METHOD: RPRS\nAsmentioned,weassumethatacandidatedocument d islikelytoberelevanttoaquerydocument\nq ifalargeproportionof d issimilarto q andalargeproportionof q issimilarto d.Aspecificchal-\nlengeinQBDtasksisthatadocumentcanbeverylong(morethan10kwords),anditmaycontain\nseveral topics. For a long query documentq, this can cause an irrelevant candidate documentd\npositioned on top, because only one topic ofd is very similar to one topic ofq. Similarly, a long\nirrelevant candidate document could be ranked on top because one of its topics is very similar\nto only one topic of query document. We address this problem in our method by integrating the\nlengthofthequeryanddocumentintoEquations( 4)and( 5)thatweelaborateoninthefollowing.\nIn the following, we define our concepts and proposed methodology.\n3.1 Definitions\nInTable 1,weshowthelegendofthesymbolswe employtoformallyintroduceourmethodology.\nInoursentence-basedrelevancemodel,wefirstretrievethesetofdocuments TKq forqwithanini-\ntialranker.Given q,wecomputethecosinesimilarityofeach −→qs witheach −→ds,i.e.,wecomputethe\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:7\nFig. 1. The workflow of computingRn given a query documentq and set of top-k candidate documents;i\nand j refer to the last sentence of each query and document.\nTable 1. Meaning of Symbols\nSymbol Meaning\nq and d query and candidate document\nqs andds a sentence ofq andd respectively\n−→qs and−→ds vector representation (sentence embedding) ofqs andds\nSd andSq the sets of all sentences ofd andq.\nTKq set of top-k candidate documents retrieved forq\nSTKq set of all sentences fromTKq\nr(qs,STKq ) a ranked list ofSTKq forqs\nrn(qs,STKq ) top-n sentences fromr(qs,STKq )\nRn(Sq,STKq ) contains allrn(qs,STKq ) for all sentences ofSq\nThe bold face is used to represent highest effectiveness in terms of a metric across the models.\nsimilarityofeachquerysentencewitheachsentencein STKq .Theresult r(qs,STKq ) isarankedlist\nof sentences fromSTKq forqs.T h es e trn(qs,STKq ) contains the top-n sentences fromr(qs,STKq ),\ni.e., the document sentences most similar toqs.F r o mn o wo n ,w ec a l lads “most similar”t oqs if\nds is a member ofrn(qs,STKq ), meaning that it is among the top-n sentences from the ranked list\nfor sentences from the top-k candidate documents. This indicates that the document sentenceds\nis placed among the top-n highest most similar document sentences to the query sentenceqs.\nThe setRn(Sq,STKq ) contains allrn(qs,STKq ) for all query sentences. Figure1shows the work-\nflow of computingRn step by step. Based on our assumption,d andq a r el i k e l yt ob er e l e v a n ti fa\nlarge proportion ofSd occurs in a large proportion of eachrnsf r o mRn.\n3.2 Re-ranker based on Proportional Relevance Score\nWe formally define the proportional relevance score forRPRS as\nRPRS(q,d,STKq ,n) =QP(Sq,Sd,STKq ,n) ×DP(Sd,Sq,STKq ,n), (1)\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:8 A. Askari et al.\nw h e r ew eu s et h eProportional Relevance Score (PRS), QP is Query Proportion, andDP is\nDocumentProportion.Given q,wecompute PRS ford basedonthe Rn andSTKq .Theparameter n\ncontrols the number of top-n similar sentences per query sentence inrn andRn. In the following,\nwedefinetwofunctions, qRn (Sq,Sd,STKq ) anddRn (Sd,Sq,STKq ),thatweuseinthelaterequations\nfor computingQP andDP,\nqRn (Sq,Sd,STKq ) =\nSq∑\nqs\nmin(1,|Sd ∩rn(qs,STKq )|), (2)\nwhere |x|denotes the cardinality of setx and themin function returns 1 if at least one of the\nsentencesof d (Sd’ssentences)isin rn(qs,STKq ),zerootherwise.Herewedonottakeintoaccount\nthe repetition ofSd’s sentences in eachrn for aqs because of themin function. However, we\nincorporate that in a controllable way in the extended variation (RPRSw/freq ) in Section3.3 by\ndefiningparameter k1.Thefunction qRn countsforhowmanysentencesof q,atleastonesentence\nof the candidate document sentences (Sd) occurs at least one time in the set ofqs’s rn.N e x t ,w e\ndefinedRn , which is the main component for computingDP as\ndRn (Sd,Sq,STKq ) =\nSd∑\nds\nmin/parenlefttpA/parenleftexA\n/parenleftbtA\n1,\nRn∑\nrn\n|{ds}∩rn|/parenrighttpA/parenrightexA\n/parenrightbtA\n, (3)\nwhere {ds}denotes a singleton, i.e., a set with only one sentence ofSd. For a query documentq\nand a candidate documentd, the functiondRn iterates over all sentences ofSd and counts how\nmany sentences ofd occur at least one time inRn. Given the parametern,i ti sp o s s i b l et h a tm o r e\nthan one sentence ofd occurs inrn for a query sentence. We argue that Equations (2) and (3)\nare complementary to each other as each equation assess the relevance from either the query or\ncandidate document perspective. We show the impact of each equation by the ablation study in\nSection 7.1.W en o wd e fi n eQP andDP based on Equations (2) and (3),\nQP(Sq,Sd,STKq ,n) =\nqRn (Sq,Sd,STKq )\ncount ofq’s sentences, (4)\nDP(Sd,Sq,STKq ,n) =\ndRn (Sd,Sq,STKq )\ncount ofd’s sentences. (5)\nIt is noteworthy that the denominator ofQP (count ofq′s sentences) has the same value for all\ncandidate documents, and thus it could be ignored for ranking; however, we keep it as it makes\nrelevance scores comparable for score analysis.\nIntuitively, the relevance score ofd should increase by having more of its sentences inRn for\nqueryq. The advantage of this design is that the candidate documentd receives the highest rele-\nvance score if allrns’ sentences inRn are fromd. However,d receives the lowest relevance score\nifnoneofitssentencesarein rns’sentencesin Rn.However,thereisadisadvantageinthisdesign,\nas it does not consider the repeated appearances of candidate document sentences inrn’s. There-\nfore, we propose another variation on RPRS called RPRS w/freq that takes into account this in a\ncontrollable way explained in the next section.\n3.3 Taking into Account Frequency (RPRS w/freq)\nIntheprevioussection,byusingtheminimumfunctioninEquations( 2)and( 3),weonlyaccounted\nfor the occurrence ofat least oneof the candidate documents’ sentences inrn and Rn. In other\nwords, we did not consider the frequency of the occurrences in our definition for functionsqRn\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:9\nanddRn .Toempowerourmodel,wedefine FqRn andFdRn asthemodifiedversionsof qRn anddRn\nthat take into account the frequency,\nFqRn (Sq,Sd,STKq ) =\nSq∑\nqs\n|Sd ∩rn(qs,STKq )|\n|Sd ∩rn(qs,STKq )|+k1\n(\n(1−b) + b·dl\navдdl\n), (6)\nFdRn (Sd,Sq,STKq ) =\nSd∑\nds\n∑Rn\nrn |{ds}∩rn|\n∑Rn\nrn |{ds}∩rn|+k1\n(\n(1−b) + b·dl\navдdl\n). (7)\nHere, inspired by the BM25 Okapi schema, we take the frequency into account in a controllable\nway.Tothisaim,weintroduceamodifiedversionofRPRS,whichwedenotebyRPRSw/freq,which\nreliesontwoparameters: k1,thefrequencysaturationparameter, 3 forcontrollingtheeffectoffre-\nquency of occurrence of a candidate document’s sentence inRn,a n db, the document length nor-\nmalizationparameter,forcontrollingtheeffectofthecandidatedocumentlength.Similarlytothe\nBM25formula, dl referstodocumentlengthand avдdl referstoaveragelengthofdocuments.The\nadvantageofthefrequencysaturationparameter( k1)oftheproposedmethodthatworkssimilarly\ntoBM25’s“termsaturation”mechanism,isthattheoccurrenceofmultiplesentencesofacandidate\ndocument in only onern of Rn has a lower impact on the relevance score than the occurrence of\nmultiplesentencesofacandidatedocumenteachofwhichoccursonlyonceinseveraldifferent rns\nofRn.ThischaracteristicisinlinewithBM25’sconceptoffrequencysaturation,whichpreventsa\ndocumentfromobtainingahighscoresolelybasedontherepetitionofasinglewordthatmatches\njustonewordfromthequery.Weparameterizethedegreeofnormalizingtherelevanceaccording\nto the document length (i.e., number of sentences of a document) with the parameterb,w h i c h\nworks similarly to BM25’s length normalization parameter [34, 58]. Ifb = 0, then the relevance\nscoreisnotnormalizedbythedocumentlengthatall,becausetherightsideofthedenominatorin\nEquations(6)and (7)willbe k1((1−b) + b·dl\navдdl ) = ((1−0) + 0·dl\navдdl ) =k1((1) +0) =k1.Therefore,\nonlyk1willbekeptinthedenominatorandthewholedenominatorwillbe |Sd ∩rn(qs,STKq )|+k1\nand ∑Rn\nrn |{ds}∩ rn|+k1 for Equations (6) and (7), respectively. Asb increases from 0 toward 1,\nthe impact of document length—compared to the average length of documents (dl/adl)—in nor-\nmalizing the relevance score will be higher. Consequently,b = 1 means the relevance score will\nbe fully normalized based on the document length, because the right side of the denominator in\nEquations(6)and( 7)willbe k1((1−b) + b·dl\navдdl ) = ((1−1) + 1·dl\navдdl ) =k1((0) + 1·dl\navдdl ) =k1· dl\navдdl .\nTherefore, the document length will be divided by the average length of documents and will be\nmultiplied tok1. As a result, the denominator will be fully normalized based on the ratio of docu-\nmentlengthtotheaveragelengthofalldocumentsinthedenominator,andthewholedenominator\nwill be|Sd ∩rn(qs,STKq )|+k1 · dl\navдdl and ∑Rn\nrn |{ds}∩rn|+k1 · dl\navдdl for Equations (6) and (7),\nrespectively.\nInsummary,thefinalproposedmethodhasthreeparameters: k1and b,asdescribedabove,and n\nthatcontrolsthenumberoftop- n sentencesin rn perquerysentence.Allparameterscanbetuned\non the training set. Furthermore, the parameters can be used with default values or with values\nthat are obtained by tuning the method on another dataset. WithRPRS and RPRS w/freq, we only\nneed to compute the cosine similarity between embeddings of query sentences and the document\n3Togainadeeperinsightintok1andfrequencysaturation,werecommendreferringtotheoriginalBM25paperRobertson\nand Walker [58]. For a more accessible explanation of frequency saturation and its effects, you can explore the details\nprovided in Reference [60]. This resource offers a step-by-step breakdown that should make it easier to understand how\nthis parameter works.\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:10 A. Askari et al.\nsentences. Our proposed method is efficient, because all documents can be pre-processed, embed-\nded,andindexedbeforequerytime.Atquerytime,producingtheembeddingforquerysentences\nusingSentenceBERTishighlyefficient[ 56].Thisgiveseither RPRSorRPRSw/freq theadvantageof\nhavingacomplexityof O(N),where N isthetotalnumberofsentencesinthequeryandcandidate\ndocumentscomparedtore-rankersbasedonCross-encoderswith O(N2).Furthermore,calculating\ncosine similarity is a simple operation, thus resulting in a fast inference time.\n4 BASELINE RETRIEVAL MODELS\nIn this section, we introduce BM25 and SDR that are the two main baselines in our experiments.\n4.1 BM25\nLexical retrievers estimate the relevance of a document to a query based on word overlap [57].\nMany lexical methods, including vector space models, Okapi BM25, and query likelihood, have\nbeen developed in previous decades. We use BM25 because of its popularity as first-stage ranker\nin current systems and its strong effectiveness on QBD tasks [59]. Based on the statistics of the\nwords that overlap between the query and the document, BM25 calculates a score for the pair:\nslex (q,d) = BM25(q,d) =\n∑\nt∈q∩d\nrsjt. tft,d\ntft,d +k1\n{\n(1−b) +b |d|\nl\n}, (8)\nwhere t is a term,tft,d is the frequency oft in documentd, rsjt is the Robertson-Spärck Jones\nweight [58]o ft,a n dl is the average document length;k1 andb are parameters.\n4.2 Birch\nYilmaz et al. [81] present a simple yet effective solution for applying BERT to long document\nretrieval: The inference is applied to each sentence in a candidate document, and sentence-level\nevidence is aggregated for ranking documents as follows:\nScored =a ·Sdoc + (1−a) ·\nn∑\ni=1\nwi ·Si, (9)\nwhere Sdoc represents the original document score andSi denotes theith top-scoring sentence\naccording to BERT. The parametersa and wi’s can be learned or used with default values. To\nreplicatethe“3S:BERT”models,referredtoas3S-Birchhereafter,weutilizetheofficialimplemen-\ntation from the Birch paper, which uses the three top-scoring sentences.4 We report the 3S-Birch\napproach,asityieldedthehighesteffectiveness,eventhoughweexperimentedwiththetop-1and\ntop-2 scoring sentences. Until now, the effectiveness of Birch has only been analyzed and proven\nfor short queries and long documents. In contrast, our investigation focuses on evaluating its per-\nformance in situations where both queries and documents are extremely long. We use the same\nBERT model that we use for our proposed method per each dataset.\n4.3 SDR [21]\nWe replicate SDR [21] as the most recent and comparable methodology to RPRS, because (1) al-\nthoughSDR’smechanismisdissimilartoRPRSfundamentally,italsoisasentence-levelrelevance\nscoring model designed for QBD tasks that cover the full length of both queries and candidate\ndocument texts using sentence embeddings, and (2) similarly to RPRS, SDR has a complexity of\nO(N) where N is count of sentences—due to the utilization of bi-encoder sentence embeddings\n4https://github.com/castorini/birch\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:11\ninstead of cross-encoders—for computing relevance scores in contrast to other techniques with\nO(N2) [6,17].Therefore,besidescomparingRPRSwiththestate-of-the-artmodeloneachdataset,\nwe compare its effectiveness with SDR. SDR’s architecture makes it suitable for both full-ranking\nandre-rankingsettings;asaresult,weevaluateitinbothconfigurationsandonlyreporttheresult\nof the setup (ranker or re-ranker) in which it performs best. In the following, we introduce SDR’s\npre-training and inference methodology.\n4.3.1 Self-supervised Pre-training. Given a collection of documentsD, SDR samples sentence\npairsfromthesameparagraphofagivendocument(intra-samples)andsentencepairsfromdiffer-\nent paragraphs taken from different documents (inter samples) with equal probability (i.e., 0.5f o r\neach type of sampling). It tokenizes sentences and aggregates them into batches, and it randomly\nmasks them in a similar way to the RoBERTa pre-training paradigm. The authors use the Roberta\nmodelfortheirimplementation[ 35].ThepretrainingobjectiveofSDRcomprisesadual-termloss:\n(1)astandardMLMlossadoptedfromReference[ 17]thatallowsthemodeltospecializeinthedo-\nmainofthegivencollectionofdocuments[ 22],and(2)acontrastiveloss[ 23]aimstominimizethe\ndistance between the representations of sentence-pairs from the same paragraph (intra-samples),\nwhile maximizing the distance between the representations of sentence-pairs from different para-\ngraphs (inter-samples).\n4.3.2 Inference. SDR computes the relevance score between a queryq and a candidate docu-\nmentd bycomputingatwo-stagehierarchicalsimilarityscore:(1)SDRcreatesasentencesimilar-\nity matrixM for all possible pairs of query paragraphs and candidate document paragraphs. Each\ncell inM represents the cosine similarity between a sentence from paragraphi of the query and\na sentence from paragraphj of the candidate document. Specifically, the rows ofM correspond\nto sentences from the query paragraph, and the columns correspond to sentences from the can-\ndidate document paragraph. Therefore, a cellMi, j\na,b represents the cosine similarity between sen-\ntencea fromparagraph i ofthequeryandsentence b fromparagraph j ofthecandidatedocument.\n(2)Next,aparagraphsimilaritymatrix P iscreatedforthecandidatedocument d basedonallpairs\nof paragraphs (query paragraph vs. document paragraph). Each cell inP represents the similarity\nbetween a paragraph from the query and a paragraph from the candidate document and is com-\nputed using the maximum cosine similarity between sentences in the two paragraphs one from\nthe query and one from the candidate document. The motivation for the creation of matrixP is\nthatsimilarparagraphpairsshouldincorporatesimilarsentencesthataremorelikelytocorrelate\nunder the cosine metric.\nFinally, P is normalized globally (NRM (P)), based on other candidate documents’ paragraph\nsimilaritymatrices.Basedonthe NRM (P) generatedforacandidatedocument,thetotalsimilarity\nscore S is obtained by computing an average of the highest cosine similarity scores in each row\nofP. The motivation forS is that the most correlated paragraph pair only contributes to the total\nsimilarity score. In contrast to RPRS, SDR does not take into account the length of candidate doc-\nument, because the denominator in its formula for computing matrixP is count of sentences in a\nquery’sparagraphandfortotalscore S isthecountofparagraphsinthequery.However,thisissue\nhas been handled in our re-ranker by integrating the length of the query and document intoQP\nandDP (see Section3.2). Moreover, if one paragraph of a candidate document be the most similar\nparagraphtoallparagraphofquery,thenSDRdoesnotpenalizethisrepetitionwhilewetakethat\nintoaccountwith k1parameterin RPRSw/freq thatcontrolsfrequencysaturation(seeSection 3.3).\nIn addition, the cosine score is used directly into the approach, while we only consider that score\nfor ranking sentences inRn (see Section3.2 and Figure1).\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:12 A. Askari et al.\n5 EXPERIMENTS\nIn this section, we first describe the datasets, the pre-trained Transformers that we use, and our\nimplementation details for the retrieval models. Finally, we provide information about parameter\ntuning and pre-processing of the dataset.\n5.1 Datasets\nWeevaluateourmodelsonthreeQBDretrievaltasks:legalcaseretrieval,patentpriorartretrieval,\nand document similarity ranking for Wikipedia pages.\nCOLIEE’21.We first use the COLIEE’21 dataset to evaluate the effectiveness of the proposed\nmethod(PRS)onlegalcaseretrieval.Thereare650querydocumentsinthetrainsetand250inthe\ntestset[ 53],with4,415documentsascandidatedocumentsinbothsets.Thecandidatedocuments’\naverage length is 5,226 (tokenized by SparkNLP [29], see below), with outliers reaching 80,322\nwords.\nCaselaw. Toinvestigatethegeneralizabilityofourmodel, RPRSw/freq,wetuneditontheCOL-\nIEE’21datasetandevaluatethetunedmodelonadifferentdataset:theCaselawdataset[ 36].There-\nfore, we use the Caselaw dataset as a test set to assess how well the tunedRPRS w/freqmodel can\ngeneralizetonewandunseendata.TheCaselawdatasetcontains100querydocuments,2,645rele-\nvanceassessments,and63,916candidatedocuments.InCaselaw,thecandidatedocuments’average\nlength is 2,344, with outliers up to 124,092 words.\nCLEF-IP 2011. For patent prior art retrieval [51], we experiment on the CLEF-IP 2011 dataset\nthat contains 300 and 3,973 query documents in the train set and test set, respectively, both of\nwhich have document-level relevance assessments. The count of candidate documents per query\nis about 3 million. In this work, we select the English subset of CLEF-IP 2011 that contains about\n900,000 candidate documents per query and 100 and 1,324 query documents in the train set and\ntest set, respectively. We concatenate title, abstract, description, and claims as the whole patent\ndocument. The candidate documents’ average length in CLEF-IP is 10,001 words, with outliers up\nto 407,308 words.\nWikipedia. For the Wikipedia datasets,we use the WVG and the WWAdatasets [21]t h a tc o n -\ntain 21,935 and 1,635 candidate documents and 90 and 92 query documents, respectively. The\nWWG collection consists of articles reviewing video games from all genres and consoles and the\nWWA collection consists of a mixture of articles discussing different types of wine categories,\nbrands,wineries,grapevarieties,andmore.Thedocuments’averagelengthis1 ,061and966,with\noutliers up to 23,048 and 13,081 words for the WVG and WWA datasets, respectively.\n5.2 Baselines\nBM25 has previously been shown to be a strong baseline for QBD retrieval [59], and it holds the\nstate of the art among all lexical models on COLIEE [5]. We implement BM25 as the initial ranker\nusing Elasticsearch on all five datasets. In addition to BM25 on the complete query document\ntext, we employ BM25 on the top-10 percent of query document terms that are extracted and\nscored usingKullback–Leibler divergence for Informativeness (KLI)[72] following prior\nwork [5, 36]. As KLI has been shown to be an effective approach for making shorter queries for\nBM25 [36], we employ KLI on documents of all three datasets and refer to that in the result tables\nas “BM25 + KLI.”\nMoreover, we replicate the SDR [21] ranker asSDRinf where “inf” refers to inference.SDRinf\nis a comparable sentence-level baseline toRPRS w/freq. It should be noted that wherever SDR is\nmentioned in the tables, it is referring to the best-performing SDR variant, either as a re-ranker\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:13\nafter the best first-stage ranker, or as a full ranker. We experiment with Roberta besides the other\nSBERT’s models as SDR’s authors use the Roberta model for their implementation [35].\nForCOLIEE2021,wecomparetheproposedmethodtothelexicalstate-of-the-artmodel,which\nis BM25optimised with optimized parameters (b = 1,k = 2.8) [5]. Additionally, we do compar-\nisonswithtwore-rankers:theBERTre-ranker[ 46],andthemulti-taskfine-tunedBERTre-ranker\n(MTFT-BERT),whichistheneuralstateoftheart on COLIEE [ 2]. For Caselaw[36], were-use the\nlexicalandneuralmodelsthatwefine-tunedonCOLIEEtoanalyzethegeneralizabilityofthemand\nthe proposed method. The state-of-the-art method for Caselaw isK [36].5 We found thatK’s run\nfileisthemosteffectiveinitialranker,andthusweusedthatasourinitialrankeronCaselaw[ 36].\nFor patent retrieval, there is no Transformer-based method baseline, which could be due to the\nfact that average length of documents in the patent dataset is around 10,001 and thus cannot be\nhandledbyTransformermodelsstraightforwardly.Therefore,wecompareourresultwiththebest\ntwo methods for the English language in the CLEF IP 2011 competition based on Figure3 from\nPiroi et al. [51]: Ch.2 and Hy.5. We found that Ch.2 is the most effective method on CLEF-IP 2011,\nand thus we used that as our initial ranker.\nFortheWikipediadatasets,wecompareourresultwith SDRinf ,whichproposedthesedatasets\nrecently and is the state-of-the-art model for them.\n5.3 Pre-trained Sentence BERT Models\nReimers and Gurevych [56] have published a set of SBERT embedding models that they trained\nand evaluated extensively with respect to their effectiveness for semantic textual similarity (Sen-\ntence Embeddings) and Semantic Search tasks on 14 and 5 different datasets respectively.6 We\nexploit the following top-4 ranked embedding models7 that are different in terms of training data\nor architecture:\n—all-mpnet-base-v2,8 all-distilroberta-v1,9 and all-MiniLM-L12-v210 are SBERT mod-\nels trained based on mpnet-base [66], distilbert-base-cased [62], and MiniLM-L12-H384-\nuncased [75] models respectively on more than 1 billion sentence pairs as general purpose\nmodels for sentence similarity;\n—multi-qa-mpnet-base-dot-v111 is a mpnet-base model [66] trained on 215M question-\nanswer pairs from various sources and domains, including StackExchange, Yahoo Answers,\nGoogle, and Bing search queries and many more as a model for question answering and IR\ntasks.\nBesidestheabove-mentionedSBERTmodels,weutilizethefollowingdomain-specificandgeneral\nwell-knownvariantsofBERTandRobertamodelsinthearchitectureofSBERTtoinvestigatetheir\neffectiveness, considering that they have the efficiency of the SBERT’s architecture:\n—BERT base uncased, BERT large uncased, Roberta base uncased, and Roberta large uncased\nare pre-trained on a large corpus of English raw data [17,35];\n—LegalBERTbaseuncased[ 11]isalight-weightmodelofBERT-BASE(33%thesizeofBERT-\nBASE) pre-trained from scratch on 12 GB of diverse English legal text of several types (e.g.,\nlegislation, court cases, contracts);\n5We do not report result with manually created boolean queries for fair comparison.\n6https://www.sbert.net/docs/pretrained_models.html\n7Please note that these models are top-ranked at the time of submission and the SBERT list can be updated later.\n8https://huggingface.co/sentence-transformers/all-mpnet-base-v2\n9https://huggingface.co/sentence-transformers/all-distilroberta-v1\n10https://huggingface.co/sentence-transformers/all-MiniLM-L12-v2\n11https://huggingface.co/sentence-transformers/multi-qa-mpnet-base-dot-v1\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:14 A. Askari et al.\n—P a t e n tB E R T[67] is trained by Google on 100M+ patents (not just U.S. patents) including\nabstract, claims, description based on BERT large uncased architecture.\nWe only report the results of the trained SBERT models as they obtained higher than the gen-\neral well-known variants of BERT models in the architecture of SBERT in all datasets except for\nthe Wikipedia datasets (WWG and WWA). For these, we report the Roberta large model in the\narchitecture of SBERT, which achieved the best result.\n5.4 Implementation Details\nFor SBERT, we first experiment with all the models that are introduced in Section5.3 and then\nadapting the two best models for each dataset on the domain using the methods proposed by\nWang et al. [73] (TSDAE) and Ginzburg et al. [21] (SDR) for domain adaptation without labeled\ndata.12 The SDR pre-training method is a dual-term loss objective that is composed of a standard\nMLMlossadoptedfromReference[ 17]—whichallowsthemodeltospecializeinthedomainofthe\ngivencollection[ 22]—andacontrastiveloss[ 23].ForSDRpre-training,wetokenizesentences,ag-\ngregatethemintobatches,andrandomlymasktheminasimilarwaytotheRoBERTapre-training\nparadigm. For TSDAE pre-training, as suggested in Reference [73], we use 10,000 sentences for\neachdomain,whichisonly2%oftheCOLIEE’21,1%oftheWikipedia(WWGandWWA)datasets,\nand less than 0.1% of the CLEF-IP 2011 dataset. PyTorch [48], HuggingFace [77], and Sentence\nBERT [56] are used to implement all of our models.\nWe useBM25optimised [5] as our initial ranker for COLIEE,K for Caselaw [36], andCh.2 for\nCLEF-IP2011[ 51].Todeterminetheoptimalre-rankingdepth,weincreasethedepthoftheinitial\nrankresultonthevalidationsetfrom15to100instepsof5.Wefound50tobeoptimalforCOLIEE,\n100 for WWG and WWA, and 20 for the CLEF-IP 2011 dataset. As we use Caselaw as test set only,\nwe employ COLIEE’s re-ranking depth for Caselaw.\n5.5 Parameter Tuning\nFor tuning parameters, we experiment with values from 1 to 10 for parametern:n = [ 1 ,2 ,3 ,4 ,5 ,\n6,7,8,9,10].Forthe b andk1parametersof RPRSw/freq,weusedthesamerangeaswhentuning\nthem for BM25 (b = [0.0, 0.1, 0.2,..., 1],k1 = [0.0, 0.2, 0.4,..., 3.0]). We found(b = 1,k1 = 2.8,\nn = 4) for COLIEE’21,(b = 0.9, k1 = 3.0, n = 4) for WWG and WWA, and(b = 0.8, k1 = 2.4,\nn = 5) for CLEF-IP.\n5.6 Pre-processing\nWe investigated sentence segmentation using three libraries, since legal case retrieval and patent\nretrieval are challenging domain-specific tasks with long legal sentences [61, 70]. We empirically\nfound that SparkNLP [29] is segmenting the legal text into sentences with a higher quality than\nNLTK [8] and Stanza [52]. We split long sentences into sequences of 25 words for COLIEE and\n30 words for CLEF-IP (average length of sentences in both collections). We then detected French\nsentences in the COLIEE data using the method in Reference [16] and translated them to English\nusing the Google Translate API.\n6 RESULTS\nWe employ a pairedt-testbetweentheproposedmethod,i.e., RPRSw/freq,andthestate-of-the-art\nmodel for each dataset. We implement four mutual baselines (BM25, BM25+KLI, SDRinf, Birch)\nacross all the datasets. Additionally, we present state-of-the-art approaches for each individual\n12The resulting models on the patent, legal and Wikipedia domain will be publicly available.\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:15\ndataset.Bydoingso,weguaranteethecomprehensivecoverageofstate-of-the-artmethodologies\nfor each dataset. It is worth highlighting that, to the best of our knowledge, no previous work\nhave undertaken the challenge of addressing query-by-document tasks across all these datasets\ncollectively. Existing works have been limited to individual datasets, each concentrating on a\nsingle dataset.\nWe answer the following research questions, assessing the effectiveness of our proposed meth-\nods, RPRS and RPRS w/freq, from different perspectives:\n—RQ1: What is the effectiveness of RPRS compared to the state-of-the-art models for QBD\nretrieval?\n—RQ2: How effective is RPRS with shorter or longer text units instead of sentences?\n—RQ3: What is the effectiveness of RPRS with parameters that were tuned on a different\ndataset in the same domain?\n—RQ4: To what extent is RPRS effective and generalizable across different domains with dif-\nferent type of documents?\nIn the following, we first address the choice of SBERT model for the proposed method,RPRS\nw/freq.Next,weanalyzetheeffectivenessoftheproposedmethodonthefivedomain-specificQBD\ndatasets and discuss the results per domain. In summary, we found thatRPRS w/freqoutperforms\nthe state-of-the-art models significantly for all official metrics in all five datasets. Please note that\ntheinitialrankerremainsconsistentacrossvariousre-rankersforalldatasets,therebyestablishing\na fair comparison. Furthermore, each of the BERT-based re-rankers utilizes the identical BERT\nmodel employed by RPRS. This choice ensures that RPRS does not gain an unfair advantage from\nusing a potentially superior BERT model compared to the baseline re-rankers.\n6.1 Choice of SBERT Model forRPRS w/freq\nToaddress RQ1,wefirstrunasetofexperimentswithdifferentSBERTmodelstofindwithwhich\nSBERT model theRPRS w/freq achieve higher effectiveness. Table2 shows that our re-ranker\nachieves a higher effectiveness than the strong initial ranker (line a) [5] using any sentence em-\nbedding models (lines b–j). This shows the effectiveness of our re-ranker is more dependent on\nthe proposed method rather than sentence embedding models, while our re-ranker effectiveness\ncould be improved with a sentence embedding model that captures the legal context more accu-\nrate. Morover, the table shows that out of the four SBERT models (lines b–e),all-MininLM-L12-v2,\nachieves highest effectiveness. Interestingly, this is the SBERT model with the lowest number of\nparameters(33M,comparedto82M,110M,and110Mfortheotherthree).Thiscouldindicatethat\nbetweenSBERTmodelsthathaveshowntheireffectivenessinother(general)domains,thechance\nof generalizing to a new domain-specific dataset for a SBERT model that has lowest parameters\nmight be higher than others. This finding is in line with the fact that, in the context of traditional\nbias-variance tradeoffs, neural models with fewer parameters are more likely to generalize to the\notherdomains[ 86,87].Thehighereffectivenessfor multi-qa-mpnet-base-dot-v1(linec)compared\nto all-mpnet-base-v2 (line b) while they both have same number of parameters and same base ar-\nchitecturecouldbeduetothefactthat multi-qa-mpnet-base-dot-v1wastrainedon(question,pair)\ninstances,andasaresult,itissuitableforsemanticsearchtaskandalignswithourre-rankingtask\nwhile all-mpnet-base-v2 is trained on various tasks.\nThesecondhighesteffectivenessisachievedby LegalSBERT,whichisthedomain-specificLegal\nBERT model [11] that is loaded into the SBERT architecture [56] and has 36M parameters (more\nthan MiniLM and less than our three other SBERT models). The effectiveness of our proposed\nmethod with the usage of Legal SBERT model could be due the fact it is pre-trained on the legal\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:16 A. Askari et al.\nTable 2. Results of the Proposed Method,RPRS w/freq , on COLIEE’21\nUsing Different Transformer Models Embedding\nModel Name Precision Recall F1\nInitial Ranker\na BM25 optimized +K L I 0.1700 0.2536 0.2035\nTop SBERT models\nb all-mpnet-base-v2 0.1760 0.2478 0.2058\nc multi-qa-mpnet-base-dot-v1 0.1840 0.2625 0.2163\nd all-distilroberta-v1 0.1900 0.2632 0.2206\ne all-MiniLM-L12-v2 0.1920 0.2745 0.2259\nDomain-Specific\nf Legal SBERT 0.1880 0.2720 0.2223\nDomain Adaptation\ng all-MiniLM-L12-v2-SDR 0.1817 0.2594 0.2137\nh all-MiniLM-L12-v2-TSDAE 0.1840 0.2584 0.2149\ni Legal SBERT-SDR 0.1890 0.2762 0.2244\nj Legal SBERT-TSDAE 0.1960 0.2891 0.2336\nTop SBERT models are top-4 trained and extensively evaluated models by SBERT\nand are publicly available. Legal SBERT is the Legal BERT [11] that is loaded into\nthe SBERT [56] architecture. Domain adaption has been done using\nSelf-Supervised, SDR, and Unsupervised, TSDAE, methods on all-MiniLM-L12-v2\nand Legal SBERT models as the models show top-two highest effectiveness\nwithout Domain Adaption.\nThe bold face is used to represent highest effectiveness in terms of a metric across\nthe models.\ndocuments. However, with domain adaptation on COLIEE corpus using TSDAE method [73], i.e.,\nLegal SBERT-TSDAE,RPRS w/freqachieves the highest effectiveness (rowj). We found the same\npattern for SDR and RPRS without frequency (Section3.2), i.e., they achieve highest effectiveness\nwith Legal-BERT-TSDAE model embeddings, but we only report it forRPRS w/freq, which is our\ncomplete proposed method.\nFurthermore, an observation by comparing rows (g,i) and (h,j) of Table2, is that the domain\nadaption with TSDA results in higher effectiveness of our re-ranker compared to SDR domain\nadaption method that is a contrastive-learning domain adaption method. We suppose the higher\neffectiveness of our re-ranker with domain adaption by TSDAE compared to SDR could be due to\nthefactthatTSDAEremoveswordsfromtheinputrandomlyandastheresultforcesthenetworkto\nproduce robust embeddings. In contrast, the input sentences for a contrastive-learning approach\nsuch as SDR are not modified, resulting in less stable embeddings. This finding was confirmed\npreviously by Wang et al. [73].\nAdditionally,bycomparingrows e withдandh,wecanseethatdomainadaptiononall-MiniLM-\nL12-v2 results in drops in the effectiveness of our re-ranker. This observation is in line with\nthe finding in prior work [73] that shows that domain adaption with TSDAE after supervised\ntrainingresultsinadropintheeffectiveness.Itisnoteworthytomentionthatall-MiniLM-L12-v2\nistrainedon1billionsamplesinasupervisedmanner.Abetterstrategyinthiscase,wouldbefirst\ndoingdomainadaptionasapre-trainingstepwithTSDAEandthendoingsupervisedtrainingwith\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:17\nTable 3. Results for COLIEE’21\nModel Precision Recall F1\nProbabilistic lexical matching baselines\na BM25 0.0770 0.1959 0.1113\nb BM25 + KLI 0.0983 0.1980 0.1313\nc BM25optimized + KLI 0.1700 0.2536 0.2035\ndT L I R 0.1533 0.2556 0.1917\nCross-encoders\neB E R T 0.1340 0.2263 0.1683\nf Legal BERT 0.1440 0.2463 0.1817\ng MTFT-BERT [2] (Previous state of the art) 0.1744 0.2999 0.2205\nSentence-based baseline\nh SDRinf 0.1470 0.2063 0.1716\niB i r c h 0.1721 0.2577 0.2064\nProposed methods\nj RPRS without frequency 0.1890 0.2799 0.2256\nkR P R S w / f r e q 0.1960†* 0.2891* 0.2336†*\n†and * indicate a statistically significant improvement over state of the art (MTFT-BERT, line g) and\nBM25optimized (line c), respectively, according to a paired-test (p < 0.005) with Bonferroni correction for\nmultiple testing. The winning team in the COLIEE 2021 competition for legal case retrieval is TLIR [39].\nThe initial ranker for all re-rankers (BERT, MTFT [5], SDR, and RPRS ) is BM25optimized+KLI. Legal\nS B E R T - T S D A Ei su s e dt oe m b e ds e n t e n c e sf o rSDRinf ,R P R S ,a n dRPRS w/freq.\nThe bold face is used to represent highest effectiveness in terms of a metric across the models.\n1 billion samples as was suggested by the authors of TSDAE [73].13 However, re-training MiniLM\nwith 1 billion examples was not possible for us due to computational limitations.\nTherefore, based on our empirical findings, for the next experiments on each domain, we pick\nthe domain-specific Transformer model and then adapt it into the the target domain, e.g., legal,\nusing TSDAE. We argue that doing this domain adaption is realistic for a real-world application,\nbecause it is trained on the document collection that are provided for training and does not need\nor use any supervision from labelled data.\n6.2 Effectiveness on the COLIEE Dataset (RQ1)\nTaking into account the finding from previous section, we exploit Legal SBERT-TSDAE in\nTable 3 to embed sentences forRPRS, RPRS w/freq, and SDR. Row i of Table3 show that even\nwithout taking into account frequency,RPRS achieves higher effectiveness than the initial ranker\n(BM25optimized +KLI, line c), Birch (line i), and the state-of-the-art re-ranker (MTFT-BERT, line\ng) on precision and the official metric (F1). This is whileRPRS has only one parameter,n.R o wk\nof the table shows thatRPRSw/freq achieve highest effectiveness and significantly better results\nover the state of the art (MTFT-BERT [2], line g) on precision and the official metric (F1), which\n13Also reported on https://github.com/UKPLab/sentence-transformers/issues/1372#issuecomment-1028525300 (visited\nFebruary 27, 2023).\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:18 A. Askari et al.\nFig.2. EffectivenessofRPRSw/frequsingfixed-lengthunitsinsteadofsentencesonCOLIEE2021(segment\nsize in tokens).\nindicates the importance and role of adding the document length normalization parameterb,a n d\nfrequencysaturationparameter k1inincreasingtheeffectivenessoftheproposedmethod.Weem-\nphasizethattheofficialmetricforCOLIEEis F1andtheCOLIEEorganizersonlyreport F1intheir\nofficialreportsforcaselawretrievaltask[ 53].Weassumeachievinglowerrecallbyourre-ranker,\nRPRSw/freq (linek),comparedtoMTFT-BERT[ 2](lineg)couldbeduetotheinverserelationship\nbetweenrecallandprecision,whichisacommonobservationbasedonexperimentalevidence[ 9].\nCleverdon[14]providesanin-depthexplanationofwhyprecisionandrecallhaveoftenaninverse\nrelationshi.SDRinf (lineh)achieveslowerperformancethan RPRSw/freq andRPRS results(lines\nj and k).\n6.3 Effect of using other Units than Sentences (RQ2)\nWe investigate the effect of using embeddings of different textual units than sentences inRPRS\nw/freqtoanalyzeif“sentence”isthemostappropriateunitforourre-ranker.Todoso,wesplitthe\ndocumentsinsegmentsoflength l withl ∈{16,32,64,128,256,512}.Figure 2showsthatsentences\nare better units than sequences with pre-defined fixed-length, also compared to sequences with\nfixed-length 16 and 32 that are similar in length to the average and median sentence length (29.4\nand24.8tokens).Thisconfirmstherelevanceofsentencesasunitsforretrieval,oneofthepremises\nof RPRS.\n6.4 Effectiveness without Parameter Optimization on Target Dataset (RQ3)\nTo address this question, we evaluate the proposed method on theCaselaw dataset [36]w i t ht h e\nparameters that were tuned on the COLIEE dataset without doing domain adaption on the col-\nlection for the Transformer model. In other words, our goal is to analyze how much the three\noptimized parameters of RPRS on the COLIEE dataset are transferable to another dataset in the\nsame (i.e., legal) domain. Table4 shows that the effectiveness of our re-ranker is higher than all\nmodels including the state-of-the-art model (K, line c) for all evaluation metrics with (line i) or\nwithout(linej)takingintoaccountfrequency.Thissupportsthecross-datageneralizabilityofour\nre-ranker,i.e.,optimiziedparametersofourproposedmethodworkeffectivelyforanotherdataset\ninthesamedomain,aswere-usethetuned RPRSw/freq onCOLIEEfortheCaselawdatasetwith-\nout optimizing its parameters on the Caselaw dataset.SDRinf (line g) and Birch (line h) baselines\nachieve lower performance thanRPRSw/freq andRPRS results (lines i and j).\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:19\nTable 4. Results for CaseLaw\nModel P@1 R@1 MAP@5 NDCG@5 MRR\nProbabilistic lexical matching baselines\na BM25 0.500 0.025 0.227 0.358 0.618\nb BM25 + KLI 0.660 0.115 0.232 0.398 0.726\ncK [ 36] (Previous State-of-the-art) 0.730 0.119 0.307 0.473 0.803\nCross-encoders\ndB E R T 0.320 0.064 0.118 0.201 0.438\ne Legal BERT 0.330 0.065 0.128 0.217 0.449\nf MTFT-BERT 0.360 0.064 0.160 0.252 0.481\nSentence-based baseline\ng SDRinf 0.500 0.070 0.171 0.320 0.585\nhB i r c h 0.680 0.117 0.270 0.425 0.774\nProposed methods\ni RPRS without frequency 0.730 0.128 0.314 0.489 0.807\njR P R S w / f r e q 0.780† 0.138† 0.321† 0.496† 0.837†\n†indicate a statistically significant improvement over the state of the art (K [36], line c) according to a paired-test\n(p < 0.05) with Bonferroni correction for multiple testing. We pick the optimized values of the proposed method\nparameters from COLIEE dataset, and use them to analyze the proposed method cross-data generalizability on\nCaselaw dataset. Legal SBERT-TSDAE is used to embed sentences forSDRinf ,R P R S ,a n dRPRS w/freq.\nThe bold face is used to represent highest effectiveness in terms of a metric across the models.\n6.5 Generalizability of Porposed Method (RQ4)\nPatent domain (CLEF-IP 2011 dataset).Table5shows that our re-ranker outperformthe state-\nof-the-artmodelontheCLEF-IP2011forallevaluationmetricswithorwithouttakingintoaccount\nfrequency(linesgandh).RowgofTable 5showthatevenwithouttakingintoaccountfrequency\nRPRS achieves higher effectiveness than the initial ranker (Ch. 2, line d), the previous state of\nthe art on CLEF-IP’11. Patent SBERT-TSDAE is used to embed sentences forSDRinf , RPRS, and\nRPRSw/freq,whichisPatentBERT[ 67]modelthatisloadedintointotheSBERT[ 56]architecture\nand adapted into the domain using TSDAE method [73]. SDRinf (line e) and Birch (line f) show\ncompetitive results compared to BM25 (lines a and b); however, it obtained much lower results\nthanRPRSw/freq .\nWikipeda domain (Wine and Video games datasets).Table6 shows thatSDRinf (line c and\nd) and Birch (line e) obtain competitive results toBM25 + KLI (line b) on SDR’s own datasets.\nHowever, as we achieved higher recall with BM25+KLI, we use it as the initial ranker for our\nre-ranker. Results show the generalizability and effectiveness ofRPRSw/freq and RPRS on a\nvery different domain (Wikipedia) for two datasets as they could outperform the state-of-the-\nart model on the WWG and WWA datasets for all evaluation metrics with or without tak-\ning into account frequency (lines f and g). It is noteworthy to mention that we report all of\nthe official metrics of WWG and WWA datasets. We refer toHit Ratio@k in the original pa-\nper [21] as recall in this table as their definition for theHit Ratiois equal to recall in Information\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:20 A. Askari et al.\nTable 5. Results for Patent Retrieval\nModel P@5 P@10 R@5 R@10 MAP@5 MAP@10 MRR\nProbabilistic lexical matching baselines\na BM25 0.084 0.066 0.010 0.147 0.053 0.065 0.187\nb BM25 + KLI 0.095 0.071 0.011 0.167 0.074 0.087 0.221\nTop-two teams in English CLEF-IP’11\ncH y . 5 0.057 0.041 0.071 0.103 0.045 0.051 0.162\nd Ch.2 [ 51] (Previous state of the art) 0.122 0.089 0.150 0.216 0.102 0.118 0.296\nSentence-based baseline\ne SDRinf + Ch.2 0.105 0.077 0.120 0.175 0.078 0.090 0.251\nf Birch + Ch.2 0.125 0.087 0.148 0.208 0.104 0.114 0.288\nProposed methods\ng RPRS + Ch.2 0.131 0.092 0.160 0.221 0.113 0.129 0.319\nh RPRS w/freq + Ch.2 0.132† 0.093† 0.167† 0.229† 0.116† 0.132† 0.332†\n†indicates the statistically significant improvement over best team on the CLEF-IP11 data (Ch.2 [51]) according to a\npaired-test (p < 0.001) with Bonferroni correction for multiple testing respectively. Patent SBERT-TSDAE is used to\nembed sentences forSDRinf ,R P R S ,a n dRPRS w/freq.\nThe bold face is used to represent highest effectiveness in terms of a metric across the models.\nRetrieval. We computeMean Percentile Rank (MPR)using the original implementation by\nSDR’s authors [21].14\n7 FURTHER ANALYSIS AND DISCUSSION\nInthissection,wefurtheranalyzeourresults,startingwithanablationstudyonthecomponentsof\nproposed methods, followed by the effect of document length, document coverage, the parameter\nspace, and a more detailed comparison to the SDR baseline.\n7.1 Ablation Study on RPRS w/freq Components\nThe contribution of each module in RPRS w/freq is assessed by an ablation study in Table7.T h e\nroleofQueryProportion(QP),DocumentProportion(DP),andtheparameters( b andk1)hasbeen\nevaluated in rowsa (No QP),b (No DP), andc (no b and k1). For the sake of clarity, we re-write\nthemodifiedequationaccordingtoeachrow.Forrow a (NoQP),weremovetheEquation( 4)from\nRPRS w/freqformula:\nRPRSw /freq = DP.\nSimilarly for rowb (No DP):\nRPRSw /freq =QP.\nTo do the ablation study of rowc (no b and k1), we modify Equation (6), FqRn , and Equation (7),\nFdRn , as follows:\nFqRn (Sq,Sd,STKq ) =\n∑Sq\nqs |Sd ∩rn(qs,STKq )|\n∑Sq\nqs |Sd ∩rn(qs,STKq )|\nFdRn (Sd,Sq,STKq ) =\n∑Sd\nds\n∑Rn\nrn |{ds}∩rn|\n∑Sd\nds\n∑Rn\nrn |{ds}∩rn|\n.\n14https://github.com/microsoft/SDR/blob/main/models/reco/wiki_recos_eval/eval_metrics.py\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:21\nTable 6. Results for the Video Games (Left) and Wines (Right) Datasets from Ginzburg et al. [21]\nModel Embedding Video games (WWG) Wines (WWA)\nrecall@10 recall@100 MPR MRR recall@10 recall@100 MPR MRR\nProbabilistic lexical matching baselines\na BM25 — 0.2200 0.4887 0.8606 0.5681 0.1704 0.5041 0.8164 0.4562\nb BM25 + KLI — 0.2425 0.5499 0.8748 0.6150 0.1732 0.6332 0.8191 0.4759\nPrevious state of the art\nc SDRinf R SDR 0.2360 0.5400 0.9740 0.6400 0.1700 0.5900 0.8930 0.5090\nd SDRinf R TSDAE 0.2384 0.5425 0.9761 0.6433 0.1712 0.5980 0.8940 0.5100\nSentence-BERT baseline\ne Birch R TSDAE 0.2322 0.5271 0.8683 0.6115 0.1720 0.5599 0.8088 0.4680\nProposed Methods\nf RPRS R TSDAE 0.2580 †* 0.5499 0.9768 0.6501 †* 0.1892 †* 0.6332 0.8955 0.5210 †*\ng RPRS w/freq R TSDAE 0.2663†* 0.5499 0.9774 0.6676 †* 0.2015†* 0.6332 0.8980 0.5360 †*\nR refers to Roberta large in this table.†and * indicate a statistically significant improvement over previous state of the\nart (SDR, line b) and BM25+KLI (line d) respectively, according to a paired test (p < 0.005) with Bonferroni correction\nfor multiple testing.\nThe bold face is used to represent highest effectiveness in terms of a metric across the models.\nTable 7. Ablation Study Results on the COLIEE 2021 Legal Case Retrieval Task\nPRF 1\nFull method (RPRS w/freq) 0.1960 0.2891 0.2336\na No QP (Equation ( 4)) 0.1670 0.2421 0.1976\nb No DP (Equation ( 5)) 0.1410 0.2180 0.1712\nc No b and k1 params 0.1860 0.2691 0.2199\nRPRS without freq (Equation (1)) 0.1890 0.2799 0.2256\nd No min function in PRS 0.1798 0.2644 0.2140\nThe bold face is used to represent highest effectiveness in terms of a metric across the models.\nThe first and second highest drop in quality of ranking is caused by removingDP andQP,w h i c h\nshows the crucial role of bothQP and DP in our re-ranker. This aligns with the assumption for\ndesigningourre-ranker:Acandidatedocumentandqueryarelikelyrelevantifalargeproportion\noftheirtextualcontentissimilartoeachother.Wefurtheranalyzetheeffectoftheminfunctionin\nRPRS for the equations ofqRn (Equation (3)) anddRn (Equation (2)) in rowd. To do so, we modify\nEquations (2),qRn , and (3),qRn , as follows:\nqRn (Sq,Sd,STKq ) =\nSq∑\nqs\n|Sd ∩rn(qs,STKq )|\ndRn (Sd,Sq,STKq ) =\nSd∑\nds\nRn∑\nrn\n|{ds}∩rn|.\nIngeneral,Table 7showsthatthefullmethodwithallcomponentsincludingQP,DP,and b andk1\nparameters obtains highest effectiveness and supports the necessity of each component. Morover,\nit ranks the importance and impact of each component in the effectiveness of the method. The\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:22 A. Askari et al.\nFig. 3. The probability of relevance and probability of retrieval on COLIEE 2021. In comparison to the prob-\nability of relevance, longer documents have a disproportionately smaller probability of being retrieved by\nMTFT-BERT whileRPRS w/freq is not biased against retrieving long documents.\nimportance of themin function inRPRS supports the intuition that if the repetition of sentences\nof a candidate document inRn is not controlled, the quality of the RPRS drops. This supports the\nmin functionfor RPRS andmotivate proposingRPRS w/freqtotake intoaccounttherepetitionsin\nan intelligent way by addingk1a n db.\n7.2 Effect of Document Length in Comparison to MTFT-BERT\nWe plot Figure3 to analyze the effect of document length on the effectiveness of the proposed\nmethod, and compare it with the MTFT-BERT model on COLIEE21. This comparison is justified\ndue to the fact that while in MTFT-BERT the document length is bounded by the maximum input\nlengthofBERT,itisthecurrentstate-of-the-artmodelonCOLIEE’21.Therefore,weselectthemost\neffective available model on this dataset for comparison in our analysis. In Figure3, probability\nof relevanceP(relevant |lenдth(doc)) indicates the chance of having a relevant document with a\nspecific length among all documents, and probability of retrievalP(ret|lenдth(doc)) shows the\nchance of retrieving a relevant document with a specific length among all retrieved documents,\nwithin top-k ranks. To compute the probability of retrieval, we setk = 5, because the average\nnumber of relevant documents per query in the collection is five. We analyze theprobability of\nretrievalandprobabilityofrelevance fromrelativelyshorterdocuments—with1,000words—toward\nlonger documents with 10,000 words. An ideal ranker does not lose effectiveness, probability of\nretrieval in Figure3, by the increment in the length of documents. As shown in Figure3,t h e\nRPRSw/freq modelnotonlydoesnotlooseeffectivenesswithincreasingthedocumentlength,but\nalso retrieves the longer relevant documents easier and gets closer to the probability of relevance\nP(relevant |lenдth(doc)) for them. This could be due to the fact that longer documents provide\nmoreinformation,andthe RPRSw/freq modelcantakeusethatinformationeffectivelyforranking.\nThis is while MTFT-BERT does not improve by increasing document length and receive similar\neffectiveness even with the more information that exist in a longer document. This indicates that\none of the reasons for gaining a higher effectiveness of F1 score, which is the official metric of\nCOLIEE dataset, by the proposed method compared to the other models could be because of the\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:23\nFig. 4. Effectiveness of RPRS w/freq over varying truncation length for queries and documents on COLIEE\n2021 where truncation size is the maximum length input.\nfact thatRPRS w/freqcould capture the full information in the lengthy documents and retrieves\nthem more effective than other models such as MTFT-BERT.\n7.3 Effect of Covering the Full Length of Queries and Documents\nWe analyze if taking into account the full length of queries and documents is an advantage for\ntheproposedmethodinFigure 4.Wearguethatanidealsemanticre-rankershouldachievehigher\neffectivenessbyreceivingthefulllengthofqueriesanddocumentsratherthantruncatedtextasthe\ninput.Therefore,weanalyzethepowerofourre-rankerwhenthelengthofqueriesanddocuments\nincreases:weexperimentwithdifferentmaximumlengthsfortheinput.Given l ∈{256,512,1024,\n2048, 4096, 8092, 16384}as the maximum input length, we select the firstl tokens of the query\nand documents as their representation. Moreover, our analysis gives an in-depth insight about\nthe robustness of the proposed method and assess whether it collapse when presented with long\nqueries and documents or not. As Figure4 shows, our re-ranker takes advantage of seeing the\nwholecontentofqueryanddocumentsintermsofeffectivenessasithasthehighesteffectiveness\nwhenwefeeditwiththefulllengthandthelowesteffectivenesswithonly256tokensastheinput.\nThefigurealsoshowsthelargestleapisbetween1,024and4,096tokens,whichisapproachingthe\naverage document length in COLIEE (5226). This confirms thatRPRSw/freq takes advantage of\nthe full document length in estimating relevance.\n7.4 Parameter Sensitivity forRPRSw /freq\nIn addition to the tuned parameters that were found and explained in Section5.5, we analyze the\nsensitivityoftheproposedparametersinotherrangesinthefollowing.Figure 5showstheeffects\nof changes in parametersn,b,a n dk1 on the overall performance of our method on the COLIEE\n2021dataset.Ineachdiagram,thevalueofeachpointrepresentstheF1scoreaccordingtodifferent\nvalues of the parameters. The following observations can be made from Figure5:\n—A comparison between different values ofn—independent fromb and k1—indicates that\nthe proposed method has the lowest effectiveness by reducing the value ofn to 1. The ef-\nfectiveness of the model increases asn is set to 5, and as we setn = 10, we see that model\neffectivenessdeclineswhilestillbeingsuperiorto nsetto1.Wearguethatthisdemonstrates\ntheexistenceofatradeoffinthevalueof n,whichdeterminesthenumberofthemostsimilar\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:24 A. Askari et al.\nFig. 5. Sensitivity analysis of our proposed method,RPRSw/freq , to changes in parameters n, b, and k1 on\nthe COLIEE 2021 dataset.\nsentences per query sentence in setrn.I fn is set to 1, then the selection ofmost similarsen-\ntences per query sentence would be very strict, and the effectiveness would decrease. This\ncould be because there may be other sentences from documents that are similar enough to\nqualifyasa mostsimilar sentencetoaquerysentence—inadditiontothetop-1mostsimilar\nsentence—but withn = 1, they will not be taken into account in the computation of rele-\nvance score. However, ifn is set to 10, then some sentences that are not similar enough to\nqualifyasa mostsimilar sentencetoaquerysentencemayappearin rn,andastheresultthe\nmodelobtainlesseffectivenesscomparedto n = 5,becauseitwouldbelessstrictincreation\nofrns for query sentences.\n—Ifthe k1 parameter is set to 0 for any value of theb parameter, then thef1 score depends\nonly on the value of then parameter. This is due to the fact thatk1 multiplies to theb\nparameter,andif k1issettozero,thentherelevancescoreisindependentofchangesinthe\nb parameter’s value. Taking this into consideration, we could reduce the search space for\nparameter optimization by eliminating the necessity of grid search forb values whenk1i s\nset to zero.\n—By increasing the value of then parameter the proposed method becomes more sensitive\nto the document length normalization parameterb, and frequency saturation parameterk1.\nThere are larger disparities in thef1 scores when thek1o rb parameter are changed for\nn = 10 than forn = 5, and forn = 5t h a nf o rn = 1. This indicates where the method is less\nstrict in qualifying a sentence as amost similarsentence to a query sentence, the impact of\nterm saturation parameter (k1) and document length normalization (b)a r eh i g h e r .\n—The proposed method performs better in terms of effectiveness without doing length nor-\nmalization (b = 0) whenn is set to 1, and, as the result, the selection of the most similar\nsentences per query sentence is very strict, and it performs better with length normaliza-\ntion(b = 0.5and b = 1.0)when nislessstrict( n = 5and n = 10).Thiscouldprobablyshows\nthatthenegativeeffectsoneffectivenessbylessstrictlyselectingthemostsimilarsentences\nperquerysentence—byincreasingthevalueof nparameter—couldmightbecontrolledwith\nb parameter.\nIn general, the parameter sensitivity analysis reveals that while optimizing RPRS parameters in-\ncreases effectiveness, it can still result in increased effectiveness even without optimization by\ninitializing each of the three parameters with the value from middle of their range, e.g.,n = 5,\nk1 = 1.5, andb = 0.5 that obtains 22.75 F1 score, which is still better than current state of the art\non the dataset (MTFT-BERT [2]) while not being the ideal result by ourRPRS w/freqre-ranker.\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:25\nTable 8. Pearson Product-Moment Correlation Coefficients between\nDocument Length and Relevance Score\nModel Pearson Product-Moment Correlation coefficient\nCOLIEE 2021 Caselaw CLEF-IP Video Games Wines datasets\nSDR 0.5768 0 .5901 0 .612 0 .542 0 .5366\nRPRS w/freq −0.0565 −0.0401 0 .0105 −0.0322 0 .0109\nThe bold face is used to represent highest effectiveness in terms of a metric across the models.\n7.5 Further Analysis of RPRS Compared to SDR\nWe compared RPRS to SDR, because it has some similarities: It also uses sentence-level relevance\nscoring based on sentence embeddings and it uses a bi-encoder architecture, like RPRS. There are\nfundamentallysignificancedifferences;however,SDRcreatesasentence-levelsimilaritymatrix M\nfor each pair of query and document paragraphs and based onM matrices, creates a paragraph\nsimilaritymatrix P foracandidatedocument.Eachcellof P containsthesimilaritybetweenapair\nofparagraphsfromqueryandacandidatedocument.Thematrix P isthennormalizedto( NRM (P))\nandthetotalscore Siscomputedbasedon NRM (P).Thedenominatorintheformulaforcomputing\nthe matrixP is the number of sentences in a query paragraph and the denominator for the total\nscoreS is the number of paragraphs in the query. An important consequence is that the length of\nthecandidatedocumentsisnottakenintoaccount,whileRPRSexplicitlydoesso.Moreover,ifone\nparagraph of a candidate document is the most similar paragraph to all paragraphs of the query,\nthenSDRdoesnotpenalizethisrepetition,whilewetakethatintoaccountwiththe k1parameter\ninRPRS w/freq,whichcontrols frequencysaturation.In addition, it is noteworthyto mention that\nSDR uses the cosine similarity directly in their approach while we only consider that score for\nranking sentences inRn.\nLosadaetal.[ 37]indicatethatanidealretrievalmodelshouldnotbetunedtofavourlongerdoc-\numentsandtherelevancescoresitproducesshouldnotbecorrelatedwithdocumentlength.They\nargue that the previous empirical evidence supporting thescope hypothesis15 is over-exaggerated\nand inaccurate due to the incompleteness of modern collections. This is even more important in\nQBD retrieval as the length of relevant documents could vary from short to very long. Therefore,\nthe retrieval model should not be biased to the document length. Consequently, to study the sen-\nsitivity of our proposed method to document length, and compare it with SDR, we analyze the\ncorrelationbetweencandidatedocumentlengthandtherelevancescoreproducedby RPRSw/freq\nand SDR in Table8. We find that the Pearson Product-Moment Correlation coefficients forRPRS\nw/freqonallfivedatasetsareclosetozerowhilefor SDRthesecorrelationsaremuchhigher,above\n0.5,foralldatasets.ThisindicatesthestrongcorrelationofSDRrelevancescorewiththedocument\nlengthandhowevertherobustnessof RPRSw/freq withrespecttodocumentlength.Inotherwords,\nRPRS w/freqis effective while it is not biased to the length of the document.\nIn addition to the correlation analysis, we further study the statistics over word counts of doc-\numents in all datasets to find out if there is a specific statistical characteristic in the Video games\nand Wines datasets that makes SDR stronger on those datasets compared to the three datasets in\nthe legal and patent domain. Figure6 shows the document length distribution over the datasets\nthat indicates higher variance among Patent and Legal datasets compared to Wikipedia datasets\n(WVG and WWA). The boxes bound the 25th to 75th percentiles, top whisker cover data within\n15The scope hypothesis in Information Retrieval states that a relationship exists between document length and rele-\nvance [58].\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:26 A. Askari et al.\nFig.6. Distributionofwordcountspereachdocumentforallofthefivedatasets:Patentretrieval(CLEF-IP),\nCase law retrieval (COLIEE 2021 and CASELAW), and Wikipedia (WWG and WWA).\nTable 9. Results for COLIEE’2020\nModel Precision Recall F1\nProbabilistic lexical matching baselines\na BM25 + KLI 0.6700 0.6117 0.6395\nBaselines\nb JNLP team [44] (Previous state of the art) 0.8025 0.7227 0.7605\nc Paraformer [ 43] 0.7346 0.7407 0.7376\nProposed methods\nd RPRS without frequency 0.7840 0.7550 0.7692\neR P R S w / f r e q 0.7980 0.7690†* 0.7832†*\n†and * indicate a statistically significant improvement over the previous state of the art (JNLP team [44],\nrow b) and BM25 + KLI (line a), respectively, according to a paired test (p < 0.005) with Bonferroni\ncorrection for multiple testing. The winning team in the COLIEE 2020 competition for legal case retrieval\nis JNLP [44]. The initial ranker is BM25optimized+KLI. Legal SBERT-TSDAE is used to embed sentences for\nRPRS andRPRS w/freq.\nThe bold face is used to represent highest effectiveness in terms of a metric across the models.\n1.5×the inter-quartile range, and outliers are removed. Moreover, the standard deviation of docu-\nmentslength for theVideoGames andWinesdatasetsare 675and721 ,respectively.However,for\nCOLIEE2021,Caselaw,andCLEF-IP,thestandarddeviationofdocumentslengthare6 ,933,4,937,\nand11,402.Thus,thedocumentsin SDR’sWikipediadatasetshaveless variancecomparedto the\nthree legal and patent datasets and are shorten than COLIEE and Patent datasets. As a result, we\nconclude that the lower effectiveness of SDR on the legal and patent datasets is likely caused by\nSDR not being robust against the length of documents and suffers from the high standard devia-\ntionandlongerdocumentsinthelegalandpatentdatasetsincomparisontoitsowndatasets.Our\nexperimental results in Section6.5 indicate that our methodRPRS w/freqalso outperforms SDR\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:27\nTable 10. Results for COLIEE’2022\nModel Precision Recall F1\nProbabilistic lexical matching baselines\na BM25 + KLI 0.3000 0.2850 0.2923\nBaselines\nb UA team [54] (Previous state of the art) 0.4111 0.3389 0.3715\nc Siat team [76] 0.3005 0.4782 0.3691\nProposed methods\nd RPRS without frequency 0.4244 0.3607 0.3900\neR P R S w / f r e q 0.4361†* 0.3904 0.4120†*\n†and * indicate a statistically significant improvement over the previous state of the art (UA\nteam [54], row b) and BM25 + KLI (line a), respectively, according to a paired test (p < 0.005) with\nBonferroni correction for multiple testing. The winning team in the COLIEE 2020 competition for\nlegal case retrieval is UA [54]. The initial ranker is BM25optimized+KLI. Legal SBERT-TSDAE is used to\nembed sentences for RPRS andRPRS w/freq.\nThe bold face is used to represent highest effectiveness in terms of a metric across the models.\non their datasets, so the robustness ofRPRS w/freqis not only beneficial for extremely lengthy\ndocuments but also in other domains.\n7.6 Effectiveness on Different Versions of the COLIEE Dataset\nTo analyze the effectiveness of the proposed methods in more depth, we investigate the effective-\nness of the proposedmethods comparedto the previous state-of-the-artmethods on different ver-\nsions of the COLIEE datasets. We analyze this on COLIEE 2020 and COLIEE 2022. Tables9and10\nshowthatourre-rankeroutperformsthestate-of-the-artmodelonbothversionsofCOLIEEforall\nevaluationmetricswithorwithouttakingintoaccountfrequency(linesdande).RowdofTables 5\nshow that even without taking into account frequency,RPRS achieves higher effectiveness than\nthe previous state-of-the-art method (b). Paraformer Nguyen et al. [43] reports F2 in their paper\nwhileF1istheofficialmetricforCOLIEEdataset.Therefore,wereportF1insteadofF2inTable 9.16\n8 CONCLUSION AND FUTURE WORK\nIn this article, we proposed methods for effectively exploiting sentence-level representations pro-\nduced by the highly efficient bi-encoder architecture for QBD re-ranking. We proposed a novel\nmodel using SBERT representations, with a frequency-based extension inspired by BM25’s “term\nsaturation”mechanismandtheincorporationofdocumentlengthnormalizationintotherelevance\nscore computation. Our experiments on five datasets show that our modelRPRS w/freqtakes ad-\nvantage of the long queries and documents that are common in QBD retrieval. While ourRPRS\nw/freq model is unsupervised with only three tunable parameters, it is more effective than state-\nof-the-artsupervisedneuralandlexicalmodels.Inaddition,itishighlyefficientforretrievaltasks\nwithlongdocumentsandlongqueries,becausetheoperationtocomputerelevancescoreby RPRS\nw/freq are based on (1) a bi-encoder, SBERT, which is about 46,800 times faster than the common\ncross-encoder BERT architecture; (2) the pre-processing, embedding, and indexing of document\n16I ti si m p o r t a n tt on o t et h a tN g u y e ne ta l .[43] focus on COLIEE 2020 and report all the baselines on COLIEE 2020 while\nmistakenly mention COLIEE 2021 as the used dataset in their experiments.\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:28 A. Askari et al.\nsentences could be done before the query time; (3) the only calculation based on the embeddings\nare cosine similarity and sorting that are simple and efficient operations.\nWeshowtheeffectivenessof RPRSw/freq onfivedatasetswithlow-resourcetrainingdata,which\nindicatesitssuitabilityforQBDretrievaltasksinwhichthetrainingdataareverylimitedcompared\ntogeneralwebsearchduetothehighcostofdatasetcreationforthesetasks.Therefore,weattain\nhigh efficiency and effectiveness while being optimized on low-recourse training data withRPRS\nw/freq.\nWhile we outperform the state-of-the-art models on each dataset with our proposed method,\nthe effectiveness results show the difficult nature of the tasks. One reason for that is the low ef-\nfectiveness of the first stage retrieval model limits the re-ranker performance. As one direction of\nimprovementforfuturework,weaimtofocusonfirst-stageretrievalbydesigningamodification\ntoourproposedmethod,whichissuitableforfirst-stageretrievaltasks.Theparametersensitivity\nanalysis reveals that while optimizing RPRS parameters increases effectiveness, it can still result\nin increased effectiveness even without optimization by initializing each of the three parameters\nwith a value from the middle of their range.\nFor further future improvement on our model, we might be inspired by two other variants of\nBM25 to make the proposed method parameter-free with dynamically computed parameters. For\nthe frequency saturation parameter,k1, a variant of BM25 called “BM25 adaptive” [38], which dy-\nnamicallycomputesthe k1parametercouldbetakenintoaccountasinspiration.Additionally,for\nthedocumentlengthnormalizationparameter, b,Lipanietal.[ 34]proposealengthnormalization\nmethod that removes the need for ab parameter in BM25, which could inspire us to dynamically\ncompute theb parameter ofRPRS w/freqin future. Considering the intuition of both approaches,\nworking on dynamically initializing then parameter could make our method completely parame-\nter free, which is an interesting direction for future work. We argue because a sensitivity analysis\nreveals a pattern that indicates robust and greater effectiveness relative to the baseline for the\nproposed method with different parameter values, finding a nearly optimal parameter value dy-\nnamically might likely be possible.\nWe believe the proposed method is suitable for other tasks that deal with long documents. In\naddition,wesuggestanalyzingtheeffectivenessofourproposedmethodonotherIRtasksinwhich\nwe do not have extremely long documents, but the query and candidate document do consist of\nmultiple sentences [1, 15]. We hope that our work will spark more research interest in retrieval\nfor extremely long queries and open up possibilities for highly efficient long-document retrieval.\nREFERENCES\n[1] Amin Abolghasemi, Arian Askari, and Suzan Verberne. 2022. On the interpolation of contextualized term-based\nranking with bm25 for query-by-example retrieval. InProceedings of the ACM SIGIR International Conference on\nTheory of Information Retrieval (ICTIR’22). Association for Computing Machinery, New York, NY, 161–170.https:\n//doi.org/10.1145/3539813.3545133\n[2] Amin Abolghasemi, Suzan Verberne, and Leif Azzopardi. 2022. Improving BERT-based query-by-document retrieval\nwith multi-task optimization. InAdvances in Information Retrieval: Proceedings of the 44th European Conference on IR\nResearch (ECIR’22).\n[3] Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Cross-domain modeling of sentence-\nlevel evidence for document retrieval. InProceedings of the Conference on Empirical Methods in Natural Language\nProcessingandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP’19) .Association\nfor Computational Linguistics, 3490–3496.https://doi.org/10.18653/v1/D19-1352\n[4] Sophia Althammer, Sebastian Hofstätter, and Allan Hanbury. 2021. Cross-domain retrieval in the legal and patent\ndomains: A reproducibility study. InEuropean Conference on Information Retrieval. Springer, 3–17.\n[5] A. Askari and S. Verberne. 2021. Combining lexical and neural retrieval with longformer-based summarization for\neffective case law retrieva. InProceedings of the 2nd International Conference on Design of Experimental Search &\nInformation REtrieval Systems. CEUR, 162–170.\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:29\n[6] OrenBarkan,NoamRazin,ItzikMalkiel,OriKatz,AviCaciularu,andNoamKoenigstein.2020.Scalableattentivesen-\ntence pair modeling via distilled sentence embedding. InProceedings of the AAAI Conference on Artificial Intelligence,\nVol. 34. 3235–3242.\n[7] Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The Long-document transformer. CoRR\nabs/2004.05150, (2020). Retrieved fromhttps://arxiv.org/abs/2004.05150\n[8] Steven Bird, Ewan Klein, and Edward Loper. 2009.Natural Language Processing with Python: Analyzing Text with the\nNatural Language Toolkit. O’Reilly Media, Inc.\n[9] MichaelBucklandandFredricGey.1994.Therelationshipbetweenrecallandprecision. J.A m.Soc.Inf.Sci. 45,1(1994),\n12–19.\n[10] Fredrik Carlsson, Amaru Cuba Gyllensten, Evangelia Gogoulou, Erik Ylipää Hellqvist, and Magnus Sahlgren. 2020.\nSemantic re-tuning with contrastive tension. InInternational Conference on Learning Representations.\n[11] IliasChalkidis,ManosFergadiotis,ProdromosMalakasiotis,NikolaosAletras,andIonAndroutsopoulos.2020.LEGAL-\nBERT: The muppets straight out of law school. arXiv:2010.02559. Retrieved fromhttps://arxiv.org/abs/2010.02559\n[12] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. 2020. A simple framework for contrastive\nlearning of visual representations. InInternational Conference on Machine Learning. PMLR, 1597–1607.\n[13] Xiaoyang Chen, Kai Hui, Ben He, Xianpei Han, Le Sun, and Zheng Ye. 2022. Incorporating ranking context for end-\nto-end BERT re-ranking. InEuropean Conference on Information Retrieval. Springer, 111–127.\n[14] Cyril W. Cleverdon. 1972. On the inverse relationship of recall and precision.J. Document.28, 3 (1972), 195–201.\n[15] Arman Cohan, Sergey Feldman, Iz Beltagy, Doug Downey, and Daniel S. Weld. 2020. Specter: Document-level repre-\nsentationlearningusingcitation-informedtransformers.arXiv:2004.07180.Retrievedfrom https://arxiv.org/abs/2004.\n07180\n[16] M.Danilak.2014.langdetect:LanguagedetectionlibraryportedfromGoogle’slanguagedetection.RetrievedJanuary\n19, 2014 fromhttps://pypi.python.org/pypi/langdetect/\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv:1810.04805. Retrieved fromhttps://arxiv.org/abs/1810.04805\n[18] Eva D’hondt, Suzan Verberne, Wouter Alink, and Roberto Cornacchia. 2011. Combining document representations\nfor prior-art retrieval. InCLEF (Notebook Papers/Labs/Workshop).\n[19] AtsushiFujii,MakotoIwayama,andNorikoKando.2007.OverviewofthepatentretrievaltaskattheNTCIR-6work-\nshop. InProceedings of the NTCIR Workshop.\n[20] Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. Simcse: Simple contrastive learning of sentence embeddings.\narXiv:2104.08821. Retrieved fromhttp://arxiv.org/abs/2104.08821\n[21] Dvir Ginzburg, Itzik Malkiel, Oren Barkan, Avi Caciularu, and Noam Koenigstein. 2021. Self-supervised document\nsimilarity ranking via contextualized language models and hierarchical inference. InFindings of the Association for\nComputational Linguistics: ACL-IJCNLP 2021. Association for Computational Linguistics, 3088–3098.https://doi.org/\n10.18653/v1/2021.findings-acl.272\n[22] Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith.\n2020. Don’t stop pretraining: Adapt language models to domains and tasks. arXiv:2004.10964. Retrieved fromhttps:\n//arxiv.org/abs/2004.10964\n[23] Raia Hadsell, Sumit Chopra, and Yann LeCun. 2006. Dimensionality reduction by learning an invariant mapping. In\nProceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’06),V o l .2 .\nIEEE, 1735–1742.\n[24] Sebastian Hofstätter, Bhaskar Mitra, Hamed Zamani, Nick Craswell, and Allan Hanbury. 2021. Intra-document cas-\ncading: Learning to select passages for neural document ranking. InProceedings of the 44th International ACM SIGIR\nConference on Research and Development in Information Retrieval. 1349–1358.\n[25] Sebastian Hofstätter, Navid Rekabsaz, Mihai Lupu, Carsten Eickhoff, and Allan Hanbury. 2019. Enriching word em-\nbeddings for patent retrieval with global context. InEuropean Conference on Information Retrieval. Springer, 810–818.\n[26] Samuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2019. Poly-encoders: Transformer archi-\ntectures and pre-training strategies for fast and accurate multi-sentence scoring. arXiv:1905.01969. Retrieved from\nhttps://arxiv.org/abs/19505.01969\n[27] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau\nYih.2020.Densepassageretrievalforopen-domainquestionanswering.In ProceedingsoftheConferenceonEmpirical\nMethods in Natural Language Processing (EMNLP’20). Association for Computational Linguistics, 6769–6781.https:\n//doi.org/10.18653/v1/2020.emnlp-main.550\n[28] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. arXiv:2001.04451. Re-\ntrieved fromhttps://arxiv.org/abs/2001.04451\n[29] Veysel Kocaman and David Talby. 2021. Spark NLP: Natural language understanding at scale.Softw. Impacts(2021),\n100058. https://doi.org/10.1016/j.simpa.2021.100058\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:30 A. Askari et al.\n[30] Steven A. Lastres. 2015. Rebooting legal research in a digital age. Insights Paper (white paper funded by LexisNexis)\n(July 2013),https://www.lxisnexis.com\n[31] Nhat X. T. Le, Moloud Shahbazi, Abdulaziz Almaslukh, and Vagelis Hristidis. 2021. Query by documents on top of a\nsearch interface.Inf. Syst.101 (2021), 101793.\n[32] BohanLi,HaoZhou, Junxian He,MingxuanWang,YimingYang,andLeiLi.2020.Onthesentenceembeddingsfrom\npre-trained language models. arXiv:2011.05864. Retrieved fromhttps://arxiv.org/abs/2011.05864\n[33] Minghan Li, Diana Nicoleta Popa, Johan Chagnon, Yagmur Gizem Cinar, and Eric Gaussier. 2023. The power of se-\nlecting key blocks with local pre-ranking for long document information retrieval.ACM Trans. Inf. Syst.41, 3 (2023),\n1–35.\n[34] Aldo Lipani, Mihai Lupu, Allan Hanbury, and Akiko Aizawa. 2015. Verboseness fission for bm25 document length\nnormalization. InProceedings of the International Conference on the Theory of Information Retrieval. 385–388.\n[35] YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,MandarJoshi,DanqiChen,OmerLevy,MikeLewis,LukeZettlemoyer,\nand Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv:1907.11692. Retrieved\nfromhttps://arxiv.org/abs/1907.11692\n[36] Daniel Locke, Guido Zuccon, and Harrisen Scells. 2017. Automatic query generation from legal texts for case law\nretrieval. InAsia Information Retrieval Symposium. Springer, 181–193.\n[37] David E. Losada, Leif Azzopardi, and Mark Baillie. 2008. Revisiting the relationship between document length and\nrelevance. InProceedings of the 17th ACM Conference on Information and Knowledge Management. 419–428.\n[38] YuanhuaLvandChengXiangZhai.2011.AdaptivetermfrequencynormalizationforBM25.In Proceedingsofthe20th\nACM International Conference on Information and Knowledge Management. 1985–1988.\n[39] YixiaoMa,YunqiuShao,BulouLiu,YiqunLiu,MinZhang,andShaopingMa.2021.Retrievinglegalcasesfromalarge-\nscale candidate corpus. InProceedings of the 8th International Competition on Legal Information Extraction/Entailment\n(COLIEE’21).\n[40] Parvaz MahdabiandFabio Crestani.2014.Query-driven miningof citationnetworks for patentcitationretrievaland\nrecommendation.In Proceedingsofthe23rdACMInternationalConferenceonConferenceonInformationandKnowledge\nManagement. 1659–1668.\n[41] Sheshera Mysore, Arman Cohan, and Tom Hope. 2021. Multi-vector models with textual guidance for fine-grained\nscientific document similarity. arXiv:2111.08366. Retrieved fromhttps://arxiv.org/abs/2111.08366\n[42] Sheshera Mysore, Tim O’Gorman, Andrew McCallum, and Hamed Zamani. 2021. CSFCube–A test collection of com-\nputer science research articles for faceted query by example. arXiv:2103.12906. Retrieved fromhttps://arxiv.org/abs/\n2103.12906\n[43] Ha-ThanhNguyen,Manh-KienPhi,Xuan-BachNgo,VuTran,Le-MinhNguyen,andMinh-PhuongTu.2022.Attentive\ndeepneuralnetworksforlegaldocumentretrieval. ArtificialIntelligenceandLaw(December2022) .DOI:https://doi.org/\n10.1007/s10506-022-09341-8\n[44] Ha-Thanh Nguyen, Hai-Yen Thi Vuong, Phuong Minh Nguyen, Binh Tran Dang, Quan Minh Bui, Sinh Trong Vu,\nChau Minh Nguyen, Vu Tran, Ken Satoh, and Minh Le Nguyen. 2020. JNLP team: Deep learning for legal processing\nin COLIEE 2020. arXiv:2011.08071. Retrieved fromhttps://arxiv.org/abs/2011.08071\n[45] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. MS\nMARCO: A human generated machine reading comprehension dataset.Choice 2640 (2016), 660.\n[46] RodrigoNogueiraandKyunghyunCho.2019.Passagere-rankingwithBERT.arXiv:1901.04085.Retrievedfrom https:\n//arxiv.org/abs/1901.04085\n[47] Harshith Padigela, Hamed Zamani, and W. Bruce Croft. 2019. Investigating the successes and failures of BERT for\npassage re-ranking. arXiv:1905.01758. Retrieved fromhttps://arxiv.org/abs/1905.01758\n[48] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin,NataliaGimelshein,LucaAntiga,etal.2019.Pytorch:Animperativestyle,high-performancedeeplearninglibrary.\nAdv. Neural Inf. Process. Syst.32 (2019), 8026–8037.\n[49] FlorinaPiroiandAllanHanbury.2019.Multilingualpatenttextretrievalevaluation:CLEF–IP.In InformationRetrieval\nEvaluation in a Changing World. Springer, 365–387.\n[50] Florina Piroi, Mihai Lupu, and Allan Hanbury. 2013. Overview of clef-ip 2013 lab. InInternational Conference of the\nCross-Language Evaluation Forum for European Languages. Springer, 232–249.\n[51] FlorinaPiroi,MihaiLupu,AllanHanbury,andVeronikaZenz.2011.CLEF-IP2011:Retrievalintheintellectualproperty\ndomain. InCLEF (Notebook Papers/Labs/Workshop). Citeseer.\n[52] Peng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A python natural\nlanguage processing toolkit for many human languages. InProceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics: System Demonstrations.\n[53] Juliano Rabelo, Randy Goebel, Mi-Young Kim, Yoshinobu Kano, Masaharu Yoshioka, and Ken Satoh. 2022. Overview\nand discussion of the competition on legal information extraction/entailment (COLIEE) 2021.Rev. Socionetw. Strateg.\n16, 1 (2022), 111–133.\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\nRetrieval for Extremely Long Queries and Documents with RPRS 115:31\n[54] Juliano Rabelo, Mi-Young Kim, and Randy Goebel. 2022. Semantic-based classification of relevant case law. InJSAI\nInternational Symposium on Artificial Intelligence. Springer, 84–95.\n[55] Juliano Rabelo, Mi-Young Kim, Randy Goebel, Masaharu Yoshioka, Yoshinobu Kano, and Ken Satoh. 2020. COL-\nIEE 2020: Methods for Legal Document Retrieval and Entailment. Retrieved fromhttps://sites.ualberta.ca/~rabelo/\nCOLIEE2021/COLIEE_2020_summary.pdf\n[56] Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks.\narXiv:1908.10084. Retrieved fromhttps://arxiv.org/abs/1908.10084\n[57] Stephen Robertson, Hugo Zaragoza, et al. 2009. The probabilistic relevance framework: BM25 and beyond.Found.\nTrends Inf. Retriev.3, 4 (2009), 333–389.\n[58] StephenE.RobertsonandSteveWalker.1994.Somesimpleeffectiveapproximationstothe2-poissonmodelforprob-\nabilistic weighted retrieval. InSIGIR’94. Springer, 232–241.\n[59] GuilhermeMoraes Rosa,RuanChaves Rodrigues, Roberto Lotufo,andRodrigo Nogueira.2021.Yes, BM25is a strong\nbaseline for legal case retrieval. arXiv:2105.05686. Retrieved fromhttps://arxiv.org/abs/2105.05686\n[60] Seitz Rudi. 2020. Understanding TF-IDF and BM-25. Retrieved from https://kmwllc.com/index.php/2020/03/20/\nunderstanding-tf-idf-and-bm-25/\n[61] George Sanchez. 2019. Sentence boundary detection in legal text. InProceedings of the Natural Legal Language Pro-\ncessing Workshop. Association for Computational Linguistics, 31–38.https://doi.org/10.18653/v1/W19-2204\n[62] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. DistilBERT, a distilled version of BERT:\nSmaller, faster, cheaper and lighter. arXiv:1910.01108. Retrieved fromhttps://arxiv.org/abs/1910.01108\n[63] Ivan Sekulić, Amir Soleimani, Mohammad Aliannejadi, and Fabio Crestani. 2020. Longformer for MS MARCO docu-\nment re-ranking task. arXiv:2009.09392. Retrieved fromhttps://arxiv.org/abs/2009.09392\n[64] WalidShalabyandWlodekZadrozny.2019.Patentretrieval:Aliteraturereview. Knowl.Inf.Syst. 61,2(2019),631–660.\n[65] Yunqiu Shao, Jiaxin Mao, Yiqun Liu, Weizhi Ma, Ken Satoh, Min Zhang, and Shaoping Ma. 2020. BERT-PLI: Model-\ning paragraph-level interactions for legal case retrieval. InProceedings of the 29th International Joint Conference on\nArtificial Intelligence (IJCAI’20). 3501–3507.\n[66] Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. Mpnet: Masked and permuted pre-training for\nlanguage understanding.Adv. Neural Inf. Process. Syst.33 (2020), 16857–16867.\n[67] Rob Srebrovic and Jay Yonamine. 2020.Leveraging the BERT Algorithm for Patents with TensorFlow and Big Query.\nTechnical Report. Google.\n[68] VuTran,MinhLeNguyen,SatoshiTojo,andKenSatoh.2020.Encodedsummarization:Summarizingdocumentsinto\ncontinuous vector space for legal case retrieval.Artif. Intell. Law28, 4 (2020), 441–467.\n[69] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser, and Illia\nPolosukhin. 2017. Attention is all you need. InAdvances in Neural Information Processing Systems. 5998–6008.\n[70] Suzan Verberne, E. K. L. D’hondt, N. H. J. Oostdijk, and Cornelis H. A. Koster. 2010. Quantifying the challenges in\nparsing patent claims. InProceedings of the 1st International Workshop on Advances in Patent Information Retrieval\n(AsPIRe’10).\n[71] Suzan Verberne and Eva D’hondt. 2009. Prior art retrieval using the claims section as a bag of words. InWorkshop of\nthe Cross-Language Evaluation Forum for European Languages. Springer, 497–501.\n[72] Suzan Verberne, Maya Sappelli, Djoerd Hiemstra, and Wessel Kraaij. 2016. Evaluation and analysis of term scoring\nmethods for term extraction.Inf. Retriev. J.19, 5 (2016), 510–545.\n[73] Kexin Wang, Nils Reimers, and Iryna Gurevych. 2021. TSDAE: Using transformer-based sequential denoising auto-\nencoder for unsupervised sentence embedding learning. arXiv:2104.06979. Retrieved fromhttps://arxiv.org/abs/2104.\n06979\n[74] KexinWang,NandanThakur, NilsReimers,andIryna Gurevych.2021.GPL:Generativepseudo labelingforunsuper-\nvised domain adaptation of dense retrieval. arXiv:2112.07577. Retrieved fromhttps://arxiv.org/abs/2112.07577\n[75] Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. Minilm: Deep self-attention distil-\nlation for task-agnostic compression of pre-trained transformers.Adv. Neural Inf. Process. Syst.33 (2020), 5776–5788.\n[76] J. Wen, Z. Zhong, Y. Bai, X. Zhao, and M. Yang. 2022. Siat@ coliee-2022: Legal case retrieval with longformer-based\ncontrastive learning. InProceedings of the 16th International Workshop on Juris-informatics (JURISIN 22).\n[77] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim\nRault, Rémi Louf, Morgan Funtowicz, et al. 2019. Huggingface’s transformers: State-of-the-art natural language pro-\ncessing. arXiv:1910.03771. Retrieved fromhttps://arxiv.org/abs/1910.03771\n[78] Ming Yan, Chenliang Li, Chen Wu, Bin Bi, Wei Wang, Jiangnan Xia, and Luo Si. 2019. IDST at TREC 2019 deep\nlearningtrack:Deepcascaderankingwithgeneration-baseddocumentexpansionandpre-trainedlanguagemodeling.\nIn Proceedings of the Text Retrieval Conference (TREC’19).\n[79] Eugene Yang, David D. Lewis, Ophir Frieder, David A. Grossman, and Roman Yurchak. 2018. Retrieval and richness\nwhen querying by document. InDesign of Experimental Search & Information REtrieval Systems (DESIRES), 68–75.\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.\n115:32 A. Askari et al.\n[80] Yin Yang, Nilesh Bansal, Wisam Dakka, Panagiotis Ipeirotis, Nick Koudas, and Dimitris Papadias. 2009. Query by\ndocument. InProceedings of the 2nd ACM International Conference on Web Search and Data Mining. 34–43.\n[81] Zeynep Akkalyoncu Yilmaz, Shengjin Wang, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Applying BERT to\ndocument retrieval with birch. InProceedings of the Conference on Empirical Methods in Natural Language Processing\nandthe9thInternationalJointConferenceonNaturalLanguageProcessing(EMNLP-IJCNLP’19):SystemDemonstrations .\n19–24.\n[82] Zeynep Akkalyoncu Yilmaz, Wei Yang, Haotian Zhang, and Jimmy Lin. 2019. Cross-domain modeling of sentence-\nlevel evidence for document retrieval. InProceedings of the Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP’19). 3490–3496.\n[83] MasaharuJulianoYoshioka.2021.COLIEE2020:Methodsforlegaldocumentretrievalandentailment.In NewFrontiers\ninArtificialIntelligence:JSAI-isAI’20Workshops,JURISIN,andLENLS’20Workshops,RevisedSelectedPapers ,Vol.12758.\nSpringer Nature, 196.\n[84] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip\nPham, Anirudh Ravula, Qifan Wang, Li Yang, et al. 2020. Big bird: Transformers for longer sequences. InProceedings\nof the Conference on Neural Information Processing Systems (NeurIPS’20).\n[85] HaotianZhang,MustafaAbualsaud,NimeshGhelani,AngshumanGhosh,MarkD.Smucker,GordonV.Cormack,and\nMaura R. Grossman. 2017. UWaterlooMDS at the TREC 2017 common core track. InProceedings of the Text Retrieval\nConference (TREC’17).\n[86] Ruiqi Zhong, Dhruba Ghosh, Dan Klein, and Jacob Steinhardt. 2021. Are larger pretrained language models uni-\nformly better? comparing performance at the instance level. arXiv:2105.06020. Retrieved fromhttps://arxiv.org/abs/\n2105.06020\n[87] Helong Zhou, Liangchen Song, Jiajie Chen, Ye Zhou, Guoli Wang, Junsong Yuan, and Qian Zhang. 2021. Rethinking\nsoft labels for knowledge distillation: A bias-variance tradeoff perspective. arXiv:2102.00650. Retrieved fromhttps:\n//arxiv.org/abs/2102.00650\nReceived 28 February 2023; revised 31 August 2023; accepted 16 October 2023\nACM Transactions on Information Systems, Vol. 42, No. 5, Article 115. Publication date: April 2024.",
  "topic": "Information retrieval",
  "concepts": [
    {
      "name": "Information retrieval",
      "score": 0.5094695687294006
    },
    {
      "name": "Computer science",
      "score": 0.44793474674224854
    },
    {
      "name": "Transformer",
      "score": 0.4374862313270569
    },
    {
      "name": "Natural language processing",
      "score": 0.35718339681625366
    },
    {
      "name": "Engineering",
      "score": 0.11370930075645447
    },
    {
      "name": "Electrical engineering",
      "score": 0.036656975746154785
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}