{
  "title": "Quantum Vision Transformers",
  "url": "https://openalex.org/W4296555515",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5000160292",
      "name": "El Amine Cherrat",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5051319114",
      "name": "Iordanis Kerenidis",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5026770655",
      "name": "Natansh Mathur",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5069310526",
      "name": "Jonas Landman",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5059364799",
      "name": "Martin Strahm",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5038512843",
      "name": "Yun Yvonna Li",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4312189222",
    "https://openalex.org/W4386794512",
    "https://openalex.org/W4280563615",
    "https://openalex.org/W2559394418",
    "https://openalex.org/W2089217417",
    "https://openalex.org/W4320032194",
    "https://openalex.org/W2752159661",
    "https://openalex.org/W4221159392",
    "https://openalex.org/W4317436377",
    "https://openalex.org/W3098400458",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W4300988299",
    "https://openalex.org/W185620388",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3200849153",
    "https://openalex.org/W3002718752",
    "https://openalex.org/W2981748719",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3203554004",
    "https://openalex.org/W4213212652",
    "https://openalex.org/W4387277693",
    "https://openalex.org/W4386875554",
    "https://openalex.org/W3095948197",
    "https://openalex.org/W3111162498",
    "https://openalex.org/W391578156",
    "https://openalex.org/W2790388700",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2061171222",
    "https://openalex.org/W3036863718",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3189829096",
    "https://openalex.org/W3007475506",
    "https://openalex.org/W3207377511",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2581719241",
    "https://openalex.org/W4312788581",
    "https://openalex.org/W4312224136",
    "https://openalex.org/W2922073769"
  ],
  "abstract": "In this work, quantum transformers are designed and analysed in detail by extending the state-of-the-art classical transformer neural network architectures known to be very performant in natural language processing and image analysis. Building upon the previous work, which uses parametrised quantum circuits for data loading and orthogonal neural layers, we introduce three types of quantum transformers for training and inference, including a quantum transformer based on compound matrices, which guarantees a theoretical advantage of the quantum attention mechanism compared to their classical counterpart both in terms of asymptotic run time and the number of model parameters. These quantum architectures can be built using shallow quantum circuits and produce qualitatively different classification models. The three proposed quantum attention layers vary on the spectrum between closely following the classical transformers and exhibiting more quantum characteristics. As building blocks of the quantum transformer, we propose a novel method for loading a matrix as quantum states as well as two new trainable quantum orthogonal layers adaptable to different levels of connectivity and quality of quantum computers. We performed extensive simulations of the quantum transformers on standard medical image datasets that showed competitively, and at times better performance compared to the classical benchmarks, including the best-in-class classical vision transformers. The quantum transformers we trained on these small-scale datasets require fewer parameters compared to standard classical benchmarks. Finally, we implemented our quantum transformers on superconducting quantum computers and obtained encouraging results for up to six qubit experiments.",
  "full_text": "Quantum Vision Transformers\nEl Amine Cherrat 1, Iordanis Kerenidis 1,2, Natansh Mathur 1,2, Jonas Landman 3,2, Martin Strahm 4,\nand Yun Yvonna Li 4\n1IRIF, CNRS - Universit ´e Paris Cit´e, France\n2QC Ware, Palo Alto, USA and Paris, France\n3School of Informatics, University of Edinburgh, Scotland, UK\n4F. Hoffmann La Roche AG\nIn this work, quantum transformers\nare designed and analysed in detail by\nextending the state-of-the-art classical\ntransformer neural network architectures\nknown to be very performant in natu-\nral language processing and image anal-\nysis. Building upon the previous work,\nwhich uses parametrised quantum circuits\nfor data loading and orthogonal neural lay-\ners, we introduce three types of quan-\ntum transformers for training and infer-\nence, including a quantum transformer\nbased on compound matrices, which guar-\nantees a theoretical advantage of the quan-\ntum attention mechanism compared to\ntheir classical counterpart both in terms\nof asymptotic run time and the number of\nmodel parameters. These quantum archi-\ntectures can be built using shallow quan-\ntum circuits and produce qualitatively dif-\nferent classification models. The three\nproposed quantum attention layers vary\non the spectrum between closely following\nthe classical transformers and exhibiting\nmore quantum characteristics. As build-\ning blocks of the quantum transformer,\nwe propose a novel method for loading a\nmatrix as quantum states as well as two\nnew trainable quantum orthogonal layers\nadaptable to different levels of connectiv-\nity and quality of quantum computers.\nWe performed extensive simulations of the\nquantum transformers on standard med-\nical image datasets that showed compet-\nitively, and at times better performance\ncompared to the classical benchmarks, in-\ncluding the best-in-class classical vision\ntransformers. The quantum transformers\nJonas Landman: jonas.landman@qcware.com\nwe trained on these small-scale datasets re-\nquire fewer parameters compared to stan-\ndard classical benchmarks. While this ob-\nservation aligns with the anticipated com-\nputational benefit of our quantum atten-\ntion layers, particularly regarding the size\nof the input images, further validation is\nnecessary to confirm these initial findings\nas quantum computers scale up. Finally,\nwe implemented our quantum transform-\ners on superconducting quantum comput-\ners and obtained encouraging results for\nup to six qubit experiments.\n1 Introduction\nQuantum machine learning [1] uses quantum\ncomputation in order to provide novel and pow-\nerful tools to enhance the performance of clas-\nsical machine learning algorithms. Some use\nparametrised quantum circuits to compute quan-\ntum neural networks and explore a higher-\ndimensional optimisation space [2, 3, 4], while\nothers exploit interesting properties native to\nquantum circuits, such as orthogonality or uni-\ntarity [5, 6].\nIn this work, we focus on transformers, a neu-\nral network architecture proposed by [7] which\nhas been applied successfully to both natural lan-\nguage processing [8] and visual tasks [9], pro-\nviding state-of-the-art performance across differ-\nent tasks and datasets [10]. While the trans-\nformer architecture and attention mechanism\nwere notably popularized by [7], antecedents of\nthese mechanisms can be found in earlier works.\nSpecifically, [11] explored such concepts in the\nrealm of neural machine translation. In ear-\nlier works, recurrent neural network approaches\nhinted at the underpinnings of attention-like\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 1\narXiv:2209.08167v2  [quant-ph]  20 Feb 2024\nFigure 1: Vision Transformer Overview\n Figure 2: Patch Division Preprocessing\nComponents of the Vision Transformer (1/2): Fig.1 shows the global architecture of a vision transformers. First,\nthe image is preprocessed using patch division (Fig.2), and then several transformer layers are applied (see details in\nFig.3 and Fig.4). The final step consists in a simple fully connected neural network for classification.\nmechanisms, which can be found in [12, 13]. At a\nhigh level, transformers are neural networks that\nuse an attention mechanism that takes into ac-\ncount the global context while processing the en-\ntire input data element-wise. For visual recogni-\ntion or text understanding, the context of each\nelement is vital, and the transformer can cap-\nture more global correlations between parts of the\nsentence or the image compared to convolutional\nneural networks without an attention mechanism\n[9]. In the case of visual analysis for example, im-\nages are divided into smaller patches, and instead\nof simply performing patch-wise operations with\nfixed size kernels, a transformer learns attention\ncoefficients per patch that weigh the attention\npaid to the rest of the image by each patch.\nIn one related work, classical transformer\narchitectures and attention mechanisms have\nbeen used to perform quantum tomography [14].\nMoreover, a quantum-enhanced transformer for\nsentiment analysis has been proposed in [15], and\na self-attention mechanism for text classification\nhas been used in [16]. These works use stan-\ndard variational quantum circuits to compute the\nneural networks, and the attention coefficients\nare calculated classically. A method for using a\nnatively quantum attention mechanism for rein-\nforcement learning has also been proposed in [17].\n[18] performed semiconductor defect detection\nusing quantum self-attention, also using standard\nvariational quantum circuits. We also note the\nproposals of [2, 19] for variational circuits with\nsimilarities to convolutional neural networks for\ngeneral purpose image classification.\nThe difference between the above-mentioned\napproaches and the proposed approached of this\nwork mainly stems from the linear algebraic tools\nwe developed which make our quantum circuits\nmuch more Noisy Intermediate-Scale Quantum\n(NISQ)-friendly with proven scalability in terms\nof run time and model parameters, in contrast\nto variational quantum circuit approaches taken\nin [20, 4]which lack proof of scalability [21].\nThis advantage in scalability of our proposed\nparametrised quantum circuits is made possible\nby the use of a specific amplitude encoding for\ntranslating vectors as quantum states, and con-\nsistent use of hamming-weight preserving quan-\ntum gates instead of general quantum ansatz. In\naddition to a quantum translation of the classical\nvision transformer, a novel and natively quan-\ntum method is proposed in this work, namely\nthe compound transformer, which invokes Clif-\nford Algebra operations that is hard to compute\nclassically.\nWhile we adapted the vision transformer archi-\ntecture to ease the translation of the attention\nlayer into quantum circuits and benchmarked\nour methods on vision tasks, the proposed ap-\nproaches for quantum attention mechanism can\nbe easily adapted to apply to other fields of ap-\nplications, for example in natural language pro-\ncessing where transformers have been proven to\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 2\nFigure 3: Transformer layer\nFigure 4: Attention Mechanism\nComponents of the Vision Transformer (2/2): Components in a single transformer layer is outlined in (Fig.3). At its\ncore, the attention mechanism learns how to weigh different parts of the input (Fig.4), where the trainable matrices\nare denoted by V and W. This attention mechanism is the focus of our quantum circuits.\nbe particularly efficient [8].\nThe main ingredient in a transformer as in-\ntroduced by [9] is the attention layer , shown in\nFig.4. This attention layer is also the focus of this\nwork which seeks to leverage quantum circuit for\ncomputational advantages. Given an input im-\nage X ∈Rn×d, we transform the input data into\nn patches each with dimension of d, and denote\neach patch i with xi ∈Rd. The trainable weight\nmatrix from the linear fully connected layer at\nthe beginning of each attention layer is denoted\nby V. The heart of the attention mechanism,\ni.e. the attention coefficients which weighs each\npatch xi to every other patch is denoted by:\nAij = xT\ni Wxj,\nwhere W represents the second trainable weight\nmatrix.\nBased on the architecture shown in Fig.4 we\npropose three types of quantum transformers\n(Sections 3.1, 3.2 and 3.4) and apply these novel\narchitectures to visual tasks for benchmarking.\nSection 3.3 outlines the approach of combining\n3.1 and 3.2 into one circuit to perform inference\non the quantum circuit once the attention coef-\nficients have been trained, while sections 3.1, 3.2\nand 3.4 propose 3 distinct quantum architecture\nfor training and inference.\nThe first quantum transformer introduced in\nSection 3.1 implements a trivial attention mecha-\nnism which where each patch pays attention only\nto itself while retaining the beneficial property of\nguaranteed orthogonality of trained weight ma-\ntrices [22]. In the second quantum transformer\nintroduced in Section 3.2, coined the Orthogonal\nTransformer, we design a quantum analogue for\neach of the two main components of a classical at-\ntention layer: a linear fully connected layer and\nthe attention matrix to capture the interaction\nbetween patches. This approach follows the clas-\nsical approach quite closely. In Section 3.4, the\nCompound Transformer, which takes advantage\nof the quantum computer to load input states in\nsuperposition, is defined. For each of our quan-\ntum methods, we provide theoretical analysis of\nthe computational complexity of the quantum at-\ntention mechanisms which is lower compared to\ntheir classical counterparts.\nThe mathematical formalism behind the Com-\npound Transformer is the second-order com-\npound matrix [23]. Compound Transformer uses\nquantum layers to first load all patches into the\nquantum circuit in uniform superposition and\nthen apply a single unitary to multiply the input\nvector in superposition with a trainable second-\norder compound matrix [24]. Here both the in-\nput vector and the trainable weight matrix are no\nlonger a simple vector or a simple matrix. Details\nare given in Sections 3 and 3.4.\nThe fundamental building blocks for the imple-\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 3\nFigure 5: Data loader circuit for a matrix X ∈Rn×d. The top register uses N qubits and the vector data loader\nto load the norms of each row, (∥x1∥,··· ,∥xn∥), to obtain the state 1\n∥X∥\n∑n\ni=1 ∥xi∥|ei⟩. The lower register uses\nd qubits to load each row xi ∈Rd sequentially, by applying the vector loader and their adjoint for each row xi,\nwith CNOTs controlled by the corresponding qubit iof the top register. Each loader on the lower register has depth\nO(log d). s\nmentation of a transformer architecture including\nthe matrix data loader and quantum orthogonal\nlayers are introduced in Sections 2.1, 2.2.\n2 Quantum Tools\nIn this work, we will use the RBS gate given in\nEq.(1). RBS gates implement the following uni-\ntary:\nRBS(θ) =\n\n\n1 0 0 0\n0 cos( θ) sin( θ) 0\n0 −sin(θ) cos( θ) 0\n0 0 0 1\n\n (1)\nThis gate can be implemented rather easily, ei-\nther as a native gate, known as FSIM [25], or us-\ning four Hadamard gates, two Ry rotation gates,\nand two two-qubits CZ gates:\nFigure 6: A possible decomposition of the RBS(θ) gate.\n2.1 Quantum Data Loaders for Matrices 1\nLoading a whole matrix X ∈Rn×d in a quan-\ntum state is a powerful technique for machine\nlearning. [26] designed quantum circuits to load\ninput vectors using using N = n+ d qubits with\na unary amplitude encoding, more specifically a\nbasis of states of hamming weight 1 where all\nqubits are in state 0 except one in state 1 is\nused. The number of required gates to load a\nvector is d−1. In this work, we extend their ap-\nproach to build a data loader for matrices (Fig.5)\n1For this section, details are provided in B.1.\nCircuit Hardware ConnectivityDepth # Gates\nPyramid Nearest Neighbour 2N−3 N(N−1)\n2\nX Nearest Neighbour N−1 2N−3\nButterfly All-to-all log(N) N\n2 log(N)\nTable 1: Comparison of different quantum orthogonal\nlayer circuits with N qubits.\nwhere every row of X is loaded in superposition.\nThe required number of gates to load a matrix is\n(n−1)+(2n−1)(d−1). The resulting state of the\nmatrix loader shown in Fig.5 is a superposition\nof the form:\n|X⟩= 1\n∥X∥\nn∑\ni=1\nd∑\nj=1\nXij |ej⟩|ei⟩ (2)\n2.2 Quantum Orthogonal Layers 2\nThe classical attention layer (Fig.4) starts with\na linear fully connected layer, where each input,\ni.e. patch xi, is a vector and is multiplied by\na weight matrix V. To perform this operation\nquantumly we generalise the work of [5], where a\nquantum orthogonal layer is defined as a quan-\ntum circuit applied on a state |x⟩(encoded in the\nunary basis) to produce the output state |Vx⟩.\nMore precisely, V is the matrix corresponding to\nthe unitary of the quantum layer, restricted to\nthe unary basis. This matrix is orthogonal due\nto the unitary nature of quantum operations.\nIn addition to the already existing Pyramid cir-\ncuit (Fig.7) from [5], we define two new types of\nquantum orthogonal layers with different levels of\nexpressivity and resource requirements: the but-\nterfly circuit (Fig.8), and the X circuit (Fig.9).\nLooking at Table 1, the X circuit is the most\nsuited for noisy hardware. It requires smaller\n2Refer to B.2 for additional details about this section.\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 4\nFigure 7: Pyramid Circuit\n Figure 8: Butterfly Circuit\n Figure 9: X Circuit\nQuantum Orthogonal Layers. Vertical lines represent two-qubit RBS gates, parametrized with independent angles θ.\nnumber of gates while maintaining a path from\nevery input qubit to every output qubit. It\nis also less expressive with a restrained set of\npossible orthogonal matrices and fewer trainable\nparameters. The butterfly circuit requires\nlogarithmic circuit depth, a linear number of\ngates, and exhibits a higher level of expressivity.\nIt originates from the classical Cooley–Tukey\nalgorithm [27] used for Fast Fourier Transform\nand, it performs an operation analogous to the\nmethod presented in [28] for classical recurrent\nneural networks when it is implemented with\nRBS gates. Note that the butterfly circuit\nrequires the ability to apply gates on all possible\nqubit pairs.\nAs shown in [24], quantum orthogonal layers\ncan be generalised to work with inputs which en-\ncode a vector on a larger basis. Namely, instead\nof the unary basis, where all qubits except one\nare in state 0, basis of hamming weight k can\nbe used as well. A basis of hamming weight k\ncomprises of\n(N\nk\n)\npossible states over N qubits.\nA vector x ∈R(N\nk ) can be loaded as a quantum\nstate |x⟩using only N qubits. Since the quantum\northogonal layers are hamming weight preserving\ncircuits, the output state from such circuits will\nalso be a vector encoded in the same basis. Let V\nbe the matrix corresponding to the quantum or-\nthogonal layer in the unary basis, and x of ham-\nming weight k, the output state will no longer\nbe |Vx⟩, but instead |V(k)x⟩, where V(k) is the\nk-th order compound matrix of V [23]. We can\nsee V(k) as the expansion of V in the hamming\nweight k basis. More precisely, given a matrix\nV ∈RN×N , the kth-order compound matrix V(k)\nfor k ∈[N] is the\n(N\nk\n)\ndimensional matrix with\nentries:\nV(k)\nIJ = det(VIJ ),\nwhere I and J are subsets of rows and columns\nof V with size k.\nRecent research supports the trainability of the\nquantum layers presented in this paper. [29] pro-\nvide evidence for the trainability and expressivity\nof hamming weight preserving circuits, indicating\nthat our layers are not prone to the vanishing gra-\ndients problem, commonly referred to as barren\nplateaus. This assertion is further reinforced by\nstudies in [30, 31]. Nonetheless, the existence and\nimplications of exponential local minima [32, 33]\nwithin our framework remain an open question.\n3 Quantum Transformers\nThe second component of the classical attention\nlayer is the interaction between patches (Fig.4)\nwhere the attention coefficients Aij = xT\ni Wxj is\ntrained by performing xT\ni Wxj for a trainable or-\nthogonal matrix W and all pairs of patches xi\nand xj. After that, a non-linearity, for exam-\nple softmax, is applied to obtain each output yi.\nThree different approaches for implementing the\nquantum attention layer are introduced in the\nnext sections, listed in the order of increasing\ncomplexity in terms of quantum resource require-\nment, which reflect the degree to which quan-\ntum circuits are leveraged to replace the atten-\ntion layer. A comparison between these different\nquantum methods is provided in Table 2, which\nis applicable to both training and inference.\nTable 2 lists 5 key parameters of the proposed\nquantum architecture which reflect their theoret-\nical scalability. The number of trainable param-\neters for a classical vision transformer is 2d2 (see\nSection A), which can be directly compared with\nthe number of trainable parameters of the pro-\nposed quantum approaches. The number of fixed\nparameters per quantum architecture is required\nfor data loading. In this table, the circuit depth\nrepresents the combined depth of both the data\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 5\nloader and the quantum layer. Furthermore, the\nbutterfly layer detailed in Fig.8 and the diago-\nnal data-loader illustrated in Fig.14 is employed,\nwhich adds logarithmic depth for loading each\nvector. The circuit depth together with the num-\nber of distinct circuits dictate the overall run time\nof the quantum architectures, which can be com-\npared to the run time of the classical transformer\nof O(nd2 + n2d) (listed under the column Cir-\ncuit Depth ). The number of distinct circuits per\nquantum architecture indicate the possibility for\neach architecture to be processed in parallel, akin\nto multi-core CPU processing.\n3.1 Orthogonal Patch-wise Neural Network\nFigure 10: Quantum circuit to perform the matrix multi-\nplication Vxi (fully connected layer) using a data loader\nfor xi and a quantum orthogonal layer for V.\nThe orthogonal patch-wise neural network can\nbe thought of as a transformer with a trivial at-\ntention mechanism, where each patch pays at-\ntention only to itself. As illustrated in Fig 10,\neach input patch is multiplied by the same train-\nable matrix V and one circuit per patch is used.\nEach circuit has N = d qubits and each patch\nxi is encoded in a quantum state with a vector\ndata loader. A quantum orthogonal layer is used\nto perform multiplication of each patch with V.\nThe output of each circuit is a quantum state en-\ncoding Vxi, a vector which is retrieved through\ntomography. Importantly, this tomography pro-\ncedure deals with states of linear size in relation\nto the number of qubits, avoiding the exponen-\ntial complexity often associated with quantum\ntomography.\nThe computational complexity of this circuit\nis calculated as follows: from Section 2.1, a data\nloader with N = d qubits qubits has a complex-\nity of log(d) steps. For the orthogonal quantum\nlayer, as shown in Table 1, a butterfly circuit\ntakes log(d) steps, with d\n2 log(d) trainable param-\neters. Overall, the complexity is O(log(d)) and\nthe trainable parameters are O(dlog d). Since\nthis circuit uses one vector data loader, the num-\nber of fixed parameters required is d−1.\n3.2 Quantum Orthogonal Transformer\nFigure 11: Quantum circuit to compute |xT\ni Wxj|2, a\nsingle attention coefficient, using data loaders for xi and\nxj and a quantum orthogonal layer for W.\nLooking at Fig.11, each attention coefficient\nAij = xT\ni Wxj, xj is calculated first by loading\nxj into the circuit with a vector loader followed\nby a trainable quantum orthogonal layer, W, re-\nsulting in the vector Wxj. Next, an inverse data\nloader of xi is applied, creating a state where the\nprobability of measuring 1 on the first qubit is\nexactly |xT\ni Wxj|2 = A2\nij.\nNote the square that appears in the quantum\ncircuit is already one type of non-linearity. Using\nthis method, coefficients of A are always positive,\nwhich can still be learned during training as we\nshow later in the Section 4. Additional methods\nalso exist to obtain the sign of the inner product\n[5]. The estimation of Aij (and therefore A′\nij if\nneeded, by applying a column-wise softmax clas-\nsically) is repeated for each pair of patches and\nthe same trainable quantum orthogonal layerW.\nThe computational complexity of this quantum\ncircuit is similar to the previous one, with one\nmore data loader.\nPutting Figures 10 and 11 together: the quan-\ntum circuit presented in Section 3.1 is imple-\nmented to obtain each Vxj. At the same time,\neach attention coefficient |xT\ni Wxj|2 is computed\non the quantum circuit, which is further post-\nprocessed column-wise with the softmax func-\ntion to obtain the A′\nij. The two parts can\nthen be classically combined to compute each\nyi = ∑\nj A′\nijVxj. In this approach, the atten-\ntion mechanism is implemented by using ham-\nming weight preserving parametrised quantum\ncircuits to compute the weight matricesV and W\nseparately. For computing |xT\ni Wxj|2, we would\nrequire two data loaders ( 2 ×(d−1) gates) for\nxi and xj, and one Quantum Orthogonal Layer\n(dlog d gates in the case of Butterfly layer) for\nW. To obtain Vxj, we require d−1 gates to\nload each xj and a Quantum Orthogonal Layer\n(dlog d gates in the case of Butterfly layer) for\nthe matrix V.\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 6\nTransformer architecture # Qubits Circuit depth # Trainable parameters # Fixed parameters # Distinct circuits\nA - Orthogonal Patch-wise d O(logd) O(dlogd) d−1 n\nB - Quantum Orthogonal Transformerd O(logd) O(dlogd) 3( d−1) n+n2\nC - Quantum Attention Mechanismn+d O(logn+nlogd+ logd) O(dlogd) n−1 + (2n−1)(d−1) n\nD - Compound Transformern+d O(logn+nlogd+ log(n+d)) O((n+d) log(n+d))) n−1 + (2n−1)(d−1) 1\nClassical Transformer - O(nd2+n2d) O(2d2) - -\nTable 2: Comparison of different quantum methods to perform a single attention layer of a transformer network. n\nand d stand respectively for the number of patches and their individual dimension. All quantum orthogonal layers\nare implemented using the butterfly circuits. See Section 3 for details.\n3.3 Direct Quantum Attention\nFigure 12: Quantum circuit to directly apply the atten-\ntion mechanism, given each coefficient in A. The first\npart of the circuit corresponds to the matrix data loader\nfrom Fig.5, where Load(∥X∥) is replaced by Load(Ai).\nA quantum orthogonal layer from Section 2.2 is used for\nV.\nIn Section 3.2, the output of the attention layer\nyi = ∑\nj A′\nijVxj is computed classically once the\nquantities A′\nij and Vxj have been computed sep-\narately with the help of quantum circuits. Dur-\ning inference, where the matrices V and W have\nbeen learnt, and the attention matrix A (or A′)\nis stored classically, Direct Quantum Attention\nimplements the attention layer directly on the\nquantum computer. The matrix data loader from\nFig.5 is used to compute each yi = ∑\nj AijVxj\nusing a single quantum circuit.\nIn Fig.12, yi, which corresponds to the output\npatch with index i, is computed using a quantum\ncircuit using N = n+ dqubits. These qubits are\nsplit into two main registers. On the top register\n(nqubits), the vector Ai, ith row of the attention\nmatrix A (or A′), is loaded via a vector data\nloader, as ∑\nj Aij |ej⟩|0⟩.\nNext, on the lower register ( d qubits), as\nin Fig.5, the data loader for each vector xi,\nand their respective adjoint, are applied sequen-\ntially, with CNOTs controlled on each qubit i of\nthe top register. This gives the quantum state∑\nj Aij |ej⟩|xj⟩, i.e. the matrix X is loaded with\nall rows re-scaled according to the attention co-\nefficients. As for any matrix data loader, this\nrequires (n−1) + (2n−1)(d−1) gates with fixed\n(non trainable) parameters.\nThe last step consists of applying the quantum\northogonal layer V that has been trained before\non the second register of the circuit. As previ-\nously established, this operation performs matrix\nmultiplication between V and the vector encoded\non the second register. Since the kth element of\nthe vector Vxj can be written as ∑\nq VkqXjq , we\nget:\n∑\nj\nAij |ej⟩|Vxj⟩\n=\n∑\nj\nAij |ej⟩\n∑\nk\n(\n∑\nq\nVkqXjq ) |ek⟩\n=\n∑\nk\n∑\nj\nAij(\n∑\nq\nVkqXjq ) |ej⟩|ek⟩ (3)\nSince yi = ∑\nj AijVxj, its kth element can be\nwritten yik = ∑\nj Aij(∑\nq VkqXjq ). Therefore, the\nquantum state at the end of the circuit can be\nwritten as |yi⟩= ∑\nk yik |ϕk⟩|ek⟩for some nor-\nmalised states |ϕk⟩. Performing tomography on\nthe second register generates the output vector\nyi.\nThis circuit is a more direct method to com-\npute each yi. Each yi uses a different Ai in the\nfirst part of the circuit. As shown in Table 2,\ncompared with the previous method, this method\nrequires fewer circuits to run, but each circuit re-\nquires more qubits and a deeper circuit. To anal-\nyse the computational complexity: the first data\nloader on the top register has n qubit and log n\ndepth; the following 2n−1 loaders on the bot-\ntom register havedqubits, so (2n−1) logddepth;\nand the final quantum orthogonal layer V imple-\nmented using a butterfly circuit, has a depth of\nlog d and O(dlog d) trainable parameters.\n3.4 Quantum Compound Transformer\nUntil now, each step of the classical vision trans-\nformer has been reproduced closely by quantum\nlinear algebraic procedures. The same quantum\ntools can also be used in a more natively quantum\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 7\nfashion, while retaining the spirit of the classical\ntransformers, as shown in Fig.13.\nAt a high level, the compound transformer first\nloads all patches with the same weight applied\neach patch in superposition, and then apply an\northogonal layer that will at the same time ex-\ntract the features from each patch and re-weight\nthe patches so that in the end the output is com-\nputed as a weighted sum of the features extracted\nfrom all patches. This means that instead of cal-\nculating two separate weight matrices V and W,\none for feature extraction and one for weighting\nto generate yi = ∑\nj A′\nijVxj individually, only\none operation is used to generate all yi directly\nfrom one circuit. Since a single quantum orthgo-\nnal layer is used to generate Y, we switch to Vc\nto denote this orthogonal layer that applies the\ncompound matrix as we explain below.\nFigure 13: Quantum circuit to execute one attention\nlayer of the Compound Transformer. We use a matrix\ndata loader for X (equivalent to Fig.5) and a quantum\northogonal layer for Vc applied on both registers.\nMore precisely, the quantum circuit we use has\ntwo registers: the top one of size n and the bot-\ntom one of size d. The full matrix X ∈Rn×d\nis loaded into the circuit using the matrix data\nloader from Section 2.1 with N = n+ d qubits.\nThis could correspond to the entire image, as ev-\nery image can be split into n patches of size d\neach. Since the encoding basis over the two reg-\nisters has more than one qubit in state 1, we are\nstepping out of the unary basis framework. The\ncorrect basis to consider is of hamming weight 2.\nNote that, among the\n(n+d\n2\n)\nstates with hamming\nweight 2, only n×dof them correspond to states\nwith one 1 in the top qubits, and another 1 in\nthe remaining bottom qubits.\nNext, a quantum orthogonal layer Vc is ap-\nplied on both registers at the same time. Note\nthat this Vc is not the same as in the previous\nconstructions, since now it is applied on a su-\nperposition of patches. As explained in Section\n2.2 and in [24], the resulting operation in this\ncase is not a simple matrix-vector multiplication\nVX. Instead of V, the multiplication involves\nits 2nd-order compound matrix V(2)\nc of dimension(n+d\n2\n)\n×\n(n+d\n2\n)\n. Similarly, the vector multiplied is\nnot simply X but a modified version of size\n(n+d\n2\n)\n,\nobtained by padding the added dimensions with\nzeros.\nThe resulting state is |Y⟩= |V(2)\nc X⟩, where\nV(2)\nc is the 2nd-order compound matrix of the\nmatrix Vc, namely the matrix corresponding to\nthe unitary of the quantum orthogonal layer in\nFig.13 restricted to the unary basis. This state\nhas dimension\n(n+d\n2\n)\n, i.e. there are exactly two\n1s in the N = n+ d qubits, but one can post-\nselect for the part of the state where there is\nexactly one qubit in state 1 on the top regis-\nter and the other 1 on the lower register. This\nway, n×d output states are generated. In other\nwords, tomography is performed for a state of\nthe form |Y⟩= 1\n∥Y∥\n∑n\ni=1\n∑d\nj=1 yij |ej⟩|ei⟩which\nis used to conclude that this quantum circuit pro-\nduces transformed patches (y1,··· ,yn) ∈Rn×d.\nNote that in this context, the proposed tomogra-\nphy approach reconstructs vectors of a quadratic\nsize, and not exponential, relative to the qubit\ncount. Furthermore, a significant fraction of the\nmeasurement shots might be discarded to narrow\ndown to the desired n×dspace as part of the the\npost-selection technique.\nTo calculate the computational complexity of\nthis circuit, we consider: the matrix data loader,\ndetailed in Fig.5 which has depth of log n +\n2nlog d; the Quantum Orthogonal Layer applied\non n+ d qubits, with a depth of log(n+ d) and\n(n+ d) log(n+ d) trainable parameters if imple-\nmented using the butterfly circuit. Since this cir-\ncuit uses exactly one matrix loader, the number\nof fixed parameters is (n−1) + (2n−1)(d−1).\nIn order to calculate the cost of performing\nthe same operation on a classical computer, con-\nsider the equivalent operation of creating the\ncompound matrix V(2)\nc by first computing all de-\nterminants of the matrix and then performing a\nmatrix-vector multiplication of dimension\n(n+d\n2\n)\n,\nwhich takes O((n+ d)4) time. Performing this\noperation on a quantum computer can provide a\npolynomial speedup with respect to n. More gen-\nerally, this compound matrix operation on an ar-\nbitrary input state of hamming weight k is quite\nhard to perform classically, since all determinants\nmust be computed, and a matrix-vector multipli-\ncation of size\n(n+d\nk\n)\nneeds to be applied.\nOverall, the compound transformer can replace\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 8\nboth the Orthogonal Patch-wise Network (3.1)\nand the Quantum Transformer layer (3.2) with\none combined operation. The use of compound\nmatrix multiplication makes this approach dif-\nferent from the classical transformers, while re-\ntaining some interesting properties with its clas-\nsical counterpart: patches are weighted in their\nglobal context and gradients are shared through\nthe determinants used to generate the compound\nmatrix.\nThe Compound Transformer operates in a sim-\nilar spirit as the MLPMixer architecture pre-\nsented in [34], which is a state-of-the-art ar-\nchitecture used for image classification tasks\nand exchanges information between the different\npatches without using convolution or attention\nmechanisms.\n4 Experiments\nIn order to benchmark the proposed methods, we\napplied them to a set of medical image classifica-\ntion tasks, using both simulations and quantum\nhardware experiments. MedMNIST, a collection\nof 12 preprocessed, two-dimensional, open source\nmedical image datasets from [35, 36], annotated\nfor classification tasks and benchmarking using a\ndiverse set of classical techniques, is used to pro-\nvide the complete training and validation data.\n4.1 Simulation Setting\nOrthogonal Patch-wise Network from Section 3.1,\nOrthogonal Transformer from Section 3.2, and\nCompound Transformer from Section 3.4 were\ntrained via simulation, along with two baseline\nmethods. The first baseline is the Vision Trans-\nformer from [9], which has been successfully ap-\nplied to different image classification tasks and\nis described in detail in A. The second baseline\nis the Orthogonal Fully-Connected Neural Net-\nwork (OrthoFNN), a quantum method without\nattention layer that has been previous trained on\nthe RetinaMNIST dataset in [5]. For each of the\nfive architectures, one model was trained on each\ndataset of MedMNIST and validated using the\nsame validation method as in [35, 36].\nTo ensure comparable evaluations between\nthe five neural networks, similar architectures\nwere implemented for all five. The bench-\nmark architectures all comprise of three parts:\npre-processing, features extraction, and post-\nprocessing. The first part is classical and pre-\nprocesses the input image of size 28 ×28 by ex-\ntracting 16 patches (n = 16) of size 7 ×7. We\nthen map every patch to a16 dimensional feature\nspace (d= 16) by using a fully connected neural\nnetwork layer. This first feature extraction com-\nponents is a single fully connected layer trained\nin conjunction to the rest of the architecture.\nFor the OrthoNN networks, used as our quan-\ntum baseline, one patch of size 16 was extracted\nfrom the complete input image using a fully con-\nnected neural network layer of size784×16. This\nfully connected layer is also trained in conjunc-\ntion to the quantum circuits. The second part\nof the common architecture transforms the ex-\ntracted features by applying a sequence of 4 at-\ntention layers on the extracted patches, which\nmaintain the dimension of the layer. Moreover,\nthe same gate layout, i.e. the butterfly circuit,\nis used for all circuits that compose the quan-\ntum layers. Finally, the last part of the neural\nnetwork is classical, which linearly projects the\nextracted features and outputs the predicted la-\nbel.\n4.2 Simulation Results\nA summary of the simulation results is shown in\nTable 3 where the area under receiver operating\ncharacteristic (ROC) curve (AUC) and the accu-\nracy (ACC) are reported as evaluation metrics.\nA full comparison with the classical benchmark\nprovided by [35] is given in Appendix D, Table 6.\nFrom Table 3, we observe that Vision Trans-\nformer, Orthogonal Transformer, and Com-\npound Transformer architectures outperform the\nOrthogonal Fully-Connected and Orthogonal\nPatch-wise neural networks for all 12 tasks. This\nis likely due to the fact that the latter two\narchitectures do not contain on any attention\nmechanism that exchange information across the\npatches, confirming the effectiveness of the at-\ntention mechanism to learn useful features from\nimages. Second, Orthogonal Transformer and\nCompound Transformer, which implements non-\ntrivial quantum attention mechanism, provide\nvery competitive performances compared to the\ntwo benchmark methods and outperform the\nbenchmark methods on 7 out of 12 MedMNIST\ndatasets.\nMoreover, comparisons can be made with re-\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 9\nNetwork PathMNIST ChestMNIST DermaMNIST OCTMNIST PneumoniaMNIST RetinaMNIST\nAUC ACC AUC ACC AUC ACC AUC ACC AUC ACC AUC ACC\nOrthoFNN (baseline) 0.939 0.643 0.701 0.947 0.883 0.719 0.819 0.516 0.950 0.864 0.731 0.548\nOrthoPatchWise 0.953 0.713 0.692 0.947 0.898 0.730 0.861 0.554 0.945 0.867 0.739 0.560\nVisionTransformer (baseline) 0.957 0.7550.718 0.9480.895 0.727 0.879 0.608 0.957 0.902 0.736 0.548\nOrthoTransformer 0.964 0.7740.703 0.947 0.891 0.719 0.875 0.606 0.947 0.885 0.745 0.542\nCompoundTransformer 0.957 0.735 0.698 0.9470.901 0.7340.867 0.545 0.947 0.885 0.740 0.565\nNetwork BreastMNIST BloodMNIST TissueMNIST OrganAMNIST OrganCMNIST OrganSMNIST\nAUC ACC AUC ACC AUC ACC AUC ACC AUC ACC AUC ACC\nOrthoFNN (baseline) 0.815 0.821 0.972 0.820 0.819 0.513 0.916 0.636 0.923 0.672 0.875 0.481\nOrthoPatchWise 0.830 0.827 0.984 0.866 0.845 0.549 0.973 0.786 0.976 0.805 0.941 0.640\nVisionTransformer (baseline) 0.824 0.8330.985 0.888 0.880 0.5960.968 0.770 0.970 0.787 0.934 0.620\nOrthoTransformer 0.770 0.744 0.982 0.860 0.856 0.557 0.968 0.763 0.973 0.785 0.946 0.635\nCompoundTransformer0.859 0.846 0.9850.870 0.841 0.544 0.975 0.789 0.978 0.819 0.943 0.647\nTable 3: Performance analysis using AUC and ACC on each test dataset of MedMNIST of our quantum architectures\n(Orthogonal PatchWise, Orthogonal Transformer and Compound Transformer) compared to the classical (Vision\nTransformer [9]) and quantum (Orthogonal FNN [5]) baselines described in Section 4.\ngard to the number of trainable parameters used\nby each architecture. Table 5 presents a resource\nanalysis for the quantum circuits that were simu-\nlated per layer. E.g. the Compound Transformer\nrequires 80 trainable parameters compared to the\n512 (2d2) required by the Classical Vision Trans-\nformer. Note that this resource analysis focuses\non the attention layer of each transformer net-\nwork, and does not include parameters used for\npre-processing, other parts found in the trans-\nformer layer, nor the single layer used in the fi-\nnal classification (Fig.1), which are common to\nall simulated methods.\nOverall, our quantum transformers have\nreached comparable levels of accuracy compared\nto the classical equivalent transformers, while us-\ning a smaller number of trainable parameters,\nproviding confirmation of our theoretical predic-\ntions on a small scale. Circuit depth and num-\nber of distinct circuits used for each of the quan-\ntum transformers are also listed in Table 5 to\nmatch the theoretical resource analysis in Table\n2. While the quantum transformers do have theo-\nretical guarantee on the asymptotic run time for\nthe attention mechanism compared to the clas-\nsical transformer, this effect is hard to observe\ngiven the small data size. Summary of the hard-\nware experiments listed in Table 4 shows very\ncompetitive levels of accuracy from the quan-\ntum transformers in comparison with the clas-\nsical benchmarks. Details to be found in C.3.\n5 Conclusion\nIn this work, three different quantum trans-\nformers are presented: Orthogonal Patchwise\nTransformer implements trivial attention mecha-\nnism; Orthogonal Transformer closely mimic the\nclassical transformers; Compound Transformer\nsteps away from the classical architecture with\na quantum-native linear algebraic operation that\ncannot be efficiently done classically: multipli-\ncation of a vector with a higher-dimensional\ncompound matrix. Inside all these quantum\ntransformers are the quantum orthogonal layers,\nwhich efficiently apply matrix multiplication on\nvectors encoded on specific quantum basis states.\nAll circuits implementing orthogonal matrix mul-\ntiplication can be trained using backpropagation\ndetailed in [5].\nAs shown in Table 2, the proposed quantum\ncircuits offer a potential computational advan-\ntage in reducing the complexity of attention lay-\ners. This opens the possibility that quantum\ntransformers may be able to match the perfor-\nmance of their classical counterparts, requiring\nfewer resources in terms of runtime and param-\neter count. On the other hand, while these ini-\ntial results are promising, they are derived from\na limited set of experiments and primarily of-\nfer a theoretical viewpoint. Practical realization\nof such advantages in quantum machine learning\nis heavily contingent upon future advancements\nin quantum hardware, for example in managing\nquantum noise, improving clock speed, and other\ncritical factors. Therefore, these findings should\nbe regarded as a promising yet preliminary step,\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 10\nModel Classical (JAX) IBM Simulator IBM Hardware\nAUC ACC AUC ACC AUC ACC\nGoogle AutoML (Best in [36]) 0.750 53.10 % - - - -\nVisionTransformer (classical benchmark) 0.736 55.75 % - - - -\nOrthoPatchWise (Pyramid Circuit) 0.738 56.50 % 0.731 54.75 % 0.727 51.75 %\nOrtho Transformer (Pyramid Circuit) 0.729 55.00 % 0.715 55.00 % 0.717 54.50 %\nOrtho Transformer with Quantum Attention 0.749 56.50 % 0.743 55.50 % 0.746 55.00 %\nCompoundTransformer (X Circuit) 0.729 56.50 % 0.683 56.50 % 0.666 45.75 %\nCompoundTransformer (\\Circuit) 0.716 55.75 % 0.718 55.50 % 0.704 49.00 %\nTable 4: Hardware Results for RetinaMNIST using various models. Classical (JAX): classical code run by JAX,\nequivalent to quantum operations. IBM Simulator: code compiled to run on actual IBM hardware and executed\nusing their Aer Simulator. Note that “ \\ Circuit” contains a single diagonal of trainable RBS gates. Details of the\nexperiment are written in C.3.2.\nModel Qubits\nNumber of Gates\nfor Loaders\n(Fixed Parameters)\nNumber of Gates\nper Orthogonal Layer\n(Trainable Parameters)\nCircuit Depth Number of\nDistinct Circuits\nOrthogonal PatchWise16 15 32 9 16\nOrthogonal Transformer16 45 64 9 & 13 272\nCompound Transformer32 480 80 150 1\nTable 5: Resource analysis on a single attention layer used for the MedMNIST simulations (Section 4.1). From\nTable 2, it can be derived that the classical transformer requires 512 trainable parameters. Note that the Orthogonal\nTransformer is using two different types of circuits per layer.\nnecessitating further empirical validation using\nfuture quantum hardware.\nIn addition to theoretical analysis, we per-\nformed extensive numerical simulations and\nquantum hardware experiments, which shows\nthat our quantum circuits can classify the small\nMedMNIST images just as well as or at times\nbetter than the state-of-the-art classical methods\n(Table 3) while using fewer parameter, thereby\nshowing potential of these quantum models to ad-\ndress over-fitting issues by using a smaller num-\nber of parameters.\nWhile the run time of the quantum fully con-\nnected layer and the quantum attention mech-\nanism has been theoretically proven to be ad-\nvantageous, this effect is hard to observe on the\ncurrent quantum computers due to their limited\nsize, high level of noise, and latency of cloud ac-\ncess. From our hardware experiments, it can be\nobserved that results from the current hardware\nbecome too noisy as soon as the number of qubits\nor the size of the quantum circuit increase.\nOverall, our results are encouraging and con-\nfirm the benefit of using trainable quantum cir-\ncuits to perform efficient linear algebra opera-\ntions. By carefully designing the quantum circuit\nto allow for much better control over the size of\nthe Hilbert space that is explored by the model,\nwe are able to provide models that are both ex-\npressive and trainable.\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 11\nReferences\n[1] Jacob Biamonte, Peter Wittek, Nicola Pan-\ncotti, Patrick Rebentrost, Nathan Wiebe,\nand Seth Lloyd. “Quantum machine learn-\ning”. Nature 549, 195–202 (2017).\n[2] Iris Cong, Soonwon Choi, and Mikhail D\nLukin. “Quantum convolutional neural\nnetworks”. Nature Physics 15, 1273–\n1278 (2019).\n[3] Kishor Bharti, Alba Cervera-Lierta, Thi Ha\nKyaw, Tobias Haug, Sumner Alperin-Lea,\nAbhinav Anand, Matthias Degroote, Her-\nmanni Heimonen, Jakob S Kottmann, Tim\nMenke, et al. “Noisy intermediate-scale\nquantum algorithms”. Reviews of Modern\nPhysics 94, 015004 (2022).\n[4] Marco Cerezo, Andrew Arrasmith, Ryan\nBabbush, Simon C Benjamin, Suguru Endo,\nKeisuke Fujii, Jarrod R McClean, Kosuke\nMitarai, Xiao Yuan, Lukasz Cincio, et al.\n“Variational quantum algorithms”. Nature\nReviews Physics 3, 625–644 (2021).\n[5] Jonas Landman, Natansh Mathur,\nYun Yvonna Li, Martin Strahm, Skan-\nder Kazdaghli, Anupam Prakash, and\nIordanis Kerenidis. “Quantum methods\nfor neural networks and application to\nmedical image classification”. Quantum 6,\n881 (2022).\n[6] Bobak Kiani, Randall Balestriero, Yann Le-\nCun, and Seth Lloyd. “projunn: Efficient\nmethod for training deep networks with uni-\ntary matrices”. Advances in Neural In-\nformation Processing Systems 35, 14448–\n14463 (2022).\n[7] Ashish Vaswani, Noam Shazeer, Niki Par-\nmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez,  Lukasz Kaiser, and Illia Polo-\nsukhin. “Attention is all you need”. Ad-\nvances in neural information processing sys-\ntems30 (2017).\n[8] Jacob Devlin, Ming-Wei Chang, Kenton\nLee, and Kristina Toutanova. “Bert: Pre-\ntraining of deep bidirectional transformers\nfor language understanding” (2018).\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexan-\nder Kolesnikov, Dirk Weissenborn, Xiao-\nhua Zhai, Thomas Unterthiner, Mostafa De-\nhghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil\nHoulsby. “An image is worth 16x16\nwords: Transformers for image recogni-\ntion at scale”. International Conference on\nLearning Representations (2021). url: open-\nreview.net/forum?id=YicbFdNTTy.\n[10] Yi Tay, Mostafa Dehghani, Dara Bahri,\nand Donald Metzler. “Efficient transform-\ners: A survey”. ACM Computing Surveys\n(CSUR) (2020).\n[11] Dzmitry Bahdanau, Kyunghyun Cho, and\nYoshua Bengio. “Neural Machine Transla-\ntion by Jointly Learning to Align and Trans-\nlate” (2016). arXiv:1409.0473 [cs, stat].\n[12] J. Schmidhuber. “Reducing the Ratio Be-\ntween Learning Complexity and Number of\nTime Varying Variables in Fully Recurrent\nNets”. In Stan Gielen and Bert Kappen,\neditors, ICANN ’93. Pages 460–463. Lon-\ndon (1993). Springer.\n[13] J¨ urgen Schmidhuber. “Learning to Con-\ntrol Fast-Weight Memories: An Alternative\nto Dynamic Recurrent Networks”. Neural\nComputation 4, 131–139 (1992).\n[14] Peter Cha, Paul Ginsparg, Felix Wu, Juan\nCarrasquilla, Peter L McMahon, and Eun-\nAh Kim. “Attention-based quantum tomog-\nraphy”. Machine Learning: Science and\nTechnology 3, 01LT01 (2021).\n[15] Riccardo Di Sipio, Jia-Hong Huang, Samuel\nYen-Chi Chen, Stefano Mangini, and Mar-\ncel Worring. “The dawn of quantum nat-\nural language processing”. In ICASSP\n2022-2022 IEEE International Conference\non Acoustics, Speech and Signal Processing\n(ICASSP). Pages 8612–8616. IEEE (2022).\n[16] Guangxi Li, Xuanqiang Zhao, and Xin\nWang. “Quantum self-attention neural net-\nworks for text classification” (2022).\n[17] Fabio Sanches, Sean Weinberg, Takanori\nIde, and Kazumitsu Kamiya. “Short quan-\ntum circuits in reinforcement learning poli-\ncies for the vehicle routing problem”. Phys-\nical Review A 105, 062403 (2022).\n[18] YuanFu Yang and Min Sun. “Semicon-\nductor defect detection by hybrid classical-\nquantum deep learning”. CVPRPages 2313–\n2322 (2022).\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 12\n[19] Maxwell Henderson, Samriddhi Shakya,\nShashindra Pradhan, and Tristan Cook.\n“Quanvolutional neural networks: power-\ning image recognition with quantum cir-\ncuits”. Quantum Machine Intelligence 2, 1–\n9 (2020).\n[20] Edward Farhi and Hartmut Neven. “Clas-\nsification with quantum neural net-\nworks on near term processors” (2018).\nurl: doi.org/10.48550/arXiv.1802.06002.\n[21] Kosuke Mitarai, Makoto Negoro, Masahiro\nKitagawa, and Keisuke Fujii. “Quantum\ncircuit learning”. Physical Review A 98,\n032309 (2018).\n[22] Kui Jia, Shuai Li, Yuxin Wen, Tongliang\nLiu, and Dacheng Tao. “Orthogonal\ndeep neural networks”. IEEE transactions\non pattern analysis and machine intelli-\ngence (2019).\n[23] Roger A Horn and Charles R Johnson. “Ma-\ntrix analysis”. Cambridge university press.\n(2012).\n[24] Iordanis Kerenidis and Anupam Prakash.\n“Quantum machine learning with subspace\nstates” (2022).\n[25] Brooks Foxen, Charles Neill, Andrew\nDunsworth, Pedram Roushan, Ben Chiaro,\nAnthony Megrant, Julian Kelly, Zijun Chen,\nKevin Satzinger, Rami Barends, et al.\n“Demonstrating a continuous set of two-\nqubit gates for near-term quantum algo-\nrithms”. Physical Review Letters 125,\n120504 (2020).\n[26] Sonika Johri, Shantanu Debnath, Avinash\nMocherla, Alexandros Singk, Anupam\nPrakash, Jungsang Kim, and Iordanis\nKerenidis. “Nearest centroid classification\non a trapped ion quantum computer”. npj\nQuantum Information 7, 122 (2021).\n[27] James W Cooley and John W Tukey. “An al-\ngorithm for the machine calculation of com-\nplex fourier series”. Mathematics of compu-\ntation 19, 297–301 (1965).\n[28] Li Jing, Yichen Shen, Tena Dubcek, John\nPeurifoy, Scott A. Skirlo, Yann LeCun, Max\nTegmark, and Marin Soljacic. “Tunable\nefficient unitary neural networks (eunn) and\ntheir application to rnns”. In International\nConference on Machine Learning. (2016).\nurl: api.semanticscholar.org/CorpusID:5287947.\n[29] L´ eo Monbroussou, Jonas Landman, Alex B.\nGrilo, Romain Kukla, and Elham Kashefi.\n“Trainability and expressivity of hamming-\nweight preserving quantum circuits for ma-\nchine learning” (2023). arXiv:2309.15547.\n[30] Enrico Fontana, Dylan Herman, Shou-\nvanik Chakrabarti, Niraj Kumar, Romina\nYalovetzky, Jamie Heredge, Shree Hari\nSureshbabu, and Marco Pistoia. “The ad-\njoint is all you need: Characterizing bar-\nren plateaus in quantum ans¨ atze” (2023).\narXiv:2309.07902.\n[31] Michael Ragone, Bojko N. Bakalov, Fr´ ed´ eric\nSauvage, Alexander F. Kemper, Carlos Or-\ntiz Marrero, Martin Larocca, and M. Cerezo.\n“A unified theory of barren plateaus for deep\nparametrized quantum circuits” (2023).\narXiv:2309.09342.\n[32] Xuchen You and Xiaodi Wu. “Exponen-\ntially many local minima in quantum neu-\nral networks”. In International Conference\non Machine Learning. Pages 12144–12155.\nPMLR (2021).\n[33] Eric R. Anschuetz and Bobak Toussi\nKiani. “Quantum variational algorithms are\nswamped with traps”. Nature Communica-\ntions13 (2022).\n[34] Ilya O. Tolstikhin, Neil Houlsby, Alexan-\nder Kolesnikov, Lucas Beyer, Xiaohua Zhai,\nThomas Unterthiner, Jessica Yung, Daniel\nKeysers, Jakob Uszkoreit, Mario Lucic, and\nAlexey Dosovitskiy. “Mlp-mixer: An all-mlp\narchitecture for vision”. In NeurIPS. (2021).\n[35] Jiancheng Yang, Rui Shi, and Bingbing\nNi. “Medmnist classification decathlon: A\nlightweight automl benchmark for medical\nimage analysis” (2020).\n[36] Jiancheng Yang, Rui Shi, Donglai Wei, Ze-\nquan Liu, Lin Zhao, Bilian Ke, Hanspeter\nPfister, and Bingbing Ni. “Medmnist v2-a\nlarge-scale lightweight benchmark for 2d and\n3d biomedical image classification”. Scien-\ntific Data 10, 41 (2023).\n[37] Angelos Katharopoulos, Apoorv Vyas,\nNikolaos Pappas, and Fran¸ cois Fleuret.\n“Transformers are rnns: Fast autoregressive\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 13\ntransformers with linear attention”. In In-\nternational Conference on Machine Learn-\ning. Pages 5156–5165. PMLR (2020).\n[38] James Bradbury, Roy Frostig, Peter\nHawkins, Matthew James Johnson, Chris\nLeary, Dougal Maclaurin, George Necula,\nAdam Paszke, Jake VanderPlas, Skye\nWanderman-Milne, and Qiao Zhang.\n“JAX: composable transformations of\nPython+NumPy programs”. Github (2018).\nurl: http://github.com/google/jax.\n[39] Diederik P. Kingma and Jimmy Ba. “Adam:\nA method for stochastic optimization”.\nCoRRabs/1412.6980 (2015).\n[40] Hyeonwoo Noh, Tackgeun You, Jonghwan\nMun, and Bohyung Han. “Regularizing deep\nneural networks by noise: Its interpretation\nand optimization”. NeurIPS (2017).\n[41] Xue Ying. “An overview of overfitting and\nits solutions”. In Journal of Physics: Con-\nference Series. Volume 1168, page 022022.\nIOP Publishing (2019).\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 14\nA Vision Transformers\nHere, the details of a classical Vision Transform-\ners introduced by [9] are outlined. Some slight\nchanges in the architecture have been made to\nease the correspondence with quantum circuits.\nWe also introduce important notations that will\nbe reused in the quantum methods.\nThe transformer network starts by decompos-\ning an image into patches and pre-processing the\nset of patches to map each one into a vector, as\nshown in Fig.2. The initial set of patches is en-\nhanced with an extra vector of the same size as\nthe patches, called class embedding. This class\nembedding vector is used at the end of the net-\nwork, to feed into a fully connected layer that\nyields the output (see Fig.1). We also include\none trainable vector called positional embedding,\nwhich is added to each vector. At the end of this\npre-processing step, we obtain the set of n vec-\ntors of dimension d, denoted xi to be used in the\nnext steps.\nNext, feature extraction is performed using a\ntransformer layer [7, 9] which is repeatedLtimes,\nas shown in Fig.3. Within the transformer layer,\nwe first apply layer normalisation over all patches\nxi, and then apply the attention mechanism de-\ntailed in Fig.4. After this part, we obtain a state\nto which we add the initial input vectors be-\nfore normalisation in an operation called residual\nlayer, represented by the blue arrow in Fig.3, fol-\nlowed by another layer normalisation. After this,\nwe apply a Multi Layer Perceptron (MLP), which\nconsists of multiple fully connected linear layers\nfor each vector that result in same-sized vectors.\nAgain, we add the residual from just before the\nlast layer normalisation, which is the output of\none transformer layer.\nAfter repeating the transformer layer Ltimes,\nwe finally take the vector corresponding to the\nclass embedding, that is the vector correspond-\ning to x0, in the final output and apply a fully\nconnected layer of dimension ( d × number of\nclasses) to provide the final classification result\n(see Fig.1). It is important to observe here that\nwe only use the first vector outcome in the fi-\nnal fully connected layer to do the classification\n(therefore the name class embedding).\nLooking inside the attention mechanism (see\nFig.4), we start by using a fully connected lin-\near layer with trainable weights V to calculate\nfor each patch xi the feature vector Vxi. Then\nto calculate the attention coefficients, we use\nanother trainable weight matrix W and define\nthe attention given by patch xi to patch xj as\nxT\ni Wxj. Next, for each patch xi, we get the fi-\nnal extracted features as the weighted sum of all\nfeature vectors Vxj where the weights are the\nattention coefficients. This is equivalent to per-\nforming a matrix multiplication with a matrix\nA defined by Aij = xT\ni Wxj. Note, in classical\ntransformer architecture, a column-wise softmax\nis applied to all Aij and attention coefficients\nA′\nij = softmaxj(Aij) is used instead. Overall,\nthe attention mechanism makes use of 2d2 train-\nable parameters, evenly divided between V and\nW, each of size d×d.\nIn fact, the above description is a slight vari-\nant from the original transformers proposed in\n[7], where the authors used two trainable matri-\nces to obtain the attention coefficients instead of\none (W) in this work. This choice was made to\nsimplify the quantum implementation but could\nbe extended to the original proposal using the\nsame quantum tools.\nComputational complexity of classical atten-\ntion mechanism depends mainly on the number of\npatches n and their individual dimension d: the\nfirst patch-wise matrix multiplication with the\nmatrix V ∈Rd×d takes O(nd2) steps, while the\nsubsequent multiplication with the large matrix\nA′takes O(n2d). Obtaining A′from W requires\nO(nd2) steps as well. Overall, the complexity is\nO(nd2 + n2d). In classical deep learning litera-\nture, the emphasis is made on the second term,\nwhich is usually the most costly. Note that a re-\ncent proposal [37] proposes a different attention\nmechanism as a linear operation that only has a\nO(nd2) computational complexity.\nWe compare the classical computational com-\nplexity with those of our quantum methods in\nTable 2. These running times have an real impact\non both training and inference, as they measure\nhow the time to perform each layer scales with\nthe number and dimension of the patches.\nB Quantum Tools (Extended)\nB.1 Quantum Data Loaders for Matrices\nIn order to perform a machine learning task with\na quantum computer, classical data (a vector, a\nmatrix) needs to be loaded into the quantum cir-\ncuit. The technique we choose for this task is\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 15\ncalled amplitude encoding, which uses the classi-\ncal scalar component of the data as amplitudes\nof a quantum state made of d qubits. In partic-\nular we build upon previous methods to define\nquantum data loaders for matrices, as shown in\nFig.5.\n[26] proposes three different circuits to load a\nvector x ∈Rd using d−1 gates for a circuit depth\nranging from O(log(d)) to O(d) as desired (see\nFig.14). These data loaders use the unary ampli-\ntude encoding, where a vector x = (x1,··· ,xd)\nis loaded in the quantum state:\n|x⟩= 1\n∥x∥\nd∑\ni=1\nxi |ei⟩,\nwhere |ei⟩is the quantum state with all qubits\nin 0 except the ith one in state 1 (e.g.\n|0 ···010 ···0⟩). The circuit uses RBS gates: a\nparametrised two-qubit gate given by Eq.1.\nFigure 14: Three possible data loaders for d-dimensional\nvectors (d= 8). From left to right: the parallel, diago-\nnal, and semi-diagonal circuit have respectively a circuit\ndepth of log(d), d, and d/2. The X gate represent the\nPauli X gate, and the vertical lines represent RBS gates\nwith tunable parameters.\nThe d−1 parameters θi of the RBS gates are\nclassically pre-computed to ensure that the out-\nput of the circuit is indeed |x⟩.\nWe require a data loader for matrices. Given a\nmatrix X ∈Rn×d, instead of loading a flattened\nvector, rows Xi are loaded in superposition. As\nshown in Fig.5, on the top qubit register, we first\nload the vector (∥x1∥,··· ,∥xn∥) made of the\nnorms of each row, using a data loader for a vec-\ntor and obtain a state 1\n∥X∥\n∑n\ni=1 ∥xi∥|ei⟩. Then,\non a lower register, we are sequentially loading\neach row Xi ∈Rd. To do so, we use vector data\nloaders and their adjoint, as well as CNOTs con-\ntrolled on the ith qubit of the top register. The\nresulting state is a superposition of the form:\n|X⟩= 1\n∥X∥\nn∑\ni=1\nd∑\nj=1\nXij |ej⟩|ei⟩\nOne immediate application of data loaders that\nconstruct amplitude encodings is the ability to\nperform fast inner product computation with\nquantum circuits. Applying the inverse data\nloader of xi after the regular data loader of xj ef-\nfectively creates a state of the form ⟨xi,xj⟩|e1⟩+\n|G⟩where |G⟩is a garbage state. The probability\nof measuring |e1⟩, which is simply the probabil-\nity of having a 1 on the first qubit, is |⟨xi,xj⟩|2.\nTechniques to retrieve the sign of the inner prod-\nuct have been developed in [5].\nB.2 Quantum Orthogonal Layers\nIn this section, we outline the concept of quan-\ntum orthogonal layers used in neural networks,\nwhich generalises the work in [5]. These layers\ncorrespond to parametrised circuits of N qubits\nmade of RBS gates. More generally, RBS gates\npreserve the number of ones and zeros in any ba-\nsis state: if the input to a quantum orthogonal\nlayer is a vector in unary amplitude encoding,\nthe output will be another vector in unary am-\nplitude encoding. Similarly, if the input quan-\ntum state is a superposition of only basis states\nof hamming weight 2, so is the output quantum\nstate. This output state is precisely the result of\na matrix-vector product, where the matrix is the\nunitary matrix of the quantum orthogonal layer,\nrestricted to the basis used. Therefore, for unary\nbasis, we consider a N×N matrix W instead of\nthe full 2N ×2N unitary. Similarly for the basis of\nhamming weight two, we can restrict the unitary\nto a\n(N\n2\n)\n×\n(N\n2\n)\nmatrix. Since the reduced matrix\nconserves its unitary property and has only real\nvalues, these are orthogonal matrices. More gen-\nerally, we can think of such hamming weight pre-\nserving circuits with N qubits as block-diagonal\nunitaries that act separately on N+ 1 subspaces,\nwhere the k-th subspace is defined by all compu-\ntational basis states with hamming weight equal\nto k. The dimension of these subspaces is equal\nto\n(N\nk\n)\n.\nThere exist many possibilities for building a\nquantum orthogonal layer, each with different\nproperties. The Pyramid circuit, proposed in [5],\nis composed of exactly N(N −1)/2 RBS gates.\nThis circuit requires only adjacent qubit connec-\ntivity, which is the case for most superconduct-\ning qubit hardware. More precisely, the set of\nmatrices that are equivalent to the quantum or-\nthogonal layers with pyramidal layout is exactly\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 16\nthe Special Orthogonal Group, made of orthogo-\nnal matrices with determinant equal to +1. We\nhave showed that by adding a final Z gate on the\nlast qubit would allow having orthogonal matri-\nces with −1 determinant. The pyramid circuit is\ntherefore very general and cover all the possible\northogonal matrices of size N ×N.\nThe two new types of quantum orthogonal lay-\ners we have introduced are the butterfly circuit\n(Fig.8), and the X circuit (Fig.9) (Section 2.2).\nThere exists a method [5] to compute the gra-\ndient of each parameter θi in order to update\nthem. This backpropagation method for the\npyramid circuit takes time O(N2), correspond-\ning to the number of gates, and provided a poly-\nnomial improvement in run time compared to\nthe previously known orthogonal neural network\ntraining algorithms [22]. The exact same method\ndeveloped for the pyramid circuit can be used to\nperform quantum backpropagation on the new\ncircuits introduced in this paper. The run time\nalso corresponds to the number of gates, which is\nlower for the butterfly and X circuits. See Table\n1 for full details on the comparison between the\nthree types of circuits. In particular, when con-\nsidering the butterfly layer, the complexity of the\nbackpropagation method transitions from O(N2)\nto O(Nlog N).\nC Medical Image Classification via\nQuantum Transformers (Extended)\nC.1 Datasets\nIn order to benchmark our models, we used\nMedMNIST, a collection of 12 pre-processed,\ntwo-dimensional medical image open datasets\n[35, 36]. The collection has been standardised\nfor classification tasks on 12 different imaging\nmodalities, each with medical images of 28 ×28\npixels. All three quantum transformers and two\nbenchmark methods were trained and validated\non all 12 MedMNIST datasets. For the hardware\nexperiments, we focused on one dataset, Reti-\nnaMNIST. The MedMNIST dataset was chosen\nfor our benchmarking efforts due to its accessi-\nble size for simulations of the quantum circuits\nand hardware experiments, while being represen-\ntative of one important field of computer vision\napplication: classification of medical images.\nC.2 Simulations\nFirst, simulations of our models are performed\non the 2D MedMNIST datasets and demonstrate\nthat the proposed quantum attention architec-\nture reaches accuracy comparable to and at times\nbetter than the various standard classical models.\nNext, the setting of our simulations are described\nand the results compared against those reported\nin the AutoML benchmark performed by the au-\nthors in [36].\nC.2.1 Simulation setting MedMNIST\nThe JAX package [38] was used to efficiently sim-\nulate the complete training procedure of the five\nbenchmark architectures. The experimental hy-\nperparameters used in [36] were replicated for\nour benchmark: every model is trained using the\ncross-entropy loss with the Adam optimiser [39]\nfor 100 epochs, with batch size of 32 and a learn-\ning rate of 10−3 that is decayed by a factor of 0.1\nafter 50 and 75 epochs.\nThe 5 different neural networks were trained\nover 3 random seeds, and the best overall perfor-\nmance for each one of them was selected. The\nevaluation procedure is similar to the AutoML\nbenchmark in [35, 36], and the benchmark re-\nsults are shown in Table 3 where the area un-\nder receiver operating characteristic (ROC) curve\n(AUC) and the accuracy (ACC) are reported as\nevaluation metrics. A full comparison with the\nclassical benchmark provided by [35] is given in\n(Appendix D, Table 6).\nC.2.2 Simulation results MedMNIST\nFrom Table 3, we observe that Quantum Orthog-\nonal and Compound Transformer architectures\noutperform the Orthogonal Fully-Connected and\nOrthogonal Patch-wise neural networks most of\nthe time. This may be due to the fact that the\nlatter do not rely on any mechanism that ex-\nchange information across the patches. Second,\nall quantum neural networks provide very com-\npetitive performances compared to the AutoML\nbenchmark and outperform their classical coun-\nterparts on 7 out of 12 MedMNIST datasets.\nMoreover, comparisons can be made with re-\ngard to the number of parameters used by each\narchitecture, in particular for feature extraction.\nTable 5 presents a resource analysis for the quan-\ntum circuits that were simulated, per layer. It\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 17\nincludes the number of qubits, the number of\ngates with trainable parameters, and the num-\nber of gates with fixed parameters used for load-\ning the data. The table shows that our quantum\narchitectures have a small number of trainable\nparameters per layer. The global count for each\nquantum method is as follows.\n• Orthogonal Patch-wise Neural Network: 32\nparameters per circuit, 16 circuits per layer\nwhich use the same 16 parameters, and 4\nlayers, for a total of 128 trainable parame-\nters.\n• Quantum Orthogonal Transformer: 32 pa-\nrameters per circuit, 17 circuits which use\nthe same 16 parameters and another 289 cir-\ncuits which use another set of 16 parameters\nper layer, and 4 layers, for a total of 256\ntrainable parameters.\n• Compound Transformer: 80 parameters per\ncircuit, 1 circuit per layer, and 4 layers, for\na total of 320 trainable parameters.\nThese numbers are to be compared with the num-\nber of trainable parameters in the classical Vi-\nsion Transformer that is used as a baseline. As\nstated in Section A, each classical attention layer\nrequires 2d2 trainable parameters, which in the\nsimulations performed here corresponds to 512.\nNote again this resource analysis focuses on the\nattention layer of the each transformer network,\nand does not include parameters used for the\npreprocessing of the images (see Section C.2.1),\nas part of other transformer layers (Fig.3), and\nfor the single layer used in the final classification\n(Fig.1), which are common in all cases.\nMore generally, performance of other classical\nneural network models provided by the authors\nof MedMNIST is compared to our approaches in\nTable 6 found in the Appendix. Some of these\nclassical neural networks reach somewhat bet-\nter levels of accuracy, but are known to use an\nextremely large number of parameters. For in-\nstance, the smallest reported residual network\nhas approximately a total number of 107 param-\neters, and the automated machine learning algo-\nrithms train numerous different architectures in\norder to reach that performance.\nBased on the results of the simulations in this\nsection, quantum transformers are able to train\nacross a number different of classification tasks,\ndeliver performances that are highly competitive\nand sometimes better than the equivalent classi-\ncal methods.\nC.3 Quantum Hardware Experiments\nQuantum hardware experiments were performed\non one specific dataset: RetinaMNIST. It has\n1080 images for training, 120 images for valida-\ntion, and 400 images for testing. Each image con-\ntains 28×28 RGB pixels. Each image is classified\ninto 1 of 5 classes (ordinal regression).\nC.3.1 Hardware Description\nThe hardware demonstration was performed on\ntwo different superconducting quantum comput-\ners provided by IBM, with the smaller experi-\nments performed on the 16-qubit ibmq guadalupe\nmachine (see Fig.15) and the larger ones on the\n27-qubit ibm hanoi machine. Results are re-\nported here from experiments with four, five and\nsix qubits; experiments with higher numbers of\nqubits, which entails higher numbers of gates and\ndepth, did not produce meaningful results.\nFigure 15: Connectivity of the 16-qubit ibmq guadalupe\nquantum computer.\nFigure 16: Connectivity of the 27-qubit ibm hanoi quan-\ntum computer.\nNote that the main sources of noise are the de-\nvice noise and the finite sampling noise. In gen-\neral, noise is undesirable during computations.\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 18\nIn the case of a neural network, however, noise\nmay not be as troublesome: noise can help es-\ncape local minima [40], or act as data augmenta-\ntion to avoid over-fitting. In classical deep learn-\ning, noise is sometimes artificially added for these\npurposes [41]. Despite this, when the noise is too\nlarge, we also see a drop in the accuracy.\nC.3.2 Hardware Results\nHardware experiments were performed with four,\nfive and six qubits to push the limits of the\ncurrent hardware, in terms of both the num-\nber of qubits and circuit depth. Three quantum\nproposals were run: the Orthogonal Patch-wise\nnetwork (from Section 3.1), the Quantum Or-\nthogonal transformers (from Sections 3 and 3.3)\nand finally the Quantum Compound Transformer\n(from Section 3.4).\nEach quantum model was trained using a JAX-\nbased simulator, and inference was performed on\nthe entire test dataset of 400 images of the Reti-\nnaMNIST on the IBM quantum computers. Re-\ngarding the experimental setting on real hard-\nware, the number of shots for the compound\nsetup using 6 qubits was maximized to 32.000.\nFor other configurations using 4 qubits, 10.000\nshots were used.\nThe first model, the Orthogonal Patch-wise\nneural network, was trained using 16 patches per\nimage, 4 features per patch, and one 4 ×4 or-\nthogonal layer, using a 4-qubit pyramid as the or-\nthogonal layer. The experiment used 16 different\nquantum circuits of 9 RBS gates per circuit per\nimage. The result was compared with an equiva-\nlent classical (non-orthogonal) patch-wise neural\nnetwork, and a small advantage in accuracy for\nthe quantum native method could be reported.\nThe second model, the Quantum Orthogonal\nTransformer, used4 patches per image, 4 features\nper patch, and an attention mechanism with one\n4 ×4 orthogonal layer and trainable attention\ncoefficients. 4-qubit pyramids were used as or-\nthogonal layers. The experiment used 25 differ-\nent quantum circuits of 12 RBS gates per circuit\nper image and 15 different quantum circuits of 9\nRBS gates per circuit per image.\nThe third set of experiments ran the Orthog-\nonal Transformer with the quantum attention\nmechanism. We used 4 patches per image, 4 fea-\ntures per patch, and a quantum attention mecha-\nnism that paid attention to only the neighbouring\npatch, thereby using a 5-qubit quantum circuit\nwith the X as the orthogonal layer. The exper-\niment used 12 different quantum circuits of 14\nRBS gates and 2 CNOT s per circuit per image.\nThe last two quantum proposals were com-\npared with a classical transformer network with\na similar architecture and demonstrated similar\nlevel of accuracy.\nFinally, the fourth experiment was performed\non the ibmq hanoi machine with 6 qubits, with\nthe Compound Transformer, using 4 patches per\nimage, 4 features per patch, and one orthogonal\nlayer using the X layout. The hardware results\nwere quite noisy with the X layer, therefore the\nsame experiments were performed with a further-\nreduced orthogonal layer named the “ \\Circuit”:\nhalf of a X Circuit (Fig.9) where only one diago-\nnal of RBS gates is kept, and which reduced the\nnoise in the outcomes. The experiment used 2\ndifferent quantum circuits of 18 RBS gates and 3\nCNOT s per circuit per image.\nNote that with the restriction to states with\na fixed hamming weight, strong error mitigation\ntechniques become available. Indeed, as we ex-\npect to obtain only quantum superpositions of\nunary states or states with hamming weight 2\nin the case of Compound Transformers, at ev-\nery layer, every measurement can be processed\nto discard the ones that have a different ham-\nming weight i.e. states with more than one (or\ntwo) qubit in state|1⟩. This error mitigation pro-\ncedure can be applied efficiently to the results of\na hardware demonstration, and has been used in\nthe results presented in this paper.\nThe conclusion from the hardware experiments\nis that all quantum proposals achieve state-\nof-the-art test accuracy, comparable to classi-\ncal networks. Looking at the simulation ex-\nperiments (details found in Table 3), the com-\npound transformer occasionally achieves superior\nperformance compared to classical transformer.\nNote that achieving such a compound implemen-\ntation in a classical setting incurs a polynomial\noverhead.\nD Extended Performance Analysis\nWe add our results to the already existing results\non the MedMNIST [36] datasets in the Table 6\nbelow.\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 19\nNetwork PathMNIST ChestMNIST DermaMNIST OCTMNIST PneumoniaMNIST RetinaMNIST\nAUC ACC AUC ACC AUC ACC AUC ACC AUC ACC AUC ACC\nResNet-18 (28) 0.983 0.907 0.768 0.947 0.917 0.735 0.943 0.743 0.944 0.854 0.717 0.524\nResNet-18 (224) 0.989 0.909 0.773 0.947 0.920 0.754 0.958 0.763 0.956 0.864 0.710 0.493\nResNet-50 (28) 0.990 0.911 0.769 0.947 0.913 0.735 0.952 0.762 0.948 0.854 0.726 0.528\nResNet-50 (224) 0.989 0.892 0.773 0.948 0.912 0.731 0.958 0.776 0.962 0.884 0.716 0.511\nauto-sklearn 0.934 0.716 0.649 0.779 0.902 0.719 0.887 0.601 0.942 0.855 0.690 0.515\nauto-keras 0.959 0.834 0.742 0.937 0.915 0.749 0.955 0.763 0.947 0.878 0.719 0.503\nauto-ml 0.944 0.728 0.914 0.948 0.914 0.768 0.963 0.771 0.991 0.946 0.750 0.531\nVisionTransformer 0.957 0.755 0.718 0.947 0.895 0.727 0.923 0.830 0.957 0.902 0.749 0.562\nOrthoFNN 0.939 0.643 0.701 0.947 0.883 0.719 0.819 0.516 0.950 0.864 0.731 0.548\nOrthoPatchWise 0.953 0.713 0.692 0.947 0.898 0.730 0.861 0.554 0.945 0.867 0.739 0.560\nOrthoTransformer 0.964 0.774 0.703 0.947 0.891 0.719 0.875 0.606 0.947 0.885 0.745 0.542\nCompoundTransformer 0.957 0.735 0.698 0.947 0.901 0.734 0.867 0.545 0.947 0.885 0.740 0.565\nNetwork BreastMNIST BloodMNIST TissueMNIST OrganAMNIST OrganCMNIST OrganSMNIST\nAUC ACC AUC ACC AUC ACC AUC ACC AUC ACC AUC ACC\nResNet-18 (28) 0.901 0.863 0.998 0.958 0.930 0.676 0.997 0.935 0.992 0.900 0.972 0.782\nResNet-18 (224) 0.891 0.833 0.998 0.963 0.933 0.681 0.998 0.951 0.994 0.920 0.974 0.778\nResNet-50 (28) 0.857 0.812 0.997 0.956 0.931 0.680 0.997 0.935 0.992 0.905 0.972 0.770\nResNet-50 (224) 0.866 0.842 0.997 0.950 0.932 0.680 0.998 0.947 0.993 0.911 0.975 0.785\nauto-sklearn 0.836 0.803 0.984 0.878 0.828 0.532 0.963 0.762 0.976 0.829 0.945 0.672\nauto-keras 0.871 0.831 0.998 0.961 0.941 0.703 0.994 0.905 0.990 0.879 0.974 0.813\nauto-ml 0.919 0.861 0.998 0.966 0.924 0.673 0.990 0.886 0.988 0.877 0.964 0.749\nVisionTransformer 0.824 0.833 0.985 0.888 0.880 0.596 0.968 0.770 0.970 0.787 0.934 0.620\nOrthoFNN 0.815 0.821 0.972 0.820 0.819 0.513 0.916 0.636 0.923 0.672 0.875 0.481\nOrthoPatchWise 0.830 0.827 0.984 0.866 0.845 0.549 0.973 0.786 0.976 0.805 0.941 0.640\nOrthoTransformer 0.770 0.744 0.982 0.860 0.856 0.557 0.968 0.763 0.973 0.785 0.946 0.635\nCompoundTransformer 0.859 0.846 0.985 0.870 0.841 0.544 0.975 0.789 0.978 0.819 0.943 0.647\nTable 6: Extended Performance Analysis in terms of AUC and ACC on each test dataset of MedMNIST. The numbers\n(28) and (224) next to ResNet denote the input image resolutions of 28x28 and 224x224, respectively, with the latter\nbeing a larger version of ResNet with more parameters. These benchmarks are based on the results reported in [36].\nAccepted in Quantum 2024-10-02, click title to verify. Published under CC-BY 4.0. 20",
  "topic": "Quantum",
  "concepts": [
    {
      "name": "Quantum",
      "score": 0.5623292922973633
    },
    {
      "name": "Computer science",
      "score": 0.5146644711494446
    },
    {
      "name": "Qubit",
      "score": 0.47511014342308044
    },
    {
      "name": "Quantum network",
      "score": 0.4719133675098419
    },
    {
      "name": "Quantum state",
      "score": 0.45265430212020874
    },
    {
      "name": "Quantum information",
      "score": 0.43397584557533264
    },
    {
      "name": "Transformer",
      "score": 0.43261271715164185
    },
    {
      "name": "Electronic engineering",
      "score": 0.3653120696544647
    },
    {
      "name": "Topology (electrical circuits)",
      "score": 0.3209189772605896
    },
    {
      "name": "Quantum mechanics",
      "score": 0.24409475922584534
    },
    {
      "name": "Physics",
      "score": 0.24135613441467285
    },
    {
      "name": "Electrical engineering",
      "score": 0.17198681831359863
    },
    {
      "name": "Engineering",
      "score": 0.14148065447807312
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 7
}