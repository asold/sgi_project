{
    "title": "Continual knowledge infusion into pre-trained biomedical language models",
    "url": "https://openalex.org/W3201023666",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2771623331",
            "name": "Kishlay Jha",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A2150231934",
            "name": "Aidong Zhang",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A2771623331",
            "name": "Kishlay Jha",
            "affiliations": [
                "University of Virginia"
            ]
        },
        {
            "id": "https://openalex.org/A2150231934",
            "name": "Aidong Zhang",
            "affiliations": [
                "University of Virginia"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2136437513",
        "https://openalex.org/W3112340563",
        "https://openalex.org/W2149151184",
        "https://openalex.org/W2150838562",
        "https://openalex.org/W6781031682",
        "https://openalex.org/W6770565238",
        "https://openalex.org/W6758479261",
        "https://openalex.org/W6764513019",
        "https://openalex.org/W3126891849",
        "https://openalex.org/W6797000528",
        "https://openalex.org/W6761724051",
        "https://openalex.org/W6675257112",
        "https://openalex.org/W2560647685",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W6767002648",
        "https://openalex.org/W2473930607",
        "https://openalex.org/W6724881955",
        "https://openalex.org/W6741087337",
        "https://openalex.org/W3005970824",
        "https://openalex.org/W2917968119",
        "https://openalex.org/W2563185281",
        "https://openalex.org/W6785309951",
        "https://openalex.org/W6682691769",
        "https://openalex.org/W6742071618",
        "https://openalex.org/W6780237715",
        "https://openalex.org/W2808838104",
        "https://openalex.org/W1997057722",
        "https://openalex.org/W2788388592",
        "https://openalex.org/W6691431627",
        "https://openalex.org/W6767038488",
        "https://openalex.org/W6681188176",
        "https://openalex.org/W2895296143",
        "https://openalex.org/W2154142897",
        "https://openalex.org/W6784807622",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6762867720",
        "https://openalex.org/W6779433498",
        "https://openalex.org/W2920882081",
        "https://openalex.org/W2944400536",
        "https://openalex.org/W6762503438",
        "https://openalex.org/W6684408796",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W2991612931",
        "https://openalex.org/W3035021780",
        "https://openalex.org/W2908082115",
        "https://openalex.org/W3170316419",
        "https://openalex.org/W2964242047",
        "https://openalex.org/W2970986510",
        "https://openalex.org/W2250539671",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W2951538941",
        "https://openalex.org/W2509884321",
        "https://openalex.org/W3035763680",
        "https://openalex.org/W4295883599",
        "https://openalex.org/W2102343050",
        "https://openalex.org/W3098749216",
        "https://openalex.org/W2954929116",
        "https://openalex.org/W2740815822",
        "https://openalex.org/W3098115739",
        "https://openalex.org/W3023802502",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2143448637",
        "https://openalex.org/W3035153870"
    ],
    "abstract": "Abstract Motivation Biomedical language models produce meaningful concept representations that are useful for a variety of biomedical natural language processing (bioNLP) applications such as named entity recognition, relationship extraction and question answering. Recent research trends have shown that the contextualized language models (e.g. BioBERT, BioELMo) possess tremendous representational power and are able to achieve impressive accuracy gains. However, these models are still unable to learn high-quality representations for concepts with low context information (i.e. rare words). Infusing the complementary information from knowledge-bases (KBs) is likely to be helpful when the corpus-specific information is insufficient to learn robust representations. Moreover, as the biomedical domain contains numerous KBs, it is imperative to develop approaches that can integrate the KBs in a continual fashion. Results We propose a new representation learning approach that progressively fuses the semantic information from multiple KBs into the pretrained biomedical language models. Since most of the KBs in the biomedical domain are expressed as parent-child hierarchies, we choose to model the hierarchical KBs and propose a new knowledge modeling strategy that encodes their topological properties at a granular level. Moreover, the proposed continual learning technique efficiently updates the concepts representations to accommodate the new knowledge while preserving the memory efficiency of contextualized language models. Altogether, the proposed approach generates knowledge-powered embeddings with high fidelity and learning efficiency. Extensive experiments conducted on bioNLP tasks validate the efficacy of the proposed approach and demonstrates its capability in generating robust concept representations.",
    "full_text": null
}