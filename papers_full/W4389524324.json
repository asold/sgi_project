{
  "title": "H2O Open Ecosystem for State-of-the-art Large Language Models",
  "url": "https://openalex.org/W4389524324",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2620053861",
      "name": "Arno Candel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5114058806",
      "name": "Jon McKinney",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096235221",
      "name": "Philipp Singer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2327865855",
      "name": "Pascal Pfeiffer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2507895475",
      "name": "Maximilian Jeblick",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2623869786",
      "name": "Chun Ming Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3202668518",
      "name": "Marcos Conde",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4379468930",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W4380993076",
    "https://openalex.org/W4287760320",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4380993527",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4378509449",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4366341216",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287674181"
  ],
  "abstract": "Arno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer, Maximilian Jeblick, Chun Ming Lee, Marcos Conde. Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. 2023.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 82–89\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nH2O Open Ecosystem for State-of-the-art Large Language Models\nArno Candel, Jon McKinney, Philipp Singer, Pascal Pfeiffer,\nMaximilian Jeblick, Chun Ming Lee, Marcos V . Conde\nH2O.ai, Inc.\nMountain View, CA\n{firstname.surname}@h2o.ai\nhttps://gpt.h2o.ai/\nAbstract\nLarge Language Models (LLMs) represent a\nrevolution in AI. However, they also pose many\nsignificant risks, such as the presence of biased,\nprivate, copyrighted or harmful text. For this\nreason we need open, transparent and safe so-\nlutions. We introduce a complete open-source\necosystem for developing and testing LLMs.\nThe goal of this project is to boost open alterna-\ntives to closed-source approaches. We release\nh2oGPT, a family of fine-tuned LLMs from 7 to\n70 Billion parameters. We also introduce H2O\nLLM Studio, a framework and no-code GUI\ndesigned for efficient fine-tuning, evaluation,\nand deployment of LLMs using the most recent\nstate-of-the-art techniques. Our code and mod-\nels are licensed under fully permissive Apache\n2.0 licenses. We believe open-source language\nmodels help to boost AI development and make\nit more accessible and trustworthy.\n1 Introduction\nSince the Transformer (Vaswani et al., 2017) was\nintroduced in the Natural Language Processing\n(NLP) community, the advances in this field have\nincreased exponentially (Wolf et al., 2020).\nStarting from popular models such as BERT (De-\nvlin et al., 2018a) or Generative Pre-trained Trans-\nformers (GPT) (Radford et al., 2018) -both intro-\nduced in 2018-, researchers have been pushing the\nlimits of scaling and learned representations in lan-\nguage models (Liu et al., 2019; Radford et al., 2019;\nBrown et al., 2020; Chowdhery et al., 2022).\nRecent advances in Large Language Models\n(LLMs) are all over the news; these models rep-\nresent a revolution in Artificial Intelligence (AI)\ndue to their real-world applications through natural\nlanguage processing (NLP), from internet chatbots\nto virtual assistants and programmers. However,\nthese also pose significant risks and challenges.\nThe most popular models (e.g., chatGPT (OpenAI,\n2023)) are proprietary and not truly open-source,\neither transparent regarding their training data.\nFigure 1: Evolution of our project in GitHub. Our tools\nhave been widely adopted by the NLP community. See\nhttps://github.com/h2oai/h2ogpt.\nThis fast advance leads to a wide range of practi-\ncal challenges that must be addressed in order for\nthese models to be widely utilized and explored.\nThe popularity and demand of LLMs call for sys-\ntems to train, fine-tune, evaluate, scale, and deploy\nthe models on a variety of platforms. Given the\ntraining costs (millions of dollars), practitioners\nincreasingly rely on pre-trained general-purpose\nLLMs and fine-tune them for specific downstream\ntasks and datasets. This requires a wide catalogue\nof open-source pre-trained LLMs, and sophisti-\ncated procedures and tools for efficient fine-tuning.\nMoreover, considering the massive size of these\nmodels (usually from 7 to 100 Billion parameters),\nwe also need compression techniques to deploy\nthem successfully on different platforms.\nWe believe open-source language models help\nto boost AI development and make it more ac-\ncessible and trustworthy. They lower entry hur-\ndles, allowing people to tailor these models to their\nneeds. This openness increases innovation, trans-\nparency, and fairness. As part of this effort, we\nintroduce two open-source libraries: h2oGPT\nand H2O LLM Studio, for LLMs development, in-\ncluding Multi LLM deployment and evaluation —\nwidely adopted in the NLP community (see Fig. 1).\n82\nh2oGPT (https://github.com/h2oai/h2ogpt)\nis a library dedicated to supporting open-source\nLLMs research, and facilitating their integration\nwhile ensuring privacy and transparency. Most in-\ntegrated models are designed for both research and\nproduction. The main use-case of this library is to\ndeploy and test efficiently a wide variety of LLMs\non private databases and documents. This tool al-\nlows users to compare different models on several\ntasks and datasets concurrently. An example of this\napplication is https://gpt.h2o.ai/.\nH2O LLM Studio (https://github.com/\nh2oai/h2o-llmstudio) complements the previ-\nous library, and allows users to efficiently fine-tune\nany LLM using the most recent state-of-the-art\ntechniques such as LoRA adapters (Hu et al.,\n2021), reinforcement learning (RLHF), and 4-bit\ntraining. After fine-tuning (or training), the\nmodels can be easily exported and deployed at\nthe Hugging Face Hub 1. Moreover, the library\nincludes a graphic user interface (GUI) specially\ndesigned for large language models.\nh2oGPT and H2O LLM Studioare an ongoing ef-\nfort maintained frequently by the team of engineers\nand researchers at H2O.ai with exciting support\nfrom the open-source NLP community and external\ncontributors. Both are released under the Apache\n2.0 license 2. Tutorials and detailed documentation\nare available at the corresponding websites and the\ntechnical report (Candel et al., 2023).\n2 Related Work\nLarge language models (LLMs) are designed to\nprocess and understand vast amounts of natural\nlanguage data e.g., internet questions, text in doc-\numents, financial data, textbook material, etc. As\nfoundation models (Bommasani et al., 2021), these\nare trained from broad data at scale (Howard and\nRuder, 2018), and can be adapted (ie. fine-tuned)\nto a wide range of down-stream tasks (Wang et al.,\n2018; Lewis et al., 2019).\nThey are built on the Transformer neural net-\nwork architecture (Vaswani et al., 2017), which\nallows them to capture complex language patterns\nand relationships. Derived from the Transformer,\nwe find BERT-like models (Devlin et al., 2018b;\nLe et al., 2020; Liu et al., 2019) focused on pre-\ntraining with bidirectional encoders. We also find\n1https://huggingface.co/models\n2https://www.apache.org/licenses/LICENSE-2.0\nthe popular Generative Pre-trained Transformers\n(GPTs) (Radford et al., 2018, 2019; Brown et al.,\n2020; OpenAI, 2023), focused on generative pre-\ntraining. These serve as the engine of chatGPT.\nSince 2022, we experience a new revolution in\nNLP with the rise of LLMs (over billion parame-\nters models). These models usually follow a multi-\nstage training strategy, starting with a task-agnostic\npre-training on large and diverse datasets. Some\nrelated LLMs are LLaMA (Touvron et al., 2023a),\nGPT-NeoX (Black et al., 2022), BLOOM (Scao\net al., 2022), Palm (Chowdhery et al., 2022),\nOPT (Zhang et al., 2022), and GPT-4 (OpenAI,\n2023). We also explore community models such as\nFalcon (Penedo et al.), Alpaca (Taori et al., 2023),\nand OpenAssistant (Köpf et al., 2023).\n2.1 Why Open-Source LLMs?\nWhile commercially hosted and centralized LLMs\nlike ChatGPT -based on GPT-4 (OpenAI, 2023)-,\nMicrosoft’s Bing AI Chat, and Google’s Bard are\npowerful and effective, they have certain risks and\nlimitations compared to open-source LLMs:\n• Data Privacy and Security: Many require\nsending data to external servers. This can\nraise concerns about data privacy, security,\nand compliance, especially for sensitive infor-\nmation or industries with strict regulations.\n• Dependency and Customization: We want\nto allow users to train LLMs on private data\nsafely, and customize the models to their spe-\ncific needs and applications. Moreover the\nusers can deploy them on their own infrastruc-\nture, and even modify the underlying code.\n• Traceability and Transparency: To under-\nstand the risky behaviours of LLMs (e.g., hal-\nlucinations, biases, private information etc.),\nand ensure their safe and trustworthy use, it is\nfundamental to analyze the dataset and train-\ning strategies used to produce such model.\n• Carbon footprint: Users tend to adopt our\nopen state-of-the-art models, instead of run-\nning expensive and complicated experiments\n(in most cases to replicate results). Therefore,\nwe aim to reduce the overall carbon footprint\n(ie. GPU hours consumption) by providing\nhigh-quality models and tools.\nOverall, open-source LLMs offer greater flexibil-\nity, control, and cost-effectiveness, while address-\ning data privacy and security concerns.\n83\nPrivate fine-tuned LLM\nDatasets\nLLM Models\nLLaMA, Falcon, etc\nLLM weights\nEvaluation\nEasy Deploy\nRLHF, LoRA\nQLoRA, 4bit\nQuery and summarize documents, chat and code locally and privately\nFigure 2: Open LLM Ecosystem. (left) The user does not need to transfer private data to 3rd parties, and can select\nany popular LLM e.g., LLaMA, Falcon. (mid) H2O LLM Studio allows to train and fine-tune any language model\nusing state-of-the-art techniques and a GUI without coding. (right) The models can be easily evaluated, exported\nand deployed. More information at https://github.com/h2oai/h2o-llmstudio. Apache 2 License.\n3 H2O LLM Studio\nAn open-source framework for efficient fine-tuning\nLLMs without coding, using a graphic user inter-\nface (GUI) specially designed for large language\nmodels 3. This is illustrated in Figures 2 and 4.\nWe use the most popular adapters for fast fine-\ntuning such as Low-Rank Adaptation (LoRA) (Hu\net al., 2021) and QLoRA (Dettmers et al., 2023), as\nwell as 8-bit (up to 4-bit) model training with a low\nmemory footprint, and the corresponding quanti-\nzation. This allows to fine-tune small LLMs in\nregular GPUs, even using Google Colab or Kaggle.\nFor example < 10B models (e.g., LlaMa-2 7B) can\nbe fine-tuned in a single NVIDIA-T4 (16GB).\nWe also integrate Reinforcement Learning from\nHuman Feedback (RLHF) (Ouyang et al., 2022;\nStiennon et al., 2020). This feature is inspired in\nTRL 4 (von Werra et al., 2020), with the Proximal\nPolicy Optimisation (PPO) by (Ziegler et al., 2019).\n3https://github.com/h2oai/h2o-llmstudio\n4https://github.com/lvwerra/trl\nLLM Studio allows complete customization of\nthe experimental setup: dataset, state-of-the-art\nmodel selection, optimizer, learning rate schedule,\ntokenizer, sequence length (number of tokens), low-\nrank adapter, validation set and metrics, etc.\nThe users can track several simultaneous ex-\nperiments, and easily export the logs and results.\nMoreover, the models can be easily exported to the\nHugging Face Hub, to be shared with the commu-\nnity or deploy locally and privately.\nThe framework supports any open-source lan-\nguage model, we here highlight the most popular\nstate-of-the-art large models: GPT-NeoX (Black\net al., 2022), Falcon (Penedo et al.), LLaMa and\nLlama 2 (Touvron et al., 2023b), Vicuna (Chiang\net al., 2023), WizardLM (Xu et al., 2023; Luo\net al., 2023), h2oGPT (Candel et al., 2023), and\nMPT (MosaicML, 2023). We summarize these\nmodels in Table 1. Most models are trained on\na large amount of data (over 1T tokens), they can\nhandle extremely long inputs (large context length),\nand are licensed for commercial use.\n84\nModel Size (B)\nLlama 2 (Touvron et al., 2023b) 7 / 13 / 70\nCodeLlama (Touvron et al., 2023b) 34\nFalcon (Penedo et al.) 7 / 40 / 180\nMistral AI (Mistral AI, 2023) 7\nGPT-NeoX (Black et al., 2022) 20\nWizardLM (Xu et al., 2023) 7 / 13 / 70\nVicuna (Chiang et al., 2023) 13\nMPT (MosaicML, 2023) 7 / 30\nh2oGPT (Candel et al., 2023) 7 to 70\nGPT-3.5 (by OpenAI) ?\nTable 1: Most popular pre-trained LLMs for fine-tuning.\nWe report the size in Billions (B) of parameters.\nWe acknowledge other existing toolssuch as\nLLMTune (Kuleshov, 2023) and EasyLM (Geng,\n2023). However, these do not include as many fea-\ntures as LLM Studio (e.g., GUI, supported models\nand techniques, etc), their licenses can be less per-\nmissive. Our tools are amongst the most adopted\nLLM-related software in GitHub (considering stars\nand forks by July 2023) — see Fig. 1.\n4 Multi LLM Deployment and Evaluation\nAny model produced from LLM Studio can be eas-\nily integrated into HuggingFace’s space & models.\nWe refer to our own space for more information\nand access to our models 5.\nIn Fig. 3 (top) we show a snapshot of our demo\nh2oGPT https://gpt.h2o.ai/. We deploy mul-\ntiple state-of-the-art LLM models including Falcon\n(7/40B), Llama 2 (7/13/70B), and GPT-3.5. This\nallows us to compare different models and setups.\nThe user’s prompt is evaluated by the different\nLLMs concurrently. We can see the answer gener-\nation progress for each model, at the same time. Us-\ning this software we can identify clear differences\nbetween LLMs easily, for example fast/low infer-\nence, hallucinations, common response patterns,\nbias, memorized data etc. Also, we can analyze\nthe effect of prompt engineeringon the different\nmodels and expose vulnerabilities. The users can\ndeploy the models on a wide variety of inference\nservers (HF TGI server, vLLM, Gradio, OpenAI),\nand evaluate performance using reward models.\nDocument Analysis h2oGPT also allows to\nquery and summarize documents in many formats\n(e.g., PDFs, Word, Code, Text, MarkDown, etc).\n5https://huggingface.co/h2oai\nWe implement an efficient use of context using\ninstruct-tuned LLMs (no need for LangChain).\nNote that this ecosystem can be reproduced\nlocally, to analyze the models in a private and\nsafe manner.We also provide a OpenAI-compliant\nPython client API for client-server control.\nGuides & Material We provide a shortVideo tu-\ntorial (2 mins), and a complete video overview of\nthe ecosystem (16 min, 340K views) on YouTube.\nAlso a step-by-step tutorial Make Your Own\nGPT With h2oGPT & H2O LLM Studio(1hr).\nWe also host all of our models in HF: https:\n//huggingface.co/h2oai. We refer the reader to\nour GitHubs for more demos, and documentation.\n5 Future Work\nOur open-source LLM Ecosystem is in constant de-\nvelopment, h2oGPT and LLM Studioare updated\nbased on the most recent research advances and\ndemands. We plan to integrate new model quan-\ntization techniques, distillation and long-context\ntraining (context length over 100K tokens).\nWe also plan to support more multi-lingual mod-\nels, and multi-modal models.\n6 Limitations\nDatasets Fine-tuning requires data text pairs of\ninstruction and expected result/answer.\nBiases and OffensivenessLLMs are trained on a\ndiverse range of unfiltered internet text data, which\nmay contain biased, racist, offensive, or otherwise\ninappropriate content. Therefore, the generated\ncontent by these models may sometimes exhibit\nbiases or produce content that is offensive or inap-\npropriate. We do not endorse, support, or promote\nany such content or viewpoints.\nUsage The large language model is an AI-based\ntool and not a human. It may produce incorrect,\noffensive, nonsensical, or irrelevant responses. It\nis the user’s responsibility to critically evaluate the\ngenerated content and use it at their discretion.\nCarbon footprint Training LLMs is expensive\nand their use is associated to tons of CO 2 emis-\nsions (Touvron et al., 2023a).\nHallucinations LLMs are probabilistic, therefore,\ncertain “random\" behaviour is natural and expected,\nespecially on complex prompts (e.g., logical para-\ndoxes, reasoning problems, etc) and “unknown con-\ntent\" not present in the training corpus.\n85\n1\nInput prompt. The users clicks on submit and the multiple LLMs will start to interact. \nYou can also save the prompt, stop execution, etc.\nMultiple LLM evaluation. This visualization-evaluation allows the user to detect clear \ndifferences  between the models for example, inference speed and clear hallucinations.\n2\n1\n2\n3\n3 Expert mode. Users can change the temperature, cumulative probabilities (top p),\ncontext (top k tokens), maximum output length, maximum runtime, etc.\nFigure 3: h2oGPT. Evaluation of multiple state-of-the-art LLM models using the same prompt. This visualization\nand evaluation allows the user to detect clear differences between the models e.g. faster or slower inference, clear\nhallucinations, common memorized patterns. Demo available at https://gpt.h2o.ai/ completely free.\n86\nComplete LLM Framework. Users can track all the experiments and the system's status. The software\nallows complete customization  of the experimental setup: dataset and model selection, validation and\nmetrics, optimizer, adapters, RLHF, bit precision, etc.\nAdvanced Settings. Users can use state-of-the-art  techniques to speed up training and obtain real-time\nperformance metrics. Also we allow Tokenizer and context customization.\nFigure 4: LLM Studioallows efficient training and fine-tuning of LLMs using state-of-the-art techniques (e.g.,\nadvanced models, LoRA, int4, RLHF), and an intuitive GUI with complete experiment’s customization. More\ninformation in https://github.com/h2oai/h2o-llmstudio. Apache 2 License.\n87\nBroad Impact\nWe advocate for the use of open-source LLMs to\naccelerate AI development and enhance its trans-\nparency, accessibility, security, and reliability. Our\nopen framework for training, fine-tuning, deploy-\nment and analysis of LLMs enables this to any user,\nin a private and safe manner. We provide a detailed\nDisclaimer for users of our software.\nReferences\nSid Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nArno Candel, Jon McKinney, Philipp Singer, Pas-\ncal Pfeiffer, Maximilian Jeblick, Prithvi Prabhu,\nJeff Gambera, Mark Landry, Shivam Bansal, Ryan\nChesler, et al. 2023. h2ogpt: Democratizing large\nlanguage models. arXiv preprint arXiv:2306.08161.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and\nLuke Zettlemoyer. 2023. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018a. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018b. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nXinyang Geng. 2023. Easylm: A simple and scalable\ntraining framework for large language models.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model fine-tuning for text classification. In\nACL.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685.\nAndreas Köpf, Yannic Kilcher, Dimitri von Rütte,\nSotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens,\nAbdullah Barhoum, Nguyen Minh Duc, Oliver Stan-\nley, Richárd Nagyfi, et al. 2023. Openassistant\nconversations–democratizing large language model\nalignment. arXiv preprint arXiv:2304.07327.\nV olodymyr Kuleshov. 2023. Llmtune: Fine-tuning large\nlanguage models on one consumer gpu. https://\ngithub.com/kuleshov-group/llmtune.\nHang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoît Crabbé, Laurent Besacier, and Didier\nSchwab. 2020. Flaubert: Unsupervised language\nmodel pre-training for french. In Proceedings of The\n12th Language Resources and Evaluation Confer-\nence, pages 2479–2490, Marseille, France. European\nLanguage Resources Association.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVes Stoyanov, and Luke Zettlemoyer. 2019. BART:\nDenoising Sequence-to-Sequence pre-training for\nnatural language generation, translation, and com-\nprehension.\n88\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-\nubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma,\nQingwei Lin, and Daxin Jiang. 2023. Wizardcoder:\nEmpowering code large language models with evol-\ninstruct.\nMistral AI. 2023. Mistral 7b introduction. https://\nmistral.ai/news/announcing-mistral-7b/.\nMosaicML. 2023. Mpt-30b: Raising the bar for open-\nsource foundation models.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Baptiste\nPannier, Ebtesam Almazrouei, and Julien Launay.\nThe refinedweb dataset for falcon llm: Outperform-\ning curated corpora with web data, and web data\nonly.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul F Christiano. 2020. Learn-\ning to summarize with human feedback. Advances\nin Neural Information Processing Systems, 33:3008–\n3021.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł Ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I Guyon, U V Luxburg, S Bengio,\nH Wallach, R Fergus, S Vishwanathan, and R Garnett,\neditors, Advances in Neural Information Processing\nSystems 30, pages 5998–6008. Curran Associates,\nInc.\nLeandro von Werra, Younes Belkada, Lewis Tunstall,\nEdward Beeching, Tristan Thrush, and Nathan Lam-\nbert. 2020. Trl: Transformer reinforcement learning.\nhttps://github.com/lvwerra/trl.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In ICLR.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2020. Transformers: State-of-the-art natural\nlanguage processing. In Proceedings of the 2020 con-\nference on empirical methods in natural language\nprocessing: system demonstrations, pages 38–45.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. Wizardlm: Empowering large lan-\nguage models to follow complex instructions. arXiv\npreprint arXiv:2304.12244.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\nguage models from human preferences. arXiv\npreprint arXiv:1909.08593.\n89",
  "topic": "Pascal (unit)",
  "concepts": [
    {
      "name": "Pascal (unit)",
      "score": 0.734682559967041
    },
    {
      "name": "Computer science",
      "score": 0.5404902100563049
    },
    {
      "name": "Art",
      "score": 0.33459943532943726
    },
    {
      "name": "Art history",
      "score": 0.32944273948669434
    },
    {
      "name": "Programming language",
      "score": 0.2493988573551178
    }
  ],
  "institutions": []
}