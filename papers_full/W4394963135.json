{
  "title": "Vision Transformers with Hierarchical Attention",
  "url": "https://openalex.org/W4394963135",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1977837230",
      "name": "Yun Liu",
      "affiliations": [
        "Institute for Infocomm Research",
        "Agency for Science, Technology and Research"
      ]
    },
    {
      "id": "https://openalex.org/A2405529015",
      "name": "Yu-Huan Wu",
      "affiliations": [
        "Institute of High Performance Computing",
        "Agency for Science, Technology and Research"
      ]
    },
    {
      "id": "https://openalex.org/A2315723189",
      "name": "Guolei Sun",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2104362018",
      "name": "Le Zhang",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2310006641",
      "name": "Ajad Chhatkuli",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2103920730",
      "name": "Luc Van Gool",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A1977837230",
      "name": "Yun Liu",
      "affiliations": [
        "Agency for Science, Technology and Research",
        "Institute for Infocomm Research"
      ]
    },
    {
      "id": "https://openalex.org/A2405529015",
      "name": "Yu-Huan Wu",
      "affiliations": [
        "Institute of High Performance Computing",
        "Agency for Science, Technology and Research"
      ]
    },
    {
      "id": "https://openalex.org/A2315723189",
      "name": "Guolei Sun",
      "affiliations": [
        "Vision Australia"
      ]
    },
    {
      "id": "https://openalex.org/A2104362018",
      "name": "Le Zhang",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2310006641",
      "name": "Ajad Chhatkuli",
      "affiliations": [
        "Vision Australia"
      ]
    },
    {
      "id": "https://openalex.org/A2103920730",
      "name": "Luc Van Gool",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2560622558",
    "https://openalex.org/W3216325077",
    "https://openalex.org/W3127842933",
    "https://openalex.org/W3112885960",
    "https://openalex.org/W3034922525",
    "https://openalex.org/W3085685449",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W4214636423",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4214614183",
    "https://openalex.org/W6846577953",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W2549139847",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W2883780447",
    "https://openalex.org/W2963918968",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W2550553598",
    "https://openalex.org/W2963495494",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W6753038380",
    "https://openalex.org/W2922509574",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W4214634256",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W6795140394",
    "https://openalex.org/W6796417832",
    "https://openalex.org/W4226297238",
    "https://openalex.org/W4285787147",
    "https://openalex.org/W6790690058",
    "https://openalex.org/W3136416617",
    "https://openalex.org/W2962834855",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W3157528469",
    "https://openalex.org/W3175515048",
    "https://openalex.org/W2910628332",
    "https://openalex.org/W2737258237",
    "https://openalex.org/W2963351448",
    "https://openalex.org/W4231059779",
    "https://openalex.org/W3139633126",
    "https://openalex.org/W4226363321"
  ],
  "abstract": "Abstract This paper tackles the high computational/space complexity associated with multi-head self-attention (MHSA) in vanilla vision transformers. To this end, we propose hierarchical MHSA (H-MHSA), a novel approach that computes sell-attention in a hierarchical fashion. Specifically, we first divide the input image into patches as commonly done, and each patch is viewed as a token. Then, the proposed H-MHSA learns token relationships within local patches, serving as local relationship modeling. Then, the small patches are merged into larger ones, and H-MHSA models the global dependencies for the small number of the merged tokens. At last, the local and global attentive features are aggregated to obtain features with powerful representation capacity. Since we only calculate attention for a limited number of tokens at each step, the computational load is reduced dramatically. Hence, H-MHSA can efficiently model global relationships among tokens without sacrificing fine-grained information. With the H-MHSA module incorporated, we build a family of hierarchical-attention-based transformer networks, namely HAT-Net. To demonstrate the superiority of HAT-Net in scene understanding, we conduct extensive experiments on fundamental vision tasks, including image classification, semantic segmentation, object detection and instance segmentation. Therefore, HAT-Net provides a new perspective for vision transformers. Code and pretrained models are available at https://github.com/yun-liu/HAT-Net .",
  "full_text": " \nVision Transformers with Hierarchical Attention\nYun Liu 1          Yu-Huan Wu 2          Guolei Sun 3          Le Zhang 4\nAjad Chhatkuli 3          Luc Van Gool 3\n1 Institute for Infocomm Research (I2R), A*STAR, Singapore 138632, Singapore\n2 Institute of High Performance Computing (IHPC), A*STAR, Singapore 138632, Singapore\n3 Computer Vision Lab, ETH Zürich, Zürich 8092, Switzerland\n4 School of Information and Communication Engineering, University of Electronic Science and\nTechnology of China (UESTC), Chengdu 611731, China\n \nAbstract:   This paper tackles the high computational/space complexity associated with multi-head self-attention (MHSA) in vanilla\nvision transformers. To this end, we propose hierarchical MHSA (H-MHSA), a novel approach that computes self-attention in a hier-\narchical fashion. Specifically, we first divide the input image into patches as commonly done, and each patch is viewed as a token. Then,\nthe proposed H-MHSA learns token relationships within local patches, serving as local relationship modeling. Then, the small patches\nare merged into larger ones, and H-MHSA models the global dependencies for the small number of the merged tokens. At last, the local\nand global attentive features are aggregated to obtain features with powerful representation capacity. Since we only calculate attention\nfor a limited number of tokens at each step, the computational load is reduced dramatically. Hence, H-MHSA can efficiently model glob-\nal relationships among tokens without sacrificing fine-grained information. With the H-MHSA module incorporated, we build a family of\nhierarchical-attention-based transformer networks, namely HAT-Net. To demonstrate the superiority of HAT-Net in scene understand-\ning, we conduct extensive experiments on fundamental vision tasks, including image classification, semantic segmentation, object detec-\ntion and instance segmentation. Therefore, HAT-Net provides a new perspective for vision transformers. Code and pretrained models\nare available at https://github.com/yun-liu/HAT-Net.\nKeywords:   Vision transformer, hierarchical attention, global attention, local attention, scene understanding.\nCitation:   Y. Liu, Y. H. Wu, G. Sun, L. Zhang, A. Chhatkuli, L. V. Gool. Vision transformers with hierarchical attention. Machine\nIntelligence Research, vol.21, no.4, pp.670–683, 2024. http://doi.org/10.1007/s11633-024-1393-8\n \n \n1   Introduction\nIn  the  last  decade,  convolutional  neural  networks\n(CNNs) have been the go-to architecture in computer vis-\nion, owing to their powerful capability in learning repres-\nentations from images/videos[1–12]. Meanwhile, in another\nfield  of  natural  language  processing  (NLP),  the  trans-\nformer architecture[13] has been the de-facto standard to\nhandle  long-range  dependencies[14, 15].  Transformers  rely\nheavily on self-attention to model global relationships of\nsequence data. Although global modeling is also essential\nfor  vision  tasks,  the  2D/3D  structures  of  vision  data\nmake it less straightforward to apply transformers ther-\nein. This predicament has been recently broken by Doso-\nvitskiy  et  al.[16],  by  applying  a  pure  transformer  to  se-\nquences of image patches.\nMotivated by [16], a large amount of literature on vis-\nion  transformers  has  emerged  to  resolve  the  problems\ncaused by the domain gap between computer vision and\nNLP[17–21]. From our point of view, one major problem of\nvision transformers is that the sequence length of image\npatches is much longer than that of tokens (words) in an\nNLP  application,  thus  leading  to  high  computational/\nspace complexity when computing the multi-head self-at-\ntention  (MHSA).  Some  efforts  have  been  dedicated  to\nresolving  this  problem.  ToMe[22] improves  the  through-\nput of existing ViT models[16] by systematically merging\nsimilar  tokens  through  the  utilization  of  a  general  and\nlight-weight  matching  algorithm.  Pyramid  vision  trans-\nformer  (PVT)[19] and  multiscale  vision  transformer\n(MViT)[21] downsample the feature to compute attention\nin  a  reduced  length  of  tokens  but  at  the  cost  of  losing\nfine-grained details. Swin transformer[18] computes atten-\ntion  within  small  windows  to  model  local  relationships,\nand  it  gradually  enlarges  the  receptive  field  by  shifting\nwindows  and  stacking  more  layers.  From  this  point  of\nview,  Swin  transformer[18] may  still  be  suboptimal  be-\ncause  it  works  in  a  similar  manner  to  CNNs  and  needs\nmany layers to model long-range dependencies[16].\nBuilding  upon  the  discussed  strengths  of  downsam-\npling-based  transformers[19, 21] and  window-based  trans-\nformers[18],  each  with  its  distinctive  merits,  we  aim  to\n \nResearch Article\nSpecial Issue on Multi-modal Representation Learning\nManuscript received on September 3, 2023; accepted on January 8,\n2024; published online on April 19, 2024\nRecommended by Associate Editor Wanli Ouyang\n \nColored figures are available in the online version at https://link.\nspringer.com/journal/11633\n© The Author(s) 2024\n \nMachine Intelligence Research\nwww.mi-research.net\n21(4), August 2024, 670-683\nDOI: 10.1007/s11633-024-1393-8\n \n\nharness their complementary advantages. Downsampling-\nbased transformers excel at directly modeling global de-\npendencies  but  may  sacrifice  fine-grained  details,  while\nwindow-based  transformers  effectively  capture  local  de-\npendencies but may fall short in global dependency mod-\neling. As widely accepted, both global and local informa-\ntion  is  essential  for  visual  scene  understanding.  Motiv-\nated by this insight, our approach seeks to amalgamate\nthe strengths of both paradigms, enabling the direct mod-\neling of both global and local dependencies.\nTo achieve this, we introduce the hierarchical multi-\nhead  self-attention  (H-MHSA),  a  novel  mechanism  that\nenhances  the  flexibility  and  efficiency  of  self-attention\ncomputation in transformers. Our methodology begins by\nsegmenting  an  image  into  patches,  treating  each  patch\nakin  to  a  token[16].  Rather  than  computing  attention\nacross all patches, we further organize these patches into\nsmall  grids,  performing  attention  computation  within\neach grid. This step is instrumental in capturing local re-\nlationships and generating more discriminative local rep-\nresentations. Subsequently, we amalgamate these smaller\npatches into larger ones and treat the merged patches as\nnew tokens, resulting in a substantial reduction in their\nnumber.  This  enables  the  direct  modeling  of  global  de-\npendencies  by  calculating  self-attention  for  the  new\ntokens. Ultimately, the attentive features from both local\nand global hierarchies are aggregated to yield potent fea-\ntures  with  rich  granularities.  Notably,  as  the  attention\ncalculation at each step is confined to a small number of\ntokens, our hierarchical strategy mitigates the computa-\ntional and space complexity of vanilla transformers. Em-\npirical  observations  underscore  the  efficacy  of  this  hier-\narchical  self-attention  mechanism,  revealing  improved\ngeneralization results in our experiments.\nBy simply incorporating H-MHSA, we build a family\nof  hierarchical-attention-based  transformer  networks\n(HAT-Net). To evaluate the efficacy of HAT-Net in scene\nunderstanding, we experiment HAT-Net for fundamental\nvision tasks, including image classification, semantic seg-\nmentation,  object  detection  and  instance  segmentation.\nExperimental  results  demonstrate  that  HAT-Net  per-\nforms  favorably  against  previous  backbone  networks.\nNote that H-MHSA is based on a very simple and intuit-\nive idea, so H-MHSA is expected to provide a new per-\nspective for the future design of vision transformers. \n2   Related work\nConvolutional  neural  networks.  More  than  two\ndecades  ago,  LeCun  et  al.[23] built  the  first  deep  CNN,\ni.e.,  LeNet,  for  document  recognition.  About  ten  years\nago, AlexNet[1] introduced pooling layers into CNNs and\npushed forward the state of the art of ImageNet classific-\nation[24] significantly. Since then, CNNs have become the\nde-facto standard of computer vision owing to its power-\nful  ability  in  representation  learning.  Brilliant  achieve-\n3 \u0002 3\n3 \u0002 3\nments have been seen in this direction. VGGNet[2] invest-\nigates  networks  of  increasing  depth  using  small  ( )\nconvolution filters. ResNet[3] manages to build very deep\nnetworks  by  resolving  the  gradient  vanishing/exploding\nproblem  with  residual  connections[25].  GoogLeNet[26]\npresents  the  inception  architecture[27, 28] using  multiple\nbranches  with  different  convolution  kernels.  ResNeXt[29]\nimproves ResNet[3] by replacing the  \n convolution in\nthe bottleneck with a grouped convolution. DenseNets[30]\npresent dense connections, i.e., using the feature maps of\nall  preceding  layers  as  inputs  for  each  layer.  Mobile-\nNets[31, 32] decompose  the  traditional  convolution  into  a\npointwise convolution and a depthwise separable convolu-\ntion  for  acceleration,  and  an  inverted  bottleneck  is  pro-\nposed for ensuring accuracy. ShuffleNets[33, 34] further de-\ncompose the pointwise convolution into pointwise group\nconvolution and channel shuffle to reduce computational\ncost.  MansNet[35] proposes  an  automated  mobile  neural\narchitecture search approach to search for a model with a\ngood  trade-off  between  accuracy  and  latency.  Efficient-\nNet[36] introduces  a  scaling  method  to  uniformly  scale\ndepth/width/resolution  dimensions  of  the  architecture\nsearched by MansNet[35]. The above advanced techniques\nare the engines driving the development of computer vis-\nion in the last decade. This paper aims at improving fea-\nture  representation  learning  by  designing  new  trans-\nformers.\nSelf-attention mechanism.  Inspired by the human\nvisual system, the self-attention mechanism is usually ad-\nopted to enhance essential information and suppress noisy\ninformation.  Spatial  transformer  network  (STN)[37] pres-\nents the spatial attention mechanism through learning an\nappropriate  spatial  transformation  for  each  input.  Chen\net al.[38] propose the channel attention model and achieve\npromising  results  on  the  image  captioning  task.  Wang\net al.[39] explore self-attention in well-known residual net-\nworks[3]. SENet[40] applies channel attention to backbone\nnetwork design and boosts the accuracy of ImageNet clas-\nsification[24].  Convolutional  block  attention  module\n(CBAM)[41] sequentially  applies  channel  and  spatial  at-\ntention for adaptive feature refinement in deep networks.\nBottleneck attention module (BAM)[42] produces a 3D at-\ntention map by combining channel and spatial attention.\nSelective kernel network (SK-Net)[43] uses channel atten-\ntion  to  selectively  fuse  multiple  branches  with  different\nkernel  sizes.  Non-local  network[44] presents  non-local  at-\ntention for capturing long-range dependencies. ResNeSt[45]\nis a milestone in this direction. It applies channel atten-\ntion  on  different  network  branches  to  capture  cross-fea-\nture  interactions  and  learn  diverse  representations.  Our\nwork shares some similarities with these works by apply-\ning self-attention for learning feature representations. The\ndifference is that we propose H-MHSA to learn global re-\nlationships rather than a simple feature recalibration us-\ning spatial or channel attention in these works.\nVision  transformer.  Transformer[13] entirely  relies\nY. Liu et al. / Vision Transformers with Hierarchical Attention 671 \n \n\non self-attention to handle long-range dependencies of se-\nquence data. It was first proposed for NLP tasks[14, 15]. In\norder  to  apply  transformers  on  image  data,  Dosovitskiy\net  al.[16] split  an  image  into  patches  and  treat  them  as\ntokens. Then, a pure transformer[13] can be adopted. Such\na  vision  transformer  (ViT)  attains  competitive  accuracy\nfor  ImageNet  classification[24].  More  recently,  lots  of  ef-\nforts have been dedicated to improving ViT. Tokens-to-\ntoken ViT (T2T-ViT)[46] proposes to split an image into\ntokens  of  overlapping  patches  so  as  to  represent  local\nstructures by surrounding tokens. CaiT[47] builds a deep-\ner  transformer  network  by  introducing  a  per-channel\nweighting  and  specific  class  attention.  DeepViT[48] pro-\nposes  Re-attention  to  re-generate  attention  maps  to  in-\ncrease their diversity at different layers. DeiT[49] presents\na knowledge distillation strategy for improving the train-\ning of ViT[16]. Srinivas et al.[50] try to add the bottleneck\nstructure  to  vision  transformers.  Some  works  build  pyr-\namid  transformer  networks  to  generate  multi-scale  fea-\ntures[17–21]. PVT[19] adopts convolution operation to down-\nsample the feature map in order to reduce the sequence\nlength in MHSA, thus reducing the computational load.\nSimilar to PVT[19], MViT[21] utilizes pooling to compute\nattention  on  a  reduced  sequence  length.  Swin  transfor-\nmer[18] computes  attention  within  small  windows  and\nshifts  windows  to  gradually  enlarge  the  receptive  field.\nCoaT[20] computes  attention  in  the  channel  dimension\nrather  than  in  the  traditional  spatial  dimension.  Token\nmerging  (ToMe)[22] enhances  the  throughput  of  existing\nViT  models[16] without  requiring  retraining,  which  is\nachieved  by  gradually  combining  similar  tokens  in  a\ntransformer using a matching algorithm. In this paper, we\nintroduce  a  novel  design  to  reduce  the  computational\ncomplexity of MHSA and learn both the global and local\nrelationship modeling through vision transformers.\nVision  MLP  networks.  While  CNNs  and  vision\ntransformers have been widely adopted for computer vis-\nion  tasks,  Tolstikhin  et  al.[51] challenge  the  necessity  of\nconvolutions  and  attention  mechanisms.  They  introduce\nthe MLP-Mixer architecture, which relies solely on multi-\nlayer  perceptrons  (MLPs).  MLP-Mixer  incorporates  two\ntypes of layers: One applies MLPs independently to im-\nage  patches,  facilitating  the  mixing  of  per-location  fea-\ntures,  and  the  other  applies  MLPs  across  patches,  en-\nabling the mixing of spatial information. Despite lacking\nconvolutions  and  attention,  MLP-Mixer  demonstrates\ncompetitive  performance  in  image  classification  com-\npared  to  state-of-the-art  models.  Liu  et  al.[52] introduce\ngMLP, an MLP-based model with gating, showcasing its\ncomparable  performance  to  transformers  in  crucial  lan-\nguage and vision applications. In contrast to other MLP-\nlike models that encode spatial information along flatte-\nned spatial dimensions, Vision permutator[53] uniquely en-\ncodes feature representations along height and width di-\nmensions using linear projections. Wang et al.[54] propose\na novel positional spatial gating unit, leveraging classical\nrelative  positional  encoding  to  efficiently  capture  cross-\ntoken relations for token mixing. Despite these advance-\nments, the performance of vision MLP networks still lags\nbehind that of vision transformers. In this paper, we fo-\ncus on the design of a new vision transformer network. \n3   Methodology\nIn this section, we first provide a brief review of vis-\nion transformers[16] in Section 3.1. Then, we present the\nproposed  H-MHSA  and  analyze  its  computational  com-\nplexity in Section 3.2. Finally, we describe the configura-\ntion details of the proposed HAT-Net in Section 3.3. \n3.1   Review of vision transformers\nX 2 RH\u0002W\u0002C\nH\nW\nC\nX\nQ\nK\nV\nTransformer[13, 16] heavily  relies  on  MHSA  to  model\nlong-range relationships. Suppose  denotes\nthe input, where ,   and  are the height, width and\nthe  feature  dimension,  respectively.  We  reshape  and\ndefine the query , key  , value  as\nX 2 RH\u0002W\u0002C ! X 2 R(H\u0002W)\u0002C;\nQ = X\nWq; K = XW k; V = XW v (1)\nWq 2 RC\u0002C\nWk 2 RC\u0002C\nWv 2 RC\u0002C\nwhere ,  , and  are\nthe  trainable  weight  matrices  of  linear  transformations.\nWith a mild assumption that the input and output have\nthe  same  dimension,  the  traditional  MHSA  can  be\nformulated as\nA = Softmax(QKT /\np\nd)V (2)\np\nd\nQKT\nin  which  means  an  approximate  normalization,  and\nthe softmax function is applied to the rows of the matrix.\nNote that we omit the concept of multiple heads here for\nsimplicity.  In  (2),  the  matrix  product  of  \n first\ncomputes  the  similarity  between  each  pair  of  tokens.\nEach new token is then derived over the combination of\nall  tokens  according  to  the  similarity.  After  the\ncomputation  of  MHSA,  a  residual  connection  is  further\nadded to ease the optimization, like\nX 2 R(H\u0002W)\u0002C ! X 2 RH\u0002W\u0002C;\nA′ = AWp + X (3)\nWp 2 RC\u0002C\nin  which  is  a  trainable  weight  matrix  for\nfeature  projection.  At  last,  a  multilayer  perceptron\n(MLP) is adopted to enhance the representation, like\nY = MLP(A′)\n+ A′ (4)\nY\nwhere  denotes the output of a transformer block.\nIt is easy to infer that the computational complexity\nof MHSA (Equation (2)) is\n 672 Machine Intelligence Research 21(4), August 2024\n \n\nΩ(MHSA)\n= 3HWC 2 + 2H2W2C: (5)\nO(H2W2)\nO(H2W2)\nSimilarly,  the  space  complexity  (memory  consump-\ntion) also includes the term of .  As commonly\nknown,  could become very large for high-resol-\nution inputs. This limits the applicability of transformers\nfor vision tasks. Motivated by this, we aim at improving\nMHSA to reduce such complexity and maintain the capa-\ncity  of  global  relationship  modeling  without  the  risk  of\nsacrificing performances. \n3.2   Hierarchical multi-head self-attention\nIn  this  section,  we  present  an  approach  to  alleviate\nthe computational and space demands associated with (2)\nthrough the utilization of our proposed H-MHSA mechan-\nism. Rather than computing attention over the entire in-\nput, we adopt a hierarchical strategy, allowing each step\nto process only a limited number of tokens.\nX 2 RH\u0002W\u0002C\nG1 \u0002 G1\nThe initial step concentrates on local attention com-\nputation. Assuming the input feature map is denoted as\n,  we partition the feature map into small\ngrids of size  and reshape it as follows:\nX 2 RH\u0002W\u0002C ! X1 2 R\n( H\nG1\n\u0002G1)\u0002( W\nG1\n\u0002G1)\u0002C\n! X1 2 R\n( H\nG1\n\u0002 W\nG1\n)\u0002(G1\u0002G1)\u0002C\n: (6)\nThe query, key and value are then calculated by\nQ1 = X1Wq\n1 ; K1 = X1Wk\n1 ; V1 = X1Wv\n1 (7)\nWq\n1 ; Wk\n1 ; Wv\n1 2 RC\u0002C\nA1\nA1\nX\nwhere  are trainable weight matri-\nces.  Subsequently,  (2)  is  applied  to  generate  the  local\nattentive feature .\n  To ease network optimization, we\nreshape  back to the shape of  through\nA1 2 R\n( H\nG1\n\u0002 W\nG1\n)\u0002(G1\u0002G1)\u0002C\n!A1 2 R\n( H\nG1\n\u0002G1)\u0002( W\nG1\n\u0002G1)\u0002C\n!A1 2 RH\u0002W\u0002C (8)\nand incorporate a residual connection:\nA1 = A1 + X: (9)\nA1\nG1 \u0002 G1\nAs the local attentive feature  is computed within\neach small  grid, a substantial reduction in com-\nputational and space complexity is achieved.\nA1\nG2\nG2 \u0002 G2\nThe  second  step  focuses  on  global  attention  calcula-\ntion. Here, we downsample  by a factor of  during\nthe computation of key and value matrices. This down-\nsampling  enables  efficient  global  attention  calculation,\ntreating each  \n grid as a token. This process can\nbe expressed as\nbA1 = AvePoolG2 (A1) (10)\nAvePoolG2 (\u0001)\nG2\nG2\nbA1 2\nR\nH\nG2\n\u0002 W\nG2\n\u0002C\nA1\nbA1\nwhere  denotes downsampling a feature map\nby  times using average pooling with both the kernel\nsize  and  stride  set  to .  Consequently,  we  have \n. We then reshape   and  as follows:\nA1 2 RH\u0002W\u0002C ! A1 2 R(H\u0002W)\u0002C;\nbA1 2 R\nH\nG2\n\u0002 W\nG2\n\u0002C\n! bA1 2 R\n( H\nG2\n\u0002 W\nG2\n)\u0002C\n: (11)\nFollowing this, we compute the query, key and value\nas\nQ2 = A1Wq\n2 ; K2 = bA1Wk\n2 ; V2 = bA1Wv\n2 (12)\nWq\n2 ; Wk\n2 ; Wv\n2 2 RC\u0002C\nQ2 2 R(H\u0002W)\u0002C\nK2 2 R\n( H\nG2\n\u0002 W\nG2\n)\u0002C\nV2 2 R\n( H\nG2\n\u0002 W\nG2\n)\u0002C\nA2 2 R(H\u0002W)\u0002C\nwhere  are trainable weight matri-\nces. It is easy to derive that we have ,\n,  and . Subsequ-\nently, (2) is called to obtain the global attentive feature\n, followed by a reshaping operation:\nA2 2 R(H\u0002W)\u0002C ! A2 2 RH\u0002W\u0002C: (13)\nThe final output of H-MHSA is given by\nH-MHSA(X)\n= ( A1 + A2)Wp + X (14)\nWp\nwhere  has the same meaning as in (3). In this way,\nH-MHSA  effectively  models  both  local  and  global\nrelationships, akin to vanilla MHSA.\nThe computational complexity of H-MHSA can be ex-\npressed as\nΩ(H-MHSA)\n= HWC (4C + 2G2\n1) + 2 HW\nG2\n2\nC(C + H\nW):\n(15)\nO(H2W2)\nO(H\nWG2\n1+\nH2W2\nG2\n2\n)\nCompared to (5), this represents a reduction in com-\nputational  complexity  from  to \n.  The  same  conclusion  can  be  easily  derived  for\nspace complexity.\nWe continue by comparing H-MHSA with existing vis-\nion  transformers,  highlighting  distinctive  features.  Swin\ntransformer[18] focuses  on  modeling  local  relationships,\nprogressively  expanding  the  receptive  field  through  shif-\nted  windows  and  additional  layers.  Conversely,  PVT[19]\nprioritizes global relationships through downsampling key\nand  value  matrices  but  overlooks  local  information.  In\ncontrast,  our  proposed  H-MHSA  excels  by  concurrently\ncapturing both local and global relationships. While Swin\ntransformer employs a fixed window size (i.e., a fixed-size\nbias matrix), and PVT uses a constant downsampling ra-\ntio (i.e., a convolution with the kernel size equal to the\nstride), these approaches necessitate retraining on the Im-\nageNet  dataset[24] for  any  re-parameterization.  In  con-\nY. Liu et al. / Vision Transformers with Hierarchical Attention 673 \n \n\nG1\nG2\ntrast,  the  parameter-free  nature  of  and  in  H-\nMHSA  allows  flexible  configuration  adjustments  for\ndownstream vision tasks without the need for retraining\non ImageNet.\nIn computer vision, achieving a comprehensive under-\nstanding  of  scenes  relies  on  the  simultaneous  considera-\ntion  of  both  global  and  local  information.  Within  the\nframework  of  our  proposed  H-MHSA,  global  self-atten-\ntion calculation (Equations (10)–(13)) is instrumental in\nestablishing  the  foundation  for  scene  interpretation,  en-\nabling the recognition of overarching patterns and aiding\nin high-level decision-making processes. Concurrently, loc-\nal self-attention calculation (Equations (6)–(9)) is crucial\nfor  refining  the  understanding  of  individual  components\nwithin the larger context, facilitating more detailed and\nnuanced  scene  analysis.  H-MHSA  excels  in  striking  the\ndelicate  balance  between  global  and  local  information,\nthereby  facilitating  a  nuanced  and  accurate  comprehen-\nsion  of  diverse  scenes.  In  essence,  the  seamless  integra-\ntion of global and local self-attention within the H-MHSA\nframework empowers transformers to navigate the intrica-\ncies of scene understanding, facilitating context-aware de-\ncision-making. \n3.3   Network architecture\nThis  part  introduces  the  network  architecture  of\nHAT-Net.  We  follow  the  common  practice  in  CNNs  to\nuse a global average pooling layer and a fully connected\nlayer to predict image classes[18]. This is different from ex-\nisting transformers which rely on another 1 D class token\nto make predictions[16, 17, 19–21, 46–49, 55–57]. We also observe\nthat  existing  transformers[16–21, 46–49] usually  adopt  the\nGaussian error linear unit (GELU) function[58] for nonlin-\near  activation.  However,  GELU  is  memory-hungry  dur-\ning network training. We empirically found that the sig-\nmoid-weighted  linear  unit  (SiLU)  function[59],  originally\ncoined in [58], performs on-par with GELU and is more\nmemory-friendly. Hence, HAT-Net uses SiLU[59] for non-\nlinear activation. Besides, we add a depthwise separable\nconvolution  (DW-Conv)[31] inside  the  MLP  as  widely\ndone.\nThe overall architecture of HAT-Net is illustrated in\nFig. 1. At the beginning of HAT-Net, instead of flatten-\n3 \u0002 3\n1\n4\n1\n4\n1\n8\n1\n16\n1\n32\n3 \u0002 3\ning  image  patches[16],  we  apply  two  sequential  vanilla\n convolutions,  each  of  which  has  a  stride  of  2,  to\ndownsample the input image into  scale. Then, we stack\nH-MHSA  and  MLP  alternatively,  which  can  be  divided\ninto four stages with pyramid feature scales of ,\n  , ,\nand ,  respectively.  For  feature  downsampling  at  the\nend  of  each  stage,  a  vanilla  convolution  with  a\nstride of 2 is used. The configuration details of HAT-Net\nare summarized in Table 1. We provide four versions of\nHAT-Net:  HAT-Net-Tiny,  HAT-Net-Small,  HAT-Net-\nMedium and HAT-Net-Large, whose number of paramet-\ners is similar to ResNet18, ResNet50, ResNet101 and Res-\nNet152[3],  respectively.  We  only  adopt  simple  parameter\nsettings without careful tuning to demonstrate the effect-\niveness  and  generality  of  HAT-Net.  The  dimension  of\neach head in the multi-head setting is set to 48 for HAT-\nNet-Tiny and 64 for other versions.\nG1\nG2\n8; 4; 2\nt\nt =\n2; 3; 4\n224 \u0002 224\nG1 =\n8; 7; 7\nG2 =\n8; 4; 2\nt\nt =\n2; 3; 4\nG1 =\n8; 8; 8\nG2 =\n16; 8; 4\nG1\nG2\nTo  enhance  the  applicability  of  HAT-Net  across  di-\nverse  vision  tasks,  we  present  guidelines  for  configuring\nthe parameter-free  \n and . While established models\nlike Swin transformer[18] adhere to a fixed window size of\n7, and PVT[19] employs a set of constant downsampling\nratios  \n for the -th stage ( ), we advocate\nfor certain adjustments. Practically, we find that a win-\ndow size of 8 is more pragmatic than 7, given that input\nresolutions often align with multiples of 8. Moreover, aug-\nmenting downsampling ratios serves to mitigate computa-\ntional  complexity.  Consequently,  for  image  classification\non  the  ImageNet  dataset[24],  where  the  standard  input\nresolution  is  \n pixels,  we  designate \nand  for  the -th  stage  ( ).  Here,  a\nwindow size of 7 is necessitated by the chosen resolution\nand  small  downsampling  rates  are  in  line  with  the  ap-\nproach  taken  by  PVT[19].  In  scenarios  involving  down-\nstream tasks like semantic segmentation, object detection\nand instance segmentation, where input resolutions tend\nto be larger, we opt for  \n for convenience and\n to  curtail  computational  expenses.  For  a\ncomprehensive analysis of the impact of different  and\n settings, we conduct an ablation study in Section 4.4. \n4   Experiments\nTo show the superiority of HAT-Net in feature repres-\n \nConv block\nDownsample\nH-MHSA\nMLP\nH-MHSA\nMLP\nH-MHSA\nMLP\nH-MHSA\nMLP\nGAP\nFC\nH\nW\n×\n4\n4\nH\nW\n×\n× L\n1\n8\n8\nH\nW\n×\n× L\n2\n16\n16\nH\nW\n×\n32\n32\nH\nW\n×\n× L\n3\n× L\n4\nDownsample\nDownsample\n \n\u0002Li\nLi\nH\nW\nFig. 1     Illustration  of  the  proposed  HAT-Net.  GAP:  Global  average  pooling;  FC:  Fully-connected  layer.    means  that  the\ntransformer block is repeated for    times.   and   denote the height and width of the input image, respectively.\n 674 Machine Intelligence Research 21(4), August 2024\n \n\nentation learning, this section evaluates HAT-Net for im-\nage  classification,  semantic  segmentation,  object  detec-\ntion and instance segmentation. \n4.1   Image classification\n224 \u0002 224\nG1 = f8; 7; 7g\nG2 = f8; 4; 2g\nt\nt = f2; 3; 4g\n224 \u0002 224\nExperimental setup.  The ImageNet dataset[24] con-\nsists  of  1.28 M  training  images  and  50 K  validation  im-\nages from 1 000 categories. We adopt the training set to\ntrain our networks and the validation set to test the per-\nformance. We implement HAT-Net using the popular Py-\nTorch framework[60]. For a fair comparison, we follow the\nsame training protocol as DeiT[49], which is the standard\nprotocol  for  training  transformer  networks  nowadays.\nSpecifically,  the  input  images  are  randomly  cropped  to\n \npixels, followed by random horizontal flipping\nand mixup[61] for data augmentation. Label smoothing[27]\nis used to avoid overfitting. The AdamW optimizer[62] is\nadopted with the momentum of 0.9, the weight decay of\n0.05, and a mini-batch size of 128 per GPU by default.\nThe initial learning rate is set to 1×10–3, which decreases\nfollowing the cosine learning rate schedule[63]. The train-\ning process lasts for 300 epochs on eight NVIDIA Tesla\nV100 GPUs. Note that for ablation studies, we utilize a\nmini-batch  size  of  64  and  100  training  epochs  to  save\ntraining  time.  Moreover,  we  set  \n and\n for  the -th  stage  ( ),  respect-\nively. The fifth stage can be processed directly using the\nvanilla MHSA mechanism. For model evaluation, we ap-\nply  a  center  crop  of  \n pixels  on  validation  im-\nages to evaluate the recognition accuracy. We report the\ntop-1 classification accuracy on the ImageNet validation\nset[24] as well as the number of parameters and the num-\nber of FLOPs for each model.\nExperimental results.  We compare HAT-Net with\nstate-of-the-art  network  architectures,  including  CNN-\nbased ones like ResNet[3], ResNeXt[29], RegNetY[64], Res-\nNeSt[45], and transformer-based ones like ViT[16], DeiT[49],\nT2T-ViT[46],  TNT[55],  CvT[65],  MViT[21],  PVT[19],  Swin\ntransformer[18],  Twins[66],  ToMe[22].  The  results  are  sum-\nmarized  in Table  2.  We  can  observe  that  HAT-Net\nachieves  state-of-the-art  performance.  Specifically,  with\nsimilar  numbers  of  parameters  and  FLOPs,  HAT-Net-\nTiny, HAT-Net-Small, HAT-Net-Medium and HAT-Net-\nLarge outperform the second best results by 1.1%, 0.6%,\n0.8%  and  0.6%  in  terms  of  the  top-1  accuracy,  respect-\nively.  Since  the  performance  for  image  classification  im-\nplies the ability of a network for learning feature repres-\nentations,  the  above  comparison  suggests  that  the  pro-\nposed HAT-Net has great potential for generic scene un-\nderstanding. \n4.2   Semantic segmentation\n\r =\n0:9\nExperimental  setup.  We  continue  by  applying\nHAT-Net  to  a  fundamental  downstream  vision  task,  se-\nmantic segmentation, which aims at predicting a class la-\nbel for each pixel in an image. We follow [19, 66] to re-\nplace the backbone of the well-known segmentor, Semant-\nic FPN[68], with HAT-Net or other backbone networks for\na  fair  comparison.  Experiments  are  conducted  on  the\nchallenging ADE20K dataset[69]. This dataset has 20 000\ntraining images, 2 000 validation images, and 3 302 test-\ning images. We train Semantic FPN[68] using the training\nset and evaluate it on the validation set. The training op-\ntimizer  is  AdamW[62] with  weight  decay  of  1×10–4.  We\napply  the  poly  learning  rate  schedule  with  \n and\nthe  initial  learning  rate  of  1×10–4.  During  training,  the\n \nC\nS\nK \u0002 K\nE\nt\nt = f2; 3; 4g\nTable 1    Network configurations of HAT-Net. The settings of building blocks are shown in brackets, with the number of blocks stacked.\nFor the first stage, each convolution has    channels and a stride of  . For the other four stages, each MLP uses a   DW-Conv and\nan expansion ratio of  .  Note that we omit the downsampling operation after the  -th stage ( )\nfor simplicity. “#Param” refers to the number of parameters.\nStage Input size Operator HAT-Net-Tiny HAT-Net-Small HAT-Net-Medium HAT-Net-Large\n1\n224 \u0002 224\n3 \u0002 3 conv:\nC =\n16; S= 2\nC = 48; S= 2\nC =\n16; S= 2\nC = 64; S= 2\nC =\n16; S= 2\nC = 64; S= 2\nC =\n16; S= 2\nC = 64; S= 2\n2\n56 \u0002 56 H-MHSA MLP\n2\n4\nC =\n48\nK = 3\nE = 8\n3\n5 \u0002 2\n2\n4\nC =\n64\nK = 3\nE = 8\n3\n5 \u0002 2\n2\n4\nC =\n64\nK = 5\nE = 8\n3\n5 \u0002 3\n2\n4\nC =\n64\nK = 3\nE = 8\n3\n5 \u0002 3\n3\n28 \u0002 28 H-MHSA MLP\n2\n4\nC =\n96\nK = 3\nE = 8\n3\n5 \u0002 2\n2\n4\nC =\n128\nK = 3\nE = 8\n3\n5 \u0002 3\n2\n4\nC =\n128\nK = 3\nE = 8\n3\n5 \u0002 6\n2\n4\nC =\n128\nK = 3\nE = 8\n3\n5 \u0002 8\n4\n14 \u0002 14 H-MHSA MLP\n2\n4\nC =\n240\nK = 3\nE = 4\n3\n5 \u0002 6\n2\n4\nC =\n320\nK = 3\nE = 4\n3\n5 \u0002 8\n2\n4\nC =\n320\nK = 5\nE = 4\n3\n5 \u0002 18\n2\n4\nC =\n320\nK = 3\nE = 4\n3\n5 \u0002 27\n5\n7 \u0002 7 H-MHSA MLP\n2\n4\nC =\n384\nK = 3\nE = 4\n3\n5 \u0002 3\n2\n4\nC =\n512\nK = 3\nE = 4\n3\n5 \u0002 3\n2\n4\nC =\n512\nK = 3\nE = 4\n3\n5 \u0002 3\n2\n4\nC =\n640\nK = 3\nE = 4\n3\n5 \u0002 3\n1 \u0002 1\n– Global average pooling, 1 000-d FC, softmax\n#Param 12.7 M 25.7 M 42.9 M 63.1 M\nY. Liu et al. / Vision Transformers with Hierarchical Attention 675 \n \n\n512 \u0002 512\nbatch  size  is  16,  and  each  image  has  a  resolution  of\n through resizing and cropping. During testing,\neach  image  is  resized  to  the  shorter  side  of  512  pixels,\nG1 = f8; 8; 8g\nG2 = f16; 8; 4g\nt\nt = f2; 3; 4g\nwithout  multi-scale  testing  or  flipping.  We  adopt  the\nwell-known MMSegmentation toolbox[70] for the above ex-\nperiments. We set  \n and  for\nthe - th stage ( ), respectively.\nExperimental  results.  The  results  are  depicted  in\nTable  3.  We  compare  with  typical  CNN  networks,  i.e.,\nResNets[3] and  ResNeXts[29],  and  transformer  networks,\ni.e.,  Swin  transformer[18],  PVT[19],  PVTv2[67] and  Twins-\nSVT[66].  As  can  be  observed,  the  proposed  HAT-Net\nachieves  significantly  better  performance  than  previous\ncompetitors.  Specifically,  HAT-Net-Tiny,  HAT-Net-\nSmall,  HAT-Net-Medium  and  HAT-Net-Large  attain\n1.9%, 0.4%, 1.9% and 0.7% higher mIoU than the second\nbetter  results  with  similar  number  of  parameters  and\nFLOPs. This demonstrates the superiority of HAT-Net in\nlearning effective feature representations for dense predic-\ntion tasks. \n4.3   Object detection and instance seg-\nmentation\nExperimental setup.  Since object detection and in-\nstance  segmentation  are  also  fundamental  downstream\nvision tasks, we apply HAT-Net to both tasks for further\nevaluating  its  effectiveness.  Specifically,  we  utilize  two\nwell-known detectors, i.e., RetinaNet[71] for object detec-\ntion and Mask R-CNN[5] for instance segmentation. HAT-\nNet  is  compared  to  some  well-known  CNN  and  trans-\nformer  networks  by  only  replacing  the  backbone  of  the\nabove  two  detectors.  Experiments  are  conducted  on  the\n \ny\n384 \u0002 384\n224 \u0002 224\nTable 2    Comparison to state-of-the-art methods on the\nImageNet validation set[24]. “*” indicates the performance of a\nmethod using the default training setting in the original paper.\n“#Param” and “#FLOPs” refer to the number of parameters\nand the number of FLOPs, respectively. “ ”  marks models that\nuse the input size of  ;  Otherwise, models\nuse the input size of  .\nArch. Models #Param #FLOPs Top-1 Acc. (%)\nCNN\nResNet18*[3] 11.7 M 1.8  G 69.8\nResNet18[3] 11.7 M 1.8  G 68.5\nTrans\nDeiT-Ti/16[49] 5.7 M 1.3  G 72.2\nPVT-Tiny[19] 13.2 M 1.9  G 75.1\nPVTv2-B1[67] 13.1 M 2.1  G 78.7\nHAT-Net-Tiny 12.7 M 2.0 G 79.8\nCNN\nResNet50*[3] 25.6 M 4.1  G 76.1\nResNet50[3] 25.6 M 4.1  G 78.5\nResNeXt50-32x4d*[29] 25.0 M 4.3  G 77.6\nResNeXt50-32x4d[29] 25.0 M 4.3  G 79.5\nRegNetY-4G[64] 20.6 M 4.0  G 80.0\nResNeSt-50[45] 27.5 M 5.4  G 81.1\nTrans\nToMe-ViT-S/16[22] 22.1 M 2.7  G 79.4\nDeiT-S/16[49] 22.1 M 4.6  G 79.8\nViTt\nT2T- -14 [46] 21.5 M 5.2  G 80.7\nTNT-S[55] 23.8 M 5.2  G 81.3\nCvT-13[65] 20.0 M 4.5  G 81.6\nPVT-Small[19] 24.5 M 3.8  G 79.8\nPVTv2-B2[67] 25.4 M 4.0  G 82.0\nSwin-T[18] 28.3 M 4.5  G 81.3\nTwins-SVT-S[66] 24.0 M 2.8  G 81.7\nHAT-Net-Small 25.7 M 4.3 G 82.6\nCNN\nResNet101*[3] 44.7 M 7.9  G 77.4\nResNet101[3] 44.7 M 7.9  G 79.8\nResNeXt101-32x4d*[29] 44.2 M 8.0  G 78.8\nResNeXt101-32x4d[29] 44.2 M 8.0  G 80.6\nRegNetY-8G[64] 39.2 M 8.0  G 81.7\nResNeSt-101[45] 48.3 M 10.3  G 83.0\nTrans\nViTt\nT2T- -19 [46] 39.2 M 8.4  G 81.4\nCvT-21[65] 31.5 M 7.1  G 82.5\nMViT-B-16[21] 37.0 M 7.8  G 82.5\nPVT-Medium[19] 44.2 M 6.7  G 81.2\nPVTv2-B3[67] 45.2 M 6.9  G 83.2\nSwin-S[18] 49.6 M 8.7  G 83.0\nHAT-Net-Medium 42.9 M 8.3 G 84.0\ny\n384 \u0002 384\n224 \u0002 224\nTable 2 (continued) Comparison to state-of-the-art methods on\nthe ImageNet validation set[24]. “*” indicates the performance of\na method using the default training setting in the original paper.\n“#Param” and “#FLOPs” refer to the number of parameters\nand the number of FLOPs, respectively. “ ”  marks models that\nuse the input size of  ;  Otherwise, models\nuse the input size of  .\nArch. Models #Param #FLOPs Top-1 Acc. (%)\nCNN\nResNet152*[3] 60.2 M 11.6  G 78.3\nResNeXt101-64x4d*[29] 83.5 M 15.6  G 79.6\nResNeXt101-64x4d[29] 83.5 M 15.6  G 81.5\nTrans\nViT-B/16†[16] 86.6 M 55.4  G 77.9\nViT-L/16†[16] 304.3 M 190.7  G 76.5\nToMe-ViT-L/16[22] 304.3 M 22.3  G 84.2\nDeiT-B/16[49] 86.6 M 17.6  G 81.8\nMViT-B-24[21] 53.5 M 10.9  G 83.1\nTNT-B[55] 65.6 M 14.1  G 82.8\nPVT-Large[19] 61.4 M 9.8  G 81.7\nPVTv2-B4[67] 62.6 M 10.1  G 83.6\nSwin-B[18] 87.8 M 15.4  G 83.3\nTwins-SVT-B[66] 56.0 M 8.3  G 83.2\nHAT-Net-Large 63.1 M 11.5 G 84.2\n 676 Machine Intelligence Research 21(4), August 2024\n \n\nG1 = f8; 8; 8g\nG2 = f16; 8; 4g\nt\nt = f2; 3; 4g\nlarge-scale  MS-COCO  dataset[72] by  training  on  the\ntrain2017  set  (~118K  images)  and  evaluating  on  the\nval2017  set  (5 K  images).  We  adopt  MMDetection  tool-\nbox[73] for  experiments  and  follow  the  experimental  set-\ntings of PVT[19] for a fair comparison. During training, we\ninitialize  the  backbone  weights  with  the  ImageNet-pre-\ntrained  models.  The  detectors  are  fine-tuned  using  the\nAdamW  optimizer[62] with  an  initial  learning  rate  of\n1×10–4 that is decreased by 10 times after the 8-th and\n11-th epochs, respectively. The whole training lasts for 12\nepochs with a batch size of 16. Each image is resized to a\nshorter side of 800 pixels, but the longer side is not al-\nlowed  to  exceed 1 333 pixels.  We  set  \n and\n for the -th stage ( ), respect-\nively.\nExperimental results.  The results are displayed in\nTable  4.  As  can  be  seen,  HAT-Net  substantially  im-\nproves  the  accuracy  over  other  network  architectures\nwith a similar number of parameters. Twins-SVT[66] com-\nbines the advantages of PVT[19] and Swin transformer[18]\nAPb\n50\nAPb\n75\nAPm\n50\nAPm\n75\nby alternatively stacking their basic blocks. When Retin-\naNet[71] is  adopted  as  the  detector,  HAT-Net-Small  at-\ntains  1.8%,  1.6%  and  1.8%  higher  results  than  Twins-\nSVT-S[66] in  terms  of  AP,  AP50 and  AP75,  respectively.\nCorrespondingly,  HAT-Net-Large  gets  1.0%,  0.5%  and\n1.5% higher results than Twins-SVT-B[66]. With Mask R-\nCNN[5] as  the  detector,  HAT-Net-Large  achieves  2.2%,\n1.7%  and  2.8%  higher  results  than  Twins-SVT-B[66] in\nterms of bounding box metrics APb,  \n and , re-\nspectively. HAT-Net-Large achieves 1.6%, 2.0% and 1.8%\nhigher  results  than  Twins-SVT-B[66] in  terms  of  mask\nmetrics APm,  \n and , respectively. Such signific-\nant  improvement  in  object  detection  and  instance  seg-\nmentation shows the superiority of HAT-Net in learning\neffective feature representations. \n4.4   Ablation studies\nIn this part, we evaluate various design choices of the\nproposed HAT-Net. As discussed above, we only train all\nablation models for 100 epochs to save training time. The\nbatch size and learning rate are also reduced by half ac-\ncordingly.  HAT-Net-Small  is  adopted  for  these  ablation\nstudies.\nEffect  of  the  proposed  H-MHSA.  Starting  from\nthe  window  attention[18] based  transformer  network,  we\ngradually  replace  the  window  attention  with  our  pro-\nposed H-MHSA at different stages. The results are sum-\nmarized  in Table  5.  Since  the  feature  map  at  the  fifth\nstage is small enough for directly computing MHSA, the\nfifth stage is excluded from Table 5. Note that the first\nstage of HAT-Net only consists of convolutions so that it\nis also excluded. From Table 5, we can observe that the\nperformance  for  both  image  classification  and  semantic\nsegmentation  is  improved  when  more  stages  adopt  H-\nMHSA. This verifies the effectiveness of the proposed H-\nMHSA  in  feature  presentation  learning.  It  is  interesting\nto  find  that  the  usage  of  H-MHSA  at  the  fourth  stage\nleads to more significant improvement than other stages.\nIntuitively,  the  fourth  stage  has  the  most  transformer\nblocks, so the changes at this stage would lead to more\nsignificant effects.\nA  pure  transformer  version  of  HAT-Net  VS.\nPVT[19].  When we remove all depthwise separable con-\nvolutions  from  HAT-Net  and  train  the  resulting  trans-\nformer  network  for  100  epochs,  it  achieves  77.7%  top-1\naccuracy on the ImageNet validation set[24]. In contrast,\nthe  well-known  transformer  network,  PVT[19],  attains\n75.8% top-1 accuracy under the same condition. This sug-\ngests that our proposed H-MHSA is very effective in fea-\nture representation learning.\nSiLU[59] VS. GELU[58].  We use SiLU function[59] for\nnonlinearization rather than the widely-used GELU func-\ntion[58] in transformers[13, 16]. Here, we evaluate the effect\nof this choice. HAT-Net with SiLU[59] attains 82.6% top-1\naccuracy on the ImageNet validation set[24] when trained\nfor 300 epochs. HAT-Net with GELU[58] gets 82.7% top-1\naccuracy,  slightly  higher  than  SiLU[59].  However,  HAT-\n \n512 \u0002 512\nTable 3    Experimental results on the ADE20K validation\ndataset[69] for semantic segmentation. We replace the\nbackbone of Semantic FPN[68] with various network\narchitectures. The number of FLOPs is calculated\nwith the input size of  .\nBackbone\nSemantic FPN[68]\n#Param (M) ↓ FLOPs (G) ↓ mIoU (%) ↑\nResNet-18[3] 15.5 31.9 32.9\nPVT-Tiny[19] 17.0 32.1 35.7\nPVTv2-B1[67] 17.8 33.1 41.5\nHAT-Net-Tiny 15.9 33.2 43.6\nResNet-50[3] 28.5 45.4 36.7\nPVT-Small[19] 28.2 42.9 39.8\nSwin-T[18] 31.9 46.0 41.5\nTwins-SVT-S[66] 28.3 37.0 43.2\nPVTv2-B2[67] 29.1 44.1 46.1\nHAT-Net-Small 29.5 49.6 46.6\nResNet-101[3] 47.5 64.8 38.8\nResNeXt-101-32x4d[29] 47.1 64.6 39.7\nPVT-Medium[19] 48.0 59.4 41.6\nSwin-S[18] 53.2 70.0 45.2\nTwins-SVT-B[66] 60.4 67.0 45.3\nPVTv2-B3[67] 49.0 60.7 47.3\nHAT-Net-Medium 46.7 74.7 49.3\nResNeXt-101-64x4d[29] 86.4 104.2 40.2\nPVT-Large[19] 65.1 78.0 42.1\nSwin-B[18] 91.2 107.0 46.0\nTwins-SVT-L[66] 102.0 103.7 46.7\nPVTv2-B4[67] 66.3 79.6 48.6\nHAT-Net-Large 66.8 96.4 49.5\nY. Liu et al. / Vision Transformers with Hierarchical Attention 677 \n \n\nNet  with  GELU[58] only  obtains  45.7%  mIoU  on  the\nADE20K dataset, 0.8% lower than HAT-Net with SiLU.\nWhen using a batch size of 128 per GPU, HAT-Net with\nSiLU[59] occupies 20.2 GB GPU memory during training,\nwhile  HAT-Net  with  GELU[58] occupies  23.8 GB  GPU\nmemory.  Hence,  HAT-Net  with  SiLU[59] can  achieve\nslightly better performance with less GPU memory con-\nsumption.\nG1\nG2\nSettings of  and .  In HAT-Net, the paramet-\nG1\nG2\nG1\nG2\nG1 = f8; 8; 8g\nG2 = f16; 8; 4g\nt\nt = f2; 3; 4g\nG1\nG2\nG1\nG2\nG1\nf8; 8; 8g\nG2\nG1 = f8; 8; 8g\nG2 = f16; 8; 4g\ners  and  play  pivotal  roles,  controlling  grid  sizes\nfor local attention calculation and downsampling rates for\nglobal  attention  calculation,  respectively.  In  this  evalu-\nation,  we  assess  the  model′s  performance  under  various\nconfigurations of  \n and . By default, for tasks such\nas  object  detection  and  instance  segmentation,  we  em-\nploy  \n and  for the -th stage\n( ),  respectively. Subsequently, we systematic-\nally vary  and , evaluating the performance of Mask\nR-CNN[5] with  HAT-Net-Small  as  the  backbone.  The\nevaluation results, conducted on the MS-COCO val2017\ndataset[72], are presented in Table 6. The findings indic-\nate that HAT-Net demonstrates robustness across differ-\nent  \nand  settings. Notably, altering  from its de-\nfault  \n configuration has a marginal impact on per-\nformance, resulting in slight performance reduction. Sim-\nilarly, adjusting the values of  \n yields a trade-off: De-\ncreasing  values  enhances  performance  at  the  expense  of\nincreased computational cost, while increasing values re-\nduces computational cost at the cost of slightly degraded\nperformance.  Our  default  choice  of  \n and\n strikes a favorable balance between accur-\nacy  and  efficiency,  offering  a  practical  configuration  for\ngeneral use. \n \n800 \u0002 1\n280\nTable 4    Object detection results with RetinaNet[71] and instance segmentation results with Mask R-CNN[5] on the\nMS-COCO val2017 set[72]. “R” and “X” represent ResNet[3] and ResNeXt[29], respectively. The number of FLOPs\nis computed with the input size of  .\nBackbone\nObject detection Instance segmentation\n#Param\n(M) ↓\n#FLOPs\n(G)↓\nRetinaNet[71]\n#Param\n(M) ↓\n#FLOPs\n(G) ↓\nMask R-CNN[5]\nAP\n(%)\nAP50\n(%)\nAP75\n(%)\nAPS\n(%)\nAPM\n(%)\nAPL\n(%)\nAPb (%)\nAPb\n50 (%)\nAPb\n75 (%)\nAPm (%)\nAPm\n50 (%)\nAPm\n75 (%)\nR-18[3] 21.3 190.0 31.8 49.6 33.6 16.3 34.3 43.2 31.2 209.0 34.0 54.0 36.7 31.2 51.0 32.7\nViL-Tiny[74] 16.6 204.0 40.8 61.3 43.6 26.7 44.9 53.6 26.9 223.0 41.4 63.5 45.0 38.1 60.3 40.8\nPVT-Tiny[19] 23.0 205.0 36.7 56.9 38.9 22.6 38.8 50.0 32.9 223.0 36.7 59.2 39.3 35.1 56.7 37.3\nHAT-Net-Tiny 21.6 212.0 42.5 63.3 45.8 26.9 46.1 56.6 31.8 231.0 43.1 65.4 47.4 39.7 62.5 42.4\nR-50[3] 37.7 239.0 36.3 55.3 38.6 19.3 40.0 48.8 44.2 260.0 38.0 58.6 41.4 34.4 55.1 36.7\nPVT-Small[19] 34.2 261.0 40.4 61.3 43.0 25.0 42.9 55.7 44.1 280.0 40.4 62.9 43.8 37.8 60.1 40.3\nSwin-T[18] 38.5 248.0 41.5 62.1 44.2 25.1 44.9 55.5 47.8 264.0 42.2 64.6 46.2 39.1 61.6 42.0\nViL-Small[74] 35.7 292.0 44.2 65.2 47.6 28.8 48.0 57.8 45.0 310.0 44.9 67.1 49.3 41.0 64.2 44.1\nTwins-SVT-S[66] 34.3 236.0 43.0 64.2 46.3 28.0 46.4 57.5 44.0 254.0 43.4 66.0 47.3 40.3 63.2 43.4\nHAT-Net-Small 35.5 286.0 44.8 65.8 48.1 28.8 48.6 59.5 45.4 303.0 45.2 67.6 49.9 41.6 64.6 44.7\nR-101[3] 56.7 315.0 38.5 57.8 41.2 21.4 42.6 51.1 63.2 336.0 40.4 61.1 44.2 36.4 57.7 38.8\nX-101-32x4d[29] 56.4 319.0 39.9 59.6 42.7 22.3 44.2 52.5 62.8 340.0 41.9 62.5 45.9 37.5 59.4 40.2\nPVT-Medium[19] 53.9 349.0 41.9 63.1 44.3 25.0 44.9 57.6 63.9 367.0 42.0 64.4 45.6 39.0 61.6 42.1\nSwin-S[18] 59.8 336.0 44.5 65.7 47.5 27.4 48.0 59.9 69.1 354.0 44.8 66.6 48.9 40.9 63.4 44.2\nHAT-Net-Medium 52.7 405.0 45.9 66.9 49.2 29.7 50.0 61.6 62.6 424.0 47.0 69.0 51.5 42.7 66.0 46.0\nX-101-64x4d[29] 95.5 473.0 41.0 60.9 44.0 23.9 45.2 54.0 101.9 493.0 42.8 63.8 47.3 38.4 60.6 41.3\nPVT-Large[19] 71.1 450.0 42.6 63.7 45.4 25.8 46.0 58.4 81.0 469.0 42.9 65.0 46.6 39.5 61.9 42.5\nTwins-SVT-B[66] 67.0 376.0 45.3 66.7 48.1 28.5 48.9 60.6 76.3 395.0 45.2 67.6 49.3 41.5 64.5 44.8\nHAT-Net-Large 73.1 519.0 46.3 67.2 49.6 30.0 50.6 62.4 82.7 537.0 47.4 69.3 52.1 43.1 66.5 46.6\n \ni\nTable 5    Ablation studies for the hierarchical attention in HAT-\nNet. The configuration of HAT-Net-Small is adopted for all\nexperiments. “√” indicates that we replace the window\nattention[18] with the hierarchical attention at the  - th stage.\n“Top-1 Acc” is the top-1 accuracy on the ImageNet validation\ndataset[24]. “mIoU” is the mean IoU for semantic\nsegmentation on the ADE20K dataset[69].\nDesign\n#Stage\nTop-1 Acc. (%) mIoU  (%)\n2 3 4\n1 78.2 42.1\n2 √ 78.2 42.4\n3 √ √ 78.4 42.5\n4 √ √ √ 79.3 43.4\n 678 Machine Intelligence Research 21(4), August 2024\n \n\n5   Conclusions\nThis  paper  addresses  the  inefficiency  inherent  in\nvanilla vision transformers due to the elevated computa-\ntional  and  space  complexity  associated  with  MHSA.  In\nresponse to this challenge, we introduce a novel hierarch-\nical  framework  for  MHSA  computation,  denoted  as  H-\nMHSA, aiming to alleviate the computational and space\ndemands.  Compared  to  existing  approaches  in  this  do-\nmain, such as PVT[19] and Swin transformer[18], H-MHSA\ndistinguishes itself by directly capturing both global de-\npendencies  and  local  relationships.  Integrating  the  pro-\nposed H-MHSA, we formulate the HAT-Net family, show-\ncasing  its  prowess  through  comprehensive  experiments\nspanning image classification, semantic segmentation, ob-\nject detection, and instance segmentation. Our results af-\nfirm the efficacy and untapped potential of HAT-Net in\nadvancing representation learning.\nG1\nG2\nApplications  of  HAT-Net.  The  versatility  of\nHAT-Net  extends  its  utility  across  diverse  real-world\nscenarios and downstream vision tasks. As a robust back-\nbone network for feature extraction, HAT-Net seamlessly\nintegrates with existing prediction heads and decoder net-\nworks, enabling proficient execution of various scene un-\nderstanding  tasks.  Furthermore,  HAT-Net′s  adaptability\nto different input resolutions and computational resource\nconstraints  is  facilitated  by  the  flexible  adjustment  of\nparameters,  specifically  \n and .  Users  can  tailor\nHAT-Net  to  their  specific  requirements,  selecting  from\ndifferent HAT-Net versions to align with their objectives.\nIn conclusion, HAT-Net not only presents a pragmat-\nic  solution  to  the  limitations  of  vanilla  vision  trans-\nformers but also opens avenues for innovation in the fu-\nture  design  of  such  architectures.  The  simplicity  of  the\nproposed  H-MHSA  underscores  its  potential  as  a  trans-\nformative  element  in  the  evolving  landscape  of  vision\ntransformer development. \nAcknowledgements\nThis work was supported by A*STAR Career Devel-\nopment Fund, Singapore (No. C233312006). Open access\nfunding  provided  by  Swiss  Federal  Institute  of  Techno-\nlogy Zurich. \nDeclarations of conflict of interest\nThe authors declared that they have no conflicts of in-\nterest to this work. \nOpen Access\nThis article is licensed under a Creative Commons At-\ntribution  4.0  International  License,  which  permits  use,\nsharing, adaptation, distribution and reproduction in any\nmedium or format, as long as you give appropriate credit\nto the original author(s) and the source, provide a link to\nthe  Creative  Commons  licence,  and  indicate  if  changes\nwere made.\nThe images or other third party material in this art-\nicle  are  included  in  the  article′s  Creative  Commons  li-\ncence,  unless  indicated  otherwise  in  a  credit  line  to  the\nmaterial. If material is not included in the article′s Creat-\nive  Commons  licence  and  your  intended  use  is  not  per-\nmitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the\ncopyright holder.\nTo  view  a  copy  of  this  licence,  visit http://creative-\ncommons.org/licenses/by/4.0/.\nReferences\n A. Krizhevsky, I. Sutskever, G. E. Hinton. ImageNet clas-\nsification with deep convolutional neural networks. In \nPro-\nceedings of the 25th International Conference on Neural\nInformation Processing Systems,  Lake  Tahoe,  USA,\npp. 1097–1105, 2012. DOI: 10.5555/2999134.2999257.\n[1]\n K. Simonyan, A. Zisserman. Very deep convolutional net-\nworks for large-scale image recognition. In Proceedings of\nthe 3rd International Conference on Learning Representa-\ntions, San Diego, USA, 2015.\n[2]\n K. M. He, X. Y. Zhang, S. Q. Ren, J. Sun. Deep residual\nlearning for image recognition. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\nLas Vegas, USA, pp. 770–778, 2016. DOI: 10.1109/CVPR.\n2016.90.\n[3]\n \nG1\nG2\nTable 6    Ablation studies for the settings of    and   in HAT-Net. The performance assessment is conducted using Mask R-CNN[5]\nwith HAT-Net-Small as the backbone. Evaluation results are reported on the MS-COCO val2017 dataset[72].\nSettings\n#FLOPs (G)↓\nMask R-CNN[5]\nG1\nG2\nAPb\n (%)\nAPb\n50  (%)\nAPb\n75  (%)\nAPm  (%)\nAPm\n50  (%)\nAPm\n75  (%)\nf8; 8; 8g\nf12; 4; 2g\n338.0 45.7 67.8 50.4 41.7 64.8 44.7\nf8; 8; 8g\nf12; 6; 3g\n313.0 45.3 67.6 49.7 41.6 64.8 44.9\nf8; 8; 8g\nf16; 8; 4g\n303.0 45.2 67.6 49.9 41.6 64.6 44.7\nf8; 8; 8g\nf32; 16; 8g\n291.0 44.6 66.8 49.1 41.0 63.8 44.3\nf16; 16; 8g\nf16; 8; 4g\n309.0 45.1 67.5 49.5 41.3 64.5 44.4\nf4; 4; 4g\nf16; 8; 4g\n300.0 45.1 67.1 49.5 41.3 64.3 44.5\nY. Liu et al. / Vision Transformers with Hierarchical Attention 679 \n \n\n S. Q. Ren, K. M. He, R. Girshick, J. Sun. Faster R-CNN:\nTowards real-time object detection with region proposal\nnetworks.  IEEE Transactions on Pattern Analysis and\nMachine Intelligence,  vol. 39, no. 6, pp. 1137–1149, 2017.\nDOI: 10.1109/TPAMI.2016.2577031.\n[4]\n K. M. He, G. Gkioxari, P. Dollár, R. Girshick. Mask R-\nCNN. In Proceedings of the IEEE International Confer-\nence on Computer Vision,  Venice,  Italy,  pp. 2980–2988,\n2017. DOI: 10.1109/ICCV.2017.322.\n[5]\n H. S. Zhao, J. P. Shi, X. J. Qi, X. G. Wang, J. Y. Jia. Pyr-\namid scene parsing network. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\nHonolulu, USA, pp. 6230–6239, 2017. DOI: 10.1109/CV-\nPR.2017.660.\n[6]\n Y. Liu, M. M. Cheng, X. W. Hu, J. W. Bian, L. Zhang, X.\nBai, J. H. Tang. Richer convolutional features for edge de-\ntection. IEEE Transactions on Pattern Analysis and Ma-\nchine Intelligence, vol. 41, no. 8, pp. 1939–1946, 2019. DOI:\n10.1109/TPAMI.2018.2878849.\n[7]\n Y. Liu, M. M. Cheng, D. P. Fan, L. Zhang, J. W. Bian, D.\nC. Tao. Semantic edge detection with diverse deep super-\nvision. International Journal of Computer Vision, vol. 130,\nno. 1, pp. 179–198, 2022. DOI: 10.1007/s11263-021-01539-\n8.\n[8]\n Y. Liu, M. M. Cheng, X. Y. Zhang, G. Y. Nie, M. Wang.\nDNA: Deeply supervised nonlinear aggregation for salient\nobject  detection.  IEEE Transactions on Cybernetics,\nvol. 52, no. 7, pp. 6131–6142, 2022. DOI: 10.1109/TCYB.\n2021.3051350.\n[9]\n Y.  Liu,  Y.  C.  Gu,  X.  Y.  Zhang,  W.  W.  Wang,  M.  M.\nCheng. Lightweight salient object detection via hierarchic-\nal visual perception learning. IEEE Transactions on Cy-\nbernetics,  vol. 51,  no. 9,  pp. 4439–4449,  2021.  DOI:  10.\n1109/TCYB.2020.3035613.\n[10]\n Y. Liu, Y. H. Wu, Y. F. Ban, H. F. Wang, M. M. Cheng.\nRethinking computer-aided tuberculosis diagnosis. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vis-\nion and Pattern Recognition, Seattle, USA, pp. 2643–2652,\n2020. DOI: 10.1109/CVPR42600.2020.00272.\n[11]\n Y. Liu, Y. H. Wu, P. S. Wen, Y. J. Shi, Y. Qiu, M. M.\nCheng. Leveraging instance-, image- and dataset-level in-\nformation  for  weakly  supervised  instance  segmentation.\nIEEE Transactions on Pattern Analysis and Machine In-\ntelligence,  vol. 44,  no. 3,  pp. 1415–1428,  2022.  DOI:  10.\n1109/TPAMI.2020.3023152.\n[12]\n A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, I. Polosukhin. Attention is all you\nneed. In Proceedings of the 31st International Conference\non Neural Information Processing Systems, Long Beach,\nUSA,  pp. 6000–6010,  2017.  DOI:  10.5555/3295222.3295\n349.\n[13]\n J. Devlin, M. W. Chang, K. Lee, K. Toutanova. BERT:\nPre-training  of  deep  bidirectional  transformers  for  lan-\nguage understanding. In Proceedings of the Conference of\nthe North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies,\nMinneapolis,  USA,  pp. 4171–4186, 2019.  DOI:  10.18653/\nv1/N19-1423.\n[14]\n Z. H. Dai, Z. L. Yang, Y. M. Yang, J. Carbonell, Q. Le, R.\nSalakhutdinov. Transformer-XL: Attentive language mod-\nels beyond a fixed-length context. In Proceedings of the\n57th Annual Meeting of the Association for Computation-\nal Linguistics, Florence, Italy, pp. 2978–2988, 2019. DOI:\n[15]\n10.18653/v1/P19-1285.\n A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. H. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G.\nHeigold, S. Gelly, J. Uszkoreit, N. Houlsby. An image is\nworth 16×16 words: Transformers for image recognition at\nscale. In Proceedings of the 9th International Conference\non Learning Representations, 2021.\n[16]\n B. Heo, S. Yun, D. Han, S. Chun, J. Choe, S. J. Oh. Re-\nthinking spatial dimensions of vision transformers. In Pro-\nceedings of the IEEE/CVF International Conference on\nComputer Vision,  Montreal,  Canada,  pp. 11916–11925,\n2021. DOI: 10.1109/ICCV48922.2021.01172.\n[17]\n Z. Liu, Y. T. Lin, Y. Cao, H. Hu, Y. X. Wei, Z. Zhang, S.\nLin,  B.  N.  Guo.  Swin  transformer:  Hierarchical  vision\ntransformer using shifted windows. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\nMontreal,  Canada,  pp. 9992–10002, 2021.  DOI:  10.1109/\nICCV48922.2021.00986.\n[18]\n W. H. Wang, E. Z. Xie, X. Li, D. P. Fan, K. T. Song, D.\nLiang, T. Lu, P. Luo, L. Shao. Pyramid vision transformer:\nA versatile backbone for dense prediction without convolu-\ntions. In Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, Montreal, Canada, pp. 548–\n558, 2021. DOI: 10.1109/ICCV48922.2021.00061.\n[19]\n W. J. Xu, Y. F. Xu, T. Chang, Z. W. Tu. Co-scale conv-at-\ntentional  image  transformers.  In  Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\nMontreal, Canada, pp. 9961–9970, 2021. DOI: 10.1109/IC-\nCV48922.2021.00983.\n[20]\n H. Q. Fan, B. Xiong, K. Mangalam, Y. H. Li, Z. C. Yan, J.\nMalik, C. Feichtenhofer. Multiscale vision transformers. In\nProceedings of the IEEE/CVF International Conference\non Computer Vision,  Montreal,  Canada,  pp. 6804–6815,\n2021. DOI: 10.1109/ICCV48922.2021.00675.\n[21]\n D. Bolya, C. Y. Fu, X. L. Dai, P. Z. Zhang, C. Feichten-\nhofer, J. Hoffman. Token merging: Your ViT but faster. In\nProceedings of the 11th International Conference on\nLearning Representations, Kigali, Rwanda, 2023.\n[22]\n Y.  LeCun,  L.  Bottou,  Y.  Bengio,  P.  Haffner.  Gradient-\nbased learning applied to document recognition. Proceed-\nings of the IEEE, vol. 86, no. 11, pp. 2278–2324, 1998. DOI:\n10.1109/5.726791.\n[23]\n O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S.\nA. Ma, Z. H. Huang, A. Karpathy, A. Khosla, M. Bern-\nstein, A. C. Berg, L. Fei-Fei. ImageNet large scale visual\nrecognition challenge. International Journal of Computer\nVision,  vol. 115, no. 3, pp. 211–252, 2015.  DOI:  10.1007/\ns11263-015-0816-y.\n[24]\n R. K. Srivastava, K. Greff, J. Schmidhuber. Highway net-\nworks,  [Online],  Available:  https://arxiv.org/abs/1505.\n00387, 2015.\n[25]\n C. Szegedy, W. Liu, Y. Q. Jia, P. Sermanet, S. Reed, D.\nAnguelov, D. Erhan, V. Vanhoucke, A. Rabinovich. Go-\ning deeper with convolutions. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\nBoston,  USA,  pp. 1–9, 2015.  DOI:  10.1109/CVPR.2015.\n7298594.\n[26]\n C. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, Z. Wojna.\nRethinking the inception architecture for computer vision.\nIn Proceedings of the IEEE Conference on Computer Vis-\nion and Pattern Recognition, Las Vegas, USA, pp. 2818–\n2826, 2016. DOI: 10.1109/CVPR.2016.308.\n[27]\n 680 Machine Intelligence Research 21(4), August 2024\n \n\n C. Szegedy, S. Ioffe, V. Vanhoucke, A. A. Alemi. Incep-\ntion-v4, Inception-ResNet and the impact of residual con-\nnections  on  learning.  In  Proceedings of the 31st AAAI\nConference on Artificial Intelligence, San Francisco, USA,\npp. 4278–4284, 2017. DOI: 10.5555/3298023.3298188.\n[28]\n S. N. Xie, R. Girshick, P. Dollár, Z. W. Tu, K. M. He. Ag-\ngregated  residual  transformations  for  deep  neural  net-\nworks. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition,  Honolulu,  USA,\npp. 5987–5995, 2017. DOI: 10.1109/CVPR.2017.634.\n[29]\n G. Huang, Z. Liu, L. Van Der Maaten, K. Q. Weinberger.\nDensely  connected  convolutional  networks.  In  Proceed-\nings of the IEEE Conference on Computer Vision and Pat-\ntern Recognition,  Honolulu,  USA,  pp. 2261–2269, 2017.\nDOI: 10.1109/CVPR.2017.243.\n[30]\n A. G. Howard, M. L. Zhu, B. Chen, D. Kalenichenko, W.\nJ. Wang, T. Weyand, M. Andreetto, H. Adam. MobileN-\nets: Efficient convolutional neural networks for mobile vis-\nion  applications,  [Online],  Available:  https://arxiv.org/\nabs/1704.04861, 2017.\n[31]\n M. Sandler, A. Howard, M. L. Zhu, A. Zhmoginov, L. C.\nChen. MobileNetV2: Inverted residuals and linear bottle-\nnecks.  In  Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition,  Salt  Lake\nCity,  USA,  pp. 4510–4520,  2018.  DOI:  10.1109/CVPR.\n2018.00474.\n[32]\n X. Y. Zhang, X. Y. Zhou, M. X. Lin, J. Sun. ShuffleNet:\nAn  extremely  efficient  convolutional  neural  network  for\nmobile devices. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, Salt\nLake City, USA, pp. 6848–6856, 2018. DOI: 10.1109/CV-\nPR.2018.00716.\n[33]\n N. N. Ma, X. Y. Zhang, H. T. Zheng, J. Sun. ShuffleNet\nV2:  Practical  guidelines  for  efficient  CNN  architecture\ndesign. In Proceedings of the 15th European Conference on\nComputer Vision,  Munich,  Germany,  pp. 122–138, 2018.\nDOI: 10.1007/978-3-030-01264-9_8.\n[34]\n M. X. Tan, B. Chen, R. M. Pang, V. Vasudevan, M. Sand-\nler, A. Howard, Q. V. Le. MnasNet: Platform-aware neur-\nal  architecture  search  for  mobile.  In  Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, Long Beach, USA, pp. 2815–2823, 2019. DOI:\n10.1109/CVPR.2019.00293.\n[35]\n M. X. Tan, Q. V. Le. EfficientNet: Rethinking model scal-\ning for convolutional neural networks. In Proceedings of\nthe 36th International Conference on Machine Learning,\nLong Beach, USA, pp. 6105–6114, 2019.\n[36]\n M. Jaderberg, K. Simonyan, A. Zisserman, K. Kavukcuo-\nglu. Spatial transformer networks. In Proceedings of the\n28th International Conference on Neural Information Pro-\ncessing Systems, Montreal, Canada, pp. 2017–2025, 2015.\nDOI: 10.5555/2969442.2969465.\n[37]\n L. Chen, H. W. Zhang, J. Xiao, L. Q. Nie, J. Shao, W. Liu,\nT. S. Chua. SCA-CNN: Spatial and channel-wise atten-\ntion  in  convolutional  networks  for  image  captioning.  In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition,  Honolulu,  USA,  pp. 6298–\n6306, 2017. DOI: 10.1109/CVPR.2017.667.\n[38]\n F. Wang, M. Q. Jiang, C. Qian, S. Yang, C. Li, H. G.\nZhang, X. G. Wang, X. O. Tang. Residual attention net-\nwork for image classification. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition,\nHonolulu,  USA,  pp. 6450–6458,  2017.  DOI:  10.1109/\nCVPR.2017.683.\n[39]\n J. Hu, L. Shen, S. Albanie, G. Sun, E. H. Wu. Squeeze-\nand-excitation networks. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, vol. 42, no. 8, pp. 2011–\n2023, 2020. DOI: 10.1109/TPAMI.2019.2913372.\n[40]\n S. Woo, J. Park, J. Y. Lee, I. S. Kweon. CBAM: Convolu-\ntional block attention module. In Proceedings of the 15th\nEuropean Conference on Computer Vision, Munich, Ger-\nmany, pp. 3–19, 2018. DOI: 10.1007/978-3-030-01234-2_1.\n[41]\n J. Park, S. Woo, J. Y. Lee, I. S. Kweon. BAM: Bottleneck\nattention module. In Proceedings of the British Machine\nVision Conference,  Newcastle,  UK,  Article  number  147,\n2018.\n[42]\n X. Li, W. H. Wang, X. L. Hu, J. Yang. Selective kernel\nnetworks. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, Long Beach,\nUSA,  pp. 510–519,  2019.  DOI:  10.1109/CVPR.2019.\n00060.\n[43]\n X. L. Wang, R. Girshick, A. Gupta, K. M. He. Non-local\nneural networks. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, Salt\nLake  City,  USA,  pp. 7794–7803,  2018.  DOI:  10.1109/\nCVPR.2018.00813.\n[44]\n H. Zhang, C. R. Wu, Z. Y. Zhang, Y. Zhu, H. B. Lin, Z.\nZhang, Y. Sun, T. He, J. Mueller, R. Manmatha, M. Li, A.\nSmola. ResNeSt: Split-attention networks. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and\nPattern Recognition Workshops,  New  Orleans,  USA,\npp. 2735–2745, 2022.  DOI:  10.1109/CVPRW56347.2022.\n00309.\n[45]\n L. Yuan, Y. P. Chen, T. Wang, W. H. Yu, Y. J. Shi, Z. H.\nJiang, F. E. H. Tay, J. S. Feng, S. C. Yan. Tokens-to-\ntoken ViT: Training vision transformers from scratch on\nImageNet. In Proceedings of the IEEE/CVF International\nConference on Computer Vision,  Montreal,  Canada,\npp. 538–547, 2021. DOI: 10.1109/ICCV48922.2021.00060.\n[46]\n H. Touvron, M. Cord, A. Sablayrolles, G. Synnaeve, H.\nJégou. Going deeper with image transformers. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, Montreal, Canada, pp. 32–42, 2021. DOI: 10.\n1109/ICCV48922.2021.00010.\n[47]\n D. Q. Zhou, B. Y. Kang, X. J. Jin, L. J. Yang, X. C. Lian,\nZ. H. Jiang, Q. B. Hou, J. S. Feng. DeepViT: Towards\ndeeper  vision  transformer,  [Online],  Available:  https://\narxiv.org/abs/2103.11886, 2021.\n[48]\n H.  Touvron,  M.  Cord,  M.  Douze,  F.  Massa,  A.  Sab-\nlayrolles,  H.  Jégou.  Training  data-efficient  image  trans-\nformers & distillation through attention. In Proceedings of\nthe 38th International Conference on Machine Learning,\npp. 10347–10357, 2021.\n[49]\n A. Srinivas, T. Y. Lin, N. Parmar, J. Shlens, P. Abbeel, A.\nVaswani. Bottleneck transformers for visual recognition.\nIn  Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition,  Nashville,  USA,\npp. 16514–16524,  2021.  DOI:  10.1109/CVPR46437.2021.\n01625.\n[50]\n I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X.\nH. Zhai, T. Unterthiner, J. Yung, A. Steiner, D. Keysers,\nJ. Uszkoreit, M. Lucic, A. Dosovitskiy. MLP-Mixer: An\nall-MLP architecture for vision. In Proceedings of the 34th\nAdvances in Neural Information Processing Systems,\npp. 24261–24272, 2021.\n[51]\n H. X. Liu, Z. H. Dai, D. R. So, Q. V. Le. Pay attention to\nMLPs. In Proceedings of the 34th Advances in Neural In-\n[52]\nY. Liu et al. / Vision Transformers with Hierarchical Attention 681 \n \n\nformation Processing Systems, pp. 9204–9215, 2021.\n Q. B. Hou, Z. H. Jiang, L. Yuan, M. M. Cheng, S. C. Yan,\nJ. S. Feng. Vision Permutator: A permutable MLP-like ar-\nchitecture  for  visual  recognition.  IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 45, no. 1,\npp. 1328–1334, 2023. DOI: 10.1109/TPAMI.2022.3145427.\n[53]\n Z. C. Wang, Y. B. Hao, X. Y. Gao, H. Zhang, S. Wang, T.\nT.  Mu,  X.  N.  He.  Parameterization  of  cross-token  rela-\ntions with relative positional encoding for vision MLP. In\nProceedings of the 30th ACM International Conference on\nMultimedia, Lisboa, Portugal, pp. 6288–6299, 2022. DOI:\n10.1145/3503161.3547953.\n[54]\n K. Han, A. Xiao, E. H. Wu, J. Y. Guo, C. J. Xu, Y. H.\nWang. Transformer in transformer. In Proceedings of the\n35th Conference on Neural Information Processing Sys-\ntems, pp. 15908–15919, 2021.\n[55]\n Y. W. Li, K. Zhang, J. Z. Cao, R. Timofte, L. Van Gool.\nLocalViT: Bringing locality to vision transformers, [On-\nline], Available: https://arxiv.org/abs/2104.05707, 2021.\n[56]\n K. Yuan, S. P. Guo, Z. W. Liu, A. J. Zhou, F. W. Yu, W.\nWu. Incorporating convolution designs into visual trans-\nformers. In Proceedings of the IEEE/CVF International\nConference on Computer Vision,  Montreal,  Canada,\npp. 559–568, 2021. DOI: 10.1109/ICCV48922.2021.00062.\n[57]\n D.  Hendrycks,  K.  Gimpel.  Gaussian  error  linear  units\n(GELUs), [Online], Available: https://arxiv.org/abs/1606.\n08415, 2016.\n[58]\n S. Elfwing, E. Uchibe, K. Doya. Sigmoid-weighted linear\nunits for neural network function approximation in rein-\nforcement  learning.  Neural Networks,  vol. 107,  pp. 3–11,\n2018. DOI: 10.1016/j.neunet.2017.12.012.\n[59]\n A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G.\nChanan, T. Killeen, Z. M. Lin, N. Gimelshein, L. Antiga,\nA. Desmaison, A. Köpf, E. Yang, Z. DeVito, M. Raison, A.\nTejani, S. Chilamkurthy, B. Steiner, L. Fang, J. J. Bai, S.\nChintala.  PyTorch:  An  imperative  style,  high-perform-\nance deep learning library. In Proceedings of the 33rd In-\nternational Conference on Neural Information Processing\nSystems, Red Hook, USA, Article number 721, 2019. DOI:\n10.5555/3454287.3455008.\n[60]\n H.  Y.  Zhang,  M.  Cissé,  Y.  N.  Dauphin,  D.  Lopez-Paz.\nmixup: Beyond empirical risk minimization. In Proceed-\nings of the 6th International Conference on Learning Rep-\nresentations, Vancouver, Canada, 2018.\n[61]\n I. Loshchilov, F. Hutter. Decoupled weight decay regular-\nization.  In  Proceedings of the 7th International Confer-\nence on Learning Representations,  New  Orleans,  USA,\n2019.\n[62]\n I. Loshchilov, F. Hutter. SGDR: Stochastic gradient des-\ncent with warm restarts. In Proceedings of the 5th Interna-\ntional Conference on Learning Representations,  Toulon,\nFrance, 2017.\n[63]\n I. Radosavovic, R. P. Kosaraju, R. Girshick, K. M. He, P.\nDollár. Designing network design spaces. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition,  Seattle,  USA,  pp. 10425–10433,  2020.\nDOI: 10.1109/CVPR42600.2020.01044.\n[64]\n H. P. Wu, B. Xiao, N. Codella, M. C. Liu, X. Y. Dai, L.\nYuan, L. Zhang. CvT: Introducing convolutions to vision\ntransformers. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, Montreal, Canada,\npp. 22–31, 2021. DOI: 10.1109/ICCV48922.2021.00009.\n[65]\n  X. X. Chu, Z. Tian, Y. Q. Wang, B. Zhang, H. B. Ren, X.\nL.  Wei,  H.  X.  Xia,  C.  H.  Shen.  Twins:  Revisiting  the\ndesign of spatial attention in vision transformers. In Pro-\nceedings of the 34th Advances in Neural Information Pro-\ncessing Systems, pp. 9355–9366, 2021.\n[66]\n W. H. Wang, E. Z. Xie, X. Li, D. P. Fan, K. T. Song, D.\nLiang,  T.  Lu,  P.  Luo,  L.  Shao.  PVT  v2:  Improved\nbaselines with pyramid vision transformer. Computation-\nal Visual Media, vol. 8, no. 3, pp. 415–424, 2022. DOI: 10.\n1007/s41095-022-0274-8.\n[67]\n A. Kirillov, R. Girshick, K. M. He, P. Dollár. Panoptic fea-\nture pyramid networks. In Proceedings of the Conference\non Computer Vision and Pattern Recognition,  Long\nBeach, USA, pp. 6392–6401, 2019. DOI: 10.1109/CVPR.2019.\n00656.\n[68]\n B. L. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, A.\nTorralba. Scene parsing through ADE20K dataset. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, Honolulu, USA, pp. 5122–5130, 2017.\nDOI: 10.1109/CVPR.2017.544.\n[69]\n MMSegmentation Contributors. MMSegmentation: Open-\nMMLab semantic segmentation toolbox and benchmark,\n[Online], Available: https://github.com/open-mmlab/mm-\nsegmentation, 2020.\n[70]\n T. Y. Lin, P. Goyal, R. Girshick, K. M. He, P. Dollár. Fo-\ncal loss for dense object detection. In Proceedings of the\nIEEE International Conference on Computer Vision,\nVenice,  Italy,  pp. 2999–3007,  2017.  DOI:  10.1109/ICCV.\n2017.324.\n[71]\n T. Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D.\nRamanan,  P.  Dollár,  C.  L.  Zitnick.  Microsoft  COCO:\nCommon objects in context. In Proceedings of the 13th\nEuropean Conference on Computer Vision,  Zürich,\nSwitzerland, pp. 740–755, 2014. DOI: 10.1007/978-3-319-\n10602-1.\n[72]\n K. Chen, J. Q. Wang, J. M. Pang, Y. H. Cao, Y. Xiong, X.\nX. Li, S. Y. Sun, W. S. Feng, Z. W. Liu, J. R. Xu, Z.\nZhang, D. Z. Cheng, C. C. Zhu, T. H. Cheng, Q. J. Zhao,\nB. Y. Li, X. Lu, R. Zhu, Y. Wu, J. F. Dai, J. D. Wang, J.\nP. Shi, W. L. Ouyang, C. C. Loy, D. H. Lin. MMDetection:\nOpen MMLab detection toolbox and benchmark, [Online],\nAvailable: https://arxiv.org/abs/1906.07155, 2019.\n[73]\n P. C. Zhang, X. Y. Dai, J. W. Yang, B. Xiao, L. Yuan, L.\nZhang, J. F. Gao. Multi-scale vision Longformer: A new\nvision transformer for high-resolution image encoding. In\nProceedings of the IEEE/CVF International Conference\non Computer Vision,  Montreal,  Canada,  pp. 2978–2988,\n2021. DOI: 10.1109/ICCV48922.2021.00299.\n[74]\nYun Liu  received the B. Eng. and Ph. D.\ndegrees in computer science from Nankai\nUniversity,  China  in  2016  and  2020,  re-\nspectively. Then, he worked with Prof. Luc\nVan  Gool  for  one  and  a  half  years  as  a\npostdoctoral  scholar  at  Computer  Vision\nLab, ETH Zürich, Switzerland. Currently,\nhe is a senior scientist at Institute for In-\nfocomm Research (I2R), A*STAR, Singa-\npore.\n     His research interests include computer vision and machine\nlearning (especially deep learning).\n     E-mail: vagrantlyun@gmail.com\n     ORCID iD: 0000-0001-6143-0264\n 682 Machine Intelligence Research 21(4), August 2024\n \n\nYu-Huan Wu received the Ph. D. degree\nin  computer  science from  Nankai  Uni-\nversity,  China  in  2022,  advised  by  Prof.\nMing-Ming Cheng. He is a scientist at the\nInstitute of High Performance Computing\n(IHPC), A*STAR, Singapore. He has pub-\nlished 10+ papers on top-tier conferences\nand  journals  such  as  IEEE  TPAMI/\nTIP/CVPR/ICCV.\n     His research interests include computer vision and medical\nimaging.\n     E-mail: wu_yuhuan@ihpc.a-star.edu.sg\n     ORCID iD: 0000-0001-8666-3435\nGuolei Sun  received the M. Sc. degree in\ncomputer science from King Abdullah Uni-\nversity of Science and Technology, Saudi\nArabia  in  2018.  From  2018  to  2019,  he\nworked as a research engineer at the Incep-\ntion  Institute  of  Artificial  Intelligence,\nUAE. Currently, he is a Ph. D. degree can-\ndidate at ETH Zürich, Switzerland under\nsupervision of Prof. Luc Van Gool. He has\npublished more than 20 papers in top journals and conferences\nsuch as TPAMI, CVPR, ICCV, and ECCV.\n     His research interests include computer vision and deep learn-\ning for tasks such as semantic segmentation, video understand-\ning, and object counting.\n     E-mail: sunguolei.kaust@gmail.com (Corresponding author)\n     ORCID iD: 0000-0001-8667-9656\nLe  Zhang  received  the  Ph. D.  degree  in\nelectrical  and  electronic  engineering from\nNanyang Technological University (NTU),\nSingapore in 2016. He is a professor with\nthe School of Information and Communic-\nation Engineering, University of Electron-\nic  Science  and  Technology  of  China\n(UESTC), China. From 2016 to 2018, he\nwas a postdoc fellow at Advanced Digital\nSciences Center (ADSC), Singapore. From 2018 to 2021, he was\na  research  scientist  at  the  Institute  for  Infocomm  Research\n(I2R), A*STAR, Singapore. He is an Associate Editor of Neural\nNetworks, Neurocomputing, and IET Biometrics.\n     His research interests include computer vision and machine\nlearning.\n     \nE-mail: lezhang@uestc.edu.cn (Corresponding author)\n     ORCID iD: 0000-0002-6930-8674\nAjad  Chhatkuli received the M. Sc. de-\ngree  in  computer  vision  from  the  Uni-\nversity of Burgundy, France in 2013, and\nthe Ph. D. degree in computer vision from\nthe  University  of  Clermont  Auvergne,\nFrance  in  2017 under  the  supervision  of\nProf.  Adrien  Bartoli  and  Dr.  Daniel\nPizarro. He is currently a postdoctoral re-\nsearcher supervised by Prof. Luc Van Gool\nat ETH Zürich, Switzerland.\n     His research interests include template-based and template-\nfree non-rigid 3D reconstruction.\n     E-mail: ajad.chhatkuli@vision.ee.ethz.ch\n     ORCID iD: 0000-0003-2051-2209\nLuc  Van  Gool received the B. Eng. de-\ngree in electromechanical engineering from\nthe Katholieke Universiteit Leuven, Belgi-\num in 1981. Currently, he is a professor at\nthe Katholieke Universiteit Leuven, Belgi-\num, and the ETH in Zürich, Switzerland.\nHe leads computer vision research at both\nplaces, and also teaches at both. He has\nbeen a program committee member of sev-\neral major computer vision conferences. He received several Best\nPaper  awards,  won  a  David  Marr  Prize  and  a  Koenderink\nAward,  and  was  nominated  Distinguished  Researcher  by  the\nIEEE Computer Science Committee. He is a co-founder of 10\nspin-off companies.\n     His research interests include 3D reconstruction and model-\nling, object recognition, tracking, gesture analysis, and the com-\nbination of those.\n     E-mail: vangool@vision.ee.ethz.ch\n     ORCID iD: 0000-0002-3445-5711\nY. Liu et al. / Vision Transformers with Hierarchical Attention 683 \n \n",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.4964905381202698
    },
    {
      "name": "Computer science",
      "score": 0.44976502656936646
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38123035430908203
    },
    {
      "name": "Engineering",
      "score": 0.21491339802742004
    },
    {
      "name": "Electrical engineering",
      "score": 0.10299438238143921
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I115228651",
      "name": "Agency for Science, Technology and Research",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I3005327000",
      "name": "Institute for Infocomm Research",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I3004594783",
      "name": "Institute of High Performance Computing",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I150229711",
      "name": "University of Electronic Science and Technology of China",
      "country": "CN"
    }
  ]
}