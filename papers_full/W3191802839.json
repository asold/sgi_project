{
  "title": "Anomaly Detection in Dynamic Graphs via Transformer",
  "url": "https://openalex.org/W3191802839",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A1994553651",
      "name": "Liu Yixin",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2753535486",
      "name": "Pan, Shirui",
      "affiliations": [
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A4222502322",
      "name": "Wang, Yu Guang",
      "affiliations": [
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A1982946578",
      "name": "Xiong Fei",
      "affiliations": [
        "Beijing Jiaotong University",
        "Beijing Municipal Education Commission"
      ]
    },
    {
      "id": "https://openalex.org/A2098147315",
      "name": "Wang Liang",
      "affiliations": [
        "Northwestern Polytechnical University"
      ]
    },
    {
      "id": "https://openalex.org/A2089192563",
      "name": "Chen Qing-feng",
      "affiliations": [
        "Guangxi University"
      ]
    },
    {
      "id": "https://openalex.org/A4227079821",
      "name": "Lee, Vincent CS",
      "affiliations": [
        "Monash University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6777195786",
    "https://openalex.org/W2510664603",
    "https://openalex.org/W3130808434",
    "https://openalex.org/W6781067599",
    "https://openalex.org/W2163557584",
    "https://openalex.org/W2966841471",
    "https://openalex.org/W2284900416",
    "https://openalex.org/W1986644953",
    "https://openalex.org/W6726873649",
    "https://openalex.org/W2809409545",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W2132914434",
    "https://openalex.org/W2783466287",
    "https://openalex.org/W2585835859",
    "https://openalex.org/W2154851992",
    "https://openalex.org/W1999507172",
    "https://openalex.org/W2052104835",
    "https://openalex.org/W6737778391",
    "https://openalex.org/W2294347342",
    "https://openalex.org/W2109026675",
    "https://openalex.org/W2962756421",
    "https://openalex.org/W3170457273",
    "https://openalex.org/W3132147085",
    "https://openalex.org/W2808771744",
    "https://openalex.org/W3017733550",
    "https://openalex.org/W2991481220",
    "https://openalex.org/W3157999218",
    "https://openalex.org/W3194841521",
    "https://openalex.org/W6600552058",
    "https://openalex.org/W6792108999",
    "https://openalex.org/W6772776185",
    "https://openalex.org/W6766892243",
    "https://openalex.org/W3133518153",
    "https://openalex.org/W6767098714",
    "https://openalex.org/W6783267081",
    "https://openalex.org/W6784614190",
    "https://openalex.org/W3012871709",
    "https://openalex.org/W3003265726",
    "https://openalex.org/W2926081294",
    "https://openalex.org/W6639055396",
    "https://openalex.org/W6779518175",
    "https://openalex.org/W6763701032",
    "https://openalex.org/W6766673545",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W6780012728",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6768021236",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W6788071488"
  ],
  "abstract": "Detecting anomalies for dynamic graphs has drawn increasing attention due to their wide applications in social networks, e-commerce, and cybersecurity. Recent deep learning-based approaches have shown promising results over shallow methods. However, they fail to address two core challenges of anomaly detection in dynamic graphs: the lack of informative encoding for unattributed nodes and the difficulty of learning discriminate knowledge from coupled spatial-temporal dynamic graphs. To overcome these challenges, in this paper, we present a novel transformer-based Anomaly Detection framework for dynamic graphs (TADDY). Our framework constructs a comprehensive node encoding strategy to better represent each nodes structural and temporal roles in an evolving graphs stream. Meanwhile, TADDY captures informative representation from dynamic graphs with coupled spatial-temporal patterns via a dynamic graph transformer model. The extensive experimental results demonstrate that our proposed TADDY framework outperforms the state-of-the-art methods by a large margin on six real-world datasets.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nAnomaly Detection in Dynamic Graphs via\nTransformer\nYixin Liu, Shirui Pan, Yu Guang Wang, Fei Xiong, Liang Wang, Qingfeng Chen, Vincent CS Lee\nAbstract—Detecting anomalies for dynamic graphs has drawn increasing attention due to their wide applications in social networks,\ne-commerce, and cybersecurity. Recent deep learning-based approaches have shown promising results over shallow methods.\nHowever, they fail to address two core challenges of anomaly detection in dynamic graphs: the lack of informative encoding for\nunattributed nodes and the difﬁculty of learning discriminate knowledge from coupled spatial-temporal dynamic graphs. To overcome\nthese challenges, in this paper, we present a novel Transformer-based Anomaly Detection framework for DYnamic graphs (TADDY).\nOur framework constructs a comprehensive node encoding strategy to better represent each node’s structural and temporal roles in an\nevolving graphs stream. Meanwhile, TADDY captures informative representation from dynamic graphs with coupled spatial-temporal\npatterns via a dynamic graph transformer model. The extensive experimental results demonstrate that our proposed TADDY framework\noutperforms the state-of-the-art methods by a large margin on six real-world datasets.\nIndex Terms—Anomaly detection, dynamic graphs, transformer.\n!\n1 I NTRODUCTION\nI\nN recent years, graphs have attracted a surge of research\nattention with the development of networked applica-\ntions in social networks [1], human knowledge networks\n[2], business networks [3] and cybersecurity [4]. However,\nthe bulk of the existing researches focus on static graphs\n[5], [6], yet the real-world graph data often evolves over\ntime [7], [8]. Taking social networks as an example, there are\nalways fresh persons who enroll in the community every\nmonth, and the relation between individuals is changing\nover time. To model and analyze graphs where nodes and\nedges change over time, mining dynamic graphs gains\nincreasing popularity in the community of graph analysis.\nAmong various analysis problems for dynamic graphs,\ndetecting the anomalous edges in an evolving graph stream\nis a critical task [9], [10]. Considering a user-item network\nin the e-commerce scenario, the attackers tend to make fake\npurchase orders to increase the inﬂuence of certain goods\nillegally. It is of great signiﬁcance to detect such fake orders\nto maintain a fair trading environment.\nDetecting anomalies in dynamic graphs, however, is not\na trivial task since there are two challenges in dynamic\ngraph learning. Challenge 1 is the lack of raw attribute\ninformation in most dynamic graphs. Due to the explosive\n• Y. Liu, S. Pan, and V . Lee are with the Department of Data Science and\nAI, Faculty of IT, Monash University, Clayton, VIC 3800, Australia\nE-mail: yixin.liu@monash.edu; shirui.pan@monash.edu; Vin-\ncent.CS.Lee@monash.edu\n• Y. G. Wang is with Shanghai Jiao Tong University, in Institute of Natural\nSciences and School of Mathematical Sciences, and with the Max Planck\nInstitute for Mathematics in Sciences, in Mathematics Machine Learning\ngroup Email: yuguang.wang@sjtu.edu.cn\n• F. Xiong is with Key Laboratory of Communication and Information\nSystems, Beijing Municipal Commission of Education, Beijing Jiaotong\nUniversity, Beijing 100044, China; Email: xiongf@bjtu.edu.cn\n• L. Wang is with School of Computer Science, Northwestern Polytechnical\nUniversity, Xi’an 10072, China; Email: liangwang@nwpu.edu.cn\n• Q. Chen is with School of Computer, Electronic and Information, Guangxi\nUniversity, Nanning, 530004, China; Email: qingfeng@gxu.edu.cn\n• Corresponding Author: Shirui Pan\ndemand for data volume of time-evolving attributes or the\ninaccessible attributes caused by privacy issues, it is hard\nto construct attribute information to represent each node\nfrom the mainstream raw dynamic graph datasets. To ﬁll the\ngap, an effective encoding method that constructs artiﬁcial\nfeatures to represent evolving nodes is required. Challenge 2\nis the difﬁculty of learning discriminative knowledge from\ndynamic graphs where spatial (structural) information and\ntemporal information are coupled. Figure 1 provides a toy\nexample to illustrate how coupled information affects the\ndetection of edge abnormality. The green edge tends to\nbe normal since there are close structural communications\nbetween their neighborhoods in the previous timestamps.\nThe red edge, on the contrary, is an anomalous edge with a\nhigh probability because the two red nodes always keep\na distance from each other in the former snapshots. The\npoint is that both structural (i.e., shared neighborhoods)\nand temporal (i.e., previous interaction) factors should be\nconsidered simultaneously when making decisions, raising\nthe challenge in understanding such coupled information.\nAiming to detect anomalies in dynamic graphs, various\ntypes of approaches are proposed in the recent decade.\nThe shallow methods like GOutlier [11] and CM-Sketch\n[12] utilize shallow learning mechanisms (e.g., structural\nconnectivity model or historical behavior analysis) to detect\nanomalies. However, empirical experiments show that these\nmethods suffer from limited performance when detecting\nanomalous edges in large and complex dynamic graphs\n[9]. Very recently, as a novel branch, deep learning-based\nmethods, have shown to be a powerful solution for dynamic\ngraph learning. For example, NetWalk [9] leverages dy-\nnamic deep graph embedding technique with a clustering-\nbased detector to detect anomalies; AddGraph [10], StrGNN\n[13] and H-VGRAE [14] further exploit end-to-end deep\nneural network models to solve the problem.\nDespite their improved performance, the existing deep\nlearning-based methods fail to address the aforementioned\narXiv:2106.09876v2  [cs.LG]  27 Oct 2021\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\n… …\n!! !\" !#\nFig. 1. A toy example to illustrate how the coupled information affects\nthe detection of edges’ legality. The three graphs are a fragment from\na dynamic graph stream at a sequential timeline {t1,t2,t3}. The solid\ngreen line represents a normal edge at t3, while the red dash line\nindicates an anomalous edge. We highlight the corresponding nodes\nin the previous timestamps with colors.\nchallenges very well. Speciﬁcally, when facing the lack of\nraw node attributes, they do not create informative node\nencodings to represent the nodes’ properties. The one-hot\nidentity features in [9], [14] and the random initialized fea-\ntures in [10] cannot express any structural or temporal prop-\nerty of each node. The distance-based node labeling strategy\nin [13] only considers local structural information, limiting\nits expressive power. Furthermore, most of them use two\nindividual network modules to extract spatial and temporal\nfeatures, resulting in their insufﬁcient capability to capture\nthe coupled information. For instance, in AddGraph [10]\nand StrGNN [13], Graph Convolutional Networks (GCNs)\nare employed to acquire spatial knowledge, with following\nGated Recurrent Units (GRUs) capturing temporal informa-\ntion. The isolated processing of two types of information\nresults in missing the coupled spatial-temporal features and\nfurther leads to a sub-optimal solution.\nAiming to resolve these challenges, in this paper, we\npropose a novel Transformer-based Anomaly Detection\nframework for DYnamic graph ( TADDY for abbreviation).\nOur theme is to construct a node encoding to cover sufﬁ-\ncient spatial and temporal knowledge and leverage a sole\ntransformer model to capture the coupled spatial-temporal\ninformation. More speciﬁcally, to overcome Challenge 1, we\ncarefully design a comprehensive node encoding composed\nof three functional terms to distill global spatial, local spa-\ntial, and temporal information. Learnable mapping func-\ntions are integrated into the node encoding, which helps\nthe framework automatically extract informative encoding\nin an end-to-end manner. For Challenge 2 , we develop a\ndynamic graph transformer model to simultaneously learn\nspatial and temporal knowledge. An edge-based substruc-\nture sampling is performed to capture contextual informa-\ntion crossing time as the input of the transformer model.\nThen, the coupled spatial-temporal information is extracted\nby the attention mechanism crossing structure and time. To\nsum up, the main contributions of this paper are:\n• We propose an end-to-end transformer-based learn-\ning framework, TADDY, for anomaly detection on\ndynamic graphs. This is the ﬁrst transformer-based\nmethod for dynamic graph learning and graph\nanomaly detection to the best of our knowledge.\n• We design a comprehensive encoding method for\nnodes in dynamic graphs. The proposed node encod-\ning integrates various knowledge, including global\nspatial, local spatial and temporal information.\n• We present a dynamic graph transformer model\nwhich aggregates spatial and temporal knowledge\nsimultaneously. A novel edge-based substructure\nsampling strategy is leveraged to provide sufﬁcient\nreceipt ﬁelds for the learning model.\n• We evaluate the effectiveness of TADDY on six\nbenchmark datasets. The extensive experiments\ndemonstrate that our method delivers state-of-the-\nart performance.\nWe organize the rest of this paper as follows. The related\nworks are reviewed in Section 2. We describe the problem\ndeﬁnition in Section 3. In Section 4 the overall pipeline\nand each component of our framework are introduced. The\nexperimental results are demonstrated in Section 5. Finally,\nwe conclude the contributions and future works of this work\nin Section 6.\n2 R ELATED WORK\nThis section brieﬂy reviews existing anomaly detection\nmethods for dynamic graphs and transformers.\n2.1 Anomaly Detection in Dynamic Graphs\nAnomaly detection in dynamic graphs attracts considerable\ninterest by the research community [15], for which, many\nmethods have been proposed in recent years. For example,\nGOutlier [11] employs a structural connectivity model to de-\ntect outliers in graph streams and builds dynamic network\npartition to maintain the connectivity behavior model. CAD\n[16] detects node relationships by tracking a measure that\ncombines information regarding changes in graph structure\nand in edge weights. CM-Sketch [12] considers both the\nlocal structural information and historical behavior to dis-\ncriminate the edge’s anomalous property. StreamSpot [17] is\na clustering-based approach that utilizes a novel similarity\nfunction for heterogeneous graphs property comparison\nand leverages a centroid-based clustering method to model\nthe behaviors of graph stream. SpotLight [18] uses a ran-\ndomized sketching technique to guarantee a large mapped\ndistance between anomalous and normal instances in the\nsketch space. Since these approaches leverage the shallow\nmechanisms to detect the anomalous edges, we categorize\nthem into shallow learning-based methods.\nAnother branch of approach employs deep learning\ntechnique to capture anomalous data in dynamic graphs,\nwhich is denoted as the category of deep learning-based\nmethod. NetWalk [9] leverages a random walk-based en-\ncoder to generate node embeddings with clique embed-\nding objective and then models the network evolving via\ndynamic updating reservoirs. Finally, a dynamic clustering-\nbased anomaly detector is employed to score the abnormal-\nity of each edge. AddGraph [10] further constructs an end-\nto-end neural network model to capture dynamic graphs’\nspatial and temporal patterns. A GCN [19] is served as a\nstructural features extractor, and a GRU-attention module\nis designed to combine short-term and long-term dynamic\nevolving. StrGNN [13] extracts the h-hop enclosing sub-\ngraph of edges and leverages stacked GCN [19] and GRU\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\nto capture the spatial and temporal information. The learn-\ning model is trained in an end-to-end way with negative\nsampling from “context-dependent” noise distribution. H-\nVGRAE [14] builds a hierarchical model by combining vari-\national graph autoencoder and recurrent neural network. To\ndetect anomalous edges, the edge reconstruction probability\nis used to measure the abnormality.\nOur proposed TADDY framework can be categorized\ninto the deep learning-based methods but has two main\ndifferences compared to the existing approaches mentioned\nabove. Most of the above approaches employ different\nnetwork modules to separately extract spatial and tempo-\nral features, while TADDY uses a transformer network to\nmodel spatial and temporal information simultaneously.\nSecondly, these methods consider naive node encoding\nfrom unattributed dynamic graphs as the network input,\nwhich may fail to provide sufﬁcient information for the\ndownstream neural network. In contrast to them, TADDY\nconstructs a comprehensive node encoding that includes\nboth spatial and temporal information.\n2.2 Transformers\nTransformers are a family of neural networks solely based\non attention mechanisms to learn representative embedding\nfor various data. The Transformer model is ﬁrst proposed\nin [20], which focuses on the machine translation tasks in\nnatural language processing (NLP). BERT [21] further ap-\nplies transformers to multiple deep language understanding\ntasks by introducing the pre-training technique. Following\nTransformer and BERT, a large number of variant works\nare presented and reach state-of-the-art results on various\nNLP tasks [22], [23], [24]. Very recently, the transformer\nmodel is extended to the ﬁeld of computer vision (CV) [25].\nFor instance, DETR [26] ﬁrst leverages transformers on the\nobject detection task. ViT [27] splits an image into multiple\npatches and uses a pure transformer model to learn the\nrepresentation for image classiﬁcation directly. SETR [28]\nutilizes a ViT-like encoder for feature extraction and adopts\na multi-level feature aggregation module for pixel-wise im-\nage segmentation. For further details about transformers on\nNLP and CV please see related surveys [29], [30].\nSome recent works also introduce transformers to the\nﬁeld of graph machine learning. GTN [31] is performed\non heterogeneous graphs with transformers by meta-path-\nbased relation learning. HGT [32] is a transformer model\nfor the representation learning on web-scale heterogeneous\ngraphs, which reaches state-of-the-art results on various\ndownstream tasks. GROVER [33] integrates the message\npassing mechanism into the transformer architecture to\nlearn representation for molecule graph data. Graph-BERT\n[34] constructs a BERT-like network model for static graph\nlearning and introduces various well-designed tasks for self-\nsupervised model pre-training [35].\nOur proposed framework introduces transformers as\nour backbone neural network model due to its powerful\nexpressive capability. Differently, we extend transformers to\ndynamic graphs, which is a more complex learning scenario\nwhere both structural and temporal features should be\nconsidered. By comparison, most of the existing methods\nfocus on static graphs.\nTABLE 1\nCommonly used notation with explanations.\nNotation Explanation\nG={Gt}Tt=1 A graph steam with a maximum timestamp ofT.\nGt= (Vt,Et) The snapshot graph at timestampt.\nVt The node set at timestampt.\nEt The edge set at timestampt.\nvti∈Vt A node with indexiat the timestampt.\neti,j= (vti,vtj) ∈Et An edge betweenvtiandvtjat the timestampt.\nnt The number of nodes at timestampt.\nmt The number of edges at timestampt.\nAt The binary adjacency matrix at timestampt.\nf(·) Anomaly score function.\nGtτ=\n{Gt−τ+1,···,Gt}\nThe sequence of graphs with timestamptas the end and\nτas the window size (length).\nS(ettgt) The substructure node set of target edgeettgt.\nxdiﬀ(vij) The diffusion-based spatial encoding of nodevij.\nxdist(vij) The distance-based spatial encoding of nodevij.\nxtemp(vij) The relative temporal encoding of nodevij.\nx(vij) The fused encoding of nodevij.\nX(ettgt) The encoding matrix of target edgeettgt.\nH(l) The output embedding of thel-th layers of Transformer.\nQ(l) The query matrix of thel-th layers of Transformer.\nK(l) The key matrix of thel-th layers of Transformer.\nV(l) The value matrix of thel-th layers of Transformer.\nz(ettgt) The embedding of target edgeettgt.\nW(l)\nQ,W(l)\nK,W(l)\nV The learnable parameters of Transformer.\nepos,i∈Et Thei-th positive edge fromEt.\neneg,,i∈Etn∼Pn(Et) Thei-th negative edge by negative samplingPn(Et).\nwS,bS The learnable parameters of Anomaly Detector.\nk The number of contextual nodes.\nτ The size of time window.\ndenc The dimension of encoding.\ndemb The dimension of embedding.\nL The number of layers of Transformer.\n3 P ROBLEM DEFINITION\nIn this paper, we model a dynamic graph as a graph stream\nrepresented by a series of discrete snapshots. The deﬁnition\nof dynamic graphs is given as follows:\nDeﬁnition 1. Considering a dynamic graph with a maxi-\nmum timestamp of T, a graph steam is represented by\nG = {Gt}T\nt=1, where each Gt = (Vt,Et) is the snapshot at\ntimestamp t, Vt is the node set at timestamp t, and Et is\nthe edge set at timestamp t. An edge et\ni,j = (vt\ni,vt\nj) ∈Et\nindicates that there is a connection between node vt\ni\nand vt\nj at the timestamp t, where vt\ni,vt\nj ∈ Vt. We use\nnt = |Vt| and mt = |Et| to denote the number of\nnodes and edges at timestamp t respectively. A binary\nadjacency matrix At ∈Rnt×nt\nis employed to denote\nGt, where At\ni,j = 1 if there is a link between nodes vi\nand vj at timestamp t, otherwise At\ni,j = 0.\nThe goal of this paper is to detect the anomalous edges in\neach timestamp. According to the aforementioned notation,\nwe formalize the anomaly detection in dynamic graphs as a\nscoring problem:\nDeﬁnition 2. Anomaly detection in dynamic graphs.Given\na dynamic graph G = {Gt}T\nt=1 where each Gt = (Vt,Et),\nfor each et\ni,j ∈Et, the goal of anomaly detection is to pro-\nduce the anomaly score f(et\ni,j), where f(·) is a learnable\nanomaly score function. The anomaly score indicates the\nabnormality degree of the edge, where a larger score\nf(et\ni,j) shows a higher anomalous probability of et\ni,j.\nFollowing the previous works [9], [10], [13], we consider\nan unsupervised setting for anomaly detection in dynamic\ngraphs. Speciﬁcally, in the training phase, no labeled data\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nfor anomalies is given, but we assume that all edges in the\ntraining set are normal. The binary labels of abnormality\nare given in the testing phase to evaluate the performance\nof algorithms. Concretely, a label yet\ni,j\n= 1 indicates that\nedge et\ni,j is an anomalous edge, and yet\ni,j\n= 0 indicates\nthat et\ni,j is normal. Note that the distribution of normal\nand anomalous edges is often imbalanced, which means the\nnumber of normal edges is much larger than anomalous\nedges.\nFor the convenience of readers, the notation used in this\npaper is summarized in Table 1.\n4 M ETHODOLOGY\nIn this section, we introduce the general framework of\nTADDY. The overview of our proposed framework is illus-\ntrated in Figure 2. On the highest level, TADDY consists of\nfour components, namely edge-based substructure sampling ,\nspatial-temporal node encoding, dynamic graph transformer, and\ndiscriminative anomaly detector . The framework is trained in\nan end-to-end manner, indicating that the anomaly scores\nare output and learned directly. At ﬁrst, to capture the\nspatial-temporal contexts of each target edge, we perform\nedge-based substructure sampling to acquire the target nodes\nand the contextual nodes in multiple timestamps. Then, the\nspatial-temporal node encoding generates the node encodings\nas the input of the transformer model. Both spatial and\ntemporal information for each node are integrated into a\nﬁxed-length encoding. After that, the dynamic graph trans-\nformer extracts the spatial-temporal knowledge of edges\nvia a sole transformer model composed of a transformer\nmodule and a pooling module. Finally, in the discriminative\nanomaly detector , we perform a negative sampling to gen-\nerate pseudo negative edges, and an edge scoring module\ntrained by binary cross-entropy loss is employed to calculate\nthe output anomaly scores. From Section 4.1 to Section\n4.4, we concretely introduce the four main components of\nTADDY framework. In Section 4.5, we analyze the time\ncomplexity of our proposed framework.\n4.1 Edge-based Substructure Sampling\nAs is noticed in previous works [13], [36], anomalies often\noccur in local substructures of graphs, indicating that we\nshould zoom our receptive ﬁeld to a suitable local scale.\nTherefore, instead of working on a complete dynamic graph,\nwe ﬁrst sample substructures as the data elements of our\nanomaly detection framework. Since we focus on detecting\nanomalous edges, we perform an edge-based sampling:\neach edge in dynamic graphs is viewed as the center of\nthe sampled substructure and is denoted as a target edge .\nFor a given target edge, we denote the source node and\ndestination node as target nodes.\nIn addition to the target nodes, it is necessary to include\nother neighboring nodes in the sampled substructure. In\nthis paper, we denote these neighboring nodes as contextual\nnodes. To acquire contextual information, a natural question\narises here is: how to efﬁciently sample contextual nodes from\na given target edge in a dynamic graph? We ﬁrst consider\nthe sampling problem in a single snapshot which can be\nregarded as a static graph. A naive solution is to extract the\nh-hop neighbors of target nodes as contexts. However, this\nstrategy has several drawbacks. First, with h-hop neighbors\nsampling, the imbalanced distribution of node degrees in\nreal-world datasets would lead to a performance decline\nand low efﬁciency. For example, the average degree of UCI\nMessages dataset is 14.47, while its maximum degree is 255.\nFor those popular nodes with high degrees, the numbers of\nh-hop neighbors would be explosive, resulting in the noisy\ninformation in sampled contexts and damaging the running\nefﬁciency. Second, sampling h-hop neighbors ignores the\ndifferent roles and importance of nodes in the substruc-\nture. Considering two target nodes with both shared and\nexclusive neighbors, it is obvious that the shared neighbors\ncontribute more to detecting the target edges. However,\nthis simple strategy just views the shared and exclusive\nneighbors equally when sampling contextual nodes.\nTo address the aforementioned limitations, in this work,\nwe borrow the graph diffusion technique [37], [38] to sample\na ﬁxed-size and importance-aware contextual node set for\neach target edge. With graph diffusion, a global view of\ngraph structure is acquired, and then we can quantify the\nimportance of each node for a given target node/edge.\nFormally, given an adjacency matrix of a static graph\nA ∈Rn×n, we deﬁne graph diffusion S ∈Rn×n by\nS =\n∞∑\nk=0\nθkTk, (1)\nwhere T ∈Rn×n is the generalized transition matrix and\nθk is the weighting coefﬁcient which determines the ra-\ntio of global-local information. To guarantee convergence,\nsome stricter conditions are considered which requires that∑∞\nk=0 θk = 1,θk ∈[0,1] and the eigenvalues λi of T are\nbounded by λi ∈ [0,1]. By applying speciﬁc deﬁnitions\nof T and θ, different instantiations of graph diffusion can\nbe computed. For instance, Personalized PageRank (PPR)\n[39] and the heat kernel [40] are two popular examples\nof diffusion. Concretely, PPR chooses T = AD−1 and\nθk = α(1 −α)k, where D ∈Rn×n is the diagonal degree\nmatrix and α ∈(0,1) is the teleport probability. The heat\nkernel considers T = AD−1 and θk = e−ββk/k!, where β\nis the diffusion time. To avoid multiple steps of iteration, the\nsolutions to PPR and heat kernel can be formulated as:\nSPPR = α\n(\nIn −(1 −α)D−1/2AD−1/2\n)−1\n, (2)\nSheat = exp\n(\nβAD−1 −β\n)\n. (3)\nGiven a diffusion matrix S, a row si indicates the\nconnectivity between the i-th node and each node from a\nglobal perspective. For example, si,j represents the degree\nof connectivity between the i- and j- th nodes with a\ncontinuous value. By leveraging this property, we can pick\na ﬁxed number of the most important enclosing nodes for a\ngiven target edge. Taking edgeetgt = (v1,v2) as an example,\nwe can compute the connectivity vector of etgt by adding\nthe connectivity vectors of two target nodes:\nsetgt = sv1 + sv2 . (4)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nEdge-based Substructure Sampling \nSpatial-temporal Node EncodingDynamic Graph Transformer\nDiscriminative Anomaly Detector\n… …\ntt -1t -2\n!!\"\n!#\"\"!\" \"#\"\"$\"\n!!\"%!\n!#\"%!\n\"!\"%!\"#\"%!\"$\"%!\n!!\"%#\n!#\"%#\n\"#\"%#\n\"!\"%#\n\"$\"%#\nDiffusion-based Spatial EncodingDistance-based Spatial EncodingRelative Temporal EncodingNodeEncoding\n…\n!!\"\n!#\"\n!#\"$#\n\"!\"\"#\"\n\"!\"$#\"#\"$#\n\"%\"$#\n+                    +                   = +                    +                   = +                    +                   = +                    +                   = +                    +                   = +                    +                   = +                    +                   = +                    +                   = \nTransformer Module\n………Multi-head Attention……Multi-head Attention……Mean Pooling\nPooling Module\nNegative Sampling\nInput Node Encoding Node Embedding\nEdge Embedding\nPositive Edge Embedding\nNegative Edge Embedding\n0.8310.370BCE Loss\nFully Connected\nScoring Module\nAnomaly ScorePseudo Label\nFig. 2. The overall framework of TADDY . The framework is composed of four components: edge-based substructure sampling, spatial-temporal node\nencoding, dynamic graph transformer, and discriminative anomaly detector. Here we regard the edge in red {vt\n1,vt\n2}as a target edge and exhibit\na running example. In edge-based substructure sampling, the target nodes (in yellow) and contextual nodes (in green) from multiple timestamps\nare sampled to construct the substructure node set, where neighboring node number kand time window size τ are both set to be 3. Then, three\ntypes of encoding are computed for each node and further fused into the node encoding. Taking the node encoding as input data, the dynamic\ngraph transformer learns latent node embedding with attention layers and leverages mean pooling to calculate the edge embedding. Finally, in\nthe discriminative anomaly detector, the negative edges are acquired by negative sampling. The scoring module computes the anomaly scores for\npositive and negative edges. The whole framework is trained with a binary cross-entropy loss in an end-to-end manner.\nThen, we sort the connectivity vectorsetgt and select the top-\nk nodes with the larger values to form the contextual node\nset U(etgt). Note that the target nodes themselves should be\nexcluded when selecting the top- k connectivity nodes. Fi-\nnally, the sampled node set for substructure can be denoted\nas the union of contextual node set and target nodes, which\ncan be formalized as S(etgt) ={v1,v2,U(etgt)}.\nAccording to the diffusion-based sampling, we can gen-\nerate the contextual nodes from a single static graph. How-\never, for dynamic graphs, multiple timestamps should be\nconsidered to capture the dynamic evolving. Here, we sim-\nply extend the static sampling method to dynamic graphs.\nGiven a target edge et\ntgt = (vt\n1,vt\n2) at timestamp t, we\nconsider a sequence of graphs Gt\nτ = {Gt−τ+1,··· ,Gt}\nwith length τ, where the time window size τ is a hyper-\nparameter and determines the receipt ﬁelds on time axis.\nWith a sliding window mechanism, TADDY is able to cap-\nture dynamic evolving between timestamps (t−τ+ 1)and\nt. Then, for each Gi ∈Gt\nτ, we calculate the diffusion matrix\nSi and acquire the corresponding connectivity vector si\net\ntgt\n.\nBy picking the top- k nodes and adding the target nodes,\nthe substructure node set of the i-th timestamp can be\nsampled as Si(et\ntgt). By integrating the node set of multiple\ntimestamps together, we can obtain the ﬁnal substructure\nnode set S(et\ntgt) =⋃t\ni=t−τ+1 Si(et\ntgt).\n4.2 Spatial-temporal Node Encoding\nUnlike image and attributed graph data where each data\ninstance (e.g., image patch or node) has its raw features,\nthe dynamic graphs we study in this paper are often\nunattributed, which indicates that it is hard to ﬁnd naturally\nappropriate data as the input of neural network models.\nThis raises the question of how to construct an informative\nencoding as network input from unattributed dynamic graphs .\nSimilar to the concept of one-hot word encoding in NLP ,\nan available solution is to use identity node encoding as\nthe raw node feature, where a unique one-hot vector repre-\nsents each node. However, identity node encoding has two\nlimitations. First, the one-hot encoding is unable to contain\nenough structural and temporal information. The one-hot\nencoding only indicates the nodes’ identity, but hard to\nexpress its structural roles and temporal status. Second, the\nidentity node encoding is not friendly to large-scale and\nnode-changing dynamic graphs. Third, the ﬁxed dimension\ncannot adapt to the dynamic changing set of nodes which is\na common situation in dynamic graphs.\nInspired by the positional encoding in Transformer [20],\nwe introduce a novel spatial-temporal node encoding for\ndynamic graph transformers. The proposed node encoding\nconsists of three components, namely diffusion-based spa-\ntial encoding, distance-based spatial encoding, and relative\ntemporal encoding. The two terms of spatial encoding repre-\nsent the structural role of each node from a global and local\nperspective respectively. The temporal encoding term, dif-\nferently, provides the temporal information of each element\nin the substructure node set. To the end, the three encoding\nterms are fused as the input node encoding which contains\ncomprehensive spatial-temporal information. Note that we\ngenerate the encoding by learnable linear projections instead\nof frequency-aware sin/cos functions used in [20]. The rea-\nson is that the learnable functions are more ﬂexible to model\nthe correlations between different timestamps or positions.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\nIn the rest of this subsection, we discuss the three en-\ncoding terms sequentially and then introduce the encoding\nfusion operation.\n4.2.1 Diffusion-based Spatial Encoding\nAs is introduced in Section 4.1, graph diffusion provides a\nglobal view of the structural role of each node. With the edge\nconnectivity vector setgt computed by Eq. (4), it is easy to ac-\nquired the strength of connectivity between the target edge\nand contextual node. Such a property inspires us to design\na spatial encoding that bases on graph diffusion. To prevent\nthe indistinguishable encoding caused by similar diffusion\nvalues, we do not adopt the raw diffusion values directly\nbut use a rank-based encoding. Speciﬁcally, for each node\nin a single-timestamp substructure node set vi\nj ∈Si(et\ntgt),\nwe sort nodes according to their diffusion values and adopt\nthe ranking as the data source. According to the ranks,\nwe compute the diffusion-based spatial encoding with a\nlearnable encoding function (a single-layer linear mapping),\nwhich is similar to the learned positional encoding in [20],\n[41]. The deﬁnition of the diffusion-based spatial encoding\nis given as:\nxdiﬀ(vi\nj) =linear(rank(si\netgt [idx(vi\nj)])) ∈Rdenc, (5)\nwhere idx(·), rank(·) and linear(·) are the index enquiring\nfunction, ranking function and learnable linear mapping\nrespectively, and denc is the dimension of node encoding.\n4.2.2 Distance-based Spatial Encoding\nWhile the diffusion-based spatial encoding capture the\nglobal structural information, the local roles of each node\nshould also be considered. Since the transformer model does\nnot take the graph structure (e.g., adjacency matrix) as input\nlike GNNs, we design a distance-based spatial encoding\nto represent the local connection around the target edge.\nConcretely, for each node in a single-timestamp substructure\nnode set vi\nj ∈Si(et\ntgt), we denote its distance to the target\nedge as the data source for encoding. The distance to the\ntarget edge can be decomposed into the minimum value of\nthe relative distances to the two target nodes. For the target\nnodes themselves, the distances are denoted as 0. A single-\nlayer linear mapping is served as the learnable encoding\nfunction here, which is the same as the diffusion-based\nencoding. Formally, the distance-based spatial encoding can\nbe expressed as:\nxdist(vi\nj) =linear(min(dist(vi\nj,vi\n1),dist(vi\nj,vi\n2)) ∈Rdenc,\n(6)\nwhere dist(·), min(·) and linear(·) are the relative distance\ncomputing function, minimum value function and learnable\nlinear mapping respectively, and denc is the dimension of\nnode encoding.\n4.2.3 Relative Temporal Encoding\nThe temporal encoding is to represent the temporal infor-\nmation of each node in the substructure node set. Instead\nof the absolute encoding in [20], we consider a relative\nencoding for dynamic graphs. Concretely, for each node in\nthe substructure node set vi\nj ∈Si(et\ntgt), the data source for\nrelative time encoding is deﬁned as the difference between\nthe occurring time t of target edge and the current time of\ntimestamp i. The motivation behind this is that our main\ntask is to predict the legality of the target edge, so the\nrelative time to the target edge is a more signiﬁcant factor for\nanomaly detection. Similar linear mapping is also applied as\nthe encoding function, and the formal expression of relative\ntemporal encoding is given as:\nxtemp(vi\nj) =linear(∥t−i∥) ∈Rdenc, (7)\nwhere ∥·∥ and linear(·) are the relative time computing\nfunction and the learnable linear mapping respectively, and\ndenc is the dimension of node encoding.\n4.2.4 Encoding Fusion\nAfter computing the three terms of encoding, we fuse them\nas the input node encoding of the downstream transformer\nmodel. For the sake of running efﬁciency, we deﬁne the\nfused node encoding as the summation of three encoding\nterms rather than concatenating them into a vector with\na higher dimension. The encoding fusion is formalized as\nfollows:\nx(vi\nj) =xdiﬀ(vi\nj) +xdist(vi\nj) +xtemp(vi\nj) ∈Rdenc. (8)\nFinally, given a target edge et\ntgt, we calculate the encod-\ning of each node in its substructure node set, and stack them\ninto a encoding matrix which represents the property ofet\ntgt.\nThe encoding matrix is represented by:\nX(et\ntgt) =\n⨁\nvi\nj∈S(et\ntgt)\n[x(vi\nj)]T ∈R(τ(k+2))×denc, (9)\nwhere ⨁ is the concatenation operation and [·]T is the\ntranspose operation.\n4.3 Dynamic Graph Transformer\nTo learn knowledge from dynamic graphs, the neural net-\nwork model should consider both of the spatial structure\ninformation and temporal dynamic information. In most\nsituations, the spatial information and temporal informa-\ntion are coupled and should be captured simultaneously\nfor efﬁcient anomaly detection. Taking the dynamic graph\nin Figure 2 as an example, the node vt\n1 and vt\n2 have an\nconnection at time t, a precursor is that their communities\nhave several connections in the previous timestamps, e.g.,\nut−1\n1 -ut−1\n2 and ut−2\n1 -ut−2\n2 . For the design of dynamic graph\nencoder, a question that arises is: how can a neural network-\nbased encoder consider the spatial and temporal information\nsimultaneously? A general solution in the existing works\nis to use hybrid networks stacked by spatial module and\ntemporal module. The spatial/temporal modules are em-\nployed in such hybrid models to capture spatial/temporal\ninformation respectively and separately. For instance, in\nStrGNN, the GCN serves as a spatial module, and GRU\nprocesses the output of GCN from different timestamps to\ncapture temporal information. A limitation of such hybrid\nmodels is that they may miss some information that crosses\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nspatial and temporal domains, which further leads to a sub-\noptimal solution.\nTo learn the spatial and temporal knowledge in the\ndynamic graphs, we propose to adopt a transformer model\nsolely as the encoder. With the multiple timestamps of node\nencoding as input, the dynamic graph transformer can simul-\ntaneously capture both spatial and temporal features with a\nsingle encoder. The dynamic graph transformer is composed\nof two modules: the transformer module and the pooling\nmodule. With the transformer module, the abundant cross-\ndomain knowledge is captured by the attention mechanism,\nand the ﬁnal attention layer generates the informative node\nembeddings. After that, the pooling module aggregates the\nembedding of all nodes in the substructure node set into an\nembedding vector for the target edge.\n4.3.1 Transformer Module\nThe target of the transformer module is to aggregate the\nencodings of nodes within a substructure node set into node\nembeddings. To this end, a number of attention layers are\nutilized to exchange the information of different nodes. To\nbe concrete, a single attention layer can be written as:\nH(l) = attention\n(\nH(l−1)\n)\n= softmax\n(\nQ(l)K(l)⊤\n√demb\n)\nV(l),\n(10)\nwhere H(l) and H(l−1) is the output embedding of the land\n(l−1) -th layer, demb is the dimension of node embedding,\nand Q(l), K(l), V(l) ∈R(τ(k+2))×demb are the query matrix,\nkey matrix and value matrix for feature transformation and\ninformation exchange. Concretely, the Q(l), K(l) and V(l)\nare computed by:\n\n\n\nQ(l) = H(l−1)W(l)\nQ ,\nK(l) = H(l−1)W(l)\nK ,\nV(l) = H(l−1)W(l)\nV ,\n(11)\nwhere W(l)\nQ ,W(l)\nK ,W(l)\nV ∈ Rdemb×demb are the learnable\nparameter matrices of the l-th attention layer. In an attention\nlayer, Q(l) and K(l) calculate the contributions of different\nnodes’ embeddings, whileV(l) projects the input into a new\nfeature space. Equation (10) combines them and acquires\nthe output embedding of each node by aggregating the\ninformation of all nodes adaptively.\nIn our transformer module, the input of the transformer\nmodule H(0) is deﬁned as the encoding matrix of the target\nedge X(et\ntgt), and here we simply set d = demb = denc to\nalign the dimension. The output of the ﬁnal attention layer\nH(L) is denoted as the output node embedding matrix Z\nof the transformer module, where each row represents an\nembedding vector of the corresponding node.\n4.3.2 Pooling Module\nThe target of pooling module is to transfer the embeddings\nof nodes in substructure Z into a target edge embedding\nvector z(et\ntgt). Here we utilize the average pooling operation\nas our pooling function, which has been applied in previous\nworks [36]. The pooling function is formalized as:\nz(et\ntgt) =pooling(Z) =\nns∑\nk=1\n(Z)k\nns\n, (12)\nwhere (Z)k is the k-th row of Z, and ns = τ(k+ 2)is the\nnumber of nodes of the substructure node set S(et\ntgt).\n4.4 Discriminative Anomaly Detector\nAfter the edge embedding is acquired, the target of anomaly\ndetection is to learn an anomaly score for each edge in the\ndynamic graph. Here, we consider an end-to-end frame-\nwork where a neural network-based anomaly detector com-\nputes the anomaly score. However, in our learning setting,\nthere is not any ground-truth anomaly sample in the train-\ning set. Such a situation brings a new challenge: How to\nlearn an anomaly detector without any given anomalous sample?\nOur solution is to generate pseudo anomalous edges via a\nnegative sampling strategy and train the anomaly detector\nwith the existing edges in the training set (positive edges)\nas well as the pseudo anomalous edges (negative edges)\ntogether.\nA simple negative sampling strategy is performed in our\nframework. For each timestamp of graph whose number\nof edges is mt, we randomly sample the equal number\nof node pairs as the candidates of negative pairs. Then,\nwe check all these node pairs to ensure that they do not\nbelong to the existing normal edge set in all the training\ntimestamps. We resample a new pair and perform validation\nfor each illegal node pair until the node pair is valid. After\nnegative sampling, we use context sampling to acquire the\nsubstructure node set of each negative edge and perform\nspatial-temporal node encoding. Then, the encoding is fed\ninto the dynamic graph transformer model to obtain the\nembedding of the negative edge.\nThe anomaly detector is constructed to discriminate the\npositive and negative edge embeddings. A fully connected\nneural network layer with Sigmoid activation is served as\nthe scoring module which computes the anomaly scores of\nedge embeddings, which is formalized by\nf(e) =Sigmoid\n(\nz(e)wS + bS\n)\n(13)\nwhere f(e) and z(e) is the anomaly score and edge em-\nbedding of edge e respectively, Sigmoid(·) is the Sigmoid\nactivation function, wS ∈Rdemb and bS ∈R are the weights\nand bias parameters of fully connected neural network layer\nrespectively.\nWe employ a binary cross-entropy loss function with\npseudo labels to train the framework in an end-to-end\nmanner. For the positive edges, we expect them to have\na small anomaly score, hence their pseudo label is 0; for\nthe negative edges, conversely, the pseudo label is 1. For a\ntraining timestamp Gt = (Vt,Et) whose edge number is mt,\nthe loss function is given as\nL= −\nmt\n∑\ni=1\nlog\n(\n1 −f(epos,i)\n)\n+ log\n(\nf(eneg,,i)\n)\n(14)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nAlgorithm 1The Overall Training Procedure of TADDY\nInput: Training set of dynamic graph: G = {Gt}T\nt=1, Num-\nber of training epochs: I, Number of sampled contextual\nnodes: k, Size of time window: τ.\n1: Randomly initialize the parameters of encoding linear\nmappings, transformer model and scoring function\n2: for i∈1,2,··· ,I do\n3: for timestamp Gt = (Vt,Et) ∈{Gt}T\nt=τ do\n4: Sample negative edge set Et\nn ∼Pn(Et) by negative\nsampling strategy\n5: for e∈Et ∪Et\nn do\n6: Set eas the target edge and sample its substruc-\nture node set S(e) with τ(k+ 2)nodes\n7: Calculate node encoding matrix X(e) via Equa-\ntion (5) - (8)\n8: Calculate edge embedding vector z(e) via Equa-\ntion (10) - (12)\n9: Calculate anomaly score f(e) via Equation (13)\n10: end for\n11: Calculate loss function Lvia Equation (14)\n12: Back propagation and update the parameters\n13: end for\n14: end for\nwhere epos,i ∈Et is the i-th positive edge and eneg,,i ∈Et\nn ∼\nPn(Et) is the i-th negative edge sampled by the negative\nsampling strategy.\nTo the end, the overall training procedure of our TADDY\nframework is depicted in Algorithm 1. The framework is\ntrained in an iterative and end-to-end manner. In each\niteration, different negative edges are sampled to prevent\ntraining bias and over-ﬁtting. For all positive and negative\nedges, we perform substructure sampling, node encoding,\ntransformer embedding and anomaly score computing se-\nquentially. Finally, the parameters are updated by back\npropagation under the supervision of binary cross-entropy\nloss function. When the framework is well trained, the\nanomaly scores for test edges can be obtained by executing\nthe line 6-9 in Algorithm 1.\n4.5 Complexity Analysis\nIn this subsection, we analyze the time complexity of each\ncomponent in TADDY framework. For edge-based substruc-\nture sampling, the complexity is mainly caused by the\ncomputation of graph diffusion, which is O(T˜n2) where ˜n\nis the average number of nodes for graph timestamps and\nT is the number of timestamps. In spatial-temporal node\nencoding and dynamic graph transformer, we process all\nnodes in the substructure node set for each target edge,\nwhich brings a complexity of O(τk) for one edge. Therefore,\nfor Iiterations, the total time complexity is O(τkmI) where\nm is the number of edges. For discriminative anomaly\ndetector, the time complexity is O(mI), which is far less\nthan the other components and can be ignored. To sum up,\nthe overall time complexity is O(τkmI + T˜n2).\n5 E XPERIMENTS\nIn this section, we evaluate the performance of the proposed\nTADDY via extensive experimental studies. We ﬁrst intro-\nTABLE 2\nThe statistics of the datasets. For each dataset, the number of nodes,\nthe number of edges, and the average degree are reported.\nDataset ♯nodes ♯edges Avg. Degree\nUCI Messages 1,899 13,838 14.57\nDigg 30,360 85,155 5.61\nEmail-DNC 1,866 39,264 42.08\nBitcoin-Alpha 3,777 24,173 12.80\nBitcoin-OTC 5,881 35,588 12.10\nAS-Topology 34,761 171,420 9.86\nduce the setup for our experiments. We demonstrate the\nexperimental results in the rest three subsections, including\nperformance comparison, parameter study, and ablation\nstudy.\n5.1 Experimental Setup\n5.1.1 Datasets\nWe evaluate our proposed TADDY framework on six real-\nworld benchmark datasets of dynamic graphs. The statistics\nof the datasets are given in Table 2, and the detailed descrip-\ntions are demonstrated as follows.\nUCI Messages1 [42] is a social network dataset collected\nfrom an online community of students at University of\nCalifornia, Irvine. In the constructed dynamic graph, each\nnode indicates a user, and each edge represents a message\nbetween two users.\nDigg2 [43] is a network dataset collected from a news\nwebsite digg.com. In Digg dataset, each node is a website\nuser, and each edge indicates that one user replies to another\nuser.\nEmail-DNC3 [44] is network of emails in the 2016 Demo-\ncratic National Committee email leak. Each node corre-\nsponds to a person in the United States Democratic Party,\nand each edge denotes that a person has sent an email to\nanother person.\nBitcoin-Alpha4 and Bitcoin-OTC5 [45], [46] are two\nwho-trusts-whom networks of bitcoin users trading on\nthe platforms from www.btc-alpha.com and www.bitcoin-\notc.com respectively. In these two datasets, the nodes are\nthe users from the platform, and an edge appears when one\nuser rates another on the platform.\nAS-Topology6 [47] is a network connection dataset col-\nlected from autonomous systems of the Internet. Each node\nin the graph corresponds to an autonomous system, and\neach edge indicates a connection between two autonomous\nsystems.\nWe pre-process the datasets following previous works\n[9], [10]. The edges in each dataset are annotated with\ntimestamps. The repeated edges in the edge stream are\nremoved in the pre-processing phase. Since there is no\nground-truth anomalous edge in the original datasets, we\nfollow the approach used in [9] to inject anomalous edges\nin each dataset. To be concrete, the training data is totally\n1. http://konect.cc/networks/opsahl-ucsocial\n2. http://konect.cc/networks/munmun digg reply\n3. http://networkrepository.com/email-dnc\n4. http://snap.stanford.edu/data/soc-sign-bitcoin-alpha\n5. http://snap.stanford.edu/data/soc-sign-bitcoin-otc\n6. http://networkrepository.com/tech-as-topology\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nclean. For each snapshot Gt in the test set, we randomly\nlink pA ×mt pairs of disconnected nodes as anomalous\nedges, where pA is the anomaly proportion indicating the\npercentage of anomalous edges in each snapshot, and mt is\nthe (original) number of edges in Gt.\n5.1.2 Baselines\nWe compared TADDY framework against six state-of-the-\nart baselines that can be categorized into two groups: graph\nembedding methods and deep dynamic graph anomaly\ndetection methods.\nDeepWalk [48] is a random walk-based method for\ngraph embedding. It generates random walks with a given\nlength starting from a target node and uses a Skip-gram-like\nmanner to learn embedding for unattributed graphs.\nnode2vec [49] considers breadth-ﬁrst traversal and\ndepth-ﬁrst traversal when generating random walks. The\nSkip-gram technology is also employed to learn node em-\nbedding in node2vec.\nSpectral Clustering [50] learns node embedding by\nmaximizing the similarity between bodes in neighborhood.\nThe intuition behind this method is to preserve the local\nconnection relationship in graphs.\nNetWalk [9] is a representative anomaly detection\nmethod for dynamic graph. It utilizes a random walk-based\napproach to generate contextual information and learns\nnode embedding with an auto-encoder model. The node\nembeddings are updated incrementally over time via a\nreservoir-based algorithm. The anomaly is detected using\na dynamic-updated clustering on the learned embedding.\nAddGraph [10] is an end-to-end dynamic graph\nanomaly detection approach. It leverages a GCN module to\ncapture spatial information, and employs a GRU-attention\nmodule to extract short- and long- term dynamic evolving.\nStrGNN [13] is an end-to-end graph neural network\nmodel for detecting anomalous edges in dynamic graphs.\nIt leverages an h-hop enclosing subgraph as the network’s\ninput and combines GCN and GRU to learn structural-\ntemporal information for each edge.\nFor the graph embedding methods, the K-means\nclustering-based anomaly detector, which is presented in\nNetWalk [9] is utilized to detect anomalies based on the\nlearned node embeddings.\n5.1.3 Experimental Design\nIn our experiments, each dataset is divided into two sub-\nsets: the ﬁrst 50% of timestamps is denoted as training\nset, while the latter 50% as test set. We consider three\ndifferent anomaly proportions pA, 1%, 5%, and 10%, when\ninjecting the anomalous data into the test set. To measure\nthe performance of the proposed framework as well as the\nbaselines, ROC-AUC (AUC for short) is employed as our\nprimary metric. The ROC curve indicates a plot of true\npositive rate against false positive rate where anomalous\nlabels are viewed as “positive”. AUC is deﬁned as the area\nunder the ROC curve, which indicates the probability that a\nrandomly selected anomalous edge is ranked higher than a\nnormal edge. The value range of AUC is 0 to 1, and a larger\nvalue represents a better anomaly detection performance.\n5.1.4 Parameter Settings\nAll the parameters can be tuned by 5-fold cross-validation\non a rolling basis. For edge-based substructure sampling, we\nset the number kof contextual nodes to be5 and τis selected\nfrom 1 to 4. We use PPR diffusion in our experiments, which\nis computed by Eq. (2). For spatial-temporal node encoding,\nthe dimension of encoding denc is 32, which is the same\nas demb in Dynamic Graph Transformer. The number of\nattention layers is 2 for all the datasets, and the number of\nattention heads is 2. The framework is trained by Adam op-\ntimizer with a learning rate of0.001. We train UCI Messages,\nBitcoin-Alpha, and Bitcoin-OTC datasets with 100 epochs\nand the rest three datasets for 200 epochs. The snapshot size\nis set to be 1,000 for UCI Messages and Bitcoin-OTC, 2,000\nfor Email-DNC and Bitcoin-Alpha, and 6,000 for Digg and\nAS-Topology, respectively.\n5.1.5 Computing Infrastructures\nThe proposed method is implemented using PyTorch 1.7.1\n[51]. All experiments are conducted on a computer server\nwith four Quadro RTX 6000 (24GB memory each) GPUs, an\nIntel Xeon Silver 4214R (2.40 GHz) CPU and 64 GB of RAM.\n5.2 Anomaly Detection Results\nIn this subsection, we report anomaly detection perfor-\nmance and compare our proposed TADDY framework with\nthe baseline methods. The anomaly detection performance\ncomparison of average AUC on all test timestamps is\ndemonstrated in Table 3. We summarize the following ob-\nservations for the results:\n• The proposed TADDY framework consistently out-\nperforms all baselines on the six dynamic graph\ndatasets with different anomaly proportions. Com-\npared to the baseline method with the best results,\nTADDY reaches a performance gain of 4.49% on\nAUC averagely. The main reason is that TADDY\nextracts the spatial-temporal information by con-\nstructing informative node encoding and captures\nstructural dynamic and temporal dynamic simulta-\nneously with a transformer encoder.\n• Compared to three graph embedding-based meth-\nods, the deep dynamic graph anomaly detec-\ntion methods, NetWalk, AddGraph, StrGNN and\nTADDY, always have a more competitive perfor-\nmance. We attribute this performance advantage to\nthe leverage of temporal information. By considering\nthe interaction in previous timestamps, these meth-\nods learn the dynamic evolving in graphs.\n• TADDY has a larger performance gain when the\nanomalies are scarce. Concretely, the average perfor-\nmance gap on AUC between TADDY and the best\nbaseline under 1% anomaly proportion is 5.35%,\nwhile the gaps under 5% and 10% anomaly propor-\ntions are 3.69% and 4.43%, respectively. A possible\nreason is that we train the framework with an efﬁ-\ncient negative sampling strategy, which ensures the\nrobustness under different anomaly proportion in the\ntest set.\n• On the two Bitcoin datasets, TADDY achieves more\nremarkable results. Compared to the best baseline,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nTABLE 3\nAnomaly detection performance comparison reported in AUC measure. The upper three baselines belong to graph embedding methods, and the\nmiddle three baselines belong to deep dynamic graph anomaly detection methods. The best performing method in each experiment is in bold.\nMethods UCI Messages Digg Email-DNC\n1% 5% 10% 1% 5% 10% 1% 5% 10%\nnode2vec 0.7371 0.7433 0.6960 0.7364 0.7081 0.6508 0.7391 0.7284 0.7103\nSpectral Clustering 0.6324 0.6104 0.5794 0.5949 0.5823 0.5591 0.8096 0.7857 0.7759\nDeepWalk 0.7514 0.7391 0.6979 0.7080 0.6881 0.6396 0.7481 0.7303 0.7197\nNetWalk 0.7758 0.7647 0.7226 0.7563 0.7176 0.6837 0.8105 0.8371 0.8305\nAddGraph 0.8083 0.8090 0.7688 0.8341 0.8470 0.8369 0.8393 0.8627 0.8773\nStrGNN 0.8179 0.8252 0.7959 0.8162 0.8254 0.8272 0.8775 0.9103 0.9080\nTADDY 0.8912 0.8398 0.8370 0.8617 0.8545 0.8440 0.9348 0.9257 0.9210\nMethods Bitcoin-Alpha Bitcoin-OTC AS-Topology\n1% 5% 10% 1% 5% 10% 1% 5% 10%\nnode2vec 0.6910 0.6802 0.6785 0.6951 0.6883 0.6745 0.6821 0.6752 0.6668\nSpectral Clustering 0.7401 0.7275 0.7167 0.7624 0.7376 0.7047 0.6685 0.6563 0.6498\nDeepWalk 0.6985 0.6874 0.6793 0.7423 0.7356 0.7287 0.6844 0.6793 0.6682\nNetWalk 0.8385 0.8357 0.8350 0.7785 0.7694 0.7534 0.8018 0.8066 0.8058\nAddGraph 0.8665 0.8403 0.8498 0.8352 0.8455 0.8592 0.8080 0.8004 0.7926\nStrGNN 0.8574 0.8667 0.8627 0.9012 0.8775 0.8836 0.8553 0.8352 0.8271\nTADDY 0.9451 0.9341 0.9423 0.9455 0.9340 0.9425 0.8953 0.8952 0.8934\nthe average AUC gain on Bitcoin-Alpha and Bitcoin-\nOTC is 6.42%, which is signiﬁcantly higher than the\nAUC gain on the rest datasets ( 3.53%). The reason\nfor such remarkable advantages is that the abnormal-\nity of Bitcoin transaction is more closely related to\nspatial-temporal dynamic, and TADDY can success-\nfully capture such dynamic by comprehensive node\nembedding and attention mechanism.\n5.3 Parameter Sensitivity\nIn this subsection, we investigate the inﬂuence of hyper-\nparameters on TADDY, including the number of contextual\nnodes and time window size in edge-based substructure\nsampling, the dimension of encoding/embedding and the\nnumber of attention layers in dynamic graph transformer,\nand the ratio of training data. Here we carry out the ex-\nperiments on three datasets (UCI Messages, Bitcoin-Alpha\nand Bitcoin-OTC). In these experiments, we keep the other\nparameter as default, and the performance is examined\nunder a 10% anomaly proportion setting.\n5.3.1 Parameters of Edge-based Substructure Sampling\nTo evaluate the effect of number of contextual nodes k and\ntime window size τ in edge-based substructure sampling,\nwe set the range of k to {1,2,3,4,5,6,7,8,9,10}and the\nrange of τ to {1,2,3,4}. The sensitivity of k and τ is\nexhibited in Figure 3. According to the results, we have the\nfollowing observations.\nWhen the contextual node number kis extremely small,\nthe anomaly detection performance is relatively pool. With\nthe growth of k, there is a signiﬁcant boost in AUC. When\nk >5, the detection performance tends to be stable, with\nk getting larger. The performance trend indicates that con-\ntextual node sets with a sufﬁcient size are signiﬁcant to\nanomaly detection since the anomalous property of edges\nhighly relies on their neighboring local structure. However,\nwhen considering an excessive number of contextual nodes,\nthe performance gain is minor. However, a large kis harm-\nful to the running efﬁciency due to the linear relationship\nbetween time complexity and k. Consequently, we ﬁx the\nvalue of kto 5 for the consideration of the trade-off between\nperformance and efﬁciency.\nFor different datasets, the appropriate size τ of the time\nwindow is different. For instance, a smaller time window is\nbeneﬁcial to UCI Messages, while the two Bitcoin datasets\nneed a more extended time horizon. The main reason is that\nthe temporal reliance of edges in dynamic graphs highly\ndepends on the datasets. When the edge appearance has a\nlong-term dependency on the previous graph evolving, a\nlarger time window is needed to capture the dependency.\nFor other datasets like UCI Messages where anomalies are\nrelated to the latest snapshots, a wide time window may\nresult in noisy and redundant input for TADDY framework.\nTherefore, we select the best τ value for each dataset in our\nexperiments.\n5.3.2 Parameters of Dynamic Graph Transformer\nWe further investigate the effort of encoding/embedding\ndimension d and the number of layers L in Transformer\nmodel. The value of d is selected from {4,8,16,32,64}\nand which of L is selected from {1,2,3}. The results are\nsummarized in Figure 4.\nIn Bitcoin-Alpha and Bitcoin-OTC, the AUC values in-\ncrease gradually from d = 4to d = 16, and then go steady\nafter d = 32. Such observation demonstrates that when d\nis small, the model may miss useful information. For UCI\nMessages, d= 8seems to be the best choice. When dgetting\nlarger, there is no signiﬁcant performance degradation. Our\nexplanation is that when dis too large, the noisy information\nwould be captured by the transformer model.\nCompared to d, the number of layers has a limited im-\npact on performance. An exception is Bitcoin-Alpha, whose\nAUC drops when L = 1, which indicates that a sufﬁcient\nnumber of layers can bring adequate interaction among\nstructuring nodes. As such, we ﬁx L = 2 for each dataset\nto balance the running speed and detection performance.\n5.3.3 Training ratios\nIn this experiment, we discuss the performance of TADDY\nframework using training data with different ratios. The\nrange of training ratio is {20%,30%,40%,50%,60%}and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\nNumber of Contextual Node 12345678910\nTime Window Size\n1\n2\n3\n4\n    AUC    \n0.73\n0.76\n0.79\n0.82\n0.85\n(a) UCI Messages\nNumber of Contextual Node 12345678910\nTime Window Size\n1\n2\n3\n4\n    AUC    \n0.87\n0.89\n0.91\n0.93\n0.95 (b) Bitcoin-Alpha\nNumber of Contextual Node 12345678910\nTime Window Size\n1\n2\n3\n4\n    AUC    \n0.88\n0.90\n0.92\n0.94 (c) Bitcoin-OTC\nFig. 3. The sensitivity of contextual node numberkand time window sizeτ on three datasets. The vertical axis represents the AUC values of TADDY\nwith different kand τ. A warmer color indicates a higher AUC value.\nEncoding/Embedding Dimension48163264\nNumber of Layers\n1\n2\n3\n    AUC    \n0.81\n0.82\n0.83\n0.84\n(a) UCI Messages\nEncoding/Embedding Dimension48163264\nNumber of Layers\n1\n2\n3\n    AUC    \n0.91\n0.92\n0.93\n0.94 (b) Bitcoin-Alpha\nEncoding/Embedding Dimension48163264\nNumber of Layers\n1\n2\n3\n    AUC    \n0.92\n0.93\n0.94 (c) Bitcoin-OTC\nFig. 4. The sensitivity of encoding/embedding dimension d and the number of layers L on three datasets. The vertical axis represents the AUC\nvalues of TADDY with differentdand L. A warmer color indicates a higher AUC value.\n20% 30% 40% 50% 60%\nTraining Ratio\n0.725\n0.750\n0.775\n0.800\n0.825\n0.850AUC\n(a) UCI Messages\n20% 30% 40% 50% 60%\nTraining Ratio\n0.930\n0.935\n0.940\n0.945AUC\n (b) Bitcoin-Alpha\n20% 30% 40% 50% 60%\nTraining Ratio\n0.930\n0.935\n0.940\n0.945AUC (c) Bitcoin-OTC\nFig. 5. AUC values of TADDY on three datasets with different training ratios. The circular markers indicate the results which is viewed as outliers.\nother parameters are set to default. Figure 5 displays the\nresults on three datasets.\nWe observe from Figure 5 that the AUC values increase\nsmoothly when the training ratio goes larger, demonstrating\nthat more training data provides a better supervision signal\nfor training. We can also ﬁnd that even if the training data\nis rate ( 20%), our framework still has a competitive perfor-\nmance, especially on two Bitcoin datasets. This observation\nshows that TADDY can learn an informative representation\neven trained with scarce data. Moreover, the variance of\nAUC decreases with the increase of the training ratio, which\nillustrates that our proposed framework tends to have a\nstable performance when training data is adequate.\n5.4 Ablation Study\nTo study the contribution of each component in the spatial-\ntemporal node encoding towards the overall performance,\nwe conduct the ablation study of the proposed TADDY\nTABLE 4\nAblation study for TADDY and its variants on three datasets.\nUCI\nMessages\nBitcoin-\nAlpha\nBitcoin-\nOTC\nTADDY 0.8370 0.9423 0.9262\nw/o diff. enc. 0.8304 0.9329 0.9153\nw/o dist. enc. 0.5362 0.5043 0.5187\nw/o temp. enc. 0.8399 0.9326 0.9021\nframework. In particular, We evaluate the following vari-\nants of the node encoding: w/o diff. enc., w/o dist.\nenc. and w/o temp. enc., where the diffusion-based\nspatial encoding, distance-based spatial encoding and rel-\native temporal encoding are discarded respectively when\nexcusing the encoding fusion. We perform the ablation\nstudy with 10% anomaly proportion for each dataset, and\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nall the parameters are set as default. Table 4 reports the\nproposed framework and its variants on three datasets. We\nhave the following observations according to the results:\n• The distance-based spatial encoding is the most crit-\nical term in node encoding. Without this term, the\nAUC values decrease sharply to about 50%, which\nindicates that the anomalies become indistinguish-\nable. This observation proves that the local structural\ninformation is signiﬁcant in detecting anomalies,\nwhich is also pointed out in previous works [13], [36].\n• The diffusion-based spatial encoding and relative\ntemporal encoding both have a minor contribution\nin detecting anomalies. In the vast majority of cases,\nremoving one of them would lead to a slight per-\nformance drop. We infer from the results that the\ndiffusion-based spatial encoding provides a global\nview for graph structure which has a minor relation\nto anomaly detection. Moreover, the temporal encod-\ning points out the occurrence time of neighborhoods\nwhich is relatively unimportant since the nodes have\nbeen selected to the substructure set.\n• In most of the cases, combining all of the three types\nof encoding has the highest AUC values, excepted\non UCI Messages dataset. This shows that using a\ncomprehensive spatial-temporal encoding is mean-\ningful to anomaly detection. As for the exception,\nwe guess that emphasizing the relative time distance\nmay lead to an over-ﬁtting on such a property in\nsome cases, which further results in a slight side\neffect on performance.\n6 C ONCLUSION\nIn this paper, we make the ﬁrst attempt to utilize trans-\nformer models for the graph anomaly detection problem\nin dynamic graph scenarios. We propose an end-to-end\nanomaly detection framework, TADDY, which is composed\nof four components: edge-based substructure sampling,\nspatial-temporal node encoding, dynamic graph trans-\nformer, and discriminative anomaly detector. Our frame-\nwork constructs an informative and comprehensive node\nencoding to better represent the roles of nodes in an evolv-\ning graph space and successfully captures the coupled\nspatial-temporal information within dynamic graphs with\na sole transformer model. Experiments on several real-\nworld datasets show that the proposed framework detects\nanomalies with high effectiveness in dynamic graphs and\noutperforms the existing methods signiﬁcantly.\nACKNOWLEDGEMENT\nThis research was supported in part by the Australian\nResearch Council (ARC) under a Future Fellowship No.\nFT210100097 and National Natural Science Foundation of\nChina project 61963004.\nREFERENCES\n[1] L. Wang, Z. Yu, F. Xiong, D. Yang, S. Pan, and Z. Yan, “Inﬂuence\nspread in geo-social networks: a multiobjective optimization per-\nspective,” IEEE TCYB, 2019.\n[2] S. Ji, S. Pan, E. Cambria, P . Marttinen, and P . S. Yu, “A survey on\nknowledge graphs: Representation, acquisition, and applications,”\nIEEE TNNLS, 2021.\n[3] Y. Zheng, R. Hu, S.-f. Fung, C. Yu, G. Long, T. Guo, and S. Pan,\n“Clustering social audiences in business information networks,”\nPattern Recognition, vol. 100, p. 107126, 2020.\n[4] Y. Gao, X. Li, H. Peng, B. Fang, and P . Yu, “Hincti: A cyber\nthreat intelligence modeling and identiﬁcation system based on\nheterogeneous information network,” IEEE TKDE, 2020.\n[5] D. Jin, Z. Yu, P . Jiao, S. Pan, D. He, J. Wu, P . S. Yu, and W. Zhang,\n“A survey of community detection approaches: From statistical\nmodeling to deep learning,” IEEE TKDE, 2021.\n[6] F. Xia, K. Sun, S. Yu, A. Aziz, L. Wan, S. Pan, and H. Liu, “Graph\nlearning: A survey,”IEEE Transactions on Artiﬁcial Intelligence, 2021.\n[7] H. Peng, R. Yang, Z. Wang, J. Li, L. He, P . Yu, A. Zomaya, and\nR. Ranjan, “Lime: Low-cost incremental learning for dynamic\nheterogeneous information networks,” IEEE Transactions on Com-\nputers, pp. 1–1, 2021.\n[8] P . Jiao, X. Guo, X. Jing, D. He, H. Wu, S. Pan, M. Gong, and\nW. Wang, “Temporal network embedding for link prediction via\nvae joint attention mechanism,” IEEE TNNLS, 2021.\n[9] W. Yu, W. Cheng, C. C. Aggarwal, K. Zhang, H. Chen, and\nW. Wang, “NetWalk: A ﬂexible deep embedding approach for\nanomaly detection in dynamic networks,” in SIGKDD, 2018, pp.\n2672–2681.\n[10] L. Zheng, Z. Li, J. Li, Z. Li, and J. Gao, “AddGraph: Anomaly\ndetection in dynamic graph using attention-based temporal gcn.”\nin IJCAI, 2019, pp. 4419–4425.\n[11] C. C. Aggarwal, Y. Zhao, and S. Y. Philip, “Outlier detection in\ngraph streams,” in ICDE. IEEE, 2011, pp. 399–409.\n[12] S. Ranshous, S. Harenberg, K. Sharma, and N. F. Samatova, “A\nscalable approach for outlier detection in edge streams using\nsketch-based approximations,” in SDM. SIAM, 2016, pp. 189–\n197.\n[13] L. Cai, Z. Chen, C. Luo, J. Gui, J. Ni, D. Li, and H. Chen,\n“Structural temporal graph neural networks for anomaly detection\nin dynamic graphs,” arXiv preprint arXiv:2005.07427, 2020.\n[14] C. Yang, L. Zhou, H. Wen, Z. Zhou, and Y. Wu, “H-VGRAE:\nA hierarchical stochastic spatial-temporal embedding method for\nrobust anomaly detection in dynamic networks,” arXiv preprint\narXiv:2007.06903, 2020.\n[15] H. Peng, J. Li, Y. Song, R. Yang, R. Ranjan, P . Yu, and L. He,\n“Streaming social event detection and evolution discovery in\nheterogeneous information networks,” ACM TKDD, 2021.\n[16] K. Sricharan and K. Das, “Localizing anomalous changes in time-\nevolving graphs,” in SIGMOD/PODS, 2014, pp. 1347–1358.\n[17] E. Manzoor, S. M. Milajerdi, and L. Akoglu, “Fast memory-\nefﬁcient anomaly detection in streaming heterogeneous graphs,”\nin SIGKDD, 2016, pp. 1035–1044.\n[18] D. Eswaran, C. Faloutsos, S. Guha, and N. Mishra, “Spotlight:\nDetecting anomalies in streaming graphs,” in SIGKDD, 2018, pp.\n1378–1386.\n[19] T. N. Kipf and M. Welling, “Semi-supervised classiﬁcation with\ngraph convolutional networks,” in ICLR, 2017.\n[20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nin NeurIPS, 2017, pp. 6000–6010.\n[21] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “BERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” in NAACL, Jun. 2019, pp. 4171–4186.\n[22] Z. Lan, M. Chen, S. Goodman, K. Gimpel, P . Sharma, and R. Sori-\ncut, “ALBERT: A lite bert for self-supervised learning of language\nrepresentations,” in ICLR, 2019.\n[23] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy,\nM. Lewis, L. Zettlemoyer, and V . Stoyanov, “RoBERTa: A ro-\nbustly optimized bert pretraining approach,” arXiv preprint\narXiv:1907.11692, 2019.\n[24] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov, and\nQ. V . Le, “XLNet: Generalized autoregressive pretraining for lan-\nguage understanding,” NeurIPS, vol. 32, pp. 5753–5763, 2019.\n[25] L. Liu, W. L. Hamilton, G. Long, J. Jiang, and H. Larochelle,\n“A universal representation transformer layer for few-shot image\nclassiﬁcation,” in ICLR, 2021.\n[26] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nECCV. Springer, 2020, pp. 213–229.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\n[27] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An image is worth 16x16 words:\nTransformers for image recognition at scale,” in ICLR, 2021.\n[28] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y. Wang, Y. Fu, J. Feng,\nT. Xiang, P . H. Torr, and L. Zhang, “Rethinking semantic segmenta-\ntion from a sequence-to-sequence perspective with transformers,”\nin CVPR, 2021.\n[29] K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang, A. Xiao,\nC. Xu, Y. Xu et al., “A survey on visual transformer,”arXiv preprint\narXiv:2012.12556, 2020.\n[30] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efﬁcient trans-\nformers: A survey,” arXiv preprint arXiv:2009.06732, 2020.\n[31] S. Yun, M. Jeong, R. Kim, J. Kang, and H. J. Kim, “Graph trans-\nformer networks,” NeurIPS, vol. 32, pp. 11 983–11 993, 2019.\n[32] Z. Hu, Y. Dong, K. Wang, and Y. Sun, “Heterogeneous graph\ntransformer,” in WWW, 2020, pp. 2704–2710.\n[33] Y. Rong, Y. Bian, T. Xu, W. Xie, Y. Wei, W. Huang, and J. Huang,\n“Self-supervised graph transformer on large-scale molecular\ndata,” NeurIPS, vol. 33, 2020.\n[34] J. Zhang, H. Zhang, C. Xia, and L. Sun, “Graph-bert: Only atten-\ntion is needed for learning graph representations,” arXiv preprint\narXiv:2001.05140, 2020.\n[35] Y. Liu, S. Pan, M. Jin, C. Zhou, F. Xia, and P . S. Yu, “Graph self-\nsupervised learning: A survey,” arXiv preprint arXiv:2103.00111 ,\n2021.\n[36] Y. Liu, Z. Li, S. Pan, C. Gong, C. Zhou, and G. Karypis, “Anomaly\ndetection on attributed networks via contrastive self-supervised\nlearning,” IEEE TNNLS, 2021.\n[37] J. Klicpera, S. Weiß enberger, and S. G ¨unnemann, “Diffusion\nimproves graph learning,” in NeurIPS, vol. 32, 2019, pp. 13 333–\n13 345.\n[38] K. Hassani and A. H. Khasahmadi, “Contrastive multi-view rep-\nresentation learning on graphs,” in ICML, 2020, pp. 3451–3461.\n[39] L. Page, S. Brin, R. Motwani, and T. Winograd, “The pagerank\ncitation ranking: Bringing order to the web.” Stanford InfoLab,\nTech. Rep., 1999.\n[40] R. I. Kondor and J. Lafferty, “Diffusion kernels on graphs and other\ndiscrete structures,” in ICML, vol. 2002, 2002, pp. 315–322.\n[41] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin,\n“Convolutional sequence to sequence learning,” inICML. PMLR,\n2017, pp. 1243–1252.\n[42] T. Opsahl and P . Panzarasa, “Clustering in weighted networks,”\nSocial networks, vol. 31, no. 2, pp. 155–163, 2009.\n[43] M. De Choudhury, H. Sundaram, A. John, and D. D. Seligmann,\n“Social synchrony: Predicting mimicry of user actions in online\nsocial media,” in International conference on computational science\nand engineering, vol. 4. IEEE, 2009, pp. 151–158.\n[44] R. A. Rossi and N. K. Ahmed, “The network data repository with\ninteractive graph analytics and visualization,” in AAAI, 2015.\n[Online]. Available: http://networkrepository.com\n[45] S. Kumar, F. Spezzano, V . Subrahmanian, and C. Faloutsos, “Edge\nweight prediction in weighted signed networks,” in ICDM. IEEE,\n2016, pp. 221–230.\n[46] S. Kumar, B. Hooi, D. Makhija, M. Kumar, C. Faloutsos, and\nV . Subrahmanian, “Rev2: Fraudulent user prediction in rating\nplatforms,” in WSDM. ACM, 2018, pp. 333–341.\n[47] B. Zhang, R. Liu, D. Massey, and L. Zhang, “Collecting the inter-\nnet as-level topology,” ACM SIGCOMM Computer Communication\nReview, vol. 35, no. 1, pp. 53–61, 2005.\n[48] B. Perozzi, R. Al-Rfou, and S. Skiena, “Deepwalk: Online learning\nof social representations,” in SIGKDD, 2014, pp. 701–710.\n[49] A. Grover and J. Leskovec, “node2vec: Scalable feature learning\nfor networks,” in SIGKDD, 2016, pp. 855–864.\n[50] U. Von Luxburg, “A tutorial on spectral clustering,” Statistics and\ncomputing, vol. 17, no. 4, pp. 395–416, 2007.\n[51] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., “Pytorch: An im-\nperative style, high-performance deep learning library,” NeurIPS,\nvol. 32, pp. 8026–8037, 2019.\nYixin Liu received the B.S. degree and M.S.\ndegree from Beihang University, Beijing, China,\nin 2017 and 2020, respectively. He is currently\npursuing his Ph.D. degree in computer science\nat Monash University, Melbourne, Australia. His\nresearch concentrates on data mining, machine\nlearning, and deep learning on graphs.\nShirui Pan received a Ph.D. in computer sci-\nence from the University of Technology Sydney\n(UTS), Ultimo, NSW, Australia. He is currently a\nSenior Lecturer with the Faculty of Information\nTechnology, Monash University, Australia. He is\nan ARC Future Fellow (awarded in 2021). His\nresearch interests include data mining and ma-\nchine learning. To date, Dr Pan has published\nover 100 research papers in top-tier journals\nand conferences, including TPAMI, TNNLS, and\nTKDE.\nYu Guang Wang received a Ph.D. in ap-\nplied mathematics from University of New South\nWales, Australia. He is an adjunct associate lec-\nturer at UNSW Sydney. He is also a scientist\nat Max Planck Institute for Mathematics in Sci-\nences, in Mathematics Machine Learning group.\nHis research interests lie in computational math-\nematics, statistics, machine learning, and data\nscience.\nFei Xiong received the Ph.D. degree from Bei-\njing Jiaotong University, Beijing, China, in 2013.\nHe was a Visiting Scholar with Carnegie Mel-\nlon University, Pittsburgh, PA, USA, from 2011\nto 2012. He is currently an Associate Profes-\nsor with the School of Electronic and Informa-\ntion Engineering, Beijing Jiaotong University. His\ncurrent research interests include Web mining,\ncomplex networks, and complex systems.\nLiang Wangthe Ph.D. degree in computer sci-\nence from the Shenyang Institute of Automa-\ntion, Chinese Academy of Sciences, Shenyang,\nChina, in 2014. He was a Post-Doctoral Re-\nsearcher with Northwestern Polytechnical Uni-\nversity, Xi’an, China, in 2017, where he is cur-\nrently an Associate Professor. His current re-\nsearch interests include ubiquitous computing,\nmobile crowd sensing, and data mining.\nQingfeng Chenreceived the BSc and MSc de-\ngrees in mathematics from Guangxi Normal Uni-\nversity, China, in 1995 and 1998, respectively,\nand the PhD degree in computer science from\nthe University of Technology Sydney, in Septem-\nber 2004. He is now a professor with Guangxi\nUniversity, China, and the Hundred Talent Pro-\ngram of Guangxi. His research interests include\nbioinformatics, data mining, and artiﬁcial intelli-\ngence.\nVincent CS Lee received the PhD degree in\nadaptive systems from The University of New-\nCastle, Australia, in 1992. He is currently an\nassociate Professor with the Department of Data\nScience and Artiﬁcial Intelligence, Faculty of\nIT, Monash University, Australia. He is a multi-\ninterdisciplinary researcher spanning adaptive\nsignal processing and control system, computa-\ntional intelligence, AI in economic and ﬁnance,\ndeep machine learning and computer vision, dig-\nital health process mining, and information secu-\nrity and network cryptography disciplines. He is a senior member of the\nIEEE.",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210145478",
      "name": "Beijing Municipal Education Commission",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I21193070",
      "name": "Beijing Jiaotong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I17145004",
      "name": "Northwestern Polytechnical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I150807315",
      "name": "Guangxi University",
      "country": "CN"
    }
  ]
}