{
  "title": "Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions",
  "url": "https://openalex.org/W4385373545",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092778157",
      "name": "Timo Wilm",
      "affiliations": [
        "Otto Group (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A5092778158",
      "name": "Philipp Normann",
      "affiliations": [
        "Otto Group (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A5092778159",
      "name": "Sophie Baumeister",
      "affiliations": [
        "Otto Group (Germany)"
      ]
    },
    {
      "id": "https://openalex.org/A5092778160",
      "name": "Paul-Vincent Kobow",
      "affiliations": [
        "Otto Group (Germany)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2017121215",
    "https://openalex.org/W2152808281",
    "https://openalex.org/W3200664681",
    "https://openalex.org/W4306317239",
    "https://openalex.org/W2626454364",
    "https://openalex.org/W4299286960",
    "https://openalex.org/W2963367478",
    "https://openalex.org/W3081170586",
    "https://openalex.org/W4384640047",
    "https://openalex.org/W4296591854",
    "https://openalex.org/W4226175509",
    "https://openalex.org/W2102035799",
    "https://openalex.org/W3102619277"
  ],
  "abstract": "This work introduces TRON, a scalable session-based Transformer Recommender using Optimized Negative-sampling. Motivated by the scalability and performance limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates top-k negative sampling and listwise loss functions to enhance its recommendation accuracy. Evaluations on relevant large-scale e-commerce datasets show that TRON improves upon the recommendation quality of current methods while maintaining training speeds similar to SASRec. A live A/B test yielded an 18.14% increase in click-through rate over SASRec, highlighting the potential of TRON in practical settings. For further research, we provide access to our source code at https://github.com/otto-de/TRON and an anonymized dataset at https://github.com/otto-de/recsys-dataset.",
  "full_text": "Scaling Session-Based Transformer Recommendations using\nOptimized Negative Sampling and Loss Functions\nTimo Wilmâˆ—\ntimo.wilm@otto.de\nOTTO (GmbH & Co KG)\nHamburg, Germany\nPhilipp Normannâˆ—\nphilipp.normann@otto.de\nOTTO (GmbH & Co KG)\nHamburg, Germany\nSophie Baumeister\nsophie.baumeister@otto.de\nOTTO (GmbH & Co KG)\nHamburg, Germany\nPaul-Vincent Kobow\npaul-vincent.kobow@otto.de\nOTTO (GmbH & Co KG)\nHamburg, Germany\nABSTRACT\nThis work introduces TRON, a scalable session-based Transformer\nRecommender using Optimized Negative-sampling. Motivated by\nthe scalability and performance limitations of prevailing models\nsuch as SASRec and GRU4Rec+, TRON integrates top-k negative\nsampling and listwise loss functions to enhance its recommendation\naccuracy. Evaluations on relevant large-scale e-commerce datasets\nshow that TRON improves upon the recommendation quality of\ncurrent methods while maintaining training speeds similar to SAS-\nRec. A live A/B test yielded an 18.14% increase in click-through\nrate over SASRec, highlighting the potential of TRON in practical\nsettings. For further research, we provide access to our source code1\nand an anonymized dataset2.\nCCS CONCEPTS\nâ€¢ Applied computing â†’Online shopping; â€¢ Information sys-\ntems â†’Recommender systems; â€¢ Computing methodologies\nâ†’Neural networks.\nKEYWORDS\nsession-based, recommender systems, transformers, negative sam-\npling, ranking loss function\nACM Reference Format:\nTimo Wilm, Philipp Normann, Sophie Baumeister, and Paul-Vincent Kobow.\n2023. Scaling Session-Based Transformer Recommendations using Opti-\nmized Negative Sampling and Loss Functions. In Seventeenth ACM Confer-\nence on Recommender Systems (RecSys â€™23), September 18â€“22, 2023, Singa-\npore, Singapore. ACM, New York, NY, USA, 4 pages. https://doi.org/10.1145/\n3604915.3610236\nâˆ—Equal contribution\n1https://github.com/otto-de/TRON\n2https://github.com/otto-de/recsys-dataset\nPermission to make digital or hard copies of part or all of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for third-party components of this work must be honored.\nFor all other uses, contact the owner/author(s).\nRecSys â€™23, September 18â€“22, 2023, Singapore, Singapore\nÂ© 2023 Copyright held by the owner/author(s).\nACM ISBN 979-8-4007-0241-9/23/09.\nhttps://doi.org/10.1145/3604915.3610236\n1 INTRODUCTION\nPersonalized real-time recommendations are a critical feature for\ne-commerce platforms such as OTTO. While recent advancements\nin deep learning models have offered promising results in session-\nbased recommendations [5, 10, 13], established systems like RNN\nbased GRU4Rec + [8] and transformer-based SASRec [ 10] often\nstruggle to maintain accuracy and scalability when dealing with\nlarge item sets. To address these limitations, we introduce TRON, a\nsession-based transformer recommendation system built upon the\noriginal SASRec, that uses top-k negative sampling and a listwise\nloss function to enhance accuracy and training time significantly.\n2 METHODS\n2.1 Negative Sampling\nSession-based recommendation systems predict the next item a user\nwill interact with based on their previous activities within a session.\nA session is a sequence of user-item interactions, represented as\nğ‘  := [ğ‘–1,ğ‘–2,...,ğ‘– ğ‘‡ âˆ’1,ğ‘–ğ‘‡ ]where ğ‘‡ is the session length. The items a\nuser has interacted with within a session are considered positive\nsamples, denoted asP(ğ‘ ):=\nğ‘‡Ã\nğ‘˜=1\n{ğ‘–ğ‘˜ }. In contrast, items that the user\nhas not interacted with are called negative samples, represented as\nNğ‘  := I\\P( ğ‘ ), where Iis the total set of available items.\nTraining a model to perform a next-item prediction across I\nis often unfeasible due to the large size of Iin real-world scenar-\nios. Consequently, a common approach is to train the model to\ndistinguish between positive and negative samples, which can be\nachieved through negative sampling [15]. A major challenge in neg-\native sampling is efficiency. Sampling directly from Nğ‘  can be com-\nputationally expensive, as it requires the exclusion of items present\nin P(ğ‘ ). This issue becomes critical when increasing the number\nof negative samples, leading to extended training times [10].\nAn often utilized solution is to sample negatives according to\na uniform distribution UI across the entire set of items I[8].\nThis strategy proves to be effective for large item sets, where the\nprobability of mistakenly sampling a positive item as a negative is\nrelatively small. Another strategy is to sample negatives from the\nempirical frequency FIof item interactions across all users. One\nmethod to efficiently and effectively sample negatives from FI,\nis in-batch negative sampling [9]. This method involves sampling\nnegatives from the batch currently being processed. This is possible\narXiv:2307.14906v2  [cs.IR]  30 Mar 2025\nRecSys â€™23, September 18â€“22, 2023, Singapore, Singapore Timo Wilm, Philipp Normann, Sophie Baumeister, and Paul-Vincent Kobow\nin GRU4Rec because, due to the way a batch is constructed, at\neach time step ğ‘¡, no other item from the session ğ‘  except ğ‘–ğ‘¡ exists\nin the batch. For transformer-based models, such as SASRec, we\nhave developed an efficient solution to employ in-batch negative\nsampling by excluding samples from ğ‘  for batches that include all\nevents of ğ‘ .\nIn practice, the combination of negative sampling from bothUI\nand FI often results in enhanced model accuracy [ 8]. This can\nbe achieved by sampling ğ‘˜ negatives from UIand ğ‘š negatives\nfrom FI. Consider a batch B:= [ğ‘†1,ğ‘†2,...,ğ‘† ğ‘]that consists of ğ‘\nuser sessions. At each time step ğ‘¡ in each user session ğ‘ , we sample\nUNğ‘¡ğ‘  := [ğ‘ˆ1,ğ‘ˆ2,...,ğ‘ˆ ğ‘˜ ]and FNğ‘¡ğ‘  := [ğ¹1,ğ¹2,...,ğ¹ ğ‘š], where each\nğ‘ˆğ‘– is a sample from UIand each ğ¹ğ‘— is a sample from FI. These\nsamples are then concatenated to form ağ‘˜+ğ‘šdimensional random\nvector Nğ‘¡ğ‘  := ğ‘ğ‘œğ‘›ğ‘ğ‘ğ‘¡[FNğ‘¡ğ‘ ,UNğ‘¡ğ‘  ]. The negative samples for the\nentire batch are represented as N.\nThis sampling process can be performed in different ways: el-\nementwise, sessionwise, or batchwise, or a combination of these\nmethods. For the elementwise approach, Nis a tensor of shape\n[ğ‘,ğ‘‡,ğ‘˜ +ğ‘š]because negatives are sampled at each time step for\neach session. In sessionwise sampling, all negatives for a session\nare sampled at once, resulting in a tensor of shape [ğ‘,1,ğ‘˜ +ğ‘š].\nWith batchwise sampling, all negatives for a batch are sampled\nat once, leading to a tensor of shape [1,1,ğ‘˜ +ğ‘š]. These different\nsampling strategies have a significant impact on the speed of train-\ning. For instance, when a large number of negatives is used, the\ndata transfers between the CPU and GPU can become a bottleneck,\nparticularly with elementwise sampling. Employing sessionwise\nor batchwise sampling can mitigate this issue, allowing the use of\nmore negative samples per time step while maintaining a training\nspeed comparable to that of SASRec. TRON uses a combination of\nuniform batchwise and in-batch sessionwise negative sampling to\nmaintain training speed while improving accuracy.\nTo further optimize the negative sampling process and enhance\nrecommendation performance TRON utilizes a top-k negative\nsampling strategy, which is inspired by a participantâ€™s idea from\nOTTOâ€™s RecSys competition on Kaggle1 and is similar to dynamic\nnegative item sampling [17]. This strategy focuses on updating the\ntop-k negatives during training instead of updating the whole set\nof negative ratings.\nInitially, we sample a set of negative items Nğ‘¡ğ‘  and obtain scores\nğ‘Ÿğ‘¡\nğ‘ ,ğ‘— for each item ğ‘— of session ğ‘  at time step ğ‘¡ in Nğ‘¡ğ‘  . Applying the\ntop-k function to the scored items, we select the top-k negatives\nKNğ‘¡ğ‘  := ğ‘¡ğ‘œğ‘ğ‘˜([ğ‘Ÿğ‘¡\nğ‘ ,1,ğ‘Ÿğ‘¡\nğ‘ ,2,...,ğ‘Ÿ ğ‘¡\nğ‘ ,|Nğ‘¡ğ‘  |]). These top-k items are then\nused for updates in the backpropagation step, while the rest are\ndiscarded.\nThis strategy allows us to retain the benefits of a large nega-\ntive sample set, which provides a broader context and helps in\nidentifying harder negatives, while substantially reducing the com-\nputational load during backpropagation. By prioritizing the update\nof negatives that the model currently misranks as likely positives,\nwe enhance the overall speed and accuracy of the recommender\nsystem.\n1https://kaggle.com/competitions/otto-recommender-system/discussion/384022\nTable 1: Statistics of the datasets used in our experiments.\nData\nTrain set Test set\nitemssessions events sessions events\nDiginetica 187k 906k 18k 87k 43k\nYoochoose 7,9M 31,6M 15k 71k 37k\nOTTO 12,9M 194,7M 1,6M 12,3M 1,8M\n2.2 Loss Functions\nFinally, we evaluate pointwise, pairwise, and listwise ranking loss\nfunctions typically used in recommendation systems [4] to further\nenhance model accuracy. The pointwise loss function is binary\ncross-entropy (BCE) [10]. The pairwise loss function is Bayesian\npersonalized ranking max (BPR-MAX) [ 8]. TRON uses sampled\nsoftmax (SSM) [2, 3], a listwise loss function with several benefi-\ncial properties, such as alleviating popularity bias and maximizing\nranking metrics [16].\n3 EXPERIMENTAL SETUP\nIn our evaluation, we assess the performance of our proposed model\nTRON, which is built upon the SASRec architecture, using three\nbenchmark datasets: Diginetica [6], Yoochoose [1], and OTTO [14].\nEach of these datasets presents increasing complexity regarding\nthe number of events and the variety of item sets. We only use click\nevents for our experiments, maintaining a minimum item support\nof five and a session length of at least two for all datasets [8]. We use\na temporal train/test split method, using the last day (Yoochoose\ndataset) or the last week of data (Diginetica and OTTO datasets) to\nform the test sets. The remaining data is used for training. Table 1\nprovides an overview of the datasets used in our experiments.\nRecall@20 and MRR@20 are used as offline metrics [ 9]. We\nperform extensive assessments encompassing all events and items\nwithin the test set to ensure rigorous and dependable evaluations.\nWe prioritize such comprehensive evaluations over sampling-based\napproaches because the latter have shown to be unreliable [11].\nWe use GRU4Rec+ and SASRec as benchmark models. GRU4Rec+\noperates with a hidden size of 100. SASRec is configured with a\nhidden size of 200 across two layers. We introduce modifications\nto SASRec with two variant configurations: one with 512 uniform\nand 16 in-batch sessionwise negatives (SASRec M-Negs) and the\nother with 8192 uniform and 127 in-batch sessionwise negatives\n(SASRec L-Negs). Additionally, SASRec BPR-MAX adopts the BPR-\nMAX loss, while SASRec SSM leverages an SSM loss function, both\nutilizing the same negative sampling strategy as SASRec L-Negs.\nOur proposed models TRON L-Negs and XL-Negs are both based\non the SASRec architecture and use an SSM loss function. TRON\nL-Negs is configured with 8192 batchwise uniform and 127 in-batch\nsessionwise negatives, whereas TRON XL-Negs operates with 16384\nbatchwise uniform negatives and 127 in-batch sessionwise nega-\ntives. Both TRON models use a top-k negative sampling strategy\nonly updating based on the top 100 negative ratings. All models are\ntrained with a batch size of 128 using an NVIDIA Tesla V100 GPU.\nScaling Session-Based Transformer Recommendations RecSys â€™23, September 18â€“22, 2023, Singapore, Singapore\nTable 2: Accuracy and training speed using various negative sampling strategies and loss functions. SASRec and TRON models\nwere trained for 100 epochs on Diginetica, and 10 for both Yoochoose and OTTO, while GRU4Rec + was trained for 10 epochs on\nDiginetica, 3 epochs on Yoochoose, and 1 epoch on OTTO. The best result for each dataset is highlighted in bold.\nMethod\nDiginetica Yoochoose OTTO\nR@20 MRR@20 Epochs/h R@20 MRR@20 Epochs/h R@20 MRR@20 Epochs/h\nGRU4Rec+ 0.455 0.144 15.126 0.725 0.31 0.478 0.443 0.205 0.019\nSASRec 0.454 0.157 94.533 0.573 0.216 2.573 0.307 0.180 0.248\nSASRec M-Negs 0.464 0.160 93.581 0.607 0.234 2.603 0.269 0.142 0.246\nSASRec L-Negs 0.467 0.161 48.247 0.571 0.211 1.245 0.226 0.114 0.204\nSASRec BPR-Max 0.526 0.175 40.608 0.722 0.297 1.049 0.377 0.178 0.194\nSASRec SSM 0.516 0.169 46.364 0.722 0.305 1.268 0.432 0.201 0.209\nTRON L-Negs 0.537 0.182 81.389 0.730 0.299 2.117 0.460 0.212 0.233\nTRON XL-Negs 0.541 0.182 68.408 0.732 0.302 1.912 0.472 0.219 0.227\nTRON XL vs. SASRec 19.1% 15.9% -27.6% 27.7% 39.8% -25.7% 53.7% 21.7% -8.5%\nTRON XL vs. GRU4Rec+ 18.9% 26.4% 352.3% 0.97% -2.6% 299.8% 6.5% 6.8% 1094.7%\n4 RESULTS\nThe offline evaluation of our experiments compared to the bench-\nmark models is presented in Table 2. The GRU4Rec+ model outper-\nforms SASRec across all datasets except MRR@20 on the Diginetica\ndataset. While previous studies on smaller datasets such as Digi-\nnetica indicated SASRecâ€™s superiority over GRU4Rec+[7, 12], our\nfindings on larger datasets and realistic training times do not sup-\nport this claim. This discrepancy could also be attributed to our\nextensive evaluation method, which avoids weaknesses associated\nwith sampling-based evaluations [11] and does not solely rely on\nthe last item of a session. SASRec M-Negs improves the accuracy of\nSASRec for the Diginetica and Yoochoose datasets but shows lower\naccuracy for the OTTO dataset while maintaining SASRecâ€™s original\nspeed. SASRec L-Negs, on the other hand, exhibits slower training\ntimes across all datasets and only improves accuracy on Diginetica.\nThis suggests that using additional negatives in a pointwise loss\nfunction such as BCE negatively impacts the modelâ€™s accuracy. SAS-\nRec SSM shows promising results, outperforming GRU4Rec + on\nthe Diginetica dataset and demonstrating competitive accuracy for\nthe other two datasets. Our proposed model TRON shows superior\naccuracy across all datasets except for MRR@20 on the Yoochoose\ndataset while demonstrating faster training times than SASRec SSM\ndue to batchwise and top-k negative sampling. TRON demonstrates\nimproved scalability as the dataset grows larger, as evidenced by\nthe decreasing relative slowdown compared to SASRec. On the\nOTTO dataset, TRON shows an accuracy increase of more than\n6.5% in both Recall@20 and MRR@20, as well as a training speedup\nof 1090% compared to GRU4Rec+. Despite handling more negatives,\nTRON maintains 92% of SASRecâ€™s training speed.\nIn the online experiment, we trained TRON XL-Negs, SASRec\nSSM, and SASRec on a private OTTO dataset from the two most\nrecent weeks using the same preprocessing as described in Section\n3. The Recall@20 for each epoch and model on the test set can\nbe seen in Figure 1. The live improvement of TRON XL-Negs and\nSASRec SSM relative to SASRec measured from May 9 to May 17\n2023 is shown in Figure 2. The results validate the effectiveness of\n1 2 3 4 5 6 7 8 9 10\n0.1\n0.2\n0.3\n0.4\nEpochs\nRecall@20 TRON XL-Negs\nSASRec SSM\nSASRec\nFigure 1: Offline evaluation results on our private OTTO\ndataset used for the online A/B test of our three groups.\nCTR carts units\n0\n10\n20\n30\nUplift in %\nTRON XL-Negs\nSASRec SSM\nFigure 2: Online results of our A/B test relative to the SASRec\nbaseline. The error bars indicate the 95% confidence interval.\nTRON in a real-world e-commerce setting, showing an increase of\n18.14% in click-through rate, 23.85% increase in add-to carts and\n23.67% uplift in units compared to SASRec.\nRecSys â€™23, September 18â€“22, 2023, Singapore, Singapore Timo Wilm, Philipp Normann, Sophie Baumeister, and Paul-Vincent Kobow\n5 CONCLUSION\nOur proposed TRON model significantly improves the accuracy and\ntraining time of transformer-based recommendation systems on\nlarge e-commerce datasets. This enhancement is achieved through\nthe strategic optimization of negative sampling methods, utilization\nof listwise loss functions, and focusing on the most misranked\nnegatives.\n6 SPEAKER BIO\nTimo Wilm, Philipp Normann, and Sophie Baumeister form\na data science trio at OTTOâ€™s recommendation team. Wilm and\nNormann are Senior Data Scientists with over five years of experi-\nence in e-commerce, specializing in the design and integration of\ncutting-edge deep learning models. Baumeister, a Junior Data Scien-\ntist, has been with the team for over one year. Together with their\nteam, they are responsible for the development and maintenance\nof OTTOâ€™s recommendation systems, which are used by millions of\ncustomers every day.\nREFERENCES\n[1] David Ben-Shimon, Alexander Tsikinovsky, Michael Friedmann, Bracha Shapira,\nLior Rokach, and Johannes Hoerle. 2015. RecSys Challenge 2015 and the YOO-\nCHOOSE Dataset. In Proceedings of the 9th ACM Conference on Recommender\nSystems. ACM, Vienna Austria, 357â€“358. https://doi.org/10.1145/2792838.2798723\n[2] Yoshua Bengio and Jean-SÃ©bastien Senecal. 2003. Quick Training of Probabilistic\nNeural Nets by Importance Sampling. InInternational Workshop on Artificial Intel-\nligence and Statistics . PMLR, 17â€“24. https://proceedings.mlr.press/r4/bengio03a.\nhtml\n[3] Y. Bengio and J.-S. Senecal. 2008. Adaptive Importance Sampling to Accelerate\nTraining of a Neural Probabilistic Language Model. IEEE Transactions on Neural\nNetworks 19, 4 (April 2008), 713â€“722. https://doi.org/10.1109/TNN.2007.912312\n[4] Wei Chen, Tie-Yan Liu, Yanyan Lan, Zhiming Ma, and Hang Li. 2009. Ranking\nmeasures and loss functions in learning to rank. In Proceedings of the 22nd Inter-\nnational Conference on Neural Information Processing Systems (NIPSâ€™09) . Curran\nAssociates Inc., Red Hook, NY, USA, 315â€“323. https://dl.acm.org/doi/10.5555/\n2984093.2984129\n[5] Gabriel De Souza Pereira Moreira, Sara Rabhi, Jeong Min Lee, Ronay Ak, and\nEven Oldridge. 2021. Transformers4Rec: Bridging the Gap between NLP and\nSequential / Session-Based Recommendation. In Fifteenth ACM Conference on\nRecommender Systems . ACM, Amsterdam Netherlands, 143â€“153. https://doi.org/\n10.1145/3460231.3474255\n[6] DIGINETICA. 2016. CIKM Cup 2016 Track 2: Personalized E-Commerce Search\nChallenge. https://competitions.codalab.org/competitions/11161\n[7] Zhankui He, Handong Zhao, Zhaowen Wang, Zhe Lin, Ajinkya Kale, and Julian\nMcauley. 2022. Query-Aware Sequential Recommendation. In Proceedings of\nthe 31st ACM International Conference on Information & Knowledge Management .\nACM, Atlanta GA USA, 4019â€“4023. https://doi.org/10.1145/3511808.3557677\n[8] BalÃ¡zs Hidasi and Alexandros Karatzoglou. 2018. Recurrent Neural Networks\nwith Top-k Gains for Session-based Recommendations. In Proceedings of the\n27th ACM International Conference on Information and Knowledge Management .\n843â€“852. https://doi.org/10.1145/3269206.3271761\n[9] BalÃ¡zs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\n2016. Session-based Recommendations with Recurrent Neural Networks. In 4th\nInternational Conference on Learning Representations, ICLR 2016, San Juan, Puerto\nRico, May 2-4, 2016, Conference Track Proceedings , Yoshua Bengio and Yann LeCun\n(Eds.). https://doi.org/10.48550/arXiv.1511.06939\n[10] W. Kang and J. McAuley. 2018. Self-Attentive Sequential Recommendation. In2018\nIEEE International Conference on Data Mining (ICDM) . IEEE Computer Society,\nLos Alamitos, CA, USA, 197â€“206. https://doi.org/10.1109/ICDM.2018.00035\n[11] Walid Krichene and Steffen Rendle. 2020. On Sampled Metrics for Item Recom-\nmendation. In Proceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining . ACM, Virtual Event CA USA, 1748â€“1757.\nhttps://doi.org/10.1145/3394486.3403226\n[12] Ming Li, Ali Vardasbi, Andrew Yates, and Maarten de Rijke. 2023. Repetition\nand Exploration in Sequential Recommendation. In Proceedings of the 46th In-\nternational ACM SIGIR Conference on Research and Development in Information\nRetrieval (SIGIR â€™23) . Association for Computing Machinery, New York, NY, USA,\n2532â€“2541. https://doi.org/10.1145/3539618.3591914\n[13] M. Jeffrey Mei, Cole Zuber, and Yasaman Khazaeni. 2022. A Lightweight\nTransformer for Next-Item Product Recommendation. In Sixteenth ACM Con-\nference on Recommender Systems . ACM, Seattle WA USA, 546â€“549. https:\n//doi.org/10.1145/3523227.3547491\n[14] Philipp Normann, Sophie Baumeister, and Timo Wilm. 2023. OTTO Recom-\nmender Systems Dataset. https://doi.org/10.34740/KAGGLE/DSV/4991874\n[15] Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme.\n2009. BPR: Bayesian Personalized Ranking from Implicit Feedback. InProceedings\nof the Twenty-Fifth Conference on Uncertainty in Artificial Intelligence (UAI â€™09) .\nAUAI Press, Arlington, Virginia, USA, 452â€“461. https://dl.acm.org/doi/10.5555/\n1795114.1795167\n[16] Jiancan Wu, Xiang Wang, Xingyu Gao, Jiawei Chen, Hongcheng Fu, Tianyu Qiu,\nand Xiangnan He. 2022. On the Effectiveness of Sampled Softmax Loss for Item\nRecommendation. ArXiv abs/2201.02327 (2022). https://doi.org/10.48550/ARXIV.\n2201.02327\n[17] Weinan Zhang, Tianqi Chen, Jun Wang, and Yong Yu. 2013. Optimizing top-n\ncollaborative filtering via dynamic negative item sampling. In Proceedings of\nthe 36th international ACM SIGIR conference on Research and development in\ninformation retrieval . ACM, Dublin Ireland, 785â€“788. https://doi.org/10.1145/\n2484028.2484126",
  "topic": "Scalability",
  "concepts": [
    {
      "name": "Scalability",
      "score": 0.8287089467048645
    },
    {
      "name": "Computer science",
      "score": 0.8193372488021851
    },
    {
      "name": "Session (web analytics)",
      "score": 0.6267374753952026
    },
    {
      "name": "Scaling",
      "score": 0.5592073202133179
    },
    {
      "name": "Recommender system",
      "score": 0.5521793961524963
    },
    {
      "name": "Transformer",
      "score": 0.5003092288970947
    },
    {
      "name": "Source code",
      "score": 0.48377034068107605
    },
    {
      "name": "Code (set theory)",
      "score": 0.438708633184433
    },
    {
      "name": "Sampling (signal processing)",
      "score": 0.4216027557849884
    },
    {
      "name": "Machine learning",
      "score": 0.3896546959877014
    },
    {
      "name": "Data mining",
      "score": 0.37853583693504333
    },
    {
      "name": "Database",
      "score": 0.2904096245765686
    },
    {
      "name": "World Wide Web",
      "score": 0.20545372366905212
    },
    {
      "name": "Programming language",
      "score": 0.14552828669548035
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Filter (signal processing)",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Computer vision",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210086827",
      "name": "Otto Group (Germany)",
      "country": "DE"
    }
  ]
}