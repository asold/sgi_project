{
  "title": "Exploring Mode Connectivity for Pre-trained Language Models",
  "url": "https://openalex.org/W4385573419",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2098243531",
      "name": "Yujia Qin",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2020544620",
      "name": "Cheng Qian",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2100298610",
      "name": "Jing Yi",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2110996955",
      "name": "Weize Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2131512363",
      "name": "Yankai Lin",
      "affiliations": [
        "Beijing Institute of Big Data Research",
        "Tsinghua University",
        "Renmin University of China"
      ]
    },
    {
      "id": "https://openalex.org/A1998709255",
      "name": "Xu Han",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": [
        "Tsinghua University",
        "Peng Cheng Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2157167650",
      "name": "Maosong Sun",
      "affiliations": [
        "Peng Cheng Laboratory",
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2093278426",
      "name": "Jie Zhou",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W3132646476",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W4221155125",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3198675127",
    "https://openalex.org/W2788838181",
    "https://openalex.org/W3034408878",
    "https://openalex.org/W2971033911",
    "https://openalex.org/W2951934643",
    "https://openalex.org/W2793333878",
    "https://openalex.org/W4286856923",
    "https://openalex.org/W3034255912",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4225691633",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4287891024",
    "https://openalex.org/W3164896303",
    "https://openalex.org/W1682403713",
    "https://openalex.org/W4285169833",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4205712089",
    "https://openalex.org/W3081410576",
    "https://openalex.org/W3198659451",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3103649165",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W2996074092",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W4221145545",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W3124687886",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2395579298",
    "https://openalex.org/W3174781392",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W3098068947",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W3153427360"
  ],
  "abstract": "Recent years have witnessed the prevalent application of pre-trained language models (PLMs) in NLP. From the perspective of parameter space, PLMs provide generic initialization, starting from which high-performance minima could be found. Although plenty of works have studied how to effectively and efficiently adapt PLMs to high-performance minima, little is known about the connection of various minima reached under different adaptation configurations. In this paper, we investigate the geometric connections of different minima through the lens of mode connectivity, which measures whether two minima can be connected with a low-loss path. We conduct empirical analyses to investigate three questions: (1) how could hyperparameters, specific tuning methods, and training data affect PLM’s mode connectivity? (2) How does mode connectivity change during pre-training? (3) How does the PLM’s task knowledge change along the path connecting two minima? In general, exploring the mode connectivity of PLMs conduces to understanding the geometric connection of different minima, which may help us fathom the inner workings of PLM downstream adaptation. The codes are publicly available at https://github.com/thunlp/Mode-Connectivity-PLM.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6726–6746\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nExploring Mode Connectivity for Pre-trained Language Models\nYujia Qin1∗, Cheng Qian1∗, Jing Yi1∗, Weize Chen1, Yankai Lin2,3†, Xu Han1,\nZhiyuan Liu1,4,5†, Maosong Sun1,4,5†, Jie Zhou6\n1NLP Group, DCST, IAI, BNRIST, Tsinghua University, Beijing\n2Gaoling School of Artificial Intelligence, Renmin University of China, Beijing\n3Beijing Key Laboratory of Big Data Management and Analysis Methods, Beijing\n4International Innovation Center of Tsinghua University, Shanghai\n5Quan Cheng Laboratory 6Pattern Recognition Center, WeChat AI, Tencent Inc.\n{qyj20, qianc20, yi-j20}@mails.tsinghua.edu.cn\nAbstract\nRecent years have witnessed the prevalent\napplication of pre-trained language models\n(PLMs) in NLP. From the perspective of pa-\nrameter space, PLMs provide generic initial-\nization, starting from which high-performance\nminima could be found. Although plenty of\nworks have studied how to effectively and effi-\nciently adapt PLMs to high-performance min-\nima, little is known about the connection of\nvarious minima reached under different adap-\ntation configurations. In this paper, we in-\nvestigate the geometric connections of differ-\nent minima through the lens of mode con-\nnectivity, which measures whether two min-\nima can be connected with a low-loss path.\nWe conduct empirical analyses to investigate\nthree questions: (1) how could hyperparame-\nters, specific tuning methods, and training data\naffect PLM’s mode connectivity? (2) How\ndoes mode connectivity change during pre-\ntraining? (3) How does the PLM’s task knowl-\nedge change along the path connecting two\nminima? In general, exploring the mode con-\nnectivity of PLMs conduces to understanding\nthe geometric connection of different minima,\nwhich may help us fathom the inner workings\nof PLM downstream adaptation. The codes are\npublicly available at https://github.com/\nthunlp/Mode-Connectivity-PLM.\n1 Introduction\nRecent years have witnessed the prevalent appli-\ncation of pre-trained language models (PLMs)\nin NLP (Han et al., 2021), with the state-of-the-\nart across various NLP tasks consistently being\npushed (Devlin et al., 2019; Liu et al., 2019b; Raffel\net al., 2020). Through large-scale self-supervised\ntraining, PLMs acquire versatile semantic (Liu\n∗Indicates equal contribution.\n†Corresponding author.\net al., 2019a) and syntactic (Tenney et al., 2019)\nknowledge, which could be utilized when conduct-\ning transfer learning on downstream tasks.\nFrom the perspective of parameter space, PLMs\nprovide generic initialization for downstream adap-\ntation. Starting from the initialization, many\nhigh-performance minima can be found through\ngradient-based optimization. Up to now, plenty\nof works have studied how to effectively and ef-\nficiently adapt PLMs to high-performance min-\nima, including adjusting hyperparameters (Liu and\nWang, 2021), conducting transfer learning using\nauxiliary training data (Pruksachatkun et al., 2020),\ntuning PLMs in a parameter-efficient way (Ding\net al., 2022), etc. Under different adaptation con-\nfigurations, PLMs may finally reach local minima\ndistributed in highly distinct regions. Although\nthese minima all correspond to excellent perfor-\nmance (low loss), little has been known about their\ngeometric connection in the parameter space.\nA straightforward way to explore such geomet-\nric connection is to look into the loss landscape\naround different minima, which is inherently in-\ntractable due to the high dimensionality brought by\nthe tremendous parameter size of PLMs. Instead\nof probing the full landscape, we propose to inves-\ntigate the relation of different minima through the\nlens of mode connectivity (Garipov et al., 2018),\nwhich measures whether two different minima can\nbe connected via a parametric path, along which\nthe loss of the downstream task remains low. Ex-\nploring the mode connectivity of PLMs contributes\nto understanding the geometric connection among\ndifferent minima. Such connection reflects the in-\nherent relation of various adaptation configurations\nand may help us fathom the inner workings of PLM\ndownstream adaptation under different settings.\nTo the best of our knowledge, systematic studies\nfor the mode connectivity of PLMs are still lacking.\n6726\nIn this paper, we first investigate what factors may\naffect PLM’s mode connectivity by answering the\nfollowing research questions:\n• (Q1) How could different adaptation config-\nurations (hyperparameters, tuning methods,\nand training data) affect PLM’s mode connec-\ntivity?\n• (Q2) How does mode connectivity change\nduring pre-training?\nWe first consider the mode connectivity when\ndifferent minima are trained on the same dataset.\nWe investigate the effects of several hyperparame-\nters (e.g., training data order, initialization of the\ntunable parameters, training steps, etc.) on PLM’s\nmode connectivity, and find that among these fac-\ntors, initialization has the greatest impact. In addi-\ntion, we show that fine-tuning leads to better mode\nconnectivity than parameter-efficient delta tuning\n(e.g., adapter (Houlsby et al., 2019)).\nThen we extend the connectivity analysis to min-\nima trained on different datasets. We demonstrate\nthat: (1) the mode connectivity is good for two\nminima trained on data belonging to the same dis-\ntribution, but without overlap of specific instances.\nThis means instead of memorizing training data,\nPLMs learn advanced task-level knowledge during\ntraining, and mode connectivity originates from\nthe high overlap of task knowledge of two minima.\n(2) Although two minima trained on different tasks\nare inherently disconnected, pre-training gradually\npulls the optimal regions of different tasks closer\nin an implicit way. This phenomenon may help\nexplain PLM’s excellent cross-task transferability.\nBeyond exploring the effects that could affect\nPLM’s mode connectivity, we also study the in-\ntrinsic properties of model solutions between two\nminima, which leads to the third question:\n• (Q3) How does the PLM’s task knowledge\nchange along the path connecting two min-\nima?\nIn the experiments, we observe that for two min-\nima obtained independently on two tasks, when\ntraversing from the minima trained on a source task\nto that of a target task, a PLM suffers from catas-\ntrophic forgetting (McCloskey and Cohen, 1989)\non the source task, and gradually absorbs the knowl-\nedge from the target task. Besides, PLMs prioritize\nforgetting those elusive source knowledge and ac-\nquiring easy-to-grasp target knowledge.\nIn general, to fathom the connection of minima\nreached under different settings, we conduct empir-\nical studies on the mode connectivity of PLMs. We\nalso show that our findings may have potential sig-\nnificance in broad research areas, such as designing\nbetter ensemble methods for PLMs, understanding\nthe task-level transferability of PLMs and reveal-\ning the mechanism of PLM downstream adaptation.\nWe expect our evaluation setup and findings could\ninspire more future works in this field.\n2 Related Work\nAdaptation Strategies of PLMs. To effectively\nand efficiently utilize the knowledge learned during\npre-training, many strategies have been developed\nto better tune a PLM, including: (1) hyperparam-\neter search, which aims to find an optimal hyper-\nparameter configuration through traditional grid\nsearch or modern automated search (Liu and Wang,\n2021); (2) pre-finetuning, which trains PLMs on\nintermediate auxiliary tasks before fine-tuning on a\ntarget task (Pruksachatkun et al., 2020; Aghajanyan\net al., 2021). In this way, PLMs achieve better\ndownstream performance by taking advantage of\ncross-dataset knowledge transfer; (3) prompt learn-\ning, which casts downstream tasks into the form\nof the pre-training objective by leveraging natural\nlanguage prompts (Brown et al., 2020; Schick and\nSchütze, 2021a,b). Prompt learning exhibits supe-\nrior performance especially under the few-shot and\nzero-shot scenarios; (4) delta tuning (also known\nas parameter-efficient tuning). Optimizing all the\nparameters of a PLM is computationally cumber-\nsome. As a lightweight alternative, delta tuning\noptimizes only a few tunable parameters and keeps\nother parameters frozen, achieving comparable per-\nformance to fine-tuning (Ding et al., 2022).\nAlthough plenty of adaptation strategies have\nbeen developed to better tune a PLM, little is un-\nderstood about the connection of local minima\nreached under different training configurations. In\nthis work, we take the first step by utilizing mode\nconnectivity as the analysis tool.\nMode Connectivity for Neural Networks. De-\nspite being extremely high-dimensional, the loss\nlandscape of neural networks exhibits a simple\ngeometric pattern of mode connectivity (Garipov\net al., 2018; Freeman and Bruna, 2017; Draxler\net al., 2018). It is shown that starting from dif-\nferent initialization, the local minima obtained by\ngradient-based optimizations are often connected\n6727\nby low-loss paths, along which high-performance\nsolutions could be easily found and ensembled to\nachieve better performance (Garipov et al., 2018).\nThese paths are typically non-linear curves, which\nrequire a process of curve finding with task super-\nvision. Excellent mode connectivity indicates that\ndifferent minima are not isolated points in the pa-\nrameter space, but essentially form a connected\nmanifold (Draxler et al., 2018).\nFrankle et al. (2020) further contend that from\nthe same initialization, local minima obtained with\ndifferent training data order can be connected by a\nlinear low-loss path, reducing the burden of curve\nfinding. Such a phenomenon is dubbed as linear\nmode connectivity, which is closely related to lot-\ntery ticket hypothesis (Frankle and Carbin, 2019),\nand has direct implications for continual learning\n(Mirzadeh et al., 2020). Compared with the non-\nlinear counterpart, linear mode connectivity is a\nstronger constraint, requiring that the convex com-\nbination of two minima stay in the same loss basin.\nPrevious works typically study mode connectiv-\nity using non-pretrained models in the field of com-\nputer vision. Until recently, Neyshabur et al. (2020)\nobserve linear mode connectivity on pre-trained\nvision models. Despite the great efforts spent, a\nsystematic understanding of the mode connectivity\nof PLMs is still lacking. In this paper, we focus\non investigating the effects that would influence\nPLM’s mode connectivity and analyze the knowl-\nedge variation along the connecting path. Different\nfrom existing works that study mode connectivity\nfor minima trained on the same dataset, we addi-\ntionally extend the analysis to different datasets.\n3 Mode Connectivity Evaluation\nPreliminaries. Consider adapting a PLM on a\ndownstream task, let C1 and C2 be two distinct\nsets of training configurations that may differ in\nhyperparameters or data. We useC1 and C2 to train\ntwo copies of the PLM independently. The specific\ntuning method determines the tunable parameters\nθ0. After training, θ0 is adapted to θC1 ∈R|θ0|and\nθC2 ∈R|θ0|, respectively, where |θ0|denotes the\ntotal number of tunable parameters.\nConnecting Path. Based on Frankle et al. (2020),\nthe same initialization generally leads to a linear\nlow-loss path between two minima. Besides, com-\npared with the non-linear counterpart, linearity is a\nmore favorable property, which indicates a closer\nconnection between different minima. Therefore,\nour first step is to investigate whether PLMs have\ngood linear mode connectivity. Specifically, as-\nsume a continuous curve ϕ(α):[0 ,1]→R|θ0|con-\nnecting θC1 and θC2 , satisfying ϕ(0) = θC1 and\nϕ(1) =θC2 , we consider a linear path as follows:\nϕ(α) = (1−α)·θC1 + α·θC2 . (1)\nConnectivity Criterion. After defining the curve\nconnecting both minima, we traverse along the\ncurve and evaluate the loss and performance of the\ninterpolations. We deem two minima θC1 and θC2\nmode connected if there does not exist a significant\nloss barrier or performance drop along the defined\ncurve between θC1 and θC2 . In the experiments, we\nevaluate evenly distributed interpolations onϕ(α).\n4 Empirical Analysis\nIn this section, we conduct experiments to investi-\ngate the aforementioned research questions.\nQ1. (a) How could different hyperparame-\nters and the specific tuning method affect the\nmode connectivity of PLMs?\nWe first investigate the effects of several hyperpa-\nrameters that could affect PLM’s mode connectiv-\nity, including (1) training data order, initialization\nof tunable parameters, training step (main paper),\n(2) learning rate and batch size (appendix B.1). To\nexplore the effects of the specific tuning method,\nwe experiment with both fine-tuning and a represen-\ntative delta tuning method, i.e., adapter (Houlsby\net al., 2019). Adapter inserts tunable modules\ninto a PLM and keeps other parameters fixed dur-\ning adaptation. Unless otherwise specified, we\nmainly conduct the experiments using T5BASE (Raf-\nfel et al., 2020), and choose two representative\nNLP tasks (MNLI (Williams et al., 2018) and\nReCoRD (Zhang et al., 2018)).\nIn each experiment, the training configurations\nof the two endpoints only differ in one hyperparam-\neter, while other settings are kept the same for a fair\ncomparison. To explore the effects of training steps,\nwe evaluate the performance when both endpoints\nare trained for {10k, 30k, 50k} steps, respectively.\nWe evaluate 24 evenly distributed interpolations\nand 2 endpoints along a linear path, i.e., we eval-\nuate a series of ϕ(α), where α ∈{ 0\n25 , 1\n25 ,..., 25\n25 }.\nSince we find that the trends of loss and perfor-\nmance are generally highly correlated (i.e., a per-\nformance drop corresponds to a loss barrier), we\n6728\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test F1 (%)\nReCoRD Finetune\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test F1 (%)\nReCoRD Adapter\nStep 10k\nStep 30k\nStep 50k\nFigure 1: The performance of linear interpolations between two minima trained with different training data order.\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test F1 (%)\nReCoRD Finetune\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test F1 (%)\nReCoRD Adapter\nStep 10k\nStep 30k\nStep 50k\nFigure 2: The performance of linear interpolations between two minima trained with different initialization.\nreport the performance in the main paper and leave\nthe results of loss in appendix D. All experiments\nare conducted 3 times with random seeds and we\nreport the average results on test sets. For more\ntraining details, please refer to appendix C.\nEffects of Training Data Order. PLM’s down-\nstream adaptation generally involves mini-batch\ngradient-based optimization, where training sam-\nples are learned in a random order. To explore its\neffect, we adapt two copies of a PLM with two\ndifferent random data order. Then we visualize the\nperformance of linear interpolations in Figure 1,\nfrom which we observe that for fine-tuning, both\nendpoints are well connected by a linear path; while\nfor adapter tuning, there exists a slight but negligi-\nble performance drop near the midpoint. In general,\nwe conclude that local minima are well connected\nunder different random training data order.\nEffects of Initialization. Before downstream\nadaptation, additional parameters (e.g., extra mod-\nules defined by delta tuning, the classification head,\netc.) may be introduced; in addition, Wu et al.\n(2022) recently show that adding noise to the pre-\ntrained weights improves the fine-tuning perfor-\nmance on downstream tasks. Thus, both fine-\ntuning and delta tuning require proper initializa-\ntion for the tunable parameters. Since different\ninitialization could lead to distinct optimization tra-\njectories, we explore the effects of initialization on\nPLM’s mode connectivity.\nSpecifically, for those newly introduced modules,\nwe randomly initialize them with a Gaussian distri-\nbution; for those pre-trained weights that require\ntuning, we add a random Gaussian noise. Two end-\npoints are initialized with the same configuration\n(e.g., mean and standard deviation of the Gaussian\ndistribution), but different random seeds. The lin-\near interpolation results are depicted in Figure 2,\nfrom which we observe that the mode connectivity\nof fine-tuning is generally good; while for adapter\ntuning, there exists a significant performance drop\nbetween two differently initialized minima. This\nmeans starting from different initialization, PLMs\ntend to reach non-connected local minima in the\nparameter space, especially for delta tuning. In\nshort, initialization of tunable parameters has a\ngreat impact on mode connectivity.\nEffects of Training Step. As mentioned before,\nthe experiments in Figure 1 and 2 are conducted\nwhen both minima are trained for {10k, 30k, 50k}\nsteps. Comparing the results at different training\nsteps, we observe that (1) longer training leads to\npoorer connectivity for adapter tuning under cer-\ntain cases; while (2) the mode connectivity of fine-\ntuning is good at different steps. In appendix B.2,\nwe further show that (1) the mode connectivity be-\ncomes poorer when one endpoint is trained with\nmore steps while the other is trained with fixed\nsteps, and (2) with the training step increasing, the\nEuclidean distance between two minima is also\nprolonged, which may partially explain the poorer\nmode connectivity.\n6729\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test F1 (%)\nReCoRD Adapter\nStep 10k\nStep 30k\nStep 50k\nFigure 3: The performance of interpolations along a non-\nlinear path connecting two minima, which are trained\nwith adapter tuning from different initialization.\nEffects of Tuning Method. Comparing the re-\nsults of fine-tuning and adapter tuning in Figure 1\nand 2, we observe that in general, the linear mode\nconnectivity of fine-tuning is better than adapter\ntuning. In other words, when using fine-tuning,\nPLMs are more likely to be optimized to linearly-\nconnected minima. A similar phenomenon also\noccurs for minima trained with different learning\nrates or batch sizes (see appendix B.1). Consider-\ning that adapter optimizes only 2.38% parameters\nthan fine-tuning, we hypothesize that more tunable\nparameters may yield better mode connectivity and\nleave more explorations as future work.\nDifferent Minima are Generally Connected by\na Non-linear Path. Considering that linearity is\na strong constraint for mode connectivity, even if\na direct linear path connecting two minima incurs\na high loss, both minima may still be connected\nby a low-loss non-linear path. To explore whether\nthis holds for PLMs, we follow the setting of tun-\ning adapters with different initialization, which has\nbeen shown in Figure 2 to have poor linear mode\nconnectivity. We try to use the supervision from the\ndownstream task to find a low-loss parametric path\nconnecting two endpoints θC1 and θC2 . Follow-\ning Garipov et al. (2018), we consider a quadratic\nBezier curve defined as follows:\nϕθ(α) = (1−α)2 ·θC1 + 2α(1 −α)θ+ α2 ·θC2 , (2)\nwhere θ ∈R|θ0| denotes tunable parameters of\nthe curve. During curve finding, θC1 and θC2 are\nkept frozen, and only θ is optimized. Denote L\nas the loss function of the task, the training ob-\njective is to minimize the expectation of loss on\nthe curve over a uniform distribution U(0,1), i.e.,\nEα∈U(0,1)L(ϕθ(α)). For more details, please refer\nto appendix A.\nWe visualize the performance of the interpola-\ntion on the found Bezier curve in Figure 3. We\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k\nFigure 4: Linear mode connectivity analysis for two\nminima trained with in-distribution MNLI data. The\nresults on ReCoRD are left in appendix B.4.\nobserve that the two minima are well-connected by\nthe found Bezier curve, without a significant perfor-\nmance drop. In fact, such a low-loss Bezier curve\nexists for minima reached under various different\nsettings (see more experiments in appendix B.3).\nGiven the above results, we conjecture that there\nmay exist multiple loss basins which are con-\nnected via a low-loss non-linear path, instead of\na linear path. For most of the minima within\nthe same loss basin, their convex combination\nalso lies in this basin. In this sense, if two minima\nare connected linearly, then both of them probably\nlie in the same basin; otherwise in different basins\n(e.g., the case of adapter tuning with different ini-\ntialization).\nQ1. (b) What are the effects of training\ndata?\nIn previous experiments, we focus on the connec-\ntivity of two minima trained with the same dataset.\nFrom now on, we extend the mode connectivity to\ntwo minima trained on different datasets, focusing\non two facets: data overlap and data domain.\nEffects of Data Overlap. PLMs have been\ndemonstrated to be adept at memorizing the train-\ning data (Carlini et al., 2021, 2022). To show that\nthe connectivity of both minima does not originate\nfrom PLM’s memorization, we explore whether\nsuch mode connectivity still exists when two min-\nima are obtained on data belonging to the same\ndistribution, but without overlap of specific train-\ning samples. Specifically, we partition the original\ntraining data of MNLI into two equal splits. Then\nwe adapt two copies of T5BASE on either split using\nthe same training configurations. The experiments\nare conducted using both fine-tuning and adapter\ntuning.\nThe performance of linear interpolations is\nrecorded in Figure 4. The results show that two\n6730\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC / F1 (%)\nANLI\nMNLI\nMNLI~ANLI Finetune\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test F1 (%)\nRotten Tomatoes\nYelp Polarity\nRT~YP Finetune\nStep 3k\nStep 9k\nStep 15k\nFigure 5: Linear mode connectivity for two minima fine-\ntuned on different data distributions of the same task.\nLeft: α= 0/ α= 1denotes the minimum of MNLI /\nANLI. Right: α= 0/ α= 1denotes the minimum of\nRotten Tomatoes / Yelp Polarity.\nminima are well connected for both tuning meth-\nods, demonstrating that mode connectivity does\nnot originate from PLM’s memorization of spe-\ncific training data; instead, during training, PLMs\nlearn advanced task-level knowledge, and the con-\nnectivity reflects the high overlap of task knowl-\nedge of two local minima.\nEffects of Data Domain. PLMs are shown\nto generalize well on out-of-distribution data\n(Hendrycks et al., 2020), implying the connection\nof minima trained with different data distributions.\nTo gain a deeper understanding, we choose two\nnatural language inference datasets (MNLI and\nANLI (Nie et al., 2020)), and two sentiment analy-\nsis datasets (Rotten Tomatoes (Pang and Lee, 2005)\nand Yelp Polarity (Zhang et al., 2015)) sourced\nfrom different domains. Then we fine-tune two\ncopies of T5BASE on two datasets of the same task,\nand evaluate the linear mode connectivity between\ntwo minima. Note previous works typically study\nmode connectivity on the same dataset; while in\nour setting, we extend the analysis by evaluating\nthe interpolations on two datasets.\nThe results are shown in Figure 5. Take the\nNLI task as an example, starting from one endpoint\n(α=0) of a source task (MNLI), with the interpo-\nlation approaching the other endpoint ( α= 1) of\nthe target task (ANLI), the performance on MNLI\n/ ANLI exhibits almost a monotonous drop / rise.\nBesides, there does not exist a performance val-\nley where the performance is significantly lower\nthan both endpoints. Intuitively, the performance\nchange reflects the variation of the interpola-\ntion’s task knowledge along the connecting path.\nDue to the difference in data domain, the task\nknowledge of two endpoints only partially overlap\nwith each other. When traversing from the source\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Finetune\nPretrain 5.0k\nPretrain 6.25k\nPretrain 7.5k\nPretrain 8.75k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Adapter\nPretrain 5.0k\nPretrain 7.5k\nPretrain 10.0k\nPretrain 15.0k\nFigure 6: The change of linear mode connectivity at dif-\nferent pre-training steps. We illustrate the performance\nof linear interpolations of two minima trained on MNLI\nusing different initialization.\nminimum to the target minimum, PLM suffers from\ncatastrophic forgetting on the source knowledge,\nbut gradually absorbs target knowledge, leading to\nthe performance drop / rise on the source / target\ntask. We defer more in-depth analyses to Q3.\nQ2. How does mode connectivity change\nduring pre-training?\nPrevious works demonstrate that compared with\nrandom initialization, the initialization obtained by\npre-training leads to a wider loss basin after down-\nstream adaptation (Hao et al., 2019; Neyshabur\net al., 2020). Intuitively, if a local minimum lies\nin a more flat basin, it should be easier to con-\nnect with other minima. In this sense, pre-training\nmay be closely related to mode connectivity. To\ninvestigate this, we re-train a RoBERTaBASE model\nfrom scratch and explore how mode connectivity\nchanges at different pre-training steps. We follow\nthe pre-training setting of Liu et al. (2019b), with\nmore details described in appendix C.4.\nPre-training Facilitates Mode Connectivity.\nWe select a series of checkpoints at different pre-\ntraining steps. For each checkpoint, we adapt two\ncopies on MNLI using different initialization, and\nevaluate the performance of their linear interpola-\ntions. From Figure 6, we observe that for both fine-\ntuning and adapter tuning, with the pre-training\nstep becoming larger, the mode connectivity of the\nPLM becomes better. This implies that pre-training\nimplicitly facilitates the mode connectivity. Specif-\nically, when using fine-tuning, there does not exist\na performance drop for checkpoints pre-trained\nwith more than 6.25k steps. Considering that pre-\ntraining with a batch size of 2048 for 6.25k steps\ncorresponds to almost 5% the computational cost\nof BERT (a batch size of 256 for 1000k steps), we\n6731\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2\nMNLI\nMNLI~SST-2 Finetune\nPretrain 15k\nPretrain 30k\nPretrain 45k\nPretrain 60k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2\nMNLI\nMNLI~SST-2 Adapter\nPretrain 30k\nPretrain 45k\nPretrain 60k\nFigure 7: The change of linear mode connectivity at dif-\nferent pre-training steps. We illustrate the performance\nof linear interpolations of two minima trained on MNLI\nand SST-2. α = 0/ α = 1denotes the minimum of\nMNLI / SST-2.\nconclude that PLMs acquire good mode connectiv-\nity at an early stage of pre-training.\nPre-training Pulls Task Boundary Closer. Fur-\nther, we look into the performance variation along\na linear path between two minima trained on two\ndifferent tasks (MNLI and SST-2 (Socher et al.,\n2013)). Similarly, we choose a series of check-\npoints at different pre-training steps. Then for each\ncheckpoint, we adapt two copies on MNLI and\nSST-2 under the same setting, and conduct linear\ninterpolation between two adapted weights. We\nalso conduct experiments on MNLI and QQP (link)\nin appendix B.6. We evaluate the performance of\neach interpolation on both tasks and illustrate the\nresults in Figure 7. It can be derived that (1) due to\nthe inherent difference of both tasks, the minimum\nobtained on one task achieves the performance near\nrandom guess (≈50% for SST-2 and ≈33.3% for\nMNLI) on another task. This indicates that minima\nof different tasks are disconnected. (2) In addition,\nthere is a strong general trend that for a checkpoint\npre-trained longer, the intersection of both tasks’\nhigh-performance regions becomes wider. In other\nwords, the boundaries of both tasks’ optimal re-\ngions are gradually pulled closer by pre-training.\n(3) For the checkpoint pre-trained with 60k steps,\nwe do not observe a region where the performance\non both tasks is poor. This means starting from\nthe initialization of a PLM pre-trained with enough\nsteps, the optimal regions of various downstream\ntasks are closely packed. This finding may help\nexplain PLM’s cross-task transferability, and we\nleave more discussions in § 5.\nQ3. How does the task knowledge change\nalong the path connecting two minima?\nHaving shown that mode connectivity reflects\nthe high overlap of task knowledge of different\nminima, we further investigate the knowledge vari-\nation along the path connecting two minima. To\nquantify a model’s task knowledge, we resort to\nthe memorization of the training data as a rough\nestimate. In experiments, we evaluate two min-\nima obtained on data of different distributions as\nmentioned in Q1. (b)1.\nSpecifically, we adapt two copies of T5BASE\non MNLI (source task) and ANLI (target task),\nrespectively. Denote θs and θt as two minima\ntrained on the source dataset Ds = {xi,yi}|Ds|\ni=1\nand the target dataset Dt = {xi,yi}|Dt|\ni=1 . We in-\nvestigate the knowledge variation from θs to θt\nby choosing 4 evenly distributed linear interpola-\ntions (ϕ1, ϕ2, ϕ3, ϕ4) and 2 endpoints (ϕ0, ϕ5), i.e.,\nϕj = θs + j\n5 ·(θt −θs), j∈{0,1,..., 5}, where\nϕ0 = θs, ϕ5 = θt. Then we measure whether each\nsource training sample xi∈Ds is memorized (cor-\nrectly classified) by ϕj. We find empirically that\nwith ϕj approaching θt, training samples of Ds are\ngradually forgotten, but seldom re-memorized un-\nder this setting. Therefore, we only record those\nnewly forgotten samples for ϕj (i.e., those clas-\nsified correctly by ϕj−1 but wrongly by ϕj) and\ndenote them as Fj. Similarly, we denote those\nnewly memorized samples of Dt as Mj (i.e., those\nclassified wrongly by ϕj−1 but correctly by ϕj).\nAfter that, we characterize the role of each sam-\nple using dataset cartography (Swayamdipta et al.,\n2020). For a brief introduction, each sample of\nDs (Dt) is characterized by the training dynam-\nics of θs (θt). Take Ds as an example, assume\nwe train the PLM for E epochs on Ds, and the\nweights of the PLM are adapted to θs(e) after the\ne-th epoch, where 1 ≤e ≤E. For each train-\ning instance (xi,yi) ∈Ds, denote Pθs(e)(yi|xi) as\nthe probability θs(e) assigns to the true label, we\nrecord the PLM’s prediction after each epoch and\ncalculate two statistics:\n•confidence measures how confidently the PLM\nassigns the true label to a given input, it is defined\nas the mean probability of the true label:\nµi = 1\nE\nE∑\ne=1\nPθs(e)(yi|xi).\n•variability captures how consistently the PLM\n1We choose this setting because (1) there does not exist\na performance valley between two minima, which means the\nknowledge is properly combined, and (2) the knowledge of\nboth tasks is diverse enough.\n6732\nF1 F2 F3 F4 F5\n50\n60\n70\n80\n90\n100Confidence (%)\nMNLI\nFinetune\nAdapter\nF1 F2 F3 F4 F5\n0\n10\n20\n30\n40Variance (%)\nMNLI\nFinetune\nAdapter\nM1 M2 M3 M4 M5\n50\n60\n70\n80\n90\n100Confidence (%)\nANLI\nFinetune\nAdapter\nM1 M2 M3 M4 M5\n0\n10\n20\n30\n40Variance (%)\nANLI\nFinetune\nAdapter\nFigure 8: The results of knowledge variation for linear interpolations (ϕ1, ϕ2, ϕ3, ϕ4) between two minima (ϕ0, ϕ5)\nadapted on MNLI and ANLI. We leave experiments on other tasks as future work.\njudges each training instance, it is defined using the\nstandard deviation of the true label’s probability:\nσi =\n√ ∑E\ne=1(Pθs(e)(yi|xi) −µi)2\nE .\nAfter obtaining both statistics for each training\nsample of Ds and Dt, we illustrate the average\nstatistics for newly forgotten / memorized samples\n(Fj / Mj) in Figure 8. We observe that with ϕj ap-\nproaching θt, the average confidence of the newly\nforgotten data gradually increases, while the vari-\nability gradually drops; symmetrically, the average\nstatistics of the newly learned data exhibit an oppo-\nsite trend. According to Swayamdipta et al. (2020),\ninstances with high confidence but low variability\nare generally easy-to-learn ones; while those with\nlow confidence are generally ambiguous or hard-to-\nlearn data. In this regard, when gradually leaving\nthe source minimum, the PLM prioritizes forget-\nting the source knowledge of those difficult in-\nstances, and then forgets the source knowledge\nof the easy-to-learn data . On the contrary, the\neasy-to-learn target knowledge is learned before\nthe elusive and obscure target knowledge.\n5 Discussion\nWeight Averaging. The property of linear mode\nconnectivity is related to recent explorations of\nweight averaging (Wortsman et al., 2021, 2022;\nMatena and Raffel, 2021), which combines inde-\npendently fine-tuned models in the parameter space.\nIn this way, the knowledge from multiple models\ncan be merged. Our findings have direct implica-\ntions for designing better weight averaging meth-\nods: (1) for two minima, weight averaging can be\nseen as choosing the midpoint on the linear path.\nWe have shown that a non-linear curve may have\nbetter mode connectivity under certain cases. This\nimplicates that linear interpolation may not find the\noptimal combination despite its simplicity; instead,\nthere may exist better methods to ensemble weights\n(see experiments in appendix B.8); (2) our findings\non the effects of different training configurations\ncan also inspire choosing more appropriate models\n(with better mode connectivity) to ensemble.\nTask-level Transferability. Although PLMs are\ndemonstrated to have excellent cross-task trans-\nferability (Vu et al., 2020; Pruksachatkun et al.,\n2020; Poth et al., 2021; Su et al., 2022), it is still\nunder-explored why PLMs have such an ability.\nOur findings that pre-training implicitly pulls the\ntask boundary closer may help explain this phe-\nnomenon. Since the optimal regions of various\ntasks are packed closely, PLMs are easier to tra-\nverse across the task boundary, without getting\nblocked by a loss barrier.\nKnowledge Quantification. Investigating the\nknowledge variation along the connecting path\nhelps better understand how different model knowl-\nedge is merged. Quantifying the task knowledge\nof various models may also provide insights for\nresearch topics like knowledge distillation (Hin-\nton et al., 2015) and knowledge transfer (Weiss\net al., 2016). While we use training data memo-\nrization as a rough estimate for task knowledge,\nit would be interesting to explore whether there\nexist more granular methods, such as knowledge\nprobing (Petroni et al., 2019; Liu et al., 2019a).\n6 Conclusions\nIn this paper, we conduct empirical analyses on\nthe mode connectivity of PLMs, aiming to fathom\nthe connection of minima reached under different\nsettings. We investigate how different downstream\nadaptation configurations and pre-training affect\nPLM’s mode connectivity. In addition, we explore\nthe knowledge variation along the path connecting\ndifferent minima. In general, exploring the mode\nconnectivity of PLMs contributes to understanding\n6733\nthe inner workings of PLM downstream adaptation.\nWe expect our evaluation setup and analyses could\ninspire more future explorations in this field.\nAcknowledgments\nThis work is supported by the National Key R&D\nProgram of China (No. 2020AAA0106502) and\nInstitute Guo Qiang at Tsinghua University.\nYujia Qin designed the experiments and wrote\nthe paper. Cheng Qian and Jing Yi conducted the\nexperiments. Yankai Lin, Zhiyuan Liu, Maosong\nSun, and Jie Zhou advised the project. All authors\nparticipated in the discussion.\nThe authors would like to thank anonymous re-\nviewers for their valuable feedback.\nLimitations\nThere are some limitations not well addressed in\nthis paper:\n• The goal of this paper is to investigate the\nconnection among different minima. However,\nsince we use mode connectivity as the analysis\ntool, we only investigate the connection between\ntwo minima at a time. In this regard, it would\nbe interesting to develop more advanced tools to\nexplore the connection among multiple minima\nsimultaneously, which is left as future work.\n• We do not give a strict mathematical definition\nfor “good mode connectivity”. For instance,\nwe do not set a specific threshold of perfor-\nmance drop (e.g., > 5% for the case of “not\nwell-connected minima”). We argue that this is\nbecause, all of our experimental results are sig-\nnificant enough, thus there is no need to follow\na strict definition.\nReferences\nArmen Aghajanyan, Anchit Gupta, Akshat Shrivastava,\nXilun Chen, Luke Zettlemoyer, and Sonal Gupta.\n2021. Muppet: Massive multi-task representations\nwith pre-finetuning. In Proceedings of the 2021 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 5799–5811, Online and Punta\nCana, Dominican Republic. Association for Com-\nputational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2022. Quantifying memorization across neural lan-\nguage models. ArXiv preprint, abs/2202.07646.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar\nErlingsson, et al. 2021. Extracting training data from\nlarge language models. In 30th USENIX Security\nSymposium (USENIX Security 21), pages 2633–2650.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zong-\nhan Yang, Yusheng Su, Shengding Hu, Yulin Chen,\nChi-Min Chan, Weize Chen, et al. 2022. Delta tuning:\nA comprehensive study of parameter efficient meth-\nods for pre-trained language models. arXiv preprint\narXiv:2203.06904.\nFelix Draxler, Kambis Veschgini, Manfred Salmhofer,\nand Fred A. Hamprecht. 2018. Essentially no bar-\nriers in neural network energy landscape. In Pro-\nceedings of the 35th International Conference on\nMachine Learning, ICML 2018, Stockholmsmässan,\nStockholm, Sweden, July 10-15, 2018, volume 80 of\nProceedings of Machine Learning Research, pages\n1308–1317. PMLR.\nJonathan Frankle and Michael Carbin. 2019. The lottery\nticket hypothesis: Finding sparse, trainable neural\nnetworks. In 7th International Conference on Learn-\ning Representations, ICLR 2019, New Orleans, LA,\nUSA, May 6-9, 2019. OpenReview.net.\nJonathan Frankle, Gintare Karolina Dziugaite, Daniel\nRoy, and Michael Carbin. 2020. Linear mode con-\nnectivity and the lottery ticket hypothesis. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, ICML 2020, 13-18 July 2020, Vir-\ntual Event, volume 119 of Proceedings of Machine\nLearning Research, pages 3259–3269. PMLR.\nC. Daniel Freeman and Joan Bruna. 2017. Topology\nand geometry of half-rectified network optimization.\n6734\nIn 5th International Conference on Learning Rep-\nresentations, ICLR 2017, Toulon, France, April 24-\n26, 2017, Conference Track Proceedings. OpenRe-\nview.net.\nTimur Garipov, Pavel Izmailov, Dmitrii Podoprikhin,\nDmitry P. Vetrov, and Andrew Gordon Wilson. 2018.\nLoss surfaces, mode connectivity, and fast ensem-\nbling of dnns. In Advances in Neural Information\nProcessing Systems 31: Annual Conference on Neu-\nral Information Processing Systems 2018, NeurIPS\n2018, December 3-8, 2018, Montréal, Canada, pages\n8803–8812.\nXu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao\nLiu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang,\nLiang Zhang, et al. 2021. Pre-trained models: Past,\npresent and future. AI Open, 2:225–250.\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visu-\nalizing and understanding the effectiveness of BERT.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4143–\n4152, Hong Kong, China. Association for Computa-\ntional Linguistics.\nDan Hendrycks, Xiaoyuan Liu, Eric Wallace, Adam\nDziedzic, Rishabh Krishnan, and Dawn Song. 2020.\nPretrained transformers improve out-of-distribution\nrobustness. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 2744–2751, Online. Association for Computa-\ntional Linguistics.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. ArXiv\npreprint, abs/1503.02531.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In Pro-\nceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings\nof Machine Learning Research , pages 2790–2799.\nPMLR.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059, Online and Punta Cana, Domini-\ncan Republic. Association for Computational Lin-\nguistics.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, et al. 2021. Datasets: A commu-\nnity library for natural language processing. arXiv\npreprint arXiv:2109.02846.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nXueqing Liu and Chi Wang. 2021. An empirical study\non hyperparameter optimization for fine-tuning pre-\ntrained language models. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 2286–2300, Online. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoBERTa: A robustly optimized BERT pretraining\napproach. ArXiv preprint, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nMichael Matena and Colin Raffel. 2021. Merging mod-\nels with fisher-weighted averaging. ArXiv preprint,\nabs/2111.09832.\nMichael McCloskey and Neal J Cohen. 1989. Catas-\ntrophic interference in connectionist networks: the se-\nquential learning problem. In Psychology of learning\nand motivation, volume 24, pages 109–165. Elsevier.\nSeyed Iman Mirzadeh, Mehrdad Farajtabar, Dilan Gorur,\nRazvan Pascanu, and Hassan Ghasemzadeh. 2020.\nLinear mode connectivity in multitask and continual\nlearning. arXiv preprint arXiv:2010.04495.\nBehnam Neyshabur, Hanie Sedghi, and Chiyuan Zhang.\n2020. What is being transferred in transfer learning?\nAdvances in neural information processing systems,\n33:512–523.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2020. Adversarial\nNLI: A new benchmark for natural language under-\nstanding. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4885–4901, Online. Association for Computa-\ntional Linguistics.\nBo Pang and Lillian Lee. 2005. Seeing stars: Exploit-\ning class relationships for sentiment categorization\nwith respect to rating scales. In Proceedings of the\n43rd Annual Meeting of the Association for Compu-\ntational Linguistics (ACL’05), pages 115–124, Ann\nArbor, Michigan. Association for Computational Lin-\nguistics.\n6735\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nClifton Poth, Jonas Pfeiffer, Andreas Rücklé, and Iryna\nGurevych. 2021. What to pre-train on? Efficient\nintermediate task selection. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 10585–10605, Online\nand Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nYada Pruksachatkun, Jason Phang, Haokun Liu,\nPhu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang,\nClara Vania, Katharina Kann, and Samuel R. Bow-\nman. 2020. Intermediate-task transfer learning with\npretrained language models: When and why does it\nwork? In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n5231–5247, Online. Association for Computational\nLinguistics.\nYujia Qin, Yankai Lin, Jing Yi, Jiajie Zhang, Xu Han,\nZhengyan Zhang, Yusheng Su, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2022a. Knowledge\ninheritance for pre-trained language models. In Pro-\nceedings of the 2022 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n3921–3937, Seattle, United States. Association for\nComputational Linguistics.\nYujia Qin, Jiajie Zhang, Yankai Lin, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2022b. ELLE: Ef-\nficient lifelong pre-training for emerging data. In\nFindings of the Association for Computational Lin-\nguistics: ACL 2022, pages 2789–2810, Dublin, Ire-\nland. Association for Computational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 255–269, Online. Association for Computa-\ntional Linguistics.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352, Online. Association\nfor Computational Linguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models for\nsemantic compositionality over a sentiment treebank.\nIn Proceedings of the 2013 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1631–1642, Seattle, Washington, USA. Association\nfor Computational Linguistics.\nYusheng Su, Xiaozhi Wang, Yujia Qin, Chi-Min Chan,\nYankai Lin, Huadong Wang, Kaiyue Wen, Zhiyuan\nLiu, Peng Li, Juanzi Li, Lei Hou, Maosong Sun, and\nJie Zhou. 2022. On transferability of prompt tuning\nfor natural language processing. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3949–3969,\nSeattle, United States. Association for Computational\nLinguistics.\nSwabha Swayamdipta, Roy Schwartz, Nicholas Lourie,\nYizhong Wang, Hannaneh Hajishirzi, Noah A. Smith,\nand Yejin Choi. 2020. Dataset cartography: Mapping\nand diagnosing datasets with training dynamics. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9275–9293, Online. Association for Computa-\ntional Linguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipanjan\nDas, and Ellie Pavlick. 2019. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nTu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessan-\ndro Sordoni, Adam Trischler, Andrew Mattarella-\nMicke, Subhransu Maji, and Mohit Iyyer. 2020. Ex-\nploring and predicting transferability across NLP\ntasks. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 7882–7926, Online. Association for\nComputational Linguistics.\nKarl Weiss, Taghi M Khoshgoftaar, and DingDing\nWang. 2016. A survey of transfer learning. Jour-\nnal of Big data, 3(1):1–40.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\n6736\nMitchell Wortsman, Gabriel Ilharco, Samir Yitzhak\nGadre, Rebecca Roelofs, Raphael Gontijo-Lopes,\nAri S Morcos, Hongseok Namkoong, Ali Farhadi,\nYair Carmon, Simon Kornblith, et al. 2022. Model\nsoups: averaging weights of multiple fine-tuned mod-\nels improves accuracy without increasing inference\ntime. ArXiv preprint, abs/2203.05482.\nMitchell Wortsman, Gabriel Ilharco, Mike Li,\nJong Wook Kim, Hannaneh Hajishirzi, Ali Farhadi,\nHongseok Namkoong, and Ludwig Schmidt. 2021.\nRobust fine-tuning of zero-shot models. ArXiv\npreprint, abs/2109.01903.\nChuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng\nHuang. 2022. NoisyTune: A little noise can help\nyou finetune pretrained language models better. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 680–685, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng\nGao, Kevin Duh, and Benjamin Van Durme. 2018.\nReCoRD: Bridging the gap between human and ma-\nchine commonsense reading comprehension.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems, volume 28. Curran Associates, Inc.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015, pages 19–27.\nIEEE Computer Society.\n6737\nAppendices\nA Details for Finding a Bezier Curve\nWe follow Garipov et al. (2018) to find a quadratic\nBezier curve connecting two endpoints θC1 and\nθC2 . The Bezier curve is defined as follows:\nϕθ(α) = (1−α)2 ·θC1 + 2α(1 −α)θ+ α2 ·θC2 .\nDuring curve finding, only θ∈R|θ0|is optimized\nto minimize the expectation of loss over a uniform\ndistribution on the curve as follows:\nLcurve(θ) =\n∫\nL(ϕθ)dϕθ∫\ndϕθ\n=\n∫1\n0 L(ϕθ(α))||ϕ′\nθ(α)||dα\n∫1\n0 ||ϕ′\nθ(α)||dα\n=\n∫1\n0\nL(ϕθ(α))qθ(α)dα=Eα∼qθ(α)L(ϕθ(α)),\nwhere qθ(α) = ||ϕ′\nθ(α)||dα∫ 1\n0 ||ϕ′\nθ(α)||dα. Since qθ(α) is depen-\ndent on θ, it is generally intractable to compute the\noriginal loss Lcurve(θ). To this end, Garipov et al.\n(2018) suggest optimizing a more computationally\ntractable loss as follows:\nL′\ncurve(θ) =Eα∈U(0,1)L(ϕθ(α)),\nwhere α is sampled from a uniform distribution\nU(0,1) instead of qθ(α). In experiments, we ini-\ntialize θ with 1\n2 θC1 + 1\n2 θC2 (i.e., starting from a\nlinear curve), which makes training more stable\nthan using randomly initialized weights or the pre-\ntrained weights.\nB Additional Experiments\nB.1 Effects of the Learning Rate and the\nBatch Size\nWe perform experiments to explore the effects of\nboth learning rates and batch sizes. For the for-\nmer, we evaluate when both endpoints are trained\nwith a learning rate of {1 ×10−4,5 ×10−4}and\n{1×10−4,5×10−5}, and the batch size is set to16;\nfor the latter, we chose a batch size of {16,8}and\n{16,32}, and the learning rate is set to 1 ×10−4.\nThe experiments are conducted using both full-\nparameter fine-tuning and adapter tuning on MNLI\nand SST-2 with T5BASE. For MNLI, we experiment\nwhen both endpoints are trained for {10k, 30k, 50k}\nsteps; for SST-2, both endpoints are trained for {3k,\n9k, 15k} steps. We illustrate the results of linear\ninterpolation in Figure 9 and Figure 10. We could\nconclude from both figures that, the minima ob-\ntained by fine-tuning are always well-connected\nby the linear path; however, the connectivity of\nadapter is poor under certain cases. This is aligned\nwith the finding in the main paper that the mode\nconnectivity of fine-tuning is generally better than\ndelta tuning. We also observe that with the training\nsteps becoming larger, the connectivity of adapter\ntuning sometimes becomes poorer.\nB.2 Additional Experiments for the Effects of\nTraining Steps\nIn the main paper, when exploring the effects\nof training steps, we experiment with the setting\nwhere both endpoints are trained with the same\nnumber of steps. We show that mode connectivity\ncould be poorer when both endpoints are trained\nfor longer steps. To more rigorously investigate the\neffects of training steps, we experiment when both\nminima are obtained when using different train-\ning steps. Specifically, we adapt T5BASE model on\nMNLI and ReCoRD using both fine-tuning and\nadapter tuning. We train two endpoints with differ-\nent initialization, which is implemented by utilizing\ndifferent random seeds, but keeping the configura-\ntion of the initialization (mean and standard devi-\nation of the normal distribution) the same. After\nthat, one endpoint (denoted as θC1 ) is adapted for\n50k steps, while the other endpoint is adapted for\n{10k, 20k, 30k, 40k} steps, and denoted as { θ10k\nC2 ,\nθ20k\nC2 , θ30k\nC2 , θ40k\nC2 }, respectively. Then we evaluate\nthe linear interpolations between {(θC1 and θ10k\nC2 ),\n(θC1 and θ20k\nC2 ), ( θC1 and θ30k\nC2 ), ( θC1 and θ40k\nC2 )},\nrespectively. The results are shown in Figure 11,\nfrom which we observe that, the mode connectivity\nof fine-tuning is generally good, while two minima\nof adapter are not well-connected. In addition, with\nthe gap of the training steps between two endpoints\nbecoming larger, the mode connectivity of adapter\nbecomes poorer. These results suggest that the\nnumber of training steps can affect PLM’s mode\nconnectivity under certain cases.\nEuclidean Distance Analysis. To better under-\nstand the reason why training steps could affect\nmode connectivity, we record the Euclidean dis-\ntance variation of two endpoints during down-\nstream adaptation. Both endpoints start from dif-\nferent initialization. We adapt the PLM on MNLI\nusing both fine-tuning and adapter tuning for {10k,\n20k, 30k, 40k, 50k} steps, and obtain a series\nof checkpoints: {( θ10k\nC1 and θ10k\nC2 ), (θ20k\nC1 and θ20k\nC2 ),\n(θ30k\nC1 and θ30k\nC2 ), (θ40k\nC1 and θ40k\nC2 ), (θ50k\nC1 and θ50k\nC2 )}.\nThen we calculate the Euclidean distance of two\nendpoints as: ||θ∗\nC1 −θ∗\nC2 ||2. The change of Eu-\n6738\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k (b)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2 Finetune\nStep 3k\nStep 9k\nStep 15k (c)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2 Adapter\nStep 3k\nStep 9k\nStep 15k (d)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n(e)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k (f)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2 Finetune\nStep 3k\nStep 9k\nStep 15k (g)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2 Adapter\nStep 3k\nStep 9k\nStep 15k (h)\nFigure 9: Experiments of the effects of the learning rate. We conduct linear interpolations for MNLI and SST-2,\nusing both fine-tuning and adapter tuning. For (a-d), both minima are obtained with a learning rate of 1 ×10−4\nand 5 ×10−5, respectively; for (e-h), both minima are obtained with a learning rate of 1 ×10−4 and 5 ×10−4,\nrespectively.\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k (b)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2 Finetune\nStep 3k\nStep 9k\nStep 15k (c)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2 Adapter\nStep 3k\nStep 9k\nStep 15k (d)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n(e)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k (f)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2 Finetune\nStep 3k\nStep 9k\nStep 15k (g)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2 Adapter\nStep 3k\nStep 9k\nStep 15k (h)\nFigure 10: Experiments of the effects of the batch size. We conduct linear interpolations for MNLI and SST-2,\nusing both fine-tuning and adapter tuning. For (a-d), both minima are obtained with a batch size of 16 and 32,\nrespectively; for (e-h), both minima are obtained with a batch size of 16 and 8, respectively.\nclidean distance is visualized in Figure 12, from\nwhich we observe that, with the training steps be-\ncoming larger, the distance between two endpoints\nis also prolonged. This may partially explain the\npoorer mode connectivity with the increasing of\ntraining steps. We have shown in the main paper\nthat PLMs have multiple loss basins connected by a\nnon-linear path, instead of a linear path. Within the\nsame loss basins, most of the solutions have good\nlinear mode connectivity. However, since the loss\nbasin has a boundary, when the distance between\ntwo minima becomes large enough, they may fi-\nnally cross the border of the loss basin. Under this\nscenario, the linear path connecting both endpoints\nwould incur a high loss.\n6739\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Finetune\nStep 10k\nStep 20k\nStep 30k\nStep 40k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Adapter\nStep 10k\nStep 20k\nStep 30k\nStep 40k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test F1 (%)\nReCoRD Finetune\nStep 10k\nStep 20k\nStep 30k\nStep 40k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test F1 (%)\nReCoRD Adapter\nStep 10k\nStep 20k\nStep 30k\nStep 40k\nFigure 11: Experiments of the effects of the training steps. We conduct linear interpolations for MNLI and ReCoRD,\nusing both fine-tuning and adapter tuning. One endpoint is trained for 50k steps, while the other endpoint is trained\nfor {10k, 20k, 30k, 40k} steps, respectively.\n10k 20k 30k 40k 50k\nTuning Step\n0.00\n0.03\n0.06\n0.09\n0.12\n0.15\nAverage\nEuclidean Distance\nMNLI Finetune\n10k 20k 30k 40k 50k\nTuning Step\n0.00\n0.03\n0.06\n0.09\n0.12\n0.15\nAverage\nEuclidean Distance\nMNLI Adapter\n10k 20k 30k 40k 50k\nT uning Step\n0.00\n0.03\n0.06\n0.09\n0.12\n0.15\nAverage\nEuclidean Distance\nReCoRD Finetune\n10k 20k 30k 40k 50k\nTuning Step\n0.00\n0.03\n0.06\n0.09\n0.12\n0.15\nAverage\nEuclidean Distance\nReCoRD Adapter\nFigure 12: Euclidean distance (per neuron) of two minima at different training steps ({10k, 20k, 30k, 40k}) during\ndownstream adaptation. Two minima are trained from different initialization.\nB.3 More Experiments for Mode Connectivity\nalong a Non-linear Path\nIn the main paper, to explore the connectivity of\ntwo minima along a non-linear path, we experiment\non the setting of tuning adapters with different ini-\ntialization. This setting has been shown to have\npoor linear mode connectivity but good non-linear\nmode connectivity. In fact, in our pilot experiments,\nwe find that such a low-loss non-linear curve exists\nfor minima reached under various different settings.\nIn this section, we provide some of the experiments\nto demonstrate the above finding using T5BASE.\nSpecifically, we experiment with three tasks:\nMNLI, ReCoRD, and SST-2. For adapter tun-\ning, we choose the setting where two minima are\ntrained with (1) different training data order and\n(2) data from the same distribution but without spe-\ncific overlap of training instances. For (2), same as\nbefore, we randomly partition the original training\ndataset into two equal splits, and adapt two copies\nof PLM on each split. For fine-tuning, we choose\nthe setting where two minima are trained with (1)\ndifferent training steps (the setting is the same as\nappendix B.2), and (2) different initialization.\nThe performance of the interpolations are visual-\nized in Figure 13 for adapter tuning and Figure 14\nfor fine-tuning. We observe that under all the set-\ntings, we do not observe a significant performance\ndrop along the non-linear curve, showing that the\nconnectivity is good. The above results demon-\nstrate that PLMs may have multiple loss basins\nwhich are connected via a low-loss non-linear path.\nIn this paper, following Neyshabur et al. (2020), we\nspend most of the efforts on convex hull and linear\ninterpolation to avoid possibly trivial connectivity\nresults.\nB.4 Additional Experiments for the Effects of\nData Overlap\nIn the main paper, when evaluating the effects of\ndata overlap, we present the results on MNLI. In\nthis section, we visualize the results when using\nReCoRD and SST-2 in Figure 15. Other settings\nare kept the same as the main paper.\nB.5 Additional Experiments for the Effects of\nData Domain\nIn the main paper, when evaluating the effects of\ndata distributions (data domain), we present the\nresults when using fine-tuning. In this section, we\nvisualize the results when using adapter tuning in\nFigure 16. Other settings are kept the same as the\nmain paper.\nB.6 Additional Experiments for the Change of\nMode Connectivity during Pre-training\nIn the main paper, when evaluating the performance\nvariation between two minima trained on two differ-\nent tasks, we report the results of MNLI and SST-2.\n6740\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100T est ACC (%)\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100T est F1 (%)\nReCoRD Adapter\nStep 10k\nStep 30k\nStep 50k (b)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100T est ACC (%)\nSST-2 Adapter\nStep 3k\nStep 9k\nStep 15k (c)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100T est ACC (%)\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k\n(d)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100T est F1 (%)\nReCoRD Adapter\nStep 10k\nStep 30k\nStep 50k (e)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100T est ACC (%)\nSST-2 Adapter\nStep 3k\nStep 9k\nStep 15k (f)\nFigure 13: The performance of interpolations along a non-linear path connecting two minima trained with adapter\ntuning. For (a-c), two minima are trained with different training data order. For (d-f), two minima are trained with\nin-distribution data of the same task.\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Finetune\nStep 10k\nStep 40k\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test F1 (%)\nReCoRD Finetune\nStep 10k\nStep 40k (b)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2 Finetune\nStep 3k\nStep 9k (c)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n(d)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test F1 (%)\nReCoRD Finetune\nStep 10k\nStep 30k\nStep 50k (e)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2 Finetune\nStep 3k\nStep 9k\nStep 15k (f)\nFigure 14: The performance of interpolations along a non-linear path connecting two minima trained with fine-\ntuning. For (a-c), two minima are trained with different training steps. For (d-f), two minima are trained with\ndifferent initialization.\nIn this section, we present the results of MNLI and\nQQP in Figure 17. In fact, in our pilot studies, we\nfind that the conclusions in diverse tasks are very\nconsistent. Due to the concern about the energy\ncost, we only report the performance of two pairs\nof tasks.\nB.7 Performance along the Connecting Path\nWe show that better performance could be achieved\nby interpolating two independently trained weights\nin the parameter space. Specifically, we choose\nthe scenario where two copies of PLMs are trained\nwith different training data order. As mentioned\nin Q1. (a) in the main paper, PLMs have excellent\nmode connectivity under this setting. We experi-\nment with T5BASE using fine-tuning and adapter tun-\n6741\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test F1 (%)\nReCoRD Finetune\nStep 10k\nStep 30k\nStep 50k\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test F1 (%)\nReCoRD Adapter\nStep 10k\nStep 30k\nStep 50k (b)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2 Finetune\nStep 3k\nStep 9k\nStep 15k (c)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nSST-2 Adapter\nStep 3k\nStep 9k\nStep 15k (d)\nFigure 15: Linear mode connectivity analysis for two minima trained with in-distribution data. In (a-b), we\nexperiment with ReCoRD. In (c-d), we experiment with SST-2.\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC / F1 (%)\nANLI\nMNLI\nMNLI~ANLI Adapter\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test F1 (%)\nRotten Tomatoes\nYelp Polarity\nRT~YP Adapter\nStep 3k\nStep 9k\nStep 15k\nFigure 16: Linear mode connectivity for two minima\ntrained on different data distributions of the same task\nusing adapter tuning. Left: α= 0/ α= 1denotes the\nminimum of MNLI / ANLI. Right: α= 0/ α= 1de-\nnotes the minimum of Rotten Tomatoes / Yelp Polarity.\ning on MNLI, and conduct both linear interpolation\nand curved interpolation. We evaluate the perfor-\nmance of 24 evenly distributed points on the curve\non a development set, select the best-performing\none and evaluate its performance on the test set. We\nalso compare the interpolation with the endpoints\n(we report the best performance of the two end-\npoints). All experiments are conducted 3 times and\nwe report the average test results in Table 1. We ob-\nserve that by traversing along the connecting curve\nbetween two minima, we could find a solution that\nperforms better than both endpoints. In addition,\ntraversing along a linear path finds an interpolation\nwith higher performance than traversing along a\ncurved path2. In general, this finding demonstrates\nthat it is promising to combine the knowledge of\nmultiple models through weight averaging.\nB.8 Other Strategies for Weight Ensemble\nAs demonstrated in the main paper, the connec-\ntivity on a non-linear path may be better than a\nlinear path under certain cases. This phenomenon\n2Although we have shown that the non-linear mode con-\nnectivity is generally good for different minima, it does not\nmean that the best performance on a non-linear curve is always\nbetter than that on a linear curve.\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nQQP\nMNLI\nMNLI~QQP Finetune\nPretrain 15k\nPretrain 30k\nPretrain 45k\nPretrain 60k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0\n20\n40\n60\n80\n100Test ACC (%)\nQQP\nMNLI\nMNLI~QQP Adapter\nPretrain 30k\nPretrain 45k\nPretrain 60k\nFigure 17: The change of linear mode connectivity at\ndifferent pre-training steps. We illustrate the perfor-\nmance of linear interpolations of two minima trained on\nMNLI and QQP. α= 0/ α= 1denotes the minimum\nof MNLI / QQP.\ndemonstrates that despite the simplicity of linear\ninterpolation, there may exist better ways to in-\nterpolate two independently trained minima. To\ndemonstrate the existence of a better combination\nfor two minima, we take an initial step and propose\nto optimize such a combination. Specifically, sup-\npose all the tunable parameters of a PLM can be\ndivided into M components, i.e., θC1 = {θi\nC1 }M\ni=1,\nθC2 = {θi\nC2 }M\ni=1. We optimize a tunable vector\nα ∈RM to combine θC1 and θC2 as follows:\nθ(α) ={σ(αi) ·θi\nC1 + (1−σ(αi)) ·θi\nC2 }M\ni=1,\nwhere σdenotes a sigmoid function. During train-\ning, both θC1 and θC2 are kept frozen, and only α\nis tuned. We design three intuitive strategies for pa-\nrameter division: (1) layer-wise division, where the\nparameters within the same layer share the same\ncombination ratio; (2) module-wise division, where\nwe discriminate the combination ratio of the feed-\nforward network module and multi-head attention\nmodule in each layer. This means each module in\neach layer is assigned with an individual combi-\nnation ratio; (3) matrix-wise division, where each\nweight matrix in each module is combined individ-\nually. Matrix-wise division is the most fine-grained\n6742\nInterpolation Step Linear Curved Endpoint\nAdapter\n10k 84.58 84.14 84 .37\n30k 85.27 85.14 84 .96\n50k 85.70 85.23 85 .55\nFine-tuning\n10k 85.97 85.77 85 .85\n30k 87.36 87.27 87 .29\n50k 87.88 87.88 87.52\nTable 1: Test performance for interpolations of two min-\nima trained on MNLI using different training data order.\nFor both linear interpolation and curved interpolation,\nwe choose the best checkpoint based on the develop-\nment set performance. For two endpoints, we report the\nendpoint that performs better on the test set.\nStep Linear Layer Module Matrix Endpoint\n10k 86.0 86 .2 86.3 86.3 85 .9\n30k 87.4 87 .4 87 .4 87.5 87.3\n50k 87.9 87 .7 87 .5 88.0 87.5\nTable 2: Test performance for interpolations of two min-\nima fine-tuned on MNLI using different training data\norder. For linear interpolation (Linear), we choose the\nbest checkpoint based on the development set perfor-\nmance. For two endpoints ( Endpoint), we report the\nendpoint that performs better on the test set. Layer,\nModule, and Matrix denote layer-wise, module-wise,\nand matrix-wise divisions we proposed.\none among the above three strategies. Since we\nuse the T5BASE model, which consists of 12 en-\ncoders and 12 decoders, the number of components\nM for block-wise, layer-wise, and matrix-wise di-\nvisions are 24, 60, and 120 for adapter; and 28,\n64, and 282 for fine-tuning. During training, we\nperform grid search on a series of learning rates\n{0.1,0.05,0.01}and set the batch size to 8, and\nmax training steps to 100k.\nTwo endpoints are obtained by fine-tuning\nT5BASE on MNLI using different training data or-\nder. The results are shown in Table 2, from which\nwe find that, among the proposed three combina-\ntion strategies, matrix-wise division achieves the\nbest performance. The performance is also better\nthan using linear interpolation. This phenomenon\ndemonstrates that there exist better ways for com-\nbining two minima’s knowledge than linear interpo-\nlation. We hope our findings on mode connectivity\nin this paper could inspire future works to design\nbetter weight ensemble methods.\nC Training Details\nFor the T5BASE model, we use the checkpoint pro-\nvided by Lester et al. (2021), who conducted ad-\nTask LR BS SI\nMNLI 5 ×10−5 32 10 k\nReCoRD 1 ×10−4 32 10 k\nANLI 1 ×10−4 32 10 k\nSST-2 5 ×10−5 8 3 k\nRotten Tomatoes 5 ×10−5 32 3 k\nYelp Polarity 5 ×10−4 8 3 k\nTable 3: Hyperparameters (LR: learning rate, BS: batch\nsize, SI: saving interval) for different tasks during the\nfine-tuning of T5BASE model.\nditional 100k steps of language modeling adap-\ntion on the official checkpoints released by Raf-\nfel et al. (2020). Such adaptation is demon-\nstrated to help stabilize downstream adaptation\nand improve the performance, especially for delta\ntuning methods (Lester et al., 2021). We use\nAdamW (Loshchilov and Hutter, 2019) as the opti-\nmizer for all the experimented PLMs. All the im-\nplementation codes, trained checkpoints and used\ndatasets would be released after publication.\nWe download all the experimented datasets from\nHuggingface Datasets (Lhoest et al., 2021). Since\nsome datasets do not contain a test set, we first\nmerge all the data points, and then split them into\nthe new training split, development split, and test\nsplit with an approximate ratio of 8 : 1 : 1. The\nabove procedure is conducted on all the experi-\nmented datasets.\nFor different tasks fine-tuned on T5BASE, we first\nconduct grid search to find an optimal hyperparam-\neter combination. Specifically, the chosen hyperpa-\nrameter of different tasks for fine-tuning is shown\nin Table 3; for adapter tuning, in our prior exper-\niments, we find that a learning rate of 5 ×10−4\nand a batch size of 16 performs good on all tasks,\nthus we set them as the default configuration. For\nboth tuning methods, we save 5 checkpoints during\ntraining, with different saving intervals for different\ntasks as shown in Table 3.\nC.1 Additional Details for the Effects of\nInitialization\nFor adapter tuning, all the modules newly intro-\nduced are initialized using a Gaussian distribution.\nAs for fine-tuning, we add Gaussian noise to all\nthe tunable parameters. The mean and standard\ndeviation of the Gaussian distribution are set to 0\nand 0.0002, respectively. We use different random\nseeds to generate different initialization.\n6743\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nReCoRD Adapter\nStep 10k\nStep 30k\nStep 50k\nFigure 18: The loss of curved interpolations between\ntwo minima trained with adapter tuning from different\ninitialization. The corresponding performance visual-\nization is Figure 3.\nC.2 Additional Details for Curve Finding\nWhen optimizing the Bezier curve, we set the learn-\ning rate to 1 ×10−4, batch size to 8, max training\nsteps to 5k for fine-tuning; and set the learning rate\nto 1 ×10−4, batch size to 16, max training steps\nto 10k for adapter tuning. During curve finding,\nwe evaluate the development performance of the\ncurrent curve for every 100 steps, using a series\ninterpolations with α∈{0.25,0.5,0.75}.\nC.3 Additional Details for Calculating\nConfidence and Variability\nAs mentioned in the main paper, we use the training\ndynamics to characterize each training sample. For\nMNLI / ANLI, we adapt the model for8 epochs / 20\nepochs to calculate both confidence and variability.\nWe tune more epochs for ANLI because the size of\nits training dataset is far smaller than that of MNLI.\nC.4 Additional Details for Pre-training\nRoBERTaBASE\nWe closely follow the pre-training setting of Liu\net al. (2019b), except that for pre-training data, we\nuse the concatenation of Wikipedia and BookCor-\npus (Zhu et al., 2015) same as BERT (Devlin et al.,\n2019), and we pre-train our model with a batch\nsize of 2048. The pre-training implementations\nfor RoBERTaBASE are based on those of Qin et al.\n(2022a,b). Adam (Loshchilov and Hutter, 2019)\nis chosen as the optimizer. The hyperparameters\nfor the optimizer is set to 1 ×10−6, 0.9, 0.98 for ϵ,\nβ1, β2, respectively. We set the dropout rate to 0.1,\nweight decay to 0.01 and use linear learning rate\ndecay. The model architecture is the same as the of-\nficial RoBERTaBASE model (Liu et al., 2019b). The\npre-training is conducted using 8 NVIDIA V100\nGPUs.\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nMNLI Finetune\nPretrain 5.0k\nPretrain 6.25k\nPretrain 7.5k\nPretrain 8.75k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.4\n0.8\n1.2\n1.6\n2.0T est Loss\nMNLI Adapter\nPretrain 5.0k\nPretrain 7.5k\nPretrain 10.0k\nPretrain 12.5k\nPretrain 15.0k\nFigure 19: The loss of the change of mode-connectivity\nat different pre-training steps. We report the results of\nlinear interpolations of two minima trained on MNLI\nusing different initialization. The corresponding perfor-\nmance visualization is Figure 6.\nD The Visualization of Loss for\nInterpolations\nAs mentioned before, we record both loss and per-\nformance for each interpolation. Since we find that\nthe trends of loss and performance are generally\nhighly correlated, due to the length limit, we only\nreport the performance in the main paper. In this\nsection, we visualize the loss for most of the ex-\nperiments conducted in this paper, see Figure 18,\nFigure 19, Figure 20, Figure 21, Figure 22, Fig-\nure 23, Figure 24, and Figure 25.\n6744\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nReCoRD Finetune\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nReCoRD Adapter\nStep 10k\nStep 30k\nStep 50k\nFigure 20: The loss of linear interpolations between two minima trained with different training data order. The\ncorresponding performance visualization is Figure 1.\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.4\n0.8\n1.2\n1.6\n2.0T est Loss\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nReCoRD Finetune\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.4\n0.8\n1.2\n1.6\n2.0T est Loss\nReCoRD Adapter\nStep 10k\nStep 30k\nStep 50k\nFigure 21: The loss of linear interpolations between two minima trained with different initialization. The corre-\nsponding performance visualization is Figure 2.\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.4\n0.8\n1.2\n1.6\n2.0T est Loss\nReCoRD Finetune\nStep 10k\nStep 30k\nStep 50k\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nReCoRD Adapter\nStep 10k\nStep 30k\nStep 50k\nFigure 22: Linear mode connectivity analysis (loss) for two minima trained with in-distribution data. The corre-\nsponding performance visualization is Figure 4 and Figure 15.\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.4\n0.8\n1.2\n1.6\n2.0Test Loss\nSST-2\nMNLI\nMNLI~SST-2 Finetune\nPretrain 15k\nPretrain 30k\nPretrain 45k\nPretrain 60k\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.4\n0.8\n1.2\n1.6\n2.0Test Loss\nSST-2\nMNLI\nMNLI~SST-2 Adapter\nPretrain 30k\nPretrain 45k\nPretrain 60k (b)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.4\n0.8\n1.2\n1.6\n2.0Test Loss\nQQP\nMNLI\nMNLI~QQP Finetune\nPretrain 15k\nPretrain 30k\nPretrain 45k\nPretrain 60k (c)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.4\n0.8\n1.2\n1.6\n2.0Test Loss\nQQP\nMNLI\nMNLI~QQP Adapter\nPretrain 30k\nPretrain 45k\nPretrain 60k (d)\nFigure 23: The results (loss) of the change of mode-connectivity at different pre-training steps. (a-b) record\nthe results of linear interpolations of two minima trained on MNLI and SST-2, (c-d) record the results of linear\ninterpolations of two minima trained on MNLI and QQP. The corresponding performance visualization is Figure 7\nand Figure 17.\n6745\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k (b)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nSST-2 Finetune\nStep 3k\nStep 9k\nStep 15k (c)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nSST-2 Adapter\nStep 3k\nStep 9k\nStep 15k (d)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n(e)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k (f)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0T est Loss\nSST-2 Finetune\nStep 3k\nStep 9k\nStep 15k (g)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.4\n0.8\n1.2\n1.6\n2.0T est Loss\nSST-2 Adapter\nStep 3k\nStep 9k\nStep 15k (h)\nFigure 24: Experiments of the effects of the learning rate. We conduct linear interpolations for MNLI and SST-2,\nusing both fine-tuning and adapter tuning, and visualize their loss. For (a-d), both minima are obtained with a\nlearning rate of 1 ×10−4 and 5 ×10−5, respectively; for (e-h), both minima are obtained with a learning rate of\n1 ×10−4 and 5 ×10−4, respectively. The corresponding performance visualization is Figure 9.\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Test Loss\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n(a)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.4\n0.8\n1.2\n1.6\n2.0Test Loss\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k (b)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Test Loss\nSST-2 Finetune\nStep 3k\nStep 9k\nStep 15k (c)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Test Loss\nSST-2 Adapter\nStep 3k\nStep 9k\nStep 15k (d)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Test Loss\nMNLI Finetune\nStep 10k\nStep 30k\nStep 50k\n(e)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Test Loss\nMNLI Adapter\nStep 10k\nStep 30k\nStep 50k (f)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Test Loss\nSST-2 Finetune\nStep 3k\nStep 9k\nStep 15k (g)\n0.0 0.2 0.4 0.6 0.8 1.0\nInterpolation \n0.0\n0.6\n1.2\n1.8\n2.4\n3.0Test Loss\nSST-2 Adapter\nStep 3k\nStep 9k\nStep 15k (h)\nFigure 25: Experiments of the effects of the batch size. We conduct linear interpolations for MNLI and SST-2,\nusing both fine-tuning and adapter tuning, and visualize their loss. For (a-d), both minima are obtained with a batch\nsize of 16 and 32, respectively; for (e-h), both minima are obtained with a batch size of 16 and 8, respectively. The\ncorresponding performance visualization is Figure 10.\n6746",
  "topic": "Maxima and minima",
  "concepts": [
    {
      "name": "Maxima and minima",
      "score": 0.9332202672958374
    },
    {
      "name": "Initialization",
      "score": 0.7609471082687378
    },
    {
      "name": "Computer science",
      "score": 0.7371169328689575
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.6485443711280823
    },
    {
      "name": "Mode (computer interface)",
      "score": 0.6111147999763489
    },
    {
      "name": "Path (computing)",
      "score": 0.6041542291641235
    },
    {
      "name": "Adaptation (eye)",
      "score": 0.590700089931488
    },
    {
      "name": "Hyperparameter",
      "score": 0.5525722503662109
    },
    {
      "name": "Machine learning",
      "score": 0.44703546166419983
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4455071687698364
    },
    {
      "name": "Human–computer interaction",
      "score": 0.22165653109550476
    },
    {
      "name": "Mathematics",
      "score": 0.11117300391197205
    },
    {
      "name": "Computer network",
      "score": 0.0770007073879242
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}