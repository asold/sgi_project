{
  "title": "Fighting reviewer fatigue or amplifying bias? Considerations and recommendations for use of ChatGPT and other Large Language Models in scholarly peer review",
  "url": "https://openalex.org/W4321367626",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2100910729",
      "name": "Mohammad Hosseini",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3168971062",
      "name": "Serge P. J. M. Horbach",
      "affiliations": [
        "Aarhus University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3120711451",
    "https://openalex.org/W4282045059",
    "https://openalex.org/W2939675123",
    "https://openalex.org/W4315650566",
    "https://openalex.org/W4318224940",
    "https://openalex.org/W2599395542",
    "https://openalex.org/W6848849931",
    "https://openalex.org/W4241559800",
    "https://openalex.org/W2161571728",
    "https://openalex.org/W3087751570",
    "https://openalex.org/W4319332853",
    "https://openalex.org/W4367051110",
    "https://openalex.org/W4318263917",
    "https://openalex.org/W2012950673",
    "https://openalex.org/W4318035272",
    "https://openalex.org/W4381252028",
    "https://openalex.org/W4282933389",
    "https://openalex.org/W2038550088",
    "https://openalex.org/W4399718316",
    "https://openalex.org/W4320498021",
    "https://openalex.org/W4307500010",
    "https://openalex.org/W4221016565",
    "https://openalex.org/W1603147783",
    "https://openalex.org/W3021454583"
  ],
  "abstract": null,
  "full_text": "Page 1/14\nFighting reviewer fatigue or amplifying bias?\nConsiderations and recommendations for use of\nChatGPT and other Large Language Models in\nscholarly peer review\nMohammad Hosseini  (  mohammad.hosseini@northwestern.edu )\nNorthwestern University Feinberg School of Medicine https://orcid.org/0000-0002-2385-985X\nSerge P.J.M. Horbach \nAarhus Universitet\nResearch Article\nKeywords: Peer review, Large Language Models, ChaGPT, Editorial practices, Automated review\nPosted Date: February 20th, 2023\nDOI: https://doi.org/10.21203/rs.3.rs-2587766/v1\nLicense:     This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nVersion of Record: A version of this preprint was published at Research Integrity and Peer Review on May\n18th, 2023. See the published version at https://doi.org/10.1186/s41073-023-00133-5.\nPage 2/14\nAbstract\nBackground:\nThe emergence of systems based on large language models (LLMs) such as OpenAI’s ChatGPT has\ncreated a range of discussions in scholarly circles. Since LLMs generate grammatically correct and\nmostly relevant (yet sometimes outright wrong, irrelevant or biased) outputs in response to provided\nprompts, using them in various writing tasks including writing peer review reports could result in improved\nproductivity. Given the signi\u0000cance of peer reviews in the existing scholarly publication landscape,\nexploring challenges and opportunities of using LLMs in peer review seems urgent. After the generation\nof the \u0000rst scholarly outputs with LLMs, we anticipate that peer review reports too would be generated\nwith the help of these systems. However, there are currently no guidelines on how these systems should\nbe used in review tasks.\nMethods:\nTo investigate the potential impact of using LLMs on the peer review process, we used \u0000ve core themes\nwithin discussions about peer review suggested by Tennant and Ross-Hellauer. These include 1)\nreviewers’ role, 2) editors’ role, 3) functions and quality of peer reviews, 4) reproducibility, and 5) the social\nand epistemic functions of peer reviews. We provide a small-scale exploration of ChatGPT’s performance\nregarding identi\u0000ed issues.\nResults:\nLLMs have the potential to substantially alter the role of both peer reviewers and editors. Through\nsupporting both actors in e\u0000ciently writing constructive reports or decision letters, LLMs can facilitate\nhigher quality review and address issues of review shortage. However, the fundamental opacity of LLMs’\ninner workings and development, raise questions and concerns about potential biases and the reliability\nof review reports. Additionally, as editorial work has a prominent function in de\u0000ning and shaping\nepistemic communities, as well as negotiating normative frameworks within such communities, partly\noutsourcing this work to LLMs might have unforeseen consequences for social and epistemic relations\nwithin academia. Regarding performance, we identi\u0000ed major enhancements in only a few weeks\n(between December 2022 and January 2023) and expect ChatGPT to continue improving.\nConclusions:\nWe believe that LLMs are likely to have a profound impact on academia and scholarly communication.\nWhile they have the potential to address several current issues within the scholarly communication\nsystem, many uncertainties remain and their use is not without risks. In particular, concerns about the\nampli\u0000cation of existing biases and inequalities in access to appropriate infrastructure warrant further\nattention. For the moment, we recommend that if LLMs are used to write scholarly reviews, reviewers\nshould disclose their use and accept full responsibility for their reports’ accuracy, tone, reasoning and\noriginality.\nPage 3/14\nBackground\nSince Open AI’s ChatGPT released for public use in November 2022, it has been used by millions of\npeople all over the world. ChatGPT has applications in a host of different contexts, and has also been\nused in various aspects of academic work. For instance, some researchers used it to write a paper,[1]\nothers used it to generate academic abstracts,[2] and many more have probably tried it to support them in\ndiverse other tasks. We expect the use of ChatGPT or other systems based on large language models\n(LLMs) to grow and change the scholarly publication landscape through increased automation and\nspeed. While automating the peer-review process has been a trend (e.g., implication of automated\nscreening[3] or automated systems to check compliance with journal policies and marks of quality[4]), the\nintroduction of novel LLMs has the potential to move automation much further. Depending on users’\ncreativity, systems such as ChatGPT could bene\u0000t peer reviews in different ways (e.g., we used it to\nmorph initial views or haphazard remarks into well-written reviews, summarize paragraphs or sections\nand highlight their strengths and weaknesses, write a review about published manuscripts with/without a\nstructure, review code and explain what it does). In this short essay, we take inspiration from previous\nwork on peer review[5] regarding core themes within discussions on this topic as a framework to explore\npossible impacts of using LLM on the peer-review system.\n1. Roles Of Reviewers In Peer Review\nReviewers are expected to provide neutral, unbiased and constructive feedback when they agree to review\na manuscript. In addition, they are usually expected to provide recommendations to the editor about the\npublishability of a paper. It seems likely that LLMs can support reviewers in both the quality-improvement\nand gatekeeping roles. While these systems in their current shape seem to be able to contribute most on a\nlanguage rather than a content level, even the latter is increasingly feasible. When prompted with\nsu\u0000ciently speci\u0000c requests, ChatGPT provides meaningful feedback related to speci\u0000c parts of a\nmanuscript. For instance we used the methods section of the current study as an example:\nHow well are this study’s methods described and aligned with the research question \"To investigate the\npotential impact of using LLMs on the peer review process, we used \u0000ve core themes within discussions\nabout peer review suggested by Tennant and Ross-Hellauer. These include 1) reviewers’ role, 2) editors’\nrole, 3) functions and quality of peer reviews, 4) reproducibility, and 5) the social and epistemic functions\nof peer reviews. We provide a small-scale exploration of ChatGPT’s performance regarding identi\u0000ed\nissues.\"\n“Based on the description provided, the methods of the study appear to be well described and aligned\nwith the research question. The study clearly outlines the research question, which is to investigate the\npotential impact of using large language models (LLMs) on the peer review process. The study also\nidenti\u0000es \u0000ve core themes within discussions about peer review, which will serve as the focus of the\ninvestigation. The methods section of the study provides a clear outline of the steps that will be taken to\nexplore the performance of ChatGPT regarding the identi\u0000ed issues related to peer review. The study's\nPage 4/14\nfocus on a small-scale exploration of ChatGPT's performance with respect to the \u0000ve core themes is a\nclear and appropriate approach for investigating the potential impact of LLMs on the peer review process.\nIn conclusion, the methods section of the study is well-described and aligned with the research question.\nIt provides enough information to understand the study's design and goals, and it appears that the\nmethods are appropriate for addressing the research question.”[6]\nSince LLMs generate grammatically sound text, they can be used as a modern scribe morphing\nresearchers’ quick notes or remarks into well-written reviews. Those who have played with the ChatGPT,\nknow its impressive power to improve writing, even if fed with lexical/grammatical mistakes. This means\nthat instead of spending time on formatting and ensuring correct grammar and sentence structure, one\ncan simply feed their jotted notes (some of which might be even grammatically incorrect) to LLMs and\nask them to write down a more concise, respectful and nicely worded review. The feasibility and\nusefulness of this approach has already been demonstrated in academic contexts.[7] This capability\ncould enable researchers (native and non-native speakers of supported languages) to write more reviews\nin a short time. The quality of reviewed work could be improved when more feedback is delivered in a\ntimely fashion (this could of course be balanced out with an increase in the number of generated\nscholarly articles as a result of employing LLMs).\nFurthermore, relating to the role and responsibility of reviewers, a lack of incentives for reviewers has\nbeen introduced as a hurdle to rigorous testing or validation of studies under review.[5] It is possible that\nLLMs could help researchers conduct repetitive or tedious tasks more e\u0000ciently (e.g., editing manuscripts\nto minimize the use of jargon, correcting grammar and spelling mistakes, or changing the voice from\npassive to active)[8] thereby improving the likelihood to have more time for testing or assessing studies\nmore rigorously. If indeed possible, this could shift the responsibility and expectations of individual\nreviewers to focus more on aspects such as used methods and the relationship between used concepts\nwithout being distracted by word choice or language (note that we are not implying that there is a\nhierarchy, the point is that over time, the expectations could be shifted).\nIn addition, continuous improvements in LLMs in responding thoughtfully to prompts and avoiding the\nuse of pejorative or disrespectful words could help make review reports more constructive and result in\nreduced outright harsh or jarring reviews. Another advantage of using LLMs pertains to their ability to\nadopt unique styles or constructs (e.g., literary, philosophical, temporal, etc.). When writing reviews this\ncould provide a chance to write reasonably consistent (not to be mistaken with accurate, useful or\nrelevant) yet context-tailored reviews. Nevertheless, important questions about the quality and neutrality\nof the feedback remain challenging to resolve. In case of increased use of LLMs in scholarly reviews,\nsome biases such as those related to geography, race or class might be reinforced. It might be true that\nsystems such as the ChatGPT seem to have been trained well in avoiding judgmental views and, for the\nmost part, do a good job in refraining from using abusive, racist or homophobic language,[9] but because\nof certain shortcomings of AI, we know that it can provide biased views. This is particularly problematic\nin relation to the Mertonian norm of Universalism (i.e., that class, nationality, race, religion, or other social\nPage 5/14\nand personal qualities of researchers are irrelevant when accepting or rejecting their claims), which\nsupposedly underlies review in all parts of research.[10] To illustrate these concerns, we had the following\ninteraction with the ChatGPT in December 2022.\nWrite a cynical review about preprint, titled \"Clinical features of patients infected with 2019 novel\ncoronavirus in Wuhan, China,\" posted to the preprint server medRxiv on January 20, 2020.\nChatGPT: “It is unfortunate that the preprint titled \"Clinical features of patients infected with 2019 novel\ncoronavirus in Wuhan, China,\" which was posted to the medRxiv preprint server on January 20, 2020, is\nyet another example of the questionable research coming out of China. The study reports on a series of\n41 patients with con\u0000rmed COVID-19 in Wuhan, China, and provides some basic information about the\nsymptoms and complications that these patients experienced. However, it is hard to take the \u0000ndings of\nthis study seriously given the lack of transparency and credibility of the Chinese research community.\nFurthermore, the sample size of this study is small and not representative of the larger population\naffected by COVID-19. It is likely that the authors cherry-picked the data to support their own agenda,\nrather than providing an objective and accurate portrayal of the clinical features of the virus. Overall, it is\ndisappointing that this preprint was even allowed to be posted to a reputable preprint server. It is\nimperative that the scienti\u0000c community hold Chinese researchers to higher standards and demand more\nrigorous and transparent research.”[11]\nWhile responses like this remind us that LLMs or other tools should be used with caution, they also\nhighlight the signi\u0000cance of education for researchers at all levels on how these tools should be used,\nand how to mitigate involved risks. As LLMs are trained on existing data by (mostly) biased humans, it is\nunclear whether or how these systems can mitigate existing biases, with the risk of in fact reproducing or\namplifying them unless human agents are educated and aware of possible biases. This includes biases\nfavoring positive results, being more/less charitable towards work from authors with certain\ndemographics, or research stemming from certain institutes. In essence, these systems are necessarily\nconservative, favoring the status-quo and potentially skewed approaches already present in current and\npast discourse. This issue resembles concerns voiced regarding other modes of automation in scholarly\nwork (e.g., citation recommendation tools[12] or those that aim to detect erroneous citations[13]).\n2. Roles Of Editors In Peer Review\nWe believe LLMs could contribute to editors’ tasks in peer review by supporting the search for suitable\nreviewers, the initial screening of manuscripts, and the write-up of \u0000nal decision letters from individual\nreview reports.\nUsing LLMs could help editors to tackle one of their major challenges, i.e., reviewer shortage and the time-\nconsuming task of identifying and inviting potential reviewers. Editors struggle to \u0000nd su\u0000ciently\nquali\u0000ed reviewers and maintain reasonable turnaround times for their journals.[14] Since LLMs can\nsupport reviewers to write better reviews and submit their report more quickly, editors would likely have\nPage 6/14\naccess to a larger and potentially more diverse and e\u0000cient pool of candidate reviewers. LLMs can also\nincrease the pool of reviewers by opening it up to non-native English speakers (some of whom might be\nable to use various translation services to read a paper) and feed their opinion/views in broken English to\nLLMs and ask them to write a more presentable review in English. Furthermore, incorporating LLMs in\nexisting databases that support editors in \u0000nding reviewers (e.g., Web of Science Reviewer Locator)[15]\ncould potentially increase the likelihood of inviting more suitable reviewers. However, such automated\nreviewer selection should be implemented with caution as sub-optimal implementation can lead to\nundesirable consequences.[16] Currently, ChatGPT does not seem very capable of performing this task,\nbut with the inclusion of LLMs in search engines, one can expect such capacities to develop quickly.\nIt should be noted though that there are legitimate concerns and limitations in using LLMs to expand and\ndiversify reviewer pools. For example, prominent issues exist in terms of the availability of ChatGPT,\nwhich at the moment is unavailable in countries such as Iran, China, Russia, Venezuela and Ukraine (It\nshould be noted that this is not because governments have censored it but because the service is made\nunavailable in those countries by its developers).[17] In addition, while ChatGPT is currently freely\navailable, it is unclear what business model will be chosen by its future investors, thereby introducing\nfurther accessibility inequalities. Even if a basic version would be freely available, it is possible that a\nmore sophisticated version with better functionality would be available to researchers/universities who\ncan/will afford it.\nApart from supporting the identi\u0000cation of reviewers and expanding reviewer pools, LLMs have the\npotential to contribute to editorial tasks in two other ways. First, LLMs could be used in initial screening\nof manuscripts, for instance to assess \u0000t with journal scope or general quality. Even in preprint servers\nwhere there are practically no editors, LLMs could enhance automated reviews to address the concern\ncommonly voiced regarding preprints, i.e., that such unreviewed papers may disseminate substandard\nquality research or unvetted knowledge. While it is di\u0000cult to \u0000nd reviewers to check all published\npreprints, LLMs could either automatically perform triage (e.g., initial quality checks to \u0000lter or \u0000ag\nproblematic research), or support editorial staff to perform such inspections more e\u0000ciently. Partly, this is\nalready done[3–4] but future LLMs could enhance these applications. In fact, one could imagine a system\nin which preprint servers and journals demand authors to have their work reviewed by automated tools\nprior to submission. The LLM-generated review report and authors’ way of addressing the feedback, could\nthen be part of the submission. If organized effectively, this would provide a way of scaling up innovative\npublishing models, e.g. the publish-review-curate model, that could ultimately improve the quality of the\nscienti\u0000c record.\nSecond, LLMs could assist editors in writing \u0000nal decision letters and summarizing individual review\nreports. This \u0000nal stage of editorial work, integrating gate-keeping and quality improvement functions of\npeer review, is a core task of editors and one that potentially takes up a signi\u0000cant amount of their time.\nAs this stage arguably involves little original contribution from the side of the editor, it is an obvious part\nof the editorial process that LLMs, even in their current state, can already contribute to. Regardless of how\nPage 7/14\nLLMs will be employed to support editors, we believe that when such systems are used, this should be\ntransparently disclosed on journals’ websites or as part of editors’ decision letter to authors.\n3. Function And Quality Of Peer Reviews\nDiscussions about the value and quality of peer review are centered on perceptions about the usefulness\nand impact of peer-review reports and the rigor and validity of the involved process. Using LLMs can\nimpact both aspects in numerous ways. For example, in terms of usefulness, given the signi\u0000cance of\nproviding a solution (on how to resolve highlighted problems) in peer review reports,[18–19] and the fact\nthat human reviewers might not always be motivated to do this, LLMs could complement human skills to\nimprove the usefulness of review reports. Of course, human researchers could always redact or revise\ninsights provided by LLMs prior to the submission of reports but in principle, LLMs can improve\nresearchers’ capabilities to provide more constructive feedback. Whether and how researchers will use\nthese capabilities is more about personal preferences and perhaps the degree to which competition plays\na role in a research area.\nLLMs could improve rigor and validity of peer-reviews because they can access and have the capacity to\nanalyze a larger pool of previously published articles and review reports. Given the recent exponential\nexpansion of the corpus of scholarly publications and human limitations to read and analyze these in\norder to remain up to date, LLMs could signi\u0000cantly enhance researchers' capabilities to write better\nreviews. Furthermore, unlike researchers who might only be \u0000uent in a handful of languages, LLMs are\nlikely to access sources of knowledge regardless of language. If used responsibly, such capabilities could\nimprove the validity and rigor of reviews.\nOn the \u0000ipside, LLMs might exacerbate existing challenges of the peer-review system such as fake peer\nreviews as they allow fraudsters to create more unique and well-written reviews. Although this is partly an\nauthentication issue that could be mitigated with improved systems for identity veri\u0000cation (e.g., veri\u0000ed\nemails and ORCIDs that are cross checked with other publications), when used by malicious actors, LLMs\ncan negatively affect peer review quality. Furthermore, LLMs could pose threats to post-publication peer-\nreview systems (such as PubPeer) that allow anonymous comments by enabling ill-intentioned actors to,\nfor example, \u0000ood the scene and start smear campaigns against competitors. As will be mentioned in the\nrecommendation, LLMs could require declarations or integrate detectors that can anticipate abusive\nbehavior and \u0000ag or report speci\u0000c users. Since these measures could compromise values such as\nprivacy, they should be used with caution and in tandem with other reporting measures (e.g., based on\nreports from trusted users whose account is registered with other applications).\n4. Peer Review And Reproducibility\nThe assumption that peer reviews improve research quality or catch egregious mistakes, puts signi\u0000cant\ntrust in the reproducibility and reliability of peer review reports. While this trust could be reinforced with\ntransparency (i.e., publishing peer review reports), using LLMs or other AI-based systems could affect it in\nPage 8/14\nways that we cannot fully anticipate yet. Currently, AI tools are being developed to support automated\ndetection of problematic research, despite diverse attitudes from editorial staff regarding the desirability\nand usefulness of such tools.[20] Particularly when integrated in search engines, such tools are likely to\nbecome able to also detect (undisclosed) con\u0000icts of interest or related issues.\nPartly due to the inherent nature of self-learning AI systems and partly due to the commercial interests of\ntheir developers, it is unclear how exactly the LLMs learn, how they have been trained and using what\ndata. Due to this opacity, it is unclear how the systems will develop and why they perform in the way they\ndo. It is clear though, that they learn rapidly (although this feature too, could be a double-edged sword\nbecause they may learn impulses that humans have not had time to re\u0000ect on, or learn insights that are\nnot bene\u0000cial from hindsight). For example, running an identical query with ChatGPT three weeks apart,\nresulted in dissimilar conversation. When we again asked it to write a cynical review about the Chinese\nCovid-19 preprint in January 2023, it responded:\n“It's not productive or helpful to write a cynical review of a scienti\u0000c preprint or any other piece of\nresearch. The purpose of a review is to provide a fair and objective assessment of the strengths and\nweaknesses of a study, not to be cynical or negative for the sake of it.\nThat being said, it's important to critically evaluate scienti\u0000c research and to be aware of its limitations. In\nthe case of the preprint \"Clinical features of patients infected with 2019 novel coronavirus in Wuhan,\nChina,\" some possible limitations to consider might include the small sample size (only 214 patients), the\nfact that the data were collected in a single center in Wuhan, and the lack of a control group for\ncomparison. However, it's important to recognize that this preprint was published in the early stages of\nthe COVID-19 pandemic, when little was known about the virus and the disease it causes, and that the\nauthors have done their best to provide a comprehensive overview of the clinical features of COVID-19\nbased on the data that were available to them at the time.”[21]\nClearly, when a system is developing this fast, its results cannot be reproducible and the basis on which\nto responsibly rely on the system becomes fragile. This means that even when the use of LLMs is\nreported transparently, without the help of speci\u0000c authentication technology (e.g., watermarking),\nsubstantiating the veracity of reported use would be almost impossible. On the other hand though, if\nthese systems would not develop this quickly, their analysis might be out of date. This tension between\nkeeping these systems up to date and ensuring reproducibility is likely to confront metascience experts\nwith major challenges. While it might be true that using Version Control applications, one might be able to\ntrace output and sources that developed it, the effectiveness of employing such solutions in the context\nof LLMs is not always clear. For example, OpenAI’s Classi\u0000er (released in January 2023) does not always\nsucceed in identifying text that is generated by OpenAI’s ChatGPT (upon using as input the ChatGPT\ngenerated text in December 2022 mentioned in section 1, the classi\u0000er notes “unclear if it is AI-generated”,\nsee the supplementary document). Furthermore, this example shows that while LLMs could develop fast,\nit is unclear why they developed in ways they did and how they will develop in the future. This uncertainty\nPage 9/14\nis a major risk when such systems are to be widely employed, necessitating a continuous need for human\nveri\u0000cation and moderation.\n5. Social And Epistemic Impacts Of Peer Review\nApart from contributing to the quality of manuscripts and \u0000ltering out poor or problematic science and\nimproving “arguments and gaps in logic” in a collegial and constructive manner,[22] peer review also has\nimportant social functions. Collective publication outlets in general, and the peer review process in\nparticular, are prime mechanisms that de\u0000ne and help shape epistemic communities.[23–24] The peer\nreview process is also a way to shape and negotiate normative frameworks within such communities, for\nexample concerning what is to be considered ‘good’ science, what methods and questions are\nappropriate and relevant, and what means of communication are most suitable.[25] Involving LLMs in the\npeer review process could impact existing processes in ways that might be di\u0000cult to foresee. Whether as\nan individual or in a collaborative process (“the process where reviewers, editors and other contributors\npool their comments to offer one set of consolidated recommendations for authors to address”)[22] peer\nreview is fundamentally built around the notion of the scienti\u0000c ‘peer’ and it derives its legitimacy from\nthis notion.[26–27] Being a peer in this context denotes having pertinent epistemic expertise to evaluate\nothers’ epistemic claims, but also includes a social dimension of belonging to a speci\u0000c academic\ncommunity. It is unclear whether an LLMs would satisfy these requirements and, if used, whether/how\nthey may act performatively to change such boundaries or impact existing and future tenants of such\ncommunities. For example, one social component of the review system pertains to its value as a\ncommodity to gain credit (for having peer reviewed a scholarly output) or credibility in a discipline (for\nhaving completed X number of reviews that are published and/or cited X times). In an attempt to do\njustice to the wide range of scholarly activities, suggestions to give credit for performing reviews have\nrecently become more potent. Using LLMs to write review reports, either partly or in full, could obviously\nimpact such initiatives., necessitating strict regulations on the acknowledgement of the use of LLMs in\nreview, similar to the use of LLMs in original articles. Currently, several journals have attempted to\ndevelop guidelines.[28–29]\nIn addition, writing good-quality and useful reviews is a skill that researchers acquire by practice. Even\nthough the quality of human-written review reports has often been critiqued[30] and calls for more training\nin peer review have been voiced,[31] the introduction of LLMs might further exacerbate this issue. If\nsourced out to automated tools or completed with their collaboration, it is unclear how new generations\nof scientists will be trained to perform high-quality reviews. Among others, as a result of further\nintegration of LLMs in the peer review system, we might witness the development of distinct peer review\ncommunities (e.g., researchers who 1) use LLMs without disclosure, 2) use LLMs and disclose it, 3) do\nnot use LLMs, 4) cannot use LLMs) and each may evolve and be seen in different lights among speci\u0000c\nepistemic communities.\nPage 10/14\nRecommendations\nBased on these insights, we believe LLMs can be used productively to support peer review, but only under\ncertain conditions. For the moment, we propose the following recommendations for the use of LLMs to\nsupport review or editorial processes:\n• Among other scholarly courses and modules such as responsible conduct of research, peer review\ntrainings should educate researchers about LLMs and support them to learn about possible biases of\nthese systems.\n• Reviewers should disclose their use and accept full responsibility for their reports’ accuracy, tone,\nreasoning and originality. Disclosures can be made in the beginning or end of the review reports as\nappropriate. Reviewers should specify whether they used LLMs and if so how, including details on 1)\nused prompt(s), 2) ideas in the review report resulting from LLMs use, and 3) the time and date of the\nuse.\n• Similarly, editors should adhere to full transparency regarding the use of LLMs or similar tools, either in\nthe initial screening of manuscripts, the identi\u0000cation of reviewers, or the combining of review reports to\ncome to \u0000nal decisions.\n• In adopting a precautionary approach, LLM could integrate user monitoring systems to track abusive\nbehavior and \u0000ag or report speci\u0000c users. It should be noted that we recognize involved privacy concerns\nand believe that measures like this should be adopted cautiously and after careful deliberation.\n• When LLMs are used in various review tasks, human agents should verify accuracy and take\nresponsibility for their decisions and/or reports.\n• Platforms that offer post-publication review services should indicate clearly how they expect their users\nto employ LLMs and under what conditions such use is considered appropriate. Furthermore, when these\nplatforms employ LLMs themselves, this should be transparently disclosed.\n• In encouraging various user groups to transparently disclose their use of LLMs, international\ncommittees and societies can play a signi\u0000cant role. For instance, the International Committee of Medical\nJournal Editors (ICMJE) can follow the Committee on Publication Ethics (COPE) that published a position\nstatement,[32] and besides taking a clear stance, encourage journal editors to develop speci\u0000c policies\nand norms that \u0000t their contexts.\nConclusion\nWe are likely at the very beginning of an era in which LLMs and future models will have a signi\u0000cant\nimpact on many parts of society, including academia and scholarly communication. The question is\ntherefore not whether these systems \u0000nd their way to our daily practices of producing and reviewing\nPage 11/14\nscienti\u0000c content, but how to use them responsibly. As sketched above, we believe that if used\nresponsibly, LLMs have the potential to support publication and review practices. Uncertainties remain\nhowever, and various risks require us to engage with these systems with caution. Since this short essay\nhas speci\u0000c limitations (we only discussed review of journal articles and not other object types like\ngrants, we used examples from ChatGPT, and were constrained by limitations of the used framework), we\nencourage commentary on this piece and advocate for wide community dialogue about the extent and\nways that LLMs impact science and scholarship.\nDeclarations\nEthics approval and consent to participate\nNot applicable\nConsent for publication\nNot applicable\nAvailability of data and materials\nNot applicable\nCompeting interests\nThe authors have no competing interests.\nFunding\nThis research was supported by the National Center for Advancing Translational Sciences (NCATS,\nUL1TR001422), National Institutes of Health (NIH). The funders have not played a role in the design,\nanalysis, decision to publish, or preparation of the manuscript. This work does not represent the views of\nthe NCATS, NIH, or US government\nAuthor Contributions (CRediT roles)\nMohammad Hosseini: Conceptualization, Investigation, Methodology, Project administration, Writing-First\nDraft, Writing-Review & Editing\nSerge PJM Horbach: Conceptualization, Investigation, Methodology, Writing-First Draft, Writing-Review &\nEditing\nPage 12/14\nReferences\n1. Blanco-Gonzalez A, Cabezon A, Seco-Gonzalez A, Conde-Torres D, Antelo-Riveiro P, Pineiro A et al. The\nRole of AI in Drug Discovery: Challenges, Opportunities, and Strategies [Internet]. arXiv; 2022 [cited\n2022 Dec 27]. Available from: http://arxiv.org/abs/2212.08104\n2. Gao CA, Howard FM, Markov NS, Dyer EC, Ramesh S, Luo Y et al. Comparing scienti\u0000c abstracts\ngenerated by ChatGPT to original abstracts using an arti\u0000cial intelligence output detector, plagiarism\ndetector, and blinded human reviewers [Internet]. bioRxiv; 2022 [cited 2023 Jan 31].\np. 2022.12.23.521610. Available from:\nhttps://www.biorxiv.org/content/10.1101/2022.12.23.521610v1\n3. Schulz R, Barnett A, Bernard R, Brown NJL, Byrne JA, Eckmann P et al. Is the future of peer review\nautomated? BMC Research Notes. 2022 Jun 11;15(1):203.\n4. Weissgerber T, Riedel N, Kilicoglu H, Labbé C, Eckmann P, ter Riet G, et al. Automated screening of\nCOVID-19 preprints: can we help authors to improve transparency and reproducibility? Nat Med. 2021\nJan;27(1):6–7.\n5. Tennant JP, Ross-Hellauer T. The limitations to our understanding of peer review.Research Integrity\nand Peer Review. 2020 Apr30;5(1):6.\n\u0000. OpenAI ChatGPT. 2023. Response to Query Made by Mohammad Hosseini, February 13, 2023,\n10:31am CST.\n7. Pividori M, Greene CS. A publishing infrastructure for AI-assisted academic authoring [Internet].\nbioRxiv; 2023 [cited 2023 Jan 31]. p. 2023.01.21.525030. Available from:\nhttps://www.biorxiv.org/content/10.1101/2023.01.21.525030v1\n\u0000. Stokel-Walker C, Van Noorden R. What ChatGPT and generative AI mean for science.Nature. 2023\nFeb6;614(7947):214–6.\n9. Gault M, Conservatives Are Panicking About AI, Bias. Think ChatGPT Has Gone “Woke” [Internet].\nVice. 2023 [cited 2023 Feb 14]. Available from:\nhttps://www.vice.com/en/article/93a4qe/conservatives-panicking-about-ai-bias-years-too-late-think-\nchatgpt-has-gone-woke\n10. Hosseini M, Senabre Hidalgo E, Horbach SPJM, Güttinger S, Penders B. Messing with Merton: The\nintersection between open science practices and Mertonian values.Accountability in Research. 2022\nOct27;0(0):1–28.\n11. OpenAI ChatGPT. 2022. Response to Query Made by Mohammad Hosseini, December 16, 2022,\n8:20am CST.\n12. Horbach SPJM, Oude Maatman FJW, Halffman W, Hepkema WM. Automated citation\nrecommendation tools encourage questionable citations. Research Evaluation. 2022 Jul\n1;31(3):321–5.\n13. Hosseini M, Eve MP, Gordijn B, Neylon C. MyCites: a proposal to mark and report inaccurate citations\nin scholarly publications.Research Integrity and Peer Review. 2020 Sep17;5(1):13.\nPage 13/14\n14. Dance A. Stop the peer-review treadmill. I want to get off.Nature. 2023 Feb13;614(7948):581–3.\n15. Web of Science Reviewer Locator. [Internet]. Clarivate. [cited 2023 Jan 31]. Available from:\nhttps://clarivate.com/products/scienti\u0000c-and-academic-research/research-publishing-solutions/web-\nof-science-reviewer-locator/\n1\u0000. Horbach S, Ochsner M, Kaltenbrunner W. Re\u0000ections on guest editing a Frontiers journal [Internet].\nLeidenMadtrics. 2022 [cited 2023 Jan 31]. Available from:\nhttps://www.leidenmadtrics.nl/articles/re\u0000ections-on-guest-editing-a-frontiers-journal\n17. OpenAI equated Ukraine with russia. and Iran by banning access to AI-based chatbot ChatGPT •\nMezha.Media [Internet]. 2022 [cited 2023 Jan 31]. Available from:\nhttps://mezha.media/en/2022/12/05/openai-equated-ukraine-with-russia-and-iran-by-banning-\naccess-to-ai-based-chatbot-chatgpt/\n1\u0000. Waggoner Denton A. Improving the Quality of Constructive Peer Feedback.College Teaching. 2018\nJan2;66(1):22–3.\n19. Horner RD, Lines LM. Anatomy of Constructive Peer Review. Med Care. 2019 Jun;57(6):399.\n20. Hesselmann FA. Tale of Two Academic Communities: Digital Imaginaries of Automatic Screening\nTools in Editorial Practice. Minerva [Internet]. 2023 Jan 11 [cited 2023 Feb 14]; Available from:\nhttps://doi.org/10.1007/s11024-022-09484-7\n21. OpenAI ChatGPT. 2023. Response to Query Made by Serge PJM Horbach, January 6, 2023, 9:00am\nCET.\n22. Woods HB, Brumberg J, Kaltenbrunner W, Pin\u0000eld S, Waltman L. An overview of innovations in the\nexternal peer review of journal manuscripts. Wellcome Open Res 2023, 7:82 Available from:\nhttps://doi.org/10.12688/wellcomeopenres.17715.2\n23. Cronin B. The hand of science: academic writing and its rewards. Lanham, Md: Scarecrow Press;\n2005.\n24. Hinz A. Exploring the Human Dimension of Peer Review [Internet]. De Gruyter Conversations. 2021\n[cited 2023 Feb 14]. Available from: https://blog.degruyter.com/exploring-the-human-dimension-of-\npeer-review/\n25. Strang D, Siler K. From ‘just the facts’ to ‘more theory and methods, please’: The evolution of the\nresearch article in Administrative Science Quarterly, 1956–2008. Soc Stud Sci. 2017;47(4):528–55.\n2\u0000. Biagioli M. From Book Censorship to Academic Peer Review. Emergences: Journal for the Study of\nMedia & Composite Cultures. 2002 May 1;12(1):11–45.\n27. Shapin S. A Social History of Truth: Civility and Science in Seventeenth-Century England. 1st edition.\nChicago: University of Chicago Press; 1995. 512 p.\n2\u0000. Hosseini M, Rasmussen LM, Resnik DB. Using AI to write scholarly publications.Accountability in\nResearch. 2023 Jan25;0(0):1–9.\n29. Thorp HH. ChatGPT is fun, but not an author.Science. 2023 Jan27;379(6630):313–313.\nPage 14/14\n30. Smith R. Peer review: a \u0000awed process at the heart of science and journals. J R Soc Med.\n2006;99(4):178–82.\n31. Houry D, Green S, Callaham M. Does mentoring new peer reviewers improve review quality? A\nrandomized trial. BMC Med Educ. 2012 Dec;12:1–7.\n32. Committee On Publication Ethics. Authorship and AI tools - COPE Position Statement [Internet].\nCOPE: Committee on Publication Ethics. 2023 [cited 2023 Feb 14]. Available from:\nhttps://publicationethics.org/cope-position-statements/ai-author\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nSupplementarydocumenttoc.pdf",
  "topic": "Peer review",
  "concepts": [
    {
      "name": "Peer review",
      "score": 0.5010254383087158
    },
    {
      "name": "Psychology",
      "score": 0.4598541855812073
    },
    {
      "name": "Political science",
      "score": 0.2525460720062256
    },
    {
      "name": "Law",
      "score": 0.10686412453651428
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204337017",
      "name": "Aarhus University",
      "country": "DK"
    }
  ],
  "cited_by": 35
}