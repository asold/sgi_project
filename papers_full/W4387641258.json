{
  "title": "Evaluation of Stock Closing Prices using Transformer Learning",
  "url": "https://openalex.org/W4387641258",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2028475082",
      "name": "Tariq Saeed Mian",
      "affiliations": [
        "Taibah University"
      ]
    },
    {
      "id": "https://openalex.org/A2028475082",
      "name": "Tariq Saeed Mian",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3080733778",
    "https://openalex.org/W2911494597",
    "https://openalex.org/W2923002376",
    "https://openalex.org/W2050654176",
    "https://openalex.org/W2169533279",
    "https://openalex.org/W3036929302",
    "https://openalex.org/W3159925151",
    "https://openalex.org/W3007854580",
    "https://openalex.org/W4200482878",
    "https://openalex.org/W1606266079",
    "https://openalex.org/W1847550128",
    "https://openalex.org/W2014341469",
    "https://openalex.org/W2087186620",
    "https://openalex.org/W4213418104",
    "https://openalex.org/W2785597854",
    "https://openalex.org/W2979673329",
    "https://openalex.org/W3017051726",
    "https://openalex.org/W2977178908",
    "https://openalex.org/W2995154733",
    "https://openalex.org/W2183365078",
    "https://openalex.org/W1969852690",
    "https://openalex.org/W2899037356",
    "https://openalex.org/W3018758730",
    "https://openalex.org/W4362587296",
    "https://openalex.org/W3099331529",
    "https://openalex.org/W2919115771",
    "https://openalex.org/W2980994438",
    "https://openalex.org/W2172198266",
    "https://openalex.org/W2560370080",
    "https://openalex.org/W1485009520",
    "https://openalex.org/W2742473260",
    "https://openalex.org/W2812669263",
    "https://openalex.org/W2966792341",
    "https://openalex.org/W6697136110",
    "https://openalex.org/W2734986640",
    "https://openalex.org/W3161071448",
    "https://openalex.org/W2900880305",
    "https://openalex.org/W2784246028",
    "https://openalex.org/W6797674293",
    "https://openalex.org/W4223522968",
    "https://openalex.org/W4386253606",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W1553469512",
    "https://openalex.org/W2076063813"
  ],
  "abstract": "Predicting stock markets remains a critical and challenging task due to many factors, such as the enormous volume of generated price data, instant price data changes, and sensitivity to human sentiments, wars, and natural disasters. Since the previous three years of the COVID-19 pandemic, forecasting stock markets is more difficult, complex, and problematic for stock market analysts. However, technical analysts of the stock market and academic researchers are continuously trying to develop innovative and modern methods for forecasting stock market prices, using statistical techniques, machine learning, and deep learning-based algorithms. This study investigated a Transformer sequential-based approach to forecast the closing price for the next day. Ten sliding window timesteps were used to forecast next-day stock closing prices. This study aimed to investigate reliable techniques based on stock input features. The proposed Transformer-based method was compared with ARIMA, Long-Short Term Memory (LSTM), and Random Forest (RF) algorithms, showing its outstanding results on Yahoo Finance data, Facebook Intra data, and JPMorgan's Intra data. Each model was evaluated using Mean Absolute Error (MSE), Root Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE).",
  "full_text": "Engineering, Technology & Applied Science Research  Vol. 13, No. 5, 2023, 11635-11642  11635   \n \nwww.etasr.com Mian: Evaluation of Stock Closing Pri ces using Transformer Learning \n \nEvaluation of Stock Closing Prices using \nTransformer Learning \n \nTariq Saeed Mian  \nDepartment of IS, College of Computer Science & Engineering, Taibah University, Saudi Arabia \ntmian@taibahu.edu.sa (corresponding author) \nReceived: 7 May 2023 | Revised: 2 June 2023, 12 July 2023, and 18 July 2023 | Accepted: 19 July 2023 \nLicensed under a CC-BY 4.0 license | Copyright (c) by the authors | DOI: https://doi.org/10.48084/etasr.6017 \nABSTRACT \nPredicting stock markets remains a critical and cha llenging task due to many factors, such as the \nenormous volume of generated price data, instant pr ice data changes, and sensitivity to human sentimen ts, \nwars, and natural disasters. Since the previous thr ee years of the COVID-19 pandemic, forecasting stoc k \nmarkets is more difficult, complex, and problematic for stock market analysts. However, technical analysts \nof the stock market and academic researchers are co ntinuously trying to develop innovative and modern \nmethods for forecasting stock market prices, using statistical techniques, machine learning, and deep \nlearning-based algorithms. This study investigated a Transformer sequential-based approach to forecast  \nthe closing price for the next day. Ten sliding win dow timesteps were used to forecast next-day stock \nclosing prices. This study aimed to investigate rel iable techniques based on stock input features. The  \nproposed Transformer-based method was compared with  ARIMA, Long-Short Term Memory (LSTM), \nand Random Forest (RF) algorithms, showing its outs tanding results on Yahoo Finance data, Facebook \nIntra data, and JPMorgan's Intra data. Each model w as evaluated using Mean Absolute Error (MSE), \nRoot Mean Square Error (RMSE), and Mean Absolute Percentage Error (MAPE). \nKeywords-stock prediction; ARIMA; SARIMA; LSTM; tra nsformer; stock volatility; stock market; stock \nmarket prediction; machine learning; deep learning \nI.  INTRODUCTION  \nThe stock market has great potential for profitmaki ng [1]. \nDifferent techniques and studies have been presented to predict \nfuture prices, maximize the returns of investments,  and \nminimize the corresponding risks [2]. However, ther e are still \nrisks due to unpredictable fluctuations in stock ma rket prices, \ncomplicated trading circumstances in practice [3], and the \ndifficulty in interpreting price movement with doma in \nknowledge. Various statistical techniques with good  \ninterpretation, such as auto-regressive [4], tree [ 5], and hidden \nMarkov [6] models, have been introduced for this pu rpose. \nPowerful prediction models [7] are expensive to tra in but \nprovide good results. The importance of deep learni ng models \ncannot be ignored in powerful prediction models for  sequential \ndata. Deep learning models are receiving more research interest \ndue to the increasing number of available datasets,  growing \ncomputational power, and long-term dependency capability [8-\n9]. Recurrent Neural Network (RNN)-based deep learn ing \nmodels such as RNN, LSTM, and Transformer have show n \nstate-of-the-art results in time-series tasks. Analyzing historical \nstock market patterns can provide valuable insight in predicting \nstock values, optimizing gain, and minimizing losse s [10-11]. \nIntercorrelation among different stocks is a crucia l factor in \nestimating future stock prices. All tradable stocks  in today's \nmarkets are interconnected through various attribut es. \nAlthough correlation does not imply causation, stro ng \nPearson's correlation scores suggest a potential re lationship \nbetween correlated stocks. Therefore, investigating  stock price \ntrends and forecasting future changes remains a pop ular \nresearch field. \nThis study aims to develop an advanced model for sh ort-\nterm price forecasting, considering factors such as  the global \neconomy, international politics, financial performa nce, \nstakeholder expectations, and financial reports. Tr aditional \ninvestors face difficulties in predicting market be havior and \nselecting effective forecasting techniques to maxim ize profits \nand minimize losses. Predicting future stock prices  is a \nchallenging task for investors, companies, and fina ncial \norganizations [12]. Variables such as the national political \nenvironment, economic situation, and investor psych ology are \ncrucial to accurate predictions. Analysts in variou s fields face \nnumerous challenges in the forecasting of financial  markets \n[13]. Stock market predictions are heavily based on  factors \nsuch as intrinsic value, financial performance, reg ulatory \nrequirements, GDP, natural disasters, and other variables [14]. \nMachine learning algorithms have transformed stock \nmarket forecasting by leveraging large and nonlinea r datasets. \nThese methods have shown significant improvements o ver \ntraditional approaches, ranging from 60% to 86% [15 ]. Long-\nterm investment in established stocks is considered  more \nprofitable and requires less effort and time compar ed to other \ninvestments like bonds. However, short-term trading  aims to \ncapitalize on small fluctuations in stock values an d requires \nsubstantial time and effort from traders, particularly when done \nmanually. In this case, staying up-to-date with the  latest stock \nEngineering, Technology & Applied Science Research  Vol. 13, No. 5, 2023, 11635-11642  11636   \n \nwww.etasr.com Mian: Evaluation of Stock Closing Pri ces using Transformer Learning \n \nnews is essential. Time-series stock market predict ion models \nusing statistical and machine learning methods help traders and \ninvestors make informed decisions based on historical data and \nmarket trends. However, these predictions are not a lways \naccurate, and investing in the stock market carries  inherent \nrisks. Traders and investors must employ well-infor med \nstrategies, risk management plans, and a deep understanding of \nthe market to mitigate these risks. It is crucial t o acknowledge \nthat relying solely on stock market predictions doe s not \nguarantee success in trading or investing. By incor porating \nthese considerations, investors can increase the li kelihood of \npositive returns while minimizing the risk of loss.  In light of \nthe contributions of statistical, machine learning,  and deep \nlearning methods, this study aims to investigate tw o specific \nresearch questions: \nRQ1: Do machine learning techniques outperform the \nstatistical techniques when applied to time series data? \nRQ2:  Do deep learning models effectively address \nsequential data challenges and yield superior outco mes when \ncompared to machine learning algorithms? \nPrevious studies used statistical time series methods such as \nmoving averages and autoregressive techniques for s tock \nmarket price forecasting. This study examined the e fficacy of \nthe Transformer deep learning model in predicting c losing \nstock prices, leveraging its ability to handle long -term \ndependencies and address the vanishing gradient problem. This \nstudy aims to make the following contributions: \n1.  Present a novel Transformer-based framework for \npredicting stock prices of real indices, such as Ya hoo, \nJPMorgan, and Facebook, using their intraday data. \n2.  Present a comparative analysis of random forest, LS TM, \nand Transformer-based models on three benchmark \ndatasets. \n3.  Assess the prevalent machine and deep learning models that \ncan benefit financial analysis. \n4.  Evaluate the effectiveness of proposed techniques u sing \nRoot Mean Squared Error (RMSE), Mean Squared Error \n(MSE), and Mean Absolute Percentage Error (MAPE). \nMachine learning and deep learning algorithms \nrevolutionized stock forecasting using historical d ata. In [16], \nthe influence between different machine learning al gorithms \nand multi-feature methods in time series was studie d. In [17], \nan algorithm was presented that combined artificial  neural \nnetworks and genetic algorithms. In [18], a look-ba ck period \nwas used for accurate forecasting. In [19], an RNN was used \nwith news data to achieve better performance than A RIMA. In \n[20], LSTM was used for growth calculation. In [21] , the \nSARIMA and BPNN models were compared on the Korean \nstock exchange. Previous studies have shown that the SARIMA \nmodel outperforms the BPNN and KOSPI models. The KO SPI \nmodel excels at predicting volatile and nonlinear d ata. \nPrediction accuracy depends on the evolution of the  model. In \n[22], regression techniques were used with ARIMA fo r stock \nmarket prediction. In [23], accurate stock price pr ediction was \nachieved in banking-related markets using ARIMA. In  [24], \nsupport vector machines, MLP, and logistic regressi on with \ntechnical indicators were used for NIFTY50 index pr ediction. \nIn [25], a rigorous selection process was used to i dentify \ndynamic stocks on the Dar es Salaam Stock Exchange,  using \nthe LSTM and GRU deep learning models to forecast c losing \nprices. The LSTM model outperformed GRU with a lowe r \nRMSE of 4.7524 and an MAE of 2.4377. In [26], a nov el \nmodel was introduced combining LBL and RNN to captu re \nshort- and long-term sentiment patterns. \nDeep learning algorithms are used in computer visio n, \nvideo games, large data, and multimedia [27-28]. RN Ns are \ncommonly used for time series prediction due to their ability to \ncapture past data [29-30]. In [31], Bayesian RNNs w ere \nimplemented using variational inference and measuri ng the \nambiguity of the prediction. CNNs are effective in forecasting \ntime sequences, such as wind power [32] and precipitation [33] \npredictions. CNN and LSTM were combined in an ensem ble \napproach for air pollution quality forecasting [34- 35]. LSTM \nnetworks were used to predict river discharge level s [36], and \nforecast stock market trends [37-38]. CNNs were als o \nsuggested for market price forecasting [39]. The MF NN model \n[40] combined convolution and recursive neurons for  feature \nextraction in fiscal time-series prediction. LSTM was also used \nto extract data and forecast stock values with impr oved \nperformance [41]. Deep learning models have gained attention \nin various domains, including estimating future val ues. For \nexample, in [42], a deep learning method was used t o forecast \nshort-term electricity demand. \nII.  METHODOLOGY \nTo achieve accurate and timely results in stock mar ket \nforecasting, it is necessary to synchronize histori cal data with \nstreaming data and train machine learning models ac cordingly. \nNumerous techniques have been proposed using machin e \nlearning for this purpose. This study used a regres sion-based \nmodel and an LSTM-based network model. Such models \nfollow a specific structural design, as shown in Figure 1, which \nincludes the following processes: \n Data Collection Stage: Stock data are collected to acquire \nthe necessary dataset for analysis. \n Data Preprocessing Stage: The collected data are su bjected \nto preprocessing techniques to be transformed into a \nsuitable format for further analysis. This stage in volves \ntasks such as data cleaning, handling missing value s, and \nnormalizing. \n Modeling Stage: At this stage, a stock forecast mod el is \ndeveloped using the preprocessed data as input. Dat a \nanalysis techniques are used to accurately predict the \nclosing values of stocks. \n Performance Evaluation Stage: The model's results a re \nevaluated by comparing them with the actual results  to \nassess its validity. If the model fails to produce the desired \nresults, further refinement of the model or data \npreprocessing techniques may be required. Otherwise , the \nresults are presented to the user. \nEngineering, Technology & Applied Science Research  Vol. 13, No. 5, 2023, 11635-11642  11637   \n \nwww.etasr.com Mian: Evaluation of Stock Closing Pri ces using Transformer Learning \n \n Results Presentation Stage: In the final stage, the model and \nits predictions are visually presented to the user,  allowing \nfor a comprehensive understanding of the results. \n \n \nFig. 1.  Time series data analysis flow. \nA.  Dataset \nThis study used three benchmark datasets: Yahoo's [ 43], \nFacebook's [44], and JPMorgan's [45] finance data, from \nJanuary 1, 2017, to September 17. There were no ide ntical or \nmissing values in the dataset. Six features, common  in all \ndatasets, were used: High, Low, Open, Close, Noise,  and \nVolume. The primary objective was to forecast the c losing \nprice based on past data. The dataset features are described as \nfollows: \n Open price represents the initial price of a stock at the \nbeginning of the market trading session. \n Close price signifies the final price of a stock wh en the \nmarket closes. \n High price indicates the highest value attained by a stock \nduring a trading day. \n Low price denotes the lowest value reached by a sto ck \nduring a trading day. \n Volume reflects the number of shares or trades executed for \na particular stock within a given trading day. \nBefore conducting a comprehensive analysis and prediction \nof the closing price, the trends in the closing pri ces were \nexamined, as shown in Figure 2. It was evident that  during the \ninitial phase of the COVID-19 pandemic, the closing  prices of \nthe stocks experienced a significant decline, reach ing their \nlowest values. \n \nFig. 2.  Closing price history. \nB.  Problem Statement \nThe multivariate time series X\nt data at timestamp t were \npassed to the time series model. This was defined as follows: \n\u0001\u0002 \u0003 \u0004\u0001\u0002\u0005\u0006\u0007\b , \u0001\u0002\u0005\u0006\u0007\n , \u0001\u0002\u0005\u0006\u0007\u000b , … … . . \u0001\u0002|\u0001\u000f ∈ \u0011\u0012\u0013 (1)  \nwhere m is the length of input multivariate time series da ta and \nXt is the multivariate vector at timestamp t. A multivariate \nvector Xt contains n variables for a single time period. The n \nvalue depends on the attribute values of the datase t and can be \nexpressed as follows: \n\u0001\u0002 \u0003 \u0004\u0001\u0002\u0014\b\u0015 , \u0001\u0002\u0014\n\u0015 , \u0001\u0002\u0014\u000b\u0015 , … … . . \u0001\u0002\u0014\u0012\u0015 |\u0001\u000f ∈ \u0011 (2) \nThe multivariate time series input Xi∈ R(m×n)  represents the \nhistorical multivariate data for a part-time period from t-m+1 to \nt. In this study, the dataset had multiple input var iables to \nforecast the close prices based on past data. The a pproach was \nto predict the multivariate vector \n\u0001\u0002\u0007\u0016 \u0003 \u0004\u0017\u0014\u0002\u0007\u0016\u0015\u0014\b\u0015, \u0017\u0014\u0002\u0007\u0016\u0015\u0014\n\u0015, … … \u0017\u0014\u0002\u0007\u0016\u0015\u0014\u0012\u0015\u0013 \nof timestamp t+r with past historical data \n\u0001\u0002  \u0003 \u0004 \u0001\u0002\u0005\u0006\u0007\b,  \u0001\u0002\u0005\u0006\u0007\n, … … … … , \u0001\u0002  \u0013  \nas input. \nC.  Learning Based Approaches \n1)  Transformer \nAlthough the Transformer architecture was originall y \ndesigned for Natural Language Processing (NLP), thi s model \nhas also found applications in time-series analysis  and \ncomputer vision tasks. One of the key elements of Transformer \nis the attention mechanism, which allows for a targ eted focus \non specific information. This study used a variant called the \nsparse-attention Transformer to predict the closing  prices of \nstocks. This variant offers reduced memory requirem ents, \nenabling effective handling of longer historical da ta for more \naccurate forecasting. Figure 3 illustrates the arch itecture of the \nproposed approach. \nThe Transformer distinguishes itself from previous \napproaches in time series data and forecasting by employing an \nattention mechanism [41]. This mechanism enables th e \nTransformer to selectively focus on relevant inform ation in \nhistorical data, disregarding irrelevant features. On the \n\nEngineering, Technology & Applied Science Research  Vol. 13, No. 5, 2023, 11635-11642  11638   \n \nwww.etasr.com Mian: Evaluation of Stock Closing Pri ces using Transformer Learning \n \ncontrary, LSTM encounters difficulties in processin g long \nsequences of data due to short-term memory issues. LSTM \nupdates a hidden state with each new input token, resulting in a \nprolonged sequence of input. However, LSTM encounte rs \nchallenges in propagating the entire set of long input sequences \ndue to the vanishing gradient problem. As a consequ ence, \nearlier tokens in the long input sequence are gradu ally \nforgotten. In contrast, Transformer retains indirec t connections \nto all preceding timestamps, facilitating informati on \npropagation across significantly longer sequences. \n \n \nFig. 3.  The Transformer architecture.  \nThe Transformer model consists of two primary \ncomponents, the encoder and the decoder. The encode r applies \ndot-product attention iteratively within the input sequence, \npreserving the sequence length. On the other hand, the decoder \ngenerates the output by using dot-product attention between the \nencoded input and output sequences, with the initia l decoder \nlayer containing a placeholder sequence. The propos ed method \nused the full encoder-decoder structure, allowing the decoder to \ndirectly generate a forecast sequence of the desired length. The \nprimary components of the Transformer model include  the \ninput, encoder, attention mechanism, and decoder. \n Input: The nonsequential processing of historical \ninformation poses a significant challenge for the \nTransformer network. To address this problem, posit ional \nencoding [47] is used to provide context on the rel ative \ndistance between each token and the current timesta mp. \nThis information is crucial for determining relevan ce in the \nself-attention mechanism. Positional encoding assig ns a \nunique real number to each location, ensuring that it is not \nduplicated. The positional encoding function PE ( ) for a \ngiven multivariate time series data X\nt is illustrated in the \nfollowing equations: \n\u0019\u001a \u0014\u0017\n\u000f, 2\u001c\u0015 \u0003 \u0017\u000f + \u001e\u001f !\n\"#\n\b$$$$ %&\n'\n(  (3) \n\u0019\u001a \u0014\u0017\u000f, 2\u001c + 1\u0015 \u0003 \u0017\u000f + *+\u001e !\n\"#\n\b$$$$ %&\n'\n(  (4) \nThe attention block calculates attention weights by  taking \nthe dot product between vectors. The size of the ve ctors \ndetermines the behavior of the attention mechanism. In this \nscenario, as the stock closing price is represented  by a one-\ndimensional vector, this is addressed by expanding the \ninput to a higher embedding dimension using a linear layer. \nThe output of the linear layer transforms the decod er \ninformation into a one-dimensional sequence. \n Encoder: This is a layer that encompasses multiread  self-\nattention along with a two-layer feedforward section. \n Attention: Dot product attention is a commonly used fusion \nmechanism within transformer models. It derives its  name \nfrom the fact that attention weights are calculated  through \ndot products of queries and key vectors. Self-atten tion is \nused in the encoder, expressed by the following equ ation \n[5]: \n,--. -\u001f+ \u0014\u0017\u0015 \u0003  / 0\n1\u00142\u0015\u00143\u00142\u0015\u0015\u00154\n√6 7 8\u0014\u0001\u0015  (5) \nThis equation pertains to self-attention, wherein t he query \nand key vectors are extracted from the input sequen ces. \nHowever, in the decoder attention block, the query vectors \nare obtained from the output sequence, while the ke y \nvectors are derived from the encoder output. \n Decoder: The decoder consists of multiple decoding layers, \nwhere each layer incorporates self-attention on the  decoder \noutput, and subsequently, attention is applied betw een the \ndecoder output and the processed sequence obtained from \nthe encoder. \n2)  Long Short-Term Memory Network (LSTM) \nRNN is a variant of neural networks that incorporat es \nfeedback, allowing the modeling of sequential data. The LSTM \nmodel is a widely used RNN that excels in various p roblem \ndomains [45-47]. LSTM units have a memory capabilit y, \nenabling them to retain information from previous \ncomputations, making them suitable for tasks that i nvolve \nsequential data. Within the LSTM model, gated cells  play a \npivotal role in regulating information flow within the network \n[48]. The LSTM architecture consists of input, forg et, and \noutput gates. The input gate (fi) controls the info rmation to be \nupdated based on the incoming signal, while the forget gate (ft) \ndetermines which states should be retained or forgo tten. The \noutput gate (Ot) decides whether the cell state should influence \nother neurons or not. Activation functions, such as  logistic \nlayers, produce values between 0 and 1 in each laye r. The \nLSTM layer generates a new vector that is added to the state, \nEngineering, Technology & Applied Science Research  Vol. 13, No. 5, 2023, 11635-11642  11639   \n \nwww.etasr.com Mian: Evaluation of Stock Closing Pri ces using Transformer Learning \n \nfacilitating memory retention and information flow [49]. \nLSTM networks are useful for examining the impact o f one \nstock's price change on multiple other stocks over time. They \ncan provide insight into the duration for which pas t stock price \npatterns should be considered and help predict futu re trends in \nstock price variations. LSTM networks offer a power ful \napproach for analyzing time-series data and extract ing \nmeaningful insights for forecasting and trend analy sis. The \nfollowing equations provide a mathematical represen tation of \nthe flow of information within LSTM: \n9\u0002 \u0003 /\u0014:\"; \u0001\u0002 + :<; =\u0002\u0005\b + >;\u0015  (6) \n\u001f\u0002 \u0003 /\u0014:\"\u000f \u0001\u0002 + :<\u000f =\u0002\u0005\b + >\u000f\u0015   (7) \n?\u0002 \u0003 -@ ℎ\u0014:\"B \u0001\u0002 + :<B =\u0002\u0005\b + >B \u0015  (8) \n\u001e\u0002 \u0003 9\u0002 ∗ \u001e\u0002\u0005\b + \u001f\u0002 ∗ ?\u0002    (9) \n+\u0002 \u0003 /\u0014:\"D \u0001\u0002 + :<D =\u0002\u0005\b + >$\u0015   (10) \nE\u0002 \u0003 +\u0002 ∗ -@ ℎ\u0014F\u0002\u0015     (11) \nThe proposed LSTM model consists of an input layer,  two \nhidden layers, and an output layer. The input layer  consists of \n50 neurons, while the first hidden layer consists o f 50 neurons, \nand the second hidden layer contains 25 neurons. Th e final \nlayer contains a single cell. The model was trained  using two \ndifferent group sizes, namely 1 and 15 epochs. This  study used \nthe Adam optimizer and MSE as the loss function. To  \ndetermine the optimal parameters, a series of tests  were \nconducted and the parameters that yielded the best results were \nselected. \n3)  Random Forest \nThis model is an ensemble approach that can assist in \ncategorization and regression problems. While learn ing, a \nrandom selection of features is chosen using a modi fied tree-\nlearning method. This approach regulates only a random subset \nof factors to determine the optimal split at each node. The input \nvector is given to each tree in the Random Forest Model (RFM) \nfor the classification assignment, and each tree ca sts a vote for \na class. Then, the class that receives the most scores is selected. \nCompared to other models, RFM can handle large inpu t \ndatasets and eliminates the overfitting problem by aggregating \nor combining the results of different decision tree s. RFM is a \nsuitable choice for time series tasks, due to its ensemble nature. \nThis study used the random forest regressor with 10 0 \nestimators to predict the closing price. The RFM aggregates the \noutputs of individual decision trees to determine t he mean \nvalue for regression tasks and uses consensus votin g among \ndecision trees for classification tasks. Several hy perparameters \ncan be tuned to enhance the RFM performance. Notabl e \nhyperparameters include maximal features, minimal s ample \nsplit, and the number of estimators. The number of estimators \ndetermines the number of decision trees constructed , and more \ntrees generally improve performance at the cost of increasing \ncomputation time. The minimal sample split determin es the \nminimum number of samples required to split an internal node, \nwhich should be adjusted based on the dataset's siz e. \nFurthermore, RFM is used to assess the relative imp ortance of \nfeatures, allowing the selection of the most signif icant \ncharacteristics for model construction. Overall, th e RFM \nprovides a robust framework for tackling categoriza tion and \nregression problems, particularly in handling large datasets and \ncapturing temporal patterns. \nIII.  RESULTS \nThree benchmarks were used to train and testing the  \nproposed models. These datasets were divided into training and \ntest datasets in a 70:30 ratio, respectively. A PC with a 2.60 \nGHz Intel i7 CPU and 16 GB of RAM and Python Pytorc h 4 \non Widows 10 was used for all tests. The time (s) r equired to \nfinish one round of processing is noted for each ty pe. The \nfollowing metrics were used, with their respective \nmathematical formulas: \n RMSE: Measures the deviation between the actual and  \npredicted values. \nRMSE \u0003K\n\b\nL ∑ \u0014yO −  y\u0015\nL\nQR\b\n   (12) \n MAE: Represents the average of the absolute errors in \nprediction across all samples in the test dataset. \nMAE \u0003\n\b\nL ∑ |yO −  y|L\nQR\b\n   (13) \n MAPE: Quantifies the accuracy of the predictions in  \npercentage terms. \nMAPE \u0003\n\b\nL ∑ |UV\u0005 U|\nU\nL\nQR\b\n    (14) \nThe investigation of RQ1 showed that machine learni ng \nalgorithms yield superior outcomes compared to the statistical \nARIMA model. This study used a range of models incl uding \nARIMA, Random Forest, LSTM, and Transformer. Tables I, II, \nand III display the comprehensive findings that cor respond to \nthe datasets collected from Yahoo Finance, Facebook , and \nJPMorgan, respectively. In particular, Tables I-III illustrate that \nthe ARIMA model performed worse than the learning-b ased \ntechniques. The investigation of RQ2 showed that de ep \nlearning algorithms delivered superior results comp ared to \nconventional machine learning models. This validati on is \nevident in Tables I-III across all benchmark datasets. The RFM \nexhibited inferior performance when compared to LST M and \nTransformer-based models. The Transformer-based app roach \ndemonstrated significantly improved results compare d to both \nmachine learning and statistical methods. \nTABLE I.  YAHOO FINANCE DATASET \nModels MAE RMSE MAPE \nARIMA 2.351 3.099 1.503 \nRandom Forest  2.250  3.154  1.500  \nLSTM 1.758 2.157 1.425 \nTransformer 1.254 2.056 1.256 \nTABLE II.  FACEBOOK STOCK FORECASTING RESULTS \nModels MAE RMSE MAPE \nARIMA 2.423 3.256 1.423 \nRandom Forest 2.265 3.164 1.240 \nLSTM 1.756 2.124 1.435 \nTransformer 1.095 1.985 1.125 \nEngineering, Technology & Applied Science Research  Vol. 13, No. 5, 2023, 11635-11642  11640   \n \nwww.etasr.com Mian: Evaluation of Stock Closing Pri ces using Transformer Learning \n \nTABLE III.  JPMORGAN FORECASTING RESULTS \nModels MAE RMSE MAPE \nARIMA 2.389 3.453 1.564 \nRandom Forest 2.145 3.256 1.356 \nLSTM 1.0995 2.125 1.235 \nTransformer 1.088 1.865 1.116 \n \n(a)  (b)  \n  \nFig. 4.  Transformer forecasting on (a) FB and (b) JP Morgan. \nIV.  DISCUSSION \nDue to continuously fluctuating stock values that depend on \nnumerous factors, predicting the return of the stoc k market is \nboth important and difficult to do. This complexity  and \ndependency on various factors make it difficult to accurately \nforecast the returns and closing prices of multiple  stocks in the \nstock market. Although each company's website lists  a small \nportion of the previous dataset, such as High, Low,  Open, \nClose, and Volume, this information is inadequate t o make a \nreliable forecast. This information only projects a  narrow \npicture of the prospects and performance of the sto ck in the \nfuture. These variables can be used to generate new variables to \nincrease the accuracy of the prediction. An increased number of \nvariables responsible for the movement of the stock market can \nmake the predictions more accurate. The most widely  used \nmethod for making stock forecasts is a trend-based strategy that \nextrapolates a company's future worth from its hist orical stock \nprices. This method assumes that the particular com pany \nmaintains its previous position and keeps going on the track, \ntherefore, maintaining the projectile based on its past record. \nThis is regarded as a tried-and-trusted strategy. T raders or \ninvestors have the greatest impact on a company's worth, and if \neveryone follows the same strategy, it will produce  the same \nresults. Since people tend to follow similar approa ches, the \nresult was expected. As a result, the primary drive r of a \ncompany's ultimate value is its investors. Predicti ng broad \ntrends is simple, but predicting a sudden shift in a company's \nworth is challenging and has long confounded academ ics and \nresearchers, as it typically occurs as a result of various factors, \nsuch as significant company news or changes to the general \nstock market. \nThis study used an LSTM and a Transformer model to \nforecast stock market prices. These models were eva luated on \nthree benchmark datasets, using three distinct situ ations, \nincluding daily predictions of using timestamps fro m 30, 60, \nand 90 days in the past. The results showed that a 90-day \ntimeframe yielded superior results as the models ad vanced in \ntheir learning of more data distribution. The sugge sted \ntechniques performed well, which can help achieve t he desired \noutcomes. The combination of LSTM and regression \nsignificantly increased the precision of the proces s and \ngenerated better outcomes. Newly developed deep lea rning \ntechniques for market forecasting produced promisin g \noutcomes, as the proposed Transformed-based method \nmaximized the accuracy of the predicted stock prices. \nDue to the complications in examining the stock dat a and \nthe fluctuating closing price of the stocks, it is difficult to \ncarefully analyze the stock data using conventional  machine \nlearning methods. To decide which company's stock t o \npurchase or sell, an investor must first understand  how the \nstock market fluctuates. Future stock prices are he avily \ninfluenced by a variety of factors, including company traits, the \nstock's historical price, and recent financial news  about that \nparticular stock. This study was designed to assist  investors in \nmaking informed decisions regarding the buy or sell  process \nand protect them from potential financial losses. \nV.  CONCLUSION AND FUTURE DIRECTION \nThe stock market is a complex and ever-changing \nenvironment that is difficult to predict accurately . Traditional \nmethods of analyzing stock data may not be enough t o provide \ninvestors with reliable insights into the stock mar ket. Machine \nlearning and deep learning algorithms offer promising solutions \nto this problem. By analyzing historical stock data  and using \nadvanced techniques, these algorithms can generate predictions \nwith greater accuracy, enabling investors to make m ore \ninformed decisions about buying or selling stocks. In addition, \nthe Transformed-based framework proposed in this study could \nhelp investors protect themselves from financial lo ss by \nallowing them to predict the price of a stock and m ake more \ninformed decisions. The proposed Transformer-based model \nhas some limitations, such as input structure, temp oral \ndependencies, interpretability, and limited trainin g data. Stock \nexchange datasets have some time missing values and  \nunstructured information that make the function and the results \nof the proposed model harder. Another problem is th at time \nseries are limited, so effective training cannot be  possible. In \nthe future, the ensemble approach of machine learni ng will be \ninvestigated with a bio-inspired technique to overc ome these \nissues. \nREFERENCES \n[1]  A. Thakkar and K. Chaudhari, \"Fusion in stock marke t prediction: A \ndecade survey on the necessity, recent developments , and potential \nfuture directions,\" Information Fusion , vol. 65, pp. 95–107, Jan. 2021, \nhttps://doi.org/10.1016/j.inffus.2020.08.019. \n[2]  S. M. Idrees, M. A. Alam, and P. Agarwal, \"A Predic tion Approach for \nStock Market Volatility Based on Time Series Data,\"  IEEE Access , vol. \n7, pp. 17287–17298, 2019, https://doi.org/10.1109/A CCESS.2019. \n2895252. \n[3]  K. C. Rasekhschaffe and R. C. Jones, \"Machine Learn ing for Stock \nSelection,\" Financial Analysts Journal , vol. 75, no. 3, pp. 70–88, Jul. \n2019, https://doi.org/10.1080/0015198X.2019.1596678. \n[4]  C. S. Wong and W. K. Li, \"On a Mixture Autoregressi ve Model,\" \nJournal of the Royal Statistical Society Series B: Statistical \nMethodology , vol. 62, no. 1, pp. 95–115, Jan. 2000, https://do i.org/ \n10.1111/1467-9868.00222. \n[5]  M. R. Hassan and B. Nath, \"Stock market forecasting  using hidden \nMarkov model: a new approach,\" in 5th International Conference on \nIntelligent Systems Design and Applications (ISDA’0 5) , Warsaw, \nPoland, Sep. 2005, pp. 192–196, https://doi.org/10.1109/ISDA.2005.85. \n[6]  E. K. Ampomah, Z. Qin, and G. Nyame, \"Evaluation of  Tree-Based \nEnsemble Machine Learning Models in Predicting Stock Price Direction \n\nEngineering, Technology & Applied Science Research  Vol. 13, No. 5, 2023, 11635-11642  11641   \n \nwww.etasr.com Mian: Evaluation of Stock Closing Pri ces using Transformer Learning \n \nof Movement,\" Information , vol. 11, no. 6, Jun. 2020, Art. no. 332, \nhttps://doi.org/10.3390/info11060332. \n[7]  X. Yu and D. Li, \"Important Trading Point Predictio n Using a Hybrid \nConvolutional Recurrent Neural Network,\" Applied Sciences , vol. 11, \nno. 9, Jan. 2021, Art. no. 3984, https://doi.org/10.3390/app11093984. \n[8]  A. P. Ruiz, A. A. Gila, U. Irusta, and J. E. Huguet, \"Why Deep Learning \nPerforms Better than Classical Learning?,\" DYNA Ingeneria e Industria , \nvol. 95, no. 2, pp. 119–122, Mar. 2020, https://doi.org/10.6036/9574. \n[9]  J. Liu, X. Guo, B. Li, and Y. Yuan, \"COINet: Adapti ve Segmentation \nwith Co-Interactive Network for Autonomous Driving, \" in 2021 \nIEEE/RSJ International Conference on Intelligent Ro bots and Systems \n(IROS) , Prague, Czech Republic, Sep. 2021, pp. 4800–4806,  \nhttps://doi.org/10.1109/IROS51168.2021.9636111. \n[10]  N. M. H. Masoud, \"The Impact of Stock Market Perfor mance upon \nEconomic Growth,\" International Journal of Economics and Financial \nIssues , vol. 3, no. 4, pp. 788–798, Dec. 2013. \n[11]  A. Murkute and T. Sarode, \"Forecasting Market Price  of Stock using \nArtificial Neural Network,\" International Journal of Computer \nApplications , vol. 124, no. 12, pp. 11–15, Aug. 2015, https://d oi.org/ \n10.5120/ijca2015905681. \n[12]  L. J. Kao, C. C. Chiu, C. J. Lu, and J. L. Yang, \"I ntegration of nonlinear \nindependent component analysis and support vector r egression for stock \nprice forecasting,\" Neurocomputing , vol. 99, pp. 534–542, Jan. 2013, \nhttps://doi.org/10.1016/j.neucom.2012.06.037. \n[13]  W. Khan, M. ali Ghazanfar, M. Assam, S. Ahmad, and J. Khan, \n\"Predicting Trend in Stock Market Exchange Using Ma chine Learning \nClassifiers,\" Science International , vol. 28, no. 2, pp. 1363–1367, May \n2016. \n[14]  R. Gupta, N. Garg, and S. Singh, \"Stock Market Pred iction Accuracy \nAnalysis Using Kappa Measure,\" in 2013 International Conference on \nCommunication Systems and Network Technologies , Gwalior, India, \nApr. 2013, pp. 635–639, https://doi.org/10.1109/CSNT.2013.136. \n[15]  A. E. Ezugwu et al. , \"A comprehensive survey of clustering algorithms:  \nState-of-the-art machine learning applications, tax onomy, challenges, \nand future research prospects,\" Engineering Applications of Artificial \nIntelligence , vol. 110, Apr. 2022, Art. no. 104743, https://doi .org/ \n10.1016/j.engappai.2022.104743. \n[16]  L. Li, Y. Wu, Y. Ou, Q. Li, Y. Zhou, and D. Chen, \" Research on \nmachine learning algorithms and feature extraction for time series,\" in \n2017 IEEE 28th Annual International Symposium on Pe rsonal, Indoor, \nand Mobile Radio Communications (PIMRC) , Montreal, QC, Canada, \nJul. 2017, pp. 1–5, https://doi.org/10.1109/PIMRC.2017.8292668. \n[17]  J. Zhao, N. Sun, and W. Cheng, \"Logistics forum bas ed prediction on \nstock index using intelligent data analysis and pro cessing of online web \nposts,\" Journal of Ambient Intelligence and Humanized Compu ting , vol. \n11, no. 9, pp. 3575–3584, Sep. 2020, https://doi.or g/10.1007/s12652-\n019-01520-x. \n[18]  A. S. Saud and S. Shakya, \"Analysis of look back pe riod for stock price \nprediction with RNN variants: A case study on banki ng sector of \nNEPSE,\" Procedia Computer Science , vol. 167, pp. 788–798, Jan. 2020, \nhttps://doi.org/10.1016/j.procs.2020.03.419. \n[19]  S. Mohan, S. Mullapudi, S. Sammeta, P. Vijayvergia,  and D. C. \nAnastasiu, \"Stock Price Prediction Using News Senti ment Analysis,\" \npresented at the 2019 IEEE Fifth International Conf erence on Big Data \nComputing Service and Applications (BigDataService) , Newark, CA, \nUSA, Apr. 2019, pp. 205–208, https://doi.org/10.110 9/ \nBigDataService.2019.00035. \n[20]  A. Ghosh, S. Bose, G. Maji, N. Debnath, and S. Sen,  \"Stock Price \nPrediction Using LSTM on Indian Share Market,\" in Proceedings of \n32nd International Conference on Computer Applicati ons in Industry \nand Engineering , 2019, pp. 101–110, https://doi.org/10.29007/qgcz. \n[21]  \"Neural Network Model vs. SARIMA Model In Forecasti ng Korean \nStock Price Index (KOSPI),\" Issues In Information Systems , vol. VIII, \nno. 2, 2007, https://doi.org/10.48009/2_iis_2007_372-378. \n[22]  A. A. Ariyo, A. O. Adewumi, and C. K. Ayo, \"Stock P rice Prediction \nUsing the ARIMA Model,\" in 2014 UKSim-AMSS 16th International \nConference on Computer Modelling and Simulation , Cambridge, UK, \nMar. 2014, pp. 106–112, https://doi.org/10.1109/UKSim.2014.67. \n[23]  M. Almasarweh and S. A. Wadi, \"ARIMA Model in Predi cting Banking \nStock Market Data,\" Modern Applied Science , vol. 12, no. 11, 2018. \n[24]  I. R. Parray, S. S. Khurana, M. Kumar, and A. A. Al talbe, \"Time series \ndata analysis of stock price movement using machine  learning \ntechniques,\" Soft Computing , vol. 24, no. 21, pp. 16509–16517, Nov. \n2020, https://doi.org/10.1007/s00500-020-04957-x. \n[25]  S. Joseph, N. Mduma, and D. Nyambo, \"A Deep Learnin g Model for \nPredicting Stock Prices in Tanzania,\" Engineering, Technology & \nApplied Science Research , vol. 13, no. 2, pp. 10517–10522, Apr. 2023, \nhttps://doi.org/10.48084/etasr.5710. \n[26]  U. P. Gurav and S. Kotrappa, \"Sentiment Aware Stock Price Forecasting \nusing an SA-RNN-LBL Learning Model,\" Engineering, Technology & \nApplied Science Research , vol. 10, no. 5, pp. 6356–6361, Oct. 2020, \nhttps://doi.org/10.48084/etasr.3805. \n[27]  Y. LeCun, Y. Bengio, and G. Hinton, \"Deep learning,\" Nature , vol. 521, \nno. 7553, pp. 436–444, May 2015, https://doi.org/10.1038/nature14539. \n[28]  D. Salinas, V. Flunkert, J. Gasthaus, and T. Janusc howski, \"DeepAR: \nProbabilistic forecasting with autoregressive recur rent networks,\" \nInternational Journal of Forecasting , vol. 36, no. 3, pp. 1181–1191, Jul. \n2020, https://doi.org/10.1016/j.ijforecast.2019.07.001. \n[29]  D. T. Mirikitani and N. Nikolaev, \"Recursive Bayesian Recurrent Neural \nNetworks for Time-Series Modeling,\" IEEE Transactions on Neural \nNetworks , vol. 21, no. 2, pp. 262–274, Oct. 2010, \nhttps://doi.org/10.1109/TNN.2009.2036174. \n[30]  H. Wang, G. Li, G. Wang, J. Peng, H. Jiang, and Y. Liu, \"Deep learning \nbased ensemble approach for probabilistic wind powe r forecasting,\" \nApplied Energy , vol. 188, pp. 56–70, Feb. 2017, https://doi.org/ \n10.1016/j.apenergy.2016.11.111. \n[31]  X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W. Wong, and  W. Woo, \n\"Convolutional LSTM Network: A Machine Learning App roach for \nPrecipitation Nowcasting,\" in Advances in Neural Information \nProcessing Systems , 2015, vol. 28. \n[32]  K. Amarasinghe, D. L. Marino, and M. Manic, \"Deep n eural networks \nfor energy load forecasting,\" in 2017 IEEE 26th International \nSymposium on Industrial Electronics (ISIE) , Edinburgh, UK, Jun. 2017, \npp. 1483–1488, https://doi.org/10.1109/ISIE.2017.8001465. \n[33]  C. J. Huang and P. H. Kuo, \"A Deep CNN-LSTM Model f or Particulate \nMatter (PM2.5) Forecasting in Smart Cities,\" Sensors , vol. 18, no. 7, \n2018, https://doi.org/10.3390/s18072220. \n[34]  Y. Sudriani, I. Ridwansyah, and H. A. Rustini, \"Lon g short term \nmemory (LSTM) recurrent neural network (RNN) for di scharge level \nprediction and forecast in Cimandiri river, Indones ia,\" IOP Conference \nSeries: Earth and Environmental Science , vol. 299, no. 1, Apr. 2019, \nArt. no. 012037, https://doi.org/10.1088/1755-1315/299/1/012037. \n[35]  X. Ding, Y. Zhang, T. Liu, and J. Duan, \"Deep learn ing for event-driven \nstock prediction,\" in Proceedings of the 24th International Conference \non Artificial Intelligence , Buenos Aires, Argentina, Apr. 2015, pp. \n2327–2333. \n[36]  D. M. Q. Nelson, A. C. M. Pereira, and R. A. de Oli veira, \"Stock \nmarket’s price movement prediction with LSTM neural  networks,\" in \n2017 International Joint Conference on Neural Netwo rks (IJCNN) , \nAnchorage, AK, USA, Feb. 2017, pp. 1419–1426, https ://doi.org/ \n10.1109/IJCNN.2017.7966019. \n[37]  S. Al-Janabi, A. Alkaim, E. Al-Janabi, A. Aljeboree , and M. Mustafa, \n\"Intelligent forecaster of concentrations (PM2.5, P M10, NO2, CO, O3, \nSO2) caused air pollution (IFCsAP),\" Neural Computing and \nApplications , vol. 33, no. 21, pp. 14199–14229, Nov. 2021, \nhttps://doi.org/10.1007/s00521-021-06067-7. \n[38]  Y. Hu, \"Stock market timing model based on convolut ional neural \nnetwork–a case study of Shanghai composite index,\" Finance& \nEconomy , vol. 4, pp. 71–74, 2018. \n[39]  W. Long, Z. Lu, and L. Cui, \"Deep learning-based fe ature engineering \nfor stock price movement prediction,\" Knowledge-Based Systems , vol. \n164, pp. 163–173, Jan. 2019, https://doi.org/10.101 6/j.knosys.2018. \n10.034. \n[40]  X. Pang, Y. Zhou, P. Wang, W. Lin, and V. Chang, \"A n innovative \nneural network approach for stock market prediction ,\" The Journal of \nEngineering, Technology & Applied Science Research  Vol. 13, No. 5, 2023, 11635-11642  11642   \n \nwww.etasr.com Mian: Evaluation of Stock Closing Pri ces using Transformer Learning \n \nSupercomputing , vol. 76, no. 3, pp. 2098–2118, Mar. 2020, \nhttps://doi.org/10.1007/s11227-017-2228-y. \n[41]  A. Zhang, Z. C. Lipton, M. Li, and A. J. Smola, \"Di ve into Deep \nLearning.\" arXiv, Feb. 10, 2023, https://doi.org/10 .48550/arXiv.2106. \n11342. \n[42]  S. K. Filipova-Petrakieva and V. Dochev, \"Short-Ter m Forecasting of \nHourly Electricity Power Demand: Reggresion and Clu ster Methods for \nShort-Term Prognosis,\" Engineering, Technology & Applied Science \nResearch, vol. 12, no. 2, pp. 8374–8381, Apr. 2022, https:/ /doi.org/ \n10.48084/etasr.4787. \n[43]  \"Yahoo Finance - Stock Market Live, Quotes, Busines s & Finance \nNews.\" https://finance.yahoo.com/. \n[44]  \"Meta - Financials.\" https://investor.fb.com/financials/default.aspx. \n[45]  \"J.P. Morgan Data and Analytics.\" https://www.jpmor gan.com/ \nsecurities-services/data-analytics. \n[46]  E. Haugsdal, E. Aune, and M. Ruocco, \"Persistence I nitialization: A \nnovel adaptation of the Transformer architecture fo r Time Series \nForecasting.\" arXiv, Aug. 30, 2022, https://doi.org /10.48550/ \narXiv.2208.14236. \n[47]  S. Hochreiter and J. Schmidhuber, \"Long Short-Term Memory,\" Neural \nComputation , vol. 9, no. 8, pp. 1735–1780, Nov. 1997, https:// doi.org/ \n10.1162/neco.1997.9.8.1735. \n[48]  S. Fernández, A. Graves, and J. Schmidhuber, \"An Ap plication of \nRecurrent Neural Networks to Discriminative Keyword  Spotting,\" in \nArtificial Neural Networks – ICANN 2007 , 2007, pp. 220–229, \nhttps://doi.org/10.1007/978-3-540-74695-9_23. \n[49]  J. Schmidhuber, \"Deep learning in neural networks: An overview,\" \nNeural Networks , vol. 61, pp. 85–117, Jan. 2015, https://doi.org/ \n10.1016/j.neunet.2014.09.003. \n ",
  "topic": "Mean squared error",
  "concepts": [
    {
      "name": "Mean squared error",
      "score": 0.6150745153427124
    },
    {
      "name": "Autoregressive integrated moving average",
      "score": 0.610230028629303
    },
    {
      "name": "Stock market",
      "score": 0.5798200964927673
    },
    {
      "name": "Econometrics",
      "score": 0.5690356492996216
    },
    {
      "name": "Stock (firearms)",
      "score": 0.5447098612785339
    },
    {
      "name": "Random forest",
      "score": 0.5302490592002869
    },
    {
      "name": "Volatility (finance)",
      "score": 0.4978187084197998
    },
    {
      "name": "Mean absolute percentage error",
      "score": 0.4939597249031067
    },
    {
      "name": "Sliding window protocol",
      "score": 0.4757411479949951
    },
    {
      "name": "Computer science",
      "score": 0.47012531757354736
    },
    {
      "name": "Transformer",
      "score": 0.4278801679611206
    },
    {
      "name": "Artificial intelligence",
      "score": 0.40188220143318176
    },
    {
      "name": "Machine learning",
      "score": 0.38615721464157104
    },
    {
      "name": "Time series",
      "score": 0.37942779064178467
    },
    {
      "name": "Artificial neural network",
      "score": 0.3689151406288147
    },
    {
      "name": "Economics",
      "score": 0.3222849369049072
    },
    {
      "name": "Statistics",
      "score": 0.30445170402526855
    },
    {
      "name": "Mathematics",
      "score": 0.22769397497177124
    },
    {
      "name": "Engineering",
      "score": 0.19876840710639954
    },
    {
      "name": "Window (computing)",
      "score": 0.1214253306388855
    },
    {
      "name": "Electrical engineering",
      "score": 0.10054650902748108
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Horse",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I23075662",
      "name": "Taibah University",
      "country": "SA"
    }
  ],
  "cited_by": 15
}