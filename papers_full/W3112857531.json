{
  "title": "A Sparse Transformer-Based Approach for Image Captioning",
  "url": "https://openalex.org/W3112857531",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1593123184",
      "name": "Zhou Lei",
      "affiliations": [
        "Shanghai University of Engineering Science"
      ]
    },
    {
      "id": "https://openalex.org/A2305838298",
      "name": "Congcong Zhou",
      "affiliations": [
        "Shanghai University of Engineering Science"
      ]
    },
    {
      "id": "https://openalex.org/A2117806245",
      "name": "Shengbo Chen",
      "affiliations": [
        "Shanghai University of Engineering Science"
      ]
    },
    {
      "id": "https://openalex.org/A2119731218",
      "name": "Yi-yong Huang",
      "affiliations": [
        "Shanghai University of Engineering Science"
      ]
    },
    {
      "id": "https://openalex.org/A2529347253",
      "name": "Xianrui Liu",
      "affiliations": [
        "Shanghai University of Engineering Science"
      ]
    },
    {
      "id": "https://openalex.org/A1593123184",
      "name": "Zhou Lei",
      "affiliations": [
        "Shanghai University of Engineering Science"
      ]
    },
    {
      "id": "https://openalex.org/A2305838298",
      "name": "Congcong Zhou",
      "affiliations": [
        "Shanghai University of Engineering Science"
      ]
    },
    {
      "id": "https://openalex.org/A2117806245",
      "name": "Shengbo Chen",
      "affiliations": [
        "Shanghai University of Engineering Science"
      ]
    },
    {
      "id": "https://openalex.org/A2119731218",
      "name": "Yi-yong Huang",
      "affiliations": [
        "Shanghai University of Engineering Science"
      ]
    },
    {
      "id": "https://openalex.org/A2529347253",
      "name": "Xianrui Liu",
      "affiliations": [
        "Shanghai University of Engineering Science"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6620707391",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W6765264507",
    "https://openalex.org/W2982553922",
    "https://openalex.org/W2972897806",
    "https://openalex.org/W2965697393",
    "https://openalex.org/W1956340063",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W6775772702",
    "https://openalex.org/W2965359408",
    "https://openalex.org/W2984138079",
    "https://openalex.org/W1895577753",
    "https://openalex.org/W2896348597",
    "https://openalex.org/W6767200354",
    "https://openalex.org/W2963101956",
    "https://openalex.org/W6726873649",
    "https://openalex.org/W3034984754",
    "https://openalex.org/W2890531016",
    "https://openalex.org/W2990818246",
    "https://openalex.org/W6763643401",
    "https://openalex.org/W6638742206",
    "https://openalex.org/W2808206191",
    "https://openalex.org/W3034655362",
    "https://openalex.org/W6761628794",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6772230799",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W1773149199",
    "https://openalex.org/W2795151422",
    "https://openalex.org/W2901988662",
    "https://openalex.org/W2979747405",
    "https://openalex.org/W2550553598",
    "https://openalex.org/W2575842049",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W6630875275",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W1947481528",
    "https://openalex.org/W2986670728",
    "https://openalex.org/W6744684495",
    "https://openalex.org/W2885013662",
    "https://openalex.org/W2887585070",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2277195237",
    "https://openalex.org/W6729624334",
    "https://openalex.org/W6621543089",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2997753998",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W4288289156",
    "https://openalex.org/W2970569830",
    "https://openalex.org/W1811254738",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W2962706528",
    "https://openalex.org/W2955227499",
    "https://openalex.org/W2754927243",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W639708223",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W648786980",
    "https://openalex.org/W4288329833",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1895989618",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2971310675",
    "https://openalex.org/W3105136412",
    "https://openalex.org/W2962760898",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2552161745",
    "https://openalex.org/W2973642807",
    "https://openalex.org/W3035284526"
  ],
  "abstract": "Image Captioning is the task of providing a natural language description for an image. It has caught significant amounts of attention from both computer vision and natural language processing communities. Most image captioning models adopt deep encoder-decoder architectures to achieve state-of-the-art performances. However, it is difficult to model knowledge on relationships between input image region pairs in the encoder. Furthermore, the word in the decoder hardly knows the correlation to specific image regions. In this article, a novel deep encoder-decoder model is proposed for image captioning which is developed on sparse Transformer framework. The encoder adopts a multi-level representation of image features based on self-attention to exploit low-level and high-level features, naturally the correlations between image region pairs are adequately modeled as self-attention operation can be seen as a way of encoding pairwise relationships. The decoder improves the concentration of multi-head self-attention on the global context by explicitly selecting the most relevant segments at each row of the attention matrix. It can help the model focus on the more contributing image regions and generate more accurate words in the context. Experiments demonstrate that our model outperforms previous methods and achieves higher performance on MSCOCO and Flickr30k datasets. Our code is available at <uri>https://github.com/2014gaokao/ImageCaptioning</uri>.",
  "full_text": "Received August 30, 2020, accepted September 13, 2020, date of publication September 18, 2020,\ndate of current version December 9, 2020.\nDigital Object Identifier 10.1 109/ACCESS.2020.3024639\nA Sparse Transformer-Based Approach for\nImage Captioning\nZHOU LEI, CONGCONG ZHOU\n, SHENGBO CHEN, YIYONG HUANG, AND XIANRUI LIU\nSchool of Computer Engineering and Science, Shanghai University, Shanghai 200444, China\nCorresponding author: Zhou Lei (leiz@shu.edu.cn)\nThis work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant 61572306, in part by the\nNational Key Research and Development Program of China under Grant 2017YFB0701600, in part by the Science and Technology\nCommittee of Shanghai Municipality under Grant 19511121002, and in part by the Shanghai Engineering Research Center of Intelligent\nComputing System under Grant 19DZ2252600.\nABSTRACT Image Captioning is the task of providing a natural language description for an image.\nIt has caught signiﬁcant amounts of attention from both computer vision and natural language processing\ncommunities. Most image captioning models adopt deep encoder-decoder architectures to achieve state-\nof-the-art performances. However, it is difﬁcult to model knowledge on relationships between input image\nregion pairs in the encoder. Furthermore, the word in the decoder hardly knows the correlation to speciﬁc\nimage regions. In this article, a novel deep encoder-decoder model is proposed for image captioning\nwhich is developed on sparse Transformer framework. The encoder adopts a multi-level representation of\nimage features based on self-attention to exploit low-level and high-level features, naturally the correlations\nbetween image region pairs are adequately modeled as self-attention operation can be seen as a way of\nencoding pairwise relationships. The decoder improves the concentration of multi-head self-attention on the\nglobal context by explicitly selecting the most relevant segments at each row of the attention matrix. It can\nhelp the model focus on the more contributing image regions and generate more accurate words in the context.\nExperiments demonstrate that our model outperforms previous methods and achieves higher performance on\nMSCOCO and Flickr30k datasets. Our code is available at https://github.com/2014gaokao/ImageCaptioning.\nINDEX TERMS Image captioning, self-attention, explict sparse, local adaptive threshold.\nI. INTRODUCTION\nNowadays, we encounter a large number of images from the\ninternet while most of them do not have a description. How-\never, machine needs to automatic interpret image captions in\nthe era of Artiﬁcial Intelligence. Image captioning has been\nan important research direction for many years. For example,\nit could help one who is visually impaired to understand\nimages on the Internet. Also it has found applications in many\nareas such as content-based image retrieval, social media\nplatforms and human-machine interaction. Image captioning\nis one of the most challenging tasks in computer vision that\nintends to automatically generate natural descriptions for\nimages. It does not only require detecting the objects and\ntheir relationships in an image but also needs to generate a\nsyntactically and semantically correct sentence [1].\nThe associate editor coordinating the review of this manuscript and\napproving it for publication was Chang-Hwan Son\n.\nThe encoder-decoder architecture is the mainstream\napproach of image captioning followed by [2], [3]. In such\na framework for image captioning, an image is ﬁrst encoded\ninto a set of feature vectors via a CNN based network [4]\nand then decoded to words via an RNN based network [5].\nIn addition, attention mechanism [6] plays an important role\nin such an encoder-decoder framework to generate encoded\nvectors based on image features and hidden states of LSTM\nat each time step which aims to align words to speciﬁc\nimage regions [7]. In recent years, the great success of\nTransformer network [8] has attracted many researchers to\nexplore towards the cutting edge techniques of self-attention\nfor image captioning. The outstanding performance of self-\nattention comes from the ability to explore the relationships\nbetween the detected entities in encoder and model the cor-\nrelation between image regions and hidden states as attention\nmechanism [9], [10].\nHowever, the self-attention operation in original Trans-\nformer framework has an obvious drawback, as the original\nVOLUME 8, 2020 This work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/ 213437\nZ. Leiet al.: Sparse Transformer-Based Approach for Image Captioning\nTransformer assigns credits to all components of the context.\nIt is unreasonable because many credits may be assigned to\nirrelevant information and should be ignored. For instance,\nthe mainstream self-attention based mechanism estimates\nattention weights via multiplying the given query(hidden\nstates) with key(encoded image features) from different\nmodalities. The attention matrix is then applied to the\nvalue(encoded image features) to derive a weighted sum.\nNevertheless, encoded image features may have little correla-\ntion to some irrelevant words(hidden states), which will lead\nto a very small fraction after multiplying the given query by\nkey. So explicit sparse Transformer framework is needed to\ntackle the problem. More recently, local attention constraints\nwas introduced in [11] and Zhao et al. [12] proposed to\nsparse attention with top-k strategy, however constraint local\nattention will break long term dependency and top-k strategy\ncan not ﬁlter irrelevant information efﬁciently when the\nattention scores are very close.\nIn order to resolve these problems mentioned above,\nwe propose a novel model named Local Adaptive Thresh-\nold to explicit sparse attention matrix for image caption-\ning, which mitigates the drawback of previous methods by\nenhancing attention selection ability. Technically, we group\nattention matrix into multiple chunks and the element will be\nﬁltered if its value is smaller than the average distribution of\nthe data in the according chunk. Local Adaptive Threshold\nwill not hurt long term dependency and is determined by the\ndistribution of neighbor nodes for each element. For further\ninvestigation, we integrate Local Adaptive Threshold into\nself-attention and acts as attention mechanism in the decoder\nwhich triggers the interaction between visual features and\nnatural description. In a nutshell, Local Adaptive Threshold\npays little attention to the least contributive states and can per-\nform more concentrated attention than original Transformer\nframework. On the encoder side, the application of self-\nattention encourages our model to explore the relationships\nbetween the detected entities. Furthermore, we extend the\ndetected entities with extra vectors to encode a persistent\nmemory, these vectors are shared across all attention heads\ninstead of relying on certain head and can be designed as\ntrainable weights.\nThe contributions of this work are summarized as follows:\n• Given that Transformer based attention mechanism may\nextract irrelevant information, we introduce a novel\nmodel named Local Adaptive Threshold to explicitly\nignore the least contributive elements in attention matrix\nto reduce the extraction of irrelevant items and enhance\nthe concentration of attention.\n• We adopt a multi-level representations of image features\nto help encoder exploit low-level and high-level features,\nconsidering different feature levels may contribute dif-\nferent inﬂuences to the performance of model. Extra\nvectors are concatenated with feature vectors to encode\na persistent memory.\n• We conducted experiments on the MSCOCO [13]\nand Flickr30k [14] datasets. Our methods achieved a\nbetter performance of 129.2 CIDEr score on ’’Karpa-\nthy’’ ofﬂine test split and 23.5 Meteor score on Flickr30k\ntest split compared to previous methods.\nThe rest of the paper is organized as follows. In Section II,\nwe review related works on modern image captioning meth-\nods including earlier popular approaches, Transformed based\nmethods and LSTM improved methods. In Section III,\nwe introduce our framework in detail. In particular, we dis-\ncuss the modiﬁcations we make on the encoder and decoder.\nSection IV reports some experimental results that demon-\nstrate the effectiveness and efﬁciency of our model. Section V\nconcludes the paper.\nII. RELATED WORK\nA. IMAGE CAPTIONING\nPopular approaches to image captioning adopt a deep\nencoder-decoder framework with attention mechanism. Typi-\ncally, Anderson et al.[7] extract a set of salient image regions\nby a pooled convolutional feature vector using Faster R-CNN\nwhich enables the attention scores to be computed at the\nlevels of salient image regions. Lu et al.[15] ﬁrst generate a\nsentence template with blank slots which will be ﬁlled in by\nvisual concepts using object detectors. Cornia et al.[16] pro-\npose a controllable approach to shift the rank of salient image\nregions by shift gate with adaptive attention. Li et al.[17]\nintroduce a new architecture to facilitate vocabulary expan-\nsion and produce novel objects via pointing mechanism\nand object learners. Derived from human intuition, atten-\ntion mechanism has achieved signiﬁcant improvements for\nmachine translation tasks. Attention mechanism normally\ncalculates an importance score for each feature vector and\nnormalizes to weights using a softmax function. Weights\nare applied to the feature vector to generate the attention\nresult [6]. Chen et al.[18] propose a spatial and channel-wise\nattention mechanism that applies an attention mechanism in\na channel-wise manner to be a process of selecting semantic\nattributes. Lu et al.[19] study an adaptive attention mecha-\nnism which is proposed to map feature vector to visual word\nor context word. Huang et al.[20] design an adaptive attention\ntime mechanism to achieve arbitrary mapping between image\nregion and word by adaptive steps across all time steps. More\nrecently, more complex information such as attributes and\nrelationships are integrated by GCN [21] to generate ﬁne-\ngrained level captions [22], [23] [24].\nB. TRANSFORMER BASED METHODS\nMore researchers tend to utilize Transformer framework to\nimprove captions due to the excellent performance of self-\nattention operation. Herdade et al.[25] design an architecture\nthat incorporates information about the spatial relationships\nbetween detected objects through geometric self-attention.\nLi et al.[26] devise a unique attention mechanism to exploit\nthe visual and semantic information simultaneously in the\ntransformer framework. In addition, Huang et al. [9] intro-\nduce an extension of the attention operator in which the\n213438 VOLUME 8, 2020\nZ. Leiet al.: Sparse Transformer-Based Approach for Image Captioning\nﬁnal attended information is concatenated with the hidden\nstate of LSTM and the attention context. For enhancing the\nconnection between encoder and decoder, Cornia et al.[10]\npropose a meshed transformer to learn a hierarchical rep-\nresentation of the relationships between image regions and\nuse a mesh-like connectivity at the decoding stage to exploit\nhierarchical image features. Multiple instance learning is also\nemployed to build an attribute extractor in order to explore\nand distill cross-modal information where multi-head atten-\ntion is adopted [27]. Pan et al. [28] introduce a X-Linear\nattention block which can measure both spatial and channel-\nwise attention distributions by bilinear pooling techniques.\nThose works both take image features and hidden states as\ninput to generate attention scores in self-attention operation,\nhowever not all the image regions or hidden states play an\nequal importance on the model, ﬁlter irrelevant elements is\nable to make attention more concentrated.\nC. LSTM IMPROVED METHODS\nA Long Short-Term Memory performs as a sequence model\nto generate words according to input image features which\nis another important branch worth exploring. Ke et al. [29]\nintroduce reﬂective position module and reﬂective attention\nmodule while the former selectively attends to the hidden\nstates and the latter determines the attention distribution over\nthe input image regions. Qin et al. [30] introduce a novel\nLSTM model consists of two parts where the one looks\nback attention value of previous time step and input into the\ncurrent time step while the other predicts forward the next two\nwords in one time step to improve captioning performance.\nZheng et al.[31] start the description with a selected object\nand generate other parts of the sequence based on this object,\nthe left-side sequence is generated according to visual fea-\ntures and the input object, while the right-side sequence is\ngenerated by left-side sequence. Deshpande et al.[32] pro-\npose to utilize part-of-speech tags to produce fast and accurate\ndescription, the captioning model take part-of-speech tags,\nimage features and hidden states as input to generate a uni-\nﬁed attention output as common methods do. The number\nof works aiming to improve LSTM is far less than other\nmethods, however it is still worth exploring.\nOur work draws on those related works and introduces\na novel model named Local Adaptive Threshold which can\nsparse attention matrix efﬁciently and perform more concen-\ntrated attention than original Transformer framework. In next\nsection, we introduce the modiﬁcations on self-attention\nmodule and describe the architecture of our image captioning\nmodel in detail.\nIII. OUR METHOD\nA. LOCAL ADAPTIVE THRESHOLD ON SELF-ATTENTION\nIn a transformer framework, an attention function consists of\na query and a set of key-value pairs where the query, key and\nvalue are all vectors. As shown in Figure 1(a), it ﬁrst measures\na similarity score between query and key and scales down\nthe score by √\ndk , where dk is normally the dimension of\nquery and key. Then it applies a softmax function to obtain\nthe weights on the values. The ﬁnal output of the matrix can\nbe computed as:\nα(q,k,v) =softmax( qkT\n√dk\n)v (1)\nwhere q, k and v represent query, key and value, respec-\ntively. Dot-product attention and multi-head attention are\nboth proven to be efﬁcient in practice in Transformer,\nwhere multi-head attention is concatenation of dot-product\nattention.\nFIGURE 1. (a) Original Scaled Dot-Product Attention. (b) Attention\nIntegrated with Local Adaptive Threshold.\nIn Figure 1(b), we integrate our sparse module between\nthe scale function and the softmax function to convergent\nattention by selecting the items with higher numerical value\nand ignoring the elements with smaller numerical values,\nsince softmax function is dominated by the largest elements.\nAs shown in Figure 2, based on the hypothesis that the\nelements in matrix P with higher values represent a closer\nrelevance, we select the most contributive elements at each\nrow of matrix P to aggregate focus, where P is the matrix of\nmultiplication between query and key.\nSpeciﬁcally, we split attention matrix into n chunks and\ncompute the mean value of each chunk. The next step is to set\nFIGURE 2. The most contributive elements are assigned with higher\nprobabilities after the operation of Local Adaptive Threshold selection\nand softmax function.\nVOLUME 8, 2020 213439\nZ. Leiet al.: Sparse Transformer-Based Approach for Image Captioning\nthe elements smaller than t percent of the average to negative\ninﬁnity:\nPt =\n\n\n\nPij, if Pij ≥\n∑n+w\nj∈n Pij\nw ×t\n−∞, if Pij <\n∑n+w\nj∈n Pij\nw ×t\n(2)\nwhere w is the window size and t represents the threshold\nvalue. The rationale is to maintain a local window for each\nelement in the attention matrix and set irrelevant information\nto negative inﬁnity according to the data distribution of neigh-\nbor nodes. After the operation of softmax function:\nαi,j = eai,j\n∑\nj eai,j (3)\nthose negative inﬁnity elements will become zero and thus\navoid the effect of negative noise. It should be noted that\nour proposed Local Adaptive Threshold draws on previous\nmethods and is able to ﬁlter noisy information effectively.\nB. MULTI-LEVEL REPRESENTATIONS IN ENCODER\nGiven an image, we ﬁrst extract a set of feature vectors\nV ={v1,v2,..., vk }using a R-CNN based network, where\nk is the number of feature vectors in V and vi is a vector that\nrepresents a salient object region. Instead of directly feeding\nthese vectors to the decoder, we build a multi-level image fea-\nture network consisting of low-level and high-level features\nto reﬁne their representations as illustrated in Figure 3.\nFIGURE 3. Multi-level encodings of image regions based on self-attention\nwith extra memory vectors.\nDiscriminant region feature vectors are essential for image\ncaptioning. In our paper, each image has a range of 10 to\n100 salient image regions and each image region has a 2048-d\ndimension vector after feature extraction of Faster R-CNN\n[7]. Given feature vectors V ∈ Rk×d , we feed them into\nthe attention module proposed in the previous section. The\noverall operation is shown as follows:\nVi+1 =Attention(WqVi,Key,Value), (4)\nKey =[Wk Vi,Mk ] (5)\nValue =[WvVi,Mv] (6)\nwhere i ∈{1, 2,... 6},Wq,Wk ,Wv are matrices of learnable\nweights, Mk ,Mv are persistent memory vectors and Vi+1 has\nthe same cardinality as Vi. We can infer that attentive weights\ndepend solely on the pairwise similarities between the linear\nprojections of the input feature vectors. Therefore the self-\nattention operator can be seen as a way of encoding pairwise\nrelationships between region pairs. We consider such a self-\nattention operation as a single layer and stack certain layers\nfor multi-level representations. Vectors generated by the ﬁrst\nfew layers are low-level feature representations and high-\nlevel feature representations are generated by the last few\nlayers. Intuitively speaking, different layers may contribute\nvarious effects to ﬁnal image features.\nDifferent from the original self-attention operation, the key\nand value are concatenated with extra vectors to encode a per-\nsistent memory [33]. These memory vectors are designed to\ncapture general knowledge instead of the information depend\non the context because these vectors are shared across all\nattention heads instead of relying on certain head and consid-\nered as trainable weights that can be regarded as the persistent\nmemory. These extra vectors do not depend on the input\nimage feature vectors and are designed as learnable weights.\nOur experiments have shown the extraordinary effect of those\nextra vectors where the CIDEr score is about 0.6 percent\nhigher.\nC. DECODER WITH SPARSE MULTI-HEAD\nSELF-ATTENTION\nA captioning model normally uses two LSTM layers to pro-\nduce words where the ﬁrst LSTM layer is used as an attention\nmodel and the second LSTM layer as a language model\nshown as Figure 4(a). Followed by [9], we apply multi-head\nself-attention to decoder. The input vector to the ﬁrst LSTM\nlayer at each time step consists of mean pooled image feature,\nthe previous output of the language LSTM and words:\nx1\nt =\n[\nh2\nt−1,¯v,We5t\n]\n(7)\nwhere We is a word embedding matrix for the vocabulary and\n5t is one-hot encoding of the input word at time step t.\nWith the LSTM adopted, the hidden state h1\nt and cell state\nc1\nt are modeled as:\nh1\nt ,c1\nt =LSTMCell(x1\nt ,h1\nt−1,c1\nt−1) (8)\nwhere xt is the input of the ﬁrst LSTM layer. The iteration is\nperformed in a for loop with an LSTMCell rather than exe-\ncuting automatically without a loop with an LSTM. Because\nwe need to obtain the attention scores at each decode step,\nan LSTMCell is able to generate outputs in a single time step\noperation, whereas an LSTM would iterate over all time steps\ncontinously and provide all the outputs at once. Following the\nidea of self-attention, suppose that\n\n\n\nquery =h1\nt ,\nkey =vi,\nvalue =vi,\n(9)\n213440 VOLUME 8, 2020\nZ. Leiet al.: Sparse Transformer-Based Approach for Image Captioning\nFIGURE 4. (a)The common used captioning model which consists of\nattention LSTM and language LSTM as well as attention module.\n(b)Captioning model used in our framework, we replace language LSTM\nwith GLU and adopt Local Adaptive Threshold based attention module\nshown as Figure 2.\nwhere i ∈{1, 2,... 6}, vi is the i-layer feature vector from\nthe encoder. For multi-head attention, we divide q, k, v into\n8 slices and generate the attention matrix P:\nP =qikT\ni\n√\nd\n,i ∈{1,.., 8} (10)\nthe ﬁnal output representation of self-attention can be com-\nputed as:\nheadi =Avi (11)\nwhere A is the result of select operation on matrix P shown\nas Figure 2. The concatenate operation is shown as follows:\natt =concat(headi) (12)\nIn order to make full use of each encoding layer, the ﬁnal att\noperation is deﬁned as:\natt =\nN∑\ni=1\nαi ×atti,i ∈{1, 2,... 6} (13)\nwhere αi is a sigmoid gate representing the weight of atti and\natti is the sparse self-attention operation of the i-th encoder\nlayer. We make full use of each encoding layer and fuse the\nlayers together by weighted sum in case of attending a single\nvisual input from the encoder. The intention is that we exploit\nmulti-level representations of image features and each level\nmay have different contribution to the context of the decoder.\nThe input to the language model consists of the extension\nattention operation, concatenated with the output of the ﬁrst\nLSTM layer, given by:\nx2\nt =\n[\natt,h1\nt\n]\n(14)\nA gate linear unit is adopted afterwards instead of LSTM\nbecause a gate linear unit is more light-weight and can model\ncontext information as LSTM does:\nct =f (x2\nt ) ×g(f (x2\nt )) (15)\nTABLE 1. Different split of MSCOCO and Flickr30k dataset.\nwhere f is a linear function and g represents a sigmoid func-\ntion. The conditional probabilities of words in vocabulary at\neach time step can be computed as:\np(yt |y1:t−1,V ) =softmax(Wpct ) (16)\nThe distribution over complete output sequences is calculated\nas the product of conditional distributions:\np(y1:T ) =\nT∏\nt=1\np(yt |y1:t−1) (17)\nIV. EXPERIMENTS\nA. DATASETS AND EVALUATION METRICS\nWe use MSCOCO Dataset [13] and Flickr30k [14] for our\nevaluation. MSCOCO Dataset is a very large image dataset\nfor many computer vision tasks such as image recognition,\nobject detection, instance segmentation and image caption-\ning. There are more than 300,000 images, 80 object cate-\ngories and 5 captions per image in MSCOCO Dataset. The\nstandard train split of MSCOCO contains 82783 images, with\n40504 for validation and 40775 for test. The ‘‘Karpathy’’ data\nsplit is used in our paper where 5000 images are used for\nvalidation, 5000 images for testing and the rest for training.\nThe ‘‘Karpathy’’ data split does not adopt standard test split.\nInstead it puts train split and validation split together and\nseparate 5000 images each for valdation and test. We use the\n‘‘Karpathy’’ split for ofﬂine evaluation and standard split for\nonline evaluation. Flickr30k is another dataset for automatic\nimage captioning which contains 31014 images and 158k\ncaptions provided by human annotators. It does not provide a\nﬁxed split of images for train, validation and test. Researchers\nare able to choose their own options for train, validation and\ntest split.\nThe MSCOCO caption evaluation tool including BLEU\n[35], METEOR [36], CIDEr [37] and ROUGE-L [38] metrics\nare used to evaluate our method and compare with other\nmethods. BLEU is used to measure the quality of machine-\ngenerated text while individual parts of text are compared\nwith reference text. However, BLEU heavily depends on the\nlength of generated text and achieves good scores only if\nthe generated text is short. Nonetheless, BLEU is a popu-\nlar metric because of its pioneer in automatic evaluation of\nmachine translation. In addition to comparing word segments\nto reference text, METEOR takes synonyms of words and\nstems of a sentence into consideration to make a better cal-\nibration. CIDEr is another important evaluation metric that\nadopts term frequency-inverse document frequency. It does\nnot only calculate the frequency and proportion of speciﬁc\nwords in a document, but also the proportion of documents\nVOLUME 8, 2020 213441\nZ. Leiet al.: Sparse Transformer-Based Approach for Image Captioning\nTABLE 2. Online evaluation performance on the MSCOCO test server where c5 and c40 represents that each image has 5 or 40 reference captions.\nwith speciﬁc words in all documents. ROUGE is a set of\nmetrics that measure the quality of text summary and are\ndesigned to compare word sequences, word pairs and n-grams\nwith a set of reference summaries.\nB. TRAINING DETAILS\nOur code is implemented using PyTorch. For preprocessing\ncoco captions, we map all words that occur less than 4 times\nto a special UNK token and create a vocabulary for all the\nremaining words. A pre-trained Faster-RCNN [39] model\non ImageNet [40] and Visual Genome [41] is employed\nto extract bottom-up [7] feature vectors of images. For the\nMSCOCO dataset, the dimension of each feature vector is\nv ∈ Rk×d where k is a variable and d is 2048. For the\nFlickr30k dataset, the dimension of each feature vector is\nsimilar v ∈Rk×d where k is ﬁxed 36 and d is 2048. Moreover\nwe project it to a new dimension of d = 1024 which is\nalso the hidden size of the LSTM in the decoder. As for the\ntraining process, the objective is to maximize the sum of the\nlog likelihood of each generated word:\nθ =arg max\nN∑\nt=1\nlogp(wt |V ,w0,... wt−1;θ) (18)\nwhere θ represents the parameters to be learned and wt rep-\nresents generated words at time step t. We train our model\nunder cross entropy loss for 25 epochs with a mini batch size\nof 10 using an Adam [42] optimizer.\nLXE (θ) =−\nT∑\nt=1\nlog(pθ(y∗\nt |y∗\n1:t−1)) (19)\nWe increase the scheduled sampling probability [43] by\n0.05 every 5 epochs and optimize the CIDEr score with SCST\n[34] for another 15 epochs to directly optimize the non-\ndifferentiable metrics. The language model can be regarded\nas the agent to interact with the environment of images and\nwords. The action is the process of predicting next word. The\nagent updates its state at the end of each action and gets\nthe reward after meeting the end-of-sentence token where\nthe state represents the hidden state and the reward indi-\ncates CIDEr score. The following formulation comes from\nSCST [34]:\nLRL (θ) =−E y1:T pθ [r(y1:T )] (20)\nwhere the reward r(·) uses the score of CIDEr. The gradients\ncan be approximated:\n∇θLRL (θ) ≈−(r (ys\n1:T ) −r(ˆy1:T ))∇θ log pθ(ys\n1:T ) (21)\nwhere ys is a result sampled from a probability distribution,\nwhile ˆy indicates a result of greedy decoding.\nC. BASELINE METHODS\nTo evaluate the effectiveness of our model in this study,\nwe compare with several strong competitors in terms of four\nevaluation metrics on the stage of reinforcement training pro-\ncess as shown in Figure 5 and Figure 6. For fair comparison,\nall the competitive models are trained under cross entropy\nloss for 25 epochs and another 15 epochs for reinforcement\ntraining. A brief introduction of those baseline methods is as\nfollows:\nFC model: This model was proposed in SCST [34]. The\nFC model ﬁrst encodes the input image using a deep CNN\nand embeds it by a linear projection. Words are fed back\ninto the LSTM and are represented using one hot vector\nthat is embedded with a linear embedding, which has the\nsame output dimension as the linear projection in the feature\nextractor. This method is quite simple compared to other\nencoder-decoder frameworks.\nShow, Attend and Tell model:Rather than using a ﬁxed\nrepresentation of the image, this attention model [6] dynam-\nically focuses on a speciﬁc region of the image at each time\nstep to boost the performance in the deep encoder-decoder\nframework.\nBottom-Up and Top-Down model:This up-down model\nconsists of bottom-up and top-down attention mechanism,\nwhich enables the attention scores to be computed at the\nlevels of salient image regions [7]. The bottom-up attention\nmechanism determines the image regions to be focused, while\nthe top-down attention mechanism decides the weights of\ndifferent image regions.\nTransformer model:This model [8] consists of components\nincluding multi-head self-attention, positional encoding and\nfeed forward. In view of the excellent performance on num-\nbers of natural language processing tasks, we compare our\nmodiﬁed model to this implementation.\nAAT model: This model is proposed in AAT [20] and\nimplements an arbitrary mapping between image regions\nand words across all attention steps. Traditional attention\n213442 VOLUME 8, 2020\nZ. Leiet al.: Sparse Transformer-Based Approach for Image Captioning\nFIGURE 5. Evaluation results of image captioning on validation split of MSCOCO dataset along the reinforcement training process compared to baseline\nmethods.\nFIGURE 6. Evaluation results of image captioning on validation split of Flickr30k dataset along the reinforcement training process compared to baseline\nmethods.\noperations only compute weights in a single step while AAT\nmodel uses a threshold value to compute weights across a\nsingle step.\nAoANet model: This model is proposed in AoANet [9]\nwhere the results of self-attention and initial query are con-\ncatenated together and fed into two linear layers to obtain\ninformation vectors by multiplication with a sigmoid gate.\nThe ﬁnal result is used as an alternative to the original self-\nattention operation.\nThe comparison suggests that our method performs better\nthan the baseline methods on the reinforcement training pro-\ncess. In particular, our method outperforms AoANet, which\nis considered to be one of the best models in recent years.\nD. QUANTITATIVE RESULTS\nIn Table 3, we compare our methods with several state-of-the-\nart methods on MSCOCO dataset. The NIC [2] method was\nthe ﬁrst work proposed to apply deep encoder-decoder frame-\nwork in image captioning. The SCST [34] method introduced\nthe notion of reinforcement learning based training which\nis the key idea of improve CIDEr score. The Up-Down [7]\nmethod proposed a combined bottom-up and top-down atten-\ntion mechanism that enables attention to be calculated at\nthe level of salient image regions. The AoANet [9] method\nintroduced an extension of the attention operator in which\nthe ﬁnal attended information is weighted by a gate. Our\nwork is developed on top of these methods. In particular, our\nwork adopts feature extraction and reinforcement learning\nfrom these methods. The results in Table 3 show that our\nmethod is 15.2% higher than SCST [34] in terms of CIDEr\nTABLE 3. The performance results of various methods on MSCOCO\nKarpathy split.\nscore. Compared with Up-Down [7], our model is 9.1%\nhigher. AoANet has been considered to be one of the most\ncompetitive model in recent years, but its Bleu score is still\nlower than ours. These results indicate that our model is better\nthan these methods in some of the evaluation metric.\nWe further evaluate our method on the Flickr30k dataset\nwhich is another popular dataset for early image captioning\ntasks. Recent research rarely adopts it because the scale of\nFlickr30k dataset is much smaller than the MSCOCO dataset.\nAs shown in table 4, our work slightly outperforms other\nmethods in terms of Meteor score, but most metrics are lower\nthan that of VS-LSTM [51], the reason may be that our model\nfocus less on semantic information than VS-LSTM.\nMoreover we evaluate our model on the ofﬁcial website\nof the MSCOCO team as shown in table 2, the evaluation\nis performed on the 40775 test split and 40504 validation\nVOLUME 8, 2020 213443\nZ. Leiet al.: Sparse Transformer-Based Approach for Image Captioning\nFIGURE 7. Examples of captions generated by our model on MSCOCO dataset, the caption is able to contain more nouns and verbs than AoANet.\nFIGURE 8. Examples of captions generated by our model on Flickr30k dataset, the caption is able to contain more objects and adjective words\nthan AoANet.\nTABLE 4. The performances of various methods on Flickr30k test split.\nsplit. There are two evaluation settings where c5 represents\nthat each image has ﬁve reference captions while c40 repre-\nsents that each image has forty reference captions. Compared\nwith the top-performing methods over the past few years,\nour model outperforms SGAE [22] slightly in some of the\nevaluation metrics such as Bleu-4 [35], Meteor [36], Rouge\n[38] and achieves a promising performance.\nE. ABLATION STUDY\nTo quantify the impact of each design of our proposed mod-\nule, we compare our method against a set of models without\nsome settings. As shown in Table 5 and 6, we compare the\neffect of memory vectors in the encoder and the effect of\ndifferent attention modules in the decoder. The ‘‘base’’ model\ndoes not have a memory vectors module in its encoder and\nadopts original Transformer as attention mechanism in the\ndecoder. We can infer that sparse attention is able to improve\nscores and our proposed Local Adaptive Threshold performs\nbetter than top-k strategy. On the encoder side, the results\nalso verify the efﬁciency of memory vectors. Obviously,\nthe experiments conducted on the two datasets demonstrate\nthe superiority of our proposed Local Adaptive Threshold\nin the decoder. Furthermore we discover that for MSCOCO\ndataset, the optimal value of w and t is 4 and 0.8, for Flickr30k\ndataset, the optimal value of w and t is 4 and 0.16 after\nextensive comparative experiments.\nFor more intuitive display, we show some examples of\nimage captions generated by our model in Figure 7 and\nFigure 8. As shown in the ﬁrst image and forth image\nin Figure 7, the captions include ‘‘a book shelf’’, ‘‘tray’’ that\nAoANet model does not cover, indicating that our method\ncan generate captions that contain more objects in an image.\nMorevoer, we infer that our model can generate more verb\nthan AoANet model, e.g. ‘‘blowing out’’ in the second image.\n213444 VOLUME 8, 2020\nZ. Leiet al.: Sparse Transformer-Based Approach for Image Captioning\nTABLE 5. Settings and results of ablation studies on MSCOCO dataset.\nTABLE 6. Settings and results of ablation studies on Flickr30k dataset.\nFor the Flickr30k dataset, it is more likely for our method to\ngenerate adjective words like ‘‘pink’’, ‘‘straw’’ and more con-\ncrete nouns, e.g. ‘‘children’’ instead of ‘‘people’’ compared\nto AoANet model. In a word, our method can generate more\ncontent related and grammatically correct sentences.\nIt is clear that the sentence generated by our method\nis able to visually describe the basic content of a picture.\nCompared to human-annotated ground truths, their sentences\ncontain more detailed information including rich adjectives\ndescribing the objects and objects that does not appear at\ngenerated captions. Moreover, our generated sentences typi-\ncally contain expressions like ‘‘a man’’ or ‘‘a woman’’, while\nhuman-annotated ground truths typically contain ‘‘a young\ngirl’’ which is more vivid. We can observe that pre-trained\nimage features are underutilized when generating sentences\nand captions can not produce rich information as human-\nannotated ground truths do. Future research could overcome\nthose drawbacks.\nV. CONCLUSION\nIn this article, we introduce a novel model called Local\nAdaptive Threshold which can explicit sparse Transformer\neffectively. Local Adaptive Threshold will not hurt long term\ndependency and is determined by the distribution of neighbor\nnodes. It is able to make the attention in original Transformer\nmore concentrated on the most contributive components.\nWe integrate Local Adaptive Threshold into self-attention\nand acts as attention mechanism in the decoder which help\nthe model generate more accurate word. Moreover, com-\nprehensive comparisons with state-of-the-art methods and\nadequate ablation studies demonstrate the effectiveness of our\nstrategies. In the future, we will develop novel frameworks\non image captioning to further advance our research such as\nunsupervised learning. The development of advanced image\ncaptioning method is vital for deep image understanding.\nACKNOWLEDGMENT\nThe authors would like to thank the High Performance Com-\nputing Center of Shanghai University for providing the com-\nputing resources and technical support.\nREFERENCES\n[1] M. Z. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga, ‘‘A comprehen-\nsive survey of deep learning for image captioning,’’ ACM Comput. Surv.,\nvol. 51, no. 6, pp. 1–36, Feb. 2019.\n[2] O. Vinyals, A. Toshev, S. Bengio, and D. Erhan, ‘‘Show and tell: A\nneural image caption generator,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., Jun. 2015, pp. 3156–3164.\n[3] A. Karpathy and L. Fei-Fei, ‘‘Deep visual-semantic alignments for gen-\nerating image descriptions,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., Jun. 2015, pp. 3128–3137.\n[4] K. He, X. Zhang, S. Ren, and J. Sun, ‘‘Deep residual learning for\nimage recognition,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2016, pp. 770–778.\n[5] S. Hochreiter and J. Schmidhuber, ‘‘Long short-term memory,’’ Neural\nComput., vol. 9, no. 8, pp. 1735–1780, 1997.\n[6] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,\nand Y. Bengio, ‘‘Show, attend and tell: Neural image caption gener-\nation with visual attention,’’ in Proc. Int. Conf. Mach. Learn., 2015,\npp. 2048–2057.\n[7] P. Anderson, X. He, C. Buehler, D. Teney, M. Johnson, S. Gould, and\nL. Zhang, ‘‘Bottom-up and top-down attention for image captioning and\nvisual question answering,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., Jun. 2018, pp. 6077–6086.\n[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nL. Kaiser, and I. Polosukhin, ‘‘Attention is all you need,’’ 2017,\narXiv:1706.03762. [Online]. Available: http://arxiv.org/abs/1706.03762\n[9] L. Huang, W. Wang, J. Chen, and X.-Y. Wei, ‘‘Attention on attention for\nimage captioning,’’ in Proc. IEEE Int. Conf. Comput. Vis., Oct. 2019,\npp. 4634–4643.\n[10] M. Cornia, M. Stefanini, L. Baraldi, and R. Cucchiara, ‘‘Meshed-memory\ntransformer for image captioning,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., Jun. 2020, pp. 10578–10587.\n[11] R. Child, S. Gray, A. Radford, and I. Sutskever, ‘‘Generating long\nsequences with sparse transformers,’’ 2019, arXiv:1904.10509. [Online].\nAvailable: http://arxiv.org/abs/1904.10509\n[12] G. Zhao, J. Lin, Z. Zhang, X. Ren, Q. Su, and X. Sun, ‘‘Explicit sparse\ntransformer: Concentrated attention through explicit selection,’’ 2019,\narXiv:1912.11637. [Online]. Available: http://arxiv.org/abs/1912.11637\n[13] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays,\nP. Perona, D. Ramanan, C. Lawrence Zitnick, and P. Dollár, ‘‘Microsoft\nCOCO: Common objects in context,’’ 2014, arXiv:1405.0312. [Online].\nAvailable: http://arxiv.org/abs/1405.0312\n[14] B. A. Plummer, L. Wang, C. M. Cervantes, J. C. Caicedo, J. Hockenmaier,\nand S. Lazebnik, ‘‘Flickr30k entities: Collecting region-to-phrase corre-\nspondences for richer image-to-sentence models,’’ in Proc. IEEE Int. Conf.\nComput. Vis., Dec. 2015, pp. 2641–2649.\n[15] J. Lu, J. Yang, D. Batra, and D. Parikh, ‘‘Neural baby talk,’’ in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 7219–7228.\n[16] M. Cornia, L. Baraldi, and R. Cucchiara, ‘‘Show, control and tell: A frame-\nwork for generating controllable and grounded captions,’’ in Proc. IEEE\nConf. Comput. Vis. Pattern Recognit., Jun. 2019, pp. 8307–8316.\n[17] Y. Li, T. Yao, Y. Pan, H. Chao, and T. Mei, ‘‘Pointing novel objects in\nimage captioning,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2019, pp. 12497–12506.\n[18] L. Chen, H. Zhang, J. Xiao, L. Nie, J. Shao, W. Liu, and T.-S. Chua, ‘‘SCA-\nCNN: Spatial and channel-wise attention in convolutional networks for\nimage captioning,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJul. 2017, pp. 5659–5667.\n[19] J. Lu, C. Xiong, D. Parikh, and R. Socher, ‘‘Knowing when to look:\nAdaptive attention via a visual sentinel for image captioning,’’ in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit., Jul. 2017, pp. 375–383.\n[20] L. Huang, W. Wang, Y. Xia, and J. Chen, ‘‘Adaptively aligned image\ncaptioning via adaptive attention time,’’ in Proc. Adv. Neural Inf. Process.\nSyst., 2019, pp. 8940–8949.\n[21] T. N. Kipf and M. Welling, ‘‘Semi-supervised classiﬁcation with graph\nconvolutional networks,’’ 2016, arXiv:1609.02907. [Online]. Available:\nhttp://arxiv.org/abs/1609.02907\n[22] X. Yang, K. Tang, H. Zhang, and J. Cai, ‘‘Auto-encoding scene graphs for\nimage captioning,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2019, pp. 10685–10694.\n[23] T. Yao, Y. Pan, Y. Li, and T. Mei, ‘‘Exploring visual relationship for image\ncaptioning,’’ in Proc. Eur. Conf. Comput. Vis., 2018, pp. 684–699.\nVOLUME 8, 2020 213445\nZ. Leiet al.: Sparse Transformer-Based Approach for Image Captioning\n[24] S. Chen, Q. Jin, P. Wang, and Q. Wu, ‘‘Say as you wish: Fine-grained\ncontrol of image caption generation with abstract scene graphs,’’ in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2020, pp. 9962–9971.\n[25] S. Herdade, A. Kappeler, K. Boakye, and J. Soares, ‘‘Image captioning:\nTransforming objects into words,’’ in Proc. Adv. Neural Inf. Process. Syst.,\n2019, pp. 11135–11145.\n[26] G. Li, L. Zhu, P. Liu, and Y. Yang, ‘‘Entangled transformer for image cap-\ntioning,’’ inProc. IEEE Int. Conf. Comput. Vis., Oct. 2019, pp. 8928–8937.\n[27] F. Liu, X. Ren, Y. Liu, K. Lei, and X. Sun, ‘‘Exploring and distilling cross-\nmodal information for image captioning,’’ in Proc. 28th Int. Joint Conf.\nArtif. Intell., Aug. 2019, pp. 10–16.\n[28] Y. Pan, T. Yao, Y. Li, and T. Mei, ‘‘X-linear attention networks for\nimage captioning,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2020, pp. 10971–10980.\n[29] L. Ke, W. Pei, R. Li, X. Shen, and Y.-W. Tai, ‘‘Reﬂective decoding network\nfor image captioning,’’ in Proc. IEEE Int. Conf. Comput. Vis., Oct. 2019,\npp. 8888–8897.\n[30] Y. Qin, J. Du, Y. Zhang, and H. Lu, ‘‘Look back and predict forward in\nimage captioning,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2019, pp. 8367–8375.\n[31] Y. Zheng, Y. Li, and S. Wang, ‘‘Intention oriented image captions with\nguiding objects,’’ in Proc. IEEE Conf. Comput. Vis. Pattern Recognit.,\nJun. 2019, pp. 8395–8404.\n[32] A. Deshpande, J. Aneja, L. Wang, A. G. Schwing, and D. Forsyth, ‘‘Fast,\ndiverse and accurate image captioning guided by part-of-speech,’’ in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2019, pp. 10695–10704.\n[33] S. Sukhbaatar, E. Grave, G. Lample, H. Jegou, and A. Joulin, ‘‘Augmenting\nself-attention with persistent memory,’’ 2019, arXiv:1907.01470. [Online].\nAvailable: http://arxiv.org/abs/1907.01470\n[34] S. J. Rennie, E. Marcheret, Y. Mroueh, J. Ross, and V. Goel, ‘‘Self-critical\nsequence training for image captioning,’’ in Proc. IEEE Conf. Comput. Vis.\nPattern Recognit., Jul. 2017, pp. 7008–7024.\n[35] T. W. K. Papineni, S. Roukos, and W.-J. Zhu, ‘‘Bleu: A method for\nautomatic evaluation of machine translation,’’ in Proc. 40th Annu. Meeting\nAssoc. Comput. Linguistics, 2002, pp. 311–318.\n[36] M. D. A. Lavie, ‘‘Meteor universal: Language speciﬁc translation evalua-\ntion for any target language,’’ in Proc. Assoc. Comput. Linguistics, 2014,\npp. 376–380.\n[37] R. Vedantam, C. L. Zitnick, and D. Parikh, ‘‘CIDEr: Consensus-based\nimage description evaluation,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., Jun. 2015, pp. 4566–4575.\n[38] C.-Y. Lin, ‘‘Rouge: A package for automatic evaluation of summaries,’’\nin Proc. Workshop Text Summarization Branches Out, Barcelona, Spain,\nvol. 8, 2004, pp. 74–81.\n[39] S. Ren, K. He, R. Girshick, and J. Sun, ‘‘Faster R-CNN: Towards real-time\nobject detection with region proposal networks,’’ in Proc. Adv. Neural Inf.\nProcess. Syst., 2015, pp. 91–99.\n[40] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, ‘‘ImageNet:\nA large-scale hierarchical image database,’’ in Proc. IEEE Conf. Comput.\nVis. Pattern Recognit., Jun. 2009, pp. 248–255.\n[41] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen,\nY. Kalantidis, L.-J. Li, D. A. Shamma, M. S. Bernstein, and L. Fei-Fei,\n‘‘Visual genome: Connecting language and vision using crowdsourced\ndense image annotations,’’ Int. J. Comput. Vis., vol. 123, no. 1, pp. 32–73,\nMay 2017.\n[42] D. P. Kingma and J. Ba, ‘‘Adam: A method for stochastic optimization,’’\nin Proc. Int. Conf. Learn. Represent., 2015, pp. 1–15.\n[43] S. Bengio, O. Vinyals, N. Jaitly, and N. Shazeer, ‘‘Scheduled sampling for\nsequence prediction with recurrent neural networks,’’ in Proc. Adv. Neural\nInf. Process. Syst., 2015, pp. 1171–1179.\n[44] T. Yao, Y. Pan, Y. Li, Z. Qiu, and T. Mei, ‘‘Boosting image cap-\ntioning with attributes,’’ 2016, arXiv:1611.01646. [Online]. Available:\nhttp://arxiv.org/abs/1611.01646\n[45] W. Jiang, L. Ma, Y.-G. Jiang, W. Liu, and T. Zhang, ‘‘Recurrent fusion\nnetwork for image captioning,’’ in Proc. Eur. Conf. Comput. Vis., 2018,\npp. 499–515.\n[46] J. Gu, J. Cai, G. Wang, and T. Chen, ‘‘Stack-captioning: Coarse-to-ﬁne\nlearning for image captioning,’’ in Proc. 32nd AAAI Conf. Artif. Intell.,\n2018, pp. 1–8.\n[47] D. Liu, Z.-J. Zha, H. Zhang, Y. Zhang, and F. Wu, ‘‘Context-aware visual\npolicy network for sequence-level image captioning,’’ in Proc. ACM Mul-\ntimedia Conf. Multimedia Conf., 2018, pp. 1416–1424.\n[48] X. Chen and C. L. Zitnick, ‘‘Mind’s eye: A recurrent visual representation\nfor image caption generation,’’ in Proc. IEEE Conf. Comput. Vis. Pattern\nRecognit., Jun. 2015, pp. 2422–2431.\n[49] J. Donahue, L. A. Hendricks, S. Guadarrama, M. Rohrbach,\nS. Venugopalan, T. Darrell, and K. Saenko, ‘‘Long-term recurrent\nconvolutional networks for visual recognition and description,’’ in Proc.\nIEEE Conf. Comput. Vis. Pattern Recognit., Jun. 2015, pp. 2625–2634.\n[50] J. Mao, W. Xu, Y. Yang, J. Wang, Z. Huang, and A. Yuille, ‘‘Deep\ncaptioning with multimodal recurrent neural networks (m-RNN),’’ 2014,\narXiv:1412.6632. [Online]. Available: http://arxiv.org/abs/1412.6632\n[51] N. Li and Z. Chen, ‘‘Image captioning with visual-semantic LSTM,’’ in\nProc. 27th Int. Joint Conf. Artif. Intell., 2018, pp. 793–799.\nZHOU LEI received the Ph.D. degree from\nthe Institute of Computing Technology, Chinese\nAcademy of Sciences, China, in 1999. He became\na Researcher with Louisiana State University\n(LSU), USA, from 2004 to 2009. His current\nresearch interests include cloud computing, big\ndata, and distributed systems.\nCONGCONG ZHOU is currently pursuing the\nmaster’s degree with the School of Computer\nEngineering and Science, Shanghai University\n(SHU). His research interests include image pro-\ncessing and deep learning.\nSHENGBO CHEN received the Ph.D. degree in\ncomputer application technology from the School\nof Computer Engineering and Science, Shanghai\nUniversity (SHU), Shanghai, China, in 2008. His\ncurrent research interests include big data, cloud\ncomputing, and software engineering.\nYIYONG HUANGis currently pursuing the mas-\nter’s degree with the School of Computer Engi-\nneering and Science, Shanghai University (SHU).\nHis current research interests include action recog-\nnition, video captioning, and video synopsis\nXIANRUI LIU is currently pursuing the mas-\nter’s degree with the School of Computer Engi-\nneering and Science, Shanghai University (SHU).\nHis current research interests include video syn-\nopsis, video understanding, big data, and cloud\ncomputing.\n213446 VOLUME 8, 2020",
  "topic": "Closed captioning",
  "concepts": [
    {
      "name": "Closed captioning",
      "score": 0.9531105756759644
    },
    {
      "name": "Computer science",
      "score": 0.8711569309234619
    },
    {
      "name": "Encoder",
      "score": 0.6034188866615295
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6007932424545288
    },
    {
      "name": "Transformer",
      "score": 0.5520899891853333
    },
    {
      "name": "Pairwise comparison",
      "score": 0.48718613386154175
    },
    {
      "name": "Image (mathematics)",
      "score": 0.4581088721752167
    },
    {
      "name": "Source code",
      "score": 0.4497048556804657
    },
    {
      "name": "Context (archaeology)",
      "score": 0.44237253069877625
    },
    {
      "name": "Computer vision",
      "score": 0.3934519290924072
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3713645935058594
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}