{
  "title": "Pose Recognition with Cascade Transformers",
  "url": "https://openalex.org/W3153807375",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100725160",
      "name": "Ke Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100374850",
      "name": "Shijie Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100368903",
      "name": "Xiang Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5103166791",
      "name": "Yifan Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102717169",
      "name": "Weijian Xu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5001760915",
      "name": "Zhuowen Tu",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2578797046",
    "https://openalex.org/W2964221239",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2889332130",
    "https://openalex.org/W3034742259",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2964721919",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W3034750257",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W607748843",
    "https://openalex.org/W2307770531",
    "https://openalex.org/W2963402313",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3034399482",
    "https://openalex.org/W2963598138",
    "https://openalex.org/W2962773068",
    "https://openalex.org/W2559085405",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2993728126",
    "https://openalex.org/W2962820842",
    "https://openalex.org/W2113325037",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2555751471",
    "https://openalex.org/W2742737904",
    "https://openalex.org/W3109769043",
    "https://openalex.org/W2988211500",
    "https://openalex.org/W2080873731",
    "https://openalex.org/W2972498556",
    "https://openalex.org/W2964304707",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W603908379",
    "https://openalex.org/W1537698211",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2136000821",
    "https://openalex.org/W2935837427",
    "https://openalex.org/W2983035142"
  ],
  "abstract": "In this paper, we present a regression-based pose recognition method using cascade Transformers. One way to categorize the existing approaches in this domain is to separate them into 1). heatmap-based and 2). regression-based. In general, heatmap-based methods achieve higher accuracy but are subject to various heuristic designs (not end-to-end mostly), whereas regression-based approaches attain relatively lower accuracy but they have less intermediate non-differentiable steps. Here we utilize the encoder-decoder structure in Transformers to perform regression-based person and keypoint detection that is general-purpose and requires less heuristic design compared with the existing approaches. We demonstrate the keypoint hypothesis (query) refinement process across different self-attention layers to reveal the recursive self-attention mechanism in Transformers. In the experiments, we report competitive results for pose recognition when compared with the competing regression-based methods.",
  "full_text": "Pose Recognition with Cascade Transformers\nKe Li* 1, Shijie Wang* 2, Xiang Zhang* 2, Yifan Xu3, Weijian Xu3, Zhuowen Tu3\n1University of Chinese Academy of Sciences, Beijing, China\n2Tsinghua University, Beijing, China\n3University of California San Diego, San Diego, USA\n{keliictcas, wang98thu, zx1239856}@gmail.com, {yix081, wex041, ztu}@ucsd.edu\nAbstract\nIn this paper, we present a regression-based pose recog-\nnition method using cascade Transformers. One way to cat-\negorize the existing approaches in this domain is to sep-\narate them into 1). heatmap-based and 2). regression-\nbased. In general, heatmap-based methods achieve higher\naccuracy but are subject to various heuristic designs (not\nend-to-end mostly), whereas regression-based approaches\nattain relatively lower accuracy but they have less interme-\ndiate non-differentiable steps. Here we utilize the encoder-\ndecoder structure in Transformers to perform regression-\nbased person and keypoint detection that is general-purpose\nand requires less heuristic design compared with the ex-\nisting approaches. We demonstrate the keypoint hypothe-\nsis (query) reÔ¨Ånement process across different self-attention\nlayers to reveal the recursive self-attention mechanism in\nTransformers. In the experiments, we report competitive re-\nsults for pose recognition when compared with the compet-\ning regression-based methods.\n1. Introduction\nWe tackle the 2D human pose recognition problem\n[19, 1, 32, 22] where keypoints (e.g. head, shoulders, knees,\netc.) for multiple people in an RGB image are to be detected\nand localized. This is an important problem in computer vi-\nsion that can be adopted in a variety of downstream tasks in-\ncluding tracking, security, animation, human-computer in-\nteraction, computer games, and robotics.\nThere has been a steady progress in 2D human pose\nrecognition [1, 32, 36, 22, 17, 2, 25, 29, 24, 6, 5, 28, 41, 23]\nwith systems becoming increasingly practical without a\nstrong constraint ( e.g. present multiple people of varying\nsize). However, pose recognition is a challenging problem\n* indicates equal contribution.\nCode: https://github.com/mlpc-ucsd/PRTR.\nWork performed during internships of K. Li, S.Wang, and X. Zhang\nwith UC San Diego.\nCNN\nTransformer \nEncoder\nLearned \nQueries\n1st decoder \nlayer\n2nd decoder \nlayer\n3rd decoder \nlayer ‚óè‚óè‚óè\n‚óè‚óè‚óè\nFigure 1: Illustration of the gradual reÔ¨Ånement for the\nkeypoints across different Transformer decoder layers.\nThrough the decoding process, PRTR predicts keypoints\nwith increasing conÔ¨Ådence and decreasing spatial deviation\nto ground truth, transforming image-ignorant queries to Ô¨Å-\nnal predictions.\nthat remains unsolved. The difÔ¨Åculty lies in various aspects\nsuch as large pose/shape variation, inter-person and self oc-\nclusion, large appearance variation, and background clutter.\nFor multiple people in an input image [19], the task of\npose recognition is to localize the human keypoints (17 in\nthe experiments) for the individual persons. This can be\nachieved by a two-stage process in which individual per-\nsons are detected Ô¨Årst, followed by keypoint detection from\nthe detected image region/patch; this is called a top-down\nprocess [28]. An alternative strategy is called a bottom-up\nprocess where human keypoints are detected directly from\nthe image without an explicit object detection stage [6]. A\ndiscussion about the top-down and bottom-up approaches\ncan be found in [6].\nAnother way to divide the existing literature in pose\nrecognition is based on the choice of using heatmap or\nregression. Heatmap-based approaches [37, 28] perform\ndense keypoint detection followed by subsequent processes\n1\narXiv:2104.06976v1  [cs.CV]  14 Apr 2021\nfor clustering and grouping; they deliver strong perfor-\nmance but are also subject to many heuristic designs that are\nmostly not end-to-end learnable. Regression based methods\n[29, 41, 35] perform regression for the keypoints directly\nwhich have less intermediate stages and speciÔ¨Åcations.\nRegression-based methods typically perform worse than\nheatmap-based ones, but can be made end-to-end and read-\nily integrated with the other downstream tasks. Reasons for\nthe existence of both heatmap-based and regression-based\nmethods are present. Heatmap-based methods are adopted\nwhen the accuracy is the priority whereas regression-based\napproaches can be considered as a convenient plug-and-play\nmodule.\nGenerally, heatmap-based methods adopt handcrafted\nor heuristic pre/post-processing to encode ground truth to\nheatmaps and decode heatmaps to predict keypoints. These\nmethods introduce design challenges and biases, making\nthem sub-optimal. They are hard to update and adapt as\nwell. In detail, SimpleBaseline [37] and HRNet [28] adopt\nthe standard coordinate decoding method designed empiri-\ncally according to model performance in [22], reÔ¨Åning the\ncoordinates 0.25 time from the maximum activation to the\nsecond maximum empirically in the heatmap. DARK [40]\npresents Taylor-expansion based coordinate decoding and\nunbiased sub-pixel centered coordinate encoding. UDP [15]\neven discovered a considerable accuracy decrease when us-\ning one-pixel Ô¨Çip shift in heatmap-based paradigms. For\ngeneral-purpose regression methods, we aim at removing\nunnecessary designs by making the training objective and\ntarget output direct and transparent. Coordinates should be\noutput directly and the loss be calculated with predictions\nand ground truth coordinates straightforward.\nBearing this in mind, we present a top-down regression-\nbased 2D human pose recognition method using cascade\nTransformers consisting of a person detection Transformer\nand a keypoint detection Transformer. Two alternatives\nhave been developed, one being a two-stage process (shown\nin Figure 2) with the two Transformers learned sequen-\ntially and the other being a sequential process (shown in\nFigure 3) with the two transfomers learned jointly in an\nend-to-end fashion. We name our method Pose Regression\nTRansformers (PRTR). We apply multi-scale features in the\nkeypoint detection Transformer. Visualization for the key-\npoint queries across different attention layers in the decoder\nis given to illustrate the internal detection process. PRTR\nis a general-purpose approach for keypoint regression and\nwe show competitive results in pose recognition when com-\npared with the existing regression-based methods in the lit-\nerature. The contributions of our work include:\n‚Ä¢ We propose a regression-based human pose recognition\nmethod by building cascade Transformers, based on a\ngeneral-purpose object detector, end-to-end object de-\ntection Transformer (DETR) [3]. Our method, named\npose recognition Transformer (PRTR), enjoys the tok-\nenized representation in Transformers with layers of self-\nattention to capture the joint spatial and appearance mod-\neling for the keypoints.\n‚Ä¢ Two types of cascade Transformers have been developed:\n1). a two-stage one with the second Transformer tak-\ning image patches detected from the Ô¨Årst Transformer, as\nshown in Figure 2; and 2). a sequential one using spatial\nTransformer network (STN) [16] to create an end-to-end\nframework, shown in Figure 3.\n‚Ä¢ We visualize the distribution of keypoint queries in var-\nious aspects to unfold the internal process of the Trans-\nformer for the gradual reÔ¨Ånement of the detection.\nOn the COCO 2D human pose recognition dataset [19],\ncompetitive results have been observed when compared\nwith the regression-based methods.\n2. Related Work\nGiven an image I, the goal of pose recognition is to pre-\ndict a possibly empty set of persons, {Pi}N\ni=1, where N is\nthe number of persons in the image. For each person, we\nneed to predict its bounding box position, bi, as well as its\nskeleton coordinates, si = {(xj,yj)}J\nj=1, where J is the\nnumber of joints pre-deÔ¨Åned in each dataset.\nWe discuss related work from several aspects. The\nÔ¨Åeld of human pose regression has witnessed a continuing\nprogress [1, 32, 36, 22, 17, 2, 25, 29, 24, 6, 5, 28, 41, 23], in\nparticular with the advancing of the deep learning technolo-\ngies [18, 12, 14]. One notable development in pose recogni-\ntion is the creation of the HRNet family model [28, 6] which\nis itself about a new convolutional neural network (CNN)\narchitecture targeting the modeling of high-resolution fea-\nture responses. HRNet [28] has shown its particular advan-\ntage in advancing the state-of-the-art for 2D human pose\nrecognition/estimation.\nHeatmap-based approaches include [2, 13, 25, 21, 17,\n24, 6, 5, 37, 28, 40, 39, 30] where various techniques have\nbeen developed to perform multi-class keypoint classiÔ¨Åca-\ntion. The classiÔ¨Åers produce dense heatmaps (classiÔ¨Åcation\nmap), followed by clustering and grouping processes. On\none hand, heatmap-based methods leverage Ô¨Åne-grained de-\ntection for the keypoints by densely scanning all the pixels;\non the other hand, heatmaps create a disconnection from the\noverall estimation of the keypoints, making the intermedi-\nate clustering and grouping process not directly integrable\nto be end-to-end learning frameworks.\nRegression-based methods [4, 41, 23, 35, 29] aim to di-\nrectly approach keypoint detection with a direct loss mini-\nmization between predicted and ground truth coordinates,\nhence, they can be more easily integrated into an end-\nto-end learning framework. However, holistic regression\ncan be intrinsically more difÔ¨Åcult to optimize due to the\n2\nCrop Transformer EncoderSelf-AttentionFeed-ForwardSelf-AttentionFeed-Forward\nTransformer DecoderSelf-AttentionCross-AttentionFeed-ForwardSelf-AttentionCross-AttentionFeed-Forward‚Ä¶ Person ClassifierBounding Box Regressor\nSet of Person Features\nTransformer EncoderSelf-AttentionFeed-ForwardSelf-AttentionFeed-Forward‚Ä¶ Transformer DecoderSelf-AttentionCross-AttentionFeed-ForwardSelf-AttentionCross-AttentionFeed-Forward‚Ä¶ Keypoint ClassifierCoordinate Regressor\nSet of KeypointFeatures √óN people\n‚Ñé!\"#(%)\n‚Ñé'(!!(%)\n‚Ñé()*!(%)\n‚Ä¶\n\"ùë¶()*!\n\"ùë¶!\"#\n\"ùë¶'(!!\nBackbone\nPositional Encoding\n\"ùë¶+,(-‚Ñé+,(-(%)\n‚Ä¶ ‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\nPositional Encoding\nBackbone\n‚Ä¶\nFigure 2: The architecture of Pose Recognition with TRansformer (PRTR), two-stage variant. First, using whole-picture image feature\nand absolute positional encoding, a person-detection Transformer detects people in the image with a set of learned person queries. After\nÔ¨Åltering background queries, we crop the original image with predicted boxes. Cropped images are fed into a keypoint-detection Trans-\nformer, together with positional encoding relative to corresponding bounding boxes. Finally, we read out J keypoints from a larger set of\nkeypoint queries by Hungarian algorithm. The keypoint-detection Transformer processes all the non-background keypoint proposals in a\nvectorized way. h(0) denotes hypotheses (queries), the feature vectors to be reÔ¨Åned to Ô¨Ånal predictions, ÀÜy , through Transformer decoder.\nhigh-precision needed by pose recognition. Furthermore,\nregression-based approaches typically have a recursive pro-\ncedure [9] that skips a large number of candidate locations,\ncreating a performance gap with the heatmap-based meth-\nods. Our work follows the line of regressive pose estima-\ntion, and formulates the process of step-by-step regression\n[9, 4] implicitly in a layered Transformer way.\nTransformers and self-attention The attention mech-\nanism [38, 33, 8] has greatly advanced the Ô¨Åeld of rep-\nresentation learning in machine learning. The introduc-\ntion of Transformers [33] to object detection gives another\nleap-forward in building end-to-end object detection frame-\nwork that is free of proposal, anchor, and post process-\ning (non-maximum suppression). Here, we build cascade\nTransformers based on the DETR [3] framework to perform\nregression-based pose recognition. Our system, named\nPRTR, aims towards a general-purpose keypoint regression\nsolution without speciÔ¨Åc heuristic-driven designs.\nRecently, Transformer architecture and self-attention\nhave seen increasing application in computer vision tasks\n[26, 3, 10], yet there are limited visualization works com-\npared with those done on language application [7, 34]. As\nfar as we know, we are the Ô¨Årst to visualize the dynamic de-\ncoding process in Transformer decoder, which brings sig-\nniÔ¨Åcant insights to future Transformer designs.\n3. Method\nWe argue that the attention mechanism in Transformer\ncan act as a general-purpose inference engine for regres-\nsion in vision tasks by writing visual perception as a\nBayesian inference P(Y|I) ‚àù P(I|Y)P(Y) with Y =\n(ÀÜyelb, ÀÜyknee,¬∑¬∑¬∑ , ÀÜynose). Here, Transformer for regression\nperforms direct learning and inference by capturing com-\nplex joint relations between inputIand prediction hypothe-\nses (queries), P(I|Y), through cross-attention, and model-\ning the prior on conÔ¨Åguration of Y, P(Y), via hypothesis\n(query) self-attention. See Figure 1.\nIn this section, we instantiate this idea as Pose Recog-\nnition with TRansformer (PRTR) for multi-person pose\nrecognition. The overall architecture is shown in Figure 2.\nWe Ô¨Årst introduce a cascaded double Transformer architec-\nture for person and keypoint detection, then an end-to-end\nvariant to streamline the entire model.\n3.1. Person-Detection Transformer\nWe tackle multi-person pose recognition problem in a\ntop-down manner, and adopt a Transformer architecture\n[33] following DEtection TRansformer (DETR) [3] as the\nbackbone for the Ô¨Årst-stage person detection. In the encoder\nstage, image features generated by a CNN are Ô¨Çattened and\nfed into a Transformer encoder to produce contextualized\nimage features; in the decoder stage, given a Ô¨Åxed set of\nlearned query embedding as input, Transformer decoder\nreasons about the relations between objects under the con-\ntext of image features, and output all the object queries in a\nparallel way. At last, a classiÔ¨Åcation head is used to classify\nthe object as person or background ( ‚àÖ), and a 4-channel\nregression head is used to predict the bounding boxes.\n3.2. Keypoint-Detection Transformer\nAfter getting the bounding boxes, we crop the RGB im-\nage and use another CNN backbone to get feature maps per\n3\nPerson-detection\nTransformer \nEncoder\nBounding\nBoxes ùê∫\nPerson-detection\nTransformer\nDecoder\nPerson \nPredictions\nKeypoint\nPredictions\nGrid generator\nKeypoint-detection\nTransformer\nEncoder\nKeypoint-detection\nTransformer\nDecoder\nùíØ(G)\nSample\nBackbone\nSample\nSample\nFigure 3: The architecture of Pose Recognition with TRansformer ( PRTR), end-to-end variant. For end-to-end learning,\ninstead of cropping at RGB image level, we apply differentiable bilinear sampling on multiple layers of backbone-generated\nfeatures to provide zoomed-in and multi-level feature for keypoint-detection Transformer.\nperson. Because only matched queries are involved in cal-\nculating the loss for keypoint-detection Transformer, we Ô¨Ål-\ntered out unmatched ones. Like the process of person detec-\ntion, we use the encoder-decoder architecture of the Trans-\nformer to predict in a parallel fashion, but we use another\nset of queries (quantity denoted Q). Finally, a classiÔ¨Åcation\nhead predicts among J types of joints and background ( ‚àÖ)\nand a 2-channel regression head outputs the coordinate of\neach keypoint.\nSince PRTR infers a Ô¨Åxed larger number of predictions\nthan ground truth (quantity denoted J), we need to Ô¨Ånd a\nmatching between them to calculate the loss. We formu-\nlate this matching problem as anoptimal bipartite matching\nproblem, which can be solved efÔ¨Åciently by Hungarian al-\ngorithm [27]. In speciÔ¨Åc, we try to Ô¨Ånd an injective function\nœÉ‚àà[J] ‚Üí[Q] that Ô¨Årstly minimizes the matching cost Cin\na discrete way:\nC= arg min\nœÉ\nJ‚àë\ni\nC\n(\nyi,ÀÜyœÉ(i)\n)\n(1)\n, where ÀÜyœÉ(i) means the prediction to be matched with the\ni-th keypoint.\nAt training stage, we match our queries using a mix-\nture of classiÔ¨Åcation probabilities and coordinate deviation.\nFor instance, the cost function for the i-th keypoint and its\nmatched query œÉ(i) is:\nCi = ‚àíÀÜpœÉ(i)(ci) +‚à•bi ‚àíÀÜbœÉ(i)‚à• (2)\n, where ÀÜpœÉ(i) is the class probabilities of the query and\nci is the class label for i-th keypoint. However, at infer-\nence stage, we do not have access to the ground-truth key-\npoint coordinates, thus we match J prototype keypoints to\nqueries using only the classiÔ¨Åcation probabilities. Therefore\nthe matching cost for i-th keypoint is simply:\nCi = ‚àíÀÜpœÉ(i)(ci) (3)\nAfter running the bipartite matching algorithm, we re-\nturn the matched J keypoints as our prediction.\nThe loss function of the model is obtained by replac-\ning negative probabilities in Equation 2 with negative log-\nlikelihood ‚àílog ÀÜpœÉ(i)(ci) for matched queries. For un-\nmatched queries we only backpropagate the classiÔ¨Åcation\nloss. To address the class imbalance caused by ‚àÖ class, as\nin [3], we set the weight of its log-probability term to 0.1.\n3.3. Multi-layer Cropping with STN\nIn the previous section, we introduce a two-stage\npipeline. However, under an end-to-end philosophy, it is\ndesired that the model is end-to-end tunable to exploit the\nsynergy between person detection and keypoint recognition\ntask. To this end, we incorporate the Spatial Transformer\nNetwork (STN) [11] to crop out image features needed by\nthe keypoint-detection Transformer directly from the fea-\nture map generated by the Ô¨Årst CNN backbone. This crop-\nping operation is differentiable not only to the feature maps,\nbut also to the bounding box coordinates.\nFor instance, an w √ó h grid generated by b =\n(xleft,xright,ytop,xdown) can be formulated by:\nxi = w‚àíi\nw xleft + i\nwxright (4)\nyj = h‚àíj\nh ytop + j\nhydown (5)\n, where bis relative to the original image, and w√óhis the\ndesired feature map size for the keypoint-detection Trans-\nformer.\nTo mitigate the resolution challenge commonly seen in\nkeypoint recognition, we apply the grid to feature maps of\ndifferent scales generated at different intermediate layers of\nthe CNN backbone using a bilinear kernel. Denoting the\nthe original W √óH feature map by U, the differentiable\n4\nTable 1: Comparisons on COCO val set. + indicates using multi-scale test. ‚àó indicates the end-to-end model variant.\nMethod Backbone Input size #Params GFLOPs AP AP50 AP75 APM APL AR\nHeatmap based\n8-stage Hourglass [22]Hourglass-8 stacked256√ó192 25.1M 14.3 66.9 ‚àí ‚àí ‚àí ‚àí ‚àí\nCPN [5] ResNet-50 256√ó192 27.0M 6.20 68.6 ‚àí ‚àí ‚àí ‚àí ‚àí\nSimpleBaseline [37] ResNet-50 384√ó288 34.0M 18.6 72.2 89.3 78.9 68.1 79.7 77.6\nSimpleBaseline [37] ResNet-101 384√ó288 53.0M 26.7 73.6 89.6 80.3 69.9 81.1 79.1\nHRNet [28] HRNet-W32 384√ó288 28.5M 16.0 75.8 90.6 82.7 71.9 82.8 81.0\nRegression based\nPointSetNet+ [35] ResNeXt-101-DCN ‚àí ‚àí ‚àí 65.7 85.4 71.8 ‚àí ‚àí ‚àí\nPointSetNet+ [35] HRNet-W48 ‚àí ‚àí ‚àí 69.8 88.8 76.3 ‚àí ‚àí ‚àí\nPRTR‚àó ResNet-101 ‚àí ‚àí ‚àí 64.8 85.1 70.2 60.4 73.8 73.9\nPRTR‚àó HRNet-W48 ‚àí ‚àí ‚àí 66.2 85.9 72.1 61.3 74.4 72.2\nPRTR ResNet-50 384√ó288 41.5M 11.0 68.2 88.2 75.2 63.2 76.2 76.0\nPRTR ResNet-50 512√ó384 41.5M 18.8 71.0 89.3 78.0 66.4 78.8 78.0\nPRTR ResNet-101 384√ó288 60.4M 19.1 70.1 88.8 77.6 65.7 77.4 77.5\nPRTR ResNet-101 512√ó384 60.4M 33.4 72.0 89.3 79.4 67.3 79.7 79.2\nPRTR HRNet-W32 384√ó288 57.2M 21.6 73.1 89.4 79.8 68.8 80.4 79.8\nPRTR HRNet-W32 512√ó384 57.2M 37.8 73.3 89.2 79.9 69.0 80.9 80.2\nsampling process can be formulated as:\nVij=\n‚àë\nm,n\nUnmmax (0,1‚àí|xi‚àím|) max (0,1‚àí|yj‚àín|) (6)\nAfter getting a series of image features of the same spa-\ntial size, we concatenate them into a single feature map for\nthe keypoint-detection Transformer. This multi-layer crop-\nping variant is illustrated in Figure 3.\n4. Experiment\nWe validate our proposed method on the COCO Key-\npoint Detection task and MPII Human Pose Dataset.\n4.1. Experiment Setup\nDatasets. We used two human pose estimation datasets,\nCOCO and MPII. The COCO dataset [19] contains over\n200,000 images and 250,000 person instances. Each per-\nson instance is labelled with 17 joints. We train our model\non COCO train2017 dataset with 57K images, and evalu-\nate our approach on the standard val2017 and test-dev2017\nsplit, containing 5K and 20K images respectively. The MPII\nsingle person dataset [1] consists of around 25K images and\n40K well-separated person instances. We follow the stan-\ndard train/val split.\nEvaluation metrics. We follow the common practice in\n[28] and use Object Keypoint Similarity (OKS) for COCO\nand Percentage of Correct Keypoints (PCK) for MPII to\nevaluate the performance.\nPerson-detection Transformer Ô¨Ånetuning. We Ô¨Årst\ntune a person detector by initializing from weights provided\nby DETR [3]. We keep all weights except prototype vec-\ntors for non-person class in the classiÔ¨Åer. The tuning lasts\nfor 10 epochs with a leaning rate of 1e ‚àí7 for ResNet-50\nbackbone and 5e‚àí6 for the rest. For pose recognition task,\npeople without any visible keypoints are not desired to be\ndetected; these people have a common characteristic of be-\ning small in area. In fact, all people with a segmentation\narea less than 322 do not contain keypoints. Given this, we\nskipped person annotations without visible keypoints at this\nstage for both training and evaluation. After tuning, the per-\nson detector scores an mAP of 67.0 on the pruned val2017\nset, and an mAP of 50.2 on the standard val2017 set.\nTwo-stage variant. For the two-stage version of our\nmodel, we extend the human detection bounding box in\nheight or width to a Ô¨Åxed aspect ratio ( 4 : 3 for COCO).\nA patch is cropped using the box and then resized to\na Ô¨Åxed size, 384 √ó288 or 512 √ó384 for COCO. The\ndata augmentation follows [37], including random rotation\n([‚àí40‚ó¶,40‚ó¶]), random scale ( [0.7,1.3]), and Ô¨Çipping. The\ndata pre-processing remains the same for MPII, except for\naspect ratio set to 1 : 1and input size available in384√ó384\nor 512 √ó512. For the Transformer part, number of encoder\nlayers, decoder layers and keypoint queries are set to 6, 6,\n100 respectively.\nWe use the AdamW optimizer [20]. The base learning\nrate is 1e ‚àí5 for ResNet backbone and 1e ‚àí4 for the rest,\nwith weight decay 1e ‚àí4. Multi-step learning rate sched-\nule is used, which halves the learning rate at the 120th and\n140th epoch respectively. The training process terminates\nwithin 200 epochs for both datasets.\nTesting. At test time, We use the person detection results\nfrom the tuned person detector (with AP 50.2 on COCO\nval2017 set) for both COCO val and test-dev set. Inspired\nby the common practice of Ô¨Çip-test [5, 22, 37] used in\nheatmap paradigms, we compute the keypoint coordinates\nby averaging the outputs of original and Ô¨Çipped images.\nEnd-to-end variant. For the end-to-end variant, we\nuse ground truth to match predicted people after person-\ndetection Transformer, and discard unmatched queries be-\n5\nTable 2: Comparisons on COCO test-dev set, excluding systems trained with external data. + means using multi-scale test.\n‚àó means end-to-end model variant. For bottom-up methods and end-to-end PRTR, computation overheads are not shown for\nbeing incomparable to two-stage methods. #Params and FLOPs are calculated for the pose estimation network, excluding\nhuman detection and keypoint grouping. Table format is adapted from [35] and [28].\nMethod Backbone Input size #Params GFLOPs AP AP50 AP75 APM APL AR\nHeatmap based: keypoint heatmap prediction and post-processing to decode coordinates\nCMU-Pose [2] 3CM-3PAF ‚àí ‚àí ‚àí 61.8 84.9 67.5 57.1 68.2 66.5\nMask-RCNN [13] ResNet-50 ‚àí ‚àí ‚àí 63.1 87.3 68.7 57.8 71.4 ‚àí\nG-RMI [25] ResNet-101 353√ó257 42.6M 57.0 64.9 85.5 71.3 62.3 70.0 69.7\nAssoc. Embed. [21] Hourglass-4 stacked ‚àí ‚àí ‚àí 65.5 86.8 72.3 60.6 72.6 70.2\nPifPaf [17] ResNet-101-dilation ‚àí ‚àí ‚àí 66.7 ‚àí ‚àí 62.4 72.9 ‚àí\nPersonLab [24] ResNet-101 ‚àí ‚àí ‚àí 65.5 87.1 71.4 61.3 71.5 70.1\nPersonLab+ ResNet-101 ‚àí ‚àí ‚àí 67.8 88.6 74.4 63.0 74.8 74.5\nHigherHRNet+ [6] HRNet-W48 ‚àí ‚àí ‚àí 70.5 89.3 77.2 66.6 75.8 74.9\nCPN [5] ResNet-Inception 384√ó288 ‚àí ‚àí 72.1 91.4 80.0 68.7 77.2 78.5\nSimpleBaseline [37] ResNet-152 384√ó288 68.6M 35.6 73.7 91.9 81.1 70.3 80.0 79.0\nHRNet [28] HRNet-W48 384√ó288 63.6M 32.9 75.5 92.5 83.3 71.9 81.5 80.5\nDARK [40] HRNet-W48 384√ó288 63.6M 32.9 76.2 92.5 83.6 72.5 82.4 81.1\nRegression based: direct keypoint coordinate prediction\nCenterNet+ [41] Hourglass-2 stacked ‚àí ‚àí ‚àí 63.0 86.8 69.6 58.9 70.4 ‚àí\nDirectPose [31] ResNet-101 ‚àí ‚àí ‚àí 63.3 86.7 69.4 57.8 71.2 ‚àí\nSPM+ [23] Hourglass-8 stacked 384√ó384 ‚àí ‚àí 66.9 88.5 72.9 62.6 73.1 ‚àí\nIntegral [29] ResNet-101 256√ó256 45.0M 11.0 67.8 88.2 74.8 63.9 74.0 ‚àí\nPointSetNet+ [35] HRNet-W48 ‚àí ‚àí ‚àí 68.7 89.9 76.3 64.8 75.3 ‚àí\nPRTR‚àó ResNet-101 ‚àí ‚àí ‚àí 63.4 86.2 69.4 59.3 72.0 73.0\nPRTR‚àó HRNet-W48 ‚àí ‚àí ‚àí 64.9 87.0 71.7 60.2 72.5 74.1\nPRTR ResNet-101 384√ó288 60.4M 19.1 68.8 89.9 76.9 64.7 75.8 76.6\nPRTR ResNet-101 512√ó384 60.4M 33.4 70.6 90.3 78.5 66.2 77.7 78.1\nPRTR HRNet-W32 384√ó288 57.2M 21.6 71.7 90.6 79.6 67.6 78.4 78.8\nPRTR HRNet-W32 512√ó384 57.2M 37.8 72.1 90.4 79.6 68.1 79.0 79.4\nTable 3: Comparisons on the MPII val set (PCKh@0.5).\nMethod Backbone Head Sho Elb Wri Hip Knee Ank Mean\nHeatmap Based\nConvolutional Pose Machines [36]CPM 96.2 95.0 87.5 82.2 87.6 82.7 78.4 87.7\nSimple Baseline [37] ResNet-152 97.0 95.9 90.3 85.0 89.2 85.3 81.3 89.6\nHRNet [28] HRNet-W32 97.1 95.9 90.3 86.4 89.1 87.1 83.3 90.3\nRegression Based\nIntegral [29] ResNet-101 ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí ‚àí 87.3\nPRTR (ours) ResNet-101 96.3 95.0 88.3 82.4 88.1 83.6 77.4 87.9\nPRTR (ours) ResNet-152 96.4 94.9 88.4 82.6 88.6 84.1 78.4 88.2\nPRTR (ours) HRNet-W32 97.3 96.0 90.6 84.5 89.7 85.5 79.0 89.5\ncause they will not be contributing to training keypoint-\ndetection Transformer. For images with more than 5 peo-\nple, we randomly sample 5 matched queries to reduce\ncomputational cost. Bounding boxes predicted by person-\ndetection Transformer are enlarged by 25% at both the\nheight and width dimension before sampling image features\nfrom backbone features, which helps predicting keypoints\nat the margin by taking in more contextual information.\nWe used the same data augmentation as DETR [3] ex-\ncept randomly resizing the image to having its shortest side\nbeing 760 to 1024 while not exceeding 1400. Optimizer\nsettings follow the two-stage variant, except for halving the\nlearning rate at the 25th and 60th epoch instead.\n4.2. Results\nResults on the COCO dataset. Table 1 and Table 2\ncompare pose estimation results on COCO val and test-\ndev set respectively. Qualitative results are given in Fig-\nure 6. For the end-to-end variant, it surpasses competing\nfully end-to-end components like CenterNet [41] and Di-\nrectPose [31]. The two-stage variant of our approach out-\nperforms the competing baselines in the regression based\ncategory. Our model with ResNet-101 backbone is compa-\nrable to PointSetNet [35] which leverages a more complex\nbackbone (HRNet-W48). Our model beneÔ¨Åts from larger\n6\nFigure 4: Visualization of PRTR‚Äôs decoding process for the keypoint detection Transformer. In the Ô¨Årst row, the last column shows the\nÔ¨Ånal predictions and the former 6 columns show the predictions for the initial query embedding and the intermediate 5 decoder layers. The\nsecond row shows an overlay of heatmaps of 100 queries for Right Ear and Left Eye respectively.\nTable 4: Ablation study w.r.t. number of queries on COCO\nval2017. Fixed stands for class-speciÔ¨Åc queries,i.e., a query\nis always mapped to a Ô¨Åxed keypoint type.\n#Queries AP AP 50 AP75 APM APL AR\n100 67.7 87.7 74.9 62.6 75.7 74.2\n50 67.6 87.7 74.8 63.0 75.4 74.1\n17 67.3 87.9 74.4 62.1 75.4 73.1\n17 (Fixed) 56.3 83.7 61.9 54.2 60.3 69.6\ninput size and stronger feature backbones. By enlarging in-\nput size from384√ó288 to 512√ó384, PRTR with ResNet-50\nand ResNet-101 receives 2.2, 1.9 improvement respectively.\nOur best model, achieving 72.1 AP, is able to emulate the\nheatmap-based HigherHRNet [6].\nResults on the MPII val dataset.Since only MPII val is\npublicly available, we report the performance of our model\ntrained on the entire MPII train set, as shown in Table 3. Our\nbest model achieves a 89.5 PCKh@0.5 score, comparable to\nthat of SimpleBaseline [37]. Not needing a person detection\nstage, MPII is not tried with the end-to-end variant.\n4.3. Ablation Studies\nWe perform ablation studies on COCO dataset to verify\nour design choices as listed in Table 4 and 5. The results\npresented are on COCO val2017, with ResNet-50 backbone\nand input size 384 √ó288.\nNon class-speciÔ¨Åc queries. We make the queries of\nTransformer decoder to predict both keypoint coordinates\nand classes, and then select the required points from all\nthe queries via class probabilities. This way, we do not\nenforce a Ô¨Åxed correspondence between J keypoint types\nand queries. Therefore, the queries are not class-speciÔ¨Åc\nTable 5: Ablation study on COCO val2017. ‚ÄôGT Box‚Äô, ‚Äô‚àÖ\nLogit‚Äô represent ground truth box for cropping, and inclu-\nsion of background logits during inference respectively.\nGT\nBox\n‚àÖ\nLogit\nFlip\nTest AP AP50 AP75 APM APL AR\n67.1 87.6 74.5 62.6 74.7 73.7\n\u0013 69.1 90.1 77.0 66.1 73.7 73.9\n\u0013 66.2 87.2 73.5 62.1 72.8 72.8\n\u0013 \u0013 68.2 89.7 75.5 65.3 72.5 72.9\n\u0013 67.7 87.7 74.9 62.6 75.7 74.2\n\u0013 \u0013 70.4 91.2 78.3 67.1 75.2 74.7\n\u0013 \u0013 66.4 86.9 73.0 62.0 73.4 72.8\n\u0013 \u0013 \u0013 68.9 89.9 75.8 65.7 73.4 73.2\nand can be used to predict different types of keypoints each\ntime. Here, we focus on two alternative designs: a) different\nnumber of queries used; b) when number of queries equals\nthe number of required points, the necessity for queries to\nbe non class-speciÔ¨Åc. From Table 4, it is clear that 100-\nquery version only has a small advantage over 50- and 17-\nquery counterparts. However, using class-speciÔ¨Åc queries\nwill greatly hamper the performance of the model, resulting\nin a large drop in AP (11.4). This illustrates the necessity\nthat each query dynamically predicts its preferred keypoint\ntype, and reads out the best estimation through Hungarian\nmatching during inference.\nExclusion of background prediction during infer-\nence. During inference, we exclude the logits of the\nbackground class ( ‚àÖ) before normalizing class probabili-\nties to provide more keypoint candidates for the Hungarian\nmatcher. From Table 5, we observe that including the logits\nof background class will result in a 0.9‚àí1.5 drop in AP.\n7\nFigure 5: Visualization of 16 keypoint (excluding the background class) prediction out of Q = 100queries in the keypoint-detection\nTransformer on COCO val2017. Each colored dot represents a predicted keypoint for the corresponding class.\nFigure 6: Qualitative COCO human pose estimation results\non images of varying sizes and poses.\nFlip test. Flipping is a common test augmentation used\nin heatmap paradigms, where input image is horizontally\nÔ¨Çipped and fed to the model, and then Ô¨Çip back, align and\naverage the predicted heatmaps to increase accuracy. The\nsame technique applies to regression models as well, with\nresults obtained by directly averaging the predicted key-\npoint coordinates. Since regression operates on continuous\ncoordinate space, one advantage is that it does not suffer\nfrom the inaccuracy caused by alignment errors in heatmap\nparadigms, as described in [15]. From Table 5, Ô¨Çip test of-\nfers a consistent performance boost for our model.\nOracle results. We also explore the room for improve-\nment by replacing the bounding boxes predicted by person-\ndetector with ground truth (GT) ones, as in Table 5. It is\nevident that GT boxes improves AP by 2 ‚àí2.5, indicating\nthe potential beneÔ¨Åt of a stronger person-detector.\n4.4. Vis. for Keypoint Detection Transformer\nIn this section, we show visualizations for the keypoint\ndetection Transformer. In Figure 5 and Figure 7 we visu-\nalize the position and class distribution for keypoint pre-\ndictions by the queries. Different queries are observed to\nbias towards different keypoints ( e.g. in our model 92.3%\nof the predictions by the 89th query are nose keypoints).\nWe also observe that queries dedicated to certain keypoints\nare biased to speciÔ¨Åc locations ( e.g. the query focusing on\nthe nose tends to predict positions in the upper part of the\nimages) while the points predicted by queries focusing on\nbackground are uniformly distributed.\nnose\nL eye\nR eye\nL ear\nR ear\nL shoulder\nR shoulder\nL elbow\nR elbow\nL wrist\nR wrist\nL hip\nR hip\nL knee\nR knee\nL ankle\nR ankle\n0 0 0 0 0 0 2 0 0 0 2 0 0 2 0 6\n0 0 99 0 0 0 0 1 0 0 0 0 0 0 0 1\n0 0 0 0 0 1 0 0 0 0 0 0 0 2 0 13\n0 0 0 0 0 0 0 2 0 0 0 99 0 2 0 2\n0 99 0 0 0 2 1 4 0 0 0 0 0 1 0 0\n0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 1 99 0 0 0 0 2 0 1\n0 0 0 0 0 0 0 4 0 0 94 0 0 21 0 3\n0 0 0 0 0 0 5 5 0 0 1 0 0 2 97 2\n0 0 0 0 0 85 0 3 0 0 0 0 0 2 0 2\n0 0 0 0 0 1 2 16 0 0 0 0 98 3 0 6\n0 0 0 0 0 0 43 3 0 0 0 0 0 1 0 4\n99 0 0 0 0 0 0 11 0 0 0 0 0 35 0 0\n0 0 0 0 0 0 0 18 0 0 0 0 0 11 0 3\n0 0 0 0 98 2 38 13 0 2 0 0 0 3 0 43\n0 0 0 0 0 0 0 5 0 91 0 0 0 1 0 2\n0 0 0 98 0 0 1 3 0 0 0 0 0 5 0 3\nFigure 7: Visualization of distributions of predicted key-\npoint classes for 16 out of a total of Q = 100 queries in\nthe keypoint-detection Transformer. Numbers on heatmap\ncorrespond to the probability (√ó100) for the individual key-\npoint classes. We observe that queries learn to specialize on\nkeypoint classes.\nIn Figure 4, we explore and visualize query output results\nin different decoder layers during inference. The Ô¨Årst row\n8\nshows the queries selected by the Hungarian algorithm and\ndemonstrate how their predictions move and reÔ¨Åne through\nlower-to-higher decoder layers. Initially, the predictions are\nrandomly located in the image. After passing some decoder\nlayers, queries predictions gradually approach the proper lo-\ncations. It is noteworthy that if a query‚Äôs prediction is close\nto the ground truth in lower layers, its prediction barely\nchanges in higher layers.\nThe second row shows the spatial probabilities of a cer-\ntain type of keypoint. For visualization, Gaussian heatmaps\nare Ô¨Årst generated around the predicted keypoint locations,\nwith their peak values proportional to class probabilities;\nthen the heatmaps of all Q queries are stacked to form a\nsingle probability map. Note that the initial query embed-\nding (the Ô¨Årst column) produces an equivocal keypoint dis-\ntribution. There exists confusion of keypoint locations in\nthe Ô¨Årst several layers of decoder, yet as the decoder layer\ngoes deeper, the reÔ¨Ånement proceeds and eventually yields\na salient keypoint probability map (the last column).\n5. Conclusion\nIn this paper, we have presented Pose Regression TRans-\nformer (PRTR), a new design for regression-based multi-\nperson pose recognition method based on the Transformer\nstructure [33, 3]. It treats the pose recognition task as a\nregression task, removes complex pre/post-processing pro-\ncedures and requires fewer heuristic designs compared with\nexisting heatmap-based approaches. Our method includes\ntwo alternatives, one as a two-stage and the other an end-to-\nend one. PRTR achieves state-of-the-art performance com-\npared with other existing regression-based methods on the\nchallenging COCO dataset. Distribution and reÔ¨Ånement vi-\nsualization of keypoint queries blazes the trail of revealing\nTransformer decoder inner mechanisms. In the future, we\nwould like to investigate more powerful backbone networks\nand combine regression-based human detection and pose\nrecognition in a more Ô¨Çexible manner.\nReferences\n[1] Mykhaylo Andriluka, Leonid Pishchulin, Peter Gehler, and\nBernt Schiele. 2d human pose estimation: New benchmark\nand state of the art analysis. In IEEE Conf. Comput. Vis.\nPattern Recog., pages 3686‚Äì3693, 2014. 1, 2, 5\n[2] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Sheikh.\nRealtime multi-person 2d pose estimation using part afÔ¨Ån-\nity Ô¨Åelds. In IEEE Conf. Comput. Vis. Pattern Recog., pages\n7291‚Äì7299, 2017. 1, 2, 6\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In Eur. Conf. Com-\nput. Vis., 2020. 2, 3, 4, 5, 6, 9\n[4] J. Carreira, Pulkit Agrawal, K. Fragkiadaki, and Jitendra\nMalik. Human pose estimation with iterative error feed-\nback. 2016 IEEE Conference on Computer Vision and Pat-\ntern Recognition (CVPR), pages 4733‚Äì4742, 2016. 2, 3\n[5] Yilun Chen, Zhicheng Wang, Yuxiang Peng, Zhiqiang\nZhang, Gang Yu, and Jian Sun. Cascaded pyramid network\nfor multi-person pose estimation. In IEEE Conf. Comput.\nVis. Pattern Recog., pages 7103‚Äì7112, 2018. 1, 2, 5, 6\n[6] Bowen Cheng, Bin Xiao, Jingdong Wang, Honghui Shi,\nThomas S Huang, and Lei Zhang. Higherhrnet: Scale-aware\nrepresentation learning for bottom-up human pose estima-\ntion. In IEEE Conf. Comput. Vis. Pattern Recog., 2020. 1, 2,\n6, 7\n[7] Andy Coenen, Emily Reif, A. Yuan, Been Kim, A. Pearce, F.\nVi¬¥egas, and M. Wattenberg. Visualizing and measuring the\ngeometry of bert. ArXiv, abs/1906.02715, 2019. 3\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 3\n[9] Piotr Doll ¬¥ar, Peter Welinder, and Pietro Perona. Cascaded\npose regression. In IEEE Conf. Comput. Vis. Pattern Recog.,\npages 1078‚Äì1085, 2010. 3\n[10] A. Dosovitskiy, Lucas Beyer, A. Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, M. Dehghani,\nMatthias Minderer, Georg Heigold, S. Gelly, Jakob Uszko-\nreit, and N. Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale. ArXiv,\nabs/2010.11929, 2020. 3\n[11] Yanyan Fang, Biyun Zhan, Wandi Cai, Shenghua Gao, and\nB. Hu. Locality-constrained spatial transformer network for\nvideo crowd counting. 2019 IEEE International Conference\non Multimedia and Expo (ICME), pages 814‚Äì819, 2019. 4\n[12] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep\nlearning, volume 1. MIT Press, 2016. 2\n[13] Kaiming He, Georgia Gkioxari, Piotr Doll ¬¥ar, and Ross Gir-\nshick. Mask r-cnn. In Int. Conf. Comput. Vis., pages 2961‚Äì\n2969, 2017. 2, 6\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In CVPR,\n2016. 2\n[15] Junjie Huang, Zheng Zhu, Feng Guo, and Guan Huang. The\ndevil is in the details: Delving into unbiased data processing\nfor human pose estimation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), June 2020. 2, 8\n[16] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al.\nSpatial transformer networks. In Advances in neural infor-\nmation processing systems, pages 2017‚Äì2025, 2015. 2\n[17] Sven Kreiss, Lorenzo Bertoni, and Alexandre Alahi. Pifpaf:\nComposite Ô¨Åelds for human pose estimation. In IEEE Conf.\nComput. Vis. Pattern Recog., pages 11977‚Äì11986, 2019. 1,\n2, 6\n[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classiÔ¨Åcation with deep convolutional neural net-\nworks. In Advances in neural information processing sys-\ntems, 2012. 2\n[19] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll¬¥ar, and C Lawrence\n9\nZitnick. Microsoft coco: Common objects in context. In Eur.\nConf. Comput. Vis., pages 740‚Äì755, 2014. 1, 2, 5\n[20] I. Loshchilov and F. Hutter. Decoupled weight decay regu-\nlarization. In ICLR, 2019. 5\n[21] Alejandro Newell, Zhiao Huang, and Jia Deng. Associa-\ntive embedding: End-to-end learning for joint detection and\ngrouping. In Advances in neural information processing sys-\ntems, pages 2277‚Äì2287, 2017. 2, 6\n[22] Alejandro Newell, Kaiyu Yang, and Jia Deng. Stacked hour-\nglass networks for human pose estimation. In Eur. Conf.\nComput. Vis., pages 483‚Äì499, 2016. 1, 2, 5\n[23] Xuecheng Nie, Jiashi Feng, Jianfeng Zhang, and Shuicheng\nYan. Single-stage multi-person pose machines. In Int. Conf.\nComput. Vis., pages 6951‚Äì6960, 2019. 1, 2, 6\n[24] George Papandreou, Tyler Zhu, Liang-Chieh Chen, Spyros\nGidaris, Jonathan Tompson, and Kevin Murphy. Person-\nlab: Person pose estimation and instance segmentation with a\nbottom-up, part-based, geometric embedding model. In Eur.\nConf. Comput. Vis., pages 269‚Äì286, 2018. 1, 2, 6\n[25] George Papandreou, Tyler Zhu, Nori Kanazawa, Alexander\nToshev, Jonathan Tompson, Chris Bregler, and Kevin Mur-\nphy. Towards accurate multi-person pose estimation in the\nwild. In IEEE Conf. Comput. Vis. Pattern Recog. , pages\n4903‚Äì4911, 2017. 1, 2, 6\n[26] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, L. Kaiser,\nNoam Shazeer, Alexander Ku, and Dustin Tran. Image trans-\nformer. ArXiv, abs/1802.05751, 2018. 3\n[27] R. Stewart, M. Andriluka, and A. Ng. End-to-end people de-\ntection in crowded scenes. 2016 IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 2325‚Äì\n2333, 2016. 4\n[28] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\nhigh-resolution representation learning for human pose esti-\nmation. In IEEE Conf. Comput. Vis. Pattern Recog. , pages\n5693‚Äì5703, 2019. 1, 2, 5, 6\n[29] Xiao Sun, Bin Xiao, Fangyin Wei, Shuang Liang, and Yichen\nWei. Integral human pose regression. In Eur. Conf. Comput.\nVis., pages 529‚Äì545, 2018. 1, 2, 6\n[30] Wei Tang, Pei Yu, and Ying Wu. Deeply learned compo-\nsitional models for human pose estimation. In Proceedings\nof the European Conference on Computer Vision (ECCV) ,\nSeptember 2018. 2\n[31] Zeyong Tian, Hao Chen, and Chunhua Shen. Directpose:\nDirect end-to-end multi-person pose estimation. ArXiv,\nabs/1911.07451, 2019. 6\n[32] Alexander Toshev and Christian Szegedy. Deeppose: Human\npose estimation via deep neural networks. In IEEE Conf.\nComput. Vis. Pattern Recog., pages 1653‚Äì1660, 2014. 1, 2\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, ≈Åukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998‚Äì6008, 2017. 3,\n9\n[34] J. Vig and Yonatan Belinkov. Analyzing the structure\nof attention in a transformer language model. ArXiv,\nabs/1906.04284, 2019. 3\n[35] Fangyun Wei, Xiao Sun, Hongyang Li, Jingdong Wang, and\nStephen Lin. Point-set anchors for object detection, instance\nsegmentation and pose estimation. In ECCV, 2020. 2, 5, 6\n[36] Shih-En Wei, Varun Ramakrishna, Takeo Kanade, and Yaser\nSheikh. Convolutional pose machines. In Proceedings of the\nIEEE conference on Computer Vision and Pattern Recogni-\ntion, pages 4724‚Äì4732, 2016. 1, 2, 6\n[37] Bin Xiao, Haiping Wu, and Yichen Wei. Simple baselines for\nhuman pose estimation and tracking. In Eur. Conf. Comput.\nVis., pages 466‚Äì481, 2018. 1, 2, 5, 6, 7\n[38] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron\nCourville, Ruslan Salakhudinov, Rich Zemel, and Yoshua\nBengio. Show, attend and tell: Neural image caption gen-\neration with visual attention. In International conference on\nmachine learning, pages 2048‚Äì2057, 2015. 3\n[39] Wei Yang, Shuang Li, Wanli Ouyang, Hongsheng Li, and\nXiaogang Wang. Learning feature pyramids for human pose\nestimation. In Proceedings of the IEEE International Con-\nference on Computer Vision (ICCV), Oct 2017. 2\n[40] Feng Zhang, Xiatian Zhu, Hanbin Dai, Mao Ye, and Ce\nZhu. Distribution-aware coordinate representation for hu-\nman pose estimation. In IEEE Conf. Comput. Vis. Pattern\nRecog., pages 7093‚Äì7102, 2020. 2, 6\n[41] Xingyi Zhou, Dequan Wang, and Philipp Kr ¬®ahenb¬®uhl. Ob-\njects as points. In arXiv preprint arXiv:1904.07850, 2019. 1,\n2, 6\n10",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7502796649932861
    },
    {
      "name": "Transformer",
      "score": 0.7110440731048584
    },
    {
      "name": "Cascade",
      "score": 0.7009766101837158
    },
    {
      "name": "Regression",
      "score": 0.635805606842041
    },
    {
      "name": "Categorization",
      "score": 0.6326831579208374
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5936378240585327
    },
    {
      "name": "Encoder",
      "score": 0.5789394378662109
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5032448172569275
    },
    {
      "name": "Machine learning",
      "score": 0.5022482872009277
    },
    {
      "name": "Heuristic",
      "score": 0.44114747643470764
    },
    {
      "name": "Regression analysis",
      "score": 0.4269866943359375
    },
    {
      "name": "Data mining",
      "score": 0.34377193450927734
    },
    {
      "name": "Mathematics",
      "score": 0.1351318359375
    },
    {
      "name": "Statistics",
      "score": 0.10984450578689575
    },
    {
      "name": "Engineering",
      "score": 0.08457085490226746
    },
    {
      "name": "Voltage",
      "score": 0.07082709670066833
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Chemical engineering",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 7
}