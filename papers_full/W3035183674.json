{
  "title": "Reranking for Efficient Transformer-based Answer Selection",
  "url": "https://openalex.org/W3035183674",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2655935441",
      "name": "Yoshitomo Matsubara",
      "affiliations": [
        "University of California, Irvine"
      ]
    },
    {
      "id": "https://openalex.org/A2100272121",
      "name": "Thuy Vu",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A48142092",
      "name": "Alessandro Moschitti",
      "affiliations": [
        "Amazon (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1966443646",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2000431947",
    "https://openalex.org/W2997090102",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2251818205",
    "https://openalex.org/W3009496244",
    "https://openalex.org/W2923890923",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2908332126",
    "https://openalex.org/W2909544278",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W2338364780",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2120735855",
    "https://openalex.org/W2130618701",
    "https://openalex.org/W3034212969",
    "https://openalex.org/W2951528484",
    "https://openalex.org/W2988869004"
  ],
  "abstract": "IR-based Question Answering (QA) systems typically use a sentence selector to extract the answer from retrieved documents. Recent studies have shown that powerful neural models based on the Transformer can provide an accurate solution to Answer Sentence Selection (AS2). Unfortunately, their computation cost prevents their use in real-world applications. In this paper, we show that standard and efficient neural rerankers can be used to reduce the amount of sentence candidates fed to Transformer models without hurting Accuracy, thus improving efficiency up to four times. This is an important finding as the internal representation of shallower neural models is dramatically different from the one used by a Transformer model, e.g., word vs. contextual embeddings.",
  "full_text": null,
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8449048399925232
    },
    {
      "name": "Computer science",
      "score": 0.7999224662780762
    },
    {
      "name": "Sentence",
      "score": 0.7874323129653931
    },
    {
      "name": "Question answering",
      "score": 0.627691388130188
    },
    {
      "name": "Computation",
      "score": 0.5507935285568237
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5432729125022888
    },
    {
      "name": "Selection (genetic algorithm)",
      "score": 0.416822612285614
    },
    {
      "name": "Machine learning",
      "score": 0.3949418365955353
    },
    {
      "name": "Natural language processing",
      "score": 0.37733206152915955
    },
    {
      "name": "Algorithm",
      "score": 0.09879428148269653
    },
    {
      "name": "Engineering",
      "score": 0.0793333649635315
    },
    {
      "name": "Voltage",
      "score": 0.0628713071346283
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}