{
    "title": "CRAFT: Complementary Recommendations Using Adversarial Feature Transformer",
    "url": "https://openalex.org/W2799063700",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5018371708",
            "name": "Cong Phuoc Huynh",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5072153368",
            "name": "Arridhana Ciptadi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5111410771",
            "name": "Ambrish Tyagi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5081491178",
            "name": "Amit Agrawal",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2962793481",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2147069236",
        "https://openalex.org/W2738136547",
        "https://openalex.org/W2027731328",
        "https://openalex.org/W2750912449",
        "https://openalex.org/W3099462466",
        "https://openalex.org/W2200092826",
        "https://openalex.org/W2135367695",
        "https://openalex.org/W2621350877",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W2963073614",
        "https://openalex.org/W2432004435",
        "https://openalex.org/W2949987290",
        "https://openalex.org/W2739273093",
        "https://openalex.org/W2567101557",
        "https://openalex.org/W1973255633",
        "https://openalex.org/W2964350391",
        "https://openalex.org/W2604474734",
        "https://openalex.org/W2949117887",
        "https://openalex.org/W2952596663",
        "https://openalex.org/W2171960770",
        "https://openalex.org/W2766527293",
        "https://openalex.org/W2949212125",
        "https://openalex.org/W2618574778",
        "https://openalex.org/W2099471712",
        "https://openalex.org/W2585630030",
        "https://openalex.org/W2527430057",
        "https://openalex.org/W2953040449",
        "https://openalex.org/W2757508077"
    ],
    "abstract": "Traditional approaches for complementary product recommendations rely on behavioral and non-visual data such as customer co-views or co-buys. However, certain domains such as fashion are primarily visual. We propose a framework that harnesses visual cues in an unsupervised manner to learn the distribution of co-occurring complementary items in real world images. Our model learns a non-linear transformation between the two manifolds of source and target complementary item categories (e.g., tops and bottoms in outfits). Given a large dataset of images containing instances of co-occurring object categories, we train a generative transformer network directly on the feature representation space by casting it as an adversarial optimization problem. Such a conditional generative model can produce multiple novel samples of complementary items (in the feature space) for a given query item. The final recommendations are selected from the closest real world examples to the synthesized complementary features. We apply our framework to the task of recommending complementary tops for a given bottom clothing item. The recommendations made by our system are diverse, and are favored by human experts over the baseline approaches.",
    "full_text": "CRAFT: Complementary Recommendations Using\nAdversarial Feature Transformer\nCong Phuoc Huynh Arridhana Ciptadi Ambrish Tyagi Amit Agrawal\nAmazon.com\n{conghuyn, ambrisht, aaagrawa}@amazon.com\nAbstract\nTraditional approaches for complementary product rec-\nommendations rely on behavioral and non-visual data such\nas customer co-views or co-buys. However, certain domains\nsuch as fashion are primarily visual. We propose a frame-\nwork that harnesses visual cues in an unsupervised man-\nner to learn the distribution of co-occurring complemen-\ntary items in real world images. Our model learns a non-\nlinear transformation between the two manifolds of source\nand target complementary item categories (e.g., tops and\nbottoms in outÔ¨Åts). Given a large dataset of images contain-\ning instances of co-occurring object categories, we train\na generative transformer network directly on the feature\nrepresentation space by casting it as an adversarial opti-\nmization problem. Such a conditional generative model can\nproduce multiple novel samples of complementary items (in\nthe feature space) for a given query item. The Ô¨Ånal recom-\nmendations are selected from the closest real world exam-\nples to the synthesized complementary features. We apply\nour framework to the task of recommending complementary\ntops for a given bottom clothing item. The recommenda-\ntions made by our system are diverse, and are favored by\nhuman experts over the baseline approaches.\n1. Introduction\nRecommendation algorithms are central to many com-\nmercial applications, particularly for online shopping. In\ndomains such as fashion, customers are looking for cloth-\ning recommendations that visually complement their cur-\nrent outÔ¨Åts, styles, and wardrobe. Traditional content-based\nand collaborative recommendation algorithms [1, 17] do\nnot make use of the visual cues to suggest complementary\nitems. Among these, collaborative Ô¨Åltering [15, 24] is a\ncommonly used approach, which primarily relies on behav-\nioral and historical data such as co-purchases, co-views, and\npast purchases to suggest new items to customers. In con-\ntrast to these approaches, this work addresses the problem\nQueryTop\tworn\twith\tQuery\nRecommendations\tusing\tNearest\tNeighbor\tof\tTop\nRecommendations\tfrom\tour\talgorithm(preferred\tby\tdomain\texperts)\nFigure 1. Recommending tops for a given query bottom. Tops that\nare visually similar to the actual top worn with the query item\nare acceptable options, but lack diversity. Our approach generates\nboth complementary and diverse recommendations that are also\npreferred by the fashion specialists. Please see the supplementary\nmaterial for more examples.\nof providing complementary item recommendations for a\ngiven query item based on visual cues.\nWe develop an unsupervised learning approach for Com-\nplementary Recommendation using Adversarial Feature\nTransform (CRAFT), by learning the co-occurrence of item\npairs in real images. The assumption here is that the co-\noccurrence frequency of item pairs is a strong indicator of\nthe likelihood of their complementary relationship. We de-\nÔ¨Åne an adversarial process to train a conditional generative\ntransformer network that can learn the joint distribution of\nitem pairs by observing samples from the real distribution,\ni.e. image features of co-occurring items.\nIn contrast to traditional generative adversarial ap-\nproaches that aim to directly synthesize images, our gener-\native transformer network is trained on and generates sam-\nples in the feature space. To generate a recommendation\n1\narXiv:1804.10871v3  [cs.CV]  10 Sep 2018\nQuery\tImageEncoder(Es)\nTransformer\tLayers\nNoise:\tz\nPre-indexedtarget\tfeaturesRecommended\tItem\tImage\nnearest\tneighbor\tsearch reverse\tlookupùë°ÃÇ\nùëì\nFigure 2. Generating recommendations using the proposed CRAFT network.\nfor a given query image, we Ô¨Årst extract its features via\na pre-trained encoder network, Es (Figure 2). The query\nfeature, f, along with a sampled noise vector, z, is fed to\nthe transformer network to generate the feature vector for\na complementary item. We then use the generated feature,\nÀÜt, to retrieve the recommended target image by performing\na nearest neighbor search on a pre-indexed candidate sub-\nspace. Our proposed approach is general (modeling visual\nco-occurrences) and can be applied to different domains\nsuch as fashion, home design, etc.\nOur approach is a novel and unique way of utilizing gen-\nerative adversarial training with several advantages over tra-\nditional Generative Adversarial Network (GAN) [7] based\nimage generation. While the quality of visual image gen-\neration using GANs has improved signiÔ¨Åcantly (especially\nfor faces [12]), it still lacks the realism required for many\nreal-world applications, such as in the fashion/apparel in-\ndustry. More importantly, the goal of a recommendation\nsystem in such applications is often not to generate synthetic\nimages, but rather to recommend real images from a cata-\nlog of items. An approach that generates synthetic images\nwill thus still need to perform a search, which is typically\ndone in the feature space, to Ô¨Ånd the most visually similar\nimages in the catalog. CRAFT directly generates features\nof the recommended items, thereby bypassing the need to\ngenerate synthetic images and enabling a simpler and more\nefÔ¨Åcient algorithm. By working in the feature space, we can\nuse a simpler network architecture, which improves stabil-\nity during training and avoids common pitfalls such as mode\ncollapse [3].\nWe evaluate our algorithm (CRAFT) on a real-world\nproblem of recommending complementary top clothing\nitems for a given query bottom. For this task, a recommen-\ndation algorithm is expected to produce both complemen-\ntary and diverse set of results. Figure 1 shows an example of\nthe recommendation results from CRAFT and from a na¬®ƒ±ve\napproach of recommending nearest neighbors of the top. It\nis clear that generating recommendations by simply Ô¨Ånd-\ning visually similar tops lacks the capability of producing\ndiverse set of results.\nGiven that the assessment of fashion recommendations\nis a subjective problem, we conducted rigorous studies with\nfashion specialists to evaluate the quality of our algorithm.\nThrough these studies, we demonstrate the effectiveness of\nour approach as compared to several baseline approaches\n(Section 4.3).\n2. Related Work\nGenerative Adversarial Networks: GANs [7] have re-\ncently emerged as a powerful framework for learning gen-\nerative models of complex data distributions. They have\nshown impressive results for various tasks including image\ngeneration [12, 21], image-to-image translation [11, 34],\ndomain adaptation [2, 26, 28], etc. In a GAN framework,\na generator is trained to synthesize samples from a latent\ndistribution and a discriminator network is used to distin-\nguish between synthetic and real samples. The generator‚Äôs\ngoal is to fool the discriminator by producing samples that\nare as close to real data as possible. A recent work by Zhuet\nal. [35] used the GAN framework to generate new clothing\non a wearer. Our approach differs from these methods since\nwe do not aim to generate an image of the complementary\nitem. Instead, we use the adversarial training framework\nto learn the joint distribution between the source and tar-\nget features in an unsupervised manner. We train a trans-\nformer that takes as input a random noise vector as well as\nthe features of a query image and generates a feature vector\nrepresentation of a complementary item.\nThe GAN paradigm has also found applications in the\nareas of image manipulation and image transformation\n[2, 11, 26, 34]. For example, Srivastava et al. [26] add re-\nalism to synthetic data by training an adversarial network\nthat transforms a synthetic image into a real image. While\nsuch an approach can be applied to transform a given image\ninto that of a complementary item, it only provides a Ô¨Åxed\nmapping. In contrast, ours is a generative approach that\ncan provide multiple complementary items by learning the\njoint distribution in the feature space. Contrary to methods\nsuch as CycleGAN [34] that perform image-to-image trans-\nlation using raw pixels, our approach works directly in the\nfeature space. Feature-based domain adaptation approaches\nsuch as [28] attempt to directly learn a visual encoder for\nthe target domain by minimizing an adversarial loss deÔ¨Åned\nGAN Input Generative Output Example\n(w/ random seed)\nN/A Yes Image Image Generation [7]\nImage No Image Image-to-Image Translation [34]\nImage + Attribute No Image Image Manipulation [16]\nSynthetic Image No Image Adding Realism [26]\nSynthetic Image Yes Image Adding Realism [2]\nImage No Features Domain Adaptation [28]\nFeatures Yes Features Ours\nTable 1. Similarities and differences between our approach and those that use adversarial loss for training.\non the source and target features. In contrast, we train a\ngenerative transformer network that operates in the feature\nspace. Table 2 shows similarities and differences between\nour approach and those that use adversarial loss for training.\nUnsupervised Learning: Recent applications of unsu-\npervised learning for visual tasks include object discov-\nery in videos [4]. In addition, there have been demonstra-\ntions of self-supervised learning [6] for the tasks of image\ncolorization, image in-painting, hole Ô¨Ålling, jigsaw puzzle\nsolving from image patches, future frame prediction using\nvideo data [31], etc. In the fashion domain, annotated data\nare typically used for predicting fashion-related attributes\nand matching street-to-catalog images [13, 20]. These ap-\nproaches involve visual search to Ô¨Åndsimilar looking items,\nwhereas our approach is focused on Ô¨Ånding complementary\nitems. Furthermore, our approach is unsupervised: we only\ntake as input a set of images to learn the feature transforma-\ntion between complementary objects.\nRecommendation: There is a rich body of literature on\nusing behavioral customer data such as browsing and pur-\nchasing history to develop recommender systems [15, 32].\nSpeciÔ¨Åc to the fashion domain, McAuley et al . [23] em-\nployed convolution neural network (CNN) features and\nnon-visual data to build a personalized model of user‚Äôs\npreference. In [9], the authors proposed a mixture of\n(non-metric) embeddings to recommend visually compat-\nible items in several categories. A related approach is to\nlearn a common embedding across categories and use a met-\nric function in the embedding subspace as a measure of vi-\nsual compatibility [30]. In [8], the authors proposed to learn\na bi-directional Long Short Term Memory (LSTM) model\nin a supervised manner, to suggest items that complement\neach other in an entire outÔ¨Åt.\nThe aforementioned recommendation approaches use\ncustomer‚Äôs behavioral data as training labels. Behavioral\nsignals do not necessarily reÔ¨Çect that items viewed or pur-\nchased together are visually complementary. In contrast,\nour unsupervised approach learns item co-occurrences from\nonly visual data. In multiple methods [8, 23, 30], the recom-\nmendation model is non-generative in the sense that it can\nonly evaluate the compatibility between two given items. In\nothers [9], the diversity of recommendation is limited by the\n(Ô¨Åxed) number of embeddings employed. In contrast, our\ngenerative model is not subject to such a constraint, thanks\nto its ability to sample an inÔ¨Ånite amount of noise vectors.\n3. Generative Feature Transformer Network\nThis section describes our generative approach for com-\nplementary recommendation based on the co-occurrence of\nitem pairs in real-world images. This could include, for ex-\nample, combinations of top and bottom clothing items that\npeople wear as part of their outÔ¨Åts or pairs of furniture items\nsuch as sofa and chairs that are present in most household\nscenes. We hypothesize that learning the joint distribution\nof such pairs can be useful for recommending new items\nthat complement a given query. We adopt an adversarial\nlearning paradigm, where our transformer network learns to\ngenerate features of the complementary items conditioned\non the query item.\n3.1. Network Architecture\nIn contrast to traditional GANs, our approach synthe-\nsizes visual features rather than images, which offers sev-\neral advantages. It is more tractable to sample the feature\nspace rather than the image space. The bulk of computation\nin the convolution and transpose convolution layers, which\nare usually required for feature learning and image synthe-\nsis, can be avoided, leading to a simpler architecture and\nstable training.\nWe Ô¨Årst select an appropriate visual representations for\nthe source and target images. The Ô¨Åxed feature represen-\ntations (encodings) are generally derived from pre-trained\nCNNs. Typically, it is advisable to use application-speciÔ¨Åc\nfeature representations, e.g., apparel feature embeddings\nfor clothing recommendations, but a general representation\nsuch as one trained on ImageNet [5] or MS-COCO [19] of-\nfer robust alternatives. Figure 3 depicts the overall archi-\ntecture of the CRAFT network. The source and the target\nfeature encoders, Es and Et, respectively, are Ô¨Åxed and are\nused to generate feature vectors for training and inference.\nOur architecture resembles traditional GAN designs with\ntwo main components: a conditional feature transformer\nand a discriminator. The role of the feature transformer\nis to transform the source feature sq into a complementary\ntarget feature ÀÜtq. The input to the transformer also con-\nsists of a random noise vector z sampled uniformly from\nSource\tImage\t(Q)Encoder(Es)\nSourceImage\t(S)Encoder(Es)\nTargetImage\t(T)Encoder(Et)\nreal\tor\tfakeTransformer\tLayers\nDiscriminator\nNoise:\tz\nConcatenated:Real\tsource\t+\tSynthesized\ttarget\nConcatenated:Real\tsource\t+\tReal\ttarget\nùë°ÃÇ#\nùë†#\nùë†%\nùë°%\nFigure 3. Architecture for the CRAFT framework. The transformer is trained using the adversarial loss to generate the target features\nconditioned on the source features and a sampled noise vector.\na unit sphere in a dz-dimensional space. By design, the\ntransformer is generative since it is able to sample various\nfeatures in the target domain.\nAs discussed, since our approach works in the feature\nspace, we can adopt a simple architecture for the feature\ntransformer and discriminator. The transformer consists of\nseveral fully-connected layers, each followed by batch nor-\nmalization [10] and leaky ReLU [22] activation layers. The\ndiscriminator is commensurate to the transformer in capac-\nity, consisting of the same number of layers. This helps\nbalance the power between the transformer and the discrim-\ninator in the two-player game, leading to stable training and\nconvergence.\n3.2. Training\nOur training data consists of N co-occurring feature\npairs C = {(si,ti),i = 1,...,N }, where si ‚ààRds and\nti ‚ààRdt denote the features corresponding to the source\nand the target images, respectively. Given a samplesq from\nthe source space, the complementary recommendation task\nis to generate target features {ÀÜtq}that maximizes the like-\nlihood that the pair (sq,ÀÜtq) belongs to the joint distribution\npCrepresented by the training data. To this end, we model\nthe composition of layers in the feature transformer and the\ndiscriminator as two functions TœÜ(s,z) : (s,z) ‚Ü¶‚ÜíÀÜt and\nDŒ∏(s,t) : (s,t) ‚Ü¶‚Üí[0,1], respectively. Here, œÜand Œ∏ are\nthe learnable parameters of the two players, transformer and\ndiscriminator, respectively, and(s,t) is a pair of source and\ntarget feature vectors, and zis a random noise vector.\nThe training process emulates an adversarial game be-\ntween the feature transformer and the discriminator, where\nthe discriminator aims to classify feature pairs as real (co-\noccurring) or synthetic. On the other hand, the feature trans-\nformer synthesizes target features {ÀÜtq}conditioned on a\ngiven source feature sq. Its objective is to fool the discrimi-\nnator into the belief that ÀÜtq co-occurs with sq. The feedback\nfrom the discriminator encourages the transformer to pro-\nduce a target feature ÀÜtq so as to maximize the co-occurrence\nprobability of the synthetic pair.\nThe adversarial game can be formulated as a mini-max\noptimization problem. The optimization approach can be\nimplemented by alternating the training of the discriminator\nand the feature transformer. In the discriminator step (D-\nstep), the discriminator‚Äôs goal is to assign a binary label,\ni.e. 0 to the synthesized feature pair (sq,ÀÜtq), where ÀÜtq =\nTœÜ(sq,z), and 1 to an actual pair (si,ti). The discriminator\nmaximizes the cross entropy loss in Equation 1.\nLD ‚âú E(si,ti)‚àºpC log DŒ∏(si,ti)\n+ Ez‚àºpz,sq‚àºps log(1 ‚àíDŒ∏(sq,TœÜ(sq,z))) (1)\nwhere pz and ps are the probability distribution function\n(pdf) of the random noise and the source feature.\nThe feature transformer maximizes the likelihood that\nthe discriminator recognizes synthetic pairs as belonging to\nthe data-generating (joint) distribution C, i.e. it assigns a la-\nbel 1 to such pairs. Therefore, the transformer step (T-step)\naims to minimize the loss in Equation 2.\nLT = Ez‚àºpz,sq‚àºps log(1 ‚àíDŒ∏(sq,TœÜ(sq,z))) (2)\nThe overall objective function of the adversarial training\nprocess is formulated in Equation 3.\nmin\nœÜ\nmax\nŒ∏\nL‚âú E(si,ti)‚àºpC log DŒ∏(si,ti)\n+ Ez‚àºpz,sq‚àºps log(1 ‚àíDŒ∏(sq,TœÜ(sq,z)))\n(3)\nWhile our approach is completely unsupervised and does\nnot require labels for complementary relationships, it can\nbe easily extended to train a semi-supervised transformer\nthat can beneÔ¨Åt from additional complementary annota-\ntions. For example, if we know that certain complementary\ncombinations are better or are ranked higher than others,\nthen we can employ an additional discriminator that can\ntake concatenated source and target features from labeled\ndata and predict the ranking of these pairs.\n3.3. Generating Recommendations\nThe recommendation work-Ô¨Çow is depicted in Figure 2.\nHere, we retain only the transformer‚Äôs layers shown in Fig-\nure 3 for recommendation. From a query image, the query\nfeature f is extracted by the source encoder, Es, and mul-\ntiple samples of transformed features {ÀÜti}are generated by\nsampling random vectors {zi}. This allows us to gener-\nate a diverse set of complementary recommendations by\nsampling the underlying conditional probability distribu-\ntion function. Subsequently, we perform a nearest neighbor\nsearch within a set of pre-indexed target features extracted\nusing the same target encoder, Et, used during training.\nActual recommendation images are retrieved by a reverse\nlookup that maps the selected features to the original target\nimages.\n4. Experimental Setup\nWe demonstrate the efÔ¨Åcacy of our approach by applying\nit to the problem of complementary apparel recommenda-\ntions. SpeciÔ¨Åcally, we train the generative transformer net-\nwork to synthesize features for top clothing items that are\nvisually compatible to a given query bottom item. The gen-\nerated features are used to retrieve the images of the nearest\nneighbors in a pre-indexed catalog of candidate top items as\nthe complementary recommendations.\n4.1. Datasets\nWe trained the proposed CRAFT network from scratch\non unlabeled images, without the need for any human anno-\ntation to deÔ¨Åne complementary relationships. Our training\ndata consisted of 473k full-length outÔ¨Åt images, each con-\ntaining a top and a bottom clothing part, under the assump-\ntion that their outÔ¨Åts are highly complementary.\nEach image was preprocessed to extract the regions of\ninterest (ROIs) corresponding to the top and bottom cloth-\ning items. To extract clothing part ROIs, we trained a se-\nmantic segmentation network [33] on the ‚ÄúHuman Parsing\nin the Wild‚Äù dataset [18]. We consolidate the original labels\nin the dataset into 15 labels, where top clothing items cor-\nrespond to the label ‚Äúupper-clothes‚Äù and bottom ones cor-\nrespond to ‚Äúpants‚Äù and ‚Äúskirt‚Äù. Using this segmentation\nnetwork, we parse the training images into clothing parts,\nbased on which tight bounding boxes around the segments\ncorresponding to top and bottom regions are selected. In\nthis manner, the training pairs are obtained automatically\nfrom images without the need for manual complementary\nannotation. Only the pre-processing stage requires super-\nvised training. Having obtained the input feature from this\nstage, the feature transformer was trained in an unsuper-\nvised manner.\nAs discussed earlier, we use a Ô¨Åxed visual feature encod-\ning for both the training and the recommendation process.\nTo this end, we extract the global averaging pooling feature\nencoded by the Inception-v4 model [27]. Rather than work-\ning in the original feature space with 1536 dimensions, we\nfurther reduce the dimensionality to 128 by Principal Com-\nponent Analysis. This helps address the curse of dimension-\nality and further reduces the computational load.\n4.2. Training and Network Parameters\nWe use the Adam optimizer [14] with starting learning\nrate of 0.0002 for both the discriminator and the transformer\nnetworks. To improve training stability, we use one-sided\nlabel noise [25]. Each minibatch for training the discrimi-\nnator consists of an equal proportion of synthetic and real\nfeature pairs. Our transformer network is composed of 3\nfully connected layers with 256 channels in the Ô¨Årst two\nlayers and 128 channels in the third. Our discriminator is\ncomposed of 3 fully connected layers with 256, 256, and\n1 channel(s), respectively. The noise vector z is uniformly\nsampled from the unit sphere in R128. We use leaky ReLU\n(Œ± = 0.2) and batch normalization for the Ô¨Årst two layers\nof both the transformer and the discriminator.\n4.3. Baseline Algorithms\nTo demonstrate the effectiveness of CRAFT, we compare\nit with the following baseline algorithms:\nRandom recommendations: A trivial baseline gener-\nates random recommendations from a given set of candidate\noptions, referred to as Random. A random selection can of-\nfer a diverse set of target items, but they may not necessarily\nbe complementary to the query item.\nNearest neighbors of source items: A strong and use-\nful baseline method is to Ô¨Ånd items similar to the query in\nthe source space, and recommend their corresponding tar-\nget items. For example, if the query item is blue jeans,\nthis approach will recommend various tops that other peo-\nple have worn with similar blue jeans. Note that these can\nbe retrieved by performing a visual search for the nearest\nneighbors of the bottom item. We refer to this method as\nNN-Source.\nIncompatible recommendations: Additionally, we il-\nlustrate that CRAFT has not only learned to recommend\ncomplementary apparel items, but also learned the concept\nof visual incompatibility. We design a method for generat-\ning Incompatible recommendations by suggesting tops that\nare assigned low discriminator scores by the CRAFT net-\nwork.\n5. Results\nNow we present an in-depth analysis of our algorithm\nand comparisons with baseline methods. In Section 5.1,\nwe visualize how the learned transformer network dynami-\ncally reacts to given queries in terms of assigning compat-\nibility scores for candidate tops. We then show qualitative\nFigure 4. Each row shows a 2D t-SNE embedding of all the can-\ndidate tops (left) with the corresponding query image (right). The\ncolors represent the discriminator score for tops conditioned on\nthe query (red: high score, yellow: low score). Note that the dis-\ncriminator in CRAFT is able to learn that common bottoms such\nas blue jeans and gray pants are compatible with a wide range of\ntops as compared to rarer query items such as the patterned skirt\nshown in the last row.\nresults of recommendations produced by various algorithms\nin Section 5.2. Finally, we describe our study with domain\nexperts in Section 5.3 and analyze the results in Section 5.4.\n5.1. Visualization of The Discriminator Output\nIn order to visualize the space of candidate top items, we\nprojected them to a two-dimensional (2D) subspace using t-\nSNE [29]. The discriminator output can be seen as a proxy\nfor the compatibility score between any top and a given\nquery item. Each row in Figure 4 shows 2D embedding\nof all the tops in the dataset, color coded by the discrimina-\ntor score for each top, given the bottom query item (shown\non the right). Note that the compatibility scores for vari-\nous candidate tops change depending on the query bottom.\nThe yellow colors in the t-SNE plot denote low compatibil-\nity, while shades of orange to red denote high compatibility\n(see color bar). It is interesting to note how universal items\nsuch as blue jeans or gray pants are compatible with a large\nset of candidate tops, while rare bottoms like the richly tex-\ntured pattern skirt shown on the bottom row are compatible\nwith only a handful of tops. This illustrates that CRAFT is\nable to model the distribution of real item pairs.\n5.2. Qualitative Results\nFigure 5 shows qualitative results of the different rec-\nommendation methods for two query items. For this ex-\nperiment, we generated 8 top recommendations from each\nalgorithm and asked a fashion specialist to identify the top\nitems that complement the given bottom query. While all of\nthe approaches produce visually diverse recommendations,\nnot all of them are compatible with the query. For a com-\nmon bottom outÔ¨Åt such as dark jeans (Figure 5(a)), NN-\nSource perform as well as CRAFT, while for a less com-\nmon bottom such as bright pink skirt (Figure 5(b)) they per-\nform worse (see Section 5.4 for a more thorough analysis).\nThis is aligned with our intuition that the quality of NN-\nSource recommendation highly depends on the proximity\nof the neighbors of the query. Interestingly, the results of\nthe ‚Äòincompatible‚Äô algorithm demonstrate that our discrimi-\nnator is able to learn not only the concept of visual compat-\nibility, but also incompatibility: it often produces unusual\noutÔ¨Åt recommendation (e.g., the fur top as the third item in\nFigure 5(a)) that is not likely to complement the given bot-\ntom query.\n5.3. User Study Design\nWhen recommendations are provided from an open\nended set, they are difÔ¨Åcult to evaluate in absolute terms.\nTypically, different recommendation approaches are com-\npared via A/B testing. Furthermore, for subjective domains\nsuch as fashion, it is preferable to obtain input from do-\nmain experts who are familiar with nuances involved in\nmaking style-appropriate recommendations. We adopt A/B\ntesting as the main methodology to compare our proposed\napproach to various baselines described in Section 4.3. We\nevaluate the relevance of recommendations generated by\neach algorithm by measuring their acceptance by domain\nexperts.\nWe approached a panel of four fashion specialists (FS) to\nprovide feedback on recommendations generated by various\nalgorithms. Each FS was presented with 17 recommenda-\ntions for a given query (bottom) item, for each of the four\nalgorithms. Among these recommendations, the FS were\nasked to select those that they judge to be complementary\nto the query. We used a total of 64 different query bottoms\nin this study, ranging for popular bottoms such as blue jeans\nto less common bottoms such as richly patterned skirts (see\nthe Supplementary material for the full list). The images\nQuery\nTop\tworn\twith\tQuery\nRecommendation\tfrom\tour\talgorithm\nRecommendation\tusing\tincompatible\talgorithm\nRecommendation\tfrom\tNN\tSource\t(corresponding\ttop\tof\tNearest\tNeighbor\tbottom)\n(a) Complementary recommendation for a common query item (dark jeans)\nQuery\nTop\tworn\twith\tQuery\nRecommendation\tfrom\tour\talgorithm\nRecommendation\tusing\tincompatible\talgorithm\nRecommendation\tfrom\tNN\tSource\t(corresponding\ttop\tof\tNearest\tNeighbor\tbottom)\n(b) Complementary recommendation for a less common query item (pink skirt)\nFigure 5. Comparison of the results from different recommendation algorithms. Highlighted in green are the items that have been marked as\ncomplementary to the query input by a fashion specialist. The CRAFT approach generates better (see Section 5.4 for quantitative analysis)\nand diverse recommendations.\nwere presented to FS in a random order to eliminate any\nbias for the algorithm or query items.\nSince some FS are in general more selective than others,\nwe need to normalize for their individual bias. To achieve\nthis, we add the actual top worn by the user in the query\noutÔ¨Åt to the set of 17 recommendations at a random loca-\ntion. We normalize the FS acceptance scores by their likeli-\nhood of selecting the actual top as an acceptable recommen-\ndation. Note that we only perform analysis on the newly\nrecommended tops, and exclude the original top from our\nresults.\n5.4. Analysis\nFigure 6(a) shows the average rate of acceptance of\ngenerated recommendations for all FS for the four algo-\nrithms. As discussed, acceptance rates were normalized by\nthe probability of each FS accepting the actual top for the\ngiven query bottom. The error bar denotes the 95% con-\nÔ¨Ådence interval for each of the results. Non-overlapping\nerror bars indicate that the differences between the two re-\nsults are statistically signiÔ¨Åcant. The NN-Source algorithm\nhas the overall acceptance score of66.5¬±1.4 and works bet-\n(a) Overall acceptance rates\n(b) Binned acceptance rates\nFigure 6. Mean acceptance rate of recommendations for the dif-\nferent algorithms, as evaluated by fashion specialists (error bars\nindicate 95% conÔ¨Ådence intervals). (a) Overall acceptance rating\nfor each algorithm. (b) Acceptance ratings binned according to the\ndensity (high, medium, low) of query items in the feature space.\nter than the Random and Incompatible baseline algorithms\nas expected. The CRAFT approach generates recommenda-\ntions with the highest FS acceptance score (70.3 ¬±1.4).\nStratiÔ¨Åcation by Feature Space Density: It is even\nmore interesting to break down the analysis of the results in\nterms of the density of the query items in the feature space.\nIntuitively, the NN-Source algorithm should perform well\nin providing diverse complementary recommendations for\nhigh density regions in bottom feature space ( i.e., popular\nbottoms such as blue jeans). However, a nearest neighbor\nbased approach would not perform well in low density re-\ngions, corresponding to rare bottom examples (e.g., striped\npurple skirt).\nTo validate this hypothesis, we approximate the density\nof each query point by taking the average distance to K =\n25 nearest neighbors and bin the queries into low, medium,\nand high density regions, respectively. Figure 6(b) shows\nthe average recommendation acceptance rate provided by\nFS for each algorithm in each density region. Again, the er-\nror bars denote the 95% conÔ¨Ådence interval for each result.\nFor queries that fall in the high density regions, the differ-\nence between our proposed approach and the NN-Source\nalgorithm is statistically insigniÔ¨Åcant (error bars overlap).\nThis is expected since nearest neighbor search is a good es-\ntimator of the conditional distribution of tops given a bottom\nfor high density regions, where a large number of bottoms\nare available. However, the NN-Source algorithm starts to\ndegrade at the medium density level, and eventually degen-\nerates to similar performance as theRandom and the Incom-\npatible recommendation algorithms for low density regions.\nIn contrast, the performance of CRAFT is consistent across\nall regions and is better than baseline algorithms for mid and\nlow density regime. Thus, the proposed conditional trans-\nformer is able to generalize well irrespective of the density\nof the neighborhood surrounding the query item.\n6. Conclusion and Future Work\nWe presented CRAFT, an approach to visual comple-\nmentary recommendation by learning the joint distribution\nof co-occurring visual objects in an unsupervised manner.\nOur approach does not require annotations or labels to indi-\ncate complementary relationships. The feature transformer\nin CRAFT samples from aconditional distribution to gener-\nate diverse and relevant item recommendations for a given\nquery. The recommendations generated by CRAFT are pre-\nferred by the domain experts over those produced by com-\npeting approaches.\nBy modeling the feature level distributions, our frame-\nwork can potentially enable a host of applications, ranging\nfrom domain adaptation to one- or few-shot learning. The\ncurrent work could be extended to incorporate the end-to-\nend learning of domain-related encoders as part of the gen-\nerative framework.\nReferences\n[1] G. Adomavicius and A. Tuzhilin. Toward the next generation\nof recommender systems: A survey of the state-of-the-art\nand possible extensions. IEEE Trans. Knowledge and Data\nEngineering, 17(6):734‚Äì749, June 2005. 1\n[2] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, and D. Kr-\nishnan. Unsupervised pixel-level domain adaptation with\ngenerative adversarial networks. In CVPR, pages 95‚Äì104,\n2017. 2, 3\n[3] T. Che, Y . Li, A. P. Jacob, Y . Bengio, and W. Li. Mode\nregularized generative adversarial networks. In ICLR, 2017.\n2\n[4] I. Croitoru, S.-V . Bogolin, and M. Leordeanu. Unsupervised\nlearning from video to detect foreground objects in single\nimages. In ICCV, pages 4335‚Äì4343, 2017. 3\n[5] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImageNet: A Large-Scale Hierarchical Image Database. In\nCVPR, 2009. 3\n[6] C. Doersch and A. Zisserman. Multi-task self-supervised\nvisual learning. In ICCV, 2017. 3\n[7] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, and S. Ozair. Generative adversarial nets.\nIn NIPS, pages 2672‚Äì2680, 2014. 2, 3\n[8] X. Han, Z. Wu, Y . Jiang, and L. S. Davis. Learning fashion\ncompatibility with bidirectional lstms. In ACM Multimedia,\n2017. 3\n[9] R. He, C. Packer, and J. McAuley. Learning compatibil-\nity across categories for heterogeneous item recommenda-\ntion. In IEEE 16th International Conference on Data Min-\ning, ICDM, pages 937‚Äì942, 2016. 3\n[10] S. Ioffe and C. Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In\nICML, pages 448‚Äì456, 2015. 4\n[11] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros. Image-to-image\ntranslation with conditional adversarial nets. InCVPR, 2017.\n2\n[12] T. Karras, T. Aila, S. Laine, and J. Lehtinen. Progressive\nGrowing of GANs for Improved Quality, Stability, and Vari-\nation. CoRR, abs/1710.10196, 2017. 2\n[13] M. H. Kiapour, X. Han, S. Lazebnik, A. C. Berg, and T. L.\nBerg. Where to buy it: Matching street clothing photos in\nonline shops. In ICCV, 2015. 3\n[14] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. ICLR, 2015. 5\n[15] Y . Koren and R. Bell. Advances in collaborative Ô¨Åltering. In\nF. Ricci, L. Rokach, B. Shapira, and P. B. Kantor, editors,\nRecommender Systems Handbook, pages 145‚Äì186. 2011. 1,\n3\n[16] G. Lample, N. Zeghidour, N. Usunier, A. Bordes, L. De-\nnoyer, and M. Ranzato. Fader networks: Manipulating im-\nages by sliding attributes. CoRR, abs/1706.00409, 2017. 3\n[17] M. S. Lew, N. Sebe, C. Djeraba, and R. Jain. Content-based\nmultimedia information retrieval: State of the art and chal-\nlenges. ACM Trans. Multimedia Comput. Commun. Appl. ,\n2(1):1‚Äì19, Feb. 2006. 1\n[18] X. Liang, S. Liu, X. Shen, J. Yang, L. Liu, J. Dong, L. Lin,\nand S. Yan. Deep human parsing with active template regres-\nsion. IEEE Trans. Pattern Anal. Mach. Intell., 37(12):2402‚Äì\n2414, Dec. 2015. 5\n[19] T.-Y . Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ra-\nmanan, P. Doll ¬¥ar, C. L. Zitnick, T. Pajdla, B. Schiele, and\nT. Tuytelaars. Microsoft COCO: Common Objects in Con-\ntext. Springer International Publishing, 2014. 3\n[20] S. Liu, Z. Song, M. Wang, C. Xu, H. Lu, and S. Yan. Street-\nto-shop: Cross-scenario clothing retrieval via parts align-\nment and auxiliary set. ACM Multimedia, pages 1335‚Äì1336,\n2012. 3\n[21] Y . Lu, Y . Tai, and C. Tang. Conditional CycleGAN\nfor Attribute Guided Face Image Generation. CoRR,\nabs/1705.09966, 2017. 2\n[22] Maas, Andrew L and Hannun, Awni Y and Ng, Andrew\nY . RectiÔ¨Åer nonlinearities improve neural network acoustic\nmodels. In ICML, volume 30, 2013. 4\n[23] J. J. McAuley, C. Targett, Q. Shi, and A. van den Hengel.\nImage-based recommendations on styles and substitutes. In\nSIGIR, 2015. 3\n[24] P. Melville, R. J. Mooney, and R. Nagarajan. Content-\nboosted collaborative Ô¨Åltering for improved recommenda-\ntions. In Eighteenth National Conference on ArtiÔ¨Åcial In-\ntelligence, pages 187‚Äì192, 2002. 1\n[25] T. Salimans, I. Goodfellow, W. Zaremba, V . Cheung, A. Rad-\nford, and X. Chen. Improved techniques for training GANs.\nIn NIPS, pages 2234‚Äì2242, 2016. 5\n[26] A. Shrivastava, T. PÔ¨Åster, O. Tuzel, J. Susskind, W. Wang,\nand R. Webb. Learning from simulated and unsupervised\nimages through adversarial training. In CVPR, 2017. 2, 3\n[27] C. Szegedy, S. Ioffe, and V . Vanhoucke. Inception-v4,\ninception-resnet and the impact of residual connections on\nlearning. AAAI, abs/1602.07261, 2017. 5\n[28] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko. Adversarial\ndiscriminative domain adaptation. In CVPR, 2017. 2, 3\n[29] L. van der Maaten and G. Hinton. Visualizing high-\ndimensional data using t-SNE. JMLR, 9:2579‚Äì2605, Nov\n2008. 6\n[30] A. Veit, B. Kovacs, S. Bell, J. McAuley, K. Bala, and S. Be-\nlongie. Learning visual clothing style with heterogeneous\ndyadic co-occurrences. In International Conference on Com-\nputer Vision (ICCV), 2015. 3\n[31] C. V ondrick and A. Torralba. Generating the future with ad-\nversarial transformers. In CVPR, 2017. 3\n[32] S. Zhang, L. Yao, and A. Sun. Deep learning based rec-\nommender system: A survey and new perspectives. CoRR,\nabs/1707.07435, 2017. 3\n[33] H. Zhao, J. Shi, X. Qi, X. Wang, and J. Jia. Pyramid scene\nparsing network. In CVPR, 2017. 5\n[34] J.-Y . Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-\nto-image translation using cycle-consistent adversarial net-\nworks. In ICCV, 2017. 2, 3\n[35] S. Zhu, S. Fidler, and R. Urtasun. Be your own prada: Fash-\nion synthesis with structural coherence. In ICCV, 2017. 2"
}