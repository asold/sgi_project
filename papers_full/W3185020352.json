{
    "title": "How Many Layers and Why? An Analysis of the Model Depth in Transformers",
    "url": "https://openalex.org/W3185020352",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2773718666",
            "name": "Antoine Simoulin",
            "affiliations": [
                "Université Paris Cité"
            ]
        },
        {
            "id": "https://openalex.org/A1835867671",
            "name": "Benoît Crabbé",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3035038672",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W3015609966",
        "https://openalex.org/W2975381464",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2951244744",
        "https://openalex.org/W2888329843",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W2946417913",
        "https://openalex.org/W2964165804",
        "https://openalex.org/W2948947170",
        "https://openalex.org/W3042631625",
        "https://openalex.org/W2996159613",
        "https://openalex.org/W3101163004",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2981757109",
        "https://openalex.org/W2325237720",
        "https://openalex.org/W2970900903",
        "https://openalex.org/W3101731278",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2790235966",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2866343820",
        "https://openalex.org/W3035030897",
        "https://openalex.org/W2944701285",
        "https://openalex.org/W2972324944",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3034560159",
        "https://openalex.org/W2995983533"
    ],
    "abstract": "Antoine Simoulin, Benoit Crabbé. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: Student Research Workshop. 2021.",
    "full_text": "Proceedings of the Joint Conference of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Processing: Student Research Workshop, pages 221–228\nAugust 5–6, 2021. ©2021 Association for Computational Linguistics\n221\nHow Many Layers and Why? An Analysis of the Model Depth in\nTransformers\nAntoine Simoulin1,2 Benoˆıt Crabb´e2\n1Quantmetry 2University of Paris, LLF\nasimoulin@quantmetry.com\nbcrabbe@linguist.univ-paris-diderot.fr\nAbstract\nIn this study, we investigate the role of the mul-\ntiple layers in deep transformer models. We\ndesign a variant of A LBERT that dynamically\nadapts the number of layers for each token\nof the input. The key speciﬁcity of A LBERT\nis that weights are tied across layers. There-\nfore, the stack of encoder layers iteratively re-\npeats the application of the same transforma-\ntion function on the input. We interpret the\nrepetition of this application as an iterative\nprocess where the token contextualized repre-\nsentations are progressively reﬁned. We ana-\nlyze this process at the token level during pre-\ntraining, ﬁne-tuning, and inference. We show\nthat tokens do not require the same amount of\niterations and that difﬁcult or crucial tokens for\nthe task are subject to more iterations.\n1 Introduction\nTransformers are admittedly over-parametrized\n(Chen et al., 2020; Hou et al., 2020; V oita et al.,\n2019). Yet the role of this over-parametrization\nis not well understood. In particular, transformers\nconsist of a ﬁxed number of stacked layers, which\nare suspected to be highly redundant (Liu et al.,\n2020) and to cause over-ﬁtting (Fan et al., 2020;\nZhou et al., 2020). In this paper we provide a study\non the role of the multiple layers traditionally used.\nThe mechanism of transformer layers is often\ncompared to intuitive NLP pipelines (Tenney et al.,\n2019). Starting with the lower layers encoding\nsurface information, middle layers encoding syn-\ntax and higher layers encoding semantics (Jawahar\net al., 2019; Peters et al., 2018). Transformers pro-\ngressively reﬁne the features, which become more\nﬁne-grained at each iteration (Xin et al., 2020).\nHowever, ALBERT (Lan et al., 2020) highlights\nthat it is possible to tie weights across layers and\nrepeat the application of the same function. Con-\nsequently, we hypothesize that it is the number\nof layer applications that gradually abstracts the\nsurface information into semantic knowledge.\nTo better study the transformation of token rep-\nresentations across layers, we propose a variant of\nALBERT . Our model implements the key speci-\nﬁcity of weights tying across layers but also dy-\nnamically adapts the number of layers applied to\neach token. Since all layers share the same weight,\nwe refer to the application of the layer to the hidden\nstates as an iteration.\nAfter reviewing the related work (Section 2), we\ndetail the model and the training methodology in\nSection 3. In particular, we encourage our model\nto be parsimonious and limit the total number of it-\nerations performed on each token. In Section 4, we\nanalyze iterations of the model during pre-training,\nﬁne-tuning and inference.\n2 Related Work\nAdapting the transformer depth is an active subject\nof research. In particular, deep transformer mod-\nels are suspected to struggle to adapt to different\nlevels of difﬁculty. While large models correctly\npredict difﬁcult examples, they over-calculate sim-\npler inputs (Liu et al., 2020). This issue can be ad-\ndressed using early-stopping: some samples might\nbe sufﬁciently simple to classify using intermediate\nfeatures. Some models couple a classiﬁer to each\nlayer (Zhou et al., 2020; Liu et al., 2020; Xin et al.,\n2020). After each layer, given the classiﬁer output,\nthe model either immediately returns the output or\npasses the sample to the next layer. Exiting too late\nmay even have negative impacts due to the network\n”over-thinking” the input (Kaya et al., 2019).\nOngoing research also reﬁnes the application of\nlayers at the token level. Wang and Kuo (2020)\nbuild sentence embeddings by combining token\nrepresentations from distinct layers. Elbayad et al.\n(2020) and Dehghani et al. (2019) successfully use\n222\ndynamic layers depth at the token level for full\ntransformers (encoder-decoder). However, to the\nbest of our knowledge, our attempt is the ﬁrst to ap-\nply such mechanism to encoder only transformers\nand to provide an analysis of the process.\n3 Method\nIn this Section, we detail the model architecture,\nillustrated in Figure 1, and pre-training procedure.\n3.1 Model architecture\nWe use a multi-layer transformer encoder (Devlin\net al., 2019) which transforms a context vector of\ntokens (u1 ···uT ) through a stack ofLtransformer\nencoder layers (Eq. 1, 2). We use weight tying\nacross layers and apply the same transformation\nfunction at each iteration (Lan et al., 2020).\nh0\nt = Weut + Wp (1)\nhn\nt = layer(hn−1\nt ) ∀n∈[1,L] (2)\nFor the ﬁrst layer, We is the token embedding\nmatrix, and Wp the position embedding matrix.\nWe augment the model with a halting mecha-\nnism, which allows dynamically adjusting the num-\nber of layers for each token (Eq. 3 to 8). We di-\nrectly adapted this mechanism from Graves (2016).\nThe main distinction with the original version is the\nuse of a transformer model instead of a recurrent\nstate transition model. The mechanism works as\nfollow: at each iteration n, we add the following\noperations after Eq. 2. We assign a probability to\nstop pn\nt for each token at index t(Eq. 3). Given\nthis probability, we compute an update weight λn\nt\n(Eq. 4), which we use to compute the ﬁnal state as\nthe linear convex combination between the previ-\nous and current hidden state (Eq. 5).\npn\nt = σ(Whhn\nt + bn) (3)\nλn\nt = pn\nt if n<N t,Rt elif n= Nt, else 0 (4)\nhn\nt = λn\nt hn\nt + (1−λn\nt )hn−1\nt (5)\nWith σ the sigmoid function. We deﬁne the\nremainder Rt and the number of iterations for the\ntoken at index t, Nt with:\nRt = 1−\nNt−1∑\nl=1\npl\nt. Nt = min\nn′\nn′\n∑\nn=1\npn\nt ≥1−ϵ (6)\nAs soon as the sum of the probability becomes\ngreater than 1, the update weights λn\nt are set to 0\nand the token is not updated anymore (Eq. 4). A\nsmall ϵ factor ensures that the network can stop\nafter the ﬁrst iteration (Eq. 6).\nFigure 1: As in ALBERT model, tokens are transformed\nthrough the iterative application of a transformer en-\ncoder layer. Our model key speciﬁcity is the applica-\ntion of the halting mechanism, which dynamically ad-\njusts the number of iterations for each token.\n3.2 Pre-training objective\nDuring the pre-training phase, we train the model\nwith the sentence order prediction ( sop) — the\ntask introduced in Lan et al. (2020) that classi-\nﬁes whether segments from the input sequence fol-\nlow the original order or were swapped — and the\nmasked language model task (mlm) (Devlin et al.,\n2019). We also encourage the network to minimize\nthe number of iterations by directly adding the pon-\nder cost into ALBERT pre-training objective. Given\na length T input sequence u, Graves (2016) deﬁnes\nthe ponder cost P(u) as:\nP(u) =\nT∑\nt=1\nNt + Rt (7)\nWe deﬁne the ﬁnal pre-training loss as the fol-\nlowing sum:\nˆL= Lsop + Lmlm + τP (8)\nwhere τ is a time penalty parameter that weights\nthe relative cost of computation versus error.\n3.3 Datum and infrastructure\nWe follow the protocol fromALBERT and pre-train\nthe model with BOOK CORPUS (Zhu et al., 2015)\n223\nand English Wikipedia. We reduce the maximum\ninput length to 128 and the number of training steps\nto 112,5001. We use a lowercase vocabulary of size\n30,000 tokenized using SentencePiece. We train\nall our models on a single TPU v2-8 from Google\nColab Pro2 and accumulate gradients to preserve a\n4,096 batch size. We optimize the parameters using\nLAMB with a learning rate at 1.76e-3.\n4 Experiments\nWe now analyze our iterative model properties dur-\ning pre-training (Section 4.1) and ﬁne-tuning (Sec-\ntion 4.2). We start by describing the setup for each\nof the subtasks.\nmlm task We generate masked inputs follow-\ning ALBERT n-gram masking. We mask 20% of\nall WordPiece tokens but do not always replace\nmasked words with the[MASK] token to avoid dis-\ncrepancy between pre-training and ﬁne-tuning. We\neffectively replace 80% of the masked position with\n[MASK] ([MASK/MASK]), 10% with a random\ntoken ([MASK/random]), and keep the original\ntoken for the last 10% ([MASK/original]).\nsop task We format our inputs as “ [CLS] x1\n[SEP] x2 [SEP]”. In 50% of the case the two\nsegments x1 and x2 are effectively consecutive\nin the text. In the other 50%, the segments are\nswapped.\nPonder cost We ﬁx the time penalty factor τ em-\npirically such that the ponder penalty represents\naround 10% of the total loss. To estimate the pon-\nder cost, we discard the remainder, as R≪N for\nsufﬁcient values of N. Given Eq. 7, the ponder cost\nthen corresponds to the total number of iterations\nin the sentence, which is given byl×T, with T the\nnumber of tokens in the sequence and lthe average\niterations per token. We observe that ALBERT base\nloss converges to around 3.5. We calibrate τ such\nthat τP ≈0.35 ≈τ ×l×T. We train distinct\nmodels, listed in Table 1, that we calibrate such\nthat their average number of iterations per token\nl is respectively 3, 6, and 12. We refer to these\nmodels as respectively tiny, small and base.\n1As emphasized in https://github.com/\ngoogle-research/bert, longer sequences are compu-\ntationally expensive. To lighten the pre-training process, they\nadvise using 128 sentence length and increase the length to\n512 only for the last 10% of the training to train the positional\nembeddings. In this work, we only perform the ﬁrst 90%\nsteps as we are not looking for brute force performances.\n2https://colab.research.google.com/\n4.1 Analysis of the pre-training\nAnalysis of the iterations We pre-train models\nwith various conﬁgurations and observe the model\nmechanisms during the pre-training in Table 1.\nModels tiny small base\nτ 1e-3 5e-4 2.5e-4\nMax iterations 6 12 24\nmlm (Acc.) 55.4 57.1 57.4\nsop (Acc.) 80.9 83.9 84.3\nAll tokens 3.8 7.1 10.0\nAll unmasked tokens 3.5 6.5 9.2\n[MASK/MASK] 5.8 10.9 16.0\n[MASK/random] 5.8 10.9 16.0\n[MASK/original] 4.0 7.4 10.5\n[CLS] 6.0 12.0 22.5\n[SEP] 2.5 7.6 8.4\nTable 1: Average number of iterations given token\ntypes during the pre-training. For each model, we re-\nport a mean number of iterations on our development\nset, at the end of the pre-training.\nWe observe that the [CLS] token receives far\nmore iterations than other tokens. This observa-\ntion is in line with Clark et al. (2019) who analyze\nBERT attention and report systematic and broad\nattention to special tokens. We interpret that the\n[CLS] token is used as input for the sop task and\naggregates a representation for the entire input. On\nthe contrary, [SEP] token beneﬁts from usually\nfew iterations. Again, this backs the observation\nemerging from the analysis of attention that inter-\nprets [SEP] as a no-op operation for attention\nheads (Clark et al., 2019).\nWe also observe an interesting behavior from\nthe [MASK] which also beneﬁts from more\niterations than average tokens. As for the\n[CLS] token, we interpret that these tokens\nare crucial for the mlm task. Looking fur-\nther, we observe that [MASK/random] and\n[MASK/MASK] number of iterations is greater\nthan [MASK/original]. In this case, al-\nthough all tokens are targeted in the mlm task,\n[MASK/random] and [MASK/MASK] are obvi-\nously more difﬁcult to identify3.\nThe model seems to have an intuitive mechanism\n3During inference, the model cannot make the distinction\nbetween [MASK/original] and unmasked tokens. How-\never, we observe in Table 1 that the two token types have a\ndistinct mean number of iterations. We believe this is due to\nthe distribution of the [MASK] tokens. Indeed, we follow the\nprocedure from ALBERT and use n-gram masking. Therefore,\n[MASK/original] tokens tend to appear in the context\nof [MASK] tokens. This speciﬁc context increases the mean\nnumber of iterations.\n224\nand distributes iterations for tokens that are either\ncrucial for the pre-training task or present a certain\nlevel of difﬁculty. This also appears in line with\nearly-exit mechanisms cited in Section 2, that adapt\nthe number of layers, for the whole example, to\nbetter scale to each sample level of difﬁculty.\nNatural Fixed point We now analyze how the\ntoken’s hidden states evolve during our model it-\nerative transformations. At each iteration n, the\nself-attentive mechanism (Vaswani et al., 2017)\ncomputes the updated state n+ 1as a weighted\nsum of the current states. This introduces a cyclic\ndependency as every token depends on each other\nduring the iterative process. As convergence within\na loopy structure is not guaranteed, we encourage\nthe model to converge towards a ﬁxed point (Bai\net al., 2019).\nFigure 2: Evolution of the cosine similarity between\nhidden states hn\nt and hn+1\nt from two consecutive iter-\nations. We use our base model and measure iterations\non our development set, at the end of the pre-training.\nWe obtain this property ”for free” thanks to our\narchitecture speciﬁcity. Indeed at each iteration, the\nhidden state is computed as a convex combination\nof the previous nand current n+ 1hidden state.\nThe combination is controlled by λn\nt (Eq. 5). If λn\nt\nis closed to 0, then hn\nt ≈hn+1\nt and by deﬁnition\n(Eq. 4, 6) λn\nt will eventually be set to 0 at a certain\niteration.\nFigure 2 represents the evolution of the mean co-\nsine similarity between two hidden states from two\nconsecutive iterations hn\nt and hn+1\nt . The network\nindeed reaches a ﬁxed point for every token. The\n[SEP] and tokens that are not masked converge\nquicker than [MASK] tokens. Finally, the [CLS]\ntoken oscillates during intermediate layers before\nreaching an equilibrium4.\n4.2 Application to downstream tasks\nDuring the pre-training phase, the model focuses\non tokens either crucial for the pre-training task\nor presents a certain level of difﬁculty. Now we\nstudy our model behavior during the ﬁne-tuning on\ndownstream syntactic or semantic tasks.\nControl test To verify that our setup has reason-\nable performance, we evaluate it on the GLUE\nbenchmark (Wang et al., 2019). Results from Ta-\nble 2 are scored by the evaluation server5. As in De-\nvlin et al. (2019), we discard results for the WNLI\ntask6. For each task, we ﬁne-tune the model on the\ntrain set and select the hyperparameters on the dev\nset using a grid search. We tune the learning rate\nbetween 5e-5, 3e-5, and 2e-5; batch size between\n16 and 32 and epochs between 2, 3, or 4. To better\ncompare our setup, we pre-trainBERT and ALBERT\nmodel using our conﬁguration, infrastructure and\ndatum.\nAvg. Glue score\nBERT-base 76.9\nALBERT -base 75.6\nALBERT -base + Adapt. Depth 75.2\nALBERT -small + Adapt. Depth 74.2\nALBERT -tiny + Adapt. Depth 72.6\nTable 2: GLUE Test results, scored by the evaluation\nserver but without the WNLI task. To facilitate the\ncomparison, we reproduce B ERT and A LBERT , with\nour pre-training dataset, infrastructure and conﬁgura-\ntion detailed in Section 3.2.\nWe present results on the test set in Table 2. As\nexpected, the average score decreases with the num-\nber of iterations. Indeed, we limit the number of\ncomputation operations performed by our model.\nMoreover, we build our model on top of ALBERT ,\nwhich share parameters across layers, thus reduc-\ning the number of parameters compared with the\noriginal BERT architecture. However, despite these\nadditional constraints, results stay in a reasonable\nrange. In particular, ALBERT -base with adaptative\ndepth is very close to the version with a ﬁxed depth.\n4We present the Figures for other model conﬁgurations in\nAppendix A\n5https://gluebenchmark.com/\nleaderboard\n6See (12) from https://gluebenchmark.com/\nfaq.\n225\nProbing tasks Conneau and Kiela (2018) intro-\nduce probing tasks, which assess whether a model\nencodes elementary linguistic properties. We con-\nsider semantic and syntactic tasks that do not in-\ntroduce random replacements. In particular, a task\nthat predicts the sequence of top constituents im-\nmediately below the sentence node (TopConst), a\ntask that predicts the tense of the main-clause verb\n(Tense), and two tasks that predict the subject (resp.\ndirect object) number in the main clause (SubjNum,\nresp. ObjNum).\nTense Subj\nNum\nObj\nNum\nTop\nConst\npunct (121k) 5.0 4.8 5.2 6.7\nprep (101k) 4.6 4.6 5.4 6.2\npobj (98k) 4.5 4.6 5.4 5.8\ndet (86k) 4.5 4.6 5.1 6.1\nnn (81k) 5.1 5.4 5.8 6.7\nnsubj (80k) 5.3 6.1 5.9 7.5\namod (66k) 4.6 4.9 5.5 6.1\ndobj (49k) 4.8 5.0 5.9 6.1\nroot (44k) 5.9 6.1 6.2 7.9\nadvmod (37k) 4.8 4.8 5.3 6.8\navg. 5.4 5.4 5.8 7.2\ntest Acc. 87.5 93.9 96.1 91.2\nbaseline Acc. 87.3 94.0 96.0 91.9\nTable 3: Distribution of the iterations across token de-\npendency types. We ﬁne-tune our base model on each\nprobing task. We then perform inference on the Penn\nTree Bank dataset and report the number of iterations\ngiven token dependency types. The number in paren-\ntheses denotes the number of dependency tags. We only\ndisplay the top 10 most frequent tags. We indicate in\nbold tags for which the number of iterations is above\navg + std. We include a baseline accuracy which we\nobtain with the ALBERT -base version without an adap-\ntative depth mechanism and therefore 12 iterations per-\nformed for each token.\nIn our setup, we ﬁne-tune the model on the task\ntrain set and select the hyperparameters on the dev\nset using a grid search. We use a 5e-5 learning rate\nand ﬁne tune the epochs between 1 to 5; we use a\n32 batch size. Finally, we compare in Table 3 the\nnumber of iterations performed for each token on\nthe Penn Tree Bank (Marcus et al., 1993) converted\nto Stanford dependencies7,8 .\nWe provide an accuracy baseline, obtained with\nthe same setup but using ALBERT without the dy-\nnamic halting mechanism. As in the previous exper-\niment, we observe that for these tasks, out model\n7Since we use sentence piece vocabulary, we assign to\neach piece the dependency tag from the whole token.\n8We present the Tables for other model conﬁgurations in\nAppendix B\nachieve competitive performances despite using\nless computational operations.\nAlthough all tasks achieve signiﬁcant and com-\nparable accuracies, they all require a distinct global\nmean of iterations. The Tense task, which can be\nsolved from the verb only, is completed in only 5.4\niterations, while the TopConst task, which requires\nto infer some sentence structure, is performed in\n7.2 iterations. This suggests the model can adapt\nitself to the complexity of the task and globally\nspare unnecessary iterations.\nLooking at the token level, as during the pre-\ntraining (Section 4.1), the iterations are unevenly\ndistributed across tokens. The model seems to iter-\nate more on tokens that are crucial for the task. For\nSubjNum, the subj tokens achieve the maximum\nnumber of iterations, while for the ObjNum task,\nthe obj and root token iterates more. Similarly, all\ntasks present a high number of iteration on the main\nverb (root) that is crucial for each prediction.\n5 Conclusion\nWe investigated the role of the layers in deep trans-\nformers. We designed an original model that pro-\ngressively transforms each token through a dy-\nnamic number of iterations. We analyzed the dis-\ntribution of these iterations during pre-training and\nconﬁrmed the results obtained by analyzing the\ndistribution of attention across BERT layers, par-\nticularly the speciﬁc behavior played by special\ntokens. Moreover, we observed that key tokens\nfor the prediction task beneﬁt from more iterations.\nWe conﬁrmed this observation during ﬁne-tuning,\nwhere the tokens with a large number of iterations\nare also suspected to be key for achieving the task.\nOur experiments provide a new interpretation\npath for the role of layers in deep transformer mod-\nels. Rather than extracting some speciﬁc features\nat each stage, layers could be interpreted as the\niteration from an iterative and convergence process.\nWe hope that this can help to better understand the\nconvergence mechanisms for transformers models,\nreduce the computational footprint or provide new\nregularization methods.\n226\nReferences\nShaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2019.\nDeep equilibrium models. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 688–699.\nDaoyuan Chen, Yaliang Li, Minghui Qiu, Zhen Wang,\nBofang Li, Bolin Ding, Hongbo Deng, Jun Huang,\nWei Lin, and Jingren Zhou. 2020. Adabert: Task-\nadaptive BERT compression with differentiable neu-\nral architecture search. In Proceedings of the\nTwenty-Ninth International Joint Conference on Ar-\ntiﬁcial Intelligence, IJCAI 2020 , pages 2463–2469.\nijcai.org.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of bert’s attention. CoRR,\nabs/1906.04341.\nAlexis Conneau and Douwe Kiela. 2018. Senteval: An\nevaluation toolkit for universal sentence representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation,\nLREC 2018, Miyazaki, Japan, May 7-12, 2018.\nMostafa Dehghani, Stephan Gouws, Oriol Vinyals,\nJakob Uszkoreit, and Lukasz Kaiser. 2019. Univer-\nsal transformers. In 7th International Conference on\nLearning Representations, ICLR 2019, New Orleans,\nLA, USA, May 6-9, 2019. OpenReview.net.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186.\nMaha Elbayad, Jiatao Gu, Edouard Grave, and Michael\nAuli. 2020. Depth-adaptive transformer. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020. OpenReview.net.\nAngela Fan, Edouard Grave, and Armand Joulin. 2020.\nReducing transformer depth on demand with struc-\ntured dropout. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nAlex Graves. 2016. Adaptive computation time for re-\ncurrent neural networks. CoRR, abs/1603.08983.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic\nBERT with adaptive width and depth. In Advances\nin Neural Information Processing Systems 33: An-\nnual Conference on Neural Information Processing\nSystems 2020, NeurIPS 2020, December 6-12, 2020,\nvirtual.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure of\nlanguage? In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 3651–3657.\nYigitcan Kaya, Sanghyun Hong, and Tudor Dumitras.\n2019. Shallow-deep networks: Understanding and\nmitigating network overthinking. In Proceedings\nof the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach,\nCalifornia, USA, volume 97 of Proceedings of Ma-\nchine Learning Research, pages 3301–3310. PMLR.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020.\nWeijie Liu, Peng Zhou, Zhiruo Wang, Zhe Zhao,\nHaotang Deng, and Qi Ju. 2020. Fastbert: a self-\ndistilling BERT with adaptive inference time. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 6035–6044. Associa-\ntion for Computational Linguistics.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of english: The penn treebank. Computa-\ntional Linguistics, 19(2):313–330.\nMatthew E. Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018. Dissecting contextual word\nembeddings: Architecture and representation. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing, Brussels,\nBelgium, October 31 - November 4, 2018 , pages\n1499–1509. Association for Computational Linguis-\ntics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Conference of the Asso-\nciation for Computational Linguistics, ACL 2019,\nFlorence, Italy, July 28- August 2, 2019, Volume\n1: Long Papers , pages 4593–4601. Association for\nComputational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 5998–6008.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n227\n57th Conference of the Association for Computa-\ntional Linguistics, ACL 2019, Florence, Italy, July\n28- August 2, 2019, Volume 1: Long Papers , pages\n5797–5808. Association for Computational Linguis-\ntics.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In 7th\nInternational Conference on Learning Representa-\ntions, ICLR 2019, New Orleans, LA, USA, May 6-9,\n2019.\nBin Wang and C.-C. Jay Kuo. 2020. SBERT-WK:\nA sentence embedding method by dissecting bert-\nbased word models. IEEE ACM Trans. Audio\nSpeech Lang. Process., 28:2146–2157.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. Deebert: Dynamic early exiting\nfor accelerating BERT inference. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 2246–2251. Association for Computa-\ntional Linguistics.\nWangchunshu Zhou, Canwen Xu, Tao Ge, Julian J.\nMcAuley, Ke Xu, and Furu Wei. 2020. BERT loses\npatience: Fast and robust inference with early exit.\nIn Advances in Neural Information Processing Sys-\ntems 33: Annual Conference on Neural Information\nProcessing Systems 2020, NeurIPS 2020, December\n6-12, 2020, virtual.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. In 2015 IEEE Interna-\ntional Conference on Computer Vision, ICCV 2015,\nSantiago, Chile, December 7-13, 2015 , pages 19–\n27.\n228\nA Natural ﬁxed point\nWe present here the evolution of the mean cosine\nsimilarity between two hidden states from two con-\nsecutive iterations for our small (Figure 3) and tiny\n(Figure 4) models. As presented in Section 3.2, we\nﬁx the maximum number of iterations at respec-\ntively 6 and 12 for the tiny and small models.\nFigure 3: Evolution of the cosine similarity between\nhidden states hn\nt and hn+1\nt from two consecutive itera-\ntions. We use our small model and measure iterations\non our development set, at the end of the pre-training.\nFigure 4: Evolution of the cosine similarity between\nhidden states hn\nt and hn+1\nt from two consecutive itera-\ntions. We use our tiny model and measure iterations on\nour development set, at the end of the pre-training.\nB Probing tasks\nWe give here the probing tasks results from Sec-\ntion 4.2 with our small (Table 4) and tiny (Table 5)\nmodels.\nTense Subj\nNum\nObj\nNum\nTop\nConst\npunct (121k) 3.1 3.1 3.1 3.9\nprep (101k) 2.9 2.9 3.0 3.6\npobj (98k) 2.9 3.0 3.1 3.5\ndet (86k) 2.7 2.8 2.7 3.6\nnn (81k) 3.2 3.5 3.2 3.9\nnsubj (80k) 3.3 3.7 3.3 4.4\namod (66k) 2.9 3.0 3.0 3.6\ndobj (49k) 3.0 3.2 3.4 3.5\nroot (44k) 3.6 3.6 3.5 4.6\nadvmod (37k) 2.9 3.0 3.0 4.0\navg. 3.2 3.3 3.3 3.9\ntest Acc. 86.4 93.2 95.5 91.1\nbaseline Acc. 87.3 94.0 96.0 91.9\nTable 4: Distribution of the iterations across token de-\npendency types. We ﬁne-tune our small model on each\nprobing task. We then perform inference on the Penn\nTree Bank dataset and report the number of iterations\ngiven token dependency types. The number in paren-\ntheses denotes the number of dependency tags. We only\ndisplay the top 10 most frequent tags. We indicate in\nbold tags for which the number of iterations is above\navg + std. We include a baseline accuracy which we\nobtain with the ALBERT -base version without an adap-\ntative depth mechanism and therefore 12 iterations per-\nformed for each token.\nTense Subj\nNum\nObj\nNum\nTop\nConst\npunct (121k) 2.1 1.9 2.0 2.5\nprep (101k) 2.0 1.7 2.0 2.3\npobj (98k) 2.0 1.8 2.0 2.2\ndet (86k) 1.9 1.7 1.8 2.3\nnn (81k) 2.2 2.0 2.0 2.5\nnsubj (80k) 2.3 2.2 2.1 2.8\namod (66k) 2.1 1.8 2.0 2.3\ndobj (49k) 2.1 1.9 2.1 2.3\nroot (44k) 2.4 2.1 2.3 2.9\nadvmod (37k) 2.1 1.8 2.0 2.6\navg. 2.2 2.0 2.1 2.5\ntest Acc. 88.6 91.1 93.8 91.1\nbaseline Acc. 87.3 94.0 96.0 91.9\nTable 5: Distribution of the iterations across token de-\npendency types. We ﬁne-tune our tiny model on each\nprobing task. We then perform inference on the Penn\nTree Bank dataset and report the number of iterations\ngiven token dependency types. The number in paren-\ntheses denotes the number of dependency tags. We only\ndisplay the top 10 most frequent tags. We indicate in\nbold tags for which the number of iterations is above\navg + std. We include a baseline accuracy which we\nobtain with the ALBERT -base version without an adap-\ntative depth mechanism and therefore 12 iterations per-\nformed for each token."
}