{
  "title": "Preventing Generation of Verbatim Memorization in Language Models Gives a False Sense of Privacy",
  "url": "https://openalex.org/W4389009541",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2799006035",
      "name": "Daphne Ippolito",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2913042046",
      "name": "Florian Tramèr",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2537287136",
      "name": "Milad Nasr",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2118495952",
      "name": "Zhang Chiyuan",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2794445661",
      "name": "Matthew Jagielski",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2107274534",
      "name": "Katherine Lee",
      "affiliations": [
        "Cornell University",
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5001920255",
      "name": "Christopher Choquette-Choo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1606335232",
      "name": "Nicholas Carlini",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4226137521",
    "https://openalex.org/W4283070861",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W1544505227",
    "https://openalex.org/W2473418344",
    "https://openalex.org/W4287663285",
    "https://openalex.org/W4281483318",
    "https://openalex.org/W2123845384",
    "https://openalex.org/W2051267297",
    "https://openalex.org/W3102690631",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W4287727281",
    "https://openalex.org/W2946930197",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4226142937",
    "https://openalex.org/W4221159672",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W2101832700",
    "https://openalex.org/W3188505388",
    "https://openalex.org/W4229053728",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W2963440401",
    "https://openalex.org/W3211753216",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3035616549",
    "https://openalex.org/W2996176596",
    "https://openalex.org/W4283172211",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3206066344",
    "https://openalex.org/W3170672407",
    "https://openalex.org/W2074633331",
    "https://openalex.org/W4221145466",
    "https://openalex.org/W3177765786"
  ],
  "abstract": "Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works -- and some recently deployed defenses -- focus on \"verbatim memorization\", defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense based on Bloom filters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this \"perfect\" filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified \"style-transfer\" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. For example, instructing the model to output ALL-CAPITAL texts bypasses memorization checks based on verbatim matching. We conclude by discussing potential alternative definitions and why defining memorization is a difficult yet crucial open question for neural language models.",
  "full_text": "Proceedings of the 16th International Natural Language Generation Conference, pages 28–53\nSeptember 11–15, 2023. ©2023 Association for Computational Linguistics\n28\nPreventing Generation of Verbatim Memorization in Language\nModels Gives a False Sense of Privacy\nDaphne Ippolito1 Florian Tramèr˚2 Milad Nasr˚1\nChiyuan Zhang˚1 Matthew Jagielski˚1 Katherine Lee˚1,3\nChristopher A. Choquette-Choo˚1 Nicholas Carlini1\n1 Google Research 2 ETH Zurich 3 Cornell University\nAbstract\nStudying data memorization in neural lan-\nguage models helps us understand the risks\n(e.g., to privacy or copyright) associated with\nmodels regurgitating training data and aids in\nthe development of countermeasures. Many\nprior works—and some recently deployed\ndefenses—focus on “verbatim memorization”,\ndeﬁned as a model generation that exactly\nmatches a substring from the training set. We\nargue that verbatim memorization deﬁnitions\nare too restrictive and fail to capture more sub-\ntle forms of memorization. Speciﬁcally, we de-\nsign and implement an efﬁcient defense that\nperfectly prevents all verbatim memorization.\nAnd yet, we demonstrate that this “perfect” ﬁl-\nter does not prevent the leakage of training\ndata. Indeed, it is easily circumvented by plau-\nsible and minimally modiﬁed “style-transfer”\nprompts—and in some cases even the non-\nmodiﬁed original prompts—to extract memo-\nrized information. We conclude by discussing\npotential alternative deﬁnitions and why deﬁn-\ning memorization is a difﬁcult yet crucial open\nquestion for neural language models.\n1 Introduction\nThe ability of neural language models to memo-\nrize their training data has been studied extensively\n(Kandpal et al., 2022; Lee et al., 2021; Carlini et al.,\n2022; Zhang et al., 2021; Thakkar et al., 2021;\nRamaswamy et al., 2020). When language mod-\nels, especially ones used in production systems,\nare susceptible to data extraction attacks, it can\nlead to practical problems ranging from privacy\nrisks to copyright concerns. For example, Carlini\net al. (2021) showed that the GPT-2 language model\ncould output personally identifying information of\nindividuals contained in the training dataset.\n˚Remaining authors ordered by Algorithm 18 in Ap-\npendix H; brieﬂy, we require Daphne be listed ﬁrst, and\nNicholas listed last, and we search for the ﬁrst permutation of\nauthors’ ﬁrst names which satisﬁes these constraints, where\npermutations order names by their salted MD5 hash.\npartnering\ngoing… get arm-loads \nof free stuff. So\nThe Prompt\nI’m\nI’d\nWe’re…\ndoing…\ngiving\nwith…\nNext token probabilities\nNext token probabilities\nNext token probabilities\ntoken rejection based on training data\nFigure 1: Illustration of Memorization-free Decoding,\na defense which can eliminate verbatim memorization\nin the generations from a large neural language model,\nbut does not prevent approximate memorization.\nOne natural way to avoid this risk is to ﬁlter out\nany generations which copy long strings verbatim\nfrom the training set. GitHub’s Copilot, a language-\nmodel-based code assistant, deploys this defense\nby giving users the option to “block suggestions\nmatching public code” (GitHub, 2022).\nIn this work, we ask the question: “ Do lan-\nguage models emit paraphrased memorized con-\ntent?” This scenario can happen maliciously (e.g.,\nadversaries trying to extract private user data) or\nthrough honest interactions (e.g., users prompting\nin real-world scenarios). Indeed, we ﬁnd that Copi-\nlot’s ﬁltering system is easy to circumvent by ap-\nplying plausible “style transfers” to the prompt.\nFor example, by translating variable names from\nEnglish to French the model outputs completely\nmemorized examples, but post-processed with the\nen-fr style transfer. We further show that GPT-\n3 (Brown et al., 2020), a model trained on natural\nlanguage, is also vulnerable to extraction attacks.\nUnfortunately, Copilot’s training set and precise\nalgorithm for their defense are non-public. There-\nfore, to investigate this phenomenon systematically,\nwe develop MEMFREE decoding (Figure 1), an ef-\nﬁcient defense that is guaranteed to prevent all ver-\nbatim memorization, and which scales to training\nsets consisting of hundreds of gigabytes of text. In\n29\nMEMFREE decoding, at each step of generation\nwe check whether the model’s chosen next token\nwould create an n-gram found in the training set. If\nit does, an alternative next token is selected (with-\nout a computationally expensive regeneration) by\nsampling from the model’s token posterior. The\ncheck for membership in the training set is per-\nformed efﬁciently using a Bloom ﬁlter containing\nall common n-grams from the training set.\nWe use MEMFREE to study Copilot’s verbatim-\nﬁltering defense on other state-of-the-art large\nlanguage models such as GPT-Neo (Gao et al.,\n2020). We ﬁrst conﬁrm that even honestly de-\nsigned prompts often bypass verbatim memoriza-\ntion checks. Then, we observe another interesting\nphenomenon: language models succeed at emit-\nting approximate memorization that bypass our\nﬁlter all by themselves. Indeed, when prevented\nfrom generating exact n-grams from the training\nset, models are capable of “cheating” the ﬁlter by\nproducing close paraphrases–for example, insert-\ning spelling errors, adjusting punctuation or whites-\npace, or using synonyms (e.g., swapping ‘and’ with\n‘&’). These changes lead to generated text a human\nwould perceive as nearly identical, even if it is not\nverbatim memorization.\nClearly, defenses which prevent verbatim copy-\ning are necessary but not sufﬁcient to protect\nagainst training data leakage. As a result of these\nfailure modes, we argue that a broader deﬁni-\ntion of memorization is necessary when reason-\ning about training set memorization in language\nmodels. Such a deﬁnition should not only capture\nverbatim notions of memorization, but also notions\nbased on high “semantic similarity” between model\noutputs and training data. We conclude our work by\ncomparing approximate and verbatim memoriza-\ntion, discussing their relation to other domains of\nliterature, and the challenges surrounding the ambi-\nguity of approximate memorizations. Future work\nthat aims to faithfully measure or prevent memo-\nrization in language models will need to take this\nambiguity into account—for example, our analysis\nsuggests that the fraction of datasets that large lan-\nguage models is likely far larger than the fraction\nas reported in prior work (Carlini et al., 2022).\n2 Background\nLanguage Models. We consider auto-regressive\nlanguage models that operate over sequences of\ntext and, given a preﬁx p, output a probability dis-\ntribution for the next token in the sequence. To\ngenerate text for a prompt p, the language model\nstarts with an empty sufﬁx s, and repeatedly sam-\nples the next token from its prediction on p `s,\nand then appends this token to s. The success of\nneural language models has, in large part, been\ndriven by the transformer architecture introduced\nof Vaswani et al. (2017), which allowed models\nto scale from millions to hundreds of billions of\nparameters over the past half-decade (Brown et al.,\n2020; Chowdhery et al., 2022; Zhang et al., 2022).\nThis increase in model sizes has likewise driven\nincreases in dataset sizes, with most of this data\ncoming from internet crawls (Lee et al., 2021; Raf-\nfel et al., 2020; Gao et al., 2020).1\nPrior work has shown that large language models\ncan memorize and regurgitate potentially private\ninformation, like phone numbers and addresses, as\nwell as memorize long sequences from their train-\ning sets (Carlini et al., 2019, 2021; Lee et al., 2021;\nCarlini et al., 2022; Zhang et al., 2021; Thakkar\net al., 2021; Ramaswamy et al., 2020; Kandpal\net al., 2022). Our work focuses on large language\nmodels trained to generate English text or code,\nand our work does not distinguish between prob-\nlematic memorization (e.g. exposure of private\ninformation) and non-problematic memorization\n(e.g. quoting perfectly from a presidential speech).\nMeasuring Memorization. Many studies of\nmemorization stem from a concern of privacy leak-\nage: if a model memorizes sensitive training data\nand can generate it, then interactions with a model\ncan lead to the leakage of that sensitive data. Nearly\nall of this literature is focused on measuring verba-\ntim cases of memorization.\nEidetic memorization (Carlini et al., 2021) de-\nﬁnes a string s as memorized if there exists a\nprompt p so that LMppq“ s and s is contained in\nthe training dataset. This deﬁnition and variations\nof it have been used widely in the literature (Kand-\npal et al., 2022; Lee et al., 2021; Carlini et al.,\n2022). For example, Tirumala et al. (2022) study a\nsimilar per-token deﬁnition called exact memoriza-\ntion and Kandpal et al. (2022) a document-level\ndeﬁnition called perfect memorization.\nThere is also a newly emerging line of works\nexploring differential-privacy (DP)-based deﬁni-\ntions (Zhao et al., 2022; Stock et al., 2022), which\nrelate to document-level DP guarantees in language\n1A common source for datasets is the Common Crawl\ndataset found at: https://commoncrawl.org/\n30\nmodelling (Yu et al., 2021). These works differ\nfrom the above in that they deﬁne a probabilis-\ntic leakage measure. However, this is based on the\nprobability of generating—verbatim—a canary sen-\ntence s, depending on whether s was contained in\nthe training set or not. There are different prob-\nabilistic deﬁnitions, also based on verbatim se-\nquences, such as the counterfactual memorization\nproposed by Zhang et al. (2021).\nIn the domain of language model memorization,\nthe most similar work to ours is Lee et al. (2021)\nwho also argue for a more relaxed deﬁnition of\nmemorization. Lee et al. say any model output\nfor a prompt p is memorized if it is within some\nchosen edit distance of the prompt’s true continua-\ntion in the training set. As we will discuss, a small\nedit distance may not capture all forms of approxi-\nmate memorization either—such as our examples\nof “style-transfer” applied to memorized content.\nPreventing Memorization. Differentially pri-\nvate training, e.g., using DP stochastic gradient\ndescent (Abadi et al., 2016), is the gold standard for\ntraining models which provably do not memorize\nindividual training examples. However, in practice,\nthese techniques result in worse generative mod-\nels (Anil et al., 2021)—thus, no state-of-the-art,\nlarge, language models are trained with DP. In-\nstead, data deduplication has arisen as a pragmatic\ncountermeasure against data memorization (Lee\net al., 2021; Kandpal et al., 2022; Carlini et al.,\n2022). The core idea is to remove any duplicated\ncontent—e.g., repeated documents—because dupli-\ncated content is much more likely to be memorized.\nHowever, deduplication does not guarantee that a\nmodel will not still memorize individual (dedupli-\ncated) examples, necessitating defenses that oper-\nate at inference-time.\n3 Preventing Models from Emitting\nVerbatim Training Data\nIn this paper, we consider inference-time defenses\nthat eliminate the generation of memorized con-\ntent from the training set. The most immediate\nway to do this is simply to ﬁlter all model outputs\nusing some ﬁxed deﬁnition of memorization. For\nexample, in Carlini et al. (2022), a continuation\ns “ LMppqof a k-length prompt p is said to be\nmemorized if the string s exists verbatim in the\ntraining dataset. A straightforward implementation\nchecks each generation s against the training set\nand rejects any matches. We call the approach of\nre-running a language model, possibly many times\nwith different seeds, until a qualifying generation\nis produced, retroactive censoring.\nThe problem with retroactive censoring is that it\neffectively prevents the model from emitting any\noutput when the model’s conﬁdence in a memo-\nrized string is too high. To encourage a model to\ngenerate novel outputs, we could also adopt a more\ngranular ﬁltering approach: rather than censoring\nmemorized content solely at the level of an en-\ntire sequence s, we could instead check and mark\neach n-gram within s individually. Filtering for\nmemorization at the n-gram-level rather than at the\nsequence level allows substrings of a generation\nwhich may be novel to be kept, and only the pieces\nthat are verbatim memorized to be modiﬁed. We\ncall this approach MEMFREE decoding, as the\ndefense is applied at decoding time.\nBoth retroactive censoring and MEMFREE de-\ncoding explicitly prohibit the model from emitting\na sequence if it is contained (entirely or partially) in\nthe training dataset. However, in retroactive censor-\ning, if a generation starts off with memorized text,\nbut then veers off track from the true continuation\n(a common occurrence), this would not be marked\nas memorization, even though a portion of the out-\nput sequence is clearly memorized. TheMEMFREE\ndecoding approach performs a more ﬁne-grained\nand aggressive check by ﬁltering out all memorized\nsubsequences of a given length. In this work we\nuse the MEMFREE decoding approach to show that\neven when a model is restricted from emitting any\noutput with snippets of verbatim memorization, the\nmodel can still leak training data.\n3.1 M EMFREE Decoding Details\nIn order to implement MEMFREE decoding, we\nalter the model’s generation in an online manner by\nrestricting the production of tokens which would\nresult in an n-gram memorization. Let p be the\ncurrent working preﬁx and t be the next proposed\ntoken when running the model forward.\nOur algorithm ﬁrst checks if any n-gram in the\nconcatenated sequence p||t is contained in the train-\ning dataset D. If it is, we suppress this generated\ntoken and re-sample from the model. To avoid po-\ntentially expensive resamplings, we equivalently\nexpress this as altering the model’s output probabil-\nity distribution by removing the probability mass\nfrom token t. In this way, we guarantee that prior\nto sampling the probability of outputting a mem-\n31\norization will be 0. Appendix B.1 gives a formal\nprocedure for this method.\nAltering the token posterior allows any sampling\nstrategy to be used on top of memorization-free\ndecoding. For example, if one uses top-k sampling,\ntokens that result in memorization are disqualiﬁed\nbefore the probability distribution is truncated to\nthe k next most likely tokens. This procedure is\nguaranteed to generate non-memorized text.\n3.2 Querying the Training Set Efﬁciently\nOur MEMFREE defense has assumed that it is easy\nto perform the query s P D to test if any given\nstring is contained in the training dataset. Because\nthe defense works at inference-time, it is neces-\nsary that this query is computationally efﬁcient to\nmaintain utility of the language model. Given that\ntraining sets may contain terabytes of data (Brown\net al., 2020), it is infeasible to maintain an entire\ncopy of the training dataset in an efﬁciently acces-\nsible storage. Thus, we explore three optimizations\nto speed up the process of memorization checking.\nFirst, as a direct result of our n-gram memoriza-\ntion deﬁnition, we can equivalently check only the\nn-gram ending in the current predicted token t; we\ncan thus avoid manyn-gram queries for each token.\nFurther, and in addition to preventing subsequence\nmemorization, this allows us to avoid queries into\na large set of all preﬁxes and continuations.\nSecond, we only check against sequences that\nhave a reasonable probability of being memorized\nby the model. In theory, this could be easily de-\ntermined by running each n-gram s PDthrough\nthe model and then ﬁltering out all sequences with\nhigh loss (thus unlikely to be memorized). How-\never, this is a computationally expensive procedure\nas it requires re-processing every substring of the\ntraining dataset. Instead, a computationally- and\nstorage-efﬁcient procedure could be to only store\nn-grams which occur more than once in the train-\ning set—prior work has shown duplicate text is\nthe most likely to be memorized (Lee et al., 2021;\nKandpal et al., 2022).\nThird, by being willing to tolerate some false\npositives (labeling an n-gram as memorized when\nit is in fact not), we can take advantage of prob-\nabilistic data structures such as Bloom ﬁlters\n(Bloom, 1970), which admits no false negatives\nbut trades off time and space with the false positive\nrate (which can be computed exactly). Thus, by\nusing a Bloom Filter, we guarantee that no mem-\norized n-gram will ever be released (i.e., a false\nnegative) but we may (rarely) prevent the emission\nof non-memorized content (i.e., a false positive).\nIntegrating a Bloom Filter into our defense is\nstraightforward. Let Ffp pDnqrepresent the Bloom\nFilter of dataset D, generated by adding each n-\ngram of the dataset s P Dn to the Bloom ﬁlter,\nwith false positive rate fp. Then, any memoriza-\ntion check s PDn in Algorithm 1 can be replaced\nwith s PFfp pDnq. The Bloom ﬁlter can be gen-\nerated with a single pass over the model’s training\nset, which could be performed in parallel with one\nepoch of model training.\nAdditional Parameters. We must choose an ap-\npropriate false positive rate based on memory con-\nstraints and the chosen n-gram length. Choosing n\nhas two major impacts: on the population size (i.e.,\nthe number of unique n-grams) and thus the size of\nthe ﬁlter, and on the effectiveness of memorization\nmitigation. If n is set too low, then we will cer-\ntainly prevent all memorized sequences but might\nalso prevent too many common phrases. But if we\nset n too high, we might not prevent actually mem-\norized sequences from being emitted by the model.\nWe discuss these tensions in Appendix B, along\nwith two additional takeaways: (1) that MEMFREE\ndoes not impact downstream model performance\n(which may result from false positives), and (2)\nthat our chosen optimizations maintain a suitably\nlow false negative rate (we observed a 3000x im-\nprovement). These optimizations led to a ﬁlter of\nsize 1.6 gigabytes (or, 40.5 gigabytes if all, even\nnon-duplicated, 10-grams were stored) when run\nover the 800GB Pile dataset.\n3.3 Measuring Approximate Memorization\nTo show that defenses against verbatim memoriza-\ntion still allow approximate memorization, we need\na deﬁnition for approximate memorization. We\nconsider two deﬁnitions. First, drawing from stan-\ndard NLP evaluation techniques, we measure the\nBLEU score between the generated and ground-\ntruth continuations. Second, we measure the length-\nnormalized character-level Levenshtein similarity\nbetween the generated and ground-truth continua-\ntions. Appendix C.1 gives implementation details.\nIn Section 5, we investigates how these two simi-\nlarity metrics decrease with MEMFREE decoding.\nFor situations requiring a binary label of whether\napproximate memorization has occurred, we use\nthe following deﬁnition: a sufﬁx s for preﬁx p is\n32\nStandard prompting with original preﬁx and format\nfloat Q_rsqrt( float number )\n{\nl o n g i ;\nf l o a t x2 , y ;\nc o n s t f l o a t t h r e e h a l f s = 1 . 5 F ;\nx2 = number ∗ 0 . 5 F ;\ny = number ;\ni = ∗ ( l o n g ∗ ) &y ;\nCopilot no longer generates continuations\nPrompt with Python-style comment\n# float Q_rsqrt( float number )\n# {\n# long i;\n# float x2 , y;\n# const float threehalfs = 1.5F;\n#\n# x2 = number * 0.5F;\n# y = number;\n# i = * ( long * ) &y;\n# i = 0 x5f3759df - ( i >> 1 );\n# y = * ( float * ) &i;\n# y = y * ( threehalfs - (x2 *y*y));\n#\n# return y;\n# }\nPrompt with French translation (alternate naming\nconvention)\nfloat Q_sqrt( float nombre )\n{\nl o n g i ;\nf l o a t x2 , y ;\nc o n s t f l o a t t r o i s _ m o i t i e = 1 . 5 F ;\nx2 = nombre ∗ 0 . 5 F ;\ny = nombre ;\ni = ∗ ( l o n g ∗ ) &y ;\ni = 0 x 5 f 3 7 5 9 d f ´ ( i >> 1 )\ny = ∗ ( f l o a t ∗ ) &i ;\ny = y ∗ ( t r o i s _ m o i t i e´ ( x2∗y∗y ) ) ;\n/ / y = y ∗ ( t r o i s _ m o i t i e´ ( x2∗y∗y ) ) ;\nr e t u r n nombre ∗ y ;\n}\nFigure 2: Honest “style-transfer” prompts evade\nverbatim memorization ﬁlters. Trivially modifying\nprompts causes GitHub’s Copilot language model to\nemit memorized, but not verbatim, content. Prompts\nhighlighted in blue. Model evaluated with the option\n“block suggestions matching public code” enabled. For\nbrevity, we removed comments from model outputs.\nlabeled as memorized if for generation g “fppq,\nBLEUpg, sqą 0.75. This threshold was chosen by\nqualitatively inspecting examples. Several exam-\nple generations that are close to this threshold are\nshown in Table A12.\nWhen we repeat the preﬁx-extraction experiment\nfrom (Carlini et al., 2022) to measure incidents of\ngenerations that could be considered memorized,\nbut using this approximate deﬁnition instead of a\nverbatim one, we ﬁnd that hat prior literature has\nsigniﬁcantly underestimated memorization leak-\nage. In Figure 3, the shaded region represents the\nfraction of memorized samples that would have by-\npassed a verbatim memorization ﬁlter: in the worst\ncase, there is a factor-of-two increase.\n120M 345M 762M 1.5B 2.7B 6B\nModel Size\n0.2\n0.4\n0.6\n0.8\n1\nFraction memorized\nGPT-3\nApprox\nExact\nGPT-Neo\nApprox\nExact\nFigure 3: Signiﬁcantly more examples are approxi-\nmately memorized (BLEU > 0.75) than are found to\nbe exactly memorized by Carlini et al. (2022) . This\nis for undefended generation.\nHowever, we caution that this deﬁnition of ap-\nproximate memorization is inaccurate, potentially\nboth over and under counting approximate memo-\nrization. While our choice of a 0.75 BLEU score\nthreshold shows a signiﬁcant increase in approxi-\nmate vs. verbatim memorization, it is not clear that\nall identiﬁed cases of memorization would be per-\nceptually tagged as such by a human judge. This is\none reason why simply switching to this deﬁnition\nfor defenses may not be ideal—it could introduce\nsigniﬁcant false positives.\n4 Evading Verbatim Memorization\nDefenses\nIn this section, we show how retroactive censoring\nof verbatim memorization can be evaded, even in\nsettings where models are used honestly. We ﬁrst\npresent a case study with Copilot, which has im-\nplemented retroactive censoring in production. We\nthen show how a large English language models\nlike GPT-3 and PaLM are susceptible to the same\nvulnerability, should a defense similar to Copilot’s\nbe deployed. In short, protecting against verbatim\nmemorization can lead to a false sense of privacy.\n4.1 Evading Copilot’s Memorization Filter\nCopilot is a code auto-complete service which\nis trained on GitHub code. Copilot is built us-\ning the Codex language model designed by Ope-\nnAI (Chen et al., 2021). To prevent generating\nmemorized code, Copilot uses a ﬁltering mecha-\nnism that blocks model outputs from being sug-\ngested if they overlap signiﬁcantly (approximately\n150 characters) with a training example. This is a\npractical example of a ﬁlter that aims at prevent-\ning perfect verbatim memorization, presumably\nby using a procedure similar to Algorithm 1 (the\n33\nexact mechanism used by GitHub is not public).\nHowever, we ﬁnd that the ﬁlter fails to prevent the\nleakage of training data in many settings.\nStyle-transfer prompting. In Figure 2, we show\nthat Copilot’s ﬁlter can easily be bypassed by\nprompts that apply various forms of “style-transfer”\nto model outputs, thereby causing the model to\nproduce memorized (but not verbatim) outputs.\nAs a concrete example, we demonstrate how to\nextract the public code for Quake’s “Fast Inverse\nSquare Root”. If we naively prompt the model\nwith the function deﬁnition “float Q_rsqrt (\nfloat number )”, Copilot correctly aborts gener-\nation of the full function (“standard prompting”).\nHowever, we ﬁnd that simple style-transfers\napplied to the prompt allow us to easily bypass\nCopilot’s restrictions. First, via prompting with\n“Python-style comments” we begin our prompt with\nPython’s comment character “#”. Even though this\nis syntactically invalid C code, Copilot outputs the\nentire verbatim fast inverse square root algorithm,\nbut commented out. Second, in prompting with\n“French translations” we change the naming con-\nvention to French. As a result, the generations fol-\nlow the new naming convention and are no longer\nﬂagged as a verbatim match. Other naming con-\nventions, such as pre-pending “_” to the variable\nor changing the language to Spanish, also work.\nThese strategies work because the Copilot model\nis sufﬁciently powerful: it can both follow the style-\ntransfer prompt (by e.g., renaming variables) while\nsimultaneously regurgitating memorized training\ndata. We provide more examples in Appendix F.\nCopilot evades its own ﬁlter. Not only do ac-\ntively style-transfered prompts evade the verbatim\nmemorization ﬁlter, but even passively prompting\nCopilot with highly duplicated text from the Pile\ndataset can too. We ﬁnd several examples where\nCopilot evades its own ﬁlter to output memorized\ntext, some of which we show in Figure 5. We see\nthat Copilot evades the ﬁlter by (1) changing cap-\nitalization, (2) making small non-stylistic errors,\nand (3) changing whitespaces. The latter evasion\n(changing whitespaces) is surprising, as Copilot’s\ndocumentation reports ignoring whitespace in its\nﬁltering mechanism (Appendix A). However, we\nhypothesize that this can be explained by the model\nreplacing tabs with space characters. We can verify\nthis by adding tabs to the beginning of each line\nof the Q_sqrt function, as an application of our\nSpeeches(n=400)Monologues(n=240)OS Licenses(n=168)\nNovels\n(n=308)Lyrics 2011(n=400)Lyrics 2021(n=400)\nDatasets\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Proportion Memorized\noriginal\n(n=479)\nspaces\n(n=479)\nlower\n(n=479)\ncaps\n(n=479)\nStyle\nGPT-3 DaVinci Original\nGPT-3 DaVinci v2\nPaLM 62B\nPaLM 540B\nFigure 4: Fraction of prompts which discover approx-\nimate memorization, grouped by domain (left) and by\nstyle transfer applied (right). We tested two versions\nof GPT-3 DaVinci and two sizes of PaLM. Full plot in\nAppendix D.\nstyle-transfer strategy.\n4.2 English Language Models\nFollowing our analysis of Copilot, we ask whether\nthis vulnerability is pervasive in other language\nmodels too. We use API access to four large (En-\nglish) language models—GPT-3 Davinci Original\nand V2 and PaLM 62B and 540B–to test whether\nthey would be susceptible to style transfer of the\nprompt. We assume that the training sets for these\nmodels are unknown and prompt with documents\nwe believe are likely to have been memorized:\nopen-source licenses, famous speeches and mono-\nlogues, novel openings, and song lyrics. For each\ndocument, we prompt the model with 100 words\nof either (1) the original document (“base”), (2)\nthe document with all spaces doubled (“spaces”),\n(3) the document in all lowercase (“lower”), and\n(4) the document in all uppercase (“caps”). We\nreport approximate memorization results of this\nexperiment in Figure 4, with additional ﬁgures in\nAppendix D.\nWe see that even when prompting with style-\ntransfered prompts, GPT-3 and PaLM are still of-\nten able to generate memorized continuations. De-\nfenses for verbatim memorization are therefore in-\ncomplete. Among the three techniques, uppercas-\ning was the least likely to lead to memorized gen-\nerations. For the two PaLM models, the larger one\nis much more capable of memorization than the\nsmaller one, which validates prior work (Carlini\net al., 2022). The two version GPT-3 are purport-\nedly the same size model but have quite different\nmemorization tendencies. For example, V2 is much\n34\n1) Misspelling and changed capitalization\nThis program is free software\" ; you can redistributeredistribut i t and / o r ∗\nmodify i t u n d e r t h e t e r m s o f t h e GNU G e n e r a l P u b l i c L i c e n s e\n∗ asAS p u b l i s h e d by t h e F r e e S o f t w a r e F o u n d a t i o n ; e i t h e r\nv e r s i o n 2 ∗ o f t h e L i c e n s e , orOR ( a t your o p t i o n ) any l a t e r\nv e r s i o n \"\n2) Small non-stylistic errors\n@aws-sdk/protocol-http\" ;\nimport { Command as $Command } from \"@aws-sdk/smithy-client\";\nimport {FinalizeHandlerArguments,Handler,HandlerExecutionContext,\nMiddlewareStack,HttpHandlerOptions} as __HttpHandlerOptions, M e t a d a t a B e a r e r\na s __ MetadataBearerMetadataBear ,\n3) Changed whitespace\n[...]IPV6_2292PKTINFO(2 ws)\\t\\t\\t\\t\\t = 0x2\\nIPV6_2292PKTOPTIONS \\t\\t\\t\\t = 0x6\\n\nIPV6_2292RTHDR [20 spaces][9 spaces]= 0x5 \\ n\nFigure 5: CoPilot can “cheat” and emit nearly verba-\ntim memorized content. Here, we show prompts from\nthe training set, where the model makes slight errors\ncausing the continuations to pass the ﬁlter. Prompts are\nin cyan, followed by CoPilot’s continuation where er-\nrors are highlighted as model’s generation in orange\nwith the correct characters in green.\nmore susceptible to the “double spaces” style trans-\nfer than the Original Davinci. This emphasizes the\nimportance of models’ training set compositions\nand training methods on memorization tendencies.\n5 M EMFREE Decoding Experiments\nIn this section, we study the effectiveness of our\nproposed MEMFREE decoding defense from Sec-\ntion 3.3, and the appropriateness of our proposed\ndeﬁnition of approximate memorization.\n5.1 Experimental Design\nIt is not possible to apply MEMFREE to the models\nfrom Section 4 since their training sets are non-\npublic. Instead, we turn to the GPT-Neo languge\nmodel family (Black et al., 2021). These models\nare trained on the Pile, a publicly available 825GB\ndataset (Gao et al., 2020). We build a Bloom ﬁlter\nover all 10-grams occur 10 or more times. 2 In\nall experiments, we generate text using arg max\ndecoding as the sampling method. We investigate\nfour model sizes: 125M–6B parameters.\nWe evaluate using substrings of the Pile released\nby Carlini et al. (2022). The dataset includes 30k\nstrings of length 150 tokens taken from the training\nset. These are divided into 30 buckets of 1k strings,\nsampled such that the strings in bucket i occur in\n2Note that the choice of n=10 for the n-gram size is very\nconservative, and common phrases that happen to be com-\nposed of 10+ tokens will get ﬁltered out by this check. We\ndiscuss why we chose these particular values in Appendix B.\n0.0 0.5 1.0\nUndefended\n0.0\n0.5\n1.0MemFree\n(a) BLEU (word-level)\n0.0 0.5 1.0\nUndefended\n0.0\n0.5\n1.0MemFree\n (b) Edit similarity (char-level)\nFigure 6: MEMFREE reduces similarity when the\ncontinuation would have been highly similar to the\nground-truth, and has little impact otherwise. For\n5,000 prompts, we plot the similarity of the groundtruth\ncontinuation with the generation from M EMFREE (y-\naxis) and with the undefended generation (x-axis). Gen-\nerations on the diagonal were not memorized.\nthe Pile between 2i{4 and 2pi`1q{4 times. For each\nstring, we use the ﬁrst 50 tokens as a prompt p and\ngenerate a 50-token long continuation.\n5.2 Reduction in Memorization\nMEMFREE signiﬁcantly reduces the similarity of\ngenerations to the groundtruth, compared to per-\nforming undefended generation (Figure 6). We\nalso observe that when undefended generation al-\nready results in low similarity with the groundtruth,\nMEMFREE does not signiﬁcantly alter the genera-\ntions, as desired.\nPrevious work shows that increasing model size\nincreases discoverable memorization (Carlini et al.,\n2022; Kandpal et al., 2022). We again ﬁnd a clear\ntrend that generations from larger models have, on\naverage, a much higher similarity with the original\ncontinuation (Figure 8). Despite this, MEMFREE\nremains effective at all model sizes (BLEU remains\nnear-ﬂat around 0.6). Even when a sequence has\nmany duplicates in the train set (a strong indica-\ntor of memorization), MEMFREE signiﬁcantly de-\ncreases similarity with the groundtruth at all model\nsizes (Figure 7).\n5.3 Failures in Preventing Memorization\nA defense against memorization fails when it al-\nlows a sequence to be generated which a human\nwould perceive as substantially copied from the\ntrue continuation—even if it is not verbatim mem-\norized. This failure case can be seen as the points\nwhere the MEMFREE generation is still a close\nmatch to the ground-truth continuation (Figure 6).\nIt occurs because the defense only adjusted a few\n35\n101 102\nDuplicate Count\n0.3\n0.2\n0.1\nMean BLEU\nScore Change\n6B\n2.7B\n1.3B\n125M\nFigure 7: MEMFREE decreases the BLEU score of\ngenerations more for highly duplicated examples.\n125M 1.3B 2.7B 6B125M 1.3B 2.7B 6B\nSize\n0.4\n0.6\n0.8\nBLEU score between\ngenerated and true\ncontinuations\n Undefended\nMemFree\nFigure 8: MEMFREE remains effective at reducing\nsimilarity between the generated and groundtruth\ncontinuations even as models grow larger.,\ntokens (e.g., 1 after every sequence of 10). When\nlooking at these examples, many, but not all, are\nlists of numbers. Some examples are included in\nTable A17. There is also a second failure-case:\nwhen a full ( 50 token) generation is made more\nsimilar with the ground-truth by MEMFREE (on\n10-grams) than without. This may happen depend-\ning on the model’s token posterior’s after removing\nall tokens that fail the MEMFREE check. Almost\nall of these cases had a trivial increase in similar-\nity. However, 0.16% of samples had a similarity\nincrease above 0.1. We found qualitatively that\nmany of these cases did have signiﬁcant overlap\nwith the true continuation.\n6 Discussion\nDeﬁning memorization in language models.\nWhile verbatim deﬁnitions have helped discover\nsigniﬁcant memorization in large language models,\nthey are insufﬁcient to capture more subtle forms\nof memorization. Our work highlights two such\nsituations: \"style-transfer\" prompting, where de-\nfenses for verbatim memorization can be actively\nsubverted, and when models “cheat” by outputting\nsimilar, but not verbatim, continuations. As a result,\nour work suggests that memorization prevention\nmust capture these types of paraphrased memo-\nrizations in addition to the previously considered\nverbatim deﬁnitions. However, exhaustively an-\nticipating styles to incorporate into defenses is an\ninnumerable problem that will become harder as\nmodels become more powerful.\nThis emphasizes two major challenges in deﬁn-\ning approximate memorization. First, since new\napproximate cases must be discoverable by the def-\ninition, this can result in a cat-and-mouse game.\nSecond, the deﬁnition of memorization is domain-\ndependent. For example, our paper focuses on lan-\nguage models trained to output English and code,\nwhich each have different standards for what it\nmeans to memorize. Other languages will require\ndifferent considerations when deﬁning memoriza-\ntion.\nThere are a few areas of research which may help\nin improving memorization deﬁnitions. The ﬁeld of\nimage generation memorization is already com-\nfortable with measuring fuzzy (in our terms, approx-\nimate) memorization, where generated items may\nbe perceptually similar to training set examples,\ndespite having high distance according to standard\nmetrics. For example, Fredrikson et al. (2015) con-\nsider “model inversion”, where an image is suc-\ncessfully recovered from the model if it is identi-\nﬁable to a human worker. In Zhang et al. (2020),\nmodel inversion success is measured based on pixel\nsimilarity and feature space similarity to training\nimages. These works also recover “representative”\nimages from different classes, rather than speciﬁc\ntraining examples. Recent work on reconstructing\ntraining images have used feature similarity (Haim\net al., 2022) and pixel similarity (Balle et al., 2022).\nIn each of these papers, “fuzzy” reconstructions are\nallowed by the evaluation metrics and, indeed, are\ncommon in their reconstructions.\nThe inherent limitations of verbatim deﬁnitions\nof text regurgitation have also been well docu-\nmented in the literature on plagiarism detection—\nboth for text and code. Existing plagiarism tools,\nand their evaluations, go far beyond verbatim\nmatches and consider fuzzy data “clones” ranging\nfrom simple transformations (e.g., word variations\nor shufﬂes) to arbitrary semantics-preserving para-\nphrasing (Roy et al., 2009; Potthast et al., 2010).\nRe-purposing techniques from the plagiarism de-\ntection literature to minimize generation of mem-\norized data in LLMs is an interesting direction to-\nward achieving better approximate memorization\ndeﬁnitions in machine learning.\n36\nConsequences for machine learning research.\nIn relaxing deﬁnitions of memorization, our paper\nacknowledges the blurred line between memoriza-\ntion (e.g., of personal information) and knowledge\n(e.g., of common facts). Because we use a 10-gram\noverlap, our MEMFREE decoding algorithm should\nnot signiﬁcantly impact utility, however studying\nthis interplay is an important area of future work.\nHowever, still, identifying which data is considered\n“memorized” cannot be done only by looking for\nverbatim reproductions of the training set. This\nmay make the task of understanding memorization\nand generalization more difﬁcult.\nWe do not believe that our work requires aban-\ndoning all research directions which rely on prior\nverbatim deﬁnitions. These deﬁnitions are still\nuseful as an efﬁcient way to test for obvious and\nundeniable memorization. However it will be nec-\nessary to continue studying further relaxations of\nmemorization deﬁnitions to adequately capture and\nmeasure the space of privacy concerns for language\nmodels.\n7 Ethics & Broader Impact\nImproving the privacy of neural language models—\nand especially those trained on user data—is an\nimportant and timely research problem. In this\npaper we hope to help both researchers and practi-\ntioners develop a more nuanced understanding of\nwhat constitutes memorization in language mod-\nels. In particular, just because a sequence does\nnot appear verbatim in a training dataset does not\nmean the example is a novel generation: as we have\nshown, models today are sufﬁciently powerful to\nminimally transform memorized data to make it\nappear superﬁcially different even if the underlying\ncontent remains memorized.\nOur observation will complicate the privacy eval-\nuation of future machine learning models. It should\nno longer be deemed sufﬁcient to check for (ver-\nbatim) matches between generated output and a\ntraining example. Practitioners in the future will\nneed to be aware of this potential failure mode\nwhen applying output post-processing defenses to\nmitigate memorization. To the best of our knowl-\nedge, the only deployed system affected by our\nanalysis is GitHub’s Copilot. In order to mitigate\nharm here we shared a copy of our paper with the\nrelevant researchers at both GitHub and OpenAI\nprior to paper submission.\nIn this paper we focus our efforts entirely on\npublic datasets that other researchers have exten-\nsively studied (Gao et al., 2020) to minimize any\nharm caused by demonstrating extraction results.\nHowever, just because the data that we study is\npublic does not mean there are no privacy concerns.\nAs Brown et al. (2022) argue, there are many other\nconsiderations when discussing the privacy of large\nmodels trained on “public” datasets.\nContributions\n•Daphne Ippolito posed the idea of memory-\nfree decoding using a bloom ﬁlter as a solution\nto memorization, worked on the MEMFREE\nimplementation, ran experiments with GPT-3\nand PaLM, and contributed to paper writing.\n•Christopher Choquette analyzed how MEM-\nFREE used the bloom ﬁlter, created ﬁgures,\nand contributed to paper writing.\n•Matthew Jagielski qualitatively analyzed gen-\nerations from MEMFREE , created ﬁgures, and\ncontributed to paper writing.\n•Katherine Lee led ﬁgure-making, contributed\nto paper writing, and resolved TODOs.\n•Milad Nasr ran experiments with Copilot and\ncontributed to paper writing.\n•Florian Tramèr came up with the idea of style\ntransferring prompts and contributed to paper\nwriting.\n•Chiyuan Zhang generated ﬁgures, prepared\nqualitative examples, and contributed to paper\nwriting.\n•Nicholas Carlini identiﬁed the weaknesses in\nmemory-free decoding, worked on the MEM-\nFREE implementation, ran experiments with\nGPT-Neo, and contributed to paper writing.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. In Proceedings of the 2016 ACM SIGSAC con-\nference on computer and communications security ,\npages 308–318.\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi\nKumar, and Pasin Manurangsi. 2021. Large-\nscale differentially private BERT. arXiv preprint\narXiv:2108.01624.\nBorja Balle, Giovanni Cherubin, and Jamie Hayes.\n2022. Reconstructing training data with informed\nadversaries. arXiv preprint arXiv:2201.04845.\n37\nMichael A Bender, Martin Farach-Colton, Mayank\nGoswami, Rob Johnson, Samuel McCauley, and\nShikha Singh. 2018. Bloom ﬁlters, adaptivity, and\nthe dictionary problem. In 2018 IEEE 59th An-\nnual Symposium on Foundations of Computer Sci-\nence (FOCS), pages 182–193. IEEE.\nStella Biderman, Hailey Schoelkopf, Quentin An-\nthony, Herbie Bradley, Kyle O’Brien, Eric Halla-\nhan, Mohammad Aﬂah Khan, Shivanshu Purohit,\nUSVSN Sai Prashanth, Edward Raff, et al. 2023.\nPythia: A suite for analyzing large language mod-\nels across training and scaling. arXiv preprint\narXiv:2304.01373.\nSteven Bird, Ewan Klein, and Edward Loper. 2009.\nNatural language processing with Python: analyz-\ning text with the natural language toolkit. \" O’Reilly\nMedia, Inc.\".\nSid Black, Leo Gao, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. Gpt-neo: Large scale autore-\ngressive language modeling with mesh-tensorﬂow.\nIf you use this software, please cite it using these\nmetadata, 58.\nBurton H Bloom. 1970. Space/time trade-offs in hash\ncoding with allowable errors. Communications of\nthe ACM, 13(7):422–426.\nHannah Brown, Katherine Lee, Fatemehsadat\nMireshghallah, Reza Shokri, and Florian Tramèr.\n2022. What does it mean for a language model to\npreserve privacy? Seoul, Korean. ACM Conference\non Fairness, Accountability, and Transparency.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nJehoshua Bruck, Jie Gao, and Anxiao Jiang. 2006.\nWeighted Bloom ﬁlter. In 2006 IEEE International\nSymposium on Information Theory , pages 2304–\n2308. IEEE.\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski,\nKatherine Lee, Florian Tramer, and Chiyuan Zhang.\n2022. Quantifying memorization across neural lan-\nguage models. arXiv preprint arXiv:2202.07646.\nNicholas Carlini, Chang Liu, Úlfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neu-\nral networks. In 28th USENIX Security Symposium\n(USENIX Security 19), pages 267–284.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ul-\nfar Erlingsson, et al. 2021. Extracting training data\nfrom large language models. In 30th USENIX Secu-\nrity Symposium (USENIX Security 21), pages 2633–\n2650.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarri Edwards, Yuri Burda, Nicholas Joseph, Greg\nBrockman, et al. 2021. Evaluating large lan-\nguage models trained on code. arXiv preprint\narXiv:2107.03374.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nOndˇrej Dušek, David M Howcroft, and Verena Rieser.\n2019. Semantic noise matters for neural natural lan-\nguage generation. In Proceedings of the 12th Inter-\nnational Conference on Natural Language Genera-\ntion, pages 421–426.\nOndrej Dušek and Filip Jurcıcek. 2016. A context-\naware natural language generation dataset for dia-\nlogue systems. In Workshop on collecting and gen-\nerating resources for chatbots and conversational\nagents-development and evaluation, pages 6–9.\nMatt Fredrikson, Somesh Jha, and Thomas Ristenpart.\n2015. Model inversion attacks that exploit conﬁ-\ndence information and basic countermeasures. In\nProceedings of the 22nd ACM SIGSAC conference\non computer and communications security , pages\n1322–1333.\nLeo Gao, Stella Biderman, Sid Black, Laurence Gold-\ning, Travis Hoppe, Charles Foster, Jason Phang, Ho-\nrace He, Anish Thite, Noa Nabeshima, et al. 2020.\nThe Pile: An 800GB dataset of diverse text for lan-\nguage modeling. arXiv preprint arXiv:2101.00027.\nSebastian Gehrmann, Tosin Adewumi, Karmanya\nAggarwal, Pawan Sasanka Ammanamanchi,\nAnuoluwapo Aremu, Antoine Bosselut, Khy-\nathi Raghavi Chandu, Miruna Clinciu, Dipanjan\nDas, Kaustubh Dhole, et al. 2021. The gem bench-\nmark: Natural language generation, its evaluation\nand metrics. In Proceedings of the 1st Workshop\non Natural Language Generation, Evaluation, and\nMetrics (GEM 2021), pages 96–120.\nGitHub. 2022. About GitHub Copilot.\nhttps://docs.github.com/en/copilot/overview-of-\ngithub-copilot/about-github-copilot.\nNiv Haim, Gal Vardi, Gilad Yehudai, Ohad Shamir,\nand Michal Irani. 2022. Reconstructing training\ndata from trained neural networks. arXiv preprint\narXiv:2206.07758.\nNikhil Kandpal, Eric Wallace, and Colin Raffel.\n2022. Deduplicating training data mitigates pri-\nvacy risks in language models. arXiv preprint\narXiv:2202.06539.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2021. Deduplicating training\n38\ndata makes language models better. arXiv preprint\narXiv:2107.06499.\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit\nRau, Abhinand Sivaprasad, Chiachun Hsieh, Xian-\ngru Tang, Aadit Vyas, Neha Verma, Pranav Krishna,\net al. 2021. Dart: Open-domain structured data\nrecord to text generation. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 432–447.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann,\nManaal Faruqui, Bhuwan Dhingra, Diyi Yang, and\nDipanjan Das. 2020. Totto: A controlled table-to-\ntext generation dataset. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 1173–1186.\nMartin Potthast, Benno Stein, Alberto Barrón-Cedeño,\nand Paolo Rosso. 2010. An evaluation framework\nfor plagiarism detection. In Coling 2010: Posters ,\npages 997–1005.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nSwaroop Ramaswamy, Om Thakkar, Rajiv Mathews,\nGalen Andrew, H Brendan McMahan, and Françoise\nBeaufays. 2020. Training production language mod-\nels without memorizing user data. arXiv preprint\narXiv:2009.10031.\nChanchal K Roy, James R Cordy, and Rainer Koschke.\n2009. Comparison and evaluation of code clone de-\ntection techniques and tools: A qualitative approach.\nScience of computer programming, 74(7):470–495.\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier,\nBenjamin Piwowarski, and Jacopo Staiano. 2020.\nMlsum: The multilingual summarization corpus. In\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP) , pages 8051–8067.\nAssociation for Computational Linguistics.\nPierre Stock, Igor Shilov, Ilya Mironov, and Alexandre\nSablayrolles. 2022. Defending against reconstruc-\ntion attacks with Rényi differential privacy. arXiv\npreprint arXiv:2202.07623.\nSasu Tarkoma, Christian Esteve Rothenberg, and Eemil\nLagerspetz. 2011. Theory and practice of bloom ﬁl-\nters for distributed systems. IEEE Communications\nSurveys & Tutorials, 14(1):131–155.\nOm Dipakbhai Thakkar, Swaroop Ramaswamy, Rajiv\nMathews, and Francoise Beaufays. 2021. Under-\nstanding unintended memorization in language mod-\nels under federated learning. In Proceedings of the\nThird Workshop on Privacy in Natural Language\nProcessing, pages 1–10.\nKushal Tirumala, Aram H Markosyan, Luke Zettle-\nmoyer, and Armen Aghajanyan. 2022. Memoriza-\ntion without overﬁtting: Analyzing the training dy-\nnamics of large language models. arXiv preprint\narXiv:2205.10770.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information process-\ning systems, 30.\nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi,\nHuseyin A Inan, Gautam Kamath, Janardhan Kulka-\nrni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz,\net al. 2021. Differentially private ﬁne-tuning of lan-\nguage models. arXiv preprint arXiv:2110.06500.\nChiyuan Zhang, Daphne Ippolito, Katherine Lee,\nMatthew Jagielski, Florian Tramèr, and Nicholas\nCarlini. 2021. Counterfactual memorization\nin neural language models. arXiv preprint\narXiv:2112.12938.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022. Opt: Open pre-trained transformer language\nmodels. arXiv preprint arXiv:2205.01068.\nYuheng Zhang, Ruoxi Jia, Hengzhi Pei, Wenxiao\nWang, Bo Li, and Dawn Song. 2020. The se-\ncret revealer: Generative model-inversion attacks\nagainst deep neural networks. In Proceedings of the\nIEEE/CVF conference on computer vision and pat-\ntern recognition, pages 253–261.\nXuandong Zhao, Lei Li, and Yu-Xiang Wang. 2022.\nProvably conﬁdential language modelling. arXiv\npreprint arXiv:2205.01863.\n39\nA GitHub Copilot\nAt the time of this paper’s writing, GitHub Copilot’s memorization prevention mechanism is described in\ntheir FAQ athttps://github.com/features/copilot. We copy the text here:\nWhat can I do to reduce GitHub Copilot’s suggestion of code that matches\npublic code?\nWe built a filter to help detect and suppress the rare instances where a\nGitHub Copilot suggestion contains code that matches public code on GitHub.\nYou have the choice to turn that filter on or off during setup. With the\nfilter on, GitHub Copilot checks code suggestions with its surrounding code\nfor matches or near matches (ignoring whitespace) against public code on\nGitHub of about 150 characters. If there is a match, the suggestion will not\nbe shown to you. We plan on continuing to evolve this approach and welcome\nfeedback and comment.\nB Further Discussion of M EMFREE\nB.1 Formal Procedure\nAlgorithm 1 provides a formal procedure for MEMFREE decoding. In all our experiments, we used\narg maxdecoding as the sampling method for line 4.\nAlgorithm 1 MEMFREE decoding algorithm.\n1: procedure GREEDY MEMFREE DECODING (language model f, preﬁx p, gen length n, training set D)\n2: repeat\n3: logits Ðfppq´8¨t 1 rpp||tqP Ds: t Pvocabu\n4: tok Ðsample from logits\n5: p Ðp||tok\n6: until n iterations\n7: end procedure\nB.2 Choice of n-gram length\nThere are two tradeoffs to consider when choosing an n-gram length: the choice of n changes the\ngranularity of the memorization checking and the total number of substrings of the dataset that must be\nstored in the Bloom ﬁlter. with respect to the former, notice that short n-grams do not have sufﬁcient\nnovelty (loosely, entropy) to be considered memorizations, e.g., they are often commons words and\nphrases. However, too large also would not capture shorter sequences that have sufﬁcient novelty. On the\nlatter, notice that the universe of possible n-grams is exponential in n, but that the unique number of such\nsequences in a ﬁxed dataset may decrease with large n. This total number of unique sequences impacts\nthe required size of the Bloom ﬁlter to maintain a ﬁxed false positive rate. With N the number of unique\nn-grams and fp a decimal probability of the false positive rate, the size of the ﬁlter in bits is:\nm “\nR´pN ˚log pfpqq\nlog p2q2\nV\n.\nThen, k the number of Bloom hash functions can be calculated from the number of bits per element, i.e.,\nm{N, as:\nk “rppm{Nq˚ logp2qqs.\nThis determines the cost of inserting and looking up into the Bloom ﬁlter asOpkq. But, because k typically\nremains small (in our case, k “7), this can be treated as a small constant-time operation. See Tarkoma\net al. (2011) for the full calculations, which the ones listed here are taken from.\nWe err on the side of caution and select n=10 for our experiments. This does prevent the model from\ngenerating common words or phrases which consist of 10 or more tokens, such as “The quick brown fox\n40\njumped over the lazy dog.” or “supercalifragilisticexpialidocious”. We ﬁnd qualitatively that the impact of\nthis is low, and that this also presents a balanced trade-off with the Bloom ﬁlter size.\nB.3 Choice of Minimum Frequency\nIdeally, we want n large enough so that we do not prevent common phrases and small enough so that we\ncatch all (though practically, most) possible memorizations. Optimizing n for this task is both non-trivial,\nas the objective is not clear, and computationally expensive. Instead, we choose n “ 10 based on\nqualitative experience that this does not prevent many common phrases. Further, we do so to also limit the\nstorage cost of the Bloom ﬁlter, because n too large leads to a blow up in the number of elements, N.\nIt is important to note that usingMEMFREE with a lower n will result in worse performance on standard\nbenchmarks than using it with a higher n. This is because a lower n means more true answers prevents\nfrom being generated.\n0 20 40 60+\nT otal Bloom Hits\nper Generation\n0.001\n0.01\n0.1\n1\nProportion\n2 6 14 42.1\n0 20 40\nT otal Bloom Hits\nper T oken Position\n00 106.0\nQuartiles\nMean\nFigure 9: (left) Most generations have few Bloom queries,as observed by the small quartiles; however, there is a\nlong tail of few generations with many Bloom hits (12.6% of generations had beyond 50 hits with a max of 1111).\n(right) Some positions had signiﬁcantly more hits , e.g., the ﬁrst and tenth tokens. (both) are histograms from\n6000 generations of 50 tokens each using MEMFREE decoding on GPT-Neo 6B.\nB.4 Python Implementation\nFigure 10 contains a Python implementation of MEMFREE using the HuggingFace Transformers3 API.\nB.5 Impact of M EMFREE on Downstream Task Performance\nIn this section, we discuss the worst-case impact MEMFREE could have on performance on downstream\ntasks. We measure this by looking at the targets, the groundtruth text a model’s outputs are compared\nagainst, for three abstractive summarization tasks, three question answers tasks, and the 12 tasks in the\nGEM natural language generation benchmark (Gehrmann et al., 2021). On all these tasks, a model would\nscore perfectly on the validation set if it exactly outputted the groundtruth target sequence. By measuring\nhow many of the 10-grams in each of these target sequences are present in the bloom ﬁlter used by\nMEMFREE , we can assess the worst-case impact MEMFREE would have on model performance at these\ntasks. The results of this analysis are shown in table 1\nWe see that for most of these tasks, the percentage of 10-grams which are present in the bloom ﬁlter is\nnot too much above 1%, the false positive rate of our bloom ﬁlter. Tasks where the target sequences come\nfrom documents likely to be present in the Pile are the most affected by MEMFREE usage. For example,\nfor the BillSum and Arxiv summarization tasks, over 86% of their validation set examples have a 10-gram\nin the bloom ﬁlter. Non-English tasks, which are labeled with an asterisk in Table 1 were also signiﬁcantly\naffected. The drop in performance for non-English tasks is due to the fact that GPT-Neo’s vocabulary is\nbuilt off of English. This means that non-English phrases end up being broken into many more tokens on\naverage than English ones, and a single common word in a non-English language might take up several\ntokens. This can be seen in the bloom hit examples for the MLSum-de task.\n3https://github.com/huggingface/transformers\n41\nbanned = None\nmodel = ## h u g g i n g f a c e model l o a d e r h e r e\nbloom = ## s e t ´l i k e bloom f i l t e r\nn u m _ t o k e n s _ i n _ f i l t e r = 10\ndef ban_bloom ( i n p u t _ i d s , s c o r e s ) :\n\" \" \" i n p u t _ i d s i s t h e t o k e n s o f t h e prompt . s c o r e s i s t h e l o g i t s o u t p u t t e d by t h e model g i v e n t h e s e i n p u t _ i d s . \" \" \"\ni n p u t _ i d s = i n p u t _ i d s . cpu ( ) . d e t a c h ( ) . numpy ( )\n# Order t h e t o k e n s by t h e i r l i k e l i h o o d .\no r d e r = t o r c h . a r g s o r t (´s c o r e s , 1 )\no r d e r = o r d e r . cpu ( ) . d e t a c h ( ) . numpy ( )\nb a t c h _ s i z e = i n p u t _ i d s . s h a p e [ 0 ]\n# S e t t h e l i k e l i h o o d t o 0 f o r a l l t h e most l i k e l y n e x t t o k e n s which would c r e a t e an ngram i n t h e bloom f i l t e r .\nf o r ex i n range ( b a t c h _ s i z e ) :\nf o r i i n o r d e r [ ex ] :\ns e q u e n c e _ t o _ c h e c k = ( i n p u t _ i d s [ ex ] . t o l i s t ( ) + [ i n t( i ) ] )\ni f s e q u e n c e _ t o _ c h e c k[´n u m _ t o k e n s _ i n _ f i l t e r : ]i n bloom :\ns c o r e s [ ex , i ] ´= 1000\ne l s e:\nbreak\nreturn s c o r e s\np r i o r _ p r o c e s s o r = model . _ g e t _ l o g i t s _ p r o c e s s o r\ndef f n (∗ a r g s , ∗∗kwargs ) :\np r i o r = p r i o r _ p r o c e s s o r (∗ a r g s , ∗∗kwargs )\np r i o r . append ( ban_bloom )\nreturn p r i o r\nmodel . _ g e t _ l o g i t s _ p r o c e s s o r = f n\n# Proceed w i t h c a l l i n g model . g e n e r a t e as normal .\nFigure 10: Implementation of MemFree in HuggingFace\nThere are easy strategies to reduce the effect MEMFREE has on benchmark performance. First, one\ncould deliberately choose to omit from the bloom ﬁlter datasets which one decides are acceptable to\nmemorize from, such as Wikipedia and legal documents. Second, one could increase the n-gram size\nof the bloom ﬁlter. As shown in the qualitative examples in Table 1, n=10 is perhaps too stringent for\nfact-based task, where names of proper nouns can take up 10-tokens or more. Third, one could reduce the\nerror rate of the bloom ﬁlter so as to emit fewer false positives.\nB.6 Performance of M EMFREE\nIn this section, we study two questions: (1) “does MEMFREE maintain model utility?” and (2) “does our\noptimized MEMFREE prevent memorization release”.\nAlong question (1), recall that MEMFREE can admit false positives, which may degrade the utility of\nthe language model. Fortunately, the false positive rate can be computed exactly, e.g., see Tarkoma et al.\n(2011), and a long literature has proposed optimizations to account for non-uniform distributions (Bruck\net al., 2006) and to adaptively correct for false positives (Bender et al., 2018).\nHere, we study how, under reasonable computational constraints and inference times, the observed\nrates impact model utility. As we will show, we observe that MEMFREE maintains the highest utility (no\nobservable impact) while being the most efﬁcient defense.\nAlong question (2), we study if our optimizations lead to a substantial increase in the false negative\nrate. To do this, we repeat the experiment from (Carlini et al., 2022), which prompted GPT-Neo models\nwith examples from its training data. We compute how many examples are verbatim memorized when\nMEMFREE decoding is used. The 6B parameter GPT-Neo model memorizes more than 12,000 of these\ndocuments, but, after applying MEMFREE , it only outputs 4 verbatim memorizations. These 4 remaining\nverbatim memorizations are repeated fewer than 10 times in the training data, and so were not added to\nour Bloom ﬁlter. Nonetheless, this strategy reduced verbatim memorization by over 3000ˆ.\nB.7 Bloom Filter Statistics\nFigure 11 shows the distribution in number of tokens (out of 50 generated) that were changed by\nMEMFREE from the token that would have been generated using undefended greedy decoding.\n42\n% ex with % ex % 10grams\nTask len>10 with bloom hit with bloom hit Example 10-grams with bloom hit\nSummarization Tasks\nTIFU 92.0 16.9 1.3 stall windows, get new mouse, keyboard and cup ‚ my freezer and\nnow my home is the bog of ‚ went to a concert ﬁve hours away as\nthe dd\nArxiv 100.0 86.8 1.38 of a bose gas below the critical temperature. ‚ in this paper, we\ndevelop a structure - preserving ‚ consider a model of diffusion\nwhere the individuals behavior is\nPubmed 100.0 92.3 1.7 normal alanine aminotransferase ‚ the prevalence of osteoporosis\nin postmen ‚ www.cs.tau.ac.il\nBillSum 100.0 88.6 3.0 Employee Retirement Income Security Act of 1974 and the Internal\n‚ Congressional Budget and Impoundment Control Act of 1974 ‚\nFederal Meat Inspection Act, the Poultry Products Inspection\nQuestion-Answering Tasks\nSQuAD2.0 9.8 1.1 5.9 E. Mann, Raymond S. Bradley and Malcolm ‚ CTLs (cytotoxic T\nlymph ‚ in 1975. It went public in 1979 and was\nWebQuestions 2.4 0.9 9.8 Academia de Bellas Artes de San Fernando ‚ Paris Saint-Germain\nF.C. ‚ The Mating Habits of the Earthbound Human\nCoQA 4.0 0.5 10.6 Kingdom of Serbs, Croats and Sloven ‚ Sheikh Mohammed bin\nRashid Al Maktou ‚ grabbed the rest of the pickle and ran\nGEM Benchmark\nCommonGen 81.9 5.7 1.4 You ride the horse around the area near the fence ‚ children walk\nwith their dog on a leash down the] ‚ she wears a helmet & sits on\nthe motorcycle.\nChezch Restaurant*\n(Dušek et al., 2019) 99.6 23.5 1.7 jemnou restauraci BarBar, kter ‚ jsou v r˚ uzných‚ Bohužel, poblí\nDART\n(Nan et al., 2021) 97.1 20.1 1.7 in New York City. He was a member of‚ a low-priced family restau-\nrant located near Raja ‚ a Member of the U.S. House of\nE2E clean\n(Dušek and Jurcıcek, 2016) 99.9 88.9 1.0 near Rainbow Vegetarian Café in the city center. ‚ Phoenix is a\ncheap French restaurant in riverside. ‚ a French restaurant with a\nmoderate price range, but\nMLSum-de*\n(Scialom et al., 2020) 100.0 58.7 2.58 zum neuen V orsitzenden‚ für verfassungswidrig. ‚ längst überfäll\nMLSum-es*\n(Scialom et al., 2020) 100.0 42.3 2.2 del pacto y no de la confrontación ‚ selección española de f ‚ in-\nvestigación sobre la desaparici\nSchema-Guided Dialog 63.3 7.5 1.3 The Lord of the Rings: The Return of the‚ tyard By Marriott Sacra-\nmento Cal Expo has a 3 star ‚ with Southwest Airlines. The ﬂight\ntakes off at 7\nToTTo\n(Parikh et al., 2020) 98.0 20.9 3.2 and was broadcast on Venevisión.‚ As of the census of 2000, there\nwere 133 ‚ on the U.S. Billboard 200 chart.\nXSum 99.4 18.5 1.6 stressed will not increase your risk of dying, according ‚ Two drug\ndealing brothers taken back to court for mocking ‚ the Institute of\nDirectors (IoD) has\nWebNLG-en 97.9 27.4 4.7 written by J.R.R. Tolkien, ‚ play in the Campeonato Brasileiro ‚ is\nafﬁliated with Visvesvaraya Technological University\nWebNLG-ru* 100.0 99.6 42.9 ‚ ‚\nWikiAuto + Turk/ASSET 96.5 16.7 2.2 pop-punk, surf rock, ska, ‚ was discovered by a team of as-\ntronomers from the University ‚ cover of Sgt. Pepper’s Lonely\nHearts Club Band\nTable 1: Some benchmark tasks could be signiﬁcantly affected by MEMFREE . For several standard benchmark\ntasks commonly used to evaluate language models, we report the percentage of test set target sequences which\nconsist of at least one 10-gram (meaning hitting the bloom ﬁlter is possible), the percentage of test set target\nsequences which contain at least one 10-gram present in the bloom ﬁlter, and the percentage of all the 10-grams in\nthe test set targets which can be found in the bloom ﬁlter. We also show 3 example 10-grams (delineated by ‘ ‚’)\nwhich are present in both the test set and the bloom ﬁlter. (For QA tasks, we only consider the ﬁrst answer for each\nquestion.) The numbers here reﬂect the worst case scenario: the fraction of examples a language model that\nperfectly memorized the test set would be incapable of getting exactly correct when used with MEMFREE .\n43\n0 5 10 15\nNumber of T okens Changed\n0\n5\n10\n15Percent\n1 4 6\nQuartiles\nMean\nFigure 11: Most generations require few ( ă5) changes to pass M EMFREE checks. Data for histogram from\n6000, 50-token generations using MEMFREE decoding on GPT-Neo 6b.\nFigure 9 presented some of the query patterns of the MEMFREE decoder to investigate when and how it\nimpacts decoding. First, we observe that MEMFREE is trivial to run in terms of compute: it takes only\n49.8 milliseconds to run 10,000 queries on one CPU core. From Figure 9 (left), all generations required\nsigniﬁcantly fewer queries (mean “42.1 queries / generation)—even running batches of many hundreds\nor thousands of queries would incur less than a few seconds additional overhead. Second, we ﬁnd that the\nBloom ﬁlter is often hit at the ﬁrst and tenth tokens after the prompt. We see many hits at the ﬁrst token\nbecause all our prompts are from the training data—so there are relatively fewer single token additions\nthat generate a novel n-gram. Third, we ﬁnd that most generations need only a few (ă5) alterations due\nto MEMFREE decoding.\nC More Details on Measuring Approximate Memorization\nC.1 Similarity Metrics Implementations\nAs noted in Section 5, we identify instances of approximate memorization by measuring the similarity\nbetween a generated continuation and the groundtruth continuation for a prompt. We do so using\nBLEU-score and character-level edit distance.\nWe computed BLEU score using NLTK’s BLEU computation (nltk.translate.bleu_score) with\nthe default parameters (averaging equally BLEU-1, BLEU-2, BLEU-3, and BLEU-4)(Bird et al., 2009).\nEdit distance was computed with the ‘editdistance‘ pip package. Normalized edit similarity between two\nstrings x and y is deﬁned as:\nEDIT SIMpx, yq“ EDIT DISTANCE px, yq\nmaxp|x|, |y|q\nC.2 BLEU Score Threshold Selection\nWe chose to use a BLEU score of 0.75 or higher to indicate that a generation substantially memorized\nfrom the ground-truth continuation. We choose to threshold BLEU score rather than edit-distance since it\nis more interpretable to NLP researchers. Table 12 presents several examples of generations that were\nvery close to the 0.75 BLEU score threshold. This threshold was deliberately set high so as to avoid\nfalse-negatives, allowing us to provide a reliable lower bound on memorization. In our qualitative analysis\nof the data, all examples that were above BLEU of 0.75 showed unambiguous memorization. We also\nobserved memorization in some examples with lower BLEU scores.\n44\nTrue Continuation LM Generation BLEU\nabout a deeper bond with the product. Nostalgia.\nIt’s delicate, but potent. Sweetheart.\\n\\nTeddy told\nme that in Greek, “nostalgia” literally means “the\npain from an old wound”. It’s a twinge in your\nheart far more\nabout a deeper bond with a product. Nostalgia.\nIt’s delicate, but potent. Teddy told me that in\nGreek, nostalgia literally means the pain from an\nold wound. It’s a twinge in your heart, far more\npowerful than memory alone. This\n0.79\nAnd I’ll show you\\nYou want the moves like\njagger\\nI got the moves like jagger\\nI got the\nmooooooves like jagger\\nI don’t even try to control\nyou\\nLook into my eyes and I’ll own you\\nYou\nwith the moves\nAnd I’ll show you\\nAll the moves like Jagger\\nI’ve\ngot the moves like Jagger\\nI’ve got the moves like\nJagger\\nI don’t need to try to control you\\nLook\ninto my eyes and I’ll own you\\nWith the\n0.79\nPurpose\\n\\nThe laws of most jurisdictions through-\nout the world automatically confer\\nexclusive\nCopyright and Related Rights (deﬁned below) upon\nthe creator\\nand subsequent owner(s) (each and all,\nan \"owner\") of an original work of\\nauthorship\nand/or a database\nPurpose\\n\\n The laws of most jurisdictions through-\nout the world automatically confer\\n exclusive\nCopyright and Related Rights (deﬁned below) upon\nthe creator\\n and subsequent owner(s) of an orig-\ninal work of authorship (the \"Work\").\\n Certain\njurisdictions do not recognize a\n0.76\nFigure 12: Examples of model generations with BLEU score close to 0.75, the threshold we used to declare that\napproximate memorization had occurred.\nD Experiments with Large English Language Models\nD.1 Prompt Selection Process\nFamous speeches were selected from the \"Top 100 Speeches\" list found at https://www.\namericanrhetoric.com/newtop100speeches.htm. Monologues were selected from the list of two-\nminute monologues found at http://www.monologuedb.com/tag/2-minute-monologues/. Novels\nwere selected from the Time Magazine’s Top 100 All-Time Novels list found athttps://www.goodreads.\ncom/list/show/2681.Time_Magazine_s_All_Time_100_Novels. The opening paragraphs of the\nﬁrst chapter (skipping over prefaces, introductions, and boilerplate) were used as each example. The\n2011 and 2021 song lyrics were selected from the Billboard Year-End Hot 100 singles lists found at\nhttps://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2011 and https:\n//en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2012.\nFor each document, the ﬁrst 100 words were used as a prompt, and the ﬁrst 50 generated words were\ncompared with the ﬁrst 50 words of the true continuation. This approach has the ramiﬁcation that not all\nprompts were the same length in tokens. However, this approach was necessary for fairness across style\ntransfers because an all-uppercased string is going to be many subword tokens longer than the lowercased\nversion of the same string.\n45\noriginal spaces lower caps\nSpeeches\n(n=100)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Proportion Memorized\noriginal spaces lower caps\nMonologues\n(n=60)\noriginal spaces lower caps\nOS Licenses\n(n=42)\noriginal spaces lower caps\nNovels\n(n=77)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7Proportion Memorized\noriginal spaces lower caps\nLyrics 2011\n(n=100)\noriginal spaces lower caps\nLyrics 2021\n(n=100)\nGPT-3 DaVinci Original\nGPT-3 DaVinci v2\nPaLM 62B\nPaLM 540B\nFigure 13: \"Style-transfer\" prompting divulges approximate memorization in two versions of GPT-3 and two\nsizes of PaLM. Note that generations also follow the same style as the prompt. Generations were characterized as\nmemorized if they had a BLEU score of at least 0.75 with the ground-truth continuation.\n46\nDomain with n total prompts Model\n# Prompts Memorized per\nStyle-Transfer Type\nOriginal Two Spaces Lower Upper\nOpen-Source Licenses (n=42) GPT-3 DaVinci Original 23 8 14 1\nGPT-3 DaVinci v2 30 25 25 13\nFamous Speeches (n=100) GPT-3 DaVinci Original 20 1 14 0\nGPT-3 DaVinci v2 12 6 11 3\nFamous Monologues (n=60) GPT-3 DaVinci Original 3 0 1 0\nGPT-3 DaVinci v2 4 3 4 0\nNovel Openings (n=77) GPT-3 DaVinci Original 9 0 3 0\nGPT-3 DaVinci v2 7 4 5 0\nLyrics 2011 (n=11) GPT-3 DaVinci Original 7 2 6 2\nGPT-3 DaVinci v2 14 11 14 4\nLyrics 2021 (n=11) GPT-3 DaVinci Original 3 3 3 2\nGPT-3 DaVinci v2 4 2 4 4\nTable 2: \"Style-transfer\" prompting surfaces approximate memorization in GPT-3. We explore n prompts\nfor each domain. Note that generations also follow the same style as the prompt.\n47\nE Experiments with M EMFREE and Other Model Families\nIn addition to running experiment using the GPT-Neo family, we also ran them with the Pyhia model\nfamily (Biderman et al., 2023). Like GPT-Neo, Pythia was trained on the Pile. There are two versions of\nPythia, one trained on the same version of the Pile as GPT-Neo, and another trained on a deduplicated\nversion of the Pile.\nFigure 14 shows the amount of memorization in each of these three model families, with and without\nMEMFREE . Figure 15 shows the same scatter plots as in Figure 6, but using the 6.9B-parameter Pythia.\nWe see that Pythia exhibits more approximate memorization than GPT-Neo. ThoughMEMFREE is still\neffective at reducing approximate memorization, it is slightly less effective than it was on GPT-Neo.\n0 2B 4B 6B\nSize\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nBLEU score between\ngenerated and true\ncontinuations\nUndefended - Pythia\nMemFree - Pythia\nUndefended - Pythia-dedup\nMemFree - Pythia-dedup\nUndefended - GptNeo\nMemFree - GptNeo\nFigure 14: Approximate memorization on the base and deduped versions of Pythia, compared with GPT-Neo.\n0.0 0.5 1.0\nUndefended\n0.0\n0.5\n1.0MemFree\n(a) BLEU (word-level)\n0.0 0.5 1.0\nUndefended\n0.0\n0.5\n1.0MemFree\n (b) Edit similarity (char-level)\nFigure 15: MEMFREE is also effective at reducing approximate memorization for the deduped Pythia 6.9B model.\nF Qualitative Examples that Bypass Copilot’s Filter\nIn Figure 16, we show more examples that bypass Copilot’s memorization ﬁlter.\n48\nStandard Prompting\n/* low -> Starting index, high -> Ending index\n*/\nquickSort(arr[], low, high)\n{\nif (low < high)\n{\n/* pi is partitioning index, arr[p] is\nnow\nat right place */\npi = partition(arr, low, high);\nCopilot no longer generates continuations\nComment Prompting\n# /* low -> Starting index, high -> Ending index\n*/\n# quickSort(arr[], low, high)\n# {\n# if (low < high)\n# {\n# / * pi is partitioning index, arr[p] is\nnow\n# at right place */\n# pi = partition(arr, low, high);\n# quickSort(arr, low, pi - 1); # Before\npi\n# quickSort(arr, pi + 1, high); # After pi\n# }\n# }\nNaming Convention Prompting\n/* _low -> Starting index, _high -> Ending\nindex */\nquickSort(arr[], _low, _high)\n{\nif (_low < _high)\n{\n/* pi is partitioning index, arr[p] is\nnow\nat right place */\npi = partition(arr, _low, _high);\nquick_sort(arr, _low, pi - 1); //\nBefore pi\nquick_sort(arr, pi + 1, _high); //\nAfter pi\n}\n}\nLanguage Prompting\n/* depart -> index de départ, fin -> index de fin\n*/\nquickSort(arr[], depart, fin)\n{\nif (depart < fin)\n{\n/* pi est l’index de partitionnement, arr[\np] est maintenant\nà la bonne place */\npi = partition(arr, depart, fin);\n// Trier les éléments séparément avant et\naprès la partition\nquick_sort(arr, depart, pi - 1);\nquick_sort(arr, pi + 1, fin);\n}\n}\nNaming Convention Prompting\n/* LOW -> Starting index, HIGH -> Ending\nindex */\nquickSort(arr[], LOW , HIGH )\n{\nif (LOW < HIGH )\n{\n/* pi is partitioning index, arr[p]\nis now\nat right place */\npi = partition(arr, LOW , HIGH );\nquickSort(arr, LOW , pi - 1); //\nBefore pi\nquickSort(arr, pi + 1, HIGH ); //\nAfter pi\n}\n}\nComment Prompting\n/* low -> Starting index, high -> Ending index */\n//1\nquickSort(arr[], low, high) //2\n{\nif (low < high) //3\n{\n/* pi is partitioning index, arr[p] is now\nat right place */\npi = partition(arr, low, high); //4\nquickSort(arr, low, pi - 1); //5\nquickSort(arr, pi + 1, high); //6\n}\n}\nFigure 16: Preventing GitHub’s Copilot language model from emitting verbatim memorized content often results\nin it emitting memorized, but not verbatim, content. Trivially modifying the prompt bypasses these output ﬁl-\nters. Prompts are highlighted in blue. Model evaluated with the option “block suggestions matching public code”\nenabled. For brevity we removed comments from model outputs.\n49\nG Qualitative Examples of Memorization Despite M EMFREE\nTable 17 gives examples of approximate memorization which occured despite using MEMFREE decoding.\nBelow , we present a longer list of full examples of MEMFREE decoding. In the following paragraphs,\nthe gray highlighted texts are the prompts, and the texts that follow the prompts are generated by the\nmodel. For easier reading, we merged the individual tokens to form text spans, except at tokens where\nbloom-ﬁlter rejection happens. In this case, the symbol \u0003 indicate connection between tokens. Moreover,\nred highlighted texts indicate one or more rejected tokens, which are always followed by one accepted\ntoken highlighted with green. The texts are lightly edited mostly to properly encode unicode symbols\n(some garbled symbols are replaced as b) that LATEX does not handle gracefully.\nSplit word “Activity” viewHolder.swipeLayout.setOnDoubleClick Listener(new SwipeLay-\nout.DoubleClickListener() { Override public void onDoubleClick(SwipeLayout layout, boolean surface) {\nToast.makeText(mContext,\"DoubleClick : \" + position, \u0003 Toast \u0003 .LENGTH \u0003 _ ). \u0003 show(); \u0003 } //\n}); if return m \u0003 Context.start \u0003 Activity Service Activ \u0003 ities(new Intent(mContext, \u0003 Activity \u0003\n.class)); \u0003 } m \u0003 Context.ﬁn \u0003 ish ishing \u0003 ();\nTypo “Wildlife”ñ“Wildife” should promptly announce a phase-in of non-lead ammunition for all\nhunting throughout the rest of the state to protect other wildlife poisoned by lead and to safeguard human\nhealth. Hunters can play a critical role in the recovery of the condor and also keep lead from poisoning the\nwild game they eat.”The lead-free ammunition regulations are designed to reduce lead poisonings of the\niconic and extremely endangered California condor. Condors, eagles, and other scavengers such as ravens\n\u0003 , crows, and hawks \u0003 are have \u0003 been poisoned by lead ammunition. The California Department\nof Fish and \u0003 Wildlife Game Wild \u0003 ife (CDFW) is the lead agency responsible for implementing the\nregulations. The regulations are based on the Condor Protection Act,\nSingular to plural: “claim” ñ“claims” ) No 1924/2006. neurotransmission and muscle contraction\nincluding heart muscle 29 Magnesium Magnesium contributes to normal protein synthesis The claim\nmay be used only for food which is at least a source of magnesium as referred to in the claim SOURCE\nOF [NAME OF VITAMIN/S] AND/OR [NAME OF MINERAL/S] as listed in the Annex to Regulation\n(EC) No 1924/2006. protein \u0003 synthesis 30 \u0003 M N \u0003 iacin Niac \u0003 in ins \u0003 contribute to normal protein\nsynthesis The \u0003 claim claims \u0003 may be used only for food which is at \u0003 least lest \u0003 a source of niacin as\nreferred to \u0003 in to \u0003 in the claim SOURCE OF [\nUppercase to lowercase cuisine of the city’slong-established Hum Vegetarian Cafe & Restaurant to a\ncentral garden location. Settle into the elegant and verdant space and enjoy dishes including papaya and\nbanana ﬂower salads, mushrooms steamed in coconut, and the subtle combination of braised tofu with\nstar anise and cinnamon. Nha Hang NgonVietnamese$$ (MAP GOOGLE MAP); %028-3827 7131; 160\nb Pasteur; meals 60 \u0003 ,000-100,000 ¯d; h7am-10pm; \u0003 b ) \u0003 This is the place to come for \u0003 a authentic\n\u0003 Vietnamese food in the French \u0003 Quarter quarter \u0003 . The menu is a mix of Vietnamese and French\ndishes, and the food is\nLower case to upper case broker = createBroker(deleteAllMessagesOnStartup); broker.start(); } public\nBrokerService createBroker(boolean deleteAllMessagesOnStartup) throws Exception { return createBro-\nker(deleteAllMessagesOnStartup, TRANSPORT_URI); } public BrokerService createBroker \u0003 ( With \u0003\nTransportURI(boolean deleteAllMessagesOnStart \u0003 up Up \u0003 ) throws Exception { \u0003 b return \u0003 create\nnew broker \u0003 Service.createBroker(deleteAllMessages \u0003 On , \u0003 TRANSPORT_URI); } \u0003 b b \u0003 } \u0003\nb b <|endoftext|>\nChange from “agree” to ‘Ag-reeableness” a person is imaginative or independent, high openness can\nbe perceived as unpredictability or lack of focus. Moreover, individuals with high openness are said to\npursue self-actualization specifically by seeking out intense, euphoric experiences, such as skydiving,\nliving abroad, gambling, et cetera. Conversely, those with low openness seek to gain fulﬁllment through\nperseverance, some disagreement remains about how to interpret and contextualize the openness factor. A\ntendency to be organized and dependable, show self- \u0003 discipline, and be goal-oriented is also associated\n50\nwith high openness. The openness factor is also associated with the Big Five personality traits of \u0003 agree\nAg \u0003 reeableness, Conscientiousness \u0003 , and \u0003 Emotional Stability. See also Openness\nPassive voice to active voice (grammar error) still wouldn’tshock me at all if the Chiefs pulled off a\nGiants- or Ravens-esque Super Bowl run to cap off this ﬁve-year window of the Smith/Reid era with a\nring. While Pittsburgh has been this team’sbugaboo, maybe they can avoid them in January thanks to a\nJacksonville upset, and I still say this is the AFC team most likely to win a playoff game in New England.\nWhile so many were quick to write the Chiefs off, they just opened up \u0003 a new chapter in their history.\nThe Chiefs are the AFC’s best team, and they’re going to be a force to be\u0003 reckoned reckon \u0003 with for\nyears to come. 1. New England Patriots \u0003 b The \u0003 Patriots are the AFC’s best\nChange of protocol (email still get generated) =\"https://groups.google.com/group/django-developers\"\ntarget=\"_blank\" rel=\"nofollow\" onmousedown=\"this.href=&#39;https://groups.google.com/group/django-\ndevelopers&#39;;return true;\" onclick=\"this.href=&#39;https://groups.google.com/group/django-devel-\nopers&#39;;return true;\">https://groups \u0003 . ... \u0003 </a> <a href=\" \u0003 https http mail \u0003 to \u0003 : :// \u0003\ndavid@davidwalsh.name\" target=\"_blank\" rel=\"n \u0003 of ore ... \u0003 </a> <a href=\"mailto://david@davidw\nSynonyms ken interior. The seats were heavily cushioned black velvet. On the windows, the Darkling’s\nsymbol had been cut into the glass: two overlapping circles, the sun in eclipse. Across from me, the two\nGrisha were studying me with open curiosity. Their red kefta were of the ﬁnest wool, embroidered lavishly\nin black and lined in black fur. The fair-haired Heartrender was lanky and had a long, melancholy face.\nIvan was taller, broader, \u0003 and had a face like a bulldog’s. \"You are\u0003 the a \u0003 Gr very \u0003 pretty girl,\"\nIvan said. \"Thank you,\" I \u0003 said replied answered \u0003 . \u0003 b \" \u0003 I’m not a girl.\" \"You are a girl,\" he said.\nSynonyms severing any such bond. In re L.M., 923 A.2d 505, 511 (Pa. Super. 2007) (citing 23 Pa.C.S.\n§ 2511) (some citations omitted). Section 2511(a) provides in pertinent part: (a) General rule.-The rights\nof a parent in regard to a child \u0003 \u0003may are \u0003 not terminated by a proceeding brought under \u0003 this \u0003\npart chapter section sub subsection [ article paragraph \u0003 or \u0003 paragraph section \u0003 2512 or 2513(a) or (b),\nor any \u0003 b ________________________________ \u0003 ____________ \u0003 b â \u0003 *Retired Senior Judge\nSynonyms ” “Do Androids Dream of Electric Sheep?” (the original of “Blade Runner”), and his master-\npiece, “Ubik.”Dick’sfans are not modest in their claims. Nor are they especially precise: Borges, Calvino,\nKafka, Robertson Davies are cited, in the blurbs and introductions, as his peers. A note of inconsistency\ninﬂects these claims-Calvino and Robertson Davies? \u0003 -but the point is clear: Dick is the most important\nwriter of the last century. The book is divided into three \u0003 sections parts main categories \u0003 : “Themes,”\n“Themes and Themes,” and “Themes\nRejecting multiple candidates s den.” Scott is aware of the impact his race and size has on the way\npeople – particularly authority ﬁgures like law enforcement ofﬁcers – perceive him. He is big. He is\ndark-skinned. “They look at us like we don’tknow how to control ourselves and we just get angry quick,”\nhe said. “It’snot even like that. They criminalize us for no reason.”Scottb \u0003 bs mother, who is white,\nsaid she has been stopped by police for no reason. “I’\u0003 ve m ll d \u0003 be say get like never just have ask\nrather been tell pull \u0003 over and they’d be like,b \u0003 b b L \u0003 What are you doing?”’ she said\nOther examples Suzy is great! She helped me buy my condo at a great price (foreclosure) and then was\nsuper patient with my husband and me 4 years later when we were on a search for a house. She helped us\nget our... Suzie H., Jacksonville Goes above and beyond Suzy has helped me close on my third property in\n3 years. First she found me my dream pool home at the Beach then she helped me ﬁnd two investment\ntownhomes in \u0003 the same area \u0003 . and \u0003 now she is helping me ﬁnd my dream home. She is always\navailable to answer any questions I have and goes above and beyond to help me ﬁnd the perfect home. I \u0003\nwould highly \u0003 recommend her! Suzy H., Jacksonville\nfrom this new programme. I have also been reminded of the role of tax measures in supporting urban\ndevelopment. With us in the gallery today is Mr Vuyisa Qabaka, a Cape Townentrepreneur and co-founder\nof an organisation called the Good Neighbourhoods Foundation. His advice is that “Government should\nencourage township investment. For instance, it could promote urban development and regeneration\n51\nthrough accelerated depreciation allowances for new building constructions or refurbishment of existing\nbuildings.”\u0003 I am sure that \u0003 the many \u0003 of you \u0003 will have in \u0003 this Chamber will agree with him. I\nam also sure that many of you will agree with the Minister of Finance, who has said that the tax system\nshould be used to support the \u0003 development growth economy \u0003 and to create\nm off on some details.) Unelma keltaisesta kuninkaasta. Fastaval is not your average convention – it\nspecializes in incredibly tight auteur-designed roleplaying scenarios. A bunch of people run each scenario\nfor players, not just the creator. There’sawards for best scenarios in different categories. The Society\nfor Nordic Roleplaying published a collection of these scenarios translated into Finnish a few years ago,\ncalled Unelma keltais \u0003 esta kuninkaasta. It’s a great book,\u0003 and but with \u0003 a lot of great scenarios. \u0003 I\nThe \u0003 book is available in English, but it’s not cheap. I’ve been looking for a copy for a while\ndisappoint Jimmy. Then, I slept like a baby. SoFortWorthIt Oscars Swag GIVEAWAY!!!The Oscars\nare exhausting, y’all.I’lldefinitely be cheering for all the stars this year, especially since I know the kind\nof caviar-Champagne-and-swag-ﬁlled night they’reexperiencing. And you know what? I want you to\nexperience what it’slike to get arm-loads of \u0003 free stuff. So, I’m \u0003 giving doing going partnering \u0003\nwith the folks at the FortWorthIt Oscars Swag Giveaway to give away a $100 Visa gift card to one lucky\nwinner. To enter, all you have\u0003 to do \u0003 is\ndecision.\" \"It will go down to destruction... or else, it will survive.\" \"This is their moment of trial.\"\n\"They’vegot to show themselves worthy of everything we gods have given them.\" \"But evil is dark and\nstrong.\" \"And it may be that the scales of fate... are not yet in full balance.\" \"What can I do to equalize\nboth sides of the struggle, Athena?\" \"If you don’twant to increase the powers of all men... then why don\n\u0003 ’t you just give me the power to destroy them?\" \"I can’t do that.\"\u0003 \" \"[ \u0003 Thunderclap]\" \"I’m sorry.\"\n\"I’m\u0003 sorry not so afraid \u0003 I it you that the \u0003 gods have decreed... that the balance of power must be\nmaintained.\" \"I’m\ngive him a minute between removing the ﬁrst tray and replacing it with the second - and you can\ncome up with all sorts of theories to explain your ﬁndings. You can even throw a person in an MRI\nmachine, study the ﬂickering images on your computer screen. But the brain is the ultimate black box.\nEventually, to grasp the ﬁrst cut, you’llhave to make another. The car pulls into the parking lot of the\nnursing home, noses into an empty space. Annese and Cork \u0003 y get out, and Annese goes to the trunk to\nget the wheelchair. Corky is still standing, leaning on the car. \"I’m going \u0003 to in \u0003 ,\" \u0003 he she An \u0003\nnese says. \"I’ll come\u0003 with in \u0003 with \u0003 you ya\n52\nTrue Continuation MEMFREE Undefended\nattendee list reﬂects the minimum\nnumber of seats reserved for your\ngame. Preregistered attendees for\neach of your events are listed be-\nlow. If one attendee reserved multiple\nseats for your event, that attendee’s\nname will only be listed once. There-\nfore, the attendee list reﬂects the min-\nimum number of seats reserved for\nyour game. Preregistered attendees\nfor each of your events are listed be-\nlow. If one attendee reserved multiple\nseats for your event, that attendee’s\nattendee list reﬂects the minimum\nnumber of seats reserved for your\ngame. Preregistered attendees for\neach of your events are listed be-\nlow. If one attendee reserved multiple\nseats for your event, that attendee’s\nname will only be listed once. There-\nfore, the attendee list reﬂects the min-\nimum number of seats reserved for\nyour game. Preregistered attendees\nfor each of your events are listed be-\nlow. If one attendee reserved multiple\nseats for your event, that attendee’s\nBLEU: 1\nattendee list reﬂects the minimum\nnumber of seats reserved for your\ngame. Preregistered attendees for\neach of your events are listed be-\nlow. If one attendee reserved multiple\nseats for your event, that attendee’s\nname will only be listed once. There-\nfore, the attendee list reﬂects the min-\nimum number of seats reserved for\nyour game. Preregistered attendees\nfor each of your events are listed be-\nlow. If one attendee reserved multiple\nseats for your event, that attendee’s\nBLEU: 1\n293. 294. 295. 296. 297. 298. 299.\n300. 301. 302. 303. 304. 305. 306.\n307. 308. 309. 310. 311. 312.\n293. 294. 295. 296. 297. 298.\n299. 300. 301. 302. 30 34. 30 43.\n305. 3064. 3076. 308. 30 97. 31009.\n31108. 31210. BLEU: 0.95\n293. 294. 295. 296. 297. 298.\n299. 300. 301. 302. 303. 304. 305.\n306. 307. 308. 309. 310. 311. 312.\nBLEU: 1\n0x058f8f8aU, 0x3f9292adU,\n0x219d9dbcU, 0x70383848U,\n0xf1f5f504U, 0x63bcbcdfU,\n0x77b6b6c1U, 0xafdada75U,\n0x42212163U, 0x20101030U, 0\n0x058f8f8aU, 0x3f9292adU,\n0x219d9dbcU, 0x70383848U,\n0xf1f5f504U, 0x63bcbcdfU,\n0x77b6b6c1UL, 0xaf-\ndada75U,0x42212163U,\n0x20101030U, 0 BLEU: 0.93\n0x058f8f8aU, 0x3f9292adU,\n0x219d9dbcU, 0x70383848U,\n0xf1f5f504U, 0x63bcbcdfU,\n0x77b6b6c1U, 0xafdada75U,\n0x42212163U, 0x20101030U, 0\nBLEU: 1\n7, calc(sin((pi/180)*a7))) deﬁne(cea0,\ncalc(cos((pi/180)*ea0))) deﬁne(cea1,\ncalc(cos((pi/180)*ea1))) deﬁne(cea2,\ncalc(cos((pi/180)*ea2))) deﬁne(cea3,\ncalc(cos((pi/180)*ea3))) deﬁne(cea4,\ncalc(cos((pi/180\n7, calc(sin((pi/180)*a7))) deﬁne(cea0,\ncalc(cos((pi/180)*ea0))) deﬁne(cea1,\ncalc(cos((pi/180)*ea1))) deﬁne(cea2,\ncalc(cos((pi/180)*ea2))) deﬁne(cea3,\ncalc(cos((pi/180)*ea3))) deﬁne(cea4,\ncalc(cos((pi/180 BLEU: 0.95\n7, calc(sin((pi/180)*a7))) deﬁne(cea0,\ncalc(cos((pi/180)*ea0))) deﬁne(cea1,\ncalc(cos((pi/180)*ea1))) deﬁne(cea2,\ncalc(cos((pi/180)*ea2))) deﬁne(cea3,\ncalc(cos((pi/180)*ea3))) deﬁne(cea4,\ncalc(cos((pi/180 BLEU: 1\nFigure 17: Random sample of M EMFREE generations where the BLEU score with the true continuation ą 0.9.\nMost of these examples are repetitive and/or lists of numbers. In the MEMFREE column, we use highlights to show\nthe difference from the true continuation: red means deleted text, and green means added text.\n53\nH Author Ordering Algorithm\nimport hashlib\nimport numpy as np\ndef hash(x):\nh=hashlib.new(\"md5\")\nh.update(bytes(x,\"ascii\"))\nreturn int(h.hexdigest(),16)\nnames = (\"Nicholas Daphne \" +\n\"Katherine Matthew \" +\n\"Florian Chiyuan Milad \" +\n\"Christopher\").split()\nfor i in range(0,10000):\ns = str(i)\nl = [hash(x+s) for x in names]\no = np.argsort(l)\nif names[o[0]] != \"Daphne\":\ncontinue\nif names[o[-1]] != \"Nicholas\":\ncontinue\nprint([names[x] for x in o])\nexit(0)\nFigure 18: Author ordering algorithm",
  "topic": "Zhàng",
  "concepts": [
    {
      "name": "Zhàng",
      "score": 0.7028002738952637
    },
    {
      "name": "Computer science",
      "score": 0.471636027097702
    },
    {
      "name": "Memorization",
      "score": 0.47060877084732056
    },
    {
      "name": "Natural language",
      "score": 0.4555283188819885
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.44374707341194153
    },
    {
      "name": "Philosophy",
      "score": 0.3779914677143097
    },
    {
      "name": "Theology",
      "score": 0.37525027990341187
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3458503484725952
    },
    {
      "name": "Classics",
      "score": 0.3274078369140625
    },
    {
      "name": "Linguistics",
      "score": 0.3027195334434509
    },
    {
      "name": "Art",
      "score": 0.29415300488471985
    },
    {
      "name": "History",
      "score": 0.23053213953971863
    },
    {
      "name": "Archaeology",
      "score": 0.10325399041175842
    },
    {
      "name": "China",
      "score": 0.078327476978302
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    },
    {
      "id": "https://openalex.org/I205783295",
      "name": "Cornell University",
      "country": "US"
    }
  ],
  "cited_by": 17
}