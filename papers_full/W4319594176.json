{
    "title": "pLMSNOSite: an ensemble-based approach for predicting protein S-nitrosylation sites by integrating supervised word embedding and embedding from pre-trained protein language model",
    "url": "https://openalex.org/W4319594176",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4286205576",
            "name": "Pawel Pratyush",
            "affiliations": [
                "Michigan Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2903365011",
            "name": "Suresh Pokharel",
            "affiliations": [
                "Michigan Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A1180009366",
            "name": "Hiroto Saigo",
            "affiliations": [
                "Kyushu University"
            ]
        },
        {
            "id": "https://openalex.org/A1910772782",
            "name": "Dukka B Kc",
            "affiliations": [
                "Michigan Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A4286205576",
            "name": "Pawel Pratyush",
            "affiliations": [
                "Michigan Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A2903365011",
            "name": "Suresh Pokharel",
            "affiliations": [
                "Michigan Technological University"
            ]
        },
        {
            "id": "https://openalex.org/A1180009366",
            "name": "Hiroto Saigo",
            "affiliations": [
                "Kyushu University"
            ]
        },
        {
            "id": "https://openalex.org/A1910772782",
            "name": "Dukka B Kc",
            "affiliations": [
                "Michigan Technological University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2974575930",
        "https://openalex.org/W1968131336",
        "https://openalex.org/W2065773243",
        "https://openalex.org/W2084529953",
        "https://openalex.org/W2140807512",
        "https://openalex.org/W1269597682",
        "https://openalex.org/W2058289888",
        "https://openalex.org/W1982536922",
        "https://openalex.org/W1973864912",
        "https://openalex.org/W2034890309",
        "https://openalex.org/W2022965734",
        "https://openalex.org/W2141214120",
        "https://openalex.org/W4285159613",
        "https://openalex.org/W4283725351",
        "https://openalex.org/W2894436452",
        "https://openalex.org/W2988751457",
        "https://openalex.org/W3127764784",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W2794764013",
        "https://openalex.org/W4311364189",
        "https://openalex.org/W3177500196",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W3165163830",
        "https://openalex.org/W4282984452",
        "https://openalex.org/W4225438928",
        "https://openalex.org/W4303645584",
        "https://openalex.org/W2156125289",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W2885583144",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1645816215",
        "https://openalex.org/W28412257",
        "https://openalex.org/W2008056655",
        "https://openalex.org/W2911964244",
        "https://openalex.org/W3157142061",
        "https://openalex.org/W3134169372",
        "https://openalex.org/W4205773061"
    ],
    "abstract": null,
    "full_text": "Open Access\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits \nuse, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original \nauthor(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third \nparty material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the mate-\nrial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// \ncreat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi \ncdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nRESEARCH\nPratyush et al. BMC Bioinformatics           (2023) 24:41  \nhttps://doi.org/10.1186/s12859-023-05164-9\nBMC Bioinformatics\npLMSNOSite: an ensemble-based approach \nfor predicting protein S-nitrosylation sites \nby integrating supervised word embedding \nand embedding from pre-trained protein \nlanguage model\nPawel Pratyush1, Suresh Pokharel1, Hiroto Saigo2 and Dukka B. KC1* \nAbstract \nBackground: Protein S-nitrosylation (SNO) plays a key role in transferring nitric oxide-\nmediated signals in both animals and plants and has emerged as an important mecha-\nnism for regulating protein functions and cell signaling of all main classes of protein. It \nis involved in several biological processes including immune response, protein stability, \ntranscription regulation, post translational regulation, DNA damage repair, redox regu-\nlation, and is an emerging paradigm of redox signaling for protection against oxida-\ntive stress. The development of robust computational tools to predict protein SNO \nsites would contribute to further interpretation of the pathological and physiological \nmechanisms of SNO.\nResults: Using an intermediate fusion-based stacked generalization approach, we \nintegrated embeddings from supervised embedding layer and contextualized protein \nlanguage model (ProtT5) and developed a tool called pLMSNOSite (protein language \nmodel-based SNO site predictor). On an independent test set of experimentally identi-\nfied SNO sites, pLMSNOSite achieved values of 0.340, 0.735 and 0.773 for MCC, sensitiv-\nity and specificity respectively. These results show that pLMSNOSite performs better \nthan the compared approaches for the prediction of S-nitrosylation sites.\nConclusion: Together, the experimental results suggest that pLMSNOSite achieves \nsignificant improvement in the prediction performance of S-nitrosylation sites and \nrepresents a robust computational approach for predicting protein S-nitrosylation sites. \npLMSNOSite could be a useful resource for further elucidation of SNO and is publicly \navailable at https:// github. com/ KCLab MTU/ pLMSN OSite.\nKeywords: S-nitrosylation, Deep learning, Convolutional neural network, Post-\ntranslational modification, Word embedding, Protein language model\n*Correspondence:   \ndbkc@mtu.edu\n1 Department of Computer \nScience, Michigan Technological \nUniversity, Houghton, MI, USA\n2 Department of Electrical \nEngineering and Computer \nScience, Kyushu University, 744, \nMotooka, Nishi-Ku 819-0395, \nJapan\nPage 2 of 20Pratyush et al. BMC Bioinformatics           (2023) 24:41 \nBackground\nNitric oxide (NO) is a highly reactive molecule, and abnormal NO levels in mammalian \ncells are associated with multiple human diseases, including cancer [1]. The role of NO \nas a major regulator of physiological function has become increasingly evident. S-nitros -\nylation (SNO) is one of the most important regulatory mechanisms of this vital signal -\ning molecule. In S-nitrosylation, the NO is covalently attached to the thiol side chain of \ncysteine residues to form S-nitrosothiol (SN), a critical mechanism of transferring NO-\nmediated signals [2]. Additionally, S-nitrosylation has unfolded as an important mech -\nanism for regulating protein functions and cell signaling of all main classes of protein \nand is involved in several biological processes including immune response [1], protein \nstability, transcription regulation, post translational regulation, DNA damage repair, \nand redox regulation [3], and is an emerging paradigm of redox signaling for protection \nagainst oxidative stress. Recently, it has also been shown that SNO also regulates diverse \nbiological processes in plants [4].\nThe experimental identification of S-nitrosylated sites is generally performed by com -\nbining the Biotin-switch technique (BST) [5]with Mass Spectrometry (MS). With few \nexceptions, all methods for the identification of S-nitrosylation sites are based on the \nBST and differ only in the utilized MS equipment, ion sources, and the use of liquid \nchromatography. Please refer to the excellent review by Lamotte et al. [4] for an in-depth \ndescription of experimental identification of S-nitrosylation.\nAlthough some studies have suggested that the target cysteine residues often lie within \nan acid–base or hydrophobic motif [6], recent studies have proven that the acid–base \nmotif is located farther from the cysteine [7]. Additionally, even though some studies \nhave suggested that the target cysteine must be within a signature motif (I/L-X-C-X2-\nD/E) and be in a suitable environment [1], there is not yet a consensus motif for SNO [8]. \nIn this regard, various mechanisms are involved in the formation of SNO.\nOwing to this fact that high throughput experimental approaches do not yet exist for \nSNO, several complimentary computational approaches have been developed to pre -\ndict protein SNO sites. These approaches are mostly based on machine learning mod -\nels that use experimentally identified S-nitrosylation sites to train the model and use \nvarious features such as identity of the neighboring residues during training. Some of \nthe existing SNO site prediction tools are: GPS-SNO [9], SNOSite [10], iSNOPSeAAC \n[11], etc. SNOSID [12], developed by Hao et al., is perhaps the first computational tool \nfor predicting S-nitrosylation sites. GPS-SNO [9] is another approach for prediction of \nS-nitrosylation sites and is based on the GPS 3.0 algorithm. Moreover, iSNO-PseAAC \n[11] is another approach developed by Xu et al. that uses PseAAC to represent protein \nsequences for prediction of protein S-nitrosylation sites. Recently, various deep learning-\nbased methods [13, 14] have been developed for prediction of various post-translation \nmodification sites including SNO sites. In that regard, DeepNitro [15], a deep learning-\nbased approach, developed by Xie et al. for the prediction of protein S-nitrosylation sites \nuses four different types of features: one-hot encoding, Property Factor Representation \n(PFR), k-space spectrum, and PSSM encoding.\nAdditionally, Hasan et al. proposed PreSNO [16] which integrates two classifiers: RF \nand SVM using Linear regression. The input to both the RF and SVM in PreSNO is \nbased on four different encoding schemes: the composition of profile-based amino acid \nPage 3 of 20\nPratyush et al. BMC Bioinformatics           (2023) 24:41 \n \npair (CPA), the K-space spectral amino acid composition (SAC), tripeptide composi -\ntion from PSSM (TCP), and physicochemical properties of amino acids (PPA). It must \nbe noted here that the DeepNitro dataset is used for training and testing of the PreSNO \nmodel. For a thorough review of the existing computational approaches for predicting \nProtein S-nitrosylation sites, please refer to Zhao et al. [17].\nLately, we have witnessed the development of exciting array of Natural Language Pro -\ncessing (NLP) algorithms and technologies including recent breakthroughs in the field \nof bioinformatics [14, 18–20]. Among these developments, language models (LMs) have \nemerged as a powerful paradigm in NLP for learning embeddings directly from large, \nunlabeled natural language datasets. In contrast to uncontextualized word embeddings, \nwhich return the same embedding for a word irrespective of the surrounding words, \nembeddings from LMs are contextualized in a way  that they render the embedding \ndependent on the surrounding words. These advances are now being explored in pro -\nteins through the development of various protein language models (pLMs) [21–24]. The \nrepresentations (embeddings) extracted from these transformer-based language models \nhave been successful for various downstream bioinformatics prediction tasks [25–27], \nsuggesting that the huge amount of information learned by these pLMs can be trans -\nferred to other tasks by extracting embeddings from these pLMs and using these embed-\ndings as an input to predict other properties of protein.\nAs discussed above, though there exist various computational approaches for pre -\ndicting SNO sites, the prediction performance of the existing approaches is not yet \nsatisfactory. Additionally, the potential uses of deep learning methods including natu -\nral language processing and language models in predicting SNO sites is largely unex -\nplored. Furthermore, the existing approaches do not leverage the distilled information \nfrom these pLMs. To the best of our knowledge, embedding from pLMS has not been \npreviously used to predict SNO sites. In this regard, here we propose pLMSNOSite, a \nstacked generalization approach based on intermediate fusion of models that combines \ntwo different learned marginal  amino acid sequence representations: per-residue  con -\ntextual embedding learned on full sequences from a pre-trained protein language model \nand per-residue  supervised word embedding learned on window sequences. Based \non independent testing, pLMSNOSite performs better than other widely available \napproaches for SNO site prediction in proteins.\nMethods\nBenchmark dataset\nThe training and testing dataset for this work was adopted from PreSNO [16]. PreSNO \nutilizes the original DeepNitro [15] dataset which is curated through an extensive lit -\nerature search for experimentally verified S-nitrosylation sites. This dataset consists of \nan experimentally confirmed 4762 sites from 3113 protein sequences. These sequences \nare first subjected to homology removal using the cd-hit algorithm [28] with an iden -\ntity cut-off of 0.3, resulting in 3734 positive sites. The remaining cysteine residues from \nthe same protein sequences (ones that have the experimental SNO sites) are considered \nas the negative S-nitrosylation sites resulting in 20,548 negative sites. Furthermore, by \neliminating the negative site if there is an identical window sequence in the set of posi -\ntive sites, we obtained 20,333 negative sites. From these sites, the independent dataset \nPage 4 of 20Pratyush et al. BMC Bioinformatics           (2023) 24:41 \nis constructed by randomly sampling 20% of the sites, and the remaining sites are used \nto construct the training dataset. This resulted in 3383 SNO sites and 17,165 non-SNO \nsites in the training set and 351 SNO sites and 3168 non-SNO sites in the independent \ntest set. Clearly, the training set is highly skewed in class distribution towards negative \nsites. This imbalance in the training dataset was resolved by randomly undersampling \nthe negative sites. The balanced training set thus obtained was used for building the \nmodels whereas the independent test set was unaltered for assessing the generaliza -\ntion ability of the trained models on unseen data. Note that the main difference between \nDeepNitro [15] and PreSNO [16] datasets is the different cut-off used in cd-hit [28]. The \ndescription of the training dataset and independent dataset used in the study is shown in \nTables 1 and 2 respectively.\nSequence representation\nA critical step before passing amino acid sequences to a machine learning model is the \nnumerical encoding of each amino acid through an encoding scheme that assigns a \nnumerical representation to the amino acid. Choosing informative, discriminating, and \nindependent encoding (or features) is a crucial element of effective machine learning \nalgorithms. Most of the existing SNO prediction tools rely on manual or hand-crafted \nfeatures for the representation of amino acids [17]. We aim to eliminate the reliance on \nhand-crafted features by leveraging two feature representation approaches for estab -\nlishing a robust representation of S-nitrosylation sites: word embeddings from a super -\nvised embedding layer and embeddings from ProtT5  (ProtT5-XL-UniRef50) [21], a \npre-trained protein language model based on Google’s T5 (Text-to-Text Transfer Trans-\nformer) [29] architecture. Below, we describe these two types of embeddings in detail.\nWord embedding using supervised embedding layer\nWord embedding is a class of approaches to represent words using a dense vector rep -\nresentation. Protein sequences can be seen as documents, and amino acids that make \nTable 1 Number of proteins, number of sites, and training set used in this study (adopted from \nPreSNO)\nThe balanced sites are used for training the model\nSites Number of proteins Number of sites (before \nbalancing)\nNumber of sites \n(after balancing)\nSNO sites 1962 3383 3383\nNon-SNO sites 340 17,165 3383\nTotal 2302 20,548 6766\nTable 2 Number of proteins, positive, and negative sites of independent test set used in the \nexperiments (adopted from PreSNO)\nSites Number of proteins Number of sites\nSNO sites 267 351\nNon-SNO sites 231 3168\nTotal 438 3519\nPage 5 of 20\nPratyush et al. BMC Bioinformatics           (2023) 24:41 \n \nthe protein sequence can be seen as words. In that regard, amino acids (words) can be \nrepresented by dense vectors using word embeddings where a vector represents the pro-\njection of the amino acid into a continuous vector space. We used Keras’s embedding \nlayer [30], as in LMSuccSite [27], to implement supervised word embedding where the \nembedding is learned as a part of training a deep learning model. The process of param -\neter learning in this approach is supervised; the parameters are updated with subsequent \nlayers during the learning process under the supervision of a label.   With subsequent \nepochs, the layer learns a feature-rich representation of sequences while still preserv -\ning the semantic relation between amino acids (each vectorized representation being \northogonal in some other dimension [31]). The input for this representation is the win -\ndow sequence centered around the site of interest flanked by an equal number of resi -\ndues upstream and downstream. In cases where there are not enough residues to create \nthe window sequence, we pad the window with virtual amino acids (‘−’). Initially, the \namino acids are integer encoded, so that each amino acid can be represented by a unique \ninteger which is provided as an input to the embedding layer. Then, the embedding layer \nis initialized with random weights, and the layer will learn better embedding for all the \namino acids with subsequent epochs as the part of the training process. There are three \nsalient parameters in word embedding (obtained through Keras’s embedding layer) that \ndetermines the quality of the feature representation of amino acid sequences. These \nparameters are input_dim  denoting the size of the vocabulary, output_dim denoting \nthe length of the feature vector for each word and input_length denoting the maximum \nlength of input sequence (in our case, the length of window sequence). The vocabulary \nsize is set to 23 to represent 20 canonical, two non-canonical, and one virtual amino \nacid (denoted by ‘−’ .). Based on fivefold cross-validation on a wide range of values of \nembedding dimension, we obtained the best performance using a dimension of size four. \nSimilarly, performing fivefold cross-validation on multiple window sizes, we obtained \nthe best results using a window size of 37. Hence, the output of the embedding layer is \n37 × 4 where 37 is the window size and four is the embedding dimension. The hyperpa -\nrameter tuning of the window size (input_length) and the embedding dimension (out -\nput_dim) is explained in detail in the result section.\nEmbedding from pre‑trained protein language model ProtT5\nAnother representation that we use in our work is based on embeddings from ProtT5, a \npre-trained protein language model (pLM). The advances in Natural Language Process -\ning (NLP) gained by the development of newer language models have been transferred \nto protein sequences by learning to predict masked or missing amino acids using a large \ncorpus of protein sequences [21–23]. Processing/distilling the information learned by \nthese pLMs yields a representation of protein sequences referred to as embeddings [21]. \nRecently, these embeddings have been shown to be beneficial in various structural bio -\ninformatics tasks including but not limited to secondary structure prediction and sub -\ncellular location, among others. In that regard, in this work, we use pLM ProtT5 [21, \n27] as a static feature encoders to extract per residue embeddings for protein sequences \nfor which we are predicting S-nitrosylation sites. It is relevant to note that the input to \nProtT5 is the overall protein sequence. ProtT5 is a pLM trained on BFD (Big Fantastic \nDatabase consisting of 2.5 billion sequences), fine-tuned on Uniref50  consisting of 45 \nPage 6 of 20Pratyush et al. BMC Bioinformatics           (2023) 24:41 \nmillion sequences, and developed at Rostlab using T5 [29] architecture. Infact, postional \nencoding is learned specific to each attention head in the transfromer architecture which \nis shared across all the layers of attention stack. Using ProtT5, the per-residue embed -\ndings were extracted from the last hidden layer of the encoder model with the size of \nLx1024, where L is the size of the protein using the overall protein sequence as the input. \nAs suggested by ProtTrans [26], LMSuccSite [27], the encoder side of ProtT5 was used, \nand embeddings were extracted in half-precision. For our purpose, as the per-residue \nembeddings are a contextualized representation, we only used the 1024 length embed -\ndings for the site of interrogation (aka cystine ‘C’). The schematic of the extraction of \nembedding from ProtT5 is shown in Fig. 1.\nDeep learning models\nGiven the input and output, we train several DL models to learn underlying patterns in \nthe protein sequence.\nFig. 1 Extraction of Embeddings from ProtT5 language model, the site is the site of interrogation (C, \nrepresented in red)\nPage 7 of 20\nPratyush et al. BMC Bioinformatics           (2023) 24:41 \n \nSequence‑based models\nThe input to the model is the sequence of amino acids which can thought of as sequence \nof words in the field of natural language processing (NLP). Hence, an obvious option is to \nemploy models designed to train and process sequences, such as recurrent neural network \n(RNN), long-short term memory (LSTM) [32], bidirectional long-short term memory (BiL-\nSTM), and so forth. The main drawback of these sequence-oriented models is that they are \ncomputationally intense, requiring a large number of parameters for training.\nConvolution neural network (CNN) model\nCNN models have demonstrated great success in various computer vision tasks, where \nconvolution kernels or filters are used to learn and discern the spatial co-relation between \npixels in images. In our SNO site prediction setting, CNNs can help learn the underlying \nrelationship among the amino acids in the input protein sequence. CNNs are less compu-\ntationally intensive models than sequence-oriented models and facilitate the training of \ndeeper networks as significantly fewer parameters are needed to be learned. The usage of \nCNNs is prevalent in several PTM prediction tasks [13, 15, 27]. In our case, we use CNN to \nprocess the feature representation of the protein sequence obtained from the word embed-\nding layer as described in the previous section. The process of obtaining feature maps of \ninput integer encoded window sequence from the convolution layer (or kernel) is given by \nthe formula:\nwhere the input sequence is denoted by f and the kernel by h. The index of rows and \ncolumns in the resultant matrix is denoted by m and n respectively. Typically, we use \nmultiple convolutions over the input sequence which helps to extract diverse features \nfrom a single input map and the output maps are stacked forming a volume. The dimen -\nsion of the obtained feature map from convolution over volume can be calculated using \nthe following formula:\nwhere n is the size input sequence, nc is the number of channels, f is the kernel size, p \nis the used padding s is the used stride and nf is the number of kernels. The convolu -\ntion layer is followed by the max-pooling layer which selects the maximum value from \nregions of feature maps, creating a downsampled map. The downsampled feature map is \nthen flattened and passed into a conventional fully connected network. All the weights \nin the network are updated using the backpropagation algorithm. It is to be mentioned \nthat we use a non-linear activation function called ReLU (Rectified Linear Unit) in all \nlayers of the architecture for capturing non-linear signals in the data. Among other acti -\nvation functions, ReLU is widely adopted in deep learning applications due to its ben -\nefits such as representational sparsity and efficiency with respect to computation. The \nReLU activation function for a domain value x is given by:\n(1)G [m , n] =\n(\nf · h\n)\n[m , n] =\n∑\nj\n∑\nkh[j, k]f[m − j, n − k]\n(2)[n,n,nc].[f,f,nc] = ﬂoor n + 2 p − f\ns + 1 ,ﬂoor n + 2 p − f\ns + 1 ,nf\n(3)RELU (x) = max (0,x)\nPage 8 of 20Pratyush et al. BMC Bioinformatics           (2023) 24:41 \npLMSNOSite architecture\nThe general framework of the stacked generalization consists of two base models \n(level-0 models) and a higher-level meta-model (level-1 model, meta-classifier). Our \napproach pLMSNOSite (protein Language Model-based S-Nitrosylation Site predic -\ntor) uses stacked generalization to combine the meta-features  (marginal representa -\ntions) learned from the base models to achieve better prediction. Specifically, the first \nbase model (herein referred to as embedding layer module) learns the representation of \nlocal information of cysteine residue of interest captured by proximal residues within \nwindow sequences using supervised word embedding. The second base model (herein \nreferred to as pLM ProtT5 module) learns the contextualized information of the same \ncysteine residue generated by unsupervised pLM using a full-length sequence as input. \nThese learned features by the base models using different representations are fused \ntogether and a meta-model is learned adopting an ensemble approach known as stacked \ngeneralization. The overall architecture of pLMSNOSite is shown in Fig.  2. As shown \nin the figure, the architecture of pLMSNOSite consists of two base models: the super -\nvised embedding layer module and the ProtT5 module, followed by a higher-level meta-\nmodel (meta-classifier) that performs the feature-level fusion of base models. We further \nFig. 2 The overall architecture of pLMSNOSite with the two base models and a meta-classifier model\nPage 9 of 20\nPratyush et al. BMC Bioinformatics           (2023) 24:41 \n \ndescribe the supervised embedding layer module and ProtT5 modules and higher-level \nmeta-model in detail below.\nSupervised (word) embedding layer module\nThe input to this module is a protein window sequence (centered around the site of \ninterest flanked by an equal number of residues on both sides) that captures the local \ninteraction between amino acids surrounding the site of interrogation (in this case \nS-nitrosylation/non-S-nitrosylation sites) within the window sequence. We choose a \ndeep two-dimensional (2D) Convolutional Neural Network (CNN) to extract feature \nmaps from these localized interactions of proximal amino acids. The advantage of CNN \nover other sequence-oriented models has been explained in the previous section. Inter -\nestingly, the CNN model also showed promising performance in fivefold cross-valida -\ntion (refer result section). The 2D-CNN architecture in this module first consists of a \nword embedding layer which takes integer encoded sequence as an input. The output \nfrom this layer (37 × 4, where 37 is the window size and 4 is the embedding dimension) \nis passed into a 2D convolutional layer to extract feature maps from a window sequence \nfollowed by a dropout layer to prevent overfitting, a max-pooling layer and a fully con -\nnected layer consisting of a flatten layer and a dense layer. The hyperparameters asso -\nciated with the model architecture were determined by performing an extensive grid \nsearch based on fivefold cross-validation. The search space and optimal hyperparameter \nvalues of the model obtained from cross-validation are reported in the Additional file  1: \nTable S2. Finally, the feature map of size 16 obtained from the final hidden layer from \nthe optimized 2D-CNN model (hereafter dubbed Embedding2DCNN) is treated as the \noutput of the first base model.\npLM ProtT5 module\nIn this module, at first per-residue embeddings are extracted from the last hidden layer \nof the encoder models of ProtT5 of the size of Lx1024, where L is the length of the pro -\ntein using the overall protein sequence as the input. Subsequently, the 1024 features \ncorresponding to the site of interest are extracted and fed as an input to this module. \nA dense neural network was used to learn the representation from the obtained  fea -\ntures. The architecture of this model and its corresponding hyperparameter values in \nthis module were also chosen based on grid search using fivefold cross-validation. The \nsearch space and selected hyperparameter values are reported in the Additional file  1: \nTable S1. Similar to Embedding2DCNN, we obtained a feature map of size 4 from this \nbase model (hereafter dubbed as ProtT5ANN module).\nStacked generalization\nTo integrate the capability of the representation learned by the base models (Embed -\nding2DCNN and ProtT5ANN), we implemented stacked generalization of these mod -\nules. To this end, instead of stacking on a decision level or a score level, we performed \nan intermediate level feature fusion by concatenating the feature maps obtained from \nthe final hidden layers of the base models  (16 x 1 from the  Embedding2DCNN and \n4 x 1  from the ProtT5ANN) as explained in previous subsections. The fused features \nPage 10 of 20Pratyush et al. BMC Bioinformatics           (2023) 24:41 \nwere then used to train the meta-model (meta-classifier) that acts as the final inference \nmodel. Since the datasets used to train base models and meta-classifier are similar, there \nis a likelihood of data leakage about target information from base models to meta-classi -\nfier [33], which could result in overestimation of cross-validation performance leading to \nspuriousness in the model selection process. Considering this, we paid special attention \nto ensure that there is no data leakage from base models to meta-classifier. In this work, \nwe performed the fivefold cross-validation algorithm called Stacking with K-fold cross-\nvalidation, developed by Wolpert [34], to ensure no target information is leaked while \ntraining the meta-classifier. Initially, the overall training data are randomly split into K \nfolds. Subsequently, base models are trained using K-1 folds, and the models are tested \nagainst the remaining onefold validation set. The predictions or features obtained from \ndifferent base models for each fold are collected to train the next-level model (meta clas-\nsifier). As a result, the meta classifier is trained on a non-overlapping dataset prevent -\ning any potential data leakage. Similar to other modules, we selected a single layer feed \nforward neural network as the architecture for the stacked generalization model using \ncross-validation.\nModel training\nAll the deep learning models were trained to minimize the binary cross-entropy loss or \nlog loss function which is given by the following equation:\nwhere yi and y\n′\ni are the ground truth and predicted probability for the ith instance of N \npoints respectively.\nThe parameters in the model were optimized to minimize the above loss function \nusing Adam stochastic optimization method (AMSGrad variant) with an adaptive learn -\ning rate of 0.001, the decay rate for the first moment as 0.9, and the decay rate for the \nsecond moment as 0.999. Prior to training, the number of epochs was set to 200 and the \nbatch size was set to 128. Additionally, an early stopping strategy with patience equal to \n5 was used which stops the training after 5 epochs if no improvement in loss is recorded. \nAny potential overfitting while training was averted by carefully monitoring accuracy/\nloss curves.\nEvaluation of models and performance metrics\nWe adopt a stratified fivefold cross-validation strategy for model selection. Subsequently, \nwe perform independent testing to assess the generalization error of our approach as \nwell as compare with it the existing approaches. Below, we define the performance met -\nrics used for evaluating the models.\n(4)− 1\nN\n∑ N\ni=1\n[\nyilog\n(\ny\n′\ni\n)\n+\n(\n1 − yi\n)\nlog\n(\n1 − y\n′\ni\n)]\n(5)Accuracy(ACC ) = TP + TN\nTP + TN + FP+ FN\n(6)Sensitivity(SN ) = TP\nTP + FN\nPage 11 of 20\nPratyush et al. BMC Bioinformatics           (2023) 24:41 \n \nwhere TP (True Positive) is the number of actual SNO sites predicted as positive, TN \n(True Negative) is the number of non-SNO sites predicted as negative, FP (False Posi -\ntive) is the number of non-SNO sites predicted as positive and FN (False Negative) is the \nnumber of actual SNO sites predicted as negative.\nWe also use AUROC (Area Under Receiver Operating Characteristic curve) and AUPR \n(Area Under Precision-Recall curve) to further evaluate the discriminating performance \nof the models.\nResults\nAs described above, pLMSNOSite uses stacked generalization to combine the super -\nvised word embedding layer module (Embedding2DCNN) and the pLM ProtT5 module \n(ProT5ANN) using a meta-classifier. The meta-classifier in fact learns from the output \nof the base models and thus the base models were first optimized to robustly learn their \ncorresponding representations. Successively, the meta-classifier was optimized to pro -\nduce the classification inference accurately.\nInitially, we analyze the comparative performance of various ML/DL architectures for \nthe selection of the optimal base models using fivefold cross-validation. Subsequently, \nthe comparative cross-validation performance of various models was analyzed for the \nselection of optimal meta-classifier. Finally, we compare the performance of the overall \narchitecture pLMNOSite against existing SNO site prediction tools using the independ -\nent test set. The details of the results obtained from these experiments are presented in \nthe following subsections.\nSelection of window size and embedding dimension for word embedding module\nAs described in the Methods section, the supervised embedding layer has three major \nparameters: vocabulary size (input_dim), window size (input_length), and embedding \ndimension (output_dim). The input_dim is fixed to 23 based on the number of canonical \namino acids (= 20), non-canonical amino acids (= 2), and virtual amino acids (= 1). The \nwindow size (input_length) is important as too few residues might result in informa -\ntion loss while too many residues might result in loss of local contextual information of \nthe site. To obtain the optimal input_length, fivefold cross-validation was performed by \nvarying window sizes from 21 to 63. Similarly, a higher embedding dimension demands \nsubstantial computational cost and thus the optimal output_dim was determined by \nexhaustively searching the value of the embedding dimension in the search space rang -\ning from 2 to 32.\nThe cross-validation experiments suggest that the output_dim (or, embedding \ndimension) of 4 and input_length (or window size) of 37 produced the highest MCC \nand these values were utilized for further analysis. The obtained value of output \ndimension is indeed a significant improvement over the traditional binarization \n(7)Speciﬁcity(SP) = TN\nTN + FP\n(8)MCC= TP ∗ TN − FP∗ FN\n(TP + FP)(TP + FN)(TN + FP)(TN + FN)\nPage 12 of 20Pratyush et al. BMC Bioinformatics           (2023) 24:41 \nencoding (or, one-hot encoding) where static and relatively higher dimensional fea -\ntures are generated. It is also worthwhile to note that the optimal window size for \nPreSNO is 41 (only 2 residue difference on each side of the central residue). The \nsensitivity analysis of MCC  (mean) on fivefold cross-validation for different win -\ndow sizes and embedding dimension for Embedding2DCNN is shown in Fig.  3a, b \nrespectively and the respective plots for other models are in Additional file  1: Fig. S1.\nSelection of model architecture for the word embedding  module\nTo obtain the best architecture for the word embedding module, we performed a \nfivefold cross-validation of the model using various architectures: 2D-CNN [29], \nANN, LSTM [ 30], ConvLSTM [31], and BiLSTM using the value of window size \n(= 37), vocabulary size (=  23) and embedding dimension (=  4) obtained from the \nprior experiments. It must be noted here that the supervised word embedding is \nobtained as a part of the training process of the model, so we only experimented \nwith DL-based architectures. These DL architectures were tuned using grid search \nwith fivefold cross-validation over wide range of search space (provided in Addi -\ntional file  1: Table S2). The results of the fivefold cross-validation of the optimized \nmodels are shown in Table  3. Similarly, the AUPR and AUC for cross-validation for \nthese models are shown in Fig.  4. It can be observed from Table  3 as well as Fig.  4 \nFig. 3 Sensitivity analysis of MCC on fivefold cross-validation when a window size is varied keeping the \ndimension and vocabulary size constant (dimension = 4, vocabulary size = 23), b dimension is varied keeping \nthe window size and vocabulary size constant (window size = 37, vocabulary size = 4)\nTable 3 Fivefold cross-validation results (mean ± one standard deviation) of embedding layer \nmodule on the training set\nThe highest values in each category are bolded\nModel ACC SN SP MCC\n2D-CNN 0.688 ± 0.018 0.760 ± 0.063 0.615 ± 0.069 0.382 ± 0.034\nANN 0.658 ± 0.018 0.697 ± 0.0351 0.619 ± 0.010 0.318 ± 0.036\nLSTM 0.674 ± 0.011 0.816 ± 0.067 0.533 ± 0.074 0.368 ± 0.024\nConvLSTM 0.667 ± 0.006 0.836 ± 0.023 0.498 ± 0.017 0.355 ± 0.017\nBiLSTM 0.686 ± 0.009 0.747 ± 0.093 0.626 ± 0.083 0.380 ± 0.022\nPage 13 of 20\nPratyush et al. BMC Bioinformatics           (2023) 24:41 \n \nthat the 2D-CNN architecture produces the best results (MCC in the table and AUC \nin the figures). Based on this, 2D-CNN architecture was chosen as the final architec -\nture for the word embedding module.\nSelection of model architecture for the pLM module (protT5)\nIt has been observed in multiple studies encompassing various bioinformatics tasks \nthat a simple machine learning model is enough to obtain a satisfactory performance \nfor pLM based embeddings [26, 27]. Based on this knowledge, we experimented with \nANN (Artificial Neural Network), SVM (Support Vector Machine) [35], RF (Random \nForest) [36], XGBoost (Extreme Gradient Boosting), and AdaBoost (Adaptive Boost -\ning) architectures for protT5 module using fivefold cross-validation. The scikit-learn’s \nGridsearchCV was used to optimize SVM, RF, XGBoost and AdaBoost with cv  as 5 and \nparam_grid (parameters grid) value as mentioned in the Additional file  1: Table S1. The \nresults of the fivefold cross-validation of the optimal models are reported in Table  3 and \nROC and PR curves for the same are shown in Fig.  5. It can be observed that the ANN \narchitecture produced the best results (MCC in Table  4 and AUC in Fig.  5). Based on \nthis, ANN architecture was chosen as the final architecture for the pLM ProtT5 module.\nFig. 4 a ROC curves and area under curve (AUC) values for different architectures for the supervised \nembedding layer module. b Precision-recall (PR) curves and area under curve (AUC) values for different \narchitectures for the supervised embedding layer module\nFig. 5 a Area under ROC curves (AUROC) values for different architectures for the ProtT5 module. b Area \nunder precision-recall (AUPR) values for different architectures for the ProtT5 module\nPage 14 of 20Pratyush et al. BMC Bioinformatics           (2023) 24:41 \nSelection of model architecture for meta classifier\nAdditionally, the optimal architecture for the stacked generalization (aka meta classifier) \nwas obtained using fivefold cross-validation on various ML models. Essentially, during \nthe cross-validation of models for the meta-classifier, the intermediate features obtained \nfrom base models were used paying special attention to any potential leakage of target \ninformation in the training of the meta classifier as described in the methods section. \nThe candidate models for the meta-classifier were optimized using this approach (data \nleakage mitigation) for fivefold cross-validation (over the search space reported in Addi -\ntional file 1: Table S3). Table 5 and Fig. 6 show the comparison of the optimized models \nbased on fivefold cross-validation. These results indicate that Artificial Neural networks \n(ANN) achieves better validation performance compared to other classifiers in terms \nof MCC and competitive results in terms of AUPR and AUROC. The meta-classifier \nTable 4 Fivefold cross validation results (mean ± one standard deviation) of different models based \non ProtT5 features\nHighest values in each column are highlighted in bold\nArchitecture ACC SN SP MCC\nANN 0.710 ± 0.015 0.745 ± 0.028 0.674 ± 0.015 0.421 ± 0.030\nSVM 0.700 ± 0.012 0.702 ± 0.016 0.699 ± 0.020 0.401 ± 0.024\nRF 0.682 ± 0.010 0.815 ± 0.815 0.549 ± 0.815 0.379 ± 0.378\nXGBoost 0.699 ± 0.008 0.752 ± 0.019 0.645 ± 0.007 0.400 ± 0.016\nAdaBoost 0.672 ± 0143 0.695 ± 0.024 0.650 ± 0.022 0.345 ± 0.029\nTable 5 Performance comparison using different architectures for meta-classifier based on fivefold \ncross-validation results (mean ± one standard deviation)\nThe highest value in each column is highlighted in bold\nModel ACC SN SP MCC\nANN 0.727 ± 0.017 0.769 ± 0.016 0.685 ± 0.033 0.4573 ± 0.032\nLR 0.703 ± 0.014 0.740 ± 0.017 0.665 ± 0.028 0.407 ± 0.027\nSVM 0.719 ± 0.021 0.807 ± 0.029 0.631 ± 0.017 0.445 ± 0.043\nRF 0.724 ± 0.010 0.771 ± 0.026 0.678 ± 0.022 0.451 ± 0.021\nXGBoost 0.697 ± 0.006 0.735 ± 0.014 0.660 ± 0.022 0.396 ± 0.011\nFig. 6 Results based on fivefold cross-validation a ROC curves and area under curve (AUC) values for \ndifferent architectures for the meta classifier model. b Precision-recall (PR) curves and area under curve (AUC) \nvalues for different architectures for meta classifier model\nPage 15 of 20\nPratyush et al. BMC Bioinformatics           (2023) 24:41 \n \nbased on ANN was hence chosen for our work and we call the overall approach as \npLMSNOSite.\nPerformance of base models and pLMSNOSite on independent test set\nTo observe the relative performance of the base models (aka Embedding2DCNN and \nProtT5ANN) and ensemble model on the independent test set, we compared the per -\nformance of these models using an independent test set. Note that this independent \ntest set is imbalanced and that these results have no effect whatsoever on model selec -\ntion (model selection was solely done based on the results of fivefold cross-validation \non training data). The ROC and PR curves of the base models and ensemble model \n(pLMSNOSite) are shown in Fig. 7 and Table 6 shows other performance metrics for the \nbase models and ensemble model. The results indicate that the ensemble model (pLM -\nSNOSite) exhibits higher AUROC, AUPR and MCC compared to the base models. This \ndemonstrates the better generalization ability of the ensemble model (pLMSNOSite) \ncompared to the base models. From the figure, the AUPR values are quite low which is \nto be expected because precision and recall are focused on minority class (minority class \nsize: 351, majority class size: 3168). Nevertheless, pLMSNOSite still has better precision \ncompared to other existing approaches (Additional file 1: Fig. S4).\nFurthermore, we analyzed the performance of pLMSNOSite and base models under \nvarious controlled specificity values. As shown in Fig.  8, we can observe that the pro -\nposed pLMSNOSite approach performs better in terms of MCC and sensitivity at vari -\nous values of controlled specificity. Also, we can concur that as the models become more \nspecific, pLMSNOSite is still able to outperform the base models.\nFig. 7 Results based on independent test set (imbalanced): a ROC curve and b AUPR curve for the base \nmodels and pLMSNOSite\nTable 6 Performance comparison of base models (aka Embedding2DCNN and ProtT5ANN models) \nand ensemble model (pLMSNOSite)\nThe highest value in each column is highlighted in bold\nModels ACC SN SP MCC\nEmbedding2DCNN 0.706 0.798 0.696 0.310\nProtT5ANN 0.791 0.598 0.812 0.293\npLMSNOSite 0.769 0.735 0.772 0.340\nPage 16 of 20Pratyush et al. BMC Bioinformatics           (2023) 24:41 \nIt must be noted that pLMSNOSite was selected as the final predictor based on the \ncross-validation experiments, and these results were presented to simply assess the per -\nformance of the base models and meta-model on the independent test set.\nComparison with other existing tools using an independent test set\nFinally, we compared the performance of our approach (pLMNOSite) with other \nexisting SNO site prediction tools using an independent test set described in the \nBenchmark dataset section. Specifically, our approach was compared against widely \navailable tools such as GPS-SNO [9 ], SNOSite [10], iSNO-PseAAC [11], DeepNitro \n[15], and PreSNO [16]. It must be pointed out that the same training and independ -\nent test set used by PreSNO predictor was employed for our analysis for fair com -\nparison. The results of the comparison are presented in Table  7 and note that the \nresults for other predictors were adopted from PreSNO [16]. It can be observed from \nTable 7 that the pLMSNOSite achieves the best MCC (=  0.340) among the compared \napproaches showing an improvement of ∼ 35.0% in MCC compared to the next best \napproach (PreSNO). Additionally, it also exhibited an ∼21.7% increase in sensitivity \nFig. 8 Comparison of MCC and Sensitivity of pLMSNOSite with base models under different controlled \nspecificity values\nPage 17 of 20\nPratyush et al. BMC Bioinformatics           (2023) 24:41 \n \nand improvements in terms of specificity and accuracy. It is worth noting that pLM -\nNOSite struck the most balance between sensitivity and specificity with a g-mean \n(geometric mean of sensitivity and specificity) of 0.754, a ∼ 10.6% improvement over \nPreSNO. Additionally, it can also be seen that the ProtT5 model alone has a better \nMCC (=  0.293) than the other compared approaches. Based on these results, it can \nbe concluded that our novel approach termed pLMSNOSite is a robust predictor of \nS-nitrosylation sites in proteins.\nt‑SNE visualization of pLMSNOSite\nAdditionally, we used t-distributed stochastic neighbor embedding (t-SNE) [37] to \nproject the learned features from the final hidden layer into  R2 cartesian space. With \na perplexity value of 50 and a learning rate of 500, the t-SNE was visualized from the \ntraining data using a scatter plot (Fig.  9). It can be inferred from the plot that the \nboundary of separation between SNO sites (blue data points) and non-SNO sites \n(orange data points) is quite pronounced indicating that the proposed stacked gener -\nalization approach is able to discriminate between the positive sites and the negative \nsites.\nTable 7 Performance comparison of pLMSNOSite against other existing approaches using the \nindependent test set\nThe highest values in each column are highlighted in bold\nNote that the values for other approaches were adopted from PreSNO. Although same independent test set was used for all \nthe approaches, there is a slight variation in the number of total positive and negative sites. Nevertheless, the integrity of \ncomparison is not compromised at all\nPredictors TP FP TN FN ACC SN SP MCC AUROC\nGPS-SNO 99 825 2337 253 0.693 0.281 0.739 0.014 0.523\niSNO-PseAAC 101 768 2394 251 0.710 0.287 0.757 0.031 –\nSNOSite 235 1749 1413 117 0.469 0.668 0.447 0.069 –\nDeepNitro 202 776 2386 148 0.737 0.578 0.737 0.222 0.731\nPreSNO 211 733 2431 141 0.752 0.604 0.769 0.252 0.756\npLMSNOSite 258 718 2446 93 0.769 0.735 0.773 0.340 0.754\nFig. 9 2D t-SNE visualization of the learned features from training data by pLMSNOSite\nPage 18 of 20Pratyush et al. BMC Bioinformatics           (2023) 24:41 \nDiscussions and conclusions\nProtein S-nitrosylation is one of the important protein post-translational modifica -\ntions that is responsible for regulating protein functions and cell signaling of all main \nclasses of proteins. In this work, we developed a computational tool to predict pro -\ntein S-nitrosylation sites called pLMSNOSite that combines a supervised embedding \nlayer model and a protein language model (based on ProtT5) using a stacked gen -\neralization approach. Based on independent test results, pLMSNOSite shows better \nperformance than the compared existing tools. As can be seen from the results, the \nimproved performance of our approach can mainly be attributed to the new embed -\nding representation obtained from ProtT5 (a protein language model). One of the \nbenefits of language models like ProtT5 is that it is learned on overall sequence to \nextract the contextualized embedding of the site of interest as a consequence of which \nthe dependency on defining local contextual information of the site based on the win -\ndow size (which demands additional overhead for hyperparameter tuning) is averted.  \nBased on the experimental results, it can be concluded that pLMSNOSite is a promis -\ning tool for predicting protein S-nitrosylation sites. The trained pLMSNOSite model \nand related dataset are provided in our public GitHub repository (https:// github. com/  \nKCLab MTU/ pLMSN OSite) for the community.\nAs in pLMSNOSite, the representation of protein sequences using protein language \nmodel could be explored to improve other protein bioinformatics tasks like protein-drug \ninteraction prediction [38]. Essentially, by representing the protein target using pLMs we \nmay expect improved protein-drug interaction prediction. Additionally, the protein lan -\nguage model could be used for improved protein–protein interaction prediction (PPI) [39] \nwhere representations for both proteins can be extracted using pLMs. Although pLM -\nSNOSite shows promising performance, the predictive performance of pLMSNOSite \ncould be improved by leveraging the vast amount of structural data made available due \nto the success of AlphaFold2 [18]. Additionally, pLMSNOSite only uses sequence features \nfrom ProtT5 language model for feature extraction but there are other recent protein lan-\nguage models (e.g. ESM-2 [24]) and exploration of these language models for SNO site pre-\ndiction could be other important future work. Since our method uses ProtT5, our method \nmight require appropriate computational resources for very long protein sequences.\nAbbreviations\nSNO  S-nitrosylation\nLMs  Language models\nT5  Text to text transfer transformer\nt-SNE  T-distributed stochastic neighbor embedding\nPTM  Post translational modification\nMCC  Mathew correlation coefficient\nROC  Receiver operating characteristics\nAUC   Area under ROC curve\nPR  Precision-recall\nReLU  Rectified linear unit\nCNN  Convolutional neural network\nLSTM  Long short-term memory\nBiLSTM  Bidirectional long short-term memory\nConvLSTM  Convolutional long short-term memory\nDL  Deep learning\nSVM  Support vector machine\nLR  Logistic regression\nRF  Random forest\nPPI  Protein–protein interaction\nPage 19 of 20\nPratyush et al. BMC Bioinformatics           (2023) 24:41 \n \nSupplementary Information\nThe online version contains supplementary material available at https:// doi. org/ 10. 1186/ s12859- 023- 05164-9.\nAdditional file 1. Contains supplementary tables and figures referred to in the manuscript. In sections 1, 2, and 3, \nwe describe various ML/DL architectures and their respective hyperparameters. Table S1. Hyperparameter search \nspace for models in the ProtT5 module. Table S2. Hyperparameter search space for models in the word embedding \nmodule. Table S3. Hyperparameter search space for models in the meta-classifier. Table S4. fivefold cross-validation \nresults of Embedding2DCNN and ProtT5ANN when imbalanced learning (based on cost-sensitive learning) is \nperformed. Table S5. Best combination (with respect to MCC) of window size and embedding dimension for each of \nthe candidate models for the word embedding module on fivefold cross-validation. Figure S1. The sensitive analysis \ncurves of each DL model in the word embedding module on fivefold cross-validation. Table S6. Comparison of \nProtT5 with other pLMs such as ProtBERT (BERT-based ProtTrans family model) and Meta’s ESM-1 using independent \ntesting. Figure S2. Frequency and WebLogo plots for train positive and train negative window sequences (window \nsize = 37). Figure S3. Precision-Recall curves were produced for base models and pLMSNOSite using an imbalanced \nindependent set and a balanced independent test set separately. Figure S4. Comparison of pLMSNOSite with other \nexisting predictors based on precision values using an independent test set.\nAcknowledgements\nWe acknowledge the High-Performance Computing resources made available to us by Michigan Tech. We also acknowl-\nedge helpful discussions with Subash C. Pakhrin, Meenal Chaudhari, Hamid Ismail, Ženia Sidorov and Soufia Bahmani.\nAuthor contributions\nPP , SP , HS, DK conceived of and designed the experiments. PP and SP performed the experiments and data analysis. PP , \nSP , DK wrote the paper. PP , SP , HS, and DK revised the manuscript. DK oversaw the overall project. All authors read and \napproved the final manuscript.\nFunding\nThis work was supported by National Science Foundation (NSF) grant nos. 1901793, 1564606 (to DK)  JSPS KAKENHI \ngrant no.s JP19H04176 and JP22K19834 (to SH).\nAvailability of data and materials\nThe datasets, trained models, source codes, and other resources used in this study are publicly available https:// github. \ncom/ KCLab MTU/ pLMSN OSite.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare that they have no competing interests.\nReceived: 14 October 2022   Accepted: 30 January 2023\nReferences\n 1. Fernando V, et al. S-nitrosylation: an emerging paradigm of redox signaling. Antioxidants (Basel). 2019;8(9):404.\n 2. Martinez-Ruiz A, Cadenas S, Lamas S. Nitric oxide signaling: classical, less classical, and nonclassical mechanisms. \nFree Radic Biol Med. 2011;51(1):17–29.\n 3. Hess DT, et al. Protein S-nitrosylation: purview and parameters. Nat Rev Mol Cell Biol. 2005;6(2):150–66.\n 4. Lamotte O, et al. Protein S-nitrosylation: specificity and identification strategies in plants. Front Chem. 2014;2:114.\n 5. Jaffrey SR, Snyder SH. The biotin switch method for the detection of S-nitrosylated proteins. Sci STKE. \n2001;2001(86):L1.\n 6. Stamler JS, Lamas S, Fang FC. Nitrosylation. The prototypic redox-based signaling mechanism. Cell. \n2001;106(6):675–83.\n 7. Marino SM, Gladyshev VN. Structural analysis of cysteine S-nitrosylation: a modified acid-based motif and the \nemerging role of trans-nitrosylation. J Mol Biol. 2010;395(4):844–59.\n 8. Smith BC, Marletta MA. Mechanisms of S-nitrosothiol formation and selectivity in nitric oxide signaling. Curr Opin \nChem Biol. 2012;16(5–6):498–506.\n 9. Xue Y, et al. GPS-SNO: computational prediction of protein S-nitrosylation sites with a modified GPS algorithm. PLoS \nONE. 2010;5(6): e11290.\n 10. Lee TY, et al. SNOSite: exploiting maximal dependence decomposition to identify cysteine S-nitrosylation with \nsubstrate site specificity. PLoS ONE. 2011;6(7): e21849.\n 11. Xu Y, et al. iSNO-PseAAC: predict cysteine S-nitrosylation sites in proteins by incorporating position specific amino \nacid propensity into pseudo amino acid composition. PLoS ONE. 2013;8(2): e55844.\nPage 20 of 20Pratyush et al. BMC Bioinformatics           (2023) 24:41 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \n 12. Hao G, et al. SNOSID, a proteomic method for identification of cysteine S-nitrosylation sites in complex protein \nmixtures. Proc Natl Acad Sci USA. 2006;103(4):1012–7.\n 13. Pakhrin SC, et al. Deep learning-based advances in protein posttranslational modification site and protein cleavage \nprediction. Methods Mol Biol. 2022;2499:285–322.\n 14. Meng LK, et al. Mini-review: recent advances in post-translational modification site prediction based on deep learn-\ning. Comput Struct Biotechnol J. 2022;20:3522–32.\n 15. Xie Y, et al. DeepNitro: prediction of protein nitration and nitrosylation sites by deep learning. Genom Proteom \nBioinform. 2018;16(4):294–306.\n 16. Hasan MM, et al. Prediction of S-nitrosylation sites by integrating support vector machines and random forest. Mol \nOmics. 2019;15(6):451–8.\n 17. Zhao Q, et al. Recent advances in predicting protein S-nitrosylation sites. Biomed Res Int. 2021;2021:5542224.\n 18. Jumper J, et al. Highly accurate protein structure prediction with AlphaFold. Nature. 2021;596(7873): p. 583-+.\n 19. Badal VD, Kundrotas PJ, Vakser IA. Natural language processing in text mining for structural modeling of protein \ncomplexes. BMC Bioinform. 2018;19(1):84.\n 20. Pokharel S, et al. NLP-based encoding techniques for prediction of post-translational modification sites and protein \nfunctions. In: K. Lukasz (ed) Machine learning in bioinformatics of protein sequences: algorithms, databases and \nresources for modern protein bioinformatics. World Scientific Publishing Company. 2023.\n 21. Elnaggar A, et al. ProtTrans: towards cracking the language of lifes code through self-supervised deep learning and \nhigh performance computing. IEEE Trans Pattern Anal Mach Intell. 2021;44(10):7112–27.\n 22. Rives A, et al., Biological structure and function emerge from scaling unsupervised learning to 250 million protein \nsequences. Proc Natl Acad Sci USA. 2021;118(15).\n 23. Brandes N, et al. ProteinBERT: a universal deep-learning model of protein sequence and function. Bioinformatics. \n2022.\n 24. Rives A, et al. Biological structure and function emerge from scaling unsupervised learning to 250 million protein \nsequences. Proc Natl Acad Sci USA. 2021. 118(15).\n 25. Heinzinger M, et al. Contrastive learning on protein embeddings enlightens midnight zone. NAR Genom Bioinform. \n2022;4(2):lqac043.\n 26. Littmann M, et al. Protein embeddings and deep learning predict binding residues for various ligand classes. Sci \nRep. 2021;11(1):23916.\n 27. Pokharel S, et al. Improving protein succinylation sites prediction using embeddings from protein language model. \nSci Rep. 2022;12(1):16933.\n 28. Li W, Godzik A. Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences. \nBioinformatics. 2006;22(13):1658–9.\n 29. Raffel C, et al. Exploring the limits of transfer learning with a unified text-to-text transformer. J Mach Learn Res. \n2020;21:5485–551.\n 30. Lee H, Song J. Introduction to convolutional neural network using Keras: an understanding from a statistician. Com-\nmun Stat Appl Methods. 2019;26(6):591–610.\n 31. Li H, et al. Deep neural network based predictions of protein interactions using primary sequences. Molecules. \n2018;23(8):1923.\n 32. Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput. 1997;9(8):1735–80.\n 33. Ting KM, Witten IH. Issues in stacked generalization. J Artif Intell Res. 1999;10:271–89.\n 34. Wolpert DH. Stacked generalization. Neural Netw. 1992;5(2):241–59.\n 35. Hearst MA. Support vector machines. IEEE Intell Syst Their Appl. 1998;13(4):18–21.\n 36. Breiman L. Random forests. Mach Learn. 2001;45(1):5–32.\n 37. van der Maaten L, Hinton G. Visualizing data using t-SNE. J Mach Learn Res. 2008;9:2579–605.\n 38. Zhao BW, et al. A novel method to predict drug-target interactions based on large-scale graph representation learn-\ning. Cancers (Basel). 2021;13(9):2111.\n 39. Hu L, et al. A survey on computational models for predicting protein–protein interactions. Brief Bioinform. \n2021;22(5).\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations."
}