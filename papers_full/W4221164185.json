{
  "title": "TextPruner: A Model Pruning Toolkit for Pre-Trained Language Models",
  "url": "https://openalex.org/W4221164185",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2633168765",
      "name": "Ziqing Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103132936",
      "name": "Yiming Cui",
      "affiliations": [
        "Harbin Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2105930127",
      "name": "Zhigang Chen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2764043458",
    "https://openalex.org/W4220998098",
    "https://openalex.org/W4297790889",
    "https://openalex.org/W3105234097",
    "https://openalex.org/W4206634569",
    "https://openalex.org/W3015609966",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4287777801",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3008219293",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Pre-trained language models have been prevailed in natural language processing and become the backbones of many NLP tasks, but the demands for computational resources have limited their applications. In this paper, we introduce TextPruner, an open-source model pruning toolkit designed for pre-trained language models, targeting fast and easy model compression. TextPruner offers structured post-training pruning methods, including vocabulary pruning and transformer pruning, and can be applied to various models and tasks. We also propose a self-supervised pruning method that can be applied without the labeled data. Our experiments with several NLP tasks demonstrate the ability of TextPruner to reduce the model size without re-training the model.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nSystem Demonstrations, pages 35 - 43\nMay 22-27, 2022 ©2022 Association for Computational Linguistics\nTextPruner: A Model Pruning Toolkit for Pre-Trained Language Models\nZiqing Yang†, Yiming Cui‡†, Zhigang Chen†\n†State Key Laboratory of Cognitive Intelligence, iFLYTEK Research, China\n‡Research Center for Social Computing and Information Retrieval (SCIR),\nHarbin Institute of Technology, Harbin, China\n†{zqyang5,ymcui,zgchen}@iflytek.com\n‡ymcui@ir.hit.edu.cn\nAbstract\nPre-trained language models have been pre-\nvailed in natural language processing and be-\ncome the backbones of many NLP tasks, but\nthe demands for computational resources have\nlimited their applications. In this paper, we\nintroduce TextPruner, an open-source model\npruning toolkit designed for pre-trained lan-\nguage models, targeting fast and easy model\ncompression. TextPruner offers structured post-\ntraining pruning methods, including vocabulary\npruning and transformer pruning, and can be\napplied to various models and tasks. We also\npropose a self-supervised pruning method that\ncan be applied without the labeled data. Our ex-\nperiments with several NLP tasks demonstrate\nthe ability of TextPruner to reduce the model\nsize without re-training the model. 1\n1 Introduction\nLarge pre-trained language models (PLMs) (De-\nvlin et al., 2019; Liu et al., 2019) have achieved\ngreat success in a variety of NLP tasks. However,\nit is difﬁcult to deploy them for real-world applica-\ntions where computation and memory resources are\nlimited. Reducing the pre-trained model size and\nspeeding up the inference have become a critical\nissue.\nPruning is a common technique for model com-\npression. It identiﬁes and removes redundant or\nless important neurons from the networks. From\nthe view of the model structure, pruning methods\ncan be categorized into unstructured pruning and\nstructured pruning. In the unstructured pruning,\neach model parameter is individually removed if\nit reaches some criteria based on the magnitude\nor importance score (Han et al., 2015; Zhu and\nGupta, 2018; Sanh et al., 2020). The unstructured\npruning results in sparse matrices and allows for\nsigniﬁcant model compression, but the inference\n1The source code and the documentation are available at\nhttp://textpruner.hfl-rc.com\nspeed can hardly be improved without specialized\ndevices. While in the structured pruning, rows or\ncolumns of the parameters are removed from the\nweight matrices (McCarley, 2019; Michel et al.,\n2019; V oita et al., 2019; Lagunas et al., 2021; Hou\net al., 2020). Thus, the resulting model speeds up\non the common CPU and GPU devices.\nPruning methods can also be classiﬁed into\noptimization-free methods (Michel et al., 2019)\nand the ones that involve optimization (Frankle and\nCarbin, 2019; Lagunas et al., 2021). The latter usu-\nally achieves higher performance, but the former\nruns faster and is more convenient to use.\nPruning PLMs has been of growing interest.\nMost of the works focus on reducing transformer\nsize while ignoring the vocabulary (Abdaoui et al.,\n2020). Pruning vocabulary can greatly reduce the\nmodel size for multilingual PLMs.\nIn this paper, we present TextPruner, a model\npruning toolkit for PLMs. It combines both trans-\nformer pruning and vocabulary pruning. The pur-\npose of TextPruner is to offer a universal, fast, and\neasy-to-use tool for model compression. We ex-\npect it can be accessible to users with little model\ntraining experience. Therefore, we implement the\nstructured optimization-free pruning methods for\nits convenient use and fast computation. Pruning\na base-sized model only requires several minutes\nwith TextPruner. TextPruner can also be a useful\nanalysis tool for inspecting the importance of the\nneurons in the model.\nTextPruner has the following highlights:\n• TextPruner is designed to be easy to use. It\nprovides both Python API and Command Line\nInterface (CLI). Working with either of them\nrequires only a couple of lines of simple code.\nBesides, TextPruner is non-intrusive and com-\npatible with Transformers (Wolf et al., 2020),\nwhich means users do not have to change their\nmodels that are built on the Transformers li-\nbrary.\n35\n• TextPruner works with different models and\ntasks. It has been tested on tasks like text\nclassiﬁcation, machine reading comprehen-\nsion (MRC), named entity recognition (NER).\nTextPruner is also designed to be extensible\nfor other models.\n• TextPruner is ﬂexible. Users can control the\npruning process and explore pruning strate-\ngies via tuning the conﬁgurations to ﬁnd the\noptimal conﬁgurations for the speciﬁc tasks.\n2 Pruning Methodology\nWe brieﬂy recall the multi-head attention (MHA)\nand the feed-forward network (FFN) in the trans-\nformers (Vaswani et al., 2017). Then we describe\nhow we prune the attention heads and the FFN\nbased on the importance scores.\n2.1 MHA and FFN\nSuppose the input to a transformer is X ∈Rn×d\nwhere n is the sequence length and d is the hidden\nsize. the MHA layer with Nh heads is parameter-\nized by WQ\ni , WK\ni , WV\ni , WO\ni ∈Rdh×d\nMHA(X) =\nNh∑\ni\nAttWQ\ni ,WK\ni ,WV\ni ,WO\ni\n(X) (1)\nwhere dh = d/Nh is the hidden size of each\nhead. AttWQ\ni ,WK\ni ,WV\ni ,WO\ni\n(X) is the bilinear self-\nattention\nAttWQ\ni ,WK\ni ,WV\ni ,WO\ni\n(X) =\nsoftmax(X(WQ\ni )⊤WK\ni X⊤\n√\nd\n)X(WV\ni )⊤WO\ni\n(2)\nEach transformer contains a fully connected\nfeed-forward network (FFN) following MHA. It\nconsists of two linear transformations with a GeLU\nactivation in between\nFFNW1,b1,W2,b2(X) =\nGeLU(XW1 + b1)W2 + b2 (3)\nwhere W1 ∈Rd×dff , W2 ∈Rdff ×d, b1 ∈Rdff ,\nb2 ∈ Rd. dff is the FFN hidden size. The\nadding operations are broadcasted along the se-\nquence length dimension n.\n2.2 Pruning with Importance Scores\nWith the hidden size ﬁxed, The size of a trans-\nformer can be reduced by removing the attention\nheads or removing the intermediate neurons in the\nFFN layer (decreasing dff , which is mathemat-\nically equal to removing columns from W1 and\nrows from W2). Following Michel et al. (2019),\nwe sort all the attention heads and FFN neurons ac-\ncording to their proxy importance scores and then\nremove them iteratively.\nA commonly used importance score is the sen-\nsitivity of the loss with respect to the values of the\nneurons. We denote a set of neurons or their out-\nputs as Θ. Its importance score is computed by\nIS(Θ) = Ex∼X\n⏐⏐⏐⏐\n∂L(x)\n∂Θ Θ\n⏐⏐⏐⏐ (4)\nThe expression in the absolute sign is the ﬁrst-order\nTaylor approximation of the loss Laround Θ = 0.\nTaking Θ to be the output of an attention head hi,\nIS(Θ) gives the importance score of the head i;\nTaking Θ to be the set of the i-th column of W1,\ni-the row of W2 and the i-th element of b1, IS(Θ)\ngives the importance score of the i-th intermeidate\nneuron in the FFN layer.\nA lower importance score means the loss is less\nsensitive to the neurons. Therefore, the neurons\nare pruned in the order of increasing scores. In\npractice, we use the development set or a subset of\nthe training set to compute the importance score.\n2.3 Self-Supervied Pruning\nIn equation (4), the loss Lusually is the training\nloss. However, there can be other choices of L. We\npropose to use the Kullback–Leibler divergence to\nmeasure the varitaion of the model outputs:\nLKL(x) = KL(stopgrad(q(x))||p(x)) (5)\nwhere q(x) is the original model prediction distribu-\ntion and p(x) is the to-be-pruned model prediction\ndistribution. The stopgrad operation is used to\nstop back-propagating gradients. An increase in\nLKL indicates an increase in the diviation of p(x)\nfrom the original predictionq(x). Thus the gradient\nof LKL reﬂects the sensitivity of the model to the\nvalue of the neurons. Evaluation of LKL does not\nrequire label information. Therefore the pruning\nprocess can be performed in a self-supervised way\nwhere the unpruned model provides the soft-labels\nq(x). We call the method self-supervised prun-\ning. TextPruner supports both supervised pruning\n36\nFigure 1: Three pruning modes in TextPruner.\n(where Lis the training loss) and self-supervised\npruning. We will compare them in the experiments.\n3 Overview of TextPruner\n3.1 Pruning Mode\nAs illustrated in Figure 1, there are three pruning\nmodes In TextPruner.\nVocabulary Pruning The pre-trained models\nhave a large vocabulary, but some tokens in the\nvocabulary rarely appear in the downstream tasks.\nThese tokens can be removed to reduce the model\nsize and accelerate the training speed of the tasks\nthat require predicting probabilities over the whole\nvocabulary. In this mode, TextPruner reads and to-\nkenizes an input corpus. TextPruner goes through\nthe vocabulary and checks if the token in the vocab-\nulary has appeared in the text ﬁle. If not, the token\nwill be removed from both the model’s embedding\nmatrix and the tokenizer’s vocabulary.\nTransformer Pruning Previous studies (Michel\net al., 2019; V oita et al., 2019) have shown that\nnot all attention heads are equally important in the\ntransformers, and some of the attention heads can\nbe pruned without performance loss (Cui et al.,\n2022). Thus, Identifying and removing the least\nimportant attention heads can reduce the model\nsize and have a small impact on performance.\nIn this mode, TextPruner reads the examples and\ncomputes the importance scores of attention heads\nand the feed-forward networks’ neurons. The heads\nand the neurons with the lowest scores are removed\nﬁrst. This process is repeated until the model has\nbeen reduced to the target size. TextPruner also\nsupports custom pruning from user-provided masks\nwithout computing the importance scores.\nPipeline Pruning In this mode, TextPruner per-\nforms transformer pruning and vocabulary pruning\nautomatically to fully reduce the model size.\n3.2 Pruners\nThe pruners are the cores of TextPruner,\nand they perform the actual pruning process.\nThere are three pruner classes, corresponding\nto the three aforementioned pruning modes:\nVocabularyPruner, TransformerPruner and\nPipelinePruner. Once the pruner is intialized, call\nthe pruner.prune(. . .) to start pruning.\n3.3 Conﬁgurations\nThe following conﬁguration objects set the pruning\nstrategies and the experiment settings.\nGeneralConﬁg It sets the device to use (CPU or\nCUDA) and the output directory for model saving.\nVocabularyPruningConﬁg It sets the token\npruning threshold min_count and whether prun-\ning the LM head prune_lm_head. The token\nis to be removed from the vocabulary if it ap-\npears less than min_count times in the corpus; if\nprune_lm_head is true, TextPruner prunes the\nlinear transformation in the LM head too.\nTransformerPruningConﬁg The transformer\npruning parameters include but not are limited to:\n• pruning_method can be mask or iterative.\nIf it is iterative, the pruner prunes the model\nbased on the importance scores; if it is mask,\nthe pruner prunes the model with the masks\ngiven by the users.\n• target_ffn_size denotes the average\nFFN hidden size dff per layer.\n• target_num_of_heads denotes the aver-\nage number of attention heads per layer.\n• n_iters is number of pruning iterations.\nFor example, if the original model has Nh\nheads per layer, the target model has N′\nh\nheads per layer, the pruner will prune (Nh −\nN′\nh)/n_iters heads on average per layer per\niteration. It also applies to the FFN neurons.\n• If ffn_even_masking is true, all the FFN\nlayers are pruned to the same size dff ; other-\nwise, the FFN sizes vary from layer to layer\nand their average size is dff .\n• If head_even_masking is true, all the\nMHAs are pruned to the same number of\nheads; otherwise, the number of attention\nheads varies from layer to layer.\n37\nFigure 2: The workﬂow of TextPruner. The yellow blocks are the general arguments for any pruners. The green\nblocks should be provided for the TransformerPruner and PipelinePruner. The blue blocks should be provided for\nthe V ocabularyPruner and PipelinePruner.\nFigure 3: A typical TextPruner workﬂow for trans-\nformer pruning and vocabulary pruning.\n• If ffn_even_masking is false, the FFN\nhidden size of each layer is restricted to be\na multiple of multiple_of. It make the\nmodel structure friendly to the device that\nworks most efﬁciently when the matrix shapes\nare multiple of a speciﬁc size.\n• If use_logits is true, self-supervised prun-\ning is enabled.\nAll the conﬁgurations can be initialized manu-\nally in python scripts or from JSON ﬁles (for the\nCLI, the conﬁgurations can only be initialized from\nthe JSON ﬁles). An example of the conﬁguration\nin a Python script is shown in Figure 3.\n3.4 Other utilities\nTextPruner contains diagnostic tools such as sum-\nmary which inspects and counts the model pa-\nrameters, and inference_time which measures the\nmodel inference speed. Readers may refer to the\nexamples in the repository to see their usages.\n3.5 Usage and Workﬂow\nTextPruner provides both Python API and CLI. The\ntypical workﬂow is shown in Figure 2. Before call-\ning or Initializing TextPruner, users should prepare:\n1. A trained a model that needs to be pruned.\n2. For vocabulary pruning, a text ﬁle that deﬁnes\nthe new vocabulary.\n3. For transformer pruning, a python script ﬁle\nthat deﬁnes a dataloader and an adaptor.\n4. For pipeline pruning, both the text ﬁle and the\npython script ﬁle.\nAdaptor It is a user-deﬁned function that takes\nthe model outputs as the argument and returns the\nloss or logits. It is responsible for interpreting the\nmodel outputs for the pruner. If the adaptor is\nNone, the pruner will try to infer the loss from the\nmodel outputs.\nPruning with Python API First, initialize\nthe conﬁgurations and the pruner, then call\npruner.prune with the required arguments, as\nshown in Figure 2. Figure 3 shows an example.\nNote that we have not constructed the GeneralCon-\nﬁg and V ocabularyPruningConﬁg. The pruners will\nuse the default conﬁgurations if they are not speci-\nﬁed, which simpliﬁes the coding.\nPruning with CLI First create the conﬁgura-\ntion JSON ﬁles, then run the textpruner-cli.\nPipeline pruning example:\ntextpruner-cli \\\n--pruning_mode pipeline \\\n--configurations vocab.json trm.json \\\n--model_class BertForClassification \\\n--tokenizer_class BertTokenizer \\\n--model_path models/ \\\n--vocabulary texts.txt \\\n--dataloader_and_adaptor dataloader.py\n38\nModel V ocabulary size Model size Dev (en) Dev (zh) Test (en) Test (zh)\nXLM-R 250002 1060 MB ( 100%) 84.8 75.1 85.7 75.0\n+ V ocabulary Pruning on en 26653 406 MB ( 38.3%) 84.6 - 85.9 -\n+ V ocabulary Pruning on zh 23553 397 MB ( 37.5%) - 74.7 - 74.5\n+ V ocabulary Pruning on en and zh 37503 438 MB ( 41.3%) 84.8 74.3 85.8 74.5\nTable 1: The accuracy scores ( ×100%) of models with the pruned vocabulary on XNLI dev set and test set.\nStructure 12 10 8 6\n3072 100%\n(1.00x)\n89%\n(1.08x)\n78%\n(1.19x)\n67%\n(1.30x)\n2560 94%\n(1.08x)\n83%\n(1.18x)\n72%\n(1.29x)\n61%\n(1.44x)\n2048 89%\n(1.17x)\n78%\n(1.28x)\n67%\n(1.43x)\n56%\n(1.63x)\n1536 83%\n(1.29x)\n72%\n(1.42x)\n61%\n(1.63x)\n50%\n(1.90x)\nTable 2: Transformer sizes (listed as percentages) and\nspeedups (listed in the parentheses) of different struc-\ntures relative to the base model (12, 3072).\n3.6 Computational Cost\nVocabulary Pruning The main computational\ncost in vocabulary pruning is tokenization. This\nprocess will take from a few minutes to tens of\nminutes, depending on the corpus size. How-\never, the computational cost is negligible if the\npre-tokenized text is provided.\nTransformer Pruning The main computational\ncost in transformer pruning is the calculation of im-\nportance scores. It involves forward and backward\npropagation of the dataset. This cost is proportional\nto n_iters and dataset size. As will be shown\nin Section 4.2, in a typical classiﬁcation task, a\ndataset with a few thousand examples and setting\nn_iters around 10 can lead to a decent perfor-\nmance. This process usually takes several minutes\non a modern GPU (e.g., Nvidia V100).\n3.7 Extensibility\nTextPruner supports different pre-trained models\nand the tokenizers via the model structure deﬁ-\nnitions and the tokenizer helper functions regis-\ntered in the MODEL_MAP dictionary. Updating\nTextPruner for supporting more pre-trained models\nis easy. Users need to write a model structure def-\ninition and register it to the MODEL_MAP, so that\nthe pruners can recognize the new model.\n4 Experiments\nIn this section, we conduct several experiments to\nshow TextPruner’s ability to prune different pre-\ntrained models on different NLP tasks. We mainly\nfocus on the text classiﬁcation task. We list the re-\nsults on the MRC task and NER task with different\npre-trained models in the Appendix.\n4.1 Dataset and Model\nWe use the Cross-lingual Natural Language Infer-\nence (XNLI) corpus (Conneau et al., 2018) as the\ntext classiﬁcation dataset and build the classiﬁca-\ntion model based on XLM-RoBERTa (Conneau\net al., 2020). The model is base-sized with 12\ntransformer layers with FFN size 3072, hidden size\n768, and 12 attention heads per layer. Since XNLI\nis a multilingual dataset, we ﬁne-tune the XLM-R\nmodel on the English training set and test it on the\nEnglish and Chinese test sets to evaluate both the\nin-language and zero-shot performance.\n4.2 Results on Text Classiﬁcation\nEffects of Vocabulary Pruning As XLM-R is a\nmultilingual model, We conduct vocabulary prun-\ning on XLM-R with different languages, as shown\nin Table 1. We prune XLM-R on the training set\nof each language, i.e., we only keep the tokens that\nappear in the training set.\nWhen pruning on the English and Chinese train-\ning sets separately, the performance drops slightly .\nAfter pruning on both training sets, the model size\nstill can be greatly reduced by about 60% while\nkeeping a decent performance.\nV ocabulary pruning is an effective method for\nreducing multilingual pre-trained model size, and it\nis especially suitable for tailoring the multilingual\nmodel for speciﬁc languages.\nEffects of Transformer Pruning For simplicity,\nwe use the notation (H, F) to denote the model\nstructure, where H is the average number of atten-\ntion heads per layer, F is the average FFN hidden\nsize per layer. With this notation, the original (un-\npruned) model is (12, 3072). Before we show the\n39\nFigure 4: The Performance of the pruned models with\ndifferent structures on the test sets. The x-axis repre-\nsents different average numbers of attention heads; the\ny-axis represents different average FFN sizes. Left col-\numn: the accuracy scores on the English test set; Right\ncolumn: the accuracy scores on the Chinese test set.\nModels in the ﬁrst row have homogenous structures,\nwhile models in the second row do not. UHF stands for\nuneven heads and FFN neurons.\nresults on the speciﬁc task, we list the transformer\nsizes and their speedups of different target struc-\ntures relative to the unpruned model (12, 3072) in\nthe Table 2.\nWe compute the importance scores on the En-\nglish development set. The number of iterations\nniters is set to 16. We report the mean accuracy of\nﬁve runs. The performance on English and Chinese\ntest sets are shown in Figure 7. The top-left corner\nof each heatmap represents the performance of the\noriginal model. The bottom right corner represents\nthe model (6, 1536), which contains half attention\nheads and half FFN neurons.\nThe models in heatmaps from the ﬁrst row have\nhomogenous structures: each transformer in the\nmodel has the same number of attention heads and\nsame FFN size, while the models in the bottom\nheatmaps have uneven numbers of attention heads\nand FFN sizes in transformers. We use the abbre-\nviation UHF (Uneven Heads and FFN neurons) to\ndistinguish them from homogenous structures. We\nsee that by allowing each transformer to have dif-\nferent sizes, the pruner has more freedom to choose\nthe neurons to prune, thus the UHF models perform\nbetter than the homogenous ones.\nNote that the model is ﬁne-tuned on the English\ndataset. The performance on Chinese is zero-shot.\nAfter pruning on the English development set, the\nFigure 5: Model Performance on the English test set\nwith different number of iterations.\ndrops in the performance on Chinese are not larger\nthan the drops in the performance on English. It\nmeans the important neurons for the Chinese task\nremain in the pruned model. In the multilingual\nmodel, the neurons that deal with semantic under-\nstanding do not specialize in speciﬁc languages but\nprovide cross-lingual understanding abilities.\nFigure 5 shows how niters affects the perfor-\nmance. We inspect both the non-UHF model\n(6, 1536) and the UHF model (6, 1536)UHF. The\nsolid lines denote the average performance over\nthe ﬁve runs. The shadowed area denotes the stan-\ndard deviation. In all cases, the performance grows\nwith the niters. Pruning with only one iteration\nis a bad choice and leads to very low scores. We\nsuggest setting niters to at least 8 for good enough\nperformance.\nIn Figure 5 we also compare the supervised\npruning (with L being the cross-entropy loss\nwith the ground-truth labels) and the proposed\nself-supervised pruning (with Lbeing the KL-\ndivergence Eq (5)) . Although no label information\nis available, the self-supervised method achieves\ncomparable and sometimes even higher results.\nHow much data are needed for model pruning?\nTo answer this question, we randomly sample10%,\n20%, . . ., 90%, 100% examples from the English\ndevelopment set for computing importance scores.\nWe inspect the (6, 1536)UHF model. Each experi-\nment has been run ﬁve times. The results are shown\n40\nFigure 6: Model Performance on the test set with dif-\nferent number of examples for computing importance\nscores. Left y-axis: accuracy on Enligh. Right y-axis:\naccuracy on Chinese.\nin Figure 6. With about 70% examples (about 1.7K\nexamples) from the development set, the pruned\nmodel achieves a performance that is nearly com-\nparable with the model pruned with the full devel-\nopment set (2490 examples).\n5 Conclusion and Future Work\nThis paper presents TextPruner, a model prun-\ning toolkit for pre-trained models. It leverages\noptimization-free pruning methods, including vo-\ncabulary pruning and transformer pruning to re-\nduce the model size. It provides rich conﬁguration\noptions for users to explore and experiment with.\nTextPruner is suitable for users who want to prune\ntheir model quickly and easily, and it can also be\nused for analyzing pre-trained models by pruning,\nas we did in the experiments.\nFor future work, we will update TextPruner to\nsupport more pre-trained models, such as the gen-\neration model T5 (Raffel et al., 2020). We also\nplan to combine TextPruner with our previously\nreleased knowledge distillation toolkit TextBrewer\n(Yang et al., 2020) into a single framework to pro-\nvide more effective model compression methods\nand a uniform interface for knowledge distillation\nand model pruning.\nAcknowledgements\nThis work is supported by the National Key Re-\nsearch and Development Program of China via\ngrant No. 2018YFB1005100.\nReferences\nAmine Abdaoui, Camille Pradel, and Grégoire Sigel.\n2020. Load what you need: Smaller versions of\nmutililingual BERT. In Proceedings of SustaiNLP:\nWorkshop on Simple and Efﬁcient Natural Language\nProcessing, pages 119–123, Online. Association for\nComputational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nYiming Cui, Wei-Nan Zhang, Wanxiang Che, Ting Liu,\nZhigang Chen, and Shijin Wang. 2022. Multilin-\ngual multi-aspect explainability analyses on machine\nreading comprehension models. iScience, 25(4).\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJonathan Frankle and Michael Carbin. 2019. The lottery\nticket hypothesis: Finding sparse, trainable neural\nnetworks. In International Conference on Learning\nRepresentations.\nSong Han, Jeff Pool, John Tran, and William J. Dally.\n2015. Learning both weights and connections for\nefﬁcient neural networks. CoRR, abs/1506.02626.\nLu Hou, Zhiqi Huang, Lifeng Shang, Xin Jiang, Xiao\nChen, and Qun Liu. 2020. Dynabert: Dynamic BERT\nwith adaptive width and depth. In Advances in Neu-\nral Information Processing Systems 33: Annual Con-\nference on Neural Information Processing Systems\n2020, NeurIPS 2020, December 6-12, 2020, virtual.\nFrançois Lagunas, Ella Charlaix, Victor Sanh, and\nAlexander Rush. 2021. Block pruning for faster trans-\nformers. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Process-\ning, pages 10619–10629, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\n41\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nJ. S. McCarley. 2019. Pruning a bert-based question\nanswering model. CoRR, abs/1910.06360.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In Ad-\nvances in Neural Information Processing Systems 32:\nAnnual Conference on Neural Information Process-\ning Systems 2019, NeurIPS 2019, December 8-14,\n2019, Vancouver, BC, Canada, pages 14014–14024.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nVictor Sanh, Thomas Wolf, and Alexander M. Rush.\n2020. Movement pruning: Adaptive sparsity by ﬁne-\ntuning. In Advances in Neural Information Process-\ning Systems 33: Annual Conference on Neural In-\nformation Processing Systems 2020, NeurIPS 2020,\nDecember 6-12, 2020, virtual.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142–\n147.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 5797–5808, Florence, Italy.\nAssociation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nZiqing Yang, Yiming Cui, Zhipeng Chen, Wanxiang\nChe, Ting Liu, Shijin Wang, and Guoping Hu. 2020.\nTextBrewer: An Open-Source Knowledge Distilla-\ntion Toolkit for Natural Language Processing. In\nProceedings of the 58th Annual Meeting of the Associ-\nation for Computational Linguistics: System Demon-\nstrations, pages 9–16. Association for Computational\nLinguistics.\nMichael Zhu and Suyog Gupta. 2018. To prune, or\nnot to prune: Exploring the efﬁcacy of pruning for\nmodel compression. In 6th International Conference\non Learning Representations, ICLR 2018, Vancouver,\nBC, Canada, April 30 - May 3, 2018, Workshop Track\nProceedings. OpenReview.net.\nA Datasets and Models\nWe experiment with different pre-trained models\nto test TextPruner’s ability to prune different mod-\nels. For the MRC task, we use SQuAD (Rajpurkar\net al., 2016) dataset and RoBERTa (Liu et al.,\n2019) model; For the NER task, we use CoNLL\n2003 (Tjong Kim Sang and De Meulder, 2003) and\nBERT (Devlin et al., 2019) model. All the models\nare base-sized, i.e., 12 transformer layers with a\nhidden size of 768, an FFN size of 3072, and 12\nattention heads per layer.\nB Transformer Pruning on MRC\nWe compute the importance scores on a subset of\nthe training set (5120 examples). The F1 score\non the SQuAD development set is listed in Table\n3. (12, 3072) is the unpruned model. The per-\nformance grows with the niters. The number of\niterations also plays an important role on model\nperformance in the SQuAD task. We also see that\npruning with only one iteration is a bad choice and\nleads to low scores. Setting niters to at least 8\nachieves good enough performance.\nC Transformer Pruning on NER\nWe compute the importance scores on the CoNLL\n2003 development set. The F1 score on the test\nis listed in Table 4. We also see large gaps in\nperformance between niters = 4 and niters = 8.\nThe performance of the pruned models with dif-\nferent structures is shown in Figure 7. We only con-\nsider the UHF case for it can achieve the best over-\nall performance. The number of iterations niters is\nset to 16.\n42\nModel 1 2 4 8 16\n(12, 3072) 91.4\n(8, 2048) 76.4 80.3 81.9 82.9 82.5\n(8, 2048)UHF 87.5 86.4 87.6 88.3 88.4\n(6, 1536) 12.8 42.6 49.5 51.5 56.5\n(6, 1536)UHF 47.2 55.6 66.1 74.1 75.2\nTable 3: The F1 score on SQuAD. Each score is aver-\naged over ﬁve runs. Different columns represent results\nunder different number of iterations. We bold the best\nF1 in each row.\nModel 1 2 4 8 16 32\n(12, 3072) 91.3\n(8, 2048) 88.5 88.4 88.7 89.2 89.2 89.4\n(8, 2048)UHF 81.8 90.0 90.6 90.7 90.8 90.8\n(6, 1536) 33.6 56.2 62.4 80.5 83.4 84.1\n(6, 1536)UHF 9.8 67.6 80.2 86.2 87.0 87.3\nTable 4: The F1 score on CoNLL 2003. Each score is\naveraged over ﬁve runs.\nFigure 7: The Performance of the pruned models with\ndifferent structures on the CoNLL 2003 test set. Each\nscore is averaged over ﬁve runs.\n43",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8652710318565369
    },
    {
      "name": "Language model",
      "score": 0.8275254964828491
    },
    {
      "name": "Pruning",
      "score": 0.7647866010665894
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6758243441581726
    },
    {
      "name": "Vocabulary",
      "score": 0.5905707478523254
    },
    {
      "name": "Natural language processing",
      "score": 0.5344938635826111
    },
    {
      "name": "Machine learning",
      "score": 0.528337836265564
    },
    {
      "name": "Transformer",
      "score": 0.49951982498168945
    },
    {
      "name": "Training set",
      "score": 0.42626258730888367
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I204983213",
      "name": "Harbin Institute of Technology",
      "country": "CN"
    }
  ]
}