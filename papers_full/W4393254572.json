{
  "title": "Formative Feedback on Student-Authored Summaries in Intelligent Textbooks Using Large Language Models",
  "url": "https://openalex.org/W4393254572",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2107583730",
      "name": "Wesley Morris",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A4214372288",
      "name": "Scott Crossley",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A4296492532",
      "name": "Langdon Holmes",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2096320473",
      "name": "Chaohua Ou",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2167677971",
      "name": "Mihai Dascalu",
      "affiliations": [
        "University of Bucharest",
        "Universitatea Națională de Știință și Tehnologie Politehnica București"
      ]
    },
    {
      "id": "https://openalex.org/A4208909440",
      "name": "Danielle McNamara",
      "affiliations": [
        "Arizona State University"
      ]
    },
    {
      "id": "https://openalex.org/A2107583730",
      "name": "Wesley Morris",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4214372288",
      "name": "Scott Crossley",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4296492532",
      "name": "Langdon Holmes",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096320473",
      "name": "Chaohua Ou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2167677971",
      "name": "Mihai Dascalu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4208909440",
      "name": "Danielle McNamara",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4327500636",
    "https://openalex.org/W3133796423",
    "https://openalex.org/W2061030947",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4288059323",
    "https://openalex.org/W2132069633",
    "https://openalex.org/W4293464023",
    "https://openalex.org/W162507755",
    "https://openalex.org/W2955975917",
    "https://openalex.org/W6748002556",
    "https://openalex.org/W2083212065",
    "https://openalex.org/W4229625716",
    "https://openalex.org/W2953001123",
    "https://openalex.org/W2898193199",
    "https://openalex.org/W3159958158",
    "https://openalex.org/W2793429163",
    "https://openalex.org/W2280974225",
    "https://openalex.org/W3011616684",
    "https://openalex.org/W2301632029",
    "https://openalex.org/W2018951238",
    "https://openalex.org/W2945886944",
    "https://openalex.org/W3096729368",
    "https://openalex.org/W2251056936",
    "https://openalex.org/W2744222256",
    "https://openalex.org/W3216581565",
    "https://openalex.org/W2577350134",
    "https://openalex.org/W2766498180",
    "https://openalex.org/W2150824314",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3211339323",
    "https://openalex.org/W2913673245",
    "https://openalex.org/W4382567099",
    "https://openalex.org/W4321442102",
    "https://openalex.org/W4207045419",
    "https://openalex.org/W1826672328",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2883454655",
    "https://openalex.org/W2117823027",
    "https://openalex.org/W4288115735",
    "https://openalex.org/W2917049430",
    "https://openalex.org/W3176923149",
    "https://openalex.org/W2947964661",
    "https://openalex.org/W4382567022",
    "https://openalex.org/W3107653269",
    "https://openalex.org/W7030677884",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3184084672",
    "https://openalex.org/W1790027761",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W6798795620"
  ],
  "abstract": "Abstract As intelligent textbooks become more ubiquitous in classrooms and educational settings, the need to make them more interactive arises. An alternative is to ask students to generate knowledge in response to textbook content and provide feedback about the produced knowledge. This study develops Natural Language Processing models to automatically provide feedback to students about the quality of summaries written at the end of intelligent textbook sections. The study builds on the work of Botarleanu et al. (2022), who used a Longformer Large Language Model (LLM) to develop a summary grading model. Their model explained around 55% of holistic summary score variance as assigned by human raters. This study uses a principal component analysis to distill summary scores from an analytic rubric into two principal components – content and wording. This study uses two encoder-only classification large language models finetuned from Longformer on the summaries and the source texts using these principal components explained 82% and 70% of the score variance for content and wording, respectively. On a dataset of summaries collected on the crowd-sourcing site Prolific, the content model was shown to be robust although the accuracy of the wording model was reduced compared to the training set. The developed models are freely available on HuggingFace and will allow formative feedback to users of intelligent textbooks to assess reading comprehension through summarization in real time. The models can also be used for other summarization applications in learning systems.",
  "full_text": "ARTICLE\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nhttps://doi.org/10.1007/s40593-024-00395-0\nAbstract\nAs intelligent textbooks become more ubiquitous in classrooms and educational \nsettings, the need to make them more interactive arises. An alternative is to ask stu -\ndents to generate knowledge in response to textbook content and provide feedback \nabout the produced knowledge. This study develops Natural Language Processing \nmodels to automatically provide feedback to students about the quality of sum -\nmaries written at the end of intelligent textbook sections. The study builds on the \nwork of Botarleanu et al. ( 2022), who used a Longformer Large Language Model \n(LLM) to develop a summary grading model. Their model explained around 55% \nof holistic summary score variance as assigned by human raters. This study uses a \nprincipal component analysis to distill summary scores from an analytic rubric into \ntwo principal components – content and wording. This study uses two encoder-only \nclassification large language models finetuned from Longformer on the summaries \nand the source texts using these principal components explained 82% and 70% of \nthe score variance for content and wording, respectively. On a dataset of summaries \ncollected on the crowd-sourcing site Prolific, the content model was shown to be \nrobust although the accuracy of the wording model was reduced compared to the \ntraining set. The developed models are freely available on HuggingFace and will \nallow formative feedback to users of intelligent textbooks to assess reading com -\nprehension through summarization in real time. The models can also be used for \nother summarization applications in learning systems.\nKeywords Intelligent textbooks · Large language models · Automated summary \nscoring · Transformers\nAccepted: 5 February 2024 / Published online: 28 March 2024\n© The Author(s) 2024\nFormative Feedback on Student-Authored Summaries in \nIntelligent Textbooks Using Large Language Models\nWesley Morris1  · Scott Crossley1 · Langdon Holmes1 · Chaohua Ou2 · \nMihai Dascalu3 · Danielle McNamara4\nExtended author information available on the last page of the article\n1 3\n\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nIntroduction\nIntelligent textbooks have become increasingly popular in recent years as the \nCOVID-19 pandemic pushed many learners into online classes (Seaman & Sea -\nman, 2020) combined with advances in Natural Language Processing (NLP) that \nhave made human-machine interaction more accessible (Brusilovsky et al., 2022; \nWang et al., 2021). Intelligent textbooks have many advantages over print textbooks, \nincluding the integration of multimedia elements such as video, audio, and hyper -\nlinks. While some studies have demonstrated no significant difference in learning \nbetween digital and print textbooks (Rockinson-Szapkiw et al., 2013), a more recent \nand comprehensive meta-analysis of 26 studies reported that interactive features such \nas those in intelligent textbooks improve reading performance with a moderate effect \nsize across multiple knowledge domains (Clinton-Lisell et al., 2021). Additionally, \ncollege students prefer the lower cost and ease of use of intelligent textbooks (Ji et \nal., 2014; Chulkov & VanAlstine, 2013).\nA textbook should be more than a static web-based version of a traditional paper \ntextbook to be considered intelligent. Instead, an intelligent textbook should be inter-\nactive and adapt to the individual user’s needs. Various forms of artificial intelligence \ntechniques can be employed to accomplish the goal of interactivity, including the use \nof Transformer-based Large Language Models (LLMs). LLMs have been used for a \nvariety of purposes, including summary generation (Khandelwal et al., 2019), ques-\ntion answering (Shao et al., 2019), text classification (Wolf et al., 2020), validation \nof peer-assigned scores in massive open online courses (Morris et al., 2023b), and \nquestion generation tasks (Lopez et al., 2021).\nPrevious research indicates that writing about textbook content in tasks such as \nsummarization can increase learning outcomes in various content domains (Graham \net al., 2020; Silva & Limongi, 2019). However, scoring summaries is time-intensive \nfor instructors, paving the way for automatic approaches to summary scoring (Laga-\nkis & Demetriadis, 2021). This study is part of a larger project called Intelligent \nTextbook for Enhanced Language Learning (iTELL) to develop a computational \nframework that converts static, web-based textbooks into interactive, intelligent text-\nbooks. iTELL converts any type of machine-readable text into an interactive web-\napp, and students can write summaries directly in the application. These summaries \ncan be scored automatically by LLMs specifically trained to generate scores which \ninform qualitative feedback to students related to content and wording. Students can \nuse the feedback from these models in different ways, including to reflect on and \nguide their learning, identify and correct misconceptions, review missed topics, and \nprepare for upcoming materials. As such, while iTELL is being developed in the con-\ntext of intelligent textbooks, it also has multiple applications outside of that context.\nThe goal of this current study is to report on the automated summary evaluation \nmodels integrated into iTELL and discuss how feedback is automatically provided \nto the users of intelligent textbooks. Based on pretrained encoder-only LLMs, our \nmodels can help students develop their knowledge while providing important infor -\nmation about reading comprehension to teachers and material developers (Phillips \nGalloway, & Uccelli, 2019). Specifically, we provide an overview of LLMs that pro-\nvide a formative assessment of summaries. These include models based on RoBERTa \n1 3\n1023\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\n(Liu et al., 2019) that predict scores based only on the summary due to constraints \non the maximum sequence length, and models using Longformer (Beltagy et al., \n2020), which are capable of an increased max sequence length, allowing them to \npredict summary scores while including considerably more text from the textbook as \ncontext. Because iTELL is designed to be domain agnostic, the models must provide \naccurate feedback to users regardless of the textbook topic. To that end, the models \nwere trained on a dataset comprising source texts on a wide range of informative top-\nics. The research questions that guide this study are the following:\n1. To what extent does the inclusion of source text from the textbook improve the \naccuracy of the LLMs in automatically scoring summaries?\n2. Does unsupervised pretraining on a large dataset of texts in the target language \ndomain improve performance on the LLMs?\n3. How well do automated summary evaluation LLMs perform when they are used \noutside of the context of the dataset and labels considered during their training?\nRelated Work\nIntelligent Textbooks\nThe earliest intelligent textbooks were designed in the 1990s using the principles \nof knowledge engineering, in which the textbook would be designed and produced \nby domain experts (Brusilovsky et al., 2022). Early work in designing intelligent \ntextbooks included the development of hypertext (Bareiss & Osgood, 1993), which \nallowed students to navigate the book efficiently (Brusilovsky & Pesin, 1998). One \nof the first web-based interactive textbooks included ELM-ART, an intelligent, inter-\nactive textbook to teach programming introduced in 1996 (Weber & Brusilovsky, \n2016).\nThe development of intelligent textbooks has increased in the past decade as com-\nputational tools become more sophisticated and accessible (Sosnovsky et al., 2023). \nMore recent research has included mining student behaviors in intelligent textbooks \nand using those data to provide an individualized learning experience. For instance, \nLan and Baraniuk (2016) developed a muti-armed bandit algorithm that uses results \nfrom previous assessments to identify and recommend pedagogical activities opti -\nmally individualized for each student. Learner behavior such as failure to correctly \nanswer comprehension questions can also be used to adaptively modify the content of \ntextbooks, thus recommending materials to remediate comprehension gaps (Thaker \net al., 2020). Other research has shown that student behaviors in intelligent text -\nbooks, such as annotation and highlighting, can predict student success in the course \n(Winchell et al., 2018) and that concept or keyphrase extraction using annotation by \ntrained experts can be used as training data for machine learning algorithms (Wang \net al., 2021).\nResearchers have also used NLP techniques to construct semantic maps of text -\nbooks which can be used to integrate the textbook with resources available on the \n1 3\n1024\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nweb (Alpizar-Chacon & Sosnovsky, 2021; Labutov et al., 2017). In addition, part-of-\nspeech taggers have been employed to develop question generation tools that auto -\nmatically generate and embed comprehension questions into intelligent textbooks \n(Kumar et al., 2015). Generative Transformer neural networks such as GPT-2 have \nalso been used to develop language generation tools to provide information to stu -\ndents within intelligent textbooks (Yarbro & Olney, 2021).\nSummarization and Reading Comprehension\nText summarization is a valuable tool to build and assess student knowledge (Head \net al., 1989) that has become more common in educational applications (Graham & \nHarris, 2015), especially in readability assessments like those found in intelligent \ntextbooks (Phillips Galloway & Uccelli, 2019). Writing tasks like summarizations \nalso help students build and consolidate their knowledge about reading materials in \naddition to their effectiveness in reading comprehension assessment. A meta-analysis \nof 56 experiments on the effect of writing on learning by Graham et al. (2020) found \nan average weighted effect size of Hedges’s g = 0.3 (p <.005) between pre and post-\ntests for students who used writing to learn from texts. This effect size held, regard -\nless of whether the knowledge domain was science, social studies, or mathematics. \nThese results may be due to the increased cognitive demands of writing, a process \nin which the learner must actively reconstruct knowledge from the text (Nelson & \nKing, 2022). For instance, Galbraith and Baaijen ( 2018) contend that writing con -\nsists of two separate domains, one in which knowledge from the text is retrieved and \nmanipulated and one in which the author actively uses their understanding of the \nworld to construct text. Concurrent research by Silva and Limongi ( 2019) indicates \nthat the practice of summary writing may help to consolidate the knowledge gained \nfrom reading into long-term memory. Despite the effectiveness of summarization in \neducation and assessment, providing feedback to learners about the quality of sum -\nmaries is time-consuming for educators (Gamage et al., 2021), making summariza-\ntion challenging to scale.\nAutomated Summary Evaluation\nAn essential component of intelligent textbooks is the capacity to provide formative \nfeedback to students about their comprehension, and real-time feedback provided \nby AI is effective at improving reading comprehension (Chen et al., 2021; Kim et \nal., 2020). Before the development of deep learning approaches to NLP, automated \nsummary evaluation (ASE) was primarily performed by comparing the summary \nbeing tested with a professionally produced reference summary. Algorithms such as \nROUGE (Lin & Hovy, 2003), closely related to BLEU (Papineni et al., 2001), were \nused to provide summary scores based on word and phrase co-occurrence between \nthe test summary and the reference summary. While ROUGE is correlated with \nhuman judgments of summary quality and is actively used in the training of summa-\nrization algorithms (Ganesan, 2018; Scialom et al., 2019), it is biased toward surface-\nlevel lexical features, a limitation which can be addressed using more advanced NLP \nfeatures including word embedding approaches (Ng & Abrecht, 2015). More impor-\n1 3\n1025\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\ntantly, ROUGE and BLEU approaches require the use of reference summaries cre -\nated by a human expert, which are resource intensive and impractical in the context \nof intelligent textbooks like those generated with the iTELL platform.\nRecent developments in NLP allow more sophisticated feedback approaches for \nopen-ended reading assessments like text summarization. For instance, Crossley et \nal. (2019) developed a summarization model to predict ratings of main idea integra -\ntion in student summaries using lexical diversity features, a word frequency metric, \nand Word2vec semantic similarity scores between summaries and the correspond -\ning source material. The model explained 53% of the variance in ratings. Martínez-\nHuertas et al. (2019) used latent semantic analysis to embed summaries into semantic \nvector spaces where the rubric scores could be extracted. Their method achieved a \nPearson’s correlation with scores from expert raters between 0.78 and 0.81. With the \nrise of LLMs, new methods of automated summary evaluation have been evaluated. \nFor instance, Botarleanu et al. (2022) used LLMs to predict overall student summa -\nrization scores derived from an analytic rubric, explaining ∼ 55% of score variance.\nCurrent Study\nThe NLP models discussed above show the potential for open-ended assessments of \ntext comprehension through summarization in intelligent textbooks. To fulfill their \npurpose in the context of the iTELL framework, the models should accurately score \nsummaries of source texts on any topic. The current study expands on the work of \nCrossley et al. ( 2019); Botarleanu et al. ( 2022); Morris et al. ( 2023a) which used a \nsimilar dataset of summaries on sources covering a variety of topics. First, instead \nof using the raw scores from an analytic rubric, we consolidated the scores into two \nprincipal components and used those as labeled data in model training. Second, we \nassessed the extent to which domain adaptation of the models improves scoring \naccuracy.\nMethods\nFour different datasets were used in this study, listed for reference in Table 1. Dur-\ning training, we used a training dataset used for finetuning the models and a dataset \nfrom Commonlit used for domain adaptation. We also used two datasets for post-hoc \ntests of validity and generalizability - a dataset of professional summaries found in a \ntextbook available on OpenStax, and a dataset of summaries written by participants \nrecruited through the Prolific crowdsourcing platform. Each of these datasets will be \ndiscussed in more detail in the subsequent sections.\nName Use Sources N Summaries N\nTraining Finetuning 101 690\nCommonlit Domain adaptation 6 93,484\nTextbook Post-hoc testing 94 94\nProlific Post-hoc testing 4 113\nTable 1 List of datasets \n1 3\n1026\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nData\nOur summary training corpus comprises 4,690 summaries written by high school, \nuniversity, and adult writers collected by three higher education institutions between \n2018 and 2021, corresponding to 101 source texts. Crossley et al. ( 2019) and Botar-\nleanu et al. ( 2022) used a subset of this data. The training corpus consolidates sev -\neral different data sources, including summaries written by workers on Amazon’s \nMechanical Turk service (Li et al., 2018), summaries written by undergraduate col -\nlege students, and summaries written by high school students. Source texts consider \na variety of topics, such as the effect of UV radiation, diabetes, computer viruses, red \nblood cells, and the dangers of smoking. The sources had a mean word count of 308.5 \n(SD = 130.49) and thus may be shorter than sections in an intelligent textbook. How-\never, the topics were academic and therefore, similar to topics from intelligent text -\nbooks. Each source had an average of 46.44 summaries written for it (SD = 62.12), \nwith a maximum of 258 summaries and a minimum of 10. The summaries had an \naverage of 75.18 words (SD = 50.51).\nThe formatting of the summary dataset was not uniform. For example, in some \ncases, source documents included several articles, and only one was summarized by \nany individual writer. We checked each source to ensure each summary was paired \nwith only one properly formatted source. This process of cleaning and normalization \nis the first and often most labor-intensive step toward training a machine learning tool \n(Shorten et al., 2021). However, cleaned datasets often significantly impact the final \naccuracy of the model (Chollet, 2018).\nSummary Scoring\nRegardless of source, all summaries in the training dataset were scored according \nto the same procedure. Two expert raters scored each summary using a 0–4 scaled \nanalytic rubric to score 7 criteria important in understanding the quality of summari-\nzations. The criteria included main point/gist (whether the summary captured the gist \nof the source), details (to what extent the summary included all relevant information \nfrom the source), language beyond the source (grammar and syntax), paraphrasing/\nwording (avoiding plagiarism and direct copying from the source), objective lan -\nguage (accurately reflecting the view of the source), and cohesion (to what extend the \nsummary was clearly and rationally organized), and text length. Appendix A contains \nthe rubric used by raters in their assessment of the summaries.\nRaters were initially normed on a small set of summaries not included in the final \nset of summaries. As such, raters talked through ∼ 20 summaries until they were com-\nfortable with the rubric. They then scored ∼ 40 summaries until reaching an accept -\nable level of inter-rater reliability ( r >.699). Afterwards, raters scored summaries \nindependently by source. Within the prompt, the scored summaries were randomized \nto reduce the potential for ordering effects. Also, raters could adjudicate any score \ndifferences greater than 1. Raters did this by talking through the summary with each \nother and then deciding whether they wanted to modify their ratings based on the \ndiscussion. If an agreement was not reached, the individual scores were not altered, \nbut their average was used in subsequent predictions. Final inter-rater reliability was \n1 3\n1027\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nacceptable (r >.800 and κ > 0.700). Average scores between the raters were calculated \nfor each essay and used for the data analysis.\nDimensionality Reduction\nWe were faced with several choices because the scoring rubric consisted of seven \ncriteria,. These included training a single multitask model to predict all seven scores \nat once at the cost of accuracy and training seven different models at the expense \nof increased compute requirements. A third choice, and the one we selected, was to \nconduct a principal component analysis (PCA) to assess the potential to reduce the \ndimensionality of the seven analytic scores in the rubric into a smaller number of \nconstructs.\nBefore conducting the PCA, the human scores were standardized using z-score \nnormalization. An initial PCA with all possible factors ( n = 7) indicated 2 compo -\nnents that reported eigenvalues over 0.70 (Jolliffe’s criteria). A Kaiser-Meyer-Olkin \n(KMO) measure of sampling adequacy indicated that no variables need to be removed \n(i.e., all KMO values were above 0.5), and the overall KMO score = 0.87 showed a \n“meritorious” sample (Kaiser, 1974). The PCA reported a Bartlett’s test of spheric -\nity, χ2 (4690) = 11,513.99, p <.001, indicating that correlations between the analytic \nscores were sufficiently large for the PCA. Within the components, there was a break \nin the cumulative variance explained between the second and the third component. \nConsidering this break, we decided on a 2-component solution when developing the \nPCA. These 2 components explained approximately 73% of the shared variance in \nthe data from the initial PCA. Figure 1 displays a scree plot showing the eigenvalue, \nFig. 1 Scree plot of eigenvalues against number of principal components\n \n1 3\n1028\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nor absolute value of the explained variance, plotted against the number of principal \ncomponents.\nThe first component was related to Content (i.e., Component 1), and the analytic \nscales of details, main point, objective language use, and cohesion were combined \ninto a weighted score. The analytic scales for paraphrasing and language beyond the \nsource were combined into a weighted score designated as Wording (i.e., Compo -\nnent 2). Text length did not load into the first components and was removed. Table 2 \ndisplays descriptive statistics for all scores, including the principal components. The \ncomponent scores were z-score normalized and rescaled such that zero represents \nthe mean for each principal component, and one unit represents one standard devia -\ntion. These transformed scores were used as outcome variables in our large language \nmodels.\nSummary Scoring Model\nWe used two pretrained LLMs to develop summary scoring models. The Transformer \n(Vaswani et al. 2017) is a neural network architecture that relies on the self-attention \nmechanism and can be trained in parallel in contrast to previous recurrent neural \narchitectures. LLMs are Transformer-based models pretrained on a large corpus of \ntext that can be further finetuned for downstream tasks. The LLMs used in this study \nwere trained using masked language modeling, in which the text is tokenized, but \nsome tokens are masked. The task of masked language modeling is to predict the \nmasked tokens based on all the tokens that come before and after the masked tokens. \nBecause of the cost and time associated with developing an LLM, only a few models \nare produced and are freely available.\nPretrained LLM models can be refined in two ways. The primary method of model \nrefinement is through finetuning. The model is trained on the target task using the \ntraining data with corresponding labels. Unlike generative models such as ChatGPT \n(Abdullah et al., 2022) or LLaMa (Touvron et al., 2023), which are trained to predict \nthe next token in a series given the preceding tokens, this study uses embedding or \nencoder-only models which are commonly used in regression or classification tasks. \nEncoder-only models include a special classification token at the beginning of the \nsequence. As the model processes the language data, the embedding of the classifi -\ncation token comes to represent semantic information about the text as a whole. A \nclassification head, which could be a linear layer or a traditional machine learning \nLanguage domain N Mean SD Min Max\nMain Point 4,690 3.05 0.80 0.5 4\nDetails 4,690 2.79 0.84 0 4\nCohesion 4,690 2.97 0.79 0 4\nObjective Language 4,690 2.79 0.73 0 4\nParaphrasing 4,690 2.23 0.91 0 4\nLanguage Beyond the \nSource\n4,690 2.26 0.70 0 4\nContent PCA 4,690 8.00 2.04 0.76 10.96\nWording PCA 4,690 3.51 1.25 0 6.28\nTable 2 Descriptive statistics \nfor summary scores in the train-\ning dataset\n \n1 3\n1029\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nalgorithm, uses the classification token as input to make predictions about the class of \nthe text. During finetuning, the parameters of the model, as well as the classification \nhead, are adjusted.\nA secondary method is domain adaptation through unsupervised pretraining (Tun-\nstall, von Werra & Wolf, 2022). Unsupervised pretraining may be used when there \nis a large amount of unlabeled data but a relatively small amount of labeled data. In \nthis case, the model is trained using masked language modeling on language data \nfrom the target language domain to allow the model greater familiarity with the target \ndomain. For example, Beltagy, Lo, and Cohen (2019) used unsupervised pretraining \non a scientific corpus to improve the accuracy of the BERT model in that domain. \nAfter domain adaptation, the resultant model is finetuned on the labeled data for clas-\nsification or regression specific tasks.\nIn this study, we considered two LLMs: the RoBERTa base model (Liu et al., \n2019) and the Longformer base LLM (Beltagy et al., 2020). RoBERTa is an encoder-\nonly Transformer model pretrained on the English Wikipedia corpus and Bookcor -\npus. The Transformer neural architecture relies on attention mechanisms in which, \nat every layer, each token embedding is modified by each other token embedding. \nAs a result, the computational requirements grow quadratically as a function of the \ninput sequence length. In RoBERTa, the length of the input sequence is limited to 512 \ntokens to ensure computational efficiency. While this length is sufficient for many \nsummaries, it is not long enough to include text from the textbook in the model input.\nThe Longformer LLM (Beltagy et al., 2020) can handle longer input sequences \nby utilizing sparse attention, in which not all tokens are compared with every other \ntoken. Instead, Longformer uses a sliding attention window so that each token only \nattends to the tokens a certain number of positions to its left and right. Sparse atten -\ntion mitigates the problem of limited sequence length by reducing the computational \ncomplexity of the attention mechanism. In addition to the sliding attention window, \nLongformer also utilizes global attention in which specific tokens attend to every \nother token. Since attention is bidirectional, all tokens will attend to global tokens \nas well. Combining these two types of attention enables Longformer to increase the \nmax sequence length from 512 tokens to 4,096 tokens while remaining efficient. The \nLongformer max sequence length allowed us to include both the summary and source \ntexts from the textbook in the input sequence. By default, Longformer places global \nattention only on the classification token at the beginning of the sequence and uses \na 512-token sliding attention window which moves across the rest of the sequence. \nBecause the summary is more salient to the score in the task of summary evaluation, \nhowever, we hypothesized that it would be beneficial to use global attention for the \nentire summary. This allows tokens in the summary to attend to every token in the \nsource text and vice-versa. We chose to include the entire summary in global atten -\ntion and shorten the sliding window to 256 tokens to conserve compute. To the best \nof our knowledge, this approach has not been used in automatic summary evaluation \nwith Longformers.\nWe divided the scored summary corpus into training, validation, and test sets. We \nselected 15 out of the 101 sources text to comprise the test set only to ensure gen -\neralizability across source texts and prompts (i.e., these source texts were not used \nin training or validation). After splitting the data, the training, validation, and test \n1 3\n1030\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nsets comprised 3,285, 703, and 702 summaries, respectively. Each summary from \nthe training set was tokenized and fed to the RoBERTa model during finetuning. For \nLongformer, the summary and the source text for the summary were concatenated \nusing a specific separator token (specifically, “< \\s>”) and then tokenized together to \ngenerate the input sequences. These token sequences were used as input data for their \nrespective models, and the final classification token was used to train a linear regres-\nsion head. We trained each model for six epochs with a batch size of 8 and a learning \nrate of 3e-05, retaining the best model. We used mean squared error as the evaluation \nmetric during the training process. After training, we tested each model’s perfor -\nmance by predicting the summary Content and Wording scores. We evaluated model \nperformance in terms of correlation with the human rater judgments and explained \nvariance (R2).\nIn addition to the finetuning procedures described above, we also domain-adapted \nthe Longformer and RoBERTa pretrained models on a different dataset of 93,484 \nsummaries written by middle and high-school students. The summaries were col -\nlected from six sources available online through the Commonlit platform. This is the \nlargest, unlabeled dataset in the target language domain to the best of our knowledge, \nand we considered it a reasonable candidate for domain adaptation, although the very \nsmall number of source texts ( N = 6) meant that, although the models were training \non a large set of student summaries, they were training on only a very small set of \nsource texts. We used a masked language modeling task to domain adapt the models \nfor eight epochs with a learning rate of 2e-5. After constructing the domain-adapted \nmodels, we finetuned them using the same methods described above and evaluated \ntheir performance by calculating the correlation between predicted scores and human \nrater judgments.\nPost-Hoc Analysis\nIn addition to training and testing the LLMs on the training dataset, we also tested \nthe LLMs on a dataset of summaries written by experts and a dataset of summaries \nwritten by participants recruited from the Prolific crowdsourcing website on con -\ntent within an intelligent textbook. We chose the 2nd edition textbook for Macro -\neconomics, freely available on the Openstax website ( https://openstax.org/details/\nbooks/principles-macroeconomics-2e). This text consists of 94 sections divided into \n21 chapters. Each section includes a professional summary ( N = 94). In addition to \nthe material in the sections, the textbook includes pages for key terms, concepts, and \nreview questions for each section.\nWe used the best-performing LLM to predict scores for the section summaries pro-\nvided in the textbook. We generated two sets of section text and section summaries, \none in which the summaries are matched to the appropriate section and one in which \nthe summaries are randomized so that they are not matched with the section they \nsummarize. We predicted the scores for content and wording for the summaries of \neach of the 94 sections in the textbook in both the matched and unmatched datasets. \nIf the model is accurate, the summaries in the matched group would score higher than \nthose in the unmatched group.\n1 3\n1031\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nIn addition to testing the models on professional summaries in the textbook, we \nalso used the crowdsourcing site Prolific to recruit 60 participants to write 113 short \nsummaries of sections from the Macroeconomics Openstax textbook. These summa-\nries were scored by two expert raters using the same rubric from the original summary \ndataset. Despite extensive efforts of rater norming, inter-rater reliability was lower \nthan in the original dataset (QWK = 0.576). Additionally, quadratic weighted kappa \nvalues showed a large amount of variability between criteria, as seen in Table 3. \nGiven that reliability was still acceptable in most cases, we generated principal com-\nponents for Content and Wording for each Macroeconomics summary. Finally, we \nevaluated the strongest LLM on the Prolific test set by comparing the LLM predicted \nscores to the PCA scores derived from human scores.\nResults\nComparing Models That Include the Source Text to Models That are Naive to the \nSource\nThe results of comparing predicted scores in the held-out test set of the training data-\nset to the actual scores assigned by expert human raters are presented in Table 4. For \nContent scores, the Longformer model, in which both the summary and the source \nwere included in the input, achieved higher accuracy than the RoBERTa model that \nconsidered only the summary (explaining 82% versus 67% of the variance, respec -\ntively). For Wording scores, the Longformer model outperformed the RoBERTa \nmodel (explaining 70% versus 41% of the variance, respectively). Scatterplots for \nthe results are presented in Fig. 2.\nContent Wording\nr R2 r R2\nRoBERTa (pretrained) 0.82 0.67 0.64 0.36\nRoBERTa (domain adapted) 0.83 0.69 0.65 0.42\nLongformer (pretrained) 0.91 0.82 0.87 0.70\nLongformer (domain adapted) 0.85 0.72 0.78 0.60\nTable 4 Results from the roberta \nand longformer models\n \nPCA Criterion QWK\nContent Organization 0.485\nMain Points 0.567\nDetails 0.611\nWording V oice 0.320\nLanguage 0.531\nWording 0.730\nTotal 0.576\nTable 3 Inter-rater reliability \nstatistics for out-of-domain \ndataset\n \n1 3\n1032\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nDomain Adaptation Through Unsupervised Pretraining\nTraining from a domain adapted model improved performance slightly in the Roberta \nmodel compared to finetuning from the pretrained model. However, the domain \nadapted Longformer model performed worse than the non-domain adapted version \nfor both content and wording (explaining 72% and 60% of the variance, respec -\ntively). Even with reduced performance compared to the base model, the domain \nadapted Longformer model still performed better than either of the two RoBERTa \nmodels with no access to the source text.\nPost-hoc Tests on the Prototype Intelligent Textbook\nWhile our non-domain adapted Longformer model performed well on the training \ndata, we further tested it using the section summaries written by the textbook authors \nfound in the Macroeconomics textbook. We did this by using the summaries and \ntheir matching sections to predict quality scores for content and wording. To create a \ncomparison group, we also ran the model on the summaries paired with unmatched \nsource texts. The results are illustrated in Fig. 3. The summaries matched to the cor-\nrect sources have scored higher than average in wording and content. The summaries \nFig. 2 Predicted scores plotted against actual scores for the four models\n \n1 3\n1033\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\npaired with unmatched sources have scored below average in Content and slightly \nabove average in Wording. The differences between matched and unmatched content \nwere statistically significant ( p <.001) in both domains with large effect sizes, but \nContent reported a greater effect size (d = 1.56) than Wording (d = 0.938).\nThe accuracy of the Longformer model on the dataset of summaries written by \nparticipants recruited through Prolific was lower than the accuracy on the original \ndataset. Pearson’s product-moment correlations showed a strong correlation between \npredictions and human scores for Content r(123) = 0.70, p <.001. for Content. How-\never, the Wording model did a poorer job at predicting human scores r(123) = 0.29, \np =.001. Figure 4 displays scatterplots showing the correlations between the predicted \nand human scores in this dataset.\nFig. 4 Scatterplot showing correlations between predicted and human scores on prolific dataset\n \nFig. 3 Boxplot showing distributions of generated scores for professional summaries\n \n1 3\n1034\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nDiscussion\nThis paper introduces robust LLMs integrated within iTELL to provide formative \nassessment for summaries written at the end of chapter sections in intelligent text -\nbooks. These summarization models can assess reading comprehension for students \nworking within the intelligent textbook, provide feedback to students to help them \nbetter understand their comprehension of material, and deliver an overview to teach-\ners about how well students understand the material provided. The summarization \nmodels presented in this study are more robust than previously reported models, \nlikely because of the training data provided and because we used a principal compo-\nnent analysis on analytic scores from a summarization rubric to aggregate scores into \ntwo summarization criteria: content and wording.\nOur gains in performance over those reported in previous studies, especially Cross-\nley et al. ( 2019) and Botarleanu et al. ( 2022), are likely the result of using cleaned \ntraining data and training the model on principal component scores that characterize \nthe summaries in a more condensed representation. The top-performing Longformer \nmodels developed in this study achieved R2 values of 0.82 and 0.70 when predicting \nhuman ratings, outperforming RoBERTa models and previous LLM-based automatic \nsummary evaluation models, including the Transformer models by Botarleanu et al. \n(2022) which achieved R2 ∼ 55%, and the more semantic approaches used by Cross-\nley et al. (2019) who reported an R2 of 0.53.\nIn answer to the first research question regarding whether there is a substantial \ndifference in accuracy when a model is provided with both the summary and the \nsource, a model trained to take both the source and the summary performed better \nthan one which only had access to the summary. The increased max sequence length \nprovided by Longformer’s sparse attention allowed us to input both the summary and \nthe source divided by a separator token, which led to increased accuracy compared \nto RoBERTa, which only had access to the summary. The differences are most appar-\nent in the case of Wording, where the model based on the pretrained Longformer \nreported almost double the R2 value relative to the RoBERTa model.\nThe answer to the second research question, whether domain adaptation can \nimprove the accuracy of the summary scoring models, reported mixed results. The \nRoBERTa model benefited from domain adaptation, but it was the weaker of the two \nmodels tested. In contrast, finetuning directly on the pretrained Longformer model \nproduced better results than finetuning on the domain-adapted model. This may be \nbecause the Commonlit dataset with many summaries only included six sources, \nwhich did not provide the language variation needed for the problem space. Instead \nof generating models that generalize and provide scores for summaries of any source, \nthe domain adaptation step may have created models that are specifically adapted to \nthe six sources in the Commonlit dataset. This hypothesis is supported by the fact that \ndomain adaptation helped somewhat in the case of the RoBERTa models in which the \nsource was omitted from the input. In the case of the Longformer model, the small \nnumber of sources may have resulted in catastrophic forgetting (Ramasesh et al., \n2021), where the model overfitted to those sources and forgot some of the parameters \nfrom its pretraining.\n1 3\n1035\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nIn answer to the third research question, the analysis of the Longformer LLM on \nexpert summaries found in a Macroeconomics textbook provided some evidence of \nconcurrent validity for the developed model. When the model was tested on summa-\nries matched with the correct source, the model outputted scores above the mean on \naverage for both Content and Wording. However, when tested on summaries matched \nwith incorrect sources, the Content score was nearly half a standard deviation lower \nthan the mean on average. In contrast, the Wording score remained above the mean \n(although significantly lower than the score when the summaries and sources were \ncorrectly matched). These results make sense because Content scores should more \nstrongly differ between matched and unmatched source texts. However, Wording \nmeasures include features related to paraphrasing and language beyond the source, \nwhich would be higher in expert summaries, but only partially reliant on the source \ntext. The results from the Macroeconomics summaries provide evidence that the \nmodels discriminate between matched and unmatched summaries. Summaries are \nscored better when paired with the correct source, especially in terms of Content.\nThe test on summaries solicited from participants on Prolific had mixed results. \nThe Content model performed well on these summaries. By contrast, although the \nWording model predictions were better than chance, it did not accurately predict \nhuman scores with fidelity. This may be a result of the difficulty in aligning the raters \nto the original scoring procedure. Although we had access to the rubric provided in \nAppendix A, we did not have access to the rater training procedure. As a result, our \nraters were not capable of attaining sufficient agreement with each other and presum-\nably were also not aligned with the original set of raters.\nApplication\nThe summary evaluation models developed here were integrated into the iTell frame-\nwork to assess reading comprehension via end-of-section summarization. The pur -\npose of the summaries is to make the textbooks more interactive and provide students \nwith opportunities to produce knowledge and test understanding, while having access \nto timely personalized feedback. Within iTELL, the summary scoring models have \nbeen combined with several other features to ensure the accuracy of the feedback \nprovided and to ensure that good-faith efforts are made by students. After students \nproduce a summary (students cannot cut and paste), and before that summary is \npassed to the scoring models, the summaries go through a filter component. The \nfilter ensures that the summary is between 50 and 200 words, and the summary is \npassed through a semantic similarity measure using Doc2Vec (Le & Mikolov, 2014) \nthat assesses whether the summary is on topic. Additionally, summaries that heavily \nborrow from the source text or contain offensive language are rejected without being \nanalyzed by the LLMs. Source borrowing scores are based on prevalence of overlap-\nping n-grams between the summary and the source (Broder, 1998) while offensive \nlanguage ratings are based on occurrences of offensive words or phrases from an \noffensive word list (Inflianskas, 2019). These filters help ensure that only effortful \nsummaries are passed to the models and help reduce the computational load required \nby the models.\n1 3\n1036\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nSummaries are then run through the LLMs, which are used to develop forma -\ntive feedback. Figure 5 displays a screenshot with examples of written feedback that \nstudents may receive from iTELL for high and low-quality summaries on Content, \nWording, source borrowing, and topic similarity. Although numerical scores are cal-\nculated for each of these criteria on the back end, the user receives written qualitative \nfeedback. If the scores are below a certain threshold, the user will be encouraged to \nrevisit the section and asked to rewrite their summary before moving on to the next \nsection. In addition, key phrases are identified within the source text using KeyBART \n(Inflianskas, 2019), an LLM trained to generate keyphrases. Using KeyBART, stu -\ndents are provided with a list of key phrases from the source text not present in their \nsummary, and are then directed to specific paragraphs or subsections they may not \nhave included in their summaries to help with revision. These feedback mechanisms \nprovide the learners with actionable formative feedback beyond the output of the \nLLMs. The feedback provided by the summary evaluation tools can help provide \ninsight into the students’ reading comprehension skills, assist students in reflecting, \nsummarizing, and articulating what they learned, provide class-level and individual \nFig. 5 Screenshots from iTELL displaying feedback from low and high-quality summaries\n \n1 3\n1037\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nstudent metrics on textbook comprehension, and help developers redesign the cur -\nriculum and the textbook itself.\nConclusion\nIn this study, we used RoBERTa and Longformer pretrained Transformers to finetune \nfour large language models to score student-written section summaries automatically. \nAlthough the source texts summarized in the dataset used to train the model are \nlikely shorter than intelligent textbook sections, the topics were similar (i.e., they \nwere academic). The accuracy and post-hoc validation scores for the Content models \nwere strong enough for inclusion into iTELL, especially in the case of the mod -\nels finetuned from the Longformer pretrained model. The Wording model showed \npromising results but more validation needs to be done using real-world data from \ntextbooks. The summarization models incorporated into iTell can provide students \nwith opportunities for open-ended comprehension assessment and interactive feed -\nback within intelligent textbooks. Additionally, the models are freely available on \nHuggingFace1,2, allowing access to learning platforms, researchers, and textbooks \ndeveloped outside the iTELL framework.\nAlthough the models are strongly predictive, they have limitations. First, \nwhile the summaries found in the training data are broadly similar to the target \ntask, and post-hoc tests provided evidence of concurrent validity, more testing \nin target domains is necessary to ensure that the model accuracy reported in this \nstudy will transfer to the task of scoring summaries within a variety of different \nacademic topics. This is particularly the case with the Wording model, since post-\nhoc tests showed lower generalizability outside of the training data. Addition -\nally, the models need to be tested on intelligent textbook users to ensure that the \nfeedback provided by the models leads to increase learning. Another limitation \ninvolves the interpretability of the LLM’s output for teachers and learners. The \ntwo numerical scores provided by the models indicate summary quality. Nev -\nertheless, the feedback provided to users should explain at a granular level the \ncomponents of the summaries that lead to the scores and provide actionable sug -\ngestions for improvement. Future work should focus on explainable Artificial \nIntelligence methods to better understand the decisions within the LLMs that lead \nto the scores.\n1 https://huggingface.co/tiedaar/longformer-content-global\n2 https://huggingface.co/tiedaar/longformer-wording-global\n1 3\n1038\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nAppendix A – Scoring Rubric\nScore Main points/Gist Details Cohesion Objective \nlanguage\nWording/\nParaphrasing\nLanguage \nbeyond \nsource \ntext\nSum-\nmary \nlength\n1 Main idea is not \nlinked to central \ntopic\nState-\nments are \nnot related \nto the \npassage\nIdeas are \nrandomly \npresented \nand do \nnot link \nto each \nother\nThe \nlanguage \nused \nis not \nobjective.\nSummary \nshows a heavy \nreliance on \nverbatim copy-\ning of source \nlanguage.\nSummary \nshows a \nvery basic \nunder-\nstanding of \nlexical and \nsyntactic \nstructures.\nMuch \nshorter \nor \nlonger \nthan \nex-\npected\n2 Main idea is linked \nto central topic but \nthere is no topic \nsentence to bring \nideas together\nSome key \ninfor-\nmation \nfrom the \npassage is \nincluded, \nbut \nimportant \nideas are \nmissing\nSome \nideas link \nto each \nother\nSome \nof the \nlanguage \nused is \nobjective\nSummary \nshows some \nuse of original \nwording, \nbut there are \nexamples \nof verbatim \nor near-\ncopy of source \nlanguage.\nSummary \nshows an \nunder-\nstanding of \nlexical and \nsyntactic \nstructures.\nShort-\ner or \nlonger \nthan \nex-\npected\n3 Main idea is linked \nto central topic \nand there is a topic \nsentence that states \nsome aspect of the \ncontent\nMost key \ninfor-\nmation \nfrom the \npassage is \nincluded, \nbut some \nideas may \nbe irrel-\nevant or \ninaccurate\nMost \nideas are \nlogically \npresented\nMost \nof the \nlanguage \nused is \nobjective\nSummary \nshows evi-\ndence of appro-\npriate levels of \nparaphrasing.\nSummary \nshows an \nappro-\npriate \nrange of \nlexical and \nsyntactic \nstructures\nA bit \nshorter \nor \nlonger \nthan \nex-\npected\n4 Main idea is linked \nto central topic and \nhas a topic sentence \nthat states the main \nidea.\nAll key \ninforma-\ntion in the \npassage is \nincluded \nwithout \nirrelevant \nideas.\nAll \nideas are \nlogically \npresented\nAll of the \nlanguage \nused is \nobjective\nSummary \nshows substan-\ntial evidence \nof appropriate \nparaphrasing \nuse.\nSummary \nshows an \nexcellent \nrange of \nlexical and \nsyntactic \nstructures.\nAp-\npropri-\nate \nlength.\nBased on Taylor (2013), Westley, Culatta, Lawrence, & Hall-Kenyon (2010)\nDeclarations\nConflict of interest The authors affirm that there are no conflicts of interest.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative \nCommons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \n1 3\n1039\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nto the material. If material is not included in the article’s Creative Commons licence and your intended use \nis not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission \ndirectly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/\nlicenses/by/4.0/.\nReferences\nAbdullah, M., Madain, A., & Jararweh, Y . (2022). ChatGPT: Fundamentals, applications and social \nimpacts. In 2022 Ninth International Conference on Social Networks Analysis, Management and \nSecurity (SNAMS) (pp. 1–8). Ieee. https://doi.org/10.1109/SNAMS58071.2022.10062688\nAlpizar-Chacon, I., & Sosnovsky, S. (2021). Knowledge models from PDF textbooks. New Review of \nHypermedia and Multimedia, 27(1–2), 128–176. https://doi.org/10.1080/13614568.2021.1889692.\nBareiss, R., & Osgood, R. (1993). Applying AI models to the design of exploratory hypermedia systems. \nProceedings of the Fifth ACM Conference on Hypertext - HYPERTEXT ’93 , 94–105. https://doi.\norg/10.1145/168750.168790.\nBeltagy, I., Peters, M. E., & Cohan, A. (2020). Longformer: The Long-Document Transformer. https://doi.\norg/10.48550/ARXIV .2004.05150.\nBotarleanu, R. M., Dascalu, M., Allen, L. K., Crossley, S. A., & McNamara, D. S. (2022). Multitask Sum-\nmary Scoring with Longformers. In M. M. Rodrigo, N. Matsuda, A. I. Cristea, & V . Dimitrova (Eds.), \nArtificial Intelligence in Education  (V ol. 13355, pp. 756–761). Springer International Publishing. \nhttps://doi.org/10.1007/978-3-031-11644-5_79.\nBroder, A. Z. (1998). On the resemblance and containment of documents. Proceedings Compression \nand Complexity of SEQUENCES 1997 (Cat no 97TB100171) , 21–29. https://doi.org/10.1109/\nSEQUEN.1997.666900.\nBrusilovsky, P., Sosnovsky, S., & Thaker, K. (2022). The return of intelligent textbooks. AI Magazine, \n43(3), 337–340. https://doi.org/10.1002/aaai.12061.\nBrusilovsky, P., & Pesin, L. (1998). Adaptive navigation support in educational hypermedia: An evaluation \nof the ISIS-Tutor. Journal of computing and Information Technology, 6(1), 27–38. https://hrcak.srce.\nhr/file/221190\nChen, C. M., Chen, L. C., & Horng, W. J. (2021). A collaborative reading annotation system with for -\nmative assessment and feedback mechanisms to promote digital reading performance. Interactive \nLearning Environments, 29(5), 848–865. https://doi.org/10.1080/10494820.2019.1636091.\nChollet, F. (2018). Deep learning with Python. Manning Publications Co.\nChulkov, D. V ., & VanAlstine, J. (2013). College student choice among electronic and printed textbook \noptions. Journal of Education for Business, 88(4), 216–222.\nClinton-Lisell, V ., Seipel, B., Gilpin, S., & Litzinger, C. (2021). Interactive features of E-texts’ effects on \nlearning: A systematic review and meta-analysis. Interactive Learning Environments, 1–16.\nCrossley, S. A., Kim, M., Allen, L., & McNamara, D. (2019). Automated Summarization Evaluation \n(ASE) Using Natural Language Processing Tools. In S. Isotani, E. Millán, A. Ogan, P. Hastings, B. \nMcLaren, & R. Luckin (Eds.), Artificial Intelligence in Education (V ol. 11625, pp. 84–95). Springer \nInternational Publishing. https://doi.org/10.1007/978-3-030-23204-7_8.\nGalbraith, D., & Baaijen, V . M. (2018). The work of writing: Raiding the Inarticulate. Educational Psy-\nchologist, 53(4), 238–257. https://doi.org/10.1080/00461520.2018.1505515.\nGamage, D., Staubitz, T., & Whiting, M. (2021). Peer assessment in MOOCs: Systematic literature review. \nDistance Education, 42(2), 268–289. https://doi.org/10.1080/01587919.2021.1911626.\nGanesan, K. (2018). ROUGE 2.0: Updated and Improved Measures for Evaluation of Summarization \nTasks. https://doi.org/10.48550/ARXIV .1803.01937.\nGraham, S., & Harris, K. R. (2015). Common Core State standards and writing: Introduction to the Special \nIssue. The Elementary School Journal, 115(4), 457–463. https://doi.org/10.1086/681963.\nGraham, S., Kiuhara, S. A., & MacKay, M. (2020). The effects of writing on learning in Science, Social \nstudies, and Mathematics: A Meta-analysis. Review of Educational Research, 90(2), 179–226. https://\ndoi.org/10.3102/0034654320914744.\nHead, M. H., Readence, J. E., & Buss, R. R. (1989). An examination of summary writing as a mea -\nsure of reading comprehension. Reading Research and Instruction , 28(4), 1–11. https://doi.\norg/10.1080/19388078909557982.\n1 3\n1040\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nInflianskas, R. (2019). Profanity Filter. GitHub repository. https://github.com/rominf/profanity-filter/blob/\nmaster/profanity_filter/data/en_profane_words.txt.\nJi, S. W., Michaels, S., & Waterman, D. (2014). Print vs. electronic readings in college courses: Cost-\nefficiency and perceived learning. The Internet and Higher Education, 21, 17–24.\nKhandelwal, U., Clark, K., Jurafsky, D., & Kaiser, L. (2019). Sample Efficient Text Summarization Using \na Single Pre-Trained Transformer. https://doi.org/10.48550/ARXIV .1905.08836.\nKim, M. K., Gaul, C. J., Bundrage, C. N., & Madathany, R. J. (2020). Technology supported reading \ncomprehension: A design research of the student mental model analyzer for research and teaching \n(SMART) technology. Interactive Learning Environments, 1–25. https://doi.org/10.1080/10494820\n.2020.1838927.\nKumar, G., Banchs, R., & D’Haro, L. F. (2015). RevUP: Automatic gap-fill question generation from \nEducational texts. Proceedings of the Tenth Workshop on Innovative Use of NLP for Building Educa-\ntional Applications, 154-161, https://doi.org/10.3115/v1/W15-0618.\nLabutov, I., Huang, Y ., Brusilovsky, P., & He, D. (2017). Semi-supervised techniques for Mining Learning \noutcomes and prerequisites. Proceedings of the 23rd ACM SIGKDD International Conference on \nKnowledge Discovery and Data Mining, 907, 915. https://doi.org/10.1145/3097983.3098187.\nLagakis, P., & Demetriadis, S. (2021). Automated essay scoring: A review of the field. 2021 Interna -\ntional Conference on Computer, Information and Telecommunication Systems (CITS) , 1–6. https://\ndoi.org/10.1109/CITS52676.2021.9618476.\nLan, A. S., & Baraniuk, R. G. (2016). A Contextual Bandits Framework for Personalized Learning Action \nSelection. EDM, 424–429.\nLe, Q., & Mikolov, T. (2014). Distributed representations of sentences and documents  (pp. 1188–1196). \nPMLR.\nLi, H., Cai, Z., & Graesser, A. C. (2018). Computerized summary scoring: Crowdsourcing-based \nlatent semantic analysis. Behavior Research Methods , 50(5), 2144–2161. https://doi.org/10.3758/\ns13428-017-0982-7.\nLin, C. Y ., & Hovy, E. (2003). Automatic evaluation of summaries using N-gram co-occurrence statis -\ntics. Proceedings of the 2003 Conference of the North American Chapter of the Association for \nComputational Linguistics on Human Language Technology - NAACL ’03 , 1, 71–78. https://doi.\norg/10.3115/1073445.1073465.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., & Stoyanov, \nV . (2019). RoBERTa: A Robustly Optimized BERT Pretraining Approach. ArXiv:1907.11692 [Cs]. \nhttp://arxiv.org/abs/1907.11692.\nLopez, L. E., Cruz, D. K., Cruz, J. C. B., & Cheng, C. (2021). Simplifying Paragraph-Level Question \nGeneration via Transformer Language Models. In D. N. Pham, T. Theeramunkong, G. Governatori, \n& F. Liu (Eds.), PRICAI 2021: Trends in Artificial Intelligence (V ol. 13032, pp. 323–334). Springer \nInternational Publishing. https://doi.org/10.1007/978-3-030-89363-7_25.\nMartínez-Huertas, J. Á., Jastrzebska, O., Olmos, R., & León, J. A. (2019). Automated summary evaluation \nwith inbuilt rubric method: An alternative to constructed responses and multiple-choice tests assess-\nments. Assessment & Evaluation in Higher Education , 44(7), 1029–1041. https://doi.org/10.1080/0\n2602938.2019.1570079.\nMorris, W., Crossley, S., Holmes, L., Ou, C., McNamara, D., & Dascalu, M. (2023a). Using Large Lan -\nguage Models to Provide Formative Feedback in Intelligent Textbooks. In International Conference \non Artificial Intelligence in Education (pp. 484–489). Cham: Springer Nature Switzerland.\nMorris, W., Crossley, S. A., Langdon, H., & Trumbore, A. (2023b). Using Transformer Language Models \nto Validate Peer-Assigned Essay Scores in Massive Open Online Courses (MOOCs). In Proceedings \nof the Thirteenth International Conference on Learning Analytics & Knowledge.\nNelson, N., & King, J. R. (2022). Discourse synthesis: Textual transformations in writing from sources. \nReading and Writing. https://doi.org/10.1007/s11145-021-10243-5.\nNg, J. P., & Abrecht, V . (2015). Better Summarization Evaluation with Word Embeddings for ROUGE  \n(arXiv:1508.06034). arXiv. http://arxiv.org/abs/1508.06034.\nPapineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2001). BLEU: A method for automatic evaluation of \nmachine translation. Proceedings of the 40th Annual Meeting on Association for Computational Lin-\nguistics - ACL ’02, 311, https://doi.org/10.3115/1073083.1073135.\nPhillips Galloway, E., & Uccelli, P. (2019). Beyond reading comprehension: Exploring the additional \ncontribution of Core Academic Language skills to early adolescents’ written summaries. Reading and \nWriting, 32(3), 729–759. https://doi.org/10.1007/s11145-018-9880-3.\n1 3\n1041\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nRamasesh, V . V ., Lewkowycz, A., & Dyer, E. (2021). Effect of scale on catastrophic forgetting in neu-\nral networks. In International Conference on Learning Representations . https://openreview.net/\npdf?id=GhVS8_yPeEa\nRockinson-Szapkiw, A. J., Courduff, J., Carter, K., & Bennett, D. (2013). Electronic versus traditional \nprint textbooks: A comparison study on the influence of university students’ learning. Computers & \nEducation, 63, 259–266.\nScialom, T., Lamprier, S., Piwowarski, B., & Staiano, J. (2019). Answers Unite! Unsupervised Metrics for \nReinforced Summarization Models. https://doi.org/10.48550/ARXIV .1909.01610.\nSeaman, J. E., & Seaman, J. (2020). Digital texts in the time of COVID: Educational resources in U.S. \nHigher Education. Bay View Analytics.\nShao, T., Guo, Y ., Chen, H., & Hao, Z. (2019). Transformer-based neural network for answer selection in \nquestion answering. Ieee Access: Practical Innovations, Open Solutions , 7, 26146–26156. https://\ndoi.org/10.1109/ACCESS.2019.2900753.\nShorten, C., Khoshgoftaar, T. M., & Furht, B. (2021). Text Data Augmentation for Deep Learning. Journal \nof Big Data, 8(1), 101. https://doi.org/10.1186/s40537-021-00492-0.\nSilva, M., A., & Limongi, R. (2019). Writing to learn increases long-term memory consolidation: A \nMental-Chronometry and computational-modeling study of Epistemic writing. Journal of Writing \nResearch, 11(vol(11 issue 1), 211–243. https://doi.org/10.17239/jowr-2019.11.01.07.\nSosnovsky, S., Brusilovsky, P., & Lan, A. (2023). Intelligent textbooks: The fifth international workshop. \nIn international conference on artificial intelligence in education  (pp. 97–102). Cham: Springer \nNature Switzerland. https://link.springer.com/chapter/10.1007/978-3-031-36336-8_15\nThaker, K., Zhang, L., He, D., & Brusilovsky, P. (2020). Recommending Remedial Readings Using Stu -\ndent Knowledge State. Educational Data Mining Society.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M. A., Lacroix, T., & Lample, G. (2023). \nLlama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\nTunstall, L., V on Werra, L., & Wolf, T. (2022). Natural Language Processing with transformers. O’Reilly \nMedia, Inc.\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,... & Polosukhin, I. (2017). \nAttention is all you need. Advances in neural information processing systems , 30. https://proceed-\nings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\nWang, M., Chau, H., Thaker, K., Brusilovsky, P., & He, D. (2021). Knowledge annotation for Intelligent \ntextbooks. Technology Knowledge and Learning. https://doi.org/10.1007/s10758-021-09544-z.\nWeber, G., & Brusilovsky, P. (2016). ELM-ART– An Interactive and Intelligent web-based Electronic \nTextbook. International Journal of Artificial Intelligence in Education , 26(1), 72–81. https://doi.\norg/10.1007/s40593-015-0066-8.\nWinchell, A., Mozer, M., Lan, A., Grimaldi, P., & Pashler, H. (2018). Can Textbook Annotations Serve as \nan Early Predictor of Student Learning? International Data Mining Society.\nWolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Fun -\ntowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y ., Plu, J., Xu, C., Le Scao, T., \nGugger, S., & Rush, A. (2020). Transformers: State-of-the-Art Natural Language Processing. Pro-\nceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System \nDemonstrations, 38–45. https://doi.org/10.18653/v1/2020.emnlp-demos.6.\nYarbro, J. T., & Olney, A. M. (2021). Contextual Definition Generation. Proceedings of the Third Interna-\ntional Workshop on Intelligent Textbooks, 2895.\nPublisher’s Note Springer Nature remains neutral with regard to jurisdictional claims in published maps \nand institutional affiliations.\n1 3\n1042\nInternational Journal of Artificial Intelligence in Education (2025) 35:1022–1043\nAuthors and Affiliations\nWesley Morris1  · Scott Crossley1 · Langdon Holmes1 · Chaohua Ou2 · \nMihai Dascalu3 · Danielle McNamara4\n \r Wesley Morris\nwesley.g.morris@vanderbilt.edu\n1 Vanderbilt University, Nashville, TN, USA\n2 Georgia Institute of Technology, Atlanta, GA, USA\n3 Polytechnic University of Bucharest, Bucharest, Romania\n4 Arizona State University, Tempe, AZ, USA\n1 3\n1043",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8379714488983154
    },
    {
      "name": "Automatic summarization",
      "score": 0.7930706143379211
    },
    {
      "name": "Formative assessment",
      "score": 0.7502233982086182
    },
    {
      "name": "Rubric",
      "score": 0.7355926632881165
    },
    {
      "name": "Natural language processing",
      "score": 0.5604563355445862
    },
    {
      "name": "Grading (engineering)",
      "score": 0.5171391367912292
    },
    {
      "name": "Language model",
      "score": 0.4917149841785431
    },
    {
      "name": "Reading comprehension",
      "score": 0.4633140563964844
    },
    {
      "name": "Variance (accounting)",
      "score": 0.4586600661277771
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4434784948825836
    },
    {
      "name": "Principal (computer security)",
      "score": 0.44187480211257935
    },
    {
      "name": "Reading (process)",
      "score": 0.35951849818229675
    },
    {
      "name": "Mathematics education",
      "score": 0.2532293200492859
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Accounting",
      "score": 0.0
    },
    {
      "name": "Civil engineering",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200719446",
      "name": "Vanderbilt University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I61641377",
      "name": "Universitatea Națională de Știință și Tehnologie Politehnica București",
      "country": "RO"
    },
    {
      "id": "https://openalex.org/I141595442",
      "name": "University of Bucharest",
      "country": "RO"
    },
    {
      "id": "https://openalex.org/I55732556",
      "name": "Arizona State University",
      "country": "US"
    }
  ]
}