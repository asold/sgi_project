{
  "title": "Exploring the Numerical Reasoning Capabilities of Language Models: A Comprehensive Analysis on Tabular Data",
  "url": "https://openalex.org/W4389518637",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3166639338",
      "name": "Mubashara Akhtar",
      "affiliations": [
        "King's College London"
      ]
    },
    {
      "id": "https://openalex.org/A5077030220",
      "name": "Abhilash Shankarampeta",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107792188",
      "name": "Vivek Gupta",
      "affiliations": [
        "California University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A3208308556",
      "name": "Arpit Patil",
      "affiliations": [
        "University of Utah"
      ]
    },
    {
      "id": "https://openalex.org/A2547056916",
      "name": "Oana Cocarascu",
      "affiliations": [
        "King's College London"
      ]
    },
    {
      "id": "https://openalex.org/A385557172",
      "name": "Elena Simperl",
      "affiliations": [
        "King's College London"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3201339301",
    "https://openalex.org/W4206068491",
    "https://openalex.org/W4287065027",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2162127011",
    "https://openalex.org/W4366835609",
    "https://openalex.org/W2513499049",
    "https://openalex.org/W1738097990",
    "https://openalex.org/W4378465094",
    "https://openalex.org/W3166890286",
    "https://openalex.org/W4385574071",
    "https://openalex.org/W2971822538",
    "https://openalex.org/W4385573636",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W3100879603",
    "https://openalex.org/W4309953112",
    "https://openalex.org/W4310882088",
    "https://openalex.org/W2963199195",
    "https://openalex.org/W3035140194",
    "https://openalex.org/W3098495697",
    "https://openalex.org/W4309674289",
    "https://openalex.org/W3170403598",
    "https://openalex.org/W2094133148",
    "https://openalex.org/W3035275890",
    "https://openalex.org/W4378470054",
    "https://openalex.org/W3102999298",
    "https://openalex.org/W4318908878",
    "https://openalex.org/W4285171787",
    "https://openalex.org/W4287854457",
    "https://openalex.org/W3212438831",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W4378501677",
    "https://openalex.org/W3184222203",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2796798338",
    "https://openalex.org/W4385574086",
    "https://openalex.org/W3034723486",
    "https://openalex.org/W3034602344",
    "https://openalex.org/W4378473852",
    "https://openalex.org/W4385573115",
    "https://openalex.org/W2951939640",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W2013299270",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2950645060",
    "https://openalex.org/W4385573798",
    "https://openalex.org/W4404752283",
    "https://openalex.org/W2903738701",
    "https://openalex.org/W4385573499",
    "https://openalex.org/W4321177655",
    "https://openalex.org/W3155039781",
    "https://openalex.org/W3198593990",
    "https://openalex.org/W4285137661",
    "https://openalex.org/W4298184221",
    "https://openalex.org/W3188892795",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4385565099",
    "https://openalex.org/W3103667349",
    "https://openalex.org/W3035231859",
    "https://openalex.org/W2100045519",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W2891308403",
    "https://openalex.org/W3035428952"
  ],
  "abstract": "Numerical data plays a crucial role in various real-world domains like finance, economics, and science. Thus, understanding and reasoning with numbers are essential in these fields. Recent benchmarks have assessed the numerical reasoning abilities of language models, revealing their limitations in limited and specific numerical aspects. In this paper, we propose a complete hierarchical taxonomy for numerical reasoning skills, encompassing over ten reasoning types across four levels: representation, number sense, manipulation, and complex reasoning. We conduct a comprehensive evaluation of state-of-the-art models on all reasoning types. To identify challenging reasoning types for different model types, we develop a diverse and extensive set of numerical probes and measure performance shifts. By employing a semi-automated approach, we focus on the tabular Natural Language Inference (TNLI) task as a case study. While no single model excels in all reasoning types, FlanT5 (few-/zero-shot) and GPT3.5 (few-shot) demonstrate strong overall numerical reasoning skills compared to other models in our probes.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 15391–15405\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nExploring the Numerical Reasoning Capabilities of Language Models:\nA Comprehensive Analysis on Tabular Data\nMubashara Akhtar1∗ , Abhilash Shankarampeta2*, Vivek Gupta3, Arpit Patil4\nOana Cocarascu1 and Elena Simperl1\n1King’s College London 2Meesho 3University of Pennsylvania 4University of Utah\nmubashara.akhtar@kcl.ac.uk\nabhilash.shankarampeta@meesho.com\nAbstract\nNumbers are crucial for various real-world\ndomains such as finance, economics, and\nscience. Thus, understanding and reasoning\nwith numbers are essential skills for language\nmodels to solve different tasks. While different\nnumerical benchmarks have been introduced\nin recent years, they are limited to specific\nnumerical aspects mostly. In this paper, we\npropose a hierarchical taxonomy for numerical\nreasoning skills with more than ten reasoning\ntypes across four levels: representation,\nnumber sense, manipulation, and complex\nreasoning. We conduct a comprehensive\nevaluation of state-of-the-art models to identify\nreasoning challenges specific to them. Hence-\nforth, we develop a diverse set of numerical\nprobes employing a semi-automated approach.\nWe focus on the tabular Natural Language\nInference (TNLI) task as a case study and\nmeasure models’ performance shifts. Our\nresults show that no model consistently excels\nacross all numerical reasoning types. Among\nthe probed models, FlanT5 (few-/zero-shot)\nand GPT-3.5 (few-shot) demonstrate strong\noverall numerical reasoning skills compared to\nother models. Label-flipping probes indicate\nthat models often exploit dataset artifacts to\npredict the correct labels.1\n1 Introduction\nNumerical data is ubiquitous in the real-world.\nMany applications in domains such as finance, eco-\nnomics and science require understanding and rea-\nsoning with numbers. In recent years, benchmarks\nwere introduced to study language models’ numer-\nacy skills (Zhang et al., 2020; Wallace et al., 2019;\nDua et al., 2019). However, these datasets mostly\nconcentrate on few, specific numerical reasoning\ntypes (e.g. scales (Zhang et al., 2020)).\n∗Equal contributions\n1Data and code are available at https://github.com/\nmubasharaak/numerical_reasoning.\nHulk\nDirected by Ang Lee\nRelease date June 20, 2003\nRunning time 138 minutes\nBudget $137 million\nBox office $245.4 million\nH1: Hulk was released on 20th June, 2003. (E)\nDate: Hulk was released on 20-06-2003. (E)\nDate Flip: Hulk was released on 12-08-2009. (C)\nH2: The movie has a length of 138 minutes. (E)\nAppr: The movie has a length of about 150 minutes. (C)\nH3: The movie can be watched in about two hours. (E)\nNum: The movie can be watched in about 2 hours. (E)\nNum Flip: The movie can be watched in 1 hours. (C)\nArith: Hulk brought in $108.4 million profit. (E)\nArith Flip: Hulk brought in $120.9 million profit. (C)\nTable 1: Base hypotheses (H 1, H 2, H 3) and ( flipped)\nprobes for heterogeneous numbers (i.e. date), approximation,\nnumeracy, and arithmetic. Labelled as Entail or Contradict.\nMoreover, evaluating models on numerical\nbenchmarks, it often remains unclear why models\nstruggle with the tasks. For example, the issues can\narise from models struggling to recognize numeri-\ncal representations in text, failing to compute arith-\nmetic operations, or predicting incorrect outputs\ndue to a lack of numerical commonsense knowl-\nedge. We aim to explore these questions in greater\ndetail in this study. Limitations of language models’\nnumerical abilities, as discussed in prior research,\ninclude tokenization and representation of numbers\nin text (Thawani et al., 2021b), hallucination (Ji\net al., 2023; Chen et al., 2023; Ye et al., 2023), and\ngeneralizability/robustness issues (Razeghi et al.,\n2022; Geva et al., 2020; Xu et al., 2022).\nSuccessful numerical reasoning requires a com-\nbination of skillsets: understanding representa-\ntion of numbers (Thawani et al., 2021a,b) and\ntheir meaning in a given context (Loukas et al.,\n2022), applying operations (Geva et al., 2020; Pa-\ntel et al., 2021), and integrating factual and com-\nmonsense numerical knowledge to solve numerical\n15391\nproblems (Lin et al., 2020; Park et al., 2022). For\nexample, classifying the hypotheses “The movie\ncan be watched in about 2 (or ‘two’) hours. ”from\nTable 1 requires understanding that both “2” and\n“two” depict the same numerical value, converting\n“2 hours” to another unit (i.e. 120 minutes), and\napplying approximation to map “120 minutes” to\n“138 minutes” in the table.\nIn this paper, we evaluate state-of-the-art models\non various numerical reasoning types. To assess\nwhich reasoning types are challenging for specific\nmodels, we create a diverse and large set of numer-\nical probes and measure shifts in models’ perfor-\nmance. We organize all probed reasoning types in\na hierarchical taxonomy. Inspired by how humans\nunderstand and reason with numbers, as well as\nprevious numerical benchmarks, we include eleven\nreasoning types across four level: representation,\nnumber sense, manipulation, and complex reason-\ning (Figure 1). We apply a semi-automated ap-\nproaches for probe creation. We select tabular NLI\n(TNLI) as a case study task, given three criteria:(i)\nnumerical data (numbers, percentages, dates, etc.)\nis prevalent in tables; (ii) tables are common in\nreal-world data sources such as in scientific publi-\ncations, database systems and financial documents;\n(iii) tables as structured data facilitate automated\nperturbations to create large-scale probing sets. See\nTable 1 for some examples of probes created from\nhypotheses (H1, H2, H3) and the given table.\nOur experiments conclude that large language\nmodels (LLMs) like FlanT5 and GPT3.5 perform\nbetter than other models on various numerical rea-\nsoning tasks. Both table-based and numerical mod-\nels struggled to understand data with flipped la-\nbels and negative values. Moreover, we observe\nthat some models’ performance improves signif-\nicantly for counterfactual probes (e.g. NT 5 and\nTAPAS) and label-flipping probes (e.g. FlanT 5\nzero-shot), which indicates that models might ex-\nploit dataset artifacts and are biased towards one\nlabel. These findings emphasize the importance\nof further systematically investigating numerical\nreasoning capabilities across various NLP models.\nOur contributions are as follows:\n• We introduce a taxonomy for numeri-\ncal reasoning skills, including representa-\ntion/number sense/manipulation skills and\ncomplex reasoning with numbers.\n• We propose a semi-automated approach to\nFigure 1: Overview of numerical reasoning types.\ncreate large-scale, numerical probe sets using\ntable NLI datasets.\n• We evaluate three different categories of lan-\nguage models (LMs) on our numerical probe\nsets: (i) numerical LMs; (ii) LMs for tabular\ndata; and (iii) zero-/few-shot LMs.\n2 A Taxonomy for Numerical Reasoning\nThis section introduces a hierarchical taxonomy for\nnumerical reasoning, inspired by previous works\non numeracy in NLP (Thawani et al., 2021b; Xu\net al., 2022) and psychology (Barrouillet and Fayol,\n1998a; Whyte and Bull, 2008; Bofferding, 2019).\nWe group numerical reasoning skills given their\ncomplexity level in four categories: R1 − R4.\n2.1 Number Representation ( R1)\nThis category includes skills for understanding the\nform of numerical data. Similar to the notion of\nform in language (Bender and Koller, 2020), this\nis the realization of numbers in text; the way they\nare represented and expressed.\nNumeration. Numeration studies language\nmodel’s understanding of representation systems\ncommon for numbers in English: the Arabic\n(“2”) and English (“two”) numeration systems.\nSpecifically, we probe if LMs can link between\ndistinct symbols used for the same number. For\nexample in Figure 1, H3 contains “two” as a word,\nwhich can be also represented through “2”.\nHeterogeneous Number Types. Formatted num-\nbers (e.g. dates, times, and fractions) are frequently\nused to convey additional information associated\nwith a numerical value. Numbers are formatted\nin a specific way given their context and purpose,\n15392\nsuch as expressing times and dates using full-stop\n(“.”), using the “%” symbol to indicate fractions,\nand different currency symbols for money (i.e. “$”\nor “C”), e.g. H1 and “Arith” in Figure 1.\nNegative Numbers. Early on in their develop-\nment, children develop some mental model for neg-\native numbers (see experiments with first-graders\nin Bofferding (2019)). Using negative numbers re-\nquires understanding the notation of negatives (i.e.\n“−” followed by a number). This also includes dis-\ntinguishing between minus in subtractions (1 − 3),\ndates (12-12-2022), counts (i.e. from one to three)\nand in negative number (−2).\n2.2 Number Sense ( R2)\nNumber sense includes reasoning skills for con-\nceptualizing number quantities and understanding\ntheir meaning in a given context.\nScale. In everyday communication, numbers\ncommonly occur with measurement scales, e.g.\nweights, distances, or heights. Understanding num-\nbers in context of scales is a basis for various ap-\nplications, e.g. question answering (e.g. “We are\ndriving 80 km/h, is this within the speed limit?”),\ncommonsense (e.g. “Cats weight between four and\nfive kilograms. ”) (Lin et al., 2020), and temporal\nreasoning (e.g. “She left the office thirty minutes\nago. ”) (Zhou et al., 2020; Zhang et al., 2020).\nComparison. Comparing numbers allows under-\nstanding numerical relationships. It involves iden-\ntifying which numbers are greater than, less than,\nor equal to others. For example, given the table in\nFigure 1, understanding “The running time of Hulk\nis longer than 120 minutes. ”requires comparison.\nRange. The question “Was the budget of the\nmovie between $130 and $245.4?” about the table\nin Figure 1 requires understanding number ranges.\nAlready at an age between two and three years,\nchildren develop numerical abilities to understand\nsequences of numbers and start reciting numbers\nin an appropriate order (Fuson, 2012; Laski and\nSiegler, 2007). Models’ that understand the nota-\ntion of ranges, can correctly answer the question\nby knowing that 137 is in the range 130 − 245.4.\nApproximation. Humans commonly approxi-\nmate number values in everyday life (Odic and\nStarr, 2018; Bonny and Lourenco, 2013). H 3 in\nFigure 1 requires approximation among other skills\nto map “about two hours” to “138 minutes” in the\ntable. As a reasoning skill, it allows to make quick\nestimations and metric unit conversations, and un-\nderstand the approximate values of numbers with-\nout calculating them explicitly.\n2.3 Manipulation ( R3)\nManipulation reasoning types are used to apply\nbasic operations on numbers such as addition. Suc-\ncessful manipulation of numbers requires under-\nstanding their representations and meaning in the\ngiven context (i.e. number sense).\nSorting. The sentence “Out of all Ang Lee’s di-\nrected movies, ‘Hulk’ was the one with the sec-\nond highest box office income. ” requires sorting\nall movies according to their box office income in\norder to select the one with the second highest in-\ncome. Sorting objects according to some criteria\nis a basic milestone for developing cognitive skills.\nBy age two, children already begin to understand\nthe concept of sorting.\nSimple arithmetic. Arithmetic reasoning is the\nability of manipulating numbers with basic oper-\nations (addition, subtraction, multiplication, divi-\nsion). While adults commonly retrieve results of\nsimple calculations from memory, children use dif-\nferent operations (Barrouillet and Fayol, 1998b).\n2.4 Complex Reasoning ( R4)\nThis category builds on all previous reasoning cat-\negories (R1 − R3) to solve numerical word prob-\nlems (NWP). NWP are expressed through natural\nlanguage and require multistep reasoning. Extract-\ning information from the problem description and\napplying numerical/mathematical reasoning using\nthe retrieved information and world/commonsense\nknowledge is required (Upadhyay and Chang,\n2017; Amini et al., 2019; Huang et al., 2016).\n3 Numerical Probing Framework\nThis section provides an overview of the probing\nframework. We use tabular Natural Language In-\nference (TNLI) for automated probe creation.\n3.1 Preliminaries\nTables for numerical probing. Tables align well\nwith our objectives given three key criteria: (i)\nnumerical data is common in tables; (ii) tables\nare frequent in real-world data sources; (iii) ta-\nbles, due to their structured formats, facilitate au-\ntomated perturbations for probe creation. Tables’\n15393\nsemi-structured format, the alignments available\nbetween table cells and column/row headers, and\nthe frequency of numbers, make them well suitable\nfor creating numerical probes automatically.\nTable NLI. Given a natural language sentence as\nhypothesis and a tabular premise, the aim of TNLI\nis to classify if the hypothesisentails or contradicts\nthe table (Gupta et al., 2020). We use the table\nNLI datasets TabFact (Chen et al., 2020) and In-\nfoTabs (Gupta et al., 2020), as well as recast the\ntable QA datasets TAT-QA (Zhu et al., 2021) and\nTabMWP (Lu et al., 2023) to NLI (i.e. TATQA-\nNLI, TabMWP-NLI). TAT-QA includes metadata,\ni.e. annotations of cells and operations per correct\nanswer. This information is not available for any\nTNLI dataset and is crucial to create probes for\nspecific reasoning types, e.g. arithmetic reasoning.\nTable 2 provides an overview of the TNLI datasets.\nPreprocessing. For each of numerical reasoning\ntype, we first identify base TNLI hypotheses and/or\ntables in the datasets that can be used for auto-\nmated probe creation. Hereby, we defined a list of\nreference tokens specific for each reasoning type\nand to filter relevant dataset samples. For exam-\nple, we used units of measurements such as “hour”,\n“meter”, or “kilogram” filter hypotheses for scale\nprobes (see §4 for more details). To recast the\nTAT-QA dataset, we follow the simple yet effective,\nrule-based approach proposed by Demszky et al.\n(2018) for QA to NLI conversion.\n3.2 Probes through Structural Perturbation\nOverall, we our framework includes three types of\nprobes, created through hypotheses perturbation\nand counterfactual tables.\n1. Hypothesis label-preserving probes We cre-\nate label-preserving probes changing the base hy-\npothesis such that its meaning is not changed and\nthe initial label is preserved. They are used to evalu-\nate model’s ability to reason and predict the correct\nlabel given semantically-equivalent changes.\n2. Hypothesis label-flipping probes To generate\nlabel-flipping probes, we modify the base hypothe-\nsis such that its meaning alters and the label of the\nprobe flips, e.g. from entailment to contradiction.\nWe aim to overcome potential dataset artefacts that\nmight be exploited for label prediction instead of\nperforming numerical reasoning.\nThese changes are specific to the reasoning types.\nFor example, to flip labels for scale probes, we\nDataset Hypotheses Tables Num cells Probes\nTabFact 118,275 16,573 59.00% 214,440\nInfoTabs 23,738 2,540 53.6% 19,779\nTATQA-NLI 4,947 2,156 59.7% 15,139\nToTTo 1,000 892 45.7% 1,000\nTabMWP 283 283 38.3% 238\nTable 2: TNLI probing datasets; num cells refers to the\naverage ratio of numerical cells in tables.\nsubstitute measurement units for a particular scale\n(e.g. “kilograms”) by another unit (e.g. “meters”)\nor introduce errors in conversion of units (e.g. 3\nkilometers replaced by 3, 000 meters).\n3. Table Probes through Counterfactual Table\nEditing We also probe with counterfactual tables\nto evaluate if models rely on spurious patterns in\nthe premise table for label prediction. We filter the\ncounterfactual datasets by Jena et al. (2022) consist-\ning of {hypothesis; original table; counterfactual\ntable} for numerical hypotheses.\n4 Probing with TNLI Datasets\nThis section discussed probes in detail and how we\ncreated them for each reasoning type from §3.2\nNumeration. To study models’ understanding of\nstring (“two”) and numerical (e.g. “ 2”) number\nrepresentations, we create two types of numeration\nprobes. One converting number representations\nfrom strings to numeric, while the second category\napplies the conversion vice versa. We filter hy-\npotheses with numbers written as strings (“two”)\nand substitute them by their numeric counterpart\n(e.g. “2”). The label-preserving probes are seman-\ntically equivalent to the base hypotheses and the la-\nbel (e.g. entailment) is not changed. Label-flipping\nprobes replace the converted numberx by a random\nnumber in the range of[x−x∗0.5; x+x∗0.5]. For\nexample, the numeration flipping probe of H1 (Ta-\nble 3) replaces 112 by one hundred and forty-four\nand flips the label from entailment to contradiction.\nHeterogeneous number types. We created het-\nerogeneous probes for the following categories fre-\nquent in the TNLI datasets: date formats, ordinals,\npercentage, currencies, and scientific notation. To\nfilter base hypotheses, we applied a simple, rule-\nbased approach specific to each category (i.e. dates\nformats, percentage, ordinals, etc.). To create label-\npreserving probes we applied representation-level\nchanges which did not change the semantic mean-\ning. For H3, we substituted 3rd June, 1986 by an-\nother English date format 03-06-1986. To flip the\n2Find details on probe statistics in Appendix B.\n15394\nRafael Nadal\nPlays Left-handed\nBorn 3 June 1986 (age 37)\nHeight 1.85 m\nTurned pro 2001\nPrize money US$116,111,561 (3rd all-time leader in earnings)\nBase Hypothesis H1 Born in 1986 , Nadal is age 37 currently.\nNumeration Probe H1 Born in nineteen eighty six, Nadal is age thirty seven currently.\nNum Flip Probe H1 Born in nineteen ninety two, Nadal is age forty one currently.\nRange Probe H1 Born in 1986, Nadal is age between 31-43 currently.\nBase Hypothesis H2 The player’s birth date is on 3rd June, 1986 .\nHeterog Probe H2 The player’s birth date is on 03-06-1986.\nHeterog Flip Probe H2 The player’s birth date is on 15-01-1999.\nBase Hypothesis H3 With $116,111,561 prize money, he is the 3rd highest earning all-time player.\nHeterog Probe H3 With $116.111561e − 6 prize money, he is the third highest earning all-time player.\nApprox Probe H3 With about $116, 000, 000 prize money, he is the 3rd highest earning all-time player.\nBase Hypothesis H4 Rafael Nadal has a height of 1.85 meters.\nScale Probe H4 Rafael Nadal has a height of 185 centimeters.\nScale Flip Probe H4 Rafael Nadal has a height of 5.2 ft.\nBase Hypothesis H5 After the year 2000, the player Nadal turned pro.\nComparison Probe H5 After the year 1995, the player Nadal turned pro.\nComparison Flip Probe H5 Before the year 1990, the player Nadal turned pro.\nTable 3: Exemplary hypotheses and non-/flipping probes for evaluated reasoning types\nlabel, we replaced the date in the adjusted format by\na random date, i.e. 15-01-1999. We replaced per-\ncentage signs by the token “percentages” and vice\nversa. Similarly, ordinals written as words ( first)\nwere exchanged by numerical representations (1st)\nand the other way around. For hypotheses with\nlarge numbers (e.g. “ $116,111,561 ” in H3), we\nintroduced scientific notations ($116.111561e−6).\nNegative numbers. To create negative probes,\nwe replaced negative numbers −n (e.g. −3) by\nstring equivalents (e.g. minus 3; negative 3)\nand evaluated changes in model performances on\nthese semantically same sentence pairs. For label-\nflipping probes, we converted negative numbers\ninto the positive counterpart n. For example, con-\nverting “The company’s monthly closing resulted\nin -5 million USD. ”to “The company’s monthly\nclosing resulted in 5 million USD. ”flips the label.\nScale. We created two types of scale probes: (i)\nconversion; (ii) mapping. Conversion convert\nnumbers within a measurement scale. For H 4 in\nTable 3, we converted the number and measure-\nment unit ( 1.85 meters ) to the next smaller unit\nwithin the same scale (185 centimeters) for the\nlabel-preserving probe. For label-flip, we intro-\nduced an error in the converted number, i.e. con-\nverting 1.85 meters. to 5.2 ft instead of 6.07 ft.\nMapping probes replace the number and measure-\nment unit by an equivalent (e.g. 1.85m by 1.85\nmeters) for label-preserving probes and a random\nmeasurement unit e.g. 1.85m to 1.85 kilograms)\nto flip the base hypotheses.\nComparison. We first created a list of sig-\nnal word-pairs by prompting GPT 3.5. The\nlist includes pairs such as {“bigger”:“smaller”},\n{“taller”:“shorter”}, and {“faster”:“slower”}. Us-\ning these pairs and their synonyms, we filtered base\nhypotheses and created three types of comparison\nprobes. First, changing the signal word with its op-\nposite counterpart to flip labels (see H5 in Table 3\nflipping “after” to “before”). Second, altering the\nnumber such that the comparison and label do not\nchange: replacing “after 2000” by “after 1995”\n(H5). Finally, we combine both prior approaches\nto create label-flipping probes, e.g. “Before the\nyear 1990, the player Nadal turned pro.”s\nApproximation. We first extract a number n\nfrom our base hypothesis and given the value of\nn, we decide the magnitude of rounding to ap-\nply. While smaller numbers are rounded to tens,\nlarger number are rounded to hundreds, thousands\nor larger decimal points. For example, we cre-\nated the probe “With about $116, 000, 000 prize\nmoney, he is the 3rd highest earning all-time player”\n15395\nby rounding n equal $116,111,561 to “about\n$116, 000, 000” (H3 in Table 3).\nRange. To create range probes, we substitute\nnumber n in the base hypothesis by an appropri-\nate range, e.g. 37 by “between 31-43” (H1). We\ndefine the radius of the range and its boundaries\nautomatically given the value of n. For example,\ngiven n < 10, we randomly sample a radius be-\ntween 1 − 5. For n = 7and a sampled radius of\n2, the range will be [5 − 9]. We select decimal\nboundaries if n is a decimal number.\nSorting. We utilized table columns as number\nsequences to create sorting probes. We generated a\nlist of position indicators in number sequences (e.g.\n“top”, “second” “3rd”,“biggest”, “lowest”). These\nwords were used to filter base hypotheses. To cre-\nate label-flipping probes, we changed the position\nof the sequence to another one. For instance, we\nmodified “in the first quarter of 2018” to “in the\nthird quarter of 2018” by selecting the value from\nthe third row instead of the first.\nSimple arithmetic. Using on TATQA-NLI its\nmetadata indicating the involved numbers and op-\nerations for numerical reasoning, we created arith-\nmetic probes. We extracted probes involving addi-\ntion, subtraction, multiplication, and division. Ad-\nditionally, we generated label-flipping probes by\nreplacing the operation output (e.g. result of sub-\ntraction) in the hypothesis with a different number.\nIn Table 1, the “Arith” probe involves calculating\nthe difference between the budget and box office\nvalues to determine the correctness of 108.4. The\nflipped arithmetic probe produces a close but incor-\nrect subtraction output, 120.9.\nNumerical word problems. We converted\nTabMWP questions and answers into declarative\nhypotheses. TabMWP is a dataset of free-text math\nword problems that involve reasoning with tabu-\nlar data. For label-flipping probes, we substituted\nnumbers in the hypotheses with random numbers\nfrom the same column.\nCounterfactual Table NLI Probes. We filtered\nthe counterfactual ToTTo (Parikh et al., 2020)\ndataset by Jena et al. (2022) for numerical hypothe-\nsis. To create counterfactual tables, they swap two\nor more table cells to modify the tables such that\nthe label of the respective hypothesis changes from\nentailment to contradiction and vice versa.\n5 Experiments and Analysis\nNext, we provide an overview of all models that\nwere evaluated on the probes from §4. We also\ndiscuss the obtained results and insights.\n5.1 Probed Models\nWe use state-of-the-art models which are divers\nin terms of architecture, size, and training setup,\ngrouped into three categories:\n(C1) Numerical LMs. This category includes\nLMs adapted for numerical reasoning. LUNA (Han\net al., 2022) is a recent transformer-based model\nwith an adapted tokenization approach for numbers.\nThe model encodes numbers as single tokens (e.g.\n3, 201) instead of splitting them down into sub-\nwords or binned tokens. NT5 (Yang et al., 2021)\nis a variation of the T5 model. It has been mod-\nified for numerical reasoning through additional\npretraining objectives and fine-tuning using numer-\nical datasets. PASTA (Gu et al., 2022) is based on\nDeBERTa and is pretrained with objectives that use\ntable-based numeric operations.\n(C2) LMs for tabular reasoning.\nTAPAS (Herzig et al., 2020) extends the BERT\nencoder with table-specific embeddings. We used a\nTAPAS model trained with intermediate pretraining\non synthetic and counterfactual data (Eisenschlos\net al., 2020). We also probe TAPEX (Liu et al.,\n2022), which uses BART (Lewis et al., 2020) as\nits base model and pretrains the model to mimic\na neural SQL executor over tables. Similarly,\nReasTAP (Zhao et al., 2022) is a BART-based\nmodel pretrained on synthetically generated data\nrequiring seven table reasoning skills, including a\nnumerical task, temporal reasoning, and conjunc-\ntion. Previous works have also shown the success\nof the *BERT models on tabular NLI tasks (Herzig\net al., 2020; Yin et al., 2020; Shankarampeta\net al., 2022; Akhtar et al., 2022). Tables are either\nlinearized or processed into sentences or structured\nformats. The transformed tables are then used\nas input to the models. We used a DeBERTa\nmodel (He et al., 2021) trained on multiple NLI\ndatasets for this setting.\n(C3) Large LMs. For few-/zero-shot evaluation,\nwe selected FlanT 5 (Shen et al., 2023), GPT 3.5,\nand PaLM 2 (Chowdhery et al., 2022) and probed\nthem in both a few-shot and zero-shot setting.\n15396\nModel Table Specific Numerical Specific Large LMs\nTAPAS DeBERTa TAPEXNT5 LUNA PASTA ReasTAP FlanT5 GPT3.5 PaLM\nReasoning few zero few zero few zero\nRepresentation\nNumeration -0.32 -1.82 -7.84 -4.18 -5.22 -7.7 -7.18 1.28 -8.84 -0.47 5.65 -1.35 -3.07\nHeterogeneous -4.03 -2.36 -5.94 -3 -10.09 -7.76 -3.18 0.34 -5.49 6.8 6.65 0.44 -2.22\nNegative -46.11 -13.77 0.56 -94.48 -75.55 -10.68 2.65 19.21 42.3 8.24 2.3 -2.17 1.14\nLabel Flipped\nNumeration -38.87 4.09 -43.3 -48.53 -71.35 -25.85 -37.21 -78.37 33.38 -37.78 44.71 -37.29 -46.45\nHeterogeneous -9.57 8.53 -32.25 -1.97 -43.48 -23.59 -16.21 -53.44 86.6 -27.97 27.79 -20.65 -25.31\nNegative -64.81 -41.56 -97.01 -17.87 76.85 -70.58 -96.46 -83.92 173.14 -63.64 2.2 -80.43 -78.41\nNumber Sense\nScale 0.03 -6.25 -12.91 1.21 -11.43 -1.56 -4.6 -9.45 -7.05 2.46 -0.52 -3.71 -17.58\nComparison -21.8 -18.18 -12.58 -29.19 -30 -35.11 -40.02 29.38 140.82 -9.39 9.13 -16.91 -20.08\nApproximation -5.61 -6.65 -18.9 -9.55 -7.67 -27.44 -7.81 -9.66 -12.94 0.03 12.08 -10.44 -12.03\nRange -18.89 -33.77 -1.96 -20.43 -86.77 -84.66 4.97 22.44 178.13 0.5 -1.07 14.37 4.41\nLabel Flipped\nScale -23.73 -64.58 -30.41 -39.08 -68.44 -51.66 -16.54 -69.56 93.77 -39.08 39.98 -17.85 -27.4\nComparison 57.67 -19.36 -4.83 -29.62 -0.28 -19.1 -15.65 -8.47 -40.75 -20.81 14.67 -17.96 -16.09\nManipulation\nSorting -34.8 28.66 -91 -22.6 54.31 -4.9 -83.96 -86.67 25 -57.39 -5.62 -32.45 -39.59\nArithmetic -58.62 -24.96 -95.53 -27.1 7.07 -49.06 -88.87 -71.53 265.07 -60.34 4.87 -67.67 -64.98\nComplex Reasoning\nComplex 63.37 6.93 -80.18 -3.22 41.41 -50.84 116.17 -89.77 -40 -60.22 -4.35 -73.4 -75.9\nCounterfactual 44.5 55.54 -12.29 159.3 0.98 -6.09 -10.12 61.5 12.23 40.63 5.26 30.57 48.56\nTable 4: Probing results given as accuracy difference (in %) between base hypotheses and probes.\n5.2 Training and Evaluation\nTo fine-tune models, we used the base hypotheses\nof the training datasets (e.g. InfoTabs) and eval-\nuated models only on probes created with their\ntestsets. The few-shot models were prompted with\n2-shot extrapolation. We evaluated all models in a\n3-step process: (1) evaluation of base hypotheses\nH; (2) evaluation of probes P, created using H;\n(3) calculating changes in model performance by\ncomparing accuracy of P to H. As our TNLI task\nis a binary classification task, we used accuracy for\nevaluation.\n5.3 Results and Discussion\nTable 4 gives on overview of all probing results.\nIf available, we separately list scores for flipped\nprobes, e.g. numeration and numeration flipped.\n(Q1) Does any model excel in all numerical\nreasoning types? While there is not one best-\nperforming model across all reasoning types and\ndifferent models struggle with different types,\nFlanT5 and GPT 3.5 show overall good perfor-\nmance in a zero-shot setting. While GPT3.5 (few-\nshot) performance drops by −60.22% for complex\nreasoning probes, the model’s average accuracy\nchange is around −16.7% for other types. This can\nbe related to (1) models pretraining data, and (2)\ntraining on chain-of-thought reasoning tasks (Wei\net al., 2022). GPT 3.5 was trained on more than\n300 TB data Common Crawl, allowing the model\nto memorize much more numerical data than other\nprobed models. In comparison, DeBERTa was\ntrained on only 78GB of data (He et al., 2021).\nInterestingly, both NT5 and FlanT5 use T5 as their\nbase model. FlanT5 was instruction-fine-tuned and\noutperforms NT5 many probing categories.\n(Q2) What factors can contribute to high per-\nformance variations across certain reasoning\ntypes? Large performance variations mainly oc-\ncur due to inconsistent numerical reasoning of mod-\nels across tasks. For example, we observe that\nsome models struggle with more basic reasoning\n(e.g., FlanT5 zero on numeration) while perform-\ning very well on more complex types. This be-\nhavior might have different reasons. One potential\nreason is memorization. Previous works (Petroni\net al., 2019; Carlini et al., 2021; Ishihara, 2023)\nshow that large pretrained language models store\nknowledge in their parameters, which they tend\nto retrieve instead of reasoning over the provided\ninput (Gupta et al., 2022a). Hence, models can\nmemorize common arithmetic operations they en-\ncounter during training and perform well on certain\ndownstream tasks. For example, flipping numbers\nas words (“two”) to numerals (“ 2”) might allow\nmodels to retrieve knowledge which they didn’t\nconsider for the initial hypothesis. Another reason\nfor high-performance drops can be the hallucina-\ntion of models. While models initially perform well\non hypotheses, adjusting the numbers can hinder\nmodels from relying on spurious patterns.\n(Q3) How do models perform on different types\nof numerical reasoning? Representation. In\n15397\nTable 4, comparing numeration probes, we find\nfor all models a performance drop of between\n[0; −10] percentages for numeration probes, ex-\ncept FlanT5 (few). This drop strongly increases\nfor almost all models evaluated numeration flipped\nprobes. For example, FlanT 5 (few) shows a per-\nformance drop of −78.37%. FlanT 5 (few) also\nperforms well on heterogeneous probes, followed\nby DeBERTa (−2.4%) and NT5 (−3%). Whereas\nLUNA performance drops significantly for hetero-\ngeneous probes (flipped and non-flipped). TAPAS,\nNT5, and LUNA show significant performance\ndrops (between −38.87% and −71.35%) on neg-\native number probes. This could be because the\nmodels exploit correlations between the “−” sign\nand labels for predicting base hypotheses. Interest-\ningly, few- and zero-shot models like FlanT5 and\nGPT3.5 show improvements on negative number\nprobes. This may be because the models under-\nstand probed versions of negative numbers (e.g.\n“minus 22”) as a negative number but not the initial\nrepresentation (e.g. “−22”).\nNumber sense. Comparing models based on\nnumber sense probes, we observe different patterns\nfor fine-tuned models and few-/zero-shot models.\nFine-tuned models struggle especially with com-\nparison probes, with a −26.7% average perfor-\nmance drop. Scale probes show a−42.1% decrease\non flipping probes, while approximation (flipping)\nprobes report a −12.0% decrease in model per-\nformance. In contrast, FlanT 5 perform better on\ncomparison and range probes, sometimes surpass-\ning predictions on the base hypotheses. All mod-\nels demonstrate lower performance on approxima-\ntion probes compared to the base hypotheses, with\nPASTA performance dropping by−27.44%.\nManipulation and Complex Reasoning. Fine-\ntuned models exhibit an average accuracy drop\nof −57% on arithmetic probes, except for LUNA\nwith a slight performance increase. The perfor-\nmance of PaLM (zero) and FlanT5 (few) drops by\n−67.67% and −71.53%, respectively. All models’\nperformance drops on sorting probes (avg. −27%),\nexcept for DeBERTa, LUNA, and FlanT5 (zero).\nUnlike most other reasoning types, fine-tuned mod-\nels outperform few-/zero-shot models on complex\nreasoning probes. ReasTAP achieves the highest\naccuracy, followed by TAPAS and LUNA. FlanT5,\nTAPEX, and PaLM have the largest performance\ndrops on complex reasoning probes.\n(Q4) Do models perform similarly for flipped\nand non-flipped probes? We observe higher per-\nformance drops for label-flipping probes compared\nto non-flipping probes across models. Models that\nstruggle with flipping probes but perform well on\ntheir non-flipping counterparts indicate a reliance\non spurious patterns for label prediction. The per-\nformance of TAPAS, TAPEX, PASTA, ReasTAP,\nFlanT5 (few), and PaLM drops significantly for\nthe representation reasoning category comparing\nnon-flipping and flipping probes. For example,\nTAPAS performance drops by−2.28% on numer-\nation probes, but show a drop of −45.98% on nu-\nmeration flipping probes. Similarly, DeBERTa per-\nforms well on scale probes ( −6.25%) compared\nto the flipping version (−64.58%). PaLM perfor-\nmance on numeration, heterogeneous, and negative\nprobes drops by approximately −35%, −20%, and\n−80% on flipping counterparts. DeBERTa exhibits\nrobust performance on number flipping probes for\nsorting and FlanT5 on negative numbers, as well\nas arithmetic probes.\n(Q5) Are numerical and table-specific models\nbetter for numerical reasoning than LLMs?\nNumerical models. Our experiments do not in-\ndicate any superiority of numerical models over\nothers. LUNA, a transformer model that uses a spe-\ncific tokenization method for numbers, performs\nsimilarly to other models on many reasoning types.\nThe only reasoning type where LUNA outperforms\nis comparison flipping probes, with a small im-\nprovement of 0.28%. PASTA is a DeBERTa-based\nmodel trained on numerical data and pretraining\nobjectives. However, compared to DeBERTa, it\nonly performs better on negative number and scale\nprobes.\n6 Related Work\nNumeracy Taxonomies in NLP. Prior works\nhave introduced surveys and taxonomies to organ-\nise numeracy in NLP research. Thawani et al.\n(2021b) categorise NLP work on numeracy into\nseven subtasks along the dimensions granularity\n(i.e. exact and approximate numbers) and unit (i.e.\nabstract and grounded numbers). Xu et al. (2022)\nfocus on the robustness of QA systems in handling\nnumerical data and organize their numeracy prob-\ning tasks in two broad categories: (i) numerical\nparsing, and (ii) semantic parsing. The DROP\nbenchmark (Dua et al., 2019) is a QA dataset that\nrequires discrete operations (e.g. subtraction, count,\n15398\nsort) to answer questions over text. While Thawani\net al. (2021b) concentrate on number representa-\ntions in NLP systems, our work includes three\nfurther numerical reasoning categories. Xu et al.\n(2022) focus on the robustness of NLP models in\nhandling numerical data. Our probing study on the\nother side pivots towards the reasoning capabilities\nof models when dealing with numerical and tabu-\nlar data. Different to prior work, our study gives\na broad and in-depth evaluation of ten different\nmodels from three different categories (numerical,\ntabular, large pretrained LMs) on more than ten\ndifferent reasoning types (representation, number\nsense, manipulation, and complex reasoning).\nLanguage Model / Numerical Skills. Var-\nious studies have evaluated LMs’ numeri-\ncal skills in recent years. Earlier works\nprobed word embeddings for numeration (e.g.\n4=four) (Naik et al., 2019), comparison (e.g.\n3 < 4) (Wallace et al., 2019), scale (Zhang\net al., 2020), and superlatives (Wallace et al.,\n2019). More recent works evaluate LMs on out-of-\ndistribution numbers (Kim et al., 2021), numera-\ntion/magnitude/sorting/superlatives (Pal and Baral,\n2021), and arithmetic (Muffo et al., 2022). Our\nwork builds upon these previous evaluation studies\nand extends them with further numerical probing\ncategories, e.g. heterogeneous numbers.\nNumerically-tuned Language Models. Various\nnumerical LMs have been developed in recent\ntimes. Geva et al. (2020) and Liang et al. (2022) in-\nject numerical skills into BERT through numerical\npretraining objectives. PASTA (Gu et al., 2022) and\nNT5 (Yang et al., 2021), which are based on De-\nBERTa and T5 respectively, fall into the same cate-\ngory of models. Another line of work adjusts LMs’\narchitectures for numerical reasoning through nu-\nmerical tokenization (Han et al., 2022) or addi-\ntional, numerical embeddings (Jin et al., 2021).\nSystematic Probes for Tables. Tables have been\nutilized previously used to create probes for ta-\nble grounding (Gupta et al., 2022b) or recasting\nnon-NLI datasets (e.g. question-answering) to\nNLI (Jena et al., 2022). Unlike unstructured text\ndata, tables have a natural structure that allows cre-\nating controlled experiments more easily (Gupta\net al., 2022a). We drew inspiration from prior tab-\nular probing approaches and extended them for\nautomating probing of numerical tabular data. Jena\net al. (2022) introduce a generic approach to adjust\ntable QA datasets and generate NLI data. For data\nrecasting, they follow a systemic approach simi-\nlar to ours. However, the focus is on transforming\nQA datasets, emphasizing the end-result (i.e. the\nNLI data) through data augmentation. They do not\nevaluate the presence of numerical data in their ta-\nbles or consider numerical reasoning in the model\nevaluation phrase.\nComparison to Prior Work. All the above men-\ntioned prior works on numerical reasoning have\nprovided motivation for our research. However,\ntheir evaluations have focused on a narrow range\nof reasoning types and models. Most study only\nconcentrated on one specific model such as T5 (Pal\nand Baral, 2021), GPT 3 (Muffo et al., 2022), or\nBERT (Park et al., 2022). In contrast, our frame-\nwork provides a comprehensive evaluation of nu-\nmerical reasoning skills. We cover a wide spectrum\nof complexity levels, ranging from representation\nto complex reasoning. Moreover, we assess a vari-\nety of models with diverse architectures, sizes, and\ntraining settings for numerical reasoning.\n7 Conclusion\nThis paper presents a framework for probing lan-\nguage models’ numerical reasoning skills. We or-\nganise skills in a taxonomy and generate large-scale\nsets of probes covering more than ten numerical\nreasoning types. Using table NLI as a case study,\nwe evaluate the numerical reasoning abilities of\nten models. These models belong to the categories\nnumerical LMs, tabular LMs, and few-/zero-shot\nLLMs. We discuss reasoning types that prove chal-\nlenging for the probed models and explore promis-\ning directions for future research.\nFuture Directions. For certain (numerical)\ntasks, tool-augmented LMs equipped with capa-\nbilities such as calculators or code execution have\nbeen proven valuable (Mialon et al., 2023). How-\never, certain tasks require implicit numerical rea-\nsoning which might not necessarily involve direct\ncalculations based on numbers. For instance, classi-\nfying sentences that incorporate numbers in varied\nsettings, like time indications (Feng et al., 2023),\ncurrencies or conversations (Macina et al., 2023).\nSuch tasks demand a numerical interpretation be-\nyond mere arithmetic computations. Moreover,\ncalling external tools using LMs requires basic nu-\nmerical comprehension to invoke an external tool\ncorrectly (Chen et al., 2022).\n15399\nLimitations\nThis work proposes a taxonomy and framework\nto probe numerical reasoning skills in LMs. It in-\nvolves the creation of large-scale probing sets using\nan automated approach. However, the evaluation of\nthis approach is currently limited to the task of table\nNLI. For future research, it is interesting to extend\nthis to include additional tasks and datasets. This\nextension serves two purposes: first, it allows eval-\nuating a more diverse range of datasets. Second, it\nenables including challenges specific to other tasks.\nIn this paper, the evaluation of most reasoning\ntypes primarily involves structural changes at the\nhypotheses level. While we include counterfac-\ntual table probes, they are limited to one dataset\nand perturbations method only. Further research is\nneeded to study models’ performance on numeri-\ncal data in the premise data. Therefore, we need\ntable-based probes for all reasoning types of the\nproposed taxonomy.\nEthics Statement\nIn this paper, we study the numerical reasoning\nskills of different LMs. However, to deploy these\nsystems in real-world applications, further studies\nand evaluations specific to the intended use cases\nare required. In order to support future research,\nwe plan to release all the scripts and resources used\nfor probe creation and model evaluation. This will\nfacilitate and encourage further research in this\nfield.\nReferences\nMubashara Akhtar, Oana Cocarascu, and Elena Simperl.\n2022. PubHealthTab: A public health table-based\ndataset for evidence-based fact checking. In Find-\nings of the Association for Computational Linguistics:\nNAACL 2022, pages 1–16, Seattle, United States. As-\nsociation for Computational Linguistics.\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. MathQA: Towards interpretable math\nword problem solving with operation-based for-\nmalisms. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n2357–2367, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nPierre Barrouillet and Michel Fayol. 1998a. From algo-\nrithmic computing to direct retrieval: Evidence from\nnumber and alphabetic arithmetic in children and\nadults. Memory &amp Cognition, 26(2):355–368.\nPierre Barrouillet and Michel Fayol. 1998b. From al-\ngorithmic computing to direct retrieval: Evidence\nfrom number and alphabetic arithmetic in children\nand adults. Memory & Cognition, 26:355–368.\nEmily M. Bender and Alexander Koller. 2020. Climbing\ntowards NLU: On meaning, form, and understanding\nin the age of data. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5185–5198, Online. Association for\nComputational Linguistics.\nLaura Bofferding. 2019. Understanding Negative Num-\nbers, pages 251–277. Springer International Publish-\ning, Cham.\nJustin W. Bonny and Stella F. Lourenco. 2013. The ap-\nproximate number system and its relation to early\nmath achievement: Evidence from the preschool\nyears. Journal of Experimental Child Psychology ,\n114(3):375–388.\nNicholas Carlini, Florian Tramèr, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom B. Brown, Dawn Song, Úl-\nfar Erlingsson, Alina Oprea, and Colin Raffel. 2021.\nExtracting training data from large language models.\nIn 30th USENIX Security Symposium, USENIX Se-\ncurity 2021, August 11-13, 2021, pages 2633–2650.\nUSENIX Association.\nAnthony Chen, Panupong Pasupat, Sameer Singh, Hon-\ngrae Lee, and Kelvin Guu. 2023. PURR: efficiently\nediting language model hallucinations by denoising\nlanguage model corruptions. CoRR, abs/2305.14908.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W. Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from rea-\nsoning for numerical reasoning tasks. CoRR,\nabs/2211.12588.\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai\nZhang, Hong Wang, Shiyang Li, Xiyou Zhou, and\nWilliam Yang Wang. 2020. Tabfact: A large-scale\ndataset for table-based fact verification. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\n15400\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nDorottya Demszky, Kelvin Guu, and Percy Liang.\n2018. Transforming question answering datasets\ninto natural language inference datasets. CoRR,\nabs/1809.02922.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2368–2378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nJulian Eisenschlos, Syrine Krichene, and Thomas\nMüller. 2020. Understanding tables with interme-\ndiate pre-training. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n281–296, Online. Association for Computational Lin-\nguistics.\nYu Feng, Ben Zhou, Haoyu Wang, Helen Jin, and Dan\nRoth. 2023. Generic temporal reasoning with dif-\nferential analysis and explanation. In Proceedings\nof the 61st Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 12013–12029, Toronto, Canada. Association\nfor Computational Linguistics.\nKaren C Fuson. 2012. Children’s counting and concepts\nof number. Springer Science & Business Media.\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020.\nInjecting numerical reasoning skills into language\nmodels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 946–958, Online. Association for Computa-\ntional Linguistics.\nZihui Gu, Ju Fan, Nan Tang, Preslav Nakov, Xiao-\nman Zhao, and Xiaoyong Du. 2022. PASTA: Table-\noperations aware fact verification via sentence-table\ncloze pre-training. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 4971–4983, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nVivek Gupta, Riyaz A. Bhat, Atreya Ghosal, Manish\nShrivastava, Maneesh Singh, and Vivek Srikumar.\n2022a. Is my model using the right evidence? sys-\ntematic probes for examining evidence-based tabular\nreasoning. Transactions of the Association for Com-\nputational Linguistics, 10:659–679.\nVivek Gupta, Maitrey Mehta, Pegah Nokhiz, and Vivek\nSrikumar. 2020. INFOTABS: Inference on tables\nas semi-structured data. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2309–2324, Online. Association\nfor Computational Linguistics.\nVivek Gupta, Shuo Zhang, Alakananda Vempala, Yu-\njie He, Temma Choji, and Vivek Srikumar. 2022b.\nRight for the right reason: Evidence extraction for\ntrustworthy tabular reasoning. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n3268–3283, Dublin, Ireland. Association for Compu-\ntational Linguistics.\nHongwei Han, Jialiang Xu, Mengyu Zhou, Yijia Shao,\nShi Han, and Dongmei Zhang. 2022. LUNA: lan-\nguage understanding with number augmentations on\ntransformers via number plugins and pre-training.\nCoRR, abs/2212.02691.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: decoding-enhanced\nbert with disentangled attention. In 9th International\nConference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021 . OpenRe-\nview.net.\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas\nMüller, Francesco Piccinno, and Julian Eisenschlos.\n2020. TaPas: Weakly supervised table parsing via\npre-training. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4320–4333, Online. Association for Computa-\ntional Linguistics.\nDanqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin,\nand Wei-Ying Ma. 2016. How well do computers\nsolve math word problems? large-scale dataset con-\nstruction and evaluation. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 887–896,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nShotaro Ishihara. 2023. Training data extraction from\npre-trained language models: A survey. CoRR,\nabs/2305.16157.\nAashna Jena, Vivek Gupta, Manish Shrivastava, and\nJulian Eisenschlos. 2022. Leveraging data recasting\nto enhance tabular reasoning. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2022, pages 4483–4496, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2023. Survey of halluci-\nnation in natural language generation. ACM Comput.\nSurv., 55(12):248:1–248:38.\n15401\nZhihua Jin, Xin Jiang, Xingbo Wang, Qun Liu, Yong\nWang, Xiaozhe Ren, and Huamin Qu. 2021. Numgpt:\nImproving numeracy ability of generative pre-trained\nmodels. CoRR, abs/2109.03137.\nJeonghwan Kim, Giwon Hong, Kyung-min Kim, Junmo\nKang, and Sung-Hyon Myaeng. 2021. Have you\nseen that number? investigating extrapolation in\nquestion answering models. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 7031–7037, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nElida V . Laski and Robert S. Siegler. 2007. Is 27 a\nbig number? correlational and causal connections\namong numerical categorization, number line estima-\ntion, and numerical magnitude comparison. Child\nDevelopment, 78(6):1723–1743.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nZhenwen Liang, Jipeng Zhang, Lei Wang, Wei Qin,\nYunshi Lan, Jie Shao, and Xiangliang Zhang. 2022.\nMWP-BERT: Numeracy-augmented pre-training for\nmath word problem solving. In Findings of the Asso-\nciation for Computational Linguistics: NAACL 2022,\npages 997–1009, Seattle, United States. Association\nfor Computational Linguistics.\nBill Yuchen Lin, Seyeon Lee, Rahul Khanna, and Xiang\nRen. 2020. Birds have four legs?! NumerSense:\nProbing Numerical Commonsense Knowledge of Pre-\nTrained Language Models. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6862–6868,\nOnline. Association for Computational Linguistics.\nQian Liu, Bei Chen, Jiaqi Guo, Morteza Ziyadi, Zeqi\nLin, Weizhu Chen, and Jian-Guang Lou. 2022.\nTAPEX: table pre-training via learning a neural SQL\nexecutor. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net.\nLefteris Loukas, Manos Fergadiotis, Ilias Chalkidis,\nEirini Spyropoulou, Prodromos Malakasiotis, Ion\nAndroutsopoulos, and Georgios Paliouras. 2022.\nFiNER: Financial numeric entity recognition for\nXBRL tagging. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 4419–4431,\nDublin, Ireland. Association for Computational Lin-\nguistics.\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu,\nSong-Chun Zhu, Tanmay Rajpurohit, Peter Clark,\nand Ashwin Kalyan. 2023. Dynamic prompt learning\nvia policy gradient for semi-structured mathematical\nreasoning. In International Conference on Learning\nRepresentations (ICLR).\nJakub Macina, Nico Daheim, Sankalan Pal Chowdhury,\nTanmay Sinha, Manu Kapur, Iryna Gurevych, and\nMrinmaya Sachan. 2023. Mathdial: A dialogue tutor-\ning dataset with rich pedagogical properties grounded\nin math reasoning problems. CoRR, abs/2305.14536.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ramakanth Pasunuru, Roberta\nRaileanu, Baptiste Rozière, Timo Schick, Jane\nDwivedi-Yu, Asli Celikyilmaz, Edouard Grave, Yann\nLeCun, and Thomas Scialom. 2023. Augmented\nlanguage models: a survey. CoRR, abs/2302.07842.\nMatteo Muffo, Aldo Cocco, and Enrico Bertino. 2022.\nEvaluating transformer language models on arith-\nmetic operations using number decomposition. In\nProceedings of the Thirteenth Language Resources\nand Evaluation Conference, LREC 2022, Marseille,\nFrance, 20-25 June 2022, pages 291–297. European\nLanguage Resources Association.\nAakanksha Naik, Abhilasha Ravichander, Carolyn Rose,\nand Eduard Hovy. 2019. Exploring numeracy in\nword embeddings. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3374–3380, Florence, Italy. Asso-\nciation for Computational Linguistics.\nDarko Odic and Ariel Starr. 2018. An introduction to\nthe approximate number system. Child Development\nPerspectives, 12(4):223–229.\nKuntal Kumar Pal and Chitta Baral. 2021. Investigating\nnumeracy learning ability of a text-to-text transfer\nmodel. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2021, pages 3095–3101,\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nAnkur Parikh, Xuezhi Wang, Sebastian Gehrmann, Man-\naal Faruqui, Bhuwan Dhingra, Diyi Yang, and Dipan-\njan Das. 2020. ToTTo: A controlled table-to-text\ngeneration dataset. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 1173–1186, Online. As-\nsociation for Computational Linguistics.\nSungjin Park, Seungwoo Ryu, and Edward Choi. 2022.\nDo language models understand measurements? In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2022 , pages 1782–1792, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080–2094, Online.\nAssociation for Computational Linguistics.\n15402\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 2463–2473, Hong Kong, China. Association\nfor Computational Linguistics.\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot numerical reasoning. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2022, pages 840–854, Abu Dhabi,\nUnited Arab Emirates. Association for Computa-\ntional Linguistics.\nAbhilash Shankarampeta, Vivek Gupta, and Shuo\nZhang. 2022. Enhancing tabular reasoning with pat-\ntern exploiting training. In Proceedings of the 2nd\nConference of the Asia-Pacific Chapter of the Asso-\nciation for Computational Linguistics and the 12th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 706–726,\nOnline only. Association for Computational Linguis-\ntics.\nSheng Shen, Le Hou, Yanqi Zhou, Nan Du, Shayne\nLongpre, Jason Wei, Hyung Won Chung, Barret\nZoph, William Fedus, Xinyun Chen, Tu Vu, Yuexin\nWu, Wuyang Chen, Albert Webson, Yunxuan Li, Vin-\ncent Zhao, Hongkun Yu, Kurt Keutzer, Trevor Dar-\nrell, and Denny Zhou. 2023. Flan-moe: Scaling\ninstruction-finetuned language models with sparse\nmixture of experts. CoRR, abs/2305.14705.\nAvijit Thawani, Jay Pujara, and Filip Ilievski. 2021a.\nNumeracy enhances the literacy of language models.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n6960–6967, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nAvijit Thawani, Jay Pujara, Filip Ilievski, and Pedro\nSzekely. 2021b. Representing numbers in NLP: a\nsurvey and a vision. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 644–656, Online. As-\nsociation for Computational Linguistics.\nShyam Upadhyay and Ming-Wei Chang. 2017. An-\nnotating derivations: A new evaluation strategy and\ndataset for algebra word problems. In Proceedings\nof the 15th Conference of the European Chapter of\nthe Association for Computational Linguistics: Vol-\nume 1, Long Papers, pages 494–504, Valencia, Spain.\nAssociation for Computational Linguistics.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know num-\nbers? probing numeracy in embeddings. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5307–5315, Hong\nKong, China. Association for Computational Linguis-\ntics.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nJemma Catherine Whyte and Rebecca Bull. 2008. Num-\nber games, magnitude representation, and basic num-\nber skills in preschoolers. Developmental Psychol-\nogy, 44(2):588–596.\nJialiang Xu, Mengyu Zhou, Xinyi He, Shi Han, and\nDongmei Zhang. 2022. Towards robust numerical\nquestion answering: Diagnosing numerical capabili-\nties of NLP systems. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 7950–7966, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nPeng-Jian Yang, Ying-Ting Chen, Yuechan Chen, and\nDaniel Cer. 2021. Nt5?! training T5 to perform\nnumerical reasoning. CoRR, abs/2104.07307.\nYunhu Ye, Binyuan Hui, Min Yang, Binhua Li, Fei\nHuang, and Yongbin Li. 2023. Large language mod-\nels are versatile decomposers: Decompose evidence\nand questions for table-based reasoning. CoRR,\nabs/2301.13808.\nPengcheng Yin, Graham Neubig, Wen-tau Yih, and Se-\nbastian Riedel. 2020. TaBERT: Pretraining for joint\nunderstanding of textual and tabular data. InProceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 8413–8426, On-\nline. Association for Computational Linguistics.\nXikun Zhang, Deepak Ramachandran, Ian Tenney,\nYanai Elazar, and Dan Roth. 2020. Do language\nembeddings capture scales? In Findings of the Asso-\nciation for Computational Linguistics: EMNLP 2020,\npages 4889–4896, Online. Association for Computa-\ntional Linguistics.\nYilun Zhao, Linyong Nan, Zhenting Qi, Rui Zhang,\nand Dragomir Radev. 2022. ReasTAP: Injecting ta-\nble reasoning skills during pre-training via synthetic\nreasoning examples. In Proceedings of the 2022 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 9006–9018, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nBen Zhou, Qiang Ning, Daniel Khashabi, and Dan Roth.\n2020. Temporal common sense acquisition with min-\nimal supervision. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 7579–7589, Online. Association for\nComputational Linguistics.\n15403\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao\nWang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-\nSeng Chua. 2021. TAT-QA: A question answering\nbenchmark on a hybrid of tabular and textual con-\ntent in finance. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 3277–3287, Online. Association for\nComputational Linguistics.\nA Insights\nMain Insights. We investigated the language mod-\nels and found that LLMs like FlanT5 and GPT3.5\nperform better than other models on various numer-\nical reasoning tasks. When the labels are switched\naround and when dealing with negative values, we\nfound that both table-based and numerical models\nhad difficulty comprehending the data. In contrast,\nDeBERTa performs relatively well compared to\nmodels like LUNA and PASTA, which are tuned\nfor improved numerical reasoning skills.\nB Probe Statistics\nReasoning Type Count\nWord Problems 238\nSorting 379\nCounterfactual 1,000\nCurrency 1,014\nNegative 3,316\nRange 4,208\nScientific notation 6,274\nArithmetic 8,082\nOrdinal 10,569\nPercentage 16,851\nDate 18,642\nApproximation 20,440\nComparison 30,763\nNumeration 166,319\nTotal 288,095\nFlipped probes 77,687\nTable 5: Breakdown of probes per reasoning type.\nTable 2 gives an overview of probes per dataset.\nMost probes (i.e. 214, 440) are created from Tab-\nFact hypotheses as this is also the biggest dataset\navailable, followed by InfoTabs (19, 779). Table 5\nprovides a breakdown of probes per reasoning type.\nIn total, we have 286, 857 probes, of which 76, 404\nare label-flipping probes.\nIn the ideal scenario with counterfactual tables,\nthe models’ performance should be similar to the\nperformance on the original tables. However, we\nobserved that TAPAS and DeBERTa’s performance\nimproved significantly, which leads to the conclu-\nsion that models are biased toward one label.\nOverall no language model excels in all the nu-\nmerical reasoning tasks. Surprisingly, models per-\nform relatively well in complex tasks like Numeri-\ncal Word Problems but struggle at simple reasoning\ntasks like numeration and comparison.\n15404\nModel Table Specific Numerical Specific Large LMs\nTAPAS DeBERTa TAPEXNT5 LUNA PASTA ReasTAPFlanT5 GPT3.5 PaLM\nReasoning few zero few zero few zero\nRepresentation\nNumeration 19.02 61.69 74.83 64.19 63.57 85.43 75.35 64.59 67.58 77.6 61.83 76.38 70.76\nHeterogeneous 79.25 56.61 81.17 66.85 70.71 89.11 76.49 70.17 71.04 78.82 61.26 76.75 69.85\nNegative 59.38 69.81 97.01 51.77 24.66 70.44 81.04 70.8 20.37 85 87 92 88\nLabel Flipped\nNumeration 23 61.92 75.79 71.98 76.34 86.96 73.64 88.93 66.93 90.57 54.11 85.97 87.91\nHeterogeneous 67.66 53.12 82.96 62.94 77.57 83.75 68.37 87.48 51.55 89.18 65.32 86.55 79.14\nNegative 59.38 69.81 97.01 51.77 24.66 70.44 72.53 70.8 20.37 88 85 92 88\nNumber Sense\nScale 74.11 67.29 78.32 63.05 72.24 77.82 31.1 61.25 69.59 79.17 69 78.04 71.15\nComparison 68.43 68.7 54.12 66.85 69.19 84.75 58.46 37.77 23.87 83.31 50.58 77.43 63.78\nApproximation 75.49 64.65 71.64 63.3 67.09 86.08 58.66 62.28 78.09 75.13 59.62 76.88 75.35\nRange 70.84 57.53 94.99 58.14 48.86 73.25 87.77 80.2 20.09 90 85.5 80 81.41\nLabel Flipped\nScale 72.33 76.74 71.78 65.17 78.8 87 95.92 85.71 59.6 92.62 60.71 79.58 80.21\nComparison 53.55 70.75 38.29 68.64 69.76 85.74 42.69 86.12 49.66 96.05 37.34 82 54.86\nManipulation\nSorting 68.71 49.98 94.65 69.83 51.11 69.87 83.33 75.44 61.4 91.44 77 68.94 70.5\nArithmetic 71.15 58.01 95.92 58.83 73.6 73.6 88.97 78.92 22.42 89.5 85.5 83.5 81.5\nComplex Reasoning\nComplex 54.6 52.65 82.69 52.94 39.22 81.93 95.92 91.69 69.03 93 56.1 94 91.3\nCounterfactual 59.71 43.23 88.7 80.98 81.3 83.52 86.9 35.46 62.5 58.18 69.09 55.38 53.85\nTable 6: Results on original sets (average accuracy).\nModel Table Specific Numerical Specific Large LMs\nTAPAS DeBERTa TAPEXNT5 LUNA PASTA ReasTAPFlanT5 GPT3.5 PaLM\nReasoning few zero few zero few zero\nRepresentation\nNumeration 18.4 59.24 68.34 60.5 59.44 79.64 69.36 64.5 63.01 77.19 64.3 75.38 74\nHeterogeneous 72.76 54.52 75.75 62.19 63.73 80.59 74.32 68.92 67.52 77.36 65.78 75.5 70.22\nNegative 32 60.2 97.55 2.86 6.03 62.92 76.19 84.4 28.98 92 89 90 89\nLabel Flipped\nNumeration 10.13 43.68 39.41 33.89 29.82 53.3 41.05 17.34 70.04 56.83 73.37 52.54 45.91\nHeterogeneous 63.1 42.45 49.44 57.08 47.97 68.75 56.58 40.57 80.34 62.16 78.83 70.12 62.18\nNegative 20.9 40.8 2.9 42.52 43.61 20.72 41.28 11.38 55.63 32 86.87 18 19\nNumber Sense\nScale 71.68 49.43 68.38 60.39 62.21 73.63 67.23 53.12 62 80.46 66 75.15 59.39\nComparison 42.78 54.91 45.32 45.94 54.79 60.04 40.73 48.84 55.33 73.74 58.69 64.97 51.71\nApproximation 62.94 58.65 56.01 56.75 61.56 61.55 51.26 56.23 67.65 74.96 66.1 69.25 66.4\nRange 57.73 38.09 93.13 46.28 6.46 11.24 92.12 98.2 55.87 90.47 84.5 91.5 85\nLabel Flipped\nScale 39.26 44.24 38.91 44.55 28.63 43.12 98.46 23.69 77.67 56.31 66.94 60.93 52.57\nComparison 62.59 57.28 37.15 46.9 66.75 69.72 34.74 66.43 50.29 75.22 54.6 69.43 44.98\nManipulation\nSorting 44.8 63.75 8.47 53.89 77.79 64.54 13.29 10.03 76.75 38.75 72.81 46.38 42.31\nArithmetic 71.15 43.52 4.28 42.89 37.5 37.5 9.88 22.45 81.08 35.5 89.5 27 28.5\nComplex Reasoning\nComplex 89.22 56.3 16.39 51.24 55.46 40.28 3.4 9.38 41.42 37 72.78 25 22\nCounterfactual 86.21 67.24 77.8 31.23 82.1 78.43 78.1 57.27 70.14 81.82 72.73 72.31 80\nTable 7: Results on probed sets (average accuracy).\n15405",
  "topic": "Qualitative reasoning",
  "concepts": [
    {
      "name": "Qualitative reasoning",
      "score": 0.6866629123687744
    },
    {
      "name": "Computer science",
      "score": 0.676979124546051
    },
    {
      "name": "Inference",
      "score": 0.6393227577209473
    },
    {
      "name": "Verbal reasoning",
      "score": 0.6099385619163513
    },
    {
      "name": "Artificial intelligence",
      "score": 0.510550856590271
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.5071077346801758
    },
    {
      "name": "Reasoning system",
      "score": 0.5051067471504211
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.488887220621109
    },
    {
      "name": "Taxonomy (biology)",
      "score": 0.4809945821762085
    },
    {
      "name": "Focus (optics)",
      "score": 0.47577229142189026
    },
    {
      "name": "Model-based reasoning",
      "score": 0.47495365142822266
    },
    {
      "name": "Representation (politics)",
      "score": 0.4722805917263031
    },
    {
      "name": "Deductive reasoning",
      "score": 0.44882848858833313
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.44623735547065735
    },
    {
      "name": "Analytic reasoning",
      "score": 0.4300157427787781
    },
    {
      "name": "Natural language understanding",
      "score": 0.4170458912849426
    },
    {
      "name": "Natural language processing",
      "score": 0.3588216006755829
    },
    {
      "name": "Natural language",
      "score": 0.33122050762176514
    },
    {
      "name": "Cognition",
      "score": 0.20782402157783508
    },
    {
      "name": "Programming language",
      "score": 0.17210239171981812
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I183935753",
      "name": "King's College London",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I36788626",
      "name": "California University of Pennsylvania",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I223532165",
      "name": "University of Utah",
      "country": "US"
    }
  ]
}