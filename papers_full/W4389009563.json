{
  "title": "Trustworthiness of Children Stories Generated by Large Language Models",
  "url": "https://openalex.org/W4389009563",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3004595824",
      "name": "Prabin Bhandari",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2222096852",
      "name": "Hannah Brennan",
      "affiliations": [
        "George Mason University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3176456866",
    "https://openalex.org/W2946615199",
    "https://openalex.org/W4312091782",
    "https://openalex.org/W4368755092",
    "https://openalex.org/W4221055872",
    "https://openalex.org/W3168771811",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4366594193",
    "https://openalex.org/W2142498761",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4317553041",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W340923925",
    "https://openalex.org/W2790165607",
    "https://openalex.org/W4221022300",
    "https://openalex.org/W4307310980",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W1967390364",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4323655724",
    "https://openalex.org/W4386566857",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3100355250"
  ],
  "abstract": "Large Language Models (LLMs) have shown a tremendous capacity for generating literary text. However, their effectiveness in generating children's stories has yet to be thoroughly examined. In this study, we evaluate the trustworthiness of children's stories generated by LLMs using various measures, and we compare and contrast our results with both old and new children's stories to better assess their significance. Our findings suggest that LLMs still struggle to generate children's stories at the level of quality and nuance found in actual stories.",
  "full_text": "Proceedings of the 16th International Natural Language Generation Conference, pages 352–361\nSeptember 11–15, 2023. ©2023 Association for Computational Linguistics\n352\nTrustworthiness of Children Stories Generated by Large Language Models\nPrabin Bhandari\nDepartment of Computer Science\nGeorge Mason University\npbhanda2@gmu.edu\nHannah Marie Brennan\nDepartment of English, Linguistics Program\nGeorge Mason University\nhbrennan@gmu.edu\nAbstract\nLarge Language Models (LLMs) have shown\na tremendous capacity for generating literary\ntext. However, their effectiveness in generating\nchildren’s stories has yet to be thoroughly ex-\namined. In this study, we evaluate the trustwor-\nthiness of children’s stories generated by LLMs\nusing various measures, and we compare and\ncontrast our results with both old and new chil-\ndren’s stories to better assess their significance.\nOur findings suggest that LLMs still struggle to\ngenerate children’s stories at the level of quality\nand nuance found in actual stories.1\n1 Introduction\nAdvancements in pretrained large language mod-\nels (LLMs) like GPT-3 (Brown et al., 2020) and\nLLaMA (Touvron et al., 2023), have made it easier\nto generate natural language text for a variety of\ndownstream tasks, including generating narrative\ntext like children’s stories. The ability to gener-\nate natural text using LLMs has seen substantial\nimprovement with the innovation of instruction-\nfollowing models like InstructGPT (Ouyang et al.,\n2022) and Alpaca (Taori et al., 2023), resulting in\na better alignment with user intentions.\nThese systems are being used as a general-\npurpose chat-bots by the general public. As these\nmodels are integrated more into everyday applica-\ntions, it is crucial to continuously evaluate LLMs’\nperformance to ensure that they are indeed trust-\nworthy and accurate.\nTrustworthiness in the case of LLMs is a broad\nterm that refers to reliability and confidence in the\ngenerated text outputs along with their suitability\nfor a specific downstream task. A trustworthy LLM\nminimizes errors, biases, and potentially harmful\n1Code and dataset are publicly available:\nhttps://github.com/prabin525/trustworthiness-of-children-\nstories-generated-by-LLMs\ncontent while consistently producing clear and con-\ntextually suitable text. With the advancing capabil-\nities of LLMs, concerns regarding their trustwor-\nthiness have arisen. Notably, they are being used\nmore frequently to support creative writing (Clark\net al., 2018), raising concerns about the generation\nof inappropriate or offensive text (Price, 2016) and\nbiased content (Lucy and Bamman, 2021a). One\ndomain in which trustworthiness is of particular\nimportance is text generation intended for children.\nThis paper seeks to evaluate the trustworthiness\nof children’s stories generated by LLMs including\ngenerative LLMs and instruction following models.\nIn the case of text generation geared towards chil-\ndren, LLMs’ ability to generate age-appropriate\nmaterials to target audiences also becomes a vital\naspect of overall trustworthiness.\nTo assess the trustworthiness of LLMs in gen-\nerating children’s stories, we use two open-source\nfoundation language models, OPT (Zhang et al.,\n2022) and LLaMA (Touvron et al., 2023), along\nwith an instruction-following model Alpaca (Taori\net al., 2023) to generate children’s stories. Then,\nwe compare these generated stories against actual\nchildren’s stories, old and modern. Our assessment\ntakes into account a number of aspects, including\nstatistics derived from the text like the Flesch read-\ning ease score (Flesch, 1948), toxicity present in\nthe text, the most influential topics present in the\ntext, and the sentence structure of these texts.\nOur findings reveal that LLMs lack a high level\nof trustworthiness when tasked with generating\nchildren’s stories. While the generated children’s\nstories do share similarities in topics and patterns\nwith the actual stories (mostly modern ones), they\nare also susceptible to generating toxic content.\nMoreover, LLMs struggle to capture the intrica-\ncies and nuances of children’s literature, evident\nfrom the disparity in sentence structure between\nthe generated and actual stories.\n353\n2 Related Work\n2.1 Story Generation\nRecently, LLMs have been increasingly used to\nsupplement creative writing efforts for entertain-\nment and social media. Applications include work\nrelated to narrative generations (Sun et al., 2023; Si-\nmon and Muise, 2022; Razumovskaia et al., 2022;\nXiang et al., 2018). Yuan et al. (2022) tested Word-\ncraft, a tool created to assist writers with story gen-\neration using LLMs. In their study, writers who\nwere tasked with working with the AI agent noted\nthat Wordcraft lacked content awareness and would\ncreate grammatical stories with nonsensical topics\nor plots.\n2.2 Children and AI\nAI and LLMs have also been applied to contexts in-\nvolving children. Researchers at MIT had children\nwork with social robots to evaluate how much the\nchildren could learn through activities involving\nrobots (Williams, 2019). There is much discussion\non how to integrate AI into early childhood educa-\ntion (Yang, 2022; Kasneci et al., 2023). With the\nincreasing use of AI by and around children, there\nis an urgent need for more thorough evaluations of\nLLMs and the appropriateness of generated content\nfor vulnerable audiences.\n2.3 Trustworthiness Testing\nChiang and Lee (2023) investigated whether LLMs\ncan replace humans in evaluating texts. Specifi-\ncally, they looked at open-ended story generation\nand adversarial attacks. They found that there were\nsimilar ratings between LLMs and human evalua-\ntors. Venkit et al. (2023) found that unbalanced\nsources of training data result in biased generations\nin GPT-2, and proposed strategies to reduce bias\nusing adversarial triggers. Tang et al. (2022) pre-\nsented EtriCA, a neural generation model which\naims to remedy issues of relevance and coherence\nof generated texts. Lucy and Bamman (2021b) stud-\nied the bias existing in GPT-3’s generated stories.\nGuo et al. (2023) have proposed a similar study\nspecifically testing how similar text generated by\nChatGPT is to text produced by human writers.\n3 Methodology\nTo investigate the trustworthiness of children’s sto-\nries generated by LLMs, we compare them with\nactual old and modern children’s stories. We col-\nlect a diverse set of stories from different sources,\nincluding both older stories such as folktales, and\nmore recent children’s stories. We use both LLMs\nand instruction-following models to generate sto-\nries with different prompt lengths and instruction\ntemplates. As story generation is an open-ended\nproblem with no reference text, we rely on other\nmetrics instead of any automatic measure of eval-\nuation like BARTScore (Yuan et al., 2021) or\nBERTScore (Zhang et al., 2019). We use various\nmetrics to compare the generated stories with actual\nstories, including in-text statistics such as sentence\nlength and a measure of toxicity in the text, as well\nas an evaluation of topics covered in these stories.\nFurthermore, we analyze and compare the gram-\nmatical structures of the stories using dependency\nstructures extracted from both the original and the\nartificially generated stories.\nIn the following section, we describe the exper-\nimental setup, including details on the collected\ndata, the story generation process, and the evalu-\nation metrics used for comparison. Subsequently,\nwe present the results obtained from our experi-\nmentation.\n4 Experiments\n4.1 Data\nOur data consists of 132 original children’s stories\ncollected from various online sources and catego-\nrized into two categories: old and modern. The\nold stories generally include traditional children’s\nstories like folktales and fairy tales, whereas the\nmodern stories include more recent children’s lit-\nerature published after the year 2000. Both sets of\noriginal children’s stories are comprised of English\ntexts aimed at children between the ages of three\nand thirteen, with both data sets representing the\nfull range of these target ages. Overall, 122 are\nclassified as old stories, and the remaining 10 as\nmodern stories. Specifically, the older stories were\nobtained via Project Gutenberg,2 and the modern\nstories from various online platforms.3 We use the\nold stories as a reference for the story generation\ntask and compare the generated stories against both\nold and modern stories.\n4.2 Story generation\nWe generate stories using language models and an\ninstruction-following model.\n2https://www.gutenberg.org/\n3https://www.freechildrenstories.com/,\nhttps://monkeypen.com/\n354\nModel Prompt Length Count\nOPT First Sentence (OPT-Line) 610\nFirst 256-tokens (OPT-256) 610\nFirst 512-tokens (OPT-512) 610\nLLaMA First Sentence (LLaMA-Line) 610\nFirst 256-tokens (LLaMA-256) 610\nFirst 512-tokens (LLaMA-512) 610\nTotal 3660\nTable 1: Breakdown of the stories generated using\nLLMs.\nLanguage Models Our story generation task us-\ning LLMs uses two foundational language models:\nOPT (Zhang et al., 2022) and LLaMA (Touvron\net al., 2023), with model sizes of 6.7 billion and 7\nbillion parameters, respectively. To generate sto-\nries, we provide a portion of each old story as con-\ntext for the LLMs. Specifically, we use the first\nsentence, the first 256 tokens, and the first 512 to-\nkens of each old story as a prompt. We use top-k\nsampling-based decoding with k set to 100 and\ngenerate five samples for each prompt, resulting in\na total of 3660 generated stories. The breakdown\nof the generated stories along with the length of the\nprompt is given in Table 1.\nInstruction-following Models For instruction-\nfollowing story generation, we use Alpaca (Taori\net al., 2023), which is an instruction-following\nmodel that is based on the LLaMA architecture\nand is fine-tuned using self-instruct (Wang et al.,\n2022). We use the Alpaca model based on the 7B\nvariant of the LLaMA model. We use four differ-\nent instruction templates to generate stories, two\nof which require a story title as input and two of\nwhich do not. For the templates that require a story\ntitle, we use the title of old stories as input. The\ntemplates are provided in Table 2. To generate sto-\nries, we use top-k sampling-based decoding with k\nset to 100 and generate five samples for each tem-\nplate, resulting in a total of 2440 generated stories\nwith 610 stories per template.\n4.3 In-text statistics\nWe compare various statistics derived from the\ntext of the generated stories against those of ac-\ntual stories. Specifically, we use two metrics: sen-\ntence length and Flesch reading ease score (Flesch,\n1948).\nFlesch Reading Ease Score The Flesch read-\ning ease score (FRES) measures the readability of\na text and is based on two factors: average sen-\ntence length and the average number of syllables\nper word. It provides a score between 0 and 100,\nwith higher scores indicating easier readability. A\nFlesch reading ease score above 60 for a text indi-\ncates that it can easily be read by children up to the\nage of 15. The formula for calculating the FRES\nof a text is shown in Equation 1.\nFRES = 206.835 − 1.015\n\u0012 total words\ntotal sentence\n\u0013\n− 84.6\n\u0012total syllables\ntotal words\n\u0013\n(1)\n4.4 Toxicity of text\nGehman et al. 2020 found that the LLMs can gener-\nate ‘toxic’ text from a very innocuous prompt and\nattribute this to a significant amount of offensive,\nfactually unreliable, and otherwise toxic content\nin the training data of these models. We want to\ninvestigate the level of toxicity in our generated\nchildren’s stories. Ideally, generated children’s sto-\nries should be free of any toxic text.\nWe use Detoxify (Hanu and Unitary team, 2020),\na BERT (Devlin et al., 2019) based toxic text de-\ntector, to identify the presence of toxic text in the\ngenerated children’s stories. Detoxify generates\nscore labels in the range of 0 to 1, assessing the\ntoxicity of the text based on categories such as\ntoxic, severely toxic, obscene, threat, insult, and\nidentity hate. Specifically, we use detoxify for each\nsentence of our actual and generated stories to get\ntoxicity measures across the six categories.\n4.5 Topic Modeling\nWe also analyze the data for topic modeling using\npyLDAvis (Tran, 2022). We compare the topics\nfound in the data set of older stories with the LLM-\ngenerated stories. The older stories and the modern\nstories are also compared to assess whether there\nhas been a shift in topics over time that would\npotentially influence topic properties in the LLM-\ngenerated stories. A probable diachronic shift in\ntopics of stories geared towards young audiences\nalso highlights the need to test the toxicity of gen-\nerated stories, as seen in the previous section.\nTo avoid uninformative topics, the data is pre-\nprocessed to remove stopwords and names. All\ntexts are categorized for specific topics using word\nclustering for a set of documents. Modeling is per-\nformed automatically without a predefined list of\n355\nS.N. Template\nT1 Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\n### Instruction:\nWrite a short children’s story given the title.\n### Input:\nTITLE\n### Response:\nT2 Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nWrite a short children’s story.\n### Response:\nT3 Below is an instruction that describes a task, paired with an input that provides further context.\nWrite a response that appropriately completes the request.\n### Instruction:\nWrite a children’s story given the title.\n### Input:\nTITLE\n### Response:\nT4 Below is an instruction that describes a task. Write a response that appropriately completes the request.\n### Instruction:\nWrite a children’s story.\n### Response:\nTable 2: Templates used by Alpaca for story generation.\nlabels. The visualizations using pyLDAvis break\ndown the topics based on the 122 older stories, the\n10 modern stories, and the generated stories from\nOPT, LLaMA, and Alpaca.\n4.6 Sentence structure\nThe structure of the sentences within a text can\nreveal the type or genre of the text. To analyze sen-\ntence structures, we construct a dependency tree for\neach sentence in both the original and generated\nchildren’s stories. The dependency tree depicts\nthe syntactic dependencies between the words in\na sentence, effectively capturing the grammatical\nstructure of the sentence. We then convert these\ndependencies into unlabeled directed graphs, pre-\nserving sentence structure while removing specific\nwords. We then generate the Weisfeiler Lehman\ngraph hash (Shervashidze et al., 2011) for each\ngraph. The Weisfeiler Lehman hashes are identical\nfor isomorphic graphs and strongly guarantee that\nnon-isomorphic graphs will get different hashes.\nWe compare the frequency of hashes to evaluate\nthe similarity between the sentence structure of the\ngenerated stories and the actual stories.\n5 Generated stories follow modern trends\nbut struggle with nuances\nFigure 1 shows the box plot of sentence lengths for\nold and modern original stories, as well as for the\ngenerated stories. Being literary texts, children’s\nstories do not strictly confine to formal English con-\nventions and many contain sentences with higher\nword counts; so for clarity, we removed all the out-\nliers from the plot. One interesting observation\nis that modern children’s stories generally have\nshorter sentence lengths than older children’s sto-\nries, which adheres to previous research that shows\na trend of decreasing sentence length in print (Haus-\nsamen, 1994). The generated stories from OPT and\nLLaMA show an increase in sentence length as the\nprompt length increases. We hypothesize that these\nmodels learn the pattern of larger sentence length\nfrom the older stories used as context, which is then\nreflected in the generated text. However, stories\n356\nFigure 1: Comparison of sentence length in generated\nchildren’s stories and actual children’s stories. The gen-\nerated children’s stories exhibit shorter sentence lengths\ncompared to the older original stories but are similar\nin sentence length to modern stories. Language mod-\nels prompted with older stories tend to generate longer\nsentences following the patterns of the context that had\nbeen provided.\ngenerated using the instruction-following model\nAlpaca, have sentence lengths similar to modern\nactual stories, indicating that language models may\nhave been trained mostly on the newer text, and\ntend to generalize modern trends when instructed\nto generate text of a specific type.\nThe Flesch reading ease score is a statistical mea-\nsure of the readability of a text and was optimized\nto be general enough at the time of its formulation,\nas can be seen with the constant values in equa-\ntion 1. That is why, we may find FRES values not\nwithin the range of 0 to 100 as seen in Figure 2a.\nWe also removed the outliers from the box plot in\nFigure 2a. Since we are not interested in exact val-\nues but in the general trend these values represent,\nwe use the FRES values in the range of 0-100 and\nshow their box plot in Figure 2b.\nOur results from the Flesch reading ease score\nreveal several interesting observations. Firstly, we\nsee that modern children’s stories have a higher\nFRES than older stories, meaning that the mod-\nern ones are easier to read. This can be attributed\nto the fact that sentences are getting shorter and\nmight have to do with simpler word selection. Sec-\nondly, we see that LLMs prompted with older sto-\nries tend to follow the pattern of the context and\ngenerate stories that are more difficult to read, as\nthe context length increases. Finally, we see that\nthe instruction-following model Alpaca generates\nstories that are easier to read compared to older\noriginal children’s stories but are not as readable as\n(a) FRES on all data (over 100 is undefined).\n(b) FRES limited to well-defined 0-100 range.\nFigure 2: Comparison of FRES in generated children’s\nstories and actual children’s stories : (a) FRES with all\ndata and (b) FRES only in the range of 0 and 100. The\ngenerated children’s stories are easier to read compared\nto older actual stories but are not as easy as modern\noriginal stories. Language models prompted with older\nstories tend to generate text that is more difficult to read,\nlikely because they follow the patterns in the prompts.\nmodern children’s stories. We posit that this obser-\nvation can be attributed to the fact that LLMs used\nin our study are generic models, and the instruction\nfollowing model is also only fine-tuned for gen-\neral instructions rather than instructions specific to\nchildren’s story generation.\nOverall, we see that modern children’s stories\nare easier to read than older children’s stories. As\nmost of the training data for LLMs comes from\nnewer text, the model tends to follow the trend of\nmodern children’s stories in their generated text for\nsentence length and word selection. However, it\nshould be noted that these models are not fine-tuned\nfor children’s stories generation, and therefore may\nnot capture the nuances of children’s stories re-\nsulting in stories that might be difficult to read for\nintended readers.\n357\n(a) toxic\n (b) severe toxic\n (c) obscene\n(d) threat\n (e) insult\n (f) identity hate\nFigure 3: Various toxicity measures for the actual and generated stories. Each cell in a subplot represents the\npercentage of sentences rated on a toxicity scale, with x-axis values indicating the toxicity level. Values for ratings\nin the range of 0-0.1 have been omitted from the plots for clarity.\n6 Generated stories may contain toxic\ntext\nOur analysis of toxicity in actual and generated\nstories reveals several noteworthy findings. We\npresent the toxicity measures for both actual and\ngenerated stories in Figure 3. Notably, we find that\nolder stories tend to be more toxic than modern\nstories across all toxicity measures. This trend is\nnot solely due to the smaller sample size of modern\nactual stories, as we have normalized the toxicity\nratings to ensure an accurate comparison. Rather,\nit suggests that writers are becoming more mindful\nof the language they use in children’s literature.\nAlthough modern stories are less toxic compared to\nolder stories, we still observe some level of toxicity\nin them. This toxicity in modern actual stories\nis often related to the narrative of the story. For\nexample, threats and insults might be needed for\nsome stories, but identity hate is not appropriate\nfor children’s stories. It is noteworthy that modern\nstories do not have toxic text related to identity hate\nbut older stories do.\nSimilar to our previous observation, we see that\nLLMs tend to learn patterns from the context they\nare provided with. As evident from the stories gen-\nerated by OPT and LLaMA, we see that the toxicity\naligns with older stories and gradually increases\nwith an increase in the length of the context. The\nstories generated using the instruction-following\nmodel Alpaca tend to be less toxic and mostly re-\nsemble modern stories. However, stories generated\nusing the T1 and T3 templates have a lot of obscene\ntext compared to stories generated using T2 and\nT4, which have none. As shown in Table 2, T1 and\nT3 take the title as input whereas T2 and T4 do\nnot. It is possible that the model remembered the\nstory title and generalized the patterns of the story\nor generalized to some other text in the template,\nleading to the generation of obscene text. This\nfinding is consistent with Gehman et al. 2020, who\nsuggest that children’s stories generated by LLMs\ncan contain highly toxic text despite an innocuous\nprompt.\nOur analysis of toxicity in original and generated\nstories reveals that older stories tend to be more\ntoxic than modern ones, that LLMs can learn toxic\npatterns from context leading to the generation of\ntoxic text, and that LLMs can even generate toxic\ntext from a very innocuous prompt. These findings\nsuggest that further work is needed to make LLMs\nuseful as tools for generating age-appropriate chil-\ndren’s literature.\n358\n7 Generated stories share main topics\nwith original stories\nAfter preprocessing the data, the original stories\nwere found to have four major topics. All of the\ntopics tended to share the existence of some small\ncharacter. The first topic mentions elements such\nas time, goodness, and greatness, and the presence\nof words like head, round, night, and water likely\nindicate specific scenes or settings within the narra-\ntive. The second topic contained new elements like\na prince, the color white, a girl, and eyes. These\nadditional keywords suggest different perspectives\nwithin the overarching narrative. The third topic\nintroduces elements like a house and a heart. Like\nthe previous topics, it shares mentions of a little\ncharacter, time, goodness, and a prince. The differ-\nence between ’house’ and ’heart’ could indicate a\nchange in the setting or moral of the narrative. The\nlast topic introduces new elements of wolf, peo-\nple, eyes, and a mother. These keywords might\nsuggest narratives that introduce new characters\nand themes. Overall, these topics provide insight\ninto the underlying themes present in the older 122\nstories in the data set. The topics revolve around\nnarratives involving a small character, time, good-\nness, and various other elements such as princes,\nnights, water, girls, and wolves.\nComparatively, the topics of the generated sto-\nries obtained from OPT, LLaMA, and Alpaca show\nminor differences. The first topic suggests a nar-\nrative that involves characters like kings, mothers,\nprinces, and princesses. It also mentions elements\nof time, goodness, greatness, and shadow. The\nprince, princess, and shadow hint at the fairy tale\nor fantasy theme. The second topic shares similari-\nties with the previous topic, with a focus on little,\nprince, time, goodness, and greatness, but it also\nintroduces new elements like eyes, houses, heads,\nand the color white. These additions suggest differ-\nent scenes, perhaps removed from the monarchy or\ncastle theme, and suggest a different narrative. The\nthird topic seems to center around family dynamics,\nwith mentions of mothers, fathers, and children. It\nalso includes keywords relating to time, goodness,\nnight, and poverty. This suggests a change in the\nnarrative away from the fantasy-focused topic. The\nlast topic includes keywords like little, time, and\ngoodness. It includes elements of fathers, eyes, and\nhouses. The presence of ’long’ and ’night’ suggests\na different tone or atmosphere within the narrative.\nThese general results show remarkable similarity\nFigure 4: Comparison of topics in generated children’s\nstories and actual children’s stories. The plot shows\nthat the most shared topics (x:2, y:2) include ’white’,\n’world’, ’great’, ’water’, ’black’, ’house’, ’little’, ’king’,\n’called’, and ’good’. The least shared topics (x:1, y:1)\ninclude ’heart’, ’head’, ’poor’, ’house’, ’looking’, ’chil-\ndren’, ’good’, ’young’, ’lady’, and ’night’.\nwith the data set on which the LLMs were trained.\nThe topics revolve around narratives involving char-\nacters such as kings, princes, mothers, fathers, and\nchildren. The topics also touched upon topics of\ntime, goodness, greatness, poverty, and setting ele-\nments of houses, nights, and the color white.\nAs with toxic content testing, we ran topic mod-\neling for a small number (10) of modern stories in\norder to compare the general topics that are cur-\nrently aimed at children. The first topic includes\nkeywords related to spatial orientation (right, in-\nside, door, left), objects (ream, head, frog), time,\nand actions (started). The keyword ‘eyes’ may sug-\ngest a focus on visual perception or observation.\nThe second topic emphasizes time, objects (ream,\ndoor), spacial orientation (right, inside), a frog, a\nhead, fairies, and ‘need’. The presence of fairies\nintroduces a fantastical or imaginative element to\nthe topic. The third topic revolves around time,\nspacial orientation (right, inside, door), physical\nattributes (head, eyes, long, hand), and a frog. The\ninclusion of ‘long’ might suggest a temporal or\nduration-related aspect. The last topic highlights\ntime, spatial orientation (right, inside, door), ob-\njects (ream, frog), physical attributes (head, eyes,\nsmall), and the action of starting something. The\nmodern stories’ topic modeling results suggest a re-\n359\ncurring theme involving concepts such as time, spa-\ntial orientation, objects, and actions. Each topic em-\nphasizes different aspects and introduces additional\nelements like fairies or physical attributes. Figure 4\nrepresents the level of similarity and difference be-\ntween the real stories and the LLM-generated sto-\nries. There are greater similarities between these\nstories than there appear to be differences in the\nmain topics.\nAs expected, the results of the topic modeling\nshowed similarities between the original 122 sto-\nries in the training corpus and the stories gener-\nated by the LLMs. These stories shared fairy tale\nand fantasy elements as well as topics of good-\nness, greatness, time, and setting elements of night,\nhouses, and the color white. Once we compare this\nwith the modern stories, we see that the focus of the\nsmall data set we have is similarly focused on time\nand fairies, but has more topics relating to spatial\norientation. We are likely seeing a change in the\ncontent of stories written for children. With only\nten modern stories, we cannot reliably generalize\nover all stories, but we noticed tendencies such as\nthat the modern story set did tend to involve more\novertly educational elements aimed at younger age\ngroups when compared to the older stories.\n8 Generated stories do not have similar\nsentence structure to original stories\nTable 3 shows the percentage of overlapping Weis-\nfeiler Lehman hashes between the dependency tree\ngraphs of sentences generated by various models\nand those actual children’s stories, both old and\nmodern. We also got an overlap of 35.57 percent-\nage between old and modern actual stories, which\nis greater than all the values in Table 3. This shows\nthat the structure of sentences in children’s liter-\nature has changed over time, which supports our\nearlier findings that children’s literature has under-\ngone noticeable changes over time.\nAdditionally, we observe a higher percentage of\noverlap between old original stories and the stories\ngenerated by OPT and LLaMA, which again aligns\nwith our earlier findings that LLMs learn from their\ncontext. Furthermore, for the stories generated by\nOPT and LLaMA, we see an average overlap of\n30% with modern stories, which can be attributed\nto the fact that these models were trained on a\ndataset consisting of recent text.\nThe stories generated by Alpaca have a slightly\nhigher overlap with modern stories compared to\nModel Percentage overlap with\nOld stories Modern stories\nOPT-Line 34.82 34.21\nOPT-256 31.37 28.88\nOPT-512 32.49 29.89\nLLaMA-Line 34.23 33.64\nLLaMA-256 32.14 29.82\nLLaMA-512 32.27 30.73\nAlpaca: T-1 17.31 20.37\nAlpaca: T-2 14.67 17.52\nAlpaca: T-3 15.20 16.92\nAlpaca: T-4 15.41 17.84\nTable 3: Overlap of the hashes of the dependency tree\ngraph of the sentences in generated stories against old\nand modern actual stories.\nold stories, but the percentage overlap in sentence\nstructures is still relatively low ( ≤ 20%). Given\nthat the old and modern actual stories share around\n35% of the same sentence structures, we expected\nAlpaca’s generated stories to overlap more with\nmodern stories. But since Alpaca is a generic\nmodel fine-tuned for instruction-following and not\nsolely trained or fine-tuned on children’s literature,\nit seems plausible that it would not be capable of\nfully generalizing over sentence or grammatical\nstructures observable in children’s literature.\n9 Conclusion and Future Work\nOur study examines the trustworthiness of chil-\ndren’s stories generated by large language models.\nWhile these generated stories may share similar\ntopics and patterns with actual stories, they fail\nto capture all the nuances present in children’s lit-\nerature, and may even contain toxic material that\nis inappropriate for children. Based on our find-\nings, we conclude that LLMs are not yet appropri-\nate for generating high-quality children’s literature.\nMoving forward, we plan to extend our work by\nimplementing reinforcement learning with both au-\ntomatic and human feedback to improve the quality\nof LLM-generated children’s stories.\nAcknowledgments\nThis work was supported by resources provided\nby the Office of Research Computing, George Ma-\nson University (URL: https://orc.gmu.edu) and by\nthe National Science Foundation (Awards Num-\nber 1625039 and 2018631). Additionally, Prabin\n360\nBhandari has been partially supported by the Na-\ntional Science Foundation Grant No. IIS-2127901.\nOur sincere appreciation goes to Antonios Anasta-\nsopoulos and G´eraldine Walther for their valuable\nsuggestions and unwavering support throughout\nthe course of this research.\nReferences\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nCheng-Han Chiang and Hung-yi Lee. 2023. Can Large\nLanguage Models Be an Alternative to Human Eval-\nuations? ArXiv:2305.01937 [cs].\nElizabeth Clark, Anne Spencer Ross, Chenhao Tan,\nYangfeng Ji, and Noah A. Smith. 2018. Creative\nWriting with a Machine in the Loop: Case Studies on\nSlogans and Stories. In 23rd International Confer-\nence on Intelligent User Interfaces, pages 329–340,\nTokyo Japan. ACM.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nRudolph Flesch. 1948. A new readability yardstick.\nJournal of applied psychology, 32(3):221.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nBiyang Guo, Xin Zhang, Ziyuan Wang, Minqi Jiang,\nJinran Nie, Yuxuan Ding, Jianwei Yue, and Yupeng\nWu. 2023. How Close is ChatGPT to Human Ex-\nperts? Comparison Corpus, Evaluation, and Detec-\ntion. arXiv.org. Place: Ithaca Publisher: Cornell\nUniversity Library, arXiv.org.\nLaura Hanu and Unitary team. 2020. Detoxify. Github.\nhttps://github.com/unitaryai/detoxify.\nBrock Haussamen. 1994. The future of the english\nsentence. Visible language., 28(1).\nEnkelejda Kasneci, Kathrin Sessler, Stefan K¨uchemann,\nMaria Bannert, Daryna Dementieva, Frank Fischer,\nUrs Gasser, Georg Groh, Stephan G¨unnemann, Eyke\nH¨ullermeier, Stepha Krusche, Gitta Kutyniok, Tilman\nMichaeli, Claudia Nerdel, J¨urgen Pfeffer, Oleksandra\nPoquet, Michael Sailer, Albrecht Schmidt, Tina Sei-\ndel, Matthias Stadler, Jochen Weller, Jochen Kuhn,\nand Gjergji Kasneci. 2023. ChatGPT for good? On\nopportunities and challenges of large language mod-\nels for education. Learning and Individual Differ-\nences, 103:102274.\nLi Lucy and David Bamman. 2021a. Gender and rep-\nresentation bias in GPT-3 generated stories. In Pro-\nceedings of the Third Workshop on Narrative Un-\nderstanding, pages 48–55, Virtual. Association for\nComputational Linguistics.\nLi Lucy and David Bamman. 2021b. Gender and Rep-\nresentation Bias in GPT-3 Generated Stories. In\nProceedings of the Third Workshop on Narrative Un-\nderstanding, pages 48–55, Virtual. Association for\nComputational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nRob Price. 2016. Microsoft deletes racist, genocidal\ntweets from AI chatbot Tay - Business Insider.\nEvgeniia Razumovskaia, Joshua Maynez, Annie Louis,\nMirella Lapata, and Shashi Narayan. 2022. Little red\nriding hood goes around the globe:crosslingual story\nplanning and generation with large language models.\nNino Shervashidze, Pascal Schweitzer, Erik Jan\nVan Leeuwen, Kurt Mehlhorn, and Karsten M Borg-\nwardt. 2011. Weisfeiler-lehman graph kernels. Jour-\nnal of Machine Learning Research, 12(9).\nNisha Simon and Christian Muise. 2022. TattleTale -\nStorytelling with Planning and Large Language Mod-\nels.\nYuqian Sun, Ying Xu, Chenhang Cheng, Yihua Li,\nChang Hee Lee, and Ali Asadipour. 2023. Explore\nthe future earth with wander 2.0: Ai chatbot driven by\nknowledge-base story generation and text-to-image\nmodel. In Extended Abstracts of the 2023 CHI Con-\nference on Human Factors in Computing Systems ,\nCHI EA ’23, New York, NY , USA. Association for\nComputing Machinery.\nChen Tang, Chenghua Lin, Henglin Huang, Frank\nGuerin, and Zhihao Zhang. 2022. EtriCA: Event-\nTriggered Context-Aware Story Generation Aug-\nmented by Cross Attention. ArXiv:2210.12463 [cs].\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\n361\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth´ee Lacroix,\nBaptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023. Llama: Open and efficient foun-\ndation language models.\nKhuyen Tran. 2022. pyLDAvis: Topic Modelling Ex-\nploration Tool That Every NLP Data Scientist Should\nKnow.\nPranav Narayanan Venkit, Sanjana Gautam, Ruchi Pan-\nchanadikar, Ting-Hao ’Kenneth’ Huang, and Shomir\nWilson. 2023. Nationality bias in text generation.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa\nLiu, Noah A Smith, Daniel Khashabi, and Hannaneh\nHajishirzi. 2022. Self-instruct: Aligning language\nmodel with self generated instructions.\nKyle Williams. 2019. Neural lexicons for slot tagging\nin spoken language understanding. In Proceedings\nof the 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Indus-\ntry Papers), pages 83–89, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nJingwen Xiang, Zoie Zhao, Mackie Zhou, Megan\nMcKenzie, Alexis Kilayko, Jamie C Macbeth, Smith\nEdu, Smith Edu, Smith Edu, Smith Edu, Smith Edu,\nand Smith Edu. 2018. Interleaving a Symbolic Story\nGenerator with a Neural Network-Based Large Lan-\nguage Model.\nWeipeng Yang. 2022. Artificial Intelligence education\nfor young children: Why, what, and how in curricu-\nlum design and implementation. Computers and\nEducation: Artificial Intelligence, 3:100061.\nAnn Yuan, Andy Coenen, Emily Reif, and Daphne Ip-\npolito. 2022. Wordcraft: Story Writing With Large\nLanguage Models. In 27th International Conference\non Intelligent User Interfaces, IUI ’22, pages 841–\n852, New York, NY , USA. Association for Comput-\ning Machinery. Event-place: Helsinki, Finland.\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021.\nBartscore: Evaluating generated text as text gener-\nation. Advances in Neural Information Processing\nSystems, 34:27263–27277.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-\nberger, and Yoav Artzi. 2019. Bertscore: Evaluating\ntext generation with bert.",
  "topic": "Trustworthiness",
  "concepts": [
    {
      "name": "Trustworthiness",
      "score": 0.8579659461975098
    },
    {
      "name": "Quality (philosophy)",
      "score": 0.5620509386062622
    },
    {
      "name": "Contrast (vision)",
      "score": 0.4774087965488434
    },
    {
      "name": "Psychology",
      "score": 0.39709219336509705
    },
    {
      "name": "Computer science",
      "score": 0.3726048469543457
    },
    {
      "name": "Social psychology",
      "score": 0.27627867460250854
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2125956416130066
    },
    {
      "name": "Epistemology",
      "score": 0.11043599247932434
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}