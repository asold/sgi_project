{
    "title": "Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach",
    "url": "https://openalex.org/W3034552719",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A5042733935",
            "name": "Wenyu Du",
            "affiliations": [
                "Westlake University",
                "Institute for Advanced Study"
            ]
        },
        {
            "id": "https://openalex.org/A5024900991",
            "name": "Zhouhan Lin",
            "affiliations": [
                "Université de Montréal"
            ]
        },
        {
            "id": "https://openalex.org/A5073742611",
            "name": "Yikang Shen",
            "affiliations": [
                "Université de Montréal"
            ]
        },
        {
            "id": "https://openalex.org/A5102014720",
            "name": "Timothy J. O’Donnell",
            "affiliations": [
                "Canadian Institute for Advanced Research",
                "McGill University"
            ]
        },
        {
            "id": "https://openalex.org/A5086198262",
            "name": "Yoshua Bengio",
            "affiliations": [
                "Université de Montréal"
            ]
        },
        {
            "id": "https://openalex.org/A5100328212",
            "name": "Yue Zhang",
            "affiliations": [
                "Westlake University",
                "Institute for Advanced Study"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2962941914",
        "https://openalex.org/W1998871699",
        "https://openalex.org/W1502742190",
        "https://openalex.org/W1974037599",
        "https://openalex.org/W4288104054",
        "https://openalex.org/W1989705153",
        "https://openalex.org/W814218721",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2019165876",
        "https://openalex.org/W2114887620",
        "https://openalex.org/W2949399644",
        "https://openalex.org/W1722351164",
        "https://openalex.org/W2167723982",
        "https://openalex.org/W2949577583",
        "https://openalex.org/W2962808339",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2964030814",
        "https://openalex.org/W2932637973",
        "https://openalex.org/W4294555862",
        "https://openalex.org/W2963754491",
        "https://openalex.org/W2963266340",
        "https://openalex.org/W2739893875",
        "https://openalex.org/W4299838440",
        "https://openalex.org/W2743945814",
        "https://openalex.org/W2798727047",
        "https://openalex.org/W2167980204",
        "https://openalex.org/W4298168696",
        "https://openalex.org/W2963411763",
        "https://openalex.org/W2888799392",
        "https://openalex.org/W1818785862",
        "https://openalex.org/W1544225532",
        "https://openalex.org/W2015093644",
        "https://openalex.org/W1843916688",
        "https://openalex.org/W2963983719",
        "https://openalex.org/W2970378492",
        "https://openalex.org/W2973723395",
        "https://openalex.org/W2970554454",
        "https://openalex.org/W2553303224",
        "https://openalex.org/W2143331230",
        "https://openalex.org/W179875071",
        "https://openalex.org/W1996672843",
        "https://openalex.org/W2129285314",
        "https://openalex.org/W2267186426",
        "https://openalex.org/W2951672049",
        "https://openalex.org/W2963572611",
        "https://openalex.org/W2106484162",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W2963537482",
        "https://openalex.org/W4301420590",
        "https://openalex.org/W4289373464",
        "https://openalex.org/W4242682556",
        "https://openalex.org/W2949629417",
        "https://openalex.org/W2889260178",
        "https://openalex.org/W2791751435",
        "https://openalex.org/W2038248725",
        "https://openalex.org/W2105847779",
        "https://openalex.org/W2525332836",
        "https://openalex.org/W2212703438",
        "https://openalex.org/W2963073938",
        "https://openalex.org/W2096204319"
    ],
    "abstract": "It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called \"syntactic distances\", where information between these two separate objectives shares the same intermediate representation. Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality.",
    "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6611–6628\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n6611\nExploiting Syntactic Structure for Better Language Modeling:\nA Syntactic Distance Approach\nWenyu Du1,2∗, Zhouhan Lin3,4∗, Yikang Shen3,4, Timothy J. O’Donnell3,5,6,\nYoshua Bengio3,4 and Yue Zhang1,2\n1School of Engineering, Westlake University\n2Institute of Advanced Technology, Westlake Institute for Advanced Study\n3Mila 4Universit´e de Montr´eal\n5Department of Linguistics, McGill University 6Canada CIFAR AI Chair\nAbstract\nIt is commonly believed that knowledge of syn-\ntactic structure should improve language mod-\neling. However, effectively and computation-\nally efﬁciently incorporating syntactic struc-\nture into neural language models has been a\nchallenging topic. In this paper, we make use\nof a multi-task objective, i.e., the models si-\nmultaneously predict words as well as ground\ntruth parse trees in a form called “syntactic\ndistances”, where information between these\ntwo separate objectives shares the same inter-\nmediate representation. Experimental results\non the Penn Treebank and Chinese Treebank\ndatasets show that when ground truth parse\ntrees are provided as additional training sig-\nnals, the model is able to achieve lower per-\nplexity and induce trees with better quality.\n1 Introduction\nIt is widely believed in linguistics, cognitive sci-\nence, and computational linguistics that the la-\ntent structure underlying how words combine to\nform sentences is best represented as a tree struc-\nture. The study of the computational mechanisms\nand systems of constraints that characterize such\nderivations or parse trees is a central question in\nthese ﬁelds (Pollard and Sag, 1994; Steedman and\nBaldridge, 2011; Huddleston and Pullum, 2002;\nAdger, 2003; Bresnan, 2001; Chomsky, 1995; Sag\net al., 2003).\nUsing syntactic information for the language\nmodeling task has been a popular research topic\nsince the 1990s. Early efforts included various\napproaches that attempted to incorporate shallow\nsyntactic information such as POS tags (Heeman\nand Allen, 1997; Srinivas, 1996) as well as a more\ncomplete structures (Wright et al., 1994; Jurafsky\net al., 1995). Most of such work falls under the\ntopic of structured language modeling (Chelba and\n∗Equal contribution.\nJelinek, 2000; Van Uytsel et al., 2001; Xu et al.,\n2002). With the resurgence of neural network ap-\nproaches, sequential, large-scale neural language\nmodels have been shown to signiﬁcantly outper-\nform traditional language models (Merity et al.,\n2017; Yang et al., 2018) without using syntactic\nstructural information. On another scenario, recent\nanalysis also reveals that state-of-the-art sequential\nneural language models still fail to learn certain\nlong-range syntactic dependencies (Kuncoro et al.,\n2018). Thus it is an interesting problem to explore\nthe relation between language models and syntax\nand investigate whether syntax can be integrated to\nenhance neural language models.\nTo this end, two main lines of work have been\ninvestigated, namely transition-based and distance-\nbased methods, respectively. The former strand of\nwork has sought to jointly train a transition-based\nparser (Nivre, 2008; Zhang and Nivre, 2011; An-\ndor et al., 2016) with a language model using a\nlinearized structured sentence. For example, recur-\nrent neural network grammars (RNNGs) model the\njoint probability of both words and trees by training\na generative, top-down parser (Dyer et al., 2016;\nCheng et al., 2017). Subsequent work (Kim et al.,\n2019b) has developed an unsupervised variant of\nRNNGs based on an expectation maximization al-\ngorithm, which enables the system to be used as a\nlanguage model without access to parser data.\nThe second strand of work designs language\nmodels that are constrained using syntactic con-\nstituents induced using the notion of syntactic dis-\ntance (Shen et al., 2017, 2018). The distances are\na sequence of scalars between consecutive words,\nwhich are higher when there is a higher level of con-\nstituent boundary between the corresponding pair\nof words. While aligning nicely with the sequential\nnature of language models, syntactic distances can\nbe transformed into syntactic tree structures with\nsimple principles (Shen et al., 2017).\n6612\nThe major difference between the above two\nstrands of work is that the former focuses more\non parsing performance while the latter aligns bet-\nter to language model settings. There are three\nmain beneﬁts of the syntactic distance approach.\nFirst, typical engineering tricks for language mod-\neling such as batching and regularization (Merity\net al., 2017) can be directly used. Second, unlike\ntransition-based methods, which requires to model\neach sentence independently, distance-based mod-\nels allow direct comparison with mainstream prior\nwork on language modeling (Gal and Ghahramani,\n2016; Merity et al., 2017; Yang et al., 2018) on\nthe same datasets, which carry information across\nsentence boundaries. Third, there is no risk of\ncompounding errors as compared to the transition-\nbased approach. However, unlike for transition-\nbased approaches (Kim et al., 2019b), for distance-\nbased approaches there have been no studies exam-\nining the relationship between induced syntactic\nstructure and human labeled syntactic structure, or\nwhether human labeled syntactic trees can be used\nto improve language modeling (Dyer et al., 2016;\nKim et al., 2019b).\nTo this end, we investigate distance-based lan-\nguage models with explicit supervision. In par-\nticular, we inject syntactic tree supervision into\ndistance-based neural language models by breaking\na syntactic tree into a label sequence, and extend-\ning a distance-based language model to include a\nmulti-task objective that also learns to predict gold-\nstandard labels. We choose the Ordered-Neuron\nLSTM (ON-LSTM) (Shen et al., 2018) as our base-\nline model, which gives the best results among\ndistance-based models.\nFor making fair comparison with the dominant\nmethods on language modeling, we also manually\nextend the most commonly-used dataset for evaluat-\ning language models, which we name PTB-Concat\n(Mikolov et al., 2010). It is a version of the Penn\nTreebank (PTB) (Marcus et al., 1993) dataset with\nsyntactic trees removed, and with preprocessing\nof numbers, punctuation and singleton words. We\nadd syntactic trees, thus directly compare distance-\nbased methods with other language models.\nExperimental results show that incorporating lin-\nguistically motivated structures could practically\nimprove language modeling performance. To the\nbest of our knowledge, this is the ﬁrst work to suc-\ncessfully incorporate gold-standard syntactic trees\ninto syntactic distance based language models. Ad-\nditional experiments suggest that the level of im-\nprovement could also be achieved in other language\nmodels. Furthermore, analyses of the trees learned\nby the multi-task models demonstrate that they are\ndifferent from both gold trees and unsupervisedly\nlearned trees. 1\n2 Related Work\nUsing syntactic information for language modeling\ndates back to the last century. Srinivas (1996) pro-\nposed using shallow syntactic structures—so-called\n“super-tags”—which successfully reduced perplex-\nity by 38% over a tri-gram based word-level lan-\nguage model. More complete parser integration is\nalso explored under the heading of “structured lan-\nguage modeling” (Chelba and Jelinek, 2000). This\nresearch covers a wide range of different parsers, al-\nbeit mostly with N-gram models (Van Uytsel et al.,\n2001; Xu et al., 2002). Wright et al. (1994) and Ju-\nrafsky et al. (1995) extend bi-gram language mod-\nels with a context-free grammar. Feed-forward\nneural language models were also explored (Xu\net al., 2003). However, the performance does not\napproach that of the modern neural LMs.\nDyer et al. (2016) ﬁrst proposed RNNG. Sub-\nsequent work extends the model with an encoder-\ndecoder architecture (Cheng et al., 2017), unsu-\npervised learning (Kim et al., 2019b), knowledge-\ndistillation (Kuncoro et al., 2019) and computa-\ntional psycholinguistics (Hale et al., 2018). Shen\net al. (2017) ﬁrst used syntactic distance to con-\nstrain language modeling. Its subsequent work\n(Shen et al., 2018) transfers the distance notion\nto LSTM cell. Our work extends distance-based\nmethods in trying to introduce supervised syntax\nto these models. A very recent work makes use of\nattention over spans instead of syntactic distance\nto inject inductive bias to language models (Peng\net al., 2019). However, the time complexity of\ninjecting supervision is much higher than distance-\nbased approach (O(n2) VS O(n) ).\n3 Model\nThe overall structure of our model is shown in Fig-\nure 1. In particular, the ON-LSTM is taken as the\nbase language model, and syntactic trees are added\nby conversion to distance metrics. The supervised\ndistance values are taken as one additional output,\nresulting in a multi-view model.\n1We release the code at https://github.com/\nwenyudu/SDLM.\n6613\nLinear \nLayer \nh wt\ncumax \nL syd\nL lm\ncumax \nx t h t-1\nLinear \nLayer \nh ft\nFigure 1: Split-head approach of constructing the two master\nforget gates in the multi-task setting.\n3.1 Ordered Neurons LSTM\nOrdered Neurons LSTM (ON-LSTM) (Shen et al.,\n2018) is built upon a vanilla LSTM model (Hochre-\niter and Schmidhuber, 1997) with two additional\ngates, namely a master input gate ˜it and a mas-\nter forget gate ˜ft, each being a vector of the same\nshape as the LSTM forget and input gates:\nft = σ(Wf ◦[xt,ht−1] +bf ) (1)\nit = σ(Wi ◦[xt,ht−1] +bi) (2)\not = σ(Wo ◦[xt,ht−1] +bo) (3)\nˆct = tanh(Wc ◦[xt,ht−1] +bc) (4)\n˜ft = cumax(W˜f ◦[xt,ht−1] +b˜f ) (5)\n˜it = 1−cumax(W˜i ◦[xt,ht−1] +b˜i) (6)\nwhere cumax is deﬁned as the cumulative\nsum of softmax outputs, i.e., cumax(·) =\ncumsum(softmax(·)). The cumax function pro-\nvides an inductive bias to model hierarchical struc-\ntures through enforcing units in the master forget\ngate ˜ft to increase monotonically from 0 to 1 and\nthose in the master input gate ˜it to decrease mono-\ntonically from 1 to 0. The two gates are applied on\nthe original input and forget gates as follows:\nωt = ˜ft ◦˜it (7)\nˆft = ft ◦ωt + (˜ft −ωt) = ˜ft ◦(ft ◦˜it + 1−˜it)\n(8)\nˆit = it ◦ωt + (˜it −ωt) =˜it ◦(it ◦˜ft + 1−˜ft)\n(9)\nct = ˆft ◦ct−1 + ˆit ◦ˆct (10)\nht = ot ◦tanh(ct). (11)\nON-LSTM can learn the implicit structure of a\nlanguage in the form of a binary tree in an unsuper-\nvised manner, through syntactic distances, which\nare calculated as:\ndt = Dm −\nDm∑\nk=1\n˜ft (12)\nFigure 2: Binarized grammar tree and its corresponding\nsyntactic distances. The heights of the bars stand for the\nvalues of the distances. To convert this tree to syntactic dis-\ntances, we ﬁrst assign all the words an initial value of 1, and\nthen the non-leaf nodes are assigned distances in the order of\nd3 →d2 →d1 →d4, according to the procedures in the\nsecond part of Model section. On the other hand, given the\ndistances, the tree can be recovered in a top-down process by\nsetting up the split boundaries in descending order of distances\n(i.e., d4 →d1 →d2 →d3). Syntactically, a shorter distance\nbetween a pair of words indicates a closer relationship be-\ntween the constituents on the two sides of the distance. Note\nthat since only the relative order of the distances could affect\nthe structure of the trees, valid values of these distances are\nnot unique.\nwhere Dm is the size of the hidden state. The syn-\ntactic distance dt between two consecutive words\nis a scalar value, which can be interpreted as re-\nﬂecting the syntactic relatedness between the con-\nstituents before and after time point t. In terms\nof trees, it can be thought of as the height the\nlowest tree node that encloses both words. In the\ncase where we consider discrete trees, the height\nis given by the maximum path length from a leaf.\nIn the more general case, it can be thought of as\na scalar value measuring a continuous notion of\nnode height. Figure 2 depicts a sample sentence\nwith its syntactic distances and corresponding tree\nstructures. More generally, the binary tree struc-\nture of a sequence with N tokens can be speciﬁed\nwith a sequence of N−1 syntactic distances. This\ndeﬁnition of distance makes the syntactic distance\nan ultrametric (Holly, 2001; Wu et al., 1999), a\nconcept which is important in the theory of hier-\narchical agglomerative clustering (Johnson, 1967)\nand was ﬁrst explored in a linguistic setting by\nLevelt (1974).\n3.2 Converting Grammar Trees to Syntactic\nDistances\nTo integrate treebank trees into ON-LSTM, we\nneed to ﬁrst convert syntactic trees into a repre-\nsentation based on syntactic distances. Since the\noriginal grammar trees are not necessarily binary,\n6614\nwe ﬁrst split non-binary nodes by adding sentinel\nintermediate nodes to form a right-branched binary\ntree, following the steps in Stern et al. (2017). Now\nfor a binary tree with N leaf nodes, we have N−1\nnon-leaf nodes that correspond to the N −1 slots\nbetween each of the adjacent word pairs, each of\nwhich are assigned a syntactic distance (Figure 2).\nThe binary tree can thus be represented as a se-\nquence of distances d1,d2,...,d N−1.\nThe conversion from binary tree to syntactic dis-\ntances thus translates to the assigning of a distance\nvalue for each of the N −1 non-leaf nodes in the\ntree. This is achieved in a bottom-up process. We\nﬁrst initialize a distance value of 1 at all of the leaf\nnodes, and then compute the syntactic distances\nof the parent nodes by recursively tracing back\ntheir parents. More speciﬁcally, for a certain parent\nnode, its corresponding syntactic distance dP is\ncomputed with respect to the syntactic distances of\nits children dL and dR, i.e.,\ndP = max{dL,dR}+ 1. (13)\nA more detailed algorithm ﬂowchart of tree-to-\ndistance conversion is given in Appendix A.1.\n3.3 Auxiliary Syntactic Distance Outputs\nIn ON-LSTM the distances dt’s in Equation 12 are\nused to infer the structure of grammar trees. Con-\nsequently, a straight-forward way to incorporate\nground truth parse trees is to use the ground truth\ndistances dg\nt to guide dt, as depicted in Figure 1. In-\nterestingly, directly forcing the structure inferred by\nlanguage models to be coherent to linguist-tagged\nground truth trees barely improves the language\nmodel performance (see Section 6). Instead, we\nintroduce a “split-head” setting, which can practi-\ncally improve LM performances by learning two\nsets of closely related syntactic distances.\nIn particular, we use another master forget gate\n˜fw\nt for inferring a set of distances that are trained\nto align with the gold-standard syntactic distances,\nwhile leaving the original distances dt computed\nfrom ˜ft intact. To achieve this, we introduce an\nextra linear layer on top of the hidden states hf\nt ,\nand from there infer a separate set of master forget\ngates. In this way, both of the master forget gates˜ft\nand ˜fw\nt share the same input hf\nt , but optimize two\ndifferent sets of trees for the language modeling\nand parsing task, respectively. i.e.,\nhf\nt = W˜f ◦[xt,ht−1] +b˜f (14)\n˜ft = cumax(hf\nt ) (15)\n˜fw\nt = cumax(Ws(hf\nt ) +bs) (16)\nThe syntactic distances for the auxiliary super-\nvised targets are then calculated as follows:\ndw\nt = Dm −\nDm∑\nk=1\n˜fw\ntk (17)\nwhere ˜fw\ntk is the k-th element in the vector ˜fw\nt\n3.4 Grammar Trees as Auxiliary Supervised\nTargets for Language Modeling\nWith the additional master forget gate ˜fw\nt , the\nmodel has two different sets of predictions. The\nﬁrst set is the language model outputs of ON-\nLSTM, predicting the next words. The second\nset is the distances calculated in Equation 17. The\noriginal language modeling structure of the ON-\nLSTM model is left intact after the modiﬁcation,\nso we can continue to use the master forget gate ˜ft\nto update hidden states and calculate the softmax\noutput in ON-LSTM for the language modeling\npart. We denote the negative log-likelihood loss in\nthe language model part as Llm. For brevity, we do\nnot discuss the details of the loss.\nFor aligning the syntactic distances, we perform\na ranking loss between the learned syntactic dis-\ntance dw\nt and ground truth distance dg, which was\nﬁrst proposed by Burges et al. (2005). The goal is\nto encourage the model to produce the distances\nthat have the same ranking order as the ground truth\ndistances:\nLsyd =\n∑\ni,j>i\nmax(0,(1−sign(dg\ni −dg\nj )(dw\ni −dw\nj ))).\n(18)\nThe joint objective function is thus to minimize\nthe following loss:\nL= Llm + αLsyd (19)\nwhere αis the scaling parameter.\n4 Datasets\nWe make test datasets in English and Chinese,\nrespectively, both of which have parse trees and\nalso language modeling benchmarks. For English,\nwe use the Penn Treebank (PTB) dataset (Marcus\n6615\net al., 1993). Mikolov et al. (2010) have provided a\nwidely accepted version of PTB for language mod-\neling. Several modiﬁcations are made to the origi-\nnal treebank. For example, all punctuation symbols\nare removed, all characters are lower-cased, the\nvocabulary size is truncated at 10,000 and all sen-\ntences are concatenated. However, this version\nof PTB discards the parse tree structures, which\nmakes it unsuitable for comparing sequential lan-\nguage models with those utilizing tree structures.\nWe refer to this version as PTB-Concat.\nDyer et al. (2016) proposed a different version of\nPTB, which retains the parse tree structures. Sen-\ntences are modeled separately, punctuation is re-\ntained, and singleton words are replaced with the\nBerkeley parser’s mapping rules, resulting in much\nlarger vocabulary size, 23,815-word types. Since\nit retains the parse trees, this dataset enables direct\ncomparison between models that utilize parse trees\nwith those who do not. But unfortunately, since the\nvocabulary is different from PTB-Concat, and the\nsentences are processed separately, the results are\nnot directly comparable with those in PTB-Concat,\non which most existing work on language mod-\neling reports results. We refer to this version as\nPTB-Sepsent.\nAs mentioned above, a salient limitation of PTB-\nSepsent is that it does not allow fair comparison\nwith existing LM work on PTB-Concat. To address\nthis issue, we propose a different variation of PTB\ndataset that both uses the same vocabulary size\nas PTB-Concat and at the same time retaining the\nground-truth grammar trees. We pre-process the\nPTB dataset by following the same steps indicated\nby Mikolov et al. (2010) to obtain a modiﬁed tree-\nbank with the same vocabulary set as PTB-Concat.\nSentences are concatenated, and we make sure that\nthe sentences are the same to PTB-Concat, from\ntoken to token, in the training, validation, and test\nsets. This results in the same vocabulary as that of\nPTB-Concat, which allows us to directly compare\nmodels that utilize parse trees with the existing\nreports of performance on PTB-Concat. We re-\nfer to this version of PTB-Concat with syntax as\nPTB-Concat-Syn and we will cover preprocessing\ndetails in Appendix A.3.\nFor Chinese, we use the Chinese Treebank 5.1\n(Xue et al., 2005), with the same settings as Kim\net al. (2019b). Sentences are modeled separately\nand singleton words are replaced with a single\n<UNK> token. It will be referred to as CTB-\nModel Param Dev Test\nGal and Ghahramani (2016) - Variational LSTM 66M− 73.4\nKim et al. (2016) - CharCNN 19M − 78.9\nMerity et al. (2016) - Pointer Sentinel-LSTM 21M72.4 70 .9\nGrave et al. (2016) - LSTM − − 82.3\nZoph and Le (2016) - NAS Cell 54M − 62.4\nZilly et al. (2017) - Variational RHN 23M 67.9 65 .4\nShen et al. (2017) - PRPN − − 62.0\nMerity et al. (2017) - 3-layer AWD-LSTM 24M60.0 57 .3\nZolna et al. (2018) - Fraternal dropout 24M 58.9 56 .8\nShen et al. (2018) - 3-layer ON-LSTM 25M 58.3 56 .2\nONLSTM-SYD 25M 57.8 55.7\nYang et al. (2018) - AWD-LSTM-MoS 22M 56.5 54 .4\nTakase et al. (2018) - AWD-LSTM-DOC 23M54.1 52 .4\nTable 1: Various language models evaluated on validation and\ntest sets on PTB-Concat. Our model is denoted as ONLSTM-\nSYD, which incorporates tree structures during training. Yang\net al. (2018) and Takase et al. (2018) focus on improving the\nsoftmax module of LSTM LM, which are orthogonal to ours.\nSepsent in the rest of the paper.\n5 Experiments\nWe evaluate the inﬂuence of syntactic supervision\non distance-based langauge models, especially in\nterms of its language modeling performance. We\nare also going to analyze the induced syntax af-\nter introducing the structural supervision. In ad-\ndition, extensive ablation tests are conducted to\nunderstand how syntactic supervision affects the\nlangauge model.\n5.1 Language Modeling\nWe ﬁrst compare our models with existing sequen-\ntial language models on PTB-Concat, and then we\ncompare our model with transition-based language\nmodels on PTB-Sepsent and CTB-Sepsent, which\nhave a larger vocabulary and also use additional\ngrammatical structure.\nResults on PTB-Concat We ﬁrst validate the\nbeneﬁt of introducing structural signal to neu-\nral language models by training our proposed\nmodel on PTB-Concat-Syn with structural super-\nvision, and then evaluate them on the plain vali-\ndation/test set. We compare our model with the\noriginal ON-LSTM model, as well as various other\nstrong LSTM language model baselines such as\nAWD-LSTM (Merity et al., 2017) and a mixture\nof softmax (Yang et al., 2018). We denote our\nsyntactic-distance-augmented ON-LSTM model as\nONLSTM-SYD.\nFor making fair comparison, we closely fol-\nlow the hyperparameters and regularization of ON-\nLSTM (Shen et al., 2018). The model is a three-\nlayer ONLSTM-SYD language model with an em-\nbedding size of 400 and hidden layer units 1150.\nThe dropout rates are 0.5,0.45,0.3,0.45 for the\n6616\nModel PTB-\nSepsent\nCTB-\nSepsent\nKim et al. (2019b) - RNNLM 93.2 201.3\nKim et al. (2019b) - RNNG 88.7 193.1\nKim et al. (2019b) - URNNG 90.6 195.7\nKim et al. (2019b) - RNNG-URNNG 85.9 181.1\nKim et al. (2019b) - PRPN (default) 126.2 290.9\nKim et al. (2019b) - PRPN (ﬁnetuned) 96.7 216.0\nONLSTM-noAWD 69.0 167.7\nONLSTM 60.0 145.7\nONLSTM-SYD-noAWD 67.6 163.1\nONLSTM-SYD 59.6 140.5\nTable 2: Language modeling perplexity on PTB-Sepsent\nand CTB-Sepsent. Kim et al. (2019b) report two results of\nPRPN, the default one using settings in Shen et al. (2017)\nand another one ﬁnetuned by themselves. Our models use the\nsame hyperparameter settings as in Section 5.1.\nword vectors, LSTM weight metrics, outputs be-\ntween LSTM layers and the output of the last layer,\nrespectively. The embedding dropout ratio is0.125.\nThe model is trained and ﬁnetuned for 1000 epochs\nin total and is switched to the ﬁne-tuning phase at\nepoch 650. The ground truth syntactic structures\nare used to supervise the syntactic distances in the\nthird layer of ONLSTM-SYD and the loss raio α\nis set to 0.75. We use this setting as the default\nsetting for all the experiments.\nThe results are shown in Table 1. After\nadding structural signals into the model, our model\nONLSTM-SYD signiﬁcantly outperforms the orig-\ninal ON-LSTM model (p-value <0.05), indicating\nthat incorporating linguist-tagged parse trees can\ncontribute to language modeling positively.\nResults on PTB-Sepsent and CTB-Sepsent\nPTB-Sepsent and CTB-Sepsent offer a compara-\nble setting with other structure-aware supervised\n(Dyer et al., 2016) and unsupervised (Kim et al.,\n2019b) baselines. The results are listed in Table 2.\n2 ONLSTM-SYD performs better than ONLSTM,\nwhich indicates that supervised syntactic informa-\ntion can help improve language modeling.\nThe margin between our models and the base-\nlines is rather large. We ﬁnd that the set of reg-\nularization and optimization techniques proposed\nby Merity et al. (2017) contribute signiﬁcantly to\nthis margin. Because of the sequential and paral-\nlel nature of our model, it can directly inherit and\nbeneﬁt from this set of tricks. In contrast, it is\nnon-trivial to use them for RNNG and URNNG.\nAs a more rigorous analysis, we further conducted\na set of experiments without those tricks (i.e. non-\n2We use the preprocessing script in URNNG’s repository\nhttps://github.com/harvardnlp/urnng, which\nmerges all UNK types.\nmonotonically triggered ASGD, weight-dropped\nLSTM, ﬁnetuning). The performance (denoted\nas ONLSTM-SYD-noAWD) drops; however, the\nmodel still outperforms the other baselines by a\nsigniﬁcant margin.\n5.2 Structure Analysis\nIn this subsection we analyze the model to see\nhow the additional structural supervision affects\nthe quality of inferred trees. Note that our goal\nhere is to analyze the inﬂuence of ground truth\nsyntactic information on the quality of the induced\ntrees rather than to yield a better grammar induction\nperformance, since our model is not strictly com-\nparable to other models due to its extra structural\nsupervision during training.\nWe follow the settings of Htut et al. (2018) to\ntest our model on the WSJ10 and WSJ test sets,\nreporting the results in Table 3. The WSJ test set\nhas 2416 sentences with arbitrary lengths, while\nWSJ10 consists of 7422 sentences of the whole\nWSJ corpora that contain no more than 10 words.\nWe use both biased and unbiased distance-to-tree\nconversion algorithms for both ON-LSTM and our\nproposed model (c.f. Appendix A.1 and A.2 for\na formal description of the biased and non-biased\nconversion algorithm). Since our model has two\nsets of trees learned simultaneously, we list all of\nthem in Table 3.\nGrammar Induction We can see that the trees\nlearned by the joint loss show improved the F1\nscore and rely less on the branching bias of the\ntree constructing algorithm (see Dyer et al. (2019)).\nThe big gap of F1 scores on WSJ between the bi-\nased and unbiased trees are altered after introduc-\ning the structural loss, and the LM unbiased trees\nsigniﬁcantly outperforms its baseline ON-LSTM.\nThese indicate that the auxiliary supervised task\nnot only lowers the perplexity, but also improves\nthe qualities of the induced trees for the LM task.\nLooking more into the trees, we ﬁnd that com-\npared to ON-LSTM, ONLSTM-SYD improves the\nlabel prediction accuracy for NP (noun phrases),\nVP (verb phrases) and PP (prepositional phrases)\nbut fails to improve ADJP (adjective phrases). This\nsuggests that different types of human-annotated\nconstituents may have different inﬂuences on lan-\nguage modeling, or that human-annotated trees are\nthemselves biased to differing degrees between dif-\nferent constituent types.\n6617\nTraining\nObjective\nInduction\nAlgorithm\nParsing F1 Depth\nWSJ\nAccuracy on WSJ by TagR/L Ratio\non WSJModel WSJ10 WSJ ADJP NP VP PP\nON-LSTM LM Unbiased 63.2 39.0 4.9 37.9 42.8 49.6 54.2 1.08\nON-LSTM LM Biased 69.5 44.2 5.5 57.0 53.0 52.4 49.6 2.09\nONLSTM-SYDsyd LM+SYD Unbiased 77.6 61.3 7.3 38.2 73.2 69.6 72.9 2.81\nONLSTM-SYDsyd LM+SYD Biased 65.7 45.5 5.5 30.4 40.6 70.7 43.9 5.07\nONLSTM-SYDlm LM+SYD Unbiased 55.1 34.5 4.8 14.9 42.2 16.7 67.4 0.83\nONLSTM-SYDlm LM+SYD Biased 58.0 36.3 5.3 41.1 53.9 52.4 43.0 1.70\nBinary Gold Standard Trees – – 88.1 85.6 6.4 100 100 100 100 2.92\nGold standard Trees – – 100 100 5.0 100 100 100 100 2.22\nRandom Trees (Htut et al., 2018) – – 32.2 18.6 5.3 17.4 22.3 – 16.0 –\nBalanced Trees (Htut et al., 2018) – – 43.4 24.5 4.6 22.1 20.2 – 9.3 –\nLeft Branching Trees – – 19.6 9.0 12.4 – – – – –\nRight Branching Trees – – 56.6 39.8 12.4 – – – – –\nTable 3: Unlabeled parsing results evaluated on the WSJ10 and the full WSJ test set. Numbers in bold font indicate that they\nare the best compared to those computed from the other parts of the model (i.e., within the same section in the table). The\nAlgorithm column represents whether bias or unbiased algorithm is performed. ONLSTM-SYD syd and ONLSTM-SYDlm\nrepresent two sets of trees induced from loss Lsyd and Llm respectively. The Accuracy columns represent the fraction of ground\ntruth constituents of a given type that correspond to constituents in the model parses. The R/L Ratio column represents the ratio\nbetween the number of words that are left children of its parent, and those that are right children.\nBranching Bias Syntactic trees of English nat-\nurally have a bias towards right branching struc-\ntures. As shown in the last section of Table 3, right\nbranching trees achieve a much higher F1 score\nthan random, balanced or left branching trees. As\npointed out by Dyer et al. (2019), PRPN and ON-\nLSTM resort to a distance-to-tree algorithm with\nright-branching biases (See Appendix A.2).\nFor our model, a biased distance-to-tree algo-\nrithm yields worse results compared to its non-\nbiased counterpart; but on unsupervised models\nsuch as ON-LSTM, biased algorithms yield better\nresults than non-biased versions. This observation\nindicates that syntactic supervision leads to better\ntree structures as compared with fully unsupervised\ntree induction, which is intuitive.\nLinguistic Analysis Our best parsing results are\nfor trees decoded from the syntactic prediction\nobjective using the unbiased algorithm. Interest-\ningly, these trees tend to be deeper on average\nthan the (binarized) gold standard trees (see Ta-\nble 3). 3 This appears to be driven by a failure\nof the model to identify constituents centered on\ndeeply-embedded head words—instead, the model\nprefers right-branching structures. Some examples\nof trees are displayed in Figure 3. In the top part\nof the ﬁgure, we see the parse produced from the\nLsyd distances of our model, in the middle the tree\nproduced the Llm distances and, on the bottom, the\ngold standard tree. As can be seen in the ﬁgure,\nthe Lsyd-based tree is largely right-branching and\nmisses constituents centered on several deeply em-\n3Please refer to Appendix A.5 for visualizations of a more\nextensive set of sentences.\nbedded heads, such as the verb said. By contrast,\nthe Llm-based tree is considerably shallower than\nthe gold-standard and consists of a sequence of\nsmaller chunks that often mis-bracket words with\nrespect to the gold-standard constituent boundaries.\nFigure 4 illustrates these phenomenon in fur-\nther detail. The plot at the top of the ﬁgure shows\nthe proportion of constituents produced from Lsyd\ndistances whose boundaries correspond to a gold\nconstituent, broken down by height of nodes in the\npredicted tree. As the plot illustrates, the model\nfares better on relatively small constituents lower\nin trees, and makes more errors for constituents\nhigher in the tree, reﬂecting mistakes on deeply-\nembedded heads. The bottom of the ﬁgure shows\nthe same breakdown for Llm-based induced trees.\nOverall, the affect is similar, although Llm-based\ntrees are shallower than the Lsyd-based trees. We\nbelieve the increased accuracy for the longest con-\nstituents is driven by the fact that, since the highest\nconstituents cover long sentence spans and there\nare few possible long spans, these constituents have\na higher baseline probability of being correct.\nIt appears that the Lsyd objective has learned a\nstrong right-branching bias, leading to very deep\ntrees (even with the unbiased decoder) whereas the\nLlm objective appears to be using a kind of pre-\ndictive chunking of the sentence into small groups\nof words. It is tempting to speculate that these\nchunks may correspond to linguistic units used in\nprosodic planning or by the human sentence proces-\nsor, while the deeper trees correspond more directly\nto the compositional structure underlying sentence\nmeaning. We leave exploring this question to future\n6618\nthecompanywhichissuedastatementontheagreementlatefridaysaidthatNmillionofthepaymentwaspreviouslyprovidedforinitsﬁnancialstatementsandthatNNwillberecognizedinitsNthird-quarterstatement\nthecompanywhichissuedastatementontheagreementlatefridaysaidthatNmillionofthepaymentwaspreviouslyprovidedforinitsﬁnancialstatementsandthatNNwillberecognizedinitsNthird-quarterstatement\nThecompanywhichissuedastatementontheagreementlateFridaysaidthat1millionofthepaymentwaspreviouslyprovidedforinitsﬁnancialstatementsandthat500,000willberecognizedinits1989third-quarterstatement\nFigure 3: Trees induced from the syntactic task distances in our model (top), the language modeling task distances\n(middle) as well as the gold-standard trees (bottom).\nFigure 4: Accuracy breakdown w.r.t. constituent height in\nunbiased trees derived from the syntactic task distances in our\nmodel (top) and the language modeling distances (bottom).\nA constituent is considered as correct if its boundaries cor-\nrespond to a true constituent. The constituents’ heights are\nthose in the predicted tree. Since constituents that represent\nthe whole sentence always have correct boundaries, they are\nexcluded from the calculation.\nwork.\nParsing performance Our models give worse\nunlabeled parsing performance compared to\ntransition-based methods. In particular, Kim et al.\n(2019a) report that unsupervised URNNG achieves\n45.4 WSJ F1 in a similar setting, while another\nURNNG that ﬁnetunes a supervised RNNG model\ngives a much better F1 of 72.8, leading a 27.4 F1\nimprovement. In contrast, the F1 of our structure\nprediction trees is 61.3 in unbiased algorithm. This\nindicates that our model brings more beneﬁts on\nthe LM side rather than the parsing side.\n6 Ablation Study\nLayer used for supervisionTable 4 (Top) shows\nthe performances where the supervised signal is\ninjected into different layers. Although injecting\nsyntax into the last layer gives the best syntactic\ndistance for grammar induction, it fails to achieve\na similar improvement on perplexity. This suggests\nthat a better syntactic structure may not always\nlead to a better language model. The observation\nis consistent with prior research (Williams et al.,\n2018).\nTree structure We study the inﬂuence of the dif-\nferent types of supervised trees to the model. In\naddition to using the ground truth parse trees, we\nalso tried to train the model with random trees in-\nstead, and without providing trees, in which case\nit degenerates to a vanilla ON-LSTM. From Table\n4 (Middle) we can ﬁnd that without supervision\nsignals from gold standard parse trees the model\nperforms worse than the full model. Random trees\nintroduce noise to the model and downgrade both\nparsing and LM performance, indicating the impor-\ntance of injecting meaningful syntax.\nMultitask variants We also explored injecting\nthe supervised syntactic information at different\nlevels. One straight forward baseline is to add su-\npervision signals directly on the syntactic distance\n6619\nAblation Experiment Validation Test WSJ\nStudy Detail PPL PPL F1\nLayer for\nSupervision\n1st layer 58.0 55.6 57.7\n2nd layer 57.8 55.5 59.7\n3rd layer 57.8 55.7 61.3\nTree\nStructure\nNo Parse Tree 58.3 55.9 39.0\nRandom Tree 60.2 57.5 32.4\nGold Parse Tree 57.8 55.7 61.3\nMultitask\nVariants\nVanilla Multitasking 60.9 58.5 24.9\nOne set of trees 58.5 55.9 54.4\nTwo sets of trees 57.8 55.7 61.3\nTable 4: Perplexity and unlabeled parsing F1 in ablation\nstudies. We choose unbiased algorithm and the layer with\nsupervision injected. For the unsupervised models, we report\nthe layer with best F1 score. (Top) When supervising on\ndifferent layers. (Middle) Using different tree structures for\nsupervision. (Bottom) Different multitasking strategies.\nin ON-LSTM, using one set of trees to guide both\nLM and parsing, as indicated in the Model sec-\ntion (Table 4 Bottom, one set of trees). Despite\ninjecting stronger syntactic signals, this direct ap-\nproach does not improve language model perplex-\nity. This also reﬂects the fact that the most suitable\nsyntactic structures for language modeling do not\nnecessarily conform to human labeled syntax. In\naddition, we also use ON-LSTM hidden states for\nsupervised syntactic distance prediction (Table 4\nBottom, vanilla multitasking). This approach fails\nto outperform its ON-LSTM baseline due to the\nsame reason. In summary, there are mutual beneﬁts\nbetween induced and supervised syntactic informa-\ntion, although they do not fully overlap.\nGeneralization to other LMs One practical\nquestion is whether the improvements found in our\nwork can be generalized to other language models.\nTo answer this question, we introduce the multi-\ntask scheme to PRPN (Shen et al., 2017), which\nis another model that is also able to learn unsuper-\nvised structures through language modeling. Simi-\nlar to ON-LSTM, PRPN is also a syntactic distance\nmethod. We modify the PRPN model in the same\nspirit as in ON-LSTM. In addition, we change the\nencoding layer and use the output as syntactic dis-\ntance embeddings lsyd. Then we map lsyd to two\nsets of syntactic distances dlm and dsyd for lan-\nguage modeling and syntactic distance prediction,\nrespectively. Syntactic supervision comes to dsyd.\nThe model reaches a test perplexity of60.5 in PTB-\nConcat (p-value <0.05), which also signiﬁcantly\noutperforms the 62.0 from the original model. We\nrefer readers to Appendix A.4 for the details of\nPRPN and our modiﬁed PRPN-SYD.\n7 Conclusion\nWe investigated linguistic supervision for distance-\nbased structure-aware language models, showing\nits strengths over transition-based counterparts in\nlanguage modeling. Apart from the explicit ob-\nservations in achieving strong perplexity scores,\nour model reveals several interesting aspects of\nthe quality of the trees learned by the model. As a\nbyproduct of our investigation, we release a version\nof PTB-Concat, which contains syntactic structures\nwhile at the same time the same pre-processing\nsteps adopted by most previous work on neural\nlanguage models.\nAcknowledgments\nWe thank Zhiyang Teng, Qi He and all members\nat Text Intelligent Lab in Westlake University for\ninsightful discussions. We also would like to thank\nall anonymous reviewers for their constructive com-\nments. This work is supported by the National\nNatural Science Foundation of China (NSFC No.\n61976180) and the Westlake University and Bright\nDream Joint Institute for Intelligent Robotics. The\ncorresponding author is Yue Zhang.\nReferences\nDavid Adger. 2003. Core Syntax: A Minimalist Per-\nspective. Oxford University Press.\nDaniel Andor, Chris Alberti, David Weiss, Aliaksei\nSeveryn, Alessandro Presta, Kuzman Ganchev, Slav\nPetrov, and Michael Collins. 2016. Globally nor-\nmalized transition-based neural networks. arXiv\npreprint arXiv:1603.06042.\nJoan Bresnan. 2001. Lexical functional syntax. Wiley-\nBlackwell, Oxford.\nChristopher Burges, Tal Shaked, Erin Renshaw, Ari\nLazier, Matt Deeds, Nicole Hamilton, and Gre-\ngory N Hullender. 2005. Learning to rank using gra-\ndient descent. In Proceedings of the 22nd Interna-\ntional Conference on Machine learning (ICML-05) ,\npages 89–96.\nCiprian Chelba and Frederick Jelinek. 2000. Struc-\ntured language modeling. Computer Speech & Lan-\nguage, 14(4):283–332.\nJianpeng Cheng, Li Dong, and Mirella Lapata. 2016.\nLong short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733.\nJianpeng Cheng, Adam Lopez, and Mirella Lap-\nata. 2017. A generative parser with a discrim-\ninative recognition algorithm. arXiv preprint\narXiv:1708.00415.\n6620\nNoam Chomsky. 1995. The Minimalist Program. The\nMIT Press, Cambridge, Massachusetts and London,\nEngland.\nChris Dyer, Adhiguna Kuncoro, Miguel Ballesteros,\nand Noah A Smith. 2016. Recurrent neural network\ngrammars. arXiv preprint arXiv:1602.07776.\nChris Dyer, G ´abor Melis, and Phil Blunsom. 2019. A\ncritical analysis of biased parsers in unsupervised\nparsing. arXiv preprint arXiv:1909.09428.\nYarin Gal and Zoubin Ghahramani. 2016. A theoret-\nically grounded application of dropout in recurrent\nneural networks. In Advances in neural information\nprocessing systems, pages 1019–1027.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2016. Improving neural language models with a con-\ntinuous cache. arXiv preprint arXiv:1612.04426.\nJohn Hale, Chris Dyer, Adhiguna Kuncoro, and\nJonathan R Brennan. 2018. Finding syntax in hu-\nman encephalography with beam search. arXiv\npreprint arXiv:1806.04127.\nPeter A Heeman and James F Allen. 1997. Incorpo-\nrating pos tagging into language modeling. arXiv\npreprint cmp-lg/9705014.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nJan E Holly. 2001. Pictures of ultrametric spaces, the\np-adic numbers, and valued ﬁelds. The American\nMathematical Monthly, 108(8).\nPhu Mon Htut, Kyunghyun Cho, and Samuel R Bow-\nman. 2018. Grammar induction with neural lan-\nguage models: An unusual replication. arXiv\npreprint arXiv:1808.10000.\nRodney Huddleston and Geoffrey K. Pullum. 2002.\nThe Cambridge Grammar of English Language .\nCambridge University Press, Cambridge.\nStephen C Johnson. 1967. Hierarchical clustering\nschemes. Psychometrika, 32(3).\nDaniel Jurafsky, Chuck Wooters, Jonathan Segal, An-\ndreas Stolcke, Eric Fosler, Gary Tajchaman, and Nel-\nson Morgan. 1995. Using a stochastic context-free\ngrammar as a language model for speech recogni-\ntion. In 1995 International Conference on Acous-\ntics, Speech, and Signal Processing, volume 1, pages\n189–192. IEEE.\nYoon Kim, Chris Dyer, and Alexander M Rush.\n2019a. Compound probabilistic context-free gram-\nmars for grammar induction. arXiv preprint\narXiv:1906.10225.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M Rush. 2016. Character-aware neural language\nmodels. In Thirtieth AAAI Conference on Artiﬁcial\nIntelligence.\nYoon Kim, Alexander M Rush, Lei Yu, Adhiguna Kun-\ncoro, Chris Dyer, and G ´abor Melis. 2019b. Unsu-\npervised recurrent neural network grammars. arXiv\npreprint arXiv:1904.03746.\nAdhiguna Kuncoro, Chris Dyer, John Hale, Dani Yo-\ngatama, Stephen Clark, and Phil Blunsom. 2018.\nLstms can learn syntax-sensitive dependencies well,\nbut modeling structure makes them better. In Pro-\nceedings of the 56th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 1426–1436.\nAdhiguna Kuncoro, Chris Dyer, Laura Rimell, Stephen\nClark, and Phil Blunsom. 2019. Scalable syntax-\naware language models using knowledge distillation.\narXiv preprint arXiv:1906.06438.\nWillem J. M. Levelt. 1974. Formal Grammars in Lin-\nguistics and Psycholinguistics, Volume 3: Psycholin-\nguistic applications.\nMitchell Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a large annotated\ncorpus of english: The penn treebank.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2017. Regularizing and optimizing lstm lan-\nguage models. arXiv preprint arXiv:1708.02182.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. arXiv preprint arXiv:1609.07843.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Eleventh\nannual conference of the international speech com-\nmunication association.\nJoakim Nivre. 2008. Algorithms for deterministic in-\ncremental dependency parsing. Computational Lin-\nguistics, 34(4):513–553.\nHao Peng, Roy Schwartz, and Noah A Smith. 2019.\nPalm: A hybrid parser and language model. arXiv\npreprint arXiv:1909.02134.\nCarl Pollard and Ivan A Sag. 1994. Head-driven\nphrase structure grammar . University of Chicago\nPress.\nIvan A. Sag, Thomas Wasow, and Emily M. Bender.\n2003. Syntactic Theory: A Formal Introduction .\nCSLI, Stanford, CA.\nYikang Shen, Zhouhan Lin, Chin-Wei Huang, and\nAaron Courville. 2017. Neural language model-\ning by jointly learning syntax and lexicon. arXiv\npreprint arXiv:1711.02013.\nYikang Shen, Shawn Tan, Alessandro Sordoni, and\nAaron Courville. 2018. Ordered neurons: Integrat-\ning tree structures into recurrent neural networks.\narXiv preprint arXiv:1810.09536.\n6621\nB Srinivas. 1996. ” almost parsing” technique for lan-\nguage modeling. In Proceeding of Fourth Interna-\ntional Conference on Spoken Language Processing.\nICSLP’96, volume 2, pages 1173–1176. IEEE.\nMark Steedman and Jason Baldridge. 2011. Combi-\nnatory categorial grammar. Non-Transformational\nSyntax: Formal and explicit models of grammar ,\npages 181–224.\nMitchell Stern, Jacob Andreas, and Dan Klein. 2017.\nA minimal span-based neural constituency parser.\narXiv preprint arXiv:1705.03919.\nSho Takase, Jun Suzuki, and Masaaki Nagata. 2018.\nDirect output connection for a high-rank language\nmodel. arXiv preprint arXiv:1808.10143.\nDong Hoon Van Uytsel, Filip Van Aelten, and Dirk\nVan Compernolle. 2001. A structured language\nmodel based on context-sensitive probabilistic left-\ncorner parsing. In Proceedings of the second meet-\ning of the North American Chapter of the Associ-\nation for Computational Linguistics on Language\ntechnologies, pages 1–8. Association for Computa-\ntional Linguistics.\nAdina Williams, Andrew Drozdov*, and Samuel R\nBowman. 2018. Do latent tree learning models iden-\ntify meaningful structure in sentences? Transac-\ntions of the Association for Computational Linguis-\ntics, 6:253–267.\nJerry H Wright, Gareth JF Jones, and Harvey Lloyd-\nThomas. 1994. A robust language model incorpo-\nrating a substring parser and extended n-grams. In\nProceedings of ICASSP’94. IEEE International Con-\nference on Acoustics, Speech and Signal Processing,\nvolume 1, pages I–361. IEEE.\nBang Ye Wu, Kun-Mao Chao, and Chuan Yi Tang.\n1999. Approximation and exact algorithms for con-\nstructing minimum ultrametric trees from distance\nmatrices. Journal of CO, 3(2).\nPeng Xu, Ciprian Chelba, and Frederick Jelinek. 2002.\nA study on richer syntactic dependencies for struc-\ntured language modeling. In Proceedings of the 40th\nAnnual Meeting on Association for Computational\nLinguistics, pages 191–198. Association for Compu-\ntational Linguistics.\nPeng Xu, Ahmad Emami, and Frederick Jelinek. 2003.\nTraining connectionist models for the structured lan-\nguage model. In Proceedings of the 2003 conference\non Empirical methods in natural language process-\ning, pages 160–167. Association for Computational\nLinguistics.\nNaiwen Xue, Fei Xia, Fu-Dong Chiou, and Marta\nPalmer. 2005. The penn chinese treebank: Phrase\nstructure annotation of a large corpus. Natural lan-\nguage engineering, 11(2):207–238.\nZhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and\nWilliam W Cohen. 2018. Breaking the softmax bot-\ntleneck: A high-rank rnn language model. ICLR.\nYue Zhang and Joakim Nivre. 2011. Transition-based\ndependency parsing with rich non-local features. In\nProceedings of the 49th Annual Meeting of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies: short papers-Volume 2 , pages\n188–193. Association for Computational Linguis-\ntics.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan\nKoutn´ık, and J¨urgen Schmidhuber. 2017. Recurrent\nhighway networks. In Proceedings of the 34th Inter-\nnational Conference on Machine Learning-Volume\n70, pages 4189–4198. JMLR. org.\nKonrad Zolna, Devansh Arpit, Dendi Suhubdy, and\nYoshua Bengio. 2018. Fraternal dropout. ICLR.\nBarret Zoph and Quoc V Le. 2016. Neural architecture\nsearch with reinforcement learning. arXiv preprint\narXiv:1611.01578.\n6622\nA Appendices\nA.1 Algorithms for transformation between\nparse trees and syntactic distances\nThe following tree-to-distance algorithm provides\na set of distances given a tree. The node indicates\nthe root node of the given tree.\nAlgorithm 1Binary Parse Tree to Distance\n(∪represents the concatenation operator of lists)\n1: function TREE 2DISTANCE (node)\n2: if node is leaf then\n3: d←1\n4: else\n5: childl,childr ←children of node\n6: t2dl ←Tree2Distance(childl)\n7: t2dr ←Tree2Distance(childr)\n8: d←max(dl,dr) + 1\n9: t2d ←t2dl ∪[d] ∪t2dr\n10: end if\n11: return t2d, d\n12: end function\nThe following distance-to-tree conversion algo-\nrithm provides an unbiased reconstruction of tree\ngiven a set of distances.\nAlgorithm 2Distance to Binary Parse Tree\n1: function DISTANCE 2TREE (d)\n2: if d ̸= []then\n3: i←arg maxi(d)\n4: childl ←Distance2Tree(d<i)\n5: childr ←Distance2Tree(d≥i)\n6: node ←Node(childl, childr)\n7: end if\n8: return node\n9: end function\nA.2 Distance-to-tree algorithm with\nright-branching bias\nAlgorithm 3Distance to Binary Parse Tree with\nRight-Branching Bias\n1: function DISTANCE 2TREE (d)\n2: if d ̸= []then\n3: i←arg maxi(d)\n4: childl ←Distance2Tree(d<i)\n5: childr ←Distance2Tree(d>i)\n6: nodebias ←Node(nodei, childr)\n7: node ←Node(childl, nodebias)\n8: end if\n9: return node\n10: end function\nA.3 Details of generating our\nPTB-Concat-Syn version\nMikolov et al. (2010) brieﬂy described the steps of\nconverting from the original Penn Treebank dataset\nto his version of dataset, which later becomes the\nstandard in language modeling task. We denote\nthis version as PTB-Concat. In our paper, to get\nstrictly the same PTB language modeling dataset,\nwe follow his steps on the original Penn Treebank,\nwhile preserving the tree structure. Speciﬁcally, we\ntook the following steps:\n1. Convert all tokens to lowercase.\n2. For tokens which are purely digits, or digits\nonly with “.” or “-” are converted to token “N”.\n3. Replace all “$” with “N”.\n4. Delete tokens “\\\\” and “wa” if their POS tags\nare “POS” and “NNP”, respectively.\n5. Delete all tokens that fall into the following\nlist:\n[‘‘,\\’\\’,,,.,:,;,-,?,!,¨,ˆ, ,\\\\,|,˜,\n-lrb-,-rrb-,-lcb-,-rcb-,(,),[,],\n{,},<,>,--,...,‘].\n6. Delete all tokens with tag “-NONE-”.\n7. Add a special token “ </s>” to the end of\neach sentence.\n8. Truncated the vocabulary at 9,999 accord-\ning to the frequencies and assign all the out-of-\nvocabulary tokens a special token “<unk>”.\n9. After the above procedures, there are still mi-\nnor differences to PTB-Concat. We then go through\nthe whole Penn Treebank corpora to manually ﬁx\nall the unmatched tokens.\nThese procedures ensures we have exactly the\nsame training, validation and test sets as PTB-\nConcat, the only difference is that our datasets has\n6623\nadditional grammar trees retained from the original\nPTB dataset. The resulting datasets then becomes\nPTB-Concat-Syn.\nA.4 PRPN and PRPN-SYD\nA.4.1 Parse-Read-Predict Network (PRPN)\nThe idea of PRPN builds upon an assumption that\nto predict a word xi, we only need information\nfor all precedent siblings in constituent tree. The\nmodel constitutes three components: (i) a parsing\nnetwork that calculates the syntactic distance and\nparsing gates. (ii) a reading network to model the\nlanguage, and (iii) a predict network to predict the\nnext word.\nPRPN ﬁrst uses a two-layer convolutional net-\nwork to calculate the syntactic distance d at\ntimestep t:\nhi = ReLU(Wc\n\n\nei−L\nei−L+1\n...\nei\n\n+ bc) (20)\ndi = ReLU (Wdhi + bd) (21)\nWhere ei−L,...,e i are word embeddings, L is\nthe lookback range.\nThen the difference between distances is fed\nthrough hardtanh to model the degree αt\nj that how\nmuch two words xt and xj are related:\nαt\nj = hardtanh ((dt −dj) ·τ) + 1\n2 (22)\nWhere hardtanh(x) = max( −1,min(1,x)),\nand τ is the temperature parameter.\nFor word xi, the ﬁrst precedent word xt with a\nsmall value αt\ni represents xt and all its precedents\nare not likely to be siblings of xi. The following\nparsing gate gt\ni models the probability of xt and xi\nbeing siblings:\ngt\ni = P(lt ≤i) =\nt−1∏\nj=i+1\nαt\nj (23)\nThe reading network is a variant of Long Short-\nTerm Memory-Network (LSTMN) (Cheng et al.,\n2016) where the attention score is softly truncated\nby parsing gates:\nst\ni = gt\ni ˜st\ni∑\ni gt\ni\n(24)\nThe predict network utilizes the structure-aware\nhidden states of reading network to predict the next\nword.\nA.4.2 The PRPN-SYD model\nWe re-designed the parsing network. We use\nLSTM to encode each embedding sequence s =\n(e0,e1,...,e n),. Because the task of language mod-\neling prohibits seeing future words, we use unidi-\nrectional LSTM:\nh0,...,h n = LSTMw(e0,...,e n) (25)\nWe stack a convolutional layer on top of the\nhidden states hi of the LSTM, which helps gather\nlocal syntactic information:\ng0,...,g n = CONV(h0,...,h n) (26)\nNext, syntactical information learned both lo-\ncally and globally are integrated by using another\nunidirectional LSTM:\nˆh0,..., ˆhn = LSTMd(g0,...,g n) (27)\nWe pass the ˆhlayer through two 2-layer fully-\nconnected networks which output two respective\nsets of distance scalars:\ndlm\ni = FFlm(ˆhi) dsyd\ni = FFsyd(ˆhi) (28)\nWhere dlm is the distance for language modeling\nwhile dsyd is for syntactic distance prediction. For\ntwo sets of distances, we use the same objective\nfunctions as described in ONLSTM-SYD.\nA.5 Trees\nWe visualize a set of sentences (14 sentences in\ntotal) and their corresponding trees in parallel to\ncontrast the qualitative differences of the model\ninduces trees and gold standard trees. Sentences\nare selected randomly from the dataset. In each\nof the following ﬁgures, we provide three trees\nfor a same sentence, which corresponds to trees\ninduced from the syntactic task (top) and language\nmodel task (middle) set of distances, as well as the\ngold-standard trees (bottom).\n6624\nboeingisalsosupposedtosendtoamericawestanotherNtwin-engineaircraftaswellasaNbyyear’send\nboeingisalsosupposedtosendtoamericawestanotherNtwin-engineaircraftaswellasaNbyyear’send\nboeingisalsosupposedtosendtoamericawestanotherNtwin-engineaircraftaswellasaNbyyear’send\nFigure 5: Sentence 1. Trees induced from the syntactic task (top) and language model task (middle) set of distances,\nas well as the gold-standard trees (bottom).\nthatdiscrepancyhurtsquantumbadlybecauseitsownplantscoveronlyabouthalfofitsethyleneneeds\nthatdiscrepancyhurtsquantumbadlybecauseitsownplantscoveronlyabouthalfofitsethyleneneeds\nthatdiscrepancyhurtsquantumbadlybecauseitsownplantscoveronlyabouthalfofitsethyleneneeds\nFigure 6: Sentence 2. Trees induced from the syntactic task (top) and language model task (middle) set of distances,\nas well as the gold-standard trees (bottom).\nbritain’sretailpriceindexroseN.N%inseptemberfromaugustandwasupN.N%fortheyearthecentralstatisticalofﬁcesaid\nbritain’sretailpriceindexroseN.N%inseptemberfromaugustandwasupN.N%fortheyearthecentralstatisticalofﬁcesaid\nbritain’sretailpriceindexroseN.N%inseptemberfromaugustandwasupN.N%fortheyearthecentralstatisticalofﬁcesaid\nFigure 7: Sentence 3. Trees induced from the syntactic task (top) and language model task (middle) set of distances,\nas well as the gold-standard trees (bottom).\n6625\nbeginninginmid-Npricesbeganacceleratingasagrowingu.s.economyandtheweakdollarspurreddemand\nbeginninginmid-Npricesbeganacceleratingasagrowingu.s.economyandtheweakdollarspurreddemand\nbeginninginmid-Npricesbeganacceleratingasagrowingu.s.economyandtheweakdollarspurreddemand\nFigure 8: Sentence 4. Trees induced from the syntactic task (top) and language model task (middle) set of distances,\nas well as the gold-standard trees (bottom).\nhoweverasexpectedbrazilwaitedforthecropestimatetocomeoutandthencuttheexportpriceofitsjuiceconcentratetoaboutN.NapoundfromaroundN.N\nhoweverasexpectedbrazilwaitedforthecropestimatetocomeoutandthencuttheexportpriceofitsjuiceconcentratetoaboutN.NapoundfromaroundN.N\nhoweverasexpectedbrazilwaitedforthecropestimatetocomeoutandthencuttheexportpriceofitsjuiceconcentratetoaboutN.NapoundfromaroundN.N\nFigure 9: Sentence 5. Trees induced from the syntactic task (top) and language model task (middle) set of distances,\nas well as the gold-standard trees (bottom).\ntotaladvertisinglinagewasmodestlylowerasclassiﬁed-advolumeincreasedwhiletherewassofterdemandforretailandnationaladlinagesaidjohncurleygannett’schiefexecutiveofﬁcer\ntotaladvertisinglinagewasmodestlylowerasclassiﬁed-advolumeincreasedwhiletherewassofterdemandforretailandnationaladlinagesaidjohncurleygannett’schiefexecutiveofﬁcer\ntotaladvertisinglinagewasmodestlylowerasclassiﬁed-advolumeincreasedwhiletherewassofterdemandforretailandnationaladlinagesaidjohncurleygannett’schiefexecutiveofﬁcer\nFigure 10: Sentence 6. Trees induced from the syntactic task (top) and language model task (middle) set of\ndistances, as well as the gold-standard trees (bottom).\n6626\nit’sturningouttobearealblockbustermr.sweigsaid\nit’sturningouttobearealblockbustermr.sweigsaid\nit’sturningouttobearealblockbustermr.sweigsaid\nFigure 11: Sentence 7. Trees induced from the syntactic task (top) and language model task (middle) set of\ndistances, as well as the gold-standard trees (bottom).\nthefactthatthishappenedtwoyearsagoandtherewasarecoverygivespeoplesomecomfortthatthiswon’tbeaproblem\nthefactthatthishappenedtwoyearsagoandtherewasarecoverygivespeoplesomecomfortthatthiswon’tbeaproblem\nthefactthatthishappenedtwoyearsagoandtherewasarecoverygivespeoplesomecomfortthatthiswon’tbeaproblem\nFigure 12: Sentence 8. Trees induced from the syntactic task (top) and language model task (middle) set of\ndistances, as well as the gold-standard trees (bottom).\nncnbwillalsoacquireNmillionoffreedom’sassetsfromthertcwhichwillrequireNmillioninassistance\nncnbwillalsoacquireNmillionoffreedom’sassetsfromthertcwhichwillrequireNmillioninassistance\nncnbwillalsoacquireNmillionoffreedom’sassetsfromthertcwhichwillrequireNmillioninassistance\nFigure 13: Sentence 9. Trees induced from the syntactic task (top) and language model task (middle) set of\ndistances, as well as the gold-standard trees (bottom).\n6627\nwhenyousuggestotherwiseyouleavetherealmofreportingandentertheorbitofspeculation\nwhenyousuggestotherwiseyouleavetherealmofreportingandentertheorbitofspeculation\nwhenyousuggestotherwiseyouleavetherealmofreportingandentertheorbitofspeculation\nFigure 14: Sentence 10. Trees induced from the syntactic task (top) and language model task (middle) set of\ndistances, as well as the gold-standard trees (bottom).\nbutnotmuchmoneywasspentontheshowseitherasituationthatencouragedcheap-to-maketalkandgameshowswhilediscouragingexpensive-to-producedramas\nbutnotmuchmoneywasspentontheshowseitherasituationthatencouragedcheap-to-maketalkandgameshowswhilediscouragingexpensive-to-producedramas\nbutnotmuchmoneywasspentontheshowseitherasituationthatencouragedcheap-to-maketalkandgameshowswhilediscouragingexpensive-to-producedramas\nFigure 15: Sentence 11. Trees induced from the syntactic task (top) and language model task (middle) set of\ndistances, as well as the gold-standard trees (bottom).\nitalsodropsaprovisionthatwouldhavepermittedcorporationstouseexcesspensionfundstopayhealthbeneﬁtsforcurrentretirees\nitalsodropsaprovisionthatwouldhavepermittedcorporationstouseexcesspensionfundstopayhealthbeneﬁtsforcurrentretirees\nitalsodropsaprovisionthatwouldhavepermittedcorporationstouseexcesspensionfundstopayhealthbeneﬁtsforcurrentretirees\nFigure 16: Sentence 12. Trees induced from the syntactic task (top) and language model task (middle) set of\ndistances, as well as the gold-standard trees (bottom).\n6628\nandi thinkinstitutionsaregoingtocomeinandbuy\nandi thinkinstitutionsaregoingtocomeinandbuy\nandi thinkinstitutionsaregoingtocomeinandbuy\nFigure 17: Sentence 13. Trees induced from the syntactic task (top) and language model task (middle) set of\ndistances, as well as the gold-standard trees (bottom).\nthere’snothingrationalaboutthiskindofaction\nthere’snothingrationalaboutthiskindofaction\nthere’snothingrationalaboutthiskindofaction\nFigure 18: Sentence 14. Trees induced from the syntactic task (top) and language model task (middle) set of\ndistances, as well as the gold-standard trees (bottom)."
}