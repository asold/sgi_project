{
  "title": "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic.",
  "url": "https://openalex.org/W3119989665",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3119026467",
      "name": "Muhammad Abdul-Mageed",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A4319245227",
      "name": "AbdelRahim Elmadany",
      "affiliations": [
        "University of British Columbia"
      ]
    },
    {
      "id": "https://openalex.org/A2620602306",
      "name": "El Moatez Billah Nagoudi",
      "affiliations": [
        "University of British Columbia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2805351602",
    "https://openalex.org/W2972070042",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3106433641",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W3089190054",
    "https://openalex.org/W2464521204",
    "https://openalex.org/W2027232045",
    "https://openalex.org/W2562238432",
    "https://openalex.org/W2970485137",
    "https://openalex.org/W2809324505",
    "https://openalex.org/W2252085349",
    "https://openalex.org/W3088592174",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W179850243",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2160802179",
    "https://openalex.org/W2914220664",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2104912629",
    "https://openalex.org/W2970814728",
    "https://openalex.org/W3032532958",
    "https://openalex.org/W2250594687",
    "https://openalex.org/W2138923868",
    "https://openalex.org/W3033940819",
    "https://openalex.org/W2945018970",
    "https://openalex.org/W3032746405",
    "https://openalex.org/W3088892776",
    "https://openalex.org/W3088728183",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3040245432",
    "https://openalex.org/W3119566336",
    "https://openalex.org/W3025939269",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2806092253",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3098637735",
    "https://openalex.org/W2295710275",
    "https://openalex.org/W2767566483",
    "https://openalex.org/W2995647371",
    "https://openalex.org/W340195604",
    "https://openalex.org/W2970960342",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3087889451",
    "https://openalex.org/W3088774333",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3088188607",
    "https://openalex.org/W3119989085",
    "https://openalex.org/W2621199241",
    "https://openalex.org/W2396324390",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2948433920",
    "https://openalex.org/W2805744755",
    "https://openalex.org/W2770803436",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W2996580882",
    "https://openalex.org/W3118485687",
    "https://openalex.org/W3154368324",
    "https://openalex.org/W3087891130",
    "https://openalex.org/W3120373775",
    "https://openalex.org/W3000128329"
  ],
  "abstract": "Pre-trained language models (LMs) are currently integral to many natural language processing systems. Although multilingual LMs were also introduced to serve many languages, these have limitations such as being costly at inference time and the size and diversity of non-English data involved in their pre-training. We remedy these issues for a collection of diverse Arabic varieties by introducing two powerful deep bidirectional transformer-based models, ARBERT and MARBERT. To evaluate our models, we also introduce ARLUE, a new benchmark for multi-dialectal Arabic language understanding evaluation. ARLUE is built using 42 datasets targeting six different task clusters, allowing us to offer a series of standardized experiments under rich conditions. When fine-tuned on ARLUE, our models collectively achieve new state-of-the-art results across the majority of tasks (37 out of 48 classification tasks, on the 42 datasets). Our best model acquires the highest ARLUE score (77.40) across all six task clusters, outperforming all other models including XLM-R Large (~ 3.4 x larger size). Our models are publicly available at this https URL and ARLUE will be released through the same repository.",
  "full_text": "ARBERT & MARBERT: Deep Bidirectional Transformers for Arabic\nMuhammad Abdul-Mageed† AbdelRahim Elmadany† El Moatez Billah Nagoudi†\nNatural Language Processing Lab\nThe University of British Columbia\n{muhammad.mageed,a.elmadany,moatez.nagoudi}@ubc.ca\nAbstract\nPre-trained language models (LMs) are cur-\nrently integral to many natural language pro-\ncessing systems. Although multilingual LMs\nwere also introduced to serve many languages,\nthese have limitations such as being costly at\ninference time and the size and diversity of\nnon-English data involved in their pre-training.\nWe remedy these issues for a collection of\ndiverse Arabic varieties by introducing two\npowerful deep bidirectional transformer-based\nmodels, ARBERT and MARBERT. To eval-\nuate our models, we also introduce ARLUE,\na new benchmark for multi-dialectal Arabic\nlanguage understanding evaluation. ARLUE\nis built using 42 datasets targeting six differ-\nent task clusters, allowing us to offer a se-\nries of standardized experiments under rich\nconditions. When ﬁne-tuned on ARLUE, our\nmodels collectively achieve new state-of-the-\nart results across the majority of tasks ( 37 out\nof 48 classiﬁcation tasks, on the 42 datasets).\nOur best model acquires the highest ARLUE\nscore (77.40) across all six task clusters, out-\nperforming all other models including XLM-\nRLarge (∼3.4×larger size). Our models are\npublicly available at https://github.com/UBC-\nNLP/marbert and ARLUE will be released\nthrough the same repository.\n1 Introduction\nLanguage models (LMs) exploiting self-supervised\nlearning such as BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019a) have recently emerged\nas powerful transfer learning tools that help im-\nprove a very wide range of natural language pro-\ncessing (NLP) tasks. Multilingual LMs such as\nmBERT (Devlin et al., 2019) and XLM-RoBERTa\n(XLM-R) (Conneau et al., 2020) have also been\nintroduced, but are usually outperformed by mono-\nlingual models pre-trained with larger vocabulary\nand bigger language-speciﬁc datasets (Virtanen\net al., 2019; Antoun et al., 2020; Dadas et al., 2020;\n† All authors contributed equally.\nde Vries et al., 2019; Le et al., 2020; Martin et al.,\n2020; Nguyen and Tuan Nguyen, 2020).\nSince LMs are costly to pre-train, it is important\nto keep in mind the end goals they will serve once\ndeveloped. For example, (i) in addition to their util-\nity on ‘standard’ data, it is useful to endow them\nwith ability to excel on wider real world settings\nsuch as in social media. Some existing LMs do not\nmeet this need since they were trained on datasets\nthat do not sufﬁciently capture the nuances of social\nmedia language (e.g., frequent use of abbreviations,\nemoticons, and hashtags; playful character repeti-\ntions; neologisms and informal language). It is also\ndesirable to build models able to (ii) serve diverse\ncommunities (e.g., speakers of dialects of a given\nlanguage), rather than focusing only on mainstream\nvarieties. In addition, once created, models should\nbe (iii) usable in energy efﬁcient scenarios. This\nmeans that, for example, medium-to-large models\nwith competitive performance should be preferred\nto large-to-mega models.\nA related issue is (iv) how LMs are evalu-\nated. Progress in NLP hinges on our ability to\ncarry out meaningful comparisons across tasks,\non carefully designed benchmarks. Although sev-\neral benchmarks have been introduced to evaluate\nLMs, the majority of these are either exclusively\nin English (e.g., DecaNLP (McCann et al., 2018),\nGLUE (Wang et al., 2018), SuperGLUE (Wang\net al., 2019)) or use machine translation in their\ntraining splits (e.g., XTREME (Hu et al., 2020)).\nAgain, useful as these benchmarks are, this circum-\nvents our ability to measure progress in real-world\nsettings (e.g., training and evaluation on native vs.\ntranslated data) for both cross-lingual NLP and in\nmonolingual, non-English environments.\nContext. Our objective is to showcase a sce-\nnario where we build LMs that meet all four needs\nlisted above. That is, we describe novel LMs that (i)\nexcel across domains, including social media, (ii)\ncan serve diverse communities, and (iii) perform\nwell compared to larger (more energy hungry) mod-\narXiv:2101.01785v3  [cs.CL]  7 Jun 2021\nels (iv) on a novel, standardized benchmark. We\nchoose Arabic as the context for our work since\nit is a widely spoken language ( ∼400M native\nspeakers), with a large number of diverse dialects\ndiffering among themselves and from the standard\nvariety, Modern Standard Arabic (MSA). Arabic\nis also covered by the popular mBERT (Devlin\net al., 2019) and XLM-R (Conneau et al., 2020),\nwhich provides us a setup for meaningful com-\nparisons. That is, not only are we able to empir-\nically measure monolingual vs. multilingual per-\nformance under robust conditions using our new\nbenchmark, ARLUE, but we can also demonstrate\nhow our base-sized models outperform (or at least\nare on par with) larger models (i.e., XLM-R Large,\nwhich is ∼3.4×larger than our models). In the\ncontext of our work, we also show how the cur-\nrently best-performing model dedicated to Arabic,\nAraBERT (Antoun et al., 2020), suffers from a\nnumber of issues. These include (a) not making\nuse of easily accessible data across domains and,\nmore seriously, (b) limited ability to handle Arabic\ndialects and (c) narrow evaluation. We rectify all\nthese limitations.\nOur contributions. With our stated goals\nin mind, we introduce ARBERT and MAR-\nBERT, two Arabic-focused LMs exploiting large-\nto-massive diverse datasets. For evaluation, we\nalso introduce a novel ARabic natural Language\nUnderstanding Evaluation benchmark (ARLUE).\nARLUE is composed of 42 different datasets, mak-\ning it by far the largest and most diverse Arabic\nNLP benchmark we know of. We arrange AR-\nLUE into six coherent cluster tasks and methodi-\ncally evaluate on each independent dataset as well\nas each cluster task, ultimately reporting a single\nARLUE score. Our models establish new state-\nof-the-art (SOTA) on the majority of tasks, across\nall cluster tasks. Our goal is for ARLUE to serve\nthe critical need for measuring progress on Arabic,\nand facilitate evaluation of multilingual and Ara-\nbic LMs. To summarize, we offer the following\ncontributions:\n1. We develop ARBERT and MARBERT ,\ntwo novel Arabic-speciﬁc Transformer LMs\npre-trained on very large and diverse datasets\nto facilitate transfer learning on MSA as well\nas Arabic dialects.\n2. We introduce ARLUE, a new benchmark de-\nveloped by collecting and standardizing splits\non 42 datasets across six different Arabic lan-\nguage understanding cluster tasks, thereby fa-\ncilitating measurement of progress on Arabic\nand multilingual NLP.\n3. We ﬁne-tune our new powerful models on\nARLUE and provide an extensive set of com-\nparisons to available models. Our models\nachieve new SOTAon all task clusters in 37\nout of 48 individual datasets and a SOTAAR-\nLUE score.\nThe rest of the paper is organized as follows:\nIn Section 2, we provide an overview of Arabic\nLMs. Section 3 describes our Arabic pre-tained\nmodels. We evaluate our models on downstream\ntasks in Section 4, and present our benchmark AR-\nLUE and evaluation on it in Section 5. Section 6\nis an overview of related work. We conclude in\nSection 7. We now introduce existing Arabic LMs.\n2 Arabic LMs\nThe term Arabic refers to a collection of languages,\nlanguage varieties, and dialects. The standard va-\nriety of Arabic is MSA, and there exists a large\nnumber of dialects that are usually deﬁned at the\nlevel of the region or country (Abdul-Mageed\net al., 2020a, 2021a,b). A number of Arabic LMs\nhas been developed. The most notable among\nthese is AraBERT (Antoun et al., 2020), which\nis trained with the same architecture as BERT (De-\nvlin et al., 2019) and uses the BERT Base conﬁg-\nuration. AraBERT is trained on 23GB of Ara-\nbic text, making ∼70M sentences and 3B words,\nfrom Arabic Wikipedia, the Open Source Inter-\nnational dataset (OSIAN) (Zeroual et al., 2019)\n(3.5M news articles from 24 Arab countries), and\n1.5B words Corpus from El-Khair (2016) (5M ar-\nticles extracted from 10 news sources). Antoun\net al. (2020) evaluate AraBERT on three Arabic\ndownstream tasks. These are (1) sentiment anal-\nysis from six different datasets: HARD (Elnagar\net al., 2018), ASTD (Nabil et al., 2015), ArsenTD-\nLev (Baly et al., 2019), LABR (Aly and Atiya,\n2013), and ArSaS (Elmadany et al., 2018). (2)\nNER, with the ANERcorp (Benajiba and Rosso,\n2007), and (3) Arabic QA, on Arabic-SQuAD\nand ARCD (Mozannar et al., 2019) datasets. An-\nother Arabic LM that was also introduced is Ara-\nbicBERT (Safaya et al., 2020), which is similarly\nbased on BERT architecture. ArabicBERT was pre-\ntrained on two datasets only, Arabic Wikipedia and\nArabic OSACAR (Su´arez et al., 2019). Since both\nof these datasets are already included in AraBERT,\nand Arabic OSACAR1 has signiﬁcant duplicates,\nwe compare to AraBERT only. GigaBERT (Lan\net al., 2020), an Arabic and English LM designed\nwith code-switching data in mind, was also intro-\nduced.2\n3 Our Models\n3.1 ARBERT\n3.1.1 Training Data\nWe train ARBERT on 61GB of MSA text (6.5B\ntokens) from the following sources:\n• Books (Hindawi) . We collect and pre-\nprocess 1, 800 Arabic books from the public\nArabic bookstore Hindawi.3\n• El-Khair. This is a 5M news articles dataset\nfrom 10 major news sources covering eight\nArab countries from El-Khair (2016).\n• Gigaword. We use Arabic Gigaword 5 th\nEdition from the Linguistic Data Consor-\ntium (LDC).4 The dataset is a comprehensive\narchive of newswire text from multiple Arabic\nnews sources.\n• OSCAR. This is the MSA and Egyptian Ara-\nbic portion of the Open Super-large Crawled\nAlmanach coRpus (Su ´arez et al., 2019), 5\na huge multilingual subset from Common\nCrawl6 obtained using language identiﬁcation\nand ﬁltering.\n• OSIAN. The Open Source International Ara-\nbic News Corpus (OSIAN) (Zeroual et al.,\n2019) consists of 3.5 million articles from 31\nnews sources in 24 Arab countries.\n• Wikipedia Arabic. We download and use the\nDecember 2019 dump of Arabic Wikipedia.\nWe use WikiExtractor7 to extract articles and\nremove markup from the dump.\n1https://oscar-corpus.com.\n2Since GigaBERT is very recent, we could not compare to\nit. However, we note that our pre-training datasets are much\nlarger (i.e., 15.6B tokens for MARBERT vs. 4.3B Arabic\ntokens for GigaBERT) and more diverse.\n3https://www.hindawi.org/books/.\n4https://catalog.ldc.upenn.edu/LDC2011T11.\n5https://oscar-corpus.com/.\n6https://commoncrawl.org.\n7https://github.com/attardi/wikiextractor.\nSource Size #Tokens\nBooks (Hindawi) 650 MB 72 .5M\nEl-Khair 16 GB 1.6B\nGigawords 10 GB 1.1B\nOSIAN 2.8GB 292 .6M\nOSCAR-MSA 31 GB 3.4B\nOSCAR-Egyptian 32 MB 3.8M\nWiki 1.4GB 156 .5M\nTotal 61 GB 6.5B\nTable 1: ARBERT ’s pre-train resources.\nWe provide relevant size and token count statistics\nabout the datasets in Table 1.\n3.1.2 Training Procedure\nPre-processing. To prepare the raw data for pre-\ntraining, we perform light pre-processing. This\nhelps retain a faithful representation of the natu-\nrally occurring text. We only remove diacritics\nand replace URLs, user mentions, and hashtags\nthat may exist in any of the collections with the\ngeneric string tokens URL, USER, and HASHTAG,\nrespectively. We do not perform any further pre-\nprocessing of the data before splitting the text off to\nwordPieces (Schuster and Nakajima, 2012). Multi-\nlingual models such as mBERT and XLM-R have\n5K (out of 110K) and 14K (out of 250K) Ara-\nbic WordPieces, respectively, in their vocabularies.\nAraBERT employs a vocabulary of 60K (out of\n64K).8 For ARBERT, we use a larger vocabulary\nof 100K WordPieces. For tokenization, we use the\nWordPiece tokenizer (Wu et al., 2016) provided\nby Devlin et al. (2019).\nPre-training. For ARBERT, we follow Devlin\net al. (2019)’s pre-training setup. To generate each\ntraining input sequence, we use the whole word\nmasking, where 15% of the N input tokens are se-\nlected for replacement. These tokens are replaced\n80% of the time with the [MASK] token,10% with\na random token, and 10% with the original token.\nWe use the original implementation of BERT in\nthe TensorFlow framework.9 As mentioned, we\nuse the same network architecture as BERT Base:\n12 layers, 768 hidden units, 12 heads, for a total\nof ∼163M parameters. We use a batch size of\n256 sequences and a maximum sequence length\nof 128 tokens ( 256 sequences ×128 tokens =\n32, 768 tokens/batch) for 8M steps, which is ap-\nproximately 42 epochs over the 6.5B tokens. For\nall our models, we use a learning rate of 1e−4.\n8The empty 4K vocabulary bin is reserved for additional\nwordPieces, if needed.\n9https://github.com/google-research/bert.\nWe pre-train the model on one Google Cloud TPU\nwith eight cores (v2.8) from TensorFlow Research\nCloud (TFRC). 10 Training took ∼ 16 days, for\n42 epochs over all the tokens. Table 2 shows a\ncomparison of ARBERT with mBERT, XLM-R,\nAraBERT, and MARBERT (see Section 3.2) in\nterms of data sources and size, vocabulary size,\nand model parameters.\n3.2 MARBERT\nAs we pointed out in Sections 1 and 2, Arabic has\na large number of diverse dialects. Most of these\ndialects are under-studied due to rarity of resources.\nMultilingual models such as mBERT and XLM-R\nare trained on mostly MSA data, which is also the\ncase for AraBERT and ARBERT. As such, these\nmodels are not best suited for downstream tasks\ninvolving dialectal Arabic. To treat this issue, we\nuse a large Twitter dataset to pre-train a new model,\nMARBERT, from scratch as we describe next.\n3.2.1 Training data\nTo pre-train MARBERT, we randomly sample1B\nArabic tweets from a large in-house dataset of\nabout 6B tweets. We only include tweets with\nat least three Arabic words, based on character\nstring matching, regardless whether the tweet has\nnon-Arabic string or not. That is, we do not re-\nmove non-Arabic so long as the tweet meets the\nthree Arabic word criterion. The dataset makes up\n128GB of text (15.6B tokens).\n3.2.2 Training Procedure\nPre-processing. We employ the same pre-\nprocessing as ARBERT.\nPre-training. We use the same network archi-\ntecture as BERT Base, but without the next sen-\ntence prediction (NSP) objective since tweets are\nshort.11 We use the same vocabulary size ( 100K\nwordPieces) as ARBERT, and MARBERT also has\n∼160M parameters. We train MARBERT for\n17M steps (∼36 epochs) with a batch size of 256\nand a maximum sequence length of 128. Training\ntook ∼40 days on one Google Cloud TPU (eight\ncores). We now present a comparison between our\nmodels and popular multilingual models as well as\nAraBERT.\n10https://www.tensorﬂow.org/tfrc.\n11It was also shown that NSP is not crucial for model per-\nformance (Liu et al., 2019a).\n3.3 Model Comparison\nOur models compare to mBERT (Devlin et al.,\n2019), XLM-R (Conneau et al., 2020) (base and\nlarge), and AraBERT (Antoun et al., 2020) in terms\nof training data size, vocabulary size, and overall\nmodel capacity as we summarize in Table 2. In\nterms of the actual Arabic variety involved, De-\nvlin et al. (2019) train mBERT with Wikipedia\nArabic data, which is MSA. XLM-R (Conneau\net al., 2020) is trained on Common Crawl data,\nwhich likely involves a small amount of Arabic\ndialects. AraBERT is trained on MSA data only.\nARBERT is trained on a large collection of MSA\ndatasets. Unlike all other models, our MAR-\nBERT model is trained on Twitter data, which in-\nvolves both MSA and diverse dialects. We now\ndescribe our ﬁne-tuning setup.\n3.4 Model Fine-Tuning\nWe evaluate our models by ﬁne-tuning them on a\nwide range of tasks, which we thematically orga-\nnize into six clusters: (1) sentiment analysis (SA),\n(2) social meaning (SM) (i.e., age and gender, dan-\ngerous and hateful speech, emotion, irony, and sar-\ncasm), (3) topic classiﬁcation (TC), (4) dialect iden-\ntiﬁcation (DI), (5) named entity recognition (NER),\nand (6) question answering (QA). For all classi-\nﬁcation tasks reported in this paper, we compare\nour models to four other models: mBERT, XLM-\nRBase, XLM-RLarge, and AraBERT. We note that\nXLM-RLarge is ∼3.4×larger than any of our own\nmodels (∼550M parameters vs. ∼160M). We of-\nfer two main types of evaluation: on (i) individual\ntasks, which allows us to compare to other works\non each individual dataset (48 classiﬁcation tasks\non 42 datasets), and (ii) ARLUE clusters (six task\nclusters).\nFor all reported experiments, we follow the same\nlight pre-processing we use for pre-training. For all\nindividual tasks and ARLUE task clusters, we ﬁne-\ntune on the respective training splits for 25 epochs,\nidentifying the best epoch on development data,\nand reporting on both development and test data.12\nWe typically use the exact data splits provided by\noriginal authors of each dataset. Whenever no clear\n12A minority of datasets came with no development split\nfrom source, and so we identify and report the best epoch\nonly on test data for these. This allows us to compare all\nthe models under the same conditions (25 epochs) and report\na fair comparison to the respective original works. For all\nARLUE cluster tasks, we identify the best epoch exclusively\non our development sets (shown in Table 10).\nModels Training Data Vocabulary Conﬁguration\nSource Tokns (ar/all) Tok Size (ar/all) B / L Param.\nmBERT Wiki. 153M/1.5B WP 5K/110K B 110M\nXLM-RB CC 2.9B/295B SP 14K/250K B 270M\nXLM-RL CC 2.9B/295B SP 14K/250K L 550M\nAraBERT 3 sources 2.5B/2.5B SP 60K/64K B 135M\nARBERT 6 sources 6.2B/6.2B WP 100K/100K B 163M\nMARBERT Ara. Tweets 15.6B/15.6B WP 100K/100K B 163M\nTable 2: Models compared. B: Base, L: Large, CC: Common Crawel, SP: SentencePiece, WP: WordPiece.\nsplits are available, or in cases where expensive\ncross-validation was used in source, we divide the\ndata following a standard 80% training, 10% de-\nvelopment, and 10% test split. For all experiments,\nwhether on individual tasks or ARLUE task clus-\nters, we use the Adam optimizer (Kingma and Ba,\n2015) with input sequence length of 256, a batch\nsize of 32, and a learning rate of 2e−6. These\nvalues were identiﬁed in initial experiments based\non development data of a few tasks. 13 We now\nintroduce individual tasks.\n4 Individual Downstream Tasks\n4.1 Sentiment Analysis\nDatasets. We ﬁne-tune the language models\non all publicly available SA datasets we could\nﬁnd in addition to those we acquired directly\nfrom authors. In total, we have the follow-\ning 17 MSA and DA datasets: AJGT (Alo-\nmari et al., 2017), AraNET Sent (Abdul-Mageed\net al., 2020b), AraSenTi-Tweet (Al-Twairesh\net al., 2017), ArSarcasm Sent (Farha and Magdy,\n2020), ArSAS (Elmadany et al., 2018), ArSenD-\nLev (Baly et al., 2019), ASTD (Nabil et al., 2015),\nAW ATIF (Abdul-Mageed and Diab, 2012), BBNS\n& SYTS (Salameh et al., 2015), CAMelSent (Obeid\net al., 2020), HARD (Elnagar et al., 2018),\nLABR (Aly and Atiya, 2013), TwitterAbdullah (Ab-\ndulla et al., 2013), Twitter Saad,14 and SemEval-\n2017 (Rosenthal et al., 2017). Details about the\ndatasets and their splits are in Section A.1.\nBaselines. We compare to the STOA listed in Ta-\nble 3 and Table 4 captions. For all datasets with\nno baseline in Table 3, we consider AraBERT our\nbaseline. Details about SA baselines are in Sec-\ntion A.2.\n13NER and QA are expetions, where we use sequence\nlengths of 128 and 384, respectively; a batch sizes of 16 for\nboth; and a learning rate of 2e−6 and 3e−5, respectively.\n14www.kaggle.com/mksaad/arabic-sentiment-twitter.\nDataset (classes) SOTA mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT\nArSAS (3)92.00⋆ 87.50 90.00 91.50 91.00 92.00 93.00ASTD (3)73.00⋆ 67.00 60.67 67.67 72.00 76.50 78.00SemEval (3)69.00⋆ 57.00 64.00 67.00 62.00 69.00 71.00AraNETSent(2) 76.20† 84.00 92.00 93.00 86.50 89.00 92.00ArSarcSent(3) - 60.50 63.50 70.00 63.50 68.00 71.50AraSenTi (3) -89.50 92.00 93.50 91.00 90.00 90.00BBN (3) - 55.50 69.50 72.00 70.00 76.50 79.00SYTS (3) - 67.00 78.00 76.50 75.50 79.00 76.50TwSaad(2) - 79.00 95.00 95.00 81.00 90.00 96.00SAMAR (5) - 22.50 54.00 57.00 36.50 43.50 55.50AW ATIF (4) -60.50 63.50 68.50 66.50 71.50 72.50TwAbdullah(2) - 81.50 91.00 92.00 89.50 91.50 95.00\nTable 3: SA results (I) in F1\nPN . ⋆ Obeid et al. (2020); † Abdul-\nMageed et al. (2020b). Default baseline is AraBERT.\nDataset (classes) SOTA mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT\nAJGT (2)93.80 86.67 89.44 91.94 92.22 94.44 96.11HARD (2)96.20 95.54 95.74 95.96 95.89 96.12 96.17ArsenTD-LEV (5)59.40 50.50 55.25 62.00 56.13 61.38 60.38LABR (2)86.70 91.20 91.23 92.20 91.97 92.51 92.49ASTD-B(2)92.60 79.32 87.59 77.44 83.08 93.23 96.24\nTable 4: SA results (II) in Acc. SOTA by Antoun et al. (2020).\nResults. To facilitate comparison to previous\nworks with the appropriate evaluation metrics, we\nsplit our results into two tables: We show results\nin F1PN in Table 3 and F1 in Table 4. We typically\nbold the best result on each dataset. Our models\nachieve best results in 13 out of the 17 classiﬁ-\ncation tasks reported in the two tables combined ,\nwhile XLM-R (which is a much larger model)\noutperforms our models in the 4 remaining tasks.\nWe also note that XLM-R acquires better results\nthan AraBERT in the majority of tasks, a trend\nthat continues for the rest of tasks. Results also\nclearly show that MARBERT is more powerful\nthan than ARBERT. This is due to MARBERT’s\nlarger and more diverse pre-training data, espe-\ncially that many of the SA datasets involve dialects\nand come from social media.\n4.2 Social Meaning Tasks\nWe collectively refer to a host of tasks as social\nmeaning. These are age and gender detection; dan-\ngerous, hateful, and offensive speech detection;\nemotion detection; irony detection; and sarcasm\ndetection. We now describe datasets we use for\nTask (classes) SOTA mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT\nAge (3)51.42‡‡56.35 59.73 53.60 57.72 58.95 62.27Dangerous (2)59.60† 62.66 62.76 65.01 64.37 63.21 67.53Emotion (8)60.32‡‡65.79 70.67 74.89 65.68 67.73 75.83Gender (2)65.30‡‡68.06 71.00 71.14 67.75 69.86 72.62Hate (2)82.28∗∗ 72.81 71.33 79.31 78.89 83.02 84.79Irony (2)82.40‡ 80.96 81.97 82.52 83.01 85.59 85.33Offensive (2)90.51∗ 84.25 85.26 88.28 86.57 90.38 92.41Sarcasm (2)46.60‡‡68.20 66.76 69.23 72.23 75.04 76.30\nTable 5: Results on social meaning tasks. F1 score is the\nevaluation metric. ⋆ Hassan et al. (2020), ⋆⋆ Djandji et al.\n(2020), ‡ Zhang and Abdul-Mageed (2019a), † Alshehri\net al. (2020), †† Farha and Magdy (2020), ‡‡ Abdul-Mageed\net al. (2020b).\neach of these tasks.\nDatasets. For both age and gender, we use\nArap-Tweet (Zaghouani and Charﬁ, 2018). We\nuse AraDan (Alshehri et al., 2020) for dangerous\nspeech. For offensive language and hate speech,\nwe use the dataset released in the shared task (sub-\ntasks A and B) of offensive speech by Mubarak\net al. (2020). We also use AraNET Emo (Abdul-\nMageed et al., 2020b), IDAT@FIRE2019 (Ghanem\net al., 2019), and ArSarcasm (Farha and Magdy,\n2020) for emotion, irony and sarcasm, respectively.\nMore information about these datasets and their\nsplits is in Appendix B.1.\nBaselines. Baselines for social meaning tasks are\nthe SOTA listed in Table 5 caption. Details about\neach baseline is in Appendix B.2.\nResults. As Table 5 shows, our models acquire\nbest results on all eight tasks. Of these, MAR-\nBERT achieves best performance on seven tasks,\nwhile ARBERT is marginally better than MAR-\nBERT on one task (irony@FIRE2019). The size-\nable gains MARBERT achieves reﬂects its su-\nperiority on social media tasks. On average,\nour models are 9.83 F1 better than all previous\nSOTA.\n4.3 Topic Classiﬁcation\nClassifying documents by topic is a classical task\nthat still has practical utility. We use four TC\ndatasets, as follows:\nDatasets. We ﬁne-tune on Arabic News Text\n(ANT) (Chouigui et al., 2017) under three pre-\ntaining settings (title only, text only, and title+text.),\nKhaleej (Abbas et al., 2011), and OSAC (Saad and\nAshour, 2010). Details about these datasets and the\nclasses therein are in Appendix C.1.\nBaselines. Since, to the best of our knowledge,\nthere are no published results exploiting deep learn-\ning on TC, we consider AraBERT a strong baseline.\nResults. As Table 6 shows, ARBERT acquires\nDataset (classes) mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT\nANTText (5) 84.89 85 .77 86 .72 88.17 86.87 85 .27ANTTitle (5) 78.29 79 .96 81 .25 81 .03 81.70 81.19ANTText+Title (5)84.67 86 .21 86 .96 87.22 87.21 85 .60Khallej (4) 92.81 91 .87 93 .56 93 .83 94.53 93.63OSAC (10) 96.84 97 .15 98.20 97.03 97 .50 97 .23\nTable 6: Performance on TC tasks. Our baseline is AraBERT.\nDataset (classes) Task SOTA mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERTArSarcDia(5) Regoin - 43.81 41.71 41.83 47.54 54.70 51.27MADAR (21) Country -34.92 35.91 35.14 34.87 37.90 40.40AOC (4) Region82.45⋆ 77.27 77.34 78.77 79.20 81.09 82.37AOC (3) Region78.81⋆ 85.76 86.39 87.56 87.68 89.06 90.85AOC (2) Binary87.23⋆ 86.19 86.85 87.30 87.76 88.46 88.59QADI (18) Country60.60† 66.57 77.00 82.73 72.23 88.63 90.89NADI (21) Country26.78‡ 13.32 16.36 17.17 17.46 22.56 29.14NADI (100) Province06.06††02.13 04.12 5.30 03.13 06.10 06.28\nTable 7: DIA results in F 1. ⋆ Elaraby and Abdul-Mageed\n(2018), † Abdelali et al. (2020), ‡ El Mekki et al. (2020), ††\nTalafha et al. (2020). Default baseline is AraBERT.\nbest results on both OSAC and Khaleej, and the\ntitle-only setting of ANT. AraBERT slightly out-\nperforms our models on the text-only and title+text\nsettings of ANT.\n4.4 Dialect Identiﬁcation\nArabic dialect identiﬁcation can be performed at\ndifferent levels of granularity, including binary (i.e.,\nMSA-DA), regional (e.g., Gulf, Levantine), coun-\ntry level (e.g., Algeria, Morocco), and recently\nprovince level (e.g., the Egyptian province ofCairo,\nthe Saudi province ofAl-Madinah) (Abdul-Mageed\net al., 2020a, 2021b).\nDatasets. We ﬁne-tune our models on the\nfollowing datasets: Arabic Online Commentary\n(AOC) (Zaidan and Callison-Burch, 2014),\nArSarcasmDia (Farha and Magdy, 2020), 15\nMADAR (sub-task 2) (Bouamor et al., 2019),\nNADI-2020 (Abdul-Mageed et al., 2020a), and\nQADI (Abdelali et al., 2020). Details about these\ndatasets are in Table D.1.\nBaselines. Our baselines are marked in Table 7 cap-\ntion. Details about the baselines are in Table D.2.\nResults. As Table 7 shows, our models outper-\nform all SOTA as well as the baseline AraBERT\nacross all classiﬁcation levels with sizeable mar-\ngins. These results reﬂect the powerful and di-\nverse dialectal representation of MARBERT, en-\nabling it to serve wider communities . Although\nARBERT is developed mainly for MSA, it also\noutperforms all other models.\n4.5 Named Entity Recognition\nWe ﬁne-tune the models on ﬁve NER datasets.\nDatasets. We use ACE03NW and ACE03BN\n(Mitchell et al., 2004), ACE04NW (Mitchell et al.,\n15ArSarcasmDia carries regional dialect labels.\nDataset SOTA mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT\nANERcorp88.77 86.78 87.24 89.94 89.13 84.38 80 .64ACE04NW91.4786.37 89.93 89.89 89.03 88.24 85 .02ACE03BN94.92 91.23 53.97 85.41 91.94 96.18 79.05ACE03NW91.2081.40 87.24 90.62 88.09 90.09 87 .76TW-NER65.34 36.83 49.16 54.44 41.26 59.17 66.67\nTable 8: NER results in F 1. SOTA by Khalifa and Shaalan\n(2019).\n2004), ANERcorp (Benajiba and Rosso, 2007), and\nTW-NER (Darwish, 2013). Table E.1 shows the\ndistribution of named entity classes across the ﬁve\ndatasets.\nBaseline. We compare our results with SOTA pre-\nsented by Khalifa and Shaalan (2019) and follow\nthem in focusing on person (PER), location (LOC)\nand organization (ORG) named entity labels while\nsetting other labels to the unnamed entity (O). De-\ntails about Khalifa and Shaalan (2019) SOTA mod-\nels are in Appendix E.2.\nResults. As Table 8 shows, our models outperform\nSOTA on two out of the ﬁve NER datasets. We\nnote that even though SOTA (Khalifa and Shaalan,\n2019) employ a complex combination of CNNs and\ncharacter-level LSTMs, which may explain their\nbetter results on two datasets, MARBERT still\nachieves highest performance on the social me-\ndia dataset (TW-NER).\n4.6 Question Answering\nDatasets. We use ARCD (Mozannar et al., 2019)\nand the three human translated Arabic test sec-\ntions of the XTREME benchmark (Hu et al., 2020):\nMLQA (Lewis et al., 2020), XQuAD (Artetxe et al.,\n2020), and TyDi QA (Artetxe et al., 2020). Details\nabout these datasets are in Table F.1.\nBaselines. We compare to Antoun et al. (2020)\nand consider their system a baseline on ARCD. We\nfollow the same splits they used where we ﬁne-tune\non Arabic SQuAD (Mozannar et al., 2019) and\n50% of ARCD and test on the remaining 50% of\nARCD (ARCD-test). For all other experiments, we\nﬁne-tune on the Arabic machine translated SQuAD\n(AR-XTREME) from the XTREME multilingual\nbenchmark (Hu et al., 2020) and test on the human\ntranslated test sets listed above. Our baselines in\nthese is Hu et al. (2020)’s mBERTBase model on\ngold (human) data.\nResults. As is standard, we report QA results in\nterms of both Exact Match (EM) and F1. We ﬁnd\nthat results with ARBERT and MARBERT on QA\nare not competitive, a clear discrepancy from what\nwe have observed thus far on other tasks. We\nhypothesize this is because the two models are\npre-trained with a sequence length of only 128,\nwhich does not allow them to sufﬁciently capture\nboth a question and its likely answer within the\nsame sequence window during the pre-training.16\nTo rectify this, we further pre-train the stronger\nmodel, MARBERT, on the same MSA data as AR-\nBERT in addition to AraNews dataset (Nagoudi\net al., 2020) (8.6GB), but with a bigger sequence\nlength of 512 tokens for 40 epochs. We call this\nfurther pre-trained model MARBERT-v2, noting\nit has 29B tokens. As Table 9 shows, MARBERT-\nv2 acquires best performance on all but one test\nset, where XLM-RLarge marginally outperforms us\n(only in F1).\n5 ARLUE\n5.1 ARLUE Categories\nWe concatenate the corresponding splits of the in-\ndividual datasets to form ARLUE, which is a con-\nglomerate of task clusters. That is, we concatenate\nall training data from each group of tasks into a\nsingle TRAIN, all development into a single DEV ,\nand all test into a single TEST. One exception is\nthe social meaning tasks whose data we keep inde-\npendent (see ARLUESM below). Table 10 shows\na summary of the ARLUE datasets. 17 We now\nbrieﬂy describe how we merge individual datasets\ninto ARLUE.\nARLUESenti. To construct ARLUESenti, we col-\nlapse the labels very negative into negative, very\npositive into positive, and objective into neu-\ntral, and remove the mixed class. This gives us\nthe 3 classes negative, positive, and neutral for\nARLUESenti. Details are in Table A.1.\nARLUESM. We refer to the different social mean-\ning datasets collectively as ARLUESM. We do not\nmerge these datasets to preserve the conceptual co-\nherence speciﬁc to each of the tasks. Details about\nindividual datasets in ARLUESM are in B.1.\nARLUETopic. We straightforwardly merge the TC\ndatasets to form ARLUETopic, without modifying\nany class labels. Details of ARLUETopic data are in\nTable C.1.\nARLUEDia. We construct three ARLUE Dia cat-\negories. Namely, we concatenate the AOC\nand AraSarcasm Dia MSA-DA classes to form\nARLUEDia-B (binary) and the region level classes\n16In addition, MARBERT is not trained on Wikipedia data\nfrom where some questions come.\n17Again, ARLUESM datasets are kept independent, but to\nprovide a summary of all ARLUE datasets we collate the\nnumbers in Table 10.\nDataset SOTA mBERT XLM-R B XLM-RL AraBERT ARBERT MARBERT MARBERT(v2)\nEM F1 EM F1 EM F1 EM F1 EM F1 EM F1 EM F1 EM F1\nARCD-test⋆ 30.10† 61.20† 29.63 60.93 30.20 59.55 32.05 64.77 30.20 62.30 30.34 63.89 21.65 54.06 36.75 68.86\nARCD-test - - 26.64 58.86 27.31 59.61 28.11 62.08 25.64 59.92 27.21 60.73 23.22 55.14 29.63 63.05\nAR-MLQA39.00‡ 58.90‡ 32.93 51.57 32.93 53.35 38.11 60.00 35.43 55.42 34.15 53.65 28.02 45.14 39.23 59.39\nAR-XQuAD54.20‡ 71.00‡ 48.66 66.26 45.88 64.91 51.85 72.19 51.60 68.79 49.92 67.90 41.09 58.46 56.55 72.48\nAR-TyiDQA39.00‡ 58.90‡ 46.36 64.02 39.41 60.99 44.41 67.06 44.19 64.39 46.80 66.94 38.98 57.51 47.45 67.67\nTable 9: QA results. ⋆ Results on this test set are with models using the same training data as Antoun et al. (2020), while rest of\nrows report models trained with AR-XTREME (Hu et al., 2020). † Antoun et al. (2020); ‡ Hu et al. (2020).\nDataset #Datasets Task TRAIN DEV TEST\nARLUESenti 17 SA 190.9K 6.5K 44.2K\nARLUESM⋆ 8 SM 1.51M 162.5K 166.1K\nARLUETopic 5 TC 47.5K 5.9K 5.9K\nARLUEDia-B 2 DI 94.9K 10.8K 12.9K\nARLUEDia-R 2 DI 38.5K 4.5K 5.3K\nARLUEDia-C 3 DI 711.9K 31.5K 52.1K\nARLUENER† 5 NER 227.7K 44.1K 66.5K\nARLUEQA‡ 4 QA 101.6K 517 7 .45K\nTable 10: ARLUE categories across the different data splits.\n⋆ Refer to Table B.1 for details about individual social mean-\ning datasets in ARLUESM. † Statistics are at the token level.\n‡ Number of question-answer pairs.\nfrom the same two datasets to acquireARLUEDia-R\n(4-classes, region). We then merge the country\nclasses from the rest of datasets to getARLUEDia-C\n(21-classes, country). Details are in Table D.1.\nARLUENER & ARLUEQA. We straightforwardly\nconcatenate all corresponding splits from the dif-\nferent NER and QA datasets to form ARLUENER\nand ARLUEQA, respectively. Details of each of\nthese task clusters data are in Tables E.1 and F.1,\nrespectively.\n5.2 Evaluation on ARLUE\nWe present results on each task cluster indepen-\ndently using the relevant metric for both the devel-\nopment split (Table 11) and test split (Table 12).\nInspired by McCann et al. (2018) and Wang et al.\n(2018) who score NLP systems based on their per-\nformance on multiple datasets, we introduce an\nARLUE score. The ARLUE score is simply the\nmacro-average of the different scores across all\ntask clusters, weighting each task equally. Fol-\nlowing Wang et al. (2018), for tasks with multi-\nple metrics (e.g., accuracy and F1), we use an un-\nweighted average of the metrics as the score for the\ntask when computing the overall macro-average.\nAs Table 12 shows, our MARBERT-v2 model\nachieves the highest ARLUE score ( 77.40), fol-\nlowed by XLM-RL (76.55) and ARBERT (76.07).\nWe also note that in spite of its superiority on social\ndata, MARBERT ranks top 4. This is due to MAR-\nBERT suffering on the QA tasks (due to its short\ninput sequence length), and to a lesser extent on\nNER and TC.\n6 Related Work\nEnglish and Multilingual LMs. Pre-trained LMs\nexploiting a self-supervised objective with mask-\ning such as BERT (Devlin et al., 2019) and\nRoBERTa (Liu et al., 2019b) have revolutionized\nNLP. Multilingual versions of these models such\nas mBERT and XLM-RoBERTa (Conneau et al.,\n2020) were also pre-trained. Other models with\ndifferent objectives and/or architectures such as\nALBERT (Lan et al., 2019), T5 (Raffel et al., 2020)\nand its multilingual version, mT5 (Xue et al., 2021),\nand GPT3 (Brown et al., 2020) were also intro-\nduced. More information about BERT-inspired\nLMs can be found in Rogers et al. (2020).\nNon-English LMs. Several models dedicated\nto individual languages other than English have\nbeen developed. These include AraBERT (An-\ntoun et al., 2020) and ArabicBERT (Safaya et al.,\n2020) for Arabic, Bertje for Dutch (de Vries et al.,\n2019), CamemBERT (Martin et al., 2020) and\nFlauBERT (Le et al., 2020) for French, PhoBERT\nfor Vietnamese (Nguyen and Tuan Nguyen, 2020),\nand the models presented by Virtanen et al.\n(2019) for Finnish, Dadas et al. (2020) for Polish,\nand Malmsten et al. (2020) for Swedish. Pyysalo\net al. (2020) also create monolingual LMs for 42\nlanguages exploiting Wikipedia data. Our models\ncontributed to this growing work of dedicated LMs,\nand has the advantage of covering a wide range\nof dialects. Our MARBERT and MARBERT-v2\nmodels are also trained with a massive scale social\nmedia dataset, endowing them with a remarkable\nability for real-world downstream tasks.\nNLP Benchmarks. In recent years, several NLP\nbenchmarks were designed for comparative eval-\nuation of pre-trained LMs. For English, McCann\net al. (2018) introduced NLP Decathlon (DecaNLP)\nwhich combines 10 common NLP datasets/tasks.\nWang et al. (2018) proposed GLUE, a popular\nbenchmark for evaluating nine NLP tasks. Wang\net al. (2019) also presented SuperGLUE, a more\nDataset mBERT XLM-R B XLM-RL AraBERT ARBERT MARBERT MARBERT (v2)\nARLUESenti⋆ 79.02/ 79.50 92.17/ 93.00 93.18/ 94.00 78.26/ 78.50 87.96/ 88.50 93.30/ 94.00 92.82/ 93.50\nARLUESM† 66.84/ 61.76 69.18/ 64.12 68.79/ 64.20 67.63/ 62.11 69.12/ 64.23 71.64/ 68.38 70.43/ 66.26\nARLUETopic 91.10/ 91.67 91.57/ 92.24 92.66/ 93.53 92.42/ 93.17 91.06/ 92.23 90.48/ 92.01 91 .52/ 92.50\nARLUEDia-B 87.83/ 87.50 88.20/ 87.93 88.92/ 88.57 89.30/ 89.06 89.53/ 89.23 89.80/ 89.50 90.05/ 89.72\nARLUEDia-R 86.45/ 85.89 86.00/ 85.46 86.97/ 86.54 87.30/ 86.92 88.85/ 88.49 90.94/ 90.65 90.04/ 89.67\nARLUEDia-C 41.08/ 32.03 40.59/ 31.75 39.73/ 31.51 37.90/ 30.41 42.51/ 34.26 43.54/ 34.25 45.37/ 35.94\nARLUENER 96.81/ 76.91 97.74/ 84.09 97.97/ 85.56 97.79/ 83.67 97.46/ 81.21 96.89/ 76.58 97 .18/ 79.34\nARLUEQA‡ 32.30/ 51.14 32.30/ 52.43 35.18/ 58.08 31.72/ 51.87 34.04/ 54.34 27.27/ 43.67 37.14/ 57.93\nAverage 72.68/ 70.80 74.72/ 73.88 75.43/ 75.79 75.75/ 71.96 75.07/ 74.06 75.48/ 73.63 76.82/ 75.61\nARLUEScore 71.74 74 .30 75 .34 72 .38 74 .56 74 .56 76.21\nTable 11: Performance of our models on the DEV splits of ARLUE. ⋆ Metric for ARLUESenti is F1\nPN. † ARLUESM results is\nthe average score across the social meaning tasks described in Table B.2. ‡ Metric for ARLUEQA is Exact Match (EM) / F1.\nDataset mBERT XLM-R B XLM-RL AraBERT ARBERT MARBERT MARBERT (v2)\nARLUESenti⋆ 79.02/ 79.50 92.17/ 93.00 93.18/ 94.00 78.26/ 78.50 87.96/ 88.50 93.30/ 94.00 93.30/ 94.00\nARLUESM† 77.76/ 69.88 79.81/ 71.19 80.01/ 73.00 78.84/ 72.03 80.39/ 74.22 82.35/ 77.13 76.34/ 77.13\nARLUETopic 90.88/ 92.12 90.90/ 91.81 92.24/ 93.40 92.15/ 92.97 90.81/ 92.65 89.67/ 90.97 90 .07/ 91.54\nARLUEDia-B 85.52/ 84.88 86.54/ 85.98 87.82/ 87.17 87.74/ 87.21 88.31/ 87.74 88.72/ 88.19 88.47/ 87.87\nARLUEDia-R 86.45/ 85.89 86.00/ 85.46 86.97/ 86.54 87.30/ 86.92 88.85/ 88.49 90.94/ 90.65 90.04/ 89.67\nARLUEDia-C 42.80/ 35.23 42.67/ 35.40 41.94/ 34.98 39.71/ 33.56 44.44/ 36.87 45.89/ 37.69 47.49/ 38.53\nARLUENER 95.90/ 69.06 96.02/ 73.27 96.13/ 74.94 96.76/ 76.19 97.00/ 76.83 96.38/ 71.93 96.75/ 74.70\nARLUEQA‡ 34.34/ 55.74 34.62/ 56.67 39.37/ 63.12 36.31/ 58.10 36.29/ 57.81 29.13/ 48.83 40.47/ 62.09\nAverage 74.08/ 71.54 76.09/ 74.10 77.21/ 75.89 74.63/ 73.19 76.76/ 75.39 77.05/ 74.92 77.87/ 76.94\nARLUEScore 72.81 75 .09 76 .55 73 .91 76 .07 75 .99 77.40\nTable 12: Performance of our models on the TEST splits of ARLUE (Acc / F 1). ⋆ Metric for ARLUESenti is Acc/ F1\nPN. †\nARLUESM results is the average score across the social meaning tasks described in Table 5. ‡ Metric for ARLUEQA is Exact\nMatch (EM) / F1.\nchallenging benchmark than GLUE covering seven\ntasks. In the cross-lingual setting, Hu et al. (2020)\nprovide a Cross-lingual TRansfer Evaluation of\nMultilingual Encoders (XTREME) benchmark for\nthe evaluation of cross-lingual transfer learning\ncovering nine tasks for 40 languages (12 language\nfamilies). ARLUE complements these bench-\nmarking efforts, and is focused on Arabic and\nits dialects. ARLUE is also diverse (involves\n42 datasets) and challenging (our best ARLUE\nscore is at 77.40).\n7 Conclusion\nWe presented our efforts to develop two power-\nful Transformer-based language models for Arabic.\nOur models are trained on large-to-massive datasets\nthat cover different domains and text genres, includ-\ning social media. By pre-training MARBERT and\nMARBERT-v2 on dialectal Arabic, we aim at en-\nabling downstream NLP technologies that serve\nwider and more diverse communities. Our best\nmodels perform better than (or on par with) XLM-\nRLarge (∼3.4×larger than our models), and hence\nare more energy efﬁcient at inference time. Our\nmodels are also signiﬁcantly better than AraBERT,\nthe currently best-performing Arabic pre-trained\nLM. We also introduced AraLU, a large and di-\nverse benchmark for Arabic NLU composed of 42\ndatasets thematically organized into six main task\nclusters. ARLUE ﬁlls a critical gap in Arabic and\nmultilingual NLP, and promises to help propel inno-\nvation and facilitate meaningful comparisons in the\nﬁeld. Our models are publicly available. We also\nplan to publicly release our ARLUE benchmark.\nIn the future, we plan to explore self-training our\nlanguage models as a way to improve performance\nfollowing Khalifa et al. (2021). We also plan to in-\nvestigate developing more energy efﬁcient models.\nAcknowledgements\nWe gratefully acknowledge support from the Nat-\nural Sciences and Engineering Research Council\nof Canada, the Social Sciences and Humanities Re-\nsearch Council of Canada, Canadian Foundation\nfor Innovation, Compute Canada and UBC ARC-\nSockeye (https://doi.org/10.14288/SOCKEYE).\nWe also thank the Google TFRC program for pro-\nviding us with free TPU access.\nEthical Considerations\nAlthough our language models are pre-trained us-\ning datasets that were public at the time of collec-\ntion, parts of these datasets might become private\nor get removed (e.g., tweets that are deleted by\nusers). For this reason, we will not release or re-\ndistribute any of the pre-training datasets. Data\ncoverage is another important consideration: Our\ndatasets have wide coverage, and one of our con-\ntributions is offering models that can serve more\ndiverse communities in better ways than existing\nmodels. However, our models may still carry bi-\nases that we have not tested for and hence we rec-\nommend they be used with caution. Finally, our\nmodels deliver better performance than larger-sized\nmodels and as such are more energy conserving.\nHowever, smaller models that can achieve simply\n‘good enough’ results should also be desirable. This\nis part of our own future research, and the commu-\nnity at large is invited to develop novel methods\nthat are more environment friendly.\nReferences\nMourad Abbas, Kamel Sma ¨ıli, and Daoud Berkani.\n2011. Evaluation of topic identiﬁcation methods on\narabic corpora. JDIM, 9(5):185–192.\nAhmed Abdelali, Hamdy Mubarak, Younes Samih,\nSabit Hassan, and Kareem Darwish. 2020. Arabic\nDialect Identiﬁcation in the Wild. Proceedings of\nthe Sixth Arabic Natural Language Processing Work-\nshop.\nMuhammad Abdul-Mageed, Mona Diab, and Sandra\nK¨ubler. 2014. Samar: Subjectivity and sentiment\nanalysis for arabic social media. Computer Speech\n& Language, 28(1):20–37.\nMuhammad Abdul-Mageed and Mona T Diab. 2012.\nAW ATIF: A Multi-Genre Corpus for Modern Stan-\ndard Arabic Subjectivity and Sentiment Analysis. In\nLREC, volume 515, pages 3907–3914. Citeseer.\nMuhammad Abdul-Mageed, Shady Elbassuoni, Jad\nDoughman, AbdelRahim Elmadany, El Moatez Bil-\nlah Nagoudi, Yorgo Zoughby, Ahmad Shaher, Iskan-\nder Gaba, Ahmed Helal, and Mohammed El-Razzaz.\n2021a. DiaLex: A benchmark for evaluating multi-\ndialectal Arabic word embeddings. In Proceedings\nof the Sixth Arabic Natural Language Processing\nWorkshop, pages 11–20, Kyiv, Ukraine (Virtual). As-\nsociation for Computational Linguistics.\nMuhammad Abdul-Mageed, Chiyu Zhang, Houda\nBouamor, and Nizar Habash. 2020a. NADI\n2020: The ﬁrst nuanced Arabic dialect identiﬁcation\nshared task. In Proceedings of the Fifth Arabic Nat-\nural Language Processing Workshop, pages 97–110,\nBarcelona, Spain (Online). Association for Compu-\ntational Linguistics.\nMuhammad Abdul-Mageed, Chiyu Zhang, Abdel-\nRahim Elmadany, Houda Bouamor, and Nizar\nHabash. 2021b. NADI 2021: The second nuanced\nArabic dialect identiﬁcation shared task. In Proceed-\nings of the Sixth Arabic Natural Language Process-\ning Workshop, pages 244–259, Kyiv, Ukraine (Vir-\ntual). Association for Computational Linguistics.\nMuhammad Abdul-Mageed, Chiyu Zhang, Azadeh\nHashemi, and El Moatez Billah Nagoudi. 2020b.\nAraNet: A deep learning toolkit for Arabic social\nmedia. In Proceedings of the 4th Workshop on\nOpen-Source Arabic Corpora and Processing Tools,\nwith a Shared Task on Offensive Language Detec-\ntion, pages 16–23, Marseille, France. European Lan-\nguage Resource Association.\nNawaf Abdulla, N Mahyoub, M Shehab, and Mah-\nmoud Al-Ayyoub. 2013. Arabic sentiment analysis:\nCorpus-based and lexicon-based. In Proceedings\nof The IEEE conference on Applied Electrical En-\ngineering and Computing Technologies (AEECT).\nNora Al-Twairesh, Hend Al-Khalifa, AbdulMalik Al-\nSalman, and Yousef Al-Ohali. 2017. Arasenti-tweet:\nA corpus for Arabic sentiment analysis of saudi\ntweets. Procedia Computer Science, 117:63–72.\nHassan Alhuzali, Muhammad Abdul-Mageed, and\nLyle Ungar. 2018. Enabling deep learning of emo-\ntion with ﬁrst-person seed expressions. In Pro-\nceedings of the Second Workshop on Computational\nModeling of People’s Opinions, Personality, and\nEmotions in Social Media, pages 25–35.\nKhaled Mohammad Alomari, Hatem M ElSherif, and\nKhaled Shaalan. 2017. Arabic tweets sentimental\nanalysis using machine learning. In International\nConference on Industrial, Engineering and Other\nApplications of Applied Intelligent Systems , pages\n602–610. Springer.\nAli Alshehri, El Moatez Billah Nagoudi, and Muham-\nmad Abdul-Mageed. 2020. Understanding and de-\ntecting dangerous speech in social media. In Pro-\nceedings of the 4th Workshop on Open-Source Ara-\nbic Corpora and Processing Tools, with a Shared\nTask on Offensive Language Detection, pages 40–47,\nMarseille, France. European Language Resource As-\nsociation.\nMohamed Aly and Amir Atiya. 2013. LABR: A Large\nScale Arabic book Reviews Dataset. In Proceed-\nings of the 51st Annual Meeting of the Association\nfor Computational Linguistics (Volume 2: Short Pa-\npers), volume 2, pages 494–498.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nArabert: Transformer-based model for arabic lan-\nguage understanding. In Proceedings of the 4th\nWorkshop on Open-Source Arabic Corpora and Pro-\ncessing Tools, with a Shared Task on Offensive Lan-\nguage Detection, pages 9–15.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the Cross-lingual Transferability of Mono-\nlingual Representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637.\nRamy Baly, Alaa Khaddaj, Hazem Hajj, Wassim El-\nHajj, and Khaled Bashir Shaban. 2019. ArSentD-\nLEV: A multi-topic corpus for target-based senti-\nment analysis in Arabic levantine tweets. arXiv\npreprint arXiv:1906.01830.\nYassine Benajiba and Paolo Rosso. 2007. ANERsys\n2.0: Conquering the NER Task for the Arabic Lan-\nguage by Combining the Maximum Entropy with\nPOS-tag Information. In IICAI, pages 1814–1823.\nHouda Bouamor, Sabit Hassan, and Nizar Habash.\n2019. The MADAR shared task on Arabic ﬁne-\ngrained dialect identiﬁcation. In Proceedings of the\nFourth Arabic Natural Language Processing Work-\nshop, pages 199–207.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In Advances in Neural Information Pro-\ncessing Systems, volume 33, pages 1877–1901. Cur-\nran Associates, Inc.\nAmina Chouigui, Oussama Ben Khiroun, and Bilel\nElayeb. 2017. ANT Corpus : An Arabic News\nText Collection for Textual Classiﬁcation. In 2017\nIEEE/ACS 14th International Conference on Com-\nputer Systems and Applications (AICCSA) , pages\n135–142. IEEE.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, et al. 2020.\nUnsupervised Cross-lingual Representation Learn-\ning at Scale. Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguis-\ntics.\nSławomir Dadas, Michał Perełkiewicz, and Rafał\nPo´swiata. 2020. Pre-training Polish Transformer-\nbased Language Models at Scale. Artiﬁcial Intelli-\ngence and Soft Computing.\nKareem Darwish. 2013. Named Entity Recognition us-\ning Cross-lingual Resources: Arabic as an Example.\nIn Proceedings of the 51st Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 1558–1567.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186.\nMarc Djandji, Fady Baly, Wissam Antoun, and Hazem\nHajj. 2020. Multi-Task Learning using AraBert for\nOffensive Language Detection. In Proceedings of\nthe 4th Workshop on Open-Source Arabic Corpora\nand Processing Tools, with a Shared Task on Offen-\nsive Language Detection , pages 97–101, Marseille,\nFrance. European Language Resource Association.\nIbrahim Abu El-Khair. 2016. 1.5 billion words Arabic\nCorpus. arXiv preprint arXiv:1611.04033.\nAbdellah El Mekki, Ahmed Alami, Hamza Alami,\nAhmed Khoumsi, and Ismail Berrada. 2020.\nWeighted combination of BERT and N-GRAM fea-\ntures for Nuanced Arabic Dialect Identiﬁcation. In\nProceedings of the Fourth Arabic Natural Language\nProcessing Workshop, Barcelona, Spain.\nMohamed Elaraby and Muhammad Abdul-Mageed.\n2018. Deep models for Arabic dialect identiﬁcation\non benchmarked data. In Proceedings of the Fifth\nWorkshop on NLP for Similar Languages, Varieties\nand Dialects (VarDial 2018), pages 263–274, Santa\nFe, New Mexico, USA. Association for Computa-\ntional Linguistics.\nAbdelRahim Elmadany, Hamdy Mubarak, and Walid\nMagdy. 2018. ArSAS: An Arabic Speech-Act and\nSentiment Corpus of Tweets. OSACT, 3:20.\nAshraf Elnagar, Yasmin S Khalifa, and Anas Einea.\n2018. Hotel Arabic-Reviews Dataset Construction\nfor Sentiment Analysis Applications. In Intelligent\nNatural Language Processing: Trends and Applica-\ntions, pages 35–52. Springer.\nIbrahim Abu Farha and Walid Magdy. 2020. From Ara-\nbic Sentiment Analysis to Sarcasm Detection: The\nArSarcasm Dataset. In Proceedings of the 4th Work-\nshop on Open-Source Arabic Corpora and Process-\ning Tools, with a Shared Task on Offensive Language\nDetection, pages 32–39.\nBilal Ghanem, Jihen Karoui, Farah Benamara,\nV´eronique Moriceau, and Paolo Rosso. 2019.\nIDAT@FIRE2019: Overview of the Track on Irony\nDetection in Arabic Tweets. . In Mehta P ., Rosso P .,\nMajumder P ., Mitra M. (Eds.) Working Notes of the\nForum for Information Retrieval Evaluation (FIRE\n2019). CEUR Workshop Proceedings. In: CEUR-\nWS.org, Kolkata, India, December 12-15.\nSabit Hassan, Younes Samih, Hamdy Mubarak, Ahmed\nAbdelali, Ammar Rashed, and Shammur Absar\nChowdhury. 2020. ALT Submission for OSACT\nShared Task on Offensive Language Detection. In\nProceedings of the 4th Workshop on Open-Source\nArabic Corpora and Processing Tools, with a Shared\nTask on Offensive Language Detection, pages 61–65,\nMarseille, France. European Language Resource As-\nsociation.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In Proceedings of the 37th International\nConference on Machine Learning , volume 119 of\nProceedings of Machine Learning Research , pages\n4411–4421. PMLR.\nFatemah Husain. 2020. OSACT4 Shared Task\non Offensive Language Detection: Intensive\nPreprocessing-Based Approach. In Proceedings of\nthe 4th Workshop on Open-Source Arabic Corpora\nand Processing Tools, with a Shared Task on Offen-\nsive Language Detection , pages 53–60, Marseille,\nFrance. European Language Resource Association.\nMuhammad Khalifa, Muhammad Abdul-Mageed, and\nKhaled Shaalan. 2021. Self-training pre-trained lan-\nguage models for zero- and few-shot multi-dialectal\nArabic sequence labeling. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Main Vol-\nume, pages 769–782, Online. Association for Com-\nputational Linguistics.\nMuhammad Khalifa and Khaled Shaalan. 2019. Char-\nacter convolutions for Arabic Named Entity Recog-\nnition with Long Short-Term Memory Networks.\nComputer Speech & Language, 58:335–346.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam:\nA method for stochastic optimization. In ICLR\n(Poster).\nSvetlana Kiritchenko, Saif Mohammad, and Moham-\nmad Salameh. 2016. SemEval-2016 Task 7: Deter-\nmining Sentiment Intensity of English and Arabic\nPhrases. In Proceedings of the 10th international\nworkshop on semantic evaluation (SEMEVAL-2016),\npages 42–51.\nWuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. 2020.\nAn Empirical Study of Pre-trained Transformers for\nArabic Information Extraction. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing (EMNLP) , pages 4727–\n4734.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nHang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoit Crabb´e, Laurent Besacier, and Didier\nSchwab. 2020. FlauBERT: Unsupervised Language\nModel Pre-training for French. In Proceedings of\nThe 12th Language Resources and Evaluation Con-\nference, pages 2479–2490.\nPatrick Lewis, Barlas O ˘guz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating Cross-lingual Extractive Question Answering.\npages 7315–7330.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019a.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint arXiv:1907.11692.\nZihan Liu, Yan Xu, Genta Indra Winata, and Pascale\nFung. 2019b. Incorporating Word and Subword\nUnits in Unsupervised Machine. Translation Using\nLanguage Model Rescoring.\nMartin Malmsten, Love B¨orjeson, and Chris Haffenden.\n2020. Playing with Words at the National Library of\nSweden–Making a Swedish BERT. arXiv preprint\narXiv:2007.01658.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Su ´arez, Yoann Dupont, Laurent Romary, ´Eric\nde la Clergerie, Djam ´e Seddah, and Beno ˆıt Sagot.\n2020. CamemBERT: a Tasty French Language\nModel . In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7203–7219, Online. Association for Computa-\ntional Linguistics.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The Natural Language\nDecathlon: Multitask Learning as Question Answer-\ning. arXiv preprint arXiv:1806.08730.\nAlexis Mitchell, Stephanie Strassel, Mark Przybocki,\nJ Davis, George Doddington, Ralph Grishman, and\nB Sundheim. 2004. Tides extraction (ACE) 2003\nmultilingual training data. Linguistic Data Consor-\ntium, Philadelphia Web Download.\nS. Bravo-Marquez Mohammad, M. F. Salameh, and\nS. Kiritchenko. 2018. Semeval-2018 Task 1: Affect\nin Tweets. In Proceedings of International Work-\nshop on Semantic Evaluation (SemEval-2018) . As-\nsociation for Computational Linguistics.\nHussein Mozannar, Karl El Hajal, Elie Maamary, and\nHazem Hajj. 2019. Neural Arabic Question Answer-\ning. In Proceedings of the Fourth Arabic Natural\nLanguage Processing Workshop, Florence, Italy. As-\nsociation for Computational Linguistics.\nHamdy Mubarak, Kareem Darwish, Walid Magdy,\nTamer Elsayed, and Hend Al-Khalifa. 2020.\nOverview of OSACT4 Arabic Offensive Language\nDetection Shared Task. In Proceedings of the 4th\nWorkshop on Open-Source Arabic Corpora and Pro-\ncessing Tools, with a Shared Task on Offensive Lan-\nguage Detection , pages 48–52, Marseille, France.\nEuropean Language Resource Association.\nMahmoud Nabil, Mohamed Aly, and Amir F Atiya.\n2015. Astd: Arabic sentiment tweets dataset. In\nProceedings of the 2015 Conference on Empirical\nMethods in Natural Language Processing , pages\n2515–2519.\nEl Moatez Billah Nagoudi, AbdelRahim Elmadany,\nMuhammad Abdul-Mageed, and Tariq Alhindi.\n2020. Machine generation and detection of Ara-\nbic manipulated and fake news. In Proceedings of\nthe Fifth Arabic Natural Language Processing Work-\nshop, pages 69–84, Barcelona, Spain (Online). Asso-\nciation for Computational Linguistics.\nDat Quoc Nguyen and Anh Tuan Nguyen. 2020.\nPhoBERT: Pre-trained language models for Viet-\nnamese. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1037–1042,\nOnline. Association for Computational Linguistics.\nOssama Obeid, Nasser Zalmout, Salam Khalifa, Dima\nTaji, Mai Oudah, Bashar Alhafni, Go Inoue, Fadhl\nEryani, Alexander Erdmann, and Nizar Habash.\n2020. CAMeL Tools: An Open Source Python\nToolkit for Arabic Natural Language Processing. In\nProceedings of The 12th Language Resources and\nEvaluation Conference, pages 7022–7032.\nSampo Pyysalo, Jenna Kanerva, Antti Virtanen, and\nFilip Ginter. 2020. WikiBERT models: deep trans-\nfer learning for many languages. arXiv preprint\narXiv:2006.01538.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J Liu. 2020. Exploring the\nLimits of Transfer Learning with a Uniﬁed Text-to-\nText Transformer. Journal of Machine Learning Re-\nsearch, 21:1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100000+ Questions for\nMachine Comprehension of Text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, Austin, Texas. Associa-\ntion for Computational Linguistics.\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky.\n2020. A Primer in BERTology: What we know\nabout how BERT works. Transactions of the Associ-\nation for Computational Linguistics, 8:842–866.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 task 4: Sentiment analysis in Twitter.\nIn Proceedings of the 11th international workshop\non semantic evaluation (SemEval-2017), pages 502–\n518.\nMotaz K Saad and Wesam M Ashour. 2010. OSAC:\nOpen Source Arabic Corpora . In 6th ArchEng Int.\nSymposiums, EEECS, volume 10.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\n2020. KUISAIL at SemEval-2020 task 12: BERT-\nCNN for offensive speech identiﬁcation in social me-\ndia. In Proceedings of the Fourteenth Workshop on\nSemantic Evaluation, pages 2054–2059, Barcelona\n(online). International Committee for Computational\nLinguistics.\nMohammad Salameh, Saif Mohammad, and Svetlana\nKiritchenko. 2015. Sentiment after Translation: A\nCase-Study on Arabic Social Media Posts. In Pro-\nceedings of the 2015 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n767–777, Denver, Colorado. Association for Compu-\ntational Linguistics.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand Korean V oice Search. In 2012 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5149–5152. IEEE.\nPedro Javier Ortiz Su ´arez, Benoˆıt Sagot, and Laurent\nRomary. 2019. Asynchronous Pipeline for Process-\ning Huge Corpora on Medium to Low Resource In-\nfrastructure. In 7th Workshop on the Challenges\nin the Management of Large Corpora (CMLC-7) .\nLeibniz-Institut f¨ur Deutsche Sprache.\nBashar Talafha, Mohammad Ali, Muhy Eddin Za’ter,\nHaitham Seelawi, Ibraheem Tuffaha, Mostafa Samir,\nWael Farhan, and Hussein T Al-Natsheh. 2020.\nMulti-Dialect Arabic BERT for Country-Level Di-\nalect Identiﬁcation. In Proceedings of the Fifth Ara-\nbic Natural Language Processing Workshop, pages\n111–118, Barcelona, Spain (Online). Association for\nComputational Linguistics.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Lu-\noma, Juhani Luotolahti, Tapio Salakoski, Filip Gin-\nter, and Sampo Pyysalo. 2019. Multilingual is\nnot enough: BERT for Finnish. arXiv preprint\narXiv:1912.07076.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. BERTje: A Dutch BERT\nModel. arXiv preprint arXiv:1912.09582.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R Bowman. 2019. Super-\nglue: A stickier benchmark for general-purpose\nlanguage understanding systems. arXiv preprint\narXiv:1905.00537.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP ,\npages 353–355, Brussels, Belgium. Association for\nComputational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s Neural Machine\nTranslation System: Bridging the Gap between\nHuman and Machine Translation. arXiv preprint\narXiv:1609.08144.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2021. mt5: A massively\nmultilingual pre-trained text-to-text transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 483–498, Online. Association for Computa-\ntional Linguistics.\nWajdi Zaghouani and Anis Charﬁ. 2018. Arap-Tweet:\nA Large Multi-Dialect Twitter Corpus for Gen-\nder, Age and Language Variety Identiﬁcation. In\nProceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation (LREC\n2018), Miyazaki, Japan. European Language Re-\nsources Association (ELRA).\nOmar F Zaidan and Chris Callison-Burch. 2014. Ara-\nbic Dialect Identiﬁcation . Computational Linguis-\ntics, 40(1):171–202.\nImad Zeroual, Dirk Goldhahn, Thomas Eckart, and Ab-\ndelhak Lakhouaja. 2019. OSIAN: Open Source In-\nternational Arabic News Corpus - Preparation and\nIntegration into the CLARIN-infrastructure. In Pro-\nceedings of the Fourth Arabic Natural Language\nProcessing Workshop , pages 175–182, Florence,\nItaly. Association for Computational Linguistics.\nChiyu Zhang and Muhammad Abdul-Mageed. 2019a.\nMulti-task bidirectional transformer representations\nfor irony detection. CoRR.\nChiyu Zhang and Muhammad Abdul-Mageed. 2019b.\nNo Army, No Navy: BERT Semi-Supervised Learn-\ning of Arabic Dialects. In Proceedings of the\nFourth Arabic Natural Language Processing Work-\nshop, pages 279–284, Florence, Italy. Association\nfor Computational Linguistics.\nAppendices\nA Sentiment Analysis\nA.1 SA Datasets\n• AJGT. The Arabic Jordanian General Tweets\n(AJGT) dataset (Alomari et al., 2017) covers\nMSA and Jordanian Arabic, with 900 positive\nand 900 negative posts.\n• AraNETSent. Abdul-Mageed et al. (2020b)\ncollect 15 datasets in both MSA and di-\nalects from Abdul-Mageed and Diab (2012)\n(AWATIF), Abdul-Mageed et al. (2014)\n(SAMAR), Abdulla et al. (2013); Nabil et al.\n(2015); Kiritchenko et al. (2016); Aly and\nAtiya (2013); Salameh et al. (2015); Rosen-\nthal et al. (2017); Alomari et al. (2017); Mo-\nhammad et al. (2018), and Baly et al. (2019).\nThese datasets carry both binary (negative\nand positive) and three-way (negative, neu-\ntral, and positive) labels, but Abdul-Mageed\net al. (2020b) map them into binary sentiment\nonly.\n• AraSenTi-Tweet. This comprises 17, 573\ngold labeled MSA and Saudi Arabic tweets\nby Al-Twairesh et al. (2017).\n• ArSarcasmSent This sarcasm dataset is la-\nbeled with sentiment tags by Farha and Magdy\n(2020) who extract it from ASTD (Nabil et al.,\n2015) ( 10, 547 tweets) and SemEval-2017\nTask 4 (Rosenthal et al., 2017) (8, 075 tweets).\n• ArSAS. This Arabic Speech Act and Senti-\nment (ArSAS) corpus (Elmadany et al., 2018)\nconsists of tweets annotated with sentiment\ntags.\n• ArSenD-Lev. The Arabic Sentiment Twit-\nter Dataset for LEVantine dialect (ArSenD-\nLev) (Baly et al., 2019) has 4, 000 tweets re-\ntrieved from the Levant region.\n• ASTD. This is a collection of 10, 006 Egyp-\ntian tweets by Nabil et al. (2015).\n• A W ATIF. This is an MSA dataset from\nnewswire, Wikipedia, and web fora intro-\nduced by Abdul-Mageed and Diab (2012).\n• BBNS & SYTS . The BBN blog posts\nSentiment (BBNS) and Syria Tweets\nSentiment (SYTS) are introduced by Salameh\net al. (2015).\n• CAMelSent. Obeid et al. (2020) merge train-\ning and development data from ArSAS (El-\nmadany et al., 2018), ASTD (Nabil et al.,\n2015), SemEval (Rosenthal et al., 2017), and\nArSenTD (Al-Twairesh et al., 2017) to create\na new training dataset (∼24K) and evaluate\non the independent test sets from each of these\nsources.\n• HARD. The Hotel Arabic Reviews Dataset\n(HARD) (Elnagar et al., 2018) consists of\n93, 700 MSA and dialect hotel reviews.\n• LABR. The Large Arabic Book Review Cor-\npus (Aly and Atiya, 2013) has 63, 257 book\nreviews from Goodreads,18 each rated with a\n1-5 stars scale.\n• TwitterAbdullah.19 This is a dataset of2, 000\nMSA and Jordanian Arabic tweets manually\nlabeled by Abdulla et al. (2013).\n• TwitterSaad. This dataset is collected using\nan emoji lexicon by Moatez Saad in 2019 and\nis available on Kaggle.20\n• SemEval-2017. This is the SemEval-2017\nsentiment analysis in Arabic Twitter task\ndatasetby Rosenthal et al. (2017).\nA.2 SA Baselines\nFor SA, we compare to the following STOA:\n• Antoun et al. (2020). We compare to best\nresults reported by the authors on ﬁve SA\ndatasets: HARD, balanced ASTD (which we\nrefer to as ASTD-B), ArSenTD-Lev, AJGT,\nand the unbalanced positive and negative\nclasses for LABR. They split each dataset into\n80/20 for Train/Test, respectively, and report\nin accuracy using the best epoch identiﬁed on\ntest data. For a valid comparison, we follow\ntheir data splits and evaluation set up.\n• Obeid et al. (2020). They ﬁne-tune mBERT\nand AraBERT on the merged CAMel sent\n18www.goodreads.com.\n19For ease of reference, we assign a name to this and other\nunnamed datasets.\n20www.kaggle.com/mksaad/arabic-sentiment-twitter-\ncorpus.\nDataset (classes) Classes TRAIN DEV TEST\nAJGT (2) {neg, pos} 1.4K - 361AraNETSent(2) {neg, pos} 100.5K 14.3K 11.8KAraSenTi-Tweet (3){neg, neut, pos} 11.1K 1.4K 1.4KArSarSent(3) {neg, neut, pos} 8.4K - 2.1KArSAS (3) {neg, neut, pos} 24.8K - 3.7KArSenD-LEV (5){neg, neut, pos, neg+, pos+} 3.2K - 801ASTD (3) {neg, neut, pos} 24.8K - 664ASTD-B (2) {neg, pos} 1.1K - 267AW ATIF (4){neg, neut, obj, pos} 2.3K 288 284BBN (3) {neg, neut, pos} 960 125 116HARD (2) {neg, pos} 84.5K - 21.1KLABR (2) {neg, pos} 13.2K - 3.3KSAMAR (5) {mix, neg, neut, obj, pos} 2.5K 310 316SemEval (3) {neg, neut, pos} 24.8K - 6.1KSYTS (3) {neg, neut, pos} 960 202 199TwAbdullah(2) {neg, pos} 1.6K 202 190TwSaad(2) {neg, pos} 47K 5.8K 5.8KARLUESenti(3) {neg, pos, neut} 190.9K 6.5K 44.2K\nTable A.1: Sentiment analysis datasets. neg+: “very neg-\native”; pos+: “very positive”. We construct ARLUE Senti by\nmerging the different datasets and collapsing, or removing,\nthe less frequent classes (details in text).\ndatasets and report in F1PN, which is the\nmacro F1 score over the positive and nega-\ntive classes only (while neglecting the neutral\nclass).\n• Abdul-Mageed et al. (2020b). They ﬁne-\ntune mBERT on the AraNETSent data and re-\nport results in F1 score on test data.\nA.3 SA Evaluation on DEV\nTable A.2 shows results of SA on DEV for datasets\nwhere there is a development split.\nDataset (classes) mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT\nAraNETSent(2) 84.00 92.00 93.00 86.50 89.00 92 .00AraSenTi(3)93.00 93.50 95.00 91.50 92.00 93 .50BBN(3) 68.00 75.00 77.00 70.00 79.50 78.50SYTS(3) 62.00 80.50 66.00 65.00 69.00 72 .50TwitterSaad(2) 80.00 95.50 95.50 81.50 90.00 96.00SAMAR(5)26.00 54.50 61.00 42.50 50.50 62.50AW ATIF(4)63.50 62.00 67.50 65.00 70.50 72.00TwitterAbdullah(2) 87.50 91.00 95.50 92.50 99.00 97.00\nTable A.2: SA results (F1) on DEV .\nB Social Meaning\nB.1 SM Tasks & Datasets\n• Age and Gender. For both age and gender,\nwe use theArap-Tweet dataset (Zaghouani and\nCharﬁ, 2018), which covers 17 different coun-\ntries from 11 Arab regions. We follow the\n80-10-10 data split of AraNet (Abdul-Mageed\net al., 2020b).\n• Dangerous Speech. We use the dangerous\nspeech AraDang dataset from Alshehri et al.\n(2020), which is composed of tweets manually\nlabeled with dangerous and safe tags.\nTask Dataset (classes) Classes TRAIN DEV TESTAge Arap-Tweet (3){≤24yrs,25−34yrs,≥35yrs} 1.3M160.7K160.7KDangerous AraDang (2){dangerous, not-dangerous} 3.5K 616 664Emotion AraNETEmo(8) {ang, anticip, disg, fear, joy, sad, surp, trust} 190K 911 942Gender Arap-Tweet (2){female, male} 1.3M160.7K160.7KHate Speech HS@OSACT (2){hate, not-hate} 10K 1K 2KIrony FIRE2019 (2){irony, not-irony} 3.6K - 404Offensive OFF@OSACT (2){offensive, not-offensive} 10K 1K 2KSarcasm AraSarcasm (2){sarcasm, not-sarcasm} 8.4K - 2.1K\nTable B.1: Social Meaning datasets.\n• Offensive Language and Hate Speech. We\nuse manually labeled data from the shared task\nof offensive speech (Mubarak et al., 2020).21\nThe shared task is divided into two sub-tasks:\nsub-task A: detecting if a tweet is offensive\nor not-offensive, and sub-task B: detecting if\na tweet is hate-speech or not-hate-speech.\n• Emotion. We use the AraNeTemo dataset\nfrom Abdul-Mageed et al. (2020b), which\nis created by merging two datasets from Al-\nhuzali et al. (2018).\n• Irony. We use the irony identiﬁca-\ntion dataset for Arabic tweets released by\nIDAT@FIRE2019 shared task (Ghanem et al.,\n2019), following Abdul-Mageed et al. (2020b)\ndata splits.\n• Sarcasm. We use the ArSarcasm dataset de-\nveloped by Farha and Magdy (2020).\nMore details about these datasets are in Ta-\nble B.1.\nB.2 SM Baselines\n• Age and Gender . We compare to\nAraNET Abdul-Mageed et al. (2020b) age and\ngender models, trained by ﬁne-tuning mBERT.\nThe authors report 51.42 and 65.30 F1 on age\nand gender, respectively.\n• Dangerous Speech. We compare to Alshehri\net al. (2020), who report a best of 59.60 F1\non test with an mBERT model ﬁned-tuned on\nemotion data.\n• Emotion. We compare to Abdul-Mageed et al.\n(2020b), who acquire 60.32 F1 on test with a\nﬁne-tuned mBERT.\n• Hate Speech. The best results on the offen-\nsive and hate speech shared task (Mubarak\net al., 2020) are at 95 F1 score and are re-\nported by Husain (2020), who employ heavy\n21http://edinburghnlp.inf.ed.ac.uk/workshops/OSACT4.\nTask (classes) mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT\nAge (3) 56.33 59.70 53.63 57.67 58 .60 62.19Dangerous (2)67.35 65.09 69.95 67.73 68 .58 75.50Emotion (8)61.34 72.09 72.78 65.46 68 .05 75.18Gender (2)68.06 71.10 71.23 67.61 69 .97 72.81Hate (2) 75.91 76.56 78.00 72.09 75 .01 82.91Irony (2) 81.08 83.12 81.29 79.12 84.83 86.77Offensive (2)84.04 85.26 86.72 87.21 88 .77 91.68\nTable B.2: SM results in F1 on DEV .\nfeature engineering with SVMs. Since our fo-\ncus is on methods exploiting language models,\nwe compare to Djandji et al. (2020) who rank\nsecond in the shared task with a ﬁne-tuned\nAraBERT (83.41 F1 on test).\n• Irony. We compare to Zhang and Abdul-\nMageed (2019a) who ﬁne-tune mBERT on\nthe irony task, with an auxiliary author proﬁl-\ning task, and report 82.4 F1 on test.\n• Offensive Language. We compare to the best\nresults on the offensive sub-task (Mubarak\net al., 2020) reported by Hassan et al. (2020).\nThey propose an ensemble of SVMs, CNN-\nBiLSTM, and mBERT with majority voting\nand acquire 90.51 F1.\n• Sarcasm. We compare to Farha and Magdy\n(2020) who train a BiLSTM model using the\nAraSarcasm dataset, reporting 46.00 F1 score.\nB.3 SM Evaluation on DEV\nTable B.2 shows results of the social meaning tasks\non development splits.\nC Topic Classiﬁcation\nC.1 TC Datasets\n• Arabic News Text. Chouigui et al. (2017)\nbuild the Arabic news text (ANT) dataset from\ntranscribed Tunisian radio broadcasts.\n• Khaleej. Abbas et al. (2011) created the\nKhaleej from Gulf Arabic websites.\n• OSAC. Saad and Ashour (2010) collect\nOSAC from news articles.\nDataset (classes) Classes TRAIN DEV TEST\nANT (5) {C, E, I, ME, S, T} 25.2K 3.2K 3.2KKhallej (4) {E, I, LOC, S} 4.6K 570 570OSAC (10){E, F, H, HIST, L, R, RLG, SPS, S, STR} 18K 2.2K 2.2KARLUETopic(16){all classes} 47.7K 5.9K 5.9K\nTable C.1: TC datasets. C: culture, E: economy, F: family,\nH: health, HIST: history, I: international news, L: law, LOC,\nlocal news, ME: middle east, R: recipes, RLG: religion, SPS:\nspace, S: sports, STR: stories, T: technology.\nDataset (classes) mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT\nANTText (5) 85.04 86 .74 87 .41 87.98 87.06 85 .80\nANTTitle (5) 79.46 80 .77 82 .04 83.56 81.10 82 .36\nANTText+Title (5)87.24 86 .36 88 .45 88.76 87.27 85 .99\nKhallej (4) 94.48 95 .32 96 .09 95 .65 96 .16 96.31\nOSAC (10) 97.87 97 .75 97 .61 97.94 97.56 97 .66\nTable C.2: TC results tasks (F1) on DEV .\nC.2 TC Evaluation on DEV\nResults of TC tasks on DEV data are in Table C.2.\nD Dialect Identiﬁcation\nD.1 DIA Datasets\nWe introduce each dataset brieﬂy here and provide\na description summary of all datasets in Table D.1.\n• Arabic Online Commentary (AOC).This is\na repository of 3M Arabic comments on on-\nline news (Zaidan and Callison-Burch, 2014).\nIt is labeled with MSA and three regional di-\nalects (Egyptian, Gulf, and Levantine).\n• ArSarcasmDia. This dataset is developed\nby Farha and Magdy (2020) for sarcasm de-\ntection but also carries regional dialect la-\nbels from the set {Egyptian, Gulf, Levantine,\nMaghrebi}.\n• MADAR. Sub-task 2 of the MADAR shared\ntask (Bouamor et al., 2019) 22 is focused on\nuser-level dialect identiﬁcation with manually-\ncurated country labels (n=21).\n• NADI-2020. The ﬁrst Nuanced Arabic\nDialect Identiﬁcation shared task (NADI\n2020) (Abdul-Mageed et al., 2020a) 23 tar-\ngets country level (n=21) as well as province\nlevel (n=100) dialects.\n• QADI. The QCRI Arabic Dialect Identiﬁca-\ntion (QADI) dataset (Abdelali et al., 2020) is\nlabeled at the country level (n=18).\nDetails of the datasets are in Table D.1.\nD.2 DIA Baselines\n• Elaraby and Abdul-Mageed (2018) report\nthree levels of classiﬁcation on AOC data: (1)\nMSA vs. DA (87.23 accuracy), (2) regional\n(i.e., Egyptian, Gulf, and Levantine) (87.81 ac-\ncuracy), and (3) MSA, Egyptian, Gulf, and\n22https://camel.abudhabi.nyu.edu/madar-shared-task-\n2019/.\n23https://github.com/UBC-NLP/nadi.\nTask (classes) Dataset Classes TRAIN DEV TEST\nAOC (2) Binary{DA, MSA} 86.5K 10.8K 10.8KAOC (3) Region{Egypt, Gulf, Levnt} 35.7K 4.5K 4.5KAOC (4) Region{Egypt, Gulf, Levnt, MSA} 86.5K 10.8K 10.8KArSarcasmDia(5) Regoin{Egypt, Gulf, Levnt, Magreb, MSA} 8.4K - 2.1KMADAR-TL (21) Country{Multiple countries⋆} 193.1K 26.6K 44KNADI (21) Country{Multiple countries⋆} 2.1K 5K 5KQADI (18) Country{Multiple countries†} 497.8K - 3.5KARLUEDia-B(2) Binary{DA, MSA} 94.9K 10.8K 12.9KARLUEDia-R(4) Region{Egypt, Gulf, Levnt, Magreb} 38.5K 4.5K 5.3KARLUEDia-C(21)Country{Multiple countries⋆} 711.9K 31.5K 52.1K\nTable D.1: Dialect datasets. ⋆ All Arab countries except\nComoros. † All Arab countries except Comoros, Djibouti,\nMauritania, and Somalia.\nDataset (classes) Task mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT\nMADAR(21) Country33.75 34.54 33.28 33.47 39.24 40.61AOC(4) Regoin80.07 78.97 79.55 80.85 81.96 83.56AOC(3) Regoin87.07 86.80 88.21 88.46 89.57 91.56AOC(2) Binary87.89 87.63 88.38 88.76 89.32 89.66NADI(21) Country14.49 17.30 18.62 16.18 23.73 26.40NADI(100) Province02.32 03.91 4.00 03.04 06.05 05.23\nTable D.2: DIA results on DEV in F1.\nLevantine (accuracy of 82.45). Their best\nresults are based on BiLSTM.\n• Abdelali et al. (2020) ﬁne-tune AraBERT on\nthe QADI dataset. They report 60.6 F1.\n• Zhang and Abdul-Mageed (2019b) devel-\noped the top ranked system in MADAR sub-\ntask 2, with 48.76 accuracy and 34.87 F1 at\ntweet level.\n• Talafha et al. (2020) developed NADI sub-\ntask 1 (country level) winning system, an en-\nsemble of ﬁne-tuned AraBERT (26.78 F1).\n• El Mekki et al. (2020) developed NADI sub-\ntask 2 (province level) winning system using\na combination of word and character n-grams\nto ﬁne-tune AraBERT (6.08 F1).\n• AraBERT. For ArSarcasmDia, where no di-\nalect id system was previously developed, we\nconsider a ﬁne-tuned AraBERT a baseline.\nD.3 DIA Evaluation on DEV\nTable D.2 shows results of the dialect identiﬁcation\ntasks on development splits.\nE Named Entity Recognition\nE.1 NER datasets\nTable E.1 and Table E.2 show the data splits across\nour NER datasets, and the results of all our models\non the development splits.\nDataset Tokens Train DEV Test\nANERcorp 150.2K 95.5K 24.8K 29.9K\nACE03BN 15.6K 11.6K 2K 2K\nACE03NW 27K 21.3K 2.7K 3K\nACE04BN 70.5K 56.5K 7K 7K\nTW-NER 74.8K 42.9K 7.4K 24.5K\nARLUENER 338.3K 227.7K 44.1K 66.5K\nTable E.1: Distribution of the Arabic NER datasets.\nDataset (classes) mBERT XLM-RB XLM-RL AraBERT ARBERT MARBERT\nANERcorp 86.20 87 .24 89 .64 90.24 83.24 80 .86ACE03NW 80.57 88 .21 90.49 89.76 88 .17 85 .02ACE03BN 80.35 80 .36 83 .39 81 .05 90.91 79.05ACE04NW 87.21 90 .08 91.94 89.70 89 .33 86 .80TW-NER 52.60 73 .61 77.70 73.61 70.78 67 .39\nTable E.2: NER results (F 1) on DEV .\nE.2 NER Baselines\nKhalifa and Shaalan (2019) apply CNNs and BiL-\nSTMs and report F 1 scores on test sets, as fol-\nlows: 88.77 (ANERcorp), 91.47 (ACE03NW),\n94.92 (ACE03BN), 91.20 (ACE04NW), and 65.34\n(Twitter). We use their exact data splits.\nF Question Answering Datasets\n• ARCD. Mozannar et al. (2019) use crowd-\nsourcing to develop the Arabic Reading Com-\nprehension Dataset. We use the same ARCD\ndata splits used by Antoun et al. (2020).\n• MLQA. This MultiLingual Question Answer-\ning benchmark is proposed by Lewis et al.\n(2020). It consists of over 5K extractive\nquestion-answer instances in SQuAD format\nin seven languages, including Arabic.\n• XQuAD. This Cross-lingual Question An-\nswering Dataset Artetxe et al. (2020) consists\nof 1, 190 question-answer pairs and 240 para-\ngraphs from SQuAD v1.1 (Rajpurkar et al.,\n2016) translated into ten languages (including\nArabic) by professional translators.\n• TyDi QA. The TyDi QA dataset Artetxe et al.\n(2020) is manually curated and covers 11 lan-\nguages (including Arabic). We focus on the\n“Gold” passage task only.\nDataset TRAIN DEV TEST\nAR-XTREME 86.7K (MT) - -\nARCD - - 1.4K (H)\nAR-MLQA - 517 (HT) 5.3K (HT)\nAR-XQuAD - - 1.2K (HT)\nAR-TyDi-QA 14.8K (H) - 921 (H)\nARLUEQA 101.6K 517 11 .6K\nTable F.1: Multilingual & Arabic QA datasets. H: Human\nCreated. HT: Human Translated. MT: Machine Translated.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7954927682876587
    },
    {
      "name": "Arabic",
      "score": 0.7079771757125854
    },
    {
      "name": "Transformer",
      "score": 0.6795659065246582
    },
    {
      "name": "Natural language processing",
      "score": 0.6555081009864807
    },
    {
      "name": "Inference",
      "score": 0.6311807632446289
    },
    {
      "name": "Language model",
      "score": 0.5842537879943848
    },
    {
      "name": "Task (project management)",
      "score": 0.5810456275939941
    },
    {
      "name": "Artificial intelligence",
      "score": 0.569013237953186
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5603438019752502
    },
    {
      "name": "Machine learning",
      "score": 0.3832196891307831
    },
    {
      "name": "Linguistics",
      "score": 0.1357288360595703
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I141945490",
      "name": "University of British Columbia",
      "country": "CA"
    }
  ]
}