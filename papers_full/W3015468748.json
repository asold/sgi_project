{
  "title": "Longformer: The Long-Document Transformer",
  "url": "https://openalex.org/W3015468748",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4200857852",
      "name": "Beltagy, Iz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222134924",
      "name": "Peters, Matthew E.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4200857849",
      "name": "Cohan, Arman",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2995575179",
    "https://openalex.org/W2338908902",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W3106298483",
    "https://openalex.org/W2985220278",
    "https://openalex.org/W2972738865",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3045733172",
    "https://openalex.org/W2962718483",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W2979196189",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963866616",
    "https://openalex.org/W2155069789",
    "https://openalex.org/W2969605360",
    "https://openalex.org/W2952509486",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W3014438226",
    "https://openalex.org/W3033182847",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W2952809536",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2804032941",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2970550868",
    "https://openalex.org/W3015854960",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3098136301",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W3034772996",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2984864519",
    "https://openalex.org/W2971008823",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3099876468",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2519091744",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W3105238007",
    "https://openalex.org/W3016915903",
    "https://openalex.org/W2170973209",
    "https://openalex.org/W3105055324",
    "https://openalex.org/W2988421999",
    "https://openalex.org/W2963087868",
    "https://openalex.org/W3034715004"
  ],
  "abstract": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",
  "full_text": "Longformer: The Long-Document Transformer\nIz Beltagy∗ Matthew E. Peters∗ Arman Cohan∗\nAllen Institute for Artiﬁcial Intelligence, Seattle, W A, USA\n{beltagy,matthewp,armanc}@allenai.org\nAbstract\nTransformer-based models are unable to pro-\ncess long sequences due to their self-attention\noperation, which scales quadratically with the\nsequence length. To address this limitation,\nwe introduce the Longformer with an attention\nmechanism that scales linearly with sequence\nlength, making it easy to process documents of\nthousands of tokens or longer. Longformer’s\nattention mechanism is a drop-in replacement\nfor the standard self-attention and combines\na local windowed attention with a task moti-\nvated global attention. Following prior work\non long-sequence transformers, we evaluate\nLongformer on character-level language mod-\neling and achieve state-of-the-art results on\ntext8 and enwik8. In contrast to most\nprior work, we also pretrain Longformer and\nﬁnetune it on a variety of downstream tasks.\nOur pretrained Longformer consistently out-\nperforms RoBERTa on long document tasks\nand sets new state-of-the-art results on Wiki-\nHop and TriviaQA. We ﬁnally introduce the\nLongformer-Encoder-Decoder (LED), a Long-\nformer variant for supporting long document\ngenerative sequence-to-sequence tasks, and\ndemonstrate its effectiveness on the arXiv sum-\nmarization dataset.1\n1 Introduction\nTransformers (Vaswani et al., 2017) have achieved\nstate-of-the-art results in a wide range of natu-\nral language tasks including generative language\nmodeling (Dai et al., 2019; Radford et al., 2019)\nand discriminative language understanding (De-\nvlin et al., 2019). This success is partly due to\nthe self-attention component which enables the net-\nwork to capture contextual information from the\nentire sequence. While powerful, the memory and\ncomputational requirements of self-attention grow\n∗Equal contribution.\n1https://github.com/allenai/longformer\nFigure 1: Runtime and memory of full self-\nattention and different implementations of Long-\nformer’s self-attention; Longformer-loop is non-\nvectorized, Longformer-chunk is vectorized, and\nLongformer-cuda is a custom cuda kernel im-\nplementations. Longformer’s memory usage scales\nlinearly with the sequence length, unlike the full\nself-attention mechanism that runs out of memory\nfor long sequences on current GPUs. Different\nimplementations vary in speed, with the vectorized\nLongformer-chunk being the fastest. More details\nare in section 3.2.\nquadratically with sequence length, making it infea-\nsible (or very expensive) to process long sequences.\nTo address this limitation, we present Long-\nformer, a modiﬁed Transformer architecture with\na self-attention operation that scales linearly with\nthe sequence length, making it versatile for pro-\ncessing long documents (Fig 1). This is an advan-\ntage for natural language tasks such as long docu-\nment classiﬁcation, question answering (QA), and\ncoreference resolution, where existing approaches\npartition or shorten the long context into smaller\nsequences that fall within the typical 512 token\nlimit of BERT-style pretrained models. Such parti-\ntioning could potentially result in loss of important\ncross-partition information, and to mitigate this\nproblem, existing methods often rely on complex\narchitectures to address such interactions. On the\nother hand, our proposed Longformer is able to\nbuild contextual representations of the entire con-\ntext using multiple layers of attention, reducing the\narXiv:2004.05150v2  [cs.CL]  2 Dec 2020\nneed for task-speciﬁc architectures.\nRecent work has addressed the computational in-\nefﬁciency of Transformers on long sequences (see\nTab. 1). However, they primarily focus on autore-\ngressive language modeling (LM), while the appli-\ncation of long document transformers to document-\nlevel NLP tasks in the transfer learning setting\n(Dai and Le, 2015; Peters et al., 2018; Howard\nand Ruder, 2018; Devlin et al., 2019) has remained\nlargely unexplored. We address this gap and show\nthat Longformer’s attention mechanism can act as\na drop-in replacement for the self-attention mecha-\nnism in pretrained Transformers, and leads to gains\nacross a suite of document NLP tasks.\nLongformer’s attention mechanism is a combina-\ntion of a windowed local-context self-attention and\nan end task motivated global attention that encodes\ninductive bias about the task. Through ablations\nand controlled trials we show both attention types\nare essential – the local attention is primarily used\nto build contextual representations, while the global\nattention allows Longformer to build full sequence\nrepresentations for prediction.\nWe ﬁrst evaluate Longformer on autoregressive\ncharacter-level language modeling using a com-\nbination of windowed and a new dilated attention\npattern, allowing the model to process sequences of\nup to 32K characters on modern GPUs. We achieve\nstate-of-the-art results on text8 and enwik8\nbenchmark datasets, demonstrating the effective-\nness of Longformer in long document modeling.\nThen, to evaluate Longformer’s ability to re-\nplace the full self-attention operation of existing\npretrained models, we pretrain it with the masked\nlanguage modeling (MLM) objective, continuing\nfrom the RoBERTa (Liu et al., 2019) released\ncheckpoint. After pretraining, we apply it to\ndownstream language tasks through ﬁnetuning and\ndemonstrate that Longformer consistently outper-\nforms RoBERTa on a wide range of document-level\nnatural language tasks including text classiﬁcation,\nQA, and coreference resolution, achieving state-of-\nthe-art results on two of these datasets.\nWe ﬁnally introduce a variant of Longformer\nwhich instead of an encoder-only Transformer\narchitecture, it follows an encoder-decoder ar-\nchitecture similar to the original Transformer\nmodel (Vaswani et al., 2017), and it is in-\ntended for sequence-to-sequence (seq2seq) learn-\ning (Sutskever et al., 2014). We call this model\nLongformer-Encoder-Decoder (LED) that uses\nModel attention char-LM other pretrain\nmatrix tasks\nTransformer-XL (2019) ltr yes no no\nAdaptive Span (2019) ltr yes no no\nCompressive (2020) ltr yes no no\nReformer (2020) sparse yes no no\nSparse (2019) sparse yes no no\nRouting (2020) sparse yes no no\nBP-Transformer (2019) sparse yes MT no\nBlockwise (2019) sparse no QA yes\nOur Longformer sparse yes multiple yes\nTable 1: Summary of prior work on adapting Trans-\nformers for long documents. ltr: left-to-right.\nLongformer’s efﬁcient attention pattern on the en-\ncoder network, allowing it to address long docu-\nment seq2seq tasks such as summarization. We\ndemonstrate the effectiveness of LED on the arXiv\nsummarization dataset (Cohan et al., 2018).\n2 Related Work\nLong-Document Transformers Tab. 1 summa-\nrizes recent prior work on long documents. Two\ntypes of self-attention approaches have been ex-\nplored. The ﬁrst is a left-to-right (ltr) approach that\nprocesses the document in chunks moving from\nleft-to-right. While such models have been success-\nful in autoregressive language modeling, they are\nunsuitable for transfer learning approaches with\ntasks that beneﬁt from bidirectional context.\nOur work falls within the other general approach\nthat deﬁnes some form of sparse attention pattern\nand avoids computing the full quadratic attention\nmatrix multiplication. The model with the most\nsimilar attention pattern to ours is Sparse Trans-\nformer (Child et al., 2019), which uses a form of\ndilated sliding window of blocks of size 8x8 pro-\nvided by BlockSparse (Gray et al., 2017). Our\nimplementation (§3) also includes a custom CUDA\nkernel, but it is more ﬂexible and maintainable than\nBlockSparse which is implemented in C++, and\ndesigned for a speciﬁc version of TensorFlow. We\nalso introduce additional task motivated global at-\ntention patterns suitable for common NLP tasks\n(§3) and show they are essential for good perfor-\nmance in the transfer learning setting.\nA few models tried tasks other than autoregres-\nsive language modeling, which is a step forward\nbecause arguably focusing on language modeling\nas the primary evaluation has led to the develop-\nment of models with limited applicability. BP-\nTransformer (Ye et al., 2019) evaluated on machine\n2\n(a) Full n2 attention\n (b) Sliding window attention\n (c) Dilated sliding window\n (d) Global+sliding window\nFigure 2: Comparing the full self-attention pattern and the conﬁguration of attention patterns in our Longformer.\ntranslation (MT), but didn’t explore the pretrain-\nﬁnetune setting. Blockwise attention (Qiu et al.,\n2019) pretrained their models and evaluated on\nquestion answering (QA). However, the evaluation\nis limited as it doesn’t include language modeling,\nand the QA datasets are of relatively short docu-\nments,2 therefore the effectiveness of this model\non long document tasks remains unexplored.\nTask-speciﬁc Models for Long Documents\nMany task-speciﬁc approaches have been devel-\noped to workaround the 512 limit of pretrained\ntransformer models like BERT. The simplest ap-\nproach just truncates the document, commonly\nused for classiﬁcation (Xie et al., 2019). An-\nother approach chunks the document into chunks\nof length 512 (could be overlapping), processes\neach chunk separately, then combines the activa-\ntions with a task speciﬁc model (Joshi et al., 2019).\nA third approach popular for multihop and open\ndomain QA tasks uses a two-stage model where\nthe ﬁrst stage retrieves relevant documents that are\npassed onto the second stage for answer extrac-\ntion (Clark and Gardner, 2017; Chen et al., 2017).\nAll of these approaches suffer from information\nloss due to truncation or cascading errors from\nthe two stage approach. In contrast, Longformer\ncan process long sequences without truncating or\nchunking, allowing us to adopt a much simpler ap-\nproach that concatenates the available context and\nprocesses it in a single pass.\nA few contemporaneous works3 have explored\nsimilar ideas to Longformer using local + global\nattention in Transformers, and pre-training it for\nlong document natural language tasks. In particu-\nlar, ETC (Ainslie et al., 2020) uses a similar local\n+ global attention instead of full self-attention to\nscale Transformers to long documents. Different\nfrom Longformer, ETC uses relative position em-\n2SQuAD contexts typically ﬁt within the 512 limit, and\nMRQA is constructed by dropping long-document examples.\n3All were published on arXiv after Longformer.\nbeddings (which we only used for the Autoregres-\nsive LM setting), introduces an additional training\nobjective (CPC loss) for pre-training, and conﬁg-\nures global attention in a slightly different way.\nIt shows strong results on several tasks including\nreading comprehension and classiﬁcation. GMAT\n(Gupta and Berant, 2020) uses a similar idea of\nfew global locations in the input serving as global\nmemory. BigBird (Zaheer et al., 2020) is an exten-\nsion over ETC with evaluation on additional tasks,\nincluding summarization. Importantly, through the-\noretical analysis, BigBird shows that sparse Trans-\nformers are universal approximators of sequence\nfunctions and preserve these properties of the full\nself-attention.\n3 Longformer\nThe original Transformer model has a self-attention\ncomponent with O(n2) time and memory complex-\nity where n is the input sequence length. To address\nthis challenge, we sparsify the full self-attention\nmatrix according to an “attention pattern” specify-\ning pairs of input locations attending to one another.\nUnlike the full self-attention, our proposed atten-\ntion pattern scales linearly with the input sequence,\nmaking it efﬁcient for longer sequences. This sec-\ntion discusses the design and implementation of\nthis attention pattern.\n3.1 Attention Pattern\nSliding Window Given the importance of local\ncontext (Kovaleva et al., 2019), our attention pat-\ntern employs a ﬁxed-size window attention sur-\nrounding each token. Using multiple stacked lay-\ners of such windowed attention results in a large\nreceptive ﬁeld, where top layers have access to all\ninput locations and have the capacity to build repre-\nsentations that incorporate information across the\nentire input, similar to CNNs (Wu et al., 2019).\nGiven a ﬁxed window size w, each token attends\nto 1\n2 w tokens on each side (Fig. 2b). The com-\nputation complexity of this pattern is O(n ×w),\n3\nwhich scales linearly with input sequence length n.\nIn a transformer with ℓ layers, the receptive ﬁeld\nsize at the top layer is ℓ ×w (assuming w is ﬁxed\nfor all layers). Depending on the application, it\nmight be helpful to use different values of w for\neach layer to balance between efﬁciency and model\nrepresentation capacity (§4.1).\nDilated Sliding Window To further increase the\nreceptive ﬁeld without increasing computation, the\nsliding window can be “dilated”. This is analogous\nto dilated CNNs (van den Oord et al., 2016) where\nthe window has gaps of size dilation d (Fig. 2c).\nAssuming a ﬁxed d and w for all layers, the recep-\ntive ﬁeld is ℓ ×d ×w, which can reach tens of\nthousands of tokens even for small values of d.\nIn multi-headed attention, each attention head\ncomputes a different attention score. We found set-\ntings with different dilation conﬁgurations per head\nimproves performance by allowing some heads\nwithout dilation to focus on local context, while\nothers with dilation focus on longer context.\nGlobal Attention In state-of-the-art BERT-style\nmodels for natural language tasks, the optimal in-\nput representation differs from language modeling\nand varies by task. For masked language modeling\n(MLM), the model uses local context to predict the\nmasked word, while for classiﬁcation, the model ag-\ngregates the representation of the whole sequence\ninto a special token ([CLS] in case of BERT). For\nQA, the question and document are concatenated,\nallowing the model to compare the question with\nthe document through self-attention.\nIn our case, the windowed and dilated attention\nare not ﬂexible enough to learn task-speciﬁc repre-\nsentations. Accordingly, we add “global attention”\non few pre-selected input locations. Importantly,\nwe make this attention operation symmetric: that\nis, a token with a global attention attends to all\ntokens across the sequence, and all tokens in the\nsequence attend to it. Fig. 2d shows an example\nof a sliding window attention with global attention\nat a few tokens at custom locations. For example\nfor classiﬁcation, global attention is used for the\n[CLS] token while in QA global attention is pro-\nvided on all question tokens. Since the number of\nsuch tokens is small relative to and independent of\nn the complexity of the combined local and global\nattention is still O(n). While specifying global\nattention is task speciﬁc, it is a easy way to add in-\nductive bias to the model’s attention, and it is much\nsimpler than existing task speciﬁc approaches that\nuse complex architecture to combine information\nacross smaller input chunks.\nLinear Projections for Global Attention Re-\ncall that given the linear projections Q, K, V , the\nTransformer model (Vaswani et al., 2017) computes\nattention scores as follows:\nAttention(Q, K, V) = softmax\n(QKT\n√dk\n)\nV (1)\nWe use two sets of projections, Qs, Ks, Vs to com-\npute attention scores of sliding window attention,\nand Qg, Kg, Vg to compute attention scores for the\nglobal attention. The additional projections provide\nﬂexibility to model the different types of attention,\nwhich we show is critical for best performance on\ndownstream tasks. Qg, Kg, Vg are all initialized\nwith values that match Qs, Ks, Vs.\n3.2 Implementation\nIn regular transformers, attention scores are com-\nputed as in Eqn. 1. The expensive operation is\nthe matrix multiplication QKT because both Q\nand K have n (sequence length) projections. For\nLongformer, the dilated sliding window attention\ncomputes only a ﬁxed number of the diagonals of\nQKT . As shown in Fig. 1, this results in a linear\nincrease in memory usage compared to quadratic\nincrease for full self-attention. However, imple-\nmenting it requires a form of banded matrix mul-\ntiplication that is not supported in existing deep\nlearning libraries like PyTorch/Tensorﬂow. Fig. 1\ncompares the performance of three different ways\nof implementing it: loop is a memory efﬁcient Py-\nTorch implementation that supports dilation but is\nunusably slow and only used for testing; chunks\nonly supports the non-dilated case and is used for\nthe pretraining/ﬁnetuning setting; and cuda is our\nfully functioning highly optimized custom CUDA\nkernel implemented using TVM (Chen et al., 2018)\nand used for the language modeling experiments\n(see Appendix A for more details).\n4 Autoregressive Language Modeling\nAutoregressive or left-to-right language modeling\nis loosely deﬁned as estimating the probability dis-\ntribution of an existing token/character given its\nprevious tokens/characters in an input sequence.\nThis task is considered one of the fundamental tasks\nin natural language and recent prior work on mod-\neling long sequences using transformers has relied\n4\non this task as their primary evaluation (Dai et al.,\n2019; Rae et al., 2020; Sukhbaatar et al., 2019).\nSimilarly, we develop and evaluate our model on\nautoregressive language modeling.\n4.1 Attention Pattern\nFor autoregressive language modeling we use\nour dilated sliding window attention. Follow-\ning Sukhbaatar et al. (2019) we use differing win-\ndow sizes across the layers. In particular, we use\nsmall window sizes for the lower layers and in-\ncrease window sizes as we move to higher layers.\nThis allows the top layers to learn higher-level rep-\nresentation of the entire sequence while having the\nlower layers capture local information. In addition,\nit provides balance between efﬁciency (smaller win-\ndow sizes are less computationally expensive due\nto fewer nonzero values) and performance (larger\nwindow sizes have richer representation power and\noften result in performance improvements).\nWe do not use dilated sliding windows for lower\nlayers to maximize their capacity to learn and uti-\nlize the immediate local context. For the higher\nlayers, we use a small amount of increasing dila-\ntion only on 2 heads. This gives the model the\nability to directly attend to distant tokens without\nsacriﬁcing local context.\n4.2 Experiment Setup\nTo compare to prior work we focus on character-\nlevel LM (text8 and enwik8; Mahoney, 2009).\nTraining Ideally, we would like to train our\nmodel on the largest window size and sequence\nlength we can ﬁt in a modern GPU memory. How-\never, we found that the model needs a large number\nof gradient updates to learn the local context ﬁrst,\nbefore learning to utilize longer context. To accom-\nmodate this, we adopt a staged training procedure\nwhere we increase the attention window size and\nsequence length across multiple training phases. In\nparticular, in the ﬁrst phase we start with a short\nsequence length and window size, then on each sub-\nsequent phase, we double the window size and the\nsequence length, and halve the learning rate. This\nmakes training fast, while keeping the slow part\n(longest sequences and window sizes) to the end.\nWe train the model over 5 total phases with start-\ning sequence length of 2,048 and ending sequence\nlength of 23,040 on the last phase (see Appendix B\nfor detailed conﬁgurations of each phase, and for\nall other hyperparameters).\nModel #Param Dev Test\nDataset text8\nT12 (Al-Rfou et al., 2018) 44M - 1.18\nAdaptive (Sukhbaatar et al., 2019) 38M 1.05 1.11\nBP-Transformer (Ye et al., 2019) 39M - 1.11\nOur Longformer 41M 1.04 1.10\nDataset enwik8\nT12 (Al-Rfou et al., 2018) 44M - 1.11\nTransformer-XL (Dai et al., 2019) 41M - 1.06\nReformer (Kitaev et al., 2020) - - 1.05\nAdaptive (Sukhbaatar et al., 2019) 39M 1.04 1.02\nBP-Transformer (Ye et al., 2019) 38M - 1.02\nOur Longformer 41M 1.02 1.00\nTable 2: Small model BPC on text8 & enwik8\nModel #Param Test BPC\nTransformer-XL (18 layers) 88M 1.03\nSparse (Child et al., 2019) ≈100M 0.99\nTransformer-XL (24 layers) 277M 0.99\nAdaptive (Sukhbaatar et al., 2019) 209M 0.98\nCompressive (Rae et al., 2020) 277M 0.97\nRouting (Roy et al., 2020) ≈223M 0.99\nOur Longformer 102M 0.99\nTable 3: Performance of large models on enwik8\nEvaluation We evaluate with sequences of\nlength 32,256. Following Dai et al. (2019), we\nsplit the dataset into overlapping sequences of size\n32,256 with a step of size 512, and report the per-\nformance on the last 512 tokens on the sequence.\n4.2.1 Results\nTab. 2 and 3 summarize evaluation results on\ntext8 and enwik8 datasets. We achieve a new\nstate-of-the-art on both text8 and enwik8 using\nthe small models with BPC of 1.10 and 1.00 on\ntext8 and enwik8 respectively, demonstrating\nthe effectiveness of our model.\nFor large models, given how expensive these\nexperiments are, and following recent work (Ki-\ntaev et al., 2020; Rae et al., 2020), we are only\nevaluating on enwik8. Tab. 3 shows that Long-\nformer outperforms the comparable Transformer-\nXL model, matches the performance of the compa-\nrable Sparse Transformer (Child et al., 2019), and\nmatches or slightly underperforms recent models\nthat have more than twice the number of parameters.\nIt is worth noting that Adaptive Span (Sukhbaatar\net al., 2019) and Compressive Transformer (Rae\net al., 2020) are not good ﬁt for the pretraining-\nﬁnetuning paradigm as discussed in §2.\n5\nModel Dev BPC\nDecreasing w (from 512 to 32) 1.24\nFixed w (= 230) 1.23\nIncreasing w (from 32 to 512) 1.21\nNo Dilation 1.21\nDilation on 2 heads 1.20\nTable 4: Top: changing window size across layers. Bot-\ntom: with/without dilation (@ 150K steps on phase1)\n4.2.2 Ablation Study\nTo show the importance of the design choices of\nour attention patterns, we tried different variants\nand report their controlled experiment results. To\nmake the ablation study more manageable, we train\neach conﬁguration for 150K steps 4 with phase 1\nconﬁguration on a small model on text8, then\nreport the BPC performance on the dev set.\nThe top of Tab. 4 demonstrates the impact of\ndifferent ways of conﬁguring the window sizes\nper layer. We observe that increasing the window\nsize from the bottom to the top layer leads to the\nbest performance, arranging them in the reverse\nway leads to worse performance, and using a ﬁxed\nwindow size (the average of window sizes of the\nother conﬁguration) leads to a performance that\nit is in between. The bottom of Tab. 4 shows the\nimpact of adding dilation. Adding some dilation to\ntwo heads leads to some improvement compared\nwith no dilation at all.\n5 Pretraining and Finetuning\nCurrent state-of-the-art systems for many NLP\ntasks ﬁnetune a pretrained model with task super-\nvision (e.g. BERT). One of our main motivations\nis to develop such a model suitable for long docu-\nment tasks. To do so, we pretrained Longformer\non a document corpus and ﬁnetune it for six tasks,\nincluding classiﬁcation, QA and coreference resolu-\ntion. The resulting model can process sequences up\nto 4,096 tokens long (8 times longer than BERT)5.\nWe pretrain Longformer with masked language\nmodeling (MLM), where the goal is to recover\nrandomly masked tokens in a sequence. Since\nMLM pretraining is expensive, we continue pre-\ntraining from the RoBERTa (Liu et al., 2019) re-\nleased checkpoint, while only making the minimal\n4One caveat is that the ordering of end performance will\nnot agree with that at step 150K. However, this approximation\nsaves the huge cost of running every experiment to completion.\n5Sequences up to 16K are possible on current GPUs.\nModel base large\nRoBERTa (seqlen: 512) 1.846 1.496\nLongformer (seqlen: 4,096) 10.299 8.738\n+ copy position embeddings 1.957 1.597\n+ 2K gradient updates 1.753 1.414\n+ 65K gradient updates 1.705 1.358\nLongformer (train extra pos. embed. only) 1.850 1.504\nTable 5: MLM BPC for RoBERTa and various pre-\ntrained Longformer conﬁgurations.\nchanges necessary to support Longformer’s atten-\ntion mechanism. Note that our attention pattern can\nbe plugged into any pretrained transformer model\nwithout the need to change the model architecture.\nAttention Pattern We use sliding window atten-\ntion with window size of 512, therefore using the\nsame amount of computation as RoBERTa.6\nPosition Embeddings RoBERTa uses learned\nabsolute position embeddings with the maximum\nposition being 512. To support longer documents,\nwe add extra position embeddings to support up to\nposition 4,096. To leverage RoBERTa’s pretrained\nweights, instead of randomly initializing the new\nposition embeddings, we initialize them by copying\nthe 512 position embeddings from RoBERTa mul-\ntiple times as analysis of BERT’s attention heads\nshows a strong learned bias to attending to local\ncontext, including the previous or next token (Clark\net al., 2019). Using the copy initialization preserves\nthis local structure everywhere except at the parti-\ntion boundaries. Despite its simplicity, we found\nthis to be a very effective (see Tab. 5), allowing\nLongformer pretraining to rapidly converge with a\nsmall number of gradient updates.\nContinued MLM Pretraining We pretrain\nLongformer using fairseq (Ott et al., 2019) on a\ncorpus of long documents that we compiled (see\nAppendix C for corpus details). We train two model\nsizes, a base model and a large model. Both models\nare trained for 65K gradient updates with sequences\nlength 4,096, batch size 64 (218 tokens), maximum\nlearning rate of 3e-5, linear warmup of 500 steps,\nfollowed by a power 3 polynomial decay. The rest\nof the hyperparameters are the same as RoBERTa.\nTab. 5 shows the BPC on the development set of\nour training corpus. The ﬁrst row shows a 1.846\n6Adding dilation on a few heads as in §4.1 hurt perfor-\nmance, likely because it is not compatible with the pretrained\nRoBERTa weights. Retraining such model from scratch might\nbe needed to improve performance.\n6\nWordpieces WH TQA HQA ON IMDB HY\navg. 1,535 6,589 1,316 506 300 705\n95th pctl. 3,627 17,126 1,889 1,147 705 1,975\nTable 6: Average and 95th percentile of context length\nof datasets in wordpieces. WH: WikiHop, TQA: Triv-\niaQA, HQA: HotpotQA, ON: OntoNotes, HY: Hyper-\npartisan news\nBPC using RoBERTa-base, which is comparable\nto the 1.880 BPC reported on the RoBERTa paper\non their corpus. This indicates our training corpus\nis from a distribution close to that used to train\nRoBERTa. The following two rows show the per-\nformance of Longformer before pretraining with\nrandomly initialized position embeddings and with\ncopied position embeddings. The signiﬁcant differ-\nence indicates the importance of the copy initial-\nization, and the relative small difference between\nthe RoBERTa BPC and the initialized BPC indi-\ncates that our sliding window attention is working\nwell with the RoBERTa weights. The following\ntwo rows show the impact of continuing pretrain-\ning. Traininig for 2K steps improves BPC from\n1.957 to 1.753, which further decreases to 1.705 af-\nter 65K steps, demonstrating the model is learning\nto better utilize the sliding window attention and\nlonger context. Similar patterns are observed with\nRoBERTa-large and Longformer-large.\nFrozen RoBERTa Weights We also pretrained\nLongformer while freezing all RoBERTa weights,\nand only training the new position embeddings.\nThe motivation for this conﬁguration is to perfectly\npreserve the RoBERTa performance on short doc-\numents. This conﬁguration has a BPC of 1.850\n(down from 1.957 at initialization), but higher than\n1.705 where all the weights are trainable.\n6 Tasks\nWe apply Longformer to multiple long document\ntasks, including QA, coreference resolution and\nclassiﬁcation. Tab. 6 shows the evaluation datasets\nhave contexts signiﬁcantly longer than 512 word-\npieces. Our primary goal is to evaluate whether\nour attention mechanism can act as a replace-\nment for the standard self-attention mechanism in\nBERT style models, and to perform controlled tri-\nals against a strong baseline. We are also interested\nin evaluating whether we can replace complicated\ntask speciﬁc models necessitated by BERT’s lim-\nited context with simpler models that just concate-\nnate all available context into a single sequence.\nOur baseline is a RoBERTa based model that\nbreaks the context into the longest possible seg-\nment, passes each individually through RoBERTa,\nand concatenates the activations for further process-\ning. For QA tasks, we also concatenate the question\nto each segment so that RoBERTa can condition\nit’s contextual representations of the context on\nthe question. The Longformer variant replaces the\nRoBERTa self-attention mechanism with our win-\ndowed attention used during pretraining, plus a task\nmotivated global attention. The global attention\nuses additional linear projections (§3.1).\n6.1 Question answering\nWe used three datasets: WikiHop (Welbl et al.,\n2018), TriviaQA (Joshi et al., 2017, Wikipedia set-\nting), and HotpotQA, (Yang et al., 2018, distractor\nsetting).7\nFor WikiHop and TriviaQA we follow the sim-\nple QA model of BERT (Devlin et al., 2019), and\nconcatenate question and documents into one long\nsequence, run it through Longformer, then have a\ndataset-speciﬁc prediction layer. WikiHop uses a\nclassiﬁcation layer for the candidate while Trivi-\naQA uses the loss function of Clark and Gardner\n(2017) to predict answer span. We include global\nattention to question tokens and answer candidates\nfor WikiHop and to question tokens for TriviaQA.\nHotpotQA is a multihop QA dataset that involves\nextracting answer spans and evidence sentences\nfrom 10 Wikipedia paragraphs, 2 of which are rele-\nvant and the rest are distractors. We use a two-stage\nmodel that ﬁrst selects the most relevant paragraphs\nthen passes them to a second stage for answer ex-\ntraction. Both stages concatenate question and con-\ntext into one sequence, run it through Longformer,\nthen use task-speciﬁc prediction layers. We train\nthe models in a multi-task way to predict relevant\nparagraphs, evidence sentences, answer spans and\nquestion types (yes/no/span) jointly. Note that this\nmodel is simpler than recent SOTA models that in-\nclude complex task-speciﬁc architectures (e.g., (Tu\net al., 2019; Chen et al., 2019; Tu et al., 2020;\nGroeneveld et al., 2020)). See Appendix D for fur-\nther details about the models and hyperparameters.\n6.2 Coreference Resolution\nWe use OntoNotes (Pradhan et al., 2012), and the\nmodel from Joshi et al. (2019), a modiﬁcation of\n7We use the full version of TriviaQA and HotpotQA, not\nthe simpliﬁed versions in MRQA (Fisch et al., 2019).\n7\nQA Coref. Classiﬁcation\nModel WikiHop TriviaQA HotpotQA OntoNotes IMDB Hyperpartisan\nRoBERTa-base 72.4 74.3 63.5 78.4 95.3 87.4\nLongformer-base 75.0 75.2 64.4 78.6 95.7 94.8\nTable 7: Summary of ﬁnetuning results on QA, coreference resolution, and document classiﬁcation. Results are on\nthe development sets comparing our Longformer-base with RoBERTa-base. TriviaQA, Hyperpartisan metrics are\nF1, WikiHop and IMDB use accuracy, HotpotQA is joint F1, OntoNotes is average F1.\nthe system from Lee et al. (2018) to replace ELMo\nwith BERT. The Longformer system is a straightfor-\nward adaption of the baseline model by replacing\nRoBERTa with Longformer and extending the se-\nquence length. We didn’t use global attention for\nthis task.\n6.3 Document Classiﬁcation\nWe evaluate on IMDB (Maas et al., 2011) and Hy-\nperpartisan news detection (Kiesel et al., 2019)\ndatasets.8 IMDB is a standard sentiment classiﬁca-\ntion datasets consisting of movie reviews. While\nmost documents in this dataset are short, about\n13.6% of them are larger than 512 wordpieces\n(Tab. 6). Documents in Hyperpartisan are relatively\nlong, and it is small with only 645 documents mak-\ning it a good test for Longformer’s ability to adapt\nto limited data. We use global attention on the\n[CLS] token.\n6.4 Results\nMain Result Tab. 7 summarizes the results of all\nour ﬁnetuning experiments. We observe that Long-\nformer consistently outperforms the RoBERTa\nbaseline. Its performance gain is especially ob-\nvious for tasks that require long context such as\nWikiHop and Hyperpartisan. For TriviaQA, the\nimprovement is more modest as the local context\nis often sufﬁcient to answer the question. In the\ncase of HotpotQA, the supporting fact auxiliary\nsupervision allows models to easily ﬁnd relevant\ncontexts and then focus on local context, leading to\nsmaller gains. This is contrasted with WikiHop that\nonly includes distant supervision of intermediate\nreasoning chains, where our approach excels by\nreasoning over the entire context. On the IMDB\nand OntoNotes datasets the performance gains are\nsmaller. For IMDB, the majority of the dataset\nconsists of short documents and thus it is expected\nto see smaller improvements. For OntoNotes, we\n8For Hyperpartisan we split the training data into 80/10/10\ntrain/dev/test sets, and report mean F1 across ﬁve seeds.\nModel WikiHop TriviaQA HotpotQA\nCurrent∗SOTA 78.3 73.3 74.2\nLongformer-large 81.9 77.3 73.2\nTable 8: Leaderboard results of Longformer-large at\ntime of submission (May 2020). All numbers are F1\nscores.\nfound that the distance between any two mentions\nis typically quite small so that a baseline that pro-\ncesses smaller chunks separately is able to stitch\ntogether mentions into coreference chains without\nconsidering cross chunk interactions.\nLongformer-large for QA We also evaluate the\nperformance of Longformer-large on long context\nQA tasks. Tab. 8 shows that our Longformer-large\nachieves new state-of-the-art results9 on WikiHop\nand TriviaQA by large margins (3.6 and 4 points\nrespectively), and for HotpotQA, it underperforms\nthe current state-of-the-art (Fang et al., 2020) by\na point. Tab. 9 shows the detailed results of Hot-\npotQA compared with published and unpublished\nconcurrent models. Longformer places second\non the published leaderboard, outperforming all\nother published results except for HGN (Fang et al.,\n2020). All published top performing models in\nthis task (Tu et al., 2019; Fang et al., 2020; Shao\net al., 2020) use GNNs (Kipf and Welling, 2017)\nor graph network of entities, which seem to encode\nan important inductive bias for the task and can po-\ntentially improve our results further. Nevertheless,\nLongformer performs strongly outperforming all\nother methods including the recent non-GNN meth-\nods (Glaß et al., 2019; Shao et al., 2020; Groen-\neveld et al., 2020).\n8\nModel ans. supp. joint\nTAP 2 (ensemble) (Glaß et al., 2019) 79.8 86.7 70.7\nSAE (Tu et al., 2019) 79.6 86.7 71.4\nQuark (dev) (Groeneveld et al., 2020) 81.2 87.0 72.3\nC2F Reader (Shao et al., 2020) 81.2 87.6 72.8\nLongformer-large 81.3 88.3 73.2\nETC-large†(Ainslie et al., 2020) 81.2 89.1 73.6\nGSAN-large† 81.6 88.7 73.9\nHGN-large (Fang et al., 2020) 82.2 88.5 74.2\nTable 9: HotpotQA results in distractor setting test set.\nQuark’s test results are not available. All numbers are\nF1 scores. † shows contemporaneous leaderboard sub-\nmissions.\nModel Accuracy / ∆\nLongformer (seqlen: 4,096) 73.8\nRoBERTa-base (seqlen: 512) 72.4 / -1.4\nLongformer (seqlen: 4,096, 15 epochs) 75.0 / +1.2\nLongformer (seqlen: 512, attention: n2) 71.7 / -2.1\nLongformer (seqlen: 2,048) 73.1 / -0.7\nLongformer (no MLM pretraining) 73.2 / -0.6\nLongformer (no linear proj.) 72.2 / -1.6\nLongformer (no linear proj. no global atten.) 65.5 / -8.3\nLongformer (pretrain extra position embed. only) 73.5 / -0.3\nTable 10: WikiHop development set ablations\n6.5 Ablations on WikiHop\nTab. 10 presents an ablation study for WikiHop on\nthe development set. All results use Longformer-\nbase, ﬁne-tuned for ﬁve epochs with identical hy-\nperparameters except where noted. Longformer\nbeneﬁts from longer sequences, global attention,\nseparate projection matrices for global attention,\nMLM pretraining, and longer training. In addition,\nwhen conﬁgured as in RoBERTa-base (seqlen: 512,\nand n2 attention) Longformer performs slightly\nworse then RoBERTa-base, conﬁrming that per-\nformance gains are not due to additional pretrain-\ning. Performance drops slightly when using the\nRoBERTa model pretrained when only unfreezing\nthe additional position embeddings, showing that\nLongformer can learn to use long range context in\ntask speciﬁc ﬁne-tuning with large training datasets\nsuch as WikiHop.\n9At submission time, May 2020. Later, BigBird (Zaheer\net al., 2020) improved leaderboard results on these datasets.\nThere are confounding factors such as using 16X more com-\npute in BigBird’s pretraining compared with Longformer, po-\ntentially affecting the performance.\n7 Longformer-Encoder-Decoder (LED)\nThe original Transformer (Vaswani et al., 2017)\nconsisted of an encoder-decoder architecture, in-\ntended for sequence-to-sequence tasks (Sutskever\net al., 2014), such as summarization and transla-\ntion. While encoder-only Transformers are effec-\ntive on a variety of NLP tasks, pre-trained encoder-\ndecoder Transformer models (e.g. BART (Lewis\net al., 2020) and T5 (Raffel et al., 2020)) have\nachieved strong results on tasks like summariza-\ntion. Yet, such models can’t efﬁciently scale to\nseq2seq tasks with longer inputs.\nTo facilitate modeling long sequences for\nseq2seq learning, we propose a Longformer variant\nthat has both the encoder and decoder Transformer\nstacks but instead of the full self-attention in the\nencoder, it uses the efﬁcient local+global attention\npattern of the Longformer. The decoder uses the\nfull self-attention to the entire encoded tokens and\nto previously decoded locations. We call this model\nLongformer-Encoder-Decoder (LED) which scales\nlinearly with the input. Since pre-training LED is\nexpensive, we initialize LED parameters from the\nBART, and follow BART’s exact architecture in\nterms of number of layers and hidden sizes. The\nonly difference is that to process longer inputs,\nwe extend position embedding to 16K tokens (up\nfrom BART’s 1K tokens) and we initialize the new\nposition embedding matrix by repeatedly copying\nBART’s 1K position embeddings 16 times as in\nSection 5 for RoBERTa. Following BART, we re-\nlease two model sizes, LED-base and LED-large,\nwhich respectively have 6 and 12 layers in both\nencoder and decoder stacks.\nWe evaluate LED on the summarization task us-\ning the arXiv summarization dataset (Cohan et al.,\n2018) which focuses on long document summariza-\ntion in the scientiﬁc domain. The 90th percentile\nof document lengths is 14.5K tokens, making it\nan appropriate testbed for evaluating LED. LED’s\nencoder reads the document and its decoder gener-\nates the output summary. The encoder uses local\nattention with window size 1,024 tokens and global\nattention on the ﬁrst <s> token. The decoder uses\nfull attention to the entire encoder and previously\ndecoded locations. As standard in seq2seq models,\nLED is trained using teacher forcing on gold train-\ning summaries and uses beam search at inference.\nTab. 11 demonstrates the results of LED-large\n16K on the arXiv summarization task. This model\nis merely initialized from BART, with no additional\n9\nR-1 R-2 R-L\nDiscourse-aware (2018) 35.80 11.05 31.80\nExtr-Abst-TLM (2020) 41.62 14.69 38.03\nDancer (2020) 42.70 16.54 38.44\nPegasus (2020) 44.21 16.95 38.83\nLED-large (seqlen: 4,096) (ours) 44.40 17.94 39.76\nBigBird (seqlen: 4,096) (2020) 46.63 19.02 41.77\nLED-large (seqlen: 16,384) (ours) 46.63 19.62 41.83\nTable 11: Summarization results of Longformer-\nEncoder-Decoder (LED) on the arXiv dataset. Met-\nrics from left to right are ROUGE-1, ROUGE-2 and\nROUGE-L.\n1K 4k 16k\n10\n15\n20\n25\n30\n35\n40\n45\n35.21\n44.48\n46.23\n11.54\n17.99\n19.62\nR1\nR2\nFigure 3: ROUGE-1 and ROUGE-2 of LED when vary-\ning the input size (arXiv validation set).\npre-training. We observe that LED achieves state-\nof-the-art results on arXiv, slightly outperform-\ning BigBird (Zaheer et al., 2020). Note that the\nBigBird summarization model supports sequence\nlength of 4K tokens but starts from and continues\npre-training Pegasus (Zhang et al., 2020), a model\nspeciﬁcally designed and pre-trained for summa-\nrization. With no pre-training or task-speciﬁc ini-\ntialization, but with ability to process longer inputs,\nLED can slightly outperform BigBird. Further im-\nprovements should be possible through pre-training\nof LED. Fig. 3 further illustrates the importance\nof sequence length showing the ablility to process\nlonger input signiﬁcantly improves the results.\n8 Conclusion and Future Work\nWe present Longformer, a transformer-based model\nthat is scalable for processing long documents\nand that makes it easy to perform a wide range\nof document-level NLP tasks without chunk-\ning/shortening the long input and without com-\nplex architecture to combine information across\nthese chunks. Longformer employs an attention\npattern that combines local and global information\nwhile also scaling linearly with the sequence length.\nLongformer achieves state-of-the-art results on the\ncharacter-level language modeling tasks of text8\nand enwik8. When pretrained, Longformer con-\nsistently outperforms RoBERTa on long document\ntasks and sets new state-of-the-art results on Wik-\niHop and TriviaQA. We further present LED, an\nencoder-decoder variant of Longformer for model-\ning sequence-to-sequence tasks, and achieve state-\nof-the-art results on the arXiv long document sum-\nmarization task. For future work, we would like\nto study other pretraining objectives, especially for\nLED, increase the sequence length, and explore\nother tasks that might beneﬁt from our model.\nAcknowledgment\nWe would like to thank Noah Smith, Dan Weld,\nDirk Groeneveld, Kyle Lo, Daniel King and Doug\nDowney for helpful discussions and feedback, and\nthe AI2 infrastructure team for technical support.\nReferences\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Va-\nclav Cvicek, Zachary Fisher, Philip Pham, Anirudh\nRavula, Sumit Sanghai, Qifan Wang, and Li Yang.\n2020. ETC: Encoding long and structured inputs\nin transformers. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 268–284, Online. Asso-\nciation for Computational Linguistics.\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. In AAAI.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine\nBordes. 2017. Reading wikipedia to answer open-\ndomain questions. In ACL.\nJifan Chen, Shih-Ting Lin, and Greg Durrett. 2019.\nMulti-hop question answering via reasoning chains.\narXiv preprint, abs/1910.02610.\nTianqi Chen, Thierry Moreau, Ziheng Jiang, Lianmin\nZheng, Eddie Yan, Haichen Shen, Meghan Cowan,\nLeyuan Wang, Yuwei Hu, Luis Ceze, et al. 2018.\nTVM: An automated end-to-end optimizing com-\npiler for deep learning. In OSDI.\nTianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos\nGuestrin. 2016. Training deep nets with sublinear\nmemory cost. arXiv preprint, abs/1604.06174.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint,\nabs/1904.10509.\nChristopher Clark and Matt Gardner. 2017. Simple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\n10\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does bert look\nat? an analysis of bert’s attention. arXiv preprint,\nabs/1906.04341.\nArman Cohan, Franck Dernoncourt, Doo Soon Kim,\nTrung Bui, Seokhwan Kim, Walter Chang, and Nazli\nGoharian. 2018. A discourse-aware attention model\nfor abstractive summarization of long documents. In\nNAACL-HLT 2018.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In NeurIPS.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc V . Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In ACL.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL-HLT.\nYuwei Fang, Siqi Sun, Zhe Gan, Rohit Pillai, Shuo-\nhang Wang, and Jingjing Liu. 2020. Hierarchical\ngraph network for multi-hop question answering. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 8823–8838, Online. Association for Computa-\ntional Linguistics.\nAdam Fisch, Alon Talmor, Robin Jia, Minjoon Seo, Eu-\nnsol Choi, and Danqi Chen. 2019. MRQA 2019\nshared task: Evaluating generalization in reading\ncomprehension. In MRQA workshop at EMNLP.\nAlexios Gidiotis and Grigorios Tsoumakas. 2020. A\ndivide-and-conquer approach to the summarization\nof academic articles. ArXiv, abs/2004.06190.\nMichael Glaß, Alﬁo Massimiliano Gliozzo, Rishav\nChakravarti, Anthony Ferritto, Lin Pan, Gaudani\nBhargav, Dinesh Garg, and Avirup Sil. 2019. Span\nselection pre-training for question answering. arXiv\npreprint, abs/1909.04120.\nScott Gray, Alec Radford, and Diederik P. Kingma.\n2017. Gpu kernels for block-sparse weights.\nDirk Groeneveld, Tushar Khot, Mausam, and Ashish\nSabhwaral. 2020. A simple yet strong pipeline for\nHotpotQA. arXiv preprint, abs/2004.06753.\nAnkit Gupta and Jonathan Berant. 2020. Gmat: Global\nmemory augmentation for transformers. ArXiv,\nabs/2006.03274.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nACL.\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale dis-\ntantly supervised challenge dataset for reading com-\nprehension. In ACL.\nMandar Joshi, Omer Levy, Luke Zettlemoyer, and\nDaniel Weld. 2019. BERT for coreference resolu-\ntion: Baselines and analysis. In EMNLP-IJCNLP.\nJohannes Kiesel, Maria Mestre, Rishabh Shukla, Em-\nmanuel Vincent, Payam Adineh, David Corney,\nBenno Stein, and Martin Potthast. 2019. SemEval-\n2019 task 4: Hyperpartisan news detection. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation, pages 829–839, Minneapo-\nlis, Minnesota, USA. Association for Computational\nLinguistics.\nThomas N Kipf and Max Welling. 2017. Semi-\nsupervised classiﬁcation with graph convolutional\nnetworks. ICLR.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In\nICLR.\nOlga V . Kovaleva, Alexey Romanov, Anna Rogers, and\nAnna Rumshisky. 2019. Revealing the dark secrets\nof bert. In EMNLP/IJCNLP.\nKenton Lee, Luheng He, and Luke Zettlemoyer. 2018.\nHigher-order coreference resolution with coarse-to-\nﬁne inference. In NAACL.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. arXiv preprint, abs/1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nMatt Mahoney. 2009. Large text compression bench-\nmark.\nA¨aron van den Oord, Sander Dieleman, Heiga Zen,\nKaren Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew W. Senior, and Koray\nKavukcuoglu. 2016. Wavenet: A generative model\nfor raw audio. In SSW.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\n11\ntoolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NAACL.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nOlga Uryupina, and Yuchen Zhang. 2012. CoNLL-\n2012 shared task: Modeling multilingual unre-\nstricted coreference in OntoNotes. In Joint Confer-\nence on EMNLP and CoNLL - Shared Task, pages\n1–40, Jeju Island, Korea. Association for Computa-\ntional Linguistics.\nJiezhong Qiu, Hao Ma, Omer Levy, Scott Yih, Sinong\nWang, and Jie Tang. 2019. Blockwise self-attention\nfor long document understanding. arXiv preprint,\nabs/1911.02972.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayaku-\nmar, and Timothy P. Lillicrap. 2020. Compressive\ntransformers for long-range sequence modelling. In\nICLR.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nW. Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2020. Efﬁcient content-based\nsparse attention with routing transformers. arXiv\npreprint, abs/2003.05997.\nNan Shao, Yiming Cui, Ting Liu, Shijin Wang, and\nGuoping Hu. 2020. Is graph structure neces-\nsary for multi-hop reasoning? arXiv preprint,\nabs/2004.03096.\nSandeep Subramanian, Raymond Li, Jonathan Pilault,\nand C. Pal. 2020. On extractive and abstractive neu-\nral document summarization with transformer lan-\nguage models. In EMNLP.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive at-\ntention span in transformers. In ACL.\nIlya Sutskever, Oriol Vinyals, and Quoc V . Le. 2014.\nSequence to sequence learning with neural networks.\nIn NIPS.\nTrieu H. Trinh and Quoc V . Le. 2018. A simple\nmethod for commonsense reasoning. arXiv preprint,\nabs/1806.02847.\nMing Tu, Jinke Huang, Xiaodong He, and Bowen\nZhou. 2020. Graph sequential network for reasoning\nover sequences. In NeurIPS Graph Representation\nLearning workshop.\nMing Tu, Kevin Huang, Guangtao Wang, Jing Huang,\nXiaodong He, and Bufang Zhou. 2019. Select, an-\nswer and explain: Interpretable multi-hop reading\ncomprehension over multiple documents. arXiv\npreprint, abs/1911.00484.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nJohannes Welbl, Pontus Stenetorp, and Sebastian\nRiedel. 2018. Constructing datasets for multi-hop\nreading comprehension across documents. TACL,\n6:287–302.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. 2019. Pay less attention with\nlightweight and dynamic convolutions. arXiv\npreprint, abs/1901.10430.\nQizhe Xie, Zihang Dai, Eduard H. Hovy, Minh-Thang\nLuong, and Quoc V . Le. 2019. Unsupervised\ndata augmentation for consistency training. arXiv\npreprint, abs/1904.12848.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shu\nxin Zheng, Chen Xing, Huishuai Zhang, Yanyan\nLan, Li-Wei Wang, and Tie-Yan Liu. 2020. On layer\nnormalization in the transformer architecture. arXiv\npreprint, abs/2002.04745.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W. Cohen, Ruslan Salakhutdinov, and\nChristopher D. Manning. 2018. HotpotQA: A\ndataset for diverse, explainable multi-hop question\nanswering. In EMNLP.\nZihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and\nZheng Zhang. 2019. BP-Transformer: Modelling\nlong-range context via binary partitioning. arXiv\npreprint, abs/1911.04070.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, C. Alberti, S. Onta ˜n´on,\nPhilip Pham, Anirudh Ravula, Qifan Wang, L. Yang,\nand A. Ahmed. 2020. Big bird: Transformers for\nlonger sequences. ArXiv, abs/2007.14062.\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. In NeurIPS.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and\nPeter J Liu. 2020. Pegasus: Pre-training with ex-\ntracted gap-sentences for abstractive summarization.\nICML.\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan\nSalakhutdinov, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Aligning books and movies:\nTowards story-like visual explanations by watching\nmovies and reading books. ICCV, pages 19–27.\n12\nA Implementation Details\nImplementing Longformer’s dilated sliding win-\ndow attention requires a form of banded matrix\nmultiplication (matrix multiplication where the out-\nput is all zero except certain diagonals) that is\nnot directly supported in existing deep learning\nlibraries like PyTorch/Tensorﬂow. Fig. 1 compares\nthe runtime and memory of three different ways of\nimplementing it.\nLongformer-loop is a naive implementation\nthat computes each diagonal separately in a loop.\nIt is memory efﬁcient because it only computes the\nnon-zero values, but it is unusably slow. We only\nuse it for testing because it is easy to implement\nbut don’t use it to run experiments.\nLongformer-chunks only supports the non-\ndilated case. It chunks Q and K into overlapping\nblocks of size w and overlap of size 1\n2 w, multiplies\nthe blocks, then mask out the diagonals. This is\nvery compute efﬁcient because it uses a single ma-\ntrix multiplication operation from PyTorch, but it\nconsumes 2x the amount of memory a perfectly op-\ntimized implementation should consume because\nit computes some of the zero values. Because of\nthe compute efﬁciency, this implementation is most\nsuitable for the pretrain/ﬁnetune case. We didn’t\nﬁnd the increase in memory to be a problem for\nthis setting.\nLongformer-cuda is a custom CUDA kernel\nthat we implement using TVM (Chen et al., 2018).\nIt is a fully functioning implementation of our at-\ntention (not limited as Longformer-chunks),\nit is the most memory efﬁcient, and it is as fast\nas the highly optimized full self-attention. 10 We\nmainly use this implementation for the autoregres-\nsive language modeling experiments because of the\nmemory efﬁciency (allows the longest sequences)\nand the support of dilation (needed for character-\nLM experiments).\nTensor Virtual Machine (TVM) We build our\ncustom CUDA kernel using TVM (Chen et al.,\n2018), a deep learning compiler stack that compiles\nhigh level description of a function into optimized\ndevice-speciﬁc code. Using TVM, we describe our\nbanded matrix multiplication in high-level python\n10It is worth noting that theoretically, a perfectly optimized\nLongformer-cuda should be faster than the n2 computa-\ntion. However, achieving this level of performance requires\nspecial knowledge of low-level GPU programming, similar to\nimplementing a highly optimized matrix multiplication. Our\ncurrent implementation is sufﬁciently fast and practical to use.\nconstructs, then TVM generates the corresponding\nCUDA code and compiles it for GPUs.\nB Character LM Hyperparameters\nWe evaluate ontext8 and enwik8, both contain\n100M characters from Wikipedia split into 90M,\n5M, 5M for train, dev, test. Our model only speci-\nﬁes how the self-attention component works, and it\nis agnostic to the other design choices for the trans-\nformer model. Our implementation is based on the\nTransformer-XL (Dai et al., 2019) code11 with the\nmemory mechanism disabled. We use relative posi-\ntion embeddings with sinusoidal weights as in Dai\net al. (2019). We use two different model sizes, a\nsmall (12 layers, 512 hidden size) model as in Dai\net al. (2019), and a large (30 layers, 512 hidden\nsize) model as in Child et al. (2019). We employed\nmixed precision training (ﬂoating points 16 and 32)\nusing apex12 to reduce memory consumption and\nspeed-up training. However, we kept the attention\ncomputation in fp32 to avoid numerical instability\nissues.13 We used gradient checkpointing (Chen\net al., 2016) to reduce memory usage, and ran our\nexperiments on 48GB RTX8000 GPUs. All hyper-\nparameters and stage conﬁgurations are listed in\nTab. 12. Our CUDA kernel supports the autoregres-\nsive mode where each token attends to a window of\nprevious tokens only. Our implementation also in-\ncludes a version of the relative position embedding\nthat is compatible with our dilated sliding window\nattention.\nWe ran the small model experiments on 4\nRTX8000 GPUs for 16 days. For the large model,\nwe ran experiments on 8 RTX8000 GPUs for 13\ndays. Most of our hyperparameter search is similar\nto the ablation in Tab. 4 where we run the conﬁgu-\nration for 150K steps ontext8. We experimented\nwith absolute position embeddings and learned po-\nsition embeddings, dropout values of [0.1, 0.2]\n(small model) and [0.1, 0.4] (large model), pre-\nlayernorm and post-layernorm (Xiong et al., 2020),\nlearning rate (LR) of phase1 of values [2.5e-5, 5e-\n4, 1e-4] constant and cosine LR schedules, and\ndifferent conﬁgurations for dilation (on all heads,\non 2 heads, no dilation). Number of gradient up-\ndates/phase reported in Tab. 12 is determined by\nrunning each phase until the validation BPC stops\n11https://github.com/kimiyoung/\ntransformer-xl\n12https://github.com/NVIDIA/apex\n13We found that using fp16 in attention operation results in\nﬂoating point overﬂow and NaNs in later stages of training.\n13\ngetting better.\nC Pretraining Data\nIn order to allow the model to learn long depen-\ndencies in pretraining, we compiled a corpus of\nlong documents. Some of these data sources were\nalso included in the original RoBERTa pretraining\nincluding the Books corpus (Zhu et al., 2015) plus\nEnglish Wikipedia. We additionally included one\nthird of a subset of the Realnews dataset (Zellers\net al., 2019) with documents longer than 1,200 to-\nkens as well as one third of the Stories (Trinh and\nLe, 2018) corpus. Our goal was to include a mix of\nlong and short documents to both allow the model\nto learn longer dependencies while not to forget in-\nformation from the original RoBERTa pretraining.\nThe statistics of the pretraining data is shown in\nTab. 13.\nD Task speciﬁc model details\nAll the QA and classiﬁcation models are imple-\nmented using PyTorch-Lightning14. We use the\nofﬁcial train/dev/test splits of all datasets except\nfor the Hyperpartisan news which we randomely\nsplit into 80/10/10 for train/dev/test.\nWikiHop Instances in WikiHop consist of: a\nquestion, answer candidates (ranging from two\ncandidates to 79 candidates), supporting contexts\n(ranging from three paragraphs to 63 paragraphs),\nand the correct answer. The dataset does not pro-\nvide any intermediate annotation for the multihop\nreasoning chains, requiring models to instead infer\nthem from the indirect answer supervision.\nTo prepare the data for input to Longformer\nand RoBERTa, we ﬁrst tokenize the question,\nanswer candidates, and support contexts using\nRoBERTa’s wordpiece tokenizer. Then we\nconcatenate the question and answer candi-\ndates with special tokens as [q] question\n[/q] [ent] candidate1 [/ent] ...\n[ent] candidateN [/ent]. The contexts\nare also concatenated using RoBERTa’s doc-\nument delimiter tokens as separators: </s>\ncontext1 </s> ... </s> contextM\n</s>. The special tokens [q], [/q],\n[ent], [/ent] were added to the RoBERTa\nvocabulary and randomly initialized before task\nﬁnetuning.\n14https://github.com/PyTorchLightning/\npytorch-lightning\nAfter preparing the input data, we compute acti-\nvations from the top layer of each model as follows.\nWe take the question and answer candidates and\nconcatenate them to as much context as possible up\nto the model sequence length (512 for RoBERTa,\n4,096 for Longformer), run the sequence through\nthe model, collect the output activations, and repeat\nuntil all of the context is exhausted (for all models\nexcept Longformer-large, where we just include\nthe ﬁrst 4,096 length sequence due to memory re-\nquirements). Then all activations for all chunks are\nconcatenated into one long sequence. In the case of\nLongformer, we use global attention to the entire\nquestion and answer candidate sequence.\nFor prediction, we attach a linear layer to each\n[ent] that outputs a single logit, average over all\nlogits for each candidate across the chunks, apply\na softmax and use the cross entropy loss with the\ncorrect answer candidate.\nTraining used the Adam optimizer with linear\nwarmup over 200 gradient updates to a maximum\nLR, and linear decay over the remainder of training.\nWe used gradient accumulation to effective batch\nsize of 32 instances, checking the development ac-\ncuracy every 250 gradient updates and reported the\nmaximum development accuracy. Other hyperpa-\nrameters (dropout, weight decay) were identical to\nRoBERTa pretraining.\nIn general, we ran minimal hyperparameter trials,\nbut for fair comparison between Longformer and\nRoBERTa ran an identical hyperparameter search\nwith Longformer-base and RoBERTa-base. This\nconsisted of a grid search of LR in [2e-5, 3e-5,\n5e-5] and number epochs in [5, 10, 15]. The\nbest Longformer-base conﬁguration used lr=3e-5,\n15 epochs. We ran two hyperparameter trials for\nLongformer-large, lr=3e-5 and number epochs in\n[5, 15] (the 5 epoch model had higher dev accuracy\nof 77.6, and was the single model submitted to the\npublic leaderboard for test set evaluation). All mod-\nels were trained on a single RTX8000 GPU, with\nLongformer-base taking about a day for 5 epochs.\nTriviaQA TriviaQA has more than 100K ques-\ntion, answer, document triplets for training. Doc-\numents are Wikipedia articles, and answers are\nnamed entities mentioned in the article. The span\nthat answers the question is not annotated, but it is\nfound using simple text matching.\nSimilar to WikiHop, we tokenize the question\nand the document using RoBERTa’s tokenizer,\nthen form the input as [s] question [/s]\n14\nParam Value\nPosition Embeddings Relative and Sinusoidal as in Dai et al. (2019)\nSmall model conﬁg 12 layers, 8 heads, 512 hidden size as in Dai et al. (2019)\nLarge model conﬁg 30 layers, 8 heads, 512 hidden size as in Child et al. (2019)\nOptimizer AdamW\nDropout 0.2 (small model), 0.4 (large model)\nGradient clipping 0.25\nWeight Decay 0.01\nLayernorm Location pre-layernorm (Xiong et al., 2020)\nActivation GeLU\nNumber of phases 5\nPhase 1 window sizes 32 (bottom layer) - 8,192 (top layer)\nPhase 5 window sizes 512 (bottom layer) - (top layer)\nPhase 1 sequence length 2,048\nPhase 5 sequence length 23,040 (gpu memory limit)\nPhase 1 LR 0.00025\nPhase 5 LR 000015625\nBatch size per phase 32, 32, 16, 16, 16\n#Steps per phase (small) 430K, 50k, 50k, 35k, 5k\n#Steps per phase (large) 350K, 25k, 10k, 5k, 5k\nWarmup 10% of the phase steps with maximum 10K steps\nLR scheduler constant throughout each phase\nDilation (small model) 0 (layers 0-5), 1 (layers 6-7), 2 (layers 8-9), 3 (layers 10-11)\nDilation (large model) 0 (layers 0-14), 1 (layers 15-19), 2 (layers 20-24), 3 (layers 25-29)\nDilation heads 2 heads only\nTable 12: Hyperparameters for the best performing model for character-level language modeling\nSource Tokens Avg doc len\nBooks (Zhu et al., 2015) 0.5B 95.9K\nEnglish Wikipedia 2.1B 506\nRealnews (Zellers et al., 2019) 1.8B 1.7K\nStories (Trinh and Le, 2018) 2.1B 7.8K\nTable 13: Pretraining data\ndocument [/s]. We truncate the document at\n4,096 wordpiece to avoid it being very slow. After-\nwards, we get the activations from RoBERTa and\nLongformer similar to WikiHop (discussed above).\nWe use global attention on all question tokens.\nFor prediction, we add one layer that predicts the\nbeginning and end of the answer span. Because of\nthe distant supervision nature of the training data\n(no gold answer spans), we use the loss function\nof Clark and Gardner (2017) which works like an\nOR that the model only needs to get one answer\nspan right, not all of them.\nHyperparameters of the best conﬁguration are\nlisted in Tab. 14. All other hyperparameters are\nsimilar to RoBERTa’s. For hyperparameter search,\nwe only tuned LR for the RoBERTa baseline and\ntried rates [3e-5, 5e-5, 1e-4], then used the best,\nwhich is 3e-5, for all subsequent experiments with\nno further tuning. We trained the Longformer-large\nwith the best conﬁguration once and submitted its\noutput to the leaderboard. We ran our experiments\non 32GB V100 GPUs. Small model takes 1 day to\ntrain on 4 GPUs, while large model takes 1 day on\n8 GPUs.\nHotpotQA HotpotQA dataset involves answer-\ning questions from a set of 10 paragraphs from\n10 different Wikipedia articles where 2 paragraphs\nare relevant to the question and the rest are dis-\ntractors. It includes 2 tasks of answer span ex-\ntraction and evidence sentence identiﬁcation. Our\nmodel for HotpotQA combines both answer span\nextraction and evidence extraction in one joint\nmodel. We found a higher performance using a\ntwo-stage Longformer model with similar setup\nthat ﬁrst identiﬁes relevant paragraphs and then\ndoes ﬁnd the ﬁnal answer span and evidence. 15\nThis is largely because removing the distracting\nparagraphs ﬁrst reduces the noise for the ﬁnal ev-\nidence and span detection as also found to be im-\nportant by recent state-of-the-art methods in this\ndataset (Fang et al., 2020). Similar to Wikihop and\nTriviaQA, to prepare the data for input to Long-\nformer, we concatenate question and then all the\n10 paragraphs in one long context. We particu-\nlarly use the following input format with special\ntokens: “ [CLS] [q] question [/q] ⟨t⟩\ntitle1 ⟨/t⟩sent1,1 [s] sent1,2 [s] ...\n15The ﬁnal dev performance of the two stage model im-\nproves over a single stage model by about 4.2 points on joint-\nF1 metric\n15\n⟨t⟩ title2 ⟨/t⟩ sent2,1 [s] sent2,2\n[s] ...” where [q], [/q], ⟨t⟩, ⟨/t⟩, [s],\n[p] are special tokens representing, question start\nand end, paragraph title start and end, and sentence,\nrespectively. The special tokens were added to the\nLongformer vocabulary and randomly initialized\nbefore task ﬁnetuning. For Longformer, we use\nglobal attention to question tokens, paragraph ti-\ntle start tokens as well as sentence tokens. The\nmodel includes additional feedforward layers on\ntop of paragraph title start tokens for prediction\nof relevant paragraphs, as well as sentence tokens\nfor predicting evidence sentences. After training\nthe ﬁrst stage model, we predict relevant paragraph\nscores for both training and development set. We\nthen keep up to 5 paragraphs whose raw score is\nhigher than a pre-speciﬁed threshold (-3.0), and\nremove the other paragraphs from the context. We\nthen train the second stage model on the resulting\nshortened context. For answer span extraction we\nuse BERT’s QA model (Devlin et al., 2019) with\naddition of a question type (yes/no/span) classiﬁ-\ncation head over the ﬁrst special token ( [CLS]).\nFor evidence extraction we apply 2 layer feedfor-\nward networks on top of the representations corre-\nsponding to sentence and paragraph tokens to get\nthe corresponding evidence prediction scores and\nuse binary cross entropy loss to train the model.\nAt inference time for evidence extraction, we use\na constrained decoding strategy similar to Groen-\neveld et al. (2020) that ensures that the evidence\nsentences come from exactly two paragraphs which\nis the setup of this dataset. We combine span, ques-\ntion classiﬁcation, sentence, and paragraphs losses\nand train the model in a multitask way using lin-\near combination of losses. Our experiments are\ndone on RTX8000 GPUs and training each epoch\ntakes approximately half a day on 4 GPUs. We\ntrained the model using Adam optimizer with lin-\near warmup (1000 steps) and linear decay. We used\nminimal hyperparameter tuning using LRs of 3e-5\nand 5e-5 and epochs of 3 to 7 and found the model\nwith LR of 3e-5 and 5 epochs to work best. We\nconduct the same hyperparameter search for the\nRoBERTa baseline as well. The rest of hyperpa-\nrameters are reported in Tab 14.\nCoreference model details The coreference\nmodel is a straightforward adaptation of the coarse-\nto-ﬁne BERT based model from Joshi et al.\n(2019). After preprocessing each document with\nthe RoBERTa wordpiece tokenizer, it splits each\nParam WikiHop TriviaQA HotpotQA\nEpochs 15 5 5\nLR 3e-5 3e-5 5e-5\nWarmup steps 200 1000 1000\nBatch size 32 32 32\nOptimizer Adam Adam Adam\nTable 14: Hyperparameters of the QA models. All mod-\nels use a similar scheduler with linear warmup and de-\ncay.\ndocument into non-overlapping segments up to the\nmaximum sequence length, then concatenates the\nactivations for the coarse-to-ﬁne clustering stage\nthat forms coreference clusters. The maximum se-\nquence length was 384 for RoBERTa-base, chosen\nafter three trials from [256, 384, 512] using the\ndefault hyperparameters in the original implemen-\ntation.16 For Longformer-base the sequence length\nwas 4,096. Similar to the original implementation,\ndifferent learning rates were used for the pretrained\nRoBERTa parameters and the randomly initialized\ntask parameters. Using a larger learning rate in the\ntask parameters allows the optimizer to adjust them\nfarther from their randomly initialized values with-\nout destroying the information in the pretrained\nRoBERTa parameters.\nHyperparameter searches were minimal and con-\nsisted of grid searches of RoBERTa LR in [1e-5,\n2e-5, 3e-5] and task LR in [1e-4, 2e-4, 3e-4] for\nboth RoBERTa and Longformer for a fair compari-\nson. The best conﬁguration for Longformer-base\nwas RoBERTa lr=1e-5, task lr=1e-4. All other hy-\nperparameters were the same as in the original im-\nplementation. Training takes about 10 hours on a\nsingle GPU.\nOur implementation is a superhack that involves\nPyTorch and Tensorﬂow sharing a single process\nand GPU. To avoid re-implementing the com-\nplicated coarse-to-ﬁne logic from Tensorﬂow in\nPyTorch (that involves a highly optimized cus-\ntom GPU kernel originally released by Lee et al.\n(2018)), we devised a system where the lower trans-\nformer portion of the model passes activations and\ngradients back and forth between PyTorch and Ten-\nsorﬂow. The input tensors are ﬁrst run through\nthe transformer in PyTorch, the activations are col-\nlected from the top layer, transferred from GPU\nto CPU then from CPU to Tensorﬂow and back to\nGPU to run the coarse-to-ﬁne clustering and com-\npute the loss. Then gradients are back propogated\n16https://github.com/mandarjoshi90/coref\n16\nin Tensorﬂow to the top of the transformer and\nthe process reversed to transfer them to PyTorch\nfor back propogation through the remainder of the\nmodel. Separate optimizers are maintained with\nidentical LR schedules for parameter updates. The\noverhead in this approach is minimal compared to\nthe overall cost of running the model.\nText classiﬁcation For classiﬁcation, following\nBERT, we used a simple binary cross entropy loss\non top of a ﬁrst [CLS] token with addition of\nglobal attention to [CLS]. We used Adam opti-\nmizer with batch sizes of 32 and linear warmup\nand decay with warmup steps equal to 0.1 of the\ntotal training steps. For both IMDB and Hyperpar-\ntisan news we did grid search of LRs [3e-5, 5e-5]\nand epochs [10, 15, 20] and found the model with\n[3e-5] and epochs 15 to work best. Experiments\nwere done on a single RTX8000 GPU.\n17",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.8308048248291016
    },
    {
      "name": "Transformer",
      "score": 0.8178280591964722
    },
    {
      "name": "Computer science",
      "score": 0.7679927945137024
    },
    {
      "name": "Generative grammar",
      "score": 0.6629030108451843
    },
    {
      "name": "Encoder",
      "score": 0.6534587144851685
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5245469212532043
    },
    {
      "name": "Natural language processing",
      "score": 0.49020296335220337
    },
    {
      "name": "Artificial intelligence",
      "score": 0.42586755752563477
    },
    {
      "name": "Speech recognition",
      "score": 0.40611904859542847
    },
    {
      "name": "Voltage",
      "score": 0.09639474749565125
    },
    {
      "name": "Engineering",
      "score": 0.079220712184906
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}