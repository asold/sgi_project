{
  "title": "TalkWithMachines: Enhancing Human-Robot Interaction Through Large/Vision Language Models",
  "url": "https://openalex.org/W4405714327",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5035308242",
      "name": "Ammar N. Abbas",
      "affiliations": [
        "Technological University Dublin"
      ]
    },
    {
      "id": "https://openalex.org/A5010414319",
      "name": "Csaba Beleznai",
      "affiliations": [
        "Austrian Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6853410760",
    "https://openalex.org/W4399374221",
    "https://openalex.org/W4224912544",
    "https://openalex.org/W4391128534",
    "https://openalex.org/W4391759936",
    "https://openalex.org/W4394828156",
    "https://openalex.org/W4206456103",
    "https://openalex.org/W6839928859",
    "https://openalex.org/W4383097638",
    "https://openalex.org/W4383108457",
    "https://openalex.org/W6853677386",
    "https://openalex.org/W4394862611",
    "https://openalex.org/W6859175081",
    "https://openalex.org/W4401414230",
    "https://openalex.org/W4390874280",
    "https://openalex.org/W6801810553",
    "https://openalex.org/W4393141140",
    "https://openalex.org/W6846254642",
    "https://openalex.org/W4385473486",
    "https://openalex.org/W6854929498",
    "https://openalex.org/W4405785427",
    "https://openalex.org/W4405786528",
    "https://openalex.org/W4394653315",
    "https://openalex.org/W4206676533",
    "https://openalex.org/W4387835481",
    "https://openalex.org/W4285428875",
    "https://openalex.org/W4403390366",
    "https://openalex.org/W4402353986",
    "https://openalex.org/W4393140159",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4378945292",
    "https://openalex.org/W4380714573",
    "https://openalex.org/W4388717300",
    "https://openalex.org/W4384264726",
    "https://openalex.org/W4389363726",
    "https://openalex.org/W4392719125",
    "https://openalex.org/W4389218004",
    "https://openalex.org/W3184465860",
    "https://openalex.org/W4389820524",
    "https://openalex.org/W3203511201",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4383994288"
  ],
  "abstract": "TalkWithMachines aims to enhance human-robot interaction by contributing to\\ninterpretable industrial robotic systems, especially for safety-critical\\napplications. The presented paper investigates recent advancements in Large\\nLanguage Models (LLMs) and Vision Language Models (VLMs), in combination with\\nrobotic perception and control. This integration allows robots to understand\\nand execute commands given in natural language and to perceive their\\nenvironment through visual and/or descriptive inputs. Moreover, translating the\\nLLM's internal states and reasoning into text that humans can easily understand\\nensures that operators gain a clearer insight into the robot's current state\\nand intentions, which is essential for effective and safe operation. Our paper\\noutlines four LLM-assisted simulated robotic control workflows, which explore\\n(i) low-level control, (ii) the generation of language-based feedback that\\ndescribes the robot's internal states, (iii) the use of visual information as\\nadditional input, and (iv) the use of robot structure information for\\ngenerating task plans and feedback, taking the robot's physical capabilities\\nand limitations into account. The proposed concepts are presented in a set of\\nexperiments, along with a brief discussion. Project description, videos, and\\nsupplementary materials will be available on the project website:\\nhttps://talk-machines.github.io.\\n",
  "full_text": "TalkWithMachines: Enhancing Human-Robot\nInteraction for Interpretable Industrial Robotics\nThrough Large/Vision Language Models\nAmmar N. Abbas\nSchool of Computer Science\nTechnological University Dublin\nDublin, Ireland\nORCID: 0000-0002-2578-5137\nCsaba Beleznai\nAssistive & Autonomous Systems\nAIT Austrian Institute of Technology\nVienna, Austria\nORCID: 0000-0003-1880-2979\nAbstract—TalkWithMachines aims to enhance human-robot\ninteraction by contributing to interpretable industrial robotic\nsystems, especially for safety-critical applications. The presented\npaper investigates recent advancements in Large Language Mod-\nels (LLMs) and Vision Language Models (VLMs), in combination\nwith robotic perception and control. This integration shall allow\nrobots to understand and execute commands given in natural\nlanguage and to perceive their environment through visual\nand/or descriptive inputs. Moreover, by translating the LLM’s\ninternal states and reasoning into text that humans can easily\nunderstand, it shall ensure that operators gain a clearer insight\ninto the robot’s current state and intentions, which is essential\nfor effective and safe operation. Our paper outlines four LLM-\nassisted simulated robotic control workflows, which explore (i)\nlow-level control, (ii) the generation of language-based feedback\nthat describes the robot’s internal states, (iii) the use of visual\ninformation as additional input, and (iv) the use of robot structure\ninformation for generating task plans and feedback, taking\nthe robot’s physical capabilities and limitations into account.\nThe proposed concepts are presented in a set of experiments,\nalong with a brief discussion. Project description, videos and\nsupplementary materials will be available on the project website:\nhttps://talk-machines.github.io.\nIndex Terms—large/vision language models, autonomous sys-\ntems, interpretable robotics.\nI. I NTRODUCTION\nIn recent years, research on autonomous systems has in-\ncreasingly concentrated on aspects that ensure these systems\nare interpretable and easy to understand, particularly in sectors\nwhere safety and user trust are of primary importance [1],\n[2]. This heightened emphasis on interpretability underscores\nthe importance of creating natural forms of human-machine\ninteraction, such as low-level control based on human language\n[3], [4]. In safety-critical industries, however, this is often\nnot recommended, and formal/traditional methods are used to\ndevelop verifiably safe autonomous systems [5], [6]. On the\nother hand, despite their existing shortcomings in providing\ndeterministic and reliable outputs, recent breakthroughs in\nLarge Language Models (LLMs) and Vision Language Models\n(VLMs) present promising possibilities as a potential inter-\nmediate communication and reasoning layer between humans\nand industrial machines or robots [7]–[10]. These develop-\nments encourage us to investigate the capabilities of LLMs\nand VLMs for robotic control and improved interpretability,\nparticularly in translating a machine’s internal states, obser-\nvations, predictions, and actions into language that is easily\nunderstandable by humans [11], [12].\nWe build on the recent advancements in LLMs and VLMs\nto demonstrate their potential for robotic manipulation and\nperception. Here, perception refers to the text- and/or image-\nbased representation of both the robot’s internal and external\nstates as input, used for reasoning. Much of the recent research\nhas concentrated on utilizing LLMs in robotics as high-level\nplanning modules [13], [14] or coding platforms [15]–[17].\nSince high-level concepts are effectively represented in text\nwhile low-level signals and percepts often exist in different\ndomains, it is commonly assumed that LLMs are not suited\nfor low-level control [3], [4]. Consequently, our objectives\nconcentrate on two less-explored areas: (i) language-based\nlow-level control, and (ii) transforming machine states and\nintentions into interpretable text, referred to as verbalization\nof machine states.\nLLMs have proven to be considered as the general pattern\nmachines [18], capable of understanding and combining low-\nlevel patterns for complex control strategies [3]. [18] demon-\nstrates the LLMs as general pattern machines for robotic\nmanipulation with examples involving sequence transforma-\ntion, sequence completion, sequence improvement, and as a\ncontroller. [3] similarly utilizes the foot contact pattern as\nan interface to bridge the communication between human\nlanguage commands and low-level quadruped robot control.\nThese combinations of basic control patterns demonstrate\ncomplex behavior understanding and lead to the commu-\nnication of actions to the user and its perception of the\nenvironment, which is also the aim of this paper. We utilize\nrobotic arm manipulation within a simulation environment as\nour experimental setup and propose an improved interface\nconcept for communication between the user and the robot.\nThe questions that we aim to address through this study are:\n1. Can low-level control command patterns be generated\narXiv:2412.15462v1  [cs.RO]  19 Dec 2024\nthrough few-shot human language-to-pattern examples given\nto the LLM? 2. Are LLMs capable of generating complex\ntrajectories (such as grasping an object while avoiding ob-\nstacles) using basic control patterns? 3. Can LLMs perceive\nand maintain contextual awareness of the environment from\na single observation and prompt the user about anomalies\n(such as object penetration, unobserved conditions, environ-\nmental constraints, or indications of obstructions)? 4. Can\nLLMs make autonomous decisions based on the real-world\nproperties of the environment (such as object characteristics\nor ground conditions)? 5. Are LLMs in robotic manipulation\ntasks generalizable to incorporate indirect communication cues\nbased on common sense? 6. Can a stack of time-consecutive\nimage frames depicting the environment enhance the situa-\ntional awareness of the VLM? 7. Can the information from\nthe Unified Robot Description Format (URDF) [19] help\nLLM/VLM to become structure-aware and identify out-of-\nreach or singularity conditions? 8. Does integrating specific\nconstraints within the environment help LLM perceive the\nenvironment for unsafe command execution? 9. Can LLM\nextrapolate or predict the next steps in a control sequence or\nbased on the environment perception?\nThe paper is structured as follows: Section II provides an\noverview on related methods. Section III describes our human-\nrobot interaction methodology via LLMs/VLMs. Section IV\nexplains the design of experiments used for addressing the\noutlined research questions. Section V provides our experi-\nmental findings. Finally, Section VI concludes the paper and\noutlines future directions.\nII. S TATE-OF-THE -ART\nLLMs and specifically Generative Pre-Trained Transformers\n(GPT) version 4 [20] have proven to generalize beyond natural\nlanguage as zero-shot or few-shot models towards the domain\nof control and automation [21], [22].\nLarge/Vision Language Models for Robotics: [23] provides\nan extensive literature review on the application of foun-\ndation models towards general-purpose robotics, discussing\nthe current gap and future directions. [24] present a broad\nreview on LLMs for robotics. Common research categories\nwithin the domain of LLM-assisted robotics can be defined as\n(i) high-level planning, (ii) context/structure-aware perception,\n(iii) code generation, and (iv) low-level control.\nHigh-Level Planning: [25] use a multimodal LLM approach\nas a planner to combine audio, video, speech, images, and text\nas input to generate action sequences. LLM-based planners are\nused by [26] to overcome the limitation of acquiring new skills\nthat are not available in the predefined set of skills, proposing\na lifelong learning framework.\nContext- and Structure-Aware Perception:[27] demonstrate\na step-wise closed-loop planning and control architecture\nthrough visual feedback using a diffusion model. LLM is\nused to generate step-by-step textual subgoals which are then\ntransformed into visual subgoals through a diffusion model\nfor planning. A goal-conditioned policy then transforms these\nvisual subgoals to control actions. Similarly, [28], [29] employ\nTABLE I\nQUALITATIVE COMPARISON TO STATE -OF-THE -ART METHODS\nMethod Task\nPlan-\nning\nMotion\nPlan-\nning\nAdapt-\nable\nExplain-\nable\nInter-\nactive\nLearn-\nable\nReAct [33] Yes No Yes Partial Yes Partial\nInner\nMonologue [13] Yes No Yes Yes Yes Partial\nVoxPoser [34] Yes Partial No No Yes Partial\nGPT-4 [4] Yes Partial Yes Limited Limited Partial\nSayTap [3] No Yes Partial Yes No No\nChatGPT [10] Yes No Yes Partial Yes Partial\nOurs Yes Yes Yes Yes Yes Yes\nthe use of language and visual observation to form a closed-\nloop context-aware perceptive planner. [30], [31] include the\nattributes of objects and their spatial information, as well.\nCode Generation: LLMs have been used to demonstrate the\ncapabilities of generating robot manipulation plans and the\ncorresponding codes to accomplish it [32]. [10] assess the\neffectiveness of ChatGPT by integrating a high-level function\nlibrary on various techniques, including free-form dialogue,\nXML parsing, code synthesis, task-specific prompting, and\nclosed-loop reasoning. These methods are tested across a\nspectrum of tasks, from simple logical problems to complex\nscenarios like aerial navigation and robotic manipulation.\nLow-Level Control [3] use foot contact pattern as a com-\nmunication interface to transform human language commands\ninto low-level quadruped robot control. They define a prompt\nstructure that specifies the role of the LLM, the movement\ndescriptions, general pattern rules, and finally some examples\nfor few-shot response fine-tuning. The control pattern output\nis then transferred to a locomotion controller. The pattern\ngeneration is generalized enough also to understand indirect\ncommands and infer the robot’s behavior. [4] experiment on\nthe use of LLMs in generating low-level control without\nproviding control examples for few-shot fine-tuning.\nLLMs as General Pattern Machines: [18] showcases the\ncapability of LLMs to utilize arbitrary symbol sequences as\ninput patterns for interpretation, synthesis, and application in\nreal-world tasks. The examples include (i) sequence transfor-\nmation: where a symbolic representation of a pattern is synthe-\nsized for transformations and validated on the abstract reason-\ning corpus [35]. (ii) Sequence completion: where a numeric\nrepresentation of simple function classes is extrapolated. This\nwas extended to robotic tasks such as drawing or sweeping,\nwhere the human demonstrates an initial trajectory which later\nis extrapolated through LLM. (iii) Sequence improvement:\nincludes online policy tuning involving symbolic and numeric\npatterns. It was demonstrated on the CartPole [36] control\noptimization problem and teaching robot via clicker signals.\nA. Contributions\nThis paper investigates the capabilities of LLMs and VLMs\nin interpreting human language for low-level control, gener-\nating complex trajectories, and acquiring contextual aware-\nness of the environment.The study shows that these models\ncan independently make informed decisions, interpret indirect\ncommunication cues, and sustain situational awareness by\nanalyzing images that represent the robot and its environmental\nconditions. Furthermore, we show that information on the\nrobot’s physical structure results in an awareness of physical\nlimitations and safe command execution. Our experimental\nresults are directed towards developing interpretable, safe, and\nhuman-centric robotic systems, a concept which is validated\nvia simulated robotic arm manipulation and a human-robot\ncommunication interface. Table I presents a qualitative com-\nparison of our proposed research concepts against the current\nstate-of-the-art.\nIII. M ETHODOLOGY\nIn this section, we define the methodology used for address-\ning the research questions of LLM-based robot control and\nfeedback on robot states. Firstly, we discuss the communica-\ntion framework to bridge human language with control and\nperception. Secondly, we define the movement descriptions\nand general pattern rules used for the translation of human\ncommand to low-level control. Lastly, we define the prompt\nstructures used in this study.\nA. Framework\nThe framework designed for evaluating LLMs in robotic\nmanipulation and perception is shown in Fig. 1. The interface\nallows the user to input a text and/or image prompt which\nis passed to GPT-4 through a Python client developed by\n[37]. The output from GPT is parsed through Python and\nthe control commands from the response are extracted. The\ncontrol commands are then transferred via ROS industrial [38]\ncontroller to a Gazebo simulation environment [39], chosen\nfor its ease of simulation-to-reality transfer. For environment\nperception, the visual and/or textual observations from the\nsimulation are fed back to GPT-4 to generate a response in\nhuman language. In our workflow LLM can play a dual role:\ninterpretation for control and for perception. This modular\napproach can help to run both in parallel or other cases use the\nconventional approach for control and the perception module\nof LLM as a safety check.\nUser: Move towards the\nobject\nRobot: I am moving\ntowards the object…\nState (S t)\nPatternInput Control Action\nFig. 1. Proposed framework: human-robot interaction interface from language\nto command and visual environment perception to human language.\nB. Movement Descriptions and General Pattern Rules\nRobot movements are described along the X, Y , and Z\naxes, representing left/right, forward/backward, and up/down\ndirections respectively, with movements quantified as multi-\nples of 0 (no movement) or ±1mm. The movement patterns are\nformatted in X, Y , Z, and G, where G represents the binary\ngripper control, with only three possible values for each axis:\n-1, 0, and 1, indicating negative movement, no movement,\nand positive movement. The gripper control values are 0 and\n1, indicating open and closed states. Each pattern comprises\nfour labeled lines: X for left/right, Y for forward/backward,\nZ for up/down, and G for gripper state, where 1 signifies a\nclosed gripper to grasp objects and 0 signifies an open gripper\nto release objects. The robot’s movement and gripper state can\nbe represented by the vector M as shown in Equation 1 that\ndefines the robot’s movements along the X, Y , and Z axes\n(see 2 for illustration, where x, y, and z are represented by\nred, green, and blue color, respectively) as well as the gripper\nstate.\nM =\n\n\nX\nY\nZ\nG\n\n (1)\nwhere:\nX ∈ {−1, 0, 1} (Left/Right movement: -1mm, 0mm, +1mm)\nY ∈ {−1, 0, 1} (Fwd/Bwd movement: -1mm, 0mm, +1mm)\nZ ∈ {−1, 0, 1} (Up/Down movement: -1mm, 0mm, +1mm)\nG ∈ {0, 1} (Gripper state: 0 = open, 1 = closed)\nFig. 2. Illustration of the axes of robot movement.\nC. Prompt Structures\nThe proposed methodology involves the development of\nconcepts for language-based control and verbalized machine\nstates. These solutions aim to improve the communication\nbetween humans and robots, enhancing the human operator\nexperience. We aim to bridge the communication gap between\nhuman language to low-level robot control for achieving tasks\nsuch as grasping the object, moving the object, placing the\nobject, obstacle avoidance, and further typical tasks. LLM-\nassisted control and task interpretation was examined in a\ngradual manner (see Fig. 3), where incrementally added\ninformation was introduced (as part of the LLM-input) to\nfacilitate LLM-based reasoning and control. In the followings,\nwe describe these input information increments in more detail.\n1) Baseline Control Prompt Structure: The baseline prompt\nstructure is inspired by the prompt structure and pattern rule\ndefined by [3]. It starts by describing the role of the LLM,\nfollowed by basic movement definitions and descriptions.\nOnce the movement primitives are defined, a specific task\nis defined using the pattern rules defined before, in form\nFig. 3. Prompt structures with incrementally added information, facilitating\nLLM-based reasoning and robot control. Blue indices refer to the experiments\nin Section V.\nof a human language input. The outputs are then concretely\ndefined by a few examples for fine-tuning (few-shot prompting\npractice). The structure is shown in the top row of Fig. 3.\n2) Context-Aware Perception: Verbalized Current Machine\nStates: Apart from controlling the robot through human\nlanguage, there is another key objective in safety-critical\napplications: to perceive the environment and plan/execute\nmachine actions accordingly. Therefore, we propose to use\nLLMs/VLMs to (i) capture the states of the machine and the\nenvironment from text and/or image, as perception, and (ii) to\ncommunicate these states to the operator. Thus, in a second set\nof experiments, we extend the baseline control prompt (Section\nIII-C1) to provide information on internal ( EE = end-effector)\nand external (scene/object) states. The additional information\non internal and external states injected into the LLM results\nin a more complete awareness about its internal states and\nsurrounding context (denoted as its internal mental model ).\nTurning this internal model into text, or the verbalization of\nrobot states represents thus in this workflow an additional\nkey added functionality. Such a verbalization shall render\nmachine/robot operation more transparent towards a user. This\nextended set of prompt-based information is depicted in the\nsecond row of Fig. 3.\n3) Added Perception Through Visual Information: The vi-\nsualization of the robot simulation (see Section III-A) can\nbe rendered into one or multiple images, depicting the robot\npose/state from an external observer’s point-of-view. Thus\nthese images can enhance the prompt in form of a vision-\nbased perception through one or a stack of visual frames. Thus\nimage-based input can complement the previously described\ntext-based movement input. Furthermore, this additional input\ncan be added in a time-incremental fashion, where image\nframes and text-based state observations are captured per time-\nstep and then are transferred to LLM/VLM after a partial\ntask completion or after a pre-defined step size. To enhance\nrobustness, the image frame stacking step was extended to\ninclude image frames from multiple viewpoints. These frames\nare stacked together along the horizontal dimension, forming\nimage-sets organized in image rows. This added set of input\nmodality is shown in the third row of Fig. 3.\n4) Enhancing Awareness to Robot Structure: Information\non the physical structure of the robot can markedly comple-\nment the LLM’s awareness to internal states and its degrees\nof freedom. This integration allows the LLM to generate more\naccurate and executable task plans by considering the robot’s\nphysical capabilities and limitations. Integrating information\nabout the physical structure can be provided in two ways: (i)\nby a URDF model (interpreted as text input) or any other\nrepresentation (e.g. technical specification sheet); and/or (iii)\nthrough visual means depicting the robot’s physical structure.\nIV. D ESIGN OF EXPERIMENTS\nThe design of experiments is split into two main categories:\n(i) LLM-based control and (ii) LLM/VLM-based perception,\nwhich focuses on situational and structural awareness.\nA. Language-Based Control\nThe scenarios used for validating language-based control\ninvolve (i) grasping task and (ii) grasping task with obstacle\navoidance. Fig. 4 shows the corresponding setup, consisting a\nred object and a black obstacle. Initial observations in form\nof object type and positions are provided as prompts before\nprocessing the task.\nFig. 4. Design of experiments for control.\nB. Perception\nPerception is validated in terms of both control and safe op-\neration. The tasks performed include (i) a pick-and-place task\nand (ii) a grasping task that requires obstacle avoidance. The\nperception involves validating the consequences of actions,\ninterpreting the environment, and placements of the objects\nin the environment. We add the possibility to assign real-\nworld object and enviroment properties within the scene, as\nshown in Fig. 5. Different colors in the figure imply different\nattributes, which can be defined in the prompt. For raising\nawareness w.r.t. robot’s and environment’ physical structure,\nwe provide the robot URDF [19] description (text from an xml\nfile) and textual description of the environment. The prompts\nare provided with the initial observation list before progressing\nwith the task.\nFurthermore, to enhance the capabilities of perception, we\nuse VLM to ingest a stack of image frames, which are time-\nincremental rendered views from external (with respect to\nthe robot) viewpoints. Multiple views, highlighted robot/object\nparts and volumes (e.g. safe zone) are used in the experiments.\nExamples are shown for a single view (Fig. 6) and for a multi-\nview case (Fig. 7). The figures show a scenario where robot\nFig. 5. Design of experiments for perception through LLM.\nmanipulation involves a red object, whereas the gray bounding\nbox representing a safe operational zone. Highlighted objects\nand spatial relations are on one hand to facilitate VLM in\nits reasoning task, on the other hand it aids the user in\nunderstanding the time-evolution of executed actions.\nFig. 6. Single-view evolution of an executed task within a safe zone, generated\nas a frame stack.\nFig. 7. Multi-view evolution of a grasping task, generated as a frame stack.\nV. E XPERIMENTS\nIn this section, we address the research questions of Section\nI as experiments. Experiments follow the structure depicted in\nFig. 3.\nA. Baseline Language-Based Control\nThese experiments involve (i) control pattern optimization\nand (ii) text-based perceptive control by providing initial\nobservations of the environment.\n1) Baseline Control Pattern: For the low-level control, first,\nwe employed a control pattern similar to [3] and adapted it for\nrobot manipulation. Full prompt of the pattern rule definitions\nis given in Appendix A.\nInput: Move forward and pick the cube\nOutput: X: 00000000000000000000000000\nY: 11111111111111111111111111\nZ: 00000000000000000000000000\nG: 00000000000000000000000001\nInput: Move backward and release the cube\nOutput:\nX: 00000000000000000000000000\nY:-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1-1\nZ: 00000000000000000000000000\nG: 11111111111111111111111110\nInput: Move right\nOutput: X: 11111111111111111111111111\nY: 00000000000000000000000000\nZ: 00000000000000000000000000\nG: 00000000000000000000000000\nThe baseline control experiments exclude environmental\nobservations and focus on evaluating the prompt structure and\ncontrol patterns. They involved general movement control,\ngrasping tasks, and grasping while avoiding obstacles, with\ninitial tests conducted without gripper actions.\n2) Improved Control Pattern: There were several draw-\nbacks involved with the baseline control pattern due to its\nhighly redundant content: (i) duration to generate the prompt,\n(ii) lower accuracy ( cm accuracy), (iii) higher failure rate\ndue to repetitive pattern generation and loss of information\nin memory. To improve pattern execution, two changes were\nimplemented: (i) modifying the control pattern structure to in-\nclude multiples of 0s, 1s, or -1s, and (ii) breaking the trajectory\ninto steps to improve task robustness, as demonstrated in an\nablation study by [4]. The improved control pattern is shown\nbelow:\nInput: Move forward 100mm and pick a cube\nOutput: X: [0] *100\nY: [1]*100\nZ: [0]*100\nG: [0]*99 + [1]*1\nInput: Move backward 50mm and release the grasped cube\nOutput: X: [0] *50\nY: [-1]*50\nZ: [0]*50\nG: [1]*49 + [0]*1\nInput: Move right for 70mm\nOutput: X: [1] *70\nY: [0]*70\nZ: [0]*70\nG: [0]*70\nThis improvement helped in (i) faster response, (ii) higher ac-\ncuracy (mm accuracy), (ii) lower failure rate, and (iv) improved\ngeneralization. The comparison for improved generalization is\nshown below which shows the control pattern generalizing to\ncomplex functional representation rather than just 0s and 1s:\nExperiment: Circular movement\nInput: Move in a circle\nOutput: X: 111110000-1-1-1-1-1-1-1-100001111\nY: 000111111110000-1-1-1-1-1-1-1-100\nZ: 0000000000000000000000000\nG: 0000000000000000000000000\nbecomes:\nInput: Move in a circle\nOutput: X: [cos(t) for t in range(360)]\nY: [sin(t) for t in range(360)]\nZ: [0]*360\nG: [0]*360\nExperiment: Obstacle avoidance\nInput: Move in a pattern to avoid an obstacle in a path and\npick the cube on the right\nOutput: X: [0] *10 + [1]*50 + [0]*10 + [0]*1\nY: [0]*71\nZ: [1]*10 + [0]*50 + [-1]*10 + [0]*1\nG: [0]*70 + [1]*1\n3) Comparative Analysis: We compare the improved con-\ntrol pattern strategy with the baseline adapted from SayTap\n[3]. A comparison is shown in Table II for tasks involving\ngrasping and obstacle avoidance. The average time for pattern\ngeneration, final goal error, and success rate of completing\nthe task were calculated over 10 runs on the prompt for\neach task with variations in the prompt sentence structure\nand morphology, grasping object, end effector, and obstacle\npositions. The test prompts for both methods were consistent.\nFrom the results of obstacle avoidance, it can be observed\nthat TalkWithMachines is slower due to the generation of\ncomparatively complex trajectories, however, has a better\nspatial accuracy and success rate.\nTABLE II\nCOMPARISON WITH THE PRIOR ART ’S CONTROL PATTERN GENERATION\nSTRATEGY .\nTask SayTap [3] Ours\nTime\n(s)\nError\n(m)\nSuccess Time\n(s)\nError\n(m)\nSuccess\nGrasping 6.90 0.01 0.90 6.10 0.00 1.00\nObstacle\navoidance\n5.84 0.05 0.3 6.82 0.018 0.5\nB. Context Aware Reasoning and Verbalized Machine States\nThese experiments emphasize text-based perception prior\nto or during manipulation. Perception enables LLMs to ask\nusers for clarification if commands are incomplete or if the\nenvironment context needs further details (e.g., unobserved\nobjects to grasp or dangerous action). Furthermore, the LLM\nis able summarize the planned action in a step-wise manner\nand indicate the validity of each step.\nExperiment: Obstructed position avoidance\nInput: place the grasped red cube to position [-270, 190,\n30] with observation = [[110, 490, 140], [-170, 190,\n30], [-270, 190, 30]]\nOutput: Given the current position of the end effector with\nthe grasped red cube at [-170, 190, 30], and the\ndesired location to place the red cube at [-270, 190,\n30], the end effector needs to move from its current\nposition to the desired location. However, there is a\nblue cube already present at the desired location\n[-270, 190, 30]. Placing the red cube at this location\nwould result in the red cube penetrating the blue\ncube, which is not possible as the cubes are solid and\nimpenetrable. Therefore, this action is not possible.\nPlease provide a different location to place the red\ncube. Let me know if you need help with anything else.\nLLM can also reason about possible outcomes based on object\nand scene real-world attributes:\nExperiment: Awareness to real-world attributes\nInput: Move the red cube to the yellow zone\nOutput: Moving the red cube to the yellow zone would be\ndangerous as the red cube is made of wood and the\nyellow zone represents fire. This could potentially\nstart a fire. Therefore, this action is not\nrecommended. Here’s the explanation:\nThe red cube is made of wood, which is a combustible\nmaterial.\nThe yellow zone represents fire, which can ignite\ncombustible materials.\nMoving the red cube to the yellow zone could potentially\nstart a fire, causing a dangerous situation.\nSo, I would advise against performing this action. Safety\nshould always be the top priority when operating\nrobotic systems. Please provide another task that\ndoesn’t involve potential hazards.\nInput: Move the blue cube to the yellow zone\nOutput: Given the environmental constraints and the\nproperties of the objects, moving the blue cube (made\nof ice) to the yellow zone (representing fire) would\nbe dangerous as it could cause the ice to melt.\nTherefore, I cannot provide a control pattern for this\naction. It’s crucial to ensure the safety of the\nenvironment and respect the real-world properties of\nthe objects. Please provide another task that doesn’t\nviolate these properties.\nC. Added Context via Image-based Inputs\nIn these experiments, we test VLMs for becoming aware\nof spatial relations within the environment and detecting\nanomalies. First, we experiment with a single visual stacks of\nframes as shown in Fig. 8. The full prompt of the experiment\nis given in Appendix B.\nOutput: Collision imminent.\nReason: End effector descends, force negative.\nFig. 8. Perception via image-based observations and text descriptions.\nRegion of Interest for Enhanced Accuracy When provid-\ning an enlarged view of a specific region of interest, it helps\nto identify a failure case, whereas a far-away view does not\nsuffice to solve the task.\nOutput: **No collision, safe grasp.**\nReason: **Cube aligns with gripper path.**\nOutput: Collision imminent.\nReason: Cube misaligned, end effector close.\nFig. 9. Perception with far-away (top) and enlarged view (bottom) of a region\nof interest.\nD. Robot Structure Awareness via URDF import\nWe added structural perception via URDF (Universal\nRobotic Description Format) (see Fig. 3), which involves\ndefining each robot component, its shape and limitations and\npermits the LLM to acquire a structural concept about the\nrobotic arm. To verify the acquired percept within the LLM, a\nvisualization of the hierarchical robot structure was generated\nusing an LLM-generated Mermaid [40] code of the robot part\nhierarchy. The generated 10-level hierarchical visualization\ncorrectly captured the robot’s physical structure (not shown).\nE. Experiments using the complete workflow D\nUsing the workflow D (Fig. 3) including image-, text- and\nURDF-based information on external scene and robot-internal\nstructure, we carried out additional experiments.\n1) Operation within a Safe Zone: A cuboid-shaped safe\noperation zone was defined by text and image-stack prompts,\nas an environment constraint. The correct VLM responses in\ntwo scenarios are shown in Fig. 10.\nOutput: No collision; grasp in range.\nRed cube clear; path safe.\nOutput: Collision imminent.\nRed cube misaligned; wall proximity.\nFig. 10. Spatially-aware task executions via text (scene+URDF) and visual\ninputs, using a single-viewpoint frame stack.\n2) Obstacle Avoidance: Fig. 11 illustrates the task execu-\ntion for obstacle avoidance using human language and URDF\ninput only. It can be seen that the LLM can correctly infer the\nnecessary obstacle avoidance from the input observation list,\neven without explicit related instructions.\nInput: Move towards the object. <observation list>\nFig. 11. Result image sequence showing a successful end effector approach\nwhile avoiding an obstacle (black object).\n3) Stacking Operation: In this successful task demonstra-\ntion (see Fig. 12), multiple spatial reasoning steps are involved:\nreaching and grasping the object, moving it to a safe height,\nand placing it on top of another object based on the positions\nand dimensions listed in the input observations.\n4) Pick and Place Into a Zone: The successful task ex-\necution shown in Fig. 13 involves placing an object into a\nuser-defined zone, with the necessary information provided in\nthe initial observation list.\nInput: Move the red cube on top of the blue cube.\n<observation list>\nFig. 12. Result of a stacking task, shown as a generated image sequence.\nInput: Grasp the red cube and place it in zone A.\n<observation list>\nFig. 13. Result of the task execution moving an object into a zone.\n5) Attribute-based Object Sorting into Respective Zones:\nIn this example control pattern generation considers real-\nworld object/scene attributes to generate sorting decisions.\nSuch a workflow imitates an industrial sorting scenario with\ncomplicated decision logic. An LLM is equipped with a\nfoundation-level world knowledge, hence complex logic rules\ndo not need to be explicitly defined.\nInput: Move the cubes to the zones\nOutput: Given the physical properties of the cubes and the\nzones, it would be hazardous to move the red cube\n(Fire) to the green zone (Forest) as it could\npotentially cause a fire. Similarly, moving the blue\ncube (Ice) to the yellow zone (Fire) would not be\nappropriate as the ice would melt. Therefore, I will\nprovide a control pattern that safely moves the red\ncube to the yellow zone and the blue cube to the green\nzone.\nControl Pattern for Red Cube (Fire) to Yellow Zone (Fire):\ngenerated control sequence (not shown in detail)\nControl Pattern for Blue Cube (Ice) to Green Zone (Forest):\ngenerated control sequence (not shown in detail)\nObserved failure modes: From all conducted experiments, it\ncan be concluded that tasks involving the spatial interaction of\na small number of objects (such as the end effector, a single\ntarget, and one obstacle) were consistently successful. How-\never, failures occurred in spatial reasoning when numerous\nclosely spaced objects or strict spatial constraints (such as\na tight safety zone) needed to be taken into account during\nthe task. Enhancing spatial resolution (for example, through a\nzoomed-in view as shown in Fig. 9) helped to mitigate some\nof these failure modes.\nVI. C ONCLUSIONS AND FUTURE WORK\nThis paper illustrates the integration of large language\nmodels and vision-language models to improve human-robot\ninteraction. Through experiments involving robotic arm ma-\nnipulation in simulated environments, we demonstrated that\nLLMs can successfully generate low-level control commands,\nperceive their surroundings, and communicate effectively with\nhumans. Notable findings include the capability of LLMs to\ncreate intricate trajectories, comprehend environmental con-\nstraints, and make autonomous decisions. Future work will\nfocus on implementing these systems in real-world scenarios,\nenhancing contextual and structural understanding through\nreal-time visual streams, developing self-learning mechanisms,\nand exploring applications in safety-critical industries. The\ngoal is to create interpretable and safe robotic systems that\nenhance user trust and foster natural human-robot interaction.\nREFERENCES\n[1] A. Kuznietsov, B. Gyevnar, C. Wang, S. Peters, and S. V . Albrecht, “Ex-\nplainable ai for safe and trustworthy autonomous driving: A systematic\nreview,” arXiv preprint arXiv:2402.10086 , 2024.\n[2] H. He, J. Gray, A. Cangelosi, Q. Meng, T. M. McGinnity, and J. Mehnen,\n“The challenges and opportunities of human-centered ai for trustworthy\nrobots and autonomous systems,” IEEE Transactions on Cognitive and\nDevelopmental Systems, vol. 14, no. 4, pp. 1398–1412, 2021.\n[3] Y . Tang, W. Yu, J. Tan, H. Zen, A. Faust, and T. Harada, “Saytap: Lan-\nguage to quadrupedal locomotion,” arXiv preprint arXiv:2306.07580 ,\n2023.\n[4] T. Kwon, N. Di Palo, and E. Johns, “Language models as zero-\nshot trajectory generators,” in 2nd Workshop on Language and Robot\nLearning: Language as Grounding , 2023.\n[5] X. Yin, B. Gao, and X. Yu, “Formal synthesis of controllers for safety-\ncritical autonomous systems: Developments and challenges,” arXiv\npreprint arXiv:2402.13075, 2024.\n[6] M. Luckcuck, “Using formal methods for autonomous systems: Five\nrecipes for formal verification,” Proceedings of the Institution of Me-\nchanical Engineers, Part O: Journal of Risk and Reliability , vol. 237,\nno. 2, pp. 278–292, 2023.\n[7] A. Brohan, Y . Chebotar, C. Finn, K. Hausman, A. Herzog, D. Ho,\nJ. Ibarz, A. Irpan, E. Jang, R. Julian et al., “Do as i can, not as i say:\nGrounding language in robotic affordances,” in Conference on robot\nlearning. PMLR, 2023, pp. 287–318.\n[8] Y . Jin, D. Li, A. Yong, J. Shi, P. Hao, F. Sun, J. Zhang, and B. Fang,\n“Robotgpt: Robot manipulation learning from chatgpt,” IEEE Robotics\nand Automation Letters , 2024.\n[9] C. Lynch, A. Wahid, J. Tompson, T. Ding, J. Betker, R. Baruch,\nT. Armstrong, and P. Florence, “Interactive language: Talking to robots\nin real time,” IEEE Robotics and Automation Letters , 2023.\n[10] S. H. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, “Chatgpt for\nrobotics: Design principles and model abilities,” IEEE Access, 2024.\n[11] C. Yang, Y . Zhu, and Y . Chen, “A review of human–machine cooperation\nin the robotics domain,”IEEE Transactions on Human-Machine Systems,\nvol. 52, no. 1, pp. 12–25, 2021.\n[12] W. Chen, C. Yu, H. Wang, Z. Wang, L. Yang, Y . Wang, W. Shi,\nand Y . Shi, “From gap to synergy: Enhancing contextual understand-\ning through human-machine collaboration in personalized systems,” in\nProceedings of the 36th Annual ACM Symposium on User Interface\nSoftware and Technology, 2023, pp. 1–15.\n[13] W. Huang, F. Xia, T. Xiao, H. Chan, J. Liang, P. Florence, A. Zeng,\nJ. Tompson, I. Mordatch, Y . Chebotaret al., “Inner monologue: Embod-\nied reasoning through planning with language models,” in Conference\non Robot Learning . PMLR, 2023, pp. 1769–1782.\n[14] H. Huang, F. Lin, Y . Hu, S. Wang, and Y . Gao, “Copa: General\nrobotic manipulation through spatial constraints of parts with foundation\nmodels,” arXiv preprint arXiv:2403.08248 , 2024.\n[15] J. Liang, W. Huang, F. Xia, P. Xu, K. Hausman, B. Ichter, P. Florence,\nand A. Zeng, “Code as policies: Language model programs for embodied\ncontrol,” in 2023 IEEE International Conference on Robotics and\nAutomation (ICRA). IEEE, 2023, pp. 9493–9500.\n[16] I. Singh, V . Blukis, A. Mousavian, A. Goyal, D. Xu, J. Tremblay, D. Fox,\nJ. Thomason, and A. Garg, “Progprompt: Generating situated robot\ntask plans using large language models,” in 2023 IEEE International\nConference on Robotics and Automation (ICRA) . IEEE, 2023, pp.\n11 523–11 530.\n[17] N. Di Palo and E. Johns, “Keypoint action tokens enable in-context\nimitation learning in robotics,” arXiv preprint arXiv:2403.19578 , 2024.\n[18] S. Mirchandani, F. Xia, P. Florence, B. Ichter, D. Driess, M. G. Arenas,\nK. Rao, D. Sadigh, and A. Zeng, “Large language models as general\npattern machines,” arXiv preprint arXiv:2307.04721 , 2023.\n[19] D. Tola and P. Corke, “Understanding urdf: A dataset and analysis,”\nIEEE Robotics and Automation Letters , 2024.\n[20] OpenAI, “Gpt-4 technical report,” arXiv preprint arXiv:2303.08774 ,\n2023.\n[21] Y . Yang, Q. Zhang, C. Li, D. S. Marta, N. Batool, and J. Folkesson,\n“Human-centric autonomous systems with llms for user command\nreasoning,” in Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision , 2024, pp. 988–994.\n[22] Y . Shentu, P. Wu, A. Rajeswaran, and P. Abbeel, “From llms to actions:\nLatent codes as bridges in hierarchical robot control,” arXiv preprint\narXiv:2405.04798, 2024.\n[23] Y . Hu, Q. Xie, V . Jain, J. Francis, J. Patrikar, N. Keetha, S. Kim, Y . Xie,\nT. Zhang, Z. Zhao et al., “Toward general-purpose robots via foundation\nmodels: A survey and meta-analysis,” arXiv preprint arXiv:2312.08782,\n2023.\n[24] Z. Kira, “Awesome-llm-robotics,” 2022. [Online]. Available: https:\n//github.com/GT-RIPL/Awesome-LLM-Robotics\n[25] M. Kambara, C. Hori, K. Sugiura, K. Ota, D. K. Jha, S. Khurana, S. Jain,\nR. Corcodel, D. Romeres, and J. Le Roux.\n[26] M. Parakh, A. Fong, A. Simeonov, T. Chen, A. Gupta, and P. Agrawal,\n“Lifelong robot learning with human assisted language planners,” in\nCoRL 2023 Workshop on Learning Effective Abstractions for Planning\n(LEAP), 2023.\n[27] S. Sharan, R. Zhao, Z. Wang, S. P. Chinchali et al. , “Plan diffuser:\nGrounding llm planners with diffusion models for robotic manipulation,”\nin Bridging the Gap between Cognitive Science and Robot Learning in\nthe Real World: Progresses and New Directions , 2024.\n[28] C. Jin, W. Tan, J. Yang, B. Liu, R. Song, L. Wang, and J. Fu,\n“Alphablock: Embodied finetuning for vision-language reasoning in\nrobot manipulation,” arXiv preprint arXiv:2305.18898 , 2023.\n[29] C. H. Song, J. Wu, C. Washington, B. M. Sadler, W.-L. Chao, and Y . Su,\n“Llm-planner: Few-shot grounded planning for embodied agents with\nlarge language models,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2023, pp. 2998–3009.\n[30] Y . Hu, F. Lin, T. Zhang, L. Yi, and Y . Gao, “Look before you leap:\nUnveiling the power of gpt-4v in robotic vision-language planning,”\narXiv preprint arXiv:2311.17842 , 2023.\n[31] M. Shridhar, L. Manuelli, and D. Fox, “Cliport: What and where\npathways for robotic manipulation,” in Conference on robot learning .\nPMLR, 2022, pp. 894–906.\n[32] J.-P. T ¨oberg and P. Cimiano, “Generation of robot manipulation plans\nusing generative large language models,” in 2023 Seventh IEEE Inter-\nnational Conference on Robotic Computing (IRC) . IEEE, 2023, pp.\n190–197.\n[33] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. Narasimhan, and Y . Cao,\n“React: Synergizing reasoning and acting in language models,” arXiv\npreprint arXiv:2210.03629, 2022.\n[34] W. Huang, C. Wang, R. Zhang, Y . Li, J. Wu, and L. Fei-Fei, “V oxposer:\nComposable 3d value maps for robotic manipulation with language\nmodels,” arXiv preprint arXiv:2307.05973 , 2023.\n[35] F. Chollet, “On the measure of intelligence,” arXiv preprint\narXiv:1911.01547, 2019.\n[36] M. Towers, J. K. Terry, A. Kwiatkowski, J. U. Balis, G. d.\nCola, T. Deleu, M. Goul ˜ao, A. Kallinteris, A. KG, M. Krimmel,\nR. Perez-Vicente, A. Pierr ´e, S. Schulhoff, J. J. Tai, A. T. J. Shen,\nand O. G. Younis, “Gymnasium,” Mar. 2023. [Online]. Available:\nhttps://zenodo.org/record/8127025\n[37] V . Sakkas et al. , “sydney.py: Python client for copilot (formerly\nbing chat),” May 2024, version 0.21.0. [Online]. Available: https:\n//github.com/vsakkas/sydney.py\n[38] ROS-Industrial, “Ros-industrial repositories,” May 2024, accessed:\n2024-05-30. [Online]. Available: https://github.com/ros-industrial\n[39] O. S. R. Foundation, “Gazebo classic: A dynamic multi-robot\nsimulator,” May 2024, version 11.14.0. [Online]. Available: https:\n//github.com/gazebosim/gazebo-classic\n[40] mermaid js, “Mermaid,” https://github.com/mermaid-js/mermaid, 2024,\naccessed: 2024-06-12.\nAPPENDIX\nA. Baseline Pattern Rule\n# Role\nYou are a robot control pattern manipulation expert.\nYour job is to give an end effector position control\npattern in the format of multiples as will be shown in\nthe examples at the end based on the input.\nAssume the necessary conditions.\nThe robot moves simultaneously in the directions defined.\nYou will always give the output in the correct format no\nmatter what the input is.\nJust give the control pattern and avoid too much\nexplanation.\n# Movement Descriptions\nThe following are descriptions of robot movements:\n1. Moving left or right is represented as moving in the\npositive or negative X direction for 1mm or -1mm,\nrespectively.\n2. Moving forward or backward is represented as moving in\nthe positive or negative Y direction for 1mm or -1mm,\nrespectively.\n3. Moving up or down is represented as moving in the\npositive or negative Z direction for 1mm or -1mm,\nrespectively.\n# General Pattern Rules\nThe following are rules for describing the robot movement\npatterns:\n1. You should output the movement patterns in X, Y, and Z\nformat and the gripper binary control in G format.\n2. There are only three values to choose from for each of\nthe axes: [-1, 0, 1], which represents movement along\nthat axis.\n3. There are only two values to choose from for gripper\ncontrol [0, 1], which represents the gripper closed\nor open.\n4. A pattern has four lines, each of which represents the\nrobot movement pattern of the end effector and\ngripper control.\n5. Each line has a label. \"X\" for the movement in the left\nor right direction, \"Y\" for the movement in the\nforward or backward direction, and \"Z\" for the\nmovement in the up or down direction. \"G\" represents\ngripper open or close.\n6. For the first three lines (X, Y, and Z), \"0\" represents\nno movement in that direction, \"1\" represents\npositive movement in that direction for 1mm, and \"-1\"\nrepresents negative movement in that direction for\n-1mm. For the fourth line (G), \"0\" represents the\ngripper opened, and \"1\" represents the gripper\nclosed. If the object has to remain grasped, the\ngripper control should be 1 and to release the object\nthe gripper value should be 0.\n# Examples\nInput: Move forward 100mm and pick a cube\nX: [0]*50\nY: [1]*100\nZ: [0]*30\nG: [0]*99 + [1]*1\nInput: Move backward 50mm and release the grasped cube\nX: [0]*10\nY: [-1]*50\nZ: [0]*20\nG: [1]*49 + [0]*1\nInput: Move left for 70mm\nX: [1]*70\nY: [0]*20\nZ: [0]*10\nG: [0]*70\nB. Single-View Image Stack with Observation Sequences\n# Role\nYou are the robot and the sensor observation is given in a\nlist of observations.\n# Observation Description\nEach observation list is ordered as:\n[[if the cube is grasped or not in a\nboolean value],\n[position of end effector (x, y, z)],\n[velocity of end effector (x, y, z)],\n[red cube position (x, y, z)],\n[blue cube position (x, y, z)],\n[force on end effector in z]]\nTo help visually, the camera image is given as a real-time\nframe stack starting from left.\n# Task Description\nIt is a grasping task with the object being the red cube.\n# Constraints\nThe object should be properly aligned in the gap with the\ngripper fingers otherwise the object will collide with\nit while the gripper is moving toward it.\n# Objective\nDescribe the robot’s state and if at any point it is going\nto or has already collided etc.\n# Safety Checks\nAlso, check if the black cube does not obstruct the red\ncube. Predict the future state or if any dangerous\nanomaly is about to occur.\n# Output\ngive output response only in 50 characters.\ngive the reason for the decision based on observation\nsequence or images only in another 50 characters.\nInput image stack:\n",
  "topic": "Robot",
  "concepts": [
    {
      "name": "Robot",
      "score": 0.7522861957550049
    },
    {
      "name": "Computer science",
      "score": 0.7400444746017456
    },
    {
      "name": "Human–computer interaction",
      "score": 0.6790841221809387
    },
    {
      "name": "Workflow",
      "score": 0.5999166965484619
    },
    {
      "name": "Task (project management)",
      "score": 0.5673528909683228
    },
    {
      "name": "Natural language",
      "score": 0.5474782586097717
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5251635909080505
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49319738149642944
    },
    {
      "name": "Control (management)",
      "score": 0.47057268023490906
    },
    {
      "name": "Perception",
      "score": 0.44946178793907166
    },
    {
      "name": "Human–robot interaction",
      "score": 0.4318593442440033
    },
    {
      "name": "Programming language",
      "score": 0.17324379086494446
    },
    {
      "name": "Engineering",
      "score": 0.16124498844146729
    },
    {
      "name": "Systems engineering",
      "score": 0.096194326877594
    },
    {
      "name": "Database",
      "score": 0.08390134572982788
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}