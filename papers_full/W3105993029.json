{
    "title": "Large Product Key Memory for Pretrained Language Models",
    "url": "https://openalex.org/W3105993029",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2114721219",
            "name": "Gyuwan Kim",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2107573675",
            "name": "Tae Hwan Jung",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2400680200",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W2251939518",
        "https://openalex.org/W4288024261",
        "https://openalex.org/W3098985395",
        "https://openalex.org/W3039578880",
        "https://openalex.org/W2970401203",
        "https://openalex.org/W639708223",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W4288284003",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W4288087322",
        "https://openalex.org/W3016309009",
        "https://openalex.org/W2951008357",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2955227499",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2973727699",
        "https://openalex.org/W4287747814",
        "https://openalex.org/W4288289156",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2779809129",
        "https://openalex.org/W3102844651",
        "https://openalex.org/W4294611325",
        "https://openalex.org/W2995154514",
        "https://openalex.org/W2540419089",
        "https://openalex.org/W2952556884",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2613718673",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4394666973",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W2995575179",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2895976713"
    ],
    "abstract": "Product key memory (PKM) proposed by Lample et al. (2019) enables to improve prediction accuracy by increasing model capacity efficiently with insignificant computational overhead. However, their empirical application is only limited to causal language modeling. Motivated by the recent success of pretrained language models (PLMs), we investigate how to incorporate large PKM into PLMs that can be finetuned for a wide variety of downstream NLP tasks. We define a new memory usage metric, and careful observation using this metric reveals that most memory slots remain outdated during the training of PKM-augmented models. To train better PLMs by tackling this issue, we propose simple but effective solutions: (1) initialization from the model weights pretrained without memory and (2) augmenting PKM by addition rather than replacing a feed-forward network. We verify that both of them are crucial for the pretraining of PKM-augmented PLMs, enhancing memory utilization and downstream performance. Code and pretrained weights are available at https://github.com/clovaai/pkm-transformers.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4060–4069\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n4060\nLarge Product Key Memory for Pretrained Language Models\nGyuwan Kim∗\nClova AI, NA VER Corp.\ngyuwan.kim@navercorp.com\nTae-Hwan Jung∗†\nKyung Hee University\nnlkey2022@gmail.com\nAbstract\nProduct key memory (PKM) proposed by\nLample et al. (2019) enables to improve pre-\ndiction accuracy by increasing model capac-\nity efﬁciently with insigniﬁcant computational\noverhead. However, their empirical applica-\ntion is only limited to causal language model-\ning. Motivated by the recent success of pre-\ntrained language models (PLMs), we investi-\ngate how to incorporate large PKM into PLMs\nthat can be ﬁnetuned for a wide variety of\ndownstream NLP tasks. We deﬁne a new mem-\nory usage metric, and careful observation us-\ning this metric reveals that most memory slots\nremain outdated during the training of PKM-\naugmented models. To train better PLMs by\ntackling this issue, we propose simple but ef-\nfective solutions: (1) initialization from the\nmodel weights pretrained without memory and\n(2) augmenting PKM by addition rather than\nreplacing a feed-forward network. We verify\nthat both of them are crucial for the pretraining\nof PKM-augmented PLMs, enhancing mem-\nory utilization and downstream performance.\nCode and pretrained weights are available at\nhttps://github.com/clovaai/pkm-transformers.\n1 Introduction\nLarger model capacity has brought improvement\nin accuracy by enabling better modeling of data.\nHowever, increasing model capacity causes a sig-\nniﬁcant increase in computational cost at both train-\ning and inference time despite better accuracy. To\naddress this issue, Lample et al. (2019) propose\nproduct key memory (PKM) that enables very efﬁ-\ncient and exact nearest neighbor search in a large\nnumber of learnable memory slots. They substi-\ntute a feed-forward network (FFN) in a transformer\nblock (Vaswani et al., 2017) with a PKM layer.\nAugmenting large PKM layers to networks allows\n∗Equal contribution.\n†TJ was an intern at Clova AI while doing this work.\nModel # Layers # Params\nInference\nSpeed\n(batch/sec)\nBERTBASE 12 110M 79.8\nBERTBASE +PKM 12 506M 61.4\nBERTBASE +ResM 12 515M 59.3\nBERTLARGE 24 340M 43.1\nBERTLARGE +PKM 24 860M 37.2\nBERTLARGE +ResM 24 876M 36.1\nTable 1: Comparison of inference speed between dif-\nferent model sizes and the memory layers. We run each\nmodel for the classiﬁcation task with batch size 1, and\nmeasure inference speed on a single V100 GPU. We\nfollow the model size settings of BERT (Devlin et al.,\n2018). We use two memory layers with the recom-\nmended setting of PKM hyper-parameters following\nLample et al. (2019) as described in §5. As marked\nbold, BERTBASE with our proposed residual memory\n(ResM) is much faster than BERTLARGE, while having\nmore parameters.\nincreasing model capacity, with only a slight in-\ncrease in inference time. Lample et al. (2019) prove\nthe efﬁciency of PKM on causal language models\n(CLMs) in terms of the superior trade-off between\nperplexity and inference speed. For instance, they\nachieve a PKM-augmented CLM with only 12 lay-\ners that is more accurate and twice faster than a\nbaseline with 24 layers.\nHowever, usage of PKM with a pretrained lan-\nguage model (PLM) such as BERT (Devlin et al.,\n2018) that is helpful for downstream tasks (Wang\net al., 2018) has not been examined in the liter-\nature. In our experiments, plain PKM improves\nmasked language modeling (MLM) perplexity but\nnot downstream performance.\nWe measure various memory utilization metrics\nto analyze how many memory slots contribute to\nthe model prediction. Careful examination about\nmemory utilization during and after the training\ndemonstrates that only a few memory slots are be-\n4061\ning used importantly (§3.1). We attribute this phe-\nnomenon, called a catastrophic drift, to the sparsely\nupdated memory parameters. The lower memory\nutilization implies that model capacity from mem-\nory is not fully exploited. It promotes us to develop\nmethods that can overcome this issue.\nWe found that initialization from weights pre-\ntrained without memory is essential for pretrain-\ning PKM-augmented PLMs. Moreover, rather\nthan replacing an FFN to a PKM as Lample et al.\n(2019) do, we show that adding PKM to a trans-\nformer layer (Vaswani et al., 2017) with a resid-\nual connection (He et al., 2016) without remov-\ning FFN is advantageous. Both the initialization\n(§4.1) and our proposed residual memory (ResM,\n§4.2) prevent a sudden change of transformer\nparameters, thus allow to train memory parame-\nters better by less suffering from the catastrophic\ndrift. Consequently, we obtain PKM-augmented-\nBERTBASE having comparable accuracy and faster\nthan BERTLARGE.\nAs demonstrated in Table 1, a model with a large\nmemory is much faster than a model having twice\nmany transformer layers, although it has far more\nweights. ResM does not slow down inference speed\nmuch. Accuracy comparison between them will\nappear in the later sections.\nThe main contributions of this work are summa-\nrized as follows. First, we explore how to incorpo-\nrate PKM to PLMs to be ﬁnetuned for downstream\ntasks and ﬁnd that simple application does not work\nwell. Secondly, we attribute this to a catastrophic\ndrift during the training by careful monitoring of\nmemory utilization. Lastly, we propose simple\nyet effective solutions to tackle the observed catas-\ntrophic drift problem: (1) weight initialization with-\nout PKM and (2) the residual memory layer. We\nempirically verify that both of them are crucial to\nachieve improved accuracy. In our knowledge, this\nis the ﬁrst work that successfully applies PKM to\nPLMs.\n2 Background\n2.1 Transformers and Product Key Memory\nA transformer encoder maps a sequence of input\ntokens into a sequence of continuous representa-\ntions based on a self-attention mechanism (Vaswani\net al., 2017). Transformer architecture is a stack\nof sub-layers, and each sub-layer consists of a\nmulti-head attention layer and a feed-forward layer.\nDue to the remarkable prediction accuracy, a trans-\nformer becomes standard architecture in natural\nlanguage processing.\nOn the other hand, memory architecture can also\nbe used to design a function that maps a contin-\nuous representation to another representation as\na layer in neural networks. When a query vector\nis given in a standard memory-augmented neural\nnetwork, the memory layer ﬁnds k-NN keys and\nreturns a weighted sum of corresponding value vec-\ntors. These weights are normalized scores of the\ndot product between the query vector and the key\nvectors.\nLample et al. (2019) propose product key mem-\nory (PKM) that can signiﬁcantly increase model\ncapacity based on fast and exact nearest neighbor\nsearch. They plug a PKM layer in a transformer\narchitecture, especially by switching an existing\nfeed-forward layer to it, while keeping similar com-\nputational efﬁciency.\nWe explain the mechanism of PKM here to be\nself-contained. A product key is a pair of sub-keys,\nmeaning that there are |K|= C2 different memory\nslots when the codebook size of each sub-key is C.\nA given query vector is partitioned to the dimension\nof half-size. The score with a product key is the\nsum of the dot product between the sub-query vec-\ntor and the sub-key vector. We can increase the size\nof key space effectively with sufﬁcient C. Exact\nnearest neighbor search in the product key set can\nbe done efﬁciently by ﬁrst ﬁnding k-NN in each\nsub-key space and then ﬁnding k-NN again from\nk2 combinations of sub-key pairs. In addition, a\nmulti-head memory attention mechanism like self-\nattention in transformers is used to increase the\nrepresentation power of the memory layer.\n2.2 Pretrained Language Models\nTransfer learning from pretrained language mod-\nels (PLMs) has brought a paradigm shift in NLP\nwith a remarkable improvement in a wide range\nof downstream tasks. Based on a transformer ar-\nchitecture (Vaswani et al., 2017), BERT (Devlin\net al., 2018) is trained with two pretraining tasks,\n(1) masked language modeling (MLM) and (2) next\nsentence prediction (NSP), which achieves signif-\nicant improvement in performance on ﬁne-tuning\ntasks. RoBERTa (Liu et al., 2019) removes the NSP\nand increases the batch size and training corpus to\ntrain a more robust language model. It indicates\nthat larger batch size and training data beneﬁt the\nperformance of PLM. In these trends, recently, lan-\n4062\nguage models with much larger parameters (Raffel\net al., 2019; Shoeybi et al., 2019; Brown et al.,\n2020) are trained with a huge amount of text cor-\npus. Despite their remarkable performance, the\ncomputational cost in training and inference is pro-\nhibitive. Improving trade-off between accuracy and\nefﬁciency is one of the crucial research directions.\n2.3 Memory-Augmented Language Models\nMemory augmented neural networks (Weston et al.,\n2014; Sukhbaatar et al., 2015) have the ability to\nsolve complex algorithmic tasks and decouple the\nmemory capacity from the number of model param-\neters. Chandar et al. (2016) propose a hierarchi-\ncal memory network to access from large external\nmemory efﬁciently. Rae et al. (2016) enable train-\ning a large memory in neural networks efﬁciently\nvia a sparse read and write mechanism. However, it\nrequires regular re-training to avoid a catastrophic\ndrift. REALM (Guu et al., 2020) also suffers from\na similar issue, so refresh the index asynchronously\nevery several hundred training steps.\nIn addition to Lample et al. (2019), augment-\ning memory architecture to a language model is a\npromising research direction. For example, EaE\n(F´evry et al., 2020) and FaE (Verga et al., 2020)\njointly train a memory that is interleaved in a trans-\nformer and dedicated to entities (or facts) with\nsparse updates, and access to only a small portion\nof the memory in inference time. On the other hand,\neach memory slot in Lample et al. (2019) and ours\ndoes not have explicit meaning.\nSukhbaatar et al. (2019) augments the self-\nattention layers with persistent memory vectors and\nremoves the feed-forward layers. Khandelwal et al.\n(2019) augments a pretrained language model with\nthe nearest neighbor language model that retrieves\nk-nearest neighbors from the datastore consisting\nof the key-value pairs of a context vector and the tar-\nget word built from training data. Khandelwal et al.\n(2019) also only considers causal language mod-\neling, and applying the same approach to masked\nlanguage modeling widely used for PLMs is non-\ntrivial.\n3 Memory Utilization Analysis\nAs shown in our experiment (Table 2), large PKM\nprovides a signiﬁcant gain in masked language\nmodeling in terms of perplexity. However, surpris-\ningly, downstream task performance ﬁnetuned from\nPKM-augmented PLMs is similar to or sometimes\nworse than that without PKM in our experiments.\nNevertheless, it is challenging to investigate what\nis going on under the hood. We presume that this\nfrustrating outcome come from the catastrophic\ndrift which will be explained later ( §3.1) and it\nfosters us to scrutinize memory utilization (§3.2)\nthoroughly.\n3.1 Catastrophic Drift\nPKM is jointly trained with other transformer pa-\nrameters. In every training step, only a small por-\ntion (chosen as k-NN) of memory parameters are\nsparsely updated. Even if a memory slot is selected\nas top-k, the frequency is low or it is only selected\nas low-rank in top-k, the update of memory param-\neters relevant to this slot might be marginal.\nIf memory parameters (especially value vectors)\nare not updated (or rarely updated) for a while, they\nbecame stale. Stale parameters are unlikely to be\nmatched with newly updated model parameters so\nthat they will get remain unused. We call this situ-\nation a catastrophic drift. Moreover, catastrophic\ndrift will be more severe in ﬁnetuning because it\nrelies on a small number of data and training steps.\nWe hypothesize this catastrophic drift occurs dur-\ning the training of a PKM-augmented LM, and it\nis one plausible cause of poor performance. This\nproblem is overlooked by Lample et al. (2019) be-\ncause it is concealed by increasing the number of\nmemory slots |K|, heads H, or k-NN. With a suf-\nﬁcient size of memory hyper-parameters, memory\nusage (see §3.2 for the deﬁnition) becomes close\nto 100%. For example, in Lample et al. (2019) and\nour experiments, memory usage is almost 100%\nwhen using 4 memory heads, selecting 32 keys per\nhead, and using 5122 memory slots. Considering\nonly top-k memory usage, memory parameters are\nseemingly regarded as used effectively to their full\nextent.\n3.2 Memory Utilization Metrics\nFollowing Lample et al. (2019), we measure the\nmemory utilization of trained PKM-augmented\nmodels in terms of (1) memory usage and (2) KL\ndivergence with the uniform distribution using held-\nout data. Besides standard memory usage, we pro-\npose to measure top-1 memory usage that only\ncounts memory slots as used when selected as top-\n1 rather than top-k and use it to monitor the degree\nof catastrophic drift.\nFor every memory slot, we count the num-\nber of selection as k-NN (or top-1) and sum the\n4063\nweights throughout all memory accesses: u′\ni =∑\nx δ(w(x)i >0), t′\ni = ∑\nx δ(arg maxj w(x)j =\ni), and w′\ni = ∑\nx w(x)i, where w(x)i is the weight\nof the key iaccessed in the memory when an input\nxis given to the language model with the memory.\nMemory usage (MU) is the fraction of values that\nare accessed at least once. Top-1 memory usage\n( ˜MU) is the fraction of values that are accessed as\ntop-1 at least once. KL divergence with the uni-\nform distribution is calculated for normalized aver-\nage counts (KLu) and normalized average weights\n(KLw). Formally, we can calculate those values\nby\nMU = 1\n|K|\n∑\ni\nδ(ui >0),\n˜MU = 1\n|K|\n∑\ni\nδ(ti >0),\nKLu = log(|K|) +\n∑\ni\nuilog(ui),\nKLw = log(|K|) +\n∑\ni\nwilog(wi)\nwhere |K|is the number of memory slots, and u,\nt, and ware the normalized value of u′, t′, and w′,\nrespectively, as sum to 1.\n4 Pretraining PKM-augmented PLMs\nLample et al. (2019) propose PKM and show its\nadvantage in causal language modeling. We in-\nvestigate how to extend the usage of large PKM\nto PLMs such as BERT (Devlin et al., 2018) and\nRoBERTa (Liu et al., 2019) that can be used as\na good initialization point for downstream tasks,\nresulting in a great performance.\nBy monitoring top-1 memory usage, we observe\nthat catastrophic drift really occurs. Low memory\nutilization PKM-augmented PLMs means that the\nmodel does not fully exploit its increased capacity\nof the memory and thus is likely not to get accu-\nracy gain much. To resolve the catastrophic drift,\nwe introduce additional modiﬁcations for better\npretraining: initialization from pretrained weights\n(§4.1) and residual memory layer (§4.2).\n4.1 Initialization from Pretrained Weights\nLearning transformer parameters and memory pa-\nrameters together from scratch is difﬁcult due to\nthe discrepancy between them as described in §3.1.\nTo remedy this issue, we ﬁrst pretrain a language\nmodel without memory layers, and then pretrain\nagain a model with memory layers initialized from\nthe already pretrained language model. Trans-\nformer parameters will be gradually changed since\nthey are initialized from a well-trained language\nmodel. We expect that staleness would be miti-\ngated as a result. Despite requiring two stages of\ntraining, a trained language model with initializa-\ntion performs much better and has higher memory\nusage than that with the same amount of training\nsteps from the scratch, as shown in Table 2.\n4.2 Residual Memory Layer\nHe et al. (2016) propose ResNet to train very deep\nconvolution networks. A residual connection en-\nables easier optimization and gains accuracy from\nincreased depth. We borrow this idea by introduc-\ning a residual connection in augmenting a PKM to\nalleviate the catastrophic drift.\nWhen we replace an FFN layer of pretrained\nnetworks with the PKM layer, it struggles to ﬁt\ndata in an early stage because the function of this\nlayer suddenly changed to random function from\na well-trained one (see a green line of Figure 3).\nWe hope to prevent this undesirable circumstance\nwhile keeping strong representation power of prod-\nuct key memory. To this end, we propose residual\nmemory (ResM) layer, adding the memory layer to\na transformer block in the form of residual connec-\ntion (He et al., 2016) instead of replacing the FFN\nlayer. Due to the residual connection, the function\nof the layer does not deviate severely from that of\nthe original pretrained weights, and it helps to start\nat a stable point.\nFigure 1 displays how the residual memory layer\nis different from the previous models. To be more\nprecise, we can formulate these layers to\nx′= LN(x+ αFFN (x) +βPKM (x)),\nwhere LN indicates layer normalization (Ba et al.,\n2016). (α,β) = (1,0),(0,1),(1,1) corresponds\nto FFN layer, PKM layer, and ResM layer, respec-\ntively.\n5 Experiment Setup\n5.1 Product Key Memory\nOur implementation is based on HuggingFace’s\nTransformers library1 (Wolf et al., 2019), and the\nPKM part is borrowed from the XLM repository.2\n1https://github.com/huggingface/transformers\n2https://github.com/facebookresearch/XLM\n4064\nMemory Layer \n(PKM)\n+\nFeed-Forward \nLayer (FFN)\n+\nFeed-Forward \nLayer (FFN)\n+\nMemory Layer \n(PKM)\n(a) FFN layer\nMemory Layer \n(PKM)\n+\nFeed-Forward \nLayer (FFN)\n+\nFeed-Forward \nLayer (FFN)\n+\nMemory Layer \n(PKM) (b) PKM layer\nMemory Layer \n(PKM)\n+\nFeed-Forward \nLayer (FFN)\n+\nFeed-Forward \nLayer (FFN)\n+\nMemory Layer \n(PKM) (c) ResM layer\nFigure 1: Illustration of the layers for the comparison. (a) displays feed-forward layer (FFN) in vanilla Transformer\narchitecture (Vaswani et al., 2017). (b) is the original version of product-key memory (PKM) layer (Lample et al.,\n2019) that replaces FFN. (c) is our proposed ResM layer. Instead of replacing FFN to PKM, ResM adds PKM in\naddition to FFN as a residual connection.\nModel\nMemory MLM\n˜MU KL u KLw WT-2 WT-103 PG-19\n(4L/8L) (%) (4L/8L) (4L/8L) (ppl) (ppl) (ppl)\n(a) BERTBASE\n† - - - 3.49 3.86 6.18\n(b) +500k steps - - - 3.40 3.72 5.88\n(c) +PKM 2.2/84.1 1.62/0.89 1.99/1.13 3.26 3.39 5.53\n(d) +ResM 75.0/81.0 1.50/0.71 1.80/0.92 3.26 3.36 5.45\n(e) +Init +PKM 97.4/95.7 0.53/0.69 0.68/0.88 3.14 3.26 5.22\n(f) +Init +ResM 98.2/97.3 0.45 /0.46 0.58 /0.60 3.10 3.20 5.14\nTable 2: Experimental results of pre-training PKM-augmented PLMs. Because standard memory usage is almost\n100%, we omit it in the table. Top-1 memory usage and KL divergence are calculated at the 4th and 8th layers. †:\nwe pre-train BERTBASE by ourself.\nWe add two memory layers in the intermediate lay-\ners at regular intervals: i.e., {4,8}in 12 layer mod-\nels, and {2,4}in 6 layer models. We will explore\nthe effect of changing the number of the position of\nmemory layers in the future. We use 5122 (≈262k)\nmemory slots with 4 memory heads and select 32\nkeys per head for each memory layer for all exper-\niments. We set the dimension of key vectors and\nvalue vectors to 256 and 768, respectively. We use\nquery batch normalization to increase key coverage\nduring training. We measure the top-1 memory us-\nage and the KL divergence to measure how much\nthe model effectively uses memory capacity.\n5.2 Pretraining\nWe use 12 layer BERT BASE models with and\nwithout PKM. For pretraining, we use English\nWikipedia and BookCorpus (Zhu et al., 2015) as a\ntraining corpus like BERT (Devlin et al., 2018), in\ntotal 17GB. We use the same vocabulary and tok-\nenizer with Devlin et al. (2018). We train models\nwith batch size of 1024 sequences for 500,000 steps.\nWe use Adam optimizer (Kingma and Ba, 2014)\nwith learning rate of 1e-4 and linear warmup sched-\nuler over the ﬁrst 10,000 steps. The memory values\nare learned with a sparse update of learning rate\n1e-3, following Lample et al. (2019). With half-\nprecision training3 on 32 NVIDIA V100 GPUs,\npretraining took 2.8 days without PKM and 5.1\ndays with PKM (or with ResM).\nTo evaluate pretrained models themselves, we\nmeasure the perplexity of masked language mod-\neling on the test set of WikiText-2, WikiText-103,\nand PG-19 (Rae et al., 2019). Since the pretrain-\ning corpus covers WikiText-2 and WikiText-103,\nperplexity on them is a proxy to the training per-\nplexity. Meanwhile, because the PG-19 dataset\ncame from different sources, perplexity on PG-19\ncan be regarded as the test perplexity.\n5.3 Finetuning\nFor ﬁne-tuning, we use SQuAD 1.1 (Rajpurkar\net al., 2016) and GLUE (Wang et al., 2018) bench-\nmark as downstream tasks. Following other PLM\nliterature, including RoBERTa (Liu et al., 2019),\n3https://github.com/NVIDIA/apex\n4065\nDataset lr bsz # epoch warmup\nratio\nweight\ndecay\nmax seq\nlength\nSQuAD 1.1 5e-5 32 3 0.06 0.01 384\nGLUE 2e-5 32 10 0.06 0.1 128\nTable 3: Fine-tuning hyper-parameters for downstream\ntasks, SQuAD 1.1 and GLUE. We use 128 doc stride\nfor SQuAD 1.1 dataset.\nwe report dev set results instead of the test set to\ncompare our variants. We report a median of 5 runs\nwith different random seeds for each ﬁne-tuning\ntask. We measure exact match (EM) and F1 scores\non SQuAD 1.1. For QQP, which is the binary clas-\nsiﬁcation task, the F1 score is used for the GLUE\nleaderboard. However, we use the accuracy as the\nmetric for development set because the F1 score\nvaries a lot depending on random seeds. Finetuning\ndetails appear in Table 3.\n6 Pretraining Results\nTable 2 shows the experimental results of pre-\ntraining. We compare models with/without the\ninitialization and PKM vs. ResM. We use\nBERTBASE architecture of 12 transformer layers\nwithout next sentence prediction following Liu\net al. (2019) for our pretraining experiments. For\nthe fair comparison between BERTBASE and PKM-\naugmented-BERTBASE after the initialization, we\ntrain BERTBASE with longer steps, but the improve-\nment was marginal.\nMemory UtilizationSurprisingly, the top-1 mem-\nory usage of the PKM-augmented PLM at the 4th\nlayer is about 2%, which is remarkably low, though\ntop-32 memory usage at this layer is almost 100%.\nIn other words, the model does not take advantage\nof the lower memory layer effectively.\nWith a residual connection, the top-1 memory\nusage of all layers become reasonably high. Similar\nto He et al. (2016), the residual connection helps to\nlearn deep networks with memory, resulting in im-\nproved accuracy. Moreover, with the initialization\nfrom pretrained weights, top-1 memory usage is\nmore than 95%. With the initialization and ResM,\ntop-1 memory usage increases, and KL divergence\ndecreases signiﬁcantly, implying better exploita-\ntion of the memory layers. It becomes possible by\npreventing memory parameters not to suffer from\nthe catastrophic drift.\nWe check when each memory slot is used at last\namong saved checkpoints. Then, we count the num-\n200K\n250K PKM\nResM\nInit+PKM\nInit+ResM\n40K\n60K\n0K 100K 200K 300K 400K 500K\nTraining Step\n0\n2K\n4K\n6K\nCount\n(a) 4th Layer\n200K\n250K PKM\nResM\nInit+PKM\nInit+ResM\n40K\n60K\n0K 100K 200K 300K 400K 500K\nTraining Step\n0\n2K\n4K\n6K\nCount\n(b) 8th Layer\nFigure 2: Histogram for staleness evaluation of PKM-\naugmented PLMs. We save model checkpoints ev-\nery 100k step during the entire 500k pre-training steps.\nThis histogram illustrates how many memory slots are\nused at last for each saved checkpoint. For example, if\na key is used at 200k model checkpoint and never used\nafter that, then it is likely to keep its state as stale af-\nter 200k. Because the total number of memory slots is\nﬁxed to 5122, the model having boxes toward the right\nin the graph is better.\nber of slots depending on the last used checkpoint.\nFigure 2 indirectly indicates how many memory\nslots are kept not selected as top-1. This ﬁgure pro-\nvides evidence that a model with the initialization\nand residual memory prevents staleness compared\nto a model with plain PKM.\nMasked Language ModelingAugmenting large\nPKM always improves masked language modeling\ncompared to a model without memory. Figure 3\nshows the training curve of the models after the\ninitialization. It proves that the residual connection\nprevents a deviation of the PKM at the beginning\n(bigger initial perplexity) even with the initializa-\ntion from the pretrained weight. Although they are\nconverged to a similar perplexity after very long\ntraining steps, the initial perplexity of PKM is much\nbigger than that of ResM. In sum, both the initial-\nization from pretrained PLM and the residual mem-\nory layer are beneﬁcial for PLM with a memory to\nperform better in masked language modeling.\n4066\nModel\nQA GLUE\nSQuAD 1.1 MNLI -(m/mm) QQP QNLI SST-2 CoLA Avg\n(EM/F1) (Acc) (Acc) (Acc) (Acc) (Matt) -\n(a) BERTBASE\n† 82.7/89.8 84.3/84.5 91.0 89.3 92.8 60.8 83.8\n(b) +500k steps 83.3/90.1 84.8/84.9 91.2 89.2 92.4 61.4 84.0\n(c) +PKM 81.9/89.1 84.4/85.0 91.1 89.0 93.6 59.7 83.8\n(d) +ResM 81.5/89.4 84.6/84.8 91.0 88.2 93.2 62.8 84.1\n(e) +Init +PKM 83.8/90.6 85.8/85.6 91.2 90.0 93.6 63.6 85.0\n(f) +Init +ResM 83.9/90.8 86.0 /85.8 91.4 90.4 94.0 64.1 85.3\n(g) BERTBASE\n⋆ 81.1/88.5 83.9/84.4 91.0 88.4 92.9 59.8 83.4\n(h) BERTLARGE\n⋆ 83.3/90.6 86.2/86.1 91.4 90.4 93.8 64.1 85.3\nTable 4: Experimental results of ﬁne-tunining PKM-augmented PLMs. Model (a)-(f) are the same one from Table\n2. ⋆: we borrow pretrained weights of BERT BASE and BERTLARGE from (Devlin et al., 2018). We ﬁne-tune these\nmodels on SQuAD 1.1 (Rajpurkar et al., 2016) and GLUE tasks (Wang et al., 2018).\n29.2\n29.4\n0 100K 200K 300K 400K 500K\nTrainining Step\n3.2\n3.4\n3.6\n3.8\n4.0\n4.2\n4.4\n4.6\n Init+PKM\nInit+ResM'\nInit+ResM\nTraining Perplexity\nFigure 3: Training curves of MLM perplexity ver-\nsus training steps during the pre-training of PKM-\naugmented MLMs. Y-axis is zoomed in the low per-\nplexity region. Initialized from the pre-trained BERT,\nPKM (green) replaces FFN to PKM, ResM (blue) add\nPKM as a residual connection. ResM’ (red) is the same\nwith ResM but randomly re-initialize FFN.\n7 Finetuning Results\nTable 4 shows the experimental results of ﬁnetun-\ning using our pretrained models.\nDownstream PerformanceAlthough large PKM\nhelps masked language modeling, the downstream\nperformance of several tasks with plain PKM is\nworse than the baseline without memory. We think\nthis is because the catastrophic drift problem is es-\npecially severe in the ﬁne-tuning step. Downstream\ndataset size and the number of training steps are\ntoo small to ﬁt memory parameters accordingly.\nBetter memory utilization coming from the ini-\ntialization and the residual connection also leads\nto better downstream accuracy in most of the\ndatasets. We report the ﬁne-tuning results using\nthe weights of pretrained BERT LARGE from De-\nModel Memory\nUpdate\nQA GLUE\nSQuAD 1.1 MNLI -m SST-2 CoLA\n(EM/F1) (Acc) (Acc) (Matt)\n(c) +PKM Y 81.9/89.1 84.4 93.6 59.7\nN 82.0/89.0 84.1 93.0 56.5\n(d) +ResM Y 81.5/89.4 84.6 93.2 62.8\nN 82.2/89.5 84.3 92.7 59.9\n(e) +Init +PKM Y 83.8/90.6 85.8 93.6 63.6\nN 83.7/90.4 85.5 93.3 58.8\n(f) +Init +ResM Y 83.9/90.8 86.0 94.0 64.1\nN 84.2/90.8 85.8 93.3 61.6\nTable 5: Ablation study on ﬁxing memory parameters\nduring ﬁne-tuning.\nvlin et al. (2018) in Table 4.4 We believe that our\nbest PKM-augmented-BERTBASE would have com-\nparable performance with BERTLARGE even after\npretraining it by ourselves, while much faster as\ndescribed in Table 1.\nOn the assumption that updating memory pa-\nrameters sparsely using a limited number of data\nand training steps might be vulnerable to the catas-\ntrophic drift, we try to ﬁx memory parameters dur-\ning ﬁne-tuning as in Table 5. However, it degrades\nthe downstream performance.\nMemory UtilizationTable 6 shows the memory\nusage and KL divergence of ﬁne-tuned PKM-\naugmented models. Comparison of ﬁne-tuned\nPKM-augmented models in terms of the mem-\nory usage has similar trends with that of pretrain-\n4Unfortunately, we could not pretrain BERTLARGE, so we\nwill prepare it after the submission. In our pretraining experi-\nments, we use almost same settings but larger batch size (256\nvs 1024) than Devlin et al. (2018). The difference between our\npretrained BERTBASE (a) and Google BERTBASE (g) and the\ndifference between our ResM-augmented BERTBASE with the\ninitialization (f) and Google BERTLARGE (h) are insigniﬁcant.\n4067\nModel Memory\nPosition\nMNLI-m SST-2 CoLA\nMU ˜MU KL u KLw MU ˜MU KL u KLw MU ˜MU KL u KLw\n(%) (%) (%) (%) (%) (%)\n(c) +PKM 4 99.4 1.2 2.14 2.36 79.0 0.6 3.23 3.49 60.5 0.4 4.62 4.89\n8 99.7 68.2 2.30 2.47 83.7 35.8 2.59 2.76 61.8 22.2 5.51 5.67\n(d) +ResM 4 98.9 64.5 2.46 2.71 79.3 35.6 3.84 4.08 61.5 24.1 4.01 4.20\n8 99.9 66.7 1.87 2.02 84.6 32.5 2.34 2.51 73.3 22.7 2.05 2.21\n(e) +Init +PKM 4 100.0 81.8 1.33 1.46 91.2 42.3 3.52 3.76 72.7 26.3 4.11 4.29\n8 99.9 78.5 1.76 1.95 86.3 35.8 2.81 3.05 65.5 21.9 4.72 4.93\n(f) +Init +ResM 4 100.0 85.6 0.94 1.06 92.0 42.8 2.98 3.18 75.5 28.6 3.99 4.15\n8 100.0 85.6 1.52 1.66 89.9 41.6 2.39 2.63 73.6 27.4 3.88 4.06\nTable 6: Memory utilization of PKM-augmented models after ﬁne-tuning. We measure memory utilization metrics\n(MU, ˜MU, KLu, and KLw) at 4th and 8th layer after ﬁne-tuning using MNLI-m (Williams et al., 2017), SST-2\n(Socher et al., 2013), and CoLA (Warstadt et al., 2019) datasets as an example. We use the same ﬁne-tuned models\nthat appeared in Table 2.\nPKM\nResM\nInit+\nPKM\nInit+\nResM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nKL\nPKM\nResM\nInit+\nPKM\nInit+\nResM\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nIOU\nPre-train Fine-tune 4L 8L\nFigure 4: Difference in memory usage between posi-\ntive examples and negative examples in SST-2 (Socher\net al., 2013). KL divergence (left) and IOU (right) be-\ntween two distributions (positive vs. negative) are vi-\nsualized. We measure those values from weights pre-\ntrained without ﬁne-tuning and after ﬁne-tuning.\ning. The initialization and the residual memory\nimprove memory usage, meaning better exploita-\ntion of model capacity for downstream tasks. Es-\npecially in a large dataset like MNLI (Williams\net al., 2017), the memory usage of the ﬁne-tuned\nmodel reaches to almost 100% similar to pretrained\nmodels due to the sufﬁcient training steps to update\nmemory parameters. On the other hand, interest-\ningly, the initialization and the residual memory\ndo not always reduce KL divergence. We presume\nthis because ﬁne-tuning of classiﬁcation tasks en-\ncourages input examples of the same class to be\nclustered into similar representations, so it requires\nto access similar patterns of memory slots while\nutilizing many of them.\nTo validate the assumption mentioned above,\nwe check the difference in memory usage be-\ntween positive examples and negative examples\nusing SST-2 (Socher et al., 2013) dataset, which\nModel\nMLM QA GLUE\nPG-19 SQuAD 1.1 MNLI-m SST-2\n(ppl) (EM/F1) (Acc) (Acc)\nDistilBERT⋆ 20.61 77.4/85.7 82.0 91.6\n+Init +ResM 5.75 80.4/88.3 84.1 93.3\nBERTBASE\n⋆ 11.82 81.1/88.5 83.9 92.9\nTable 7: Experimental results on DistilBERT with and\nwithout our method. We add results of BERT BASE for\ncomparison. ⋆ means our reproduced results using\nmodel weights from Sanh et al. (2019) and Devlin et al.\n(2018). The initial model weights of DistilBERT is\nfrom the part of BERTBASE.\nis the binary classiﬁcation tasks to predict the sen-\ntiment of a movie review. To measure the differ-\nence, we calculate (1) KL divergence between two\ndistributions (positive/negative) and (2) intersec-\ntion over union (IOU), which is a widely used\nmetric in object detection (Ren et al., 2015) on\nthe top-1 memory usage. We calculate IOU as∑\ni min(t+\ni ,t−\ni )/∑\ni max(t+\ni ,t−\ni ), where t+\ni and\nt−\ni is a top-1 usage at memory positionifor positive\nexamples and negative examples, respectively. As\nillustrated in Figure 4, our best PKM-augmented\nmodel shows much higher KL and lower IOU in\nevery layer than the plain PKM-augmented model,\nimplying better discriminative ability.\nOther Pretrained ModelsWe release the code\nand pretrained weights to encourage researchers\nand practitioners to easily utilize and reproduce our\nwork, allowing the application to different model\nsizes and other backbone architectures. In particu-\nlar, we employ our methods to DistilBERT model\n(Sanh et al., 2019), which is a 6-layer transformer\nmodel trained by knowledge distillation (Hinton\n4068\net al., 2015) from BERTBASE. Similarly, it obtains\naccuracy comparable to BERTBASE as shown in Ta-\nble 7.5 Moreover, we believe our approaches could\nalso be helpful to any other task.\nPKM vs. ResMOne might argue that the gap be-\ntween the PKM model and the ResM model might\nbe attributed to the difference in model size. We\nclaim that the impact of the architectural difference\nbetween PKM and ResM is more than from more\nparameters. ResM achieves better memory utiliza-\ntion, resulting in a better ﬁnal performance. 0.3\nhigher average GLUE score with only 9M more\nparameters (smaller than 2% of the entire model) is\nsigniﬁcant considering that BERT-Large achieves\na 1.9 higher average GLUE score with 230M more\nparameters than BERT-Base (0.3\n9 ≫1.9\n230 ).\n8 Conclusion and Future Work\nThis work starts from unexpected results that di-\nrectly applying PKM to PLMs does not work well\nin downstream tasks, contrary to (Lample et al.,\n2019). In this paper, we successfully augment\nPKM to PLMs with two ingredients, weight ini-\ntialization and residual connection, based on the\nobservation of memory utilization and catastrophic\ndrift during the training. Consequently, we encour-\nage to utilize memory architecture such as PKM\nfor PLMs in practical use.\nAlthough our approach mitigates the catas-\ntrophic drift problem somehow, we leave further\nstudy on it during both pretraining and ﬁnetuning\nas future work. One possible solution is to regular-\nize a PKM memory by a structured dropout on the\nmemory keys like DropHead (Zhou et al., 2020).\nIt would also help to prune unnecessary memory\nslots on-demand during the inference time.\nAcknowledgments\nThe authors would like to thank Clova AI members\nand the anonymous reviewers for their constructive\nfeedback. Primarily, LaRva (Language Representa-\ntion by Clova) team members, including Sungdong\nKim, Dongjun Lee, and Minjeong Kim, provided\ntheir full support to this work happens. Moreover,\nGuillaume Lample shared his experience and tips\n5Like Table 4, we borrow DistilBERT and BERTBASE of\npublic weights, so their training setting does not match. We\nuse a batch size of 2048 for pretraining ResM-augmented\nDistilBERT initialized from the weights of DistilBERT (c.f.\nbatch size of DistilBERT and BERTBASE in their training is\n4096 and 256, respectively).\nfor our preliminary experiments regarding prod-\nuct key memory. Lastly, GK greatly appreciates\nKyunghyun Cho, Minjoon Seo, Jinhyuk Lee, and\nJung-Woo Ha for proofreading this manuscript thor-\noughly. We use Naver Smart Machine Learning\n(Sung et al., 2017; Kim et al., 2018) platform for\nthe experiments.\nReferences\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. 2016. Layer normalization. arXiv preprint\narXiv:1607.06450.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nSarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal\nVincent, Gerald Tesauro, and Yoshua Bengio. 2016.\nHierarchical memory networks. arXiv preprint\narXiv:1605.07427.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nThibault F´evry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as experts: Sparse memory access with entity\nsupervision. arXiv preprint arXiv:2004.07202.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. arXiv\npreprint arXiv:2002.08909.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015.\nDistilling the knowledge in a neural network. arXiv\npreprint arXiv:1503.02531.\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough memorization: Nearest neighbor language\nmodels. arXiv preprint arXiv:1911.00172.\nHanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong\nKim, Heungseok Park, Soeun Park, Hyunwoo Jo,\nKyungHyun Kim, Youngil Yang, Youngkwan Kim,\net al. 2018. Nsml: Meet the mlaas platform\nwith a real-world case study. arXiv preprint\narXiv:1810.09957.\n4069\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nGuillaume Lample, Alexandre Sablayrolles,\nMarc’Aurelio Ranzato, Ludovic Denoyer, and\nHerv´e J ´egou. 2019. Large memory layers with\nproduct keys. In Advances in Neural Information\nProcessing Systems, pages 8546–8557.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nJack Rae, Jonathan J Hunt, Ivo Danihelka, Timo-\nthy Harley, Andrew W Senior, Gregory Wayne,\nAlex Graves, and Timothy Lillicrap. 2016. Scal-\ning memory-augmented neural networks with sparse\nreads and writes. In Advances in Neural Information\nProcessing Systems, pages 3621–3629.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar,\nand Timothy P Lillicrap. 2019. Compressive trans-\nformers for long-range sequence modelling. arXiv\npreprint arXiv:1911.05507.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian\nSun. 2015. Faster r-cnn: Towards real-time ob-\nject detection with region proposal networks. In\nAdvances in neural information processing systems,\npages 91–99.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion\nparameter language models using gpu model paral-\nlelism. arXiv preprint arXiv:1909.08053.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642.\nSainbayar Sukhbaatar, Edouard Grave, Guillaume\nLample, Herve Jegou, and Armand Joulin. 2019.\nAugmenting self-attention with persistent memory.\narXiv preprint arXiv:1907.01470.\nSainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al.\n2015. End-to-end memory networks. In Advances\nin neural information processing systems, pages\n2440–2448.\nNako Sung, Minkyu Kim, Hyunwoo Jo, Youngil Yang,\nJingwoong Kim, Leonard Lausen, Youngkwan Kim,\nGayoung Lee, Donghyun Kwak, Jung-Woo Ha, et al.\n2017. Nsml: A machine learning platform that en-\nables you to focus on your models. arXiv preprint\narXiv:1712.05902.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nPat Verga, Haitian Sun, Livio Baldini Soares, and\nWilliam W Cohen. 2020. Facts as experts: Adapt-\nable and interpretable neural memory over symbolic\nknowledge. arXiv preprint arXiv:2007.00849.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nJason Weston, Sumit Chopra, and Antoine Bor-\ndes. 2014. Memory networks. arXiv preprint\narXiv:1410.3916.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Fun-\ntowicz, et al. 2019. Transformers: State-of-the-\nart natural language processing. arXiv preprint\narXiv:1910.03771.\nWangchunshu Zhou, Tao Ge, Ke Xu, Furu Wei, and\nMing Zhou. 2020. Scheduled drophead: A regu-\nlarization method for transformer models. arXiv\npreprint arXiv:2004.13342.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In Proceedings of the IEEE inter-\nnational conference on computer vision, pages 19–\n27."
}