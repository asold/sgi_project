{
    "title": "A Sketch-Transformer Network for Face Photo-Sketch Synthesis",
    "url": "https://openalex.org/W3187308034",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2148228300",
            "name": "Mingrui Zhu",
            "affiliations": [
                "Xidian University"
            ]
        },
        {
            "id": "https://openalex.org/A2482906519",
            "name": "Changcheng Liang",
            "affiliations": [
                "Xidian University"
            ]
        },
        {
            "id": "https://openalex.org/A2096424371",
            "name": "Nannan Wang",
            "affiliations": [
                "Xidian University"
            ]
        },
        {
            "id": "https://openalex.org/A2062469911",
            "name": "Xiaoyu Wang",
            "affiliations": [
                "Chinese University of Hong Kong, Shenzhen"
            ]
        },
        {
            "id": "https://openalex.org/A2107648850",
            "name": "Zhifeng Li",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2122193271",
            "name": "XinBo GAO",
            "affiliations": [
                "Chongqing University of Posts and Telecommunications"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1591385104",
        "https://openalex.org/W2331128040",
        "https://openalex.org/W3009525597",
        "https://openalex.org/W2905515171",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2033419168",
        "https://openalex.org/W4320013936",
        "https://openalex.org/W1980093854",
        "https://openalex.org/W1527240141",
        "https://openalex.org/W2962974533",
        "https://openalex.org/W2886934227",
        "https://openalex.org/W2593414223",
        "https://openalex.org/W2970800101",
        "https://openalex.org/W2029986162",
        "https://openalex.org/W2141345255",
        "https://openalex.org/W2962785568",
        "https://openalex.org/W2300234500",
        "https://openalex.org/W2963073614",
        "https://openalex.org/W2153288431",
        "https://openalex.org/W3117450517",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4288284643",
        "https://openalex.org/W2741049043",
        "https://openalex.org/W2141983208",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W4301206121",
        "https://openalex.org/W1985436611",
        "https://openalex.org/W4245134515",
        "https://openalex.org/W2964323748",
        "https://openalex.org/W2963981733",
        "https://openalex.org/W2034136097",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2962793481"
    ],
    "abstract": "We present a face photo-sketch synthesis model, which converts a face photo into an artistic face sketch or recover a photo-realistic facial image from a sketch portrait. Recent progress has been made by convolutional neural networks (CNNs) and generative adversarial networks (GANs), so that promising results can be obtained through real-time end-to-end architectures. However, convolutional architectures tend to focus on local information and neglect long-range spatial dependency, which limits the ability of existing approaches in keeping global structural information. In this paper, we propose a Sketch-Transformer network for face photo-sketch synthesis, which consists of three closely-related modules, including a multi-scale feature and position encoder for patch-level feature and position embedding, a self-attention module for capturing long-range spatial dependency, and a multi-scale spatially-adaptive de-normalization decoder for image reconstruction. Such a design enables the model to generate reasonable detail texture while maintaining global structural information. Extensive experiments show that the proposed method achieves significant improvements over state-of-the-art approaches on both quantitative and qualitative evaluations.",
    "full_text": "A Sketch-Transformer Network for Face Photo-Sketch Synthesis\nMingrui Zhu1 , Changcheng Liang1 , Nannan Wang1\u0003 , Xiaoyu Wang2 , Zhifeng Li3\nand Xinbo Gao4\n1State Key Laboratory of Integrated Services Networks, Xidian University, Xi‚Äôan, China\n2The Chinese University of Hong Kong (Shenzhen), Shenzhen, China\n3Tencent, Shenzhen, China\n4Chongqing Key Laboratory of Image Cognition, Chongqing University of Posts and\nTelecommunications, Chongqing, China\nmrzhu@xidian.edu.cn, ccliang@stu.xidian.edu.cn, nnwang@xidian.edu.cn, fanghuaxue@gmail.com,\nmichaelzÔ¨Çi@tencent.com, gaoxb@cqupt.edu.cn\nAbstract\nWe present a face photo-sketch synthesis model,\nwhich converts a face photo into an artistic face\nsketch or recover a photo-realistic facial image\nfrom a sketch portrait. Recent progress has been\nmade by convolutional neural networks (CNNs)\nand generative adversarial networks (GANs), so\nthat promising results can be obtained through real-\ntime end-to-end architectures. However, convolu-\ntional architectures tend to focus on local infor-\nmation and neglect long-range spatial dependency,\nwhich limits the ability of existing approaches in\nkeeping global structural information. In this pa-\nper, we propose a Sketch-Transformer network for\nface photo-sketch synthesis, which consists of three\nclosely-related modules, including a multi-scale\nfeature and position encoder for patch-level feature\nand position embedding, a self-attention module\nfor capturing long-range spatial dependency, and a\nmulti-scale spatially-adaptive de-normalization de-\ncoder for image reconstruction. Such a design en-\nables the model to generate reasonable detail tex-\nture while maintaining global structural informa-\ntion. Extensive experiments show that the pro-\nposed method achieves signiÔ¨Åcant improvements\nover state-of-the-art approaches on both quantita-\ntive and qualitative evaluations.\n1 Introduction\nGenerating a face sketch (photo) from a face photo (sketch),\noften referred as face photo-sketch synthesis, is an important\ntask in computer vision. It has many applications in digi-\ntal entertainment, animation production and law enforcement\n[Wang et al., 2014; Li et al., 2016]. The core challenge of\nface photo-sketch synthesis lies in synthesizing visually re-\nalistic and semantically plausible images and surpassing the\nconsiderable discrepancies (shape, texture and color) barrier.\nEarly studies [Liu et al., 2005; Liang Chang et al., 2010;\nZhu et al., 2017b ] attempt to solve the problem in an\n\u0003Corresponding Author: Nannan Wang\nSOTA Ours GT\nFigure 1: A comparision of face photo sketch synthesis re-\nsults between the proposed Sketch-Transformer and a state-of-the-\nart (SOTA) approach. Sketch-Transformer (ours) can capture long-\nrange spatial dependency while generate reasonable detail texture.\nexemplar-based manner, i.e. matching and combining sam-\nple images (image patches) in a reference set of photo-sketch\npairs to synthesize the target image. These approaches work\nwell under constrained conditions such as less illumination\nvariations, pose changes, and deformations, but will fail when\ncome across more complicate conditions. Moreover, two\nmain Ô¨Çaws often limit their performance: 1) blurry or over\nsmooth, i.e not realistic; 2) time-consuming. Rapid progress\nin deep convolutional neural networks (CNN), especially in\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1352\ngenerative adversarial networks (GAN) [Goodfellow et al.,\n2014], has inspired recent studies [Wang et al., 2018; Yu et\nal., 2020; Chen et al., 2018] to formulate face photo sketch\nsynthesis as a image-to-image translation [Isola et al., 2017;\nZhu et al., 2017a] problem. With the assistance of the adver-\nsarial loss, these approaches have capacity to generate images\nwith realistic textures.\nAlthough promising results have been obtained, the intrin-\nsic shortage of convolutional architectures that lacks of the\nability of capturing long-range spatial dependency has lim-\nited the performance of existing approaches, which may re-\nsults in the loss of global structure information and thus gen-\nerating images with compromised visual quality. As shown\nin Figure 1, the results of a state-of-the-art (SOTA) method\n[Yu et al., 2020 ] have undesirable artifacts and distorted\nstructures. Recently, transformer models [Vaswani et al.,\n2017] which mainly based on self-attention mechanism have\ndemonstrated exemplary performance on natural language\nprocessing (NLP) tasks and intrigued the vision community\nto investigate their application to computer vision problems\n[Dosovitskiy et al., 2020]. Inspired by the power of trans-\nformer in NLP and many computer vision tasks, we inves-\ntigate its application in face photo-sketch synthesis task in\nthis work. However, there are three factors that limit the ap-\nplication of existing transformer models in this task: 1) The\ntraining samples are limited so that the model should not be\ntoo large; 2) The resolution of the image is relatively large\nso that the self-attention module consumes lots of computing\nresources; 3) The self-attention module is unable to capture\npositional information of the tokens in an image.\nTo address these problems, we propose a Sketch-\nTransformer which can properly introduce the self-attention\nmechanism into the face photo-sketch synthesis task. Specif-\nically, three closely-related modules are proposed. First, we\npropose a multi-scale feature and position encoder (MFP-\nEncoder) which integrates convolutional architectures and\na face parsing model to extract multi-scale feature embed-\ndings and positional encodings in each local area. Sec-\nond, we stack several residual self-attention layers in the\nbottleneck to capture the long-range spatial dependency be-\ntween the tokens (local embeddings). Finally, we pro-\npose a multi-scale spatially-adaptive de-normalization de-\ncoder (MSPADE-Decoder) which takes as input the output\nof the self-attention module, multi-scale feature embeddings\nand positional encodings generated by the multi-scale fea-\nture and position encoder to reconstruct the target image.\nThe overall design enables our Sketch-Transformer to cap-\nture long-range spatial dependency while generate reasonable\ndetail texture and therefore achieve a better visual result com-\npared with state-of-the-art approaches (as shown in Figure 1).\nThe contributions of this work are summarized as follows:\n‚Ä¢ We propose to learn the key elements of the transformer\narchitecture and adapt them to face photo-sketch synthe-\nsis task.\n‚Ä¢ We propose a Sketch-Transformer with three closely-\nrelated modules to properly introduce the self-attention\nmechanism. The proposed model can capture long-\nrange spatial dependency while generate reasonable de-\ntail texture.\n‚Ä¢ Quantitative and qualitative experiments demonstrate\nthat the proposed model achieves superior performance\ncompared with other state-of-the-art methods on public\nbenchmarks and face images in real scenarios.\n2 Related Work\nIn this section, we review previous studies of face photo-\nsketch synthesis and transformer which are the most relevant\nto our work.\n2.1 Face Photo-Sketch Synthesis\nExisting works for face photo-sketch synthesis can be mainly\ndivided into two categories. Exemplar-based methods recon-\nstruct target image by mining correspondences between input\nimage (image patch) and images (image patches) in a refer-\nence set of photo-sketch pairs. Deep learning-based meth-\nods attempt to predict the target image pixels from the source\nimage pixels through an end-to-end convolutional neural net-\nworks.\nExemplar-based methods can be further grouped into three\ntypes: subspace learning-based approaches [Liu et al., 2005],\nsparse representation-based approaches [Liang Chang et al.,\n2010], and Bayesian inference-based approaches [Zhu et al.,\n2017b]. A detailed overview of existing exemplar-based\nmethods can be found in [Wang et al., 2014].\nRecently, CNN-based and GAN-based approaches have\nemerged as a promising paradigm for face photo-sketch syn-\nthesis. Initial effort [Zhang et al., 2015] trains an end-to-end\nfully convolutional neural networks (FCN) for directly mod-\neling the nonlinear mapping between face photos and face\nsketches. Limited by shallow layers and pixel-level loss,\nhowever, it fails to capture texture details and fails to pre-\nserve reasonable structures. Isola et al. [2017] use condi-\ntional GAN (cGAN) as a uniÔ¨Åed solution (pix2pix) for several\nimage-to-image translation tasks such as edges to photos, la-\nbels to street scenes, day to night, etc. Zhu et al. [2017a] pro-\npose a CycleGAN model for unpaired image-to-image trans-\nlation by introducing a cycle consistency loss. These two\nmodels can be directly applied to face photo-sketch synthe-\nsis task. Several works follow ideas from image-to-image\ntranslation and focus on improving face photo-sketch syn-\nthesis performance by adding prior information. Wang et al.\n[2018] propose a multi-scale discriminator to provide adver-\nsarial supervision on different image resolution. SCAGAN\n[Yu et al., 2020] introduces facial composition information\nas additional input to help the generation of sketch portraits\nand proposes a compositional loss based on facial composi-\ntion information. To tackle the problem of insufÔ¨Åcient paired\ntraining data, Chen et al. [2018] propose a semi-supervised\nlearning method to augment paired training samples by syn-\nthesizing pseudo sketch features of additional training pho-\ntos and learn the mapping function between them. Although\ngreat progress has been made by above approaches, undesir-\nable artifacts and distorted structures, however, are still exists,\nespecially in the results of real scenarios.\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1353\nInput Photo\nResidual\nSelf-attention Module\nMFP-Encoder\nMSPADE-Decoder\nOutput Sketch\nFigure 2: The illustration of the Sketch-Transformer architecture.\n2.2 Transformer and Self-attention\nTransformer is Ô¨Årstly applied on natural language process-\ning (NLP) tasks, which mainly leverages self-attention mech-\nanism to capture long-range dependencies in the input do-\nmain. The seminal work of Vaswani et al. [2017] pro-\nposes to use solely attention mechanisms for machine trans-\nlation. Since then, transformer architecture has opened up\na new route. Lots of popular methods have been proposed\nand have achieved the state-of-the-art performance in differ-\nent NLP tasks. The breakthroughs achieved by transformer\nin NLP domain have attracted lots of interest in the computer\nvision community. Many studies have successfully adapted\ntransformer models to varies computer vision tasks including\nimage recognition, object detection, image super-resolution\nand several other tasks. A comprehensive overview of the vi-\nsion transformer literature has been introduced by Han et al.\n[2020].\n3 Method\nGiven paired training face photo-sketch samples f(xi;yi) 2\n(X;Y )gN\ni=1, our goal is to learn a mapping function G that\nmaps images from photo domain X to sketch domain Y or\nlearn a mapping function F that maps images from sketch\ndomain Y to photo domain X. The pipeline of the pro-\nposed Sketch-Transformer is shown in Figure 2. It con-\nsists of three closely-related modules, including a multi-\nscale feature and position encoder (MFP-Encoder) for patch-\nlevel feature and position embedding, a residual self-attention\nmodule for capturing long-range spatial dependency, and\na multi-scale spatially-adaptive de-normalization decoder\n(MSPADE-Decoder) for image reconstruction.\n3.1 MFP-Encoder\nThe MFP-Encoder integrates convolutional architectures and\na face parsing model to extract multi-scale feature embed-\ndings and positional encodings in each local area. It consists\nof two paths: a feature embedding path and a position em-\nbedding path, as shown in Figure 3.\nThe feature embedding path utilizes a series of convolution\nlayers (a stride-1 convolution layer and four stride-2 convolu-\ntion layers) to gradually extract multi-scale features. There-\nfore, the feature vector of each position in the last activation\n(FP5) represents the high-level features of a16\u000216 patch in\nthe corresponding local area of the input image. The position\nembedding path utilizes a face parsing model[Yuet al., 2018]\nto extract semantic facial labels and scale them to different\nInput\nPhoto\nFace Parsing \nModel\nùëÉùëÉ1\n‚äï ‚äï ‚äï\nP\nC ‚äï\nùëÉùëÉ2\nùëÉùëÉ3 ùëÉùëÉ4\nùêπùêπ1\nùêπùêπ2\nùêπùêπ3\nùêπùêπ4\nùêπùêπ5\nùêπùêπùëÉùëÉ1 ùêπùêπùëÉùëÉ2 ùêπùêπùëÉùëÉ3 ùêπùêπùëÉùëÉ4 ùêπùêπùëÉùëÉ5\nP C ConvBNReLU\n(stride=1)\nConvBNReLU (stride=2)\nIterpolate (scale factor = 1/2)\n‚äï Concatenate\nFigure 3: The illustration of the MFP-Encoder.\nspatial resolution. Denote semantic facial labels of each layer\nas Ml, Ml 2<cl\u0002hl\u0002wl , where cl;hl;wl denote component\nnumber, height and width of the semantic labels of the lth\nfeature layer. Each value (0 or 1) in Ml denotes whether the\nposition belongs to the c-th component. Such semantic facial\nlabels actually contain sufÔ¨Åcient positional information and\ncan indicate which semantic component the feature embed-\ndings of each position belongs to. We concatenate the feature\nembeddings and position embeddings at different level to ob-\ntain the multi-scale feature and position embeddings. Then,\nthe Ô¨Årst four feature and position embeddings are passed to\nthe MSPADE-Decoder as spatial information to help supple-\nment spatial and texture information and the last one is passed\nto a residual self-attention model to learn long-range depen-\ndencies between the embeddings (tokens) from all positions.\n3.2 Residual Self-attention Module\nSelf-attention is the core component of the transformer ar-\nchitecture, which can capture long-range dependency be-\ntween tokens. From the MFP-Encoder, we obtain the patch-\nlevel feature and position embeddings of all positions. How-\never, the relationships between these embeddings are neglect.\nTherefore, we introduce a residual self-attention module to\ncapture their dependencies. The module consists of nine ba-\nsic residual self-attention layers. The illustration of each layer\nis shown in Figure 4.\nThe intuition behind this module is to update each vector\nat each position of the embeddings by aggregating global in-\nformation from all other positions. Through this module, we\ncan get the revised embeddings ^FP5 which have learned the\nlong-range dependencies.\n3.3 MSPADE-Decoder\nWe utilize the spatially-adaptive de-normalization (SPADE)\nblock [Park et al., 2019] on multi-scale feature and position\nembeddings to gradually reconstruct the target image. More\nspeciÔ¨Åcally, we utilize positional normalization (PN) [Li et\nal., 2019] instead of batch normalization (BN) to better pre-\nserving the structure information synthesized in prior layers.\nThe illustration of the MSPADE-Decoder is shown in Figure\n4.\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1354\nResidual Self-attention Module\nùë•ùë•\n1x1 conv 1x1 conv 1x1 conv\nQ K V\n‚®Ç\ntranspose\nAttention Map\nsoftmax\n‚®Ç\n‚®Å\nÔøΩ ùë•ùë•\ndot product\ndot product\nadd\nFigure 4: The illustration of the residual self-attention mudule.\nSPADE\nSPADE\nSPADE\nSPADE\nOutput Sketch\nùêπùêπùëÉùëÉ5\nùêπùêπùëÉùëÉ2ùêπùêπùëÉùëÉ3ùêπùêπùëÉùëÉ4 ùêπùêπùëÉùëÉ1\nFigure 5: The illustration of the SPADE-Decoder.\n3.4 Loss Function\nThe full loss of our model consists of two loss functions: ad-\nversarial loss and perceptual loss. For the sake of brevity, we\nonly describe the losses for photo to sketch synthesis task.\nThe losses for sketch to photo synthesis has the same form.\nFor convenience of expression, we denote the SketchTrans-\nformer as G. In order to provide the adversarial loss, we uti-\nlize a 70 √ó 70 PatchGAN discriminator [Isola et al., 2017],\nwhich is denoted as D.\nAdversarial Loss\nIn this work, instead of using the vanilla GAN [Goodfellow\net al., 2014], we use the Least Squares GAN [Mao et al.,\n2017] for stable training. For the mapping function Gand its\ndiscriminator D, we express the objective as:\nLadversarial = Ey[(DY (y))2] +Ex[(1 \u0000DY (G(x)))2] (1)\nPerceptual Loss\nTo ensure that the generated image and its ground truth are\nsimilar in semantic feature level, we introduce the perceptual\nDatabase Training Pairs Testing Pairs\nCUFS\nCUHK Student 88 100\nAR 80 43\nXM2VTS 100 195\nCUFSF 250 944\nTable 1: Partition settings of the databases\nloss [Johnson et al., 2016]:\nLperceptual = Ex[ 1\nCjHjWj\nk\u001ej(G(x)) \u0000\u001ej(y) k1] (2)\nwhere \u001ej indicates feature maps of the jth layer of a pre-\ntrained VGG-19 model[Simonyan and Zisserman, 2014], Cj,\nHj and Wj indicate channel numbers, height and width of the\nfeature maps, respectively.\nFull Loss\nBy combining above losses, we can achieve our full loss:\nLfull = \u00151Ladversarial + \u00152Lperceptual (3)\nIn this work, we set \u00151 = 1, \u00152 = 5 to keep corresponding\nlosses in the same order of magnitude.\n4 Experiments\nIn this section, we Ô¨Årst discuss the experimental settings. We\nwill then conduct ablation study to quantify the contribution\nof different conÔ¨Ågurations to overall effectiveness. Finally,\nwe will compare our results with state-of-the-art methods\nboth qualitatively and quantitatively.\n4.1 Implement Details\nAll models are trained on a NVIDIA Tesla V100 GPU using\nAdam optimizer with \f1 = 0:5 and \f2 = 0:99. We train all\nmodels with a Ô¨Åxed learning rate of 0.0002 until 300,000 iter-\nations. The batch size is set to 1 for all experiments. Weights\nwere initialized from a Gaussian distribution with mean 0 and\nstandard deviation 0.02. We scaled the size of the input im-\nages to 256 \u0002256 and normalized the pixel value to the inter-\nval [\u00001;1] before putting them into the model. During train-\ning, we updated Gand Dalternatively at every iteration.\n4.2 Database\nThe experiments are conducted on two public databases: the\nCUFS database [Tang and Wang, 2009 ] and the CUFSF\ndataset [Zhang et al., 2011b]. The CUFS database consists\nof 188 identities from the Chinese University of Hong Kong\n(CUHK) student database [Tang and Wang, 2003], 123 iden-\ntities from the AR database [Martinez and Benavente, 1998],\nand 295 identities from the XM2VTS database [Messer\net al., 1999 ]. Each identity has a photo-sketch pair un-\nder normal light condition, and with a neutral expression.\nThe CUFSF database has 1,194 identities from the FERET\ndatabase [Phillips et al., 2000]. For each identity, there is\na photo with illumination variation and a sketch with exag-\ngerated structure. Therefore, face photo-sketch synthesis on\nthe CUFSF database is more challenging than on the CUFS\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1355\ndataset. All images are processed by aligning the center of\ntwo eyes to the Ô¨Åxed position and cropping to the size of\n200 \u0002250. The way we divide the training set and the test set\nis the same as [Zhu et al., 2017b]. For the CUFS database,\n88 face photo-sketch pairs in CUHK database, 80 face photo-\nsketch pairs in AR database and 100 face photo-sketch pairs\nin XM2VTS database are selected for training and the rest are\nused for testing. For the CUFSF database, 250 face photo-\nsketch pairs are selected for training and the rest are used for\ntesting. Table 1 shows the partition settings of the databases.\n4.3 Baselines\nFor fair comparison, we run face photo-sketch synthesis\non our method and all baselines for input images of size\n200 \u0002250 under the same partition setting. We compare our\nmethod with seven state-of-the-art methods: DGFL [Zhu et\nal., 2017b], FCN [Zhang et al., 2015], pix2pix [Isola et al.,\n2017], CycleGAN [Zhu et al., 2017a], PS2MAN [Wang et\nal., 2018], Wild [Chen et al., 2018] and SCAGAN [Yu et al.,\n2020]. Among these baselines, DGFL is traditional exemplar-\nbased method which achieves the best performance while the\nothers are deep learning-based methods. All results are ob-\ntained from the source codes provided by the authors except\nthe results of FCN. We implement FCN by ourselves and get\nthe results which are consistent with the original work. Be-\ncause FCN, DGFL and Wild methods are designed for face\nphoto !sketch synthesis task, we only compare with their\nsynthetic face sketches. Other methods have both synthetic\nface photos and face sketches that used for comparison.\n4.4 Evaluation Metrics\nIn this paper, we use three types of evaluation metrics to\nevaluate the objective quality of the synthetic images: the\nlearned perceptual image patch similarity (LPIPS) [Zhang et\nal., 2018 ], the Fr ¬¥echet Inception Distance (FID) [Heusel et\nal., 2017] and the feature similarity index (FSIM) [Zhang et\nal., 2011a]. The LPIPS takes two images (image patches) as\nthe input, calculates the L2 distance between their normalized\ndeep feature embeddings, and predicts the perceptual judg-\nment score through the linear layer. A lower score indicates\nbetter quality of synthetic images. FID is designed to capture\nthe Fr¬¥echet difference of two Gaussians (synthetic and real-\nworld images). We compute the FID score between the syn-\nthetic images and real ones. Lower FID score indicates better\nquality of synthetic images. FSIM is a commonly used metric\nfor full-reference image quality assessment, which captures\nthe similarity between low-level features of images. It shows\nhigher consistency with human visual perception. We calcu-\nlated the average FSIM score between synthetic images and\nreal ones. A higher FSIM score indicates better quality of\nsynthetic images.\n4.5 Ablation Study\nWe compute the LPIPS (alex) score between the synthetic im-\nages and real ones on CUHK database under different con-\nÔ¨Ågurations to quantify the contribution of different conÔ¨Ågura-\ntions to overall effectiveness. The ablation study is conducted\non four conÔ¨Ågurations: (a) U-net [Ronneberger et al., 2015]\nConÔ¨Ågurations Photo-LPIPS(ale\nx) # Sketch-LPIPS(alex) #\n(a) 0.1686 0.1732\n(b) 0.1529 0.1700\n(c) 0.1537 0.1657\n(d) 0.1511 0.1662\nTable 2: Ablation study: LPIPS (alex) scores for different vari-\nants of conÔ¨Ågurations, evaluated on CUHK photo ! sketch and\nsketch ! photo.\nTest Sketch pix2pix CycleGAN PS2MAN SCAGAN Ground Truth\nOurs\nFigure 6: Examples of synthetic face photos on the CUFS dataset\nand the CUFSF dataset. From top to bottom, the examples are\nselected from the CUHK student database, the AR database, the\nXM2VTS database and the CUFSF database, sequentially.\narchitecture; (b) Using MSPADE-Decoder to replace the ori-\ngin decoder in U-net; (c) Adding residual self-attention mod-\nule on the basis of (b); (d) Adding position embeddings on\nthe basis of (c) (i.e. Full Sketch-Transformer). The evalua-\ntion scores are shown in Table 2, from which we can conclude\nthat all the modiÔ¨Åcations are critical to the Ô¨Ånal effectiveness\nof the proposed method.\n4.6 Comparison with Baselines\nFigure 6 presents some synthetic face photos from different\nmethods on the CUFS dataset and the CUFSF dataset. The\nresults of pix2pix and CycleGAN have sharp edges but pos-\nsess obvious artifacts and noise. PS2MAN produces less ar-\ntifacts but its results are blurry. Face photos synthesized by\nthe SCAGAN have reasonable texture and less artifacts, but\nstill possess some structure distortions. As shown in the Ô¨Åg-\nure, synthetic photos of the proposed method retain consider-\nable structural information and achieve the most reasonable\ntexture distribution, and therefore has the best visual perfor-\nmance.\nSome synthetic face sketches from different methods on\nthe CUFS dataset and the CUFSF dataset are shown in Fig-\nure 7. The results of DGFL and FCN are too blurry. GAN-\nbased methods (pix2pix, CycleGAN, PS2MAN and SCA-\nGAN) can generate sketch-like textures. However, some un-\ndesirable textures are produced in eye and hair areas. Wild\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1356\nTest Photo pix2pix CycleGAN PS2MAN SCAGAN Ours Ground TruthFCN WildDGFL\nFigure 7: Examples of synthetic face sketches on the CUFS dataset\nand the CUFSF dataset. From top to bottom, the examples are\nselected from the CUHK student database, the AR database, the\nXM2VTS database and the CUFSF database, sequentially.\nTest Photo pix2pix CycleGAN PS2MAN SCAGAN OursWildDGFL\nFigure 8: Examples of synthetic face sketches on face photos in the\nwild.\nhas stronger robustness to the environment noises but tends\nto generate over smooth results, and the texture distribution\nof its synthetic sketches is inconsistent with that of training\nsketches. The proposed Sketch-Transformer can generate the\nmost sketch-like textures while maintain the global structures.\nFigure 8 presents some synthetic face sketches from dif-\nferent methods on face photos with deformations and illumi-\nnation variations. Results of DGFL are able to preserve de-\nsirable structures but lose texture details. Results of pix2pix,\nPS2MAN tend to lose structural information and mistake the\nshaded area as the hair area. CycleGAN can preserve consid-\nerable structures but its synthetic sketches are more like face\nphotos. Wild has desirable visual performance but the texture\ndistribution of its synthetic sketches is inconsistent with that\nof training sketches. Our results can preserve enough struc-\ntural information while generate satisfactory textures.\nTable 3 presents the evaluation scores of the synthetic face\nphotos/sketches on the CUFS dataset and the CUFSF dataset.\nThe proposed model obtains the best score in most cases,\nwhich indicate that it achieves the best performance.\nSynthetic\nImage Metrics\nDGFL\nFCN\npix2pix\nCycleGAN\nPS2MAN\nWild\nSCAGAN\nSketch-T\nransformer\nLPIPS(alex) # - -\n0.1993 0.2096 0.2464 - 0.1727 0.1538\nLPIPS(squeeze) # - -\n0.1830 0.2094 0.2158 0.1643 0.1310\nCUFS\nPhoto LPIPS(vgg) # - -\n0.3525 0.3882 0.3254 - 0.3053 0.2738\nFSIM \" - -\n0.7726 0.7450 0.7819 - 0.7937 0.7851\nFID # - -\n73.56 80.44 65.04 - 80.53 27.88\nLPIPS(alex) # 0.3316 0.4517\n0.2263 0.2139 0.2961 0.2807 0.2408 0.1807\nLPIPS(squeeze) # 0.2635 0.3596\n0.1552 0.1529 0.2265 0.2210 0.1722 0.1233\nCUFS\nSketch LPIPS(vgg) # 0.3654 0.4350\n0.3734 0.3598 0.3707 0.3639 0.3627 0.3019\nFSIM \" 0.7079 0.6936\n0.7363 0.7219 0.7230 0.7114 0.7086 0.7350\nFID # 70.81 69.93\n44.91 23.76 48.95 59.26 38.61 20.92\nLPIPS(alex) # - -\n0.2463 0.2557 0.3145 - 0.1735 0.2199\nLPIPS(squeeze) # - -\n0.2005 0.2002 0.2853 - 0.1469 0.1714\nCUFSF\nPhoto LPIPS(vgg) # - -\n0.4019 0.3791 0.4237 - 0.3128 0.3474\nFSIM \" - -\n0.7777 0.7645 0.7812 - 0.8395 0.7861\nFID # - -\n39.82 14.46 78.03 - 18.84 15.22\nLPIPS(alex) # 0.3524 0.4793\n0.2408 0.2371 0.3288 0.3288 0.2188 0.1971\nLPIPS(squeeze) # 0.2794 0.3895\n0.1628 0.1589 0.2397 0.2473 0.1500 0.1349\nCUFSF\nSketch LPIPS(vgg) # 0.3972 0.5305\n0.3824 0.3744 0.4170 0.4053 0.3536 0.3400\nFSIM \" 0.6957 0.6624 0.7283 0.7088\n0.7233 0.6821 0.7270 0.7259\nFID # 57.33 124.40\n35.52 14.62 64.42 59.76 18.32 9.39\nTable 3: Quantitative results of the comparison with state-of-the-\nart methods on synthetic face photos/sketches of the CUFS database\nand CUFSF database.\n5 Conclusion\nIn this paper, we investigate the application potential of trans-\nformer architecture (especially the self-attention mechanism)\non face photo-sketch synthesis task. For this purpose, we pro-\npose a Sketch-Transformer network which consists of three\nclosely-related modules: a MFP-Encoder, a self-attention\nmodule, and a MSPADE-Decoder. We compare the proposed\nmodels with recent state-of-the-art methods on two public\ndatasets and face images in real scenarios. Both qualitative\nand quantitative results demonstrate that the proposed method\nachieves signiÔ¨Åcant improvements in both retaining structural\ninformation and generating appropriate textures. In the fu-\nture, we intend to further investigate the method of applying\nthe self-attention module to multi-scale feature embeddings.\nAcknowledgments\nThis work was supported in part by the National Key Re-\nsearch and Development Program of China under Grant\n2018AAA0103202; in part by the National Natural Science\nFoundation of China under Grants 62036007, 61922066,\n61876142, 61772402, and 62050175; in part bt the Nat-\nural Science Basic Research Plan in Shaanxi Province of\nChina under Grant 2021JQ-198; in part by the Fundamen-\ntal Research Funds for the Central Universities under Grant\nXJS210102.\nReferences\n[Chen et al., 2018] Chaofeng Chen, Wei Liu, Xiao Tan, and\nKwan-Yee K. Wong. Semi-supervised learning for face\nsketch synthesis in the wild. In ACCV, 2018.\n[Dosovitskiy et al., 2020] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, et al. An image\nis worth 16x16 words: Transformers for image recognition\nat scale. arXiv:2010.11929, 2020.\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1357\n[Goodfellow et al., 2014] Ian J. Goodfellow, Jean Pouget-\nAbadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. Gen-\nerative adversarial nets. In NeurIPS, pages 2672‚Äì2680,\n2014.\n[Han et al., 2020] Kai Han, Yunhe Wang, Hanting Chen,\nXinghao Chen, Jianyuan Guo, Zhenhua Liu, et al. A sur-\nvey on visual transformer. arXiv:2012.12556, 2020.\n[Heusel et al., 2017] Martin Heusel, Hubert Ramsauer,\nThomas Unterthiner, Bernhard Nessler, and Sepp Hochre-\niter. Gans trained by a two time-scale update rule converge\nto a local nash equilibrium. In Conference and Workshop\non NeurIPS, 2017.\n[Isola et al., 2017] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou,\nand Alexei A. Efros. Image-to-image translation with con-\nditional adversarial networks. InCVPR, pages 1125‚Äì1134,\n2017.\n[Johnson et al., 2016] Justin Johnson, Alexandre Alahi, and\nFei-Fei Li. Perceptual losses for real-time style transfer\nand super-resolution. In ECCV, 2016.\n[Li et al., 2016] Zhifeng Li, Dihong Gong, Qiang Li,\nDacheng Tao, and Xuelong Li. Mutual component analy-\nsis for heterogeneous face recognition.ACM TIST, 7(3):1‚Äì\n23, 2016.\n[Li et al., 2019] Boyi Li, Felix Wu, Kilian Q Weinberger,\nand Serge Belongie. Positional normalization. InNeurIPS,\npages 1622‚Äì1634, 2019.\n[Liang Chang et al., 2010] Mingquan Zhou Liang Chang,\nYanjun Han, and Xiaoming Deng. Face sketch synthe-\nsis via sparse representation. In ICPR, pages 2146‚Äì2149,\n2010.\n[Liu et al., 2005] Qingshan Liu, Xiaoou Tang, Hongliang\nJin, Hanqing Lu, and Songde Ma. A nonlinear approach\nfor face sketch synthesis and recognition. In CVPR, pages\n1005‚Äì1010, 2005.\n[Mao et al., 2017] Xudong Mao, Qing Li, Haoran Xie, Ray-\nmond Y .K. Lau, Zhen Wang, and Stephen Paul Smolley.\nLeast squares generative adversarial networks. In ICCV,\n2017.\n[Martinez and Benavente, 1998] A. M. Martinez and Robert\nBenavente. The ar face database. Technical report, CVC\nTechnical Report #24, 1998.\n[Messer et al., 1999] Kieron Messer, Jiri Matas, J. Kittler,\nJ. Luettin, and G. Maitre. Xm2vtsdb: The extended m2vts\ndatabase. In AVBPA, pages 72‚Äì77, 1999.\n[Park et al., 2019] Taesung Park, Ming-Yu Liu, Ting-Chun\nWang, and Jun-Yan Zhu. Semantic image synthesis with\nspatially-adaptive normalization. In CVPR, pages 2337‚Äì\n2346, 2019.\n[Phillips et al., 2000] P. Jonathon Phillips, Hyeonjoon\nMoon, Syed A. Rizvi, and Patrick J. Rauss. The feret\nevaluation methodology for face recognition algorithms.\nTPAMI, 22(10):1090‚Äì1104, 2000.\n[Ronneberger et al., 2015] Olaf Ronneberger, Philipp Fis-\ncher, and Thomas Brox. U-net: Convolutional networks\nfor biomedical image segmentation. In International\nConference on Medical image computing and computer-\nassisted intervention, pages 234‚Äì241. Springer, 2015.\n[Simonyan and Zisserman, 2014] Karen Simonyan and An-\ndrew Zisserman. Very deep convolutional networks for\nlarge-scale image recognition. arXiv:1409.1556, 2014.\n[Tang and Wang, 2003] Xiaoou Tang and Xiaogang Wang.\nFace sketch synthesis and recognition. In ICCV, pages\n687‚Äì694, 2003.\n[Tang and Wang, 2009] Xiaoou Tang and Xiaogang Wang.\nFace photo-sketch synthesis and recognition. TPAMI,\n31(11):1955‚Äì1967, November 2009.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n≈Åukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In NeurIPS, pages 5998‚Äì6008, 2017.\n[Wang et al., 2014] Nannan Wang, Dacheng Tao, Xinbo\nGao, Xuelong Li, and Jie Li. A comprehensive survey\nto face hallucination. IJCV, 106(1):9‚Äì30, January 2014.\n[Wang et al., 2018] Lidan Wang, Vishwanath A. Sindagi,\nand Vishal M. Patel. High-quality facial photo-sketch syn-\nthesis using multi-adversarial networks. In FG, pages 83‚Äì\n90, 2018.\n[Yu et al., 2018] Changqian Yu, Jingbo Wang, Chao Peng,\nChangxin Gao, Gang Yu, and Nong Song. Bisenet: Bi-\nlateral segmentation network for real-time semantic seg-\nmentation. In arXiv:1808.00897v1, 2018.\n[Yu et al., 2020] Jun Yu, Xingxin Xu, Fei Gao, Shengjie Shi,\nMeng Wang, Dacheng Tao, and Qingming Huang. Toward\nrealistic face photo-sketch synthesis via composition-\naided gans. TC, 2020.\n[Zhang et al., 2011a] Lin Zhang, Lei Zhang, Xuanqin Mou,\nand David Zhang. Fsim: A feature similarity index for\nimage quality assessment. TIP, 20(8):2378‚Äì2386, 2011.\n[Zhang et al., 2011b] Wei Zhang, Xiaogang Wang, and Xi-\naoou Tang. Coupled information-theoretic encoding for\nface photo-sketch recognition. In CVPR, pages 513‚Äì520,\n2011.\n[Zhang et al., 2015] Liliang Zhang, Liang Lin, Xian Wu,\nShengyong Ding, and Lei Zhang. End-to-end photo-sketch\ngeneration via fully convolutional representation learning.\nIn ICMR, pages 627‚Äì634, 2015.\n[Zhang et al., 2018] Richard Zhang, Phillip Isola, Alexei A\nEfros, Eli Shechtman, and Oliver Wang. The unreasonable\neffectiveness of deep features as a perceptual metric. In\nCVPR, pages 586‚Äì595, 2018.\n[Zhu et al., 2017a] Jun-Yan Zhu, Taesung Park, Phillip Isola,\nand Alexei A. Efros. Unpaired image-to-image transla-\ntion using cycle-consistent adversarial networks. In ICCV,\npages 2223‚Äì2232, 2017.\n[Zhu et al., 2017b] Mingrui Zhu, Nannan Wang, Xinbo Gao,\nand Jie Li. Deep graphical feature learning for face sketch\nsynthesis. In IJCAI, pages 3574‚Äì3580, 2017.\nProceedings of the Thirtieth International Joint Conference on ArtiÔ¨Åcial Intelligence (IJCAI-21)\n1358"
}