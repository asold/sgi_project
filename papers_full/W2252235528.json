{
  "title": "Enriching Cold Start Personalized Language Model Using Social Network Information",
  "url": "https://openalex.org/W2252235528",
  "year": 2014,
  "authors": [
    {
      "id": "https://openalex.org/A2244551346",
      "name": "Yu-Yang Huang",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A2109109241",
      "name": "Rui Yan",
      "affiliations": [
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A4212467910",
      "name": "Tsung-Ting Kuo",
      "affiliations": [
        "National Taiwan University"
      ]
    },
    {
      "id": "https://openalex.org/A2707088141",
      "name": "Shou-De Lin",
      "affiliations": [
        "National Taiwan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2097927681",
    "https://openalex.org/W2950186769",
    "https://openalex.org/W2014337138",
    "https://openalex.org/W144443890",
    "https://openalex.org/W2098697179",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W4240913316",
    "https://openalex.org/W2137813581",
    "https://openalex.org/W2151752770",
    "https://openalex.org/W1560512119",
    "https://openalex.org/W568081588",
    "https://openalex.org/W2115205574",
    "https://openalex.org/W2163382007",
    "https://openalex.org/W1660606671",
    "https://openalex.org/W2132957691",
    "https://openalex.org/W2076006872",
    "https://openalex.org/W2290492448",
    "https://openalex.org/W2004858782",
    "https://openalex.org/W1971111363",
    "https://openalex.org/W2136542423",
    "https://openalex.org/W4297945192",
    "https://openalex.org/W2155520241",
    "https://openalex.org/W2143570397",
    "https://openalex.org/W1943015726",
    "https://openalex.org/W2065512234",
    "https://openalex.org/W2108168165"
  ],
  "abstract": "We introduce a generalized framework to enrich the personalized language models for cold start users.The cold start problem is solved with content written by friends on social network services.Our framework consists of a mixture language model, whose mixture weights are estimated with a factor graph.The factor graph is used to incorporate prior knowledge and heuristics to identify the most appropriate weights.The intrinsic and extrinsic experiments show significant improvement on cold start users.",
  "full_text": "Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 611â€“617,\nBaltimore, Maryland, USA, June 23-25 2014.câƒ2014 Association for Computational Linguistics\nEnriching Cold Start Personalized Language Model \nUsing Social Network Information \n  Yu-Yang Huang\nâ€ \n, Rui Yan*, Tsung-Ting Kuo\nâ€¡\n, Shou-De Lin\nâ€ â€¡\n \nâ€ \nGraduate Institute of Computer Science and Information Engineering,  \nNational Taiwan University, Taipei, Taiwan \nâ€¡\nGraduate Institute of Network and Multimedia,  \nNational Taiwan University, Taipei, Taiwan \n*Computer and Information Science Department,  \nUniversity of Pennsylvania, Philadelphia, PA 19104, U.S.A. \n{r02922050, d97944007, sdlin}@csie.ntu.edu.tw, ruiyan@seas.upenn.edu \n \nAbstract \nWe introduce a generalized framework to enrich \nthe personalized language model s for cold start \nusers. The cold start problem  is solved  with \ncontent written by friends on  social network \nservices. Our framework consists of a mixture \nlanguage model, whose mixture weights are es-\ntimated with a factor graph . The factor graph is \nused to incorporate prior knowledge and heuris-\ntics to identify  the most appropriate w eights. \nThe intrinsic and extrinsic  experiments show \nsignificant improvement on cold start users. \n1 Introduction \nPersonalized language model s (PLM) on social \nnetwork services are useful in many aspects (Xue \net al., 2009 ; Wen et al., 2012 ; Clements, 2007), \nFor instance, if the authorship of a document  is \nin doubt, a PLM may  be used as a generative \nmodel to identify it. In this sense, a PLM serves \nas a proxy of  oneâ€™s writing style . Furthermore, \nPLMs can improve the quality of information \nretrieval and content-based recommendation sys-\ntems, where d ocuments or topics can be recom-\nmended based on the generative probabilities. \nHowever, it is challenging to build a PLM for \nusers who just entered the system, and  whose \ncontent is thus insufficient to characterize them . \nThese are called â€œcold startâ€  users. Producing \nbetter recommendations is even more critical for \ncold start users to make them continue to use the \nsystem. Therefore, this paper focuses on how to \novercome the cold start problem and obtain a \nbetter PLM for cold start users. \nThe content written by friends  on a social \nnetwork service, such as Facebook or Twitter , is \nexploited. It can be either a reply to an original \npost or posts by friends.  Here the hypothesis is \nthat friends, who usually share common interests, \ntend to discuss similar topics and use similar \nwords than non -friends. In other words , we be-\nlieve that a cold start userâ€™s  language model can \nbe enriched and better personalized by incorpo-\nrating content written by friends. \nIntuitively, a linear combination of document -\nlevel language models can be used to incorporate \ncontent written by friends. However, it should be \nnoticed that some documents are more relevant \nthan others, and should be weighted higher. To \nobtain better weights, some simple heuristics  \ncould be exploited. For example, we can measure \nthe s imilarity or distance  between a user lan-\nguage model and a document language model. In \naddition, documents that are shared frequently in \na social network  are usually  considered to be \nmore influential, and could contribute more to \nthe language model . More complex heuristics  \ncan also be derived. For instance, if two  docu-\nments are posted by  the same person, their \nweights should be more similar. The main chal-\nlenge lies in how such heuristics can be utilized \nin a systematic manner to infer the weights of \neach document-level language model. \nIn this paper, w e exploit the information on \nsocial network services in two ways. First, we \nimpose the social dependency assumption via a \nfinite mixture model. We model t he true, albeit \nunknown, personalized language model as a \ncombination of a biased user language model and \na set of relevant document language models. Due \nto the noise inevitably contained in social media \ncontent, instead of using all available documents, \nwe argue that by properly specifying the set of \nrelevant documents , a better personalized lan-\nguage model can be learnt. In other words,  each \nuser language model is enriched by a personal-\nized collection of background documents. \nSecond, we propose a factor graph model  \n(FGM) to incorporate prior knowledge (e.g. the \nheuristics described above) into our model . Each \n611\nmixture weight is represented by a random vari-\nable in the factor graph, and  an efficient  algo-\nrithm is proposed to optimize the model and infer \nthe marginal distribution of these variables.  Use-\nful information about these variables is encoded \nby a set of potential functions. \nThe main contributions of this work are sum-\nmarized below: \nï‚Ÿ To solve the cold start problem  encountered \nwhen estimating PLM s, a generalized frame-\nwork based on FGM is proposed. We incorpo-\nrate social network information into user lan-\nguage models through the use of FGM . An it-\nerative optimization procedur e utilizing per-\nplexity is presented to  learn the  parameters. \nTo our knowledge, this is the first proposal to \nuse FGM to enrich language models. \nï‚Ÿ Perplexity is selected as an intrinsic evalua-\ntion, and experiment on authorship attribution \nis used as an extrinsic evaluation. The results \nshow that our model yield s significant im-\nprovements for cold start users. \n2 Methodology \n2.1 Social-Driven Personalized Language \nModel \nThe language model of a collection of documents \ncan be estimated by normalizing the count s of \nwords in the entire collection (Zhai, 2008 ). To \nbuild a user language model, one naÃ¯ve way is to \nfirst normalize word frequency ğ‘(ğ‘¤, ğ‘‘)  within \neach document, and then  average over all the \ndocuments in a userâ€™s document collection. The \nresulting unigram user language model is: \nğ‘ƒğ‘¢(ğ‘¤) = 1\n|ğ’Ÿğ‘¢| âˆ‘ ğ‘(ğ‘¤, ğ‘‘)\n|ğ‘‘|ğ‘‘âˆˆğ’Ÿğ‘¢\n \n= 1\n|ğ’Ÿğ‘¢| âˆ‘ ğ‘ƒğ‘‘(ğ‘¤)\nğ‘‘âˆˆğ’Ÿğ‘¢\n \n(1) \nwhere ğ‘ƒğ‘‘(ğ‘¤) is the language model  of a particu-\nlar document, and ğ’Ÿğ‘¢ is the userâ€™s document col-\nlection. This formulation is basically an equal-\nweighted finite mixture model. \nA simple yet effective way to smooth a lan-\nguage model i s to linearly interpolate with a \nbackground language model  (Chen and Good-\nman, 1996; Zhai and Lafferty, 2001). In the line-\nar interpolation method , all background docu-\nments are treated equally. The entire document \ncollection is  added to the user language model \nğ‘ƒğ‘¢(ğ‘¤) with the same interpolation coefficient. \nOur main idea is to specify a set of relevant \ndocuments for the target user using information \nembedded in a social network,  and enrich the \nsmoothing procedure with these documents. L et \nğ’Ÿğ‘Ÿğ‘’ğ‘™  denote the content from relevant persons \n(e.g. social neighbors) of u1, our idea can be con-\ncisely expressed as: \nğ‘ƒğ‘¢1\nâ€² (ğ‘¤) = ğœ†ğ‘¢1 ğ‘ƒğ‘¢1(ğ‘¤) + âˆ‘ ğœ†ğ‘‘ğ‘– ğ‘ƒğ‘‘ğ‘– (ğ‘¤)\nğ‘‘ğ‘–âˆˆğ’Ÿğ‘Ÿğ‘’ğ‘™\n (2) \nwhere ğœ†ğ‘‘ğ‘– is the mi xture weight of the language \nmodel of document  di, and  ğœ†ğ‘¢1 + âˆ‘ ğœ†ğ‘‘ğ‘– = 1 . \nDocuments posted by irrelevant users  are not \nincluded as we believe the user language model \ncan be better personalized by exploiting the so-\ncial relationship in a more structured way. In our \nexperiment, we choose the first degree neighbor \ndocuments as ğ’Ÿğ‘Ÿğ‘’ğ‘™. \nAlso n ote that we have made no assumption \nabout how the â€œbaseâ€ user language model  \nğ‘ƒğ‘¢1 (ğ‘¤) is built. In practice, it need not be models \nfollowing maximum likelihood estimation, but \nany language model can be integrated into our \nframework to achieve  a better  refined model. \nFurthermore, any smoothing method can be ap-\nplied to the language model without degrading \nthe effectiveness. \n2.2 Factor Graph Model (FGM) \nNow we discuss how the mixture weights  can be \nestimated. We introduce a factor graph model  \n(FGM) to make use of the diverse information on \na social network . Factor graph (Kschischang et \nal., 2006) is a bipartite graph consisting of a set \nof random variables and a set of factors  which \nsignifies the relationships among the variables. It \nis best suited in situations where the data is clear-\nly of a relational  nature (Wang et al., 2012 ). The \njoint distribution of the variables is factored ac-\ncording to the graph structure. Using FGM, one \ncan incorporate the knowledge into the potential \nfunction for optimization  and perform joi nt in-\nference over documents. As shown in Figure 1, \nthe variables included in the model are described \nas follows: \nCandidate variables ğ‘¦ğ‘– = âŒ©ğ‘¢, ğ‘‘ğ‘–âŒª . The ran-\ndom variables in the top layer stand for the de-\ngrees of belief that a document di should be in-\ncluded in the PLM of the target user ğ‘¢. \nFigure 1: A two -layered factor graph (FGM) \nproposed to estimate the mixture weights. \n612\nAttribute variables  xi. Local information is \nstored as the random variables in the bottom lay-\ner. For example, x1 might represent the number \nof common friends between the author of a doc-\nument di and our target user. \nThe potential functions in the FGM are: \nAttribute-to-candidate function. This poten-\ntial function captures the local dependencies of a \ncandidate variable to the relevant attributes. Let \nthe candidate variable  yi correspond to a doc u-\nment di, the attribute -to-candidate function of yi \nis defined in a log-linear form: \nğ‘“(ğ‘¦ğ‘–, ğ´) = 1\nğ‘ğ›¼\nğ‘’ğ‘¥ğ‘{ğ›¼ğ‘‡ğŸ(ğ‘¦ğ‘–, ğ´)} (3) \nwhere A is the set of attributes of either the doc-\nument di or target user u; f is a vector of feature \nfunctions which locally model the value of  yi \nwith attributes  in A; ğ‘ğ›¼  is the local partition \nfunction and ğ›¼ is the weight vector to be learnt. \nIn our experiment, we define the vector of \nfunctions as ğŸ = âŒ©ğ‘“ğ‘ ğ‘–ğ‘š, ğ‘“ğ‘œğ‘œğ‘£, ğ‘“ğ‘ğ‘œğ‘, ğ‘“ğ‘ğ‘šğ‘“, ğ‘“ğ‘ğ‘“âŒªğ‘‡ as: \nï‚Ÿ Similarity function ğ‘“ğ‘ ğ‘–ğ‘š . The similarity be-\ntween language models of the target user and \na document should play an important role. We \nuse c osine similarity  between two unigram \nmodels in our experiments. \nï‚Ÿ Document quality function ğ‘“ğ‘œğ‘œğ‘£. The out-of-\nvocabulary (OOV) ratio is used to measure the \nquality of a document. It is defined as \nğ‘“ğ‘œğ‘œğ‘£ = 1 âˆ’ |{ğ‘¤: ğ‘¤ âˆˆ ğ‘‘ğ‘– âˆ© ğ‘¤ âˆ‰ ğ‘‰}|\n|ğ‘‘ğ‘–|  (4) \nwhere ğ‘‰  is the vocabulary set of the entire \ncorpus, with stop words excluded. \nï‚Ÿ Document popularity function ğ‘“ğ‘ğ‘œğ‘ . This \nfunction is defined as the number of times di is \nshared to model the popularity of documents. \nï‚Ÿ Common friend function ğ‘“ğ‘ğ‘šğ‘“. It is d efined \nas the number of common friends between the \ntarget user u1 and the author of di. \nï‚Ÿ Author friendship function ğ‘“ğ‘ğ‘“ . Assuming \nthat documents posted by a user with more \nfriends are more influential , this function is \ndefined as the number of friends of diâ€™s author. \nCandidate-to-candidate function. This po-\ntential function defines the correlation of a can-\ndidate variable yi with another candidate variable \nyj in the factor graph. The function is defined as \nğ‘”(ğ‘¦ğ‘–, ğ‘¦ğ‘—) = 1\nğ‘ğ‘–ğ‘—,ğ›½\nğ‘’ğ‘¥ğ‘{ğ›½ğ‘‡ğ (ğ‘¦ğ‘–, ğ‘¦ğ‘—)} (5) \nwhere g is a vector of feature functions indicat-\ning whether two variables are correlated. If we \nfurther denote the set of all related variables as \nğº(ğ‘¦ğ‘–) , then for any candidate variable yi, we \nhave the following brief expression: \nğ‘”(ğ‘¦ğ‘–, ğº(ğ‘¦ğ‘–)) = âˆ ğ‘”(ğ‘¦ğ‘–, ğ‘¦ğ‘—)\nğ‘¦ğ‘—âˆˆğº(ğ‘¦ğ‘–)\n (6) \nFor two candidate variables, let the corre-\nsponding document be di and dj, respectively, we \ndefine the vector ğ  = âŒ©ğ‘”ğ‘Ÿğ‘’ğ‘™, ğ‘”ğ‘ğ‘ğ‘¡âŒªğ‘‡ as: \nï‚Ÿ User relationship function ğ‘”ğ‘Ÿğ‘’ğ‘™. We assume \nthat two candidate variables  have higher de-\npendency if they represent documents of the \nsame author  or the two authors are friends . \nThe dependency should be even greater if two \ndocuments are similar. Let ğ‘(ğ‘‘)  denote the \nauthor of a document d and ğ’©[ğ‘¢] denote the \nclosed neighborhood of a user u, we define \nğ‘”ğ‘Ÿğ‘’ğ‘™ = ğ•€{ğ‘(ğ‘‘ğ‘—) âˆˆ ğ’©[ğ‘(ğ‘‘ğ‘–)]} Ã— ğ‘ ğ‘–ğ‘š(ğ‘‘ğ‘–, ğ‘‘ğ‘—) (7) \nï‚Ÿ Co-category function ğ‘”ğ‘ğ‘ğ‘¡. For any two can-\ndidate variables, it is intuitive that the two var-\niables would have a higher correlation if di \nand dj are of the same category.  Let ğ‘(ğ‘‘) de-\nnote the category of document d, we define \nğ‘”ğ‘ğ‘ğ‘¡ = ğ•€{ğ‘(ğ‘‘ğ‘–) = ğ‘(ğ‘‘ğ‘—)} Ã— ğ‘ ğ‘–ğ‘š(ğ‘‘ğ‘–, ğ‘‘ğ‘—) (8) \n2.3 Model Inference and Optimization \nLet Y and X be the set of all candidate variables \nand attribute variables, respectively . The joint \ndistribution encoded by the FGM is given by \nmultiplying all potential functions. \nğ‘ƒ(ğ‘Œ, ğ‘‹) = âˆ ğ‘“(ğ‘¦ğ‘–, ğ´)ğ‘”(ğ‘¦ğ‘–, ğº(ğ‘¦ğ‘–))\nğ‘–\n (9) \nThe desired marginal distribution can be ob-\ntained by marginalizing all other variables. Since \nunder most circumstances, however,  the factor \ngraph is densely connected, the exact inference is \nintractable and approximate inference is required.  \nAfter obtaining the marginal probabilities, t he \nmixture weights ğœ†ğ‘‘ğ‘–  in Eq. 2 are  estimated by \nnormalizing the  corresponding marginal proba-\nbilities ğ‘ƒ(ğ‘¦ğ‘–) over all candidate variables, which \ncan be written as \nğœ†ğ‘‘ğ‘– = (1 âˆ’ ğœ†ğ‘¢1 ) ğ‘ƒ(ğ‘¦ğ‘–)\nâˆ‘ ğ‘ƒ(ğ‘¦ğ‘—)ğ‘—:ğ‘‘ğ‘—âˆˆğ’Ÿğ‘Ÿğ‘’ğ‘™\n (10) \nwhere the constraint ğœ†ğ‘¢1 + âˆ‘ ğœ†ğ‘‘ğ‘– = 1 leads to a \nvalid proba bility distribut ion for  our mixture \nmodel. \nA factor graph is normally optimized by gra-\ndient-based methods. Unfortunately, since the \nground truth  values of the mixture weights are \nnot available, we are  prohibited from using su-\npervised approaches. Here we propose a two-step \niterative procedure  to optimize our model. At \n613\nfirst, all the model parameters (i.e. ğ›¼, ğ›½, ğœ†ğ‘¢) are \nrandomly initialized. Then, we infer the marginal \nprobabilities of candidate variables. Given these \nmarginal probabilities, w e can evaluate the per-\nplexity of the user language model  on a held-out \ndataset, and search for better paramete rs. This \nprocedure is repeated until convergence.  Also, \nnotice that by using FGM, we reduce the number \nof parameters from 1 + |ğ’Ÿğ‘Ÿğ‘’ğ‘™| to 1 + |ğ›¼| + |ğ›½|, \nlowering the risk of overfitting. \n3 Experiments \n3.1 Dataset and Experiment Setup \nWe perform experiment s on the Twitter dataset \ncollected by Galuba et al.  (2010). Twitter data \nhave been used to verify models with different \npurposes (Lin et al., 2011 ; Tan et al., 2011 ). To \nemphasize on the cold start scenario, we random-\nly selected 15 users with about 35 tweets and 70 \nfriends as candidates for an authorship attribution \ntask. Our experiment  corpus consists of 4322 \ntweets. All words with less than  5 occurrences \nare removed. Stop words and URLs are also re-\nmoved and all tweets are stemme d. We identify \nthe 100 most frequent terms as categories.  The \nsize of the vocabulary set is 1377. \nWe randomly partitioned the twee ts of each \nuser into training, validation and testing sets. The \nreported result is the average of 10 random splits. \nIn all experiments, we vary the size of training \ndata from 1% to 15%, and hold out the same \nnumber of tweets from each user as validation \nand testing data.  The statistics  of our dataset , \ngiven 15% training data, are shown in Table 1. \n Loopy belief propagation (LBP) is used to ob-\ntain the marginal probabilities of the variables \n(Murphy et al., 1999 ). Parameters are searched \nwith the pattern search algorithm (Audet and \nDennis, 2002). To not lose generality, we use the \ndefault configuration in all experiments. \n# of Max. Min. Avg. \nTweets 70 19 35.4 \nFriends 139 24 68.9 \nVariables 467 97 252.7 \nEdges 9216 231 3427.1 \nTable 1: Dataset statistics \n3.2 Baseline Methods \nWe compare our framework with two baseline \nmethods. The first (â€œCosineâ€) is a straight for-\nward implementation that sets all mixture \nweights ğœ†ğ‘‘ğ‘– to the cosine similarity between the \nprobability mass vectors of the document and \nuser unigram language models . The second \n(â€œPSâ€) uses the pattern search algorithm to per-\nform constrained optimization over the mixture \nweights. As mentioned in se ction 2.3, the main \ndifference between this method and ours \n(â€œFGMâ€) is that we reduce the search space of \nthe parameters by FGM. Furthermore, social \nnetwork information is exploited  in our frame-\nwork, while the PS method perform s a direct \nsearch over mixture weights, discarding valuable \nknowledge. \nDifferent from other smoothing methods that \nare usually mutually exclusive, any other \nsmoothing methods can be  easily merged into  \nour framework. In Eq. 2 , the base language \nmodel ğ‘ƒğ‘¢1 (ğ‘¤) can be already smoothed by any \ntechniques before being plugged into our frame-\nwork. Our framework then enriches the user lan-\nguage model with social network information . \nWe select four popular smoothing methods to \ndemonstrate such effect , name ly additive \nsmoothing, absolute smoothing (Ney et al., 1995), \nJelinek-Mercer smoothing  (Jelinek and Merc er, \n1980) and Dirichlet smoothing  (MacKay and \nPeto, 1994 ). The results of using only the base \nmodel (i.e. set ğœ†ğ‘‘ğ‘– = 0 in Eq. 2 ) are denoted as \nâ€œBaseâ€ in the following tables. \nTrain % Additive Absolute \nBase Cosine PS FGM Base Cosine PS FGM \n1% 900.4 712.6 725.5 537.5** 895.3 703.1 722.1 544.5** \n5% 814.5 623.4 690.5 506.8** 782.4 607.9 678.4 510.2** \n10% 757.7 566.6 684.8 481.2** 708.4 552.7 661.0 485.8** \n15% 693.8 521.0 635.2 474.8** 647.4 504.3 622.3 474.1** \nTrain % Jelinek-Mercer Dirichlet \nBase Cosine PS FGM Base Cosine PS FGM \n1% 637.8 571.4 643.1 541.0** 638.5 571.3 643.1 541.0** \n5% 593.9 526.1 602.9 505.4** 595.0 526.6 616.5 507.2** \n10% 559.2 494.1 573.8 483.6** 560.4 494.9 579.6 486.0** \n15% 535.3 473.4 560.2 473.0 535.7 473.6 563.2 474.4 \nTable 2: Testing set perplexity. ** indicates that the best score among all methods is significantly bet-\nter than the next highest score, by t-test at a significance level of 0.05. \n614\n3.3 Perplexity \nAs an intrinsic evaluation , we first compute the \nperplexity of unseen sentences under each user \nlanguage model. The result is shown in Table 2. \nOur method significantly outperforms all of \nthe methods in almost al l settings . We observe \nthat the â€œPSâ€ method takes a long time to con-\nverge and is prone to overfitting, likely because \nit has to search about a few hundred  parameters \non average . As expected, t he advantage of our \nmodel is more apparent when the data is sparse. \n3.4 Authorship Attribution (AA) \nThe authorship attribution (AA) task is chosen as \nthe extrinsic evaluation metric. Here the goal is \nnot about comparing with the state -of-the-art ap-\nproaches in AA, but showing that LM-based ap-\nproaches can benefit from our framework. \nTo apply PLM on this task, a  naÃ¯ ve Bayes \nclassifier is implemented (Peng et al., 2004). The \nmost probable author of a document d is the one \nwhose PLM yields the highest probability, and is \ndetermined by ğ‘¢âˆ— = argmaxğ‘¢{âˆ ğ‘ƒğ‘¢(ğ‘¤)ğ‘¤âˆˆğ‘‘ }. \nThe result is shown in Table 3. Our model im-\nproves personalization and outperforms the base-\nlines under cold start  settings. When data is \nsparse, t he â€œPSâ€ method tends to overfit the  \nnoise, while the â€œCosineâ€ method contains too \nfew information and is severely biased . Our \nmethod strikes a balance between model com-\nplexity and the amount of information  included, \nand hence performs better than the others. \n4 Related Work \nPersonalization has long b een studied in various \ntextual related tasks. Personalized search is es-\ntablished by modeling user behavior when using \nsearch engines (Shen et al., 2005 ; Xue et al., \n2009). Query language model could be also ex-\npanded based on personaliz ed user modeling \n(Chirita et al., 2007 ). Personalization has also \nbeen modeled in many NLP tasks such as sum-\nmarization (Yan et al., 2011 ) and recommenda-\ntion (Yan et al., 2012 ). Different from our pur-\npose, these models do not aim at exploiting so-\ncial media content to enrich a language model.  \nWen et al.  (2012) combines user-level language \nmodels from a social network, but instead of fo-\ncusing on the cold start problem, they try to  im-\nprove the speech recognition performance using \na mass amount of texts on social network. On the \nother hand, our work explicitly models the more \nsophisticated document-level relationships using \na probabilistic graphical model. \n5 Conclusion \nThe advantage of our model is threefold. First, \nprior knowledge and heuristics about the social \nnetwork can be adapted in a structured way \nthrough the use of FGM. Second, by exploiting a \nwell-studied graphical model, mature inference \ntechniques, such as LB P, can be applied in the \noptimization procedure, making it much more \neffective and efficient . Finally, different from \nmost smoothing methods that are mutually ex-\nclusive, any other  smoothing method can be in-\ncorporated into our framework to be further en-\nhanced. Using only 1% of the training corpus, \nour model can improve the perplexity of base \nmodels by as much as 40% and the accuracy of \nauthorship attribution by at most 15%. \n6 Acknowledgement \nThis work was sponsored by AOARD grant \nnumber No. FA2386 -13-1-4045 an d National \nScience Council, National Taiwan University \nand Intel Corporation under Grants NSC102 -\n2911-I-002-001 and NTU103R7501 and grant \n102-2923-E-002-007-MY2, 102-2221-E-002-170, \n101-2628-E-002-028-MY2. \nTrain % Additive Absolute \nBase Cosine PS FGM Base Cosine PS FGM \n1% 54.67 58.27 61.07 63.74 49.47 57.60 58.27 64.27** \n5% 61.47 63.20 62.67 68.40** 59.60 62.40 61.33 66.53** \n10% 61.47 65.73 66.27 69.20** 61.47 65.20 64.67 71.87** \n15% 64.27 67.07 62.13 70.40** 64.67 68.27 63.33 71.60** \nTrain % Jelinek-Mercer Dirichlet \nBase Cosine PS FGM Base Cosine PS FGM \n1% 54.00 60.93 62.00 64.80** 52.80 60.40 61.87 64.67** \n5% 62.67 65.47 64.00 68.00 60.80 65.33 62.40 66.93 \n10% 63.87 68.00 67.87 68.53 62.53 67.87 66.40 68.53 \n15% 65.87 70.40 64.14 69.87 65.47 70.27 64.53 68.40 \nTable 3: Accuracy (%) of authorship attribution. ** indicates that the best score among all methods is \nsignificantly better than the next highest score, by t-test at a significance level of 0.05. \n615\nReference \nCharles Audet and J. E. Dennis, Jr. 2002. Analysis of \ngeneralized pattern searches. SIAM J. on Optimiza-\ntion, 13(3):889â€“903, August. \nStanley F. Chen and Joshua Goodman. 1996. An em-\npirical study of smoothing techniques for language \nmodeling. In Proceedings of the 34th Annual Meet-\ning on Association for Computational Linguistics , \nACL â€™96, pages 310 â€“318, Stroudsburg, PA, USA. \nAssociation for Computational Linguistics. \nPaul Alexandru Chirita, Claudiu  S. Firan, and Wolf-\ngang Nejdl. 2007. Per sonalized query expansion \nfor the web. In Proceedings of the 30th Annual In-\nternational ACM SIGIR Conference on Research \nand Development in Information Retrieval , \nSIGIR â€™07, pages 7 â€“14, New York, NY, USA. \nACM. \nMaarten Clements. 2007. Personali zation of social \nmedia. In Proceedings of the 1st BCS IRSG Con-\nference on Future Directions in Information Access, \nFDIAâ€™07, pages 14 â€“14, Swinton, UK, UK. British \nComputer Society. \nWojciech Galuba, Karl Aberer, Dipanjan Chakraborty, \nZoran Despotovic, and Wolfgang Kellerer. 2010. \nOuttweeting the twitterers - predicting information \ncascades in microblogs. In Proceedings of the 3rd \nConference on Online Social Networks, WOSNâ€™10, \npages 3â€“3, Berkeley, CA, USA. USENIX Associa-\ntion. \nFrederick Jelinek and Robert  L. Mercer. 1980. Inter-\npolated estimation of markov source parameters \nfrom sparse data. In In Proceedings of the Work-\nshop on Pattern Recognition in Practice , pages \n381â€“397, Amsterdam, The Netherlands: North -\nHolland, May. \nF. R. Kschischang, B.  J. Frey, and H.  A. Loeliger. \n2006. Factor graphs and the sum -product algorithm. \nIEEE Trans. Inf. Theor. , 47(2):498 â€“519, Septem-\nber. \nJimmy Lin, Rion Snow, and William Morgan. 2011. \nSmoothing techniques for adaptive online language \nmodels: Topic tracking in tweet streams. In Pro-\nceedings of the 17th ACM SIGKDD International \nConference on Knowledge Discovery and Data \nMining, KDD â€™11, pages 422â€“429, New York, NY, \nUSA. ACM. \nDavid J.C. MacKay and Linda C. Bauman Peto. 1994. \nA hierarchical dirichlet language model. Natural \nLanguage Engineering, 1:1â€“19. \nKevin P. Murphy, Yair Weiss, and Michael  I. Jordan. \n1999. Loopy belief propagation for approximate in-\nference: An em pirical study. In Proceedings of the \nFifteenth Conference on Uncertainty in Artificial \nIntelligence, UAIâ€™99, pages 467 â€“475, San Francis-\nco, CA, USA. Morgan Kaufmann Publishers Inc. \nHermann Ney, Ute Essen, and Reinhard Kneser. 1995. \nOn the es timation of â€™smallâ€™ probabilities by leav-\ning-one-out. IEEE Trans. Pattern Anal. Mach. In-\ntell., 17(12):1202â€“1212, December. \nFuchun Peng, Dale Schuurmans, and Shaojun Wang. \n2004. Augmenting naive bayes classifiers with sta-\ntistical language models. Inf. Retr., 7(3-4):317â€“345, \nSeptember. \nXuehua Shen, Bin Tan, and ChengXiang Zhai. 2005. \nImplicit user modeling for personalized search. In \nProceedings of the 14th ACM International Con-\nference on Information and Knowledge Manage-\nment, CIKM â€™05, pages 824 â€“831, New York, NY, \nUSA. ACM. \nChenhao Tan, Lillian Lee, Jie Tang, Long Jiang, \nMing Zhou, and Ping Li. 2011. User -level senti-\nment analysis incorporating social networks. In \nProceedings of the 17th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery and \nData Mining , KDD â€™11, pages 1397 â€“1405, New \nYork, NY, USA. ACM. \nZhichun Wang, Juanzi Li, Zhigang Wang, and Jie \nTang. 2012. Cross -lingual knowledge linking \nacross wiki knowledge bases. In Proceedings of the \n21st International Conference on World Wide Web , \nWWW â€™12, pages 459 â€“468, New York, NY, USA. \nACM. \nTsung-Hsien Wen, Hung-Yi Lee, Tai-Yuan Chen, and \nLin-Shan Lee. 2012. Personalized language model-\ning by crowd sourcing with social network data for \nvoice access of cloud applications. In Spoken Lan-\nguage Technology Workshop (SLT), 2012 IEEE , \npages 188â€“193. \nGui-Rong Xue, Jie Han, Yong Yu, and Qiang Yang. \n2009. User language model for collaborative per-\nsonalized search. ACM Trans.  Inf. Syst. , \n27(2):11:1â€“11:28, March. \nRui Yan, Jian -Yun Nie, and Xiaoming Li. 2011. \nSummarize what you are interested in: An optimi-\nzation framework for interactive personalized \nsummarization. In Proceedings of the Conference \non Empirical Me thods in Natural Language Pro-\ncessing, EMNLP â€™11, pages 1342 â€“1351, Strouds-\nburg, PA, USA. Association for Computational \nLinguistics. \nRui Yan, Mirella Lapata, and Xiaoming Li. 2012. \nTweet recommendation with graph co -ranking. In \nProceedings of the 50th Annual Meeting of the As-\nsociation for Computational Linguistics: Long Pa-\npers - Volume 1 , ACL â€™12, pages 516 â€“525, \nStroudsburg, PA, USA. Association for Computa-\ntional Linguistics. \nChengXiang Zhai. 2008. Statistical Language Models \nfor Inf ormation Retrieval . Now Publishers Inc., \nHanover, MA, USA. \n616\nChengxiang Zhai and John Lafferty. 2001. A study of \nsmoothing methods for language models applied to \nad hoc information retrieval. In Proceedings of the \n24th Annual Internati onal ACM SIGIR Conference \non Research and Development in Information Re-\ntrieval, SIGIR â€™01, pages 334 â€“342, New York, NY, \nUSA. ACM. \n \n617",
  "topic": "Heuristics",
  "concepts": [
    {
      "name": "Heuristics",
      "score": 0.8046237230300903
    },
    {
      "name": "Computer science",
      "score": 0.7488110661506653
    },
    {
      "name": "Cold start (automotive)",
      "score": 0.6983983516693115
    },
    {
      "name": "Social network (sociolinguistics)",
      "score": 0.5566970705986023
    },
    {
      "name": "Factor (programming language)",
      "score": 0.5553532838821411
    },
    {
      "name": "Graph",
      "score": 0.5311728119850159
    },
    {
      "name": "Language model",
      "score": 0.4881909489631653
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3749692738056183
    },
    {
      "name": "Social media",
      "score": 0.30532118678092957
    },
    {
      "name": "Theoretical computer science",
      "score": 0.25679725408554077
    },
    {
      "name": "World Wide Web",
      "score": 0.24173319339752197
    },
    {
      "name": "Programming language",
      "score": 0.12436193227767944
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Aerospace engineering",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16733864",
      "name": "National Taiwan University",
      "country": "TW"
    },
    {
      "id": "https://openalex.org/I79576946",
      "name": "University of Pennsylvania",
      "country": "US"
    }
  ]
}