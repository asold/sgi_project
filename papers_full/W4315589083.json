{
  "title": "Unsupervised Multivariate Time-Series Transformers for Seizure Identification on EEG",
  "url": "https://openalex.org/W4315589083",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5103021871",
      "name": "İlkay Yıldız Potter",
      "affiliations": [
        "BioSensics (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5008481063",
      "name": "George Zerveas",
      "affiliations": [
        "Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A5014921416",
      "name": "Carsten Eickhoff",
      "affiliations": [
        "Brown University"
      ]
    },
    {
      "id": "https://openalex.org/A5050145653",
      "name": "Dominique Duncan",
      "affiliations": [
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2911761011",
    "https://openalex.org/W2884184673",
    "https://openalex.org/W2038235993",
    "https://openalex.org/W2810229480",
    "https://openalex.org/W3122023346",
    "https://openalex.org/W2521265470",
    "https://openalex.org/W2985003680",
    "https://openalex.org/W3010234284",
    "https://openalex.org/W3172982478",
    "https://openalex.org/W3144792525",
    "https://openalex.org/W3005778357",
    "https://openalex.org/W3006560451",
    "https://openalex.org/W3190152617",
    "https://openalex.org/W3082122168",
    "https://openalex.org/W2766013185",
    "https://openalex.org/W2569550367",
    "https://openalex.org/W2466222258",
    "https://openalex.org/W3021258298",
    "https://openalex.org/W3013070006",
    "https://openalex.org/W4200545967",
    "https://openalex.org/W3188872815",
    "https://openalex.org/W2994921215",
    "https://openalex.org/W3087997944",
    "https://openalex.org/W3015929423",
    "https://openalex.org/W3188671801",
    "https://openalex.org/W2892542319",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3177342940",
    "https://openalex.org/W3096831136",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W6638667902",
    "https://openalex.org/W6718683173",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W4255052368",
    "https://openalex.org/W6633205339",
    "https://openalex.org/W2345279893",
    "https://openalex.org/W2295598076",
    "https://openalex.org/W2982438846",
    "https://openalex.org/W2158698691",
    "https://openalex.org/W6758101687",
    "https://openalex.org/W2157825442",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W3042807565",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2910068345",
    "https://openalex.org/W1556131344",
    "https://openalex.org/W565955800",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2462831000"
  ],
  "abstract": "Epilepsy is one of the most common neurological disorders, typically observed\\nvia seizure episodes. Epileptic seizures are commonly monitored through\\nelectroencephalogram (EEG) recordings due to their routine and low expense\\ncollection. The stochastic nature of EEG makes seizure identification via\\nmanual inspections performed by highly-trained experts a tedious endeavor,\\nmotivating the use of automated identification. The literature on automated\\nidentification focuses mostly on supervised learning methods requiring expert\\nlabels of EEG segments that contain seizures, which are difficult to obtain.\\nMotivated by these observations, we pose seizure identification as an\\nunsupervised anomaly detection problem. To this end, we employ the first\\nunsupervised transformer-based model for seizure identification on raw EEG. We\\ntrain an autoencoder involving a transformer encoder via an unsupervised loss\\nfunction, incorporating a novel masking strategy uniquely designed for\\nmultivariate time-series data such as EEG. Training employs EEG recordings that\\ndo not contain any seizures, while seizures are identified with respect to\\nreconstruction errors at inference time. We evaluate our method on three\\npublicly available benchmark EEG datasets for distinguishing seizure vs.\\nnon-seizure windows. Our method leads to significantly better seizure\\nidentification performance than supervised learning counterparts, by up to 16%\\nrecall, 9% accuracy, and 9% Area under the Receiver Operating Characteristics\\nCurve (AUC), establishing particular benefits on highly imbalanced data.\\nThrough accurate seizure identification, our method could facilitate widely\\naccessible and early detection of epilepsy development, without needing\\nexpensive label collection or manual feature extraction.\\n",
  "full_text": "Unsupervised Multivariate Time-Series\nTransformers for Seizure Identiﬁcation on EEG\n˙Ilkay Yıldız Potter\nBioSensics LLC\nNewton, MA, USA\nilkay.yildiz@biosensics.com\nGeorge Zerveas\nDept. of Computer Science\nBrown University\nProvidence, RI, USA\ngeorge_zerveas@brown.edu\nCarsten Eickhoff\nDept. of Computer Science\nBrown University\nProvidence, RI, USA\ncarsten@brown.edu\nDominique Duncan\nKeck School of Medicine\nUniversity of Southern California\nLos Angeles, CA, USA\nDominique.Duncan@loni.usc.edu\nAbstract—Epilepsy is one of the most common neurologi-\ncal disorders, typically observed via seizure episodes. Epileptic\nseizures are commonly monitored through electroencephalogram\n(EEG) recordings due to their routine and low expense collection.\nThe stochastic nature of EEG makes seizure identiﬁcation via\nmanual inspections performed by highly-trained experts a tedious\nendeavor, motivating the use of automated identiﬁcation. The lit-\nerature on automated identiﬁcation focuses mostly on supervised\nlearning methods requiring expert labels of EEG segments that\ncontain seizures, which are difﬁcult to obtain. Motivated by these\nobservations, we pose seizure identiﬁcation as an unsupervised\nanomaly detection problem. To this end, we employ the ﬁrst\nunsupervised transformer-based model for seizure identiﬁcation\non raw EEG. We train an autoencoder involving a transformer\nencoder via an unsupervised loss function, incorporating a novel\nmasking strategy uniquely designed for multivariate time-series\ndata such as EEG. Training employs EEG recordings that do not\ncontain any seizures, while seizures are identiﬁed with respect\nto reconstruction errors at inference time. We evaluate our\nmethod on three publicly available benchmark EEG datasets\nfor distinguishing seizure vs. non-seizure windows. Our method\nleads to signiﬁcantly better seizure identiﬁcation performance\nthan supervised learning counterparts , by up to 16% recall, 9%\naccuracy, and 9% Area under the Receiver Operating Charac-\nteristics Curve (AUC), establishing particular beneﬁts on highly\nimbalanced data. Through accurate seizure identiﬁcation, our\nmethod could facilitate widely accessible and early detection of\nepilepsy development, without needing expensive label collection\nor manual feature extraction.\nIndex Terms—Epilepsy, Seizure, EEG, Unsupervised Learning,\nTime-series Transformer\nI. I NTRODUCTION\nEpilepsy is one of the most common neurological disorders,\naffecting over 70 million people worldwide [35]. Epilepsy\npatients typically suffer from seizures, involving uncontrolled\njerking movements or momentary losses of awareness due to\nabnormal excessive or synchronous activities in the brain [39].\nThe degraded quality of life for patients strongly motivates\nearly seizure identiﬁcation, as early seizures have been shown\nThis is a pre-print version of the article: I. Yıldız Potter, G. Zerveas, C.\nEickhoff, D. Duncan, “Unsupervised Multivariate Time-Series Transformers\nfor Seizure Identiﬁcation on EEG”, IEEE Conference on Machine Learning\nand Applications (ICMLA) 2022, DOI 10.1109/ICMLA55696.2022.00208.\nWork was performed at University of Southern California and supported by\nthe National Institutes of Health (NIH) National Institute of Neurological Dis-\norders and Stroke (NINDS) grant R01NS111744. Code is publicly available\nat https://github.com/ilkyyldz95/EEG_MVTS.\nto be prognostic markers for later epileptogenic develop-\nment. Successful identiﬁcation of early seizures can initiate\nantiepileptogenic intervention and therapies that can remark-\nably improve the quality of life for patients and their care-\ngivers. To this end, electroencephalogram (EEG) recordings\nreceived particular attention for seizure identiﬁcation [33], due\nto their routine and low expense collection compared to, e.g.,\nneuroimaging. Seizures on EEG are deﬁned as generalized\nspike-wave discharges at three per second or faster, and clearly\nevolving discharges of any type that reach a frequency of four\nper second or faster.\nDespite their volume and rich information content, EEG\nrecordings are known to contain many artifacts due to move-\nment, physiological activity such as perspiration, and measure-\nment hardware [13], [30]. The stochastic nature of clinically-\nacquired EEG makes seizure identiﬁcation via manual inspec-\ntion laborious and difﬁcult, leading to signiﬁcant variability\nacross clinical labels of different experts [43]. This challenge\nmotivated the recent literature to focus on automated identi-\nﬁcation of epileptic seizures on EEG as a promising comple-\nment to manual inspection. The literature on automated EEG\nseizure identiﬁcation is extensive (c.f. Section II), focusing\nmostly on supervised machine learning methods using both\nmanual feature extraction [1], [26], [29], [46], as well as deep\nneural networks (DNNs) without manual feature extraction\n[16], [25], [44].\nDespite their success, supervised methods require expert\nlabels indicating EEG segments that contain seizures, while\nobtaining large and consistently-labeled EEG datasets is un-\nfavourable due to the stochastic nature of EEG [43]. Difﬁ-\nculty of label collection also leads to severely imbalanced\nEEG datasets, in which the number of non-seizure recordings\nsigniﬁcantly exceeds the number of seizure recordings; this\nposes a further challenge for supervised learning that is prone\nto overﬁtting towards dominant class predictions [23].\nUnsupervised machine learning methods, which do not rely\non labeled data have not yet been widely explored. A few\nmethods employed traditional shallow models for unsuper-\nvised seizure identiﬁcation on both raw EEG [7], as well as\nspatio-temporal features extracted from EEG [2], [3], [10].\nTo the best of our knowledge, unsupervised DNN methods\nfor EEG seizure identiﬁcation have been limited to a couple\narXiv:2301.03470v1  [eess.SP]  3 Jan 2023\nof recent works, requiring feature extraction prior to training\n[40] or employing convolutional DNN architectures that are\nnot tailored for multivariate time-series data such as EEG [41].\nWe propose a fully-unsupervised deep learning approach\nthat can identify seizures on raw EEG recordings. To this end,\nwe make the following contributions:\n• We employ the ﬁrst unsupervised transformer-based\nmodel for seizure identiﬁcation on raw EEG, inspired by\nrecent advances in multivariate time-series analysis [42].\n• We pose seizure identiﬁcation as an anomaly detection\nproblem. To this end, we train an autoencoder involving\na transformer encoder via an unsupervised loss function,\nincorporating a novel masking strategy uniquely designed\nfor modeling multivariate time-series data such as EEG.\nAs training employs EEG recordings that do not contain\nseizures, seizures are identiﬁed via mean reconstruction\nerrors at inference time.\n• We extensively validate the seizure identiﬁcation per-\nformance of our method on three publicly available\nbenchmark EEG datasets. Our method can successfully\ndistinguish between non-seizure vs. seizure windows,\nwith up to 0.94 Area under the Receiver Operating Char-\nacteristics Curve (AUC) . Moreover, our unsupervised\nanomaly detection approach leads to signiﬁcantly better\nseizure identiﬁcation performance than the supervised\nlearning counterparts, by up to 16% recall, 9% accuracy,\nand 9% AUC, establishing a particular beneﬁt for learning\nfrom highly imbalanced data.\nII. R ELATED WORK\nThe literature on automated seizure identiﬁcation on EEG\nis vast; we refer the reader to the review by [5] for more\ndetails. A signiﬁcant body of works focus on extracting spatio-\ntemporal features from EEG via, e.g., wavelet transformations\n[1], [28], local mean decomposition [43], Fourier transfor-\nmations [26], [29], and power spectra [46]. Extracted fea-\ntures are used to train supervised machine learning methods,\nincluding support vector machines and neural networks, to\nidentify whether a given EEG contains a seizure in a binary\nclassiﬁcation setting.\nDeep neural network (DNN)-based supervised seizure iden-\ntiﬁcation methods have lately dominated the literature [45]\nand obviated the need for manual feature extraction. DNN\nmethods further improved in combination with recurrent neural\nnetworks to aid time-series modeling [6], adversarial training\nto generalize identiﬁcation across patients [44], autoencoder-\nbased feature extraction [34], and attention mechanisms to\nimprove predictions and interpretability [25].\nIn recent years, self-attention modules have become an in-\ntegral part of DNN methods employed in machine vision [15],\nnatural language processing [14], and time-series modeling\n[42]; the resulting DNN architectures are termed as trans-\nformers. Transformer architectures have been very recently\napplied for various identiﬁcation tasks on EEG, including,\ne.g., sleep-stage classiﬁcation , human-computer interface-\nbased action recognition, and seizure identiﬁcation [16], [24].\nThese methods employ unsupervised pre-training prior to\nsupervised training on ground-truth expert labels pertaining to\nthe identiﬁcation task. The unsupervised pre-training objective\ninvolves different augmentations of the same EEG segment\nand trains the transformer by maximizing (minimizing) the\nsimilarity of different augmentations (segments).\nAll in all, the literature on automated seizure identiﬁca-\ntion often focuses on supervised machine learning methods.\nDespite their success, these methods require expert labels\nindicating EEG segments that contain seizures, which are\ndifﬁcult to obtain due to the stochastic nature of EEG [43].\nMeanwhile, unsupervised machine learning methods that do\nnot rely on labeled data have not yet been widely explored. A\nfew methods applied shallow models for unsupervised seizure\nidentiﬁcation, including K-means, hierarchical clustering, and\nGaussian mixture models, on both raw EEG [7], as well as\nspatio-temporal features extracted from EEG [2], [3].\nRecently, a couple of unsupervised DNN methods for\nseizure identiﬁcation on EEG have been proposed. You et\nal. (2020) preprocess EEGs to extract time-frequency spectro-\ngram images and train a generative adversarial network (GAN)\n[40] on the spectrograms that do not contain seizures. For each\nspectrogram at testing time, they have to search for the latent\nGAN input that leads to the smallest loss value and use the cor-\nresponding generated spectrogram for seizure identiﬁcation.\nAs training involves non-seizure activity, test spectrograms\nthat signiﬁcantly differ from the spectrograms generated by the\nGAN are successfully identiﬁed to contain seizures. Yıldız et\nal. (2022) train a convolutional variational autoencoder (V AE)\nover raw EEG, employing an objective tailored for suppressing\nEEG artifacts. Unlike You et al. (2020), they identify seizures\nwith respect to the reconstruction errors at inference time.\nWe differ from the existing works by applying the ﬁrst\nfully-unsupervised transformer-based model on raw EEG. Our\narchitecture and training objective are particularly designed\nfor multivariate time-series analysis and do not require a\nsophisticated minimax optimization such as GAN training.\nThe fundamental beneﬁt of a transformer encoder over other\nDNN architectures is that self-attention can selectively high-\nlight important input features and sequence segments, without\nrelying on sequence-aligned convolutions or slow recurrent\nmodules [38]; we also experimentally demonstrate this advan-\ntage against unsupervised V AE-based seizure identiﬁcation in\nSection IV-E.\nIII. P ROBLEM FORMULATION\nWe consider a dataset of N EEG recordings, each collected\nfrom M electrode channels and consisting of T time points.\nFormally, we denote each EEG recording by X(i) ∈RT×M ,\nfor i ∈ [1,...,N ]. Our aim is to design an unsupervised\nmethod that does not rely on ground-truth expert labels during\nlearning and can identify the existence of seizures in a given\nEEG recording. To this end, we employ an autoencoder\narchitecture involving a transformer network encoder that is\nuniquely designed for multivariate time-series data [42], such\nas EEG. We note that our method naturally generalizes to\nEEG recordings comprising different numbers of time points\nand channels (see our preprocessing setup in Section IV-B).\nA. Multivariate Time-Series Transformer\nOur autoencoder architecture is based on a transformer\nencoder and is depicted in Figure 1: the model learns to\nextract and transform latent features from a given EEG\nrecording in order to reconstruct the stochastically-masked\ninput [42]. Formally, the transformer encoder network receives\na recording X(i) ∈ RT×M , i ∈ [1,...,N ], and extracts\nlatent features Z(i) ∈ RT×D. The output layer applies an\nafﬁne transformation on Z(i) to reconstruct the recording as\nˆX(i) ∈RT×M .\n1) Transformer Encoder: Transformer encoder operations\nbegin with projecting a recording X(i) from M dimensions\nto D dimensions via a trainable afﬁne transformation P ∈\nRM×D. To preserve the ordering information of the input\nsequence, a fully-trainable positional encoding E ∈RT×D\nis added for each input. The resulting latent features extracted\nfrom each recording are thus: Z(i) = X(i)P + E.\nDimensional projection and positional encoding are fol-\nlowed by the successive application of several transformer\nlayers. Each transformer layer consists of a multi-headed\nself-attention (MSA) module, a stochastic dropout operation\n˜d[32], batch normalization (Norm) [22], and a fully-connected\nnetwork (FCN) consisting of two linear layers separated by a\nGELU [21] activation, a non-linearity designed to be used in\ncombination with dropout and batch normalization. Formally,\nlatent features are updated by each transformer layer via:\nZ(i) ←Norm\n(\n˜d\n(\nMSA(Z(i))\n)\n+ Z(i)\n)\n,\nZ(i) ←Norm\n(\n˜d\n(\nFCN(Z(i))\n)\n+ Z(i)\n)\n. (1)\nThe summation of each latent feature with its transformation\nis a skip connection that aids generalization [20], along with\nbatch normalization that has been shown improvement against\nlayer normalization for multivariate time-series analysis [42].\n2) Multi-headed Self-attention Module: An MSA module\nis designed to assign selective importance to latent features\nextracted for each time point by the preceding layers of the\nencoder [38]. Particularly, MSA contains trainable parameters\nthat capture the similarity between input features at different\ntime points via their query, key, and value representations.\nMultiple attention heads enable adaptations to long-term de-\npendencies and capture relevance between segments of multi-\nvariate data, without prior bias based on position [42].\nFormally, at each time point t, the output representation is\ncomputed via a weighted sum over the value vectors z(i)\nv,t′ ∈\nRDv , t′ ∈[1,...,T ], where the importance weight assigned\nto the value vector at time t′ is computed as a dot-product\nsimilarity between its corresponding key vector z(i)\nk,t′ ∈RDq\nand a query vector z(i)\nq,t ∈RDq at time t. As a result, given a\nlatent feature Z(i), a query Z(i)\nq = [z(i)\nq,1; ... ; z(i)\nq,T ] ∈RT×Dq ,\na key Z(i)\nk = [ z(i)\nk,1; ... ; z(i)\nk,T ] ∈ RT×Dq , and a value\nZ(i)\nv = [z(i)\nv,1; ... ; z(i)\nv,T ] ∈RT×Dv are computed by applying\nthree different trainable afﬁne transformations on Z(i). The\nself-attention output for a single attention head (SA) is then\ncomputed via a scaled dot-product:\nSA(Z(i)) =softmax\n(\nZ(i)\nq Z(i)⊤\nk√\nDq\n)\nZ(i)\nv , (2)\nwhere softmax converts the similarity scores to a probability\ndistribution over the input sequence of length T. This opera-\ntion is performed in parallel for each of the H attention heads\n(each with its own trainable transformations). The resulting\noutputs SAh ∈RT×Dv , h∈[1,...,H ] are ﬁrst concatenated\nand ﬁnally aggregated into a single representation through a\ntrainable linear transformation WA ∈RHDv×D:\nMSA(Z(i))=[SA1(Z(i)) SA2(Z(i))... SAH(Z(i))]WA. (3)\nB. Reconstruction-Based Loss Function\nWe aim for the transformer model to extract discriminative\nlatent features that govern the generation of EEG recordings,\ni.e., to model the input data distribution. To this end, we\ncorrupt each input sample by a novel masking strategy that\nis uniquely designed for modeling multivariate time-series\ndata such as EEG [42]. We train the transformer model\nvia a loss function that minimizes the error between the\noriginal (unmasked) recording X(i) and the corresponding\nreconstruction ˆX(i).\nFormally, a proportion r ∈ (0,1) of each channel m ∈\n{1,...,M } in each EEG recording X(i) is dynamically\nmasked at the beginning of each training step by setting\nthe encoder input values at chosen time points to 0. The\nvalues at each channel alternate between consecutive masked\nand unmasked sequences. The number of masked time points\nfollows a geometric distribution with mean lm, while the num-\nber of unmasked time points follows a geometric distribution\nwith mean lu = 1−r\nr lm. This transition paradigm is also\nknown as an M/M/1 queue, in which the number of customers\nin a system is geometrically distributed [18]. The resulting\nmasking strategy encourages the transformer to attend on time\npoints preceding and following the masked segments both in\nindividual channels, as well as across the aligned time points\nin other channels to capture inter-channel dependencies, and\nhas been found more effective than other denoising strategies\nfor downstream tasks, including Bernoulli masking (c.f. Table\nII & [42]).\nFinally, the reconstruction loss for end-to-end training of our\nmodel is the mean-squared reconstruction error. Crucially, the\nloss is computer over only the set of masked time points M=\n{(t,m) |masked X(i)\nt,m,t ∈{1,...,T },m ∈{1,...,M }}:\n1\n|M|\n∑\n(t,m)∈M\n(X(i)\nt,m − ˆX(i)\nt,m)2. (4)\nC. Seizure Identiﬁcation\nWe aim to employ the trained transformer to distinguish\nbetween EEG recordings that contain seizures and those\nwhich do not; this motivates us to pose unsupervised seizure\nTransformer Encoder Output \nLayerTime\nChannels\nAffine \nDimension \nReduction\nPositional \nEncoding\nMulti-head \nself-attention\nBatch \nnormalization\nFully-connected \nNeural Network\nBatch \nnormalization\nAffine \nTransform\nTransformer layers repeated\nMultivariate Input \n Reconstruction               \nLatent features \nFig. 1: Our autoencoder architecture. The transformer encoder network receives a recording X(i), and extracts latent features\nZ(i). The output layer applies an afﬁne transformation on Z(i) to reconstruct the recording as ˆX(i) ∈RT×M . During training,\na proportion of each channel is masked by setting the input values at masked time points (shaded in gray) to 0.\nidentiﬁcation as an anomaly detection problem. Thus, we train\nthe transformer architecture on recordings that do not contain\nseizures. This allows for the learned latent features to capture\nnon-seizure activity [40]. As the transformer is trained to\nmodel non-seizure activity, recordings with no seizures are\nexpected to be reconstructed with low error in inference time.\nIn contrast, EEG recordings including seizure activity come\nfrom a different distribution, and thus, the model naturally\nreconstructs such input recordings with a relatively larger\nerror; we use this observation as an indicator for a seizure\n(c.f. Section IV-D).\nWe note that the exclusion of seizure recordings from the\ntraining set does not constitute supervision or require any\nspecial annotation, as the default states of patients and healthy\nindividuals alike are non-seizure, whose recordings can be\ncollected and kept separate from the recordings of seizure\nepisodes (which we only use for evaluating our method). In\nreal-life applications, EEG data with no seizure activity can\nbe easily augmented with recordings from healthy individuals,\nwhich are trivially accessible compared to the ones from\npatients experiencing seizures.\nIV. E XPERIMENTS\nA. Datasets\nWe evaluate our method on three publicly available EEG\ndatasets collected at the: (i) Massachusetts Institute of Tech-\nnology (MIT) and Boston Children’s Hospital [31] (ii) Uni-\nversity of Pennsylvania (UPenn) and Mayo Clinic [36], and\n(iii) Temple University Hospital of Philadelphia (TUH) [27].\nThe MIT dataset contains EEG recordings acquired on the\nscalp with 256 Hz sampling rate from a maximum of M = 38\nchannels. 198 seizure recordings were labeled w.r.t. their start\nand end times. The total duration of non-seizure recordings is\n40,800 seconds and seizure recordings is 2889 seconds.\nThe UPenn dataset contains 1-second long EEG recordings\nacquired intracranially at 500 −5000 Hz from a maximum of\nM = 72channels. The total duration of non-seizure recordings\nis 7164 seconds and seizure recordings is 653 seconds.\nThe TUH dataset contains EEG recordings acquired on the\nscalp with 250 Hz sampling rate from a maximum of M = 38\nchannels. 1229 seizure recordings were labeled w.r.t. their start\nand end times. The total duration of non-seizure recordings is\n49,922 seconds and seizure recordings is 2600 seconds.\nB. Preprocessing\nEEG recordings are typically preprocessed to eliminate the\npowerline noise at 60 Hz [40]. We ﬁrst unify the sampling rates\nin each dataset by downsampling to the smallest sampling rate\nacross all recordings. Then, we ﬁlter the recordings via a 4-th\norder Butterworth bandpass ﬁlter with range 0.5-50 Hz.\nTo construct samples with the same size, we extract sliding\nwindows over each recording, where each window contains\nT time points and overlaps with its consecutive window by\n50%. We choose T based on the shortest seizure segment\nin each dataset. In doing so, T = 1536 for MIT, T = 500\nfor UPenn, and T = 462 for TUH. This process results in\n13,600 windows with non-seizure activity and 963 windows\nwith seizure activity for MIT, 14,329 windows with non-\nseizure activity and 1307 windows with seizure activity for\nUPenn, and 54,264 windows with non-seizure activity and\n2826 windows with seizure activity for TUH. In real-life appli-\ncations, a minimum seizure window length can be decided by\nclinical experts, as in UPenn that directly provides 1 second-\nlong seizure recordings.\nMoreover, we aim to consistently formT×M size windows,\nwhile not disregarding any channels with potential seizure ac-\ntivity. Thus, to construct samples with the same number of M\nchannels, we reuse data from other channels for the recordings\nthat have missing data at certain channels, compared to the\nrecording with the largest number of channels in each dataset.\nAgain, in real-life applications, clinical experts can determine\nwhich channels to employ or discard for seizure identiﬁcation.\nFinally, we normalize windows by subtracting the mean and\ndividing by the standard deviation across all windows to aid\nthe convergence of training [22].\nDataset Method Precision Recall Accuracy AUC\nMIT Unsupervised Transformer 0.98 ± 0.003 0 .9 ± 0.006 0 .87 ± 0.006 0 .94 ± 0.023\nUnsupervised K-means 0.33 ± 0.008 0 .5 ± 0.009 0 .5 ± 0.009 0 .59 ± 0.041\nUnsupervised V AE 0.97 ± 0.003 0 .75 ± 0.008 0 .61 ± 0.009 0 .61 ± 0.041\nSupervised XGBoost 0.98 ± 0.003 0 .8 ± 0.007 0 .8 ± 0.007 0 .88 ± 0.031\nSupervised ROCKET 0.98 ± 0.003 0 .74 ± 0.008 0 .78 ± 0.008 0 .86 ± 0.032\nSupervised Transformer 0.98 ± 0.003 0 .83 ± 0.007 0 .83 ± 0.007 0 .88 ± 0.031\nPre-trained 50% Supervised Transformer 0.97 ± 0.003 0 .72 ± 0.008 0 .63 ± 0.009 0 .66 ± 0.021\nPre-trained 100% Supervised Transformer 0.99 ± 0.002 0 .98 ± 0.003 0 .94 ± 0.005 0 .97 ± 0.017\nUPenn Unsupervised Transformer 0.88 ± 0.01 0 .76 ± 0.013 0 .68 ± 0.014 0 .73 ± 0.027\nUnsupervised K-means 0.33 ± 0.014 0 .5 ± 0.015 0 .5 ± 0.015 0 .56 ± 0.028\nUnsupervised V AE 0.8 ± 0.012 0 .5 ± 0.015 0 .49 ± 0.015 0 .47 ± 0.027\nSupervised XGBoost 0.87 ± 0.01 0 .62 ± 0.015 0 .6 ± 0.015 0 .65 ± 0.028\nSupervised ROCKET 0.87 ± 0.01 0 .67 ± 0.014 0 .62 ± 0.015 0 .67 ± 0.028\nSupervised Transformer 0.87 ± 0.01 0 .69 ± 0.014 0 .62 ± 0.015 0 .64 ± 0.028\nPre-trained 50% Supervised Transformer 0.86 ± 0.011 0 .77 ± 0.013 0 .63 ± 0.015 0 .64 ± 0.032\nPre-trained 100% Supervised Transformer 0.92 ± 0.008 0 .85 ± 0.011 0 .82 ± 0.012 0 .89 ± 0.02\nTUH Unsupervised Transformer 0.92 ± 0.005 0 .57 ± 0.009 0 .61 ± 0.009 0 .57 ± 0.013\nUnsupervised K-means 0.17 ± 0.007 0 .5 ± 0.009 0 .35 ± 0.008 0 .57 ± 0.013\nUnsupervised VAE 0.93 ± 0.005 0 .86 ± 0.006 0 .83 ± 0.007 0 .86 ± 0.009\nSupervised XGBoost 0.93 ± 0.005 0 .73 ± 0.008 0 .71 ± 0.008 0 .78 ± 0.011\nSupervised ROCKET 0.93 ± 0.005 0 .7 ± 0.008 0 .66 ± 0.008 0 .74 ± 0.012\nSupervised Transformer 0.92 ± 0.005 0 .37 ± 0.009 0 .54 ± 0.009 0 .52 ± 0.012\nPre-trained 50% Supervised Transformer 0.94 ± 0.005 0 .61 ± 0.009 0 .75 ± 0.008 0 .71 ± 0.025\nPre-trained 100% Supervised Transformer 0.93 ± 0.005 0 .66 ± 0.008 0 .7 ± 0.008 0 .72 ± 0.012\nTABLE I: Seizure identiﬁcation performance metrics and conﬁdence intervals on UPenn, MIT and TUH. We compare our\ntransformer-based unsupervised identiﬁcation method (in bold) with unsupervised methods comprising V AE and t-SNE followed\nby K-means clustering, as well as supervised methods comprising XGBoost, ROCKET, and the same transformer architecture\ntrained via supervised and pre-trained supervised learning. Best performance for each dataset are in italics.\nC. Experiment Setup and Competing Methods\nWe partition all windows containing non-seizure and seizure\nactivity into training, validation, and test sets in a stratiﬁed\nmanner, allocating 60% for training, 20% for validation, and\nthe remaining 20% for testing. As baseline methods, we im-\nplement shallow and deep learning models for both supervised\nand unsupervised settings.\n1) Unsupervised Learning Methods: For our method, we\nemploy the transformer encoder architecture proposed by\nVaswani et al. (2017), with the modiﬁcations of fully-trainable\npositional encoding, batch normalization and the same hyper-\nparameters suggested by Zerveas et al. (2021). We train the\nautoencoder over only non-seizure training windows using the\nunsupervised loss given by Eq. (4). We monitor the loss value\ncomputed over the non-seizure windows in the validation set\nand use the model that attains the lowest validation loss.\nFollowing the literature on shallow unsupervised methods\n[8], we reduce the dimension of all EEG windows in the test\nset to 3 using the t-Distributed Stochastic Neighbor Embed-\nding (t-SNE) [37] algorithm, and apply K-means clustering\n[4] on the resulting windows with two clusters indicating\nnon-seizure and seizure. Moreover, as an unsupervised deep\nlearning baseline, we train a state-of-the-art convolutional V AE\n[41].\n2) Supervised Learning Methods: First, we employ the\nsame transformer encoder architecture described in Section\nIII-A and map the latent features learned from each window\nto a binary prediction. In doing so, we concatenate all latent\nfeatures corresponding to all time points of each window\ninto a single vector and apply a fully-connected layer com-\nprising a scalar output with sigmoid activation. We train the\nresulting architecture via cross-entropy loss over all training\nwindows, employing the same hyperparameters found optimal\nby Zerveas et al. (2021). To combat overﬁtting due to class\nimbalance in supervised learning, we oversample and augment\nthe seizure windows in training via random reversing and\ndrifting. We monitor the F1-score computed over the validation\nset and use the model that attains the best validation score.\nMoreover, we train state-of-the-art shallow models XGBoost\n[11] and ROCKET [12] over the supervised training set.\nXGBoost is a decision-tree classiﬁer using gradient boosting\nfor ensembling. ROCKET transforms time-series using 500\nrandom convolutional kernels and uses the extracted features\nto train a ridge regression classiﬁer. Ridge regression hyper-\n0.0 0.2 0.4 0.6 0.8 1.0\nTime (s)\n200000\n100000\n0\n100000\n200000\n300000\nV\nPatient_8_ictal_2869\n0.0 0.2 0.4 0.6 0.8 1.0\nTime (s)\n200000\n150000\n100000\n50000\n0\n50000\nPatient_8_ictal_2870\n(a) Correctly Identiﬁed Seizure Windows (True Positive)\n0.0 0.2 0.4 0.6 0.8 1.0\nTime (s)\n60\n40\n20\n0\n20\n40\nV\nPatient_2_interictal_1658\n0.0 0.2 0.4 0.6 0.8 1.0\nTime (s)\n20\n0\n20\n40\n60\nPatient_2_interictal_1566 (b) Correctly Identiﬁed Non-seizure Windows (True Negative)\n0.0 0.2 0.4 0.6 0.8 1.0\nTime (s)\n60\n40\n20\n0\n20\n40\nV\nPatient_2_ictal_3983\n0.0 0.2 0.4 0.6 0.8 1.0\nTime (s)\n40\n20\n0\n20\nPatient_2_ictal_3984\n(c) Falsely Identiﬁed Seizure Windows (False Negative)\n0.0 0.2 0.4 0.6 0.8 1.0\nTime (s)\n1000\n500\n0\n500\n1000\nV\nPatient_2_interictal_31\n0.0 0.2 0.4 0.6 0.8 1.0\nTime (s)\n1500\n1000\n500\n0\n500\n1000\n1500\n2000\n2500\nPatient_2_interictal_602 (d) Falsely Identiﬁed Non-seizure Windows (False Positive)\nFig. 2: Example EEG windows, corresponding seizure identiﬁcations and self-attention weights on UPenn. First and third rows\ncontain example windows of true positive, true negative, false negative, and false positive identiﬁcations, respectively. Second\nand fourth rows contain the corresponding self-attention weight heatmaps computed by the transformer architecture, where\ndarker colors indicate higher importance. For each window, we visualize the channel with the largest reconstruction error.\nparameter is varied in [10−3,103] and best hyperparameter is\ndetermined w.r.t. the accuracy over the validation set.\n3) Pre-trained Supervised Learning: Finally, we combine\nthe transformer-based seizure identiﬁcation methods via unsu-\npervised pre-training and supervised ﬁne-tuning [42]. Follow-\ning the unsupervised approach described in Section IV-C1, we\nﬁrst pre-train the transformer encoder over non-seizure training\nwindows. Having initialized its weights accordingly, we then\nﬁne-tune the model via both non-seizure and seizure training\nwindows, using the same setup described in Section IV-C2.\nD. Evaluation Metrics\nTo evaluate the seizure identiﬁcation performance of our\napproach, as well as the V AE baseline, we use the mean ab-\nsolute error over the time points and electrode channels in each\nEEG window from the test set as the corresponding seizure\nprediction score. For all supervised competing methods, we\nuse the traditional prediction score for inference.\nFor all competing methods described in Section IV-C, we\nreport AUC for distinguishing seizure vs. non-seizure windows\nin the test set. To compute binary decision metrics, we thresh-\nold the prediction score of each window at the value for which\nthe geometric mean of recall and true negative rate is maxi-\nmal [17]. Using the respective threshold, we calculate class-\nweighted precision and recall, as well as balanced accuracy\nfor binary identiﬁcation of seizure vs. non-seizure windows in\nthe test set, considering the imbalanced distribution between\nthe two. In real-life applications, decision thresholds may be\ndetermined by clinical experts with respect to the desired\ntrade-off between false positives and negatives [9].\nWe report all metrics along with the 95% conﬁdence in-\ntervals, which are computed as 1.96 ×σA, where σ2\nA is the\nvariance for metric A. Variance for AUC is computed by:\nσ2\nA= 1\nmn\n(\nA(1−A)+(m−1)(Px−A2)+(n−1)(Py−A2)\n)\n, (5)\nwhere Px = A/(2 −A), Py = 2A2/(1 +A), and m, n are\nthe number of seizure and non-seizure windows, respectively\n[19]. Variance for other metrics are computed by:\nσ2\nA = A(1 −A)/(m+ n). (6)\nE. Results and Discussion\n1) Seizure Identiﬁcation Performance: Table I shows the\nseizure identiﬁcation performance of our transformer-based\nunsupervised method vs. supervised and pre-trained super-\nvised transformers, XGBoost, ROCKET, V AE, and t-SNE\nfollowed by K-means clustering over all datasets. Our novel\ntransformer-based anomaly detection method establishes a\ndramatic improvement among all unsupervised methods, by\nsuccessfully distinguishing between non-seizure vs. seizure\nwindows with up to 0.94 AUC and outperforming its state-\nof-the-art deep learning counterpart V AE by up to 33% AUC\non MIT. Clustering on raw EEG windows cannot capture the\ncomplex evolution of EEG and predicts all windows as non-\nseizure. These observations demonstrate the beneﬁt of the\ntransformer architecture for unsupervised anomaly detection\nin our setting.\nCrucially, despite the lack of seizure labels during training,\nour unsupervised anomaly detection approach leads to signif-\nicantly better seizure identiﬁcation than all purely supervised\nlearning baselines and the pre-trained transformer ﬁne-tuned\nwith 50% of the training labels over UPenn and MIT, by\nup to 16% recall, 9% accuracy, and 9% AUC. Moreover,\nunlike supervised learning, class imbalance strongly biases\nsupervised models towards non-seizure predictions and hinders\ngeneralization over the distribution of held-out test samples.\nAs a result, unsupervised anomaly detection via transformers\nestablishes a consistently better balance between precision and\nrecall than supervised learning and further demonstrates its\nbeneﬁt in learning from imbalanced datasets such as ours.\nThe TUH dataset is particularly challenging by being a\ncompilation of several EEG databases collected over years\nfrom patients with vast variations in demographic and medical\nbackgrounds [27], compared to self-contained UPenn and MIT\ndatasets collected from only 8 and 24 patients, respectively. In\nthis case, our unsupervised transformer still fares signiﬁcantly\nbetter than the purely supervised transformer, while unsuper-\nvised V AE outperforms all supervised learning baselines, in-\ncluding the pre-trained transformer. These observations further\nmotivate unsupervised learning for our task.\nAs expected, the computationally expensive transformer\nmodel, which has ﬁrst undergone unsupervised pre-training\nand then supervised ﬁne-tuning with all training labels, outper-\nforms both purely supervised as well as purely unsupervised\ntransformer models (the latter by a smaller margin). However,\nour unsupervised anomaly detection method does not require\nground-truth seizure labels during training as a crucial advan-\ntage, while still leading to successful seizure identiﬁcation.\n2) Seizure Identiﬁcation Examples: We visualize example\nEEG windows from UPenn and the corresponding seizure\nidentiﬁcations of the unsupervised transformer in the ﬁrst and\nthird rows of Figure 2, selecting the channel with the largest\nmean reconstruction error for each window. Agreeing with\nthe clinical description of seizures, true seizure windows in\nFigure 2a contain high-frequency waves with large amplitudes\n[39]. Meanwhile, true non-seizure windows in Fig. 2b attain\nsigniﬁcantly less amplitude changes and spikes compared to\ntrue positive windows. Note that the seizure patterns cannot\nbe identiﬁed w.r.t. only large amplitude or high frequency,\nmotivating a more sophisticated approach such as ours. For\nDataset Method Precision Recall Accuracy AUC\nMIT Geometric (Ours) 0.98 0 .9 0 .87 0 .94\nBernoulli 0.98 0.85 0.85 0.9\nUPenn Geometric (Ours) 0.88 0 .76 0 .68 0 .73\nBernoulli 0.86 0.72 0.65 0.72\nTUH Geometric (Ours) 0.92 0.57 0 .61 0 .57\nBernoulli 0.93 0.4 0.59 0.54\nTABLE II: Effect of masking strategy on seizure identiﬁcation.\ninstance, non-seizure windows in Fig. 2d have a larger ampli-\ntude range than the seizure windows in Figure 2c, while the\nseizure windows in Fig. 2c contain similar spikes to the non-\nseizure windows in Figure 2b w.r.t. amplitude and frequency.\n3) Beneﬁt of Self-Attention: We visualize the self-attention\nweights computed by the last encoder layer of the unsuper-\nvised transformer on example EEG windows from UPenn as\n2D heatmaps in the second and fourth rows of Figure 2. For\neach time point along the horizontal axis of each heatmap, self-\nattention weights (c.f. Equation (3)) from other time points\nare indicated along the vertical axis. Darker heatmap colors\ncorrespond to larger weights and, thus, higher importance.\nIt appears that the transformer model within our unsu-\npervised identiﬁcation method can successfully learn to pay\nmore attention to seizure patterns including high-frequency\nspikes and waves evolving with large amplitudes [39]. More-\nover, when the model predicts the existence of seizures, it\nshows patterns of focused attention, containing only few time\npoints with large weights (Figures 2a and 2d), while windows\nidentiﬁed as non-seizure (Figures 2b and 2c) lead to much\nmore evenly distributed attention. These observations indicate\nthat employing a transformer architecture with self-attention\ncan improve both performance, as well as explainability of\nseizure identiﬁcation decisions, by underlining, e.g., spike-\nwave discharges that are indicative of seizures [39].\n4) Effect of Masking Strategy: Table II shows the seizure\nidentiﬁcation performance of training with our geometric\nmasking strategy against masking each time point indepen-\ndently at random with a Bernoulli distribution. Our approach\nof unsupervised training with geometric masking consistently\nleads to better performance than Bernoulli masking, demon-\nstrating its beneﬁt in modeling multivariate data such as EEG.\nV. C ONCLUSION\nWe propose a fully-unsupervised transformer-based method\nfor seizure identiﬁcation on raw EEG. Our method can suc-\ncessfully distinguish between non-seizure and seizure windows\nand can even achieve signiﬁcantly better seizure identiﬁca-\ntion performance than state-of-the-art supervised time-series\nmethods, including its purely supervised transformer-based\ncounterpart. Generalizing our method to other applications\ninvolving anomalous activity detection on multivariate time-\nseries data is a promising future direction.\nOur unsupervised approach can signiﬁcantly alleviate the\nburden on clinical experts regarding laborious and difﬁcult\nEEG inspections to provide labels indicating segments that\ncontain seizures. Furthermore, if automated identiﬁcation per-\nformance meets clinical requirements, our method can aid\navailability of seizure diagnoses for the wider public, espe-\ncially in areas where access to well-trained healthcare profes-\nsionals is limited.\nREFERENCES\n[1] Hafeez Ullah Amin, Mohd Zuki Yusoff, and Rana Fayyaz Ahmad. A\nnovel approach based on wavelet analysis and arithmetic coding for\nautomated detection and diagnosis of epileptic seizure in EEG signals\nusing machine learning techniques. Biomedical Signal Processing and\nControl, 56, 2020.\n[2] Sabrina Belhadj, Abdelouahab Attia, Ahmed Bachir Adnane, Zoubir\nAhmed-Foitih, and Abdelmalik Ahmed Taleb. Whole brain epileptic\nseizure detection using unsupervised classiﬁcation. In 2016 8th Inter-\nnational Conference on Modelling, Identiﬁcation and Control (ICMIC) ,\npages 977–982. IEEE, 2016.\n[3] Javad Birjandtalab, Maziyar Baran Pouyan, and Mehrdad Nourani.\nUnsupervised EEG analysis for automated epileptic seizure detection.\nIn First International Workshop on Pattern Recognition, volume 10011,\npage 100110M. International Society for Optics and Photonics, 2016.\n[4] Christopher M Bishop. Pattern recognition. Machine learning, 128(9),\n2006.\n[5] Poomipat Boonyakitanont, Apiwat Lek-Uthai, Krisnachai Chomtho, and\nJitkomut Songsiri. A review of feature extraction and performance\nevaluation in epileptic seizure detection using EEG. Biomedical Signal\nProcessing and Control, 57:101702, 2020.\n[6] Satarupa Chakrabarti, Aleena Swetapadma, and Prasant Kumar Pat-\ntnaik. A channel independent generalized seizure detection method\nfor pediatric epileptic seizures. Computer Methods and Programs in\nBiomedicine, 209, 2021.\n[7] Satarupa Chakrabarti, Aleena Swetapadma, Prasant Kumar Pattnaik, and\nTina Samajdar. Pediatric seizure prediction from EEG signals based on\nunsupervised learning techniques using various distance measures. In\n2017 1st International Conference on Electronics, Materials Engineer-\ning and Nano-Technology (IEMENTech), 2017.\n[8] Satarupa Chakrabarti, Aleena Swetapadma, Prasant Kumar Pattnaik, and\nTina Samajdar. Pediatric seizure prediction from EEG signals based on\nunsupervised learning techniques using various distance measures. In\n2017 1st International Conference on Electronics, Materials Engineer-\ning and Nano-Technology, pages 1–5, 2017.\n[9] Raghavendra Chalapathy and Sanjay Chawla. Deep learning for anomaly\ndetection: A survey. CoRR, abs/1901.03407, 2019.\n[10] Krit Charupanit, Indranil Sen-Gupta, Jack J Lin, and Beth A Lopour.\nDetection of anomalous high-frequency events in human intracranial\nEEG. Epilepsia Open, 5(2):263–273, 2020.\n[11] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting\nsystem. In Proceedings of the 22nd SIGKDD International Conference\non Knowledge Discovery and Data Mining , pages 785–794, 2016.\n[12] Angus Dempster, François Petitjean, and Geoffrey I Webb. ROCKET:\nExceptionally fast and accurate time series classiﬁcation using random\nconvolutional kernels. Data Mining and Knowledge Discovery , 34(5),\n2020.\n[13] Zhaohong Deng, Peng Xu, Lixiao Xie, Kup-Sze Choi, and Shitong\nWang. Transductive joint-knowledge-transfer TSK-FS for recognition\nof epileptic EEG signals. Transactions on Neural Systems and Rehabil-\nitation Engineering, 26(8):1481–1494, 2018.\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.\nBERT: Pre-training of deep bidirectional transformers for language\nunderstanding. In Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, 2019.\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani,\nMatthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929, 2020.\n[16] Emadeldeen Eldele, Mohamed Ragab, Zhenghua Chen, Min Wu,\nChee Keong Kwoh, Xiaoli Li, and Cuntai Guan. Time-series represen-\ntation learning via temporal and contextual contrasting. arXiv preprint\narXiv:2106.14112, 2021.\n[17] Tom Fawcett. An introduction to ROC analysis. Pattern Recognition\nLetters, 27(8):861–874, 2006.\n[18] Robert G Gallager. Stochastic Processes: Theory for Applications .\nCambridge University Press, 2013.\n[19] James A Hanley and Barbara J McNeil. The meaning and use of the\narea under a receiver operating characteristic (ROC) curve. Radiology,\n143(1):29–36, 1982.\n[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition , pages 770–\n778, 2016.\n[21] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochas-\ntic regularizers with gaussian error linear units. CoRR, abs/1606.08415,\n2016.\n[22] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating\ndeep network training by reducing internal covariate shift. In Interna-\ntional Conference on Machine Learning , pages 448–456. PMLR, 2015.\n[23] Nizar Islah, Jamie Koerner, Roman Genov, Tauﬁk A Valiante, and\nGerard O’Leary. Machine learning with imbalanced EEG datasets using\noutlier-based sampling. In 2020 42nd Annual International Conference\nof the IEEE Engineering in Medicine & Biology Society, pages 112–115.\nIEEE, 2020.\n[24] Demetres Kostas, Stéphane Aroca-Ouellette, and Frank Rudzicz.\nBENDR: Using transformers and a contrastive self-supervised learning\ntask to learn from massive amounts of EEG data. Frontiers in Human\nNeuroscience, 15:253, 2021.\n[25] Yang Li, Yu Liu, Wei-Gang Cui, Yu-Zhu Guo, Hui Huang, and Zhong-Yi\nHu. Epileptic seizure detection in EEG signals using a uniﬁed temporal-\nspectral squeeze-and-excitation network. IEEE Transactions on Neural\nSystems and Rehabilitation Engineering , 28(4):782–794, 2020.\n[26] Virender Kumar Mehla, Amit Singhal, Pushpendra Singh, and Ram Bilas\nPachori. An efﬁcient method for identiﬁcation of epileptic seizures from\nEEG signals using fourier analysis. Physical and Engineering Sciences\nin Medicine, 2021.\n[27] Iyad Obeid and Joseph Picone. The temple university hospital EEG data\ncorpus. Frontiers in Neuroscience, 10:196, 2016.\n[28] Moein Radman, Milad Moradi, Ali Chaibakhsh, Mojtaba Kordestani,\nand Mehrdad Saif. Multi-feature fusion approach for epileptic seizure\ndetection from EEG signals. IEEE Sensors Journal , 21(3):3533–3543,\n2020.\n[29] Ricardo Ramos-Aguilar, J Arturo Olvera-López, Ivan Olmos-Pineda, and\nSusana Sánchez-Urrieta. Feature extraction from EEG spectrograms for\nepileptic seizure detection. Pattern Recognition Letters , 133:202–209,\n2020.\n[30] Sari Saba-Sadiya, Eric Chantland, Tuka Alhanai, Taosheng Liu, and\nMohammad M Ghassemi. Unsupervised EEG artifact detection and\ncorrection. Frontiers in Digital Health , 2:57, 2021.\n[31] Ali Hossam Shoeb. Application of machine learning to epileptic seizure\nonset detection and treatment . PhD thesis, Massachusetts Institute of\nTechnology, 2009.\n[32] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever,\nand Ruslan Salakhutdinov. Dropout: A simple way to prevent neural\nnetworks from overﬁtting. Journal of Machine Learning Research ,\n15(56), 2014.\n[33] Richard J Staba, Matt Stead, and Gregory A Worrell. Electrophysiolog-\nical biomarkers of epilepsy. Neurotherapeutics, 11(2), 2014.\n[34] Leilei Sun, Bo Jin, Haoyu Yang, Jianing Tong, Chuanren Liu, and\nHui Xiong. Unsupervised EEG feature extraction based on echo state\nnetwork. Information Sciences, 475:1–17, 2019.\n[35] Roland D Thijs, Rainer Surges, Terence J O’Brien, and Josemir W\nSander. Epilepsy in adults. The Lancet, 393(10172):689–701, 2019.\n[36] UPenn and MayoClinic. Upenn and Mayo Clinic’s seizure detection\nchallenge, 2014.\n[37] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using\nt-SNE. Journal of Machine Learning Research , 9(11), 2008.\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. In Advances in Neural Information Processing Systems ,\npages 5998–6008, 2017.\n[39] Paul M Vespa, Vikesh Shrestha, Nicholas Abend, Denes Agoston, Alicia\nAu, Michael J Bell, Thomas P Bleck, Manuel Buitrago Blanco, Jan\nClaassen, Ramon Diaz-Arrastia, et al. The epilepsy bioinformatics study\nfor anti-epileptogenic therapy (EpiBioS4Rx) clinical biomarker: study\ndesign and protocol. Neurobiology of Disease , 123:110–114, 2019.\n[40] Sungmin You, Baek Hwan Cho, Soonhyun Yook, Joo Young Kim,\nYoung-Min Shon, Dae-Won Seo, and In Young Kim. Unsupervised au-\ntomatic seizure detection for focal-onset seizures recorded with behind-\nthe-ear EEG using an anomaly-detecting generative adversarial network.\nComputer Methods and Programs in Biomedicine , 193:105472, 2020.\n[41] ˙Ilkay Yıldız, Rachael Garner, Matthew Lai, and Dominique Duncan.\nUnsupervised seizure identiﬁcation on EEG. Computer Methods and\nPrograms in Biomedicine , 215:106604, 2022.\n[42] George Zerveas, Srideepika Jayaraman, Dhaval Patel, Anuradha\nBhamidipaty, and Carsten Eickhoff. A transformer-based framework\nfor multivariate time series representation learning. In Proceedings of\nthe 27th ACM SIGKDD Conference on Knowledge Discovery & Data\nMining, pages 2114–2124, 2021.\n[43] Tao Zhang and Wanzhong Chen. LMD based features for the automatic\nseizure detection of EEG signals using SVM. IEEE Transactions on\nNeural Systems and Rehabilitation Engineering , 25(8), 2016.\n[44] Xiang Zhang, Lina Yao, Manqing Dong, Zhe Liu, Yu Zhang, and Yong\nLi. Adversarial representation learning for robust patient-independent\nepileptic seizure detection. IEEE Journal of Biomedical and Health\nInformatics, 24(10):2852–2859, 2020.\n[45] Wei Zhao, Wenbing Zhao, Wenfeng Wang, Xiaolu Jiang, Xiaodong\nZhang, Yonghong Peng, Baocan Zhang, and Guokai Zhang. A novel\ndeep neural network for robust detection of seizures using EEG signals.\nComputational and Mathematical Methods in Medicine , 2020, 2020.\n[46] Bingzhao Zhu and Mahsa Shoaran. Unsupervised domain adaptation for\ncross-subject few-shot neurological symptom detection. In 2021 10th\nInternational IEEE/EMBS Conference on Neural Engineering (NER) ,\n2021.",
  "topic": "Electroencephalography",
  "concepts": [
    {
      "name": "Electroencephalography",
      "score": 0.7914945483207703
    },
    {
      "name": "Computer science",
      "score": 0.7355347871780396
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6559621095657349
    },
    {
      "name": "Epilepsy",
      "score": 0.5795423984527588
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5483695268630981
    },
    {
      "name": "Autoencoder",
      "score": 0.5292729139328003
    },
    {
      "name": "Identification (biology)",
      "score": 0.45771920680999756
    },
    {
      "name": "Epileptic seizure",
      "score": 0.4447677731513977
    },
    {
      "name": "Machine learning",
      "score": 0.4445193409919739
    },
    {
      "name": "Feature extraction",
      "score": 0.42205631732940674
    },
    {
      "name": "Speech recognition",
      "score": 0.37173181772232056
    },
    {
      "name": "Deep learning",
      "score": 0.2471165955066681
    },
    {
      "name": "Psychology",
      "score": 0.16643008589744568
    },
    {
      "name": "Neuroscience",
      "score": 0.08809453248977661
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210108346",
      "name": "BioSensics (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I27804330",
      "name": "Brown University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ],
  "cited_by": 12
}