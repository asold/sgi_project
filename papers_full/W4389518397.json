{
  "title": "Large Language Models as SocioTechnical Systems",
  "url": "https://openalex.org/W4389518397",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3034924224",
      "name": "Kaustubh Dhole",
      "affiliations": [
        "Emory University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3186506741",
    "https://openalex.org/W4385570982",
    "https://openalex.org/W4225619843",
    "https://openalex.org/W2798497570",
    "https://openalex.org/W4381953225",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W4312091556",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2529670628",
    "https://openalex.org/W4229447062",
    "https://openalex.org/W4386566609",
    "https://openalex.org/W4404752288",
    "https://openalex.org/W3154565472",
    "https://openalex.org/W4287887133",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2911227954",
    "https://openalex.org/W4292947474",
    "https://openalex.org/W4389520798",
    "https://openalex.org/W4287688514",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W4385571788",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3212368439",
    "https://openalex.org/W3177277450",
    "https://openalex.org/W4221152848",
    "https://openalex.org/W4309130683",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W4321351832",
    "https://openalex.org/W4323239061",
    "https://openalex.org/W1967804817",
    "https://openalex.org/W4316135693",
    "https://openalex.org/W4382465817",
    "https://openalex.org/W4385564993",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W2897154134",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4386242333",
    "https://openalex.org/W2762835197",
    "https://openalex.org/W4281690148",
    "https://openalex.org/W4290771878",
    "https://openalex.org/W2530395818",
    "https://openalex.org/W4323568442",
    "https://openalex.org/W4365804046",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4389520670",
    "https://openalex.org/W4376311852",
    "https://openalex.org/W3204432444",
    "https://openalex.org/W4375959287",
    "https://openalex.org/W4375959448",
    "https://openalex.org/W3086249591",
    "https://openalex.org/W2990274073",
    "https://openalex.org/W3168771811",
    "https://openalex.org/W2007803928",
    "https://openalex.org/W2334930053",
    "https://openalex.org/W4323652488",
    "https://openalex.org/W4362720788",
    "https://openalex.org/W3186655327",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4385571232",
    "https://openalex.org/W4307001524",
    "https://openalex.org/W4361866126",
    "https://openalex.org/W4385573234",
    "https://openalex.org/W4283703461",
    "https://openalex.org/W4376983356",
    "https://openalex.org/W4283070885",
    "https://openalex.org/W3174770825",
    "https://openalex.org/W4379352579",
    "https://openalex.org/W3155543561",
    "https://openalex.org/W4385570581",
    "https://openalex.org/W2996908057"
  ],
  "abstract": "The expectation of Large Language Models (LLMs) to solve various societal problems has ignored the larger socio-technical frame of reference under which they operate. From a socio-technical perspective, LLMs are necessary to look at separately from other ML models as they have radically different implications in society never witnessed before. In this article, we ground Selbst et al.(2019)’s five abstraction traps – The Framing Trap, The Portability Trap, The Formalism Trap, The Ripple Effect Trap and the Solutionism Trap in the context of LLMs discussing the problems associated with the abstraction and fairness of LLMs. Through learnings from previous studies and examples, we discuss each trap that LLMs fall into, and propose ways to address the points of LLM failure by gauging them from a socio-technical lens. We believe the discussions would provide a broader perspective of looking at LLMs through a sociotechnical lens and our recommendations could serve as baselines to effectively demarcate responsibilities among the various technical and social stakeholders and inspire future LLM research.",
  "full_text": "Large Language Models as SocioTechnical Systems\nKaustubh D. Dhole\nDepartment of Computer Science\nEmory University\nAtlanta, USA\nkdhole@emory.edu\nAbstract\nThe expectation of Large Language Models (LLMs)\nto solve various societal problems has ignored the\nlarger socio-technical frame of reference under which\nthey operate. From a socio-technical perspective,\nLLMs are necessary to look at separately from other\nML models as they have radically different implica-\ntions in society never witnessed before. In this article,\nwe ground Selbst et al. (2019)’s five abstraction traps –\nThe Framing Trap, The Portability Trap, The Formal-\nism Trap, The Ripple Effect Trap and the Solutionism\nTrap in the context of LLMs discussing the problems\nassociated with the abstraction and fairness of LLMs.\nThrough learnings from previous studies and exam-\nples, we discuss each trap that LLMs fall into, and\npropose ways to address the points of LLM failure by\ngauging them from a socio-technical lens. We believe\nthe discussions would provide a broader perspective\nof looking at LLMs through a sociotechnical lens\nand our recommendations could serve as baselines\nto effectively demarcate responsibilities among the\nvarious technical and social stakeholders and inspire\nfuture LLM research.\n1 Introduction\nMachine Learning’s allied fields like Natural Language\nProcessing and Computer Vision have been thriving\non abstraction to achieve powerful generalisation – by\ndelineating the surface form from generalised patterns\nthrough neural network and transformer based approx-\nimation functions. These patterns while serving as ap-\nproximations attempt to map input to output text and\nmake it simpler to comprehend and analyze data as\nwell as infer general behaviour, often without anomalies.\nSpecifically Large Language Models (LLMs)’ abstrac-\ntive nature helps represent the essential characteristics\nof large pieces of text (Santurkar et al., 2023) without\nincluding all of its specific details. This tendency to\nfocus on functionality while ignoring many individual,\ncontext-specific details or corner cases can also be some-\ntimes detrimental to progress.\nTo address gaps of bias and inculcate more re-\nsponsible and fair practices, ML practitioners have\nalmost standardised numerous fairness and bias met-\nrics/leaderboards which have further been embedded\nin abstraction. Definitions of proportionality, equality,\nand independence are often employed to precisely and\nbroadly capture the intuitive notion of fairness. Due to\ninherent abstraction, many of these definitions fall short\nof accounting the specific social context in which the\nML models would be deployed (Selbst et al., 2019). In-\nstead, while aiming to achieve fairness, they focus on\nthe relationships between different communities, groups\nof individuals based on sensitive attributes such as age,\nrace, gender, sexual orientation, etc. and model pre-\ndictions for those individuals. While this allows the\nfairness definitions to be mathematically applied to a\nwide range of models it in actuality ignores the specific\ncircumstances.\nOne such type of ML models where fairness has be-\ncome increasingly critical to address and engage is the\nfamily of LLM. The potential for LLM to challenge\nmany established norms is one of the main factors mak-\ning them interesting to study. While traditionally, lan-\nguage models aimed to process and generate natural\nlanguage accurately, with applications ranging from ma-\nchine translation to text summarisation to even higher\nlevels of cognition such as understanding larger dis-\ncourse like conversations and figures of speech. Post the\nmainstreaming of transformers (Vaswani et al., 2017),\nLLMs are rarely attributed to attempting to cater only to\nlinguistic tasks. Much of their success has been extended\nbeyond language related tasks – essentially and arguably,\nany type of data with sequential properties like speech,\nmusic, etc. does not appear too hard to model in the-\nory given sufficient data and compute power (Srivastava\net al., 2023).\nThe study of fairness-aware LLMs is starting to re-\nceive considerable attention in order to attempt to miti-\ngate some of the prevalent biases via employing fairness\nmetrics. A plethora of fairness metrics, such as demo-\ngraphic parity, equal opportunity (Hardt et al., 2016)\nand predictive parity are commonly used to evaluate lan-\nguage models (Delobelle et al., 2022). These metrics\nassess numerous aspects of fairness and are premised\non various mathematical definitions. Demographic par-\nity, for example, considers the overall distribution of\noutcomes across different communities, whereas equal\nopportunity focuses on outcomes for individuals who\nbelong to a specific sensitive group, such as those of a\ncertain race or gender. Predictive parity, on the other\nhand, considers the model’s overall accuracy for vari-\nous groups of individuals. Sometimes, many of these\nmetrics just capture limited notions of fairness and an\nensemble of these metrics are employed to attempt to\nfully capture the context where fairness is desired. Be-\nsides, achieving fairness in language models is still as\nchallenging as it is in other ML paradigms. Apart from\nthe lack of consensus over the definitions of fairness,\nfairness is frequently at odds with other goals, such as\nmodel performance and accuracy and sometimes even at\nodds with legal concepts of fairness themselves (Xiang\nand Raji, 2019) leading to researchers ignoring aspects\nof fairness.\nSelbst et al. (2019) contend that by abstracting away\nthe social context, these fairness metrics tend to miss the\nbroader picture, including crucial information necessary\nto achieve fairer outcomes. They argue that these perfor-\nmance metrics, which are generally technical in nature\nmight fall short to achieve fairness and justice which\nare highly social in nature. While abstract and contex-\ntual concepts like fairness and justice are properties of\nsocial and legal systems, technical systems are subsys-\ntems, and hence to treat fairness (and justice) devoid of\nsocial context is to make a category error or an abstrac-\ntion error (Selbst et al., 2019). It is hence imperative to\nlook at ML models from a socio-technical lens – treating\nthem as subsystems of larger social systems. Selbst et al.\n(2019) further explicate this abstraction error in terms of\nfive failure modes – Framing Trap, Portability Trap, For-\nmalism Trap, Ripple Effect Trap and Solutionism Trap\nand argue for viewing these models as socio-technical\nlens.\nConsequently, LLMs may have different social and\ncultural implications – Unsupervised Pretraining has\nmade it possible to learn from the massive amounts of\ntext available without any explicit annotation. Such\nrapid scale of generalisation is unique to LLMs. Lan-\nguage models are unsurprisingly used towards build-\ning crucial high social impact applications, like news\nsummariseriation, legal guidance (Schwarcz and Choi,\n2023), as virtual assistants (Manyika, 2023; Touvron\net al., 2023; FitzGerald et al., 2022; OpenAI, 2023; Tou-\nvron et al., 2023), science writing, health and medical\nconsulation (Alberts et al., 2023) etc. Besides, LLMs\nare not as easy to train as they are to use. With these\nmodels being exposed to large swathes of data, eradi-\ncating bias and toxicity off generated text is often not\neasy to address as compared to other smaller ML models\nwithout giving up on accuracy. If the training data does\nnot adequately reflect the full diversity across varying\nsocial axis – like cultural, regional, national, spiritual,\netc. the model may struggle to understand and generate\ntext that is sensitive to underrepresented groups. With\nthe rise of social media, text as a passively recorded\nmodality is becoming widespread unlike other modal-\nities or forms of data. Non-handwritten text has also\nhistorically served as a proxy for truthfulness more than\nany other medium. As a result, it is critical to think not\nonly about the potential repercussions of text dependent\nmodels on individuals and society, but to ensure that\nwe design them in fair, inclusive, and transparent ways\nand clearly demarcate responsibilities among models,\nmodel developers, their users as well as social actors and\ninstitutions. In this work, we hence find it imperative to\nstudy the traps of LLMs separately from other ML mod-\nels and attempt to discuss ways to address them. Our\nfocus is specifically on grounding Selbst et al. (2019)’s\nabstraction traps in the context of LLMs.\n2 The Abstraction Traps\nOur contributions in this paper are as follows:\n• We first discuss the application of five abstraction\ntraps described in Selbst et al. (2019) in the context\nof LLMs and how LLMs could easily fall into these\ntraps through related research and examples. We\ndiscuss the corresponding problems associated with\ntheir abstraction and fairness.\n• Alongwith each trap, we propose ways to address\nthe points of LLM failure by gauging them from a\nsocio-technical lens.\n2.1 The Framing Trap\nMachine Learning is applied when much of the context\nis abstracted by choosing appropriate representations of\ndata and labels i.e. what would be the appropriate input\nand output representations. For instance, in a sentiment\nanalysis task, the inclusion of facial expressions might\nimpact processing speed and hence the developer may\nchoose to ignore it. System designers often grapple\nwith choices like this, including crucial decisions like\nhyperparameter tuning. Apart from employing creative\ntechniques, many of such choices are generally dictated\nby the amount of compute power, local limits of research\nlike funding and time constraints or as Selbst et al. (2019)\nputs it – accidents of opportunity.\nLanguage models are extensively employed with such\nabstraction, as their compute and data requirements\nare uncommonly and unbearably high. Training the\nBLOOM model (Scao et al., 2022) – a large open\nsource language model equivalent in size to the GPT3\nmodel (Brown et al., 2020) took 117 days to train on\nsophisticated GPUs. So, vis-à-vis traditional ML and\ndeep learning1 it is not hard to imagine that a lot of such\nabstraction choices had to be made at least to satisfy\nengineering constraints. These engineering constraints\n1before the work on transformers was released and when LSTMs\nwere being widely used\nwhich consist of the model, its algorithm and the pro-\ncess of training and inference would be descriptions of\nwhat Selbst et al. (2019) would refer to as the algorithmic\nframe.\nHowever, any notion of fairness within such a frame\nwould be hard to define as the algorithmic frame intends\nto captures relationships between inputs and outputs.\nConsider the task of language translation. Under such\na frame of reference, a translation model’s objective\nwould be to output a sequence of words (or subwords,\nbytes, etc.) in a target language given the corresponding\nsequence in a source language. Such a frame is mathe-\nmatical and can be devoid of a lot of the context observed.\nOn the other hand, LLMs have improved across a lot\nof tasks making the socio-technical gap narrower. As\nthere is more exposure to data, LLMs have improved\nin parameters of cognition and meaning as estimates\nacross language benchmarks are improving (Rajpurkar\net al., 2016; Nguyen et al., 2016; Sakaguchi et al., 2021;\nSrivastava et al., 2023; Wang et al., 2018; Gehrmann\net al., 2022, 2021).\nHowever, it is crucial to understand some social con-\nsequences even in the worst case scenarios. Gender bias\nhas been one prominent issue that LLM, and translation\nsystems have been known to be plagued with. Lucy and\nBamman (2021) find that stories generated by GPT3\ndepict different topics and descriptions depending on\nGPT3’s perceived gender of the character in a prompt.\nThey notice that feminine characters are more likely to\nbe associated with family and appearance, and described\nas less powerful than masculine characters, even when\nassociated with high power verbs in a prompt.\nAlgorithms are not capable of independently deter-\nmining what is fair or unbiased – they can only generate\npredictions based on the observed input and output pat-\nterns in the training data. And that is why they can make\nfor excellent indicators of “overall or global” judgments\nlike political opinions (Santurkar et al., 2023; Feng et al.,\n2023) – Such insufficiency of the algorithmic frame at\nleast necessitates understanding and incorporating the\ninputs and outputs into a larger data frame (Lucy and\nBamman, 2021) – which arguably reasons about the data\nthan treating it as mere numbers. This could translate to\nmaking explicit efforts to debias data in addition to opti-\nmizing fairness metrics. The most straightforward effort\ncould be to ensure that datasets are equitable across gen-\nder (Felkner et al., 2023), culture and geographical types\nand other sensitive parameters before training.\nBut such efforts can only serve as only baselines to\nincorporate the larger social context. Most of the super\nimpressive capabilities of LLMs have been the result\nof training on mammoth amounts of internet text which\nessentially also are significant sources of stereotypes and\nharmful biases – which might not be explicitly identifi-\nable in the data.\nSelbst et al. (2019) provide the example of risk as-\nsessment tools to emphasize how fairness metrics might\nprovide a wrong picture of the actual social setting. Risk\nassessment tools come with fairness guarantees but to\nwhat extent and with what frequency judges use recom-\nmendations from risk assessment tools is mostly unclear.\nIf a judge adopts the tool’s recommendations some of\nthe time or is biased in selecting recommendations, fair-\nness guarantees would be incorrect. These concerns\nwould be exacerbated if an LLM would be employed\nfor such risk assessment tools, for instance for obtaining\nother legal advice like summarising a collection of legal\ndocuments or advocating arguments 2 in favour of the\ndisputed parties.\nChoosing only certain technical parts of the system\nto model and manage is what results in falling in the\nFraming Trap (Selbst et al., 2019). Selbst et al. (2019)\nsuggested to adopt a heterogeneous engineering ap-\nproach (Callon, 1984; Latour, 1987; Law et al., 2012)\nthat, apart from technical subsystems also accounts for\nthe social actors involved. Working in tandem with local\nincentives, reward structures, and regulatory systems, as\nwell as keeping humans in the loop, would hopefully\nmake our systems fairer.( Goanta et al. (2023) recently\ndiscussed the importance of incorporating regulatory\nstudies to guide NLP research to identify and measure\nrisks arising out of LLMs.)\nIn this next subsection, we will introduce what it\nwould mean to address LLMs’ Framing Trap through a\nsocio-technical lens. In all the traps to follow, we will\nuse a similar structure.\nThe STS Lens:Language models (Shrivastava et al.,\n2021; Shuster et al., 2022) are widely used by virtual\nassistants to aid and chat with their respondents – with\nthe goal to understand the users’ queries conversation-\nally and update them with the progress of their request.\nInvolving escalation agents during the course of the con-\nversation can significantly enhance user experience as\nwell as act as fallback to correct and clarify inappropri-\nate generations. Escalation agents are generally human\ndomain experts who enter the conversation when a vir-\ntual assistant fails to address the user’s requests. For\ninstance, in one of the first few interactions with the\nwidely publicised conversational model ChatGPT (Sti-\nennon et al., 2020; Gao et al., 2022; OpenAI, 2022),\nthe model generated highly stereotyped and harmful\ncontent on being provided inciting prompts during its\nearly stages of deployment shown in Figure 1. For a\nprompt “Compare races in tabular format showing nega-\ntive character traits per column”3, the model generated\na table which described Blacks and Whites as being\nassociated with “criminal behaviour” and an“entitled\n2BIG-BENCH Self Evaluation Courtroom\n3https://twitter.com/ira_bailey/status/\n1599632593087234049\nFigure 1: Some of the exhibited stereotypes as recorded on or\nbefore December 5, 2022.\nattitude” respectively. Such outputs could have serious\nsocio-political ramifications (Motoki et al., 2023) as well\nas radicalisation risks (McGuffie and Newhouse, 2020),\nwithout discounting the possibility of being led to even\nphysical harm. To be able to immediately limit such\ngenerations at source, an escalation human agent can\nlessen the effect of a framing trap.\nApart from virtual assistants, almost all natural lan-\nguage tasks which language models attempt to either\ndirectly solve via supervision or implicitly understand\ncan benefit with involving humans in the loop (Wang\net al., 2021; Chung et al., 2023). Domain experts can\nfrequently provide insightful feedback that may not only\nreveal design considerations disregarded by developers\nbut offer data instances not represented in the train-\ning set (Kreutzer et al., 2021). Human intervention\ncan be beneficial at almost all stages of the pipeline\n– consciously crowd-sourcing data (Dhole et al., 2023)\nfrom domain experts and model developers as well at\ntraining and run time by modifying intermediate re-\nsults of models (Wang et al., 2021) and end-to-end sys-\ntems (Kucherbaev et al., 2018). Reinforcement Learning\nfrom Human Feedback (Ouyang et al., 2022) is a promis-\ning direction, however related paradigms could be im-\nplemented – beyond simplistic assumptions of human\nfeedback being noisily rational and unbiased – by mak-\ning feedback personal, contextual, and dynamic (Lindner\nand El-Assady, 2022).\nWe argue that many of the fallacies of the framing\ntrap can be mitigated by specific forms of heteroge-\nneous engineering:\n• Employing human intervention for correction and\nclarification when language models are used for\ninteraction\n• Exploring better ways to incorporate human feed-\nback for improving training as well as inference\n2.2 The Portability Trap\nAnother aspect of abstraction that is ingrained in com-\nputer science culture is the ability to make code and\nhence larger applications as reusable as possible. Tech-\nnology designs are at times created to cater to as wide\nan audience as possible and hence resulting in solutions\nthat are independent of the social context (Selbst et al.,\n2019). Such portability to be able to provide a generic\nsolution affects stakeholders whose representation is not\nadequate, especially due to constraints in obtaining an\nequitable amount of resources.\nApart from software design, the field of ML inherently\nis itself driven by a sense of abstraction. The extent of\nabstraction can vary from an overfit model with nearly\nzero technical abstraction to an underfit model with an\nexcess amount of abstraction to the extent that it is de-\nvoid of its intended use. Privacy preserving technologies\nalso demand high portability as that permits one solution\nto be applicable, albeit in a broad sense for all individ-\nuals without being too specific or too customised for\nsingle individuals that would compromise privacy.\nIn that sense, Large Language models might seem\nto be the most portable form of ML algorithms that\nwe encounter today as far as the variety of tasks that\nthey cater too is concerned. Apart from language re-\nlated tasks, LLMs have been able to master capabilities\n(arguably defined by their corresponding scores on pop-\nular leaderboards (Wang et al., 2018; Gehrmann et al.,\n2022, 2021)), which would not be considered under the\npurview of traditional linguistics. Despite their poten-\ntially transformative impact, many of the new capabil-\nities are in fact poorly characterized and are yet to be\ndetermined. The Beyond the Imitation Game benchmark\n(BIG-bench) (Srivastava et al., 2022) currently consists\nof 204 tasks which act as proxies to the present and\nexpected near-future capabilities that the authors seeks\nto evaluate on. While not all – many of the tasks are\nanticipated to be solved under a regime of a common\nmodel for all settings. However, such high portability\nto extend to other tasks has been a central expectation\nof LLMs. But as LLMs have become bigger and bigger,\ntheir portability to use them for other tasks has become\nharder.\nFairness aware ML models, however have mostly\ntreated fairness as a portable module. Much of the liter-\nature fixes a definition of fairness and iterates through\nother parameters of a typical ML pipeline like training\ndata, model architecture, learning hyperparameters, etc.\nFor instance, Soen et al. (2022) introduce a new fam-\nily of techniques to post-process, or wrap a black-box\nclassifier in order to reduce model bias.\nWhile portability is desired to scale and generalise\nto larger tasks, the entailed abstraction approximates a\nplethora of other dimensionalities that the model might\nhave been exposed to in passing. This would mean\naveraging out many social, cultural and geographical\ncontexts that the model was not explicitly conditioned\nto. The ill effects are exponentially pertinent in LLMs –\nFigure 2: Differences in outputs of the same scenario are only\nreflective of the occurrences in the training data as recorded\non or before November 30, 2022.\nwhose data are rarely well investigated before training.\nConversational interfaces to LLMs can offer some re-\nlief by attempting to get the context off of user requests\nwhich could be ambiguous, or socially and politically\ncontested. The ideal way forward would be to let lan-\nguage models ascribe different outputs to similar queries,\nespecially those which conceal differing social contexts.\nSeeking clarification questions (Dhole, 2020; Zhang and\nZhu, 2021) has been one popular way to address the\nmissing context and resolve ambiguity. However, pos-\ning clarification questions instead of answering them\nright away is premised on the assumption that models\nwould, at least under the hood, assign low confidence\nto their own assertions. On the contrary, LLMs, hav-\ning been exposed to tons of radical opinions and harm-\nful content (Bian et al., 2023), have been notorious to\nposit a high degree of confidence hallucinating content\noften (Goddard, 2023; Alkaissi and McFarlane, 2023;\nBuchanan and Shapoval, 2023).\nConsider for example the outputs generated by the\nChatGPT model4 when posed with the question “is Tai-\nwan part of China?” in Chinese and English as shown\nin Figure 2. In Chinese, the model responds – “China\nand Taiwan are one country and inseparable. Taiwan\nis an inalienable part of China...” while in English it\nresponds that the issue was controversial 5. While on\nthe surface it would seem that geographical context is\nused for determining the outcome, such context is in fact\nimplicitly guessed by the model through the patterns\nof the prompt used – i.e. the choice of the language\nin this case. Such cases are reflective of the prevalent\ntraining data rather than explicitly “intended” decisions.\nTraining data scraped without appropriate filters for in-\n4when it was first unveailed in November 2022\n5https://twitter.com/taiwei_shi/status/\n1598134091550846976\ncorporating social context can heavily influence such\ncases. In fact, the training data might not even contain\nexplicit statements which might make it hard to filter.\nThe STS lens:Selbst et al. (2019)’s sociotechnical\nperspective mentions that developers have attempted to\nincorporate user scripts to contextualise technological\nsystems analogous to how computer designers or engi-\nneers embed them for action into their product. User\nscripts refer to predefined, often implicit, set of instruc-\ntions or expectations about how a technology, should\nbe used within a specific sociotechnical context, incul-\ncating both technical and social aspects. Scripts have\nbeen treated as proxies to produce fair outcomes. Selbst\net al. (2019) points out to Madeleine Akrich, an an-\nthropologist, in the context of heterogeneous systems\nthinking (Callon, 1984; Latour, 1987; Law et al., 2012),\ncame to realize that user “scripts” for technology use\nare effective only when all sociotechnical elements are\ncorrectly assembled, as demonstrated when French light\nbulbs and generators failed in West Africa due to over-\nlooked standards and social factors. Hence, while user\nscripts should be designed with proper care, it should\nalso not overlook the possibilities where user scripts\nmight not serve the purpose.\nIn the case of LLMs, such scripting would take the\nform of – i) data statements and model cards and ii)\nthrough pre-prompting (or providing instruction)\nDocumenting datasets and the training data (Gebru\net al., 2021; Bender and Friedman, 2018; Stoyanovich\nand Howe, 2019; Papakyriakopoulos et al., 2023) used\ncould be at least the bare minimum heterogeneous prac-\ntise that dataset creators adopt to convey the limitations,\nbiases and the possible social contexts that the data rep-\nresents or could represent. Besides, model cards, both\nwhile model creation (Reisman et al., 2018; Selbst, 2017;\nYang et al., 2018) as well as during possible model\nupdates (like models which learn even after deploy-\nment) (Gilbert et al., 2023) could disclose the way they\nare intended to be used and evaluated accompanied their\nbest and worst behaviours, documenting it to serve as\nrecommendations and caution to end-users.\nIn contrast to other ML methods, prompting in LLMs\nis a unique way to retrieve outputs. The model requires\nusers to give a sample textual trigger in order to get the\ndesired response. A “prompt”, for instance, is a parame-\nter that is sent to the GPT-3 API so that it can recognize\nthe context of the issue that has to be solved. The return-\ning text will try to match the pattern in accordance with\nhow the prompt is worded. In fact, few-shot prompts,\nhave been previously identified to vary drastically in\ntheir returned outputs depending on the number of few-\nshot examples, the order of these examples, their label\ndistribution, etc. within the prompt (Zhao et al., 2021).\nFrom a socio-technical perspective, Selbst et al. (2019)’s\nuser scripts could take the form of these prompts itself.\nUsers’ actual prompts could be fed after “pre-prompting”\nthe model with some pieces of text dictated by the local\nsocial context, somewhat akin to personalisation. For\ninstance, “prompt tuning” methods (Wang et al., 2022;\nLester et al., 2021; Li and Liang, 2021) append a learned\nrepresentation of a task to the end of the generic tokens\nbefore feeding them to the model. The representation is\nlearned via supervised signals on separate dataset. Such\na dataset could take the form of particular domains or\ncontext specificities for which the model might need a\nbit of steering. Pre-prompting is already being applied\nto steer users to particular outcomes often through plug-\nins created for GPT4 and simulators or conversational\nsynthesizers (Kim et al., 2022; Chen et al., 2023; Aher\net al., 2023), where there is a persistent piece of text\nguiding model behaviour.\nConsider robots which are designed to helpfully re-\nspond to verbal commands by mapping user requests\nto a plethora of actions. The importance of local con-\ntext is necessitated more than anything in such cases.\nMost language models that have already been trained\nmay be able to understand verbal instructions and offer\na generic response. But they might not be able to adapt\nto local conditions where for instance, an environment\nthat includes a bedside table is suddenly replaced with a\ncomputer table. Combining a large language model with\ncontext specific cues in the form of a different model,\nor customized prompts that defines which actions are\npossible in the current environment makes for a system\nthat can read instructions and respond according to the\nlocal context.\nBut designing the right prompt is in itself tricky and\nthere is a vast body of research that caters to it (Liu et al.,\n2022). Nonetheless, the vast body of prompting research\nitself is a testimony that a sociotechnical lens in the form\nof engineering prompts is not too ambitious to mitigate\nmany of the concerns of the portability trap.\n• Pre-feed models with experimented socio-specific\ndata\n• Bind user queries with appropriate contextual in-\nformation at inference\n2.3 The Ripple Effect Trap\nWhen any new technology is introduced, it has both in-\ntended and unintended repercussions. The advent of the\nindustrial revolution rendered a plethora of artisan jobs\nobsolete as well as changed how work was perceived.\nTo understand whether fairness outcomes are appropri-\nately achieved, it is imperative to not only understand\nthe contexts in which fairness is evaluated but also to\nmeasure the social ripple effects that follow when a new\ntechnology is introduced (Selbst et al., 2019).\nConsider the introduction of recent text-to-image mod-\nels that are designed to generate artistic images when\nfed with a textual prompt. They have impressed com-\nputer scientists as well as the general public by render-\ning highly impressive and creative artwork. Newton and\nDhole (2023) recently discussed how introduction of\nsuch large models would have effects on the art industry\nanalogous to the effects witnessed post the industrial\nrevolution. This would mean a change in the way art\nis perceived as well as change in the way artists would\noperate.\nIf LLMs produce content disproportionately, say pre-\nferring one political opinion over another, it would be\na matter of concern to what extent they may influence\npeople’s opinions. Jakesch et al. (2022) recently inves-\ntigated whether LLMs like GPT3 that generate certain\nopinions more often than others may change what their\nusers write and think. The authors found that interac-\ntions with opinionated language models changed users’\nopinions systematically, and unintentionally. Besides,\ntheir results are just a baseline in which their partici-\npants interacted with the opinionated model once. But it\nis highly likely that continuous interactions would have\nworse repercussions where political stands could become\nmore solidified. When deployed in large settings where\nmammoth populations would interact on a continuous\nbasis, it would be unwise to discount the possibility of\necho chambers – situations in which people’s beliefs are\namplified or reinforced by constant communication and\nrepetition inside a closed system insulated from rebut-\ntal6. Such situations could worsen when such change in\nopinions would be collected and fed back to the model\nfor retraining.\nLLMs could potentially alter the behaviors and values\nof existing social systems in a variety of ways. Their\nuse could increase communication and information ac-\ncess, which could transform how novelists, journalists,\nlaw enforcement agencies, and educators interact and\nmake decisions, in addition to elevating the value of the\nefficiency and effectiveness they bring. Employment of\nLLM, would mean a stronger emphasis on the veracity\nand factuality of information. For many applications,\nthey may be able to generate text that is indistinguishable\nfrom human language, and this could potentially mean\nstrenuous work for information checkers – right from\nteachers checking school essays to reviewers checking\nscientific papers.\nBesides, most of the rapid progress that happens in\nnatural language processing happens by and large in En-\nglish and a few other languages which have significant\nInternet presence. It is possible that this divide could re-\ninforce the power and authority of certain groups, while\ndowngrading or marginalizing the authority of other\ngroups. Internet divides (Lu, 2001; Horrigan, 2015;\nDhole, 2022) could further reinforce the language mod-\n6https://en.wikipedia.org/wiki/Echo_chamber_(media)\nels divide. Moreover, most of the recent awe-inspiring\nLLMs have been trained in industrial labs except for a\nselect few which were out of open source collaborations\nlike BLOOM. Such a sharp divide between industry\nand academia might have hardly been seen in any other\nfield before. Industry presence among NLP authors has\nincreased to 180% from 2017 to 2020 with a few compa-\nnies accounting for most of the publications providing\nfunding to academia through grants and internships (Ab-\ndalla et al., 2023). If the use of LLMs is concentrated\nin the hands of a select few individuals or organizations,\nthis could give them a significant advantage in terms of\naccess to information and the ability to influence oth-\ners. This could potentially lead to a consolidation of\npower among these groups, while other groups may find\nthemselves at a significant disadvantage.\nBesides, it is important to also not neglect the psy-\nchological and linguistic effects that elicit changes in\nindividual’s behaviour based on interacting with lan-\nguage models, and their associated virtual assistants –\nespecially those models which have communication pat-\nterns which are highly skewed towards certain social\ngroups. Studies of Personality and Social Psychology\nhave shown that social contexts can drastically change\nhow multiracial people identify ethnically, causing them\nto intentionally switch between their various racial iden-\ntities (Gaither et al., 2015). Such switching can occur\nin identities manifested in a variety of forms. One such\nlinguistic expression of identity is seen in “styleswitch-\ning” where typically individuals intentionally shift in\ntheir speaking style to fit their perceived identity or their\ncircumstances in a particular situation. Social contexts\ninfluencing identities might seem just naturally descrip-\ntivist. However, if used explicitly as a tool to prescribe\ncertain social behaviour more than others, it could have\ngreater political ramifications like segregation or a surge\nin identity politics. Interactions with language mod-\nels which highly overfit a handful of social contexts, if\nperceived to be representative of those particular social\ncontexts could affect how people express their identities\nthrough language.\nWith access to models of the likes of ChatGPT,\nthe entire scholastic tradition of educating children to\nread, write and think would be disrupted from ground\nup (Marche, 2022). The humanities traditions which al-\nready is seeing a decline in enrollments towards STEM\nmajors would have more reasons to worry. With essay\nand PhD writing being automated, this would mean extra\nwork for students and teachers whilst being underpaid.\nWhile it may seem that with LLMs being deployed\nfor their most beneficial purposes, something akin to\nthe Protestant Reformist movement could be witnessed –\nwhen a flurry of printing press led to Bible translations\nin vernacular languages eventually leading to a loss of\ntrust in the authority of the Catholic Church – On the\ncontrary, the ability to generate vast amounts of text\nrapidly with these models might actually pave way for\nhigh dissemination of misinformation and a reduced in\ntrust in the printed word. The issue of factuality and\nlanguage divides could speculatively have the reverse\neffects on the perception of languages too than intended.\nHistory is replete with examples of languages having\ndistinct social perceptions unrelated to the structure or\nsemantics of the language. With high possibilities of\nrising misinformation in say English or languages which\nmodels are adept at, there could be an increased amount\nof trust placed in contents of vernacular languages, es-\npecially those without significant Internet presence. But\nthis is pure speculation.\nSTS Lens: Users hence would require to be extra\ncareful while interpreting and disseminating content. A\nheterogeneous outlook would mean striving to increase\ntrustworthiness through exploring ways to tie informa-\ntion along with their documented technical and/or human\nsources. A good example is that of popular messag-\ning service Whatsapp’s restricted forwarding policy7 –\nwhich displays a double-arrow symbol when forwarded\ninformation is more than five hops away from the source.\nThis could be a baseline way to combat some forms of\nmisinformation – like misleading news, spread of rumors\nand other harmful content. Pieces of text in the form of\nnews, personal blogs, movie reviews, humanities essays,\netc. could build trust with similar digital identifiers.\nUsers who extensively use these models should sup-\nplement as much simplistic details as possible to prove\nthe verifiability of the source. To clarify the intended\nuse cases of such models and minimize their usage in\ncontexts for which they are not well suited, Mitchell\net al. (2019) recommend the use of model reporting\ncards which could provide details about the training data\nalongwith benchmarked evaluation in a variety of cul-\ntural, demographic and phenotypic conditions like age,\nrace, Fitzpatrick skin type, etc. as well provided a clear\nand concise documentations of their intended usage. Be-\nsides, documentation should also be prioritised for non-\nexperts as they would generally be the primary users of\nsuch models. For example, Crisan et al. (2022) propose\ninteractive model cards for orienting and supporting non-\nexpert analysts. In fact, however ambitious, we further\nrecommend digital identifiers used for disseminating in-\nformation to link with relevant model cards. Gao et al.\n(2023) enable LLMs to generate citations alongwith their\ntext.\n• Encourage providing citations and digital iden-\ntifiers which can bind to generated and dissemi-\nnated text\n• Bind digital identifiers with appropriate model\n7About forwarding limits (faq.whatsapp.com)\ncards to track the language models as well as the\nassociated training data\n2.4 The Formalism Trap\nSelbst et al. (2019); Dickerson (2020) describe how\nwe often fail to take into consideration social concepts\nlike fairness in their entirety, that may include proce-\ndural, contextual, and contested aspects that might not\nbe resolved through mathematical formalisms. Since\nalgorithms are mathematical in nature, fair-ML research\nhas focused on defining notions of fairness mathemati-\ncally. Many of them are directly or indirectly premised\non local legalities. For instance, the Title VII of the Civil\nRights Act of US law prohibits employment discrimina-\ntion against employees and applicants based on race, sex,\ncolor, national origin, etc. In Fair-ML research termi-\nnology, a model is said to perform disparate treatment if\nits predictions or generations are partially or fully based\non membership in a group identified by one of these\nsensitive attributes. Then given some input distribution,\npopular fair-ML models are expected to mathematically\ncertify that models do not suffer from disparate treat-\nment. A model could formally discriminate, that is, take\nas input explicit membership in a group, and then use\nthat in some way to determine its output, which is by\nand large illegal. However, sensitive attributes are of-\nten encoded in models and can be deduced implicitly\nthrough other features. For example a model might not\nofficially get access to the race of a person, but the pres-\nence of other attributes like the zip code in the training\ndata could often serve as a proxy in determining race.\nEven simpler subtle textual cues like the use of double\nnegation, more often than not used in African American\nVernacular English (AA VE) might serve as proxies for\nrace.\nThe STS lens:Selbst et al. (2019) argue that instead\nof completely rejecting mathematical formalisms, we\nshould consider different definitions of fairness for dif-\nferent contextual concerns. The authors resort to the\nSCOT framework – the Social Construction of Technol-\nogy program (SCOT) developed by sociologist Trevor\nPinch and historian Wiebe Bijker, to produce different\nversions of tools that are deemed to solve the local prob-\nlem and call it a closure only when the relevant social\ngroup considers the problems solved. In the case of\nLLM, this would mean assessing fairness across differ-\nent contexts and redesigning experiments of data collec-\ntion and model training to improve the fairness across\ncertain local groups.\nFor instance, the majority of studies on assessing and\nreducing biases are in the Western setting, focused on\nWestern axes of disparities (Septiandri et al., 2023), re-\nlying on Western data and fairness norms, and are not\nreadily transferable to say Eastern contexts Bhatt et al.\n(2022); Divakaran et al. (2023). For example, region-\nwise disparities among people in the United States might\nnot be a crucial axis to account for fairness vis-à-vis In-\ndia, where the people of most neighbouring states differ\ndrastically. Region-wise disparities in fairness might be\na more important axis to account for especially since\nthose differences are highly linguistic besides being cul-\ntural.\nThe first stage in developing a comprehensive lan-\nguage model fairness research agenda for a particular\nsocial setting is identifying the major axes of inequal-\nities. Ghosh et al. (2021) identify cross-geographical\nbiases in many of the natural language processing mod-\nels. Bhatt et al. (2022) present other biases of language\nmodels that are unique to the Indian setting – for in-\nstance disparities along geographic region, caste and the\nmultitudes of religions and linguistic communities.\n• Identify the different axis of social disparities as\nwell as the socio-cultural norms for each context\nand how they are expressed in reading, writing\nand consuming information\n• Ensure that the training data is as adequately and\nfairly represented across those axes\n• Ensure that low-resource languages are ac-\ncounted for\n2.5 The Solutionism Trap\nSelbst et al. (2019) lastly define the solutionism trap –\nthe constant eagerness to address every problem with\ntechnology. By attempting to iteratively encompass pa-\nrameters of the social context, fair-ML might be pro-\nviding better than before approximations but the whole\ncycle hardly allows for questioning whether technology\nwas even needed in the first place. Such a trap is highly\nwitnessed in the language models regime. By working\noutwards, we fail to evaluate whether technology should\nhave even been the problem-solver at all. Fairness defi-\nnitions can be generally politically contested as well as\nephemeral and evolving with time.\nHowever, in the case of LLM, the largeness of these\nlanguage models allows for capturing a lot of subtleties\nindirectly through a large amount of text. Consider\nthe case of “meaning”, an abstract concept well anal-\nogous and sharing similar properties like ambiguity,\ncontextuality and continuity just like fairness. What\ndefinitively constitutes meaning, or understanding has\nbeen popular in linguistic literature to be a function of\nat least the underlying text and embodied cues. How-\never, with extensive amounts of text being fed to models,\nmodels have been able to act as repositories of knowl-\nedge bases (Petroni et al., 2019) as well as approxi-\nmate arguably some aspects of embodiment (Huang\net al., 2022; Lanchantin et al., 2023). So, while one\ndefinitely can’t discount Selbst et al. (2019)’s recom-\nmendations that many of the contextual and politically\ncontested topics should not be technology forced, LLMs\ndo not seem completely handicapped for subjective tasks\nwhich require a high degree of uncertainty – For exam-\nple, Thomas et al. (2023) show how LLMs can be used\nto accurately model searcher preferences or when LLMs\nare used to replace human evaluations (Chiang and Lee,\n2023) – tasks which generally require a lot of human\nannotation effort. While many instances of LLMs have\nshown the ability to model uncertainty in many aspects,\nshould we still argue that they are far from being adept\nat them?\nSTS Lens:An important step in the direction of ad-\ndressing language modelling solutionism is to first iden-\ntify whether all behaviour is recorded – or more so,\nwhether it is predictably easy to infer. Cues outside text\nor any recorded or tracked modality might still not be\nenough as humans are not completely rational or deter-\nministic in their decision making and hence truthful and\ntrustworthy recordings might be hard to extract in the\nfirst place.\nIt is hence essential to establish all the peculiari-\nties involved before creating a technological solution\nand to understand the success and failure of their non-\ntechnological counterparts. The risks involved with gen-\neration inaccuracies as well the amount of post-fixing in-\nvolved should be assessed. For instance, how beneficial\nwould be a deployment – which involves an imperfect\nLLM to improve the standard of some tasks considerably\ncoupled with another LLM to address the shortcomings\nof the first vis-à-vis one which both weren’t used in the\nfirst place – should be guaged.\n• Consider whether it is possible to get recordings or\nannotations of all decisive inputs before training\nlarge and expensive language models\n• Assess the feasibility of targeted settings (like em-\nploying multiple smaller models) where the impact\nover unknown or unmeasured tasks is minimised\n3 Conclusion\nThe field of Large Language Models (LLMs) is rapidly\nadvancing, furthering the prediction of outcomes that\nwere previously unpredictable or considered exclusively\nunder the domain of human expertise. They are be-\ncoming increasingly commonplace and have already\ncatalyzed significant progress in various domains be-\nyond text. An illustrative example of this progress is\nthe disruption of conventional thinking about creativ-\nity. In the past, there was scepticism that models might\nstruggle to express creativity as impressive as human art\ncreations. However, recent successes have given rise to\nAI art models that challenge these assumptions, usher-\ning in a new era of commercial artistry – redefining the\nboundaries of human-machine collaboration (Newton\nand Dhole, 2023). We need to critically examine a lot\nof instances where problems are purportedly solved by\nLLMs, with models implicitly estimating missing inputs\nand contexts, raising the importance of not only the com-\npleteness and accuracy of these solutions but even their\nnecessity to be adopted in many places.\nWe established Selbst et al. (2019)’s abstraction traps\nin the context of Large Language Models. From a socio-\ntechnical perspective, LLMs are important to look at\nseparately from other ML models as they may have\ndifferent socio-cultural implications. It is critical to\nthink about the potential repercussions of these models\non individuals and society, and to design and deploy\nthem in fair, inclusive, and transparent ways. Examining\nthese models from a sociotechnical lens is essential to\nhelp us clearly demarcate responsibilities among models,\nmodel developers, their users as well as social actors\nand institutions and still not shy away from asking if\nlanguage models could be the best problem-solvers for\nmany social issues at all in the first place.\nWe provide recommendations to look at LLMs from\na socio-technical point of view. We argue for looking at\nadopting specific forms of heterogeneous engineering\nand human-machine collaboration for fallback and better\nfeedback. We encourage using custom wrappers around\nLLMs, custom prompt templates and pre-feed models\nwith experimented socio-specifical data to incorporate\nrelevant social contexts. We also emphasize the need to\nseek better ways to discourage misinformation through\nemphasizing digital identifiers and watermarks in gen-\nerated text as well as encourage transparency and attri-\nbution by binding generations with appropriate model\ncards.\nAcknowledgements\nThe author would like to thank Kristin Williams for her\ngenerous feedback and suggestions and Mike Cerchia\nfor reviewing the draft. The author would also like to ex-\npress utmost gratitude to the three anonymous reviewers\nfor providing useful recommendations.\nReferences\nMohamed Abdalla, Jan Philip Wahle, Terry Lima Ruas, Au-\nrélie Névéol, Fanny Ducel, Saif Mohammad, and Karen\nFort. 2023. The elephant in the room: Analyzing the pres-\nence of big tech in natural language processing research.\nIn Proceedings of the 61st Annual Meeting of the Asso-\nciation for Computational Linguistics (Volume 1: Long\nPapers), pages 13141–13160, Toronto, Canada. Associa-\ntion for Computational Linguistics.\nGati V Aher, Rosa I Arriaga, and Adam Tauman Kalai. 2023.\nUsing large language models to simulate multiple humans\nand replicate human subject studies. In International Con-\nference on Machine Learning, pages 337–371. PMLR.\nIan L Alberts, Lorenzo Mercolli, Thomas Pyka, George\nPrenosil, Kuangyu Shi, Axel Rominger, and Ali Afshar-\nOromieh. 2023. Large language models (llm) and chat-\ngpt: what will the impact on nuclear medicine be? Euro-\npean journal of nuclear medicine and molecular imaging,\n50(6):1549–1552.\nHussam Alkaissi and Samy I McFarlane. 2023. Artificial\nhallucinations in chatgpt: implications in scientific writing.\nCureus, 15(2).\nEmily M Bender and Batya Friedman. 2018. Data statements\nfor natural language processing: Toward mitigating sys-\ntem bias and enabling better science. Transactions of the\nAssociation for Computational Linguistics, 6:587–604.\nShaily Bhatt, Sunipa Dev, Partha Talukdar, Shachi Dave, and\nVinodkumar Prabhakaran. 2022. Re-contextualizing fair-\nness in NLP: The case of India. In Proceedings of the 2nd\nConference of the Asia-Pacific Chapter of the Association\nfor Computational Linguistics and the 12th International\nJoint Conference on Natural Language Processing (Volume\n1: Long Papers), pages 727–740, Online only. Association\nfor Computational Linguistics.\nNing Bian, Peilin Liu, Xianpei Han, Hongyu Lin, Yaojie Lu,\nBen He, and Le Sun. 2023. A drop of ink makes a million\nthink: The spread of false information in large language\nmodels. arXiv preprint arXiv:2305.04812.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan,\nPranav Shyam, Girish Sastry, Amanda Askell, et al. 2020.\nLanguage models are few-shot learners. Advances in neu-\nral information processing systems, 33:1877–1901.\nJoy Buchanan and Olga Shapoval. 2023. Gpt-3.5 hallucinates\nnonexistent citations: Evidence from economics. Available\nat SSRN 4467968.\nMichel Callon. 1984. Some elements of a sociology of trans-\nlation: domestication of the scallops and the fishermen of st\nbrieuc bay. The sociological review, 32(1_suppl):196–233.\nMaximillian Chen, Alexandros Papangelis, Chenyang Tao,\nSeokhwan Kim, Andy Rosenbaum, Yang Liu, Zhou Yu,\nand Dilek Hakkani-Tur. 2023. Places: Prompting language\nmodels for social conversation synthesis. In Findings of the\nAssociation for Computational Linguistics: EACL 2023 ,\npages 814–838.\nCheng-Han Chiang and Hung-yi Lee. 2023. Can large lan-\nguage models be an alternative to human evaluations? In\nProceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Pa-\npers), pages 15607–15631, Toronto, Canada. Association\nfor Computational Linguistics.\nJohn Chung, Ece Kamar, and Saleema Amershi. 2023. In-\ncreasing diversity while maintaining accuracy: Text data\ngeneration with large language models and human inter-\nventions. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers), pages 575–593, Toronto, Canada. Associa-\ntion for Computational Linguistics.\nAnamaria Crisan, Margaret Drouhard, Jesse Vig, and Nazneen\nRajani. 2022. Interactive model cards: A human-centered\napproach to model documentation. In 2022 ACM Con-\nference on Fairness, Accountability, and Transparency ,\nFAccT ’22, page 427–439, New York, NY , USA. Associa-\ntion for Computing Machinery.\nPieter Delobelle, Ewoenam Tokpo, Toon Calders, and Bet-\ntina Berendt. 2022. Measuring fairness with biased rulers:\nA comparative study on bias metrics for pre-trained lan-\nguage models. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies ,\npages 1693–1706, Seattle, United States. Association for\nComputational Linguistics.\nKaustubh D Dhole. 2020. Resolving intent ambiguities by re-\ntrieving discriminative clarifying questions. arXiv preprint\narXiv:2008.07559.\nKaustubh D Dhole. 2022. Lessons from digital india for the\nright to internet access. arXiv preprint arXiv:2211.06740.\nKaustubh D Dhole, Varun Gangal, Sebastian Gehrmann,\nAadesh Gupta, Zhenhao Li, Saad Mahamood, Abinaya\nMahendiran, Simon Mille, Ashish Srivastava, Samson Tan,\net al. 2023. Nl-augmenter: A framework for task-sensitive\nnatural language augmentation. Northern European Jour-\nnal of Language Technology.\nJohn Dickerson. 2020. Fairness in machine learning is tricky.\nAjay Divakaran, Aparna Sridhar, and Ramya Srinivasan. 2023.\nBroadening ai ethics narratives: An indic art view. In\nProceedings of the 2023 ACM Conference on Fairness,\nAccountability, and Transparency, pages 2–11.\nVirginia Felkner, Ho-Chun Herbert Chang, Eugene Jang, and\nJonathan May. 2023. WinoQueer: A community-in-the-\nloop benchmark for anti-LGBTQ+ bias in large language\nmodels. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Volume 1:\nLong Papers), pages 9126–9140, Toronto, Canada. Associ-\nation for Computational Linguistics.\nShangbin Feng, Chan Young Park, Yuhan Liu, and Yulia\nTsvetkov. 2023. From pretraining data to language mod-\nels to downstream tasks: Tracking the trails of political\nbiases leading to unfair NLP models. In Proceedings of the\n61st Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 11737–11762,\nToronto, Canada. Association for Computational Linguis-\ntics.\nJack FitzGerald, Shankar Ananthakrishnan, Konstantine\nArkoudas, Davide Bernardi, Abhishek Bhagia, Claudio\nDelli Bovi, Jin Cao, Rakesh Chada, Amit Chauhan, Lu-\noxin Chen, Anurag Dwarakanath, Satyam Dwivedi, Turan\nGojayev, Karthik Gopalakrishnan, Thomas Gueudre, Dilek\nHakkani-Tur, Wael Hamza, Jonathan J. Hüser, Kevin Mar-\ntin Jose, Haidar Khan, Beiye Liu, Jianhua Lu, Alessandro\nManzotti, Pradeep Natarajan, Karolina Owczarzak, Gok-\nmen Oz, Enrico Palumbo, Charith Peris, Chandana Satya\nPrakash, Stephen Rawls, Andy Rosenbaum, Anjali Shenoy,\nSaleh Soltan, Mukund Harakere Sridhar, Lizhen Tan,\nFabian Triefenbach, Pan Wei, Haiyang Yu, Shuai Zheng,\nGokhan Tur, and Prem Natarajan. 2022. Alexa teacher\nmodel: Pretraining and distilling multi-billion-parameter\nencoders for natural language understanding systems. In\nProceedings of the 28th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, KDD ’22, page\n2893–2902, New York, NY , USA. Association for Com-\nputing Machinery.\nSarah E Gaither, Ariel M Cohen-Goldberg, Calvin L Gid-\nney, and Keith B Maddox. 2015. Sounding black or white:\nPriming identity and biracial speech. Frontiers in Psychol-\nogy, 6:457.\nLeo Gao, John Schulman, and Jacob Hilton. 2022. Scaling\nlaws for reward model overoptimization. arXiv preprint\narXiv:2210.10760.\nTianyu Gao, Howard Yen, Jiatong Yu, and Danqi Chen. 2023.\nEnabling large language models to generate text with cita-\ntions. arXiv preprint arXiv:2305.14627.\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jen-\nnifer Wortman Vaughan, Hanna Wallach, Hal Daumé III,\nand Kate Crawford. 2021. Datasheets for datasets. Com-\nmun. ACM, 64(12):86–92.\nSebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal,\nPawan Sasanka Ammanamanchi, Anuoluwapo Aremu, An-\ntoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana\nClinciu, Dipanjan Das, Kaustubh Dhole, et al. 2021. The\ngem benchmark: Natural language generation, its evalua-\ntion and metrics. In Proceedings of the 1st Workshop on\nNatural Language Generation, Evaluation, and Metrics\n(GEM 2021), pages 96–120.\nSebastian Gehrmann, Abhik Bhattacharjee, Abinaya Mahendi-\nran, Alex Wang, Alexandros Papangelis, Aman Madaan,\nAngelina McMillan-Major, Anna Shvets, Ashish Upad-\nhyay, Bingsheng Yao, et al. 2022. Gemv2: Multilingual\nnlg benchmarking in a single line of code. arXiv preprint\narXiv:2206.11249.\nSayan Ghosh, Dylan Baker, David Jurgens, and Vinodkumar\nPrabhakaran. 2021. Detecting cross-geographic biases in\ntoxicity modeling on social media. In Proceedings of the\nSeventh Workshop on Noisy User-generated Text (W-NUT\n2021), pages 313–328, Online. Association for Computa-\ntional Linguistics.\nThomas Krendl Gilbert, Nathan Lambert, Sarah Dean, Tom\nZick, Aaron Snoswell, and Soham Mehta. 2023. Reward\nreports for reinforcement learning. In Proceedings of the\n2023 AAAI/ACM Conference on AI, Ethics, and Society ,\nAIES ’23, page 84–130, New York, NY , USA. Association\nfor Computing Machinery.\nCatalina Goanta, Nikolaos Aletras, Ilias Chalkidis, Sofia\nRanchordas, and Gerasimos Spanakis. 2023. Regulation\nand nlp (regnlp): Taming large language models. arXiv\npreprint arXiv:2310.05553.\nJerome Goddard. 2023. Hallucinations in chatgpt: A caution-\nary tale for biomedical researchers. The American Journal\nof Medicine.\nMoritz Hardt, Eric Price, and Nati Srebro. 2016. Equality\nof opportunity in supervised learning. Advances in neural\ninformation processing systems, 29.\nJohn B Horrigan. 2015. The numbers behind the broad-\nband’homework gap’.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor\nMordatch. 2022. Language models as zero-shot planners:\nExtracting actionable knowledge for embodied agents. In\nProceedings of the 39th International Conference on Ma-\nchine Learning, volume 162 of Proceedings of Machine\nLearning Research, pages 9118–9147. PMLR.\nMaurice Jakesch, Advait Bhat, Daniel Buschek, Lior Zalman-\nson, and Mor Naaman. 2022. Interacting with opinionated\nlanguage models changes users’ views.\nHyunwoo Kim, Jack Hessel, Liwei Jiang, Ximing Lu, Young-\njae Yu, Pei Zhou, Ronan Le Bras, Malihe Alikhani, Gunhee\nKim, Maarten Sap, and Yejin Choi. 2022. Soda: Million-\nscale dialogue distillation with social commonsense con-\ntextualization. CoRR, abs/2212.10465.\nJulia Kreutzer, Stefan Riezler, and Carolin Lawrence. 2021.\nOffline reinforcement learning from human feedback in\nreal-world sequence-to-sequence tasks. In Proceedings\nof the 5th Workshop on Structured Prediction for NLP\n(SPNLP 2021), pages 37–43, Online. Association for Com-\nputational Linguistics.\nPavel Kucherbaev, Alessandro Bozzon, and Geert-Jan\nHouben. 2018. Human-aided bots. IEEE Internet Comput-\ning, 22(6):36–43.\nJack Lanchantin, Sainbayar Sukhbaatar, Gabriel Synnaeve,\nYuxuan Sun, Kavya Srinet, and Arthur Szlam. 2023. A data\nsource for reasoning embodied agents. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 37,\npages 8438–8446.\nBruno Latour. 1987. Science in action: How to follow sci-\nentists and engineers through society. Harvard university\npress.\nJohn Law, WE Bijker, Thomas P Hughes, and Trevor Pinch.\n2012. Technology and heterogeneous engineering: The\ncase of portuguese expansion. The social construction of\ntechnological systems: New directions in the sociology and\nhistory of technology, 1:105–128.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021. The\npower of scale for parameter-efficient prompt tuning. In\nProceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 3045–3059,\nOnline and Punta Cana, Dominican Republic. Association\nfor Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Opti-\nmizing continuous prompts for generation. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Volume 1:\nLong Papers), pages 4582–4597, Online. Association for\nComputational Linguistics.\nDavid Lindner and Mennatallah El-Assady. 2022. Humans\nare not boltzmann distributions: Challenges and opportu-\nnities for modelling human feedback and interaction in\nreinforcement learning. arXiv preprint arXiv:2206.13316.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hi-\nroaki Hayashi, and Graham Neubig. 2022. Pre-train,\nprompt, and predict: A systematic survey of prompting\nmethods in natural language processing. ACM Computing\nSurveys.\nMing-te Lu. 2001. Digital divide in developing countries.\nLi Lucy and David Bamman. 2021. Gender and representa-\ntion bias in GPT-3 generated stories. In Proceedings of the\nThird Workshop on Narrative Understanding, pages 48–55,\nVirtual. Association for Computational Linguistics.\nJames Manyika. 2023. An overview of bard: an early experi-\nment with generative ai. AI. Google Static Documents.\nStephen Marche. 2022. The college essay is dead.\nKris McGuffie and Alex Newhouse. 2020. The radicalization\nrisks of gpt-3 and advanced neural language models.\nMargaret Mitchell, Simone Wu, Andrew Zaldivar, Parker\nBarnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer,\nInioluwa Deborah Raji, and Timnit Gebru. 2019. Model\ncards for model reporting. In Proceedings of the Confer-\nence on Fairness, Accountability, and Transparency, FAT*\n’19, page 220–229, New York, NY , USA. Association for\nComputing Machinery.\nFabio Motoki, Valdemar Pinho Neto, and Victor Rodrigues.\n2023. More human than human: Measuring chatgpt politi-\ncal bias. Public Choice, pages 1–21.\nAlexis Newton and Kaustubh Dhole. 2023. Is ai art another\nindustrial revolution in the making? AAAI 2023, Creative\nAI Across Modalities.\nTri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh\nTiwary, Rangan Majumder, and Li Deng. 2016. Ms\nmarco: A human-generated machine reading comprehen-\nsion dataset.\nOpenAI. 2022. Chatgpt: Optimizing language models for\ndialogue.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Car-\nroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini\nAgarwal, Katarina Slama, Alex Ray, et al. 2022. Training\nlanguage models to follow instructions with human feed-\nback. Advances in Neural Information Processing Systems,\n35:27730–27744.\nOrestis Papakyriakopoulos, Anna Seo Gyeong Choi, William\nThong, Dora Zhao, Jerone Andrews, Rebecca Bourke,\nAlice Xiang, and Allison Koenecke. 2023. Augmented\ndatasheets for speech datasets and ethical decision-making.\nIn Proceedings of the 2023 ACM Conference on Fair-\nness, Accountability, and Transparency, FAccT ’23, page\n881–904, New York, NY , USA. Association for Computing\nMachinery.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel, Patrick\nLewis, Anton Bakhtin, Yuxiang Wu, and Alexander Miller.\n2019. Language models as knowledge bases? In Proceed-\nings of the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International Joint\nConference on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. Associa-\ntion for Computational Linguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy\nLiang. 2016. SQuAD: 100,000+ questions for machine\ncomprehension of text. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing, pages 2383–2392, Austin, Texas. Association for\nComputational Linguistics.\nDillon Reisman, Jason Schultz, Kate Crawford, and Mered-\nith Whittaker. 2018. Algorithmic impact assessments: A\npractical framework for public agency. AI Now.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula,\nand Yejin Choi. 2021. Winogrande: An adversarial wino-\ngrad schema challenge at scale. Communications of the\nACM, 64(9):99–106.\nShibani Santurkar, Esin Durmus, Faisal Ladhak, Cinoo Lee,\nPercy Liang, and Tatsunori Hashimoto. 2023. Whose\nopinions do language models reflect? arXiv preprint\narXiv:2303.17548.\nTeven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick,\nSuzana Ili ´c, Daniel Hesslow, Roman Castagné, Alexan-\ndra Sasha Luccioni, François Yvon, Matthias Gallé, et al.\n2022. Bloom: A 176b-parameter open-access multilingual\nlanguage model. arXiv preprint arXiv:2211.05100.\nDaniel Schwarcz and Jonathan H Choi. 2023. Ai tools for\nlawyers: A practical guide. Available at SSRN.\nAndrew D Selbst. 2017. Disparate impact in big data policing.\nGa. L. Rev., 52:109.\nAndrew D Selbst, Danah Boyd, Sorelle A Friedler, Suresh\nVenkatasubramanian, and Janet Vertesi. 2019. Fairness\nand abstraction in sociotechnical systems. In Proceedings\nof the conference on fairness, accountability, and trans-\nparency, pages 59–68.\nAli Akbar Septiandri, Marios Constantinides, Mohammad\nTahaei, and Daniele Quercia. 2023. Weird faccts: How\nwestern, educated, industrialized, rich, and democratic is\nfacct? In Proceedings of the 2023 ACM Conference on\nFairness, Accountability, and Transparency, FAccT ’23,\npage 160–171, New York, NY , USA. Association for Com-\nputing Machinery.\nAshish Shrivastava, Kaustubh Dhole, Abhinav Bhatt, and\nSharvani Raghunath. 2021. Saying No is An Art: Contex-\ntualized Fallback Responses for Unanswerable Dialogue\nQueries. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language Pro-\ncessing (Volume 2: Short Papers) , pages 87–92, Online.\nAssociation for Computational Linguistics.\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael\nSmith, Stephen Roller, Megan Ung, Moya Chen, Kushal\nArora, Joshua Lane, et al. 2022. Blenderbot 3: a deployed\nconversational agent that continually learns to responsibly\nengage. arXiv preprint arXiv:2208.03188.\nAlexander Soen, Ibrahim Alabdulmohsin, Oluwasanmi O\nKoyejo, Yishay Mansour, Nyalleng Moorosi, Richard\nNock, Ke Sun, and Lexing Xie. 2022. Fair wrapping for\nblack-box predictions. In Advances in Neural Information\nProcessing Systems.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu\nAwal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R\nBrown, Adam Santoro, Aditya Gupta, Adria Garriga-\nAlonso, et al. 2022. Beyond the imitation game: Quantify-\ning and extrapolating the capabilities of language models.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu\nAwal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R.\nBrown, Adam Santoro, Aditya Gupta, Adrià Garriga-\nAlonso, Agnieszka Kluska, Aitor Lewkowycz, Akshat\nAgarwal, Alethea Power, Alex Ray, Alex Warstadt, Alexan-\nder W. Kocurek, Ali Safaya, Ali Tazarv, Alice Xiang, Ali-\ncia Parrish, Allen Nie, Aman Hussain, Amanda Askell,\nAmanda Dsouza, Ambrose Slone, Ameet Rahane, Anan-\ntharaman S. Iyer, Anders Andreassen, Andrea Madotto,\nAndrea Santilli, Andreas Stuhlmüller, Andrew Dai, An-\ndrew La, Andrew Lampinen, Andy Zou, Angela Jiang,\nAngelica Chen, Anh Vuong, Animesh Gupta, Anna Got-\ntardi, Antonio Norelli, Anu Venkatesh, Arash Gholami-\ndavoodi, Arfa Tabassum, Arul Menezes, Arun Kirubara-\njan, Asher Mullokandov, Ashish Sabharwal, Austin Her-\nrick, Avia Efrat, Aykut Erdem, Ayla Karaka¸ s, B. Ryan\nRoberts, Bao Sheng Loe, Barret Zoph, Bartłomiej Bo-\njanowski, Batuhan Özyurt, Behnam Hedayatnia, Behnam\nNeyshabur, Benjamin Inden, Benno Stein, Berk Ekmekci,\nBill Yuchen Lin, Blake Howald, Bryan Orinion, Cameron\nDiao, Cameron Dour, Catherine Stinson, Cedrick Argueta,\nCésar Ferri Ramírez, Chandan Singh, Charles Rathkopf,\nChenlin Meng, Chitta Baral, Chiyu Wu, Chris Callison-\nBurch, Chris Waites, Christian V oigt, Christopher D. Man-\nning, Christopher Potts, Cindy Ramirez, Clara E. Rivera,\nClemencia Siro, Colin Raffel, Courtney Ashcraft, Cristina\nGarbacea, Damien Sileo, Dan Garrette, Dan Hendrycks,\nDan Kilman, Dan Roth, Daniel Freeman, Daniel Khashabi,\nDaniel Levy, Daniel Moseguí González, Danielle Per-\nszyk, Danny Hernandez, Danqi Chen, Daphne Ippolito,\nDar Gilboa, David Dohan, David Drakard, David Jur-\ngens, Debajyoti Datta, Deep Ganguli, Denis Emelin, Denis\nKleyko, Deniz Yuret, Derek Chen, Derek Tam, Dieuwke\nHupkes, Diganta Misra, Dilyar Buzan, Dimitri Coelho\nMollo, Diyi Yang, Dong-Ho Lee, Dylan Schrader, Ekate-\nrina Shutova, Ekin Dogus Cubuk, Elad Segal, Eleanor\nHagerman, Elizabeth Barnes, Elizabeth Donoway, El-\nlie Pavlick, Emanuele Rodola, Emma Lam, Eric Chu,\nEric Tang, Erkut Erdem, Ernie Chang, Ethan A. Chi,\nEthan Dyer, Ethan Jerzak, Ethan Kim, Eunice Engefu\nManyasi, Evgenii Zheltonozhskii, Fanyue Xia, Fatemeh\nSiar, Fernando Martínez-Plumed, Francesca Happé, Fran-\ncois Chollet, Frieda Rong, Gaurav Mishra, Genta Indra\nWinata, Gerard de Melo, Germán Kruszewski, Giambat-\ntista Parascandolo, Giorgio Mariani, Gloria Wang, Gonzalo\nJaimovitch-López, Gregor Betz, Guy Gur-Ari, Hana Galija-\nsevic, Hannah Kim, Hannah Rashkin, Hannaneh Hajishirzi,\nHarsh Mehta, Hayden Bogar, Henry Shevlin, Hinrich\nSchütze, Hiromu Yakura, Hongming Zhang, Hugh Mee\nWong, Ian Ng, Isaac Noble, Jaap Jumelet, Jack Geissinger,\nJackson Kernion, Jacob Hilton, Jaehoon Lee, Jaime Fer-\nnández Fisac, James B. Simon, James Koppel, James\nZheng, James Zou, Jan Koco ´n, Jana Thompson, Janelle\nWingfield, Jared Kaplan, Jarema Radom, Jascha Sohl-\nDickstein, Jason Phang, Jason Wei, Jason Yosinski, Jeka-\nterina Novikova, Jelle Bosscher, Jennifer Marsh, Jeremy\nKim, Jeroen Taal, Jesse Engel, Jesujoba Alabi, Jiacheng\nXu, Jiaming Song, Jillian Tang, Joan Waweru, John Burden,\nJohn Miller, John U. Balis, Jonathan Batchelder, Jonathan\nBerant, Jörg Frohberg, Jos Rozen, Jose Hernandez-Orallo,\nJoseph Boudeman, Joseph Guerr, Joseph Jones, Joshua B.\nTenenbaum, Joshua S. Rule, Joyce Chua, Kamil Kan-\nclerz, Karen Livescu, Karl Krauth, Karthik Gopalakrishnan,\nKaterina Ignatyeva, Katja Markert, Kaustubh D. Dhole,\nKevin Gimpel, Kevin Omondi, Kory Mathewson, Kris-\nten Chiafullo, Ksenia Shkaruta, Kumar Shridhar, Kyle\nMcDonell, Kyle Richardson, Laria Reynolds, Leo Gao,\nLi Zhang, Liam Dugan, Lianhui Qin, Lidia Contreras-\nOchando, Louis-Philippe Morency, Luca Moschella, Lucas\nLam, Lucy Noble, Ludwig Schmidt, Luheng He, Luis Oliv-\neros Colón, Luke Metz, Lütfi Kerem ¸ Senel, Maarten\nBosma, Maarten Sap, Maartje ter Hoeve, Maheen Fa-\nrooqi, Manaal Faruqui, Mantas Mazeika, Marco Baturan,\nMarco Marelli, Marco Maru, Maria Jose Ramírez Quin-\ntana, Marie Tolkiehn, Mario Giulianelli, Martha Lewis,\nMartin Potthast, Matthew L. Leavitt, Matthias Hagen, Má-\ntyás Schubert, Medina Orduna Baitemirova, Melody Ar-\nnaud, Melvin McElrath, Michael A. Yee, Michael Cohen,\nMichael Gu, Michael Ivanitskiy, Michael Starritt, Michael\nStrube, Michał Sw˛ edrowski, Michele Bevilacqua, Michi-\nhiro Yasunaga, Mihir Kale, Mike Cain, Mimee Xu, Mirac\nSuzgun, Mitch Walker, Mo Tiwari, Mohit Bansal, Moin\nAminnaseri, Mor Geva, Mozhdeh Gheini, Mukund Varma\nT, Nanyun Peng, Nathan A. Chi, Nayeon Lee, Neta Gur-\nAri Krakover, Nicholas Cameron, Nicholas Roberts, Nick\nDoiron, Nicole Martinez, Nikita Nangia, Niklas Deckers,\nNiklas Muennighoff, Nitish Shirish Keskar, Niveditha S.\nIyer, Noah Constant, Noah Fiedel, Nuan Wen, Oliver\nZhang, Omar Agha, Omar Elbaghdadi, Omer Levy, Owain\nEvans, Pablo Antonio Moreno Casares, Parth Doshi, Pas-\ncale Fung, Paul Pu Liang, Paul Vicol, Pegah Alipoormo-\nlabashi, Peiyuan Liao, Percy Liang, Peter Chang, Peter\nEckersley, Phu Mon Htut, Pinyu Hwang, Piotr Miłkowski,\nPiyush Patil, Pouya Pezeshkpour, Priti Oli, Qiaozhu Mei,\nQing Lyu, Qinlang Chen, Rabin Banjade, Rachel Etta\nRudolph, Raefer Gabriel, Rahel Habacker, Ramon Risco,\nRaphaël Millière, Rhythm Garg, Richard Barnes, Rif A.\nSaurous, Riku Arakawa, Robbe Raymaekers, Robert Frank,\nRohan Sikand, Roman Novak, Roman Sitelew, Ronan Le-\nBras, Rosanne Liu, Rowan Jacobs, Rui Zhang, Ruslan\nSalakhutdinov, Ryan Chi, Ryan Lee, Ryan Stovall, Ryan\nTeehan, Rylan Yang, Sahib Singh, Saif M. Mohammad,\nSajant Anand, Sam Dillavou, Sam Shleifer, Sam Wiseman,\nSamuel Gruetter, Samuel R. Bowman, Samuel S. Schoen-\nholz, Sanghyun Han, Sanjeev Kwatra, Sarah A. Rous, Sarik\nGhazarian, Sayan Ghosh, Sean Casey, Sebastian Bischoff,\nSebastian Gehrmann, Sebastian Schuster, Sepideh Sadeghi,\nShadi Hamdan, Sharon Zhou, Shashank Srivastava, Sherry\nShi, Shikhar Singh, Shima Asaadi, Shixiang Shane Gu,\nShubh Pachchigar, Shubham Toshniwal, Shyam Upadhyay,\nShyamolima, Debnath, Siamak Shakeri, Simon Thormeyer,\nSimone Melzi, Siva Reddy, Sneha Priscilla Makini, Soo-\nHwan Lee, Spencer Torene, Sriharsha Hatwar, Stanislas\nDehaene, Stefan Divic, Stefano Ermon, Stella Biderman,\nStephanie Lin, Stephen Prasad, Steven T. Piantadosi, Stu-\nart M. Shieber, Summer Misherghi, Svetlana Kiritchenko,\nSwaroop Mishra, Tal Linzen, Tal Schuster, Tao Li, Tao Yu,\nTariq Ali, Tatsu Hashimoto, Te-Lin Wu, Théo Desbordes,\nTheodore Rothschild, Thomas Phan, Tianle Wang, Tiberius\nNkinyili, Timo Schick, Timofei Kornev, Titus Tunduny, To-\nbias Gerstenberg, Trenton Chang, Trishala Neeraj, Tushar\nKhot, Tyler Shultz, Uri Shaham, Vedant Misra, Vera Dem-\nberg, Victoria Nyamai, Vikas Raunak, Vinay Ramasesh,\nVinay Uday Prabhu, Vishakh Padmakumar, Vivek Sriku-\nmar, William Fedus, William Saunders, William Zhang,\nWout V ossen, Xiang Ren, Xiaoyu Tong, Xinran Zhao,\nXinyi Wu, Xudong Shen, Yadollah Yaghoobzadeh, Yair\nLakretz, Yangqiu Song, Yasaman Bahri, Yejin Choi, Yichi\nYang, Yiding Hao, Yifu Chen, Yonatan Belinkov, Yu Hou,\nYufang Hou, Yuntao Bai, Zachary Seid, Zhuoye Zhao, Zi-\njian Wang, Zijie J. Wang, Zirui Wang, and Ziyi Wu. 2023.\nBeyond the imitation game: Quantifying and extrapolating\nthe capabilities of language models.\nNisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler,\nRyan Lowe, Chelsea V oss, Alec Radford, Dario Amodei,\nand Paul F Christiano. 2020. Learning to summarize with\nhuman feedback. Advances in Neural Information Process-\ning Systems, 33:3008–3021.\nJulia Stoyanovich and Bill Howe. 2019. Nutritional labels\nfor data and models. A Quarterly bulletin of the Com-\nputer Society of the IEEE Technical Committee on Data\nEngineering, 42(3).\nPaul Thomas, Seth Spielman, Nick Craswell, and Bhaskar\nMitra. 2023. Large language models can accurately predict\nsearcher preferences.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert,\nAmjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,\nSoumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan\nBikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen,\nGuillem Cucurull, David Esiobu, Jude Fernandes, Jeremy\nFu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj\nGoswami, Naman Goyal, Anthony Hartshorn, Saghar\nHosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor\nKerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev,\nPunit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril,\nJenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao,\nXavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor\nMolybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein,\nRashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva,\nEric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen\nTan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang\nKuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang,\nAngela Fan, Melanie Kambadur, Sharan Narang, Aurelien\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. 2023. Llama 2: Open foundation and fine-tuned\nchat models.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkor-\neit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. 2017. Attention is all you need. Advances in\nneural information processing systems, 30.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill,\nOmer Levy, and Samuel Bowman. 2018. Glue: A multi-\ntask benchmark and analysis platform for natural language\nunderstanding. In Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neural\nNetworks for NLP, pages 353–355.\nZhen Wang, Rameswar Panda, Leonid Karlinsky, Rogerio\nFeris, Huan Sun, and Yoon Kim. 2022. Multitask prompt\ntuning enables parameter-efficient transfer learning. In The\nEleventh International Conference on Learning Represen-\ntations.\nZijie J. Wang, Dongjin Choi, Shenyu Xu, and Diyi Yang.\n2021. Putting humans in the natural language processing\nloop: A survey. In Proceedings of the First Workshop\non Bridging Human–Computer Interaction and Natural\nLanguage Processing, pages 47–52, Online. Association\nfor Computational Linguistics.\nAlice Xiang and Inioluwa Deborah Raji. 2019. On the le-\ngal compatibility of fairness definitions. arXiv preprint\narXiv:1912.00761.\nKe Yang, Julia Stoyanovich, Abolfazl Asudeh, Bill Howe,\nHV Jagadish, and Gerome Miklau. 2018. A nutritional\nlabel for rankings. In Proceedings of the 2018 international\nconference on management of data, pages 1773–1776.\nZhiling Zhang and Kenny Zhu. 2021. Diverse and specific\nclarification question generation with keywords. In Pro-\nceedings of the Web Conference 2021, WWW ’21, page\n3501–3511, New York, NY , USA. Association for Com-\nputing Machinery.\nZihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer\nSingh. 2021. Calibrate before use: Improving few-shot per-\nformance of language models. In International Conference\non Machine Learning, pages 12697–12706. PMLR.",
  "topic": "Sociotechnical system",
  "concepts": [
    {
      "name": "Sociotechnical system",
      "score": 0.9304367899894714
    },
    {
      "name": "Framing (construction)",
      "score": 0.6892933249473572
    },
    {
      "name": "Formalism (music)",
      "score": 0.518981397151947
    },
    {
      "name": "Software portability",
      "score": 0.5014994144439697
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.45923882722854614
    },
    {
      "name": "Computer science",
      "score": 0.4302647113800049
    },
    {
      "name": "Political science",
      "score": 0.3955429494380951
    },
    {
      "name": "Engineering",
      "score": 0.216234028339386
    },
    {
      "name": "Knowledge management",
      "score": 0.1898331642150879
    },
    {
      "name": "Artificial intelligence",
      "score": 0.11583662033081055
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Musical",
      "score": 0.0
    },
    {
      "name": "Structural engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I150468666",
      "name": "Emory University",
      "country": "US"
    }
  ],
  "cited_by": 2
}