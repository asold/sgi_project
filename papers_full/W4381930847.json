{
    "title": "ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge",
    "url": "https://openalex.org/W4381930847",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5100690428",
            "name": "Yunxiang Li",
            "affiliations": [
                "Hangzhou Dianzi University",
                "The Ohio State University",
                "The University of Texas Southwestern Medical Center",
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A5100672514",
            "name": "Zihan Li",
            "affiliations": [
                "Hangzhou Dianzi University",
                "The Ohio State University",
                "The University of Texas Southwestern Medical Center",
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A5100324056",
            "name": "Kai Zhang",
            "affiliations": [
                "Hangzhou Dianzi University",
                "The Ohio State University",
                "The University of Texas Southwestern Medical Center",
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A5006541976",
            "name": "Ruilong Dan",
            "affiliations": [
                "Hangzhou Dianzi University",
                "The Ohio State University",
                "The University of Texas Southwestern Medical Center",
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A5018120191",
            "name": "Steve Jiang",
            "affiliations": [
                "Hangzhou Dianzi University",
                "The Ohio State University",
                "The University of Texas Southwestern Medical Center",
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A5101904472",
            "name": "You Zhang",
            "affiliations": [
                "Hangzhou Dianzi University",
                "The Ohio State University",
                "The University of Texas Southwestern Medical Center",
                "University of Illinois Urbana-Champaign"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W632432350",
        "https://openalex.org/W4307343290",
        "https://openalex.org/W4310783829",
        "https://openalex.org/W4376133468",
        "https://openalex.org/W4366269319",
        "https://openalex.org/W2114309227",
        "https://openalex.org/W6851120502",
        "https://openalex.org/W4324387439",
        "https://openalex.org/W3013872196",
        "https://openalex.org/W4361289889",
        "https://openalex.org/W4319460874"
    ],
    "abstract": null,
    "full_text": "Review began\n 06/15/2023 \nReview ended\n 06/21/2023 \nPublished\n 06/24/2023\n© Copyright \n2023\nLi et al. This is an open access article\ndistributed under the terms of the Creative\nCommons Attribution License CC-BY 4.0.,\nwhich permits unrestricted use, distribution,\nand reproduction in any medium, provided\nthe original author and source are credited.\nChatDoctor: A Medical Chat Model Fine-Tuned on\na Large Language Model Meta-AI (LLaMA) Using\nMedical Domain Knowledge\nYunxiang Li \n \n, \nZihan Li \n \n, \nKai Zhang \n \n, \nRuilong Dan \n \n, \nSteve Jiang \n \n, \nYou Zhang \n1.\n Department of Radiation Oncology, University of Texas Southwestern Medical Center, Dallas, USA \n2.\n Department of\nComputer Science, University of Illinois at Urbana-Champaign, Illinois, USA \n3.\n Department of Computer Science and\nEngineering, The Ohio State University, Columbus, USA \n4.\n College of Computer Science and Technology, Hangzhou\nDianzi University, Hangzhou, CHN\nCorresponding author: \nYou Zhang, \nyou.zhang@utsouthwestern.edu\nAbstract\nObjective\nThe primary aim of this research was to address the limitations observed in the medical knowledge of\nprevalent large language models (LLMs) such as ChatGPT, by creating a specialized language model with\nenhanced accuracy in medical advice.\nMethods\nWe achieved this by adapting and refining the large language model meta-AI (LLaMA) using a large dataset\nof 100,000 patient-doctor dialogues sourced from a widely used online medical consultation platform. These\nconversations were cleaned and anonymized to respect privacy concerns. In addition to the model\nrefinement, we incorporated a self-directed information retrieval mechanism, allowing the model to access\nand utilize real-time information from online sources like Wikipedia and data from curated offline medical\ndatabases.\nResults\nThe fine-tuning of the model with real-world patient-doctor interactions significantly improved the model's\nability to understand patient needs and provide informed advice. By equipping the model with self-directed\ninformation retrieval from reliable online and offline sources, we observed substantial improvements in the\naccuracy of its responses.\nConclusion\nOur proposed ChatDoctor, represents a significant advancement in medical LLMs, demonstrating a\nsignificant improvement in understanding patient inquiries and providing accurate advice. Given the high\nstakes and low error tolerance in the medical field, such enhancements in providing accurate and reliable\ninformation are not only beneficial but essential.\nCategories:\n Family/General Practice, Medical Physics, Integrative/Complementary Medicine\nKeywords:\n ai chatbot, large language model, llama, chat gpt, gpt\nIntroduction\nThe development of instruction-following large language models (LLMs), such as ChatGPT \n[1]\n, has gained\nsignificant attention due to their remarkable success in instruction understanding and human-like response\ngeneration. These auto-regressive LLMs \n[2]\n are pre-trained on web-scale natural language by predicting the\nnext token and then fine-tuned to follow large-scale human instructions. These models show robust\nperformance on a wide range of natural language processing (NLP) tasks and can generalize to unseen tasks,\ndemonstrating their potential as unified solutions to various problems in natural language understanding,\ntext generation, and conversational artificial intelligence. However, the exploration of such general-domain\nLLMs in the medical domain remains relatively scarce \n[3]\n, despite their great potential in revolutionizing\nmedical communication and decision-making \n[4]\n. In general, these common-domain models were not\ntrained to capture the medical-domain knowledge specifically or in detail, resulting in models that often\nprovide incorrect medical responses.\nBy fine-tuning large linguistic dialogue models on data from real-world patient-physician conversations,\nthese models’ ability in understanding patients’ inquiries and needs can be significantly improved. In\naddition, to further enhance the models’ credibility, a knowledge brain based on online sources such as\nWikipedia or offline sources like medical-domain databases can be incorporated into the models to retrieve\nreal-time information to facilitate answering medical questions. The enhanced reliability of such answers is\n1\n2\n3\n4\n1\n1\n \n Open Access Original\nArticle\n \nDOI:\n 10.7759/cureus.40895\nHow to cite this article\nLi Y, Li Z, Zhang K, et al. (June 24, 2023) ChatDoctor: A Medical Chat Model Fine-Tuned on a Large Language Model Meta-AI (LLaMA) Using\nMedical Domain Knowledge. Cureus 15(6): e40895. \nDOI 10.7759/cureus.40895\nvital for the medical field, as a wrong answer can be detrimental to patients’ treatments and well-being. In\nthis study, we investigated the use of these two strategies: model fine-tuning and knowledge brain\ninstillation, to enhance the capability of LLMs to serve as medical chatbots. Since the prevalent ChatGPT\nmodel is not open source, we used Meta’s public large language model meta-AI (LLaMA) model as the\nplatform for development and evaluation. In detail, we first trained a generic conversation model based\non LLaMA, using 52K instruction-following data from Stanford University’s Alpaca project \n[5]\n. We then fine-\ntuned the conversation model on our collected dataset of 100K patient-physician conversations from an\nonline medical consultation website (www.healthcaremagic.com). Through extensive experiments, we found\nthat the fine-tuned model by patient-physician dialogues outperforms ChatGPT in terms of precision, recall,\nand the F1 score \n[6]\n. In addition, the autonomous ChatDoctor model, which is able to retrieve the latest\nonline/offline information, can also answer medical questions about relatively new diseases that are not\nincluded in the patient-physician training dialogues, for instance, the Monkeypox (Mpox) disease \n[7,8]\n.\nIn summary, the ChatDoctor model has the following three main contributions:\n1. We established a methodology for fine-tuning LLMs for application in the medical field.\n2. We compiled and publicly shared a comprehensive dataset of 100,000 patient-doctor interactions to serve\nas a training resource for refining the LLM. This dataset includes a wealth of terms, knowledge, and\nexpertise essential for training LLMs in the medical domain. Additionally, we curated and openly shared\nanother dataset consisting of 10,000 patient-doctor conversations from a separate source (www.icliniq.com)\nto serve as a testing resource for the model. To support and stimulate future advancements in the\ndevelopment of dialogue models in healthcare, we provide public access to all relevant resources such as\nsource codes, datasets, and model weights. These can be found at https://github.com/Kent0n-Li/ChatDoctor.\n3. We proposed an autonomous ChatDoctor model that can retrieve online and offline medical domain\nknowledge to answer medical questions on up-to-date medical terms and diseases, which can potentially\nreduce the errors and hallucinations of LLMs \n[9-11]\n.\nThis article was previously posted to the arXiv preprint server on March 24, 2023.\nMaterials And Methods\nCollection and preparation of patient-physician conversation dataset\nThe initial step in refining our model involved curating a dataset comprising patient-physician interactions.\nOften, patients describe their symptoms in casual and somewhat superficial language. If we attempted to\ngenerate these dialogues synthetically, similar to Alpaca \n[5]\n, it could lead to over-specific descriptions with\nlimited diversity and relevance to the real world. Hence, we chose to gather authentic patient-doctor\nconversations, collecting around 100k such interactions from the online medical consultation website,\nHealthCareMagic. The data were filtered both manually and automatically. Specifically, we automatically\nfiltered out conversations that were too short, most of which did not answer anything of practical\nsignificance. And we manually filtered the content of the responses that had errors. To maintain privacy, we\nerased any information identifying the doctor or the patient and employed LanguageTool to rectify any\ngrammatical errors. This dataset was labeled HealthCareMagic100k, illustrated in Figure \n1\n. We also sourced\nroughly 10k additional conversations from another independent online medical consultation site, iCliniq, to\ntest our model's performance. The iCliniq dataset was chosen randomly in a stratified manner to guarantee\nrepresentation across various medical specialties. It was also made certain that the selected data contained\nno identifiable patient information, in strict compliance with privacy and ethical standards.\nFIGURE\n 1: A summary of the process involved in gathering the patient-\nphysician conversation dataset and the steps involved in training the\nChatDoctor model.\n2023 Li et al. Cureus 15(6): e40895. DOI 10.7759/cureus.40895\n2\n of \n12\nCreation of external knowledge database\nLLMs typically predict the next word in a sequence, leading to potential inaccuracies or erroneous responses\nto questions (hallucinations) \n[12]\n. In addition, the model's output can be unpredictable to some extent,\nwhich is unacceptable in the medical field. However, the accuracy of these models could be significantly\nimproved if they could generate or assess responses based on a reliable knowledge database, depicted in\nFigure \n2\n. Consequently, we curated a database (sample shown in Figure \n3\n) encompassing diseases, their\nsymptoms, relevant medical tests/treatment procedures, and potential medications. This database serves as\nan external and offline knowledge brain for ChatDoctor. Continually updatable without requiring model\nretraining, this database can be tailored to specific diseases or medical specialties. We utilized MedlinePlus\nto construct this disease database, but other reliable sources can also be used. Additionally, online\ninformation sources like Wikipedia can supplement the knowledge base of our autonomous model. It is\nworth noting that Wikipedia may not be a fully reliable database, but our framework can be easily extended\nto more reliable online databases such as reputable academic journals.\nFIGURE\n 2: Overview of the autonomous ChatDoctor model based on\ninformation retrieval from an external knowledge brain.\n2023 Li et al. Cureus 15(6): e40895. DOI 10.7759/cureus.40895\n3\n of \n12\nFIGURE\n 3: Some samples in our offline disease database consist of\nsymptoms, clinical test/treatment approaches, and medication\nsuggestions.\nDevelopment of autonomous ChatDoctor with knowledge brain\nArmed with the external knowledge brain, i.e., Wikipedia or our custom disease database, ChatDoctor can\nmore accurately answer patient inquiries by retrieving reliable information. Upon establishing the external\nknowledge brain, we devised a mechanism to enable ChatDoctor to autonomously retrieve necessary\ninformation to answer queries. This was accomplished by constructing appropriate prompts to input into the\nChatDoctor model. Specifically, we designed keyword mining prompts (Figure \n4\n) as the initial step for\nChatDoctor to extract key terms from patient queries for relevant knowledge search. Based on these\nkeywords, top-ranked information was retrieved from the knowledge brain using a term-matching retrieval\nsystem \n[13]\n. Given the LLM's word limit (token size), we divided the texts to be read into equal sections and\nranked each section by the number of keyword hits. The ChatDoctor model then reads the first N sections\n(five used in our study) sequentially, selecting and summarizing pertinent information via prompts (Figure\n5\n). Ultimately, the model processes and compiles all the knowledge entries to generate a final response\n(Figure \n6\n). This information retrieval approach ensures patients receive precise, well-informed responses\nbacked by credible sources and can serve as a verification method for responses generated by ChatDoctor\n2023 Li et al. Cureus 15(6): e40895. DOI 10.7759/cureus.40895\n4\n of \n12\nfrom prior knowledge.\nFIGURE\n 4: Autonomously extract keywords for information retrieval.\nFIGURE\n 5: Autonomous information retrieval from the disease database\nthrough the prompt.\nFIGURE\n 6: Instruct the ChatDoctor to read the retrieved domain\nknowledge and provide a reliable answer.\nModel training\nWe developed the ChatDoctor model using Meta’s publicly accessible LLaMA-7B model \n[14]\n, which uses\nTransformers with the structure of the decoder only. Despite its relatively modest 7 billion parameters, the\nLLaMA model exhibits comparable performance to the much larger GPT-3 model (with 175 billion\nparameters) across several NLP benchmarks. This performance enhancement was achieved by diversifying\nthe training data rather than increasing network parameters. Specifically, LLaMA was trained on 1.0 trillion\ntokens from publicly accessible data sources like CommonCrawl and arXiv documents. We used\nconversations from HealthCareMagic-100k to fine-tune the LLaMA model \n[15]\n in line with Stanford Alpaca\n[5]\n training methodology. The model was first fine-tuned with Alpaca’s data to acquire basic conversation\nskills, followed by further refinement on HealthCareMagic-100k using 6 * A100 GPUs for three hours. The\ntraining process followed these hyperparameters: total batch size of 192, learning rate of \n, 3 epochs,\nmaximum sequence length of 512 tokens, and a warmup ratio of 0.03, with no weight decay.\nResults\nTo evaluate the proficiency of the autonomous ChatDoctor model, we tested it using a variety of\ncontemporary medical queries. One of these included a question related to “Monkeypox” (abbreviated as\nMpox), as illustrated in Figure \n7\n. Monkeypox was recently designated by the World Health Organization\n(WHO) on November 28, 2022, making it a relatively novel term. While ChatGPT was incapable of providing\na satisfactory response, ChatDoctor, due to its autonomous knowledge retrieval feature, was able to extract\npertinent information about Monkeypox from Wikipedia and deliver a precise answer. Similarly, for more\n2023 Li et al. Cureus 15(6): e40895. DOI 10.7759/cureus.40895\n5\n of \n12\ngeneral medical inquiries such as “Otitis,” as shown in Figure \n8\n, ChatDoctor was able to provide a reliable\nresponse following the retrieval of relevant knowledge. In another instance, a question about “Daybue,” a\ndrug that received FDA approval in March 2023, was accurately addressed by our model after it\nautonomously retrieved relevant information, demonstrating an advantage over ChatGPT, as shown in\nFigure \n9\n.\nFIGURE\n 7: Comparison between the ChatGPT and the autonomous\nChatDoctor for relatively new medical diseases/terms. The ChatGPT\ncannot recognize the word Mpox (aka, Monkeypox), while our\nChatDoctor can provide the precise answer for the relevant medical\ntests of Mpox, with the help of the external knowledge brain.\nFIGURE\n 8: Comparison between the ChatGPT and the autonomous\nChatDoctor. The ChatGPT provided a more general answer about otitis,\nwhile the ChatDoctor provided a more specialized response about the\ntreatments of otitis, with the help of the external knowledge brain.\n2023 Li et al. Cureus 15(6): e40895. DOI 10.7759/cureus.40895\n6\n of \n12\nFIGURE\n 9: Comparison between the ChatGPT and the autonomous\nChatDoctor. The ChatGPT is unfamiliar with the “Daybue” medication\nwhich received approval from the Food and Drug Administration (FDA)\nin early 2023. The ChatDoctor accurately pointed out the purpose of\nDaybue (trofinetide), with the help of the external knowledge brain.\nFor a quantitative evaluation of ChatDoctor's performance, we utilized questions from the independently\nsourced iCliniq database as inputs, with the corresponding responses from actual human physicians serving\nas the benchmark or “ground truth.” We compared these with responses generated by both ChatDoctor and\nChatGPT. In this evaluation, we employed BERTScore \n[6]\n to compute Precision, Recall, and F1 scores for both\nChatDoctor and ChatGPT. BERTScore leverages pre-trained BERT to match words in the candidate and\nreference sentences via cosine similarity, and BERTScore was chosen for its ability to evaluate the semantic\nsimilarity between our model's responses and the reference sentences, which we believe is of utmost\nimportance in the medical context. This method of evaluation closely aligns with human judgment at both\nsentence and system levels. In all three metrics, a higher value denotes a better match. As seen in the results\nillustrated in Table \n1\n, the fine-tuned ChatDoctor model outperforms ChatGPT across all three metrics, with\nspecific dialogue examples detailed in Figures \n10\n-\n13\n.\n \nChatGPT\nChatDoctor\nP-value\nPrecision\n0.837±0.0188\n0.8444±0.0185\nRecall\n0.8445±0.0164\n0.8451±0.0157\nF1 Score\n0.8406±0.0143\n0.8446±0.0138\nTABLE\n 1: Quantitative comparison with BERTScore between ChatDoctor and ChatGPT. The p-\nvalues in the table are derived from our paired t-test.\n \n2023 Li et al. Cureus 15(6): e40895. DOI 10.7759/cureus.40895\n7\n of \n12\nFIGURE\n 10: Example 1: \na patient suffering from a unilateral headache\nexpressed concerns about a potential association with a brain tumor.\nOur ChatDoctor accurately proposed sinusitis as a possible cause for\nthe headache, mirroring the diagnosis provided by the physician from\niCliniq. On the other hand, ChatGPT failed to deliver a congruent\ninterpretation regarding the root cause of the one-sided headache.\n \n2023 Li et al. Cureus 15(6): e40895. DOI 10.7759/cureus.40895\n8\n of \n12\nFIGURE\n 11: Example 2: a patient reported having a white lump in their\nthroat for several months and expressed concerns about potential\ncancer. All three entities, iCliniq, ChatGPT, and ChatDoctor suggested\nthat the patient could be dealing with abnormally enlarged lymph\nnodes. Both iCliniq and ChatDoctor additionally recommended that a\nbiopsy and radiological diagnosis would be necessary if initial\ntreatments proved unsuccessful. However, ChatGPT's response was\nlimited to advising the patient to consult with an Ear, Nose, and Throat\n(ENT) specialist.\n2023 Li et al. Cureus 15(6): e40895. DOI 10.7759/cureus.40895\n9\n of \n12\nFIGURE\n 12: Example 3: a patient reported experiencing a sharp back\npain during exercise, which intensified during breathing and rotation of\nthe torso or neck. The patient was unsure whether urgent medical\nattention was necessary. ChatDoctor generated a closer answer to\niCliniq than ChatGPT.\n2023 Li et al. Cureus 15(6): e40895. DOI 10.7759/cureus.40895\n10\n of \n12\nFIGURE\n 13: Example 4: a patient experienced blurred vision and was\nparticularly concerned about the health of their left eye. Taking into\nconsideration the patient's past medical history of retinal detachment,\nall three sources—iCliniq, ChatGPT, and ChatDoctor—advised the\nindividual to seek professional consultation with ophthalmologists for a\ncomprehensive assessment and swift treatment. Due to possible\nlimitations in providing medical diagnoses (and advice), ChatGPT did\nnot speculate on the cause of the diminished vision. On the other hand,\nboth iCliniq and ChatDoctor identified the possibility of retinal\ndetachment or bleeding as potential issues.\nDiscussion\nThe medical LLM, ChatDoctor, which has been fine-tuned on medical data, has extensive potential uses.\nThese range from preliminary patient assessment and automated case adjudication to proactive healthcare\nmeasures. Nevertheless, owing to the complex nature of medical information \n[16]\n, any concealed\ninaccuracies in diagnoses and health advice could lead to severe outcomes \n[17]\n. LLMs are known to\noccasionally generate fallacious and harmful assertions (hallucinations) about areas beyond their knowledge\nexpertise, potentially causing medical malpractice \n[18]\n. To mitigate this, ChatDoctor has been trained using\nreal-world patient-doctor interactions to better understand patients' questions and deliver more\nknowledgeable responses. To make the model most capable of answering questions about the latest medical\nterms (which may not be contained in the training dataset), and to introduce additional external references\nfor verification, we also equipped the ChatDoctor model with the ability to autonomously retrieve\ninformation from external knowledge brains to provide answers, further enhancing the credibility of the\nmodel \n[19]\n. Such external knowledge retrieval can be called by inputting pre-configured prompts into the\nmodel. In future developments, the internal prior knowledge of the ChatDoctor model (gained through\ntraining) and the external knowledge brain can be further combined by training ChatDoctor to select a more\ntrustworthy answer, or merge and fuse both answers or provide alternative opinions.\nLimitations\nIt is important to emphasize that the current ChatDoctor model is still in the investigation phase and has\nbeen developed for academic research only. The actual clinical use is subject to the risk of wrong answers\n2023 Li et al. Cureus 15(6): e40895. DOI 10.7759/cureus.40895\n11\n of \n12\nbeing output by the model, and the use of exclusively LLMs in medical diagnosis is still plagued by false\npositives and false negatives for the time being. Additional security measures, including automated\nreference checking and human expert evaluation, are needed to cross-validate the answers provided by\nChatDoctor to flag potentially inaccurate answers and prevent hallucinations. The exact design,\ndevelopment and deployment of such security measures remains an important topic for further research. A\nmore secure application at this stage is the use of LLMs to assist physicians in their face-to-face\nconsultations. Physicians and ChatDoctor work together to ensure not only that the technology is consistent\nwith clinical practice, but also that patient safety is ensured. The evaluation and potential approval of such\ntools for healthcare-related purposes also needs further investigation.\nConclusions\nWith adequate training and online/offline supervision, ChatDoctor can potentially improve accuracy and\nefficiency in medical diagnosis and reduce the workload for medical professionals. It may also increase\naccess to high-quality medical consultations, especially for patients in underserved regions with limited\nmedical resources. The further developments and applications of ChatDoctor may eventually help to\nimprove patient outcomes and advance medical research.\nAdditional Information\nDisclosures\nHuman subjects:\n All authors have confirmed that this study did not involve human participants or tissue.\nAnimal subjects:\n All authors have confirmed that this study did not involve animal subjects or tissue.\nConflicts of interest:\n In compliance with the ICMJE uniform disclosure form, all authors declare the\nfollowing: \nPayment/services info:\n This work was supported by the National Institutes of Health (Grant No.\nR01 CA240808, R01 CA258987). \nFinancial relationships:\n All authors have declared that they have no\nfinancial relationships at present or within the previous three years with any organizations that might have\nan interest in the submitted work. \nOther relationships:\n All authors have declared that there are no other\nrelationships or activities that could appear to have influenced the submitted work.\nReferences\n1\n. \nTraining language models to follow instructions with human feedback\n. (2022). Accessed: April 3, 2023:\nhttp://arXiv:2203.02155\n.\n2\n. \nSelf-instruct: aligning language model with self generated instructions\n. (2022). Accessed: December 20,\n2022: \nhttp://arXiv:2212.10560\n.\n3\n. \nAidan Gilson, Conrad W Safranek, Thomas Huang, et al.: \nHow does chatgpt perform on the united states\nmedical licensing examination? the implications of large language models for medical education and\nknowledge assessment\n. JMIR Med Educ. 2023, 9:45312-2023.\n4\n. \nAbacha AB, Zweigenbaum P: \nMeans: a medical question-answering system combining NLP techniques and\nsemantic web technologies\n. Inf Process Manag. 2015, 51:570-94.\n5\n. \nStanford alpaca: an instruction-following llama model\n. (2023). Accessed: April 3, 2023:\nhttps://github.com/tatsu-lab/stanford_alpaca\n.\n6\n. \nBertscore: Evaluating text generation with bert\n. (2020). Accessed: April 21, 2020: \nhttp://arXiv:1904.09675\n.\n7\n. \nGessain A, Nakoune E, Yazdanpanah Y: \nMonkeypox\n. N Engl J Med. 2022, 387:1783-93.\n10.1056/NEJMra2208860\n8\n. \nBeeson AM, Haston J, McCormick DW, Reynolds M, Chatham-Stephens K, McCollum AM, Godfred-Cato S:\nMpox in children and adolescents: epidemiology, clinical features, diagnosis, and management\n. Pediatrics.\n2023, 151:e2022060179.\n9\n. \nA multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity\n.\n(2023). Accessed: February 8, 2023: \nhttp://arXiv:2302.04023\n.\n10\n. \nSelfcheckgpt: zero-resource black-box hallucination detection for generative large language models\n. (2023).\nAccessed: March 15, 2023: \nhttp://arXiv:2303.08896\n.\n11\n. \nSalvagno M, Taccone FS, Gerli AG: \nArtificial intelligence hallucinations\n. Crit Care. 2023, 27:180.\n10.1186/s13054-023-04473-y\n12\n. \nBeutel G, Geerits E, Kielstein JT: \nArtificial hallucination: GPT on LSD?\n. Crit Care. 2023, 27:148.\n10.1186/s13054-023-04425-6\n13\n. \nRetrieval system evaluation\n. (2005). Accessed: September 26, 2005:\nhttps://www.nist.gov/publications/retrieval-system-evaluation\n.\n14\n. \nLLaMA: open and efficient foundation language models\n. (2023). Accessed: February 27, 2023:\nhttp://arXiv:2302.13971\n.\n15\n. \nRaise a child in large language model: towards effective and generalizable fine-tuning\n. (2021). Accessed:\nSeptember 13, 2021: \nhttp://arXiv:2109.05687\n.\n16\n. \nHammerling JA: \nA review of medical errors in laboratory diagnostics and where we are today\n. Laboratory\nMed. 2012, 43:41-4. \n10.1309/LM6ER9WJR1IHQAUY\n17\n. \nLee P, Bubeck S, Petro J: \nBenefits, limits, and risks of GPT-4 as an AI chatbot for medicine\n. New England J\nMed. 2023, 388:1233-9.\n18\n. \nVaishya R, Misra A, Vaish A: \nChatGPT: is this version good for healthcare and research?\n. Diabet Metabol\nSyndr. 2023, 17:102744.\n19\n. \nHatherley JJ: \nLimits of trust in medical AI\n. J Med Ethics. 2020, 46:478-81. \n10.1136/medethics-2019-105935\n2023 Li et al. Cureus 15(6): e40895. DOI 10.7759/cureus.40895\n12\n of \n12"
}