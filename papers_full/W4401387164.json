{
  "title": "From Specifications to Prompts: On the Future of Generative Large Language Models in Requirements Engineering",
  "url": "https://openalex.org/W4401387164",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2746454425",
      "name": "Vogelsang, Andreas",
      "affiliations": [
        "University of Cologne"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385245566",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2575641950",
    "https://openalex.org/W4362499986",
    "https://openalex.org/W3092205878",
    "https://openalex.org/W4387123286",
    "https://openalex.org/W6862005606",
    "https://openalex.org/W4308518385",
    "https://openalex.org/W2291920259",
    "https://openalex.org/W4313061396",
    "https://openalex.org/W2896457183"
  ],
  "abstract": "Generative LLMs, such as GPT, have the potential to revolutionize\\nRequirements Engineering (RE) by automating tasks in new ways. This column\\nexplores the novelties and introduces the importance of precise prompts for\\neffective interactions. Human evaluation and prompt engineering are essential\\nin leveraging LLM capabilities.\\n",
  "full_text": "arXiv:2408.09127v1  [cs.SE]  17 Aug 2024\nFrom Speciﬁcations to Prompts: On the Future\nof Generative LLMs in Requirements Engineering\nAndreas V ogelsang, University of Cologne, Germany\nAbstract—Generative LLMs, such as GPT, have the potential t o revolutionize\nRequirements Engineering (RE) by automating tasks in new wa ys. This column explores\nthe novelties and introduces the importance of precise prom pts for eﬀective interactions.\nHuman evaluation and prompt engineering are essential in le veraging LLM capabilities.\nFROM THE EDITOR\nThe theme of this issue is Generative AI for Software\nEngineering, a versatile approach that will impact\nthe entire development lifecycle. F requent readers will\nrecall the summary of a panel discussion at the RE\n2023 Conference from three issues ago. In this guest\ncolumn, Andreas V ogelsang, drawing from his keynote\nat the 7th W orkshop on Natural Language Processing\nfor RE, continues this discussion. He explores the\npotential of generative AI to automate tasks, from\ncapturing requirements to verifying consistency and\ncompleteness. Additionally , he discusses how RE can\nenhance prompt engineering. I’m sure generative AI\nwill quickly evolve from a novel tool to an integral\npart of the software engineering context. Here are some\ninsights from an expert at the forefront of this shift!\nIntroduction\nGenerative LLMs, such as GPT, have revolutionized\nour interaction with artiﬁcial intelligence. Their abil-\nity to understand, generate, and manipulate language\npresents unprecedented opportunities and challenges\nacross various disciplines, including RE. Generative\nLLMs have the potential to redeﬁne the landscape of\nrequirements elicitation, speciﬁcation, and validation.\nThe main messages conveyed in this column are:\n1) Generative LLMs provide novel features to sup-\nport RE tasks. However, the human in the loop\nbecomes even more important for evaluating gen-\nerative LLMs.\n2) Prompts are expressions of requirements. RE can\nhelp improve prompt engineering and position\nprompts as artifacts in the software development\nlifecycle.\nXXXX-XXX © 2024 IEEE\nDigital Object Identiﬁer 10.1109/XXX.0000.0000000\nPreliminaries: Decoder-only\n(Generative) LLMs\nGenerative LLMs build upon a decoder-only architec-\nture. Decoder-only architectures are derived from the\nwell-known transformer architecture [\n1]. The original\ntransformer architecture addressed translation tasks\nby combining an encoder, which encodes the input,\nand a decoder, which generates the output. However,\nit turned out that the encoder and the decoder parts\nalone can be adapted to create helpful models. F or\nexample, the successful BER T model [\n2] is an encoder-\nonly model. Encoder-only models are primarily used\nfor predictive tasks where patterns in data are used\nto make forecasts and predictions (e.g., classiﬁcation,\nclustering, similarity estimation).\nDecoder-only LLMs have been designed to generate\ntext. T o support the generative capabilities of decoder-\nonly LLMs, they are primarily pre-trained with a next-\nword prediction (NWP) objective, where the models\npredict the next word or words in a given sequence\nof words. After pre-training, decoder-only LLMs are\ntriggered by a so-called prompt. A prompt is a textual\ninput instructing the generative LLM to generate the\ndesired response.\nF eeding decoder-only LLMs with prompts oﬀers a\nnew paradigm for interaction. In encoder-only or full\ntransformer models, information about the task had\nto be reﬂected in the training data. The input had\nto be preprocessed before the model could make a\nprediction. In decoder-only models, the task and the\ninput can be expressed in natural language and passed\ndirectly to the model. The model’s output is also\nricher than for non-generative LLMs since it produces\n(customizable) text instead of conﬁdence values for\npredeﬁned outcomes. A prompt contains any of the\nfollowing elements:\n• Instruction: a speciﬁc task or instruction you\nwant the model to perform\n• Context: external information or additional\ncontext that can steer the model to better re-\nsponses\n• Input Data: the input or question that we are\nPublished by the IEEE Computer Society 1\ninterested in ﬁnding a response for\n• Output Indicator: the type or format of the\noutput.\nY ou do not need all four elements for a prompt,\nand the ideal format depends on the task. Although\nprompting an LLM sounds relatively straightforward,\nthe creation and exact phrasing of a prompt is crucial\nfor the quality of the LLM output. F or most tasks, it\nis necessary to experiment with diﬀerent prompts and\niteratively reﬁne them to yield the best results. This\nso-called prompt engineering step is similar to feature\nengineering in more traditional machine learning (ML)\napproaches. Prompt engineering is a relatively new\nﬁeld where only heuristics and simple rules exist that\nhelp create eﬀective prompts.\nUsing Generative LLMs for RE tasks\nGenerative LLMs provide a new way to solve RE tasks.\nT raditional predictive LLMs (i.e., encoder-only LLMs)\nare trained or ﬁne-tuned to predict the most likely\noutput for a given input. Usually , the set of possible\noutputs is constrained. It is either a set of classes [\n3],\n[4], a subset of the input (i.e., information retrieval [ 5]),\nor a numeric value in a deﬁned range (e.g., similarity\nestimation).\nWith generative LLMs, a speciﬁc RE task is writ-\nten as a prompt, which serves as input. The prompt\nusually contains the task and the input data to be\nprocessed. The model’s output provides the answer to\nthe speciﬁc task.\nHere is an example of how a trace link recovery\ntask can be formulated as prompt [\n6]:\nUser Prompt:\nBelow are artifacts from the same software\nsystem. Is there a traceability link between (1)\nand (2)?\n(1) The DPU-TMALI shall utilize\nSCM_DCI_SR, along with ERRNO provided\nby DPU-DCI to decode errors and place them\non an error queue for DPU-CCM.\n(2) Error Collection and Reporting The\nccmErrEnq() function tracks the last error\nreported and its frequency of occurrence. Once\nan error code has been reported it becomes the\npreviously reported error code maintained by\nccmErrEnq(). A repetition count is then in-\ncremented for each subsequent, consecutively\nreported, identical instance of this previously\nreported error. If this error code is reported\nmore than once in one high-rate housekeep-\ning reporting period, then a special error,\nS_ccm_ERR_REPEA T is enqueued with the\nrepetition count for the error encoded in the\nleast signiﬁcant byte. This mechanism eﬀec-\ntively reduces the potential for housekeeping\ntelemetry to become ﬂooded with a single re-\npeated error.\nLLM Assistant:\nY es, there is a traceability link between (1) and\n(2). (1) mentions that the DPU-TMALI places\nerrors on an error queue for the DPU-CCM.\n(2) describes the error collection and report-\ning mechanism of the DPU-CCM, speciﬁcally\nreferring to enqueuing errors.\nSo (2) provides an implementation detail for\nthe error queuing functionality brieﬂy men-\ntioned in (1). Therefore, there is a traceability\nlink between these two artifacts in the software\nsystem.\nThe quality of the provided output depends on the\nspeciﬁc prompt. Prompts can be reﬁned by reﬁning\nany component (i.e., instructions, context, input ques-\ntion). Providing more context and instructions is an\neﬀective strategy to increase the semantic quality of\nthe output. In a recent publication, we describe few-\nshot prompting , instruction ﬁne-tuning , and retrieval\naugmented generation , which can be used to reﬁne the\nprompt and increase the model’s performance [\n7].\nBesides the novel input paradigm, LLMs also oﬀer\n2 2024\ntwo novel ways to work with the output.\nW e can ask generative LLMs to augment\nthe response with rationales and explanations.\nIn contrast to predictive LLMs, which only provide\nthe prediction as output, generative LLMs can be\ninstructed to provide explanations along the result. As\nalso shown in the example above, the LLM assistant\nprovides the answer (“yes, there is a trace link”) fol-\nlowed by an explanation. This additional information\nimpacts the perception of the human analyst using the\ntool. It may also increase the chances that humans\ncan uncover mispredictions by the model and also help\nhumans to improve quality ﬂaws.\nW e can enter into a dialog with a generative\nLLM. The second novel advantage is that generative\nLLMs can incorporate prior answers as context into\nnew prompts. This gives the user the possibility to ask\nfor additional information or even correct certain de-\ntails of an answer. The generative LLM can respond to\nthis additional human input to reﬁne, clarify , or even\ncorrect its answer. If, for example, we have spotted a\nmistake in the answer, we may correct the model and\nget a reﬁned answer:\nUser Prompt:\nThe explanation is incorrect. (2) does not de-\nscribe the DPU-CCM but another component.\nLLM Assistant:\nI apologize for the confusion in my previous\nresponse. Based on the information provided,\nthere is no direct traceability link between (1)\nand (2) in terms of speciﬁc components or\nfunctionalities. (1) describes the error handling\nprocess in the DPU-TMALI component, while\n(2) describes the error collection and reporting\nmechanism in a diﬀerent component that is not\nexplicitly mentioned.\nWithout further context or explicit connec-\ntions between the components mentioned in\n(1) and (2), it is not possible to establish a\ndirect traceability link between the two arti-\nfacts.\nW e can combine diﬀerent modalities. Al-\nthough LLMs were originally designed to process\nand produce text, recent LLMs can also process and\ngenerate image, audio, and even video data. Much\nrequirements-related information is in non-text for-\nmat (e.g., visual models, interview recordings, vision\nvideos). The recent LLM advancements allow the in-\ntegration of these representations into the automation\npipeline.\nEvaluation becomes a challenge. Augment a\nresponse with explanations, entering into a dialog, and\ncombining modalities are features of generative LLMs\nthat distinguish them from predictive LLMs we used\nin the past to automate RE tasks. These features,\nhowever, make the evaluation of LLMs more challeng-\ning. When generative LLMs are used like predictive\nmodels, the evaluation is straightforward and should\nadhere to existing best practices (e.g., as suggested by\nDell’Anna et al. [\n8]). However, when using the genera-\ntive, conversational, and multimodal capabilities, the\nhuman-in-the-loop becomes much more important for\nevaluating generative LLMs. This starts with diﬀerent\nmetrics that are used to compare generated text (e.g.,\nBLEU and ROUGE instead of precision and recall).\nEven more importantly , the performance of an LLM\nassistant will strongly depend on the conversational\nﬂow, the perceived quality of answers, the trust of\nhumans in the answers, and the way the assistant\nhandles human feedback.\nUsing RE for Eﬀective Prompting\nIn the rapidly advancing landscape of AI, prompt\nengineering emerged as a new discipline concerning the\ndevelopment and optimization of prompts for LLMs.\nV arious prompting techniques have been introduced by\ndiﬀerent sources, ranging from blog articles to peer-\nreviewed academic publications. However, the vast\nmajority seem to lack a scientiﬁc foundation and are\nmore based on trial and error. Some of these rules\naddress the style of how prompts should be phrased,\ne.g.,\n• Start simple: start with a simple prompt and\nbuild on it\n• Call to action: start the prompt with an action\nword like “W rite”, “Create”, or “Summarize”\ninstead of “Can you”\n• Add context: add speciﬁc and relevant context\nto the task you want to perform\n• Add expectations: add clear and direct expec-\ntations for the content, like how long it should\nbe and what to include\nPrompt templates such as the CRISPE framework\n1\n(Capacity and Role, Insight, Statement, Personality ,\nExperiment) or the RICE framework 2 (Role,\n1 https://sourcingdenis.medium.com/crispe- prompt- engineering- framework- e47eaaf83611\n2 https://www.geeky- gadgets.com/harnessing- the- power-of- the- rice- framework- \n2024 3\nInstructions, Context, Constraints, and Examples)\nprovide guidelines and best practices for structuring\nthe content of a prompt. A prompt for a trace link\nrecovery task following the CRISPE framework could\nlook like this:\nUser Prompt:\nCapacity and Role: Requirement Analyst\nInsight: W e’re looking for traceability links\nbetween high-level and low-level requirements.\nA traceability link indicates that a low-level\nrequirement is related to a high-level require-\nment (e.g. because it reﬁnes it).\nStatement: Find all traceability links be-\ntween the following low-level and high-level\nrequirements.\nPersonality: Accurate and precise.\nExperiment: Provide just one answer.\n[Low-level Requirements]: . . .\n[High-level Requirements]: . . .\nOther prompting strategies suggest adding exam-\nples (few-shot prompting) or breaking down the prob-\nlem into smaller tasks (chain-of-thought prompting).\nF rom an RE perspective, prompts express the\nuser’s needs and expectations to an LLM. Therefore,\nprompts are expressions of requirements. RE research\nhas a long tradition of thinking about and coming\nup with methods to describe stakeholder needs in a\nprecise manner. This also includes thinking about the\nrole and importance of prompts in a broader devel-\nopment context and not just aiming at the quality of\nthe resulting outputs. Generally , the goals of RE are\n(1) knowing all relevant requirements, (2) achieving a\nconsensus among the stakeholders about these require-\nments, (3) documenting requirements appropriately ,\nand (4) managing requirements systematically . Adapt-\ning these goals to requirements expressed as prompts\nleads to interesting questions:\nKnowing all prompts\n• Have we included all relevant content in a\nprompt?\n• Do we need additional prompts to improve the\nresults?\nAchieving a consensus among the stakeholders\nabout these prompts:\n• Do all stakeholders agree with the prompt?\n• Are the perspectives of all stakeholders reﬂected\nin a (set of) prompts?\nDocumenting prompts appropriately\n• Is there a need to document and store the\nprompts?\n• Who may need to read, change, or reuse the\nprompts?\nManaging prompts systematically\n• Which (version of a) prompt generated which\nartifact?\n• Which LLM (version) has been used?\n• Which prompts have been derived from which\nrequirements?\n• Is all prompt content backed up by require-\nments?\nRecently , we investigated reproducible indicators\nwithin prompts that may predict a loss of quality\nor ﬂaw. W e base these ﬂaw indicators on established\nrequirements smells, which are reliable indicators for\nrequirements quality [\n9], [ 10]. Our initial experiments\nshowed interesting results. F or example, in a code\ngeneration task, ChatGPT interpreted the vague re-\nquirement “If the points of one player are too low,\nthe game ends and the player loses the game. ” by\nsetting the boundary to 0 points without asking for\nclariﬁcation or hinting at the ambiguity:\nUser Prompt:\nW e want to develop a game for mobile devices.\nThe game has two players who throw a dice in\neach turn. Both players start with 10 points.\nIf the dice shows an even number, the number\nis added to the points of the player, if it shows\nan uneven number, the points are subtracted.\nIf the points of one player are too low,\nthe game ends and the player loses the\ngame.\nCreate Python code that implements this\ngame.\nLLM Assistant:\nCertainly! Below is a Python implementation\nof the game:\n[. . . ]\nif player1_points <= 0 :\nprint(“Player 1 has lost the game!”)\nbreak\n[. . . ]\nConsidering prompts as a new manifestation of\nrequirements is a novel and relevant research direction.\nThis line of research should start with characterizing\nthe role of prompts in the development process. Should\n4 2024\nprompts be considered similarly to requirements ex-\npressed in requirements documents? Are they more\nlike intermediate and transitory artifacts? Or should\nthey be treated as very high-level code?\nConclusions\nGenerative large language models (LLMs) are opening\nup new opportunities for requirements analysts, pro-\nviding innovative tools to enhance their work. Unlike\ntraditional predictive models, generative LLMs oﬀer\ndetailed explanations and allow users to engage in a\ndialogue to reﬁne or correct responses. This brings\nexciting possibilities but also introduces challenges.\nW e need to rethink how we evaluate these models’\ncapabilities and develop new ways to assess their ef-\nfectiveness, especially when humans are involved.\nAdditionally , there is a growing interest in the in-\ntersection of RE and prompt engineering. How can we\ncreate precise prompts or break down complex goals\ninto actionable steps? Is there a need for systematic\nprompt documentation and management? This is an\nintriguing area for RE researchers and enthusiasts to\nexplore.\nREFERENCES\n1. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit,\nL. Jones, A. N. Gomez, L. u. Kaiser, and I. Polo-\nsukhin, “Attention is all you need,” in Advances in\nNeural Information Processing Systems , I. Guyon,\nU. V. Luxburg, S. Bengio, H. Wallach, R. Fergus,\nS. Vishwanathan, and R. Garnett, Eds., vol. 30.\nCurran Associates, Inc., 2017.\n2. J. Devlin, M.-W. Chang, K. Lee, and K. T outanova,\n“BERT: Pre-training of deep bidirectional transform-\ners for language understanding,” 2018.\n3. J. Winkler and A. Vogelsang, “Automatic classiﬁca-\ntion of requirements based on convolutional neural\nnetworks,” in 2016 IEEE 24th International Require-\nments Engineering Conference Workshops (REW) .\nIEEE, 2016, pp. 39–45.\n4. M. Binder, A. Vogt, A. Bajraktari, and A. Vogelsang,\n“Automatically Classifying Kano Model Factors in\nApp Reviews,” in International Working Conference\non Requirements Engineering: Foundation for Soft-\nware Quality . Springer Nature Switzerland Cham,\n2023, pp. 245–261.\n5. J. Fischbach, B. Hauptmann, L. Konwitschny ,\nD. Spies, and A. Vogelsang, “T owards causality ex-\ntraction from requirements,” in IEEE 28th Interna-\ntional Requirements Engineering Conference (RE) .\nIEEE, 2020, pp. 388–393.\n6. A. D. Rodriguez, K. R. Dearstyne, and J. Cleland-\nHuang, “Prompts matter: Insights and strategies for\nprompt engineering in automated software traceabil-\nity ,” in IEEE 31st International Requirements En-\ngineering Conference Workshops (REW) , 2023, pp.\n455–464.\n7. A. Vogelsang and J. Fischbach, “Using Large Lan-\nguage Models for Natural Language Processing T asks\nin Requirements Engineering: A Systematic Guide-\nline,” in Handbook of Natural Language Processing\nfor Requirements Engineering , A. Ferrari and G. G.\nDeshpande, Eds. Cham: Springer International Pub-\nlishing, 2024.\n8. D. Dell’Anna, F. B. Aydemir, and F. Dalpiaz, “Eval-\nuating classiﬁers in SE research: the ECSER pipeline\nand two replication studies,” Empirical Software En-\ngineering, vol. 28, no. 1, Nov. 2022.\n9. H. Femmer, D. Méndez Fernández, S. Wagner, and\nS. Eder, “Rapid quality assurance with requirements\nsmells,” Journal of Systems and Software , vol. 123,\npp. 190–213, 2017.\n10. J. Frattini, L. Montgomery , J. Fischbach, M. Un-\nterkalmsteiner, D. Mendez, and D. Fucci, “A live\nextensible ontology of quality factors for textual\nrequirements,” in IEEE 30th International Require-\nments Engineering Conference (RE) . IEEE, 2022.\nAndreas Vogelsang is a full professor of Software\nEngineering at the University of Cologne, 50923 Cologne,\nGermany . Contact him at vogelsang@cs.uni-koeln.de.\n2024 5",
  "topic": "Software engineering",
  "concepts": [
    {
      "name": "Software engineering",
      "score": 0.6648493409156799
    },
    {
      "name": "Computer science",
      "score": 0.6396182179450989
    },
    {
      "name": "Requirements engineering",
      "score": 0.5969605445861816
    },
    {
      "name": "Generative grammar",
      "score": 0.5360119342803955
    },
    {
      "name": "Programming language",
      "score": 0.5190242528915405
    },
    {
      "name": "Software requirements",
      "score": 0.494931161403656
    },
    {
      "name": "Software requirements specification",
      "score": 0.47466447949409485
    },
    {
      "name": "Requirements analysis",
      "score": 0.46255412697792053
    },
    {
      "name": "Model-driven architecture",
      "score": 0.45104312896728516
    },
    {
      "name": "Systems engineering",
      "score": 0.41479700803756714
    },
    {
      "name": "Software development",
      "score": 0.33176323771476746
    },
    {
      "name": "Software",
      "score": 0.3008647561073303
    },
    {
      "name": "Engineering",
      "score": 0.295589417219162
    },
    {
      "name": "Software design",
      "score": 0.24728822708129883
    },
    {
      "name": "Artificial intelligence",
      "score": 0.2304457128047943
    }
  ]
}