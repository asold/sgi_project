{
    "title": "Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding",
    "url": "https://openalex.org/W4389523983",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2274734547",
            "name": "Sangmin Bae",
            "affiliations": [
                "Kootenay Association for Science & Technology",
                "Korea Advanced Institute of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A3056022470",
            "name": "Jongwoo Ko",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology",
                "Kootenay Association for Science & Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2260726766",
            "name": "Hwanjun Song",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2142069088",
            "name": "Se-Young Yun",
            "affiliations": [
                "Korea Advanced Institute of Science and Technology",
                "Kootenay Association for Science & Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3008374555",
        "https://openalex.org/W2962677625",
        "https://openalex.org/W4225727438",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W4382461569",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W4321177657",
        "https://openalex.org/W3112673818",
        "https://openalex.org/W4385571586",
        "https://openalex.org/W4225598930",
        "https://openalex.org/W4288347505",
        "https://openalex.org/W3197876970",
        "https://openalex.org/W2987861506",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W2953280096",
        "https://openalex.org/W4293569541",
        "https://openalex.org/W4387995158",
        "https://openalex.org/W4310561894",
        "https://openalex.org/W2333131589",
        "https://openalex.org/W4295312788",
        "https://openalex.org/W4292119927",
        "https://openalex.org/W4285595056",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W2963204221",
        "https://openalex.org/W4362679631",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W4385970120",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W3105966348",
        "https://openalex.org/W2165072487",
        "https://openalex.org/W4386566583",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W2767206889",
        "https://openalex.org/W4319166707",
        "https://openalex.org/W3100560913",
        "https://openalex.org/W3005389111",
        "https://openalex.org/W2325237720",
        "https://openalex.org/W4309591680",
        "https://openalex.org/W2963929190",
        "https://openalex.org/W2995607862",
        "https://openalex.org/W2981757109",
        "https://openalex.org/W3177172118",
        "https://openalex.org/W3199246732",
        "https://openalex.org/W4361806892",
        "https://openalex.org/W4226126941",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2988975212"
    ],
    "abstract": "To tackle the high inference latency exhibited by autoregressive language models, previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5910‚Äì5924\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nFast and Robust Early-Exiting Framework for\nAutoregressive Language Models with Synchronized Parallel Decoding\nSangmin Bae1‚àó Jongwoo Ko1‚àó Hwanjun Song2‚Ä† Se-Young Yun1‚Ä†\n1KAIST AI 2AWS AI\n{bsmn0223, jongwoo.ko, yunseyoung}@kaist.ac.kr hwanjuns@amazon.com\nhttps://github.com/raymin0223/fast_robust_early_exit\nAbstract\nTo tackle the high inference latency exhibited\nby autoregressive language models, previous\nstudies have proposed an early-exiting frame-\nwork that allocates adaptive computation paths\nfor each token based on the complexity of gen-\nerating the subsequent token. However, we\nobserved several shortcomings, including per-\nformance degradation caused by a state copy-\ning mechanism or numerous exit paths, and\nsensitivity to exit confidence thresholds. Con-\nsequently, we propose a Fast and Robust Early-\nExiting (FREE) framework, which incorpo-\nrates a shallow-deep module and a synchro-\nnized parallel decoding. Our framework en-\nables faster inference by synchronizing the de-\ncoding process of the current token with previ-\nously stacked early-exited tokens. Furthermore,\nas parallel decoding allows us to observe pre-\ndictions from both shallow and deep models,\nwe present a novel adaptive threshold estimator\nthat exploits a Beta mixture model to determine\nsuitable confidence thresholds. We empirically\ndemonstrated the superiority of our proposed\nframework on extensive generation tasks.\n1 Introduction\nRecent advancements in autoregressive language\nmodels (Brown et al., 2020; Raffel et al., 2020;\nHoffmann et al., 2022; Touvron et al., 2023) have\nrevolutionized the quality of language generation\nin various generative tasks, including question\nanswering (Rajpurkar et al., 2016a), summariza-\ntion (Nallapati et al., 2016; Fabbri et al., 2019b),\nand machine translation (Cettolo et al., 2017a).\nNevertheless, these large transformer models have\nshown high inference latency due to the consider-\nable number of layers and the autoregressive de-\ncoding step. As the multiple stacks of transformer\nlayers have to be computed sequentially for each\nindividual token, the inference process poses sig-\n‚àóequal contribution ‚Ä†corresponding authors\nnificant computational burdens and hinders their\nreal-time adaptability (Jiao et al., 2020).\nIn light of the necessity to expedite inference la-\ntency, the early-exiting framework (Elbayad et al.,\n2020; Liu et al., 2021; Schuster et al., 2022)\nemerges as a promising approach that dynamically\nallocates computation paths based on the complex-\nity of generation for each token. As illustrated in\nFigure 1a, tokens that are relatively easy to predict\nthe subsequent token yield consistent predictions\nwith only a few layer computations, while those\nwith higher difficulty require computations across a\nlarger number of layers to generate accurate predic-\ntions. In an ideal scenario, the early-exiting method\nempowers models to achieve notable acceleration\nin inference without compromising the generation\nquality when compared to that of a full model.\nHowever, our extensive analysis identified four\nchallenges in the early-exiting framework. Firstly,\ndespite the potential to exit at earlier layers, key\nand value states for remaining layers are still re-\nquired for processing subsequent tokens. While\nprevious works have proposed the state copying\nmechanism (Elbayad et al., 2020; Schuster et al.,\n2022) to efficiently compute these states by reusing\nhidden states from the early-exited layer, our find-\nings reveal that this method performs poorly with\nlarger models and longer output sequences (see Sec-\ntion 4.1). Additionally, setting all layers as possible\nexit positions does not guarantee faster inference\ndue to (1) the defective performance of earlier lay-\ners that can generate abnormally long sequence\noutputs, and (2) the computational overhead from\nconfidence measurement at every layer (see Sec-\ntion 4.2 and 4.3). Lastly, achieving the desired\nlevel of latency and accuracy with early-exiting\nheavily depends on selecting the appropriate con-\nfidence threshold for the target task. This often\nentails significant efforts and additional computa-\ntional overhead (see Section 4.4). Hence, these\nchallenges call for a new approach that consistently\n5910\nùê°!\" ùê°\"! ùê°#$%! ùê°&!\n<S>Earth\nùê°#!\nùê°#\"\nis flat\nùê°&!\nLayer ùüèùê°!!\nùê°\"!\nùê°&!\nEarthis flat<E>Layer ùë≥\nùê°!\"\nü§®\nLayer ùüêùê°\"!\n(a) Conventional Early-Exiting\nisDobby\nùê°!\"#! ùê°$\"#! ùê°%\"#!\n<S>Earth\nùê°$! ùê°%!\nùê°!$ ùê°%$\nis\nùê°&$\nLayer ùüèùê°!!\nùê°$$\nEarthis round<E>\n0.850.950.1Conf (shallow) :Identical? :\nCalibration Set (ùììùíÑ)\nBeta Mixture Modelfind MLE\nFalseTrueTrue\nEarthis flatPred(shallow) :EarthisPred(deep) :Layer ùë≥(deep)\nü§©\nround\nround\nLayer ùüê(shallow)ùê°&! (b) Fast and Robust Early-Exiting (FREE)\nFigure 1: Overview of our FREE framework compared to the conventional early-exiting framework. FREE exhibits\nthree key differences: (1) FREE employs a shallow-deep module that utilizes two exit points instead of employing\nall layers as exit points, (2) FREE replaces the state copying mechanism (yellow colored) with synchronized parallel\ndecoding (red colored) to prevent performance degradation while accelerating inference speed, and (3) FREE utilizes\nan adaptive threshold estimator to determine the appropriate threshold values for each dataset during inference.\ndemonstrates high performance and low latency\nacross diverse language models and datasets.\nIn this paper, we introduce a Fast and Robust\nEarly-Exiting ( FREE) framework that incorpo-\nrates a shallow-deep module and synchronized par-\nallel decoding. Our framework not only offers con-\nsistent speedup and performance even for larger\nmodels and longer output sequences, but also elim-\ninates the need for the computationally expensive\nprocess of finding the appropriate exit threshold.\nSpecifically, the shallow-deep module bifurcates\nthe computation paths into a shallow model (with a\nspecified number of early layers) and a deep model\n(including all layers). Our synchronized parallel\ndecoding accumulates consecutive early-exited to-\nkens that only pass through the shallow model until\na non-exiting token is encountered. Thereby, we\nsynchronize the decoding process of the current\nnon-exiting token with the previously stacked to-\nkens, as shown in the left of Figure 1b. This pre-\nvents performance degradation by utilizing actual\nattention computed key and value instead of approx-\nimated states through state copying, while it also\nachieves a more efficient approach compared to de-\ncoding each token autoregressively. Furthermore,\nwe devise a novel adaptive threshold estimator, as\nshown in the right of Figure 1b, by leveraging the\nfact that parallel decoding outputs predictions even\nfor early-exited tokens from the deep model. This\nestimator uses a Beta mixture model (BMM) to cap-\nture the correlation between confidence scores and\nprediction alignment of two models, determining\nthe proper confidence threshold for each dataset.\nIn practice, we demonstrate the efficiency of our\nFREE framework on extensive generation tasks.\n2 Related Work\n2.1 Early-exiting Framework\nAs the size of language models has significantly\nincreased, there have been numerous efforts to de-\nvelop efficient decoding methods that reduce the\ncomputational cost of language generation tasks.\nMotivated by prior literature (Teerapittayanon et al.,\n2016; Graves, 2016; Zhang et al., 2019a), Elbayad\net al. (2020) introduced an early-exiting framework\nfor faster inference, which dynamically adjusts the\ndepth of the decoder for each token generation by\nmaking predictions at an intermediate layer. To\nachieve the better trade-off between speed and ac-\ncuracy, Schuster et al. (2022) recently explored\nconfidence thresholding methodologies, including\nvarious confidence measures, a decaying threshold\nfunction, and a calibration method.\nHowever, their experiments were primarily con-\nducted on small-sized decoder models, necessitat-\ning further validation on larger models. In addition,\ntheir approaches require additional training time for\nstatistical tests on the extra calibration sets, which\nprevents them from real deployment scenarios.\n2.2 Parallel Decoding\nThe non-autoregressive decoding, which gener-\nates multiple output tokens in parallel, was ini-\ntially proposed by Gu et al. (2018). Several works\n(Ghazvininejad et al., 2019; Gu and Kong, 2021;\nSavinov et al., 2022; Santilli et al., 2023) have\nsince focused on enhancing generation quality in\nmachine translation tasks. Subsequently, Leviathan\net al. (2023) introduced speculative decoding for\nsequence generation tasks. In this approach, an\n5911\napproximation model (small size) predicts outputs\nautoregressively, while a target model (large size)\nruns in parallel to verify the acceptance of predic-\ntions made by the approximation model. With only\naccepted tokens, they resample the next token from\nan adjusted distribution. Related approaches have\nbeen proposed by Chen et al. (2023) and Kim et al.\n(2023), where they also utilize two models of vary-\ning depth and focus on refining the small model\nthrough speculative sampling or a rollback policy\nin a non-autoregressive manner.\nOur approach is notably distinct from the afore-\nmentioned works as we focus on early-exiting\nframework by introducing synchronized parallel de-\ncoding within a single network, which incorporates\na shallow-deep module. While we also leverage the\nadvantage of simultaneously obtaining predictions\nfrom models of different depth, we rather aim to de-\nvelop a novel and effective estimation methodology\nto adaptively determine the optimal threshold for\neach dataset. It is worth noting that their refining\nstrategies may result in unbounded latency increase\nas they restart from incorrect predictions.\n3 Preliminary\nA Transformer network (Vaswani et al., 2017) is\ncomposed of Llayers, where each layer consists\nof two sublayers, a multi-head attention (MHA)\nlayer and a feed-forward network (FFN) layer. The\ncomputation for hidden states at time step t+ 1via\nstacked Transformer blocks is as follows:\nh‚Ñì\nt+1 = Transformer‚Ñì(h‚Ñì‚àí1\nt+1), ‚Ñì‚àà[1,L],\nwhere h0\nt+1 is the embedding layer outputs of yt\nthat represents the generated token at time step t.\nAfter Lth layer of the decoder network, the pre-\ndicted token ÀÜyt+1, is determined by the probability\noutput from a softmax classifier WL:\np(yt+1|hL\nt+1) = softmax(W‚ä∫\nLhL\nt+1)\nHowever, unlike the standard LMs, the early-\nexiting framework enables the generation of\na subsequent token in earlier layers by using\np(yt+1|h‚Ñì\nt+1). If the confidence score c‚Ñì is larger\nthan the predefined threshold, we can make a pre-\ndiction at time step t+ 1 as arg maxp(yt+1|h‚Ñì\nt+1).\nWhile classifiers can be parameterized indepen-\ndently or shared across the Llayers, most early-\nexiting methods (Elbayad et al., 2020; Liu et al.,\n2021; Schuster et al., 2022) utilize the shared clas-\nsifier due to its large number of parameters caused\nby enormous vocabulary size.\nTable 1: Comparison of ROUGE-L scores between a\nfull model, fine-tuned using all layer outputs, andoracle-\nexiting. We also measured cosine similarity between\nhidden states of the last layer and oracle-exited layer.\nDataset Model Full M. Oracle Sim.\nSAMSum T5-small 44.84 44.17 (-0.67)0.913\nT5-large 48.82 47.58 (-1.24)0.809\nCNN/DM T5-small 37.82 37.60 (-0.22)0.902\nT5-large 41.15 40.15 (-1.00)0.792\nMulti-NewsLongT5-base37.62 29.63 (-7.99)0.724\nBIGPATENTLongT5-base49.68 44.99 (-4.69)0.686\nAfter the current token is early-exited at the\n‚Ñìth layer, we need to calculate the key and value\nstates for all deeper blocks in order to perform\nthe self-attention for the subsequent tokens that\npass through deeper blocks. For a more efficient\napproach of caching key and value states, the early-\nexiting frameworks employ the state copying mech-\nanism. It duplicates the hidden states of the early-\nexited layer (i.e., hi\nt+1 = h‚Ñì\nt+1,‚àÄi ‚àà[‚Ñì+ 1,L]),\nallowing us to compute the approximate key and\nvalue states required for the self-attention of Trans-\nformer networks. Schuster et al. (2022) have veri-\nfied that state copying from lower layers does not\nhave a detrimental effect on performance in the\ncase of small-sized T5 models (Raffel et al., 2020).\n4 Re-evaluating Early-exit Framework\nIn this section, we present four new findings from\nour re-evaluation of the early-existing framework.\nWe utilized different model sizes of T5 (Raf-\nfel et al., 2020) on SAMSum (Gliwa et al.,\n2019) and CNN/DailyMail (See et al., 2017),\nand LongT5-base (Guo et al., 2022) architectures\non Multi-News (Fabbri et al., 2019a) and BIG-\nPATENT (Sharma et al., 2019).\n4.1 Lack of Robustness to Model Size and\nOutput Sequence Length\nWe first re-evaluate the state copying mechanism\nwhich is an essential component of the early-exiting\nframework. Following Schuster et al. (2022), we\nuse an oracle confidence measure that enables to-\nkens to exit at the earliest layer, such that their pre-\ndictions are identical to those of the final layer. No-\ntably, as observed in Table 1,the degradation of the\ngeneration quality with the state copying gets se-\nvere on larger models and datasets with the longer\nsequence (‚ñ∑ Obs. 1). For instance, when consider-\ning the oracle-exiting results, the T5-small model\n5912\ndemonstrates the degradation of only 0.67 on the\nSAMSum dataset, whereas the T5-large model ex-\nperiences a much larger drop of 1.24. Similarly,\non datasets such as Multi-News and BIGPATENT,\nwhich consist of relatively long output sequences,\nthe oracle-exiting results exhibit a decrease of 7.99\nand 4.69, respectively.\nTo strengthen the supporting evidence, we fur-\nther discover the substantial variation in the distri-\nbution of hidden states across different layers. In\nTable 1, we also reported cosine similarity between\nthe hidden states of the final layer and the oracle-\nexited layer. Even though the hidden states of the\nfinal and oracle-exited layers yield the same predic-\ntions, the cosine similarity between them decreases\nsignificantly as the decoder network gets larger and\nthe output sequences become longer.\n4.2 Performance Drop by Exit Position\nTo facilitate early-exiting for all decoder layers, the\ntraining objectives need to be a combination of the\ntraining objectives for each individual layer. We\ncan present as follows:\nL=\nL‚àë\ni=1\nŒ±iLi where\n‚àë\ni\nŒ±i = 1, (1)\nLi and Œ±i is negative log-likelihood loss function\nand weight coefficient forith layer, respectively. Es-\npecially, previous work set Œ±i as 1/L(unweighted\naverage; Elbayad et al. 2020) or i/‚àë\nii(weighted\naverage; Schuster et al. 2022). They demonstrated\nthat these weighting rules effectively facilitate\nlearning in earlier layers without compromising\nthe overall performance of the full model on small-\nsized decoder models.\nHowever, as shown in Figure 2, we observed\na notable decrease in the performance of static-\nexiting, which utilizes the same number of layers\nfor all tokens, when utilizing only a small por-\ntion of the early layers from the T5-large model.\n(‚ñ∑ Obs. 2). For instance, if all tokens are exited\nin the first or second layers, the model achieved\nnearly zero ROUGE-L scores. Furthermore, when\nwe apply the early-exiting framework to these mod-\nels during inference, we verified that the T5-large\nmodel generates abnormally long sentences, actu-\nally consuming more inference time. Based on\nthese results, in the subsequent experiments, we\nhave excluded the first two or four layers from the\ncandidates for early-exiting layers of base and large\nmodels, respectively.\nLayer Index Layer Index\nROUGE-L Gen. Length\nROUGE-L ( )\n0\n10\n20\n30\n40\n50\nGen. Length\n0\n25\n50\n75\n100\n1 2 3 4 5 6 1 2 3 4 5 6 9 12 18 24\nFigure 2: Illustration of the ROUGE-L scores and gen-\nerated sequence length from the static-exiting approach\nin T5-small (left) and T5-large (right) on the SAMSum\ndataset. The horizontal dashed line represents the aver-\nage sequence length of the ground truth.\n[SA] KV\n[SA] Attn\n[CA] KV\n[CA] Attn\nFFN\nConf\n41.2\n48.8\n47.8\n46.8\n45.0\n40.7\n40.2\n37.6\n37.4\n36.9\n35.9\n39.2\nNormalized Latency\n0\n0.5\n1.0\n1.5\nSAMSum CNN/DM Multi-News\nFigure 3: Component-wise computational cost on three\ndatasets. Four bars correspond to full model and early-\nexiting with thresholds of 0.9, 0.7, and 0.5. The hatched\ncolor denotes the elapsed time after the token exits,\nrelated to the state copying mechanism. The numbers\nabove the bars represent the ROUGE-L scores. SA and\nCA denote self- and cross-attention, respectively.\n4.3 Non-negligible Computational Cost\nDuring our analysis, we observed that the conven-\ntional early-exiting framework not only presents\nperformance disadvantages but also poses chal-\nlenges for inference latency. In Figure 3, we con-\nducted a breakdown of the computational costs\nassociated with a decoder model across three sum-\nmarization datasets. Surprisingly, early-exiting has\noften shown an unexpected increase in total de-\ncoding time when compared to the baseline model\nwithout using early-exiting.\nThis can be attributed to the non-negligible com-\nputational cost involved in measuring confidence\nat each layer, particularly due to the softmax oper-\nations with the large vocabulary size. In addition,\nalthough the state copying method aims to reduce\ncomputation time in the MHA and FFN layers of\nthe remaining layers, the computation of key and\nvalue states using duplicated hidden states incurs\nadditional non-negligible overhead (‚ñ∑ Obs. 3).\n5913\nTable 2: The optimal confidence threshold to achieve\ndesired performance. We chose the best values among\nthreshold value from 0 to 1 with step size of 0.1. The\nnumbers sequentially represent the selected threshold\nand corresponding performance (gray colored).\nPerformance Drop\nTask Dataset ‚àº1% ‚àº5% ‚àº10%\nSUM\nSAMSum 1.0 (48.8) 0.7 (46.8) 0.5 (45.0)\nCNN/DM 1.0 (41.2) 0.5 (39.2) 0.3 (37.3)\nMulti-News 0.8 (37.3) 0.5 (35.9) 0.4 (34.9)\nBIGPATENT1.0 (49.7) 0.8 (47.3) 0.6 (45.2)\nQA SQuAD 0.1 (90.1) 0.0 (88.3) 0.0 (88.3)\nMT IWSLT 1.0 (39.4) 1.0 (39.4) 1.0 (39.4)\n4.4 Disparate Optimal Confidence Threshold\nDetermining the appropriate threshold for exit con-\nfidence is a crucial challenge in the early-exiting\nframework as it directly impacts the trade-off\nbetween performance and latency (Zhang et al.,\n2019b; Schuster et al., 2022). As summarized in\nTable 2, our observations indicate that the opti-\nmal confidence thresholds for achieving the low-\nest latency in the same performance significantly\nvary across datasets (‚ñ∑ Obs. 4 ). For instance,\nSQuAD and CNN/DailyMail datasets can maintain\nperformance with relatively lower exit thresholds,\nwhereas higher threshold values are required in the\ncase of the IWSLT dataset. Previous work (Schus-\nter et al., 2022) has leveraged distribution-free risk\ncontrol techniques for confident generations. How-\never, these methods require additional training time\nfor statistical tests on the extra calibration set be-\nfore the deployment, where time can be also influ-\nenced by the size of the threshold candidate sets.\n5 Novel Early-Exiting Framework: FREE\nBuilding upon the discoveries in Section 4, we in-\ntroduce a Fast and Robust Early-Exiting framework\nnamed FREE, leveraging a shallow-deep module\nand capitalizing on the structure of parallel decod-\ning. Furthermore, we present a confidence estima-\ntion algorithm designed to enhance the robustness\nof early-exiting within the FREE framework.\n5.1 Shallow-Deep Module\nWe present an effective shallow-deep module,\nwhich strategically assigns a predetermined num-\nber of early layers (LS) as a shallow model, while\nall the layers as a deep model. This module tack-\nles the performance degradation associated with\n<S>   Artificial   intelligence   is   the   new   electricity<S>   Artificial   intelligence   is   the   new<S>   Artificial   intelligence   is   the<S>   Artificial   intelligence   is<S>   Artificial   intelligence<S>   Artificialùíï=ùüèùíï=ùüêùíï=ùüëùíï=ùüíùíï=ùüìùíï=ùüî\n<S>\n<S>   Artificial   intelligence   is   the   new   electricity   <E>\nùíï=ùüé\nùíï=ùüï\ndeep modelshallow model\nFigure 4: Overview of synchronized parallel decoding.\nWe colored the tokens used to generate the next token\nbased on the model that they forward.\nco-training numerous exiting layers in the conven-\ntional early-exiting framework.\nTo enhance the performance of the shallow\nmodel, we exploit layerwise knowledge distilla-\ntion (KD) as an additive loss term to Eq. (1) with\nŒ±Ls = Ls/(L+ Ls) and Œ±L = L/(L+ Ls):\nLKD = 1\n|LS|\nLS‚àë\ni=1\nMSE(Hi\nS,Hm(i)\nD ),\nwhere m(i) indicates the layer in the deep model\nthat extracts knowledge into the corresponding\nlayer i of the shallow model. HS and HD are\nhidden states from shallow and deep models.\nWe have experimented with the distillation from\nthe last layer (KD-last; Wang et al. 2020; Ko et al.\n2023), from fixed uniform mapped layers (KD-unif;\nJiao et al. 2020; Park et al. 2021), and from dynam-\nically mapped layers (KD-dyna; Xia et al. 2022).\nEspecially, dynamic mapping function allows us to\nalign each deep model layer with its closest coun-\nterpart in the shallow model:\nm(i) = arg min\nj\nMSE(Hi\nS,Hj\nD)\nwhere jdenotes the layer indices of the deep model\nselected by the total number of LS, and the condi-\ntion of m(1) ‚â§¬∑¬∑¬∑‚â§ m(LS) should be satisfied.\nBased on the consistently superior performance of\nKD-dyna loss (see Appendix D.2), we utilized it\nfor all experiments with the shallow-deep module.\n5.2 Synchronized Parallel Decoding\nWe present synchronized parallel decoding as an al-\nternative to the state copying mechanism, which is\na key component of the conventional early-exiting\nframework but can lead to a significant perfor-\nmance decrease, as demonstrated in Section 4.1. In\ncontrast to traditional approaches that have multiple\nexit points, our method incorporates the shallow-\ndeep module, enabling us to stack consecutive\n5914\nearly-exited tokens in the shallow model until a\nnon-exiting token is encountered. When decoding\nthe token with the deep model, we enhance effi-\nciency and effectiveness through parallel decoding,\nsynchronously computing the key and value states\nof previously stacked tokens. The example of the\nparallel decoding process is depicted in Figure 4.\nThe underlying principle of this approach is to\nleverage the enhanced parallelism offered by mod-\nern hardware accelerators. This allows for efficient\ncomputations to be carried out simultaneously on\nthe large number of sequences. Thus, by employ-\ning synchronized parallel decoding, we can directly\ncompute multiple hidden states similar to a single\ntoken processing time. Besides, this can eliminate\nthe potential performance degradation that may\narise from inaccurate approximations of hidden\nstates resulting from the state copying mechanism.\n5.3 Adaptive Threshold Estimation\nWe propose a novel adaptive threshold estimation\nmethod that updates the threshold to be retailed\nfor different datasets. Unlike the previous methods\nthat utilize extra calibration sets (Schuster et al.,\n2022), we quickly adapt the threshold by using\nthe information of early-stage instances, regardless\nof the initial threshold values. Especially, during\nparallel decoding, we collect samples to evaluate\nthe correspondence between the confidence scores\nof the shallow model and the prediction alignment\nbetween shallow and deep models.\nAs depicted in Figure 1b, we observe that when\nthe predictions of the deep and shallow models are\nidentical, the confidence tends to skew towards one,\notherwise it skews towards zero. To model this\nskewed distribution over [0,1], we utilize a beta\nmixture model (BMM; Ma and Leijon 2011) due to\nits flexibility and the appropriate support set of the\nbeta distribution. The probability density function\nof beta distribution over x‚àà[0,1] is defined as:\np(x|Œ±,Œ≤) = Œì(Œ±+ Œ≤)\nŒì(Œ±)Œì(Œ≤)xŒ±‚àí1(1 ‚àíx)Œ≤‚àí1\nThe parameters of the BMM are updated using\nthe maximum likelihood estimator (MLE; Norden\n1972) with observed data points.\nŒ±k = ¬Øck\n(\n¬Øck(1‚àí¬Øck)\ns2\nk\n‚àí1\n)\n,Œ≤k = Œ±k(1‚àí¬Øck)\n¬Øck\n, (2)\nwhere ¬Øck being a average of the confidence\n{cLs\ni }|Dc|\ni=1 for corresponding k. kis set to 1 if the\npredictions of the two models are identical, and 0\nAlgorithm 1 Adaptive Threshold Estimation\nInput: empty calibration dataset Dc, initial confi-\ndence threshold Œª0\nc, posterior condition Œ∂, update\nnumber T\nOutput: updated confidence threshold Œªc\n1: initialize t‚Üê0, Œªc ‚ÜêŒª0\nc\n2: while t‚â§T do\n3: Generate tth sentence with Nt tokens\n4: /* Update Dc */\n5: Dc ‚ÜêDc ‚à™{cLs\ni ,I(ÀÜyLs\ni = ÀÜyL\ni )}Nt\ni=1\n6: /* Find threshold with Eq.(2)-(4)*/\n7: Œ±k,Œ≤k ‚ÜêMLEBMM(Dc) for k‚àà{0,1}\n8: Œªc ‚Üêarg minŒª:p(k=1|Œª)‚â•Œ∂ Œª\n9: update t‚Üêt+ 1\n10: end while\notherwise. Similarly, sk is the standard deviation\nof confidence of related k.\n¬Øck =\n‚àëN\ni=1 Œ≥icLs\ni‚àëN\ni=1 Œ≥i\n,¬Øs2\nk =\n‚àëN\ni=1 Œ≥i(cLs\ni ‚àí¬Øck)2\n‚àëN\ni=1 Œ≥i\n, (3)\nwhere Œ≥i := I(ÀÜyLs\ni = ÀÜyL\ni ) denote whether the pre-\ndiction of two models are same.\nAfter updating the BMM, we find an appropriate\nthreshold for future tokens by identifying the point\nat which the posterior probability, defined as below,\nreaches Œ∂:\np(k = 1|Œªc) = p(k=1)p(Œªc|Œ±1,Œ≤1)‚àë\nj‚àà{0,1}p(k=j)p(Œªc|Œ±j ,Œ≤j ) . (4)\nHere, as we observe the severe imbalance between\nthe case of k= 0 and 1, we restrict the prior value\nof each class to 0.5 for the balance between two\ncases (i.e., p(k= j) = 0.5 ‚àÄj). As this restriction\nmakes us to use a smaller value of Œ∂, we na√Øvely\nset it as 0.4. A detailed algorithm can be found in\nAlgorithm 1.\n6 Experiments\n6.1 Experimental Setup\nWe conducted experiments on various sequence\nmodeling tasks, including question answering\n(SQuAD; Rajpurkar et al. 2016b), machine transla-\ntion (IWSLT 2017 En-De; Cettolo et al. 2017b),\nand text summarization tasks using SAMSum,\nCNN/DailyMail, Multi-News, and BIGPATENT\ndatasets. The LongT5-base model was used for\nthe Multi-News and BIGPATENT datasets, while\nthe T5-large model was used for the other datasets.\nAll implementations are based on PyTorch using\nHuggingface (Wolf et al., 2020; Lhoest et al.,\n2021). Further details can be found in Appendix B.\n5915\nROUGE-L ( )\n35\n40\n45\n50\nNormalized Latency\n0.2 0.4 0.6 0.8 1.0\n30\n35\n40\nNormalized Latency\n0.2 0.4 0.6 0.8 1.0\n25\n30\n35\n40\nNormalized Latency\n0.6 0.8 1.0\n30\n40\n50\nNormalized Latency\n0.2 0.4 0.6 0.8 1.0\nStatic-exit CALM FREE‚Ä† FREE\n(a) SAMSum (b) CNN/DailyMail (c) Multi-News (d) BIGPATENT\nFigure 5: The trade-off between the generated output quality and normalized latency under different exit conditions.\nWe varied the exit threshold values between 0 and 1 for both CALM and FREE‚Ä† and the number of exit layers for\nthe static-exiting framework. We exclude the inner point of the Pareto curve, and the dashed line represents the\nROUGE-L score of the full model, which is the fine-tuned shallow-deep module.\nTable 3: Comparison between early-exiting frameworks on various datasets. For CALM and FREE‚Ä†, we reported\nthe performance using the smallest threshold value that achieves 99% performance of the full model, fine-tuned by\nweighted average or KD-dyna losses, respectively. The parentheses denote relative speedup based on the first row.\nSUM QA MT\nMethod SAMSum CNN/DailyMail Multi-News BIGPATENT SQuAD IWSLT De-En\nFull Model 48.82 (√ó1.00) 41.15 ( √ó1.00) 37.62 ( √ó1.00) 49.68 (√ó1.00) 90.63 (√ó1.00) 39.19 (√ó1.00)\nCALM 48.37 (√ó0.72) 40.78 ( √ó0.86) 37.27 ( √ó0.85) 49.21 (√ó0.65) 90.09 (√ó2.03) 39.19 (√ó1.00)\nFull Model 49.11 (√ó1.00) 41.09 ( √ó1.00) 39.20 ( √ó1.00) 49.68 (√ó1.00) 91.90 (√ó1.00) 39.39 (√ó1.00)\nFREE‚Ä† 48.65 (√ó1.50) 40.89 ( √ó1.80) 38.93 ( √ó1.07) 49.51 (√ó1.62) 91.31 (√ó2.76) 39.04 (√ó1.07)\nFREE 48.66 (√ó1.47) 40.99 ( √ó1.65) 38.66 ( √ó1.23) 49.47 (√ó1.58) 91.82 (√ó2.16) 38.17 (√ó1.18)\n6.2 Experimental Results\nIn order to investigate the effect of the individual\ncomponent of our proposed framework, we evalu-\nate both FREE without and with an adaptive thresh-\nold estimator, denoted as FREE‚Ä†and FREE.\nOverall performance. In Figure 5, we present\na comparison of the quality of generated output\n(ROUGE-L) and the inference latency between the\nFREE framework and baselines, including static-\nexiting and the conventional early-exiting method\n(CALM; Schuster et al. 2022). CALM method\nexhibited poorer performance compared to a sim-\nple static-exiting approach on all datasets, likely\ndue to the state copying mechanism and the pres-\nence of numerous exit positions, as observed in\nSection 4. In contrast, FREE‚Ä†demonstrated robust\nperformance and the larger AUC (area under the\ncurve) across datasets by adjusting exit thresholds.\nAdaptive threshold evaluation. In the early-\nexiting framework, choosing the appropriate con-\nfidence threshold is crucial for achieving the best\ntrade-off between generation quality and latency.\nUnlike previous calibration methods (Schuster\net al., 2022) that require an extra calibration set\nROUGE-L ( )\n40\n45\n50\nNormalized Latency\n0.2 0.4 0.6 0.8 1.0\n32\n34\n36\n38\n40\n42\nNormalized Latency\n0.2 0.4 0.6 0.8 1.0\nStatic-exit CALM FREE‚Ä† FREE\n(a) SAMSum (b) CNN/DailyMail\nFigure 6: The trade-off between the generated output\nquality and normalized latency on T5-3B models.\nand training time, our methodology effectively ad-\ndresses this challenge by leveraging the byprod-\nuct of parallel decoding. As summarized in Ta-\nble 3, FREE with adaptive threshold estimation\nsuccessfully achieved significant speedup, by up\nto √ó2.16, when preserving the 99% of full model\nperformance. Furthermore, in Figure 5, the esti-\nmated threshold demonstrated nearly the maximum\nachievable speed improvement without sacrificing\nperformance, represented by red stars.\nLarge language models. Recently, various stud-\nies (Dettmers et al., 2022; Xiao et al., 2023;\nLeviathan et al., 2023; Liu et al., 2023b) have\n5916\nTable 4: The comparison of ROUGE-L and speedup\nbased on different numbers of layers for shallow model\nand confidence thresholds.\nThreshold\nDataset LS 0.7 0.5 0.3\nSAMSum\n4 48.27 (√ó1.04) 46.95 (√ó1.09) 44.72 (√ó1.15)\n6 48.89 (√ó1.32) 48.65 (√ó1.50) 47.60 (√ó1.80)\n8 48.74 (√ó1.11) 47.97 (√ó1.17) 47.09 (√ó1.31)\n12 48.97(√ó1.21)48.74(√ó1.28)48.10(√ó1.37)\nCNN/DM\n4 41.03 (√ó1.45) 40.59 (√ó1.68) 39.88 (√ó1.86)\n6 41.08 (√ó1.53) 41.00 (√ó1.69) 40.60 (√ó2.07)\n8 41.19(√ó1.44)41.15(√ó1.64) 40.95 (√ó1.69)\n12 41.11 (√ó1.33) 41.09 (√ó1.47)40.95(√ó1.55)\naimed at boosting the inference speed of large\nlanguage models (LLMs). To validate the ap-\nplicability of the FREE framework on LLMs,\nwe conducted experiments utilizing the T5-3B\nmodel (Raffel et al., 2020) on the SAMSum and\nCNN/DailyMail datasets. Due to substantial com-\nputational overhead, we utilized the LoRA adapter\n(Hu et al., 2022), targeting both self-attention and\nfeed-forward layers with a rank of 64. Figure 6\nsummarized a comprehensive comparison of early-\nexiting methods. Our method maintained superi-\nority over the baselines in terms of latency and\nROUGE-L scores, showing the consistent perfor-\nmance trend observed in the T5-large model. Thus,\nwe are confident that our proposed framework\nwould demonstrate consistent level of inference\nacceleration, even with larger language models.\n6.3 Ablation Study\nDifferent depth of shallow model. In Table 4,\nwe also ablate on the number of layers for the\nshallow model to observe the trade-offs. While\nour method demonstrated a trend towards higher\nspeedup gains as the depth of the shallow model\ndecreases, we experienced some decreases in per-\nformance and speed gain when the depth of the\nmodel is reduced too much (e.g., four layers). We\nassumed that this is due to incorrect and redundant\noutput sentences, similarly observed in the conven-\ntional early-exiting framework. Consequently, with\nenough depth (e.g., six layers), FREE consistently\nshowed robust performance and inference speedup.\nRobustness of parallel decoding. In order to\nverify the robustness of our decoding mechanism,\nwe conducted a comparative analysis between syn-\nchronized parallel decoding (SPD) and state copy-\ning (SC), both implemented with the shallow-deep\nTable 5: The comparison between synchronized parallel\ndecoding (SPC) and state copying (SC). The shallow-\ndeep module is utilized in both decoding methods.\nMethod Threshold\nDataset SC SPD 0.9 0.7 0.5 0.3 0.1\nSAMSum ‚úì ‚úó 46.35 44.59 43.92 42.36 41.27\n‚úó ‚úì 48.89 48.89 48.65 47.60 45.27\nCNN/DM ‚úì ‚úó 40.92 40.92 40.71 39.99 38.17\n‚úó ‚úì 41.12 41.08 41.00 40.60 39.30\nMulti-News‚úì ‚úó 38.43 37.61 36.55 33.99 29.34\n‚úó ‚úì 39.16 39.06 38.78 37.87 33.98\nTable 6: The experimental results of FREE framework\nbased on different sizes of the calibration set.\n3% 10% 100%\nDataset Thr. Perf. SpeedThr. Perf. SpeedThr.\nSAMSum 0.51 48.66√ó1.470.49 48.69√ó1.51 0.48\nBIGPATENT0.54 49.47√ó1.580.54 49.39√ó1.63 0.54\nmodule. Synchronized parallel decoding consis-\ntently outperformed state copying across all three\ndatasets by much higher ROUGE-L metrics, as\nsummarized in Table 5. This improvement can be\nattributed to the updated hidden states that are ob-\ntained through the accurate computation of Trans-\nformer layers during parallel decoding. These find-\nings suggest that our efficient decoding method for\nearly-exited tokens can enhance the overall perfor-\nmance of the early-exiting framework as well.\nDependency on size of calibration set. By using\nthe early-stage instances as the calibration set, we\niteratively update the adaptive confidence threshold\nto converge to the appropriate value. Here, we\nhave observed the sample efficiency of the adaptive\nthreshold estimator by varying the sizes of this\ncalibration set. Interestingly, even with only 3% of\nthe total samples, our estimator can approximate\nthe threshold, measured by the full sample set, as\nshown in Table 6. This ensures minimal additional\ncomputation time required for threshold estimation.\nRefining shallow model predictions. Prior\nworks (Leviathan et al., 2023; Chen et al., 2023;\nKim et al., 2023) have proposed refinement meth-\nods to correct erroneous outputs from an approxi-\nmation model. Specifically, when a wrong token\nis detected in previous sequences, they remove all\nsubsequently generated tokens and restart the gen-\neration process from that point. In Table 7, we\nconducted experiments in order to evaluate the ef-\nfects of this refinement method (Kim et al., 2023)\n5917\nTable 7: The evaluation of refinement methods in the\nFREE framework. Refining thresholds control the level\nof acceptance for predictions from the shallow model.\nThr. 0.7 Thr. 0.3\nDataset Ref. Thr. Perf. Speed Perf. Speed\nSAMSum\n‚úó - 48.89 √ó1.33 47.60 √ó1.80\n‚úì 1.0 49.08 √ó1.26 48.31 √ó1.50\n‚úì 0.1 49.06 √ó1.17 48.27 √ó1.12\nCNN/DM\n‚úó - 41.08 √ó1.53 40.60 √ó2.07\n‚úì 1.0 40.86 √ó1.51 40.78 √ó1.67\n‚úì 0.1 40.85 √ó1.35 40.75 √ó1.21\nin our early-exiting framework. We observed that\nwhen the refinement threshold is set low, allowing\nfor more correction by the deep model, the per-\nformance improvement is minimal compared to\nthe increase in latency. Our findings suggest that\nthese approaches that cannot guarantee an upper\nbound on latency increase may not be well-suited\nfor integration into the early-exiting framework.\nHuman-like summarization evaluation. Recent\nstudies (Gao et al., 2023; Liu et al., 2023a; Zhang\net al., 2023) have argued that existing summariza-\ntion evaluation metrics like ROUGE-L do not ac-\ncurately represent the true summarization capabil-\nities. Instead, they explored the human-like eval-\nuation using LLMs based on their strong corre-\nlation with human judgment. Thereby, we con-\nducted two human-like evaluation methods, Likert\nscale scoring and pairwise comparison (Gao et al.,\n2023), using ChatGPT API (gpt-3.5-turbo-0613).\nWe compared a full model and our FREE frame-\nwork on 100 instances, randomly drawn from the\nCNN/DailyMail dataset. Figure 7 and 8 provide\nthe templates used for each evaluation task. For the\nfull model, we observed scores of [4.73, 3.83, 3.87,\n3.77], while our FREE method returned scores of\n[4.68, 3.84, 3.84, 3.72] across the four dimensions.\nBesides, the win counts for each method were 101\nand 99, respectively. Given ROUGE-L scores of\n41.09 (√ó1.00) for the full model and 40.99 (√ó1.65)\nfor the FREE method, our method is certainly capa-\nble of yielding predictions of similar quality, while\nnotably reducing computational overhead.\n7 Conclusion\nWe proposed FREE framework to address the chal-\nlenges of conventional early-exiting frameworks\nfor autoregressive language models. Our approach\nincorporates three key components: (1) shallow-\ndeep module, (2) synchronized parallel decoding,\nEvaluate the quality of summaries written for a news\narticle. Rate each summary on four dimensions:\n{Dimension_1}, {Dimension_2}, {Dimension_3},\nand {Dimension_4}. You should rate on a scale from\n1 (worst) to 5 (best).\nArticle: {Article}\nSummary: {Summary}\nFigure 7: The template for Likert scale scoring. The\nfour dimensions are relevance, informativeness, fluency,\nand coherence.\nGiven a new article, which summary is better?\nAnswer \"Summary 0\" or \"Summary 1\". You do not\nneed to explain the reason.\nArticle: {Article}\nSummary 0: {Summary_0}\nSummary 1: {Summary_1}\nFigure 8: The template for pairwise comparison. We\nmeasured twice by changing the order of summaries for\na fair comparison.\nand (3) adaptive threshold estimation. Through\nextensive experiments on various generation tasks,\nwe empirically demonstrated the superior perfor-\nmance of FREE framework, achieving significant\nacceleration in latency without compromising the\nquality of the generated output.\nLimitations. Our work addressed a fast and ro-\nbust existing framework that can be efficiently uti-\nlized without concerns about performance degrada-\ntion. However, our approach does have a few lim-\nitations which we discuss below: (1) Our method\nrequires additional computational resources to fine-\ntune the shallow model. However, as we have\ndemonstrated, parameter-efficient fine-tuning meth-\nods would be a promising solution to overcome this\nlimitation. (2) While our work demonstrates ro-\nbustness in the depth of the shallow model, further\ninvestigation is required to determine the appropri-\nate depth for various language models. This aspect\nremains an area for additional research.\nAcknowledgement. This work was supported by\nInstitute of Information & communications Tech-\nnology Planning & Evaluation (IITP) grant funded\nby Korea government (MSIT) [No. 2021-0-00907,\nDevelopment of Adaptive and Lightweight Edge-\nCollaborative Analysis Technology for Enabling\nProactively Immediate Response and Rapid Learn-\ning, 90%] and [No. 2019-0-00075, Artificial Intelli-\ngence Graduate School Program (KAIST), 10%].\n5918\nReferences\nSangmin Bae, Sungnyun Kim, Jongwoo Ko, Gihun\nLee, Seungjong Noh, and Se-Young Yun. 2021.\nSelf-contrastive learning: Single-viewed supervised\ncontrastive framework using sub-network. arXiv\npreprint arXiv:2106.15499.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nMauro Cettolo, Marcello Federico, Luisa Bentivogli,\nNiehues Jan, St√ºker Sebastian, Sudoh Katsuitho,\nYoshino Koichiro, and Federmann Christian. 2017a.\nOverview of the iwslt 2017 evaluation campaign. In\nProceedings of the 14th International Workshop on\nSpoken Language Translation, pages 2‚Äì14.\nMauro Cettolo, Marcello Federico, Luisa Bentivogli,\nJan Niehues, Sebastian St√ºker, Katsuhito Sudoh,\nKoichiro Yoshino, and Christian Federmann. 2017b.\nOverview of the IWSLT 2017 evaluation campaign.\nIn Proceedings of the 14th International Conference\non Spoken Language Translation, pages 2‚Äì14, Tokyo,\nJapan. International Workshop on Spoken Language\nTranslation.\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving,\nJean-Baptiste Lespiau, Laurent Sifre, and John\nJumper. 2023. Accelerating large language model\ndecoding with speculative sampling. arXiv preprint\narXiv:2302.01318.\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke\nZettlemoyer. 2022. Llm. int8 (): 8-bit matrix mul-\ntiplication for transformers at scale. arXiv preprint\narXiv:2208.07339.\nMaha Elbayad, Jiatao Gu, Edouard Grave, and Michael\nAuli. 2020. Depth-adaptive transformer. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nAlexander Fabbri, Irene Li, Tianwei She, Suyi Li, and\nDragomir Radev. 2019a. Multi-news: A large-scale\nmulti-document summarization dataset and abstrac-\ntive hierarchical model. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 1074‚Äì1084, Florence, Italy. Asso-\nciation for Computational Linguistics.\nAlexander R Fabbri, Irene Li, Tianwei She, Suyi Li,\nand Dragomir R Radev. 2019b. Multi-news: A\nlarge-scale multi-document summarization dataset\nand abstractive hierarchical model. arXiv preprint\narXiv:1906.01749.\nMingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Ship-\ning Yang, and Xiaojun Wan. 2023. Human-like sum-\nmarization evaluation with chatgpt. arXiv preprint\narXiv:2304.02554.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel\ndecoding of conditional masked language models.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 6111‚Äì6120.\nAssociation for Computational Linguistics.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\nsander Wawer. 2019. SAMSum corpus: A human-\nannotated dialogue dataset for abstractive summa-\nrization. In Proceedings of the 2nd Workshop on\nNew Frontiers in Summarization, pages 70‚Äì79, Hong\nKong, China. Association for Computational Linguis-\ntics.\nAlex Graves. 2016. Adaptive computation time\nfor recurrent neural networks. arXiv preprint\narXiv:1603.08983.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor O.K.\nLi, and Richard Socher. 2018. Non-autoregressive\nneural machine translation. In International Confer-\nence on Learning Representations.\nJiatao Gu and Xiang Kong. 2021. Fully non-\nautoregressive neural machine translation: Tricks of\nthe trade. In Findings of the Association for Com-\nputational Linguistics: ACL-IJCNLP 2021 , pages\n120‚Äì133, Online. Association for Computational Lin-\nguistics.\nMandy Guo, Joshua Ainslie, David Uthus, Santiago On-\ntanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang.\n2022. LongT5: Efficient text-to-text transformer for\nlong sequences. In Findings of the Association for\nComputational Linguistics: NAACL 2022, pages 724‚Äì\n736, Seattle, United States. Association for Compu-\ntational Linguistics.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch,\nElena Buchatskaya, Trevor Cai, Eliza Rutherford,\nDiego de las Casas, Lisa Anne Hendricks, Johannes\nWelbl, Aidan Clark, Tom Hennigan, Eric Noland,\nKatherine Millican, George van den Driessche, Bog-\ndan Damoc, Aurelia Guy, Simon Osindero, Karen\nSimonyan, Erich Elsen, Oriol Vinyals, Jack William\nRae, and Laurent Sifre. 2022. An empirical analysis\nof compute-optimal large language model training.\nIn Advances in Neural Information Processing Sys-\ntems.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text de-\ngeneration. In International Conference on Learning\nRepresentations.\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and\nWeizhu Chen. 2022. Lora: Low-rank adaptation of\nlarge language models. In The Tenth International\nConference on Learning Representations, ICLR 2022,\nVirtual Event, April 25-29, 2022. OpenReview.net.\n5919\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2020.\nTinyBERT: Distilling BERT for natural language un-\nderstanding. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pages 4163‚Äì\n4174, Online. Association for Computational Lin-\nguistics.\nSehoon Kim, Karttikeya Mangalam, Jitendra Malik,\nMichael W Mahoney, Amir Gholami, and Kurt\nKeutzer. 2023. Big little transformer decoder. arXiv\npreprint arXiv:2302.07863.\nJongwoo Ko, Seungjoon Park, Minchan Jeong, Sukjin\nHong, Euijai Ahn, Du-Seong Chang, and Se-Young\nYun. 2023. Revisiting intermediate layer distillation\nfor compressing language models: An overfitting\nperspective. In Findings of the Association for Com-\nputational Linguistics: EACL 2023, pages 158‚Äì175,\nDubrovnik, Croatia. Association for Computational\nLinguistics.\nYaniv Leviathan, Matan Kalman, and Yossi Matias.\n2023. Fast inference from transformers via spec-\nulative decoding. In International Conference on\nMachine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA, volume 202 of Proceedings\nof Machine Learning Research, pages 19274‚Äì19286.\nPMLR.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, Joe Davison, Mario Sasko, Gun-\njan Chhablani, Bhavitvya Malik, Simon Brandeis,\nTeven Le Scao, Victor Sanh, Canwen Xu, Nicolas\nPatry, Angelina McMillan-Major, Philipp Schmid,\nSylvain Gugger, Cl√©ment Delangue, Th√©o Matus-\nsi√®re, Lysandre Debut, Stas Bekman, Pierric Cistac,\nThibault Goehringer, Victor Mustar, Fran√ßois Lagu-\nnas, Alexander M. Rush, and Thomas Wolf. 2021.\nDatasets: A community library for natural language\nprocessing. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning: System Demonstrations, EMNLP 2021, Online\nand Punta Cana, Dominican Republic, 7-11 Novem-\nber, 2021, pages 175‚Äì184. Association for Computa-\ntional Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out, pages 74‚Äì81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang,\nRuochen Xu, and Chenguang Zhu. 2023a. Gpte-\nval: Nlg evaluation using gpt-4 with better human\nalignment. arXiv preprint arXiv:2303.16634.\nYijin Liu, Fandong Meng, Jie Zhou, Yufeng Chen, and\nJinan Xu. 2021. Faster depth-adaptive transformers.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 35, pages 13424‚Äì13432.\nZichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang\nYuan, Zhao Song, Anshumali Shrivastava, Ce Zhang,\nYuandong Tian, Christopher R√©, and Beidi Chen.\n2023b. Deja vu: Contextual sparsity for efficient\nllms at inference time. In International Conference\non Machine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA, volume 202 of Proceedings\nof Machine Learning Research, pages 22137‚Äì22176.\nPMLR.\nZhanyu Ma and Arne Leijon. 2011. Bayesian estimation\nof beta mixture models with variational inference.\nIEEE Transactions on Pattern Analysis and Machine\nIntelligence, 33(11):2160‚Äì2173.\nRamesh Nallapati, Bowen Zhou, C√≠cero Nogueira dos\nSantos, √áaglar G√ºl√ßehre, and Bing Xiang. 2016.\nAbstractive text summarization using sequence-to-\nsequence rnns and beyond. In Proceedings of the\n20th SIGNLL Conference on Computational Natural\nLanguage Learning, CoNLL 2016, Berlin, Germany,\nAugust 11-12, 2016, pages 280‚Äì290. ACL.\nRH Norden. 1972. A survey of maximum likelihood\nestimation. International Statistical Review/Revue\nInternationale de Statistique, pages 329‚Äì354.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311‚Äì318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nGeondo Park, Gyeongman Kim, and Eunho Yang. 2021.\nDistilling linguistic context for language model com-\npression. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 364‚Äì378, Online and Punta Cana, Dominican\nRepublic. Association for Computational Linguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances in\nneural information processing systems, 32.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485‚Äì5551.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016a. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\n5920\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016b. SQuAD: 100,000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2383‚Äì2392, Austin,\nTexas. Association for Computational Linguistics.\nAndrea Santilli, Silvio Severino, Emilian Postolache,\nValentino Maiorca, Michele Mancusi, Riccardo\nMarin, and Emanuele Rodol√†. 2023. Accelerating\ntransformer inference for translation via parallel de-\ncoding. In Proceedings of the 61st Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), ACL 2023, Toronto, Canada,\nJuly 9-14, 2023, pages 12336‚Äì12355. Association for\nComputational Linguistics.\nNikolay Savinov, Junyoung Chung, Mikolaj Binkowski,\nErich Elsen, and Aaron van den Oord. 2022. Step-\nunrolled denoising autoencoders for text generation.\nIn International Conference on Learning Representa-\ntions.\nTal Schuster, Adam Fisch, Jai Gupta, Mostafa Dehghani,\nDara Bahri, Vinh Tran, Yi Tay, and Donald Metzler.\n2022. Confident adaptive language modeling. Ad-\nvances in Neural Information Processing Systems ,\n35:17456‚Äì17472.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1073‚Äì\n1083, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nEva Sharma, Chen Li, and Lu Wang. 2019. BIG-\nPATENT: A large-scale dataset for abstractive and\ncoherent summarization. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 2204‚Äì2213, Florence, Italy. Asso-\nciation for Computational Linguistics.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning,\npages 4596‚Äì4604. PMLR.\nSurat Teerapittayanon, Bradley McDanel, and Hsiang-\nTsung Kung. 2016. Branchynet: Fast inference via\nearly exiting from deep neural networks. In 2016\n23rd International Conference on Pattern Recogni-\ntion (ICPR), pages 2464‚Äì2469. IEEE.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. 2019.\nContrastive representation distillation. arXiv preprint\narXiv:1910.10699.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth√©e Lacroix,\nBaptiste Rozi√®re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Åukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan\nYang, and Ming Zhou. 2020. Minilm: Deep self-\nattention distillation for task-agnostic compression\nof pre-trained transformers. Advances in Neural In-\nformation Processing Systems, 33:5776‚Äì5788.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transform-\ners: State-of-the-art natural language processing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38‚Äì45, Online. Association\nfor Computational Linguistics.\nMengzhou Xia, Zexuan Zhong, and Danqi Chen. 2022.\nStructured pruning learns compact and accurate mod-\nels. In Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1513‚Äì1528, Dublin, Ireland.\nAssociation for Computational Linguistics.\nGuangxuan Xiao, Ji Lin, Micka√´l Seznec, Hao Wu,\nJulien Demouth, and Song Han. 2023. Smoothquant:\nAccurate and efficient post-training quantization for\nlarge language models. In International Conference\non Machine Learning, ICML 2023, 23-29 July 2023,\nHonolulu, Hawaii, USA, volume 202 of Proceedings\nof Machine Learning Research, pages 38087‚Äì38099.\nPMLR.\nLinfeng Zhang, Jiebo Song, Anni Gao, Jingwei Chen,\nChenglong Bao, and Kaisheng Ma. 2019a. Be your\nown teacher: Improve the performance of convolu-\ntional neural networks via self distillation. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision, pages 3713‚Äì3722.\nLinfeng Zhang, Zhanhong Tan, Jiebo Song, Jingwei\nChen, Chenglong Bao, and Kaisheng Ma. 2019b.\nScan: A scalable neural networks framework towards\ncompact and efficient models. Advances in Neural\nInformation Processing Systems, 32.\nXinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv,\nTingwen Liu, Fei Huang, Hongbo Xu, and Yongbin\nLi. 2023. Wider and deeper llm networks are fairer\nllm evaluators. arXiv preprint arXiv:2308.01862.\n5921\nA Dataset Description\nWe apply FREE on various generation tasks includ-\ning summarization, question answering, and ma-\nchine translation. We provide detailed descriptions\nof the datasets used.\n‚Ä¢ SAMSum (Summarization): SAMSum (Gliwa\net al., 2019) consists of 16K messenger-like con-\nversations that are annotated with a summary\nfor providing a concise overview of the conver-\nsation‚Äôs content in the third person.\n‚Ä¢ CNN/DailyMail (Summarization): CNN/ Dai-\nlyMail (See et al., 2017) consists of over 300K\nEnglish news articles that were originally de-\nsigned for machine-reading and comprehension\nas well as abstractive question answering, but\nit now also supports extractive and abstractive\nsummarization.\n‚Ä¢ Multi-News (Summarization): Multi-News\n(Fabbri et al., 2019a) comprises 45K news arti-\ncles and corresponding summaries, where each\nsummary is professionally crafted and provides\nlinks to the original articles referenced.\n‚Ä¢ BIGPATENT(Summarization): BIGPATENT\n(Sharma et al., 2019) contains 1.3M records of\nU.S. patent documents, each accompanied by\nabstractive summaries written by humans. In\nour work, we specifically focus on the Fixed\nConstructions category, which is one of the nine\nclassification categories available in the dataset.\n‚Ä¢ SQuAD (Question Answering): The Stanford\nQuestion Answering (SQuAD, Rajpurkar et al.\n2016b) is a collection of 87.6K reading compre-\nhension tasks. It includes questions generated\nby crowd workers based on a set of Wikipedia\narticles.\n‚Ä¢ IWSLT 2017 (Machine Translation): IWSLT\n2017 (Cettolo et al., 2017b) addresses text trans-\nlation, using a single machine translation (MT)\nsystem for multiple language directions such\nas English and German. Here, we specifically\nfocus on a German-to-English translation task.\nB Detailed Experimental Setup\nTraining hyperparameters. In this section, we\ndescribe the detailed hyperparameter values for our\nwork. We utilize the NVIDIA RTX 3090 GPUs for\ntraining the language models, and we summarize\nTable 8: Optimized hyperparameters for training\nshallow-deep T5 models. The column labeled ‚Äò# Batch‚Äô\nindicates the product of the batch size per GPU and the\nnumber of GPUs. ‚ÄòIn len.‚Äô and ‚ÄòOut len.‚Äô represent the\nmaximum length of the input and output, respectively.\nDataset Model # Batch Epochs In len. Out len.\nSAMSum T5-large 4√ó2 20 512 128\nCNN/DM T5-large 4√ó4 3 512 128\nMulti-News LongT5-base2√ó2 3 2048 512\nBIGPATENT LongT5-base2√ó2 3 2048 512\nSQuAD T5-large 4√ó2 10 512 30\nIWSLT 2017 mT5-large4√ó4 2 1024 128\nthe training configuration in Table 8. For all dataset,\nwe use AdaFactor (Shazeer and Stern, 2018) opti-\nmizer with the learning rate of 1e-4. For the adap-\ntive threshold estimation, we set the initial thresh-\nold value Œª0\nc as 0.9, Œ∂ as 0.4, T as 3% of total\nsample number (refer to Algorithm 1).\nPerformance metrics. To numerically measure\nthe output quality of our method, we utilize the\nF1 score for SQuAD, BLEU score (Papineni et al.,\n2002) for IWSLT2017, and ROUGE score (Lin,\n2004) for the four summarization tasks.\nC Inference Latency Evaluation\nFor measuring inference speed, we execute 500\ninference predictions for each dataset under each\nexamined configuration in PyTorch (Paszke et al.,\n2019) compiled function in a single server with a\nsingle NVIDIA GeForce RTX 3039 GPU and 12th\nGen Intel(R) Core(TM) i7-12700K CPU. For each\ninference prediction, we use batch size 1, which is a\ncommon use case for online serving (Schuster et al.,\n2022). Also, we use to generate output sequences\nthrough greedy sampling with a beam size of 1. We\nmeasure the time including all decoding steps until\ncompletion.\n5922\nF1 ( )\n90\n91\n92\nNormalized Latency\n0.2 0.4 0.6 0.8 1.0\nBLEU ( )\n20\n30\n40\nNormalized Latency\n0.2 0.4 0.6 0.8 1.0\nStatic-exit CALM FREE‚Ä† FREE\n(a) SQuAD (b) IWSLT De-En\nFigure 9: The trade-off between the generated output\nquality and normalized latency under different exit con-\nditions. The dashed line represents the F1 and BLEU\nscores of the full model, which is the fine-tuned shallow-\ndeep module, respectively. Similar to Figure 5, we ex-\nclude the inner point of the Pareto curve.\nBaseline\nROUGE-L ( )\n40\n45\n50\nNormalized Latency\n0.2 0.4 0.6 0.8 1.0\nBaseline\n34\n36\n38\n40\n42\nNormalized Latency\n0.2 0.4 0.6 0.8 1.0\nWCE KD-last KD-unif KD-dyna\n(a) SAMSum (b) CNN/DailyMail\nFigure 10: The trade-off between performance and nor-\nmalized latency per sentence. We varied the exit thresh-\nolds in the range of {0.0, 0.1, 0.3, 0.5, 0.7, 0.9 }. The\nlatency values are normalized by the latency of a base-\nline, which is a simple fine-tuned full model.\nD Additional Experimental Results\nIn this section, we provide additional experimen-\ntal results to demonstrate the effectiveness of our\nproposed method and its individual components.\nD.1 Performance on Different Datasets\nIn this section, we present a comparison of the qual-\nity of the generated output (F1 or BLEU) and the\ninference latency on the SQuAD and IWSLT 2017\ndatasets, similar to the experiments in Figure 5.\nFigure 9 illustrates that both FREE‚Ä†and FREE con-\nsistently outperform the CALM and static-exiting\nbaselines in the SQuAD dataset, which aligns with\nour previous findings.\nHowever, their performance advantages in the\nIWSLT dataset are slightly reduced compared to\nother datasets. This can be attributed to the larger\nvocabulary size of mT5 compared to T5, result-\ning in longer processing times for the confidence\nmeasurement. The CALM approach, which also\nutilizes large linear classifiers, exhibits much lower\nTable 9: Comparison between FREE with T5-large\nand directly trained small-sized T5-base. We apply\nthreshold values of FREE‚Ä†as 0.1 for SQuAD and 0.2\nfor CNN/DailyMail.\nSQuAD CNN/DailyMail\nMethod Model F1 Speedup ROUGE-L Speedup\nFull Model T5-large91.82 √ó1.00 41.09 √ó1.00\nFull Model T5-base90.50 √ó1.86 40.22 √ó2.06\nFREE‚Ä† T5-large90.95 √ó2.76 40.17 √ó2.07\neffectiveness in this dataset as well. We believe that\nthis challenge, regarding the large vocabulary size,\ncan be mitigated by employing a vocabulary size-\nindependent confidence measure that proposed in\nprevious work (Schuster et al., 2022). Nonetheless,\nour proposed algorithm still outperforms the other\nbaselines on various datasets.\nD.2 Layerwise Knowledge Distillation\nGiven the only two exit positions in our shallow-\ndeep module, since their performance significantly\nimpacts the overall robustness of the early-exiting\napproach, we carefully design the loss function\nfor training. In Figure 10, we observed the per-\nformance trends of four different loss functions\nas we varied the exit thresholds. While the differ-\nences are not significant, the KD-dyna loss demon-\nstrates better trade-offs compared to a weighted\naverage or other KD-based losses. Specifically, the\nlower performance of KD-unif on the SAMSum\ndataset suggests that dynamically determining the\nlayer mapping can facilitate more effective knowl-\nedge transfer between the deep and shallow models.\nConsequently, we trained our shallow-deep module\nusing the KD-dyna loss for all experiments, and left\nthe exploration of additional loss functions, such\nas contrastive distillation losses (Tian et al., 2019;\nBae et al., 2021), for future work.\nD.3 Comparison with Small-sized Models\nWe conducted a comparison between the inference\nspeed of FREE using T5-large model and a directly\ntrained T5-base model. To ensure a fair compar-\nison, we manually selected the appropriate confi-\ndence threshold for FREE‚Ä†(without relying on an\nadaptive threshold estimator) to align its perfor-\nmance closely with that of T5-base. The results,\npresented in Table 9, demonstrate that our proposed\nmethod exhibited a competitive speedup in infer-\nence performance on the CNN/DailyMail dataset.\nMoreover, it demonstrated a superior F1 score and\nsignificantly higher speedup on the SQuAD dataset.\n5923\nTable 10: Comparison between early-exiting frame-\nworks on SAMSum with different decoding strategies.\ntop-k(k= 50) nucleus (p= 0.92)\nMethod ROUGE-L SpeedupROUGE-L Speedup\nFull Model 44.34 √ó1.00 45.84 √ó1.00\nCALM 42.35 √ó0.78 44.48 √ó0.82\nFREE 43.58 √ó1.30 45.78 √ó1.31\nWe believe that the variance in speedup across\nthe datasets can be attributed to the performance\nachievable by a directly trained smaller model, as\nwell as a shallow model within the FREE frame-\nwork. In the case of SQuAD, the T5-base model\n(12 layers) achieved a ROUGE-L score of 90.50,\nwhereas a shallow model (6 layers) of our FREE\nframework yielded a similar score of 90.24. Our\nmethod effectively leverage these inherent benefits,\nthereby facilitating the inference speedup through\nexiting at lower layers.\nD.4 Various Decoding Strategies\nTo evaluate the applicability of FREE on various\ndecoding methods, we conducted experiments with\ntop-k sampling (Radford et al., 2019) and nucleus\nsampling (top-p sampling; Holtzman et al. 2020).\ntop-k sampling samples the next word from the\ntop k most probable choices, instead of aiming\nto decode text that maximizes likelihood. On the\nother hand, nucleus sampling chooses from the\nsmallest possible set of words whose cumulative\nprobability exceeds the probability p. As detailed\nin Table 10, FREE method exhibited consistent\nand robust performance while achieving a larger\nspeedup compared to CALM. These results affirm\nthat our FREE framework can be widely applied,\nirrespective of the chosen decoding method.\n5924"
}