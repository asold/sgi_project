{
    "title": "Transformers on Multilingual Clause-Level Morphology",
    "url": "https://openalex.org/W4385573458",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5038735910",
            "name": "Emre Can Acikgoz",
            "affiliations": [
                "Bridge University"
            ]
        },
        {
            "id": "https://openalex.org/A5050950935",
            "name": "Tilek Chubakov",
            "affiliations": [
                "Koç University"
            ]
        },
        {
            "id": "https://openalex.org/A5073193338",
            "name": "Müge Kural",
            "affiliations": [
                "Koç University"
            ]
        },
        {
            "id": "https://openalex.org/A5078696090",
            "name": "Gözde Gül Şahin",
            "affiliations": [
                "Koç University"
            ]
        },
        {
            "id": "https://openalex.org/A5013522667",
            "name": "Deniz Yüret",
            "affiliations": [
                "Koç University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4224247062",
        "https://openalex.org/W2963211188",
        "https://openalex.org/W2885096223",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3097363050",
        "https://openalex.org/W4285232264",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2970963828",
        "https://openalex.org/W2996993760",
        "https://openalex.org/W3205231356",
        "https://openalex.org/W2508815538",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W2963376582",
        "https://openalex.org/W2768282280",
        "https://openalex.org/W2799074487",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W3174770825",
        "https://openalex.org/W3153224075",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2950825792",
        "https://openalex.org/W3103671331"
    ],
    "abstract": "This paper describes the KUIS-AI NLP team's submission for the 1st Shared Task on Multilingual Clause-level Morphology (MRL2022). We present our work on all three parts of the shared task: inflection, reinflection, and analysis. We mainly explore two approaches: Trans- former models in combination with data augmentation, and exploiting the state-of-the-art language modeling techniques for morphological analysis. Data augmentation leads to a remarkable performance improvement for most of the languages in the inflection task. Prefix-tuning on pretrained mGPT model helps us to adapt reinflection and analysis tasks in a low-data setting. Additionally, we used pipeline architectures using publicly available open-source lemmatization tools and monolingual BERT- based morphological feature classifiers for rein- flection and analysis tasks, respectively. While Transformer architectures with data augmentation and pipeline architectures achieved the best results for inflection and reinflection tasks, pipelines and prefix-tuning on mGPT received the highest results for the analysis task. Our methods achieved first place in each of the three tasks and outperforms mT5-baseline with 89% for inflection, 80% for reflection, and 12% for analysis. Our code 1 is publicly available.",
    "full_text": "Proceedings of the The 2nd Workshop on Multi-lingual Representation Learning (MRL), pages 100 - 105\nDecember 8, 2022 ©2022 Association for Computational Linguistics\nTransformers on Multilingual Clause-Level Morphology\nEmre Can Acikgoz\nKUIS AI, Koç University\neacikgoz17@ku.edu.tr\nTilek Chubakov\nKUIS AI, Koç University\ntchubakov@ku.edu.tr\nMüge Kural\nKUIS AI, Koç University\nmugekural@ku.edu.tr\nGözde Gül ¸ Sahin\nKUIS AI, Koç University\ngosahin@ku.edu.tr\nDeniz Yuret\nKUIS AI, Koç University\ndyuret@ku.edu.tr\nAbstract\nThis paper describes our winning systems in\nMRL: The 1st Shared Task on Multilingual\nClause-level Morphology (EMNLP 2022 Work-\nshop) designed by KUIS AI NLP team. We\npresent our work for all three parts of the\nshared task: inflection, reinflection, and analy-\nsis. We mainly explore transformers with two\napproaches: (i) training models from scratch\nin combination with data augmentation, and\n(ii) transfer learning with prefix-tuning at mul-\ntilingual morphological tasks. Data augmen-\ntation significantly improves performance for\nmost languages in the inflection and reinflec-\ntion tasks. On the other hand, Prefix-tuning on\na pre-trained mGPT model helps us to adapt\nanalysis tasks in low-data and multilingual set-\ntings. While transformer architectures with\ndata augmentation achieved the most promis-\ning results for inflection and reinflection tasks,\nprefix-tuning on mGPT received the highest re-\nsults for the analysis task. Our systems received\n1st place in all three tasks in MRL 2022.1\n1 Introduction\nThe shared task on multilingual clause-level mor-\nphology was designed to provide a benchmark for\nmorphological analysis and generation at the level\nof clauses for various typologically diverse lan-\nguages. The shared task is composed of three sub-\ntasks: inflection, reinflection and analysis. For the\ninflection task, participants are required to gener-\nate an output clause, given a verbal lemma and a\nspecific set of morphological tags (features) as an\ninput. In the reinflection task the input is an in-\nflected clause, accompanied by its features (tags).\nParticipants need to predict the target word given a\nnew set of tags (features). Finally, the analysis task\nrequires predicting the underlying lemma and tags\n(features) given the clauses.\n1https://github.com/emrecanacikgoz/\nmrl2022\nTask1: Inflection\nSourceLemma give\nFeatures IND;FUT;NOM(1,SG);\nACC(3,SG,MASC);DAT(3,SG,FEM)\nTarget Clause I will give him to her\nTask2: Reinflection\nSource\nClause I will give him to her\nFeatures IND;FUT;NOM(1,SG);\nACC(3,SG,MASC);DAT(3,SG,FEM)\nDesired FeaturesIND;PRS;NOM(1,PL);\nACC(2);DAT(3,PL);NEG\nTarget Desired ClauseWe don’t give you to them\nTask3: Analysis\nSourceClause I will give him to her\nTarget Lemma give\nFeatures IND;FUT;NOM(1,SG);\nACC(3,SG,MASC);DAT(3,SG,FEM)\nTable 1: Description of the each three task: inflec-\ntion, reinflection, analysis. Task1 (Inflection). For\nthe given lemma and the features, target is the desired\nclause.Task2 (Reinflection). Input is the clause, its\nfeatures, and the desired output features. Target is the\ndesired clause that represented by the desired features\nin the source. Task3 (Analysis). For a given clause, out-\nput is the corresponding lemma and the morphological\nfeatures.\nLiterature has examined morphology mainly at\nthe word level, but morphological processes are\nnot confined to words. Phonetic, syntactic, or se-\nmantic relations can be studied at phrase-level to\nexplain these processes. Thus, this shared task\nexamines phrase-level morphology and questions\nthe generalization of the relations between the lay-\ners of language among languages with different\nmorphological features. The shared task includes\neight languages with different complexity and vary-\ning morphological characteristics: English, French,\nGerman, Hebrew, Russian, Spanish, Swahili, and\nTurkish.\nIn our work, we explored two main approaches:\n(1) training character-based transformer architec-\ntures from scratch with data augmentation, (2)\nadapting a recent prefix-tuning method for lan-\nguage models at multilingual morphological tasks.\n100\nFigure 1: Task3 (Analysis) example by using prefix-\ntuning method. We freeze all the parameters of the\npre-trained mGPT model and only optimize the prefix,\nwhich are shown inside the red block. Each vertical\nblock denote transformer activations at one time step.\n2 Methods\nIn this section, first we cover the model architec-\ntures and training strategies that we have used\n(Vaswani et al., 2017; Shliazhko et al., 2022; Li\nand Liang, 2021), and then discuss our data aug-\nmentation strategies in details (Anastasopoulos and\nNeubig, 2019).\n2.1 Vanilla Transformer\nWe used a modified version of vanilla Transformer\narchitecture in Vaswani et al. (2017) which con-\ntains 4 layers of encoder and decoder with 4 multi-\nhead attentions. The embedding size and the feed-\nforward dimension is set to 256 and 1024, respec-\ntively. As suggested in Wu et al. (2021), we used\nlayer normalization before the self-attention and\nfeed-forward layers of the network that leads to\nslightly better results. We used these in inflection\nand reinflections tasks.\n2.2 Prefix-Tuning\nUsing prefix-tuning reduces computational costs\nby optimizing a small continuous task-specific vec-\ntors, called prefixes, while keeping frozen all the\nother parameters of the LLM. We added two pre-\nfixes, called virtual tokens in Li and Liang (2021),\nthe gradient optimization made across these pre-\nfixes that is described in the Figure 1. We used\nShliazhko et al. (2022) weights during prompting.\nPrefix-tuning method outperforms other fine-tuning\napproaches in low-data resources and better adapts\nto unseen topics during prompting (Li and Liang,\n2021).\n2.3 Data Augmentation\nHallucinating the data for low-resource languages\nresults with a remarkable performance increase for\ninflection Anastasopoulos and Neubig (2019). The\nhallucinated data is generated by replacing the stem\nFigure 2: In order to create the hallucinated samples,\nwe first align the characters of the lemma and the in-\nflected forms. After that, we substitute the stem parts of\nthe input with random characters that comes from the\nvalidation set and test set, as shown in the figure.\ncharacters of the aligned word with random char-\nacters by using the validation or test sets (see Fig.\n2). This way, the amount increase in the training\ndata helps the model to learn and generalize rare\nseen samples. On the other hand, the amount of\nhallucinated data that will be added to the training\nset, hyperparameter N, is also another parameter\nthat directly effects our accuracy. Therefore, hy-\nperparameter N needs to be decided specifically\nfor each language according to corresponding lan-\nguage’s complexity and topology.\n3 Experimental Settings\n3.1 Dataset\nIn the shared task, there are eight different lan-\nguages with varying linguistic complexity which\ncomes from different language families: English,\nFrench, German, Hebrew, Russian, Swahili, Span-\nish, Turkish. For Hebrew there are two versions as\nHebrew-vocalized and Hebrew-unvocalized. Train-\ning data contains 10,000 instances for each lan-\nguage and there are 1,000 samples both in devel-\nopment set and test set. Swahili and Spanish are\nthe surprise languages that announced two weeks\nbefore the final submission day, together with the\nunlabeled test data for each language.\n3.2 Evaluation\nModels are evaluated according to Exact Match\n(EM), Edit Distance (ED), and F1 accuracy. For\ntask1 (inflection) and task2 (reinflection) ED is\nthe leaderboard metric. For task3 (analysis), F1\nscore is the objective. EM accuracy represents the\nratio of correctly predicted lemma and features, and\nED is calculated based on Levenshtein Distance\nwhich indicates how different two strings are, (the\nground truth and prediction for our case) from each\nother. F1 accuracy is the harmonic mean of the\nprecision and recall. F1 accuracy is upweighted for\nthe lemma score in our task. In the leaderboard, the\nresults are averaged across each language.\n101\nTask1: Inflection Task2: Reinflection Task3: Analysis\nModel Transformer + D.A. Transformer Prefix Tuning\nMetrics F1↑ EM↑ ED↓ F1↑ EM↑ ED↓ F1↑ EM↑ ED↓\nDeu 97.71 91.80 0.241 92.40 66.50 0.788 95.89 83.40 0.991\nEng 98.02 88.90 0.221 95.42 72.30 0.477 99.61 98.50 0.064\nFra 98.59 93.20 0.124 92.64 68.30 0.758 95.63 81.90 0.933\nHeb 97.73 89.80 0.550 94.00 83.30 0.796 92.84 73.50 1.322\nHeb-Unvoc 97.96 94.20 0.113 86.70 57.70 1.002 82.09 36.20 2.044\nRus 97.57 87.70 0.828 97.29 84.90 0.854 97.51 88.60 3.252\nSwa 99.72 99.61 0.019 92.05 84.47 0.182 90.51 62.63 3.114\nSpa 98.79 92.00 0.199 96.42 77.60 0.480 98.11 89.40 0.560\nTur 97.50 89.80 0.333 95.36 84.70 0.593 95.36 84.70 0.593\nAverage 91.89 98.18 0.292 93.14 74.72 0.705 94.17 77.65 1.430\nTable 2: Results on the test sets for all tasks and languages with the corresponding models. Edit Distance is the\nleaderboard ranking metric for Task1: Inflection and Task2: Reinflection, and F1 score is used for leaderboard\nranking in Task3: Analysis. D.A. indicates data augmentation.\n3.3 Shared Task\nMultilingual Clause-level Morphology (MRL\n2022) contains three different tasks as Task1: In-\nflection, Task2: Reinflection, and Task3: Analysis.\nAs KUIS AI team, we have attended each of them\nseparately.\n3.3.1 Task1: Inflection\nThe goal of the task is to produce the output clause\nand its features forgiven verbal lemma and a set of\nmorphological features, see Table 1. For inflection\ntask, we have trained a vanilla transformer model\nfrom scratch by adding some hallucinated data for\nthe training set. The data hallucination method,\ndiscussed in 2.3, improved our results significantly.\nAs suggested in Wu et al. (2021), we observed the\neffect of the large batch sizes that results with an\nincrease in accuracy. Thus, we set the batch size to\n400 and we trained our model for 20 epochs. We\nused Adam optimizer by setting β1 to 0.9 and β2 to\n0.98. We started with a learning rate of 0.001 with\n4,000 warm-up steps. Then, we decrease it with the\ninverse of the square-root for the remaining steps.\nWe have used label smoothing with a factor of 0.1\nand applied the same dropout rate of 0.3.\n3.3.2 Task2: Reinflection\nIn reinflection the task is to generate the desired\noutput format as in inflection; however, the input\nis consist of an inflected clause, its corresponding\nfeatures, and a new set of features that represents\nthe desired output form. We again use the same\nvanilla Transformer architecture, and exactly the\nsame training parameters that we have used in in-\nflection task. We tried both (i) giving the all source\ndata as input, and (ii) using only the inflected clause\nand its desired features. We have examined that,\nboth our EM and ED accuracy increased in a large\nmanner when we ignore source clause’s features in\ninput before feeding it to the model.\n3.3.3 Task3: Analysis\nAnalysis task can be seen as the opposite of the\ninflection task. For given clauses and its features,\nwe try to generate the lemma and the correspond-\ning morphological features. We used the prefix-\ntuning method for the analysis task. The prefix tem-\nplate was given as the source and the features were\nmasked. During prompting, we gave the clause-\nlevel in input and the target lemma together with\nits features were expected from the output, like a\nmachine translation task. The source and target\nare given together with the trainable prefixes, i.e.\ncontinuous prompt vectors, and the gradient opti-\nmization made across these prefixes. For the mGPT-\nbased Prefix-Tuning model, we have used the Hug-\ngingface, Wolf et al. (2019) and the corresponding\nmodel weights sberbank-ai/mGPT. The prefixes\nwere trained for 10 epochs with a batch size of 5\ndue computational resource constraints. We used\nAdam optimizer with weight decay fix which is\nintroduced in Loshchilov and Hutter (2017) with\nβ1=0.9 and β2=0.999. The learning rate is initial-\nized to 5 ×10−5 and a linear scheduler is used\nwithout any warm-up steps.\n102\nSystem Inflection Reinflection Analysis\nTransformer Baseline 3.278 4.642 80.00\nmT5 Baseline 2.577 2.826 84.50\nKUIS AI 0.292 0.705 94.17\nTable 3: Submitted results for MRL shared task that\nis averaged across 9 languages. Metrics for the inflec-\ntion and reinflection tasks is the edit distance, and for\nanalysis the metric is averaged F1 score with the lemma\nbeing treated as an up-weighted feature.\n3.4 Results\nOur submitted results are provided in Table 2. The\nannounced results by the shared task are in the\nTable 3 which are evaluated among the provided\nunlabeled test set.\nFor the inflection task, with the help of data aug-\nmentation, we have achieved best average edit dis-\ntance for languages. Specially, for Swahili the edit\ndistance is nearly perfect as well as the exact match.\nIt is followed by Hebrew-Unvoc and French. We\nobserved the highest edit distance and the lowest\nexact match scores for Russian. At the end, we ob-\nserved that, reducing edit distance does not always\nbring better exact match.\nFor the reinflection task, using trained trans-\nformer models from scratch, we again see the best\nresults for Swahili with the lowest edit distance.\nThis time, the highest edit distance belongs to\nHebrew-Unvoc as well as the lowest exact match.\nThe number of words and characters in the exam-\nples of task datasets may be the factors and should\nalso be considered.\nFinally for the analysis, with the help of prefix-\ntuning, we achieved the best results for English\nwith highest F1 score. The ease of finding En-\nglish pre-trained models led us to experiment with\nEnglish-only GPT models, and we subsequently\ndiscovered that multilingual GPT gives better re-\nsults when using prefix-tuning. Tuning on mGPT\nhas the lowest performance with Hebrew-Unvoc,\ndue the low ratio of training samples in Hebrew\nduring pre-training compared to other languages.\n4 Related Work\nWord-level morphological tasks have been stud-\nied to a great extent, with LSTM (Wu and Cot-\nterell, 2019; Cotterell et al., 2016; Malaviya et al.,\n2019; Sahin and Steedman, 2018), GRU (Conforti\net al., 2018), variants of Transformer Vaswani et al.\n(2017); Wu et al. (2021) and other neural mod-\nels (e.g., invertible neural networks (Sahin and\nGurevych, 2020)). Unlike word-level, there is lim-\nited work on clause-level morpho-syntactic mod-\neling. Goldman and Tsarfaty (2022) presents a\nnew dataset for clause-level morphology covering\n4 typologically-different languages (English, Ger-\nman, Turkish, and Hebrew); motivates redefining\nthe problem at the clause-level to enable the cross-\nlinguistical study of neural morphological model-\ning; and derives clause-level inflection, reinflection,\nand analysis tasks together with baseline model re-\nsults.\nPre-trained LLMs have been successfully ap-\nplied to downstream tasks like sentiment analysis,\nquestion answering, named entity recognition, and\npart-of-speech (POS) tagging (Devlin et al., 2019;\nYang et al., 2019; Raffel et al., 2020). Even though,\nthere is limited work on applications of LLMs to\nmorphological tasks, it has been demonstrated that\nusing pre-trained contextualized word embeddings\ncan significantly improve the performance of mod-\nels for downstream morphological tasks. Inoue\net al. (2022) explored BERT-based classifiers for\ntraining morphosyntactic tagging models for Ara-\nbic and its dialect. Anastasyev (2020) explored the\nusage of ELMo and BERT embeddings to improve\nthe performance of joint morpho-syntactic parser\nfor Russian. Hofmann et al. (2020) used a fine-\ntuning approach to BERT for the derivational mor-\nphology generation task. Finally, Seker et al. (2022)\npresented a large pre-trained language model for\nModern Hebrew that shows promising results at\nseveral tasks.\nOn the other hand, since fine-tuning LLMs\nrequires to modify and store all the parameters\nin a LM that results with a huge computational\ncost. Rebuffi et al. (2017); Houlsby et al. (2019)\nused adapter-tuning which adds task-specific layers\n(adapters) between the each layer of a pre-trained\nlanguage model and tunes only the 2%-4% param-\neters of a LM. Similarly, Li and Liang (2021) pro-\nposed prefix-tuning which is a light-weight alterna-\ntive method for adapter-tuning that is inspired by\nprompting.\n5 Conclusion\nIn this paper, we described our winning methods\nmultilingual clause-level morphology shared task\nfor inflection, reinflection, and analysis. Due to the\ndifferent complexity between tasks and the varying\nmorphological characteristics of languages, there is\n103\nno single best model that achieves the best results\nfor each task in each language. Thus, we try to\nimplement different types of systems with different\nobjectives. For inflection we used a vanilla Trans-\nformer adapted from Vaswani et al. (2017) and\napplying data hallucination substantially improves\naccuracy (Anastasopoulos and Neubig, 2019). The\nreinflection task is more challenging compared to\nthe other tasks due to its complex input form. To\novercome this issue, we have removed the original\nfeature tags from the input. We only used the in-\nflected clause and target features in the input. We\nagain used a vanilla Transformer as a model choice.\nFinally, for the analysis task, we used the prefix-\ntuning method based on mGPT. On average, we\nhave achieved the best results for every three tasks\namong all participants.\nAcknowledgements\nThis work is supported by KUIS AI Center from\nKoç University, Istanbul. We gratefully acknowl-\nedge this support. Last but not least, we would like\nto kindly thank our organizers for answering our\nquestions and for the effort they have made to fix\nthe issues that we struggled during the competition\nprocess.\nReferences\nAntonios Anastasopoulos and Graham Neubig. 2019.\nPushing the limits of low-resource morphological in-\nflection. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019, pages 984–\n996. Association for Computational Linguistics.\nD.G. Anastasyev. 2020. Exploring pretrained models\nfor joint morpho-syntactic parsing of russian. volume\n2020-June, page 1 – 12. Cited by: 4; All Open\nAccess, Bronze Open Access.\nCostanza Conforti, Matthias Huck, and Alexander M.\nFraser. 2018. Neural morphological tagging of\nlemma sequences for machine translation. In Pro-\nceedings of the 13th Conference of the Association for\nMachine Translation in the Americas, AMTA 2018,\nBoston, MA, USA, March 17-21, 2018 - Volume 1:\nResearch Papers, pages 39–53. Association for Ma-\nchine Translation in the Americas.\nRyan Cotterell, Christo Kirov, John Sylak-Glassman,\nDavid Yarowsky, Jason Eisner, and Mans Hulden.\n2016. The SIGMORPHON 2016 shared task - mor-\nphological reinflection. In Proceedings of the 14th\nSIGMORPHON Workshop on Computational Re-\nsearch in Phonetics, Phonology, and Morphology,\nBerlin, Germany, August 11, 2016, pages 10–22. As-\nsociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nOmer Goldman and Reut Tsarfaty. 2022. Morphology\nwithout borders: Clause-level morphological annota-\ntion. CoRR, abs/2202.12832.\nValentin Hofmann, Janet B. Pierrehumbert, and Hinrich\nSchütze. 2020. Dagobert: Generating derivational\nmorphology with a pretrained language model. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 3848–\n3861. Association for Computational Linguistics.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea Ges-\nmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. CoRR,\nabs/1902.00751.\nGo Inoue, Salam Khalifa, and Nizar Habash. 2022. Mor-\nphosyntactic tagging with pre-trained language mod-\nels for arabic and its dialects. In Findings of the As-\nsociation for Computational Linguistics: ACL 2022,\nDublin, Ireland, May 22-27, 2022, pages 1708–1719.\nAssociation for Computational Linguistics.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing, ACL/IJCNLP 2021, (Volume 1: Long\nPapers), Virtual Event, August 1-6, 2021, pages 4582–\n4597. Association for Computational Linguistics.\nIlya Loshchilov and Frank Hutter. 2017. Fixing\nweight decay regularization in adam. CoRR,\nabs/1711.05101.\nChaitanya Malaviya, Shijie Wu, and Ryan Cotterell.\n2019. A simple joint model for improved contextual\nneural lemmatization. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2019, Min-\nneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long\nand Short Papers), pages 1517–1528. Association for\nComputational Linguistics.\n104\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1–140:67.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In Advances in Neural Infor-\nmation Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017, De-\ncember 4-9, 2017, Long Beach, CA, USA, pages 506–\n516.\nGözde Gül Sahin and Iryna Gurevych. 2020. Two birds\nwith one stone: Investigating invertible neural net-\nworks for inverse problems in morphology. In The\nThirty-Fourth AAAI Conference on Artificial Intelli-\ngence, AAAI 2020, The Thirty-Second Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2020, The Tenth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 7814–\n7821. AAAI Press.\nGözde Gül Sahin and Mark Steedman. 2018. Character-\nlevel models versus morphology in semantic role\nlabeling. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2018, Melbourne, Australia, July 15-20, 2018,\nVolume 1: Long Papers, pages 386–396. Association\nfor Computational Linguistics.\nAmit Seker, Elron Bandel, Dan Bareket, Idan\nBrusilovsky, Refael Shaked Greenfeld, and Reut Tsar-\nfaty. 2022. Alephbert: Language model pre-training\nand evaluation from sub-word to sentence level. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 46–56. Association for Computational\nLinguistics.\nOleh Shliazhko, Alena Fenogenova, Maria Tikhonova,\nVladislav Mikhailov, Anastasia Kozlova, and Tatiana\nShavrina. 2022. mgpt: Few-shot learners go multilin-\ngual. CoRR, abs/2204.07580.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. CoRR, abs/1706.03762.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nand Jamie Brew. 2019. Huggingface’s transformers:\nState-of-the-art natural language processing. CoRR,\nabs/1910.03771.\nShijie Wu and Ryan Cotterell. 2019. Exact hard mono-\ntonic attention for character-level transduction. In\nProceedings of the 57th Conference of the Associa-\ntion for Computational Linguistics, ACL 2019, Flo-\nrence, Italy, July 28- August 2, 2019, Volume 1: Long\nPapers, pages 1530–1537. Association for Computa-\ntional Linguistics.\nShijie Wu, Ryan Cotterell, and Mans Hulden. 2021.\nApplying the transformer to character-level transduc-\ntion. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, EACL 2021, Online,\nApril 19 - 23, 2021, pages 1901–1907. Association\nfor Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 5754–5764.\n105"
}