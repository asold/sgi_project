{
  "title": "Enhancing user experience in large language models through human-centered design: Integrating theoretical insights with an experimental study to meet diverse software learning needs with a single document knowledge base",
  "url": "https://openalex.org/W4394956895",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2098270760",
      "name": "Yuchen Wang",
      "affiliations": [
        "University of Hawaiʻi at Mānoa"
      ]
    },
    {
      "id": "https://openalex.org/A4365260437",
      "name": "Yin-Shan Lin",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2658471510",
      "name": "Ruixin Huang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2245341227",
      "name": "Jinyin Wang",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A2158096901",
      "name": "Sensen Liu",
      "affiliations": [
        "Washington University in St. Louis"
      ]
    },
    {
      "id": "https://openalex.org/A2098270760",
      "name": "Yuchen Wang",
      "affiliations": [
        "University of Hawaiʻi at Mānoa"
      ]
    },
    {
      "id": "https://openalex.org/A4365260437",
      "name": "Yin-Shan Lin",
      "affiliations": [
        "Northeastern University"
      ]
    },
    {
      "id": "https://openalex.org/A2658471510",
      "name": "Ruixin Huang",
      "affiliations": [
        "Carnegie Mellon University"
      ]
    },
    {
      "id": "https://openalex.org/A2245341227",
      "name": "Jinyin Wang",
      "affiliations": [
        "Stony Brook University"
      ]
    },
    {
      "id": "https://openalex.org/A2158096901",
      "name": "Sensen Liu",
      "affiliations": [
        "Washington University in St. Louis"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4368370665",
    "https://openalex.org/W3122241445",
    "https://openalex.org/W4392902855",
    "https://openalex.org/W4386908049",
    "https://openalex.org/W4390961219",
    "https://openalex.org/W4388274718",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W3039295386",
    "https://openalex.org/W4224991249",
    "https://openalex.org/W4382517466",
    "https://openalex.org/W2929876432",
    "https://openalex.org/W2108353880",
    "https://openalex.org/W2092692434",
    "https://openalex.org/W4225001134",
    "https://openalex.org/W4362669048",
    "https://openalex.org/W4387245194",
    "https://openalex.org/W4237741047",
    "https://openalex.org/W2995092782",
    "https://openalex.org/W4366548599",
    "https://openalex.org/W4387993473",
    "https://openalex.org/W4283171081",
    "https://openalex.org/W6849994323",
    "https://openalex.org/W4386246835",
    "https://openalex.org/W4393868014",
    "https://openalex.org/W4394671747"
  ],
  "abstract": "This paper begins with a theoretical exploration of the rise of large language models (LLMs) in Human-Computer Interaction (HCI), their impact on user experience (HX) and related challenges. It then discusses the benefits of Human-Centered Design (HCD) principles and the possibility of their application within LLMs, subsequently deriving six specific HCD guidelines for LLMs. Following this, a preliminary experiment is presented as an example to demonstrate how HCD principles can be employed to enhance user experience within GPT by using a single document input to GPT’s Knowledge base as new knowledge resource to control the interactions between GPT and users, aiming to meet the diverse needs of hypothetical software learners as much as possible. The experimental results demonstrate the effect of different elements’ forms and organizational methods in the document, as well as GPT’s relevant configurations, on the interaction effectiveness between GPT and software learners. A series of trials are conducted to explore better methods to realize text and image displaying, and jump action. Two template documents are compared in the aspects of the performances of the four interaction modes. Through continuous optimization, an improved version of the document was obtained to serve as a template for future use and research.",
  "full_text": "Computing and Artificial Intelligence 2024, 2(1), 535. \nhttps://doi.org/10.59400/cai.v2i1.535 \n1 \nArticle \nEnhancing user experience in large language models through human-\ncentered design: Integrating theoretical insights with an experimental study \nto meet diverse software learning needs with a single document knowledge \nbase \nYuchen Wang1, Yin-Shan Lin2,*, Ruixin Huang3, Jinyin Wang4, Sensen Liu5 \n1 School of Architecture, University of Hawaii at Manoa, 2410 Campus Road, Honolulu, HI 96822, United States \n2 Khoury College of Computer Science, Northeastern University, 360 Huntington Ave, Boston, MA 02115, United States \n3 Department of Electrical and Computer Engineering, Carnegie Mellon University, 5000 Forbes Ave, Pittsburgh, PA 15213, United States \n4 Department of Computer Science, Stony Brook University, 100 Nicolls Rd, Stony Brook, NY 11794, United States \n5 Department of Electrical and Systems Engineering, Washington University in St. Louis, 1 Brookings Dr, St. Louis, MO 63130, United States \n* Corresponding author: Yin-Shan Lin, lilianlin003@163.com \nAbstract: This paper begins with a theoretical exploration of the rise of large language models \n(LLMs) in Human-Computer Interaction (HCI), their impact on user experience (HX) and \nrelated challenges. It then discusses the benefits of Human-Centered Design (HCD) principles \nand the possibility of their application within LLMs, subsequently deriving six specific HCD \nguidelines for LLMs. Following this, a preliminary experiment is presented as an example to \ndemonstrate how HCD principles can be employed to enhance user experience within GPT by \nusing a single document input to GPT’s Knowledge base as new knowledge resource to control \nthe interactions between GPT and users, aiming to meet the diverse needs of hypothetical \nsoftware learners as much as possible. The experimental results demonstrate the effect of \ndifferent elements’ forms and organizational methods in the document, as well as GPT’s \nrelevant configurations, on the interaction effectiveness between GPT and software learners. \nA series of trials are conducted to explore better methods to realize text and image displaying, \nand jump action. Two template documents are compared in the aspects of the performances of \nthe four interaction modes. Through continuous optimization, an improved version of the \ndocument was obtained to serve as a template for future use and research. \nKeywords: Large Language Models (LLMs); Human-Computer Interaction (HCI); User \nExperience (UX); Human-Centered Design (HCD); GPTs; knowledge base; user needs \n1. Introduction \nSince the emergence of Large Language Models (LLMs) and blowout \ndevelopment from 2022, their integration with Human-computer interaction (HCI) \nmarks the beginning of a new chapter in this interaction. This shift heralds a shift away \nfrom traditional HCI, which primarily relied on graphical user interfaces and \ncommand-line inputs, toward more sophisticated AI-driven interfaces and models. As \nGokul [1] points out, LLMs are reshaping the Artificial Intelligence (AI) landscape \nwith their advanced capabilities in processing and generating human-like language. \nTheir applications extend into various creative domains, including music, art, and \nstorytelling. However, in the aspect of user experience (UX), the LLMs and their \napplications still present challenges. \nCITATION \nWang Y, Lin Y, Huang R, et al. \nEnhancing user experience in large \nlanguage models through human-\ncentered design: Integrating \ntheoretical insights with an \nexperimental study to meet diverse \nsoftware learning needs with a single \ndocument knowledge base. \nComputing and Artificial \nIntelligence. 2024; 2(1): 535. \nhttps://doi.org/10.59400/cai.v2i1.535 \nARTICLE INFO \nReceived: 2 February 2024 \nAccepted: 1 April 2024 \nAvailable online: 19 April 2024 \nCOPYRIGHT \n Copyright © 2024 by author(s). \nComputing and Artificial Intelligence \nis published by Academic Publishing \nPte. Ltd. This work is licensed under \nthe Creative Commons Attribution \n(CC BY) license. \nhttps://creativecommons.org/licenses/\nby/4.0/ \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n2 \n2. The impact of LLMs on HCI and UX \n2.1. Redefining UX with LLMs in HCI \nLLMs have been pivotal in transforming UX. \nOne of the big transformations brought about by LLMs is personalization. These \nmodels analyze user data and learn from individual interaction patterns to tailor \nresponses and suggestions. \nThe incorporation of advanced Natural Language Processing (NLP) capabilities \nin LLMs marks another stride forward. This development allows for more intuitive \nand human-like interactions. \nAdditionally, context-aware interactions signify a significant advancement in \nHCI, brought about by LLMs [2]. These models not only recognize words but also \ncomprehend the context of user requests [3] and predict user’s preference [4]. \n2.2. Challenges from UX \nAs we transition from the exploration of the positive advancements of LLMs in \nHCI, it becomes imperative to critically examine the multifaceted challenges that \naccompany this technological integration. \n2.2.1. Ethical consideration \nAs for ethical consideration, while LLMs offer immense potential in HCI, they \nintroduce complex ethical challenges that significantly impact user experience. Ethical \nchallenges mainly come from two aspects: The technology inherent defects, such as \nspecification gaming and side effects [5], pressure to deploy unsafe systems [6] and \nrisks from advanced misaligned AI [7], and inappropriate use, such as Misinformation \nHarms and Malicious Uses [8]. \n2.2.2. Supportiveness of user needs \nThe integration of LLMs into HCI presents a range of technical complexities to \nmeet user’s advanced needs, such as the need for higher-speed content generation and \nmore accuracy to the background context, which involves transformers, tokens, \nreinforcement learning from human feedback (RLHF) and natural language \nprocessing (NLP) [9]. \nAdditionally, models often produce outright fabrications that may appear \nplausible [10]. It’s widely acknowledged, through both research and anecdotal \nevidence, that LLMs often face a pervasive problem of hallucination, or “hallucinated” \ncontent. \nSubramonyam et al. [11] focus on integrating user experience and needs into the \nAI development process, finding the problems such as low-level design and share \ninformation across expertise boundaries. Zhang et al. [12] use LLM to answer student \nquestions classified into four types, and finds the system effectively ignores questions \nthat it cannot address. \n3. Introducing HCD to LLMs and their applications \n3.1. Principles of HCD \nComputing and Artificial Intelligence 2024, 2(1), 535.  \n3 \nHCD, or HCAI, introduced by Don [13], is a problem-solving approach with its \ncore positioning real individuals at the center of the development process. This \napproach is focused on delivering equitable results and upholding the utmost respect \nfor privacy, thereby aligning AI functionalities with human values and ethics [14]. The \nessence of HCD lies in consistently prioritizing the user’s desires, challenges, and \npreferences throughout every stage of the design and development process [15]. \nMajor principles of UCD includes early and active involvement of the user during \nthe design process, clarification of user, user feedback is incorporated into the \nproduct’s lifecycle and the product is improved using an iterative design process [16]. \nFor instance, Jaimes et al. [17] emphasize the importance of mixed-initiative human-\ncomputer systems, highlighting how user input plays a crucial role in shaping the \nfunctionality and responsiveness of these systems. Similarly, Mack et al. discusses the \ncriticality of including diverse user perspectives in research methods, ensuring that \nsystems are accessible and meet varied user needs [18]. \nSome research explores the way to encourage public early participation in public \ndecision making or affecting users’ climate-controlling behavior by using new \ntechnology, such as augmented reality (AR) [19] and virtual reality (VR) [20], which \nare also applications of HCAI. Research by Seffah  and Andreevskaia [21] developed \na skill-oriented program towards developers and students based on analyzing UCD \nknowledge and techniques. \n3.2. Previous attempts to reflect HCD in LLMs applications \nThis study mainly focusses on the methods of enhancing the supportiveness of \nuser needs by applying HCD principles. HCD prioritizes the needs, preferences, and \ncontexts of users [22], ensuring that LLM-driven interactions are not only efficient but \nalso resonate with the users’ expectations. \nPetridis et al. [23] explore the possibility of incorporating prompt-based \nprototyping into designing functional user interface (UI) mock- ups, finding LLMs \npotentially reduce the time needed to create a functional prototype. Park and Choi [24] \nintroduce LLMs into audience simulation for public speech and uses AudiLens to \nprovide flexibility to the speaker. Di Fede et al. [25] introduce the Idea Machine \ncombined with LLMs to empower people engaged in idea generation tasks. \nKorbak et al. [26] explore alternative objectives for pretraining LMs (Language \nModel) to create text aligned with human preferences. Study by Rastogi et al. [27] find \nexisting auditing tools use either or both humans and AI to find failures. They create \nthe evaluation tool: AdaTest++, which is powered by GPT3 and Azure’s sentiment \nanalysis model. \n3.3. Build HCD guidelines to enhance UX in LLMs \nFrom the above discussion, the HCD principles related to LLMs can be concluded \nas the following (Table 1). \nThese six guidelines are crucial to LLMs like GPT in meeting user expectations \nand needs effectively. They also provide possible ways to optimize related design \nincluding AI agent, application, platform, user interface and the construction of \nknowledge base. \nComputing and Artificial Intelligence 2024, 2(1), 535.  \n4 \nTable 1. HCD guidelines related to LLMs for enhancing UX. \nPrinciples Requirements \nHigh efficiency Fast response \nFeedback consideration Collect feedback; Update periodically or imperiodically \nHigh supportiveness for diverse needs Generality and specificity consideration; Personalization and Customization \nEmotional consideration Understanding the emotional of user conversations and providing more humanized interactions \nHigh Simplicity Easy input; Effortless expression; Multimodal input and output: images, text, voice, etc. \nHigh Reliability Authenticity; Accuracy \nIn the following section, a preliminary experiment is conducted to apply these \nHCD principles into the enhancement of LLM’s interaction capabilities. \n4. Improving UX by optimizing a single document as principal \nknowledge in GPT: A preliminary experimental study \nThis preliminary experiment mainly focuses on the UX enhancement from the \naspect of supportiveness for users’ diverse needs. Other HCD principles, such as \nsimplicity and reliability, will also be taken into consideration in the experiment design. \nThe experiment takes ChatGPT-4 as an example, exploring how to use a single \ndocument as the main material of knowledge base to construct a custom GPT. \n4.1. Materials and methods \n4.1.1. Virtual experimental environment: ChatGPT-4 and GPTs editor \nThe working environment for this study is set in ChatGPT-4 and GPTs Editor. \nGPTs editor is a relatively new function as one part of ChatGPT-4. It’s a \nspecialized environment for creating and tuning GPT models based on GPT editor’s \npreset configuration, including descriptions of this GPT, instructions, knowledge, \nstarters and actions, allowing adjustments to the model’s responses, capabilities, and \ninteraction style. In “Configure” interface, the “Instructions” area provides overall \ncontrol rules for GPT to follow during interaction. “Conversation starters” allows \nusers to start a conversation by just clicking corresponding buttons. “Knowledge” \nprovides a preset knowledge base where editor can upload files as data in certain \nformats, such as docx, pdf or jpg. \nAfter the new GPT being created, it will be imported in ChatGPT-4 automatically, \nwhich provides an environment for users to interact. \n4.1.2. Principal objective: Meeting users’ diverse needs in software learning \ninteraction \nThis study defines a goal as taking ChatGPT-4 as a software learning tool that \nprovides knowledge and solutions for novices in learning a new software. This \nhypothetical scenario is designed to simulate how LLMs can synthesize newly \ninputted knowledge and utilize it in multiple ways, which can be considered as one of \nthe typical applications which use LLMs to serve a specific group of people. UX in \nthis study can be evaluated by the quality of dialogues during interaction. \nVisual Scripting, a tool inside Unity software, is taken as the software in the \noptimization process. It allows for the creation of logic and game behaviors without \nComputing and Artificial Intelligence 2024, 2(1), 535.  \n5 \nwriting code directly. By using visual graphical nodes and connecting them with lines, \nUnity developers can construct complex game logic and interactions. The advantages \nare as follows: \n(1) GPT has less inherent knowledge about Visual Scripting itself, even if some \nrelated coding knowledge is trained into GPT. Therefore, the pre-existing \nknowledge will less interfere the evaluation. \n(2) The Visual Scripting Manual on official website can be used as a reliable source \nfor constructing the knowledge for GPT. \n(3) The images of how to use Visual Scripting are easier to make and since it is an \nintuitive tool. \n4.1.3. New knowledge resource: A single document input in knowledge base \nA single document is used as the main new knowledge resource uploaded in \nGPTs Editor’s “Knowledge” area. It is a Microsoft Word document in docx format, \nserving as the new knowledge resource and control module. It is composed of a control \npart and a software knowledge part. The advantages of using a single document are as \nfollows: \n(1) Simplicity and customization consideration: It is easy for a real creator to replace \ncertain parts of the template document to make another GPT as a tutorial for \nlearning other software. \n(2) Compatibility consideration: The docx document can contain the knowledge both \nin forms of natural languages or codes.  The arrangement of content is also easy \nto be adjusted. \n(3) Variables control: To avoid black box effect which often exists in AI product, the \nsingle document can be easily optimized, which helps to explore a method of \ngetting a relatively controllable result. \n4.1.4. Users’ needs and requirements definition \nDifferent groups of users may have different needs for the usage of a software \nlearning GPT, while a single user may also have needs for multiple ways to use it. The \nfollowing diagram shows the possible needs (Figure 1). \n \nFigure 1. Software learners’ diverse needs in using this GPT. \nIt can be considered that different using modes of this GPT are based on users’ \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n6 \nrequirements for varying degrees of input and output freedom. The overall goal is to \nintegrate these modes. \nFor the input freedom, inputting single a number or letter based on the given \nprompts is an alternative to select a desired action, such as to start the tutorial, or jump \nto a certain section of the tutorial, which requires less input freedom. Users also have \nthe demands for inputting a complex issue and then getting solutions, which requires \nmore input freedom. \nFor the output freedom, the alternative of strictly showing the original content \nfrom the knowledge part of the document is needed, meaning less output freedom, \nwhich can be applied in the scene that users hope to strictly obey the software guidance \nfrom a traceable source. In other cases, the output needs to display content in a creative \nway by using more natural and coherent language to rewrite and reorganize the \nknowledge, meaning more output freedom. \nTherefore, four types of modes are supposed to be realized: \nMode 1: Learning step-by-step with original content, enabling users to learn from \nprinted original content retrieved from the knowledge bases words by words. \nMode 2: Learning step-by-step, similar to the previous one, but use NLP to \nreinterpret original content. \nMode 3: Learning by issue solutions, allowing users to receive solutions for their \nissues while using Visual Scripting, and the solutions should print the original \nsentences of related knowledge. \nMode 4: Learning by issue solutions, similar to the previous one, but use NLP to \nreinterpret original content. \n4.1.5. Expected outcomes \nThe overall optimization process can be illustrated as the following \ndiagram (Figure 2). \n \nFigure 2. Document optimization process. \nThe process involves a series of examinations. Firstly, different functions to \nrealize these modes will be analyzed. Different methods will be tested to within the \ndocument initial structure. Then, these methods will be filtered and selected based on \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n7 \nthe results of tests. The document will be adjusted by using the preferred methods. The \nfour modes will be tested within the new document to check the result of performance \nand finally, it will be optimized again based on the result and then be retested. The \nselection principles in each step include high accuracy, less code use and less module \nuse. \nThe expected outcomes of this study are as follows: \n(1) To figure out how the elements inside the single knowledge document as well as \nthe GPT configurations will affect interaction quality. \n(2) To explore the possibility to integrate this GPT’s four modes. \n(3) If possible, through optimization process, a document that can better integrate \nthese modes get be finally obtained as a template for future use, which can be \nseen as the application of “feedback consideration” HCD principle. \n4.1.6. Methods of UX evaluation \nThe method of UX evaluation is to assess the accuracy of interaction rather than \nspeed. To emphasize the ability of GPT comprehensively utilizing newly input \nknowledge, is supposed to control the variables in the input-output process. The \nfollowing methods are employed to prevent GPT’s directly using knowledge of Visual \nScripting to interfere the evaluation: \n(1) Multiple forms of knowledge composition \nThe images of Visual Scripting nodes and connections are taken as knowledge \ntogether with text. Some of the original text and images from Unity Visual Scripting \nManual 1.9.1 version [28] is extracted or rewritten and then be placed into the \ndocument. \n(2) Closing web browsing \nWeb browsing action in GPT may introduce original online resources, so closing \nit can isolate environment. \n(3) Methods of output control \nThe output is supposed to mix text and images from the knowledge, making it \nchallenging to achieve user’s goals. \nThe average accuracy of the results in each trial will be assessed through 5 times \nof repetitive complete chatting, the functional elements of the four modes and their \nevaluation criteria of the interaction result are as follows (Table 2). \nTable 2. Criteria for approximate accuracy assessment. \nFunctional Elements and Criteria List for modes \n(A) Whether the jump action is successful and smooth; \n(B) Whether the output obey the sequences of the original steps; \n(C) Whether related images can be successfully displayed together with text. \n(D) Whether the output display original content in each step completely; \n(E) How much the reinterpreting using NLP deviates the original contents in the document, producing wrong content or “hallucinated” content \n(content that seems to be correct but has no relevance with the document original content); \n(F) How helpful the selected content is to the user’s question (the designed questions are designed to be satisfied by some certain parts in the \ndocument); \n(G) Whether the output solutions cross enough necessary range of knowledge chapter in the document. \nMode 1 Mode 2 \n(A), (B), (C), (D) (A), (B), (C), (E) \nMode 3 Mode 4 \n(C), (D), (F), (G) (C), (E), (F), (G) \nComputing and Artificial Intelligence 2024, 2(1), 535.  \n8 \nFor testing the function in Mode 3 and 4 that answer user’s questions, the question \nlist is designed as follows (Table 3). \nTable 3. Question list for Mode 3 and 4. \nQuestion Number Content \nQ1 How can I use nodes to change the sprite of GameObject “A” when a time duration finishes? \nQ2 How can I use nodes to sets the velocity of GameObject “B” to half of its original velocity when a “B” \nenters a trigger collider in 2D space? \nQ3 How can I use nodes to add value 1 to the existing object variable named “C” and set back to “C”? \nQ4 \nHow can I make another Script Graph named “D” inside a Script Graph named “E” to receive a float from \n“Script E”, then returns true if the float is greater than 1 and less than 2, otherwise returns false to “Script \nE”? \nQ5 If UI button “G” has a Script Graph named “H” and GameObject “J” has a Script Graph named “K”, how \ncan I use nodes in “H” to trigger the event in “K” when clicking the button “G”? \nThese questions are with high complexity and less specificity, meaning to require \ncrossing different sections of knowledge part in the document to find answers, and less \nmention any specific name of node and the phrase “Visual Scripting”. The intent is to \nmake it easy to recognize whether it use the new knowledge (Figure 3). \n (a) (b) \nFigure 3. GPT’s responses to different questions without new document input. (a) Question with less complexity and \nhigh specificity; (b) Question with high complexity and less specificity. \n4.1.7. Structural design of the single document \nThe initial structure of the new document is designed as follows Figure 4. \nThe document includes two parts: Part 0 provides an introduction and response \nmethods, serving as a general control part; Part 1 provides the knowledge of Visual \nScripting, structured into chapters and sections based on the content. After each output, \nusers can change interaction mode directly. \nHere is a brief introduction of these parts: \nPart 0 includes: \n(1) “Overview of This GPT’s Rules” section: It provides general rules to control \nthe interaction. It includes the following parts: \na) Descriptions of this GPT and the document. \nb) The general rules, such as “Refer to Section 2 in Part 0 for initial dialogue \nrules” and “Interactive requests needing user responses are enclosed in braces {}, such \nas {Enter 1: Continue}”. Prohibited interaction ways area also provided, such as \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n9 \n“Refuse interactions that does not meet current interactive requests inside {}”. \n(2) “Initial Response Method for Dialogues” section: It provides ways to go to \ndifferent parts corresponding to the input Starters. \n(3) “Start learning” section: It provides ways to process Part 1 section by section. \n(4) “Finding Solution” section: It provides ways to provide solutions to user’s \nquestions. \n(5) “Introduction” section: It provides the basic information about how to use this \nGPT. \n \nFigure 4. Initial structure of the single document. \nPart 1 includes: \n \nFigure 5. Configuration of GPT. \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n10 \n14 Chapters of detailed knowledge are provided as well as some guidance. There \nis a title of each chapter and each section, but no overall introduction in each Chapter. \nIt is found that the extraction of images directly from the Word file is not possible. \nTherefore, a zip file containing multiple images is uploaded as supplementary material. \nEach image is named in following format: “Chapter number” + “Section number” + \nthe image’s sequence number, such as “060401”. Also, the “Instructions” area is filled \nwith content from “Overview of This GPT’s Rules” section. In addition, the code \ninterpreter is turned on for processing code in the document. The configuration of the \nnew GPT is set as Figure 5. \n4.2. Module and structure trials: Results and analysis \n4.2.1. Original text display \nFor displaying the original text in the document, different ways are examined by \nseveral trials. \n(1) Trial 1: Executing the “print ( )” function when jumping from somewhere else \nHere is an example. The instructions in Part 0’s “Overview of This GPT’s Rules” \nreads: “Text that needs to be directly printed will be with clear instructions such as \n‘print (Hello)’, and will be enclosed in brackets marks ().” The instruction in “Initial \nResponse Method for Dialogues” in Part 0 reads: “If ‘Start Learning’ is inputted by \nthe user, go to Part0, Section 3A.” and in Section 3A, it uses “print ( )” function in \neach step following a serial number, such as “1. Hi. Welcome…” and “2. The \nfollowing is…”. The result shows it can proceed printing text step by step easily as \nfollowing screenshot (Figure 6). The limitation is it has to proceed from the first step \nin a section. \n \nFigure 6. Result of text display Trial 1 (part of the whole result). \nIt is also examined whether it can work if removing the “print ( )” function in \nTrial 1. It shows that when jumping from somewhere else, this approach cannot always \nkeep the text printed in its original form. \n(2) Trial 2: Using printing command to print text in another place \nHere is an example. The instruction in “Initial Response Method for Dialogues” \nin Part 0 reads: “If ‘Start Learning’ is inputted by user, print Point 4 in Part 1, Chapter \n1, Section 1.1, then print Point 4 in Part 1, Chapter 1, Section 1.3” or “If ‘Start \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n11 \nLearning’ is inputted by user, proceed the following steps: Step 1. print Point 4 in Part \n1, Chapter 1, Section 1.1; Step 2. print Point 4 in Part 1, Chapter 1, Section 1.3.” \nSome variables here include: a) whether directly providing the text content in \nSection 3A, or with “printing ( )” function; b) whether use single number at the \nbeginning of the text like “2.”, or use serial number like “Step 2”, “Point 2” \nAll results with different combination of variables successfully printed the text, \nhowever, they have to continue printing until the end of the section. Also, if it is \nrequired to print sperate parts in Part 1, it can just finish the first one. \n(3) Trial 3: Using command to execute the “print ( )” function in somewhere else \nHere is an example. “If ‘Start Learning’ is inputted by user, proceed Step 1 in \nPart 1, Chapter 3, Section 3.1” and in the corresponding section, it use “print ( )” \nfunction. \nWith different variable forms, the results are similar to those in trial 2. However, \nit can display two separated parts of text in one time. \n(4) Trial 4: Searching text to display \nAn approach is to use instructions in Part 0 to force GPT answer user’s question \nwith original text content as follows: “If users ask you any question, please print any \nuseful information in Part 1 that can answer user’s question. Please print the original \ntext and do not rewrite them or add your own words. Please notice that the useful \ninformation in Part 1 can be over one place, so please find as much as possible.” \nThe results are as follows (Figure 7). The red square is to mark the original text. \nIt can be seen that even under strong instructions, it still tries to rewrite the original \ntext to make the content coherent. The reason might be it has a strong weight of using \nNPL since it is a LLMs. \n \nFigure 7. Result of text display Trial 4 with easy input (part of the whole result). \nHowever, if the requirement of using original text is added into user’s input, GPT \nwill largely increase the weight of using original text, as shown in Figure 8, which \nshows that the user’s input plays a decisive role during interaction. \nAnother test is to use an existing printed instruction as context to force GPT print \nthe original text. It induces GPT to print out the command first as context and then the \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n12 \nuser adds a signal from the printed instruction in new input. The result shows this \nmethod also does not work. \n \nFigure 8. Result of text display Trial 4 with strong input (part of the whole result). \nIt can be inferred that without advanced code, it is difficult to force GPT print the \noriginal text when asked a question that is not preset, because this process may involve \nseveral steps with different ability. Therefore, it is preferred to design the Mode 3 to \nwork with user’s additional input instruction. The results also show that “print ( )” \nfunction of every step in Part 1 rarely interfere the information searching process. \nMove the conditions into “Instructions” area are also testes and the results show \nthey sometime works. \n(5) Trial 5: Reinterpreting text in natural language \nReinterpreting text in natural language is required by Mode 2 and Mode 4. Due \nto GPT’s NLP characteristic, if there is no additional command, it is easy to realize \nthe reinterpreting function. However, if there is command like “print ( )” in the content, \nor instruction to print original text, it is needed to add a conversion \nmechanism.Considering the convenience for future editors of this document, it is not \nsupposed to provide conditional statements with many options after each section in \nPart 1 for switching modes. It is found that when conditional statements are only \nplaced in part 0, it is difficult to reinterpret the text in Mode 4. It works only if the user \nadd prompt like “in natural language” into the input. However, when trying to move \nconditional statements into “Instructions” area, it works well. \n4.2.2. Image display \nImages are compressed into zip file named “Album” uploaded additionally. Four \nways to display images are examined here. \nTrial 1: Directly commanding to show images \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n13 \nWhen using command like “show the image named ‘010101.jpg’ from the zip \nfile”, “display image named ‘010101’ from ‘/mnt/data/Album.zip’” or “extract the \nimage named ‘010101.jpg’ from the zip file “Album.zip” in “/mnt/data/” and display”, \nit is sometimes failed and no image is shown. \nTrial 2: Using half code to extract every time \nWhen using sentence like “Please use code including ‘display.display(image)’ to \ndisplay image named “010101.jpg” in path ‘/mnt/data/Album.zip’” to display code, it \nsometimes works, while in other time it fails. Sometimes, even if it generates the \ncomplete required information, the sequence of image and text is disordered and it \nalways display an image at first. This may be because GPT takes this order as a natural-\nlanguage description rather than a code, so it mixes the information with other to \nutilize. \nTrial 3: Using prompt to call a shared piece of code \nA shared piece of code to display images lis put in the Part 0 (Figure 9a), for \nexample, in Section 7. And in Part 1, it provides the following prompt in each place \nwhere needed: “Please use code in Part 0, Section 7 to display an image (image_name \n= ‘010101.jpg’).” This approach can show the image successfully. However, it also \nhas the disadvantages of disorder, giving extra description (marked with red square in \nthe figure) (Figure 9b) and sometimes turn into reinterpreting in the middle. \n  (a) (b) \nFigure 9. Code and Result of image display Trial 3. (a) Code of Trial 30; (b) Result of Trial 30. \nTrial 4: Using complete code to extract one image every time \nThis approach is using code in (Figure 9a) in each place where needed. \nThe code is executed every time when being proceed together with text printing. \nIn most results, the image can be displayed in correct sequence. The \ndisadvantage is that the document needs to repetitively provide the code. \nTrial 5: Extracting all images from the beginning \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n14 \nThis is to use a complete code (Figure 10a) to extract all images from the \nbeginning, then use following request to show the extracted images where \nneeded: \n“Please use code <image_path = ‘/mnt/data/extracted_images/010101.jpg’ \ndisplay.display(Image.open(image_path))> to display an image in path \n‘/mnt/data/extracted_images’.” \nThe performance is relatively good with correct sequence and makes less \nerrors (Figure 10b). \n  (a) (b) \nFigure 10. Code and Result of image display Trial 5. (a) Code of Trial 5; (b) Result of Trial 5. \nTrial 6: Display image by searching \nIt is also required to display an image when giving solutions to user’s question. \nTwo ways are explored based on Trial 5’s method which extracts all images first. \nOne approach is to add additional instruction to all of the conditional statements \nin “Control Center” section (will be discussed later), “Overall Rules” section and \n“Instructions” area as follows: \n“Please also execute the steps with code for displaying images that is very close \nto the information you find and in the same section of the information you find, which \nhelps to illustrate the text information.” \nThe results show it works in printing the text it found and displaying the \ncorresponding images. Figure 11 shows two pieces of one result. There is one \ndisadvantage that it sometimes put all images together, even when an addition \ninstruction. \nAnother approach is similar to the previous one, but placing code in part 0 and in \nPart 1, telling GPT to execute this code to display an image. The results and \ndisadvantages are almost the same as the first approach. \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n15 \n  (a) Piece 1. (b) Piece 2. \nFigure 11. Results of image display Trial 6. \n4.2.3. Jump action \nThis part explores how to jump from one place to another in different ways. Using \nconditional statements is to add an additional part where needed with several “if” \nconditions corresponding to users’ input, and they just exist in the document and are \nnot be printed. Using interactive request is to provide a request enclosed in braces such \nas {Enter 1: go to Section 3} at the end of each section. Using section title is to place \nprompt in section title for positioning. \n(1) Trial 1: About section title and “Initial Response Method for Dialogues” \n  (a) (b) \nFigure 12. Result of Jumping to a specific section. (a) Piece 1; (b) Piece 2. \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n16 \nThe weight of section title and “Initial Response Method for Dialogues” section \ndesigned previously are tested. It is found that actually sometimes GPT tends to go to \na section with title that is same as input words, more than go to a section according to \nconditional statement in “Initial Response Method for Dialogues” section. Therefore, \nit is better to use a different name in section title if conditional statements are supposed \nto work. \nJumping from the end of a section to any specific section in Part 1 is also a \nrequired function for user’s step-by-step learning. In a test, each Section in Part 1 is \nlabeled with a unique code at the end, such as “0201” meaning the Section 1 in Chapter \n2. It is found the jump action can work no matter whether there are any conditional \nstatements or an interactive request like “{Enter the CODE of section to go}” (Figure \n12a). When the section title is input, it also works (Figure 12b). It can be inferred that \nGPT actually jump to corresponding section by searching section title.Trial 2: About \nInteractive requests \nThe “Instructions” area in “Configure” interface is filled with the rule that \nthe user can only interact with interactive requests. When interactive requests \nand Control Center provides different directions, it is found that the interactive \nrequests have a large weight when it is explicitly stated, such as {Enter 1: \nContinue} (Figure 13a). However, if it is obscure, GPT will locate user’s input \nto other conditions (Figure 13b). \n  (a) (b) \nFigure 13. Results of different interactive requests in Trial 2. (a) Trial 2a; (b) Trial 2b. \nTrial 3: About positions of conditional statements \nThere are two methods to respond user’s input by conditional statement. \nOne method that uses conditional statement right after current position has been \nexamined in previous discussion. However, it needs to provide a part of conditional \nstatements in each place where needed in Part 1. The advantage is that it can provide \ndifferent responses in different places even the user’s inputs are totally the same. \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n17 \nThe other method is to use a Section named “Control Center” in Part 0 with all \n“if” conditional statements to proceed common response to the user’s input. Since a \ngeneral rule in “Instructions” area reads that the user can only interact with interactive \nrequest, if the user’s input is not covered by current interactive input, the “Control \nCenter” does not work. One test shows that when the Control Center works, it cannot \njump back automatically. For example, if the interactive request in a section in Part 1 \njust has {Enter x}, and there is one conditional statement in Control Center reads: “If \na single letter “x” is entered by the user, go to the next section.” Then, it cannot go to \nthe next section in Part 1, instead, it goes to the next section in Part 0. \nOne test also shows that not only the first approach, but also the second can \nproceed several steps after the jumping action. The follows are the result of one test \n(Figure 14). The sequence of original text (with red frame)-solutions-original text is \ngenerated by the preset steps in Control Center when jumping from another place. \n \n \n(a) (b) \nFigure 14. Result of proceeding steps in conditional statements. (a) Piece 1; (b) Piece 2. \nBased on the above trials from 1, 2 and 3, it can be concluded that when similar \nprompts exist in a section title, an interactive request, conditional statements in current \nposition and conditional statements in “Control Center”, GPT will comprehensively \njudge the degree of similarity to select the closet one to jump to. \nTrial 4: About “Instructions” area \nIf the “Instructions” area tells GPT to go to Part 0, Section 1, and the interactive \nrequest tells GPT to go to Part 0, Section 2, it will select the latter way to go. It may \nbe because the “Instructions” area is filled with the rule that the user can only interact \nthe interactive requests. It can be inferred that the “Instructions” area has been tested \nto have highest weight to control the overall interaction. \nOne approach that can perfectly avoid the Control Center’s defect of relocation \nis placing all conditional statements only in “Instructions” area. Therefore, it can be \nconsidered to use only the “Instructions” area to provide common rules as the Control \nCenter, and make Part 0 and Part 1 in the document all the modules for providing \ndetailed information and interactive methods. Figure 15 shows a good result when \nusing this approach. The user can switch from showing content in original form \n(Figure 15a) to showing content in reinterpreted form (Figure 15b) after inputting \nsimple codes that points to conditional statements preset in “Instruction” area. The \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n18 \nprocess is firstly the user enters the code representing a certain mode that can be found \nin previous dialog, or the code provided in current interactive request to restart the \nmode selection module. After the mode switched, the user continues the learning by \ninputting a code representing resuming. \n  (a) Switching from showing original content. (b)  Switching to reinterpreting content. \nFigure 15. Result of switching modes in Trial 4. \n4.3. Integration and optimization \n4.3.1. Methods selection \nThe preferred approaches based on all above trials to compose different functions \nin the document is listed in Table 4. The list remains those with better performance \nand filtered some options based on selection priority discussed previously. Some \nfunctions have over one option. \nTable 4. Preferred methods. \nFunctions Approaches \nPrinting text directly Using “print ( )” function \nDisplaying image directly \nUsing code to extract all images at the beginning \nOption 1: Display the extracted image with local code \nOption 2: Display the extracted image with code in “Instructions” area/Control Center \nDisplaying text by searching \nUsing interactive requests \nOption 1: Using conditional statements in Control Center + user additional input \nOption 2: Using conditional statements in “Instructions” area \nDisplaying image by searching Option 1: Using Control Center \nOption 2: Using “Instructions” area \nJumping to the next chapter Option 1: Just using interactive requests \nOption 2: Using interactive requests + “Instructions” area \nInitial mode selection \nOption 1: Using Control Center \nOption 2: Using section title \nOption 3: Using an individual start section + “Instructions” area \nJumping to any specific section in Part 1 Just using unique code in title and interactive requests \nOverall requirement \nConsidering to make section titles, conditional statements and interactive requests same or \ndifferent \nAdjusting “Instructions” area to be consistent with all functions \nIt can be seen that all functions in Figure 4 can be realized without placing \nconditional statements at the end of each section in Part 1. \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n19 \n4.3.2. Methods integration \nConsidered integrating all the selected methods, two forms of templates are \ndesigned based on the initial document after several times of optimization.  \nThe first template is as follows (Figure 16). The features include: \n(1) It mainly uses a Control Center to respond to user’s questions. \n(2) The jump action from section to section in Part 1 has been simplified and can \nnow be achieved through interactive requests. \n(3) For first mode selection, it uses a single section. \n(4) The user has to add additional words in the input to change response style from \noriginal content to reinterpreting. \n \nFigure 16. Structure of template 1. \nThe second template is as follows (Figure 17). The features include: \n(1) It places all conditional statements into the “Instructions” area (text provided in \nAppendix A) to control all the jump actions. \n(2) It uses the printed content to tell GPT the current mode. It prints the mode type \nfirst when user’s entering a mode, then the following response will be initiated in \ncorresponding style based on the condition statements about current mode. \n(3) Interaction can switch between different modes at any time. An option is provided \nif the user needs to switch the GPT’s response between using original content and \nreinterpreting and directly go back. Users can also restart from beginning. \n \nFigure 17. Structure of template 2. \n\nComputing and Artificial Intelligence 2024, 2(1), 535.  \n20 \n4.3.3. Modes tests and final optimization \nThe four interaction modes based on the final optimized document are tested. The \napproximate accuracies are categorized as “perfect”, “excellent”, “good”, “fair” and \n“bad” based on 5 times of tests. \nAs shown in Table 5, generally, the performance of template 1 is good. There \nare some problems. For example, in Mode 2, when the user adds additional words to \ninput, all contents will be changed including the interactive request. \nTable 5. Performance of template 1. \nModes Accuracy Problems Description \n1 perfect None \n2 good Changes the section title and interactive request after using NLP \n3 fair Cannot completely use original content \n4 excellent Cannot mix images and text from the beginning \noverall  The user has to add additional words in input to change current mode \nOriginal structure is likely messed after changing modes \nAs shown in Table 6, the performance of template 2 is also good. But in some \nmodes, it occasionally makes more mistakes compared to template 1. For example, in \nMode 1, sometimes it skips the code and miss the image display, and it is solved when \nthe user gives a reminder. \nTable 6. Performance of template 2. \nModes Accuracy Problems description \n1 good Sometimes miss the image when first entering and needs a reminder \nSometime mess the steps \n2 excellent Occasionally miss the images \n3 good Cannot completely use original content \n4 good \nCannot mix images and text from the beginning \nOccasionally gives a wrong image to a right answer (but the code is correct) \nOccasionally provides insufficient image for the answer \noverall  None \nCompared to the two performances, it is preferred to select template 2 as the final \noptimization outcome. The reason is that it does not require the user to add additional \nworks in input, which meets the simplicity of HCD principles. Another reason is that \nit has larger space for promotion, since it uses “Instructions” area to avoid the inherent \nlimitation of using control section in the document, because it can provide conditional \nstatements without jumping to it. \nDetail information of the final document of Template can be found in Appendix \nB. \n5. Discussion \nSome findings in the series of experiments include: \n(1) Changing interaction modes while using a shared knowledge part is not an \neasy task. The more users’ needs integrated, the more difficult the organization the \nComputing and Artificial Intelligence 2024, 2(1), 535.  \n21 \ndocument is optimized. \n(2) GPT’s jump action is an abstract description of GPT’s behavior. Jump action \nbased on decision of where to go, essentially is a searching and proceeding action, \nwhich means it has to search the information inside the document and decide what it \nthe most relevant one to user’s input. This characteristic makes it difficult for GPT to \nconsider both the conditional statements in current position and in another position \ninside the document, because when it goes to conditional statements somewhere else \nfrom current location and proceeds some steps, it usually has already “lost” current \nlocation. Such complex action may require inherent workflow with different \ncomponents with higher complexity. \n(3) “Instructions” area has the highest priority, so creators are supposed to check \nwhether rules in it contradict specific conditions in the document. Text in “Instructions” \narea can work simultaneously with any text it is positioning, which is useful. \n(4) Section title, conditional statements, interactive requests, user’s input, and \ncontent that has already been generated as context can all effect GPT’s new content \ngeneration. This is easy to explain when an interactive request does not provide \nexplicit way of what to do, GPT will find explicit way somewhere else. Since the \nessence of relocation is actually a searching action,  \n(5) GPT tends to use NLP to give responses, unless there are explicit steps with \nhigh weight to force it give original content from the document. Even within “print ( )” \nfunction, sometimes GPT refuse to give irrelevant contents and use NLP to change \nthem. \n(6) It is hard for GPT to accurately identify the correct way to go if provided a \nseries of conditional statements structured in tree branch, which may be because when \nGPT has already found an information inside a conditional statement branch, it may \nthink this is the most relevant information and stop finding other. Therefore, the \nstructure of conditions needs to be well designed. The document and “Instructions” \narea can work together to achieve this. \n(7) A failure like going to a wrong section can make the following interaction \nchaotic and not easy to correct the order. It may be because the incorrect context has \nalready been produced, interfering GPT’s following judgement. \n6. Conclusion \nThis study concludes HCD guidelines in LLMs and tries to integrate them into \nan experiment of using a single document as new knowledge in GPT to meet user’s \ndiverse software learning needs. It is found that without high-level code, it is not easy \nto integrate all diverse needs perfectly into one GPT. The natural language \ncharacteristic of GPT is generally a merit for comprehensively understand the \ndocument and user’s input, while in some cases, becomes an interference of \nproceeding mixed steps to generate preset content and creative content together, which \nmay need preset components and workflow inside GPT with higher control level. The \noutcomes provide preliminary thinking about how to organize different elements in \nthe document as GPT’s new knowledge and setup GPTs’ configuration, and the final \noptimized document also provides a template for futural application or research with \nthe same requirements. \nComputing and Artificial Intelligence 2024, 2(1), 535.  \n22 \nSome variables are not considered into the experiment, which can be explored in \nthe future, such as the follows: \n(1) How much the inherent knowledge of LLMs will interfere the understanding and \nextracting the content in the new knowledge [29]? How will the experiment in \nthe study result is if the knowledge of the software is replaced by that from a \ntotally new software? \n(2) Will the length of context affect the GPT’s judgement of current modes in the \nexperiment [30]? \n(3) Are there any other types of element’s organization of the document that can help \nenhance user experience? \n(4) If speed is also taken into consideration, how to optimize the document to better \nachieve HCD principles. \nLooking forward, the focus should be on advancing LLMs and their application \nto better enhance UX. Continued exploration in this domain will likely lead to more \nsophisticated, user-centric HCI systems, aligning technology more closely with human \nneeds and behaviors. \nAuthor contributions: Conceptualization, YW and YSL; methodology, YW; \nliterature, YW, YSL, RH, JW and SL; software, YW; Investigation, YW; validation, \nYW; formal analysis, YW; resources, YW and YSL; data curation, YW and YSL; \nwriting—original draft preparation, YW; writing—review and editing, YW and YSL; \nvisualization, YW; supervision, YW and YSL. All authors have read and agreed to the \npublished version of the manuscript. \nConflict of interest: The authors declare no conflict of interest. \nReferences \n1. Gokul A. LLMs and AI: Understanding Its Reach and Impact. Published online May 4, 2023. doi: \n10.20944/preprints202305.0195.v1 \n2. Liu J, Shen D, Zhang Y, et al. What Makes Good In-Context Examples for GPT-3? Published online 2021. doi: \n10.48550/ARXIV.2101.06804 \n3. Park TJ, Dhawan K, Koluguri N, et al. Enhancing Speaker Diarization with Large Language Models: A Contextual Beam \nSearch Approach. Published online 2023. doi: 10.48550/ARXIV.2309.05248 \n4. Thomas P, Spielman S, Craswell N, et al. Large language models can accurately predict searcher preferences. Published \nonline 2023. doi: 10.48550/ARXIV.2309.10621 \n5. Ortega Pedro A, Maini V, DeepMind Safety Team. Building safe artificial intelligence: specification, robustness, and \nassurance. Available online: https://deepmindsafetyresearch.medium.com/building-safe-artificial-intelligence-52f5f75058f1 \n(accessed on 2 January 2024). \n6. Xiong S, Payani A, Kompella R, et al. Large Language Models Can Learn Temporal Reasoning. Published online Feburary \n20, 2024. doi: 10.48550/arXiv.2401.06853 \n7. Ji J, Qiu T, Chen B, et al. Ai alignment: A comprehensive survey. arXiv. 2023; arXiv:2310.19852. doi: \n10.48550/arXiv.2310.19852  \n8. Weidinger L, Mellor J, Rauh M, et al. Ethical and social risks of harm from language models. arXiv. 2021; \narXiv:2112.04359. doi: 10.48550/arXiv.2112.04359 \n9. Guinness H. How does ChatGPT work? Available online: https://zapier.com/blog/how-does-chatgpt-work/ (accessed on 6 \nJanuary 2024). \n10. Yan C, Qiu Y, Zhu Y. Predict Oil Production with LSTM Neural Network. Proceedings of the 9th International Conference \non Computer Engineering and Networks. Publish online 2021. doi: 10.1007/978-981-15-3753-0_34 \nComputing and Artificial Intelligence 2024, 2(1), 535.  \n23 \n11. Subramonyam H, Im J, Seifert C, et al. Solving Separation-of-Concerns Problems in Collaborative Design of Human-AI \nSystems through Leaky Abstractions. CHI Conference on Human Factors in Computing Systems. Published online April 29, \n2022. doi: 10.1145/3491102.3517537 \n12. Zhang P, Jaipersaud B, Ba J, et al. Classifying Course Discussion Board Questions using LLMs. Proceedings of the 2023 \nConference on Innovation and Technology in Computer Science Education V 2. Published online June 29, 2023. doi: \n10.1145/3587103.3594202 \n13. Don N. User centered system design—New perspectives on human-computer interaction. CRC Press; 1986. \n14. Geyer W, Weisz J, Pinhanez CS. What is human-centered AI? Available online: https://research.ibm.com/blog/what-is-\nhuman-centered-ai (accessed 12 January 2024). \n15. Farooqui T, Rana T, Jafari F. Impact of human-centered design process (HCDP) on software development process. In: 2019 \n2nd International Conference on Communication, Computing and Digital systems (C-CODE) 2019 Mar 6. doi: 10.1109/C-\nCODE.2019.8680978  \n16. Gulliksen J, Göransson B, Boivie I, et al. Key principles for user-centred systems design. Behaviour and Information \nTechnology. 2003; 22(6): 397-409. doi: 10.1080/01449290310001624329 \n17. Jaimes A, Gatica-Perez D, Sebe N, et al. Guest Editors’ Introduction: Human-Centered Computing--Toward a Human \nRevolution. Computer. 2007; 40(5): 30-34. doi: 10.1109/mc.2007.169 \n18. Mack K, McDonnell E, Potluri V, et al. Anticipate and Adjust: Cultivating Access in Human-Centered Methods. CHI \nConference on Human Factors in Computing Systems. Published online April 29, 2022. doi: 10.1145/3491102.3501882 \n19. Wang Y, Lin YS. Public participation in urban design with augmented reality technology based on indicator evaluation. \nFrontiers in Virtual Reality. 2023; 4. doi: 10.3389/frvir.2023.1071355 \n20. Wu C. The Impact of Public Green Space Views on Indoor Thermal Perception and Environment Control Behavior of \nResidents - A Survey Study in Shanghai. European Journal of Sustainable Development. 2023; 12(3): 131. doi: \n10.14207/ejsd.2023.v12n3p131 \n21. Seffah A, Andreevskaia A. Empowering software engineers in human-centered design. 25th International Conference on \nSoftware Engineering, 2003 Proceedings. Published online 2003. doi: 10.1109/icse.2003.1201251 \n22. Chao Yan. Predict Lightning Location and Movement with Atmospherical Electrical Field Instrument. Proceedings of the \n10th Annual Information Technology, Electronics and Mobile Communication Conference (IEMCON). Publish online 2019. \ndoi: 10.1109/IEMCON.2019.8936293 \n23. Petridis S, Terry M, Cai CJ. PromptInfuser: Bringing User Interface Mock-ups to Life with Large Language Models. \nExtended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems. Published online April 19, 2023. \ndoi: 10.1145/3544549.3585628 \n24. Park J, Choi D. AudiLens: Configurable LLM-Generated Audiences for Public Speech Practice. Adjunct Proceedings of the \n36th Annual ACM Symposium on User Interface Software and Technology. Published online October 29, 2023. doi: \n10.1145/3586182.3625114 \n25. Di Fede G, Rocchesso D, Dow SP, et al. The Idea Machine: LLM-based Expansion, Rewriting, Combination, and \nSuggestion of Ideas. Creativity and Cognition. Published online June 20, 2022. doi: 10.1145/3527927.3535197 \n26. Korbak T, Shi K, Chen A, et al. Pretraining language models with human preferences. Available online: \nhttps://arxiv.org/abs/2302.08582 (accessed on 5 January 2024). \n27. Rastogi C, Tulio Ribeiro M, King N, et al. Supporting Human-AI Collaboration in Auditing LLMs with LLMs. Proceedings \nof the 2023 AAAI/ACM Conference on AI, Ethics, and Society. Published online August 8, 2023. doi: \n10.1145/3600211.3604712 \n28. Unity. Available online: https://docs.unity3d.com/Packages/com.unity.visualscripting@1.9/manual/ (accessed 11 January \n2024). \n29. Weng Y, Wu J. Fortifying the Global Data Fortress: A Multidimensional Examination of Cyber Security Indexes and Data \nProtection Measures across 193 Nations. International Journal of Frontiers in Engineering Technology. Publish online 2024. \ndoi: 10.25236/IJFET.2024.060203 \n30. Wang C, Yang Y, Li R, et al. Proceedings of the 2024 International Conference on Image Processing and Computer \nApplications (IPCA 2024). Adapting LLMs for Efficient Context Processing through Soft Prompt Compression. Publish \nonline April 7, 2024. doi: 10.48550/arXiv.2404.04997 \n  \nComputing and Artificial Intelligence 2024, 2(1), 535.  \n24 \nAppendix A \nFor details, please refer to \nhttps://onedrive.live.com/edit?id=463C91BDAAC45E41!sdc4a46768b8b4df38b02af6269061337&resid=463C91BD\nAAC45E41!sdc4a46768b8b4df38b02af6269061337&cid=463c91bdaac45e41&ithint=file%2Cdocx&redeem=aHR0c\nHM6Ly8xZHJ2Lm1zL3cvYy80NjNjOTFiZGFhYzQ1ZTQxL0VYWkdTdHlMaV9OTml3S3ZZbWtHRXpjQmNHbX\nRweXpDWjhNUkktdXJPQ1VFMEE_ZT05aFRnWms&migratedtospo=true&wdo=2 \n  \nComputing and Artificial Intelligence 2024, 2(1), 535.  \n25 \nAppendix B \nFor details, please refer to \nhttps://onedrive.live.com/edit?id=463C91BDAAC45E41!sc0b5fa94e7fd4f098d0dc2db2a52348e&resid=463C91BDA\nAC45E41!sc0b5fa94e7fd4f098d0dc2db2a52348e&cid=463c91bdaac45e41&ithint=file%2Cdocx&redeem=aHR0cHM\n6Ly8xZHJ2Lm1zL3cvYy80NjNjOTFiZGFhYzQ1ZTQxL0VaVDZ0Y0Q5NXdsUGpRM0MyeXBTTkk0QjQ2OW9o\nLXJMNjctZWZ6eXkwY1J5YWc_ZT1vME8wa0E&migratedtospo=true&wdo=2 ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6316946148872375
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5544718503952026
    },
    {
      "name": "Software",
      "score": 0.5203692317008972
    },
    {
      "name": "Knowledge base",
      "score": 0.4910769462585449
    },
    {
      "name": "User experience design",
      "score": 0.46925708651542664
    },
    {
      "name": "Knowledge management",
      "score": 0.4646814167499542
    },
    {
      "name": "Software engineering",
      "score": 0.34354454278945923
    },
    {
      "name": "World Wide Web",
      "score": 0.2376047968864441
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I117965899",
      "name": "University of Hawaiʻi at Mānoa",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I12912129",
      "name": "Northeastern University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I74973139",
      "name": "Carnegie Mellon University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I59553526",
      "name": "Stony Brook University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I204465549",
      "name": "Washington University in St. Louis",
      "country": "US"
    }
  ]
}