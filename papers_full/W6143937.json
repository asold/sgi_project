{
  "title": "Language models for automatic speech recognition : construction and complexity control",
  "url": "https://openalex.org/W6143937",
  "year": 2007,
  "authors": [
    {
      "id": "https://openalex.org/A295460231",
      "name": "Vesa Siivola",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1983389172",
    "https://openalex.org/W2013196554",
    "https://openalex.org/W1888315835",
    "https://openalex.org/W119220656",
    "https://openalex.org/W2103139809",
    "https://openalex.org/W2056590938",
    "https://openalex.org/W2157477135",
    "https://openalex.org/W2902905787",
    "https://openalex.org/W2032942114",
    "https://openalex.org/W45102278",
    "https://openalex.org/W2167934275",
    "https://openalex.org/W2050693797",
    "https://openalex.org/W2100373303",
    "https://openalex.org/W2151687858",
    "https://openalex.org/W1590952807",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W47415966",
    "https://openalex.org/W57918715",
    "https://openalex.org/W2049631025",
    "https://openalex.org/W2159518412",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W2107878631",
    "https://openalex.org/W38194800",
    "https://openalex.org/W2161898425",
    "https://openalex.org/W1903115690",
    "https://openalex.org/W2125838338",
    "https://openalex.org/W1517590677",
    "https://openalex.org/W1591095982",
    "https://openalex.org/W139293362",
    "https://openalex.org/W2088857750",
    "https://openalex.org/W2042783153",
    "https://openalex.org/W2053306448",
    "https://openalex.org/W2117621558",
    "https://openalex.org/W1601018556",
    "https://openalex.org/W120322644",
    "https://openalex.org/W1991133427",
    "https://openalex.org/W2118714763",
    "https://openalex.org/W2049633694",
    "https://openalex.org/W1989705153",
    "https://openalex.org/W2129940916",
    "https://openalex.org/W1970689298",
    "https://openalex.org/W1991696363",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W1955233831",
    "https://openalex.org/W9740975",
    "https://openalex.org/W2127836646",
    "https://openalex.org/W2124008567",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2050938027",
    "https://openalex.org/W2171457693",
    "https://openalex.org/W2056250865",
    "https://openalex.org/W2066539191",
    "https://openalex.org/W262058279",
    "https://openalex.org/W58893626",
    "https://openalex.org/W2035238564",
    "https://openalex.org/W2134237567",
    "https://openalex.org/W4930780",
    "https://openalex.org/W813000",
    "https://openalex.org/W2304025599",
    "https://openalex.org/W1607753411",
    "https://openalex.org/W1559536185",
    "https://openalex.org/W2075201173",
    "https://openalex.org/W91079317",
    "https://openalex.org/W2133486044",
    "https://openalex.org/W1495314497",
    "https://openalex.org/W3183153947",
    "https://openalex.org/W2082092506",
    "https://openalex.org/W2010910318",
    "https://openalex.org/W1757803263",
    "https://openalex.org/W2121227244",
    "https://openalex.org/W1600680149",
    "https://openalex.org/W155299729",
    "https://openalex.org/W2100895413",
    "https://openalex.org/W2105934661",
    "https://openalex.org/W1967325250",
    "https://openalex.org/W2400565116",
    "https://openalex.org/W2106554350",
    "https://openalex.org/W2610491700",
    "https://openalex.org/W1679913846",
    "https://openalex.org/W1926502259",
    "https://openalex.org/W2080213370",
    "https://openalex.org/W36903255",
    "https://openalex.org/W1966812932",
    "https://openalex.org/W1683106298",
    "https://openalex.org/W1542491098",
    "https://openalex.org/W2129812225",
    "https://openalex.org/W1498225184",
    "https://openalex.org/W1644652583",
    "https://openalex.org/W2097009961",
    "https://openalex.org/W2127384912",
    "https://openalex.org/W1797288984",
    "https://openalex.org/W1549285799",
    "https://openalex.org/W154368987",
    "https://openalex.org/W2101711363",
    "https://openalex.org/W1956785191",
    "https://openalex.org/W2099926694",
    "https://openalex.org/W2042143122",
    "https://openalex.org/W2047012355",
    "https://openalex.org/W1707124376",
    "https://openalex.org/W187516025",
    "https://openalex.org/W2133692508",
    "https://openalex.org/W10704533",
    "https://openalex.org/W1818785862",
    "https://openalex.org/W201288405",
    "https://openalex.org/W2402261002",
    "https://openalex.org/W1808032177",
    "https://openalex.org/W1966458180",
    "https://openalex.org/W2026408911",
    "https://openalex.org/W1545214528",
    "https://openalex.org/W68797657",
    "https://openalex.org/W2155693943",
    "https://openalex.org/W2015039982"
  ],
  "abstract": "The language model is one of the key components of a large vocabulary continuous speech recognition system. Huge text corpora can be used for training the language models. In this thesis, methods for extracting the essential information from the training data and expressing the information as a compact model are studied.\\n\\nThe thesis is divided in three main parts. In the first part, the issue of choosing the best base modeling unit for the prevalent language modeling method, n-gram language modeling, is examined. The experiments are focused on morpheme-like subword units, although syllables are also tried. Rule-based grammatical methods and unsupervised statistical methods for finding morphemes are compared with the baseline word model. The Finnish cross-entropy and speech recognition experiments show that significantly more efficient models can be created using automatically induced morpheme-like subword units as the basis of the language model.\\n\\nIn the second part, methods for choosing the n-grams that have explicit probability estimates in the n-gram model are studied. Two new methods specialized on selecting the n-grams for Kneser-Ney smoothed n-gram models are presented, one for pruning and one for growing the model. The methods are compared with entropy-based pruning and Kneser pruning. Experiments on Finnish and English text corpora show that the proposed pruning method gives considerable improvements over the previous pruning algorithms for Kneser-Ney smoothed models and also is better than entropy pruned Good-Turing smoothed model. Using the growing algorithm for creating a starting point for the pruning algorithm further improves the results. The improvements in Finnish speech recognition over the other Kneser-Ney smoothed models were significant as well.\\n\\nTo extract more information from the training corpus, words should not be treated as independent tokens. The syntactic and semantic similarities of the words should be taken into account in the language model. The last part of this thesis explores, how these similarities can be modeled by mapping the words into continuous space representations. A language model formulated in the state-space modeling framework is presented. Theoretically, the state-space language model has several desirable properties. The state dimension should determine, how much the model is forced to generalize. The need to learn long-term dependencies should be automatically balanced with the need to remember the short-term dependencies in detail. The experiments show that training a model that fulfills all the theoretical promises is hard: the training algorithm has high computational complexity and it mainly finds local minima. These problems still need further research.",
  "full_text": null,
  "topic": "Language model",
  "concepts": [
    {
      "name": "Language model",
      "score": 0.8602546453475952
    },
    {
      "name": "Computer science",
      "score": 0.7627512216567993
    },
    {
      "name": "Pruning",
      "score": 0.6797587871551514
    },
    {
      "name": "n-gram",
      "score": 0.6267046332359314
    },
    {
      "name": "Morpheme",
      "score": 0.625923216342926
    },
    {
      "name": "Artificial intelligence",
      "score": 0.623444676399231
    },
    {
      "name": "Natural language processing",
      "score": 0.5662124752998352
    },
    {
      "name": "Cache language model",
      "score": 0.5488420724868774
    },
    {
      "name": "Speech recognition",
      "score": 0.49511900544166565
    },
    {
      "name": "Vocabulary",
      "score": 0.4531247913837433
    },
    {
      "name": "Principle of maximum entropy",
      "score": 0.44807836413383484
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.43581804633140564
    },
    {
      "name": "Natural language",
      "score": 0.19486621022224426
    },
    {
      "name": "Linguistics",
      "score": 0.13039955496788025
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Universal Networking Language",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Comprehension approach",
      "score": 0.0
    }
  ],
  "institutions": []
}