{
    "title": "GROVER: Self-supervised Message Passing Transformer on Large-scale Molecular Data.",
    "url": "https://openalex.org/W3038324632",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A5100767600",
            "name": "Yu Rong",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5045777220",
            "name": "Yatao Bian",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5005345630",
            "name": "Tingyang Xu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5007120124",
            "name": "Weiyang Xie",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5011883201",
            "name": "Ying Wei",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5032642601",
            "name": "Wenbing Huang",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5068865316",
            "name": "Junzhou Huang",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2996443485",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W2406943157",
        "https://openalex.org/W1677182931",
        "https://openalex.org/W2962876364",
        "https://openalex.org/W2790808809",
        "https://openalex.org/W2060531713",
        "https://openalex.org/W2952254971",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2962756421",
        "https://openalex.org/W2973114758",
        "https://openalex.org/W2461620095",
        "https://openalex.org/W2624431344",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2912078280",
        "https://openalex.org/W2968734407",
        "https://openalex.org/W3104097132",
        "https://openalex.org/W2994710732",
        "https://openalex.org/W1757990252",
        "https://openalex.org/W2527189750",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2153693853",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2008505552",
        "https://openalex.org/W2991425343"
    ],
    "abstract": "How to obtain informative representations of molecules is a crucial prerequisite in AI-driven drug design and discovery. Recent researches abstract molecules as graphs and employ Graph Neural Networks (GNNs) for task-specific and data-driven molecular representation learning. Nevertheless, two dark clouds impede the usage of GNNs in real scenarios: (1) insufficient labeled molecules for supervised training; (2) poor generalization capabilities to new-synthesized molecules. To address them both, we propose a novel molecular representation framework, GROVER, which stands for Graph Representation frOm self-superVised mEssage passing tRansformer. With carefully designed self-supervised tasks in node, edge and graph-level, GROVER can learn rich structural and semantic information of molecules from enormous unlabelled molecular data. Rather, to encode such complex information, GROVER integrates Message Passing Networks with the Transformer-style architecture to deliver a class of more expressive encoders of molecules. The flexibility of GROVER allows it to be trained efficiently on large-scale molecular dataset without requiring any supervision, thus being immunized to the two issues mentioned above. We pre-train GROVER with 100 million parameters on 10 million unlabelled molecules---the biggest GNN and the largest training dataset that we have ever met. We then leverage the pre-trained GROVER to downstream molecular property prediction tasks followed by task-specific fine-tuning, where we observe a huge improvement (more than 6% on average) over current state-of-the-art methods on 11 challenging benchmarks. The insights we gained are that well-designed self-supervision losses and largely-expressive pre-trained models enjoy the significant potential on performance boosting.",
    "full_text": null
}