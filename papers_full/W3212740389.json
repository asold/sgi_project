{
  "title": "Masking and Transformer-based Models for Hyperpartisanship Detection in News",
  "url": "https://openalex.org/W3212740389",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5079679653",
      "name": "Javier Sánchez-Junquera",
      "affiliations": [
        "Universitat Politècnica de València"
      ]
    },
    {
      "id": "https://openalex.org/A5053947754",
      "name": "Paolo Rosso",
      "affiliations": [
        "Universitat Politècnica de València"
      ]
    },
    {
      "id": "https://openalex.org/A5050780136",
      "name": "Manuel Montes-y-Gómez",
      "affiliations": [
        "National Institute of Astrophysics, Optics and Electronics"
      ]
    },
    {
      "id": "https://openalex.org/A5017409996",
      "name": "Simone Paolo Ponzetto",
      "affiliations": [
        "University of Mannheim"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2955041501",
    "https://openalex.org/W3165606256",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2143259515",
    "https://openalex.org/W2478081149",
    "https://openalex.org/W3152857499",
    "https://openalex.org/W2034045724",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W2593408211",
    "https://openalex.org/W2804927761",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2740642134"
  ],
  "abstract": "Hyperpartisan news show an extreme manipulation of reality based on an underlying and extreme ideological orientation.Because of its harmful effects at reinforcing one's bias and the posterior behavior of people, hyperpartisan news detection has become an important task for computational linguists.In this paper, we evaluate two different approaches to detect hyperpartisan news.First, a text masking technique that allows us to compare style vs. topicrelated features in a different perspective from previous work.Second, the transformer-based models BERT, XLM-RoBERTa, and M-BERT, known for their ability to capture semantic and syntactic patterns in the same representation.Our results corroborate previous research on this task in that topic-related features yield better results than style-based ones, although they also highlight the relevance of using higherlength n-grams.Furthermore, they show that transformer-based models are more effective than traditional methods, but this at the cost of greater computational complexity and lack of transparency.Based on our experiments, we conclude that the beginning of the news show relevant information for the transformers at distinguishing effectively between left-wing, mainstream, and right-wing orientations.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7821245789527893
    },
    {
      "name": "Transformer",
      "score": 0.7289849519729614
    },
    {
      "name": "Natural language processing",
      "score": 0.46366220712661743
    },
    {
      "name": "Salient",
      "score": 0.42967334389686584
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4290415346622467
    },
    {
      "name": "Language model",
      "score": 0.4225325584411621
    },
    {
      "name": "Transparency (behavior)",
      "score": 0.41391342878341675
    },
    {
      "name": "Machine learning",
      "score": 0.3296578526496887
    },
    {
      "name": "Computer security",
      "score": 0.10009488463401794
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}