{
  "title": "Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks",
  "url": "https://openalex.org/W3127853158",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5069505458",
      "name": "Antonio Mastropaolo",
      "affiliations": [
        "Università della Svizzera italiana"
      ]
    },
    {
      "id": "https://openalex.org/A5079406478",
      "name": "Simone Scalabrino",
      "affiliations": [
        "University of Molise"
      ]
    },
    {
      "id": "https://openalex.org/A5027300975",
      "name": "Nathan Cooper",
      "affiliations": [
        "William & Mary"
      ]
    },
    {
      "id": "https://openalex.org/A5031468932",
      "name": "David N. Palacio",
      "affiliations": [
        "William & Mary"
      ]
    },
    {
      "id": "https://openalex.org/A5041262116",
      "name": "Denys Poshyvanyk",
      "affiliations": [
        "William & Mary"
      ]
    },
    {
      "id": "https://openalex.org/A5009727039",
      "name": "Rocco Oliveto",
      "affiliations": [
        "University of Molise"
      ]
    },
    {
      "id": "https://openalex.org/A5056526226",
      "name": "Gabriele Bavota",
      "affiliations": [
        "Università della Svizzera italiana"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2077273779",
    "https://openalex.org/W2739564891",
    "https://openalex.org/W2039168567",
    "https://openalex.org/W4254753190",
    "https://openalex.org/W2143960295",
    "https://openalex.org/W2142537222",
    "https://openalex.org/W2025791343",
    "https://openalex.org/W2142741391",
    "https://openalex.org/W2979679630",
    "https://openalex.org/W2736762043",
    "https://openalex.org/W2042124591",
    "https://openalex.org/W6638749077",
    "https://openalex.org/W6765469073",
    "https://openalex.org/W2395935897",
    "https://openalex.org/W4231241365",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W2145373440",
    "https://openalex.org/W2143861926",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3006962146",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W2907705732",
    "https://openalex.org/W2972082064",
    "https://openalex.org/W6778305296",
    "https://openalex.org/W6637698695",
    "https://openalex.org/W6778205038",
    "https://openalex.org/W6683258052",
    "https://openalex.org/W6679436768",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W2516621648",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W6768003788",
    "https://openalex.org/W6674330103",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W2267186426",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6692071231",
    "https://openalex.org/W2888312537",
    "https://openalex.org/W3016234956",
    "https://openalex.org/W1971650562",
    "https://openalex.org/W2242083635",
    "https://openalex.org/W2740220421",
    "https://openalex.org/W2884681705",
    "https://openalex.org/W2993007949",
    "https://openalex.org/W3005628256",
    "https://openalex.org/W6760150090",
    "https://openalex.org/W6779831603",
    "https://openalex.org/W6776236589",
    "https://openalex.org/W6757842265",
    "https://openalex.org/W2967096374",
    "https://openalex.org/W2090878800",
    "https://openalex.org/W2964194820",
    "https://openalex.org/W1972141422",
    "https://openalex.org/W2884276923",
    "https://openalex.org/W2964322208",
    "https://openalex.org/W2619636279",
    "https://openalex.org/W2081749632",
    "https://openalex.org/W2294980783",
    "https://openalex.org/W2166879716",
    "https://openalex.org/W1965194038",
    "https://openalex.org/W2117228548",
    "https://openalex.org/W2133333349",
    "https://openalex.org/W3101506519",
    "https://openalex.org/W3092172192",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2921792613",
    "https://openalex.org/W2905288168",
    "https://openalex.org/W2257123346",
    "https://openalex.org/W3105903381",
    "https://openalex.org/W3028741749",
    "https://openalex.org/W2949888546",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2958953787",
    "https://openalex.org/W2973529529",
    "https://openalex.org/W1753482797",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W2145124323",
    "https://openalex.org/W3014797428",
    "https://openalex.org/W1975455521",
    "https://openalex.org/W3038079173",
    "https://openalex.org/W3025993830",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2784121710",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2064232673"
  ],
  "abstract": "Deep learning (DL) techniques are gaining more and more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies in the Natural Language Processing (NLP) field have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of NLP tasks. The basic idea behind T5 is to first pre-train a model on a large and generic dataset using a self-supervised task ( e.g: filling masked words in sentences). Once the model is pre-trained, it is fine-tuned on smaller and specialized datasets, each one related to a specific task ( e.g: language translation, sentence classification). In this paper, we empirically investigate how the T5 model performs when pre-trained and fine-tuned to support code-related tasks. We pre-train a T5 model on a dataset composed of natural language English text and source code. Then, we fine-tune such a model by reusing datasets used in four previous works that used DL techniques to: (i) fix bugs, (ii) inject code mutants, (iii) generate assert statements, and (iv) generate code comments. We compared the performance of this single model with the results reported in the four original papers proposing DL-based solutions for those four tasks. We show that our T5 model, exploiting additional data for the self-supervised pre-training phase, can achieve performance improvements over the four baselines.",
  "full_text": "Studying the Usage of Text-To-Text Transfer\nTransformer to Support Code-Related Tasks\nAntonio Mastropaolo∗, Simone Scalabrino †, Nathan Cooper ‡, David Nader Palacio ‡, Denys Poshyvanyk ‡,\nRocco Oliveto†, Gabriele Bavota ∗\n∗SEART @ Software Institute, Universit `a della Svizzera italiana (USI), Switzerland\n†University of Molise, Italy\n‡SEMERU @ Computer Science Department, William and Mary, USA\nAbstract—Deep learning (DL) techniques are gaining more and\nmore attention in the software engineering community. They have\nbeen used to support several code-related tasks, such as automatic\nbug ﬁxing and code comments generation. Recent studies in the\nNatural Language Processing (NLP) ﬁeld have shown that the\nText-To-Text Transfer Transformer (T5) architecture can achieve\nstate-of-the-art performance for a variety of NLP tasks. The basic\nidea behind T5 is to ﬁrst pre-train a model on a large and generic\ndataset using a self-supervised task ( e.g., ﬁlling masked words in\nsentences). Once the model is pre-trained, it is ﬁne-tuned on\nsmaller and specialized datasets, each one related to a speciﬁc\ntask ( e.g., language translation, sentence classiﬁcation). In this\npaper, we empirically investigate how the T5 model performs\nwhen pre-trained and ﬁne-tuned to support code-related tasks.\nWe pre-train a T5 model on a dataset composed of natural\nlanguage English text and source code. Then, we ﬁne-tune such\na model by reusing datasets used in four previous works that\nused DL techniques to: (i) ﬁx bugs, (ii) inject code mutants, (iii)\ngenerate assert statements, and (iv) generate code comments. We\ncompared the performance of this single model with the results\nreported in the four original papers proposing DL-based solutions\nfor those four tasks. We show that our T5 model, exploiting\nadditional data for the self-supervised pre-training phase, can\nachieve performance improvements over the four baselines.\nIndex Terms—Empirical software engineering, Deep Learning\nI. I NTRODUCTION\nDeep Learning (DL) has been used to support a vast vari-\nety of code-related tasks. Some examples include automatic\nbug ﬁxing [1]–[4], learning generic code changes [5], code\nmigration [6], [7], code summarization [8]–[11], pseudo-code\ngeneration [12], code deobfuscation [13], [14], injection of\ncode mutants [15], automatic generation of assert statements\n[16], and code completion [17]–[21]. These works customize\nDL models proposed in the Natural Language Processing\n(NLP) ﬁeld to support the previously listed tasks. For instance,\nTufano et al. [1] used an RNN Encoder-Decoder architecture,\ncommonly adopted in Neural Machine Translation (NMT)\n[22]–[24], to learn how to automatically ﬁx bugs in Java\nmethods. The model learned bug-ﬁxing patterns by being\ntrained on pairs of buggy and ﬁxed methods mined from\nsoftware repositories. This work, as the vast majority of the\nones previously mentioned ( e.g., [5], [9], [11], [15], [16]),\nshare one common characteristic: They shape the problem at\nhand as a text-to-text transformation, in which the input and\nthe output of the model are text strings .\nFor example, in the work by Watson et al. [16] the input is a\nstring representing a test method without an assert statement,\nand the output is an appropriate assert statement for the\ngiven test. In the approach by Haque et al. [11], the input\nis composed of strings representing a subroutine to document,\nwhile the output is a natural language summary documenting\nthe subroutine.\nRecent years have seen the raise of transfer learning in the\nﬁeld of natural language processing. The basic idea is to ﬁrst\npre-train a model on a large and generic dataset by using a self-\nsupervised task, e.g., masking tokens in strings and asking the\nmodel to guess the masked tokens. Then, the trained model is\nﬁne-tuned on smaller and specialized datasets, each one aimed\nat supporting a speciﬁc task. In this context, Raffel et al. [25]\nproposed the T5 (Text-To-Text Transfer Transformer) model,\npre-trained on a large natural language corpus and ﬁne-tuned\nto achieve state-of-the-art performance on many tasks, all\ncharacterized by text-to-text transformations.\nThe goal of this work is to empirically investigate the\npotential of a T5 model when pre-trained and ﬁne-tuned to\nsupport many of the previously listed code-related tasks also\ncharacterized by text-to-text transformations. We started by\npre-training a T5 model using a large dataset consisting of\n499,618 English sentences and 1,569,889 source code compo-\nnents (i.e., methods). Then, we ﬁne-tune the model using four\ndatasets from previous work with the goal of supporting four\ncode-related tasks:\nAutomatic bug-ﬁxing. We use the dataset by Tufano et al.\n[1], composed of instances in which the “input string” is\nrepresented by a buggy Java method and the “output string”\nis the ﬁxed version of the same method.\nInjection of code mutants. This dataset is also by Tufano\net al. [15], and features instances in which the input-output\nstrings are reversed as compared to automatic bug-ﬁxing ( i.e.,\nthe input is a ﬁxed method, while the output is its buggy\nversion). The model must learn how to inject bugs (mutants)\nin code instead of ﬁxing bugs.\nGeneration of assert statements in test methods. We use the\ndataset by Watson et al. [16], composed of instances in which\nthe input string is a representation of a test method without\nan assert statement and a focal method it tests ( i.e., the main\nproduction method tested), while the output string encodes an\nappropriate assert statement for the input test method.\narXiv:2102.02017v1  [cs.SE]  3 Feb 2021\nCode Summarization. We use the dataset by Haque et al.\n[11] where input strings are some representations of a Java\nmethod to summarize, & an output string is a textual summary.\nOnce the T5 model has been ﬁne-tuned on all these tasks,\nwe run it on the same test sets used in the four referenced\nworks [1], [11], [15], [16] comparing the achieved results to\nthose reported in the original work. Our results show that the\nT5 model is able to improve the performance of the original\nmodels in all four tasks.\nWorth noticing is that, besides the different architecture of\nthe T5 model, the latter can take advantage of a pre-training\nphase in which additional training data is provided as input\nas compared to the four baselines. This could explain, at least\npartially, the boost of performance that we observed. Also, as\npreviously said, the additional pre-training is done in a self-\nsupervised way (i.e., by simply masking random tokens in the\ncode/text used for pre-training), making this step relatively\ncheap to perform and scalable to large code bases that can be\neasily collected from sources such as GitHub. In contrast, the\nfour baselines exploit a completely supervised training ( e.g.,\nin the case of automatic bug-ﬁxing, the baseline needs pairs\nof buggy and ﬁxed methods to be trained). Building such\na dataset for supervised training has a cost, and there are\nlimitations in terms of the amount of data one can mine.\nBesides the good performance ensured by the T5, having\na single model able to support different tasks can beneﬁt\ntechnological transfer since it simpliﬁes the implementation\nand the maintenance of a tool supporting several tasks. The\ncode and data used in this work are publicly available [26].\nII. R ELATED WORK\nDL techniques have been used to support many software\nengineering tasks. Due to space limitations, we discuss only\nthe approaches related to the four tasks we subject to our\nstudy, with particular attention on those used as baselines. We\nalso introduce notions needed to understand our experimental\ndesign.\nA. Automatic Bug-Fixing\nMany techniques have been proposed for the automatic\nﬁxing of software bugs. Several of them [27]–[35] rely on the\nredundancy assumption, claiming that large programs contain\nthe seeds of their own repair. Such an assumption has been\nveriﬁed by at least two independent studies [36], [37]. In this\nsection we focus on techniques exploiting DL for bug-ﬁxing.\nMesbah et al. [3] focus on build-time compilation failures\nby presenting DeepDelta, an approach using NMT to ﬁx the\nbuild. The input is represented by features characterizing the\ncompilation failure ( e.g., kind of error, AST path, etc.). As\noutput, DeepDelta provides the AST changes needed to ﬁx\nthe error. In the presented empirical evaluation, DeepDelta\ncorrectly ﬁxed 19,314 out of 38,788 (50%) compilation errors.\nChen et al. [2] present SequenceR, a sequence-to-sequence\napproach trained on over 35k single-line bug-ﬁxes. SequenceR\ntakes as input the buggy line together with its “abstract buggy\ncontext”, meaning the relevant code lines from the buggy class.\nThe output of the approach is the recommended ﬁx for\nthe buggy line. The approach, tested on a set of 4,711 bugs,\nwas able to automatically ﬁx 950 ( ∼20%) of them. Similar\napproaches have been proposed by Hata et al. [4] and Tufano\net al. [1]. The latter is the one we compared our approach with\nand, thus, we describe it in more details.\nTufano et al. [1] investigate the performance of an NMT-\nbased approach in the context of automatic bug-ﬁxing.\nThey train an encoder-decoder model on a set of bug-ﬁx\npairs (BFPs), meaning pairs of strings in which the ﬁrst one\n(input) represents a Java method that has been subject to a\nbug-ﬁxing activity, and the second one (target) represents the\nsame Java method once the bug was ﬁxed.\nTo build this dataset, the authors mined ∼787k bug-ﬁxing\ncommits from GitHub, from which they extracted ∼2.3M\nBFPs. After that, the code of the BFPs is abstracted to make\nit more suitable for the NMT model ( i.e., to reduce the\nvocabulary of terms used in the source code identiﬁers and\nliterals). The abstraction process is depicted in Fig. 1.\nraw source code\nabstracted code\nabstracted code with idioms\npublic Integer getMinElement(List myList) {\n   if(myList.size() >= 0) {\n      return ListManager.getFirst(myList);\n   }\n   return 0;\n}\npublic TYPE_1 METHOD_1 ( TYPE_2 VAR_1 ) \n{ if ( VAR_1 . METHOD_2 ( ) >= INT_1 ) \n{ return TYPE_3 . METHOD_3 ( VAR_1 ) ; } \nreturn INT_1 ; }\npublic TYPE_1 METHOD_1 ( List VAR_1 ) \n{ if ( VAR_1 . size ( ) >= 0 ) \n{ return TYPE_2 . METHOD_3 ( VAR_1 ) ; } \nreturn 0 ; }\nFig. 1: Abstraction process [1]\nThe top part of the ﬁgure represents the raw source code to\nabstract. The authors use a Java lexer and a parser to represent\neach method as a stream of tokens, in which Java keywords\nand punctuation symbols are preserved and the role of each\nidentiﬁer ( e.g., whether it represents a variable, method, etc.)\nas well as the type of a literal is discerned.\nIDs are assigned to identiﬁers and literals by considering\ntheir position in the method to abstract: The ﬁrst variable name\nfound will be assigned the ID of V AR 1, likewise the second\nvariable name will receive the ID of V AR 2. This process\ncontinues for all identiﬁers as well as for the literals ( e.g.,\nSTRING X, INT X, FLOAT X). The output of this stage is\nthe code reported in the middle of Fig. 1 (i.e., abstracted code).\nSince some identiﬁers and literals appear very often in the\ncode (e.g., variables i, j, literals 0, 1, method names such as\nsize), those are treated as “idioms” and are not abstracted\n(see bottom part of Fig. 1, idioms are in bold). Tufano et al.\nconsider as idioms the top 0.005% frequent words in their\ndataset. During the abstraction a mapping between the raw\nand the abstracted tokens is maintained, thus allowing to\nreconstruct the concrete code from the abstract code generated\nby the model.\nThe set of abstracted BFPs has been used to train and\ntest the approach. The authors build two different sets,\nnamely BFP small, only including methods having a maxi-\nmum length of 50 tokens (for a total of 58,350 instances), and\nBFP medium, including methods up to 100 tokens (65,455).\nThe model was able to correctly predict the patch for the\nbuggy code in 9% and 3% of cases in the BFP small and\nBFP medium dataset, respectively.\nWhile other works have tackled the automatic bug-ﬁxing\nproblem, the approach by Tufano et al. has been tested on\na variety of different bugs, rather than on speciﬁc types of\nbugs/warnings ( e.g., only single-line bugs are considered in\n[2], while compilation failures are addressed in [3]).\nThus, we picked it as representative DL technique for\nautomatic bug-ﬁxing and we use the two datasets by Tufano\net al. [1] to ﬁne-tune the T5 model for the “automatic bug-\nﬁxing” problem, comparing the achieved performance with the\none reported in the original paper.\nB. Injection of Code Mutants\nBrown et al. [38] were the ﬁrst to propose a data-driven\napproach for generating code mutants, leveraging bug-ﬁxes\nperformed in software systems to extract syntactic-mutation\npatterns from the diffs of patches. Tufano et al. [15] built\non this idea by presenting an approach using NMT to inject\nmutants representative of real bugs. The idea is similar to the\npreviously described “bug-ﬁxing” paper [1] with, however, the\nlearning happening in the opposite direction. Indeed, given\na bug-ﬁxing commit, the input to the model is in this case\nthe “ﬁxed method” ( i.e., the method obtained after the bug-\nﬁxing activity) while the target is the buggy method (before\nthe bug-ﬁx). This allows the model to learn how to inject\nin a working code a mutant representative of real bugs. The\napplied methodology is the same described for the bug-ﬁxing\nwork [15], including the abstraction process.\nThis is, to date, the only DL-based technique for injecting\ncode mutants. Thus, we use the dataset exploited by Tufano\net al. [15] to ﬁne-tune the T5 model for the problem of\n“injecting code mutants”, comparing the achieved results with\nthe ones reported in the original paper. Speciﬁcally, we reused\ntheir largest dataset, referred to as GMident in the paper 1,\nfeaturing 92,476 training instances, 11,560 used for hyperpa-\nrameter tuning (evaluation set), and 11,559 used for testing. On\nthis data, the approach by Tufano et al. was able to correctly\npredict the bug to inject in 17% of cases (1,991).\nC. Generation of Assert Statements in Test Methods\nWatson et al. [16] start from the work by Shamshiri\net al. [39], who observed that tools for the automatic gen-\neration of test cases such as Evosuite [40], Randoop [41]\nand Agitar [42] exhibit insufﬁciencies in the automatically\ngenerated assert statements.\n1A subset of this dataset named GMident−lit has also been used in\nthe original paper [15] to avoid including in the study bugs requiring the\ngeneration of previously unseen literals. We decided to test the T5 model on\nthe most complex and complete dataset.\nThus, they propose ATLAS, an approach for generating\nsyntactically and semantically correct unit test assert state-\nments using NMT. To train ATLAS, the authors mined 2.5M\ntest methods from GitHub with their corresponding assert\nstatement. For each of those test methods, they also identiﬁed\nthe focal method, meaning the main production code method\nexercised by the test. A preprocessing of the dataset has\nbeen performed to remove all test methods longer than 1K\ntokens. Also, test methods requiring the synthesis of one or\nmore unknown tokens for generating the appropriate assert\nstatements have been removed. Indeed, if the required tokens\ncannot be found in the vocabulary of the test method they\ncannot be synthesized when the model attempts to generate\nthe prediction. Finally, all duplicates have been removed from\nthe dataset, leading to a ﬁnal set of 158,096 Test-Assert\nPairs (TAPs). Each method left in the dataset has then been\nabstracted using the same approach previously described by\nTufano et al. [1]. However, in this case the authors experiment\nwith two datasets, one containing raw source code and one\nabstracted code. ATLAS was able to generate asserts identical\nto the ones written by developers in 31.42% of cases (4,968\nperfectly predicted assert statements) when only considering\nthe top-1 prediction, and 49.69% (7,857) when looking at the\ntop-5 in the abstracted dataset, while performance is lower on\nthe raw dataset (17.66% for top-1 and 23.33% for top-5).\nThis is the only DL-based technique proposed in the lit-\nerature to generate assert statements. We use the datasets\nby Watson et al. [16] to ﬁne-tune our T5 model for the\n“generation of assert statements” problem, and compare the\nachieved performance with the one in the original paper.\nD. Code Summarization\nCode summarization is one of the mainstream methods for\nautomatic documentation of source code. The proposed sum-\nmarization techniques fall into two categories: extractive [43]–\n[46] and abstractive [9], [11], [47]–[49]. The former create a\nsummary of a code component which includes information\nextracted from the component being summarized, while the\nlatter may include in the generated summaries information\nthat is not present in the code component to document.\nDL techniques have been used to support the generation of\nabstractive summaries.\nHu et al. [49] use a Deep Neural Network (DNN) to\nautomatically generate comments for a given Java method. The\nauthors mine ∼9k Java projects hosted on GitHub to collect\npairs of ⟨method, comment ⟩, where “comment” is the ﬁrst\nsentence of the Javadoc linked to the method. These pairs,\nproperly processed, are used to train and test the DNN. The\nauthors assess the effectiveness of their technique by using the\nBLEU-4 score [50], showing the superiority of their approach\nwith respect to the competitive technique presented in [51].\nAllamanis et al. [52] use attention mechanisms in neural\nnetworks to suggest a descriptive method name starting from\nan arbitrary snippet of code. Their approach can name a code\nsnippet exactly as a developer would do in ∼25% of cases.\nLeClair et al. [8] present a neural model combining the\nAST source code structure and words from code to generate\ncoherent summaries of Java methods. The approach, tested\non 2.1M methods, showed its superiority as compared to the\nprevious works by Hu et al. [49] and Iyer et al. [51].\nThe approach by Haque et al. [11] is the most recent in\nthe area of DL-aided source code summarization, and it is an\nimprovement of the work by LeClair et al. [8].\nIt still aims at documenting Java methods through an\nencoder-decoder architecture but, in this case, three inputs\nare provided to the model to generate the summary: (i) the\nsource code of the method, as a ﬂattened sequence of tokens\nrepresenting the method; (ii) its AST representation; and (iii)\nthe “ﬁle context”, meaning the code of every other method\nin the same ﬁle. The authors show that adding the contextual\ninformation as one of the inputs substantially improves the\nBLEU score obtained by deep learning techniques. The dataset\nused in the evaluation is composed of 2.1M Java methods\npaired with summaries. We reuse this dataset for the ﬁne-\ntuning of the T5 model for the code summarization problem,\nand compare its performance to the state-of-the-art approach\nproposed by Haque et al. [11].\nIII. M ULTITASK LEARNING FOR CODE -RELATED TASKS\nThe T5 model was introduced by Raffelet al. [25] to support\nmultitask learning in the domain of NLP. This approach is\nbased on two phases: pre-training, which allows deﬁning a\nshared knowledge-base useful for a large class of sequence-to-\nsequence tasks, and ﬁne-tuning, which specializes the model to\nspeciﬁc tasks of interest. In this section, we ﬁrst provide basic\ninformation about the T5 model (refer to [25] for a detailed\nexplanation of the architecture). Then, we explain how we\nadapted it to the software engineering domain, with the goal of\nsupporting the four tasks previously described: automatic bug-\nﬁxing, generation of assert statements in test methods , code\nsummarization, and injection of code mutants . Such a process\nis depicted in Fig. 2. Finally, we describe the hyperparameter\ntuning of the model and the adopted decoding strategy.\nA. T5 in a Nutshell\nThe T5 model is based on the transformer model architec-\nture [53] that allows to handle a variable-sized input using\nstacks of self-attention layers [54] instead of RNNs or CNNs.\nWhen an input sequence is provided, it is mapped to a\nsequence of embeddings that is passed into the encoder.\nThe encoders are all identical in structure and each one\nis comprised of two subcomponents: a self-attention layer\nfollowed by a small feed-forward network . Layer normal-\nization [55] is applied to the input of each subcomponent\nwhile a residual skip connection [56] adds each input of the\nsubcomponent to its output. Dropout [57] is applied within the\nfeed-forward network, on the skip connection, on the attention\nweights, and at the input and output of the entire stack. The\ndecoders work similarly to the encoders: Each self-attention\nlayer is followed by an additional attention mechanism that\nattends to the output of the encoder.\nThe output of the ﬁnal decoder block is fed into a dense\nlayer with a softmax output, to produce the output probabilities\nover the vocabulary. Differently from the generic transformer\nmodel, the T5 model [25] uses a simpliﬁed form of position\nembeddings, where each embedding is a scalar that is added\nto the corresponding logit used for computing the attention\nweights. As pointed out by the authors, for efﬁciency they also\nshare the position embedding parameters across all layers.\nThe T5, in particular, and a transformer model, in general,\noffer two main advantages over other state-of-the-art models:\n(i) it is more efﬁcient than RNNs since it allows to compute the\noutput layers in parallel, and (ii) it is able to detect hidden and\nlong-ranged dependencies among tokens, without assuming\nthat nearest tokens are more related than distant ones. This last\nproperty is particularly relevant in code-related tasks since a\nvariable declaration may be distant from its usage.\nFive different versions of T5 have been proposed [25]:\nsmall, base, large, 3 Billion , and 11 Billion . These variants\ndiffer in terms of complexity, with the smaller model (T5 small)\nhaving 60M parameters against the 11B of the largest one\n(T511B). As acknowledged by the authors [25], even if the\naccuracy of the most complex variants are higher than the\nless complex models, the training complexity increases with\nthe number of parameters. Considering the available compu-\ntational resources, in our work we decided to use the simplest\nT5small model. We expect the results achieved in our study to\nbe a lower bound for the performance of a T5-based model.\nNevertheless—as reported in Section V—the T5 small model is\nstill able to outperform state-of-the-art approaches.\nThe T5 small architecture is characterized by six blocks for\nencoders and decoders. The feed-forward networks in each\nblock consist of a dense layer with an output dimensionality\n(dff ) of 2,048. The key and value matrices of all attention\nmechanisms have an inner dimensionality ( dkv) of 64, and\nall attention mechanisms have eight heads. All the other sub-\nlayers and embeddings have a dimensionality ( dmodel) of 512.\nB. Pre-training of T5\nIn the pre-training phase we use a self-supervised task\nsimilar to the one used by Raffel et al. [25], consisting of\nmasking tokens in natural language sentences and asking\nthe model to guess the masked tokens. Differently, we did\nnot perform the pre-training by only using natural language\nsentences, since all the tasks we target involve source code.\nThus, we use a dataset composed of both (technical) natural\nlanguage (i.e., code comments) and source code. To obtain the\ndataset for the pre-training we start from the CodeSearchNet\ndataset [58], which provides 6M functions from open-source\ncode. We only focus on the ∼1.5M methods written in Java,\nsince the four tasks we aim at supporting are all related to\nJava code. Then, since for three of the four tasks we support\n(i.e., automatic bug-ﬁxing [1], generation of assert statements\n[16], and injection of code mutants [15]) the authors of the\noriginal papers used an abstracted version of source code (see\nSection II), we used the src2abs tool by Tufano [1] to create\nan abstracted version of each mined Java method.\n2.67M \ninstances\nCodeSearchNet\nbuggy code\nﬁxed code\npublic MyList checkList(MyList \nl){\n   if(l.size() < 0){\n      populateList(l);\n   }\n   return l;\n}\npublic MyList checkList(MyList \nl){\n   if(l.size() < 1){\n      populateList(l);\n   }\n   return l;\n}\nbuggy code\nﬁxed code\npublic MyList checkList(MyList \nl){\n   if(l.size() < 0){\n      populateList(l);\n   }\n   return l;\n}\npublic MyList checkList(MyList \nl){\n   if(l.size() < 1){\n      populateList(l);\n   }\n   return l;\n}\nBFsmall\n[1]\nsrc2abs\npre-training dataset \npreparation\npre-training dataset \npreparation\nBFmedium\n[1]\nAGraw\n[16]\nAGabt\n[16]\nMGident\n[15]\nCS\n[11]\n1 2\nRaw code\nAbstracted\ncode\nComments\npre-training \nT5 small\n3 ﬁne-tuning datasets \npreparation\n4\n2.42M \ninstances\nbuggy code\nﬁxed code\npublic MyList checkList(MyList \nl){\n   if(l.size() < 0){\n      populateList(l);\n   }\n   return l;\n}\npublic MyList checkList(MyList \nl){\n   if(l.size() < 1){\n      populateList(l);\n   }\n   return l;\n}\nbuggy code\nﬁxed code\npublic MyList checkList(MyList \nl){\n   if(l.size() < 0){\n      populateList(l);\n   }\n   return l;\n}\npublic MyList checkList(MyList \nl){\n   if(l.size() < 1){\n      populateList(l);\n   }\n   return l;\n}\nText-to-Text \ntask-speciﬁc \npairs\n5 ﬁne-tuning on \ntasks-mixture\nT5 pre-trained network\n6\n T5 ﬁne-tuned \non tasks-mixture\nFig. 2: Overview of the approach used to pre-train and ﬁne-tune the T5 model.\nNote that, since the tool was run on Java methods in isola-\ntion (i.e., without providing it the whole code of the projects\nthey belong to), src2abs raised a parsing error in ∼600k of\nthe ∼1.5M methods (due e.g., to missing references), leaving\nus with ∼900k abstracted methods. We still consider such a\ndataset as sufﬁcient for the pre-training.\nThe CodeSearchNet dataset does also provide, for a subset\nof the considered Java source code methods, the ﬁrst sentence\nin their Javadoc. We extracted such a documentation using the\ndocstring tokens ﬁeld in CodeSearchNet, obtaining it for\n499,618 of the considered methods. We added these sentences\nto the pre-training dataset. This whole process resulted in\na total of 2,984,627 pre-training instances, including raw\nsource code methods, abstracted methods, and code comment\nsentences. Finally, in the obtained dataset there could be du-\nplicates between (i) different raw methods that become equal\nonce abstracted, and (ii) comments re-used across different\nmethods. Thus, we remove duplicates, obtaining the ﬁnal set\nof 2,672,450 instances reported in Table I. This is the dataset\nwe use for pre-training the T5 model, using the BERT-style\nobjective function Raffel et al. used in their ﬁnal experiments\nand consisting of randomly masking 15% of tokens (i.e., words\nin comments and code tokens in the raw and abstracted code).\nData sources Instances\nSource code 1,569,773\nAbstracted source code 766,129\nTechnical natural language 336,548\nTotal 2,672,450\nTABLE I: Datasets used for the pre-training of T5.\nFinally, since we pre-train the model on a software-speciﬁc\ndataset, we needed to create a new vocabulary to accommodate\nthe tokens in our dataset. For this reason, we created a new\nSentencePiece model [59] ( i.e., a tokenizer for neural text\nprocessing) by using the entire pre-training dataset.\nC. Fine-tuning of T5\nWe use a slightly modiﬁed version of the multi-task learning\napproach used by Raffel et al. [25]: we ﬁne-tune the model on\na mixture of tasks instead of performing ﬁne-tuning for each\nsingle task.\nTask Dataset Evaluation-set Training-set Test-set\nBug Fixing BFsmall [1] 5,835 46,680 5,835\nBFmedium[1] 6,546 52,364 6,545\nMutant Generation MGident[15] 11,560 92,476 11,559\nAssert Generation AGabs [16] 15,809 126,477 15,810\nAGraw [16] 18,816 150,523 18,815\nCode SummarizationCS [11] 104,272 1,953,940 90,908\nTotal 162,838 2,422,460 149,472\nTABLE II: Task-speciﬁc datasets used for ﬁne-tuning T5.\nWe do this because of the relatively small size of the\nspecialized datasets available. Table II reports summary char-\nacteristics of the datasets we use for each task. Also, for each\ntask we have to provide a consistent framing of the input\nthat allows the model to recognize the tasks that should be\nperformed given an input sequence of tokens. We use a special\ntoken sequence indicating the task at hand ( e.g., “generate\nsmall patch” for BFsmall , followed by the token “:” and by\nthe input required by the task.\n1) Datasets Used for Fine-Tuning: In the following, we\ndescribe the details of the datasets we use for ﬁne-tuning the\nmodel for the four targeted tasks.\nAutomatic Bug Fixing (BF). We use the dataset by Tufano\net al. [1] composed by triplets BFm = ⟨mb, mf , M⟩, where\nmb and mf are the abstracted version of the buggy and ﬁxed\nversion of Java method, respectively, and M represents the\nmapping between the abstracted tokens and the raw code\ntokens (e.g., VAR 1 →webServerPort), which allows to track\nback the output of the model to source code. The triplets refer\nto methods with at most 100 tokens and they are split into two\nsub-datasets: (i) the small version, containing methods with up\nto 50 tokens, and a medium version, with methods with at most\n100 tokens. We train the model to predict the ﬁxed versions,\nmf , given the buggy versions, mb. Given the presence of two\ndatasets, we divide the BF task in two sub-tasks, BFsmall and\nBFmedium, depending on the size of the method [1].\nInjection of Code Mutants (MG) . For the MG task we\nexploited one of the two datasets provided by Tufano et al.\n[5]: MGident and MGident−lit . In both datasets each instance\nis represented by a triple ⟨mf , mb, M⟩, where, similarly to the\nBF datasets, mb and mf are the buggy and ﬁxed version of the\nsnippet, respectively, and M represents the mapping between\nthe abstracted tokens and the code tokens.\nThe ﬁrst dataset ( MGident ) represents the most general\n(and challenging) case, in which the mutated version, mb,\ncan also contain new tokens ( i.e., identiﬁers, types, or method\nnames) not contained in the version provided as input ( mf ).\nMGident−lit , instead, only contains samples in which the mu-\ntated version contains a subset of the tokens in the non-mutated\ncode. In other words, MGident−lit represents a simpliﬁed\nversion of the task. For this reason, we decided to focus on the\nmost general scenario and we only use the MGident dataset.\nGeneration of Assertions in Test Methods (AG) . For the\nAG task we used the dataset provided by Watson et al. [16]\ncontaining triplets ⟨T, TMn, A⟩, where T is a given test case,\nTMn is the focal method tested by T, i.e., the last method\ncalled in T before the assert [60], and A is the assertion\nthat must be generated (output). For such a task, we use two\nversions of the dataset: AGraw, which contains the raw source\ncode for the input ( T +TMn) and the output ( A), and AGabs,\nwhich contains the abstracted version of input and output, i.e.,\nsrc2abs(T + TMn) and src2abs(A), respectively. These are\nthe same datasets used in the original paper.\nCode Summarization (CS) . For code summarization, we\nexploited the dataset provided by Haque et al. [11] containing\n2,149,120 instances, in which each instance is represented by a\ntuple ⟨S, AS, CS, D⟩, where S represents the raw source code\nof the method, AS is its AST representation, CS is the code\nof other methods in the same ﬁle, and D is the summary of\nthe method, i.e., the textual description that the model should\ngenerate [11]. For this speciﬁc task, we consider a variation\nof the original dataset to make it more coherent with the\nperformed pre-training. In particular, since in the pre-training\nwe did not use any AST representation of code, we decided to\nexperiment with the T5 model in a more challenging scenario\nin which only the raw source code to summarize ( i.e., S) is\navailable to the model. Therefore, the instances of our dataset\nare represented by tuples ⟨S, D⟩: We train our model to predict\nD given only S.\n2) Data Balancing: The datasets we use for ﬁne-tuning\nhave different sizes, with the one for code summarization\ndominating the others. This could result in an unbalanced\neffectiveness of the model on the different tasks. In our case,\nthe model could become very effective in summarizing code\nand less in the other three tasks. However, as pointed out by\nArivazhagan et al. [61], there is no free lunch in choosing the\nbalancing strategy when training a multi-task model, with each\nstrategy having its pros and cons ( e.g., oversampling of less\nrepresented datasets negatively impacts the performance of the\nmost representative task). For this reason, while ﬁne-tuning,\nwe decided not to perform any particular adaptation of our\ntraining set, following the true data distribution when creating\neach batch: We sample instances from the tasks in such a way\nthat each batch during the training has a proportional number\nof samples accordingly to the size of the training dataset.\nD. Decoding Strategy\nGiven the values of the output layer, different decoding\nstrategies can be used to generate the output token streams.\nT5 allows to use both greedy decoding and Beam-search.\nWhen generating an output sequence, the greedy decoding\nselects, at each time step t, the symbol having the highest\nprobability. The main limitation of greedy decoding is that\nit only allows the model to generate one possible output\nsequence (e.g., one possible bug ﬁx) for a given input sequence\n(e.g., the buggy method).\nBeam-search is an alternative decoding strategy previously\nused in many DL applications [62]–[65]. Unlike greedy de-\ncoding, which keeps only a single hypothesis during decoding,\nbeam-search of order K, with K >1, allows the decoder to\nkeep K hypotheses in parallel: At each time step t, beam-\nsearch picks the K hypotheses ( i.e., sequences of tokens up\nto t) with the highest probability, allowing the model to output\nK possible output sequences.\nWe used Beam-search to provide several output sequences\ngiven a single input, and report results with different K\nvalues. It is worth noting that having a large K increases the\nprobability that one of the output sequences is correct, but, on\nthe other hand, it also increases the cost of manually analyzing\nthe output for a user ( i.e., a developer, in our context).\nE. Hyperparameter Tuning\nFor the pre-training phase, we use the default parameters\ndeﬁned for the T5 model [25]. Such a phase, indeed, is task-\nagnostic, and hyperparameter tuning would provide limited\nbeneﬁts. Instead, we tried different learning rate strategies\nfor the ﬁne-tuning phase. Especially, we tested four different\nlearning rates: (i) Constant Learning Rate (C-LR): the learning\nrate is ﬁxed during the whole training (we use LR = 0.001,\ni.e., the value used in the original paper [25]); (ii) Inverse\nSquare Root Learning Rate (ISR-LR): the learning rate decays\nas the inverse square root of the training step (the same\nused for pre-training by Raffel et al.); (iii) Slanted Triangular\nLearning Rate [66] (ST-LR): the learning rate ﬁrst linearly\nincreases and then linearly decays to the starting learning rate;\n(iv) Polynomial Decay Learning Rate (PD-LR): the learning\nrate decays polynomially from an initial value to an ending\nvalue in the given decay steps.\nTable III reports the speciﬁc parameters we use for each\nscheduling strategy: the values are the default ones reported\nin the papers that introduced them.\nLearning Rate Type Parameters\nConstant LR = 0.001\nInverse Square Root LRstarting = 0.01\nWarmup = 10, 000\nSlanted Triangular LRstarting = 0.001\nLRmax = 0.01\nRatio = 32\nCut = 0.1\nPolynomial Decay LRstarting = 0.01\nLRend = 1e−06\nPower = 0.5\nTABLE III: Learning-rates tested for hyperparameter tuning.\nDataset Metric C-LR ST-LR ISQ-LR PD-LR\nBFsmall [1] Accuracy@1 6.9% 13.2% 11.0% 0.27%\nBFmedium [1] Accuracy@1 2.9% 5.5% 3.3% 0.0%\nMGident [15] BLEU-A 75.6% 78.2% 77.7% 12.0%\nAGabs [16] Accuracy@1 33.7% 39.7% 39.8% 2.0%\nAGraw [16] Accuracy@1 48.9% 57.6% 56.7% 2.1%\nCS [11] BLEU-A 23.3% 23.6% 24.3% 3.4%\n# Best Results 0 4 2 0\nTABLE IV: Hyperparameter tuning results.\nWe pre-train the model for a total of 100k steps in the four\nconﬁgurations on the whole pre-training set and we test it on\nthe evaluation sets of the datasets provided by the original\npapers we compare with.\nWe compute the following metrics: for BF and AG, we\ncompute the percentage of perfect predictions achieved with\nthe greedy decoding strategy (Accuracy@1); for MG, we\ncompute the BLEU score [50]; for CS, we compute BLEU-\nA, the geometric average of the BLEU- {1,2,3,4}scores [50].\nBasically, for each task we adopt one of the evaluation\nmetrics used in the original paper (details about these metrics\nare provided in Section IV-A). We report in Table IV the\nachieved results (in bold the learning rate obtaining the best\nperformance for each metric/dataset). As it can be noticed, the\nSlanted Triangular Learning Rate (ST-LR) allows to achieve\nthe best performance in most of the cases. For this reason, we\ndecided to use this particular learning rate in our model.\nSeveral other hyperparameters could have been tuned. Given\nthe high computational cost to train the model ( ∼343 hours\non a colab [67] instance with 8 tpu cores and 35.5GB of\nRAM), we did not manage to perform a comprehensive\nhyperparameter tuning. The high dimensionality of the model,\nindeed, makes hyperparameter tuning not very cost-effective:\nwe preferred to use the computational power available to\nincrease the number of steps for training the model.\nIV. S TUDY DESIGN\nThe goal of our study is to understand whether multi-task\nlearning, in general, and a T5-based model, in particular,\nis suitable for automating code-related tasks. The context is\nrepresented by the datasets introduced in Section II, i.e., the\nones by Tufano et al. for bug ﬁxing [1] and injection of\nmutants [15], by Watson et al. for assert statement generation\n[16], and by Haque et al. for code summarization [11].\nOur study is steered by the following research question: Is\nthe T5 model suitable for code-related tasks such as automatic\nbug ﬁxing, injection of mutants, assert statement generation\nand code summarization?\nA. Experimental Procedure\nWe use the model we trained and tuned as we speciﬁed\nin Section III and we run it on the test sets provided in\nthe previously described datasets. Our baselines are the state-\nof-the-art models described in Section II. For each task and\ndataset, we compare the results achieved by our model with\nthe results reported in the original papers.\nTask Baseline Accuracy@K BLEU-n ROUGE LCS\nBF [1] {1, 5, 10, 25, 50} - -\nMG [15] {1} { A} -\nAG [16] {1, 5, 10, 25, 50} - -\nCS [11] - {1, 2, 3, 4, A} {P, R, F}\nTABLE V: Baselines and evaluation metrics for the tasks.\nWe use different metrics for the different tasks, depending\non the metrics reported in the papers that introduced our\nbaselines. Table V reports the baselines and metrics used to\nevaluate the results for each task, that we deﬁne below.\nAccuracy@K measures the percentage of cases ( i.e., in-\nstances in the test set) in which the sequence predicted by\nthe model equals the oracle sequence ( i.e., perfect prediction).\nSince we use beam-search, we report the results for different\nK values (i.e., 1, 5, 10, 25, and 50), as done in [1] (BF) and\n[16] (AG). Tufano et al. [5] do not report results for K >1 for\nthe MG task. Thus, we only compare the results with K = 1.\nBLEU score (Bilingual Evaluation Understudy) [50] mea-\nsures how similar the candidate (predicted) and reference (ora-\ncle) texts are. Given a size n, the candidate and reference texts\nare broken into n-grams and the algorithm determines how\nmany n-grams of the candidate text appear in the reference\ntext. The BLEU score ranges between 0 (the sequences are\ncompletely different) and 1 (the sequences are identical). We\nuse different BLEU- n scores, depending on the ones used in\nthe reference paper of the baseline. For the CS task, we report\nBLEU-{1, 2, 3, 4 }and their geometric mean ( i.e., BLEU-A);\nfor the MG task we only report BLEU-A.\nROUGE (Recall-Oriented Understudy for Gisting Eval-\nuation) is a set of metrics for evaluating both automatic\nsummarization of texts and machine translation techniques\nin general [68]. ROUGE metrics compare an automatically\ngenerated summary or translation with a set of reference\nsummaries (typically, human-produced). We use the ROUGE\nLCS metrics based on the Longest Common Subsequence for\nthe CS task [11]. Given two token sequences, X and Y , and\ntheir respective length, m and n, it is possible to compute three\nROUGE LCS metrics: R (recall), computed as LCS(X,Y )\nm ,\nP (precision), computed as LCS(X,Y )\nn , and F (F-measure),\ncomputed as the harmonic mean of P and R.\nBesides such effectiveness metrics, we also perform an\nadditional analysis: we compute the inference time , i.e., the\ntime needed to run the model on a given input. We run such\nan experiment on a laptop equipped with a 2.3GHz 8-core\n9th-generation Intel Core i9 and 16 GB of RAM. We do this\nfor different beam search sizes, with K ∈{1, 5, 10, 25, 50}.\nFor each K, we report the average inference time on all the\ninstances of each task. This allows understanding the efﬁciency\nof the model and to what extent it can be used in practice.\nFinally, for each task, we also compute the complementarity\nbetween T5 and the baseline approach. For each dataset d we\nconsider and the related baseline approach BLd, we ﬁrst deﬁne\nthe sets of perfect predictions obtained by the two approaches\nPPT5d and PPBLd with a ﬁxed beam size K = 1.\nThen, we compute three metrics:\nSharedd = |PPT5d ∩PPBLd |\n|PPT5d ∪PPBLd |\nOnlyT5d = |PPT5d \\PPBLd |\n|PPT5d ∪PPBLd |\nOnlyBLd = |PPBLd \\PPT5d |\n|PPT5d ∪PPBLd |\nSharedd measures the percentage of perfect predictions\nshared between the two compared approaches, while OnlyT5d\nand OnlyBLd measure the percentage of cases in which the\nperfect prediction is only achieved by T5 or the baseline,\nrespectively, on the dataset d.\nV. R ESULTS DISCUSSION\nWe report a summary of the results achieved by T5 (in\nred) and by the respective baselines (in orange) for the four\ntasks we consider ( i.e., BF, MG, AG, and CS) in Fig. 3.\nWe also show the inference times for all the tasks and the\noverlap metrics between T5 and the experimented baselines in\nTable VI and Table VII, respectively. We discuss the results\ntask by task below.\nTABLE VI: Inference time with different beam size values.\nK BFsmall BFmedium MGident AGabs AGraw CS\n1 0.41 1.84 0.31 0.35 0.36 0.12\n5 0.62 1.13 0.54 0.79 0.66 0.17\n10 0.72 1.55 0.62 1.17 1.20 0.24\n25 1.30 3.35 1.13 2.45 2.66 0.40\n50 2.16 5.31 2.04 4.82 4.96 0.74\nA. Automatic Bug Fixing (BF)\nWhen using T5 for automatically ﬁxing bugs, the accuracy\nachieved using a greedy decoding strategy ( K = 1) is very\nsimilar to the one achieved by the baseline on both the datasets\nwe consider, i.e., BFsmall and BFmedium. While on the ﬁrst\none there is a 1% improvement, on the other the results are\nexactly the same. However, when increasing the beam size,\nthe difference becomes larger: on BFsmall the improvement\nranges between 8-10%, while on BFmedium it is lower, and\nit ranges between 4-9%. In general, it can be noticed that the\nimprovement margin is constant.\nThe time needed to generate a ﬁx depends on the dataset,\ni.e., on the number of tokens of the input. If we use the\nBFsmall dataset, the average inference time ranges between\n0.41s (K = 1) and 2.16s ( K = 50), while it is larger on the\nBFmedium dataset (1.84s for K = 1 and 5.31s for K = 50).\nTABLE VII: Overlap metrics for correct predictions generated\nby the T5 model and the baselines.\nDataset (d) Sharedd OnlyT5d OnlyBLd\nBFsmall 37.67% 36.52% 25.81%\nBFmedium 28.78% 36.06% 35.16%\nMGident 41.03% 46.65% 12.32%\nAGabs 39.78% 36.19% 24.03%\nAGraw 11.68% 84.30% 4.02%\nCS 4.97% 93.46% 1.57%\nThere is a considerable overlap between the perfect predic-\ntions done by the two approaches (see Table VII): ∼38% of\nperfect predictions on BFsmall and ∼29% on BFmedium are\nshared by the two techniques.\nThe remainder are perfect predictions only with T5 ( ∼36%\non BFsmall and ∼36% on BFmedium) or only with the\nbaseline (∼26% on BFsmall and ∼35% on BFmedium). This\nindicates that the two approaches are complementary for the\nBF task suggesting that, even if T5 was not able to ﬁx some\nbugs, it is still possible to automatically ﬁx (a subset of) such\nbugs with a specialized ML-based approach. This recalls the\nneed to further enrich the architecture of a transfer learning\nmethod with the goal of further improving its ability to exploit\nthe knowledge acquired on speciﬁc tasks.\nB. Injection of Code Mutants (MG)\nLooking at Fig. 3, we can observe that using T5 to generate\nmutants allows to obtain much more accurate results than\nthe baseline, with the Accuracy@1 improving by 11%, with\n1,240 additional perfect predictions (+62% as compared to the\nbaseline). The average BLEU score improves by ∼0.01 on top\nof the very good results already obtained by the baseline ( i.e.,\n0.77). Minor improvements in BLEU score can still indicate\nmajor advances in the quality of the generated solutions [69].\nAs for the inference time (Table VI), we observed similar\nresults compared to the BF task on the BFsmall dataset: with\nK = 1, the average inference time is 0.31s, while for K = 50\nit is 2.04s. We do not report perfect predictions at K = 50\nsince those were not reported in the original paper [15].\nSimilarly to BF, also for MG the percentage of shared\nperfect predictions (Table VII) is quite high ( ∼41%) with,\nhowever, T5 being the only one generating ∼46% of perfect\npredictions as compared to the ∼12% of the baseline approach.\nC. Generation of Assertions in Test Methods (AG)\nT5 achieves very similar results compared to the baseline\non the AGabs (see Fig. 3): when abstracting the tokens, both\napproaches achieve very similar levels of accuracy, and such\nvalues are reasonably high with the increase of K (e.g., they\nboth achieve 65% accuracy with K = 50). However, when\nusing the more challenging non-abstracted dataset AGraw,\nT5 allows to achieve much better results: it achieves a 29%\nhigher accuracy with K = 1 , while for larger K values\nthe gap in performance ranges between 35-38%. The most\ninteresting result, however, is that T5 achieves similar results\nboth with and without abstraction, with the the Accuracy@1\nbeing higher when considering AGraw then when considering\nAGabs. The fact that T5 is capable of handling raw source\ncode makes its usage more straightforward compared to the\nbaseline: it does not need pre- and post-processing steps for\nsuch a task.\nAssert generation is very fast for low values of K (0.36s for\nboth the datasets with K = 1), while it gets much slower for\nhigher values of K, at a higher rate compared to other tasks\n(4.82s for AGabs and 4.96s for AGraw with K = 50).\nAutomatic Bug Fixing (BF)\nGeneration of Asserts in Tests (AG) Code Summarization (CS)\nInjection of Code Mutants (MG)\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n5 10 15 20 25 30 35 40 50\nbeam width\n451\nAGabs \n(Abstracted code)\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n5 10 15 20 25 30 35 40 50\nbeam width\n451\nAGraw \n(Raw code)\naccuracy\naccuracy\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n9%\n5 10 15 20 25 30 35 40 50\nbeam width\n451\nBFsmall \n(Methods up to 50 tokens)\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\n5 10 15 20 25 30 35 40 50\nbeam width\n451\n27%\n36%\n45%\n50%\n13%\n18%\n24%\n29%\nBFmedium \n(Methods up to 100 tokens)\naccuracy\naccuracy\nBaseline [1] T5 [25]\n23%\n27%31%\n50%\n65%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nBLEU-2\nBLEU Variants\nBLEU-1\nBLEU score\nBaseline [11] T5 [25]\nBLEU-3 BLEU-4 BLEU-A\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\n90%\n100%\nR\nROUGE LCS\nP\nROUGE score\nF\nBaseline [15] T5 [25]\nBaseline [1]: 17% (1,991)\nT5 [25]: 28% (3,231)\nAccuracy@1\n10%\n36%\n44%\n55%\n60%\n17%\n24%\n32%\n38%\n55% 62%34%\n53%\n63%\n58%\n66%\n47%\n57%\n61% 63% 65%\nBLEU-A\nBaseline [1]: 0.77\nT5 [25]: 0.78\n3%\n18%\n26%\nBaseline [16] T5 [25]\nFig. 3: Performance of the T5 model against the experimented baselines.\nIn terms of overlap, we found a trend similar to BF on\nAGabs: we have ∼40% of perfect predictions shared between\nthe two approaches, while the remainder instances are dis-\ntributed between the ones only predicted by T5 ( ∼36%) and\nthe ones only predicted by the baseline ( ∼24%).\nThere is, instead, a small overlap on the AGraw dataset:\nonly ∼12% of the instances are perfectly predicted by both\nthe approaches, with ∼84% of them correctly predicted only\nby T5.\nD. Code Summarization (CS)\nOn this task, T5 achieves a substantial increase in BLEU\nscore as compared to the baseline. When considering the\naverage BLEU (BLEU-A), the improvement is of ∼5%. On\nthe other hand, it can be noticed that the ROUGE-LCS scores\nachieved when using T5 are lower than the ones achieved\nby the baseline ( ∼6% lower on the F-measure score). Thus,\nlooking at these metrics, there is no clear winner, but T5 seems\nto be at least comparable to the baseline. To have something\neasier to interpret, we compared the two approaches in terms\nof the number of perfect predictions they generate, despite the\nfact that such a metric was not used in the original paper [11].\nThis means counting the comments generated by a technique\nthat are exactly equal to the ones manually written by humans.\nT5 managed to generate 11.4% of perfect predictions (10,401\ninstances) against the 3.4% (3,048) of the baseline technique\n(over 3 ×better).\nCode summarization is the fastest task to complete for T5:\nit takes only 0.12s for K = 1 and 0.74s for K = 50.\nAs expected from previous results, the majority of the\nperfect predictions for this task can be done only using T5\n(∼93%). A limited percentage of perfect predictions is shared\n(∼5%), and a minority of instances can be only predicted\nthrough the baseline ( ∼2%).\nE. Qualitative Examples\nWe show in Fig. 4 four examples of perfect predictions by\nT5; for the sake of space limitations, for BF and MG, we\nonly report the parts of code that T5 modiﬁed. In the ﬁrst one\n(BF), the developers used the != operator for comparing two\nobjects instead of calling the equals method. T5 was able to\nﬁx the bug by (i) adding a call to equals and, less obvious,\n(ii) adding the ! operator before the method call. In the second\nexample (MG), T5 generates a mutant by replacing the correct\nstring ( string 1) with a different one ( string 2). In the\nthird example (AG), the test checks if a call to setPosition\non the variable instant1 does not change its value ( i.e., it\nshould be equal to result). Such an assertion is not trivial\nto generate since result is used in the assertion even if p is\ncloser to the assert placeholder. Finally, in the last example\n(CS), T5 detects that (i) the method applies a given procedure\nto the pairs, (ii) the pairs belong to a receiver, and (iii) this\nhappens only if there is a receiver. This shows how T5 is able\nto generate a summary for a method that even a developer\ncould struggle to understand.\nAutomatic Bug Fixing (BF)\npublic void method_1 ( int var_1 , int var_2 , Intent data ) {\nsuper . method_1 ( var_1 , var_2 , data );\nif (( data != null ) && (( data . method_2 ( var_3 )) != string_1 )) {\nvar_4 . setText ( data . method_2 ( var_3 ));\n}\n}\n> if (( data != null ) && ( !( data . method_2 ( var_3 )).equals( string_1 ))) {\nInjection of Code Mutants (MG)\npublic void method_1 ( result ) {\nif ( result == null ) {\nvar_1 . setEnabled ( false );\nvar_2 . setEnabled ( false );\nreturn ;\n}\nvar_3 . setText ((string 1 + result ));\n}\n> var_3 . setText ((string 2 + result ));\nGeneration of Assert Statements in Test Methods (AG)\n// test method\nvoid testSetPosition () {\nPosition result = instant1 . getPosition ();\nPosition p = org . geotools . temporal . Object . defaultPosition ( new Date ());\n(( org . geotools . temporal . Object . DefaultInstant )( instant1 )). setPosition (p);\n<assertplaceholder>;\n}\n// focal method\npublic Position getPosition () { return this ; }\n> assertFalse(instant1.getPosition().equals(result))\nCode Summarization (CS)\npublic boolean forEachPair ( final IntObjectProcedure procedure ) {\nfor ( int i = table . length ; i-- > 0;) {\nif ( state [i] == FULL )\nif (! procedure . apply ( table [i], values [i]))\nreturn false ;\n}\nreturn true ;\n}\n> \"applies a procedure to each key value pair of the receiver if any\"\nFig. 4: Examples of perfect predictions done by T5.\nF . Answer to our Research Question\nOur study showcases the potential of T5 for code-related\ntasks. The T5 model achieved better performance as compared\nto all baselines we experimented with. However, it is impor-\ntant to highlight that there are many factors that may have\ncontributed to such a result. Indeed, the high effectiveness\nwe obtained on all the tasks we experimented with might\nnot only be related to the T5 architecture ( e.g., the fact that\nthe T5 supports transfer learning with knowledge acquired\non a task that can be reused on other tasks) but to other\ndifferences between the study presented in this paper and\nthe experiments performed in the original work. While the\ndatasets used for testing the techniques are exactly the same,\ntwo aspects must be considered. First, the type of the model\nwe use ( i.e., the transformer model): using such a model in a\nsingle-task setting may still allow to achieve an improvement\nover the respective baselines Second, as previously explained,\nthe pre-training phase may provide “knowledge” to the model\nthat is not available in the training sets used for the ﬁne-tuning\n(and, thus, not used by the competitive techniques).\nThe results in terms of inference time show that T5 is able\nto complete all the tasks very quickly: it always takes less than\n6 seconds even to generate 50 alternative solutions.\nNote that the inference times we reported are based on the\nusage of a consumer-level device and by only using CPUs:\nwhen using GPUs (Nvidia Tesla P100 provided by Google\nColab), the time needed for each task ﬂattens to at most\n∼0.5 seconds for K = 50, i.e., the task variability previously\nreported disappears. Finally, the overlap analysis indicates that\nsome instances of the considered tasks were not resolved\ncorrectly by T5 but were resolved by the baselines. This means\nthat such instances can be still resolved automatically by a\nML-based approach. Such a consideration suggests that there\nis still room for improving the accuracy of T5.\nVI. T HREATS TO VALIDITY\nConstruct validity. For both the pre-training and the ﬁne-\ntuning of our model we re-used available datasets, just per-\nforming some additional cleaning ( e.g., removal of duplicates\nafter abstraction in the dataset used for the pre-training). Even\nthough we remove duplicates from the pre-training dataset\nand double-check all the datasets used for the ﬁne-tuning, it\nis possible that instances in the pre-training dataset appear\nin some of the test datasets we reused. For example, a code\ncomment included among the pre-training instances we used\ncould have a duplicate, by chance, in the test set of the CS task,\nthus helping the T5 model in the prediction. While removing\ninstances from the test sets was not an option since this would\nnot allow a fair comparison between the T5 results and the\nones reported in the original papers, we decided to investigate\nsuch overlap, to have an idea of the extent to which it could\nhave inﬂuenced our ﬁndings. We found 0 duplicates between\nthe pre-training dataset and the test sets of: BFsmall , AGabs,\nAGraw; 1 in MGident ; 2 in BFmedium; and 147 (out of\n90,908) in the CS test set. Thus, the inﬂuence of duplicates\non the reported results should be marginal.\nInternal validity. An important factor that inﬂuences DL\nperformance is hyperparameters tuning. For the pre-training\nphase, we used the default T5 parameters selected in the orig-\ninal paper [25] since we expect little margin of improvement\nfor such a task-agnostic phase. For the ﬁne-tuning, due to\nfeasibility reasons, we did not change the model architecture\n(e.g., number of layers) but we experiment with different\nlearning rates. We are aware that a more extensive calibration\nwould likely produce better results.\nExternal validity. We experimented the T5 model on four\ntasks using six datasets. The main generalizability issue is\nrelated to the focus on Java code. However, excluding the\nabstraction component, our approach is language agnostic.\nVII. C ONCLUSION\nWe investigated the usage of a T5 model to support four\ncode-related tasks: automatic bug-ﬁxing, generation of assert\nstatements in test methods , code summarization, and injection\nof code mutants. The achieved results show that the T5 model\ncan be successfully used for these tasks, with performance\nsuperior to the four baselines. However, as explained in\nSection V-F, such a ﬁnding deserves additional investigations\nTO better understand what makes T5 performing better.\nAlso, Raffel et al. [25], who originally introduced T5,\nshowed that larger T5 models are able to achieve much better\nresults as compared to the small T5 model we used in this\nwork. From this perspective, the results reported in this paper\nshould be considered as a lower bound of the T5 capabilities.\nCode and data used in this paper are publicly available [26].\nACKNOWLEDGMENT\nThis project has received funding from the European Re-\nsearch Council (ERC) under the European Union’s Horizon\n2020 research and innovation programme (grant agreement\nNo. 851720). W&M team was supported in part by the\nNSF CCF-1955853 and CCF-2007246 grants. Any opinions,\nﬁndings, and conclusions expressed herein are the authors’ and\ndo not necessarily reﬂect those of the sponsors.\nREFERENCES\n[1] M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, and\nD. Poshyvanyk, “An empirical study on learning bug-ﬁxing patches\nin the wild via neural machine translation,” ACM Trans. Softw. Eng.\nMethodol., vol. 28, no. 4, pp. 19:1–19:29, 2019.\n[2] Z. Chen, S. Kommrusch, M. Tufano, L. Pouchet, D. Poshyvanyk,\nand M. Monperrus, “Sequencer: Sequence-to-sequence learning for\nend-to-end program repair,” CoRR, 2019. [Online]. Available: http:\n//arxiv.org/abs/1901.01808\n[3] A. Mesbah, A. Rice, E. Johnston, N. Glorioso, and E. Aftandilian,\n“Deepdelta: Learning to repair compilation errors,” in Proceedings of\nthe 2019 27th ACM Joint Meeting on European Software Engineering\nConference and Symposium on the Foundations of Software Engineering,\nser. ESEC/FSE 2019, 2019, pp. 925–936.\n[4] H. Hata, E. Shihab, and G. Neubig, “Learning to generate corrective\npatches using neural machine translation,” CoRR, vol. abs/1812.07170,\n2018. [Online]. Available: http://arxiv.org/abs/1812.07170\n[5] M. Tufano, J. Pantiuchina, C. Watson, G. Bavota, and D. Poshyvanyk,\n“On learning via neural machine translation,” in Proceedings of the\n41st International Conference on Software Engineering, ICSE 2019,\nMontreal, QC, Canada, May 25-31, 2019 , 2019, pp. 25–36.\n[6] A. T. Nguyen, T. T. Nguyen, and T. N. Nguyen, “Migrating code\nwith statistical machine translation,” in Companion Proceedings of\nthe 36th International Conference on Software Engineering , ser. ICSE\nCompanion 2014, 2014, pp. 544–547.\n[7] ——, “Lexical statistical machine translation for language migration,” in\nProceedings of the 2013 9th Joint Meeting on Foundations of Software\nEngineering, ser. ESEC/FSE 2013, 2013, pp. 651–654.\n[8] A. LeClair, S. Jiang, and C. McMillan, “A neural model for generating\nnatural language summaries of program subroutines,” in Proceedings of\nthe 41st International Conference on Software Engineering , ser. ICSE\n’19, 2019, pp. 795–806.\n[9] S. Jiang, A. Armaly, and C. McMillan, “Automatically generating\ncommit messages from diffs using neural machine translation,” in\n2017 32nd IEEE/ACM International Conference on Automated Software\nEngineering (ASE), ser. ASE’17, Oct. 2017, pp. 135–146, iSSN:.\n[10] Z. Liu, X. Xia, A. E. Hassan, D. Lo, Z. Xing, and X. Wang, “Neural-\nmachine-translation-based commit message generation: How far are\nwe?” in Proceedings of the 33rd ACM/IEEE International Conference\non Automated Software Engineering, ser. ASE 2018, 2018, pp. 373–384.\n[11] S. Haque, A. LeClair, L. Wu, and C. McMillan, “Improved automatic\nsummarization of subroutines via attention to ﬁle context,” 2020.\n[12] Y . Oda, H. Fudaba, G. Neubig, H. Hata, S. Sakti, T. Toda, and\nS. Nakamura, “Learning to generate pseudo-code from source code using\nstatistical machine translation,” in Proceedings of the 30th IEEE/ACM\nInternational Conference on Automated Software Engineering , ser. ASE\n’15, 2015, pp. 574–584.\n[13] B. Vasilescu, C. Casalnuovo, and P. Devanbu, “Recovering clear, natural\nidentiﬁers from obfuscated js names,” in Proceedings of the 2017 11th\nJoint Meeting on Foundations of Software Engineering , ser. ESEC/FSE\n2017, 2017, pp. 683–693.\n[14] A. Jaffe, J. Lacomis, E. J. Schwartz, C. L. Goues, and B. Vasilescu,\n“Meaningful variable names for decompiled code: A machine trans-\nlation approach,” in Proceedings of the 26th Conference on Program\nComprehension, ser. ICPC ’18, 2018, pp. 20–30.\n[15] M. Tufano, C. Watson, G. Bavota, M. D. Penta, M. White, and\nD. Poshyvanyk, “Learning how to mutate source code from bug-ﬁxes,”\nin 2019 IEEE International Conference on Software Maintenance and\nEvolution, ICSME 2019, Cleveland, OH, USA, September 29 - October\n4, 2019, 2019, pp. 301–312.\n[16] C. Watson, M. Tufano, K. Moran, G. Bavota, and D. Poshyvanyk, “On\nlearning meaningful assert statements for unit test cases,” in Proceedings\nof the 42nd International Conference on Software Engineering, ICSE\n2020, 2020, p. To Appear.\n[17] R. Karampatsis and C. A. Sutton, “Maybe deep neural networks are\nthe best choice for modeling source code,” CoRR, vol. abs/1903.05734,\n2019. [Online]. Available: http://arxiv.org/abs/1903.05734\n[18] U. Alon, R. Sadaka, O. Levy, and E. Yahav, “Structural language models\nof code,” arXiv, pp. arXiv–1910, 2019.\n[19] S. Kim, J. Zhao, Y . Tian, and S. Chandra, “Code prediction by feeding\ntrees to transformers,” arXiv preprint arXiv:2003.13848 , 2020.\n[20] A. Svyatkovskiy, S. K. Deng, S. Fu, and N. Sundaresan, “Intelli-\ncode compose: Code generation using transformer,” arXiv preprint\narXiv:2005.08025, 2020.\n[21] S. Brody, U. Alon, and E. Yahav, “Neural edit completion,” arXiv\npreprint arXiv:2005.13209, 2020.\n[22] N. Kalchbrenner and P. Blunsom, “Recurrent continuous translation\nmodels,” in Proceedings of the 2013 Conference on Empirical Methods\nin Natural Language Processing . Seattle, Washington, USA: Associa-\ntion for Computational Linguistics, October 2013, pp. 1700–1709.\n[23] I. Sutskever, O. Vinyals, and Q. V . Le, “Sequence to sequence learning\nwith neural networks,” CoRR, vol. abs/1409.3215, 2014.\n[24] K. Cho, B. van Merrienboer, C ¸ . G ¨ulc ¸ehre, F. Bougares, H. Schwenk,\nand Y . Bengio, “Learning phrase representations using RNN encoder-\ndecoder for statistical machine translation,” CoRR, vol. abs/1406.1078,\n2014.\n[25] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY . Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer learning\nwith a uniﬁed text-to-text transformer,” 2019.\n[26] “Replication package https://github.com/antonio-mastropaolo/\nT5-learning-ICSE 2021.”\n[27] C. Le Goues, T. Nguyen, S. Forrest, and W. Weimer, “Genprog: A\ngeneric method for automatic software repair,” IEEE Trans. Software\nEng., vol. 38, no. 1, pp. 54–72, 2012.\n[28] C. L. Goues, M. Dewey-V ogt, S. Forrest, and W. Weimer, “A systematic\nstudy of automated program repair: Fixing 55 out of 105 bugs for $8\neach,” ser. ICSE’12.\n[29] S. Sidiroglou-Douskos, E. Lahtinen, F. Long, and M. Rinard, “Automatic\nerror elimination by horizontal code transfer across multiple applica-\ntions,” SIGPLAN Not., vol. 50, no. 6, pp. 43–54, Jun. 2015.\n[30] D. Pierret and D. Poshyvanyk, “An empirical exploration of regularities\nin open-source software lexicons,” in The 17th IEEE International\nConference on Program Comprehension, ICPC 2009, Vancouver, British\nColumbia, Canada, May 17-19, 2009 , 2009, pp. 228–232.\n[31] M. Gabel and Z. Su, “A study of the uniqueness of source code,” in\nProceedings of the Eighteenth ACM SIGSOFT International Symposium\non Foundations of Software Engineering, ser. FSE ’10. New York, NY ,\nUSA: ACM, 2010, pp. 147–156.\n[32] A. Carzaniga, A. Gorla, A. Mattavelli, N. Perino, and M. Pezz `e,\n“Automatic recovery from runtime failures,” in Proceedings of the\n2013 International Conference on Software Engineering , ser. ICSE ’13.\nPiscataway, NJ, USA: IEEE Press, 2013, pp. 782–791.\n[33] H. A. Nguyen, A. T. Nguyen, T. T. Nguyen, T. N. Nguyen, and H. Rajan,\n“A study of repetitiveness of code changes in software evolution,”\nin Proceedings of the 28th IEEE/ACM International Conference on\nAutomated Software Engineering , ser. ASE’13. Piscataway, NJ, USA:\nIEEE Press, 2013, pp. 180–190.\n[34] M. White, M. Tufano, M. Martinez, M. Monperrus, and D. Poshyvanyk,\n“Sorting and transforming program repair ingredients via deep learning\ncode similarities,” in 2019 IEEE 26th International Conference on\nSoftware Analysis, Evolution and Reengineering (SANER). IEEE, 2019,\np. to appear.\n[35] J. Bader, A. Scott, M. Pradel, and S. Chandra, “Getaﬁx: learning to ﬁx\nbugs automatically,” Proc. ACM Program. Lang. , vol. 3, no. OOPSLA,\npp. 159:1–159:27, 2019.\n[36] M. Martinez, W. Weimer, and M. Monperrus, “Do the ﬁx ingredients\nalready exist? an empirical inquiry into the redundancy assumptions of\nprogram repair approaches,” in Companion Proceedings of the 36th In-\nternational Conference on Software Engineering , ser. ICSE Companion\n2014. New York, NY , USA: ACM, 2014, pp. 492–495.\n[37] E. T. Barr, Y . Brun, P. Devanbu, M. Harman, and F. Sarro, “The\nplastic surgery hypothesis,” in Proceedings of the 22Nd ACM SIGSOFT\nInternational Symposium on Foundations of Software Engineering , ser.\nFSE 2014. New York, NY , USA: ACM, 2014, pp. 306–317.\n[38] D. B. Brown, M. Vaughn, B. Liblit, and T. Reps, “The care and\nfeeding of wild-caught mutants,” in Proceedings of the 2017 11th Joint\nMeeting on Foundations of Software Engineering , ser. ESEC/FSE 2017.\nNew York, NY , USA: ACM, 2017, pp. 511–522. [Online]. Available:\nhttp://doi.acm.org/10.1145/3106237.3106280\n[39] S. Shamshiri, “Automated Unit Test Generation for Evolving Software,”\nin Proceedings of the 2015 10th Joint Meeting on Foundations of\nSoftware Engineering, ser. FSE’15. Bergamo, Italy: ACM, 2015, pp.\n1038–1041.\n[40] G. Fraser and A. Arcuri, “EvoSuite: Automatic Test Suite Generation for\nObject-oriented Software,” in Proceedings of the 19th ACM SIGSOFT\nSymposium and the 13th European Conference on Foundations of\nSoftware Engineering, ser. ESEC/FSE ’11. ACM, 2011, pp. 416–419.\n[41] C. Pacheco and M. D. Ernst, “Randoop: Feedback-directed random\ntesting for java,” in OOPSLA’07, 01 2007, pp. 815–816.\n[42] “Utilizing fast testing to transform java development into an agile, quick\nrelease, low risk process.” [Online]. Available: http://www.agitar.com/\n[43] S. Haiduc, J. Aponte, L. Moreno, and A. Marcus, “On the use of\nautomated text summarization techniques for summarizing source code,”\nin 2010 17th Working Conference on Reverse Engineering , 2010, pp.\n35–44.\n[44] G. Sridhara, L. Pollock, and K. Vijay-Shanker, “Generating parameter\ncomments and integrating with method summaries,” in 2011 IEEE 19th\nInternational Conference on Program Comprehension, 2011, pp. 71–80.\n[45] L. Moreno, J. Aponte, G. Sridhara, A. Marcus, L. Pollock, and K. Vijay-\nShanker, “Automatic generation of natural language summaries for java\nclasses,” in 2013 21st International Conference on Program Compre-\nhension (ICPC), 2013, pp. 23–32.\n[46] P. Rodeghero, S. Jiang, A. Armaly, and C. McMillan, “Detecting user\nstory information in developer-client conversations to generate extractive\nsummaries,” in Proceedings of the 39th International Conference on\nSoftware Engineering, ser. ICSE ?17, 2017, p. 49?59.\n[47] G. Sridhara, L. Pollock, and K. Vijay-Shanker, “Automatically detecting\nand describing high level actions within methods,” in 2011 33rd Interna-\ntional Conference on Software Engineering (ICSE) , 2011, pp. 101–110.\n[48] P. W. McBurney and C. McMillan, “Automatic source code summa-\nrization of context for java methods,” IEEE Transactions on Software\nEngineering, vol. 42, no. 2, pp. 103–119, 2016.\n[49] X. Hu, G. Li, X. Xia, D. Lo, and Z. Jin, “Deep code comment\ngeneration,” inProceedings of the 26th Conference on Program Compre-\nhension, ser. ICPC ?18. Association for Computing Machinery, 2018,\np. 200?210.\n[50] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: A method for\nautomatic evaluation of machine translation,” in Proceedings of the 40th\nAnnual Meeting on Association for Computational Linguistics , ser. ACL\n’02, 2002, pp. 311–318.\n[51] S. Iyer, I. Konstas, A. Cheung, and L. Zettlemoyer, “Summarizing\nsource code using a neural attention model,” in Proceedings of\nthe 54th Annual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) . Berlin, Germany: Association\nfor Computational Linguistics, Aug. 2016, pp. 2073–2083. [Online].\nAvailable: https://www.aclweb.org/anthology/P16-1195\n[52] M. Allamanis, H. Peng, and C. A. Sutton, “A convolutional attention\nnetwork for extreme summarization of source code,” CoRR, vol.\nabs/1602.03001, 2016. [Online]. Available: http://arxiv.org/abs/1602.\n03001\n[53] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” in Advances\nin neural information processing systems , 2017, pp. 5998–6008.\n[54] J. Cheng, L. Dong, and M. Lapata, “Long short-term memory-networks\nfor machine reading,” in Proceedings of the 2016 Conference on\nEmpirical Methods in Natural Language Processing , 2016, pp. 551–\n561.\n[55] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n[56] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 770–778.\n[57] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhut-\ndinov, “Dropout: a simple way to prevent neural networks from over-\nﬁtting,” The journal of machine learning research , vol. 15, no. 1, pp.\n1929–1958, 2014.\n[58] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\n“Codesearchnet challenge: Evaluating the state of semantic code search,”\narXiv preprint arXiv:1909.09436 , 2019.\n[59] T. Kudo and J. Richardson, “Sentencepiece: A simple and language\nindependent subword tokenizer and detokenizer for neural text\nprocessing,” CoRR, vol. abs/1808.06226, 2018. [Online]. Available:\nhttp://arxiv.org/abs/1808.06226\n[60] A. Qusef, G. Bavota, R. Oliveto, A. De Lucia, and D. Binkley,\n“Recovering test-to-code traceability using slicing and textual analysis,”\nJ. Syst. Softw., vol. 88, no. C, p. 147–168, Feb. 2014.\n[61] N. Arivazhagan, A. Bapna, O. Firat, D. Lepikhin, M. Johnson,\nM. Krikun, M. X. Chen, Y . Cao, G. F. Foster, C. Cherry, W. Macherey,\nZ. Chen, and Y . Wu, “Massively multilingual neural machine translation\nin the wild: Findings and challenges,” CoRR, vol. abs/1907.05019,\n2019. [Online]. Available: http://arxiv.org/abs/1907.05019\n[62] A. Graves, “Sequence transduction with recurrent neural networks,”\nCoRR, vol. abs/1211.3711, 2012. [Online]. Available: http://arxiv.org/\nabs/1211.3711\n[63] N. Boulanger-Lewandowski, Y . Bengio, and P. Vincent, “Audio chord\nrecognition with recurrent neural networks.” in ISMIR. Citeseer, 2013,\npp. 335–340.\n[64] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine translation by\njointly learning to align and translate,” CoRR, vol. abs/1409.0473, 2014.\n[65] V . Raychev, M. Vechev, and E. Yahav, “Code completion with\nstatistical language models,” in Proceedings of the 35th ACM SIGPLAN\nConference on Programming Language Design and Implementation ,\nser. PLDI ’14. New York, NY , USA: ACM, 2014, pp. 419–428.\n[Online]. Available: http://doi.acm.org/10.1145/2594291.2594321\n[66] J. Howard and S. Ruder, “Universal language model ﬁne-tuning for text\nclassiﬁcation,” arXiv preprint arXiv:1801.06146 , 2018.\n[67] “Google colaboraty,” https://colab.research.google.com/.\n[68] C.-Y . Lin, “Rouge: A package for automatic evaluation of summaries,”\nin Text summarization branches out , 2004, pp. 74–81.\n[69] I. Caswell and B. Liang, “Recent advances in google translate,” https:\n//ai.googleblog.com/2020/06/recent-advances-in-google-translate.html,\n2020.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8469157218933105
    },
    {
      "name": "Natural language processing",
      "score": 0.6328398585319519
    },
    {
      "name": "Transformer",
      "score": 0.6261571645736694
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6169975996017456
    },
    {
      "name": "Sentence",
      "score": 0.593798816204071
    },
    {
      "name": "Transfer of learning",
      "score": 0.5135593414306641
    },
    {
      "name": "Source code",
      "score": 0.4619460701942444
    },
    {
      "name": "Reuse",
      "score": 0.4597087502479553
    },
    {
      "name": "Language model",
      "score": 0.4581034183502197
    },
    {
      "name": "Code (set theory)",
      "score": 0.4536740481853485
    },
    {
      "name": "Natural language",
      "score": 0.42743125557899475
    },
    {
      "name": "Task (project management)",
      "score": 0.4258253276348114
    },
    {
      "name": "Programming language",
      "score": 0.18451553583145142
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.09300664067268372
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    }
  ],
  "cited_by": 9
}