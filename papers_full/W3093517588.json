{
    "title": "mT5: A massively multilingual pre-trained text-to-text transformer",
    "url": "https://openalex.org/W3093517588",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4296406389",
            "name": "Xue, Linting",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4281801133",
            "name": "Constant, Noah",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224145415",
            "name": "Roberts, Adam",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221359253",
            "name": "Kale, Mihir",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4282221971",
            "name": "Al-Rfou, Rami",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227343800",
            "name": "Siddhant, Aditya",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4281801129",
            "name": "Barua, Aditya",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222913930",
            "name": "Raffel, Colin",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2809324505",
        "https://openalex.org/W2999168658",
        "https://openalex.org/W3032532958",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2982399380",
        "https://openalex.org/W2986154550",
        "https://openalex.org/W2963979492",
        "https://openalex.org/W3085479580",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W3042711927",
        "https://openalex.org/W3030045039",
        "https://openalex.org/W3168481568",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W2970752815",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2953109491",
        "https://openalex.org/W3026997957",
        "https://openalex.org/W3094152627",
        "https://openalex.org/W3081210419",
        "https://openalex.org/W3097879195",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W3098637735",
        "https://openalex.org/W3045462440",
        "https://openalex.org/W3099655892",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W3037854022",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3039017601",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2953958347",
        "https://openalex.org/W3032816972",
        "https://openalex.org/W3013840636",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W2980404057",
        "https://openalex.org/W3023133114",
        "https://openalex.org/W2891555348",
        "https://openalex.org/W2974593375",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W3034469191",
        "https://openalex.org/W3040245432",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2990188683",
        "https://openalex.org/W2996580882",
        "https://openalex.org/W3006439205",
        "https://openalex.org/W2958953787",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3100107515",
        "https://openalex.org/W2940024477",
        "https://openalex.org/W3023690688"
    ],
    "abstract": "The recent \"Text-to-Text Transfer Transformer\" (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent \"accidental translation\" in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.",
    "full_text": "mT5: A Massively Multilingual Pre-trained Text-to-Text Transformer\nLinting Xue∗ Noah Constant∗ Adam Roberts∗\nMihir Kale Rami Al-Rfou Aditya Siddhant Aditya Barua Colin Raffel\nGoogle Research\nAbstract\nThe recent “Text-to-Text Transfer Trans-\nformer” (T5) leveraged a uniﬁed text-to-text\nformat and scale to attain state-of-the-art re-\nsults on a wide variety of English-language\nNLP tasks. In this paper, we introduce mT5, a\nmultilingual variant of T5 that was pre-trained\non a new Common Crawl-based dataset cover-\ning 101 languages. We detail the design and\nmodiﬁed training of mT5 and demonstrate its\nstate-of-the-art performance on many multilin-\ngual benchmarks. We also describe a simple\ntechnique to prevent “accidental translation”\nin the zero-shot setting, where a generative\nmodel chooses to (partially) translate its pre-\ndiction into the wrong language. All of the\ncode and model checkpoints used in this work\nare publicly available.1\n1 Introduction\nCurrent natural language processing (NLP)\npipelines often make use of transfer learning, where\na model is pre-trained on a data-rich task before\nbeing ﬁne-tuned on a downstream task of interest\n(Ruder et al., 2019). The success of this paradigm\nis partially thanks to the release of parameter check-\npoints for pre-trained models. These checkpoints\nallow members of the NLP community to quickly\nattain strong performance on many tasks without\nneeding to perform expensive pre-training them-\nselves. As one example, the pre-trained check-\npoints for the “Text-to-Text Transfer Transformer”\n(T5) model released by Raffel et al. (2020) have\nbeen used to achieve state-of-the-art results on\nmany benchmarks (Khashabi et al., 2020; Roberts\net al., 2020; Kale, 2020; Izacard and Grave, 2020;\nNogueira et al., 2020; Narang et al., 2020, etc.).\nUnfortunately, many of these language models\nwere pre-trained solely on English-language text.\n∗Equal Contribution. Please direct correspondence to\nlintingx@google.com, nconstant@google.com,\nadarob@google.com, and craffel@google.com\n1https://goo.gle/mt5-code\nThis signiﬁcantly limits their use given that roughly\n80% of the world population does not speak En-\nglish (Crystal, 2008). One way the community\nhas addressed this English-centricity has been to\nrelease dozens of models, each pre-trained on a\nsingle non-English language (Carmo et al., 2020;\nde Vries et al., 2019; Le et al., 2020; Martin et al.,\n2020; Delobelle et al., 2020; Malmsten et al., 2020;\nNguyen and Tuan Nguyen, 2020; Polignano et al.,\n2019, etc.). A more general solution is to produce\nmultilingual models that have been pre-trained on\na mixture of many languages. Popular models of\nthis type are mBERT (Devlin, 2018), mBART (Liu\net al., 2020a), and XLM-R (Conneau et al., 2020),\nwhich are multilingual variants of BERT (Devlin\net al., 2019), BART (Lewis et al., 2020b), and\nRoBERTa (Liu et al., 2019), respectively.\nIn this paper, we continue this tradition by re-\nleasing mT5, a multilingual variant of T5. Our goal\nwith mT5 is to produce a massively multilingual\nmodel that deviates as little as possible from the\nrecipe used to create T5. As such, mT5 inherits\nall of the beneﬁts of T5 (described in section 2),\nsuch as its general-purpose text-to-text format, its\ndesign based on insights from a large-scale em-\npirical study, and its scale. To train mT5, we in-\ntroduce a multilingual variant of the C4 dataset\ncalled mC4. mC4 comprises natural text in 101\nlanguages drawn from the public Common Crawl\nweb scrape. To validate the performance of mT5,\nwe include results on several benchmark datasets,\nshowing state-of-the-art results in many cases. Fi-\nnally, we characterize a problematic behavior of\npre-trained generative multilingual language mod-\nels in the zero-shot setting, where they erroneously\ntranslate part of their prediction into the wrong lan-\nguage. To address this “accidental translation”, we\ndescribe a simple procedure that involves mixing\nin unlabeled pre-training data during ﬁne-tuning\nand demonstrate that it dramatically alleviates this\nissue. We release our pre-trained models and code\narXiv:2010.11934v3  [cs.CL]  11 Mar 2021\nso that the community can leverage our work.1\n2 Background on T5 and C4\nIn this section, we provide a short overview of T5\nand the C4 pre-training dataset. Further details are\navailable in Raffel et al. (2020).\nT5 is a pre-trained language model whose pri-\nmary distinction is its use of a uniﬁed “text-to-\ntext” format for all text-based NLP problems. This\napproach is natural for generative tasks (such as\nmachine translation or abstractive summarization)\nwhere the task format requires the model to gen-\nerate text conditioned on some input. It is more\nunusual for classiﬁcation tasks, where T5 is trained\nto output the literal text of the label (e.g. “posi-\ntive” or “negative” for sentiment analysis) instead\nof a class index. The primary advantage of this\napproach is that it allows the use of exactly the\nsame training objective (teacher-forced maximum-\nlikelihood) for every task, which in practice means\nthat a single set of hyperparameters can be used for\neffective ﬁne-tuning on any downstream task. Sim-\nilar unifying frameworks were proposed by Keskar\net al. (2019) and McCann et al. (2018). Given the\nsequence-to-sequence structure of this task format,\nT5 uses a basic encoder-decoder Transformer ar-\nchitecture as originally proposed by Vaswani et al.\n(2017). T5 is pre-trained on a masked language\nmodeling “span-corruption” objective, where con-\nsecutive spans of input tokens are replaced with a\nmask token and the model is trained to reconstruct\nthe masked-out tokens.\nAn additional distinguishing factor of T5 is its\nscale, with pre-trained model sizes available from\n60 million to 11 billion parameters. These models\nwere pre-trained on around 1 trillion tokens of data.\nUnlabeled data comes from the C4 dataset, which\nis a collection of about 750GB of English-language\ntext sourced from the public Common Crawl web\nscrape. C4 includes heuristics to extract only nat-\nural language (as opposed to boilerplate and other\ngibberish) in addition to extensive deduplication.\nThe pre-training objective, model architecture, scal-\ning strategy, and many other design choices for T5\nwere chosen based on a large-scale empirical study\ndescribed in detail in Raffel et al. (2020).\n3 mC4 and mT5\nOur goal in this paper is to create a massively mul-\ntilingual model that follows T5’s recipe as closely\nas possible. Towards this end, we develop an ex-\ntended version of the C4 pre-training dataset that\ncovers 101 languages and introduce changes to T5\nto better suit this multilinguality.\n3.1 mC4\nThe C4 dataset was explicitly designed to be\nEnglish only: any page that was not given a\nprobability of at least 99% of being English by\nlangdetect2 was discarded. In contrast, for\nmC4 we use cld33 to identify over 100 languages.\nSince some of these languages are relatively scarce\non the internet, we make use of all of the 71\nmonthly web scrapes released so far by Common\nCrawl. This is dramatically more source data\nthan was used for C4, for which the April 2019\nweb scrape alone was enough to provide plenty of\nEnglish-language data.\nAn important heuristic ﬁltering step in C4 was\nthe removal of lines that did not end in an English\nterminal punctuation mark. Since many languages\ndo not use English terminal punctuation marks, we\ninstead apply a “line length ﬁlter” that requires\npages to contain at least three lines of text with 200\nor more characters. Otherwise, we follow C4’s ﬁl-\ntering by deduplicating lines across documents and\nremoving pages containing bad words.4 Finally, we\ndetect each page’s primary language usingcld3\nand remove those with a conﬁdence below 70%.\nAfter these ﬁlters are applied, we group the re-\nmaining pages by language and include in the cor-\npus all languages with 10,000 or more pages. This\nproduces text in 107 “languages” as deﬁned by\ncld3. However, we note that six of these are\njust script variants of the same spoken language\n(e.g. ru is Russian in Cyrillic script andru-Latn\nis Russian in Latin script). A histogram of the\npage counts for each language is shown in ﬁg. 1.\nDetailed dataset statistics including per-language\ntoken counts are shown in the appendix.\n3.2 mT5\nThe model architecture and training procedure that\nwe use for mT5 closely follows that of T5. Speciﬁ-\ncally, we base mT5 on the “T5.1.1” recipe,5 which\nimproves upon T5 by using GeGLU nonlinearities\n(Shazeer, 2020), scaling bothdmodel and dﬀ instead\n2https://pypi.org/project/langdetect/\n3https://github.com/google/cld3\n4https://github.com/LDNOOBW/\n5https://github.com/google-research/\ntext-to-text-transfer-transformer/blob/\nmaster/released_checkpoints.md#t511\nen\nru\nes\nde\nfr\nit\npt\npl\nnl\ntr\nja\nvi\nid\ncs\nzh\nfa\nar\nsv\nro\nel\nuk\nhu\nda\nfi\nno\nbg\nhi\nsk\nko\nth\nca\nms\niw\nlt\nsl\nmr\nbn\net\nlv\naz\ngl\ncy\nsq\nta\nsr\nne\nlb\nhy\nkk\nka\nmt\naf\nfil\nis\nmk\nml\nmn\nur\nbe\nla\neu\ntg\nte\nfy\nkn\nky\nsw\nso\nmy\nuz\nkm\nru-Latn\nsd\ngu\nhi-Latn\njv\nzu\nsi\nja-Latn\neo\nco\nga\nel-Latn\nzh-Latn\npa\nceb\nmg\nps\nsn\ngd\nku\nhmn\nsu\nht\nha\nny\nam\nbg-Latn\nyi\nlo\nmi\nsm\nig\nhaw\nxh\nst\nyo\n105\n106\n107\n108\n109\nPages of mC4 training text\n0.008\n0.04\n0.2\n1\n5\n25\n% of mT5 training examples\n=0.2\n=0.3\n=0.7\nFigure 1: Page counts per language in mC4 (left axis), and percentage of mT5 training examples coming from\neach language, for different language sampling exponents α(right axis). Our ﬁnal model uses α=0.3.\nModel Architecture Parameters # languages Data source\nmBERT (Devlin, 2018) Encoder-only 180M 104 Wikipedia\nXLM (Conneau and Lample, 2019) Encoder-only 570M 100 Wikipedia\nXLM-R (Conneau et al., 2020) Encoder-only 270M – 550M 100 Common Crawl (CCNet)\nmBART (Lewis et al., 2020b) Encoder-decoder 680M 25 Common Crawl (CC25)\nMARGE (Lewis et al., 2020a) Encoder-decoder 960M 26 Wikipedia or CC-News\nmT5 (ours) Encoder-decoder 300M – 13B 101 Common Crawl (mC4)\nTable 1: Comparison of mT5 to existing massively multilingual pre-trained language models. Multiple versions of\nXLM and mBERT exist; we refer here to the ones that cover the most languages. Note that XLM-R counts ﬁve\nRomanized variants as separate languages, while we ignore six Romanized variants in the mT5 language count.\nof just dﬀ in the larger models, and pre-training on\nunlabeled data only with no dropout. We refer to\nRaffel et al. (2020) for further details on T5.\nA major factor in pre-training multilingual mod-\nels is how to sample data from each language.\nUltimately, this choice is a zero-sum game: If\nlow-resource languages are sampled too often, the\nmodel may overﬁt; if high-resource languages are\nnot trained on enough, the model will underﬁt. We\ntherefore take the approach used in (Devlin, 2018;\nConneau et al., 2020; Arivazhagan et al., 2019) and\nboost lower-resource languages by sampling ex-\namples according to the probability p(L) ∝|L|α,\nwhere p(L) is the probability of sampling text from\na given language during pre-training and |L|is the\nnumber of examples in the language. The hyper-\nparameter α(typically with α <1) allows us to\ncontrol how much to “boost” the probability of\ntraining on low-resource languages. Values used\nby prior work include α= 0.7 for mBERT (Devlin,\n2018), α= 0.3 for XLM-R (Conneau et al., 2020),\nand α = 0.2 for MMNMT (Arivazhagan et al.,\n2019). We tried all three of these values (ablation\nresults in section 4.2) and found α= 0.3 to give a\nreasonable compromise between performance on\nhigh- and low-resource languages.\nThe fact that our model covers over 100 lan-\nguages necessitates a larger vocabulary. Following\nXLM-R (Conneau et al., 2018), we increase the vo-\ncabulary size to 250,000 wordpieces. As in T5, we\nuse SentencePiece (Kudo and Richardson, 2018;\nKudo, 2018) models trained with the language sam-\npling rates used during pre-training. To accom-\nmodate languages with large character sets like\nChinese, we use a character coverage of 0.99999\nand enable SentencePiece’s “byte-fallback” feature\nto ensure that any string can be uniquely encoded.\n3.3 Comparison to related models\nTo contextualize our new model, we provide a brief\ncomparison with existing massively multilingual\npre-trained language models. For brevity, we focus\non models that support more than a few dozen lan-\nguages. Table 1 gives a high-level comparison of\nmT5 to the most similar models.\nmBERT (Devlin, 2018) is a multilingual ver-\nsion of BERT (Devlin et al., 2019). Similar to our\napproach with mT5, mBERT follows the BERT\nrecipe as closely as possible (same architecture, ob-\njective, etc.). The primary difference is the training\nset: Instead of training on English Wikipedia and\nthe Toronto Books Corpus, mBERT is trained on\nup to 104 languages from Wikipedia. XLM (Con-\nneau and Lample, 2019) is also based on BERT but\napplies improved methods for pre-training multi-\nlingual language models including explicitly cross-\nlingual pre-training objectives. Many pre-trained\nversions of XLM have been released; the most\nmassively-multilingual variant was trained on 100\nlanguages from Wikipedia. XLM-R (Conneau\nModel Sentence pair Structured Question answering\nXNLI PAWS-X WikiAnn NER XQuAD MLQA TyDiQA-GoldP\nMetrics Acc. Acc. F1 F1 / EM F1 / EM F1 / EM\nCross-lingual zero-shot transfer (models ﬁne-tuned on English data only)\nmBERT 65.4 81.9 62.2 64.5 / 49.4 61.4 / 44.2 59.7 / 43.9\nXLM 69.1 80.9 61.2 59.8 / 44.3 48.5 / 32.6 43.6 / 29.1\nInfoXLM 81.4 - - - / - 73.6 / 55.2 - / -\nX-STILTs 80.4 87.7 64.7 77.2 / 61.3 72.3 / 53.5 76.0 / 59.5\nXLM-R 79.2 86.4 65.4 76.6 / 60.8 71.6 / 53.2 65.1 / 45.0\nVECO 79.9 88.7 65.7 77.3 / 61.8 71.7 / 53.2 67.6 / 49.1\nRemBERT 80.8 87.5 70.1 79.6 / 64.0 73.1 / 55.0 77.0 / 63.0\nmT5-Small 67.5 82.4 50.5 58.1 / 42.5 54.6 / 37.1 35.2 / 23.2\nmT5-Base 75.4 86.4 55.7 67.0 / 49.0 64.6 / 45.0 57.2 / 41.2\nmT5-Large 81.1 88.9 58.5 77.8 / 61.5 71.2 / 51.7 69.9 / 52.2\nmT5-XL 82.9 89.6 65.5 79.5 / 63.6 73.5 / 54.5 75.9 / 59.4\nmT5-XXL 85.0 90.0 69.2 82.5 / 66.8 76.0 / 57.4 80.8 / 65.9\nTranslate-train (models ﬁne-tuned on English data plus translations in all target languages)\nXLM-R 82.6 90.4 - 80.2 / 65.9 72.8 / 54.3 66.5 / 47.7\nFILTER + Self-Teaching 83.9 91.4 - 82.4 / 68.0 76.2 / 57.7 68.3 / 50.9\nVECO 83.0 91.1 - 79.9 / 66.3 73.1 / 54.9 75.0 / 58.9\nmT5-Small 64.7 79.9 - 64.3 / 49.5 56.6 / 38.8 48.2 / 34.0\nmT5-Base 75.9 89.3 - 75.3 / 59.7 67.6 / 48.5 64.0 / 47.7\nmT5-Large 81.8 91.2 - 81.2 / 65.9 73.9 / 55.2 71.1 / 54.9\nmT5-XL 84.8 91.0 - 82.7 / 68.1 75.1 / 56.6 79.9 / 65.3\nmT5-XXL 87.8 91.5 - 85.2 / 71.3 76.9 / 58.3 82.8 / 68.8\nIn-language multitask (models ﬁne-tuned on gold data in all target languages)\nmBERT - - 89.1 - - 77.6 / 68.0\nmT5-Small - - 83.4 - - 73.0 / 62.0\nmT5-Base - - 85.4 - - 80.8 / 70.0\nmT5-Large - - 88.4 - - 85.5 / 75.3\nmT5-XL - - 90.9 - - 87.5 / 78.1\nmT5-XXL - - 91.2 - - 88.5 / 79.1\nTable 2: Results on XTREME sentence-pair classiﬁcation, structured prediction and question answering tasks.\nmBERT metrics are from Hu et al. (2020). Metrics for XLM, InfoXLM, X-STILTs and XLM-R are from Fang\net al. (2020), though Conneau et al. (2020) report better performance of XLM-R on XNLI (80.9). All other metrics\nare from the original sources: F ILTER (Fang et al., 2020), VECO (Luo et al., 2020) and RemBERT (Chung et al.,\n2020). For the “translate-train” setting, we include English training data, so as to be comparable with Fang et al.\n(2020) and Luo et al. (2020). This differs from the XTREME “translate-train” setup of Hu et al. (2020). For mT5\nresults on TyDi QA zero-shot, we report the median across ﬁve ﬁne-tuning runs, as we observed high variance\nacross runs. Full results for all languages in all tasks are provided in the appendix.\net al., 2020) is an improved version of XLM based\non the RoBERTa model (Liu et al., 2019). XLM-R\nis trained with a cross-lingual masked language\nmodeling objective on data in 100 languages from\nCommon Crawl. To improve the pre-training data\nquality, pages from Common Crawl were ﬁltered\nby an n-gram language model trained on Wikipedia\n(Wenzek et al., 2020). mBART (Liu et al., 2020a)\nis a multilingual encoder-decoder model that is\nbased on BART (Lewis et al., 2020b). mBART is\ntrained with a combination of span masking and\nsentence shufﬂing objectives on a subset of 25 lan-\nguages from the same data as XLM-R. MARGE\n(Lewis et al., 2020a) is a multilingual encoder-\ndecoder model that is trained to reconstruct a docu-\nment in one language by retrieving documents in\nother languages. It uses data in 26 languages from\nWikipedia and CC-News (Liu et al., 2019).\n4 Experiments\nTo validate the performance of mT5, we evaluate\nour models on 6 tasks from the XTREME multilin-\ngual benchmark (Hu et al., 2020): the XNLI (Con-\nneau et al., 2018) entailment task covering 14 lan-\nguages; the XQuAD (Artetxe et al., 2020), MLQA\n(Lewis et al., 2019), and TyDi QA (Clark et al.,\n2020) reading comprehension benchmarks with 10,\n7, and 11 languages respectively; the Named En-\ntity Recognition (NER) dataset of WikiAnn (Pan\net al., 2017) restricted to the 40 languages from\nXTREME (Hu et al., 2020), and the PAWS-X(Yang\net al., 2019) paraphrase identiﬁcation dataset with\n7 languages. We cast all tasks into the text-to-text\nformat, i.e. generating the label text (XNLI and\nPAWS-X), entity tags and labels (WikiAnn NER),\nor answer (XQuAD, MLQA, and TyDi QA) di-\nrectly in a generative fashion. For NER, if there\nare multiple entities, then they are concatenated\nin the order they appear, and if there are no en-\ntities then the target text is “None”. We con-\nsider three variants of these tasks: (1) “zero-shot”,\nwhere the model is ﬁne-tuned only on English data,\n(2) “translate-train”, adding machine translations\nfrom English into each target language, and (3) “in-\nlanguage multitask”, training on gold data in all\ntarget languages. For brevity, we refer to Hu et al.\n(2020) for further details on these benchmarks.\nFollowing the original T5 recipe, we consider\nﬁve model sizes: Small (≈ 300M parameters),\nBase (580M), Large (1.2B), XL (3.7B), and XXL\n(13B). The increase in parameter counts com-\npared to the corresponding T5 model variants\ncomes from the larger vocabulary used in mT5.\nNote that, because mT5 is an encoder-decoder\nmodel, it has roughly twice as many parameters as\ncorrespondingly-sized encoder-only models such\nas XLM-R. For example, the “Large” variant of\nXLM-R has 550 million parameters whereas mT5-\nLarge has around 1 billion. However, the compu-\ntational cost for text classiﬁcation is roughly the\nsame: In both cases, the model processes a length-\nT input sequence with an encoder of approximately\nequal size. In an encoder-only model like XLM-R,\nthe encoder processes one additional \"CLS\" token,\nwhich is used to generate the representation for clas-\nsiﬁcation. In mT5, the decoder typically produces\ntwo additional tokens: the class label and an end-\nof-sequence token. Since the decoder has the same\narchitecture (ignoring encoder-decoder attention)\nas the encoder, the computational cost of classiﬁ-\ncation with mT5 typically amounts to the cost of\nprocessing T + 2tokens compared to T + 1for\nan encoder-only model. However, encoder-decoder\narchitectures have the additional beneﬁt of being\napplicable to generative tasks like abstractive sum-\nmarization or dialog.\nWe pre-train our mT5 model variants for1 mil-\nlion steps on batches of 1024 length-1024 input\nsequences, corresponding to roughly 1 trillion in-\nput tokens total. This is the same amount of pre-\ntraining as T5 and about 1\n6 as much as XLM-R.\nWe use the same inverse square-root learning\nrate schedule used by T5 during pre-training, with\nthe learning rate set to 1/\n√\nmax(n,k) where nis\nthe current training iteration and k = 104 is the\nnumber of warm-up steps. Following the T5.1.1\nrecipe, we do not apply dropout during pre-training.\nWe use the same self-supervised objective as T5,\nwith 15% of tokens masked and an average noise\nspan length of 3. We ablate some of these experi-\nmental details in section 4.2.\nFor ﬁne-tuning, we use a constant learning rate\nof 0.001 and dropout rate of 0.1 for all tasks. We\nuse batch size 217 for most tasks but increased this\nup to 220 in a few cases based on performance\non the validation set. For early stopping, we save\ncheckpoints every 200 steps and choose the check-\npoint with the highest validation performance.\n4.1 Results\nTable 2 presents our main results, with per-\nlanguage breakdowns for each task given in the\nappendix. Our largest model mT5-XXL exceeds\nstate-of-the-art on all classiﬁcation and QA tasks\nand is near SOTA on NER (69.2 vs. 70.1). Note\nthat unlike our model, InfoXLM (Chi et al., 2020)\nand VECO (Luo et al., 2020) beneﬁt from paral-\nlel training data, while X-STILTs (Phang et al.,\n2020) leverages labeled data from tasks similar to\nthe target task. Overall, our results highlight the\nimportance of model capacity in cross-lingual rep-\nresentation learning and suggest that scaling up a\nsimple pre-training recipe can be a viable alterna-\ntive to more complex techniques relying on LM\nﬁltering, parallel data, or intermediate tasks.\nIn the “translate-train” setting, we exceed state-\nof-the-art on all XTREME classiﬁcation and QA\ntasks. For these tasks, we ﬁne-tune on the combina-\ntion of the labeled English data and machine trans-\nlations thereof.6 This allows direct comparison\nwith both FILTER (Fang et al., 2020) as well as the\nXLM-R baseline of Fang et al. (2020). Note that\nthis setup differs from XTREME “translate-train”\n(Hu et al., 2020), which excludes English.\nFigure 2 shows that model capacity is key to\nimproving performance on variants of the TyDi\nQA GoldP task in the absence of “gold” multi-\nlingual data: For the smallest model, training on\ngold datasets (in-language multitask) achieves dra-\n6We use the translation data provided by Hu et al. (2020)\nthroughout. On the PAWS-X task, FILTER used translation\ndata from the original task instead. Switching to this data\nwould improve our scores slightly (mT5-XXL 91.5 →92.0).\nT5 mT5\nSmall 87.2 / 79.1 84.7 / 76.4\nBase 92.1 / 85.4 89.6 / 83.8\nLarge 93.8 / 86.7 93.0 / 87.0\nXL 95.0 / 88.5 94.5 / 88.9\nXXL 96.2 / 91.3 95.6 / 90.4\nTable 3: Comparison of T5 vs. mT5 on SQuAD ques-\ntion answering (F1/EM).\n109 1010\n# Parameters\n40\n50\n60\n70\n80\n90F1\nHuman\nIn-Language Multitask\nTranslate-Train\nZero-Shot\nFigure 2: Average F1 on the TyDi QA GoldP task\nacross languages. Performance improves with increas-\ning model capacity. The importance of in-language\ntraining data (whether gold In-Lanugage Multitask or\nsynthetic Translate-Train) decreases with model scale,\nas seen by Zero-Shot closing the quality gap.\nmatically better performance than using weakly\nsupervised data (translate-train) or English-only\ndata (zero-shot), whereas the gap between these\nthree settings is much smaller for the largest model.\nFor our two largest models, zero-shot and translate-\ntrain performance is nearly the same, showing that\nmachine translations of the monolingual dataset\nbring diminishing returns as model capacity in-\ncreases. Overall, these trends point to the possibil-\nity of avoiding the costly step of annotating data in\nmore than one language when using large models.\nMassively multilingual models have been ob-\nserved to underperform on a given language when\ncompared to a similarly-sized “dedicated” model\ntrained speciﬁcally for that language (Arivazhagan\net al., 2019). To quantify this effect, we compare\nthe performance of mT5 and T5 when ﬁne-tuned\non the SQuAD reading comprehension benchmark\n(Rajpurkar et al., 2016). The results are shown in\ntable 3, with results for T5 reproduced from Raffel\net al. (2020). While the Small and Base mT5 mod-\nels fall short of their English T5 counterparts, we\nﬁnd that the larger models close the gap. This sug-\ngests there may be a turning point past which the\nModel Accuracy\nBaseline (mT5-Large) 81.1\nDropout 0.1 77.6\nSequence length 512 80.5\nSpan length 10 78.6\nα= 0.7 80.7\nα= 0.2 80.7\nNo line length ﬁlter 79.1\nAdd Wikipedia data 80.3\nTable 4: Average XNLI zero-shot accuracy of various\nablations on our mT5-Large model. Per-language met-\nrics are shown in the appendix.\nmodel has enough capacity to effectively learn 101\nlanguages without signiﬁcant interference effects.\n4.2 Ablation\nWe run six ablations, modifying various settings,\nusing our Large model as a baseline: (i) increase\ndropout to 0.1 in hopes of mitigating overﬁtting\non low-resource languages, (ii) decrease sequence\nlength to 512 (as was used in T5), (iii) increase the\naverage noise span length in the pre-training objec-\ntive to 10 since we observe fewer characters per\ntoken than T5, (iv) adjust the language sampling\nexponent αto {0.2, 0.7} as used in MMNMT (Ari-\nvazhagan et al., 2019) and mBERT (Devlin, 2018),\nrespectively, (v) turn off the “line length ﬁlter” in\nthe mC4 data pipeline, and (vi) supplement mC4\nwith Wikipedia data7 from 103 languages.\nThe effect of these ablations on XNLI zero-shot\naccuracy is shown in table 4. In each case, the\naverage XNLI score is lower than the mT5-Large\nbaseline, justifying our chosen settings. The line\nlength ﬁlter provides a +2 point boost, corrobo-\nrating the ﬁndings of Conneau et al. (2020) and\nRaffel et al. (2020) that ﬁltering low-quality pages\nfrom Common Crawl is valuable. Increasing the\nlanguage sampling exponent αto 0.7 has the ex-\npected effect of improving performance in high-\nresource languages (e.g. Russian 81.5 →82.8),\nwhile hurting low-resource languages (e.g. Swahili\n75.4 →70.6), with the average effect being nega-\ntive. Conversely, loweringαto 0.2 boosts one tail\nlanguage slightly (Urdu 73.5 →73.9) but is harm-\nful elsewhere. Detailed per-language metrics on\nXNLI and the results of our ablations on zero-shot\nXQuAD are provided in the appendix, showing\nsimilar trends.\n7We use the 2020 Wikipedia data from TensorFlow\nDatasets, selecting the same languages as mBERT.\nhttps://www.tensorflow.org/datasets/\ncatalog/wikipedia\n5 Zero-shot generation\nSince mT5 is a generative model, it can output\narbitrary text predictions in a free form fashion.\nThis is in contrast to “encoder-only” models like\nmBERT and XLM(-R) that make a prediction by ei-\nther extracting it from the input or producing a class\nlabel. We found that the lack of constraints during\nprediction caused mT5 to sometimes have trouble\ngenerating a well-formed prediction in a language\nunseen during ﬁne-tuning. Focusing on XQuAD\nzero-shot, we ﬁnd that many of these errors are\ndue to “accidental translation” into the ﬁne-tuning\nlanguage (English). In this section, we characterize\nthis behavior and demonstrate that it can be counter-\nacted by mixing a small amount of our multilingual\npre-training task into the ﬁne-tuning stage.\n5.1 Illegal predictions\nIn using a generative model for span selection (as\nin extractive QA tasks), we hope the model learns\nto generate “legal” spansthat are substrings of the\nprovided context. However, unlike encoder-based\nmodels like BERT, this is not a hard constraint of\nthe model. Notably, T5 learns to always output\nlegal spans on SQuAD, suggesting this is not a\nmajor issue for generative models in simple cases.\nA more challenging case for generative models is\nzero-shot cross-lingual span selection. Here, a pre-\ntrained multilingual model is ﬁne-tuned on English\nbut tested on other languages. We want the model\nto generate legal non-English predictions despite\nhaving only seen English targets in ﬁne-tuning.\nIn practice, while mT5 achieves SOTA on\nthe zero-shot variants of XQuAD, MLQA and\nTyDi QA, illegal predictions are still a problem. For\nexample, on zero-shot XQuAD, a non-trivial por-\ntion of mT5 mistakes are in fact illegal spans, for all\nmodel sizes (cf. ﬁg. 4 “Baseline”). Through inspec-\ntion, we ﬁnd these illegal predictions mainly fall\ninto three categories: (i) normalization, (ii) gram-\nmatical adjustment, and (iii) accidental translation.\nTable 5 provides examples of each type.\nNormalization indicates predictions that would\nbe legal, except that “equivalent” Unicode charac-\nters have been substituted, so a legal span may be\nrecovered through Unicode NFKC normalization.\nThis is particularly common in Thai, Chinese and\nHindi, where most mT5-XXL illegal predictions\nare resolved by normalization, as seen in ﬁg. 3b.\nGrammatical adjustmentinvolves minor mor-\nphological changes to the original text. We fre-\nTarget Prediction Explanation\nจํานวนเฉพาะ จํานวนเฉพาะ Decomposed Thai  ํา into  ํ + า\nलोथर डे माइिज़यर लोथर डे माइिज़यर Decomposed Hindi ज़ into ज + ◌़\n27 - 30 ％ 27 - 30 % Replaced full-width percent sign\n12 . ª 12 . a Removed superscript\n اﻟﻼھﻮاﺋﯿﺔ ﻟﻠﺒﻜﺘﺮﯾﺎ  اﻟﻼھﻮاﺋﯿﺔ اﻟﺒﻜﺘﺮﯾﺎArabic “for anaerobic bacteria”\n⇒ “anaerobic bacteria”\nстроками битов строки битов Russian “bit strings (instrumental)”\n⇒ “bit strings (nominative)”\nseis años six years Translated from Spanish\nZweiten Weltkrieg the Second World War Translated from German\n新英格兰爱国者队 New英格兰爱国者队 Partially translated Chinese\n“New England Patriots”\nхлоропласт chlorопласт Partially translated Russian \n“chloroplast”\nTable 5: Illegal mT5-XXL predictions on XQuAD zero-\nshot, illustrating normalization (top), grammatical ad-\njustment (middle) and translation (bottom).\nquently observe these adjustments when the target\nspan cannot stand as a well-formed answer on its\nown. For example, mT5-XXL’s Arabic and Russian\npredictions in the middle rows of table 5 are judged\nby native speakers as correct and grammatical an-\nswers to the posed XQuAD questions, while the\ngold targets are judged as ungrammatical answers.\nThis type of illegal prediction is most common in\nlanguages with extensive grammatical case mark-\ning, such as Russian, Turkish and German.\nAccidental translation involves the model\ntranslating part or all of a contextual span into En-\nglish (the language of all ﬁne-tuning data). On\nthe one hand, it is remarkable that mT5 performs\n“spontaneous” translation despite never seeing par-\nallel training data. On the other, as practitioners we\nwould ideally be able to control this behavior.\nWe observe accidental translation across all\nmodel sizes and all XQuAD languages. The prob-\nlem is most prevalent in mT5-Small and mT5-Base,\nwhere from manual inspection, half or more of the\nillegal predictions within each language exhibit\naccidental translation, with many of the illegal pre-\ndictions coming from Greek and Russian, as shown\nin ﬁg. 3a. While we do observe full phrase transla-\ntions, a more common occurrence is partial trans-\nlation, where the model outputs a token or two of\nEnglish before reverting to the correct target lan-\nguage. The transition may even occur mid-word,\nas in the prediction “chlorопласт”, where the ﬁrst\nhalf of the target “хлоропласт” (Russian: chloro-\nplast) has been translated to English.\n5.2 Preventing accidental translation\nThe most direct solution to avoiding accidental\ntranslation on span selection tasks would be to mod-\nel ru th ar de hi zh es tr vi en\n0\n10\n20\n30\n40\n50\n60\n70Percent\nIncorrect Illegal Illegal after norm\n(a) mT5-Small\nth ru zh hi de es el ar tr vi en\n0\n10\n20\n30\n40Percent\n(b) mT5-XXL\nFigure 3: Per-language error rates on XQuAD zero-\nshot, sorted by illegal rate. Incorrect: Not matching\nthe target span. Illegal: Missing from the input context.\nIllegal after norm: Illegal even after Unicode NFKC\nnormalization is applied to the prediction and context.\nify our inference procedure. As is common practice\nwith encoder-based models, we could devise a task-\nspeciﬁc ﬁne-tuning mechanism that restricts the\nmodel to perform ranking over legal spans, remov-\ning the possibility of illegal predictions entirely.\nWhile this would likely improve our zero-shot met-\nrics, it is unsatisfying for two reasons: First, it\nimplies taking a step backward from the general\ntext-to-text interface, as different tasks would de-\nmand different types of inference. Second, this\nsolution won’t extend to more “open-ended” zero-\nshot generative tasks like summarization, where\nthe legal output space can’t be easily delimited.\nFor these reasons, we consider a more general\nsolution that remains within the text-to-text frame-\nwork and can apply to all zero-shot generation\ntasks. Our motivating intuition is that the reason the\nmodel outputs English when given a non-English\ntest input is that it has never observed a non-English\ntarget during ﬁne-tuning. As English-only ﬁne-\ntuning proceeds, the model’s assigned likelihood\nof non-English tokens presumably decreases, even-\ntually reaching the point where English becomes\nthe most likely answer to any question.\nTo prevent the model from “forgetting” how to\ngenerate other languages, we use a strategy inspired\nby domain/task-adaptive pre-training (Howard and\nRuder, 2018; Gururangan et al., 2020): We simply\nmix in our unsupervised multilingual pre-training\nSmall Base Large XL XXL\n0\n10\n20\n30\n40\n50\n60\n70Percent\nBaseline Incorrect\nBaseline Illegal\nBaseline Illegal after norm\nDPT Incorrect\nDPT Illegal\nDPT Illegal after norm\nFigure 4: Error rates of mT5 on XQuAD zero-shot.\nBaseline: Fine-tuning on XQuAD alone. Domain Pre-\nserving Training (DPT): Mixing in the unsupervised\nmC4 task with ﬁne-tuning.\ntask during ﬁne-tuning. A similar approach was\nexplored by Liu et al. (2020b). We use the same\nmC4 task deﬁnition as in pre-training, with two\nadjustments: First, we remove all “sentinel” tokens\n(corresponding to non-masked spans in the input\ntext) from the target sequence, as otherwise we\nobserve occasional sentinels in downstream predic-\ntions. Second, we reduce the language sampling\nparameter αfrom 0.3 to 0.1. This produces a near-\nuniform distribution of languages, encouraging the\nmodel to treat all languages as equally likely.8\nWith these changes, we mix a small amount of\nour unsupervised task (covering 101 languages)\ninto XQuAD ﬁne-tuning, at a ratio of just 1:100.\nFigure 4 shows the results on XQuAD zero-shot er-\nror rates. The addition of even this small amount of\nmultilingual data has a marked effect on the mT5-\nSmall and mT5-Base models (where accidental\ntranslation was most rampant), reducing the illegal\nprediction rates by more than 70% (relative), and\ncontributing to an overall reduction in errors.\n6 Conclusion\nIn this paper, we introduced mT5 and mC4: mas-\nsively multilingual variants of the T5 model and\nC4 dataset. We demonstrated that the T5 recipe is\nstraightforwardly applicable to the multilingual set-\nting, and achieved strong performance on a diverse\nset of benchmarks. We also characterized illegal\npredictions that can occur in zero-shot evaluation\nof multilingual pre-trained generative models, and\ndescribed a simple technique to avoid this issue.\nWe release all code and pre-trained datasets used in\nthis paper to facilitate future work on multilingual\n8Alternatively, one could mix in unlabeled data only for a\nsingle language at a time. However, we believe this is contrary\nto the spirit of multilingual models and zero-shot evaluation.\nlanguage understanding.9\nAcknowledgements\nWe thank Melvin Johnson for tips on the translate-\ntrain procedure for XTREME and Itai Rolnick for\nhelp with infrastructure.\nReferences\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat,\nDmitry Lepikhin, Melvin Johnson, Maxim Krikun,\nMia Xu Chen, Yuan Cao, George Foster, Colin\nCherry, et al. 2019. Massively multilingual neural\nmachine translation in the wild: Findings and chal-\nlenges. arXiv preprint arXiv:1907.05019.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nDiedre Carmo, Marcos Piau, Israel Campiotti, Rodrigo\nNogueira, and Roberto Lotufo. 2020. PTT5: Pre-\ntraining and validating the t5 model on brazilian por-\ntuguese data. arXiv preprint arXiv:2008.09144.\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Sak-\nsham Singhal, Wenhui Wang, Xia Song, Xian-Ling\nMao, Heyan Huang, and Ming Zhou. 2020. In-\nfoXLM: An information-theoretic framework for\ncross-lingual language model pre-training. arXiv\npreprint arXiv:2007.07834.\nHyung Won Chung, Thibault Févry, Henry Tsai,\nMelvin Johnson, and Sebastian Ruder. 2020. Re-\nthinking embedding coupling in pre-trained lan-\nguage models. arXiv preprint arXiv:2010.12821.\nJonathan H. Clark, Eunsol Choi, Michael Collins, Dan\nGarrette, Tom Kwiatkowski, Vitaly Nikolaev, and\nJennimaria Palomaki. 2020. TyDi QA: A bench-\nmark for information-seeking question answering in\ntypologically diverse languages. Transactions of the\nAssociation for Computational Linguistics , 8:454–\n470.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances in\nNeural Information Processing Systems, volume 32,\npages 7059–7069.\n9https://goo.gle/mt5-code\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nDavid Crystal. 2008. Two thousand million? English\ntoday, 24(1):3–6.\nWietse de Vries, Andreas van Cranenburgh, Arianna\nBisazza, Tommaso Caselli, Gertjan van Noord, and\nMalvina Nissim. 2019. BERTje: A dutch BERT\nmodel. arXiv preprint arXiv:1912.09582.\nPieter Delobelle, Thomas Winters, and Bettina Berendt.\n2020. RobBERT: a dutch RoBERTa-based language\nmodel. arXiv preprint arXiv:2001.06286.\nJacob Devlin. 2018. Multilingual BERT\nREADME. https://github.com/\ngoogle-research/bert/blob/master/\nmultilingual.md.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nYuwei Fang, Shuohang Wang, Zhe Gan, Siqi Sun,\nand Jingjing Liu. 2020. FILTER: An enhanced fu-\nsion method for cross-lingual language understand-\ning. arXiv preprint arXiv:2009.05166.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339, Melbourne, Australia.\nAssociation for Computational Linguistics.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual general-\nization. arXiv preprint arXiv:2003.11080.\nGautier Izacard and Edouard Grave. 2020. Lever-\naging passage retrieval with generative models for\nopen domain question answering. arXiv preprint\narXiv:2007.01282.\nMihir Kale. 2020. Text-to-text pre-training for data-to-\ntext tasks. arXiv preprint arXiv:2005.10433.\nNitish Shirish Keskar, Bryan McCann, Caiming Xiong,\nand Richard Socher. 2019. Unifying question an-\nswering and text classiﬁcation via span extraction.\narXiv preprint arXiv:1904.09286.\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish\nSabharwal, Oyvind Tafjord, Peter Clark, and Han-\nnaneh Hajishirzi. 2020. UniﬁedQA: Crossing for-\nmat boundaries with a single QA system. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020 , pages 1896–1907, Online. As-\nsociation for Computational Linguistics.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 66–75, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nHang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoit Crabbé, Laurent Besacier, and Didier\nSchwab. 2020. FlauBERT: Unsupervised language\nmodel pre-training for French. In Proceedings of\nthe 12th Language Resources and Evaluation Con-\nference, pages 2479–2490, Marseille, France. Euro-\npean Language Resources Association.\nMike Lewis, Marjan Ghazvininejad, Gargi Ghosh, Ar-\nmen Aghajanyan, Sida Wang, and Luke Zettle-\nmoyer. 2020a. Pre-training via paraphrasing. arXiv\npreprint arXiv:2006.15020.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020b. BART: Denoising sequence-to-sequence\npre-training for natural language generation, trans-\nlation, and comprehension. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 7871–7880, Online. As-\nsociation for Computational Linguistics.\nPatrick Lewis, Barlas O ˘guz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2019. MLQA: Eval-\nuating cross-lingual extractive question answering.\narXiv preprint arXiv:1910.07475.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020a. Multilingual denoising\npre-training for neural machine translation. arXiv\npreprint arXiv:2001.08210.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nZihan Liu, Genta Indra Winata, Andrea Madotto, and\nPascale Fung. 2020b. Exploring ﬁne-tuning tech-\nniques for pre-trained cross-lingual models via con-\ntinual learning. arXiv preprint arXiv:2004.14218.\nFuli Luo, Wei Wang, Jiahao Liu, Yijia Liu, Bin Bi,\nSongfang Huang, Fei Huang, and Luo Si. 2020.\nVeco: Variable encoder-decoder pre-training for\ncross-lingual understanding and generation. arXiv\npreprint arXiv:2010.16046.\nMartin Malmsten, Love Börjeson, and Chris Haffenden.\n2020. Playing with words at the national library of\nsweden–making a swedish BERT. arXiv preprint\narXiv:2007.01658.\nLouis Martin, Benjamin Muller, Pedro Javier Or-\ntiz Suárez, Yoann Dupont, Laurent Romary, Éric\nde la Clergerie, Djamé Seddah, and Benoît Sagot.\n2020. CamemBERT: a tasty French language model.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n7203–7219, Online. Association for Computational\nLinguistics.\nBryan McCann, Nitish Shirish Keskar, Caiming Xiong,\nand Richard Socher. 2018. The natural language de-\ncathlon: Multitask learning as question answering.\narXiv preprint arXiv:1806.08730.\nSharan Narang, Colin Raffel, Katherine Lee, Adam\nRoberts, Noah Fiedel, and Karishma Malkan. 2020.\nWT5?! Training text-to-text models to explain their\npredictions. arXiv preprint arXiv:2004.14546.\nDat Quoc Nguyen and Anh Tuan Nguyen. 2020.\nPhoBERT: Pre-trained language models for Viet-\nnamese. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1037–1042,\nOnline. Association for Computational Linguistics.\nRodrigo Nogueira, Zhiying Jiang, Ronak Pradeep, and\nJimmy Lin. 2020. Document ranking with a pre-\ntrained sequence-to-sequence model. In Findings\nof the Association for Computational Linguistics:\nEMNLP 2020, pages 708–718, Online. Association\nfor Computational Linguistics.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers) , pages 1946–1958, Vancouver,\nCanada. Association for Computational Linguistics.\nJason Phang, Phu Mon Htut, Yada Pruksachatkun,\nHaokun Liu, Clara Vania, Katharina Kann, Iacer\nCalixto, and Samuel R Bowman. 2020. En-\nglish intermediate-task training improves zero-\nshot cross-lingual transfer too. arXiv preprint\narXiv:2005.13013.\nMarco Polignano, Pierpaolo Basile, Marco de Gem-\nmis, Giovanni Semeraro, and Valerio Basile. 2019.\nAlBERTo: Italian BERT language understanding\nmodel for NLP challenging tasks based on tweets.\nIn CLiC-it.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nSebastian Ruder, Matthew E. Peters, Swabha\nSwayamdipta, and Thomas Wolf. 2019. Trans-\nfer learning in natural language processing. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Com-\nputational Linguistics: Tutorials , pages 15–18,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nNoam Shazeer. 2020. GLU variants improve trans-\nformer. arXiv preprint arXiv:2002.05202.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30, pages 5998–6008.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzmán, Ar-\nmand Joulin, and Edouard Grave. 2020. CCNet:\nExtracting high quality monolingual datasets from\nweb crawl data. In Proceedings of the 12th Lan-\nguage Resources and Evaluation Conference, pages\n4003–4012, Marseille, France. European Language\nResources Association.\nYinfei Yang, Yuan Zhang, Chris Tar, and Jason\nBaldridge. 2019. PAWS-X: A cross-lingual ad-\nversarial dataset for paraphrase identiﬁcation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 3687–\n3692, Hong Kong, China. Association for Computa-\ntional Linguistics.\nISO Tokens Pages mT5 ISO Tokens Pages mT5\nCode Language (B) (M) (%) Code Language (B) (M) (%)\nen English 2,733 3,067 5.67 mk Macedonian 1.8 2.1 0.62\nru Russian 713 756 3.71 ml Malayalam 1.8 2.1 0.62\nes Spanish 433 416 3.09 mn Mongolian 2.7 2.1 0.62\nde German 347 397 3.05 ur Urdu 2.4 1.9 0.61\nfr French 318 333 2.89 be Belarusian 2.0 1.7 0.59\nit Italian 162 186 2.43 la Latin 1.3 1.7 0.58\npt Portuguese 146 169 2.36 eu Basque 1.4 1.6 0.57\npl Polish 130 126 2.15 tg Tajik 1.4 1.3 0.54\nnl Dutch 73 96 1.98 te Telugu 1.3 1.2 0.52\ntr Turkish 71 88 1.93 fy West Frisian 0.4 1.1 0.51\nja Japanese 164 87 1.92 kn Kannada 1.1 1.1 0.51\nvi Vietnamese 116 79 1.87 ky Kyrgyz 1.0 1.0 0.50\nid Indonesian 69 70 1.80 sw Swahili 1.0 1.0 0.50\ncs Czech 63 60 1.72 so Somali 1.4 0.9 0.48\nzh Chinese 39 55 1.67 my Burmese 0.9 0.8 0.47\nfa Persian 52 54 1.67 uz Uzbek 0.9 0.8 0.46\nar Arabic 57 53 1.66 km Khmer 0.6 0.8 0.46\nsv Swedish 45 49 1.61 - Russian (Latin) 0.9 0.7 0.46\nro Romanian 52 46 1.58 sd Sindhi 1.6 0.7 0.45\nel Greek 43 42 1.54 gu Gujarati 0.8 0.6 0.43\nuk Ukrainian 41 39 1.51 - Hindi (Latin) 0.6 0.6 0.43\nhu Hungarian 39 37 1.48 jv Javanese 0.3 0.6 0.42\nda Danish 29 29 1.38 zu Zulu 0.2 0.6 0.42\nﬁ Finnish 25 27 1.35 si Sinhala 0.8 0.5 0.41\nno Norwegian 27 25 1.33 - Japanese (Latin) 0.3 0.5 0.41\nbg Bulgarian 22 23 1.29 eo Esperanto 0.7 0.5 0.40\nhi Hindi 24 19 1.21 co Corsican 0.2 0.5 0.40\nsk Slovak 18 18 1.19 ga Irish 0.5 0.5 0.40\nko Korean 26 16 1.14 - Greek (Latin) 0.4 0.4 0.39\nth Thai 11 15 1.14 - Chinese (Latin) 0.2 0.4 0.37\nca Catalan 13 14 1.12 pa Punjabi 0.6 0.4 0.37\nms Malay 13 13 1.09 ceb Cebuano 0.2 0.4 0.36\niw Hebrew 17 12 1.06 mg Malagasy 0.2 0.3 0.36\nlt Lithuanian 11 11 1.04 ps Pashto 0.4 0.3 0.36\nsl Slovenian 8.8 8.5 0.95 sn Shona 0.2 0.3 0.35\nmr Marathi 14 7.8 0.93 gd Scottish Gaelic 0.4 0.3 0.35\nbn Bengali 7.3 7.4 0.91 ku Kurdish 0.4 0.3 0.34\net Estonian 6.9 6.9 0.89 hmn Hmong 0.2 0.3 0.34\nlv Latvian 7.0 6.4 0.87 su Sundanese 0.1 0.3 0.34\naz Azerbaijani 4.4 5.3 0.82 ht Haitian Creole 0.2 0.3 0.33\ngl Galician 2.4 4.6 0.79 ha Hausa 0.2 0.2 0.33\ncy Welsh 4.9 4.1 0.76 ny Chichewa 0.1 0.2 0.29\nsq Albanian 4.0 4.1 0.76 am Amharic 0.3 0.2 0.29\nta Tamil 3.4 3.5 0.73 - Bulgarian (Latin) 0.09 0.2 0.29\nsr Serbian 4.3 3.4 0.72 yi Yiddish 0.3 0.1 0.28\nne Nepali 3.2 2.9 0.69 lo Lao 0.1 0.1 0.28\nlb Luxembourgish 1.0 2.7 0.68 mi Maori 0.1 0.1 0.25\nhy Armenian 2.4 2.4 0.65 sm Samoan 0.09 0.1 0.25\nkk Kazakh 3.1 2.4 0.65 ig Igbo 0.09 0.09 0.24\nka Georgian 2.5 2.3 0.64 haw Hawaiian 0.09 0.08 0.24\nmt Maltese 5.2 2.3 0.64 xh Xhosa 0.06 0.07 0.22\naf Afrikaans 1.7 2.2 0.63 st Sotho 0.08 0.07 0.22\nﬁl Filipino 2.1 2.1 0.62 yo Yoruba 0.05 0.05 0.20\nis Icelandic 2.6 2.1 0.62\nTable 6: Statistics of the mC4 corpus, totaling 6.6B pages and 6.3T tokens. The “mT5” column indicates the\npercentage of mT5 training data coming from a given language, using the default exponential smoothing value of\nα=0.3. We list 107 “languages” as detected by cld3, but note six of these (marked “Latin”) are just Romanized\nvariants of existing languages.\nModel en ar bg de el es fr hi ru sw th tr ur vi zh avg\nCross-lingual zero-shot transfer (models ﬁne-tune on English data only)\nmBERT 80.8 64.3 68.0 70.0 65.3 73.5 73.4 58.9 67.8 49.7 54.1 60.9 57.2 69.3 67.8 65.4\nXLM 82.8 66.0 71.9 72.7 70.4 75.5 74.3 62.5 69.9 58.1 65.5 66.4 59.8 70.7 70.2 69.1\nXLM-R 88.7 77.2 83.0 82.5 80.8 83.7 82.2 75.6 79.1 71.2 77.4 78.0 71.7 79.3 78.2 79.2\nmT5-Small 79.6 65.2 71.3 69.2 68.6 72.7 70.7 62.5 70.1 59.7 66.3 64.4 59.9 66.3 65.8 67.5\nmT5-Base 84.7 73.3 78.6 77.4 77.1 80.3 79.1 70.8 77.1 69.4 73.2 72.8 68.3 74.2 74.1 75.4\nmT5-Large 89.4 79.8 84.1 83.4 83.2 84.2 84.1 77.6 81.5 75.4 79.4 80.1 73.5 81.0 80.3 81.1\nmT5-XL 90.6 82.2 85.4 85.8 85.4 81.3 85.3 80.4 83.7 78.6 80.9 82.0 77.0 81.8 82.7 82.9\nmT5-XXL 91.6 84.5 87.7 87.3 87.3 87.8 86.9 83.2 85.1 80.3 81.7 83.8 79.8 84.6 83.6 84.5\nTranslate-train (models ﬁne-tune on English training data plus translations in all target languages)\nmt5-Small 69.5 63.7 67.5 65.7 66.4 67.5 67.3 61.9 66.4 59.6 63.9 63.5 60.4 63.3 64.5 64.7\nmt5-Base 82.0 74.4 78.5 77.7 78.1 79.1 77.9 72.2 76.5 71.5 75.0 74.8 70.4 74.5 76.0 75.9\nmt5-Large 88.3 80.3 84.1 84.0 83.7 84.9 83.8 79.8 82.0 76.4 79.9 81.0 75.9 81.3 81.7 81.8\nmt5-XL 90.9 84.2 86.8 86.8 86.4 87.4 86.8 83.1 84.9 81.3 82.3 84.4 79.4 83.9 84.0 84.8\nmT5-XXL 92.7 87.2 89.4 89.8 89.5 90.0 89.1 86.5 87.6 84.3 85.6 87.1 83.8 87.5 86.5 87.8\nTable 7: XNLI accuracy scores for each language.\nModel en de es fr ja ko zh avg\nCross-lingual zero-shot transfer (models ﬁne-tune on English data only)\nmBERT 94.0 85.7 87.4 87.0 73.0 69.6 77.0 81.9\nXLM 94.0 85.9 88.3 87.4 69.3 64.8 76.5 80.9\nXLM-R 94.7 89.7 90.1 90.4 78.7 79.0 82.3 86.4\nmT5-Small 92.2 86.2 86.1 86.6 74.7 73.5 77.9 82.4\nmT5-Base 95.4 89.4 89.6 91.2 79.8 78.5 81.1 86.4\nmT5-Large 96.1 91.3 92.0 92.7 82.5 82.7 84.7 88.9\nmT5-XL 96.0 92.8 92.7 92.4 83.6 83.1 86.5 89.6\nmT5-XXL 96.3 92.9 92.6 92.7 84.5 83.9 87.2 90.0\nTranslate-train (models ﬁne-tune on English training data plus translations in all target languages)\nmT5-Small 87.9 81.4 83.1 84.1 74.2 71.7 76.7 79.9\nmT5-Base 95.5 90.9 91.4 92.5 83.6 84.8 86.4 89.3\nmT5-Large 96.4 92.7 93.3 93.6 86.5 87.4 88.4 91.2\nmT5-XL 96.4 92.5 93.1 93.6 85.5 86.9 89.0 91.0\nmT5-XXL 96.1 92.9 93.6 94.2 87.0 87.9 89.0 91.5\nTable 8: PAWS-X accuracy scores for each language.\nModel af ar bg bn de el en es et eu fa ﬁ fr he hi hu id it ja jv\nCross-lingual zero-shot transfer (models ﬁne-tune on English data only)\nmBERT 77.4 41.1 77.0 70.0 78.0 72.5 85.2 77.4 75.4 66.3 46.2 77.2 79.6 56.6 65.0 76.4 53.5 81.5 29.0 66.4\nXLM 74.9 44.8 76.7 70.0 78.1 73.5 82.6 74.8 74.8 62.3 49.2 79.6 78.5 57.7 66.1 76.5 53.1 80.7 23.6 63.0\nXLM-R 78.9 53.0 81.4 78.8 78.8 79.5 84.7 79.6 79.1 60.9 61.9 79.2 80.5 56.8 73.0 79.8 53.0 81.3 23.2 62.5\nmT5-Small 67.4 36.6 64.6 60.4 66.1 59.1 80.7 63.6 58.4 42.3 25.3 64.5 74.6 39.6 57.9 61.5 46.7 73.4 28.8 50.6\nmT5-Base 73.8 48.4 68.2 67.1 72.5 63.5 83.2 71.7 67.3 49.2 31.9 68.6 78.6 47.4 67.6 64.7 49.7 78.9 35.3 56.9\nmT5-Large 74.7 55.0 60.6 64.5 75.2 68.2 84.2 74.2 67.0 48.7 51.4 66.4 82.4 55.8 69.0 67.3 51.1 80.7 43.0 57.1\nmT5-XL 79.8 60.2 81.0 78.1 80.6 78.3 86.3 74.7 71.8 52.2 61.5 70.1 86.2 65.5 76.5 71.9 56.8 83.3 48.0 64.5\nmT5-XXL 80.4 66.2 85.1 79.3 81.7 79.0 86.7 86.0 73.5 57.6 58.8 70.4 86.8 65.1 77.8 74.2 73.5 85.8 50.7 66.4\nka kk ko ml mr ms my nl pt ru sw ta te th tl tr ur vi yo zh avg\nmBERT 64.6 45.8 59.6 52.3 58.2 72.7 45.2 81.8 80.8 64.0 67.5 50.7 48.5 3.6 71.7 71.8 36.9 71.8 44.9 42.7 62.2\nXLM 67.7 57.2 26.3 59.4 62.4 69.6 47.6 81.2 77.9 63.5 68.4 53.6 49.6 0.3 78.6 71.0 43.0 70.1 26.5 32.4 61.2\nXLM-R 71.6 56.2 60.0 67.8 68.1 57.1 54.3 84.0 81.9 69.1 70.5 59.5 55.8 1.3 73.2 76.1 56.4 79.4 33.6 33.1 65.4\nmT5-Small 53.2 23.4 26.6 39.4 39.4 70.0 30.1 75.4 70.8 46.5 54.8 37.5 32.6 7.2 69.4 56.0 26.4 63.8 58.8 37.9 51.0\nmT5-Base 50.1 23.4 33.9 48.2 43.8 72.6 37.0 80.1 76.0 55.4 62.4 41.2 42.7 9.5 74.6 58.4 38.4 73.0 59.3 41.5 56.6\nmT5-Large 58.2 23.3 36.2 46.3 46.5 69.4 32.2 82.7 79.6 50.2 72.4 46.4 44.5 10.5 79.0 65.1 44.2 77.1 48.4 44.0 58.8\nmT5-XL 66.0 31.6 38.1 54.1 57.6 74.8 42.6 85.7 85.2 66.9 72.8 49.0 54.7 9.6 84.1 67.7 64.7 79.6 59.9 54.4. 65.7\nmT5-XXL 66.0 38.7 43.5 54.5 63.1 77.6 44.7 87.7 86.9 72.0 72.9 56.5 59.5 10.4 85.2 71.4 80.7 84.6 70.0 56.8 69.2\nTable 9: WikiAnn NER F1 scores for each language.\nModel en ar de el es hi ru th tr vi zh avg\nCross-lingual zero-shot transfer (models ﬁne-tune on English data only)\nmBERT 83.5 / 72.2 61.5 / 45.1 70.6 / 54.0 62.6 / 44.9 75.5 / 56.9 59.2 / 46.0 71.3 / 53.3 42.7 / 33.5 55.4 / 40.1 69.5 / 49.6 58.0 / 48.3 64.5 / 49.4\nXLM 74.2 / 62.1 61.4 / 44.7 66.0 / 49.7 57.5 / 39.1 68.2 / 49.8 56.6 / 40.3 65.3 / 48.2 35.4 / 24.5 57.9 / 41.2 65.8 / 47.6 49.7 / 39.7 59.8 / 44.3\nXLM-R 86.5 / 75.7 68.6 / 49.0 80.4 / 63.4 79.8 / 61.7 82.0 / 63.9 76.7 / 59.7 80.1 / 64.3 74.2 / 62.8 75.9 / 59.3 79.1 / 59.0 59.3 / 50.0 76.6 / 60.8\nmT5-Small 78.5 / 66.1 51.4 / 34.0 63.8 / 45.9 53.8 / 33.4 67.0 / 50.3 47.8 / 34.5 50.5 / 30.1 54.0 / 44.5 55.7 / 38.9 58.1 / 41.3 58.9 / 48.7 58.1 / 42.5\nmT5-Base 84.6 / 71.7 63.8 / 44.3 73.8 / 54.5 59.6 / 35.6 74.8 / 56.1 60.3 / 43.4 57.8 / 34.7 57.6 / 45.7 67.9 / 48.2 70.7 / 50.3 66.1 / 54.1 67.0 / 49.0\nmT5-Large 88.4 / 77.3 75.2 / 56.7 80.0 / 62.9 77.5 / 57.6 81.8 / 64.2 73.4 / 56.6 74.7 / 56.9 73.4 / 62.0 76.5 / 56.3 79.4 / 60.3 75.9 / 65.5 77.8 / 61.5\nmT5-XL 88.8 / 78.1 77.4 / 60.8 80.4 / 63.5 80.4 / 61.2 82.7 / 64.5 76.1 / 60.3 76.2 / 58.8 74.2 / 62.5 77.7 / 58.4 80.5 / 60.8 80.5 / 71.0 79.5 / 63.6\nmT5-XXL 90.9 / 80.1 80.3 / 62.6 83.1 / 65.5 83.3 / 65.5 85.1 / 68.1 81.7 / 65.9 79.3 / 63.6 77.8 / 66.1 80.2 / 60.9 83.1 / 63.6 83.1 / 73.4 82.5 / 66.8\nTranslate-train (models ﬁne-tune on English training data plus translations in all target languages)\nmT5-Small 74.0 / 61.2 61.0 / 45.0 66.0 / 50.2 64.1 / 47.2 67.5 / 50.8 60.2 / 43.7 64.4 / 46.7 58.9 / 52.9 59.0 / 39.4 63.5 / 46.0 68.2 / 61.2 64.3 / 49.5\nmT5-Base 83.1 / 70.3 72.4 / 55.2 76.9 / 59.7 76.8 / 58.8 79.0 / 61.2 71.4 / 53.4 76.1 / 58.5 67.9 / 62.0 72.5 / 51.4 75.9 / 56.3 76.9 / 69.7 75.3 / 59.7\nmT5-Large 87.3 / 75.5 79.4 / 62.7 82.7 / 66.0 81.8 / 63.5 83.8 / 66.1 78.0 / 59.8 81.9 / 66.3 74.7 / 68.2 80.2 / 59.2 80.4 / 60.8 83.2 / 76.9 81.2 / 65.9\nmT5-XL 88.5 / 77.1 80.9 / 65.4 83.4 / 66.7 83.6 / 64.9 84.9 / 68.2 79.6 / 63.1 82.7 / 67.1 78.5 / 72.9 82.4 / 63.8 82.4 / 64.1 83.2 / 75.9 82.7 / 68.1\nmT5-XXL 91.3 / 80.3 83.4 / 68.2 85.0 / 68.2 85.9 / 68.9 87.4 / 70.8 83.7 / 68.2 85.2 / 70.4 80.2 / 74.5 84.4 / 67.7 85.3 / 67.1 85.7 / 80.0 85.2 / 71.3\nTable 10: XQuAD results (F1/EM) for each language.\nModel en ar de es hi vi zh avg\nCross-lingual zero-shot transfer (models ﬁne-tune on English data only)\nmBERT 80.2 / 67.0 52.3 / 34.6 59.0 / 43.8 67.4 / 49.2 50.2 / 35.3 61.2 / 40.7 59.6 / 38.6 61.4 / 44.2\nXLM 68.6 / 55.2 42.5 / 25.2 50.8 / 37.2 54.7 / 37.9 34.4 / 21.1 48.3 / 30.2 40.5 / 21.9 48.5 / 32.6\nXLM-R 83.5 / 70.6 66.6 / 47.1 70.1 / 54.9 74.1 / 56.6 70.6 / 53.1 74.0 / 52.9 62.1 / 37.0 71.6 / 53.2\nmT5-Small 77.2 / 63.0 44.7 / 27.3 53.3 / 35.7 60.1 / 41.5 43.0 / 29.2 52.9 / 33.2 51.3 / 29.7 54.6 / 37.1\nmT5-Base 81.7 / 66.9 57.1 / 36.9 62.1 / 43.2 67.1 / 47.2 55.4 / 37.9 65.9 / 44.1 61.6 / 38.6 64.4 / 45.0\nmT5-Large 84.9 / 70.7 65.3 / 44.6 68.9 / 51.8 73.5 / 54.1 66.9 / 47.7 72.5 / 50.7 66.2 / 42.0 71.2 / 51.7\nmT5-XL 85.5 / 71.9 68.0 / 47.4 70.5 / 54.4 75.2 / 56.3 70.5 / 51.0 74.2 / 52.8 70.5 / 47.2 73.5 / 54.4\nmT5-XXL 86.7 / 73.5 70.7 / 50.4 74.0 / 57.8 76.8 / 58.4 75.6 / 57.3 76.4 / 56.0 71.8 / 48.8 76.0 / 57.4\nTranslate-train (models ﬁne-tune on English training data plus translations in all target languages)\nmT5-Small 70.5 / 56.2 49.3 / 31.0 55.6 / 40.6 60.5 / 43.0 50.4 / 32.9 55.2 / 36.3 54.4 / 31.6 56.6 / 38.8\nmT5-Base 80.7 / 66.3 61.1 / 40.7 65.5 / 49.2 70.7 / 52.1 63.6 / 44.3 68.0 / 47.6 63.5 / 39.4 67.6 / 48.5\nmT5-Large 85.3 / 72.0 68.5 / 47.7 71.6 / 55.8 75.7 / 57.1 71.8 / 52.6 74.3 / 54.0 70.1 / 47.1 73.9 / 55.2\nmT5-XL 86.0 / 73.0 70.0 / 49.8 72.7 / 56.8 76.9 / 58.3 73.4 / 55.0 75.4 / 55.0 71.4 / 48.4 75.1 / 56.6\nmT5-XXL 86.5 / 73.5 71.7 / 51.4 74.9 / 58.7 78.8 / 60.3 76.6 / 58.5 77.1 / 56.3 72.5 / 49.8 76.9 / 58.3\nTable 11: MLQA results (F1/EM) for each language.\nModel en ar bn ﬁ id ko ru sw te avg\nCross-lingual zero-shot transfer (models ﬁne-tune on English data only)\nmBERT 75.3 / 63.6 62.2 / 42.8 49.3 / 32.7 59.7 / 45.3 64.8 / 45.8 58.8 / 50.0 60.0 / 38.8 57.5 / 37.9 49.6 / 38.4 59.7 / 43.9\nXLM 66.9 / 53.9 59.4 / 41.2 27.2 / 15.0 58.2 / 41.4 62.5 / 45.8 14.2 / 5.1 49.2 / 30.7 39.4 / 21.6 15.5 / 6.9 43.6 / 29.1\nXLM-R 71.5 / 56.8 67.6 / 40.4 64.0 / 47.8 70.5 / 53.2 77.4 / 61.9 31.9 / 10.9 67.0 / 42.1 66.1 / 48.1 70.1 / 43.6 65.1 / 45.0\nmT5-Small 53.9 / 43.6 41.1 / 26.0 18.9 / 13.3 39.2 / 22.6 44.4 / 31.7 24.9 / 16.3 40.5 / 24.3 34.8 / 21.2 16.9 / 11.5 34.9 / 23.4\nmT5-Base 71.8 / 60.9 67.1 / 50.4 40.7 / 22.1 67.0 / 52.2 71.3 / 54.5 49.5 / 37.7 54.9 / 32.6 60.4 / 43.9 40.6 / 31.1 58.1 / 42.8\nmT5-Large 71.6 / 58.9 60.5 / 40.4 42.0 / 23.9 64.6 / 48.8 67.0 / 49.2 47.6 / 37.3 58.9 / 36.8 65.7 / 45.3 41.9 / 29.7 57.8 / 41.2\nmT5-XL 80.3 / 70.9 81.7 / 65.5 74.5 / 57.5 79.4 / 65.3 83.5 / 70.4 70.0 / 60.5 71.6 / 47.8 77.3 / 59.7 77.9 / 55.8 77.4 / 61.5\nmT5-XXL 83.7 / 72.5 82.8 / 66.0 80.2 / 63.7 83.3 / 70.2 85.3 / 73.3 76.2 / 64.1 76.6 / 55.8 81.9 / 66.1 79.2 / 58.7 81.0 / 65.6\nTranslate-train (models ﬁne-tune on English training data plus translations in all target languages)\nmT5-Small 57.1 / 46.6 56.8 / 39.7 37.2 / 21.2 50.9 / 37.2 60.1 / 45.1 40.4 / 29.3 50.7 / 33.6 51.5 / 35.3 29.3 / 18.1 48.2 / 34.0\nmT5-Base 71.1 / 58.9 68.0 / 50.2 57.4 / 35.4 68.8 / 55.2 73.5 / 57.2 56.5 / 43.8 64.0 / 45.8 65.8 / 48.3 51.2 / 34.1 64.0 / 47.7\nmT5-Large 75.6 / 62.7 74.8 / 57.9 65.0 / 46.0 72.3 / 57.5 78.7 / 63.5 66.4 / 53.6 70.9 / 50.5 74.0 / 56.7 62.0 / 45.1 71.1 / 54.9\nmT5-XL 82.0 / 65.7 79.3 / 65.5 80.4 / 68.9 79.1 / 64.7 84.7 / 71.0 70.5 / 56.2 78.3 / 61.1 83.9 / 70.9 80.9 / 64.0 79.9 / 65.3\nmT5-XXL 83.3 / 71.6 83.0 / 66.3 82.3 / 70.8 82.9 / 67.8 86.6 / 72.0 75.0 / 62.3 80.7 / 63.1 86.9 / 75.8 84.6 / 69.2 82.8 / 68.8\nIn-language multitask (models ﬁne-tuned on gold data in all target languages)\nmT5-Small 66.4 / 56.1 80.3 / 68.7 71.7 / 60.2 71.9 / 59.5 78.8 / 67.6 55.5 / 46.7 70.1 / 57.1 77.7 / 68.9 82.7 / 71.6 73.0 / 62.0\nmT5-Base 76.6 / 65.2 84.2 / 71.8 80.0 / 69.0 80.1 / 69.3 85.5 / 75.0 70.3 / 61.6 77.5 / 64.4 83.6 / 74.9 88.2 / 78.0 80.8 / 70.0\nmT5-Large 82.4 / 70.9 87.1 / 75.1 86.3 / 78.8 85.5 / 73.4 87.3 / 77.9 79.1 / 69.9 84.3 / 71.3 87.4 / 79.6 90.2 / 81.2 85.5 / 75.3\nmT5-XL 84.1 / 74.3 88.5 / 76.0 87.7 / 80.5 87.4 / 76.1 89.9 / 81.2 82.8 / 75.4 84.9 / 73.2 90.1 / 82.8 92.0 / 83.7 87.5 / 78.1\nmT5-XXL 85.7 / 75.5 88.4 /76.9 88.7 / 80.5 87.5 / 76.3 90.3 / 81.8 83.7 / 75.7 87.9 / 76.8 91.9 / 84.4 92.6 / 83.9 88.5 / 79.1\nTable 12: TyDi QA GoldP results (F1/EM) for each language.\nModel ar bg de el en es fr hi ru sw th tr ur vi zh avg\nBaseline (mT5-large) 79.8 84.1 83.4 83.2 89.4 84.2 84.1 77.6 81.5 75.4 79.4 80.1 73.5 81.0 80.3 81.1\nDropout 0.1 76.4 82.1 81.7 81.0 88.0 70.8 80.3 74.4 79.0 72.3 75.8 75.9 70.6 78.6 76.5 77.6\nSequence length512 78.1 83.4 83.1 82.1 88.8 84.5 82.8 77.3 81.2 75.4 78.2 79.6 73.8 80.0 78.9 80.5\nSpan length10 77.6 81.5 80.5 81.2 87.2 83.0 81.2 74.7 79.8 73.6 76.7 75.9 71.3 78.6 76.5 78.6\n↵ =0 .7 79.3 84.1 84.5 83.1 89.4 85.3 84.4 76.4 82.8 70.6 78.7 79.8 71.7 80.3 79.9 80.7\n↵ =0 .2 78.7 83.8 83.3 82.5 89.3 83.4 83.6 77.3 81.2 75.4 78.6 79.4 73.9 79.9 79.7 80.7\nNo line length ﬁlter 78.4 83.3 81.5 81.4 88.9 83.8 82.5 74.4 80.5 69.4 77.6 76.9 71.3 78.8 78.3 79.1\nAdd Wikipedia data 79.3 83.1 83.1 82.7 88.6 80.1 83.2 77.3 81.4 75.0 78.9 79.3 73.5 80.2 79.2 80.3\nTable 13: XNLI zero-shot accuracy of various ablations on our mT5-Large model.\nModel en ar de el es hi ru th tr vi zh avg\nBaseline(mT5-large) 88.4 / 77.3 75.2 / 56.7 80.0 / 62.9 77.5 / 57.6 81.8 / 64.2 73.4 / 56.6 74.7 / 56.9 73.4 / 62.0 76.5 / 56.3 79.4 / 60.3 75.9 / 65.5 77.8 / 61.5\nSpan length10 88.1 / 76.3 70.0 / 50.6 78.1 / 60.2 68.8 / 44.0 79.0 / 60.8 67.3 / 48.4 65.4 / 43.3 68.1 / 57.2 74.4 / 53.6 77.9 / 57.7 76.6 / 66.4 74.0 / 56.2\nDropout 0.1 87.3 / 76.0 54.9 / 33.9 77.6 / 60.2 64.4 / 40.1 79.2 / 60.6 59.1 / 40.4 59.5 / 38.4 65.7 / 51.0 73.6 / 52.8 75.8 / 55.8 77.0 / 64.5 70.4 / 52.1\nSequence length512 88.0 / 76.9 77.0 / 59.6 80.2 / 62.4 79.8 / 60.0 81.7 /64.4 75.1 / 57.5 77.4 / 58.5 72.7 / 59.8 75.3 / 53.9 79.4 / 58.9 78.5 / 67.2 78.6 / 61.7\n↵ =0 .7 88.4 / 77.1 76.5 / 58.8 78.5 / 59.8 77.2 / 55.5 78.7 / 59.5 74.6 / 56.8 73.1 / 54.5 72.5 / 60.2 75.7 / 55.0 79.2 / 58.3 78.6 / 66.2 77.5 / 60.2\n↵ =0 .2 87.9 / 76.8 75.5 / 57.3 80.2 / 62.4 76.2 / 54.0 81.6 / 63.7 73.7 / 57.0 70.7 / 50.8 72.2 / 60.4 75.5 / 55.7 79.7 / 59.7 78.3 / 67.5 77.4 / 60.5\nNo line length ﬁlter 88.9 / 77.4 73.8 / 54.0 80.8 / 62.7 74.2 / 51.8 80.9 / 62.8 74.1 / 56.6 75.0 / 56.4 71.7 / 60.3 76.7 / 56.0 78.8 / 58.6 78.5 / 67.1 77.6 / 60.3\nAdd Wikipedia data 89.3 / 78.4 69.6 / 48.9 79.6 / 61.1 59.5 / 36.0 80.6 / 61.0 73.6 / 55.0 68.7 / 47.0 70.5 / 58.1 76.7 / 56.9 78.6 / 56.4 77.5 / 66.3 74.9 / 56.8\nTable 14: XQuAD zero-shot F1/EM of various ablations on our mT5-Large model."
}