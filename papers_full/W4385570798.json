{
  "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
  "url": "https://openalex.org/W4385570798",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2606960406",
      "name": "Zichun Yu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2226924701",
      "name": "Chenyan Xiong",
      "affiliations": [
        "Microsoft (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2099988764",
      "name": "SHI Yu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2051269448",
      "name": "Zhiyuan Liu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3083182073",
    "https://openalex.org/W4287891464",
    "https://openalex.org/W3118668786",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3156785025",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W3206786886",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4319792222",
    "https://openalex.org/W4385571865",
    "https://openalex.org/W4385572608",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W3121904249",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4318719006",
    "https://openalex.org/W4312091849",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4313483736",
    "https://openalex.org/W4297899309",
    "https://openalex.org/W3001279689",
    "https://openalex.org/W4226082499",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W4320813768",
    "https://openalex.org/W3169283738",
    "https://openalex.org/W4287185415",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4385570569",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3156789018",
    "https://openalex.org/W4293565699",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4301243929",
    "https://openalex.org/W2951534261",
    "https://openalex.org/W4321472314",
    "https://openalex.org/W4385574243",
    "https://openalex.org/W4224875176"
  ],
  "abstract": "Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM's preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 2421–2436\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nAugmentation-Adapted Retriever Improves Generalization of Language\nModels as Generic Plug-In\nZichun Yu1 Chenyan Xiong2 Shi Yu1 Zhiyuan Liu13\n1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China\n2Microsoft Research, Redmond, USA\n3Beijing National Research Center for Information Science and Technology, Beijing, China\n{yuzc19, yus21}@mails.tsinghua.edu.cn; chenyan.xiong@microsoft.com\nliuzy@tsinghua.edu.cn\nAbstract\nRetrieval augmentation can aid language\nmodels (LMs) in knowledge-intensive tasks\nby supplying them with external information.\nPrior works on retrieval augmentation usually\njointly fine-tune the retriever and the LM,\nmaking them closely coupled. In this paper, we\nexplore the scheme of generic retrieval plug-in:\nthe retriever is to assist target LMs that may\nnot be known beforehand or are unable to\nbe fine-tuned together. To retrieve useful\ndocuments for unseen target LMs, we propose\naugmentation-adapted retriever (AAR), which\nlearns LM’s preferences obtained from a\nknown source LM. Experiments on the MMLU\nand PopQA datasets demonstrate that our AAR\ntrained with a small source LM is able to signif-\nicantly improve the zero-shot generalization of\nlarger target LMs ranging from 250M Flan-T5\nto 175B InstructGPT. Further analysis indicates\nthat the preferences of different LMs overlap,\nenabling AAR trained with a single source\nLM to serve as a generic plug-in for various\ntarget LMs. Our code is open-sourced at\nhttps://github.com/OpenMatch/Augmentation-\nAdapted-Retriever.\n1 Introduction\nLarge language models (LMs) that possess bil-\nlions of parameters are able to capture a signif-\nicant amount of human knowledge, leading to\nconsistent improvements on various downstream\ntasks (Brown et al., 2020; Kaplan et al., 2020;\nRoberts et al., 2020). However, the undeniable\ndrawback of large LMs lies in their high compu-\ntational cost, which negatively impacts their effi-\nciency (Strubell et al., 2019; Bender et al., 2021).\nFurthermore, the knowledge memorized from pre-\ntraining and the implicit reasoning process of LMs\ncan be inaccurate and intractable sometimes, hin-\ndering their applications on knowledge-intensive\ntasks (Guu et al., 2020; Lewis et al., 2020; Mallen\net al., 2022; Wei et al., 2022).\nFlan-T5Base\n(250M)\nFlan-T5Large\n(780M)\nFlan-T5XL\n(3B)\nInstructGPT\n(175B)\n# Parameters\n35\n40\n45\n50\n55\n60\n65MMLU Accuracy\nStandalone LM\nLM w/ Few-Shot Prompting\nLM w/ Adaptive Retrieval\nLM w/ AAR (Ours)\nFigure 1: Performance of LM w/ AAR (Ours).\nInstead of leveraging the knowledge and rea-\nsoning abilities embedded within the parameters\nof the LMs, retrieval augmentation (Guu et al.,\n2020; Lewis et al., 2020; Borgeaud et al., 2022)\nenhances the LM with a retriever that can retrieve\nknowledge from an external corpus. On the other\nhand, prior retrieval augmentation methods (Izac-\nard and Grave, 2021a; Izacard et al., 2022) necessi-\ntate fine-tuning the backbone LM to adjust to the\nretriever and tackle specific downstream tasks. This\nkind of fine-tuning can be expensive when more\nand more unique demands emerge (Maronikolakis\nand Schütze, 2021). More importantly, many top-\ntier LMs can only be accessed through black-box\nAPIs (Ouyang et al., 2022; OpenAI, 2023). These\nAPIs allow users to submit queries and receive re-\nsponses but typically do not support fine-tuning.\nIn this paper, we introduce Augmentation-\nAdapted Retriever (AAR) to assist black-box LMs\nwith downstream tasks as generic plug-in. To re-\ntrieve valuable documents for many unseen LMs,\nwe propose to leverage a small source LM to pro-\nvide LM-preferred signals for retriever’s training.\nThe retriever after training (i.e., AAR) can be di-\nrectly utilized to assist a large target LM by plug-\nging in the retrieved documents.\nSpecifically, we choose a small encoder-decoder\nLM as the source LM and utilize its fusion-\n2421\nin-decoder attention scores (Izacard and Grave,\n2021a) to annotate LM-preferred documents. The\nLM-preferred documents are then combined with\nhuman-preferred documents to form the positive\ndocument set. Negative documents are mined by\nthe retriever itself using the ANCE (Xiong et al.,\n2021) technique. After fine-tuning the retriever\nwith LM’s preferences, it can directly assist unseen\ntarget LMs in the zero-shot task generalization.\nWe evaluate AAR on a multi-task language\nunderstanding dataset MMLU (Hendrycks et al.,\n2021) and an entity-centric question answering\ndataset PopQA (Mallen et al., 2022). For the tar-\nget LMs, we choose Flan-T5 (Chung et al., 2022)\nseries as our backbone for encoder-decoder LMs\nand InstructGPT (Ouyang et al., 2022) as our back-\nbone for decoder-only LMs. Figure 1 shows that\nassisted with a generic AAR, LMs of different sizes\nand architectures can consistently outperform the\nstandalone LMs; the performance of smaller LMs\ncan sometimes surpass the standalone counterparts\nof significantly larger sizes (e.g., Flan-T5 Large w/\nAAR outperforms standalone Flan-T5XL by 0.6%).\nAAR also demonstrates advantages over other aug-\nmentation approaches such as few-shot prompting\nand adaptive retrieval (Mallen et al., 2022).\nFurther analysis reveals that the preferences ob-\ntained from different-sized source LMs are similar,\nand LMs with near capacities tend to yield closer\npreferred document sets. As a result, our AAR\nmodel trained from a small source LM can be con-\nsidered as a generic plug-in to enhance the zero-\nshot generalization of a significantly larger target\nLM. We also discover that the documents preferred\nby LMs can provide assistance to the model from\nalternative perspectives, rather than relying solely\non the full information favored by search users.\n2 Related Work\nRetrieval Augmentation.Augmenting LMs with\nretrieved information from external memories has\nshown effective on diverse knowledge-intensive\ntasks (Guu et al., 2020). Prior works explore\nnovel ways to train the whole retriever-LM sys-\ntem in an end-to-end fashion, using retrieval-\naugmented sequence log-likelihood (Lewis et al.,\n2020; Borgeaud et al., 2022), fusion-in-decoder\nattention distillation (Izacard and Grave, 2021a;\nIzacard et al., 2022), or knowledge graph (Ju et al.,\n2022). To decouple the retriever from LM, Rubin\net al. (2022) train an independent prompt retriever\nfor in-context learning, and Lin et al. (2022) only\nfine-tune the LM via the retrieved data that is simi-\nlar to few-shot unsupervised samples.\nRecent researches adopt zero-shot retrieval aug-\nmentation that does not fine-tune the LM on In-\nstructGPT (Ouyang et al., 2022). It can benefit\nentity-centric question answering (Mallen et al.,\n2022), chain-of-thought reasoning (He et al., 2022),\nand multi-hop question answering (Khattab et al.,\n2022). Parallel work (Shi et al., 2023) uses LM\nlikelihood to train the retriever for satisfying black-\nbox LM’s preferences, and they adopt GPT-3\nCurie (Brown et al., 2020) to provide the super-\nvision signals. In this work, we devise the retriever\nthat can be used as a generic plug-in to assist a\nvariety of unseen LMs.\nZero-shot Learning and Reasoning. Large-\nscale unsupervised pre-trained LMs like GPT-\n3 (Brown et al., 2020), GPT-4 (OpenAI, 2023),\nand PaLM (Chowdhery et al., 2022) are able to\nperform zero-shot learning on many downstream\ntasks with a task description provided at inference\ntime. Instruction-finetuned LMs (Sanh et al., 2022;\nChung et al., 2022; Ouyang et al., 2022), which\nare pre-trained on multiple supervised tasks using\nhuman instructions, also also exhibit robust zero-\nshot learning capabilities. Yu et al. (2023) pro-\npose a new scheme of zero-shot reasoning, which\nfirst prompts large LMs to generate relevant docu-\nments and then perform reading comprehension\non the generated contents. Recently, there has\nbeen a growing trend of utilizing plug-and-play\nknowledge injection to enhance the zero-shot per-\nformance of LMs, which is achieved through map-\nping network (Zhang et al., 2023) or document\nencoding (Xiao et al., 2023). Our work improves\nthe zero-shot generalization of LMs by utilizing the\nretrieved information. We demonstrate that identi-\nfying LMs’ preferences to train the retriever can in\nturn bring additional evidence texts for LMs.\n3 Method\nIn this section, we first introduce the preliminaries\nof the dense retrieval and the retrieval-augmented\nLM (§ 3.1), then propose our augmentation-\nadapted retriever (§ 3.2).\n3.1 Preliminaries\nRetrieval-augmented LM (Guu et al., 2020; Lewis\net al., 2020) is a type of LM that leverages external\ninformation to improve its performance. It retrieves\n2422\nrelevant documents from a corpus using a retriever,\nand then utilizes the documents to enhance its lan-\nguage generation capabilities.\nThe objective of the retriever is to find an aug-\nmentation document set Da from a corpus C that\nhelps the LM handle a given query q. Previous\nresearches (Karpukhin et al., 2020; Xiong et al.,\n2021) concentrate primarily on the dense retrieval\nsystem that searches in the dense vector space since\ndense retrieval usually performs more accurately\nand efficiently than sparse one.\nA dense retrieval model first represents q and\nthe document d into an embedding space using a\npre-trained encoder g,\nq = g(q); d = g(d), d∈C, (1)\nand match their embeddings by dot product func-\ntion f, which supports fast approximate nearest\nneighbor search (ANN) (André et al., 2016; John-\nson et al., 2021). We then define Da that contains\ntop-N retrieved documents as:\nDa = {da\n1 . . . da\nN }= ANNN\nf(q,◦). (2)\nFor the LM backbones, the decoder-only and\nthe encoder-decoder models are the two primary\nchoices of the retrieval-augmented LMs (Izacard\nand Grave, 2021b; Yu et al., 2023).\nGiven a decoder-only LM like GPT-3 (Brown\net al., 2020), the LM input can be a simple concate-\nnation of the query and all the augmentation docu-\nments {da\n1 . . . da\nN }. Then, the LM will generate the\nanswer based on the inputs auto-regressively.\nFor an encoder-decoder LM like T5 (Raffel et al.,\n2020), taking simple concatenation as the encoder\ninput may still be effective. However, this method\nmay not scale to a large volume of documents due\nto the quadratic self-attention computation associ-\nated with the number of documents. To aggregate\nmultiple documents more efficiently, Izacard and\nGrave (2021b) propose the fusion-in-decoder (FiD)\nmechanism, which soon becomes the mainstream\nin the development of encoder-decoder retrieval-\naugmented LMs. It first encodes each concatena-\ntion of the (da\ni , q) pair separately and then lets the\ndecoder attend to all parts:\nFiD(q) =Dec(Enc(da\n1 ⊕q) . . .Enc(da\nN ⊕q)). (3)\nIn this way, the encoder computes self-attention\nover one document at a time so that the compu-\ntational cost can grow linearly with the number\nof documents. Furthermore, FiD cross-attention\nis found effective in estimating the relative im-\nportance of the augmentation documents from\nNegatives  \nANCE Sampling Positives  \nGround Truth  \nTop-K FiDAtt \n  \n  \n \nPre-Trained Retriever\nQ + D1 ...\nEnc Enc\nDec\nEnc...\nSource LMFusion-in-Decoder\nRetrieve \nN Docs \nSource Task Q + D2Q + DN\nAugmentation-Adapted Retriever\nTarget LMsTarget TasksGeneric  \nPlug-In \nFigure 2: Illustration of augmentation-adapted retriever.\nthe LM’s perspective (Izacard and Grave, 2021a).\nTherefore, soft FiD distillation (Izacard and Grave,\n2021a; Izacard et al., 2022; Shi et al., 2023), which\nminimizes the KL-divergence between retrieval\nlikelihood and LM likelihood, is often used to train\nthe retriever and the LM end-to-end.\n3.2 Augmentation-adapted Retriever\nDue to the emerging real-world demands and\nthe limitations of black-box APIs, fine-tuning\nretrieval-augmented LM for each possible down-\nstream task can be infeasible. Hence, we intro-\nduce Augmentation-Adapted Retriever (AAR) as a\ngeneric plug-in for black-box LMs. As illustrated\nin Figure 2, AAR can learn the preferences of LMs\nwithout the need for fine-tuning them.\nSpecifically, we utilize an encoder-decoder LM\nas source LM (Ls) to provide LM-preferred signals\non a source task (Ts) for fine-tuning a pre-trained\nretriever. Then, we plug the fine-tuned retriever\ninto unseen target LM (Lt) on a set of target tasks\n(Tt) non-intersecting with Ts.\nOur training method starts from a source task Ts,\nwhere we aggregate the source LM Ls’s average\nFiD cross-attention (FiDAtt) scores Sa\ni correspond-\ning to document da\ni from the first decoder token\nover all the layers, all the heads and all the input\ntokens t of da\ni ⊕q:\nSa\ni = 1\nln ∗hn ∗tn\n∑\nlayers\n∑\nheads\n∑\nt∈da\ni ⊕q\nFiDAtt(FiD(q)). (4)\nwhere ln, hn, tn are the numbers of the layers, the\nheads and the input tokens.\nTo make the training process more robust, we uti-\nlize the FiDAtt scores to annotate the LM-preferred\npositive documents in a discrete way:\nDa+ = Dh+ ∪Top-KSa\ni ,Da , (5)\n2423\nwhere Dh+ is the human-preferred positive doc-\nument set (i.e., ground truth) on Ts. Top-KSa\ni ,Da\nmeans the documents with the top-k average Fi-\nDAtt scores Sa\ni in the retrieved document set Da.\nThen, we sample hard negatives following\nANCE (Xiong et al., 2021) and formulate the train-\ning loss Lof the retriever as:\nD−= ANNM\nf(q,◦)\\Da+, (6)\nL=\n∑\nq\n∑\nd+∈Da+\n∑\nd−∈D−\nl(f(q, d+), f(q, d−)), (7)\nwhere M is the hyperparameter of the negative\nsampling depth and l is the standard cross entropy\nloss. After fine-tuning the retriever, we directly use\nit to augment unseen target LM Lt on each task\nfrom target task set Tt.\n4 Experimental Methodologies\nIn this section, we discuss our main experimental\nsetup. More details can be found in Appendix A.\n4.1 Target Tasks\nFollowing prior works (Chung et al., 2022; Mallen\net al., 2022), we choose MMLU (Hendrycks et al.,\n2021) and PopQA (Mallen et al., 2022) as target\ntasks Tt.\nMMLU is a multitask language understanding\ndataset, which includes 57 multi-choice question\nanswering subtasks. These subtasks can be gen-\nerally classified into four categories: humanities,\nsocial sciences, STEM, and other. We average the\naccuracy of the subtasks in each category to ob-\ntain the final score. We report the accuracy of the\nevaluation set in our main experiments.\nPopQA is an entity-centric question answering\ndataset concentrated on long-tail questions. We\nreport the test accuracy in our main experiments.\n4.2 Our Method\nRetrievers. We adopt two widely used retriev-\ners to initialize AAR: ANCE initialized from\nT5Base (Raffel et al., 2020; Ge et al., 2023) and\nContriever (Izacard et al., 2021) initialized from\nBERTBase (Devlin et al., 2019). Both of them have\nbeen fine-tuned on MS MARCO (Bajaj et al., 2016)\npreviously. For the retrieval corpus, we choose the\nMS MARCO (Bajaj et al., 2016) for MMLU and\nthe KILT-Wikipedia (Petroni et al.) for PopQA.\nLanguage Models. We adopt Flan-T5 (Chung\net al., 2022) series as our backbone for encoder-\ndecoder LMs and InstructGPT 1 (Ouyang et al.,\n2022) as our backbone for decoder-only LMs.\nThese models have been multi-task instruction-\nfinetuned and are widely utilized for assessing zero-\nshot generalization (Zhou et al., 2023).\nImplementation Details.MSMARCO QA (Bajaj\net al., 2016) is our source task Ts. It is the com-\nmon choice to train the retriever (Xin et al., 2022).\nThis dataset consists of high-quality questions that\nrequire real-world knowledge to answer, which\naligns strongly with our target tasks Tt and pos-\nsesses no overlap with them. Considering the im-\nplementation efficiency, we take the Flan-T5Base as\nthe source LM Ls and treat the larger model as the\ntarget LM Lt. We directly set the total document\nnumber N = 10, LM-preferred document num-\nber K = 2, and negative mining depth M = 100\nin the augmentation-adapted training. We run all\nexperiments on a single A100-40G GPU.\n4.3 Baselines\nZero-shot Setting.We compare our method with\nthe state-of-the-art zero-shot baselines. Standalone\nLMs, including Flan-T5 (Chung et al., 2022), In-\nstructGPT (Ouyang et al., 2022), GAL (Taylor\net al., 2022) and OPT-IML-Max (Iyer et al., 2022),\nare prompted by a natural language instruction that\ndescribes the desired task and question. Adaptive\nretrieval (Mallen et al., 2022) selectively utilizes\nnon-parametric memory (retrieval augmentation)\nand parametric memory (the knowledge obtained\nfrom pre-training) based on questions’ popularity.\nIn our main experiment, we select the optimal com-\nbination in their paper, which consists of Contriever\nas the non-parametric memory and GenRead (Yu\net al., 2023) as the parametric memory.\nFew-shot Setting.We also include the results of\nprevious few-shot models for reference. Flan-T5,\nInstructGPT, Chinchilla (Hoffmann et al., 2022)\nand OPT-IML-Max adopt few-shot demonstrations,\nwhich provide the LMs with a limited number of\ntask examples. This enables the models to gener-\nalize from these examples and generate accurate\nresponses (Gao et al., 2021). Atlas (Izacard et al.,\n2022) is a state-of-the-art retrieval-augmented LM,\nwhich jointly pre-trains the retriever with the LM\nusing unsupervised data and fine-tunes the retriever\nvia the attention distillation on few-shot data.\n1We use the GPT-3text-davinci-002 December 2022 version.\n2424\nSettings Methods # Parameters MMLU PopQA\nAll Hum. Soc. Sci. STEM Other All\nBase Setting:T5 Base Size\nFew-shot Flan-T5Base(Chung et al., 2022) 250M 35.8 39.6 39.8 26.3 41.2 8.0\nZero-shot\nFlan-T5Base 250M 36.1 40.4 39.8 27.0 40.6 8.8\nFlan-T5Basew/ AR (Mallen et al., 2022) 250M 42.8 43.5 44.0 35.8 50.0 29.4\nFlan-T5Basew/ AARContriever(Ours) 250M 44.4 44.7 47.7 35.8 52.2 31.9\nFlan-T5Basew/ AARANCE(Ours) 250M 44.8 42.2 46.4 39.0 53.2 37.7\nLarge Setting:T5 Large Size\nFew-shot AtlasLargeFT (Izacard et al., 2022) 770M 38.9 37.3 41.7 32.3 44.9 n.a.\nFlan-T5Large 780M 45.1 47.7 53.5 34.4 49.2 9.3\nZero-shot\nFlan-T5Large 780M 44.8 46.3 51.4 34.8 50.6 7.2\nFlan-T5Largew/ AR 780M 49.8 50.0 55.6 38.4 59.5 29.6\nFlan-T5Largew/ AARContriever(Ours) 780M 51.8 50.8 59.7 39.4 61.8 33.4\nFlan-T5Largew/ AARANCE(Ours) 780M 50.4 48.0 58.1 39.3 60.2 39.3\nXL Setting:T5 XL Size\nFew-shot AtlasXLFT 3B 42.3 40.0 46.8 35.0 48.1 n.a.\nFlan-T5XL 3B 51.6 55.0 61.1 36.8 59.5 11.1\nZero-shot\nFlan-T5XL 3B 51.2 55.5 57.4 38.1 58.7 11.3\nFlan-T5XLw/ AR 3B 55.5 56.7 64.5 43.0 62.6 33.7\nFlan-T5XLw/ AARContriever(Ours) 3B 56.7 57.7 65.4 43.6 65.1 31.5\nFlan-T5XLw/ AARANCE(Ours) 3B 56.2 59.4 64.8 41.5 64.9 38.0\nGiant Setting:Over 70B Size\nFew-shot\nChinchilla (Hoffmann et al., 2022) 70B 67.5 63.6 79.3 55.0 73.9 n.a.\nOPT-IML-Max (Iyer et al., 2022) 175B 47.1 n.a. n.a. n.a. n.a. n.a.\nInstructGPT (Ouyang et al., 2022) 175B 60.5 62.0 71.8 44.3 70.1 35.2\nZero-shot\nGAL (Taylor et al., 2022) 120B 52.6 n.a. n.a. n.a. n.a. n.a.\nOPT-IML-Max 175B 49.1 n.a. n.a. n.a. n.a. n.a.\nInstructGPT 175B 60.2 65.7 68.0 46.1 66.5 34.7\nInstructGPT w/ AR 175B 60.5 62.2 71.3 44.7 69.7 43.3\nInstructGPT w/ AARContriever(Ours) 175B 61.5 64.5 73.1 45.0 69.9 43.9\nInstructGPT w/ AARANCE(Ours) 175B 62.2 62.0 72.0 49.2 70.7 52.0\nTable 1: Main results on MMLU and PopQA. We group the methods by the parameters. Our Ls is Flan-T5Base.\nAARContriever: AAR initialized from Contriever; AAR ANCE: AAR initialized from ANCE; FT: fine-tuning; AR:\nadaptive retrieval. Unspecified methods represent direct prompting. The score marked in bold represents the highest\nperformance achieved among the models in the zero-shot setting.\n2.5 5.0 7.5\nTraining FLOPs 1e21\n30\n35\n40\n45\n50\n55\n60MMLU Accuracy\nAARANCE\nLs=Flan-T5Base, Lt=Flan-T5Base\nAARANCE\nLs=Flan-T5Base, Lt=Flan-T5Large\nAARANCE\nLs=Flan-T5Base, Lt=Flan-T5XL\nAARANCE\nLs=Lt=Flan-T5Large\nAARANCE\nLs=Lt=Flan-T5XL\nAtlasLarge\nAtlasXL\nFigure 3: Training FLOPs of AARANCE and Atlas.\n5 Evaluation Results\nIn this section, we discuss our main results on\nMMLU and PopQA datasets (§ 5.1) and conduct\ncomprehensive studies about how (§ 5.2, § 5.3,\n§ 5.4) and when (§ 5.5, § 5.6) AAR helps.\n5.1 Overall Performance\nTable 1 demonstrates that, with the assistance of a\ngeneric AAR, target LMs of different sizes and\narchitectures can significantly outperform their\nstandalone baselines in the zero-shot setting. No-\ntably, AAR even improves powerful InstructGPT\nby 2% on MMLU and by nearly 20% on PopQA.\nWe hypothesize that the PopQA dataset mainly\ncomprises long-tail questions and thus necessitates\nmore augmentation information to attain high accu-\nracy. AAR outperforms other augmentation meth-\nods like few-shot prompting and adaptive retrieval,\nas they may not offer as extensive evidence text as\nAAR does.\nMeanwhile, AAR is a highly efficient augmenta-\ntion approach since it only relies on a small source\nLM Flan-T5Base (250M) to provide training signals\nand can generalize well to target LMs of larger ca-\npacities. Figure 3 illustrates that solely setting the\n2425\n250M 780M 3B 175B\n# Parameters\n40\n45\n50\n55\n60\n65\n70\n75MMLU Accuracy\nANCE\nAARANCE\nContriever\nAARContriever\n(a) Pre-trained retrievers.\n250M 780M 3B 175B\n# Parameters\n40\n45\n50\n55\n60\n65\n70\n75MMLU Accuracy\nHuman\nLs=Flan-T5Base LM\nLs=Flan-T5Base\nLs=Flan-T5Large (b) Positive docs selection.\nFigure 4: AAR’s performance when (a) using differ-\nent pre-trained retrievers and (b) trained with different\npositive documents, using Flan-T5 Base (250M), Flan-\nT5Large (780M), Flan-T5XL (3B), InstructGPT (175B)\nas Lt. The retriever in (b) is initialized from ANCE.\nsource LM as the target LM (represented by the in-\nverted triangles) does not significantly enhance the\nMMLU accuracy. However, it may triple the train-\ning budget required. Only using a small source LM\nis able to outperform the powerful Atlas by large\nmargins with fewer training FLOPs.\n5.2 Ablation Study\nIn this experiment, we conduct the ablation study of\naugmentation-adapted training and analyze model\nbehaviors during the training process.\nFigure 4a illustrates that augmentation-adapted\ntraining can bring additional improvements com-\npared to the pre-trained retrievers. In general,\nANCE benefits more from augmentation-adapted\ntraining than Contriever. This may be due to the\nfact that Contriever has been already intensively\npre-trained on massive data augmentations as well\nas MS MARCO whereas ANCE is trained only on\nMS MARCO. We provide exact numbers in Table 7\nand PopQA results in Figure 8, which yield similar\nobservations as MMLU.\nIn Figure 4b, we compare retrievers trained with\ndifferent positive documents, including human-\npreferred documents annotated by search users (the\nblue bar), LM-preferred documents obtained by\nthe source LM (the orange bar), and their combi-\nnations (the green bar and the red bar). Since the\nretriever has been pre-trained on user-annotated\nMS MARCO, simply using human-preferred docu-\nments to train it may be meaningless and therefore\nperforms the worst among all approaches. Only\nusing LM-preferred documents demonstrates no-\ntable gains over only using human-preferred doc-\numents, and merging both human-preferred and\nLM-preferred documents (our main setup) further\nenhances the retriever’s performance. Finally, us-\n0 10K 30K 50K 70K\nTraining step\n1.0\n1.1\n1.2\n1.3\n1.4MS MARCO Loss 28\n30\n32\n34\n36\nMS MARCO MRR@10\nTrain Loss\nEval Loss\nMRR@10\n(a) Retriever’s performance.\n0 10K 30K 50K 70K\nTraining step\n42\n43\n44\n45\n46\n47MMLU Accuracy\nMMLU\n28.0\n28.4\n28.8\n29.2\n29.6\n30.0\nMSMARCO QA Rouge-L\nMSMARCO QA (b) Lt’s performance.\nFigure 5: AAR’s training process. (a) exhibits the re-\ntriever’s (ANCE) performance on MS MARCO. (b)\npresents the Lt’s (Flan-T5Base) performance on MS-\nMARCO QA and MMLU.\nBase Large XL Human\nBase\nLarge\nXL\nHuman\n100.0%\n60.6%\n55.2%\n13.2%\n60.6%\n100.0%\n69.2%\n13.3%\n55.2%\n69.2%\n100.0%\n13.1%\n13.2%\n13.3%\n13.1%\n100.0%\n 0.2\n0.4\n0.6\n0.8\n1.0\nSet Overlap\n(a) Positive docs overlap.\n0\n10\n20\n30\n40\n50\n60MSMARCO QA Rouge-L\nStandalone LM\nHuman\nHuman (Ans-Deletion)\nLM\nLM (Ans-Deletion) (b) Answer-deletion test.\nFigure 6: Analysis of LM-preferred documents. (a)\nshows the overlaps of positive document sets, where\nused LMs are Flan-T5 series. (b) presents the answer-\ndeletion experiments on the MSMARCO QA dataset.\nThe retriever is initialized from ANCE.\ning Flan-T5Base as source LM yields better results\ncompared to using Flan-T5 Large when the target\nLMs are relatively small. However, as the target\nLM’s size increases, both approaches achieve com-\nparable performance. Hence, our choice to utilize\na small source LM in the augmentation-adapted\ntraining is reasonable and effective.\nFigure 5a and Figure 5b plot the retriever’s and\nLM’s performance during augmentation-adapted\ntraining, respectively. At the beginning of the train-\ning, the retriever’s MRR@10 on the MS MARCO\ndrops dramatically, indicating a large distribution\ngap between human-preferred and LM-preferred\ndocuments. As the retriever’s train and dev loss\ncontinually decline, the retrieval-augmented LM\ngradually performs better on MSMARCO QA and\neventually, on MMLU. This result implies that LMs\non different task may share common preferences,\nmaking AAR generalize well from single source\ntask to heterogeneous target tasks.\n5.3 Analysis of LM-preferred Documents\nWe highlight the necessity of adapting existing re-\ntrievers to LMs by comparing the preferred docu-\n2426\nQuestion Human-preferred Document LM-preferred Document\nwhat happens if you miss\nyour cruise ship\nIf you do miss the ship, go into the\ncruise terminal and talk with the port\nagents, who are in contact with both\nshipboard and shoreside personnel.\nThey can help you decide the best way\nto meet your ...\nThe cruise line is not financially respon-\nsible for getting passengers to the next\nport if they miss the ship.Your travel to\nthe subsequent port, or home, is on your\ndime, as are any necessary hotel stays\nand meals...\nwhat is annexation? Annexation is an activity in which two\nthings are joined together, usually with\na subordinate or lesser thing being at-\ntached to a larger thing.In strict legal\nterms, annexation simply involves...\nAnnexation (Latin ad, to, and nexus,\njoining) is the administrative action and\nconcept in international law relating to\nthe forcible transition of one state’s ter-\nritory by another state. It is generally\nheld to be an illegal act...\nTable 2: Case study on MSMARCO QA. We show the human-preferred documents and the Top-1 LM-preferred\ndocuments. Red texts are gold answer spans. Green texts are related spans covering other aspects of the question.\nments between search users and LMs. In general,\nwe discover that LM-preferred documents can as-\nsist LM from alternative perspectives rather than\nthe full information favored by search users.\nFirst, we define the set overlap O between two\npositive documents set D+\n1 and D+\n2 as:\nO = (D+\n1 ∩D+\n2 )/(D+\n1 ∪D+\n2 ). (8)\nAs illustrated in Figure 6a, the set overlaps of the\npositive document sets annotated by human users\n(Dh+) and LMs (Top-KSa\ni ,Da) are quite low (near\n13%), demonstrating their distinct tendencies in\nselecting valuable documents. On the contrary, the\noverlaps between different LMs are relatively high\n(over 55%). This evidence provides a strong ratio-\nnale for the generalization ability of AAR since\nLMs with different sizes tend to annotate simi-\nlar positive documents. Furthermore, LMs whose\nsizes are closer generally possess higher overlaps.\nThis implies a better generalization ability of the\nAAR to the LMs whose capacity is near the source\nLM. The findings further validate the results illus-\ntrated in Figure 4b.\nTo give an in-depth analysis of how human-\npreferred and LM-preferred documents differ, we\nshow two representative cases sampled from the\nMSMARCO QA in Table 2. We observe that the\nhuman-preferred document can always present the\ngold answer at the beginning of the text, while the\nLM-preferred document may not contain the exact\nanswer. However, an LM-preferred document may\n(1) deliver a new perspective to answer the given\nquestion, e.g., the cruise line’s responsibility if you\nmiss your cruise ship, or (2) give a specific explana-\ntion instead of an abstract definition, e.g., “forcible\ntransition of one state’s territory by another state”,\n250M 780M 3B 175B\n# Parameters\n40\n45\n50\n55\n60\n65\n70\n75MMLU Accuracy\nTART\nAARContriever (MSMARCO QA)\nAARContriever (KILT)\nAARANCE (MSMARCO QA)\nAARANCE (KILT)\nFigure 7: Comparison between single-task (MS-\nMARCO QA) and multi-task (KILT) trained AAR.\nTART (Asai et al., 2022) is a multi-task instruction-\nfinetuned retriever that has not been finetuned with LM-\npreferred signals.\nThese characteristics differ from search users who\nwant the full information and can further assist\nLMs in knowledge-based reasoning.\nWe further examine the unique characteristics\nof LM-preferred documents through the answer-\ndeletion test (i.e., deleting the exact answer span\nfrom the retrieved documents). As shown in\nFigure 6b, the retriever trained by either human-\npreferred (i.e., human-preferred retriever) or LM-\npreferred documents (i.e., LM-preferred retriever)\ncan help LM answer the given question. Never-\ntheless, after the answer-deletion, the performance\nof LM with the human-preferred retriever declines\nmore significantly than with the LM-preferred re-\ntriever. Despite having fewer exact match answers\n(0.6% for LM-preferred documents vs. 13.0% for\nhuman-preferred documents), LM-preferred docu-\nments provide helpful information from alternative\nperspectives. Therefore, adapting retrievers with\nLM-preferred documents can in turn make retrieval-\naugmented LM perform better.\n2427\nCorpora MMLU PopQA\nAll Hum. Soc. Sci. STEM OtherAll\nMS MARCO44.8 42.2 46.4 39.0 53.2 13.6\nKILT-Wikipedia42.6 42.5 45.9 34.3 50.5 37.7\nStandalone LM36.1 40.4 39.8 27.0 40.6 8.8\nTable 3: Performance with different retrieval corpora,\nusing Flan-T5Base as Lt and AARANCE as retriever.\n5.4 Multi-task Training of AAR\nIn this section, we explore if the multi-task training\nof AAR can endow the retriever with better gener-\nalization to the target task. Specifically, we choose\nKILT (Petroni et al.) as our multi-task data source,\nwhich consists of 5 categories (Fact Checking, En-\ntity Linking, Slot Filling, Open Domain QA, and\nDialogue). We take one representative subtask per\ncategory to form a mixture of multiple source tasks.\nFigure 7 illustrates that ANCE trained with\nmulti-task KILT can consistently outperform the\nsingle-task MSMARCO QA, proving the bet-\nter generalization ability brought by multi-task\naugmentation-adapted training. It is possible that\nLMs may vary slightly in preferred documents for\ndifferent tasks and AAR can switch more smoothly\nto the target task with the help of multi-task train-\ning. Contriever does not benefit greatly from multi-\ntask training. We conjecture that this is because\nContriever has been pre-trained with multiple for-\nmats of data augmentations and thus generalizes\nbetter to new data distribution than ANCE. Inter-\nestingly, multi-task instruction-finetuned retriever\nTART (Asai et al., 2022) has an overall worse per-\nformance compared to AAR, highlighting the ben-\nefits of having LM-preferred documents during the\nmulti-task training. A more detailed analysis about\nthe selection of source tasks is in Appendix B.\n5.5 Effect of Retrieval Corpus\nTable 3 demonstrates that regardless of the retrieval\ncorpus, AAR results in consistent and substantial\nperformance gains over the standalone LM.\nOn MMLU, using MS MARCO as the retrieval\ncorpus improves the LM more compared to KILT-\nWikipedia. We hypothesize that the retriever has\nbeen trained with MS MARCO corpus and thus\nholds better retrieval performance on it.\nOn PopQA, model performance will drop by\nlarge margins if we use MS MARCO as the re-\ntrieval corpus instead of KILT-Wikipedia. The pri-\nmary reason is that the PopQA dataset is sampled\nfrom Wikidata and designed for long-tail questions.\nPartial long-tail knowledge can be only found in\nSettings Methods MMLUPopQA\nAll All\nFew-shotOPT (Zhang et al., 2022) 26.0 12.3\nGPT-neo (Black et al., 2021)28.7 11.3\nZero-shot\nOPT 22.7 12.0\nGPT-neo 25.3 9.9\nOPT GenRead 22.3 12.2\nGPT-neo GenRead 24.4 11.9\nOPT w/ AARContriever(Ours) 23.2 29.1\nGPT-neo w/ AARContriever(Ours) 25.2 27.8\nOPT w/ AARANCE(Ours) 23.7 32.9\nGPT-neo w/ AARANCE(Ours) 26.6 30.1\nTable 4: Results of using models that have not been\nmulti-task instruction-finetuned as Lt. We experiment\nwith the 1.3B version of OPT and GPT-neo.\nKILT-Wikipedia (Mallen et al., 2022) while MS\nMARCO lacks the indispensable evidence that\nshould be utilized for answer prediction. For in-\nstance, given the question “Who is the mother\nof Melissa Benn?”, there is no document in MS\nMARCO containing the answer “Caroline Benn”.\nUnder such circumstances, aligning the retrieval\ncorpus with the data source can be necessary to\nleverage AAR’s ability.\n5.6 Application Scenarios of AAR\nTo examine if AAR works for unseen LMs that may\nlack zero-shot generalization ability, we report the\nresults of using OPT (Zhang et al., 2022) and GPT-\nneo (Black et al., 2021) as Lt, which have not been\nmulti-task instruction-finetuned.\nFrom Table 4, we observe that AAR improves\nboth LMs marginally on MMLU while achieving\nsignificant gains on PopQA. We conjecture that\nLMs can benefit more easily from retrieval augmen-\ntation on the knowledge-probing task like PopQA,\nwhere the answer span can be directly acquired\nfrom the retrieved documents. MMLU requires the\nLM to not only comprehend the retrieved pieces of\nevidence but also perform knowledge-based reason-\ning over them. OPT and GPT-neo may not possess\nsuch abilities in zero-shot scenarios.\nIn summary, although AAR perfectly fits the\nmulti-task instruction-finetuned LMs such as the\nFlan-T5 series and InstructGPT, it may not bring\nsignificant gains for LMs whose zero-shot perfor-\nmance is sometimes poor, especially on knowledge-\nbased reasoning. However, we believe that multi-\ntask instruction-finetuned models will be the foun-\ndation of future work due to their outstanding zero-\nshot generalization capabilities, ensuring the wide-\nranging application scenarios of AAR.\n2428\n6 Discussions\nLM-preferred Documents. Acquiring discrete\nfeedback signals from LMs is challenging as it re-\nquires superior labeling ability, which is not the de-\nsigned purpose of LMs. Inspired by ADist (Izacard\nand Grave, 2021a) and Atlas (Izacard et al., 2022),\nwe utilize the FiDAtt scores to select LM-preferred\ndocuments for the augmentation-adapted training.\nHowever, FiDAtt scores may not reflect the actual\ncontribution of each document faithfully since LM\nmay prefer attending to readable rather than in-\nformative documents. Furthermore, the quality of\nLM-preferred documents depends heavily on the\ninitial performance of the retrieval-augmented LM.\nParallel work (Shi et al., 2023) computes the KL\ndivergence between retrieval likelihood and LM\nlikelihood to train the retriever. Nevertheless, they\nrequire a larger source LM, Curie (6.7B), to pro-\nvide accurate LM likelihood signals. In the future,\nreinforcement learning could serve as an alterna-\ntive method to train the retriever, as it optimizes\nthe retriever by directly leveraging LM’s signals\nwithout relying on the devised rule.\nGeneric Retrieval Plug-in. Chatgpt-retrieval-\nplugin2 has recently gained attention in the NLP\ncommunity as a generic retrieval plug-in. It re-\ntrieves the most relevant document from users’ data\nsources and tailor ChatGPT’s response to meet their\nspecific needs. We believe that techniques such as\nAAR will enhance the ability of black-box Chat-\nGPT to generate more reasonable responses based\non the retrieved information, thereby promoting the\ndevelopment of human-centered LM design.\n7 Conclusion and Future Work\nThis paper introduces generic retrieval plug-in that\nutilizes a generic retriever to enhance target LMs\nthat may be unknown in advance or are unable\nto be fine-tuned jointly. Our proposed retriever,\nAAR, can directly support black-box LMs without\nrequiring any fine-tuning of the LMs. This is ac-\ncomplished by building the AAR’s training data\nwith preferred documents from a small source LM\ntogether with the ground truth.\nEmpirical results on MMLU and PopQA demon-\nstrate that AAR-assisted LMs greatly outperform\nthe standalone ones in zero-shot scenarios, and\nAAR generalizes well to LMs of different sizes\n2https://github.com/openai/chatgpt-retrieval-plugin\nand structures. Analytical results reveal that LM-\npreferred and human-preferred documents comple-\nment each other; LM-preferred documents from\ndifferent LMs overlap significantly, and LMs with\nsimilar sizes tend to yield closer document sets.\nWe leave a more detailed explanation of how dif-\nferent LMs interact with augmentation documents\nand a more reasonable selection of LM-preferred\ndocuments for future work. We hope our work\nshed light on a path to a generic way of treating\nlarge LMs as black boxes and adapting retrievers\nto augment them.\nLimitations\nDue to the limitation of computational resources,\nwe have not evaluated the Flan-T5XXL whose num-\nber of parameters is 11B, and the OPT whose num-\nber of parameters is greater than 1.3B.\nSince OPT and GPT-neo perform poorly in the\nzero-shot setting and separating attention scores of\neach document in the input is tedious for decoder-\nonly models, we choose not to use them as source\nLMs. However, we prove that taking the encoder-\ndecoder model Flan-T5 Base as our source LM is\nalso robust to augment decoder-only models. We\nwill explore new methods to annotate LM-preferred\ndocuments of decoder-only models based on their\ninherent signals.\nAcknowledgement\nZichun Yu, Shi Yu, and Zhiyuan Liu are supported\nby Institute Guo Qiang at Tsinghua University, Bei-\njing Academy of Artificial Intelligence (BAAI).\nAll authors proposed the original idea together.\nZichun Yu conducted the experiments. Zichun Yu,\nChenyan Xiong, Shi Yu, and Zhiyuan Liu wrote\nthe paper. Chenyan Xiong and Zhiyuan Liu pro-\nvided valuable suggestions for the research. We\nthank Suyu Ge for sharing the ANCE checkpoint\ninitialized from T5Base.\nReferences\nFabien André, Anne-Marie Kermarrec, and Nicolas\nLe Scouarnec. 2016. Cache locality is not enough:\nHigh-performance nearest neighbor search with prod-\nuct quantization fast scan. In VLDB, page 12.\nAkari Asai, Timo Schick, Patrick Lewis, Xilun Chen,\nGautier Izacard, Sebastian Riedel, Hannaneh Ha-\njishirzi, and Wen-tau Yih. 2022. Task-aware retrieval\nwith instructions. arXiv preprint arXiv:2211.09260.\n2429\nPayal Bajaj, Daniel Campos, Nick Craswell, Li Deng,\nJianfeng Gao, Xiaodong Liu, Rangan Majumder, An-\ndrew McNamara, Bhaskar Mitra, Tri Nguyen, et al.\n2016. Ms marco: A human generated machine read-\ning comprehension dataset. In CoCo@NeurIPS.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In Proceedings of ACM FAccT, pages\n610–623.\nSid Black, Gao Leo, Phil Wang, Connor Leahy, and\nStella Biderman. 2021. Gpt-neo: Large scale autore-\ngressive language modeling with mesh-tensorflow.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, Diego\nDe Las Casas, Aurelia Guy, Jacob Menick, Roman\nRing, Tom Hennigan, Saffron Huang, Loren Mag-\ngiore, Chris Jones, Albin Cassirer, Andy Brock,\nMichela Paganini, Geoffrey Irving, Oriol Vinyals,\nSimon Osindero, Karen Simonyan, Jack Rae, Erich\nElsen, and Laurent Sifre. 2022. Improving language\nmodels by retrieving from trillions of tokens. In\nICML, pages 2206–2240.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. In NeurIPS, pages 1877–1901.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts, Paul\nBarham, Hyung Won Chung, Charles Sutton, Sebas-\ntian Gehrmann, Parker Schuh, and et al. 2022. Palm:\nScaling language modeling with pathways. arXiv\npreprint arXiv:2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Zhao,\nYanping Huang, Andrew Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL, pages 4171–\n4186.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of ACL, pages 3816–3830.\nSuyu Ge, Chenyan Xiong, Corby Rosset, Arnold Over-\nwijk, Jiawei Han, and Paul Bennett. 2023. Augment-\ning zero-shot dense retrievers with plug-in mixture-\nof-memories. arXiv preprint arXiv:2302.03754.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training. In ICML,\npages 3929–3938.\nHangfeng He, Hongming Zhang, and Dan Roth. 2022.\nRethinking with retrieval: Faithful large language\nmodel inference. arXiv preprint arXiv:2301.00303.\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\n2021. Measuring massive multitask language under-\nstanding. In ICLR.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks, Jo-\nhannes Welbl, Aidan Clark, Thomas Hennigan, Eric\nNoland, Katherine Millican, George van den Driess-\nche, Bogdan Damoc, Aurelia Guy, Simon Osindero,\nKarén Simonyan, Erich Elsen, Oriol Vinyals, Jack\nRae, and Laurent Sifre. 2022. An empirical analysis\nof compute-optimal large language model training.\nIn NeurIPS, pages 30016–30030.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,\nTianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,\nBrian O’Horo, Gabriel Pereyra, Jeff Wang, Christo-\npher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,\nand Ves Stoyanov. 2022. Opt-iml: Scaling language\nmodel instruction meta learning through the lens of\ngeneralization. arXiv preprint arXiv:2212.12017.\nGautier Izacard, Mathilde Caron, Lucas Hosseini, Se-\nbastian Riedel, Piotr Bojanowski, Armand Joulin,\nand Edouard Grave. 2021. Unsupervised dense infor-\nmation retrieval with contrastive learning. TMLR.\nGautier Izacard and Edouard Grave. 2021a. Distilling\nknowledge from reader to retriever for question an-\nswering. In ICLR.\nGautier Izacard and Edouard Grave. 2021b. Leveraging\npassage retrieval with generative models for open\ndomain question answering. In Proceedings of EACL,\npages 874–880.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot Learning with Re-\ntrieval Augmented Language Models. arXiv preprint\narXiv:2208.03299.\nJeff Johnson, Matthijs Douze, and Herve Jegou. 2021.\nBillion-scale similarity search with gpus. IEEE TBD,\n7(3):535–547.\n2430\nMingxuan Ju, Wenhao Yu, Tong Zhao, Chuxu Zhang,\nand Yanfang Ye. 2022. Grape: Knowledge graph\nenhanced passage reader for open-domain question\nanswering. In Findings of EMNLP.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings\nof EMNLP, pages 6769–6781.\nOmar Khattab, Keshav Santhanam, Xiang Lisa\nLi, David Hall, Percy Liang, Christopher Potts,\nand Matei Zaharia. 2022. Demonstrate-search-\npredict: Composing retrieval and language mod-\nels for knowledge-intensive nlp. arXiv preprint\narXiv:2212.14024.\nPatrick Lewis, Ethan Perez, Aleksandara Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, Sebastian Riedel, and Douwe Kiela. 2020.\nRetrieval-augmented generation for knowledge-\nintensive NLP tasks. In NeurIPS, pages 9459–9474.\nBill Yuchen Lin, Kangmin Tan, Chris Miller, Beiwen\nTian, and Xiang Ren. 2022. Unsupervised cross-\ntask generalization via retrieval augmentation. In\nNeurIPS, pages 22003–22017.\nAlex Mallen, Akari Asai, Victor Zhong, Rajarshi\nDas, Hannaneh Hajishirzi, and Daniel Khashabi.\n2022. When not to trust language models: Inves-\ntigating effectiveness and limitations of paramet-\nric and non-parametric memories. arXiv preprint\narXiv:2212.10511.\nAntonis Maronikolakis and Hinrich Schütze. 2021. Mul-\ntidomain pretrained language models for green NLP.\nIn Proceedings of AdaptNLP, pages 1–8.\nOpenAI. 2023. Gpt-4 technical report. arXiv preprint\narXiv:2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. In NeurIPS, pages 27730–27744.\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick\nLewis, Majid Yazdani, Nicola De Cao, James Thorne,\nYacine Jernite, Vladimir Karpukhin, Jean Maillard,\nVassilis Plachouras, Tim Rocktäschel, and Sebastian\nRiedel. KILT: a benchmark for knowledge intensive\nlanguage tasks. In Proceedings of NAACL, pages\n2523–2544.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR, 21:140:1–140:67.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parame-\nters of a language model? In Proceedings of EMNLP,\npages 5418–5426.\nOhad Rubin, Jonathan Herzig, and Jonathan Berant.\n2022. Learning to retrieve prompts for in-context\nlearning. In Proceedings of NAACL, pages 2655–\n2671.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, and et al. 2022.\nMultitask prompted training enables zero-shot task\ngeneralization. In ICLR.\nWeijia Shi, Sewon Min, Michihiro Yasunaga, Min-\njoon Seo, Rich James, Mike Lewis, Luke Zettle-\nmoyer, and Wen tau Yih. 2023. Replug: Retrieval-\naugmented black-box language models. arXiv\npreprint arXiv:2301.12652.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in NLP. In Proceedings of ACL, pages\n3645–3650.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain-of-thought prompt-\ning elicits reasoning in large language models. In\nNeurIPS, pages 24824–24837.\nChaojun Xiao, Zhengyan Zhang, Xu Han, Chi-Min\nChan, Yankai Lin, Zhiyuan Liu, Xiangyang Li,\nZhonghua Li, Zhao Cao, and Maosong Sun. 2023.\nPlug-and-play document modules for pre-trained\nmodels. In Proceedings of ACL.\nJi Xin, Chenyan Xiong, Ashwin Srinivasan, Ankita\nSharma, Damien Jose, and Paul Bennett. 2022. Zero-\nshot dense retrieval with momentum adversarial do-\nmain invariant representations. In Findings of ACL,\npages 4008–4020.\nLee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang,\nJialin Liu, Paul N. Bennett, Junaid Ahmed, and\nArnold Overwijk. 2021. Approximate nearest neigh-\nbor negative contrastive learning for dense text re-\ntrieval. In ICLR.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\n2431\nrather than retrieve: Large language models are\nstrong context generators. In ICLR.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models. arXiv preprint\narXiv:2205.01068.\nZhengyan Zhang, Zhiyuan Zeng, Yankai Lin, Huadong\nWang, Deming Ye, Chaojun Xiao, Xu Han, Zhiyuan\nLiu, Peng Li, Maosong Sun, and Jie Zhou. 2023.\nPlug-and-play knowledge injection for pre-trained\nlanguage models. In Proceedings of ACL.\nCe Zhou, Qian Li, Chen Li, Jun Yu, Yixin Liu,\nGuangjing Wang, Kai Zhang, Cheng Ji, Qiben Yan,\nLifang He, Hao Peng, Jianxin Li, Jia Wu, Ziwei Liu,\nPengtao Xie, Caiming Xiong, Jian Pei, Philip S. Yu,\nand Lichao Sun. 2023. A comprehensive survey on\npretrained foundation models: A history from bert to\nchatgpt. arXiv preprint arXiv:2302.09419.\n2432\nA Experimental Settings\nIn this section, we discuss additional experimental\nsetup, as a supplement of Section 4.\nA.1 Training Hyperparameters\nWe take the ANCE initialized from T5Base3 (Xiong\net al., 2021; Ge et al., 2023) and Contriever4 (Izac-\nard et al., 2021)’s hyperparameters in the\naugmentation-adapted training. Specifically, we fix\nbatch size as 8, learning rate as 5e-6, and epochs as\n6 for ANCE while taking batch size as 8, learning\nrate as 1e-5, and epochs as 3 for Contriever. We\nchoose their best checkpoints based on the perfor-\nmance of the development set. The statistics about\nour source and target tasks are in Table 6.\nA.2 Number of Augmentation Documents\nFor MMLU, we analyze how the number of aug-\nmentation documents affects LMs’ performance.\nAs illustrated in Figure 9, we discover that LMs of\nlarger capacity generally benefit more from more\naugmentation documents. A possible explanation\nis that larger LMs are more capable of integrating\ninformation from multiple documents and perform-\ning complicated reasoning based on them.\nFor PopQA, using 3 augmentation documents\nachieves the best performance across all LMs.\nA.3 Prompt Templates\nThe prompt template for MMLU is:\nHere’s a problem to solve: {question}\nAmong the 4 following options, which is\nthe correct answer?\n- A: {choice_A}\n- B: {choice_B}\n- C: {choice_C}\n- D: {choice_D}\nThe prompt template for PopQA is:\nQ: {question} A:\nB Selection of Source Task\nWe provide a detailed selection of the source tasks\nhere, using a variety of source and target tasks to an-\nalyze. MSMARCO QA, KILT-TriviaQA, and NQ\nbelong to Open Domain QA, while KILT-T-REx\nand zsRE belong to Slot Filling. MMLU belongs\nto Multi-task Language Understanding, which is\ncloser to the Open Domain QA in terms of the task\nobjective. As shown in Table 5, when we align the\n3https://huggingface.co/OpenMatch/t5-ance\n4https://huggingface.co/facebook/contriever-msmarco\nTs\nTt MMLU NQ zsRE\nMSMARCO QA 44.8 46.7 75.1\nKILT-TriviaQA 43.6 46.4 74.9\nKILT-T-REx 44.1 45.9 77.2\nTable 5: Relationship between the selection of source\ntask Ts and the performance of target task Tt. The\nmodel is Flan-T5Base w/ AARANCE. As NQ and zsRE\nare included in the Flan-T5 training data, we only report\ntheir F1 results here for reference.\n250M 780M 3B 175B\n# Parameters\n25\n30\n35\n40\n45\n50\n55PopQA Accuracy\nANCE\nAARANCE\nContriever\nAARContriever\nFigure 8: AAR’s improvements on PopQA, using Flan-\nT5Base (250M), Flan-T5Large (780M), Flan-T5XL (3B),\nInstructGPT (175B) as target LMs.\ncategory of the source task with the target task, the\nLM w/ AAR can generally achieve the best results.\nWe suppose that this is because LM may share sim-\nilar document preferences on the tasks from the\nsame dataset category, making AAR easier to gen-\neralize. Furthermore, taking MSMARCO QA as\nthe source task performs the best on MMLU. This\nvalidates the rationality to set Ts as MSMARCO\nQA in our main experimental settings.\nC AAR’s Improvements on PopQA\nWe show AAR’s improvements on PopQA in Fig-\nure 8. The observations are similar to Figure 4a.\nD Fine-tuning Results\nWe also report the fine-tuning results of Flan-\nT5Base and Flan-T5Large on MMLU auxiliary train-\ning data (Hendrycks et al., 2021) in Table 7. Due to\nthe limitation of the computational resources, we\ndo not include the fine-tuning result of Flan-T5XL.\nWe take batch size as 32, learning rate as 5e-5, and\nepochs as 3 in fine-tuning. In general, the LM that\nhas already been massively multi-task instruction-\nfinetuned, such as Flan-T5, improves little from\nfine-tuning on extra tasks but benefits greatly from\nour AAR. The results further validate the power of\nzero-shot retrieval augmentation.\n2433\n1 2 3 4 5 6 7 8 9 10\nNumber of documents\n36\n38\n40\n42\n44\n46MMLU Accuracy\nStandalone LM\nLt=Flan-T5Base\n(a) Flan-T5Base w/ AARANCE.\n1 2 3 4 5 6 7 8 9 10\nNumber of documents\n44\n46\n48\n50\n52MMLU Accuracy\nStandalone LM\nLt=Flan-T5Large (b) Flan-T5Large w/ AARANCE.\n1 2 3 4 5 6 7 8 9 10\nNumber of documents\n50\n52\n54\n56\n58MMLU Accuracy\nStandalone LM\nLt=Flan-T5XL (c) Flan-T5XL w/ AARANCE.\nFigure 9: Relationship between LM’s performance and the number of augmentation documents.\nSource/target Task Category # Queries\nTs\nMSMARCO QA Open Domain QA 148122\nKILT-FEVER Fact Checking 10444\nKILT-WNED Entity Linking 3396\nKILT-T-REx Slot Filling 5000\nKILT-TriviaQA Open Domain QA 5359\nKILT-Wizard of Wikipedia Dialogue 3054\nTt\nMMLU Multi-task Language Understanding 1531\nPopQA Open Domain QA 14267\nTable 6: Statistics of source and target tasks.\nMethods MMLU\nAll Hum. Soc. Sci. STEM Other\nFlan-T5Base 36.1 40.4 39.8 27.0 40.6\nFlan-T5Base Fine-tuning 36.1 38.9 41.2 27.9 39.9\nFlan-T5Base w/ Contriever 43.7 44.4 45.0 36.4 51.1\nFlan-T5Base w/ ANCE 43.0 44.2 44.3 34.5 51.9\nFlan-T5Base w/ AARContriever (Ours) 44.4 44.7 47.7 35.8 52.2\nFlan-T5Base w/ AARANCE (Ours) 44.8 42.2 46.4 39.0 53.2\nFlan-T5Large 45.1 47.7 53.5 34.4 49.2\nFlan-T5Large Fine-tuning 45.3 47.6 54.1 35.2 48.7\nFlan-T5Large w/ Contriever 50.7 50.5 56.4 38.9 61.1\nFlan-T5Large w/ ANCE 49.2 49.3 56.7 38.1 57.2\nFlan-T5Large w/ AARContriever (Ours) 51.8 50.8 59.7 39.4 61.8\nFlan-T5Large w/ AARANCE (Ours) 50.4 48.0 58.1 39.3 60.2\nFlan-T5XL 51.2 55.5 57.4 38.1 58.7\nFlan-T5XL w/ Contriever 56.4 57.3 66.1 43.9 63.2\nFlan-T5XL w/ ANCE 55.3 55.9 64.0 41.5 64.9\nFlan-T5XL w/ AARContriever (Ours) 56.7 57.7 65.4 43.6 65.1\nFlan-T5XL w/ AARANCE (Ours) 56.2 59.4 64.8 41.5 64.9\nInstructGPT 60.2 65.7 68.0 46.1 66.5\nInstructGPT w/ Contriever 60.5 62.0 71.8 44.3 70.1\nInstructGPT w/ ANCE 61.6 62.4 73.4 47.6 68.6\nInstructGPT w/ AARContriever (Ours) 61.5 64.5 73.1 45.0 69.9\nInstructGPT w/ AARANCE (Ours) 62.2 62.0 72.0 49.2 70.7\nTable 7: Fine-tuning results on MMLU. We use the official auxiliary training data of MMLU to fine-tune the LM.\n2434\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 8\n□\u0017 A2. Did you discuss any potential risks of your work?\nNo potential risks\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 0 and 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4.1\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4.1\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4.1\nC □\u0013 Did you run computational experiments?\nSection 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 5\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n2435\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4.2 and A\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n2436",
  "topic": "Generalization",
  "concepts": [
    {
      "name": "Generalization",
      "score": 0.7783203721046448
    },
    {
      "name": "Computer science",
      "score": 0.7530878782272339
    },
    {
      "name": "Scheme (mathematics)",
      "score": 0.6541805267333984
    },
    {
      "name": "Source code",
      "score": 0.5852347016334534
    },
    {
      "name": "Labrador Retriever",
      "score": 0.5561806559562683
    },
    {
      "name": "Open source",
      "score": 0.5233939290046692
    },
    {
      "name": "Plug-in",
      "score": 0.5105167627334595
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.44887733459472656
    },
    {
      "name": "Plug and play",
      "score": 0.42820772528648376
    },
    {
      "name": "Code (set theory)",
      "score": 0.4234829545021057
    },
    {
      "name": "Artificial intelligence",
      "score": 0.343344122171402
    },
    {
      "name": "Operating system",
      "score": 0.1004195511341095
    },
    {
      "name": "Mathematics",
      "score": 0.09116831421852112
    },
    {
      "name": "Programming language",
      "score": 0.0901125967502594
    },
    {
      "name": "Software",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Pathology",
      "score": 0.0
    }
  ]
}