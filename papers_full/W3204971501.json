{
  "title": "mLUKE: The Power of Entity Representations in Multilingual Pretrained Language Models",
  "url": "https://openalex.org/W3204971501",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3019104427",
      "name": "Ryokan Ri",
      "affiliations": [
        "The University of Tokyo"
      ]
    },
    {
      "id": "https://openalex.org/A2119014096",
      "name": "Ikuya Yamada",
      "affiliations": [
        "RIKEN Center for Advanced Intelligence Project"
      ]
    },
    {
      "id": "https://openalex.org/A1788978749",
      "name": "Yoshimasa Tsuruoka",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2962712421",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W3091998909",
    "https://openalex.org/W4303684595",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W2995118574",
    "https://openalex.org/W4319915529",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3102859667",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2892252202",
    "https://openalex.org/W1838058638",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W2971145411",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W3176489198",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W2144578941",
    "https://openalex.org/W3180299705",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2995015695",
    "https://openalex.org/W2915429162",
    "https://openalex.org/W3166445280",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3202026069",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2952087486",
    "https://openalex.org/W3102844651",
    "https://openalex.org/W3001434439",
    "https://openalex.org/W3156170450",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2970476646"
  ],
  "abstract": "Recent studies have shown that multilingual pretrained language models can be effectively improved with cross-lingual alignment information from Wikipedia entities.However, existing methods only exploit entity information in pretraining and do not explicitly use entities in downstream tasks.In this study, we explore the effectiveness of leveraging entity representations for downstream cross-lingual tasks.We train a multilingual language model with 24 languages with entity representations and showthe model consistently outperforms word-based pretrained models in various cross-lingual transfer tasks.We also analyze the model and the key insight is that incorporating entity representations into the input allows us to extract more language-agnostic features.We also evaluate the model with a multilingual cloze prompt task with the mLAMA dataset.We show that entity-based prompt elicits correct factual knowledge more likely than using only word representations.",
  "full_text": "mLUKE: The Power of Entity Representations\nin Multilingual Pretrained Language Models\nRyokan Ri1,2∗\nryo0123@ousia.jp\nIkuya Yamada1,3\nikuya@ousia.jp\nYoshimasa Tsuruoka2\ntsuruoka@logos.t.u-tokyo.ac.jp\n1Studio Ousia, Tokyo, Japan\n2The University of Tokyo, Tokyo, Japan\n3RIKEN AIP, Tokyo, Japan\nAbstract\nRecent studies have shown that multilingual\npretrained language models can be effectively\nimproved with cross-lingual alignment infor-\nmation from Wikipedia entities. However, ex-\nisting methods only exploit entity information\nin pretraining and do not explicitly use enti-\nties in downstream tasks. In this study, we\nexplore the effectiveness of leveraging entity\nrepresentations for downstream cross-lingual\ntasks. We train a multilingual language model\nwith 24 languages with entity representations\nand show the model consistently outperforms\nword-based pretrained models in various cross-\nlingual transfer tasks. We also analyze the\nmodel and the key insight is that incorporat-\ning entity representations into the input allows\nus to extract more language-agnostic features.\nWe also evaluate the model with a multilingual\ncloze prompt task with the mLAMA dataset.\nWe show that entity-based prompt elicits cor-\nrect factual knowledge more likely than using\nonly word representations. Our source code\nand pretrained models are available athttps:\n//github.com/studio-ousia/luke.\n1 Introduction\nPretrained language models have become crucial\nfor achieving state-of-the-art performance in mod-\nern natural language processing. In particular, mul-\ntilingual language models (Conneau and Lample,\n2019; Conneau et al., 2020a; Doddapaneni et al.,\n2021) have attracted considerable attention particu-\nlarly due to their utility in cross-lingual transfer.\nIn zero-shot cross-lingual transfer, a pretrained\nencoder is ﬁne-tuned in a single resource-rich lan-\nguage (typically English), and then evaluated on\nother languages never seen during ﬁne-tuning. A\nkey to solving cross-lingual transfer tasks is to ob-\ntain representations that generalize well across lan-\nguages. Several studies aim to improve multilin-\ngual models with cross-lingual supervision such as\n∗ Work done as an intern at Studio Ousia.\nbilingual word dictionaries (Conneau et al., 2020b)\nor parallel sentences (Conneau and Lample, 2019).\nAnother source of such information is the cross-\nlingual mappings of Wikipedia entities (articles).\nWikipedia entities are aligned across languages via\ninter-language links and the text contains numer-\nous entity annotations (hyperlinks). With these\ndata, models can learn cross-lingual correspon-\ndence such as the words Tokyo (English) and 東\n京 (Japanese) refers to the same entity. Wikipedia\nentity annotations have been shown to provide rich\ncross-lingual alignment information to improve\nmultilingual language models (Calixto et al., 2021;\nJiang et al., 2022). However, previous studies only\nincorporate entity information through an auxiliary\nloss function during pretraining, and the models do\nnot explicitly have entity representations used for\ndownstream tasks.\nIn this study, we investigate the effectiveness\nof entity representations in multilingual language\nmodels. Entity representations are known to en-\nhance language models in mono-lingual settings\n(Zhang et al., 2019; Peters et al., 2019; Wang et al.,\n2021; Xiong et al., 2020; Yamada et al., 2020)\npresumably by introducing real-world knowledge.\nWe show that using entity representations facili-\ntates cross-lingual transfer by providing language-\nindependent features. To this end, we present a\nmultilingual extension of LUKE (Yamada et al.,\n2020). The model is trained with the multilingual\nmasked language modeling (MLM) task as well\nas the masked entity prediction (MEP) task with\nWikipedia entity embeddings.\nWe investigate two ways of using the entity rep-\nresentations in cross-lingual transfer tasks: (1) per-\nform entity linking for the input text, and append\nthe detected entity tokens to the input sequence.\nThe entity tokens are expected to provide language-\nindependent features to the model. We evaluate\nthis approach with cross-lingual question answer-\ning (QA) datasets: XQuAD (Artetxe et al., 2020)\nand MLQA (Lewis et al., 2020); (2) use the entity\n[MASK] token from the MEP task as a language-\nindependent feature extractor. In the MEP task,\nword tokens in a mention span are associated with\nan entity [MASK] token, the contextualized rep-\nresentation of which is used to train the model to\npredict its original identity. Here, we apply similar\ninput formulations to tasks involving mention-span\nclassiﬁcation, relation extraction (RE) and named\nentity recognition (NER): the attribute of a mention\nor a pair of mentions is predicted using their con-\ntextualized entity [MASK] feature. We evaluate\nthis approach with the RELX (Köksal and Özgür,\n2020) and CoNLL NER (Tjong Kim Sang, 2002;\nTjong Kim Sang and De Meulder, 2003) datasets.\nThe experimental results show that these entity-\nbased approaches consistently outperform word-\nbased baselines. Our analysis reveals that entity\nrepresentations provide more language-agnostic\nfeatures to solve the downstream tasks.\nWe also explore solving a multilingual zero-shot\ncloze prompt task (Liu et al., 2021) with the entity\n[MASK] token. Recent studies have shown that we\ncan address various downstream tasks by querying\na language model for blanks in prompts (Petroni\net al., 2019; Cui et al., 2021). Typically, the answer\ntokens are predicted from the model’s word-piece\nvocabulary but here we incorporate the prediction\nfrom the entity vocabulary queried by the entity\n[MASK] token. We evaluate our approach with the\nmLAMA dataset (Kassner et al., 2021) in various\nlanguages and show that using the entity [MASK]\ntoken reduces language bias and elicits correct fac-\ntual knowledge more likely than using only the\nword [MASK] token.\n2 Multilingual Language Models with\nEntity Representations\n2.1 Model: mulitlingual LUKE\nTo evaluate the effectiveness of entity representa-\ntions for cross-lingual downstream tasks, we in-\ntroduce a new multilingual language model based\non a bidirectional transformer encoder: Multilin-\ngual LUKE (mLUKE), a multilingual extension of\nLUKE (Yamada et al., 2020). The model is trained\nwith the masked language modeling (MLM) task\n(Vaswani et al., 2017) as well as the masked entity\nprediction (MEP) task. In MEP, some of the input\nentity tokens are randomly masked with the spe-\ncial entity [MASK] token, and the model is trained\nto predict the original entities. Note that the entity\n[MASK] token is different from the word[MASK]\ntoken for MLM.\nThe model takes as input a tokenized text\n(w1,w2,...,w m) and the entities appearing in the\ntext (e1,e2,...,e n), and compute the contextualized\nrepresentation for each token ( hw1,hw2,..., hwm\nand he1,he2,..., hen). The word and entity tokens\nequally undergo self-attention computation (i.e., no\nentity-aware self-attention in Yamada et al. (2020))\nafter embedding layers.\nThe word and entity embeddings are computed\nas the summation of the following three embed-\ndings: token embeddings, type embeddings, and\nposition embeddings (Devlin et al., 2019). The\nentity tokens are associated with the word tokens\nthrough position embeddings: the position of an\nentity token is deﬁned as the positions of its cor-\nresponding word tokens, and the entity position\nembeddings are summed over the positions.\nModel Conﬁguration. The model conﬁgurations\nof mLUKE follow the base and large conﬁgura-\ntions of XLM-RoBERTa (Conneau et al., 2020a), a\nvariant of BERT (Devlin et al., 2019) trained with\nCommonCrawl data from 100 languages. Before\npretraining, the parameters in common ( e.g., the\nweights of the transformer encoder and the word\nembeddings) are initialized using the checkpoint\nfrom the Transformers library.1\nThe size of the entity embeddings is set to 256\nand they are projected to the size of the word em-\nbeddings before being fed into the encoder.\n2.2 Training Corpus: Wikipedia\nWe use Wikipedia dumps in 24 languages (Ap-\npendix A) as the training data. These languages\nare selected to cover reasonable numbers of lan-\nguages that appear in downstream cross-lingual\ndatasets. We generate input sequences by splitting\nthe content of each page into sequences of sen-\ntences comprising ≤ 512 words with their entity\nannotations (i.e., hyperlinks). During training, data\nare sampled from each language withni items with\nthe following multinomial distribution:\npi = nα\ni∑N\nk=1 nα\nk\n, (1)\nwhere αis a smoothing parameter and set to 0.7\nfollowing multilingual BERT.2\n1https://huggingface.co/transformers/\n2https://github.com/google-research/\nbert/blob/master/multilingual.md\nFigure 1: How to use entity representations in downstream tasks. The input entity embeddings are associated with\ntheir mentions (indicated by dotted lines) via positional embeddings.\nEntity Vocabulary. Entities used in mLUKE are\ndeﬁned as Wikipedia articles. The articles from dif-\nferent languages are aligned through inter-language\nlinks3 and the aligned articles are treated as a sin-\ngle entity. We include in the vocabulary the most\nfrequent 1.2M entities in terms of the number of hy-\nperlinks that appear across at least three languages\nto facilitate cross-lingual learning.\nOptimization. We optimize the models with\na batch size of 2048 for 1M steps in total us-\ning AdamW (Loshchilov and Hutter, 2019) with\nwarmup and linear decay of the learning rate. To\nstabilize training, we perform pretraining in two\nstages: (1) in the ﬁrst 500K steps, we update only\nthose parameters that are randomly initialized (e.g.,\nentity embeddings); (2) we update all parameters\nin the remaining 500K steps. The learning rate\nscheduler is reset at each training stage. For further\ndetails on hyperparameters, see Appendix A.\n2.3 Baseline Models\nWe compare the primary model that we investi-\ngate, multilingual LUKE used with entity repre-\nsentations (mLUKE-E), against several baselines\npretrained models and an ablation model based on\nword representations:\nmBERT (Devlin et al., 2019) is one of the earliest\nmultilingual language models. We provide these\nresults as a reference.\nXLM-R (Conneau et al., 2020a) is the model that\nmLUKE is built on. This result indicates how our\nadditional pretraining step and entity representa-\n3https://en.wikipedia.org/wiki/Help:\nInterlanguage_links. We build an inter-language\ndatabase from the wikidatawiki dump from November 30,\n2020.\ntion impact the performance. Since earlier studies\n(Liu et al., 2019; Lan et al., 2020) indicated longer\npretraining would simply improve performance,\nwe train another model based on XLM-Rbase with\nextra MLM pretraining following the same conﬁg-\nuration of mLUKE.\nmLUKE-W is an ablation model of mLUKE-E.\nThis model discards the entity embeddings learned\nduring pretraining and only takes word tokens as\ninput as with the other baseline models. The results\nfrom this model indicate the effect of MEP only\nas an auxiliary task in pretraining, and the com-\nparison with this model will highlight the effect of\nusing entity representations for downstream tasks\nin mLUKE-E.\nThe above models are ﬁne-tuned with the same\nhyperparameter search space and computational\nbudget as described in Appendix B.\nWe also present the results of XLM-K (Jiang\net al., 2022) for ease of reference. XLM-K is based\non XLM-Rbase and trained with entity information\nfrom Wikipedia but does not use entity representa-\ntions in downstream tasks. Notice that their results\nare not strictly comparable to ours, because the\npretraining and ﬁne-tuning settings are different.\n3 Adding Entities as Language-Agnostic\nFeatures in QA\nWe evaluate the approach of adding entity embed-\ndings to the input of mLUKE-E with cross-lingual\nextractive QA tasks. The task is, given a question\nand a context passage, to extract the answer span\nfrom the context. The entity embeddings provide\nlanguage-agnostic features and thus should facili-\ntate cross-lingual transfer learning.\nXQuAD en es de el ru tr ar vi th zh hi avg.\nmBERT 84.5 76.1 73.1 59.0 70.2 53.2 62.1 68.5 40.7 58.3 57.0 63.9\nXLM-Rbase 84.0 76.5 76.4 73.9 74.4 67.8 68.1 74.2 66.8 61.5 68.7 72.0\n+ extra training 86.1 76.9 76.5 73.7 74.7 66.3 68.2 74.5 67.7 64.7 66.6 72.4\nmLUKE-Wbase 85.7 78.0 77.4 74.7 75.7 68.3 71.7 75.9 67.1 65.1 69.9 73.6\nmLUKE-Ebase 86.3 78.9 78.9 73.9 76.0 68.8 71.4 76.4 67.5 65.9 72.2 74.2\nXLM-Rlarge 88.5 82.4 82.0 81.4 81.2 75.5 75.9 80.7 72.3 67.6 77.2 78.6\nmLUKE-Wlarge 89.0 83.1 82.4 81.3 81.3 75.3 77.9 81.2 75.1 71.5 77.3 79.6\nmLUKE-Elarge 88.6 83.0 81.7 81.4 80.8 75.8 77.7 81.9 75.4 71.9 77.5 79.6\nMLQA en es de ar hi vi zh avg. G-XLT avg.\nmBERT 79.1 65.9 58.6 48.6 44.8 58.5 58.1 59.1 40.9\nXLM-Rbase 79.7 67.7 62.2 55.8 59.9 65.3 62.5 64.7 33.4\n+ extra training 81.3 69.8 65.0 54.8 59.3 65.6 64.2 65.7 50.2\nmLUKE-Wbase 81.3 69.7 65.4 60.4 63.2 68.3 66.1 67.8 54.0\nmLUKE-Ebase 80.8 70.0 65.5 60.8 63.7 68.4 66.2 67.9 55.6\nXLM-K (Jiang et al., 2022) 80.8 69.2 63.8 60.0 65.3 70.1 63.8 67.7 -\nXLM-Rlarge 83.9 74.7 69.9 64.9 69.9 73.3 70.3 72.4 65.3\nmLUKE-Wlarge 84.0 74.3 70.3 66.2 70.2 74.2 69.7 72.7 67.4\nmLUKE-Elarge 84.1 74.5 70.5 66.2 71.4 74.3 70.5 73.1 67.7\nTable 1: F1 scores on the XQuAD and MLQA dataset in the cross-lingual transfer settings. The scores without\nreference are from the best model tuned with the English development data.\n3.1 Main Experiments\nDatasets. We ﬁne-tune the pretrained models with\nthe SQuAD 1.1 dataset (Rajpurkar et al., 2016), and\nevaluate them with the two multilingual datasets:\nXQuAD (Artetxe et al., 2020) and MLQA (Lewis\net al., 2020). XQuAD is created by translating a\nsubset of the SQuAD development set while the\nsource of MLQA is natural text in Wikipedia. Be-\nsides multiple monolingual evaluation data splits,\nMLQA also offers data to evaluate generalized\ncross-lingual transfer (G-XLT), where the question\nand context texts are in different languages.\nModels. All QA models used in this experiment\nfollow Devlin et al. (2019). The model takes the\nquestion and context word tokens as input and pre-\ndicts a score for each span of the context word to-\nkens. The span with the highest score is predicted\nas the answer to the question.\nmLUKE-E takes entity tokens as additional fea-\ntures in the input (Figure 1) to enrich word repre-\nsentations. The entities are automatically detected\nusing a heuristic string matching based on the orig-\ninal Wikipedia article from which the dataset in-\nstance is created. See Appendix C for more details.\nResults. Table 1 summarizes the model’s F1 scores\nfor each language. First, we discuss the base mod-\nels. On the effectiveness of entity representations,\nmLUKE-Ebase performs better than its word-based\ncounterpart mLUKE-Wbase (0.6 average points im-\nprovement in the XQuAD average score, 0.1 points\nin MLQA) and XLM-K (0.2 points improvement\nin MLQA), which indicates the input entity tokens\nprovide useful features to facilitate cross-lingual\ntransfer. The usefulness of entities is demonstrated\nespecially in the MLQA’s G-XLT setting (full re-\nsults available in Appendix F); mLUKE-Ebase ex-\nhibits a substantial 1.6 point improvement in the\nG-XLT average score over mLUKE-Wbase. This\nsuggests that entity representations are beneﬁcial\nin a challenging situation where the model needs\nto capture language-agnostic semantics from text\nsegments in different languages.\nWe also observe that XLM-Rbase beneﬁts from\nextra training (0.4 points improvement in the av-\nerage score on XQuAD and 2.1 points in MLQA).\nThe mLUKE-Wbase model further improves the\naverage score from XLM-Rbase with extra training\n(1.2 points improvement in XQuAD and 2.1 points\nin MLQA), showing the effectiveness of the MEP\ntask for cross-lingual QA.\nBy comparing large models, we still observe\nsubstantial improvements from XLM-Rlarge to the\nmLUKE models. Also we can see that mLUKE-\nElarge overall provides better results than mLUKE-\nWlarge (0.4 and 0.3 points improvements in the\nMLQA average and G-XLT scores; comparable\nscores in XQuAD), conﬁrming the effectiveness of\nentity representations.\n3.2 Analysis\nHow do the entity representations help the model\nin cross-lingual transfer? In the mLUKE-E model,\nthe input entity tokens annotate mention spans on\nwhich the model performs prediction. We hypothe-\nsize that this allows the encoder to inject language-\nagnostic entity knowledge into span representa-\ntions, which help better align representations across\nlanguages. To support this hypothesis, we compare\nthe degree of alignment between span representa-\ntions before and after adding entity embeddings in\nthe input, i.e., mLUKE-W and mLUKE-E.\nTask. We quantify the degree of alignment as\nperformance on the contextualized word retrieval\n(CWR) task (Cao et al., 2020). The task is, given\na word within a sentence in the query language, to\nﬁnd the word with the same meaning in the context\nfrom a candidate pool in the target language.\nDataset. We use the MLQA dev set (Lewis et al.,\n2020). As MLQA is constructed from parallel sen-\ntences mined from Wikipedia, some sentences and\nanswer spans are aligned and thus the dataset can\nbe easily adapted for the CWR task. As the query\nand target word, we use the answer span 4 anno-\ntated in the dataset, which is also parallel across\nthe languages. We use the English dataset as the\nquery language and other languages as the target.\nWe discard query instances that do not have their\nparallel data in the target language. The candidate\npool is all answer spans in the target language data.\nModels. We evaluate the mLUKE-W base and\nmLUKE-Ebase models without ﬁne-tuning. The\nretrieval is performed by ranking the cosine simi-\nlarity of contextualized span representations, which\nis computed by mean-pooling the output word vec-\ntors in the span.\nResults. Table 2 shows the retrieval performance\nin terms of the mean reciprocal rank score. We\nobserve that the scores of mLUKE-Ebase are higher\nthan mLUKE-Wbase across all the languages. This\ndemonstrates that adding entities improves the de-\ngree of alignment of span representations, which\nmay explain the improvement of mLUKE-E in the\ncross-lingual QA task.\nar de es hi vi zh avg.\nmLUKE-Wbase 55.6 66.1 68.4 60.4 69.7 56.1 62.7\nmLUKE-Ebase 56.9 68.1 70.4 61.5 71.2 60.0 64.7\nTable 2: The mean reciprocal rank score of the CWR\ntask with the MLQA dev set.\n4Answer spans are not necessarily a word, but here we\ngeneralize the task as span retrieval for our purpose.\n4 The Entity MASK Token as Feature\nExtractor in RE and NER\nIn this section, we evaluate the approach of using\nthe entity [MASK] token to extract features from\nmLUKE-E for two entity-related tasks: relation\nextraction and named entity recognition.\nWe formulate both tasks as the classiﬁcation of\nmention spans. The baseline models extract the\nfeature of spans as the contextualized representa-\ntions of word tokens, while mLUKE-E extracts the\nfeature as the contextualized representations of the\nspecial language-independent entity tokens associ-\nated with the mentions (Figure 1). We demonstrate\nthat this approach consistently improves the perfor-\nmance in cross-lingual transfer.\n4.1 Relation Extraction\nRelation Extraction (RE) is a task to determine the\ncorrect relation between the two (head and tail) enti-\nties in a sentence. Adding entity type features have\nbeen shown to be effective to cross-lingual transfer\nin RE (Subburathinam et al., 2019; Ahmad et al.,\n2021), but here we investigate an approach that\ndoes not require predeﬁned entity types but utilize\nspecial entity embeddings learned in pretraining.\nDatasets. We ﬁne-tune the models with the En-\nglish KBP-37 dataset (Zhang and Wang, 2015) and\nevaluate the models with the RELX dataset (Köksal\nand Özgür, 2020), which is created by translating\na subset of 502 sentences from KBP-37’s test set\ninto four different languages. Following Köksal\nand Özgür (2020), we report the macro average of\nF1 scores of the 18 relations.\nModels. In the input text, the head and tail enti-\nties are surrounded with special markers (<ent>,\n<ent2>). The baseline models extract the feature\nvectors for the entities as the contextualized vector\nof the ﬁrst marker followed by their mentions. The\ntwo entity features are concatenated and fed into a\nlinear classiﬁer to predict their relation.\nFor mLUKE-E, we introduce two special enti-\nties, [HEAD] and [TAIL], to represent the head\nand tail entities (Yamada et al., 2020). Their em-\nbeddings are initialized with the entity [MASK]\nembedding. They are added to the input sequence\nbeing associated with the entity mentions in the\ninput, and their contextualized representations are\nextracted as the feature vectors. As with the word-\nbased models, the features are concatenated and\ninput to a linear classiﬁer.\nRE NER\nen de es fr tr avg. en de nl es avg.\nmBERT 65.0 57.3 61.6 58.9 56.2 59.8 89.7 70.0 75.2 77.1 78.0\nXLM-Rbase 66.5 60.8 62.9 60.9 57.7 61.7 91.5 74.3 80.7 79.8 81.6\n+ extra training 67.0 61.3 62.9 64.3 61.9 63.5 91.8 75.7 80.3 79.8 81.9\nmLUKE-Wbase 68.7 64.3 65.8 62.1 65.0 65.2 91.6 75.1 80.2 79.2 81.5\nmLUKE-Ebase 69.3 64.5 65.2 64.7 68.7 66.5 93.6 77.2 81.8 77.7 82.6\nXLM-K (Jiang et al., 2022) - - - - - - 90.7 73.3 80.0 76.6 80.1\nXLM-Rlarge 68.0 65.3 65.0 63.3 64.1 65.1 92.5 75.1 82.9 80.5 82.8\nmLUKE-Wlarge 66.2 65.3 68.1 66.5 64.7 66.2 92.3 76.5 82.6 80.7 83.0\nmLUKE-Elarge 68.1 65.8 67.8 66.4 64.4 66.5 94.0 78.3 83.5 81.4 84.3\nTable 3: F1 scores on relation extraction (RE) and named entity recognition (NER).\n4.2 Named Entity Recognition\nNamed Entity Recognition (NER) is the task to\ndetect entities in a sentence and classify their type.\nWe use the CoNLL-2003 English dataset (Tjong\nKim Sang and De Meulder, 2003) as the training\ndata, and evaluate the models with the CoNLL-\n2003 German dataset and the CoNLL-2002 Span-\nish and Dutch dataset (Tjong Kim Sang, 2002).\nModels. We adopt the model of Sohrab and Miwa\n(2018) as the baseline model, which enumerates all\npossible spans in a sentence and classiﬁes them into\nthe target entity types or non-entity type. In this\nexperiment, we enumerate spans with at most 16\ntokens. For the baseline models, the span features\nare computed as the concatenation of the word\nrepresentations of the ﬁrst and last tokens. The span\nfeatures are fed into a linear classiﬁer to predict\ntheir entity type.\nThe input of mLUKE-E contains the entity\n[MASK] tokens associated with all possible spans.\nThe span features are computed as the contextual-\nized representations of the entity [MASK] tokens.\nThe features are input to a linear classiﬁer as with\nthe word-based models.\n4.3 Main Results\nThe results are shown in Table 3. The mLUKE-E\nmodels outperform their word-based counterparts\nmLUKE-W in the average score in all the compara-\nble settings (the base and large settings; the RE and\nNER tasks), which shows entity-based features are\nuseful in cross-lingual tasks. We also observe that\nXLM-Rbase beneﬁts from extra training (1.8 aver-\nage points improvement in RE and 0.3 points in\nNER), but mLUKE-E still outperforms the results.\n4.4 Analysis\nThe performance gain of mLUKE-E over mLUKE-\nW can be partly explained as the entity [MASK]\nde es fr tr\nmLUKE-Wbase 0.71 0.74 0.74 0.84\nmLUKE-Ebase 0.25 0.28 0.24 0.36\nTable 4: The modularity of word and entity features\ncomputed with the same mLUKE model. The data are\nfrom pairs of English and the other languages in the\nRELX dataset.\ntoken extracts better features for predicting entity\nattributes because it resembles how mLUKE is pre-\ntrained with the MEP task. We hypothesize that\nthere exists another factor for the improvement in\ncross-lingual performance: language neutrality of\nrepresentations.\nThe entity [MASK] token is shared across lan-\nguages and their contextualized representations\nmay be less affected by the difference of input\nlanguages, resulting in features that generalize\nwell for cross-lingual transfer. To ﬁnd out if the\nentity-based features are actually more language-\nindependent than word-based features, we evaluate\nthe modularity (Fujinuma et al., 2019) of the fea-\ntures extracted for the RELX dataset.\nModularity is computed for the k-nearest neigh-\nbor graph of embeddings and measures the degree\nto which embeddings tend to form clusters within\nthe same language. We refer readers to Fujinuma\net al. (2019) for how to compute the metric. Note\nthat the maximum value of modularity is 1, and 0\nmeans the embeddings are completely randomly\ndistributed regardless of language.\nWe compare the modularity of the word fea-\ntures from mLUKE-Wbase and entity features from\nmLUKE-Ebase before ﬁne-tuning. Note that the\nfeatures here are concatenated vectors of head and\ntail features. Table 4 shows that the modularity of\nmLUKE-Ebase is much lower than mLUKE-Wbase,\nar en ﬁ fr id ja ru vi zh avg.\nmBERT 17.1 36.8 24.0 24.3 42.9 14.3 19.5 39.4 26.2 27.2\nXLM-Rbase 14.2 27.2 16.2 14.9 28.2 11.9 11.7 25.1 17.6 18.5\n+ extra training 21.2 35.0 23.0 22.2 46.8 19.6 17.5 34.4 30.7 27.8\nmLUKE-Wbase 22.3 31.3 18.4 19.6 46.7 18.4 16.7 31.9 29.3 26.1\nmLUKE-Ebase ([Y]) 27.8 37.5 30.4 28.4 44.2 28.9 25.8 42.1 33.4 33.2\nmLUKE-Ebase ([X] & [Y]) 42.4 47.5 44.2 35.9 56.2 40.3 35.5 55.2 46.7 44.9\nTable 5: The top-1 accuracies from 9 languages from the mLAMA dataset.\ndemonstrating that entity-based features are more\nlanguage-neutral. However, with entity-based fea-\ntures, the modularities are still greater than zero. In\nparticular, the modularity computed with Turkish,\nwhich is the most distant language from English\nhere, is signiﬁcantly higher than the others, indi-\ncating that the contextualized entity-based features\nare still somewhat language-dependent.\n5 Cloze Prompt Task with Entity\nRepresentations\nIn this section, we show that using the entity repre-\nsentations is effective in a cloze prompt task (Liu\net al., 2021) with the mLAMA dataset (Kassner\net al., 2021). The task is, given a cloze template\nsuch as “[X] was born in [Y]” with [X] ﬁlled\nwith an entity ( e.g., Mozart), to predict a correct\nentity in [Y] (e.g., Austria). We adopt the typed\nquerying setting (Kassner et al., 2021), where a\ntemplate has a set of candidate answer entities and\nthe prediction becomes the one with the highest\nscore assigned by the language model.\nModel. As in Kassner et al. (2021), the word-based\nbaseline models compute the candidate score as the\nlog-probability from the MLM classiﬁer. When\na candidate entity in [Y] is tokenized into multi-\nple tokens, the same number of the word [MASK]\ntokens are placed in the input sequence, and the\nscore is computed by taking the average of the log-\nprobabilities for its individual tokens.\nOn the other hand, mLUKE-E computes the log-\nprobability of the candidate entity in [Y] with\nthe entity [MASK] token. Each candidate entity\nis associated with an entity in mLUKE’s entity\nvocabulary via string matching. The input sequence\nhas the entity [MASK] token associated with the\nword [MASK] tokens in [Y], and the candidate\nscore is computed as the log-probability from the\nMEP classiﬁer. We also try additionally appending\nthe entity token of [X] to the input sequence if the\nentity is found in the vocabulary.\nTo accurately measure the difference between\nword-based and entity-based prediction, we restrict\nthe candidate entities to the ones found in the en-\ntity vocabulary and exclude the questions if their\nanswers are not included in the candidates (results\nwith full candidates and questions in the dataset are\nin Appendix G).\nResults. We experiment in total with 16 languages\nwhich are available both in the mLAMA dataset\nand the mLUKE’s entity vocabulary. Here we only\npresent the top-1 accuracy results from 9 languages\non Table 5, as we can make similar observations\nwith the other languages.\nWe observe that XLM-Rbase performs notably\nworse than mBERT as mentioned in Kassner et al.\n(2021). However, with extra training with the\nWikipedia corpus, XLM-Rbase shows a signiﬁcant\n9.3 points improvement in the average score and\noutperforms mBERT (27.8 vs. 27.2). We conjec-\nture that this shows the importance of the training\ncorpus for this task. The original XLM-R is only\ntrained with the CommonCrawl corpus (Conneau\net al., 2020a), text scraped from a wide variety of\nweb pages, while mBERT and XLM-R + training\nare trained on Wikipedia. The performance gaps\nindicate that Wikipedia is particularly useful for\nthe model to learn factual knowledge.\nThe mLUKE-Wbase model lags behind XLM-\nRbase + extra training by 1.7 average points but we\ncan see 5.4 points improvement from XLM-Rbase\n+ extra training to mLUKE-Ebase ([Y]), indicating\nentity representations are more suitable to elicit\ncorrect factual knowledge from mLUKE than word\nrepresentations. Adding the entity corresponding\nto [X] to the input (mLUKE-Ebase ([X] & [Y]))\nfurther pushes the performance by 11.7 points to\n44.9 %, which further demonstrates the effective-\nness of entity representations.\nAnalysis of Language Bias. Kassner et al. (2021)\nnotes that the prediction of mBERT is biased by\nthe input language. For example, when queried in\nItalian (e.g., “[X] e stato creato in[MASK].”), the\nmodel tends to predict entities that often appear in\nItalian text (e.g., Italy) for any question to answer\nen ja fr\nmBERT The Bahamas, 41% (355/870) Japan, 82% (361/439) Pays-Bas, 71% (632/895)\nXLM-Rbase London, 78% (664/850) Japan, 99% (437/440) Allemagne, 96% (877/916)\n+ extra training Australia, 27% (247/899) Japan, 99% (437/442) Allemagne, 93% (854/917)\nmLUKE-Wbase Germany, 22% (198/895) Japan, 97% (428/442) Allemagne, 99% (906/918)\nmLUKE-Ebase ([Y]) London, 37% (310/846) Japan, 56% (241/430) Suède, 40% (362/908)\nmLUKE-Ebase ([X] & [Y]) London, 27% (213/797) Japan, 44% (176/401) Suède, 30% (266/895)\nTable 6: The top incorrect predictions in three languages for the template “ [X] was founded in [Y].” for each\nmodel. The predictions in the original language are translated into English.\nlocation. We expect that using entity representa-\ntions would reduce language bias because entities\nare shared among languages and less affected by\nthe frequency in the language of questions.\nWe qualitatively assess the degree of language\nbias in the models looking at their incorrect pre-\ndictions. We show the top incorrect prediction for\nthe template “[X] was founded in [Y].” for each\nmodel in Table 6, together with the top-1 incor-\nrect ratio, that is, the ratio of the number of the\nmost common incorrect prediction to the total false\npredictions, which indicates how much the false\npredictions are dominated by few frequent entities.\nThe examples show that the different models ex-\nhibit bias towards different entities as in English\nand French, although in Japanese the model consis-\ntently tends to predictJapan. Looking at the degree\nof language bias, mLUKE-Ebase ([X] & [Y]) ex-\nhibits lower top-1 incorrect ratios overall (27% in\nfr, 44% in ja, and 30% in fr), which indicates us-\ning entity representations reduces language bias.\nHowever, lower language bias does not necessarily\nmean better performance: in French (fr), mLUKE-\nEbase ([X] & [Y]) gives a lower top-1 incorrect\nratio than mBERT (30% vs. 71%) but their num-\nbers of total false predictions are the same (895).\nLanguage bias is only one of several factors in the\nperformance bottleneck.\n6 Related Work\n6.1 Multilingual Pretrained Language\nModels\nMultilingual pretrained language models have re-\ncently seen a surge of interest due to their effective-\nness in cross-lingual transfer learning (Conneau\nand Lample, 2019; Liu et al., 2020). A straight-\nforward way to train such models is multilingual\nmasked language modeling (mMLM) (Devlin et al.,\n2019; Conneau et al., 2020a), i.e., training a single\nmodel with a collection of monolingual corpora\nin multiple languages. Although models trained\nwith mMLM exhibit a strong cross-lingual abil-\nity without any cross-lingual supervision (K et al.,\n2020; Conneau et al., 2020b), several studies aim\nto develop better multilingual models with explicit\ncross-lingual supervision such as bilingual word\ndictionaries (Conneau et al., 2020b) or parallel sen-\ntences (Conneau and Lample, 2019). In this study,\nwe build a multilingual pretrained language model\non the basis of XLM-RoBERTa (Conneau et al.,\n2020a), trained with mMLM as well as the masked\nentity prediction (MEP) (Yamada et al., 2020) with\nentity representations.\n6.2 Pretrained Language Models with Entity\nKnowledge\nLanguage models trained with a large corpus con-\ntain knowledge about real-world entities, which is\nuseful for entity-related downstream tasks such as\nrelation classiﬁcation, named entity recognition,\nand question answering. Previous studies have\nshown that we can improve language models for\nsuch tasks by incorporating entity information into\nthe model (Zhang et al., 2019; Peters et al., 2019;\nWang et al., 2021; Xiong et al., 2020; Févry et al.,\n2020; Yamada et al., 2020).\nWhen incorporated into multilingual language\nmodels, entity information can bring another ben-\neﬁt: entities may serve as anchors for the model\nto align representations across languages. Multi-\nlingual knowledge bases such as Wikipedia often\noffer mappings between different surface forms\nacross languages for the same entity. Calixto et al.\n(2021) ﬁne-tuned the top two layers of multilin-\ngual BERT by predicting language-agnostic en-\ntity ID from hyperlinks in Wikipedia articles. As\nour concurrent work, Jiang et al. (2022) trained\na model based on XLM-RoBERTa with an entity\nprediction task along with an object entailment pre-\ndiction task. While the previous studies focus on\nimproving cross-lingual language representations\nby pretraining with entity information, our work in-\nvestigates a multilingual model not only pretrained\nwith entities but also explicitly having entity repre-\nsentations and how to extract better features from\nsuch model.\n7 Conclusion\nWe investigated the effectiveness of entity repre-\nsentations in multilingual language models. Our\npretrained model, mLUKE, not only exhibits strong\nempirical results with the word inputs (mLUKE-W)\nbut also shows even better performance with the\nentity representations (mLUKE-E) in cross-lingual\ntransfer tasks. We also show that a cloze-prompt-\nstyle fact completion task can effectively be solved\nwith the query and answer space in the entity vocab-\nulary. Our results suggest a promising direction to\npursue further on how to leverage entity represen-\ntations in multilingual tasks. Also, in the current\nmodel, entities are represented as individual vec-\ntors, which may incur a large memory footprint in\npractice. One can investigate an efﬁcient way of\nhaving entity representations.\nReferences\nWasi Ahmad, Nanyun Peng, and Kai-Wei Chang. 2021.\nGate: Graph attention transformer encoder for cross-\nlingual relation and event extraction. InProceedings\nof the Thirty-Fifth AAAI Conference on Artiﬁcial In-\ntelligence.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the Cross-lingual Transferability of Mono-\nlingual Representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics.\nIacer Calixto, Alessandro Raganato, and Tommaso\nPasini. 2021. Wikipedia Entities as Rendezvous\nacross Languages: Grounding Multilingual Lan-\nguage Models by Predicting Wikipedia Hyperlinks.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics.\nSteven Cao, Nikita Kitaev, and Dan Klein. 2020. Mul-\ntilingual Alignment of Contextual Word Representa-\ntions. In International Conference on Learning Rep-\nresentations.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020a. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual Language Model Pretraining. In Advances in\nNeural Information Processing Systems, volume 32.\nAlexis Conneau, Shijie Wu, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020b. Emerg-\ning Cross-lingual Structure in Pretrained Language\nModels. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics.\nLeyang Cui, Yu Wu, Jian Liu, Sen Yang, and Yue\nZhang. 2021. Template-Based Named Entity Recog-\nnition Using BART. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1.\nSumanth Doddapaneni, Gowtham Ramesh, Anoop\nKunchukuttan, Pratyush Kumar, and Mitesh M.\nKhapra. 2021. A Primer on Pretrained Multilingual\nLanguage Models. ArXiv, abs/2107.00676.\nThibault Févry, Livio Baldini Soares, Nicholas FitzGer-\nald, Eunsol Choi, and Tom Kwiatkowski. 2020. En-\ntities as Experts: Sparse Memory Access with Entity\nSupervision. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning.\nYoshinari Fujinuma, Jordan Boyd-Graber, and\nMichael J. Paul. 2019. A Resource-Free Evalua-\ntion Metric for Cross-Lingual Word Embeddings\nBased on Graph Modularity. In Proceedings of\nthe 57th Annual Meeting of the Association for\nComputational Linguistics.\nXiaoze Jiang, Yaobo Liang, Weizhu Chen, and Nan\nDuan. 2022. XLM-K: Improving Cross-Lingual\nLanguage Model Pre-Training with Multilingual\nKnowledge. In Proceedings of the Thirty-Sixth\nAAAI Conference on Artiﬁcial Intelligence.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-Lingual Ability of Multilin-\ngual BERT: An Empirical Study. In International\nConference on Learning Representations.\nNora Kassner, Philipp Dufter, and Hinrich Schütze.\n2021. Multilingual LAMA: Investigating Knowl-\nedge in Multilingual Pretrained Language Models.\nIn Proceedings of the 16th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Main Volume.\nAbdullatif Köksal and Arzucan Özgür. 2020. The\nRELX Dataset and Matching the Multilingual\nBlanks for Cross-Lingual Relation Classiﬁcation. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A Lite BERT for Self-supervised\nLearning of Language Representations. In Interna-\ntional Conference on Learning Representations.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating Cross-lingual Extractive Question Answering.\nIn Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, Prompt, and Predict: A Systematic Survey of\nPrompting Methods in Natural Language Processing.\nArXiv, abs/2107.13586.\nY . Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, M. Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. ArXiv, abs/1907.11692.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual Denoising\nPre-training for Neural Machine Translation. Trans-\nactions of the Association for Computational Lin-\nguistics, 8:726–742.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nWeight Decay Regularization. In International Con-\nference on Learning Representations.\nMatthew E. Peters, Mark Neumann, Robert Logan, Roy\nSchwartz, Vidur Joshi, Sameer Singh, and Noah A.\nSmith. 2019. Knowledge Enhanced Contextual\nWord Representations. In Proceedings of the 2019\nConference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint\nConference on Natural Language Processing.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language Models as\nKnowledge Bases? In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. In Proceed-\nings of the 2016 Conference on Empirical Methods\nin Natural Language Processing.\nMohammad Golam Sohrab and Makoto Miwa. 2018.\nDeep Exhaustive Model for Nested Named Entity\nRecognition. In Proceedings of the 2018 Confer-\nence on Empirical Methods in Natural Language\nProcessing.\nAnanya Subburathinam, Di Lu, Heng Ji, Jonathan\nMay, Shih-Fu Chang, Avirup Sil, and Clare V oss.\n2019. Cross-lingual structure transfer for rela-\ntion and event extraction. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP).\nErik F. Tjong Kim Sang. 2002. Introduction to the\nCoNLL-2002 Shared Task: Language-Independent\nNamed Entity Recognition. In COLING-02: The\n6th Conference on Natural Language Learning\n2002.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 Shared Task:\nLanguage-Independent Named Entity Recognition.\nIn Proceedings of the Seventh Conference on Nat-\nural Language Learning at HLT-NAACL 2003.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Advances in Neural Information Pro-\ncessing Systems, volume 30.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhiyuan\nLiu, Juan-Zi Li, and J. Tang. 2021. KEPLER:\nA Uniﬁed Model for Knowledge Embedding and\nPre-trained Language Representation. Transactions\nof the Association for Computational Linguistics ,\n9:176–194.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained Encyclope-\ndia: Weakly Supervised Knowledge-Pretrained Lan-\nguage Model. In International Conference on Learn-\ning Representations.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: Deep\nContextualized Entity Representations with Entity-\naware Self-attention. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nDongxu Zhang and Dong Wang. 2015. Relation Clas-\nsiﬁcation via Recurrent Neural Network. ArXiv,\nabs/1508.01006.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: En-\nhanced Language Representation with Informative\nEntities. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics.\nAppendix for “mLUKE: The Power of Entity Representations in Multilingual Pretrained\nLanguage Models”\nA Details of Pretraining\nDataset. We download the Wikipedia dumps from December 1st, 2020. We show the 24 languages\nincluded in the dataset on Table 7, along with the data size and the number of entities in the vocabulary.\nLanguage Code Size # entities in vocab Language Code Size # entities in vocab\nar 851M 427,460 ko 537M 378,399\nbn 117M 62,595 nl 1.1G 483,277\nde 3.5G 540,347 pl 1.3G 489,109\nel 315M 135,277 pt 1.0G 537,028\nen 6.9G 613,718 ru 2.5G 529,171\nes 2.1G 587,525 sv 1.1G 390,313\nﬁ 480M 300,333 sw 27M 30,129\nfr 3.1G 630,355 te 66M 14,368\nhi 90M 54,038 th 153M 100,231\nid 327M 217,758 tr 326M 297,280\nit 1.9G 590,147 vi 516M 263,424\nja 2.3G 369,470 zh 955M 332,970\nTotal 31.4G 8,374,722\nTable 7: Training Data Statistics: the size of training data, and the number of entities found in the 1.2M entity\nvocabulary.\nOptimization. We optimize the mLUKE models for 1M steps in total using AdamW (Loshchilov and\nHutter, 2019) with learning rate warmup and linear decay of the learning rate. The pretraining consists\nof two stages: (1) in the ﬁrst 500K steps, we update only those parameters that are randomly initialized\n(e.g., entity embeddings); (2) we update all parameters in the remaining 500K steps. The learning rate\nscheduler is reset at each training stage. The detailed hyper-parameters are shown in Table 8.\nMaximum word length 512 Mask probability for entities 15%\nBatch size 2048 The size of word token embeddings 768\nPeak learning rate 1e-4 The size of entity token embeddings 256\nPeak learning rate (ﬁrst 500K steps) 5e-4 Dropout 0.1\nLearning rate decay linear Weight decay 0.01\nWarmup steps 2500 Adam β1 0.9\nMask probability for words 15% Adam β2 0.999\nRandom-word probability for words 10% Adam ϵ 1e-6\nUnmasked probability for words 10% Gradient clipping none\nTable 8: Hyper-parameters used to pretrain mLUKE.\nComputing Infrastructure. We run the pretraining on NVIDIA’s PyTorch Docker container 19.02 hosted\non a server with two Intel Xeon Platinum 8168 CPUs and 16 NVIDIA Tesla V100 GPUs. The training\ntakes approximately 2 months.\nB Details of Downstream Experiments\nHyperparameter Search. For each downstream task, we perform hyperparameter searching for all the\nmodels with the same computational budget to ensure a fair comparison. For each task, we use the ﬁnal\nevaluation metric on the validation split of the training English corpus as the validation score. The models\nare optimized with the AdamW optimizer (Loshchilov and Hutter, 2019) with the weight decay term set\nto 0.01 and a linear warmup scheduler. The learning rate is linearly increased to a speciﬁed value in the\nﬁrst 6 % of training steps, and then gradually decreased to zero towards the end. Table 9 summarizes the\ntask-speciﬁc hyperparameter search spaces.\nQA\n(SQuAD)\nRelation Classiﬁcation\n(KBP37)\nNER\n(CoNLL 2003)\nLearning rate 2e-5 2e-5 2e-5\nBatch size {16, 32} {4, 8, 16} {4, 8, 16}\nEpochs 2 5 5\n# of random seeds 3 3 3\nValidation metric F1 F1 F1\nTable 9: The hyperparameters search spaces and other details of downstream experiments.\nComputing Infrastructure. We run the ﬁne-tuning on a server with a Intel(R) Core(TM) i7-6950X CPU\nand 4 NVIDIA GeForce RTX 3090 GPUs.\nC Detecting Entities in the QA datasets\nFor each question–passage pair in the QA datasets, we ﬁrst create a mapping from the entity mention\nstrings (e.g., “U.S.”) to their referent Wikipedia entities (e.g., United States) using the entity hyperlinks on\nthe source Wikipedia page of the passage. We then perform simple string matching to extract all entity\nnames in the question and the passage and treat all matched entity names as entity annotations for their\nreferent entities. We ignore an entity name if the name refers to multiple entities on the page. Further, to\nreduce noise, we also exclude an entity name if its link probability, the probability that the name appears\nas a hyperlink in Wikipedia, is lower than 1%.\nThe XQuAD datasets are created by translating English Wikipedia articles into target languages. For\neach translated article, we create the mention-entity mapping from the source English article by the\nfollowing procedure: for all the entities found in the source article, we ﬁnd the corresponding entity in the\ntarget language through inter-language links, and then collect its possible mention strings (i.e., hyperlinks\nto the entity) from a Wikipedia dump of the target language; the entity and the collected mention strings\nform the mention-entity mapping for the translated article.\nD The Model Size\n# of layers hidden size # of heads vocabulary size # of parameters\nmBERT 12 768 12 120K 177M\nXLM-Rbase 12 768 8 250K 278M\nmLUKE-Ebase 12 768 8 250K 585M\nXLM-Rlarge 24 1024 16 250K 559M\nmLUKE-Elarge 24 1024 16 250K 867M\nTable 10: The model sizes of the pretrained models.\nE Ablation Study of Entity Embeddings\nIn Section 3 and 4, we have shown that using entity representations in mLUKE improves the cross-\nlingual transfer performance in QA, RE, and NER. Here we conduct an additional ablation study to\ninvestigate whether the learned entity embeddings are crucial to the success of our approach. We train an\nablated model of mLUKE-E whose entity embeddings are re-initialized randomly before ﬁne-tuning (-\nablation). Table 11 and Table 12 show that the ablated model performs signiﬁcantly worse than the full\nmodel (mLUKE-E), indicating that using pretrained entity embeddings is crucial rather than applying our\napproach during ﬁne-tuning in an ad-hoc manner without entity-aware pretraining.\nXQuAD en es de el ru tr ar vi th zh hi avg.\nmLUKE-E 86.3 78.9 78.9 73.9 76.0 68.8 71.4 76.4 67.5 65.9 72.2 74.2\n- ablation 84.3 76.8 76.4 71.9 74.3 67.4 70.2 75.3 67.1 64.4 68.4 72.4\nMLQA en es de ar hi vi zh avg. G-XLT avg.\nmLUKE-Ebase 80.8 70.0 65.5 60.8 63.7 68.4 66.2 67.9 55.6\n- ablation 80.3 69.4 64.5 59.1 59.2 66.5 63.6 66.1 50.7\nTable 11: F1 scores on the XQuAD and MLQA datasets in the cross-lingual transfer settings.\nRE NER\nen de es fr tr avg. en de du es avg.\nmLUKE-Ebase 69.3 64.5 65.2 64.7 68.7 66.5 93.6 77.2 81.8 77.7 82.6\n- ablation 62.5 59.3 60.7 61.0 60.5 50.8 93.0 76.3 80.8 76.1 81.6\nTable 12: F1 scores on relation extraction (RE) and named entity recognition (NER).\nF Full Results of MLQA\nc/q en es de ar hi vi zh\nen 79.1 65.4 63.4 37.9 29.7 47.1 43.2\nes 67.7 65.9 58.2 38.2 24.4 43.6 39.5\nde 61.7 55.9 58.6 32.3 29.7 38.4 36.8\nar 49.9 43.2 44.6 48.6 23.4 29.4 27.1\nhi 47.0 37.8 39.1 26.2 44.8 28.0 23.0\nvi 59.9 49.4 48.6 26.7 25.6 58.5 40.7\nzh 55.3 44.2 45.3 28.3 22.7 38.7 58.1\nTable 13: MLQA full results of mBERT\n.\nc/q en es de ar hi vi zh\nen 79.6 52.3 59.6 30.8 43.2 40.0 36.0\nes 67.0 67.7 52.0 25.2 31.8 32.9 31.5\nde 59.5 41.7 62.1 22.2 27.8 29.2 29.5\nar 49.6 23.2 30.9 55.8 10.6 11.6 10.3\nhi 58.5 34.6 42.3 17.8 59.8 22.4 23.0\nvi 61.1 28.1 39.5 17.0 27.5 65.2 26.5\nzh 55.2 22.7 28.1 9.26 21.1 17.5 62.4\nTable 14: MLQA full results of XLM-Rbase\nc/q en es de ar hi vi zh\nen 81.3 71.2 70.1 40.6 52.3 54.8 48.2\nes 70.6 69.8 66.2 43.3 47.9 52.8 49.0\nde 64.4 60.4 64.9 36.8 42.3 44.3 42.9\nar 59.3 52.3 52.2 54.8 30.3 37.1 31.5\nhi 65.0 56.5 56.8 33.8 59.3 43.0 39.9\nvi 67.0 57.1 58.2 31.7 43.8 65.5 44.0\nzh 62.4 53.7 54.2 33.3 40.2 44.8 64.2\nTable 15: MLQA full results of XLM-Rbase + train-\ning\nc/q en es de ar hi vi zh\nen 81.2 69.5 69.1 53.6 60.8 60.4 58.4\nes 70.3 69.6 65.5 52.1 52.9 56.1 56.4\nde 64.7 59.8 65.3 45.4 48.9 49.9 49.3\nar 60.4 52.3 54.3 60.3 34.0 43.4 41.3\nhi 65.5 56.9 58.3 35.4 63.1 49.0 44.6\nvi 66.8 54.4 57.1 39.7 49.3 68.3 52.4\nzh 63.2 55.1 56.6 39.8 43.3 49.6 66.1\nTable 16: MLQA full results of mLUKE-Wbase\nc/q en es de ar hi vi zh\nen 80.8 71.3 69.9 55.9 61.9 62.8 62.1\nes 70.6 69.9 66.4 52.6 53.7 57.6 58.0\nde 65.2 61.2 65.4 47.2 49.3 51.8 51.7\nar 61.1 54.6 56.9 60.7 39.5 47.0 44.8\nhi 65.1 58.4 59.2 38.3 63.7 50.5 46.2\nvi 66.7 56.5 59.5 44.3 51.1 68.4 54.2\nzh 62.7 56.3 56.2 41.1 44.3 51.7 66.2\nTable 17: MLQA full results of mLUKE-Ebase\nc/q en es de ar hi vi zh\nen 83.9 79.6 79.0 62.0 70.6 70.5 69.5\nes 75.2 74.7 73.0 60.3 63.4 66.6 65.9\nde 69.4 69.0 69.9 58.9 59.7 62.0 60.6\nar 67.0 63.6 66.2 64.9 54.5 58.9 57.7\nhi 72.1 67.3 67.2 56.1 69.9 61.0 62.1\nvi 73.5 69.6 70.7 57.1 63.0 73.3 64.5\nzh 69.1 64.0 65.7 53.4 58.2 62.7 70.3\nTable 18: MLQA full results of XLM-Rlarge\n.\nc/q en es de ar hi vi zh\nen 84.0 80.1 79.9 71.5 74.2 72.8 72.8\nes 74.6 74.3 74.6 65.5 64.3 66.0 66.0\nde 70.1 69.5 70.3 63.9 60.8 61.7 62.6\nar 67.9 65.0 67.9 66.2 58.6 60.2 58.7\nhi 72.9 69.7 70.3 60.8 70.2 63.1 62.6\nvi 73.9 69.5 72.2 65.5 64.9 74.2 67.3\nzh 69.6 66.5 68.5 61.5 58.3 64.5 69.7\nTable 19: MLQA full results of mLUKE-Wlarge\nc/q en es de ar hi vi zh\nen 84.1 80.5 80.2 70.0 75.0 75.0 73.5\nes 75.2 74.5 74.8 62.4 65.3 67.6 66.5\nde 71.1 70.2 70.5 62.2 61.0 63.5 62.3\nar 68.4 65.6 68.4 66.2 57.7 62.3 58.0\nhi 72.9 70.9 71.6 59.1 71.4 65.6 62.1\nvi 74.7 71.0 73.1 61.7 64.7 74.3 66.8\nzh 70.1 66.1 68.8 59.2 60.9 66.3 70.5\nTable 20: MLQA full results of mLUKE-Elarge\nG Full Results of mLAMA\nTable 5 shows the results from the setting where the entity candidates not in the mLUKE’s entity vocabulary\nare excluded. Here we provide in Table 21 the results with the full candidate set provided in the dataset\nfor ease of comparison with other literature. When the candidate entity is not found in the mLUKE’s\nentity vocabulary, the log-probability from the word [MASK] tokens are used instead.\nar bn de el en es ﬁ fr\nmBERT 15.1 12.7 28.6 19.4 34.8 30.2 19.2 27.1\nXLM-Rbase 14.9 7.5 18.4 12.7 24.2 18.5 14.5 16.1\n+ extra training 20.7 14.0 29.3 18.2 31.6 26.4 19.2 25.0\nmLUKE-Wbase 21.3 12.9 25.7 17.5 27.1 23.3 15.9 23.0\nmLUKE-Ebase ([Y]) 25.6 21.6 32.9 25.2 34.9 28.5 24.7 27.7\nmLUKE-Ebase ([X] & [Y]) 37.3 32.3 43.7 34.4 43.2 36.4 35.3 34.2\nid ja ko pl pt ru vi zh avg.\nmBERT 37.4 14.2 17.8 21.9 32.0 17.4 36.5 24.2 24.3\nXLM-Rbase 24.6 11.4 10.9 16.6 22.2 12.6 23.0 15.5 16.5\n+ extra training 38.2 19.1 21.4 20.5 29.6 20.6 33.8 28.1 24.7\nmLUKE-Wbase 36.6 18.0 17.9 20.2 29.4 19.6 31.0 26.9 22.9\nmLUKE-Ebase ([Y]) 35.3 27.2 26.3 25.7 34.7 23.8 39.1 29.5 28.9\nmLUKE-Ebase ([X] & [Y]) 47.6 37.7 41.6 37.7 44.8 31.4 50.1 41.6 39.3\nTable 21: The average of Top-1 accuracies from 16 languages from the mLAMA dataset.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9022936224937439
    },
    {
      "name": "Natural language processing",
      "score": 0.7502442598342896
    },
    {
      "name": "Exploit",
      "score": 0.6692823171615601
    },
    {
      "name": "Task (project management)",
      "score": 0.6564720273017883
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6495944261550903
    },
    {
      "name": "Language model",
      "score": 0.5940529704093933
    },
    {
      "name": "Word (group theory)",
      "score": 0.5428526401519775
    },
    {
      "name": "Entity linking",
      "score": 0.5212150812149048
    },
    {
      "name": "Key (lock)",
      "score": 0.5189864039421082
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.48019111156463623
    },
    {
      "name": "Question answering",
      "score": 0.4116646647453308
    },
    {
      "name": "Linguistics",
      "score": 0.2025345265865326
    },
    {
      "name": "Knowledge base",
      "score": 0.10685282945632935
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ]
}