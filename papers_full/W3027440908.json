{
    "title": "BERTweet: A pre-trained language model for English Tweets",
    "url": "https://openalex.org/W3027440908",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A2103676582",
            "name": "Dat Quoc Nguyen",
            "affiliations": [
                "VinUniversity"
            ]
        },
        {
            "id": "https://openalex.org/A2087773565",
            "name": "Thanh Tra Vu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1983959534",
            "name": "Anh Tuan Nguyen",
            "affiliations": [
                "Nvidia (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2933138175",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2016443085",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W2951221758",
        "https://openalex.org/W3033999739",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2898697840",
        "https://openalex.org/W2250263931",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2588986918",
        "https://openalex.org/W2807333695",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W2760505947",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W3099342932",
        "https://openalex.org/W2756655895",
        "https://openalex.org/W157541337",
        "https://openalex.org/W2153848201",
        "https://openalex.org/W1521626219",
        "https://openalex.org/W371426616",
        "https://openalex.org/W2888421737",
        "https://openalex.org/W2798264625",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2757947833",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2750747353",
        "https://openalex.org/W2916132663",
        "https://openalex.org/W2101761627",
        "https://openalex.org/W2806834924",
        "https://openalex.org/W2970771982",
        "https://openalex.org/W2172112754",
        "https://openalex.org/W3034238904",
        "https://openalex.org/W2157765050",
        "https://openalex.org/W2759245808",
        "https://openalex.org/W2964078775",
        "https://openalex.org/W2963626623",
        "https://openalex.org/W2963119602"
    ],
    "abstract": "We present BERTweet, the first public large-scale pre-trained language model for English Tweets. Our BERTweet, having the same architecture as BERT-base (Devlin et al., 2019), is trained using the RoBERTa pre-training procedure (Liu et al., 2019). Experiments show that BERTweet outperforms strong baselines RoBERTa-base and XLM-R-base (Conneau et al., 2020), producing better performance results than the previous state-of-the-art models on three Tweet NLP tasks: Part-of-speech tagging, Named-entity recognition and text classification. We release BERTweet under the MIT License to facilitate future research and applications on Tweet data. Our BERTweet is available at https://github.com/VinAIResearch/BERTweet",
    "full_text": "Proceedings of the 2020 EMNLP (Systems Demonstrations), pages 9–14\nNovember 16-20, 2020.c⃝2020 Association for Computational Linguistics\n9\nBERTweet: A pre-trained language model for English Tweets\nDat Quoc Nguyen1, Thanh Vu2,∗and Anh Tuan Nguyen3,†\n1VinAI Research, Vietnam; 2Oracle Digital Assistant, Oracle, Australia; 3NVIDIA, USA\nv.datnq9@vinai.io; thanh.v.vu@oracle.com; tuananhn@nvidia.com\nAbstract\nWe present BERTweet, the ﬁrst public large-\nscale pre-trained language model for English\nTweets. Our BERTweet, having the same ar-\nchitecture as BERTbase (Devlin et al., 2019), is\ntrained using the RoBERTa pre-training pro-\ncedure (Liu et al., 2019). Experiments show\nthat BERTweet outperforms strong baselines\nRoBERTabase and XLM-Rbase (Conneau et al.,\n2020), producing better performance results\nthan the previous state-of-the-art models on\nthree Tweet NLP tasks: Part-of-speech tag-\nging, Named-entity recognition and text clas-\nsiﬁcation. We release BERTweet under the\nMIT License to facilitate future research and\napplications on Tweet data. Our BERTweet\nis available at: https://github.com/\nVinAIResearch/BERTweet.\n1 Introduction\nThe language model BERT (Devlin et al., 2019)—\nthe Bidirectional Encoder Representations from\nTransformers (Vaswani et al., 2017)—and its vari-\nants have successfully helped produce new state-\nof-the-art performance results for various NLP\ntasks. Their success has largely covered the com-\nmon English domains such as Wikipedia, news\nand books. For speciﬁc domains such as biomed-\nical or scientiﬁc, we could retrain a domain-\nspeciﬁc model using the BERTology architecture\n(Beltagy et al., 2019; Lee et al., 2019; Gururangan\net al., 2020).\nTwitter has been one of the most popular micro-\nblogging platforms where users can share real-\ntime information related to all kinds of topics and\nevents. The enormous and plentiful Tweet data\nhas been proven to be a widely-used and real-time\nsource of information in various important ana-\nlytic tasks (Ghani et al., 2019). Note that the char-\nacteristics of Tweets are generally different from\n∗Most of the work done when Thanh Vu was at the Aus-\ntralian e-Health Research Centre, CSIRO, Australia.\n†Work done during internship at VinAI Research.\nthose of traditional written text such as Wikipedia\nand news articles, due to the typical short length of\nTweets and frequent use of informal grammar as\nwell as irregular vocabulary e.g. abbreviations, ty-\npographical errors and hashtags (Eisenstein, 2013;\nHan et al., 2013). Thus this might lead to a chal-\nlenge in applying existing language models pre-\ntrained on large-scale conventional text corpora\nwith formal grammar and regular vocabulary to\nhandle text analytic tasks on Tweet data. To the\nbest of our knowledge, there is not an existing lan-\nguage model pre-trained on a large-scale corpus of\nEnglish Tweets.\nTo ﬁll the gap, we train the ﬁrst large-scale lan-\nguage model for English Tweets using a 80GB\ncorpus of 850M English Tweets. Our model uses\nthe BERTbase model conﬁguration, trained based\non the RoBERTa pre-training procedure (Liu et al.,\n2019). We evaluate our model and compare it with\nstrong competitors, i.e. RoBERTa base and XLM-\nRbase (Conneau et al., 2020), on three downstream\nTweet NLP tasks: Part-of-speech (POS) tagging,\nNamed-entity recognition (NER) and text classi-\nﬁcation. Experiments show that our model out-\nperforms RoBERTabase and XLM-Rbase as well as\nthe previous state-of-the-art (SOTA) models on all\nthese tasks. Our contributions are as follows:\n• We present the ﬁrst large-scale pre-trained lan-\nguage model for English Tweets.\n• Our model does better than its competitors\nRoBERTabase and XLM-Rbase and outperforms\nprevious SOTA models on three downstream\nTweet NLP tasks of POS tagging, NER and text\nclassiﬁcation, thus conﬁrming the effectiveness\nof the large-scale and domain-speciﬁc language\nmodel pre-trained for English Tweets.\n• We also provide the ﬁrst set of experiments in-\nvestigating whether a commonly used approach\nof applying lexical normalization dictionaries\non Tweets (Han et al., 2012) would help im-\n10\nprove the performance of the pre-trained lan-\nguage models on the downstream tasks.\n• We publicly release our model under the name\nBERTweet which can be used with fairseq\n(Ott et al., 2019) and transformers (Wolf\net al., 2019). We hope that BERTweet can serve\nas a strong baseline for future research and ap-\nplications of Tweet analytic tasks.\n2 BERTweet\nIn this section, we outline the architecture, and de-\nscribe the pre-training data and optimization setup\nthat we use for BERTweet.\nArchitecture\nOur BERTweet uses the same architecture as\nBERTbase, which is trained with a masked lan-\nguage modeling objective (Devlin et al., 2019).\nBERTweet pre-training procedure is based on\nRoBERTa (Liu et al., 2019) which optimizes the\nBERT pre-training approach for more robust per-\nformance. Given the widespread usage of BERT\nand RoBERTa, we do not detail the architecture\nhere. See Devlin et al. (2019) and Liu et al. (2019)\nfor more details.\nPre-training data\nWe use an 80GB pre-training dataset of uncom-\npressed texts, containing 850M Tweets (16B word\ntokens). Here, each Tweet consists of at least 10\nand at most 64 word tokens. In particular, this\ndataset is a concatenation of two corpora:\n• We ﬁrst download the general Twitter Stream\ngrabbed by the Archive Team, 1 containing\n4TB of Tweet data streamed from 01/2012 to\n08/2019 on Twitter. To identify English Tweets,\nwe employ the language identiﬁcation compo-\nnent of fastText (Joulin et al., 2017). We to-\nkenize those English Tweets using “TweetTo-\nkenizer” from the NLTK toolkit (Bird et al.,\n2009) and use the emoji package to translate\nemotion icons into text strings (here, each icon\nis referred to as a word token).2 We also normal-\nize the Tweets by converting user mentions and\nweb/url links into special tokens @USER and\nHTTPURL, respectively. We ﬁlter out retweeted\nTweets and the ones shorter than 10 or longer\nthan 64 word tokens. This pre-process results in\nthe ﬁrst corpus of 845M English Tweets.\n1https://archive.org/details/\ntwitterstream\n2https://pypi.org/project/emoji\n• We also stream Tweets related to the COVID-19\npandemic, available from 01/2020 to 03/2020.3\nWe apply the same data pre-process step as de-\nscribed above, thus resulting in the second cor-\npus of 5M English Tweets.\nWe then applyfastBPE (Sennrich et al., 2016)\nto segment all 850M Tweets with subword units,\nusing a vocabulary of 64K subword types. On av-\nerage there are 25 subword tokens per Tweet.\nOptimization\nWe utilize the RoBERTa implementation in the\nfairseq library (Ott et al., 2019). We set a\nmaximum sequence length at 128, thus generating\n850M × 25 / 128 ≈ 166M sequence blocks. Fol-\nlowing Liu et al. (2019), we optimize the model\nusing Adam (Kingma and Ba, 2014), and use a\nbatch size of 7K across 8 V100 GPUs (32GB each)\nand a peak learning rate of 0.0004. We pre-train\nBERTweet for 40 epochs in about 4 weeks (here,\nwe use the ﬁrst 2 epochs for warming up the learn-\ning rate), equivalent to 166M × 40 / 7K ≈ 950K\ntraining steps.\n3 Experimental setup\nWe evaluate and compare the performance of\nBERTweet with strong baselines on three down-\nstream NLP tasks of POS tagging, NER and text\nclassiﬁcation, using benchmark Tweet datasets.\nDownstream task datasets\nFor POS tagging, we use three datasets Ritter11-\nT-POS (Ritter et al., 2011), ARK-Twitter 4 (Gim-\npel et al., 2011; Owoputi et al., 2013) and\nTWEEBANK -V25 (Liu et al., 2018). For NER, we\nemploy datasets from the WNUT16 NER shared\ntask (Strauss et al., 2016) and the WNUT17 shared\ntask on novel and emerging entity recognition\n(Derczynski et al., 2017). For text classiﬁcation,\nwe employ the 3-class sentiment analysis dataset\nfrom the SemEval2017 Task 4A (Rosenthal et al.,\n2017) and the 2-class irony detection dataset from\nthe SemEval2018 Task 3A (Van Hee et al., 2018).\nFor Ritter11-T-POS, we employ a 70/15/15\ntraining/validation/test pre-split available from\nGui et al. (2017). 6 ARK-Twitter contains two\n3We collect Tweets containing at least one of 11 COVID-\n19 related keywords, e.g. covid19, coronavirus, sars-cov-2.\n4https://code.google.com/archive/p/\nark-tweet-nlp/downloads (twpos-data-v0.3.tgz)\n5https://github.com/Oneplus/Tweebank\n6https://github.com/guitaowufeng/TPANN\n11\nﬁles daily547.conll and oct27.conll in\nwhich oct27.conll is further split into ﬁles\noct27.traindev and oct27.test. Fol-\nlowing Owoputi et al. (2013) and Gui et al.\n(2017), we employ daily547.conll as a test\nset. In addition, we use oct27.traindev and\noct27.test as training and validation sets, re-\nspectively. For the T WEEBANK -V2, WNUT16\nand WNUT17 datasets, we use their existing\ntraining/validation/test split. The SemEval2017-\nTask4A and SemEval2018-Task3A datasets are\nprovided with training and test sets only (i.e. there\nis not a standard split for validation), thus we sam-\nple 10% of the training set for validation and use\nthe remaining 90% for training.\nWe use a “soft” normalization strategy to all of\nthe experimental datasets by translating word to-\nkens of user mentions and web/url links into spe-\ncial tokens @USER and HTTPURL, respectively,\nand converting emotion icon tokens into corre-\nsponding strings. We also apply a “hard” strategy\nby further applying lexical normalization dictio-\nnaries (Aramaki, 2010; Liu et al., 2012; Han et al.,\n2012) to normalize word tokens in Tweets.\nFine-tuning\nFollowing Devlin et al. (2019), for POS tagging\nand NER, we append a linear prediction layer on\ntop of the last Transformer layer of BERTweet\nwith regards to the ﬁrst subword of each word to-\nken, while for text classiﬁcation we append a lin-\near prediction layer on top of the pooled output.\nWe employ the transformers library (Wolf\net al., 2019) to independently ﬁne-tune BERTweet\nfor each task and each dataset in 30 training\nepochs. We use AdamW (Loshchilov and Hut-\nter, 2019) with a ﬁxed learning rate of 1.e-5 and\na batch size of 32 (Liu et al., 2019). We compute\nthe task performance after each training epoch on\nthe validation set (here, we apply early stopping\nwhen no improvement is observed after 5 continu-\nous epochs), and select the best model checkpoint\nto compute the performance score on the test set.\nWe repeat this ﬁne-tuning process 5 times with\ndifferent random seeds, i.e. 5 runs for each task\nand each dataset. We report each ﬁnal test result\nas an average over the test scores from the 5 runs.\nBaselines\nOur main competitors are the pre-trained lan-\nguage models RoBERTa base (Liu et al., 2019)\nand XLM-R base (Conneau et al., 2020), which\nModel Ritter11 ARK TB- V2\nsoft hard soft hard soft hard\nOur results\nRoBERTalarge 91.7 91.5 93.7 93.2 94.9 94.6\nXLM-Rlarge 92.6 92.1 94.2 93.8 95.5 95.1\nRoBERTabase 88.7 88.3 91.8 91.6 93.7 93.5\nXLM-Rbase 90.4 90.392.8 92.6 94.7 94.3\nBERTweet 90.1 89.5 94.1 93.4 95.2 94.7\nDCNN (Gui et al.) 89.9\nDCNN (Gui et al.) 91.2 [+a] 92.4 [+a+b]\nTPANN 90.9 [+a] 92.8 [+a+b]\nARKtagger 90.4 93.2 [+b] 94.6 [+c]\nBiLSTM-CNN-CRF 92.5 [+c]\nTable 1: POS tagging accuracy results on the\nRitter11-T-POS (Ritter11), ARK-Twitter (ARK) and\nTWEEBANK -V2 (TB-v2) test sets. Result of ARK-\ntagger (Owoputi et al., 2013) on Ritter11 is reported\nin the TPANN paper (Gui et al., 2017). Note that\nRitter11 uses Twitter-speciﬁc POS tags for retweeted\n(RT), user-account, hashtag and url word tokens which\ncan be tagged perfectly using some simple regular ex-\npressions. Therefore, we follow Gui et al. (2017) and\nGui et al. (2018) to tag those words appropriately for\nall models. Results of ARKtagger and BiLSTM-CNN-\nCRF (Ma and Hovy, 2016) on TB-v2 are reported by\nLiu et al. (2018). Also note that “+a”, “+b” and “+c”\ndenote the additional use of extra training data, i.e.\nmodels trained on bigger training data. “+a”: addi-\ntional use of the POS annotated data from the En-\nglish WSJ Penn treebank sections 00-24 (Marcus et al.,\n1993). “+b”: the use of both training and validation\nsets for learning models. “+c”: additional use of the\nPOS annotated data from the UD English-EWT train-\ning set (Silveira et al., 2014).\nhave the same architecture conﬁguration as our\nBERTweet. In addition, we also evaluate the pre-\ntrained RoBERTalarge and XLM-Rlarge although it\nis not a fair comparison due to their signiﬁcantly\nlarger model conﬁgurations.\nThe pre-trained RoBERTa is a strong language\nmodel for English, learned from 160GB of texts\ncovering books, Wikipedia, CommonCrawl news,\nCommonCrawl stories, and web text contents.\nXLM-R is a cross-lingual variant of RoBERTa,\ntrained on a 2.5TB multilingual corpus which con-\ntains 301GB of English CommonCrawl texts.\nWe ﬁne-tune RoBERTa and XLM-R using the\nsame ﬁne-tuning approach we use for BERTweet.\n4 Experimental results\nMain results\nTables 1, 2, 3 and 4 present our obtained scores for\nBERTweet and baselines regarding both “soft” and\n“hard” normalization strategies. We ﬁnd that for\n12\nModel\nWNUT16 WNUT17\nsoft hard entity surface\nsoft hard soft hard\nOur results\nRoBERTalarge 55.4 54.8 56.9 57.0 55.6 55.6\nXLM-Rlarge 55.8 55.3 57.1 57.5 55.9 56.4\nRoBERTabase 49.7 49.2 52.2 52.0 51.2 51.0\nXLM-Rbase 49.9 49.4 53.5 53.0 51.9 51.6\nBERTweet 52.1 51.3 56.5 55.6 55.1 54.1\nCambridgeLTL 52.4 [+b]\nDATNet (Zhou et al.) 53.0 [+b] 42.3\nAguilar et al. (2017) 41.9 40.2\nTable 2: F1 scores on the WNUT16 and WNUT17 test\nsets. CambridgeLTL result is reported by Limsopatham\nand Collier (2016). “entity” and “surface” denote the\nscores computed for the standard entity level and the\nsurface level (Derczynski et al., 2017), respectively.\nModel AvgRec F 1NP Accuracy\nsoft hard soft hard soft hard\nOur results\nRoBERTalarge 72.5 72.2 72.0 71.8 70.7 71.3\nXLM-Rlarge 71.7 71.7 71.1 70.9 70.7 70.6\nRoBERTabase 71.6 71.8 71.2 71.2 71.6 70.9\nXLM-Rbase 70.3 70.3 69.4 69.6 69.3 69.7\nBERTweet 73.2 72.8 72.8 72.5 71.7 72.0\nCliche (2017) 68.1 68.5 65.8\nBaziotis et al. (2017) 68.1 67.7 65.1\nTable 3: Performance scores on the SemEval2017-\nTask4A test set. See Rosenthal et al. (2017) for the\ndeﬁnitions of the AvgRec and F 1NP metrics, in which\nAvgRec is the main ranking metric.\nModel F1pos Accuracy\nsoft hard soft hard\nOur results\nRoBERTalarge 73.2 71.9 76.5 75.1\nXLM-Rlarge 70.8 69.7 74.2 73.2\nRoBERTabase 71.0 71.2 74.0 74.0\nXLM-Rbase 66.6 66.2 70.8 70.8\nBERTweet 74.6 74.3 78.2 78.2\nWu et al. (2018) 70.5 73.5\nBaziotis et al. (2018) 67.2 73.2\nTable 4: Performance scores on the SemEval2018-\nTask3A test set. F 1pos—the main ranking metric—\ndenotes the F1 score computed for the positive label.\neach pre-trained language model the “soft” scores\nare generally higher than the corresponding “hard”\nscores, i.e. applying lexical normalization dictio-\nnaries to normalize word tokens in Tweets gener-\nally does not help improve the performance of the\npre-trained language models on downstream tasks.\nOur BERTweet outperforms its main competi-\ntors RoBERTabase and XLM-R base on all exper-\nimental datasets (with only one exception that\nXLM-Rbase does slightly better than BERTweet on\nRitter11-T-POS). Compared to RoBERTalarge and\nXLM-Rlarge which use signiﬁcantly larger model\nconﬁgurations, we ﬁnd that they obtain better POS\ntagging and NER scores than BERTweet. How-\never, BERTweet performs better than those large\nmodels on the two text classiﬁcation datasets.\nTables 1, 2, 3 and 4 also compare our obtained\nscores with the previous highest reported results\non the same test sets. Clearly, the pre-trained lan-\nguage models help achieve new SOTA results on\nall experimental datasets. Speciﬁcally, BERTweet\nimproves the previous SOTA in the novel and\nemerging entity recognition by absolute 14+% on\nthe WNUT17 dataset, and in text classiﬁcation\nby 5% and 4% on the SemEval2017-Task4A and\nSemEval2018-Task3A test sets, respectively. Our\nresults conﬁrm the effectiveness of the large-scale\nBERTweet for Tweet NLP.\nDiscussion\nOur results comparing the “soft” and “hard” nor-\nmalization strategies with regards to the pre-\ntrained language models conﬁrm the previous\nview that lexical normalization on Tweets is a\nlossy translation task (Owoputi et al., 2013). We\nﬁnd that RoBERTa outperforms XLM-R on the\ntext classiﬁcation datasets. This ﬁnding is similar\nto what is found in the XLM-R paper (Conneau\net al., 2020) where XLM-R obtains lower perfor-\nmance scores than RoBERTa for sequence classiﬁ-\ncation tasks on traditional written English corpora.\nWe also recall that although RoBERTa and\nXLM-R use 160 / 80 = 2 times and 301 / 80≈ 3.75\ntimes bigger English data than our BERTweet, re-\nspectively, BERTweet does better than its com-\npetitors RoBERTabase and XLM-Rbase. Thus this\nconﬁrms the effectiveness of a large-scale and\ndomain-speciﬁc pre-trained language model for\nEnglish Tweets. In future work, we will release a\n“large” version of BERTweet, which possibly per-\nforms better than RoBERTa large and XLM-R large\non all three evaluation tasks.\n5 Conclusion\nWe have presented the ﬁrst large-scale language\nmodel BERTweet pre-trained for English Tweets.\nWe demonstrate the usefulness of BERTweet by\nshowing that BERTweet outperforms its baselines\nRoBERTabase and XLM-Rbase and helps produce\nbetter performances than the previous SOTA mod-\nels for three downstream Tweet NLP tasks of POS\ntagging, NER, and text classiﬁcation (i.e. senti-\nment analysis & irony detection).\n13\nAs of September 2020, we have collected a\ncorpus of about 23M “cased” COVID-19 English\nTweets consisting of at least 10 and at most 64\nword tokens. In addition, we also create an “un-\ncased” version of this corpus. Then we con-\ntinue pre-training from our pre-trained BERTweet\non each of the “cased” and “uncased” corpora of\n23M Tweets for 40 additional epochs, resulting\nin two BERTweet variants of pre-trained “cased”\nand “uncased” BERTweet-COVID19 models, re-\nspectively. By publicly releasing BERTweet and\nits two variants, we hope that they can foster future\nresearch and applications of Tweet analytic tasks,\nsuch as identifying informative COVID-19 Tweets\n(Nguyen et al., 2020) or extracting COVID-19\nevents from Tweets (Zong et al., 2020).\nReferences\nGustavo Aguilar, Suraj Maharjan, Adrian Pastor\nL´opez-Monroy, and Thamar Solorio. 2017. A\nMulti-task Approach for Named Entity Recognition\nin Social Media Data. In Proceedings of WNUT ,\npages 148–153.\nEiji Aramaki. 2010. TYPO CORPUS.\nhttp://luululu.com/tweet/.\nChristos Baziotis, Athanasiou Nikolaos, Pinelopi\nPapalampidi, Athanasia Kolovou, Georgios\nParaskevopoulos, Nikolaos Ellinas, and Alexandros\nPotamianos. 2018. NTUA-SLP at SemEval-2018\ntask 3: Tracking ironic tweets using ensembles\nof word and character level attentive RNNs. In\nProceedings of SemEval, pages 613–621.\nChristos Baziotis, Nikos Pelekis, and Christos Doulk-\neridis. 2017. DataStories at SemEval-2017 task 4:\nDeep LSTM with attention for message-level and\ntopic-based sentiment analysis. In Proceedings of\nSemEval, pages 747–754.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientiﬁc text.\nIn Proceedings of EMNLP-IJCNLP , pages 3615–\n3620.\nSteven Bird, Ewan Klein, and Edward Loper, editors.\n2009. Natural language processing with Python .\nO’Reilly.\nMathieu Cliche. 2017. BB twtr at SemEval-2017\ntask 4: Twitter sentiment analysis with CNNs and\nLSTMs. In Proceedings of SemEval , pages 573–\n580.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\nCross-lingual Representation Learning at Scale. In\nProceedings of ACL, page to appear.\nLeon Derczynski, Eric Nichols, Marieke van Erp,\nand Nut Limsopatham. 2017. Results of the\nWNUT2017 Shared Task on Novel and Emerging\nEntity Recognition. In Proceedings of WNUT, pages\n140–147.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL, pages 4171–\n4186.\nJacob Eisenstein. 2013. What to do about bad lan-\nguage on the internet. In Proceedings of NAACL-\nHLT, pages 359–369.\nNorjihan Abdul Ghani, Suraya Hamid, Ibrahim\nAbaker Targio Hashem, and Ejaz Ahmed. 2019. So-\ncial media big data analytics: A survey. Comput.\nHum. Behav., 101:417–428.\nKevin Gimpel, Nathan Schneider, Brendan O’Connor,\nDipanjan Das, Daniel Mills, Jacob Eisenstein,\nMichael Heilman, Dani Yogatama, Jeffrey Flanigan,\nand Noah A. Smith. 2011. Part-of-Speech Tagging\nfor Twitter: Annotation, Features, and Experiments.\nIn Proceedings of ACL-HLT, pages 42–47.\nTao Gui, Qi Zhang, Jingjing Gong, Minlong Peng,\nDi Liang, Keyu Ding, and Xuanjing Huang. 2018.\nTransferring from Formal Newswire Domain with\nHypernet for Twitter POS Tagging. In Proceedings\nof EMNLP, pages 2540–2549.\nTao Gui, Qi Zhang, Haoran Huang, Minlong Peng, and\nXuanjing Huang. 2017. Part-of-Speech Tagging for\nTwitter with Adversarial Neural Networks. In Pro-\nceedings of EMNLP, pages 2411–2420.\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t Stop Pretraining:\nAdapt Language Models to Domains and Tasks. In\nProceedings of ACL, pages 8342–8360.\nBo Han, Paul Cook, and Timothy Baldwin. 2012. Au-\ntomatically Constructing a Normalisation Dictio-\nnary for Microblogs. In Proceedings of EMNLP-\nCoNLL, pages 421–432.\nBo Han, Paul Cook, and Timothy Baldwin. 2013. Lex-\nical Normalization for Social Media Text. ACM\nTransactions on Intelligent Systems and Technology,\n4(1).\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2017. Bag of tricks for efﬁcient\ntext classiﬁcation. In Proceedings of EACL, pages\n427–431.\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A\nMethod for Stochastic Optimization. arXiv preprint,\narXiv:1412.6980.\n14\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So,\nand Jaewoo Kang. 2019. BioBERT: a pre-trained\nbiomedical language representation model for\nbiomedical text mining. Bioinformatics, page\nbtz682.\nNut Limsopatham and Nigel Collier. 2016. Bidirec-\ntional LSTM for Named Entity Recognition in Twit-\nter Messages. In Proceedings of WNUT, pages 145–\n152.\nFei Liu, Fuliang Weng, and Xiao Jiang. 2012. A\nBroad-Coverage Normalization System for Social\nMedia Language. In Proceedings of ACL , pages\n1035–1044.\nYijia Liu, Yi Zhu, Wanxiang Che, Bing Qin, Nathan\nSchneider, and Noah A. Smith. 2018. Parsing\nTweets into Universal Dependencies. In Proceed-\nings of NAACL-HLT, pages 965–975.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv preprint, arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nWeight Decay Regularization. In Proceedings of\nICLR.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end\nsequence labeling via bi-directional LSTM-CNNs-\nCRF. In Proceedings of ACL, pages 1064–1074.\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann\nMarcinkiewicz. 1993. Building a Large Annotated\nCorpus of English: The Penn Treebank. Computa-\ntional Linguistics, 19(2):313–330.\nDat Quoc Nguyen, Thanh Vu, Afshin Rahimi,\nMai Hoang Dao, Linh The Nguyen, and Long Doan.\n2020. WNUT-2020 Task 2: Identiﬁcation of Infor-\nmative COVID-19 English Tweets. In Proceedings\nof WNUT.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A Fast, Extensible\nToolkit for Sequence Modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations, pages 48–53.\nOlutobi Owoputi, Brendan O’Connor, Chris Dyer,\nKevin Gimpel, Nathan Schneider, and Noah A.\nSmith. 2013. Improved Part-of-Speech Tagging for\nOnline Conversational Text with Word Clusters. In\nProceedings of NAACL-HLT, pages 380–390.\nAlan Ritter, Sam Clark, Mausam, and Oren Etzioni.\n2011. Named Entity Recognition in Tweets: An Ex-\nperimental Study. In Proceedings of EMNLP, pages\n1524–1534.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemEval-2017 Task 4: Sentiment Analysis in Twit-\nter. In Proceedings of SemEval, pages 502–518.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of ACL, pages\n1715–1725.\nNatalia Silveira, Timothy Dozat, Marie-Catherine\nde Marneffe, Samuel Bowman, Miriam Connor,\nJohn Bauer, and Christopher D. Manning. 2014. A\ngold standard dependency corpus for English. In\nProceedings of LREC.\nBenjamin Strauss, Bethany Toma, Alan Ritter, Marie-\nCatherine de Marneffe, and Wei Xu. 2016. Results\nof the WNUT16 Named Entity Recognition Shared\nTask. In Proceedings of WNUT, pages 138–144.\nCynthia Van Hee, Els Lefever, and V ´eronique Hoste.\n2018. SemEval-2018 Task 3: Irony Detection in\nEnglish Tweets. In Proceedings of SemEval, pages\n39–50.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. HuggingFace’s Trans-\nformers: State-of-the-art Natural Language Process-\ning. arXiv preprint, arXiv:1910.03771.\nChuhan Wu, Fangzhao Wu, Sixing Wu, Junxin\nLiu, Zhigang Yuan, and Yongfeng Huang. 2018.\nTHU NGN at SemEval-2018 task 3: Tweet irony\ndetection with densely connected LSTM and multi-\ntask learning. In Proceedings of SemEval, pages 51–\n56.\nJoey Tianyi Zhou, Hao Zhang, Di Jin, Hongyuan Zhu,\nMeng Fang, Rick Siow Mong Goh, and Kenneth\nKwok. 2019. Dual Adversarial Neural Transfer for\nLow-Resource Named Entity Recognition. In Pro-\nceedings of ACL, pages 3461–3471.\nShi Zong, Ashutosh Baheti, Wei Xu, and Alan Ritter.\n2020. Extracting COVID-19 Events from Twitter.\narXiv preprint, arXiv:2006.02567."
}