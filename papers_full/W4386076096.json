{
  "title": "CompletionFormer: Depth Completion with Convolutions and Vision Transformers",
  "url": "https://openalex.org/W4386076096",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2151438581",
      "name": "You-min Zhang",
      "affiliations": [
        "University of Bologna"
      ]
    },
    {
      "id": "https://openalex.org/A2693800701",
      "name": "Xianda Guo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2108168314",
      "name": "Matteo Poggi",
      "affiliations": [
        "University of Bologna"
      ]
    },
    {
      "id": "https://openalex.org/A2099939147",
      "name": "Zheng Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103070070",
      "name": "Guan Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1983922316",
      "name": "Stefano Mattoccia",
      "affiliations": [
        "University of Bologna"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998031326",
    "https://openalex.org/W2885093229",
    "https://openalex.org/W6784333009",
    "https://openalex.org/W3174688521",
    "https://openalex.org/W4312950730",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W3206335707",
    "https://openalex.org/W3175720495",
    "https://openalex.org/W3147597595",
    "https://openalex.org/W2745471877",
    "https://openalex.org/W4312977443",
    "https://openalex.org/W4312871721",
    "https://openalex.org/W3096678291",
    "https://openalex.org/W4221146773",
    "https://openalex.org/W3175561084",
    "https://openalex.org/W6743759585",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2969202876",
    "https://openalex.org/W2963045776",
    "https://openalex.org/W4312566889",
    "https://openalex.org/W4226224676",
    "https://openalex.org/W3109128945",
    "https://openalex.org/W6766978945",
    "https://openalex.org/W3160694286",
    "https://openalex.org/W6756174191",
    "https://openalex.org/W4214520160",
    "https://openalex.org/W6848330267",
    "https://openalex.org/W2963163009",
    "https://openalex.org/W125693051",
    "https://openalex.org/W4312636847",
    "https://openalex.org/W3110653837",
    "https://openalex.org/W2963316641",
    "https://openalex.org/W2964326562",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W4289752563",
    "https://openalex.org/W6797399245",
    "https://openalex.org/W3103830518",
    "https://openalex.org/W4214709605",
    "https://openalex.org/W3091564232",
    "https://openalex.org/W4312793957",
    "https://openalex.org/W2794739174",
    "https://openalex.org/W3089805692",
    "https://openalex.org/W4321512633",
    "https://openalex.org/W3080631149",
    "https://openalex.org/W6763265080",
    "https://openalex.org/W4225592318",
    "https://openalex.org/W3165495321",
    "https://openalex.org/W2964242696",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3211490618",
    "https://openalex.org/W2949138505",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4385245566"
  ],
  "abstract": "Given sparse depths and the corresponding RGB images, depth completion aims at spatially propagating the sparse measurements throughout the whole image to get a dense depth prediction. Despite the tremendous progress of deep-learning-based depth completion methods, the locality of the convolutional layer or graph model makes it hard for the network to model the long-range relationship between pixels. While recent fully Transformer-based architecture has reported encouraging results with the global receptive field, the performance and efficiency gaps to the well-developed CNN models still exist because of its deteriorative local feature details. This paper proposes a Joint Convolutional Attention and Transformer block (JCAT), which deeply couples the convolutional attention layer and Vision Transformer into one block, as the basic unit to construct our depth completion model in a pyramidal structure. This hybrid architecture naturally benefits both the local connectivity of convolutions and the global context of the Transformer in one single model. As a result, our Completion-Former outperforms state-of-the-art CNNs-based methods on the outdoor KITTI Depth Completion benchmark and indoor NYUv2 dataset, achieving significantly higher efficiency (nearly 1/3 FLOPs) compared to pure Transformer-based methods. Code is available at https://github.com/youmi-zym/CompletionFormer.",
  "full_text": null,
  "topic": "FLOPS",
  "concepts": [
    {
      "name": "FLOPS",
      "score": 0.7343162298202515
    },
    {
      "name": "Computer science",
      "score": 0.7324174642562866
    },
    {
      "name": "Transformer",
      "score": 0.6649013757705688
    },
    {
      "name": "Convolutional neural network",
      "score": 0.5575634837150574
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5439467430114746
    },
    {
      "name": "Deep learning",
      "score": 0.48941460251808167
    },
    {
      "name": "RGB color model",
      "score": 0.4708927273750305
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3566088080406189
    },
    {
      "name": "Parallel computing",
      "score": 0.19077527523040771
    },
    {
      "name": "Engineering",
      "score": 0.14200711250305176
    },
    {
      "name": "Electrical engineering",
      "score": 0.08924815058708191
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}