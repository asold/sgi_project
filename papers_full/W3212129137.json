{
  "title": "Large-Scale Relation Learning for Question Answering over Knowledge Bases with Pre-trained Language Models",
  "url": "https://openalex.org/W3212129137",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A3005777502",
      "name": "Yuanmeng Yan",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2134682881",
      "name": "Rumei Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105896935",
      "name": "Sirui Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099256167",
      "name": "Hongzhi Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3212878548",
      "name": "Zan, Daoguang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110384818",
      "name": "Fuzheng Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1993208100",
      "name": "Wei Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099469527",
      "name": "Weiran Xu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3137492414",
    "https://openalex.org/W2963448850",
    "https://openalex.org/W2964022985",
    "https://openalex.org/W2170344111",
    "https://openalex.org/W2997591266",
    "https://openalex.org/W2963777632",
    "https://openalex.org/W3170037207",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034862985",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2890961898",
    "https://openalex.org/W4311565391",
    "https://openalex.org/W4233775226",
    "https://openalex.org/W3116847845",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W2971155257",
    "https://openalex.org/W2251287417",
    "https://openalex.org/W2964212344",
    "https://openalex.org/W2251079237",
    "https://openalex.org/W3130796325"
  ],
  "abstract": "The key challenge of question answering over knowledge bases (KBQA) is the inconsistency between the natural language questions and the reasoning paths in the knowledge base (KB). Recent graph-based KBQA methods are good at grasping the topological structure of the graph but often ignore the textual information carried by the nodes and edges. Meanwhile, pre-trained language models learn massive open-world knowledge from the large corpus, but it is in the natural language form and not structured. To bridge the gap between the natural language and the structured KB, we propose three relation learning tasks for BERT-based KBQA, including relation extraction, relation matching, and relation reasoning. By relation-augmented training, the model learns to align the natural language expressions to the relations in the KB as well as reason over the missing connections in the KB. Experiments on WebQSP show that our method consistently outperforms other baselines, especially when the KB is incomplete.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3653–3660\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n3653\nLarge-Scale Relation Learning for Question Answering over Knowledge\nBases with Pre-trained Language Models\nYuanmeng Yan1, Rumei Li2, Sirui Wang2, Hongzhi Zhang2, Daoguang Zan3\nFuzheng Zhang2, Wei Wu2, Weiran Xu1∗\n1Beijing University of Posts and Telecommunications, Beijing, China\n2Meituan Inc., Beijing, China 3University of Chinese Academy of Sciences, Beijing, China\n{yanyuanmeng,xuweiran}@bupt.edu.cn\n{lirumei,wangsirui,zhanghongzhi}@meituan.com\nAbstract\nThe key challenge of question answering over\nknowledge bases (KBQA) is the inconsistency\nbetween the natural language questions and\nthe reasoning paths in the knowledge base\n(KB). Recent graph-based KBQA methods are\ngood at grasping the topological structure of\nthe graph but often ignore the textual informa-\ntion carried by the nodes and edges. Mean-\nwhile, pre-trained language models learn mas-\nsive open-world knowledge from the large cor-\npus, but it is in the natural language form and\nnot structured. To bridge the gap between the\nnatural language and the structured KB, we\npropose three relation learning tasks for BERT-\nbased KBQA, including relation extraction, re-\nlation matching, and relation reasoning. By\nrelation-augmented training, the model learns\nto align the natural language expressions to the\nrelations in the KB as well as reason over the\nmissing connections in the KB. Experiments\non WebQSP show that our method consistently\noutperforms other baselines, especially when\nthe KB is incomplete.\n1 Introduction\nQuestion Answering over Knowledge Base\n(KBQA) aims to ﬁnd the answers to a natural lan-\nguage question given the structured knowledge\nbase (KB) and is widely used in modern ques-\ntion answering and information retrieval systems.\nTraditional retrieval-based KBQA approaches typi-\ncally build it as a pipeline system, including name\nentity recognization, entity linking, subgraph re-\ntrieval, and entity scoring. In recent years, with\nthe help of deep representation learning, such ap-\nproaches have achieved remarkable performance\n(Dong et al., 2015; Miller et al., 2016; Xu et al.,\n2016; Sun et al., 2018, 2019; Saxena et al., 2020;\nHe et al., 2021).\n∗ Work done during internship at Meituan Inc. Weiran\nXu is the corresponding author.\nQuestion: what is monta ellis \ncareer high points ?\nMonta \nEllis\nCVT1\nNBA Most \nImproved Player \nAward\nCVT2 Lanier High \nSchool\nCorrect Answer: NBA Most \nImproved Player Award\nModel Prediction: Lanier High \nSchool\na) Shallow Matching: The model fails to interpret “career high points” as “honor” or \n“award”, but matched “high school” according to the surface similarity  (the word “high”).\nScore: 0.01\nScore: 0.59\n×\n√\nQuestion: what is the name of \nking george vi wife ?\nCorrect Answer:  Queen \nElizabeth The Queen Mother\nModel Prediction: \n1923-04-26 ×\n√ George VI\nCVT\nb) Incomplete KB: The model matches “wife” to “spouse” and “marriage”. However, \nsome connections are missing from George VI to Queen Elizabeth. On the other hand, \nthe model also fails to reason over the path “George VI – people.person.children – \nPrincess Margaret – people.person.parents – Queen Elizabeth The Queen Mother ”\n1923-04-26\nQueen Elizabeth \nThe Queen Mother\nPrincess \nMargaret\nScore: 0.05\nScore: 0.33\nFigure 1: Two error cases from the WebQSP (Yih et al.,\n2015a) dataset. CVT indicates the Compound Value\nType in Freebase. For brevity, we abbreviate the name\nof some entities and relations.\nHowever, the KBQA task is still challenging es-\npecially for multi-hop questions because of two\nreasons: 1) Due to the complexity of human lan-\nguage, it is often difﬁcult to align the natural lan-\nguage questions with the reasoning paths in the KB.\nThe model tends to learn by surface matching and\neasily takes shortcut features (Du et al., 2021) for\nprediction (shown in Figure 1a). 2) In practice, the\nKB is often incomplete, which also requires the\nmodel to reason over the incomplete graph. But the\nmodel always fails to do that since it lacks explicit\ntraining on reasoning (shown in Figure 1b).\nPrevious works such as GraftNet (Sun et al.,\n2018) and PullNet (Sun et al., 2019) mainly solve\nthese problems by introducing external text corpus\n(e.g. all wikipedia documents) and use specially\ndesigned network architecture to incorporate infor-\nmation from the documents. However, the required\nexternal resources may be hard to collect in prac-\ntice. EmbedKGQA (Saxena et al., 2020) solves\nthe KB’s incompleteness issue by introducing the\npre-trained KB embeddings and trains the ques-\n3654\nQuestion: what was antoni gaudi inspired by ?\nAntoni \nGaudi\nWilliam \nMorris\nVisual \nArtist\nArchitect\ninfluenced_by\ninfluenced\nentity linking\nPath 2: <E1> Antoni Gaudi </E1> influenced by \n<E2> William Morris </E2>\nPath 3: <E1> Antoni Gaudi </E1> influenced <E2> \nWilliam Morris </E2>\nPath 4: <E1> Antoni Gaudi </E1> profession \nArchitect profession <E2> William Morris </E2>\nPath 1: <E1> Antoni Gaudi </E1> profession Visual \nArtist profession <E2> William Morris </E2>\nGraph Linearization\n[CLS] Question [SEP] Path 1 [SEP] Path 2 [SEP] Path 3 [SEP] Path 4 [SEP]\nTrm Trm Trm\nTrm Trm Trm\nTrm\nTrm\nBERT Encoder\nconcat\nLinear Sigmoid P(Answer = William Morris | q, G) = 0.98\nSentence 1: OBJ{Wikipedia} is a registered \ntrademark of the SUBJ{Wikimedia Foundation, \nInc.}, a non-profit organization.\nRelation 1: owner of\nSentence 2: The SUBJ{Amagi Line} is a \nJapanese railway line connecting Kiyama Station \n(on the OBJ{Kagoshima Main Line}), Kiyama \nand Amagi Station, Asakura.\nRelation 2: connects with\nRelation Extraction Dataset ↓\n[CLS] Sentence 1 [SEP] Wikimedia Foundation, Inc. owner of Wikipedia [SEP]\n[CLS] Sentence 2 [SEP] Amagi Line connects with Kagoshima Main Line [SEP]\n[CLS] Sentence 1 [SEP] Sentence 2 [SEP]\nRelation Extraction (RE) Task\nRelation Matching (RM) Task\na) The overall architecture of BERT-based KBQA approach b) The proposed three auxiliary tasks for relation learning, RE, RM and RR.\nGeorge VI\nElizabeth II\nQueen Elizabeth \nThe Queen Mother\nPrincess \nMargaret\nFreebase Subgraph ↓\nRelation Reasoning (RR) Task\nReasoning Path 1\nGeorge VI children Princess Margaret parents Queen Elizabeth\nReasoning Path 2\nGeorge VI children Elizabeth II named_after Queen Elizabeth\nTarget Triple\nGeorge VI people.person.spouse_s Queen Elizabeth\nGraph Linearization\n[CLS] Target Triple [SEP] Reasoning Path 1 [SEP] Reasoning Path 2 [SEP]\nFigure 2: An overview of our approach. For brevity, we abbreviate the name of some entities and relations.\ntion embeddings to be ﬁt in the relation embedding\nspace such that they can directly use the scoring\nfunction to rank answers. However, their approach\nmainly grasps the topological structure of the graph\nbut ignores the textual information in entities and\nrelations that should be also useful to score candi-\ndate entities.\nIn this paper, to learn a better mapping from the\nnatural language questions to the reasoning paths in\nthe KB (Gao et al., 2020; Bouraoui et al., 2020), we\nreformulate the retrieval-based KBQA task to make\nit a question-context matching form and propose\nthree auxiliary tasks for relation learning, namely\nrelation extraction (RE), relation matching (RM)\nand relation reasoning (RR). RE and RM both take\nadvantage of the relation extraction datasets, includ-\ning WebRED (Ormandi et al., 2021) and FewRel\n(Han et al., 2018). RE trains the model through\ninferring relations from the sentences, and RM\nthrough determining whether two sentences ex-\npress the same relation. RR constructs the training\ndata from the KB in a self-supervised manner and\ntrains the model to reason over the missing KB\nconnections given the existing paths.\nOur contributions can be summarized as follows:\n1) To bridge the gap between natural language and\nthe structured KB, we reformulate the KBQA task\nto be a question-context matching problem and pro-\npose auxiliary tasks to enhance the implicit relation\nlearning for pre-trained language models (Devlin\net al., 2019). 2) To mitigate the KB’s incomplete-\nness issue, we further propose a task for relation\nreasoning on the KB. 3) Experiments on WebQSP\nshow the effectiveness of our proposed approach,\nespecially when the KB is highly incomplete.1\n1Our code is available at https://github.com/\nyym6472/KBQARelationLearning\n2 Approach\nProblem Deﬁnition In this paper, we mainly focus\non the retrieval-based KBQA. Given an input query\nq, we ﬁrst annotate the named entities in the query\nand link them to the nodes in the KB. Then some\nheuristic algorithm2 is applied to retrieve a query-\nspeciﬁed subgraph G= {⟨e,r,e′⟩|e,e′ ∈E,r ∈\nR}, where Eis the set of all candidate entities that\nprobably contains the answer of q, and Rdenotes\nthe relation set. Our task is to calculate a score si\nfor each candidate entity ei ∈E indicating whether\nei is the answer entity or not.\nIn this section, we ﬁrst present how to solve\nKBQA with BERT, then we introduce three pro-\nposed auxiliary tasks to augment the relation learn-\ning for BERT.\n2.1 BERT for KBQA\nFor each question q, we can obtain its topic entity\netopic from the entity linking system. 3 Then, as\nshown in Figure 2a, we convert the candidate entity\nscoring problem into a question-context matching\ntask as follows.\nWe ﬁrst ﬁnd all paths in Gthat connect the topic\nentity etopic and the candidate entity ei. We set\na maximum number of paths 4 and apply down-\nsampling when the number exceeds the thresh-\nold. Then we construct the textual form of each\npath by replacing the nodes with entity names\nand the edges with relation names in the KB.\nFinally, we concatenate the question q and all\npaths p1,...,p n to make an input sample xi =\n2Following previous works (Sun et al., 2018), we use the\nPersonalized PageRank algorithm (Haveliwala, 2002).\n3It is guaranteed thatetopic ∈ Gwhen retrieving subgraphs.\nFor samples without linked topic entity, we remove them from\nthe train set and count them as wrong cases when testing.\n4The number is set to 10 in our experiments.\n3655\n[CLS]q[SEP]p1[SEP] ...p n[SEP].\nHere, we regard these paths as the facts between\nthe topic entity etopic and the candidate entity ei.\nWe aim to use BERT to predict whether the hypoth-\nesis “ei is the answer of q” is supported by those\nKB facts. Thus, we feed the sample to BERT and\ntake the representation corresponding to [CLS]\ntoken for binary classiﬁcation:\nsi = σ(wTBERTCLS(xi)) (1)\nLi = −(y·log si + (1−y) ·log(1 −si)) (2)\n, where σis the sigmoid function and yis ground\ntruth label indicating whetherei is the answer entity\nof qor not.\n2.2 Auxiliary Tasks for Relation Learning\nThe performance of KBQA depends heavily on the\nmapping from the natural language questions to the\nrelations in the path. To further enhance the relation\nlearning of BERT, we propose three auxiliary tasks\nfor relation learning, as shown in Figure 2b.\nRelation Extraction (RE) One straightforward\nidea is to use the relation extraction dataset, where\nthe model learns to extract the relation expressed in\nthe sentence between the given head and tail entity.\nSimilar to KBQA, we concatenate the sentence and\nthe one-hop path to construct an RE example for\nBERT: [CLS]s[SEP]h,r,t[SEP], where s, h,\nrand tindicates sentence, head entity, relation and\ntail entity respectively.\nMoreover, to simulate the 2-hop reason-\ning in KBQA, we also combine two RE\nexamples to make a compositional one:\n[CLS]s1,s2[SEP]h1,r1,t1(h2),r2,t2[SEP],\nwhere the tail entity of the ﬁrst example is same to\nthe head entity of the second example.\nRelation Matching (RM) In relation matching\ntask, we assume that two sentences with the same\nrelation should have similar representations. Thus,\nwe concatenate two sentences and train BERT\nthrough predicting whether two sentences express\nthe same relation: [CLS]s1[SEP]s2[SEP],\nwhere the label is 1 if s1 and s2 express the same\nrelation and 0 otherwise.\nRelation Reasoning (RR) BERTRL (Zha et al.,\n2021) proposes a self-supervised approach for KB\ncompletion task. They choose one triplet (h,r,t)\nfrom the KB and assume it is missing. Then they\nﬁnd other multi-hop paths from h to t, and use\nthem to predict whether (h,r,t) exists in the KB or\nnot: [CLS]h,r,t[SEP]p1[SEP] ...p n[SEP]\nBy training on BERTRL, the model learns to rea-\nson and complete the missing connections, which\nis extremely helpful for KBQA on the incomplete\nKB.\nTraining Since all three auxiliary tasks are for-\nmulated as a binary classiﬁcation task and only\ndiffer in the data construction phase, we can either\nuse them to pre-train BERT before KBQA (noted\nas pre-train) or train them jointly with KBQA in a\nmulti-task paradigm (noded as joint). In our experi-\nments, we ﬁnd both settings work well and produce\nsimilar results (see Section 3.4 for more details).\n3 Experiments\n3.1 Datasets\nKBQA Dataset To evaluate the effectiveness of\nour approach, we conduct experiments on We-\nbQuestionsSP (WebQSP, Yih et al. 2015a) dataset.\nIt contains 4737 questions that are answerable us-\ning Freebase. Following Sun et al. (2018), we re-\nserve 250 examples from the training set for tuning\nhyperparameters and early stopping, resulting in\n2848/250/1639 examples for training, validation,\nand test respectively.\nWe obtain and preprocess WebQSP using the\nscripts5 released by Sun et al. (2018). It mainly\nincludes entity linking and subgraph retrieval in\ntwo steps. The entity linking results are directly\ntaken from the codebase 6 released by Yih et al.\n(2015b). For each question, there is a set of seed\nentities7 and will be used in the subgraph retrieval\nphase. The subgraphs are retrieved through the\nPersonalized PageRank (PPR) algorithm (Haveli-\nwala, 2002), and we set the max number of entities\nin each subgraph to 500. Among the 1639 exam-\nples in the test set, the answers of 120 questions\nare not retrieved from the subgraph, so the answer\ncoverage is about 92.68% in the subgraph retrieval\nphase.\nRelation Extraction Datasets In the relation\nlearning tasks, we use WebRED (Ormandi et al.,\n2021) and FewRel (Han et al., 2018) dataset as\nexternal resources. For more details about these\ndatasets and how we process them to construct re-\nlation learning tasks, please refer to Appendix A.\n5https://github.com/OceanskySun/\nGraftNet/tree/master/preprocessing\n6https://github.com/scottyih/STAGG\n7It can be an empty set if the named entity recognization\nor entity linking fails.\n3656\n3.2 Baselines\nWe compare our approach to several baselines, in-\ncluding KV-Mem (Miller et al., 2016), GraftNet\n(Sun et al., 2018), PullNet (Sun et al., 2019), Em-\nbedKGQA (Saxena et al., 2020) and NSM (He\net al., 2021). Please refer to Appendix B for more\ndetails. Besides, we also provide results of BERT\n(without additional relation learning) as a baseline\nto show the effectiveness of our proposed relation\nlearning tasks.\n3.3 Metrics\nWhen evaluating our model, we ﬁrst feed each lin-\nearized input to BERT and get the corresponding\nscore between 0 to 1. For each question, we rank\nall candidate entities in the subgraph by the scores\nand calculate the hits@1 and F1 as follows:\n• Hits@1 If the highest-ranked entity is the\nanswer entity, then hits@1 is 1. Otherwise,\nhits@1 is 0.\n• F1 score Given a threshold, we consider all\ncandidate entities whose scores are greater\nthan the threshold as the answers predicted by\nthe model. Then we calculate the F1 score be-\ntween the ground truth answer entities and the\nmodel predicted answer entities. In our exper-\niments, we select the threshold that performs\nbest in the validation set.\nThen we average the Hits@1 and F1 scores over\nall test examples. For questions whose answers\nare not covered by the retrieved subgraph, we re-\ngard them as wrong predictions. Note that we treat\nhits@1 as the primary metrics, since the results of\nF1 score show a large variance due to its sensitivity\nto the threshold. We provide more training details\nin Appendix C.\n3.4 Main Results\nThe experimental results are shown in Table 1. We\nﬁnd that the results with BERT outperform most of\nthe baselines (except for the NSM). When compar-\ning to PullNet, BERT achieves a relative improve-\nment of 4.6%, demonstrating the effectiveness of\nsolving KBQA with BERT.\nOn the other hand, the results with all three rela-\ntion learning tasks (72.3) signiﬁcantly outperform\nthe BERT baseline (71.2), showing that the pro-\nposed auxiliary tasks beneﬁt the relation matching\nand relation reasoning of BERT.\nModel dev set test set\nHits@1 F1 Hits@1\nBaselines\nKV-Mem† - 38.6 46.7\nGraftNet‡ - 62.4 66.7\nPullNet† - - 68.1\nEmbedKGQA† - - 66.6\nNSM† - 67.4 74.3\nOur implementation\nBERT 71.0 63.4 71.2\nBERT+REpre-train 68.1 62.1 72.8*\nBERT+RMpre-train 71.8 63.4 72.6*\nBERT+RRpre-train 69.8 61.8 71.7*\nBERT+RE,RM,RRpre-train 69.4 62.5 72.3*\nBERT+REjoint 67.3 57.4 72.4*\nBERT+RMjoint 72.2 64.5 72.9*\nBERT+RRjoint 67.3 62.9 71.2\nBERT+RE,RM,RRjoint 71.8 60.0 72.0*\nTable 1: Experimental results on WebQSP dataset.\nBaseline results with † are taken from He et al. (2021),\nwhile results with ‡ are taken from Sun et al. (2018).\nThe numbers with * indicate the signiﬁcant improve-\nment over the BERT baseline with p < 0.05 under t-test.\nAblation Studies To check which task con-\ntributes to the ﬁnal result most, we conduct ex-\nperiments where only one task is applied at a time.\nFrom the second part of Table 1, we can observe\nthat RE and RM are the two most contributing\ntasks, and even training with them individually can\noutperform training with all three tasks together.\nMeanwhile, RR also brings performance improve-\nment (from 71.2 to 71.7) under thepre-train setting,\nbut its improvement is not as signiﬁcant as RE and\nRM. This may be because the model doesn’t re-\nquire much reasoning ability under the full KB\nsetting.\nPre-training or Joint Training When compar-\ning the pre-training setting with joint training, we\nﬁnd both settings work well and outperform the\nBERT baseline. For RE and RR, pre-training\nseems better thanjoint training, while for RM,joint\ntraining is slightly better.\n3.5 Analysis\nResults over the Incomplete KB To verify the\nrobustness of our approach when the KB is incom-\nplete, we randomly remove 50% of the KB facts in\nthe retrieved subgraphs and conduct experiments\non this incomplete version of the WebQSP dataset.\nThe results8 are illustrated in Figure 3. We can\n8Appendix D provides more results under the incomplete\nKB (with different proportions) as well as the comparison to\nbaselines.\n3657\n50% KB Full KB\n30\n40\n50\n60\n70Hits@1\n32.7\n46.748.2\n66.4\n50.1\n68.1\n53.2\n66.6\n56.7\n71.2\n58.8\n72.3\nKV-Mem\nGraftNet\nPullNet\nEmbedKGQA\nBERT (ours)\nBERT + RE,RM,RR (ours)\nFigure 3: The performance comparison with the full\nKB and the 50% KB. We only compare to baselines\nthat also report results with 50% KB.\nModel Annotation Type\nnone all head-tail\nBERT 69.8 70.9 71.2\nBERT+RE,RM,RRjoint 70.1 71.3 72.0\nTable 2: Hits@1 results on WebQSP with different an-\nnotation types of entity spans.\nobserve that: 1) Our approach consistently outper-\nforms other baselines under both the full KB and\nthe 50% KB settings. 2) With 50% KB, adding\nrelation learning tasks achieve more performance\ngain than with full KB (+2.1 vs +1.1), demonstrat-\ning that our relation learning tasks are especially\nuseful when the KB is incomplete.\nAnnotation of Entity Spans As discussed in\nSoares et al. (2019), different markers for entity\nspans have a great impact on the BERT-based rela-\ntion extraction task. To ﬁnd out the best annotation\nstrategy for KBQA, we conduct experiments with\nthree types of annotations: 1) Using no annotation\n(noted as none). 2) Using <E> and </E> to anno-\ntate all entities in the reasoning paths (noted as all).\n3) Using <E1> and </E1> to annotate all head\nentities and using <E2> and </E2> to annotate\nall tail entities (noted as head-tail).\nAs shown in Table 2, we ﬁnd none performs\nworst while head-tail achieves the best result. We\ncan conclude that the annotations of the entity\nspans are still required for the BERT model. They\nbring structural information that helps the model to\nidentify the entity. Meanwhile, ﬁne-grained annota-\ntions (head-tail) are better than the coarse-grained\nones (all).\nInﬂuence of Negative Samples In our experi-\nments, we want to speed up the training by down-\nsampling negative samples of KBQA. However, as\nshown in Table 3, we ﬁnd that the performance is\nalso related to the number of negative samples. In\ngeneral, more negative samples will bring a higher\n# Neg. Samples 20 50 100 200 500\nF1 score 60.6 62.3 63.5 63.2 63.8\nHits@1 65.0 69.0 69.7 70.7 72.0\nTable 3: Results on WebQSP when downsampling neg-\native samples during training.\nHits@1 score. One potential solution to this issue\nis hard negative mining, and we will leave it for\nfuture work.\n4 Conclusion\nIn this paper, we propose three auxiliary tasks to\naugment relation learning for BERT-based KBQA\nmethod, including relation extraction, matching\nand reasoning. These tasks not only bridge the gap\nbetween the natural language and the structured\nKB, but also explicitly train the model to reason\nover the incomplete KB. The experimental results\non WebQSP demonstrate the effectiveness of our\napproach, especially when the KB is incomplete.\nAcknowledgements\nWe thank all anonymous reviewers for their helpful\ncomments and suggestions. This work was par-\ntially supported by National Key R&D Program of\nChina No. 2019YFF0303300 and Subject II No.\n2019YFF0303302, DOCOMO Beijing Communi-\ncations Laboratories Co., Ltd, MoE-CMCC “Artiﬁ-\ncal Intelligence\" Project No. MCM20190701.\nBroader Impact\nKBQA is a widely applied technique in natural\nlanguage processing, especially in question answer-\ning and information retrieval tasks. This work fo-\ncuses on the retrieval-based approaches and pro-\nposes to use BERT-like pre-trained language mod-\nels to improve the scoring function for ranking\nthe candidates. Though our approach takes ad-\nvantage of the learned open-world knowledge in\nBERT and achieves better results, the introduction\nof pre-trained language models may lead to some\npotential risks such as introducing extra data biases\nand being sensitive to adversarial examples. On\nthe other hand, our proposed relation extraction\nand relation matching tasks use external resources\n(i.e. the relation extraction datasets) that may con-\ntain the ethical risk. Therefore, users should pay\nspecial attention when preparing these resources\nand guarantee they are task-relevant, unbiased, and\nethical.\n3658\nReferences\nZied Bouraoui, Jose Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom bert. In Proceedings of the AAAI Conference\non Artiﬁcial Intelligence , volume 34, pages 7456–\n7463.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nLi Dong, Furu Wei, Ming Zhou, and Ke Xu.\n2015. Question answering over freebase with multi-\ncolumn convolutional neural networks. In Proceed-\nings of the 53rd Annual Meeting of the Association\nfor Computational Linguistics and the 7th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers), pages 260–269.\nMengnan Du, Varun Manjunatha, Rajiv Jain, Ruchi\nDeshpande, Franck Dernoncourt, Jiuxiang Gu, Tong\nSun, and Xia Hu. 2021. Towards interpreting and\nmitigating shortcut learning behavior of nlu models.\narXiv preprint arXiv:2103.06922.\nTianyu Gao, Xu Han, Ruobing Xie, Zhiyuan Liu, Fen\nLin, Leyu Lin, and Maosong Sun. 2020. Neural\nsnowball for few-shot relation learning. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelli-\ngence, volume 34, pages 7772–7779.\nXu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan\nYao, Zhiyuan Liu, and Maosong Sun. 2018. Fewrel:\nA large-scale supervised few-shot relation classiﬁca-\ntion dataset with state-of-the-art evaluation. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing , pages 4803–\n4809.\nTaher H Haveliwala. 2002. Topic-sensitive pagerank.\nIn Proceedings of the 11th international conference\non World Wide Web, pages 517–526.\nGaole He, Yunshi Lan, Jing Jiang, Wayne Xin\nZhao, and Ji-Rong Wen. 2021. Improving multi-\nhop knowledge base question answering by learn-\ning intermediate supervision signals. In Proceed-\nings of the 14th ACM International Conference on\nWeb Search and Data Mining , WSDM ’21, page\n553–561, New York, NY , USA. Association for\nComputing Machinery.\nAlexander Miller, Adam Fisch, Jesse Dodge, Amir-\nHossein Karimi, Antoine Bordes, and Jason Weston.\n2016. Key-value memory networks for directly read-\ning documents. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pages 1400–1409.\nRobert Ormandi, Mohammad Saleh, Erin Winter, and\nVinay Rao. 2021. Webred: Effective pretraining and\nﬁnetuning for relation extraction on the web. arXiv\npreprint arXiv:2102.09681.\nApoorv Saxena, Aditay Tripathi, and Partha Taluk-\ndar. 2020. Improving multi-hop question answering\nover knowledge graphs using knowledge base em-\nbeddings. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4498–4507.\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey\nLing, and Tom Kwiatkowski. 2019. Matching the\nblanks: Distributional similarity for relation learn-\ning. In Proceedings of the 57th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2895–2905.\nHaitian Sun, Tania Bedrax-Weiss, and William Cohen.\n2019. Pullnet: Open domain question answering\nwith iterative retrieval on knowledge bases and text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 2380–\n2390.\nHaitian Sun, Bhuwan Dhingra, Manzil Zaheer, Kathryn\nMazaitis, Ruslan Salakhutdinov, and William Cohen.\n2018. Open domain question answering using early\nfusion of knowledge bases and text. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 4231–4242.\nKun Xu, Siva Reddy, Yansong Feng, Songfang Huang,\nand Dongyan Zhao. 2016. Question answering on\nfreebase via relation extraction and textual evidence.\nIn Proceedings of the 54th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 2326–2336.\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and\nJianfeng Gao. 2015a. Semantic parsing via staged\nquery graph generation: Question answering with\nknowledge base. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1321–1331.\nWen-tau Yih, Ming-Wei Chang, Xiaodong He, and\nJianfeng Gao. 2015b. Semantic parsing via staged\nquery graph generation: Question answering with\nknowledge base. In Proceedings of the 53rd Annual\nMeeting of the Association for Computational Lin-\nguistics and the 7th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1321–1331, Beijing, China. Associa-\ntion for Computational Linguistics.\nHanwen Zha, Zhiyu Chen, and Xifeng Yan. 2021. In-\nductive relation prediction by bert. arXiv preprint\narXiv:2103.07102.\n3659\nModel 10% KB 30% KB 50% KB Full KB\nF1 Hits@1 F1 Hits@1 F1 Hits@1 F1 Hits@1\nBaselines\nKV-Mem 4.3 12.5 13.8 25.8 21.3 33.3 38.6 46.7\nGraftNet 6.5 15.5 20.4 34.9 34.3 47.7 62.4 66.7\nPullNet - - - - - 50.3 - 68.1\nEmbedKGQA - - - - - 53.2 - 66.6\nNSM - - - - - - 67.4 74.3\nOur implementation\nBERT 12.95 25.89 30.88 44.84 41.81 56.72 63.39 71.18\nBERT+RE 13.51 26.79 31.09 46.51 42.19 58.28 62.05 72.77\nBERT+RM 13.72 26.79 31.23 46.57 43.09 59.02 64.51 72.89\nBERT+RR 13.65 26.98 29.59 46.63 42.10 57.11 61.83 71.73\nBERT+RE,RM,RR 13.58 26.79 30.50 46.57 41.42 58.77 62.46 72.28\nTable 4: Experimental results on WebQSP dataset. The baseline results are taken from their corresponding paper.\n10%, 30% and 50% indicate the incomplete KB settings, where the facts in the subgraphs are randomly removed\nto 10%, 30% and 50%. For our implemented results, we report the best results among pre-train and joint settings.\nA Data Processing for Relation Learning\nDataset Details WebRED9 is a relation extrac-\ntion dataset based on WikiData. It is ﬁrstly con-\nstructed through distant supervision and then de-\nnoised by human annotators. WebRED contains\nmore than 500 relations in WikiData and releases\n107819/3898 denoised examples for train/test. We\nfurther remove those contradictory examples (i.e.\nthe number of positive raters is equal to the number\nof negative raters) and obtain 107761/3898 exam-\nples for train/test.\nFewRel10 includes 80 relations in WikiData and\n700 examples for each relation, resulting in totally\n56,000 examples. Among 80 relations, 64/16 rela-\ntions are split for training/test.\nData Processing For Relation Extraction (RE)\ntask, we directly use the negative examples in the\nWebRED dataset for negative sampling. For Rela-\ntion Matching (RM) task, we randomly sample one\nsentence with the same relation label to construct\nthe positive pair and randomly sample 9 sentences\nwith other relation labels to construct negative pairs.\nFor Relation Reasoning (RR) task, we use all re-\ntrieved subgraphs from the WebQSP’strain split\nand run the script11 released by Zha et al. (2021) to\ngenerate training samples.\n9Available at https://github.com/\ngoogle-research-datasets/WebRED\n10Available at http://www.zhuhao.me/fewrel/\nindex.html\n11https://github.com/zhw12/BERTRL\nB Baselines\nWe compare our approach to the following base-\nlines:\nKV-Mem (Miller et al., 2016) adopts a key-\nvalue memory network to store the KB facts and\nuses it to augment the open domain question an-\nswering.\nGraftNet (Sun et al., 2018) propose to solve\nopen domain question answering task by retrieving\nfrom the KB and the textual corpus and design a\nvariant of graph convolution network for the het-\nerogeneous graph.\nPullNet (Sun et al., 2019) uses GraftNet as the\nmodel architecture but it also learns how to retrieve\ninformation and expand the subgraph during the\ntraining and test phase.\nEmbedKGQA (Saxena et al., 2020) uses the\npre-trained KB embeddings and trains the question\nencoder to make question embeddings aligned with\nthe relation embedding space such that they can\ndirectly use the scoring function to predict whether\na given entity is the answer or not.\nNSM (He et al., 2021) propose to use the neu-\nral state machine (NSM) to solve the KBQA task\nand uses bidirectional hybrid reasoning and a two-\nstage teacher-student architecture to augment the\nreasoning ability of the student model.\nC Training Details\nWe run all our experiments on one single NVIDIA\nTesla V100 (32GB) GPU. We set the batch size\nto 128 and set the max sequence length to 128\n3660\nfor BERT. We evaluate the model every 1000 or\n3000 training steps depending on the number of\ntotal training steps in one epoch, and the evalua-\ntion takes about 6 minutes. We train the model\nfor up to 3 epochs and use a learning rate of 2e-\n5. For the pre-trained BERT, we download the\nbert-base-uncased model from Hugging-\nFace12, and set the dropout rate to 0.2 during train-\ning. The best results are typically achieved after\ntraining BERT for 2-3 epochs (roughly 15,000 -\n25,000 steps), which often takes 6-8 hours (roughly\n1.8 steps per second) for training. The number of\nmodel parameters is 109,483,009 (109M), includ-\ning the parameters of BERT and the linear head\nfor binary classiﬁcation. For all hyperparameters\nused in our experiments, we manually tune them\non the reserved 250 train examples of WebQSP.\nF1-score is used as the metric to select the best\nhyperparameters.\nD More Results over the Incomplete KB\nWe show more experimental results on incomplete\nKB in Table 4. We can make the following obser-\nvations: 1) When the KB is extremely incomplete\n(10% KB and 30% KB), our approach can achieve\nsigniﬁcant performance gain compared to previous\nwork GraftNet (Sun et al., 2018) (+7.2 on 10% KB\nsetting and +11.5 on 30% KB setting). 2) The rela-\ntion reasoning (RR) task is well performed when\nthe KB is extremely incomplete (10% KB and 30%\nKB), but the performance gain decreases when the\nKB is relatively complete (50% KB and Full KB).\n3) The relation matching (RM) task is the most ro-\nbust task that shows very strong performance gain\nwith different KB’s incompleteness.\n12https://huggingface.co/\nbert-base-uncased",
  "topic": "Question answering",
  "concepts": [
    {
      "name": "Question answering",
      "score": 0.8212366104125977
    },
    {
      "name": "Computer science",
      "score": 0.7865296602249146
    },
    {
      "name": "Relation (database)",
      "score": 0.748702347278595
    },
    {
      "name": "Natural language processing",
      "score": 0.6854866147041321
    },
    {
      "name": "Relationship extraction",
      "score": 0.6621562838554382
    },
    {
      "name": "Natural language",
      "score": 0.6489385962486267
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6419095993041992
    },
    {
      "name": "Knowledge graph",
      "score": 0.5797884464263916
    },
    {
      "name": "Knowledge base",
      "score": 0.563722550868988
    },
    {
      "name": "Bridge (graph theory)",
      "score": 0.5143786072731018
    },
    {
      "name": "Matching (statistics)",
      "score": 0.5013706684112549
    },
    {
      "name": "Natural language understanding",
      "score": 0.4513794183731079
    },
    {
      "name": "Graph",
      "score": 0.44460904598236084
    },
    {
      "name": "Language model",
      "score": 0.4125509560108185
    },
    {
      "name": "Information extraction",
      "score": 0.3822137415409088
    },
    {
      "name": "Theoretical computer science",
      "score": 0.14351889491081238
    },
    {
      "name": "Mathematics",
      "score": 0.10182538628578186
    },
    {
      "name": "Data mining",
      "score": 0.0821157693862915
    },
    {
      "name": "Medicine",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Internal medicine",
      "score": 0.0
    }
  ]
}