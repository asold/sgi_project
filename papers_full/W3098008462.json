{
  "title": "Do Fine-tuned Commonsense Language Models Really Generalize?",
  "url": "https://openalex.org/W3098008462",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3160244852",
      "name": "Kejriwal Mayank",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2212161766",
      "name": "Shen Ke",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2970062726",
    "https://openalex.org/W2963350032",
    "https://openalex.org/W2963101081",
    "https://openalex.org/W2963466847",
    "https://openalex.org/W2112085250",
    "https://openalex.org/W2038688024",
    "https://openalex.org/W2161109368",
    "https://openalex.org/W2094916185",
    "https://openalex.org/W2990752173",
    "https://openalex.org/W7334523",
    "https://openalex.org/W2044173330",
    "https://openalex.org/W2100313866",
    "https://openalex.org/W2898654274",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3037763555",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W1579181926",
    "https://openalex.org/W2746236423",
    "https://openalex.org/W1608789752",
    "https://openalex.org/W1492581928",
    "https://openalex.org/W3103326498",
    "https://openalex.org/W2810840719",
    "https://openalex.org/W2903073381",
    "https://openalex.org/W2998617917",
    "https://openalex.org/W2107658650",
    "https://openalex.org/W2962700793",
    "https://openalex.org/W2947469743",
    "https://openalex.org/W2995643077",
    "https://openalex.org/W94800547",
    "https://openalex.org/W2783549597",
    "https://openalex.org/W2998277219",
    "https://openalex.org/W2803831897",
    "https://openalex.org/W2085567745",
    "https://openalex.org/W2585926623",
    "https://openalex.org/W2163908084",
    "https://openalex.org/W2950352427",
    "https://openalex.org/W1531543048",
    "https://openalex.org/W2887768933",
    "https://openalex.org/W145108188",
    "https://openalex.org/W2798801120",
    "https://openalex.org/W3023710830",
    "https://openalex.org/W1504527707",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2748533788",
    "https://openalex.org/W2757177109",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2963520511",
    "https://openalex.org/W1966678693",
    "https://openalex.org/W2792026792",
    "https://openalex.org/W3039127676"
  ],
  "abstract": "Recently, transformer-based methods such as RoBERTa and GPT-3 have led to significant experimental advances in natural language processing tasks such as question answering and commonsense reasoning. The latter is typically evaluated through multiple benchmarks framed as multiple-choice instances of the former. According to influential leaderboards hosted by the Allen Institute (evaluating state-of-the-art performance on commonsense reasoning benchmarks), models based on such transformer methods are approaching human-like performance and have average accuracy well over 80% on many benchmarks. Since these are commonsense benchmarks, a model that generalizes on commonsense reasoning should not experience much performance loss across multiple commonsense benchmarks. In this paper, we study the generalization issue in detail by designing and conducting a rigorous scientific study. Using five common benchmarks, multiple controls and statistical analysis, we find clear evidence that fine-tuned commonsense language models still do not generalize well, even with moderate changes to the experimental setup, and may, in fact, be susceptible to dataset bias. We also perform selective studies, including qualitative and consistency analyses, to gain deeper insight into the problem.",
  "full_text": "Do Fine-tuned Commonsense Language Models Really Generalize?\nMayank Kejriwal, Ke Shen\nInformation Sciences Institute\nUSC Viterbi School of Engineering\n4676 Admiralty Way 1001\nMarina Del Rey, California 90292\nAbstract\nRecently, transformer-based methods such as RoBERTa and\nGPT-3 have led to signiﬁcant experimental advances in nat-\nural language processing tasks such as question answering\nand commonsense reasoning. The latter is typically evaluated\nthrough multiple benchmarks framed as multiple-choice in-\nstances of the former. According to inﬂuential leaderboards\nhosted by the Allen Institute (evaluating state-of-the-art per-\nformance on commonsense reasoning benchmarks), models\nbased on such transformer methods are approaching human-\nlike performance and have average accuracy well over 80%\non many benchmarks. Since these are commonsense bench-\nmarks, a model that generalizes on commonsense reasoning\nshould not experience much performance loss across multi-\nple commonsense benchmarks. In this paper, we study the\ngeneralization issue in detail by designing and conducting\na rigorous scientiﬁc study. Using ﬁve common benchmarks,\nmultiple controls and statistical analysis, we ﬁnd clear evi-\ndence that ﬁne-tuned commonsense language models still do\nnot generalize well, even with moderate changes to the exper-\nimental setup, and may, in fact, be susceptible to dataset bias.\nWe also perform selective studies, including qualitative and\nconsistency analyses, to gain deeper insight into the problem.\nIntroduction\nCommonsense reasoning has become a resurgent area of\nresearch in both the NLP and broader AI communities 1\n(Davis 2014), (Zang et al. 2013), (Storks, Gao, and Chai\n2019), despite having been introduced as an early AI chal-\nlenge more than 50 years ago (in the context of machine\ntranslation) (Bar-Hillel 1960). Traditionally, it was believed\nthat the problem could only be solved through a combina-\ntion of techniques, including Web mining, logical reason-\ning, handcrafted knowledge bases and crowdsourcing (Davis\nand Marcus 2015), (Liu and Singh 2004), (Moore 1982).\nMore recently, the advent of powerful ‘transformer’ neu-\nral networks, especially in NLP (Devlin et al. 2018), (Liu\net al. 2019), suggests that the time is right to build common-\nsense reasoners that generalize to a wide variety of situa-\ntions, including those involving social and physical reason-\ning (Sap et al. 2019b), (Bisk et al. 2020). There are several\nCopyright © 2018, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\n1Two other example domains include computer vision (Zellers\net al. 2019a) and social networks (Dinakar et al. 2012).\nrelated reasons why commonsense reasoning is such an im-\nportant topic in AI. Commonsense reasoning is an innately\nhuman ability that machines have (thus far) not proven adept\nat ‘conquering’ unlike other task-speciﬁc domains such as\nface recognition (Liu et al. 2017). Perhaps for that reason, it\nhas always presented an enticing challenge to many AI re-\nsearchers throughout the decades (Lenat, Prakash, and Shep-\nherd 1985), (Marcus 1998), (Singh 2002), (Chklovski 2003).\nThere is also the widely held belief that, for a ‘general AI’\nto truly emerge, commonsense reasoning is one problem\n(among others) that will need to be solved in a sufﬁciently\nrobust manner (Baroni et al. 2017). A more functional rea-\nson for increased interest in commonsense reasoning is the\nrise of chatbots and other such ‘conversational AI’ services\n(e.g., Siri and Alexa) that represent an important area of in-\nnovation in industry (Ram et al. 2018), (Gao, Galley, and Li\n2018), (Basu 2019), (Young et al. 2017). Recently, the US\nDepartment of Defense also launched a machine common\nsense (MCS) program in which a diverse set of researchers\nand organizations, including the Allen Institute of Artiﬁcial\nIntelligence, is involved (Sap et al. 2019a).\nDespite the success of these models, there is some evi-\ndence (not necessarily all quantitative) to suggest that the\nmodels are still superﬁcial i.e. do not have the same com-\nmonsense abilities as humans, despite what the performance\nnumbers suggest. (Davis and Marcus 2015) suggested in a\nseminal review article that, for truly human-level, perfor-\nmance ‘knowledge of the commonsense world– time, space,\nphysical interactions, people, and so on—will be necessary.’\nWhile we do not deny the theoretical possibility that a lan-\nguage representation model such as BERT, RoBERTa or\nGPT-3 may have learned these different aspects of the real\nworld purely by ‘reading’ large corpora of natural language\n(Devlin et al. 2018), (Liu et al. 2019), (Brown et al. 2020),\nwe do claim that such possibilities can (and must) be tested\nthrough rigorous evaluation. Unfortunately, as we cover in\nthe Related Work, there has been little to no work by way of\nconducting such a systematic and focused analysis (with the\ncentral goal of evaluating generalization of a system on com-\nmonsense reasoning), using a publicly available and replica-\nble system, though there is plenty of precedent for this type\nof study, as discussed in the Related Work.\nIn this paper, we attempt to address this gap by care-\nfully designing and conducting an empirical study with the\narXiv:2011.09159v1  [cs.CL]  18 Nov 2020\nspeciﬁc intent of answering the question of whether ﬁne-\ntuned commonsense language models generalize in robust\nways. Our goal is not to attack either a model or a particular\nbenchmark (or a set of benchmarks) but to present clear (and\ncautionary) evidence that the current set of evaluations (and\nevaluation practices) and reported results need to be consid-\nered with more skepticism by the community. Considering\nthe pace at which research on commonsense reasoning con-\ntinues, we posit that this is a timely study and could serve as\na methodology for future such studies assessing the general-\nization of commonsense AI.\nRelated Work\nAs noted in the Introduction, commonsense reasoning has\nrecently experienced a resurgence in the AI research com-\nmunity. Central references that attest to this resurgence in-\nclude (Davis 2017), (Davis 2014), (Zang et al. 2013), (Tan-\ndon, Varde, and de Melo 2018), (Sap et al. 2020), (Zellers\net al. 2019a). We also noted that commonsense reasoning\nhas also been an ambitious agenda in the past. It is not\nfeasible to cite all relevant work herein; instead, we refer\nthe reader both to the review article by (Davis and Marcus\n2015), as well as more recent surveys on commonsense rea-\nsoning tasks and benchmarks (Storks, Gao, and Chai 2019).\nMuch progress has been made on speciﬁc kinds of com-\nmonsense reasoning, especially in reasoning about time and\ninternal relations (Ladkin 1986), (Pinto and Reiter 1995),\nreasoning about actions and change (Narayanan 2000), and\nthe sign calculus (Davis and Marcus 2015). Semantics have\nplayed an important role in some of these successes (Ra-\njagopal et al. 2013) including ‘semantic networks’ and other\nstructured sources, important ones of which include Con-\nceptNet (Havasi, Speer, and Alonso 2007). WordNet (Miller\n1995) and Cyc (Lenat 1995). These resources have been fre-\nquently applied in multiple reasoning systems (Botschen,\nSorokin, and Gurevych 2018), (Angeli and Manning 2014),\n(Lin, Sun, and Han 2017). In contrast with WordNet and\nConceptNet, Cyc focuses on designing a universal schema\n(a higher-order logic) to represent commonsense asser-\ntions, which also supports reasoning systems (Panton et al.\n2006), (Ramachandran, Reagan, and Goolsbey 2005) con-\nduct richer logical inference.\nIn the last decade, in particular, as more accurate but also\nmore ‘non-interpretable’ (or explainable) models like neu-\nral networks have become more prevalent, a relevant line\nof research has developed in ‘adversarially attacking’ these\nmodels to understand their weaknesses in a variety of do-\nmains (Akhtar and Mian 2018), (Z ¨ugner, Akbarnejad, and\nG¨unnemann 2018), (Ilyas et al. 2018). Other problems, that\nrequire more precise inputs and prompts, include bias in the\ndata and also in the model (Kim et al. 2019), (Lu et al. 2018).\nThis line of work is valuable precedent for our own work,\nand there has been some early work already on conducting\nsuch robustness tests on transformer-based language repre-\nsentation models (Hsieh et al. 2019), (Cheng et al. 2020),\n(Michel et al. 2019). However, this paper signiﬁcantly de-\nparts in at least one respect from these lines of work–namely,\nwe do not adversarially or selectively modify the input or\nFigure 1: Question-answer instances from ﬁve common-\nsense benchmark datasets used for the evaluations in this\npaper. The question-like ‘prompt’ is highlighted in yellow,\nand the correct answer in blue.\nthe model in any way. Our results show, in fact, that so-\nphisticated adversarial modiﬁcations are not necessary for\nconcluding that generalization is a concern for transformer-\nbased QA models.\nTheoretical work on commonsense reasoning along the\nlines of cognitive science and computational commonsense\nparadigms should also be noted (Hobbs et al. 1987), (Hobbs\nand Kreinovich 2001), (Gordon and Hobbs 2017). We note\nthis line of work because it could potentially be used for\ndesigning better evaluations, as well as for diagnosing why\nsome transformer-based models are not generalizing better,\ndespite (individually) good performance across the board on\nmany benchmark datasets.\nBackground and Preliminaries\nCommonsense Question Answering (QA)\nBenchmarks\nAs noted in both the introduction and the related work, com-\nmonsense reasoning has emerged as an important and chal-\nlenging research agenda in the last several years. The usual\nway to evaluate systems (with the state-of-the-art systems\nbeing based, in some signiﬁcant way, on the transformer-\nbased models described in the next section) purporting to\nbe capable of commonsense reasoning in the natural lan-\nguage setting is to use question answering benchmarks with\nmultiple choices per ‘question’ from which exactly one\ncorrect answer must be selected by the system. The NLP\n(and broader AI) community has developed numerous such\nbenchmarks, especially in the last 3-5 years, using a range\nof methodologies both for acquiring the questions and for\ndevising the answers. We describe the ﬁve benchmarks used\nin the research study in this paper below, with references for\nfurther reading. Examples are provided in Figure 1.\n1. aNLI (Abductive Natural Language Inference): Ab-\nductive Natural Language Inference (aNLI) 2 (Bhagavat-\nula et al. 2020) is a new commonsense benchmark dataset\ndesigned to test an AI system’s capability to apply ab-\nductive reasoning and common sense to form possible ex-\nplanations for a given set of observations. Formulated as\na binary-classiﬁcation task, the goal is to pick the most\nplausible explanatory hypothesis given two observations\nfrom narrative contexts.\n2. HellaSwag: HellaSW AG3 (Zellers et al. 2019b) is a\ndataset for studying grounded commonsense inference.\nIt consists of 70,000 multiple choice questions about\n‘grounded situations’: each question comes from one of\ntwo domains–Activity Net or wikiHow–with four answer\nchoices about what might happen next in the scene. The\ncorrect answer is the (real) sentence for the next event;\nthe three incorrect answers are adversarially generated\nand human-veriﬁed, ensuring a non-trivial probability of\n‘fooling’ machines but not (most) humans.\n3. PIQA: Physical Interaction QA (PIQA) 4 (Bisk et al.\n2020) is a novel commonsense QA benchmark for naive\nphysics reasoning, primarily concerned with testing ma-\nchines on how we interact with everyday objects in com-\nmon situations. It tests, for example, what actions a phys-\nical object ‘affords’ (e.g., it is possible to use a cup as a\ndoorstop), and also what physical interactions a group of\nobjects afford (e.g., it is possible to place an computer on\ntop of a table, but not the other way around). The dataset\nrequires reasoning about both the prototypical use of ob-\njects (e.g., glasses are used for drinking) but also non-\nprototypical (but practically plausible) uses of objects.\n4. Social IQA: Social Interaction QA 5 (Sap et al. 2019b)\nis a QA benchmark for testing social common sense.\nIn contrast with prior benchmarks focusing primarily on\nphysical or taxonomic knowledge, Social IQA is mainly\nconcerned with testing a machine’s reasoning capabil-\nities about people’s actions and their social implica-\ntions. Actions in Social IQA span many social situa-\ntions, and answer-candidates contain both human-curated\nanswers and (‘adversarially-ﬁltered’) machine-generated\ncandidates.\n2https://storage.googleapis.\ncom/ai2-mosaic/public/alphanli/\nalphanli-train-dev.zip\n3https://storage.googleapis.\ncom/ai2-mosaic/public/hellaswag/\nhellaswag-train-dev.zip\n4https://storage.googleapis.com/\nai2-mosaic/public/physicaliqa/\nphysicaliqa-train-dev.zip\n5https://storage.googleapis.\ncom/ai2-mosaic/public/socialiqa/\nsocialiqa-train-dev.zip\nTable 1: ‘Instance formats’ of commonsense QA bench-\nmarks. The -> is used to separate what is given (e.g., obs1,\nobs2 for aNLI) and the answer choices (hypo1, hypo2).\nHere, ‘obs’, ‘hypo’, ‘opt’, ‘ans’, ‘sol’ and ‘ctx’ stand forob-\nservation, hypothesis, option, answer, solution and context\nrespectively.\nFormat\naNLI obs1, obs2->hypo1, hypo2\nHellaSwag ctx a, ctx b->opt1, opt2,\nopt3, opt4\nPIQA goal->sol1, sol2\nSocial IQA context, question->ans1, ans2, ans3\nCyc question->ans0, ans1, ans2, ans3, ans4\n5. CycIC: Cyc Intelligence Challenge dataset (Cyc) 6 is a\nset of multiple choice and true/false questions requir-\ning general common sense knowledge and reasoning in\na very broad variety of areas, including simple reason-\ning about time, place, everyday objects, events, and sit-\nuations. Some questions may require some logic to get\nthe correct answer. Here, we only use the multiple-choice\nquestions (and not true/false questions) for experiments.\nOne important aspect to note about these benchmarks\nis that, while all offer multiple-choice answer formats, the\n‘prompt’ is not always a question. For example, in the case\nof the aNLI benchmark, the ‘prompt’ is a set of two observa-\ntions, and the ‘choices’ are two hypotheses (of which the one\nthat best ﬁts these observations should be selected). For this\nreason, we refer to each question and corresponding answer\nchoices as an instance. The instance formats of the bench-\nmarks described above are stated in Table 1.\nTransformer-based Models and RoBERTa\nAs covered in the Related Work, transformer-based models\nhave rapidly emerged as state-of-the-art in the natural lan-\nguage processing community, both for speciﬁc tasks like\nquestion answering, but also for deriving ‘contextual em-\nbeddings’ (for more details, we refer the reader to the ci-\ntations in that section). RoBERTa has been a particularly\nsuccessful model, and is (in essence), a highly optimized\nand better-trained version of BERT. Unlike the most recent\nmodel (GPT-3), a pre-trained version of RoBERTa is fully\navailable for researchers to use and can be ‘ﬁne-tuned’ for\nspeciﬁc tasks (Liu et al. 2019). Unsurprisingly, many of the\nsystems occupying the top-5 leaderboard positions 7 for the\ncommonsense reasoning benchmarks described earlier are\nbased on RoBERTa in some signiﬁcant manner. The exper-\niments in this paper, described next, use a publicly avail-\nable RoBERTa Ensemble model8 that was not developed\nby any the authors, either in principle or practice, can be\ndownloaded and replicated very precisely, and on average,\n6https://storage.googleapis.com/\nai2-mosaic/public/cycic/CycIC-train-dev.zip\n7https://leaderboard.allenai.org/.\n8https://github.com/isi-nlp/ai2/tree/base\nTable 2: State-of-the-art (SOTA) performance and number\n(num.) of instances in training (train.) and development\n(dev.) partitions of ﬁve commonsense QA benchmarks.\nBenchmark Num. train.\nset instances\nNum. dev.\nset instances\nSOTA\naccuracy\naNLI 169,654 1,532 0.873\nHellaSwag 39,905 10,042 0.939\nPIQa 16,113 1,838 0.901\nSocial IQA 33,410 1,954 0.832\nCyc 6,570 947 0.913\nachieves over 80% on the ﬁve benchmarks when ﬁne-tuned\non the benchmark without any change to the model itself.\nExperiments\nWe design and conduct a rigorous series of experiments\n(with full statistical analyses) to study the question noted in\nthe title of this paper itself. While the data and system have\nalready been described in the previous section, we use the\nnext section to provide some relevant technical and method-\nological details, followed by the results.\nData and Methodology\nWe use the ﬁve benchmarks described earlier for our eval-\nuation datasets. Each of these benchmarks is publicly avail-\nable, and even has a leaderboard dedicated to it. Many re-\nsearchers have used these benchmarks for evaluating com-\nmonsense reasoning models (Storks, Gao, and Chai 2019).\nNote that the ‘test’ partition of these benchmarks is not avail-\nable publicly; hence, for research purposes, the development\nor ‘dev.’ set is used as the test set. To ensure replication,\nwe do the same. Our goal here is not to develop a supe-\nrior algorithm that may do better on the unknown test set,\nbut to explore the capabilities of a popular language model-\nbased solution to this problem. Details on the benchmarks’\ntraining and development set partitions, as well as current\nstate-of-the-art (SOTA) performance by a highly optimized\nRoBERTa system on the leaderboard (designed and ﬁne-\ntuned on just that speciﬁc task or benchmark) are shown 9\nin Table 2. As described in the previous section, we used\nthe RoBERTa Ensemble model for our experiments, which\nachieves over 80% performance (on average) over the ﬁve\nbenchmarks and is not substantially different from the SOTA\nmodel. While some formatting was necessary in order to en-\nsure that the RoBERTa Ensemble system, when trained on\none dataset (say Cyc), could be applied to instances from\nanother dataset (say Social IQA), we did not modify any of\nthe information content within the instances (either in the\nquestions/prompts or in the answers).\nFurthermore, since one of our goals is to test the gener-\nalization ability of precisely such models (i.e. a ‘common-\nsense QA’ model that has been trained on one kind of com-\n9These numbers were acquired from the Allen Institute’s\nleaderboards in late August, and may have shifted slightly.\nmonsense data, but evaluated on another), we deﬁne a per-\nformance loss metric (PL) as:\nPL = Accindomain − Accoutdomain\nAccindomain\n. (1)\nHere, Accindomain is the ‘in-domain’ prediction accuracy\nachieved on the benchmark when we train the model on that\nbenchmark’s training set partition and evaluate on the devel-\nopment set from the same benchmark; Accoutdomain is the\n‘out-domain’ prediction accuracy achieved when one bench-\nmark is used for training and another benchmark’s dev. set\npartition is used for evaluation. Since there are four training\noptions (the other four benchmarks) once the dev. bench-\nmark has been ﬁxed, it is possible to compute four separate\nAccoutdomain metrics for any given benchmark. The PL has\nan intuitive interpretation: how much of the ‘in-domain’ per-\nformance (in percentage terms) does the model ‘lose’ when\nthe evaluation benchmark is changed? Given the descrip-\ntions of the ﬁve benchmarks used in this paper in the previ-\nous section, we would expect that the PL would be greatest\nfor the benchmarks that are highly narrow in their domains\n(e.g., if we train on Social IQA and test on PIQA) as opposed\nto cases when the training benchmark is broad (such as when\nthe training benchmark is aNLI, HellaSwag and Cyc). In the\nnext section, we assess the validity of this hypothesis. Note\nthat a high PL implies that the model is generalizing less.\nA negative PL is theoretically possible (and potentially un-\nbounded from below, as Accindomain tends to 0), but not\nobserved. The PL can never be greater than 100% (when\nAccoutdomain = 0).\nStatistical signiﬁcance is an important part of such stud-\nies. To ensure that results are valid, we conduct two kinds of\nsigniﬁcance testing for each ‘out-domain’ experiment. Both\ntests use the ‘base’ or in-domain setting (train. and dev. par-\ntitions come from the same benchmark during testing) as the\n‘reference’ distribution against which we test for equality of\nmeans. Speciﬁcally, we conduct the paired Student’s t-test\nbetween the in-domain accuracy data and the out-domain\naccuracy data when the dev. set of the reference and the test\ndistribution coincide (but the train. does not). For example,\nfor the experiment where we train on aNLI, but test on Hel-\nlaSwag, we conduct the paired Student’s t-test between the\nout-domain accuracy data on HellaSwag and the in-domain\naccuracy data on HellaSwag (since the same dev. set was\nused for collecting both sets of data). Since the results are\n‘aligned’,10 the paired Student’s t-test is applicable.\nThe second test (the unpaired Student’s t-test) is the con-\nverse of the above–namely, when the training sets coincide\nbut thedev.sets are different. Since thedev.sets are different,\nwe can only test for equality of means in the unpaired set-\nting, since data points are not aligned across systems. Taking\nagain the above example, we would conduct the unpaired\nStudent’s t-test between the out-domain result (trained on\naNLI, but tested on HellaSwag) and the in-domain result\n10The reason being that the ‘reference’ (the model trained on\nHellaSwag) and the out-domain system (the model trained on\naNLI) are both tested on the same questions, namely the dev. par-\ntition of HellaSwag.\nachieved on aNLI (trained on aNLI, tested on aNLI). Note\nthat both tests are complementary and necessary in this kind\nof experiment, since both the training and test settings can\nchange. The ﬁrst test evaluates signiﬁcance of results hold-\ning the dev.set ﬁxed, while the second test keeps the training\nset ﬁxed.\nResults\nTable 3: Accuracy (fraction of questions of which the an-\nswers are correctly predicted) of the Roberta Ensemble\nmodel in different evaluation settings. The row represent the\nbenchmark of which the training set partition is used to train,\nwhile the column name represents the benchmark of which\nthe development (dev.) set partition is used to test.\naNLI Hella\nSwag\nPIQA Social\nIQA\nCyc\naNLI 0.819 0.611 0.702 0.531 0.442\nHellaSwag 0.681 0.835 0.699 0.515 0.351\nPIQA 0.68 0.564 0.756 0.51 0.371\nSocial IQA 0.688 0.604 0.687 0.769 0.508\nCyc 0.628 0.49 0.628 0.493 0.811\nTable 4: The performance loss (PL; Equation 1) of the\nRoberta Ensemble model when evaluated on a different\nbenchmark (‘out-domain’) than it was trained on (‘in-\ndomain’). The row represent the benchmark of which the\ntraining set partition is used to train, while the column name\nrepresents the benchmark of which the development (dev.)\nset partition is used to test.\naNLI Hella\nSwag\nPIQA Social\nIQA\nCyc\naNLI 0 0.268 0.071 0.309 0.455\nHellaSwag 0.168 0 0.075 0.330 0.567\nPIQa 0.169 0.325 0 0.336 0.543\nSocial IQA 0.159 0.276 0.09 0 0.374\nCyc 0.233 0.413 0.169 0.358 0\nThe absolute accuracy results of the RoBERTa Ensemble\nmodel when trained on one benchmark and tested on an-\nother are tabulated in Table 3. Overall, we see very clear ev-\nidence that, regardless of the train. dataset used, out-domain\nperformance inevitably declines, sometimes by signiﬁcant\nmargins. It is telling that these declines occur, regardless of\nwhether we train on a ‘broader’ commonsense benchmark\n(like HellaSwag) or whether we test on a broad or narrow\nbenchmark (e.g., PIQA). For better analysis of relative dif-\nferences, we tabulate the performance loss (PL) metric in\nTable 4. The diagonals are all 0, since the performance loss\nis 0 when the training and testing benchmark are the same\n(per Equation 1). The numbers correspond closely to those\nin Table 3, but generally tend in the opposite direction (i.e.\nPL is lower when the absolute accuracy is higher for a test\nbenchmark, all else being the same.).\nRecall that, in the previous section, we stated a hypothesis\nthat we expect test benchmarks that are too ‘narrow’ (such\nas PIQA or Social IQA) to exhibit more PL than benchmarks\nwhich are broader, except (possibly) when the training set is\nalso broad. Table 4 shows that the data on this question is\nsurprisingly mixed. In particular, PL on PIQA is always low\nwhen it is used as a test set, despite the fact that it covers\nsuch a narrow domain. In contrast, the PL on Social IQA is\nhigh (usually, the second highest after Cyc). Similarly, with\nrespect to testing on the ‘broader’ benchmarks, PL is low on\naNLI but higher on HellaSwag. When training on aNLI or\nHellaSwag, and comparing to training on either PIQA or So-\ncial IQA, we ﬁnd that the difference is not considerable e.g.,\nthe system trained on HellaSwag achieves PL of 16.8%, 33%\nand 56.7% respectively on aNLI, Social IQA and Cyc, and\nthe system trained on PIQA achieves PL of 16.9%, 33.6%\nand 54.4% respectively on the same three test sets. There-\nfore, it is simply not true that performance loss is observed\nsimply because the ‘domains are different’ (though by def-\ninition, they are all commonsense benchmarks), which is\nsometimes the cause in similarly designed (and more tradi-\ntional) transfer learning and weak supervision experiments.\nInterestingly, based both on the data in Tables 3 and 4, we\nﬁnd clear evidence that Cyc is the most ‘different’ bench-\nmark, since the PL is markedly higher with Cyc used as the\ntraining (and also the testing) dataset. Namely, the PLs ob-\nserved in the Cyc ‘column’ are the highest among the val-\nues in the corresponding training dataset’s ‘row’ e.g., when\nPIQA is used as the training dataset, the PL of 54.3% ob-\nserved for Cyc is the highest in that row.\nSigniﬁcance Analysis. The paired Student’s t-test\nmethodology (the ‘ﬁrst’ signiﬁcance test mentioned in the\nprevious section) was ﬁrst applied to all values in Table 3,\nand compared against the ‘diagonal’ value. For example, the\nresults obtained when training (respectively) on HellaSwag,\nPIQA, Social IQA and Cyc, and testing on aNLI, are tested\nindividually using the test statistic from a paired Student’s\nt-test analysis against the the in-domain aNLI setting (the\ndiagonal accuracy value of 81.9% in Table 3). We ﬁnd that\nthe null hypothesis can be rejected at the 99% level for all\nsuch paired tests, for all test benchmarks. The differences in\naccuracy are therefore signiﬁcant, as are the PLs in Table 4,\nwhich is just a scaled, afﬁne transformation of Table 3.\nWe also conducted the unpaired Student’s t-test (the ‘sec-\nond’ signiﬁcance test mentioned in the previous section).\nFor example, we compared thedev.set results on HellaSwag,\nPIQA, Social IQA and Cyc (individually), when trained on\naNLI, against the ( dev. set) results obtained on aNLI for\nthe same model. Therefore, the training set (and hence, the\nmodel) is identical in all settings, but thedev. set is different.\nThe results for the unpaired Student’s t-test showed that the\nmajority of results are signiﬁcant at the 99% level; however,\nfor the models trained on Social IQA and Cyc, we found\nthat the (respective) results obtained when aNLI’sdev. set is\nused, are signiﬁcant only at the 95% level against the ‘refer-\nence’ results obtained when the dev. set of Social IQA and\nCyc is used, respectively. A lone insigniﬁcant result (even at\nthe 90% level) is when we train on HellaSwag and test on\naNLI, and compare it to the results of the same model tested\non HellaSwag.\nTable 5: Consistency analysis of out-domain models’\n(ODMs) predictions on the questions on which the in-\ndomain model got correct (top) and incorrect (bottom) pre-\ndictions. The column name indicates the test benchmark.\nThe sum of each column (in the respective table) is the\nnumber of questions on which the corresponding in-domain\nmodel gets correct/incorrect answers.\naNLI Hella\nSwag\nPIQA Social\nIQA\nCyc\nAll ODMs give the\nright answer\n512 2,421 707 297 100\nSome ODMs give\nthe right answer\n683 5,033 630 1,043 503\nAll ODMs give\nsame wrong answer\n59 340 52 66 48\nSome ODMs dis-\nagree on wrong an-\nswers\n0 594 0 97 118\naNLI Hella\nSwag\nPIQA Social\nIQA\nCyc\nAll ODMs give the\nright answer\n31 101 55 33 7\nSome ODMs give\nthe right answer\n170 957 266 296 89\nAll ODMs give\nsame wrong answer\n77 195 128 55 20\nSome ODMs dis-\nagree on wrong an-\nswers\n0 401 0 67 63\nDiscussion on Observed Differences. The previous re-\nsults clearly illustrate that the choice of the training bench-\nmark matters, often in surprising (but statistically signiﬁ-\ncant) ways. One hypothetical reason why this behavior is\nobserved is that PIQA may just be an ‘easier’ dataset, and\nCyc may just be a ‘harder’ dataset (hence leading to lower\nand higher PLs respectively). However, if this were the case,\nthen the ‘diagonal’ values in Table 3 would reﬂect it. The ob-\nserved values in Table 3 tell a different story; all results are\nclustered in the range of 75-83%, and the in-domain result\nfor PIQA is similar to that of Social IQA, suggesting that\nthe two are of reasonably equal difﬁculty. Yet, one proves\nto be more ‘generalizable’ than another in out-domain set-\ntings. Another hypothetical reason could be the ‘number’ of\nanswer choices available. The hypothesis is that, once there\nis a mismatch between the training and testing setting, the\nperformance becomes more ‘random’. While this hypothe-\nsis may explain why Cyc has the highest PL (it has 5 an-\nswer choices, generally, for every question; see Table 1), it\ndoes not explain why Social IQA (which has three answer\nchoices per question) has higher PL than HellaSwag (which\nhas four). Furthermore, the large differences in accuracy ob-\nserved in the out-domain settings in Table 3 cannot be ex-\nplained only by differences in the number of answer choices\navailable in the different benchmarks. Finally, if the model\nhad become more ‘random’, expected accuracy on Cyc and\naNLI would be around 20% and 50% respectively (assuming\nrelatively equal distribution of answer choices); however,\nthe accuracies are signiﬁcantly higher, according to Table\n3. The model is clearly learning ‘something’. Furthermore,\ngiven the relatively large sizes and broad domains covered\nby some of these datasets (see Table 2; even Cyc, the small-\nest ‘training’ dataset has more than 6,000 questions in its\ntraining partition), it is unlikely that the fault lies purely with\nthe data. The most plausible explanation that remains is that\nthe ﬁne-tuned RoBERTa model is subject to some degree of\ndataset bias, and that leaderboard performance numbers on\nindividual benchmarks should not necessarily be assumed\nto be a reﬂection of advances in human-like ‘commonsense\nreasoning’ without signiﬁcantly more qualiﬁcation.\nAnalysis of Incorrect/Correct Question Partitions.\nWhile Tables 3 and 4 reveal that there are differences be-\ntween in-domain and out-domain settings, it is not clear\nfrom that data where the different models are ‘going wrong’;\nspeciﬁcally, where the in-domain training produces the right\npredictions, but an out-domain training setting does not. Ta-\nble 5 produces the results of two consistency analyses, with\nthe top table considering the benchmark-speciﬁc questions\nthat the in-domain model (corresponding to that benchmark)\ngot correct, and the bottom table for the questions that the in-\ndomain model got wrong. Intriguingly, we ﬁnd that, while\nrelatively rare, there are questions (e.g., in the third row in\nthe top table) that the in-domain model gets right but where\nall out-domain models (ODMs) give the same wrong an-\nswer. On the aNLI and PIQAdev. sets, this situation is more\ncommon than the situation when all ODMs get the wrong\nanswer, but at least two ODMs differ on the answer (the\nsame situation arises in the bottom table). More research\nis required to investigate the deeper implications of this re-\nsult, but one hypothesis is a ‘choice bias’, at least in aNLI\nand PIQA. A choice bias arises when all answer choices\ndo not have the same prior probability of being selected by\nthe model. In particularly egregious cases, the question itself\nmay not be necessary11 for the model to choose the right an-\nswer. A set of experimental studies, not dissimilar in design\nto the studies herein, could be used to prove or disprove the\nexistence of a choice bias by not using the actual ‘question’\n(only the answer choices) during test-time (and subsequent\nto data collection, testing for signiﬁcance against a random\nbaseline). We leave such a study for future work.\nQualitative Analysis. To gain an ‘intuitive’ sense of what\nthe different out-domain models are learning, it is also in-\nstructive to consider samples of questions from all ﬁve\nbenchmarks such that the in-domain model made the correct\nprediction but all out-domain models made the same wrong\nprediction. Figure 2 lists some examples of such questions.\nWe ﬁnd that, in some cases, the ‘wrong’ answer is an ‘easy’\nwrong answer (yet either the in-domain or the out-domain\nmodel got it wrong). The example for aNLI in the third\ncolumn (IDMW & ODMR) is particularly telling. In other\ncases, the wrong answer is qualitatively ‘harder’ (such as the\n11This could arise, for example, if only one answer choice is\ngrammatical, non-nonsensical, or both.\nFigure 2: Example instances for selected settings from Tables 5 and 6. IDMR and ODMR represent the population of instances\nwhere the in-domain model is right, and all out-domain models are also right. Similarly, IDMW and ODMW represent the\ninstances where the in-domain model was wrong, and all out-domain models retrieved the same wrong answer. For each\ninstance, the right and answers are highlighted in green and red, respectively. Note that, in the last column, it may be that the\nIDM wrong answer differs from the (same) wrong answer retrieved by the ODMs, in which case, we highlight the former in\norange, and the latter in red.\nexample for Social IQA in the same column). In yet other\ncases, we may be more inclined to agree with the machine\nlearning models, such as for the Social IQA row and the\nlast column. Some of the questions may require a degree of\nspecialized or culture-speciﬁc knowledge (such as the Hel-\nlaSwag question in the last column).\nThe analysis here suggests several different aspects of an\nevaluation setup that are serving as ‘confounds’ to testing\nnot only the generalization of commonsense reasoning sys-\ntems, but commonsense reasoning ability itself. Some of the\nquestions in these benchmarks may not be commonsense\nquestions, in the way that cognitive scientists have under-\nstood the phenomenon, and may have a domain- or culture-\nspeciﬁc dependence that may cause noise in evaluations.\nHowever, in a few questions, we clearly see that the model\ngets some very obvious answers wrong. Therefore, it is not\nthe case that the model only has trouble on difﬁcult cases,\nthough this also plays a role in the results that we observe.\nAddressing the ﬂaws in these benchmarks, as well as using\nadditional metrics (such as the performance loss) to evalu-\nate candidate models, are both important avenues for future\nresearch. Eventually, a more advanced kind of benchmark\nmay be warranted, one that has weighted or dynamically\ngenerated questions. Some researchers have suggested mov-\ning beyond multiple-choice questions altogether and focus-\ning on generation tasks instead (Lin et al. 2019), (Radev et\nal. 2020).\nConclusion\nLanguage representation models such as BERT, RoBERTa\nand (more recently) GPT-3 have received proliﬁc academic\nand media attention due to their ability to achieve near-\nhuman (or even, ‘super-human’) performance on a range of\nindividual benchmarks, including on several commonsense\nbenchmarks. In this paper, we showed that there is still a sig-\nniﬁcant performance drop when one such competitive model\nis trained on one kind of commonsense dataset but tested on\nanother. It is important to remember that all datasets consid-\nered in this work were supposed to test ‘commonsense rea-\nsoning’, although some are more diverse and broader than\nothers. The breadth of either the training or testing dataset\nis not found to signiﬁcantly impact the overall conclusions.\nAt minimum, our analyses suggest a potential source of\ndataset bias when evaluating commonsense reasoning. The\nlarge values of performance loss observed in several set-\ntings strongly suggest that these commonsense models are\nnot generalizing, and that more research is required before\n‘human-like’ commonsense reasoning performance can be\nconﬁdently said to be within reach of these systems.\nReferences\n[Akhtar and Mian 2018] Akhtar, N., and Mian, A. 2018.\nThreat of adversarial attacks on deep learning in computer\nvision: A survey. IEEE Access 6:14410–14430.\n[Angeli and Manning 2014] Angeli, G., and Manning, C.\n2014. Naturalli: Natural logic inference for common sense\nreasoning. 534–545.\n[Bar-Hillel 1960] Bar-Hillel, Y . 1960. The present status of\nautomatic translation of languages. In Advances in comput-\ners, volume 1. Elsevier. 91–163.\n[Baroni et al. 2017] Baroni, M.; Joulin, A.; Jabri, A.;\nKruszewski, G.; Lazaridou, A.; Simonic, K.; and Mikolov,\nT. 2017. Commai: Evaluating the ﬁrst steps towards a use-\nful general ai. arXiv preprint arXiv:1701.08954.\n[Basu 2019] Basu, K. 2019. Conversational ai: Open domain\nquestion answering and commonsense reasoning. arXiv\npreprint arXiv:1909.08258.\n[Bhagavatula et al. 2020] Bhagavatula, C.; Bras, R. L.;\nMalaviya, C.; Sakaguchi, K.; Holtzman, A.; Rashkin, H.;\nDowney, D.; tau Yih, W.; and Choi, Y . 2020. Abductive\ncommonsense reasoning. In International Conference on\nLearning Representations.\n[Bisk et al. 2020] Bisk, Y .; Zellers, R.; Bras, R. L.; Gao, J.;\nand Choi, Y . 2020. Piqa: Reasoning about physical com-\nmonsense in natural language. In Thirty-Fourth AAAI Con-\nference on Artiﬁcial Intelligence.\n[Botschen, Sorokin, and Gurevych 2018] Botschen, T.;\nSorokin, D.; and Gurevych, I. 2018. Frame- and\nentity-based knowledge for common-sense argumentative\nreasoning. In Proceedings of the 5th Workshop on Argu-\nment Mining , 90–96. Brussels, Belgium: Association for\nComputational Linguistics.\n[Brown et al. 2020] Brown, T. B.; Mann, B.; Ryder, N.; Sub-\nbiah, M.; Kaplan, J.; Dhariwal, P.; Neelakantan, A.; Shyam,\nP.; Sastry, G.; Askell, A.; et al. 2020. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165.\n[Cheng et al. 2020] Cheng, M.; Yi, J.; Chen, P.-Y .; Zhang,\nH.; and Hsieh, C.-J. 2020. Seq2sick: Evaluating the ro-\nbustness of sequence-to-sequence models with adversarial\nexamples. In AAAI, 3601–3608.\n[Chklovski 2003] Chklovski, T. 2003. Learner: a system for\nacquiring commonsense knowledge by analogy. InProceed-\nings of the 2nd international conference on Knowledge cap-\nture, 4–12.\n[Davis and Marcus 2015] Davis, E., and Marcus, G. 2015.\nCommonsense reasoning and commonsense knowledge\nin artiﬁcial intelligence. Communications of the ACM\n58(9):92–103.\n[Davis 2014] Davis, E. 2014. Representations of common-\nsense knowledge. Morgan Kaufmann.\n[Davis 2017] Davis, E. 2017. Logical formalizations of com-\nmonsense reasoning: a survey. Journal of Artiﬁcial Intelli-\ngence Research 59:651–723.\n[Devlin et al. 2018] Devlin, J.; Chang, M.-W.; Lee, K.; and\nToutanova, K. 2018. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805.\n[Dinakar et al. 2012] Dinakar, K.; Jones, B.; Havasi, C.;\nLieberman, H.; and Picard, R. 2012. Common sense rea-\nsoning for detection, prevention, and mitigation of cyberbul-\nlying. ACM Transactions on Interactive Intelligent Systems\n(TiiS) 2(3):1–30.\n[Gao, Galley, and Li 2018] Gao, J.; Galley, M.; and Li, L.\n2018. Neural approaches to conversational ai. In The 41st\nInternational ACM SIGIR Conference on Research & De-\nvelopment in Information Retrieval, 1371–1374.\n[Gordon and Hobbs 2017] Gordon, A. S., and Hobbs, J. R.\n2017. A formal theory of commonsense psychology: How\npeople think people think. Cambridge University Press.\n[Havasi, Speer, and Alonso 2007] Havasi, C.; Speer, R.; and\nAlonso, J. 2007. Conceptnet 3: a ﬂexible, multilingual se-\nmantic network for common sense knowledge. In Recent\nadvances in natural language processing, 27–29. Citeseer.\n[Hobbs and Kreinovich 2001] Hobbs, J. R., and Kreinovich,\nV . 2001. Optimal choice of granularity in commonsense\nestimation: why half-orders of magnitude. In Proceedings\nJoint 9th IFSA World Congress and 20th NAFIPS Interna-\ntional Conference (Cat. No. 01TH8569) , volume 3, 1343–\n1348. IEEE.\n[Hobbs et al. 1987] Hobbs, J.; Croft, W.; Davies, T.; Ed-\nwards, D.; and Laws, K. 1987. Commonsense meta-\nphysics and lexical semantics. Computational linguistics\n13(3-4):241–250.\n[Hsieh et al. 2019] Hsieh, Y .-L.; Cheng, M.; Juan, D.-C.;\nWei, W.; Hsu, W.-L.; and Hsieh, C.-J. 2019. On the ro-\nbustness of self-attentive models. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Lin-\nguistics, 1520–1529.\n[Ilyas et al. 2018] Ilyas, A.; Engstrom, L.; Athalye, A.; and\nLin, J. 2018. Black-box adversarial attacks with limited\nqueries and information. arXiv preprint arXiv:1804.08598.\n[Kim et al. 2019] Kim, B.; Kim, H.; Kim, K.; Kim, S.; and\nKim, J. 2019. Learning not to learn: Training deep neural\nnetworks with biased data. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition , 9012–\n9020.\n[Ladkin 1986] Ladkin, P. B. 1986. Time representation: A\ntaxonomy of internal relations. In AAAI, 360–366.\n[Lenat, Prakash, and Shepherd 1985] Lenat, D. B.; Prakash,\nM.; and Shepherd, M. 1985. Cyc: Using common sense\nknowledge to overcome brittleness and knowledge acquisi-\ntion bottlenecks. AI magazine 6(4):65–65.\n[Lenat 1995] Lenat, D. B. 1995. Cyc: A large-scale in-\nvestment in knowledge infrastructure. Commun. ACM\n38(11):33–38.\n[Lin et al. 2019] Lin, B. Y .; Shen, M.; Xing, Y .; Zhou, P.; and\nRen, X. 2019. Commongen: A constrained text generation\ndataset towards generative commonsense reasoning. arXiv\npreprint arXiv:1911.03705.\n[Lin, Sun, and Han 2017] Lin, H.; Sun, L.; and Han, X.\n2017. Reasoning with heterogeneous knowledge for com-\nmonsense machine comprehension. In EMNLP.\n[Liu and Singh 2004] Liu, H., and Singh, P. 2004. Concept-\nnet—a practical commonsense reasoning tool-kit. BT tech-\nnology journal 22(4):211–226.\n[Liu et al. 2017] Liu, W.; Wen, Y .; Yu, Z.; Li, M.; Raj, B.; and\nSong, L. 2017. Sphereface: Deep hypersphere embedding\nfor face recognition. In Proceedings of the IEEE conference\non computer vision and pattern recognition, 212–220.\n[Liu et al. 2019] Liu, Y .; Ott, M.; Goyal, N.; Du, J.; Joshi,\nM.; Chen, D.; Levy, O.; Lewis, M.; Zettlemoyer, L.; and\nStoyanov, V . 2019. Roberta: A robustly optimized bert pre-\ntraining approach. arXiv preprint arXiv:1907.11692.\n[Lu et al. 2018] Lu, K.; Mardziel, P.; Wu, F.; Amancharla, P.;\nand Datta, A. 2018. Gender bias in neural natural language\nprocessing. arXiv preprint arXiv:1807.11714.\n[Marcus 1998] Marcus, G. F. 1998. Rethinking eliminative\nconnectionism. Cognitive psychology 37(3):243–282.\n[Michel et al. 2019] Michel, P.; Li, X.; Neubig, G.; and Pino,\nJ. M. 2019. On evaluation of adversarial perturba-\ntions for sequence-to-sequence models. arXiv preprint\narXiv:1903.06620.\n[Miller 1995] Miller, G. A. 1995. Wordnet: A lexical\ndatabase for english. 38(11):39–41.\n[Moore 1982] Moore, R. C. 1982. The role of logic in knowl-\nedge representation and commonsense reasoning. SRI Inter-\nnational. Artiﬁcial Intelligence Center.\n[Narayanan 2000] Narayanan, S. 2000. Reasoning about ac-\ntions in narrative understanding. Proceedings of the 16th\nInternational Joint Conference on Artiﬁcial Intelligence.\n[Panton et al. 2006] Panton, K.; Matuszek, C.; Lenat, D.;\nSchneider, D.; Witbrock, M.; Siegel, N.; and Shepard, B.\n2006. Common Sense Reasoning – From Cyc to Intelligent\nAssistant, volume 3864. 1–31.\n[Pinto and Reiter 1995] Pinto, J., and Reiter, R. 1995. Rea-\nsoning about time in the situation calculus. Annals of Math-\nematics and Artiﬁcial Intelligence 14:251–268.\n[Radev et al. 2020] Radev, D.; Zhang, R.; Rau, A.;\nSivaprasad, A.; Hsieh, C.; Rajani, N. F.; Tang, X.; Vyas, A.;\nVerma, N.; Krishna, P.; et al. 2020. Dart: Open-domain\nstructured data record to text generation. arXiv preprint\narXiv:2007.02871.\n[Rajagopal et al. 2013] Rajagopal, D.; Cambria, E.; Olsher,\nD.; and Kwok, K. 2013. A graph-based approach to com-\nmonsense concept extraction and semantic similarity detec-\ntion. In Proceedings of the 22nd International Conference\non World Wide Web, 565–570.\n[Ram et al. 2018] Ram, A.; Prasad, R.; Khatri, C.;\nVenkatesh, A.; Gabriel, R.; Liu, Q.; Nunn, J.; Heday-\natnia, B.; Cheng, M.; Nagar, A.; et al. 2018. Conversational\nai: The science behind the alexa prize. arXiv preprint\narXiv:1801.03604.\n[Ramachandran, Reagan, and Goolsbey 2005]\nRamachandran, D.; Reagan, P.; and Goolsbey, K. 2005.\nFirst-orderized researchcyc: Expressivity and efﬁciency in a\ncommon-sense ontology.\n[Sap et al. 2019a] Sap, M.; Le Bras, R.; Allaway, E.; Bhaga-\nvatula, C.; Lourie, N.; Rashkin, H.; Roof, B.; Smith, N. A.;\nand Choi, Y . 2019a. Atomic: An atlas of machine common-\nsense for if-then reasoning. InProceedings of the AAAI Con-\nference on Artiﬁcial Intelligence, volume 33, 3027–3035.\n[Sap et al. 2019b] Sap, M.; Rashkin, H.; Chen, D.; Bras,\nR. L.; and Choi, Y . 2019b. Social iqa: Commonsense rea-\nsoning about social interactions. In EMNLP 2019.\n[Sap et al. 2020] Sap, M.; Shwartz, V .; Bosselut, A.; Choi,\nY .; and Roth, D. 2020. Commonsense reasoning for natu-\nral language processing. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Linguistics:\nTutorial Abstracts, 27–33.\n[Singh 2002] Singh, P. 2002. The open mind common sense\nproject. KurzweilAI. net.\n[Storks, Gao, and Chai 2019] Storks, S.; Gao, Q.; and Chai,\nJ. Y . 2019. Commonsense reasoning for natural language\nunderstanding: A survey of benchmarks, resources, and ap-\nproaches. arXiv preprint arXiv:1904.01172 1–60.\n[Tandon, Varde, and de Melo 2018] Tandon, N.; Varde,\nA. S.; and de Melo, G. 2018. Commonsense knowledge in\nmachine intelligence. ACM SIGMOD Record 46(4):49–52.\n[Young et al. 2017] Young, T.; Cambria, E.; Chaturvedi, I.;\nHuang, M.; Zhou, H.; and Biswas, S. 2017. Augmenting\nend-to-end dialog systems with commonsense knowledge.\narXiv preprint arXiv:1709.05453.\n[Zang et al. 2013] Zang, L.-J.; Cao, C.; Cao, Y .-N.; Wu, Y .-\nM.; and Cun-Gen, C. 2013. A survey of commonsense\nknowledge acquisition. Journal of Computer Science and\nTechnology 28(4):689–719.\n[Zellers et al. 2019a] Zellers, R.; Bisk, Y .; Farhadi, A.; and\nChoi, Y . 2019a. From recognition to cognition: Visual com-\nmonsense reasoning. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition , 6720–\n6731.\n[Zellers et al. 2019b] Zellers, R.; Holtzman, A.; Bisk, Y .;\nFarhadi, A.; and Choi, Y . 2019b. Hellaswag: Can a ma-\nchine really ﬁnish your sentence? In Proceedings of the 57th\nAnnual Meeting of the Association for Computational Lin-\nguistics.\n[Z¨ugner, Akbarnejad, and G¨unnemann 2018] Z ¨ugner, D.;\nAkbarnejad, A.; and G ¨unnemann, S. 2018. Adversarial\nattacks on neural networks for graph data. In Proceedings\nof the 24th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, 2847–2856.",
  "topic": "Commonsense reasoning",
  "concepts": [
    {
      "name": "Commonsense reasoning",
      "score": 0.9355185031890869
    },
    {
      "name": "Computer science",
      "score": 0.7350369691848755
    },
    {
      "name": "Transformer",
      "score": 0.5916734933853149
    },
    {
      "name": "Language model",
      "score": 0.5911441445350647
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.578253984451294
    },
    {
      "name": "Generalization",
      "score": 0.5545133352279663
    },
    {
      "name": "Consistency (knowledge bases)",
      "score": 0.5424162149429321
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5218402743339539
    },
    {
      "name": "Question answering",
      "score": 0.5017457008361816
    },
    {
      "name": "Natural language processing",
      "score": 0.40109431743621826
    },
    {
      "name": "Epistemology",
      "score": 0.12312948703765869
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.08034321665763855
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 7
}