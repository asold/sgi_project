{
  "title": "SwinFace: A Multi-Task Transformer for Face Recognition, Expression Recognition, Age Estimation and Attribute Estimation",
  "url": "https://openalex.org/W4385820037",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101318656",
      "name": "Lixiong Qin",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A1964326208",
      "name": "Mei Wang",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2015559494",
      "name": "Chao Deng",
      "affiliations": [
        "China Mobile (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2105148038",
      "name": "Ke Wang",
      "affiliations": [
        "China Mobile (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1983188002",
      "name": "Xi Chen",
      "affiliations": [
        "China Mobile (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2123383674",
      "name": "Jiani Hu",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2099485827",
      "name": "Weihong Deng",
      "affiliations": [
        "Beijing University of Posts and Telecommunications"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3167584510",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3195286673",
    "https://openalex.org/W3034882062",
    "https://openalex.org/W3217435001",
    "https://openalex.org/W2515770085",
    "https://openalex.org/W2745497104",
    "https://openalex.org/W2738672149",
    "https://openalex.org/W2239239723",
    "https://openalex.org/W2118664399",
    "https://openalex.org/W2239890062",
    "https://openalex.org/W1834627138",
    "https://openalex.org/W2963377935",
    "https://openalex.org/W2548780814",
    "https://openalex.org/W2963466847",
    "https://openalex.org/W2962898354",
    "https://openalex.org/W2969985801",
    "https://openalex.org/W6793716030",
    "https://openalex.org/W3003720578",
    "https://openalex.org/W3035336958",
    "https://openalex.org/W3118530108",
    "https://openalex.org/W3118405827",
    "https://openalex.org/W2896277673",
    "https://openalex.org/W2904483377",
    "https://openalex.org/W3126991911",
    "https://openalex.org/W4226368482",
    "https://openalex.org/W2510725918",
    "https://openalex.org/W2969597887",
    "https://openalex.org/W2955216108",
    "https://openalex.org/W2440214111",
    "https://openalex.org/W2748140016",
    "https://openalex.org/W2798868324",
    "https://openalex.org/W3096840866",
    "https://openalex.org/W2962851632",
    "https://openalex.org/W2971130703",
    "https://openalex.org/W2771116199",
    "https://openalex.org/W4312771329",
    "https://openalex.org/W2147414309",
    "https://openalex.org/W2311038409",
    "https://openalex.org/W2605124728",
    "https://openalex.org/W2963882342",
    "https://openalex.org/W3001906049",
    "https://openalex.org/W2913340405",
    "https://openalex.org/W3043178985",
    "https://openalex.org/W4311033427",
    "https://openalex.org/W3012050083",
    "https://openalex.org/W3199474181",
    "https://openalex.org/W4304083983",
    "https://openalex.org/W3117658046",
    "https://openalex.org/W2949662773",
    "https://openalex.org/W2998428583",
    "https://openalex.org/W3037859578",
    "https://openalex.org/W2963342110",
    "https://openalex.org/W4226047505",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4289752563",
    "https://openalex.org/W1905153633",
    "https://openalex.org/W2404498690",
    "https://openalex.org/W2663800299",
    "https://openalex.org/W6744072679",
    "https://openalex.org/W2871667416",
    "https://openalex.org/W2341528187",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W3035682985",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2963363102",
    "https://openalex.org/W2248200858",
    "https://openalex.org/W2784318199",
    "https://openalex.org/W6630649318",
    "https://openalex.org/W2752828042",
    "https://openalex.org/W1509966554",
    "https://openalex.org/W3122081138",
    "https://openalex.org/W3104792420",
    "https://openalex.org/W3105024549",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3101998545",
    "https://openalex.org/W4206096925",
    "https://openalex.org/W3106113529",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W1782590233",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3151034650",
    "https://openalex.org/W2884585870"
  ],
  "abstract": "In recent years, vision transformers have been introduced into face recognition and analysis and have achieved performance breakthroughs. However, most previous methods generally train a single model or an ensemble of models to perform the desired task, which ignores the synergy among different tasks and fails to achieve improved prediction accuracy, increased data efficiency, and reduced training time. This paper presents a multi-purpose algorithm for simultaneous face recognition, facial expression recognition, age estimation, and face attribute estimation (40 attributes including gender) based on a single Swin Transformer. Our design, the SwinFace, consists of a single shared backbone together with a subnet for each set of related tasks. To address the conflicts among multiple tasks and meet the different demands of tasks, a Multi-Level Channel Attention (MLCA) module is integrated into each task-specific analysis subnet, which can adaptively select the features from optimal levels and channels to perform the desired tasks. Extensive experiments show that the proposed model has a better understanding of the face and achieves excellent performance for all tasks. Especially, it achieves 90.97% accuracy on RAF-DB and 0.22 $\\epsilon$-error on CLAP2015, which are state-of-the-art results on facial expression recognition and age estimation respectively. The code and models will be made publicly available at https://github.com/lxq1000/SwinFace.",
  "full_text": "1\nSwinFace: A Multi-task Transformer for\nFace Recognition, Expression Recognition,\nAge Estimation and Attribute Estimation\nLixiong Qin, Mei Wang, Chao Deng, Ke Wang, Xi Chen, Jiani Hu, Weihong Deng, Member, IEEE\nAbstract—In recent years, vision transformers have been\nintroduced into face recognition and analysis and have achieved\nperformance breakthroughs. However, most previous methods\ngenerally train a single model or an ensemble of models to\nperform the desired task, which ignores the synergy among\ndifferent tasks and fails to achieve improved prediction accuracy,\nincreased data efficiency, and reduced training time. This paper\npresents a multi-purpose algorithm for simultaneous face recog-\nnition, facial expression recognition, age estimation, and face\nattribute estimation (40 attributes including gender) based on\na single Swin Transformer. Our design, the SwinFace, consists\nof a single shared backbone together with a subnet for each\nset of related tasks. To address the conflicts among multiple\ntasks and meet the different demands of tasks, a Multi-Level\nChannel Attention (MLCA) module is integrated into each task-\nspecific analysis subnet, which can adaptively select the features\nfrom optimal levels and channels to perform the desired tasks.\nExtensive experiments show that the proposed model has a better\nunderstanding of the face and achieves excellent performance\nfor all tasks. Especially, it achieves 90.97% accuracy on RAF-\nDB and 0.22 ϵ-error on CLAP2015, which are state-of-the-\nart results on facial expression recognition and age estimation\nrespectively. The code and models will be made publicly available\nat https://github.com/lxq1000/SwinFace.\nIndex Terms—Multi-task Learning, Swin Transformer, Face\nRecognition, Facial Expression Recognition, Age Estimation, Face\nAttribute Estimation.\nI. I NTRODUCTION\nF\nACE recognition and analysis are important topics in\nthe field of computer vision, and have a wide range\nof applications in security monitoring, digital entertainment,\nemotion recognition, etc. Recently, researchers have intro-\nduced vision transformers into face recognition and analysis\nand have achieved performance breakthroughs on some tasks.\nFor example, An et al. [1] proved that ViT [2]-based networks\ncan obtain better performance than ResNet [3]-based networks\non face recognition; TransFER [4] explored transformers for\nfacial expression recognition and achieved significant perfor-\nmance improvement.\nHowever, these transformer-based models are typically de-\nsigned to achieve only one particular task, which suffers from\nLixiong Qin, Mei Wang, Jiani Hu, Weihong Deng are with the\nSchool of Artificial Intelligence, Beijing University of Posts and Telecom-\nmunications, Beijing 100876, China (e-mail: lxqin@bupt.edu.cn; wang-\nmei1@bupt.edu.cn; jnhu@bupt.edu.cn; whdeng@bupt.edu.cn). Chao Deng,\nKe Wang, Xi Chen are with China Mobile Research Institute, Beijing,\nChina (e-mail: dengchao@chinamobile.com; wangkeai@chinamobile.com;\nchenxiyjy@chinamobile.com).\nIdentity\nExpression\nAge\n\n\n\nExpression\nAge\n\nMLCA\nMLCA\nBackbone\nSubnet\n(a) Previous methods\n(b) Our SwinFace\nIdentity\n...\n......\nFig. 1. Overview of the previous methods for face recognition and analysis\ncontrasted with our methodology. With shared parameters and the proposed\nMLCA module, our model can achieve increased application efficiency and\nimproved prediction accuracy.\nthe following limitations. 1) Data sparsity. Compared with\nface recognition, face analysis tasks such as facial expression\nrecognition, age estimation, and facial attribute classification\nare still challenging due to the lack of large available train-\ning data, as shown in Table I. When laser-focused on a\nsingle task, big data in face recognition cannot benefit the\ntraining of face analysis tasks through knowledge sharing\namong tasks. 2) Model efficiency. Learning separated networks\nfor different tasks would result in inefficiency in terms of\nmemory and inference speed. Since transformers learn general\nfeatures common to different tasks, they can be shared by face\nrecognition and analysis tasks. Although multi-task learning is\nproposed in convolution neural networks (CNNs) to address\nthese issues, it is still an understudied field of research in\ntransformers.\nIn this paper, we train a transformer jointly in a multi-task\nlearning framework that simultaneously solves the tasks of\nface recognition, facial expression recognition, age estimation,\nand face attribute estimation (40 attributes including gender).\nOur design, the SwinFace, consists of a single shared Swin\nTransformer backbone together with a face recognition subnet\nCopyright ©20xx IEEE. Personal use of this material is permitted. However, permission to use this material for any other purposes must\nbe obtained from the IEEE by sending an email to pubs-permissions@ieee.org.\narXiv:2308.11509v1  [cs.CV]  22 Aug 2023\n2\nTABLE I\nCOMPARISON OVER DATASETS FOR FACE RECOGNITION AND FACE\nANALYSIS .\nTask Dataset Images\nRecognition Webface42M [1] 42M\nRecognition MillionCelebs [5] 18.8M\nRecognition Glint360K [6] 17M\nRecognition MS-Celeb-1M [7] 5.8M\nExpression AffectNet [8] 420K\nExpression RAF-DB [9] 15,339\nAge IMDB+WIKI [10] 523K\nAge MORPH [11] 55K\nAge CLAP2015 [12] 4699\nAttribute CelebA [13] 200K\nand 11 face analysis subnets. As shown in Table II, to reduce\ncomputation, we group some related face analysis tasks and\nprocess them by one subnet instead of designing a subnet\nfor each task. By sharing representations and leveraging the\nsynergy between related tasks, the knowledge contained in a\ntask can be utilized by other tasks, with the hope of improving\nthe performance of all the tasks at hand and reducing the\ntraining time. Fig. 1 compares the previous methods to our\nmethod.\nIn addition, multi-task learning is inherently a multi-\nobjective problem and different tasks may conflict. For ex-\nample, the face recognition task learns to extract expression-\ninvariant identity representations whereas the facial expression\nrecognition task is subject-independent, that is, the classifica-\ntion aims to focus on expression but ignores face identity. If\nthese two task-specific subnets are simultaneously branched\nout from the top layer, it is obvious that they will mutually\nlimit performance improvement due to the conflicting tar-\ngets. To avoid this conflict, a Multi-Level Channel Attention\n(MLCA) module is proposed and integrated into each face\nanalysis subnet, which consists of Multi-Level Feature Fusion\n(MLFF) and Channel Attention (CA). First, MLFF combines\nfeatures at different levels in an efficient way, which enables\nour SwinFace to rely on both local and global information\nof the face to perform face analysis. Second, considering that\ndifferent tasks have different preferences for local and global\ninformation, the features at different levels should not be\ntreated equally. Therefore, we utilize CA to adaptively assign\nweights for the features from different levels such that different\ntasks are allowed to make their own choices for features,\nwhich also solves the problem of conflict among tasks to\nsome extent. Different from other multi-task learning methods\nin CNNs, e.g., Hyperface [14] and AIO [15], which fuse\nthe features from the empirically-selected layers for training\nthe task-specific subnets, our SwinFace adaptively finds the\nfeatures from appropriate levels, achieving better performance.\nThis paper makes the following contributions.\n1) This is the first multi-task model that simultaneously\nsolves a diverse set of face analysis (42 tasks) and\nrecognition tasks using a single transformer.\n2) We propose the Multi-Level Channel Attention (MLCA)\nmodule to handle feature extraction conflict of backbone\nand feature selection of subnets.\n3) The proposed model achieves 90.97% accuracy on RAF-\nDB, 0.20 and 0.22 ϵ-error on the validation and test\nsets of CLAP2015, which are SOTA results on facial\nexpression recognition and age estimation respectively.\nII. R ELATED WORK\nIn this section, related works about face recognition, facial\nexpression recognition, age estimation, face attribute estima-\ntion, and multi-task learning are reviewed briefly.\nA. Face Recognition\nFace Recognition (FR) is a vital task in computer vision.\nIn recent years, the development of FR can be summarized\nin three aspects: loss function, data, and framework. In terms\nof loss function, various margin penalties such as SphereFace\n[16], CosFace [17], and ArcFace [18] are proposed. Develop-\ning ResNet [3] with IR, ArcFace takes the lead in obtaining a\nsaturation accuracy of 99.83% on LFW [19]. In terms of data,\nlarge-scale training sets [1], [5]–[7] have been developed. With\nthe expansion of the dataset, the memory and computing costs\nlinearly scale up to the number of identities in the training set,\nwhich calls for a new training framework. A sparsely updating\nvariant of the FC layer, named Partial FC (PFC) [6] is invented\nto save overhead. To sum up, learning discriminative deep\nfeature embeddings by using million-scale in-the-wild datasets\nand margin-based softmax loss is the current state-of-the-art\napproach for FR.\nSome researchers have also explored transformer-based FR.\nZhong et al. [20] demonstrate that face transformer models\ntrained on MS-Celeb-1M [7] can achieve comparable perfor-\nmance as CNNs with a similar number of parameters. An et\nal. [1] prove that the ViT-based method will be more advan-\ntageous on a larger dataset such as WebFace42M. Existing\nworks rarely explore the facilitation of FR tasks for down-\nstream analysis tasks. The improvement in descriptive ability\nbrought by large-scale datasets cannot benefit the prediction\nfor expression and age.\nB. Facial Expression Recognition\nFacial Expression Recognition (FER) is still a challenging\ntask mainly due to two reasons: 1) Large inter-class similari-\nties. Even for the same face identity, a slight change to a small\nregion of the face can determine the expression. 2) Small intra-\nclass similarities. Samples belonging to the same class may\nexist large differences in visual appearances, such as skin tone,\ngender, and age. Due to such characteristics, although FER\ngenerally uses FR to provide initialization for the networks [4],\n[21]–[23], there is almost no research on the synergy between\nFER and FR during training. In existing methods, DLP-CNN\n[9] and DACL [23] enhance the discriminative ability of ex-\npression features by introducing new loss functions. KTN [24]\nuses adaptive loss re-weight category importance coefficients,\nalleviating the imbalanced class distribution. IPA2LT [25] and\nSCN [22] address label uncertainties in FER. gACNN [26],\nRAN [21] use attention mechanism to adaptively capture the\n3\nimportance of facial regions for occlusion and pose variant\nFER. Zhang et al. [27] propose an end-to-end deep model for\nsimultaneous facial expression recognition and facial image\nsynthesis, aiming to address the limitation of insufficient train-\ning data in improving performance in the field of FER. Our\nmethod adopts a multi-task learning framework, integrating a\nmore diverse set of tasks to enhance the model’s understanding\nof face, thereby achieving improved performance in facial\nexpression recognition. AMP-Net [28] can adaptively capture\nthe diversity and key information from global, local, and\nsalient facial regions. While both approaches employ adaptive\nfeature extraction methods, our motivation lies in mitigating\nthe negative impact of target conflicts between tasks, whereas\nthe motivation behind AMP-Net is to improve the robustness\nof FER in real-world scenarios. TransFER [4] first introduces\nthe vision transformer into FER. This method uses ResNet\n[3] as stem, reshapes the feature map from stem into a set\nof tokens, and uses a transformer to model the relationship\nbetween these tokens. Adding a deep ViT to the ResNet stem,\nthe number of parameters of the model reaches 65.2M, which\nmakes it less economical.\nC. Age Estimation\nAge Estimation (AE) is an important and very challenging\nproblem in computer vision. The existing age estimators can\nbe mainly divided into four categories. 1) Classification: DEX\n[29] treats age estimation as a classification problem with\n101 classes. AL [30] introduces LSTM units to extract local\nfeatures of age-sensitive regions, improving the age estimation\naccuracy. Our model enables the subnet to adaptively select\nfeatures from different levels, effectively leveraging the local\nfeatures from the lower level of the backbone. 2) Regres-\nsion: BridgeNet [31] uses local regressors with overlapping\nsubspaces and gating networks with the proposed bridge-\ntree structure to efficiently mine the continuous relationship\nbetween age labels. 3)Ranking: OR-CNN [32] and Ranking-\nCNN [33] treat ages as ranks. To estimate a person’s age,\nthey dichotomize whether the person is older than each age\nor not. The final estimate is obtained by combining a series\nof binary classification results. 4) Distribution learning: Age\ntags are not precise tags and have some ambiguity. MV\n[34] and A VDL [35] robust age estimation using distribution\nlearning. In addition, DRF [36] and DLDLF [37] recognize\nthat age estimation is a nonlinear regression problem. Both\nof them connect split nodes to the top layer of convolutional\nneural networks (CNNs) and deal with inhomogeneous data\nby jointly learning input-dependent data partitions at the split\nnodes and age distributions at the leaf nodes. Current age\nestimation methods have under-emphasized the importance of\nface recognition initialization. These methods [29], [31], [34],\n[35], [38], [39] generally use the large-scale age estimation\ndataset IMDB-WIKI [10] for pre-training, which restricts\nperformance improvement. Our experiments prove that large-\nscale face initialization can significantly improve the accuracy\nof age prediction. Although our model did not incorporate\ndedicated structures like DRF [36] and DLDLF [37] to explic-\nitly handle the issue of age inhomogeneity, it still exhibited\nimpressive performance. This outcome further underscores the\nsignificance of face recognition initialization and multi-task\ntraining frameworks in the context of age estimation.\nD. Face Attribute Estimation\nFace attributes give intuitive descriptions of human-\ncomprehensible facial features, such as smile, gender, glasses,\nbeard, etc. Face Attribute Estimation (FAE) is usually a binary\njudgment of whether a face has a certain attribute. Liu et al.\n[13] released the CelebA dataset, containing about 200K near-\nfrontal images with 40 attributes, accelerating research in this\narea [13], [40], [41]. In recent years, some new methods have\nbeen continuously proposed to improve the performance of\nface attribute estimation. MCNN-AUX [42] takes advantage\nof attribute relationships by dividing 40 attributes into nine\ngroups. MCFA [43] and DMM-CNN [44] exploit the inherent\ndependencies between face attribute estimation and auxiliary\ntasks, such as facial landmark localization, improving the\nperformance of face attribute estimation by taking advantage\nof multi-task learning. Our method also introduces grouping\nfor the purpose of improving performance and reducing com-\nputation.\nE. Multi-task Learning\nMulti-task learning (MTL) is first analyzed in detail by\nCaruana [45]. In recent years, multi-task learning has been\nwidely applied in computer vision tasks, such as image search\n[46], object detection [47], face recognition [48], [49], and\nfacial analysis [27]. In general, multi-task learning is motivated\nby the following aspects. 1) Increased efficiency. Multiple\ntasks can be accomplished simultaneously. I-Net [46] jointly\nperforms person re-identification and person search without\nthe need for first detecting and cropping person regions for\nfeature matching. BoostGAN [48] and TSGAN [49] recover\nfaces from occluded but profile inputs, simultaneously elimi-\nnating the impact of pose variation and occlusion, which are\ntwo key factors affecting the accuracy of face recognition.\nIn our method, by combining downstream face analysis tasks\ninto a single model, they can collectively benefit from the\nadvantages of face recognition initialization without the need\nfor multiple sets of backbone network parameters, thereby\nimproving data efficiency and reducing training time. 2) Im-\nproved performance. Zhang et al. [27] perform expression\nsynthesis and representation jointly. Both tasks can boost\ntheir performance for each other via the unified model. In\nour method, a multi-task learning framework can effectively\nexplore synergy among tasks, improving the performance of\nboth face recognition and downstream face analysis tasks.\n3) Learning paradigm. Object detection [47] tasks inherently\ninvolve multi-task learning, requiring the joint optimization of\nobject classification and bounding box regression. In our work,\nwe do not have the motivation for this aspect.\nOur work is primarily inspired by HyperFace and AIO,\nwhere the multi-task learning framework can fully explore the\nrelationship between different tasks and enhance the discrimi-\nnative ability of the shared backbone. HyperFace [14] trained\nan MTL network for face detection, landmarks localization,\n4\nAvgpool\nLayer Norm\nFC\nBatch Norm\nFC\nBatch Norm\nFace Recognition Subnet\nPartial FC\n512\nMaxPool\nReLU\nFC ×2\nCA\nMLFF\nFace Analysis Subnet ×11\n512\nMLCA\n7×7×512\nFC FC FC...\n112×112×3\nShared Backbone (Swin Transformer)\nPatch Partition\nLinear Embedding\nSwin Transformer \nBlock  ×2\nPatch Merging\nSwin Transformer \nBlock  ×2\nPatch Merging\nSwin Transformer \nBlock  ×6\nPatch Merging\nSwin Transformer \nBlock  ×2\n56×56×48\n56×56×96\nFM4\nFM1 FM2 FM3\n28×28×192\n14×14×384    7×7×768    7×7×768\nFig. 2. Overall architecture for the proposed method.\npose, and gender estimation by fusing the intermediate layers\nof CNN for improved feature extraction. AIO [15] further ex-\npands the function of Hyperface, adds smile detection and age\nestimation, and demonstrates that analysis tasks benefit from\ndomain-based regularization and network initialization from\nface recognition task. AIO has noticed that different tasks have\ndifferent preferences for features from various levels. In that\nmethod, analysis tasks are divided into two categories: subject-\nindependent and subject-dependent. AIO believes that subject-\nindependent tasks rely more on local information available\nfrom the lower layers of the network, while subject-dependent\nones are the opposite. Under this consideration, the first, third,\nand fifth convolutional layers are fused for training the subject-\nindependent tasks while subject-dependent tasks are branched\nout from the sixth convolutional layer. Unlike HyperFace and\nAIO, which fuse features from empirically-selected layers\nfor training the task-specific subnets, our method adaptively\nselects features from the appropriate levels, achieving better\nperformance.\nOur ultimate goal is to train a generic model capable of\nhandling all face-related tasks. In addition to the tasks already\nperformed in SwinFace, we also aim to include localization\ntasks such as pose estimation [50], [51], alignment [52],\nparsing [53], and 3D reconstruction [54], [55]. For tasks that\nlack sufficient labeled data, we will consider leveraging semi-\nsupervised learning [56] to enhance the model’s performance.\nIII. M ETHOD\nWe proposed a multi-purpose transformer-based model for\nsimultaneous face recognition, facial expression recognition,\nage estimation, and face attribute estimation. With a well-\ndesigned structure, the model leverages synergy and alleviates\ntarget conflict among tasks, resulting in excellent perfor-\nmances. In this section, we will provide the details of network\nstructure design and training procedure.\nA. Overall Architecture\nAn overview of the SwinFace architecture is presented in\nFig. 2. In this paper, we adopt a single Swin Transformer [57]\nto extract shared feature maps at different levels. Based on\nshared feature maps, we further perform multi-task learning\nwith a face recognition subnet and 11 face analysis subnets.\n1) Shared Backbone: The shared Swin Transformer back-\nbone can produce a hierarchical representation. The cropped\n112 × 112 × 3 face image is first split into non-overlapping\npatches by a patch partition module. In our implementation,\nwe use a patch size of 2 × 2 and thus the number of tokens\nfor the subsequent module is 56 ×56 with a dimension of 48.\nA linear embedding layer is applied to this raw-valued feature\nto project it to 96 dimensions. After that, the patch merging\nlayers and Swin Transformer blocks are utilized alternately.\nThe patch merging layers can reduce the number of tokens\nby a multiple of 2 × 2 = 4 , and double the dimension\nof the tokens. The Swin Transformer blocks are applied for\nfeature transformation, with the resolution unchanged. Four\nfeature maps from different levels are finally output for face\nrecognition or analysis tasks. The scales of these feature maps\nare 28×28×192, 14×14×384, 7×7×768, and 7×7×768,\nrespectively, and are denoted as FM1 to FM4.\n2) Face Recognition Subnet: Face recognition requires ro-\nbust representations that are not affected by local variations.\nTherefore, we only provide the feature map extracted from the\ntop layer, namely FM4, to the face recognition subnet. Similar\nto ArcFace [18], we introduced the structure that includes BN\n[58] to get the final 512- D embedding feature. Experiments in\nSection IV-D demonstrate that the recognition subnet with FC-\nBN-FC-BN structure can outperform the counterpart without\nthis structure.\n3) Face Analysis Subnets: The proposed model is able to\nperform 42 analysis tasks, which are divided into 11 groups\naccording to the relevance of the tasks, as shown in Table II.\nTasks in the same group share a face analysis subnet to\nreduce computation. Each subnet consists of a Multi-Level\nChannel Attention (MLCA) module, a max pooling layer, a\nReLU activation layer, two consecutive fully connected layers,\nand a series of fully connected layers for output. MLCA is\nthe critical structure that enables subnets to make their own\nchoices for features solving the problem of conflict among\ntasks to some extent.\nB. Multi-Level Channel Attention\nConventional face recognition and analysis methods branch\nout task-specific subnets only from the top layer of the\nbackbone. Tasks with conflicting targets will therefore mu-\ntually limit performance improvement. To solve this issue,\nwe propose Multi-Level Channel Attention (MLCA) module\nand integrate it into each face analysis subnet. The MLCA\nmodule consists of a Multi-Level Feature Fusion (MLFF)\nmodule and a Channel Attention (CA) module. MLFF is used\n5\nTABLE II\nTASK ASSIGNMENT FOR FACE ANALYSIS SUBNETS . THE OUTPUT SCALES\nOF FER, AE, AND FAE ARE 7, 1, AND 2, RESPECTIVELY .\nSubnet Tasks Number\nof Tasks\nExpression Expression(7), Smiling 2\nAge Age(1), Young 2\nGender Male 1\nWhole Attractive, Blurry, Chubby, Heavy Makeup, 6Oval Face, Pale Skin\nHair\nBald, Bangs, Black Hair, Blond Hair, Brown Hair,\n10 Gray Hair, Receding Hairline, Straight Hair,\nWavy Hair, Wearing Hat\nEyes Arched Eyebrows, Bags Under Eyes, 5Bushy Eyebrows, Eyeglasses, Narrow Eyes\nNose Big Nose, Pointy Nose 2\nCheek High Cheekbones, Rosy Cheeks, 4Wearing Earrings, Sideburns\nMouth 5 o’clock Shadow, Big Lips, Mouth Slightly Open, 6Mustache, Wearing Lipstick, No Beard\nChin Double Chin, Goatee 2\nNeck Wearing Necklace, Wearing Necktie 2\nTotal 42\nto combine feature maps at different levels enabling the task-\nspecific subnet to rely on both local and global information\nof the faces and CA emphasizes the contributions of different\nlevels for the specific group of tasks. It is important to note\nthat previous methods primarily utilized feature fusion and\nadaptation to enhance the robustness of features in single-\ntask scenarios. For instance, in the AMP-Net [28] for FER,\nthe Gate-OSA, a feature fusion module, is employed in the\nGP module to learn facial features from diverse receptive\nfields. The LP and AP modules leverage CBAM [59], a feature\nadaptation module, to further enhance the extracted features. In\ncontrast, the primary motivation behind the proposed MLCA\nis to alleviate target conflicts among tasks in the multi-task\nscenario.\n1) Multi-Level Feature Fusion: Swin Transformer has a hi-\nerarchical architecture, which is a stack of multiple transformer\nblocks. Blocks at lower levels capture low-level elements such\nas basic colors and edges, while the ones at higher levels\nencode abstract and semantic cues. It is natural to combine the\nfeatures from different levels for analysis tasks. In doing so,\nthe individual analysis task is enabled to rely on both local and\nglobal information. As shown in Fig. 3, to keep the scale of the\nfeature maps from each level consistent, the FM1 and FM2 are\nfirst down-sampled by average pooling. Then, 4 independent\n3 × 3 convolutions are applied to the input feature maps of\nFM1-4 to proportionally reduce the number of channels. We\nfurther concatenate them in the channel dimension to get a\n512-dimensional feature map. The MLFF module enables the\ntransformer blocks in the backbone to be directed both by\nthe successive blocks as well as the neighboring face analysis\nsubnets during training, which can speed up the convergence of\nthe model and also improves the generalization of the extracted\nfeatures.\n 3×3 Conv  3×3 Conv 3x3 Conv 3x3 Conv\nConcatenation\n Multi-Level \nFeature Fusion Module\n7×7×47 7×7×93 7×7×186 7×7×186\n7×7×512\nFM2 FM3 FM4\n AvgPool  AvgPool\nFM1\nFig. 3. Multi-Level Feature Fusion module.\n＋\n512\n×\nChannel Attention Mudule\nShared \nMLP\n512\n512 Attention\nVector\n7×7×512 7×7×512\n512\nMaxPool\nAvgPool\n512\nFig. 4. Channel Attention module.\n2) Channel Attention: Different analysis tasks have dif-\nferent preferences for local and global information. MLFF\ncombines features at different levels in an efficient way and\nprovides a 512-dimensional concatenated feature map. We\nhope to separately emphasize the contributions of different\nchannels in the feature map with an attention vector, in which\nthe i-th activation value of the attention vector corresponds\nto the i-th channel of the feature map. For this motivation,\nin our Channel Attention (CA) module, we follow CBAM\n[59] to calculate the attention vector, as shown in Fig. 4.\nFirst, the average-pooled and max-pooled features are obtained\nfrom the concatenated feature map as two different spatial\ncontext descriptors. Both descriptors are then forwarded to a\nshared multi-layer perceptron (MLP) with one hidden layer.\nThe output feature vectors of the two descriptors are merged\nusing element-wise summation, and then the attention vector\nis obtained through a sigmoid function. The input concate-\nnated feature map and the attention vector are element-wise\nmultiplied to obtain the channel-weighted feature map as the\noutput of the CA module.\nC. Training\nWe use a multi-task learning framework so that the model\ncan simultaneously solve the tasks of face recognition, fa-\ncial expression recognition, age estimation, and face attribute\nestimation. As shown in Table III, the training sets can be\ndivided into four categories according to the provided labels.\nWe start with a pre-trained model which only includes a\nSwin Transformer backbone and a face recognition subnet.\n6\nTABLE III\nTRAINING DATASETS FOR MULTI -TASK TRAINING PHASE CAN BE DIVIDED\nINTO FOUR CATEGORIES BY LABEL TYPES .\nLabel Dataset\nIdentity MS-Celeb-1M [7]\nExpression RAF-DB [9] AffectNet [8]\nAge Gender IMDB+WIKI [10] Adience [60] MORPH [11]\nAttribute CelebA [13]\nThe Swin Transformer backbone is then shared by the face\nrecognition task and all 42 face analysis tasks. The loss\nfunctions and training datasets are illustrated as follows.\n1) Face Recognition: We train the task of face recognition\non the large-scale face recognition dataset MS-Celeb-1M [7]\nwith CosFace [17]:\nLR = − 1\nNR\nNRX\ni=1\nlog es(cos θyi−m)\nes(cos θyi−m) + Pn\nj=1,j̸=yi es cos θj\n. (1)\nAssume that the weight of the last fully connected layer is\nwritten as W ∈ Rd×n, where n is the number of identities.\nWe use Wj ∈ Rd to denote the j-th column of the weight W\nand xi ∈ Rd to denote the deep feature of the i-th sample,\nbelonging to the yi-th class. θj is the angle between the weight\nWj and the feature xi. The embedding feature ∥xi∥ is fixed\nby l2 normalization and re-scaled to s. m is the CosFace\nmargin penalty. In our implementation, s is set to 64, and\nm is set to 0.4. NR is the number of samples with identity\nlabels in each training batch. In addition, we introduce PFC\n[6] to conserve computing resources. Sampling ratio r is set\nto 0.3. Experiments in Section IV-D demonstrate that CosFace\n[17] is more suitable for our model than other loss functions\nsuch as ArcFace [18].\n2) Facial Expression Recognition: It is a multi-\nclassification problem. Due to the lack of a large-scale\ntraining set, we merge AffectNet [8] and RAF-DB [9] by\nlabels for basic expression analysis. The expressions include\nsurprise, fear, disgust, happiness, sadness, anger, and neutral.\nThe loss function for training is as follows:\nLE = − 1\nNE\nNEX\ni=1\n\" 7X\nc=1\nyic log(pic)\n#\n, (2)\nwhere yic = 1 if the i-th sample belongs to expression class\nc, otherwise 0. The predicted probability that the i-th sample\nbelongs to expression class c is given by pic. NE is the number\nof samples with expression labels in each training batch.\n3) Age Estimation: We formulate the age estimation task\nas a regression problem. IMDB+WIKI [10], Adience [60]\nand MORPH [11] are used for training. we use a linear\ncombination of Gaussian loss and Euclidean loss following\nRanjan et. al. [15]:\nLA = 1\nNA\nNAX\ni=1\n\u0014\n(1 − λ) 1\n2 ( ˆai − ai)2 + λ\n\u0012\n1 − exp(−( ˆai − ai)2\n2σ2 )\n\u0013\u0015\n,\n(3)\nwhere ˆai is the predicted age for sample xi, ai is the ground-\ntruth age and σ is the standard deviation of the annotated age\nvalue. λ is initialized with 0 at the start of the training, and\nincreased to 1 subsequently. σ is fixed at 3 if not provided by\nthe training set. NA is the number of samples with age labels\nin each training batch.\n4) Face Attribute Estimation: Face attribute estimation con-\nsists of 40 binary classification problems and is trained using\nCelebA [13]. Especially, the training of gender recognition\nalso uses labels from IMDB+WIKI [10], Adience [60] and\nMORPH [11]. The loss function for a single FAE task is as\nfollows:\nLAj = 1\nNAj\nNAjX\ni=1\n[−(1 − qi) · log(1 − pi) − qi · log(pi)] ,\n(4)\nwhere qi = 1for the j-th attribute exists and 0 otherwise. pi\nis the predicted probability that the i-th input face contains\nthe j-th attribute. NAj is the number of samples with the j-th\nattribute labels in each training batch.\n5) Total Loss function: The final overall loss L is the\nweighted sum of individual loss functions, given in (5):\nLtotal =\nX\nt∈T\nαtLt, (5)\nwhere T = {R, E, A, A1, A2, ..., A40} represents tasks and αt\nis the weight of task t. The loss-weights are respectively set\nto 1.0 in experience.\nIV. E XPERIMENTS\nA. Datasets\n1) Face Recognition: We use MS-Celeb-1M [7] for pre-\ntraining and multi-task training. MS-Celeb-1M is one of the\nmost popular large-scale training databases for face recogni-\ntion and we use the clean version refined by ArcFace [18],\nwhich contains 5.8M images of 85,742 celebrities. For testing,\nwe report the verification performance of models on several\nmainstream benchmarks including LFW [19], CFP-FP [61],\nAgeDB-30 [62], CALFW [63] , CPLFW [64], and IJB-C [65]\ndatabases. LFW database contains 13,233 face images from\n5,749 different identities, which is a classic benchmark for\nunconstrained face verification. CFP-FP and CPLFW are built\nto emphasize the cross-pose challenge while AgeDB-30 and\nCALFW are built for the cross-age challenge. IJB-C contains\nfaces with extreme viewpoints, resolution, and illumination,\nwhich makes it more challenging.\n2) Facial Expression Recognition: RAF-DB [9] is a real-\nworld expression dataset containing 12,271 training and 3,068\ntest images for basic expression analysis. AffectNet [8] is the\nlargest publicly available FER dataset so far, containing 420K\nimages with manually annotated labels. RAF-DB provides\nmore accurate labels, but with a smaller sample amount, while\nAffectNet is just the opposite. We merge AffectNet and RAF-\nDB by labels for boosting performance. We report the overall\naccuracy on the RAF-DB test set.\n3) Age Estimation: We use IMDB+WIKI [10], Adience\n[60], and MORPH [11] for training. IMDB+WIKI, which\ncontains 523K images in total, is the largest dataset for age\nestimation, where the images are crawled from celebrities\n7\nTABLE IV\nCOMPARISON FOR FACE RECOGNITION MODELS . NUMBER OF BACKBONE PARAMETERS OF FACE RECOGNITION MODELS . THE 1:1 VERIFICATION\nACCURACY ON THE LFW [19], CFP-FP [61], A GEDB-30 [62], CALFW [63] AND CPLFW [64] AND IJB-C [65] DATASETS .\nMethod Params Verification Accuray IJB-C TAR@FAR\n(M) LFW CFP-FP AgeDB-30 CALFW CPLFW 1e-6 1e-5 1e-4 1e-3 1e-2 1e-1\nResNet-50 [3] 43.6 99.69 98.14 97.53 95.87 92.45 81.43 90.98 94.32 96.38 97.82 98.75\nViT [2] 63.2 99.83 96.19 97.82 95.92 92.55 - - 95.96 97.28 98.22 98.99\nV2T-ViT [67] 63.5 99.82 96.59 98.07 95.85 93.00 - - 95.67 97.10 98.14 98.90\nViT-P10S8 [20] 63.5 99.77 96.43 97.83 95.95 92.93 - - 96.06 97.45 98.23 98.96\nViT-P12S12 [20] 63.5 99.80 96.77 98.05 96.18 93.08 - - 96.31 97.49 98.38 99.04\nSwin-T [57] 28.5 99.80 97.91 97.85 95.98 92.60 88.54 93.71 95.75 97.13 98.01 98.86\nSwinFace 28.5 99.87 98.60 98.15 96.10 93.42 90.82 94.93 96.73 97.79 98.43 99.08\non IMDb and Wikipedia websites. Adience contains 26,580\nimages across 2,284 subjects with a label from eight different\nage groups. We take the average age of each group as\nthe regression label. MORPH is the largest database with\nprecise age labeling and ethnicities, including about 55K face\nimages and age ranges from 16 to 77 years. The Chalearn\nLAP challenge [12] is the first competition for apparent age\nestimation, collecting 2476 images for training, 1136 images\nfor validation, and 1079 images for testing. The dataset offers\nthe standard deviation for each age label. After finetuning the\nage subnet, we report age estimation performance on both the\nvalidation and test split.\n4) Face Attribute Estimation: CelebA [13] consists of\n162,770 images for training, 19,867 images for validation, and\n19,962 images for testing. We report the FAE performance\non the testing split. In particular, the training of gender\nrecognition also uses labels from IMDB+WIKI [10], Adience\n[60], and MORPH [11].\nFor data prepossessing, we follow the recent papers [16]–\n[18] to generate the aligned face crops (112 × 112). We\nperform face alignment using affine transform and matrix\nrotation in OpenCV . For facial images without key-point\nlabels, MTCNN [66] is used to collect landmarks.\nB. Implementation Details\nThe Swin Transformer backbone adopts the tiny version\n(Swin-T) which includes 28.5M parameters. The face recogni-\ntion subnet has about 1M parameters excluding PFC and each\nface analysis subnet has about 3.5M parameters. The model\nis trained on 4 NVIDIA Tesla T4 GPUs.\nWe first pre-train the Swin Transformer backbone and face\nrecognition subnet for robust face recognition initialization.\nFor multi-task learning, we load the pre-trained backbone\nand face recognition subnet and randomly initialize 11 face\nanalysis subnets.\n1) Pre-training: We employ an AdamW [68] optimizer for\n40 epochs using a cosine decay learning rate scheduler and\n5 epochs of linear warm-up. A batch size of 512, an initial\nlearning rate of 5×10−4, a warm-up learning rate of 5×10−7,\na minimum learning rate of 5 × 10−6, and a weight decay\nof 0.05 are used. Data augmentation includes horizontal flip\naugmentation only.\n2) Multi-task Learning: The training lasts 80k steps, of\nwhich 8k steps are warm-up steps. We set the number of\nsamples from each of the four categories (shown in Table III)\nTABLE V\nCOMPARISON FOR FACIAL EXPRESSION RECOGNITION ON RAF-DB [9].\nMethod Accuray\nDLP-CNN [9] 80.89\ngACNN [26] 85.07\nIPA2LT [25] 86.77\nRAN [21] 86.90\nCovPool [71] 87.00\nSCN [22] 87.03\nDACL [23] 87.78\nKTN [24] 88.07\nZhang et al. [27] 89.01\nAMP-Net [28] 89.25\nTransFER [4] 90.91\nSwinFace 90.97\nto be 128 at each step. (Note that there are thus 256 samples\nper step for training the task of gender recognition.) For\nface recognition samples, Only a horizontal flip is utilized.\nFor other samples, data augmentation includes horizontal flip,\nRandaugment [69], and Random Erasing [70] to alleviate the\nlack of variations. Other settings are the same as the pre-\ntraining phase.\nC. Performance Evaluation\n1) Face Recognition: Table IV gives the comparison be-\ntween SwinFace and other face recognition models based on\nResNet [3] and Transformers [2], [20], [67]. The performance\nof models based on transformers is quoted from [20]. These\nmodels are trained on MS-Celeb-1M [7], so a fair comparison\ncan be achieved. The results prove that SwinFace outperforms\nother models in almost all test protocols, although the number\nof parameters of its backbone is much smaller than other\nmodels. In particular, The SwinFace outperforms the Swin-T\nmodel on all benchmarks for face recognition which shows that\nmulti-task learning can enhance face recognition capabilities.\n2) Facial Expression Recognition: As shown in Table V,\nwe compare SwinFace with the state-of-the-art methods on\nRAF-DB [9]. Our method outperforms state-of-the-art meth-\nods, resulting in an accuracy of 90.97%. It is noting that\nsome methods such as SCN [27] and KTN [17] achieve\nthe reported performance by applying trivial loss functions,\nwhile our method achieves better performance with the stan-\ndard cross-entropy loss only. Compared to AMP-Net [28],\nour approach does not require complex feature enhancement\n8\nTABLE VI\nCOMPARISON FOR AGE ESTIMATION ON CLAP2015 [12].\nMethod Validatation Test\nMAE ϵ-error MAE ϵ-error\nAIO [15] - 0.29 - -\nAgeNet [72] 3.33 0.29 - 0.26\nDEX [29] 3.25 0.28 - 0.26\nAGEn [38] 3.21 0.28 2.94 0.26\nAL-RoR [30] 3.14 0.27 - 0.25\nBridgeNet [31] 2.98 0.26 2.87 0.26\nMWR [39] 2.95 0.26 2.77 0.25\nSwinFace 2.50 0.20 2.47 0.22\nstructures for each region. We achieve superior results solely\nthrough the use of a simple MLCA module. Furthermore, to\nobtain the reported performance, TransFER [4] experimentally\ndetermines that features should be extracted from the third\nstage of the backbone and includes 65.2M parameters. Allow-\ning the expression subnet to adaptively extract features from\nappropriate levels, our method achieves higher accuracy with\na total of 32M parameters in the Swin Transformer backbone\nand the expression subnet, which proves the great benefits of\nmulti-task learning in improving accuracy and efficiency.\n3) Age Estimation: As shown in Table VI, we evaluate\nage estimation task on CLAP2015 using the metrics of MAE\nand ϵ-error. For each face, CLAP2015 provides the standard\ndeviation of age values by multiple annotators. ϵ-error is\ndefined as 1−exp(−(ˆa−a)2\n2σ2 ), where σ is the standard deviation\nof the sample, ˆa is the predicted age, and a is the ground-\ntruth age. The average ϵ-error over all test images is reported.\nBoth validation and test splits of CLAP2015 are used. For\nevaluation on the validation set, we use the training set to\nfinetune the age subnet. For evaluation on the test set, we use\nthe validation set, as well as the training set, to finetune the\nage subnet, as in [31], [38], [39], [72]. For both splits, the\nfinetuning lasts 4k steps, without warm-up steps. A minimum\nlearning rate of 5 ×10−7 is used. Data augmentation includes\nhorizontal flip, Randaugment [69], and Random Erasing [70]to\nalleviate the lack of variations. Other settings are the same as\nthe Swin Transformer’s pre-training phase. Conventional algo-\nrithms [31], [39] introduce complex mechanisms to improve\nperformance. However, only applying a simple subnet with\nMLCA, SwinFace outperforms all conventional algorithms.\nSignificant MAE margin of 0.45 (0.30) and ϵ-error margin\nof 0.06 (0.03) are achieved on the validation (test) split.\n4) Face Attribute Estimation: As shown in Table VII, We\nevaluate the face attribute estimation performance on CelebA\n[13]. CelebA contains some attributes of hair and neck. Since\nwe do not want to lose these parts of information, the images\ncannot be aligned in the way of face recognition as shown in\nFig. 5(a). The model can still achieve an average accuracy of\n91.32% comparable to other state-of-the-art methods, proving\nthat the model has excellent generalization ability for faces of\ndifferent scales.\n(a) (b)\nFig. 5. (a) Face recognition alignment, which will lose part of the hair\nand neck information, and is used for datasets other than CelebA [13]. (b)\nAlignment adopted for CelebA [13].\nExpression Age Gender Whole\n \n0\n1\n2\n3 \nFM1\nFM2\nFM3\nFM4\nMean Importance Score\nSubnets\nFig. 6. Importance of feature maps from different levels for expression, age,\ngender, and whole face attribute subnets.\nD. Ablation Study\n1) Multi-task Framework: Table VIII compares the per-\nformance of single-task and multi-task training for analysis\ntasks. The Swin Transformer backbone is first pre-trained on\nMS-Celeb-1M [7]. For simplicity, when evaluating on the\nCLAP2015 validation set, we do not perform fine-tuning on\nthe CLAP2015 training set. The experimental results demon-\nstrate the effectiveness of the multi-task learning framework.\nCompared with single-task learning, multi-task learning sig-\nnificantly provides superior results of age estimation with\na ϵ-error margin of 0.039. Sharing parameters of the sub-\nnet with facial expression recognition (age estimation) task,\nthe attribute classification task for “Smiling” (“Young”) also\nachieves an increased accuracy by 0.78 (0.67). We believe\nthat the multi-task learning framework can effectively explore\ninter-task synergy and learn the correlation among data from\ndifferent distributions.\n2) Model Initialization from Face Recognition Task:\nImageNet-1K and MS-Celeb-1M are among the most pop-\nular datasets for general recognition and face recognition\nrespectively. We report the final performances of models pre-\ntrained on ImageNet-1K and MS-Celeb-1M for facial expres-\nsion recognition and age estimation in Table IX. The pre-\ntrained model is finetuned on RAF-DB [9] and AffectNet [8]\n(IMDB+WIKI [10], Adience [60] and MORPH [11]) for facial\nexpression recognition (age estimation). For simplicity, when\nevaluating on the CLAP2015 validation set, we do not further\nperform fine-tuning on the CLAP2015 training set. Results\nshow that the model initialization from face recognition can\nsignificantly improve the performance of analysis tasks lacking\nlarge-scale clean labels. The ϵ-error on the CLAP2015 [12]\nvaluation set decreases by 0.025 and the accuracy on RAF-\nDB [9] increases by 4.6.\n3) Multi-Level Channel Attention: Table X shows the re-\nsults of three networks. In the baseline network, analysis\nsubnets only use the feature map from the top layer of the\n9\nTABLE VII\nCOMPARISON FOR FACE ATTRIBUTE ESTIMATION ON CELEB A [13].\n5 o’clock Shadow\nArched Eyebrows\nAttractive\nBags Under Eyes\nBald\nBangs\nBig Lips\nBig Nose\nBlack Hair\nBlond Hair\nBlurry\nBrown Hair\nBushy Eyebrows\nChubby\nPANDA-1 [40] 88.00 78.00 81.00 79.00 96.00 92.00 67.00 75.00 85.00 93.00 86.00 77.00 86.00 86.00\nLNets+ANet [13] 91.00 79.00 81.00 79.00 98.00 95.00 68.00 78.00 88.00 95.00 84.00 80.00 90.00 91.00\nMOON [41] 94.03 82.26 81.67 84.92 98.77 95.80 71.48 84.00 89.40 95.86 95.67 89.38 92.62 95.44\nNSA [73] 93.13 82.56 82.76 84.86 98.03 95.71 69.28 83.81 89.03 95.76 95.96 88.25 92.66 94.94\nMCNN-AUX [42] 94.51 83.42 83.06 84.92 98.90 96.05 71.47 84.53 89.78 96.01 96.17 89.15 92.84 95.67\nMCFA [43] 94.00 83.00 83.00 85.00 99.00 96.00 72.00 84.00 89.00 96.00 96.00 88.00 92.00 96.00\nDMM-CNN [44] 94.84 84.57 83.37 85.81 99.03 96.22 72.93 84.78 90.50 96.13 96.40 89.46 93.01 95.86\nSwinFace 94.60 83.91 82.61 84.24 98.99 96.09 71.26 83.98 90.17 95.94 96.04 89.11 92.62 95.69\nDouble Chin\nEyeglasses\nGoatee\nGrayHair\nHeavy Makeup\nHigh Cheekbones\nMale\nMouth Slightly Open\nMustache\nNarrow Eyes\nNo Beard\nOval Face\nPale Skin\nPointy Nose\nPANDA-1 [40] 88.00 98.00 93.00 94.00 90.00 86.00 97.00 93.00 93.00 84.00 93.00 65.00 91.00 71.00\nLNets+ANet [13] 92.00 99.00 95.00 97.00 90.00 88.00 98.00 92.00 95.00 81.00 95.00 66.00 91.00 72.00\nMOON [41] 96.32 99.47 97.04 98.10 90.99 87.01 98.10 93.54 96.82 86.52 95.58 75.73 97.00 76.46\nNSA [73] 95.80 99.51 96.68 97.45 91.59 87.61 97.95 93.78 95.86 86.88 96.17 74.93 97.00 76.47\nMCNN-AUX [42] 96.32 99.63 97.24 98.20 91.55 87.58 98.17 93.74 96.88 87.23 96.05 75.84 97.05 77.47\nMCFA [43] 96.00 100.00 97.00 98.00 92.00 87.00 98.00 93.00 97.00 87.00 96.00 75.00 97.00 77.00\nDMM-CNN [44] 96.39 99.69 97.63 98.27 91.85 87.73 98.29 94.16 97.03 87.73 96.41 75.89 97.00 77.19\nSwinFace 96.09 99.67 97.21 98.27 91.41 87.24 98.96 93.78 96.91 87.30 96.14 74.72 96.85 77.08\nReceding Hairline\nRosy Cheeks\nSideburns\nSmiling\nStraight Hair\nWavy Hair\nWearing Earrings\nWearing Hat\nWearing Lipstick\nWearing Necklace\nWearing Necktie\nYoung\nAverage\nPANDA-1 [40] 85.00 87.00 93.00 92.00 69.00 77.00 78.00 96.00 93.00 67.00 91.00 84.00 85.43\nLNets+ANet [13] 89.00 90.00 96.00 92.00 73.00 80.00 82.00 99.00 93.00 71.00 93.00 87.00 87.33\nMOON [41] 93.56 94.82 97.59 92.60 82.26 82.47 89.60 98.95 93.93 87.04 96.63 88.08 90.94\nNSA [73] 92.25 94.79 97.17 92.70 80.41 81.70 89.44 98.74 93.21 85.61 96.05 88.01 90.61\nMCNN-AUX [42] 93.81 95.16 97.85 92.73 83.58 83.91 90.43 99.05 94.11 86.63 96.51 88.48 91.29\nMCFA [43] 94.00 95.00 98.00 93.00 85.00 85.00 90.00 99.00 94.00 88.00 97.00 88.00 91.23\nDMM-CNN [44] 94.12 95.32 97.91 93.22 84.72 86.01 90.78 99.12 94.49 88.03 97.15 88.98 91.70\nSwinFace 93.92 94.96 97.75 93.18 84.73 85.57 89.87 99.19 94.07 86.72 96.97 89.05 91.32\nTABLE VIII\nCOMPARISON FOR MULTI -TASK AND SINGLE -TASK LEARNING .\nSetting\nAge Smiling Young\nϵ-error on Acc. on Acc. on\nCLAP2015 [12] val CelebA [13] CelebA [13]\nSingle-task 0.357 92.40 88.38\nMulti-task 0.318 93.18 89.05\nTABLE IX\nCOMPARISON FOR DIFFERENT INITIALIZATION .\nInitialization Expression Acc. on Age ϵ-error on\nRAF-DB [9] CLAP2015 [12] val\nGeneral recognition 86.54 0.382\nFace recognition 91.13 0.357\nTABLE X\nCOMPARISON FOR MULTI -LEVEL CHANNEL ATTENTION MODULE .\nSetting\nExpression Age Attribute\nAcc. on ϵ-error on Mean Acc. on\nRAF-DB [9] CLAP2015 [12] val CelebA [13]\nBaseline 89.50 0.332 91.20\nMLFF 90.51 0.336 91.38\nMLFF + CA 90.97 0.318 91.32\nbackbone. It can be found that using both MLFF and CA\ncan improve the performance of age estimation, face attribute\nestimation, and facial expression recognition. This shows that\nMLCA can effectively alleviate the feature extraction conflict\nof the backbone while adaptively selecting robust feature rep-\nresentations for subnets. Among them, FER benefits most from\nthe MLCA mechanism, with a particularly obvious accuracy\n10\nTABLE XI\nCOMPARISON FOR FACE RECOGNITION USING DIFFERENT LOSS\nFUNCTIONS AND SUBNET DESIGNS .\nLoss FC-BN-FC-BN LFW CFP-FP AgeDB-30\nArcFace without 95.68 88.44 78.85\nCosFace without 98.80 90.66 90.13\nCosFace with 98.97 91.66 90.23\nincrease of 1.47% on RAF-DB [9]. This makes sense, as the\ntarget conflict between FER and FR is the most serious.\n4) Importance of Feature Maps for Different Subnets: We\nwant to know which level of the feature maps the subnets\nprefer. In a face analysis subnet, the channel-weighted feature\nmap is passed through a max pooling layer and a ReLU acti-\nvation layer to obtain a 512-dimensional vector. The activation\nvalues in the 512-dimensional vector can represent the impor-\ntance scores of the corresponding channels in the inference\nphase. As shown in Fig. 6, we average the importance scores\nof channels from FM1, FM2, FM3, and FM4. In general,\na feature map from deeper layers gets a higher importance\nscore. It is worth noting that the feature map from the top\nlayer is not always the most useful. The whole face attribute\nsubnet handles six estimation tasks of “Attractive”, “Blurry”,\n“Chubby”, “Heavy Makeup”, “Oval Face”, and “Pale Skin”,\nas shown in Table II. For this subnet, the third feature map is\nmore beneficial.\n5) Loss Function Selection and Subnet Design for Face\nRecognition: To determine the loss function and subnet de-\nsign details for face recognition, we conduct experiments on\nCASIA-WebFace [74] which contains 494K images of 10,572\ncelebrities. Table XI reports the performance with different\nloss functions and subnet designs. For Swin Transformer-based\nface recognition, CosFace [17] loss function can provide better\nperformance than ArcFace, which is commonly used for CNN-\nbased models. The introduced FC-BN-FC-BN structure also\ncontributes to performance improvement.\n6) Model Running Efficiency: We evaluate the running\nefficiency of the proposed SwinFace on one NVIDIA Tesla\nT4 GPU and the batch size is set to 32. It takes an average\nof 3.205ms to calculate the recognition feature using the Swin\nTransformer backbone and face recognition subnet, while it\ntakes an average of 4.357ms to gain all 43 outputs, which\nincreases the time overhead by 36%. The result shows that\nour method can significantly improve the overall application\nefficiency of face recognition and analysis.\nV. C ONCLUSION\nThis paper proposes a multi-task face recognition and\nanalysis model based on Swin Transformer [57], which can\nperform face recognition, facial expression recognition, age\nestimation, and face attribute estimation simultaneously. Ex-\ntensive experiments show that our method can lead to a\nbetter understanding of faces. The proposed MLCA module\nallows analysis subnets to acquire features from different\nlevels of the backbone and adaptively find the features from\nappropriate levels, improving performance. In addition, our\nwork emphasizes the importance of robust initialization from\nface recognition for facial expression recognition and age\nestimation and achieves SOTA on both tasks. The current\nmodel still has limitations in terms of its functionality. In the\nfuture, we plan to extend our model for face localization tasks,\nsuch as head pose estimation, face alignment, face parsing, and\n3D face reconstruction. Besides, although not validated in our\nwork, an iterative pseudo-labeling process for semi-supervised\nlearning could potentially further enhance model performance\nin tasks with limited labeled data, such as age estimation and\nfacial expression recognition.\nACKNOWLEDGMENTS\nThis work was funded by the Beijing University of Posts and\nTelecommunications-China Mobile Research Institute Joint\nInnovation Center, National Natural Science Foundation of\nChina under Grant No. 62236003, and China Postdoctoral\nScience Foundation under Grant 2022M720517. This work\nwas also supported by Program for Youth Innovative Research\nTeam of BUPT No. 2023QNTD02.\nREFERENCES\n[1] Z. Zhu, G. Huang, J. Deng, Y . Ye, J. Huang, X. Chen, J. Zhu, T. Yang,\nJ. Lu, D. Du et al., “Webface260m: A benchmark unveiling the power of\nmillion-scale deep face recognition,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2021, pp.\n10 492–10 502.\n[2] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al. ,\n“An image is worth 16x16 words: Transformers for image recognition at\nscale,” in International Conference on Learning Representations .\n[3] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image\nrecognition,” in Proceedings of the IEEE conference on computer vision\nand pattern recognition , 2016, pp. 770–778.\n[4] F. Xue, Q. Wang, and G. Guo, “Transfer: Learning relation-aware\nfacial expression representations with transformers,” in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision , 2021, pp.\n3601–3610.\n[5] Y . Zhang, W. Deng, M. Wang, J. Hu, X. Li, D. Zhao, and D. Wen,\n“Global-local gcn: Large-scale label noise cleansing for face recognition,”\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2020, pp. 7731–7740.\n[6] X. An, X. Zhu, Y . Gao, Y . Xiao, Y . Zhao, Z. Feng, L. Wu, B. Qin,\nM. Zhang, D. Zhang et al. , “Partial fc: Training 10 million identities\non a single machine,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2021, pp. 1445–1449.\n[7] Y . Guo, L. Zhang, Y . Hu, X. He, and J. Gao, “Ms-celeb-1m: A dataset and\nbenchmark for large-scale face recognition,” in Computer Vision–ECCV\n2016: 14th European Conference, Amsterdam, The Netherlands, October\n11-14, 2016, Proceedings, Part III 14 . Springer, 2016, pp. 87–102.\n[8] A. Mollahosseini, B. Hasani, and M. H. Mahoor, “Affectnet: A database\nfor facial expression, valence, and arousal computing in the wild,” IEEE\nTransactions on Affective Computing , vol. 10, no. 1, pp. 18–31, 2017.\n[9] S. Li, W. Deng, and J. Du, “Reliable crowdsourcing and deep locality-\npreserving learning for expression recognition in the wild,” inProceedings\nof the IEEE conference on computer vision and pattern recognition, 2017,\npp. 2852–2861.\n[10] R. Rothe, R. Timofte, and L. Van Gool, “Dex: Deep expectation\nof apparent age from a single image,” in Proceedings of the IEEE\ninternational conference on computer vision workshops , 2015, pp. 10–\n15.\n[11] K. Ricanek and T. Tesafaye, “Morph: A longitudinal image database\nof normal adult age-progression,” in 7th international conference on\nautomatic face and gesture recognition (FGR06) . IEEE, 2006, pp. 341–\n345.\n[12] S. Escalera, J. Fabian, P. Pardo, X. Bar ´o, J. Gonzalez, H. J. Escalante,\nD. Misevic, U. Steiner, and I. Guyon, “Chalearn looking at people 2015:\nApparent age and cultural event recognition datasets and results,” in\nProceedings of the IEEE International Conference on Computer Vision\nWorkshops, 2015, pp. 1–9.\n11\n[13] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes\nin the wild,” in Proceedings of the IEEE international conference on\ncomputer vision, 2015, pp. 3730–3738.\n[14] R. Ranjan, V . M. Patel, and R. Chellappa, “Hyperface: A deep multi-task\nlearning framework for face detection, landmark localization, pose esti-\nmation, and gender recognition,” IEEE transactions on pattern analysis\nand machine intelligence , vol. 41, no. 1, pp. 121–135, 2017.\n[15] R. Ranjan, S. Sankaranarayanan, C. D. Castillo, and R. Chellappa, “An\nall-in-one convolutional neural network for face analysis,” in 2017 12th\nIEEE international conference on automatic face & gesture recognition\n(FG 2017). IEEE, 2017, pp. 17–24.\n[16] W. Liu, Y . Wen, Z. Yu, M. Li, B. Raj, and L. Song, “Sphereface: Deep\nhypersphere embedding for face recognition,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2017, pp. 212–\n220.\n[17] H. Wang, Y . Wang, Z. Zhou, X. Ji, D. Gong, J. Zhou, Z. Li, and\nW. Liu, “Cosface: Large margin cosine loss for deep face recognition,”\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, 2018, pp. 5265–5274.\n[18] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, “Arcface: Additive angular\nmargin loss for deep face recognition,” in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition , 2019, pp. 4690–\n4699.\n[19] G. B. Huang, M. Mattar, T. Berg, and E. Learned-Miller, “Labeled faces\nin the wild: A database forstudying face recognition in unconstrained\nenvironments,” in Workshop on faces in’Real-Life’Images: detection,\nalignment, and recognition , 2008.\n[20] Y . Zhong and W. Deng, “Face transformer for recognition,” arXiv\npreprint arXiv:2103.14803, 2021.\n[21] K. Wang, X. Peng, J. Yang, D. Meng, and Y . Qiao, “Region attention\nnetworks for pose and occlusion robust facial expression recognition,”\nIEEE Transactions on Image Processing , vol. 29, pp. 4057–4069, 2020.\n[22] K. Wang, X. Peng, J. Yang, S. Lu, and Y . Qiao, “Suppressing uncertain-\nties for large-scale facial expression recognition,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition , 2020,\npp. 6897–6906.\n[23] A. H. Farzaneh and X. Qi, “Facial expression recognition in the wild\nvia deep attentive center loss,” in Proceedings of the IEEE/CVF winter\nconference on applications of computer vision , 2021, pp. 2402–2411.\n[24] H. Li, N. Wang, X. Ding, X. Yang, and X. Gao, “Adaptively learning\nfacial expression representation via cf labels and distillation,” IEEE\nTransactions on Image Processing , vol. 30, pp. 2016–2028, 2021.\n[25] J. Zeng, S. Shan, and X. Chen, “Facial expression recognition with\ninconsistently annotated datasets,” in Proceedings of the European con-\nference on computer vision (ECCV) , 2018, pp. 222–237.\n[26] Y . Li, J. Zeng, S. Shan, and X. Chen, “Occlusion aware facial expression\nrecognition using cnn with attention mechanism,” IEEE Transactions on\nImage Processing, vol. 28, no. 5, pp. 2439–2450, 2018.\n[27] X. Zhang, F. Zhang, and C. Xu, “Joint expression synthesis and repre-\nsentation learning for facial expression recognition,” IEEE Transactions\non Circuits and Systems for Video Technology , vol. 32, no. 3, pp. 1681–\n1695, 2021.\n[28] H. Liu, H. Cai, Q. Lin, X. Li, and H. Xiao, “Adaptive multilayer\nperceptual attention network for facial expression recognition,” IEEE\nTransactions on Circuits and Systems for Video Technology, vol. 32, no. 9,\npp. 6253–6266, 2022.\n[29] R. Rothe, R. Timofte, and L. Van Gool, “Deep expectation of real and\napparent age from a single image without facial landmarks,” International\nJournal of Computer Vision , vol. 126, no. 2, pp. 144–157, 2018.\n[30] K. Zhang, N. Liu, X. Yuan, X. Guo, C. Gao, Z. Zhao, and Z. Ma, “Fine-\ngrained age estimation in the wild with attention lstm networks,” IEEE\nTransactions on Circuits and Systems for Video Technology, vol. 30, no. 9,\npp. 3140–3152, 2019.\n[31] W. Li, J. Lu, J. Feng, C. Xu, J. Zhou, and Q. Tian, “Bridgenet: A\ncontinuity-aware probabilistic network for age estimation,” inProceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, 2019, pp. 1145–1154.\n[32] Z. Niu, M. Zhou, L. Wang, X. Gao, and G. Hua, “Ordinal regression\nwith multiple output cnn for age estimation,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2016, pp. 4920–\n4928.\n[33] S. Chen, C. Zhang, M. Dong, J. Le, and M. Rao, “Using ranking-cnn\nfor age estimation,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition , 2017, pp. 5183–5192.\n[34] H. Pan, H. Han, S. Shan, and X. Chen, “Mean-variance loss for deep\nage estimation from a face,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition , 2018, pp. 5285–5294.\n[35] X. Wen, B. Li, H. Guo, Z. Liu, G. Hu, M. Tang, and J. Wang, “Adaptive\nvariance based label distribution learning for facial age estimation,” in\nEuropean Conference on Computer Vision . Springer, 2020, pp. 379–\n395.\n[36] W. Shen, Y . Guo, Y . Wang, K. Zhao, B. Wang, and A. L. Yuille,\n“Deep regression forests for age estimation,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition , 2018, pp. 2304–\n2313.\n[37] W. Shen, Y . Guo, Y . Wang, K. Zhao, B. Wang, and A. Yuille, “Deep\ndifferentiable random forests for age estimation,” IEEE transactions on\npattern analysis and machine intelligence , vol. 43, no. 2, pp. 404–419,\n2019.\n[38] Z. Tan, J. Wan, Z. Lei, R. Zhi, G. Guo, and S. Z. Li, “Efficient group-n\nencoding and decoding for facial age estimation,” IEEE transactions on\npattern analysis and machine intelligence, vol. 40, no. 11, pp. 2610–2623,\n2017.\n[39] N.-H. Shin, S.-H. Lee, and C.-S. Kim, “Moving window regression: A\nnovel approach to ordinal regression,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2022, pp.\n18 760–18 769.\n[40] N. Zhang, M. Paluri, M. Ranzato, T. Darrell, and L. Bourdev, “Panda:\nPose aligned networks for deep attribute modeling,” in Proceedings of\nthe IEEE conference on computer vision and pattern recognition , 2014,\npp. 1637–1644.\n[41] E. M. Rudd, M. G ¨unther, and T. E. Boult, “Moon: A mixed objective\noptimization network for the recognition of facial attributes,” in European\nConference on Computer Vision . Springer, 2016, pp. 19–35.\n[42] E. M. Hand and R. Chellappa, “Attributes for improved attributes: A\nmulti-task network utilizing implicit and explicit relationships for facial\nattribute classification,” in Thirty-First AAAI Conference on Artificial\nIntelligence, 2017.\n[43] N. Zhuang, Y . Yan, S. Chen, and H. Wang, “Multi-task learning of\ncascaded cnn for facial attribute classification,” in 2018 24th International\nConference on Pattern Recognition (ICPR). IEEE, 2018, pp. 2069–2074.\n[44] L. Mao, Y . Yan, J.-H. Xue, and H. Wang, “Deep multi-task multi-label\ncnn for effective facial attribute classification,” IEEE Transactions on\nAffective Computing, 2020.\n[45] R. Caruana, “Multitask learning,” Machine learning, vol. 28, no. 1, pp.\n41–75, 1997.\n[46] L. Zhang, Z. He, Y . Yang, L. Wang, and X. Gao, “Tasks integrated\nnetworks: Joint detection and retrieval for image search,” IEEE Transac-\ntions on Pattern Analysis and Machine Intelligence , vol. 44, no. 1, pp.\n456–473, 2020.\n[47] Z. He, L. Zhang, X. Gao, and D. Zhang, “Multi-adversarial faster-rcnn\nwith paradigm teacher for unrestricted object detection,” International\nJournal of Computer Vision , vol. 131, no. 3, pp. 680–700, 2023.\n[48] Q. Duan and L. Zhang, “Look more into occlusion: Realistic face\nfrontalization and recognition with boostgan,” IEEE transactions on\nneural networks and learning systems , vol. 32, no. 1, pp. 214–228, 2020.\n[49] Q. Duan, L. Zhang, and X. Gao, “Simultaneous face completion and\nfrontalization via mask guided two-stage gan,” IEEE Transactions on\nCircuits and Systems for Video Technology , vol. 32, no. 6, pp. 3761–\n3773, 2021.\n[50] J. Zhang, Y . Chen, and Z. Tu, “Uncertainty-aware 3d human pose\nestimation from monocular video,” in Proceedings of the 30th ACM\nInternational Conference on Multimedia , 2022, pp. 5102–5113.\n[51] R. Valle, J. M. Buenaposada, and L. Baumela, “Multi-task head pose\nestimation in-the-wild,” IEEE Transactions on Pattern Analysis and\nMachine Intelligence, vol. 43, no. 8, pp. 2874–2881, 2020.\n[52] A. Bulat and G. Tzimiropoulos, “How far are we from solving the\n2d & 3d face alignment problem?(and a dataset of 230,000 3d facial\nlandmarks),” in Proceedings of the IEEE international conference on\ncomputer vision, 2017, pp. 1021–1030.\n[53] Y . Liu, H. Shi, H. Shen, Y . Si, X. Wang, and T. Mei, “A new\ndataset and boundary-attention semantic segmentation for face parsing,”\nin Proceedings of the AAAI Conference on Artificial Intelligence , vol. 34,\nno. 07, 2020, pp. 11 637–11 644.\n[54] Y . Chen, Z. Tu, D. Kang, R. Chen, L. Bao, Z. Zhang, and J. Yuan,\n“Joint hand-object 3d reconstruction from a single image with cross-\nbranch feature fusion,” IEEE Transactions on Image Processing , vol. 30,\npp. 4008–4021, 2021.\n[55] Y . Feng, F. Wu, X. Shao, Y . Wang, and X. Zhou, “Joint 3d face re-\nconstruction and dense alignment with position map regression network,”\nin Proceedings of the European conference on computer vision (ECCV) ,\n2018, pp. 534–551.\n12\n[56] Z. Tu, Z. Huang, Y . Chen, D. Kang, L. Bao, B. Yang, and J. Yuan,\n“Consistent 3d hand reconstruction in video via self-supervised learning,”\nIEEE Transactions on Pattern Analysis and Machine Intelligence , 2023.\n[57] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and\nB. Guo, “Swin transformer: Hierarchical vision transformer using shifted\nwindows,” in Proceedings of the IEEE/CVF international conference on\ncomputer vision, 2021, pp. 10 012–10 022.\n[58] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep\nnetwork training by reducing internal covariate shift,” in International\nconference on machine learning . PMLR, 2015, pp. 448–456.\n[59] S. Woo, J. Park, J.-Y . Lee, and I. S. Kweon, “Cbam: Convolutional\nblock attention module,” in Proceedings of the European conference on\ncomputer vision (ECCV) , 2018, pp. 3–19.\n[60] G. Levi and T. Hassner, “Age and gender classification using convo-\nlutional neural networks,” in Proceedings of the IEEE conference on\ncomputer vision and pattern recognition workshops , 2015, pp. 34–42.\n[61] S. Sengupta, J.-C. Chen, C. Castillo, V . M. Patel, R. Chellappa, and\nD. W. Jacobs, “Frontal to profile face verification in the wild,” in 2016\nIEEE winter conference on applications of computer vision (WACV) .\nIEEE, 2016, pp. 1–9.\n[62] S. Moschoglou, A. Papaioannou, C. Sagonas, J. Deng, I. Kotsia,\nand S. Zafeiriou, “Agedb: the first manually collected, in-the-wild age\ndatabase,” in proceedings of the IEEE conference on computer vision\nand pattern recognition workshops , 2017, pp. 51–59.\n[63] T. Zheng, W. Deng, and J. Hu, “Cross-age lfw: A database for studying\ncross-age face recognition in unconstrained environments,” arXiv preprint\narXiv:1708.08197, 2017.\n[64] T. Zheng and W. Deng, “Cross-pose lfw: A database for studying cross-\npose face recognition in unconstrained environments,” Beijing University\nof Posts and Telecommunications, Tech. Rep , vol. 5, p. 7, 2018.\n[65] B. Maze, J. Adams, J. A. Duncan, N. Kalka, T. Miller, C. Otto, A. K.\nJain, W. T. Niggel, J. Anderson, J. Cheney et al., “Iarpa janus benchmark-\nc: Face dataset and protocol,” in 2018 international conference on\nbiometrics (ICB). IEEE, 2018, pp. 158–165.\n[66] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao, “Joint face detection and\nalignment using multitask cascaded convolutional networks,” IEEE signal\nprocessing letters, vol. 23, no. 10, pp. 1499–1503, 2016.\n[67] L. Yuan, Y . Chen, T. Wang, W. Yu, Y . Shi, Z.-H. Jiang, F. E. Tay,\nJ. Feng, and S. Yan, “Tokens-to-token vit: Training vision transformers\nfrom scratch on imagenet,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision , 2021, pp. 558–567.\n[68] I. Loshchilov and F. Hutter, “Decoupled weight decay regularization,”\nin International Conference on Learning Representations , 2018.\n[69] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V . Le, “Randaugment:\nPractical automated data augmentation with a reduced search space,” in\nProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition workshops, 2020, pp. 702–703.\n[70] Z. Zhong, L. Zheng, G. Kang, S. Li, and Y . Yang, “Random erasing\ndata augmentation,” in Proceedings of the AAAI conference on artificial\nintelligence, vol. 34, no. 07, 2020, pp. 13 001–13 008.\n[71] D. Acharya, Z. Huang, D. Pani Paudel, and L. Van Gool, “Covariance\npooling for facial expression recognition,” in Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition Workshops ,\n2018, pp. 367–374.\n[72] X. Liu, S. Li, M. Kan, J. Zhang, S. Wu, W. Liu, H. Han, S. Shan,\nand X. Chen, “Agenet: Deeply learned regressor and classifier for robust\napparent age estimation,” in Proceedings of the IEEE International\nConference on Computer Vision Workshops , 2015, pp. 16–24.\n[73] U. Mahbub, S. Sarkar, and R. Chellappa, “Segment-based methods\nfor facial attribute detection from partial faces,” IEEE Transactions on\nAffective Computing, vol. 11, no. 4, pp. 601–613, 2018.\n[74] D. Yi, Z. Lei, S. Liao, and S. Z. Li, “Learning face representation from\nscratch,” arXiv preprint arXiv:1411.7923 , 2014.\nLixiong Qin received the B.E. degree from Bei-\njing University of Posts and Telecommunications\n(BUPT), Beijing, China, in 2022, where he is cur-\nrently pursuing the master’s degree in artificial in-\ntelligence with the School of Artificial Intelligence.\nHis research interests include computer vision and\nface analysis.\nMei Wang received the B.E. degree and Ph.D. de-\ngree in information and communication engineering\nfrom Beijing University of Posts and Telecommuni-\ncations (BUPT), Beijing, China, in 2013 and 2022,\nrespectively. She is currently a Postdoc in the School\nof Artificial Intelligence, Beijing University of Posts\nand Telecommunications, China. Her research in-\nterests include computer vision, with a particular\nemphasis in computer vision, domain adaptation and\nAI fairness.\nChao Deng received the M.S. degree and the Ph.D.\ndegree from Harbin Institute of Technology, Harbin,\nChina, in 2003 and 2009 respectively. He is currently\na deputy general manager with AI center of China\nMobile Research Institute. His research interests\ninclude machine learning and artificial intelligence\nfor ICT operations.\nKe Wang received the M.S. degree form Beijing\nInstitute of Technology in 2018. He is currently an\nalgorithm engineer in China Mobile Research Insti-\ntute. His research interests include 3D reconstruc-\ntion, 3D face recognition and multi-modal fusion.\nXi Chen received her M.S. degree from Communi-\ncation University of China. She works as an algo-\nrithm engineer in image and video processing and\ncomputer vision at China Mobile Research Institute\nsince 2018.\n13\nJiani Hu received the B.E. degree in telecom-\nmunication engineering from China University of\nGeosciences in 2003, and the Ph.D. degree in signal\nand information processing from Beijing University\nof Posts and Telecommunications (BUPT), Beijing,\nChina, in 2008. She is currently an associate pro-\nfessor in School of Artificial Intelligence, BUPT.\nHer research interests include information retrieval,\nstatistical pattern recognition and computer vision.\nWeihong Deng received the B.E. degree in infor-\nmation engineering and the Ph.D. degree in signal\nand information processing from the Beijing Uni-\nversity of Posts and Telecommunications (BUPT),\nBeijing, China, in 2004 and 2009, respectively.\nHe is currently a professor in School of Artificial\nIntelligence, BUPT. His research interests include\ntrustworthy biometrics and affective computing, with\na particular emphasis in face recognition and ex-\npression analysis. He has published over 100 papers\nin international journals and conferences, such as\nIEEE TPAMI, TIP, IJCV , CVPR and ICCV . He serves as area chair for\nmajor international conferences such as IJCB, FG, IJCAI, ACMMM, and\nICME, guest editor for IEEE Transactions on Biometrics, Behavior, and\nIdentity Science, and Image and Vision Computing Journal, and the reviewer\nfor dozens of international journals and conferences. His Dissertation was\nawarded the outstanding doctoral dissertation award by Beijing Municipal\nCommission of Education. He has been supported by the programs for New\nCentury Excellent Talents and Young Changjiang Scholar by Ministry of\nEducation.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6548177599906921
    },
    {
      "name": "Facial recognition system",
      "score": 0.6424239277839661
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5776617527008057
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5623347163200378
    },
    {
      "name": "Transformer",
      "score": 0.49351322650909424
    },
    {
      "name": "Estimation",
      "score": 0.4414023458957672
    },
    {
      "name": "Face detection",
      "score": 0.4397597908973694
    },
    {
      "name": "Computer vision",
      "score": 0.39676278829574585
    },
    {
      "name": "Engineering",
      "score": 0.12624087929725647
    },
    {
      "name": "Voltage",
      "score": 0.07011944055557251
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}