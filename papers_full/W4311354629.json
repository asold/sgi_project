{
  "title": "Global reconstruction of language models with linguistic rules – Explainable AI for online consumer reviews",
  "url": "https://openalex.org/W4311354629",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2047374811",
      "name": "Markus Binder",
      "affiliations": [
        "University of Regensburg"
      ]
    },
    {
      "id": "https://openalex.org/A2029560042",
      "name": "Bernd Heinrich",
      "affiliations": [
        "University of Regensburg"
      ]
    },
    {
      "id": "https://openalex.org/A2970501359",
      "name": "Marcus Hopf",
      "affiliations": [
        "University of Regensburg"
      ]
    },
    {
      "id": "https://openalex.org/A2145458303",
      "name": "Alexander Schiller",
      "affiliations": [
        "University of Regensburg"
      ]
    },
    {
      "id": "https://openalex.org/A2047374811",
      "name": "Markus Binder",
      "affiliations": [
        "University of Regensburg"
      ]
    },
    {
      "id": "https://openalex.org/A2029560042",
      "name": "Bernd Heinrich",
      "affiliations": [
        "University of Regensburg"
      ]
    },
    {
      "id": "https://openalex.org/A2970501359",
      "name": "Marcus Hopf",
      "affiliations": [
        "University of Regensburg"
      ]
    },
    {
      "id": "https://openalex.org/A2145458303",
      "name": "Alexander Schiller",
      "affiliations": [
        "University of Regensburg"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891503716",
    "https://openalex.org/W4221167913",
    "https://openalex.org/W2981731882",
    "https://openalex.org/W1918570897",
    "https://openalex.org/W2003463929",
    "https://openalex.org/W2948038898",
    "https://openalex.org/W4285184086",
    "https://openalex.org/W2915171383",
    "https://openalex.org/W3094478020",
    "https://openalex.org/W6763527631",
    "https://openalex.org/W3033776495",
    "https://openalex.org/W2963063806",
    "https://openalex.org/W3090395639",
    "https://openalex.org/W4220820301",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2762968537",
    "https://openalex.org/W2946015932",
    "https://openalex.org/W4205444062",
    "https://openalex.org/W3128015545",
    "https://openalex.org/W3025915057",
    "https://openalex.org/W3118627298",
    "https://openalex.org/W4292347335",
    "https://openalex.org/W3117168610",
    "https://openalex.org/W2963847595",
    "https://openalex.org/W3121648738",
    "https://openalex.org/W2962772482",
    "https://openalex.org/W3126694655",
    "https://openalex.org/W6912843683",
    "https://openalex.org/W3093588751",
    "https://openalex.org/W2970464057",
    "https://openalex.org/W2964117978",
    "https://openalex.org/W1951269370",
    "https://openalex.org/W2941666437",
    "https://openalex.org/W3154507271",
    "https://openalex.org/W2970120757",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W2234079371",
    "https://openalex.org/W4206361803",
    "https://openalex.org/W2618851150",
    "https://openalex.org/W3210314098",
    "https://openalex.org/W4224312275",
    "https://openalex.org/W2123442489",
    "https://openalex.org/W2971196067",
    "https://openalex.org/W3123136864",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W3011573503",
    "https://openalex.org/W6891861599",
    "https://openalex.org/W3166915941",
    "https://openalex.org/W2788403449",
    "https://openalex.org/W2965411951",
    "https://openalex.org/W3094605956",
    "https://openalex.org/W4220844856",
    "https://openalex.org/W2954278700",
    "https://openalex.org/W2962912504",
    "https://openalex.org/W4200048988",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2908854766",
    "https://openalex.org/W4288086191",
    "https://openalex.org/W3210818730",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2925618549",
    "https://openalex.org/W3006026155",
    "https://openalex.org/W4320351531",
    "https://openalex.org/W3037252472",
    "https://openalex.org/W3125189097",
    "https://openalex.org/W2884001105",
    "https://openalex.org/W4226520980",
    "https://openalex.org/W3004680503"
  ],
  "abstract": "Abstract Analyzing textual data by means of AI models has been recognized as highly relevant in information systems research and practice, since a vast amount of data on eCommerce platforms, review portals or social media is given in textual form. Here, language models such as BERT, which are deep learning AI models, constitute a breakthrough and achieve leading-edge results in many applications of text analytics such as sentiment analysis in online consumer reviews. However, these language models are “black boxes”: It is unclear how they arrive at their predictions. Yet, applications of language models, for instance, in eCommerce require checks and justifications by means of global reconstruction of their predictions, since the decisions based thereon can have large impacts or are even mandatory due to regulations such as the GDPR. To this end, we propose a novel XAI approach for global reconstructions of language model predictions for token-level classifications (e.g., aspect term detection) by means of linguistic rules based on NLP building blocks (e.g., part-of-speech). The approach is analyzed on different datasets of online consumer reviews and NLP tasks. Since our approach allows for different setups, we further are the first to analyze the trade-off between comprehensibility and fidelity of global reconstructions of language model predictions. With respect to this trade-off, we find that our approach indeed allows for balanced setups for global reconstructions of BERT’s predictions. Thus, our approach paves the way for a thorough understanding of language model predictions in text analytics. In practice, our approach can assist businesses in their decision-making and supports compliance with regulatory requirements.",
  "full_text": "Vol.:(0123456789)1 3\nElectronic Markets (2022) 32:2123–2138 \nhttps://doi.org/10.1007/s12525-022-00612-5\nRESEARCH PAPER\nGlobal reconstruction of language models with linguistic rules \n– Explainable AI for online consumer reviews\nMarkus Binder1 · Bernd Heinrich1 · Marcus Hopf1 · Alexander Schiller1\nReceived: 30 May 2022 / Accepted: 27 October 2022 / Published online: 13 December 2022 \n© The Author(s) 2022\nAbstract\nAnalyzing textual data by means of AI models has been recognized as highly relevant in information systems research and \npractice, since a vast amount of data on eCommerce platforms, review portals or social media is given in textual form. Here, \nlanguage models such as BERT, which are deep learning AI models, constitute a breakthrough and achieve leading-edge \nresults in many applications of text analytics such as sentiment analysis in online consumer reviews. However, these language \nmodels are “black boxes”: It is unclear how they arrive at their predictions. Yet, applications of language models, for instance, \nin eCommerce require checks and justifications by means of global reconstruction of their predictions, since the decisions \nbased thereon can have large impacts or are even mandatory due to regulations such as the GDPR. To this end, we propose a \nnovel XAI approach for global reconstructions of language model predictions for token-level classifications (e.g., aspect term \ndetection) by means of linguistic rules based on NLP building blocks (e.g., part-of-speech). The approach is analyzed on dif-\nferent datasets of online consumer reviews and NLP tasks. Since our approach allows for different setups, we further are the \nfirst to analyze the trade-off between comprehensibility and fidelity of global reconstructions of language model predictions. \nWith respect to this trade-off, we find that our approach indeed allows for balanced setups for global reconstructions of BERT’s \npredictions. Thus, our approach paves the way for a thorough understanding of language model predictions in text analytics. In \npractice, our approach can assist businesses in their decision-making and supports compliance with regulatory requirements.\nKeywords Explainable AI · Text analytics · Language models · BERT · Linguistic rules · Online consumer reviews\nJEL Classification C80\nIntroduction\nHuge amounts of unstructured textual data are generated \nacross various channels of information systems (IS) such as \neCommerce platforms, review portals or social media every \nsecond (Potnis, 2018). Consequently, the need for techniques \nthat automatically analyze textual data is increasing: Until \n2028, the revenues from the natural language processing \n(NLP) market worldwide are expected to increase at a com-\npound annual growth rate of almost 30% to over 100 billion \nUSD, with text analytics expected to have the highest growth \n(Fortune Business Insights, 2021). As text analytics facili -\ntate diverse applications such as sentiment analysis or text \nsummarization (Young et al., 2018), various organizations in \ndifferent business areas benefit from techniques of text ana-\nlytics (Coheur, 2020; Zhang et al., 2020). For instance, prod-\nuct or service providers can use such techniques to analyze \nconsumer sentiments in large amounts of online consumer \nreviews. Using this consumer feedback enables organizations \nResponsible Editor: Fethi Abderrahmane Rabhi.\n * Bernd Heinrich \n bernd.heinrich@ur.de\n Markus Binder \n Markus1.Binder@ur.de\n Marcus Hopf \n Marcus.Hopf@ur.de\n Alexander Schiller \n Alexander.Schiller@ur.de\n1 University of Regensburg, Germany, at the Faculty \nof Informatics and Data Science, Regensburg, Germany\n2124 M. Binder et al.\n1 3\nto effectively improve their products and services (Chatterjee, \n2019; Heinrich et al., 2022; Heinrich et al., 2020).\nThe state-of-the-art techniques of text analytics are lan-\nguage models, such as the popular deep learning AI model \n‘Bidirectional Encoder Representations from Transform-\ners’ (BERT) (Devlin et al., 2019) or its descendants (e.g., \nALBERT; Lan et al., 2020), as they have achieved leading-\nedge results in many tasks such as aspect-based sentiment \nanalysis (Wang et al., 2018). Language models enable a con-\ntextualized representation of textual data by assessing the \nconditional probability of each token (e.g., a word) given \nthe contextual tokens surrounding it (Peters et al., 2018a). \nBesides coarser classification tasks for sentences, for exam-\nple, these language model representations can then be used, \nin particular, as basis for central token-level classifications \nsuch as aspect term and sentiment term detection. Since the \nlanguage model BERT is already incorporated in a plethora \nof business IS applications, we demonstrate our approach \nby means of BERT as leading exponent of language models \nin this paper. Amongst others, popular application scenarios \nof BERT in electronic markets are eCommerce, chatbots, \nfinance or online recruiting (Coheur, 2020; Dastin, 2018; Luo \net al., 2022; Repke & Krestel, 2021; Shrestha et al., 2021; S. \nXu et al., 2020; Yang et al., 2020; Zhang et al., 2020). How-\never, similar to most other state-of-the-art deep learning mod-\nels, BERT is a “black box”. That is, over 100 million learned \nparameters (Devlin et al., 2019) and various hidden layers \ncontribute to BERT’s immense complexity, making it hardly \n(if at all) possible to comprehend why and how BERT arrives \nat its predictions (Kovaleva et al., 2019). To address this \nblack box nature of AI models, a vastly increasing focus on \nexplainable AI (XAI) in IS research and practice has emerged \n(Adadi & Berrada, 2018; Förster et al., 2021; Förster et al., \n2020b). Literature agrees that the need for reconstructions \nand justifications is urgent and a ‘huge open scientific chal-\nlenge’ (Guidotti et al., 2018). It is even expected that “algo-\nrithmic auditing and ‘data protection by design’ practices will \nlikely become the new gold standard for enterprises deploy-\ning machine learning systems” (Casey et al., 2019). Thereby, \nregulations such as the General Data Protection Regulation \n(GDPR) in the European Union impose an extensive ‘right to \nexplanation’ for automated data processing systems in gen-\neral and thereby lay the foundation to enforce algorithmic \nauditing in companies. In particular, algorithmic auditing \nis highly relevant for domain experts, managers and data \nscientists that utilize the language models’ predictions for \nbusiness-critical decisions or implementations and need to \njustify their actions. This is especially the case for application \nscenarios (AS) in electronic markets, as exemplarily outlined \nin the following and captured later on:\n• eCommerce (AS1): In eCommerce, BERT is used to con-\nduct token-level classification in the course of sentiment \nanalyses of online consumer reviews on online platforms \nsuch as Airbnb, Yelp or TripAdvisor for product develop-\nment, services offerings and forecasting future demand \n(Heidari & Rafatirad, 2020; Shrestha et al., 2021; S. Xu \net al., 2020). Since these analyses and decisions have \nlarge impacts, they require additional validation checks \nand justifications, far beyond measuring only the pre -\ndiction accuracy of BERT. For instance, it needs to be \nensured that specific groups of consumers are not dis-\ncriminated against by assigning a negative sentiment to \ncertain countries, ethnicities or genders.\n• Chatbots (AS2): In applications in consumer services \n(Luo et al., 2022), BERT-based chatbots conduct direct \nconsumer interaction and embody the company’s voice. \nThereby, reconstructions and justifications regarding \nthe underlying BERT model are mandatory to prevent \nunhelpful, rude or misleading dialogues and thus, to sup-\nport consumer satisfaction.\n• Financial applications (AS3): BERT descendants such as \nFinBERT (Yang et al., 2020) enable token-level classifica-\ntions of financial entities, sentiments and their relations from \ntexts such as social media posts (e.g., tweets from CEOs or \nother experts) or contract documents. The extracted infor-\nmation is used for key tasks in finance such as accounting, \nauditing, compliance and risk assessment. Furthermore, \nlanguage models enable to automatically process millions \nof documents as contained in data leaks such as the Panama \nPapers (O’Donovan et al., 2019) for tax fraud detection. In \nparticular, if legal actions are initiated based on predictions \nfrom language models (e.g., tax prosecution based on data \nleaks), validation checks are mandatory.\n• Online recruiting (AS4): Supporting text analytics of \napplication documents (Schiller, 2019), language mod-\nels such as BERT enable pre-processing und pre-filtering \nof applications and candidates on online job platforms. \nHere, auditing and validation are required as such auto-\nmated recruitment may lead to discrimination (e.g., by \ngender or origin; Dastin, 2018). Reconstructions of mod-\nels help to avoid such discriminations.\nThese application scenarios show that it is crucial to \nreconstruct BERT’s predictions to be able to justify the deci-\nsions based thereon. Here, the reconstructions and explana-\ntions in these scenarios are required on a global level as in \nall those application scenarios the predictions of language \nmodels are used in ongoing operations on a daily basis. This \nmeans that a vast number of decisions are made based on \nthese predictions day-by-day for newly generated and hitherto \nunknown textual data (e.g., chatbots or review summariza-\ntions are applied in real-time on consumer texts). Therefore, \nit is not feasible to use local approaches for reconstruc-\ntion, as this would require huge efforts for manual checks \nof each local reconstruction and could practically only be \n2125\nGlobal Reconstruction of Language Models with Linguistic Rules\n1 3\ndone a-posteriori if at all. Therefore, global approaches are \nessential for reconstructions of language model predictions \nin many applications. Here, we focus on global reconstruc-\ntions of BERT’s predictions for token-level classifications in \nthis work, since this constitutes popular application scenarios \nof BERT (e.g., AS1, AS3) and since BERT also establishes \ntext representations based on tokens. Moreover, as Zafar \net al. (2021) and Yan et al. (2022) indicate, a reconstruction \napproach for token-level classifications can also serve as a \nbasis for reconstructions of coarser classification tasks, for \ninstance, for sentence-level classifications (e.g., AS2, AS4).\nA promising way to obtain such a reconstruction and thus \njustify BERT’s predictions is to conduct a rule-based XAI \napproach. On the one hand, rules are highly concrete, which \nalso has been emphasized by Förster et al. (2020a) as decisive \nXAI characteristic. Indeed, studies have shown that users “pre-\nfer, trust and understand rules better than alternatives” (Ribeiro \net al., 2018; cf. also Arrieta et al., 2020). On the other hand, \nrule-based approaches preserve the AI model itself and thus, \nits high performance, while offering post-hoc reconstructions \nfor explanations (Adadi and Berrada, 2018). Here, local rule-\nbased approaches focus on explaining each prediction for a \nspecific input separately, for instance, by using specific words \nto predict the sentiment term in a single sentence of an online \nconsumer review. In contrast, global approaches aim at recon-\nstructing the model’s predictions as a whole (Danilevsky et al., \n2020). A global approach ideally requires a smaller rule set for \nreconstructing multiple predictions of a language model com-\npared to local approaches that establish a separate and highly \nspecific rule for each individual prediction and therefore are \nnot really generalizable (Danilevsky et al., 2020).\nTo enable such a global approach, our idea is to build \nrules based on linguistic information (so-called linguistic \nrules) which generalize specific words and sentences and \ncan be modeled by NLP building blocks such as part-of-\nspeech tags or dependency relations (Qi et al., 2020). Using \nNLP building blocks instead of single words as rule argu-\nments is promising for global reconstruction, as they allow \nfor rule arguments and rules analyzing (much) more than, for \ninstance, one single sentence in an online consumer review. \nMoreover, NLP relation building blocks allow to account \nfor the contextual information in a sentence (i.e., relations \nbetween words), which is crucial for the reconstruction of \nlanguage model predictions for token-level classifications, \nsince language models also use contextual information. \nThus, we focus on the following main research question:\nRQ1: How can language model predictions for token-\nlevel classifications be globally reconstructed by means \nof an XAI approach based on linguistic rules?\nAnalogous to local reconstructions, a global reconstruc-\ntion has to be analyzed regarding its fidelity (Danilevsky \net al., 2020; Gilpin et al., 2018) and comprehensibility \n(Guidotti et al., 2018). In case of rule-based approaches, \nthe comprehensibility of the rule set depends on the com-\nplexity (with respect to the length of the rules; cf. Guidotti \net al., 2018) and the generalizability (words vs. NLP build-\ning blocks as discussed above) of the rules. Thereby, our \napproach allows for different setups regarding the com-\nprehensibility of the rule set (e.g., by varying rule length), \nwhich is in general outlined as an important requirement of \nan XAI approach (Gilpin et al., 2018). This enables to ana-\nlyze the trade-off between these two objectives in a recon-\nstruction, which further supports adoption in IS. Thus, the \nsecond research question is as follows:\nRQ2: How can the trade-off between fidelity and compre-\nhensibility of global reconstructions of language model \npredictions by linguistic rules be analyzed?\nHence, our contribution is twofold: (1) We are the first to \npropose a global XAI approach for reconstructing predic-\ntions of language models by linguistic rules. In particular, \n(2) this paper is thus the first to analyze the trade-off between \nfidelity and comprehensibility (i.e., complexity and general-\nizability) in this setting.\nFor our analysis, we focus on the highly relevant tasks \nof aspect term detection and sentiment term detection in \nonline consumer reviews. To that end, we use two recog-\nnized online consumer review datasets from the domains \nof laptops and restaurants to account for different types of \ngoods (i.e., laptops as search goods and restaurants as expe-\nrience goods). We find that our linguistic rules are indeed \nsuited for a global reconstruction of BERT’s predictions in \nonline consumer reviews and in particular allow for balanced \nsetups with respect to the trade-off between comprehensibil-\nity and fidelity of the reconstruction.\nThe remainder of this paper is structured as follows. The \nnext section presents the background of our research. Sub-\nsequently, we discuss how to globally reconstruct language \nmodels such as BERT with linguistic rules. Thereafter, we \nanalyze different global reconstructions of BERT, discuss \ntheir results and outline implications for research and prac-\ntice. Finally, we summarize the paper and provide an outlook \non future research directions.\nBackground\nIn this section, we first outline which different types of XAI \napproaches exist in the context of language models. Sec-\nond, several NLP building blocks recognized by literature \nare introduced forming the basis for our approach. The sec-\ntion concludes with a discussion of related work yielding the \naddressed research gap.\n2126 M. Binder et al.\n1 3\nTypes of XAI approaches in the context of language \nmodels\nTo clarify the notion of XAI (i.e., what explainable AI really \nmeans), a characterization in opaque systems, interpretable \nsystems and comprehensible systems has been proposed \n(Doran et al., 2017). Here, opaque systems offer no insights \ninto the system’s reasoning on how inputs are mapped to \nthe corresponding outputs. In that line, modern language \nmodels such as BERT are opaque systems, as it is not pos-\nsible to comprehend its mappings, for instance, compris -\ning over 100 million learned parameter values in the case \nof BERT. Based on that, there are two separate notions of \naddressing this problem. First, interpretable systems allow \nto understand how inputs are mapped to outputs by subdivid-\ning the mapping. This is not feasible for language models \nsuch as BERT due to its large amount of parameters and \nlayers, which results in highly complex concatenated func-\ntions (Devlin et al., 2019). Second, comprehensible systems \nallow to relate properties of the inputs, for instance, single \nterms of an input sentence, to their output such as a clas-\nsification of sentiment terms (Doran et al., 2017). While \nresearch in both areas is important, it has to be pointed out \nthat the resulting XAI approaches are not “actually” explana-\ntion systems (Doran et al., 2017). For instance, rule-based \napproaches mostly give insights on how, but not why specific \npredictions are made (Doran et al., 2017). That is, causality \ncannot be directly established. To account for these differ -\nent notions, we deliberately refer to “reconstructing” BERT \nrather than “explaining” in this paper.\nRelated to the two notions of interpretable and compre-\nhensible systems, there are, in general, two main approaches \nin XAI (Adadi and Berrada 2018): On the one hand, intrin-\nsic XAI approaches ‘force’ the AI model (during training) \nto produce interpretable mappings from input to output \n(Adadi and Berrada 2018). The drawback of these intrinsic \napproaches is that they are limited in the type of interpreta-\ntions they can provide, as they need to restrict the model to \nobtain interpretable mappings, thus usually worsening the \nmodel’s performance (Adadi and Berrada 2018). Due to its \ncomplexity, BERT would have to be extremely simplified to \nenable interpretable mappings. On the other hand, post-hoc \nXAI approaches aim to comprehensibly reconstruct the map-\npings from input to output of an AI model. These approaches \ndo not require to restrict the model during training (Adadi \nand Berrada 2018). Here, a popular method is rule extrac-\ntion, since rules can potentially exhibit a high degree of \ncomprehensibility (Ribeiro et al., 2018). In general, there \nare two categories of rule extraction techniques (Adadi and \nBerrada 2018): 1) Decompositional rule extraction aims at \nextracting rules at selected, often single nodes within a neu-\nral network. To comprehend the predictions of a language \nmodel, it is then necessary to concatenate multiple extracted \nrules for various hidden layers. Thus, the drawback of this \ntechnique is that concatenations of rules are highly complex \nfor deep neural networks such as BERT (Augasta & Kathir-\nvalavakumar, 2012). Since the resulting rules would again \nbe difficult to comprehend, decompositional rule extraction \nis not feasible for comprehensibly reconstructing language \nmodels. 2) In contrast,  pedagogical rule extraction aims at \nextracting rules considering only the inputs and outputs. \nIn particular, rules are extracted based on properties of the \ninputs and the corresponding outputs to reconstruct the map-\npings of the AI model. Thus, this approach can contribute to \na comprehensible reconstruction even for language models \nsuch as BERT, since the extracted rules do not have to be \nconcatenated through the various hidden layers.\nAdditionally, a further important differentiation \nwithin post-hoc XAI research is between global and \nlocal approaches (Danilevsky et al., 2020). Here, global \napproaches aim at reconstructing the predictions of an AI \nmodel by means of one single global model (Danilevsky \net al., 2020). In contrast, local approaches create separate, \nhighly specific reconstruction models for each prediction \n(e.g., in a single sentence of an online consumer review). \nTo enable local reconstructions for IS text analytics appli-\ncations, rules solely based on specific words are used by \nextant literature (e.g., Ribeiro et al., 2018). However, such \nrules lack the ability to generalize. In contrast, linguistic \nrules based on NLP building blocks are more promising for \nthe global reconstruction of language models. Indeed, rule \narguments with NLP building blocks generalize much better \nthan rule arguments with specific words, and NLP relation \nbuilding blocks enable to incorporate contextual informa-\ntion, which is a main component of language models.\nBoth objectives fidelity and comprehensibility are crucial \nfor global post-hoc XAI approaches (Arrieta et al., 2020; \nGuidotti et al., 2018; Szczepański et al., 2021). Indeed, on \nthe one hand, a global reconstruction needs to match the \npredictions of an AI model to avoid false conclusions, which \nis measured by fidelity (Gilpin et al., 2018). On the other \nhand, comprehensibility (i.e., complexity and generalizabil-\nity; commonly measured in terms of model size) enables the \nuse of the reconstruction (Guidotti et al., 2018). Thus, we \nanalyze the reconstruction of BERT regarding its fidelity and \nits comprehensibility and strive to enable different setups \nbetween the two objectives.\nNLP building blocks\nTo enable a reconstruction using linguistic rules, our idea \nis to use different semantical and syntactical NLP build -\ning blocks (cf. Introduction). Thus, we briefly outline NLP \nbuilding blocks that are widely recognized in the literature \n(Fellbaum, 2013; Kamps et al., 2004; Tenney et al., 2019b) \nand that constitute a basis for our reconstruction. Table  1 \n2127\nGlobal Reconstruction of Language Models with Linguistic Rules\n1 3\nsummarizes these different building blocks. Thereby, the \ncolumn ‘type’ characterizes a building block as tag or rela-\ntion (as described in the following). In addition, the column \n‘linguistic information’ shows whether a building block pro-\nvides semantic or syntactic information. For each building \nblock, an example is given in the last column.\nA tag building block provides tag labels for selected \ntokens (e.g., words or punctuation marks) of a sentence. \nTag labels describe a certain syntactic or semantic infor -\nmation of tokens in consideration of the whole sentence. \nPart-of-speech (POS) tags provide information on the syn-\ntactic structure of a sentence. Thereby, the POS tag, such \nas noun (NN), adjective (JJ) or verb (VB), is assigned to \na single token. The building block synsets (SYN) consid-\ners the semantic information of tokens. In particular, SYN \nlabels (e.g., derived from the lexical database WordNet) \nindicate words which share the same or a similar meaning \n(Fellbaum, 2013) taking into account its word context in a \nsentence.\nA relation building block provides a label for a pair of \ntokens in a sentence describing a certain syntactic or seman-\ntic relation between these tokens. These relation building \nblocks enable to account for the contextual information in \na sentence (i.e., the relation between tokens in a sentence), \nwhich is crucial for a reconstruction of BERT as BERT also \nconsiders contextual information. A basic syntactic informa-\ntion is the distance between two tokens, which is covered by \nthe proximity (PROX) building block. For instance, if two \ntokens are next to each other in a sentence, their distance \nis 1. Dependencies (DEP) also link two tokens based on \ntheir syntactical relationship, such as the adjectival modifier \n(amod) or nominal subject (nsubj) dependencies (Manning \net al., 2014). Semantic information is provided by the build-\ning blocks semantic role labeling (SRL) and coreference \n(COREF). SRL relations identify combinations of predicates \nand semantic arguments in a sentence (Tenney et al. 2019b). \nCOREF links two tokens referring to the same entity (Ten-\nney et al. 2019a; b). Consequently, information referring to \none part of the relation can be traced back to the other part.\nRelated work\nOur goal is to reconstruct the language model BERT by \nmeans of linguistic (pedagogical) rules composed of NLP \nbuilding blocks. Hence, XAI approaches analyzing language \nmodels regarding NLP building blocks (category A), XAI \napproaches analyzing pedagogical rules for reconstruct-\ning language models (category B) and XAI approaches for \nlanguage models based on other techniques (category C) \nconstitute the related work. In contrast, general rule-based \nXAI approaches (cf. Adadi and Berrada 2018) and XAI \napproaches (Ramon et al., 2020; Sushil et al., 2018) relying \non a simple ‘bag-of-words’ analysis – both without any focus \non language models – are not in the scope for our research.\nAd category A): Several existing works analyze language \nmodels by using their (contextualized) word embeddings \nor internal states as input to predict  NLP building blocks \n(Coenen et al., 2019; Hewitt & Manning, 2019; Jumelet & \nHupkes, 2018; Kim et al., 2019; Peters et al., 2018b; Tenney \net al., 2019a; Tenney et al. 2019b; Van Aken et al., 2019). \nThen, the quality of these predictions is used as an indica-\ntion whether a certain NLP building block is encoded in \nparticular word embeddings (i.e., vector representations) \nor specific layers of the language models. That is, instead \nof reconstructing predictions of language models for NLP \ntasks in IS (e.g., sentiment term detection), an analysis of the \ngeneral word embeddings themselves is aimed for in these \nworks. For instance, different NLP building blocks have \nbeen predicted by word embeddings of the language models \nELMo (Peters et al. 2018b) and BERT (Tenney et al. 2019a; \nb). However, the aim of our research is a different one. As \ndiscussed in the Introduction, our focus is to better compre-\nhend BERT’s predictions on NLP tasks in IS, for instance, \nto be able to justify decisions made based on its results. To \nenable that, it is necessary to reconstruct the predictions of \nBERT for relevant NLP tasks (such as the extracted senti-\nment terms in online consumer reviews), since these predic-\ntions and not particular word embeddings in form of vector \nrepresentations are the foundation for further decisions. In \nTable 1  Overview of NLP building blocks\nBuilding block Type Linguistic \ninforma-\ntion\nExample labels for the sentence “The waiter of The Burger House was nice,  \nhe smiled at us.”\nPart-of-speech tags (POS) Tags Syntactic POS-label (“waiter”) = NN (Noun)\nSynsets (SYN) Tags Semantic SYN-label (“nice”) = nice.a.01 (Synset description: “pleasant or pleasing or agreeable in \nnature or appearance”)\nDependencies (DEP) Relations Syntactic DEP-label (“waiter”, “nice”) = amod (adjectival modifier)\nSemantic role labeling (SRL) Relations Semantic SRL-label (“he”, “smiled”) = agent-predicate-relation\nCoreferences (COREF) Relations Semantic COREF-label (“waiter”, “he”) = True (referring to the same entity)\nProximity (PROX) Relations Syntactic PROX-label (“waiter”, “nice”) = 6\n2128 M. Binder et al.\n1 3\nthat line, none of the approaches in this category considers \npedagogical rules to enable a reconstruction of predictions \nof a language model for NLP tasks in IS.\nAd category B): There also exist recent, interesting works \nthat analyze language models by means of pedagogical rules \nin a local manner (i.e., for single predictions). In Ribeiro \net al. (2018), individual predictions of simple recurrent \nneural network-based language models are reconstructed \nby separate if–then rules. Building on this work, BERT’s \npredictions in an application of fake news detection on social \nmedia are analyzed in Szczepański et al. (2021). Both works \nhardly incorporate contextual information for reconstruc-\ntions. That is, only information of the previous token is \nconsidered to obtain local reconstruction rules. Thus, both \nworks consider only short rules of low complexity. In addi-\ntion, rules based on individual tokens (e.g., specific words) \nare used. Hence, both works do not discuss the composition \nof tag and relation building blocks when extracting rules \nfor reconstruction and as a result, the proposed rules exhibit \nonly low generalizability. In particular, relation building \nblocks such as DEP or COREF, which enable rules to com-\nprise vital contextual information, are not considered.\nAd category C): Moreover, local non-rule-based XAI \napproaches have been proposed to reason language model \npredictions. In Malkiel et al. (2022), saliency maps are used \nto reason similarity predictions of online consumer reviews \nby a BERT-based model, aiming to highlight important \nword-pairs for specific similarity predictions. Moreover, \ndifferent visualizations with respect to neuron activations \nin the hidden layers have been applied to reason specific \nlanguage model predictions (Brasoveanu & Andonie, 2022). \nIn Kokalj et al. (2021), the known feature importance XAI \napproach ‘shapley additive explanations’ (Lundberg & Lee, \n2017) has been adapted to account for the contextualized \n(token-based) text representation in language models. Fur -\nther, approaches based on the attention weights in language \nmodels have been recently proposed (Ali et al., 2022; S. \nLiu et al., 2021), similarly establishing feature importance \nscores for language model predictions. As an application \ncase, these approaches aim to determine important words for \nsentence sentiment classifications of a BERT-based model. \nHowever, all of these works focus on local reconstructions, \nfor instance, for individual sentences, for which they do not \nconsider NLP building blocks. That is, global (token-level) \nreconstructions by linguistic rules are out of their scope.\nOverall, while the approaches in category A) give inter -\nesting indications on how NLP building blocks may be \nencoded in contextualized word embeddings, they do not \nenable to reconstruct the predictions of language models in \nNLP tasks in IS. In contrast, the approaches in category B) \nindeed analyze rules for reconstructing specific predictions, \nbut only enable local reconstructions and do not incorpo-\nrate different NLP building blocks comprising contextual \nlinguistic information. Thus, they exhibit only low gener -\nalizability. Similarly, the approaches in category C) focus \non reasoning specific language model predictions locally by \nnon-rule-based approaches and do not incorporate different \nNLP building blocks either.\nSumming up, there are very interesting contributions in \nthe field of XAI regarding language models. However, litera-\nture lacks an approach for global reconstructions of language \nmodel predictions for NLP tasks in IS (e.g., sentiment term \ndetection in online consumer reviews) based on pedagogical \nrules. To address this research gap, this paper proposes, to \nthe best of our knowledge, the first global XAI approach for \nreconstructing token-level language model predictions by \nlinguistic (pedagogical) rules. In particular, this paper is thus \nthe first to enable an analysis of the trade-off between fidelity \nand comprehensibility (i.e., complexity and generalizability) \nin this setting.\nGlobal reconstruction of BERT with linguistic \nrules\nIn this section, we introduce our approach by postulating \nthe formal structure of linguistic rules for the global recon-\nstruction of BERT’s predictions and then outline appropriate \nmeasures to analyze this reconstruction.\nFormal structure of linguistic rules \nfor reconstructing BERT’s predictions\nWe begin by deriving the formal structure of linguistic \nrules. Thereby, for illustration purpose, the language model \nBERT is applied for the token classification tasks aspect \nterm detection and sentiment term detection that are fre -\nquently used in online consumer reviews (Dai & Song, 2019; \nSun et al., 2019; H. Xu et al. 2019). More precisely, each \nsentence in a document comprises a string value and can be \nsplit up by tokenization into disjunct substrings (so-called \ntokens), which have a linguistic meaning, such as (sub)words \nor punctuation marks. The precise tokenization of sentences \ndepends on specific tokenization policies. For this work, we \nused w. l. o. g. the widely applied tokenization of the python \npackage NLTK (cf. https:// www. nltk. org). The goal of the \ntoken classification tasks performed by BERT is to assign \nclass labels to such tokens. For example, the second token \n‘fish’ in the tokenized sentence (‘The’, ‘fish’, ‘was’, ‘good’, \n‘!’) is assigned with the class label ASP indicating an aspect \nterm. The following postulates P1)-P3) provide the founda-\ntion for linguistic rules based on NLP building blocks, which \nenable a global reconstruction of BERT's predictions (i.e., \nthe predicted class labels for the tokens of a sentence).\nP1) “L abel assignments ”: In our approach, we assign \nlabels only to single tokens or token pairs. Hence, we do not \n2129\nGlobal Reconstruction of Language Models with Linguistic Rules\n1 3\nconsider label assignments for whole sentences, documents \nnor for single character values. This focus is promising for \nreconstructing BERT, as BERT internally also establishes \ntext representations on a token level.\nP1.1) “tag label  assignments ”: A tag building block \ntbb∈ TBB  (where TBB is the set of tag building blocks) \nassigns at most one tag label tbb(ti)∈ L tbb to a token ti ( L tbb \nis the set of all labels from tbb ). For instance, the tag build-\ning block POS with LPOS ={ NN , VB, JJ, …} assigns the \nlabel POS /parenleft.s1t2\n/parenright.s1== NN  (= ‘noun’) to the token t2 =‘fish’ in \nthe exemplary sentence above.\nP1.2) “ Relation  label  assignments ”: A relation \nbuilding block rbb∈ RBB  (where RBB is the set of rela-\ntion building blocks) assigns at most one relation label  \nrbb(t i, tj)∈L rbb  to a token pair \n/parenleft.s1ti , tj\n/parenright.s1\n ( Lrbb is the set of all \nlabels from rbb ). For example, the relation building block \nDEP with LDEP ={ amod, nsubj, …} assigns the label \nDEP /parenleft.s1t2 ,t4\n/parenright.s1== nsubj (= ‘nominal subject’) to the token \npair \n/parenleft.s1t2 ,t4\n/parenright.s1\n = (‘fish’, ‘good’). In particular, relation building \nblocks enable to capture contextual information in a sen-\ntence, which is a main component of language models such \nas BERT.\nP1.3) “Class label assignments”: BERT assigns a class \nlabel l/u1D70F\n/parenleft.s1ti\n/parenright.s1∈ L /u1D70F to each token ti ( L/u1D70F is the set of all class \nlabels in a token classification task /u1D70F ). For instance, in the \naspect term detection task with class labels \nLASP =\n/braceleft.s2\nASP, ASP\n/braceright.s2\n , the token t2 =‘fish’ is assigned with \nthe class label ASP by BERT indicating that ‘fish’ is an \naspect term.\nP2) “Feasible  aRguments  FoR Rules”: In our approach, \nfeasible arguments in the antecedent and consequents of a \nrule only reference to labels for tokens or token pairs as \npostulated in P1).\nP2.1) “Feasible aRguments in Rule antecedents”: A fea-\nsible argument in the rule antecedent only contains condi-\ntions regarding tag labels of tokens (cf. P1.1)) and relation \nlabels of token pairs (cf. P1.2)).\nP2.2) “Feasible aRguments in Rule consequents”: A fea-\nsible argument in the rule consequent only contains class \nlabel assignments of tokens (cf. P1.3). Considering the clas-\nsification task of sentiment term detection, the argument \nlSENT\n/parenleft.s1t4\n/parenright.s1→ SENT  assigns the class label SENT to the token \nt4 =‘good’, indicating that ‘good’ is labelled as a sentiment \nterm by BERT in the sentence ‘The fish was good!’.\nP3) “C onFlicting  classi Fication  Results  oF multiple  \nRules”: Multiple rules R1 ,…,RnR\n ( nR ∈ ℕ ) may result in \nconflicting classification results l1\n/u1D70F\n/parenleft.s1ti\n/parenright.s1,…,lnR\n/u1D70F\n/parenleft.s1ti\n/parenright.s1∈ L /u1D70F for \nthe same token ti . To resolve such conflicting classification \nresults for a token ti , it is sensible to assign the class of the \nrule with the highest precision (cf. next section).\nGiven the postulates P1)-P3), the structure of linguistic \nrules can be defined. A linguistic rule R is an “if–then-else” \nrule in the form of IF antecedent THEN “then”-consequent \n(ELSE “else”-consequent). Here, the antecedent is an arbi-\ntrary combination of feasible arguments as postulated in \nP2.1) by means of logical operators such as AND (i.e., “ ∧”), \nOR (i.e., “ ∨ ”) and NOT (i.e., “ ¬”). Further, each “then”-con-\nsequent and each “else”-consequent consists of one feasible \nargument as postulated in P2.2). Thus, a rule R outputs the \nclass assignments of the “then”-consequent in case that the \nantecedent is tRue (otherwise and if an “else”-consequent is \ncontained in the rule, it outputs the class assignments of the \n“else”-consequent). Moreover, rules can be characterized by \ntheir length, which is given by the number of tokens that are \nconnected by a relation building block in the antecedent of \na rule. A brief example of a rule of length two is given by:\nIF /parenleft.s1/bracketleft.s1POS /parenleft.s1ti\n/parenright.s1== NN /bracketright.s1∨¬ /bracketleft.s1POS /parenleft.s1tj\n/parenright.s1== VB /bracketright.s1/parenright.s1∧ /bracketleft.s1DEP /parenleft.s1ti, tj\n/parenright.s1== nsubj/bracketright.s1\nTHEN lASP\n/parenleft.s1ti\n/parenright.s1→ ASP\nThis rule can be applied to the tokenized sentence (‘The’, \n‘fish’, ‘was’, ‘good’, ‘!’) from above. For this sentence, the \nantecedent of the rule is only tRue if ti = t2 =‘fish’ and \ntj = t4 =‘good’. For any other selection of ti and tj , the ante-\ncedent is False since only the token pair (‘fish’, ‘good’) has \nthe relation “nsubj” in this sentence. Hence, this linguistic \nrule correctly detects the aspect term ‘fish’. Rules of the \noutlined formal structure based on the postulates P1)-P3) \nconstitute the foundation for our approach for reconstruct-\ning BERT.\nAssessing fidelity and comprehensibility of global \nreconstructions\nTo globally reconstruct BERT, all predictions of BERT for a \ntoken classification task have to be considered. Here, fidelity \nand comprehensibility are the most relevant measures (cf. \nSection “Types of XAI approaches in the context of lan-\nguage models”) and assessing both measures is required to \nanalyze the trade-off between fidelity and comprehensibility. \nSince we focus on global reconstructions of language mod-\nels, we outline in detail how both measures can be assessed \nfor global reconstructions in the following.\nTo measure fidelity, we consider the predictions of BERT \nfor each class label. More precisely, the set of token ids (i.e., \nthe positions of tokens in the text corpus) predicted by BERT \nas class C ∈ L/u1D70F is given by IC ,BERT = /braceleft.s1i∈ I/uni007C.varlBERT\n/parenleft.s1ti\n/parenright.s1= C /braceright.s1 , \nwhere I is the set of all token ids. These token ids IC,BERT  \nare used as the basis for extracting the linguistic rules on \ntraining data Itrain,C ,BERT  and validation data Ivalidation,C ,BERT  as \nwell as for assessing their fidelity of globally reconstructing \nBERT on test data Itest,C ,BERT  . Once a set Σ of linguistic rules \nis extracted, the F1 score is appropriate to assess the fidel-\nity of the rule set (Sushil et al., 2018) as - in contrast to the \naccuracy measure - it accounts for imbalanced class distribu-\ntions. The F1 score (i.e., based on precision and recall) of \n2130 M. Binder et al.\n1 3\nthe rule set Σ for reconstructing BERT’s predictions IC,BERT  \nis given by:\nHere, Itest,C ,Σ = /braceleft.s1i ∈ Itest/uni007C.varlΣ\n/parenleft.s1ti\n/parenright.s1== C /braceright.s1 is the set of token \nids from the test data that are assigned with class C by the \nrule set Σ . In case of multiclass classification the fidelity \nis then assessed by the average F1 score per class label \nC , denoted as F1(Σ) (i.e., by the macro-averaged F1 score \n(Sushil et al., 2018)). In contrast to the regular formulas for \nclassifier evaluation, which aim to evaluate the predictions \nof a classifier regarding the true class labels, the formulas \n(1)-(3) enable to evaluate the linguistic rules regarding the \npredicted class labels by BERT and hence, to assess the \nfidelity of reconstructing BERT by certain sets of linguistic \nrules Σ.\nIn contrast to the comprehensibility of local reconstruc-\ntions (e.g., complexity of single rules), literature suggests to \nassess the comprehensibility of a global reconstruction by \nits model size (Guidotti et al., 2018 ). Since our model is a \nset of rules Σ , both the number of rules NR(Σ) in the rule set \nand the number of unique argument values NUAV(Σ) in the \nantecedents in the rule set (Vilone & Longo, 2021) deter -\nmine its comprehensibility. These measures are given by:\nHere, AAV = LPOS ∪ LSYN ∪ LDEP ∪ LSRL ∪ LCOREF ∪ LPROX is the \nset of all argument values of all NLP building blocks. For \nboth measures, a lower value indicates higher comprehen-\nsibility. That is, we leverage two different measures which \ncapture two important perspectives on comprehensibility. \nOverall, based on the measures (1) – (5) the fidelity and \ncomprehensibility of global reconstructions can be assessed.\nAnalysis\nIn this section we analyze the reconstruction of BERT’s pre-\ndictions by our approach. First, we outline the selected tasks, \ndatasets and the conducted automated extraction of linguis-\ntic rules for global reconstruction. Then, we demonstrate \nhow our approach based on linguistic rules can reconstruct \n(1)PrC (Σ) =\n/uni007C.x/uni007C.xItest,C ,BERT ∩ Itest,C ,Σ/uni007C.x/uni007C.x\n/uni007C.x/uni007C.xItest,C ,Σ/uni007C.x/uni007C.x\n(2)RecC (Σ) =\n/uni007C.x/uni007C.xItest,C ,BERT ∩ Itest,C ,Σ/uni007C.x/uni007C.x\n/uni007C.x/uni007C.xItest,C ,BERT /uni007C.x/uni007C.x\n(3)F1C (Σ) = 2 ∗ PrC (Σ) ∗ RecC (Σ)\nPrC (Σ) + RecC (Σ)\n(4)NR(Σ) = /uni007C.varΣ/uni007C.var\n(5)NUAV(Σ) = /uni007C.var{v∈ AAV/uni007C.var∃R∈Σ∶v ∈ R}/uni007C.var\npredictions of BERT. After that, we present and discuss the \nresults as well as implications for research and practice.\nTask selection, data preparation and rule extraction\nFor a meaningful analysis of the reconstruction of BERT’s \npredictions, we selected the NLP tasks aspect term detection \nand sentiment term detection as these tasks are frequently \nanalyzed in the IS field and constitute common applications \nfor BERT and text analytics (Dai and Song, 2019; Sun et al., \n2019; H. Xu et al., 2019), in particular in electronic markets \n(Chatterjee et al., 2021; Steur et al., 2022). Also, we chose \ntwo publicly available datasets that exhibit different charac-\nteristics – with restaurants reviews from the platform Yelp  \n(Yelp Dataset Challenge; cf. https:// www. yelp. com/ datas et) \nas experience goods vs. laptop reviews from the platform \nAmazon (Ni et al., 2019) as search goods – to enable broader \ninsights independent of specific item domains. To extract lin-\nguistic rules based on the formal structure postulated in the \nprevious section, we used state-of-the-art toolkits for annotat-\ning both datasets with the NLP building blocks discussed in \nSection “NLP building blocks” and leveraged and extended \nrule generation and rule selection techniques from the litera-\nture. The following paragraphs provide more details.\nThe goal of aspect term detection and sentiment term \ndetection is to classify tokens in online consumer reviews \nthat express aspects or sentiments. An aspect term (e.g., \n‘laptop screen’) represents an item aspect for which an opin-\nion polarity is expressed by a sentiment term (e.g., ‘very \ngood’) (Sun et al., 2019). The task of token classification is \nto assign a class label C ∈ L/u1D70F (i.e., LASP =\n/braceleft.s2\nASP, ASP\n/braceright.s2\n and \nLSENT =\n/braceleft.s2\nSENT, SENT\n/braceright.s2\n ) to tokens of a sentence. To con-\nduct aspect term detection and sentiment term detection, we \nused the publicly available state-of-the-art language model \nBERT. In particular, we used pre-trained BERT models, \nwhich were specifically adapted to the domains of restaurant \nreviews and laptop reviews, respectively (H. Xu et al., 2019). \nWe fine-tuned these BERT models for the tasks aspect term \nand sentiment term detection on both domains using the \npublicly available, labeled dataset SemEval2014 provided \nby Fan et al. (2019). After that, the fine-tuned BERT models \nwere used in this work to predict aspect terms and sentiment \nterms in the two review datasets. That is, the tokens of both \nreview datasets were assigned with the class labels of \nBERT’s predictions. An overview of the (randomly sampled) \ndataset excerpts used for analysis, including the predictions \nof BERT regarding both tasks, is given in Table 2.\nFor annotation of NLP building blocks on these data -\nsets, we used the state-of-the-art toolkits Stanza (Qi et al., \n2020) and AllenNLP (Gardner et al., 2018) as well as the \nlexical database WordNet (Fellbaum, 2013). More pre-\ncisely, POS tags and DEP relations were annotated based \n2131\nGlobal Reconstruction of Language Models with Linguistic Rules\n1 3\non Stanza, SRL relations and COREF relations based on \nAllenNLP; PROX relations were directly tangible and \nSYN tags could be extracted from WordNet.\nTo prepare the datasets for the analysis, we randomly \nsplit the sentences of the datasets into 65% training data, \n15% validation data and 20% test data. Then, the extraction \nof linguistic rules comprises two steps. Firstly, automated \nrule generation determines linguistic rules that appear at \nminimum ten times in the training data to avoid rules that \nare only applicable for very few and highly specific sen-\ntences. Secondly, the rule selection assembles a subset of \nthese linguistic rules by iteratively adding rules to a (ini-\ntially empty) rule set if the F1 score of the rule set is thereby \nenhanced on the validation data (Q. Liu et al., 2015).\nTo conduct the extraction of rules, we used and extended \nexisting techniques for automated rule generation (Dai and \nSong 2019) and automated rule selection (Q. Liu et al., 2015) \nto enable an integration and combination of different NLP \nbuilding blocks. That is, Dai and Song (2019) proposed a rule \ngeneration algorithm for aspect and sentiment term extraction \nbased on POS tags and DEP relations, which we extended to \nallow for further NLP building blocks – including the com-\nbination of different NLP building blocks – in a single rule. \nBased on further extensions to allow for an evaluation of \nthese rules and the generated rule sets by means of precision \nand recall on validation data, the automated rule selection \napproach of Q. Liu et al. (2015) could be applied.\nThen, we assessed the F1 score of the extracted set of \nlinguistic rules on the test data. To assess comprehensibility \nof the extracted rules, we focused on rules with antecedents \ncontaining at most two arguments regarding tag building \nblocks and at most one argument regarding a relation build-\ning block. Hence, the rules are of at most length two. In that \nline, we only used the logical operator “AND” to preserve \ncomprehensibility (Askira-Gelman, 1998).\nWe made the annotated datasets and our source code \navailable at https:// github. com/ BertR ules/ Global_ recon struc \ntion_ of_ langu age_ models_ with_ lingu istic_ rules.\nDemonstration of reconstructing BERT’s predictions \nwith linguistic rules\nBefore discussing the results based on the introduced data-\nsets and tasks, we give a brief preliminary demonstration of \nhow our approach based on linguistic rules can be utilized \nto reconstruct predictions of BERT. Thereby, we consider \nthe following three exemplary sentences of real restaurant \nreviews and highlight the extracted sentiment terms of BERT \nby bold font: “The Homeburger was huge.”, “Moreover, John \nis friendly and welcoming. ”, “Overall, the BurgerBarn is \namazing.”. A linguistic rule proposed by our approach that \nreconstructs these predicted sentiment terms is given by:\nIF /bracketleft.s1POS /parenleft.s1ti\n/parenright.s1== NNP /bracketright.s1∧ /bracketleft.s1POS /parenleft.s1tk\n/parenright.s1== JJ/bracketright.s1∧ /bracketleft.s1DEP /parenleft.s1ti, tk\n/parenright.s1== nsubj/bracketright.s1\nTHEN lSENT\n/parenleft.s1tk\n/parenright.s1→ SENT\nThis single rule detects the adjectives (“JJ”), which are \nin a nominal subject relation (“nsubj”) with a proper noun \n(“NNP”), as sentiment terms. The application of this rule for \nthe three sentences is given in Table 3.\nAs illustrated in Table  3, the rule reconstructs the senti-\nment terms detected by BERT in these example sentences \nand constitutes a generalizing, plausible rule, which is \nimportant for online consumer reviews, as special product/\nservice names or attributes (e.g., special dishes or waiters \nin restaurant reviews) are often referenced by proper nouns. \nOverall, this rule alone already reconstructs around 350 sen-\ntiment terms in the restaurant dataset with a precision of \n89% with respect to BERT’s predictions. In contrast, recon-\nstructing these sentiment terms by means of rules with spe-\ncific tokens instead of NLP tag building blocks, a separate \nrule for each instantiation in Table  3 would be required for \neach of the sentiment terms. For instance, the rule.\nIF\n \n/bracketleft.s1TOKEN /parenleft.s1ti\n/parenright.s1== John/bracketright.s1∧ /bracketleft.s1TOKEN /parenleft.s1tk\n/parenright.s1== friendly/bracketright.s1∧ /bracketleft.s1DEP /parenleft.s1ti, tk\n/parenright.s1== nsubj/bracketright.s1\nTHEN lSENT\n/parenleft.s1tk\n/parenright.s1→ SENT\nis obviously highly specific and cannot reconstruct the \nsentiment terms ‘huge, ‘welcoming’ or ‘amazing’. There-\nfore, this example emphasizes that linguistic rules with NLP \nbuilding blocks enable to achieve higher generalizability for \nTable 2  Datasets for analysis\nDataset characteristic Restaurants (Yelp reviews) Laptops (Amazon reviews)\n# of sentences 150,000 150,000\n# of tokens 2,320,726 2,575,492\n# of predicted aspect tokens by BERT 230,505 236,692\n# of predicted sentiment tokens by BERT 186,204 166,109\nRelative frequency of predicted aspect \ntokens by BERT (relative to # of tokens or \n# of sentences)\n0.099 (rel. to tokens); 1.537 (rel. to sentences) 0.092 (rel. to tokens); 1.578 (rel. to sentences)\nRelative frequency of predicted sentiment \ntokens by BERT (relative to # of tokens or \n# of sentences)\n0.080 (rel. to tokens); 1.241 (rel. to sentences) 0.064 (rel. to tokens); 1.107 (rel. to sentences)\n2132 M. Binder et al.\n1 3\na reconstruction of the predictions of language models (e.g., \nin online consumer reviews).\nResults\nIn this section, we present the results of the proposed \napproach for the reconstruction of BERT’s predictions. In \nparticular, we analyze the fidelity and the comprehensibility \nto which an extracted set of rules is able to globally recon-\nstruct BERT. As outlined in the Section “Assessing fidelity \nand comprehensibility of global reconstructions” in detail, \nwe determined the fidelity by the F1 score between the token \nclassification of the linguistic rules and BERT’s predic-\ntions and assessed comprehensibility by NR and NUAV . To \naccount for the objectives of high fidelity and high compre-\nhensibility, we consider four different setups of (low vs. high) \nrule complexity and (low vs. high) rule generalizability: To \nanalyze rule complexity, we distinguish between “L1-rules” \ncontaining rules of length one and “L2-rules” compris-\ning rules of length at most two (i.e., every L1-rule is also \na L2-rule, but not vice versa). We point out that L2-rules \ncontain relation labels and thus consider contextual informa-\ntion, while this is not possible for L1-rules. To analyze rule \ngeneralizability, we compare “rules with specific tokens” as \narguments (low generalizability) against “rules with (only) \nNLP building blocks” (high generalizability). Given this, the \ncomprehensibility of the four setups is shown in the Tables 4, \n5, 6 and 7 regarding both tasks on the respective datasets.\nDiscussion of the results\nWe elaborate on the major findings of applying our approach \nfor global reconstruction of language model predictions by \ndiscussing the results related to the research questions RQ1 \nand RQ2:\nAd RQ1: Our approach based on linguistic rules allows \nfor the global reconstruction of language model predic-\ntions (e.g., in online consumer reviews).\nOur analysis shows that the predictions of BERT can be \nglobally reconstructed by our approach with a fidelity of \n78%-82% based on L2-rules with tokens (i.e., rules with \nhigh complexity and low generalizability) on the consid-\nered tasks for online consumer reviews (cf. Tables  5 and \n7). In more detail, the recall of these global reconstruc-\ntions (i.e., how many classified tokens of BERT could be \nreconstructed) ranges between 79%-83%, while the preci-\nsion ranges between 76%-83%. This shows that incorpo -\nrating relation building blocks by means of rules of length \ntwo, which enables capturing contextual information, is \nindeed helpful to globally reconstruct BERT’s predictions \nwith higher fidelity. In comparison, the rule sets with NLP \nbuilding blocks (i.e., rules with high generalizability) yield \nhigher comprehensibility, which is indicated by low num-\nbers of unique argument values (298 at most) compared to \nover 165,000 classified tokens by BERT. At the same time, \nTable 3  Application of a linguistic rule to reconstruct BERT’s predictions in exemplary sentences\nExample sentence Application of the above linguistic rule\n“The Homeburger was huge.” IF \n/bracketleft.s1POS (Homeburger) == NNP /bracketright.s1∧ /bracketleft.s1POS (huge) == JJ/bracketright.s1∧ /bracketleft.s1DEP (Homeburger, huge) == nsubj/bracketright.s1\nTHEN lSENT (huge) → SENT\n“Moreover, John is friendly and welcom-\ning.”\nIF [POS (John) == NNP ] ∧ /bracketleft.s1POS (f riendly) == JJ/bracketright.s1∧ /bracketleft.s1DEP (John,f riendly) == nsubj/bracketright.s1\nTHEN lSENT (f riendly) → SENT\n“Moreover, John is friendly and welcom-\ning.”\nIF [POS (John) == NNP ] ∧ /bracketleft.s1POS (welcoming) == JJ/bracketright.s1∧ /bracketleft.s1DEP (John,welcoming) == nsubj/bracketright.s1\nTHEN lSENT (welcoming) → SENT\n“Overall, the BurgerBarn is amazing.” IF /bracketleft.s1POS (BurgerBarn) == NNP /bracketright.s1∧ /bracketleft.s1POS (amazing) == JJ/bracketright.s1∧ /bracketleft.s1DEP (BurgerBarn,amazing) == nsubj/bracketright.s1\nTHEN lSENT (amazing) → SENT\nTable 4  Comprehensibility of the global reconstruction of BERT’s predictions for aspect term detection measured by NR ; NUAV\nAspect term detection Restaurants (Yelp reviews) Laptops (Amazon reviews)\nLow generalizability (i.e., \nrules with specific tokens)\nHigh generalizability (i.e., \nrules with NLP building \nblocks)\nLow generalizability (i.e., \nrules with specific tokens)\nHigh generalizability (i.e., \nrules with NLP building \nblocks)\nLow complexity (i.e., \nL1-rules)\n1,169; 1,169 27; 27 944; 944 35; 35\nHigh complexity (i.e., \nL2-rules)\n9,770; 2,565 2,791; 237 9,004; 2,201 2,718; 298\n2133\nGlobal Reconstruction of Language Models with Linguistic Rules\n1 3\nthese rule sets also maintain fidelities of up to 70%. Here, \nit could be substantiated that the reconstruction of BERT’s \npredictions is constituted by transparent and well-general-\nizing rules. For instance, the rule “IF a term is a synset of \n‘good’ and an adjectival modifier (DEP-relation ‘amod’) of \na noun (POS-tag ‘NN’), THEN that token is labelled as \na sentiment term by BERT.” achieved 99% precision and \nenables to reconstruct over 1,000 sentiment terms in the \nrestaurant dataset. Furthermore, in an additional analysis, \nno discriminating factors such as specific synsets regarding \ngender, origin or neglected negative sentiments for specific \nproducts/services were detected. In total, this yields that our \nglobal reconstruction approach by means of linguistic rules \nis suitable to support algorithmic auditing including valida-\ntion checks in application scenarios such as discussed in the \nintroduction (AS1-4).\nAd RQ2: Our approach enables to establish a balanced \ntrade-off between fidelity and comprehensibility.\nAs the proposed linguistic rules allow to vary their rule \ncomplexity (e.g., L2-rules vs. L1-rules) and their rule gener-\nalizability (e.g., rules with NLP tag building blocks vs. spe-\ncific tokens), it is possible to create setups for global recon-\nstructions with different comprehensibility (cf. Tables  4 \nand  6). Our analysis of these setups shows that higher \nfidelity is achieved by reducing comprehensibility and vice \nversa. This yields that fidelity and comprehensibility are \ntwo conflicting objectives, which has also been a topic of \ndiscussion in general XAI literature (Arrieta et al., 2020; \nGilpin et al., 2018). Indeed, the reconstruction by means of \nlinguistic rules can either have a higher fidelity or a higher \ncomprehensibility, while both objectives cannot be achieved \nTable 5  Fidelity of the global reconstruction of BERT’s predictions for aspect term detection\nFidelity is measured by F1 Score (numbers in parentheses indicate Precision, Recall)\nAspect term detection Restaurants (Yelp reviews) Laptops (Amazon reviews)\nLow generalizability (i.e., \nrules with specific tokens)\nHigh generalizability (i.e., \nrules with NLP building \nblocks)\nLow generalizability (i.e., \nrules with specific tokens)\nHigh generalizability (i.e., \nrules with NLP building \nblocks)\nLow complexity (i.e., \nL1-rules)\n75.0% (75.5%,74.5%) 58.4% (44.1%,86.1%) 78.0% (80.1%,76.0%) 53.4% (38.9%,85.0%)\nHigh complexity (i.e., \nL2-rules)\n78.2% (75.6%,81.1%) 66.0% (56.9%,78.6%) 82.4% (82.1%,82.8%) 62.8% (55.3%,72.5%)\nTable 6  Comprehensibility of the global reconstruction of BERT’s predictions for sentiment term detection measured by NR ; NUAV\nSentiment term detection Restaurants (Yelp reviews) Laptops (Amazon reviews)\nLow generalizability (i.e., \nrules with specific tokens)\nHigh generalizability (i.e., \nrules with NLP building \nblocks)\nLow generalizability (i.e., \nrules with specific tokens)\nHigh generalizability (i.e., \nrules with NLP building \nblocks)\nLow complexity (i.e., \nL1-rules)\n757; 757 15; 15 700; 700 21; 21\nHigh complexity (i.e., \nL2-rules)\n5,627; 1,615 1,787; 258 5,491; 1,434 1,973; 288\nTable 7  Fidelity of the global reconstruction of BERT’s predictions for sentiment term detection\nFidelity is measured by F1 Score (numbers in parentheses indicate Precision, Recall)\nSentiment term detection Restaurants (Yelp reviews) Laptops (Amazon reviews)\nLow generalizability (i.e., \nrules with specific tokens)\nHigh generalizability (i.e., \nrules with NLP building \nblocks)\nLow generalizability (i.e., \nrules with specific tokens)\nHigh generalizability (i.e., \nrules with NLP building \nblocks)\nLow complexity (i.e., \nL1-rules)\n76.8% (83.8%,70.9%) 65.9% (60.1%,72.8%) 75.0% (77.8%,72.3%) 57.8% (50.2%,68.1%)\nHigh complexity (i.e., \nL2-rules)\n81.7% (83.1%,80.4%) 69.9% (66.8%,73.2%) 79.1% (79.5%,78.6%) 65.7% (65.8%,65.6%)\n2134 M. Binder et al.\n1 3\nsimultaneously. In particular, our results show that L1-rules \nwith NLP building blocks, which have low complexity and \nhigh generalizability, yield the global reconstruction with \nthe highest comprehensibility (i.e., NR and NUAV are at \nmost 35; cf. Tables 4 and 6) in comparison. These rule sets \nachieve fidelities between 53 to 66% (cf. Tables  5 and 7 ). \nThis means that BERT’s predictions on the tasks of aspect \nterm detection and sentiment term detection can already be \npartly reconstructed in a very comprehensible manner with a \nsmall set of rules of only one tag building block as argument. \nConversely, when utilizing specific tokens instead of NLP \nbuilding blocks as arguments in L1-rules, a higher fidelity \nof 75% to 78% is achieved (cf. rules with low generaliz-\nability in Tables  5 and 7 ). However, such rules (e.g., the \nrule “flavorful is a sentiment term”) are highly specific and \nhave low generalizability, which results in rule sets with at \nleast 700 rules and unique argument values in the anteced-\nents (cf. Tables 4 and 6). Furthermore, Tables 5 and Table 7 \nindicate that the fidelity increases when the rules become \nmore complex, but this is accompanied by a decreasing com-\nprehensibility as indicated in Tables 4 and 6. Here, L2-rules \nwith NLP building blocks achieve fidelities between 63 to \n70% (cf. Tables 5 and 7) with at most 298 unique argument \nvalues. Contrarily, L2-rules with tokens achieve the high-\nest fidelities with values from 78% up to 82% (cf. Tables  5 \nand 7), but they exhibit the lowest generalizability and thus, \nglobal reconstructions with low comprehensibility which \nis indicated by multiple thousands of rules and between \nabout 1,400 and 2,600 unique argument values (cf. Tables 4 \nand 6) in the rule sets. These different setups show that either \nhigher fidelity or higher comprehensibility can be achieved \nby reconstructing BERT’s predictions with linguistic rules. \nHowever, if both objectives are crucial and focused equally, \nthe best setup may be L2-rules with NLP building blocks, \nwhich exhibit decent fidelity and comprehensibility at the \nsame time. The advantage of these L2-rules compared to \nL1-rules with tokens is the much lower number of unique \nargument values, which is based on the higher generalizabil-\nity of NLP building blocks compared to specific tokens, and \nin particular, the use of contextual information in form of \nrelation building blocks. Overall, our linguistic rules enable \nto establish different relevant setups with respect to fidelity \nand comprehensibility depending on the requirements for an \nXAI approach in practice.\nImplications for research and practice\nOur work contributes to the comprehensibility of opaque \nAI models in text analytics, as it allows for comprehensi-\nble global reconstructions of language models. Therefore, \nour work is not only valuable for multiple different research \nstrands, but it is also highly relevant for applications and \nsupports the adoption of language models, as outlined in \nthe following.\nImplications for research\n1) Linguistic rules enable global reconstructions of high \nfidelity for language model predictions in text analytics.\nExisting literature on XAI (e.g., Arrieta et al., 2020) dis-\ncusses that rule-based XAI models can exhibit high compre-\nhensibility but tend to lack high fidelity for reconstructions \nof complex AI models. Our findings extend this existing \nbody of knowledge, as our analysis shows that our approach \nbased on linguistic rules enables reconstructions with higher \nfidelity as well as reconstructions with higher comprehensi-\nbility for language model predictions. In particular, linguis-\ntic rules can achieve high fidelity by means of the contained \nrelation building blocks capturing contextual information \nwhich is relevant for many text analytics tasks (Devlin et al., \n2019; Geng et al., 2021; Peters et al. 2018a). As both, a \nglobal reconstruction approach by linguistic rules and an \nanalysis of the trade-off between fidelity and comprehen-\nsibility thereby, do not exist in the field of text analytics so \nfar, we extend the existing body of knowledge for rule-based \nXAI approaches.\n2) Global reconstruction by means of linguistic rules paves \nthe way for a thorough understanding of language mod-\nels.\nIn contrast to the existing body of knowledge from local \nreconstruction approaches, the proposed approach based on \nlinguistic rules enables a global reconstruction of language \nmodels (cf. Section “Discussion of the results”). Hence, \nlinguistic rules constitute a first step for global and thor -\nough understanding of these black boxes, which cannot \nbe achieved by local reconstruction approaches (cf. Sec-\ntion “Introduction”). With linguistic rules as vital instru-\nment, researchers in the field of XAI can now focus on how \nto thoroughly justify predictions of language models for \ntext analytics tasks (e.g., by leveraging tests of statistical \nsignificance for linguistic rules in a global reconstruction \nfor language model predictions). Moreover, researchers \ncan aim to improve language models in text analytics tasks \nbased on our approach. That is, our approach could be used \nto additionally reconstruct and analyze false predictions of \nlanguage models, to detect its flaws and by means of that, to \nenhance these language models. Furthermore, an analysis \nof linguistic rules reconstructing a language model’s pre-\ndictions could enable to derive deeper insights regarding \neffects of different types of review texts (e.g., reviews for \nsearch goods vs. experience goods or reviews of different \nconsumer segments). That is, such analyses could support \n2135\nGlobal Reconstruction of Language Models with Linguistic Rules\n1 3\nto analyze whether language model predictions for reviews \nof different review types vary in the NLP building blocks \ncontained in the rules for global reconstruction. In particu-\nlar, our approach allows for assessing the contribution of \nspecific NLP building blocks to global reconstructions of \nlanguage model predictions, which supports enhancing the \nunderstanding and use of language model predictions in text \nanalytics.\n3) Global reconstructions help to understand language \nmodel-detected features.\nOur work also has implications for other research \nstrands such as text analytics of online consumer reviews \nregarding star ratings (e.g., Binder et al., 2019; Goeken \net al., 2020; Heinrich et al. 2021) or review helpfulness \n(e.g., Yin et al., 2014). Here, many IS researchers aim at \nanalyzing and explaining the relations between (aspect-\nbased) sentiments and a target variable (e.g., star ratings \nor review helpfulness). To enable such analyses, it is nec-\nessary to extract high-quality features from large review \ndatasets by means of state-of-the-art language models in \na first step (e.g., to extract aspect-based sentiments from \nreview texts). Similar as in the practical application sce-\nnarios (AS1-4), it is also vital for researchers to base their \nanalyses and insights on reliable and comprehensible fea-\ntures. Hence, a comprehensible global reconstruction of \nlanguage model predictions detecting such features may \nfurther enable a better understanding of the target variable \nbased on the review texts as it reduces the opacity of the \nfeature detection in the first step of such analyses of online \nconsumer reviews. That is, our approach can help to shed \nlight on black-box language models used for feature extrac-\ntion in IS text analytics research.\nImplications for practice\n1) Global reconstructions with high comprehensibility can \nimprove acceptance of language models, and support \ntheir adoption in practice.\nThe language model BERT is already used in various \napplications (cf. AS1-AS4 in Section “Introduction”). Here, \nreconstructions with higher comprehensibility by means \nof our approach can help to shed light on language model \npredictions in these applications, and thereby, to improve \nacceptance of such models. In particular, a reconstruction \nby our approach allows to verify that a language model \napplied in an electronic marketplace does not discriminate \nagainst specific groups. For instance, when online consumer \nreviews are analyzed (cf. AS1), our approach can help to \nprevent that specific groups of consumers are discriminated \nagainst (e.g., by assigning a negative sentiment to certain \ncountries, ethnicities or genders). In text analytics-assisted \nrecruitment processes (cf. AS4), the rules provided by the \npresented approach can be examined whether they contain \narguments regarding gender or other discriminating attrib-\nutes (detected by particular synsets) indicating undesired \nbiases or discriminations. Similarly, our approach helps to \nreconstruct and justify BERT’s predictions in chatbots (AS2) \nand finance applications (AS3). Further, the rules provided \nby our approach support an algorithmic auditing based on \nthe GDPR and thus to comply with regulatory requirements. \nFor instance, a data scientist has to be able to show that the \ndata processing is fair according to the GDPR, which can \nbe supported by analyses with respect to discriminations as \noutlined above. This is especially relevant since algorithmic \nauditing will likely become the gold standard for companies \ndeploying AI models (Casey et al., 2019).\n2) Linguistic rules enable different relevant setups with \nrespect to the trade-off between fidelity and compre -\nhensibility depending on the requirements of different \nstakeholders for XAI approaches in practice.\nOur approach based on linguistic rules is particularly \npromising, as it enables to establish different setups with \nrespect to the trade-off between fidelity and comprehensi-\nbility, allowing for more profound analyses (Gilpin et al., \n2018). That is, reconstructions with higher fidelity might be \nleveraged by data scientists to analyze language model pre-\ndictions in detail. In addition, domain experts might lever -\nage reconstructions with higher comprehensibility to assess, \ndisclose and communicate the justifications (e.g., of BERT’s \naspect term detection) in a given domain. In particular, AI \ntext analytics models in practice can thus be analyzed with \ndifferent setups by means of our approach, which can be \ncombined to gain more robust insights and to comply with \nregulatory requirements.\nConclusion\nGlobal reconstruction of language model predictions such \nas for the state-of-the-art model BERT is an important issue \nin both research and practice, since it can enable to justify \ndecisions based thereon in many application scenarios (e.g., \nin eCommerce or finance) and thereby allow to comply with \nnecessary algorithmic auditing. In this paper, we thus pro-\nposed a global XAI approach in text analytics for recon-\nstructing predictions of language models for token-level \nclassifications by linguistic rules. Further, we discussed \nthe trade-off between fidelity and comprehensibility for the \nglobal reconstructions. For the analysis of our approach \nand the trade-off, we considered aspect term and sentiment \nterm detection in two datasets of different domains. That \n2136 M. Binder et al.\n1 3\nis, we considered laptops as search goods and restaurants \nas experience goods. The results for both domains showed \nthat linguistic rules enable global reconstructions of higher \nfidelity for language models, which paves the way for a \nthorough understanding of language models in text analyt-\nics in the future. Further, our approach helps to understand \nlanguage model-detected features used for further analyt-\nics in research. For practical application scenarios such as \neCommerce, finance or online recruitment, our approach can \nimprove acceptance of language models and thus support \ntheir adoption in text analytics. Here, our approach also sup-\nports compliance with regulatory requirements.\nNevertheless, our research has some limitations, which \ncould be starting points for future works. In this paper, we \nfocused on the predictions of BERT without further consid-\nering the correctness of these predictions. Thus, our research \ncould also be transferred to an analysis of BERT’s predic-\ntion errors aiming towards a further enhancement of lan-\nguage models (i.e., by using linguistic rules to specifically \nreconstruct false predictions). Moreover, as we focused on \nthe tasks of aspect and sentiment detection for search and \nexperience goods in eCommerce, other NLP tasks in differ-\nent domains would be possible for examination and could \nfurther substantiate our findings. Here, our work provides \nthe necessary first step toward such insights.\nFunding Open Access funding enabled and organized by Projekt \nDEAL.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAdadi, A., & Berrada, M. (2018). Peeking inside the black-box: \nA survey on explainable artificial intelligence (XAI). IEEE \nAccess, 6, 52138–52160.  https:// doi. org/ 10. 1109/ ACCESS. \n2018. 28700 52\nAli, A., Schnake, T., Eberle, O., Montavon, G., Müller, K.-R., & Wolf, \nL. (2022). XAI for transformers: Better explanations through con-\nservative propagation. ArXiv Preprint. https:// doi. org/ 10. 48550/ \narXiv. 2202. 07304\nArrieta, A. B., Diaz-Rodriguez, N., Del Ser, J., Bennetot, A., Tabik, \nS., Barbado, A., Garcia, S., Gil-Lopez, S., Molina, D., Ben-\njamins, R., Chatila, R., & Herrera, F. (2020). Explainable \nartificial intelligence (XAI): Concepts, taxonomies, opportuni-\nties and challenges toward responsible AI. Information Fusion , \n58, 82–115. https:// doi. org/ 10. 1016/j. inffus. 2019. 12. 012\nAskira-Gelman, I. (1998). Knowledge discovery: Comprehensibility \nof the results. Proceedings of the thirty-first Hawaii international \nconference on system sciences (Vol. 5, pp. 247–255). IEEE.\nAugasta, M. G., & Kathirvalavakumar, T. (2012). Rule extraction from \nneural networks – A comparative study. International Conference \non Pattern Recognition, Informatics and Medical Engineering \n(PRIME-2012) (pp. 404–408). IEEE.\nBinder, M., Heinrich, B., Klier, M., Obermeier, A. A., & Schiller, A. \n(2019). Explaining the stars: Aspect-based sentiment analysis of \nonline customer reviews. Proceedings of the 27th European Con-\nference on Information Systems (ECIS).\nBrasoveanu, A. M. P., & Andonie, R. (2022). Visualizing and explain-\ning language models. Integrating Artificial Intelligence and \nVisualization for Visual Knowledge Discovery (pp. 213–237). \nSpringer, Cham.\nCasey, B., Farhangi, A., & Vogl, R. (2019). Rethinking explainable \nmachines: The GDPR’s \"right to explanation\" debate and the rise \nof algorithmic audits in enterprise. Berkeley Tech. LJ, 34, 143.\nChatterjee, S. (2019). Explaining customer ratings and recommenda-\ntions by combining qualitative and quantitative user generated \ncontents. Decision Support Systems, 119, 14–22. https:// doi. org/ \n10. 1016/j. dss. 2019. 02. 008\nChatterjee, S., Goyal, D., Prakash, A., & Sharma, J. (2021). Exploring \nhealthcare/health-product ecommerce satisfaction: A text mining \nand machine learning application. Journal of Business Research, \n131, 815–825. https:// doi. org/ 10. 1016/j. jbusr es. 2020. 10. 043\nCoenen, A., Reif, E., Yuan, A., Kim, B., Pearce, A., Viégas, F., & Wat-\ntenberg, M. (2019). Visualizing and measuring the geometry of \nBERT. Advances in Neural Information Processing Systems, 32.\nCoheur, L. (2020). From Eliza to Siri and beyond. International Con-\nference on Information Processing and Management of Uncer -\ntainty in Knowledge-Based Systems (pp. 29–41). Springer, Cham.\nDai, H., & Song, Y. (2019). Neural aspect and opinion term extraction \nwith mined rules as weak supervision. Proceedings of the 57th \nAnnual Meeting of the Association for Computational Linguistics \n(pp. 5268–5277). ACL.\nDanilevsky, M., Qian, K., Aharonov, R., Katsis, Y., Kawas, B., & Sen, \nP. (2020). A survey of the state of explainable AI for natural lan-\nguage processing. ArXiv Preprint. https:// doi. org/ 10. 48550/ arXiv. \n2010. 00711\nDastin, J. (2018). Amazon scraps secret AI recruiting tool that showed \nbias against women. Retrieved from https:// www. reute rs. com/ artic \nle/ us- amazon- com- jobs- autom ation- insig ht- idUSK CN1MK 08G. \nAccessed 30 Aug 2022.\nDevlin, J., Chang, M.- W., Lee, K., & Toutanova, K. (2019). BERT: \nPre-training of deep bidirectional transformers for language \nunderstanding. Proceedings of the 2019 NAACL (pp. 4171–4186). \nhttps:// doi. org/ 10. 18653/ v1/ N19- 1423\nDoran, D., Schulz, S., & Besold, T. R. (2017). What does explainable \nAI really mean? A new conceptualization of perspectives. ArXiv \npreprint. https:// doi. org/ 10. 48550/ arXiv. 1710. 00794\nFan, Z., Wu, Z., Dai, X., Huang, S., & Chen, J. (2019). Target-oriented \nopinion words extraction with target-fused neural sequence labe-\nling. Proceedings of the 2019 Conference of the North American \nChapter of the Association for Computational Linguistics: Human \nLanguage Technologies, Volume 1 (Long and Short Papers) (pp. \n2509–2518).\nFellbaum, C. (2013). Wordnet in the encyclopedia of applied linguis-\ntics. Boston: Wiley-Blackwell. https:// doi. org/ 10. 1002/ 97814  \n05198 431. wbeal 1285\nFörster, M., Hühn, P., Klier, M., & Kluge, K. (2021). Capturing users’ \nreality: A novel approach to generate coherent counterfactual \n2137\nGlobal Reconstruction of Language Models with Linguistic Rules\n1 3\nexplanations. Proceedings of the 54th Hawaii International Con-\nference on System Sciences (p. 1274).\nFörster, M., Klier, M., Kluge, K., & Sigler, I. (2020a). Evaluating \nexplainable artifical intelligence ‐What users really appreciate. \nProceedings of the 28th European Conference on Information \nSystems (ECIS).\nFörster, M., Klier, M., Kluge, K., & Sigler, I. (2020b). Fostering human \nagency: A process for the design of user-centric XAI systems. \nICIS 2020 Proceedings.\nFortune Business Insights (2021). Natural Language Processing (NLP) \nMarket size, share and Covid-19 impact analysis . Retrieved \nfrom https:// www. fortu nebus iness insig hts. com/ indus try- repor ts/ \nnatur al- langu age- proce ssing- nlp- market- 101933. Accessed 30 \nAug 2022.\nGardner, M., Grus, J., Neumann, M., Tafjord, O., Dasigi, P., Liu, N., \nPeters, M., Schmitz, M., & Zettlemoyer, L. (2018). AllenNLP: A \ndeep semantic natural language processing platform. ArXiv Pre-\nprint. https:// doi. org/ 10. 48550/ arXiv. 1803. 07640\nGeng, Z., Zhang, Y. [Yanhui], & Han, Y. (2021). Joint entity and \nrelation extraction model based on rich semantics. Neurocom-\nputing, 429, 132–140. https:// doi. org/ 10. 1016/j. neucom. 2020. \n12. 037\nGilpin, L. H., Bau, D., Yuan, B. Z., Bajwa, A., Specter, M., & Kagal, L. \n(2018). Explaining explanations: An overview of interpretability \nof machine learning. 2018 IEEE 5th International Conference on \ndata science and advanced analytics (DSAA) (pp. 80–89). IEEE.\nGoeken, T., Tsekouras, D., Heimbach, I., & Gutt, D. (2020). The rise \nof robo-reviews-The effects of chatbot-mediated review elicitation \non review valence. ECIS 2020 Proceedings.\nGuidotti, R., Monreale, A., Ruggieri, S., Turini, F., Giannotti, F., \n& Pedreschi, D. (2018). A survey of methods for explaining \nblack box models. ACM Computing Surveys (CSUR), 51(5), \n1–42. https:// doi. org/ 10. 1145/ 32360 09\nHeidari, M., & Rafatirad, S. (2020). Semantic convolutional neural \nnetwork model for safe business investment by using BERT. 2020 \nSeventh International Conference on Social Networks Analysis, \nManagement and Security (SNAMS) (pp. 1–6). IEEE. https:// doi. \norg/ 10. 1109/ SNAMS 52053. 2020. 93365 75\nHeinrich, B., Hollnberger, T., Hopf, M., & Schiller, A. (2022). Long-\nterm sequential and temporal dynamics in online consumer rat-\nings. ECIS 2022 Proceedings.\nHeinrich, B., Hopf, M., Lohninger, D., Schiller, A., & Szubartowicz, \nM. (2020). Something’s missing? A procedure for extending item \ncontent data sets in the context of recommender systems. Infor -\nmation Systems Frontiers, 24, 267–286. https:// doi. org/ 10. 1007/ \ns10796- 020- 10071-y\nHeinrich, B., Hopf, M., Lohninger, D., Schiller, A., & Szubartowicz, \nM. (2021). Data quality in recommender systems: the impact of \ncompleteness of item content data on prediction accuracy of rec-\nommender systems. Electronic Markets, 31(2), 389–409. https:// \ndoi. org/ 10. 1007/ s12525- 019- 00366-7\nHewitt, J., & Manning, C. D. (2019). A structural probe for finding \nsyntax in Word representations. Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, Volume \n1 (Long and Short Papers) (pp. 4129–4138).\nJumelet, J., & Hupkes, D. (2018). Do language models understand \nanything? On the ability of LSTMs to understand negative polarity \nitems. Proceedings of the Workshop: Analyzing and Interpreting \nNeural Networks for NLP (BlackboxNLP@EMNLP 2018) (pp. \n222–231). ACL.\nKamps, J., Marx, M., Mokken, R. J., & de Rijke, M. (2004). Using \nWordNet to measure semantic orientations of adjectives. In LREC \n(Vol. 4, pp. 1115–1118). ACL.\nKim, N., Patel, R., Poliak, A., Wang, A., Xia, P., McCoy, R. T., Ten-\nney, I., Ross, A., Linzen, T., Van Durme, B., Bowman, S. R., \n& Pavlick, E. (2019). Probing what different NLP tasks teach \nmachines about function word comprehension. Proceedings of the \nEighth Joint Conference on Lexical and Computational Semantics \n(*SEM 2019). ACL.\nKokalj, E., Škrlj, B., Lavrač, N., Pollak, S., & Robnik-Šikonja, M. \n(2021). BERT meets shapley: Extending SHAP explanations to \ntransformer-based classifiers. Proceedings of the EACL Hack -\nashop on News Media Content Analysis and Automated Report \nGeneration (pp. 16–21).\nKovaleva, O., Romanov, A., Rogers, A., & Rumshisky, A. (2019). \nRevealing the dark secrets of BERT. In EMNLP-IJCNLP (pp. \n4365–4374). ACL. https:// doi. org/ 10. 18653/ v1/ D19- 1445\nLan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, \nR. (2020). ALBERT: A Lite BERT for self-supervised learning of \nlanguage representations. Proceedings of the International Con-\nference on Learning Representations 2020 (ICLR).\nLiu, Q., Gao, Z., Liu, B., & Zhang, Y. [Yuanlin] (2015). Automated \nrule selection for aspect extraction in opinion mining. Twenty-\nFourth international joint conference on artificial intelligence.  \nAAAI.\nLiu, S., Le, F., Chakraborty, S., & Abdelzaher, T. (2021). On explor -\ning attention-based explanation for transformer models in text \nclassification. 2021 IEEE International Conference on Big Data \n(Big Data) (pp. 1193–1203). IEEE.\nLundberg, S. M., & Lee, S.-I. (2017). A unified approach to interpret-\ning model predictions. In Advances in neural information process-\ning systems, 30.\nLuo, B., Lau, R. Y. K., Li, C., & Si, Y.- W. (2022). A critical review \nof state‐of‐the‐art chatbot designs and applications. WIREs Data \nMining and Knowledge Discovery, 12(1). https:// doi. org/ 10. 1002/ \nwidm. 1434\nMalkiel, I., Ginzburg, D., Barkan, O., Caciularu, A., Weill, J., & \nKoenigstein, N. (2022). Interpreting BERT-based text similarity \nvia activation and saliency maps. Proceedings of the ACM Web \nConference 2022 (pp. 3259–3268).\nManning, C. D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S. J., & \nMcClosky, D. (2014). The Stanford CoreNLP natural language pro-\ncessing toolkit. In ACL System Demonstrations (pp. 55–60). ACL. \nRetrieved from http:// www. aclweb. org/ antho logy/P/ P14/ P14- 5010. \nAccessed 30 Aug 2022.\nNi, J., Li, J., & McAuley, J. (2019). Justifying recommendations using \ndistantly-labeled reviews and fine-grained aspects. Proceedings of \nthe 2019 Conference on Empirical Methods in Natural Language \nProcessing and the 9th International Joint Conference on Natural \nLanguage Processing (EMNLP-IJCNLP) (pp. 188–197).\nO’Donovan, J., Wagner, H. F., & Zeume, S. (2019). The value of off-\nshore secrets: Evidence from the Panama Papers. The Review of \nFinancial Studies, 32(11), 4117–4155. https:// doi. org/ 10. 1093/ \nrfs/ hhz017\nPeters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, \nK., & Zettlemoyer, L. (2018a). Deep contextualized word rep-\nresentations. Proceedings of the 2018 Conference of the North \nAmerican Chapter of the Association for Computational Linguis-\ntics: Human Language Technologies, Volume 1 (Long Papers)\n(pp. 2227–2237).\nPeters, M. E., Neumann, M., Zettlemoyer, L., & Yih, W. (2018b). Dis-\nsecting contextual word embeddings: Architecture and representa-\ntion. Proceedings of the 2018 Conference on Empirical Methods \nin Natural Language Processing. ACL.\nPotnis, A. (2018). Illuminating insight for unstructured data at scale. \nRetrieved from https:// www. ibm. com/ downl oads/ cas/ Z2ZBA Y6R. \nAccessed 30 Aug 2022.\nQi, P., Zhang, Y. [Yuhao], Zhang, Y. [Yuhui], Bolton, J., & Manning, \nC. D. (2020). Stanza: A Python natural language processing \ntoolkit for many human languages. In ACL System Demonstrations \n2138 M. Binder et al.\n1 3\n(pp. 101–108). ACL. Retrieved from https:// arxiv. org/ pdf/ 2003. \n07082. Accessed 30 Aug 2022.\nRamon, Y., Martens, D., Evgeniou, T., & Praet, S. (2020). Metafea-\ntures-based rule-extraction for classifiers on behavioral and textual \ndata. ArXiv Preprint. Accessed 30 Aug 2022.https:// doi. org/ 10. \n48550/ arXiv. 2003. 04792\nRepke, T., & Krestel, R. (2021). Extraction and representation of finan-\ncial entities from text. In S. Consoli, D. Reforgiato Recupero, \n& M. Saisana (Eds.), Springer eBook Collection. Data science \nfor economics and finance: Methodologies and applications (pp. \n241–263). Cham, Switzerland: Springer k. https:// doi. org/ 10.  \n1007/ 978-3- 030- 66891-4_ 11\nRibeiro, M. T., Singh, S., & Guestrin, C. (2018). Anchors: High-\nprecision model-agnostic explanations. Proceedings of the AAAI \nconference on artificial intelligence (Vol. 32, No. 1).\nSchiller, A. (2019). Knowledge discovery from CVs: A topic modeling \nprocedure. Proceedings of the 14th International Conference on \nbusiness informatics (Wirtschaftsinformatik).\nShrestha, Y. R., Krishna, V., & von Krogh, G. (2021). Augmenting \norganizational decision-making with deep learning algorithms: \nPrinciples, promises, and challenges. Journal of Business \nResearch, 123, 588–603. https:// doi. org/ 10. 1016/j. jbusr es. 2020. \n09. 068\nSteur, A. J., Fritzsche, F., & Seiter, M. (2022). It’s all about the text: An \nexperimental investigation of inconsistent reviews on restaurant \nbooking platforms. Electronic Markets, 32(3), 1187–1220. https:// \ndoi. org/ 10. 1007/ s12525- 022- 00525-3\nSun, C., Huang, L., & Qiu, X. (2019). Utilizing BERT for aspect-based \nsentiment analysis via constructing auxiliary sentence. Conference \nof the North American Chapter of the ACL (pp. 380–385). ACL. \nhttps:// doi. org/ 10. 18653/ v1/ N19- 1035\nSushil, M., Šuster, S., & Daelemans, W. (2018). Rule induction for \nglobal explanation of trained models. Proceedings of the 2018 \nEMNLP Workshop BlackboxNLP: Analyzing and Interpreting \nNeural Networks for NLP (pp. 82–97). ACL.\nSzczepański, M., Pawlicki, M., Kozik, R., & Choraś, M. (2021). New \nexplainability method for BERT-based model in fake news detec-\ntion. Nature Scientific Reports, 11(1), 1–13.  https:// doi. org/ 10. \n1038/ s41598- 021- 03100-6\nTenney, I., Das, D., & Pavlick, E. (2019a). Bert rediscovers the classi-\ncal nlp pipeline. Proceedings of the 57th Annual Meeting of the \nAssociation for Computational Linguistics. ACL.\nTenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., McCoy, R. T., \nKim, N., Van Durme, B., Bowman, S. R., Das, D., & Pavlick, E. \n(2019b). What do you learn from context? Probing for sentence \nstructure in contextualized word representations. International \nConference on Learning Representations 2019 (ICLR).\nVan Aken, B., Winter, B., Löser, A., & Gers, F. A. (2019). How does \nBERT answer questions? A layer-wise analysis of transformer rep-\nresentations. Proceedings of the 28th ACM International Confer-\nence on Information and Knowledge Management (pp. 1823–1832).\nVilone, G., & Longo, L. (2021). A Quantitative evaluation of global, \nrule-based explanations of post-hoc, model agnostic methods. \nFrontiers in Artificial Intelligence, 4. https:// doi. org/ 10. 3389/ \nfrai. 2021. 717899\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., & Bowman, S. \n(2018). GLUE: A multi-task benchmark and analysis platform for \nnatural language understanding. EMNLP Workshop BlackboxNLP \n(pp. 353–355). ACL. https:// doi. org/ 10. 18653/ v1/ W18- 5446\nXu, H., Liu, B., Shu, L., & Yu, P. (2019). BERT post-training for \nreview reading comprehension and aspect-based sentiment analy-\nsis. Conference of the North American Chapter of the ACL (pp. \n2324–2335). ACL. https:// doi. org/ 10. 18653/ v1/ N19- 1242\nXu, S., Barbosa, S. E., & Hong, D. (2020). BERT feature based model \nfor predicting the helpfulness scores of online customers reviews. \nIn K. Arai, S. Kapoor, & R. Bhatia (Eds.), Advances in Intelligent \nSystems and Computing. Advances in Information and Commu-\nnication (Vol. 1130, pp. 270–281). Cham: Springer International \nPublishing. https:// doi. org/ 10. 1007/ 978-3- 030- 39442-4_ 21\nYan, H., Gui, L., & He, Y. (2022). Hierarchical interpretation of neural \ntext classification. ArXiv Preprint. https:// doi. org/ 10. 48550/ arXiv. \n2202. 09792\nYang, Y., Uy, M. C. S., & Huang, A. (2020). FinBERT: A pretrained \nlanguage model for financial communications. ArXiv Preprint. \nhttps:// doi. org/ 10. 48550/ arXiv. 2006. 08097\nYin, D., Bond, S. D., & Zhang, H. (2014). Anxious or angry? Effects of \ndiscrete emotions on the perceived helpfulness of online reviews. \nMIS Quarterly, 38(2), 539–560. https:// doi. org/ 10. 25300/ MISQ/ \n2014/ 38.2. 10\nYoung, T., Hazarika, D., Poria, S., & Cambria, E. (2018). Recent trends \nin deep learning based natural language processing. IEEE Com-\nputational intelligence magazine, 13(3), 55–75. https:// doi. org/ 10. \n1109/ MCI. 2018. 28407 38\nZafar, M. B., Schmidt, P., Donini, M., Archambeau, C., Biessmann, F., \nDas, S. R., & Kenthapadi, K. (2021). More than words: Towards \nbetter quality interpretations of text classifiers. ArXiv Preprint.  \nhttps:// doi. org/ 10. 48550/ arXiv. 2112. 12444\nZhang, R., Yang, W., Lin, L., Tu, Z., Xie, Y., Fu, Z., Xie, Y., Tan, L., \nXiong, K., Lin, J. (2020). Rapid adaptation of BERT for infor -\nmation extraction on domain-specific business documents. ArXiv \nPreprint. https:// doi. org/ 10. 48550/ arXiv. 2002. 01861\nPublisher's note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8009995222091675
    },
    {
      "name": "Language model",
      "score": 0.6562761068344116
    },
    {
      "name": "Sentiment analysis",
      "score": 0.6286153793334961
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5277248620986938
    },
    {
      "name": "Security token",
      "score": 0.5107524394989014
    },
    {
      "name": "Fidelity",
      "score": 0.48516446352005005
    },
    {
      "name": "Analytics",
      "score": 0.48344478011131287
    },
    {
      "name": "Data science",
      "score": 0.48013946413993835
    },
    {
      "name": "Topic model",
      "score": 0.42766839265823364
    },
    {
      "name": "Natural language processing",
      "score": 0.4024776816368103
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I60668342",
      "name": "University of Regensburg",
      "country": "DE"
    }
  ],
  "cited_by": 14
}