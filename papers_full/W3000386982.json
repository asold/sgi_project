{
  "title": "Spatial-Temporal Transformer Networks for Traffic Flow Forecasting",
  "url": "https://openalex.org/W3000386982",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2224898071",
      "name": "Xu, Mingxing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352705226",
      "name": "Dai Wen-rui",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2358184367",
      "name": "Liu Chun-miao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1926832370",
      "name": "Gao Xing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747650734",
      "name": "Lin Weiyao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222669529",
      "name": "Qi, Guo-Jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2380955454",
      "name": "Xiong Hongkai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2606780347",
    "https://openalex.org/W2519887557",
    "https://openalex.org/W2117618130",
    "https://openalex.org/W2904449562",
    "https://openalex.org/W2579495707",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2005436527",
    "https://openalex.org/W2963984147",
    "https://openalex.org/W2560675361",
    "https://openalex.org/W2166901389",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W1903029394",
    "https://openalex.org/W2963358464",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W1989491491",
    "https://openalex.org/W1525783482",
    "https://openalex.org/W2528639018",
    "https://openalex.org/W3103720336",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2965341826",
    "https://openalex.org/W2950817888",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2004353783",
    "https://openalex.org/W2964321699",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2069929199",
    "https://openalex.org/W1982978808",
    "https://openalex.org/W1991770012"
  ],
  "abstract": "Traffic forecasting has emerged as a core component of intelligent transportation systems. However, timely accurate traffic forecasting, especially long-term forecasting, still remains an open challenge due to the highly nonlinear and dynamic spatial-temporal dependencies of traffic flows. In this paper, we propose a novel paradigm of Spatial-Temporal Transformer Networks (STTNs) that leverages dynamical directed spatial dependencies and long-range temporal dependencies to improve the accuracy of long-term traffic forecasting. Specifically, we present a new variant of graph neural networks, named spatial transformer, by dynamically modeling directed spatial dependencies with self-attention mechanism to capture realtime traffic conditions as well as the directionality of traffic flows. Furthermore, different spatial dependency patterns can be jointly modeled with multi-heads attention mechanism to consider diverse relationships related to different factors (e.g. similarity, connectivity and covariance). On the other hand, the temporal transformer is utilized to model long-range bidirectional temporal dependencies across multiple time steps. Finally, they are composed as a block to jointly model the spatial-temporal dependencies for accurate traffic prediction. Compared to existing works, the proposed model enables fast and scalable training over a long range spatial-temporal dependencies. Experiment results demonstrate that the proposed model achieves competitive results compared with the state-of-the-arts, especially forecasting long-term traffic flows on real-world PeMS-Bay and PeMSD7(M) datasets.",
  "full_text": "1\nSpatial-Temporal Transformer Networks for\nTrafï¬c Flow Forecasting\nMingxing Xu, Wenrui Dai, Member, IEEE,Chunmiao Liu, Xing Gao, Weiyao Lin, Senior Member, IEEE,\nGuo-Jun Qi, Senior Member, IEEE,and Hongkai Xiong, Senior Member, IEEE\n!\nAbstractâ€”Trafï¬c forecasting has emerged as a core component of\nintelligent transportation systems. However, it still remains an open\nchallenge for timely accurate trafï¬c forecasting, especially long-term\nforecasting, due to the highly nonlinear and dynamical spatial-temporal\ndependencies of trafï¬c ï¬‚ows. In this paper, we propose a novel paradigm\nof Spatial-Temporal Transformer Networks (STTNs) that jointly leverage\ndynamical directed spatial dependencies and long-range temporal de-\npendencies to improve the accuracy of long-term trafï¬c ï¬‚ow forecasting.\nA new variant of graph neural networks, named spatial transformer, is\npresented to dynamically model directed spatial dependencies with self-\nattention mechanism to capture real-time conditions and directions of\ntrafï¬c ï¬‚ows. Various patterns of spatial dependencies are jointly mod-\neled with multi-head attention mechanism to consider multiple factors,\nincluding similarity, connectivity and covariance. Furthermore, temporal\ntransformer is developed to model long-range bidirectional temporal de-\npendencies across multiple time steps. In comparison to existing works,\nSTTNs enable efï¬cient and scalable training for long-range spatial-\ntemporal dependencies. Experimental results demonstrate that STTNs\nare competitive with the state-of-the-arts, especially for long-term trafï¬c\nï¬‚ow forecasting, on real-world PeMS-Bay and PeMSD7(M) datasets.\nIndex Termsâ€”Trafï¬c ï¬‚ow prediction, spatial-temporal dependency, dy-\nnamic graph neural networks, transformer.\n1 I NTRODUCTION\nW\nITH the deployment of affordable trafï¬c sensor tech-\nnologies, the exploding trafï¬c data are bringing us to\nthe era of transportation big data. Intelligent Transportation\nSystem (ITS) [1] is thus developed to leverage transportation\nbig data for efï¬cient urban trafï¬c controlling and planning.\nAs a core component of ITS, accurate trafï¬c forecasting in a\ntimely fashion has attracted increasing attentions.\nIn trafï¬c forecasting, the future trafï¬c conditions (e.g.\nspeeds, volumes and density) of a node are predicted from\nthe historical trafï¬c data of itself and its neighboring nodes.\nIt is important for a forecasting model to effectively and\nefï¬ciently capture the spatial and temporal dependencies\nwithin trafï¬c ï¬‚ows. Trafï¬c forecasting is commonly classi-\nï¬ed into two scales, i.e., short-term (â‰¤30 minutes) and long-\nterm ( â‰¥30 minutes). Existing approaches like time-series\nmodels [2] and Kalman ï¬ltering [3] perform well on short-\nterm forecasting. However, the stationary assumption for\nthese models is not practical in long-term forecasting, as\ntrafï¬c ï¬‚ows are highly dynamical in nature. Furthermore,\nthey fail to jointly exploit the spatial and temporal correla-\ntions in the trafï¬c ï¬‚ows to make long-term forecasting.\nğ‰ğ‰-1ğ‰-ğŸğ‰-ğŸ‘\nğœ-3\nğœ-2\nğœ-1\nğœ-2\nğœ-3\nğœ-1\nâ€¦\nâ€¦ â€¦\n(a)\n0 50 100 150 200\nSensors ID\n10\n20\n30\n40\n50\n60\n70Speed/(km/h)\nThe evolution of traffic speeds spatial distribution\nBaseline\n30 mintues later\n120 mintues later\n(b)\nFig. 1. (a) Trafï¬c forecasting models with joint spatial temporal depen-\ndencies where spatial dependencies are evolving with time. (b) Evolution\nof the spatial distribution of real-time trafï¬c speeds.\nTrafï¬c networks can be represented as graphs in which\nthe nodes represent trafï¬c sensors and the edges together\nwith their weights are determined by the connectivity as\nwell as Euclidean distances between sensors. Consequently,\ntrafï¬c ï¬‚ows can be viewed as graph signals evolving with\ntime. Recently, Graph Neural Networks (GNNs) [4], [5], [6]\nhave emerged as a powerful tool for processing graph data.\nSequential models are improved by incorporating GNNs to\njointly capture spatio-temporal correlations for both short-\nterm and long-term forecasting. GNN-based trafï¬c forecast-\ning models are ï¬rst developed in [7] and [8] to improve the\nprediction performance by introducing the inherent topol-\nogy of trafï¬c networks into sequential models. Spatial [4] or\nspectral [5] Graph Convolutional Networks (GCNs) are inte-\ngrated with convolution-based sequence learning models [9]\nor recurrent neural networks (RNNs) to jointly capture the\nspatial and temporal dependencies. However, these models\nare still restricted for trafï¬c forecasting, especially long-term\nprediction, in the following two aspects:\nFixed Spatial Dependencies: In trafï¬c forecasting task,\nspatial dependencies are highly dynamical due to road\narXiv:2001.02908v2  [eess.SP]  29 Mar 2021\n2\nğ‘£\"ğ‘£#ğ‘£$ğ‘£%ğ‘£&ğ‘£'ğ‘£(ğ‘£)ğ‘£*ğ‘£\"+ğ‘£\"\"ğ‘£\"#\nğ‘£\"$ğ‘£\"%ğ‘£\"& ğ‘£##ğ‘£\"'ğ‘£\"(ğ‘£\")ğ‘£\"*ğ‘£#+ğ‘£#\"ğ‘£#$ğ‘£#%\nHistorical Traffic Conditions\nPredictions\nShort-range Dependencies \n5 minutesTime Steps \nPrediction Error\nâ€¦\n(a) Autoregressive short-term trafï¬c prediction\nğ‘£\"ğ‘£#ğ‘£$ğ‘£%ğ‘£&ğ‘£'ğ‘£(ğ‘£)ğ‘£*ğ‘£\"+ğ‘£\"\"ğ‘£\"#\nğ‘£\"$ğ‘£\"%ğ‘£\"& ğ‘£##ğ‘£\"'ğ‘£\"(ğ‘£\")ğ‘£\"*ğ‘£#+ğ‘£#\"ğ‘£#$ğ‘£#%\nHistorical Traffic Conditions\nPredictions\nLong-range Dependencies â€¦\n5 minutesTime Steps \nPrediction Error (b) Multi-step long-term trafï¬c prediction\nFig. 2. Autoregressive short-term prediction with short-range temporal dependencies and multi-step long-term prediction with long-range temporal\ndependencies for trafï¬c ï¬‚ow forecasting. Trafï¬c data are aggregated every ï¬ve minutes. Green rectangles are error-free historical observations,\nwhile red ones are predictions. For predictions (red), darker color stands for larger prediction error.\ntopology, varying trafï¬c speeds and multiple factors (e.g.,\nweather conditions, rush hours and trafï¬c accidents). For\neach sensor, its correlated sensors vary with the time steps.\nFig. 1(a) provides a simple example for trafï¬c forecasting,\nwhere trafï¬c speeds excluding target nodes (purple) do\nnot change across different time steps. For the target node\n(purple), different correlated nodes (red) are considered to\nformulate the spatial dependencies for different time steps\n(e.g., Ï„ âˆ’1, Ï„ âˆ’2, Ï„ âˆ’3), according to the range (red circle)\ndetermined by the trafï¬c speeds and distances. Given the\nconnectivity and distance between arbitrary two sensors,\ntheir spatial dependencies are complicated, due to the time-\nvarying trafï¬c speeds. Spatial dependencies would also vary\nfor trafï¬c ï¬‚ows with different directions, i.e., upstream and\ndownstream. Furthermore, spatial dependencies irregularly\noscillate with time steps, due to the periodical effect of\nrush hours, varying weather conditions and unexpected\noccurrence of trafï¬c accidents, as shown in Fig. 1(b). Conse-\nquently, it is necessary to effectively capture these dynami-\ncal spatial dependencies to improve trafï¬c forecasting.\nLimited-range Temporal Dependencies:Long-range tem-\nporal dependencies are usually ignored in existing methods.\nHowever, long-term trafï¬c ï¬‚ow forecasting can be facili-\ntated by considering dependencies with varying scales for\ndifferent time steps.\nFig. 1(a) illustrates spatial dependencies with various\nscales for different time steps, which implies that predic-\ntion performance would be degraded by limiting the range\nof temporal dependencies. Furthermore, prediction errors\nwould be propagated and accumulated for long-term trafï¬c\nforecasting with existing auto-regressive methods trained\nwith either individual loss for each time step [7] or joint\nloss for multiple time steps [8], as depicted in Fig. 2(a).\nIt is desirable to achieve accurate long-term prediction\nbased on long-range temporal dependencies extracted from\nerror-free temporal contexts, as shown in Fig. 2(b).\nIn this paper, we propose a novel paradigm of Spatial-\nTemporal Transformer Networks (STTNs) to address afore-\nmentioned challenges in trafï¬c ï¬‚ow forecasting. The contri-\nbutions of this paper are summarized as below.\nâ€¢ We develop a spatial-temporal block to dynamically\nmodel long-range spatial-temporal dependencies.\nâ€¢ We present a new variant of GNNs, named spa-\ntial transformer, to model the time-varying directed\nspatial dependencies and dynamically capture the\nhidden spatial patterns of trafï¬c ï¬‚ows.\nâ€¢ We design a temporal transformer to achieve multi-\nstep prediction using long-range temporal depen-\ndencies.\nTo be concrete, the spatial transformer dynamically mod-\nels directed spatial dependencies based on the real-time\ntrafï¬c speeds, connectivity and distance between sensors\nand directions of trafï¬c ï¬‚ows. High-dimensional latent sub-\nspaces are learned from the input spatial-temporal features\ntogether with positional embeddings of road topology and\ntemporal information to infer time-varying spatial depen-\ndencies. To represent abrupt changes of trafï¬c ï¬‚ows, long-\nrange time-varying hidden patterns are captured from local\nand global dependencies using the self-attention mecha-\nnism. Furthermore, the temporal transformer simultane-\nously achieves multi-step predictions for future trafï¬c con-\nditions based on the long-range temporal dependencies. It\nsuppresses the propagation of prediction error and allows\nparallel training and prediction to improve efï¬ciency, as\nillustrated in Fig. 2.\nDifferent from [10], STTN facilitates trafï¬c ï¬‚ow fore-\ncasting by dynamically modeling sptail-temporal depen-\ndencies varying with road topology and time steps, rather\nthan ï¬xed spatial dependencies. To validate the efï¬cacy of\nSTTN, we evaluate it on two real-world trafï¬c datasets,\ni.e., PeMSD7(M) and PEMS-BAY. Extensive experiments\ndemonstrate that STTNs can achieve the state-of-the-arts\nperformance in trafï¬c ï¬‚ow forecasting, especially for long-\nterm predictions.\nThe rest of this paper is organized as follows. In Sec-\ntion 2, we brieï¬‚y review existing approaches for modeling\nspatial and temporal dependencies. Section 3 formulates the\nspatial-temporal graph prediction problem for trafï¬c ï¬‚ow\nforecasting and elaborates the proposed STTN for solution.\nExtensive experiments on real-world trafï¬c datasets are\nperformed in Section 4 to evaluate STTN with the state-of-\nthe-art methods. Finally, we conclude this paper and discuss\nthe further work in Section 5.\n2 R ELATED WORK\nWe begin with a brief overview of the existing approaches\nfor modeling spatial and temporal dependencies in trafï¬c\nï¬‚ow forecasting.\n3\n2.1 Spatial Dependencies\nStatistical and neural network based models are ï¬rst de-\nveloped for trafï¬c ï¬‚ow forecasting. Statistical models like\nautoregressive integrated moving average (ARIMA) [11]\nand Bayesian networks [12] model spatial dependencies\nfrom a probabilistic view. Although they help to analyze the\nuncertainty within trafï¬c ï¬‚ows, their linear natures impedes\nthem to effectively model the highly-nonlinearity within\ntrafï¬c ï¬‚ows. Neural networks are introduced to capture the\nnonlinearity of trafï¬c ï¬‚ows, but their fully-connected struc-\ntures are computation intensive and memory consuming.\nFurthermore, the lack of assumptions make it impossible to\ncapture the complicated spatial patterns in trafï¬c ï¬‚ows.\nWith the development of convolutional neural networks\n(CNNs), they have been employed for trafï¬c forecasting\nconsidering their powerful feature extraction abilities in\nmany applications [9], [13], [14]. CNNs are adopted in [15],\n[16], [17] to extract the spatial features in which trafï¬c\nnetworks are converted to regular grids. However, this grid\nconversion leads to the loss of inherent topology informa-\ntion characterizing irregular trafï¬c networks. Graph neural\nnetworks (GNNs) [18], [19] are developed to generalize\nthe deep learning to non-Euclidean domains. As a variant\nof GNNs, graph convolution networks (GCNs) [4], [5],\n[6] generalize classical convolutions to the graph domain.\nRecently, GCNs are widely considered to model the spa-\ntial dependencies of trafï¬c ï¬‚ows to explore the inherent\ntrafï¬c topology. STGCN [7] models spatial dependencies\nwith spectral graph convolutions deï¬ned on an undirected\ngraph, while DCRNN [8] employs diffusion graph convo-\nlutions on a directed graph to accommodate the directions\nof trafï¬c ï¬‚ows. However, they ignore the dynamic changes\nof trafï¬c conditions (e.g. rush hours and trafï¬c accidents),\nas spatial dependencies are ï¬xed once trained. In [20],\nspatial dependencies are dynamically generated with the\ndepth of spatial and temporal blocks, rather than the actual\ntime steps. Dynamical spatial dependencies are modeled\nby incorporating the graph attention networks (GATs) [21]\nand the embedded geo-graph features summarized by extra\nmeta-learner [22]. However, the predeï¬ned graph topology\nusing k nearest neighbors is limited to discover hidden\npatterns of spatial dependencies at various scales beyond\nlocal nodes. Graph WaveNet [10] improves the accuracy of\ntrafï¬c forecasting with hidden spatial patterns through a\nlearnable embedding for each node in the graph, but their\nspatial dependencies are still ï¬xed once trained. In this\npaper, STTNs efï¬ciently model dynamical directed spatial\ndependencies in high-dimensional latent subspaces, rather\nthan adopt predeï¬ned graph structures and local nodes.\n2.2 Temporal Dependencies\nAs stated in [23] and [24], RNNs are limited for mod-\neling temporal dependencies, due to exploding or van-\nishing gradients in training and inaccurate determination\nof sequence lengths. To alleviate these drawbacks, Gated\nRecurrent Units (GRUs) [25] and Long-Short Term Memory\n(LSTM) [26] are developed to model long-range dependen-\ncies for trafï¬c forecasting [20], [22], [23], [24] However,\nthese sequential models still suffer from time-consuming\ntraining process and limited scalability for modeling long\nsequences. Convolution-based sequence learning models [9]\nare adopted as an alternative [7], [20], but require multiple\nhidden layers to cover large contexts under limited size\nof receptive ï¬elds. WaveNet with dilation convolution is\nadopted in [10] to enlarge the receptive ï¬elds, and con-\nsequently, reduce the number of hidden layers. However,\nits model scalability is restricted for long input sequences,\nas the number of hidden layers increases linearly with\nthe lengths of input sequences. Furthermore, the efï¬ciency\nfor capturing long-range dependencies would be affected\nby deeper layers, due to the increasing lengths of paths\nbetween components in the sequence [27], [28]. These facts\nimply that it would be prohibitive to ï¬nd the optimal\nlengths of input sequences, as the model needs to be re-\ndesigned for input sequences with different lengths. Trans-\nformers [27] achieve efï¬cient sequence learning with the\nhighly parallelizable self-attention mechanism. Long-range\ntime-varying dependencies can be adaptively captured from\ninput sequences with various lengths with one single layer.\n3 P ROPOSED MODEL\nIn this section, we introduce the proposed spatial-temporal\ntransformer network. We ï¬rst formulate the trafï¬c fore-\ncasting task as a spatial-temporal prediction problem. To\naddress this problem, we describe the overall architecture\nof the proposed model that consists of two main com-\nponents: spatial-temporal (ST) block and prediction layer.\nSubsequently, we elaborate the proposed spatial transformer\nand temporal transformerin ST block, respectively.\n3.1 Problem Formulation\nA trafï¬c network can be naturally represented as a graph\nG= (V,E,A), where Vis the set of N nodes representing\nthe sensors, E is the set of edges reï¬‚ecting physical con-\nnectivity between sensors and A âˆˆRNÃ—N is the adjacency\nmatrix constructed with the Euclidean distances between\nsensors via Gaussian kernel. Trafï¬c forecasting is a classic\nspatial-temporal prediction problem. In this paper, we focus\non forecasting trafï¬c speeds vÏ„ âˆˆRN at time step Ï„ for the\nN sensors, and volume and density can be similarly calcu-\nlated as trafï¬c speeds. Given M historical trafï¬c conditions\n[vÏ„âˆ’M+1,Â·Â·Â· ,vÏ„] observed by the N sensors and a trafï¬c\nnetwork G, a trafï¬c forecasting model Fis learned to predict\nT future trafï¬c conditions [Ë†vÏ„+1,Â·Â·Â· ,Ë†vÏ„+T].\nË†vÏ„+1,Â·Â·Â· ,Ë†vÏ„+T = F(vÏ„âˆ’M+1,Â·Â·Â· ,vÏ„; G) (1)\nTo achieve accurate prediction, Fcaptures dynamical spa-\ntial dependencies SS\nÏ„ âˆˆ RNÃ—N and long-range temporal\ndependencies ST\nÏ„ âˆˆRMÃ—M from vÏ„âˆ’M+1,Â·Â·Â· ,vÏ„ and G.\nHowever, existing methods are limited in long-term predic-\ntion, as they only consider ï¬xed spatial dependencies and\nshort-range temporal dependencies. In this paper, spatial\ntransformer is developed to dynamically train Fwith time-\nvarying spatial dependencies SS\nÏ„ âˆˆ RNÃ—N for each time\nstep Ï„, as deï¬ned in Section 3.3. Furthermore, long-term\ntemporal dependencies ST\nÏ„ âˆˆRMÃ—M are efï¬ciently learned\nwith the temporal transformer based on self-attention mech-\nanism, as shown in Section 3.4. Error propagation can\nbe addressed by simultaneously making T predictions\n4\nSpatial TransformerTemporal Transformerğ“¢ ğ“£Spatial TransformerTemporal Transformer\n1st Spatial-Temporal Block Prediction Layer\nğ‘¿ğŸğ“£\nğ“¢ ğ“£ Conv1Conv2\nğ‘¿ğ“¢ğ“£ğ’€&â€¦\nFeature  Aggregation\nğ’€=(ğ’—ğ‰+ğŸ,â€¦,(ğ’—ğ‰+ğ‘»\n(a) Overall architectureğ‘¿ğ“¢âˆˆâ„ğ‘´Ã—ğ‘µÃ—ğ’…ğ“–\nPositional Embedding\n2nd Spatial-Temporal Block\n1Ã—1 Convolutional Layer Spatial TransformerTemporal Transformerğ“¢ ğ“£kth Spatial-Temporal Block\n1Ã—1 Convolutional Layer1Ã—1 Convolutional Layer\nğ‘¿ğ“£âˆˆâ„ğ‘´Ã—ğ‘µÃ—ğ’…ğ“–\nPositional Embedding\n(c) Temporal Transformer ğ“£\nMessage Passing ğ‘­LinearLinearLinearğ‘¾ğ’Œğ“£ğ‘¾ğ’’ğ“£\nScaled Dot Productğ‘¸ğ“£ ğ‘²ğ“£\nWeighted Sumğ‘ºğ“£\nFeed Forward Networkğ‘´ğ“£\n+\nğ’€ğ“£=[>ğ’€ğŸğ“£,â€¦,>ğ’€ğ‘µğ“£]\nğ‘¼ğ“£Update ğ‘® ğ‘´â€²ğ“£\nğ‘¾ğ’—ğ“£\nğ‘½ğ“£\nMessage Passing ğ‘­ LinearLinearLinearğ‘¾ğ’Œğ“¢ğ‘¾ğ’’ğ“¢ ğ‘¾ğ’—ğ“¢\nScaled Dot Productğ‘¸ğ“¢ ğ‘²ğ“¢ ğ‘½ğ“¢\nWeighted Sumğ‘ºğ“¢\nFeed Forward Networkğ‘´ğ“¢\nâˆ¼\nğ’€ğ“¢=[>ğ’€ğŸğ“¢,â€¦,>ğ’€ğ‘´ğ“¢]\nGraph Convolutionğ‘¿ğ“–\nUpdate ğ‘®\n(b) Spatial Transformer ğ“¢\nğ‘´&ğ“¢\nğ‘¿&ğ“¢=[>ğ‘¿ğŸğ“¢,â€¦,>ğ‘¿ğ‘´ğ“¢]\n>ğ’€ğ“¢âˆˆâ„ğ‘µÃ—ğ’…ğ“–\n>ğ‘¿ğ“¢âˆˆâ„ğ‘µÃ—ğ’…ğ“– >ğ‘¿ğ“£âˆˆâ„ğ‘´Ã—ğ’…ğ“–ğ‘¿&ğ“£=[>ğ‘¿ğŸğ“£,â€¦,>ğ‘¿ğ‘µğ“£]\n>ğ’€ğ“£âˆˆâ„ğ‘´Ã—ğ’…ğ“–\nParallel\nParallel\nğ‘¼ğ“¢\nğ‘¿=ğ’—ğ‰Eğ‘´+ğŸ,â€¦,ğ’—ğ‰ ğ‘¿ğŸğ“¢ ğ‘¿ğŸğ“¢ ğ‘¿ğŸğ“£ ğ‘¿ğ’Œğ“¢ ğ‘¿ğ’Œğ“£\nFig. 3. Illustrative architecture of the proposed spatial-temporal transformer network (STTN). STTN consists of stacked spatial-temporal (ST) blocks\nand one prediction layer. Each ST block leverages one spatial transformer and one temporal transformer to jointly model the spatial-temporal\ndependencies. Skip connections are adopted to combine all levels of spatial-temporal features by the ST blocks.\nË†vÏ„+1,Â·Â·Â· ,Ë†vÏ„+T from the error-free historical observations\nvÏ„âˆ’M+1,Â·Â·Â· ,vÏ„ with the spatial-temporal features learned\nbased on SS\nÏ„ and ST\nÏ„ . For simplicity, we omit the subscript\nÏ„ in SS\nÏ„ and ST\nÏ„ in the rest of this section.\n3.2 Overall Architecture\nAs depicted in Fig. 3, the proposed spatial-temporal trans-\nformer network consists of stacked spatial-temporal blocks\nand a prediction layer. Here, each spatial-temporal block\nis composed of one spatial transformer and one temporal\ntransformer to jointly extract spatial-temporal features in\nthe context of dynamical dependencies. Spatial-temporal\nblocks can be further stacked to form deep models for deep\nspatial-temporal features. Subsequently, the prediction layer\nutilizes two 1 Ã—1 convolutional layers to aggregate these\nspatial-temporal features for trafï¬c forecasting.\n3.2.1 Spatial-temporal Blocks\nThe future trafï¬c conditions of one node are determined by\nthe trafï¬c conditions of its neighboring nodes, the time steps\nof observations and abrupt changes like trafï¬c accidents and\nweather conditions. In this section, we develop a spatial-\ntemporal block to integrate spatial and temporal transform-\ners to jointly model the spatial and temporal dependencies\nwithin trafï¬c networks for accurate prediction, as illustrated\nin Fig. 3. The input to the l-th spatial-temporal block is a 3-\nD tensor XS\nl âˆˆRMÃ—NÃ—dG of dG-dimensional features for\nthe N nodes at time steps Ï„ âˆ’M + 1,Â·Â·Â· ,Ï„ extracted by\nthe lâˆ’1-th spatial-temporal block. The spatial transformer\nSand temporal transformer T are stacked to generate the\n3-D output tensor. Residual connections are adopted to for\nstable training. In the l-th spatial-temporal block, the spatial\ntransformer Sextracts spatial features YS\nl from the input\nnode feature XS\nl as well as graph adjacency matrix A.\nYS\nl = S(XS\nl ,A) (2)\nYS\nl is combined with XS\nl to generate the input XT\nl to the\nsubsequent temporal transformer.\nYT\nl = T(XT\nl ) (3)\nConsequently, we obtain the output tensorXS\nl+1 = YT\nl +XT\nl\nand feed XS\nl+1 into the l+ 1-th spatial-temporal block. Mul-\ntiple spatial-temporal blocks can be stacked to improve the\nmodel capacity according to the tasks at hand. In Section 3.3\nand 3.4, we elaborate the spatial and temporal transformers.\nWithout loss of generality, we omit the subscript l of XS\nl ,\nXT\nl , YS\nl and YT\nl for the l-th spatial-temporal block.\n3.2.2 Prediction Layer\nThe prediction layer leverages two classical convolutional\nlayers to make multi-step prediction based on the spatial-\ntemporal features from the last spatial-temporal block. Its\ninput is a 2-D tensor XST âˆˆRNÃ—dST\nthat consists of the\ndST-dimensional spatial-temporal features of the N nodes\nfor last time step Ï„. The multi-step prediction Y âˆˆRNÃ—T\nfor T future trafï¬c conditions of the N nodes is\nY = Conv(Conv(XST)) (4)\nMean absolute loss are adopted to train the model.\nL= âˆ¥Y âˆ’Ygtâˆ¥1 (5)\nwhere Ygt âˆˆRNÃ—T is the groundtruth trafï¬c speeds.\n3.3 Spatial Transformer\nAs shown in Fig. 3(b), the spatial transformer consists of\nspatial-temporal positional embedding layer, ï¬xed graph\nconvolution layer, dynamical graph convolution layer\nand gate mechanism for information fusion. The spatial-\ntemporal positional embedding layer incorporates spatial-\ntemporal position information (e.g., topology, connectivity,\ntime steps) into each node. According to [29], the trafï¬c\n5\nsignal over a period of time can be decomposed into a sta-\ntionary component determined by the road topology (e.g.,\nconnectivity and distance between sensors) and a dynam-\nical component determined by real-time trafï¬c conditions\nand sudden changes (e.g., accidents and weather changes).\nConsequently, we develop a ï¬xed graph convolutional layer\nand a dynamical graph convolutional layer to explore the\nstationary and directed dynamical components of spatial\ndependencies, respectively. The learned stationary and dy-\nnamical spatial features are fused with gate mechanism. We\nfurther show that the proposed spatial transformer can be\nviewed as a general message passing GNN for dynamical\ngraph construction and feature learning.\n3.3.1 Spatial-Temporal Positional Embedding\nFig. 1(a) shows that the spatial dependencies of two nodes\nin the graph would be determined by their distances and\nobserved time steps. Transformer [27] cannot capture the\nspatial (position) and temporal information of observations\nwith the fully connected feed-forward structures. Thus,\nthe prior positional embedding is required to inject the\nâ€™positionâ€™ information into the input sequences. In the pro-\nposed spatial transformer, we adopt learnable spatial and\ntemporal positional embedding layer to learn the spatial-\ntemporal embedding into each node feature. The dictionar-\nies Ë†DS âˆˆRNÃ—N and Ë†DT âˆˆRMÃ—M are learned as spatial\nand temporal positional embedding matrices, respectively.\nË†DSis initialized with the graph adjacency matrix to consider\nthe connectivity and distance between nodes for modeling\nspatial dependencies, while Ë†DT is initialized with one-hot\ntime encoding to inject the time step into each node. Ë†DS\nand Ë†DT are tiled along the spatial and temporal axes to\ngenerate DS âˆˆ RMÃ—NÃ—N and DT âˆˆ RMÃ—NÃ—M, respec-\ntively. the embedded features Xâ€²S = Ft([XS,DS,DT]) âˆˆ\nRMÃ—NÃ—dG with a ï¬xed dimension of dG are obtained from\nXS âˆˆ RMÃ—NÃ—dG that concatenates DS âˆˆ RMÃ—NÃ—N and\nDT âˆˆRMÃ—NÃ—M. Here, Ft is a 1 Ã—1 convolutional layer to\ntransform the concatenated features into a dG-dimensional\nvector for each node at each time step. Xâ€²S is fed into the\nï¬xed and dynamical graph convolutional layers for spatial\nfeature learning. Since graph convolution operations can\nbe realized in parallel for the M time steps via tensor\noperations, we consider the 2-D tensor Ë†XSâˆˆRNÃ—dG of Xâ€²S\nfor arbitrary one time step for brevity.\n3.3.2 Fixed Graph Convolutional Layer\nGraph convolution is generalizes classical grid-based con-\nvolution to the graph domain. Node features are derived\nby aggregating the information from its neighboring nodes\nbased on the learned weights and predeï¬ned graph. In this\nsubsection, graph convolution based on Chebyshev poly-\nnomial approximation is employed to learn the structure-\naware node features, and consequently, capture the station-\nary spatial dependencies from the road topology. Let us\ndenote Dthe degree matrix of Gwith its diagonal elements\nDii = âˆ‘\niAij for i = 1,Â·Â·Â· ,N. The normalized Laplacian\nmatrix L is deï¬ned by L = In âˆ’Dâˆ’1/2ADâˆ’1/2 and the\nscaled Laplacian matrix ËœL = 2L/Î»max âˆ’In for Chebyshev\npolynomials, where Î»max is the largest eigenvalues of L.\nGiven the embedded features Ë†XS, the structure-aware node\nfeatures Ë†XGâˆˆRNÃ—dG are obtained with the graph convolu-\ntion approximated by the Chebyshev polynomials Tk with\nthe orders k= 1,Â·Â·Â· ,K for each time step.\nË†XG\n:,j =\ndGâˆ‘\ni=1\nKâˆ‘\nk=0\nÎ¸ij,kTk(ËœL) Ë†XS\n:,i âˆ€j = 0,Â·Â·Â· ,dG, (6)\nwhere Ë†XG\n:,j is the j-th channel (column) of Ë†XG and Î¸ij,k\nis the learned weights. Since G is constructed based on\nthe physical connectivity and distance between sensors,\nthe stationary spatial dependencies determined by the road\ntopology can be explicitly explored through the ï¬xed graph\nconvolutional layer.\n3.3.3 Dynamical Graph Convolutional Layer\nGCN-based models like [7] and [8] only model stationary\nspatial dependencies. To capture the time-evolving hidden\nspatial dependencies, we propose a novel dynamical graph\nconvolutional layer to achieve training and modeling in\nthe high-dimensional latent subspaces. Speciï¬cally, we learn\nthe linear mappings that project the input features of each\nnode to the high-dimensional latent subspaces. As shonw\nin Fig. 4, the self-attention mechanism is adopted for the\nprojected features to efï¬ciently model the dynamical spa-\ntial dependencies between nodes according to the varying\ngraph signals. Although it is also adopted in [20], the\nweights of edges are calculated based on the predeï¬ned\nroad topology. Predeï¬ned road topology cannot sufï¬ciently\nrepresent the dynamical spatial dependencies within trafï¬c\nnetworks. Consequently, we learn multiple linear mappings\nto model dynamical directed spatial dependencies affected\nby various factors in various latent subspaces.\nThe embedded feature Ë†XSof each time step is ï¬rst pro-\njected into the high-dimensional latent subspaces. The map-\npings are realized with the feed-forward neural networks.\nWhen the single-head attention model is considered for one\npattern of spatial dependencies, three latent subspaces are\ntrained for each node based on Ë†XS, including the query\nsubspace spanned by QS âˆˆRNÃ—dS\nA, the key subspace by\nKSâˆˆRNÃ—dS\nA and the value subspace by VSâˆˆRNÃ—dG.\nQS= Ë†XSWS\nq\nKS= Ë†XSWS\nk\nVS= Ë†XSWS\nv (7)\nHere, WS\nq âˆˆRdGÃ—dS\nA, WS\nk âˆˆRdGÃ—dS\nA and WS\nv âˆˆRdGÃ—dG are\nthe weight matrices for QS, KSand VS, respectively.\nDynamical spatial dependencies SS âˆˆRNÃ—N between\nnodes are calculated with the dot product of QSand KS.\nSS= softmax(QS(KS)T/\nâˆš\ndS\nA) (8)\nIn Eq. (8), dot product is adopted reduce the computational\nand storage costs in calculation. Softmax is used to normal-\nize the spatial dependencies and the scale\nâˆš\ndS\nAprevents the\nsaturation led by Softmax function. Thus,the node features\nMSâˆˆRNÃ—dG are updated with SS.\nMS= SSVS. (9)\n6\nIt is worth mentioning that multiple patterns of spatial de-\npendencies can be learned with multi-head attention mech-\nanism by introducing multiple pairs of subspaces, which is\nable to model different hidden spatial dependencies from\nvarious latent subspaces.\nFurthermore, a shared three-layer feed-forward neural\nnetwork with nonlinear activation is applied on each node\nto further improve the prediction conditioned on the learned\nnode features MS. The interactions among feature channels\nare explored to update MSwith USâˆˆRNÃ—dG.\nUS= ReLu(ReLu( Ë†Mâ€²SWS\n0 )WS\n1 )WS\n2 (10)\nwhere Mâ€²S = Ë†XS + MS is the residual connection for\nstable training and WS\n0 , WS\n1 and WS\n2 are the weight ma-\ntrices for the three layers. US and Mâ€²S are combined by\nË†YS= US+Mâ€²Sfor feature fusion with the gate mechanism.\nIt should be noted that we can stack multiple dynamical\ngraph convolution layers for deep models to improve the\nmodel capacity for complicated spatial dependencies.\n3.3.4 Gate Mechanism for Feature Fusion\nThe gate mechanism is developed to fuse the spatial features\nlearned from ï¬xed and dynamical graph convolutional lay-\ners. The gate gis derived from Yâ€²Sand XSof the ï¬xed and\ndynamical graph convolutional layers.\ng= sigmoid(fS( Ë†YS) +fG(XG)), (11)\nwhere fS and fG are linear projections that transform Yâ€²S\nand XGinto 1-D vectors, respectively. As a result, the output\nYSis obtained by weighting Yâ€²Sand XSwith the gate g.\nYâ€²S= gË†YS+ (1âˆ’g)XG (12)\nThe output YS âˆˆRMÃ—NÃ—dG of the spatial transformer col-\nlects Yâ€²Sobtained in parallel for the M time steps and is fed\ninto the subsequent temporal transformer with XT = YS.\n3.3.5 General Dynamical Graph Neural Networks\nExisting spectral and spatial graph convolutional networks\nrely on predeï¬ned graph topologies that cannot adapt to\nthe input graph signals. In this subsection, we demonstrate\nthat the spatial transformer can be formulated as an iterative\nfeature learning process of message passing and update for\nall the nodes v âˆˆV in a dynamical graph neural network.\nLet us denote xv âˆˆRdG the input features of node v. For\narbitrary v âˆˆV, it receives the message mv âˆˆRdG from the\nnodes in V.\nmv =\nâˆ‘\nuâˆˆV\nF(xv,xu) =\nâˆ‘\nuâˆˆV\nâŸ¨(WS\nq )Txv,(WS\nk )TxuâŸ©xu (13)\nwhere F is the composite message-passing function that\nrealizes Eqs. (7), (8) and (9). When mv is obtained, xv is\nupdated with yv calculated from mv and xv.\nyv = G(mv,xv) (14)\nIn the spatial transformer,Gis the shared position-wise feed\nforward network deï¬ned in Eq. (10). Comparing Eq. (2) with\nEqs. (13) and (14), the spatial transformer can be viewed as a\ngeneral message-passing dynamical graph neural network.\nğ‘„\"\nğ¾\"\nğ‘‰\"\nğ‘£\"\nLinear\nLinear\nLinear ğ‘Š'\nğ‘Š(\nğ‘Š)\nâ€¦\nPosition-wise \nLinear Layer\nDot Product\nDependencies\n\tğ‘†\"\", ğ‘†\"-, â€¦ , ğ‘†\")\nDot Product\nğ‘š\" ğ‘š0\nğ‘†\"\"\nâ€¦\nğ‘†\"-\nğ‘†\") ğ‘†)\"\nğ‘†)- Dependencies\nğ‘†)\",ğ‘†)-, â€¦ , ğ‘†))\nğ‘„-\nğ¾-\nğ‘‰-\nğ‘£-\nLinear\nLinear\nLinear ğ‘Š'\nğ‘Š(\nğ‘Š)\nğ‘„0\nğ¾0\nğ‘‰0\nğ‘£)\nLinear\nLinear\nLinear ğ‘Š'\nğ‘Š(\nğ‘Š)\nğ‘š-\nğ‘†))\nFig. 4. Self-attention mechanism for long-range temporal dependencies.\n3.4 Temporal Transformers\nFig. 3(c) depicts the proposed temporal transformer for\nefï¬ciently capturing the long-range temporal dependen-\ncies [27]. In comparison to RNNs and their variants, tempo-\nral transformer can be easily scaled to long sequences with\nparallel processing of long-range dependencies. Similar to\nthe spatial transformer, Xâ€²T = Gt([XT,DT]) âˆˆRMÃ—NÃ—dG\nis obtained from the concatenation of the input features\nXT = XS+ YSâˆˆRMÃ—NÃ—dG and the temporal embedding\nDT, where Gt is a 1Ã—1 convolutional layer that yields a dG-\ndimensional vector for each node at each time step. Here,\nwe also parallelize over the nodes to model temporal de-\npendencies. The 2-D tensor of spatial features Ë†XT âˆˆRMÃ—dG\nis considered for arbitrary one node in G.\nSelf-attention mechanism is also adopted to model tem-\nporal dependencies. The input to the temporal transformer\nis a temporal sequence Ë†XT âˆˆRMÃ—dG with a slide window\nof length M and dG channels. Similar to spatial trans-\nformer, temporal dependencies are dynamically computed\nin high-dimensional latent subspaces, including the query\nsubspace spanned by QT âˆˆRMÃ—dT\nA, the key subspace by\nKT âˆˆRMÃ—dT\nA and the value subspace by VT âˆˆRMÃ—dG.\nQT = Ë†XTWT\nq\nKT = Ë†XTWT\nk\nVT = Ë†XTWT\nv , (15)\nwhere Wq âˆˆRdGÃ—dT\nA,Wk âˆˆRdGÃ—dT\nA and Wv âˆˆRdGÃ—dG are\nthe learned liner mappings. According to Eq. (1), multi-step\nprediction for vÏ„+1,Â·Â·Â· ,vÏ„+T is simultaneously made from\nthe historical observations vÏ„âˆ’M+1,Â·Â·Â· ,vÏ„. We introduce\nthe scaled dot product function to consider bi-directional\ntemporal dependencies within vÏ„âˆ’M+1,Â·Â·Â· ,vÏ„.\nST = softmax(QT(KT)T/\nâˆš\ndT\nA) (16)\nRNN-based models are limited to consider temporal depen-\ndencies based on preceding time steps, as shown in [30],\nthis left-to-right architecture is sub-optimal to model context\ndependencies. We further aggregate the values VT with the\nweights ST for temporal features MT.\nMT = STVT (17)\nFig. 4 illustrates the modeling of temporal dependencies.\n7\nTo explore the interactions among latent features, a\nshared three-layer feed-forward neural network is devel-\noped for MT.\nUT = ReLu(ReLu(Mâ€²TWT\n0 )WT\n1 )WT\n2 (18)\nHere, the residual connection Mâ€²T = MT + XT is adopted\nfor stable training. For each node, its output is Ë†YT =\nUT + Mâ€²T. As a result, the output of temporal transformer\nis YT âˆˆRMÃ—NÃ—dG by collecting Ë†YT for all the nodes.\nLong-range bidirectional temporal dependencies are ef-\nï¬ciently captured in each layer of the temporal transformer,\nas each time step attends to the remaining time steps within\nthe slide window. The temporal transformer can be easily\nscaled to long sequences by increasing M without much\nsacriï¬ce in computation efï¬ciency. By contrast, RNN-based\nmodels would suffer from vanishing or exploding gradients,\nwhile convolution-based models have to explicitly specify\nthe number of convolutional layers that grows with M.\n4 E XPERIMENTS\nWe demonstrate that STTN achieves the state-of-the-art per-\nformance in trafï¬c ï¬‚ow forecasting, especially for long-term\npredictions, on two real-world datasets, i.e., PeMSD7(M)\nand PEMS-BAY. Furthermore, ablation studies have been\nmade to validate the multi-step prediction and the effec-\ntiveness of spatial and temporal transformers for long-term\ntrafï¬c forecasting. We also analyze the model conï¬gura-\ntions, including the number of blocks, feature channels and\nlayers, attention heads for the self-attention mechanism and\nspatial-temporal positional embedding.\n4.1 Dataset and Data Preprocessing\nTwo real-world trafï¬c datasets are adopted for evaluations:\nPeMSD7(M): Trafï¬c data from 228 sensor stations in the\nCalifornia state highway system during the weekdays from\nMay through June in 2012.\nPEMS-BAY:6-month trafï¬c data collected from 325 sensors\nin the Bay Area of California, starting from January 1st 2017\nthrough May 31th 2017.\nTrafï¬c speeds are aggregated every ï¬ve minutes and\nnormalized with Z-Score as inputs. The road topology in-\nformation is represented by a graph adjacency matrix. The\ngraph of PeMS-BAY dataset is pre-designed as a directed\ngraph to differentiate the inï¬‚uence of different directions.\nIn [8], forward and backward diffusion graph convolutions\nare adopted to model the directed spatial dependencies.\nHowever, it is difï¬cult to construct a directed graph with\na proper metric for the inï¬‚uence of directions. In this paper,\nwe use the self-attention mechanism to model directed\nspatial dependencies in a data-driven manner and alleviate\nthe computation burden on differentiating the inï¬‚uence of\ndirections. Only the undirected graph adjacency matrix is\nrequired to represent the distance and connectivity between\nsensors. In PEMS-BAY, the undirected graph for road topol-\nogy is generated by selecting the larger weight from the\ntwo directions (i.e., upstream and downstream) of each pair\nof nodes. The adjacency matrix is symmetric based on the\ndistances between sensors in PeMSD7.\n4.2 Experimental Settings\nAll experiments are conducted on a NVIDIA 1080Ti GPU.\nThe proposed model is trained with the mean absolute error\n(MAE) loss using the RMSprop optimizer for 50 epochs\nwith a batch size of 50. The initial leaning rate is set to\n10âˆ’3 and decays at a rate of 0.7 for every ï¬ve epochs.\nTable 1 shows the average results for STTN obtained by\nï¬ve independent trials of the same experiment on each\ndataset. For evaluations, we adopt the results reported in [7]\nand [8], where 12 current observations (60 minutes) are\nused to predict the trafï¬c conditions in the next 15, 30 and\n45 minutes in PeMSD7(M) and 15, 30 and 60 minutes in\nPEMS-BAY, respectively. Graph WaveNet [10] is trained on\nPeMSD7(M) using its the publicly released code and the best\nperformance is reported in Table 1.\n4.3 Evaluation Metrics and Baselines\nWe evaluate STTN and benchmark trafï¬c forecasting meth-\nods in terms of mean absolute error (MAE), mean ab-\nsolute percentage error (MAPE) and root mean squared\nerror (RMSE). Baselines include historical average (HA),\nautoregressive integrated moving average (ARIMA) with\nKalman ï¬ltering [31], linear support vector regressiion\n(LSVR) [32], feed-forward neural network (FNN), fully-\nconnected LSTM (FC-LSTM) [33], STGCN [7], DRCNN [8]\nand Graph WaveNet [10] .\nFor PeMSD7(M), one spatial-temporal block is adopted.\nTwo hidden layers and one single attention are adopted\nfor each spatial and temproal transformer with 64 feature\nchannels. Considering that PeMS-BAY is much larger than\nPeMSD7(M) in spatial and temporal scale, three spatial-\ntemporal blocks are stacked to model spatial-temporal de-\npendencies. In each spatial-temporal block, each spatial and\ntemporal transformer consists of one hidden layer and sin-\ngle attention with 64 feature channels. Residual structures\nare adopted for stable learning and fast convergence.\n4.4 Experimental Results\nTable 1 provides MAE, MAPE and RMSE for STTN and\nbaselines for trafï¬c forecasting with varying period of time\nsteps on PEMS-BAY and PeMSD7(M).\nPeMSD7(M): STTN outperforms STGCN [7] and\nDCRNN [8] by a large margin that grows with the\nrange of time steps for prediction. In comparison to\nGraph WaveNet [10], STTN performs better in long-\nterm prediction ( â‰¥30 minutes) and yields competitive\nperformance for short-term prediction (â‰¤30 minutes). These\nfacts imply that long-term prediction can be facilitated by\njointly considering dynamic spatial dependencies and long-\nrange temporal dependencies. By contrast, Graph WaveNet\nleverages convolutional kernels with small receptive ï¬elds\nto capture stationary spatial-temporal dependencies for\nshort-term prediction.\nPEMS-BAY:STTN is competitive with Graph WaveNet and\noutperforms STGCN and DCRNN. Compared with STGCN,\nGraph WaveNet and DCRNN employ bi-directional dif-\nfusion graph convolutions based on the non-symmetric\nadjacency matrix explicitly designed for the inï¬‚uence of\ndirections. STTN leverages the self-attention mechanism\n8\nTABLE 1\nMAE, MAPE (%) and RMSE for PEMS-BAY and PeMSD7(M) obtained by STTN and the baselines. Trafï¬c conditions in the next 15, 30 and 45\nminutes are predicted for PeMSD7(M) and 15, 30 and 60 minutes for PEMS-BAY .\nModel PEMS-BAY (15/30/60 min) PeMSD7(M) (15/30/45 min)\nMAE MAPE (%) RMSE MAE MAPE (%) RMSE\nHA 2.88 6.8 5.59 4.01 10.61 7.20\nARIMA [31] 1.62/2.33/3.38 3.5/5.4/8.3 3.30/4.76/6.50 5.55/5.86/6.27 12.92/13.94/15.20 9.00/9.13/9.38\nLSVR [32] 1.85/2.48/3.28 3.8/5.5/8.0 3.59/5.18/7.08 2.50/3.63/4.54 5.81/8.88/11.50 4.55/6.67/8.28\nFNN 2.20/2.30/2.46 5.19/5.43/5.89 4.42/4.63/4.89 2.74/4.02/5.04 6.38/9.72/12.38 4.75/6.98/8.58\nFC-LSTM [33] 2.05/2.20/2.37 4.8/5.2/5.7 4.19/4.55/4.96 3.57/3.94/4.16 8.60/9.55/10.10 6.20/7.03/7.51\nDCRNN [8] 1.38/1.74/2.07 2.9/3.9/4.9 2.95/3.97/4.74 2.37/3.31/4.01 5.54/8.06/9.99 4.21/5.96/7.13\nSTGCN [7] 1.39/1.84/2.42 3.00/4.22/5.58 2.92/4.12/5.33 2.25/3.03/3.57 5.26/7.33/8.69 4.04/5.70/6.77\nGraph WaveNet [10] 1.30/1.63/1.95 2.74/3.70/4.52 2.73/3.67/4.63 2.14/2.80/3.19 4.93/6.89/8.04 4.01/5.48/6.25\nSTTN 1.36/1.67/1.95 2.89/3.78/4.58 2.87/3.79/4.50 2.14/2.70/3.03 5.05/6.68/7.61 4.04/5.37/6.05\n0 12 24 36 48 60 72 84 96 108 120 132 144 156 168 180 192 204 216 228 240 252 264 276 288\nTime_steps\n40\n45\n50\n55\n60\n65\n70\n75Speed km/h\nAverage Prediction results with different predcition time of STTN\nSTTN-5min\nSTTN-10min\nSTTN-15min\nSTTN-20min\nSTTN-25min\nSTTN-30min\nSTTN-35min\nSTTN-40min\nSTTN-45min\nGround_truth\n(a) STTN\n0 12 24 36 48 60 72 84 96 108 120 132 144 156 168 180 192 204 216 228 240 252 264 276 288\nTime_steps\n40\n45\n50\n55\n60\n65\n70\n75Speed km/h\nAverage prediction results with different predcition time of GraphWaveNet\nGraphWaveNet-5min\nGraphWaveNet-10min\nGraphWaveNet-15min\nGraphWaveNet-20min\nGraphWaveNet-25min\nGraphWaveNet-30min\nGraphWaveNet-35min\nGraphWaveNet-40min\nGraphWaveNet-45min\nGround_truth (b) Graph WaveNet [10]\n0 12 24 36 48 60 72 84 96 108 120 132 144 156 168 180 192 204 216 228 240 252 264 276 288\nTime_steps\n40\n45\n50\n55\n60\n65\n70\n75Speed km/h\nAverage Prediction results with different predcition time of STGCN\nSTGCN-5min\nSTGCN-10min\nSTGCN-15min\nSTGCN-20min\nSTGCN-25min\nSTGCN-30min\nSTGCN-35min\nSTGCN-40min\nSTGCN-45min\nGround_truth\n(c) STGCN [7]\n0 12 24 36 48 60 72 84 96 108 120 132 144 156 168 180 192 204 216 228 240 252 264 276 288\nTime_steps\n40\n45\n50\n55\n60\n65\n70\n75Speed km/h\nAverage Prediction results with different predcition time of DCRNN\nDCRNN-5min\nDCRNN-10min\nDCRNN-15min\nDCRNN-20min\nDCRNN-25min\nDCRNN-30min\nDCRNN-35min\nDCRNN-40min\nDCRNN-45min\nGround_truth (d) DCRNN [8]\nFig. 5. Visualization of one-day trafï¬c ï¬‚ow forecasting for PeMSD7 obtained by STTN, Graph WaveNet [10], STGCN [7] and DCRNN [8].\nto learn dynamical directed spatial dependencies from the\nsymmetric adjacency matrix (without prior information for\nupstream/downstream trafï¬c ï¬‚ows). It is worth mentioning\nthat, compared with STGCN that also consists of three\nspatial-temporal blocks, STTN improves the prediction per-\nformance considerately.\nFor further evaluation, trafï¬c forecasting for one-day pe-\nriod is visualized in for STNN, Graph WaveNet, STGCN and\nDCRNN. Here, the one-day prediction is obtained for each\ntime step by averaging along the spatial dimension on test\ndataset of PeMSD7(M). Fig. 5 shows that STTN and Graph\nWaveNet improve trafï¬c ï¬‚ow forecasting in changing area,\ne.g. Ï„ âˆˆ [60,84], in comparison to STGCN and DCRNN.\nNote that time shifts of the curves of predictions are evident\nfor STGCN and DCRNN, which suggests that prediction\nerrors grow with the time steps, especially in the areas with\nsharp variation. Moreover, STTN can capture continuous\nchanges in a long period of time steps, e.g.Ï„ âˆˆ[84,192]. This\nfact implies that dynamical spatial dependencies and long-\nrange temporal dependencies captured by STTN beneï¬t\ntrafï¬c ï¬‚ow forecasting, especially long-term prediction.\n4.5 Computational Complexity\nWe further evaluate the computational costs for DCRNN,\nSTGCN, Graph WaveNet and STTN. All the experiments\nare conducted on the same GPU. Table 2 reports the average\ntraining speed for one epoch. STGCN is efï¬cient with the\nfully convolutional structures. DRCNN is time-consuming\ndue to the recurrent structures for training with joint loss for\nmultiple time steps, as its training time is proportional to the\nnumber of prediction time steps. STTN yields a reduction of\n10-40% and 40-60% in computational costs in comparison to\nGraph WaveNet and DCRNN, respectively. Note that STTN\nis scalable to achieve long-term prediction without excessive\ncomputational complexity.\n9\nTABLE 2\nAverage training time (sec/epoch) for PEMS-BAY and PeMSD7(M)\nobtained by STTN, Graph WaveNet, STGCN and DRCNN, respectively.\nDataset Average training time (sec/epoch)\nSTTN Graph WaveNet STGCN DRCNN\nPEMS-bay 458 507 99 809\nPeMSD7(M) 45 72 10 108\nTABLE 3\nMAE, MAPE (%) and RMSE for PeMSD7(M) obtained by STGCN and\nSTTN with autoregressive (AR) and multi-step (MS) prediction.\nModel PeMSD7(M) (15/30/45 min)\nMAE MAPE (%) RMSE\nSTGCN AR 2.25/3.03/3.57 5.26/7.33/8.69 4.04/5.70/6.77\nMS 2.25/2.90/3.32 5.33/7.19/8.42 4.19/5.59/6.42\nSTTN AR 2.19/2.97/3.54 5.10/7.20/8.75 4.17/5.89/7.05\nMS 2.14/2.75/3.12 5.06/6.84/7.89 4.03/5.41/6.17\nTABLE 4\nMAE, MAPE (%) and RMSE for PeMSD7(M) obtained by STTN with\nï¬xed graph convolution (Baseline), STTN with attention to local nodes\n(STTN-S (local)) and STTN using the spatial transformer with a\nattention heads and h hidden layers (STTN-S(a, h)).\nModel PeMSD7(M) (15/30/45 min)\nMAE MAPE (%) RMSE\nBaseline 2.18/2.92/3.43 5.12/7.18/8.65 4.04/5.57/6.58\nSTTN-S (local) 2.18/2.86/3.30 5.15/7.05/8.33 4.04/5.48/6.36\nSTTN-S(1,1) 2.16/2.79/3.16 5.09/6.92/8.00 4.04/5.42/6.19\nSTTN-S(2,1) 2.15/2.77/3.12 5.09/6.88/7.91 4.01/5.37/6.12\nSTTN-S(4,1) 2.14/2.75/3.09 5.03/6.78/7.79 4.00/5.35/6.08\nSTTN-S(1,2) 2.14/2.75/3.08 5.00/6.69/7.63 4.01/5.36/6.08\nSTTN-S(1,3) 2.15/2.75/3.08 5.07/6.80/7.75 4.02/5.39/6.09\n4.6 Ablation Studies\nAblation studies have been made on the PeMSD7(M) dataset\nto verify the design of STTN. Here, PeMSD7(M) is selected,\nas it is much more challenging than PEMS-BAY in the sense\nof smaller scale and complex sptial-temporal dependencies.\nFor example, trafï¬c speeds in PeMSD7(M) have a larger\nstandard deviation than those in PEMS-BAY. For effective\nevaluation, we use only one spatial-temporal block with 64\nfeature channels for the spatial and temporal transformer.\n4.6.1 Multi-step Prediction vs. Autoregressive Prediction\nAutoregressive prediction is prevailing in trafï¬c ï¬‚ow fore-\ncasting, in which prediction for each time step is leveraged\nfor the succeeding predictions. However, autoregressive\nprediction would cause error prediction, due to the accumu-\nlated error for step-by-step prediction. Thus, it would ham-\nper long-term prediction. DCRNN [8] develops a sampling\nscheme to address this problem. In this paper, we explicitly\nmake long-term multi-step prediction from the historical\nobservations, rather than based on the predicted values.\nFor validation, we compare STGCN [7] and STTN with one\nST block in both autoregressive and multi-step prediction.\nTable 3 shows that STTN yields noticeable gains in MAE,\nMAPE and RMSE in comparison to STGCN and STTN with\nautoregressive prediction. It should be noted that prediction\nerror grows slowly for STTN in long-term prediction, when\ncompared with the other models.\n4.6.2 Effectiveness of Spatial Transformer\nWe demonstrate that the proposed spatial transformer can\nmodel dynamical spatial dependencies to improve the per-\nformance of long-term prediction. Variants of STTN are\nconsidered to evaluate the methods for modeling spatial\ndependencies. The baseline consists of one ï¬xed graph\nconvolutional layer realized by Chebyshev polynomial ap-\nproximation and one convolution-based sequence modeling\nmodule (GLU) adopted in STGCN [7]. Similar to [22],\nSTTN-S (local) is the attention-based method limited to the\nk-nearest neighboring nodes by masking the learned matrix\nfor dynamical dependencies. STTN-S(a,h) stands for STTN\nusing the proposed spatial transformer with a attention\nheads and hhidden layers. Consequently, the baseline only\nmodels ï¬xed spatial dependencies, while STTN-S (local) and\nSTTN-S(a,h) consider local and global dynamical spatial\ndependencies, respectively.\nTable 4 shows that STTN-S (1,1) outperforms the base-\nline by a large margin, especially for long-term prediction.\nThis fact implies that the spatial transformer can exploit\ndynamical spatial dependencies to achieve accurate long-\nterm prediction. Fig. 6 illustrates the averaged results for\none-day trafï¬c ï¬‚ow forecasting with short-term (5-min)\nand long-term (60-min) prediction on the test dataset for\nthe baseline and STTN-S(1,1). STTN-S(1,1) achieves better\nperformance for long-term predictions, especially in sharply\nchanging areas, e.g., the period of time steps [48,84] in\nFig. 6(b). We further evaluate the spatial transformers that\ncapture local and global spatial dependencies. According to\nTable 4, STTN-S (local) with the local constraint is inferior\nto STTN with the proposed spatial transformer. This fact\nsuggests that global dynamical spatial dependencies can\nfacilitate the trafï¬c ï¬‚ow forecasting, when compared with\nlocal dependencies. We also compare the learned spatial\ndependencies for STTN-S (local) and STTN-S (1,1) in Fig. 7.\nFig. 7(c) shows that the spatial dependencies STTN-S (lo-\ncal) relates the sensors within a local neighborhood, while\nSTTN-S(1,1) increase with the growth of time steps, due to\nthe small distances between most adjacent sensors.\nFurthermore, Table 4 provides the MAE, MAPE and\nRMSE obtained under various numbers of attention heads\nand hidden layers in spatial transformer. Trafï¬c ï¬‚ow fore-\ncasting performance is continuously improved when in-\ncreasing attention heads, as multi-head attention can model\nspatial dependencies in different latent subspaces to further\nutilize hidden patterns of dependenceis. On the contrary, a\nlarger number of hidden layers would trivially beneï¬t the\nperformance. This fact implies that one hidden layer would\nbe enough to capture the spatial dependencies for relatively\nsmall PeMSD7(M).\n4.6.3 Effectiveness of Temporal Transformer\nWe further validate that the proposed temporal transformer\nis efï¬cient to capture long-range temporal dependencies for\naccurate trafï¬c ï¬‚ow forecasting. The same baseline with\nï¬xed graph convolution is adopted as in Section 4.6.2 The\nreceptive ï¬elds of convolution kernel in GLU layer are\nadjusted to control the range of temporal dependencies.\nHere, we consider the convolution kernel size 3 (baseline),\n6 (Conv-6), 9 (Conv-9) and 12 (Conv-12). Table 5 shows\n10\n0 12 24 36 48 60 72 84 96 108 120 132 144 156 168 180 192 204 216 228 240 252 264 276 288\nTime_steps\n50\n55\n60\n65Speed km/h\nAverage Prediction results for short term predictions\nFixed spatial dependencies\nDynamical spatial dependencies\nGround_truth\n0 12 24 36 48 60 72 84 96 108 120 132 144 156 168 180 192 204 216 228 240 252 264 276 288\nTime_steps\n1.0\n1.5\n2.0MAE km/h Fixed spatial dependencies\nDynamical spatial dependencies\n(a) Short-term prediction (5 minutes)\n0 12 24 36 48 60 72 84 96 108 120 132 144 156 168 180 192 204 216 228 240 252 264 276 288\nTime_steps\n50\n55\n60\n65Speed km/h\nAverage Prediction results for long term predictions\nFixed spatial dependencies\nDynamical spatial dependencies\nGround_truth\n0 12 24 36 48 60 72 84 96 108 120 132 144 156 168 180 192 204 216 228 240 252 264 276 288\nTime_steps\n2\n4\n6\n8MAE km/h\nFixed spatial dependencies\nDynamical spatial dependencies (b) Long-term prediction (60 minutes)\nFig. 6. Average trafï¬c speeds and MAE for one-day trafï¬c ï¬‚ow forecasting using short-term and long-term trafï¬c prediction with the baseline (ï¬xed\nspatial dependencies) and STTN-S(1,1) (dynamical spatial dependencies).\n(a)  Adjacency Matrix(b)  STTN-S (local) at time step 1 and 12(c)  STTN-S(1,1) at time step 1 and 12\nFig. 7. Spatial dependencies learned for the ï¬rst 50 sensors in PeMSD7(M). The adjacency matrix only keeps local nodes.\nTABLE 5\nMAE, MAPE (%) and RMSE for PeMSD7(M) obtained by the ï¬xed\ngraph convolution with convolution kernel sizes 3 (Baseline), 6\n(Conv-6), 9 (Conv-9) and 12 (Conv-12) and STTN using the temporal\ntransformer with a attention heads and h hidden layers (STTN-T(a, h)).\nModel MAE MAPE (%) RMSE\nBaseline 2.18/2.92/3.43 5.12/7.18/8.65 4.04/5.57/6.58\nConv-6 2.16/2.87/3.35 5.07/7.05/8.42 4.01/5.49/6.44\nConv-9 2.16/2.87/3.34 5.08/7.08/8.48 4.02/5.50/6.43\nConv-12 2.16/2.85/3.31 5.07/7.02/8.36 4.00/5.45/6.37\nSTTN-T(1,1) 2.19/2.88/3.36 5.15/7.10/8.46 4.08/5.53/6.46\nSTTN-T(2,1) 2.19/2.88/3.36 5.15/7.10/8.46 4.08/5.53/6.46\nSTTN-T(4,1) 2.18/2.89/3.37 5.14/7.10/8.48 4.07/5.53/6.47\nSTTN-T(1,2) 2.17/2.86/3.32 5.10/7.05/8.40 4.05/5.50/6.43\nSTTN-T(1,3) 2.16/2.86/3.32 5.09/7.07/8.42 4.03/5.50/6.43\nthat long-term prediction can be improved with long-range\ntemporal dependencies determined by the large convolution\nkernel sizes. Thus, we substitute the GLU layer with the\nproposed temporal transformer to validate its effectiveness.\nTable 5 shows that temporal transformer is superior to\nthe ï¬xed graph convolution in long-term prediction. In\nFig. 8, we further illustrate the attention matrices of the\nï¬rst nine sensors for the temporal transformer. The weights\nfor temporal attention are different for different sensors. In\nsome cases, earliest time-steps are utilized with long-range\ndependencies for multi-step prediction.\nThe effects of the number of attention heads and hidden\nlayers are also evaluated for the proposed temporal trans-\nformer. Table 5 indicates that multi-head attention would\nnot beneï¬t the trafï¬c ï¬‚ow forecasting, as temporal depen-\ndencies are not as complex as spatial dependencies. We also\nï¬nd that trafï¬c ï¬‚ow forecasting tends to be improved by\n0 2 4 6 8 10\n0 0.071 0.13 0 0.14 0.15 0 0.0750.068 0.05 0.0480.0410.075\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0 2 4 6 8 10\n0 0.02 0.0120.0250.076 0.1 0.11 0.15 0.16 0.14 0.13 0 0.098\n0.00\n0.03\n0.06\n0.09\n0.12\n0.15\n0 2 4 6 8 10\n0 0.14 0.15 0.14 0.13 0.12 0.0960.0760.0630.0510.045 0 0.063\n0.00\n0.03\n0.06\n0.09\n0.12\n0.15\n0 2 4 6 8 10\n0 0.17 0.16 0.14 0.13 0.12 0.11 0.08 0.07 0.04 0.0280.0210.047\n0.03\n0.06\n0.09\n0.12\n0.15\n0 2 4 6 8 10\n0 0.12 0 0.0660.069 0.13 0.12 0.11 0.097 0 0.0950.073 0.08\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\n0 2 4 6 8 10\n0 0.084 0.1 0.086 0.11 0.098 0.09 0.1 0.1 0.0950.084 0 0.072\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0 2 4 6 8 10\n0 0 0.088 0 0.0810.0830.085 0 0.1 0 0.11 0.12 0.087\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0 2 4 6 8 10\n0 0.075 0.1 0.18 0.16 0 0.13 0.0820.0660.0590.0510.0350.056\n0.00\n0.03\n0.06\n0.09\n0.12\n0.15\n0 2 4 6 8 10\n0 0.27 0.18 0.16 0.16 0.11 0.0710.0390.0370.0280.0160.0120.026\n0.05\n0.10\n0.15\n0.20\n0.25\nFig. 8. Attention matrices of the ï¬rst nine sensors for the temporal\ntransformer generated by 15-min prediction on test dataset. Darker color\nindicates larger attention weights.\nincreasing the number of hidden layers.\n4.7 Model Conï¬gurations\nFinally, we discuss the model conï¬gurations for STTN, in-\ncluding the number of spatial-temporal blocks, the number\nof feature channels, the number of hidden layers, the num-\nber of attention heads and positional embedding. Table 6\nsummarizes the MAE for PeMSD7(M) obtained by STTNs\nwith various model conï¬gurations. MAE decreases by cas-\ncading multiple spatial-temporal blocks to jointly model\nspatial-temporal dependencies, but would be stable when\nenough spatial-temporal blocks are stacked (e.g., greater\nthan 2 blocks in Table 6).\nIn each spatial-temporal block, we investigate the effect\nof number of feature channels, hidden layers and atten-\ntion heads. Here, the number of feature channel indicates\nthe dimension of subspace in which dependencies are dy-\nnamically computed. The latent subspaces with higher di-\n11\nTABLE 6\nMAE for PeMSD7(M) obtained by STTN with various model conï¬gurations.\nModel # of # of # of hidden layers # of attention heads Positional MAE (15/30/45 min)conï¬gurations blocks feature channels (hS, hT ) ( aS, aT ) embedding\nBlocks\n1 [64,64] (1,1) (1,1) âœ“ 2.17/2.78/3.14\n2 [64,64] Ã—2 (1,1) (1,1) âœ“ 2.13/2.71/3.04\n3 [64,64] Ã—3 (1,1) (1,1) âœ“ 2.13/2.71/3.05\nChannels 1 [32,32] (1,1) (1,1) âœ“ 2.18/2.82/3.21\n1 [128,128] (1,1) (1,1) âœ“ 2.16/2.76/2.13\nLayers\n1 [64,64] (1,2) (1,1) âœ“ 2.16/2.77/3.13\n1 [64,64] (2,1) (1,1) âœ“ 2.15/2.75/3.08\n1 [64,64] (2,2) (1,1) âœ“ 2.13/2.72/3.05\nAttention\n1 [64,64] (1,1) (4,1) âœ“ 2.15/2.74/3.09\n1 [64,64] (1,1) (1,4) âœ“ 2.15/2.75/3.11\n1 [64,64] (1,1) (2,2) âœ“ 2.14/2.74/3.10\nEmbeddings\n1 [64,64] (1,1) (1,1) w/o S 2.18/2.84/3.26\n1 [64,64] (1,1) (1,1) w/o T 2.17/2.79/3.16\n1 [64,64] (1,1) (1,1) w/o ST 2.19/2.86/3.31\nmensions are demonstrated to exploit more information to\nachieve accurate prediction. In comparison to the temporal\ntransformer, trafï¬c ï¬‚ow forecasting would be improved by\nincreasing the number of hidden layers for the spatial trans-\nformer. This fact implies that long-term prediction tends to\nbe affected by the model capacity of spatial transformer.\nTable 6 also suggests that it would be better to jointly en-\nhance the capacity of spatial and temporal transformer. Fur-\nthermore, multi-head attention is demonstrated to facilitate\nSTTN in the trafï¬c ï¬‚ow forecasting, especially for long-term\nprediction. For real-world trafï¬c networks, relations among\nnodes are supposed to reside in different latent subspaces\nto evaluate the similarities of hidden patterns of trafï¬c\nï¬‚ows. This fact suggests that multi-head attention tends\nto be helpful for exploiting these hidden patterns, whereas\nthe performance gain led by additional hidden patterns of\nspatial dependencies would be limited. Finally, we ï¬nd that\nboth spatial and temporal positional embedding boost the\nperformance of trafï¬c ï¬‚ow forecasting with STTN.\n5 C ONCLUSION\nIn this paper, we propose a novel paradigm of spatial-\ntemporal transformer networks to improve the long-term\nprediction of trafï¬c ï¬‚ows. It can dynamically model various\nscales of spatial dependencies as well as capture long-\nrange temporal dependencies. Experimental results on two\nreal-world datasets demonstrate the superior performance\nof the proposed STTN, especially in long-term prediction.\nFurthermore, the proposed spatial transformer can be gen-\neralized for dynamical graph feature learning in a variety of\napplications. We will further investigate this topic in future.\nREFERENCES\n[1] U. Mori, A. Mendiburu, M. Â´Alvarez, and J. A. Lozano, â€œA review\nof travel time estimation and forecasting for advanced traveller\ninformation systems,â€ Transportmetrica A, vol. 11, no. 2, pp. 119â€“\n157, 2015.\n[2] W. Liu, Y. Zheng, S. Chawla, J. Yuan, and X. Xing, â€œDiscovering\nspatio-temporal causal interactions in trafï¬c data streams,â€ inProc.\n17th ACM SIGKDD Int. Conf. Knowl. Disc. Data Min., San Diego,\nCA, USA, Aug. 2011, pp. 1010â€“1018.\n[3] M. Lippi, M. Bertini, and P . Frasconi, â€œShort-term trafï¬c ï¬‚ow\nforecasting: An experimental comparison of time-series analysis\nand supervised learning,â€ vol. 14, no. 2, pp. 871â€“882, Jun. 2013.\n[4] J. Atwood and D. Towsley, â€œDiffusion-convolutional neural net-\nworks,â€ in Adv. Neural Inf. Process. Syst. 29, Barcelona, Spain, Dec.\n2016, pp. 1993â€“2001.\n[5] M. Defferrard, X. Bresson, and P . Vandergheynst, â€œConvolutional\nneural networks on graphs with fast localized spectral ï¬ltering,â€\nin Adv. Neural Inf. Process. Syst. 29, Barcelona, Spain, Dec. 2016, pp.\n3844â€“3852.\n[6] T. N. Kipf and M. Welling, â€œSemi-supervised classiï¬cation with\ngraph convolutional networks,â€ in 5th Int. Conf. Learn. Rep. (ICLR),\nToulon, France, Apr. 2017.\n[7] B. Yu, H. Yin, and Z. Zhu, â€œSpatio-temporal graph convolutional\nnetworks: A deep learning framework for trafï¬c forecasting,â€ in\nProc. 27th Int. Joint Conf. Artif. Intell., Stockholm, Sweden, Jul. 2018,\npp. 3634â€“3640.\n[8] Y. Li, R. Yu, C. Shahabi, and Y. Liu, â€œDiffusion convolutional\nrecurrent neural network: Data-driven trafï¬c forecasting,â€ in 6th\nInt. Conf. Learn. Rep. (ICLR), Vancouver, BC, Canada, May 2018.\n[9] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin,\nâ€œConvolutional sequence to sequence learning,â€ in Proc. 34th Int.\nConf. Mach. Learn., Sydney, NSW, Australia, Aug. 2017, pp. 1243â€“\n1252.\n[10] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang, â€œGraph wavenet\nfor deep spatial-temporal graph modeling,â€ in Proc. 28th Int. Joint\nConf. Artif. Intell., Macao, China, Aug. 2019, pp. 1907â€“1913.\n[11] W. Min and L. Wynter, â€œReal-time road trafï¬c prediction with\nspatio-temporal correlations,â€ Transp. Res. Part C Emerg. Technol.,\nvol. 19, no. 4, pp. 606â€“616, Aug. 2011.\n[12] J. Wang, W. Deng, and Y. Guo, â€œNew bayesian combination\nmethod for short-term trafï¬c ï¬‚ow forecasting,â€ Transp. Res. Part\nC Emerg. Technol., vol. 43, pp. 79â€“94, Jun. 2014.\n[13] A. Krizhevsky, I. Sutskever, and G. E. Hinton, â€œImagenet classiï¬ca-\ntion with deep convolutional neural networks,â€ in Adv. Neural Inf.\nProcess. Syst. 25, Lake Tahoe, NV , USA, Dec. 2012, pp. 1097â€“1105.\n[14] J. Long, E. Shelhamer, and T. Darrell, â€œFully convolutional net-\nworks for semantic segmentation,â€ in 2015 IEEE Conf. Comput. Vis.\nPattern Recognit. (CVPR), Boston, MA, USA, Jun. 2015, pp. 3431â€“\n3440.\n[15] X. Ma et al., â€œLearning trafï¬c as images: A deep convolutional\nneural network for large-scale transportation network speed pre-\ndiction,â€ Sensors, vol. 17, no. 4, p. 818, Apr. 2017.\n[16] J. Zhang, Y. Zheng, and D. Qi, â€œDeep spatio-temporal residual\nnetworks for citywide crowd ï¬‚ows prediction,â€ in Proc. 31st AAAI\nConf. Artif. Intell., San Francisco, CA, USA, Feb. 2017, pp. 1655â€“\n1661.\n[17] J. Zhang, Y. Zheng, J. Sun, and D. Qi, â€œFlow prediction in spatio-\ntemporal networks based on multitask deep learning,â€ 2019.\n[18] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfar-\ndini, â€œThe graph neural network model,â€ vol. 20, no. 1, pp. 61â€“80,\nJan. 2008.\n[19] J. Gilmer, S. S. Schoenholz, P . F. Riley, O. Vinyals, and G. E. Dahl,\nâ€œNeural message passing for quantum chemistry,â€ in Proc. 34th\nInt. Conf. Mach. Learn., Sydney, NSW, Australia, Aug. 2017, pp.\n1263â€“1272.\n[20] S. Guo, Y. Lin, N. Feng, C. Song, and H. Wan, â€œAttention based\nspatial-temporal graph convolutional networks for trafï¬c ï¬‚ow\n12\nforecasting,â€ in Proc. 33rd AAAI Conf. Artif. Intell., Honolulu, HI,\nUSA, Jan. 2019, pp. 922â€“929.\n[21] P . VeliË‡ckoviÂ´c et al., â€œGraph attention networks,â€ in 6th Int. Conf.\nLearn. Rep. (ICLR), Vancouver, BC, Canada, May 2017.\n[22] Z. Pan et al., â€œUrban trafï¬c prediction from spatio-temporal data\nusing deep meta learning,â€ in Proc. 25th ACM SIGKDD Int. Conf.\nKnowl. Disc. Data Min., Anchorage, AK, USA, Aug. 2019, pp. 1720â€“\n1730.\n[23] X. Ma, Z. Tao, Y. Wang, H. Yu, and Y. Wang, â€œLong short-term\nmemory neural network for trafï¬c speed prediction using remote\nmicrowave sensor data,â€ Transp. Res. Part C Emerg. Technol., vol. 54,\npp. 187â€“197, May 2015.\n[24] Y. Wu and H. Tan, â€œShort-term trafï¬c ï¬‚ow forecasting with spatial-\ntemporal correlation in a hybrid deep learning framework,â€ arXiv\npreprint arXiv:1612.01022, 2016.\n[25] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, â€œEmpirical evalua-\ntion of gated recurrent neural networks on sequence modeling,â€\nin NIPS 2014 Workshop Deep Learn., Montreal, QC, Canada, Dec.\n2014.\n[26] S. Hochreiter and J. Schmidhuber, â€œLong short-term memory,â€\nNeural Comput., vol. 9, no. 8, pp. 1735â€“1780, Nov. 1997.\n[27] A. Vaswani et al., â€œAttention is all you need,â€ in Adv. Neural Inf.\nProcess. Syst. 30, Long Beach, CA, USA, Dec. 2017, pp. 5998â€“6008.\n[28] S. Hochreiter, Y. Bengio, P . Frasconi, and J. Schmidhuber, â€œGradi-\nent ï¬‚ow in recurrent nets: The difï¬culty of learning long-term\ndependencies,â€ in A Field Guide to Dynamical Recurrent Neural\nNetworks. IEEE Press, 2001, pp. 237â€“244.\n[29] Z. Diao, X. Wang, D. Zhang, Y. Liu, K. Xie, and S. He, â€œDynamic\nspatial-temporal graph convolutional neural networks for trafï¬c\nforecasting,â€ in Proc. 33rd AAAI Conf. Artif. Intell., Hononlulu, HI,\nUSA, Jan. 2019, pp. 890â€“897.\n[30] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, â€œBERT: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,â€ in Proc. 2019 Conf. NAACL HLT, Minneapolis, MN,\nUSA, Jun. 2018, pp. 4171â€“4186.\n[31] S. Makridakis and M. Hibon, â€œARMA models and the Box-Jenkins\nmethodology,â€ J. Forecast., vol. 16, no. 3, pp. 147â€“163, May 1997.\n[32] C.-H. Wu, J.-M. Ho, and D.-T. Lee, â€œTravel-time prediction with\nsupport vector regression,â€ vol. 5, no. 4, pp. 276â€“281, Dec. 2004.\n[33] I. Sutskever, O. Vinyals, and V . L. Quoc, â€œSequence to sequence\nlearning with neural networks,â€ in Adv. Neural Inf. Process. Syst.\n27, Vancouver, BC, Canada, Dec. 2014, pp. 3104â€“3112.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6411944031715393
    },
    {
      "name": "Computer science",
      "score": 0.50880366563797
    },
    {
      "name": "Traffic flow (computer networking)",
      "score": 0.42177531123161316
    },
    {
      "name": "Computer network",
      "score": 0.20146000385284424
    },
    {
      "name": "Engineering",
      "score": 0.19128650426864624
    },
    {
      "name": "Electrical engineering",
      "score": 0.14011797308921814
    },
    {
      "name": "Voltage",
      "score": 0.10075724124908447
    }
  ],
  "institutions": [],
  "cited_by": 264
}