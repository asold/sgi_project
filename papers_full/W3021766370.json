{
  "title": "From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual Transfer with Multilingual Transformers",
  "url": "https://openalex.org/W3021766370",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4222820496",
      "name": "Lauscher, Anne",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223214301",
      "name": "Ravishankar, Vinit",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222725273",
      "name": "Vulić, Ivan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222398570",
      "name": "Glavaš, Goran",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3088675891",
    "https://openalex.org/W2739967986",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2963563735",
    "https://openalex.org/W2168199177",
    "https://openalex.org/W2626534681",
    "https://openalex.org/W2109439962",
    "https://openalex.org/W2875291789",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2963571341",
    "https://openalex.org/W3032816972",
    "https://openalex.org/W2964075320",
    "https://openalex.org/W2279316390",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2129494713",
    "https://openalex.org/W2126725946",
    "https://openalex.org/W2963165489",
    "https://openalex.org/W3016191782",
    "https://openalex.org/W2104583100",
    "https://openalex.org/W2741602058",
    "https://openalex.org/W2810095012",
    "https://openalex.org/W2517502945",
    "https://openalex.org/W1988584482",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2964084097",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W31002508",
    "https://openalex.org/W2995118574",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2995015695",
    "https://openalex.org/W2970037872",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2620558438",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W1552847225",
    "https://openalex.org/W2963809228",
    "https://openalex.org/W2127289991",
    "https://openalex.org/W2594021297",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2948384082",
    "https://openalex.org/W2250790822",
    "https://openalex.org/W2757931423",
    "https://openalex.org/W2963047628",
    "https://openalex.org/W3088382025",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2987225727",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2982180741",
    "https://openalex.org/W2120699290",
    "https://openalex.org/W2517456239"
  ],
  "abstract": "Massively multilingual transformers pretrained with language modeling objectives (e.g., mBERT, XLM-R) have become a de facto default transfer paradigm for zero-shot cross-lingual transfer in NLP, offering unmatched transfer performance. Current downstream evaluations, however, verify their efficacy predominantly in transfer settings involving languages with sufficient amounts of pretraining data, and with lexically and typologically close languages. In this work, we analyze their limitations and show that cross-lingual transfer via massively multilingual transformers, much like transfer via cross-lingual word embeddings, is substantially less effective in resource-lean scenarios and for distant languages. Our experiments, encompassing three lower-level tasks (POS tagging, dependency parsing, NER), as well as two high-level semantic tasks (NLI, QA), empirically correlate transfer performance with linguistic similarity between the source and target languages, but also with the size of pretraining corpora of target languages. We also demonstrate a surprising effectiveness of inexpensive few-shot transfer (i.e., fine-tuning on a few target-language instances after fine-tuning in the source) across the board. This suggests that additional research efforts should be invested to reach beyond the limiting zero-shot conditions.",
  "full_text": "From Zero to Hero: On the Limitations of Zero-Shot Cross-Lingual\nTransfer with Multilingual Transformers\nAnne Lauscher1∗, Vinit Ravishankar2∗, Ivan Vuli´c3, and Goran Glavaˇs1\n1Data and Web Science Group, University of Mannheim, Germany\n2Language Technology Group, University of Oslo, Norway\n3Language Technology Lab, University of Cambridge, UK\n1{anne,goran}@informatik.uni-mannheim.de,\n2vinitr@ifi.uio.no, 3iv250@cam.ac.uk\nAbstract\nMassively multilingual transformers pre-\ntrained with language modeling objectives\n(e.g., mBERT, XLM-R) have become a de\nfacto default transfer paradigm for zero-\nshot cross-lingual transfer in NLP, offering\nunmatched transfer performance. Current\ndownstream evaluations, however, verify their\nefﬁcacy predominantly in transfer settings\ninvolving languages with sufﬁcient amounts\nof pretraining data, and with lexically and\ntypologically close languages. In this work,\nwe analyze their limitations and show that\ncross-lingual transfer via massively multi-\nlingual transformers, much like transfer via\ncross-lingual word embeddings, is substan-\ntially less effective in resource-lean scenarios\nand for distant languages. Our experiments,\nencompassing three lower-level tasks (POS\ntagging, dependency parsing, NER), as well\nas two high-level semantic tasks (NLI, QA),\nempirically correlate transfer performance\nwith linguistic similarity between the source\nand target languages, but also with the size of\npretraining corpora of target languages. We\nalso demonstrate a surprising effectiveness\nof inexpensive few-shot transfer (i.e., ﬁne-\ntuning on a few target-language instances\nafter ﬁne-tuning in the source) across the\nboard. This suggests that additional research\nefforts should be invested to reach beyond the\nlimiting zero-shot conditions.\n1 Introduction and Motivation\nLabeled datasets of sufﬁcient size support super-\nvised learning and development in NLP. However,\ngiven the notorious tediousness, subjectivity, and\ncost of linguistic annotation (Dandapat et al., 2009;\nSabou et al., 2012; Fort, 2016), as well as a large\nnumber of structurally different NLP tasks, such\ndata typically exist only for English and a handful\n∗Equal contribution.\nof resource-rich languages (Bender, 2011; Ponti\net al., 2019; Joshi et al., 2020). The data scarcity\nissue renders the need for effective cross-lingual\ntransfer strategies: how can one exploit abundant\nlabeled data from resource-rich languages to make\npredictions in resource-lean languages?\nIn the most extreme scenario, termed zero-shot\ncross-lingual transfer, not even a single annotated\nexample is available for the target language. Re-\ncent work has placed much emphasis on the zero-\nshot scenario exactly; in theory, it offers the widest\nportability across the world’s (more than) 7,000\nlanguages (Pires et al., 2019; Artetxe et al., 2019;\nLin et al., 2019; Cao et al., 2020; Hu et al., 2020).\nThe current mainstay of cross-lingual transfer\nin NLP are approaches based on continuous cross-\nlingual representation spaces such as cross-lingual\nword embeddings (Ruder et al., 2019) and, most\nrecently, massively multilingual transformer mod-\nels that are pretrained on multilingual corpora us-\ning language modeling objectives (Devlin et al.,\n2019; Conneau and Lample, 2019; Conneau et al.,\n2020). The latter have de facto become the default\nparadigm for cross-lingual transfer, with a number\nof studies reporting their unparalleled cross-lingual\ntransfer performance (Pires et al., 2019; Wu and\nDredze, 2019; R¨onnqvist et al., 2019; Karthikeyan\net al., 2020; Wu et al., 2020, inter alia).\nKey Questions and Contributions. In this work,\nwe dissect the current state-of-the-art approaches to\n(zero-shot) cross-lingual transfer, and analyze a va-\nriety of conditions and underlying factors that criti-\ncally impact or limit the ability to conduct effective\ncross-lingual transfer via massively multilingual\ntransformer models. We aim to provide answers to\nthe following crucial questions.\n(Q1) What is the role of language (dis)similarity\nand language-speciﬁc corpora size for pretraining?\nCurrent cross-lingual transfer with massively mul-\narXiv:2005.00633v1  [cs.CL]  1 May 2020\ntilingual models is still primarily focused on trans-\nfer to either (1) languages that are typologically\nor etymologically close to English (e.g., German,\nScandinavian languages, French, Spanish), or (2)\nlanguages with large monolingual corpora, well-\nrepresented in the massively multilingual pretrain-\ning corpora (e.g., Arabic, Hindi, Chinese). Further-\nmore, Wu et al. (2020) suggest that pretrained trans-\nformers, much like static word embedding spaces,\nyield language representations that are easily (lin-\nearly) alignable between languages, but limit their\nstudy to major languages: Chinese, Russian, and\nFrench. However, language transfer with static\ncross-lingual word embeddings has been shown\nineffective when involving dissimilar languages\n(Søgaard et al., 2018; Vuli ´c et al., 2019) or lan-\nguages with small corpora (Vuli´c et al., 2020).\nWe therefore probe pretrained multilingual mod-\nels in diverse transfer settings encompassing more\ndistant languages, and languages with varying size\nof pretraining corpora. We demonstrate that, simi-\nlar to prior research in cross-lingual word embed-\ndings, transfer performance crucially depends on\ntwo factors: (1) linguistic (dis)similarity between\nthe source and target language and (2) size of the\npretraining corpus of the target language.\n(Q2) What is the role of a particular task in consid-\neration for transfer performance?\nWe conduct all analyses across ﬁve different tasks,\nwhich we roughly divide into two groups: (1)\n“lower-level” tasks (POS-tagging, dependency pars-\ning, and NER); and (2) “higher-level” language\nunderstanding tasks (NLI and QA). We show that\ntransfer performance in both zero-shot and few-\nshot scenarios largely depends on the “task level”.\n(Q3) Can we even predict transfer performance?\nRunning a simple regression model on available\ntransfer results, we show that transfer performance\ncan (roughly) be predicted from the two crucial fac-\ntors: linguistic (dis)similarity (Littell et al., 2017) is\na strong predictor of transfer performance in lower-\nlevel tasks; for higher-level tasks such as NLI and\nQA, both factors seem to contribute.\n(Q4) Should we focus more on few-shot transfer\nscenarios and quick annotation cycles?\nComplementary to the efforts on improving zero-\nshot transfer (Cao et al., 2020), we point to few-\nshot transfer as a very effective mechanism for\nimproving language transfer performance in down-\nstream tasks. Similar to the seminal “pre-neural”\nwork of Garrette and Baldridge (2013), our results\nsuggest that only several hours or even minutes of\nannotation work can “buy” a lot of performance\npoints in the low-resource target tasks. For all ﬁve\ntasks in our study, we obtain substantial (and in\nsome cases surprisingly large) improvements with\nminimal annotation effort. For instance, for de-\npendency parsing, in some target languages we\nimprove up to 40 UAS points by additional ﬁne-\ntuning on as few as 10 target language sentences.\nMoreover, the few-shot gains are most prominent\nexactly where zero-shot transfer fails: for distant\ntarget languages with small monolingual corpora.\n2 Background and Related Work\nFor completeness, we now provide a brief overview\nof 1) different approaches to cross-lingual transfer,\nwith a focus on 2) the state-of-the-art massively\nmultilingual transformer (MMT) models, and then\n3) position our work with respect to other studies\nthat examine different properties of MMTs.\n2.1 Cross-Lingual Transfer Methods in NLP\nIn order to enable language transfer, we must rep-\nresent the texts from both the source and target\nlanguage in a shared cross-lingual representation\nspace, which can be discrete or continuous. Lan-\nguage transfer paradigms based on discrete rep-\nresentations include machine translation of tar-\nget language text to the source language (or vice-\nversa) (Mayhew et al., 2017; Eger et al., 2018), and\ngrounding texts from both languages in multilin-\ngual knowledge bases (KBs) (Navigli and Ponzetto,\n2012; Lehmann et al., 2015). While reliable ma-\nchine translation hinges on the availability of suf-\nﬁciently large and in-domain parallel corpora, a\nprerequisite still unsatisﬁed for the vast majority\nof language pairs, transfer through multilingual\nKBs (Camacho-Collados et al., 2016; Mrkˇsi´c et al.,\n2017) is impaired by the limited KB coverage\nand inaccurate entity linking (Mendes et al., 2011;\nMoro et al., 2014; Raiman and Raiman, 2018).\nTherefore, recent years have seen a surge of\ncross-lingual transfer approaches based on contin-\nuous cross-lingual representation spaces. The pre-\nvious state-of-the-art approaches, predominantly\nbased on cross-lingual word embeddings (Mikolov\net al., 2013; Ammar et al., 2016; Artetxe et al.,\n2017; Smith et al., 2017; Glavaˇs et al., 2019; Vuli´c\net al., 2019) and sentence embeddings (Artetxe and\nSchwenk, 2019), are now getting replaced by mas-\nsively multilingual transformers based on language\nmodeling objectives (Devlin et al., 2019; Conneau\nand Lample, 2019; Conneau et al., 2020).\n2.2 Massively Multilingual Transformers\nMultilingual BERT (mBERT). At the core of\nBERT (Devlin et al., 2019) is a multi-layer trans-\nformer network (Vaswani et al., 2017). Its param-\neters are pretrained using two objectives: masked\nlanguage modeling (MLM) and next sentence pre-\ndiction (NSP). In MLM, some percentage of tokens\nare masked out and they need to be recovered from\nthe context. NSP predicts if two given sentences are\nadjacent in text and serves to model longer-distance\ndependencies spanning across sentence boundaries.\nLiu et al. (2019) introduce RoBERTa, a robust vari-\nant of BERT pretrained on larger corpora, show-\ning that NSP can be omitted if the transformer’s\nparameters are trained with MLM on sufﬁciently\nlarge corpora. Multilingual BERT (mBERT) is a\nBERT model trained on concatenated multilingual\nWikipedia corpora of 104 languages with largest\nWikipedias.1 To alleviate underﬁtting (for lan-\nguages with smaller Wikipedias) and overﬁtting,\nup-sampling and down-sampling are done via ex-\nponentially smoothed weighting.\nXLM on RoBERTa (XLM-R).XLM-R (Conneau\net al., 2020) is a robustly trained RoBERTa, ex-\nposed to a much larger multilingual corpus than\nmBERT. It is trained on the CommonCrawl-100\ndata (Wenzek et al., 2019) of 100 languages. There\nare 88 languages in the intersection of XLM-R’s\nand mBERT’s corpora; for some languages (e.g.,\nKiswahili), XLM-R’s monolingual data are several\norders of magnitude larger than with mBERT.\nThe “Curse of Multilinguality”. Conneau et al.\n(2020) observe the following phenomenon work-\ning with XLM-R: for a ﬁxed model capacity, the\ncross-lingual transfer performance improves when\nadding more pretraining languages only up to a cer-\ntain point. After that, adding more languages to pre-\ntraining degrades transfer performance. This effect,\ntermed the “curse of multilinguality”, can be mit-\nigated by increasing the model capacity (Artetxe\net al., 2019). However, this also suggests that the\nmodel capacity is a critical limitation to zero-shot\ncross-lingual transfer, especially when dealing with\nlower computational budgets.\n1https://github.com/google-research/\nbert/blob/master/multilingual.md\nIn this work (see §4), we suggest that a light-\nweight strategy to mitigate this effect/curse for\nimproved transfer performance is abandoning the\nzero-shot paradigm in favor of few-shot transfer.\nIf one targets improvements in a particular target\nlanguage, it is possible to obtain large gains across\ndifferent tasks at a very small annotation cost in\nthe target language, and without the need to train a\nlarger-capacity MMT from scratch.\n2.3 Cross-Lingual Transfer with MMTs\nA number of recent BERTology efforts, 2 which\nall emerged within the last year, aim at extending\nour understanding of the knowledge encoded and\nabilities of MMTs. Libovick `y et al. (2020) analyze\nlanguage-speciﬁc versus language-universal knowl-\nedge in mBERT. Pires et al. (2019) show that zero-\nshot cross-lingual transfer with mBERT is effective\nfor POS tagging and NER, and that it is more ef-\nfective between related languages. Wu and Dredze\n(2019) extend the analysis to more tasks and lan-\nguages; they show that transfer via mBERT is com-\npetitive to the best task-speciﬁc zero-shot transfer\napproach in each task. Similarly, Karthikeyan et al.\n(2020) prove mBERT to be effective for NER and\nNLI transfer to Hindi, Spanish, and Russian (note\nthat all languages are Indo-European and high-\nresource with large Wikipedias). Importantly, they\nshow that transfer effectiveness does not depend on\nthe vocabulary overlap between the languages.\nIn very recent work, concurrent to ours, Hu et al.\n(2020) introduce XTREME, a benchmark for eval-\nuating multilingual encoders encompassing 9 tasks\nand 40 languages in total.3 Their primary focus is\nzero-shot transfer evaluation, while they also ex-\nperiment with target-language ﬁne-tuning on 1,000\ninstances for POS tagging and NER; this leads to\nsubstantial gains over zero-shot transfer. While Hu\net al. (2020) focus on the evaluation aspects and\nprotocols, in this work, we provide a more detailed\nanalysis and understanding of the factors that hin-\nder effective zero-shot transfer across diverse tasks.\nWe also put more emphasis on few-shot learning\nscenarios, and approach it differently: we ﬁrst ﬁne-\ntune the MMTs on the (large) English task-speciﬁc\ntraining set and then ﬁne-tune/adapt it further with\na small number of target-language instances (e.g.,\neven with as few as 10 instances).\n2A name for the body of work analyzing the abilities of\nBERT and the knowledge encoded in its parameters.\n3Note that individual tasks in XTREME do not cover all 40\nlanguages, but rather signiﬁcantly smaller language subsets.\nArtetxe et al. (2019) and Wu et al. (2020) have\nanalyzed monolingual BERT models in differ-\nent languages to explain transfer effectiveness of\nMMTs. Their main conclusion is that, similar as\nwith static word embeddings, it is the topological\nsimilarities4 between the subspaces of individual\nlanguages captured by MMTs that enable effec-\ntive cross-lingual transfer. For cross-lingual word\nembedding spaces, the assumption of approximate\nisomorphism does not hold for distant languages\n(Søgaard et al., 2018; Vuli ´c et al., 2019), and in\nface of limited-size monolingual corpora (Vuli ´c\net al., 2020). In this work, we empirically demon-\nstrate that the same is true for zero-shot transfer\nwith MMTs: transfer performance substantially\ndecreases as we extend our focus to more distant\ntarget languages with smaller pretraining corpora.\n3 Zero-Shot Transfer: Analyses\nWe ﬁrst focus on Q1 and Q2 (see §1): we conduct\nzero-shot language transfer experiments on ﬁve\ndifferent tasks. We then analyze the drops with\nrespect to linguistic (dis)similarities and sizes of\npretraining corpora of target languages.\n3.1 Experimental Setup\nTasks and Languages. We experiment with –\na) lower-level structured prediction tasks: POS-\ntagging, dependency parsing, and NER and b)\nhigher-level language understanding tasks: NLI\nand QA. The aim is to probe if the factors contribut-\ning to transfer performance differ between these\ntwo task groups. Across all tasks, we experiment\nwith 21 languages in total.\nDependency Parsing (DEP). We use Universal De-\npendency treebanks (UD, Nivre et al., 2017) for\nEnglish and following target languages (from 8 lan-\nguage families): Arabic (AR), Basque (EU), (Man-\ndarin) Chinese ( ZH), Finnish ( FI), Hebrew ( HE),\nHindi (HI), Italian (IT), Japanese (JA), Korean (KO),\nRussian (RU), Swedish (SV), and Turkish (TR).\nPart-of-speech Tagging(POS). Again, we use UD\nand obtain the Universal POS-tag (UPOS) annota-\ntions from the same treebanks as with DEP.\nNamed Entity Recognition (NER). We use the NER\nWikiANN dataset from Rahimi et al. (2019). We\nexperiment with the same set of 12 target languages\nas in DEP and POS.\n4Wu et al. (2020) call it “latent symmetries”. This is es-\nsentially the assumption of approximate (weak) isomorphism\nbetween monolingual (sub)spaces (Søgaard et al., 2018).\nCross-lingual Natural Language Inference (XNLI).\nWe run our experiments on the XNLI corpus (Con-\nneau et al., 2018) created by crowd-translating the\ndev and test portions of the English Multi-NLI\ndataset (Williams et al., 2018) into 14 languages\n(French (FR), Spanish (ES), German (DE), Greek\n(EL), Bulgarian (BG), Russian (RU), Turkish (TR),\nArabic (AR), Vietnamese (VI), Thai (TH), Chinese\n(ZH), Hindi (HI), Swahili (SW), and Urdu (UR)).\nCross-lingual Question Answering (XQuAD). We\nrely on the XQuAD dataset (Artetxe et al., 2019),\ncreated by translating the 240 development para-\ngraphs (from 48 documents) and their corre-\nsponding 1,190 question-answer pairs of SQuAD\nv1.1 (Rajpurkar et al., 2016) to 11 languages (ES,\nDE, EL, RU, TR, AR, VI, TH, ZH, and HI). Given\na paragraph-question pair, the task is to identify\nthe exact span in the paragraph, which contains the\nanswer to the question. To enable the comparison\nbetween zero-shot and few-shot transfer (see §4),\nwe reserve a portion of 10 articles as the develop-\nment set in our experiments and report the ﬁnal\nperformance on the remaining 38 articles.\nFine-tuning. We adopt a standard ﬁne-tuning-\nbased approach for both mBERT and XLM-R in\nall tasks. Tokenization for both models is done in a\nstandard fashion.5 Particular classiﬁcation architec-\ntures on top of the MMTs depend on the task: for\nDEP we use a biafﬁne dependency parser (Dozat\nand Manning, 2017); for POS, we rely on a simple\nfeed-forward token-level classiﬁer; for NER, we\nfeed MMT representations to a CRF-based clas-\nsiﬁer, similar to (Peters et al., 2017). For XNLI,\nwe add a simple softmax classiﬁer taking the trans-\nformed representation of the sequence start token\nas input ([CLS] for mBERT, <s> for XLM-R);\nfor XQuAD, we pool the transformed represen-\ntations of all subword tokens as input to a span\nclassiﬁcation head – a linear layer that computes\nthe start and end of the answer span.\nTraining and Evaluation Details. We evaluate\nmBERT Base cased: L = 12 transformer layers\nwith the hidden state size of H = 768, A = 12\nself-attention heads. In addition, we work with\nXLM-R Base: L= 12, H = 768, A= 12.\n5We tokenize the input for each of the two models with\ntheir accompanying pretrained ﬁxed-vocabulary tokenizers:\nWordPiece tokenizer (Wu et al., 2016) with the vocabulary\nof 110K tokens for mBERT (we add special tokens [CLS]\nand [SEP]), and the SentencePiece BPE tokenizer (Sennrich\net al., 2016) with the vocabulary of 250K tokens for XLM-R.\nSpecial tokens <s> and </s> are also added.\nFor XNLI, we limit the input sequence length to\nT = 128 subword tokens and train in batches of32\ninstances. For XQuAD, we limit the input length\nof paragraphs to T = 384 tokens and the length of\nquestions to Q = 64 tokens. We slide over para-\ngraphs with a stride of 128 and train in 12-instance\nbatches. For XNLI and XQuAD, we grid-search for\nthe optimal learning rate λ∈{5 ·10−5,3 ·10−5},\nand number of training epochs n ∈{2,3}. For\nDEP and POS, we ﬁx the number of training epochs\nto 50; for NER, we train for 10 epochs. We train\nin batches of 128 sentences, with maximal se-\nquence length of T = 512 subword tokens. For all\ntasks we use Adam as the optimization algorithm\n(Kingma and Ba, 2015).\nWe report DEP performance in terms of Unla-\nbeled Attachment Scores (UAS).6 For POS, NER,\nand XNLI we report accuracy, and for XQuAD, we\nreport the Exact Match (EM) score.\n3.2 Results and Preliminary Discussion\nA summary of the zero-shot cross-lingual transfer\nresults per target language is provided in Table 1.\nUnsurprisingly, we observe substantial drops in per-\nformance for all tasks and all target languages com-\npared to the reference EN performance. However,\nthe extent of decrease varies greatly across lan-\nguages. For instance, NER transfer with mBERT\nfor IT drops a mere 0.8%, whereas it is 51% on JA\nNER. On XNLI, transferring with XLM-R yields\na moderate decrease of 6.1% for FR, but a much\nlarger drop of 20% for SW. At ﬁrst glance, it would\nappear – as suggested in prior work – that the drops\nin transfer performance primarily depend on the\nlanguage (dis)similarity, and that they are much\nmore pronounced for languages which are more\ndistant from EN (e.g., JA, ZH, AR, TH, SW).\nWhile we do not observe a notable exception\nto this pattern on the three lower-level tasks, lan-\nguage proximity alone does not explain many re-\nsults obtained on XNLI and XQuAD. For instance,\non XNLI (for both mBERT and XLM-R), the RU\nscores are comparable to those on ZH, while they\nare lower for HI and UR: this is despite the fact\nthat as Indo-European languages RU, HI, and UR\nare linguistically closer to EN than ZH. Similarly,\nwe observe comparable scores on XQuAD for TH,\n6Using Labeled Attachment Score (LAS) would make the\nsmall differences in annotation schemes between languages a\nconfounding factor for the analyses of transfer performance,\nimpeding insights into relevant factors of the study: language\nproximity and size of target language corpora.\nRU, and ES. Therefore, in what follows we seek\na more informed explanation of the obtained zero-\nshot transfer results.\n3.3 Analysis\nFor each task, we now analyze the correlations\nbetween transfer performance in the task and a)\nvarious measures of linguistic proximity (i.e., simi-\nlarity) between languages and b) the size of MMT\npretraining corpora of each target language.\nLanguage Vectors and Corpora Sizes. In or-\nder to estimate linguistic similarity, we rely on\nlanguage vectors from LANG 2VEC , which en-\ncode various linguistic features from the URIEL\ndatabase (Littell et al., 2017). We consider sev-\neral LANG 2VEC -based language vectors as follows:\nsyntax (SYN ) vectors encode syntactic proper-\nties, e.g., whether a subject appears before or af-\nter a verb; phonology (PHON ) vectors encode\nphonological properties of a language, e.g. the\nconsonant-vowel ratio; inventory (INV ) vec-\ntors encode presence or absence of natural classes\nof sounds, e.g., voiced uvulars; FAM vectors\ndenote memberships in language families,\ne.g., Indo-Germanic; and GEO vectors express or-\nthodromic distances for languages w.r.t. a ﬁxed\nnumber of points on the Earths surface. Language\nsimilarity is then computed as the cosine similarity\nbetween the languages’ correspondingLANG 2VEC\nvectors. Each aspect listed above (e.g., SYN, GEO,\nFAM) yields one scalar feature for our analysis.\nWe also include another feature: the z-\nnormalized size of the target language corpus used\nin MMT model pretraining (SIZE). 7\nCorrelation Analysis. We ﬁrst correlate individ-\nual features with the zero-shot transfer scores for\neach task and show the results in Table 2. Quite\nintuitively, the zero-shot scores for low-level syn-\ntactic tasks – POS and DEP – best correlate with\nsyntactic similarity ( SYN ). SYN similarity also\ncorrelates quite highly with transfer results for\nhigher-level tasks, except for XLM-R on XQuAD.\nPhonological similarity ( PHON ) correlates best\nwith the transfer results of mBERT on NER and\nXLM-R on XQuAD. Interestingly, for both high-\n7For XLM-R, we take the reported sizes of language-\nspeciﬁc portions of CommonCrawl-100 from Conneau et al.\n(2020); for mBERT, we take the sizes of language-speciﬁc\nWikipedias. We take the sizes of Wikipedia snapshots from\nOctober 21, 2018. While these are not the exact snapshots\non which mBERT was pretrained, the size ratios between\nlanguages should be roughly the same.\nEN ZH TR RU AR HI EU FI HE IT JA KO SV VI TH ES EL DE FR BG SW UR\nTask Model EN ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆ ∆\nDEP B 92.3 -40.9 -41.2 -23.5 -47.9 -49.6 -42.0 -26.7 -29.7 -10.6 -55.4 -53.4 -12.5 - - - - - - - - -\nPOS B 95.5 -33.6 -26.6 -9.5 -32.8 -33.9 -28.3 -14.6 -21.4 -6.0 -47.3 -37.3 -6.2 - - - - - - - - -\nNER B 92.3 -31.5 -6.5 -9.2 -29.2 -12.8 -8.5 -0.9 -9.2 -0.8 -51.1 -12.9 -1.9 - - - - - - - - -\nXNLI B 82.8 -13.6 -20.6 -13.5 -17.3 -21.3 - - - - - - - -11.9 -28.1 -8.1 -14.1 -10.5 -7.8 -13.3 -33.0 -23.4\nX 84.3 -11.0 -11.3 -9.0 -13.0 -14.2 - - - - - - - -9.7 -12.3 -5.8 -8.9 -7.8 -6.1 -6.6 -20.2 -17.3\nXQuAD B 71.1 -22.9 -34.2 -19.2 -24.7 -28.6 - - - - - - - -22.1 -43.2 -16.6 -28.2 -14.8 - - - -\nX 72.5 -26.2 -18.7 -15.4 -24.1 -22.8 - - - - - - - -19.7 -14.8 -14.5 -15.7 -16.2 - - - -\nTable 1: Zero-shot cross-lingual transfer performance on ﬁve tasks (DEP, POS, NER, XNLI, and XQuAD) with\nmBERT (B) and XLM-R (X). We show the monolingualEN performance and report drops in performance relative\nto EN for all target languages. Numbers in bold indicate the largest zero-shot performance drops for each task.\nSYN PHON INV FAM GEO SIZE\nTask Model P S P S P S P S P S P S\nDEP mBERT 0.93 0.92 0.80 0.81 0.51 0.02 0.73 0.63 0.70 0.71 0.75 0.55\nPOS mBERT 0.89 0.87 0.84 0.84 0.43 -0.04 0.66 0.63 0.78 0.81 0.65 0.50\nNER mBERT 0.56 0.62 0.78 0.86 0.23 0.02 0.39 0.49 0.75 0.88 0.27 0.23\nXNLI XLM-R 0.88 0.90 0.29 0.27 0.31 -0.11 0.63 0.54 0.54 0.74 0.70 0.76\nmBERT 0.87 0.86 0.21 0.08 0.29 0.04 0.61 0.47 0.55 0.67 0.77 0.91\nXQuAD XLM-R 0.69 0.53 0.85 0.81 0.62 -0.01 0.81 0.54 0.43 0.50 0.81 0.55\nmBERT 0.84 0.89 0.56 0.48 0.55 0.22 0.79 0.64 0.51 0.55 0.89 0.96\nTable 2: Correlations between zero-shot transfer performance with mBERT and XLM-R for different downstream\ntasks, across a set of target languages, with linguistic proximity scores (features SYN, PHON, INV, FAM and\nGEO) and the pretraining size of the target language corpora (feature SIZE). The results are reported in terms of\nPearson (P) and Spearman (S) correlation coefﬁcients. Highest correlations for each task-model pair are in bold.\nlevel tasks and both models, we observe very high\ncorrelations between transfer performance and the\nsize of pretraining corpora of the target language\n(SIZE ). On the other hand, SIZE shows substan-\ntially lower correlations with transfer performance\nacross lower-level tasks (DEP, POS, NER). We be-\nlieve that this is because language understanding\ntasks such as NLI and QA require rich represen-\ntations of the semantic phenomena of a language,\nwhereas low-level tasks require simpler structural\nknowledge of the language – to acquire the former\nwith distributional models, one simply needs much\nmore text than to acquire the latter.\nMeta-Regression. We observe high correlations\nbetween the transfer scores and several individual\nfeatures (e.g., SYN, PHON and SIZE ). Therefore,\nwe further test if even higher correlations can be\nachieved by a (linear) combination of the individual\nfeatures. For each task, we ﬁt a linear regression\nusing target language performances on the task as\nlabels. For each task, we have between 11 and\n14 target languages (i.e., instances for ﬁtting the\nlinear regression); we thus evaluate the regressor’s\nperformance, i.e., a Pearson correlation score be-\ntween the learned linear combination of features\nand transfer performance, via leave-one-out cross-\nTask Model Selected features P S MAE\nPOS B SYN (.99) 0.94 0.90 4.60\nDEP B SYN(.99) 0.93 0.92 5.77\nNER B PHON(.99) 0.69 0.82 9.45\nXNLI X SYN (.51); SIZE (.49) 0.84 0.85 2.01\nB SYN (.35); SIZE (.34), 0.89 0.90 2.78FAM (.31)\nXQuAD X PHON (.99) 0.95 0.83 2.89\nB SIZE (.99) 0.89 0.93 4.76\nTable 3: Results of the (linear) meta-regressor: predict-\ning zero-shot transfer performance with mBERT (B)\nand XLM-R (X), for each of our ﬁve tasks, from the set\nof features indicating language proximity to the source\nlanguage (EN) and the size of the target language cor-\npora in pretraining MMT pretraining. We list only the\nfeatures with assigned weights ≥ 0.01. P=Pearson;\nS=Spearman; MAE=Mean Average Error.\nvalidation (LOOCV). In order to allow for only a\nsubset of most useful features to be selected, we\nperform greedy forward feature selection: we start\nfrom an empty feature set and in each iteration add\nthe feature that boosts LOOCV performance the\nmost; we stop when none of the remaining features\nfurther improve the Pearson correlation.\nThe “meta-regression” results are summarized\nin Table 3. For each task-model pair, we list the fea-\ntures selected with the greedy feature selection and\nshow (normalized) weights assigned to each fea-\nture. Except for NER, a linear combination of fea-\ntures yields higher correlations with the zero-shot\ntransfer results than any of the individual features.\nThese results empirically conﬁrm our intuitions\nfrom the previous section that (structural) linguis-\ntic proximity (SYN ) explains zero-short transfer\nperformance for the low-level structural tasks (DEP\nand POS), but that it cannot fully explain perfor-\nmance in the two language understanding tasks.\nFor XNLI, the transfer results are best explained\nwith the combination of structural language proxim-\nity (SYN ) and the size of the target-language pre-\ntraining corpora (SIZE). For XQuAD with mBERT,\nSIZE alone best explains zero-short transfer scores.\nWe note that the features are also mutually very cor-\nrelated (e.g., languages closer to EN tend to have\nlarger corpora): if the regressor selects only one\nfeature, this does not suggest that other features do\nnot correlate with transfer results (see Table 2).\nThe coefﬁcients in Table 3 again indicate the\nimportance of SIZE for the language understand-\ning tasks and highlight our core ﬁnding: pretrain-\ning corpora sizes are stronger features for predict-\ning zero-shot performance in higher-level tasks,\nwhereas the results in lower-level tasks are more\naffected by typological language proximity.\n4 From Zero to Hero: Few-Shot\nMotivated by the low zero-shot results across many\ntasks and languages in §3, we now investigate Q4\nfrom §1, aiming to mitigate the transfer gap by\nrelying on inexpensive few-shot transfer settings\nwith a small number of target–language examples.\nExperimental Setup. We rely on the same mod-\nels, tasks, and evaluation protocols as described in\n§3.1. However, instead of ﬁne-tuning the model on\ntask-speciﬁc data in EN only, we continue the ﬁne-\ntuning process by feeding kadditional training ex-\namples randomly chosen from reserved target lan-\nguage data portions, disjoint with the test sets.8 For\nour lower-level tasks, we compare three sampling\nmethods: (i) random sampling (RAND ) of ktarget\nlanguage sentences, (ii) selection of the kshortest\n(SHORTEST ) and (iii) the klongest (LONGEST ) sen-\ntences. In all three cases, we only choose between\n8Note that for XQuAD, we performed the split on the\narticle level to avoid topical overlap. Consequently, k there\nrefers to the number of articles.\nsentences with ≥3 and ≤50 tokens. For XNLI\nand XQuAD, we run the experiments ﬁve times\nand report the average score.\n4.1 Results and Discussion\nThe results on each task, conditioned on the num-\nber of examples k and averaged across all target\nlanguages, are presented in Table 4. We note sub-\nstantial improvements in few-shot learning setups\nfor all tasks. However, the results also reveal no-\ntable differences between different types of tasks.\nFor higher-level language understanding tasks the\nimprovements are less pronounced; the maximum\ngains after seeing k = 1,000 target-language in-\nstances and 10 articles, respectively, are between\n2.1 and 4.57 points. On the other hand, the gains\nfor the lower-level tasks are massive: between\n14.11 and 26 percentage points, choosing the best\nsampling strategy for each task. Moreover, the\ngains in all lower-level tasks are substantial even\nwhen we add only 10 annotated sentences in the\ntarget language (on average, up to 14.45 percentage\npoints on DEP and 13.42 points on POS). What is\nmore, additional experiments (omitted for brevity)\nshow substantial gains for DEP and POS even when\nwe add fewer than 5 annotated sentences.\nA comparison of different sampling strategies\nfor the lower-level tasks is also shown in Figure 1.\nFor DEP and POS, the pattern is very clear and\nvery expected – adding longer sentences results in\nbetter scores. For NER, however, RAND appears\nto perform best, with a larger gap between RAND\nand SMALLEST . We hypothesize that this is due\nto very long sentences being relatively sparse with\nnamed entities, resulting in our model seeing a lot\nof negative examples; shorter sentences are also\nless helpful than for DEP and POS because they\nconsist of (conﬁrmed by inspection) a single named\nentity mention, without non-NE tokens.\nFigure 2 illustrates few-shot performance for in-\ndividual languages on one lower-level (DEP) and\none higher-level task (XQuAD), for different val-\nues of k.9 Across languages, we see a clear trend\n– more distant target languages beneﬁt much more\nfrom the few-shot data. Observe, for instance, SV\n(DEP, a) or DE (XQuAD, b). Both are closely\nrelated to EN, both generally have high scores\nin the zero-shot transfer, and both beneﬁt only\nmarginally from few-shot data points. We sus-\n9We provide the analogous ﬁgures for the remaining three\ntasks in the Appendix.\nk k = 10 k = 50 k = 100 k = 500 k = 1000\nTask Model Sampling k = 0 score ∆ score ∆ score ∆ score ∆ score ∆\nDEP MBERT\nRandom 59.00 69.32 10.32 75.20 16.20 77.32 18.32 82.26 23.26 84.32 25.32\nShortest 59.00 59.34 0.34 61.12 2.12 63.25 4.25 72.94 13.94 76.82 17.82\nLongest 59.00 73.45 14.45 78.45 19.45 80.04 21.04 84.16 25.16 85.73 26.73\nPOS MBERT\nRandom 72.65 81.69 9.04 86.45 13.80 88.10 15.45 91.75 19.10 93.07 20.42\nShortest 72.65 73.51 0.86 77.81 5.16 81.90 9.25 87.91 15.26 90.36 17.71\nLongest 72.65 86.07 13.42 89.63 16.98 90.94 18.29 93.42 20.77 94.21 21.56\nNER MBERT\nRandom 78.33 86.13 7.80 89.11 10.78 90.04 11.71 91.48 13.15 92.44 14.11\nShortest 78.33 76.04 -2.29 75.49 -2.84 76.97 -1.36 76.93 -1.40 80.19 1.86\nLongest 78.33 83.30 4.97 84.33 6.00 84.98 6.65 87.49 9.16 88.88 10.55\nXNLI MBERT Random 65.92 65.89 -0.03 65.08 -0.84 64.92 -1.00 67.41 1.49 68.16 2.24\nXLM-R Random 73.32 73.73 0.41 73.76 0.45 75.03 1.71 75.34 2.02 75.84 2.52\nk = 2 k = 4 k = 6 k = 8 k = 10\nXQUAD MBERT Random 45.62 48.12 2.50 48.66 3.04 49.34 3.72 49.91 4.29 50.19 4.57\nXLM-R Random 53.68 53.73 0.05 53.84 0.17 54.76 1.08 55.56 1.88 55.78 2.10\nTable 4: Results of the few-shot experiments with varying numbers of target-language examplesk. For each k, we\nreport performance averaged across languages and the difference (∆) with respect to the zero-shot setting.\nFigure 1: Heatmaps showing trends in performance improvement with few-shot data augmentation; the X-axis\nindicates the number of target-language instances k, whereas the Y-axis indicates the choice of sampling method.\npect that for such closely related languages, with\nenough MMT pretraining data, the model is able\nto extract missing knowledge from a few language-\nspeciﬁc data points; its priors for languages closer\nto EN are already quite sensible and a priori have\na smaller room for improvement. In stark contrast,\nKO (DEP, a) and TH (XQuAD, b), both exhibit\nfairly poor zero-shot performance and understand-\nably so, given their linguistic distance toEN. Given\nin-language data, however, both see rapid leaps in\nperformance, displaying a gain of almost 40% UAS\non DEP, and almost 5% on XQuAD. In a sense, this\ncan be seen as the models’ ability to rapidly learn\nto utilize the multilingual representation space to\nadapt its knowledge of the downstream task to the\ntarget language.\nOther interesting patterns emerge. Particularly\ninteresting are DEP results for JA and AR, where\nwe observe massive UAS improvements with only\n10 annotated sentences. For XQuAD, we observe a\nsubstantial improvement from only 2 in-language\ndocuments for TH. In sum, we see the largest gains\nfrom few-shot transfer exactly on languages where\nthe zero-shot transfer setup yields the worst perfor-\nmance: languages distant from EN and represented\nwith a small corpus in MMT pretraining.\n4.2 Further Discussion\nAs the results in §4.1 prove, moving to the few-shot\ntransfer setting can substantially improve perfor-\nmance and reduce the gaps observed with zero-shot\ntransfer, especially for lower-resourced languages.\nWhile additional ﬁne-tuning of MMT on the small\nnumber of target-language instances is computa-\ntionally cheap, the potential bottleneck lies in pos-\nsibly expensive data annotation. This is, especially\nfor minor languages, potentially a major issue and\ndeserves further analysis. What are the annotation\ncosts, and at which conversion rate do they trans-\nlate into performance points? Here, we provide\nsome ballpark estimates based on annotation costs\nreported by other researchers.\nNatural Language Inference. Marelli et al.\n(2014) reportedly paid $ 2,030 for 200k judge-\nments, which would amount to $0.01015 per NLI\ninstance and, in turn, to $10.15 for 1,000 annota-\n(a) DEP\n (b) XQuAD\nFigure 2: Few-shot results for each language with varying k for a) DEP, reported in terms of UAS score, and b)\nXQuAD accuracy. For DEP, kcorresponds to the number of sampled sentences, and for XQuAD, to the number\nof sampled articles. For the other tasks, we refer the reader to Appendix B.\ntions. In our few-shot experiments this would yield\nan average improvement of 2.24 and 2.54 accuracy\npoints for mBERT and XLM-R, respectively.\nQuestion Answering. Rajpurkar et al. (2016) re-\nport a payment cost of $9 per hour and a time effort\nof 4 minutes per paragraph. With an average of 5\nparagraphs per article, our few-shot scenario ( 10\narticles) roughly requires 50 paragraphs-level an-\nnotations, i.e., 200 minutes of annotation effort and\nwould in total cost around $30 (for respective per-\nformance improvements of 4.5 and 2.1 points for\nmBERT and XLM-R).\nOn the one hand, compared to language under-\nstanding tasks, our lower-level (DEP, POS) tasks\nare presumably more expensive to annotate, as they\nrequire some linguistic knowledge and annotation\ntraining. On the other hand, as shown in our few-\nshot experiments, we typically need much fewer\nannotated instances (i.e., we observe high gains\nwith already 10 target language sentences) for sub-\nstantial gains in these tasks.\nDependency Parsing. Tratz (2019) provide an\noverview of crowd-sourcing annotations for depen-\ndency parsing; they report obtaining a fully correct\ndependency tree from at least one annotator for\n72% of sentences. At the reported cost of $0.28\nper sentence this amounts to spending $280 for an-\nnotating 1,000 sentences. Somewhat shockingly,\nannotating 10 sentences with dependency trees –\nwhich for some languages like AR and JA corre-\nsponds to performance gains of 30-40 UAS points\n(see Figure 2) – amounts to spending merely $3-5.\nPart-of-Speech Tagging. Hovy et al. (2014) mea-\nsure agreement of crowdsourced POS annotations\nwith expert annotations; they crowdsource annota-\ntions for 1,000 tweets, at a cost of $0.05 for every\n10 tokens. With a total of 14,619 tokens in the cor-\npus, this amounts to approximately $73 for 1,000\ntweets, which is ≥1,000 sentences.10 Based on\nTable 4, 2 hours of POS annotation work trans-\nlates to gains of up to 18 points on average over\nzero-shot transfer methods.\nNamed Entity Recognition. Bontcheva et al.\n(2017) provide estimates for crowdsourcing anno-\ntation for named entity recognition; they pay $0.06\nper sentence, resulting in $60 cost for 1,000 an-\nnotated sentences. At a median pay of $11.37/hr,\nthis amounts to around 190 sentences annotated\nin an hour. In other words, in less than 3 hours,\nwe can collect more than 500 annotated examples.\nAccording to Table 4, this can result in gains of\n10-14 points on average, and even more for some\nlanguages (e.g., 27 points for AR).\nA provocative high-level question that calls for\nfurther discussion in future work can be framed as:\nare GPU hours effectively more costly11 than data\nannotations are in the long run? While MMTs are\nextremely useful as general-purpose models of lan-\nguage, their potential for some (target) languages\n10Note, however, that lower-level tasks do come with an\nadditional risk of poorer quality annotation, due to crowd-\nsourced annotators not being experts. Garrette and Baldridge\n(2013) report that even for truly low-resource languages (e.g.,\nKinyarwanda, Malagasy), it is possible to obtain ≈ 100 POS-\nannotated sentences.\n11ﬁnancially, but also ecologically (Strubell et al., 2019).\ncan be quickly unlocked by pairing them with a\nsmall number of annotated target-language exam-\nples. Effectively, this suggests leveraging the best\nof both worlds, i.e., coupling knowledge encoded\nin large MMTs with a small annotation effort.\n5 Conclusion\nResearch on zero-shot language transfer in NLP\nis motivated by inherent data scarcity: the fact\nthat most languages have no annotated data for\nmost NLP tasks. Massively multilingual trans-\nformers (MMTs) have recently been praised for\ntheir zero-shot transfer capabilities that mitigate the\ndata scarcity issue. In this work we have demon-\nstrated that, similar to earlier language transfer\nparadigms, MMTs perform poorly in zero-shot\ntransfer to distant target languages, and for lan-\nguages with smaller monolingual data for pretrain-\ning. We have presented a detailed empirical anal-\nysis of factors affecting transfer performance of\nMMTs across diverse tasks and languages. Our\nresults have revealed that structural language sim-\nilarity determines the transfer success for lower-\nlevel tasks like POS-tagging and parsing; on the\nother hand, the pretraining corpora size of the target\nlanguage is crucial for explaining transfer results\nfor higher-level language understanding tasks. Fi-\nnally, we have shown that the MMT potential on\ndistant and lower-resource target languages can be\nquickly unlocked if they are offered a handful of\nannotated target-language instances. This ﬁnding\nprovides strong evidence towards intensifying fu-\nture research efforts focused on the more effective\nfew-shot learning setups.\nAcknowledgements\nThis work is supported by the Eliteprogramm of\nthe Baden-W¨urttemberg Stiftung (AGREE Grant).\nThe work of Ivan Vuli´c is supported by the ERC\nConsolidator Grant LEXICAL (no 648909).\nReferences\nWaleed Ammar, George Mulcaire, Miguel Ballesteros,\nChris Dyer, and Noah A. Smith. 2016. Many lan-\nguages, one parser. Transactions of the ACL, 4:431–\n444.\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2017.\nLearning bilingual word embeddings with (almost)\nno bilingual data. In Proceedings of the 55th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 451–462.\nMikel Artetxe, Sebastian Ruder, and Dani Yo-\ngatama. 2019. On the cross-lingual transferabil-\nity of monolingual representations. arXiv preprint\narXiv:1910.11856.\nMikel Artetxe and Holger Schwenk. 2019. Mas-\nsively multilingual sentence embeddings for zero-\nshot cross-lingual transfer and beyond. Transac-\ntions of the Association for Computational Linguis-\ntics, 7:597–610.\nEmily M Bender. 2011. On achieving and evaluating\nlanguage-independence in nlp. Linguistic Issues in\nLanguage Technology, 6(3):1–26.\nKalina Bontcheva, Leon Derczynski, and Ian Roberts.\n2017. Crowdsourcing Named Entity Recognition\nand Entity Linking Corpora. In Nancy Ide and\nJames Pustejovsky, editors, Handbook of Linguistic\nAnnotation, pages 875–892. Springer Netherlands,\nDordrecht.\nJos´e Camacho-Collados, Mohammad Taher Pilehvar,\nand Roberto Navigli. 2016. Nasari: Integrating ex-\nplicit knowledge and corpus statistics for a multilin-\ngual representation of concepts and entities. Artiﬁ-\ncial Intelligence, 240:36–64.\nSteven Cao, Nikita Kitaev, and Dan Klein. 2020. Mul-\ntilingual alignment of contextual word representa-\ntions. In Proceedings of ICLR.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of ACL.\nAlexis Conneau and Guillaume Lample. 2019. Cross-\nlingual language model pretraining. In Advances\nin Neural Information Processing Systems , pages\n7057–7067.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. In Proceedings of\nthe 2018 Conference on Empirical Methods in Natu-\nral Language Processing. Association for Computa-\ntional Linguistics.\nSandipan Dandapat, Priyanka Biswas, Monojit Choud-\nhury, and Kalika Bali. 2009. Complex linguistic\nannotation—no easy way out!: a case from bangla\nand hindi pos labeling tasks. In Proceedings of the\nthird linguistic annotation workshop , pages 10–18.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nTimothy Dozat and Christopher D. Manning. 2017.\nDeep Biafﬁne Attention for Neural Dependency\nParsing. In Proceedings of ICLR . ArXiv:\n1611.01734.\nSteffen Eger, Johannes Daxenberger, Christian Stab,\nand Iryna Gurevych. 2018. Cross-lingual argumen-\ntation mining: Machine translation (and a bit of pro-\njection) is all you need! In Proceedings of the 27th\nInternational Conference on Computational Linguis-\ntics, pages 831–844.\nKar¨en Fort. 2016. Collaborative Annotation for Reli-\nable Natural Language Processing: Technical and\nSociological Aspects. John Wiley & Sons.\nDan Garrette and Jason Baldridge. 2013. Learning a\npart-of-speech tagger from two hours of annotation.\nIn Proceedings of NAACL-HLT, pages 138–147.\nGoran Glavaˇs, Robert Litschko, Sebastian Ruder, and\nIvan Vuli´c. 2019. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, com-\nparative analyses, and some misconceptions. In Pro-\nceedings of the 57th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 710–721.\nDirk Hovy, Barbara Plank, and Anders Søgaard. 2014.\nExperiments with crowdsourced re-annotation of a\npos tagging data set. In Proceedings of the 52nd An-\nnual Meeting of the Association for Computational\nLinguistics (volume 2: Short Papers) , pages 377–\n382.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual general-\nization. arXiv preprint arXiv:2003.11080.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the nlp\nworld. In Proceedings of ACL.\nK Karthikeyan, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multilin-\ngual bert: An empirical study. In Proceedings of\nICLR.\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof ICLR.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nDimitris Kontokostas, Pablo N Mendes, Sebastian\nHellmann, Mohamed Morsey, Patrick Van Kleef,\nS¨oren Auer, et al. 2015. Dbpedia–a large-scale, mul-\ntilingual knowledge base extracted from wikipedia.\nSemantic Web, 6(2):167–195.\nJindˇrich Libovick `y, Rudolf Rosa, and Alexander\nFraser. 2020. On the language neutrality of pre-\ntrained multilingual representations. arXiv preprint\narXiv:2004.05160.\nYu-Hsiang Lin, Chian-Yu Chen, Jean Lee, Zirui Li,\nYuyan Zhang, Mengzhou Xia, Shruti Rijhwani,\nJunxian He, Zhisong Zhang, Xuezhe Ma, Antonios\nAnastasopoulos, Patrick Littell, and Graham Neubig.\n2019. Choosing transfer languages for cross-lingual\nlearning. In Proceedings of ACL, pages 3125–3135.\nPatrick Littell, David R. Mortensen, Ke Lin, Kather-\nine Kairis, Carlisle Turner, and Lori Levin. 2017.\nURIEL and lang2vec: Representing languages as\ntypological, geographical, and phylogenetic vectors.\nIn Proceedings of the 15th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Volume 2, Short Papers , pages 8–14,\nValencia, Spain. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach. arXiv:1907.11692 [cs] . ArXiv:\n1907.11692.\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa\nBentivogli, Raffaella Bernardi, and Roberto Zampar-\nelli. 2014. A SICK cure for the evaluation of compo-\nsitional distributional semantic models. In Proceed-\nings of the Ninth International Conference on Lan-\nguage Resources and Evaluation (LREC’14), pages\n216–223, Reykjavik, Iceland. European Language\nResources Association (ELRA).\nStephen Mayhew, Chen-Tse Tsai, and Dan Roth. 2017.\nCheap translation for cross-lingual named entity\nrecognition. In Proceedings of the 2017 conference\non empirical methods in natural language process-\ning, pages 2536–2545.\nPablo N Mendes, Max Jakob, Andr´es Garc´ıa-Silva, and\nChristian Bizer. 2011. Dbpedia spotlight: shedding\nlight on the web of documents. In Proceedings of\nthe 7th international conference on semantic sys-\ntems, pages 1–8.\nTomas Mikolov, Quoc V Le, and Ilya Sutskever. 2013.\nExploiting similarities among languages for ma-\nchine translation. arXiv preprint arXiv:1309.4168.\nAndrea Moro, Alessandro Raganato, and Roberto Nav-\nigli. 2014. Entity linking meets word sense disam-\nbiguation: a uniﬁed approach. Transactions of the\nAssociation for Computational Linguistics , 2:231–\n244.\nNikola Mrkˇsi´c, Ivan Vuli´c, Diarmuid ´O S´eaghdha, Ira\nLeviant, Roi Reichart, Milica Ga ˇsi´c, Anna Korho-\nnen, and Steve Young. 2017. Semantic specializa-\ntion of distributional word vector spaces using mono-\nlingual and cross-lingual constraints. Transactions\nof the Association for Computational Linguistics ,\n5:309–324.\nRoberto Navigli and Simone Paolo Ponzetto. 2012. Ba-\nbelnet: The automatic construction, evaluation and\napplication of a wide-coverage multilingual seman-\ntic network. Artiﬁcial Intelligence, 193:217–250.\nJoakim Nivre, eljko Agi, Lars Ahrenberg, Lene An-\ntonsen, Maria Jesus Aranzabe, Masayuki Asahara,\nLuma Ateyah, Mohammed Attia, Aitziber Atutxa,\nand Liesbeth Augustinus. 2017. Universal Depen-\ndencies 2.1.\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn Proceedings of the 55th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 1756–1765.\nTelmo Pires, Eva Schlinger, and Dan Garrette.\n2019. How multilingual is Multilingual BERT?\narXiv:1906.01502 [cs]. ArXiv: 1906.01502.\nEdoardo Maria Ponti, Helen O’Horan, Yevgeni Berzak,\nIvan Vuli´c, Roi Reichart, Thierry Poibeau, Ekaterina\nShutova, and Anna Korhonen. 2019. Modeling lan-\nguage variation and universals: A survey on typo-\nlogical linguistics for natural language processing.\nComputational Linguistics, 45(3):559–601.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019. Mas-\nsively multilingual transfer for NER. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 151–164, Flo-\nrence, Italy. Association for Computational Linguis-\ntics.\nJonathan Raphael Raiman and Olivier Michel Raiman.\n2018. Deeptype: multilingual entity linking by neu-\nral type system evolution. In Thirty-Second AAAI\nConference on Artiﬁcial Intelligence.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392.\nSamuel R ¨onnqvist, Jenna Kanerva, Tapio Salakoski,\nand Filip Ginter. 2019. Is multilingual bert ﬂuent\nin language generation? In Proceedings of the First\nNLPL Workshop on Deep Learning for Natural Lan-\nguage Processing, pages 29–36.\nSebastian Ruder, Ivan Vuli ´c, and Anders Søgaard.\n2019. A survey of cross-lingual embedding models.\nJournal of Artiﬁcial Intelligence Research , 65:569–\n631.\nMarta Sabou, Kalina Bontcheva, and Arno Scharl.\n2012. Crowdsourcing research opportunities:\nlessons from natural language processing. In Pro-\nceedings of the 12th International Conference on\nKnowledge Management and Knowledge Technolo-\ngies, pages 1–8.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725.\nSamuel L. Smith, David H.P. Turban, Steven Hamblin,\nand Nils Y . Hammerla. 2017. Ofﬂine bilingual word\nvectors, orthogonal transformations and the inverted\nsoftmax. In Proceedings of ICLR.\nAnders Søgaard, Sebastian Ruder, and Ivan Vuli ´c.\n2018. On the limitations of unsupervised bilingual\ndictionary induction. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 778–\n788.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and policy considerations for\ndeep learning in nlp. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 3645–3650.\nStephen Tratz. 2019. Dependency Tree Annotation\nwith Mechanical Turk. In Proceedings of the First\nWorkshop on Aggregating and Analysing Crowd-\nsourced Annotations for NLP , pages 1–5, Hong\nKong. Association for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of NeurIPS, pages 5998–\n6008.\nIvan Vuli´c, Goran Glavaˇs, Roi Reichart, and Anna Ko-\nrhonen. 2019. Do we really need fully unsuper-\nvised cross-lingual embeddings? In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 4398–4409.\nIvan Vuli ´c, Sebastian Ruder, and Anders Søgaard.\n2020. Are all good word vector spaces isomorphic?\narXiv preprint arXiv:2004.04070.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Con-\nneau, Vishrav Chaudhary, Francisco Guzm ´an, Ar-\nmand Joulin, and Edouard Grave. 2019. CCNet: Ex-\ntracting high quality monolingual datasets from Web\ncrawl data. CoRR, abs/1911.00359.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nShijie Wu, Alexis Conneau, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Emerging cross-\nlingual structure in pretrained language models. In\nProceedings of ACL.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Lukasz Kaiser, Stephan Gouws,\nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith\nStevens, George Kurian, Nishant Patil, Wei Wang,\nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes,\nand Jeffrey Dean. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human\nand machine translation. CoRR, abs/1609.08144.\nA Full data\nPOS ar eu zh ﬁ he hi it ja ko ru sv tr\n0 62.76 67.25 61.92 80.93 74.11 61.62 89.58 48.27 58.27 86.00 89.32 68.89\n10 83.58 76.18 74.23 82.07 81.06 79.20 90.48 81.67 73.54 85.60 88.41 71.09\n50 90.12 82.30 83.98 82.79 89.80 84.14 93.65 89.28 75.02 88.54 91.18 77.92\n100 91.50 83.83 85.61 85.00 89.90 86.53 93.87 90.97 81.29 89.83 92.13 79.61\n500 94.66 87.72 89.78 88.83 94.83 90.34 95.75 93.76 87.34 93.75 93.55 87.56\n1000 95.35 88.65 91.40 90.79 95.85 93.39 96.30 94.28 90.29 94.31 94.86 89.00\nNER ar eu zh ﬁ he hi it ja ko ru sv tr\n0 63.16 83.81 60.87 91.44 83.16 79.56 91.51 41.21 79.39 83.08 90.43 85.86\n10 82.70 93.44 82.17 92.33 82.55 79.56 92.46 76.28 81.36 85.29 93.88 91.73\n50 86.94 94.37 86.83 93.12 86.99 85.57 92.83 78.53 86.27 89.52 96.02 92.06\n100 87.66 95.12 87.71 93.61 87.67 86.58 93.26 80.90 89.21 90.32 96.02 92.65\n500 90.46 95.36 90.44 94.55 91.03 88.17 94.03 81.50 91.44 91.17 97.09 94.40\n1000 90.46 95.95 91.27 94.31 91.52 90.44 94.96 85.63 92.17 92.84 97.33 94.39\nDEP ar eu zh ﬁ he hi it ja ko ru sv tr\n0 44.46 50.31 51.41 65.66 62.65 42.75 81.76 36.93 38.98 68.85 79.79 51.11\n10 71.00 57.23 57.73 65.13 74.75 56.76 85.80 77.67 52.76 75.23 79.55 55.22\n50 75.84 63.99 66.73 69.26 79.45 72.84 88.10 85.75 63.76 77.95 81.89 59.73\n100 78.50 65.70 69.91 70.25 80.98 78.47 88.54 88.35 68.00 79.78 81.93 62.54\n500 82.73 72.37 77.19 75.04 87.99 85.99 90.63 91.90 76.55 84.59 84.51 67.71\n1000 83.85 74.75 80.12 78.00 89.54 90.31 91.97 93.63 79.49 86.45 85.21 70.58\nXNLI fr es el bg ru tr ar vi th zh hi sw ur de\n0 75.05 74.71 68.68 69.50 69.34 62.18 65.53 70.88 54.69 69.26 61.50 49.84 59.38 72.34\n10 75.09 73.62 67.04 69.35 69.80 61.86 65.56 69.26 55.30 70.89 61.92 51.79 59.28 71.63\n50 74.60 73.91 66.44 68.37 69.05 60.99 64.63 70.29 51.17 71.32 60.08 49.95 58.83 71.43\n100 73.85 73.50 65.67 68.47 70.24 60.13 64.93 69.59 51.68 71.46 60.01 48.96 58.78 71.60\n500 75.36 74.97 68.04 71.03 70.59 63.21 66.71 72.38 58.12 72.81 64.06 52.26 61.15 73.09\n1000 76.20 76.24 68.73 71.73 71.41 65.01 67.04 72.35 59.19 73.47 64.75 52.47 62.38 73.21\nXQ U AD zh vi tr th ru hi es el de ar\n0 48.14 49.02 36.90 27.84 51.86 42.47 54.48 42.90 56.22 46.40\n2 48.93 50.50 40.87 39.43 51.07 44.19 56.14 46.46 56.66 46.99\n4 49.72 51.38 40.22 41.24 51.33 45.90 56.62 47.25 56.38 46.57\n6 50.81 50.81 41.59 44.04 51.20 46.81 57.14 47.16 56.40 47.45\n8 51.53 51.29 41.99 45.28 51.29 47.10 57.45 47.95 57.07 48.21\n10 50.87 51.57 42.55 46.05 52.05 48.06 57.03 48.60 57.29 47.82\nTable 5: Final results with a different number of target-language data instancesk, per language. Random sampling\nis used.\nB Score gain trends\n(a) XNLI\n(b) POS\n (c) NER\nFigure 3: Few-shot results for each language with varying kfor the remainder of tasks. All tasks report accuracy.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7988327741622925
    },
    {
      "name": "Transformer",
      "score": 0.7002229690551758
    },
    {
      "name": "Natural language processing",
      "score": 0.5855976343154907
    },
    {
      "name": "Transfer of learning",
      "score": 0.5537025332450867
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5478869676589966
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.49578461050987244
    },
    {
      "name": "Parsing",
      "score": 0.4885926842689514
    },
    {
      "name": "Transfer (computing)",
      "score": 0.48830246925354004
    },
    {
      "name": "Limiting",
      "score": 0.42511501908302307
    },
    {
      "name": "Linguistics",
      "score": 0.2844758629798889
    },
    {
      "name": "Physics",
      "score": 0.1155957281589508
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Parallel computing",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}