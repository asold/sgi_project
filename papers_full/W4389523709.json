{
  "title": "The Past, Present and Better Future of Feedback Learning in Large Language Models for Subjective Human Preferences and Values",
  "url": "https://openalex.org/W4389523709",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2167382435",
      "name": "Hannah Kirk",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2184999529",
      "name": "Andrew Bean",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A2434094854",
      "name": "Bertie Vidgen",
      "affiliations": [
        "University of Oxford"
      ]
    },
    {
      "id": "https://openalex.org/A3119509355",
      "name": "Paul Röttger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2185796115",
      "name": "Scott Hale",
      "affiliations": [
        "University of Oxford",
        "Hamedan University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2579486552",
    "https://openalex.org/W3125444967",
    "https://openalex.org/W4288431687",
    "https://openalex.org/W4385571841",
    "https://openalex.org/W4281392527",
    "https://openalex.org/W2953981431",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W4362514994",
    "https://openalex.org/W2739515658",
    "https://openalex.org/W4224866872",
    "https://openalex.org/W2950314731",
    "https://openalex.org/W3200980294",
    "https://openalex.org/W2739752763",
    "https://openalex.org/W2996314454",
    "https://openalex.org/W4309067685",
    "https://openalex.org/W4318908710",
    "https://openalex.org/W4288261525",
    "https://openalex.org/W4323929307",
    "https://openalex.org/W4379474731",
    "https://openalex.org/W3098467034",
    "https://openalex.org/W4308615640",
    "https://openalex.org/W2997607995",
    "https://openalex.org/W4224294371",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W2963119657",
    "https://openalex.org/W4313483770",
    "https://openalex.org/W3094997972",
    "https://openalex.org/W4378771755",
    "https://openalex.org/W2971032890",
    "https://openalex.org/W2962788902",
    "https://openalex.org/W4366196653",
    "https://openalex.org/W2034637506",
    "https://openalex.org/W3176618728",
    "https://openalex.org/W4287854995",
    "https://openalex.org/W4404752327",
    "https://openalex.org/W3034418628",
    "https://openalex.org/W4308860661",
    "https://openalex.org/W2251180427",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4221164017",
    "https://openalex.org/W4385734243",
    "https://openalex.org/W4299830158",
    "https://openalex.org/W4321392329",
    "https://openalex.org/W4226399820",
    "https://openalex.org/W4312091890",
    "https://openalex.org/W3104041537",
    "https://openalex.org/W1994785817",
    "https://openalex.org/W1774659654",
    "https://openalex.org/W3174090807",
    "https://openalex.org/W2125016606",
    "https://openalex.org/W3098258760",
    "https://openalex.org/W2963068985",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3024029620",
    "https://openalex.org/W4287116904",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W3102788578",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W4312091576",
    "https://openalex.org/W1962580118",
    "https://openalex.org/W3093233911",
    "https://openalex.org/W4365211596",
    "https://openalex.org/W2963393617",
    "https://openalex.org/W4383046813",
    "https://openalex.org/W4310419543",
    "https://openalex.org/W2104227891",
    "https://openalex.org/W4377297670",
    "https://openalex.org/W3034937117",
    "https://openalex.org/W4387389810",
    "https://openalex.org/W3201174429",
    "https://openalex.org/W3202173763",
    "https://openalex.org/W2946291122",
    "https://openalex.org/W4302306219",
    "https://openalex.org/W3184144760",
    "https://openalex.org/W2962955856",
    "https://openalex.org/W2250969425",
    "https://openalex.org/W3047185145",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W3158733382",
    "https://openalex.org/W4308902180",
    "https://openalex.org/W2773181053",
    "https://openalex.org/W4378718328",
    "https://openalex.org/W4200635688",
    "https://openalex.org/W2964123521",
    "https://openalex.org/W4225134894",
    "https://openalex.org/W4223477065",
    "https://openalex.org/W2958608582",
    "https://openalex.org/W2970442950",
    "https://openalex.org/W4296405185",
    "https://openalex.org/W4287674181",
    "https://openalex.org/W3008655042",
    "https://openalex.org/W2132007419",
    "https://openalex.org/W2963167310",
    "https://openalex.org/W3171850892",
    "https://openalex.org/W2963567240",
    "https://openalex.org/W2953522645",
    "https://openalex.org/W4296413526",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W4298181573",
    "https://openalex.org/W3043104149",
    "https://openalex.org/W4301369855",
    "https://openalex.org/W2951583236",
    "https://openalex.org/W4309088836",
    "https://openalex.org/W4285217123",
    "https://openalex.org/W3206428286",
    "https://openalex.org/W3168771811",
    "https://openalex.org/W2963202576",
    "https://openalex.org/W2619275098",
    "https://openalex.org/W2970990259",
    "https://openalex.org/W4300206276",
    "https://openalex.org/W4225006216",
    "https://openalex.org/W4385570984",
    "https://openalex.org/W3212281239",
    "https://openalex.org/W4306706478",
    "https://openalex.org/W2911227954",
    "https://openalex.org/W4285295903",
    "https://openalex.org/W3176477796",
    "https://openalex.org/W4387223260",
    "https://openalex.org/W4288273448",
    "https://openalex.org/W2962897543",
    "https://openalex.org/W2539350388",
    "https://openalex.org/W4299927810",
    "https://openalex.org/W4319453300",
    "https://openalex.org/W4221163727",
    "https://openalex.org/W2741642835",
    "https://openalex.org/W4312050653",
    "https://openalex.org/W3214105842",
    "https://openalex.org/W4287854312",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2962767298",
    "https://openalex.org/W3052814785",
    "https://openalex.org/W4307782108",
    "https://openalex.org/W2963228265",
    "https://openalex.org/W3174256071",
    "https://openalex.org/W4385571710",
    "https://openalex.org/W2964033619",
    "https://openalex.org/W4285484119",
    "https://openalex.org/W2992807692",
    "https://openalex.org/W4221103197",
    "https://openalex.org/W2787403940",
    "https://openalex.org/W2962739339"
  ],
  "abstract": "Human feedback is increasingly used to steer the behaviours of Large Language Models (LLMs). However, it is unclear how to collect and incorporate feedback in a way that is efficient, effective and unbiased, especially for highly subjective human preferences and values. In this paper, we survey existing approaches for learning from human feedback, drawing on 95 papers primarily from the ACL and arXiv repositories. First, we summarise the past, pre-LLM trends for integrating human feedback into language models. Second, we give an overview of present techniques and practices, as well as the motivations for using feedback; conceptual frameworks for defining values and preferences; and how feedback is collected and from whom. Finally, we encourage a better future of feedback learning in LLMs by raising five unresolved conceptual and practical challenges.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2409–2430\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nThe Past, Present and Better Future of Feedback Learning in Large\nLanguage Models for Subjective Human Preferences and Values\nHannah Rose Kirk1‡, Andrew M. Bean1, Bertie Vidgen1, Paul Röttger2, Scott A. Hale1,3\n1University of Oxford, 2Bocconi University, 3Meedan\n‡hannah.kirk@oii.ox.ac.uk\nAbstract\nHuman feedback is increasingly used to steer\nthe behaviours of Large Language Models\n(LLMs). However, it is unclear how to col-\nlect and incorporate feedback in a way that\nis efficient, effective and unbiased, especially\nfor highly subjective human preferences and\nvalues. In this paper, we survey existing ap-\nproaches for learning from human feedback,\ndrawing on 95 papers primarily from the ACL\nand arXiv repositories. First, we summarise\nthe past, pre-LLM trends for integrating hu-\nman feedback into language models. Second,\nwe give an overview of present techniques and\npractices, as well as the motivations for using\nfeedback; conceptual frameworks for defining\nvalues and preferences; and how feedback is\ncollected and from whom. Finally, we encour-\nage a better future of feedback learning in\nLLMs by raising five unresolved conceptual\nand practical challenges.\n1 Introduction\nIncorporating human feedback into Large Lan-\nguage Models (LLMs) is a welcome development\nto create models that are better aligned with hu-\nman preferences or values, and exhibit traits such\nas helpfulness, honesty and harmlessness (Askell\net al., 2021; Bai et al., 2022a) or safety, quality and\ngroundedness (Thoppilan et al., 2022). However,\nlearning from human feedback introduces new bi-\nases and challenges, and there are many unresolved\nquestions in this fast-moving field of research. It\nis important to take stock of current practices, pos-\nsible blindspots, and new frontiers of research, so\nthat tangible progress can continue to be made.\nIn this paper, we adopt the dual aim to both sur-\nvey existing literature on human feedback learning,\nthen draw on the regularities, commonalities and\ncritiques of this survey to also provide recommen-\ndations for future work. We review 95 articles\nthat use human feedback to steer, guide or tailor\nthe behaviours of language models. This includes\nmaking models’ responses more coherent and en-\ngaging (Lu et al., 2022); assisting models to infer\nuser intent (Ouyang et al., 2022); rejecting and re-\nbutting unsafe requests (Ganguli et al., 2022; Bai\net al., 2022a); or minimising the risk of halluci-\nnation (Nakano et al., 2021; Glaese et al., 2022).\nWe source articles primarily from the ACL and\narXiv repositories, coding each according to a de-\ntailed conceptual and methodological schema. Our\nreview makes three contributions:\n• The Past: We include articles released both\nbefore and after the advent of LLMs, which\navoids recency bias and allows us to track\ndevelopments through time.\n• The Present: We summarise current practices\nfor incorporating human feedback learning\ninto LLMs, such as reinforcement learning\nfine-tuning, supervised fine-tuning, and pre-\ntraining. We also document how feedback is\ncollected and from whom.\n• The Future: We draw on the findings of our\nreview to highlight five unresolved challenges\nin the field; two challenges are conceptual\nand three are practical. Conceptual challenges\nrevolve around the fundamental difficulty of\nspecifying a clear shared set of preferences\nand values. And, even if the conceptual chal-\nlenges can be resolved, practical challenges\nremain for converting abstract concepts into\nreliable signals to guide model behaviours.\nWe find that current processes of incorporating\nhuman feedback into LLMs often rely on unsatis-\nfactory simplifying assumptions about the stabil-\n2409\nity, universality and objectivity of human prefer-\nences and values. What counts as a “good”, “high-\nquality”, “preferred” or “value-aligned” output is\nonly objective in the abstract (Kirk et al., 2023a);\nso, we explicitly centre our review on subjective\nhuman preferences and values because we believe\nmost text attributes retain some degree of contex-\ntual subjectivity. With this in mind, we call for\nmore open, democratically-grounded and interdis-\nciplinary collaboration, supported by robust pro-\ncesses of external scrutiny, to decide how different\nvoices shape current and future LLMs.\n2 Methods\n2.1 Selecting Articles\nWe use a semi-automated method, casting a wide\nnet of keywords to retrieve articles, then manually\nassessing their relevance for our review (see Tab. 1\nfor keywords and Appendix A for a schema).\nInitial set (S0) We retrieve articles from two cor-\npora. First, we download the ACL anthology as\na .bib file. Second, we use the arXiv API with\nthe computation and language subclass (cs.CL) to\nfind new or industry-led preprints that are not peer-\nreviewed but have early impact on the field. We\nmatch titles with ≥ 2 keywords (n = 187), and\ndeduplicate dual-posted articles (n = 175).1\nInclusion criteria Two authors read the abstract\nand introduction of S0 articles, and included them\nif all the following questions were answered ‘yes’:\n1. Topic: Does the article seek alignment or\nadaptation of AI systems to human preferences\nand values? This criterion excludes articles\nthat functionally align aspects of language\nmodels e.g., word embedding alignment or\nsequence alignment.\n2. Modality: Does the article discuss language\nagents or language as its primary modality?\nThis criterion excludes any multimodal mod-\nels, delegate agents or games-based RL.\n3. Empirical: Does the article contain empiri-\ncal analysis or artefacts (such as experiments,\ndatasets or models)? This criterion excludes\nopinion papers, reviews or policy frameworks.\nTo ensure consistency, both authors coded the same\n70 articles, finding 82% agreement in inclusion\n1We match on title to increase relevancy and because some\nACL articles lack abstract metadata. In a sample of 100 re-\ntrieved articles, we determined that ≥ 2 keywords best bal-\nanced relevancy with the size of the retrieved set. The cut-off\nfor our automated collection is 17/02/2023.\ndecisions. We then discussed and refined the cri-\nteria before continuing. In total, 57 articles were\nincluded from S0.\nSnowballed set (S1) To address blindspots in our\nkeywords and corpora, we gather additional articles\nreferenced within S0 articles, regardless of where\nthey were published (n = 72), and then reapply the\ninclusion criteria to ensure relevance. This results\nin 38 additional articles from S1.\nWe further narrow our scope with two categori-\nsations on the 95 articles from S0 + S1:\nDominant contribution types We categorise ar-\nticles into: (i) those that evaluate or benchmark\nmodel’s capabilities, ethics or worldviews ( n =\n14); (ii) those that predict human preferences and\nvalues from social media data using specialised\nmodels not intended for other downstream genera-\ntive or classification tasks (n = 9); and (iii) those\nthat train or seek to align models with a general\nnotion of human preferred or valued text (n = 72).\nThe last category is the focus of our review.2\nUse of LLMs For the purposes of our review, we\ndefine LLMs as any encoder-only, decoder-only or\nencoder-decoder model that is pre-trained with self-\nsupervised learning over large internet corpora. As\na rule of thumb, we consider BERT (Devlin et al.,\n2019) and ELMO (Peters et al., 2018) among the\nfirst LLMs; so, any articles published before 2018\nfall outside our definition. Of the 72 train articles,\nwe cover 22 articles published in the pre-LLM era\nin our review of The Past (§3) and 50 articles using\nLLMs in The Present (§4).\n2.2 Coding Articles\nWe examine each article under two main themes.3\nThe Conceptual theme documents the motivations\nfor collecting human feedback; the definitions of\nhuman preferences or values; whether these are\nunderstood as universal or contextual/cultural; and\nthe level of interpretative freedom in applying pref-\nerence or value concepts. The Methodological\ntheme covers sub-categories on (i) annotation or\nlabour force details, such as how feedback was col-\nlected and from whom; and (ii) modelling details,\nsuch as how feedback is integrated into training\nand evaluation phases, and the target task. We also\n2There is overlap between categories—papers that fine-\ntune LLMs according to human preferences also evaluate\nthese trained models. See Appendix D for further detail.\n3The full code book is presented in Appendix B.\n2410\nKeywords Stemmed Keywords\nalignment, human, value, moral, ethic, feedback,\nreinforcement, instruction, red teaming, red-teaming,\npreferences, harm, honest, helpful, personalis, personaliz\nalign, human, value, moral, ethic, feedback, reinforc, instruct,\nred team, red-team, prefer, harm, honest, helpful, personalis,\npersonaliz\nTable 1: Keywords for retrieving articles from ACL and arXiv repositories. Highlighted keywords were not stemmed\ndue to irrelevant matches e.g., “value” as “valu” returns many false positives including the title word “evaluation”.\ncollect procedural details on academia vs industry\nauthorship, whether empirical artefacts (data and/or\nmodels) are publicly available, and if (to the best of\nour knowledge) the paper has been peer-reviewed.\n3 The Past\nIn this section, we review 22 articles released be-\ntween 2014-2019 that use human feedback but with\nolder generation model architectures. Highlighting\nthese works ensures that foundational research is\nadequately attributed for advancements in today’s\nmodels, and demonstrates the evolution from indi-\nrect or proxied human feedback.\n3.1 Conceptual Classification\nNone of the articles released in this period seek\nalignment to human values. Instead, they generate\ntext according to human preferences in machine\ntranslation (Mirkin et al., 2015; Mirkin and Meu-\nnier, 2015; Lawrence et al., 2017; Nguyen et al.,\n2017; Rabinovich et al., 2017; Kreutzer et al., 2018)\nand dialogue (Li et al., 2016; Mo et al., 2016; Li\net al., 2017b; Wang et al., 2017; Liu et al., 2018;\nJaques et al., 2019). Preferences are defined in\nboth personal and universal contexts, reflecting the\npersistent difficulties of separating the two. Ficler\nand Goldberg (2017) focus on modulating formal-\nity depending on context, while others focus on\nthe personalisation of language models, such as\nreflecting author personality in machine translation\n(Mirkin et al., 2015; Mirkin and Meunier, 2015;\nRabinovich et al., 2017); providing financial recom-\nmendations via chat bots (Den Hengst et al., 2019);\nor enabling customised online shopping (Mo et al.,\n2016). Most studies target human preferences as-\nsumed to be commonly-held and stable, such as\nword order (Futrell and Levy, 2019), sense making\n(De Deyne et al., 2016; Seminck and Amsili, 2017)\nand vocabulary matching (Campano et al., 2014;\nDubuisson Duplessis et al., 2017). In contrast,\nNguyen et al. (2017) and Kreutzer et al. (2017)\nacknowledge the noisiness of human feedback but\nattempt to extract a single, unified preference.\n3.2 Methodological Classification\nMost articles use pre-transformer recurrent neural\nnetworks such as LSTMs (Hochreiter and Schmid-\nhuber, 1997; Vaswani et al., 2017). Few articles use\ndirect human feedback, mostly in information re-\ntrieval tasks. In two cases, humans answer a series\nof yes/no questions to provide a more expressive\nreward for reinforcement learning (RL) (Li et al.,\n2017a; Lawrence and Riezler, 2018). Dhingra et al.\n(2017) use requests for additional information to\nform better queries with a binary ‘success/failure’\nreward. Lawrence et al. (2017) and Lawrence and\nRiezler (2018) compare forms of human feedback,\nfinding cardinal feedback to be more useful than\npairwise comparison.\nHuman feedback is an expensive and time-\nconsuming source of data to collect, which mo-\ntivates efforts to find reliable proxies (Lawrence\net al., 2017; Nguyen et al., 2017). Implicit feed-\nback methods attempt to utilise naturally-occurring\nsignals in human interactions, such as sentiment\n(Wang et al., 2017; Jaques et al., 2019) and re-\nsponse length (Campano et al., 2014). Other arti-\ncles define rules on desirable dialogue properties,\nsuch as length (Li et al., 2016), vocabulary align-\nment (Dubuisson Duplessis et al., 2017), or tone\n(Ficler and Goldberg, 2017), and score the agent\nfor achieving them. Only Li et al. (2016) apply RL\nto further train the model from the feedback.\nSimulating human feedback is also a commonly-\nused and cost effective approach where ‘feed-\nback’ is generated by measuring similarity to\nthe gold standard in pre-existing, human-labelled\ndatasets. Parallel translation corpora are a com-\nmon source of gold demonstrations, e.g., translated\nTED talks (Mirkin et al., 2015; Mirkin and Meu-\nnier, 2015; Nguyen et al., 2017) or European Parlia-\nment speeches (Kreutzer et al., 2017; Rabinovich\net al., 2017). User simulators typically use a ‘suc-\ncess/failure’ score for RL (Mo et al., 2016; Liu\net al., 2018), while ‘gold standard’ approaches use\na more complex loss function on output similarity\n(Mirkin and Meunier, 2015; Nguyen et al., 2017).\n2411\n4 The Present\nTurning our attention to the heart of our review,\nthis section discusses the 50 papers that incorporate\nhuman feedback to steer LLM behaviours.\n4.1 Conceptual Classification\nWe first seek to understand why human feed-\nback is collected . The motivations for eliciting\nfeedback form two groups. The first group gen-\nerally seeks value alignment , i.e., some notion\nof steering language models towards producing\nsocietally-desirable text (Zhao et al., 2021; Liu\net al., 2021a). We note a variety of vague goals\nsuch as to reduce “non-normative” (Peng et al.,\n2020) or “immoral” text (Liu et al., 2023c); to\ngenerate more “pro-social” (Liu et al., 2022) or\n“legitimate” text (Bakker et al., 2022); or to en-\ncourage that LLM technologies have a “positive\nimpact on society” (Liu et al., 2023b). Specific\nmotivations include minimising toxic or offensive\nlanguage (Dinan et al., 2019; Xu et al., 2021a; Ju\net al., 2022; Scheurer et al., 2022; Korbak et al.,\n2023); improving safety (Liu et al., 2021a; Xu et al.,\n2021b; Thoppilan et al., 2022; Ganguli et al., 2022;\nJin et al., 2022); adapting to ethical or moral sce-\nnarios (Forbes et al., 2020; Jiang et al., 2022; Jin\net al., 2022); or achieving political ideological bal-\nance (Liu et al., 2021b). The broad definitions of\nvalue alignment mostly assume some universality\nof value dimensions.4 However, some do seek to\nalign LLMs to specific groups, sets of values or\naccording to cultural context (Solaiman and Denni-\nson, 2021; Qiu et al., 2021; Bang et al., 2022).\nThe second group of articles is motivated by\nmore practical target concepts of improving model\ncapabilities, particularly when clear optimisation\nmetrics or programmatic rewards are lacking\n(Ziegler et al., 2019; Wu et al., 2021; Glaese et al.,\n2022; Bai et al., 2022b). Motivations often revolve\naround generating high-quality or human-preferred\noutputs (Gao et al., 2018; Böhm et al., 2019; Jaques\net al., 2020; Stiennon et al., 2020; Wang et al.,\n2021; Scheurer et al., 2022; Nguyen et al., 2022; Xu\net al., 2022), without much discussion of why this\nmatters or whether humans agree amongst them-\nselves what is “high-quality”. Specific target at-\ntributes include minimising repetitiveness (Arora\n4An example of a vague definition is that a “value-aligned\nsystem should make decisions that align with human decisions\nin similar situations and, in theory, make decisions which are\nunlikely to be harmful” (Nahian et al., 2020, p. 1).\net al., 2022); increasing coherence (Lu et al., 2022),\nusefulness (Liu et al., 2021a), engagingness (Gao\net al., 2020; Xu et al., 2021b; Lu et al., 2022), or\ninterest (Thoppilan et al., 2022); and producing\nhuman-like conversations (Hancock et al., 2019;\nJaques et al., 2020). Some seek greater explainabil-\nity and factuality in generated text (Nakano et al.,\n2021; Menick et al., 2022; Scheurer et al., 2022;\nThoppilan et al., 2022) or correctness in code (Kor-\nbak et al., 2023). Preferences can also be elicited\nfor customisation and personalisation (Majumder\net al., 2019; Zhou et al., 2021; Deng et al., 2022).\nThe boundary between preference- and value-\nmotivated aims is not always clear-cut. Commonly-\nadopted mixed motivations include helpful, hon-\nest and harmless behaviours, introduced by Askell\net al. (2021) and adopted by others (Bai et al.,\n2022b,a; Bakker et al., 2022; Menick et al., 2022).\nThoppilan et al. (2022) target safety, quality and\ngroundedness—concepts that similarly blur the\nlines between practical preferences and value-laden\njudgements. Even for instruction-tuning articles\nmotivated by inferring user intent, what Ouyang\net al. (2022) call “explicit” and “implicit” intent\nis synonymous with the helpfulness versus hon-\nesty/harmlessness distinction.5\n4.2 Methodological Classification\nWe primarily discuss how feedback is collected\n(§4.2.1) and integrated into LLMs (§4.2.2). We\nadditionally present an overview of target tasks and\nevaluation methods in Appendix C.\n4.2.1 Collecting Feedback\nFirst, we address how feedback is collected. Ex-\nplicit comparisons collected on model outputs are\nused to reveal the preferences of human raters (Gao\net al., 2018; Ziegler et al., 2019; Askell et al., 2021;\nJaques et al., 2020; Stiennon et al., 2020; Gan-\nguli et al., 2022; Glaese et al., 2022).6 More fine-\ngrained feedback includes binary or Likert scale\nquestions on text attributes (Nakano et al., 2021;\n5Ouyang et al. (2022, p.2) include “explicit intentions\nsuch as following instructions and implicit intentions such\nas staying truthful, and not being biased, toxic, or otherwise\nharmful.”\n6Usually, ratings are collected between two outputs (Bai\net al., 2022b; Ganguli et al., 2022) but others use four (Ziegler\net al., 2019) or even up to 9 items for comparison (Ouyang\net al., 2022). A null vote is predominately not included (nei-\nther of these outputs are good ) which may be a particular\nproblem for harm assessment (Ganguli et al., 2022)—though\nsome address ties in preference strength (e.g., Bai et al., 2022a;\nMenick et al., 2022).\n2412\nMenick et al., 2022; Thoppilan et al., 2022); natu-\nral language comments (Ju et al., 2022; Scheurer\net al., 2022); or edits (Hancock et al., 2019; Lu\net al., 2022; Liu et al., 2023c). Ideal demonstra-\ntions are used to ground norm-dependent or ethical\njudgements (Forbes et al., 2020; Zhao et al., 2021;\nPyatkin et al., 2022; Jiang et al., 2022; Jin et al.,\n2022), or in combination with ratings to prime\nmodel behaviour (Nakano et al., 2021; Wu et al.,\n2021; Ouyang et al., 2022; Bakker et al., 2022).\nSeveral articles collect negative feedback via ad-\nversarial examples (Dinan et al., 2019; Xu et al.,\n2021a,b; Glaese et al., 2022). Xu et al. (2022) test\nvarious feedback types including binary, free-form\nconversation, and fine-grained failure modes.\nHuman input can be further removed from di-\nrectly assessing model outputs. For example, simu-\nlating feedback with an “oracle” assumed to prefer\nspecific text attributes measured via automated met-\nrics (Wang et al., 2021; Nguyen et al., 2022; Korbak\net al., 2023) or predictions from a separate classifier\n(Peng et al., 2020; Liu et al., 2021b). In Bai et al.\n(2022b) human input defines the constitution but\nAI feedback is used to implement it during training.\nA seed of human generated examples guiding syn-\nthetic data generation also applies elsewhere (Bang\net al., 2022; Castricato et al., 2022; Honovich et al.,\n2022; Wang et al., 2022). Other articles adopt hu-\nman labels on pre-existing datasets (Böhm et al.,\n2019; Liu et al., 2021a; Arora et al., 2022; Jiang\net al., 2022), or leverage implicit feedback data\nfrom stories (Nahian et al., 2020) and social media\nsuch as Reddit or StackOverflow (Gao et al., 2020;\nAskell et al., 2021; Bai et al., 2022a). Feedback\ncan also be inferred from certain language patterns\nor emotive attributes in conversations with human\npartners (Hancock et al., 2019; Zhou et al., 2021).\nSecond, we address who feedback is collected\nfrom. Almost all articles use crowdworkers for\ntraining and/or evaluation, recruited from a variety\nof sources—including MTurk (Nahian et al., 2020;\nPeng et al., 2020; Jaques et al., 2020; Liu et al.,\n2021a,b; Qiu et al., 2021; Bai et al., 2022a; Gan-\nguli et al., 2022; Jin et al., 2022; Xu et al., 2022;\nJu et al., 2022); Upwork (Stiennon et al., 2020;\nBai et al., 2022a; Ganguli et al., 2022); ScaleAI\n(Ouyang et al., 2022; Stiennon et al., 2020; Ziegler\net al., 2019); and SurgeAI (Solaiman and Denni-\nson, 2021; Nakano et al., 2021; Bai et al., 2022b).\nWith ‘in-the-wild’ social media data, social me-\ndia users unknowingly become the ‘raters’ (Gao\net al., 2020; Askell et al., 2021; Bai et al., 2022a).\nOuyang et al. (2022) include OpenAI API users as\n“demonstrators”. At least 13 articles rely on their\nauthors for a variety of tasks: 7 writing seeds to\nscale synthetic data (Honovich et al., 2022; Wang\net al., 2022); hand-crafting conditioning prompts\n(Askell et al., 2021; Glaese et al., 2022); defining a\nconstitution (Bai et al., 2022b); specifying topics\nor starter questions (Solaiman and Dennison, 2021;\nBakker et al., 2022), and ethical scenarios (Zhao\net al., 2021); conducting evaluation (Stiennon et al.,\n2020; Ganguli et al., 2022; Lu et al., 2022) or gen-\nerating benchmarks (Bai et al., 2022a); and com-\npiling training tasks for crowdworkers (Qiu et al.,\n2021; Glaese et al., 2022). Even without direct\ninvolvement, authors can influence feedback col-\nlection by writing annotator guidelines.\n4.2.2 Integrating Feedback\nRL with Direct Human Feedback A reward\nsignal can first be extracted by asking actual hu-\nmans about their preferences for model outputs\nthen embedded into the LLM via RL fine-tuning.\nThe general recipe is as follows: (Step 1): Either\ntake an “off-the-shelf” pre-trained LLM (Lu et al.,\n2022); Or adapt this model via prompt-guiding\n(Askell et al., 2021; Bakker et al., 2022; Bai et al.,\n2022a; Glaese et al., 2022) or supervised fine-\ntuning and behavioural cloning over ideal demon-\nstrations (Ziegler et al., 2019; Stiennon et al., 2020;\nNakano et al., 2021; Ouyang et al., 2022; Menick\net al., 2022). 8 (Step 2): Generate multiple out-\nputs from this model, and employ crowdworkers\nto create a comparisons dataset. (Step 3): Train\na preference reward model (PM) on this feedback\nso “better” items are assigned higher score (Bai\net al., 2022a)—either a scalar reward for a given\nitem or an ELO score i.e., the log odds that A\n≻ B (Stiennon et al., 2020; Nakano et al., 2021;\nGlaese et al., 2022). The PM can be pre-trained\n7In Scheurer et al. (2022), the article relies on two authors\nto provide the feedback data and two other authors to do the\nhuman evaluation experiments.\n8For example, Lu et al. (2022) use a SOTA chinese chat-\nbot; Nakano et al. (2021) start with GPT-3 architecture (760M,\n13B, 175B); Bai et al. (2022a) use the Context-Distilled LM\n(52B) from Askell et al. (2021); Glaese et al. (2022) hand-\nauthor prompts to demonstrate ‘good’ behaviour in a Dialogue\nPrompted Chincilla model (70B); Stiennon et al. (2020) start\nwith versions of GPT-3 (1.3B, 6.7B) fine-tuned on filtered\nTL;DR Reddit dataset; Ziegler et al. (2019) use a fine-tuned\nGPT-2 model; Menick et al. (2022) use supervised fine-tuning\non Gopher family models with examples rated positively by\nlabellers; and Ouyang et al. (2022) fine-tune GPT-3 on demon-\nstrations of desired behaviours.\n2413\non naturally-occurring text and ratings e.g., from\nReddit or Stackoverflow (Askell et al., 2021; Bai\net al., 2022a). (Step 4): Fine-tune a RL policy\n(another LM) that generates text autoregressively,\nwhilst the PM provides a reward signal. Often, the\npolicy is updated using the PPO algorithm (Ziegler\net al., 2019; Stiennon et al., 2020; Nakano et al.,\n2021) and a KL-penalty term is applied to control\ndeviations from the base model (Jaques et al., 2019;\nZiegler et al., 2019; Stiennon et al., 2020; Nakano\net al., 2021; Menick et al., 2022; Ouyang et al.,\n2022; Liu et al., 2023c). This pipeline can be im-\nplemented in offline, online or batched settings (see\nZiegler et al., 2019). Modifications to the recipe\ninclude using recursive subtasks (Wu et al., 2021);\napplying a rule reward model in addition to the PM\nto penalise undesired outputs (Glaese et al., 2022);\nor using the PM to re-rank or reject sample gen-\nerations from the supervised model (Askell et al.,\n2021; Nakano et al., 2021; Glaese et al., 2022; Gan-\nguli et al., 2022; Bai et al., 2022a; Xu et al., 2022;\nBakker et al., 2022), which can match or outper-\nform optimising a model via RL (Menick et al.,\n2022; Thoppilan et al., 2022).\nRL with Indirect Human Feedback A reward\ncan be inferred without directly asking humans\nabout their preferences over model outputs. These\narticles skip the step of training a PM from compar-\nisons data, and instead infer preferences from tex-\ntual attributes of outputs (Jaques et al., 2020; Zhou\net al., 2021). It varies how far removed the human\ninput is, for example in designing the constitution\n(Bai et al., 2022a), in determining the automated\nmetric (Nguyen et al., 2022; Korbak et al., 2023) or\nin compiling the word lists to measure political bias\n(Liu et al., 2021b). Often another model is treated\nas the ‘oracle’ to simulate human rewards—Gao\net al. (2018), for example, simulate preferences\non two summaries with perfect, noisy and logistic\nnoisy “oracles” based on ROGUE scores; Wang\net al. (2021) take the reward as human revisions\nfrom parallel machine translation corpora; while\nothers deploy the rewards from a value, moral or\ntoxicity classifier trained on crowdworker labels to\nreinforce a generator (Qiu et al., 2021; Liu et al.,\n2022; Castricato et al., 2022; Pyatkin et al., 2022).\nGenerator and Discriminator Some use a uni-\nfied generator and classifier step to steer the LLM\naway from undesirable text (Arora et al., 2022), for\nexample using other fine-tuned LLMs to modify\nthe predicted probability in a base model for the\nnext token at decoding time (Liu et al., 2021a). A\ncombined model that functions as a generator and\na discriminator can be trained sequentially (Thop-\npilan et al., 2022) or jointly (Lu et al., 2022).\nPreference Pre-training Korbak et al. (2023)\nargue that incorporating human feedback in super-\nvised or RL fine-tuning phases is suboptimal. In-\nstead, they approach alignment in the pre-training\nphase of GPT-2, finding that conditional training\nis the most effective pre-training objective, and is\nmore robust than later fine-tuning an already pre-\ntrained model.\nPreference Fine-Tuning Human feedback can\nbe incorporated via supervised fine-tuning (Han-\ncock et al., 2019; Nahian et al., 2020; Jiang et al.,\n2022). For example, Gao et al. (2020) apply\ncontrastive learning with a GPT-2 based dialogue\nmodel over 133M pairs of human feedback data\nwith a loss designed to simultaneously maximise\nthe positive sample score and minimise the negative\nscore. Liu et al. (2023b) use “chain of hindsight”\nfine-tuning to include both positive and negative\nfeedback. Fine-tuning data is often filtered rela-\ntive to the value or preference goal (Solaiman and\nDennison, 2021; Xu et al., 2022; Bang et al., 2022).\nPeng et al. (2020) instead train a reward model\n(normative text classifier) but this reward is applied\nto the loss and backpropagated during fine-tuning.\nPrompting Prompting is a simple way to align\nLLMs with specified human preferences and values.\nJin et al. (2022) cast moral situations as multi-step\nprompts to elicit chain of thought reasoning in In-\nstructGPT, while Zhao et al. (2021) use zero- and\nfew-shot prompts for responsive questioning on\nunethical behaviours. Askell et al. (2021) show\nthat using a long prompt (4,600 words) from ideal\nauthor-written conversations is an effective alter-\nnative to fine-tuning in data-constrained scenarios.\nThey also use context distillation by training a new\nLLM to replicate the behaviour of another LLM\nthat is using a specific prompt.\n5 Challenges and Recommendations for\nthe Future\nDrawing on our analysis of the reviewed papers, we\nidentify five key challenges for future researchers.\nThese challenges are divided into conceptual and\npractical issues. The conceptual challenges (C1-\nC3) revolve around the difficulty of specifying a\n2414\nclear set (or sets) of preferences and values. Even\nassuming the resolution of the conceptual chal-\nlenges, practical challenges remain in converting\nconceptual ideals into empirical signals, which in\nturn steer language model behaviours.\n(C1) Preferences and values are not universal\n‘Aligning’ a language model requires a set of de-\nsired preferences or values to align with; but spec-\nifying such a set is an unresolved problem. One\npopular approach is to specify a minimal set of\nostensibly unobjectionable and widely-shared val-\nues, such as helpfulness, honesty and harmless-\nness (Bai et al., 2022a,b; Thoppilan et al., 2022).\nHowever, these values are only unobjectionable be-\ncause they are abstract and not precisely defined\n(Kirk et al., 2023a). These terms can be considered\nwhat Levi-Strauss and Laclau call ‘empty signi-\nfiers’ (Lévi-Strauss, 1987; Laclau, 2005); terms\nthat are viewed positively but are inscribed with\ndifferent meanings by different people. For exam-\nple, when Bai et al. (2022a) design a constitution to\nproduce outputs as “ethical and harmless as possi-\nble”, this can have varying interpretations based on\nan individual’s own ethical frameworks and socio-\ncultural background. Establishing priorities over\nsets of preferences or values to embed in LLMs,\nand ensuring consistent interpretation of conceptual\nmeaning across people, is a persistent challenge\nwhich cannot alone be resolved via purely techni-\ncal solutions. One possible approach is to draw on\nlegal theory, and values protected in human rights\nlaw (Solaiman and Dennison, 2021). Translating\nabstract shared values into decisions is a core func-\ntion of legal systems and legal theory offers a long\nhistory of scholarship which combines the philo-\nsophical and practical. One approach along these\nlines was proposed by Kirk et al. (2023b) which\napplies a principle of subsidiarity to govern the per-\nsonalisation of generative AI systems for different\nuse cases. We also advocate for anchoring closely\nto existing legal systems as a matter of democratic\nprinciple: it is dangerous for moral and value judge-\nments with broad societal impacts to be made by\nsmall independent groups.\n(C2) Preferences and values are inconsistently\ndefined Although the terminology of ‘prefer-\nences’ and ‘values’ implies some difference be-\ntween the two, the conceptual basis and norma-\ntive implications of this distinction is often unclear.\nColloquially, values are understood to be stronger\nthan preferences, and potentially carry greater nor-\nmative weight as guiding principles or life goals\n(Fischer, 2017). As such, users may have greater\nconcerns about an LLM misaligned with their val-\nues than with their preferences; So, it is important\nto be clear about which is being discussed. Within\nthe broad terms, there are many meanings: ‘pref-\nerences’ have been defined as ‘instrumental util-\nity’ (Dubuisson Duplessis et al., 2017; Gao et al.,\n2018; Nguyen et al., 2022), ‘stylistic taste’ (Mirkin\nand Meunier, 2015; Seminck and Amsili, 2017;\nJaques et al., 2020), and ‘behavioural principles’\n(Bai et al., 2022b; Castricato et al., 2022). ‘Val-\nues’ definitions are based on ‘instrumental and in-\ntrinsic value’ (Askell et al., 2021), ‘acceptable so-\ncial behaviours’ (Forbes et al., 2020; Bang et al.,\n2022), or ‘making decisions which are unlikely to\nbe harmful’ (Nahian et al., 2020). The differences\nbetween individual (subjective) and global (objec-\ntive) preferences is often blurred—for example,\nwhich properties of a “better” summary are univer-\nsal, and which depend on subjective appreciation,\nlike writing style and tone. Clearer definitions of\npreferences and values in the context of alignment\nwould serve to motivate and clarify what we are\naligning LLMs to.\n(C3) Human feedback is inherently incomplete\nAlignment via human feedback ultimately relies on\nLLMs being capable of successfully generalising\nfrom few examples to new cases and domains. This\nis because the space of possible behaviours over\nwhich to collect feedback is prohibitively large and\nis not fully known. An open question is the extent\nto which models generalise from partial human\nfeedback, especially when presented with data that\nis completely out-of-domain for their training or at\nthe margins of its distribution. For instance, if an\nLLM is trained with examples of safe responses to\nuser prompts which deny the Holocaust, it may gen-\neralise to different expressions of the same canoni-\ncal request. However, it will not necessarily learn\nhow to handle denials that the earth is round and\ndenials of vaccine efficacy, or have domain exper-\ntise for other harmful requests, such as users who\nask how to make a bomb or bio-weapon. Human\nvalues are considered to be fairly stable guiding\nprinciples that manifest similarly across situations\nfor a given individual (Fischer, 2017) but the same\ngeneralisation cannot be guaranteed of LLMs.\nSeveral related epistemological issues arise from\ntechnical details of the methods being used. Rein-\n2415\nforcement learning introduces a path-dependence\nproblem, where the particular order in which feed-\nback is given may change the quality of the final\nresults. As a result, it is difficult to know whether a\nlocal optimum is reached which is notably worse\nthan the global optimum. With any form of learning\nfrom feedback, language models may also overfit\nor appear to be aligned externally, but have per-\nsistent internal misalignment which manifests sub-\ntly in cases more distant from the training data\n(Perez et al., 2022). These challenges become yet\nmore convoluted when dealing with more complex\ntasks—an issue that Bowman et al. (2022) examine\nin their discussion of scalable oversight.\n(C4) Operationalising a “good” output is\ndifficult Even if a shared set of values could be\nagreed upon, converting these thick normative con-\ncepts into signals that models can use, such as by\ncollecting annotator ratings, is hard. Complex goal\noperationalisation is itself a motivator for collect-\ning feedback—when humans may not be able to\narticulate their preferences or write ideal demon-\nstrations but can rate outputs, a kind of “I know it\nwhen I see it” logic. However, training with human\nfeedback involves moving values and preferences\nfrom the abstract to particular survey or rating in-\nstruments, reinforcing differences in interpretation.\nTo reduce disagreements, some authors write very\nprescriptive and/or comprehensive guidelines for\nthe task in order to “make comparisons as unam-\nbiguous as possible” (Nakano et al., 2021, p.18).\nSeveral papers still find low inter-annotator agree-\nment with such prescriptive approaches (Stiennon\net al., 2020; Glaese et al., 2022; Bai et al., 2022a;\nOuyang et al., 2022). In other cases, annotators\nare explicitly allowed to use their own subjective\nassessment, to “interpret these concepts as they see\nfit” (Bai et al., 2022a, p.4), but then agreement\nbetween annotators is no longer ensured. When\nmultiple text attributes affect annotators’ prefer-\nences, it is hard to pin down what we are actually\nmeasuring. For example, Stiennon et al. (2020) and\nWu et al. (2021) condition their evaluation question\nas “how good is this summary, given that it is X\nwords long?”. Hypothetically, if “good” is subjec-\ntive then the question should be “how good is this\nsummary for individual Y?”. Some guidelines do\nask annotators to role-play or put themselves in the\nshoes of others, for example to infer the intent of a\nprompt (Ouyang et al., 2022) or question (Nakano\net al., 2021), but this may introduce further prob-\nlems, especially for value-laden judgements where\nthe rater may have a biased interpretation of how\nto apply another person’s values (Qiu et al., 2021).\nTo aid transparent communication, it should be\nclearly documented whether researchers aspire to\nfollow the prescriptive or subjective paradigm of\ndata annotation, rather than leaving it unspecified\n(Röttger et al., 2022; Kirk et al., 2023a). Increased\ninterdisciplinary communication with practitioners\nin other fields would impart wisdom on measuring\nthe perspectives and behaviours of human subjects.\nFor example, Human-Computer Interaction litera-\nture shows how interfaces and incentives can be op-\ntimally designed to avoid participant response bias\n(Deng and Poole, 2010; Dell et al., 2012; Hsieh and\nKocielnik, 2016); Experimental psychology and be-\nhavioural economics research show how the presen-\ntation of scales and order effects influence ratings\n(Friedman et al., 1994; Maeda, 2015; Westland,\n2022) and that human preferences are unstable, in-\ntransitive and vulnerable to experimental artefacts\n(Tversky, 1969; Lacy, 2001; Chiesa and Hobbs,\n2008; Lee et al., 2009; Chuang and Schechter,\n2015). Researchers should consider techniques to\nmodel the noise and distribution of human feedback\n(Ju et al., 2022) or establish post-hoc consensus\n(Bakker et al., 2022), rather than ignoring disagree-\nment by aggregating responses. However, there are\ntrade-offs: the specific nuances and minutiae cap-\ntured in fine-grained feedback might heighten bi-\nases and reduce generalisability when drawn from\nunrepresentative samples—which we now discuss.\n(C5) Crowdworkers and social media users are\nneither representative nor sufficient A degree\nof subjectivity persists even with prescriptive guide-\nlines and well-designed experimental instruments;\nSo, outcomes critically depend on who is inter-\npreting value or preference-based concepts. In\nthe majority of articles, fewer than 100 humans\nare employed to guide or evaluate language model\nbehaviours (Jaques et al., 2020; Stiennon et al.,\n2020; Nakano et al., 2021; Menick et al., 2022;\nBai et al., 2022a; Ouyang et al., 2022; Jin et al.,\n2022; Pyatkin et al., 2022), which is concerning\nfor ethically or morally ambiguous scenarios. It\nis striking that so few voices have so much power\nin shaping LLM behaviours—in Bai et al. (2022a)\njust 20 humans contributed 80% of the feedback\ndata, and in Nakano et al. (2021) the top 5 hu-\nmans contributed 50%. Workforces employed\nfor evaluation are similarly small, with some em-\n2416\nploying <25 workers (Scheurer et al., 2022; Cas-\ntricato et al., 2022; Gao et al., 2018; Liu et al.,\n2023b). Overwhelmingly, these humans are US-\nbased, English-speaking crowdworkers with Mas-\nter’s degrees and between the ages of 25-34. This\nresults in a non-democratic and non-diverse feed-\nback process, termed “the tyranny of crowdworker”\nby Kirk et al. (2023b), which has been shown to\nintroduce political and religious biases in model\nbehaviours (Perez et al., 2022). The limitations of\nrelying on the subjective interpretations of a small\nand non-representative work force are exacerbated\nby inadequate documentation. Only nine out of\n50 papers provided solid documentation, such as\ndemographic breakdowns (Stiennon et al., 2020;\nThoppilan et al., 2022; Bai et al., 2022a; Ganguli\net al., 2022; Glaese et al., 2022; Jin et al., 2022; Liu\net al., 2022; Ouyang et al., 2022; Liu et al., 2023c).\nOthers provide high-level details of the rater pool\nsuch as number of workers, hiring platform, or ag-\ngregate demographics. The majority of articles do\nnot document their workforce, nor discuss sample\nbiases or annotator artefacts.\nWhen soliciting human feedback, attempts\nshould be made to diversify who is given a voice,\nsuch as by applying democratic or jury-based prin-\nciples in how these voices are weighted (Gordon\net al., 2022) and by employing bottom-up partici-\npatory approaches (Martin Jr. et al., 2020; Birhane\net al., 2022; Zytko et al., 2022; Derczynski et al.,\n2023); Or to seek top-down sampling that better\nrepresents the population being studied (Bakker\net al., 2022). Mirroring the move in other areas of\nNLP to document and explore annotator disagree-\nment (Aroyo and Welty, 2015; Geva et al., 2019;\nNie et al., 2020; Prabhakaran et al., 2021; Davani\net al., 2022), each item of feedback should be asso-\nciated with a pseudo-anonymised annotator ID. So\nfar as privacy allows, documentation of annotator\nbackground should be provided in a data statement\n(Bender and Friedman, 2018).\n6 Conclusion\nThis review provided an overview of incorporating\nhuman feedback into LLMs, with a focus on subjec-\ntive preferences and values that lack ‘ground truth\nalignment’. We have witnessed two notable shifts\nin the field from past to present—first, a move away\nfrom specialist systems towards general purpose\nlanguage agents capable of handling many NLP\nsubtasks via instruction or open-ended dialogue;\nsecond, more use of direct human feedback which\nsurpasses the limitations of user simulations or au-\ntomated metrics.\nWhile the shift to incorporate human voices di-\nrectly into LLM development is welcome, it in-\ntroduces new challenges that require careful nav-\nigation. Some challenges are more tractable than\nothers—for example, practitioners will always have\nto deal with the complexities and intricacies of\nunstable and idiosyncratic preferences across end\nusers of their model, but can take practical steps to\nbetter approximate this distribution by diversifying\nthe recruitment of feedback providers.\nExternal scrutiny is crucial to ensure the in-\ntegrity and reliability of research efforts. Our re-\nview shows that many influential papers lack open\nand externally-validated peer review, especially\nthose published by big industry labs like Google\nDeepMind, Anthropic, Google, and OpenAI. Fur-\nthermore, the majority of reviewed papers do not\nrelease model artefacts, or only do so behind a\npaywalled API. To foster progress, we advocate\nfor a greater degree of open, interdisciplinary and\ndemocratically-grounded discussion on how hu-\nmans can meaningfully shape future LLM be-\nhaviours in a way which is well-bounded, oper-\nationalisable, and equitable.\n7 Limitations\nWe discuss limitations associated with our review:\nApplying topic exclusion We exclude articles on\nthe basis of being unrelated to the topic of value\nor preference alignment, but found it consistently\ndifficult to draw a clear distinction between arti-\ncles in and out of scope. For validation purposes,\nwe had both reviewers read a portion of the arti-\ncles, and found the cases of disagreement help-\nful to highlight this challenge. One such example\nwas with two articles using similar methods to ap-\nproach translation which we initially classified dif-\nferently, Kreutzer et al. (2017) and Lawrence et al.\n(2017). The papers primarily treat translation as\nan objective task focused on BLEU scores, which\nwould make them out of scope. However, trans-\nlation inherently involves stylistic and subjective\njudgements, and the methods developed seek to\nreplicate these judgements from the training data,\nblurring the distinction. Honesty is another target\nconcept with these issues—whether in-text claims\nare referenced is fairly black and white, but whether\nan end user ascribes more trust to the system be-\n2417\ncause of these references is subject to idiosyncratic\nepistemology. We use this case to highlight the\nweaknesses of creating a dichotomy between sub-\njective and objective preferences in practice.\nRelated NLP subfields A related issue is where\nto draw the line for what is and is not in scope.\nSome narrowing was needed to make the review\nfocused, feasible and instrumentally useful to fu-\nture practitioners. However, technically fairness\nand bias are values of intrinsic utility—hence their\ninclusion in many AI principles around the world\n(Jobin et al., 2019). That said, there exists a very\nwide and distinct literature on fairness and bias\nin LLMs that would be too expansive for this re-\nview (see, e.g., Chang et al., 2019; Lucy and Bam-\nman, 2021; Abid et al., 2021; Nadeem et al., 2021;\nKirk et al., 2021; Smith et al., 2022). Similarly,\nthere are sub-literatures on other aspects of LLM\nbehaviours—such as toxicity (Gehman et al., 2020;\nWelbl et al., 2021), truthfulness (Lin et al., 2022)\nor hallucination (Ji et al., 2022). We explicitly fo-\ncus on papers that target some notion of human\npreferences and values in their motivations, but the\nchallenges raised from our review can be applied to\nother fields which similarly suffer from subjectiv-\nity in interpretative scope—e.g., carefully deciding\nwho the human labellers are and what guidelines\ngovern their interpretation of concepts.\nBlindspots in reviewed articles Blindspots\ncome from a number of sources. First,keyword and\ncorpora blindspots: We ground our initial review\non articles from arXiv and ACL using a set of de-\nfined keywords. We attempt to mitigate blindspots\nby snowballing related and relevant articles out-\nside our initial collection; however, it is almost\ncertain that we have missed some papers in the\nfield as a whole. Second, language blindspots: Our\nreview only contains articles written in English,\nlimited by the expertise of authors who acted as\nthe coders. This bias however reflects the domi-\nnance of English in academic publishing in general,\nbut English language proficiency may gatekeep the\nconcepts and voices already contributing to LLM\ndevelopment. Third, community blindspots: we\nonly look at academic papers—but issues surround-\ning large language model behaviours or alignment\nhave become a hot topic of conversation on blogs\nand social media forums. We inherently exclude\nsuch discussions from these other stakeholder com-\nmunities. Fourth, modality blindspots: there is a\nrich history of using RL to align models in other\nmodalities, such as delegate agents acting in toy or\ngame worlds (see, e.g., Christiano et al., 2017). We\ndo not cover the insights from these related litera-\ntures. Finally, temporal blindspots: research into\nLLMs is a fast-paced field—in one week, there can\nbe as many as 500 articles posted on the cs.CL sub-\nclass of arXiv. Inevitably, other influential articles\nhave been released after the review was completed\nand more were released during its peer review pe-\nriod. A good example of this is Rafailov et al.\n(2023) who introduce Direct Preference Optimisa-\ntion, a technique that could substantially change\nhow people approach feedback learning in the fu-\nture. Other relevant papers that appeared after the\ncut-off for this review include Dong et al. (2023);\nHosking et al. (2023); Liu et al. (2023d,a); Song\net al. (2023); Yuan et al. (2023); Wu et al. (2023);\nZhou et al. (2023). With the field’s rapid develop-\nments, any review paper runs the risk of lagging\nbehind the latest research. However, given the sub-\nstantial number of articles that we did review, we\nexpect many of the general findings and highlighted\nchallenges to apply in upcoming future work.\nExternal scrutiny of reviewed articles We con-\nsciously made the decision to include articles which\nhave not yet been peer reviewed to stay ahead of\nthe curve with early-released pre-prints and also to\ntrack industry contributions (which are often not ex-\nternally peer reviewed). In the 22 papers appearing\nthe The Past section, 18 were peer reviewed. Of the\n50 papers appearing in The Present section, only 21\nwere clearly peer-reviewed. It is a contentious is-\nsue that many influential papers lack standard prac-\ntices of external scrutiny and rigorous academic\nbackstops, though often industry-authored papers\ndo undergo a process of internal review before a\npreprint is released.\nAcknowledgements\nThis paper received funding from a MetaAI Dyn-\nabench grant as part of a research agenda on opti-\nmising feedback between human-and-model-in-the-\nloop. H.R.K.’s PhD is supported by the Economic\nand Social Research Council grant ES/P000649/1.\nA.M.B.’s PhD is supported by the Clarendon Fund.\nP.R. received funding through the INDOMITA\nproject (CUP number J43C22000990001) and the\nEuropean Research Council (ERC) under the Eu-\nropean Union’s Horizon 2020 research and innova-\ntion program (No. 949944, INTEGRATOR).\n2418\nReferences\nAbubakar Abid, Maheen Farooqi, and James Zou. 2021.\nPersistent Anti-Muslim Bias in Large Language Mod-\nels. In Proceedings of the 2021 AAAI/ACM Confer-\nence on AI, Ethics, and Society , AIES ’21, pages\n298–306, New York, NY , USA. Association for Com-\nputing Machinery.\nKushal Arora, Kurt Shuster, Sainbayar Sukhbaatar, and\nJason Weston. 2022. Director: Generator-Classifiers\nFor Supervised Language Modeling. In Proceed-\nings of the 2nd Conference of the Asia-Pacific Chap-\nter of the Association for Computational Linguistics\nand the 12th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 512–526, Online only. Association for Compu-\ntational Linguistics.\nLora Aroyo and Chris Welty. 2015. Truth is a lie: Crowd\ntruth and the seven myths of human annotation. AI\nMagazine, 36(1):15–24.\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain,\nDeep Ganguli, Tom Henighan, Andy Jones, Nicholas\nJoseph, Ben Mann, Nova DasSarma, Nelson El-\nhage, Zac Hatfield-Dodds, Danny Hernandez, Jack-\nson Kernion, Kamal Ndousse, Catherine Olsson,\nDario Amodei, Tom Brown, Jack Clark, Sam McCan-\ndlish, Chris Olah, and Jared Kaplan. 2021. A General\nLanguage Assistant as a Laboratory for Alignment.\narXiv:2112.00861 [cs].\nLuigi Asprino, Luana Bulla, Stefano De Giorgis, Aldo\nGangemi, Ludovica Marinucci, and Misael Mongiovi.\n2022. Uncovering Values: Detecting Latent Moral\nContent from Natural Language with Explainable\nand Non-Trained Methods. In Proceedings of Deep\nLearning Inside Out (DeeLIO 2022): The 3rd Work-\nshop on Knowledge Extraction and Integration for\nDeep Learning Architectures, pages 33–41, Dublin,\nIreland and Online. Association for Computational\nLinguistics.\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda\nAskell, Anna Chen, Nova DasSarma, Dawn Drain,\nStanislav Fort, Deep Ganguli, Tom Henighan,\nNicholas Joseph, Saurav Kadavath, Jackson Kernion,\nTom Conerly, Sheer El-Showk, Nelson Elhage, Zac\nHatfield-Dodds, Danny Hernandez, Tristan Hume,\nScott Johnston, Shauna Kravec, Liane Lovitt, Neel\nNanda, Catherine Olsson, Dario Amodei, Tom\nBrown, Jack Clark, Sam McCandlish, Chris Olah,\nBen Mann, and Jared Kaplan. 2022a. Training\na Helpful and Harmless Assistant with Reinforce-\nment Learning from Human Feedback. arXiv:\n2204.05862.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones, Anna\nChen, Anna Goldie, Azalia Mirhoseini, Cameron\nMcKinnon, Carol Chen, Catherine Olsson, Christo-\npher Olah, Danny Hernandez, Dawn Drain, Deep\nGanguli, Dustin Li, Eli Tran-Johnson, Ethan Perez,\nJamie Kerr, Jared Mueller, Jeffrey Ladish, Joshua\nLandau, Kamal Ndousse, Kamile Lukosuite, Liane\nLovitt, Michael Sellitto, Nelson Elhage, Nicholas\nSchiefer, Noemi Mercado, Nova DasSarma, Robert\nLasenby, Robin Larson, Sam Ringer, Scott John-\nston, Shauna Kravec, Sheer El Showk, Stanislav Fort,\nTamera Lanham, Timothy Telleen-Lawton, Tom Con-\nerly, Tom Henighan, Tristan Hume, Samuel R. Bow-\nman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei,\nNicholas Joseph, Sam McCandlish, Tom Brown, and\nJared Kaplan. 2022b. Constitutional AI: Harmless-\nness from AI Feedback. arXiv: 2212.08073.\nMichiel A. Bakker, Martin J. Chadwick, Hannah R.\nSheahan, Michael Henry Tessler, Lucy Campbell-\nGillingham, Jan Balaguer, Nat McAleese, Amelia\nGlaese, John Aslanides, Matthew M. Botvinick, and\nChristopher Summerfield. 2022. Fine-tuning lan-\nguage models to find agreement among humans with\ndiverse preferences. arXiv: 2211.15006v1.\nYejin Bang, Tiezheng Yu, Andrea Madotto, Zhaojiang\nLin, Mona Diab, and Pascale Fung. 2022. Enabling\nClassifiers to Make Judgements Explicitly Aligned\nwith Human Values. arXiv: 2210.07652v1.\nEmily M Bender and Batya Friedman. 2018. Data\nstatements for natural language processing: Toward\nmitigating system bias and enabling better science.\nTransactions of the Association for Computational\nLinguistics, 6:587–604.\nAbeba Birhane, William Isaac, Vinodkumar Prab-\nhakaran, Mark Diaz, Madeleine Clare Elish, Iason\nGabriel, and Shakir Mohamed. 2022. Power to the\nPeople? Opportunities and Challenges for Participa-\ntory AI. In Equity and Access in Algorithms, Mech-\nanisms, and Optimization, EAAMO ’22, pages 1–8,\nNew York, NY , USA. Association for Computing\nMachinery.\nSamuel R. Bowman, Jeeyoon Hyun, Ethan Perez, Edwin\nChen, Craig Pettit, Scott Heiner, Kamil˙e Lukoši¯ut˙e,\nAmanda Askell, Andy Jones, Anna Chen, Anna\nGoldie, Azalia Mirhoseini, Cameron McKinnon,\nChristopher Olah, Daniela Amodei, Dario Amodei,\nDawn Drain, Dustin Li, Eli Tran-Johnson, Jackson\nKernion, Jamie Kerr, Jared Mueller, Jeffrey Ladish,\nJoshua Landau, Kamal Ndousse, Liane Lovitt, Nel-\nson Elhage, Nicholas Schiefer, Nicholas Joseph,\nNoemí Mercado, Nova DasSarma, Robin Larson,\nSam McCandlish, Sandipan Kundu, Scott Johnston,\nShauna Kravec, Sheer El Showk, Stanislav Fort, Tim-\nothy Telleen-Lawton, Tom Brown, Tom Henighan,\nTristan Hume, Yuntao Bai, Zac Hatfield-Dodds, Ben\nMann, and Jared Kaplan. 2022. Measuring Progress\non Scalable Oversight for Large Language Models.\narXiv:2211.03540 [cs].\nFlorian Böhm, Yang Gao, Christian M. Meyer, Ori\nShapira, Ido Dagan, and Iryna Gurevych. 2019. Bet-\nter Rewards Yield Better Summaries: Learning to\nSummarise Without References. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n2419\n(EMNLP-IJCNLP), pages 3110–3120, Hong Kong,\nChina. Association for Computational Linguistics.\nSabrina Campano, Jessica Durand, and Chloé Clavel.\n2014. Comparative analysis of verbal alignment\nin human-human and human-agent interactions. In\nProceedings of the Ninth International Conference\non Language Resources and Evaluation (LREC’14),\npages 4415–4422, Reykjavik, Iceland. European Lan-\nguage Resources Association (ELRA).\nLouis Castricato, Alexander Havrilla, Shahbuland Ma-\ntiana, Michael Pieler, Anbang Ye, Ian Yang, Spencer\nFrazier, and Mark Riedl. 2022. Robust preference\nlearning for storytelling via contrastive reinforcement\nlearning. arXiv: 2210.07792v2 [cs.CL].\nKai-Wei Chang, Vinodkumar Prabhakaran, and Vicente\nOrdonez. 2019. Bias and fairness in natural language\nprocessing. In Proceedings of the 2019 Conference\non Empirical Methods in Natural Language Process-\ning and the 9th International Joint Conference on\nNatural Language Processing (EMNLP-IJCNLP):\nTutorial Abstracts.\nMecca Chiesa and Sandy Hobbs. 2008. Making sense of\nsocial research: How useful is the hawthorne effect?\nEuropean Journal of Social Psychology , 38(1):67–\n74.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. Ad-\nvances in neural information processing systems, 30.\nYating Chuang and Laura Schechter. 2015. Stability of\nexperimental and survey measures of risk, time, and\nsocial preferences: A review and some new results.\nJournal of development economics, 117:151–170.\nAida Mostafazadeh Davani, Mark Díaz, and Vinodku-\nmar Prabhakaran. 2022. Dealing with disagreements:\nLooking beyond the majority vote in subjective an-\nnotations. Transactions of the Association for Com-\nputational Linguistics, 10:92–110.\nSimon De Deyne, Amy Perfors, and Daniel J Navarro.\n2016. Predicting human similarity judgments with\ndistributional models: The value of word associa-\ntions. In Proceedings of COLING 2016, the 26th\nInternational Conference on Computational Linguis-\ntics: Technical Papers , pages 1861–1870, Osaka,\nJapan. The COLING 2016 Organizing Committee.\nNicola Dell, Vidya Vaidyanathan, Indrani Medhi, Ed-\nward Cutrell, and William Thies. 2012. \" yours is\nbetter!\" participant response bias in hci. In Proceed-\nings of the sigchi conference on human factors in\ncomputing systems, pages 1321–1330.\nFloris Den Hengst, Mark Hoogendoorn, Frank\nVan Harmelen, and Joost Bosman. 2019. Reinforce-\nment learning for personalized dialogue management.\nIn IEEE/WIC/ACM International Conference on Web\nIntelligence, pages 59–67.\nLiqiong Deng and Marshall Scott Poole. 2010. Affect in\nweb interfaces: A study of the impacts of web page\nvisual complexity and order. Mis Quarterly, pages\n711–730.\nYang Deng, Yaliang Li, Wenxuan Zhang, Bolin Ding,\nand Wai Lam. 2022. Toward personalized answer\ngeneration in e-commerce via multi-perspective pref-\nerence modeling. ACM Transactions on Information\nSystems (TOIS), 40(4):1–28.\nLeon Derczynski, Hannah Rose Kirk, Vidhisha Bal-\nachandran, Sachin Kumar, Yulia Tsvetkov, M. R.\nLeiser, and Saif Mohammad. 2023. Assessing\nLanguage Model Deployment with Risk Cards.\narXiv:2303.18190 [cs].\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding. arXiv:1810.04805 [cs].\nBhuwan Dhingra, Lihong Li, Xiujun Li, Jianfeng Gao,\nYun-Nung Chen, Faisal Ahmed, and Li Deng. 2017.\nTowards End-to-End Reinforcement Learning of Dia-\nlogue Agents for Information Access. InProceedings\nof the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 484–495, Vancouver, Canada. Association for\nComputational Linguistics.\nEmily Dinan, Samuel Humeau, Bharath Chintagunta,\nand Jason Weston. 2019. Build it Break it Fix it\nfor Dialogue Safety: Robustness from Adversarial\nHuman Attack. arXiv:1908.06083 [cs].\nHanze Dong, Wei Xiong, Deepanshu Goyal, Yihan\nZhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng\nZhang, Kashun Shum, and Tong Zhang. 2023. RAFT:\nReward rAnked FineTuning for Generative Founda-\ntion Model Alignment. arXiv: 2304.06767.\nGuillaume Dubuisson Duplessis, Chloé Clavel, and\nFrédéric Landragin. 2017. Automatic Measures to\nCharacterise Verbal Alignment in Human-Agent In-\nteraction. In Proceedings of the 18th Annual SIGdial\nMeeting on Discourse and Dialogue , pages 71–81,\nSaarbrücken, Germany. Association for Computa-\ntional Linguistics.\nJessica Ficler and Yoav Goldberg. 2017. Controlling\nLinguistic Style Aspects in Neural Language Gener-\nation. In Proceedings of the Workshop on Stylistic\nVariation, pages 94–104, Copenhagen, Denmark. As-\nsociation for Computational Linguistics.\nRonald Fischer. 2017. Personality, Values, Culture. In\nPersonality, Values, Culture: An Evolutionary Ap-\nproach, Culture and Psychology, pages i–ii. Cam-\nbridge University Press, Cambridge.\nMaxwell Forbes, Jena D. Hwang, Vered Shwartz,\nMaarten Sap, and Yejin Choi. 2020. Social Chem-\nistry 101: Learning to Reason about Social and Moral\nNorms. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n2420\n(EMNLP), pages 653–670, Online. Association for\nComputational Linguistics.\nHershey H Friedman, Paul J Herskovitz, and Simcha\nPollack. 1994. The biasing effects of scale-checking\nstyles on response to a likert scale. In Proceedings of\nthe American statistical association annual confer-\nence: survey research methods, volume 792, pages\n792–795.\nRichard Futrell and Roger P. Levy. 2019. Do RNNs\nlearn human-like abstract word order preferences?\nIn Proceedings of the Society for Computation in\nLinguistics (SCiL) 2019, pages 50–59.\nDeep Ganguli, Liane Lovitt, Jackson Kernion, Amanda\nAskell, Yuntao Bai, Saurav Kadavath, Ben Mann,\nEthan Perez, Nicholas Schiefer, Kamal Ndousse,\nAndy Jones, Sam Bowman, Anna Chen, Tom Con-\nerly, Nova DasSarma, Dawn Drain, Nelson Elhage,\nSheer El-Showk, Stanislav Fort, Zac Hatfield-Dodds,\nTom Henighan, Danny Hernandez, Tristan Hume,\nJosh Jacobson, Scott Johnston, Shauna Kravec,\nCatherine Olsson, Sam Ringer, Eli Tran-Johnson,\nDario Amodei, Tom Brown, Nicholas Joseph, Sam\nMcCandlish, Chris Olah, Jared Kaplan, and Jack\nClark. 2022. Red Teaming Language Models to\nReduce Harms: Methods, Scaling Behaviors, and\nLessons Learned. arXiv: 2209.07858v2.\nXiang Gao, Yizhe Zhang, Michel Galley, Chris Brockett,\nand Bill Dolan. 2020. Dialogue Response Ranking\nTraining with Large-Scale Human Feedback Data. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 386–395, Online. Association for Computa-\ntional Linguistics.\nYang Gao, Christian M. Meyer, and Iryna Gurevych.\n2018. APRIL: Interactively learning to summarise\nby combining active preference learning and rein-\nforcement learning. In Proceedings of the 2018 con-\nference on empirical methods in natural language\nprocessing, pages 4120–4130, Brussels, Belgium.\nAssociation for Computational Linguistics.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealToxi-\ncityPrompts: Evaluating Neural Toxic Degeneration\nin Language Models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nMor Geva, Yoav Goldberg, and Jonathan Berant. 2019.\nAre we modeling the task or the annotator? an inves-\ntigation of annotator bias in natural language under-\nstanding datasets. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP),\npages 1161–1166.\nAmelia Glaese, Nat McAleese, Maja Tr˛ ebacz, John\nAslanides, Vlad Firoiu, Timo Ewalds, Maribeth Rauh,\nLaura Weidinger, Martin Chadwick, Phoebe Thacker,\nLucy Campbell-Gillingham, Jonathan Uesato, Po-\nSen Huang, Ramona Comanescu, Fan Yang, Abigail\nSee, Sumanth Dathathri, Rory Greig, Charlie Chen,\nDoug Fritz, Jaume Sanchez Elias, Richard Green,\nSoˇna Mokrá, Nicholas Fernando, Boxi Wu, Rachel\nFoley, Susannah Young, Iason Gabriel, William Isaac,\nJohn Mellor, Demis Hassabis, Koray Kavukcuoglu,\nLisa Anne Hendricks, and Geoffrey Irving. 2022.\nImproving alignment of dialogue agents via targeted\nhuman judgements. arXiv: 2209.14375v1.\nMitchell L Gordon, Michelle S Lam, Joon Sung Park,\nKayur Patel, Jeff Hancock, Tatsunori Hashimoto, and\nMichael S Bernstein. 2022. Jury learning: Integrat-\ning dissenting voices into machine learning models.\nIn Proceedings of the 2022 CHI Conference on Hu-\nman Factors in Computing Systems, pages 1–19.\nDavid Gros, Yu Li, and Zhou Yu. 2021. The R-U-A-\nRobot Dataset: Helping Avoid Chatbot Deception\nby Detecting User Questions About Human or Non-\nHuman Identity. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 6999–7013, Online. Association for\nComputational Linguistics.\nBraden Hancock, Antoine Bordes, Pierre-Emmanuel\nMazare, and Jason Weston. 2019. Learning from\nDialogue after Deployment: Feed Yourself, Chat-\nbot! In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n3667–3684, Florence, Italy. Association for Compu-\ntational Linguistics.\nDan Hendrycks, Collin Burns, Steven Basart, Andrew\nCritch, Jerry Li, Dawn Song, and Jacob Steinhardt.\n2020. Aligning AI With Shared Human Values.\narXiv: 2008.02275v5.\nSepp Hochreiter and Jürgen Schmidhuber. 1997. Long\nShort-term Memory. Neural computation, 9:1735–\n80.\nOr Honovich, Thomas Scialom, Omer Levy, and Timo\nSchick. 2022. Unnatural Instructions: Tuning Lan-\nguage Models with (Almost) No Human Labor.\narXiv: 2212.09689v1.\nJoe Hoover, Gwenyth Portillo-Wightman, Leigh\nYeh, Shreya Havaldar, Aida Mostafazadeh Davani,\nYing Lin, Brendan Kennedy, Mohammad Atari,\nZahra Kamel, Madelyn Mendlen, Gabriela Moreno,\nChristina Park, Tingyee E. Chang, Jenna Chin, Chris-\ntian Leong, Jun Yen Leung, Arineh Mirinjian, and\nMorteza Dehghani. 2020. Moral Foundations Twitter\nCorpus: A Collection of 35k Tweets Annotated for\nMoral Sentiment. Social Psychological and Person-\nality Science, 11(8):1057–1071. Publisher: SAGE\nPublications Inc.\nTom Hosking, Phil Blunsom, and Max Bartolo. 2023.\nHuman Feedback is not Gold Standard. arXiv:\n2309.16349.\n2421\nGary Hsieh and Rafał Kocielnik. 2016. You get who you\npay for: The impact of incentives on participation\nbias. In Proceedings of the 19th ACM conference\non computer-supported cooperative work & social\ncomputing, pages 823–835.\nXiaolei Huang, Alexandra Wormley, and Adam Cohen.\n2022. Learning to Adapt Domain Shifts of Moral Val-\nues via Instance Weighting. arXiv: 2204.07603v2.\nNatasha Jaques, Asma Ghandeharioun, Judy Han-\nwen Shen, Craig Ferguson, Agata Lapedriza, Noah\nJones, Shixiang Gu, and Rosalind Picard. 2019.\nWay Off-Policy Batch Deep Reinforcement Learn-\ning of Implicit Human Preferences in Dialog.\narXiv:1907.00456 [cs, stat].\nNatasha Jaques, Judy Hanwen Shen, Asma Ghandehari-\noun, Craig Ferguson, Agata Lapedriza, Noah Jones,\nShixiang Gu, and Rosalind Picard. 2020. Human-\ncentric dialog training via offline reinforcement learn-\ning. In Proceedings of the 2020 conference on\nempirical methods in natural language processing\n(EMNLP), pages 3985–4003, Online. Association for\nComputational Linguistics.\nSophie Jentzsch, Patrick Schramowski, Constantin\nRothkopf, and Kristian Kersting. 2019. Semantics\nDerived Automatically from Language Corpora Con-\ntain Human-like Moral Choices. In Proceedings\nof the 2019 AAAI/ACM Conference on AI, Ethics,\nand Society, AIES ’19, pages 37–44, New York, NY ,\nUSA. Association for Computing Machinery.\nZiwei Ji, Nayeon Lee, Rita Frieske, Tiezheng Yu,\nDan Su, Yan Xu, Etsuko Ishii, Yejin Bang, Andrea\nMadotto, and Pascale Fung. 2022. Survey of Halluci-\nnation in Natural Language Generation.\nLiwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ro-\nnan Le Bras, Jenny Liang, Jesse Dodge, Keisuke\nSakaguchi, Maxwell Forbes, Jon Borchardt, Saa-\ndia Gabriel, Yulia Tsvetkov, Oren Etzioni, Maarten\nSap, Regina Rini, and Yejin Choi. 2022. Can Ma-\nchines Learn Morality? The Delphi Experiment.\narXiv:2110.07574 [cs].\nZhijing Jin, Sydney Levine, Fernando Gonzalez, Ojasv\nKamal, Maarten Sap, Mrinmaya Sachan, Rada Mi-\nhalcea, Josh Tenenbaum, and Bernhard Schölkopf.\n2022. When to Make Exceptions: Exploring Lan-\nguage Models as Accounts of Human Moral Judg-\nment. arXiv: 2210.01478v3.\nAnna Jobin, Marcello Ienca, and Effy Vayena. 2019.\nThe global landscape of ai ethics guidelines. Nature\nMachine Intelligence, 1(9):389–399.\nDa Ju, Jing Xu, Y .-Lan Boureau, and Jason Weston.\n2022. Learning from data in the mixed adversarial\nnon-adversarial case: Finding the helpers and ignor-\ning the trolls. arXiv:2208.03295 [cs].\nJohannes Kiesel, Milad Alshomary, Nicolas Handke,\nXiaoni Cai, Henning Wachsmuth, and Benno Stein.\n2022. Identifying the Human Values behind Argu-\nments. In Proceedings of the 60th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers) , pages 4459–4471, Dublin,\nIreland. Association for Computational Linguistics.\nHannah Rose Kirk, Yennie Jun, Filippo V olpin, Haider\nIqbal, Elias Benussi, Frederic Dreyer, Aleksandar\nShtedritski, and Yuki Asano. 2021. Bias Out-of-the-\nBox: An Empirical Analysis of Intersectional Oc-\ncupational Biases in Popular Generative Language\nModels. In Advances in Neural Information Process-\ning Systems, volume 34, pages 2611–2624.\nHannah Rose Kirk, Bertie Vidgen, Paul Röttger, and\nScott A. Hale. 2023a. The Empty Signifier Prob-\nlem: Towards Clearer Paradigms for Operationalis-\ning \"Alignment\" in Large Language Models. arXiv:\n2310.02457.\nHannah Rose Kirk, Bertie Vidgen, Paul Röttger, and\nScott A. Hale. 2023b. Personalisation within bounds:\nA risk taxonomy and policy framework for the align-\nment of large language models with personalised\nfeedback. arXiv:2303.05453.\nTomasz Korbak, Kejian Shi, Angelica Chen, Rasika\nBhalerao, Christopher L. Buckley, Jason Phang,\nSamuel R. Bowman, and Ethan Perez. 2023. Pre-\ntraining Language Models with Human Preferences.\narXiv:2302.08582 [cs].\nJulia Kreutzer, Artem Sokolov, and Stefan Riezler. 2017.\nBandit Structured Prediction for Neural Sequence-\nto-Sequence Learning. In Proceedings of the 55th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1503–\n1513, Vancouver, Canada. Association for Computa-\ntional Linguistics.\nJulia Kreutzer, Joshua Uyheng, and Stefan Riezler.\n2018. Reliability and Learnability of Human Bandit\nFeedback for Sequence-to-Sequence Reinforcement\nLearning. In Proceedings of the 56th Annual Meeting\nof the Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1777–1788, Melbourne,\nAustralia. Association for Computational Linguistics.\nErnesto Laclau. 2005. On populist reason. Verso, Lon-\ndon New York (N.Y .).\nDean Lacy. 2001. A theory of nonseparable preferences\nin survey responses. American Journal of Political\nScience, pages 239–258.\nCarolin Lawrence and Stefan Riezler. 2018. Improving\na Neural Semantic Parser by Counterfactual Learn-\ning from Human Bandit Feedback. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1820–1830, Melbourne, Australia. Association\nfor Computational Linguistics.\nCarolin Lawrence, Artem Sokolov, and Stefan Riezler.\n2017. Counterfactual Learning from Bandit Feed-\nback under Deterministic Logging : A Case Study in\n2422\nStatistical Machine Translation. In Proceedings of\nthe 2017 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2566–2576, Copen-\nhagen, Denmark. Association for Computational Lin-\nguistics.\nLeonard Lee, On Amir, and Dan Ariely. 2009. In search\nof homo economicus: Cognitive noise and the role\nof emotion in preference consistency. Journal of\nconsumer research, 36(2):173–187.\nJiwei Li, Alexander H. Miller, Sumit Chopra,\nMarc’Aurelio Ranzato, and Jason Weston. 2017a.\nDialogue Learning With Human-In-The-Loop.\narXiv:1611.09823 [cs].\nJiwei Li, Alexander H. Miller, Sumit Chopra,\nMarc’Aurelio Ranzato, and Jason Weston. 2017b.\nLearning through Dialogue Interactions by Asking\nQuestions. arXiv:1612.04936 [cs].\nJiwei Li, Will Monroe, Alan Ritter, Dan Jurafsky,\nMichel Galley, and Jianfeng Gao. 2016. Deep Rein-\nforcement Learning for Dialogue Generation. In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing , pages 1192–\n1202, Austin, Texas. Association for Computational\nLinguistics.\nManling Li, Ying Lin, Joseph Hoover, Spencer White-\nhead, Clare V oss, Morteza Dehghani, and Heng Ji.\n2019. Multilingual Entity, Relation, Event and Hu-\nman Value Extraction. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics (Demon-\nstrations), pages 110–115, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022.\nTruthfulQA: Measuring How Models Mimic Human\nFalsehoods. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3214–3252, Dublin,\nIreland. Association for Computational Linguistics.\nYing Lin, Joe Hoover, Morteza Dehghani, Marlon\nMooijman, and Heng Ji. 2017. Acquiring Back-\nground Knowledge to Improve Moral Value Predic-\ntion. arXiv: 1709.05467v1.\nAlisa Liu, Maarten Sap, Ximing Lu, Swabha\nSwayamdipta, Chandra Bhagavatula, Noah A. Smith,\nand Yejin Choi. 2021a. DExperts: Decoding-Time\nControlled Text Generation with Experts and Anti-\nExperts. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 6691–6706, Online. Association for Computa-\ntional Linguistics.\nBing Liu, Gokhan Tür, Dilek Hakkani-Tür, Pararth\nShah, and Larry Heck. 2018. Dialogue Learning\nwith Human Teaching and Feedback in End-to-End\nTrainable Task-Oriented Dialogue Systems. In Pro-\nceedings of the 2018 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume\n1 (Long Papers), pages 2060–2069, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nHao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023a.\nChain of Hindsight Aligns Language Models with\nFeedback. arXiv: 2302.02676.\nHao Liu, Carmelo Sferrazza, and Pieter Abbeel. 2023b.\nLanguages are rewards: Chain of hindsight finetun-\ning using human feedback. arXiv: 2302.02676v2\n[cs.LG].\nRuibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, Lili\nWang, and Soroush V osoughi. 2021b. Mitigating Po-\nlitical Bias in Language Models through Reinforced\nCalibration. Proceedings of the AAAI Conference on\nArtificial Intelligence, 35(17):14857–14866. Num-\nber: 17.\nRuibo Liu, Chenyan Jia, Ge Zhang, Ziyu Zhuang,\nTony X. Liu, and Soroush V osoughi. 2023c. Second\nThoughts are Best: Learning to Re-Align With Hu-\nman Values from Text Edits. arXiv: 2301.00355v2.\nRuibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny\nZhou, Andrew M. Dai, Diyi Yang, and Soroush\nV osoughi. 2023d. Training Socially Aligned Lan-\nguage Models in Simulated Human Society. arXiv:\n2305.16960.\nRuibo Liu, Ge Zhang, Xinyu Feng, and Soroush\nV osoughi. 2022. Aligning Generative Language\nModels with Human Values. In Findings of the Asso-\nciation for Computational Linguistics: NAACL 2022,\npages 241–252, Seattle, United States. Association\nfor Computational Linguistics.\nNicholas Lourie, Ronan Le Bras, and Yejin Choi. 2021.\nSCRUPLES: A Corpus of Community Ethical Judg-\nments on 32,000 Real-Life Anecdotes. Proceedings\nof the AAAI Conference on Artificial Intelligence ,\n35(15):13470–13479. Number: 15.\nHua Lu, Siqi Bao, Huang He, Fan Wang, Hua Wu,\nand Haifeng Wang. 2022. Towards Boosting the\nOpen-Domain Chatbot with Human Feedback. arXiv:\n2208.14165v1.\nLi Lucy and David Bamman. 2021. Gender and Repre-\nsentation Bias in GPT-3 Generated Stories. In Pro-\nceedings of the Third Workshop on Narrative Un-\nderstanding, pages 48–55, Virtual. Association for\nComputational Linguistics.\nClaude Lévi-Strauss. 1987. Introduction to the work of\nMarcel Mauss. Routledge & Kegan Paul, London.\nHotaka Maeda. 2015. Response option configuration\nof online administered likert scales. International\nJournal of Social Research Methodology, 18(1):15–\n26.\n2423\nTushar Maheshwari, Aishwarya N. Reganti, Samik-\nsha Gupta, Anupam Jamatia, Upendra Kumar, Björn\nGambäck, and Amitava Das. 2017. A Societal Sen-\ntiment Analysis: Predicting the Values and Ethics\nof Individuals by Analysing Social Media Content.\nIn Proceedings of the 15th Conference of the Euro-\npean Chapter of the Association for Computational\nLinguistics: Volume 1, Long Papers, pages 731–741,\nValencia, Spain. Association for Computational Lin-\nguistics.\nBodhisattwa Prasad Majumder, Shuyang Li, Jianmo Ni,\nand Julian McAuley. 2019. Generating personalized\nrecipes from historical user preferences. In Proceed-\nings of the 2019 conference on empirical methods\nin natural language processing and the 9th inter-\nnational joint conference on natural language pro-\ncessing (EMNLP-IJCNLP), pages 5976–5982, Hong\nKong, China. Association for Computational Linguis-\ntics.\nDonald Martin Jr., Vinodkumar Prabhakaran, Jill\nKuhlberg, Andrew Smart, and William S. Isaac. 2020.\nParticipatory Problem Formulation for Fairer Ma-\nchine Learning Through Community Based System\nDynamics. arXiv:2005.07572 [cs, stat].\nJacob Menick, Maja Trebacz, Vladimir Mikulik,\nJohn Aslanides, Francis Song, Martin Chadwick,\nMia Glaese, Susannah Young, Lucy Campbell-\nGillingham, Geoffrey Irving, and Nat McAleese.\n2022. Teaching language models to support answers\nwith verified quotes. arXiv:2203.11147 [cs].\nShachar Mirkin and Jean-Luc Meunier. 2015. Person-\nalized machine translation: Predicting translational\npreferences. In Proceedings of the 2015 conference\non empirical methods in natural language process-\ning, pages 2019–2025, Lisbon, Portugal. Association\nfor Computational Linguistics.\nShachar Mirkin, Scott Nowson, Caroline Brun, and\nJulien Perez. 2015. Motivating Personality-aware\nMachine Translation. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1102–1108, Lisbon, Portu-\ngal. Association for Computational Linguistics.\nNailia Mirzakhmedova, Johannes Kiesel, Milad Al-\nshomary, Maximilian Heinrich, Nicolas Handke, Xi-\naoni Cai, Barriere Valentin, Doratossadat Dastgheib,\nOmid Ghahroodi, Mohammad Ali Sadraei, Ehsaned-\ndin Asgari, Lea Kawaletz, Henning Wachsmuth,\nand Benno Stein. 2023. The touché23-ValueEval\ndataset for identifying human values behind argu-\nments. arXiv: 2301.13771v1 [cs.CL].\nKaixiang Mo, Shuangyin Li, Yu Zhang, Jiajun Li, and\nQiang Yang. 2016. Personalizing a dialogue sys-\ntem with transfer reinforcement learning. arXiv:\n1610.02891v3 [cs.AI].\nMoin Nadeem, Anna Bethke, and Siva Reddy. 2021.\nStereoSet: Measuring stereotypical bias in pretrained\nlanguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 5356–5371, Online. Association for\nComputational Linguistics.\nMd Sultan Al Nahian, Spencer Frazier, Mark Riedl, and\nBrent Harrison. 2020. Learning norms from stories:\nA prior for value aligned agents. In Proceedings of\nthe AAAI/ACM Conference on AI, Ethics, and Society,\npages 124–130.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff\nWu, Long Ouyang, Christina Kim, Christopher\nHesse, Shantanu Jain, Vineet Kosaraju, William\nSaunders, Xu Jiang, Karl Cobbe, Tyna Eloundou,\nGretchen Krueger, Kevin Button, Matthew Knight,\nBenjamin Chess, and John Schulman. 2021. We-\nbGPT: Browser-assisted question-answering with hu-\nman feedback. arXiv: 2112.09332v3.\nDuy-Hung Nguyen, Nguyen Viet Dung Nghiem, Bao-\nSinh Nguyen, Dung Tien Tien Le, Shahab Sabahi,\nMinh-Tien Nguyen, and Hung Le. 2022. Make The\nMost of Prior Data: A Solution for Interactive Text\nSummarization with Preference Feedback. In Find-\nings of the Association for Computational Linguis-\ntics: NAACL 2022, pages 1919–1930, Seattle, United\nStates. Association for Computational Linguistics.\nKhanh Nguyen, Hal Daumé III, and Jordan Boyd-\nGraber. 2017. Reinforcement Learning for Bandit\nNeural Machine Translation with Simulated Human\nFeedback. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1464–1474, Copenhagen, Denmark. Associa-\ntion for Computational Linguistics.\nYixin Nie, Xiang Zhou, and Mohit Bansal. 2020. What\ncan we learn from collective human opinions on nat-\nural language inference data? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 9131–9143.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. arXiv: 2203.02155v1.\nXiangyu Peng, Siyan Li, Spencer Frazier, and Mark\nRiedl. 2020. Reducing Non-Normative Text Gen-\neration from Language Models. In Proceedings of\nthe 13th International Conference on Natural Lan-\nguage Generation, pages 374–383, Dublin, Ireland.\nAssociation for Computational Linguistics.\nEthan Perez, Sam Ringer, Kamil ˙e Lukoši ¯ut˙e, Karina\nNguyen, Edwin Chen, Scott Heiner, Craig Pettit,\nCatherine Olsson, Sandipan Kundu, Saurav Kada-\nvath, Andy Jones, Anna Chen, Ben Mann, Brian\nIsrael, Bryan Seethor, Cameron McKinnon, Christo-\npher Olah, Da Yan, Daniela Amodei, Dario Amodei,\n2424\nDawn Drain, Dustin Li, Eli Tran-Johnson, Guro\nKhundadze, Jackson Kernion, James Landis, Jamie\nKerr, Jared Mueller, Jeeyoon Hyun, Joshua Lan-\ndau, Kamal Ndousse, Landon Goldberg, Liane\nLovitt, Martin Lucas, Michael Sellitto, Miranda\nZhang, Neerav Kingsland, Nelson Elhage, Nicholas\nJoseph, Noemí Mercado, Nova DasSarma, Oliver\nRausch, Robin Larson, Sam McCandlish, Scott John-\nston, Shauna Kravec, Sheer El Showk, Tamera Lan-\nham, Timothy Telleen-Lawton, Tom Brown, Tom\nHenighan, Tristan Hume, Yuntao Bai, Zac Hatfield-\nDodds, Jack Clark, Samuel R. Bowman, Amanda\nAskell, Roger Grosse, Danny Hernandez, Deep Gan-\nguli, Evan Hubinger, Nicholas Schiefer, and Jared Ka-\nplan. 2022. Discovering Language Model Behaviors\nwith Model-Written Evaluations. arXiv:2212.09251\n[cs].\nME Peters, M Neumann, M Iyyer, M Gardner, C Clark,\nK Lee, and L Zettlemoyer. 2018. Deep contextual-\nized word representations. arxiv 2018. arXiv preprint\narXiv:1802.05365, 12.\nVinodkumar Prabhakaran, Aida Mostafazadeh Davani,\nand Mark Diaz. 2021. On releasing annotator-level\nlabels and information in datasets. In Proceedings\nof The Joint 15th Linguistic Annotation Workshop\n(LAW) and 3rd Designing Meaning Representations\n(DMR) Workshop, pages 133–138.\nVjosa Preniqi, Kyriaki Kalimeri, and Charalampos\nSaitis. 2022. \"More Than Words\": Linking Music\nPreferences and Moral Values Through Lyrics. arXiv:\n2209.01169v1.\nValentina Pyatkin, Jena D. Hwang, Vivek Srikumar,\nXiming Lu, Liwei Jiang, Yejin Choi, and Chandra\nBhagavatula. 2022. Reinforced clarification ques-\ntion generation with defeasibility rewards for dis-\nambiguating social and moral situations. arXiv:\n2212.10409v1 [cs.CL].\nLiang Qiu, Yizhou Zhao, Jinchao Li, Pan Lu, Baolin\nPeng, Jianfeng Gao, and Song-Chun Zhu. 2021. Val-\nueNet: A New Dataset for Human Value Driven Dia-\nlogue System. arXiv: 2112.06346v1.\nElla Rabinovich, Raj Nath Patel, Shachar Mirkin, Lu-\ncia Specia, and Shuly Wintner. 2017. Personalized\nMachine Translation: Preserving Original Author\nTraits. In Proceedings of the 15th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Volume 1, Long Papers , pages\n1074–1084, Valencia, Spain. Association for Compu-\ntational Linguistics.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Stefano\nErmon, Christopher D. Manning, and Chelsea Finn.\n2023. Direct Preference Optimization: Your Lan-\nguage Model is Secretly a Reward Model. arXiv:\n2305.18290.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards Empathetic Open-\ndomain Conversation Models: A New Benchmark\nand Dataset. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5370–5381, Florence, Italy. Association for\nComputational Linguistics.\nPaul Röttger, Bertie Vidgen, Dirk Hovy, and Janet Pier-\nrehumbert. 2022. Two contrasting data annotation\nparadigms for subjective nlp tasks. In Proceedings\nof the 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 175–190.\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Juraf-\nsky, Noah A. Smith, and Yejin Choi. 2020. Social\nBias Frames: Reasoning about Social and Power Im-\nplications of Language. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 5477–5490, Online. Association\nfor Computational Linguistics.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social IQa: Com-\nmonsense Reasoning about Social Interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4463–\n4473, Hong Kong, China. Association for Computa-\ntional Linguistics.\nJérémy Scheurer, Jon Ander Campos, Jun Shern Chan,\nAngelica Chen, Kyunghyun Cho, and Ethan Perez.\n2022. Training Language Models with Language\nFeedback. arXiv:2204.14146 [cs].\nPatrick Schramowski, Cigdem Turan, Sophie Jentzsch,\nConstantin Rothkopf, and Kristian Kersting. 2019.\nBERT has a Moral Compass: Improvements of\nethical and moral values of machines. arXiv:\n1912.05238v1.\nOlga Seminck and Pascal Amsili. 2017. A Compu-\ntational Model of Human Preferences for Pronoun\nResolution. In Proceedings of the Student Research\nWorkshop at the 15th Conference of the European\nChapter of the Association for Computational Lin-\nguistics, pages 53–63, Valencia, Spain. Association\nfor Computational Linguistics.\nEric Michael Smith, Melissa Hall Melanie Kambadur,\nEleonora Presani, and Adina Williams. 2022. \"I’m\nsorry to hear that\": Finding bias in language models\nwith a holistic descriptor dataset.\nIrene Solaiman and Christy Dennison. 2021. Process\nfor Adapting Language Models to Society (PALMS)\nwith Values-Targeted Datasets. In Advances in\nNeural Information Processing Systems, volume 34,\npages 5861–5873. Curran Associates, Inc.\nFeifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei\nHuang, Yongbin Li, and Houfeng Wang. 2023. Pref-\nerence Ranking Optimization for Human Alignment.\narXiv: 2306.17492.\n2425\nNisan Stiennon, Long Ouyang, Jeff Wu, Daniel M.\nZiegler, Ryan Lowe, Chelsea V oss, Alec Radford,\nDario Amodei, and Paul Christiano. 2020. Learn-\ning to summarize from human feedback. arXiv:\n2009.01325v3.\nYi Tay, Donovan Ong, Jie Fu, Alvin Chan, Nancy Chen,\nAnh Tuan Luu, and Chris Pal. 2020. Would you\nRather? A New Benchmark for Learning Machine\nAlignment with Cultural Values and Social Prefer-\nences. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n5369–5373, Online. Association for Computational\nLinguistics.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall,\nNoam Shazeer, Apoorv Kulshreshtha, Heng-Tze\nCheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du,\nYaGuang Li, Hongrae Lee, Huaixiu Steven Zheng,\nAmin Ghafouri, Marcelo Menegali, Yanping Huang,\nMaxim Krikun, Dmitry Lepikhin, James Qin, Dehao\nChen, Yuanzhong Xu, Zhifeng Chen, Adam Roberts,\nMaarten Bosma, Vincent Zhao, Yanqi Zhou, Chung-\nChing Chang, Igor Krivokon, Will Rusch, Marc\nPickett, Pranesh Srinivasan, Laichee Man, Kathleen\nMeier-Hellstern, Meredith Ringel Morris, Tulsee\nDoshi, Renelito Delos Santos, Toju Duke, Johnny So-\nraker, Ben Zevenbergen, Vinodkumar Prabhakaran,\nMark Diaz, Ben Hutchinson, Kristen Olson, Ale-\njandra Molina, Erin Hoffman-John, Josh Lee, Lora\nAroyo, Ravi Rajakumar, Alena Butryna, Matthew\nLamm, Viktoriya Kuzmina, Joe Fenton, Aaron Co-\nhen, Rachel Bernstein, Ray Kurzweil, Blaise Aguera-\nArcas, Claire Cui, Marian Croak, Ed Chi, and Quoc\nLe. 2022. LaMDA: Language Models for Dialog\nApplications. arXiv:2201.08239 [cs].\nAmos Tversky. 1969. Intransitivity of preferences. Psy-\nchological review, 76(1):31.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need.\nDongqi Wang, Haoran Wei, Zhirui Zhang, Shujian\nHuang, Jun Xie, and Jiajun Chen. 2021. Non-\nParametric Online Learning from Human Feed-\nback for Neural Machine Translation. arXiv:\n2109.11136v3.\nXin Wang, Jianan Wang, Yuanchao Liu, Xiaolong Wang,\nZhuoran Wang, and Baoxun Wang. 2017. Predicting\nUsers’ Negative Feedbacks in Multi-Turn Human-\nComputer Dialogues. In Proceedings of the Eighth\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers) , pages 713–\n722, Taipei, Taiwan. Asian Federation of Natural\nLanguage Processing.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A. Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-Instruct: Aligning Lan-\nguage Model with Self Generated Instructions. arXiv:\n2212.10560v1.\nJohannes Welbl, Amelia Glaese, Jonathan Uesato,\nSumanth Dathathri, John Mellor, Lisa Anne Hen-\ndricks, Kirsty Anderson, Pushmeet Kohli, Ben Cop-\npin, and Po-Sen Huang. 2021. Challenges in Detoxi-\nfying Language Models.\nJ Christopher Westland. 2022. Information loss and bias\nin likert survey responses. Plos one, 17(7):e0271949.\nJeff Wu, Long Ouyang, Daniel M. Ziegler, Nisan Sti-\nennon, Ryan Lowe, Jan Leike, and Paul Christiano.\n2021. Recursively Summarizing Books with Human\nFeedback. arXiv: 2109.10862v2.\nZeqiu Wu, Yushi Hu, Weijia Shi, Nouha Dziri, Alane\nSuhr, Prithviraj Ammanabrolu, Noah A. Smith, Mari\nOstendorf, and Hannaneh Hajishirzi. 2023. Fine-\nGrained Human Feedback Gives Better Rewards for\nLanguage Model Training. arXiv: 2306.01693.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason\nWeston, and Emily Dinan. 2021a. Bot-Adversarial\nDialogue for Safe Conversational Agents. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n2950–2968, Online. Association for Computational\nLinguistics.\nJing Xu, Da Ju, Margaret Li, Y .-Lan Boureau, Jason\nWeston, and Emily Dinan. 2021b. Recipes for Safety\nin Open-domain Chatbots. arXiv:2010.07079 [cs].\nJing Xu, Megan Ung, Mojtaba Komeili, Kushal Arora,\nY .-Lan Boureau, and Jason Weston. 2022. Learn-\ning New Skills after Deployment: Improving open-\ndomain internet-driven dialogue with human feed-\nback. arXiv: 2208.03270v2.\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang,\nSongfang Huang, and Fei Huang. 2023. RRHF: Rank\nResponses to Align Language Models with Human\nFeedback without tears. arXiv: 2304.05302.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing Dialogue Agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 2204–2213,\nMelbourne, Australia. Association for Computational\nLinguistics.\nJieyu Zhao, Daniel Khashabi, Tushar Khot, Ashish Sab-\nharwal, and Kai-Wei Chang. 2021. Ethical-Advice\nTaker: Do Language Models Understand Natural Lan-\nguage Interventions? In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 4158–4164, Online. Association for Computa-\ntional Linguistics.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao\nSun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu,\nLili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis,\nLuke Zettlemoyer, and Omer Levy. 2023. LIMA:\nLess Is More for Alignment. arXiv: 2305.11206.\n2426\nRuijie Zhou, Soham Deshmukh, Jeremiah Greer, and\nCharles Lee. 2021. NaRLE: Natural language models\nusing reinforcement learning with emotion feedback.\narXiv: 2110.02148v1 [cs.CL].\nDaniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B.\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-Tuning Lan-\nguage Models from Human Preferences. arXiv:\n1909.08593v2.\nCaleb Ziems, Jane Yu, Yi-Chia Wang, Alon Halevy, and\nDiyi Yang. 2022. The Moral Integrity Corpus: A\nBenchmark for Ethical Dialogue Systems. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 3755–3773, Dublin, Ireland. Associa-\ntion for Computational Linguistics.\nDouglas Zytko, Pamela J. Wisniewski, Shion Guha,\nEric P. S. Baumer, and Min Kyung Lee. 2022. Par-\nticipatory Design of AI Systems: Opportunities and\nChallenges Across Diverse Users, Relationships, and\nApplication Domains. In Extended Abstracts of the\n2022 CHI Conference on Human Factors in Comput-\ning Systems, CHI EA ’22, pages 1–4, New York, NY ,\nUSA. Association for Computing Machinery.\nA Flowchart of Articles for Scoping the\nReview\nIn Fig. 1, we schematically summarise the process\nof selecting articles for our review.\nB Code Book\nWe present the full code book used for each article\nin Tab. 3. These questions were inputted into an\nonline form then coded by two authors of the paper,\nwith frequent check-ins to ensure similarity of in-\nterpretation on how the form should be used. The\nfirst theme (conceptual) makes up our conceptual\ncomments in the main paper, while the laboural\nand technical themes make up our methodological\ncomments in the main paper.\nC Additional Information on Reviewed\nArticles\nC.1 Target Tasks\nIn Tab. 2, we summarise the core target tasks ap-\nproached by each article. Reflecting the recent\nmovement away from specialist NLP systems to-\nwards general purpose language agents, the major-\nity of articles work with generalised models that\ncan handle many other NLP subtasks via instruc-\ntion or dialogue.\nC.2 Evaluating Models\nEven articles employing indirect or simulated hu-\nman feedback usually conduct a human evalua-\ntion stage (Peng et al., 2020; Liu et al., 2021b,\n2022). Differently-trained models are often com-\npared via ELO scores or win rates (Ziegler et al.,\n2019; Nakano et al., 2021; Bai et al., 2022b,a;\nScheurer et al., 2022; Bakker et al., 2022; Glaese\net al., 2022; Ouyang et al., 2022). Most evalua-\ntions include fine-grained questions about model\noutputs, including quality or usefulness (Wu et al.,\n2021; Nakano et al., 2021; Liu et al., 2022; Bakker\net al., 2022); political bias (Liu et al., 2021b); co-\nherence (Wu et al., 2021; Nakano et al., 2021; Liu\net al., 2022; Bakker et al., 2022); safety or harm-\nlessness (Xu et al., 2021a; Lu et al., 2022; Gan-\nguli et al., 2022; Thoppilan et al., 2022); informa-\ntiveness, correctness or trustworthiness (Wu et al.,\n2021; Nakano et al., 2021; Lu et al., 2022); cre-\nativity (Honovich et al., 2022); and alignment with\na human value or trait (Solaiman and Dennison,\n2021; Liu et al., 2022; Castricato et al., 2022; Liu\net al., 2023b).\nOthers use automated metrics to quantitatively\ncompare models and outputs, with Böhm et al.\n(2019) and Stiennon et al. (2020) performing a\ncomparison of how such automated metrics cor-\nrelate with human preferences. Metrics include\nROGUE (Böhm et al., 2019; Ziegler et al., 2019;\nStiennon et al., 2020; Liu et al., 2022; Nguyen et al.,\n2022; Wang et al., 2022; Wu et al., 2021; Liu et al.,\n2023c), summary length (Stiennon et al., 2020),\nperplexity (Liu et al., 2021b, 2022, 2023c) or Sacre-\nBLEU (Wang et al., 2021). Sometimes separate\ndiscriminative classifier are deployed to measure\ntextual attributes (Thoppilan et al., 2022), such as\ntoxicity measured via Perspective API scores (So-\nlaiman and Dennison, 2021; Arora et al., 2022).\nScheurer et al. (2022) score how close feedback\nand refinements are in the embedding space be-\ncause they find written feedback often describes an\n“ideal” output. Any prediction tasks – e.g., whether\nan ethical judgement is fair or unfair (Jiang et al.,\n2022), a situation is normative or non-normative\n(Nahian et al., 2020; Forbes et al., 2020), a norm\nexception is permissible or not permissible (Jin\net al., 2022) or an utterance is value aligned or\nmisaligned (Qiu et al., 2021) – use F1-score or\naccuracy as evaluation metrics.\nMetrics or human evaluations that measure how\naligned a resultant model is with human prefer-\n2427\nTask References\nText generation\n(Peng et al., 2020; Liu et al., 2021b; Solaiman and Dennison, 2021; Arora et al., 2022; Liu\net al., 2022; Korbak et al., 2023), including story generation (Castricato et al., 2022) and\ncode generation (Korbak et al., 2023)\nInstruction following (Honovich et al., 2022; Ouyang et al., 2022; Wang et al., 2022)\nOpen-ended dialogue\n(Hancock et al., 2019; Gao et al., 2020; Jaques et al., 2020; Askell et al., 2021; Qiu et al.,\n2021; Thoppilan et al., 2022; Bai et al., 2022b,a; Ganguli et al., 2022; Lu et al., 2022;\nXu et al., 2022; Liu et al., 2023b), including information-seeking dialogue (Glaese et al.,\n2022)\nOpen-book generative QA (Zhao et al., 2021; Deng et al., 2022; Nakano et al., 2021; Menick et al., 2022)\nSummarization\n(Gao et al., 2018; Böhm et al., 2019; Ziegler et al., 2019; Stiennon et al., 2020; Scheurer\net al., 2022; Nguyen et al., 2022; Liu et al., 2023b), including long-form book summarisa-\ntion (Wu et al., 2021) and opinion consensus summarisation (Bakker et al., 2022)\nToxic language (Dinan et al., 2019; Peng et al., 2020; Liu et al., 2021a; Scheurer et al., 2022; Ju et al.,\n2022; Bang et al., 2022; Liu et al., 2022)\nMoral & normative judgements (Forbes et al., 2020; Nahian et al., 2020; Jiang et al., 2022; Liu et al., 2022; Jin et al., 2022;\nPyatkin et al., 2022; Liu et al., 2023c)\nOthers\nSentiment and style transfer (Ziegler et al., 2019; Peng et al., 2020; Liu et al., 2021a);\nrecipe generation (Majumder et al., 2019); predicting intent of emails (Zhou et al., 2021);\nmachine translation (Wang et al., 2021)\nTable 2: Articles categorised by target task.\nences or values can be contrasted with general in-\nvestigations of model capabilities to estimate the\nso-called “alignment tax” (Liu et al., 2022). For\ninstance, Korbak et al. (2023) rely on two met-\nrics: (i) misalignment score, calculated using the\nsame automated reward functions as training (tox-\nicity score, number of PII instances per character,\nnumber of PEP errors per character), and (ii) ca-\npability score, calculated as the KL divergence of\noutput distribution from a highly capable model\n(GPT-3). Some articles assess the drop in other\nperformance measures on NLP benchmark tasks\nmeasuring truthfulness, toxicity or bias (Bai et al.,\n2022a; Ouyang et al., 2022).\nD Articles with Other Contribution Types\nIn the main paper, we discuss papers that seek to\nembed, train or align LLMs with human prefer-\nences and values. Here, we give a brief overview\nof the other categories of papers which are excluded\nfrom the main review.\nPredict These articles include detecting moral\ncontent from tweets (Hoover et al., 2020; Asprino\net al., 2022) or adapting to moral shifts (Huang\net al., 2022); predicting values and ethics from so-\ncial media content (Maheshwari et al., 2017) or mu-\nsic preferences (Preniqi et al., 2022); linking event\nor entity extraction with moral values in knowledge\nbases (Lin et al., 2017; Li et al., 2019); and iden-\ntifying human values in arguments (Kiesel et al.,\n2022).\nEvaluate These articles include those that bench-\nmark judgements in moral or ethical situations\n(Tay et al., 2020; Hendrycks et al., 2020; Lourie\net al., 2021; Ziems et al., 2022; Mirzakhmedova\net al., 2023); assess social biases or social reason-\ning (Sap et al., 2019, 2020); evaluate performance\non personality-aware dialogue (Zhang et al., 2018)\nor empathetic dialogue (Rashkin et al., 2019); and\ndetect non-human identity in conversations (Gros\net al., 2021). Others directly evaluate the values or\ntraits of existing models (Schramowski et al., 2019;\nJentzsch et al., 2019; Perez et al., 2022).\n2428\nFigure 1: Flowchart of the selection process for articles in our review. We first match on the keywords defined in\nTab. 1, keeping only articles with >1 matches in the title. We then de-duplicate articles posted on both arXiv and\nACL. This initial set is called S0. We apply the inclusion criteria listed in §2.1, and also add any possibly relevant\nreferences to the snowballed set (S1) regardless of publishing venue. We also apply the inclusion criteria to S1.\nFinally, we make two additional categorisations — the dominant contribution type of the article (predict, evaluate or\ntrain) and whether it uses LLMs.\n2429\nMETADATA\nrelevance Whether to include or exclude article from set single choice: [exclude, include]\nexclusion criteria Reason for excluding the article single choice: [topic, modality, empirical, other]\nexclusion detail Additional text summary of exclusion criteria\nreasoning\nfree-text\nsnowball keys The bib keys of additional references free-text\ncontribution type Dominant contribution type of article single choice: [evaluate, predict, train, other]\ncontribution de-\ntail\nHow are the main contributions of the article\ndescribed?\nfree-text\nabout LLMs Whether the article uses LLMs single choice: [yes, no]\nshort summary 1-3 sentence summary of the article free-text\nCONCEPTUAL THEME\nterminology Is feedback discussed using the terms ‘prefer-\nences’ or ‘values’?\nsingle choice: [ preferences, values, mix, other]\nmotivation What is the motivation for feedback learning? free-text\ntarget concepts Which human values or preferences are priori-\ntised or included?\nfree-text\nconcept defs How are human values or preferences defined? free-text\ntheories What theories (if any) are used to define prefer-\nences/values?\nfree-text\nconcept scope Are concepts defined as universal or cultur-\nally/contextually understood?\nfree-text\ninterpretation\nfreedom\nWhat level of freedom are humans given in\ninterpreting the in-scope target concepts, e.g.,\n“helpfulness”?\nsingle choice: [prescriptive paradigm, subjec-\ntive paradigm, unclear]\nLABOURAL THEME\nfeedback genera-\ntion\nHow is feedback data collected? multi choice: [human-generated explicit,\nhuman-generated implicit, model-generated,\ncombined, other]\nfeedback types What forms of feedback are collected? At what\nstage, and if for training or for evaluation?\nfree-text\nlabour documen-\ntation\nIs the labour force documented? single choice: [yes, no, nan]\nlabour details What level of documentation or what details are\ndocumented?\nfree-text\nlabour force Which human group(s) generate the feedback? multi choice: [crowdworkers, in-house team\nauthors, unknown]\nlabour force detail What further detail is provided on who gener-\nates feedback?\nfree-text\nlabour force size How many humans are involved in feedback\ncollection for training and/or evaluation?\nfree-text\nTECHNICAL THEME\ndata size What is the size of the feedback dataset for\ntraining and for evaluation?\nfree-text\nintervention stage When and how is feedback integrated into the\nmodel?\nmulti choice: [pre-training, fine-tuning, prompt-\ning, other]\nmetrics What metrics and which evaluation datasets are\nused?\nfree-text\nmodel approach Summarise the modelling methodology free-text\nPROCEDURAL THEME\nauthorship Authorship composition of the article single choice: [academia, industry, mixed]\ndata availability Whether the data artifacts are publicly available single choice: [yes, no, unclear]\nmodel availability Whether the model artifacts are publicly avail-\nable\nsingle choice: [yes, no, unclear]\npeer review Whether the article is peer-reviewed single choice: [yes, no, unclear]\nTable 3: Code book used for each article included in the review . We show the field name, the prompt or\ninstruction for the coder and the type of response variable (including options if it is a single or multiple choice\nquestion). Frequent communication between two coders was established to ensure the fields were being applied\nconsistently.\n2430",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6485047936439514
    },
    {
      "name": "Raising (metalworking)",
      "score": 0.4101041555404663
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3316057324409485
    },
    {
      "name": "Engineering",
      "score": 0.09926983714103699
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ]
}