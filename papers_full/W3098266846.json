{
  "title": "BERT-MK: Integrating Graph Contextualized Knowledge into Pre-trained Language Models",
  "url": "https://openalex.org/W3098266846",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A1968357941",
      "name": "Bin He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099450696",
      "name": "Di Zhou",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2144198606",
      "name": "Jinghui Xiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097242334",
      "name": "Xin Jiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096490164",
      "name": "Nicholas Jing Yuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101240275",
      "name": "Tong Xu",
      "affiliations": [
        "University of Science and Technology of China"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2804220263",
    "https://openalex.org/W2136437513",
    "https://openalex.org/W2604314403",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4297733535",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W2882319491",
    "https://openalex.org/W2774837955",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2972119829",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W2889583850",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2047782770",
    "https://openalex.org/W3036446966",
    "https://openalex.org/W2884668708",
    "https://openalex.org/W2979401726",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W4289096842",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W3005441132",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3099152386",
    "https://openalex.org/W2922551710",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W3035153870",
    "https://openalex.org/W2951105272",
    "https://openalex.org/W2168041406",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2998230451",
    "https://openalex.org/W2052217781"
  ],
  "abstract": "Complex node interactions are common in knowledge graphs (KGs), and these interactions can be considered as contextualized knowledge exists in the topological structure of KGs. Traditional knowledge representation learning (KRL) methods usually treat a single triple as a training unit, neglecting the usage of graph contextualized knowledge. To utilize these unexploited graph-level knowledge, we propose an approach to model subgraphs in a medical KG. Then, the learned knowledge is integrated with a pre-trained language model to do the knowledge generalization. Experimental results demonstrate that our model achieves the state-of-the-art performance on several medical NLP tasks, and the improvement above MedERNIE indicates that graph contextualized knowledge is beneficial.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2281–2290\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n2281\nBERT-MK: Integrating Graph Contextualized Knowledge into\nPre-trained Language Models\nBin He1, Di Zhou1, Jinghui Xiao1, Xin Jiang1, Qun Liu1, Nicholas Jing Yuan2, Tong Xu3\n1Huawei Noah’s Ark Lab\n2Huawei Cloud & AI\n3School of Computer Science, University of Science and Technology of China\n{hebin.nlp, zhoudi7, xiaojinghui4, jiang.xin, qun.liu, nicholas.yuan}@huawei.com, tongxu@ustc.edu.cn\nAbstract\nComplex node interactions are common in\nknowledge graphs (KGs), and these inter-\nactions can be considered as contextualized\nknowledge exists in the topological structure\nof KGs. Traditional knowledge representation\nlearning (KRL) methods usually treat a single\ntriple as a training unit, neglecting the usage\nof graph contextualized knowledge. To uti-\nlize these unexploited graph-level knowledge,\nwe propose an approach to model subgraphs\nin a medical KG. Then, the learned knowl-\nedge is integrated with a pre-trained language\nmodel to do the knowledge generalization. Ex-\nperimental results demonstrate that our model\nachieves the state-of-the-art performance on\nseveral medical NLP tasks, and the improve-\nment above MedERNIE indicates that graph\ncontextualized knowledge is beneﬁcial.\n1 Introduction\nIn 1954, Harris (1954) proposed a distributional\nhypothesis that words occur in the same contexts\ntend to have similar meanings. Firth (1957) ex-\nplained the context-dependent nature of meaning\nin linguistics by his famous quotation “you shall\nknow a word by the company it keeps” . Although\nthe above-mentioned distributional hypothesis is\nproposed for language models, if we look at the\nknowledge graph from the perspective of this hy-\npothesis, we can ﬁnd that similar hypothesis exists\nin knowledge graphs (KGs). We call it KG distri-\nbutional hypothesis: you shall know an entity by\nthe relationships it involves.\nGiven this hypothesis, contextualized informa-\ntion in language models can be mapped to knowl-\nedge graphs, which we call “graph contextualized\nknowledge”. Figure 1 illustrates a knowledge sub-\ngraph that includes several medical entities. In this\nﬁgure, four incoming and four outgoing neighbor-\ning nodes (hereinafter called “in-entity” and “out-\nentity”) of node “Bacterial pneumonia” are linked\nFigure 1: A subgraph extracted from a medical knowl-\nedge graph. The rectangles represent entities and di-\nrected arrows denote relations.\nby various relation paths. These linked nodes and\ncorrelations can be seen as “graph contextualized\ninformation” of entity node “ Bacterial pneumo-\nnia”. In this study, we will explore how to integrate\ngraph contextualized knowledge into pre-trained\nlanguage models.\nPre-trained language models learn contextual-\nized word representations on large-scale text cor-\npus through self-supervised learning methods, and\nobtain new state-of-the-art (SOTA) results on most\ndownstream tasks (Peters et al., 2018; Radford\net al., 2018; Devlin et al., 2019). This gradually\nbecomes a new paradigm for natural language pro-\ncessing research. Recently, several knowledge-\nenhanced pre-trained language models have been\nproposed, such as ERNIE-Baidu (Sun et al., 2019),\nERNIE-Tsinghua (Zhang et al., 2019a), WKLM\n(Xiong et al., 2019) and K-ADAPTER (Wang et al.,\n2020).\nIn this study, since we need to learn graph contex-\ntualized knowledge in a large-scale medical knowl-\nedge graph, ERNIE-Tsinghua (hereinafter called\n“ERNIE”) is chosen as our backbone model. In\nERNIE, entity embeddings are learned by TransE\n(Bordes et al., 2013), which is a popular transition-\nbased method for knowledge representation learn-\ning (KRL). However, TransE cannot deal with\nthe modeling of complex relations (Lin et al.,\n2282\n2018), such as 1-to-n, n-to-1 and n-to-n relations.\nThis shortcoming will be ampliﬁed in the medical\nknowledge graph, in which many entities have a\nlarge number of related neighbors.\nInspired by previous work (Veli ˇckovi´c et al.,\n2018; Nathani et al., 2019), we propose an ap-\nproach to learn knowledge from subgraphs, and\ninject graph contextualized knowledge into the pre-\ntrained language model. We call this model BERT-\nMK (a BERT-based language model integrated\nwith Medical Knowledge), our contributions are as\nfollows:\n•We propose a novel knowledge-enhanced pre-\ntrained language model BERT-MK for medi-\ncal NLP tasks, which integrates graph contex-\ntualized knowledge learned from the medical\nKG.\n•Experimental results show that BERT-MK\nachieves better performance than previous\nstate-of-the-art biomedical pre-trained lan-\nguage models on entity typing and relation\nclassiﬁcation tasks.\n2 Methodology\nOur model consists of two modules: the knowl-\nedge learning module and the language model pre-\ntraining module. The ﬁrst module is utilized to\nlearn graph contextualized knowledge existing in\nKGs, and the second one integrates the learned\nknowledge into the language model for knowledge\ngeneralization. The details will be described in the\nfollowing subsections.\n2.1 Learning Graph Contextualized\nKnowledge\nWe denote a knowledge graph as G = (E,R),\nwhere Erepresents the entity set and Ris the set\nof relations between enity pairs. A triple in Gis\nformalized as (es,r,eo), where es is a subjective\nentity, eo is an objective entity, and r is the rela-\ntion between es and eo. In Figure 1, two entities\n(rectangles) and a relation (arrow) between them\nconstructs a knowledge triple, for example, (Bacte-\nrial pneumonia, causative agent of, Bacteria).\n2.1.1 Subgraph Conversion\nTo enrich the contextualized information in knowl-\nedge representations, we extract subgraphs from\nthe knowlege graph to be the modeling objectives,\nand the generation process is described in Algo-\nrithm 1. For a given entity, its two 1-hop in-entities\nAlgorithm 1: Subgraph generation.\nInput: Knowledge graph G= (E, R, T), duplicate number M\nOutput: Subgraph set S\n1 Initial S= [];\n2 foreach e ∈E do\n3 din\ne = calculatein degree(G, e);\n4 dout\ne = calculateout degree(G, e);\n5 Tin\ne = extractin triples(G, e);\n6 Tout\ne = extractout triples(G, e);\n7 i = 0;\n8 while i < (din\ne + dout\ne ) ∗M/2 do\n9 Tin\ni = randomsample(Tin\ne , 2);\n10 Tout\ni = randomsample(Tout\ne , 2);\n11 subgraph = Tin\ni + Tout\ni ;\n12 S= S+ subgraph;\n13 i = i + 1;\n14 end\n15 end\n16 return S\nFigure 2: Converting a subgraph extracted from the\nknowledge graph into the input of the model. (a) e\nrefers to the entity, and r represents the relation. (b)\nRelations are transformed into sequence nodes, and all\nnodes are assigned a numeric index. (c) Each row in\nthe matrix of node position indexes represents the in-\ndex list of an triple in (b); the adjacent matrix indicates\nthe connectivity (the red points equal to 1 and the white\npoints are 0) between the nodes in (b).\nand out-entities are sampled to generate a sub-\ngraph1, and we repeat the generation process M\ntimes for each entity. Figure 2(a) shows an instance\nof the knowledge subgraph, which consists of four\n1-hop and four 2-hop relations. In this study, we\npropose a Transformer-based (Vaswani et al., 2017)\nmodule to model subgraphs. Relations are learned\n1In this study, longer n-hop relations are not involved in\nthe subgraph generation process, we leave more arbitrary sub-\ngraph to the future work.\n2283\nFigure 3: The model architecture of BERT-MK. The left part is the pre-trained language model, in which entity\ninformation learned from the knowledge graph is incorporated. The right part is GCKE module. The subgraph in\nFigure 2 is utilized to describe the learning process. e1, e(1)\n1 , eO\n1 is the embedding of the input node, the updated\nnode and the output node, respectively.\nas nodes equivalent to entities in our model, and\nthe relation conversion process is illustrated in Fig-\nure 2(b). Therefore, knowledge graph Gcan be\nredeﬁned as G= (V,E), where V represents the\nnodes in G, involving entities in Eand relations\nin R, and Edenotes the directed edges among the\nnodes in V.\nThen, subgraphs are converted into sequences\nof nodes. The conversion result of a subgraph is\nshown in Figure 2(c), including a node sequence, a\nnode position index matrix and an adjacency ma-\ntrix. Each row of the node position index matrix\ncorresponds to a triple in the subgraph. For exam-\nple, the triple (e1,r1,e) is represented as the ﬁrst\nrow (0,1,4) in this matrix. In the adjacency matrix,\nthe element Aij equals 1 if the node iis connected\nto node jin Figure 2(b), and 0 otherwise.\n2.1.2 GCKE\nAfter the subgraph conversion preprocessing, the\ninput samples to learn graph contextualized knowl-\nedge are generated. Formally, we denote the node\nsequence as {x1,...,x N }, where N is the length\nof the input sequence. Besides, the node position in-\ndex matrix and the adjacency matrix are deﬁned as\nP and A, respectively. Entity embeddings and rela-\ntion embeddings are integrated in the same matrix\nV, where V ∈R(ne+nr)×d, ne is the entity number\nin Eand nr is the relation type number in R. The\nnode embeddings X = {x1,..., xN }can be gen-\nerated by looking up node sequence {x1,...,x N }\nin embedding matrix V. X, P and A constitute the\ninput of the graph contextualized knowledge em-\nbedding learning module, called GCKE, as shown\nin Figure 3.\nThe inputs are fed into a Transformer-based\nmodel to encode the node information.\nx′\ni =\nH⨁\nh=1\nN∑\nj=1\nαh\nij ·(xj ·Wh\nv), (1)\nαh\nij =\nexp(ah\nij)\n√\nd/H·∑N\nn=1 exp(ah\nin)\n, (2)\nah\nij = Masking((xi·Wh\nq)·(xj·Wh\nk)T),Aji+Iij),\n(3)\nwhere x′\ni is the new embedding for node xi. ⨁\ndenotes the concatenation of the H attention heads\nin this layer, αh\nij and Wh\nv are the attention weight\nof node xj and a linear transformation of node em-\nbedding xj in the hth attention head, respectively.\nThe Masking function in Equation 3 restraints the\ncontextualized dependency among the input nodes,\nonly the degree-in nodes and the current node it-\nself are involved to update the node embedding.\nThe subﬁgure in the lower right corner of Figure 3\nshows the contextualized dependencies. Similar to\nWh\nv, Wh\nq and Wh\nk are independent linear transfor-\nmations of node embeddings. Then, the updated\n2284\nnode representations are fed into the feed forward\nlayer for further encoding. The aforementioned\nTransformer blocks are stacked by Ltimes, and the\noutput hidden states can be formalized as\nXO = {xO\n1 ,..., xO\nN }. (4)\nThen, the node position indexes P is utilized to\nrestore triple representations:\nT = TripleRestoration(XO,P), (5)\nwhere Pk = (ek\ns,rk,ek\no) is the position index of a\nvalid knowledge triple, and Tk = (xO\neks\n,xO\nrk ,xO\neko\n)\nis the representation of this triple. The subﬁgure in\nthe upper right corner of Figure 3 shows the triple\nrestoration process.\nIn this study, the translation-based scoring func-\ntion (Han et al., 2018) is adopted to measure the\nenergy of a knowledge triple. The node embed-\ndings are learned by minimizing a margin-based\nloss function on the training data:\nL=\n∑\nt∈T\nmax{d(t) −d(f(t)) +γ,0}, (6)\nwhere t = (ts,tr,to), d(t) =|ts + tr −to|, γ >\n0 is a margin hyperparameter, f(t) is an entity\nreplacement operation that the head entity or the\ntail entity in a triple is replaced and the replaced\ntriple is an invalid triple in the KG.\n2.2 Integrating Knowledge into the\nLanguage Model\nGiven a comprehensive medical knowledge graph,\ngraph contextualized knowledge representations\ncan be learned using the GCKE module. We fol-\nlow the language model architecture proposed in\n(Zhang et al., 2019a), and utilize graph contextual-\nized knowledge to enhance medical language rep-\nresentations. The pre-training process is shown in\nthe left part of Figure 3. The Transformer block en-\ncodes word contextualized representation while the\naggregator block implements the fusion of knowl-\nedge and language information.\nAccording to the characteristics of medical NLP\ntasks, domain-speciﬁc ﬁnetuning procedure is de-\nsigned. Similar to BioBERT (Lee et al., 2019),\nsymbol “@” and “$” are used to mark the entity\nboundary, which indicate the entity positions in a\nsample and distinguish different relation samples\nsharing the same sentence. For example, the input\nsequence for the relation classiﬁcation task can be\nTable 1: Statistics of UMLS.\n# Entities # Relations # Triples\n2,842,735 874 13,555,037\nIn-degree Out-degree Median degree\n5.05 5.05 4\nmodiﬁed into “[CLS] pain control was initiated\nwith morphine but was then changed to @ demerol\n$, which gave the patient better relief of @ his\nepigastric pain $”. In the entity typing task, entity\nmention and its context are critical to predict the\nentity type, so more localized features of the entity\nmention will beneﬁt this prediction process. In our\nexperiments, the entity start symbol is selected to\nrepresent an entity typing sample.\n3 Experiments\n3.1 Dataset\n3.1.1 Medical Knowledge Graph\nThe Uniﬁed Medical Language System (UMLS)\n(Bodenreider, 2004) is a comprehensive knowledge\nbase in the biomedical domain, which contains\nlarge-scale concept names and relations among\nthem. The metathesaurus in UMLS involves vari-\nous terminology systems and comprises about 14\nmillion terms covering 25 different languages. In\nthis study, a subset of this knowledge base is ex-\ntracted to construct the medical knowledge graph.\nNon-English and long terms are ﬁltered, and the\nﬁnal statistics is shown in Table 1.\n3.1.2 Corpus for Pre-training\nTo ensure that sufﬁcient medical knowledge can\nbe integrated into the language model, PubMed ab-\nstracts2 and PubMed Central full-text papers3 are\nchosen as the pre-training corpus, which are open-\naccess datasets for biomedical and life sciences\njournal literature. Since sentences in different para-\ngraphs may not have good context coherence, para-\ngraphs are selected as the document unit for next\nsentence prediction. The Natural Language Toolkit\n(NLTK)4 is utilized to split the sentences within a\nparagraph, and sentences having less than 5 words\nare discarded. As a result, a large corpus contain-\ning 9.9B tokens is achieved for language model\npre-training.\n2 https://www.ncbi.nlm.nih.gov/pubmed/.\n3https://www.ncbi.nlm.nih.gov/pmc/.\n4https://www.nltk.org/.\n2285\nTable 2: Statistics of the datasets. Most of these datasets do not follow a standard train-valid-test set partition, and\nwe adopt some traditional data partition ways to do model training and evaluation.\nTask Dataset # Train # Valid # Test\nEntity Typing 2010 i2b2/V A (Uzuner et al., 2011) 16,519 - 31,161\nJNLPBA (Kim et al., 2004) 51,301 - 8,653\nBC5CDR (Li et al., 2016) 9,385 9,593 9,809\nRelation Classiﬁcation 2010 i2b2/V A (Uzuner et al., 2011) 10,233 - 19,115\nGAD (Bravo et al., 2015) 5,339 - -\nEU-ADR (Van Mulligen et al., 2012) 355 - -\nIn our model, medical terms appearing in the cor-\npus need to be aligned to the entities in the UMLS\nmetathesaurus before pre-training. To make sure\nthe coverage of identiﬁed entities in the metathe-\nsaurus, the forward maximum matching (FMM)\nalgorithm is used to extract the term spans from the\ncorpus aforementioned, and spans less than 5 char-\nacters are ﬁltered. Then, BERT vocabulary is used\nto tokenize the input text into word pieces, and the\nmedical entity is aligned with the ﬁrst subword of\nthe identiﬁed term.\n3.1.3 Downstream Tasks\nIn this study, entity typing and relation classiﬁca-\ntion tasks in the medical domain are used to evalu-\nate the models.\nEntity Typing Given a sentence with an entity\nmention tagged, this task is to identify the seman-\ntic type of this entity mention. For example, the\ntype “medical problem” is used to label the en-\ntity mention “ asystole” in the sentence “ he had\na differential diagnosis of ⟨e⟩asystole ⟨/e⟩”. To\nthe best of our knowledge, there are no publicly\navailable entity typing datasets in the medical do-\nmain. Therefore, three entity typing datasets are\nconstructed from the corresponding medical named\nentity recognition datasets. Entity mentions and\nentity types are annotated in these datasets, in this\nstudy, entity mentions are considered as input while\nentity types are the output labels. Table 2 shows\nthe statistics of the datasets for the entity typing\ntask. Datasets can be download from here5.\nRelation Classiﬁcation Given two entities\nwithin one sentence, this task aims to determine the\nrelation type between the entities. For example, in\nsentence “pain control was initiated with morphine\nbut was then changed to⟨e1⟩demerol ⟨/e1⟩, which\n5https://drive.google.com/file/d/\n1OletxmPYNkz2ltOr9pyT0b0iBtUWxslh/view.\ngave the patient better relief of ⟨e2⟩his epigastric\npain ⟨/e2⟩”, the relation type between two entities\nis TrIP (Treatment Improves medical Problem). In\nthis study, three relation classiﬁcation datasets are\nutilized to evaluate our models, and the statistics\nof these datasets are shown in Table 2. Datasets\ncan be download from here6.\n3.2 Baselines\nIn addition to the state-of-the-art models on these\ndatasets, we have also added the popular BERT-\nBase model and another two models pre-trained on\nbiomedical literature for further comparison.\nBERT-Base (Devlin et al., 2019) This is the orig-\ninal bidirectional pre-trained language model pro-\nposed by Google, which achieves state-of-the-art\nperformance on a wide range of NLP tasks.\nBioBERT (Lee et al., 2019) This model follows\nthe same model architecture as the BERT-Base\nmodel, but with the PubMed abstracts and PubMed\nCentral full-text articles (about 18B tokens) used\nto do model ﬁnetuning upon BERT-Base.\nSCIBERT (Beltagy et al., 2019) In this model,\na new wordpiece vocabulary is built based on a\nlarge scientiﬁc corpus (about 3.2B tokens). Then,\na new BERT-based model is trained from scratch\nusing this scientiﬁc vocabulary and the scientiﬁc\ncorpus. Since a large portion of the scientiﬁc cor-\npus consists of biomedical articles, this scientiﬁc\nvocabulary can also be regarded as a biomedical\nvocabulary, and helps improve the performance of\ndownstream tasks in the biomedical domain.\n3.3 Implementation Details\n3.3.1 Graph Contextualized Knowledge\nFirstly, UMLS triples are fed into the TransE model\nto achieve a basic knowledge representation. We\n6https://drive.google.com/file/d/\n1-jDKGcXREb2X9xTFnuiJ36PvsqoyHWcw/view.\n2286\nTable 3: Experimental results on the entity typing and relation classiﬁcation tasks. Accuracy (Acc), Precision,\nRecall, and F1 scores are used to evaluate the model performance. The results reported in previous work are under-\nlined. E-SVM is short for Ensemble SVM (Bhasuran and Natarajan, 2018), which achieves SOTA performance in\nthe GAD dataset. CNN-M stands for CNN using multi-pooling (He et al., 2019), which is the SOTA model in the\n2010 i2b2/V A dataset.\nTask Dataset Metrics E-SVM CNN-M BERT-Base BioBERT SCIBERT BERT-MK\nEntity 2010 i2b2/V A Acc - - 96.76 97.43 97.74 97.70\nTyping JNLPBA Acc - - 94.12 94.37 94.60 94.55\nBC5CDR Acc - - 98.78 99.27 99.38 99.54\nRelation 2010 i2b2/V A P - 73.1 72.6 76.1 74.8 77.6\nClassiﬁcation R - 66.7 65.7 71.3 71.6 72.0\nF - 69.7 69.2 73.6 73.1 74.7\nGAD P 79.21 - 74.28 76.43 77.47 81.67\nR 89.25 - 85.11 87.65 85.94 92.79\nF 83.93 - 79.33 81.66 81.45 86.87\nEU-ADR P - - 75.4 5 81.05 78.42 84.43\nR - - 96.55 93.90 90.09 91.17\nF - - 84.71 87.00 85.51 87.49\nuse OpenKE toolkit (Han et al., 2018) to learn en-\ntity and relation embeddings. Knowledge embed-\nding dimension is set to 100, while training epoch\nnumber is set to 10000.\nFollowing the initialization method used in\n(Nguyen et al., 2018; Nathani et al., 2019), the\nembeddings produced by TransE are utilized to\ninitialize knowledge representations of the GCKE\nmodule. We set the layer number to 4, and each\nlayer contains 4 heads. Due to the median degree\nof entities in UMLS is 4 (shown in Table1), we set\nthe count of in-entities and two out-entities to 4, so\neach subgraph contains four 1-hop and four 2-hop\nrelations. The GCKE module runs 1200 epochs on\na single NVIDIA Tesla V100 (32GB) GPU to learn\ngraph contextualized knowledge. The batch size is\nset to 50000.\n3.3.2 Pre-training\nIn this study, two pre-trained language models are\ntrained. The ﬁrst one is MedERNIE, a medical\nERNIE model trained on the UMLS triples and the\nPubMed corpus, inheriting the same model hyper-\nparameters used in (Zhang et al., 2019a). Besides,\nthe entity embeddings learned by GCKE module\nare integrated into the language model to train the\nBERT-MK model. In our work, we align the same\npre-training epochs with BioBERT, which uses the\nsame pre-training corpus as ours, and ﬁnetune the\nBERT-Base model on the PubMed corpus for one\nepoch.\n3.3.3 Finetune\nAs shown in Table 2, there is no standard valid or\ntest set in some datasets. For datasets containing\na standard test set, if no standard valid set is pro-\nvided, we divide the training set into new train/valid\nsets by 4:1. We preform each experiment 5 times\nunder speciﬁc experimental settings with different\nrandom seeds. Besides, 10-fold cross-validation\nmethod is used to evaluate the model performance\nfor the datasets without a standard test set. Accord-\ning to the maximum sequence length of the sen-\ntences in each dataset, the input sequence length\nfor 2010 i2b2/V A (Uzuner et al., 2011), JNLPBA\n(Kim et al., 2004), BC5CDR (Li et al., 2016), GAD\n(Bravo et al., 2015) and EU-ADR (Van Mulligen\net al., 2012) are set to 390, 280, 280, 130 and 220,\nrespectively. The initial learning rate is set to 2e-5.\n3.4 Results\n3.4.1 Entity Typing\nTable 3 presents the experimental results on the\nentity typing and relation classiﬁcation tasks. For\nentity typing tasks, all these pre-trained language\nmodels achieve high accuracy, indicating that the\ntype of a medical entity is not as ambiguous as\nthat in the general domain. BERT-MK outperforms\nBERT-Base and BioBERT on three datasets, and is\ncompetitive with SCIBERT. Without using exter-\nnal knowledge in the pre-trained language model,\nSCIBERT achieves comparable results to BERT-\nMK, which proves that a domain-speciﬁc vocab-\nulary is critical to the feature encoding of inputs.\nLong tokens are relatively common in the medical\ndomain, and these tokens will be split into short\npieces when a domain-independent vocabulary is\nused, which will cause an overgeneralization of\nlexical features. Therefore, a medical vocabulary\ngenerated by the PubMed corpus can be introduced\n2287\ninto BERT-MK in the following work.\n3.4.2 Relation Classiﬁcation\nOn the relation classiﬁcation tasks, BERT-Base\ndoes not perform as well as other models, which in-\ndicates that pre-trained language models require a\ndomain adaptation process when used in restricted\ndomains. Compared with BioBERT, which utilizes\nthe same domain-speciﬁc corpus as ours for domain\nadaptation, BERT-MK improves the F score of\n2010 i2b2/V A, GAD and EU-ADR by 1.1%, 5.21%\nand 0.49%, respectively, which demonstrates medi-\ncal knowledge has indeed played a positive role in\nthe identiﬁcation of medical relations.\nThe following example provides a brief expla-\nnation of why medical knowledge improves the\nmodel performance of the relation classiﬁcation\ntasks. “On postoperative day number three , patient\nwent into ⟨e1⟩atrial ﬁbrillation ⟨/e1⟩, which was\ntreated appropriately with ⟨e2⟩metoprolol ⟨/e2⟩\nand digoxin and converted back to sinus rhythm”\nis a relation sample from the 2010 i2b2/V A dataset,\nand the relation label isTrIP. Meanwhile, the above\nentity pair can be aligned to a knowledge triple\n(atrial ﬁbrillation, may be treated by, metoprolol)\nin the medical knowledge graph. Obviously, this\nknowledge information is advantageous to identify\nthe relation type of the aforementioned example.\n3.5 Discussion\n3.5.1 TransE vs. GCKE\nIn order to explicitly analyze the improvement ef-\nfect of the GCKE module on pre-trained language\nmodels, we compare MedERNIE (TransE-based)\nand BERT-MK (GCKE-based) on two relation clas-\nsiﬁcation datasets. Table 4 demonstrates the re-\nsults of these two models. As we can see, inte-\ngrating graph contextualized knowledge into the\npre-trained language model, the performance in-\ncreases F score by 0.9% and 0.64% on these two\nrelation classiﬁcation datasets, respectively.\nIn Figure 4, as the amount of pre-training data\nincreases, BERT-MK always outperforms Med-\nERNIE on the 2010 i2b2/V A relation dataset, and\nTable 4: TransE vs. GCKE on the 2010 i2b2/V A rela-\ntion and GAD datasets.\nDataset MedERNIE BERT-MK\nP R F P R F\n2010 i2b2/V A 76.6 71.1 73.8 77.6 72.0 74.7\nGAD 81.28 91.86 86.23 81.67 92.79 86.87\nFigure 4: Model performance comparison with in-\ncreasing amount of the pre-trained data. The x-axis\nrepresents the proportion of the medical data used for\npre-training. 0 means no medical data is utilized, so the\nBERT-Base is used as an initialization parameter for\nthe model ﬁnetuning. 100 indicates the model is pre-\ntrained on the medical corpus for one epoch. BioBERT\npre-trains on the PubMed corpus for one epoch, which\nis drawn with dashed lines in the ﬁgure as a comparable\nbaseline.\nthe performance gap has an increasing trend. How-\never, on the GAD dataset, the performance of\nBERT-MK and MedERNIE are intertwined. We\nlink the entities in each relation sample to the med-\nical KG, and ﬁnd that some entity pairs have a con-\nnected relationship in the KG. Statistical analysis\non 2-hop neighbor relationships between these en-\ntity pairs shows that there are 136 cases in the 2010\ni2b2/V A dataset, while only 1 in GAD. The second\ncase shown in Table 5 gives an example of the ob-\nservation described above. Triple (CAD, member of,\nOther ischemic heart disease) and (Other ischemic\nheart disease, has member, Angina symptom) are\ntriples in the medical KG, which indicates entity\npair cad and angina symptoms in the relation sam-\nple have a 2-hop neighbor relationship in the KG.\nGCKE learns these 2-hop neighbor relationships\nin 2010 i2b2/V A and produces an improvement for\nBERT-MK. However, due to the characteristics of\n2288\nTable 5: Case study on the 2010 i2b2/V A relation dataset. The bold text spans in two cases are entities. In the\nﬁrst case, the corresponding triple can help identify the relationship between the entity pair in this relation sample.\nNPP, no relation between two medical problems; PIP, medical problem indicates medical problem. MI, myocardial\ninfarction; CAD, coronary artery disease.\nCases The Corresponding Triples BioBERT MedERNIE BERT-\nMK\nGround\nTruth\n1 ... coronary artery disease, status post mi x0, cabg ... (Coronary artery disease, associated with , MI) NPP PIP PIP PIP\n2 0. cad: presented with anginal symptoms and ekg\nchanges (stemi), with cardiac catheterization revealing\nlesions in lad, lcx, and plb.\n(CAD, member of, Other ischemic heart dis-\nease); (Other ischemic heart disease, has mem-\nber, Angina symptom)\nNPP NPP PIP PIP\nthe GAD dataset, the capability of GCKE is lim-\nited.\n3.5.2 Effect of Different Corpus Sizes in\nPre-training\nFigure 4 shows the model performance comparison\nwith different proportion of the pre-training corpus.\nFrom this ﬁgure, we observe that BERT-MK out-\nperforms BioBERT by using only 10%-20% of the\ncorpus, which indicates that medical knowledge\nhas the capability to enhance pre-trained language\nmodels and save computational costs (Schwartz\net al., 2019).\n4 Related Work\nPre-trained language models represented by ELMO\n(Peters et al., 2018), GPT (Radford et al., 2018) and\nBERT (Devlin et al., 2019) have attracted great at-\ntention, and a large number of variant models have\nbeen proposed. Among these studies, some re-\nsearchers devote their efforts to introducing knowl-\nedge into language models (Levine et al., 2019;\nLauscher et al., 2019; Liu et al., 2019; Zhang et al.,\n2019b). ERNIE-Baidu (Sun et al., 2019) introduces\nnew masking units such as phrases and entities\nto learn knowledge information in these masking\nunits. As a reward, syntactic and semantic infor-\nmation from phrases and entities is implicitly in-\ntegrated into the language model. Furthermore,\na different knowledge information is explored in\nERNIE-Tsinghua (Zhang et al., 2019a), which in-\ncorporates knowledge graph into BERT to learn\nlexical, syntactic and knowledge information si-\nmultaneously. Xiong et al. (2019) introduce entity\nreplacement checking task into the pre-trained lan-\nguage model, and improve several entity-related\ndownstream tasks, such as question answering and\nentity typing. Wang et al. (2020) propose a plug-in\nway to infuse knowledge into language models, and\ntheir method keeps different kinds of knowledge\nin different adapters. The knowledge information\nintroduced by these methods does not pay much\nattention to the graph contextualized knowledge in\nthe KG.\nRecently, several KRL methods have attempted\nto introduce more contextualized information into\nknowledge representations. Relational Graph Con-\nvolutional Networks (R-GCNs) (Schlichtkrull et al.,\n2018) is proposed to learn entity embeddings from\ntheir incoming neighbors, which greatly enhances\nthe information interaction between related triples.\nNathani et al. (2019) further extend the informa-\ntion ﬂow from 1-hop in-entities to n-hop during\nthe learning process of entity representations, and\nachieves the SOTA performance on multiple rela-\ntion prediction datasets, especially for the ones con-\ntaining higher in-degree nodes. We believe that the\ninformation contained in knowledge graphs is far\nfrom being sufﬁciently exploited. In this study, we\ndevelop an approach to integrate more graph con-\ntextualized information, which models subgraphs\nas training samples. This module has the ability to\nmodel any information in the KG. In addition, this\nlearned knowledge is integrated into the language\nmodel to obtain an enhanced version of the medical\npre-trained language model.\n5 Conclusion and Future Work\nWe propose a novel approach to learn more com-\nprehensive knowledge, focusing on modeling sub-\ngraphs in the knowledge graph by a knowledge\nlearning module. Additionally, the learned medical\nknowledge is integrated into the pre-trained lan-\nguage model, which outperforms BERT-Base and\nanother two domain-speciﬁc pre-trained language\nmodels on several medical NLP tasks. Our work\nvalidates the intuition that medical knowledge is\nbeneﬁcial to some medical NLP tasks and provides\na preliminary exploration for the application of\nmedical knowledge.\nIn the follow-up work, some knowledge-guided\ntasks will be used to validate the effectiveness of\nthe knowledge learning module GCKE. Moreover,\nwe will explore some other knowledge injection\n2289\nways to combine medical knowledge with language\nmodels, such as multi-task learning. More sub-\ngraph sampling strategies need to be explored, such\nas r-ego subgraph (Qiu et al., 2020) and degree-\ndependent subgraph.\nAcknowledgment\nThe authors would like to thank all the anonymous\nreviewers for their insightful comments. Thank\nYasheng Wang for his help in code implementation.\nReferences\nIz Beltagy, Arman Cohan, and Kyle Lo. 2019. Scibert:\nPretrained contextualized embeddings for scientiﬁc\ntext. arXiv preprint arXiv:1903.10676.\nBalu Bhasuran and Jeyakumar Natarajan. 2018. Auto-\nmatic extraction of gene-disease associations from\nliterature using joint ensemble learning. PloS one,\n13(7):e0200699.\nOlivier Bodenreider. 2004. The uniﬁed medical lan-\nguage system (umls): integrating biomedical termi-\nnology. Nucleic acids research, 32(suppl 1):D267–\nD270.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n2013. Translating embeddings for modeling multi-\nrelational data. In Advances in neural information\nprocessing systems, pages 2787–2795.\n`Alex Bravo, Janet Pi ˜nero, N ´uria Queralt-Rosinach,\nMichael Rautschka, and Laura I Furlong. 2015. Ex-\ntraction of relations between genes and diseases\nfrom text and large-scale data analysis: implica-\ntions for translational research. BMC bioinformat-\nics, 16(1):55.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nJohn R Firth. 1957. A synopsis of linguistic theory,\n1930-1955. Studies in linguistic analysis.\nXu Han, Shulin Cao, Xin Lv, Yankai Lin, Zhiyuan Liu,\nMaosong Sun, and Juanzi Li. 2018. Openke: An\nopen toolkit for knowledge embedding. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing: System Demon-\nstrations, pages 139–144.\nZellig S Harris. 1954. Distributional structure. Word,\n10(2-3):146–162.\nBin He, Yi Guan, and Rui Dai. 2019. Classifying med-\nical relations in clinical text via convolutional neural\nnetworks. Artiﬁcial intelligence in medicine, 93:43–\n49.\nJin-Dong Kim, Tomoko Ohta, Yoshimasa Tsuruoka,\nYuka Tateisi, and Nigel Collier. 2004. Introduction\nto the bio-entity recognition task at jnlpba. In Pro-\nceedings of the international joint workshop on nat-\nural language processing in biomedicine and its ap-\nplications, pages 70–75. Citeseer.\nAnne Lauscher, Ivan Vuli ´c, Edoardo Maria Ponti,\nAnna Korhonen, and Goran Glava ˇs. 2019. Inform-\ning unsupervised pretraining with external linguistic\nknowledge. arXiv preprint arXiv:1909.02339.\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim,\nDonghyeon Kim, Sunkyu Kim, Chan Ho So, and\nJaewoo Kang. 2019. Biobert: pre-trained biomed-\nical language representation model for biomedical\ntext mining. arXiv preprint arXiv:1901.08746.\nYoav Levine, Barak Lenz, Or Dagan, Dan Padnos,\nOr Sharir, Shai Shalev-Shwartz, Amnon Shashua,\nand Yoav Shoham. 2019. Sensebert: Driving some\nsense into bert. arXiv preprint arXiv:1908.05646.\nJiao Li, Yueping Sun, Robin J Johnson, Daniela Sci-\naky, Chih-Hsuan Wei, Robert Leaman, Allan Peter\nDavis, Carolyn J Mattingly, Thomas C Wiegers, and\nZhiyong Lu. 2016. Biocreative v cdr task corpus:\na resource for chemical disease relation extraction.\nDatabase, 2016.\nYankai Lin, Xu Han, Ruobing Xie, Zhiyuan Liu, and\nMaosong Sun. 2018. Knowledge representation\nlearning: A quantitative review. arXiv preprint\narXiv:1812.10901.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2019. K-bert:\nEnabling language representation with knowledge\ngraph. arXiv preprint arXiv:1909.07606.\nDeepak Nathani, Jatin Chauhan, Charu Sharma, and\nManohar Kaul. 2019. Learning attention-based\nembeddings for relation prediction in knowledge\ngraphs. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 4710–4723, Florence, Italy. Association\nfor Computational Linguistics.\nDai Quoc Nguyen, Tu Dinh Nguyen, Dat Quoc\nNguyen, and Dinh Phung. 2018. A novel embed-\nding model for knowledge base completion based\non convolutional neural network. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 2 (Short Pa-\npers), pages 327–333.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of NAACL-HLT, pages\n2227–2237.\n2290\nJiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang,\nHongxia Yang, Ming Ding, Kuansan Wang, and Jie\nTang. 2020. Gcc: Graph contrastive coding for\ngraph neural network pre-training. arXiv preprint\narXiv:2006.09963.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem,\nRianne Van Den Berg, Ivan Titov, and Max Welling.\n2018. Modeling relational data with graph convolu-\ntional networks. In European Semantic Web Confer-\nence, pages 593–607. Springer.\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren\nEtzioni. 2019. Green ai.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\n¨Ozlem Uzuner, Brett R South, Shuying Shen, and\nScott L DuVall. 2011. 2010 i2b2/va challenge on\nconcepts, assertions, and relations in clinical text.\nJournal of the American Medical Informatics Asso-\nciation, 18(5):552–556.\nErik M Van Mulligen, Annie Fourrier-Reglat, David\nGurwitz, Mariam Molokhia, Ainhoa Nieto, Gian-\nluca Triﬁro, Jan A Kors, and Laura I Furlong. 2012.\nThe eu-adr corpus: annotated drugs, diseases, tar-\ngets, and their relationships. Journal of biomedical\ninformatics, 45(5):879–884.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Li `o, and Yoshua Bengio.\n2018. Graph Attention Networks. International\nConference on Learning Representations.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Cuihong Cao, Daxin Jiang, Ming\nZhou, et al. 2020. K-adapter: Infusing knowl-\nedge into pre-trained models with adapters. arXiv\npreprint arXiv:2002.01808.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2019. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. arXiv preprint arXiv:1912.09637.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019a. ERNIE: En-\nhanced language representation with informative en-\ntities. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguis-\ntics, pages 1441–1451, Florence, Italy. Association\nfor Computational Linguistics.\nZhuosheng Zhang, Yuwei Wu, Hai Zhao, Zuchao Li,\nShuailiang Zhang, Xi Zhou, and Xiang Zhou. 2019b.\nSemantics-aware bert for language understanding.\narXiv preprint arXiv:1909.02209.\nA Appendices\nA.1 Comparison between MedERNIE and\nBERT-MK\nAs shown in Table 6, BERT-MK outperforms Med-\nERNIE on all datasets except BC5CDR.\nTable 6: MedERNIE vs. BERT-MK.\nEntity Typing (Acc)\n2010\ni2b2/V A\nJNLPBA BC5CDR\nMedERNIE 97.37 94.46 99.62\nBERT-MK 97.70 94.55 99.54\nRelation Classiﬁcation (F)\n2010\ni2b2/V A\nGAD EU-ADR\nMedERNIE 73.8 86.23 86.99\nBERT-MK 74.7 86.87 87.49",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7642580270767212
    },
    {
      "name": "Knowledge graph",
      "score": 0.6862565279006958
    },
    {
      "name": "Graph",
      "score": 0.5634328722953796
    },
    {
      "name": "Generalization",
      "score": 0.561445415019989
    },
    {
      "name": "Knowledge representation and reasoning",
      "score": 0.53034508228302
    },
    {
      "name": "Natural language processing",
      "score": 0.5040827989578247
    },
    {
      "name": "Medical knowledge",
      "score": 0.4975140392780304
    },
    {
      "name": "Language model",
      "score": 0.4963257908821106
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4952508509159088
    },
    {
      "name": "Node (physics)",
      "score": 0.4366473853588104
    },
    {
      "name": "Open Knowledge Base Connectivity",
      "score": 0.42358818650245667
    },
    {
      "name": "Knowledge base",
      "score": 0.4231944978237152
    },
    {
      "name": "Theoretical computer science",
      "score": 0.42069047689437866
    },
    {
      "name": "Knowledge-based systems",
      "score": 0.41808032989501953
    },
    {
      "name": "Domain knowledge",
      "score": 0.41276291012763977
    },
    {
      "name": "Knowledge management",
      "score": 0.1840682327747345
    },
    {
      "name": "Personal knowledge management",
      "score": 0.11203056573867798
    },
    {
      "name": "Mathematics",
      "score": 0.09019061923027039
    },
    {
      "name": "Organizational learning",
      "score": 0.08189135789871216
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Medical education",
      "score": 0.0
    },
    {
      "name": "Structural engineering",
      "score": 0.0
    },
    {
      "name": "Medicine",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210116209",
      "name": "The Ark",
      "country": "IE"
    },
    {
      "id": "https://openalex.org/I126520041",
      "name": "University of Science and Technology of China",
      "country": "CN"
    }
  ]
}