{
    "title": "TransBTS: Multimodal Brain Tumor Segmentation Using Transformer",
    "url": "https://openalex.org/W3135385363",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2320793692",
            "name": "Wang, Wenxuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2095964268",
            "name": "Chen Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2154584843",
            "name": "Ding Meng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2490704442",
            "name": "Li Jiangyun",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1978159643",
            "name": "Yu Hong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221528281",
            "name": "Zha, Sen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3090974769",
        "https://openalex.org/W2884436604",
        "https://openalex.org/W1641498739",
        "https://openalex.org/W2900298334",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W2774320778",
        "https://openalex.org/W1903029394",
        "https://openalex.org/W2798122215",
        "https://openalex.org/W3031346872",
        "https://openalex.org/W2962914239",
        "https://openalex.org/W2751069891",
        "https://openalex.org/W2888358068",
        "https://openalex.org/W3031101471",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3014512070",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W3031146363",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2464708700",
        "https://openalex.org/W3116489684"
    ],
    "abstract": "Transformer, which can benefit from global (long-range) information modeling using self-attention mechanisms, has been successful in natural language processing and 2D image classification recently. However, both local and global features are crucial for dense prediction tasks, especially for 3D medical image segmentation. In this paper, we for the first time exploit Transformer in 3D CNN for MRI Brain Tumor Segmentation and propose a novel network named TransBTS based on the encoder-decoder structure. To capture the local 3D context information, the encoder first utilizes 3D CNN to extract the volumetric spatial feature maps. Meanwhile, the feature maps are reformed elaborately for tokens that are fed into Transformer for global feature modeling. The decoder leverages the features embedded by Transformer and performs progressive upsampling to predict the detailed segmentation map. Extensive experimental results on both BraTS 2019 and 2020 datasets show that TransBTS achieves comparable or higher results than previous state-of-the-art 3D methods for brain tumor segmentation on 3D MRI scans. The source code is available at https://github.com/Wenxuan-1119/TransBTS",
    "full_text": "TransBTS: Multimodal Brain Tumor\nSegmentation Using Transformer\nWenxuan Wang1, Chen Chen2, Meng Ding3, Hong Yu1, Sen Zha1, Jiangyun\nLi1,†\n1 School of Automation and Electrical Engineering, University of Science and\nTechnology Beijing, China, s20200579@xs.ustb.edu.cn,\ng20198754@xs.ustb.edu.cn, g20198675@xs.ustb.edu.cn, leejy@ustb.edu.cn\n2 Center for Research in Computer Vision, University of Central Florida, USA,\nchen.chen@ucf.edu\n3 Scoop Medical, Houston, TX, USA, meng.ding@okstate.edu\n†Corresponding author: Jiangyun Li\nAbstract. Transformer, which can beneﬁt from global (long-range) in-\nformation modeling using self-attention mechanisms, has been success-\nful in natural language processing and 2D image classiﬁcation recently.\nHowever, both local and global features are crucial for dense predic-\ntion tasks, especially for 3D medical image segmentation. In this paper,\nwe for the ﬁrst time exploit Transformer in 3D CNN for MRI Brain\nTumor Segmentation and propose a novel network named TransBTS\nbased on the encoder-decoder structure. To capture the local 3D con-\ntext information, the encoder ﬁrst utilizes 3D CNN to extract the volu-\nmetric spatial feature maps. Meanwhile, the feature maps are reformed\nelaborately for tokens that are fed into Transformer for global feature\nmodeling. The decoder leverages the features embedded by Transformer\nand performs progressive upsampling to predict the detailed segmen-\ntation map. Extensive experimental results on both BraTS 2019 and\n2020 datasets show that TransBTS achieves comparable or higher re-\nsults than previous state-of-the-art 3D methods for brain tumor seg-\nmentation on 3D MRI scans. The source code is available at https:\n//github.com/Wenxuan-1119/TransBTS.\nKeywords: Segmentation · Brain Tumor· MRI · Transformer · 3D CNN\n1 Introduction\nGliomas are the most common malignant brain tumors with diﬀerent levels of\naggressiveness. Automated and accurate segmentation of these malignancies on\nmagnetic resonance imaging (MRI) is of vital importance for clinical diagnosis.\nConvolutional Neural Networks (CNN) have achieved great success in vari-\nous vision tasks such as classiﬁcation, segmentation and object detection. Fully\nConvolutional Networks (FCN) [10] realize end-to-end semantic segmentation\nfor the ﬁrst time with impressive results. U-Net [15] uses a symmetric encoder-\ndecoder structure with skip-connections to improve detail retention, becoming\narXiv:2103.04430v2  [cs.CV]  26 Jun 2021\n2 Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, Jiangyun Li\nthe mainstream architecture for medical image segmentation. Many U-Net vari-\nants such as U-Net++ [24] and Res-UNet [23] further improve the performance\nfor image segmentation. Although CNN-based methods have excellent represen-\ntation ability, it is diﬃcult to build an explicit long-distance dependence due\nto limited receptive ﬁelds of convolution kernels. This limitation of convolution\noperation raises challenges to learn global semantic information which is critical\nfor dense prediction tasks like segmentation.\nInspired by the attention mechanism [1] in natural language processing, ex-\nisting research overcomes this limitation by fusing the attention mechanism with\nCNN models. Non-local neural networks [21] design a plug-and-play non-local\noperator based on the self-attention mechanism, which can capture the long-\ndistance dependence in the feature map but suﬀers from the high memory and\ncomputation cost. Schlemper et al. [16] propose an attention gate model, which\ncan be integrated into standard CNN models with minimal computational over-\nhead while increasing the model sensitivity and prediction accuracy. On the\nother hand, Transformer [19] is designed to model long-range dependencies in\nsequence-to-sequence tasks and capture the relations between arbitrary positions\nin the sequence. This architecture is proposed based solely on self-attention, dis-\npensing with convolutions entirely. Unlike previous CNN-based methods, Trans-\nformer is not only powerful in modeling global context, but also can achieve\nexcellent results on downstream tasks in the case of large-scale pre-training.\nRecently, Transformer-based frameworks have also reached state-of-the-art\nperformance on various computer vision tasks. Vision Transformer (ViT) [7]\nsplits the image into patches and models the correlation between these patches\nas sequences with Transformer, achieving satisfactory results on image classiﬁ-\ncation. DeiT [17] further introduces a knowledge distillation method for training\nTransformer. DETR [4] treats object detection as a set prediction task with the\nhelp of Transformer. TransUNet [5] is a concurrent work which employs ViT\nfor medical image segmentation. We will elaborate the diﬀerences between our\napproach and TransUNet in Sec. 2.4.\nResearch Motivation. The success of Transformer has been witnessed\nmostly on image classiﬁcation. For dense prediction tasks such as segmenta-\ntion, both local and global (or long-range) information is important. However,\nas pointed out by [22], local structures are ignored when directly splitting im-\nages into patches as tokens for Transformer. Moreover, for medical volumetric\ndata (e.g. 3D MRI scans) which is beyond 2D, local feature modeling among\ncontinuous slices (i.e. depth dimension) is also critical for volumetric segmen-\ntation. We are therefore inspired to ask: How to design a neural network that\ncan eﬀectively model local and global features in spatial and depth dimensions of\nvolumetric data by leveraging the highly expressive Transformer?\nIn this paper, we present theﬁrst attempt to exploit Transformer in 3D CNN\nfor 3D MRI Brain Tumor Segmentation (TransBTS). The proposed TransBTS\nbuilds upon the encoder-decoder structure. The network encoder ﬁrst utilizes\n3D CNN to extract the volumetric spatial features and downsample the input\n3D images at the same time, resulting in compact volumetric feature maps that\nTransBTS: Multimodal Brain Tumor Segmentation Using Transformer 3\neﬀectively captures the local 3D context information. Then each volume is re-\nshaped into a vector (i.e. token) and fed into Transformer for global feature\nmodeling. The 3D CNN decoder takes the feature embedding from Transformer\nand performs progressive upsampling to predict the full resolution segmenta-\ntion map. Experiments on BraTS 2019 and 2020 datasets show that TransBTS\nachieves comparable or higher results than previous state-of-the-art 3D methods\nfor brain tumor segmentation on 3D MRI scans. We also conduct comprehen-\nsive ablation study to shed light on architecture engineering of incorporating\nTransformer in 3D CNN to unleash the power of both architectures. We hope\nTransBTS can serve as a strong 3D baseline to facilitate future research on\nvolumetric segmentation.\n2 Method\n2.1 Overall Architecture of TransBTS\nAn overview of the proposed TransBTS is presented in Fig. 1. Concretely, given\nan input MRI scan X ∈RC×H×W×D with a spatial resolution of H ×W, depth\ndimension of D (# of slices) and C channels (# of modalities), we ﬁrst utilize\n3D CNN to generate compact feature maps capturing spatial and depth infor-\nmation, and then leverage the Transformer encoder to model the long distance\ndependency in a global space. After that, we repeatedly stack the upsampling\nand convolutional layers to gradually produce a high-resolution segmentation\nresult. The network details of TransBTS are provided in the Appendix. Next,\nwe will describe the components of TransBTS in detail.\nTransformer LayerTransformer Layer\n……\n…\nLinear Projection\nfPE\nDownsample: StridedConvolutionUpsample: Deconvolution \nFf\nX \nz0 zL\nR(C, H, W, D)\n(16, H, W, D)\n(32, H/2, W/2, D/2)\n(64, H/4, W/4, D/4)\n(128, H/8, W/8, D/8)(128, H/8, W/8, D/8)\n(d=512, N)\nLinear Projection3×3×3 Convolution àReshapePEPosition Embedding\nFFNLayer Norm \nLayer Norm \nMulti-HeadAttention\n×L\nFeature Concatenation\nSkip-Connection\nFeature Mapping\nFeature MappingReshape à3×3×3 Convolution\nElement-wise Addition\nZ\n(H, W, D)\nFig. 1.Overall architecture of the proposed TransBTS.\n4 Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, Jiangyun Li\n2.2 Network Encoder\nAs the computational complexity of Transformer is quadratic with respect to\nthe number of tokens (i.e. sequence length), directly ﬂattening the input im-\nage to a sequence as the Transformer input is impractical. Therefore, ViT [7]\nsplits an image into ﬁxed-size (16 ×16) patches and then reshapes each patch\ninto a token, reducing the sequence length to 16 2. For 3D volumetric data, the\nstraightforward tokenization, following ViT, would be splitting the data into 3D\npatches. However, this simple strategy makes Transformer unable to model the\nimage local context information across spatial and depth dimensions for volumet-\nric segmentation. To address this challenge, our solution is to stack the 3 ×3 ×3\nconvolution blocks with downsamping (strided convolution with stride=2) to\ngradually encode input images into low-resolution/high-level feature representa-\ntion F ∈RK×H\n8 ×W\n8 ×D\n8 (K = 128), which is 1/8 of input dimensions of H, W\nand D (overall stride (OS)=8). In this way, rich local 3D context features are ef-\nfectively embedded in F. Then, F is fed into the Transformer encoder to further\nlearn long-range correlations with a global receptive ﬁeld.\nFeature Embedding of Transformer Encoder.Given the feature map F,\nto ensure a comprehensive representation of each volume, a linear projection (a\n3×3×3 convolutional layer) is used to increase the channel dimension fromK =\n128 to d = 512. The Transformer layer expects a sequence as input. Therefore, we\ncollapse the spatial and depth dimensions into one dimension, resulting in ad×N\n(N = H\n8 ×W\n8 ×D\n8 ) feature map f, which can be also regarded asN d-dimensional\ntokens. To encode the location information which is vital in segmentation task,\nwe introduce the learnable position embeddings and fuse them with the feature\nmap f by direct addition, creating the feature embeddings as follows:\nz0 = f + PE = W ×F + PE (1)\nwhere W is the linear projection operation, PE ∈Rd×N denotes the position\nembeddings, and z0 ∈Rd×N refers to the feature embeddings.\nTransformer Layers.The Transformer encoder is composed of L Transformer\nlayers, each of them has a standard architecture, which consists of a Multi-Head\nAttention (MHA) block and a Feed Forward Network (FFN). The output of the\nℓ-th (ℓ ∈[1, 2, ..., L]) Transformer layer can be calculated by:\nz\n′\nℓ = MHA (LN(zℓ−1)) + zℓ−1 (2)\nzℓ = FFN (LN(z\n′\nℓ)) + z\n′\nℓ (3)\nwhere LN(∗) denotes the layer normalization and zℓ is the output of ℓ-th Trans-\nformer layer.\n2.3 Network Decoder\nIn order to generate the segmentation results in the original 3D image space\n(H ×W ×D), we introduce a 3D CNN decoder to perform feature upsampling\nand pixel-level segmentation (see the right part of Fig. 1).\nTransBTS: Multimodal Brain Tumor Segmentation Using Transformer 5\nFeature Mapping. To ﬁt the input dimension of 3D CNN decoder, we ﬁrst\ndesign a feature mapping module to project the sequence data back to a standard\n4D feature map. Speciﬁcally, the output sequence of Transformer zL ∈Rd×N is\nﬁrst reshaped to d×H\n8 ×W\n8 ×D\n8 . In order to reduce the computational complexity\nof decoder, a convolution block is employed to reduce the channel dimension from\nd to K. Through these operations, the feature map Z ∈RK×H\n8 ×W\n8 ×D\n8 , which\nhas the same dimension as F in the feature encoding part, is obtained.\nProgressive Feature Upsampling.After the feature mapping, cascaded up-\nsampling operations and convolution blocks are applied toZ to gradually recover\na full resolution segmentation result R ∈RH×W×D. Moreover, skip-connections\nare employed to fuse the encoder features with the decoder counterparts by\nconcatenation for ﬁner segmentation masks with richer spatial details.\n2.4 Discussion\nA very recent work TransUNet [5] also employs Transformer for medical im-\nage segmentation. Here we want to highlight a few key distinctions between our\nTransBTS and TransUNet. (1) TransUNet is a 2D network that processes each\n3D medical image in a slice-by-slice manner. However, our TransBTS is based\non 3D CNN and processes all the image slices at a time, allowing the exploita-\ntion of better representations of continuous information between slices. In other\nwords, TransUNet only focuses on the spatial correlation between tokenized im-\nage patches, but our method can model the long-range dependencies in both\nslice/depth dimension and spatial dimension simultaneously for volumetric seg-\nmentation. (2) As TransUNet adopts the ViT structure, it relies on pre-trained\nViT models on large-scale image datasets. In contrast, our TransBTS has a ﬂex-\nible network design and is trained from scratch on task-speciﬁc dataset without\nthe dependence on pre-trained weights.\n3 Experiments\nData and Evaluation Metric.The ﬁrst 3D MRI dataset used in the exper-\niments is provided by the Brain Tumor Segmentation (BraTS) 2019 challenge\n[11,2,3]. It contains 335 cases of patients for training and 125 cases for valida-\ntion. Each sample is composed of four modalities of brain MRI scans, namely\nnative T1-weighted (T1), post-contrast T1-weighted (T1ce), T2-weighted (T2)\nand Fluid Attenuated Inversion Recovery (FLAIR). Each modality has a volume\nof 240×240×155 which has been aligned into the same space. The labels contain\n4 classes: background (label 0), necrotic and non-enhancing tumor (label 1), per-\nitumoral edema (label 2) and GD-enhancing tumor (label 4). The segmentation\naccuracy is measured by the Dice score and the Hausdorﬀ distance (95%) metrics\nfor enhancing tumor region (ET, label 1), regions of the tumor core (TC, labels\n1 and 4), and the whole tumor region (WT, labels 1,2 and 4). The second 3D\nMRI dataset is provided by the Brain Tumor Segmentation Challenge (BraTS)\n2020 [11,2,3]. It consists of 369 cases for training, 125 cases for validation and\n6 Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, Jiangyun Li\n166 cases for testing. Except for the number of samples in the dataset, the other\ninformation about these two datasets are the same.\nImplementation Details.The proposed TransBTS is implemented in Pytorch\nand trained with 8 NVIDIA Titan RTX GPUs (each has 24GB memory) for 8000\nepochs from scratch using a batch size of 16. We adopt the Adam optimizer to\ntrain the model. The initial learning rate is set to 0.0004 with a poly learning\nrate strategy, in which the initial rate decays by each iteration with power 0.9.\nThe following data augmentation techniques are applied: (1) random cropping\nthe data from 240 ×240 ×155 to 128 ×128 ×128 voxels; (2) random mirror\nﬂipping across the axial, coronal and sagittal planes by a probability of 0.5;\n(3) random intensity shift between [-0.1, 0.1] and scale between [0.9, 1.1]. The\nsoftmax Dice loss is employed to train the network and L2 Norm is also applied\nfor model regularization with a weight decay rate of 10 −5. In the testing phase,\nwe utilize Test Time Augmentation (TTA) to further improve the performance\nof our proposed TransBTS.\nTable 1.Comparison on BraTS 2019 validation set.\nMethod Dice Score (%) ↑ Hausdorﬀ Dist. (mm) ↓\nET WT TC ET WT TC\n3D U-Net [6] 70.86 87.38 72.48 5.062 9.432 8.719\nV-Net [12] 73.89 88.73 76.56 6.131 6.256 8.705\nKiU-Net [18] 73.21 87.60 73.92 6.323 8.942 9.893\nAttention U-Net [14] 75.96 88.81 77.20 5.202 7.756 8.258\nWang et al. [20] 73.70 89.40 80.70 5.994 5.677 7.357\nLi et al. [9] 77.10 88.60 81.30 6.033 6.232 7.409\nFrey et al. [8] 78.7 89.6 80.0 6.005 8.171 8.241\nMyronenko et al. [13] 80.0 89.4 83.4 3.921 5.89 6.562\nTransBTS w/o TTA78.36 88.89 81.41 5.908 7.599 7.584\nTransBTS w/ TTA 78.93 90.00 81.94 3.736 5.644 6.049\nTable 2.Comparison on BraTS 2020 validation set.\nMethod Dice Score (%) ↑ Hausdorﬀ Dist. (mm) ↓\nET WT TC ET WT TC\n3D U-Net [6] 68.76 84.11 79.06 50.983 13.366 13.607\nBasic V-Net [12] 61.79 84.63 75.26 47.702 20.407 12.175\nDeeper V-Net [12] 68.97 86.11 77.90 43.518 14.499 16.153\nResidual 3D U-Net 71.63 82.46 76.47 37.422 12.337 13.105\nTransBTS w/o TTA78.50 89.00 81.36 16.716 6.469 10.468\nTransBTS w/ TTA 78.73 90.09 81.73 17.947 4.964 9.769\n3.1 Main Results\nBraTS 2019.We ﬁrst conduct ﬁve-fold cross-validation evaluation on the train-\ning set – a conventional setting followed by many existing works. Our TransBTS\nachieves average Dice scores of 78.69%, 90.98%, 82.85% respectively for ET, WT\nand TC. We also conduct experiments on the BraTS 2019 validation set and\nTransBTS: Multimodal Brain Tumor Segmentation Using Transformer 7\n3D U-Net VNet Attention U-Net TransBTS (Ours) Ground Truth\nFig. 2.The visual comparison of MRI brain tumor segmentation results.\ncompare TransBTS with state-of-the-art (SOTA) 3D approaches. The quanti-\ntative results are presented in Table 1. TransBTS achieves the Dice scores of\n78.93%, 90.00%, 81.94% on ET, WT, TC, respectively, which are comparable\nor higher results than previous SOTA 3D methods presented in Table 1. In\nterms of Hausdorﬀ distance metric, a considerable improvement has also been\nachieved for segmentation. Compared with 3D U-Net[6], TransBTS shows great\nsuperiority in both metrics with signiﬁcant improvements. This clearly reveals\nthe beneﬁt of leveraging Transformer for modeling the global relationships. For\nqualitative analysis, we also show a visual comparison of the brain tumor\nsegmentation results of various methods including 3D U-Net[6] , V-Net[12], At-\ntention U-Net[14] and our TransBTS in Fig. 2. Since the ground truth for the\nvalidation set is not available, we conduct ﬁve-fold cross-validation evaluation\non the training set for all methods. It is evident from Fig. 2 that TransBTS can\ndescribe brain tumors more accurately and generate much better segmentation\nmasks by modeling long-range dependencies between each volume.\nBraTS 2020. We also evaluate TransBTS on BraTS 2020 validation set and\nthe results are reported in Table 2. We directly adopt the hyperparameters on\nBraTS19 for model training, our TransBTS achieves Dice scores of 78 .73%,\n90.09%, 81 .73% and HD of 17.947mm, 4.964mm, 9.769mm on ET, WT, TC.\nCompared with 3D U-Net[6], V-Net[12] and Residual 3D U-Net, our TransBTS\nshows great superiority in both metrics with signiﬁcant improvements. This\nclearly reveals the beneﬁt of leveraging Transformer for modeling the global\nrelationships.\n8 Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, Jiangyun Li\n3.2 Model Complexity\nTransBTS has 32.99M parameters and 333G FLOPs which is a moderate size\nmodel. Besides, by reducing the number of stacked Transformer layers from 4 to\n1 and halving the hidden dimension of the FFN, we reach a lightweight Trans-\nBTS which only has 15.14M parameters and 208G FLOPs while achieving Dice\nscores of 78 .94%, 90.36%, 81.76% and HD of 4.552mm, 6.004mm, 6.173mm on\nET, WT, TC on BraTS2019 validation set. In other words, by reducing the\nlayers in Transformer as a simple and straightforward way to reduce complex-\nity (54 .11% reduction in parameters and 37 .54% reduction in FLOPs of our\nlightweight TransBTS), the performance only drops marginally. Compared with\n3D U-Net[6] which has 16.21M parameters and 1670G FLOPs, our lightweight\nTransBTS shows great superiority in terms of model complexity. Note that eﬃ-\ncient Transformer variants can be used in our framework to replace the vanilla\nTransformer to further reduce the memory and computation complexity while\nmaintaining the accuracy. But this is beyond the scope of this work.\n3.3 Ablation Study\nWe conduct extensive ablation experiments to verify the eﬀectiveness of Trans-\nBTS and justify the rationale of its design choices based on ﬁve-fold cross-\nvalidation evaluations on the BraTS 2019 training set. (1) We investigate the\nimpact of the sequence length (N) of tokens for Transformer, which is controlled\nby the overall stride (OS) of 3D CNN in the network encoder. (2) We explore\nTransformer at various model scales (i.e. depth ( L) and embedding dimension\n(d)). (3) We also analyze the impact of diﬀerent positions of skip-connections.\nSequence length N. Table 3 presents the ablation study of various sequence\nlengths for Transformer. The ﬁrst row (OS=16) and the second row (OS=8) both\nreshape each volume of the feature map to a feature vector after downsampling.\nIt is noticeable that increasing the length of tokens, by adjusting the OS from 16\nto 8, leads to a signiﬁcant improvement on performance. Speciﬁcally, 1.66% and\n2.41% have been attained for the Dice score of ET and WT respectively. Due to\nthe memory constraint, after setting the OS to 4, we can not directly reshape\neach volume to a feature vector. So we make a slight modiﬁcation to keep the\nsequence length to 4096, which is unfolding each 2 ×2 ×2 patch into a feature\nvector before passing to the Transformer. We ﬁnd that although the OS drops\nfrom 8 to 4, without the essential increase of sequence length, the performance\ndoes not improve or even gets worse.\nTransformer Scale.Two hyper-parameters, the feature embedding dimension\n(d) and the number of Transformer layers (depthL), mainly determines the scale\nof Transformer. We conduct ablation study to verify the impact of Transformer\nscale on the segmentation performance. For eﬃciency, we only train each model\nconﬁguration for 1000 epochs. As shown in Table 4, the network with d = 512\nand L = 4 achieves the best scores of ET and WT. Increasing the embedding\ndimension (d) may not necessarily lead to improved performance ( L = 4, d: 512\nTransBTS: Multimodal Brain Tumor Segmentation Using Transformer 9\nTable 3. Ablation study on se-\nquence length (N).\nOS Sequence\nlength(N)\nDice score(%)\nET WT TC\n16 512 73.30 87.59 81.36\n8 4096 74.96 90.00 79.96\n4 4096 74.86 87.10 77.46\nTable 4.Ablation study on Transformer.\nDepth (L) Embedding dim(d) Dice score(%)\nET WT TC\n4 384 68.95 83.31 66.89\n4 512 73.72 88.02 73.14\n4 768 69.38 83.54 74.16\n1 512 70.11 85.84 70.95\n8 512 66.48 79.16 67.22\nTable 5.Ablation study on the positions of skip-connections (SC).\nNumber of SC Position of SC Dice score(%)\nET WT TC\n3 Transformer layer 74.96 90.00 79.96\n3 3D Conv (Fig. 1) 78.92 90.23 81.19\nvs. 768) yet brings extra computational cost. We also observe that L = 4 is a\n“sweet spot” for the Transformer in terms of performance and complexity.\nPositions of Skip-connections (SC).To improve the representation ability\nof the model, we further investigate the positions for skip-connections (orange\ndash lines “ ” in Fig. 1). The ablation results are listed in Table 5. If skip-\nconnections are attached to the ﬁrst three Transformer layers, it is more alike\nto feature aggregation from adjacent layers without the compensation for loss of\nspatial details. Following the traditional design of skip-connections from U-Net\n(i.e. attach to the 3D Conv layers as shown in Fig. 1), considerable gains (3.96%\nand 1 .23%) have been achieved for the important ET and TC, thanks to the\nrecovery of low-level spatial detail information.\n4 Conclusion\nWe present a novel segmentation framework that eﬀectively incorporates Trans-\nformer in 3D CNN for multimodal brain tumor segmentation in MRI. The re-\nsulting architecture, TransBTS, not only inherits the advantage of 3D CNN for\nmodeling local context information, but also leverages Transformer on learning\nglobal semantic correlations. Experimental results on two datasets (BraTS 2019\nand 2020) validate the eﬀectiveness of the proposed TransBTS. In future work,\nwe will explore computational and memory eﬃcient attention mechanisms in\nTransformer to develop eﬃciency-focused models for volumetric segmentation.\nReferences\n1. Bahdanau, D., Cho, K., Bengio, Y.: Neural machine translation by jointly learning\nto align and translate. arXiv preprint arXiv:1409.0473 (2014)\n2. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J.S., Freymann,\nJ.B., Farahani, K., Davatzikos, C.: Advancing the cancer genome atlas glioma mri\ncollections with expert segmentation labels and radiomic features. Scientiﬁc data\n4, 170117 (2017)\n10 Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, Jiangyun Li\n3. Bakas, S., Reyes, M., Jakab, A., Bauer, S., Rempﬂer, M., Crimi, A., Shinohara,\nR.T., Berger, C., Ha, S.M., Rozycki, M., et al.: Identifying the best machine learn-\ning algorithms for brain tumor segmentation, progression assessment, and overall\nsurvival prediction in the brats challenge. arXiv preprint arXiv:1811.02629 (2018)\n4. Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko, S.: End-\nto-end object detection with transformers. In: European Conference on Computer\nVision. pp. 213–229. Springer (2020)\n5. Chen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., Lu, L., Yuille, A.L., Zhou,\nY.: Transunet: Transformers make strong encoders for medical image segmentation.\narXiv preprint arXiv:2102.04306 (2021)\n6. C ¸ i¸ cek,¨O., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.: 3d u-net:\nlearning dense volumetric segmentation from sparse annotation. In: International\nconference on medical image computing and computer-assisted intervention. pp.\n424–432. Springer (2016)\n7. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner,\nT., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is\nworth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929 (2020)\n8. Frey, M., Nau, M.: Memory eﬃcient brain tumor segmentation using an\nautoencoder-regularized u-net. In: International MICCAI Brainlesion Workshop.\npp. 388–396. Springer (2019)\n9. Li, X., Luo, G., Wang, K.: Multi-step cascaded networks for brain tumor segmen-\ntation. In: International MICCAI Brainlesion Workshop. pp. 163–173. Springer\n(2019)\n10. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic\nsegmentation. In: Proceedings of the IEEE conference on computer vision and\npattern recognition. pp. 3431–3440 (2015)\n11. Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J.,\nBurren, Y., Porz, N., Slotboom, J., Wiest, R., et al.: The multimodal brain tumor\nimage segmentation benchmark (brats). IEEE transactions on medical imaging\n34(10), 1993–2024 (2014)\n12. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks\nfor volumetric medical image segmentation. In: 2016 fourth international confer-\nence on 3D vision (3DV). pp. 565–571. IEEE (2016)\n13. Myronenko, A., Hatamizadeh, A.: Robust semantic segmentation of brain tumor\nregions from 3d mris. In: International MICCAI Brainlesion Workshop. pp. 82–89.\nSpringer (2019)\n14. Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori,\nK., McDonagh, S., Hammerla, N.Y., Kainz, B., et al.: Attention u-net: Learning\nwhere to look for the pancreas. arXiv preprint arXiv:1804.03999 (2018)\n15. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomedi-\ncal image segmentation. In: International Conference on Medical image computing\nand computer-assisted intervention. pp. 234–241. Springer (2015)\n16. Schlemper, J., Oktay, O., Schaap, M., Heinrich, M., Kainz, B., Glocker, B., Rueck-\nert, D.: Attention gated networks: Learning to leverage salient regions in medical\nimages. Medical image analysis 53, 197–207 (2019)\n17. Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., J´ egou, H.: Training\ndata-eﬃcient image transformers & distillation through attention. arXiv preprint\narXiv:2012.12877 (2020)\nTransBTS: Multimodal Brain Tumor Segmentation Using Transformer 11\n18. Valanarasu, J.M.J., Sindagi, V.A., Hacihaliloglu, I., Patel, V.M.: Kiu-net: Over-\ncomplete convolutional architectures for biomedical image and volumetric segmen-\ntation. arXiv preprint arXiv:2010.01663 (2020)\n19. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n L., Polosukhin, I.: Attention is all you need. In: Advances in neural information\nprocessing systems. pp. 5998–6008 (2017)\n20. Wang, F., Jiang, R., Zheng, L., Meng, C., Biswal, B.: 3d u-net based brain tumor\nsegmentation and survival days prediction. In: International MICCAI Brainlesion\nWorkshop. pp. 131–141. Springer (2019)\n21. Wang, X., Girshick, R., Gupta, A., He, K.: Non-local neural networks. In: Pro-\nceedings of the IEEE conference on computer vision and pattern recognition. pp.\n7794–7803 (2018)\n22. Yuan, L., Chen, Y., Wang, T., Yu, W., Shi, Y., Tay, F.E., Feng, J., Yan, S.: Tokens-\nto-token vit: Training vision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986 (2021)\n23. Zhang, Z., Liu, Q., Wang, Y.: Road extraction by deep residual u-net. IEEE Geo-\nscience and Remote Sensing Letters 15(5), 749–753 (2018)\n24. Zhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J.: Unet++: A nested u-net\narchitecture for medical image segmentation. In: Deep learning in medical image\nanalysis and multimodal learning for clinical decision support, pp. 3–11. Springer\n(2018)\n12 Wenxuan Wang, Chen Chen, Meng Ding, Hong Yu, Sen Zha, Jiangyun Li\nAppendix\nIn this Appendix, we provide the network details of TransBTS in Table 6.\nTable 6.The design details of our propsoed TransBTS network. Conv3 denotes a 3×3×\n3 convolutional layer; BN denotes Batch Normalization; DeConv denotes deconvolution\nlayer. Each block of encoder and decoder is a residual block. Note that the size of input\nimage is 4 ×128 ×128 ×128.\nstage block name details output size\n3D CNN Encoder\nInitConv Conv3, Dropout 16 ×128 ×128 ×128\nEnBlock1\n[ BN, ReLU, Conv3\nBN, ReLU, Conv3\n]\n×1 16 ×128 ×128 ×128\nDownSample1 Conv3(stride2) 32 ×64 ×64 ×64\nEnBlock2\n[ BN, ReLU, Conv3\nBN, ReLU, Conv3\n]\n×2 32 ×64 ×64 ×64\nDownSample2 Conv3(stride2) 64 ×32 ×32 ×32\nEnBlock3\n[ BN, ReLU, Conv3\nBN, ReLU, Conv3\n]\n×2 64 ×32 ×32 ×32\nDownSample3 Conv3(stride2) 128 ×16 ×16 ×16\nEnBlock4\n[ BN, ReLU, Conv3\nBN, ReLU, Conv3\n]\n×2 128 ×16 ×16 ×16\nTransformer Encoder Linear Projection Conv3, Reshape 512 ×4096 (d ×N)\nTransformer Transformer Layer ×4 512 ×4096 (d ×N)\n3D CNN Decoder\nFeature Mapping Reshape, Conv3 128 ×16 ×16 ×16\nDeBlock1\n[ BN, ReLU, Conv3\nBN, ReLU, Conv3\n]\n×2 128 ×16 ×16 ×16\nUpSample1 Conv3, DeConv, Conv3 64 ×32 ×32 ×32\nDeBlock2\n[ BN, ReLU, Conv3\nBN, ReLU, Conv3\n]\n×1 64 ×32 ×32 ×32\nUpSample2 Conv3, DeConv, Conv3 32 ×64 ×64 ×64\nDeBlock3\n[ BN, ReLU, Conv3\nBN, ReLU, Conv3\n]\n×1 32 ×64 ×64 ×64\nUpSample3 Conv3, DeConv, Conv3 16 ×128 ×128 ×128\nDeBlock4\n[ BN, ReLU, Conv3\nBN, ReLU, Conv3\n]\n×1 16 ×128 ×128 ×128\nEndConv Conv1, Softmax 4 ×128 ×128 ×128"
}