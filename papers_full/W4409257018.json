{
  "title": "MMAgentRec, a personalized multi-modal recommendation agent with large language model",
  "url": "https://openalex.org/W4409257018",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A3175230125",
      "name": "Xiaochen Xiao",
      "affiliations": [
        "Institute of Electrical and Electronics Engineers"
      ]
    },
    {
      "id": "https://openalex.org/A3175230125",
      "name": "Xiaochen Xiao",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3118466402",
    "https://openalex.org/W4293792606",
    "https://openalex.org/W2970036099",
    "https://openalex.org/W4281647015",
    "https://openalex.org/W4200226791",
    "https://openalex.org/W3175462020",
    "https://openalex.org/W4382600039",
    "https://openalex.org/W4385066358",
    "https://openalex.org/W4386318603",
    "https://openalex.org/W4385682046",
    "https://openalex.org/W4205091644",
    "https://openalex.org/W4285288414",
    "https://openalex.org/W4284882462",
    "https://openalex.org/W2969960436",
    "https://openalex.org/W3045903937",
    "https://openalex.org/W2949655105",
    "https://openalex.org/W2081061686",
    "https://openalex.org/W2609979540",
    "https://openalex.org/W4213448193",
    "https://openalex.org/W2786995169",
    "https://openalex.org/W3034838175",
    "https://openalex.org/W2788376297",
    "https://openalex.org/W4321780122",
    "https://openalex.org/W4383174046",
    "https://openalex.org/W4383472964",
    "https://openalex.org/W4383175795",
    "https://openalex.org/W4385567603",
    "https://openalex.org/W6600001191",
    "https://openalex.org/W4375959083",
    "https://openalex.org/W4392846385",
    "https://openalex.org/W4389524458",
    "https://openalex.org/W6602989878",
    "https://openalex.org/W6600103761",
    "https://openalex.org/W4376312061",
    "https://openalex.org/W4296591820",
    "https://openalex.org/W4405643374",
    "https://openalex.org/W4386730022",
    "https://openalex.org/W6609090796",
    "https://openalex.org/W3031273498",
    "https://openalex.org/W3004578093",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4307073350",
    "https://openalex.org/W3100921056",
    "https://openalex.org/W3099790621",
    "https://openalex.org/W2913699508",
    "https://openalex.org/W4226054951"
  ],
  "abstract": null,
  "full_text": "MMAgentRec, a personalized \nmulti-modal recommendation \nagent with large language model\nXiaochen Xiao\nIn multimodal recommendation, various data types, including text, images, and user dialogues, \nare utilized. However, it faces two primary challenges. Firstly, identifying user requirements is \nchallenging due to their inherent complexity and diverse intentions. Secondly, the scarcity of high \nquality datasets and the unnaturalness of recommendation systems pose pressing issues. Especially \ninteractive datasets,and datasets that can evaluate large models and human temporal interactions.\nIn multimodal recommendation, users often face problems such as fragmented information and \nunclear needs. At the same time, data scarcity affects the accuracy and comprehensiveness of model \nevaluation and recommendation. This is a pain point in multimodal recommendation. Addressing \nthese issues presents a significant opportunity for advancement. Combining multimodal backgrounds \nwith large language models offers prospects for alleviating pain points. This integration enables \nsystems to support a broader array of inputs, facilitating seamless dialogues and coherent responses. \nThis article employs multimodal techniques, introducing cross-attention mechanisms, self-reflection \nmechanisms, along with multi-graph neural networks and residual networks. Multimodal techniques \nare responsible for handling data input problems. Cross-attention mechanisms are used to handle \nthe combination of images and texts. Multi-graph neural networks and residual networks are used to \nbuild a recommendation system framework to improve the accuracy of recommendations. These are \ncombined with an adapted large language model (LLM) using the reflection methodology,LLM takes \nadvantage of its ease of communication with humans, proposing an autonomous decision-making \nand intelligent recommendation-capable multimodal system with self-reflective capabilities. The \nsystem includes a recommendation module that seeks advice from different domain experts based on \nuser requirements. Through experimentation, our multimodal system has made significant strides in \nunderstanding user intent based on input keywords, demonstrating superiority over classic multimodal \nrecommendation algorithms such as Blip2, clip. This indicates that our system can intelligently \ngenerate suggestions, meeting user requirements and enhancing user experience. Our approach \nprovides novel perspectives for the development of multimodal recommendation systems, holding \nsubstantial practical application potential and promising to propel their evolution in the information \ntechnology domain. This indicates that our system can intelligently generate suggestions, meeting \nuser requirements and enhancing user experience. Our approach provides novel perspectives for \nthe development of multimodal recommendation systems, holding substantial practical application \npotential and promising to propel their evolution in the information technology domain. We conducted \nextensive evaluations to assess the effectiveness of our proposed model, including an ablation study, \ncomparison with state-of-the-art methods, and performance analysis on multiple datasets. Ablation \nStudy results demonstrate that the full model achieves the highest performance across all metrics, \nwith an accuracy of 0.9526, precision of 0.94, recall of 0.95, and an F1 score of 0.94. Removing key \ncomponents leads to performance degradation, with the exclusion of the LLM component having the \nmost significant impact, reducing the F1 score to 0.91. The absence of MGCN and Cross-Attention also \nresults in lower accuracy, confirming their critical role in enhancing model effectiveness. Comparison \nwith state-of-the-art methods indicates that our model outperforms LightGCN and DualGNN in all \nkey metrics. Specifically, LightGCN achieves an accuracy of 0.9210, while DualGNN reaches 0.9285, \nboth falling short of the proposed model’s performance. These results validate the superiority of our \napproach in handling complex multimodal tasks. Experimental results on multiple datasets further \nhighlight the effectiveness of MGCN and Cross-Attention. On the QK-Video and QB-Video datasets, \nMGCN achieves the highest recall scores, with Recall@5 reaching 0.6556 and 0.6856, and Recall@50 \nattaining 0.9559 and 0.9059, respectively. Cross-Attention exhibits strong early recall capabilities, \nOPEN\nScientific Reports |        (2025) 15:12062 1| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports\n\nachieving Recall@10 of 0.8522 on the Tourism dataset. In contrast, Clip and Blip2 show moderate \nrecall performance, with Clip achieving only 0.3423 for Recall@5 and Blip2 reaching 0.4531 on the \nTourism dataset. Overall, our model consistently surpasses existing approaches, with MGCN and \nCross-Attention demonstrating superior retrieval and classification performance across various tasks, \nunderscoring their effectiveness in visual question answering (VQA). At the same time, this paper has \nconstructed a comprehensive dataset in this field, each column contains 9004 data entries.\nKeywords Multi-graph convolutional network, Resnet, Cross attention, Multimodal recommendation, \nIntelligent recommendation generation, Reinforcement learning, Multi-agent interaction, Large language \nmodal\nIEEE Publication Technology Group, Piscataway, NJ, USA. email: rx_7811@qq.com\nWith the continuous development of artificial intelligence technology, intelligent dialogue systems have found \nwidespread applications across various domains. The aim of this project is to construct a multifunctional model \nthat, through natural language processing and multimodal computation techniques, provides users with more \nintelligent and personalized travel consultation and dialogue experiences. This large model represents the \nforefront of information technology, integrating knowledge and technology from multiple domains to cater to \ndiverse user requirements1. Concurrently, in the field of tourism consultation, the lack of high-quality datasets2,3 \nand the issue of unnatural machine-generated chat present challenges. The MMAgentRec proposed in this paper \nseeks to address this complexity, offering a comprehensive solution and a high-quality scenic dataset to enhance \nuser experiences in travel planning and tourism consultation.\nResearch Objectives is that this large-scale model is multi functional. Firstly, it can identify user intentions \nto discern travel requirements. If the user’s request is for casual conversation or dialogue, the large model will \nengage the chat module, enabling natural conversation with the user. If the user requires travel advice, the \nlarge model will provide suggestions from experts in various fields, including natural sciences, social sciences, \nhumanities, engineering and technology, medical and health sciences, as well as business and management, \nbased on user demands and destination information. To construct this multifunctional large-scale model, we \nutilized a dataset containing information on attractions in major tourist cities in Guangdong Province. The \ndataset includes key information such as attraction names, ratings, and comment counts, with each column \ncomprising 9004 data entries. The attraction descriptions were transformed into word vectors using a BERT \npre-trained model, while attraction images were converted into vectors using a pre-trained ResNet50 model. \nThis dataset provides the necessary inputs and labels for the model’s training. In the application scenarios of \ntravel planning and consultation, various information sources and data types are involved, including text, \nimages, and user conversations4. In order to solve this problem, the system proposed in this paper introduces \nmultimodal technology to solve the data input problem. To achieve this objective, This paper introduces multi-\nview convolutional neural networks (MGCN) to build a recommendation model to handle multimodal data. \nSimultaneously, we employed residual networks (ResNet) to address the training challenges of deep networks. \nCollaborative attention mechanisms facilitate effective information sharing and collaboration among various \ncomponents of the system. In this paper,the main contribution is the following four aspects. New multimodal \nattraction rating framework. We propose a novel multimodal attraction rating framework that utilizes both \ntextual and visual information to assess the attractiveness and quality of tourist attractions more comprehensively \nand accurately. This framework introduces and multi-view convolutional neural networks (MGCN) to handle \nmultimodal data, allowing the model to synthesize information from different kinds of data. Then obtain the \ndata input score required by a recommendation system. Improvements of the large language model (LLM) \nprompt. This paper prompt the Large Language Model to serve multiple roles, including Actor, Evaluator, Self-\nreflection, and Memory. This prompt positions LLM as a core component of a multi-agent system, providing \nusers with more intelligent and personalized travel consultation and dialogue experiences while improving \ndecision making quality in that it can identify user intent clearly and accurately. Used to decide whether to \nask the user for consulting services or chat. Recommendations from Multidisciplinary Experts. Our research \nintroduces prompt large language models (llm) to represent experts in different domains, including natural \nsciences, social sciences, humanities, engineering and technology, medical and health sciences, and business and \nmanagement. The suggestions from these experts provide users with interdisciplinary travel advice, enhancing \nthe diversity and adaptability of recommendations. This module is used to provide users with a complete solution \nfrom different disciplines and perspectives. New dataset construction. To support our research, we constructed \na new dataset containing information on attractions in major tourist cities in Guangdong Province. The dataset \nincludes key information such as attraction names, ratings, comment counts, etc. Each column in the dataset \ncomprises 9004 data entries, and the attraction descriptions and images were transformed into vectors using \npre-trained BERT and ResNet50 models, respectively. This dataset provides the necessary inputs and labels for \nmodel training, enhancing the operational feasibility of our research. The purpose of constructing this dataset is \nto solve the problem of lack of recommendation datasets in the tourism field and to verify the recommendation \neffect of the system proposed in this paper.\nThe outcome of this study shows that MGCN out performs other methods in all recall metrics. On the \nQK-Video dataset, MGCN achieves Recall@5 of 0.6556, Recall@10 of 0.7677, and Recall@50 of 0.9559. Cross \nattention also performs well, with Recall@5 of 0.5520 and Recall@10 of 0.8522 on the Tourism dataset. Clip and \nBlip/Blip2 show moderate performance; for example, Clip has Recall@5 of 0.3423 and Recall@50 of 0.6682 on \nthe Tourism dataset. Overall, MGCN is the top performer.\nScientific Reports |        (2025) 15:12062 2| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nRelate work\nLarge language model\nIn the realm of multimodal recommendation systems, extensive research has been conducted across diverse \ndomainss1,2. However, the field of tourism remains notably underexplored, lacking dedicated multimodal \nrecommendation datasets. For instance, studies on hybrid tourism recommendation systems1 and personalized \ntourism recommendation systemss 2 have not addressed this gap. The absence of such datasets limits \nadvancements in multimodal tourism recommendation research. Future efforts should prioritize the creation \nand public availability of tailored multimodal datasets to propel research in this specific domain. Prominent \nresearch in multimodal tourism recommendation systems, focusing on attraction rating frameworks, includes \nthe introduction of Cornac by Aghiles Salah et al.'s. 3 “ Asking Images” , proposed by Guangli Li et al., addresses \ndata sparsity challenges through a hierarchical sampling approach, enhancing accuracy in combating information \noverloads4.\nCurrently, researchers have explored various technologies in this field, including sentiment analysis, \nrecommendation algorithms, knowledge graphs, and more5–18. However, none of them have incorporated large \nlanguage models, and they lack interactive capabilities with users.\nSome research used AHP and PROMETHEE for hotel website quality evaluation, showing positive results \nbut lacking a multi-modal perspective 19. Various approaches have been proposed in the field of tourism \nrecommendation. Some research presented an innovative context-aware tourism recommendation framework20, \nemphasizing the importance of considering contextual information. However, a common limitation in existing \nmodels is the lack of integration with large language models. For instance, Some research introduced POWERec \nand while other research proposed MGCN as tourism recommendation. Inspired by models 11,12, MGCN \nalgorithm is introduced into the multimodal recommendation framework of this paper.\nSimultaneously, researchers have introduced a variety of innovative frameworks to enhance performance \nand address specific challenges21. Their algorithms have demonstrated outstanding performance in benchmark \ntests, combining document and comment-level approaches effectively to overcome certain limitations22,23. The \nintroduction of “NARRE” aims for improved rating prediction24.\nThese efforts in related work contribute to the evolution of recommendation systems, showcasing progress \nand addressing specific challenges. However, a common theme across these studies is the need for further \nexploration and integration of multi-modal capabilities to enhance the overall effectiveness of recommendation \nmodels.\nJiang et al. proposed “DLSCF” to address data sparsity, capturing shared and domain-specific rating patterns. \nZhang et al. revisited DLSCF , emphasizing knowledge transfer and showcasing its effectiveness25,26.\nLarge language model agent\nThere has been considerable exploration into the personality aspects of large language models (LLMs). \nResearchers introduced GenRec, an LLM-based recommender, leveraging prompts for task understanding, \nand demonstrated improved results on large-scale datasets 27. The proposal of GIRL for job recommendations, \nutilizing supervised prompt and reinforcement learning, exemplifies the efforts to transform personalized job-\nseeking28. Studies on the impact of LLMs on personality traits in generated text showcase reliable simulations and \nplasticity29. They further advocate for the necessity of introducing a reflection mechanism 27–29. An anonymous \nauthor introduced the Machine Personality Inventory (MPI) dataset, using personality as a guiding principle \nfor downstream tasks and employing chain prompting for controlled induction30. The“PrefRec” system engages \nlong-term participation by leveraging user preferences, benefiting from LLM principles in handling natural \nlanguage feedback31. Inspired by these works, I introduced multi-domain expert agent technology in the chat \nmodule, employing agents with different personalities from various professional domains to provide users with \nrecommendations, aiming to facilitate optimal decision-making30,31.\nIn recent research, the academic community has delved into the application of Large Language Models (LLMs) \nin recommendation systems. Some scholars have focused on zero-shot next-item recommendation, employing \nthe “NIR prompting”method and showcasing remarkable performance on the MovieLens 100K dataset32. On the \nother hand, researchers have successfully balanced the performance of recommendation models by combining \nLLMs with Information Retrieval (IR) ranking and leveraging domain-specific list prompts 33. Simultaneously, \nscholars have delved into the advancements of LLMs, particularly exploring Adaptive Generation Rules (AGR) \nand assessing the performance of ChatGPT under AGR principles 34. Additionally, researchers have proposed \na privacy-preserving recommendation system utilizing differentially private LLMs, demonstrating superior \nperformance in the context of deep retrieval models compared to traditional methods35.\nHowever, these studies share a common challenge—in specific domains such as tourism, the accuracy of \nrecommendations remains relatively low. This underscores a collective challenge in improving recommendation \nsystems“accuracy in niche domains. In this context, this paper introduces an innovative recommendation \nsystem integrating multi-modal capabilities, aiming to address this common challenge and enhance accuracy. \nWhile a CRS roadmap utilizing LLMs has addressed certain challenges, it falls short of exploring specific \nscenarios36. The formalization of LLM recommendations has showcased potential, yet difficulties persist in \nterms of sequence order and bias37. The introduction of iEvaLM for CRSs, emphasizing the performance boost \nfrom ChatGPT, adds to the spectrum of innovative approaches 38. An end-to-end framework for aspect-based \nrecommendations, incorporating aspect extraction, has demonstrated effectiveness39. Evaluation of ChatGPT’s \nnews recommendations, focusing on personalization, fairness, and fake news detection, has been undertaken40. \nInspired by these endeavors38–41, I have incorporated multi-agent collaboration technology into the model.\nRecent research has delved into the impact of prompt Large Language Models (LLMs) on recommendation \ntasks. Some studies have demonstrated prompt strategies for LLMs that exhibit comparable performance even \nwith minimal training data42. Additionally, there are discussions in the literature regarding strategies that leverage \nScientific Reports |        (2025) 15:12062 3| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nboth open-source and proprietary LLMs to enhance content-based recommendation systems, showcasing \nsignificant performance improvements 43. Despite these efforts, a persistent challenge in recommendation \nsystems is the considerable divergence between generated recommendations and human preferences. To \naddress this challenge, we propose a “Human-Machine Interaction” (HMI) paradigm 44, integrating human \nfeedback into the recommendation system. The goal of this approach is to tackle the inconsistency between \ngenerated recommendations and user preferences, emphasizing the crucial role of user feedback in refining the \nrecommendation process. Inspired by these advancements, our work introduces a reflection mechanism inspired \nby fine-tuned LLMs and leverages both open-source and proprietary LLMs. This innovative approach aims to \nenhance the personalization of recommendation systems and further improve the overall user experience.\nIn the realm of recommendation systems, researchers have extensively explored the application of Large \nLanguage Models (LLMs). Some studies have made significant strides by introducing a novel user preference \nformat, achieving notable success on GPT-3 45. To address fairness issues in LLM-based recommendations, \nresearchers proposed “FaiRLLM” and shed light on the associated challenges on ChatGPT 46. Additionally, an \napproach named PARL has been introduced for personalized recommendations, demonstrating exceptional \nperformance in sequential tasks. However, a common limitation across these methods is the lack of integration \nwith multimodal techniques47.\nDataset introduction\nThe data for this study was meticulously obtained through web scraping from Qunar, capturing comprehensive \ndetails of major tourist cities in Guangdong Province, including Foshan, Shantou, Shenzhen, Zhuhai, Zhanjiang, \nDongguan, Huizhou, Guangzhou, Qingyuan, Y angjiang, and Shaoguan. The consolidated data, stored in CSV \nformat files, encompasses a diverse range of columns, each representing specific attributes such as Name, English \nName, ID, POI ID, Longitude, Latitude, Tags, Features, Price, Minimum Price, Rating, Review Count, Cover \nImage, Adult Ticket Price, Senior Ticket Price, Student Ticket Price, Child Ticket Price, Recommended Playtime, \nOpening Hours, Introduction, and Discount Policy. Notably, each of these columns contains a non-empty record \ncount, totaling 9004 data entries, thereby providing a rich and comprehensive dataset with varying entry counts \nfor specific attributes. For more information about this data, please refer to Table  1. Refer to Table  2 for how \nmuch data each column in this dataset contains.\nModel structure\nFigures  1,  6, and  7 were created using the built-in graphic components of WPS Office. These figures were \nindependently designed within the software and are not derived from any external dataset or database. Therefore, \nno specific dataset or database link is associated with them.\nThe multimodal recommendation model consists of two key components. A text module and an image \nmodule. These modules work collaboratively to assess user input keywords, determine user intentions, and \nprovide personalized travel recommendations. Below, we provide a detailed technical description of each \ncomponent, including pseudocode, mathematical formulations, and architectural diagrams.\nText module\nThe text module processes user-input textual information to identify their needs. It consists of two main \ncomponents. The Natural Language Processing (NLP) Module and the Intent Recognition Module. The NLP \nModule employs a pre-trained BERT model to convert user-input text into word vector representations, \ncapturing semantic information to facilitate intent understanding. The Intent Recognition Module utilizes a \nlarge language model (LLM) to classify user intent based on the BERT embeddings, enabling the system to \nprovide personalized recommendations.\nData type Number of non-null records\nName 9004\nEnglish Name 6278\nid 9004\npoiID 9004\nSpecial Policy 9004\nTable 2. Number of non-null records for various data types.\n \nType Mean Std dev Min 25% 50% 75%\nid 4.044656 × 107 5.344155 × 107 2776 1.714919 × 106 4.640143 × 106 8.381238 × 107\npoiID 6.316131 × 107 4.576692 × 107 75,867 2.286746 × 107 5.203278 × 107 1.034565 × 108\nLongitude 22.898298 0.999441 18.577111 22.445939 22.755552 23.466038\nLatitude 114.023458 1.803496 108.389501 113.508336 114.077610 114.751411\nTable 1. Statistics of data.\n \nScientific Reports |        (2025) 15:12062 4| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nNatural language processing (NLP) module\nThis module utilizes a pre-trained Bidirectional Encoder Representations from Transformers (BERT) model to \nconvert user-input text into word vector representations. The input text is tokenized into subword units using the \nWordPiece tokenizer, and the resulting tokens are fed into the BERT model. The output is a sequence of dense \nvectors, each representing the contextualized embedding of a token.\n (a) Mathematical formulation: Given an input sequence of tokens x =[ x1,x 2,...,x n], the BERT model \noutputs a sequence of embeddings E(x)=[ e1, e2,..., en], where each ei is a 768-dimensional vector.\nIntent recognition\nIntent recognition is achieved through a Large Language Model (LLM) that relies on the deductive capabilities \nof the language model to determine user intent. The LLM processes the BERT embeddings and outputs a \nprobability distribution over possible intents.\n (a) Mathematical formulation: The intent recognition can be formulated as:\n P(intent|E(x)) =softmax(W · E(x)+ b)\nwhere W and b are learnable parameters.\nPurpose recognizer\nFollowing intent recognition, the model employs a purpose recognizer to classify the user’s purpose. This classifier \nis composed of multiple domain-specific expert models, each focusing on recognizing a specific domain.\n(a) Pseudocode:\nFig. 1. Overall structure of the model.\n \nScientific Reports |        (2025) 15:12062 5| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nAlgorithm 1. Purpose Recognizer\nImage module\nThe image module processes information from scenic images and collaborates with the text module to provide \nmore comprehensive travel recommendations. It includes the following components.\nImage feature extractor\nThe image feature extractor uses a pre-trained ResNet50 model to convert scenic images into high-dimensional \nfeature vectors. ResNet50 is chosen for its deep architecture and residual connections, which allow it to capture \nfine-grained visual features while mitigating the vanishing gradient problem.\n(a) Mathematical Formulation: Given an input image I, the ResNet50 model outputs a 2048-dimensional \nfeature vector v = ResNet50(I).\nCross-modal attention mechanism\nTo fuse textual and image information, we introduce a cross-modal attention mechanism. This mechanism \nallows the model to interact between different modalities (text and image) for a better understanding of the \nrelationship between scenic spots and user requirements.\n(a) Mathematical formulation: Given the text embeddings Q and image features K, the attention weights αij  \nare computed as follows:\n \nαij = exp(Qi · Kj )∑\nj exp(Qi · Kj )  (1)\nwhere Qi represents the query vector for the i-th text token, and Kj  represents the key vector for the j-th image \nfeature. The attention weights αij  indicate the importance of the j-th image feature to the i-th text token. The \nattended image features V  are then computed as:\n \nVi =\n∑\nj\nαij Kj  (2)\nThis mechanism allows the model to focus on the most relevant image features for each text token, enhancing \nthe fusion of multimodal information.\nFormula connection\nMulti-graph convolutional networks (MGCN)\nThe multi-graph convolutional network (MGCN) is a key component of our model, designed to handle \nmultimodal data by constructing and fusing multiple graphs representing different views of the data (e.g., text \nand image views).\nGraph construction\nFor the text view, we construct a graph where nodes represent words or phrases, and edges represent semantic \nrelationships between them. The adjacency matrix Atext is computed using cosine similarity between word \nembeddings.\nFor the image view, we construct a graph where nodes represent regions of the image, and edges represent \nspatial or feature-based relationships. The adjacency matrix Aimage is computed using similarity between image \nfeature vectors.\nGraph fusion\nThe adjacency matrices from different views are fused using a weighted summation.\nScientific Reports |        (2025) 15:12062 6| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\n \nA =\n∑\ni\nwiAi\nwhere wi are learnable weights that determine the contribution of each view to the final graph.\nGraph convolution\nThe graph convolution operation is performed using the following formula.\n H(l+1) = σ( ˜D−1/2 ˜A ˜D−1/2H(l)W(l))\nwhere ˜A = A + I is the adjacency matrix with self-loops, ˜D is the degree matrix of ˜A, H(l) is the node feature \nmatrix at layer , and W(l) is the learnable weight matrix at layer .\nResidual networks (ResNet)\nThe Residual Network (ResNet) is used to address the vanishing gradient problem in deep networks by \nintroducing skip connections that allow the network to learn residual mappings.\nResidual block\nThe basic building block of ResNet is the residual block, which can be expressed as\n y = F(x, {Wi})+ x\nwhere x is the input feature, F(x, {Wi}) is the residual mapping learned by the network, and y is the output \nfeature.\nIntegration with MGCN\nThe output of the MGCN H is passed through a series of residual blocks to further refine the feature \nrepresentations. The final output is a high-level feature representation that captures both textual and visual \ninformation.\nSelf-reflective large language model (LLM)\nThe Self-Reflective Large Language Model (LLM) is a core component of our system, enabling the model to \nreflect on its own decisions and improve its performance over time.\nReflection mechanism\nThe reflection mechanism allows the LLM to evaluate its own responses and adjust its behavior based on user \nfeedback. This is achieved through a reinforcement learning framework where the LLM receives rewards based \non the quality of its responses.\nMathematical Formulation The reflection mechanism can be formulated as:\n Reﬂection(s, a)= LLM(s, a)+ λ · Feedback(s, a)\nwhere s is the state (user query), a is the action (LLM response), and λ is a weighting factor for the feedback.\nDecision-making\nDuring decision-making, the LLM queries its memory to retrieve relevant past interactions and uses this \ninformation to generate more accurate and context-aware responses. The LLM also employs a self-attention \nmechanism to weigh the importance of different parts of the input when generating responses.\nFigure 1 is overall structure of the model. Figure 2 is multimodal framework. Figure 3 is comparison of model \nversions.\nEmbedding\nThe Embedding layer discrete input data into dense vector representations48. Its purpose is to convert sequences \nof words in the text into fixed-length dense vector representations. These vectors capture semantic relationships \nbetween words, helping neural networks better understand and process textual data. Using pre-trained word \nembeddings GloVe, the embedding layer maps each word to a low-dimensional continuous vector space, thereby \nenhancing the model’s performance and generalization ability in tasks like text classification.\nGiven a vocabulary size V  and an embedding dimension d, the layer consists of an embedding matrix W of \nshape V × d. For an input sequence x =[ x1,x 2,...,x n] where each xi is an integer index within the range \n[0,V − 1], the output E(x) of the Embedding layer is computed as\n E(x)=[ W [x1],W [x2],...,W [xn]]\nHere, W[xi] denotes the xi-th row of the embedding matrix W, representing the embedding vector for the xi\n-th word in the input sequence.\nScientific Reports |        (2025) 15:12062 7| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nMulti-graph convolutional networks\nIn the field of deep learning, Multi-Graph Convolutional Networks (MGCN) are models specifically designed \nfor handling multiple graph data12. The following provides a brief introduction to the fundamental formulas and \nsymbols of Multi-Graph Neural Networks.\nFirst, adjacency matrices are constructed separately for each view. For the text view, cosine similarity between \ntext data is used to build the adjacency matrix. For the image view, similarity between image feature vectors or \nrelationships between regions are used to construct the adjacency matrix. These adjacency matrices are then \nintegrated using weighted summation or stacking to form a comprehensive adjacency matrix A. This integrated \nadjacency matrix A serves as the input to the multi-view graph convolutional network (MGCN) model for \nsubsequent graph convolution operations and deep learning tasks.\nThere are n graphs. Each graph can be represented by an adjacency matrix A(i), where i denotes the i-th \ngraph. The elements A(i)\njk  of A(i) represent the connection weights from node j to node k in graph i.\nFurthermore, let the node feature matrix of each graph be X(i). In this matrix, X(i)\nij  represents the feature \nof node j in graph i.\nThe basic formulas for Multi-Graph Convolutional Networks are outlined below.\nIn the context of graph analysis, the process of node aggregation is succinctly captured by the following \nformula is\n \nh(i)\nj = σ\n( n∑\nk=1\nA(i)\njk X(i)\nk W(i)\n)\n (3)\nHere, h(i)\nj  denotes the aggregated representation of node j in graph i. This is obtained through a weighted \nsummation of the features X(i)\nk  of its neighboring nodes, where the weights are determined by A(i)\njk  and the \nweight matrix W(i). The activation function σ further processes this sum.\nFor the entire graph, the aggregation process is encapsulated by the following equation:\n \nH(i) = σ\n( N∑\nj=1\nh(i)\nj U(i)\n)\n (4)\nHere, H(i) signifies the aggregated representation of graph i. This is achieved through a summation of the \naggregated node representations h(i)\nj , weighted by the matrix U(i). The weight matrix U(i) plays a crucial role in \ndetermining the contribution of each node to the overall graph representation. The activation function σ further \nrefines the resulting sum.\nIn the realm of Multi-Graph Fusion, the fusion operation is elegantly formulated as follows\nFig. 2. Multimodal framework.\n \nScientific Reports |        (2025) 15:12062 8| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\n \nH = σ\n( n∑\ni=1\nH(i)V\n)\n (5)\nHere, H embodies the amalgamated representation resulting from the aggregation of multiple graphs, with V  \nassuming a pivotal role as the weight matrix crucial for multi-graph fusion.\nFig. 3. Comparison of model versions.\n \nScientific Reports |        (2025) 15:12062 9| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nFor a comprehensive understanding of the symbols involved, let’s delve into their interpretations. The \nprovided formulas define various components in the context of graph representation is\nA(i) denotes the adjacency matrix of the i-th graph. X(i) represents the node features of the i-th graph. h(i)\nj  \nsignifies the aggregated representation of node j in the i-th graph. H(i) represents the aggregated representation \nof the i-th graph. H denotes the fused representation after aggregating multiple graphs. σ refers to the activation \nfunction. The weight matrices are represented by W(i), U(i), and V .\nTo connect the Embedding layer with Multi-Graph Convolutional Networks (MGCN), the output of the \nEmbedding layer needs to be used as the input node feature matrix X(i) for the MGCN. The specific steps are \nas follows\nFirst,for an input sequence x =[ x1,x 2,...,x n], obtain the embedding vector sequence E(x) through the \nembedding matrix W.\nSecond,use the embedding vector sequence E(x) as the node feature matrix X(i) for each graph in the \nMGCN.\nThen utilize the obtained node feature matrix X(i) in the node and graph aggregation formulas of the MGCN.\nSpecifically, given an input sequence x, we first obtain the node feature matrix X(i) = E(x), and then apply \nthe node aggregation formula in the MGCN is\n \n[h(i)\nj = σ\n( n∑\nk=1\nA(i)\njk E(x)[k]W(i)\n)\n] (6)\nNext, we perform the entire graph aggregation is\n \n[H(i) = σ\n( N∑\nj=1\nh(i)\nj U(i)\n)\n] (7)\nFinally, in the multi-graph fusion stage, we aggregate the representations of multiple graphs is\n \n[H = σ\n( n∑\ni=1\nH(i)V\n)\n] (8)\nResidual networks\nResidual networks (ResNet) are widely used in deep learning, with the core idea of solving the vanishing gradient \nproblem by adding short connections (or skip connections), allowing the building of very deep neural networks. \nIn intelligent decision support models, especially in multi-agent systems, deep networks contribute to learning \ncomplex decision strategies and recommendation generation. The introduction of ResNet can help overcome \ngradient issues during deep model training, thereby improving system performance.\nThe fundamental building block of ResNet is the Residual Block. The basic formula for a Residual Block is\nInput is x.Output is F(x)+ x, where F(x) represents the learned residual, and x is the input feature.\nSpecific formula for residual Block\nAssuming the input is x, the output of the Residual Block is F(x)+ x. The specific formula is\n y = F(x, {Wi})+ x (9)\nx represents the input feature, y signifies the output feature of the Residual Block, F(x, {Wi}) represents the \nresidual learning mapping controlled by weight parameters.\nSpecific form of the mapping function\nThe mapping function typically consists of convolutional layers, batch normalization, and an activation function. \nIt can be expressed as\n y = σ(W2 ∗ δ(W1 ∗ x + b1)+ b2)+ x (10)\nIn this context, the symbols represent specific operations and parameters\n∗ denotes the convolution operation, W1 and W2 are the weight parameters associated with the convolutional \nkernels, b1 and b2 are the bias terms, δ signifies the non-linear activation function, σ denotes the batch \nnormalization operation.\nInterpretation of symbols\nx represents the input feature, y is the output feature of the Residual Block, F(x, {Wi}) represents the residual \nlearning mapping function controlled by weight parameters.\nScientific Reports |        (2025) 15:12062 10| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nConnecting MGCN with ResNet\nTo connect Multi-Graph Convolutional Networks (MGCN) with Residual Networks (ResNet), we need to \nintegrate the graph-level representations from the MGCN into the residual learning framework of ResNet. The \nsteps are as follows:\nFirst,use the output H from the MGCN as the input to the Residual Block of ResNet.\nSecond, apply the Residual Block operations on the graph-level representations H obtained from the MGCN.\nSpecifically, given the graph-level representation H from the MGCN, the Residual Block in ResNet can be \nformulated as\n [y = F(H, {Wi})+ H] (11)\nHere, H serves as the input feature for the Residual Block, and y is the output feature. The residual mapping \nF(H, {Wi}) is learned through convolutional layers, batch normalization, and activation functions within the \nResNet framework.\nThe detailed operation for the mapping function is\n [y = σ(W2 ∗ δ(W1 ∗ H + b1)+ b2)+ H] (12)\nCross attention mechanisms\nIn a multi-agent system, collaboration and information sharing are crucial. The introduction of cooperative \nattention mechanisms enables effective information transmission and collaboration between different \ncomponents. This mechanism ensures that each component can focus on important tasks while sharing key \ninformation to optimize the overall performance of the system.\nIn the context of computer vision and natural language processing, cross attention is commonly employed for \nhandling multimodal data, such as images and text. Assuming we have two input sequences represented as the \nimage features X ∈ Rm×dx  and text features Y ∈ Rn×dy , where m and n are the lengths of the sequences, and \ndx and dy are the dimensions of each feature.\nThe goal of cross attention is to assign weights to each image feature based on the text features. This can be \nachieved through the following steps:\nThe representations in the context of the model are as follows: Q and K are the Query and Key representations, \nrespectively, with dimensions m × dq and n × dq, where dq is the query dimension. K′ and Q′ represent the \nKey and Query representations for text features, respectively, with dimensions m × dk and n × dk, where dk is \nthe key dimension. Similarly, V  and V ′ are the Value representations for image and text features, respectively, \nwith dimensions m × dv and n × dv, where dv is the value dimension.\nCompute Cross Attention Weights For each image feature xi, calculate its cross attention weights with all text \nfeatures. The formula for cross attention weight αij  is given by: The computation of cross-attention weights is \nexpressed by the formula.\n \nαij = exp(qi · k′\nj )∑\nj′ exp(qi · kj′ )  (13)\nTo generate the cross-attention output for image features, a weighted sum is performed using the computed \nweights.\n \nyi =\n∑\nj\nαij v′\nj  (14)\nHere, · denotes the dot product of vectors, and exp represents the exponential function.\nThe symbol explanations are as follows: In the context provided, the symbols are interpreted as follows: X \nrepresents the sequence of image features, Y  denotes the sequence of text features, and Q ,K ,V represent the \nquery, key, and value representations for image features, respectively. Additionally, Q′,K ′,V ′ represent the \nquery, key, and value representations for text features.\nAdditionally, αij  represents the cross-attention weight between the image feature xi and text feature yj , \nwhile yi represents the cross-attention output for the image feature xi.\nConnecting ResNet with cross attention\nTo connect Residual Networks (ResNet) with Cross Attention mechanisms, the output features of the Residual \nBlock are used as the input for the Cross Attention layer. The steps are as follows:\nFirst use the output feature y obtained from the Residual Block as the query representation Q for the Cross \nAttention mechanism.\nSecond use the output feature y from the Residual Block as the image features X. Obtain the text features Y  \nfrom a separate source.\nThird compute the cross-attention weights and outputs using the formulas provided in the Cross Attention \nsection.\nGiven the output feature y from the Residual Block, the Cross Attention layer processes it as follows\n Q = y\nScientific Reports |        (2025) 15:12062 11| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nCompute the cross-attention weights like\n \n[αij = exp(qi · k′\nj )∑\nj′ exp(qi · kj′ ) ] (15)\nGenerate the cross-attention output is\n \n[yi =\n∑\nj\nαij v′\nj ] (16)\nHere, y is the input feature from the Residual Block, and yi is the cross-attention output feature.\nBy integrating the outputs from ResNet with the Cross Attention mechanism, the model can effectively \nhandle multimodal data, enhancing the overall performance of multi-agent systems.\nIntroduction to experimental setup, dataset, and hyperparameters\nDatasets\nWe gathered information from Ctrip pertaining to key tourist destinations in Guangdong Province, encompassing \ncities such as Foshan, Shantou, Shenzhen, Zhuhai, Zhanjiang, Dongguan, Huizhou, Guangzhou, Qingyuan, \nY angjiang, and Shaoguan. The data is stored in CSV format files and encompasses various attributes, including \n‘Name’ , ‘English Name’ , ‘id’ , ‘poiID’ , ‘Longitude’ , ‘Latitude’ , ‘Tags’ , ‘Features’ , ‘Price’ , ‘Minimum Price’ , ‘Rating Score’ , \n‘Number of Reviews’ , ‘Cover Image’ , ‘ Adult Ticket Price’ , ‘Senior Ticket Price’ , ‘Student Ticket Price’ , ‘Children \nTicket Price’ , ‘Recommended Duration’ , ‘Opening Hours’ , ‘Introduction’ , and ‘Concession Policy’ . Each of these \ncolumns comprises 9004 data entries.\nWe evaluate our model on two datasets.\n• Guangdong Tourism Dataset: This dataset contains information on major tourist cities in Guangdong Prov -\nince, including Foshan, Shantou, Shenzhen, and Guangzhou. It comprises 9004 entries, with each entry con-\ntaining attributes such as attraction names, ratings, review counts, and images.\n• QK-Video Dataset: This is a large-scale, multi-scene real-world dataset collaboratively released by Tencent \nand Westlake University. It includes approximately 5 million users and 140 million interaction records, such \nas clicks, likes, and shares.\nBaseline methods\nWe compare our model against several state-of-the-art baseline methods, including LightGCN, DualGNN, \nBlip2, and CLIP . LightGCN is a graph-based recommendation model that simplifies the traditional graph \nconvolutional network (GCN) by removing feature transformation and nonlinear activation49. It is widely used \nin collaborative filtering tasks due to its efficiency and strong performance. DualGNN is a dual graph neural \nnetwork that captures both user-item interactions and item-item relationships, making it suitable for complex \nrecommendation scenarios 50. Blip2 is a multimodal model that excels in natural language processing and \nimage processing, particularly in tasks requiring joint understanding of text and images51. CLIP is a contrastive \nlearning-based multimodal model with powerful text and image understanding capabilities, making it a strong \nbaseline for multimodal recommendation tasks52. These baselines were chosen because they represent the latest \nadvancements in recommendation systems and provide a comprehensive benchmark for evaluating our model’s \nperformance in handling multimodal data and user intent (Table 3).\nModel variant Accuracy Precision Recall F1 score\nFull model 0.9526 0.94 0.95 0.94\nWithout MGCN 0.9421 0.92 0.93 0.92\nWithout cross-attn 0.9483 0.93 0.94 0.93\nWithout LLM 0.9305 0.91 0.92 0.91\nTable 4. Ablation study results.\n \nModel variant Accuracy Precision Recall F1 score\nFull model 0.9526 0.94 0.95 0.94\nWithout MGCN 0.9123 0.89 0.90 0.89\nWithout cross-attention 0.9321 0.91 0.92 0.91\nWithout LLM 0.9025 0.88 0.89 0.88\nTable 3. Ablation study results.\n \nScientific Reports |        (2025) 15:12062 12| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nAblation studies\nTo demonstrate the contribution of each component, we conduct ablation studies by removing one component \nat a time and measuring the performance drop. The results are shown in Table 4.\nComparison with state-of-the-art methods\nWe compare our model against recent baselines in multimodal recommendation, including LightGCN 49 and \nDualGNN50. Our model achieves superior performance across all metrics, as shown in Table5.\nAblation study analysis\nThe purpose of the ablation study is to evaluate the contribution of each component to the overall performance \nof the model. By removing each component and observing the changes in performance, we can determine which \ncomponents are crucial for the model’s success.\n(a) Key Analysis: To evaluate the contribution of each component, we conducted an ablation study by removing \none component at a time. The results are shown in Table  3. The full model, which includes all components \n(MGCN, Cross-Attention, and LLM), achieves the highest performance across all metrics, with an accuracy of \n0.9526, precision of 0.94, recall of 0.95, and F1 score of 0.94.\nRemoving the MGCN component led to a significant drop in accuracy (from 0.9526 to 0.9123), precision \n(from 0.94 to 0.89), recall (from 0.95 to 0.90), and F1 score (from 0.94 to 0.89). This indicates that MGCN plays \na crucial role in capturing the complex relationships between textual and image features. The MGCN’s ability to \nconstruct and fuse multiple graphs representing different views of the data is essential for handling multimodal \ndata effectively.\nRemoving the Cross-Attention mechanism resulted in a moderate performance drop, with accuracy \ndecreasing from 0.9526 to 0.9321, precision from 0.94 to 0.91, recall from 0.95 to 0.92, and F1 score from 0.94 to \n0.91. This suggests that while Cross-Attention is important, its impact is less pronounced compared to MGCN. \nThe Cross-Attention mechanism primarily enhances the fusion of text and image features, but the model can still \nrely on other mechanisms (e.g., MGCN) to some extent.\nFinally, removing the LLM component caused a significant performance drop, with accuracy decreasing \nfrom 0.9526 to 0.9025, precision from 0.94 to 0.88, recall from 0.95 to 0.89, and F1 score from 0.94 to 0.88. \nThis highlights the importance of the LLM in understanding user intent and generating personalized \nrecommendations. The LLM’s ability to process natural language and reflect on user feedback is critical for \nimproving recommendation accuracy and user satisfaction.\nIn summary, the ablation study demonstrates that all components (MGCN, Cross-Attention, and LLM) \ncontribute to the model’s performance, with MGCN and LLM having the most significant impact. The full \nmodel, which integrates all components, achieves the best performance, underscoring the importance of their \ncollaborative functioning.\n(b) Conclusion: MGCN and LLM are the most critical components of the model, and their removal leads \nto a significant decrease in performance. The Cross-Attention Mechanism has a relatively minor impact on \nperformance improvement, but it still positively contributes to the model’s multimodal information fusion \ncapability. The full model, which includes all components, achieves the best performance across all metrics, \ndemonstrating the importance of the collaborative functioning of each component.\nComparison with state-of-the-art methods\nWe also compared our proposed model with two state-of-the-art baseline methods (LightGCN and DualGNN), \nwith results as follows \n(a) Key Analysis: Our Model versus LightGCN, our model outperforms LightGCN by 0.316 in accuracy and \nby 0.406 in F1 score (0.94 vs. 0.90). This indicates that our model significantly surpasses LightGCN in handling \nmultimodal data and generating personalized recommendations.\nOur Model versus DualGNN,our model exceeds DualGNN by 0.241 in accuracy (0.9526 vs. 0.9285) and by \n0.330 in F1 score (0.94 vs. 0.91). This suggests that our model is superior in capturing user intent and integrating \nmultimodal information compared to DualGNN.\nBaseline Performance,the performances of LightGCN and DualGNN are similar, though DualGNN slightly \noutperforms LightGCN across all metrics. Nevertheless, our model significantly outperforms both baseline \nmethods in accuracy, precision, recall, and F1 score.\n(b) Conclusion: Our model significantly outperforms the latest baseline methods (LightGCN and DualGNN) \nin terms of accuracy, precision, recall, and F1 score. This indicates that our model has stronger performance in \nmultimodal recommendation tasks, effectively handling complex user intents and multimodal data.\nModel Accuracy Precision Recall F1 score\nOur model 0.9526 0.94 0.95 0.94\nLightGCN 0.9210 0.90 0.91 0.90\nDualGNN 0.9285 0.91 0.92 0.91\nTable 5. Comparison with state-of-the-art methods.\n \nScientific Reports |        (2025) 15:12062 13| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nSummary analysis\nAblation Study,MGCN and LLM are the most critical components of the model, and their removal leads to \na significant decline in performance. The Cross-Attention Mechanism has a relatively minor impact on \nperformance improvement but still positively contributes to multimodal information fusion. The full model \n(with all components) performs best across all metrics, underscoring the importance of the collaborative \nfunctionality of each component.\nComparison with Baseline Methods,Our model significantly outperforms the latest baseline methods \n(LightGCN and DualGNN) in accuracy, precision, recall, and F1 score. This demonstrates that our model \nexhibits superior performance in multimodal recommendation tasks, effectively addressing complex user \nintents and multimodal data.\nExperimental setup\nThis study aims to construct a multimodal dialogue system that determines user intent based on keywords \nprovided by the user and offers corresponding services according to different intents. The specific tasks include \nchat dialogues and travel consultations, with the latter involving expert advice from various fields such as natural \nsciences, social sciences, humanities, engineering and technology, medical and health sciences, business, and \nmanagement.\nLightGCN is a streamlined yet powerful Graph Convolution Network (GCN) specifically designed for \ncollaborative filtering recommendation tasks49. Due to its excellent performance, I integrated it with BILP and \nCILP to endow it with multimodal recommendation capabilities, enabling comparisons with my algorithm.\nHyperparameters\nIn our deep learning model, we carefully selected a set of hyperparameters to achieve optimal performance in \ntext and image processing tasks. The choice of these hyperparameters was meticulously considered to maximize \nthe model’s performance for the given tasks. For the specific parameters of the models mentioned in this article, \nplease refer to Table 3.\nFirstly, we defined the maximum length of text sequences ( Max_sequence_length) as 1000, reflecting \nthe need to process text data containing rich information. To represent the text, we utilized a 300-dimensional \nembedding layer (Embedding_dim) to ensure the retention of sufficient semantic information (Table 6).\nIn the design of the Convolutional Neural Network (Conv1D), we chose 100 filters ( num_filters) with a \nfilter size (filter_size) of 3. This configuration was carefully selected to maintain moderate complexity while \ncapturing local features in the text. Additionally, we introduced a 50% Dropout ( drop) to prevent overfitting, \nthereby enhancing the model’s generalization capability.\nThe vocabulary size ( vocab_size) was set to 10000, balancing the richness of the vocabulary and the \ncomputational efficiency of the model. This choice was based on our analysis of the dataset to ensure an adequate \nvocabulary size to capture the diversity of the text.\nFor the optimizer selection, we adopted the Adam optimizer with an initial learning rate (lr) of 0.001. This \nchoice was based on the widespread application of the Adam optimizer in training deep learning models and has \nbeen empirically proven effective for our task.\nFinally, during the training process, Mean Squared Error (MSE) was employed as the loss function since our \ntask is a regression problem. Accuracy was used as the metric to evaluate the model’s performance.\nComparison of model effects\nIn the part of evaluation metrics,to assess the performance of our model, we employ the following metrics. \nAccuracy is used to measure the model’s accuracy in the task of purpose identification, i.e., the proportion of \ncorrectly classifying user needs and purposes. In the part of model descriptions, Blip2 is a multimodal model \nexcelling in natural language processing and image processing. We select Blip2 as a benchmark for comparison \nwith existing state-of-the-art models51. In our dataset, the algorithm achieves an accuracy of 0.6541.\nCLIP is a contrastive learning-based multimodal model with powerful text and image understanding \ncapabilities. We use CLIP as a benchmark to evaluate our model’s performance on text-image contrast tasks52. In \nour dataset, the model achieves an accuracy of 0.5561.\nThe initial version performs exceptionally well on our dataset, with an accuracy of 0.9519.\nThe cross-attention version achieves an accuracy of 0.9524 on our dataset.\nThe MGCN version attains an accuracy of 0.9526 on our dataset.\nComprehensive Comparison In the comprehensive comparison, the introduction of the cross-attention \nmechanism improves accuracy by 0.0005, and the introduction of MGCN improves accuracy by 0.0002. We also \nParameter Value\nMax sequence length 1000\nEmbedding dimension 300\nNumber of filters 100\nFilter size 3\nDropout rate 0.5\nVocabulary size 10000\nTable 6. Model configuration.\n \nScientific Reports |        (2025) 15:12062 14| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\ntrain and evaluate comparative models such as Blip2, CLIP to compare with state-of-the-art multimodal models. \nThrough a detailed analysis of the model’s performance on different tasks and validation of the experimental \nresults“robustness and feasibility through cross-validation and actual user feedback, we can comprehensively \nevaluate the performance of our multimodal consultation model in the tourism consultation task. This allows us \nto determine its strengths and weaknesses relative to other models. These results will guide further improvement \nand optimization of the model to provide a better travel experience.Table 7 shows the key performance indicators \nof different models.\nThis table covers key performance indicators for five different models, including accuracy, precision, recall, \nF1 score, and root mean square error (RMSE). A preliminary data analysis is conducted for each model. Blip2 \nexhibits a high accuracy of 0.6541, but in comparison to other metrics, its precision and recall are relatively \nbalanced. With an F1 score of 0.63, it performs well in balancing precision and recall. The RMSE of 0.23 indicates \nrelatively small errors for regression tasks. CLIP has a relatively lower accuracy of 0.5561. Its precision, recall, \nand F1 score are 0.55, 0.58, and 0.56, respectively, suggesting limited performance in certain tasks. The RMSE of \n0.31 may indicate significant errors in regression tasks. The Initial Version outperforms all models with a high \naccuracy of 0.9519. It boasts excellent precision (0.94) and recall (0.96), resulting in an impressive F1 score of 0.95. \nThe RMSE of 0.08 signifies relatively small errors. Cross Attention achieves an accuracy of 0.9524, comparable \nto the Regular Version. It demonstrates good precision (0.93) and recall (0.94), yielding an F1 score of 0.93. \nThe RMSE of 0.07 indicates minor errors. MGCN achieves an accuracy of 0.9526, slightly higher than Cross \nAttention. It has good precision (0.94), recall (0.95), and F1 score (0.94). With an RMSE of 0.06, it exhibits the \nsmallest errors among all models. Overall, Initial Version, Cross Attention, and MGCN perform exceptionally \nwell across all metrics, especially in terms of accuracy and F1 score. Blip2 shows a more balanced performance, \nwhile CLIP falls slightly short in accuracy and related metrics. Model selection should be based on specific task \nrequirements and performance criteria.\nIn my research, I focus on specialized datasets such as the Guangdong Tourism Dataset and QK-Video, which \ndiffer significantly from classic recommendation system datasets like Movielens and Netflix. These specialized \ndatasets possess characteristics such as content specificity, user behavior sparsity, and context dependency, \nposing challenges for traditional recommendation models in capturing sparse user feedback. The motivation \nbehind my work is to address these recommendation challenges in specialized datasets, including sparsity, \ncontext dependency, and the need for recommendations tailored to specific content. Meanwhile, I highlight the \nlimitations of existing recommendation models in capturing sparse user feedback, making them slow to adapt \nand learn. To address this, I propose a framework that combines advanced recommendation models with Low-\nRank Models (LLM), aiming to quickly capture sparse user feedback through efficient representation learning \nand provide more accurate recommendations in a shorter time. Experimental results demonstrate that my \nrecommendation+LLM framework performs excellently on both objective and subjective evaluation metrics, \nachieving impressive performance not only on public datasets but also exhibiting significant advantages on \nprivate datasets, showcasing its strong generalization capability and practicality.\nAdditionally, this paper leveraged the QK-video dataset 53, a large-scale, multi-scene real-world dataset \ncollaboratively released by Tencent and Westlake University, specifically tailored for evaluating recommendation \nsystems53. Originating from Tencent’s two major information flow recommendation platforms, OK platform \nand QB platform, the dataset encompasses both articles and videos with varying degrees of overlap, boasting \nan expansive scale with approximately 5 million users and 140 million interaction records, including clicks, \nlikes, and shares. The dataset further provides intrinsic user characteristics like age and gender, along with \nitem features such as video types. It’s worth noting that the click behavior in the QK-video dataset follows a \nlong-tail distribution, while session lengths are predominantly distributed between 0 and 20. Furthermore, we \ndiscussed the Tenrec dataset, an extension of the QK-video dataset, tailored for evaluating a diverse array of \nrecommendation tasks, including click-through rate prediction, session-based recommendation, multi-task \nrecommendation, and more. By publicly releasing the dataset, code, and leaderboards, Tenrec aims to catalyze \nfurther advancements in the recommendation system research community.\nThis study comprehensively demonstrate its core contributions through experiments. Firstly, the MGCN \nmodel excels on both public and private datasets, particularly on the Guangdong Tourism Dataset, QK-Video, \nand QB-Video datasets, where its Recall metrics significantly outperform other comparative models. Secondly, \nthe introduction of a Large Language Model (LLM) enhances the model’s performance in cold start and sparse \nuser feedback scenarios. The application of LLM across multiple roles, including Actor, Evaluator, Self-reflection, \nand Memory, bolsters the decision-making quality and personalized recommendation capabilities of the model. \nThrough in-depth analysis of user-system interactions, LLM extracts user preferences and subtle interaction \nnuances. Lastly, we have constructed a novel dataset encompassing information on major tourist cities in \nGuangdong Province. This dataset furnishes essential inputs and labels for the research on multimodal attraction \nModel Accuracy Precision Recall F1 Score RMSE\nBlip2 0.6541 0.64 0.66 0.63 0.23\nCLIP 0.5561 0.55 0.58 0.56 0.31\nRegular Version 0.9519 0.94 0.96 0.95 0.08\nCross Attention 0.9524 0.93 0.94 0.93 0.07\nMGCN 0.9526 0.94 0.95 0.94 0.06\nTable 7. Key performance indicators for different models.\n \nScientific Reports |        (2025) 15:12062 15| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nrating frameworks, thereby enhancing the operational feasibility and accuracy of the model. This unique dataset \nnot only broadens the model’s applicability but also provides a valuable resource for researchers to delve deeper \ninto multimodal data processing and recommendation system applications. Table  8 shows the experimental \nresults of different data sets on the Guangdong tourism data set.\nModule function and performance analysis\nThe model comprises a MGCN, Residual Network (ResNet), and Cross Attention Mechanism. Cross Attention \nMechanism, in a multi-agent system, collaborative cooperation and information sharing are crucial. The \nintroduction of a collaborative attention mechanism enables effective information transfer and collaboration \namong different components. This mechanism ensures that each component can focus on important tasks while \nsharing critical information to optimize the overall system performance. The incorporation of Cross Attention \ninto the model primarily addresses the processing requirements for multimodal data. Intelligent decision \nsupport systems may need to handle various data types such as text and images. Through Cross Attention, \nkey features can be extracted from images, aiding the system in better understanding and processing image \ninformation, such as scenic spot pictures. Multi-View Convolutional Neural Network.,the Multi-View CNN \nextends the capabilities of traditional CNN, particularly suitable for handling multimodal data. In this context, \nboth scenic descriptions and pictures can be considered as different “views” , and the Multi-View CNN effectively \nmerges this information, providing a more comprehensive data representation. This contributes to enhancing \nthe overall performance of the system and the quality of intelligent consultations. ResNetis widely employed \nin deep learning, with its core idea of addressing the vanishing gradient problem by adding short connections \n(or skip connections), allowing the construction of very deep neural networks. In intelligent decision support \nmodels, especially in multi-agent systems, deep networks contribute to learning complex decision strategies and \nrecommendation generation. The introduction of ResNet helps overcome gradient issues when training deep \nmodels, thereby improving system performance.\nIntegrating technologies such as MGCN, Cross Attention, , and ResNet into the tourism advisory system \nbrings various advantages and addresses several challenges in the field. MGCN efficiently handles multimodal \ndata, including text and images, facilitating a better integration and understanding of diverse modalities. \nThis is particularly crucial for tourism advisory systems, as users may provide textual descriptions, images, \nor a combination of both. The Cross Attention mechanism enhances the model’s understanding of inter-\nmodality relationships, empowering the model to concurrently consider both textual and visual information, \nthereby comprehensively meeting user needs. ResNet serve as powerful models for processing image data. In \ntourism advisory scenarios where users may submit images of attractions, employing these models allows the \nsystem to extract more advanced and meaningful features from the images, providing richer information for \nrecommendations and consultations. The residual network structure of ResNet aids in mitigating the vanishing \ngradient problem during the training of deep neural networks, making the network more amenable to training \nand optimization. Multi-View Neural Networks contribute to handling diverse modal data, preserving their \nrespective information and integrating them. This is particularly crucial for synthesizing textual and visual \ninformation provided by users. The Self-Reflection mechanism enables the model to consider its own state \nduring decision-making, enhancing the system’s self-awareness and adjustment capabilities. This contributes \nto improving the accuracy of recommendations and user satisfaction. Pretraining text with BERT facilitates a \nbetter understanding and processing of descriptions of attractions within the system. Pretraining images with \nResNet aids in extracting advanced features from images, offering the system additional visual information. By \nintegrating these advanced technologies, the tourism advisory system can comprehensively and intelligently \nhandle user queries, providing more accurate and personalized recommendations. This integration aims to \nenhance user experience and overall system performance.\nDataset Method Recall@5 Recall@10 Recall@50\nTourism Dataset\nClip 0.3423 0.4867 0.6682\nBlip2 0.4531 0.5763 0.7283\nCross attention 0.5520 0.8522 0.9524\nMGCN 0.5524 0.6524 0.9524\nQK-Video\nClip 0.4863 0.5974 0.7104\nBlip 0.5143 0.63967 0.76931\nCross attention 0.5866 0.6077 0.8605\nMGCN 0.6556 0.7677 0.9559\nQB-Video\nClip 0.46631 0.5882 0.7563\nBlip 0.5021 0.63854 0.77862\nCross attention 0.5766 0.6874 0.8705\nMGCN 0.6856 0.7977 0.9059\nQK-article\nCross attention 0.5864 0.6976 0.8984\nMGCN 0.6857 0.7056 0.9161\nTable 8. Experimental results on Guangdong tourism dataset.\n \nScientific Reports |        (2025) 15:12062 16| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nIn this code segment, my focus is on text and image processing to address challenges within the tourism \nadvisory system. For textual data, Tokenizer is employed for word segmentation, followed by the utilization \nof the Embedding layer to transform the text into vector representations. Additionally, the MGCN is applied \nto extract crucial features from the textual data. To enhance model performance, pre-trained word vectors are \nincorporated using GloVe embeddings. For image data, I introduced ResNet (Residual Neural Network) to handle \nscenic images, utilizing the MGCN for convolutional operations and ultimately applying GlobalMaxPooling1D \nto achieve global pooling on the convolutional results. This aids in extracting advanced features from images, \nproviding more information for recommendations and consultations.\nReflexion improves interactive performance\nThe function of this module is to enhance LLM’s recognition of the user’s purpose and improve the user’s \nexperience during the conversation with the user. Through this module, LLM will conduct a simple reinforcement \nlearning of its own conversation with the user and constantly prompt LLM according to the user’s language, so as \nto more accurately recognize the user’s intention and improve the user experience.\nReflexion ablation experiment\nThis paper designed the following experiment, which consists of two groups: the experimental group and the \ncontrol group. The experimental group utilizes the reflexion mechanism, while the control group does not. The \npurpose of the experiment is to evaluate the impact of the reflexion mechanism on the model’s performance. \nFig. 4 is the flowchart of reflection. The experiment process is Participants in the experimental group carry out \na travel recommendation task, and the model uses the reflexion mechanism during the task execution. Firstly, \nGroup Pass1 accuracy Pass2 accuracy\nExperimental 68% 91%\nControl 65% 72%\nTable 9. Results of the reflexion mechanism experiment.\n \nFig. 4. The flowchart of reflection.\n \nScientific Reports |        (2025) 15:12062 17| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nthe model queries experts from different fields based on the participant’s stated travel purpose and preferences \nto obtain travel recommendations. Then, based on the expert advice and participant responses, the model \nengages in decision-making reasoning using the reflexion mechanism and selects the next course of action. \nFinally, the model provides the ultimate travel recommendation. As shown in Table 9,the pass1 accuracy for the \nexperimental group is 0.68, and the pass2 accuracy is 0.91. Participants in the control group perform the same \ntravel recommendation task as the experimental group, but the model does not utilize the reflexion mechanism \nduring the task execution. The model directly engages in decision-making reasoning based on expert advice and \nparticipant responses, selecting the next course of action. The pass1 accuracy for the control group is 0.65, and \nthe pass2 accuracy is 0.72. Table 6 is the results of the reflection mechanism experiment.\nDecision making\nFor decision-making, in the task part, users are planning a trip with the objective of choosing a destination \nrecommended by experts. In the trajectory part, six expert agents responded to the user’s inquiries simultaneously \nbased on their expertise. The natural sciences expert suggested Iceland for its natural landscapes. The social \nsciences expert recommended Japan for its cultural experiences. The humanities expert proposed Italy for its \nhistorical sites. The medical and health sciences expert advised Switzerland for its relaxation and health facilities. \nThe engineering and technology expert favored Singapore for its advanced technology and architecture. The \nbusiness and management expert suggested New Y ork for its business and financial center.\nIn the evaluation part, users responded positively or negatively. Despite the natural sciences expert not \nbeing online at the time, the user continued to consider advice from other experts. In the reflection part, \nusers contemplated each destination’s unique attractions and their personal interests based on the advice from \ndifferent experts. For the next trajectory, users ultimately made a decision that balanced personal interests with \nthe experts’ recommended travel destination. Specifically, the process is as shown in Fig. 5.\nReasoning task\nFor the reasoning task,\nIn the task part, a tourist is planning their vacation next month and seeks evaluations and recommendations \nabout travel destinations, aiming to choose a place that aligns with personal interests and receives expert \nendorsement.\nIn the trajectory part, the tourist asked various experts for evaluations of a destination: the tourist rates it \n8/10 for its scenic beauty with a bridge spanning the sea offering views and opportunities to study marine life; \nthe natural sciences expert rates it 7/10, seeing the bridge linking regions as fostering economic and cultural \nexchange; the social sciences expert rates it 7/10, viewing the bridge as a symbol of human ingenuity; the \nhumanities expert rates it 6/10, appreciating the fresh air on the bridge; the engineering and technology expert \nrates it 9/10, praising the modern engineering of the bridge; the business and management expert rates it 7/10, \nbelieving the bridge’s construction promotes economic growth; the medical and health sciences expert rates it \n8/10, praising the bridge’s health and relaxation environment.\nIn the evaluation part, the tourist feels the experts’ evaluations are not quite suitable and requests \nreconsideration.\nIn the reflection part, the system thanks the tourist for their feedback and asks for specifics on dissatisfaction \nwith the evaluations or expert opinions to improve recommendations. The tourist points out that the natural \nsciences expert’s rating is too high and the engineering and technology expert’s rating is also slightly high.\nThe experts provide adjustment reasons: although the bridge offers beautiful scenery, biodiversity and \nresearch opportunities are limited, so the natural sciences expert’s rating is adjusted to 7/10. Despite excellent \nengineering, maintenance challenges and environmental impact reduce the engineering and technology expert’s \nrating to 8/10.\nFor the next trajectory, the tourist accepts the adjusted ratings and expresses satisfaction. The system suggests \nfurther optimizing the tourist’s travel experience and asks if they would like more information about this \nattraction.Specifically, the process is as shown in Fig. 6.\nTravel plan\nIn the travel plan task, the tourist is preparing to arrange a trip but has not yet determined specific plans. They \nseek assistance from experts to plan a perfect travel itinerary.\nIn the task section, the tourist seeks expert help to craft a flawless travel plan. In the trajectory section, \nthe tourist inquires about advice and ratings for Shantou Old Town, expressing a love for ancient architecture. \nThroughout the process, experts provide ratings and recommendations.\nThen in the evaluation section, based on the tourist’s interests and schedule, they choose the most suitable \ntravel destination and develop a corresponding travel plan. In the reflection section, the natural sciences expert \nrates Shantou Old Town highly for its exceptionally beautiful natural environment, perfect for nature enthusiasts. \nThe social sciences expert notes the town’s rich history and culture, suggesting that exploring its streets provides \ndeep insights into local traditions and history. The humanities expert praises the town’s many unique ancient \nbuildings, ideal for architecture enthusiasts to appreciate and study. The medical and health sciences expert \nrecommends Shantou Old Town for walking but advises on sunscreen and hydration for a comfortable \nsightseeing experience. The engineering and technology expert acknowledges the town’s aging infrastructure \nbut finds its architectural techniques and historical value very intriguing. The business and management expert \nhighlights the town’s numerous unique shops and restaurants, offering local cuisine and a vibrant commercial \natmosphere. In the next trajectory section, the tourist expresses gratitude for the experts’ advice, looks forward \nto the trip, and indicates no further need for suggestions. Specifically, the process is as shown in Fig. 7.\nScientific Reports |        (2025) 15:12062 18| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nHuman study\nEthical Statement as fellow.\nAll human subject experiments and/or the use of human tissue samples reported in this manuscript were \nconducted in full compliance with the relevant ethical guidelines and regulations. The experimental protocols \nwere reviewed and approved by licensing committee of Fenglang Technology Co. All experiments were \nperformed in accordance with the prescribed ethical standards. Furthermore, informed consent was obtained \nfrom all participants and/or their legal guardians prior to participation in the study. Detailed information \nFig. 5. Reflection works on decision-making.\n \nScientific Reports |        (2025) 15:12062 19| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nregarding the approval process, including the approving institutions and relevant committee, is provided in the \n“Methods” section of the manuscript.\nAll methods were conducted following the relevant guidelines and regulations. The experiments were \napproved by the designated institution and/or licensing committee. Informed consent from all participants and/\nor their legal guardians was obtained. This should fulfill the requirements of the approval committee.\nTo validate the performance of the system, we conducted a human study. The research design includes the \nfollowing steps:\nFig. 6. Reflection works on reasoning.\n \nScientific Reports |        (2025) 15:12062 20| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nParticipant recruitment: We recruited participants from a variety of ages and backgrounds to ensure that the \nfindings were representative.\nExperimental process is participants were asked to conduct a series of conversations with the system, involving \nissues and tasks in different fields. Their inputs and the system’s responses are recorded for subsequent analysis.\nFeedback collection is that after the experiment, we collected feedback from participants, including their \nsatisfaction with the system’s performance, understanding, and suggestions for improvement. This feedback will \nbe used to evaluate the effectiveness of the system and user experience.\nFig. 7. Reflection works on travel plan tasks.\n \nScientific Reports |        (2025) 15:12062 21| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nExperiments in adapting models to user preferences\nWe designed this experiment through a questionnaire survey. A total of 1000 questionnaires were distributed, of \nwhich 500 were male Female: 450 Others: 50 people\nQuestionnaire sample\nDear Participant,\nThank you for participating in our research! Y our valuable feedback is crucial for improving our conversational \nsystem. Please take some time to complete the following survey to help us better understand the system’s \nperformance and user experience.\nQuestionnaire data and data analysis of questionnaire data\nAnalysis of user age\nIn terms of demographic characteristics, we observed that the age distribution of participants was wide, with an \naverage age of 34 years old, and the youngest and oldest participants were 16 and 78 years old, respectively. In \nterms of gender, male and female participants were basically balanced, while the proportion of participants of \nother genders was small. The occupational distribution showed that the system mainly attracted students and \nworking people, while freelancers and participants of other occupations were relatively small, which showed that \nthe system met the needs of people of all ages and occupational backgrounds and had the potential for practical \nimplementation.\nAnalysis of user experience\nRegarding user experience, most participants said that they had used similar systems before, accounting for 0.65\nAnalysis of dialogue system experience\nOverall, the participants’ satisfaction with the system was mainly concentrated in the satisfied and general levels, \naccounting for 0.60, while 0.25 indicated dissatisfied and very dissatisfied. In terms of system understanding \nand response accuracy, 0.30 participants indicated full understanding, 0.40 indicated some understanding, 0.20 \nindicated little understanding, and 0.10 indicated no understanding at all. These results indicate that the system \nexhibits good practicality and ease of use in terms of user experience, meets human usage needs, and has the \npotential for practical implementation.\nSystem function analysis\nThe evaluation of the chat module of the system is more scattered, with 0.25 participants believing it is excellent, \n0.35 believing it is good, and 0.10 believing it is relatively poor or very poor. For the tourism consultation \nmodule, 0.30 participants believed it fully met expectations, 0.35 believed it generally met expectations, and \n0.10 believed it did not meet expectations well or did not meet expectations at all. These evaluations reflect the \nactual performance of the system under different functional modules, and also highlight the system’s usability \nand user satisfaction, proving that the system meets human usage needs and has the potential for practical \nimplementation.\nWillingness to participate in future research\nRegarding their willingness to participate in similar research in the future, 0.60 of the participants expressed \ntheir willingness to participate, indicating that they showed a strong interest in system improvement and future \nresearch. This further proves the practicality and user acceptance of the system, and provides strong support for \nfuture system improvement and further research.\nIn-depth data analysis\nAt a glance, this looks like what is shown in Fig. 8.According to data analysis, there may be a correlation between \nage and satisfaction. The mean age is about 1.7 and the variance is about 0.61, while the mean satisfaction is \nabout 0.7143 and the variance is about 0.2041, indicating that the younger the age, the higher the satisfaction \nmay be, but more in-depth analysis is needed. In addition, there may be a correlation between usage experience \nand system understanding. The mean usage experience is about 2.0 and the variance is about 0.5714, and the \nmean system understanding is about 0.5 and the variance is about 0.1429, indicating that the richer the usage \nexperience, the higher the system understanding may be.\nConclusion\nThis paper focuses on constructing an innovative multimodal attraction rating framework, utilizing both textual \nand visual information to comprehensively assess the attractiveness and quality of tourist attractions. The \nframework introduces and MGCN to handle multimodal data, allowing the model to synthesize information \nfrom different data sources for a more comprehensive evaluation in travel planning and consultation.\nAnother significant contribution lies in the incorporation of interdisciplinary expert recommendations. \nThrough prompt a Large LLM, representing experts from various fields including natural sciences, social sciences, \nhumanities, engineering and technology, medical and health sciences, and business and management, the model \nprovides users with interdisciplinary travel advice, enriching the diversity and adaptability of recommendations.\nThe prompt of the Large Language Model serves multiple roles, such as Actor, Evaluator, Self-reflection, and \nMemory, positioning the LLM as a core component of a multi-agent system. This enhances the user experience \nin travel consultation and dialogue, while simultaneously improving decision-making quality.\nScientific Reports |        (2025) 15:12062 22| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\nThe construction of a new dataset supports our research, containing key information about attractions in \nmajor tourist cities in Guangdong Province. This dataset, comprising 9004 entries per column, transforms \nattraction descriptions and images into vectors using pre-trained BERT and ResNet50 models, respectively, \nproviding essential inputs and labels for model training and enhancing the operational feasibility of our research.\nIn terms of model results, a detailed evaluation and comparison of different versions of multimodal models, \nincluding Blip2, CLIP , Regular Version, Cross Attention, and MGCN, was conducted. These versions achieved \nsignificant accuracy on our dataset, with improvements of 0.0005 and 0.0002 introduced by the cross-attention \nmechanism and MGCN, respectively. Comparative analysis with state-of-the-art multimodal models like \nBlip2, CLIP was performed. Through a thorough examination of the model’s performance across various tasks \nand validation of results through cross-validation and real user feedback, we comprehensively evaluated the \nperformance of our multimodal consultation model in the tourism consultation task. These findings will guide \nfurther improvements and optimizations to provide an enhanced travel experience.\nData availability\nThe data supporting the findings of this study are openly available in the Tenrec Benchmark Dataset repository \nat https://openreview.net/forum?id=PfuW84q25y9. This dataset includes a comprehensive collection of  b e n c h \nm a r k s for recommender systems, as described in the paper “Tenrec: A Large-scale Multipurpose Benchmark \nDataset for Recommender Systems” by Guanghu Yuan et al. The data used for this study are publicly accessible \nin the following GitHub repository: Tourism-data-set. This dataset contains data for tourism research, manually \ncrawled and organized by the authors of this study. The specific address is  h t t p s : / / g i t h u b . c o m / 9 7 1 8 5 1 3 0 9 / T o u r \ni s m - d a t a - s e t      \nReceived: 13 December 2024; Accepted: 28 March 2025\nReferences\n 1. Fararni, K. A. et al. Hybrid recommender system for tourism based on big data and AI: A conceptual framework. Big Data Min. \nAnal. 4(1), 47–55. https://doi.org/10.26599/BDMA.2020.9020015 (2021).\n 2. Nan, X., Kanato, K. & Wang, X. Design and implementation of a personalized tourism recommendation system based on the data \nmining and collaborative filtering algorithm. Comput. Intell. Neurosci.2022, article ID 1424097, 14.  h t t p s : / / d o i . o r g / 1 0 . 1 1 5 5 / 2 0 2 2 / \n1 4 2 4 0 9 7     (2022).\nFig. 8. Statistics on survey option selections.\n \nScientific Reports |        (2025) 15:12062 23| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\n 3. Salah, A., Truong, Q.-T. & Lauw, H. W . Cornac: A comparative framework for multimodal recommender systems. J. Mach. Learn. \nRes. 21(1), 95 (2020).\n 4. Li, G. et al. Asking images: Hybrid recommendation system for tourist spots by hierarchical sampling statistics and multimodal \nvisual Bayesian personalized ranking. IEEE Access 7, 126539–126560. https://doi.org/10.1109/ACCESS.2019.2937375 (2019).\n 5. Wei, S. & Song, S. Sentiment classification of tourism reviews based on visual and textual multifeature fusion. Wirel. Commun. \nMobile Comput. 2022 (2022).\n 6. Zhang, Y . & Tang, Z. Cross-modal travel route recommendation algorithm based on internet of things awareness. J. Sens.2021, \nArticle ID 5981385, 11. https://doi.org/10.1155/2021/5981385 (2021).\n 7. Do, P ., Phan, T. H. V . & Gupta, B. B. Developing a vietnamese tourism question answering system using knowledge graph and deep \nlearning, ACM Trans. Asian Low-Resour. Lang. Inf. Process.20(5), Art. no. 81, 1–18. https://doi.org/10.1145/3453651 (2021).\n 8. Liu, Y . & Zhu, S. Multimodal wireless situational awareness-based tourism service scene. J. Sens. 2021, 1–9.  h t t p s : / / d o i . o r g / 1 0 . 1 1 5 \n5 / 2 0 2 1 / 8 8 2 7 4 1 6     (2021).\n 9. Hu, X., Yu, W ., Wu, Y . & Chen, Y . Multi-modal recommendation algorithm fusing visual and textual features. PLoS ONE 18(6), \ne0287927 (2023).\n 10. Liu, R. Development of a travel recommendation algorithm based on multi-modal and multi-vector data mining. PeerJ Comput. \nSci. 9, 1436. https://doi.org/10.7717/peerj-cs.1436 (2023).\n 11. Dong, X., Song, X., Tian, M. & Hu, L. Prompt-based and weak-modality enhanced multimodal recommendation. Inf. Fus. 101, \n101989. https://doi.org/10.1016/j.inffus.2023.101989 (2024).\n 12. Yu, P ., Tan, Z., Lu, G., & Bao, B. Multi-view graph convolutional network for multimedia recommendation. arXiv preprint \narXiv:2308.03588 (2023).\n 13. Wang, Q. et al. DualGNN: Dual graph neural network for multimedia recommendation. IEEE Trans. Multimed. 25, 1074–1084. \nhttps://doi.org/10.1109/TMM.2021.3138298 (2023).\n 14. Tao, Z. et al. Self-supervised learning for multimedia recommendation. IEEE Trans. Multimed. 25, 5107–5116 (2022).\n 15. Wu, C., Wu, F ., Qi, T., Zhang, C., Huang, Y . & Xu, T. Mm-rec: Visiolinguistic model empowered multimodal news recommendation. \nIn Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, 2560–2564 \n(2022).\n 16. Liu, F ., Cheng, Z., Sun, C., Wang, Y ., Nie, L., & Kankanhalli, M. User diverse preference modeling by multimodal attentive metric \nlearning. In Proceedings of the 27th ACM International Conference on Multimedia, 1526-1534 (2019).\n 17. Xu, C. et al. Recommendation by users“multimodal preferences for smart city applications. IEEE Trans. Ind. Inf. 17(6), 4197–4205 \n(2020).\n 18. Liu, D., Li, J., Du, B., Chang, J., & Gao, R. DAML: Dual attention mutual learning between ratings and reviews for item \nrecommendation. In Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining , \n344–352 (2019).\n 19. Akincilar, A. & Dagdeviren, M. A hybrid multi-criteria decision-making model to evaluate hotel websites. Int. J. Hosp. Manag. 36, \n263–271 (2014).\n 20. Kashevnik, A. M., Ponomarev, A. V . & Smirnov, A. V . A multimodal context-aware tourism recommendation service: Approach \nand architecture. J. Comput. Syst. Sci. Int. 56, 245–258 (2017).\n 21. Wang, X., Chen, H., Zhou, Y ., Ma, J. & Zhu, W . Disentangled representation learning for recommendation. IEEE Trans. Pattern \nAnal. Mach. Intell. 45(1), 408–424. https://doi.org/10.1109/TPAMI.2022.3153112 (2023).\n 22. Tay, Y ., Luu, A. T. & Hui, S. C.: Multi-pointer co-attention networks for recommendation. In Proceedings of the 24th ACM SIGKDD \nInternational Conference on Knowledge Discovery & Data Mining, 2309-2318 (2018).\n 23. Liu, H., Wang, W ., Xu, H., Peng, Q., & Jiao, P . Neural unified review recommendation with cross attention. In Proceedings of the \n43rd International ACM SIGIR Conference on Research and Development in Information Retrieval, 1789–1792 (2020).\n 24. Chen, C., Zhang, M., Liu, Y ., Ma, S. Neural attentional rating regression with review-level explanations. In Proceedings of the 2018 \nWorld Wide Web Conference, 1583-1592 (2018).\n 25. Jiang, S., Ding, Z. & Fu, Y . Heterogeneous recommendation via deep low-rank sparse collective factorization. IEEE Trans. Pattern \nAnal. Mach. Intell. 42(5), 1097–1111 (2019).\n 26. Zhang, S. et al. Personalized latent structure learning for recommendation. IEEE Trans. Pattern Anal. Mach. Intell. 45(8), 10285–\n10299 (2023).\n 27. Ji, J., et al. GenRec: Large language model for generative recommendation. arXiv preprint arXiv:2307.00457,  h t t p s : / / d o i . o r g / 1 0 . 4 8 \n5 5 0 / a r X i v . 2 3 0 7 . 0 0 4 5 7     (2023).\n 28. Zheng, Z., Qiu, Z., Hu, X., Wu, L., Zhu, H., Xiong, H. Generative job recommendations with large language model. arXiv preprint \narXiv:2307.02157, https://doi.org/10.48550/arXiv.2307.02157 (2023).\n 29. Safdari, M., et al. Personality traits in large language models, arXiv preprint arXiv:2307.00184,  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r X i v . 2 3 0 \n7 . 0 0 1 8 4     (2023).\n 30. Jiang, J., Xu, M., Zhu, S.-C., Han, W ., Zhang, C. & Zhu, Y . Evaluating and inducing personality in pre-trained language models. \nhttps://doi.org/10.48550/arXiv.2307.00184 (2022).\n 31. Xue, W ., et al., PrefRec: Recommender systems with human preferences for reinforcing long-term user engagement, arXiv preprint \narXiv:2212.02779 [cs.IR] (2022).\n 32. Wang, L., & Lim, E.-P . Zero-shot next-item recommendation using large pretrained language models, arXiv preprint \narXiv:2304.03153 (2023).\n 33. Dai, S., et al., Uncovering ChatGPT’s capabilities in recommender systems, arXiv preprint arXiv:2305.02182 (2023).\n 34. Lin, G. & Zhang, Y . Sparks of artificial general recommender (AGR): Early experiments with ChatGPT, arXiv preprint \narXiv:2305.04518 [cs.IR], https://doi.org/10.48550/arXiv.2305.04518 (2023).\n 35. Carranza, A. G., Farahani, R., Ponomareva, N., Kurakin, A., Jagielski, M., & Nasr, M. Privacy-preserving recommender systems \nwith synthetic query generation using differentially private large language models, arXiv preprint arXiv:2305.05973 (2023).\n 36. Friedman, L. et al., Leveraging large language models in conversational recommender systems, arXiv preprint arXiv:2305.07961 \n(2023).\n 37. Hou, Y ., Zhang, J., Lin, Z., Lu, H., Xie, R., McAuley, J. & Zhao, W . X. Large language models are zero-shot rankers for recommender \nsystems, arXiv preprint arXiv:2305.08845 (2023).\n 38. Wang, X., Tang, X., Zhao, W . X., Wang, J., & Wen, J.-R. Rethinking the evaluation for conversational recommendation in the era of \nlarge language models, arXiv preprint arXiv:2305.13112 (2023).\n 39. Li, P ., Wang, Y ., Chi, E. H., & Chen, M. Prompt tuning large language models on personalized aspect extraction for recommendations, \narXiv preprint arXiv:2306.01475 (2023).\n 40. Li, X., Zhang, Y ., & Malthouse, E. C. A preliminary study of ChatGPT on news recommendation: Personalization, provider \nfairness, fake news, arXiv preprint arXiv:2306.10702 (2023).\n 41. Xi, Y ., et al., Towards open-world recommendation with knowledge augmentation from large language models, arXiv preprint \narXiv:2306.10933 (2023).\n 42. Kang, W .-C., et al. , Do LLMs understand user preferences? Evaluating LLMs on user rating prediction, arXiv preprint \narXiv:2305.06474 (2023).\n 43. Liu, Q., Chen, N., Sakai, T. & Wu, X.-M. ONCE: Boosting content-based recommendation with both open- and closed-source large \nlanguage models, arXiv preprint arXiv:2305.06566 [cs.IR], https://doi.org/10.48550/arXiv.2305.06566 (2023).\nScientific Reports |        (2025) 15:12062 24| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/\n 44. Ustalov, D., Fedorova, N., & Pavlichenko, N. Improving recommender systems with human-in-the-loop. In Proceedings of the \n16th ACM Conference on Recommender Systems, RecSys ’22 708-709 (Seattle, W A, USA). https://doi.org/10.1145/3523227.3547373 \n(2022).\n 45. Zhang, J., Xie, R., Hou, Y ., Zhao, W . X., Lin, L., & Wen, J.-R. Recommendation as instruction following: A large language model \nempowered recommendation approach, arXiv preprint arXiv:2305.07001 (2023).\n 46. Zhang, J., Bao, K., Zhang, Y ., Wang, W ., Feng, F ., & He, X. Is ChatGPT fair for recommendation? Evaluating fairness in large \nlanguage model recommendation, arXiv preprint arXiv:2305.07609 (2023).\n 47. Y ang, F ., Chen, Z., Jiang, Z., Cho, E., Huang, X. & Lu, Y . PALR: Personalization aware LLMs for recommendation, arXiv preprint \narXiv:2305.07622 (2023).\n 48. @inproceedingsgrohe2020word2vec, author = Grohe, Martin, title = word2vec, node2vec, graph2vec, x2vec: Towards a theory \nof vector embeddings of structured data, booktitle = Proceedings of the 39th ACM SIGMOD-SIGACT-SIGAI Symposium on \nPrinciples of Database Systems, year = 2020, month = jun, pages = 1–16,\n 49. Author(s). Title of the Paper, Accepted by SIGIR, arXiv preprint arXiv:2002.02126 [cs.IR], Available:  h t t p s : / / d o i . o r g / 1 0 . 4 8 5 5 0 / a r X i \nv . 2 0 0 2 . 0 2 1 2 6     (2020).\n 50. Wang, Q. et al. DualGNN: Dual graph neural network for multimedia recommendation. IEEE Trans. Multimed. 25, 1074–1084 \n(2021).\n 51. Li, J., Li, D., Savarese, S. & Hoi, S. BLIP-2: Bootstrapping language-image pre-training with frozen image encoders and large \nlanguage models. arXiv preprint arXiv:2301.12597 [cs.CV], https://doi.org/10.48550/arXiv.2301.12597 (2023).\n 52. Radford, A., et al.. Learning transferable visual models from natural language supervision, arXiv preprint arXiv:2103.00020 [cs.\nCV], https://doi.org/10.48550/arXiv.2103.00020 (2023).\n 53. Yuan, G. et al. Tenrec: A large-scale multipurpose benchmark dataset for recommender systems. Adv. Neural. Inf. Process. Syst. 35, \n11480–11493 (2022).\nAuthor contributions\nXiaochen Xiao(Member, IEEE). In 2023, in Sydney, Australia, he studied for a postgraduate degree in data sci -\nence and innovation at the University of Technology Sydney. His research interests include reinforcement learn-\ning and multi-modal deep learning.\nAdditional information\nSupplementary Information The online version contains supplementary material available at  h t t p s : / / d o i . o r g / 1 \n0 . 1 0 3 8 / s 4 1 5 9 8 - 0 2 5 - 9 6 4 5 8 - w     .  \nCorrespondence and requests for materials should be addressed to X.X.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives \n4.0 International License, which permits any non-commercial use, sharing, distribution and reproduction in \nany medium or format, as long as you give appropriate credit to the original author(s) and the source, provide \na link to the Creative Commons licence, and indicate if you modified the licensed material. Y ou do not have \npermission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence \nand your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to \nobtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p : / / c r e a t i v e c o m m o \nn s . o r g / l i c e n s e s / b y - n c - n d / 4 . 0 /     .  \n© The Author(s) 2025 \nScientific Reports |        (2025) 15:12062 25| https://doi.org/10.1038/s41598-025-96458-w\nwww.nature.com/scientificreports/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.665604829788208
    },
    {
      "name": "Modal",
      "score": 0.5553219318389893
    },
    {
      "name": "World Wide Web",
      "score": 0.42536628246307373
    },
    {
      "name": "Natural language processing",
      "score": 0.35765010118484497
    },
    {
      "name": "Data science",
      "score": 0.32467061281204224
    },
    {
      "name": "Chemistry",
      "score": 0.07495629787445068
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I3132238960",
      "name": "Institute of Electrical and Electronics Engineers",
      "country": "US"
    }
  ],
  "cited_by": 2
}