{
  "title": "TENER: Adapting Transformer Encoder for Named Entity Recognition",
  "url": "https://openalex.org/W2983180560",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2148872945",
      "name": "Yan Hang",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Deng, Bocao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1748266280",
      "name": "Li, Xiaonan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2378878701",
      "name": "Qiu, Xipeng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2144578941",
    "https://openalex.org/W2962904552",
    "https://openalex.org/W2962906565",
    "https://openalex.org/W2970323499",
    "https://openalex.org/W2965690110",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2904442979",
    "https://openalex.org/W2964054038",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W1940872118",
    "https://openalex.org/W2930726706",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2962941447",
    "https://openalex.org/W2606333299",
    "https://openalex.org/W2963625095",
    "https://openalex.org/W2296283641",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2158899491",
    "https://openalex.org/W2251435463",
    "https://openalex.org/W2963172229",
    "https://openalex.org/W2250709962",
    "https://openalex.org/W2964035777",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W2963563735",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2613709840",
    "https://openalex.org/W2252066972",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2147880316",
    "https://openalex.org/W2229639163",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2886790750",
    "https://openalex.org/W2740462959",
    "https://openalex.org/W2962676330",
    "https://openalex.org/W2886923263",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W2807583563",
    "https://openalex.org/W2962902328"
  ],
  "abstract": "The Bidirectional long short-term memory networks (BiLSTM) have been widely used as an encoder in models solving the named entity recognition (NER) task. Recently, the Transformer is broadly adopted in various Natural Language Processing (NLP) tasks owing to its parallelism and advantageous performance. Nevertheless, the performance of the Transformer in NER is not as good as it is in other NLP tasks. In this paper, we propose TENER, a NER architecture adopting adapted Transformer Encoder to model the character-level features and word-level features. By incorporating the direction and relative distance aware attention and the un-scaled attention, we prove the Transformer-like encoder is just as effective for NER as other NLP tasks.",
  "full_text": "TENER: Adapting Transformer Encoder for Named Entity Recognition\nHang Yan, Bocao Deng, Xiaonan Li, Xipeng Qiu∗\nSchool of Computer Science, Fudan University\nShanghai Key Laboratory of Intelligent Information Processing, Fudan University\n{hyan19,xpqiu}@fudan.edu.cn, {dengbocao,lixiaonan1208}@gmail.com\nAbstract\nBidirectional long short-term memory net-\nworks (BiLSTMs) have been widely used\nas an encoder for named entity recognition\n(NER) task. Recently, the fully-connected\nself-attention architecture (aka Transformer) is\nbroadly adopted in various natural language\nprocessing (NLP) tasks owing to its paral-\nlelism and advantage in modeling the long-\nrange context. Nevertheless, the performance\nof the vanilla Transformer in NER is not as\ngood as it is in other NLP tasks. In this\npaper, we propose TENER, a NER architec-\nture adopting adapted Transformer Encoder to\nmodel the character-level features and word-\nlevel features. By incorporating the direction-\naware, distance-aware and un-scaled atten-\ntion, we prove the Transformer-like encoder is\njust as effective for NER as other NLP tasks.\nExperiments on six NER datasets show that\nTENER achieves superior performance than\nthe prevailing BiLSTM-based models.\n1 Introduction\nThe named entity recognition (NER) is the task of\nﬁnding the start and end of an entity in a sentence\nand assigning a class for this entity. NER has been\nwidely studied in the ﬁeld of natural language pro-\ncessing (NLP) because of its potential assistance\nin question generation (Zhou et al., 2017), relation\nextraction (Miwa and Bansal, 2016), and corefer-\nence resolution (Fragkou, 2017). Since (Collobert\net al., 2011), various neural models have been in-\ntroduced to avoid hand-crafted features (Huang\net al., 2015; Ma and Hovy, 2016; Lample et al.,\n2016).\nNER is usually viewed as a sequence labeling\ntask, the neural models usually contain three com-\nponents: word embedding layer, context encoder\nlayer, and decoder layer (Huang et al., 2015; Ma\n∗Corresponding author.\nand Hovy, 2016; Lample et al., 2016; Chiu and\nNichols, 2016; Chen et al., 2019; Zhang et al.,\n2018; Gui et al., 2019b). The difference between\nvarious NER models mainly lies in the variance in\nthese components.\nRecurrent Neural Networks (RNNs) are widely\nemployed in NLP tasks due to its sequential char-\nacteristic, which is aligned well with language.\nSpeciﬁcally, bidirectional long short-term mem-\nory networks (BiLSTM) (Hochreiter and Schmid-\nhuber, 1997) is one of the most widely used RNN\nstructures. (Huang et al., 2015) was the ﬁrst one\nto apply the BiLSTM and Conditional Random\nFields (CRF) (Lafferty et al., 2001) to sequence\nlabeling tasks. Owing to BiLSTM’s high power to\nlearn the contextual representation of words, it has\nbeen adopted by the majority of NER models as\nthe encoder (Ma and Hovy, 2016; Lample et al.,\n2016; Zhang et al., 2018; Gui et al., 2019b).\nRecently, Transformer (Vaswani et al., 2017)\nbegan to prevail in various NLP tasks, like ma-\nchine translation (Vaswani et al., 2017), language\nmodeling (Radford et al., 2018), and pretraining\nmodels (Devlin et al., 2018). The Transformer\nencoder adopts a fully-connected self-attention\nstructure to model the long-range context, which\nis the weakness of RNNs. Moreover, Transformer\nhas better parallelism ability than RNNs. How-\never, in the NER task, Transformer encoder has\nbeen reported to perform poorly (Guo et al., 2019),\nour experiments also conﬁrm this result. There-\nfore, it is intriguing to explore the reason why\nTransformer does not work well in NER task.\nIn this paper, we analyze the properties of\nTransformer and propose two speciﬁc improve-\nments for NER.\nThe ﬁrst is that the sinusoidal position embed-\nding used in the vanilla Transformer is aware of\ndistance but unaware of the directionality. In ad-\ndition, this property will lose when used in the\narXiv:1911.04474v3  [cs.CL]  10 Dec 2019\nLouis Vuitton founded Louis Vuitton Inc. in 1854 .\nPER PER ORG ORG ORG TIME\nFigure 1: An example for NER. The relative direction\nis important in the NER task, because words before\n“Inc.” are mostly to be an organization, words after “in”\nare more likely to be time or location. Besides, the dis-\ntance between words is also important, since only con-\ntinuous words can form an entity, the former “Louis\nVuitton” can not form an entity with the “Inc.”.\nvanilla Transformer. However, both the direction\nand distance information are important in the NER\ntask. For example in Fig 1, words after “in” are\nmore likely to be a location or time than words be-\nfore it, and words before “Inc.” are mostly likely\nto be of the entity type “ORG”. Besides, an en-\ntity is a continuous span of words. Therefore, the\nawareness of distance might help the word bet-\nter recognizes its neighbor. To endow the Trans-\nformer with the ability of direction- and distance-\nawareness, we adopt the relative positional encod-\ning (Shaw et al., 2018; Huang et al., 2019; Dai\net al., 2019). instead of the absolute position en-\ncoding. We propose a revised relative positional\nencoding that uses fewer parameters and performs\nbetter.\nThe second is an empirical ﬁnding. The at-\ntention distribution of the vanilla Transformer is\nscaled and smooth. But for NER, a sparse atten-\ntion is suitable since not all words are necessary\nto be attended. Given a current word, a few con-\ntextual words are enough to judge its label. The\nsmooth attention could include some noisy infor-\nmation. Therefore, we abandon the scale factor of\ndot-production attention and use an un-scaled and\nsharp attention.\nWith the above improvements, we can greatly\nboost the performance of Transformer encoder for\nNER.\nOther than only using Transformer to model\nthe word-level context, we also tried to apply it\nas a character encoder to model word represen-\ntation with character-level information. The pre-\nvious work has proved that character encoder is\nnecessary to capture the character-level features\nand alleviate the out-of-vocabulary (OOV) prob-\nlem (Lample et al., 2016; Ma and Hovy, 2016;\nChiu and Nichols, 2016; Xin et al., 2018). In NER,\nCNN is commonly used as the character encoder.\nHowever, we argue that CNN is also not perfect\nfor representing character-level information, be-\ncause the receptive ﬁeld of CNN is limited, and the\nkernel size of the CNN character encoder is usu-\nally 3, which means it cannot correctly recognize\n2-gram or 4-gram patterns. Although we can de-\nliberately design different kernels, CNN still can-\nnot solve patterns with discontinuous characters,\nsuch as “un..ily” in “unhappily” and “unnecessar-\nily”. Instead, the Transformer-based character en-\ncoder shall not only fully make use of the concur-\nrence power of GPUs, but also have the potential-\nity to recognize different n-grams and even discon-\ntinuous patterns. Therefore, in this paper, we also\ntry to use Transformer as the character encoder,\nand we compare four kinds of character encoders.\nIn summary, to improve the performance of the\nTransformer-based model in the NER task, we ex-\nplicitly utilize the directional relative positional\nencoding, reduce the number of parameters and\nsharp the attention distribution. After the adap-\ntation, the performance raises a lot, making our\nmodel even performs better than BiLSTM based\nmodels. Furthermore, in the six NER datasets, we\nachieve state-of-the-art performance among mod-\nels without considering the pre-trained language\nmodels or designed features.\n2 Related Work\n2.1 Neural Architecture for NER\nCollobert et al. (2011) utilized the Multi-Layer\nPerceptron (MLP) and CNN to avoid using task-\nspeciﬁc features to tackle different sequence la-\nbeling tasks, such as Chunking, Part-of-Speech\n(POS) and NER. In (Huang et al., 2015),\nBiLSTM-CRF was introduced to solve sequence\nlabeling questions. Since then, the BiLSTM has\nbeen extensively used in the ﬁeld of NER (Chiu\nand Nichols, 2016; Dong et al., 2016; Yang et al.,\n2018; Ma and Hovy, 2016).\nDespite BiLSTM’s great success in the NER\ntask, it has to compute token representations one\nby one, which massively hinders full exploita-\ntion of GPU’s parallelism. Therefore, CNN has\nbeen proposed by (Strubell et al., 2017; Gui et al.,\n2019a) to encode words concurrently. In order\nto enlarge the receptive ﬁeld of CNNs, (Strubell\net al., 2017) used iterative dilated CNNs (ID-\nCNN).\nSince the word shape information, such as\nthe capitalization and n-gram, is important in\nrecognizing named entities, CNN and BiLSTM\nhave been used to extract character-level informa-\ntion (Chiu and Nichols, 2016; Lample et al., 2016;\nMa and Hovy, 2016; Strubell et al., 2017; Chen\net al., 2019).\nAlmost all neural-based NER models used pre-\ntrained word embeddings, like Word2vec and\nGlove (Pennington et al., 2014; Mikolov et al.,\n2013). And when contextual word embeddings are\ncombined, the performance of NER models will\nboost a lot (Peters et al., 2017, 2018; Akbik et al.,\n2018). ELMo introduced by (Peters et al., 2018)\nused the CNN character encoder and BiLSTM lan-\nguage models to get contextualized word represen-\ntations. Except for the BiLSTM based pre-trained\nmodels, BERT was based on Transformer (Devlin\net al., 2018).\n2.2 Transformer\nTransformer was introduced by (Vaswani et al.,\n2017), which was mainly based on self-attention.\nIt achieved great success in various NLP tasks.\nSince the self-attention mechanism used in the\nTransformer is unaware of positions, to avoid\nthis shortage, position embeddings were used\n(Vaswani et al., 2017; Devlin et al., 2018). In-\nstead of using the sinusoidal position embedding\n(Vaswani et al., 2017) and learned absolute posi-\ntion embedding, Shaw et al. (2018) argued that\nthe distance between two tokens should be consid-\nered when calculating their attention score. Huang\net al. (2019) reduced the computation complex-\nity of relative positional encoding from O(l2d) to\nO(ld), where l is the length of sequences and d is\nthe hidden size. Dai et al. (2019) derived a new\nform of relative positional encodings, so that the\nrelative relation could be better considered.\n2.2.1 Transformer Encoder Architecture\nWe ﬁrst introduce the Transformer encoder pro-\nposed in (Vaswani et al., 2017). The Transformer\nencoder takes in an matrix H ∈ Rl×d, where l\nis the sequence length, d is the input dimension.\nThen three learnable matrix Wq, Wk, Wv are used\nto project H into different spaces. Usually, the ma-\ntrix size of the three matrix are all Rd×dk, where\ndk is a hyper-parameter. After that, the scaled dot-\nproduct attention can be calculated by the follow-\ning equations,\nQ, K, V= HWq, HWk, HWv, (1)\nAt,j = QtKT\nj , (2)\nAttn(K, Q, V) = softmax( A√dk\n)V, (3)\nwhere Qt is the query vector of the tth token, j is\nthe token the tth token attends. Kj is the key vec-\ntor representation of the jth token. The softmax\nis along the last dimension. Instead of using one\ngroup of Wq, Wk, Wv, using several groups will\nenhance the ability of self-attention. When sev-\neral groups are used, it is called multi-head self-\nattention, the calculation can be formulated as fol-\nlows,\nQ(h), K(h), V(h) = HW (h)\nq , HW(h)\nk , HW(h)\nv , (4)\nhead(h) = Attn(Q(h), K(h), V(h)), (5)\nMultiHead(H) = [head(1); ...; head(n)]WO, (6)\nwhere n is the number of heads, the superscript h\nrepresents the head index. [head(1); ...; head(n)]\nmeans concatenation in the last dimension. Usu-\nally dk ×n = d, which means the output of\n[head(1); ...; head(n)] will be of size Rl×d. Wo is\na learnable parameter, which is of size Rd×d.\nThe output of the multi-head attention will\nbe further processed by the position-wise feed-\nforward networks, which can be represented as\nfollows,\nFFN(x) =max(0, xW1 + b1)W2 + b2, (7)\nwhere W1, W2, b1, b2 are learnable parameters,\nand W1 ∈ Rd×dff , W2 ∈ Rdff ×d, b1 ∈ Rdff ,\nb2 ∈Rd. dff is a hyper-parameter. Other compo-\nnents of the Transformer encoder includes layer\nnormalization and Residual connection, we use\nthem the same as (Vaswani et al., 2017).\n2.2.2 Position Embedding\nThe self-attention is not aware of the positions of\ndifferent tokens, making it unable to capture the\nsequential characteristic of languages. In order\nto solve this problem, (Vaswani et al., 2017) sug-\ngested to use position embeddings generated by\nsinusoids of varying frequency. The tth token’s\nposition embedding can be represented by the fol-\nlowing equations\nPEt,2i = sin(t/100002i/d), (8)\nPEt,2i+1 = cos(t/100002i/d), (9)\nwhere i is in the range of [0, d\n2 ], d is the input di-\nmension. This sinusoid based position embedding\nmakes Transformer have an ability to model the\nposition of a token and the distance of each two to-\nkens. For any ﬁxed offset k, PEt+k can be repre-\nsented by a linear transformation ofPEt (Vaswani\net al., 2017).\nN x\nO I-LOCOO\nCharacter EmbeddingWord EmbeddingExtracted Character Features\nEmbedding Module\nTransformer\nCRF\n…\n…\n…B-LOC\nSelf Attention\n… …\n…\nMulti Head\n……\nFigure 2: Model structure of TENER for English NER\ntasks. In TENER, Transformer encoder is used not only\nto extract the word-level contextual information, but\nalso to encode character-level information in a word.\n3 Proposed Model\nIn this paper, we utilize the Transformer encoder\nto model the long-range and complicated interac-\ntions of sentence for NER. The structure of pro-\nposed model is shown in Fig 2. We detail each\nparts in the following sections.\n3.1 Embedding Layer\nTo alleviate the problems of data sparsity and out-\nof-vocabulary (OOV), most NER models adopted\nthe CNN character encoder (Ma and Hovy, 2016;\nYe and Ling, 2018; Chen et al., 2019) to represent\nwords. Compared to BiLSTM based character en-\ncoder (Lample et al., 2016; Ghaddar and Langlais,\n2018), CNN is more efﬁcient. Since Transformer\ncan also fully exploit the GPU’s parallelism, it is\ninteresting to use Transformer as the character en-\ncoder. A potential beneﬁt of Transformer-based\ncharacter encoder is to extract different n-grams\nand even uncontinuous character patterns, like\n“un..ily” in “unhappily” and “uneasily”. For the\nmodel’s uniformity, we use the “adapted Trans-\nformer” to represent the Transformer introduced\nin next subsection.\nThe ﬁnal word embedding is the concatenation\nof the character features extracted by the character\nencoder and the pre-trained word embeddings.\n3.2 Encoding Layer with Adapted\nTransformer\nAlthough Transformer encoder has potential ad-\nvantage in modeling long-range context, it is not\nworking well for NER task. In this paper, we pro-\npose an adapted Transformer for NER task with\ntwo improvements.\n3.2.1 Direction- and Distance-Aware\nAttention\nInspired by the success of BiLSTM in NER tasks,\nwe consider what properties the Transformer lacks\ncompared to BiLSTM-based models. One obser-\nvation is that BiLSTM can discriminatively collect\nthe context information of a token from its left and\nright sides. But it is not easy for the Transformer\nto distinguish which side the context information\ncomes from.\nAlthough the dot product between two sinu-\nsoidal position embeddings is able to reﬂect their\ndistance, it lacks directionality and this property\nwill be broken by the vanilla Transformer atten-\ntion. To illustrate this, we ﬁrst prove two proper-\nties of the sinusoidal position embeddings.\nProperty 1. For an offset k and a position t,\nPE T\nt+kPEt only depends on k, which means the\ndot product of two sinusoidal position embeddings\ncan reﬂect the distance between two tokens.\nProof. Based on the deﬁnitions of Eq.(8) and\nEq.(9), the position embedding of t-th token is\nPEt =\n\n\nsin(c0t)\ncos(c0t)\n...\nsin(cd\n2 −1t)\ncos(cd\n2 −1t)\n\n\n, (10)\nwhere d is the dimension of the position embed-\nding, ci is a constant decided by i, and its value is\n1/100002i/d.\nTherefore,\nPE T\nt PEt+k =\nd\n2 −1∑\nj=0\n[sin(cjt) sin(cj(t + k))\n+ cos(cjt) cos(cj(t + k))] (11)\n=\nd\n2 −1∑\nj=0\ncos(cj(t −(t + k))) (12)\n=\nd\n2 −1∑\nj=0\ncos(cjk), (13)\nwhere Eq.(11) to Eq.(12) is based on the equation\ncos(x −y) = sin(x) sin(y) + cos(x) cos(y).\nFigure 3: Dot product between two sinusoidal posi-\ntion embeddings whose distance is k. It is clear that\nthe product is symmetrical, and with the increment of\n|k|, it has a trend to decrease, but this decrease is not\nmonotonous.\nFigure 4: The upper line is the product between\nPE T\nt PEt+k. The lower two lines are the prod-\nucts of PE T\nt WPE t+k with two random Ws. Al-\nthough PE T\nt PEt+k can reﬂect the distance, the\nPE T\nt WPE t+k has no clear pattern.\nProperty 2. For an offset k and a position t,\nPE T\nt PEt−k = PE T\nt PEt+k, which means the si-\nnusoidal position embeddings is unware of direc-\ntionality.\nProof. Let j = t −k, according to property 1, we\nhave\nPE T\nt PEt+k = PE T\nj PEj+k (14)\n= PE T\nt−kPEt. (15)\nThe relation between d, k and PE T\nt PEt+k is\ndisplayed in Fig 3. The sinusoidal position embed-\ndings are distance-aware but lacks directionality.\nHowever, the property of distance-awareness\nalso disappears when PEt is projected into the\nquery and key space of self-attention. Since in\nvanilla Transformer the calculation between PEt\nand PEt+k is actually PE T\nt WT\nq WkPEt+k, where\nWq, Wk are parameters in Eq.(1). Mathematically,\nit can be viewed as PE T\nt WPE t+k with only one\nparameter W. The relation between PE T\nt PEt+k\nand PE T\nt WPE t+k is depicted in Fig 4.\nTherefore, to improve the Transformer with\ndirection- and distance-aware characteristic, we\ncalculate the attention scores using the equations\nbelow:\nQ, K, V= HWq, Hdk , HWv, (16)\nRt−j = [. . .sin( t −j\n100002i/dk\n) cos( t −j\n100002i/dk\n) . . .]T ,\n(17)\nArel\nt,j = QtKT\nj + QtRT\nt−j + uKT\nj + vRT\nt−j, (18)\nAttn(Q, K, V) = softmax(Arel)V, (19)\nwhere t is index of the target token, j is the in-\ndex of the context token, Qt, Kj is the query\nvector and key vector of token t, jrespectively,\nWq, Wv ∈ Rd×dk. To get Hdk ∈ Rl×dk, we\nﬁrst split H into d/dk partitions in the second\ndimension, then for each head we use one parti-\ntion. u ∈Rdk, v ∈Rdk are learnable parame-\nters, Rt−j is the relative positional encoding, and\nRt−j ∈Rdk, i in Eq.(17) is in the range [0, dk\n2 ].\nQT\nt Kj in Eq.(18) is the attention score between\ntwo tokens; QT\nt Rt−j is the tth token’s bias on cer-\ntain relative distance; uT Kj is the bias on the jth\ntoken; vT Rt−j is the bias term for certain distance\nand direction.\nBased on Eq.(17), we have\nRt, R−t =\n\n\nsin(c0t)\ncos(c0t)\n...\nsin(cd\n2 −1t)\ncos(cd\n2 −1t)\n\n\n,\n\n\n−sin(c0t)\ncos(c0t)\n...\n−sin(cd\n2 −1t)\ncos(cd\n2 −1t)\n\n\n, (20)\nbecause sin(−x) =−sin(x), cos(x) = cos(−x).\nThis means for an offset t, the forward and back-\nward relative positional encoding are the same\nwith respect to the cos(cit) terms, but is the oppo-\nsite with respect to the sin(cit) terms. Therefore,\nby using Rt−j, the attention score can distinguish\ndifferent directions and distances.\nThe above improvement is based on the\nwork (Shaw et al., 2018; Dai et al., 2019). Since\nthe size of NER datasets is usually small, we avoid\ndirect multiplication of two learnable parameters,\nbecause they can be represented by one learnable\nparameter. Therefore we do not useWk in Eq.(16).\nThe multi-head version is the same as Eq.(6), but\nwe discard Wo since it is directly multiplied byW1\nin Eq.(7).\nTable 1: Details of Datasets.\nDataset Type Train Dev Test\nEnglish\nCoNLL2003 Sentence 14.0k 3.2k 3.5k\nToken 203.6k 51.4k 46.4k\nOntoNotes 5.0 Sentence 59.9k 8.5k 8.3k\nToken 1088.5k 147.7k 152.7k\nChinese\nOntoNotes 4.0 Sentence 15.7k 4.3k 4.3k\nToken 491.9k 200.5k 208.1k\nMSRA Sentence 46.4k 4.4k 4.4k\nToken 2169.9k 172.6k 172.6k\nWeibo Sentence 1.4k 0.3k 0.3k\nToken 73.5k 14.4k 14.8k\nResume Sentence 3.8k 0.5k 0.5k\nToken 124.1k 13.9k 15.1k\n3.2.2 Un-scaled Dot-Product Attention\nThe vanilla Transformer use the scaled dot-\nproduct attention to smooth the output of soft-\nmax function. In Eq.(3), the dot product of key\nand value matrices is divided by the scaling factor√dk.\nWe empirically found that models perform bet-\nter without the scaling factor √dk. We presume\nthis is because without the scaling factor the at-\ntention will be sharper. And the sharper attention\nmight be beneﬁcial in the NER task since only few\nwords in the sentence are named entities.\n3.3 CRF Layer\nIn order to take advantage of dependency be-\ntween different tags, the Conditional Random\nField (CRF) was used in all of our models. Given\na sequence s = [s1, s2, ..., sT ], the corresponding\ngolden label sequence is y = [y1, y2, ..., yT ], and\nY(s) represents all valid label sequences. The\nprobability of y is calculated by the following\nequation\nP(y|s) =\n∑T\nt=1 ef(yt−1,yt,s)\n∑Y(s)\ny′\n∑T\nt=1 ef(y′\nt−1,y′\nt,s) , (21)\nwhere f(yt−1, yt, s) computes the transition score\nfrom yt−1 to yt and the score for yt. The opti-\nmization target is to maximize P(y|s). When de-\ncoding, the Viterbi Algorithm is used to ﬁnd the\npath achieves the maximum probability.\n4 Experiment\n4.1 Data\nWe evaluate our model in two English NER\ndatasets and four Chinese NER datasets.\n(1) CoNLL2003 is one of the most evaluated\nEnglish NER datasets, which contains four differ-\nent named entities: PERSON, LOCATION, OR-\nGANIZATION, and MISC (Sang and Meulder,\n2003).\n(2) OntoNotes 5.0 is an English NER dataset\nwhose corpus comes from different domains, such\nas telephone conversation, newswire. We ex-\nclude the New Testaments portion since there is\nno named entity in it (Chen et al., 2019; Chiu\nand Nichols, 2016). This dataset has eleven entity\nnames and seven value types, like CARDINAL,\nMONEY , LOC.\n(3) Weischedel (2011) released OntoNotes 4.0.\nIn this paper, we use the Chinese part. We adopted\nthe same pre-process as (Che et al., 2013).\n(4) The corpus of the Chinese NER dataset\nMSRA came from news domain (Levow, 2006).\n(5) Weibo NER was built based on text in Chi-\nnese social media Sina Weibo (Peng and Dredze,\n2015), and it contained 4 kinds of entities.\n(6) Resume NER was annotated by (Zhang and\nYang, 2018).\nTheir statistics are listed in Table 1. For all\ndatasets, we replace all digits with “0”, and use\nthe BIOES tag schema. For English, we use the\nGlove 100d pre-trained embedding (Pennington\net al., 2014). For the character encoder, we use 30d\nrandomly initialized character embeddings. More\ndetails on models’ hyper-parameters can be found\nin the supplementary material. For Chinese, we\nused the character embedding and bigram embed-\nding released by (Zhang and Yang, 2018). All pre-\ntrained embeddings are ﬁnetuned during training.\nIn order to reduce the impact of randomness, we\nran all of our experiments at least three times, and\nits average F1 score and standard deviation are re-\nported.\nWe used random-search to ﬁnd the opti-\nmal hyper-parameters, hyper-parameters and their\nranges are displayed in the supplemental material.\nWe use SGD and 0.9 momentum to optimize the\nmodel. We run 100 epochs and each batch has 16\nsamples. During the optimization, we use the tri-\nangle learning rate (Smith, 2017) where the learn-\ning rate rises to the pre-set learning rate at the\nﬁrst 1% steps and decreases to 0 in the left 99%\nsteps. The model achieves the highest develop-\nment performance was used to evaluate the test\nset. The hyper-parameter search range and other\nsettings can be found in the supplementary mate-\nrial. Codes are available at https://github.\ncom/fastnlp/TENER.\nModels Weibo Resume OntoNotes4.0 MSRA\nBiLSTM ♣ 56.75 94.41 71.81 91.87\nID-CNN ♠ - 93.75 62.25 -\nCAN-NER∗(Zhu and Wang, 2019) 59.31 94.94 73.64 92.97\nTransformer 46.38 ±0.78 93.43 ±0.26 66.49 ±0.30 88.35 ±0.60\nTENER(Ours) 58.17 ±0.22 95.00 ±0.25 72.43 ±0.39 92.74 ±0.27\nw/ scale 57.40 ±0.3 94.00 ±0.51 71.72 ±0.08 91.67 ±0.23\nTable 2: The F1 scores on Chinese NER datasets. ♣,♠are results reported in (Zhang and Yang, 2018) and (Gui\net al., 2019a), respectively. “w/ scale” means TENER using the scaled attention in Eq.(19). ∗their results are not\ndirectly comparable with ours, since they used 100d pre-trained character and bigram embeddings. Other models\nuse the same embeddings.\n4.2 Results on Chinese NER Datasets\nWe ﬁrst present our results in the four Chinese\nNER datasets. Since Chinese NER is directly\nbased on the characters, it is more straightforward\nto show the abilities of different models without\nconsidering the inﬂuence of word representation.\nAs shown in Table 2, the vanilla Transformer\ndoes not perform well and is worse than the BiL-\nSTM and CNN based models. However, when\nrelative positional encoding combined, the perfor-\nmance was enhanced greatly, resulting in better re-\nsults than the BiLSTM and CNN in all datasets.\nThe number of training examples of the Weibo\ndataset is tiny, therefore the performance of the\nTransformer is abysmal, which is as expected\nsince the Transformer is data-hungry. Neverthe-\nless, when enhanced with the relative positional\nencoding and unscaled attention, it can achieve\neven better performance than the BiLSTM-based\nmodel. The superior performance of the adapted\nTransformer in four datasets ranging from small\ndatasets to big datasets depicts that the adapted\nTransformer is more robust to the number of train-\ning examples than the vanilla Transformer. As the\nlast line of Table 2 depicts, the scaled attention will\ndeteriorate the performance.\n4.3 Results on English NER datasets\nThe comparison between different NER models\non English NER datasets is shown in Table 3.\nThe poor performance of the Transformer in the\nNER datasets was also reported by (Guo et al.,\n2019). Although performance of the Transformer\nis higher than (Guo et al., 2019), it still lags be-\nhind the BiLSTM-based models (Ma and Hovy,\n2016). Nonetheless, the performance is massively\nenhanced by incorporating the relative positional\nencoding and unscaled attention into the Trans-\nformer. The adaptation not only makes the Trans-\nformer achieve superior performance than BiL-\nModels CoNLL2003 OntoNotes 5.0\nBiLSTM-CRF (Huang et al.,\n2015)\n88.83\nCNN-BiLSTM-CRF (Chiu\nand Nichols, 2016)\n90.91 ±0.20 86.12 ±0.22\nBiLSTM-BiLSTM-CRF\n(Lample et al., 2016)\n90.94\nCNN-BiLSTM-CRF (Ma and\nHovy, 2016)\n91.21\nID-CNN (Strubell et al.,\n2017)\n90.54 ±0.18 86.84 ±0.19\nLM-LSTM-CRF (Liu et al.,\n2018)\n91.24 ±0.12\nCRF+HSCRF (Ye and Ling,\n2018)\n91.26 ±0.1\nBiLSTM-BiLSTM-CRF\n(Akhundov et al., 2018)\n91.11\nLS+BiLSTM-CRF (Ghaddar\nand Langlais, 2018)\n90.52 ±0.20 86.57 ±0.1\nCN3 (Liu et al., 2019) 91.1\nGRN (Chen et al., 2019) 91.44 ±0.16 87.67 ±0.17\nTransformer 89.57 ±0.12 86.73 ±0.07\nTENER (Ours) 91.33 ±0.05 88.43 ±0.12\nw/ scale 91.06 ±0.09 87.94 ±0.1\nw/ CNN-char 91.45 ±0.07 88.25 ±0.11\nTable 3: The F1 scores on English NER datasets. We\nonly list results based on non-contextualized embed-\ndings, and methods utilized pre-trained language mod-\nels, pre-trained features, or higher dimension word vec-\ntors are excluded. TENER (Ours) uses the Trans-\nformer encoder both in the character-level and word-\nlevel. “w/ scale” means TENER using the scaled atten-\ntion in Eq.(19). “w/ CNN-char” means TENER using\nCNN as character encoder instead of AdaTrans.\nSTM based models, but also unveil the new state-\nof-the-art performance in two NER datasets when\nonly the Glove 100d embedding and CNN char-\nacter embedding are used. The same deteriora-\ntion of performance was observed when using the\nscaled attention. Besides, if ELMo was used (Pe-\nters et al., 2018), the performance of TENER can\nbe further boosted as depicted in Table 4.\nModels CoNLL2003 OntoNotes 5.0\nBiLSTM 92.55 ±0.10 88.88 ±0.16\nGRN (Chen et al., 2019) 92.34 ±0.1 -\nTENER (Ours) 92.62±0.09 89.78 ±0.15\nTable 4: Performance of models with ELMo as their\nembeddings in English NER datasets. “BiLSTM” is\nour run. In the larger OntoNotes5.0, TENER achieves\nmuch better F1 score.\nChar Word BiLSTM ID-CNN AdaTrans\nNo Char 88.34 ±0.32 87.30 ±0.15 88.37 ±0.27\nBiLSTM 91.32 ±0.13 89.99 ±0.14 91.29 ±0.12\nCNN 91.22 ±0.10 90.17 ±0.02 91.45 ±0.07\nTransformer 91.12 ±0.10 90.05 ±0.13 91.23 ±0.06\nAdaTrans 91.38 ±0.15 89.99 ±0.05 91.33 ±0.05\n(a) CoNLL2003\nChar Word BiLSTM ID-CNN AdaTrans\nNo Char 85.20 ±0.23 84.26 ±0.07 85.80 ±0.10\nBiLSTM 87.85 ±0.09 87.38 ±0.17 88.12 ±0.16\nCNN 87.79 ±0.14 87.10 ±0.06 88.25 ±0.11\nTransformer 88.01 ±0.06 87.31 ±0.10 88.20 ±0.07\nAdaTrans 88.12 ±0.17 87.51 ±0.11 88.43 ±0.12\n(b) OntoNotes 5.0\nTable 5: F1 scores in the CoNLL2003 and OntoNotes\n5.0. “Char” means character-level encoder, and\n“Word” means word-level encoder. “AdaTrans” means\nour adapted Transformer encoder.\n4.4 Analysis of Different Character Encoders\nThe character-level encoder has been widely used\nin the English NER task to alleviate the data spar-\nsity and OOV problem in word representation. In\nthis section, we cross different character-level en-\ncoders (BiLSTM, CNN, Transformer encoder and\nour adapted Transformer encoder (AdaTrans for\nshort) ) and different word-level encoders (BiL-\nSTM, ID-CNN and AdaTrans) to implement the\nNER task. Results on CoNLL2003 and OntoNotes\n5.0 are presented in Table 5a and Table 5b, respec-\ntively.\nThe ID-CNN encoder is from (Strubell et al.,\n2017), and we re-implement their model in Py-\nTorch. For different combinations, we use random\nsearch to ﬁnd its best hyper-parameters. Hyper-\nparameters for character encoders were ﬁxed. The\ndetails can be found in the supplementary mate-\nrial.\nFor the results on CoNLL2003 dataset which\nis depicted in Table 5a, the AdaTrans performs\nas good as the BiLSTM in different character en-\ncoder scenario averagely. In addition, from Ta-\nble 5b, we can ﬁnd the pattern that the AdaTrans\ncharacter encoder outpaces the BiLSTM and CNN\ncharacter encoders when different word-level en-\ncoders being used. Moreover, no matter what\ncharacter encoder being used or none being used,\nthe AdaTrans word-level encoder gets the best\nperformance. This implies that when the num-\nber of training examples increases, the AdaTrans\ncharacter-level and word-level encoder can better\nrealize their ability.\n4.5 Convergent Speed Comparison\nFigure 5: Convergent speed in the development dataset\nof OntoNotes 5.0 for four kinds of models.\nWe compare the convergent speed of BiLSTM,\nID-CNN, Transformer, and TENER in the devel-\nopment set of the OntoNotes 5.0. The curves are\nshown in Fig 5. TENER converges as fast as\nthe BiLSTM model and outperforms the vanilla\nTransformer.\n5 Conclusion\nIn this paper, we propose TENER, a model adopt-\ning Transformer Encoder with speciﬁc customiza-\ntions for the NER task. Transformer Encoder has a\npowerful ability to capture the long-range context.\nIn order to make the Transformer more suitable to\nthe NER task, we introduce the direction-aware,\ndistance-aware and un-scaled attention. Experi-\nments in two English NER tasks and four Chi-\nnese NER tasks show that the performance can\nbe massively increased. Under the same pre-\ntrained embeddings and external knowledge, our\nproposed modiﬁcation outperforms previous mod-\nels in the six datasets. Meanwhile, we also found\nthe adapted Transformer is suitable for being used\nas the English character encoder, because it has the\npotentiality to extract intricate patterns from char-\nacters. Experiments in two English NER datasets\nshow that the adapted Transformer character en-\ncoder performs better than BiLSTM and CNN\ncharacter encoders.\nReferences\nAlan Akbik, Duncan Blythe, and Roland V ollgraf.\n2018. Contextual string embeddings for sequence\nlabeling. In COLING, pages 1638–1649.\nAdnan Akhundov, Dietrich Trautmann, and Georg\nGroh. 2018. Sequence labeling: A practical ap-\nproach. CoRR, abs/1808.03926.\nWanxiang Che, Mengqiu Wang, Christopher D. Man-\nning, and Ting Liu. 2013. Named entity recognition\nwith bilingual constraints. In NAACL, pages 52–62.\nHui Chen, Zijia Lin, Guiguang Ding, Jianguang Lou,\nYusen Zhang, and Borje Karlsson. 2019. GRN:\nGated relation network to enhance convolutional\nneural network for named entity recognition. In\nAAAI.\nJason P. C. Chiu and Eric Nichols. 2016. Named en-\ntity recognition with bidirectional lstm-cnns. TACL,\n4:357–370.\nRonan Collobert, Jason Weston, L´eon Bottou, Michael\nKarlen, Koray Kavukcuoglu, and Pavel Kuksa.\n2011. Natural language processing (almost) from\nscratch. Journal of Machine Learning Research ,\n12(Aug):2493–2537.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Car-\nbonell, Quoc Viet Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language models\nbeyond a ﬁxed-length context. In ACL, pages 2978–\n2988.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. CoRR, abs/1810.04805.\nChuanhai Dong, Jiajun Zhang, Chengqing Zong,\nMasanori Hattori, and Hui Di. 2016. Character-\nbased LSTM-CRF with radical-level features for\nchinese named entity recognition. In NLPCC, pages\n239–250.\nPavlina Fragkou. 2017. Applying named entity recog-\nnition and co-reference resolution for segmenting\nenglish texts. Progress in Artiﬁcial Intelligence ,\n6(4):325–346.\nAbbas Ghaddar and Philippe Langlais. 2018. Ro-\nbust lexical features for improved neural net-\nwork named-entity recognition. arXiv preprint\narXiv:1806.03489.\nTao Gui, Ruotian Ma, Qi Zhang, Lujun Zhao, Yu-Gang\nJiang, and Xuanjing Huang. 2019a. Cnn-based chi-\nnese NER with lexicon rethinking. In IJCAI, pages\n4982–4988.\nTao Gui, Yicheng Zou, Qi Zhang, Minlong Peng, Jinlan\nFu, Zhongyu Wei, and Xuanjing Huang. 2019b. A\nlexicon-based graph neural network for chinese ner.\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,\nXiangyang Xue, and Zheng Zhang. 2019. Star-\ntransformer. In NAACL, pages 1315–1325.\nSepp Hochreiter and J ¨urgen Schmidhuber. 1997.\nLong short-term memory. Neural computation ,\n9(8):1735–1780.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob\nUszkoreit, Ian Simon, Curtis Hawthorne, Noam\nShazeer, Andrew M. Dai, Matthew D. Hoffman,\nMonica Dinculescu, and Douglas Eck. 2019. Mu-\nsic transformer: Generating music with long-term\nstructure. In ICLR.\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirec-\ntional lstm-crf models for sequence tagging. arXiv\npreprint arXiv:1508.01991.\nJohn D. Lafferty, Andrew McCallum, and Fernando\nC. N. Pereira. 2001. Conditional random ﬁelds:\nProbabilistic models for segmenting and labeling se-\nquence data. In Proceedings of the Eighteenth Inter-\nnational Conference on Machine Learning.\nGuillaume Lample, Miguel Ballesteros, Sandeep Sub-\nramanian, Kazuya Kawakami, and Chris Dyer. 2016.\nNeural architectures for named entity recognition.\nIn Proceedings of NAACL-HLT, pages 260–270.\nGina-Anne Levow. 2006. The third international chi-\nnese language processing bakeoff: Word segmen-\ntation and named entity recognition. In Proceed-\nings of the Fifth Workshop on Chinese Language\nProcessing, SIGHAN@COLING/ACL 2006, Sydney,\nAustralia, July 22-23, 2006, pages 108–117.\nLiyuan Liu, Jingbo Shang, Xiang Ren,\nFrank Fangzheng Xu, Huan Gui, Jian Peng,\nand Jiawei Han. 2018. Empower sequence labeling\nwith task-aware neural language model. In AAAI,\npages 5253–5260.\nPengfei Liu, Shuaichen Chang, Xuanjing Huang, Jian\nTang, and Jackie Chi Kit Cheung. 2019. Contextu-\nalized non-local neural networks for sequence learn-\ning. In AAAI, volume 33, pages 6762–6769.\nXuezhe Ma and Eduard Hovy. 2016. End-to-end se-\nquence labeling via bi-directional lstm-cnns-crf. In\nACL, volume 1, pages 1064–1074.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jef-\nfrey Dean. 2013. Efﬁcient estimation of word\nrepresentations in vector space. arXiv preprint\narXiv:1301.3781.\nMakoto Miwa and Mohit Bansal. 2016. End-to-end re-\nlation extraction using lstms on sequences and tree\nstructures. arXiv preprint arXiv:1601.00770.\nNanyun Peng and Mark Dredze. 2015. Named en-\ntity recognition for chinese social media with jointly\ntrained embeddings. In EMNLP, pages 548–554.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. Glove: Global vectors for word\nrepresentation. In EMNLP, pages 1532–1543.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In NACCL, volume 1, pages 2227–2237.\nMatthew E. Peters, Waleed Ammar, Chandra Bhaga-\nvatula, and Russell Power. 2017. Semi-supervised\nsequence tagging with bidirectional language mod-\nels. In ACL (1), pages 1756–1765. ACL.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nNAACL, pages 142–147.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In NAACL, pages 464–468.\nLeslie N. Smith. 2017. Cyclical learning rates for train-\ning neural networks. In 2017 IEEE Winter Con-\nference on Applications of Computer Vision, WACV\n2017, Santa Rosa, CA, USA, March 24-31, 2017 ,\npages 464–472.\nEmma Strubell, Patrick Verga, David Belanger, and\nAndrew McCallum. 2017. Fast and accurate entity\nrecognition with iterated dilated convolutions.arXiv\npreprint arXiv:1702.02098.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 5998–6008.\nRalph Weischedel. 2011. Ontonotes release 4.0\nLDC2011T03.\nYingwei Xin, Ethan Hart, Vibhuti Mahajan, and Jean-\nDavid Ruvini. 2018. Learning better internal struc-\nture of words for sequence labeling. In EMNLP,\npages 2584–2593.\nFan Yang, Jianhu Zhang, Gongshen Liu, Jie Zhou,\nCheng Zhou, and Huanrong Sun. 2018. Five-stroke\nbased cnn-birnn-crf network for chinese named en-\ntity recognition. In NLPCC, pages 184–195.\nEnglish\nnumber of layers [1, 2]\nhidden size [200, 400, 600, 800, 1200]\nlearning rate [0.01, 0.007, 0.005]\nfc dropout 0.4\nTable 6: The hyper-parameters and hyper-parameter\nsearch ranges for BiLSTM.\nZhi-Xiu Ye and Zhen-Hua Ling. 2018. Hybrid semi-\nmarkov CRF for neural sequence labeling. arXiv\npreprint arXiv:1805.03838.\nYue Zhang, Qi Liu, and Linfeng Song. 2018. Sentence-\nstate LSTM for text representation. In ACL (1) ,\npages 317–327. Association for Computational Lin-\nguistics.\nYue Zhang and Jie Yang. 2018. Chinese NER using\nlattice LSTM. In ACL, pages 1554–1564.\nQingyu Zhou, Nan Yang, Furu Wei, Chuanqi Tan,\nHangbo Bao, and Ming Zhou. 2017. Neural ques-\ntion generation from text: A preliminary study. In\nNLPCC, pages 662–671.\nYuying Zhu and Guoxin Wang. 2019. CAN-NER: con-\nvolutional attention network for chinese named en-\ntity recognition. In NAACL, pages 3384–3393.\n6 Supplemental Material\n6.1 Character Encoder\nWe exploit four kinds of character encoders. For\nall character encoders, the randomly initialized\ncharacter embeddings are 30d. The hidden size\nof BiLSTM used in the character encoder is 50d\nin each direction. The kernel size of CNN used\nin the character encoder is 3, and we used 30 ker-\nnels with stride 1. For Transformer and adapted\nTransformer, the number of heads is 3, and ev-\nery head is 10d, the dropout rate is 0.15, the feed-\nforward dimension is 60. The Transformer used\nthe sinusoid position embedding. The number of\nparameters for the character encoder (excluding\ncharacter embedding) when using BiLSTM, CNN,\nTransformer and adapted Transformer are 35830,\n3660, 8460 and 6600 respectively. For all experi-\nments, the hyper-parameters of character encoders\nstay unchanged.\n6.2 Hyper-parameters\nThe hyper-parameters and search ranges for differ-\nent encoders are presented in Table 6, Table 7 and\nTable 8.\nEnglish\nnumber of layers [2, 3, 4, 5, 6]\nnumber of kernels [200, 400, 600, 800]\nkernel size 3\nlearning rate [2e-3, 1.5e-3, 1e-3, 7e-4]\nfc dropout 0.4\nTable 7: The hyper-parameters and hyper-parameter\nsearch ranges for ID-CNN.\nChinese English\nnumber of layers [1, 2] [1, 2]\nnumber of head [4, 6, 8, 10] [8, 10, 12, 14]\nhead dimension [32, 48, 64, 80, 96] [64, 80, 96, 112, 128]\nlearning rate [1e-3, 5e-4, 7e-4] [9e-4, 7e-4, 5e-4]\ntransformer dropout 0.15 0.15\nfc dropout 0.4 0.4\nTable 8: The hyper-parameters and hyper-parameter\nsearch ranges for Transformer and adapted Trans-\nformer in Chinese and English NER datasets.",
  "topic": "Encoder",
  "concepts": [
    {
      "name": "Encoder",
      "score": 0.691728413105011
    },
    {
      "name": "Transformer",
      "score": 0.6518805027008057
    },
    {
      "name": "Computer science",
      "score": 0.5884231328964233
    },
    {
      "name": "Named-entity recognition",
      "score": 0.4528641402721405
    },
    {
      "name": "Natural language processing",
      "score": 0.3882039189338684
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3621242046356201
    },
    {
      "name": "Speech recognition",
      "score": 0.323152095079422
    },
    {
      "name": "Engineering",
      "score": 0.1311594545841217
    },
    {
      "name": "Electrical engineering",
      "score": 0.09306943416595459
    },
    {
      "name": "Operating system",
      "score": 0.05109092593193054
    },
    {
      "name": "Voltage",
      "score": 0.036905258893966675
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I24943067",
      "name": "Fudan University",
      "country": "CN"
    }
  ]
}