{
  "title": "Mind the Biases: Quantifying Cognitive Biases in Language Model Prompting",
  "url": "https://openalex.org/W4385571301",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5041215917",
      "name": "Ruixi Lin",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A5110081955",
      "name": "Hwee Tou Ng",
      "affiliations": [
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2148488766",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W4244135171",
    "https://openalex.org/W4287207937",
    "https://openalex.org/W4221142858",
    "https://openalex.org/W4206214875",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W4221045317",
    "https://openalex.org/W4246105142",
    "https://openalex.org/W2765742249",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W2147091291",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W3172943453",
    "https://openalex.org/W2989696285",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W1843246605",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3172642864",
    "https://openalex.org/W2962700793",
    "https://openalex.org/W2015769833",
    "https://openalex.org/W4226067440",
    "https://openalex.org/W2982756474",
    "https://openalex.org/W2548421507",
    "https://openalex.org/W3166846774",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4221153690",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W4287891033",
    "https://openalex.org/W2333886213",
    "https://openalex.org/W3198599617",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4225959163",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4292157289",
    "https://openalex.org/W3167602185",
    "https://openalex.org/W4281644150",
    "https://openalex.org/W2594475271",
    "https://openalex.org/W4205991051"
  ],
  "abstract": "We advocate the importance of exposing uncertainty on results of language model prompting which display bias modes resembling cognitive biases, and propose to help users grasp the level of uncertainty via simple quantifying metrics. Cognitive biases in the human decision making process can lead to flawed responses when we are under uncertainty. Not surprisingly, we have seen biases in language models resembling cognitive biases as a result of training on biased textual data, raising dangers in downstream tasks that are centered around people's lives if users trust their results too much. In this work, we reveal two bias modes leveraging cognitive biases when we prompt BERT, accompanied by two bias metrics. On a drug-drug interaction extraction task, our bias measurements reveal an error pattern similar to the availability bias when the labels for training prompts are imbalanced, and show that a toning-down transformation of the drug-drug description in a prompt can elicit a bias similar to the framing effect, warning users to distrust when prompting language models for answers.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 5269–5281\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nMind the Biases: Quantifying Cognitive Biases\nin Language Model Prompting\nRuixi Lin and Hwee Tou Ng\nDepartment of Computer Science\nNational University of Singapore\n{ruixi,nght}@comp.nus.edu.sg\nAbstract\nWe advocate the importance of exposing uncer-\ntainty on results of language model prompting\nwhich display bias modes resembling cognitive\nbiases, and propose to help users grasp the level\nof uncertainty via simple quantifying metrics.\nCognitive biases in the human decision mak-\ning process can lead to flawed responses when\nwe face uncertainty. Not surprisingly, we have\nseen biases in language models resembling cog-\nnitive biases as a result of training on biased\ntext, raising dangers in downstream tasks that\nare centered around people’s lives if users trust\ntheir results too much. In this work, we re-\nveal two bias modes leveraging cognitive biases\nwhen we prompt BERT, accompanied by two\nbias metrics. On a drug-drug interaction extrac-\ntion task, our bias measurements reveal an error\npattern similar to the availability bias when the\nlabels for training prompts are imbalanced, and\nshow that a toning-down transformation of the\ndrug-drug description in a prompt can elicit a\nbias similar to the framing effect, warning users\nto distrust when prompting language models\nfor answers.1\n1 Introduction\nCognitive biases describe the flawed human re-\nsponse patterns for decision making under uncer-\ntainty (Tversky and Kahneman, 1974, 1981; Ja-\ncowitz and Kahneman, 1995; Kahneman and Fred-\nerick, 2002; Meyer, 2004). For example, when\npeople are biased by the availability heuristic, they\nmake probability judgments based on the ease with\nwhich information comes to mind (Tversky and\nKahneman, 1973). Knowing cognitive biases can\nhelp predict what types of error will be made,\nwhich is also helpful for interpreting behaviors of\ngenerative systems such as language models, as\nthey may err in a similar pattern as humans do, espe-\ncially when the data used to build the systems carry\n1The source code of this paper is available at https:\n//github.com/nusnlp/CBPrompt.\nman-made biases (Schwartz et al., 2022; Jones and\nSteinhardt, 2022). We are inspired by leveraging\ncognitive biases – systematic error patterns which\ndeviate from rational decisions – to study error pat-\nterns of language models. We highlight the impor-\ntance of exposing uncertainty to users of language\nmodels (Pinhanez, 2021), and leverage cognitive\nbiases to quantify the level of imprecision in results\nwhen performing language model prompting via\nsimple, perceptual metrics.\nSome would argue that the biases in machines\nare a result of unmatched data distributions in train-\ning and test sets. However, merely matching train-\ning and test distributions does not solve the prob-\nlem of biased predictions for long-tailed input dis-\ntributions. For example, on the drug-drug interac-\ntion (DDI) dataset (Segura-Bedmar et al., 2013),\nthe training and test distributions are identically\nskewed, and there are 100 times more Negative\n(non-interacting) drug pairs than the interacting\ndrug pairs in both sets. Though performances on\nthe development set and test set are not too bad for\npositive class inputs with a prompt-based BERT\nmodel (Devlin et al., 2019), the model still most fre-\nquently mistakes positive pairs for negative pairs,\nas shown by the confusion matrix in the left part of\nFigure 1. This label bias towards Negative mimics\nthe availability bias. The availability bias is one of\nthe most common cognitive biases in real life, es-\npecially in doctors’ diagnoses which increase with\nyears of training (Mamede et al., 2010; Saposnik\net al., 2016). Moreover, an equal number of sam-\nples in each class during training does not guaran-\ntee that a “majority” class does not exist, especially\nwhen the input distribution of the negative class has\na higher variance (i.e., highly diversified samples\nin the negative class) and the samples within the\npositive class share more common characteristics.\nConsidering the input variance, even though sam-\nple sizes are the same, the negative class still can\nbe viewed as the majority class. More on this can\n5269\nFigure 1: Left: An illustration of the negative class vs. the four positive classes of the DDI data. The confusion\nmatrix of the development set predictions shows that the most frequent wrong prediction for positive input pairs is\nthe negative class (the majority label), calling for the need to quantify the availability bias. Right: An observation\non the framing effect paraphrase (in purple) and the original sentence (in blue): the paraphrase describes the two\nnon-interacting drugs in a slightly more toned-down way compared to the original sentence, and it leads to the\ncorrect prediction whereas the original sentence does not, calling for the need to quantify the framing effect.\nbe found in Appendix A.\nIn addition to the availability bias, the framing\neffect is another common cognitive bias. The fram-\ning effect is prevalent in medical diagnoses (Loke\nand Tan, 1992), where doctors intentionally frame\ndiagnoses positively (“90% chance to survive”) or\nnegatively (“10% chance to die”) to make patients\nperceive the results differently. It was recently\nfound that a failure mode of a code generation lan-\nguage model Codex (Chen et al., 2021) resembles\nthe framing effect – how an input prompt is framed\nchanges the model output predictably (Jones and\nSteinhardt, 2022). In our study, prompting BERT\nwith paraphrases generated by toning-down the\noriginal inputs improves prediction results, sug-\ngesting a bias brought by the tone of the input\nsentences.\nIt is important to see that the biases found above\nare not the expected behavior of BERT as a prompt-\nbased classification model, and our goal of this pa-\nper is to analyze these failure modes from the lens\nof cognitive biases and quantify them via simple\nmetrics. We are devoted to warning practitioners\nabout the risks of biased language model predic-\ntions, especially on biomedical tasks.\nOn a case study of the DDI extraction task, we\nmeasure output label distribution with content-free\nprompts and how model output changes when ap-\nplying a toning-down transformation to prompt\ntexts. Our key findings are:\n• We have identified an error pattern similar to\nthe availability bias when the labels for train-\ning prompts are imbalanced, and our measure-\nments quantitatively show that the bias is high-\nest towards the majority label.\n• We have motivated a toning-down transforma-\ntion of the drug-drug description in a prompt\nand found that this framing can elicit a bias\nsimilar to the framing effect.\n2 Related Work\n2.1 Cognitive Biases in Language Models\nRecent work on studying behaviors of pretrained\nlanguage models (PLMs) has revealed that some\nfailure modes bear resemblance to cognitive biases.\nWallace et al. (2019) study triggering prompts that\nfool the GPT-2 language model to generate con-\ntents that mimic hallucinations arising from cog-\nnitive biases. Zhao et al. (2021) find the major-\nity label bias to be one of the pitfalls for GPT-3\n(Brown et al., 2020), resembling the availability\nbias. Liu et al. (2022a) and Lu et al. (2022) show\nthat specific order of training examples can lead to\ndifferent model performance for GPT-3, analogous\nto the anchoring bias where estimates may be in-\nfluenced by what information is provided first or\nlast. Jones and Steinhardt (2022) capture failures\nin GPT-3 and Codex and find that error patterns of\nlarge language models (LLMs) resemble cognitive\nbiases in humans. Agrawal et al. (2022) also find a\nbias in GPT-3 which is similar to the framing effect,\nwhere using separate prompts rather than a chained\nprompt leads to wrong answers for medication ex-\ntraction. In a nutshell, most of these works focus on\n5270\nstudying issues of LLMs and have discerned their\nerror patterns’ resemblance to human cognitive bi-\nases. We follow this line of research, and argue that\nrelatively small PLMs, such as BERT, also display\nbiases resembling human cognitive biases, and we\npropose metrics to quantify two of these biases.\n2.2 Prompt-Based Language Models\nAs a booming research area, prompt-based methods\nshow their success through few-shot learning per-\nformance for language models (Zhao et al., 2021;\nJones and Steinhardt, 2022; Lu et al., 2022). How-\never, prompts may not be understood by models\nthe way humans do (Khashabi et al., 2022) and\nthey affect biases in models (Webson and Pavlick,\n2022; Utama et al., 2021; Prabhumoye et al., 2021).\nFrom a taxonomy viewpoint, prompt-based meth-\nods include: Prompt design, where the job is de-\nsigning human-readable prompts to demonstrate\nto a frozen language model for downstream tasks\n(Brown et al., 2020); Prompt tuning, where tunable\nsoft prompts are used for a frozen language model\n(Lester et al., 2021; Qin and Eisner, 2021; Sanh\net al., 2022; Liu et al., 2022b); and Prompt-based\nfine-tuning, which utilizes fixed human-readable\nprompts to fine-tune a model (Scao and Rush, 2021;\nGao et al., 2021; Schick and Schütze, 2021a,b;\nTam et al., 2021), such as pattern-exploiting train-\ning (Schick and Schütze, 2021b; Tam et al., 2021).\nWhile the first two types are popular for large lan-\nguage models such as GPTs, prompt-based fine-\ntuning is more common when prompting BERT\nand other relatively small language models. In this\nwork, we focus on prompt-based fine-tuning meth-\nods for BERT. Studies on interpretability focus on\nproviding measures for the incompleteness that pro-\nduces unquantified biases (Doshi-Velez and Kim,\n2017). Here we aim to fill in the gap for quantify-\ning the biases of prompt-based language models.\nIn addition, adversarial input is a popular technique\nto interpret how a model is fooled, by tweaking im-\nage pixels (Akhtar and Mian, 2018; Li et al., 2019)\nor textual triggers (Wallace et al., 2019). However,\nin this work, we seek to study the effect of altered\ntexts by leveraging cognitive bias patterns.\n3 Proposed Metrics\nWe propose two metrics for quantifying the bias\nmodes by the availability bias and the framing ef-\nfect respectively in prompt-based BERT, helping\nusers perceive how much bias comes with prompt-\ning results.\n3.1 The Availability Bias Metric\nThe error by the availability bias can be viewed\nas a shortcut of how a model “thinks” an answer\nis easier to recall and occurs more readily than\nit actually occurs at test time, as long as it has\nseen many prompted instances of the same answer\nduring training. On the DDI dataset, the majority\nlabel of prompts during training isNegative and the\ninference results show many false negatives. This\nresembles a situation when a human sees many\nnegative examples, then the human inferences are\nmore likely to be negative.\nThe Availability Bias Score.To quantify the avail-\nability bias for the DDI task, we are inspired by\nthe work of (Zhao et al., 2021), where a language\nmodel’s bias towards certain answers is estimated\nby feeding into the model a dummy test input that\nis content-free, i.e., with a dummy prompt, and\nmeasuring the deviation of the content-free pre-\ndiction score from the uniform prediction score.\nFollowing this idea, we propose an availability bias\nmetric via querying a model with multiple dummy\ntest prompt inputs and computing the deviation of\nthe prediction scores from the uniform prediction\nscore as the bias measurement.\nThe intuition is that, when a dummy-content test\nprompt is given, the best that an unbiased model\ncan do is to make a uniform random guess. If avail-\nability biases are present in the results, the number\nof predictions in each class will not be uniform.\nHenceforth, we can measure the deviation of the\nimbalanced predictions from the uniform predic-\ntion score to quantify the availability bias. We input\ndummy prompts to a language model, and measure\nthe frequency of prediction of each class, and then\ncompute the difference between class frequency\nand the uniform prediction score. For example,\nthe DDI task features 5 classes, including 4 DDI\ntypes and Negative. Hence, the difference from\n1/5 = 20% is the availability bias score of each\nclass.\nIn particular, we evaluate against a prompt-based\nfine-tuned BERT model and first obtain predictions\nconditioned on dummy prompt inputs. Let N de-\nnote the number of dummy test prompts, xdummy\ndenote a dummy prompt input.\nˆy= arg max\ny\np(y|xdummy) (1)\n5271\nwhere p(y|xdummy) is the softmax score obtained\nfrom the classification layer. Then we measure the\nfrequency of each class prediction, i.e., the number\nof dummy predictions in each class (denoted by\ncount(Ci)) divided by total number of dummy test\nprompts N.\ncount(Ci) =\nN∑\nj=1\n1 {ˆyj = Ci} (2)\nwhere 1 {·}evaluates to 1 when the condition in\nthe curly braces is met and 0 otherwise. Let M\ndenote the number of classes. We propose the ab-\nsolute deviation of the frequency from 1/Mas the\navailability bias score for each classCi, denoted by\nAvailability(Ci), and computed as follows:\nAvailability(Ci) =\n⏐⏐⏐count(Ci)\nN − 1\nM\n⏐⏐⏐ (3)\nFor fairness in the dummy prompt design, we\nextract from each class an equal number of test\ninstances and replace any UMLS keyword in the\ntext with a dummy word, N/A, to form dummy\nprompts. The choice of dummy word follows the\ncontent-free prompt design in (Zhao et al., 2021).\nThe reason to construct dummy prompts by extract-\ning templates from each class is to mitigate the\neffect that a class-specific content-free input may\ncorrelate with surface class patterns. Moreover,\nour metric is robust to the number of dummy test\nprompts used, and we discuss it in Appendix B.\n3.2 The Framing Effect Metric\nThe framing effect describes a biased perception\nabout the same thing when it is framed differently,\ne.g., toning down an expression. We observe\nsimilar biases in BERT prompting when we trans-\nform the same input text describing a drug-drug\ninteraction into a toned-down expression. When\nwe use paraphrases as input for prompt-based\nfine-tuning and testing, the test predictions change\nand test F1 score increases.\nMeasuring Framing Effect via Paraphrasing\nTo measure the framing effect, we paraphrase the\noriginal drug-drug interaction descriptions to sound\nsofter. We leverage the GPT-3 (Brown et al., 2020)\nmodel to build a paraphrase dataset, which contains\n500 training instances, 50 development instances,\nand 300 test instances. To gauge the quality of para-\nphrase generation, we first compute BERTScore\n(Zhang et al., 2020) of the 850 generated sentences\nand their source reference sentences. BERTScore is\na cosine similarity metric based on contextual em-\nbeddings to evaluate how much candidate and refer-\nence sentences match. The average BERTScore of\nall pairs is 97%, suggesting that the generated sen-\ntences are similar to the original sentences. How-\never, BERTScore does not take into account the\nspecific characteristics of a candidate, such as how\ntoned-down a paraphrase is compared to the origi-\nnal sentence.\nTherefore, we extend BERTScore and propose a\nFraming Effect Paraphrase (FEP) score to measure\nthe framing effect-based P,R,F 1 scores for para-\nphrases and their source sentences. We focus on\nthe framing effect of toning down a description and\nintroduce a dictionary of toned-down words. The\nFEP score will award a paraphrase if any word in\nthe paraphrase occurs in the dictionary, and penal-\nize the source sentence if it already contains toned-\ndown words. The reason to award a paraphrase\nis to encourage the use of toned-down words, and\nthe source sentence is penalized because the best\na paraphrase can do is to retain a tone-down word\n(since it is already in the source sentence), so the\nparaphrase will not receive a score for that word\nmatch. The dictionary of toned-down words, de-\nnoted as A, is a list of toned-down words/rules,\nsuch as hedging words and uncertainty adjectives\nor adverbs, such as “may”, “can”, and “reportedly”,\nand words indicating conditions, such as “if” and\n“when”.\nGiven a source sentence xand a paraphrase ˆx,\nto compute precision, the FEP score not only com-\nputes a maximal matching similarity (by greedy\nmatching) of each token ˆxj in ˆxto a token in x, but\nalso computes a reward score of each token in ˆxby\na scoring function ϕA, and precision is the larger of\nthe two. Similarly, to compute recall, the FEP score\ncomputes both a matching similarity of each token\nxi in xto a token in ˆxand a penalty score of each\ntoken in xby 1 −ϕA(xi), and recall is the smaller\nof the two. We then measure F1 score by combin-\ning the precision and recall. The FEP precision,\nrecall, and F1 are denoted as PFEP,RFEP,FFEP\nrespectively and are defined as follows:\nPFEP = 1\n|ˆx|\n∑\nˆxj ∈ˆx\nmax(max\nxi∈x\n(x⊤\ni ˆxj),ϕA(ˆxj))\n(4)\n5272\nwhere\nϕA(ˆxj) =\n{\n1 if ˆxj ∈A\n0 if ˆxj /∈A (5)\nRFEP = 1\n|x|\n∑\nxi∈x\nmin(max\nˆxj ∈ˆx\n(x⊤\ni ˆxj),1 −ϕA(xi))\n(6)\nFFEP = 2 PFEP ·RFEP\nPFEP + RFEP\n(7)\nThe original sentence x and the paraphrase ˆx\nare used as the input sentence of a prompt for\nfine-tuning BERT and testing, respectively. The\nprompt pattern will be introduced in Section 4.1.\nWe then calculate conditional probabilities in a\ngiven FFEP score range to measure the fine-grained\nperformance changes caused by the toning down\neffect. For FFEP in [a,b), we compute the con-\nditional probability of test pairs that are correctly\npredicted using the paraphrase input, given that\nthe predictions of their original sentence are wrong.\nSpecifically, we propose to measure the conditional\nprobability, denoted as ∆ in a given FFEP score\nrange, as follows:\n∆ =\n∑\nk∈T\n1 {f(xk) ̸= yk,f(ˆxk) = yk}\n∑\nk∈T\n1 {f(xk) ̸= yk}\n, (8)\ngiven FFEP(xk,ˆxk) in [a,b)\nwhere T denotes the indices of test instances with\nFFEP scores in the given range, f denotes the\nprompt-based language model, f(xk) and f(ˆxk)\ndenote the model prediction for the k-th test input\nxk and ˆxk respectively, and ydenotes the correct\nlabel.\n4 Experiments\n4.1 Dataset and Model\nWe focus on the relation extraction task of drug-\ndrug interactions, and use the DDIExtraction\ndataset (Segura-Bedmar et al., 2013) for our ex-\nperiments. The DDI dataset was constructed with\nMedLine abstracts and DrugBank documents on\ndrug-drug interactions. The DDI dataset uses 4\npositive DDI types to annotate the semantic re-\nlation for the interaction of a drug pair, includ-\ning Mechanism (DDI-mechanism), Effect (DDI-\neffect), Advice (DDI-advise), and Int (DDI-int),\nand a false class, which we refer to as the Nega-\ntive class. Mechanism denotes the relation about\na pharmacokinetic mechanism, Effect is used to\nannotate an effect or a pharmacodynamics mech-\nanism, Advice is the relation describing an advice\nor recommendation regarding a drug interaction,\nand Int is the type for any other positive interaction\ntypes (Zhang et al., 2018). The classes are im-\nbalanced with 85.2% Negative, 6.2% Effect, 4.9%\nMechanism, 3.1% Advice, and 0.6% Int. Among\nall positive DDI types, Mechanism and Advice are\nbetter recognized, while Effect and Int are harder\nto be identified. For data preprocessing, we fol-\nlow (Yasunaga et al., 2022) to replace the names of\ndrugs of a pair to be classified with “@DRUG$”,\nand split the dataset into 25,296 training, 2,496\ndevelopment, and 5,716 test instances.\nThe language model we study in this work is\nBERT-base2, which uses a transformer (Vaswani\net al., 2017) neural network pretrained on a 3.3 bil-\nlion word corpus of general-domain English texts.\nFor prompting BERT, we use the prompt-based\nfine-tuning method ADAPET3 (Tam et al., 2021).\nThe ADAPET method fine-tunes BERT via cloze-\nstyle prompt inputs with a [MASK] token (or to-\nkens). The output is a softmax score produced by\nBERT over the [MASK] token vocabulary, which\nthen corresponds to a class label. During training,\nthe model is fine-tuned to minimize the sum of\nthe decoupled label loss and the label-conditioned\nMLM loss. We stick to a single prompt pattern,\n“([MASK]) [TEXT]”, where [MASK] is the label\nphrase to be predicted and [TEXT] is the input\ndrug pair description. The verbalizers are {“0\":\n“false\", “DDI-effect\": “effect\", “DDI-mechanism\":\n“mechanism\", “DDI-advise\": “advice\", “DDI-int\":\n“interaction\"}. We use a simple prompt pattern in\nthis work. Since we obtain similar findings with\nmore complex prompt patterns, we do not include\nthem in this paper.\n4.2 Measuring Availability Bias\nIn our experiments, we construct a total of 100\ndummy test prompts, with 20 templates randomly\nextracted from each class. For the dummy test\nprompt design, we search for UMLS keyword\ncontents in a sentence and replace them with\ndummy phrases N/A, and apply the prompt pattern:\n([MASK]) [TEXT]. The [TEXT] part contains a\n2https://huggingface.co/\nbert-base-uncased\n3https://github.com/rrmenon10/ADAPET\n5273\nAvailability bias score (%)\nTraining size 10-shot 100-shot 1,000-shot 10,000-shot 25,296\nNegative 26.3 (2.1) 77.7 (2.1) 39.7 (3.9) 47.0 (3.6) 52.0 (2.8)\nMechanism 20.0 (0.0) 20.0 (0.0) 13.7 (0.5) 17.3 (1.2) 16.7 (1.3)\nAdvice 20.0 (0.0) 18.3 (1.7) 8.3 (2.6) 7.0 (2.4) 8.3 (2.6)\nEffect 33.7 (2.1) 20.0 (0.0) 16.3 (2.1) 12.7 (2.5) 11.7 (2.4)\nInt 20.0 (0.0) 19.3 (0.5) 1.3 (0.5) 10.0 (0.8) 15.3 (1.2)\nTable 1: Availability bias score (%) for each class on the DDI task. Column 2-5: few-shot training settings. Column\n6: full training set. Mean and standard deviation are shown.\nTest data # pairs F1\nParaphrase, including\ninvalid paraphrases 300 44.6\nOriginal sentences\nof the above 300 10.2\nParaphrase, excluding\ninvalid paraphrases 208 55.7\nOriginal sentences\nof the above 208 9.0\nTable 2: F1 score (%) on the framing effect test set.\ntest sentence with multiple N/As and the [MASK]\npart will be the predicted label during testing. For\nUMLS keyword extraction, we exploit MetaMap4\nand its Python wrapper5. An example dummy test\nprompt is shown below.\n([MASK]) @DRUG$ competes with a N/A\nof N/A for N/A N/A N/A notably\nN/A N/A N/A N/A N/A @DRUG$\nN/A N/A N/A and N/A N/A\nThe language model we measure against is\nBERT-base, fine-tuned via the ADAPET method\nwith prompt inputs of the full DDI training set and\nfew-shot training sets including 10, 100, 1,000,\n10,000-shot training settings. Note that the original\ntest F1 scores of the positive DDI types on the 10,\n100, 1,000, 10,000-shot, and full training set are\n5.04%, 12.36%, 56.16%, 74.64%, and 80.36% re-\nspectively. We repeat the experiments three times\nwith different random seeds, and report the mean\nand standard deviation.\n4https://lhncbc.nlm.nih.gov/ii/tools/\nMetaMap.html\n5https://github.com/AnthonyMRios/\npymetamap\nTable 1 shows the availability bias score (%) for\neach class, on different fine-tuned BERT models.\nThe rightmost column in Table 1 represents the\nscores for the BERT model fine-tuned on the full\ntraining set. The upper limit for the availability bias\nscore is (100-20)/100=80%, and the closer the bias\nscore gets to the upper limit, the more biased the\nmodel makes predictions towards the associated\nclass. As expected, the bias towards the Negative\nclass is the largest, by 52%, suggesting that when\nsupposedly making random guess for dummy in-\nputs, the model’s behavior is vastly biased towards\npredicting drug pairs as no relation.\nIn addition, results in column 2 to column 5 in\nTable 1 present availability bias scores for few-\nshot training cases. It is interesting to see that the\n10-shot trained model exhibits lower bias score\ntowards the majority class. However, its accuracy\non the original full test set is only 5.04%. For\nthe remaining cases, the conclusion that the model\noutputs are biased towards the majority class also\nholds in few-shot training settings.\nThough one may argue that labels in prompts\ndo not matter much for classification as in tradi-\ntional supervised learning (Min et al., 2022), we\nfind that it is not true from our availability bias\nscores obtained. The label in a prompt still plays an\nimportant part in prompt-based training, leading to\navailability bias-like predictions. The practical im-\nplication of knowing this bias pattern is that when\nusers see model predictions, they can be informed\nthat a prediction given by a model is biased towards\nthe predicted label by the quantified amount.\n4.3 Measuring Framing Effect\nWe first build the paraphrase dataset, where\nwe randomly select 500 training instances, 50\ndevelopment instances, and 300 test instances\nfrom the full DDI training, development, and\n5274\nFFEP\n# Ori.\nwrong\n# Pp.\ncorrect ∆\n[0.99, 1.00) 78 77 98.7\n[0.97, 0.99) 23 12 52.2\n[0.95, 0.97) 35 27 77.1\n[0.00, 1.00) 141 119 84.4\nTable 3: Framing effect score: conditional probabilities\n(∆) on the DDI test set. # Ori. wrong denotes the\nnumber of wrong predictions with the original inputs.\n# Pp. correctdenotes the number of correct predictions\nusing paraphrase inputs within those that are originally\npredicted wrongly. Results for FFEP scores lower than\n0.95 are not presented and used for analysis as there are\ntoo few such test instances (less than 3).\ntest set respectively for paraphrasing. The\nparaphrases are generated by prompting GPT-3\nwith a demonstration and the actual query, where a\npriming example (in blue) is appended to the test\nsentence to be paraphrased (denoted as [INPUT]).\nIn our experiments, we design 8 priming examples\nand randomly pick one of them as demonstration.\nAn example GPT-3 query is given below.\nParaphrase the following drug interaction\ndescription. === Although @DRUG$ exerts\na slight intrinsic anticonvulsant effect, its\nabrupt suppression of the protective effect of\na @DRUG$ agonist can give rise to convulsions\nin epileptic patients. Description: @DRUG$\nexerts a slight intrinsic anticonvulsant effect,\nand its abrupt suppression of the protective\neffect of a @DRUG$ agonist is reportedly to\ngive rise to convulsions in epileptic patients.\n=== [INPUT] Rephrase the above description\nto sound soft. Write the description in a warm\ntone. Description:\nWe illustrate several GPT-3 generated para-\nphrases of the test instances in Figure 2. For\ntraining and testing, we use all the generated\nparaphrases, although some paraphrases contain\nhallucinations (e.g., an untruthful trailing sentence\nthat may come from the priming example) or miss\nmajor content (e.g., missing the mention of a drug\nto be predicted). The language model we measure\nagainst is the BERT-base model, fine-tuned via the\nADAPET method on the 500 training instances.\nAn example of a prompt input to BERT is as\nfollows:\n([MASK]) If you are taking @DRUG$ or\nother potent CYP3A4 inhibitors such as\nother azole antifungals (eg, itraconazole,\n@DRUG$) or macrolide antibiotics (eg,\nerythromycin, clarithromycin) or\ncyclosporine or vinblastine, the\nrecommended dose of DETROL LA is\n2 mg daily.\nTable 2 shows the testF1 scores on both the orig-\ninal test sentences and their GPT-3 paraphrases. As\nshown by the last two rows, the 208 valid para-\nphrases obtain an F1 score of 55.7%, which is\n46.7% higher than the 208 original sentences which\nobtain an F1 score of 9.0%.\nMore importantly, for the 208 drug pairs with\nvalid paraphrases, we show in Table 3 that the ∆\nis 84.4%, and if we focus on highly toned-down\nparaphrases in FFEP range [0.99,1.00), the con-\nditional probability reaches 98.7%, showing that\nframing an original drug-drug interaction descrip-\ntion into a toned-down paraphrase helps to improve\nrelation extraction. These results suggest that ton-\ning down the input text in a prompt can elicit a bias\nin predictions qualitatively similar to the framing\neffect.\nFurthermore, we illustrate the original sentences\nand their framed paraphrases through some test\npairs in Figure 2. In Example 1, the correct re-\nlation “effect” is identified given the paraphrase\ninput, while no interaction is detected given the\noriginal sentence input. Compared to the original\ntext which uses the word “produce” to describe side\neffects, the words “can cause ” used in the para-\nphrase are more toned-down. In Example 2, the\ncorrect relation is no interaction, which is identi-\nfied correctly using the paraphrase input, while the\nwrong prediction “effect” is made using the origi-\nnal sentence. In the original sentence, “requires” is\nused for the list of drugs, while “may require” is\nused in the paraphrase, toning down the expression.\n5 Discussion\nFew-shot Training vs. the Availability Bias.We\nhave seen from Table 1 that at 10-shot, the avail-\nability bias towards Negative is not as obvious and\nthe scores are more similar among the five classes.\nThis is in contrast to the other few-shot learning\ncases with more training instances, where the avail-\nability bias becomes more obvious for the negative\n5275\nFigure 2: Examples of GPT-3 generated paraphrases of the test instances. FFEP (toned-down paraphrasing scores)\nfrom top to bottom: 0.994, 0.988.\nclass as the number of training instances increases.\nIt does not suggest that training on more instances\nwill worsen the availability bias, but more class-\nbiased training prompts will amplify the availabil-\nity bias. That is, since more negative class instances\nare drawn for a larger number of training instances,\nthe majority class has been seen by the model more\nfrequently, causing biased predictions due to this\nincreased availability. Prompt-based learning is\nnot immune to imbalanced class distribution even\nunder few-shot settings, as it is sometimes hard to\nobtain real class-balanced few-shot instances (this\nis elaborated in Appendix A).\n6 Conclusion\nIn this work, we identify and quantify two bias\nmodes in BERT’s prompt-based predictions, lever-\naging the availability bias and the framing effect on\nbiomedical drug-drug interaction extraction. The\nerror mode of the availability bias suggests that the\nlabel for a prompt still matters for prompt-based\nlearning, as shown by a large availability bias score\ntowards the majority class, which is 52% on a scale\nof 0 to 80%. We also find that a toning-down trans-\nformation of the drug-drug description in a prompt\ncan elicit a bias similar to the framing effect, since\nwhen we tone down the input description, 84.4%\nof drug pairs that are wrongly classified with the\noriginal text are now correctly predicted with their\ntoned-down paraphrases. For highly toned-down\nparaphrases (as measured by FFEP above 0.99),\nthis conditional probability reaches 98.7%. The\nmagnitude of these biases suggests that language\nmodel users need to be aware of the imprecision of\ntheir prompting results.\n7 Limitations\nThe limitations are that our use of GPT-3 some-\ntimes generates hallucinated texts, thus reducing\nthe effectiveness in generating valid paraphrases.\nThe dictionary of toned-down words could include\nmore semantic rules or could be built automatically,\nwhich will be left as future work.\nReferences\nMonica Agrawal, Stefan Hegselmann, Hunter Lang,\nYoon Kim, and David Sontag. 2022. Large language\nmodels are zero-shot clinical information extractors.\narXiv preprint arXiv:2205.12689.\nNaveed Akhtar and Ajmal Mian. 2018. Threat of adver-\nsarial attacks on deep learning in computer vision: A\nsurvey. arXiv preprint arXiv:1801.00553.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Josh Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\n5276\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. arXiv\npreprint arXiv:2107.03374.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 4171–4186.\nFinale Doshi-Velez and Been Kim. 2017. Towards a\nrigorous science of interpretable machine learning.\narXiv preprint arXiv:1702.08608.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021.\nMaking pre-trained language models better few-shot\nlearners. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 3816–3830.\nKaren E. Jacowitz and Daniel Kahneman. 1995. Mea-\nsures of anchoring in estimation tasks. Personality\nand Social Psychology Bulletin, 21:1161–1166.\nErik Jones and Jacob Steinhardt. 2022. Capturing fail-\nures of large language models via human cognitive\nbiases. In Advances in Neural Information Process-\ning Systems.\nDaniel Kahneman and Shane Frederick. 2002. Rep-\nresentativeness revisited: Attribute substitution in\nintuitive judgment. Heuristics and biases: The psy-\nchology of intuitive judgment, 49:49–81.\nDaniel Khashabi, Xinxi Lyu, Sewon Min, Lianhui\nQin, Kyle Richardson, Sean Welleck, Hannaneh Ha-\njishirzi, Tushar Khot, Ashish Sabharwal, Sameer\nSingh, and Yejin Choi. 2022. Prompt wayward-\nness: The curious case of discretized interpretation\nof continuous prompts. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 3631–3643.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3045–3059.\nBai Li, Changyou Chen, Wenlin Wang, and Lawrence\nCarin. 2019. Certified adversarial robustness with\nadditive noise. In Advances in Neural Information\nProcessing Systems, pages 9464–9474.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022a. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Weng Tam, Zhengx-\niao Du, Zhilin Yang, and Jie Tang. 2022b. P-tuning:\nPrompt tuning can be comparable to fine-tuning\nacross scales and tasks. In Proceedings of the 60th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 61–68.\nWing Hong Loke and Kai Foong Tan. 1992. Effects\nof framing and missing information in expert and\nnovice judgment. Bulletin of the Psychonomic Soci-\nety, 30:187–190.\nYao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel,\nand Pontus Stenetorp. 2022. Fantastically ordered\nprompts and where to find them: Overcoming few-\nshot prompt order sensitivity. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 8086–8098.\nSílvia Mamede, Tamara van Gog, Kees van den Berge,\nRemy M. J. P. Rikers, Jan L. C. M. van Saase, Coen\nvan Guldener, and Henk G. Schmidt. 2010. Effect of\navailability bias and reflective reasoning on diagnos-\ntic accuracy among internal medicine residents. Jour-\nnal of the American Medical Association, 304:1198–\n1203.\nDavid E. Meyer. 2004. Semantic priming well estab-\nlished. Science, 345:523–523.\nSewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe,\nMike Lewis, Hannaneh Hajishirzi, and Luke Zettle-\nmoyer. 2022. Rethinking the role of demonstra-\ntions: What makes in-context learning work? arXiv\npreprint arXiv:2202.12837.\nClaudio S. Pinhanez. 2021. Expose uncertainty, instill\ndistrust, avoid explanations: Towards ethical guide-\nlines for AI. CoRR, abs/2112.01281.\nShrimai Prabhumoye, Rafal Kocielnik, Mohammad\nShoeybi, Anima Anandkumar, and Bryan Catan-\nzaro. 2021. Few-shot instruction prompts for pre-\ntrained language models to detect social biases.arXiv\npreprint arXiv:2112.07868.\nGuanghui Qin and Jason Eisner. 2021. Learning how\nto ask: Querying LMs with mixtures of soft prompts.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5203–5212.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Fevry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\n5277\nThomas Wolf, and Alexander M Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In International Conference on Learning\nRepresentations.\nGustavo Saposnik, Donald Redelmeier, Christian C\nRuff, and Philippe N Tobler. 2016. Cognitive bi-\nases associated with medical decisions: A system-\natic review. BMC Medical Informatics and Decision\nMaking, 16.\nTeven Le Scao and Alexander Rush. 2021. How many\ndata points is a prompt worth? In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2627–2636.\nTimo Schick and Hinrich Schütze. 2021a. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In Proceedings of the\n16th Conference of the European Chapter of the As-\nsociation for Computational Linguistics, pages 255–\n269.\nTimo Schick and Hinrich Schütze. 2021b. It’s not just\nsize that matters: Small language models are also few-\nshot learners. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2339–2352.\nReva Schwartz, Apostol Vassilev, Kristen K. Greene,\nLori Perine, Andrew Burt, and Patrick Hall. 2022.\nTowards a standard for identifying and managing\nbias in artificial intelligence. Special Publication\n(NIST SP).\nIsabel Segura-Bedmar, Paloma Martínez, and María\nHerrero-Zazo. 2013. Semeval-2013 task 9 : Extrac-\ntion of drug-drug interactions from biomedical texts\n(ddiextraction 2013). In Second Joint Conference\non Lexical and Computational Semantics (*SEM),\nVolume 2: Proceedings of the Seventh International\nWorkshop on Semantic Evaluation (SemEval 2013),\npages 341–350.\nDerek Tam, Rakesh R. Menon, Mohit Bansal, Shashank\nSrivastava, and Colin Raffel. 2021. Improving and\nsimplifying pattern exploiting training. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, pages 4980–4991.\nAmos Tversky and Daniel Kahneman. 1973. Availabil-\nity: A heuristic for judging frequency and probability.\nCognitive Psychology, 5:207–232.\nAmos Tversky and Daniel Kahneman. 1974. Judgment\nunder uncertainty: Heuristics and biases. Science,\n185:1124–1131.\nAmos Tversky and Daniel Kahneman. 1981. The fram-\ning of decisions and the psychology of choice. Sci-\nence, 211:453–458.\nPrasetya Utama, Nafise Sadat Moosavi, Victor Sanh,\nand Iryna Gurevych. 2021. Avoiding inference\nheuristics in few-shot prompt-based finetunings. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing , pages\n9063–9074.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30.\nEric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner,\nand Sameer Singh. 2019. Universal adversarial trig-\ngers for attacking and analyzing nlp. In Proceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\npages 2153–2162.\nAlbert Webson and Ellie Pavlick. 2022. Do prompt-\nbased models really understand the meaning of their\nprompts? In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 2300–2344.\nMichihiro Yasunaga, Jure Leskovec, and Percy Liang.\n2022. LinkBERT: Pretraining language models with\ndocument links. In Proceedings of the 60th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 8003–8016.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. BERTScore:\nEvaluating text generation with BERT. In Interna-\ntional Conference on Learning Representations.\nYijia Zhang, Wei Zheng, Hongfei Lin, Jian Wang, Zhi-\nhao Yang, and Michel Dumontier. 2018. Drug–drug\ninteraction extraction via hierarchical rnns on se-\nquence and shortest dependency paths. Bioinformat-\nics, 34:828–835.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models. In\nProceedings of the 38th International Conference on\nMachine Learning.\nAppendix\nA The Majority Label in Training\nSince the availability bias arises from the major-\nity label in training, where the majority label is\ntypically defined by class size, one may argue that\nthere should not be an availability bias with an\nequal number of class instances. However, we con-\nstructed a balanced training data set of equal class\nsize for fine-tuning (by sampling 2,000 instances\nfrom each of the five classes, and all four positive\n5278\nclasses include duplicate instances since their class\nsizes are less than 2,000). Interestingly, we still ob-\nserve that the predictions for the positive classes are\nbiased towards the negative class on the test set, as\nshown by the confusion matrix in Figure 3. Except\nInt, wrong predictions most frequently fall into the\nnegative class for all other positive classes Effect,\nMechanism, Advice. This does not contradict our\nconclusion that the availability bias exists, and it\nfurther suggests that the majority label should not\nbe solely defined by class size, but the class with\nthe highest input variance.\nFigure 3: Test confusion matrix for class-balanced train-\ning data (2,000 instances per class). Balanced training\ndata does not alleviate the availability bias at inference\ntime. For positive classes except Int, wrong predictions\nmost frequently fall into the negative class.\nFigure 4: Number of dummy test prompts for availabil-\nity bias measurement.\nB Number of Dummy Test Prompts for\nAvailability Bias Measurement\nWe increase the number of dummy test prompts N\nto show the stability of our availability bias metric,\nwhere N ranges from 100 to 1000 with a step size\nof 100. We repeat our experiments three times\nfor each N and calculate the mean and standard\ndeviation. When creating dummy test prompts,\nif the number of dummy templates that need to\nbe drawn from a class exceeds the class size, we\nenable upsampling of duplicate templates from that\nclass. Figure 4 shows that the availability bias\nmeasurement is stable for N ≥100, suggesting\nthat our proposed metric can be used with as few\nas 100 dummy test prompts.\n5279\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 7\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract, Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 4, we use pretrained BERT and build upon the ADAPET open source code for experiments\nSection 4.2, we use the MetaMap software for UMLS keywords extraction Section 4.3, we use GPT-3 for\nparaphrase generation\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 3, Section 4\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNot applicable. Left blank.\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nSection 4.1\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n5280\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4.1\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4.2, 4.3\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 4\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n5281",
  "topic": "Distrust",
  "concepts": [
    {
      "name": "Distrust",
      "score": 0.8328844308853149
    },
    {
      "name": "Computer science",
      "score": 0.6977004408836365
    },
    {
      "name": "Cognitive bias",
      "score": 0.6622477769851685
    },
    {
      "name": "Cognition",
      "score": 0.6474105715751648
    },
    {
      "name": "Language model",
      "score": 0.560967743396759
    },
    {
      "name": "Cognitive psychology",
      "score": 0.5416986346244812
    },
    {
      "name": "Debiasing",
      "score": 0.5362303853034973
    },
    {
      "name": "GRASP",
      "score": 0.4940655529499054
    },
    {
      "name": "Natural language processing",
      "score": 0.4471125304698944
    },
    {
      "name": "Framing (construction)",
      "score": 0.44175416231155396
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4101729094982147
    },
    {
      "name": "Psychology",
      "score": 0.33505693078041077
    },
    {
      "name": "Cognitive science",
      "score": 0.19676604866981506
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Structural engineering",
      "score": 0.0
    },
    {
      "name": "Neuroscience",
      "score": 0.0
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ]
}